<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240929.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Space-time 2D Gaussian Splatting for Accurate Surface Reconstruction\n  under Complex Dynamic Scenes", "author": "Shuo Wang and Binbin Huang and Ruoyu Wang and Shenghua Gao", "abstract": "  Previous surface reconstruction methods either suffer from low geometric\naccuracy or lengthy training times when dealing with real-world complex dynamic\nscenes involving multi-person activities, and human-object interactions. To\ntackle the dynamic contents and the occlusions in complex scenes, we present a\nspace-time 2D Gaussian Splatting approach. Specifically, to improve geometric\nquality in dynamic scenes, we learn canonical 2D Gaussian splats and deform\nthese 2D Gaussian splats while enforcing the disks of the Gaussian located on\nthe surface of the objects by introducing depth and normal regularizers.\nFurther, to tackle the occlusion issues in complex scenes, we introduce a\ncompositional opacity deformation strategy, which further reduces the surface\nrecovery of those occluded areas. Experiments on real-world sparse-view video\ndatasets and monocular dynamic datasets demonstrate that our reconstructions\noutperform state-of-the-art methods, especially for the surface of the details.\nThe project page and more visualizations can be found at:\nhttps://tb2-sy.github.io/st-2dgs/.\n", "link": "http://arxiv.org/abs/2409.18852v1", "date": "2024-09-27", "relevancy": 3.5822, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7371}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7089}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Space-time%202D%20Gaussian%20Splatting%20for%20Accurate%20Surface%20Reconstruction%0A%20%20under%20Complex%20Dynamic%20Scenes&body=Title%3A%20Space-time%202D%20Gaussian%20Splatting%20for%20Accurate%20Surface%20Reconstruction%0A%20%20under%20Complex%20Dynamic%20Scenes%0AAuthor%3A%20Shuo%20Wang%20and%20Binbin%20Huang%20and%20Ruoyu%20Wang%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%20Previous%20surface%20reconstruction%20methods%20either%20suffer%20from%20low%20geometric%0Aaccuracy%20or%20lengthy%20training%20times%20when%20dealing%20with%20real-world%20complex%20dynamic%0Ascenes%20involving%20multi-person%20activities%2C%20and%20human-object%20interactions.%20To%0Atackle%20the%20dynamic%20contents%20and%20the%20occlusions%20in%20complex%20scenes%2C%20we%20present%20a%0Aspace-time%202D%20Gaussian%20Splatting%20approach.%20Specifically%2C%20to%20improve%20geometric%0Aquality%20in%20dynamic%20scenes%2C%20we%20learn%20canonical%202D%20Gaussian%20splats%20and%20deform%0Athese%202D%20Gaussian%20splats%20while%20enforcing%20the%20disks%20of%20the%20Gaussian%20located%20on%0Athe%20surface%20of%20the%20objects%20by%20introducing%20depth%20and%20normal%20regularizers.%0AFurther%2C%20to%20tackle%20the%20occlusion%20issues%20in%20complex%20scenes%2C%20we%20introduce%20a%0Acompositional%20opacity%20deformation%20strategy%2C%20which%20further%20reduces%20the%20surface%0Arecovery%20of%20those%20occluded%20areas.%20Experiments%20on%20real-world%20sparse-view%20video%0Adatasets%20and%20monocular%20dynamic%20datasets%20demonstrate%20that%20our%20reconstructions%0Aoutperform%20state-of-the-art%20methods%2C%20especially%20for%20the%20surface%20of%20the%20details.%0AThe%20project%20page%20and%20more%20visualizations%20can%20be%20found%20at%3A%0Ahttps%3A//tb2-sy.github.io/st-2dgs/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpace-time%25202D%2520Gaussian%2520Splatting%2520for%2520Accurate%2520Surface%2520Reconstruction%250A%2520%2520under%2520Complex%2520Dynamic%2520Scenes%26entry.906535625%3DShuo%2520Wang%2520and%2520Binbin%2520Huang%2520and%2520Ruoyu%2520Wang%2520and%2520Shenghua%2520Gao%26entry.1292438233%3D%2520%2520Previous%2520surface%2520reconstruction%2520methods%2520either%2520suffer%2520from%2520low%2520geometric%250Aaccuracy%2520or%2520lengthy%2520training%2520times%2520when%2520dealing%2520with%2520real-world%2520complex%2520dynamic%250Ascenes%2520involving%2520multi-person%2520activities%252C%2520and%2520human-object%2520interactions.%2520To%250Atackle%2520the%2520dynamic%2520contents%2520and%2520the%2520occlusions%2520in%2520complex%2520scenes%252C%2520we%2520present%2520a%250Aspace-time%25202D%2520Gaussian%2520Splatting%2520approach.%2520Specifically%252C%2520to%2520improve%2520geometric%250Aquality%2520in%2520dynamic%2520scenes%252C%2520we%2520learn%2520canonical%25202D%2520Gaussian%2520splats%2520and%2520deform%250Athese%25202D%2520Gaussian%2520splats%2520while%2520enforcing%2520the%2520disks%2520of%2520the%2520Gaussian%2520located%2520on%250Athe%2520surface%2520of%2520the%2520objects%2520by%2520introducing%2520depth%2520and%2520normal%2520regularizers.%250AFurther%252C%2520to%2520tackle%2520the%2520occlusion%2520issues%2520in%2520complex%2520scenes%252C%2520we%2520introduce%2520a%250Acompositional%2520opacity%2520deformation%2520strategy%252C%2520which%2520further%2520reduces%2520the%2520surface%250Arecovery%2520of%2520those%2520occluded%2520areas.%2520Experiments%2520on%2520real-world%2520sparse-view%2520video%250Adatasets%2520and%2520monocular%2520dynamic%2520datasets%2520demonstrate%2520that%2520our%2520reconstructions%250Aoutperform%2520state-of-the-art%2520methods%252C%2520especially%2520for%2520the%2520surface%2520of%2520the%2520details.%250AThe%2520project%2520page%2520and%2520more%2520visualizations%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//tb2-sy.github.io/st-2dgs/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Space-time%202D%20Gaussian%20Splatting%20for%20Accurate%20Surface%20Reconstruction%0A%20%20under%20Complex%20Dynamic%20Scenes&entry.906535625=Shuo%20Wang%20and%20Binbin%20Huang%20and%20Ruoyu%20Wang%20and%20Shenghua%20Gao&entry.1292438233=%20%20Previous%20surface%20reconstruction%20methods%20either%20suffer%20from%20low%20geometric%0Aaccuracy%20or%20lengthy%20training%20times%20when%20dealing%20with%20real-world%20complex%20dynamic%0Ascenes%20involving%20multi-person%20activities%2C%20and%20human-object%20interactions.%20To%0Atackle%20the%20dynamic%20contents%20and%20the%20occlusions%20in%20complex%20scenes%2C%20we%20present%20a%0Aspace-time%202D%20Gaussian%20Splatting%20approach.%20Specifically%2C%20to%20improve%20geometric%0Aquality%20in%20dynamic%20scenes%2C%20we%20learn%20canonical%202D%20Gaussian%20splats%20and%20deform%0Athese%202D%20Gaussian%20splats%20while%20enforcing%20the%20disks%20of%20the%20Gaussian%20located%20on%0Athe%20surface%20of%20the%20objects%20by%20introducing%20depth%20and%20normal%20regularizers.%0AFurther%2C%20to%20tackle%20the%20occlusion%20issues%20in%20complex%20scenes%2C%20we%20introduce%20a%0Acompositional%20opacity%20deformation%20strategy%2C%20which%20further%20reduces%20the%20surface%0Arecovery%20of%20those%20occluded%20areas.%20Experiments%20on%20real-world%20sparse-view%20video%0Adatasets%20and%20monocular%20dynamic%20datasets%20demonstrate%20that%20our%20reconstructions%0Aoutperform%20state-of-the-art%20methods%2C%20especially%20for%20the%20surface%20of%20the%20details.%0AThe%20project%20page%20and%20more%20visualizations%20can%20be%20found%20at%3A%0Ahttps%3A//tb2-sy.github.io/st-2dgs/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18852v1&entry.124074799=Read"},
{"title": "From Seconds to Hours: Reviewing MultiModal Large Language Models on\n  Comprehensive Long Video Understanding", "author": "Heqing Zou and Tianze Luo and Guiyang Xie and  Victor and  Zhang and Fengmao Lv and Guangcong Wang and Juanyang Chen and Zhuochen Wang and Hansheng Zhang and Huaijian Zhang", "abstract": "  The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding.\n", "link": "http://arxiv.org/abs/2409.18938v1", "date": "2024-09-27", "relevancy": 3.1172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6445}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Seconds%20to%20Hours%3A%20Reviewing%20MultiModal%20Large%20Language%20Models%20on%0A%20%20Comprehensive%20Long%20Video%20Understanding&body=Title%3A%20From%20Seconds%20to%20Hours%3A%20Reviewing%20MultiModal%20Large%20Language%20Models%20on%0A%20%20Comprehensive%20Long%20Video%20Understanding%0AAuthor%3A%20Heqing%20Zou%20and%20Tianze%20Luo%20and%20Guiyang%20Xie%20and%20%20Victor%20and%20%20Zhang%20and%20Fengmao%20Lv%20and%20Guangcong%20Wang%20and%20Juanyang%20Chen%20and%20Zhuochen%20Wang%20and%20Hansheng%20Zhang%20and%20Huaijian%20Zhang%0AAbstract%3A%20%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20visual%20encoders%20has%0Arecently%20shown%20promising%20performance%20in%20visual%20understanding%20tasks%2C%20leveraging%0Atheir%20inherent%20capability%20to%20comprehend%20and%20generate%20human-like%20text%20for%20visual%0Areasoning.%20Given%20the%20diverse%20nature%20of%20visual%20data%2C%20MultiModal%20Large%20Language%0AModels%20%28MM-LLMs%29%20exhibit%20variations%20in%20model%20designing%20and%20training%20for%0Aunderstanding%20images%2C%20short%20videos%2C%20and%20long%20videos.%20Our%20paper%20focuses%20on%20the%0Asubstantial%20differences%20and%20unique%20challenges%20posed%20by%20long%20video%20understanding%0Acompared%20to%20static%20image%20and%20short%20video%20understanding.%20Unlike%20static%20images%2C%0Ashort%20videos%20encompass%20sequential%20frames%20with%20both%20spatial%20and%20within-event%0Atemporal%20information%2C%20while%20long%20videos%20consist%20of%20multiple%20events%20with%0Abetween-event%20and%20long-term%20temporal%20information.%20In%20this%20survey%2C%20we%20aim%20to%0Atrace%20and%20summarize%20the%20advancements%20of%20MM-LLMs%20from%20image%20understanding%20to%0Along%20video%20understanding.%20We%20review%20the%20differences%20among%20various%20visual%0Aunderstanding%20tasks%20and%20highlight%20the%20challenges%20in%20long%20video%20understanding%2C%0Aincluding%20more%20fine-grained%20spatiotemporal%20details%2C%20dynamic%20events%2C%20and%0Along-term%20dependencies.%20We%20then%20provide%20a%20detailed%20summary%20of%20the%20advancements%0Ain%20MM-LLMs%20in%20terms%20of%20model%20design%20and%20training%20methodologies%20for%0Aunderstanding%20long%20videos.%20Finally%2C%20we%20compare%20the%20performance%20of%20existing%0AMM-LLMs%20on%20video%20understanding%20benchmarks%20of%20various%20lengths%20and%20discuss%0Apotential%20future%20directions%20for%20MM-LLMs%20in%20long%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Seconds%2520to%2520Hours%253A%2520Reviewing%2520MultiModal%2520Large%2520Language%2520Models%2520on%250A%2520%2520Comprehensive%2520Long%2520Video%2520Understanding%26entry.906535625%3DHeqing%2520Zou%2520and%2520Tianze%2520Luo%2520and%2520Guiyang%2520Xie%2520and%2520%2520Victor%2520and%2520%2520Zhang%2520and%2520Fengmao%2520Lv%2520and%2520Guangcong%2520Wang%2520and%2520Juanyang%2520Chen%2520and%2520Zhuochen%2520Wang%2520and%2520Hansheng%2520Zhang%2520and%2520Huaijian%2520Zhang%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520visual%2520encoders%2520has%250Arecently%2520shown%2520promising%2520performance%2520in%2520visual%2520understanding%2520tasks%252C%2520leveraging%250Atheir%2520inherent%2520capability%2520to%2520comprehend%2520and%2520generate%2520human-like%2520text%2520for%2520visual%250Areasoning.%2520Given%2520the%2520diverse%2520nature%2520of%2520visual%2520data%252C%2520MultiModal%2520Large%2520Language%250AModels%2520%2528MM-LLMs%2529%2520exhibit%2520variations%2520in%2520model%2520designing%2520and%2520training%2520for%250Aunderstanding%2520images%252C%2520short%2520videos%252C%2520and%2520long%2520videos.%2520Our%2520paper%2520focuses%2520on%2520the%250Asubstantial%2520differences%2520and%2520unique%2520challenges%2520posed%2520by%2520long%2520video%2520understanding%250Acompared%2520to%2520static%2520image%2520and%2520short%2520video%2520understanding.%2520Unlike%2520static%2520images%252C%250Ashort%2520videos%2520encompass%2520sequential%2520frames%2520with%2520both%2520spatial%2520and%2520within-event%250Atemporal%2520information%252C%2520while%2520long%2520videos%2520consist%2520of%2520multiple%2520events%2520with%250Abetween-event%2520and%2520long-term%2520temporal%2520information.%2520In%2520this%2520survey%252C%2520we%2520aim%2520to%250Atrace%2520and%2520summarize%2520the%2520advancements%2520of%2520MM-LLMs%2520from%2520image%2520understanding%2520to%250Along%2520video%2520understanding.%2520We%2520review%2520the%2520differences%2520among%2520various%2520visual%250Aunderstanding%2520tasks%2520and%2520highlight%2520the%2520challenges%2520in%2520long%2520video%2520understanding%252C%250Aincluding%2520more%2520fine-grained%2520spatiotemporal%2520details%252C%2520dynamic%2520events%252C%2520and%250Along-term%2520dependencies.%2520We%2520then%2520provide%2520a%2520detailed%2520summary%2520of%2520the%2520advancements%250Ain%2520MM-LLMs%2520in%2520terms%2520of%2520model%2520design%2520and%2520training%2520methodologies%2520for%250Aunderstanding%2520long%2520videos.%2520Finally%252C%2520we%2520compare%2520the%2520performance%2520of%2520existing%250AMM-LLMs%2520on%2520video%2520understanding%2520benchmarks%2520of%2520various%2520lengths%2520and%2520discuss%250Apotential%2520future%2520directions%2520for%2520MM-LLMs%2520in%2520long%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Seconds%20to%20Hours%3A%20Reviewing%20MultiModal%20Large%20Language%20Models%20on%0A%20%20Comprehensive%20Long%20Video%20Understanding&entry.906535625=Heqing%20Zou%20and%20Tianze%20Luo%20and%20Guiyang%20Xie%20and%20%20Victor%20and%20%20Zhang%20and%20Fengmao%20Lv%20and%20Guangcong%20Wang%20and%20Juanyang%20Chen%20and%20Zhuochen%20Wang%20and%20Hansheng%20Zhang%20and%20Huaijian%20Zhang&entry.1292438233=%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20visual%20encoders%20has%0Arecently%20shown%20promising%20performance%20in%20visual%20understanding%20tasks%2C%20leveraging%0Atheir%20inherent%20capability%20to%20comprehend%20and%20generate%20human-like%20text%20for%20visual%0Areasoning.%20Given%20the%20diverse%20nature%20of%20visual%20data%2C%20MultiModal%20Large%20Language%0AModels%20%28MM-LLMs%29%20exhibit%20variations%20in%20model%20designing%20and%20training%20for%0Aunderstanding%20images%2C%20short%20videos%2C%20and%20long%20videos.%20Our%20paper%20focuses%20on%20the%0Asubstantial%20differences%20and%20unique%20challenges%20posed%20by%20long%20video%20understanding%0Acompared%20to%20static%20image%20and%20short%20video%20understanding.%20Unlike%20static%20images%2C%0Ashort%20videos%20encompass%20sequential%20frames%20with%20both%20spatial%20and%20within-event%0Atemporal%20information%2C%20while%20long%20videos%20consist%20of%20multiple%20events%20with%0Abetween-event%20and%20long-term%20temporal%20information.%20In%20this%20survey%2C%20we%20aim%20to%0Atrace%20and%20summarize%20the%20advancements%20of%20MM-LLMs%20from%20image%20understanding%20to%0Along%20video%20understanding.%20We%20review%20the%20differences%20among%20various%20visual%0Aunderstanding%20tasks%20and%20highlight%20the%20challenges%20in%20long%20video%20understanding%2C%0Aincluding%20more%20fine-grained%20spatiotemporal%20details%2C%20dynamic%20events%2C%20and%0Along-term%20dependencies.%20We%20then%20provide%20a%20detailed%20summary%20of%20the%20advancements%0Ain%20MM-LLMs%20in%20terms%20of%20model%20design%20and%20training%20methodologies%20for%0Aunderstanding%20long%20videos.%20Finally%2C%20we%20compare%20the%20performance%20of%20existing%0AMM-LLMs%20on%20video%20understanding%20benchmarks%20of%20various%20lengths%20and%20discuss%0Apotential%20future%20directions%20for%20MM-LLMs%20in%20long%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18938v1&entry.124074799=Read"},
{"title": "DeRainGS: Gaussian Splatting for Enhanced Scene Reconstruction in Rainy\n  Environments", "author": "Shuhong Liu and Xiang Chen and Hongming Chen and Quanfeng Xu and Mingrui Li", "abstract": "  Reconstruction under adverse rainy conditions poses significant challenges\ndue to reduced visibility and the distortion of visual perception. These\nconditions can severely impair the quality of geometric maps, which is\nessential for applications ranging from autonomous planning to environmental\nmonitoring. In response to these challenges, this study introduces the novel\ntask of 3D Reconstruction in Rainy Environments (3DRRE), specifically designed\nto address the complexities of reconstructing 3D scenes under rainy conditions.\nTo benchmark this task, we construct the HydroViews dataset that comprises a\ndiverse collection of both synthesized and real-world scene images\ncharacterized by various intensities of rain streaks and raindrops.\nFurthermore, we propose DeRainGS, the first 3DGS method tailored for\nreconstruction in adverse rainy environments. Extensive experiments across a\nwide range of rain scenarios demonstrate that our method delivers\nstate-of-the-art performance, remarkably outperforming existing occlusion-free\nmethods.\n", "link": "http://arxiv.org/abs/2408.11540v3", "date": "2024-09-27", "relevancy": 3.0835, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6732}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5983}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeRainGS%3A%20Gaussian%20Splatting%20for%20Enhanced%20Scene%20Reconstruction%20in%20Rainy%0A%20%20Environments&body=Title%3A%20DeRainGS%3A%20Gaussian%20Splatting%20for%20Enhanced%20Scene%20Reconstruction%20in%20Rainy%0A%20%20Environments%0AAuthor%3A%20Shuhong%20Liu%20and%20Xiang%20Chen%20and%20Hongming%20Chen%20and%20Quanfeng%20Xu%20and%20Mingrui%20Li%0AAbstract%3A%20%20%20Reconstruction%20under%20adverse%20rainy%20conditions%20poses%20significant%20challenges%0Adue%20to%20reduced%20visibility%20and%20the%20distortion%20of%20visual%20perception.%20These%0Aconditions%20can%20severely%20impair%20the%20quality%20of%20geometric%20maps%2C%20which%20is%0Aessential%20for%20applications%20ranging%20from%20autonomous%20planning%20to%20environmental%0Amonitoring.%20In%20response%20to%20these%20challenges%2C%20this%20study%20introduces%20the%20novel%0Atask%20of%203D%20Reconstruction%20in%20Rainy%20Environments%20%283DRRE%29%2C%20specifically%20designed%0Ato%20address%20the%20complexities%20of%20reconstructing%203D%20scenes%20under%20rainy%20conditions.%0ATo%20benchmark%20this%20task%2C%20we%20construct%20the%20HydroViews%20dataset%20that%20comprises%20a%0Adiverse%20collection%20of%20both%20synthesized%20and%20real-world%20scene%20images%0Acharacterized%20by%20various%20intensities%20of%20rain%20streaks%20and%20raindrops.%0AFurthermore%2C%20we%20propose%20DeRainGS%2C%20the%20first%203DGS%20method%20tailored%20for%0Areconstruction%20in%20adverse%20rainy%20environments.%20Extensive%20experiments%20across%20a%0Awide%20range%20of%20rain%20scenarios%20demonstrate%20that%20our%20method%20delivers%0Astate-of-the-art%20performance%2C%20remarkably%20outperforming%20existing%20occlusion-free%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11540v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeRainGS%253A%2520Gaussian%2520Splatting%2520for%2520Enhanced%2520Scene%2520Reconstruction%2520in%2520Rainy%250A%2520%2520Environments%26entry.906535625%3DShuhong%2520Liu%2520and%2520Xiang%2520Chen%2520and%2520Hongming%2520Chen%2520and%2520Quanfeng%2520Xu%2520and%2520Mingrui%2520Li%26entry.1292438233%3D%2520%2520Reconstruction%2520under%2520adverse%2520rainy%2520conditions%2520poses%2520significant%2520challenges%250Adue%2520to%2520reduced%2520visibility%2520and%2520the%2520distortion%2520of%2520visual%2520perception.%2520These%250Aconditions%2520can%2520severely%2520impair%2520the%2520quality%2520of%2520geometric%2520maps%252C%2520which%2520is%250Aessential%2520for%2520applications%2520ranging%2520from%2520autonomous%2520planning%2520to%2520environmental%250Amonitoring.%2520In%2520response%2520to%2520these%2520challenges%252C%2520this%2520study%2520introduces%2520the%2520novel%250Atask%2520of%25203D%2520Reconstruction%2520in%2520Rainy%2520Environments%2520%25283DRRE%2529%252C%2520specifically%2520designed%250Ato%2520address%2520the%2520complexities%2520of%2520reconstructing%25203D%2520scenes%2520under%2520rainy%2520conditions.%250ATo%2520benchmark%2520this%2520task%252C%2520we%2520construct%2520the%2520HydroViews%2520dataset%2520that%2520comprises%2520a%250Adiverse%2520collection%2520of%2520both%2520synthesized%2520and%2520real-world%2520scene%2520images%250Acharacterized%2520by%2520various%2520intensities%2520of%2520rain%2520streaks%2520and%2520raindrops.%250AFurthermore%252C%2520we%2520propose%2520DeRainGS%252C%2520the%2520first%25203DGS%2520method%2520tailored%2520for%250Areconstruction%2520in%2520adverse%2520rainy%2520environments.%2520Extensive%2520experiments%2520across%2520a%250Awide%2520range%2520of%2520rain%2520scenarios%2520demonstrate%2520that%2520our%2520method%2520delivers%250Astate-of-the-art%2520performance%252C%2520remarkably%2520outperforming%2520existing%2520occlusion-free%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11540v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeRainGS%3A%20Gaussian%20Splatting%20for%20Enhanced%20Scene%20Reconstruction%20in%20Rainy%0A%20%20Environments&entry.906535625=Shuhong%20Liu%20and%20Xiang%20Chen%20and%20Hongming%20Chen%20and%20Quanfeng%20Xu%20and%20Mingrui%20Li&entry.1292438233=%20%20Reconstruction%20under%20adverse%20rainy%20conditions%20poses%20significant%20challenges%0Adue%20to%20reduced%20visibility%20and%20the%20distortion%20of%20visual%20perception.%20These%0Aconditions%20can%20severely%20impair%20the%20quality%20of%20geometric%20maps%2C%20which%20is%0Aessential%20for%20applications%20ranging%20from%20autonomous%20planning%20to%20environmental%0Amonitoring.%20In%20response%20to%20these%20challenges%2C%20this%20study%20introduces%20the%20novel%0Atask%20of%203D%20Reconstruction%20in%20Rainy%20Environments%20%283DRRE%29%2C%20specifically%20designed%0Ato%20address%20the%20complexities%20of%20reconstructing%203D%20scenes%20under%20rainy%20conditions.%0ATo%20benchmark%20this%20task%2C%20we%20construct%20the%20HydroViews%20dataset%20that%20comprises%20a%0Adiverse%20collection%20of%20both%20synthesized%20and%20real-world%20scene%20images%0Acharacterized%20by%20various%20intensities%20of%20rain%20streaks%20and%20raindrops.%0AFurthermore%2C%20we%20propose%20DeRainGS%2C%20the%20first%203DGS%20method%20tailored%20for%0Areconstruction%20in%20adverse%20rainy%20environments.%20Extensive%20experiments%20across%20a%0Awide%20range%20of%20rain%20scenarios%20demonstrate%20that%20our%20method%20delivers%0Astate-of-the-art%20performance%2C%20remarkably%20outperforming%20existing%20occlusion-free%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11540v3&entry.124074799=Read"},
{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "author": "Shaowei Liu and Zhongzheng Ren and Saurabh Gupta and Shenlong Wang", "abstract": "  We present PhysGen, a novel image-to-video generation method that converts a\nsingle image and an input condition (e.g., force and torque applied to an\nobject in the image) to produce a realistic, physically plausible, and\ntemporally consistent video. Our key insight is to integrate model-based\nphysical simulation with a data-driven video generation process, enabling\nplausible image-space dynamics. At the heart of our system are three core\ncomponents: (i) an image understanding module that effectively captures the\ngeometry, materials, and physical parameters of the image; (ii) an image-space\ndynamics simulation model that utilizes rigid-body physics and inferred\nparameters to simulate realistic behaviors; and (iii) an image-based rendering\nand refinement module that leverages generative video diffusion to produce\nrealistic video footage featuring the simulated motion. The resulting videos\nare realistic in both physics and appearance and are even precisely\ncontrollable, showcasing superior results over existing data-driven\nimage-to-video generation works through quantitative comparison and\ncomprehensive user study. PhysGen's resulting videos can be used for various\ndownstream applications, such as turning an image into a realistic animation or\nallowing users to interact with the image and create various dynamics. Project\npage: https://stevenlsw.github.io/physgen/\n", "link": "http://arxiv.org/abs/2409.18964v1", "date": "2024-09-27", "relevancy": 3.0074, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6094}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6051}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysGen%3A%20Rigid-Body%20Physics-Grounded%20Image-to-Video%20Generation&body=Title%3A%20PhysGen%3A%20Rigid-Body%20Physics-Grounded%20Image-to-Video%20Generation%0AAuthor%3A%20Shaowei%20Liu%20and%20Zhongzheng%20Ren%20and%20Saurabh%20Gupta%20and%20Shenlong%20Wang%0AAbstract%3A%20%20%20We%20present%20PhysGen%2C%20a%20novel%20image-to-video%20generation%20method%20that%20converts%20a%0Asingle%20image%20and%20an%20input%20condition%20%28e.g.%2C%20force%20and%20torque%20applied%20to%20an%0Aobject%20in%20the%20image%29%20to%20produce%20a%20realistic%2C%20physically%20plausible%2C%20and%0Atemporally%20consistent%20video.%20Our%20key%20insight%20is%20to%20integrate%20model-based%0Aphysical%20simulation%20with%20a%20data-driven%20video%20generation%20process%2C%20enabling%0Aplausible%20image-space%20dynamics.%20At%20the%20heart%20of%20our%20system%20are%20three%20core%0Acomponents%3A%20%28i%29%20an%20image%20understanding%20module%20that%20effectively%20captures%20the%0Ageometry%2C%20materials%2C%20and%20physical%20parameters%20of%20the%20image%3B%20%28ii%29%20an%20image-space%0Adynamics%20simulation%20model%20that%20utilizes%20rigid-body%20physics%20and%20inferred%0Aparameters%20to%20simulate%20realistic%20behaviors%3B%20and%20%28iii%29%20an%20image-based%20rendering%0Aand%20refinement%20module%20that%20leverages%20generative%20video%20diffusion%20to%20produce%0Arealistic%20video%20footage%20featuring%20the%20simulated%20motion.%20The%20resulting%20videos%0Aare%20realistic%20in%20both%20physics%20and%20appearance%20and%20are%20even%20precisely%0Acontrollable%2C%20showcasing%20superior%20results%20over%20existing%20data-driven%0Aimage-to-video%20generation%20works%20through%20quantitative%20comparison%20and%0Acomprehensive%20user%20study.%20PhysGen%27s%20resulting%20videos%20can%20be%20used%20for%20various%0Adownstream%20applications%2C%20such%20as%20turning%20an%20image%20into%20a%20realistic%20animation%20or%0Aallowing%20users%20to%20interact%20with%20the%20image%20and%20create%20various%20dynamics.%20Project%0Apage%3A%20https%3A//stevenlsw.github.io/physgen/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysGen%253A%2520Rigid-Body%2520Physics-Grounded%2520Image-to-Video%2520Generation%26entry.906535625%3DShaowei%2520Liu%2520and%2520Zhongzheng%2520Ren%2520and%2520Saurabh%2520Gupta%2520and%2520Shenlong%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520PhysGen%252C%2520a%2520novel%2520image-to-video%2520generation%2520method%2520that%2520converts%2520a%250Asingle%2520image%2520and%2520an%2520input%2520condition%2520%2528e.g.%252C%2520force%2520and%2520torque%2520applied%2520to%2520an%250Aobject%2520in%2520the%2520image%2529%2520to%2520produce%2520a%2520realistic%252C%2520physically%2520plausible%252C%2520and%250Atemporally%2520consistent%2520video.%2520Our%2520key%2520insight%2520is%2520to%2520integrate%2520model-based%250Aphysical%2520simulation%2520with%2520a%2520data-driven%2520video%2520generation%2520process%252C%2520enabling%250Aplausible%2520image-space%2520dynamics.%2520At%2520the%2520heart%2520of%2520our%2520system%2520are%2520three%2520core%250Acomponents%253A%2520%2528i%2529%2520an%2520image%2520understanding%2520module%2520that%2520effectively%2520captures%2520the%250Ageometry%252C%2520materials%252C%2520and%2520physical%2520parameters%2520of%2520the%2520image%253B%2520%2528ii%2529%2520an%2520image-space%250Adynamics%2520simulation%2520model%2520that%2520utilizes%2520rigid-body%2520physics%2520and%2520inferred%250Aparameters%2520to%2520simulate%2520realistic%2520behaviors%253B%2520and%2520%2528iii%2529%2520an%2520image-based%2520rendering%250Aand%2520refinement%2520module%2520that%2520leverages%2520generative%2520video%2520diffusion%2520to%2520produce%250Arealistic%2520video%2520footage%2520featuring%2520the%2520simulated%2520motion.%2520The%2520resulting%2520videos%250Aare%2520realistic%2520in%2520both%2520physics%2520and%2520appearance%2520and%2520are%2520even%2520precisely%250Acontrollable%252C%2520showcasing%2520superior%2520results%2520over%2520existing%2520data-driven%250Aimage-to-video%2520generation%2520works%2520through%2520quantitative%2520comparison%2520and%250Acomprehensive%2520user%2520study.%2520PhysGen%2527s%2520resulting%2520videos%2520can%2520be%2520used%2520for%2520various%250Adownstream%2520applications%252C%2520such%2520as%2520turning%2520an%2520image%2520into%2520a%2520realistic%2520animation%2520or%250Aallowing%2520users%2520to%2520interact%2520with%2520the%2520image%2520and%2520create%2520various%2520dynamics.%2520Project%250Apage%253A%2520https%253A//stevenlsw.github.io/physgen/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysGen%3A%20Rigid-Body%20Physics-Grounded%20Image-to-Video%20Generation&entry.906535625=Shaowei%20Liu%20and%20Zhongzheng%20Ren%20and%20Saurabh%20Gupta%20and%20Shenlong%20Wang&entry.1292438233=%20%20We%20present%20PhysGen%2C%20a%20novel%20image-to-video%20generation%20method%20that%20converts%20a%0Asingle%20image%20and%20an%20input%20condition%20%28e.g.%2C%20force%20and%20torque%20applied%20to%20an%0Aobject%20in%20the%20image%29%20to%20produce%20a%20realistic%2C%20physically%20plausible%2C%20and%0Atemporally%20consistent%20video.%20Our%20key%20insight%20is%20to%20integrate%20model-based%0Aphysical%20simulation%20with%20a%20data-driven%20video%20generation%20process%2C%20enabling%0Aplausible%20image-space%20dynamics.%20At%20the%20heart%20of%20our%20system%20are%20three%20core%0Acomponents%3A%20%28i%29%20an%20image%20understanding%20module%20that%20effectively%20captures%20the%0Ageometry%2C%20materials%2C%20and%20physical%20parameters%20of%20the%20image%3B%20%28ii%29%20an%20image-space%0Adynamics%20simulation%20model%20that%20utilizes%20rigid-body%20physics%20and%20inferred%0Aparameters%20to%20simulate%20realistic%20behaviors%3B%20and%20%28iii%29%20an%20image-based%20rendering%0Aand%20refinement%20module%20that%20leverages%20generative%20video%20diffusion%20to%20produce%0Arealistic%20video%20footage%20featuring%20the%20simulated%20motion.%20The%20resulting%20videos%0Aare%20realistic%20in%20both%20physics%20and%20appearance%20and%20are%20even%20precisely%0Acontrollable%2C%20showcasing%20superior%20results%20over%20existing%20data-driven%0Aimage-to-video%20generation%20works%20through%20quantitative%20comparison%20and%0Acomprehensive%20user%20study.%20PhysGen%27s%20resulting%20videos%20can%20be%20used%20for%20various%0Adownstream%20applications%2C%20such%20as%20turning%20an%20image%20into%20a%20realistic%20animation%20or%0Aallowing%20users%20to%20interact%20with%20the%20image%20and%20create%20various%20dynamics.%20Project%0Apage%3A%20https%3A//stevenlsw.github.io/physgen/%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18964v1&entry.124074799=Read"},
{"title": "UniEmoX: Cross-modal Semantic-Guided Large-Scale Pretraining for\n  Universal Scene Emotion Perception", "author": "Chuang Chen and Xiao Sun and Zhi Liu", "abstract": "  Visual emotion analysis holds significant research value in both computer\nvision and psychology. However, existing methods for visual emotion analysis\nsuffer from limited generalizability due to the ambiguity of emotion perception\nand the diversity of data scenarios. To tackle this issue, we introduce\nUniEmoX, a cross-modal semantic-guided large-scale pretraining framework.\nInspired by psychological research emphasizing the inseparability of the\nemotional exploration process from the interaction between individuals and\ntheir environment, UniEmoX integrates scene-centric and person-centric\nlow-level image spatial structural information, aiming to derive more nuanced\nand discriminative emotional representations. By exploiting the similarity\nbetween paired and unpaired image-text samples, UniEmoX distills rich semantic\nknowledge from the CLIP model to enhance emotional embedding representations\nmore effectively. To the best of our knowledge, this is the first large-scale\npretraining framework that integrates psychological theories with contemporary\ncontrastive learning and masked image modeling techniques for emotion analysis\nacross diverse scenarios. Additionally, we develop a visual emotional dataset\ntitled Emo8. Emo8 samples cover a range of domains, including cartoon, natural,\nrealistic, science fiction and advertising cover styles, covering nearly all\ncommon emotional scenes. Comprehensive experiments conducted on six benchmark\ndatasets across two downstream tasks validate the effectiveness of UniEmoX. The\nsource code is available at https://github.com/chincharles/u-emo.\n", "link": "http://arxiv.org/abs/2409.18877v1", "date": "2024-09-27", "relevancy": 2.9814, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniEmoX%3A%20Cross-modal%20Semantic-Guided%20Large-Scale%20Pretraining%20for%0A%20%20Universal%20Scene%20Emotion%20Perception&body=Title%3A%20UniEmoX%3A%20Cross-modal%20Semantic-Guided%20Large-Scale%20Pretraining%20for%0A%20%20Universal%20Scene%20Emotion%20Perception%0AAuthor%3A%20Chuang%20Chen%20and%20Xiao%20Sun%20and%20Zhi%20Liu%0AAbstract%3A%20%20%20Visual%20emotion%20analysis%20holds%20significant%20research%20value%20in%20both%20computer%0Avision%20and%20psychology.%20However%2C%20existing%20methods%20for%20visual%20emotion%20analysis%0Asuffer%20from%20limited%20generalizability%20due%20to%20the%20ambiguity%20of%20emotion%20perception%0Aand%20the%20diversity%20of%20data%20scenarios.%20To%20tackle%20this%20issue%2C%20we%20introduce%0AUniEmoX%2C%20a%20cross-modal%20semantic-guided%20large-scale%20pretraining%20framework.%0AInspired%20by%20psychological%20research%20emphasizing%20the%20inseparability%20of%20the%0Aemotional%20exploration%20process%20from%20the%20interaction%20between%20individuals%20and%0Atheir%20environment%2C%20UniEmoX%20integrates%20scene-centric%20and%20person-centric%0Alow-level%20image%20spatial%20structural%20information%2C%20aiming%20to%20derive%20more%20nuanced%0Aand%20discriminative%20emotional%20representations.%20By%20exploiting%20the%20similarity%0Abetween%20paired%20and%20unpaired%20image-text%20samples%2C%20UniEmoX%20distills%20rich%20semantic%0Aknowledge%20from%20the%20CLIP%20model%20to%20enhance%20emotional%20embedding%20representations%0Amore%20effectively.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20large-scale%0Apretraining%20framework%20that%20integrates%20psychological%20theories%20with%20contemporary%0Acontrastive%20learning%20and%20masked%20image%20modeling%20techniques%20for%20emotion%20analysis%0Aacross%20diverse%20scenarios.%20Additionally%2C%20we%20develop%20a%20visual%20emotional%20dataset%0Atitled%20Emo8.%20Emo8%20samples%20cover%20a%20range%20of%20domains%2C%20including%20cartoon%2C%20natural%2C%0Arealistic%2C%20science%20fiction%20and%20advertising%20cover%20styles%2C%20covering%20nearly%20all%0Acommon%20emotional%20scenes.%20Comprehensive%20experiments%20conducted%20on%20six%20benchmark%0Adatasets%20across%20two%20downstream%20tasks%20validate%20the%20effectiveness%20of%20UniEmoX.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/chincharles/u-emo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniEmoX%253A%2520Cross-modal%2520Semantic-Guided%2520Large-Scale%2520Pretraining%2520for%250A%2520%2520Universal%2520Scene%2520Emotion%2520Perception%26entry.906535625%3DChuang%2520Chen%2520and%2520Xiao%2520Sun%2520and%2520Zhi%2520Liu%26entry.1292438233%3D%2520%2520Visual%2520emotion%2520analysis%2520holds%2520significant%2520research%2520value%2520in%2520both%2520computer%250Avision%2520and%2520psychology.%2520However%252C%2520existing%2520methods%2520for%2520visual%2520emotion%2520analysis%250Asuffer%2520from%2520limited%2520generalizability%2520due%2520to%2520the%2520ambiguity%2520of%2520emotion%2520perception%250Aand%2520the%2520diversity%2520of%2520data%2520scenarios.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520introduce%250AUniEmoX%252C%2520a%2520cross-modal%2520semantic-guided%2520large-scale%2520pretraining%2520framework.%250AInspired%2520by%2520psychological%2520research%2520emphasizing%2520the%2520inseparability%2520of%2520the%250Aemotional%2520exploration%2520process%2520from%2520the%2520interaction%2520between%2520individuals%2520and%250Atheir%2520environment%252C%2520UniEmoX%2520integrates%2520scene-centric%2520and%2520person-centric%250Alow-level%2520image%2520spatial%2520structural%2520information%252C%2520aiming%2520to%2520derive%2520more%2520nuanced%250Aand%2520discriminative%2520emotional%2520representations.%2520By%2520exploiting%2520the%2520similarity%250Abetween%2520paired%2520and%2520unpaired%2520image-text%2520samples%252C%2520UniEmoX%2520distills%2520rich%2520semantic%250Aknowledge%2520from%2520the%2520CLIP%2520model%2520to%2520enhance%2520emotional%2520embedding%2520representations%250Amore%2520effectively.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520large-scale%250Apretraining%2520framework%2520that%2520integrates%2520psychological%2520theories%2520with%2520contemporary%250Acontrastive%2520learning%2520and%2520masked%2520image%2520modeling%2520techniques%2520for%2520emotion%2520analysis%250Aacross%2520diverse%2520scenarios.%2520Additionally%252C%2520we%2520develop%2520a%2520visual%2520emotional%2520dataset%250Atitled%2520Emo8.%2520Emo8%2520samples%2520cover%2520a%2520range%2520of%2520domains%252C%2520including%2520cartoon%252C%2520natural%252C%250Arealistic%252C%2520science%2520fiction%2520and%2520advertising%2520cover%2520styles%252C%2520covering%2520nearly%2520all%250Acommon%2520emotional%2520scenes.%2520Comprehensive%2520experiments%2520conducted%2520on%2520six%2520benchmark%250Adatasets%2520across%2520two%2520downstream%2520tasks%2520validate%2520the%2520effectiveness%2520of%2520UniEmoX.%2520The%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/chincharles/u-emo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniEmoX%3A%20Cross-modal%20Semantic-Guided%20Large-Scale%20Pretraining%20for%0A%20%20Universal%20Scene%20Emotion%20Perception&entry.906535625=Chuang%20Chen%20and%20Xiao%20Sun%20and%20Zhi%20Liu&entry.1292438233=%20%20Visual%20emotion%20analysis%20holds%20significant%20research%20value%20in%20both%20computer%0Avision%20and%20psychology.%20However%2C%20existing%20methods%20for%20visual%20emotion%20analysis%0Asuffer%20from%20limited%20generalizability%20due%20to%20the%20ambiguity%20of%20emotion%20perception%0Aand%20the%20diversity%20of%20data%20scenarios.%20To%20tackle%20this%20issue%2C%20we%20introduce%0AUniEmoX%2C%20a%20cross-modal%20semantic-guided%20large-scale%20pretraining%20framework.%0AInspired%20by%20psychological%20research%20emphasizing%20the%20inseparability%20of%20the%0Aemotional%20exploration%20process%20from%20the%20interaction%20between%20individuals%20and%0Atheir%20environment%2C%20UniEmoX%20integrates%20scene-centric%20and%20person-centric%0Alow-level%20image%20spatial%20structural%20information%2C%20aiming%20to%20derive%20more%20nuanced%0Aand%20discriminative%20emotional%20representations.%20By%20exploiting%20the%20similarity%0Abetween%20paired%20and%20unpaired%20image-text%20samples%2C%20UniEmoX%20distills%20rich%20semantic%0Aknowledge%20from%20the%20CLIP%20model%20to%20enhance%20emotional%20embedding%20representations%0Amore%20effectively.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20large-scale%0Apretraining%20framework%20that%20integrates%20psychological%20theories%20with%20contemporary%0Acontrastive%20learning%20and%20masked%20image%20modeling%20techniques%20for%20emotion%20analysis%0Aacross%20diverse%20scenarios.%20Additionally%2C%20we%20develop%20a%20visual%20emotional%20dataset%0Atitled%20Emo8.%20Emo8%20samples%20cover%20a%20range%20of%20domains%2C%20including%20cartoon%2C%20natural%2C%0Arealistic%2C%20science%20fiction%20and%20advertising%20cover%20styles%2C%20covering%20nearly%20all%0Acommon%20emotional%20scenes.%20Comprehensive%20experiments%20conducted%20on%20six%20benchmark%0Adatasets%20across%20two%20downstream%20tasks%20validate%20the%20effectiveness%20of%20UniEmoX.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/chincharles/u-emo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18877v1&entry.124074799=Read"},
{"title": "When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive\n  Evaluation and Adaptation", "author": "Yuli Zhou and Guolei Sun and Yawei Li and Luca Benini and Ender Konukoglu", "abstract": "  This study investigates the application and performance of the Segment\nAnything Model 2 (SAM2) in the challenging task of video camouflaged object\nsegmentation (VCOS). VCOS involves detecting objects that blend seamlessly in\nthe surroundings for videos, due to similar colors and textures, poor light\nconditions, etc. Compared to the objects in normal scenes, camouflaged objects\nare much more difficult to detect. SAM2, a video foundation model, has shown\npotential in various tasks. But its effectiveness in dynamic camouflaged\nscenarios remains under-explored. This study presents a comprehensive study on\nSAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged\nvideo datasets using different models and prompts (click, box, and mask).\nSecond, we explore the integration of SAM2 with existing multimodal large\nlanguage models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by\nfine-tuning it on the video camouflaged dataset. Our comprehensive experiments\ndemonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged\nobjects in videos. We also show that this ability could be further improved by\nspecifically adjusting SAM2's parameters for VCOS. The code will be available\nat https://github.com/zhoustan/SAM2-VCOS\n", "link": "http://arxiv.org/abs/2409.18653v1", "date": "2024-09-27", "relevancy": 2.9579, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20SAM2%20Meets%20Video%20Camouflaged%20Object%20Segmentation%3A%20A%20Comprehensive%0A%20%20Evaluation%20and%20Adaptation&body=Title%3A%20When%20SAM2%20Meets%20Video%20Camouflaged%20Object%20Segmentation%3A%20A%20Comprehensive%0A%20%20Evaluation%20and%20Adaptation%0AAuthor%3A%20Yuli%20Zhou%20and%20Guolei%20Sun%20and%20Yawei%20Li%20and%20Luca%20Benini%20and%20Ender%20Konukoglu%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20application%20and%20performance%20of%20the%20Segment%0AAnything%20Model%202%20%28SAM2%29%20in%20the%20challenging%20task%20of%20video%20camouflaged%20object%0Asegmentation%20%28VCOS%29.%20VCOS%20involves%20detecting%20objects%20that%20blend%20seamlessly%20in%0Athe%20surroundings%20for%20videos%2C%20due%20to%20similar%20colors%20and%20textures%2C%20poor%20light%0Aconditions%2C%20etc.%20Compared%20to%20the%20objects%20in%20normal%20scenes%2C%20camouflaged%20objects%0Aare%20much%20more%20difficult%20to%20detect.%20SAM2%2C%20a%20video%20foundation%20model%2C%20has%20shown%0Apotential%20in%20various%20tasks.%20But%20its%20effectiveness%20in%20dynamic%20camouflaged%0Ascenarios%20remains%20under-explored.%20This%20study%20presents%20a%20comprehensive%20study%20on%0ASAM2%27s%20ability%20in%20VCOS.%20First%2C%20we%20assess%20SAM2%27s%20performance%20on%20camouflaged%0Avideo%20datasets%20using%20different%20models%20and%20prompts%20%28click%2C%20box%2C%20and%20mask%29.%0ASecond%2C%20we%20explore%20the%20integration%20of%20SAM2%20with%20existing%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20and%20VCOS%20methods.%20Third%2C%20we%20specifically%20adapt%20SAM2%20by%0Afine-tuning%20it%20on%20the%20video%20camouflaged%20dataset.%20Our%20comprehensive%20experiments%0Ademonstrate%20that%20SAM2%20has%20excellent%20zero-shot%20ability%20of%20detecting%20camouflaged%0Aobjects%20in%20videos.%20We%20also%20show%20that%20this%20ability%20could%20be%20further%20improved%20by%0Aspecifically%20adjusting%20SAM2%27s%20parameters%20for%20VCOS.%20The%20code%20will%20be%20available%0Aat%20https%3A//github.com/zhoustan/SAM2-VCOS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520SAM2%2520Meets%2520Video%2520Camouflaged%2520Object%2520Segmentation%253A%2520A%2520Comprehensive%250A%2520%2520Evaluation%2520and%2520Adaptation%26entry.906535625%3DYuli%2520Zhou%2520and%2520Guolei%2520Sun%2520and%2520Yawei%2520Li%2520and%2520Luca%2520Benini%2520and%2520Ender%2520Konukoglu%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520application%2520and%2520performance%2520of%2520the%2520Segment%250AAnything%2520Model%25202%2520%2528SAM2%2529%2520in%2520the%2520challenging%2520task%2520of%2520video%2520camouflaged%2520object%250Asegmentation%2520%2528VCOS%2529.%2520VCOS%2520involves%2520detecting%2520objects%2520that%2520blend%2520seamlessly%2520in%250Athe%2520surroundings%2520for%2520videos%252C%2520due%2520to%2520similar%2520colors%2520and%2520textures%252C%2520poor%2520light%250Aconditions%252C%2520etc.%2520Compared%2520to%2520the%2520objects%2520in%2520normal%2520scenes%252C%2520camouflaged%2520objects%250Aare%2520much%2520more%2520difficult%2520to%2520detect.%2520SAM2%252C%2520a%2520video%2520foundation%2520model%252C%2520has%2520shown%250Apotential%2520in%2520various%2520tasks.%2520But%2520its%2520effectiveness%2520in%2520dynamic%2520camouflaged%250Ascenarios%2520remains%2520under-explored.%2520This%2520study%2520presents%2520a%2520comprehensive%2520study%2520on%250ASAM2%2527s%2520ability%2520in%2520VCOS.%2520First%252C%2520we%2520assess%2520SAM2%2527s%2520performance%2520on%2520camouflaged%250Avideo%2520datasets%2520using%2520different%2520models%2520and%2520prompts%2520%2528click%252C%2520box%252C%2520and%2520mask%2529.%250ASecond%252C%2520we%2520explore%2520the%2520integration%2520of%2520SAM2%2520with%2520existing%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%2520and%2520VCOS%2520methods.%2520Third%252C%2520we%2520specifically%2520adapt%2520SAM2%2520by%250Afine-tuning%2520it%2520on%2520the%2520video%2520camouflaged%2520dataset.%2520Our%2520comprehensive%2520experiments%250Ademonstrate%2520that%2520SAM2%2520has%2520excellent%2520zero-shot%2520ability%2520of%2520detecting%2520camouflaged%250Aobjects%2520in%2520videos.%2520We%2520also%2520show%2520that%2520this%2520ability%2520could%2520be%2520further%2520improved%2520by%250Aspecifically%2520adjusting%2520SAM2%2527s%2520parameters%2520for%2520VCOS.%2520The%2520code%2520will%2520be%2520available%250Aat%2520https%253A//github.com/zhoustan/SAM2-VCOS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20SAM2%20Meets%20Video%20Camouflaged%20Object%20Segmentation%3A%20A%20Comprehensive%0A%20%20Evaluation%20and%20Adaptation&entry.906535625=Yuli%20Zhou%20and%20Guolei%20Sun%20and%20Yawei%20Li%20and%20Luca%20Benini%20and%20Ender%20Konukoglu&entry.1292438233=%20%20This%20study%20investigates%20the%20application%20and%20performance%20of%20the%20Segment%0AAnything%20Model%202%20%28SAM2%29%20in%20the%20challenging%20task%20of%20video%20camouflaged%20object%0Asegmentation%20%28VCOS%29.%20VCOS%20involves%20detecting%20objects%20that%20blend%20seamlessly%20in%0Athe%20surroundings%20for%20videos%2C%20due%20to%20similar%20colors%20and%20textures%2C%20poor%20light%0Aconditions%2C%20etc.%20Compared%20to%20the%20objects%20in%20normal%20scenes%2C%20camouflaged%20objects%0Aare%20much%20more%20difficult%20to%20detect.%20SAM2%2C%20a%20video%20foundation%20model%2C%20has%20shown%0Apotential%20in%20various%20tasks.%20But%20its%20effectiveness%20in%20dynamic%20camouflaged%0Ascenarios%20remains%20under-explored.%20This%20study%20presents%20a%20comprehensive%20study%20on%0ASAM2%27s%20ability%20in%20VCOS.%20First%2C%20we%20assess%20SAM2%27s%20performance%20on%20camouflaged%0Avideo%20datasets%20using%20different%20models%20and%20prompts%20%28click%2C%20box%2C%20and%20mask%29.%0ASecond%2C%20we%20explore%20the%20integration%20of%20SAM2%20with%20existing%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20and%20VCOS%20methods.%20Third%2C%20we%20specifically%20adapt%20SAM2%20by%0Afine-tuning%20it%20on%20the%20video%20camouflaged%20dataset.%20Our%20comprehensive%20experiments%0Ademonstrate%20that%20SAM2%20has%20excellent%20zero-shot%20ability%20of%20detecting%20camouflaged%0Aobjects%20in%20videos.%20We%20also%20show%20that%20this%20ability%20could%20be%20further%20improved%20by%0Aspecifically%20adjusting%20SAM2%27s%20parameters%20for%20VCOS.%20The%20code%20will%20be%20available%0Aat%20https%3A//github.com/zhoustan/SAM2-VCOS%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18653v1&entry.124074799=Read"},
{"title": "3DPX: Single Panoramic X-ray Analysis Guided by 3D Oral Structure\n  Reconstruction", "author": "Xiaoshuang Li and Zimo Huang and Mingyuan Meng and Eduardo Delamare and Dagan Feng and Lei Bi and Bin Sheng and Lingyong Jiang and Bo Li and Jinman Kim", "abstract": "  Panoramic X-ray (PX) is a prevalent modality in dentistry practice owing to\nits wide availability and low cost. However, as a 2D projection of a 3D\nstructure, PX suffers from anatomical information loss and PX diagnosis is\nlimited compared to that with 3D imaging modalities. 2D-to-3D reconstruction\nmethods have been explored for the ability to synthesize the absent 3D\nanatomical information from 2D PX for use in PX image analysis. However, there\nare challenges in leveraging such 3D synthesized reconstructions. First,\ninferring 3D depth from 2D images remains a challenging task with limited\naccuracy. The second challenge is the joint analysis of 2D PX with its 3D\nsynthesized counterpart, with the aim to maximize the 2D-3D synergy while\nminimizing the errors arising from the synthesized image. In this study, we\npropose a new method termed 3DPX - PX image analysis guided by 2D-to-3D\nreconstruction, to overcome these challenges. 3DPX consists of (i) a novel\nprogressive reconstruction network to improve 2D-to-3D reconstruction and, (ii)\na contrastive-guided bidirectional multimodality alignment module for 3D-guided\n2D PX classification and segmentation tasks. The reconstruction network\nprogressively reconstructs 3D images with knowledge imposed on the intermediate\nreconstructions at multiple pyramid levels and incorporates Multilayer\nPerceptrons to improve semantic understanding. The downstream networks leverage\nthe reconstructed images as 3D anatomical guidance to the PX analysis through\nfeature alignment, which increases the 2D-3D synergy with bidirectional feature\nprojection and decease the impact of potential errors with contrastive\nguidance. Extensive experiments on two oral datasets involving 464 studies\ndemonstrate that 3DPX outperforms the state-of-the-art methods in various tasks\nincluding 2D-to-3D reconstruction, PX classification and lesion segmentation.\n", "link": "http://arxiv.org/abs/2409.18701v1", "date": "2024-09-27", "relevancy": 2.9187, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5858}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DPX%3A%20Single%20Panoramic%20X-ray%20Analysis%20Guided%20by%203D%20Oral%20Structure%0A%20%20Reconstruction&body=Title%3A%203DPX%3A%20Single%20Panoramic%20X-ray%20Analysis%20Guided%20by%203D%20Oral%20Structure%0A%20%20Reconstruction%0AAuthor%3A%20Xiaoshuang%20Li%20and%20Zimo%20Huang%20and%20Mingyuan%20Meng%20and%20Eduardo%20Delamare%20and%20Dagan%20Feng%20and%20Lei%20Bi%20and%20Bin%20Sheng%20and%20Lingyong%20Jiang%20and%20Bo%20Li%20and%20Jinman%20Kim%0AAbstract%3A%20%20%20Panoramic%20X-ray%20%28PX%29%20is%20a%20prevalent%20modality%20in%20dentistry%20practice%20owing%20to%0Aits%20wide%20availability%20and%20low%20cost.%20However%2C%20as%20a%202D%20projection%20of%20a%203D%0Astructure%2C%20PX%20suffers%20from%20anatomical%20information%20loss%20and%20PX%20diagnosis%20is%0Alimited%20compared%20to%20that%20with%203D%20imaging%20modalities.%202D-to-3D%20reconstruction%0Amethods%20have%20been%20explored%20for%20the%20ability%20to%20synthesize%20the%20absent%203D%0Aanatomical%20information%20from%202D%20PX%20for%20use%20in%20PX%20image%20analysis.%20However%2C%20there%0Aare%20challenges%20in%20leveraging%20such%203D%20synthesized%20reconstructions.%20First%2C%0Ainferring%203D%20depth%20from%202D%20images%20remains%20a%20challenging%20task%20with%20limited%0Aaccuracy.%20The%20second%20challenge%20is%20the%20joint%20analysis%20of%202D%20PX%20with%20its%203D%0Asynthesized%20counterpart%2C%20with%20the%20aim%20to%20maximize%20the%202D-3D%20synergy%20while%0Aminimizing%20the%20errors%20arising%20from%20the%20synthesized%20image.%20In%20this%20study%2C%20we%0Apropose%20a%20new%20method%20termed%203DPX%20-%20PX%20image%20analysis%20guided%20by%202D-to-3D%0Areconstruction%2C%20to%20overcome%20these%20challenges.%203DPX%20consists%20of%20%28i%29%20a%20novel%0Aprogressive%20reconstruction%20network%20to%20improve%202D-to-3D%20reconstruction%20and%2C%20%28ii%29%0Aa%20contrastive-guided%20bidirectional%20multimodality%20alignment%20module%20for%203D-guided%0A2D%20PX%20classification%20and%20segmentation%20tasks.%20The%20reconstruction%20network%0Aprogressively%20reconstructs%203D%20images%20with%20knowledge%20imposed%20on%20the%20intermediate%0Areconstructions%20at%20multiple%20pyramid%20levels%20and%20incorporates%20Multilayer%0APerceptrons%20to%20improve%20semantic%20understanding.%20The%20downstream%20networks%20leverage%0Athe%20reconstructed%20images%20as%203D%20anatomical%20guidance%20to%20the%20PX%20analysis%20through%0Afeature%20alignment%2C%20which%20increases%20the%202D-3D%20synergy%20with%20bidirectional%20feature%0Aprojection%20and%20decease%20the%20impact%20of%20potential%20errors%20with%20contrastive%0Aguidance.%20Extensive%20experiments%20on%20two%20oral%20datasets%20involving%20464%20studies%0Ademonstrate%20that%203DPX%20outperforms%20the%20state-of-the-art%20methods%20in%20various%20tasks%0Aincluding%202D-to-3D%20reconstruction%2C%20PX%20classification%20and%20lesion%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DPX%253A%2520Single%2520Panoramic%2520X-ray%2520Analysis%2520Guided%2520by%25203D%2520Oral%2520Structure%250A%2520%2520Reconstruction%26entry.906535625%3DXiaoshuang%2520Li%2520and%2520Zimo%2520Huang%2520and%2520Mingyuan%2520Meng%2520and%2520Eduardo%2520Delamare%2520and%2520Dagan%2520Feng%2520and%2520Lei%2520Bi%2520and%2520Bin%2520Sheng%2520and%2520Lingyong%2520Jiang%2520and%2520Bo%2520Li%2520and%2520Jinman%2520Kim%26entry.1292438233%3D%2520%2520Panoramic%2520X-ray%2520%2528PX%2529%2520is%2520a%2520prevalent%2520modality%2520in%2520dentistry%2520practice%2520owing%2520to%250Aits%2520wide%2520availability%2520and%2520low%2520cost.%2520However%252C%2520as%2520a%25202D%2520projection%2520of%2520a%25203D%250Astructure%252C%2520PX%2520suffers%2520from%2520anatomical%2520information%2520loss%2520and%2520PX%2520diagnosis%2520is%250Alimited%2520compared%2520to%2520that%2520with%25203D%2520imaging%2520modalities.%25202D-to-3D%2520reconstruction%250Amethods%2520have%2520been%2520explored%2520for%2520the%2520ability%2520to%2520synthesize%2520the%2520absent%25203D%250Aanatomical%2520information%2520from%25202D%2520PX%2520for%2520use%2520in%2520PX%2520image%2520analysis.%2520However%252C%2520there%250Aare%2520challenges%2520in%2520leveraging%2520such%25203D%2520synthesized%2520reconstructions.%2520First%252C%250Ainferring%25203D%2520depth%2520from%25202D%2520images%2520remains%2520a%2520challenging%2520task%2520with%2520limited%250Aaccuracy.%2520The%2520second%2520challenge%2520is%2520the%2520joint%2520analysis%2520of%25202D%2520PX%2520with%2520its%25203D%250Asynthesized%2520counterpart%252C%2520with%2520the%2520aim%2520to%2520maximize%2520the%25202D-3D%2520synergy%2520while%250Aminimizing%2520the%2520errors%2520arising%2520from%2520the%2520synthesized%2520image.%2520In%2520this%2520study%252C%2520we%250Apropose%2520a%2520new%2520method%2520termed%25203DPX%2520-%2520PX%2520image%2520analysis%2520guided%2520by%25202D-to-3D%250Areconstruction%252C%2520to%2520overcome%2520these%2520challenges.%25203DPX%2520consists%2520of%2520%2528i%2529%2520a%2520novel%250Aprogressive%2520reconstruction%2520network%2520to%2520improve%25202D-to-3D%2520reconstruction%2520and%252C%2520%2528ii%2529%250Aa%2520contrastive-guided%2520bidirectional%2520multimodality%2520alignment%2520module%2520for%25203D-guided%250A2D%2520PX%2520classification%2520and%2520segmentation%2520tasks.%2520The%2520reconstruction%2520network%250Aprogressively%2520reconstructs%25203D%2520images%2520with%2520knowledge%2520imposed%2520on%2520the%2520intermediate%250Areconstructions%2520at%2520multiple%2520pyramid%2520levels%2520and%2520incorporates%2520Multilayer%250APerceptrons%2520to%2520improve%2520semantic%2520understanding.%2520The%2520downstream%2520networks%2520leverage%250Athe%2520reconstructed%2520images%2520as%25203D%2520anatomical%2520guidance%2520to%2520the%2520PX%2520analysis%2520through%250Afeature%2520alignment%252C%2520which%2520increases%2520the%25202D-3D%2520synergy%2520with%2520bidirectional%2520feature%250Aprojection%2520and%2520decease%2520the%2520impact%2520of%2520potential%2520errors%2520with%2520contrastive%250Aguidance.%2520Extensive%2520experiments%2520on%2520two%2520oral%2520datasets%2520involving%2520464%2520studies%250Ademonstrate%2520that%25203DPX%2520outperforms%2520the%2520state-of-the-art%2520methods%2520in%2520various%2520tasks%250Aincluding%25202D-to-3D%2520reconstruction%252C%2520PX%2520classification%2520and%2520lesion%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DPX%3A%20Single%20Panoramic%20X-ray%20Analysis%20Guided%20by%203D%20Oral%20Structure%0A%20%20Reconstruction&entry.906535625=Xiaoshuang%20Li%20and%20Zimo%20Huang%20and%20Mingyuan%20Meng%20and%20Eduardo%20Delamare%20and%20Dagan%20Feng%20and%20Lei%20Bi%20and%20Bin%20Sheng%20and%20Lingyong%20Jiang%20and%20Bo%20Li%20and%20Jinman%20Kim&entry.1292438233=%20%20Panoramic%20X-ray%20%28PX%29%20is%20a%20prevalent%20modality%20in%20dentistry%20practice%20owing%20to%0Aits%20wide%20availability%20and%20low%20cost.%20However%2C%20as%20a%202D%20projection%20of%20a%203D%0Astructure%2C%20PX%20suffers%20from%20anatomical%20information%20loss%20and%20PX%20diagnosis%20is%0Alimited%20compared%20to%20that%20with%203D%20imaging%20modalities.%202D-to-3D%20reconstruction%0Amethods%20have%20been%20explored%20for%20the%20ability%20to%20synthesize%20the%20absent%203D%0Aanatomical%20information%20from%202D%20PX%20for%20use%20in%20PX%20image%20analysis.%20However%2C%20there%0Aare%20challenges%20in%20leveraging%20such%203D%20synthesized%20reconstructions.%20First%2C%0Ainferring%203D%20depth%20from%202D%20images%20remains%20a%20challenging%20task%20with%20limited%0Aaccuracy.%20The%20second%20challenge%20is%20the%20joint%20analysis%20of%202D%20PX%20with%20its%203D%0Asynthesized%20counterpart%2C%20with%20the%20aim%20to%20maximize%20the%202D-3D%20synergy%20while%0Aminimizing%20the%20errors%20arising%20from%20the%20synthesized%20image.%20In%20this%20study%2C%20we%0Apropose%20a%20new%20method%20termed%203DPX%20-%20PX%20image%20analysis%20guided%20by%202D-to-3D%0Areconstruction%2C%20to%20overcome%20these%20challenges.%203DPX%20consists%20of%20%28i%29%20a%20novel%0Aprogressive%20reconstruction%20network%20to%20improve%202D-to-3D%20reconstruction%20and%2C%20%28ii%29%0Aa%20contrastive-guided%20bidirectional%20multimodality%20alignment%20module%20for%203D-guided%0A2D%20PX%20classification%20and%20segmentation%20tasks.%20The%20reconstruction%20network%0Aprogressively%20reconstructs%203D%20images%20with%20knowledge%20imposed%20on%20the%20intermediate%0Areconstructions%20at%20multiple%20pyramid%20levels%20and%20incorporates%20Multilayer%0APerceptrons%20to%20improve%20semantic%20understanding.%20The%20downstream%20networks%20leverage%0Athe%20reconstructed%20images%20as%203D%20anatomical%20guidance%20to%20the%20PX%20analysis%20through%0Afeature%20alignment%2C%20which%20increases%20the%202D-3D%20synergy%20with%20bidirectional%20feature%0Aprojection%20and%20decease%20the%20impact%20of%20potential%20errors%20with%20contrastive%0Aguidance.%20Extensive%20experiments%20on%20two%20oral%20datasets%20involving%20464%20studies%0Ademonstrate%20that%203DPX%20outperforms%20the%20state-of-the-art%20methods%20in%20various%20tasks%0Aincluding%202D-to-3D%20reconstruction%2C%20PX%20classification%20and%20lesion%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18701v1&entry.124074799=Read"},
{"title": "Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in\n  Continuous Environment with Open-Source LLMs", "author": "Yanyuan Qiao and Wenqi Lyu and Hui Wang and Zixu Wang and Zerui Li and Yuan Zhang and Mingkui Tan and Qi Wu", "abstract": "  Vision-and-Language Navigation (VLN) tasks require an agent to follow textual\ninstructions to navigate through 3D environments. Traditional approaches use\nsupervised learning methods, relying heavily on domain-specific datasets to\ntrain VLN models. Recent methods try to utilize closed-source large language\nmodels (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but face\nchallenges related to expensive token costs and potential data breaches in\nreal-world applications. In this work, we introduce Open-Nav, a novel study\nthat explores open-source LLMs for zero-shot VLN in the continuous environment.\nOpen-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approach\nto break down tasks into instruction comprehension, progress estimation, and\ndecision-making. It enhances scene perceptions with fine-grained object and\nspatial knowledge to improve LLM's reasoning in navigation. Our extensive\nexperiments in both simulated and real-world environments demonstrate that\nOpen-Nav achieves competitive performance compared to using closed-source LLMs.\n", "link": "http://arxiv.org/abs/2409.18794v1", "date": "2024-09-27", "relevancy": 2.8921, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Nav%3A%20Exploring%20Zero-Shot%20Vision-and-Language%20Navigation%20in%0A%20%20Continuous%20Environment%20with%20Open-Source%20LLMs&body=Title%3A%20Open-Nav%3A%20Exploring%20Zero-Shot%20Vision-and-Language%20Navigation%20in%0A%20%20Continuous%20Environment%20with%20Open-Source%20LLMs%0AAuthor%3A%20Yanyuan%20Qiao%20and%20Wenqi%20Lyu%20and%20Hui%20Wang%20and%20Zixu%20Wang%20and%20Zerui%20Li%20and%20Yuan%20Zhang%20and%20Mingkui%20Tan%20and%20Qi%20Wu%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%20tasks%20require%20an%20agent%20to%20follow%20textual%0Ainstructions%20to%20navigate%20through%203D%20environments.%20Traditional%20approaches%20use%0Asupervised%20learning%20methods%2C%20relying%20heavily%20on%20domain-specific%20datasets%20to%0Atrain%20VLN%20models.%20Recent%20methods%20try%20to%20utilize%20closed-source%20large%20language%0Amodels%20%28LLMs%29%20like%20GPT-4%20to%20solve%20VLN%20tasks%20in%20zero-shot%20manners%2C%20but%20face%0Achallenges%20related%20to%20expensive%20token%20costs%20and%20potential%20data%20breaches%20in%0Areal-world%20applications.%20In%20this%20work%2C%20we%20introduce%20Open-Nav%2C%20a%20novel%20study%0Athat%20explores%20open-source%20LLMs%20for%20zero-shot%20VLN%20in%20the%20continuous%20environment.%0AOpen-Nav%20employs%20a%20spatial-temporal%20chain-of-thought%20%28CoT%29%20reasoning%20approach%0Ato%20break%20down%20tasks%20into%20instruction%20comprehension%2C%20progress%20estimation%2C%20and%0Adecision-making.%20It%20enhances%20scene%20perceptions%20with%20fine-grained%20object%20and%0Aspatial%20knowledge%20to%20improve%20LLM%27s%20reasoning%20in%20navigation.%20Our%20extensive%0Aexperiments%20in%20both%20simulated%20and%20real-world%20environments%20demonstrate%20that%0AOpen-Nav%20achieves%20competitive%20performance%20compared%20to%20using%20closed-source%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Nav%253A%2520Exploring%2520Zero-Shot%2520Vision-and-Language%2520Navigation%2520in%250A%2520%2520Continuous%2520Environment%2520with%2520Open-Source%2520LLMs%26entry.906535625%3DYanyuan%2520Qiao%2520and%2520Wenqi%2520Lyu%2520and%2520Hui%2520Wang%2520and%2520Zixu%2520Wang%2520and%2520Zerui%2520Li%2520and%2520Yuan%2520Zhang%2520and%2520Mingkui%2520Tan%2520and%2520Qi%2520Wu%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520tasks%2520require%2520an%2520agent%2520to%2520follow%2520textual%250Ainstructions%2520to%2520navigate%2520through%25203D%2520environments.%2520Traditional%2520approaches%2520use%250Asupervised%2520learning%2520methods%252C%2520relying%2520heavily%2520on%2520domain-specific%2520datasets%2520to%250Atrain%2520VLN%2520models.%2520Recent%2520methods%2520try%2520to%2520utilize%2520closed-source%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520like%2520GPT-4%2520to%2520solve%2520VLN%2520tasks%2520in%2520zero-shot%2520manners%252C%2520but%2520face%250Achallenges%2520related%2520to%2520expensive%2520token%2520costs%2520and%2520potential%2520data%2520breaches%2520in%250Areal-world%2520applications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Open-Nav%252C%2520a%2520novel%2520study%250Athat%2520explores%2520open-source%2520LLMs%2520for%2520zero-shot%2520VLN%2520in%2520the%2520continuous%2520environment.%250AOpen-Nav%2520employs%2520a%2520spatial-temporal%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520approach%250Ato%2520break%2520down%2520tasks%2520into%2520instruction%2520comprehension%252C%2520progress%2520estimation%252C%2520and%250Adecision-making.%2520It%2520enhances%2520scene%2520perceptions%2520with%2520fine-grained%2520object%2520and%250Aspatial%2520knowledge%2520to%2520improve%2520LLM%2527s%2520reasoning%2520in%2520navigation.%2520Our%2520extensive%250Aexperiments%2520in%2520both%2520simulated%2520and%2520real-world%2520environments%2520demonstrate%2520that%250AOpen-Nav%2520achieves%2520competitive%2520performance%2520compared%2520to%2520using%2520closed-source%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Nav%3A%20Exploring%20Zero-Shot%20Vision-and-Language%20Navigation%20in%0A%20%20Continuous%20Environment%20with%20Open-Source%20LLMs&entry.906535625=Yanyuan%20Qiao%20and%20Wenqi%20Lyu%20and%20Hui%20Wang%20and%20Zixu%20Wang%20and%20Zerui%20Li%20and%20Yuan%20Zhang%20and%20Mingkui%20Tan%20and%20Qi%20Wu&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%20tasks%20require%20an%20agent%20to%20follow%20textual%0Ainstructions%20to%20navigate%20through%203D%20environments.%20Traditional%20approaches%20use%0Asupervised%20learning%20methods%2C%20relying%20heavily%20on%20domain-specific%20datasets%20to%0Atrain%20VLN%20models.%20Recent%20methods%20try%20to%20utilize%20closed-source%20large%20language%0Amodels%20%28LLMs%29%20like%20GPT-4%20to%20solve%20VLN%20tasks%20in%20zero-shot%20manners%2C%20but%20face%0Achallenges%20related%20to%20expensive%20token%20costs%20and%20potential%20data%20breaches%20in%0Areal-world%20applications.%20In%20this%20work%2C%20we%20introduce%20Open-Nav%2C%20a%20novel%20study%0Athat%20explores%20open-source%20LLMs%20for%20zero-shot%20VLN%20in%20the%20continuous%20environment.%0AOpen-Nav%20employs%20a%20spatial-temporal%20chain-of-thought%20%28CoT%29%20reasoning%20approach%0Ato%20break%20down%20tasks%20into%20instruction%20comprehension%2C%20progress%20estimation%2C%20and%0Adecision-making.%20It%20enhances%20scene%20perceptions%20with%20fine-grained%20object%20and%0Aspatial%20knowledge%20to%20improve%20LLM%27s%20reasoning%20in%20navigation.%20Our%20extensive%0Aexperiments%20in%20both%20simulated%20and%20real-world%20environments%20demonstrate%20that%0AOpen-Nav%20achieves%20competitive%20performance%20compared%20to%20using%20closed-source%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18794v1&entry.124074799=Read"},
{"title": "Leveraging Anthropometric Measurements to Improve Human Mesh Estimation\n  and Ensure Consistent Body Shapes", "author": "Katja Ludwig and Julian Lorenz and Daniel Kienzle and Tuan Bui and Rainer Lienhart", "abstract": "  The basic body shape of a person does not change within a single video.\nHowever, most SOTA human mesh estimation (HME) models output a slightly\ndifferent body shape for each video frame, which results in inconsistent body\nshapes for the same person. In contrast, we leverage anthropometric\nmeasurements like tailors are already obtaining from humans for centuries. We\ncreate a model called A2B that converts such anthropometric measurements to\nbody shape parameters of human mesh models. Moreover, we find that finetuned\nSOTA 3D human pose estimation (HPE) models outperform HME models regarding the\nprecision of the estimated keypoints. We show that applying inverse kinematics\n(IK) to the results of such a 3D HPE model and combining the resulting body\npose with the A2B body shape leads to superior and consistent human meshes for\nchallenging datasets like ASPset or fit3D, where we can lower the MPJPE by over\n30 mm compared to SOTA HME models. Further, replacing HME models estimates of\nthe body shape parameters with A2B model results not only increases the\nperformance of these HME models, but also leads to consistent body shapes.\n", "link": "http://arxiv.org/abs/2409.17671v2", "date": "2024-09-27", "relevancy": 2.8066, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6054}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5393}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Anthropometric%20Measurements%20to%20Improve%20Human%20Mesh%20Estimation%0A%20%20and%20Ensure%20Consistent%20Body%20Shapes&body=Title%3A%20Leveraging%20Anthropometric%20Measurements%20to%20Improve%20Human%20Mesh%20Estimation%0A%20%20and%20Ensure%20Consistent%20Body%20Shapes%0AAuthor%3A%20Katja%20Ludwig%20and%20Julian%20Lorenz%20and%20Daniel%20Kienzle%20and%20Tuan%20Bui%20and%20Rainer%20Lienhart%0AAbstract%3A%20%20%20The%20basic%20body%20shape%20of%20a%20person%20does%20not%20change%20within%20a%20single%20video.%0AHowever%2C%20most%20SOTA%20human%20mesh%20estimation%20%28HME%29%20models%20output%20a%20slightly%0Adifferent%20body%20shape%20for%20each%20video%20frame%2C%20which%20results%20in%20inconsistent%20body%0Ashapes%20for%20the%20same%20person.%20In%20contrast%2C%20we%20leverage%20anthropometric%0Ameasurements%20like%20tailors%20are%20already%20obtaining%20from%20humans%20for%20centuries.%20We%0Acreate%20a%20model%20called%20A2B%20that%20converts%20such%20anthropometric%20measurements%20to%0Abody%20shape%20parameters%20of%20human%20mesh%20models.%20Moreover%2C%20we%20find%20that%20finetuned%0ASOTA%203D%20human%20pose%20estimation%20%28HPE%29%20models%20outperform%20HME%20models%20regarding%20the%0Aprecision%20of%20the%20estimated%20keypoints.%20We%20show%20that%20applying%20inverse%20kinematics%0A%28IK%29%20to%20the%20results%20of%20such%20a%203D%20HPE%20model%20and%20combining%20the%20resulting%20body%0Apose%20with%20the%20A2B%20body%20shape%20leads%20to%20superior%20and%20consistent%20human%20meshes%20for%0Achallenging%20datasets%20like%20ASPset%20or%20fit3D%2C%20where%20we%20can%20lower%20the%20MPJPE%20by%20over%0A30%20mm%20compared%20to%20SOTA%20HME%20models.%20Further%2C%20replacing%20HME%20models%20estimates%20of%0Athe%20body%20shape%20parameters%20with%20A2B%20model%20results%20not%20only%20increases%20the%0Aperformance%20of%20these%20HME%20models%2C%20but%20also%20leads%20to%20consistent%20body%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17671v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Anthropometric%2520Measurements%2520to%2520Improve%2520Human%2520Mesh%2520Estimation%250A%2520%2520and%2520Ensure%2520Consistent%2520Body%2520Shapes%26entry.906535625%3DKatja%2520Ludwig%2520and%2520Julian%2520Lorenz%2520and%2520Daniel%2520Kienzle%2520and%2520Tuan%2520Bui%2520and%2520Rainer%2520Lienhart%26entry.1292438233%3D%2520%2520The%2520basic%2520body%2520shape%2520of%2520a%2520person%2520does%2520not%2520change%2520within%2520a%2520single%2520video.%250AHowever%252C%2520most%2520SOTA%2520human%2520mesh%2520estimation%2520%2528HME%2529%2520models%2520output%2520a%2520slightly%250Adifferent%2520body%2520shape%2520for%2520each%2520video%2520frame%252C%2520which%2520results%2520in%2520inconsistent%2520body%250Ashapes%2520for%2520the%2520same%2520person.%2520In%2520contrast%252C%2520we%2520leverage%2520anthropometric%250Ameasurements%2520like%2520tailors%2520are%2520already%2520obtaining%2520from%2520humans%2520for%2520centuries.%2520We%250Acreate%2520a%2520model%2520called%2520A2B%2520that%2520converts%2520such%2520anthropometric%2520measurements%2520to%250Abody%2520shape%2520parameters%2520of%2520human%2520mesh%2520models.%2520Moreover%252C%2520we%2520find%2520that%2520finetuned%250ASOTA%25203D%2520human%2520pose%2520estimation%2520%2528HPE%2529%2520models%2520outperform%2520HME%2520models%2520regarding%2520the%250Aprecision%2520of%2520the%2520estimated%2520keypoints.%2520We%2520show%2520that%2520applying%2520inverse%2520kinematics%250A%2528IK%2529%2520to%2520the%2520results%2520of%2520such%2520a%25203D%2520HPE%2520model%2520and%2520combining%2520the%2520resulting%2520body%250Apose%2520with%2520the%2520A2B%2520body%2520shape%2520leads%2520to%2520superior%2520and%2520consistent%2520human%2520meshes%2520for%250Achallenging%2520datasets%2520like%2520ASPset%2520or%2520fit3D%252C%2520where%2520we%2520can%2520lower%2520the%2520MPJPE%2520by%2520over%250A30%2520mm%2520compared%2520to%2520SOTA%2520HME%2520models.%2520Further%252C%2520replacing%2520HME%2520models%2520estimates%2520of%250Athe%2520body%2520shape%2520parameters%2520with%2520A2B%2520model%2520results%2520not%2520only%2520increases%2520the%250Aperformance%2520of%2520these%2520HME%2520models%252C%2520but%2520also%2520leads%2520to%2520consistent%2520body%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17671v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Anthropometric%20Measurements%20to%20Improve%20Human%20Mesh%20Estimation%0A%20%20and%20Ensure%20Consistent%20Body%20Shapes&entry.906535625=Katja%20Ludwig%20and%20Julian%20Lorenz%20and%20Daniel%20Kienzle%20and%20Tuan%20Bui%20and%20Rainer%20Lienhart&entry.1292438233=%20%20The%20basic%20body%20shape%20of%20a%20person%20does%20not%20change%20within%20a%20single%20video.%0AHowever%2C%20most%20SOTA%20human%20mesh%20estimation%20%28HME%29%20models%20output%20a%20slightly%0Adifferent%20body%20shape%20for%20each%20video%20frame%2C%20which%20results%20in%20inconsistent%20body%0Ashapes%20for%20the%20same%20person.%20In%20contrast%2C%20we%20leverage%20anthropometric%0Ameasurements%20like%20tailors%20are%20already%20obtaining%20from%20humans%20for%20centuries.%20We%0Acreate%20a%20model%20called%20A2B%20that%20converts%20such%20anthropometric%20measurements%20to%0Abody%20shape%20parameters%20of%20human%20mesh%20models.%20Moreover%2C%20we%20find%20that%20finetuned%0ASOTA%203D%20human%20pose%20estimation%20%28HPE%29%20models%20outperform%20HME%20models%20regarding%20the%0Aprecision%20of%20the%20estimated%20keypoints.%20We%20show%20that%20applying%20inverse%20kinematics%0A%28IK%29%20to%20the%20results%20of%20such%20a%203D%20HPE%20model%20and%20combining%20the%20resulting%20body%0Apose%20with%20the%20A2B%20body%20shape%20leads%20to%20superior%20and%20consistent%20human%20meshes%20for%0Achallenging%20datasets%20like%20ASPset%20or%20fit3D%2C%20where%20we%20can%20lower%20the%20MPJPE%20by%20over%0A30%20mm%20compared%20to%20SOTA%20HME%20models.%20Further%2C%20replacing%20HME%20models%20estimates%20of%0Athe%20body%20shape%20parameters%20with%20A2B%20model%20results%20not%20only%20increases%20the%0Aperformance%20of%20these%20HME%20models%2C%20but%20also%20leads%20to%20consistent%20body%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17671v2&entry.124074799=Read"},
{"title": "Individuation in Neural Models with and without Visual Grounding", "author": "Alexey Tikhonov and Lisa Bylinina and Ivan P. Yamshchikov", "abstract": "  We show differences between a language-and-vision model CLIP and two\ntext-only models - FastText and SBERT - when it comes to the encoding of\nindividuation information. We study latent representations that CLIP provides\nfor substrates, granular aggregates, and various numbers of objects. We\ndemonstrate that CLIP embeddings capture quantitative differences in\nindividuation better than models trained on text-only data. Moreover, the\nindividuation hierarchy we deduce from the CLIP embeddings agrees with the\nhierarchies proposed in linguistics and cognitive science.\n", "link": "http://arxiv.org/abs/2409.18868v1", "date": "2024-09-27", "relevancy": 2.7867, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Individuation%20in%20Neural%20Models%20with%20and%20without%20Visual%20Grounding&body=Title%3A%20Individuation%20in%20Neural%20Models%20with%20and%20without%20Visual%20Grounding%0AAuthor%3A%20Alexey%20Tikhonov%20and%20Lisa%20Bylinina%20and%20Ivan%20P.%20Yamshchikov%0AAbstract%3A%20%20%20We%20show%20differences%20between%20a%20language-and-vision%20model%20CLIP%20and%20two%0Atext-only%20models%20-%20FastText%20and%20SBERT%20-%20when%20it%20comes%20to%20the%20encoding%20of%0Aindividuation%20information.%20We%20study%20latent%20representations%20that%20CLIP%20provides%0Afor%20substrates%2C%20granular%20aggregates%2C%20and%20various%20numbers%20of%20objects.%20We%0Ademonstrate%20that%20CLIP%20embeddings%20capture%20quantitative%20differences%20in%0Aindividuation%20better%20than%20models%20trained%20on%20text-only%20data.%20Moreover%2C%20the%0Aindividuation%20hierarchy%20we%20deduce%20from%20the%20CLIP%20embeddings%20agrees%20with%20the%0Ahierarchies%20proposed%20in%20linguistics%20and%20cognitive%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndividuation%2520in%2520Neural%2520Models%2520with%2520and%2520without%2520Visual%2520Grounding%26entry.906535625%3DAlexey%2520Tikhonov%2520and%2520Lisa%2520Bylinina%2520and%2520Ivan%2520P.%2520Yamshchikov%26entry.1292438233%3D%2520%2520We%2520show%2520differences%2520between%2520a%2520language-and-vision%2520model%2520CLIP%2520and%2520two%250Atext-only%2520models%2520-%2520FastText%2520and%2520SBERT%2520-%2520when%2520it%2520comes%2520to%2520the%2520encoding%2520of%250Aindividuation%2520information.%2520We%2520study%2520latent%2520representations%2520that%2520CLIP%2520provides%250Afor%2520substrates%252C%2520granular%2520aggregates%252C%2520and%2520various%2520numbers%2520of%2520objects.%2520We%250Ademonstrate%2520that%2520CLIP%2520embeddings%2520capture%2520quantitative%2520differences%2520in%250Aindividuation%2520better%2520than%2520models%2520trained%2520on%2520text-only%2520data.%2520Moreover%252C%2520the%250Aindividuation%2520hierarchy%2520we%2520deduce%2520from%2520the%2520CLIP%2520embeddings%2520agrees%2520with%2520the%250Ahierarchies%2520proposed%2520in%2520linguistics%2520and%2520cognitive%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Individuation%20in%20Neural%20Models%20with%20and%20without%20Visual%20Grounding&entry.906535625=Alexey%20Tikhonov%20and%20Lisa%20Bylinina%20and%20Ivan%20P.%20Yamshchikov&entry.1292438233=%20%20We%20show%20differences%20between%20a%20language-and-vision%20model%20CLIP%20and%20two%0Atext-only%20models%20-%20FastText%20and%20SBERT%20-%20when%20it%20comes%20to%20the%20encoding%20of%0Aindividuation%20information.%20We%20study%20latent%20representations%20that%20CLIP%20provides%0Afor%20substrates%2C%20granular%20aggregates%2C%20and%20various%20numbers%20of%20objects.%20We%0Ademonstrate%20that%20CLIP%20embeddings%20capture%20quantitative%20differences%20in%0Aindividuation%20better%20than%20models%20trained%20on%20text-only%20data.%20Moreover%2C%20the%0Aindividuation%20hierarchy%20we%20deduce%20from%20the%20CLIP%20embeddings%20agrees%20with%20the%0Ahierarchies%20proposed%20in%20linguistics%20and%20cognitive%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18868v1&entry.124074799=Read"},
{"title": "Emu3: Next-Token Prediction is All You Need", "author": "Xinlong Wang and Xiaosong Zhang and Zhengxiong Luo and Quan Sun and Yufeng Cui and Jinsheng Wang and Fan Zhang and Yueze Wang and Zhen Li and Qiying Yu and Yingli Zhao and Yulong Ao and Xuebin Min and Tao Li and Boya Wu and Bo Zhao and Bowen Zhang and Liangdong Wang and Guang Liu and Zheqi He and Xi Yang and Jingjing Liu and Yonghua Lin and Tiejun Huang and Zhongyuan Wang", "abstract": "  While next-token prediction is considered a promising path towards artificial\ngeneral intelligence, it has struggled to excel in multimodal tasks, which are\nstill dominated by diffusion models (e.g., Stable Diffusion) and compositional\napproaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a\nnew suite of state-of-the-art multimodal models trained solely with next-token\nprediction. By tokenizing images, text, and videos into a discrete space, we\ntrain a single transformer from scratch on a mixture of multimodal sequences.\nEmu3 outperforms several well-established task-specific models in both\ngeneration and perception tasks, surpassing flagship models such as SDXL and\nLLaVA-1.6, while eliminating the need for diffusion or compositional\narchitectures. Emu3 is also capable of generating high-fidelity video via\npredicting the next token in a video sequence. We simplify complex multimodal\nmodel designs by converging on a singular focus: tokens, unlocking great\npotential for scaling both during training and inference. Our results\ndemonstrate that next-token prediction is a promising path towards building\ngeneral multimodal intelligence beyond language. We open-source key techniques\nand models to support further research in this direction.\n", "link": "http://arxiv.org/abs/2409.18869v1", "date": "2024-09-27", "relevancy": 2.7787, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emu3%3A%20Next-Token%20Prediction%20is%20All%20You%20Need&body=Title%3A%20Emu3%3A%20Next-Token%20Prediction%20is%20All%20You%20Need%0AAuthor%3A%20Xinlong%20Wang%20and%20Xiaosong%20Zhang%20and%20Zhengxiong%20Luo%20and%20Quan%20Sun%20and%20Yufeng%20Cui%20and%20Jinsheng%20Wang%20and%20Fan%20Zhang%20and%20Yueze%20Wang%20and%20Zhen%20Li%20and%20Qiying%20Yu%20and%20Yingli%20Zhao%20and%20Yulong%20Ao%20and%20Xuebin%20Min%20and%20Tao%20Li%20and%20Boya%20Wu%20and%20Bo%20Zhao%20and%20Bowen%20Zhang%20and%20Liangdong%20Wang%20and%20Guang%20Liu%20and%20Zheqi%20He%20and%20Xi%20Yang%20and%20Jingjing%20Liu%20and%20Yonghua%20Lin%20and%20Tiejun%20Huang%20and%20Zhongyuan%20Wang%0AAbstract%3A%20%20%20While%20next-token%20prediction%20is%20considered%20a%20promising%20path%20towards%20artificial%0Ageneral%20intelligence%2C%20it%20has%20struggled%20to%20excel%20in%20multimodal%20tasks%2C%20which%20are%0Astill%20dominated%20by%20diffusion%20models%20%28e.g.%2C%20Stable%20Diffusion%29%20and%20compositional%0Aapproaches%20%28e.g.%2C%20CLIP%20combined%20with%20LLMs%29.%20In%20this%20paper%2C%20we%20introduce%20Emu3%2C%20a%0Anew%20suite%20of%20state-of-the-art%20multimodal%20models%20trained%20solely%20with%20next-token%0Aprediction.%20By%20tokenizing%20images%2C%20text%2C%20and%20videos%20into%20a%20discrete%20space%2C%20we%0Atrain%20a%20single%20transformer%20from%20scratch%20on%20a%20mixture%20of%20multimodal%20sequences.%0AEmu3%20outperforms%20several%20well-established%20task-specific%20models%20in%20both%0Ageneration%20and%20perception%20tasks%2C%20surpassing%20flagship%20models%20such%20as%20SDXL%20and%0ALLaVA-1.6%2C%20while%20eliminating%20the%20need%20for%20diffusion%20or%20compositional%0Aarchitectures.%20Emu3%20is%20also%20capable%20of%20generating%20high-fidelity%20video%20via%0Apredicting%20the%20next%20token%20in%20a%20video%20sequence.%20We%20simplify%20complex%20multimodal%0Amodel%20designs%20by%20converging%20on%20a%20singular%20focus%3A%20tokens%2C%20unlocking%20great%0Apotential%20for%20scaling%20both%20during%20training%20and%20inference.%20Our%20results%0Ademonstrate%20that%20next-token%20prediction%20is%20a%20promising%20path%20towards%20building%0Ageneral%20multimodal%20intelligence%20beyond%20language.%20We%20open-source%20key%20techniques%0Aand%20models%20to%20support%20further%20research%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmu3%253A%2520Next-Token%2520Prediction%2520is%2520All%2520You%2520Need%26entry.906535625%3DXinlong%2520Wang%2520and%2520Xiaosong%2520Zhang%2520and%2520Zhengxiong%2520Luo%2520and%2520Quan%2520Sun%2520and%2520Yufeng%2520Cui%2520and%2520Jinsheng%2520Wang%2520and%2520Fan%2520Zhang%2520and%2520Yueze%2520Wang%2520and%2520Zhen%2520Li%2520and%2520Qiying%2520Yu%2520and%2520Yingli%2520Zhao%2520and%2520Yulong%2520Ao%2520and%2520Xuebin%2520Min%2520and%2520Tao%2520Li%2520and%2520Boya%2520Wu%2520and%2520Bo%2520Zhao%2520and%2520Bowen%2520Zhang%2520and%2520Liangdong%2520Wang%2520and%2520Guang%2520Liu%2520and%2520Zheqi%2520He%2520and%2520Xi%2520Yang%2520and%2520Jingjing%2520Liu%2520and%2520Yonghua%2520Lin%2520and%2520Tiejun%2520Huang%2520and%2520Zhongyuan%2520Wang%26entry.1292438233%3D%2520%2520While%2520next-token%2520prediction%2520is%2520considered%2520a%2520promising%2520path%2520towards%2520artificial%250Ageneral%2520intelligence%252C%2520it%2520has%2520struggled%2520to%2520excel%2520in%2520multimodal%2520tasks%252C%2520which%2520are%250Astill%2520dominated%2520by%2520diffusion%2520models%2520%2528e.g.%252C%2520Stable%2520Diffusion%2529%2520and%2520compositional%250Aapproaches%2520%2528e.g.%252C%2520CLIP%2520combined%2520with%2520LLMs%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Emu3%252C%2520a%250Anew%2520suite%2520of%2520state-of-the-art%2520multimodal%2520models%2520trained%2520solely%2520with%2520next-token%250Aprediction.%2520By%2520tokenizing%2520images%252C%2520text%252C%2520and%2520videos%2520into%2520a%2520discrete%2520space%252C%2520we%250Atrain%2520a%2520single%2520transformer%2520from%2520scratch%2520on%2520a%2520mixture%2520of%2520multimodal%2520sequences.%250AEmu3%2520outperforms%2520several%2520well-established%2520task-specific%2520models%2520in%2520both%250Ageneration%2520and%2520perception%2520tasks%252C%2520surpassing%2520flagship%2520models%2520such%2520as%2520SDXL%2520and%250ALLaVA-1.6%252C%2520while%2520eliminating%2520the%2520need%2520for%2520diffusion%2520or%2520compositional%250Aarchitectures.%2520Emu3%2520is%2520also%2520capable%2520of%2520generating%2520high-fidelity%2520video%2520via%250Apredicting%2520the%2520next%2520token%2520in%2520a%2520video%2520sequence.%2520We%2520simplify%2520complex%2520multimodal%250Amodel%2520designs%2520by%2520converging%2520on%2520a%2520singular%2520focus%253A%2520tokens%252C%2520unlocking%2520great%250Apotential%2520for%2520scaling%2520both%2520during%2520training%2520and%2520inference.%2520Our%2520results%250Ademonstrate%2520that%2520next-token%2520prediction%2520is%2520a%2520promising%2520path%2520towards%2520building%250Ageneral%2520multimodal%2520intelligence%2520beyond%2520language.%2520We%2520open-source%2520key%2520techniques%250Aand%2520models%2520to%2520support%2520further%2520research%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emu3%3A%20Next-Token%20Prediction%20is%20All%20You%20Need&entry.906535625=Xinlong%20Wang%20and%20Xiaosong%20Zhang%20and%20Zhengxiong%20Luo%20and%20Quan%20Sun%20and%20Yufeng%20Cui%20and%20Jinsheng%20Wang%20and%20Fan%20Zhang%20and%20Yueze%20Wang%20and%20Zhen%20Li%20and%20Qiying%20Yu%20and%20Yingli%20Zhao%20and%20Yulong%20Ao%20and%20Xuebin%20Min%20and%20Tao%20Li%20and%20Boya%20Wu%20and%20Bo%20Zhao%20and%20Bowen%20Zhang%20and%20Liangdong%20Wang%20and%20Guang%20Liu%20and%20Zheqi%20He%20and%20Xi%20Yang%20and%20Jingjing%20Liu%20and%20Yonghua%20Lin%20and%20Tiejun%20Huang%20and%20Zhongyuan%20Wang&entry.1292438233=%20%20While%20next-token%20prediction%20is%20considered%20a%20promising%20path%20towards%20artificial%0Ageneral%20intelligence%2C%20it%20has%20struggled%20to%20excel%20in%20multimodal%20tasks%2C%20which%20are%0Astill%20dominated%20by%20diffusion%20models%20%28e.g.%2C%20Stable%20Diffusion%29%20and%20compositional%0Aapproaches%20%28e.g.%2C%20CLIP%20combined%20with%20LLMs%29.%20In%20this%20paper%2C%20we%20introduce%20Emu3%2C%20a%0Anew%20suite%20of%20state-of-the-art%20multimodal%20models%20trained%20solely%20with%20next-token%0Aprediction.%20By%20tokenizing%20images%2C%20text%2C%20and%20videos%20into%20a%20discrete%20space%2C%20we%0Atrain%20a%20single%20transformer%20from%20scratch%20on%20a%20mixture%20of%20multimodal%20sequences.%0AEmu3%20outperforms%20several%20well-established%20task-specific%20models%20in%20both%0Ageneration%20and%20perception%20tasks%2C%20surpassing%20flagship%20models%20such%20as%20SDXL%20and%0ALLaVA-1.6%2C%20while%20eliminating%20the%20need%20for%20diffusion%20or%20compositional%0Aarchitectures.%20Emu3%20is%20also%20capable%20of%20generating%20high-fidelity%20video%20via%0Apredicting%20the%20next%20token%20in%20a%20video%20sequence.%20We%20simplify%20complex%20multimodal%0Amodel%20designs%20by%20converging%20on%20a%20singular%20focus%3A%20tokens%2C%20unlocking%20great%0Apotential%20for%20scaling%20both%20during%20training%20and%20inference.%20Our%20results%0Ademonstrate%20that%20next-token%20prediction%20is%20a%20promising%20path%20towards%20building%0Ageneral%20multimodal%20intelligence%20beyond%20language.%20We%20open-source%20key%20techniques%0Aand%20models%20to%20support%20further%20research%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18869v1&entry.124074799=Read"},
{"title": "Geometric deep learning for galaxy-halo connection: a case study for\n  galaxy intrinsic alignments", "author": "Yesukhei Jagvaral and Francois Lanusse and Rachel Mandelbaum", "abstract": "  Forthcoming cosmological imaging surveys, such as the Rubin Observatory LSST,\nrequire large-scale simulations encompassing realistic galaxy populations for a\nvariety of scientific applications. Of particular concern is the phenomenon of\nintrinsic alignments (IA), whereby galaxies orient themselves towards\noverdensities, potentially introducing significant systematic biases in weak\ngravitational lensing analyses if they are not properly modeled. Due to\ncomputational constraints, simulating the intricate details of galaxy formation\nand evolution relevant to IA across vast volumes is impractical. As an\nalternative, we propose a Deep Generative Model trained on the IllustrisTNG-100\nsimulation to sample 3D galaxy shapes and orientations to accurately reproduce\nintrinsic alignments along with correlated scalar features. We model the cosmic\nweb as a set of graphs, each graph representing a halo with nodes representing\nthe subhalos/galaxies. The architecture consists of a SO(3) $\\times$\n$\\mathbb{R}^n$ diffusion generative model, for galaxy orientations and $n$\nscalars, implemented with E(3) equivariant Graph Neural Networks that\nexplicitly respect the Euclidean symmetries of our Universe. The model is able\nto learn and predict features such as galaxy orientations that are\nstatistically consistent with the reference simulation. Notably, our model\ndemonstrates the ability to jointly model Euclidean-valued scalars (galaxy\nsizes, shapes, and colors) along with non-Euclidean valued SO(3) quantities\n(galaxy orientations) that are governed by highly complex galactic physics at\nnon-linear scales.\n", "link": "http://arxiv.org/abs/2409.18761v1", "date": "2024-09-27", "relevancy": 2.7743, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.577}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5469}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20deep%20learning%20for%20galaxy-halo%20connection%3A%20a%20case%20study%20for%0A%20%20galaxy%20intrinsic%20alignments&body=Title%3A%20Geometric%20deep%20learning%20for%20galaxy-halo%20connection%3A%20a%20case%20study%20for%0A%20%20galaxy%20intrinsic%20alignments%0AAuthor%3A%20Yesukhei%20Jagvaral%20and%20Francois%20Lanusse%20and%20Rachel%20Mandelbaum%0AAbstract%3A%20%20%20Forthcoming%20cosmological%20imaging%20surveys%2C%20such%20as%20the%20Rubin%20Observatory%20LSST%2C%0Arequire%20large-scale%20simulations%20encompassing%20realistic%20galaxy%20populations%20for%20a%0Avariety%20of%20scientific%20applications.%20Of%20particular%20concern%20is%20the%20phenomenon%20of%0Aintrinsic%20alignments%20%28IA%29%2C%20whereby%20galaxies%20orient%20themselves%20towards%0Aoverdensities%2C%20potentially%20introducing%20significant%20systematic%20biases%20in%20weak%0Agravitational%20lensing%20analyses%20if%20they%20are%20not%20properly%20modeled.%20Due%20to%0Acomputational%20constraints%2C%20simulating%20the%20intricate%20details%20of%20galaxy%20formation%0Aand%20evolution%20relevant%20to%20IA%20across%20vast%20volumes%20is%20impractical.%20As%20an%0Aalternative%2C%20we%20propose%20a%20Deep%20Generative%20Model%20trained%20on%20the%20IllustrisTNG-100%0Asimulation%20to%20sample%203D%20galaxy%20shapes%20and%20orientations%20to%20accurately%20reproduce%0Aintrinsic%20alignments%20along%20with%20correlated%20scalar%20features.%20We%20model%20the%20cosmic%0Aweb%20as%20a%20set%20of%20graphs%2C%20each%20graph%20representing%20a%20halo%20with%20nodes%20representing%0Athe%20subhalos/galaxies.%20The%20architecture%20consists%20of%20a%20SO%283%29%20%24%5Ctimes%24%0A%24%5Cmathbb%7BR%7D%5En%24%20diffusion%20generative%20model%2C%20for%20galaxy%20orientations%20and%20%24n%24%0Ascalars%2C%20implemented%20with%20E%283%29%20equivariant%20Graph%20Neural%20Networks%20that%0Aexplicitly%20respect%20the%20Euclidean%20symmetries%20of%20our%20Universe.%20The%20model%20is%20able%0Ato%20learn%20and%20predict%20features%20such%20as%20galaxy%20orientations%20that%20are%0Astatistically%20consistent%20with%20the%20reference%20simulation.%20Notably%2C%20our%20model%0Ademonstrates%20the%20ability%20to%20jointly%20model%20Euclidean-valued%20scalars%20%28galaxy%0Asizes%2C%20shapes%2C%20and%20colors%29%20along%20with%20non-Euclidean%20valued%20SO%283%29%20quantities%0A%28galaxy%20orientations%29%20that%20are%20governed%20by%20highly%20complex%20galactic%20physics%20at%0Anon-linear%20scales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520deep%2520learning%2520for%2520galaxy-halo%2520connection%253A%2520a%2520case%2520study%2520for%250A%2520%2520galaxy%2520intrinsic%2520alignments%26entry.906535625%3DYesukhei%2520Jagvaral%2520and%2520Francois%2520Lanusse%2520and%2520Rachel%2520Mandelbaum%26entry.1292438233%3D%2520%2520Forthcoming%2520cosmological%2520imaging%2520surveys%252C%2520such%2520as%2520the%2520Rubin%2520Observatory%2520LSST%252C%250Arequire%2520large-scale%2520simulations%2520encompassing%2520realistic%2520galaxy%2520populations%2520for%2520a%250Avariety%2520of%2520scientific%2520applications.%2520Of%2520particular%2520concern%2520is%2520the%2520phenomenon%2520of%250Aintrinsic%2520alignments%2520%2528IA%2529%252C%2520whereby%2520galaxies%2520orient%2520themselves%2520towards%250Aoverdensities%252C%2520potentially%2520introducing%2520significant%2520systematic%2520biases%2520in%2520weak%250Agravitational%2520lensing%2520analyses%2520if%2520they%2520are%2520not%2520properly%2520modeled.%2520Due%2520to%250Acomputational%2520constraints%252C%2520simulating%2520the%2520intricate%2520details%2520of%2520galaxy%2520formation%250Aand%2520evolution%2520relevant%2520to%2520IA%2520across%2520vast%2520volumes%2520is%2520impractical.%2520As%2520an%250Aalternative%252C%2520we%2520propose%2520a%2520Deep%2520Generative%2520Model%2520trained%2520on%2520the%2520IllustrisTNG-100%250Asimulation%2520to%2520sample%25203D%2520galaxy%2520shapes%2520and%2520orientations%2520to%2520accurately%2520reproduce%250Aintrinsic%2520alignments%2520along%2520with%2520correlated%2520scalar%2520features.%2520We%2520model%2520the%2520cosmic%250Aweb%2520as%2520a%2520set%2520of%2520graphs%252C%2520each%2520graph%2520representing%2520a%2520halo%2520with%2520nodes%2520representing%250Athe%2520subhalos/galaxies.%2520The%2520architecture%2520consists%2520of%2520a%2520SO%25283%2529%2520%2524%255Ctimes%2524%250A%2524%255Cmathbb%257BR%257D%255En%2524%2520diffusion%2520generative%2520model%252C%2520for%2520galaxy%2520orientations%2520and%2520%2524n%2524%250Ascalars%252C%2520implemented%2520with%2520E%25283%2529%2520equivariant%2520Graph%2520Neural%2520Networks%2520that%250Aexplicitly%2520respect%2520the%2520Euclidean%2520symmetries%2520of%2520our%2520Universe.%2520The%2520model%2520is%2520able%250Ato%2520learn%2520and%2520predict%2520features%2520such%2520as%2520galaxy%2520orientations%2520that%2520are%250Astatistically%2520consistent%2520with%2520the%2520reference%2520simulation.%2520Notably%252C%2520our%2520model%250Ademonstrates%2520the%2520ability%2520to%2520jointly%2520model%2520Euclidean-valued%2520scalars%2520%2528galaxy%250Asizes%252C%2520shapes%252C%2520and%2520colors%2529%2520along%2520with%2520non-Euclidean%2520valued%2520SO%25283%2529%2520quantities%250A%2528galaxy%2520orientations%2529%2520that%2520are%2520governed%2520by%2520highly%2520complex%2520galactic%2520physics%2520at%250Anon-linear%2520scales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20deep%20learning%20for%20galaxy-halo%20connection%3A%20a%20case%20study%20for%0A%20%20galaxy%20intrinsic%20alignments&entry.906535625=Yesukhei%20Jagvaral%20and%20Francois%20Lanusse%20and%20Rachel%20Mandelbaum&entry.1292438233=%20%20Forthcoming%20cosmological%20imaging%20surveys%2C%20such%20as%20the%20Rubin%20Observatory%20LSST%2C%0Arequire%20large-scale%20simulations%20encompassing%20realistic%20galaxy%20populations%20for%20a%0Avariety%20of%20scientific%20applications.%20Of%20particular%20concern%20is%20the%20phenomenon%20of%0Aintrinsic%20alignments%20%28IA%29%2C%20whereby%20galaxies%20orient%20themselves%20towards%0Aoverdensities%2C%20potentially%20introducing%20significant%20systematic%20biases%20in%20weak%0Agravitational%20lensing%20analyses%20if%20they%20are%20not%20properly%20modeled.%20Due%20to%0Acomputational%20constraints%2C%20simulating%20the%20intricate%20details%20of%20galaxy%20formation%0Aand%20evolution%20relevant%20to%20IA%20across%20vast%20volumes%20is%20impractical.%20As%20an%0Aalternative%2C%20we%20propose%20a%20Deep%20Generative%20Model%20trained%20on%20the%20IllustrisTNG-100%0Asimulation%20to%20sample%203D%20galaxy%20shapes%20and%20orientations%20to%20accurately%20reproduce%0Aintrinsic%20alignments%20along%20with%20correlated%20scalar%20features.%20We%20model%20the%20cosmic%0Aweb%20as%20a%20set%20of%20graphs%2C%20each%20graph%20representing%20a%20halo%20with%20nodes%20representing%0Athe%20subhalos/galaxies.%20The%20architecture%20consists%20of%20a%20SO%283%29%20%24%5Ctimes%24%0A%24%5Cmathbb%7BR%7D%5En%24%20diffusion%20generative%20model%2C%20for%20galaxy%20orientations%20and%20%24n%24%0Ascalars%2C%20implemented%20with%20E%283%29%20equivariant%20Graph%20Neural%20Networks%20that%0Aexplicitly%20respect%20the%20Euclidean%20symmetries%20of%20our%20Universe.%20The%20model%20is%20able%0Ato%20learn%20and%20predict%20features%20such%20as%20galaxy%20orientations%20that%20are%0Astatistically%20consistent%20with%20the%20reference%20simulation.%20Notably%2C%20our%20model%0Ademonstrates%20the%20ability%20to%20jointly%20model%20Euclidean-valued%20scalars%20%28galaxy%0Asizes%2C%20shapes%2C%20and%20colors%29%20along%20with%20non-Euclidean%20valued%20SO%283%29%20quantities%0A%28galaxy%20orientations%29%20that%20are%20governed%20by%20highly%20complex%20galactic%20physics%20at%0Anon-linear%20scales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18761v1&entry.124074799=Read"},
{"title": "Exploiting Motion Prior for Accurate Pose Estimation of Dashboard\n  Cameras", "author": "Yipeng Lu and Yifan Zhao and Haiping Wang and Zhiwei Ruan and Yuan Liu and Zhen Dong and Bisheng Yang", "abstract": "  Dashboard cameras (dashcams) record millions of driving videos daily,\noffering a valuable potential data source for various applications, including\ndriving map production and updates. A necessary step for utilizing these\ndashcam data involves the estimation of camera poses. However, the low-quality\nimages captured by dashcams, characterized by motion blurs and dynamic objects,\npose challenges for existing image-matching methods in accurately estimating\ncamera poses. In this study, we propose a precise pose estimation method for\ndashcam images, leveraging the inherent camera motion prior. Typically, image\nsequences captured by dash cameras exhibit pronounced motion prior, such as\nforward movement or lateral turns, which serve as essential cues for\ncorrespondence estimation. Building upon this observation, we devise a pose\nregression module aimed at learning camera motion prior, subsequently\nintegrating these prior into both correspondences and pose estimation\nprocesses. The experiment shows that, in real dashcams dataset, our method is\n22% better than the baseline for pose estimation in AUC5\\textdegree, and it can\nestimate poses for 19% more images with less reprojection error in Structure\nfrom Motion (SfM).\n", "link": "http://arxiv.org/abs/2409.18673v1", "date": "2024-09-27", "relevancy": 2.6789, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5688}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5227}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Motion%20Prior%20for%20Accurate%20Pose%20Estimation%20of%20Dashboard%0A%20%20Cameras&body=Title%3A%20Exploiting%20Motion%20Prior%20for%20Accurate%20Pose%20Estimation%20of%20Dashboard%0A%20%20Cameras%0AAuthor%3A%20Yipeng%20Lu%20and%20Yifan%20Zhao%20and%20Haiping%20Wang%20and%20Zhiwei%20Ruan%20and%20Yuan%20Liu%20and%20Zhen%20Dong%20and%20Bisheng%20Yang%0AAbstract%3A%20%20%20Dashboard%20cameras%20%28dashcams%29%20record%20millions%20of%20driving%20videos%20daily%2C%0Aoffering%20a%20valuable%20potential%20data%20source%20for%20various%20applications%2C%20including%0Adriving%20map%20production%20and%20updates.%20A%20necessary%20step%20for%20utilizing%20these%0Adashcam%20data%20involves%20the%20estimation%20of%20camera%20poses.%20However%2C%20the%20low-quality%0Aimages%20captured%20by%20dashcams%2C%20characterized%20by%20motion%20blurs%20and%20dynamic%20objects%2C%0Apose%20challenges%20for%20existing%20image-matching%20methods%20in%20accurately%20estimating%0Acamera%20poses.%20In%20this%20study%2C%20we%20propose%20a%20precise%20pose%20estimation%20method%20for%0Adashcam%20images%2C%20leveraging%20the%20inherent%20camera%20motion%20prior.%20Typically%2C%20image%0Asequences%20captured%20by%20dash%20cameras%20exhibit%20pronounced%20motion%20prior%2C%20such%20as%0Aforward%20movement%20or%20lateral%20turns%2C%20which%20serve%20as%20essential%20cues%20for%0Acorrespondence%20estimation.%20Building%20upon%20this%20observation%2C%20we%20devise%20a%20pose%0Aregression%20module%20aimed%20at%20learning%20camera%20motion%20prior%2C%20subsequently%0Aintegrating%20these%20prior%20into%20both%20correspondences%20and%20pose%20estimation%0Aprocesses.%20The%20experiment%20shows%20that%2C%20in%20real%20dashcams%20dataset%2C%20our%20method%20is%0A22%25%20better%20than%20the%20baseline%20for%20pose%20estimation%20in%20AUC5%5Ctextdegree%2C%20and%20it%20can%0Aestimate%20poses%20for%2019%25%20more%20images%20with%20less%20reprojection%20error%20in%20Structure%0Afrom%20Motion%20%28SfM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Motion%2520Prior%2520for%2520Accurate%2520Pose%2520Estimation%2520of%2520Dashboard%250A%2520%2520Cameras%26entry.906535625%3DYipeng%2520Lu%2520and%2520Yifan%2520Zhao%2520and%2520Haiping%2520Wang%2520and%2520Zhiwei%2520Ruan%2520and%2520Yuan%2520Liu%2520and%2520Zhen%2520Dong%2520and%2520Bisheng%2520Yang%26entry.1292438233%3D%2520%2520Dashboard%2520cameras%2520%2528dashcams%2529%2520record%2520millions%2520of%2520driving%2520videos%2520daily%252C%250Aoffering%2520a%2520valuable%2520potential%2520data%2520source%2520for%2520various%2520applications%252C%2520including%250Adriving%2520map%2520production%2520and%2520updates.%2520A%2520necessary%2520step%2520for%2520utilizing%2520these%250Adashcam%2520data%2520involves%2520the%2520estimation%2520of%2520camera%2520poses.%2520However%252C%2520the%2520low-quality%250Aimages%2520captured%2520by%2520dashcams%252C%2520characterized%2520by%2520motion%2520blurs%2520and%2520dynamic%2520objects%252C%250Apose%2520challenges%2520for%2520existing%2520image-matching%2520methods%2520in%2520accurately%2520estimating%250Acamera%2520poses.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520precise%2520pose%2520estimation%2520method%2520for%250Adashcam%2520images%252C%2520leveraging%2520the%2520inherent%2520camera%2520motion%2520prior.%2520Typically%252C%2520image%250Asequences%2520captured%2520by%2520dash%2520cameras%2520exhibit%2520pronounced%2520motion%2520prior%252C%2520such%2520as%250Aforward%2520movement%2520or%2520lateral%2520turns%252C%2520which%2520serve%2520as%2520essential%2520cues%2520for%250Acorrespondence%2520estimation.%2520Building%2520upon%2520this%2520observation%252C%2520we%2520devise%2520a%2520pose%250Aregression%2520module%2520aimed%2520at%2520learning%2520camera%2520motion%2520prior%252C%2520subsequently%250Aintegrating%2520these%2520prior%2520into%2520both%2520correspondences%2520and%2520pose%2520estimation%250Aprocesses.%2520The%2520experiment%2520shows%2520that%252C%2520in%2520real%2520dashcams%2520dataset%252C%2520our%2520method%2520is%250A22%2525%2520better%2520than%2520the%2520baseline%2520for%2520pose%2520estimation%2520in%2520AUC5%255Ctextdegree%252C%2520and%2520it%2520can%250Aestimate%2520poses%2520for%252019%2525%2520more%2520images%2520with%2520less%2520reprojection%2520error%2520in%2520Structure%250Afrom%2520Motion%2520%2528SfM%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Motion%20Prior%20for%20Accurate%20Pose%20Estimation%20of%20Dashboard%0A%20%20Cameras&entry.906535625=Yipeng%20Lu%20and%20Yifan%20Zhao%20and%20Haiping%20Wang%20and%20Zhiwei%20Ruan%20and%20Yuan%20Liu%20and%20Zhen%20Dong%20and%20Bisheng%20Yang&entry.1292438233=%20%20Dashboard%20cameras%20%28dashcams%29%20record%20millions%20of%20driving%20videos%20daily%2C%0Aoffering%20a%20valuable%20potential%20data%20source%20for%20various%20applications%2C%20including%0Adriving%20map%20production%20and%20updates.%20A%20necessary%20step%20for%20utilizing%20these%0Adashcam%20data%20involves%20the%20estimation%20of%20camera%20poses.%20However%2C%20the%20low-quality%0Aimages%20captured%20by%20dashcams%2C%20characterized%20by%20motion%20blurs%20and%20dynamic%20objects%2C%0Apose%20challenges%20for%20existing%20image-matching%20methods%20in%20accurately%20estimating%0Acamera%20poses.%20In%20this%20study%2C%20we%20propose%20a%20precise%20pose%20estimation%20method%20for%0Adashcam%20images%2C%20leveraging%20the%20inherent%20camera%20motion%20prior.%20Typically%2C%20image%0Asequences%20captured%20by%20dash%20cameras%20exhibit%20pronounced%20motion%20prior%2C%20such%20as%0Aforward%20movement%20or%20lateral%20turns%2C%20which%20serve%20as%20essential%20cues%20for%0Acorrespondence%20estimation.%20Building%20upon%20this%20observation%2C%20we%20devise%20a%20pose%0Aregression%20module%20aimed%20at%20learning%20camera%20motion%20prior%2C%20subsequently%0Aintegrating%20these%20prior%20into%20both%20correspondences%20and%20pose%20estimation%0Aprocesses.%20The%20experiment%20shows%20that%2C%20in%20real%20dashcams%20dataset%2C%20our%20method%20is%0A22%25%20better%20than%20the%20baseline%20for%20pose%20estimation%20in%20AUC5%5Ctextdegree%2C%20and%20it%20can%0Aestimate%20poses%20for%2019%25%20more%20images%20with%20less%20reprojection%20error%20in%20Structure%0Afrom%20Motion%20%28SfM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18673v1&entry.124074799=Read"},
{"title": "TemporalPaD: a reinforcement-learning framework for temporal feature\n  representation and dimension reduction", "author": "Xuechen Mu and Zhenyu Huang and Kewei Li and Haotian Zhang and Xiuli Wang and Yusi Fan and Kai Zhang and Fengfeng Zhou", "abstract": "  Recent advancements in feature representation and dimension reduction have\nhighlighted their crucial role in enhancing the efficacy of predictive\nmodeling. This work introduces TemporalPaD, a novel end-to-end deep learning\nframework designed for temporal pattern datasets. TemporalPaD integrates\nreinforcement learning (RL) with neural networks to achieve concurrent feature\nrepresentation and feature reduction. The framework consists of three\ncooperative modules: a Policy Module, a Representation Module, and a\nClassification Module, structured based on the Actor-Critic (AC) framework. The\nPolicy Module, responsible for dimensionality reduction through RL, functions\nas the actor, while the Representation Module for feature extraction and the\nClassification Module collectively serve as the critic. We comprehensively\nevaluate TemporalPaD using 29 UCI datasets, a well-known benchmark for\nvalidating feature reduction algorithms, through 10 independent tests and\n10-fold cross-validation. Additionally, given that TemporalPaD is specifically\ndesigned for time series data, we apply it to a real-world DNA classification\nproblem involving enhancer category and enhancer strength. The results\ndemonstrate that TemporalPaD is an efficient and effective framework for\nachieving feature reduction, applicable to both structured data and sequence\ndatasets. The source code of the proposed TemporalPaD is freely available as\nsupplementary material to this article and at\nhttp://www.healthinformaticslab.org/supp/.\n", "link": "http://arxiv.org/abs/2409.18597v1", "date": "2024-09-27", "relevancy": 2.6752, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5478}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TemporalPaD%3A%20a%20reinforcement-learning%20framework%20for%20temporal%20feature%0A%20%20representation%20and%20dimension%20reduction&body=Title%3A%20TemporalPaD%3A%20a%20reinforcement-learning%20framework%20for%20temporal%20feature%0A%20%20representation%20and%20dimension%20reduction%0AAuthor%3A%20Xuechen%20Mu%20and%20Zhenyu%20Huang%20and%20Kewei%20Li%20and%20Haotian%20Zhang%20and%20Xiuli%20Wang%20and%20Yusi%20Fan%20and%20Kai%20Zhang%20and%20Fengfeng%20Zhou%0AAbstract%3A%20%20%20Recent%20advancements%20in%20feature%20representation%20and%20dimension%20reduction%20have%0Ahighlighted%20their%20crucial%20role%20in%20enhancing%20the%20efficacy%20of%20predictive%0Amodeling.%20This%20work%20introduces%20TemporalPaD%2C%20a%20novel%20end-to-end%20deep%20learning%0Aframework%20designed%20for%20temporal%20pattern%20datasets.%20TemporalPaD%20integrates%0Areinforcement%20learning%20%28RL%29%20with%20neural%20networks%20to%20achieve%20concurrent%20feature%0Arepresentation%20and%20feature%20reduction.%20The%20framework%20consists%20of%20three%0Acooperative%20modules%3A%20a%20Policy%20Module%2C%20a%20Representation%20Module%2C%20and%20a%0AClassification%20Module%2C%20structured%20based%20on%20the%20Actor-Critic%20%28AC%29%20framework.%20The%0APolicy%20Module%2C%20responsible%20for%20dimensionality%20reduction%20through%20RL%2C%20functions%0Aas%20the%20actor%2C%20while%20the%20Representation%20Module%20for%20feature%20extraction%20and%20the%0AClassification%20Module%20collectively%20serve%20as%20the%20critic.%20We%20comprehensively%0Aevaluate%20TemporalPaD%20using%2029%20UCI%20datasets%2C%20a%20well-known%20benchmark%20for%0Avalidating%20feature%20reduction%20algorithms%2C%20through%2010%20independent%20tests%20and%0A10-fold%20cross-validation.%20Additionally%2C%20given%20that%20TemporalPaD%20is%20specifically%0Adesigned%20for%20time%20series%20data%2C%20we%20apply%20it%20to%20a%20real-world%20DNA%20classification%0Aproblem%20involving%20enhancer%20category%20and%20enhancer%20strength.%20The%20results%0Ademonstrate%20that%20TemporalPaD%20is%20an%20efficient%20and%20effective%20framework%20for%0Aachieving%20feature%20reduction%2C%20applicable%20to%20both%20structured%20data%20and%20sequence%0Adatasets.%20The%20source%20code%20of%20the%20proposed%20TemporalPaD%20is%20freely%20available%20as%0Asupplementary%20material%20to%20this%20article%20and%20at%0Ahttp%3A//www.healthinformaticslab.org/supp/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporalPaD%253A%2520a%2520reinforcement-learning%2520framework%2520for%2520temporal%2520feature%250A%2520%2520representation%2520and%2520dimension%2520reduction%26entry.906535625%3DXuechen%2520Mu%2520and%2520Zhenyu%2520Huang%2520and%2520Kewei%2520Li%2520and%2520Haotian%2520Zhang%2520and%2520Xiuli%2520Wang%2520and%2520Yusi%2520Fan%2520and%2520Kai%2520Zhang%2520and%2520Fengfeng%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520feature%2520representation%2520and%2520dimension%2520reduction%2520have%250Ahighlighted%2520their%2520crucial%2520role%2520in%2520enhancing%2520the%2520efficacy%2520of%2520predictive%250Amodeling.%2520This%2520work%2520introduces%2520TemporalPaD%252C%2520a%2520novel%2520end-to-end%2520deep%2520learning%250Aframework%2520designed%2520for%2520temporal%2520pattern%2520datasets.%2520TemporalPaD%2520integrates%250Areinforcement%2520learning%2520%2528RL%2529%2520with%2520neural%2520networks%2520to%2520achieve%2520concurrent%2520feature%250Arepresentation%2520and%2520feature%2520reduction.%2520The%2520framework%2520consists%2520of%2520three%250Acooperative%2520modules%253A%2520a%2520Policy%2520Module%252C%2520a%2520Representation%2520Module%252C%2520and%2520a%250AClassification%2520Module%252C%2520structured%2520based%2520on%2520the%2520Actor-Critic%2520%2528AC%2529%2520framework.%2520The%250APolicy%2520Module%252C%2520responsible%2520for%2520dimensionality%2520reduction%2520through%2520RL%252C%2520functions%250Aas%2520the%2520actor%252C%2520while%2520the%2520Representation%2520Module%2520for%2520feature%2520extraction%2520and%2520the%250AClassification%2520Module%2520collectively%2520serve%2520as%2520the%2520critic.%2520We%2520comprehensively%250Aevaluate%2520TemporalPaD%2520using%252029%2520UCI%2520datasets%252C%2520a%2520well-known%2520benchmark%2520for%250Avalidating%2520feature%2520reduction%2520algorithms%252C%2520through%252010%2520independent%2520tests%2520and%250A10-fold%2520cross-validation.%2520Additionally%252C%2520given%2520that%2520TemporalPaD%2520is%2520specifically%250Adesigned%2520for%2520time%2520series%2520data%252C%2520we%2520apply%2520it%2520to%2520a%2520real-world%2520DNA%2520classification%250Aproblem%2520involving%2520enhancer%2520category%2520and%2520enhancer%2520strength.%2520The%2520results%250Ademonstrate%2520that%2520TemporalPaD%2520is%2520an%2520efficient%2520and%2520effective%2520framework%2520for%250Aachieving%2520feature%2520reduction%252C%2520applicable%2520to%2520both%2520structured%2520data%2520and%2520sequence%250Adatasets.%2520The%2520source%2520code%2520of%2520the%2520proposed%2520TemporalPaD%2520is%2520freely%2520available%2520as%250Asupplementary%2520material%2520to%2520this%2520article%2520and%2520at%250Ahttp%253A//www.healthinformaticslab.org/supp/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TemporalPaD%3A%20a%20reinforcement-learning%20framework%20for%20temporal%20feature%0A%20%20representation%20and%20dimension%20reduction&entry.906535625=Xuechen%20Mu%20and%20Zhenyu%20Huang%20and%20Kewei%20Li%20and%20Haotian%20Zhang%20and%20Xiuli%20Wang%20and%20Yusi%20Fan%20and%20Kai%20Zhang%20and%20Fengfeng%20Zhou&entry.1292438233=%20%20Recent%20advancements%20in%20feature%20representation%20and%20dimension%20reduction%20have%0Ahighlighted%20their%20crucial%20role%20in%20enhancing%20the%20efficacy%20of%20predictive%0Amodeling.%20This%20work%20introduces%20TemporalPaD%2C%20a%20novel%20end-to-end%20deep%20learning%0Aframework%20designed%20for%20temporal%20pattern%20datasets.%20TemporalPaD%20integrates%0Areinforcement%20learning%20%28RL%29%20with%20neural%20networks%20to%20achieve%20concurrent%20feature%0Arepresentation%20and%20feature%20reduction.%20The%20framework%20consists%20of%20three%0Acooperative%20modules%3A%20a%20Policy%20Module%2C%20a%20Representation%20Module%2C%20and%20a%0AClassification%20Module%2C%20structured%20based%20on%20the%20Actor-Critic%20%28AC%29%20framework.%20The%0APolicy%20Module%2C%20responsible%20for%20dimensionality%20reduction%20through%20RL%2C%20functions%0Aas%20the%20actor%2C%20while%20the%20Representation%20Module%20for%20feature%20extraction%20and%20the%0AClassification%20Module%20collectively%20serve%20as%20the%20critic.%20We%20comprehensively%0Aevaluate%20TemporalPaD%20using%2029%20UCI%20datasets%2C%20a%20well-known%20benchmark%20for%0Avalidating%20feature%20reduction%20algorithms%2C%20through%2010%20independent%20tests%20and%0A10-fold%20cross-validation.%20Additionally%2C%20given%20that%20TemporalPaD%20is%20specifically%0Adesigned%20for%20time%20series%20data%2C%20we%20apply%20it%20to%20a%20real-world%20DNA%20classification%0Aproblem%20involving%20enhancer%20category%20and%20enhancer%20strength.%20The%20results%0Ademonstrate%20that%20TemporalPaD%20is%20an%20efficient%20and%20effective%20framework%20for%0Aachieving%20feature%20reduction%2C%20applicable%20to%20both%20structured%20data%20and%20sequence%0Adatasets.%20The%20source%20code%20of%20the%20proposed%20TemporalPaD%20is%20freely%20available%20as%0Asupplementary%20material%20to%20this%20article%20and%20at%0Ahttp%3A//www.healthinformaticslab.org/supp/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18597v1&entry.124074799=Read"},
{"title": "FracGM: A Fast Fractional Programming Technique for Geman-McClure Robust\n  Estimator", "author": "Bang-Shien Chen and Yu-Kai Lin and Jian-Yu Chen and Chih-Wei Huang and Jann-Long Chern and Ching-Cherng Sun", "abstract": "  Robust estimation is essential in computer vision, robotics, and navigation,\naiming to minimize the impact of outlier measurements for improved accuracy. We\npresent a fast algorithm for Geman-McClure robust estimation, FracGM,\nleveraging fractional programming techniques. This solver reformulates the\noriginal non-convex fractional problem to a convex dual problem and a linear\nequation system, iteratively solving them in an alternating optimization\npattern. Compared to graduated non-convexity approaches, this strategy exhibits\na faster convergence rate and better outlier rejection capability. In addition,\nthe global optimality of the proposed solver can be guaranteed under given\nconditions. We demonstrate the proposed FracGM solver with Wahba's rotation\nproblem and 3-D point-cloud registration along with relaxation pre-processing\nand projection post-processing. Compared to state-of-the-art algorithms, when\nthe outlier rates increase from 20% to 80%, FracGM shows 53% and 88% lower\nrotation and translation increases. In real-world scenarios, FracGM achieves\nbetter results in 13 out of 18 outcomes, while having a 19.43% improvement in\nthe computation time.\n", "link": "http://arxiv.org/abs/2409.13978v2", "date": "2024-09-27", "relevancy": 2.672, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5556}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5319}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FracGM%3A%20A%20Fast%20Fractional%20Programming%20Technique%20for%20Geman-McClure%20Robust%0A%20%20Estimator&body=Title%3A%20FracGM%3A%20A%20Fast%20Fractional%20Programming%20Technique%20for%20Geman-McClure%20Robust%0A%20%20Estimator%0AAuthor%3A%20Bang-Shien%20Chen%20and%20Yu-Kai%20Lin%20and%20Jian-Yu%20Chen%20and%20Chih-Wei%20Huang%20and%20Jann-Long%20Chern%20and%20Ching-Cherng%20Sun%0AAbstract%3A%20%20%20Robust%20estimation%20is%20essential%20in%20computer%20vision%2C%20robotics%2C%20and%20navigation%2C%0Aaiming%20to%20minimize%20the%20impact%20of%20outlier%20measurements%20for%20improved%20accuracy.%20We%0Apresent%20a%20fast%20algorithm%20for%20Geman-McClure%20robust%20estimation%2C%20FracGM%2C%0Aleveraging%20fractional%20programming%20techniques.%20This%20solver%20reformulates%20the%0Aoriginal%20non-convex%20fractional%20problem%20to%20a%20convex%20dual%20problem%20and%20a%20linear%0Aequation%20system%2C%20iteratively%20solving%20them%20in%20an%20alternating%20optimization%0Apattern.%20Compared%20to%20graduated%20non-convexity%20approaches%2C%20this%20strategy%20exhibits%0Aa%20faster%20convergence%20rate%20and%20better%20outlier%20rejection%20capability.%20In%20addition%2C%0Athe%20global%20optimality%20of%20the%20proposed%20solver%20can%20be%20guaranteed%20under%20given%0Aconditions.%20We%20demonstrate%20the%20proposed%20FracGM%20solver%20with%20Wahba%27s%20rotation%0Aproblem%20and%203-D%20point-cloud%20registration%20along%20with%20relaxation%20pre-processing%0Aand%20projection%20post-processing.%20Compared%20to%20state-of-the-art%20algorithms%2C%20when%0Athe%20outlier%20rates%20increase%20from%2020%25%20to%2080%25%2C%20FracGM%20shows%2053%25%20and%2088%25%20lower%0Arotation%20and%20translation%20increases.%20In%20real-world%20scenarios%2C%20FracGM%20achieves%0Abetter%20results%20in%2013%20out%20of%2018%20outcomes%2C%20while%20having%20a%2019.43%25%20improvement%20in%0Athe%20computation%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13978v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFracGM%253A%2520A%2520Fast%2520Fractional%2520Programming%2520Technique%2520for%2520Geman-McClure%2520Robust%250A%2520%2520Estimator%26entry.906535625%3DBang-Shien%2520Chen%2520and%2520Yu-Kai%2520Lin%2520and%2520Jian-Yu%2520Chen%2520and%2520Chih-Wei%2520Huang%2520and%2520Jann-Long%2520Chern%2520and%2520Ching-Cherng%2520Sun%26entry.1292438233%3D%2520%2520Robust%2520estimation%2520is%2520essential%2520in%2520computer%2520vision%252C%2520robotics%252C%2520and%2520navigation%252C%250Aaiming%2520to%2520minimize%2520the%2520impact%2520of%2520outlier%2520measurements%2520for%2520improved%2520accuracy.%2520We%250Apresent%2520a%2520fast%2520algorithm%2520for%2520Geman-McClure%2520robust%2520estimation%252C%2520FracGM%252C%250Aleveraging%2520fractional%2520programming%2520techniques.%2520This%2520solver%2520reformulates%2520the%250Aoriginal%2520non-convex%2520fractional%2520problem%2520to%2520a%2520convex%2520dual%2520problem%2520and%2520a%2520linear%250Aequation%2520system%252C%2520iteratively%2520solving%2520them%2520in%2520an%2520alternating%2520optimization%250Apattern.%2520Compared%2520to%2520graduated%2520non-convexity%2520approaches%252C%2520this%2520strategy%2520exhibits%250Aa%2520faster%2520convergence%2520rate%2520and%2520better%2520outlier%2520rejection%2520capability.%2520In%2520addition%252C%250Athe%2520global%2520optimality%2520of%2520the%2520proposed%2520solver%2520can%2520be%2520guaranteed%2520under%2520given%250Aconditions.%2520We%2520demonstrate%2520the%2520proposed%2520FracGM%2520solver%2520with%2520Wahba%2527s%2520rotation%250Aproblem%2520and%25203-D%2520point-cloud%2520registration%2520along%2520with%2520relaxation%2520pre-processing%250Aand%2520projection%2520post-processing.%2520Compared%2520to%2520state-of-the-art%2520algorithms%252C%2520when%250Athe%2520outlier%2520rates%2520increase%2520from%252020%2525%2520to%252080%2525%252C%2520FracGM%2520shows%252053%2525%2520and%252088%2525%2520lower%250Arotation%2520and%2520translation%2520increases.%2520In%2520real-world%2520scenarios%252C%2520FracGM%2520achieves%250Abetter%2520results%2520in%252013%2520out%2520of%252018%2520outcomes%252C%2520while%2520having%2520a%252019.43%2525%2520improvement%2520in%250Athe%2520computation%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13978v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FracGM%3A%20A%20Fast%20Fractional%20Programming%20Technique%20for%20Geman-McClure%20Robust%0A%20%20Estimator&entry.906535625=Bang-Shien%20Chen%20and%20Yu-Kai%20Lin%20and%20Jian-Yu%20Chen%20and%20Chih-Wei%20Huang%20and%20Jann-Long%20Chern%20and%20Ching-Cherng%20Sun&entry.1292438233=%20%20Robust%20estimation%20is%20essential%20in%20computer%20vision%2C%20robotics%2C%20and%20navigation%2C%0Aaiming%20to%20minimize%20the%20impact%20of%20outlier%20measurements%20for%20improved%20accuracy.%20We%0Apresent%20a%20fast%20algorithm%20for%20Geman-McClure%20robust%20estimation%2C%20FracGM%2C%0Aleveraging%20fractional%20programming%20techniques.%20This%20solver%20reformulates%20the%0Aoriginal%20non-convex%20fractional%20problem%20to%20a%20convex%20dual%20problem%20and%20a%20linear%0Aequation%20system%2C%20iteratively%20solving%20them%20in%20an%20alternating%20optimization%0Apattern.%20Compared%20to%20graduated%20non-convexity%20approaches%2C%20this%20strategy%20exhibits%0Aa%20faster%20convergence%20rate%20and%20better%20outlier%20rejection%20capability.%20In%20addition%2C%0Athe%20global%20optimality%20of%20the%20proposed%20solver%20can%20be%20guaranteed%20under%20given%0Aconditions.%20We%20demonstrate%20the%20proposed%20FracGM%20solver%20with%20Wahba%27s%20rotation%0Aproblem%20and%203-D%20point-cloud%20registration%20along%20with%20relaxation%20pre-processing%0Aand%20projection%20post-processing.%20Compared%20to%20state-of-the-art%20algorithms%2C%20when%0Athe%20outlier%20rates%20increase%20from%2020%25%20to%2080%25%2C%20FracGM%20shows%2053%25%20and%2088%25%20lower%0Arotation%20and%20translation%20increases.%20In%20real-world%20scenarios%2C%20FracGM%20achieves%0Abetter%20results%20in%2013%20out%20of%2018%20outcomes%2C%20while%20having%20a%2019.43%25%20improvement%20in%0Athe%20computation%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13978v2&entry.124074799=Read"},
{"title": "JVID: Joint Video-Image Diffusion for Visual-Quality and\n  Temporal-Consistency in Video Generation", "author": "Hadrien Reynaud and Matthew Baugh and Mischa Dombrowski and Sarah Cechnicka and Qingjie Meng and Bernhard Kainz", "abstract": "  We introduce the Joint Video-Image Diffusion model (JVID), a novel approach\nto generating high-quality and temporally coherent videos. We achieve this by\nintegrating two diffusion models: a Latent Image Diffusion Model (LIDM) trained\non images and a Latent Video Diffusion Model (LVDM) trained on video data. Our\nmethod combines these models in the reverse diffusion process, where the LIDM\nenhances image quality and the LVDM ensures temporal consistency. This unique\ncombination allows us to effectively handle the complex spatio-temporal\ndynamics in video generation. Our results demonstrate quantitative and\nqualitative improvements in producing realistic and coherent videos.\n", "link": "http://arxiv.org/abs/2409.14149v2", "date": "2024-09-27", "relevancy": 2.6421, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6836}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6454}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JVID%3A%20Joint%20Video-Image%20Diffusion%20for%20Visual-Quality%20and%0A%20%20Temporal-Consistency%20in%20Video%20Generation&body=Title%3A%20JVID%3A%20Joint%20Video-Image%20Diffusion%20for%20Visual-Quality%20and%0A%20%20Temporal-Consistency%20in%20Video%20Generation%0AAuthor%3A%20Hadrien%20Reynaud%20and%20Matthew%20Baugh%20and%20Mischa%20Dombrowski%20and%20Sarah%20Cechnicka%20and%20Qingjie%20Meng%20and%20Bernhard%20Kainz%0AAbstract%3A%20%20%20We%20introduce%20the%20Joint%20Video-Image%20Diffusion%20model%20%28JVID%29%2C%20a%20novel%20approach%0Ato%20generating%20high-quality%20and%20temporally%20coherent%20videos.%20We%20achieve%20this%20by%0Aintegrating%20two%20diffusion%20models%3A%20a%20Latent%20Image%20Diffusion%20Model%20%28LIDM%29%20trained%0Aon%20images%20and%20a%20Latent%20Video%20Diffusion%20Model%20%28LVDM%29%20trained%20on%20video%20data.%20Our%0Amethod%20combines%20these%20models%20in%20the%20reverse%20diffusion%20process%2C%20where%20the%20LIDM%0Aenhances%20image%20quality%20and%20the%20LVDM%20ensures%20temporal%20consistency.%20This%20unique%0Acombination%20allows%20us%20to%20effectively%20handle%20the%20complex%20spatio-temporal%0Adynamics%20in%20video%20generation.%20Our%20results%20demonstrate%20quantitative%20and%0Aqualitative%20improvements%20in%20producing%20realistic%20and%20coherent%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14149v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJVID%253A%2520Joint%2520Video-Image%2520Diffusion%2520for%2520Visual-Quality%2520and%250A%2520%2520Temporal-Consistency%2520in%2520Video%2520Generation%26entry.906535625%3DHadrien%2520Reynaud%2520and%2520Matthew%2520Baugh%2520and%2520Mischa%2520Dombrowski%2520and%2520Sarah%2520Cechnicka%2520and%2520Qingjie%2520Meng%2520and%2520Bernhard%2520Kainz%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Joint%2520Video-Image%2520Diffusion%2520model%2520%2528JVID%2529%252C%2520a%2520novel%2520approach%250Ato%2520generating%2520high-quality%2520and%2520temporally%2520coherent%2520videos.%2520We%2520achieve%2520this%2520by%250Aintegrating%2520two%2520diffusion%2520models%253A%2520a%2520Latent%2520Image%2520Diffusion%2520Model%2520%2528LIDM%2529%2520trained%250Aon%2520images%2520and%2520a%2520Latent%2520Video%2520Diffusion%2520Model%2520%2528LVDM%2529%2520trained%2520on%2520video%2520data.%2520Our%250Amethod%2520combines%2520these%2520models%2520in%2520the%2520reverse%2520diffusion%2520process%252C%2520where%2520the%2520LIDM%250Aenhances%2520image%2520quality%2520and%2520the%2520LVDM%2520ensures%2520temporal%2520consistency.%2520This%2520unique%250Acombination%2520allows%2520us%2520to%2520effectively%2520handle%2520the%2520complex%2520spatio-temporal%250Adynamics%2520in%2520video%2520generation.%2520Our%2520results%2520demonstrate%2520quantitative%2520and%250Aqualitative%2520improvements%2520in%2520producing%2520realistic%2520and%2520coherent%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14149v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JVID%3A%20Joint%20Video-Image%20Diffusion%20for%20Visual-Quality%20and%0A%20%20Temporal-Consistency%20in%20Video%20Generation&entry.906535625=Hadrien%20Reynaud%20and%20Matthew%20Baugh%20and%20Mischa%20Dombrowski%20and%20Sarah%20Cechnicka%20and%20Qingjie%20Meng%20and%20Bernhard%20Kainz&entry.1292438233=%20%20We%20introduce%20the%20Joint%20Video-Image%20Diffusion%20model%20%28JVID%29%2C%20a%20novel%20approach%0Ato%20generating%20high-quality%20and%20temporally%20coherent%20videos.%20We%20achieve%20this%20by%0Aintegrating%20two%20diffusion%20models%3A%20a%20Latent%20Image%20Diffusion%20Model%20%28LIDM%29%20trained%0Aon%20images%20and%20a%20Latent%20Video%20Diffusion%20Model%20%28LVDM%29%20trained%20on%20video%20data.%20Our%0Amethod%20combines%20these%20models%20in%20the%20reverse%20diffusion%20process%2C%20where%20the%20LIDM%0Aenhances%20image%20quality%20and%20the%20LVDM%20ensures%20temporal%20consistency.%20This%20unique%0Acombination%20allows%20us%20to%20effectively%20handle%20the%20complex%20spatio-temporal%0Adynamics%20in%20video%20generation.%20Our%20results%20demonstrate%20quantitative%20and%0Aqualitative%20improvements%20in%20producing%20realistic%20and%20coherent%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14149v2&entry.124074799=Read"},
{"title": "Dual-Layer Training and Decoding of Large Language Model with\n  Simultaneously Thinking and Speaking", "author": "Ningyuan Xi and Xiaoyu Wang and Yetao Wu and Teng Chen and Qingqing Gu and Jinxian Qu and Zhonglin Jiang and Yong Chen and Luo Ji", "abstract": "  Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.\n", "link": "http://arxiv.org/abs/2409.12059v2", "date": "2024-09-27", "relevancy": 2.6349, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Layer%20Training%20and%20Decoding%20of%20Large%20Language%20Model%20with%0A%20%20Simultaneously%20Thinking%20and%20Speaking&body=Title%3A%20Dual-Layer%20Training%20and%20Decoding%20of%20Large%20Language%20Model%20with%0A%20%20Simultaneously%20Thinking%20and%20Speaking%0AAuthor%3A%20Ningyuan%20Xi%20and%20Xiaoyu%20Wang%20and%20Yetao%20Wu%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Jinxian%20Qu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji%0AAbstract%3A%20%20%20Large%20Language%20Model%20can%20reasonably%20understand%20and%20generate%20human%20expressions%0Abut%20may%20lack%20of%20thorough%20thinking%20and%20reasoning%20mechanisms.%20Recently%20there%20have%0Abeen%20several%20studies%20which%20enhance%20the%20thinking%20ability%20of%20language%20models%20but%0Amost%20of%20them%20are%20not%20data-driven%20or%20training-based.%20In%20this%20paper%2C%20we%20are%0Amotivated%20by%20the%20cognitive%20mechanism%20in%20the%20natural%20world%2C%20and%20design%20a%20novel%0Amodel%20architecture%20called%20TaS%20which%20allows%20it%20to%20first%20consider%20the%20thoughts%0Aand%20then%20express%20the%20response%20based%20upon%20the%20query.%20We%20design%20several%20pipelines%0Ato%20annotate%20or%20generate%20the%20thought%20contents%20from%20prompt-response%20samples%2C%20then%0Aadd%20language%20heads%20in%20a%20middle%20layer%20which%20behaves%20as%20the%20thinking%20layer.%20We%0Atrain%20the%20language%20model%20by%20the%20thoughts-augmented%20data%20and%20successfully%20let%0Athe%20thinking%20layer%20automatically%20generate%20reasonable%20thoughts%20and%20finally%0Aoutput%20more%20reasonable%20responses.%20Both%20qualitative%20examples%20and%20quantitative%0Aresults%20validate%20the%20effectiveness%20and%20performance%20of%20TaS.%20Our%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/TadE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12059v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Layer%2520Training%2520and%2520Decoding%2520of%2520Large%2520Language%2520Model%2520with%250A%2520%2520Simultaneously%2520Thinking%2520and%2520Speaking%26entry.906535625%3DNingyuan%2520Xi%2520and%2520Xiaoyu%2520Wang%2520and%2520Yetao%2520Wu%2520and%2520Teng%2520Chen%2520and%2520Qingqing%2520Gu%2520and%2520Jinxian%2520Qu%2520and%2520Zhonglin%2520Jiang%2520and%2520Yong%2520Chen%2520and%2520Luo%2520Ji%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520can%2520reasonably%2520understand%2520and%2520generate%2520human%2520expressions%250Abut%2520may%2520lack%2520of%2520thorough%2520thinking%2520and%2520reasoning%2520mechanisms.%2520Recently%2520there%2520have%250Abeen%2520several%2520studies%2520which%2520enhance%2520the%2520thinking%2520ability%2520of%2520language%2520models%2520but%250Amost%2520of%2520them%2520are%2520not%2520data-driven%2520or%2520training-based.%2520In%2520this%2520paper%252C%2520we%2520are%250Amotivated%2520by%2520the%2520cognitive%2520mechanism%2520in%2520the%2520natural%2520world%252C%2520and%2520design%2520a%2520novel%250Amodel%2520architecture%2520called%2520TaS%2520which%2520allows%2520it%2520to%2520first%2520consider%2520the%2520thoughts%250Aand%2520then%2520express%2520the%2520response%2520based%2520upon%2520the%2520query.%2520We%2520design%2520several%2520pipelines%250Ato%2520annotate%2520or%2520generate%2520the%2520thought%2520contents%2520from%2520prompt-response%2520samples%252C%2520then%250Aadd%2520language%2520heads%2520in%2520a%2520middle%2520layer%2520which%2520behaves%2520as%2520the%2520thinking%2520layer.%2520We%250Atrain%2520the%2520language%2520model%2520by%2520the%2520thoughts-augmented%2520data%2520and%2520successfully%2520let%250Athe%2520thinking%2520layer%2520automatically%2520generate%2520reasonable%2520thoughts%2520and%2520finally%250Aoutput%2520more%2520reasonable%2520responses.%2520Both%2520qualitative%2520examples%2520and%2520quantitative%250Aresults%2520validate%2520the%2520effectiveness%2520and%2520performance%2520of%2520TaS.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//anonymous.4open.science/r/TadE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12059v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Layer%20Training%20and%20Decoding%20of%20Large%20Language%20Model%20with%0A%20%20Simultaneously%20Thinking%20and%20Speaking&entry.906535625=Ningyuan%20Xi%20and%20Xiaoyu%20Wang%20and%20Yetao%20Wu%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Jinxian%20Qu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji&entry.1292438233=%20%20Large%20Language%20Model%20can%20reasonably%20understand%20and%20generate%20human%20expressions%0Abut%20may%20lack%20of%20thorough%20thinking%20and%20reasoning%20mechanisms.%20Recently%20there%20have%0Abeen%20several%20studies%20which%20enhance%20the%20thinking%20ability%20of%20language%20models%20but%0Amost%20of%20them%20are%20not%20data-driven%20or%20training-based.%20In%20this%20paper%2C%20we%20are%0Amotivated%20by%20the%20cognitive%20mechanism%20in%20the%20natural%20world%2C%20and%20design%20a%20novel%0Amodel%20architecture%20called%20TaS%20which%20allows%20it%20to%20first%20consider%20the%20thoughts%0Aand%20then%20express%20the%20response%20based%20upon%20the%20query.%20We%20design%20several%20pipelines%0Ato%20annotate%20or%20generate%20the%20thought%20contents%20from%20prompt-response%20samples%2C%20then%0Aadd%20language%20heads%20in%20a%20middle%20layer%20which%20behaves%20as%20the%20thinking%20layer.%20We%0Atrain%20the%20language%20model%20by%20the%20thoughts-augmented%20data%20and%20successfully%20let%0Athe%20thinking%20layer%20automatically%20generate%20reasonable%20thoughts%20and%20finally%0Aoutput%20more%20reasonable%20responses.%20Both%20qualitative%20examples%20and%20quantitative%0Aresults%20validate%20the%20effectiveness%20and%20performance%20of%20TaS.%20Our%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/TadE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12059v2&entry.124074799=Read"},
{"title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models", "author": "Yiming Chen and Xianghu Yue and Xiaoxue Gao and Chen Zhang and Luis Fernando D'Haro and Robby T. Tan and Haizhou Li", "abstract": "  Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.\n", "link": "http://arxiv.org/abs/2409.18680v1", "date": "2024-09-27", "relevancy": 2.5978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Single-Audio%3A%20Advancing%20Multi-Audio%20Processing%20in%20Audio%20Large%0A%20%20Language%20Models&body=Title%3A%20Beyond%20Single-Audio%3A%20Advancing%20Multi-Audio%20Processing%20in%20Audio%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yiming%20Chen%20and%20Xianghu%20Yue%20and%20Xiaoxue%20Gao%20and%20Chen%20Zhang%20and%20Luis%20Fernando%20D%27Haro%20and%20Robby%20T.%20Tan%20and%20Haizhou%20Li%0AAbstract%3A%20%20%20Various%20audio-LLMs%20%28ALLMs%29%20have%20been%20explored%20recently%20for%20tackling%20different%0Aaudio%20tasks%20simultaneously%20using%20a%20single%2C%20unified%20model.%20While%20existing%0Aevaluations%20of%20ALLMs%20primarily%20focus%20on%20single-audio%20tasks%2C%20real-world%0Aapplications%20often%20involve%20processing%20multiple%20audio%20streams%20simultaneously.%20To%0Abridge%20this%20gap%2C%20we%20propose%20the%20first%20multi-audio%20evaluation%20%28MAE%29%20benchmark%0Athat%20consists%20of%2020%20datasets%20from%2011%20multi-audio%20tasks%20encompassing%20both%20speech%0Aand%20sound%20scenarios.%20Comprehensive%20experiments%20on%20MAE%20demonstrate%20that%20the%0Aexisting%20ALLMs%2C%20while%20being%20powerful%20in%20comprehending%20primary%20audio%20elements%20in%0Aindividual%20audio%20inputs%2C%20struggling%20to%20handle%20multi-audio%20scenarios.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20multi-audio-LLM%20%28MALLM%29%20to%20capture%20audio%20context%20among%0Amultiple%20similar%20audios%20using%20discriminative%20learning%20on%20our%20proposed%20synthetic%0Adata.%20The%20results%20demonstrate%20that%20the%20proposed%20MALLM%20outperforms%20all%20baselines%0Aand%20achieves%20high%20data%20efficiency%20using%20synthetic%20data%20without%20requiring%20human%0Aannotations.%20The%20proposed%20MALLM%20opens%20the%20door%20for%20ALLMs%20towards%20multi-audio%0Aprocessing%20era%20and%20brings%20us%20closer%20to%20replicating%20human%20auditory%20capabilities%0Ain%20machines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Single-Audio%253A%2520Advancing%2520Multi-Audio%2520Processing%2520in%2520Audio%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYiming%2520Chen%2520and%2520Xianghu%2520Yue%2520and%2520Xiaoxue%2520Gao%2520and%2520Chen%2520Zhang%2520and%2520Luis%2520Fernando%2520D%2527Haro%2520and%2520Robby%2520T.%2520Tan%2520and%2520Haizhou%2520Li%26entry.1292438233%3D%2520%2520Various%2520audio-LLMs%2520%2528ALLMs%2529%2520have%2520been%2520explored%2520recently%2520for%2520tackling%2520different%250Aaudio%2520tasks%2520simultaneously%2520using%2520a%2520single%252C%2520unified%2520model.%2520While%2520existing%250Aevaluations%2520of%2520ALLMs%2520primarily%2520focus%2520on%2520single-audio%2520tasks%252C%2520real-world%250Aapplications%2520often%2520involve%2520processing%2520multiple%2520audio%2520streams%2520simultaneously.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520propose%2520the%2520first%2520multi-audio%2520evaluation%2520%2528MAE%2529%2520benchmark%250Athat%2520consists%2520of%252020%2520datasets%2520from%252011%2520multi-audio%2520tasks%2520encompassing%2520both%2520speech%250Aand%2520sound%2520scenarios.%2520Comprehensive%2520experiments%2520on%2520MAE%2520demonstrate%2520that%2520the%250Aexisting%2520ALLMs%252C%2520while%2520being%2520powerful%2520in%2520comprehending%2520primary%2520audio%2520elements%2520in%250Aindividual%2520audio%2520inputs%252C%2520struggling%2520to%2520handle%2520multi-audio%2520scenarios.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520novel%2520multi-audio-LLM%2520%2528MALLM%2529%2520to%2520capture%2520audio%2520context%2520among%250Amultiple%2520similar%2520audios%2520using%2520discriminative%2520learning%2520on%2520our%2520proposed%2520synthetic%250Adata.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520MALLM%2520outperforms%2520all%2520baselines%250Aand%2520achieves%2520high%2520data%2520efficiency%2520using%2520synthetic%2520data%2520without%2520requiring%2520human%250Aannotations.%2520The%2520proposed%2520MALLM%2520opens%2520the%2520door%2520for%2520ALLMs%2520towards%2520multi-audio%250Aprocessing%2520era%2520and%2520brings%2520us%2520closer%2520to%2520replicating%2520human%2520auditory%2520capabilities%250Ain%2520machines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Single-Audio%3A%20Advancing%20Multi-Audio%20Processing%20in%20Audio%20Large%0A%20%20Language%20Models&entry.906535625=Yiming%20Chen%20and%20Xianghu%20Yue%20and%20Xiaoxue%20Gao%20and%20Chen%20Zhang%20and%20Luis%20Fernando%20D%27Haro%20and%20Robby%20T.%20Tan%20and%20Haizhou%20Li&entry.1292438233=%20%20Various%20audio-LLMs%20%28ALLMs%29%20have%20been%20explored%20recently%20for%20tackling%20different%0Aaudio%20tasks%20simultaneously%20using%20a%20single%2C%20unified%20model.%20While%20existing%0Aevaluations%20of%20ALLMs%20primarily%20focus%20on%20single-audio%20tasks%2C%20real-world%0Aapplications%20often%20involve%20processing%20multiple%20audio%20streams%20simultaneously.%20To%0Abridge%20this%20gap%2C%20we%20propose%20the%20first%20multi-audio%20evaluation%20%28MAE%29%20benchmark%0Athat%20consists%20of%2020%20datasets%20from%2011%20multi-audio%20tasks%20encompassing%20both%20speech%0Aand%20sound%20scenarios.%20Comprehensive%20experiments%20on%20MAE%20demonstrate%20that%20the%0Aexisting%20ALLMs%2C%20while%20being%20powerful%20in%20comprehending%20primary%20audio%20elements%20in%0Aindividual%20audio%20inputs%2C%20struggling%20to%20handle%20multi-audio%20scenarios.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20multi-audio-LLM%20%28MALLM%29%20to%20capture%20audio%20context%20among%0Amultiple%20similar%20audios%20using%20discriminative%20learning%20on%20our%20proposed%20synthetic%0Adata.%20The%20results%20demonstrate%20that%20the%20proposed%20MALLM%20outperforms%20all%20baselines%0Aand%20achieves%20high%20data%20efficiency%20using%20synthetic%20data%20without%20requiring%20human%0Aannotations.%20The%20proposed%20MALLM%20opens%20the%20door%20for%20ALLMs%20towards%20multi-audio%0Aprocessing%20era%20and%20brings%20us%20closer%20to%20replicating%20human%20auditory%20capabilities%0Ain%20machines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18680v1&entry.124074799=Read"},
{"title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models", "author": "Yu Zhou and Xingyu Wu and Jibin Wu and Liang Feng and Kay Chen Tan", "abstract": "  Model merging is a technique that combines multiple large pretrained models\ninto a single model with enhanced performance and broader task adaptability. It\nhas gained popularity in large pretrained model development due to its ability\nto bypass the need for original training data and further training processes.\nHowever, most existing model merging approaches focus solely on exploring the\nparameter space, merging models with identical architectures. Merging within\nthe architecture space, despite its potential, remains in its early stages due\nto the vast search space and the challenges of layer compatibility. This paper\nmarks a significant advance toward more flexible and comprehensive model\nmerging techniques by modeling the architecture-space merging process as a\nreinforcement learning task. We train policy and value networks using offline\nsampling of weight vectors, which are then employed for the online optimization\nof merging strategies. Moreover, a multi-objective optimization paradigm is\nintroduced to accommodate users' diverse task preferences, learning the Pareto\nfront of optimal models to offer customized merging suggestions. Experimental\nresults across multiple tasks, including text translation, mathematical\nreasoning, and code generation, validate the effectiveness and superiority of\nthe proposed framework in model merging. The code will be made publicly\navailable after the review process.\n", "link": "http://arxiv.org/abs/2409.18893v1", "date": "2024-09-27", "relevancy": 2.5798, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HM3%3A%20Hierarchical%20Multi-Objective%20Model%20Merging%20for%20Pretrained%20Models&body=Title%3A%20HM3%3A%20Hierarchical%20Multi-Objective%20Model%20Merging%20for%20Pretrained%20Models%0AAuthor%3A%20Yu%20Zhou%20and%20Xingyu%20Wu%20and%20Jibin%20Wu%20and%20Liang%20Feng%20and%20Kay%20Chen%20Tan%0AAbstract%3A%20%20%20Model%20merging%20is%20a%20technique%20that%20combines%20multiple%20large%20pretrained%20models%0Ainto%20a%20single%20model%20with%20enhanced%20performance%20and%20broader%20task%20adaptability.%20It%0Ahas%20gained%20popularity%20in%20large%20pretrained%20model%20development%20due%20to%20its%20ability%0Ato%20bypass%20the%20need%20for%20original%20training%20data%20and%20further%20training%20processes.%0AHowever%2C%20most%20existing%20model%20merging%20approaches%20focus%20solely%20on%20exploring%20the%0Aparameter%20space%2C%20merging%20models%20with%20identical%20architectures.%20Merging%20within%0Athe%20architecture%20space%2C%20despite%20its%20potential%2C%20remains%20in%20its%20early%20stages%20due%0Ato%20the%20vast%20search%20space%20and%20the%20challenges%20of%20layer%20compatibility.%20This%20paper%0Amarks%20a%20significant%20advance%20toward%20more%20flexible%20and%20comprehensive%20model%0Amerging%20techniques%20by%20modeling%20the%20architecture-space%20merging%20process%20as%20a%0Areinforcement%20learning%20task.%20We%20train%20policy%20and%20value%20networks%20using%20offline%0Asampling%20of%20weight%20vectors%2C%20which%20are%20then%20employed%20for%20the%20online%20optimization%0Aof%20merging%20strategies.%20Moreover%2C%20a%20multi-objective%20optimization%20paradigm%20is%0Aintroduced%20to%20accommodate%20users%27%20diverse%20task%20preferences%2C%20learning%20the%20Pareto%0Afront%20of%20optimal%20models%20to%20offer%20customized%20merging%20suggestions.%20Experimental%0Aresults%20across%20multiple%20tasks%2C%20including%20text%20translation%2C%20mathematical%0Areasoning%2C%20and%20code%20generation%2C%20validate%20the%20effectiveness%20and%20superiority%20of%0Athe%20proposed%20framework%20in%20model%20merging.%20The%20code%20will%20be%20made%20publicly%0Aavailable%20after%20the%20review%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHM3%253A%2520Hierarchical%2520Multi-Objective%2520Model%2520Merging%2520for%2520Pretrained%2520Models%26entry.906535625%3DYu%2520Zhou%2520and%2520Xingyu%2520Wu%2520and%2520Jibin%2520Wu%2520and%2520Liang%2520Feng%2520and%2520Kay%2520Chen%2520Tan%26entry.1292438233%3D%2520%2520Model%2520merging%2520is%2520a%2520technique%2520that%2520combines%2520multiple%2520large%2520pretrained%2520models%250Ainto%2520a%2520single%2520model%2520with%2520enhanced%2520performance%2520and%2520broader%2520task%2520adaptability.%2520It%250Ahas%2520gained%2520popularity%2520in%2520large%2520pretrained%2520model%2520development%2520due%2520to%2520its%2520ability%250Ato%2520bypass%2520the%2520need%2520for%2520original%2520training%2520data%2520and%2520further%2520training%2520processes.%250AHowever%252C%2520most%2520existing%2520model%2520merging%2520approaches%2520focus%2520solely%2520on%2520exploring%2520the%250Aparameter%2520space%252C%2520merging%2520models%2520with%2520identical%2520architectures.%2520Merging%2520within%250Athe%2520architecture%2520space%252C%2520despite%2520its%2520potential%252C%2520remains%2520in%2520its%2520early%2520stages%2520due%250Ato%2520the%2520vast%2520search%2520space%2520and%2520the%2520challenges%2520of%2520layer%2520compatibility.%2520This%2520paper%250Amarks%2520a%2520significant%2520advance%2520toward%2520more%2520flexible%2520and%2520comprehensive%2520model%250Amerging%2520techniques%2520by%2520modeling%2520the%2520architecture-space%2520merging%2520process%2520as%2520a%250Areinforcement%2520learning%2520task.%2520We%2520train%2520policy%2520and%2520value%2520networks%2520using%2520offline%250Asampling%2520of%2520weight%2520vectors%252C%2520which%2520are%2520then%2520employed%2520for%2520the%2520online%2520optimization%250Aof%2520merging%2520strategies.%2520Moreover%252C%2520a%2520multi-objective%2520optimization%2520paradigm%2520is%250Aintroduced%2520to%2520accommodate%2520users%2527%2520diverse%2520task%2520preferences%252C%2520learning%2520the%2520Pareto%250Afront%2520of%2520optimal%2520models%2520to%2520offer%2520customized%2520merging%2520suggestions.%2520Experimental%250Aresults%2520across%2520multiple%2520tasks%252C%2520including%2520text%2520translation%252C%2520mathematical%250Areasoning%252C%2520and%2520code%2520generation%252C%2520validate%2520the%2520effectiveness%2520and%2520superiority%2520of%250Athe%2520proposed%2520framework%2520in%2520model%2520merging.%2520The%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520after%2520the%2520review%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HM3%3A%20Hierarchical%20Multi-Objective%20Model%20Merging%20for%20Pretrained%20Models&entry.906535625=Yu%20Zhou%20and%20Xingyu%20Wu%20and%20Jibin%20Wu%20and%20Liang%20Feng%20and%20Kay%20Chen%20Tan&entry.1292438233=%20%20Model%20merging%20is%20a%20technique%20that%20combines%20multiple%20large%20pretrained%20models%0Ainto%20a%20single%20model%20with%20enhanced%20performance%20and%20broader%20task%20adaptability.%20It%0Ahas%20gained%20popularity%20in%20large%20pretrained%20model%20development%20due%20to%20its%20ability%0Ato%20bypass%20the%20need%20for%20original%20training%20data%20and%20further%20training%20processes.%0AHowever%2C%20most%20existing%20model%20merging%20approaches%20focus%20solely%20on%20exploring%20the%0Aparameter%20space%2C%20merging%20models%20with%20identical%20architectures.%20Merging%20within%0Athe%20architecture%20space%2C%20despite%20its%20potential%2C%20remains%20in%20its%20early%20stages%20due%0Ato%20the%20vast%20search%20space%20and%20the%20challenges%20of%20layer%20compatibility.%20This%20paper%0Amarks%20a%20significant%20advance%20toward%20more%20flexible%20and%20comprehensive%20model%0Amerging%20techniques%20by%20modeling%20the%20architecture-space%20merging%20process%20as%20a%0Areinforcement%20learning%20task.%20We%20train%20policy%20and%20value%20networks%20using%20offline%0Asampling%20of%20weight%20vectors%2C%20which%20are%20then%20employed%20for%20the%20online%20optimization%0Aof%20merging%20strategies.%20Moreover%2C%20a%20multi-objective%20optimization%20paradigm%20is%0Aintroduced%20to%20accommodate%20users%27%20diverse%20task%20preferences%2C%20learning%20the%20Pareto%0Afront%20of%20optimal%20models%20to%20offer%20customized%20merging%20suggestions.%20Experimental%0Aresults%20across%20multiple%20tasks%2C%20including%20text%20translation%2C%20mathematical%0Areasoning%2C%20and%20code%20generation%2C%20validate%20the%20effectiveness%20and%20superiority%20of%0Athe%20proposed%20framework%20in%20model%20merging.%20The%20code%20will%20be%20made%20publicly%0Aavailable%20after%20the%20review%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18893v1&entry.124074799=Read"},
{"title": "Understanding the Benefits of SimCLR Pre-Training in Two-Layer\n  Convolutional Neural Networks", "author": "Han Zhang and Yuan Cao", "abstract": "  SimCLR is one of the most popular contrastive learning methods for vision\ntasks. It pre-trains deep neural networks based on a large amount of unlabeled\ndata by teaching the model to distinguish between positive and negative pairs\nof augmented images. It is believed that SimCLR can pre-train a deep neural\nnetwork to learn efficient representations that can lead to a better\nperformance of future supervised fine-tuning. Despite its effectiveness, our\ntheoretical understanding of the underlying mechanisms of SimCLR is still\nlimited. In this paper, we theoretically introduce a case study of the SimCLR\nmethod. Specifically, we consider training a two-layer convolutional neural\nnetwork (CNN) to learn a toy image data model. We show that, under certain\nconditions on the number of labeled data, SimCLR pre-training combined with\nsupervised fine-tuning achieves almost optimal test loss. Notably, the label\ncomplexity for SimCLR pre-training is far less demanding compared to direct\ntraining on supervised data. Our analysis sheds light on the benefits of SimCLR\nin learning with fewer labels.\n", "link": "http://arxiv.org/abs/2409.18685v1", "date": "2024-09-27", "relevancy": 2.5474, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Benefits%20of%20SimCLR%20Pre-Training%20in%20Two-Layer%0A%20%20Convolutional%20Neural%20Networks&body=Title%3A%20Understanding%20the%20Benefits%20of%20SimCLR%20Pre-Training%20in%20Two-Layer%0A%20%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Han%20Zhang%20and%20Yuan%20Cao%0AAbstract%3A%20%20%20SimCLR%20is%20one%20of%20the%20most%20popular%20contrastive%20learning%20methods%20for%20vision%0Atasks.%20It%20pre-trains%20deep%20neural%20networks%20based%20on%20a%20large%20amount%20of%20unlabeled%0Adata%20by%20teaching%20the%20model%20to%20distinguish%20between%20positive%20and%20negative%20pairs%0Aof%20augmented%20images.%20It%20is%20believed%20that%20SimCLR%20can%20pre-train%20a%20deep%20neural%0Anetwork%20to%20learn%20efficient%20representations%20that%20can%20lead%20to%20a%20better%0Aperformance%20of%20future%20supervised%20fine-tuning.%20Despite%20its%20effectiveness%2C%20our%0Atheoretical%20understanding%20of%20the%20underlying%20mechanisms%20of%20SimCLR%20is%20still%0Alimited.%20In%20this%20paper%2C%20we%20theoretically%20introduce%20a%20case%20study%20of%20the%20SimCLR%0Amethod.%20Specifically%2C%20we%20consider%20training%20a%20two-layer%20convolutional%20neural%0Anetwork%20%28CNN%29%20to%20learn%20a%20toy%20image%20data%20model.%20We%20show%20that%2C%20under%20certain%0Aconditions%20on%20the%20number%20of%20labeled%20data%2C%20SimCLR%20pre-training%20combined%20with%0Asupervised%20fine-tuning%20achieves%20almost%20optimal%20test%20loss.%20Notably%2C%20the%20label%0Acomplexity%20for%20SimCLR%20pre-training%20is%20far%20less%20demanding%20compared%20to%20direct%0Atraining%20on%20supervised%20data.%20Our%20analysis%20sheds%20light%20on%20the%20benefits%20of%20SimCLR%0Ain%20learning%20with%20fewer%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520the%2520Benefits%2520of%2520SimCLR%2520Pre-Training%2520in%2520Two-Layer%250A%2520%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DHan%2520Zhang%2520and%2520Yuan%2520Cao%26entry.1292438233%3D%2520%2520SimCLR%2520is%2520one%2520of%2520the%2520most%2520popular%2520contrastive%2520learning%2520methods%2520for%2520vision%250Atasks.%2520It%2520pre-trains%2520deep%2520neural%2520networks%2520based%2520on%2520a%2520large%2520amount%2520of%2520unlabeled%250Adata%2520by%2520teaching%2520the%2520model%2520to%2520distinguish%2520between%2520positive%2520and%2520negative%2520pairs%250Aof%2520augmented%2520images.%2520It%2520is%2520believed%2520that%2520SimCLR%2520can%2520pre-train%2520a%2520deep%2520neural%250Anetwork%2520to%2520learn%2520efficient%2520representations%2520that%2520can%2520lead%2520to%2520a%2520better%250Aperformance%2520of%2520future%2520supervised%2520fine-tuning.%2520Despite%2520its%2520effectiveness%252C%2520our%250Atheoretical%2520understanding%2520of%2520the%2520underlying%2520mechanisms%2520of%2520SimCLR%2520is%2520still%250Alimited.%2520In%2520this%2520paper%252C%2520we%2520theoretically%2520introduce%2520a%2520case%2520study%2520of%2520the%2520SimCLR%250Amethod.%2520Specifically%252C%2520we%2520consider%2520training%2520a%2520two-layer%2520convolutional%2520neural%250Anetwork%2520%2528CNN%2529%2520to%2520learn%2520a%2520toy%2520image%2520data%2520model.%2520We%2520show%2520that%252C%2520under%2520certain%250Aconditions%2520on%2520the%2520number%2520of%2520labeled%2520data%252C%2520SimCLR%2520pre-training%2520combined%2520with%250Asupervised%2520fine-tuning%2520achieves%2520almost%2520optimal%2520test%2520loss.%2520Notably%252C%2520the%2520label%250Acomplexity%2520for%2520SimCLR%2520pre-training%2520is%2520far%2520less%2520demanding%2520compared%2520to%2520direct%250Atraining%2520on%2520supervised%2520data.%2520Our%2520analysis%2520sheds%2520light%2520on%2520the%2520benefits%2520of%2520SimCLR%250Ain%2520learning%2520with%2520fewer%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Benefits%20of%20SimCLR%20Pre-Training%20in%20Two-Layer%0A%20%20Convolutional%20Neural%20Networks&entry.906535625=Han%20Zhang%20and%20Yuan%20Cao&entry.1292438233=%20%20SimCLR%20is%20one%20of%20the%20most%20popular%20contrastive%20learning%20methods%20for%20vision%0Atasks.%20It%20pre-trains%20deep%20neural%20networks%20based%20on%20a%20large%20amount%20of%20unlabeled%0Adata%20by%20teaching%20the%20model%20to%20distinguish%20between%20positive%20and%20negative%20pairs%0Aof%20augmented%20images.%20It%20is%20believed%20that%20SimCLR%20can%20pre-train%20a%20deep%20neural%0Anetwork%20to%20learn%20efficient%20representations%20that%20can%20lead%20to%20a%20better%0Aperformance%20of%20future%20supervised%20fine-tuning.%20Despite%20its%20effectiveness%2C%20our%0Atheoretical%20understanding%20of%20the%20underlying%20mechanisms%20of%20SimCLR%20is%20still%0Alimited.%20In%20this%20paper%2C%20we%20theoretically%20introduce%20a%20case%20study%20of%20the%20SimCLR%0Amethod.%20Specifically%2C%20we%20consider%20training%20a%20two-layer%20convolutional%20neural%0Anetwork%20%28CNN%29%20to%20learn%20a%20toy%20image%20data%20model.%20We%20show%20that%2C%20under%20certain%0Aconditions%20on%20the%20number%20of%20labeled%20data%2C%20SimCLR%20pre-training%20combined%20with%0Asupervised%20fine-tuning%20achieves%20almost%20optimal%20test%20loss.%20Notably%2C%20the%20label%0Acomplexity%20for%20SimCLR%20pre-training%20is%20far%20less%20demanding%20compared%20to%20direct%0Atraining%20on%20supervised%20data.%20Our%20analysis%20sheds%20light%20on%20the%20benefits%20of%20SimCLR%0Ain%20learning%20with%20fewer%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18685v1&entry.124074799=Read"},
{"title": "EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event\n  Slicing", "author": "Argha Sen and Nuwan Bandara and Ila Gokarn and Thivya Kandappu and Archan Misra", "abstract": "  Eye-tracking technology has gained significant attention in recent years due\nto its wide range of applications in human-computer interaction, virtual and\naugmented reality, and wearable health. Traditional RGB camera-based\neye-tracking systems often struggle with poor temporal resolution and\ncomputational constraints, limiting their effectiveness in capturing rapid eye\nmovements. To address these limitations, we propose EyeTrAES, a novel approach\nusing neuromorphic event cameras for high-fidelity tracking of natural\npupillary movement that shows significant kinematic variance. One of EyeTrAES's\nhighlights is the use of a novel adaptive windowing/slicing algorithm that\nensures just the right amount of descriptive asynchronous event data\naccumulation within an event frame, across a wide range of eye movement\npatterns. EyeTrAES then applies lightweight image processing functions over\naccumulated event frames from just a single eye to perform pupil segmentation\nand tracking. We show that these methods boost pupil tracking fidelity by 6+%,\nachieving IoU~=92%, while incurring at least 3x lower latency than competing\npure event-based eye tracking alternatives [38]. We additionally demonstrate\nthat the microscopic pupillary motion captured by EyeTrAES exhibits distinctive\nvariations across individuals and can thus serve as a biometric fingerprint.\nFor robust user authentication, we train a lightweight per-user Random Forest\nclassifier using a novel feature vector of short-term pupillary kinematics,\ncomprising a sliding window of pupil (location, velocity, acceleration)\ntriples. Experimental studies with two different datasets demonstrate that the\nEyeTrAES-based authentication technique can simultaneously achieve high\nauthentication accuracy (~=0.82) and low processing latency (~=12ms), and\nsignificantly outperform multiple state-of-the-art competitive baselines.\n", "link": "http://arxiv.org/abs/2409.18813v1", "date": "2024-09-27", "relevancy": 2.5467, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5371}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4977}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EyeTrAES%3A%20Fine-grained%2C%20Low-Latency%20Eye%20Tracking%20via%20Adaptive%20Event%0A%20%20Slicing&body=Title%3A%20EyeTrAES%3A%20Fine-grained%2C%20Low-Latency%20Eye%20Tracking%20via%20Adaptive%20Event%0A%20%20Slicing%0AAuthor%3A%20Argha%20Sen%20and%20Nuwan%20Bandara%20and%20Ila%20Gokarn%20and%20Thivya%20Kandappu%20and%20Archan%20Misra%0AAbstract%3A%20%20%20Eye-tracking%20technology%20has%20gained%20significant%20attention%20in%20recent%20years%20due%0Ato%20its%20wide%20range%20of%20applications%20in%20human-computer%20interaction%2C%20virtual%20and%0Aaugmented%20reality%2C%20and%20wearable%20health.%20Traditional%20RGB%20camera-based%0Aeye-tracking%20systems%20often%20struggle%20with%20poor%20temporal%20resolution%20and%0Acomputational%20constraints%2C%20limiting%20their%20effectiveness%20in%20capturing%20rapid%20eye%0Amovements.%20To%20address%20these%20limitations%2C%20we%20propose%20EyeTrAES%2C%20a%20novel%20approach%0Ausing%20neuromorphic%20event%20cameras%20for%20high-fidelity%20tracking%20of%20natural%0Apupillary%20movement%20that%20shows%20significant%20kinematic%20variance.%20One%20of%20EyeTrAES%27s%0Ahighlights%20is%20the%20use%20of%20a%20novel%20adaptive%20windowing/slicing%20algorithm%20that%0Aensures%20just%20the%20right%20amount%20of%20descriptive%20asynchronous%20event%20data%0Aaccumulation%20within%20an%20event%20frame%2C%20across%20a%20wide%20range%20of%20eye%20movement%0Apatterns.%20EyeTrAES%20then%20applies%20lightweight%20image%20processing%20functions%20over%0Aaccumulated%20event%20frames%20from%20just%20a%20single%20eye%20to%20perform%20pupil%20segmentation%0Aand%20tracking.%20We%20show%20that%20these%20methods%20boost%20pupil%20tracking%20fidelity%20by%206%2B%25%2C%0Aachieving%20IoU~%3D92%25%2C%20while%20incurring%20at%20least%203x%20lower%20latency%20than%20competing%0Apure%20event-based%20eye%20tracking%20alternatives%20%5B38%5D.%20We%20additionally%20demonstrate%0Athat%20the%20microscopic%20pupillary%20motion%20captured%20by%20EyeTrAES%20exhibits%20distinctive%0Avariations%20across%20individuals%20and%20can%20thus%20serve%20as%20a%20biometric%20fingerprint.%0AFor%20robust%20user%20authentication%2C%20we%20train%20a%20lightweight%20per-user%20Random%20Forest%0Aclassifier%20using%20a%20novel%20feature%20vector%20of%20short-term%20pupillary%20kinematics%2C%0Acomprising%20a%20sliding%20window%20of%20pupil%20%28location%2C%20velocity%2C%20acceleration%29%0Atriples.%20Experimental%20studies%20with%20two%20different%20datasets%20demonstrate%20that%20the%0AEyeTrAES-based%20authentication%20technique%20can%20simultaneously%20achieve%20high%0Aauthentication%20accuracy%20%28~%3D0.82%29%20and%20low%20processing%20latency%20%28~%3D12ms%29%2C%20and%0Asignificantly%20outperform%20multiple%20state-of-the-art%20competitive%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyeTrAES%253A%2520Fine-grained%252C%2520Low-Latency%2520Eye%2520Tracking%2520via%2520Adaptive%2520Event%250A%2520%2520Slicing%26entry.906535625%3DArgha%2520Sen%2520and%2520Nuwan%2520Bandara%2520and%2520Ila%2520Gokarn%2520and%2520Thivya%2520Kandappu%2520and%2520Archan%2520Misra%26entry.1292438233%3D%2520%2520Eye-tracking%2520technology%2520has%2520gained%2520significant%2520attention%2520in%2520recent%2520years%2520due%250Ato%2520its%2520wide%2520range%2520of%2520applications%2520in%2520human-computer%2520interaction%252C%2520virtual%2520and%250Aaugmented%2520reality%252C%2520and%2520wearable%2520health.%2520Traditional%2520RGB%2520camera-based%250Aeye-tracking%2520systems%2520often%2520struggle%2520with%2520poor%2520temporal%2520resolution%2520and%250Acomputational%2520constraints%252C%2520limiting%2520their%2520effectiveness%2520in%2520capturing%2520rapid%2520eye%250Amovements.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520EyeTrAES%252C%2520a%2520novel%2520approach%250Ausing%2520neuromorphic%2520event%2520cameras%2520for%2520high-fidelity%2520tracking%2520of%2520natural%250Apupillary%2520movement%2520that%2520shows%2520significant%2520kinematic%2520variance.%2520One%2520of%2520EyeTrAES%2527s%250Ahighlights%2520is%2520the%2520use%2520of%2520a%2520novel%2520adaptive%2520windowing/slicing%2520algorithm%2520that%250Aensures%2520just%2520the%2520right%2520amount%2520of%2520descriptive%2520asynchronous%2520event%2520data%250Aaccumulation%2520within%2520an%2520event%2520frame%252C%2520across%2520a%2520wide%2520range%2520of%2520eye%2520movement%250Apatterns.%2520EyeTrAES%2520then%2520applies%2520lightweight%2520image%2520processing%2520functions%2520over%250Aaccumulated%2520event%2520frames%2520from%2520just%2520a%2520single%2520eye%2520to%2520perform%2520pupil%2520segmentation%250Aand%2520tracking.%2520We%2520show%2520that%2520these%2520methods%2520boost%2520pupil%2520tracking%2520fidelity%2520by%25206%252B%2525%252C%250Aachieving%2520IoU~%253D92%2525%252C%2520while%2520incurring%2520at%2520least%25203x%2520lower%2520latency%2520than%2520competing%250Apure%2520event-based%2520eye%2520tracking%2520alternatives%2520%255B38%255D.%2520We%2520additionally%2520demonstrate%250Athat%2520the%2520microscopic%2520pupillary%2520motion%2520captured%2520by%2520EyeTrAES%2520exhibits%2520distinctive%250Avariations%2520across%2520individuals%2520and%2520can%2520thus%2520serve%2520as%2520a%2520biometric%2520fingerprint.%250AFor%2520robust%2520user%2520authentication%252C%2520we%2520train%2520a%2520lightweight%2520per-user%2520Random%2520Forest%250Aclassifier%2520using%2520a%2520novel%2520feature%2520vector%2520of%2520short-term%2520pupillary%2520kinematics%252C%250Acomprising%2520a%2520sliding%2520window%2520of%2520pupil%2520%2528location%252C%2520velocity%252C%2520acceleration%2529%250Atriples.%2520Experimental%2520studies%2520with%2520two%2520different%2520datasets%2520demonstrate%2520that%2520the%250AEyeTrAES-based%2520authentication%2520technique%2520can%2520simultaneously%2520achieve%2520high%250Aauthentication%2520accuracy%2520%2528~%253D0.82%2529%2520and%2520low%2520processing%2520latency%2520%2528~%253D12ms%2529%252C%2520and%250Asignificantly%2520outperform%2520multiple%2520state-of-the-art%2520competitive%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EyeTrAES%3A%20Fine-grained%2C%20Low-Latency%20Eye%20Tracking%20via%20Adaptive%20Event%0A%20%20Slicing&entry.906535625=Argha%20Sen%20and%20Nuwan%20Bandara%20and%20Ila%20Gokarn%20and%20Thivya%20Kandappu%20and%20Archan%20Misra&entry.1292438233=%20%20Eye-tracking%20technology%20has%20gained%20significant%20attention%20in%20recent%20years%20due%0Ato%20its%20wide%20range%20of%20applications%20in%20human-computer%20interaction%2C%20virtual%20and%0Aaugmented%20reality%2C%20and%20wearable%20health.%20Traditional%20RGB%20camera-based%0Aeye-tracking%20systems%20often%20struggle%20with%20poor%20temporal%20resolution%20and%0Acomputational%20constraints%2C%20limiting%20their%20effectiveness%20in%20capturing%20rapid%20eye%0Amovements.%20To%20address%20these%20limitations%2C%20we%20propose%20EyeTrAES%2C%20a%20novel%20approach%0Ausing%20neuromorphic%20event%20cameras%20for%20high-fidelity%20tracking%20of%20natural%0Apupillary%20movement%20that%20shows%20significant%20kinematic%20variance.%20One%20of%20EyeTrAES%27s%0Ahighlights%20is%20the%20use%20of%20a%20novel%20adaptive%20windowing/slicing%20algorithm%20that%0Aensures%20just%20the%20right%20amount%20of%20descriptive%20asynchronous%20event%20data%0Aaccumulation%20within%20an%20event%20frame%2C%20across%20a%20wide%20range%20of%20eye%20movement%0Apatterns.%20EyeTrAES%20then%20applies%20lightweight%20image%20processing%20functions%20over%0Aaccumulated%20event%20frames%20from%20just%20a%20single%20eye%20to%20perform%20pupil%20segmentation%0Aand%20tracking.%20We%20show%20that%20these%20methods%20boost%20pupil%20tracking%20fidelity%20by%206%2B%25%2C%0Aachieving%20IoU~%3D92%25%2C%20while%20incurring%20at%20least%203x%20lower%20latency%20than%20competing%0Apure%20event-based%20eye%20tracking%20alternatives%20%5B38%5D.%20We%20additionally%20demonstrate%0Athat%20the%20microscopic%20pupillary%20motion%20captured%20by%20EyeTrAES%20exhibits%20distinctive%0Avariations%20across%20individuals%20and%20can%20thus%20serve%20as%20a%20biometric%20fingerprint.%0AFor%20robust%20user%20authentication%2C%20we%20train%20a%20lightweight%20per-user%20Random%20Forest%0Aclassifier%20using%20a%20novel%20feature%20vector%20of%20short-term%20pupillary%20kinematics%2C%0Acomprising%20a%20sliding%20window%20of%20pupil%20%28location%2C%20velocity%2C%20acceleration%29%0Atriples.%20Experimental%20studies%20with%20two%20different%20datasets%20demonstrate%20that%20the%0AEyeTrAES-based%20authentication%20technique%20can%20simultaneously%20achieve%20high%0Aauthentication%20accuracy%20%28~%3D0.82%29%20and%20low%20processing%20latency%20%28~%3D12ms%29%2C%20and%0Asignificantly%20outperform%20multiple%20state-of-the-art%20competitive%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18813v1&entry.124074799=Read"},
{"title": "How green is continual learning, really? Analyzing the energy\n  consumption in continual training of vision foundation models", "author": "Tomaso Trinci and Simone Magistri and Roberto Verdecchia and Andrew D. Bagdanov", "abstract": "  With the ever-growing adoption of AI, its impact on the environment is no\nlonger negligible. Despite the potential that continual learning could have\ntowards Green AI, its environmental sustainability remains relatively\nuncharted. In this work we aim to gain a systematic understanding of the energy\nefficiency of continual learning algorithms. To that end, we conducted an\nextensive set of empirical experiments comparing the energy consumption of\nrecent representation-, prompt-, and exemplar-based continual learning\nalgorithms and two standard baseline (fine tuning and joint training) when used\nto continually adapt a pre-trained ViT-B/16 foundation model. We performed our\nexperiments on three standard datasets: CIFAR-100, ImageNet-R, and DomainNet.\nAdditionally, we propose a novel metric, the Energy NetScore, which we use\nmeasure the algorithm efficiency in terms of energy-accuracy trade-off. Through\nnumerous evaluations varying the number and size of the incremental learning\nsteps, our experiments demonstrate that different types of continual learning\nalgorithms have very different impacts on energy consumption during both\ntraining and inference. Although often overlooked in the continual learning\nliterature, we found that the energy consumed during the inference phase is\ncrucial for evaluating the environmental sustainability of continual learning\nmodels.\n", "link": "http://arxiv.org/abs/2409.18664v1", "date": "2024-09-27", "relevancy": 2.521, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20green%20is%20continual%20learning%2C%20really%3F%20Analyzing%20the%20energy%0A%20%20consumption%20in%20continual%20training%20of%20vision%20foundation%20models&body=Title%3A%20How%20green%20is%20continual%20learning%2C%20really%3F%20Analyzing%20the%20energy%0A%20%20consumption%20in%20continual%20training%20of%20vision%20foundation%20models%0AAuthor%3A%20Tomaso%20Trinci%20and%20Simone%20Magistri%20and%20Roberto%20Verdecchia%20and%20Andrew%20D.%20Bagdanov%0AAbstract%3A%20%20%20With%20the%20ever-growing%20adoption%20of%20AI%2C%20its%20impact%20on%20the%20environment%20is%20no%0Alonger%20negligible.%20Despite%20the%20potential%20that%20continual%20learning%20could%20have%0Atowards%20Green%20AI%2C%20its%20environmental%20sustainability%20remains%20relatively%0Auncharted.%20In%20this%20work%20we%20aim%20to%20gain%20a%20systematic%20understanding%20of%20the%20energy%0Aefficiency%20of%20continual%20learning%20algorithms.%20To%20that%20end%2C%20we%20conducted%20an%0Aextensive%20set%20of%20empirical%20experiments%20comparing%20the%20energy%20consumption%20of%0Arecent%20representation-%2C%20prompt-%2C%20and%20exemplar-based%20continual%20learning%0Aalgorithms%20and%20two%20standard%20baseline%20%28fine%20tuning%20and%20joint%20training%29%20when%20used%0Ato%20continually%20adapt%20a%20pre-trained%20ViT-B/16%20foundation%20model.%20We%20performed%20our%0Aexperiments%20on%20three%20standard%20datasets%3A%20CIFAR-100%2C%20ImageNet-R%2C%20and%20DomainNet.%0AAdditionally%2C%20we%20propose%20a%20novel%20metric%2C%20the%20Energy%20NetScore%2C%20which%20we%20use%0Ameasure%20the%20algorithm%20efficiency%20in%20terms%20of%20energy-accuracy%20trade-off.%20Through%0Anumerous%20evaluations%20varying%20the%20number%20and%20size%20of%20the%20incremental%20learning%0Asteps%2C%20our%20experiments%20demonstrate%20that%20different%20types%20of%20continual%20learning%0Aalgorithms%20have%20very%20different%20impacts%20on%20energy%20consumption%20during%20both%0Atraining%20and%20inference.%20Although%20often%20overlooked%20in%20the%20continual%20learning%0Aliterature%2C%20we%20found%20that%20the%20energy%20consumed%20during%20the%20inference%20phase%20is%0Acrucial%20for%20evaluating%20the%20environmental%20sustainability%20of%20continual%20learning%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520green%2520is%2520continual%2520learning%252C%2520really%253F%2520Analyzing%2520the%2520energy%250A%2520%2520consumption%2520in%2520continual%2520training%2520of%2520vision%2520foundation%2520models%26entry.906535625%3DTomaso%2520Trinci%2520and%2520Simone%2520Magistri%2520and%2520Roberto%2520Verdecchia%2520and%2520Andrew%2520D.%2520Bagdanov%26entry.1292438233%3D%2520%2520With%2520the%2520ever-growing%2520adoption%2520of%2520AI%252C%2520its%2520impact%2520on%2520the%2520environment%2520is%2520no%250Alonger%2520negligible.%2520Despite%2520the%2520potential%2520that%2520continual%2520learning%2520could%2520have%250Atowards%2520Green%2520AI%252C%2520its%2520environmental%2520sustainability%2520remains%2520relatively%250Auncharted.%2520In%2520this%2520work%2520we%2520aim%2520to%2520gain%2520a%2520systematic%2520understanding%2520of%2520the%2520energy%250Aefficiency%2520of%2520continual%2520learning%2520algorithms.%2520To%2520that%2520end%252C%2520we%2520conducted%2520an%250Aextensive%2520set%2520of%2520empirical%2520experiments%2520comparing%2520the%2520energy%2520consumption%2520of%250Arecent%2520representation-%252C%2520prompt-%252C%2520and%2520exemplar-based%2520continual%2520learning%250Aalgorithms%2520and%2520two%2520standard%2520baseline%2520%2528fine%2520tuning%2520and%2520joint%2520training%2529%2520when%2520used%250Ato%2520continually%2520adapt%2520a%2520pre-trained%2520ViT-B/16%2520foundation%2520model.%2520We%2520performed%2520our%250Aexperiments%2520on%2520three%2520standard%2520datasets%253A%2520CIFAR-100%252C%2520ImageNet-R%252C%2520and%2520DomainNet.%250AAdditionally%252C%2520we%2520propose%2520a%2520novel%2520metric%252C%2520the%2520Energy%2520NetScore%252C%2520which%2520we%2520use%250Ameasure%2520the%2520algorithm%2520efficiency%2520in%2520terms%2520of%2520energy-accuracy%2520trade-off.%2520Through%250Anumerous%2520evaluations%2520varying%2520the%2520number%2520and%2520size%2520of%2520the%2520incremental%2520learning%250Asteps%252C%2520our%2520experiments%2520demonstrate%2520that%2520different%2520types%2520of%2520continual%2520learning%250Aalgorithms%2520have%2520very%2520different%2520impacts%2520on%2520energy%2520consumption%2520during%2520both%250Atraining%2520and%2520inference.%2520Although%2520often%2520overlooked%2520in%2520the%2520continual%2520learning%250Aliterature%252C%2520we%2520found%2520that%2520the%2520energy%2520consumed%2520during%2520the%2520inference%2520phase%2520is%250Acrucial%2520for%2520evaluating%2520the%2520environmental%2520sustainability%2520of%2520continual%2520learning%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20green%20is%20continual%20learning%2C%20really%3F%20Analyzing%20the%20energy%0A%20%20consumption%20in%20continual%20training%20of%20vision%20foundation%20models&entry.906535625=Tomaso%20Trinci%20and%20Simone%20Magistri%20and%20Roberto%20Verdecchia%20and%20Andrew%20D.%20Bagdanov&entry.1292438233=%20%20With%20the%20ever-growing%20adoption%20of%20AI%2C%20its%20impact%20on%20the%20environment%20is%20no%0Alonger%20negligible.%20Despite%20the%20potential%20that%20continual%20learning%20could%20have%0Atowards%20Green%20AI%2C%20its%20environmental%20sustainability%20remains%20relatively%0Auncharted.%20In%20this%20work%20we%20aim%20to%20gain%20a%20systematic%20understanding%20of%20the%20energy%0Aefficiency%20of%20continual%20learning%20algorithms.%20To%20that%20end%2C%20we%20conducted%20an%0Aextensive%20set%20of%20empirical%20experiments%20comparing%20the%20energy%20consumption%20of%0Arecent%20representation-%2C%20prompt-%2C%20and%20exemplar-based%20continual%20learning%0Aalgorithms%20and%20two%20standard%20baseline%20%28fine%20tuning%20and%20joint%20training%29%20when%20used%0Ato%20continually%20adapt%20a%20pre-trained%20ViT-B/16%20foundation%20model.%20We%20performed%20our%0Aexperiments%20on%20three%20standard%20datasets%3A%20CIFAR-100%2C%20ImageNet-R%2C%20and%20DomainNet.%0AAdditionally%2C%20we%20propose%20a%20novel%20metric%2C%20the%20Energy%20NetScore%2C%20which%20we%20use%0Ameasure%20the%20algorithm%20efficiency%20in%20terms%20of%20energy-accuracy%20trade-off.%20Through%0Anumerous%20evaluations%20varying%20the%20number%20and%20size%20of%20the%20incremental%20learning%0Asteps%2C%20our%20experiments%20demonstrate%20that%20different%20types%20of%20continual%20learning%0Aalgorithms%20have%20very%20different%20impacts%20on%20energy%20consumption%20during%20both%0Atraining%20and%20inference.%20Although%20often%20overlooked%20in%20the%20continual%20learning%0Aliterature%2C%20we%20found%20that%20the%20energy%20consumed%20during%20the%20inference%20phase%20is%0Acrucial%20for%20evaluating%20the%20environmental%20sustainability%20of%20continual%20learning%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18664v1&entry.124074799=Read"},
{"title": "Charting the Future: Using Chart Question-Answering for Scalable\n  Evaluation of LLM-Driven Data Visualizations", "author": "James Ford and Xingmeng Zhao and Dan Schumacher and Anthony Rios", "abstract": "  We propose a novel framework that leverages Visual Question Answering (VQA)\nmodels to automate the evaluation of LLM-generated data visualizations.\nTraditional evaluation methods often rely on human judgment, which is costly\nand unscalable, or focus solely on data accuracy, neglecting the effectiveness\nof visual communication. By employing VQA models, we assess data representation\nquality and the general communicative clarity of charts. Experiments were\nconducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with\nvisualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1\n70B-Instruct models. Our results indicate that LLM-generated charts do not\nmatch the accuracy of the original non-LLM-generated charts based on VQA\nperformance measures. Moreover, while our results demonstrate that few-shot\nprompting significantly boosts the accuracy of chart generation, considerable\nprogress remains to be made before LLMs can fully match the precision of\nhuman-generated graphs. This underscores the importance of our work, which\nexpedites the research process by enabling rapid iteration without the need for\nhuman annotation, thus accelerating advancements in this field.\n", "link": "http://arxiv.org/abs/2409.18764v1", "date": "2024-09-27", "relevancy": 2.5168, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Charting%20the%20Future%3A%20Using%20Chart%20Question-Answering%20for%20Scalable%0A%20%20Evaluation%20of%20LLM-Driven%20Data%20Visualizations&body=Title%3A%20Charting%20the%20Future%3A%20Using%20Chart%20Question-Answering%20for%20Scalable%0A%20%20Evaluation%20of%20LLM-Driven%20Data%20Visualizations%0AAuthor%3A%20James%20Ford%20and%20Xingmeng%20Zhao%20and%20Dan%20Schumacher%20and%20Anthony%20Rios%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20framework%20that%20leverages%20Visual%20Question%20Answering%20%28VQA%29%0Amodels%20to%20automate%20the%20evaluation%20of%20LLM-generated%20data%20visualizations.%0ATraditional%20evaluation%20methods%20often%20rely%20on%20human%20judgment%2C%20which%20is%20costly%0Aand%20unscalable%2C%20or%20focus%20solely%20on%20data%20accuracy%2C%20neglecting%20the%20effectiveness%0Aof%20visual%20communication.%20By%20employing%20VQA%20models%2C%20we%20assess%20data%20representation%0Aquality%20and%20the%20general%20communicative%20clarity%20of%20charts.%20Experiments%20were%0Aconducted%20using%20two%20leading%20VQA%20benchmark%20datasets%2C%20ChartQA%20and%20PlotQA%2C%20with%0Avisualizations%20generated%20by%20OpenAI%27s%20GPT-3.5%20Turbo%20and%20Meta%27s%20Llama%203.1%0A70B-Instruct%20models.%20Our%20results%20indicate%20that%20LLM-generated%20charts%20do%20not%0Amatch%20the%20accuracy%20of%20the%20original%20non-LLM-generated%20charts%20based%20on%20VQA%0Aperformance%20measures.%20Moreover%2C%20while%20our%20results%20demonstrate%20that%20few-shot%0Aprompting%20significantly%20boosts%20the%20accuracy%20of%20chart%20generation%2C%20considerable%0Aprogress%20remains%20to%20be%20made%20before%20LLMs%20can%20fully%20match%20the%20precision%20of%0Ahuman-generated%20graphs.%20This%20underscores%20the%20importance%20of%20our%20work%2C%20which%0Aexpedites%20the%20research%20process%20by%20enabling%20rapid%20iteration%20without%20the%20need%20for%0Ahuman%20annotation%2C%20thus%20accelerating%20advancements%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharting%2520the%2520Future%253A%2520Using%2520Chart%2520Question-Answering%2520for%2520Scalable%250A%2520%2520Evaluation%2520of%2520LLM-Driven%2520Data%2520Visualizations%26entry.906535625%3DJames%2520Ford%2520and%2520Xingmeng%2520Zhao%2520and%2520Dan%2520Schumacher%2520and%2520Anthony%2520Rios%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520framework%2520that%2520leverages%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%250Amodels%2520to%2520automate%2520the%2520evaluation%2520of%2520LLM-generated%2520data%2520visualizations.%250ATraditional%2520evaluation%2520methods%2520often%2520rely%2520on%2520human%2520judgment%252C%2520which%2520is%2520costly%250Aand%2520unscalable%252C%2520or%2520focus%2520solely%2520on%2520data%2520accuracy%252C%2520neglecting%2520the%2520effectiveness%250Aof%2520visual%2520communication.%2520By%2520employing%2520VQA%2520models%252C%2520we%2520assess%2520data%2520representation%250Aquality%2520and%2520the%2520general%2520communicative%2520clarity%2520of%2520charts.%2520Experiments%2520were%250Aconducted%2520using%2520two%2520leading%2520VQA%2520benchmark%2520datasets%252C%2520ChartQA%2520and%2520PlotQA%252C%2520with%250Avisualizations%2520generated%2520by%2520OpenAI%2527s%2520GPT-3.5%2520Turbo%2520and%2520Meta%2527s%2520Llama%25203.1%250A70B-Instruct%2520models.%2520Our%2520results%2520indicate%2520that%2520LLM-generated%2520charts%2520do%2520not%250Amatch%2520the%2520accuracy%2520of%2520the%2520original%2520non-LLM-generated%2520charts%2520based%2520on%2520VQA%250Aperformance%2520measures.%2520Moreover%252C%2520while%2520our%2520results%2520demonstrate%2520that%2520few-shot%250Aprompting%2520significantly%2520boosts%2520the%2520accuracy%2520of%2520chart%2520generation%252C%2520considerable%250Aprogress%2520remains%2520to%2520be%2520made%2520before%2520LLMs%2520can%2520fully%2520match%2520the%2520precision%2520of%250Ahuman-generated%2520graphs.%2520This%2520underscores%2520the%2520importance%2520of%2520our%2520work%252C%2520which%250Aexpedites%2520the%2520research%2520process%2520by%2520enabling%2520rapid%2520iteration%2520without%2520the%2520need%2520for%250Ahuman%2520annotation%252C%2520thus%2520accelerating%2520advancements%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Charting%20the%20Future%3A%20Using%20Chart%20Question-Answering%20for%20Scalable%0A%20%20Evaluation%20of%20LLM-Driven%20Data%20Visualizations&entry.906535625=James%20Ford%20and%20Xingmeng%20Zhao%20and%20Dan%20Schumacher%20and%20Anthony%20Rios&entry.1292438233=%20%20We%20propose%20a%20novel%20framework%20that%20leverages%20Visual%20Question%20Answering%20%28VQA%29%0Amodels%20to%20automate%20the%20evaluation%20of%20LLM-generated%20data%20visualizations.%0ATraditional%20evaluation%20methods%20often%20rely%20on%20human%20judgment%2C%20which%20is%20costly%0Aand%20unscalable%2C%20or%20focus%20solely%20on%20data%20accuracy%2C%20neglecting%20the%20effectiveness%0Aof%20visual%20communication.%20By%20employing%20VQA%20models%2C%20we%20assess%20data%20representation%0Aquality%20and%20the%20general%20communicative%20clarity%20of%20charts.%20Experiments%20were%0Aconducted%20using%20two%20leading%20VQA%20benchmark%20datasets%2C%20ChartQA%20and%20PlotQA%2C%20with%0Avisualizations%20generated%20by%20OpenAI%27s%20GPT-3.5%20Turbo%20and%20Meta%27s%20Llama%203.1%0A70B-Instruct%20models.%20Our%20results%20indicate%20that%20LLM-generated%20charts%20do%20not%0Amatch%20the%20accuracy%20of%20the%20original%20non-LLM-generated%20charts%20based%20on%20VQA%0Aperformance%20measures.%20Moreover%2C%20while%20our%20results%20demonstrate%20that%20few-shot%0Aprompting%20significantly%20boosts%20the%20accuracy%20of%20chart%20generation%2C%20considerable%0Aprogress%20remains%20to%20be%20made%20before%20LLMs%20can%20fully%20match%20the%20precision%20of%0Ahuman-generated%20graphs.%20This%20underscores%20the%20importance%20of%20our%20work%2C%20which%0Aexpedites%20the%20research%20process%20by%20enabling%20rapid%20iteration%20without%20the%20need%20for%0Ahuman%20annotation%2C%20thus%20accelerating%20advancements%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18764v1&entry.124074799=Read"},
{"title": "\"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot\n  Decision Tree Induction and Embedding with Large Language Models", "author": "Ricardo Knauer and Mario Koddenbrock and Raphael Wallsberger and Nicholas M. Brisson and Georg N. Duda and Deborah Falla and David W. Evans and Erik Rodner", "abstract": "  Large language models (LLMs) provide powerful means to leverage prior\nknowledge for predictive modeling when data is limited. In this work, we\ndemonstrate how LLMs can use their compressed world knowledge to generate\nintrinsically interpretable machine learning models, i.e., decision trees,\nwithout any training data. We find that these zero-shot decision trees can\nsurpass data-driven trees on some small-sized tabular datasets and that\nembeddings derived from these trees perform on par with data-driven tree-based\nembeddings on average. Our knowledge-driven decision tree induction and\nembedding approaches therefore serve as strong new baselines for data-driven\nmachine learning methods in the low-data regime.\n", "link": "http://arxiv.org/abs/2409.18594v1", "date": "2024-09-27", "relevancy": 2.5118, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Oh%20LLM%2C%20I%27m%20Asking%20Thee%2C%20Please%20Give%20Me%20a%20Decision%20Tree%22%3A%20Zero-Shot%0A%20%20Decision%20Tree%20Induction%20and%20Embedding%20with%20Large%20Language%20Models&body=Title%3A%20%22Oh%20LLM%2C%20I%27m%20Asking%20Thee%2C%20Please%20Give%20Me%20a%20Decision%20Tree%22%3A%20Zero-Shot%0A%20%20Decision%20Tree%20Induction%20and%20Embedding%20with%20Large%20Language%20Models%0AAuthor%3A%20Ricardo%20Knauer%20and%20Mario%20Koddenbrock%20and%20Raphael%20Wallsberger%20and%20Nicholas%20M.%20Brisson%20and%20Georg%20N.%20Duda%20and%20Deborah%20Falla%20and%20David%20W.%20Evans%20and%20Erik%20Rodner%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20provide%20powerful%20means%20to%20leverage%20prior%0Aknowledge%20for%20predictive%20modeling%20when%20data%20is%20limited.%20In%20this%20work%2C%20we%0Ademonstrate%20how%20LLMs%20can%20use%20their%20compressed%20world%20knowledge%20to%20generate%0Aintrinsically%20interpretable%20machine%20learning%20models%2C%20i.e.%2C%20decision%20trees%2C%0Awithout%20any%20training%20data.%20We%20find%20that%20these%20zero-shot%20decision%20trees%20can%0Asurpass%20data-driven%20trees%20on%20some%20small-sized%20tabular%20datasets%20and%20that%0Aembeddings%20derived%20from%20these%20trees%20perform%20on%20par%20with%20data-driven%20tree-based%0Aembeddings%20on%20average.%20Our%20knowledge-driven%20decision%20tree%20induction%20and%0Aembedding%20approaches%20therefore%20serve%20as%20strong%20new%20baselines%20for%20data-driven%0Amachine%20learning%20methods%20in%20the%20low-data%20regime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Oh%2520LLM%252C%2520I%2527m%2520Asking%2520Thee%252C%2520Please%2520Give%2520Me%2520a%2520Decision%2520Tree%2522%253A%2520Zero-Shot%250A%2520%2520Decision%2520Tree%2520Induction%2520and%2520Embedding%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DRicardo%2520Knauer%2520and%2520Mario%2520Koddenbrock%2520and%2520Raphael%2520Wallsberger%2520and%2520Nicholas%2520M.%2520Brisson%2520and%2520Georg%2520N.%2520Duda%2520and%2520Deborah%2520Falla%2520and%2520David%2520W.%2520Evans%2520and%2520Erik%2520Rodner%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520provide%2520powerful%2520means%2520to%2520leverage%2520prior%250Aknowledge%2520for%2520predictive%2520modeling%2520when%2520data%2520is%2520limited.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520how%2520LLMs%2520can%2520use%2520their%2520compressed%2520world%2520knowledge%2520to%2520generate%250Aintrinsically%2520interpretable%2520machine%2520learning%2520models%252C%2520i.e.%252C%2520decision%2520trees%252C%250Awithout%2520any%2520training%2520data.%2520We%2520find%2520that%2520these%2520zero-shot%2520decision%2520trees%2520can%250Asurpass%2520data-driven%2520trees%2520on%2520some%2520small-sized%2520tabular%2520datasets%2520and%2520that%250Aembeddings%2520derived%2520from%2520these%2520trees%2520perform%2520on%2520par%2520with%2520data-driven%2520tree-based%250Aembeddings%2520on%2520average.%2520Our%2520knowledge-driven%2520decision%2520tree%2520induction%2520and%250Aembedding%2520approaches%2520therefore%2520serve%2520as%2520strong%2520new%2520baselines%2520for%2520data-driven%250Amachine%2520learning%2520methods%2520in%2520the%2520low-data%2520regime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Oh%20LLM%2C%20I%27m%20Asking%20Thee%2C%20Please%20Give%20Me%20a%20Decision%20Tree%22%3A%20Zero-Shot%0A%20%20Decision%20Tree%20Induction%20and%20Embedding%20with%20Large%20Language%20Models&entry.906535625=Ricardo%20Knauer%20and%20Mario%20Koddenbrock%20and%20Raphael%20Wallsberger%20and%20Nicholas%20M.%20Brisson%20and%20Georg%20N.%20Duda%20and%20Deborah%20Falla%20and%20David%20W.%20Evans%20and%20Erik%20Rodner&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20provide%20powerful%20means%20to%20leverage%20prior%0Aknowledge%20for%20predictive%20modeling%20when%20data%20is%20limited.%20In%20this%20work%2C%20we%0Ademonstrate%20how%20LLMs%20can%20use%20their%20compressed%20world%20knowledge%20to%20generate%0Aintrinsically%20interpretable%20machine%20learning%20models%2C%20i.e.%2C%20decision%20trees%2C%0Awithout%20any%20training%20data.%20We%20find%20that%20these%20zero-shot%20decision%20trees%20can%0Asurpass%20data-driven%20trees%20on%20some%20small-sized%20tabular%20datasets%20and%20that%0Aembeddings%20derived%20from%20these%20trees%20perform%20on%20par%20with%20data-driven%20tree-based%0Aembeddings%20on%20average.%20Our%20knowledge-driven%20decision%20tree%20induction%20and%0Aembedding%20approaches%20therefore%20serve%20as%20strong%20new%20baselines%20for%20data-driven%0Amachine%20learning%20methods%20in%20the%20low-data%20regime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18594v1&entry.124074799=Read"},
{"title": "Positional Encoder Graph Quantile Neural Networks for Geographic Data", "author": "William E. R. de Amorim and Scott A. Sisson and T. Rodrigues and David J. Nott and Guilherme S. Rodrigues", "abstract": "  Positional Encoder Graph Neural Networks (PE-GNNs) are a leading approach for\nmodeling continuous spatial data. However, they often fail to produce\ncalibrated predictive distributions, limiting their effectiveness for\nuncertainty quantification. We introduce the Positional Encoder Graph Quantile\nNeural Network (PE-GQNN), a novel method that integrates PE-GNNs, Quantile\nNeural Networks, and recalibration techniques in a fully nonparametric\nframework, requiring minimal assumptions about the predictive distributions. We\npropose a new network architecture that, when combined with a quantile-based\nloss function, yields accurate and reliable probabilistic models without\nincreasing computational complexity. Our approach provides a flexible, robust\nframework for conditional density estimation, applicable beyond spatial data\ncontexts. We further introduce a structured method for incorporating a KNN\npredictor into the model while avoiding data leakage through the GNN layer\noperation. Experiments on benchmark datasets demonstrate that PE-GQNN\nsignificantly outperforms existing state-of-the-art methods in both predictive\naccuracy and uncertainty quantification.\n", "link": "http://arxiv.org/abs/2409.18865v1", "date": "2024-09-27", "relevancy": 2.4898, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5208}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4893}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Positional%20Encoder%20Graph%20Quantile%20Neural%20Networks%20for%20Geographic%20Data&body=Title%3A%20Positional%20Encoder%20Graph%20Quantile%20Neural%20Networks%20for%20Geographic%20Data%0AAuthor%3A%20William%20E.%20R.%20de%20Amorim%20and%20Scott%20A.%20Sisson%20and%20T.%20Rodrigues%20and%20David%20J.%20Nott%20and%20Guilherme%20S.%20Rodrigues%0AAbstract%3A%20%20%20Positional%20Encoder%20Graph%20Neural%20Networks%20%28PE-GNNs%29%20are%20a%20leading%20approach%20for%0Amodeling%20continuous%20spatial%20data.%20However%2C%20they%20often%20fail%20to%20produce%0Acalibrated%20predictive%20distributions%2C%20limiting%20their%20effectiveness%20for%0Auncertainty%20quantification.%20We%20introduce%20the%20Positional%20Encoder%20Graph%20Quantile%0ANeural%20Network%20%28PE-GQNN%29%2C%20a%20novel%20method%20that%20integrates%20PE-GNNs%2C%20Quantile%0ANeural%20Networks%2C%20and%20recalibration%20techniques%20in%20a%20fully%20nonparametric%0Aframework%2C%20requiring%20minimal%20assumptions%20about%20the%20predictive%20distributions.%20We%0Apropose%20a%20new%20network%20architecture%20that%2C%20when%20combined%20with%20a%20quantile-based%0Aloss%20function%2C%20yields%20accurate%20and%20reliable%20probabilistic%20models%20without%0Aincreasing%20computational%20complexity.%20Our%20approach%20provides%20a%20flexible%2C%20robust%0Aframework%20for%20conditional%20density%20estimation%2C%20applicable%20beyond%20spatial%20data%0Acontexts.%20We%20further%20introduce%20a%20structured%20method%20for%20incorporating%20a%20KNN%0Apredictor%20into%20the%20model%20while%20avoiding%20data%20leakage%20through%20the%20GNN%20layer%0Aoperation.%20Experiments%20on%20benchmark%20datasets%20demonstrate%20that%20PE-GQNN%0Asignificantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20both%20predictive%0Aaccuracy%20and%20uncertainty%20quantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPositional%2520Encoder%2520Graph%2520Quantile%2520Neural%2520Networks%2520for%2520Geographic%2520Data%26entry.906535625%3DWilliam%2520E.%2520R.%2520de%2520Amorim%2520and%2520Scott%2520A.%2520Sisson%2520and%2520T.%2520Rodrigues%2520and%2520David%2520J.%2520Nott%2520and%2520Guilherme%2520S.%2520Rodrigues%26entry.1292438233%3D%2520%2520Positional%2520Encoder%2520Graph%2520Neural%2520Networks%2520%2528PE-GNNs%2529%2520are%2520a%2520leading%2520approach%2520for%250Amodeling%2520continuous%2520spatial%2520data.%2520However%252C%2520they%2520often%2520fail%2520to%2520produce%250Acalibrated%2520predictive%2520distributions%252C%2520limiting%2520their%2520effectiveness%2520for%250Auncertainty%2520quantification.%2520We%2520introduce%2520the%2520Positional%2520Encoder%2520Graph%2520Quantile%250ANeural%2520Network%2520%2528PE-GQNN%2529%252C%2520a%2520novel%2520method%2520that%2520integrates%2520PE-GNNs%252C%2520Quantile%250ANeural%2520Networks%252C%2520and%2520recalibration%2520techniques%2520in%2520a%2520fully%2520nonparametric%250Aframework%252C%2520requiring%2520minimal%2520assumptions%2520about%2520the%2520predictive%2520distributions.%2520We%250Apropose%2520a%2520new%2520network%2520architecture%2520that%252C%2520when%2520combined%2520with%2520a%2520quantile-based%250Aloss%2520function%252C%2520yields%2520accurate%2520and%2520reliable%2520probabilistic%2520models%2520without%250Aincreasing%2520computational%2520complexity.%2520Our%2520approach%2520provides%2520a%2520flexible%252C%2520robust%250Aframework%2520for%2520conditional%2520density%2520estimation%252C%2520applicable%2520beyond%2520spatial%2520data%250Acontexts.%2520We%2520further%2520introduce%2520a%2520structured%2520method%2520for%2520incorporating%2520a%2520KNN%250Apredictor%2520into%2520the%2520model%2520while%2520avoiding%2520data%2520leakage%2520through%2520the%2520GNN%2520layer%250Aoperation.%2520Experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520PE-GQNN%250Asignificantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520both%2520predictive%250Aaccuracy%2520and%2520uncertainty%2520quantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Positional%20Encoder%20Graph%20Quantile%20Neural%20Networks%20for%20Geographic%20Data&entry.906535625=William%20E.%20R.%20de%20Amorim%20and%20Scott%20A.%20Sisson%20and%20T.%20Rodrigues%20and%20David%20J.%20Nott%20and%20Guilherme%20S.%20Rodrigues&entry.1292438233=%20%20Positional%20Encoder%20Graph%20Neural%20Networks%20%28PE-GNNs%29%20are%20a%20leading%20approach%20for%0Amodeling%20continuous%20spatial%20data.%20However%2C%20they%20often%20fail%20to%20produce%0Acalibrated%20predictive%20distributions%2C%20limiting%20their%20effectiveness%20for%0Auncertainty%20quantification.%20We%20introduce%20the%20Positional%20Encoder%20Graph%20Quantile%0ANeural%20Network%20%28PE-GQNN%29%2C%20a%20novel%20method%20that%20integrates%20PE-GNNs%2C%20Quantile%0ANeural%20Networks%2C%20and%20recalibration%20techniques%20in%20a%20fully%20nonparametric%0Aframework%2C%20requiring%20minimal%20assumptions%20about%20the%20predictive%20distributions.%20We%0Apropose%20a%20new%20network%20architecture%20that%2C%20when%20combined%20with%20a%20quantile-based%0Aloss%20function%2C%20yields%20accurate%20and%20reliable%20probabilistic%20models%20without%0Aincreasing%20computational%20complexity.%20Our%20approach%20provides%20a%20flexible%2C%20robust%0Aframework%20for%20conditional%20density%20estimation%2C%20applicable%20beyond%20spatial%20data%0Acontexts.%20We%20further%20introduce%20a%20structured%20method%20for%20incorporating%20a%20KNN%0Apredictor%20into%20the%20model%20while%20avoiding%20data%20leakage%20through%20the%20GNN%20layer%0Aoperation.%20Experiments%20on%20benchmark%20datasets%20demonstrate%20that%20PE-GQNN%0Asignificantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20both%20predictive%0Aaccuracy%20and%20uncertainty%20quantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18865v1&entry.124074799=Read"},
{"title": "Learning from Pattern Completion: Self-supervised Controllable\n  Generation", "author": "Zhiqiang Chen and Guofan Fan and Jinying Gao and Lei Ma and Bo Lei and Tiejun Huang and Shan Yu", "abstract": "  The human brain exhibits a strong ability to spontaneously associate\ndifferent visual attributes of the same or similar visual scene, such as\nassociating sketches and graffiti with real-world visual objects, usually\nwithout supervising information. In contrast, in the field of artificial\nintelligence, controllable generation methods like ControlNet heavily rely on\nannotated training datasets such as depth maps, semantic segmentation maps, and\nposes, which limits the method's scalability. Inspired by the neural mechanisms\nthat may contribute to the brain's associative power, specifically the cortical\nmodularization and hippocampal pattern completion, here we propose a\nself-supervised controllable generation (SCG) framework. Firstly, we introduce\nan equivariant constraint to promote inter-module independence and intra-module\ncorrelation in a modular autoencoder network, thereby achieving functional\nspecialization. Subsequently, based on these specialized modules, we employ a\nself-supervised pattern completion approach for controllable generation\ntraining. Experimental results demonstrate that the proposed modular\nautoencoder effectively achieves functional specialization, including the\nmodular processing of color, brightness, and edge detection, and exhibits\nbrain-like features including orientation selectivity, color antagonism, and\ncenter-surround receptive fields. Through self-supervised training, associative\ngeneration capabilities spontaneously emerge in SCG, demonstrating excellent\ngeneralization ability to various tasks such as associative generation on\npainting, sketches, and ancient graffiti. Compared to the previous\nrepresentative method ControlNet, our proposed approach not only demonstrates\nsuperior robustness in more challenging high-noise scenarios but also possesses\nmore promising scalability potential due to its self-supervised manner.\n", "link": "http://arxiv.org/abs/2409.18694v1", "date": "2024-09-27", "relevancy": 2.4878, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6318}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6257}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Pattern%20Completion%3A%20Self-supervised%20Controllable%0A%20%20Generation&body=Title%3A%20Learning%20from%20Pattern%20Completion%3A%20Self-supervised%20Controllable%0A%20%20Generation%0AAuthor%3A%20Zhiqiang%20Chen%20and%20Guofan%20Fan%20and%20Jinying%20Gao%20and%20Lei%20Ma%20and%20Bo%20Lei%20and%20Tiejun%20Huang%20and%20Shan%20Yu%0AAbstract%3A%20%20%20The%20human%20brain%20exhibits%20a%20strong%20ability%20to%20spontaneously%20associate%0Adifferent%20visual%20attributes%20of%20the%20same%20or%20similar%20visual%20scene%2C%20such%20as%0Aassociating%20sketches%20and%20graffiti%20with%20real-world%20visual%20objects%2C%20usually%0Awithout%20supervising%20information.%20In%20contrast%2C%20in%20the%20field%20of%20artificial%0Aintelligence%2C%20controllable%20generation%20methods%20like%20ControlNet%20heavily%20rely%20on%0Aannotated%20training%20datasets%20such%20as%20depth%20maps%2C%20semantic%20segmentation%20maps%2C%20and%0Aposes%2C%20which%20limits%20the%20method%27s%20scalability.%20Inspired%20by%20the%20neural%20mechanisms%0Athat%20may%20contribute%20to%20the%20brain%27s%20associative%20power%2C%20specifically%20the%20cortical%0Amodularization%20and%20hippocampal%20pattern%20completion%2C%20here%20we%20propose%20a%0Aself-supervised%20controllable%20generation%20%28SCG%29%20framework.%20Firstly%2C%20we%20introduce%0Aan%20equivariant%20constraint%20to%20promote%20inter-module%20independence%20and%20intra-module%0Acorrelation%20in%20a%20modular%20autoencoder%20network%2C%20thereby%20achieving%20functional%0Aspecialization.%20Subsequently%2C%20based%20on%20these%20specialized%20modules%2C%20we%20employ%20a%0Aself-supervised%20pattern%20completion%20approach%20for%20controllable%20generation%0Atraining.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20modular%0Aautoencoder%20effectively%20achieves%20functional%20specialization%2C%20including%20the%0Amodular%20processing%20of%20color%2C%20brightness%2C%20and%20edge%20detection%2C%20and%20exhibits%0Abrain-like%20features%20including%20orientation%20selectivity%2C%20color%20antagonism%2C%20and%0Acenter-surround%20receptive%20fields.%20Through%20self-supervised%20training%2C%20associative%0Ageneration%20capabilities%20spontaneously%20emerge%20in%20SCG%2C%20demonstrating%20excellent%0Ageneralization%20ability%20to%20various%20tasks%20such%20as%20associative%20generation%20on%0Apainting%2C%20sketches%2C%20and%20ancient%20graffiti.%20Compared%20to%20the%20previous%0Arepresentative%20method%20ControlNet%2C%20our%20proposed%20approach%20not%20only%20demonstrates%0Asuperior%20robustness%20in%20more%20challenging%20high-noise%20scenarios%20but%20also%20possesses%0Amore%20promising%20scalability%20potential%20due%20to%20its%20self-supervised%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Pattern%2520Completion%253A%2520Self-supervised%2520Controllable%250A%2520%2520Generation%26entry.906535625%3DZhiqiang%2520Chen%2520and%2520Guofan%2520Fan%2520and%2520Jinying%2520Gao%2520and%2520Lei%2520Ma%2520and%2520Bo%2520Lei%2520and%2520Tiejun%2520Huang%2520and%2520Shan%2520Yu%26entry.1292438233%3D%2520%2520The%2520human%2520brain%2520exhibits%2520a%2520strong%2520ability%2520to%2520spontaneously%2520associate%250Adifferent%2520visual%2520attributes%2520of%2520the%2520same%2520or%2520similar%2520visual%2520scene%252C%2520such%2520as%250Aassociating%2520sketches%2520and%2520graffiti%2520with%2520real-world%2520visual%2520objects%252C%2520usually%250Awithout%2520supervising%2520information.%2520In%2520contrast%252C%2520in%2520the%2520field%2520of%2520artificial%250Aintelligence%252C%2520controllable%2520generation%2520methods%2520like%2520ControlNet%2520heavily%2520rely%2520on%250Aannotated%2520training%2520datasets%2520such%2520as%2520depth%2520maps%252C%2520semantic%2520segmentation%2520maps%252C%2520and%250Aposes%252C%2520which%2520limits%2520the%2520method%2527s%2520scalability.%2520Inspired%2520by%2520the%2520neural%2520mechanisms%250Athat%2520may%2520contribute%2520to%2520the%2520brain%2527s%2520associative%2520power%252C%2520specifically%2520the%2520cortical%250Amodularization%2520and%2520hippocampal%2520pattern%2520completion%252C%2520here%2520we%2520propose%2520a%250Aself-supervised%2520controllable%2520generation%2520%2528SCG%2529%2520framework.%2520Firstly%252C%2520we%2520introduce%250Aan%2520equivariant%2520constraint%2520to%2520promote%2520inter-module%2520independence%2520and%2520intra-module%250Acorrelation%2520in%2520a%2520modular%2520autoencoder%2520network%252C%2520thereby%2520achieving%2520functional%250Aspecialization.%2520Subsequently%252C%2520based%2520on%2520these%2520specialized%2520modules%252C%2520we%2520employ%2520a%250Aself-supervised%2520pattern%2520completion%2520approach%2520for%2520controllable%2520generation%250Atraining.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520modular%250Aautoencoder%2520effectively%2520achieves%2520functional%2520specialization%252C%2520including%2520the%250Amodular%2520processing%2520of%2520color%252C%2520brightness%252C%2520and%2520edge%2520detection%252C%2520and%2520exhibits%250Abrain-like%2520features%2520including%2520orientation%2520selectivity%252C%2520color%2520antagonism%252C%2520and%250Acenter-surround%2520receptive%2520fields.%2520Through%2520self-supervised%2520training%252C%2520associative%250Ageneration%2520capabilities%2520spontaneously%2520emerge%2520in%2520SCG%252C%2520demonstrating%2520excellent%250Ageneralization%2520ability%2520to%2520various%2520tasks%2520such%2520as%2520associative%2520generation%2520on%250Apainting%252C%2520sketches%252C%2520and%2520ancient%2520graffiti.%2520Compared%2520to%2520the%2520previous%250Arepresentative%2520method%2520ControlNet%252C%2520our%2520proposed%2520approach%2520not%2520only%2520demonstrates%250Asuperior%2520robustness%2520in%2520more%2520challenging%2520high-noise%2520scenarios%2520but%2520also%2520possesses%250Amore%2520promising%2520scalability%2520potential%2520due%2520to%2520its%2520self-supervised%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Pattern%20Completion%3A%20Self-supervised%20Controllable%0A%20%20Generation&entry.906535625=Zhiqiang%20Chen%20and%20Guofan%20Fan%20and%20Jinying%20Gao%20and%20Lei%20Ma%20and%20Bo%20Lei%20and%20Tiejun%20Huang%20and%20Shan%20Yu&entry.1292438233=%20%20The%20human%20brain%20exhibits%20a%20strong%20ability%20to%20spontaneously%20associate%0Adifferent%20visual%20attributes%20of%20the%20same%20or%20similar%20visual%20scene%2C%20such%20as%0Aassociating%20sketches%20and%20graffiti%20with%20real-world%20visual%20objects%2C%20usually%0Awithout%20supervising%20information.%20In%20contrast%2C%20in%20the%20field%20of%20artificial%0Aintelligence%2C%20controllable%20generation%20methods%20like%20ControlNet%20heavily%20rely%20on%0Aannotated%20training%20datasets%20such%20as%20depth%20maps%2C%20semantic%20segmentation%20maps%2C%20and%0Aposes%2C%20which%20limits%20the%20method%27s%20scalability.%20Inspired%20by%20the%20neural%20mechanisms%0Athat%20may%20contribute%20to%20the%20brain%27s%20associative%20power%2C%20specifically%20the%20cortical%0Amodularization%20and%20hippocampal%20pattern%20completion%2C%20here%20we%20propose%20a%0Aself-supervised%20controllable%20generation%20%28SCG%29%20framework.%20Firstly%2C%20we%20introduce%0Aan%20equivariant%20constraint%20to%20promote%20inter-module%20independence%20and%20intra-module%0Acorrelation%20in%20a%20modular%20autoencoder%20network%2C%20thereby%20achieving%20functional%0Aspecialization.%20Subsequently%2C%20based%20on%20these%20specialized%20modules%2C%20we%20employ%20a%0Aself-supervised%20pattern%20completion%20approach%20for%20controllable%20generation%0Atraining.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20modular%0Aautoencoder%20effectively%20achieves%20functional%20specialization%2C%20including%20the%0Amodular%20processing%20of%20color%2C%20brightness%2C%20and%20edge%20detection%2C%20and%20exhibits%0Abrain-like%20features%20including%20orientation%20selectivity%2C%20color%20antagonism%2C%20and%0Acenter-surround%20receptive%20fields.%20Through%20self-supervised%20training%2C%20associative%0Ageneration%20capabilities%20spontaneously%20emerge%20in%20SCG%2C%20demonstrating%20excellent%0Ageneralization%20ability%20to%20various%20tasks%20such%20as%20associative%20generation%20on%0Apainting%2C%20sketches%2C%20and%20ancient%20graffiti.%20Compared%20to%20the%20previous%0Arepresentative%20method%20ControlNet%2C%20our%20proposed%20approach%20not%20only%20demonstrates%0Asuperior%20robustness%20in%20more%20challenging%20high-noise%20scenarios%20but%20also%20possesses%0Amore%20promising%20scalability%20potential%20due%20to%20its%20self-supervised%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18694v1&entry.124074799=Read"},
{"title": "SurfaceAI: Automated creation of cohesive road surface quality datasets\n  based on open street-level imagery", "author": "Alexandra Kapp and Edith Hoffmann and Esther Weigmann and Helena Mihaljevi\u0107", "abstract": "  This paper introduces SurfaceAI, a pipeline designed to generate\ncomprehensive georeferenced datasets on road surface type and quality from\nopenly available street-level imagery. The motivation stems from the\nsignificant impact of road unevenness on the safety and comfort of traffic\nparticipants, especially vulnerable road users, emphasizing the need for\ndetailed road surface data in infrastructure modeling and analysis. SurfaceAI\naddresses this gap by leveraging crowdsourced Mapillary data to train models\nthat predict the type and quality of road surfaces visible in street-level\nimages, which are then aggregated to provide cohesive information on entire\nroad segment conditions.\n", "link": "http://arxiv.org/abs/2409.18922v1", "date": "2024-09-27", "relevancy": 2.4761, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5122}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4868}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurfaceAI%3A%20Automated%20creation%20of%20cohesive%20road%20surface%20quality%20datasets%0A%20%20based%20on%20open%20street-level%20imagery&body=Title%3A%20SurfaceAI%3A%20Automated%20creation%20of%20cohesive%20road%20surface%20quality%20datasets%0A%20%20based%20on%20open%20street-level%20imagery%0AAuthor%3A%20Alexandra%20Kapp%20and%20Edith%20Hoffmann%20and%20Esther%20Weigmann%20and%20Helena%20Mihaljevi%C4%87%0AAbstract%3A%20%20%20This%20paper%20introduces%20SurfaceAI%2C%20a%20pipeline%20designed%20to%20generate%0Acomprehensive%20georeferenced%20datasets%20on%20road%20surface%20type%20and%20quality%20from%0Aopenly%20available%20street-level%20imagery.%20The%20motivation%20stems%20from%20the%0Asignificant%20impact%20of%20road%20unevenness%20on%20the%20safety%20and%20comfort%20of%20traffic%0Aparticipants%2C%20especially%20vulnerable%20road%20users%2C%20emphasizing%20the%20need%20for%0Adetailed%20road%20surface%20data%20in%20infrastructure%20modeling%20and%20analysis.%20SurfaceAI%0Aaddresses%20this%20gap%20by%20leveraging%20crowdsourced%20Mapillary%20data%20to%20train%20models%0Athat%20predict%20the%20type%20and%20quality%20of%20road%20surfaces%20visible%20in%20street-level%0Aimages%2C%20which%20are%20then%20aggregated%20to%20provide%20cohesive%20information%20on%20entire%0Aroad%20segment%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurfaceAI%253A%2520Automated%2520creation%2520of%2520cohesive%2520road%2520surface%2520quality%2520datasets%250A%2520%2520based%2520on%2520open%2520street-level%2520imagery%26entry.906535625%3DAlexandra%2520Kapp%2520and%2520Edith%2520Hoffmann%2520and%2520Esther%2520Weigmann%2520and%2520Helena%2520Mihaljevi%25C4%2587%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520SurfaceAI%252C%2520a%2520pipeline%2520designed%2520to%2520generate%250Acomprehensive%2520georeferenced%2520datasets%2520on%2520road%2520surface%2520type%2520and%2520quality%2520from%250Aopenly%2520available%2520street-level%2520imagery.%2520The%2520motivation%2520stems%2520from%2520the%250Asignificant%2520impact%2520of%2520road%2520unevenness%2520on%2520the%2520safety%2520and%2520comfort%2520of%2520traffic%250Aparticipants%252C%2520especially%2520vulnerable%2520road%2520users%252C%2520emphasizing%2520the%2520need%2520for%250Adetailed%2520road%2520surface%2520data%2520in%2520infrastructure%2520modeling%2520and%2520analysis.%2520SurfaceAI%250Aaddresses%2520this%2520gap%2520by%2520leveraging%2520crowdsourced%2520Mapillary%2520data%2520to%2520train%2520models%250Athat%2520predict%2520the%2520type%2520and%2520quality%2520of%2520road%2520surfaces%2520visible%2520in%2520street-level%250Aimages%252C%2520which%2520are%2520then%2520aggregated%2520to%2520provide%2520cohesive%2520information%2520on%2520entire%250Aroad%2520segment%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurfaceAI%3A%20Automated%20creation%20of%20cohesive%20road%20surface%20quality%20datasets%0A%20%20based%20on%20open%20street-level%20imagery&entry.906535625=Alexandra%20Kapp%20and%20Edith%20Hoffmann%20and%20Esther%20Weigmann%20and%20Helena%20Mihaljevi%C4%87&entry.1292438233=%20%20This%20paper%20introduces%20SurfaceAI%2C%20a%20pipeline%20designed%20to%20generate%0Acomprehensive%20georeferenced%20datasets%20on%20road%20surface%20type%20and%20quality%20from%0Aopenly%20available%20street-level%20imagery.%20The%20motivation%20stems%20from%20the%0Asignificant%20impact%20of%20road%20unevenness%20on%20the%20safety%20and%20comfort%20of%20traffic%0Aparticipants%2C%20especially%20vulnerable%20road%20users%2C%20emphasizing%20the%20need%20for%0Adetailed%20road%20surface%20data%20in%20infrastructure%20modeling%20and%20analysis.%20SurfaceAI%0Aaddresses%20this%20gap%20by%20leveraging%20crowdsourced%20Mapillary%20data%20to%20train%20models%0Athat%20predict%20the%20type%20and%20quality%20of%20road%20surfaces%20visible%20in%20street-level%0Aimages%2C%20which%20are%20then%20aggregated%20to%20provide%20cohesive%20information%20on%20entire%0Aroad%20segment%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18922v1&entry.124074799=Read"},
{"title": "Enhanced Convolution Neural Network with Optimized Pooling and\n  Hyperparameter Tuning for Network Intrusion Detection", "author": "Ayush Kumar Sharma and Sourav Patel and Supriya Bharat Wakchaure and Abirami S", "abstract": "  Network Intrusion Detection Systems (NIDS) are essential for protecting\ncomputer networks from malicious activities, including Denial of Service (DoS),\nProbing, User-to-Root (U2R), and Remote-to-Local (R2L) attacks. Without\neffective NIDS, networks are vulnerable to significant security breaches and\ndata loss. Machine learning techniques provide a promising approach to enhance\nNIDS by automating threat detection and improving accuracy. In this research,\nwe propose an Enhanced Convolutional Neural Network (EnCNN) for NIDS and\nevaluate its performance using the KDDCUP'99 dataset. Our methodology includes\ncomprehensive data preprocessing, exploratory data analysis (EDA), and feature\nengineering. We compare EnCNN with various machine learning algorithms,\nincluding Logistic Regression, Decision Trees, Support Vector Machines (SVM),\nand ensemble methods like Random Forest, AdaBoost, and Voting Ensemble. The\nresults show that EnCNN significantly improves detection accuracy, with a\nnotable 10% increase over state-of-art approaches. This demonstrates the\neffectiveness of EnCNN in real-time network intrusion detection, offering a\nrobust solution for identifying and mitigating security threats, and enhancing\noverall network resilience.\n", "link": "http://arxiv.org/abs/2409.18642v1", "date": "2024-09-27", "relevancy": 2.4622, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5491}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Convolution%20Neural%20Network%20with%20Optimized%20Pooling%20and%0A%20%20Hyperparameter%20Tuning%20for%20Network%20Intrusion%20Detection&body=Title%3A%20Enhanced%20Convolution%20Neural%20Network%20with%20Optimized%20Pooling%20and%0A%20%20Hyperparameter%20Tuning%20for%20Network%20Intrusion%20Detection%0AAuthor%3A%20Ayush%20Kumar%20Sharma%20and%20Sourav%20Patel%20and%20Supriya%20Bharat%20Wakchaure%20and%20Abirami%20S%0AAbstract%3A%20%20%20Network%20Intrusion%20Detection%20Systems%20%28NIDS%29%20are%20essential%20for%20protecting%0Acomputer%20networks%20from%20malicious%20activities%2C%20including%20Denial%20of%20Service%20%28DoS%29%2C%0AProbing%2C%20User-to-Root%20%28U2R%29%2C%20and%20Remote-to-Local%20%28R2L%29%20attacks.%20Without%0Aeffective%20NIDS%2C%20networks%20are%20vulnerable%20to%20significant%20security%20breaches%20and%0Adata%20loss.%20Machine%20learning%20techniques%20provide%20a%20promising%20approach%20to%20enhance%0ANIDS%20by%20automating%20threat%20detection%20and%20improving%20accuracy.%20In%20this%20research%2C%0Awe%20propose%20an%20Enhanced%20Convolutional%20Neural%20Network%20%28EnCNN%29%20for%20NIDS%20and%0Aevaluate%20its%20performance%20using%20the%20KDDCUP%2799%20dataset.%20Our%20methodology%20includes%0Acomprehensive%20data%20preprocessing%2C%20exploratory%20data%20analysis%20%28EDA%29%2C%20and%20feature%0Aengineering.%20We%20compare%20EnCNN%20with%20various%20machine%20learning%20algorithms%2C%0Aincluding%20Logistic%20Regression%2C%20Decision%20Trees%2C%20Support%20Vector%20Machines%20%28SVM%29%2C%0Aand%20ensemble%20methods%20like%20Random%20Forest%2C%20AdaBoost%2C%20and%20Voting%20Ensemble.%20The%0Aresults%20show%20that%20EnCNN%20significantly%20improves%20detection%20accuracy%2C%20with%20a%0Anotable%2010%25%20increase%20over%20state-of-art%20approaches.%20This%20demonstrates%20the%0Aeffectiveness%20of%20EnCNN%20in%20real-time%20network%20intrusion%20detection%2C%20offering%20a%0Arobust%20solution%20for%20identifying%20and%20mitigating%20security%20threats%2C%20and%20enhancing%0Aoverall%20network%20resilience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Convolution%2520Neural%2520Network%2520with%2520Optimized%2520Pooling%2520and%250A%2520%2520Hyperparameter%2520Tuning%2520for%2520Network%2520Intrusion%2520Detection%26entry.906535625%3DAyush%2520Kumar%2520Sharma%2520and%2520Sourav%2520Patel%2520and%2520Supriya%2520Bharat%2520Wakchaure%2520and%2520Abirami%2520S%26entry.1292438233%3D%2520%2520Network%2520Intrusion%2520Detection%2520Systems%2520%2528NIDS%2529%2520are%2520essential%2520for%2520protecting%250Acomputer%2520networks%2520from%2520malicious%2520activities%252C%2520including%2520Denial%2520of%2520Service%2520%2528DoS%2529%252C%250AProbing%252C%2520User-to-Root%2520%2528U2R%2529%252C%2520and%2520Remote-to-Local%2520%2528R2L%2529%2520attacks.%2520Without%250Aeffective%2520NIDS%252C%2520networks%2520are%2520vulnerable%2520to%2520significant%2520security%2520breaches%2520and%250Adata%2520loss.%2520Machine%2520learning%2520techniques%2520provide%2520a%2520promising%2520approach%2520to%2520enhance%250ANIDS%2520by%2520automating%2520threat%2520detection%2520and%2520improving%2520accuracy.%2520In%2520this%2520research%252C%250Awe%2520propose%2520an%2520Enhanced%2520Convolutional%2520Neural%2520Network%2520%2528EnCNN%2529%2520for%2520NIDS%2520and%250Aevaluate%2520its%2520performance%2520using%2520the%2520KDDCUP%252799%2520dataset.%2520Our%2520methodology%2520includes%250Acomprehensive%2520data%2520preprocessing%252C%2520exploratory%2520data%2520analysis%2520%2528EDA%2529%252C%2520and%2520feature%250Aengineering.%2520We%2520compare%2520EnCNN%2520with%2520various%2520machine%2520learning%2520algorithms%252C%250Aincluding%2520Logistic%2520Regression%252C%2520Decision%2520Trees%252C%2520Support%2520Vector%2520Machines%2520%2528SVM%2529%252C%250Aand%2520ensemble%2520methods%2520like%2520Random%2520Forest%252C%2520AdaBoost%252C%2520and%2520Voting%2520Ensemble.%2520The%250Aresults%2520show%2520that%2520EnCNN%2520significantly%2520improves%2520detection%2520accuracy%252C%2520with%2520a%250Anotable%252010%2525%2520increase%2520over%2520state-of-art%2520approaches.%2520This%2520demonstrates%2520the%250Aeffectiveness%2520of%2520EnCNN%2520in%2520real-time%2520network%2520intrusion%2520detection%252C%2520offering%2520a%250Arobust%2520solution%2520for%2520identifying%2520and%2520mitigating%2520security%2520threats%252C%2520and%2520enhancing%250Aoverall%2520network%2520resilience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Convolution%20Neural%20Network%20with%20Optimized%20Pooling%20and%0A%20%20Hyperparameter%20Tuning%20for%20Network%20Intrusion%20Detection&entry.906535625=Ayush%20Kumar%20Sharma%20and%20Sourav%20Patel%20and%20Supriya%20Bharat%20Wakchaure%20and%20Abirami%20S&entry.1292438233=%20%20Network%20Intrusion%20Detection%20Systems%20%28NIDS%29%20are%20essential%20for%20protecting%0Acomputer%20networks%20from%20malicious%20activities%2C%20including%20Denial%20of%20Service%20%28DoS%29%2C%0AProbing%2C%20User-to-Root%20%28U2R%29%2C%20and%20Remote-to-Local%20%28R2L%29%20attacks.%20Without%0Aeffective%20NIDS%2C%20networks%20are%20vulnerable%20to%20significant%20security%20breaches%20and%0Adata%20loss.%20Machine%20learning%20techniques%20provide%20a%20promising%20approach%20to%20enhance%0ANIDS%20by%20automating%20threat%20detection%20and%20improving%20accuracy.%20In%20this%20research%2C%0Awe%20propose%20an%20Enhanced%20Convolutional%20Neural%20Network%20%28EnCNN%29%20for%20NIDS%20and%0Aevaluate%20its%20performance%20using%20the%20KDDCUP%2799%20dataset.%20Our%20methodology%20includes%0Acomprehensive%20data%20preprocessing%2C%20exploratory%20data%20analysis%20%28EDA%29%2C%20and%20feature%0Aengineering.%20We%20compare%20EnCNN%20with%20various%20machine%20learning%20algorithms%2C%0Aincluding%20Logistic%20Regression%2C%20Decision%20Trees%2C%20Support%20Vector%20Machines%20%28SVM%29%2C%0Aand%20ensemble%20methods%20like%20Random%20Forest%2C%20AdaBoost%2C%20and%20Voting%20Ensemble.%20The%0Aresults%20show%20that%20EnCNN%20significantly%20improves%20detection%20accuracy%2C%20with%20a%0Anotable%2010%25%20increase%20over%20state-of-art%20approaches.%20This%20demonstrates%20the%0Aeffectiveness%20of%20EnCNN%20in%20real-time%20network%20intrusion%20detection%2C%20offering%20a%0Arobust%20solution%20for%20identifying%20and%20mitigating%20security%20threats%2C%20and%20enhancing%0Aoverall%20network%20resilience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18642v1&entry.124074799=Read"},
{"title": "Hierarchical Windowed Graph Attention Network and a Large Scale Dataset\n  for Isolated Indian Sign Language Recognition", "author": "Suvajit Patra and Arkadip Maitra and Megha Tiwari and K. Kumaran and Swathy Prabhu and Swami Punyeshwarananda and Soumitra Samanta", "abstract": "  Automatic Sign Language (SL) recognition is an important task in the computer\nvision community. To build a robust SL recognition system, we need a\nconsiderable amount of data which is lacking particularly in Indian sign\nlanguage (ISL). In this paper, we introduce a large-scale isolated ISL dataset\nand a novel SL recognition model based on skeleton graph structure. The dataset\ncovers 2002 daily used common words in the deaf community recorded by 20 (10\nmale and 10 female) deaf adult signers (contains 40033 videos). We propose a SL\nrecognition model namely Hierarchical Windowed Graph Attention Network (HWGAT)\nby utilizing the human upper body skeleton graph. The HWGAT tries to capture\ndistinctive motions by giving attention to different body parts induced by the\nhuman skeleton graph. The utility of the proposed dataset and the usefulness of\nour model are evaluated through extensive experiments. We pre-trained the\nproposed model on the presented dataset and fine-tuned it across different sign\nlanguage datasets further boosting the performance of 1.10, 0.46, 0.78, and\n6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL respectively compared\nto the existing state-of-the-art keypoints-based models.\n", "link": "http://arxiv.org/abs/2407.14224v2", "date": "2024-09-27", "relevancy": 2.4485, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4934}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4909}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Windowed%20Graph%20Attention%20Network%20and%20a%20Large%20Scale%20Dataset%0A%20%20for%20Isolated%20Indian%20Sign%20Language%20Recognition&body=Title%3A%20Hierarchical%20Windowed%20Graph%20Attention%20Network%20and%20a%20Large%20Scale%20Dataset%0A%20%20for%20Isolated%20Indian%20Sign%20Language%20Recognition%0AAuthor%3A%20Suvajit%20Patra%20and%20Arkadip%20Maitra%20and%20Megha%20Tiwari%20and%20K.%20Kumaran%20and%20Swathy%20Prabhu%20and%20Swami%20Punyeshwarananda%20and%20Soumitra%20Samanta%0AAbstract%3A%20%20%20Automatic%20Sign%20Language%20%28SL%29%20recognition%20is%20an%20important%20task%20in%20the%20computer%0Avision%20community.%20To%20build%20a%20robust%20SL%20recognition%20system%2C%20we%20need%20a%0Aconsiderable%20amount%20of%20data%20which%20is%20lacking%20particularly%20in%20Indian%20sign%0Alanguage%20%28ISL%29.%20In%20this%20paper%2C%20we%20introduce%20a%20large-scale%20isolated%20ISL%20dataset%0Aand%20a%20novel%20SL%20recognition%20model%20based%20on%20skeleton%20graph%20structure.%20The%20dataset%0Acovers%202002%20daily%20used%20common%20words%20in%20the%20deaf%20community%20recorded%20by%2020%20%2810%0Amale%20and%2010%20female%29%20deaf%20adult%20signers%20%28contains%2040033%20videos%29.%20We%20propose%20a%20SL%0Arecognition%20model%20namely%20Hierarchical%20Windowed%20Graph%20Attention%20Network%20%28HWGAT%29%0Aby%20utilizing%20the%20human%20upper%20body%20skeleton%20graph.%20The%20HWGAT%20tries%20to%20capture%0Adistinctive%20motions%20by%20giving%20attention%20to%20different%20body%20parts%20induced%20by%20the%0Ahuman%20skeleton%20graph.%20The%20utility%20of%20the%20proposed%20dataset%20and%20the%20usefulness%20of%0Aour%20model%20are%20evaluated%20through%20extensive%20experiments.%20We%20pre-trained%20the%0Aproposed%20model%20on%20the%20presented%20dataset%20and%20fine-tuned%20it%20across%20different%20sign%0Alanguage%20datasets%20further%20boosting%20the%20performance%20of%201.10%2C%200.46%2C%200.78%2C%20and%0A6.84%20percentage%20points%20on%20INCLUDE%2C%20LSA64%2C%20AUTSL%20and%20WLASL%20respectively%20compared%0Ato%20the%20existing%20state-of-the-art%20keypoints-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14224v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Windowed%2520Graph%2520Attention%2520Network%2520and%2520a%2520Large%2520Scale%2520Dataset%250A%2520%2520for%2520Isolated%2520Indian%2520Sign%2520Language%2520Recognition%26entry.906535625%3DSuvajit%2520Patra%2520and%2520Arkadip%2520Maitra%2520and%2520Megha%2520Tiwari%2520and%2520K.%2520Kumaran%2520and%2520Swathy%2520Prabhu%2520and%2520Swami%2520Punyeshwarananda%2520and%2520Soumitra%2520Samanta%26entry.1292438233%3D%2520%2520Automatic%2520Sign%2520Language%2520%2528SL%2529%2520recognition%2520is%2520an%2520important%2520task%2520in%2520the%2520computer%250Avision%2520community.%2520To%2520build%2520a%2520robust%2520SL%2520recognition%2520system%252C%2520we%2520need%2520a%250Aconsiderable%2520amount%2520of%2520data%2520which%2520is%2520lacking%2520particularly%2520in%2520Indian%2520sign%250Alanguage%2520%2528ISL%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520large-scale%2520isolated%2520ISL%2520dataset%250Aand%2520a%2520novel%2520SL%2520recognition%2520model%2520based%2520on%2520skeleton%2520graph%2520structure.%2520The%2520dataset%250Acovers%25202002%2520daily%2520used%2520common%2520words%2520in%2520the%2520deaf%2520community%2520recorded%2520by%252020%2520%252810%250Amale%2520and%252010%2520female%2529%2520deaf%2520adult%2520signers%2520%2528contains%252040033%2520videos%2529.%2520We%2520propose%2520a%2520SL%250Arecognition%2520model%2520namely%2520Hierarchical%2520Windowed%2520Graph%2520Attention%2520Network%2520%2528HWGAT%2529%250Aby%2520utilizing%2520the%2520human%2520upper%2520body%2520skeleton%2520graph.%2520The%2520HWGAT%2520tries%2520to%2520capture%250Adistinctive%2520motions%2520by%2520giving%2520attention%2520to%2520different%2520body%2520parts%2520induced%2520by%2520the%250Ahuman%2520skeleton%2520graph.%2520The%2520utility%2520of%2520the%2520proposed%2520dataset%2520and%2520the%2520usefulness%2520of%250Aour%2520model%2520are%2520evaluated%2520through%2520extensive%2520experiments.%2520We%2520pre-trained%2520the%250Aproposed%2520model%2520on%2520the%2520presented%2520dataset%2520and%2520fine-tuned%2520it%2520across%2520different%2520sign%250Alanguage%2520datasets%2520further%2520boosting%2520the%2520performance%2520of%25201.10%252C%25200.46%252C%25200.78%252C%2520and%250A6.84%2520percentage%2520points%2520on%2520INCLUDE%252C%2520LSA64%252C%2520AUTSL%2520and%2520WLASL%2520respectively%2520compared%250Ato%2520the%2520existing%2520state-of-the-art%2520keypoints-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14224v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Windowed%20Graph%20Attention%20Network%20and%20a%20Large%20Scale%20Dataset%0A%20%20for%20Isolated%20Indian%20Sign%20Language%20Recognition&entry.906535625=Suvajit%20Patra%20and%20Arkadip%20Maitra%20and%20Megha%20Tiwari%20and%20K.%20Kumaran%20and%20Swathy%20Prabhu%20and%20Swami%20Punyeshwarananda%20and%20Soumitra%20Samanta&entry.1292438233=%20%20Automatic%20Sign%20Language%20%28SL%29%20recognition%20is%20an%20important%20task%20in%20the%20computer%0Avision%20community.%20To%20build%20a%20robust%20SL%20recognition%20system%2C%20we%20need%20a%0Aconsiderable%20amount%20of%20data%20which%20is%20lacking%20particularly%20in%20Indian%20sign%0Alanguage%20%28ISL%29.%20In%20this%20paper%2C%20we%20introduce%20a%20large-scale%20isolated%20ISL%20dataset%0Aand%20a%20novel%20SL%20recognition%20model%20based%20on%20skeleton%20graph%20structure.%20The%20dataset%0Acovers%202002%20daily%20used%20common%20words%20in%20the%20deaf%20community%20recorded%20by%2020%20%2810%0Amale%20and%2010%20female%29%20deaf%20adult%20signers%20%28contains%2040033%20videos%29.%20We%20propose%20a%20SL%0Arecognition%20model%20namely%20Hierarchical%20Windowed%20Graph%20Attention%20Network%20%28HWGAT%29%0Aby%20utilizing%20the%20human%20upper%20body%20skeleton%20graph.%20The%20HWGAT%20tries%20to%20capture%0Adistinctive%20motions%20by%20giving%20attention%20to%20different%20body%20parts%20induced%20by%20the%0Ahuman%20skeleton%20graph.%20The%20utility%20of%20the%20proposed%20dataset%20and%20the%20usefulness%20of%0Aour%20model%20are%20evaluated%20through%20extensive%20experiments.%20We%20pre-trained%20the%0Aproposed%20model%20on%20the%20presented%20dataset%20and%20fine-tuned%20it%20across%20different%20sign%0Alanguage%20datasets%20further%20boosting%20the%20performance%20of%201.10%2C%200.46%2C%200.78%2C%20and%0A6.84%20percentage%20points%20on%20INCLUDE%2C%20LSA64%2C%20AUTSL%20and%20WLASL%20respectively%20compared%0Ato%20the%20existing%20state-of-the-art%20keypoints-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14224v2&entry.124074799=Read"},
{"title": "Bi-Directional Transformers vs. word2vec: Discovering Vulnerabilities in\n  Lifted Compiled Code", "author": "Gary A. McCully and John D. Hastings and Shengjie Xu and Adam Fortier", "abstract": "  Detecting vulnerabilities within compiled binaries is challenging due to lost\nhigh-level code structures and other factors such as architectural\ndependencies, compilers, and optimization options. To address these obstacles,\nthis research explores vulnerability detection using natural language\nprocessing (NLP) embedding techniques with word2vec, BERT, and RoBERTa to learn\nsemantics from intermediate representation (LLVM IR) code. Long short-term\nmemory (LSTM) neural networks were trained on embeddings from encoders created\nusing approximately 48k LLVM functions from the Juliet dataset. This study is\npioneering in its comparison of word2vec models with multiple bidirectional\ntransformers (BERT, RoBERTa) embeddings built using LLVM code to train neural\nnetworks to detect vulnerabilities in compiled binaries. Word2vec Skip-Gram\nmodels achieved 92% validation accuracy in detecting vulnerabilities,\noutperforming word2vec Continuous Bag of Words (CBOW), BERT, and RoBERTa. This\nsuggests that complex contextual embeddings may not provide advantages over\nsimpler word2vec models for this task when a limited number (e.g. 48K) of data\nsamples are used to train the bidirectional transformer-based models. The\ncomparative results provide novel insights into selecting optimal embeddings\nfor learning compiler-independent semantic code representations to advance\nmachine learning detection of vulnerabilities in compiled binaries.\n", "link": "http://arxiv.org/abs/2405.20611v3", "date": "2024-09-27", "relevancy": 2.4482, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bi-Directional%20Transformers%20vs.%20word2vec%3A%20Discovering%20Vulnerabilities%20in%0A%20%20Lifted%20Compiled%20Code&body=Title%3A%20Bi-Directional%20Transformers%20vs.%20word2vec%3A%20Discovering%20Vulnerabilities%20in%0A%20%20Lifted%20Compiled%20Code%0AAuthor%3A%20Gary%20A.%20McCully%20and%20John%20D.%20Hastings%20and%20Shengjie%20Xu%20and%20Adam%20Fortier%0AAbstract%3A%20%20%20Detecting%20vulnerabilities%20within%20compiled%20binaries%20is%20challenging%20due%20to%20lost%0Ahigh-level%20code%20structures%20and%20other%20factors%20such%20as%20architectural%0Adependencies%2C%20compilers%2C%20and%20optimization%20options.%20To%20address%20these%20obstacles%2C%0Athis%20research%20explores%20vulnerability%20detection%20using%20natural%20language%0Aprocessing%20%28NLP%29%20embedding%20techniques%20with%20word2vec%2C%20BERT%2C%20and%20RoBERTa%20to%20learn%0Asemantics%20from%20intermediate%20representation%20%28LLVM%20IR%29%20code.%20Long%20short-term%0Amemory%20%28LSTM%29%20neural%20networks%20were%20trained%20on%20embeddings%20from%20encoders%20created%0Ausing%20approximately%2048k%20LLVM%20functions%20from%20the%20Juliet%20dataset.%20This%20study%20is%0Apioneering%20in%20its%20comparison%20of%20word2vec%20models%20with%20multiple%20bidirectional%0Atransformers%20%28BERT%2C%20RoBERTa%29%20embeddings%20built%20using%20LLVM%20code%20to%20train%20neural%0Anetworks%20to%20detect%20vulnerabilities%20in%20compiled%20binaries.%20Word2vec%20Skip-Gram%0Amodels%20achieved%2092%25%20validation%20accuracy%20in%20detecting%20vulnerabilities%2C%0Aoutperforming%20word2vec%20Continuous%20Bag%20of%20Words%20%28CBOW%29%2C%20BERT%2C%20and%20RoBERTa.%20This%0Asuggests%20that%20complex%20contextual%20embeddings%20may%20not%20provide%20advantages%20over%0Asimpler%20word2vec%20models%20for%20this%20task%20when%20a%20limited%20number%20%28e.g.%2048K%29%20of%20data%0Asamples%20are%20used%20to%20train%20the%20bidirectional%20transformer-based%20models.%20The%0Acomparative%20results%20provide%20novel%20insights%20into%20selecting%20optimal%20embeddings%0Afor%20learning%20compiler-independent%20semantic%20code%20representations%20to%20advance%0Amachine%20learning%20detection%20of%20vulnerabilities%20in%20compiled%20binaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20611v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBi-Directional%2520Transformers%2520vs.%2520word2vec%253A%2520Discovering%2520Vulnerabilities%2520in%250A%2520%2520Lifted%2520Compiled%2520Code%26entry.906535625%3DGary%2520A.%2520McCully%2520and%2520John%2520D.%2520Hastings%2520and%2520Shengjie%2520Xu%2520and%2520Adam%2520Fortier%26entry.1292438233%3D%2520%2520Detecting%2520vulnerabilities%2520within%2520compiled%2520binaries%2520is%2520challenging%2520due%2520to%2520lost%250Ahigh-level%2520code%2520structures%2520and%2520other%2520factors%2520such%2520as%2520architectural%250Adependencies%252C%2520compilers%252C%2520and%2520optimization%2520options.%2520To%2520address%2520these%2520obstacles%252C%250Athis%2520research%2520explores%2520vulnerability%2520detection%2520using%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%2520embedding%2520techniques%2520with%2520word2vec%252C%2520BERT%252C%2520and%2520RoBERTa%2520to%2520learn%250Asemantics%2520from%2520intermediate%2520representation%2520%2528LLVM%2520IR%2529%2520code.%2520Long%2520short-term%250Amemory%2520%2528LSTM%2529%2520neural%2520networks%2520were%2520trained%2520on%2520embeddings%2520from%2520encoders%2520created%250Ausing%2520approximately%252048k%2520LLVM%2520functions%2520from%2520the%2520Juliet%2520dataset.%2520This%2520study%2520is%250Apioneering%2520in%2520its%2520comparison%2520of%2520word2vec%2520models%2520with%2520multiple%2520bidirectional%250Atransformers%2520%2528BERT%252C%2520RoBERTa%2529%2520embeddings%2520built%2520using%2520LLVM%2520code%2520to%2520train%2520neural%250Anetworks%2520to%2520detect%2520vulnerabilities%2520in%2520compiled%2520binaries.%2520Word2vec%2520Skip-Gram%250Amodels%2520achieved%252092%2525%2520validation%2520accuracy%2520in%2520detecting%2520vulnerabilities%252C%250Aoutperforming%2520word2vec%2520Continuous%2520Bag%2520of%2520Words%2520%2528CBOW%2529%252C%2520BERT%252C%2520and%2520RoBERTa.%2520This%250Asuggests%2520that%2520complex%2520contextual%2520embeddings%2520may%2520not%2520provide%2520advantages%2520over%250Asimpler%2520word2vec%2520models%2520for%2520this%2520task%2520when%2520a%2520limited%2520number%2520%2528e.g.%252048K%2529%2520of%2520data%250Asamples%2520are%2520used%2520to%2520train%2520the%2520bidirectional%2520transformer-based%2520models.%2520The%250Acomparative%2520results%2520provide%2520novel%2520insights%2520into%2520selecting%2520optimal%2520embeddings%250Afor%2520learning%2520compiler-independent%2520semantic%2520code%2520representations%2520to%2520advance%250Amachine%2520learning%2520detection%2520of%2520vulnerabilities%2520in%2520compiled%2520binaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20611v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bi-Directional%20Transformers%20vs.%20word2vec%3A%20Discovering%20Vulnerabilities%20in%0A%20%20Lifted%20Compiled%20Code&entry.906535625=Gary%20A.%20McCully%20and%20John%20D.%20Hastings%20and%20Shengjie%20Xu%20and%20Adam%20Fortier&entry.1292438233=%20%20Detecting%20vulnerabilities%20within%20compiled%20binaries%20is%20challenging%20due%20to%20lost%0Ahigh-level%20code%20structures%20and%20other%20factors%20such%20as%20architectural%0Adependencies%2C%20compilers%2C%20and%20optimization%20options.%20To%20address%20these%20obstacles%2C%0Athis%20research%20explores%20vulnerability%20detection%20using%20natural%20language%0Aprocessing%20%28NLP%29%20embedding%20techniques%20with%20word2vec%2C%20BERT%2C%20and%20RoBERTa%20to%20learn%0Asemantics%20from%20intermediate%20representation%20%28LLVM%20IR%29%20code.%20Long%20short-term%0Amemory%20%28LSTM%29%20neural%20networks%20were%20trained%20on%20embeddings%20from%20encoders%20created%0Ausing%20approximately%2048k%20LLVM%20functions%20from%20the%20Juliet%20dataset.%20This%20study%20is%0Apioneering%20in%20its%20comparison%20of%20word2vec%20models%20with%20multiple%20bidirectional%0Atransformers%20%28BERT%2C%20RoBERTa%29%20embeddings%20built%20using%20LLVM%20code%20to%20train%20neural%0Anetworks%20to%20detect%20vulnerabilities%20in%20compiled%20binaries.%20Word2vec%20Skip-Gram%0Amodels%20achieved%2092%25%20validation%20accuracy%20in%20detecting%20vulnerabilities%2C%0Aoutperforming%20word2vec%20Continuous%20Bag%20of%20Words%20%28CBOW%29%2C%20BERT%2C%20and%20RoBERTa.%20This%0Asuggests%20that%20complex%20contextual%20embeddings%20may%20not%20provide%20advantages%20over%0Asimpler%20word2vec%20models%20for%20this%20task%20when%20a%20limited%20number%20%28e.g.%2048K%29%20of%20data%0Asamples%20are%20used%20to%20train%20the%20bidirectional%20transformer-based%20models.%20The%0Acomparative%20results%20provide%20novel%20insights%20into%20selecting%20optimal%20embeddings%0Afor%20learning%20compiler-independent%20semantic%20code%20representations%20to%20advance%0Amachine%20learning%20detection%20of%20vulnerabilities%20in%20compiled%20binaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20611v3&entry.124074799=Read"},
{"title": "Cluster Exploration using Informative Manifold Projections", "author": "Stavros Gerolymatos and Xenophon Evangelopoulos and Vladimir Gusev and John Y. Goulermas", "abstract": "  Dimensionality reduction (DR) is one of the key tools for the visual\nexploration of high-dimensional data and uncovering its cluster structure in\ntwo- or three-dimensional spaces. The vast majority of DR methods in the\nliterature do not take into account any prior knowledge a practitioner may have\nregarding the dataset under consideration. We propose a novel method to\ngenerate informative embeddings which not only factor out the structure\nassociated with different kinds of prior knowledge but also aim to reveal any\nremaining underlying structure. To achieve this, we employ a linear combination\nof two objectives: firstly, contrastive PCA that discounts the structure\nassociated with the prior information, and secondly, kurtosis projection\npursuit which ensures meaningful data separation in the obtained embeddings. We\nformulate this task as a manifold optimization problem and validate it\nempirically across a variety of datasets considering three distinct types of\nprior knowledge. Lastly, we provide an automated framework to perform iterative\nvisual exploration of high-dimensional data.\n", "link": "http://arxiv.org/abs/2309.14857v3", "date": "2024-09-27", "relevancy": 2.4288, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster%20Exploration%20using%20Informative%20Manifold%20Projections&body=Title%3A%20Cluster%20Exploration%20using%20Informative%20Manifold%20Projections%0AAuthor%3A%20Stavros%20Gerolymatos%20and%20Xenophon%20Evangelopoulos%20and%20Vladimir%20Gusev%20and%20John%20Y.%20Goulermas%0AAbstract%3A%20%20%20Dimensionality%20reduction%20%28DR%29%20is%20one%20of%20the%20key%20tools%20for%20the%20visual%0Aexploration%20of%20high-dimensional%20data%20and%20uncovering%20its%20cluster%20structure%20in%0Atwo-%20or%20three-dimensional%20spaces.%20The%20vast%20majority%20of%20DR%20methods%20in%20the%0Aliterature%20do%20not%20take%20into%20account%20any%20prior%20knowledge%20a%20practitioner%20may%20have%0Aregarding%20the%20dataset%20under%20consideration.%20We%20propose%20a%20novel%20method%20to%0Agenerate%20informative%20embeddings%20which%20not%20only%20factor%20out%20the%20structure%0Aassociated%20with%20different%20kinds%20of%20prior%20knowledge%20but%20also%20aim%20to%20reveal%20any%0Aremaining%20underlying%20structure.%20To%20achieve%20this%2C%20we%20employ%20a%20linear%20combination%0Aof%20two%20objectives%3A%20firstly%2C%20contrastive%20PCA%20that%20discounts%20the%20structure%0Aassociated%20with%20the%20prior%20information%2C%20and%20secondly%2C%20kurtosis%20projection%0Apursuit%20which%20ensures%20meaningful%20data%20separation%20in%20the%20obtained%20embeddings.%20We%0Aformulate%20this%20task%20as%20a%20manifold%20optimization%20problem%20and%20validate%20it%0Aempirically%20across%20a%20variety%20of%20datasets%20considering%20three%20distinct%20types%20of%0Aprior%20knowledge.%20Lastly%2C%20we%20provide%20an%20automated%20framework%20to%20perform%20iterative%0Avisual%20exploration%20of%20high-dimensional%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14857v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster%2520Exploration%2520using%2520Informative%2520Manifold%2520Projections%26entry.906535625%3DStavros%2520Gerolymatos%2520and%2520Xenophon%2520Evangelopoulos%2520and%2520Vladimir%2520Gusev%2520and%2520John%2520Y.%2520Goulermas%26entry.1292438233%3D%2520%2520Dimensionality%2520reduction%2520%2528DR%2529%2520is%2520one%2520of%2520the%2520key%2520tools%2520for%2520the%2520visual%250Aexploration%2520of%2520high-dimensional%2520data%2520and%2520uncovering%2520its%2520cluster%2520structure%2520in%250Atwo-%2520or%2520three-dimensional%2520spaces.%2520The%2520vast%2520majority%2520of%2520DR%2520methods%2520in%2520the%250Aliterature%2520do%2520not%2520take%2520into%2520account%2520any%2520prior%2520knowledge%2520a%2520practitioner%2520may%2520have%250Aregarding%2520the%2520dataset%2520under%2520consideration.%2520We%2520propose%2520a%2520novel%2520method%2520to%250Agenerate%2520informative%2520embeddings%2520which%2520not%2520only%2520factor%2520out%2520the%2520structure%250Aassociated%2520with%2520different%2520kinds%2520of%2520prior%2520knowledge%2520but%2520also%2520aim%2520to%2520reveal%2520any%250Aremaining%2520underlying%2520structure.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520a%2520linear%2520combination%250Aof%2520two%2520objectives%253A%2520firstly%252C%2520contrastive%2520PCA%2520that%2520discounts%2520the%2520structure%250Aassociated%2520with%2520the%2520prior%2520information%252C%2520and%2520secondly%252C%2520kurtosis%2520projection%250Apursuit%2520which%2520ensures%2520meaningful%2520data%2520separation%2520in%2520the%2520obtained%2520embeddings.%2520We%250Aformulate%2520this%2520task%2520as%2520a%2520manifold%2520optimization%2520problem%2520and%2520validate%2520it%250Aempirically%2520across%2520a%2520variety%2520of%2520datasets%2520considering%2520three%2520distinct%2520types%2520of%250Aprior%2520knowledge.%2520Lastly%252C%2520we%2520provide%2520an%2520automated%2520framework%2520to%2520perform%2520iterative%250Avisual%2520exploration%2520of%2520high-dimensional%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14857v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster%20Exploration%20using%20Informative%20Manifold%20Projections&entry.906535625=Stavros%20Gerolymatos%20and%20Xenophon%20Evangelopoulos%20and%20Vladimir%20Gusev%20and%20John%20Y.%20Goulermas&entry.1292438233=%20%20Dimensionality%20reduction%20%28DR%29%20is%20one%20of%20the%20key%20tools%20for%20the%20visual%0Aexploration%20of%20high-dimensional%20data%20and%20uncovering%20its%20cluster%20structure%20in%0Atwo-%20or%20three-dimensional%20spaces.%20The%20vast%20majority%20of%20DR%20methods%20in%20the%0Aliterature%20do%20not%20take%20into%20account%20any%20prior%20knowledge%20a%20practitioner%20may%20have%0Aregarding%20the%20dataset%20under%20consideration.%20We%20propose%20a%20novel%20method%20to%0Agenerate%20informative%20embeddings%20which%20not%20only%20factor%20out%20the%20structure%0Aassociated%20with%20different%20kinds%20of%20prior%20knowledge%20but%20also%20aim%20to%20reveal%20any%0Aremaining%20underlying%20structure.%20To%20achieve%20this%2C%20we%20employ%20a%20linear%20combination%0Aof%20two%20objectives%3A%20firstly%2C%20contrastive%20PCA%20that%20discounts%20the%20structure%0Aassociated%20with%20the%20prior%20information%2C%20and%20secondly%2C%20kurtosis%20projection%0Apursuit%20which%20ensures%20meaningful%20data%20separation%20in%20the%20obtained%20embeddings.%20We%0Aformulate%20this%20task%20as%20a%20manifold%20optimization%20problem%20and%20validate%20it%0Aempirically%20across%20a%20variety%20of%20datasets%20considering%20three%20distinct%20types%20of%0Aprior%20knowledge.%20Lastly%2C%20we%20provide%20an%20automated%20framework%20to%20perform%20iterative%0Avisual%20exploration%20of%20high-dimensional%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14857v3&entry.124074799=Read"},
{"title": "Hierarchical Federated ADMM", "author": "Seyed Mohammad Azimi-Abarghouyi and Nicola Bastianello and Karl H. Johansson and Viktoria Fodor", "abstract": "  In this paper, we depart from the widely-used gradient descent-based\nhierarchical federated learning (FL) algorithms to develop a novel hierarchical\nFL framework based on the alternating direction method of multipliers (ADMM).\nWithin this framework, we propose two novel FL algorithms, which both use ADMM\nin the top layer: one that employs ADMM in the lower layer and another that\nuses the conventional gradient descent-based approach. The proposed framework\nenhances privacy, and experiments demonstrate the superiority of the proposed\nalgorithms compared to the conventional algorithms in terms of learning\nconvergence and accuracy. Additionally, gradient descent on the lower layer\nperforms well even if the number of local steps is very limited, while ADMM on\nboth layers lead to better performance otherwise.\n", "link": "http://arxiv.org/abs/2409.18796v1", "date": "2024-09-27", "relevancy": 2.4016, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4814}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4807}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Federated%20ADMM&body=Title%3A%20Hierarchical%20Federated%20ADMM%0AAuthor%3A%20Seyed%20Mohammad%20Azimi-Abarghouyi%20and%20Nicola%20Bastianello%20and%20Karl%20H.%20Johansson%20and%20Viktoria%20Fodor%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20depart%20from%20the%20widely-used%20gradient%20descent-based%0Ahierarchical%20federated%20learning%20%28FL%29%20algorithms%20to%20develop%20a%20novel%20hierarchical%0AFL%20framework%20based%20on%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29.%0AWithin%20this%20framework%2C%20we%20propose%20two%20novel%20FL%20algorithms%2C%20which%20both%20use%20ADMM%0Ain%20the%20top%20layer%3A%20one%20that%20employs%20ADMM%20in%20the%20lower%20layer%20and%20another%20that%0Auses%20the%20conventional%20gradient%20descent-based%20approach.%20The%20proposed%20framework%0Aenhances%20privacy%2C%20and%20experiments%20demonstrate%20the%20superiority%20of%20the%20proposed%0Aalgorithms%20compared%20to%20the%20conventional%20algorithms%20in%20terms%20of%20learning%0Aconvergence%20and%20accuracy.%20Additionally%2C%20gradient%20descent%20on%20the%20lower%20layer%0Aperforms%20well%20even%20if%20the%20number%20of%20local%20steps%20is%20very%20limited%2C%20while%20ADMM%20on%0Aboth%20layers%20lead%20to%20better%20performance%20otherwise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Federated%2520ADMM%26entry.906535625%3DSeyed%2520Mohammad%2520Azimi-Abarghouyi%2520and%2520Nicola%2520Bastianello%2520and%2520Karl%2520H.%2520Johansson%2520and%2520Viktoria%2520Fodor%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520depart%2520from%2520the%2520widely-used%2520gradient%2520descent-based%250Ahierarchical%2520federated%2520learning%2520%2528FL%2529%2520algorithms%2520to%2520develop%2520a%2520novel%2520hierarchical%250AFL%2520framework%2520based%2520on%2520the%2520alternating%2520direction%2520method%2520of%2520multipliers%2520%2528ADMM%2529.%250AWithin%2520this%2520framework%252C%2520we%2520propose%2520two%2520novel%2520FL%2520algorithms%252C%2520which%2520both%2520use%2520ADMM%250Ain%2520the%2520top%2520layer%253A%2520one%2520that%2520employs%2520ADMM%2520in%2520the%2520lower%2520layer%2520and%2520another%2520that%250Auses%2520the%2520conventional%2520gradient%2520descent-based%2520approach.%2520The%2520proposed%2520framework%250Aenhances%2520privacy%252C%2520and%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%250Aalgorithms%2520compared%2520to%2520the%2520conventional%2520algorithms%2520in%2520terms%2520of%2520learning%250Aconvergence%2520and%2520accuracy.%2520Additionally%252C%2520gradient%2520descent%2520on%2520the%2520lower%2520layer%250Aperforms%2520well%2520even%2520if%2520the%2520number%2520of%2520local%2520steps%2520is%2520very%2520limited%252C%2520while%2520ADMM%2520on%250Aboth%2520layers%2520lead%2520to%2520better%2520performance%2520otherwise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Federated%20ADMM&entry.906535625=Seyed%20Mohammad%20Azimi-Abarghouyi%20and%20Nicola%20Bastianello%20and%20Karl%20H.%20Johansson%20and%20Viktoria%20Fodor&entry.1292438233=%20%20In%20this%20paper%2C%20we%20depart%20from%20the%20widely-used%20gradient%20descent-based%0Ahierarchical%20federated%20learning%20%28FL%29%20algorithms%20to%20develop%20a%20novel%20hierarchical%0AFL%20framework%20based%20on%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29.%0AWithin%20this%20framework%2C%20we%20propose%20two%20novel%20FL%20algorithms%2C%20which%20both%20use%20ADMM%0Ain%20the%20top%20layer%3A%20one%20that%20employs%20ADMM%20in%20the%20lower%20layer%20and%20another%20that%0Auses%20the%20conventional%20gradient%20descent-based%20approach.%20The%20proposed%20framework%0Aenhances%20privacy%2C%20and%20experiments%20demonstrate%20the%20superiority%20of%20the%20proposed%0Aalgorithms%20compared%20to%20the%20conventional%20algorithms%20in%20terms%20of%20learning%0Aconvergence%20and%20accuracy.%20Additionally%2C%20gradient%20descent%20on%20the%20lower%20layer%0Aperforms%20well%20even%20if%20the%20number%20of%20local%20steps%20is%20very%20limited%2C%20while%20ADMM%20on%0Aboth%20layers%20lead%20to%20better%20performance%20otherwise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18796v1&entry.124074799=Read"},
{"title": "Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion\n  Estimation", "author": "Pawe\u0142 A. Pierzchlewicz and Caio O. da Silva and R. James Cotton and Fabian H. Sinz", "abstract": "  Single camera 3D pose estimation is an ill-defined problem due to inherent\nambiguities from depth, occlusion or keypoint noise. Multi-hypothesis pose\nestimation accounts for this uncertainty by providing multiple 3D poses\nconsistent with the 2D measurements. Current research has predominantly\nconcentrated on generating multiple hypotheses for single frame static pose\nestimation or single hypothesis motion estimation. In this study we focus on\nthe new task of multi-hypothesis motion estimation. Multi-hypothesis motion\nestimation is not simply multi-hypothesis pose estimation applied to multiple\nframes, which would ignore temporal correlation across frames. Instead, it\nrequires distributions which are capable of generating temporally consistent\nsamples, which is significantly more challenging than multi-hypothesis pose\nestimation or single-hypothesis motion estimation. To this end, we introduce\nPlatypose, a framework that uses a diffusion model pretrained on 3D human\nmotion sequences for zero-shot 3D pose sequence estimation. Platypose\noutperforms baseline methods on multiple hypotheses for motion estimation.\nAdditionally, Platypose also achieves state-of-the-art calibration and\ncompetitive joint error when tested on static poses from Human3.6M,\nMPI-INF-3DHP and 3DPW. Finally, because it is zero-shot, our method generalizes\nflexibly to different settings such as multi-camera inference.\n", "link": "http://arxiv.org/abs/2403.06164v2", "date": "2024-09-27", "relevancy": 2.3944, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6205}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5991}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Platypose%3A%20Calibrated%20Zero-Shot%20Multi-Hypothesis%203D%20Human%20Motion%0A%20%20Estimation&body=Title%3A%20Platypose%3A%20Calibrated%20Zero-Shot%20Multi-Hypothesis%203D%20Human%20Motion%0A%20%20Estimation%0AAuthor%3A%20Pawe%C5%82%20A.%20Pierzchlewicz%20and%20Caio%20O.%20da%20Silva%20and%20R.%20James%20Cotton%20and%20Fabian%20H.%20Sinz%0AAbstract%3A%20%20%20Single%20camera%203D%20pose%20estimation%20is%20an%20ill-defined%20problem%20due%20to%20inherent%0Aambiguities%20from%20depth%2C%20occlusion%20or%20keypoint%20noise.%20Multi-hypothesis%20pose%0Aestimation%20accounts%20for%20this%20uncertainty%20by%20providing%20multiple%203D%20poses%0Aconsistent%20with%20the%202D%20measurements.%20Current%20research%20has%20predominantly%0Aconcentrated%20on%20generating%20multiple%20hypotheses%20for%20single%20frame%20static%20pose%0Aestimation%20or%20single%20hypothesis%20motion%20estimation.%20In%20this%20study%20we%20focus%20on%0Athe%20new%20task%20of%20multi-hypothesis%20motion%20estimation.%20Multi-hypothesis%20motion%0Aestimation%20is%20not%20simply%20multi-hypothesis%20pose%20estimation%20applied%20to%20multiple%0Aframes%2C%20which%20would%20ignore%20temporal%20correlation%20across%20frames.%20Instead%2C%20it%0Arequires%20distributions%20which%20are%20capable%20of%20generating%20temporally%20consistent%0Asamples%2C%20which%20is%20significantly%20more%20challenging%20than%20multi-hypothesis%20pose%0Aestimation%20or%20single-hypothesis%20motion%20estimation.%20To%20this%20end%2C%20we%20introduce%0APlatypose%2C%20a%20framework%20that%20uses%20a%20diffusion%20model%20pretrained%20on%203D%20human%0Amotion%20sequences%20for%20zero-shot%203D%20pose%20sequence%20estimation.%20Platypose%0Aoutperforms%20baseline%20methods%20on%20multiple%20hypotheses%20for%20motion%20estimation.%0AAdditionally%2C%20Platypose%20also%20achieves%20state-of-the-art%20calibration%20and%0Acompetitive%20joint%20error%20when%20tested%20on%20static%20poses%20from%20Human3.6M%2C%0AMPI-INF-3DHP%20and%203DPW.%20Finally%2C%20because%20it%20is%20zero-shot%2C%20our%20method%20generalizes%0Aflexibly%20to%20different%20settings%20such%20as%20multi-camera%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06164v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlatypose%253A%2520Calibrated%2520Zero-Shot%2520Multi-Hypothesis%25203D%2520Human%2520Motion%250A%2520%2520Estimation%26entry.906535625%3DPawe%25C5%2582%2520A.%2520Pierzchlewicz%2520and%2520Caio%2520O.%2520da%2520Silva%2520and%2520R.%2520James%2520Cotton%2520and%2520Fabian%2520H.%2520Sinz%26entry.1292438233%3D%2520%2520Single%2520camera%25203D%2520pose%2520estimation%2520is%2520an%2520ill-defined%2520problem%2520due%2520to%2520inherent%250Aambiguities%2520from%2520depth%252C%2520occlusion%2520or%2520keypoint%2520noise.%2520Multi-hypothesis%2520pose%250Aestimation%2520accounts%2520for%2520this%2520uncertainty%2520by%2520providing%2520multiple%25203D%2520poses%250Aconsistent%2520with%2520the%25202D%2520measurements.%2520Current%2520research%2520has%2520predominantly%250Aconcentrated%2520on%2520generating%2520multiple%2520hypotheses%2520for%2520single%2520frame%2520static%2520pose%250Aestimation%2520or%2520single%2520hypothesis%2520motion%2520estimation.%2520In%2520this%2520study%2520we%2520focus%2520on%250Athe%2520new%2520task%2520of%2520multi-hypothesis%2520motion%2520estimation.%2520Multi-hypothesis%2520motion%250Aestimation%2520is%2520not%2520simply%2520multi-hypothesis%2520pose%2520estimation%2520applied%2520to%2520multiple%250Aframes%252C%2520which%2520would%2520ignore%2520temporal%2520correlation%2520across%2520frames.%2520Instead%252C%2520it%250Arequires%2520distributions%2520which%2520are%2520capable%2520of%2520generating%2520temporally%2520consistent%250Asamples%252C%2520which%2520is%2520significantly%2520more%2520challenging%2520than%2520multi-hypothesis%2520pose%250Aestimation%2520or%2520single-hypothesis%2520motion%2520estimation.%2520To%2520this%2520end%252C%2520we%2520introduce%250APlatypose%252C%2520a%2520framework%2520that%2520uses%2520a%2520diffusion%2520model%2520pretrained%2520on%25203D%2520human%250Amotion%2520sequences%2520for%2520zero-shot%25203D%2520pose%2520sequence%2520estimation.%2520Platypose%250Aoutperforms%2520baseline%2520methods%2520on%2520multiple%2520hypotheses%2520for%2520motion%2520estimation.%250AAdditionally%252C%2520Platypose%2520also%2520achieves%2520state-of-the-art%2520calibration%2520and%250Acompetitive%2520joint%2520error%2520when%2520tested%2520on%2520static%2520poses%2520from%2520Human3.6M%252C%250AMPI-INF-3DHP%2520and%25203DPW.%2520Finally%252C%2520because%2520it%2520is%2520zero-shot%252C%2520our%2520method%2520generalizes%250Aflexibly%2520to%2520different%2520settings%2520such%2520as%2520multi-camera%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06164v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Platypose%3A%20Calibrated%20Zero-Shot%20Multi-Hypothesis%203D%20Human%20Motion%0A%20%20Estimation&entry.906535625=Pawe%C5%82%20A.%20Pierzchlewicz%20and%20Caio%20O.%20da%20Silva%20and%20R.%20James%20Cotton%20and%20Fabian%20H.%20Sinz&entry.1292438233=%20%20Single%20camera%203D%20pose%20estimation%20is%20an%20ill-defined%20problem%20due%20to%20inherent%0Aambiguities%20from%20depth%2C%20occlusion%20or%20keypoint%20noise.%20Multi-hypothesis%20pose%0Aestimation%20accounts%20for%20this%20uncertainty%20by%20providing%20multiple%203D%20poses%0Aconsistent%20with%20the%202D%20measurements.%20Current%20research%20has%20predominantly%0Aconcentrated%20on%20generating%20multiple%20hypotheses%20for%20single%20frame%20static%20pose%0Aestimation%20or%20single%20hypothesis%20motion%20estimation.%20In%20this%20study%20we%20focus%20on%0Athe%20new%20task%20of%20multi-hypothesis%20motion%20estimation.%20Multi-hypothesis%20motion%0Aestimation%20is%20not%20simply%20multi-hypothesis%20pose%20estimation%20applied%20to%20multiple%0Aframes%2C%20which%20would%20ignore%20temporal%20correlation%20across%20frames.%20Instead%2C%20it%0Arequires%20distributions%20which%20are%20capable%20of%20generating%20temporally%20consistent%0Asamples%2C%20which%20is%20significantly%20more%20challenging%20than%20multi-hypothesis%20pose%0Aestimation%20or%20single-hypothesis%20motion%20estimation.%20To%20this%20end%2C%20we%20introduce%0APlatypose%2C%20a%20framework%20that%20uses%20a%20diffusion%20model%20pretrained%20on%203D%20human%0Amotion%20sequences%20for%20zero-shot%203D%20pose%20sequence%20estimation.%20Platypose%0Aoutperforms%20baseline%20methods%20on%20multiple%20hypotheses%20for%20motion%20estimation.%0AAdditionally%2C%20Platypose%20also%20achieves%20state-of-the-art%20calibration%20and%0Acompetitive%20joint%20error%20when%20tested%20on%20static%20poses%20from%20Human3.6M%2C%0AMPI-INF-3DHP%20and%203DPW.%20Finally%2C%20because%20it%20is%20zero-shot%2C%20our%20method%20generalizes%0Aflexibly%20to%20different%20settings%20such%20as%20multi-camera%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06164v2&entry.124074799=Read"},
{"title": "SpaRED benchmark: Enhancing Gene Expression Prediction from Histology\n  Images with Spatial Transcriptomics Completion", "author": "Gabriel Mejia and Daniela Ruiz and Paula C\u00e1rdenas and Leonardo Manrique and Daniela Vega and Pablo Arbel\u00e1ez", "abstract": "  Spatial Transcriptomics is a novel technology that aligns histology images\nwith spatially resolved gene expression profiles. Although groundbreaking, it\nstruggles with gene capture yielding high corruption in acquired data. Given\npotential applications, recent efforts have focused on predicting\ntranscriptomic profiles solely from histology images. However, differences in\ndatabases, preprocessing techniques, and training hyperparameters hinder a fair\ncomparison between methods. To address these challenges, we present a\nsystematically curated and processed database collected from 26 public sources,\nrepresenting an 8.6-fold increase compared to previous works. Additionally, we\npropose a state-of-the-art transformer based completion technique for inferring\nmissing gene expression, which significantly boosts the performance of\ntranscriptomic profile predictions across all datasets. Altogether, our\ncontributions constitute the most comprehensive benchmark of gene expression\nprediction from histology images to date and a stepping stone for future\nresearch on spatial transcriptomics.\n", "link": "http://arxiv.org/abs/2407.13027v2", "date": "2024-09-27", "relevancy": 2.3905, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5039}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4769}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaRED%20benchmark%3A%20Enhancing%20Gene%20Expression%20Prediction%20from%20Histology%0A%20%20Images%20with%20Spatial%20Transcriptomics%20Completion&body=Title%3A%20SpaRED%20benchmark%3A%20Enhancing%20Gene%20Expression%20Prediction%20from%20Histology%0A%20%20Images%20with%20Spatial%20Transcriptomics%20Completion%0AAuthor%3A%20Gabriel%20Mejia%20and%20Daniela%20Ruiz%20and%20Paula%20C%C3%A1rdenas%20and%20Leonardo%20Manrique%20and%20Daniela%20Vega%20and%20Pablo%20Arbel%C3%A1ez%0AAbstract%3A%20%20%20Spatial%20Transcriptomics%20is%20a%20novel%20technology%20that%20aligns%20histology%20images%0Awith%20spatially%20resolved%20gene%20expression%20profiles.%20Although%20groundbreaking%2C%20it%0Astruggles%20with%20gene%20capture%20yielding%20high%20corruption%20in%20acquired%20data.%20Given%0Apotential%20applications%2C%20recent%20efforts%20have%20focused%20on%20predicting%0Atranscriptomic%20profiles%20solely%20from%20histology%20images.%20However%2C%20differences%20in%0Adatabases%2C%20preprocessing%20techniques%2C%20and%20training%20hyperparameters%20hinder%20a%20fair%0Acomparison%20between%20methods.%20To%20address%20these%20challenges%2C%20we%20present%20a%0Asystematically%20curated%20and%20processed%20database%20collected%20from%2026%20public%20sources%2C%0Arepresenting%20an%208.6-fold%20increase%20compared%20to%20previous%20works.%20Additionally%2C%20we%0Apropose%20a%20state-of-the-art%20transformer%20based%20completion%20technique%20for%20inferring%0Amissing%20gene%20expression%2C%20which%20significantly%20boosts%20the%20performance%20of%0Atranscriptomic%20profile%20predictions%20across%20all%20datasets.%20Altogether%2C%20our%0Acontributions%20constitute%20the%20most%20comprehensive%20benchmark%20of%20gene%20expression%0Aprediction%20from%20histology%20images%20to%20date%20and%20a%20stepping%20stone%20for%20future%0Aresearch%20on%20spatial%20transcriptomics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaRED%2520benchmark%253A%2520Enhancing%2520Gene%2520Expression%2520Prediction%2520from%2520Histology%250A%2520%2520Images%2520with%2520Spatial%2520Transcriptomics%2520Completion%26entry.906535625%3DGabriel%2520Mejia%2520and%2520Daniela%2520Ruiz%2520and%2520Paula%2520C%25C3%25A1rdenas%2520and%2520Leonardo%2520Manrique%2520and%2520Daniela%2520Vega%2520and%2520Pablo%2520Arbel%25C3%25A1ez%26entry.1292438233%3D%2520%2520Spatial%2520Transcriptomics%2520is%2520a%2520novel%2520technology%2520that%2520aligns%2520histology%2520images%250Awith%2520spatially%2520resolved%2520gene%2520expression%2520profiles.%2520Although%2520groundbreaking%252C%2520it%250Astruggles%2520with%2520gene%2520capture%2520yielding%2520high%2520corruption%2520in%2520acquired%2520data.%2520Given%250Apotential%2520applications%252C%2520recent%2520efforts%2520have%2520focused%2520on%2520predicting%250Atranscriptomic%2520profiles%2520solely%2520from%2520histology%2520images.%2520However%252C%2520differences%2520in%250Adatabases%252C%2520preprocessing%2520techniques%252C%2520and%2520training%2520hyperparameters%2520hinder%2520a%2520fair%250Acomparison%2520between%2520methods.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520a%250Asystematically%2520curated%2520and%2520processed%2520database%2520collected%2520from%252026%2520public%2520sources%252C%250Arepresenting%2520an%25208.6-fold%2520increase%2520compared%2520to%2520previous%2520works.%2520Additionally%252C%2520we%250Apropose%2520a%2520state-of-the-art%2520transformer%2520based%2520completion%2520technique%2520for%2520inferring%250Amissing%2520gene%2520expression%252C%2520which%2520significantly%2520boosts%2520the%2520performance%2520of%250Atranscriptomic%2520profile%2520predictions%2520across%2520all%2520datasets.%2520Altogether%252C%2520our%250Acontributions%2520constitute%2520the%2520most%2520comprehensive%2520benchmark%2520of%2520gene%2520expression%250Aprediction%2520from%2520histology%2520images%2520to%2520date%2520and%2520a%2520stepping%2520stone%2520for%2520future%250Aresearch%2520on%2520spatial%2520transcriptomics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaRED%20benchmark%3A%20Enhancing%20Gene%20Expression%20Prediction%20from%20Histology%0A%20%20Images%20with%20Spatial%20Transcriptomics%20Completion&entry.906535625=Gabriel%20Mejia%20and%20Daniela%20Ruiz%20and%20Paula%20C%C3%A1rdenas%20and%20Leonardo%20Manrique%20and%20Daniela%20Vega%20and%20Pablo%20Arbel%C3%A1ez&entry.1292438233=%20%20Spatial%20Transcriptomics%20is%20a%20novel%20technology%20that%20aligns%20histology%20images%0Awith%20spatially%20resolved%20gene%20expression%20profiles.%20Although%20groundbreaking%2C%20it%0Astruggles%20with%20gene%20capture%20yielding%20high%20corruption%20in%20acquired%20data.%20Given%0Apotential%20applications%2C%20recent%20efforts%20have%20focused%20on%20predicting%0Atranscriptomic%20profiles%20solely%20from%20histology%20images.%20However%2C%20differences%20in%0Adatabases%2C%20preprocessing%20techniques%2C%20and%20training%20hyperparameters%20hinder%20a%20fair%0Acomparison%20between%20methods.%20To%20address%20these%20challenges%2C%20we%20present%20a%0Asystematically%20curated%20and%20processed%20database%20collected%20from%2026%20public%20sources%2C%0Arepresenting%20an%208.6-fold%20increase%20compared%20to%20previous%20works.%20Additionally%2C%20we%0Apropose%20a%20state-of-the-art%20transformer%20based%20completion%20technique%20for%20inferring%0Amissing%20gene%20expression%2C%20which%20significantly%20boosts%20the%20performance%20of%0Atranscriptomic%20profile%20predictions%20across%20all%20datasets.%20Altogether%2C%20our%0Acontributions%20constitute%20the%20most%20comprehensive%20benchmark%20of%20gene%20expression%0Aprediction%20from%20histology%20images%20to%20date%20and%20a%20stepping%20stone%20for%20future%0Aresearch%20on%20spatial%20transcriptomics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13027v2&entry.124074799=Read"},
{"title": "Deep Bayesian Future Fusion for Self-Supervised, High-Resolution,\n  Off-Road Mapping", "author": "Shubhra Aich and Wenshan Wang and Parv Maheshwari and Matthew Sivaprakasam and Samuel Triest and Cherie Ho and Jason M. Gregory and John G. Rogers III and Sebastian Scherer", "abstract": "  High-speed off-road navigation requires long-range, high-resolution maps to\nenable robots to safely navigate over different surfaces while avoiding\ndangerous obstacles. However, due to limited computational power and sensing\nnoise, most approaches to off-road mapping focus on producing coarse (20-40cm)\nmaps of the environment. In this paper, we propose Future Fusion, a framework\ncapable of generating dense, high-resolution maps from sparse sensing data (30m\nforward at 2cm). This is accomplished by - (1) the efficient realization of the\nwell-known Bayes filtering within the standard deep learning models that\nexplicitly accounts for the sparsity pattern in stereo and LiDAR depth data,\nand (2) leveraging perceptual losses common in generative image completion. The\nproposed methodology outperforms the conventional baselines. Moreover, the\nlearned features and the completed dense maps lead to improvements in the\ndownstream navigation task.\n", "link": "http://arxiv.org/abs/2403.11876v2", "date": "2024-09-27", "relevancy": 2.3675, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6292}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Bayesian%20Future%20Fusion%20for%20Self-Supervised%2C%20High-Resolution%2C%0A%20%20Off-Road%20Mapping&body=Title%3A%20Deep%20Bayesian%20Future%20Fusion%20for%20Self-Supervised%2C%20High-Resolution%2C%0A%20%20Off-Road%20Mapping%0AAuthor%3A%20Shubhra%20Aich%20and%20Wenshan%20Wang%20and%20Parv%20Maheshwari%20and%20Matthew%20Sivaprakasam%20and%20Samuel%20Triest%20and%20Cherie%20Ho%20and%20Jason%20M.%20Gregory%20and%20John%20G.%20Rogers%20III%20and%20Sebastian%20Scherer%0AAbstract%3A%20%20%20High-speed%20off-road%20navigation%20requires%20long-range%2C%20high-resolution%20maps%20to%0Aenable%20robots%20to%20safely%20navigate%20over%20different%20surfaces%20while%20avoiding%0Adangerous%20obstacles.%20However%2C%20due%20to%20limited%20computational%20power%20and%20sensing%0Anoise%2C%20most%20approaches%20to%20off-road%20mapping%20focus%20on%20producing%20coarse%20%2820-40cm%29%0Amaps%20of%20the%20environment.%20In%20this%20paper%2C%20we%20propose%20Future%20Fusion%2C%20a%20framework%0Acapable%20of%20generating%20dense%2C%20high-resolution%20maps%20from%20sparse%20sensing%20data%20%2830m%0Aforward%20at%202cm%29.%20This%20is%20accomplished%20by%20-%20%281%29%20the%20efficient%20realization%20of%20the%0Awell-known%20Bayes%20filtering%20within%20the%20standard%20deep%20learning%20models%20that%0Aexplicitly%20accounts%20for%20the%20sparsity%20pattern%20in%20stereo%20and%20LiDAR%20depth%20data%2C%0Aand%20%282%29%20leveraging%20perceptual%20losses%20common%20in%20generative%20image%20completion.%20The%0Aproposed%20methodology%20outperforms%20the%20conventional%20baselines.%20Moreover%2C%20the%0Alearned%20features%20and%20the%20completed%20dense%20maps%20lead%20to%20improvements%20in%20the%0Adownstream%20navigation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Bayesian%2520Future%2520Fusion%2520for%2520Self-Supervised%252C%2520High-Resolution%252C%250A%2520%2520Off-Road%2520Mapping%26entry.906535625%3DShubhra%2520Aich%2520and%2520Wenshan%2520Wang%2520and%2520Parv%2520Maheshwari%2520and%2520Matthew%2520Sivaprakasam%2520and%2520Samuel%2520Triest%2520and%2520Cherie%2520Ho%2520and%2520Jason%2520M.%2520Gregory%2520and%2520John%2520G.%2520Rogers%2520III%2520and%2520Sebastian%2520Scherer%26entry.1292438233%3D%2520%2520High-speed%2520off-road%2520navigation%2520requires%2520long-range%252C%2520high-resolution%2520maps%2520to%250Aenable%2520robots%2520to%2520safely%2520navigate%2520over%2520different%2520surfaces%2520while%2520avoiding%250Adangerous%2520obstacles.%2520However%252C%2520due%2520to%2520limited%2520computational%2520power%2520and%2520sensing%250Anoise%252C%2520most%2520approaches%2520to%2520off-road%2520mapping%2520focus%2520on%2520producing%2520coarse%2520%252820-40cm%2529%250Amaps%2520of%2520the%2520environment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Future%2520Fusion%252C%2520a%2520framework%250Acapable%2520of%2520generating%2520dense%252C%2520high-resolution%2520maps%2520from%2520sparse%2520sensing%2520data%2520%252830m%250Aforward%2520at%25202cm%2529.%2520This%2520is%2520accomplished%2520by%2520-%2520%25281%2529%2520the%2520efficient%2520realization%2520of%2520the%250Awell-known%2520Bayes%2520filtering%2520within%2520the%2520standard%2520deep%2520learning%2520models%2520that%250Aexplicitly%2520accounts%2520for%2520the%2520sparsity%2520pattern%2520in%2520stereo%2520and%2520LiDAR%2520depth%2520data%252C%250Aand%2520%25282%2529%2520leveraging%2520perceptual%2520losses%2520common%2520in%2520generative%2520image%2520completion.%2520The%250Aproposed%2520methodology%2520outperforms%2520the%2520conventional%2520baselines.%2520Moreover%252C%2520the%250Alearned%2520features%2520and%2520the%2520completed%2520dense%2520maps%2520lead%2520to%2520improvements%2520in%2520the%250Adownstream%2520navigation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Bayesian%20Future%20Fusion%20for%20Self-Supervised%2C%20High-Resolution%2C%0A%20%20Off-Road%20Mapping&entry.906535625=Shubhra%20Aich%20and%20Wenshan%20Wang%20and%20Parv%20Maheshwari%20and%20Matthew%20Sivaprakasam%20and%20Samuel%20Triest%20and%20Cherie%20Ho%20and%20Jason%20M.%20Gregory%20and%20John%20G.%20Rogers%20III%20and%20Sebastian%20Scherer&entry.1292438233=%20%20High-speed%20off-road%20navigation%20requires%20long-range%2C%20high-resolution%20maps%20to%0Aenable%20robots%20to%20safely%20navigate%20over%20different%20surfaces%20while%20avoiding%0Adangerous%20obstacles.%20However%2C%20due%20to%20limited%20computational%20power%20and%20sensing%0Anoise%2C%20most%20approaches%20to%20off-road%20mapping%20focus%20on%20producing%20coarse%20%2820-40cm%29%0Amaps%20of%20the%20environment.%20In%20this%20paper%2C%20we%20propose%20Future%20Fusion%2C%20a%20framework%0Acapable%20of%20generating%20dense%2C%20high-resolution%20maps%20from%20sparse%20sensing%20data%20%2830m%0Aforward%20at%202cm%29.%20This%20is%20accomplished%20by%20-%20%281%29%20the%20efficient%20realization%20of%20the%0Awell-known%20Bayes%20filtering%20within%20the%20standard%20deep%20learning%20models%20that%0Aexplicitly%20accounts%20for%20the%20sparsity%20pattern%20in%20stereo%20and%20LiDAR%20depth%20data%2C%0Aand%20%282%29%20leveraging%20perceptual%20losses%20common%20in%20generative%20image%20completion.%20The%0Aproposed%20methodology%20outperforms%20the%20conventional%20baselines.%20Moreover%2C%20the%0Alearned%20features%20and%20the%20completed%20dense%20maps%20lead%20to%20improvements%20in%20the%0Adownstream%20navigation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11876v2&entry.124074799=Read"},
{"title": "EasyRec: Simple yet Effective Language Models for Recommendation", "author": "Xubin Ren and Chao Huang", "abstract": "  Deep neural networks have become a powerful technique for learning\nrepresentations from user-item interaction data in collaborative filtering (CF)\nfor recommender systems. However, many existing methods heavily rely on unique\nuser and item IDs, which limits their ability to perform well in practical\nzero-shot learning scenarios where sufficient training data may be unavailable.\nInspired by the success of language models (LMs) and their strong\ngeneralization capabilities, a crucial question arises: How can we harness the\npotential of language models to empower recommender systems and elevate its\ngeneralization capabilities to new heights? In this study, we propose EasyRec -\nan effective and easy-to-use approach that seamlessly integrates text-based\nsemantic understanding with collaborative signals. EasyRec employs a\ntext-behavior alignment framework, which combines contrastive learning with\ncollaborative language model tuning, to ensure a strong alignment between the\ntext-enhanced semantic space and the collaborative behavior information.\nExtensive empirical evaluations across diverse real-world datasets demonstrate\nthe superior performance of EasyRec compared to state-of-the-art alternative\nmodels, particularly in the challenging text-based zero-shot recommendation\nscenarios. Furthermore, the study highlights the potential of seamlessly\nintegrating EasyRec as a plug-and-play component into text-enhanced\ncollaborative filtering frameworks, thereby empowering existing recommender\nsystems to elevate their recommendation performance and adapt to the evolving\nuser preferences in dynamic environments. For better result reproducibility of\nour EasyRec framework, the model implementation details, source code, and\ndatasets are available at the link: https://github.com/HKUDS/EasyRec.\n", "link": "http://arxiv.org/abs/2408.08821v2", "date": "2024-09-27", "relevancy": 2.3539, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyRec%3A%20Simple%20yet%20Effective%20Language%20Models%20for%20Recommendation&body=Title%3A%20EasyRec%3A%20Simple%20yet%20Effective%20Language%20Models%20for%20Recommendation%0AAuthor%3A%20Xubin%20Ren%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20become%20a%20powerful%20technique%20for%20learning%0Arepresentations%20from%20user-item%20interaction%20data%20in%20collaborative%20filtering%20%28CF%29%0Afor%20recommender%20systems.%20However%2C%20many%20existing%20methods%20heavily%20rely%20on%20unique%0Auser%20and%20item%20IDs%2C%20which%20limits%20their%20ability%20to%20perform%20well%20in%20practical%0Azero-shot%20learning%20scenarios%20where%20sufficient%20training%20data%20may%20be%20unavailable.%0AInspired%20by%20the%20success%20of%20language%20models%20%28LMs%29%20and%20their%20strong%0Ageneralization%20capabilities%2C%20a%20crucial%20question%20arises%3A%20How%20can%20we%20harness%20the%0Apotential%20of%20language%20models%20to%20empower%20recommender%20systems%20and%20elevate%20its%0Ageneralization%20capabilities%20to%20new%20heights%3F%20In%20this%20study%2C%20we%20propose%20EasyRec%20-%0Aan%20effective%20and%20easy-to-use%20approach%20that%20seamlessly%20integrates%20text-based%0Asemantic%20understanding%20with%20collaborative%20signals.%20EasyRec%20employs%20a%0Atext-behavior%20alignment%20framework%2C%20which%20combines%20contrastive%20learning%20with%0Acollaborative%20language%20model%20tuning%2C%20to%20ensure%20a%20strong%20alignment%20between%20the%0Atext-enhanced%20semantic%20space%20and%20the%20collaborative%20behavior%20information.%0AExtensive%20empirical%20evaluations%20across%20diverse%20real-world%20datasets%20demonstrate%0Athe%20superior%20performance%20of%20EasyRec%20compared%20to%20state-of-the-art%20alternative%0Amodels%2C%20particularly%20in%20the%20challenging%20text-based%20zero-shot%20recommendation%0Ascenarios.%20Furthermore%2C%20the%20study%20highlights%20the%20potential%20of%20seamlessly%0Aintegrating%20EasyRec%20as%20a%20plug-and-play%20component%20into%20text-enhanced%0Acollaborative%20filtering%20frameworks%2C%20thereby%20empowering%20existing%20recommender%0Asystems%20to%20elevate%20their%20recommendation%20performance%20and%20adapt%20to%20the%20evolving%0Auser%20preferences%20in%20dynamic%20environments.%20For%20better%20result%20reproducibility%20of%0Aour%20EasyRec%20framework%2C%20the%20model%20implementation%20details%2C%20source%20code%2C%20and%0Adatasets%20are%20available%20at%20the%20link%3A%20https%3A//github.com/HKUDS/EasyRec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08821v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyRec%253A%2520Simple%2520yet%2520Effective%2520Language%2520Models%2520for%2520Recommendation%26entry.906535625%3DXubin%2520Ren%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520become%2520a%2520powerful%2520technique%2520for%2520learning%250Arepresentations%2520from%2520user-item%2520interaction%2520data%2520in%2520collaborative%2520filtering%2520%2528CF%2529%250Afor%2520recommender%2520systems.%2520However%252C%2520many%2520existing%2520methods%2520heavily%2520rely%2520on%2520unique%250Auser%2520and%2520item%2520IDs%252C%2520which%2520limits%2520their%2520ability%2520to%2520perform%2520well%2520in%2520practical%250Azero-shot%2520learning%2520scenarios%2520where%2520sufficient%2520training%2520data%2520may%2520be%2520unavailable.%250AInspired%2520by%2520the%2520success%2520of%2520language%2520models%2520%2528LMs%2529%2520and%2520their%2520strong%250Ageneralization%2520capabilities%252C%2520a%2520crucial%2520question%2520arises%253A%2520How%2520can%2520we%2520harness%2520the%250Apotential%2520of%2520language%2520models%2520to%2520empower%2520recommender%2520systems%2520and%2520elevate%2520its%250Ageneralization%2520capabilities%2520to%2520new%2520heights%253F%2520In%2520this%2520study%252C%2520we%2520propose%2520EasyRec%2520-%250Aan%2520effective%2520and%2520easy-to-use%2520approach%2520that%2520seamlessly%2520integrates%2520text-based%250Asemantic%2520understanding%2520with%2520collaborative%2520signals.%2520EasyRec%2520employs%2520a%250Atext-behavior%2520alignment%2520framework%252C%2520which%2520combines%2520contrastive%2520learning%2520with%250Acollaborative%2520language%2520model%2520tuning%252C%2520to%2520ensure%2520a%2520strong%2520alignment%2520between%2520the%250Atext-enhanced%2520semantic%2520space%2520and%2520the%2520collaborative%2520behavior%2520information.%250AExtensive%2520empirical%2520evaluations%2520across%2520diverse%2520real-world%2520datasets%2520demonstrate%250Athe%2520superior%2520performance%2520of%2520EasyRec%2520compared%2520to%2520state-of-the-art%2520alternative%250Amodels%252C%2520particularly%2520in%2520the%2520challenging%2520text-based%2520zero-shot%2520recommendation%250Ascenarios.%2520Furthermore%252C%2520the%2520study%2520highlights%2520the%2520potential%2520of%2520seamlessly%250Aintegrating%2520EasyRec%2520as%2520a%2520plug-and-play%2520component%2520into%2520text-enhanced%250Acollaborative%2520filtering%2520frameworks%252C%2520thereby%2520empowering%2520existing%2520recommender%250Asystems%2520to%2520elevate%2520their%2520recommendation%2520performance%2520and%2520adapt%2520to%2520the%2520evolving%250Auser%2520preferences%2520in%2520dynamic%2520environments.%2520For%2520better%2520result%2520reproducibility%2520of%250Aour%2520EasyRec%2520framework%252C%2520the%2520model%2520implementation%2520details%252C%2520source%2520code%252C%2520and%250Adatasets%2520are%2520available%2520at%2520the%2520link%253A%2520https%253A//github.com/HKUDS/EasyRec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08821v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyRec%3A%20Simple%20yet%20Effective%20Language%20Models%20for%20Recommendation&entry.906535625=Xubin%20Ren%20and%20Chao%20Huang&entry.1292438233=%20%20Deep%20neural%20networks%20have%20become%20a%20powerful%20technique%20for%20learning%0Arepresentations%20from%20user-item%20interaction%20data%20in%20collaborative%20filtering%20%28CF%29%0Afor%20recommender%20systems.%20However%2C%20many%20existing%20methods%20heavily%20rely%20on%20unique%0Auser%20and%20item%20IDs%2C%20which%20limits%20their%20ability%20to%20perform%20well%20in%20practical%0Azero-shot%20learning%20scenarios%20where%20sufficient%20training%20data%20may%20be%20unavailable.%0AInspired%20by%20the%20success%20of%20language%20models%20%28LMs%29%20and%20their%20strong%0Ageneralization%20capabilities%2C%20a%20crucial%20question%20arises%3A%20How%20can%20we%20harness%20the%0Apotential%20of%20language%20models%20to%20empower%20recommender%20systems%20and%20elevate%20its%0Ageneralization%20capabilities%20to%20new%20heights%3F%20In%20this%20study%2C%20we%20propose%20EasyRec%20-%0Aan%20effective%20and%20easy-to-use%20approach%20that%20seamlessly%20integrates%20text-based%0Asemantic%20understanding%20with%20collaborative%20signals.%20EasyRec%20employs%20a%0Atext-behavior%20alignment%20framework%2C%20which%20combines%20contrastive%20learning%20with%0Acollaborative%20language%20model%20tuning%2C%20to%20ensure%20a%20strong%20alignment%20between%20the%0Atext-enhanced%20semantic%20space%20and%20the%20collaborative%20behavior%20information.%0AExtensive%20empirical%20evaluations%20across%20diverse%20real-world%20datasets%20demonstrate%0Athe%20superior%20performance%20of%20EasyRec%20compared%20to%20state-of-the-art%20alternative%0Amodels%2C%20particularly%20in%20the%20challenging%20text-based%20zero-shot%20recommendation%0Ascenarios.%20Furthermore%2C%20the%20study%20highlights%20the%20potential%20of%20seamlessly%0Aintegrating%20EasyRec%20as%20a%20plug-and-play%20component%20into%20text-enhanced%0Acollaborative%20filtering%20frameworks%2C%20thereby%20empowering%20existing%20recommender%0Asystems%20to%20elevate%20their%20recommendation%20performance%20and%20adapt%20to%20the%20evolving%0Auser%20preferences%20in%20dynamic%20environments.%20For%20better%20result%20reproducibility%20of%0Aour%20EasyRec%20framework%2C%20the%20model%20implementation%20details%2C%20source%20code%2C%20and%0Adatasets%20are%20available%20at%20the%20link%3A%20https%3A//github.com/HKUDS/EasyRec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08821v2&entry.124074799=Read"},
{"title": "QPaug: Question and Passage Augmentation for Open-Domain Question\n  Answering of LLMs", "author": "Minsang Kim and Cheoneum Park and Seungjun Baek", "abstract": "  Retrieval-augmented generation (RAG) has received much attention for\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\nparametric knowledge of large language models (LLMs). While previous approaches\nfocused on processing retrieved passages to remove irrelevant context, they\nstill rely heavily on the quality of retrieved passages which can degrade if\nthe question is ambiguous or complex. In this paper, we propose a simple yet\nefficient method called question and passage augmentation (QPaug) via LLMs for\nopen-domain QA. QPaug first decomposes the original questions into\nmultiple-step sub-questions. By augmenting the original question with detailed\nsub-questions and planning, we are able to make the query more specific on what\nneeds to be retrieved, improving the retrieval performance. In addition, to\ncompensate for the case where the retrieved passages contain distracting\ninformation or divided opinions, we augment the retrieved passages with\nself-generated passages by LLMs to guide the answer extraction. Experimental\nresults show that QPaug outperforms the previous state-of-the-art and achieves\nsignificant performance gain over existing RAG methods. The source code is\navailable at \\url{https://github.com/kmswin1/QPaug}.\n", "link": "http://arxiv.org/abs/2406.14277v2", "date": "2024-09-27", "relevancy": 2.3495, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4713}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QPaug%3A%20Question%20and%20Passage%20Augmentation%20for%20Open-Domain%20Question%0A%20%20Answering%20of%20LLMs&body=Title%3A%20QPaug%3A%20Question%20and%20Passage%20Augmentation%20for%20Open-Domain%20Question%0A%20%20Answering%20of%20LLMs%0AAuthor%3A%20Minsang%20Kim%20and%20Cheoneum%20Park%20and%20Seungjun%20Baek%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20received%20much%20attention%20for%0AOpen-domain%20question-answering%20%28ODQA%29%20tasks%20as%20a%20means%20to%20compensate%20for%20the%0Aparametric%20knowledge%20of%20large%20language%20models%20%28LLMs%29.%20While%20previous%20approaches%0Afocused%20on%20processing%20retrieved%20passages%20to%20remove%20irrelevant%20context%2C%20they%0Astill%20rely%20heavily%20on%20the%20quality%20of%20retrieved%20passages%20which%20can%20degrade%20if%0Athe%20question%20is%20ambiguous%20or%20complex.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%0Aefficient%20method%20called%20question%20and%20passage%20augmentation%20%28QPaug%29%20via%20LLMs%20for%0Aopen-domain%20QA.%20QPaug%20first%20decomposes%20the%20original%20questions%20into%0Amultiple-step%20sub-questions.%20By%20augmenting%20the%20original%20question%20with%20detailed%0Asub-questions%20and%20planning%2C%20we%20are%20able%20to%20make%20the%20query%20more%20specific%20on%20what%0Aneeds%20to%20be%20retrieved%2C%20improving%20the%20retrieval%20performance.%20In%20addition%2C%20to%0Acompensate%20for%20the%20case%20where%20the%20retrieved%20passages%20contain%20distracting%0Ainformation%20or%20divided%20opinions%2C%20we%20augment%20the%20retrieved%20passages%20with%0Aself-generated%20passages%20by%20LLMs%20to%20guide%20the%20answer%20extraction.%20Experimental%0Aresults%20show%20that%20QPaug%20outperforms%20the%20previous%20state-of-the-art%20and%20achieves%0Asignificant%20performance%20gain%20over%20existing%20RAG%20methods.%20The%20source%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/kmswin1/QPaug%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQPaug%253A%2520Question%2520and%2520Passage%2520Augmentation%2520for%2520Open-Domain%2520Question%250A%2520%2520Answering%2520of%2520LLMs%26entry.906535625%3DMinsang%2520Kim%2520and%2520Cheoneum%2520Park%2520and%2520Seungjun%2520Baek%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520received%2520much%2520attention%2520for%250AOpen-domain%2520question-answering%2520%2528ODQA%2529%2520tasks%2520as%2520a%2520means%2520to%2520compensate%2520for%2520the%250Aparametric%2520knowledge%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520While%2520previous%2520approaches%250Afocused%2520on%2520processing%2520retrieved%2520passages%2520to%2520remove%2520irrelevant%2520context%252C%2520they%250Astill%2520rely%2520heavily%2520on%2520the%2520quality%2520of%2520retrieved%2520passages%2520which%2520can%2520degrade%2520if%250Athe%2520question%2520is%2520ambiguous%2520or%2520complex.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520simple%2520yet%250Aefficient%2520method%2520called%2520question%2520and%2520passage%2520augmentation%2520%2528QPaug%2529%2520via%2520LLMs%2520for%250Aopen-domain%2520QA.%2520QPaug%2520first%2520decomposes%2520the%2520original%2520questions%2520into%250Amultiple-step%2520sub-questions.%2520By%2520augmenting%2520the%2520original%2520question%2520with%2520detailed%250Asub-questions%2520and%2520planning%252C%2520we%2520are%2520able%2520to%2520make%2520the%2520query%2520more%2520specific%2520on%2520what%250Aneeds%2520to%2520be%2520retrieved%252C%2520improving%2520the%2520retrieval%2520performance.%2520In%2520addition%252C%2520to%250Acompensate%2520for%2520the%2520case%2520where%2520the%2520retrieved%2520passages%2520contain%2520distracting%250Ainformation%2520or%2520divided%2520opinions%252C%2520we%2520augment%2520the%2520retrieved%2520passages%2520with%250Aself-generated%2520passages%2520by%2520LLMs%2520to%2520guide%2520the%2520answer%2520extraction.%2520Experimental%250Aresults%2520show%2520that%2520QPaug%2520outperforms%2520the%2520previous%2520state-of-the-art%2520and%2520achieves%250Asignificant%2520performance%2520gain%2520over%2520existing%2520RAG%2520methods.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/kmswin1/QPaug%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QPaug%3A%20Question%20and%20Passage%20Augmentation%20for%20Open-Domain%20Question%0A%20%20Answering%20of%20LLMs&entry.906535625=Minsang%20Kim%20and%20Cheoneum%20Park%20and%20Seungjun%20Baek&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20received%20much%20attention%20for%0AOpen-domain%20question-answering%20%28ODQA%29%20tasks%20as%20a%20means%20to%20compensate%20for%20the%0Aparametric%20knowledge%20of%20large%20language%20models%20%28LLMs%29.%20While%20previous%20approaches%0Afocused%20on%20processing%20retrieved%20passages%20to%20remove%20irrelevant%20context%2C%20they%0Astill%20rely%20heavily%20on%20the%20quality%20of%20retrieved%20passages%20which%20can%20degrade%20if%0Athe%20question%20is%20ambiguous%20or%20complex.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%0Aefficient%20method%20called%20question%20and%20passage%20augmentation%20%28QPaug%29%20via%20LLMs%20for%0Aopen-domain%20QA.%20QPaug%20first%20decomposes%20the%20original%20questions%20into%0Amultiple-step%20sub-questions.%20By%20augmenting%20the%20original%20question%20with%20detailed%0Asub-questions%20and%20planning%2C%20we%20are%20able%20to%20make%20the%20query%20more%20specific%20on%20what%0Aneeds%20to%20be%20retrieved%2C%20improving%20the%20retrieval%20performance.%20In%20addition%2C%20to%0Acompensate%20for%20the%20case%20where%20the%20retrieved%20passages%20contain%20distracting%0Ainformation%20or%20divided%20opinions%2C%20we%20augment%20the%20retrieved%20passages%20with%0Aself-generated%20passages%20by%20LLMs%20to%20guide%20the%20answer%20extraction.%20Experimental%0Aresults%20show%20that%20QPaug%20outperforms%20the%20previous%20state-of-the-art%20and%20achieves%0Asignificant%20performance%20gain%20over%20existing%20RAG%20methods.%20The%20source%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/kmswin1/QPaug%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14277v2&entry.124074799=Read"},
{"title": "UniCal: Unified Neural Sensor Calibration", "author": "Ze Yang and George Chen and Haowei Zhang and Kevin Ta and Ioan Andrei B\u00e2rsan and Daniel Murphy and Sivabalan Manivasagam and Raquel Urtasun", "abstract": "  Self-driving vehicles (SDVs) require accurate calibration of LiDARs and\ncameras to fuse sensor data accurately for autonomy. Traditional calibration\nmethods typically leverage fiducials captured in a controlled and structured\nscene and compute correspondences to optimize over. These approaches are costly\nand require substantial infrastructure and operations, making it challenging to\nscale for vehicle fleets. In this work, we propose UniCal, a unified framework\nfor effortlessly calibrating SDVs equipped with multiple LiDARs and cameras.\nOur approach is built upon a differentiable scene representation capable of\nrendering multi-view geometrically and photometrically consistent sensor\nobservations. We jointly learn the sensor calibration and the underlying scene\nrepresentation through differentiable volume rendering, utilizing outdoor\nsensor data without the need for specific calibration fiducials. This\n\"drive-and-calibrate\" approach significantly reduces costs and operational\noverhead compared to existing calibration systems, enabling efficient\ncalibration for large SDV fleets at scale. To ensure geometric consistency\nacross observations from different sensors, we introduce a novel surface\nalignment loss that combines feature-based registration with neural rendering.\nComprehensive evaluations on multiple datasets demonstrate that UniCal\noutperforms or matches the accuracy of existing calibration approaches while\nbeing more efficient, demonstrating the value of UniCal for scalable\ncalibration.\n", "link": "http://arxiv.org/abs/2409.18953v1", "date": "2024-09-27", "relevancy": 2.3412, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6244}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5579}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniCal%3A%20Unified%20Neural%20Sensor%20Calibration&body=Title%3A%20UniCal%3A%20Unified%20Neural%20Sensor%20Calibration%0AAuthor%3A%20Ze%20Yang%20and%20George%20Chen%20and%20Haowei%20Zhang%20and%20Kevin%20Ta%20and%20Ioan%20Andrei%20B%C3%A2rsan%20and%20Daniel%20Murphy%20and%20Sivabalan%20Manivasagam%20and%20Raquel%20Urtasun%0AAbstract%3A%20%20%20Self-driving%20vehicles%20%28SDVs%29%20require%20accurate%20calibration%20of%20LiDARs%20and%0Acameras%20to%20fuse%20sensor%20data%20accurately%20for%20autonomy.%20Traditional%20calibration%0Amethods%20typically%20leverage%20fiducials%20captured%20in%20a%20controlled%20and%20structured%0Ascene%20and%20compute%20correspondences%20to%20optimize%20over.%20These%20approaches%20are%20costly%0Aand%20require%20substantial%20infrastructure%20and%20operations%2C%20making%20it%20challenging%20to%0Ascale%20for%20vehicle%20fleets.%20In%20this%20work%2C%20we%20propose%20UniCal%2C%20a%20unified%20framework%0Afor%20effortlessly%20calibrating%20SDVs%20equipped%20with%20multiple%20LiDARs%20and%20cameras.%0AOur%20approach%20is%20built%20upon%20a%20differentiable%20scene%20representation%20capable%20of%0Arendering%20multi-view%20geometrically%20and%20photometrically%20consistent%20sensor%0Aobservations.%20We%20jointly%20learn%20the%20sensor%20calibration%20and%20the%20underlying%20scene%0Arepresentation%20through%20differentiable%20volume%20rendering%2C%20utilizing%20outdoor%0Asensor%20data%20without%20the%20need%20for%20specific%20calibration%20fiducials.%20This%0A%22drive-and-calibrate%22%20approach%20significantly%20reduces%20costs%20and%20operational%0Aoverhead%20compared%20to%20existing%20calibration%20systems%2C%20enabling%20efficient%0Acalibration%20for%20large%20SDV%20fleets%20at%20scale.%20To%20ensure%20geometric%20consistency%0Aacross%20observations%20from%20different%20sensors%2C%20we%20introduce%20a%20novel%20surface%0Aalignment%20loss%20that%20combines%20feature-based%20registration%20with%20neural%20rendering.%0AComprehensive%20evaluations%20on%20multiple%20datasets%20demonstrate%20that%20UniCal%0Aoutperforms%20or%20matches%20the%20accuracy%20of%20existing%20calibration%20approaches%20while%0Abeing%20more%20efficient%2C%20demonstrating%20the%20value%20of%20UniCal%20for%20scalable%0Acalibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniCal%253A%2520Unified%2520Neural%2520Sensor%2520Calibration%26entry.906535625%3DZe%2520Yang%2520and%2520George%2520Chen%2520and%2520Haowei%2520Zhang%2520and%2520Kevin%2520Ta%2520and%2520Ioan%2520Andrei%2520B%25C3%25A2rsan%2520and%2520Daniel%2520Murphy%2520and%2520Sivabalan%2520Manivasagam%2520and%2520Raquel%2520Urtasun%26entry.1292438233%3D%2520%2520Self-driving%2520vehicles%2520%2528SDVs%2529%2520require%2520accurate%2520calibration%2520of%2520LiDARs%2520and%250Acameras%2520to%2520fuse%2520sensor%2520data%2520accurately%2520for%2520autonomy.%2520Traditional%2520calibration%250Amethods%2520typically%2520leverage%2520fiducials%2520captured%2520in%2520a%2520controlled%2520and%2520structured%250Ascene%2520and%2520compute%2520correspondences%2520to%2520optimize%2520over.%2520These%2520approaches%2520are%2520costly%250Aand%2520require%2520substantial%2520infrastructure%2520and%2520operations%252C%2520making%2520it%2520challenging%2520to%250Ascale%2520for%2520vehicle%2520fleets.%2520In%2520this%2520work%252C%2520we%2520propose%2520UniCal%252C%2520a%2520unified%2520framework%250Afor%2520effortlessly%2520calibrating%2520SDVs%2520equipped%2520with%2520multiple%2520LiDARs%2520and%2520cameras.%250AOur%2520approach%2520is%2520built%2520upon%2520a%2520differentiable%2520scene%2520representation%2520capable%2520of%250Arendering%2520multi-view%2520geometrically%2520and%2520photometrically%2520consistent%2520sensor%250Aobservations.%2520We%2520jointly%2520learn%2520the%2520sensor%2520calibration%2520and%2520the%2520underlying%2520scene%250Arepresentation%2520through%2520differentiable%2520volume%2520rendering%252C%2520utilizing%2520outdoor%250Asensor%2520data%2520without%2520the%2520need%2520for%2520specific%2520calibration%2520fiducials.%2520This%250A%2522drive-and-calibrate%2522%2520approach%2520significantly%2520reduces%2520costs%2520and%2520operational%250Aoverhead%2520compared%2520to%2520existing%2520calibration%2520systems%252C%2520enabling%2520efficient%250Acalibration%2520for%2520large%2520SDV%2520fleets%2520at%2520scale.%2520To%2520ensure%2520geometric%2520consistency%250Aacross%2520observations%2520from%2520different%2520sensors%252C%2520we%2520introduce%2520a%2520novel%2520surface%250Aalignment%2520loss%2520that%2520combines%2520feature-based%2520registration%2520with%2520neural%2520rendering.%250AComprehensive%2520evaluations%2520on%2520multiple%2520datasets%2520demonstrate%2520that%2520UniCal%250Aoutperforms%2520or%2520matches%2520the%2520accuracy%2520of%2520existing%2520calibration%2520approaches%2520while%250Abeing%2520more%2520efficient%252C%2520demonstrating%2520the%2520value%2520of%2520UniCal%2520for%2520scalable%250Acalibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniCal%3A%20Unified%20Neural%20Sensor%20Calibration&entry.906535625=Ze%20Yang%20and%20George%20Chen%20and%20Haowei%20Zhang%20and%20Kevin%20Ta%20and%20Ioan%20Andrei%20B%C3%A2rsan%20and%20Daniel%20Murphy%20and%20Sivabalan%20Manivasagam%20and%20Raquel%20Urtasun&entry.1292438233=%20%20Self-driving%20vehicles%20%28SDVs%29%20require%20accurate%20calibration%20of%20LiDARs%20and%0Acameras%20to%20fuse%20sensor%20data%20accurately%20for%20autonomy.%20Traditional%20calibration%0Amethods%20typically%20leverage%20fiducials%20captured%20in%20a%20controlled%20and%20structured%0Ascene%20and%20compute%20correspondences%20to%20optimize%20over.%20These%20approaches%20are%20costly%0Aand%20require%20substantial%20infrastructure%20and%20operations%2C%20making%20it%20challenging%20to%0Ascale%20for%20vehicle%20fleets.%20In%20this%20work%2C%20we%20propose%20UniCal%2C%20a%20unified%20framework%0Afor%20effortlessly%20calibrating%20SDVs%20equipped%20with%20multiple%20LiDARs%20and%20cameras.%0AOur%20approach%20is%20built%20upon%20a%20differentiable%20scene%20representation%20capable%20of%0Arendering%20multi-view%20geometrically%20and%20photometrically%20consistent%20sensor%0Aobservations.%20We%20jointly%20learn%20the%20sensor%20calibration%20and%20the%20underlying%20scene%0Arepresentation%20through%20differentiable%20volume%20rendering%2C%20utilizing%20outdoor%0Asensor%20data%20without%20the%20need%20for%20specific%20calibration%20fiducials.%20This%0A%22drive-and-calibrate%22%20approach%20significantly%20reduces%20costs%20and%20operational%0Aoverhead%20compared%20to%20existing%20calibration%20systems%2C%20enabling%20efficient%0Acalibration%20for%20large%20SDV%20fleets%20at%20scale.%20To%20ensure%20geometric%20consistency%0Aacross%20observations%20from%20different%20sensors%2C%20we%20introduce%20a%20novel%20surface%0Aalignment%20loss%20that%20combines%20feature-based%20registration%20with%20neural%20rendering.%0AComprehensive%20evaluations%20on%20multiple%20datasets%20demonstrate%20that%20UniCal%0Aoutperforms%20or%20matches%20the%20accuracy%20of%20existing%20calibration%20approaches%20while%0Abeing%20more%20efficient%2C%20demonstrating%20the%20value%20of%20UniCal%20for%20scalable%0Acalibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18953v1&entry.124074799=Read"},
{"title": "A-FedPD: Aligning Dual-Drift is All Federated Primal-Dual Learning Needs", "author": "Yan Sun and Li Shen and Dacheng Tao", "abstract": "  As a popular paradigm for juggling data privacy and collaborative training,\nfederated learning (FL) is flourishing to distributively process the large\nscale of heterogeneous datasets on edged clients. Due to bandwidth limitations\nand security considerations, it ingeniously splits the original problem into\nmultiple subproblems to be solved in parallel, which empowers primal dual\nsolutions to great application values in FL. In this paper, we review the\nrecent development of classical federated primal dual methods and point out a\nserious common defect of such methods in non-convex scenarios, which we say is\na \"dual drift\" caused by dual hysteresis of those longstanding inactive clients\nunder partial participation training. To further address this problem, we\npropose a novel Aligned Federated Primal Dual (A-FedPD) method, which\nconstructs virtual dual updates to align global consensus and local dual\nvariables for those protracted unparticipated local clients. Meanwhile, we\nprovide a comprehensive analysis of the optimization and generalization\nefficiency for the A-FedPD method on smooth non-convex objectives, which\nconfirms its high efficiency and practicality. Extensive experiments are\nconducted on several classical FL setups to validate the effectiveness of our\nproposed method.\n", "link": "http://arxiv.org/abs/2409.18915v1", "date": "2024-09-27", "relevancy": 2.3303, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4677}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4663}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A-FedPD%3A%20Aligning%20Dual-Drift%20is%20All%20Federated%20Primal-Dual%20Learning%20Needs&body=Title%3A%20A-FedPD%3A%20Aligning%20Dual-Drift%20is%20All%20Federated%20Primal-Dual%20Learning%20Needs%0AAuthor%3A%20Yan%20Sun%20and%20Li%20Shen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20As%20a%20popular%20paradigm%20for%20juggling%20data%20privacy%20and%20collaborative%20training%2C%0Afederated%20learning%20%28FL%29%20is%20flourishing%20to%20distributively%20process%20the%20large%0Ascale%20of%20heterogeneous%20datasets%20on%20edged%20clients.%20Due%20to%20bandwidth%20limitations%0Aand%20security%20considerations%2C%20it%20ingeniously%20splits%20the%20original%20problem%20into%0Amultiple%20subproblems%20to%20be%20solved%20in%20parallel%2C%20which%20empowers%20primal%20dual%0Asolutions%20to%20great%20application%20values%20in%20FL.%20In%20this%20paper%2C%20we%20review%20the%0Arecent%20development%20of%20classical%20federated%20primal%20dual%20methods%20and%20point%20out%20a%0Aserious%20common%20defect%20of%20such%20methods%20in%20non-convex%20scenarios%2C%20which%20we%20say%20is%0Aa%20%22dual%20drift%22%20caused%20by%20dual%20hysteresis%20of%20those%20longstanding%20inactive%20clients%0Aunder%20partial%20participation%20training.%20To%20further%20address%20this%20problem%2C%20we%0Apropose%20a%20novel%20Aligned%20Federated%20Primal%20Dual%20%28A-FedPD%29%20method%2C%20which%0Aconstructs%20virtual%20dual%20updates%20to%20align%20global%20consensus%20and%20local%20dual%0Avariables%20for%20those%20protracted%20unparticipated%20local%20clients.%20Meanwhile%2C%20we%0Aprovide%20a%20comprehensive%20analysis%20of%20the%20optimization%20and%20generalization%0Aefficiency%20for%20the%20A-FedPD%20method%20on%20smooth%20non-convex%20objectives%2C%20which%0Aconfirms%20its%20high%20efficiency%20and%20practicality.%20Extensive%20experiments%20are%0Aconducted%20on%20several%20classical%20FL%20setups%20to%20validate%20the%20effectiveness%20of%20our%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA-FedPD%253A%2520Aligning%2520Dual-Drift%2520is%2520All%2520Federated%2520Primal-Dual%2520Learning%2520Needs%26entry.906535625%3DYan%2520Sun%2520and%2520Li%2520Shen%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520As%2520a%2520popular%2520paradigm%2520for%2520juggling%2520data%2520privacy%2520and%2520collaborative%2520training%252C%250Afederated%2520learning%2520%2528FL%2529%2520is%2520flourishing%2520to%2520distributively%2520process%2520the%2520large%250Ascale%2520of%2520heterogeneous%2520datasets%2520on%2520edged%2520clients.%2520Due%2520to%2520bandwidth%2520limitations%250Aand%2520security%2520considerations%252C%2520it%2520ingeniously%2520splits%2520the%2520original%2520problem%2520into%250Amultiple%2520subproblems%2520to%2520be%2520solved%2520in%2520parallel%252C%2520which%2520empowers%2520primal%2520dual%250Asolutions%2520to%2520great%2520application%2520values%2520in%2520FL.%2520In%2520this%2520paper%252C%2520we%2520review%2520the%250Arecent%2520development%2520of%2520classical%2520federated%2520primal%2520dual%2520methods%2520and%2520point%2520out%2520a%250Aserious%2520common%2520defect%2520of%2520such%2520methods%2520in%2520non-convex%2520scenarios%252C%2520which%2520we%2520say%2520is%250Aa%2520%2522dual%2520drift%2522%2520caused%2520by%2520dual%2520hysteresis%2520of%2520those%2520longstanding%2520inactive%2520clients%250Aunder%2520partial%2520participation%2520training.%2520To%2520further%2520address%2520this%2520problem%252C%2520we%250Apropose%2520a%2520novel%2520Aligned%2520Federated%2520Primal%2520Dual%2520%2528A-FedPD%2529%2520method%252C%2520which%250Aconstructs%2520virtual%2520dual%2520updates%2520to%2520align%2520global%2520consensus%2520and%2520local%2520dual%250Avariables%2520for%2520those%2520protracted%2520unparticipated%2520local%2520clients.%2520Meanwhile%252C%2520we%250Aprovide%2520a%2520comprehensive%2520analysis%2520of%2520the%2520optimization%2520and%2520generalization%250Aefficiency%2520for%2520the%2520A-FedPD%2520method%2520on%2520smooth%2520non-convex%2520objectives%252C%2520which%250Aconfirms%2520its%2520high%2520efficiency%2520and%2520practicality.%2520Extensive%2520experiments%2520are%250Aconducted%2520on%2520several%2520classical%2520FL%2520setups%2520to%2520validate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A-FedPD%3A%20Aligning%20Dual-Drift%20is%20All%20Federated%20Primal-Dual%20Learning%20Needs&entry.906535625=Yan%20Sun%20and%20Li%20Shen%20and%20Dacheng%20Tao&entry.1292438233=%20%20As%20a%20popular%20paradigm%20for%20juggling%20data%20privacy%20and%20collaborative%20training%2C%0Afederated%20learning%20%28FL%29%20is%20flourishing%20to%20distributively%20process%20the%20large%0Ascale%20of%20heterogeneous%20datasets%20on%20edged%20clients.%20Due%20to%20bandwidth%20limitations%0Aand%20security%20considerations%2C%20it%20ingeniously%20splits%20the%20original%20problem%20into%0Amultiple%20subproblems%20to%20be%20solved%20in%20parallel%2C%20which%20empowers%20primal%20dual%0Asolutions%20to%20great%20application%20values%20in%20FL.%20In%20this%20paper%2C%20we%20review%20the%0Arecent%20development%20of%20classical%20federated%20primal%20dual%20methods%20and%20point%20out%20a%0Aserious%20common%20defect%20of%20such%20methods%20in%20non-convex%20scenarios%2C%20which%20we%20say%20is%0Aa%20%22dual%20drift%22%20caused%20by%20dual%20hysteresis%20of%20those%20longstanding%20inactive%20clients%0Aunder%20partial%20participation%20training.%20To%20further%20address%20this%20problem%2C%20we%0Apropose%20a%20novel%20Aligned%20Federated%20Primal%20Dual%20%28A-FedPD%29%20method%2C%20which%0Aconstructs%20virtual%20dual%20updates%20to%20align%20global%20consensus%20and%20local%20dual%0Avariables%20for%20those%20protracted%20unparticipated%20local%20clients.%20Meanwhile%2C%20we%0Aprovide%20a%20comprehensive%20analysis%20of%20the%20optimization%20and%20generalization%0Aefficiency%20for%20the%20A-FedPD%20method%20on%20smooth%20non-convex%20objectives%2C%20which%0Aconfirms%20its%20high%20efficiency%20and%20practicality.%20Extensive%20experiments%20are%0Aconducted%20on%20several%20classical%20FL%20setups%20to%20validate%20the%20effectiveness%20of%20our%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18915v1&entry.124074799=Read"},
{"title": "VideoPatchCore: An Effective Method to Memorize Normality for Video\n  Anomaly Detection", "author": "Sunghyun Ahn and Youngwan Jo and Kijung Lee and Sanghyun Park", "abstract": "  Video anomaly detection (VAD) is a crucial task in video analysis and\nsurveillance within computer vision. Currently, VAD is gaining attention with\nmemory techniques that store the features of normal frames. The stored features\nare utilized for frame reconstruction, identifying an abnormality when a\nsignificant difference exists between the reconstructed and input frames.\nHowever, this approach faces several challenges due to the simultaneous\noptimization required for both the memory and encoder-decoder model. These\nchallenges include increased optimization difficulty, complexity of\nimplementation, and performance variability depending on the memory size. To\naddress these challenges,we propose an effective memory method for VAD, called\nVideoPatchCore. Inspired by PatchCore, our approach introduces a structure that\nprioritizes memory optimization and configures three types of memory tailored\nto the characteristics of video data. This method effectively addresses the\nlimitations of existing memory-based methods, achieving good performance\ncomparable to state-of-the-art methods. Furthermore, our method requires no\ntraining and is straightforward to implement, making VAD tasks more accessible.\nOur code is available online at github.com/SkiddieAhn/Paper-VideoPatchCore.\n", "link": "http://arxiv.org/abs/2409.16225v3", "date": "2024-09-27", "relevancy": 2.3002, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5887}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5744}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoPatchCore%3A%20An%20Effective%20Method%20to%20Memorize%20Normality%20for%20Video%0A%20%20Anomaly%20Detection&body=Title%3A%20VideoPatchCore%3A%20An%20Effective%20Method%20to%20Memorize%20Normality%20for%20Video%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Sunghyun%20Ahn%20and%20Youngwan%20Jo%20and%20Kijung%20Lee%20and%20Sanghyun%20Park%0AAbstract%3A%20%20%20Video%20anomaly%20detection%20%28VAD%29%20is%20a%20crucial%20task%20in%20video%20analysis%20and%0Asurveillance%20within%20computer%20vision.%20Currently%2C%20VAD%20is%20gaining%20attention%20with%0Amemory%20techniques%20that%20store%20the%20features%20of%20normal%20frames.%20The%20stored%20features%0Aare%20utilized%20for%20frame%20reconstruction%2C%20identifying%20an%20abnormality%20when%20a%0Asignificant%20difference%20exists%20between%20the%20reconstructed%20and%20input%20frames.%0AHowever%2C%20this%20approach%20faces%20several%20challenges%20due%20to%20the%20simultaneous%0Aoptimization%20required%20for%20both%20the%20memory%20and%20encoder-decoder%20model.%20These%0Achallenges%20include%20increased%20optimization%20difficulty%2C%20complexity%20of%0Aimplementation%2C%20and%20performance%20variability%20depending%20on%20the%20memory%20size.%20To%0Aaddress%20these%20challenges%2Cwe%20propose%20an%20effective%20memory%20method%20for%20VAD%2C%20called%0AVideoPatchCore.%20Inspired%20by%20PatchCore%2C%20our%20approach%20introduces%20a%20structure%20that%0Aprioritizes%20memory%20optimization%20and%20configures%20three%20types%20of%20memory%20tailored%0Ato%20the%20characteristics%20of%20video%20data.%20This%20method%20effectively%20addresses%20the%0Alimitations%20of%20existing%20memory-based%20methods%2C%20achieving%20good%20performance%0Acomparable%20to%20state-of-the-art%20methods.%20Furthermore%2C%20our%20method%20requires%20no%0Atraining%20and%20is%20straightforward%20to%20implement%2C%20making%20VAD%20tasks%20more%20accessible.%0AOur%20code%20is%20available%20online%20at%20github.com/SkiddieAhn/Paper-VideoPatchCore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16225v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoPatchCore%253A%2520An%2520Effective%2520Method%2520to%2520Memorize%2520Normality%2520for%2520Video%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DSunghyun%2520Ahn%2520and%2520Youngwan%2520Jo%2520and%2520Kijung%2520Lee%2520and%2520Sanghyun%2520Park%26entry.1292438233%3D%2520%2520Video%2520anomaly%2520detection%2520%2528VAD%2529%2520is%2520a%2520crucial%2520task%2520in%2520video%2520analysis%2520and%250Asurveillance%2520within%2520computer%2520vision.%2520Currently%252C%2520VAD%2520is%2520gaining%2520attention%2520with%250Amemory%2520techniques%2520that%2520store%2520the%2520features%2520of%2520normal%2520frames.%2520The%2520stored%2520features%250Aare%2520utilized%2520for%2520frame%2520reconstruction%252C%2520identifying%2520an%2520abnormality%2520when%2520a%250Asignificant%2520difference%2520exists%2520between%2520the%2520reconstructed%2520and%2520input%2520frames.%250AHowever%252C%2520this%2520approach%2520faces%2520several%2520challenges%2520due%2520to%2520the%2520simultaneous%250Aoptimization%2520required%2520for%2520both%2520the%2520memory%2520and%2520encoder-decoder%2520model.%2520These%250Achallenges%2520include%2520increased%2520optimization%2520difficulty%252C%2520complexity%2520of%250Aimplementation%252C%2520and%2520performance%2520variability%2520depending%2520on%2520the%2520memory%2520size.%2520To%250Aaddress%2520these%2520challenges%252Cwe%2520propose%2520an%2520effective%2520memory%2520method%2520for%2520VAD%252C%2520called%250AVideoPatchCore.%2520Inspired%2520by%2520PatchCore%252C%2520our%2520approach%2520introduces%2520a%2520structure%2520that%250Aprioritizes%2520memory%2520optimization%2520and%2520configures%2520three%2520types%2520of%2520memory%2520tailored%250Ato%2520the%2520characteristics%2520of%2520video%2520data.%2520This%2520method%2520effectively%2520addresses%2520the%250Alimitations%2520of%2520existing%2520memory-based%2520methods%252C%2520achieving%2520good%2520performance%250Acomparable%2520to%2520state-of-the-art%2520methods.%2520Furthermore%252C%2520our%2520method%2520requires%2520no%250Atraining%2520and%2520is%2520straightforward%2520to%2520implement%252C%2520making%2520VAD%2520tasks%2520more%2520accessible.%250AOur%2520code%2520is%2520available%2520online%2520at%2520github.com/SkiddieAhn/Paper-VideoPatchCore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16225v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoPatchCore%3A%20An%20Effective%20Method%20to%20Memorize%20Normality%20for%20Video%0A%20%20Anomaly%20Detection&entry.906535625=Sunghyun%20Ahn%20and%20Youngwan%20Jo%20and%20Kijung%20Lee%20and%20Sanghyun%20Park&entry.1292438233=%20%20Video%20anomaly%20detection%20%28VAD%29%20is%20a%20crucial%20task%20in%20video%20analysis%20and%0Asurveillance%20within%20computer%20vision.%20Currently%2C%20VAD%20is%20gaining%20attention%20with%0Amemory%20techniques%20that%20store%20the%20features%20of%20normal%20frames.%20The%20stored%20features%0Aare%20utilized%20for%20frame%20reconstruction%2C%20identifying%20an%20abnormality%20when%20a%0Asignificant%20difference%20exists%20between%20the%20reconstructed%20and%20input%20frames.%0AHowever%2C%20this%20approach%20faces%20several%20challenges%20due%20to%20the%20simultaneous%0Aoptimization%20required%20for%20both%20the%20memory%20and%20encoder-decoder%20model.%20These%0Achallenges%20include%20increased%20optimization%20difficulty%2C%20complexity%20of%0Aimplementation%2C%20and%20performance%20variability%20depending%20on%20the%20memory%20size.%20To%0Aaddress%20these%20challenges%2Cwe%20propose%20an%20effective%20memory%20method%20for%20VAD%2C%20called%0AVideoPatchCore.%20Inspired%20by%20PatchCore%2C%20our%20approach%20introduces%20a%20structure%20that%0Aprioritizes%20memory%20optimization%20and%20configures%20three%20types%20of%20memory%20tailored%0Ato%20the%20characteristics%20of%20video%20data.%20This%20method%20effectively%20addresses%20the%0Alimitations%20of%20existing%20memory-based%20methods%2C%20achieving%20good%20performance%0Acomparable%20to%20state-of-the-art%20methods.%20Furthermore%2C%20our%20method%20requires%20no%0Atraining%20and%20is%20straightforward%20to%20implement%2C%20making%20VAD%20tasks%20more%20accessible.%0AOur%20code%20is%20available%20online%20at%20github.com/SkiddieAhn/Paper-VideoPatchCore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16225v3&entry.124074799=Read"},
{"title": "Detecting Dataset Abuse in Fine-Tuning Stable Diffusion Models for\n  Text-to-Image Synthesis", "author": "Songrui Wang and Yubo Zhu and Wei Tong and Sheng Zhong", "abstract": "  Text-to-image synthesis has become highly popular for generating realistic\nand stylized images, often requiring fine-tuning generative models with\ndomain-specific datasets for specialized tasks. However, these valuable\ndatasets face risks of unauthorized usage and unapproved sharing, compromising\nthe rights of the owners. In this paper, we address the issue of dataset abuse\nduring the fine-tuning of Stable Diffusion models for text-to-image synthesis.\nWe present a dataset watermarking framework designed to detect unauthorized\nusage and trace data leaks. The framework employs two key strategies across\nmultiple watermarking schemes and is effective for large-scale dataset\nauthorization. Extensive experiments demonstrate the framework's effectiveness,\nminimal impact on the dataset (only 2% of the data required to be modified for\nhigh detection accuracy), and ability to trace data leaks. Our results also\nhighlight the robustness and transferability of the framework, proving its\npractical applicability in detecting dataset abuse.\n", "link": "http://arxiv.org/abs/2409.18897v1", "date": "2024-09-27", "relevancy": 2.2885, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6016}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.567}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Dataset%20Abuse%20in%20Fine-Tuning%20Stable%20Diffusion%20Models%20for%0A%20%20Text-to-Image%20Synthesis&body=Title%3A%20Detecting%20Dataset%20Abuse%20in%20Fine-Tuning%20Stable%20Diffusion%20Models%20for%0A%20%20Text-to-Image%20Synthesis%0AAuthor%3A%20Songrui%20Wang%20and%20Yubo%20Zhu%20and%20Wei%20Tong%20and%20Sheng%20Zhong%0AAbstract%3A%20%20%20Text-to-image%20synthesis%20has%20become%20highly%20popular%20for%20generating%20realistic%0Aand%20stylized%20images%2C%20often%20requiring%20fine-tuning%20generative%20models%20with%0Adomain-specific%20datasets%20for%20specialized%20tasks.%20However%2C%20these%20valuable%0Adatasets%20face%20risks%20of%20unauthorized%20usage%20and%20unapproved%20sharing%2C%20compromising%0Athe%20rights%20of%20the%20owners.%20In%20this%20paper%2C%20we%20address%20the%20issue%20of%20dataset%20abuse%0Aduring%20the%20fine-tuning%20of%20Stable%20Diffusion%20models%20for%20text-to-image%20synthesis.%0AWe%20present%20a%20dataset%20watermarking%20framework%20designed%20to%20detect%20unauthorized%0Ausage%20and%20trace%20data%20leaks.%20The%20framework%20employs%20two%20key%20strategies%20across%0Amultiple%20watermarking%20schemes%20and%20is%20effective%20for%20large-scale%20dataset%0Aauthorization.%20Extensive%20experiments%20demonstrate%20the%20framework%27s%20effectiveness%2C%0Aminimal%20impact%20on%20the%20dataset%20%28only%202%25%20of%20the%20data%20required%20to%20be%20modified%20for%0Ahigh%20detection%20accuracy%29%2C%20and%20ability%20to%20trace%20data%20leaks.%20Our%20results%20also%0Ahighlight%20the%20robustness%20and%20transferability%20of%20the%20framework%2C%20proving%20its%0Apractical%20applicability%20in%20detecting%20dataset%20abuse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Dataset%2520Abuse%2520in%2520Fine-Tuning%2520Stable%2520Diffusion%2520Models%2520for%250A%2520%2520Text-to-Image%2520Synthesis%26entry.906535625%3DSongrui%2520Wang%2520and%2520Yubo%2520Zhu%2520and%2520Wei%2520Tong%2520and%2520Sheng%2520Zhong%26entry.1292438233%3D%2520%2520Text-to-image%2520synthesis%2520has%2520become%2520highly%2520popular%2520for%2520generating%2520realistic%250Aand%2520stylized%2520images%252C%2520often%2520requiring%2520fine-tuning%2520generative%2520models%2520with%250Adomain-specific%2520datasets%2520for%2520specialized%2520tasks.%2520However%252C%2520these%2520valuable%250Adatasets%2520face%2520risks%2520of%2520unauthorized%2520usage%2520and%2520unapproved%2520sharing%252C%2520compromising%250Athe%2520rights%2520of%2520the%2520owners.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520issue%2520of%2520dataset%2520abuse%250Aduring%2520the%2520fine-tuning%2520of%2520Stable%2520Diffusion%2520models%2520for%2520text-to-image%2520synthesis.%250AWe%2520present%2520a%2520dataset%2520watermarking%2520framework%2520designed%2520to%2520detect%2520unauthorized%250Ausage%2520and%2520trace%2520data%2520leaks.%2520The%2520framework%2520employs%2520two%2520key%2520strategies%2520across%250Amultiple%2520watermarking%2520schemes%2520and%2520is%2520effective%2520for%2520large-scale%2520dataset%250Aauthorization.%2520Extensive%2520experiments%2520demonstrate%2520the%2520framework%2527s%2520effectiveness%252C%250Aminimal%2520impact%2520on%2520the%2520dataset%2520%2528only%25202%2525%2520of%2520the%2520data%2520required%2520to%2520be%2520modified%2520for%250Ahigh%2520detection%2520accuracy%2529%252C%2520and%2520ability%2520to%2520trace%2520data%2520leaks.%2520Our%2520results%2520also%250Ahighlight%2520the%2520robustness%2520and%2520transferability%2520of%2520the%2520framework%252C%2520proving%2520its%250Apractical%2520applicability%2520in%2520detecting%2520dataset%2520abuse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Dataset%20Abuse%20in%20Fine-Tuning%20Stable%20Diffusion%20Models%20for%0A%20%20Text-to-Image%20Synthesis&entry.906535625=Songrui%20Wang%20and%20Yubo%20Zhu%20and%20Wei%20Tong%20and%20Sheng%20Zhong&entry.1292438233=%20%20Text-to-image%20synthesis%20has%20become%20highly%20popular%20for%20generating%20realistic%0Aand%20stylized%20images%2C%20often%20requiring%20fine-tuning%20generative%20models%20with%0Adomain-specific%20datasets%20for%20specialized%20tasks.%20However%2C%20these%20valuable%0Adatasets%20face%20risks%20of%20unauthorized%20usage%20and%20unapproved%20sharing%2C%20compromising%0Athe%20rights%20of%20the%20owners.%20In%20this%20paper%2C%20we%20address%20the%20issue%20of%20dataset%20abuse%0Aduring%20the%20fine-tuning%20of%20Stable%20Diffusion%20models%20for%20text-to-image%20synthesis.%0AWe%20present%20a%20dataset%20watermarking%20framework%20designed%20to%20detect%20unauthorized%0Ausage%20and%20trace%20data%20leaks.%20The%20framework%20employs%20two%20key%20strategies%20across%0Amultiple%20watermarking%20schemes%20and%20is%20effective%20for%20large-scale%20dataset%0Aauthorization.%20Extensive%20experiments%20demonstrate%20the%20framework%27s%20effectiveness%2C%0Aminimal%20impact%20on%20the%20dataset%20%28only%202%25%20of%20the%20data%20required%20to%20be%20modified%20for%0Ahigh%20detection%20accuracy%29%2C%20and%20ability%20to%20trace%20data%20leaks.%20Our%20results%20also%0Ahighlight%20the%20robustness%20and%20transferability%20of%20the%20framework%2C%20proving%20its%0Apractical%20applicability%20in%20detecting%20dataset%20abuse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18897v1&entry.124074799=Read"},
{"title": "Proprioception Is All You Need: Terrain Classification for Boreal\n  Forests", "author": "Damien LaRocque and William Guimont-Martin and David-Alexandre Duclos and Philippe Gigu\u00e8re and Fran\u00e7ois Pomerleau", "abstract": "  Recent works in field robotics highlighted the importance of resiliency\nagainst different types of terrains. Boreal forests, in particular, are home to\nmany mobility-impeding terrains that should be considered for off-road\nautonomous navigation. Also, being one of the largest land biomes on Earth,\nboreal forests are an area where autonomous vehicles are expected to become\nincreasingly common. In this paper, we address this issue by introducing\nBorealTC, a publicly available dataset for proprioceptive-based terrain\nclassification (TC). Recorded with a Husky A200, our dataset contains 116 min\nof Inertial Measurement Unit (IMU), motor current, and wheel odometry data,\nfocusing on typical boreal forest terrains, notably snow, ice, and silty loam.\nCombining our dataset with another dataset from the state-of-the-art, we\nevaluate both a Convolutional Neural Network (CNN) and the novel state space\nmodel (SSM)-based Mamba architecture on a TC task. Interestingly, we show that\nwhile CNN outperforms Mamba on each separate dataset, Mamba achieves greater\naccuracy when trained on a combination of both. In addition, we demonstrate\nthat Mamba's learning capacity is greater than a CNN for increasing amounts of\ndata. We show that the combination of two TC datasets yields a latent space\nthat can be interpreted with the properties of the terrains. We also discuss\nthe implications of merging datasets on classification. Our source code and\ndataset are publicly available online:\nhttps://github.com/norlab-ulaval/BorealTC.\n", "link": "http://arxiv.org/abs/2403.16877v2", "date": "2024-09-27", "relevancy": 2.2803, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6548}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5758}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proprioception%20Is%20All%20You%20Need%3A%20Terrain%20Classification%20for%20Boreal%0A%20%20Forests&body=Title%3A%20Proprioception%20Is%20All%20You%20Need%3A%20Terrain%20Classification%20for%20Boreal%0A%20%20Forests%0AAuthor%3A%20Damien%20LaRocque%20and%20William%20Guimont-Martin%20and%20David-Alexandre%20Duclos%20and%20Philippe%20Gigu%C3%A8re%20and%20Fran%C3%A7ois%20Pomerleau%0AAbstract%3A%20%20%20Recent%20works%20in%20field%20robotics%20highlighted%20the%20importance%20of%20resiliency%0Aagainst%20different%20types%20of%20terrains.%20Boreal%20forests%2C%20in%20particular%2C%20are%20home%20to%0Amany%20mobility-impeding%20terrains%20that%20should%20be%20considered%20for%20off-road%0Aautonomous%20navigation.%20Also%2C%20being%20one%20of%20the%20largest%20land%20biomes%20on%20Earth%2C%0Aboreal%20forests%20are%20an%20area%20where%20autonomous%20vehicles%20are%20expected%20to%20become%0Aincreasingly%20common.%20In%20this%20paper%2C%20we%20address%20this%20issue%20by%20introducing%0ABorealTC%2C%20a%20publicly%20available%20dataset%20for%20proprioceptive-based%20terrain%0Aclassification%20%28TC%29.%20Recorded%20with%20a%20Husky%20A200%2C%20our%20dataset%20contains%20116%20min%0Aof%20Inertial%20Measurement%20Unit%20%28IMU%29%2C%20motor%20current%2C%20and%20wheel%20odometry%20data%2C%0Afocusing%20on%20typical%20boreal%20forest%20terrains%2C%20notably%20snow%2C%20ice%2C%20and%20silty%20loam.%0ACombining%20our%20dataset%20with%20another%20dataset%20from%20the%20state-of-the-art%2C%20we%0Aevaluate%20both%20a%20Convolutional%20Neural%20Network%20%28CNN%29%20and%20the%20novel%20state%20space%0Amodel%20%28SSM%29-based%20Mamba%20architecture%20on%20a%20TC%20task.%20Interestingly%2C%20we%20show%20that%0Awhile%20CNN%20outperforms%20Mamba%20on%20each%20separate%20dataset%2C%20Mamba%20achieves%20greater%0Aaccuracy%20when%20trained%20on%20a%20combination%20of%20both.%20In%20addition%2C%20we%20demonstrate%0Athat%20Mamba%27s%20learning%20capacity%20is%20greater%20than%20a%20CNN%20for%20increasing%20amounts%20of%0Adata.%20We%20show%20that%20the%20combination%20of%20two%20TC%20datasets%20yields%20a%20latent%20space%0Athat%20can%20be%20interpreted%20with%20the%20properties%20of%20the%20terrains.%20We%20also%20discuss%0Athe%20implications%20of%20merging%20datasets%20on%20classification.%20Our%20source%20code%20and%0Adataset%20are%20publicly%20available%20online%3A%0Ahttps%3A//github.com/norlab-ulaval/BorealTC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProprioception%2520Is%2520All%2520You%2520Need%253A%2520Terrain%2520Classification%2520for%2520Boreal%250A%2520%2520Forests%26entry.906535625%3DDamien%2520LaRocque%2520and%2520William%2520Guimont-Martin%2520and%2520David-Alexandre%2520Duclos%2520and%2520Philippe%2520Gigu%25C3%25A8re%2520and%2520Fran%25C3%25A7ois%2520Pomerleau%26entry.1292438233%3D%2520%2520Recent%2520works%2520in%2520field%2520robotics%2520highlighted%2520the%2520importance%2520of%2520resiliency%250Aagainst%2520different%2520types%2520of%2520terrains.%2520Boreal%2520forests%252C%2520in%2520particular%252C%2520are%2520home%2520to%250Amany%2520mobility-impeding%2520terrains%2520that%2520should%2520be%2520considered%2520for%2520off-road%250Aautonomous%2520navigation.%2520Also%252C%2520being%2520one%2520of%2520the%2520largest%2520land%2520biomes%2520on%2520Earth%252C%250Aboreal%2520forests%2520are%2520an%2520area%2520where%2520autonomous%2520vehicles%2520are%2520expected%2520to%2520become%250Aincreasingly%2520common.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520issue%2520by%2520introducing%250ABorealTC%252C%2520a%2520publicly%2520available%2520dataset%2520for%2520proprioceptive-based%2520terrain%250Aclassification%2520%2528TC%2529.%2520Recorded%2520with%2520a%2520Husky%2520A200%252C%2520our%2520dataset%2520contains%2520116%2520min%250Aof%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%252C%2520motor%2520current%252C%2520and%2520wheel%2520odometry%2520data%252C%250Afocusing%2520on%2520typical%2520boreal%2520forest%2520terrains%252C%2520notably%2520snow%252C%2520ice%252C%2520and%2520silty%2520loam.%250ACombining%2520our%2520dataset%2520with%2520another%2520dataset%2520from%2520the%2520state-of-the-art%252C%2520we%250Aevaluate%2520both%2520a%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520and%2520the%2520novel%2520state%2520space%250Amodel%2520%2528SSM%2529-based%2520Mamba%2520architecture%2520on%2520a%2520TC%2520task.%2520Interestingly%252C%2520we%2520show%2520that%250Awhile%2520CNN%2520outperforms%2520Mamba%2520on%2520each%2520separate%2520dataset%252C%2520Mamba%2520achieves%2520greater%250Aaccuracy%2520when%2520trained%2520on%2520a%2520combination%2520of%2520both.%2520In%2520addition%252C%2520we%2520demonstrate%250Athat%2520Mamba%2527s%2520learning%2520capacity%2520is%2520greater%2520than%2520a%2520CNN%2520for%2520increasing%2520amounts%2520of%250Adata.%2520We%2520show%2520that%2520the%2520combination%2520of%2520two%2520TC%2520datasets%2520yields%2520a%2520latent%2520space%250Athat%2520can%2520be%2520interpreted%2520with%2520the%2520properties%2520of%2520the%2520terrains.%2520We%2520also%2520discuss%250Athe%2520implications%2520of%2520merging%2520datasets%2520on%2520classification.%2520Our%2520source%2520code%2520and%250Adataset%2520are%2520publicly%2520available%2520online%253A%250Ahttps%253A//github.com/norlab-ulaval/BorealTC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proprioception%20Is%20All%20You%20Need%3A%20Terrain%20Classification%20for%20Boreal%0A%20%20Forests&entry.906535625=Damien%20LaRocque%20and%20William%20Guimont-Martin%20and%20David-Alexandre%20Duclos%20and%20Philippe%20Gigu%C3%A8re%20and%20Fran%C3%A7ois%20Pomerleau&entry.1292438233=%20%20Recent%20works%20in%20field%20robotics%20highlighted%20the%20importance%20of%20resiliency%0Aagainst%20different%20types%20of%20terrains.%20Boreal%20forests%2C%20in%20particular%2C%20are%20home%20to%0Amany%20mobility-impeding%20terrains%20that%20should%20be%20considered%20for%20off-road%0Aautonomous%20navigation.%20Also%2C%20being%20one%20of%20the%20largest%20land%20biomes%20on%20Earth%2C%0Aboreal%20forests%20are%20an%20area%20where%20autonomous%20vehicles%20are%20expected%20to%20become%0Aincreasingly%20common.%20In%20this%20paper%2C%20we%20address%20this%20issue%20by%20introducing%0ABorealTC%2C%20a%20publicly%20available%20dataset%20for%20proprioceptive-based%20terrain%0Aclassification%20%28TC%29.%20Recorded%20with%20a%20Husky%20A200%2C%20our%20dataset%20contains%20116%20min%0Aof%20Inertial%20Measurement%20Unit%20%28IMU%29%2C%20motor%20current%2C%20and%20wheel%20odometry%20data%2C%0Afocusing%20on%20typical%20boreal%20forest%20terrains%2C%20notably%20snow%2C%20ice%2C%20and%20silty%20loam.%0ACombining%20our%20dataset%20with%20another%20dataset%20from%20the%20state-of-the-art%2C%20we%0Aevaluate%20both%20a%20Convolutional%20Neural%20Network%20%28CNN%29%20and%20the%20novel%20state%20space%0Amodel%20%28SSM%29-based%20Mamba%20architecture%20on%20a%20TC%20task.%20Interestingly%2C%20we%20show%20that%0Awhile%20CNN%20outperforms%20Mamba%20on%20each%20separate%20dataset%2C%20Mamba%20achieves%20greater%0Aaccuracy%20when%20trained%20on%20a%20combination%20of%20both.%20In%20addition%2C%20we%20demonstrate%0Athat%20Mamba%27s%20learning%20capacity%20is%20greater%20than%20a%20CNN%20for%20increasing%20amounts%20of%0Adata.%20We%20show%20that%20the%20combination%20of%20two%20TC%20datasets%20yields%20a%20latent%20space%0Athat%20can%20be%20interpreted%20with%20the%20properties%20of%20the%20terrains.%20We%20also%20discuss%0Athe%20implications%20of%20merging%20datasets%20on%20classification.%20Our%20source%20code%20and%0Adataset%20are%20publicly%20available%20online%3A%0Ahttps%3A//github.com/norlab-ulaval/BorealTC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16877v2&entry.124074799=Read"},
{"title": "Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free\n  Efficient Vision Transformer", "author": "Huihong Shi and Haikuo Shao and Wendong Mao and Zhongfeng Wang", "abstract": "  Motivated by the huge success of Transformers in the field of natural\nlanguage processing (NLP), Vision Transformers (ViTs) have been rapidly\ndeveloped and achieved remarkable performance in various computer vision tasks.\nHowever, their huge model sizes and intensive computations hinder ViTs'\ndeployment on embedded devices, calling for effective model compression\nmethods, such as quantization. Unfortunately, due to the existence of\nhardware-unfriendly and quantization-sensitive non-linear operations,\nparticularly {Softmax}, it is non-trivial to completely quantize all operations\nin ViTs, yielding either significant accuracy drops or non-negligible hardware\ncosts. In response to challenges associated with \\textit{standard ViTs}, we\nfocus our attention towards the quantization and acceleration for\n\\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but\nalso integrate linear attention with low computational complexity, and propose\nTrio-ViT accordingly. Specifically, at the algorithm level, we develop a\n{tailored post-training quantization engine} taking the unique activation\ndistributions of Softmax-free efficient ViTs into full consideration, aiming to\nboost quantization accuracy. Furthermore, at the hardware level, we build an\naccelerator dedicated to the specific Convolution-Transformer hybrid\narchitecture of efficient ViTs, thereby enhancing hardware efficiency.\nExtensive experimental results consistently prove the effectiveness of our\nTrio-ViT framework. {Particularly, we can gain up to\n$\\uparrow$$\\mathbf{3.6}\\times$, $\\uparrow$$\\mathbf{5.0}\\times$, and\n$\\uparrow$$\\mathbf{7.3}\\times$ FPS under comparable accuracy over\nstate-of-the-art ViT accelerators, as well as $\\uparrow$$\\mathbf{6.0}\\times$,\n$\\uparrow$$\\mathbf{1.5}\\times$, and $\\uparrow$$\\mathbf{2.1}\\times$ DSP\nefficiency.} Codes are available at\n\\url{https://github.com/shihuihong214/Trio-ViT}.\n", "link": "http://arxiv.org/abs/2405.03882v2", "date": "2024-09-27", "relevancy": 2.2717, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.578}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.568}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trio-ViT%3A%20Post-Training%20Quantization%20and%20Acceleration%20for%20Softmax-Free%0A%20%20Efficient%20Vision%20Transformer&body=Title%3A%20Trio-ViT%3A%20Post-Training%20Quantization%20and%20Acceleration%20for%20Softmax-Free%0A%20%20Efficient%20Vision%20Transformer%0AAuthor%3A%20Huihong%20Shi%20and%20Haikuo%20Shao%20and%20Wendong%20Mao%20and%20Zhongfeng%20Wang%0AAbstract%3A%20%20%20Motivated%20by%20the%20huge%20success%20of%20Transformers%20in%20the%20field%20of%20natural%0Alanguage%20processing%20%28NLP%29%2C%20Vision%20Transformers%20%28ViTs%29%20have%20been%20rapidly%0Adeveloped%20and%20achieved%20remarkable%20performance%20in%20various%20computer%20vision%20tasks.%0AHowever%2C%20their%20huge%20model%20sizes%20and%20intensive%20computations%20hinder%20ViTs%27%0Adeployment%20on%20embedded%20devices%2C%20calling%20for%20effective%20model%20compression%0Amethods%2C%20such%20as%20quantization.%20Unfortunately%2C%20due%20to%20the%20existence%20of%0Ahardware-unfriendly%20and%20quantization-sensitive%20non-linear%20operations%2C%0Aparticularly%20%7BSoftmax%7D%2C%20it%20is%20non-trivial%20to%20completely%20quantize%20all%20operations%0Ain%20ViTs%2C%20yielding%20either%20significant%20accuracy%20drops%20or%20non-negligible%20hardware%0Acosts.%20In%20response%20to%20challenges%20associated%20with%20%5Ctextit%7Bstandard%20ViTs%7D%2C%20we%0Afocus%20our%20attention%20towards%20the%20quantization%20and%20acceleration%20for%0A%5Ctextit%7Befficient%20ViTs%7D%2C%20which%20not%20only%20eliminate%20the%20troublesome%20Softmax%20but%0Aalso%20integrate%20linear%20attention%20with%20low%20computational%20complexity%2C%20and%20propose%0ATrio-ViT%20accordingly.%20Specifically%2C%20at%20the%20algorithm%20level%2C%20we%20develop%20a%0A%7Btailored%20post-training%20quantization%20engine%7D%20taking%20the%20unique%20activation%0Adistributions%20of%20Softmax-free%20efficient%20ViTs%20into%20full%20consideration%2C%20aiming%20to%0Aboost%20quantization%20accuracy.%20Furthermore%2C%20at%20the%20hardware%20level%2C%20we%20build%20an%0Aaccelerator%20dedicated%20to%20the%20specific%20Convolution-Transformer%20hybrid%0Aarchitecture%20of%20efficient%20ViTs%2C%20thereby%20enhancing%20hardware%20efficiency.%0AExtensive%20experimental%20results%20consistently%20prove%20the%20effectiveness%20of%20our%0ATrio-ViT%20framework.%20%7BParticularly%2C%20we%20can%20gain%20up%20to%0A%24%5Cuparrow%24%24%5Cmathbf%7B3.6%7D%5Ctimes%24%2C%20%24%5Cuparrow%24%24%5Cmathbf%7B5.0%7D%5Ctimes%24%2C%20and%0A%24%5Cuparrow%24%24%5Cmathbf%7B7.3%7D%5Ctimes%24%20FPS%20under%20comparable%20accuracy%20over%0Astate-of-the-art%20ViT%20accelerators%2C%20as%20well%20as%20%24%5Cuparrow%24%24%5Cmathbf%7B6.0%7D%5Ctimes%24%2C%0A%24%5Cuparrow%24%24%5Cmathbf%7B1.5%7D%5Ctimes%24%2C%20and%20%24%5Cuparrow%24%24%5Cmathbf%7B2.1%7D%5Ctimes%24%20DSP%0Aefficiency.%7D%20Codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/shihuihong214/Trio-ViT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03882v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrio-ViT%253A%2520Post-Training%2520Quantization%2520and%2520Acceleration%2520for%2520Softmax-Free%250A%2520%2520Efficient%2520Vision%2520Transformer%26entry.906535625%3DHuihong%2520Shi%2520and%2520Haikuo%2520Shao%2520and%2520Wendong%2520Mao%2520and%2520Zhongfeng%2520Wang%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520huge%2520success%2520of%2520Transformers%2520in%2520the%2520field%2520of%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520been%2520rapidly%250Adeveloped%2520and%2520achieved%2520remarkable%2520performance%2520in%2520various%2520computer%2520vision%2520tasks.%250AHowever%252C%2520their%2520huge%2520model%2520sizes%2520and%2520intensive%2520computations%2520hinder%2520ViTs%2527%250Adeployment%2520on%2520embedded%2520devices%252C%2520calling%2520for%2520effective%2520model%2520compression%250Amethods%252C%2520such%2520as%2520quantization.%2520Unfortunately%252C%2520due%2520to%2520the%2520existence%2520of%250Ahardware-unfriendly%2520and%2520quantization-sensitive%2520non-linear%2520operations%252C%250Aparticularly%2520%257BSoftmax%257D%252C%2520it%2520is%2520non-trivial%2520to%2520completely%2520quantize%2520all%2520operations%250Ain%2520ViTs%252C%2520yielding%2520either%2520significant%2520accuracy%2520drops%2520or%2520non-negligible%2520hardware%250Acosts.%2520In%2520response%2520to%2520challenges%2520associated%2520with%2520%255Ctextit%257Bstandard%2520ViTs%257D%252C%2520we%250Afocus%2520our%2520attention%2520towards%2520the%2520quantization%2520and%2520acceleration%2520for%250A%255Ctextit%257Befficient%2520ViTs%257D%252C%2520which%2520not%2520only%2520eliminate%2520the%2520troublesome%2520Softmax%2520but%250Aalso%2520integrate%2520linear%2520attention%2520with%2520low%2520computational%2520complexity%252C%2520and%2520propose%250ATrio-ViT%2520accordingly.%2520Specifically%252C%2520at%2520the%2520algorithm%2520level%252C%2520we%2520develop%2520a%250A%257Btailored%2520post-training%2520quantization%2520engine%257D%2520taking%2520the%2520unique%2520activation%250Adistributions%2520of%2520Softmax-free%2520efficient%2520ViTs%2520into%2520full%2520consideration%252C%2520aiming%2520to%250Aboost%2520quantization%2520accuracy.%2520Furthermore%252C%2520at%2520the%2520hardware%2520level%252C%2520we%2520build%2520an%250Aaccelerator%2520dedicated%2520to%2520the%2520specific%2520Convolution-Transformer%2520hybrid%250Aarchitecture%2520of%2520efficient%2520ViTs%252C%2520thereby%2520enhancing%2520hardware%2520efficiency.%250AExtensive%2520experimental%2520results%2520consistently%2520prove%2520the%2520effectiveness%2520of%2520our%250ATrio-ViT%2520framework.%2520%257BParticularly%252C%2520we%2520can%2520gain%2520up%2520to%250A%2524%255Cuparrow%2524%2524%255Cmathbf%257B3.6%257D%255Ctimes%2524%252C%2520%2524%255Cuparrow%2524%2524%255Cmathbf%257B5.0%257D%255Ctimes%2524%252C%2520and%250A%2524%255Cuparrow%2524%2524%255Cmathbf%257B7.3%257D%255Ctimes%2524%2520FPS%2520under%2520comparable%2520accuracy%2520over%250Astate-of-the-art%2520ViT%2520accelerators%252C%2520as%2520well%2520as%2520%2524%255Cuparrow%2524%2524%255Cmathbf%257B6.0%257D%255Ctimes%2524%252C%250A%2524%255Cuparrow%2524%2524%255Cmathbf%257B1.5%257D%255Ctimes%2524%252C%2520and%2520%2524%255Cuparrow%2524%2524%255Cmathbf%257B2.1%257D%255Ctimes%2524%2520DSP%250Aefficiency.%257D%2520Codes%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/shihuihong214/Trio-ViT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03882v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trio-ViT%3A%20Post-Training%20Quantization%20and%20Acceleration%20for%20Softmax-Free%0A%20%20Efficient%20Vision%20Transformer&entry.906535625=Huihong%20Shi%20and%20Haikuo%20Shao%20and%20Wendong%20Mao%20and%20Zhongfeng%20Wang&entry.1292438233=%20%20Motivated%20by%20the%20huge%20success%20of%20Transformers%20in%20the%20field%20of%20natural%0Alanguage%20processing%20%28NLP%29%2C%20Vision%20Transformers%20%28ViTs%29%20have%20been%20rapidly%0Adeveloped%20and%20achieved%20remarkable%20performance%20in%20various%20computer%20vision%20tasks.%0AHowever%2C%20their%20huge%20model%20sizes%20and%20intensive%20computations%20hinder%20ViTs%27%0Adeployment%20on%20embedded%20devices%2C%20calling%20for%20effective%20model%20compression%0Amethods%2C%20such%20as%20quantization.%20Unfortunately%2C%20due%20to%20the%20existence%20of%0Ahardware-unfriendly%20and%20quantization-sensitive%20non-linear%20operations%2C%0Aparticularly%20%7BSoftmax%7D%2C%20it%20is%20non-trivial%20to%20completely%20quantize%20all%20operations%0Ain%20ViTs%2C%20yielding%20either%20significant%20accuracy%20drops%20or%20non-negligible%20hardware%0Acosts.%20In%20response%20to%20challenges%20associated%20with%20%5Ctextit%7Bstandard%20ViTs%7D%2C%20we%0Afocus%20our%20attention%20towards%20the%20quantization%20and%20acceleration%20for%0A%5Ctextit%7Befficient%20ViTs%7D%2C%20which%20not%20only%20eliminate%20the%20troublesome%20Softmax%20but%0Aalso%20integrate%20linear%20attention%20with%20low%20computational%20complexity%2C%20and%20propose%0ATrio-ViT%20accordingly.%20Specifically%2C%20at%20the%20algorithm%20level%2C%20we%20develop%20a%0A%7Btailored%20post-training%20quantization%20engine%7D%20taking%20the%20unique%20activation%0Adistributions%20of%20Softmax-free%20efficient%20ViTs%20into%20full%20consideration%2C%20aiming%20to%0Aboost%20quantization%20accuracy.%20Furthermore%2C%20at%20the%20hardware%20level%2C%20we%20build%20an%0Aaccelerator%20dedicated%20to%20the%20specific%20Convolution-Transformer%20hybrid%0Aarchitecture%20of%20efficient%20ViTs%2C%20thereby%20enhancing%20hardware%20efficiency.%0AExtensive%20experimental%20results%20consistently%20prove%20the%20effectiveness%20of%20our%0ATrio-ViT%20framework.%20%7BParticularly%2C%20we%20can%20gain%20up%20to%0A%24%5Cuparrow%24%24%5Cmathbf%7B3.6%7D%5Ctimes%24%2C%20%24%5Cuparrow%24%24%5Cmathbf%7B5.0%7D%5Ctimes%24%2C%20and%0A%24%5Cuparrow%24%24%5Cmathbf%7B7.3%7D%5Ctimes%24%20FPS%20under%20comparable%20accuracy%20over%0Astate-of-the-art%20ViT%20accelerators%2C%20as%20well%20as%20%24%5Cuparrow%24%24%5Cmathbf%7B6.0%7D%5Ctimes%24%2C%0A%24%5Cuparrow%24%24%5Cmathbf%7B1.5%7D%5Ctimes%24%2C%20and%20%24%5Cuparrow%24%24%5Cmathbf%7B2.1%7D%5Ctimes%24%20DSP%0Aefficiency.%7D%20Codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/shihuihong214/Trio-ViT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03882v2&entry.124074799=Read"},
{"title": "Personalized Video Relighting With an At-Home Light Stage", "author": "Jun Myeong Choi and Max Christman and Roni Sengupta", "abstract": "  In this paper, we develop a personalized video relighting algorithm that\nproduces high-quality and temporally consistent relit videos under any pose,\nexpression, and lighting condition in real-time. Existing relighting algorithms\ntypically rely either on publicly available synthetic data, which yields poor\nrelighting results, or on actual light stage data which is difficult to\nacquire. We show that by just capturing recordings of a user watching YouTube\nvideos on a monitor we can train a personalized algorithm capable of performing\nhigh-quality relighting under any condition. Our key contribution is a novel\nimage-based neural relighting architecture that effectively separates the\nintrinsic appearance features - the geometry and reflectance of the face - from\nthe source lighting and then combines them with the target lighting to generate\na relit image. This neural architecture enables smoothing of intrinsic\nappearance features leading to temporally stable video relighting. Both\nqualitative and quantitative evaluations show that our architecture improves\nportrait image relighting quality and temporal consistency over\nstate-of-the-art approaches on both casually captured `Light Stage at Your\nDesk' (LSYD) and light-stage-captured `One Light At a Time' (OLAT) datasets.\n", "link": "http://arxiv.org/abs/2311.08843v4", "date": "2024-09-27", "relevancy": 2.2625, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5836}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5646}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Video%20Relighting%20With%20an%20At-Home%20Light%20Stage&body=Title%3A%20Personalized%20Video%20Relighting%20With%20an%20At-Home%20Light%20Stage%0AAuthor%3A%20Jun%20Myeong%20Choi%20and%20Max%20Christman%20and%20Roni%20Sengupta%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20develop%20a%20personalized%20video%20relighting%20algorithm%20that%0Aproduces%20high-quality%20and%20temporally%20consistent%20relit%20videos%20under%20any%20pose%2C%0Aexpression%2C%20and%20lighting%20condition%20in%20real-time.%20Existing%20relighting%20algorithms%0Atypically%20rely%20either%20on%20publicly%20available%20synthetic%20data%2C%20which%20yields%20poor%0Arelighting%20results%2C%20or%20on%20actual%20light%20stage%20data%20which%20is%20difficult%20to%0Aacquire.%20We%20show%20that%20by%20just%20capturing%20recordings%20of%20a%20user%20watching%20YouTube%0Avideos%20on%20a%20monitor%20we%20can%20train%20a%20personalized%20algorithm%20capable%20of%20performing%0Ahigh-quality%20relighting%20under%20any%20condition.%20Our%20key%20contribution%20is%20a%20novel%0Aimage-based%20neural%20relighting%20architecture%20that%20effectively%20separates%20the%0Aintrinsic%20appearance%20features%20-%20the%20geometry%20and%20reflectance%20of%20the%20face%20-%20from%0Athe%20source%20lighting%20and%20then%20combines%20them%20with%20the%20target%20lighting%20to%20generate%0Aa%20relit%20image.%20This%20neural%20architecture%20enables%20smoothing%20of%20intrinsic%0Aappearance%20features%20leading%20to%20temporally%20stable%20video%20relighting.%20Both%0Aqualitative%20and%20quantitative%20evaluations%20show%20that%20our%20architecture%20improves%0Aportrait%20image%20relighting%20quality%20and%20temporal%20consistency%20over%0Astate-of-the-art%20approaches%20on%20both%20casually%20captured%20%60Light%20Stage%20at%20Your%0ADesk%27%20%28LSYD%29%20and%20light-stage-captured%20%60One%20Light%20At%20a%20Time%27%20%28OLAT%29%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08843v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Video%2520Relighting%2520With%2520an%2520At-Home%2520Light%2520Stage%26entry.906535625%3DJun%2520Myeong%2520Choi%2520and%2520Max%2520Christman%2520and%2520Roni%2520Sengupta%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520personalized%2520video%2520relighting%2520algorithm%2520that%250Aproduces%2520high-quality%2520and%2520temporally%2520consistent%2520relit%2520videos%2520under%2520any%2520pose%252C%250Aexpression%252C%2520and%2520lighting%2520condition%2520in%2520real-time.%2520Existing%2520relighting%2520algorithms%250Atypically%2520rely%2520either%2520on%2520publicly%2520available%2520synthetic%2520data%252C%2520which%2520yields%2520poor%250Arelighting%2520results%252C%2520or%2520on%2520actual%2520light%2520stage%2520data%2520which%2520is%2520difficult%2520to%250Aacquire.%2520We%2520show%2520that%2520by%2520just%2520capturing%2520recordings%2520of%2520a%2520user%2520watching%2520YouTube%250Avideos%2520on%2520a%2520monitor%2520we%2520can%2520train%2520a%2520personalized%2520algorithm%2520capable%2520of%2520performing%250Ahigh-quality%2520relighting%2520under%2520any%2520condition.%2520Our%2520key%2520contribution%2520is%2520a%2520novel%250Aimage-based%2520neural%2520relighting%2520architecture%2520that%2520effectively%2520separates%2520the%250Aintrinsic%2520appearance%2520features%2520-%2520the%2520geometry%2520and%2520reflectance%2520of%2520the%2520face%2520-%2520from%250Athe%2520source%2520lighting%2520and%2520then%2520combines%2520them%2520with%2520the%2520target%2520lighting%2520to%2520generate%250Aa%2520relit%2520image.%2520This%2520neural%2520architecture%2520enables%2520smoothing%2520of%2520intrinsic%250Aappearance%2520features%2520leading%2520to%2520temporally%2520stable%2520video%2520relighting.%2520Both%250Aqualitative%2520and%2520quantitative%2520evaluations%2520show%2520that%2520our%2520architecture%2520improves%250Aportrait%2520image%2520relighting%2520quality%2520and%2520temporal%2520consistency%2520over%250Astate-of-the-art%2520approaches%2520on%2520both%2520casually%2520captured%2520%2560Light%2520Stage%2520at%2520Your%250ADesk%2527%2520%2528LSYD%2529%2520and%2520light-stage-captured%2520%2560One%2520Light%2520At%2520a%2520Time%2527%2520%2528OLAT%2529%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08843v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Video%20Relighting%20With%20an%20At-Home%20Light%20Stage&entry.906535625=Jun%20Myeong%20Choi%20and%20Max%20Christman%20and%20Roni%20Sengupta&entry.1292438233=%20%20In%20this%20paper%2C%20we%20develop%20a%20personalized%20video%20relighting%20algorithm%20that%0Aproduces%20high-quality%20and%20temporally%20consistent%20relit%20videos%20under%20any%20pose%2C%0Aexpression%2C%20and%20lighting%20condition%20in%20real-time.%20Existing%20relighting%20algorithms%0Atypically%20rely%20either%20on%20publicly%20available%20synthetic%20data%2C%20which%20yields%20poor%0Arelighting%20results%2C%20or%20on%20actual%20light%20stage%20data%20which%20is%20difficult%20to%0Aacquire.%20We%20show%20that%20by%20just%20capturing%20recordings%20of%20a%20user%20watching%20YouTube%0Avideos%20on%20a%20monitor%20we%20can%20train%20a%20personalized%20algorithm%20capable%20of%20performing%0Ahigh-quality%20relighting%20under%20any%20condition.%20Our%20key%20contribution%20is%20a%20novel%0Aimage-based%20neural%20relighting%20architecture%20that%20effectively%20separates%20the%0Aintrinsic%20appearance%20features%20-%20the%20geometry%20and%20reflectance%20of%20the%20face%20-%20from%0Athe%20source%20lighting%20and%20then%20combines%20them%20with%20the%20target%20lighting%20to%20generate%0Aa%20relit%20image.%20This%20neural%20architecture%20enables%20smoothing%20of%20intrinsic%0Aappearance%20features%20leading%20to%20temporally%20stable%20video%20relighting.%20Both%0Aqualitative%20and%20quantitative%20evaluations%20show%20that%20our%20architecture%20improves%0Aportrait%20image%20relighting%20quality%20and%20temporal%20consistency%20over%0Astate-of-the-art%20approaches%20on%20both%20casually%20captured%20%60Light%20Stage%20at%20Your%0ADesk%27%20%28LSYD%29%20and%20light-stage-captured%20%60One%20Light%20At%20a%20Time%27%20%28OLAT%29%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08843v4&entry.124074799=Read"},
{"title": "Lens: A Foundation Model for Network Traffic", "author": "Qineng Wang and Chen Qian and Xiaochang Li and Ziyu Yao and Gang Zhou and Huajie Shao", "abstract": "  Network traffic refers to the amount of data being sent and received over the\ninternet or any system that connects computers. Analyzing and understanding\nnetwork traffic is vital for improving network security and management.\nHowever, the analysis of network traffic is challenging due to the diverse\nnature of data packets, which often feature heterogeneous headers and encrypted\npayloads lacking semantics. To capture the latent semantics of traffic, a few\nstudies have adopted pre-training techniques based on the Transformer encoder\nor decoder to learn the representations from massive traffic data. However,\nthese methods typically excel in traffic understanding (classification) or\ntraffic generation tasks. To address this issue, we develop Lens, a foundation\nmodel for network traffic that leverages the T5 architecture to learn the\npre-trained representations from large-scale unlabeled data. Harnessing the\nstrength of the encoder-decoder framework, which captures the global\ninformation while preserving the generative ability, our model can better learn\nthe representations from raw data. To further enhance pre-training\neffectiveness, we design a novel loss that combines three distinct tasks:\nMasked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous\nTraffic Prediction (HTP). Evaluation results across various benchmark datasets\ndemonstrate that the proposed Lens outperforms the baselines in most downstream\ntasks related to both traffic understanding and generation. Notably, it also\nrequires much less labeled data for fine-tuning compared to current methods.\n", "link": "http://arxiv.org/abs/2402.03646v4", "date": "2024-09-27", "relevancy": 2.2498, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lens%3A%20A%20Foundation%20Model%20for%20Network%20Traffic&body=Title%3A%20Lens%3A%20A%20Foundation%20Model%20for%20Network%20Traffic%0AAuthor%3A%20Qineng%20Wang%20and%20Chen%20Qian%20and%20Xiaochang%20Li%20and%20Ziyu%20Yao%20and%20Gang%20Zhou%20and%20Huajie%20Shao%0AAbstract%3A%20%20%20Network%20traffic%20refers%20to%20the%20amount%20of%20data%20being%20sent%20and%20received%20over%20the%0Ainternet%20or%20any%20system%20that%20connects%20computers.%20Analyzing%20and%20understanding%0Anetwork%20traffic%20is%20vital%20for%20improving%20network%20security%20and%20management.%0AHowever%2C%20the%20analysis%20of%20network%20traffic%20is%20challenging%20due%20to%20the%20diverse%0Anature%20of%20data%20packets%2C%20which%20often%20feature%20heterogeneous%20headers%20and%20encrypted%0Apayloads%20lacking%20semantics.%20To%20capture%20the%20latent%20semantics%20of%20traffic%2C%20a%20few%0Astudies%20have%20adopted%20pre-training%20techniques%20based%20on%20the%20Transformer%20encoder%0Aor%20decoder%20to%20learn%20the%20representations%20from%20massive%20traffic%20data.%20However%2C%0Athese%20methods%20typically%20excel%20in%20traffic%20understanding%20%28classification%29%20or%0Atraffic%20generation%20tasks.%20To%20address%20this%20issue%2C%20we%20develop%20Lens%2C%20a%20foundation%0Amodel%20for%20network%20traffic%20that%20leverages%20the%20T5%20architecture%20to%20learn%20the%0Apre-trained%20representations%20from%20large-scale%20unlabeled%20data.%20Harnessing%20the%0Astrength%20of%20the%20encoder-decoder%20framework%2C%20which%20captures%20the%20global%0Ainformation%20while%20preserving%20the%20generative%20ability%2C%20our%20model%20can%20better%20learn%0Athe%20representations%20from%20raw%20data.%20To%20further%20enhance%20pre-training%0Aeffectiveness%2C%20we%20design%20a%20novel%20loss%20that%20combines%20three%20distinct%20tasks%3A%0AMasked%20Span%20Prediction%20%28MSP%29%2C%20Packet%20Order%20Prediction%20%28POP%29%2C%20and%20Homologous%0ATraffic%20Prediction%20%28HTP%29.%20Evaluation%20results%20across%20various%20benchmark%20datasets%0Ademonstrate%20that%20the%20proposed%20Lens%20outperforms%20the%20baselines%20in%20most%20downstream%0Atasks%20related%20to%20both%20traffic%20understanding%20and%20generation.%20Notably%2C%20it%20also%0Arequires%20much%20less%20labeled%20data%20for%20fine-tuning%20compared%20to%20current%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03646v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLens%253A%2520A%2520Foundation%2520Model%2520for%2520Network%2520Traffic%26entry.906535625%3DQineng%2520Wang%2520and%2520Chen%2520Qian%2520and%2520Xiaochang%2520Li%2520and%2520Ziyu%2520Yao%2520and%2520Gang%2520Zhou%2520and%2520Huajie%2520Shao%26entry.1292438233%3D%2520%2520Network%2520traffic%2520refers%2520to%2520the%2520amount%2520of%2520data%2520being%2520sent%2520and%2520received%2520over%2520the%250Ainternet%2520or%2520any%2520system%2520that%2520connects%2520computers.%2520Analyzing%2520and%2520understanding%250Anetwork%2520traffic%2520is%2520vital%2520for%2520improving%2520network%2520security%2520and%2520management.%250AHowever%252C%2520the%2520analysis%2520of%2520network%2520traffic%2520is%2520challenging%2520due%2520to%2520the%2520diverse%250Anature%2520of%2520data%2520packets%252C%2520which%2520often%2520feature%2520heterogeneous%2520headers%2520and%2520encrypted%250Apayloads%2520lacking%2520semantics.%2520To%2520capture%2520the%2520latent%2520semantics%2520of%2520traffic%252C%2520a%2520few%250Astudies%2520have%2520adopted%2520pre-training%2520techniques%2520based%2520on%2520the%2520Transformer%2520encoder%250Aor%2520decoder%2520to%2520learn%2520the%2520representations%2520from%2520massive%2520traffic%2520data.%2520However%252C%250Athese%2520methods%2520typically%2520excel%2520in%2520traffic%2520understanding%2520%2528classification%2529%2520or%250Atraffic%2520generation%2520tasks.%2520To%2520address%2520this%2520issue%252C%2520we%2520develop%2520Lens%252C%2520a%2520foundation%250Amodel%2520for%2520network%2520traffic%2520that%2520leverages%2520the%2520T5%2520architecture%2520to%2520learn%2520the%250Apre-trained%2520representations%2520from%2520large-scale%2520unlabeled%2520data.%2520Harnessing%2520the%250Astrength%2520of%2520the%2520encoder-decoder%2520framework%252C%2520which%2520captures%2520the%2520global%250Ainformation%2520while%2520preserving%2520the%2520generative%2520ability%252C%2520our%2520model%2520can%2520better%2520learn%250Athe%2520representations%2520from%2520raw%2520data.%2520To%2520further%2520enhance%2520pre-training%250Aeffectiveness%252C%2520we%2520design%2520a%2520novel%2520loss%2520that%2520combines%2520three%2520distinct%2520tasks%253A%250AMasked%2520Span%2520Prediction%2520%2528MSP%2529%252C%2520Packet%2520Order%2520Prediction%2520%2528POP%2529%252C%2520and%2520Homologous%250ATraffic%2520Prediction%2520%2528HTP%2529.%2520Evaluation%2520results%2520across%2520various%2520benchmark%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520Lens%2520outperforms%2520the%2520baselines%2520in%2520most%2520downstream%250Atasks%2520related%2520to%2520both%2520traffic%2520understanding%2520and%2520generation.%2520Notably%252C%2520it%2520also%250Arequires%2520much%2520less%2520labeled%2520data%2520for%2520fine-tuning%2520compared%2520to%2520current%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03646v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lens%3A%20A%20Foundation%20Model%20for%20Network%20Traffic&entry.906535625=Qineng%20Wang%20and%20Chen%20Qian%20and%20Xiaochang%20Li%20and%20Ziyu%20Yao%20and%20Gang%20Zhou%20and%20Huajie%20Shao&entry.1292438233=%20%20Network%20traffic%20refers%20to%20the%20amount%20of%20data%20being%20sent%20and%20received%20over%20the%0Ainternet%20or%20any%20system%20that%20connects%20computers.%20Analyzing%20and%20understanding%0Anetwork%20traffic%20is%20vital%20for%20improving%20network%20security%20and%20management.%0AHowever%2C%20the%20analysis%20of%20network%20traffic%20is%20challenging%20due%20to%20the%20diverse%0Anature%20of%20data%20packets%2C%20which%20often%20feature%20heterogeneous%20headers%20and%20encrypted%0Apayloads%20lacking%20semantics.%20To%20capture%20the%20latent%20semantics%20of%20traffic%2C%20a%20few%0Astudies%20have%20adopted%20pre-training%20techniques%20based%20on%20the%20Transformer%20encoder%0Aor%20decoder%20to%20learn%20the%20representations%20from%20massive%20traffic%20data.%20However%2C%0Athese%20methods%20typically%20excel%20in%20traffic%20understanding%20%28classification%29%20or%0Atraffic%20generation%20tasks.%20To%20address%20this%20issue%2C%20we%20develop%20Lens%2C%20a%20foundation%0Amodel%20for%20network%20traffic%20that%20leverages%20the%20T5%20architecture%20to%20learn%20the%0Apre-trained%20representations%20from%20large-scale%20unlabeled%20data.%20Harnessing%20the%0Astrength%20of%20the%20encoder-decoder%20framework%2C%20which%20captures%20the%20global%0Ainformation%20while%20preserving%20the%20generative%20ability%2C%20our%20model%20can%20better%20learn%0Athe%20representations%20from%20raw%20data.%20To%20further%20enhance%20pre-training%0Aeffectiveness%2C%20we%20design%20a%20novel%20loss%20that%20combines%20three%20distinct%20tasks%3A%0AMasked%20Span%20Prediction%20%28MSP%29%2C%20Packet%20Order%20Prediction%20%28POP%29%2C%20and%20Homologous%0ATraffic%20Prediction%20%28HTP%29.%20Evaluation%20results%20across%20various%20benchmark%20datasets%0Ademonstrate%20that%20the%20proposed%20Lens%20outperforms%20the%20baselines%20in%20most%20downstream%0Atasks%20related%20to%20both%20traffic%20understanding%20and%20generation.%20Notably%2C%20it%20also%0Arequires%20much%20less%20labeled%20data%20for%20fine-tuning%20compared%20to%20current%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03646v4&entry.124074799=Read"},
{"title": "Enhancing Spectrum Efficiency in 6G Satellite Networks: A GAIL-Powered\n  Policy Learning via Asynchronous Federated Inverse Reinforcement Learning", "author": "Sheikh Salman Hassan and Yu Min Park and Yan Kyaw Tun and Walid Saad and Zhu Han and Choong Seon Hong", "abstract": "  In this paper, a novel generative adversarial imitation learning\n(GAIL)-powered policy learning approach is proposed for optimizing beamforming,\nspectrum allocation, and remote user equipment (RUE) association in NTNs.\nTraditional reinforcement learning (RL) methods for wireless network\noptimization often rely on manually designed reward functions, which can\nrequire extensive parameter tuning. To overcome these limitations, we employ\ninverse RL (IRL), specifically leveraging the GAIL framework, to automatically\nlearn reward functions without manual design. We augment this framework with an\nasynchronous federated learning approach, enabling decentralized\nmulti-satellite systems to collaboratively derive optimal policies. The\nproposed method aims to maximize spectrum efficiency (SE) while meeting minimum\ninformation rate requirements for RUEs. To address the non-convex, NP-hard\nnature of this problem, we combine the many-to-one matching theory with a\nmulti-agent asynchronous federated IRL (MA-AFIRL) framework. This allows agents\nto learn through asynchronous environmental interactions, improving training\nefficiency and scalability. The expert policy is generated using the Whale\noptimization algorithm (WOA), providing data to train the automatic reward\nfunction within GAIL. Simulation results show that the proposed MA-AFIRL method\noutperforms traditional RL approaches, achieving a $14.6\\%$ improvement in\nconvergence and reward value. The novel GAIL-driven policy learning establishes\na novel benchmark for 6G NTN optimization.\n", "link": "http://arxiv.org/abs/2409.18718v1", "date": "2024-09-27", "relevancy": 2.2389, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4561}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4519}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Spectrum%20Efficiency%20in%206G%20Satellite%20Networks%3A%20A%20GAIL-Powered%0A%20%20Policy%20Learning%20via%20Asynchronous%20Federated%20Inverse%20Reinforcement%20Learning&body=Title%3A%20Enhancing%20Spectrum%20Efficiency%20in%206G%20Satellite%20Networks%3A%20A%20GAIL-Powered%0A%20%20Policy%20Learning%20via%20Asynchronous%20Federated%20Inverse%20Reinforcement%20Learning%0AAuthor%3A%20Sheikh%20Salman%20Hassan%20and%20Yu%20Min%20Park%20and%20Yan%20Kyaw%20Tun%20and%20Walid%20Saad%20and%20Zhu%20Han%20and%20Choong%20Seon%20Hong%0AAbstract%3A%20%20%20In%20this%20paper%2C%20a%20novel%20generative%20adversarial%20imitation%20learning%0A%28GAIL%29-powered%20policy%20learning%20approach%20is%20proposed%20for%20optimizing%20beamforming%2C%0Aspectrum%20allocation%2C%20and%20remote%20user%20equipment%20%28RUE%29%20association%20in%20NTNs.%0ATraditional%20reinforcement%20learning%20%28RL%29%20methods%20for%20wireless%20network%0Aoptimization%20often%20rely%20on%20manually%20designed%20reward%20functions%2C%20which%20can%0Arequire%20extensive%20parameter%20tuning.%20To%20overcome%20these%20limitations%2C%20we%20employ%0Ainverse%20RL%20%28IRL%29%2C%20specifically%20leveraging%20the%20GAIL%20framework%2C%20to%20automatically%0Alearn%20reward%20functions%20without%20manual%20design.%20We%20augment%20this%20framework%20with%20an%0Aasynchronous%20federated%20learning%20approach%2C%20enabling%20decentralized%0Amulti-satellite%20systems%20to%20collaboratively%20derive%20optimal%20policies.%20The%0Aproposed%20method%20aims%20to%20maximize%20spectrum%20efficiency%20%28SE%29%20while%20meeting%20minimum%0Ainformation%20rate%20requirements%20for%20RUEs.%20To%20address%20the%20non-convex%2C%20NP-hard%0Anature%20of%20this%20problem%2C%20we%20combine%20the%20many-to-one%20matching%20theory%20with%20a%0Amulti-agent%20asynchronous%20federated%20IRL%20%28MA-AFIRL%29%20framework.%20This%20allows%20agents%0Ato%20learn%20through%20asynchronous%20environmental%20interactions%2C%20improving%20training%0Aefficiency%20and%20scalability.%20The%20expert%20policy%20is%20generated%20using%20the%20Whale%0Aoptimization%20algorithm%20%28WOA%29%2C%20providing%20data%20to%20train%20the%20automatic%20reward%0Afunction%20within%20GAIL.%20Simulation%20results%20show%20that%20the%20proposed%20MA-AFIRL%20method%0Aoutperforms%20traditional%20RL%20approaches%2C%20achieving%20a%20%2414.6%5C%25%24%20improvement%20in%0Aconvergence%20and%20reward%20value.%20The%20novel%20GAIL-driven%20policy%20learning%20establishes%0Aa%20novel%20benchmark%20for%206G%20NTN%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Spectrum%2520Efficiency%2520in%25206G%2520Satellite%2520Networks%253A%2520A%2520GAIL-Powered%250A%2520%2520Policy%2520Learning%2520via%2520Asynchronous%2520Federated%2520Inverse%2520Reinforcement%2520Learning%26entry.906535625%3DSheikh%2520Salman%2520Hassan%2520and%2520Yu%2520Min%2520Park%2520and%2520Yan%2520Kyaw%2520Tun%2520and%2520Walid%2520Saad%2520and%2520Zhu%2520Han%2520and%2520Choong%2520Seon%2520Hong%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520a%2520novel%2520generative%2520adversarial%2520imitation%2520learning%250A%2528GAIL%2529-powered%2520policy%2520learning%2520approach%2520is%2520proposed%2520for%2520optimizing%2520beamforming%252C%250Aspectrum%2520allocation%252C%2520and%2520remote%2520user%2520equipment%2520%2528RUE%2529%2520association%2520in%2520NTNs.%250ATraditional%2520reinforcement%2520learning%2520%2528RL%2529%2520methods%2520for%2520wireless%2520network%250Aoptimization%2520often%2520rely%2520on%2520manually%2520designed%2520reward%2520functions%252C%2520which%2520can%250Arequire%2520extensive%2520parameter%2520tuning.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520employ%250Ainverse%2520RL%2520%2528IRL%2529%252C%2520specifically%2520leveraging%2520the%2520GAIL%2520framework%252C%2520to%2520automatically%250Alearn%2520reward%2520functions%2520without%2520manual%2520design.%2520We%2520augment%2520this%2520framework%2520with%2520an%250Aasynchronous%2520federated%2520learning%2520approach%252C%2520enabling%2520decentralized%250Amulti-satellite%2520systems%2520to%2520collaboratively%2520derive%2520optimal%2520policies.%2520The%250Aproposed%2520method%2520aims%2520to%2520maximize%2520spectrum%2520efficiency%2520%2528SE%2529%2520while%2520meeting%2520minimum%250Ainformation%2520rate%2520requirements%2520for%2520RUEs.%2520To%2520address%2520the%2520non-convex%252C%2520NP-hard%250Anature%2520of%2520this%2520problem%252C%2520we%2520combine%2520the%2520many-to-one%2520matching%2520theory%2520with%2520a%250Amulti-agent%2520asynchronous%2520federated%2520IRL%2520%2528MA-AFIRL%2529%2520framework.%2520This%2520allows%2520agents%250Ato%2520learn%2520through%2520asynchronous%2520environmental%2520interactions%252C%2520improving%2520training%250Aefficiency%2520and%2520scalability.%2520The%2520expert%2520policy%2520is%2520generated%2520using%2520the%2520Whale%250Aoptimization%2520algorithm%2520%2528WOA%2529%252C%2520providing%2520data%2520to%2520train%2520the%2520automatic%2520reward%250Afunction%2520within%2520GAIL.%2520Simulation%2520results%2520show%2520that%2520the%2520proposed%2520MA-AFIRL%2520method%250Aoutperforms%2520traditional%2520RL%2520approaches%252C%2520achieving%2520a%2520%252414.6%255C%2525%2524%2520improvement%2520in%250Aconvergence%2520and%2520reward%2520value.%2520The%2520novel%2520GAIL-driven%2520policy%2520learning%2520establishes%250Aa%2520novel%2520benchmark%2520for%25206G%2520NTN%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Spectrum%20Efficiency%20in%206G%20Satellite%20Networks%3A%20A%20GAIL-Powered%0A%20%20Policy%20Learning%20via%20Asynchronous%20Federated%20Inverse%20Reinforcement%20Learning&entry.906535625=Sheikh%20Salman%20Hassan%20and%20Yu%20Min%20Park%20and%20Yan%20Kyaw%20Tun%20and%20Walid%20Saad%20and%20Zhu%20Han%20and%20Choong%20Seon%20Hong&entry.1292438233=%20%20In%20this%20paper%2C%20a%20novel%20generative%20adversarial%20imitation%20learning%0A%28GAIL%29-powered%20policy%20learning%20approach%20is%20proposed%20for%20optimizing%20beamforming%2C%0Aspectrum%20allocation%2C%20and%20remote%20user%20equipment%20%28RUE%29%20association%20in%20NTNs.%0ATraditional%20reinforcement%20learning%20%28RL%29%20methods%20for%20wireless%20network%0Aoptimization%20often%20rely%20on%20manually%20designed%20reward%20functions%2C%20which%20can%0Arequire%20extensive%20parameter%20tuning.%20To%20overcome%20these%20limitations%2C%20we%20employ%0Ainverse%20RL%20%28IRL%29%2C%20specifically%20leveraging%20the%20GAIL%20framework%2C%20to%20automatically%0Alearn%20reward%20functions%20without%20manual%20design.%20We%20augment%20this%20framework%20with%20an%0Aasynchronous%20federated%20learning%20approach%2C%20enabling%20decentralized%0Amulti-satellite%20systems%20to%20collaboratively%20derive%20optimal%20policies.%20The%0Aproposed%20method%20aims%20to%20maximize%20spectrum%20efficiency%20%28SE%29%20while%20meeting%20minimum%0Ainformation%20rate%20requirements%20for%20RUEs.%20To%20address%20the%20non-convex%2C%20NP-hard%0Anature%20of%20this%20problem%2C%20we%20combine%20the%20many-to-one%20matching%20theory%20with%20a%0Amulti-agent%20asynchronous%20federated%20IRL%20%28MA-AFIRL%29%20framework.%20This%20allows%20agents%0Ato%20learn%20through%20asynchronous%20environmental%20interactions%2C%20improving%20training%0Aefficiency%20and%20scalability.%20The%20expert%20policy%20is%20generated%20using%20the%20Whale%0Aoptimization%20algorithm%20%28WOA%29%2C%20providing%20data%20to%20train%20the%20automatic%20reward%0Afunction%20within%20GAIL.%20Simulation%20results%20show%20that%20the%20proposed%20MA-AFIRL%20method%0Aoutperforms%20traditional%20RL%20approaches%2C%20achieving%20a%20%2414.6%5C%25%24%20improvement%20in%0Aconvergence%20and%20reward%20value.%20The%20novel%20GAIL-driven%20policy%20learning%20establishes%0Aa%20novel%20benchmark%20for%206G%20NTN%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18718v1&entry.124074799=Read"},
{"title": "Royal Reveals: LiDAR Mapping of Kronborg Castle, Echoes of Hamlet's\n  Halls", "author": "Leon Davies and Simon S\u00f8lvsten", "abstract": "  This paper presents a large scale dataset from a meticulous 360-degree LiDAR\n(Light Detection and Ranging) scan conducted on Kronborg Castle, a renowned\nRenaissance fortress located in Elsinore (Helsing{\\o}r), Denmark, famously\nassociated with Shakespeare's \"Hamlet.\" Utilising a vertical mounted, gimbal\nstabilised, 16 channel, 360-degree Velodyne VLP-16 LiDAR scanner, paired with\nan Intel RealSense L515 depth camera. This research offers an unparalleled\ndigital representation of the castle's intricate architectural details and\nstructural nuances, enabling fellow researchers to conduct experiments\nutilising the data for SLAM (Simultaneous Localisation and Mapping) as well as\nfloorplan generation.\n", "link": "http://arxiv.org/abs/2409.18752v1", "date": "2024-09-27", "relevancy": 2.2351, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Royal%20Reveals%3A%20LiDAR%20Mapping%20of%20Kronborg%20Castle%2C%20Echoes%20of%20Hamlet%27s%0A%20%20Halls&body=Title%3A%20Royal%20Reveals%3A%20LiDAR%20Mapping%20of%20Kronborg%20Castle%2C%20Echoes%20of%20Hamlet%27s%0A%20%20Halls%0AAuthor%3A%20Leon%20Davies%20and%20Simon%20S%C3%B8lvsten%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20large%20scale%20dataset%20from%20a%20meticulous%20360-degree%20LiDAR%0A%28Light%20Detection%20and%20Ranging%29%20scan%20conducted%20on%20Kronborg%20Castle%2C%20a%20renowned%0ARenaissance%20fortress%20located%20in%20Elsinore%20%28Helsing%7B%5Co%7Dr%29%2C%20Denmark%2C%20famously%0Aassociated%20with%20Shakespeare%27s%20%22Hamlet.%22%20Utilising%20a%20vertical%20mounted%2C%20gimbal%0Astabilised%2C%2016%20channel%2C%20360-degree%20Velodyne%20VLP-16%20LiDAR%20scanner%2C%20paired%20with%0Aan%20Intel%20RealSense%20L515%20depth%20camera.%20This%20research%20offers%20an%20unparalleled%0Adigital%20representation%20of%20the%20castle%27s%20intricate%20architectural%20details%20and%0Astructural%20nuances%2C%20enabling%20fellow%20researchers%20to%20conduct%20experiments%0Autilising%20the%20data%20for%20SLAM%20%28Simultaneous%20Localisation%20and%20Mapping%29%20as%20well%20as%0Afloorplan%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoyal%2520Reveals%253A%2520LiDAR%2520Mapping%2520of%2520Kronborg%2520Castle%252C%2520Echoes%2520of%2520Hamlet%2527s%250A%2520%2520Halls%26entry.906535625%3DLeon%2520Davies%2520and%2520Simon%2520S%25C3%25B8lvsten%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520large%2520scale%2520dataset%2520from%2520a%2520meticulous%2520360-degree%2520LiDAR%250A%2528Light%2520Detection%2520and%2520Ranging%2529%2520scan%2520conducted%2520on%2520Kronborg%2520Castle%252C%2520a%2520renowned%250ARenaissance%2520fortress%2520located%2520in%2520Elsinore%2520%2528Helsing%257B%255Co%257Dr%2529%252C%2520Denmark%252C%2520famously%250Aassociated%2520with%2520Shakespeare%2527s%2520%2522Hamlet.%2522%2520Utilising%2520a%2520vertical%2520mounted%252C%2520gimbal%250Astabilised%252C%252016%2520channel%252C%2520360-degree%2520Velodyne%2520VLP-16%2520LiDAR%2520scanner%252C%2520paired%2520with%250Aan%2520Intel%2520RealSense%2520L515%2520depth%2520camera.%2520This%2520research%2520offers%2520an%2520unparalleled%250Adigital%2520representation%2520of%2520the%2520castle%2527s%2520intricate%2520architectural%2520details%2520and%250Astructural%2520nuances%252C%2520enabling%2520fellow%2520researchers%2520to%2520conduct%2520experiments%250Autilising%2520the%2520data%2520for%2520SLAM%2520%2528Simultaneous%2520Localisation%2520and%2520Mapping%2529%2520as%2520well%2520as%250Afloorplan%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Royal%20Reveals%3A%20LiDAR%20Mapping%20of%20Kronborg%20Castle%2C%20Echoes%20of%20Hamlet%27s%0A%20%20Halls&entry.906535625=Leon%20Davies%20and%20Simon%20S%C3%B8lvsten&entry.1292438233=%20%20This%20paper%20presents%20a%20large%20scale%20dataset%20from%20a%20meticulous%20360-degree%20LiDAR%0A%28Light%20Detection%20and%20Ranging%29%20scan%20conducted%20on%20Kronborg%20Castle%2C%20a%20renowned%0ARenaissance%20fortress%20located%20in%20Elsinore%20%28Helsing%7B%5Co%7Dr%29%2C%20Denmark%2C%20famously%0Aassociated%20with%20Shakespeare%27s%20%22Hamlet.%22%20Utilising%20a%20vertical%20mounted%2C%20gimbal%0Astabilised%2C%2016%20channel%2C%20360-degree%20Velodyne%20VLP-16%20LiDAR%20scanner%2C%20paired%20with%0Aan%20Intel%20RealSense%20L515%20depth%20camera.%20This%20research%20offers%20an%20unparalleled%0Adigital%20representation%20of%20the%20castle%27s%20intricate%20architectural%20details%20and%0Astructural%20nuances%2C%20enabling%20fellow%20researchers%20to%20conduct%20experiments%0Autilising%20the%20data%20for%20SLAM%20%28Simultaneous%20Localisation%20and%20Mapping%29%20as%20well%20as%0Afloorplan%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18752v1&entry.124074799=Read"},
{"title": "DualDn: Dual-domain Denoising via Differentiable ISP", "author": "Ruikang Li and Yujin Wang and Shiqi Chen and Fan Zhang and Jinwei Gu and Tianfan Xue", "abstract": "  Image denoising is a critical component in a camera's Image Signal Processing\n(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP\npipeline: applying a denoiser directly to captured raw frames (raw domain) or\nto the ISP's output sRGB images (sRGB domain). However, both approaches have\ntheir limitations. Residual noise from raw-domain denoising can be amplified by\nthe subsequent ISP processing, and the sRGB domain struggles to handle\nspatially varying noise since it only sees noise distorted by the ISP.\nConsequently, most raw or sRGB domain denoising works only for specific noise\ndistributions and ISP configurations. To address these challenges, we propose\nDualDn, a novel learning-based dual-domain denoising. Unlike previous\nsingle-domain denoising, DualDn consists of two denoising networks: one in the\nraw domain and one in the sRGB domain. The raw domain denoising adapts to\nsensor-specific noise as well as spatially varying noise levels, while the sRGB\ndomain denoising adapts to ISP variations and removes residual noise amplified\nby the ISP. Both denoising networks are connected with a differentiable ISP,\nwhich is trained end-to-end and discarded during the inference stage. With this\ndesign, DualDn achieves greater generalizability compared to most\nlearning-based denoising methods, as it can adapt to different unseen noises,\nISP parameters, and even novel ISP pipelines. Experiments show that DualDn\nachieves state-of-the-art performance and can adapt to different denoising\narchitectures. Moreover, DualDn can be used as a plug-and-play denoising module\nwith real cameras without retraining, and still demonstrate better performance\nthan commercial on-camera denoising. The project website is available at:\nhttps://openimaginglab.github.io/DualDn/\n", "link": "http://arxiv.org/abs/2409.18783v1", "date": "2024-09-27", "relevancy": 2.2181, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5781}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.567}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualDn%3A%20Dual-domain%20Denoising%20via%20Differentiable%20ISP&body=Title%3A%20DualDn%3A%20Dual-domain%20Denoising%20via%20Differentiable%20ISP%0AAuthor%3A%20Ruikang%20Li%20and%20Yujin%20Wang%20and%20Shiqi%20Chen%20and%20Fan%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%0AAbstract%3A%20%20%20Image%20denoising%20is%20a%20critical%20component%20in%20a%20camera%27s%20Image%20Signal%20Processing%0A%28ISP%29%20pipeline.%20There%20are%20two%20typical%20ways%20to%20inject%20a%20denoiser%20into%20the%20ISP%0Apipeline%3A%20applying%20a%20denoiser%20directly%20to%20captured%20raw%20frames%20%28raw%20domain%29%20or%0Ato%20the%20ISP%27s%20output%20sRGB%20images%20%28sRGB%20domain%29.%20However%2C%20both%20approaches%20have%0Atheir%20limitations.%20Residual%20noise%20from%20raw-domain%20denoising%20can%20be%20amplified%20by%0Athe%20subsequent%20ISP%20processing%2C%20and%20the%20sRGB%20domain%20struggles%20to%20handle%0Aspatially%20varying%20noise%20since%20it%20only%20sees%20noise%20distorted%20by%20the%20ISP.%0AConsequently%2C%20most%20raw%20or%20sRGB%20domain%20denoising%20works%20only%20for%20specific%20noise%0Adistributions%20and%20ISP%20configurations.%20To%20address%20these%20challenges%2C%20we%20propose%0ADualDn%2C%20a%20novel%20learning-based%20dual-domain%20denoising.%20Unlike%20previous%0Asingle-domain%20denoising%2C%20DualDn%20consists%20of%20two%20denoising%20networks%3A%20one%20in%20the%0Araw%20domain%20and%20one%20in%20the%20sRGB%20domain.%20The%20raw%20domain%20denoising%20adapts%20to%0Asensor-specific%20noise%20as%20well%20as%20spatially%20varying%20noise%20levels%2C%20while%20the%20sRGB%0Adomain%20denoising%20adapts%20to%20ISP%20variations%20and%20removes%20residual%20noise%20amplified%0Aby%20the%20ISP.%20Both%20denoising%20networks%20are%20connected%20with%20a%20differentiable%20ISP%2C%0Awhich%20is%20trained%20end-to-end%20and%20discarded%20during%20the%20inference%20stage.%20With%20this%0Adesign%2C%20DualDn%20achieves%20greater%20generalizability%20compared%20to%20most%0Alearning-based%20denoising%20methods%2C%20as%20it%20can%20adapt%20to%20different%20unseen%20noises%2C%0AISP%20parameters%2C%20and%20even%20novel%20ISP%20pipelines.%20Experiments%20show%20that%20DualDn%0Aachieves%20state-of-the-art%20performance%20and%20can%20adapt%20to%20different%20denoising%0Aarchitectures.%20Moreover%2C%20DualDn%20can%20be%20used%20as%20a%20plug-and-play%20denoising%20module%0Awith%20real%20cameras%20without%20retraining%2C%20and%20still%20demonstrate%20better%20performance%0Athan%20commercial%20on-camera%20denoising.%20The%20project%20website%20is%20available%20at%3A%0Ahttps%3A//openimaginglab.github.io/DualDn/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualDn%253A%2520Dual-domain%2520Denoising%2520via%2520Differentiable%2520ISP%26entry.906535625%3DRuikang%2520Li%2520and%2520Yujin%2520Wang%2520and%2520Shiqi%2520Chen%2520and%2520Fan%2520Zhang%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%26entry.1292438233%3D%2520%2520Image%2520denoising%2520is%2520a%2520critical%2520component%2520in%2520a%2520camera%2527s%2520Image%2520Signal%2520Processing%250A%2528ISP%2529%2520pipeline.%2520There%2520are%2520two%2520typical%2520ways%2520to%2520inject%2520a%2520denoiser%2520into%2520the%2520ISP%250Apipeline%253A%2520applying%2520a%2520denoiser%2520directly%2520to%2520captured%2520raw%2520frames%2520%2528raw%2520domain%2529%2520or%250Ato%2520the%2520ISP%2527s%2520output%2520sRGB%2520images%2520%2528sRGB%2520domain%2529.%2520However%252C%2520both%2520approaches%2520have%250Atheir%2520limitations.%2520Residual%2520noise%2520from%2520raw-domain%2520denoising%2520can%2520be%2520amplified%2520by%250Athe%2520subsequent%2520ISP%2520processing%252C%2520and%2520the%2520sRGB%2520domain%2520struggles%2520to%2520handle%250Aspatially%2520varying%2520noise%2520since%2520it%2520only%2520sees%2520noise%2520distorted%2520by%2520the%2520ISP.%250AConsequently%252C%2520most%2520raw%2520or%2520sRGB%2520domain%2520denoising%2520works%2520only%2520for%2520specific%2520noise%250Adistributions%2520and%2520ISP%2520configurations.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250ADualDn%252C%2520a%2520novel%2520learning-based%2520dual-domain%2520denoising.%2520Unlike%2520previous%250Asingle-domain%2520denoising%252C%2520DualDn%2520consists%2520of%2520two%2520denoising%2520networks%253A%2520one%2520in%2520the%250Araw%2520domain%2520and%2520one%2520in%2520the%2520sRGB%2520domain.%2520The%2520raw%2520domain%2520denoising%2520adapts%2520to%250Asensor-specific%2520noise%2520as%2520well%2520as%2520spatially%2520varying%2520noise%2520levels%252C%2520while%2520the%2520sRGB%250Adomain%2520denoising%2520adapts%2520to%2520ISP%2520variations%2520and%2520removes%2520residual%2520noise%2520amplified%250Aby%2520the%2520ISP.%2520Both%2520denoising%2520networks%2520are%2520connected%2520with%2520a%2520differentiable%2520ISP%252C%250Awhich%2520is%2520trained%2520end-to-end%2520and%2520discarded%2520during%2520the%2520inference%2520stage.%2520With%2520this%250Adesign%252C%2520DualDn%2520achieves%2520greater%2520generalizability%2520compared%2520to%2520most%250Alearning-based%2520denoising%2520methods%252C%2520as%2520it%2520can%2520adapt%2520to%2520different%2520unseen%2520noises%252C%250AISP%2520parameters%252C%2520and%2520even%2520novel%2520ISP%2520pipelines.%2520Experiments%2520show%2520that%2520DualDn%250Aachieves%2520state-of-the-art%2520performance%2520and%2520can%2520adapt%2520to%2520different%2520denoising%250Aarchitectures.%2520Moreover%252C%2520DualDn%2520can%2520be%2520used%2520as%2520a%2520plug-and-play%2520denoising%2520module%250Awith%2520real%2520cameras%2520without%2520retraining%252C%2520and%2520still%2520demonstrate%2520better%2520performance%250Athan%2520commercial%2520on-camera%2520denoising.%2520The%2520project%2520website%2520is%2520available%2520at%253A%250Ahttps%253A//openimaginglab.github.io/DualDn/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualDn%3A%20Dual-domain%20Denoising%20via%20Differentiable%20ISP&entry.906535625=Ruikang%20Li%20and%20Yujin%20Wang%20and%20Shiqi%20Chen%20and%20Fan%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue&entry.1292438233=%20%20Image%20denoising%20is%20a%20critical%20component%20in%20a%20camera%27s%20Image%20Signal%20Processing%0A%28ISP%29%20pipeline.%20There%20are%20two%20typical%20ways%20to%20inject%20a%20denoiser%20into%20the%20ISP%0Apipeline%3A%20applying%20a%20denoiser%20directly%20to%20captured%20raw%20frames%20%28raw%20domain%29%20or%0Ato%20the%20ISP%27s%20output%20sRGB%20images%20%28sRGB%20domain%29.%20However%2C%20both%20approaches%20have%0Atheir%20limitations.%20Residual%20noise%20from%20raw-domain%20denoising%20can%20be%20amplified%20by%0Athe%20subsequent%20ISP%20processing%2C%20and%20the%20sRGB%20domain%20struggles%20to%20handle%0Aspatially%20varying%20noise%20since%20it%20only%20sees%20noise%20distorted%20by%20the%20ISP.%0AConsequently%2C%20most%20raw%20or%20sRGB%20domain%20denoising%20works%20only%20for%20specific%20noise%0Adistributions%20and%20ISP%20configurations.%20To%20address%20these%20challenges%2C%20we%20propose%0ADualDn%2C%20a%20novel%20learning-based%20dual-domain%20denoising.%20Unlike%20previous%0Asingle-domain%20denoising%2C%20DualDn%20consists%20of%20two%20denoising%20networks%3A%20one%20in%20the%0Araw%20domain%20and%20one%20in%20the%20sRGB%20domain.%20The%20raw%20domain%20denoising%20adapts%20to%0Asensor-specific%20noise%20as%20well%20as%20spatially%20varying%20noise%20levels%2C%20while%20the%20sRGB%0Adomain%20denoising%20adapts%20to%20ISP%20variations%20and%20removes%20residual%20noise%20amplified%0Aby%20the%20ISP.%20Both%20denoising%20networks%20are%20connected%20with%20a%20differentiable%20ISP%2C%0Awhich%20is%20trained%20end-to-end%20and%20discarded%20during%20the%20inference%20stage.%20With%20this%0Adesign%2C%20DualDn%20achieves%20greater%20generalizability%20compared%20to%20most%0Alearning-based%20denoising%20methods%2C%20as%20it%20can%20adapt%20to%20different%20unseen%20noises%2C%0AISP%20parameters%2C%20and%20even%20novel%20ISP%20pipelines.%20Experiments%20show%20that%20DualDn%0Aachieves%20state-of-the-art%20performance%20and%20can%20adapt%20to%20different%20denoising%0Aarchitectures.%20Moreover%2C%20DualDn%20can%20be%20used%20as%20a%20plug-and-play%20denoising%20module%0Awith%20real%20cameras%20without%20retraining%2C%20and%20still%20demonstrate%20better%20performance%0Athan%20commercial%20on-camera%20denoising.%20The%20project%20website%20is%20available%20at%3A%0Ahttps%3A//openimaginglab.github.io/DualDn/%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18783v1&entry.124074799=Read"},
{"title": "Enhancing Explainability in Multimodal Large Language Models Using\n  Ontological Context", "author": "Jihen Amara and Birgitta K\u00f6nig-Ries and Sheeba Samuel", "abstract": "  Recently, there has been a growing interest in Multimodal Large Language\nModels (MLLMs) due to their remarkable potential in various tasks integrating\ndifferent modalities, such as image and text, as well as applications such as\nimage captioning and visual question answering. However, such models still face\nchallenges in accurately captioning and interpreting specific visual concepts\nand classes, particularly in domain-specific applications. We argue that\nintegrating domain knowledge in the form of an ontology can significantly\naddress these issues. In this work, as a proof of concept, we propose a new\nframework that combines ontology with MLLMs to classify images of plant\ndiseases. Our method uses concepts about plant diseases from an existing\ndisease ontology to query MLLMs and extract relevant visual concepts from\nimages. Then, we use the reasoning capabilities of the ontology to classify the\ndisease according to the identified concepts. Ensuring that the model\naccurately uses the concepts describing the disease is crucial in\ndomain-specific applications. By employing an ontology, we can assist in\nverifying this alignment. Additionally, using the ontology's inference\ncapabilities increases transparency, explainability, and trust in the\ndecision-making process while serving as a judge by checking if the annotations\nof the concepts by MLLMs are aligned with those in the ontology and displaying\nthe rationales behind their errors. Our framework offers a new direction for\nsynergizing ontologies and MLLMs, supported by an empirical study using\ndifferent well-known MLLMs.\n", "link": "http://arxiv.org/abs/2409.18753v1", "date": "2024-09-27", "relevancy": 2.1923, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Explainability%20in%20Multimodal%20Large%20Language%20Models%20Using%0A%20%20Ontological%20Context&body=Title%3A%20Enhancing%20Explainability%20in%20Multimodal%20Large%20Language%20Models%20Using%0A%20%20Ontological%20Context%0AAuthor%3A%20Jihen%20Amara%20and%20Birgitta%20K%C3%B6nig-Ries%20and%20Sheeba%20Samuel%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20a%20growing%20interest%20in%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20due%20to%20their%20remarkable%20potential%20in%20various%20tasks%20integrating%0Adifferent%20modalities%2C%20such%20as%20image%20and%20text%2C%20as%20well%20as%20applications%20such%20as%0Aimage%20captioning%20and%20visual%20question%20answering.%20However%2C%20such%20models%20still%20face%0Achallenges%20in%20accurately%20captioning%20and%20interpreting%20specific%20visual%20concepts%0Aand%20classes%2C%20particularly%20in%20domain-specific%20applications.%20We%20argue%20that%0Aintegrating%20domain%20knowledge%20in%20the%20form%20of%20an%20ontology%20can%20significantly%0Aaddress%20these%20issues.%20In%20this%20work%2C%20as%20a%20proof%20of%20concept%2C%20we%20propose%20a%20new%0Aframework%20that%20combines%20ontology%20with%20MLLMs%20to%20classify%20images%20of%20plant%0Adiseases.%20Our%20method%20uses%20concepts%20about%20plant%20diseases%20from%20an%20existing%0Adisease%20ontology%20to%20query%20MLLMs%20and%20extract%20relevant%20visual%20concepts%20from%0Aimages.%20Then%2C%20we%20use%20the%20reasoning%20capabilities%20of%20the%20ontology%20to%20classify%20the%0Adisease%20according%20to%20the%20identified%20concepts.%20Ensuring%20that%20the%20model%0Aaccurately%20uses%20the%20concepts%20describing%20the%20disease%20is%20crucial%20in%0Adomain-specific%20applications.%20By%20employing%20an%20ontology%2C%20we%20can%20assist%20in%0Averifying%20this%20alignment.%20Additionally%2C%20using%20the%20ontology%27s%20inference%0Acapabilities%20increases%20transparency%2C%20explainability%2C%20and%20trust%20in%20the%0Adecision-making%20process%20while%20serving%20as%20a%20judge%20by%20checking%20if%20the%20annotations%0Aof%20the%20concepts%20by%20MLLMs%20are%20aligned%20with%20those%20in%20the%20ontology%20and%20displaying%0Athe%20rationales%20behind%20their%20errors.%20Our%20framework%20offers%20a%20new%20direction%20for%0Asynergizing%20ontologies%20and%20MLLMs%2C%20supported%20by%20an%20empirical%20study%20using%0Adifferent%20well-known%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Explainability%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520Using%250A%2520%2520Ontological%2520Context%26entry.906535625%3DJihen%2520Amara%2520and%2520Birgitta%2520K%25C3%25B6nig-Ries%2520and%2520Sheeba%2520Samuel%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520a%2520growing%2520interest%2520in%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520due%2520to%2520their%2520remarkable%2520potential%2520in%2520various%2520tasks%2520integrating%250Adifferent%2520modalities%252C%2520such%2520as%2520image%2520and%2520text%252C%2520as%2520well%2520as%2520applications%2520such%2520as%250Aimage%2520captioning%2520and%2520visual%2520question%2520answering.%2520However%252C%2520such%2520models%2520still%2520face%250Achallenges%2520in%2520accurately%2520captioning%2520and%2520interpreting%2520specific%2520visual%2520concepts%250Aand%2520classes%252C%2520particularly%2520in%2520domain-specific%2520applications.%2520We%2520argue%2520that%250Aintegrating%2520domain%2520knowledge%2520in%2520the%2520form%2520of%2520an%2520ontology%2520can%2520significantly%250Aaddress%2520these%2520issues.%2520In%2520this%2520work%252C%2520as%2520a%2520proof%2520of%2520concept%252C%2520we%2520propose%2520a%2520new%250Aframework%2520that%2520combines%2520ontology%2520with%2520MLLMs%2520to%2520classify%2520images%2520of%2520plant%250Adiseases.%2520Our%2520method%2520uses%2520concepts%2520about%2520plant%2520diseases%2520from%2520an%2520existing%250Adisease%2520ontology%2520to%2520query%2520MLLMs%2520and%2520extract%2520relevant%2520visual%2520concepts%2520from%250Aimages.%2520Then%252C%2520we%2520use%2520the%2520reasoning%2520capabilities%2520of%2520the%2520ontology%2520to%2520classify%2520the%250Adisease%2520according%2520to%2520the%2520identified%2520concepts.%2520Ensuring%2520that%2520the%2520model%250Aaccurately%2520uses%2520the%2520concepts%2520describing%2520the%2520disease%2520is%2520crucial%2520in%250Adomain-specific%2520applications.%2520By%2520employing%2520an%2520ontology%252C%2520we%2520can%2520assist%2520in%250Averifying%2520this%2520alignment.%2520Additionally%252C%2520using%2520the%2520ontology%2527s%2520inference%250Acapabilities%2520increases%2520transparency%252C%2520explainability%252C%2520and%2520trust%2520in%2520the%250Adecision-making%2520process%2520while%2520serving%2520as%2520a%2520judge%2520by%2520checking%2520if%2520the%2520annotations%250Aof%2520the%2520concepts%2520by%2520MLLMs%2520are%2520aligned%2520with%2520those%2520in%2520the%2520ontology%2520and%2520displaying%250Athe%2520rationales%2520behind%2520their%2520errors.%2520Our%2520framework%2520offers%2520a%2520new%2520direction%2520for%250Asynergizing%2520ontologies%2520and%2520MLLMs%252C%2520supported%2520by%2520an%2520empirical%2520study%2520using%250Adifferent%2520well-known%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Explainability%20in%20Multimodal%20Large%20Language%20Models%20Using%0A%20%20Ontological%20Context&entry.906535625=Jihen%20Amara%20and%20Birgitta%20K%C3%B6nig-Ries%20and%20Sheeba%20Samuel&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20a%20growing%20interest%20in%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20due%20to%20their%20remarkable%20potential%20in%20various%20tasks%20integrating%0Adifferent%20modalities%2C%20such%20as%20image%20and%20text%2C%20as%20well%20as%20applications%20such%20as%0Aimage%20captioning%20and%20visual%20question%20answering.%20However%2C%20such%20models%20still%20face%0Achallenges%20in%20accurately%20captioning%20and%20interpreting%20specific%20visual%20concepts%0Aand%20classes%2C%20particularly%20in%20domain-specific%20applications.%20We%20argue%20that%0Aintegrating%20domain%20knowledge%20in%20the%20form%20of%20an%20ontology%20can%20significantly%0Aaddress%20these%20issues.%20In%20this%20work%2C%20as%20a%20proof%20of%20concept%2C%20we%20propose%20a%20new%0Aframework%20that%20combines%20ontology%20with%20MLLMs%20to%20classify%20images%20of%20plant%0Adiseases.%20Our%20method%20uses%20concepts%20about%20plant%20diseases%20from%20an%20existing%0Adisease%20ontology%20to%20query%20MLLMs%20and%20extract%20relevant%20visual%20concepts%20from%0Aimages.%20Then%2C%20we%20use%20the%20reasoning%20capabilities%20of%20the%20ontology%20to%20classify%20the%0Adisease%20according%20to%20the%20identified%20concepts.%20Ensuring%20that%20the%20model%0Aaccurately%20uses%20the%20concepts%20describing%20the%20disease%20is%20crucial%20in%0Adomain-specific%20applications.%20By%20employing%20an%20ontology%2C%20we%20can%20assist%20in%0Averifying%20this%20alignment.%20Additionally%2C%20using%20the%20ontology%27s%20inference%0Acapabilities%20increases%20transparency%2C%20explainability%2C%20and%20trust%20in%20the%0Adecision-making%20process%20while%20serving%20as%20a%20judge%20by%20checking%20if%20the%20annotations%0Aof%20the%20concepts%20by%20MLLMs%20are%20aligned%20with%20those%20in%20the%20ontology%20and%20displaying%0Athe%20rationales%20behind%20their%20errors.%20Our%20framework%20offers%20a%20new%20direction%20for%0Asynergizing%20ontologies%20and%20MLLMs%2C%20supported%20by%20an%20empirical%20study%20using%0Adifferent%20well-known%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18753v1&entry.124074799=Read"},
{"title": "Trained Without My Consent: Detecting Code Inclusion In Language Models\n  Trained on Code", "author": "Vahid Majdinasab and Amin Nikanjam and Foutse Khomh", "abstract": "  Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets.\n", "link": "http://arxiv.org/abs/2402.09299v2", "date": "2024-09-27", "relevancy": 2.1915, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4569}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4361}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trained%20Without%20My%20Consent%3A%20Detecting%20Code%20Inclusion%20In%20Language%20Models%0A%20%20Trained%20on%20Code&body=Title%3A%20Trained%20Without%20My%20Consent%3A%20Detecting%20Code%20Inclusion%20In%20Language%20Models%0A%20%20Trained%20on%20Code%0AAuthor%3A%20Vahid%20Majdinasab%20and%20Amin%20Nikanjam%20and%20Foutse%20Khomh%0AAbstract%3A%20%20%20Code%20auditing%20ensures%20that%20the%20developed%20code%20adheres%20to%20standards%2C%0Aregulations%2C%20and%20copyright%20protection%20by%20verifying%20that%20it%20does%20not%20contain%0Acode%20from%20protected%20sources.%20The%20recent%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%0Aas%20coding%20assistants%20in%20the%20software%20development%20process%20poses%20new%20challenges%0Afor%20code%20auditing.%20The%20dataset%20for%20training%20these%20models%20is%20mainly%20collected%0Afrom%20publicly%20available%20sources.%20This%20raises%20the%20issue%20of%20intellectual%20property%0Ainfringement%20as%20developers%27%20codes%20are%20already%20included%20in%20the%20dataset.%0ATherefore%2C%20auditing%20code%20developed%20using%20LLMs%20is%20challenging%2C%20as%20it%20is%0Adifficult%20to%20reliably%20assert%20if%20an%20LLM%20used%20during%20development%20has%20been%20trained%0Aon%20specific%20copyrighted%20codes%2C%20given%20that%20we%20do%20not%20have%20access%20to%20the%20training%0Adatasets%20of%20these%20models.%20Given%20the%20non-disclosure%20of%20the%20training%20datasets%2C%0Atraditional%20approaches%20such%20as%20code%20clone%20detection%20are%20insufficient%20for%0Aasserting%20copyright%20infringement.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20new%0Aapproach%2C%20TraWiC%3B%20a%20model-agnostic%20and%20interpretable%20method%20based%20on%20membership%0Ainference%20for%20detecting%20code%20inclusion%20in%20an%20LLM%27s%20training%20dataset.%20We%20extract%0Asyntactic%20and%20semantic%20identifiers%20unique%20to%20each%20program%20to%20train%20a%20classifier%0Afor%20detecting%20code%20inclusion.%20In%20our%20experiments%2C%20we%20observe%20that%20TraWiC%20is%0Acapable%20of%20detecting%2083.87%25%20of%20codes%20that%20were%20used%20to%20train%20an%20LLM.%20In%0Acomparison%2C%20the%20prevalent%20clone%20detection%20tool%20NiCad%20is%20only%20capable%20of%0Adetecting%2047.64%25.%20In%20addition%20to%20its%20remarkable%20performance%2C%20TraWiC%20has%20low%0Aresource%20overhead%20in%20contrast%20to%20pair-wise%20clone%20detection%20that%20is%20conducted%0Aduring%20the%20auditing%20process%20of%20tools%20like%20CodeWhisperer%20reference%20tracker%2C%0Aacross%20thousands%20of%20code%20snippets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrained%2520Without%2520My%2520Consent%253A%2520Detecting%2520Code%2520Inclusion%2520In%2520Language%2520Models%250A%2520%2520Trained%2520on%2520Code%26entry.906535625%3DVahid%2520Majdinasab%2520and%2520Amin%2520Nikanjam%2520and%2520Foutse%2520Khomh%26entry.1292438233%3D%2520%2520Code%2520auditing%2520ensures%2520that%2520the%2520developed%2520code%2520adheres%2520to%2520standards%252C%250Aregulations%252C%2520and%2520copyright%2520protection%2520by%2520verifying%2520that%2520it%2520does%2520not%2520contain%250Acode%2520from%2520protected%2520sources.%2520The%2520recent%2520advent%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Aas%2520coding%2520assistants%2520in%2520the%2520software%2520development%2520process%2520poses%2520new%2520challenges%250Afor%2520code%2520auditing.%2520The%2520dataset%2520for%2520training%2520these%2520models%2520is%2520mainly%2520collected%250Afrom%2520publicly%2520available%2520sources.%2520This%2520raises%2520the%2520issue%2520of%2520intellectual%2520property%250Ainfringement%2520as%2520developers%2527%2520codes%2520are%2520already%2520included%2520in%2520the%2520dataset.%250ATherefore%252C%2520auditing%2520code%2520developed%2520using%2520LLMs%2520is%2520challenging%252C%2520as%2520it%2520is%250Adifficult%2520to%2520reliably%2520assert%2520if%2520an%2520LLM%2520used%2520during%2520development%2520has%2520been%2520trained%250Aon%2520specific%2520copyrighted%2520codes%252C%2520given%2520that%2520we%2520do%2520not%2520have%2520access%2520to%2520the%2520training%250Adatasets%2520of%2520these%2520models.%2520Given%2520the%2520non-disclosure%2520of%2520the%2520training%2520datasets%252C%250Atraditional%2520approaches%2520such%2520as%2520code%2520clone%2520detection%2520are%2520insufficient%2520for%250Aasserting%2520copyright%2520infringement.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520new%250Aapproach%252C%2520TraWiC%253B%2520a%2520model-agnostic%2520and%2520interpretable%2520method%2520based%2520on%2520membership%250Ainference%2520for%2520detecting%2520code%2520inclusion%2520in%2520an%2520LLM%2527s%2520training%2520dataset.%2520We%2520extract%250Asyntactic%2520and%2520semantic%2520identifiers%2520unique%2520to%2520each%2520program%2520to%2520train%2520a%2520classifier%250Afor%2520detecting%2520code%2520inclusion.%2520In%2520our%2520experiments%252C%2520we%2520observe%2520that%2520TraWiC%2520is%250Acapable%2520of%2520detecting%252083.87%2525%2520of%2520codes%2520that%2520were%2520used%2520to%2520train%2520an%2520LLM.%2520In%250Acomparison%252C%2520the%2520prevalent%2520clone%2520detection%2520tool%2520NiCad%2520is%2520only%2520capable%2520of%250Adetecting%252047.64%2525.%2520In%2520addition%2520to%2520its%2520remarkable%2520performance%252C%2520TraWiC%2520has%2520low%250Aresource%2520overhead%2520in%2520contrast%2520to%2520pair-wise%2520clone%2520detection%2520that%2520is%2520conducted%250Aduring%2520the%2520auditing%2520process%2520of%2520tools%2520like%2520CodeWhisperer%2520reference%2520tracker%252C%250Aacross%2520thousands%2520of%2520code%2520snippets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trained%20Without%20My%20Consent%3A%20Detecting%20Code%20Inclusion%20In%20Language%20Models%0A%20%20Trained%20on%20Code&entry.906535625=Vahid%20Majdinasab%20and%20Amin%20Nikanjam%20and%20Foutse%20Khomh&entry.1292438233=%20%20Code%20auditing%20ensures%20that%20the%20developed%20code%20adheres%20to%20standards%2C%0Aregulations%2C%20and%20copyright%20protection%20by%20verifying%20that%20it%20does%20not%20contain%0Acode%20from%20protected%20sources.%20The%20recent%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%0Aas%20coding%20assistants%20in%20the%20software%20development%20process%20poses%20new%20challenges%0Afor%20code%20auditing.%20The%20dataset%20for%20training%20these%20models%20is%20mainly%20collected%0Afrom%20publicly%20available%20sources.%20This%20raises%20the%20issue%20of%20intellectual%20property%0Ainfringement%20as%20developers%27%20codes%20are%20already%20included%20in%20the%20dataset.%0ATherefore%2C%20auditing%20code%20developed%20using%20LLMs%20is%20challenging%2C%20as%20it%20is%0Adifficult%20to%20reliably%20assert%20if%20an%20LLM%20used%20during%20development%20has%20been%20trained%0Aon%20specific%20copyrighted%20codes%2C%20given%20that%20we%20do%20not%20have%20access%20to%20the%20training%0Adatasets%20of%20these%20models.%20Given%20the%20non-disclosure%20of%20the%20training%20datasets%2C%0Atraditional%20approaches%20such%20as%20code%20clone%20detection%20are%20insufficient%20for%0Aasserting%20copyright%20infringement.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20new%0Aapproach%2C%20TraWiC%3B%20a%20model-agnostic%20and%20interpretable%20method%20based%20on%20membership%0Ainference%20for%20detecting%20code%20inclusion%20in%20an%20LLM%27s%20training%20dataset.%20We%20extract%0Asyntactic%20and%20semantic%20identifiers%20unique%20to%20each%20program%20to%20train%20a%20classifier%0Afor%20detecting%20code%20inclusion.%20In%20our%20experiments%2C%20we%20observe%20that%20TraWiC%20is%0Acapable%20of%20detecting%2083.87%25%20of%20codes%20that%20were%20used%20to%20train%20an%20LLM.%20In%0Acomparison%2C%20the%20prevalent%20clone%20detection%20tool%20NiCad%20is%20only%20capable%20of%0Adetecting%2047.64%25.%20In%20addition%20to%20its%20remarkable%20performance%2C%20TraWiC%20has%20low%0Aresource%20overhead%20in%20contrast%20to%20pair-wise%20clone%20detection%20that%20is%20conducted%0Aduring%20the%20auditing%20process%20of%20tools%20like%20CodeWhisperer%20reference%20tracker%2C%0Aacross%20thousands%20of%20code%20snippets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09299v2&entry.124074799=Read"},
{"title": "Student-Oriented Teacher Knowledge Refinement for Knowledge Distillation", "author": "Chaomin Shen and Yaomin Huang and Haokun Zhu and Jinsong Fan and Guixu Zhang", "abstract": "  Knowledge distillation has become widely recognized for its ability to\ntransfer knowledge from a large teacher network to a compact and more\nstreamlined student network. Traditional knowledge distillation methods\nprimarily follow a teacher-oriented paradigm that imposes the task of learning\nthe teacher's complex knowledge onto the student network. However, significant\ndisparities in model capacity and architectural design hinder the student's\ncomprehension of the complex knowledge imparted by the teacher, resulting in\nsub-optimal performance. This paper introduces a novel perspective emphasizing\nstudent-oriented and refining the teacher's knowledge to better align with the\nstudent's needs, thereby improving knowledge transfer effectiveness.\nSpecifically, we present the Student-Oriented Knowledge Distillation (SoKD),\nwhich incorporates a learnable feature augmentation strategy during training to\nrefine the teacher's knowledge of the student dynamically. Furthermore, we\ndeploy the Distinctive Area Detection Module (DAM) to identify areas of mutual\ninterest between the teacher and student, concentrating knowledge transfer\nwithin these critical areas to avoid transferring irrelevant information. This\ncustomized module ensures a more focused and effective knowledge distillation\nprocess. Our approach, functioning as a plug-in, could be integrated with\nvarious knowledge distillation methods. Extensive experimental results\ndemonstrate the efficacy and generalizability of our method.\n", "link": "http://arxiv.org/abs/2409.18785v1", "date": "2024-09-27", "relevancy": 2.1868, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4485}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4319}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Student-Oriented%20Teacher%20Knowledge%20Refinement%20for%20Knowledge%20Distillation&body=Title%3A%20Student-Oriented%20Teacher%20Knowledge%20Refinement%20for%20Knowledge%20Distillation%0AAuthor%3A%20Chaomin%20Shen%20and%20Yaomin%20Huang%20and%20Haokun%20Zhu%20and%20Jinsong%20Fan%20and%20Guixu%20Zhang%0AAbstract%3A%20%20%20Knowledge%20distillation%20has%20become%20widely%20recognized%20for%20its%20ability%20to%0Atransfer%20knowledge%20from%20a%20large%20teacher%20network%20to%20a%20compact%20and%20more%0Astreamlined%20student%20network.%20Traditional%20knowledge%20distillation%20methods%0Aprimarily%20follow%20a%20teacher-oriented%20paradigm%20that%20imposes%20the%20task%20of%20learning%0Athe%20teacher%27s%20complex%20knowledge%20onto%20the%20student%20network.%20However%2C%20significant%0Adisparities%20in%20model%20capacity%20and%20architectural%20design%20hinder%20the%20student%27s%0Acomprehension%20of%20the%20complex%20knowledge%20imparted%20by%20the%20teacher%2C%20resulting%20in%0Asub-optimal%20performance.%20This%20paper%20introduces%20a%20novel%20perspective%20emphasizing%0Astudent-oriented%20and%20refining%20the%20teacher%27s%20knowledge%20to%20better%20align%20with%20the%0Astudent%27s%20needs%2C%20thereby%20improving%20knowledge%20transfer%20effectiveness.%0ASpecifically%2C%20we%20present%20the%20Student-Oriented%20Knowledge%20Distillation%20%28SoKD%29%2C%0Awhich%20incorporates%20a%20learnable%20feature%20augmentation%20strategy%20during%20training%20to%0Arefine%20the%20teacher%27s%20knowledge%20of%20the%20student%20dynamically.%20Furthermore%2C%20we%0Adeploy%20the%20Distinctive%20Area%20Detection%20Module%20%28DAM%29%20to%20identify%20areas%20of%20mutual%0Ainterest%20between%20the%20teacher%20and%20student%2C%20concentrating%20knowledge%20transfer%0Awithin%20these%20critical%20areas%20to%20avoid%20transferring%20irrelevant%20information.%20This%0Acustomized%20module%20ensures%20a%20more%20focused%20and%20effective%20knowledge%20distillation%0Aprocess.%20Our%20approach%2C%20functioning%20as%20a%20plug-in%2C%20could%20be%20integrated%20with%0Avarious%20knowledge%20distillation%20methods.%20Extensive%20experimental%20results%0Ademonstrate%20the%20efficacy%20and%20generalizability%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudent-Oriented%2520Teacher%2520Knowledge%2520Refinement%2520for%2520Knowledge%2520Distillation%26entry.906535625%3DChaomin%2520Shen%2520and%2520Yaomin%2520Huang%2520and%2520Haokun%2520Zhu%2520and%2520Jinsong%2520Fan%2520and%2520Guixu%2520Zhang%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520has%2520become%2520widely%2520recognized%2520for%2520its%2520ability%2520to%250Atransfer%2520knowledge%2520from%2520a%2520large%2520teacher%2520network%2520to%2520a%2520compact%2520and%2520more%250Astreamlined%2520student%2520network.%2520Traditional%2520knowledge%2520distillation%2520methods%250Aprimarily%2520follow%2520a%2520teacher-oriented%2520paradigm%2520that%2520imposes%2520the%2520task%2520of%2520learning%250Athe%2520teacher%2527s%2520complex%2520knowledge%2520onto%2520the%2520student%2520network.%2520However%252C%2520significant%250Adisparities%2520in%2520model%2520capacity%2520and%2520architectural%2520design%2520hinder%2520the%2520student%2527s%250Acomprehension%2520of%2520the%2520complex%2520knowledge%2520imparted%2520by%2520the%2520teacher%252C%2520resulting%2520in%250Asub-optimal%2520performance.%2520This%2520paper%2520introduces%2520a%2520novel%2520perspective%2520emphasizing%250Astudent-oriented%2520and%2520refining%2520the%2520teacher%2527s%2520knowledge%2520to%2520better%2520align%2520with%2520the%250Astudent%2527s%2520needs%252C%2520thereby%2520improving%2520knowledge%2520transfer%2520effectiveness.%250ASpecifically%252C%2520we%2520present%2520the%2520Student-Oriented%2520Knowledge%2520Distillation%2520%2528SoKD%2529%252C%250Awhich%2520incorporates%2520a%2520learnable%2520feature%2520augmentation%2520strategy%2520during%2520training%2520to%250Arefine%2520the%2520teacher%2527s%2520knowledge%2520of%2520the%2520student%2520dynamically.%2520Furthermore%252C%2520we%250Adeploy%2520the%2520Distinctive%2520Area%2520Detection%2520Module%2520%2528DAM%2529%2520to%2520identify%2520areas%2520of%2520mutual%250Ainterest%2520between%2520the%2520teacher%2520and%2520student%252C%2520concentrating%2520knowledge%2520transfer%250Awithin%2520these%2520critical%2520areas%2520to%2520avoid%2520transferring%2520irrelevant%2520information.%2520This%250Acustomized%2520module%2520ensures%2520a%2520more%2520focused%2520and%2520effective%2520knowledge%2520distillation%250Aprocess.%2520Our%2520approach%252C%2520functioning%2520as%2520a%2520plug-in%252C%2520could%2520be%2520integrated%2520with%250Avarious%2520knowledge%2520distillation%2520methods.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520the%2520efficacy%2520and%2520generalizability%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Student-Oriented%20Teacher%20Knowledge%20Refinement%20for%20Knowledge%20Distillation&entry.906535625=Chaomin%20Shen%20and%20Yaomin%20Huang%20and%20Haokun%20Zhu%20and%20Jinsong%20Fan%20and%20Guixu%20Zhang&entry.1292438233=%20%20Knowledge%20distillation%20has%20become%20widely%20recognized%20for%20its%20ability%20to%0Atransfer%20knowledge%20from%20a%20large%20teacher%20network%20to%20a%20compact%20and%20more%0Astreamlined%20student%20network.%20Traditional%20knowledge%20distillation%20methods%0Aprimarily%20follow%20a%20teacher-oriented%20paradigm%20that%20imposes%20the%20task%20of%20learning%0Athe%20teacher%27s%20complex%20knowledge%20onto%20the%20student%20network.%20However%2C%20significant%0Adisparities%20in%20model%20capacity%20and%20architectural%20design%20hinder%20the%20student%27s%0Acomprehension%20of%20the%20complex%20knowledge%20imparted%20by%20the%20teacher%2C%20resulting%20in%0Asub-optimal%20performance.%20This%20paper%20introduces%20a%20novel%20perspective%20emphasizing%0Astudent-oriented%20and%20refining%20the%20teacher%27s%20knowledge%20to%20better%20align%20with%20the%0Astudent%27s%20needs%2C%20thereby%20improving%20knowledge%20transfer%20effectiveness.%0ASpecifically%2C%20we%20present%20the%20Student-Oriented%20Knowledge%20Distillation%20%28SoKD%29%2C%0Awhich%20incorporates%20a%20learnable%20feature%20augmentation%20strategy%20during%20training%20to%0Arefine%20the%20teacher%27s%20knowledge%20of%20the%20student%20dynamically.%20Furthermore%2C%20we%0Adeploy%20the%20Distinctive%20Area%20Detection%20Module%20%28DAM%29%20to%20identify%20areas%20of%20mutual%0Ainterest%20between%20the%20teacher%20and%20student%2C%20concentrating%20knowledge%20transfer%0Awithin%20these%20critical%20areas%20to%20avoid%20transferring%20irrelevant%20information.%20This%0Acustomized%20module%20ensures%20a%20more%20focused%20and%20effective%20knowledge%20distillation%0Aprocess.%20Our%20approach%2C%20functioning%20as%20a%20plug-in%2C%20could%20be%20integrated%20with%0Avarious%20knowledge%20distillation%20methods.%20Extensive%20experimental%20results%0Ademonstrate%20the%20efficacy%20and%20generalizability%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18785v1&entry.124074799=Read"},
{"title": "Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation", "author": "Raphael Hagmanns and Peter Mortimer and Miguel Granero and Thorsten Luettel and Janko Petereit", "abstract": "  The successful deployment of deep learning-based techniques for autonomous\nsystems is highly dependent on the data availability for the respective system\nin its deployment environment. Especially for unstructured outdoor\nenvironments, very few datasets exist for even fewer robotic platforms and\nscenarios. In an earlier work, we presented the German Outdoor and Offroad\nDataset (GOOSE) framework along with 10000 multimodal frames from an offroad\nvehicle to enhance the perception capabilities in unstructured environments. In\nthis work, we address the generalizability of the GOOSE framework. To\naccomplish this, we open-source the GOOSE-Ex dataset, which contains additional\n5000 labeled multimodal frames from various completely different environments,\nrecorded on a robotic excavator and a quadruped platform. We perform a\ncomprehensive analysis of the semantic segmentation performance on different\nplatforms and sensor modalities in unseen environments. In addition, we\ndemonstrate how the combined datasets can be utilized for different downstream\napplications or competitions such as offroad navigation, object manipulation or\nscene completion. The dataset, its platform documentation and pre-trained\nstate-of-the-art models for offroad perception will be made available on\nhttps://goose-dataset.de/.\n  \\\n", "link": "http://arxiv.org/abs/2409.18788v1", "date": "2024-09-27", "relevancy": 2.182, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Excavating%20in%20the%20Wild%3A%20The%20GOOSE-Ex%20Dataset%20for%20Semantic%20Segmentation&body=Title%3A%20Excavating%20in%20the%20Wild%3A%20The%20GOOSE-Ex%20Dataset%20for%20Semantic%20Segmentation%0AAuthor%3A%20Raphael%20Hagmanns%20and%20Peter%20Mortimer%20and%20Miguel%20Granero%20and%20Thorsten%20Luettel%20and%20Janko%20Petereit%0AAbstract%3A%20%20%20The%20successful%20deployment%20of%20deep%20learning-based%20techniques%20for%20autonomous%0Asystems%20is%20highly%20dependent%20on%20the%20data%20availability%20for%20the%20respective%20system%0Ain%20its%20deployment%20environment.%20Especially%20for%20unstructured%20outdoor%0Aenvironments%2C%20very%20few%20datasets%20exist%20for%20even%20fewer%20robotic%20platforms%20and%0Ascenarios.%20In%20an%20earlier%20work%2C%20we%20presented%20the%20German%20Outdoor%20and%20Offroad%0ADataset%20%28GOOSE%29%20framework%20along%20with%2010000%20multimodal%20frames%20from%20an%20offroad%0Avehicle%20to%20enhance%20the%20perception%20capabilities%20in%20unstructured%20environments.%20In%0Athis%20work%2C%20we%20address%20the%20generalizability%20of%20the%20GOOSE%20framework.%20To%0Aaccomplish%20this%2C%20we%20open-source%20the%20GOOSE-Ex%20dataset%2C%20which%20contains%20additional%0A5000%20labeled%20multimodal%20frames%20from%20various%20completely%20different%20environments%2C%0Arecorded%20on%20a%20robotic%20excavator%20and%20a%20quadruped%20platform.%20We%20perform%20a%0Acomprehensive%20analysis%20of%20the%20semantic%20segmentation%20performance%20on%20different%0Aplatforms%20and%20sensor%20modalities%20in%20unseen%20environments.%20In%20addition%2C%20we%0Ademonstrate%20how%20the%20combined%20datasets%20can%20be%20utilized%20for%20different%20downstream%0Aapplications%20or%20competitions%20such%20as%20offroad%20navigation%2C%20object%20manipulation%20or%0Ascene%20completion.%20The%20dataset%2C%20its%20platform%20documentation%20and%20pre-trained%0Astate-of-the-art%20models%20for%20offroad%20perception%20will%20be%20made%20available%20on%0Ahttps%3A//goose-dataset.de/.%0A%20%20%5C%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExcavating%2520in%2520the%2520Wild%253A%2520The%2520GOOSE-Ex%2520Dataset%2520for%2520Semantic%2520Segmentation%26entry.906535625%3DRaphael%2520Hagmanns%2520and%2520Peter%2520Mortimer%2520and%2520Miguel%2520Granero%2520and%2520Thorsten%2520Luettel%2520and%2520Janko%2520Petereit%26entry.1292438233%3D%2520%2520The%2520successful%2520deployment%2520of%2520deep%2520learning-based%2520techniques%2520for%2520autonomous%250Asystems%2520is%2520highly%2520dependent%2520on%2520the%2520data%2520availability%2520for%2520the%2520respective%2520system%250Ain%2520its%2520deployment%2520environment.%2520Especially%2520for%2520unstructured%2520outdoor%250Aenvironments%252C%2520very%2520few%2520datasets%2520exist%2520for%2520even%2520fewer%2520robotic%2520platforms%2520and%250Ascenarios.%2520In%2520an%2520earlier%2520work%252C%2520we%2520presented%2520the%2520German%2520Outdoor%2520and%2520Offroad%250ADataset%2520%2528GOOSE%2529%2520framework%2520along%2520with%252010000%2520multimodal%2520frames%2520from%2520an%2520offroad%250Avehicle%2520to%2520enhance%2520the%2520perception%2520capabilities%2520in%2520unstructured%2520environments.%2520In%250Athis%2520work%252C%2520we%2520address%2520the%2520generalizability%2520of%2520the%2520GOOSE%2520framework.%2520To%250Aaccomplish%2520this%252C%2520we%2520open-source%2520the%2520GOOSE-Ex%2520dataset%252C%2520which%2520contains%2520additional%250A5000%2520labeled%2520multimodal%2520frames%2520from%2520various%2520completely%2520different%2520environments%252C%250Arecorded%2520on%2520a%2520robotic%2520excavator%2520and%2520a%2520quadruped%2520platform.%2520We%2520perform%2520a%250Acomprehensive%2520analysis%2520of%2520the%2520semantic%2520segmentation%2520performance%2520on%2520different%250Aplatforms%2520and%2520sensor%2520modalities%2520in%2520unseen%2520environments.%2520In%2520addition%252C%2520we%250Ademonstrate%2520how%2520the%2520combined%2520datasets%2520can%2520be%2520utilized%2520for%2520different%2520downstream%250Aapplications%2520or%2520competitions%2520such%2520as%2520offroad%2520navigation%252C%2520object%2520manipulation%2520or%250Ascene%2520completion.%2520The%2520dataset%252C%2520its%2520platform%2520documentation%2520and%2520pre-trained%250Astate-of-the-art%2520models%2520for%2520offroad%2520perception%2520will%2520be%2520made%2520available%2520on%250Ahttps%253A//goose-dataset.de/.%250A%2520%2520%255C%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Excavating%20in%20the%20Wild%3A%20The%20GOOSE-Ex%20Dataset%20for%20Semantic%20Segmentation&entry.906535625=Raphael%20Hagmanns%20and%20Peter%20Mortimer%20and%20Miguel%20Granero%20and%20Thorsten%20Luettel%20and%20Janko%20Petereit&entry.1292438233=%20%20The%20successful%20deployment%20of%20deep%20learning-based%20techniques%20for%20autonomous%0Asystems%20is%20highly%20dependent%20on%20the%20data%20availability%20for%20the%20respective%20system%0Ain%20its%20deployment%20environment.%20Especially%20for%20unstructured%20outdoor%0Aenvironments%2C%20very%20few%20datasets%20exist%20for%20even%20fewer%20robotic%20platforms%20and%0Ascenarios.%20In%20an%20earlier%20work%2C%20we%20presented%20the%20German%20Outdoor%20and%20Offroad%0ADataset%20%28GOOSE%29%20framework%20along%20with%2010000%20multimodal%20frames%20from%20an%20offroad%0Avehicle%20to%20enhance%20the%20perception%20capabilities%20in%20unstructured%20environments.%20In%0Athis%20work%2C%20we%20address%20the%20generalizability%20of%20the%20GOOSE%20framework.%20To%0Aaccomplish%20this%2C%20we%20open-source%20the%20GOOSE-Ex%20dataset%2C%20which%20contains%20additional%0A5000%20labeled%20multimodal%20frames%20from%20various%20completely%20different%20environments%2C%0Arecorded%20on%20a%20robotic%20excavator%20and%20a%20quadruped%20platform.%20We%20perform%20a%0Acomprehensive%20analysis%20of%20the%20semantic%20segmentation%20performance%20on%20different%0Aplatforms%20and%20sensor%20modalities%20in%20unseen%20environments.%20In%20addition%2C%20we%0Ademonstrate%20how%20the%20combined%20datasets%20can%20be%20utilized%20for%20different%20downstream%0Aapplications%20or%20competitions%20such%20as%20offroad%20navigation%2C%20object%20manipulation%20or%0Ascene%20completion.%20The%20dataset%2C%20its%20platform%20documentation%20and%20pre-trained%0Astate-of-the-art%20models%20for%20offroad%20perception%20will%20be%20made%20available%20on%0Ahttps%3A//goose-dataset.de/.%0A%20%20%5C%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18788v1&entry.124074799=Read"},
{"title": "Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using\n  Conditional Generative Adversarial Networks", "author": "Richard Osuala and Smriti Joshi and Apostolia Tsirikoglou and Lidia Garrucho and Walter H. L. Pinaya and Daniel M. Lang and Julia A. Schnabel and Oliver Diaz and Karim Lekadir", "abstract": "  This paper presents a method for virtual contrast enhancement in breast MRI,\noffering a promising non-invasive alternative to traditional contrast\nagent-based DCE-MRI acquisition. Using a conditional generative adversarial\nnetwork, we predict DCE-MRI images, including jointly-generated sequences of\nmultiple corresponding DCE-MRI timepoints, from non-contrast-enhanced MRIs,\nenabling tumor localization and characterization without the associated health\nrisks. Furthermore, we qualitatively and quantitatively evaluate the synthetic\nDCE-MRI images, proposing a multi-metric Scaled Aggregate Measure (SAMe),\nassessing their utility in a tumor segmentation downstream task, and conclude\nwith an analysis of the temporal patterns in multi-sequence DCE-MRI generation.\nOur approach demonstrates promising results in generating realistic and useful\nDCE-MRI sequences, highlighting the potential of virtual contrast enhancement\nfor improving breast cancer diagnosis and treatment, particularly for patients\nwhere contrast agent administration is contraindicated.\n", "link": "http://arxiv.org/abs/2409.18872v1", "date": "2024-09-27", "relevancy": 2.164, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5452}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5452}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulating%20Dynamic%20Tumor%20Contrast%20Enhancement%20in%20Breast%20MRI%20using%0A%20%20Conditional%20Generative%20Adversarial%20Networks&body=Title%3A%20Simulating%20Dynamic%20Tumor%20Contrast%20Enhancement%20in%20Breast%20MRI%20using%0A%20%20Conditional%20Generative%20Adversarial%20Networks%0AAuthor%3A%20Richard%20Osuala%20and%20Smriti%20Joshi%20and%20Apostolia%20Tsirikoglou%20and%20Lidia%20Garrucho%20and%20Walter%20H.%20L.%20Pinaya%20and%20Daniel%20M.%20Lang%20and%20Julia%20A.%20Schnabel%20and%20Oliver%20Diaz%20and%20Karim%20Lekadir%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20method%20for%20virtual%20contrast%20enhancement%20in%20breast%20MRI%2C%0Aoffering%20a%20promising%20non-invasive%20alternative%20to%20traditional%20contrast%0Aagent-based%20DCE-MRI%20acquisition.%20Using%20a%20conditional%20generative%20adversarial%0Anetwork%2C%20we%20predict%20DCE-MRI%20images%2C%20including%20jointly-generated%20sequences%20of%0Amultiple%20corresponding%20DCE-MRI%20timepoints%2C%20from%20non-contrast-enhanced%20MRIs%2C%0Aenabling%20tumor%20localization%20and%20characterization%20without%20the%20associated%20health%0Arisks.%20Furthermore%2C%20we%20qualitatively%20and%20quantitatively%20evaluate%20the%20synthetic%0ADCE-MRI%20images%2C%20proposing%20a%20multi-metric%20Scaled%20Aggregate%20Measure%20%28SAMe%29%2C%0Aassessing%20their%20utility%20in%20a%20tumor%20segmentation%20downstream%20task%2C%20and%20conclude%0Awith%20an%20analysis%20of%20the%20temporal%20patterns%20in%20multi-sequence%20DCE-MRI%20generation.%0AOur%20approach%20demonstrates%20promising%20results%20in%20generating%20realistic%20and%20useful%0ADCE-MRI%20sequences%2C%20highlighting%20the%20potential%20of%20virtual%20contrast%20enhancement%0Afor%20improving%20breast%20cancer%20diagnosis%20and%20treatment%2C%20particularly%20for%20patients%0Awhere%20contrast%20agent%20administration%20is%20contraindicated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulating%2520Dynamic%2520Tumor%2520Contrast%2520Enhancement%2520in%2520Breast%2520MRI%2520using%250A%2520%2520Conditional%2520Generative%2520Adversarial%2520Networks%26entry.906535625%3DRichard%2520Osuala%2520and%2520Smriti%2520Joshi%2520and%2520Apostolia%2520Tsirikoglou%2520and%2520Lidia%2520Garrucho%2520and%2520Walter%2520H.%2520L.%2520Pinaya%2520and%2520Daniel%2520M.%2520Lang%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Oliver%2520Diaz%2520and%2520Karim%2520Lekadir%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520method%2520for%2520virtual%2520contrast%2520enhancement%2520in%2520breast%2520MRI%252C%250Aoffering%2520a%2520promising%2520non-invasive%2520alternative%2520to%2520traditional%2520contrast%250Aagent-based%2520DCE-MRI%2520acquisition.%2520Using%2520a%2520conditional%2520generative%2520adversarial%250Anetwork%252C%2520we%2520predict%2520DCE-MRI%2520images%252C%2520including%2520jointly-generated%2520sequences%2520of%250Amultiple%2520corresponding%2520DCE-MRI%2520timepoints%252C%2520from%2520non-contrast-enhanced%2520MRIs%252C%250Aenabling%2520tumor%2520localization%2520and%2520characterization%2520without%2520the%2520associated%2520health%250Arisks.%2520Furthermore%252C%2520we%2520qualitatively%2520and%2520quantitatively%2520evaluate%2520the%2520synthetic%250ADCE-MRI%2520images%252C%2520proposing%2520a%2520multi-metric%2520Scaled%2520Aggregate%2520Measure%2520%2528SAMe%2529%252C%250Aassessing%2520their%2520utility%2520in%2520a%2520tumor%2520segmentation%2520downstream%2520task%252C%2520and%2520conclude%250Awith%2520an%2520analysis%2520of%2520the%2520temporal%2520patterns%2520in%2520multi-sequence%2520DCE-MRI%2520generation.%250AOur%2520approach%2520demonstrates%2520promising%2520results%2520in%2520generating%2520realistic%2520and%2520useful%250ADCE-MRI%2520sequences%252C%2520highlighting%2520the%2520potential%2520of%2520virtual%2520contrast%2520enhancement%250Afor%2520improving%2520breast%2520cancer%2520diagnosis%2520and%2520treatment%252C%2520particularly%2520for%2520patients%250Awhere%2520contrast%2520agent%2520administration%2520is%2520contraindicated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulating%20Dynamic%20Tumor%20Contrast%20Enhancement%20in%20Breast%20MRI%20using%0A%20%20Conditional%20Generative%20Adversarial%20Networks&entry.906535625=Richard%20Osuala%20and%20Smriti%20Joshi%20and%20Apostolia%20Tsirikoglou%20and%20Lidia%20Garrucho%20and%20Walter%20H.%20L.%20Pinaya%20and%20Daniel%20M.%20Lang%20and%20Julia%20A.%20Schnabel%20and%20Oliver%20Diaz%20and%20Karim%20Lekadir&entry.1292438233=%20%20This%20paper%20presents%20a%20method%20for%20virtual%20contrast%20enhancement%20in%20breast%20MRI%2C%0Aoffering%20a%20promising%20non-invasive%20alternative%20to%20traditional%20contrast%0Aagent-based%20DCE-MRI%20acquisition.%20Using%20a%20conditional%20generative%20adversarial%0Anetwork%2C%20we%20predict%20DCE-MRI%20images%2C%20including%20jointly-generated%20sequences%20of%0Amultiple%20corresponding%20DCE-MRI%20timepoints%2C%20from%20non-contrast-enhanced%20MRIs%2C%0Aenabling%20tumor%20localization%20and%20characterization%20without%20the%20associated%20health%0Arisks.%20Furthermore%2C%20we%20qualitatively%20and%20quantitatively%20evaluate%20the%20synthetic%0ADCE-MRI%20images%2C%20proposing%20a%20multi-metric%20Scaled%20Aggregate%20Measure%20%28SAMe%29%2C%0Aassessing%20their%20utility%20in%20a%20tumor%20segmentation%20downstream%20task%2C%20and%20conclude%0Awith%20an%20analysis%20of%20the%20temporal%20patterns%20in%20multi-sequence%20DCE-MRI%20generation.%0AOur%20approach%20demonstrates%20promising%20results%20in%20generating%20realistic%20and%20useful%0ADCE-MRI%20sequences%2C%20highlighting%20the%20potential%20of%20virtual%20contrast%20enhancement%0Afor%20improving%20breast%20cancer%20diagnosis%20and%20treatment%2C%20particularly%20for%20patients%0Awhere%20contrast%20agent%20administration%20is%20contraindicated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18872v1&entry.124074799=Read"},
{"title": "Lego: Learning to Disentangle and Invert Personalized Concepts Beyond\n  Object Appearance in Text-to-Image Diffusion Models", "author": "Saman Motamed and Danda Pani Paudel and Luc Van Gool", "abstract": "  Text-to-Image (T2I) models excel at synthesizing concepts such as nouns,\nappearances, and styles. To enable customized content creation based on a few\nexample images of a concept, methods such as Textual Inversion and DreamBooth\ninvert the desired concept and enable synthesizing it in new scenes. However,\ninverting personalized concepts that go beyond object appearance and style\n(adjectives and verbs) through natural language remains a challenge. Two key\ncharacteristics of these concepts contribute to the limitations of current\ninversion methods. 1) Adjectives and verbs are entangled with nouns (subject)\nand can hinder appearance-based inversion methods, where the subject appearance\nleaks into the concept embedding, and 2) describing such concepts often extends\nbeyond single word embeddings.\n  In this study, we introduce Lego, a textual inversion method designed to\ninvert subject-entangled concepts from a few example images. Lego disentangles\nconcepts from their associated subjects using a simple yet effective Subject\nSeparation step and employs a Context Loss that guides the inversion of\nsingle/multi-embedding concepts. In a thorough user study, Lego-generated\nconcepts were preferred over 70% of the time when compared to the baseline in\nterms of authentically generating concepts according to a reference.\nAdditionally, visual question answering using an LLM suggested Lego-generated\nconcepts are better aligned with the text description of the concept.\n", "link": "http://arxiv.org/abs/2311.13833v2", "date": "2024-09-27", "relevancy": 2.1638, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5558}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5393}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lego%3A%20Learning%20to%20Disentangle%20and%20Invert%20Personalized%20Concepts%20Beyond%0A%20%20Object%20Appearance%20in%20Text-to-Image%20Diffusion%20Models&body=Title%3A%20Lego%3A%20Learning%20to%20Disentangle%20and%20Invert%20Personalized%20Concepts%20Beyond%0A%20%20Object%20Appearance%20in%20Text-to-Image%20Diffusion%20Models%0AAuthor%3A%20Saman%20Motamed%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Text-to-Image%20%28T2I%29%20models%20excel%20at%20synthesizing%20concepts%20such%20as%20nouns%2C%0Aappearances%2C%20and%20styles.%20To%20enable%20customized%20content%20creation%20based%20on%20a%20few%0Aexample%20images%20of%20a%20concept%2C%20methods%20such%20as%20Textual%20Inversion%20and%20DreamBooth%0Ainvert%20the%20desired%20concept%20and%20enable%20synthesizing%20it%20in%20new%20scenes.%20However%2C%0Ainverting%20personalized%20concepts%20that%20go%20beyond%20object%20appearance%20and%20style%0A%28adjectives%20and%20verbs%29%20through%20natural%20language%20remains%20a%20challenge.%20Two%20key%0Acharacteristics%20of%20these%20concepts%20contribute%20to%20the%20limitations%20of%20current%0Ainversion%20methods.%201%29%20Adjectives%20and%20verbs%20are%20entangled%20with%20nouns%20%28subject%29%0Aand%20can%20hinder%20appearance-based%20inversion%20methods%2C%20where%20the%20subject%20appearance%0Aleaks%20into%20the%20concept%20embedding%2C%20and%202%29%20describing%20such%20concepts%20often%20extends%0Abeyond%20single%20word%20embeddings.%0A%20%20In%20this%20study%2C%20we%20introduce%20Lego%2C%20a%20textual%20inversion%20method%20designed%20to%0Ainvert%20subject-entangled%20concepts%20from%20a%20few%20example%20images.%20Lego%20disentangles%0Aconcepts%20from%20their%20associated%20subjects%20using%20a%20simple%20yet%20effective%20Subject%0ASeparation%20step%20and%20employs%20a%20Context%20Loss%20that%20guides%20the%20inversion%20of%0Asingle/multi-embedding%20concepts.%20In%20a%20thorough%20user%20study%2C%20Lego-generated%0Aconcepts%20were%20preferred%20over%2070%25%20of%20the%20time%20when%20compared%20to%20the%20baseline%20in%0Aterms%20of%20authentically%20generating%20concepts%20according%20to%20a%20reference.%0AAdditionally%2C%20visual%20question%20answering%20using%20an%20LLM%20suggested%20Lego-generated%0Aconcepts%20are%20better%20aligned%20with%20the%20text%20description%20of%20the%20concept.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLego%253A%2520Learning%2520to%2520Disentangle%2520and%2520Invert%2520Personalized%2520Concepts%2520Beyond%250A%2520%2520Object%2520Appearance%2520in%2520Text-to-Image%2520Diffusion%2520Models%26entry.906535625%3DSaman%2520Motamed%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Text-to-Image%2520%2528T2I%2529%2520models%2520excel%2520at%2520synthesizing%2520concepts%2520such%2520as%2520nouns%252C%250Aappearances%252C%2520and%2520styles.%2520To%2520enable%2520customized%2520content%2520creation%2520based%2520on%2520a%2520few%250Aexample%2520images%2520of%2520a%2520concept%252C%2520methods%2520such%2520as%2520Textual%2520Inversion%2520and%2520DreamBooth%250Ainvert%2520the%2520desired%2520concept%2520and%2520enable%2520synthesizing%2520it%2520in%2520new%2520scenes.%2520However%252C%250Ainverting%2520personalized%2520concepts%2520that%2520go%2520beyond%2520object%2520appearance%2520and%2520style%250A%2528adjectives%2520and%2520verbs%2529%2520through%2520natural%2520language%2520remains%2520a%2520challenge.%2520Two%2520key%250Acharacteristics%2520of%2520these%2520concepts%2520contribute%2520to%2520the%2520limitations%2520of%2520current%250Ainversion%2520methods.%25201%2529%2520Adjectives%2520and%2520verbs%2520are%2520entangled%2520with%2520nouns%2520%2528subject%2529%250Aand%2520can%2520hinder%2520appearance-based%2520inversion%2520methods%252C%2520where%2520the%2520subject%2520appearance%250Aleaks%2520into%2520the%2520concept%2520embedding%252C%2520and%25202%2529%2520describing%2520such%2520concepts%2520often%2520extends%250Abeyond%2520single%2520word%2520embeddings.%250A%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520Lego%252C%2520a%2520textual%2520inversion%2520method%2520designed%2520to%250Ainvert%2520subject-entangled%2520concepts%2520from%2520a%2520few%2520example%2520images.%2520Lego%2520disentangles%250Aconcepts%2520from%2520their%2520associated%2520subjects%2520using%2520a%2520simple%2520yet%2520effective%2520Subject%250ASeparation%2520step%2520and%2520employs%2520a%2520Context%2520Loss%2520that%2520guides%2520the%2520inversion%2520of%250Asingle/multi-embedding%2520concepts.%2520In%2520a%2520thorough%2520user%2520study%252C%2520Lego-generated%250Aconcepts%2520were%2520preferred%2520over%252070%2525%2520of%2520the%2520time%2520when%2520compared%2520to%2520the%2520baseline%2520in%250Aterms%2520of%2520authentically%2520generating%2520concepts%2520according%2520to%2520a%2520reference.%250AAdditionally%252C%2520visual%2520question%2520answering%2520using%2520an%2520LLM%2520suggested%2520Lego-generated%250Aconcepts%2520are%2520better%2520aligned%2520with%2520the%2520text%2520description%2520of%2520the%2520concept.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lego%3A%20Learning%20to%20Disentangle%20and%20Invert%20Personalized%20Concepts%20Beyond%0A%20%20Object%20Appearance%20in%20Text-to-Image%20Diffusion%20Models&entry.906535625=Saman%20Motamed%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Text-to-Image%20%28T2I%29%20models%20excel%20at%20synthesizing%20concepts%20such%20as%20nouns%2C%0Aappearances%2C%20and%20styles.%20To%20enable%20customized%20content%20creation%20based%20on%20a%20few%0Aexample%20images%20of%20a%20concept%2C%20methods%20such%20as%20Textual%20Inversion%20and%20DreamBooth%0Ainvert%20the%20desired%20concept%20and%20enable%20synthesizing%20it%20in%20new%20scenes.%20However%2C%0Ainverting%20personalized%20concepts%20that%20go%20beyond%20object%20appearance%20and%20style%0A%28adjectives%20and%20verbs%29%20through%20natural%20language%20remains%20a%20challenge.%20Two%20key%0Acharacteristics%20of%20these%20concepts%20contribute%20to%20the%20limitations%20of%20current%0Ainversion%20methods.%201%29%20Adjectives%20and%20verbs%20are%20entangled%20with%20nouns%20%28subject%29%0Aand%20can%20hinder%20appearance-based%20inversion%20methods%2C%20where%20the%20subject%20appearance%0Aleaks%20into%20the%20concept%20embedding%2C%20and%202%29%20describing%20such%20concepts%20often%20extends%0Abeyond%20single%20word%20embeddings.%0A%20%20In%20this%20study%2C%20we%20introduce%20Lego%2C%20a%20textual%20inversion%20method%20designed%20to%0Ainvert%20subject-entangled%20concepts%20from%20a%20few%20example%20images.%20Lego%20disentangles%0Aconcepts%20from%20their%20associated%20subjects%20using%20a%20simple%20yet%20effective%20Subject%0ASeparation%20step%20and%20employs%20a%20Context%20Loss%20that%20guides%20the%20inversion%20of%0Asingle/multi-embedding%20concepts.%20In%20a%20thorough%20user%20study%2C%20Lego-generated%0Aconcepts%20were%20preferred%20over%2070%25%20of%20the%20time%20when%20compared%20to%20the%20baseline%20in%0Aterms%20of%20authentically%20generating%20concepts%20according%20to%20a%20reference.%0AAdditionally%2C%20visual%20question%20answering%20using%20an%20LLM%20suggested%20Lego-generated%0Aconcepts%20are%20better%20aligned%20with%20the%20text%20description%20of%20the%20concept.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13833v2&entry.124074799=Read"},
{"title": "Automatic Gain Tuning for Humanoid Robots Walking Architectures Using\n  Gradient-Free Optimization Techniques", "author": "Carlotta Sartore and Marco Rando and Giulio Romualdi and Cesare Molinari and Lorenzo Rosasco and Daniele Pucci", "abstract": "  Developing sophisticated control architectures has endowed robots,\nparticularly humanoid robots, with numerous capabilities. However, tuning these\narchitectures remains a challenging and time-consuming task that requires\nexpert intervention. In this work, we propose a methodology to automatically\ntune the gains of all layers of a hierarchical control architecture for walking\nhumanoids. We tested our methodology by employing different gradient-free\noptimization methods: Genetic Algorithm (GA), Covariance Matrix Adaptation\nEvolution Strategy (CMA-ES), Evolution Strategy (ES), and Differential\nEvolution (DE). We validated the parameter found both in simulation and on the\nreal ergoCub humanoid robot. Our results show that GA achieves the fastest\nconvergence (10 x 10^3 function evaluations vs 25 x 10^3 needed by the other\nalgorithms) and 100% success rate in completing the task both in simulation and\nwhen transferred on the real robotic platform. These findings highlight the\npotential of our proposed method to automate the tuning process, reducing the\nneed for manual intervention.\n", "link": "http://arxiv.org/abs/2409.18649v1", "date": "2024-09-27", "relevancy": 2.1632, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5455}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5382}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Gain%20Tuning%20for%20Humanoid%20Robots%20Walking%20Architectures%20Using%0A%20%20Gradient-Free%20Optimization%20Techniques&body=Title%3A%20Automatic%20Gain%20Tuning%20for%20Humanoid%20Robots%20Walking%20Architectures%20Using%0A%20%20Gradient-Free%20Optimization%20Techniques%0AAuthor%3A%20Carlotta%20Sartore%20and%20Marco%20Rando%20and%20Giulio%20Romualdi%20and%20Cesare%20Molinari%20and%20Lorenzo%20Rosasco%20and%20Daniele%20Pucci%0AAbstract%3A%20%20%20Developing%20sophisticated%20control%20architectures%20has%20endowed%20robots%2C%0Aparticularly%20humanoid%20robots%2C%20with%20numerous%20capabilities.%20However%2C%20tuning%20these%0Aarchitectures%20remains%20a%20challenging%20and%20time-consuming%20task%20that%20requires%0Aexpert%20intervention.%20In%20this%20work%2C%20we%20propose%20a%20methodology%20to%20automatically%0Atune%20the%20gains%20of%20all%20layers%20of%20a%20hierarchical%20control%20architecture%20for%20walking%0Ahumanoids.%20We%20tested%20our%20methodology%20by%20employing%20different%20gradient-free%0Aoptimization%20methods%3A%20Genetic%20Algorithm%20%28GA%29%2C%20Covariance%20Matrix%20Adaptation%0AEvolution%20Strategy%20%28CMA-ES%29%2C%20Evolution%20Strategy%20%28ES%29%2C%20and%20Differential%0AEvolution%20%28DE%29.%20We%20validated%20the%20parameter%20found%20both%20in%20simulation%20and%20on%20the%0Areal%20ergoCub%20humanoid%20robot.%20Our%20results%20show%20that%20GA%20achieves%20the%20fastest%0Aconvergence%20%2810%20x%2010%5E3%20function%20evaluations%20vs%2025%20x%2010%5E3%20needed%20by%20the%20other%0Aalgorithms%29%20and%20100%25%20success%20rate%20in%20completing%20the%20task%20both%20in%20simulation%20and%0Awhen%20transferred%20on%20the%20real%20robotic%20platform.%20These%20findings%20highlight%20the%0Apotential%20of%20our%20proposed%20method%20to%20automate%20the%20tuning%20process%2C%20reducing%20the%0Aneed%20for%20manual%20intervention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Gain%2520Tuning%2520for%2520Humanoid%2520Robots%2520Walking%2520Architectures%2520Using%250A%2520%2520Gradient-Free%2520Optimization%2520Techniques%26entry.906535625%3DCarlotta%2520Sartore%2520and%2520Marco%2520Rando%2520and%2520Giulio%2520Romualdi%2520and%2520Cesare%2520Molinari%2520and%2520Lorenzo%2520Rosasco%2520and%2520Daniele%2520Pucci%26entry.1292438233%3D%2520%2520Developing%2520sophisticated%2520control%2520architectures%2520has%2520endowed%2520robots%252C%250Aparticularly%2520humanoid%2520robots%252C%2520with%2520numerous%2520capabilities.%2520However%252C%2520tuning%2520these%250Aarchitectures%2520remains%2520a%2520challenging%2520and%2520time-consuming%2520task%2520that%2520requires%250Aexpert%2520intervention.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520methodology%2520to%2520automatically%250Atune%2520the%2520gains%2520of%2520all%2520layers%2520of%2520a%2520hierarchical%2520control%2520architecture%2520for%2520walking%250Ahumanoids.%2520We%2520tested%2520our%2520methodology%2520by%2520employing%2520different%2520gradient-free%250Aoptimization%2520methods%253A%2520Genetic%2520Algorithm%2520%2528GA%2529%252C%2520Covariance%2520Matrix%2520Adaptation%250AEvolution%2520Strategy%2520%2528CMA-ES%2529%252C%2520Evolution%2520Strategy%2520%2528ES%2529%252C%2520and%2520Differential%250AEvolution%2520%2528DE%2529.%2520We%2520validated%2520the%2520parameter%2520found%2520both%2520in%2520simulation%2520and%2520on%2520the%250Areal%2520ergoCub%2520humanoid%2520robot.%2520Our%2520results%2520show%2520that%2520GA%2520achieves%2520the%2520fastest%250Aconvergence%2520%252810%2520x%252010%255E3%2520function%2520evaluations%2520vs%252025%2520x%252010%255E3%2520needed%2520by%2520the%2520other%250Aalgorithms%2529%2520and%2520100%2525%2520success%2520rate%2520in%2520completing%2520the%2520task%2520both%2520in%2520simulation%2520and%250Awhen%2520transferred%2520on%2520the%2520real%2520robotic%2520platform.%2520These%2520findings%2520highlight%2520the%250Apotential%2520of%2520our%2520proposed%2520method%2520to%2520automate%2520the%2520tuning%2520process%252C%2520reducing%2520the%250Aneed%2520for%2520manual%2520intervention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Gain%20Tuning%20for%20Humanoid%20Robots%20Walking%20Architectures%20Using%0A%20%20Gradient-Free%20Optimization%20Techniques&entry.906535625=Carlotta%20Sartore%20and%20Marco%20Rando%20and%20Giulio%20Romualdi%20and%20Cesare%20Molinari%20and%20Lorenzo%20Rosasco%20and%20Daniele%20Pucci&entry.1292438233=%20%20Developing%20sophisticated%20control%20architectures%20has%20endowed%20robots%2C%0Aparticularly%20humanoid%20robots%2C%20with%20numerous%20capabilities.%20However%2C%20tuning%20these%0Aarchitectures%20remains%20a%20challenging%20and%20time-consuming%20task%20that%20requires%0Aexpert%20intervention.%20In%20this%20work%2C%20we%20propose%20a%20methodology%20to%20automatically%0Atune%20the%20gains%20of%20all%20layers%20of%20a%20hierarchical%20control%20architecture%20for%20walking%0Ahumanoids.%20We%20tested%20our%20methodology%20by%20employing%20different%20gradient-free%0Aoptimization%20methods%3A%20Genetic%20Algorithm%20%28GA%29%2C%20Covariance%20Matrix%20Adaptation%0AEvolution%20Strategy%20%28CMA-ES%29%2C%20Evolution%20Strategy%20%28ES%29%2C%20and%20Differential%0AEvolution%20%28DE%29.%20We%20validated%20the%20parameter%20found%20both%20in%20simulation%20and%20on%20the%0Areal%20ergoCub%20humanoid%20robot.%20Our%20results%20show%20that%20GA%20achieves%20the%20fastest%0Aconvergence%20%2810%20x%2010%5E3%20function%20evaluations%20vs%2025%20x%2010%5E3%20needed%20by%20the%20other%0Aalgorithms%29%20and%20100%25%20success%20rate%20in%20completing%20the%20task%20both%20in%20simulation%20and%0Awhen%20transferred%20on%20the%20real%20robotic%20platform.%20These%20findings%20highlight%20the%0Apotential%20of%20our%20proposed%20method%20to%20automate%20the%20tuning%20process%2C%20reducing%20the%0Aneed%20for%20manual%20intervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18649v1&entry.124074799=Read"},
{"title": "LML: Language Model Learning a Dataset for Data-Augmented Prediction", "author": "Praneeth Vadlapati", "abstract": "  This paper introduces a new approach to using Large Language Models (LLMs)\nfor classification tasks, which are typically handled using Machine Learning\n(ML) models. Unlike ML models that rely heavily on data cleaning and feature\nengineering, this method streamlines the process using LLMs. This paper\nproposes a new concept called \"Language Model Learning (LML)\" powered by a new\nmethod called \"Data-Augmented Prediction (DAP)\". The classification is\nperformed by LLMs using a method similar to humans manually exploring and\nunderstanding the data and deciding classifications using data as a reference.\nTraining data is summarized and evaluated to determine the features that lead\nto the classification of each label the most. In the process of DAP, the system\nuses the data summary to automatically create a query, which is used to\nretrieve relevant rows from the dataset. A classification is generated by the\nLLM using data summary and relevant rows, ensuring satisfactory accuracy even\nwith complex data. Usage of data summary and similar data in DAP ensures\ncontext-aware decision-making. The proposed method uses the words \"Act as an\nExplainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP\n", "link": "http://arxiv.org/abs/2409.18957v1", "date": "2024-09-27", "relevancy": 2.1309, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5483}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5296}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LML%3A%20Language%20Model%20Learning%20a%20Dataset%20for%20Data-Augmented%20Prediction&body=Title%3A%20LML%3A%20Language%20Model%20Learning%20a%20Dataset%20for%20Data-Augmented%20Prediction%0AAuthor%3A%20Praneeth%20Vadlapati%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20new%20approach%20to%20using%20Large%20Language%20Models%20%28LLMs%29%0Afor%20classification%20tasks%2C%20which%20are%20typically%20handled%20using%20Machine%20Learning%0A%28ML%29%20models.%20Unlike%20ML%20models%20that%20rely%20heavily%20on%20data%20cleaning%20and%20feature%0Aengineering%2C%20this%20method%20streamlines%20the%20process%20using%20LLMs.%20This%20paper%0Aproposes%20a%20new%20concept%20called%20%22Language%20Model%20Learning%20%28LML%29%22%20powered%20by%20a%20new%0Amethod%20called%20%22Data-Augmented%20Prediction%20%28DAP%29%22.%20The%20classification%20is%0Aperformed%20by%20LLMs%20using%20a%20method%20similar%20to%20humans%20manually%20exploring%20and%0Aunderstanding%20the%20data%20and%20deciding%20classifications%20using%20data%20as%20a%20reference.%0ATraining%20data%20is%20summarized%20and%20evaluated%20to%20determine%20the%20features%20that%20lead%0Ato%20the%20classification%20of%20each%20label%20the%20most.%20In%20the%20process%20of%20DAP%2C%20the%20system%0Auses%20the%20data%20summary%20to%20automatically%20create%20a%20query%2C%20which%20is%20used%20to%0Aretrieve%20relevant%20rows%20from%20the%20dataset.%20A%20classification%20is%20generated%20by%20the%0ALLM%20using%20data%20summary%20and%20relevant%20rows%2C%20ensuring%20satisfactory%20accuracy%20even%0Awith%20complex%20data.%20Usage%20of%20data%20summary%20and%20similar%20data%20in%20DAP%20ensures%0Acontext-aware%20decision-making.%20The%20proposed%20method%20uses%20the%20words%20%22Act%20as%20an%0AExplainable%20Machine%20Learning%20Model%22%20in%20the%20prompt%20to%20enhance%20the%0Ainterpretability%20of%20the%20predictions%20by%20allowing%20users%20to%20review%20the%20logic%0Abehind%20each%20prediction.%20In%20some%20test%20cases%2C%20the%20system%20scored%20an%20accuracy%20above%0A90%25%2C%20proving%20the%20effectiveness%20of%20the%20system%20and%20its%20potential%20to%20outperform%0Aconventional%20ML%20models%20in%20various%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Pro-GenAI/LML-DAP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLML%253A%2520Language%2520Model%2520Learning%2520a%2520Dataset%2520for%2520Data-Augmented%2520Prediction%26entry.906535625%3DPraneeth%2520Vadlapati%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520new%2520approach%2520to%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Afor%2520classification%2520tasks%252C%2520which%2520are%2520typically%2520handled%2520using%2520Machine%2520Learning%250A%2528ML%2529%2520models.%2520Unlike%2520ML%2520models%2520that%2520rely%2520heavily%2520on%2520data%2520cleaning%2520and%2520feature%250Aengineering%252C%2520this%2520method%2520streamlines%2520the%2520process%2520using%2520LLMs.%2520This%2520paper%250Aproposes%2520a%2520new%2520concept%2520called%2520%2522Language%2520Model%2520Learning%2520%2528LML%2529%2522%2520powered%2520by%2520a%2520new%250Amethod%2520called%2520%2522Data-Augmented%2520Prediction%2520%2528DAP%2529%2522.%2520The%2520classification%2520is%250Aperformed%2520by%2520LLMs%2520using%2520a%2520method%2520similar%2520to%2520humans%2520manually%2520exploring%2520and%250Aunderstanding%2520the%2520data%2520and%2520deciding%2520classifications%2520using%2520data%2520as%2520a%2520reference.%250ATraining%2520data%2520is%2520summarized%2520and%2520evaluated%2520to%2520determine%2520the%2520features%2520that%2520lead%250Ato%2520the%2520classification%2520of%2520each%2520label%2520the%2520most.%2520In%2520the%2520process%2520of%2520DAP%252C%2520the%2520system%250Auses%2520the%2520data%2520summary%2520to%2520automatically%2520create%2520a%2520query%252C%2520which%2520is%2520used%2520to%250Aretrieve%2520relevant%2520rows%2520from%2520the%2520dataset.%2520A%2520classification%2520is%2520generated%2520by%2520the%250ALLM%2520using%2520data%2520summary%2520and%2520relevant%2520rows%252C%2520ensuring%2520satisfactory%2520accuracy%2520even%250Awith%2520complex%2520data.%2520Usage%2520of%2520data%2520summary%2520and%2520similar%2520data%2520in%2520DAP%2520ensures%250Acontext-aware%2520decision-making.%2520The%2520proposed%2520method%2520uses%2520the%2520words%2520%2522Act%2520as%2520an%250AExplainable%2520Machine%2520Learning%2520Model%2522%2520in%2520the%2520prompt%2520to%2520enhance%2520the%250Ainterpretability%2520of%2520the%2520predictions%2520by%2520allowing%2520users%2520to%2520review%2520the%2520logic%250Abehind%2520each%2520prediction.%2520In%2520some%2520test%2520cases%252C%2520the%2520system%2520scored%2520an%2520accuracy%2520above%250A90%2525%252C%2520proving%2520the%2520effectiveness%2520of%2520the%2520system%2520and%2520its%2520potential%2520to%2520outperform%250Aconventional%2520ML%2520models%2520in%2520various%2520scenarios.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Pro-GenAI/LML-DAP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LML%3A%20Language%20Model%20Learning%20a%20Dataset%20for%20Data-Augmented%20Prediction&entry.906535625=Praneeth%20Vadlapati&entry.1292438233=%20%20This%20paper%20introduces%20a%20new%20approach%20to%20using%20Large%20Language%20Models%20%28LLMs%29%0Afor%20classification%20tasks%2C%20which%20are%20typically%20handled%20using%20Machine%20Learning%0A%28ML%29%20models.%20Unlike%20ML%20models%20that%20rely%20heavily%20on%20data%20cleaning%20and%20feature%0Aengineering%2C%20this%20method%20streamlines%20the%20process%20using%20LLMs.%20This%20paper%0Aproposes%20a%20new%20concept%20called%20%22Language%20Model%20Learning%20%28LML%29%22%20powered%20by%20a%20new%0Amethod%20called%20%22Data-Augmented%20Prediction%20%28DAP%29%22.%20The%20classification%20is%0Aperformed%20by%20LLMs%20using%20a%20method%20similar%20to%20humans%20manually%20exploring%20and%0Aunderstanding%20the%20data%20and%20deciding%20classifications%20using%20data%20as%20a%20reference.%0ATraining%20data%20is%20summarized%20and%20evaluated%20to%20determine%20the%20features%20that%20lead%0Ato%20the%20classification%20of%20each%20label%20the%20most.%20In%20the%20process%20of%20DAP%2C%20the%20system%0Auses%20the%20data%20summary%20to%20automatically%20create%20a%20query%2C%20which%20is%20used%20to%0Aretrieve%20relevant%20rows%20from%20the%20dataset.%20A%20classification%20is%20generated%20by%20the%0ALLM%20using%20data%20summary%20and%20relevant%20rows%2C%20ensuring%20satisfactory%20accuracy%20even%0Awith%20complex%20data.%20Usage%20of%20data%20summary%20and%20similar%20data%20in%20DAP%20ensures%0Acontext-aware%20decision-making.%20The%20proposed%20method%20uses%20the%20words%20%22Act%20as%20an%0AExplainable%20Machine%20Learning%20Model%22%20in%20the%20prompt%20to%20enhance%20the%0Ainterpretability%20of%20the%20predictions%20by%20allowing%20users%20to%20review%20the%20logic%0Abehind%20each%20prediction.%20In%20some%20test%20cases%2C%20the%20system%20scored%20an%20accuracy%20above%0A90%25%2C%20proving%20the%20effectiveness%20of%20the%20system%20and%20its%20potential%20to%20outperform%0Aconventional%20ML%20models%20in%20various%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Pro-GenAI/LML-DAP%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18957v1&entry.124074799=Read"},
{"title": "Exploring Token Pruning in Vision State Space Models", "author": "Zheng Zhan and Zhenglun Kong and Yifan Gong and Yushu Wu and Zichong Meng and Hangyu Zheng and Xuan Shen and Stratis Ioannidis and Wei Niu and Pu Zhao and Yanzhi Wang", "abstract": "  State Space Models (SSMs) have the advantage of keeping linear computational\ncomplexity compared to attention modules in transformers, and have been applied\nto vision tasks as a new type of powerful vision foundation model. Inspired by\nthe observations that the final prediction in vision transformers (ViTs) is\nonly based on a subset of most informative tokens, we take the novel step of\nenhancing the efficiency of SSM-based vision models through token-based\npruning. However, direct applications of existing token pruning techniques\ndesigned for ViTs fail to deliver good performance, even with extensive\nfine-tuning. To address this issue, we revisit the unique computational\ncharacteristics of SSMs and discover that naive application disrupts the\nsequential token positions. This insight motivates us to design a novel and\ngeneral token pruning method specifically for SSM-based vision models. We first\nintroduce a pruning-aware hidden state alignment method to stabilize the\nneighborhood of remaining tokens for performance enhancement. Besides, based on\nour detailed analysis, we propose a token importance evaluation method adapted\nfor SSM models, to guide the token pruning. With efficient implementation and\npractical acceleration methods, our method brings actual speedup. Extensive\nexperiments demonstrate that our approach can achieve significant computation\nreduction with minimal impact on performance across different tasks. Notably,\nwe achieve 81.7\\% accuracy on ImageNet with a 41.6\\% reduction in the FLOPs for\npruned PlainMamba-L3. Furthermore, our work provides deeper insights into\nunderstanding the behavior of SSM-based vision models for future research.\n", "link": "http://arxiv.org/abs/2409.18962v1", "date": "2024-09-27", "relevancy": 2.1271, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5831}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Token%20Pruning%20in%20Vision%20State%20Space%20Models&body=Title%3A%20Exploring%20Token%20Pruning%20in%20Vision%20State%20Space%20Models%0AAuthor%3A%20Zheng%20Zhan%20and%20Zhenglun%20Kong%20and%20Yifan%20Gong%20and%20Yushu%20Wu%20and%20Zichong%20Meng%20and%20Hangyu%20Zheng%20and%20Xuan%20Shen%20and%20Stratis%20Ioannidis%20and%20Wei%20Niu%20and%20Pu%20Zhao%20and%20Yanzhi%20Wang%0AAbstract%3A%20%20%20State%20Space%20Models%20%28SSMs%29%20have%20the%20advantage%20of%20keeping%20linear%20computational%0Acomplexity%20compared%20to%20attention%20modules%20in%20transformers%2C%20and%20have%20been%20applied%0Ato%20vision%20tasks%20as%20a%20new%20type%20of%20powerful%20vision%20foundation%20model.%20Inspired%20by%0Athe%20observations%20that%20the%20final%20prediction%20in%20vision%20transformers%20%28ViTs%29%20is%0Aonly%20based%20on%20a%20subset%20of%20most%20informative%20tokens%2C%20we%20take%20the%20novel%20step%20of%0Aenhancing%20the%20efficiency%20of%20SSM-based%20vision%20models%20through%20token-based%0Apruning.%20However%2C%20direct%20applications%20of%20existing%20token%20pruning%20techniques%0Adesigned%20for%20ViTs%20fail%20to%20deliver%20good%20performance%2C%20even%20with%20extensive%0Afine-tuning.%20To%20address%20this%20issue%2C%20we%20revisit%20the%20unique%20computational%0Acharacteristics%20of%20SSMs%20and%20discover%20that%20naive%20application%20disrupts%20the%0Asequential%20token%20positions.%20This%20insight%20motivates%20us%20to%20design%20a%20novel%20and%0Ageneral%20token%20pruning%20method%20specifically%20for%20SSM-based%20vision%20models.%20We%20first%0Aintroduce%20a%20pruning-aware%20hidden%20state%20alignment%20method%20to%20stabilize%20the%0Aneighborhood%20of%20remaining%20tokens%20for%20performance%20enhancement.%20Besides%2C%20based%20on%0Aour%20detailed%20analysis%2C%20we%20propose%20a%20token%20importance%20evaluation%20method%20adapted%0Afor%20SSM%20models%2C%20to%20guide%20the%20token%20pruning.%20With%20efficient%20implementation%20and%0Apractical%20acceleration%20methods%2C%20our%20method%20brings%20actual%20speedup.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20can%20achieve%20significant%20computation%0Areduction%20with%20minimal%20impact%20on%20performance%20across%20different%20tasks.%20Notably%2C%0Awe%20achieve%2081.7%5C%25%20accuracy%20on%20ImageNet%20with%20a%2041.6%5C%25%20reduction%20in%20the%20FLOPs%20for%0Apruned%20PlainMamba-L3.%20Furthermore%2C%20our%20work%20provides%20deeper%20insights%20into%0Aunderstanding%20the%20behavior%20of%20SSM-based%20vision%20models%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Token%2520Pruning%2520in%2520Vision%2520State%2520Space%2520Models%26entry.906535625%3DZheng%2520Zhan%2520and%2520Zhenglun%2520Kong%2520and%2520Yifan%2520Gong%2520and%2520Yushu%2520Wu%2520and%2520Zichong%2520Meng%2520and%2520Hangyu%2520Zheng%2520and%2520Xuan%2520Shen%2520and%2520Stratis%2520Ioannidis%2520and%2520Wei%2520Niu%2520and%2520Pu%2520Zhao%2520and%2520Yanzhi%2520Wang%26entry.1292438233%3D%2520%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520have%2520the%2520advantage%2520of%2520keeping%2520linear%2520computational%250Acomplexity%2520compared%2520to%2520attention%2520modules%2520in%2520transformers%252C%2520and%2520have%2520been%2520applied%250Ato%2520vision%2520tasks%2520as%2520a%2520new%2520type%2520of%2520powerful%2520vision%2520foundation%2520model.%2520Inspired%2520by%250Athe%2520observations%2520that%2520the%2520final%2520prediction%2520in%2520vision%2520transformers%2520%2528ViTs%2529%2520is%250Aonly%2520based%2520on%2520a%2520subset%2520of%2520most%2520informative%2520tokens%252C%2520we%2520take%2520the%2520novel%2520step%2520of%250Aenhancing%2520the%2520efficiency%2520of%2520SSM-based%2520vision%2520models%2520through%2520token-based%250Apruning.%2520However%252C%2520direct%2520applications%2520of%2520existing%2520token%2520pruning%2520techniques%250Adesigned%2520for%2520ViTs%2520fail%2520to%2520deliver%2520good%2520performance%252C%2520even%2520with%2520extensive%250Afine-tuning.%2520To%2520address%2520this%2520issue%252C%2520we%2520revisit%2520the%2520unique%2520computational%250Acharacteristics%2520of%2520SSMs%2520and%2520discover%2520that%2520naive%2520application%2520disrupts%2520the%250Asequential%2520token%2520positions.%2520This%2520insight%2520motivates%2520us%2520to%2520design%2520a%2520novel%2520and%250Ageneral%2520token%2520pruning%2520method%2520specifically%2520for%2520SSM-based%2520vision%2520models.%2520We%2520first%250Aintroduce%2520a%2520pruning-aware%2520hidden%2520state%2520alignment%2520method%2520to%2520stabilize%2520the%250Aneighborhood%2520of%2520remaining%2520tokens%2520for%2520performance%2520enhancement.%2520Besides%252C%2520based%2520on%250Aour%2520detailed%2520analysis%252C%2520we%2520propose%2520a%2520token%2520importance%2520evaluation%2520method%2520adapted%250Afor%2520SSM%2520models%252C%2520to%2520guide%2520the%2520token%2520pruning.%2520With%2520efficient%2520implementation%2520and%250Apractical%2520acceleration%2520methods%252C%2520our%2520method%2520brings%2520actual%2520speedup.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520can%2520achieve%2520significant%2520computation%250Areduction%2520with%2520minimal%2520impact%2520on%2520performance%2520across%2520different%2520tasks.%2520Notably%252C%250Awe%2520achieve%252081.7%255C%2525%2520accuracy%2520on%2520ImageNet%2520with%2520a%252041.6%255C%2525%2520reduction%2520in%2520the%2520FLOPs%2520for%250Apruned%2520PlainMamba-L3.%2520Furthermore%252C%2520our%2520work%2520provides%2520deeper%2520insights%2520into%250Aunderstanding%2520the%2520behavior%2520of%2520SSM-based%2520vision%2520models%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Token%20Pruning%20in%20Vision%20State%20Space%20Models&entry.906535625=Zheng%20Zhan%20and%20Zhenglun%20Kong%20and%20Yifan%20Gong%20and%20Yushu%20Wu%20and%20Zichong%20Meng%20and%20Hangyu%20Zheng%20and%20Xuan%20Shen%20and%20Stratis%20Ioannidis%20and%20Wei%20Niu%20and%20Pu%20Zhao%20and%20Yanzhi%20Wang&entry.1292438233=%20%20State%20Space%20Models%20%28SSMs%29%20have%20the%20advantage%20of%20keeping%20linear%20computational%0Acomplexity%20compared%20to%20attention%20modules%20in%20transformers%2C%20and%20have%20been%20applied%0Ato%20vision%20tasks%20as%20a%20new%20type%20of%20powerful%20vision%20foundation%20model.%20Inspired%20by%0Athe%20observations%20that%20the%20final%20prediction%20in%20vision%20transformers%20%28ViTs%29%20is%0Aonly%20based%20on%20a%20subset%20of%20most%20informative%20tokens%2C%20we%20take%20the%20novel%20step%20of%0Aenhancing%20the%20efficiency%20of%20SSM-based%20vision%20models%20through%20token-based%0Apruning.%20However%2C%20direct%20applications%20of%20existing%20token%20pruning%20techniques%0Adesigned%20for%20ViTs%20fail%20to%20deliver%20good%20performance%2C%20even%20with%20extensive%0Afine-tuning.%20To%20address%20this%20issue%2C%20we%20revisit%20the%20unique%20computational%0Acharacteristics%20of%20SSMs%20and%20discover%20that%20naive%20application%20disrupts%20the%0Asequential%20token%20positions.%20This%20insight%20motivates%20us%20to%20design%20a%20novel%20and%0Ageneral%20token%20pruning%20method%20specifically%20for%20SSM-based%20vision%20models.%20We%20first%0Aintroduce%20a%20pruning-aware%20hidden%20state%20alignment%20method%20to%20stabilize%20the%0Aneighborhood%20of%20remaining%20tokens%20for%20performance%20enhancement.%20Besides%2C%20based%20on%0Aour%20detailed%20analysis%2C%20we%20propose%20a%20token%20importance%20evaluation%20method%20adapted%0Afor%20SSM%20models%2C%20to%20guide%20the%20token%20pruning.%20With%20efficient%20implementation%20and%0Apractical%20acceleration%20methods%2C%20our%20method%20brings%20actual%20speedup.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20can%20achieve%20significant%20computation%0Areduction%20with%20minimal%20impact%20on%20performance%20across%20different%20tasks.%20Notably%2C%0Awe%20achieve%2081.7%5C%25%20accuracy%20on%20ImageNet%20with%20a%2041.6%5C%25%20reduction%20in%20the%20FLOPs%20for%0Apruned%20PlainMamba-L3.%20Furthermore%2C%20our%20work%20provides%20deeper%20insights%20into%0Aunderstanding%20the%20behavior%20of%20SSM-based%20vision%20models%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18962v1&entry.124074799=Read"},
{"title": "MiniVLN: Efficient Vision-and-Language Navigation by Progressive\n  Knowledge Distillation", "author": "Junyou Zhu and Yanyuan Qiao and Siqi Zhang and Xingjian He and Qi Wu and Jing Liu", "abstract": "  In recent years, Embodied Artificial Intelligence (Embodied AI) has advanced\nrapidly, yet the increasing size of models conflicts with the limited\ncomputational capabilities of Embodied AI platforms. To address this challenge,\nwe aim to achieve both high model performance and practical deployability.\nSpecifically, we focus on Vision-and-Language Navigation (VLN), a core task in\nEmbodied AI. This paper introduces a two-stage knowledge distillation\nframework, producing a student model, MiniVLN, and showcasing the significant\npotential of distillation techniques in developing lightweight models. The\nproposed method aims to capture fine-grained knowledge during the pretraining\nphase and navigation-specific knowledge during the fine-tuning phase. Our\nfindings indicate that the two-stage distillation approach is more effective in\nnarrowing the performance gap between the teacher model and the student model\ncompared to single-stage distillation. On the public R2R and REVERIE\nbenchmarks, MiniVLN achieves performance on par with the teacher model while\nhaving only about 12% of the teacher model's parameter count.\n", "link": "http://arxiv.org/abs/2409.18800v1", "date": "2024-09-27", "relevancy": 2.1072, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5305}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5305}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiniVLN%3A%20Efficient%20Vision-and-Language%20Navigation%20by%20Progressive%0A%20%20Knowledge%20Distillation&body=Title%3A%20MiniVLN%3A%20Efficient%20Vision-and-Language%20Navigation%20by%20Progressive%0A%20%20Knowledge%20Distillation%0AAuthor%3A%20Junyou%20Zhu%20and%20Yanyuan%20Qiao%20and%20Siqi%20Zhang%20and%20Xingjian%20He%20and%20Qi%20Wu%20and%20Jing%20Liu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Embodied%20Artificial%20Intelligence%20%28Embodied%20AI%29%20has%20advanced%0Arapidly%2C%20yet%20the%20increasing%20size%20of%20models%20conflicts%20with%20the%20limited%0Acomputational%20capabilities%20of%20Embodied%20AI%20platforms.%20To%20address%20this%20challenge%2C%0Awe%20aim%20to%20achieve%20both%20high%20model%20performance%20and%20practical%20deployability.%0ASpecifically%2C%20we%20focus%20on%20Vision-and-Language%20Navigation%20%28VLN%29%2C%20a%20core%20task%20in%0AEmbodied%20AI.%20This%20paper%20introduces%20a%20two-stage%20knowledge%20distillation%0Aframework%2C%20producing%20a%20student%20model%2C%20MiniVLN%2C%20and%20showcasing%20the%20significant%0Apotential%20of%20distillation%20techniques%20in%20developing%20lightweight%20models.%20The%0Aproposed%20method%20aims%20to%20capture%20fine-grained%20knowledge%20during%20the%20pretraining%0Aphase%20and%20navigation-specific%20knowledge%20during%20the%20fine-tuning%20phase.%20Our%0Afindings%20indicate%20that%20the%20two-stage%20distillation%20approach%20is%20more%20effective%20in%0Anarrowing%20the%20performance%20gap%20between%20the%20teacher%20model%20and%20the%20student%20model%0Acompared%20to%20single-stage%20distillation.%20On%20the%20public%20R2R%20and%20REVERIE%0Abenchmarks%2C%20MiniVLN%20achieves%20performance%20on%20par%20with%20the%20teacher%20model%20while%0Ahaving%20only%20about%2012%25%20of%20the%20teacher%20model%27s%20parameter%20count.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiniVLN%253A%2520Efficient%2520Vision-and-Language%2520Navigation%2520by%2520Progressive%250A%2520%2520Knowledge%2520Distillation%26entry.906535625%3DJunyou%2520Zhu%2520and%2520Yanyuan%2520Qiao%2520and%2520Siqi%2520Zhang%2520and%2520Xingjian%2520He%2520and%2520Qi%2520Wu%2520and%2520Jing%2520Liu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Embodied%2520Artificial%2520Intelligence%2520%2528Embodied%2520AI%2529%2520has%2520advanced%250Arapidly%252C%2520yet%2520the%2520increasing%2520size%2520of%2520models%2520conflicts%2520with%2520the%2520limited%250Acomputational%2520capabilities%2520of%2520Embodied%2520AI%2520platforms.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520aim%2520to%2520achieve%2520both%2520high%2520model%2520performance%2520and%2520practical%2520deployability.%250ASpecifically%252C%2520we%2520focus%2520on%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%252C%2520a%2520core%2520task%2520in%250AEmbodied%2520AI.%2520This%2520paper%2520introduces%2520a%2520two-stage%2520knowledge%2520distillation%250Aframework%252C%2520producing%2520a%2520student%2520model%252C%2520MiniVLN%252C%2520and%2520showcasing%2520the%2520significant%250Apotential%2520of%2520distillation%2520techniques%2520in%2520developing%2520lightweight%2520models.%2520The%250Aproposed%2520method%2520aims%2520to%2520capture%2520fine-grained%2520knowledge%2520during%2520the%2520pretraining%250Aphase%2520and%2520navigation-specific%2520knowledge%2520during%2520the%2520fine-tuning%2520phase.%2520Our%250Afindings%2520indicate%2520that%2520the%2520two-stage%2520distillation%2520approach%2520is%2520more%2520effective%2520in%250Anarrowing%2520the%2520performance%2520gap%2520between%2520the%2520teacher%2520model%2520and%2520the%2520student%2520model%250Acompared%2520to%2520single-stage%2520distillation.%2520On%2520the%2520public%2520R2R%2520and%2520REVERIE%250Abenchmarks%252C%2520MiniVLN%2520achieves%2520performance%2520on%2520par%2520with%2520the%2520teacher%2520model%2520while%250Ahaving%2520only%2520about%252012%2525%2520of%2520the%2520teacher%2520model%2527s%2520parameter%2520count.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniVLN%3A%20Efficient%20Vision-and-Language%20Navigation%20by%20Progressive%0A%20%20Knowledge%20Distillation&entry.906535625=Junyou%20Zhu%20and%20Yanyuan%20Qiao%20and%20Siqi%20Zhang%20and%20Xingjian%20He%20and%20Qi%20Wu%20and%20Jing%20Liu&entry.1292438233=%20%20In%20recent%20years%2C%20Embodied%20Artificial%20Intelligence%20%28Embodied%20AI%29%20has%20advanced%0Arapidly%2C%20yet%20the%20increasing%20size%20of%20models%20conflicts%20with%20the%20limited%0Acomputational%20capabilities%20of%20Embodied%20AI%20platforms.%20To%20address%20this%20challenge%2C%0Awe%20aim%20to%20achieve%20both%20high%20model%20performance%20and%20practical%20deployability.%0ASpecifically%2C%20we%20focus%20on%20Vision-and-Language%20Navigation%20%28VLN%29%2C%20a%20core%20task%20in%0AEmbodied%20AI.%20This%20paper%20introduces%20a%20two-stage%20knowledge%20distillation%0Aframework%2C%20producing%20a%20student%20model%2C%20MiniVLN%2C%20and%20showcasing%20the%20significant%0Apotential%20of%20distillation%20techniques%20in%20developing%20lightweight%20models.%20The%0Aproposed%20method%20aims%20to%20capture%20fine-grained%20knowledge%20during%20the%20pretraining%0Aphase%20and%20navigation-specific%20knowledge%20during%20the%20fine-tuning%20phase.%20Our%0Afindings%20indicate%20that%20the%20two-stage%20distillation%20approach%20is%20more%20effective%20in%0Anarrowing%20the%20performance%20gap%20between%20the%20teacher%20model%20and%20the%20student%20model%0Acompared%20to%20single-stage%20distillation.%20On%20the%20public%20R2R%20and%20REVERIE%0Abenchmarks%2C%20MiniVLN%20achieves%20performance%20on%20par%20with%20the%20teacher%20model%20while%0Ahaving%20only%20about%2012%25%20of%20the%20teacher%20model%27s%20parameter%20count.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18800v1&entry.124074799=Read"},
{"title": "Learning from Demonstration with Implicit Nonlinear Dynamics Models", "author": "Peter David Fagan and Subramanian Ramamoorthy", "abstract": "  Learning from Demonstration (LfD) is a useful paradigm for training policies\nthat solve tasks involving complex motions. In practice, the successful\napplication of LfD requires overcoming error accumulation during policy\nexecution, i.e. the problem of drift due to errors compounding over time and\nthe consequent out-of-distribution behaviours. Existing works seek to address\nthis problem through scaling data collection, correcting policy errors with a\nhuman-in-the-loop, temporally ensembling policy predictions or through learning\nthe parameters of a dynamical system model. In this work, we propose and\nvalidate an alternative approach to overcoming this issue. Inspired by\nreservoir computing, we develop a novel neural network layer that includes a\nfixed nonlinear dynamical system with tunable dynamical properties. We validate\nthe efficacy of our neural network layer on the task of reproducing human\nhandwriting motions using the LASA Human Handwriting Dataset. Through empirical\nexperiments we demonstrate that incorporating our layer into existing neural\nnetwork architectures addresses the issue of compounding errors in LfD.\nFurthermore, we perform a comparative evaluation against existing approaches\nincluding a temporal ensemble of policy predictions and an Echo State Networks\n(ESNs) implementation. We find that our approach yields greater policy\nprecision and robustness on the handwriting task while also generalising to\nmultiple dynamics regimes and maintaining competitive latency scores.\n", "link": "http://arxiv.org/abs/2409.18768v1", "date": "2024-09-27", "relevancy": 2.1012, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.53}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5221}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Demonstration%20with%20Implicit%20Nonlinear%20Dynamics%20Models&body=Title%3A%20Learning%20from%20Demonstration%20with%20Implicit%20Nonlinear%20Dynamics%20Models%0AAuthor%3A%20Peter%20David%20Fagan%20and%20Subramanian%20Ramamoorthy%0AAbstract%3A%20%20%20Learning%20from%20Demonstration%20%28LfD%29%20is%20a%20useful%20paradigm%20for%20training%20policies%0Athat%20solve%20tasks%20involving%20complex%20motions.%20In%20practice%2C%20the%20successful%0Aapplication%20of%20LfD%20requires%20overcoming%20error%20accumulation%20during%20policy%0Aexecution%2C%20i.e.%20the%20problem%20of%20drift%20due%20to%20errors%20compounding%20over%20time%20and%0Athe%20consequent%20out-of-distribution%20behaviours.%20Existing%20works%20seek%20to%20address%0Athis%20problem%20through%20scaling%20data%20collection%2C%20correcting%20policy%20errors%20with%20a%0Ahuman-in-the-loop%2C%20temporally%20ensembling%20policy%20predictions%20or%20through%20learning%0Athe%20parameters%20of%20a%20dynamical%20system%20model.%20In%20this%20work%2C%20we%20propose%20and%0Avalidate%20an%20alternative%20approach%20to%20overcoming%20this%20issue.%20Inspired%20by%0Areservoir%20computing%2C%20we%20develop%20a%20novel%20neural%20network%20layer%20that%20includes%20a%0Afixed%20nonlinear%20dynamical%20system%20with%20tunable%20dynamical%20properties.%20We%20validate%0Athe%20efficacy%20of%20our%20neural%20network%20layer%20on%20the%20task%20of%20reproducing%20human%0Ahandwriting%20motions%20using%20the%20LASA%20Human%20Handwriting%20Dataset.%20Through%20empirical%0Aexperiments%20we%20demonstrate%20that%20incorporating%20our%20layer%20into%20existing%20neural%0Anetwork%20architectures%20addresses%20the%20issue%20of%20compounding%20errors%20in%20LfD.%0AFurthermore%2C%20we%20perform%20a%20comparative%20evaluation%20against%20existing%20approaches%0Aincluding%20a%20temporal%20ensemble%20of%20policy%20predictions%20and%20an%20Echo%20State%20Networks%0A%28ESNs%29%20implementation.%20We%20find%20that%20our%20approach%20yields%20greater%20policy%0Aprecision%20and%20robustness%20on%20the%20handwriting%20task%20while%20also%20generalising%20to%0Amultiple%20dynamics%20regimes%20and%20maintaining%20competitive%20latency%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Demonstration%2520with%2520Implicit%2520Nonlinear%2520Dynamics%2520Models%26entry.906535625%3DPeter%2520David%2520Fagan%2520and%2520Subramanian%2520Ramamoorthy%26entry.1292438233%3D%2520%2520Learning%2520from%2520Demonstration%2520%2528LfD%2529%2520is%2520a%2520useful%2520paradigm%2520for%2520training%2520policies%250Athat%2520solve%2520tasks%2520involving%2520complex%2520motions.%2520In%2520practice%252C%2520the%2520successful%250Aapplication%2520of%2520LfD%2520requires%2520overcoming%2520error%2520accumulation%2520during%2520policy%250Aexecution%252C%2520i.e.%2520the%2520problem%2520of%2520drift%2520due%2520to%2520errors%2520compounding%2520over%2520time%2520and%250Athe%2520consequent%2520out-of-distribution%2520behaviours.%2520Existing%2520works%2520seek%2520to%2520address%250Athis%2520problem%2520through%2520scaling%2520data%2520collection%252C%2520correcting%2520policy%2520errors%2520with%2520a%250Ahuman-in-the-loop%252C%2520temporally%2520ensembling%2520policy%2520predictions%2520or%2520through%2520learning%250Athe%2520parameters%2520of%2520a%2520dynamical%2520system%2520model.%2520In%2520this%2520work%252C%2520we%2520propose%2520and%250Avalidate%2520an%2520alternative%2520approach%2520to%2520overcoming%2520this%2520issue.%2520Inspired%2520by%250Areservoir%2520computing%252C%2520we%2520develop%2520a%2520novel%2520neural%2520network%2520layer%2520that%2520includes%2520a%250Afixed%2520nonlinear%2520dynamical%2520system%2520with%2520tunable%2520dynamical%2520properties.%2520We%2520validate%250Athe%2520efficacy%2520of%2520our%2520neural%2520network%2520layer%2520on%2520the%2520task%2520of%2520reproducing%2520human%250Ahandwriting%2520motions%2520using%2520the%2520LASA%2520Human%2520Handwriting%2520Dataset.%2520Through%2520empirical%250Aexperiments%2520we%2520demonstrate%2520that%2520incorporating%2520our%2520layer%2520into%2520existing%2520neural%250Anetwork%2520architectures%2520addresses%2520the%2520issue%2520of%2520compounding%2520errors%2520in%2520LfD.%250AFurthermore%252C%2520we%2520perform%2520a%2520comparative%2520evaluation%2520against%2520existing%2520approaches%250Aincluding%2520a%2520temporal%2520ensemble%2520of%2520policy%2520predictions%2520and%2520an%2520Echo%2520State%2520Networks%250A%2528ESNs%2529%2520implementation.%2520We%2520find%2520that%2520our%2520approach%2520yields%2520greater%2520policy%250Aprecision%2520and%2520robustness%2520on%2520the%2520handwriting%2520task%2520while%2520also%2520generalising%2520to%250Amultiple%2520dynamics%2520regimes%2520and%2520maintaining%2520competitive%2520latency%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Demonstration%20with%20Implicit%20Nonlinear%20Dynamics%20Models&entry.906535625=Peter%20David%20Fagan%20and%20Subramanian%20Ramamoorthy&entry.1292438233=%20%20Learning%20from%20Demonstration%20%28LfD%29%20is%20a%20useful%20paradigm%20for%20training%20policies%0Athat%20solve%20tasks%20involving%20complex%20motions.%20In%20practice%2C%20the%20successful%0Aapplication%20of%20LfD%20requires%20overcoming%20error%20accumulation%20during%20policy%0Aexecution%2C%20i.e.%20the%20problem%20of%20drift%20due%20to%20errors%20compounding%20over%20time%20and%0Athe%20consequent%20out-of-distribution%20behaviours.%20Existing%20works%20seek%20to%20address%0Athis%20problem%20through%20scaling%20data%20collection%2C%20correcting%20policy%20errors%20with%20a%0Ahuman-in-the-loop%2C%20temporally%20ensembling%20policy%20predictions%20or%20through%20learning%0Athe%20parameters%20of%20a%20dynamical%20system%20model.%20In%20this%20work%2C%20we%20propose%20and%0Avalidate%20an%20alternative%20approach%20to%20overcoming%20this%20issue.%20Inspired%20by%0Areservoir%20computing%2C%20we%20develop%20a%20novel%20neural%20network%20layer%20that%20includes%20a%0Afixed%20nonlinear%20dynamical%20system%20with%20tunable%20dynamical%20properties.%20We%20validate%0Athe%20efficacy%20of%20our%20neural%20network%20layer%20on%20the%20task%20of%20reproducing%20human%0Ahandwriting%20motions%20using%20the%20LASA%20Human%20Handwriting%20Dataset.%20Through%20empirical%0Aexperiments%20we%20demonstrate%20that%20incorporating%20our%20layer%20into%20existing%20neural%0Anetwork%20architectures%20addresses%20the%20issue%20of%20compounding%20errors%20in%20LfD.%0AFurthermore%2C%20we%20perform%20a%20comparative%20evaluation%20against%20existing%20approaches%0Aincluding%20a%20temporal%20ensemble%20of%20policy%20predictions%20and%20an%20Echo%20State%20Networks%0A%28ESNs%29%20implementation.%20We%20find%20that%20our%20approach%20yields%20greater%20policy%0Aprecision%20and%20robustness%20on%20the%20handwriting%20task%20while%20also%20generalising%20to%0Amultiple%20dynamics%20regimes%20and%20maintaining%20competitive%20latency%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18768v1&entry.124074799=Read"},
{"title": "2D or not 2D: How Does the Dimensionality of Gesture Representation\n  Affect 3D Co-Speech Gesture Generation?", "author": "T\u00e9o Guichoux and Laure Soulier and Nicolas Obin and Catherine Pelachaud", "abstract": "  Co-speech gestures are fundamental for communication. The advent of recent\ndeep learning techniques has facilitated the creation of lifelike, synchronous\nco-speech gestures for Embodied Conversational Agents. \"In-the-wild\" datasets,\naggregating video content from platforms like YouTube via human pose detection\ntechnologies, provide a feasible solution by offering 2D skeletal sequences\naligned with speech. Concurrent developments in lifting models enable the\nconversion of these 2D sequences into 3D gesture databases. However, it is\nimportant to note that the 3D poses estimated from the 2D extracted poses are,\nin essence, approximations of the ground-truth, which remains in the 2D domain.\nThis distinction raises questions about the impact of gesture representation\ndimensionality on the quality of generated motions - a topic that, to our\nknowledge, remains largely unexplored. Our study examines the effect of using\neither 2D or 3D joint coordinates as training data on the performance of\nspeech-to-gesture deep generative models. We employ a lifting model for\nconverting generated 2D pose sequences into 3D and assess how gestures created\ndirectly in 3D stack up against those initially generated in 2D and then\nconverted to 3D. We perform an objective evaluation using widely used metrics\nin the gesture generation field as well as a user study to qualitatively\nevaluate the different approaches.\n", "link": "http://arxiv.org/abs/2409.10357v2", "date": "2024-09-27", "relevancy": 2.0923, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5305}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202D%20or%20not%202D%3A%20How%20Does%20the%20Dimensionality%20of%20Gesture%20Representation%0A%20%20Affect%203D%20Co-Speech%20Gesture%20Generation%3F&body=Title%3A%202D%20or%20not%202D%3A%20How%20Does%20the%20Dimensionality%20of%20Gesture%20Representation%0A%20%20Affect%203D%20Co-Speech%20Gesture%20Generation%3F%0AAuthor%3A%20T%C3%A9o%20Guichoux%20and%20Laure%20Soulier%20and%20Nicolas%20Obin%20and%20Catherine%20Pelachaud%0AAbstract%3A%20%20%20Co-speech%20gestures%20are%20fundamental%20for%20communication.%20The%20advent%20of%20recent%0Adeep%20learning%20techniques%20has%20facilitated%20the%20creation%20of%20lifelike%2C%20synchronous%0Aco-speech%20gestures%20for%20Embodied%20Conversational%20Agents.%20%22In-the-wild%22%20datasets%2C%0Aaggregating%20video%20content%20from%20platforms%20like%20YouTube%20via%20human%20pose%20detection%0Atechnologies%2C%20provide%20a%20feasible%20solution%20by%20offering%202D%20skeletal%20sequences%0Aaligned%20with%20speech.%20Concurrent%20developments%20in%20lifting%20models%20enable%20the%0Aconversion%20of%20these%202D%20sequences%20into%203D%20gesture%20databases.%20However%2C%20it%20is%0Aimportant%20to%20note%20that%20the%203D%20poses%20estimated%20from%20the%202D%20extracted%20poses%20are%2C%0Ain%20essence%2C%20approximations%20of%20the%20ground-truth%2C%20which%20remains%20in%20the%202D%20domain.%0AThis%20distinction%20raises%20questions%20about%20the%20impact%20of%20gesture%20representation%0Adimensionality%20on%20the%20quality%20of%20generated%20motions%20-%20a%20topic%20that%2C%20to%20our%0Aknowledge%2C%20remains%20largely%20unexplored.%20Our%20study%20examines%20the%20effect%20of%20using%0Aeither%202D%20or%203D%20joint%20coordinates%20as%20training%20data%20on%20the%20performance%20of%0Aspeech-to-gesture%20deep%20generative%20models.%20We%20employ%20a%20lifting%20model%20for%0Aconverting%20generated%202D%20pose%20sequences%20into%203D%20and%20assess%20how%20gestures%20created%0Adirectly%20in%203D%20stack%20up%20against%20those%20initially%20generated%20in%202D%20and%20then%0Aconverted%20to%203D.%20We%20perform%20an%20objective%20evaluation%20using%20widely%20used%20metrics%0Ain%20the%20gesture%20generation%20field%20as%20well%20as%20a%20user%20study%20to%20qualitatively%0Aevaluate%20the%20different%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10357v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2D%2520or%2520not%25202D%253A%2520How%2520Does%2520the%2520Dimensionality%2520of%2520Gesture%2520Representation%250A%2520%2520Affect%25203D%2520Co-Speech%2520Gesture%2520Generation%253F%26entry.906535625%3DT%25C3%25A9o%2520Guichoux%2520and%2520Laure%2520Soulier%2520and%2520Nicolas%2520Obin%2520and%2520Catherine%2520Pelachaud%26entry.1292438233%3D%2520%2520Co-speech%2520gestures%2520are%2520fundamental%2520for%2520communication.%2520The%2520advent%2520of%2520recent%250Adeep%2520learning%2520techniques%2520has%2520facilitated%2520the%2520creation%2520of%2520lifelike%252C%2520synchronous%250Aco-speech%2520gestures%2520for%2520Embodied%2520Conversational%2520Agents.%2520%2522In-the-wild%2522%2520datasets%252C%250Aaggregating%2520video%2520content%2520from%2520platforms%2520like%2520YouTube%2520via%2520human%2520pose%2520detection%250Atechnologies%252C%2520provide%2520a%2520feasible%2520solution%2520by%2520offering%25202D%2520skeletal%2520sequences%250Aaligned%2520with%2520speech.%2520Concurrent%2520developments%2520in%2520lifting%2520models%2520enable%2520the%250Aconversion%2520of%2520these%25202D%2520sequences%2520into%25203D%2520gesture%2520databases.%2520However%252C%2520it%2520is%250Aimportant%2520to%2520note%2520that%2520the%25203D%2520poses%2520estimated%2520from%2520the%25202D%2520extracted%2520poses%2520are%252C%250Ain%2520essence%252C%2520approximations%2520of%2520the%2520ground-truth%252C%2520which%2520remains%2520in%2520the%25202D%2520domain.%250AThis%2520distinction%2520raises%2520questions%2520about%2520the%2520impact%2520of%2520gesture%2520representation%250Adimensionality%2520on%2520the%2520quality%2520of%2520generated%2520motions%2520-%2520a%2520topic%2520that%252C%2520to%2520our%250Aknowledge%252C%2520remains%2520largely%2520unexplored.%2520Our%2520study%2520examines%2520the%2520effect%2520of%2520using%250Aeither%25202D%2520or%25203D%2520joint%2520coordinates%2520as%2520training%2520data%2520on%2520the%2520performance%2520of%250Aspeech-to-gesture%2520deep%2520generative%2520models.%2520We%2520employ%2520a%2520lifting%2520model%2520for%250Aconverting%2520generated%25202D%2520pose%2520sequences%2520into%25203D%2520and%2520assess%2520how%2520gestures%2520created%250Adirectly%2520in%25203D%2520stack%2520up%2520against%2520those%2520initially%2520generated%2520in%25202D%2520and%2520then%250Aconverted%2520to%25203D.%2520We%2520perform%2520an%2520objective%2520evaluation%2520using%2520widely%2520used%2520metrics%250Ain%2520the%2520gesture%2520generation%2520field%2520as%2520well%2520as%2520a%2520user%2520study%2520to%2520qualitatively%250Aevaluate%2520the%2520different%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10357v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2D%20or%20not%202D%3A%20How%20Does%20the%20Dimensionality%20of%20Gesture%20Representation%0A%20%20Affect%203D%20Co-Speech%20Gesture%20Generation%3F&entry.906535625=T%C3%A9o%20Guichoux%20and%20Laure%20Soulier%20and%20Nicolas%20Obin%20and%20Catherine%20Pelachaud&entry.1292438233=%20%20Co-speech%20gestures%20are%20fundamental%20for%20communication.%20The%20advent%20of%20recent%0Adeep%20learning%20techniques%20has%20facilitated%20the%20creation%20of%20lifelike%2C%20synchronous%0Aco-speech%20gestures%20for%20Embodied%20Conversational%20Agents.%20%22In-the-wild%22%20datasets%2C%0Aaggregating%20video%20content%20from%20platforms%20like%20YouTube%20via%20human%20pose%20detection%0Atechnologies%2C%20provide%20a%20feasible%20solution%20by%20offering%202D%20skeletal%20sequences%0Aaligned%20with%20speech.%20Concurrent%20developments%20in%20lifting%20models%20enable%20the%0Aconversion%20of%20these%202D%20sequences%20into%203D%20gesture%20databases.%20However%2C%20it%20is%0Aimportant%20to%20note%20that%20the%203D%20poses%20estimated%20from%20the%202D%20extracted%20poses%20are%2C%0Ain%20essence%2C%20approximations%20of%20the%20ground-truth%2C%20which%20remains%20in%20the%202D%20domain.%0AThis%20distinction%20raises%20questions%20about%20the%20impact%20of%20gesture%20representation%0Adimensionality%20on%20the%20quality%20of%20generated%20motions%20-%20a%20topic%20that%2C%20to%20our%0Aknowledge%2C%20remains%20largely%20unexplored.%20Our%20study%20examines%20the%20effect%20of%20using%0Aeither%202D%20or%203D%20joint%20coordinates%20as%20training%20data%20on%20the%20performance%20of%0Aspeech-to-gesture%20deep%20generative%20models.%20We%20employ%20a%20lifting%20model%20for%0Aconverting%20generated%202D%20pose%20sequences%20into%203D%20and%20assess%20how%20gestures%20created%0Adirectly%20in%203D%20stack%20up%20against%20those%20initially%20generated%20in%202D%20and%20then%0Aconverted%20to%203D.%20We%20perform%20an%20objective%20evaluation%20using%20widely%20used%20metrics%0Ain%20the%20gesture%20generation%20field%20as%20well%20as%20a%20user%20study%20to%20qualitatively%0Aevaluate%20the%20different%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10357v2&entry.124074799=Read"},
{"title": "Effects of AI Feedback on Learning, the Skill Gap, and Intellectual\n  Diversity", "author": "Christoph Riedl and Eric Bogert", "abstract": "  Can human decision-makers learn from AI feedback? Using data on 52,000\ndecision-makers from a large online chess platform, we investigate how their AI\nuse affects three interrelated long-term outcomes: Learning, skill gap, and\ndiversity of decision strategies. First, we show that individuals are far more\nlikely to seek AI feedback in situations in which they experienced success\nrather than failure. This AI feedback seeking strategy turns out to be\ndetrimental to learning: Feedback on successes decreases future performance,\nwhile feedback on failures increases it. Second, higher-skilled decision-makers\nseek AI feedback more often and are far more likely to seek AI feedback after a\nfailure, and benefit more from AI feedback than lower-skilled individuals. As a\nresult, access to AI feedback increases, rather than decreases, the skill gap\nbetween high- and low-skilled individuals. Finally, we leverage 42 major\nplatform updates as natural experiments to show that access to AI feedback\ncauses a decrease in intellectual diversity of the population as individuals\ntend to specialize in the same areas. Together, those results indicate that\nlearning from AI feedback is not automatic and using AI correctly seems to be a\nskill itself. Furthermore, despite its individual-level benefits, access to AI\nfeedback can have significant population-level downsides including loss of\nintellectual diversity and an increasing skill gap.\n", "link": "http://arxiv.org/abs/2409.18660v1", "date": "2024-09-27", "relevancy": 2.0921, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4213}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4173}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effects%20of%20AI%20Feedback%20on%20Learning%2C%20the%20Skill%20Gap%2C%20and%20Intellectual%0A%20%20Diversity&body=Title%3A%20Effects%20of%20AI%20Feedback%20on%20Learning%2C%20the%20Skill%20Gap%2C%20and%20Intellectual%0A%20%20Diversity%0AAuthor%3A%20Christoph%20Riedl%20and%20Eric%20Bogert%0AAbstract%3A%20%20%20Can%20human%20decision-makers%20learn%20from%20AI%20feedback%3F%20Using%20data%20on%2052%2C000%0Adecision-makers%20from%20a%20large%20online%20chess%20platform%2C%20we%20investigate%20how%20their%20AI%0Ause%20affects%20three%20interrelated%20long-term%20outcomes%3A%20Learning%2C%20skill%20gap%2C%20and%0Adiversity%20of%20decision%20strategies.%20First%2C%20we%20show%20that%20individuals%20are%20far%20more%0Alikely%20to%20seek%20AI%20feedback%20in%20situations%20in%20which%20they%20experienced%20success%0Arather%20than%20failure.%20This%20AI%20feedback%20seeking%20strategy%20turns%20out%20to%20be%0Adetrimental%20to%20learning%3A%20Feedback%20on%20successes%20decreases%20future%20performance%2C%0Awhile%20feedback%20on%20failures%20increases%20it.%20Second%2C%20higher-skilled%20decision-makers%0Aseek%20AI%20feedback%20more%20often%20and%20are%20far%20more%20likely%20to%20seek%20AI%20feedback%20after%20a%0Afailure%2C%20and%20benefit%20more%20from%20AI%20feedback%20than%20lower-skilled%20individuals.%20As%20a%0Aresult%2C%20access%20to%20AI%20feedback%20increases%2C%20rather%20than%20decreases%2C%20the%20skill%20gap%0Abetween%20high-%20and%20low-skilled%20individuals.%20Finally%2C%20we%20leverage%2042%20major%0Aplatform%20updates%20as%20natural%20experiments%20to%20show%20that%20access%20to%20AI%20feedback%0Acauses%20a%20decrease%20in%20intellectual%20diversity%20of%20the%20population%20as%20individuals%0Atend%20to%20specialize%20in%20the%20same%20areas.%20Together%2C%20those%20results%20indicate%20that%0Alearning%20from%20AI%20feedback%20is%20not%20automatic%20and%20using%20AI%20correctly%20seems%20to%20be%20a%0Askill%20itself.%20Furthermore%2C%20despite%20its%20individual-level%20benefits%2C%20access%20to%20AI%0Afeedback%20can%20have%20significant%20population-level%20downsides%20including%20loss%20of%0Aintellectual%20diversity%20and%20an%20increasing%20skill%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffects%2520of%2520AI%2520Feedback%2520on%2520Learning%252C%2520the%2520Skill%2520Gap%252C%2520and%2520Intellectual%250A%2520%2520Diversity%26entry.906535625%3DChristoph%2520Riedl%2520and%2520Eric%2520Bogert%26entry.1292438233%3D%2520%2520Can%2520human%2520decision-makers%2520learn%2520from%2520AI%2520feedback%253F%2520Using%2520data%2520on%252052%252C000%250Adecision-makers%2520from%2520a%2520large%2520online%2520chess%2520platform%252C%2520we%2520investigate%2520how%2520their%2520AI%250Ause%2520affects%2520three%2520interrelated%2520long-term%2520outcomes%253A%2520Learning%252C%2520skill%2520gap%252C%2520and%250Adiversity%2520of%2520decision%2520strategies.%2520First%252C%2520we%2520show%2520that%2520individuals%2520are%2520far%2520more%250Alikely%2520to%2520seek%2520AI%2520feedback%2520in%2520situations%2520in%2520which%2520they%2520experienced%2520success%250Arather%2520than%2520failure.%2520This%2520AI%2520feedback%2520seeking%2520strategy%2520turns%2520out%2520to%2520be%250Adetrimental%2520to%2520learning%253A%2520Feedback%2520on%2520successes%2520decreases%2520future%2520performance%252C%250Awhile%2520feedback%2520on%2520failures%2520increases%2520it.%2520Second%252C%2520higher-skilled%2520decision-makers%250Aseek%2520AI%2520feedback%2520more%2520often%2520and%2520are%2520far%2520more%2520likely%2520to%2520seek%2520AI%2520feedback%2520after%2520a%250Afailure%252C%2520and%2520benefit%2520more%2520from%2520AI%2520feedback%2520than%2520lower-skilled%2520individuals.%2520As%2520a%250Aresult%252C%2520access%2520to%2520AI%2520feedback%2520increases%252C%2520rather%2520than%2520decreases%252C%2520the%2520skill%2520gap%250Abetween%2520high-%2520and%2520low-skilled%2520individuals.%2520Finally%252C%2520we%2520leverage%252042%2520major%250Aplatform%2520updates%2520as%2520natural%2520experiments%2520to%2520show%2520that%2520access%2520to%2520AI%2520feedback%250Acauses%2520a%2520decrease%2520in%2520intellectual%2520diversity%2520of%2520the%2520population%2520as%2520individuals%250Atend%2520to%2520specialize%2520in%2520the%2520same%2520areas.%2520Together%252C%2520those%2520results%2520indicate%2520that%250Alearning%2520from%2520AI%2520feedback%2520is%2520not%2520automatic%2520and%2520using%2520AI%2520correctly%2520seems%2520to%2520be%2520a%250Askill%2520itself.%2520Furthermore%252C%2520despite%2520its%2520individual-level%2520benefits%252C%2520access%2520to%2520AI%250Afeedback%2520can%2520have%2520significant%2520population-level%2520downsides%2520including%2520loss%2520of%250Aintellectual%2520diversity%2520and%2520an%2520increasing%2520skill%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effects%20of%20AI%20Feedback%20on%20Learning%2C%20the%20Skill%20Gap%2C%20and%20Intellectual%0A%20%20Diversity&entry.906535625=Christoph%20Riedl%20and%20Eric%20Bogert&entry.1292438233=%20%20Can%20human%20decision-makers%20learn%20from%20AI%20feedback%3F%20Using%20data%20on%2052%2C000%0Adecision-makers%20from%20a%20large%20online%20chess%20platform%2C%20we%20investigate%20how%20their%20AI%0Ause%20affects%20three%20interrelated%20long-term%20outcomes%3A%20Learning%2C%20skill%20gap%2C%20and%0Adiversity%20of%20decision%20strategies.%20First%2C%20we%20show%20that%20individuals%20are%20far%20more%0Alikely%20to%20seek%20AI%20feedback%20in%20situations%20in%20which%20they%20experienced%20success%0Arather%20than%20failure.%20This%20AI%20feedback%20seeking%20strategy%20turns%20out%20to%20be%0Adetrimental%20to%20learning%3A%20Feedback%20on%20successes%20decreases%20future%20performance%2C%0Awhile%20feedback%20on%20failures%20increases%20it.%20Second%2C%20higher-skilled%20decision-makers%0Aseek%20AI%20feedback%20more%20often%20and%20are%20far%20more%20likely%20to%20seek%20AI%20feedback%20after%20a%0Afailure%2C%20and%20benefit%20more%20from%20AI%20feedback%20than%20lower-skilled%20individuals.%20As%20a%0Aresult%2C%20access%20to%20AI%20feedback%20increases%2C%20rather%20than%20decreases%2C%20the%20skill%20gap%0Abetween%20high-%20and%20low-skilled%20individuals.%20Finally%2C%20we%20leverage%2042%20major%0Aplatform%20updates%20as%20natural%20experiments%20to%20show%20that%20access%20to%20AI%20feedback%0Acauses%20a%20decrease%20in%20intellectual%20diversity%20of%20the%20population%20as%20individuals%0Atend%20to%20specialize%20in%20the%20same%20areas.%20Together%2C%20those%20results%20indicate%20that%0Alearning%20from%20AI%20feedback%20is%20not%20automatic%20and%20using%20AI%20correctly%20seems%20to%20be%20a%0Askill%20itself.%20Furthermore%2C%20despite%20its%20individual-level%20benefits%2C%20access%20to%20AI%0Afeedback%20can%20have%20significant%20population-level%20downsides%20including%20loss%20of%0Aintellectual%20diversity%20and%20an%20increasing%20skill%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18660v1&entry.124074799=Read"},
{"title": "A Novel Framework for the Automated Characterization of Gram-Stained\n  Blood Culture Slides Using a Large-Scale Vision Transformer", "author": "Jack McMahon and Naofumi Tomita and Elizabeth S. Tatishev and Adrienne A. Workman and Cristina R Costales and Niaz Banaei and Isabella W. Martin and Saeed Hassanpour", "abstract": "  This study introduces a new framework for the artificial\nintelligence-assisted characterization of Gram-stained whole-slide images\n(WSIs). As a test for the diagnosis of bloodstream infections, Gram stains\nprovide critical early data to inform patient treatment. Rapid and reliable\nanalysis of Gram stains has been shown to be positively associated with better\nclinical outcomes, underscoring the need for improved tools to automate Gram\nstain analysis. In this work, we developed a novel transformer-based model for\nGram-stained WSI classification, which is more scalable to large datasets than\nprevious convolutional neural network (CNN) -based methods as it does not\nrequire patch-level manual annotations. We also introduce a large Gram stain\ndataset from Dartmouth-Hitchcock Medical Center (Lebanon, New Hampshire, USA)\nto evaluate our model, exploring the classification of five major categories of\nGram-stained WSIs: Gram-positive cocci in clusters, Gram-positive cocci in\npairs/chains, Gram-positive rods, Gram-negative rods, and slides with no\nbacteria. Our model achieves a classification accuracy of 0.858 (95% CI: 0.805,\n0.905) and an AUC of 0.952 (95% CI: 0.922, 0.976) using five-fold nested\ncross-validation on our 475-slide dataset, demonstrating the potential of\nlarge-scale transformer models for Gram stain classification. We further\ndemonstrate the generalizability of our trained model, which achieves strong\nperformance on external datasets without additional fine-tuning.\n", "link": "http://arxiv.org/abs/2409.15546v2", "date": "2024-09-27", "relevancy": 2.0862, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5607}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5244}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Framework%20for%20the%20Automated%20Characterization%20of%20Gram-Stained%0A%20%20Blood%20Culture%20Slides%20Using%20a%20Large-Scale%20Vision%20Transformer&body=Title%3A%20A%20Novel%20Framework%20for%20the%20Automated%20Characterization%20of%20Gram-Stained%0A%20%20Blood%20Culture%20Slides%20Using%20a%20Large-Scale%20Vision%20Transformer%0AAuthor%3A%20Jack%20McMahon%20and%20Naofumi%20Tomita%20and%20Elizabeth%20S.%20Tatishev%20and%20Adrienne%20A.%20Workman%20and%20Cristina%20R%20Costales%20and%20Niaz%20Banaei%20and%20Isabella%20W.%20Martin%20and%20Saeed%20Hassanpour%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20new%20framework%20for%20the%20artificial%0Aintelligence-assisted%20characterization%20of%20Gram-stained%20whole-slide%20images%0A%28WSIs%29.%20As%20a%20test%20for%20the%20diagnosis%20of%20bloodstream%20infections%2C%20Gram%20stains%0Aprovide%20critical%20early%20data%20to%20inform%20patient%20treatment.%20Rapid%20and%20reliable%0Aanalysis%20of%20Gram%20stains%20has%20been%20shown%20to%20be%20positively%20associated%20with%20better%0Aclinical%20outcomes%2C%20underscoring%20the%20need%20for%20improved%20tools%20to%20automate%20Gram%0Astain%20analysis.%20In%20this%20work%2C%20we%20developed%20a%20novel%20transformer-based%20model%20for%0AGram-stained%20WSI%20classification%2C%20which%20is%20more%20scalable%20to%20large%20datasets%20than%0Aprevious%20convolutional%20neural%20network%20%28CNN%29%20-based%20methods%20as%20it%20does%20not%0Arequire%20patch-level%20manual%20annotations.%20We%20also%20introduce%20a%20large%20Gram%20stain%0Adataset%20from%20Dartmouth-Hitchcock%20Medical%20Center%20%28Lebanon%2C%20New%20Hampshire%2C%20USA%29%0Ato%20evaluate%20our%20model%2C%20exploring%20the%20classification%20of%20five%20major%20categories%20of%0AGram-stained%20WSIs%3A%20Gram-positive%20cocci%20in%20clusters%2C%20Gram-positive%20cocci%20in%0Apairs/chains%2C%20Gram-positive%20rods%2C%20Gram-negative%20rods%2C%20and%20slides%20with%20no%0Abacteria.%20Our%20model%20achieves%20a%20classification%20accuracy%20of%200.858%20%2895%25%20CI%3A%200.805%2C%0A0.905%29%20and%20an%20AUC%20of%200.952%20%2895%25%20CI%3A%200.922%2C%200.976%29%20using%20five-fold%20nested%0Across-validation%20on%20our%20475-slide%20dataset%2C%20demonstrating%20the%20potential%20of%0Alarge-scale%20transformer%20models%20for%20Gram%20stain%20classification.%20We%20further%0Ademonstrate%20the%20generalizability%20of%20our%20trained%20model%2C%20which%20achieves%20strong%0Aperformance%20on%20external%20datasets%20without%20additional%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Framework%2520for%2520the%2520Automated%2520Characterization%2520of%2520Gram-Stained%250A%2520%2520Blood%2520Culture%2520Slides%2520Using%2520a%2520Large-Scale%2520Vision%2520Transformer%26entry.906535625%3DJack%2520McMahon%2520and%2520Naofumi%2520Tomita%2520and%2520Elizabeth%2520S.%2520Tatishev%2520and%2520Adrienne%2520A.%2520Workman%2520and%2520Cristina%2520R%2520Costales%2520and%2520Niaz%2520Banaei%2520and%2520Isabella%2520W.%2520Martin%2520and%2520Saeed%2520Hassanpour%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520new%2520framework%2520for%2520the%2520artificial%250Aintelligence-assisted%2520characterization%2520of%2520Gram-stained%2520whole-slide%2520images%250A%2528WSIs%2529.%2520As%2520a%2520test%2520for%2520the%2520diagnosis%2520of%2520bloodstream%2520infections%252C%2520Gram%2520stains%250Aprovide%2520critical%2520early%2520data%2520to%2520inform%2520patient%2520treatment.%2520Rapid%2520and%2520reliable%250Aanalysis%2520of%2520Gram%2520stains%2520has%2520been%2520shown%2520to%2520be%2520positively%2520associated%2520with%2520better%250Aclinical%2520outcomes%252C%2520underscoring%2520the%2520need%2520for%2520improved%2520tools%2520to%2520automate%2520Gram%250Astain%2520analysis.%2520In%2520this%2520work%252C%2520we%2520developed%2520a%2520novel%2520transformer-based%2520model%2520for%250AGram-stained%2520WSI%2520classification%252C%2520which%2520is%2520more%2520scalable%2520to%2520large%2520datasets%2520than%250Aprevious%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520-based%2520methods%2520as%2520it%2520does%2520not%250Arequire%2520patch-level%2520manual%2520annotations.%2520We%2520also%2520introduce%2520a%2520large%2520Gram%2520stain%250Adataset%2520from%2520Dartmouth-Hitchcock%2520Medical%2520Center%2520%2528Lebanon%252C%2520New%2520Hampshire%252C%2520USA%2529%250Ato%2520evaluate%2520our%2520model%252C%2520exploring%2520the%2520classification%2520of%2520five%2520major%2520categories%2520of%250AGram-stained%2520WSIs%253A%2520Gram-positive%2520cocci%2520in%2520clusters%252C%2520Gram-positive%2520cocci%2520in%250Apairs/chains%252C%2520Gram-positive%2520rods%252C%2520Gram-negative%2520rods%252C%2520and%2520slides%2520with%2520no%250Abacteria.%2520Our%2520model%2520achieves%2520a%2520classification%2520accuracy%2520of%25200.858%2520%252895%2525%2520CI%253A%25200.805%252C%250A0.905%2529%2520and%2520an%2520AUC%2520of%25200.952%2520%252895%2525%2520CI%253A%25200.922%252C%25200.976%2529%2520using%2520five-fold%2520nested%250Across-validation%2520on%2520our%2520475-slide%2520dataset%252C%2520demonstrating%2520the%2520potential%2520of%250Alarge-scale%2520transformer%2520models%2520for%2520Gram%2520stain%2520classification.%2520We%2520further%250Ademonstrate%2520the%2520generalizability%2520of%2520our%2520trained%2520model%252C%2520which%2520achieves%2520strong%250Aperformance%2520on%2520external%2520datasets%2520without%2520additional%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Framework%20for%20the%20Automated%20Characterization%20of%20Gram-Stained%0A%20%20Blood%20Culture%20Slides%20Using%20a%20Large-Scale%20Vision%20Transformer&entry.906535625=Jack%20McMahon%20and%20Naofumi%20Tomita%20and%20Elizabeth%20S.%20Tatishev%20and%20Adrienne%20A.%20Workman%20and%20Cristina%20R%20Costales%20and%20Niaz%20Banaei%20and%20Isabella%20W.%20Martin%20and%20Saeed%20Hassanpour&entry.1292438233=%20%20This%20study%20introduces%20a%20new%20framework%20for%20the%20artificial%0Aintelligence-assisted%20characterization%20of%20Gram-stained%20whole-slide%20images%0A%28WSIs%29.%20As%20a%20test%20for%20the%20diagnosis%20of%20bloodstream%20infections%2C%20Gram%20stains%0Aprovide%20critical%20early%20data%20to%20inform%20patient%20treatment.%20Rapid%20and%20reliable%0Aanalysis%20of%20Gram%20stains%20has%20been%20shown%20to%20be%20positively%20associated%20with%20better%0Aclinical%20outcomes%2C%20underscoring%20the%20need%20for%20improved%20tools%20to%20automate%20Gram%0Astain%20analysis.%20In%20this%20work%2C%20we%20developed%20a%20novel%20transformer-based%20model%20for%0AGram-stained%20WSI%20classification%2C%20which%20is%20more%20scalable%20to%20large%20datasets%20than%0Aprevious%20convolutional%20neural%20network%20%28CNN%29%20-based%20methods%20as%20it%20does%20not%0Arequire%20patch-level%20manual%20annotations.%20We%20also%20introduce%20a%20large%20Gram%20stain%0Adataset%20from%20Dartmouth-Hitchcock%20Medical%20Center%20%28Lebanon%2C%20New%20Hampshire%2C%20USA%29%0Ato%20evaluate%20our%20model%2C%20exploring%20the%20classification%20of%20five%20major%20categories%20of%0AGram-stained%20WSIs%3A%20Gram-positive%20cocci%20in%20clusters%2C%20Gram-positive%20cocci%20in%0Apairs/chains%2C%20Gram-positive%20rods%2C%20Gram-negative%20rods%2C%20and%20slides%20with%20no%0Abacteria.%20Our%20model%20achieves%20a%20classification%20accuracy%20of%200.858%20%2895%25%20CI%3A%200.805%2C%0A0.905%29%20and%20an%20AUC%20of%200.952%20%2895%25%20CI%3A%200.922%2C%200.976%29%20using%20five-fold%20nested%0Across-validation%20on%20our%20475-slide%20dataset%2C%20demonstrating%20the%20potential%20of%0Alarge-scale%20transformer%20models%20for%20Gram%20stain%20classification.%20We%20further%0Ademonstrate%20the%20generalizability%20of%20our%20trained%20model%2C%20which%20achieves%20strong%0Aperformance%20on%20external%20datasets%20without%20additional%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15546v2&entry.124074799=Read"},
{"title": "Image-guided topic modeling for interpretable privacy classification", "author": "Alina Elena Baia and Andrea Cavallaro", "abstract": "  Predicting and explaining the private information contained in an image in\nhuman-understandable terms is a complex and contextual task. This task is\nchallenging even for large language models. To facilitate the understanding of\nprivacy decisions, we propose to predict image privacy based on a set of\nnatural language content descriptors. These content descriptors are associated\nwith privacy scores that reflect how people perceive image content. We generate\ndescriptors with our novel Image-guided Topic Modeling (ITM) approach. ITM\nleverages, via multimodality alignment, both vision information and image\ntextual descriptions from a vision language model. We use the ITM-generated\ndescriptors to learn a privacy predictor, Priv$\\times$ITM, whose decisions are\ninterpretable by design. Our Priv$\\times$ITM classifier outperforms the\nreference interpretable method by 5 percentage points in accuracy and performs\ncomparably to the current non-interpretable state-of-the-art model.\n", "link": "http://arxiv.org/abs/2409.18674v1", "date": "2024-09-27", "relevancy": 2.0798, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5217}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-guided%20topic%20modeling%20for%20interpretable%20privacy%20classification&body=Title%3A%20Image-guided%20topic%20modeling%20for%20interpretable%20privacy%20classification%0AAuthor%3A%20Alina%20Elena%20Baia%20and%20Andrea%20Cavallaro%0AAbstract%3A%20%20%20Predicting%20and%20explaining%20the%20private%20information%20contained%20in%20an%20image%20in%0Ahuman-understandable%20terms%20is%20a%20complex%20and%20contextual%20task.%20This%20task%20is%0Achallenging%20even%20for%20large%20language%20models.%20To%20facilitate%20the%20understanding%20of%0Aprivacy%20decisions%2C%20we%20propose%20to%20predict%20image%20privacy%20based%20on%20a%20set%20of%0Anatural%20language%20content%20descriptors.%20These%20content%20descriptors%20are%20associated%0Awith%20privacy%20scores%20that%20reflect%20how%20people%20perceive%20image%20content.%20We%20generate%0Adescriptors%20with%20our%20novel%20Image-guided%20Topic%20Modeling%20%28ITM%29%20approach.%20ITM%0Aleverages%2C%20via%20multimodality%20alignment%2C%20both%20vision%20information%20and%20image%0Atextual%20descriptions%20from%20a%20vision%20language%20model.%20We%20use%20the%20ITM-generated%0Adescriptors%20to%20learn%20a%20privacy%20predictor%2C%20Priv%24%5Ctimes%24ITM%2C%20whose%20decisions%20are%0Ainterpretable%20by%20design.%20Our%20Priv%24%5Ctimes%24ITM%20classifier%20outperforms%20the%0Areference%20interpretable%20method%20by%205%20percentage%20points%20in%20accuracy%20and%20performs%0Acomparably%20to%20the%20current%20non-interpretable%20state-of-the-art%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-guided%2520topic%2520modeling%2520for%2520interpretable%2520privacy%2520classification%26entry.906535625%3DAlina%2520Elena%2520Baia%2520and%2520Andrea%2520Cavallaro%26entry.1292438233%3D%2520%2520Predicting%2520and%2520explaining%2520the%2520private%2520information%2520contained%2520in%2520an%2520image%2520in%250Ahuman-understandable%2520terms%2520is%2520a%2520complex%2520and%2520contextual%2520task.%2520This%2520task%2520is%250Achallenging%2520even%2520for%2520large%2520language%2520models.%2520To%2520facilitate%2520the%2520understanding%2520of%250Aprivacy%2520decisions%252C%2520we%2520propose%2520to%2520predict%2520image%2520privacy%2520based%2520on%2520a%2520set%2520of%250Anatural%2520language%2520content%2520descriptors.%2520These%2520content%2520descriptors%2520are%2520associated%250Awith%2520privacy%2520scores%2520that%2520reflect%2520how%2520people%2520perceive%2520image%2520content.%2520We%2520generate%250Adescriptors%2520with%2520our%2520novel%2520Image-guided%2520Topic%2520Modeling%2520%2528ITM%2529%2520approach.%2520ITM%250Aleverages%252C%2520via%2520multimodality%2520alignment%252C%2520both%2520vision%2520information%2520and%2520image%250Atextual%2520descriptions%2520from%2520a%2520vision%2520language%2520model.%2520We%2520use%2520the%2520ITM-generated%250Adescriptors%2520to%2520learn%2520a%2520privacy%2520predictor%252C%2520Priv%2524%255Ctimes%2524ITM%252C%2520whose%2520decisions%2520are%250Ainterpretable%2520by%2520design.%2520Our%2520Priv%2524%255Ctimes%2524ITM%2520classifier%2520outperforms%2520the%250Areference%2520interpretable%2520method%2520by%25205%2520percentage%2520points%2520in%2520accuracy%2520and%2520performs%250Acomparably%2520to%2520the%2520current%2520non-interpretable%2520state-of-the-art%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-guided%20topic%20modeling%20for%20interpretable%20privacy%20classification&entry.906535625=Alina%20Elena%20Baia%20and%20Andrea%20Cavallaro&entry.1292438233=%20%20Predicting%20and%20explaining%20the%20private%20information%20contained%20in%20an%20image%20in%0Ahuman-understandable%20terms%20is%20a%20complex%20and%20contextual%20task.%20This%20task%20is%0Achallenging%20even%20for%20large%20language%20models.%20To%20facilitate%20the%20understanding%20of%0Aprivacy%20decisions%2C%20we%20propose%20to%20predict%20image%20privacy%20based%20on%20a%20set%20of%0Anatural%20language%20content%20descriptors.%20These%20content%20descriptors%20are%20associated%0Awith%20privacy%20scores%20that%20reflect%20how%20people%20perceive%20image%20content.%20We%20generate%0Adescriptors%20with%20our%20novel%20Image-guided%20Topic%20Modeling%20%28ITM%29%20approach.%20ITM%0Aleverages%2C%20via%20multimodality%20alignment%2C%20both%20vision%20information%20and%20image%0Atextual%20descriptions%20from%20a%20vision%20language%20model.%20We%20use%20the%20ITM-generated%0Adescriptors%20to%20learn%20a%20privacy%20predictor%2C%20Priv%24%5Ctimes%24ITM%2C%20whose%20decisions%20are%0Ainterpretable%20by%20design.%20Our%20Priv%24%5Ctimes%24ITM%20classifier%20outperforms%20the%0Areference%20interpretable%20method%20by%205%20percentage%20points%20in%20accuracy%20and%20performs%0Acomparably%20to%20the%20current%20non-interpretable%20state-of-the-art%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18674v1&entry.124074799=Read"},
{"title": "Universal Trajectory Optimization Framework for Differential Drive Robot\n  Class", "author": "Mengke Zhang and Nanhe Chen and Hu Wang and Jianxiong Qiu and Zhichao Han and Qiuyu Ren and Chao Xu and Fei Gao and Yanjun Cao", "abstract": "  Differential drive robots are widely used in various scenarios thanks to\ntheir straightforward principle, from household service robots to disaster\nresponse field robots. There are several types of driving mechanisms for\nreal-world applications, including two-wheeled, four-wheeled skid-steering,\ntracked robots, and so on. The differences in the driving mechanisms usually\nrequire specific kinematic modeling when precise control is desired.\nFurthermore, the nonholonomic dynamics and possible lateral slip lead to\ndifferent degrees of difficulty in getting feasible and high-quality\ntrajectories. Therefore, a comprehensive trajectory optimization framework to\ncompute trajectories efficiently for various kinds of differential drive robots\nis highly desirable. In this paper, we propose a universal trajectory\noptimization framework that can be applied to differential drive robots,\nenabling the generation of high-quality trajectories within a restricted\ncomputational timeframe. We introduce a novel trajectory representation based\non polynomial parameterization of motion states or their integrals, such as\nangular and linear velocities, which inherently matches the robots' motion to\nthe control principle. The trajectory optimization problem is formulated to\nminimize complexity while prioritizing safety and operational efficiency. We\nthen build a full-stack autonomous planning and control system to demonstrate\nits feasibility and robustness. We conduct extensive simulations and real-world\ntesting in crowded environments with three kinds of differential drive robots\nto validate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2409.07924v2", "date": "2024-09-27", "relevancy": 2.0789, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5365}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.534}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Trajectory%20Optimization%20Framework%20for%20Differential%20Drive%20Robot%0A%20%20Class&body=Title%3A%20Universal%20Trajectory%20Optimization%20Framework%20for%20Differential%20Drive%20Robot%0A%20%20Class%0AAuthor%3A%20Mengke%20Zhang%20and%20Nanhe%20Chen%20and%20Hu%20Wang%20and%20Jianxiong%20Qiu%20and%20Zhichao%20Han%20and%20Qiuyu%20Ren%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao%0AAbstract%3A%20%20%20Differential%20drive%20robots%20are%20widely%20used%20in%20various%20scenarios%20thanks%20to%0Atheir%20straightforward%20principle%2C%20from%20household%20service%20robots%20to%20disaster%0Aresponse%20field%20robots.%20There%20are%20several%20types%20of%20driving%20mechanisms%20for%0Areal-world%20applications%2C%20including%20two-wheeled%2C%20four-wheeled%20skid-steering%2C%0Atracked%20robots%2C%20and%20so%20on.%20The%20differences%20in%20the%20driving%20mechanisms%20usually%0Arequire%20specific%20kinematic%20modeling%20when%20precise%20control%20is%20desired.%0AFurthermore%2C%20the%20nonholonomic%20dynamics%20and%20possible%20lateral%20slip%20lead%20to%0Adifferent%20degrees%20of%20difficulty%20in%20getting%20feasible%20and%20high-quality%0Atrajectories.%20Therefore%2C%20a%20comprehensive%20trajectory%20optimization%20framework%20to%0Acompute%20trajectories%20efficiently%20for%20various%20kinds%20of%20differential%20drive%20robots%0Ais%20highly%20desirable.%20In%20this%20paper%2C%20we%20propose%20a%20universal%20trajectory%0Aoptimization%20framework%20that%20can%20be%20applied%20to%20differential%20drive%20robots%2C%0Aenabling%20the%20generation%20of%20high-quality%20trajectories%20within%20a%20restricted%0Acomputational%20timeframe.%20We%20introduce%20a%20novel%20trajectory%20representation%20based%0Aon%20polynomial%20parameterization%20of%20motion%20states%20or%20their%20integrals%2C%20such%20as%0Aangular%20and%20linear%20velocities%2C%20which%20inherently%20matches%20the%20robots%27%20motion%20to%0Athe%20control%20principle.%20The%20trajectory%20optimization%20problem%20is%20formulated%20to%0Aminimize%20complexity%20while%20prioritizing%20safety%20and%20operational%20efficiency.%20We%0Athen%20build%20a%20full-stack%20autonomous%20planning%20and%20control%20system%20to%20demonstrate%0Aits%20feasibility%20and%20robustness.%20We%20conduct%20extensive%20simulations%20and%20real-world%0Atesting%20in%20crowded%20environments%20with%20three%20kinds%20of%20differential%20drive%20robots%0Ato%20validate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07924v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Trajectory%2520Optimization%2520Framework%2520for%2520Differential%2520Drive%2520Robot%250A%2520%2520Class%26entry.906535625%3DMengke%2520Zhang%2520and%2520Nanhe%2520Chen%2520and%2520Hu%2520Wang%2520and%2520Jianxiong%2520Qiu%2520and%2520Zhichao%2520Han%2520and%2520Qiuyu%2520Ren%2520and%2520Chao%2520Xu%2520and%2520Fei%2520Gao%2520and%2520Yanjun%2520Cao%26entry.1292438233%3D%2520%2520Differential%2520drive%2520robots%2520are%2520widely%2520used%2520in%2520various%2520scenarios%2520thanks%2520to%250Atheir%2520straightforward%2520principle%252C%2520from%2520household%2520service%2520robots%2520to%2520disaster%250Aresponse%2520field%2520robots.%2520There%2520are%2520several%2520types%2520of%2520driving%2520mechanisms%2520for%250Areal-world%2520applications%252C%2520including%2520two-wheeled%252C%2520four-wheeled%2520skid-steering%252C%250Atracked%2520robots%252C%2520and%2520so%2520on.%2520The%2520differences%2520in%2520the%2520driving%2520mechanisms%2520usually%250Arequire%2520specific%2520kinematic%2520modeling%2520when%2520precise%2520control%2520is%2520desired.%250AFurthermore%252C%2520the%2520nonholonomic%2520dynamics%2520and%2520possible%2520lateral%2520slip%2520lead%2520to%250Adifferent%2520degrees%2520of%2520difficulty%2520in%2520getting%2520feasible%2520and%2520high-quality%250Atrajectories.%2520Therefore%252C%2520a%2520comprehensive%2520trajectory%2520optimization%2520framework%2520to%250Acompute%2520trajectories%2520efficiently%2520for%2520various%2520kinds%2520of%2520differential%2520drive%2520robots%250Ais%2520highly%2520desirable.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520universal%2520trajectory%250Aoptimization%2520framework%2520that%2520can%2520be%2520applied%2520to%2520differential%2520drive%2520robots%252C%250Aenabling%2520the%2520generation%2520of%2520high-quality%2520trajectories%2520within%2520a%2520restricted%250Acomputational%2520timeframe.%2520We%2520introduce%2520a%2520novel%2520trajectory%2520representation%2520based%250Aon%2520polynomial%2520parameterization%2520of%2520motion%2520states%2520or%2520their%2520integrals%252C%2520such%2520as%250Aangular%2520and%2520linear%2520velocities%252C%2520which%2520inherently%2520matches%2520the%2520robots%2527%2520motion%2520to%250Athe%2520control%2520principle.%2520The%2520trajectory%2520optimization%2520problem%2520is%2520formulated%2520to%250Aminimize%2520complexity%2520while%2520prioritizing%2520safety%2520and%2520operational%2520efficiency.%2520We%250Athen%2520build%2520a%2520full-stack%2520autonomous%2520planning%2520and%2520control%2520system%2520to%2520demonstrate%250Aits%2520feasibility%2520and%2520robustness.%2520We%2520conduct%2520extensive%2520simulations%2520and%2520real-world%250Atesting%2520in%2520crowded%2520environments%2520with%2520three%2520kinds%2520of%2520differential%2520drive%2520robots%250Ato%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07924v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Trajectory%20Optimization%20Framework%20for%20Differential%20Drive%20Robot%0A%20%20Class&entry.906535625=Mengke%20Zhang%20and%20Nanhe%20Chen%20and%20Hu%20Wang%20and%20Jianxiong%20Qiu%20and%20Zhichao%20Han%20and%20Qiuyu%20Ren%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao&entry.1292438233=%20%20Differential%20drive%20robots%20are%20widely%20used%20in%20various%20scenarios%20thanks%20to%0Atheir%20straightforward%20principle%2C%20from%20household%20service%20robots%20to%20disaster%0Aresponse%20field%20robots.%20There%20are%20several%20types%20of%20driving%20mechanisms%20for%0Areal-world%20applications%2C%20including%20two-wheeled%2C%20four-wheeled%20skid-steering%2C%0Atracked%20robots%2C%20and%20so%20on.%20The%20differences%20in%20the%20driving%20mechanisms%20usually%0Arequire%20specific%20kinematic%20modeling%20when%20precise%20control%20is%20desired.%0AFurthermore%2C%20the%20nonholonomic%20dynamics%20and%20possible%20lateral%20slip%20lead%20to%0Adifferent%20degrees%20of%20difficulty%20in%20getting%20feasible%20and%20high-quality%0Atrajectories.%20Therefore%2C%20a%20comprehensive%20trajectory%20optimization%20framework%20to%0Acompute%20trajectories%20efficiently%20for%20various%20kinds%20of%20differential%20drive%20robots%0Ais%20highly%20desirable.%20In%20this%20paper%2C%20we%20propose%20a%20universal%20trajectory%0Aoptimization%20framework%20that%20can%20be%20applied%20to%20differential%20drive%20robots%2C%0Aenabling%20the%20generation%20of%20high-quality%20trajectories%20within%20a%20restricted%0Acomputational%20timeframe.%20We%20introduce%20a%20novel%20trajectory%20representation%20based%0Aon%20polynomial%20parameterization%20of%20motion%20states%20or%20their%20integrals%2C%20such%20as%0Aangular%20and%20linear%20velocities%2C%20which%20inherently%20matches%20the%20robots%27%20motion%20to%0Athe%20control%20principle.%20The%20trajectory%20optimization%20problem%20is%20formulated%20to%0Aminimize%20complexity%20while%20prioritizing%20safety%20and%20operational%20efficiency.%20We%0Athen%20build%20a%20full-stack%20autonomous%20planning%20and%20control%20system%20to%20demonstrate%0Aits%20feasibility%20and%20robustness.%20We%20conduct%20extensive%20simulations%20and%20real-world%0Atesting%20in%20crowded%20environments%20with%20three%20kinds%20of%20differential%20drive%20robots%0Ato%20validate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07924v2&entry.124074799=Read"},
{"title": "Two Sparse Matrices are Better than One: Sparsifying Neural Networks\n  with Double Sparse Factorization", "author": "Vladim\u00edr Bo\u017ea and Vladim\u00edr Macko", "abstract": "  Neural networks are often challenging to work with due to their large size\nand complexity. To address this, various methods aim to reduce model size by\nsparsifying or decomposing weight matrices, such as magnitude pruning and\nlow-rank or block-diagonal factorization. In this work, we present Double\nSparse Factorization (DSF), where we factorize each weight matrix into two\nsparse matrices. Although solving this problem exactly is computationally\ninfeasible, we propose an efficient heuristic based on alternating minimization\nvia ADMM that achieves state-of-the-art results, enabling unprecedented\nsparsification of neural networks. For instance, in a one-shot pruning setting,\nour method can reduce the size of the LLaMA2-13B model by 50% while maintaining\nbetter performance than the dense LLaMA2-7B model. We also compare favorably\nwith Optimal Brain Compression, the state-of-the-art layer-wise pruning\napproach for convolutional neural networks. Furthermore, accuracy improvements\nof our method persist even after further model fine-tuning.\n  Code available at: https://github.com/usamec/double_sparse.\n", "link": "http://arxiv.org/abs/2409.18850v1", "date": "2024-09-27", "relevancy": 2.0752, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5458}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.519}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Sparse%20Matrices%20are%20Better%20than%20One%3A%20Sparsifying%20Neural%20Networks%0A%20%20with%20Double%20Sparse%20Factorization&body=Title%3A%20Two%20Sparse%20Matrices%20are%20Better%20than%20One%3A%20Sparsifying%20Neural%20Networks%0A%20%20with%20Double%20Sparse%20Factorization%0AAuthor%3A%20Vladim%C3%ADr%20Bo%C5%BEa%20and%20Vladim%C3%ADr%20Macko%0AAbstract%3A%20%20%20Neural%20networks%20are%20often%20challenging%20to%20work%20with%20due%20to%20their%20large%20size%0Aand%20complexity.%20To%20address%20this%2C%20various%20methods%20aim%20to%20reduce%20model%20size%20by%0Asparsifying%20or%20decomposing%20weight%20matrices%2C%20such%20as%20magnitude%20pruning%20and%0Alow-rank%20or%20block-diagonal%20factorization.%20In%20this%20work%2C%20we%20present%20Double%0ASparse%20Factorization%20%28DSF%29%2C%20where%20we%20factorize%20each%20weight%20matrix%20into%20two%0Asparse%20matrices.%20Although%20solving%20this%20problem%20exactly%20is%20computationally%0Ainfeasible%2C%20we%20propose%20an%20efficient%20heuristic%20based%20on%20alternating%20minimization%0Avia%20ADMM%20that%20achieves%20state-of-the-art%20results%2C%20enabling%20unprecedented%0Asparsification%20of%20neural%20networks.%20For%20instance%2C%20in%20a%20one-shot%20pruning%20setting%2C%0Aour%20method%20can%20reduce%20the%20size%20of%20the%20LLaMA2-13B%20model%20by%2050%25%20while%20maintaining%0Abetter%20performance%20than%20the%20dense%20LLaMA2-7B%20model.%20We%20also%20compare%20favorably%0Awith%20Optimal%20Brain%20Compression%2C%20the%20state-of-the-art%20layer-wise%20pruning%0Aapproach%20for%20convolutional%20neural%20networks.%20Furthermore%2C%20accuracy%20improvements%0Aof%20our%20method%20persist%20even%20after%20further%20model%20fine-tuning.%0A%20%20Code%20available%20at%3A%20https%3A//github.com/usamec/double_sparse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Sparse%2520Matrices%2520are%2520Better%2520than%2520One%253A%2520Sparsifying%2520Neural%2520Networks%250A%2520%2520with%2520Double%2520Sparse%2520Factorization%26entry.906535625%3DVladim%25C3%25ADr%2520Bo%25C5%25BEa%2520and%2520Vladim%25C3%25ADr%2520Macko%26entry.1292438233%3D%2520%2520Neural%2520networks%2520are%2520often%2520challenging%2520to%2520work%2520with%2520due%2520to%2520their%2520large%2520size%250Aand%2520complexity.%2520To%2520address%2520this%252C%2520various%2520methods%2520aim%2520to%2520reduce%2520model%2520size%2520by%250Asparsifying%2520or%2520decomposing%2520weight%2520matrices%252C%2520such%2520as%2520magnitude%2520pruning%2520and%250Alow-rank%2520or%2520block-diagonal%2520factorization.%2520In%2520this%2520work%252C%2520we%2520present%2520Double%250ASparse%2520Factorization%2520%2528DSF%2529%252C%2520where%2520we%2520factorize%2520each%2520weight%2520matrix%2520into%2520two%250Asparse%2520matrices.%2520Although%2520solving%2520this%2520problem%2520exactly%2520is%2520computationally%250Ainfeasible%252C%2520we%2520propose%2520an%2520efficient%2520heuristic%2520based%2520on%2520alternating%2520minimization%250Avia%2520ADMM%2520that%2520achieves%2520state-of-the-art%2520results%252C%2520enabling%2520unprecedented%250Asparsification%2520of%2520neural%2520networks.%2520For%2520instance%252C%2520in%2520a%2520one-shot%2520pruning%2520setting%252C%250Aour%2520method%2520can%2520reduce%2520the%2520size%2520of%2520the%2520LLaMA2-13B%2520model%2520by%252050%2525%2520while%2520maintaining%250Abetter%2520performance%2520than%2520the%2520dense%2520LLaMA2-7B%2520model.%2520We%2520also%2520compare%2520favorably%250Awith%2520Optimal%2520Brain%2520Compression%252C%2520the%2520state-of-the-art%2520layer-wise%2520pruning%250Aapproach%2520for%2520convolutional%2520neural%2520networks.%2520Furthermore%252C%2520accuracy%2520improvements%250Aof%2520our%2520method%2520persist%2520even%2520after%2520further%2520model%2520fine-tuning.%250A%2520%2520Code%2520available%2520at%253A%2520https%253A//github.com/usamec/double_sparse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Sparse%20Matrices%20are%20Better%20than%20One%3A%20Sparsifying%20Neural%20Networks%0A%20%20with%20Double%20Sparse%20Factorization&entry.906535625=Vladim%C3%ADr%20Bo%C5%BEa%20and%20Vladim%C3%ADr%20Macko&entry.1292438233=%20%20Neural%20networks%20are%20often%20challenging%20to%20work%20with%20due%20to%20their%20large%20size%0Aand%20complexity.%20To%20address%20this%2C%20various%20methods%20aim%20to%20reduce%20model%20size%20by%0Asparsifying%20or%20decomposing%20weight%20matrices%2C%20such%20as%20magnitude%20pruning%20and%0Alow-rank%20or%20block-diagonal%20factorization.%20In%20this%20work%2C%20we%20present%20Double%0ASparse%20Factorization%20%28DSF%29%2C%20where%20we%20factorize%20each%20weight%20matrix%20into%20two%0Asparse%20matrices.%20Although%20solving%20this%20problem%20exactly%20is%20computationally%0Ainfeasible%2C%20we%20propose%20an%20efficient%20heuristic%20based%20on%20alternating%20minimization%0Avia%20ADMM%20that%20achieves%20state-of-the-art%20results%2C%20enabling%20unprecedented%0Asparsification%20of%20neural%20networks.%20For%20instance%2C%20in%20a%20one-shot%20pruning%20setting%2C%0Aour%20method%20can%20reduce%20the%20size%20of%20the%20LLaMA2-13B%20model%20by%2050%25%20while%20maintaining%0Abetter%20performance%20than%20the%20dense%20LLaMA2-7B%20model.%20We%20also%20compare%20favorably%0Awith%20Optimal%20Brain%20Compression%2C%20the%20state-of-the-art%20layer-wise%20pruning%0Aapproach%20for%20convolutional%20neural%20networks.%20Furthermore%2C%20accuracy%20improvements%0Aof%20our%20method%20persist%20even%20after%20further%20model%20fine-tuning.%0A%20%20Code%20available%20at%3A%20https%3A//github.com/usamec/double_sparse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18850v1&entry.124074799=Read"},
{"title": "A preliminary study on continual learning in computer vision using\n  Kolmogorov-Arnold Networks", "author": "Alessandro Cacciatore and Valerio Morelli and Federica Paganica and Emanuele Frontoni and Lucia Migliorelli and Daniele Berardini", "abstract": "  Deep learning has long been dominated by multi-layer perceptrons (MLPs),\nwhich have demonstrated superiority over other optimizable models in various\ndomains. Recently, a new alternative to MLPs has emerged - Kolmogorov-Arnold\nNetworks (KAN)- which are based on a fundamentally different mathematical\nframework. According to their authors, KANs address several major issues in\nMLPs, such as catastrophic forgetting in continual learning scenarios. However,\nthis claim has only been supported by results from a regression task on a toy\n1D dataset. In this paper, we extend the investigation by evaluating the\nperformance of KANs in continual learning tasks within computer vision,\nspecifically using the MNIST datasets. To this end, we conduct a structured\nanalysis of the behavior of MLPs and two KAN-based models in a\nclass-incremental learning scenario, ensuring that the architectures involved\nhave the same number of trainable parameters. Our results demonstrate that an\nefficient version of KAN outperforms both traditional MLPs and the original KAN\nimplementation. We further analyze the influence of hyperparameters in MLPs and\nKANs, as well as the impact of certain trainable parameters in KANs, such as\nbias and scale weights. Additionally, we provide a preliminary investigation of\nrecent KAN-based convolutional networks and compare their performance with that\nof traditional convolutional neural networks. Our codes can be found at\nhttps://github.com/MrPio/KAN-Continual_Learning_tests.\n", "link": "http://arxiv.org/abs/2409.13550v2", "date": "2024-09-27", "relevancy": 2.07, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5293}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20preliminary%20study%20on%20continual%20learning%20in%20computer%20vision%20using%0A%20%20Kolmogorov-Arnold%20Networks&body=Title%3A%20A%20preliminary%20study%20on%20continual%20learning%20in%20computer%20vision%20using%0A%20%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Alessandro%20Cacciatore%20and%20Valerio%20Morelli%20and%20Federica%20Paganica%20and%20Emanuele%20Frontoni%20and%20Lucia%20Migliorelli%20and%20Daniele%20Berardini%0AAbstract%3A%20%20%20Deep%20learning%20has%20long%20been%20dominated%20by%20multi-layer%20perceptrons%20%28MLPs%29%2C%0Awhich%20have%20demonstrated%20superiority%20over%20other%20optimizable%20models%20in%20various%0Adomains.%20Recently%2C%20a%20new%20alternative%20to%20MLPs%20has%20emerged%20-%20Kolmogorov-Arnold%0ANetworks%20%28KAN%29-%20which%20are%20based%20on%20a%20fundamentally%20different%20mathematical%0Aframework.%20According%20to%20their%20authors%2C%20KANs%20address%20several%20major%20issues%20in%0AMLPs%2C%20such%20as%20catastrophic%20forgetting%20in%20continual%20learning%20scenarios.%20However%2C%0Athis%20claim%20has%20only%20been%20supported%20by%20results%20from%20a%20regression%20task%20on%20a%20toy%0A1D%20dataset.%20In%20this%20paper%2C%20we%20extend%20the%20investigation%20by%20evaluating%20the%0Aperformance%20of%20KANs%20in%20continual%20learning%20tasks%20within%20computer%20vision%2C%0Aspecifically%20using%20the%20MNIST%20datasets.%20To%20this%20end%2C%20we%20conduct%20a%20structured%0Aanalysis%20of%20the%20behavior%20of%20MLPs%20and%20two%20KAN-based%20models%20in%20a%0Aclass-incremental%20learning%20scenario%2C%20ensuring%20that%20the%20architectures%20involved%0Ahave%20the%20same%20number%20of%20trainable%20parameters.%20Our%20results%20demonstrate%20that%20an%0Aefficient%20version%20of%20KAN%20outperforms%20both%20traditional%20MLPs%20and%20the%20original%20KAN%0Aimplementation.%20We%20further%20analyze%20the%20influence%20of%20hyperparameters%20in%20MLPs%20and%0AKANs%2C%20as%20well%20as%20the%20impact%20of%20certain%20trainable%20parameters%20in%20KANs%2C%20such%20as%0Abias%20and%20scale%20weights.%20Additionally%2C%20we%20provide%20a%20preliminary%20investigation%20of%0Arecent%20KAN-based%20convolutional%20networks%20and%20compare%20their%20performance%20with%20that%0Aof%20traditional%20convolutional%20neural%20networks.%20Our%20codes%20can%20be%20found%20at%0Ahttps%3A//github.com/MrPio/KAN-Continual_Learning_tests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13550v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520preliminary%2520study%2520on%2520continual%2520learning%2520in%2520computer%2520vision%2520using%250A%2520%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DAlessandro%2520Cacciatore%2520and%2520Valerio%2520Morelli%2520and%2520Federica%2520Paganica%2520and%2520Emanuele%2520Frontoni%2520and%2520Lucia%2520Migliorelli%2520and%2520Daniele%2520Berardini%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520long%2520been%2520dominated%2520by%2520multi-layer%2520perceptrons%2520%2528MLPs%2529%252C%250Awhich%2520have%2520demonstrated%2520superiority%2520over%2520other%2520optimizable%2520models%2520in%2520various%250Adomains.%2520Recently%252C%2520a%2520new%2520alternative%2520to%2520MLPs%2520has%2520emerged%2520-%2520Kolmogorov-Arnold%250ANetworks%2520%2528KAN%2529-%2520which%2520are%2520based%2520on%2520a%2520fundamentally%2520different%2520mathematical%250Aframework.%2520According%2520to%2520their%2520authors%252C%2520KANs%2520address%2520several%2520major%2520issues%2520in%250AMLPs%252C%2520such%2520as%2520catastrophic%2520forgetting%2520in%2520continual%2520learning%2520scenarios.%2520However%252C%250Athis%2520claim%2520has%2520only%2520been%2520supported%2520by%2520results%2520from%2520a%2520regression%2520task%2520on%2520a%2520toy%250A1D%2520dataset.%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520investigation%2520by%2520evaluating%2520the%250Aperformance%2520of%2520KANs%2520in%2520continual%2520learning%2520tasks%2520within%2520computer%2520vision%252C%250Aspecifically%2520using%2520the%2520MNIST%2520datasets.%2520To%2520this%2520end%252C%2520we%2520conduct%2520a%2520structured%250Aanalysis%2520of%2520the%2520behavior%2520of%2520MLPs%2520and%2520two%2520KAN-based%2520models%2520in%2520a%250Aclass-incremental%2520learning%2520scenario%252C%2520ensuring%2520that%2520the%2520architectures%2520involved%250Ahave%2520the%2520same%2520number%2520of%2520trainable%2520parameters.%2520Our%2520results%2520demonstrate%2520that%2520an%250Aefficient%2520version%2520of%2520KAN%2520outperforms%2520both%2520traditional%2520MLPs%2520and%2520the%2520original%2520KAN%250Aimplementation.%2520We%2520further%2520analyze%2520the%2520influence%2520of%2520hyperparameters%2520in%2520MLPs%2520and%250AKANs%252C%2520as%2520well%2520as%2520the%2520impact%2520of%2520certain%2520trainable%2520parameters%2520in%2520KANs%252C%2520such%2520as%250Abias%2520and%2520scale%2520weights.%2520Additionally%252C%2520we%2520provide%2520a%2520preliminary%2520investigation%2520of%250Arecent%2520KAN-based%2520convolutional%2520networks%2520and%2520compare%2520their%2520performance%2520with%2520that%250Aof%2520traditional%2520convolutional%2520neural%2520networks.%2520Our%2520codes%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/MrPio/KAN-Continual_Learning_tests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13550v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20preliminary%20study%20on%20continual%20learning%20in%20computer%20vision%20using%0A%20%20Kolmogorov-Arnold%20Networks&entry.906535625=Alessandro%20Cacciatore%20and%20Valerio%20Morelli%20and%20Federica%20Paganica%20and%20Emanuele%20Frontoni%20and%20Lucia%20Migliorelli%20and%20Daniele%20Berardini&entry.1292438233=%20%20Deep%20learning%20has%20long%20been%20dominated%20by%20multi-layer%20perceptrons%20%28MLPs%29%2C%0Awhich%20have%20demonstrated%20superiority%20over%20other%20optimizable%20models%20in%20various%0Adomains.%20Recently%2C%20a%20new%20alternative%20to%20MLPs%20has%20emerged%20-%20Kolmogorov-Arnold%0ANetworks%20%28KAN%29-%20which%20are%20based%20on%20a%20fundamentally%20different%20mathematical%0Aframework.%20According%20to%20their%20authors%2C%20KANs%20address%20several%20major%20issues%20in%0AMLPs%2C%20such%20as%20catastrophic%20forgetting%20in%20continual%20learning%20scenarios.%20However%2C%0Athis%20claim%20has%20only%20been%20supported%20by%20results%20from%20a%20regression%20task%20on%20a%20toy%0A1D%20dataset.%20In%20this%20paper%2C%20we%20extend%20the%20investigation%20by%20evaluating%20the%0Aperformance%20of%20KANs%20in%20continual%20learning%20tasks%20within%20computer%20vision%2C%0Aspecifically%20using%20the%20MNIST%20datasets.%20To%20this%20end%2C%20we%20conduct%20a%20structured%0Aanalysis%20of%20the%20behavior%20of%20MLPs%20and%20two%20KAN-based%20models%20in%20a%0Aclass-incremental%20learning%20scenario%2C%20ensuring%20that%20the%20architectures%20involved%0Ahave%20the%20same%20number%20of%20trainable%20parameters.%20Our%20results%20demonstrate%20that%20an%0Aefficient%20version%20of%20KAN%20outperforms%20both%20traditional%20MLPs%20and%20the%20original%20KAN%0Aimplementation.%20We%20further%20analyze%20the%20influence%20of%20hyperparameters%20in%20MLPs%20and%0AKANs%2C%20as%20well%20as%20the%20impact%20of%20certain%20trainable%20parameters%20in%20KANs%2C%20such%20as%0Abias%20and%20scale%20weights.%20Additionally%2C%20we%20provide%20a%20preliminary%20investigation%20of%0Arecent%20KAN-based%20convolutional%20networks%20and%20compare%20their%20performance%20with%20that%0Aof%20traditional%20convolutional%20neural%20networks.%20Our%20codes%20can%20be%20found%20at%0Ahttps%3A//github.com/MrPio/KAN-Continual_Learning_tests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13550v2&entry.124074799=Read"},
{"title": "Improving Visual Object Tracking through Visual Prompting", "author": "Shih-Fang Chen and Jun-Cheng Chen and I-Hong Jhuo and Yen-Yu Lin", "abstract": "  Learning a discriminative model to distinguish a target from its surrounding\ndistractors is essential to generic visual object tracking. Dynamic target\nrepresentation adaptation against distractors is challenging due to the limited\ndiscriminative capabilities of prevailing trackers. We present a new visual\nPrompting mechanism for generic Visual Object Tracking (PiVOT) to address this\nissue. PiVOT proposes a prompt generation network with the pre-trained\nfoundation model CLIP to automatically generate and refine visual prompts,\nenabling the transfer of foundation model knowledge for tracking. While CLIP\noffers broad category-level knowledge, the tracker, trained on\ninstance-specific data, excels at recognizing unique object instances. Thus,\nPiVOT first compiles a visual prompt highlighting potential target locations.\nTo transfer the knowledge of CLIP to the tracker, PiVOT leverages CLIP to\nrefine the visual prompt based on the similarities between candidate objects\nand the reference templates across potential targets. Once the visual prompt is\nrefined, it can better highlight potential target locations, thereby reducing\nirrelevant prompt information. With the proposed prompting mechanism, the\ntracker can generate improved instance-aware feature maps through the guidance\nof the visual prompt, thus effectively reducing distractors. The proposed\nmethod does not involve CLIP during training, thereby keeping the same training\ncomplexity and preserving the generalization capability of the pretrained\nfoundation model. Extensive experiments across multiple benchmarks indicate\nthat PiVOT, using the proposed prompting method can suppress distracting\nobjects and enhance the tracker.\n", "link": "http://arxiv.org/abs/2409.18901v1", "date": "2024-09-27", "relevancy": 2.0631, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Visual%20Object%20Tracking%20through%20Visual%20Prompting&body=Title%3A%20Improving%20Visual%20Object%20Tracking%20through%20Visual%20Prompting%0AAuthor%3A%20Shih-Fang%20Chen%20and%20Jun-Cheng%20Chen%20and%20I-Hong%20Jhuo%20and%20Yen-Yu%20Lin%0AAbstract%3A%20%20%20Learning%20a%20discriminative%20model%20to%20distinguish%20a%20target%20from%20its%20surrounding%0Adistractors%20is%20essential%20to%20generic%20visual%20object%20tracking.%20Dynamic%20target%0Arepresentation%20adaptation%20against%20distractors%20is%20challenging%20due%20to%20the%20limited%0Adiscriminative%20capabilities%20of%20prevailing%20trackers.%20We%20present%20a%20new%20visual%0APrompting%20mechanism%20for%20generic%20Visual%20Object%20Tracking%20%28PiVOT%29%20to%20address%20this%0Aissue.%20PiVOT%20proposes%20a%20prompt%20generation%20network%20with%20the%20pre-trained%0Afoundation%20model%20CLIP%20to%20automatically%20generate%20and%20refine%20visual%20prompts%2C%0Aenabling%20the%20transfer%20of%20foundation%20model%20knowledge%20for%20tracking.%20While%20CLIP%0Aoffers%20broad%20category-level%20knowledge%2C%20the%20tracker%2C%20trained%20on%0Ainstance-specific%20data%2C%20excels%20at%20recognizing%20unique%20object%20instances.%20Thus%2C%0APiVOT%20first%20compiles%20a%20visual%20prompt%20highlighting%20potential%20target%20locations.%0ATo%20transfer%20the%20knowledge%20of%20CLIP%20to%20the%20tracker%2C%20PiVOT%20leverages%20CLIP%20to%0Arefine%20the%20visual%20prompt%20based%20on%20the%20similarities%20between%20candidate%20objects%0Aand%20the%20reference%20templates%20across%20potential%20targets.%20Once%20the%20visual%20prompt%20is%0Arefined%2C%20it%20can%20better%20highlight%20potential%20target%20locations%2C%20thereby%20reducing%0Airrelevant%20prompt%20information.%20With%20the%20proposed%20prompting%20mechanism%2C%20the%0Atracker%20can%20generate%20improved%20instance-aware%20feature%20maps%20through%20the%20guidance%0Aof%20the%20visual%20prompt%2C%20thus%20effectively%20reducing%20distractors.%20The%20proposed%0Amethod%20does%20not%20involve%20CLIP%20during%20training%2C%20thereby%20keeping%20the%20same%20training%0Acomplexity%20and%20preserving%20the%20generalization%20capability%20of%20the%20pretrained%0Afoundation%20model.%20Extensive%20experiments%20across%20multiple%20benchmarks%20indicate%0Athat%20PiVOT%2C%20using%20the%20proposed%20prompting%20method%20can%20suppress%20distracting%0Aobjects%20and%20enhance%20the%20tracker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Visual%2520Object%2520Tracking%2520through%2520Visual%2520Prompting%26entry.906535625%3DShih-Fang%2520Chen%2520and%2520Jun-Cheng%2520Chen%2520and%2520I-Hong%2520Jhuo%2520and%2520Yen-Yu%2520Lin%26entry.1292438233%3D%2520%2520Learning%2520a%2520discriminative%2520model%2520to%2520distinguish%2520a%2520target%2520from%2520its%2520surrounding%250Adistractors%2520is%2520essential%2520to%2520generic%2520visual%2520object%2520tracking.%2520Dynamic%2520target%250Arepresentation%2520adaptation%2520against%2520distractors%2520is%2520challenging%2520due%2520to%2520the%2520limited%250Adiscriminative%2520capabilities%2520of%2520prevailing%2520trackers.%2520We%2520present%2520a%2520new%2520visual%250APrompting%2520mechanism%2520for%2520generic%2520Visual%2520Object%2520Tracking%2520%2528PiVOT%2529%2520to%2520address%2520this%250Aissue.%2520PiVOT%2520proposes%2520a%2520prompt%2520generation%2520network%2520with%2520the%2520pre-trained%250Afoundation%2520model%2520CLIP%2520to%2520automatically%2520generate%2520and%2520refine%2520visual%2520prompts%252C%250Aenabling%2520the%2520transfer%2520of%2520foundation%2520model%2520knowledge%2520for%2520tracking.%2520While%2520CLIP%250Aoffers%2520broad%2520category-level%2520knowledge%252C%2520the%2520tracker%252C%2520trained%2520on%250Ainstance-specific%2520data%252C%2520excels%2520at%2520recognizing%2520unique%2520object%2520instances.%2520Thus%252C%250APiVOT%2520first%2520compiles%2520a%2520visual%2520prompt%2520highlighting%2520potential%2520target%2520locations.%250ATo%2520transfer%2520the%2520knowledge%2520of%2520CLIP%2520to%2520the%2520tracker%252C%2520PiVOT%2520leverages%2520CLIP%2520to%250Arefine%2520the%2520visual%2520prompt%2520based%2520on%2520the%2520similarities%2520between%2520candidate%2520objects%250Aand%2520the%2520reference%2520templates%2520across%2520potential%2520targets.%2520Once%2520the%2520visual%2520prompt%2520is%250Arefined%252C%2520it%2520can%2520better%2520highlight%2520potential%2520target%2520locations%252C%2520thereby%2520reducing%250Airrelevant%2520prompt%2520information.%2520With%2520the%2520proposed%2520prompting%2520mechanism%252C%2520the%250Atracker%2520can%2520generate%2520improved%2520instance-aware%2520feature%2520maps%2520through%2520the%2520guidance%250Aof%2520the%2520visual%2520prompt%252C%2520thus%2520effectively%2520reducing%2520distractors.%2520The%2520proposed%250Amethod%2520does%2520not%2520involve%2520CLIP%2520during%2520training%252C%2520thereby%2520keeping%2520the%2520same%2520training%250Acomplexity%2520and%2520preserving%2520the%2520generalization%2520capability%2520of%2520the%2520pretrained%250Afoundation%2520model.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520indicate%250Athat%2520PiVOT%252C%2520using%2520the%2520proposed%2520prompting%2520method%2520can%2520suppress%2520distracting%250Aobjects%2520and%2520enhance%2520the%2520tracker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Visual%20Object%20Tracking%20through%20Visual%20Prompting&entry.906535625=Shih-Fang%20Chen%20and%20Jun-Cheng%20Chen%20and%20I-Hong%20Jhuo%20and%20Yen-Yu%20Lin&entry.1292438233=%20%20Learning%20a%20discriminative%20model%20to%20distinguish%20a%20target%20from%20its%20surrounding%0Adistractors%20is%20essential%20to%20generic%20visual%20object%20tracking.%20Dynamic%20target%0Arepresentation%20adaptation%20against%20distractors%20is%20challenging%20due%20to%20the%20limited%0Adiscriminative%20capabilities%20of%20prevailing%20trackers.%20We%20present%20a%20new%20visual%0APrompting%20mechanism%20for%20generic%20Visual%20Object%20Tracking%20%28PiVOT%29%20to%20address%20this%0Aissue.%20PiVOT%20proposes%20a%20prompt%20generation%20network%20with%20the%20pre-trained%0Afoundation%20model%20CLIP%20to%20automatically%20generate%20and%20refine%20visual%20prompts%2C%0Aenabling%20the%20transfer%20of%20foundation%20model%20knowledge%20for%20tracking.%20While%20CLIP%0Aoffers%20broad%20category-level%20knowledge%2C%20the%20tracker%2C%20trained%20on%0Ainstance-specific%20data%2C%20excels%20at%20recognizing%20unique%20object%20instances.%20Thus%2C%0APiVOT%20first%20compiles%20a%20visual%20prompt%20highlighting%20potential%20target%20locations.%0ATo%20transfer%20the%20knowledge%20of%20CLIP%20to%20the%20tracker%2C%20PiVOT%20leverages%20CLIP%20to%0Arefine%20the%20visual%20prompt%20based%20on%20the%20similarities%20between%20candidate%20objects%0Aand%20the%20reference%20templates%20across%20potential%20targets.%20Once%20the%20visual%20prompt%20is%0Arefined%2C%20it%20can%20better%20highlight%20potential%20target%20locations%2C%20thereby%20reducing%0Airrelevant%20prompt%20information.%20With%20the%20proposed%20prompting%20mechanism%2C%20the%0Atracker%20can%20generate%20improved%20instance-aware%20feature%20maps%20through%20the%20guidance%0Aof%20the%20visual%20prompt%2C%20thus%20effectively%20reducing%20distractors.%20The%20proposed%0Amethod%20does%20not%20involve%20CLIP%20during%20training%2C%20thereby%20keeping%20the%20same%20training%0Acomplexity%20and%20preserving%20the%20generalization%20capability%20of%20the%20pretrained%0Afoundation%20model.%20Extensive%20experiments%20across%20multiple%20benchmarks%20indicate%0Athat%20PiVOT%2C%20using%20the%20proposed%20prompting%20method%20can%20suppress%20distracting%0Aobjects%20and%20enhance%20the%20tracker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18901v1&entry.124074799=Read"},
{"title": "ProMerge: Prompt and Merge for Unsupervised Instance Segmentation", "author": "Dylan Li and Gyungin Shin", "abstract": "  Unsupervised instance segmentation aims to segment distinct object instances\nin an image without relying on human-labeled data. This field has recently seen\nsignificant advancements, partly due to the strong local correspondences\nafforded by rich visual feature representations from self-supervised models\n(e.g., DINO). Recent state-of-the-art approaches use self-supervised features\nto represent images as graphs and solve a generalized eigenvalue system (i.e.,\nnormalized-cut) to generate foreground masks. While effective, this strategy is\nlimited by its attendant computational demands, leading to slow inference\nspeeds. In this paper, we propose Prompt and Merge (ProMerge), which leverages\nself-supervised visual features to obtain initial groupings of patches and\napplies a strategic merging to these segments, aided by a sophisticated\nbackground-based mask pruning technique. ProMerge not only yields competitive\nresults but also offers a significant reduction in inference time compared to\nstate-of-the-art normalized-cut-based approaches. Furthermore, when training an\nobject detector using our mask predictions as pseudo-labels, the resulting\ndetector surpasses the current leading unsupervised model on various\nchallenging instance segmentation benchmarks.\n", "link": "http://arxiv.org/abs/2409.18961v1", "date": "2024-09-27", "relevancy": 2.0514, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5292}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProMerge%3A%20Prompt%20and%20Merge%20for%20Unsupervised%20Instance%20Segmentation&body=Title%3A%20ProMerge%3A%20Prompt%20and%20Merge%20for%20Unsupervised%20Instance%20Segmentation%0AAuthor%3A%20Dylan%20Li%20and%20Gyungin%20Shin%0AAbstract%3A%20%20%20Unsupervised%20instance%20segmentation%20aims%20to%20segment%20distinct%20object%20instances%0Ain%20an%20image%20without%20relying%20on%20human-labeled%20data.%20This%20field%20has%20recently%20seen%0Asignificant%20advancements%2C%20partly%20due%20to%20the%20strong%20local%20correspondences%0Aafforded%20by%20rich%20visual%20feature%20representations%20from%20self-supervised%20models%0A%28e.g.%2C%20DINO%29.%20Recent%20state-of-the-art%20approaches%20use%20self-supervised%20features%0Ato%20represent%20images%20as%20graphs%20and%20solve%20a%20generalized%20eigenvalue%20system%20%28i.e.%2C%0Anormalized-cut%29%20to%20generate%20foreground%20masks.%20While%20effective%2C%20this%20strategy%20is%0Alimited%20by%20its%20attendant%20computational%20demands%2C%20leading%20to%20slow%20inference%0Aspeeds.%20In%20this%20paper%2C%20we%20propose%20Prompt%20and%20Merge%20%28ProMerge%29%2C%20which%20leverages%0Aself-supervised%20visual%20features%20to%20obtain%20initial%20groupings%20of%20patches%20and%0Aapplies%20a%20strategic%20merging%20to%20these%20segments%2C%20aided%20by%20a%20sophisticated%0Abackground-based%20mask%20pruning%20technique.%20ProMerge%20not%20only%20yields%20competitive%0Aresults%20but%20also%20offers%20a%20significant%20reduction%20in%20inference%20time%20compared%20to%0Astate-of-the-art%20normalized-cut-based%20approaches.%20Furthermore%2C%20when%20training%20an%0Aobject%20detector%20using%20our%20mask%20predictions%20as%20pseudo-labels%2C%20the%20resulting%0Adetector%20surpasses%20the%20current%20leading%20unsupervised%20model%20on%20various%0Achallenging%20instance%20segmentation%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProMerge%253A%2520Prompt%2520and%2520Merge%2520for%2520Unsupervised%2520Instance%2520Segmentation%26entry.906535625%3DDylan%2520Li%2520and%2520Gyungin%2520Shin%26entry.1292438233%3D%2520%2520Unsupervised%2520instance%2520segmentation%2520aims%2520to%2520segment%2520distinct%2520object%2520instances%250Ain%2520an%2520image%2520without%2520relying%2520on%2520human-labeled%2520data.%2520This%2520field%2520has%2520recently%2520seen%250Asignificant%2520advancements%252C%2520partly%2520due%2520to%2520the%2520strong%2520local%2520correspondences%250Aafforded%2520by%2520rich%2520visual%2520feature%2520representations%2520from%2520self-supervised%2520models%250A%2528e.g.%252C%2520DINO%2529.%2520Recent%2520state-of-the-art%2520approaches%2520use%2520self-supervised%2520features%250Ato%2520represent%2520images%2520as%2520graphs%2520and%2520solve%2520a%2520generalized%2520eigenvalue%2520system%2520%2528i.e.%252C%250Anormalized-cut%2529%2520to%2520generate%2520foreground%2520masks.%2520While%2520effective%252C%2520this%2520strategy%2520is%250Alimited%2520by%2520its%2520attendant%2520computational%2520demands%252C%2520leading%2520to%2520slow%2520inference%250Aspeeds.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Prompt%2520and%2520Merge%2520%2528ProMerge%2529%252C%2520which%2520leverages%250Aself-supervised%2520visual%2520features%2520to%2520obtain%2520initial%2520groupings%2520of%2520patches%2520and%250Aapplies%2520a%2520strategic%2520merging%2520to%2520these%2520segments%252C%2520aided%2520by%2520a%2520sophisticated%250Abackground-based%2520mask%2520pruning%2520technique.%2520ProMerge%2520not%2520only%2520yields%2520competitive%250Aresults%2520but%2520also%2520offers%2520a%2520significant%2520reduction%2520in%2520inference%2520time%2520compared%2520to%250Astate-of-the-art%2520normalized-cut-based%2520approaches.%2520Furthermore%252C%2520when%2520training%2520an%250Aobject%2520detector%2520using%2520our%2520mask%2520predictions%2520as%2520pseudo-labels%252C%2520the%2520resulting%250Adetector%2520surpasses%2520the%2520current%2520leading%2520unsupervised%2520model%2520on%2520various%250Achallenging%2520instance%2520segmentation%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProMerge%3A%20Prompt%20and%20Merge%20for%20Unsupervised%20Instance%20Segmentation&entry.906535625=Dylan%20Li%20and%20Gyungin%20Shin&entry.1292438233=%20%20Unsupervised%20instance%20segmentation%20aims%20to%20segment%20distinct%20object%20instances%0Ain%20an%20image%20without%20relying%20on%20human-labeled%20data.%20This%20field%20has%20recently%20seen%0Asignificant%20advancements%2C%20partly%20due%20to%20the%20strong%20local%20correspondences%0Aafforded%20by%20rich%20visual%20feature%20representations%20from%20self-supervised%20models%0A%28e.g.%2C%20DINO%29.%20Recent%20state-of-the-art%20approaches%20use%20self-supervised%20features%0Ato%20represent%20images%20as%20graphs%20and%20solve%20a%20generalized%20eigenvalue%20system%20%28i.e.%2C%0Anormalized-cut%29%20to%20generate%20foreground%20masks.%20While%20effective%2C%20this%20strategy%20is%0Alimited%20by%20its%20attendant%20computational%20demands%2C%20leading%20to%20slow%20inference%0Aspeeds.%20In%20this%20paper%2C%20we%20propose%20Prompt%20and%20Merge%20%28ProMerge%29%2C%20which%20leverages%0Aself-supervised%20visual%20features%20to%20obtain%20initial%20groupings%20of%20patches%20and%0Aapplies%20a%20strategic%20merging%20to%20these%20segments%2C%20aided%20by%20a%20sophisticated%0Abackground-based%20mask%20pruning%20technique.%20ProMerge%20not%20only%20yields%20competitive%0Aresults%20but%20also%20offers%20a%20significant%20reduction%20in%20inference%20time%20compared%20to%0Astate-of-the-art%20normalized-cut-based%20approaches.%20Furthermore%2C%20when%20training%20an%0Aobject%20detector%20using%20our%20mask%20predictions%20as%20pseudo-labels%2C%20the%20resulting%0Adetector%20surpasses%20the%20current%20leading%20unsupervised%20model%20on%20various%0Achallenging%20instance%20segmentation%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18961v1&entry.124074799=Read"},
{"title": "A Novel Unified Architecture for Low-Shot Counting by Detection and\n  Segmentation", "author": "Jer Pelhan and Alan Luke\u017ei\u010d and Vitjan Zavrtanik and Matej Kristan", "abstract": "  Low-shot object counters estimate the number of objects in an image using few\nor no annotated exemplars. Objects are localized by matching them to\nprototypes, which are constructed by unsupervised image-wide object appearance\naggregation. Due to potentially diverse object appearances, the existing\napproaches often lead to overgeneralization and false positive detections.\nFurthermore, the best-performing methods train object localization by a\nsurrogate loss, that predicts a unit Gaussian at each object center. This loss\nis sensitive to annotation error, hyperparameters and does not directly\noptimize the detection task, leading to suboptimal counts. We introduce GeCo, a\nnovel low-shot counter that achieves accurate object detection, segmentation,\nand count estimation in a unified architecture. GeCo robustly generalizes the\nprototypes across objects appearances through a novel dense object query\nformulation. In addition, a novel counting loss is proposed, that directly\noptimizes the detection task and avoids the issues of the standard surrogate\nloss. GeCo surpasses the leading few-shot detection-based counters by\n$\\sim$25\\% in the total count MAE, achieves superior detection accuracy and\nsets a new solid state-of-the-art result across all low-shot counting setups.\n", "link": "http://arxiv.org/abs/2409.18686v1", "date": "2024-09-27", "relevancy": 2.0449, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5409}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5081}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Unified%20Architecture%20for%20Low-Shot%20Counting%20by%20Detection%20and%0A%20%20Segmentation&body=Title%3A%20A%20Novel%20Unified%20Architecture%20for%20Low-Shot%20Counting%20by%20Detection%20and%0A%20%20Segmentation%0AAuthor%3A%20Jer%20Pelhan%20and%20Alan%20Luke%C5%BEi%C4%8D%20and%20Vitjan%20Zavrtanik%20and%20Matej%20Kristan%0AAbstract%3A%20%20%20Low-shot%20object%20counters%20estimate%20the%20number%20of%20objects%20in%20an%20image%20using%20few%0Aor%20no%20annotated%20exemplars.%20Objects%20are%20localized%20by%20matching%20them%20to%0Aprototypes%2C%20which%20are%20constructed%20by%20unsupervised%20image-wide%20object%20appearance%0Aaggregation.%20Due%20to%20potentially%20diverse%20object%20appearances%2C%20the%20existing%0Aapproaches%20often%20lead%20to%20overgeneralization%20and%20false%20positive%20detections.%0AFurthermore%2C%20the%20best-performing%20methods%20train%20object%20localization%20by%20a%0Asurrogate%20loss%2C%20that%20predicts%20a%20unit%20Gaussian%20at%20each%20object%20center.%20This%20loss%0Ais%20sensitive%20to%20annotation%20error%2C%20hyperparameters%20and%20does%20not%20directly%0Aoptimize%20the%20detection%20task%2C%20leading%20to%20suboptimal%20counts.%20We%20introduce%20GeCo%2C%20a%0Anovel%20low-shot%20counter%20that%20achieves%20accurate%20object%20detection%2C%20segmentation%2C%0Aand%20count%20estimation%20in%20a%20unified%20architecture.%20GeCo%20robustly%20generalizes%20the%0Aprototypes%20across%20objects%20appearances%20through%20a%20novel%20dense%20object%20query%0Aformulation.%20In%20addition%2C%20a%20novel%20counting%20loss%20is%20proposed%2C%20that%20directly%0Aoptimizes%20the%20detection%20task%20and%20avoids%20the%20issues%20of%20the%20standard%20surrogate%0Aloss.%20GeCo%20surpasses%20the%20leading%20few-shot%20detection-based%20counters%20by%0A%24%5Csim%2425%5C%25%20in%20the%20total%20count%20MAE%2C%20achieves%20superior%20detection%20accuracy%20and%0Asets%20a%20new%20solid%20state-of-the-art%20result%20across%20all%20low-shot%20counting%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Unified%2520Architecture%2520for%2520Low-Shot%2520Counting%2520by%2520Detection%2520and%250A%2520%2520Segmentation%26entry.906535625%3DJer%2520Pelhan%2520and%2520Alan%2520Luke%25C5%25BEi%25C4%258D%2520and%2520Vitjan%2520Zavrtanik%2520and%2520Matej%2520Kristan%26entry.1292438233%3D%2520%2520Low-shot%2520object%2520counters%2520estimate%2520the%2520number%2520of%2520objects%2520in%2520an%2520image%2520using%2520few%250Aor%2520no%2520annotated%2520exemplars.%2520Objects%2520are%2520localized%2520by%2520matching%2520them%2520to%250Aprototypes%252C%2520which%2520are%2520constructed%2520by%2520unsupervised%2520image-wide%2520object%2520appearance%250Aaggregation.%2520Due%2520to%2520potentially%2520diverse%2520object%2520appearances%252C%2520the%2520existing%250Aapproaches%2520often%2520lead%2520to%2520overgeneralization%2520and%2520false%2520positive%2520detections.%250AFurthermore%252C%2520the%2520best-performing%2520methods%2520train%2520object%2520localization%2520by%2520a%250Asurrogate%2520loss%252C%2520that%2520predicts%2520a%2520unit%2520Gaussian%2520at%2520each%2520object%2520center.%2520This%2520loss%250Ais%2520sensitive%2520to%2520annotation%2520error%252C%2520hyperparameters%2520and%2520does%2520not%2520directly%250Aoptimize%2520the%2520detection%2520task%252C%2520leading%2520to%2520suboptimal%2520counts.%2520We%2520introduce%2520GeCo%252C%2520a%250Anovel%2520low-shot%2520counter%2520that%2520achieves%2520accurate%2520object%2520detection%252C%2520segmentation%252C%250Aand%2520count%2520estimation%2520in%2520a%2520unified%2520architecture.%2520GeCo%2520robustly%2520generalizes%2520the%250Aprototypes%2520across%2520objects%2520appearances%2520through%2520a%2520novel%2520dense%2520object%2520query%250Aformulation.%2520In%2520addition%252C%2520a%2520novel%2520counting%2520loss%2520is%2520proposed%252C%2520that%2520directly%250Aoptimizes%2520the%2520detection%2520task%2520and%2520avoids%2520the%2520issues%2520of%2520the%2520standard%2520surrogate%250Aloss.%2520GeCo%2520surpasses%2520the%2520leading%2520few-shot%2520detection-based%2520counters%2520by%250A%2524%255Csim%252425%255C%2525%2520in%2520the%2520total%2520count%2520MAE%252C%2520achieves%2520superior%2520detection%2520accuracy%2520and%250Asets%2520a%2520new%2520solid%2520state-of-the-art%2520result%2520across%2520all%2520low-shot%2520counting%2520setups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Unified%20Architecture%20for%20Low-Shot%20Counting%20by%20Detection%20and%0A%20%20Segmentation&entry.906535625=Jer%20Pelhan%20and%20Alan%20Luke%C5%BEi%C4%8D%20and%20Vitjan%20Zavrtanik%20and%20Matej%20Kristan&entry.1292438233=%20%20Low-shot%20object%20counters%20estimate%20the%20number%20of%20objects%20in%20an%20image%20using%20few%0Aor%20no%20annotated%20exemplars.%20Objects%20are%20localized%20by%20matching%20them%20to%0Aprototypes%2C%20which%20are%20constructed%20by%20unsupervised%20image-wide%20object%20appearance%0Aaggregation.%20Due%20to%20potentially%20diverse%20object%20appearances%2C%20the%20existing%0Aapproaches%20often%20lead%20to%20overgeneralization%20and%20false%20positive%20detections.%0AFurthermore%2C%20the%20best-performing%20methods%20train%20object%20localization%20by%20a%0Asurrogate%20loss%2C%20that%20predicts%20a%20unit%20Gaussian%20at%20each%20object%20center.%20This%20loss%0Ais%20sensitive%20to%20annotation%20error%2C%20hyperparameters%20and%20does%20not%20directly%0Aoptimize%20the%20detection%20task%2C%20leading%20to%20suboptimal%20counts.%20We%20introduce%20GeCo%2C%20a%0Anovel%20low-shot%20counter%20that%20achieves%20accurate%20object%20detection%2C%20segmentation%2C%0Aand%20count%20estimation%20in%20a%20unified%20architecture.%20GeCo%20robustly%20generalizes%20the%0Aprototypes%20across%20objects%20appearances%20through%20a%20novel%20dense%20object%20query%0Aformulation.%20In%20addition%2C%20a%20novel%20counting%20loss%20is%20proposed%2C%20that%20directly%0Aoptimizes%20the%20detection%20task%20and%20avoids%20the%20issues%20of%20the%20standard%20surrogate%0Aloss.%20GeCo%20surpasses%20the%20leading%20few-shot%20detection-based%20counters%20by%0A%24%5Csim%2425%5C%25%20in%20the%20total%20count%20MAE%2C%20achieves%20superior%20detection%20accuracy%20and%0Asets%20a%20new%20solid%20state-of-the-art%20result%20across%20all%20low-shot%20counting%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18686v1&entry.124074799=Read"},
{"title": "A method of using RSVD in residual calculation of LowBit GEMM", "author": "Hongyaoxing Gu", "abstract": "  The advancements of hardware technology in recent years has brought many\npossibilities for low-precision applications. However, the use of low precision\ncan introduce significant computational errors, posing a considerable challenge\nto maintaining the computational accuracy.\n  We propose low-rank residuals quantized matrix multiplication(LRQMM) method\nwhich introduces low-rank approximation in residual compensation for dense low\nprecision quantization matrix multiplication. It can bring several times\naccuracy improvement with only BLAS-2 level extra time overhead. Moreover,\nLRQMM is a completely data-free quantization method that does not require\nadditional data for pre-training. And it only works with low precision GEMM\noperator, which is easy to couple with other methods.\n  Through experimentation, LRQMM can reduce the error of direct quantized\nmatrix multiplication by 1~2 orders of magnitude, when dealing with larger\nmatrix sizes, the computational speed is only reduced by approximately 20\\%. In\ndeep learning networks, LRQMM-4bit achieves 61.8% ImageNet Top-1 accuracy in\nResnet-50, while the Direct Quant accuracy is only 8.3%.\n", "link": "http://arxiv.org/abs/2409.18772v1", "date": "2024-09-27", "relevancy": 2.0416, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5312}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5034}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20method%20of%20using%20RSVD%20in%20residual%20calculation%20of%20LowBit%20GEMM&body=Title%3A%20A%20method%20of%20using%20RSVD%20in%20residual%20calculation%20of%20LowBit%20GEMM%0AAuthor%3A%20Hongyaoxing%20Gu%0AAbstract%3A%20%20%20The%20advancements%20of%20hardware%20technology%20in%20recent%20years%20has%20brought%20many%0Apossibilities%20for%20low-precision%20applications.%20However%2C%20the%20use%20of%20low%20precision%0Acan%20introduce%20significant%20computational%20errors%2C%20posing%20a%20considerable%20challenge%0Ato%20maintaining%20the%20computational%20accuracy.%0A%20%20We%20propose%20low-rank%20residuals%20quantized%20matrix%20multiplication%28LRQMM%29%20method%0Awhich%20introduces%20low-rank%20approximation%20in%20residual%20compensation%20for%20dense%20low%0Aprecision%20quantization%20matrix%20multiplication.%20It%20can%20bring%20several%20times%0Aaccuracy%20improvement%20with%20only%20BLAS-2%20level%20extra%20time%20overhead.%20Moreover%2C%0ALRQMM%20is%20a%20completely%20data-free%20quantization%20method%20that%20does%20not%20require%0Aadditional%20data%20for%20pre-training.%20And%20it%20only%20works%20with%20low%20precision%20GEMM%0Aoperator%2C%20which%20is%20easy%20to%20couple%20with%20other%20methods.%0A%20%20Through%20experimentation%2C%20LRQMM%20can%20reduce%20the%20error%20of%20direct%20quantized%0Amatrix%20multiplication%20by%201~2%20orders%20of%20magnitude%2C%20when%20dealing%20with%20larger%0Amatrix%20sizes%2C%20the%20computational%20speed%20is%20only%20reduced%20by%20approximately%2020%5C%25.%20In%0Adeep%20learning%20networks%2C%20LRQMM-4bit%20achieves%2061.8%25%20ImageNet%20Top-1%20accuracy%20in%0AResnet-50%2C%20while%20the%20Direct%20Quant%20accuracy%20is%20only%208.3%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520method%2520of%2520using%2520RSVD%2520in%2520residual%2520calculation%2520of%2520LowBit%2520GEMM%26entry.906535625%3DHongyaoxing%2520Gu%26entry.1292438233%3D%2520%2520The%2520advancements%2520of%2520hardware%2520technology%2520in%2520recent%2520years%2520has%2520brought%2520many%250Apossibilities%2520for%2520low-precision%2520applications.%2520However%252C%2520the%2520use%2520of%2520low%2520precision%250Acan%2520introduce%2520significant%2520computational%2520errors%252C%2520posing%2520a%2520considerable%2520challenge%250Ato%2520maintaining%2520the%2520computational%2520accuracy.%250A%2520%2520We%2520propose%2520low-rank%2520residuals%2520quantized%2520matrix%2520multiplication%2528LRQMM%2529%2520method%250Awhich%2520introduces%2520low-rank%2520approximation%2520in%2520residual%2520compensation%2520for%2520dense%2520low%250Aprecision%2520quantization%2520matrix%2520multiplication.%2520It%2520can%2520bring%2520several%2520times%250Aaccuracy%2520improvement%2520with%2520only%2520BLAS-2%2520level%2520extra%2520time%2520overhead.%2520Moreover%252C%250ALRQMM%2520is%2520a%2520completely%2520data-free%2520quantization%2520method%2520that%2520does%2520not%2520require%250Aadditional%2520data%2520for%2520pre-training.%2520And%2520it%2520only%2520works%2520with%2520low%2520precision%2520GEMM%250Aoperator%252C%2520which%2520is%2520easy%2520to%2520couple%2520with%2520other%2520methods.%250A%2520%2520Through%2520experimentation%252C%2520LRQMM%2520can%2520reduce%2520the%2520error%2520of%2520direct%2520quantized%250Amatrix%2520multiplication%2520by%25201~2%2520orders%2520of%2520magnitude%252C%2520when%2520dealing%2520with%2520larger%250Amatrix%2520sizes%252C%2520the%2520computational%2520speed%2520is%2520only%2520reduced%2520by%2520approximately%252020%255C%2525.%2520In%250Adeep%2520learning%2520networks%252C%2520LRQMM-4bit%2520achieves%252061.8%2525%2520ImageNet%2520Top-1%2520accuracy%2520in%250AResnet-50%252C%2520while%2520the%2520Direct%2520Quant%2520accuracy%2520is%2520only%25208.3%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20method%20of%20using%20RSVD%20in%20residual%20calculation%20of%20LowBit%20GEMM&entry.906535625=Hongyaoxing%20Gu&entry.1292438233=%20%20The%20advancements%20of%20hardware%20technology%20in%20recent%20years%20has%20brought%20many%0Apossibilities%20for%20low-precision%20applications.%20However%2C%20the%20use%20of%20low%20precision%0Acan%20introduce%20significant%20computational%20errors%2C%20posing%20a%20considerable%20challenge%0Ato%20maintaining%20the%20computational%20accuracy.%0A%20%20We%20propose%20low-rank%20residuals%20quantized%20matrix%20multiplication%28LRQMM%29%20method%0Awhich%20introduces%20low-rank%20approximation%20in%20residual%20compensation%20for%20dense%20low%0Aprecision%20quantization%20matrix%20multiplication.%20It%20can%20bring%20several%20times%0Aaccuracy%20improvement%20with%20only%20BLAS-2%20level%20extra%20time%20overhead.%20Moreover%2C%0ALRQMM%20is%20a%20completely%20data-free%20quantization%20method%20that%20does%20not%20require%0Aadditional%20data%20for%20pre-training.%20And%20it%20only%20works%20with%20low%20precision%20GEMM%0Aoperator%2C%20which%20is%20easy%20to%20couple%20with%20other%20methods.%0A%20%20Through%20experimentation%2C%20LRQMM%20can%20reduce%20the%20error%20of%20direct%20quantized%0Amatrix%20multiplication%20by%201~2%20orders%20of%20magnitude%2C%20when%20dealing%20with%20larger%0Amatrix%20sizes%2C%20the%20computational%20speed%20is%20only%20reduced%20by%20approximately%2020%5C%25.%20In%0Adeep%20learning%20networks%2C%20LRQMM-4bit%20achieves%2061.8%25%20ImageNet%20Top-1%20accuracy%20in%0AResnet-50%2C%20while%20the%20Direct%20Quant%20accuracy%20is%20only%208.3%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18772v1&entry.124074799=Read"},
{"title": "Pseudo-kinematic trajectory control of tracked vehicles", "author": "Michele Focchi and Daniele Fontanelli and Luigi Palopoli", "abstract": "  Tracked vehicles are used in complex scenarios, where motion planning and\nnavigation can be very complex. They have complex dynamics, with many\nparameters that are difficult to identify and that change significantly based\non the operating conditions. We propose a simple pseudo-kinematic model, where\nthe intricate dynamic effects underlying the vehicle's motion are captured in a\nsmall set of velocity-dependent parameters. This choice enables the development\nof a Lyapunov-based trajectory controller with guaranteed performance and small\ncomputation time. We demonstrate the correctness of our approach with both\nsimulation and experimental data.\n", "link": "http://arxiv.org/abs/2409.18641v1", "date": "2024-09-27", "relevancy": 2.0041, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5386}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5072}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-kinematic%20trajectory%20control%20of%20tracked%20vehicles&body=Title%3A%20Pseudo-kinematic%20trajectory%20control%20of%20tracked%20vehicles%0AAuthor%3A%20Michele%20Focchi%20and%20Daniele%20Fontanelli%20and%20Luigi%20Palopoli%0AAbstract%3A%20%20%20Tracked%20vehicles%20are%20used%20in%20complex%20scenarios%2C%20where%20motion%20planning%20and%0Anavigation%20can%20be%20very%20complex.%20They%20have%20complex%20dynamics%2C%20with%20many%0Aparameters%20that%20are%20difficult%20to%20identify%20and%20that%20change%20significantly%20based%0Aon%20the%20operating%20conditions.%20We%20propose%20a%20simple%20pseudo-kinematic%20model%2C%20where%0Athe%20intricate%20dynamic%20effects%20underlying%20the%20vehicle%27s%20motion%20are%20captured%20in%20a%0Asmall%20set%20of%20velocity-dependent%20parameters.%20This%20choice%20enables%20the%20development%0Aof%20a%20Lyapunov-based%20trajectory%20controller%20with%20guaranteed%20performance%20and%20small%0Acomputation%20time.%20We%20demonstrate%20the%20correctness%20of%20our%20approach%20with%20both%0Asimulation%20and%20experimental%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-kinematic%2520trajectory%2520control%2520of%2520tracked%2520vehicles%26entry.906535625%3DMichele%2520Focchi%2520and%2520Daniele%2520Fontanelli%2520and%2520Luigi%2520Palopoli%26entry.1292438233%3D%2520%2520Tracked%2520vehicles%2520are%2520used%2520in%2520complex%2520scenarios%252C%2520where%2520motion%2520planning%2520and%250Anavigation%2520can%2520be%2520very%2520complex.%2520They%2520have%2520complex%2520dynamics%252C%2520with%2520many%250Aparameters%2520that%2520are%2520difficult%2520to%2520identify%2520and%2520that%2520change%2520significantly%2520based%250Aon%2520the%2520operating%2520conditions.%2520We%2520propose%2520a%2520simple%2520pseudo-kinematic%2520model%252C%2520where%250Athe%2520intricate%2520dynamic%2520effects%2520underlying%2520the%2520vehicle%2527s%2520motion%2520are%2520captured%2520in%2520a%250Asmall%2520set%2520of%2520velocity-dependent%2520parameters.%2520This%2520choice%2520enables%2520the%2520development%250Aof%2520a%2520Lyapunov-based%2520trajectory%2520controller%2520with%2520guaranteed%2520performance%2520and%2520small%250Acomputation%2520time.%2520We%2520demonstrate%2520the%2520correctness%2520of%2520our%2520approach%2520with%2520both%250Asimulation%2520and%2520experimental%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-kinematic%20trajectory%20control%20of%20tracked%20vehicles&entry.906535625=Michele%20Focchi%20and%20Daniele%20Fontanelli%20and%20Luigi%20Palopoli&entry.1292438233=%20%20Tracked%20vehicles%20are%20used%20in%20complex%20scenarios%2C%20where%20motion%20planning%20and%0Anavigation%20can%20be%20very%20complex.%20They%20have%20complex%20dynamics%2C%20with%20many%0Aparameters%20that%20are%20difficult%20to%20identify%20and%20that%20change%20significantly%20based%0Aon%20the%20operating%20conditions.%20We%20propose%20a%20simple%20pseudo-kinematic%20model%2C%20where%0Athe%20intricate%20dynamic%20effects%20underlying%20the%20vehicle%27s%20motion%20are%20captured%20in%20a%0Asmall%20set%20of%20velocity-dependent%20parameters.%20This%20choice%20enables%20the%20development%0Aof%20a%20Lyapunov-based%20trajectory%20controller%20with%20guaranteed%20performance%20and%20small%0Acomputation%20time.%20We%20demonstrate%20the%20correctness%20of%20our%20approach%20with%20both%0Asimulation%20and%20experimental%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18641v1&entry.124074799=Read"},
{"title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated\n  Short News-Like Posts", "author": "Henrique Da Silva Gameiro and Andrei Kucharavy and Ljiljana Dolamic", "abstract": "  With the emergence of widely available powerful LLMs, disinformation\ngenerated by large Language Models (LLMs) has become a major concern.\nHistorically, LLM detectors have been touted as a solution, but their\neffectiveness in the real world is still to be proven. In this paper, we focus\non an important setting in information operations -- short news-like posts\ngenerated by moderately sophisticated attackers.\n  We demonstrate that existing LLM detectors, whether zero-shot or\npurpose-trained, are not ready for real-world use in that setting. All tested\nzero-shot detectors perform inconsistently with prior benchmarks and are highly\nvulnerable to sampling temperature increase, a trivial attack absent from\nrecent benchmarks. A purpose-trained detector generalizing across LLMs and\nunseen attacks can be developed, but it fails to generalize to new\nhuman-written texts.\n  We argue that the former indicates domain-specific benchmarking is needed,\nwhile the latter suggests a trade-off between the adversarial evasion\nresilience and overfitting to the reference human text, with both needing\nevaluation in benchmarks and currently absent. We believe this suggests a\nre-consideration of current LLM detector benchmarking approaches and provides a\ndynamically extensible benchmark to allow it\n(https://github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection).\n", "link": "http://arxiv.org/abs/2409.03291v2", "date": "2024-09-27", "relevancy": 1.9864, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Detectors%20Still%20Fall%20Short%20of%20Real%20World%3A%20Case%20of%20LLM-Generated%0A%20%20Short%20News-Like%20Posts&body=Title%3A%20LLM%20Detectors%20Still%20Fall%20Short%20of%20Real%20World%3A%20Case%20of%20LLM-Generated%0A%20%20Short%20News-Like%20Posts%0AAuthor%3A%20Henrique%20Da%20Silva%20Gameiro%20and%20Andrei%20Kucharavy%20and%20Ljiljana%20Dolamic%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20widely%20available%20powerful%20LLMs%2C%20disinformation%0Agenerated%20by%20large%20Language%20Models%20%28LLMs%29%20has%20become%20a%20major%20concern.%0AHistorically%2C%20LLM%20detectors%20have%20been%20touted%20as%20a%20solution%2C%20but%20their%0Aeffectiveness%20in%20the%20real%20world%20is%20still%20to%20be%20proven.%20In%20this%20paper%2C%20we%20focus%0Aon%20an%20important%20setting%20in%20information%20operations%20--%20short%20news-like%20posts%0Agenerated%20by%20moderately%20sophisticated%20attackers.%0A%20%20We%20demonstrate%20that%20existing%20LLM%20detectors%2C%20whether%20zero-shot%20or%0Apurpose-trained%2C%20are%20not%20ready%20for%20real-world%20use%20in%20that%20setting.%20All%20tested%0Azero-shot%20detectors%20perform%20inconsistently%20with%20prior%20benchmarks%20and%20are%20highly%0Avulnerable%20to%20sampling%20temperature%20increase%2C%20a%20trivial%20attack%20absent%20from%0Arecent%20benchmarks.%20A%20purpose-trained%20detector%20generalizing%20across%20LLMs%20and%0Aunseen%20attacks%20can%20be%20developed%2C%20but%20it%20fails%20to%20generalize%20to%20new%0Ahuman-written%20texts.%0A%20%20We%20argue%20that%20the%20former%20indicates%20domain-specific%20benchmarking%20is%20needed%2C%0Awhile%20the%20latter%20suggests%20a%20trade-off%20between%20the%20adversarial%20evasion%0Aresilience%20and%20overfitting%20to%20the%20reference%20human%20text%2C%20with%20both%20needing%0Aevaluation%20in%20benchmarks%20and%20currently%20absent.%20We%20believe%20this%20suggests%20a%0Are-consideration%20of%20current%20LLM%20detector%20benchmarking%20approaches%20and%20provides%20a%0Adynamically%20extensible%20benchmark%20to%20allow%20it%0A%28https%3A//github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03291v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Detectors%2520Still%2520Fall%2520Short%2520of%2520Real%2520World%253A%2520Case%2520of%2520LLM-Generated%250A%2520%2520Short%2520News-Like%2520Posts%26entry.906535625%3DHenrique%2520Da%2520Silva%2520Gameiro%2520and%2520Andrei%2520Kucharavy%2520and%2520Ljiljana%2520Dolamic%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520widely%2520available%2520powerful%2520LLMs%252C%2520disinformation%250Agenerated%2520by%2520large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520become%2520a%2520major%2520concern.%250AHistorically%252C%2520LLM%2520detectors%2520have%2520been%2520touted%2520as%2520a%2520solution%252C%2520but%2520their%250Aeffectiveness%2520in%2520the%2520real%2520world%2520is%2520still%2520to%2520be%2520proven.%2520In%2520this%2520paper%252C%2520we%2520focus%250Aon%2520an%2520important%2520setting%2520in%2520information%2520operations%2520--%2520short%2520news-like%2520posts%250Agenerated%2520by%2520moderately%2520sophisticated%2520attackers.%250A%2520%2520We%2520demonstrate%2520that%2520existing%2520LLM%2520detectors%252C%2520whether%2520zero-shot%2520or%250Apurpose-trained%252C%2520are%2520not%2520ready%2520for%2520real-world%2520use%2520in%2520that%2520setting.%2520All%2520tested%250Azero-shot%2520detectors%2520perform%2520inconsistently%2520with%2520prior%2520benchmarks%2520and%2520are%2520highly%250Avulnerable%2520to%2520sampling%2520temperature%2520increase%252C%2520a%2520trivial%2520attack%2520absent%2520from%250Arecent%2520benchmarks.%2520A%2520purpose-trained%2520detector%2520generalizing%2520across%2520LLMs%2520and%250Aunseen%2520attacks%2520can%2520be%2520developed%252C%2520but%2520it%2520fails%2520to%2520generalize%2520to%2520new%250Ahuman-written%2520texts.%250A%2520%2520We%2520argue%2520that%2520the%2520former%2520indicates%2520domain-specific%2520benchmarking%2520is%2520needed%252C%250Awhile%2520the%2520latter%2520suggests%2520a%2520trade-off%2520between%2520the%2520adversarial%2520evasion%250Aresilience%2520and%2520overfitting%2520to%2520the%2520reference%2520human%2520text%252C%2520with%2520both%2520needing%250Aevaluation%2520in%2520benchmarks%2520and%2520currently%2520absent.%2520We%2520believe%2520this%2520suggests%2520a%250Are-consideration%2520of%2520current%2520LLM%2520detector%2520benchmarking%2520approaches%2520and%2520provides%2520a%250Adynamically%2520extensible%2520benchmark%2520to%2520allow%2520it%250A%2528https%253A//github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03291v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Detectors%20Still%20Fall%20Short%20of%20Real%20World%3A%20Case%20of%20LLM-Generated%0A%20%20Short%20News-Like%20Posts&entry.906535625=Henrique%20Da%20Silva%20Gameiro%20and%20Andrei%20Kucharavy%20and%20Ljiljana%20Dolamic&entry.1292438233=%20%20With%20the%20emergence%20of%20widely%20available%20powerful%20LLMs%2C%20disinformation%0Agenerated%20by%20large%20Language%20Models%20%28LLMs%29%20has%20become%20a%20major%20concern.%0AHistorically%2C%20LLM%20detectors%20have%20been%20touted%20as%20a%20solution%2C%20but%20their%0Aeffectiveness%20in%20the%20real%20world%20is%20still%20to%20be%20proven.%20In%20this%20paper%2C%20we%20focus%0Aon%20an%20important%20setting%20in%20information%20operations%20--%20short%20news-like%20posts%0Agenerated%20by%20moderately%20sophisticated%20attackers.%0A%20%20We%20demonstrate%20that%20existing%20LLM%20detectors%2C%20whether%20zero-shot%20or%0Apurpose-trained%2C%20are%20not%20ready%20for%20real-world%20use%20in%20that%20setting.%20All%20tested%0Azero-shot%20detectors%20perform%20inconsistently%20with%20prior%20benchmarks%20and%20are%20highly%0Avulnerable%20to%20sampling%20temperature%20increase%2C%20a%20trivial%20attack%20absent%20from%0Arecent%20benchmarks.%20A%20purpose-trained%20detector%20generalizing%20across%20LLMs%20and%0Aunseen%20attacks%20can%20be%20developed%2C%20but%20it%20fails%20to%20generalize%20to%20new%0Ahuman-written%20texts.%0A%20%20We%20argue%20that%20the%20former%20indicates%20domain-specific%20benchmarking%20is%20needed%2C%0Awhile%20the%20latter%20suggests%20a%20trade-off%20between%20the%20adversarial%20evasion%0Aresilience%20and%20overfitting%20to%20the%20reference%20human%20text%2C%20with%20both%20needing%0Aevaluation%20in%20benchmarks%20and%20currently%20absent.%20We%20believe%20this%20suggests%20a%0Are-consideration%20of%20current%20LLM%20detector%20benchmarking%20approaches%20and%20provides%20a%0Adynamically%20extensible%20benchmark%20to%20allow%20it%0A%28https%3A//github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03291v2&entry.124074799=Read"},
{"title": "I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing", "author": "Yiwei Ma and Jiayi Ji and Ke Ye and Weihuang Lin and Zhibin Wang and Yonghan Zheng and Qiang Zhou and Xiaoshuai Sun and Rongrong Ji", "abstract": "  Significant progress has been made in the field of Instruction-based Image\nEditing (IIE). However, evaluating these models poses a significant challenge.\nA crucial requirement in this field is the establishment of a comprehensive\nevaluation benchmark for accurately assessing editing results and providing\nvaluable insights for its further development. In response to this need, we\npropose I2EBench, a comprehensive benchmark designed to automatically evaluate\nthe quality of edited images produced by IIE models from multiple dimensions.\nI2EBench consists of 2,000+ images for editing, along with 4,000+ corresponding\noriginal and diverse instructions. It offers three distinctive characteristics:\n1) Comprehensive Evaluation Dimensions: I2EBench comprises 16 evaluation\ndimensions that cover both high-level and low-level aspects, providing a\ncomprehensive assessment of each IIE model. 2) Human Perception Alignment: To\nensure the alignment of our benchmark with human perception, we conducted an\nextensive user study for each evaluation dimension. 3) Valuable Research\nInsights: By analyzing the advantages and disadvantages of existing IIE models\nacross the 16 dimensions, we offer valuable research insights to guide future\ndevelopment in the field. We will open-source I2EBench, including all\ninstructions, input images, human annotations, edited images from all evaluated\nmethods, and a simple script for evaluating the results from new IIE models.\nThe code, dataset and generated images from all IIE models are provided in\ngithub: https://github.com/cocoshe/I2EBench.\n", "link": "http://arxiv.org/abs/2408.14180v2", "date": "2024-09-27", "relevancy": 1.9833, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.505}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I2EBench%3A%20A%20Comprehensive%20Benchmark%20for%20Instruction-based%20Image%20Editing&body=Title%3A%20I2EBench%3A%20A%20Comprehensive%20Benchmark%20for%20Instruction-based%20Image%20Editing%0AAuthor%3A%20Yiwei%20Ma%20and%20Jiayi%20Ji%20and%20Ke%20Ye%20and%20Weihuang%20Lin%20and%20Zhibin%20Wang%20and%20Yonghan%20Zheng%20and%20Qiang%20Zhou%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Significant%20progress%20has%20been%20made%20in%20the%20field%20of%20Instruction-based%20Image%0AEditing%20%28IIE%29.%20However%2C%20evaluating%20these%20models%20poses%20a%20significant%20challenge.%0AA%20crucial%20requirement%20in%20this%20field%20is%20the%20establishment%20of%20a%20comprehensive%0Aevaluation%20benchmark%20for%20accurately%20assessing%20editing%20results%20and%20providing%0Avaluable%20insights%20for%20its%20further%20development.%20In%20response%20to%20this%20need%2C%20we%0Apropose%20I2EBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20automatically%20evaluate%0Athe%20quality%20of%20edited%20images%20produced%20by%20IIE%20models%20from%20multiple%20dimensions.%0AI2EBench%20consists%20of%202%2C000%2B%20images%20for%20editing%2C%20along%20with%204%2C000%2B%20corresponding%0Aoriginal%20and%20diverse%20instructions.%20It%20offers%20three%20distinctive%20characteristics%3A%0A1%29%20Comprehensive%20Evaluation%20Dimensions%3A%20I2EBench%20comprises%2016%20evaluation%0Adimensions%20that%20cover%20both%20high-level%20and%20low-level%20aspects%2C%20providing%20a%0Acomprehensive%20assessment%20of%20each%20IIE%20model.%202%29%20Human%20Perception%20Alignment%3A%20To%0Aensure%20the%20alignment%20of%20our%20benchmark%20with%20human%20perception%2C%20we%20conducted%20an%0Aextensive%20user%20study%20for%20each%20evaluation%20dimension.%203%29%20Valuable%20Research%0AInsights%3A%20By%20analyzing%20the%20advantages%20and%20disadvantages%20of%20existing%20IIE%20models%0Aacross%20the%2016%20dimensions%2C%20we%20offer%20valuable%20research%20insights%20to%20guide%20future%0Adevelopment%20in%20the%20field.%20We%20will%20open-source%20I2EBench%2C%20including%20all%0Ainstructions%2C%20input%20images%2C%20human%20annotations%2C%20edited%20images%20from%20all%20evaluated%0Amethods%2C%20and%20a%20simple%20script%20for%20evaluating%20the%20results%20from%20new%20IIE%20models.%0AThe%20code%2C%20dataset%20and%20generated%20images%20from%20all%20IIE%20models%20are%20provided%20in%0Agithub%3A%20https%3A//github.com/cocoshe/I2EBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI2EBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Instruction-based%2520Image%2520Editing%26entry.906535625%3DYiwei%2520Ma%2520and%2520Jiayi%2520Ji%2520and%2520Ke%2520Ye%2520and%2520Weihuang%2520Lin%2520and%2520Zhibin%2520Wang%2520and%2520Yonghan%2520Zheng%2520and%2520Qiang%2520Zhou%2520and%2520Xiaoshuai%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Significant%2520progress%2520has%2520been%2520made%2520in%2520the%2520field%2520of%2520Instruction-based%2520Image%250AEditing%2520%2528IIE%2529.%2520However%252C%2520evaluating%2520these%2520models%2520poses%2520a%2520significant%2520challenge.%250AA%2520crucial%2520requirement%2520in%2520this%2520field%2520is%2520the%2520establishment%2520of%2520a%2520comprehensive%250Aevaluation%2520benchmark%2520for%2520accurately%2520assessing%2520editing%2520results%2520and%2520providing%250Avaluable%2520insights%2520for%2520its%2520further%2520development.%2520In%2520response%2520to%2520this%2520need%252C%2520we%250Apropose%2520I2EBench%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520automatically%2520evaluate%250Athe%2520quality%2520of%2520edited%2520images%2520produced%2520by%2520IIE%2520models%2520from%2520multiple%2520dimensions.%250AI2EBench%2520consists%2520of%25202%252C000%252B%2520images%2520for%2520editing%252C%2520along%2520with%25204%252C000%252B%2520corresponding%250Aoriginal%2520and%2520diverse%2520instructions.%2520It%2520offers%2520three%2520distinctive%2520characteristics%253A%250A1%2529%2520Comprehensive%2520Evaluation%2520Dimensions%253A%2520I2EBench%2520comprises%252016%2520evaluation%250Adimensions%2520that%2520cover%2520both%2520high-level%2520and%2520low-level%2520aspects%252C%2520providing%2520a%250Acomprehensive%2520assessment%2520of%2520each%2520IIE%2520model.%25202%2529%2520Human%2520Perception%2520Alignment%253A%2520To%250Aensure%2520the%2520alignment%2520of%2520our%2520benchmark%2520with%2520human%2520perception%252C%2520we%2520conducted%2520an%250Aextensive%2520user%2520study%2520for%2520each%2520evaluation%2520dimension.%25203%2529%2520Valuable%2520Research%250AInsights%253A%2520By%2520analyzing%2520the%2520advantages%2520and%2520disadvantages%2520of%2520existing%2520IIE%2520models%250Aacross%2520the%252016%2520dimensions%252C%2520we%2520offer%2520valuable%2520research%2520insights%2520to%2520guide%2520future%250Adevelopment%2520in%2520the%2520field.%2520We%2520will%2520open-source%2520I2EBench%252C%2520including%2520all%250Ainstructions%252C%2520input%2520images%252C%2520human%2520annotations%252C%2520edited%2520images%2520from%2520all%2520evaluated%250Amethods%252C%2520and%2520a%2520simple%2520script%2520for%2520evaluating%2520the%2520results%2520from%2520new%2520IIE%2520models.%250AThe%2520code%252C%2520dataset%2520and%2520generated%2520images%2520from%2520all%2520IIE%2520models%2520are%2520provided%2520in%250Agithub%253A%2520https%253A//github.com/cocoshe/I2EBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I2EBench%3A%20A%20Comprehensive%20Benchmark%20for%20Instruction-based%20Image%20Editing&entry.906535625=Yiwei%20Ma%20and%20Jiayi%20Ji%20and%20Ke%20Ye%20and%20Weihuang%20Lin%20and%20Zhibin%20Wang%20and%20Yonghan%20Zheng%20and%20Qiang%20Zhou%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20Significant%20progress%20has%20been%20made%20in%20the%20field%20of%20Instruction-based%20Image%0AEditing%20%28IIE%29.%20However%2C%20evaluating%20these%20models%20poses%20a%20significant%20challenge.%0AA%20crucial%20requirement%20in%20this%20field%20is%20the%20establishment%20of%20a%20comprehensive%0Aevaluation%20benchmark%20for%20accurately%20assessing%20editing%20results%20and%20providing%0Avaluable%20insights%20for%20its%20further%20development.%20In%20response%20to%20this%20need%2C%20we%0Apropose%20I2EBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20automatically%20evaluate%0Athe%20quality%20of%20edited%20images%20produced%20by%20IIE%20models%20from%20multiple%20dimensions.%0AI2EBench%20consists%20of%202%2C000%2B%20images%20for%20editing%2C%20along%20with%204%2C000%2B%20corresponding%0Aoriginal%20and%20diverse%20instructions.%20It%20offers%20three%20distinctive%20characteristics%3A%0A1%29%20Comprehensive%20Evaluation%20Dimensions%3A%20I2EBench%20comprises%2016%20evaluation%0Adimensions%20that%20cover%20both%20high-level%20and%20low-level%20aspects%2C%20providing%20a%0Acomprehensive%20assessment%20of%20each%20IIE%20model.%202%29%20Human%20Perception%20Alignment%3A%20To%0Aensure%20the%20alignment%20of%20our%20benchmark%20with%20human%20perception%2C%20we%20conducted%20an%0Aextensive%20user%20study%20for%20each%20evaluation%20dimension.%203%29%20Valuable%20Research%0AInsights%3A%20By%20analyzing%20the%20advantages%20and%20disadvantages%20of%20existing%20IIE%20models%0Aacross%20the%2016%20dimensions%2C%20we%20offer%20valuable%20research%20insights%20to%20guide%20future%0Adevelopment%20in%20the%20field.%20We%20will%20open-source%20I2EBench%2C%20including%20all%0Ainstructions%2C%20input%20images%2C%20human%20annotations%2C%20edited%20images%20from%20all%20evaluated%0Amethods%2C%20and%20a%20simple%20script%20for%20evaluating%20the%20results%20from%20new%20IIE%20models.%0AThe%20code%2C%20dataset%20and%20generated%20images%20from%20all%20IIE%20models%20are%20provided%20in%0Agithub%3A%20https%3A//github.com/cocoshe/I2EBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14180v2&entry.124074799=Read"},
{"title": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback", "author": "Jaepill Choi and Kyubyung Chae and Jiwoo Song and Yohan Jo and Taesup Kim", "abstract": "  In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.\n", "link": "http://arxiv.org/abs/2409.18618v1", "date": "2024-09-27", "relevancy": 1.9789, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4982}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4982}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-based%20Preference%20Optimization%20in%20Abstractive%20Summarization%20without%0A%20%20Human%20Feedback&body=Title%3A%20Model-based%20Preference%20Optimization%20in%20Abstractive%20Summarization%20without%0A%20%20Human%20Feedback%0AAuthor%3A%20Jaepill%20Choi%20and%20Kyubyung%20Chae%20and%20Jiwoo%20Song%20and%20Yohan%20Jo%20and%20Taesup%20Kim%0AAbstract%3A%20%20%20In%20abstractive%20summarization%2C%20the%20challenge%20of%20producing%20concise%20and%20accurate%0Asummaries%20arises%20from%20the%20vast%20amount%20of%20information%20contained%20in%20the%20source%0Adocument.%20Consequently%2C%20although%20Large%20Language%20Models%20%28LLMs%29%20can%20generate%0Afluent%20text%2C%20they%20often%20introduce%20inaccuracies%20by%20hallucinating%20content%20not%0Afound%20in%20the%20original%20source.%20While%20supervised%20fine-tuning%20methods%20that%0Amaximize%20likelihood%20contribute%20to%20this%20issue%2C%20they%20do%20not%20consistently%20enhance%0Athe%20faithfulness%20of%20the%20summaries.%20Preference-based%20optimization%20methods%2C%20such%0Aas%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20can%20further%20refine%20the%20model%20to%20align%0Awith%20human%20preferences.%20However%2C%20these%20methods%20still%20heavily%20depend%20on%20costly%0Ahuman%20feedback.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20and%20straightforward%20approach%0Acalled%20Model-based%20Preference%20Optimization%20%28MPO%29%20to%20fine-tune%20LLMs%20for%20improved%0Asummarization%20abilities%20without%20any%20human%20feedback.%20By%20leveraging%20the%20model%27s%0Ainherent%20summarization%20capabilities%2C%20we%20create%20a%20preference%20dataset%20that%20is%0Afully%20generated%20by%20the%20model%20using%20different%20decoding%20strategies.%20Our%0Aexperiments%20on%20standard%20summarization%20datasets%20and%20various%20metrics%20demonstrate%0Athat%20our%20proposed%20MPO%20significantly%20enhances%20the%20quality%20of%20generated%20summaries%0Awithout%20relying%20on%20human%20feedback.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-based%2520Preference%2520Optimization%2520in%2520Abstractive%2520Summarization%2520without%250A%2520%2520Human%2520Feedback%26entry.906535625%3DJaepill%2520Choi%2520and%2520Kyubyung%2520Chae%2520and%2520Jiwoo%2520Song%2520and%2520Yohan%2520Jo%2520and%2520Taesup%2520Kim%26entry.1292438233%3D%2520%2520In%2520abstractive%2520summarization%252C%2520the%2520challenge%2520of%2520producing%2520concise%2520and%2520accurate%250Asummaries%2520arises%2520from%2520the%2520vast%2520amount%2520of%2520information%2520contained%2520in%2520the%2520source%250Adocument.%2520Consequently%252C%2520although%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520generate%250Afluent%2520text%252C%2520they%2520often%2520introduce%2520inaccuracies%2520by%2520hallucinating%2520content%2520not%250Afound%2520in%2520the%2520original%2520source.%2520While%2520supervised%2520fine-tuning%2520methods%2520that%250Amaximize%2520likelihood%2520contribute%2520to%2520this%2520issue%252C%2520they%2520do%2520not%2520consistently%2520enhance%250Athe%2520faithfulness%2520of%2520the%2520summaries.%2520Preference-based%2520optimization%2520methods%252C%2520such%250Aas%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%252C%2520can%2520further%2520refine%2520the%2520model%2520to%2520align%250Awith%2520human%2520preferences.%2520However%252C%2520these%2520methods%2520still%2520heavily%2520depend%2520on%2520costly%250Ahuman%2520feedback.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520and%2520straightforward%2520approach%250Acalled%2520Model-based%2520Preference%2520Optimization%2520%2528MPO%2529%2520to%2520fine-tune%2520LLMs%2520for%2520improved%250Asummarization%2520abilities%2520without%2520any%2520human%2520feedback.%2520By%2520leveraging%2520the%2520model%2527s%250Ainherent%2520summarization%2520capabilities%252C%2520we%2520create%2520a%2520preference%2520dataset%2520that%2520is%250Afully%2520generated%2520by%2520the%2520model%2520using%2520different%2520decoding%2520strategies.%2520Our%250Aexperiments%2520on%2520standard%2520summarization%2520datasets%2520and%2520various%2520metrics%2520demonstrate%250Athat%2520our%2520proposed%2520MPO%2520significantly%2520enhances%2520the%2520quality%2520of%2520generated%2520summaries%250Awithout%2520relying%2520on%2520human%2520feedback.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-based%20Preference%20Optimization%20in%20Abstractive%20Summarization%20without%0A%20%20Human%20Feedback&entry.906535625=Jaepill%20Choi%20and%20Kyubyung%20Chae%20and%20Jiwoo%20Song%20and%20Yohan%20Jo%20and%20Taesup%20Kim&entry.1292438233=%20%20In%20abstractive%20summarization%2C%20the%20challenge%20of%20producing%20concise%20and%20accurate%0Asummaries%20arises%20from%20the%20vast%20amount%20of%20information%20contained%20in%20the%20source%0Adocument.%20Consequently%2C%20although%20Large%20Language%20Models%20%28LLMs%29%20can%20generate%0Afluent%20text%2C%20they%20often%20introduce%20inaccuracies%20by%20hallucinating%20content%20not%0Afound%20in%20the%20original%20source.%20While%20supervised%20fine-tuning%20methods%20that%0Amaximize%20likelihood%20contribute%20to%20this%20issue%2C%20they%20do%20not%20consistently%20enhance%0Athe%20faithfulness%20of%20the%20summaries.%20Preference-based%20optimization%20methods%2C%20such%0Aas%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20can%20further%20refine%20the%20model%20to%20align%0Awith%20human%20preferences.%20However%2C%20these%20methods%20still%20heavily%20depend%20on%20costly%0Ahuman%20feedback.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20and%20straightforward%20approach%0Acalled%20Model-based%20Preference%20Optimization%20%28MPO%29%20to%20fine-tune%20LLMs%20for%20improved%0Asummarization%20abilities%20without%20any%20human%20feedback.%20By%20leveraging%20the%20model%27s%0Ainherent%20summarization%20capabilities%2C%20we%20create%20a%20preference%20dataset%20that%20is%0Afully%20generated%20by%20the%20model%20using%20different%20decoding%20strategies.%20Our%0Aexperiments%20on%20standard%20summarization%20datasets%20and%20various%20metrics%20demonstrate%0Athat%20our%20proposed%20MPO%20significantly%20enhances%20the%20quality%20of%20generated%20summaries%0Awithout%20relying%20on%20human%20feedback.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18618v1&entry.124074799=Read"},
{"title": "Sparse Low-Ranked Self-Attention Transformer for Remaining Useful\n  Lifetime Prediction of Optical Fiber Amplifiers", "author": "Dominic Schneider and Lutz Rapp", "abstract": "  Optical fiber amplifiers are key elements in present optical networks.\nFailures of these components result in high financial loss of income of the\nnetwork operator as the communication traffic over an affected link is\ninterrupted. Applying Remaining useful lifetime (RUL) prediction in the context\nof Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming\nsystem failures at an early stage, so that network outages can be minimized\nthrough planning of targeted maintenance actions, ensures reliability and\nsafety. Optical fiber amplifier are complex systems, that work under various\noperating conditions, which makes correct forecasting a difficult task.\nIncreased monitoring capabilities of systems results in datasets that\nfacilitate the application of data-driven RUL prediction methods. Deep learning\nmodels in particular have shown good performance, but generalization based on\ncomparatively small datasets for RUL prediction is difficult. In this paper, we\npropose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL\nprediction method. SLAT is based on an encoder-decoder architecture, wherein\ntwo parallel working encoders extract features for sensors and time steps. By\nutilizing the self-attention mechanism, long-term dependencies can be learned\nfrom long sequences. The implementation of sparsity in the attention matrix and\na low-rank parametrization reduce overfitting and increase generalization.\nExperimental application to optical fiber amplifiers exemplified on EDFA, as\nwell as a reference dataset from turbofan engines, shows that SLAT outperforms\nthe state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2409.14378v2", "date": "2024-09-27", "relevancy": 1.9758, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5088}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.492}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Low-Ranked%20Self-Attention%20Transformer%20for%20Remaining%20Useful%0A%20%20Lifetime%20Prediction%20of%20Optical%20Fiber%20Amplifiers&body=Title%3A%20Sparse%20Low-Ranked%20Self-Attention%20Transformer%20for%20Remaining%20Useful%0A%20%20Lifetime%20Prediction%20of%20Optical%20Fiber%20Amplifiers%0AAuthor%3A%20Dominic%20Schneider%20and%20Lutz%20Rapp%0AAbstract%3A%20%20%20Optical%20fiber%20amplifiers%20are%20key%20elements%20in%20present%20optical%20networks.%0AFailures%20of%20these%20components%20result%20in%20high%20financial%20loss%20of%20income%20of%20the%0Anetwork%20operator%20as%20the%20communication%20traffic%20over%20an%20affected%20link%20is%0Ainterrupted.%20Applying%20Remaining%20useful%20lifetime%20%28RUL%29%20prediction%20in%20the%20context%0Aof%20Predictive%20Maintenance%20%28PdM%29%20to%20optical%20fiber%20amplifiers%20to%20predict%20upcoming%0Asystem%20failures%20at%20an%20early%20stage%2C%20so%20that%20network%20outages%20can%20be%20minimized%0Athrough%20planning%20of%20targeted%20maintenance%20actions%2C%20ensures%20reliability%20and%0Asafety.%20Optical%20fiber%20amplifier%20are%20complex%20systems%2C%20that%20work%20under%20various%0Aoperating%20conditions%2C%20which%20makes%20correct%20forecasting%20a%20difficult%20task.%0AIncreased%20monitoring%20capabilities%20of%20systems%20results%20in%20datasets%20that%0Afacilitate%20the%20application%20of%20data-driven%20RUL%20prediction%20methods.%20Deep%20learning%0Amodels%20in%20particular%20have%20shown%20good%20performance%2C%20but%20generalization%20based%20on%0Acomparatively%20small%20datasets%20for%20RUL%20prediction%20is%20difficult.%20In%20this%20paper%2C%20we%0Apropose%20Sparse%20Low-ranked%20self-Attention%20Transformer%20%28SLAT%29%20as%20a%20novel%20RUL%0Aprediction%20method.%20SLAT%20is%20based%20on%20an%20encoder-decoder%20architecture%2C%20wherein%0Atwo%20parallel%20working%20encoders%20extract%20features%20for%20sensors%20and%20time%20steps.%20By%0Autilizing%20the%20self-attention%20mechanism%2C%20long-term%20dependencies%20can%20be%20learned%0Afrom%20long%20sequences.%20The%20implementation%20of%20sparsity%20in%20the%20attention%20matrix%20and%0Aa%20low-rank%20parametrization%20reduce%20overfitting%20and%20increase%20generalization.%0AExperimental%20application%20to%20optical%20fiber%20amplifiers%20exemplified%20on%20EDFA%2C%20as%0Awell%20as%20a%20reference%20dataset%20from%20turbofan%20engines%2C%20shows%20that%20SLAT%20outperforms%0Athe%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Low-Ranked%2520Self-Attention%2520Transformer%2520for%2520Remaining%2520Useful%250A%2520%2520Lifetime%2520Prediction%2520of%2520Optical%2520Fiber%2520Amplifiers%26entry.906535625%3DDominic%2520Schneider%2520and%2520Lutz%2520Rapp%26entry.1292438233%3D%2520%2520Optical%2520fiber%2520amplifiers%2520are%2520key%2520elements%2520in%2520present%2520optical%2520networks.%250AFailures%2520of%2520these%2520components%2520result%2520in%2520high%2520financial%2520loss%2520of%2520income%2520of%2520the%250Anetwork%2520operator%2520as%2520the%2520communication%2520traffic%2520over%2520an%2520affected%2520link%2520is%250Ainterrupted.%2520Applying%2520Remaining%2520useful%2520lifetime%2520%2528RUL%2529%2520prediction%2520in%2520the%2520context%250Aof%2520Predictive%2520Maintenance%2520%2528PdM%2529%2520to%2520optical%2520fiber%2520amplifiers%2520to%2520predict%2520upcoming%250Asystem%2520failures%2520at%2520an%2520early%2520stage%252C%2520so%2520that%2520network%2520outages%2520can%2520be%2520minimized%250Athrough%2520planning%2520of%2520targeted%2520maintenance%2520actions%252C%2520ensures%2520reliability%2520and%250Asafety.%2520Optical%2520fiber%2520amplifier%2520are%2520complex%2520systems%252C%2520that%2520work%2520under%2520various%250Aoperating%2520conditions%252C%2520which%2520makes%2520correct%2520forecasting%2520a%2520difficult%2520task.%250AIncreased%2520monitoring%2520capabilities%2520of%2520systems%2520results%2520in%2520datasets%2520that%250Afacilitate%2520the%2520application%2520of%2520data-driven%2520RUL%2520prediction%2520methods.%2520Deep%2520learning%250Amodels%2520in%2520particular%2520have%2520shown%2520good%2520performance%252C%2520but%2520generalization%2520based%2520on%250Acomparatively%2520small%2520datasets%2520for%2520RUL%2520prediction%2520is%2520difficult.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Sparse%2520Low-ranked%2520self-Attention%2520Transformer%2520%2528SLAT%2529%2520as%2520a%2520novel%2520RUL%250Aprediction%2520method.%2520SLAT%2520is%2520based%2520on%2520an%2520encoder-decoder%2520architecture%252C%2520wherein%250Atwo%2520parallel%2520working%2520encoders%2520extract%2520features%2520for%2520sensors%2520and%2520time%2520steps.%2520By%250Autilizing%2520the%2520self-attention%2520mechanism%252C%2520long-term%2520dependencies%2520can%2520be%2520learned%250Afrom%2520long%2520sequences.%2520The%2520implementation%2520of%2520sparsity%2520in%2520the%2520attention%2520matrix%2520and%250Aa%2520low-rank%2520parametrization%2520reduce%2520overfitting%2520and%2520increase%2520generalization.%250AExperimental%2520application%2520to%2520optical%2520fiber%2520amplifiers%2520exemplified%2520on%2520EDFA%252C%2520as%250Awell%2520as%2520a%2520reference%2520dataset%2520from%2520turbofan%2520engines%252C%2520shows%2520that%2520SLAT%2520outperforms%250Athe%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Low-Ranked%20Self-Attention%20Transformer%20for%20Remaining%20Useful%0A%20%20Lifetime%20Prediction%20of%20Optical%20Fiber%20Amplifiers&entry.906535625=Dominic%20Schneider%20and%20Lutz%20Rapp&entry.1292438233=%20%20Optical%20fiber%20amplifiers%20are%20key%20elements%20in%20present%20optical%20networks.%0AFailures%20of%20these%20components%20result%20in%20high%20financial%20loss%20of%20income%20of%20the%0Anetwork%20operator%20as%20the%20communication%20traffic%20over%20an%20affected%20link%20is%0Ainterrupted.%20Applying%20Remaining%20useful%20lifetime%20%28RUL%29%20prediction%20in%20the%20context%0Aof%20Predictive%20Maintenance%20%28PdM%29%20to%20optical%20fiber%20amplifiers%20to%20predict%20upcoming%0Asystem%20failures%20at%20an%20early%20stage%2C%20so%20that%20network%20outages%20can%20be%20minimized%0Athrough%20planning%20of%20targeted%20maintenance%20actions%2C%20ensures%20reliability%20and%0Asafety.%20Optical%20fiber%20amplifier%20are%20complex%20systems%2C%20that%20work%20under%20various%0Aoperating%20conditions%2C%20which%20makes%20correct%20forecasting%20a%20difficult%20task.%0AIncreased%20monitoring%20capabilities%20of%20systems%20results%20in%20datasets%20that%0Afacilitate%20the%20application%20of%20data-driven%20RUL%20prediction%20methods.%20Deep%20learning%0Amodels%20in%20particular%20have%20shown%20good%20performance%2C%20but%20generalization%20based%20on%0Acomparatively%20small%20datasets%20for%20RUL%20prediction%20is%20difficult.%20In%20this%20paper%2C%20we%0Apropose%20Sparse%20Low-ranked%20self-Attention%20Transformer%20%28SLAT%29%20as%20a%20novel%20RUL%0Aprediction%20method.%20SLAT%20is%20based%20on%20an%20encoder-decoder%20architecture%2C%20wherein%0Atwo%20parallel%20working%20encoders%20extract%20features%20for%20sensors%20and%20time%20steps.%20By%0Autilizing%20the%20self-attention%20mechanism%2C%20long-term%20dependencies%20can%20be%20learned%0Afrom%20long%20sequences.%20The%20implementation%20of%20sparsity%20in%20the%20attention%20matrix%20and%0Aa%20low-rank%20parametrization%20reduce%20overfitting%20and%20increase%20generalization.%0AExperimental%20application%20to%20optical%20fiber%20amplifiers%20exemplified%20on%20EDFA%2C%20as%0Awell%20as%20a%20reference%20dataset%20from%20turbofan%20engines%2C%20shows%20that%20SLAT%20outperforms%0Athe%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14378v2&entry.124074799=Read"},
{"title": "Cottention: Linear Transformers With Cosine Attention", "author": "Gabriel Mongaras and Trevor Dohm and Eric C. Larson", "abstract": "  Attention mechanisms, particularly softmax attention, have been instrumental\nin the success of transformer-based models such as GPT. However, the quadratic\nmemory complexity of softmax attention with respect to sequence length poses\nsignificant challenges for processing longer sequences. We introduce\nCottention, a novel attention mechanism that replaces the softmax operation\nwith cosine similarity. By leveraging the properties of cosine similarity and\nrearranging the attention equation, Cottention achieves native linear memory\ncomplexity with respect to sequence length, making it inherently more\nmemory-efficient than softmax attention. We demonstrate that Cottention can be\nreformulated as a recurrent neural network (RNN) with a finite hidden state,\nallowing for constant memory usage during inference. We evaluate Cottention on\nboth the bidirectional BERT and causal GPT tasks, demonstrating comparable\nperformance to softmax attention while significantly reducing memory\nrequirements. To ensure efficient computation, we develop a custom CUDA kernel\nfor Cottention. Our results show that Cottention is a promising alternative to\nsoftmax attention, enabling the processing of longer sequences without\nsacrificing performance, due to its native linear memory complexity and ability\nto maintain a constant memory footprint during inference.\n", "link": "http://arxiv.org/abs/2409.18747v1", "date": "2024-09-27", "relevancy": 1.9635, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5387}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4727}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cottention%3A%20Linear%20Transformers%20With%20Cosine%20Attention&body=Title%3A%20Cottention%3A%20Linear%20Transformers%20With%20Cosine%20Attention%0AAuthor%3A%20Gabriel%20Mongaras%20and%20Trevor%20Dohm%20and%20Eric%20C.%20Larson%0AAbstract%3A%20%20%20Attention%20mechanisms%2C%20particularly%20softmax%20attention%2C%20have%20been%20instrumental%0Ain%20the%20success%20of%20transformer-based%20models%20such%20as%20GPT.%20However%2C%20the%20quadratic%0Amemory%20complexity%20of%20softmax%20attention%20with%20respect%20to%20sequence%20length%20poses%0Asignificant%20challenges%20for%20processing%20longer%20sequences.%20We%20introduce%0ACottention%2C%20a%20novel%20attention%20mechanism%20that%20replaces%20the%20softmax%20operation%0Awith%20cosine%20similarity.%20By%20leveraging%20the%20properties%20of%20cosine%20similarity%20and%0Arearranging%20the%20attention%20equation%2C%20Cottention%20achieves%20native%20linear%20memory%0Acomplexity%20with%20respect%20to%20sequence%20length%2C%20making%20it%20inherently%20more%0Amemory-efficient%20than%20softmax%20attention.%20We%20demonstrate%20that%20Cottention%20can%20be%0Areformulated%20as%20a%20recurrent%20neural%20network%20%28RNN%29%20with%20a%20finite%20hidden%20state%2C%0Aallowing%20for%20constant%20memory%20usage%20during%20inference.%20We%20evaluate%20Cottention%20on%0Aboth%20the%20bidirectional%20BERT%20and%20causal%20GPT%20tasks%2C%20demonstrating%20comparable%0Aperformance%20to%20softmax%20attention%20while%20significantly%20reducing%20memory%0Arequirements.%20To%20ensure%20efficient%20computation%2C%20we%20develop%20a%20custom%20CUDA%20kernel%0Afor%20Cottention.%20Our%20results%20show%20that%20Cottention%20is%20a%20promising%20alternative%20to%0Asoftmax%20attention%2C%20enabling%20the%20processing%20of%20longer%20sequences%20without%0Asacrificing%20performance%2C%20due%20to%20its%20native%20linear%20memory%20complexity%20and%20ability%0Ato%20maintain%20a%20constant%20memory%20footprint%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCottention%253A%2520Linear%2520Transformers%2520With%2520Cosine%2520Attention%26entry.906535625%3DGabriel%2520Mongaras%2520and%2520Trevor%2520Dohm%2520and%2520Eric%2520C.%2520Larson%26entry.1292438233%3D%2520%2520Attention%2520mechanisms%252C%2520particularly%2520softmax%2520attention%252C%2520have%2520been%2520instrumental%250Ain%2520the%2520success%2520of%2520transformer-based%2520models%2520such%2520as%2520GPT.%2520However%252C%2520the%2520quadratic%250Amemory%2520complexity%2520of%2520softmax%2520attention%2520with%2520respect%2520to%2520sequence%2520length%2520poses%250Asignificant%2520challenges%2520for%2520processing%2520longer%2520sequences.%2520We%2520introduce%250ACottention%252C%2520a%2520novel%2520attention%2520mechanism%2520that%2520replaces%2520the%2520softmax%2520operation%250Awith%2520cosine%2520similarity.%2520By%2520leveraging%2520the%2520properties%2520of%2520cosine%2520similarity%2520and%250Arearranging%2520the%2520attention%2520equation%252C%2520Cottention%2520achieves%2520native%2520linear%2520memory%250Acomplexity%2520with%2520respect%2520to%2520sequence%2520length%252C%2520making%2520it%2520inherently%2520more%250Amemory-efficient%2520than%2520softmax%2520attention.%2520We%2520demonstrate%2520that%2520Cottention%2520can%2520be%250Areformulated%2520as%2520a%2520recurrent%2520neural%2520network%2520%2528RNN%2529%2520with%2520a%2520finite%2520hidden%2520state%252C%250Aallowing%2520for%2520constant%2520memory%2520usage%2520during%2520inference.%2520We%2520evaluate%2520Cottention%2520on%250Aboth%2520the%2520bidirectional%2520BERT%2520and%2520causal%2520GPT%2520tasks%252C%2520demonstrating%2520comparable%250Aperformance%2520to%2520softmax%2520attention%2520while%2520significantly%2520reducing%2520memory%250Arequirements.%2520To%2520ensure%2520efficient%2520computation%252C%2520we%2520develop%2520a%2520custom%2520CUDA%2520kernel%250Afor%2520Cottention.%2520Our%2520results%2520show%2520that%2520Cottention%2520is%2520a%2520promising%2520alternative%2520to%250Asoftmax%2520attention%252C%2520enabling%2520the%2520processing%2520of%2520longer%2520sequences%2520without%250Asacrificing%2520performance%252C%2520due%2520to%2520its%2520native%2520linear%2520memory%2520complexity%2520and%2520ability%250Ato%2520maintain%2520a%2520constant%2520memory%2520footprint%2520during%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cottention%3A%20Linear%20Transformers%20With%20Cosine%20Attention&entry.906535625=Gabriel%20Mongaras%20and%20Trevor%20Dohm%20and%20Eric%20C.%20Larson&entry.1292438233=%20%20Attention%20mechanisms%2C%20particularly%20softmax%20attention%2C%20have%20been%20instrumental%0Ain%20the%20success%20of%20transformer-based%20models%20such%20as%20GPT.%20However%2C%20the%20quadratic%0Amemory%20complexity%20of%20softmax%20attention%20with%20respect%20to%20sequence%20length%20poses%0Asignificant%20challenges%20for%20processing%20longer%20sequences.%20We%20introduce%0ACottention%2C%20a%20novel%20attention%20mechanism%20that%20replaces%20the%20softmax%20operation%0Awith%20cosine%20similarity.%20By%20leveraging%20the%20properties%20of%20cosine%20similarity%20and%0Arearranging%20the%20attention%20equation%2C%20Cottention%20achieves%20native%20linear%20memory%0Acomplexity%20with%20respect%20to%20sequence%20length%2C%20making%20it%20inherently%20more%0Amemory-efficient%20than%20softmax%20attention.%20We%20demonstrate%20that%20Cottention%20can%20be%0Areformulated%20as%20a%20recurrent%20neural%20network%20%28RNN%29%20with%20a%20finite%20hidden%20state%2C%0Aallowing%20for%20constant%20memory%20usage%20during%20inference.%20We%20evaluate%20Cottention%20on%0Aboth%20the%20bidirectional%20BERT%20and%20causal%20GPT%20tasks%2C%20demonstrating%20comparable%0Aperformance%20to%20softmax%20attention%20while%20significantly%20reducing%20memory%0Arequirements.%20To%20ensure%20efficient%20computation%2C%20we%20develop%20a%20custom%20CUDA%20kernel%0Afor%20Cottention.%20Our%20results%20show%20that%20Cottention%20is%20a%20promising%20alternative%20to%0Asoftmax%20attention%2C%20enabling%20the%20processing%20of%20longer%20sequences%20without%0Asacrificing%20performance%2C%20due%20to%20its%20native%20linear%20memory%20complexity%20and%20ability%0Ato%20maintain%20a%20constant%20memory%20footprint%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18747v1&entry.124074799=Read"},
{"title": "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node\n  Classification on Text-Attributed Hypergraphs", "author": "Adri\u00e1n Bazaga and Pietro Li\u00f2 and Gos Micklem", "abstract": "  Hypergraphs are characterized by complex topological structure, representing\nhigher-order interactions among multiple entities through hyperedges. Lately,\nhypergraph-based deep learning methods to learn informative data\nrepresentations for the problem of node classification on text-attributed\nhypergraphs have garnered increasing research attention. However, existing\nmethods struggle to simultaneously capture the full extent of hypergraph\nstructural information and the rich linguistic attributes inherent in the nodes\nattributes, which largely hampers their effectiveness and generalizability. To\novercome these challenges, we explore ways to further augment a pretrained BERT\nmodel with specialized hypergraph-aware layers for the task of node\nclassification. Such layers introduce higher-order structural inductive bias\ninto the language model, thus improving the model's capacity to harness both\nhigher-order context information from the hypergraph structure and semantic\ninformation present in text. In this paper, we propose a new architecture,\nHyperBERT, a mixed text-hypergraph model which simultaneously models hypergraph\nrelational structure while maintaining the high-quality text encoding\ncapabilities of a pre-trained BERT. Notably, HyperBERT presents results that\nachieve a new state-of-the-art on five challenging text-attributed hypergraph\nnode classification benchmarks.\n", "link": "http://arxiv.org/abs/2402.07309v4", "date": "2024-09-27", "relevancy": 1.9456, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4927}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4832}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperBERT%3A%20Mixing%20Hypergraph-Aware%20Layers%20with%20Language%20Models%20for%20Node%0A%20%20Classification%20on%20Text-Attributed%20Hypergraphs&body=Title%3A%20HyperBERT%3A%20Mixing%20Hypergraph-Aware%20Layers%20with%20Language%20Models%20for%20Node%0A%20%20Classification%20on%20Text-Attributed%20Hypergraphs%0AAuthor%3A%20Adri%C3%A1n%20Bazaga%20and%20Pietro%20Li%C3%B2%20and%20Gos%20Micklem%0AAbstract%3A%20%20%20Hypergraphs%20are%20characterized%20by%20complex%20topological%20structure%2C%20representing%0Ahigher-order%20interactions%20among%20multiple%20entities%20through%20hyperedges.%20Lately%2C%0Ahypergraph-based%20deep%20learning%20methods%20to%20learn%20informative%20data%0Arepresentations%20for%20the%20problem%20of%20node%20classification%20on%20text-attributed%0Ahypergraphs%20have%20garnered%20increasing%20research%20attention.%20However%2C%20existing%0Amethods%20struggle%20to%20simultaneously%20capture%20the%20full%20extent%20of%20hypergraph%0Astructural%20information%20and%20the%20rich%20linguistic%20attributes%20inherent%20in%20the%20nodes%0Aattributes%2C%20which%20largely%20hampers%20their%20effectiveness%20and%20generalizability.%20To%0Aovercome%20these%20challenges%2C%20we%20explore%20ways%20to%20further%20augment%20a%20pretrained%20BERT%0Amodel%20with%20specialized%20hypergraph-aware%20layers%20for%20the%20task%20of%20node%0Aclassification.%20Such%20layers%20introduce%20higher-order%20structural%20inductive%20bias%0Ainto%20the%20language%20model%2C%20thus%20improving%20the%20model%27s%20capacity%20to%20harness%20both%0Ahigher-order%20context%20information%20from%20the%20hypergraph%20structure%20and%20semantic%0Ainformation%20present%20in%20text.%20In%20this%20paper%2C%20we%20propose%20a%20new%20architecture%2C%0AHyperBERT%2C%20a%20mixed%20text-hypergraph%20model%20which%20simultaneously%20models%20hypergraph%0Arelational%20structure%20while%20maintaining%20the%20high-quality%20text%20encoding%0Acapabilities%20of%20a%20pre-trained%20BERT.%20Notably%2C%20HyperBERT%20presents%20results%20that%0Aachieve%20a%20new%20state-of-the-art%20on%20five%20challenging%20text-attributed%20hypergraph%0Anode%20classification%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07309v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperBERT%253A%2520Mixing%2520Hypergraph-Aware%2520Layers%2520with%2520Language%2520Models%2520for%2520Node%250A%2520%2520Classification%2520on%2520Text-Attributed%2520Hypergraphs%26entry.906535625%3DAdri%25C3%25A1n%2520Bazaga%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Gos%2520Micklem%26entry.1292438233%3D%2520%2520Hypergraphs%2520are%2520characterized%2520by%2520complex%2520topological%2520structure%252C%2520representing%250Ahigher-order%2520interactions%2520among%2520multiple%2520entities%2520through%2520hyperedges.%2520Lately%252C%250Ahypergraph-based%2520deep%2520learning%2520methods%2520to%2520learn%2520informative%2520data%250Arepresentations%2520for%2520the%2520problem%2520of%2520node%2520classification%2520on%2520text-attributed%250Ahypergraphs%2520have%2520garnered%2520increasing%2520research%2520attention.%2520However%252C%2520existing%250Amethods%2520struggle%2520to%2520simultaneously%2520capture%2520the%2520full%2520extent%2520of%2520hypergraph%250Astructural%2520information%2520and%2520the%2520rich%2520linguistic%2520attributes%2520inherent%2520in%2520the%2520nodes%250Aattributes%252C%2520which%2520largely%2520hampers%2520their%2520effectiveness%2520and%2520generalizability.%2520To%250Aovercome%2520these%2520challenges%252C%2520we%2520explore%2520ways%2520to%2520further%2520augment%2520a%2520pretrained%2520BERT%250Amodel%2520with%2520specialized%2520hypergraph-aware%2520layers%2520for%2520the%2520task%2520of%2520node%250Aclassification.%2520Such%2520layers%2520introduce%2520higher-order%2520structural%2520inductive%2520bias%250Ainto%2520the%2520language%2520model%252C%2520thus%2520improving%2520the%2520model%2527s%2520capacity%2520to%2520harness%2520both%250Ahigher-order%2520context%2520information%2520from%2520the%2520hypergraph%2520structure%2520and%2520semantic%250Ainformation%2520present%2520in%2520text.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520architecture%252C%250AHyperBERT%252C%2520a%2520mixed%2520text-hypergraph%2520model%2520which%2520simultaneously%2520models%2520hypergraph%250Arelational%2520structure%2520while%2520maintaining%2520the%2520high-quality%2520text%2520encoding%250Acapabilities%2520of%2520a%2520pre-trained%2520BERT.%2520Notably%252C%2520HyperBERT%2520presents%2520results%2520that%250Aachieve%2520a%2520new%2520state-of-the-art%2520on%2520five%2520challenging%2520text-attributed%2520hypergraph%250Anode%2520classification%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07309v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperBERT%3A%20Mixing%20Hypergraph-Aware%20Layers%20with%20Language%20Models%20for%20Node%0A%20%20Classification%20on%20Text-Attributed%20Hypergraphs&entry.906535625=Adri%C3%A1n%20Bazaga%20and%20Pietro%20Li%C3%B2%20and%20Gos%20Micklem&entry.1292438233=%20%20Hypergraphs%20are%20characterized%20by%20complex%20topological%20structure%2C%20representing%0Ahigher-order%20interactions%20among%20multiple%20entities%20through%20hyperedges.%20Lately%2C%0Ahypergraph-based%20deep%20learning%20methods%20to%20learn%20informative%20data%0Arepresentations%20for%20the%20problem%20of%20node%20classification%20on%20text-attributed%0Ahypergraphs%20have%20garnered%20increasing%20research%20attention.%20However%2C%20existing%0Amethods%20struggle%20to%20simultaneously%20capture%20the%20full%20extent%20of%20hypergraph%0Astructural%20information%20and%20the%20rich%20linguistic%20attributes%20inherent%20in%20the%20nodes%0Aattributes%2C%20which%20largely%20hampers%20their%20effectiveness%20and%20generalizability.%20To%0Aovercome%20these%20challenges%2C%20we%20explore%20ways%20to%20further%20augment%20a%20pretrained%20BERT%0Amodel%20with%20specialized%20hypergraph-aware%20layers%20for%20the%20task%20of%20node%0Aclassification.%20Such%20layers%20introduce%20higher-order%20structural%20inductive%20bias%0Ainto%20the%20language%20model%2C%20thus%20improving%20the%20model%27s%20capacity%20to%20harness%20both%0Ahigher-order%20context%20information%20from%20the%20hypergraph%20structure%20and%20semantic%0Ainformation%20present%20in%20text.%20In%20this%20paper%2C%20we%20propose%20a%20new%20architecture%2C%0AHyperBERT%2C%20a%20mixed%20text-hypergraph%20model%20which%20simultaneously%20models%20hypergraph%0Arelational%20structure%20while%20maintaining%20the%20high-quality%20text%20encoding%0Acapabilities%20of%20a%20pre-trained%20BERT.%20Notably%2C%20HyperBERT%20presents%20results%20that%0Aachieve%20a%20new%20state-of-the-art%20on%20five%20challenging%20text-attributed%20hypergraph%0Anode%20classification%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07309v4&entry.124074799=Read"},
{"title": "KALE-LM: Unleash The Power Of AI For Science Via Knowledge And Logic\n  Enhanced Large Model", "author": "Weichen Dai and Yezeng Chen and Zijie Dai and Zhijie Huang and Yubo Liu and Yixuan Pan and Baiyang Song and Chengli Zhong and Xinhe Li and Zeyu Wang and Zhuoying Feng and Yi Zhou", "abstract": "  Artificial intelligence is gradually demonstrating its immense potential, and\nincreasing attention is being given to how AI can be harnessed to advance\nscientific research. In this vision paper, we present our perspectives on how\nAI can better assist scientific inquiry and explore corresponding technical\napproach. We have proposed and open-sourced a large model of our KALE-LM model\nseries, Llama3-KALE-LM-Chem-8B, which has achieved outstanding performance in\ntasks related to the field of chemistry. We hope that our work serves as a\nstrong starting point, helping to realize more intelligent AI and promoting the\nadvancement of human science and technology, as well as societal development.\n", "link": "http://arxiv.org/abs/2409.18695v1", "date": "2024-09-27", "relevancy": 1.943, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KALE-LM%3A%20Unleash%20The%20Power%20Of%20AI%20For%20Science%20Via%20Knowledge%20And%20Logic%0A%20%20Enhanced%20Large%20Model&body=Title%3A%20KALE-LM%3A%20Unleash%20The%20Power%20Of%20AI%20For%20Science%20Via%20Knowledge%20And%20Logic%0A%20%20Enhanced%20Large%20Model%0AAuthor%3A%20Weichen%20Dai%20and%20Yezeng%20Chen%20and%20Zijie%20Dai%20and%20Zhijie%20Huang%20and%20Yubo%20Liu%20and%20Yixuan%20Pan%20and%20Baiyang%20Song%20and%20Chengli%20Zhong%20and%20Xinhe%20Li%20and%20Zeyu%20Wang%20and%20Zhuoying%20Feng%20and%20Yi%20Zhou%0AAbstract%3A%20%20%20Artificial%20intelligence%20is%20gradually%20demonstrating%20its%20immense%20potential%2C%20and%0Aincreasing%20attention%20is%20being%20given%20to%20how%20AI%20can%20be%20harnessed%20to%20advance%0Ascientific%20research.%20In%20this%20vision%20paper%2C%20we%20present%20our%20perspectives%20on%20how%0AAI%20can%20better%20assist%20scientific%20inquiry%20and%20explore%20corresponding%20technical%0Aapproach.%20We%20have%20proposed%20and%20open-sourced%20a%20large%20model%20of%20our%20KALE-LM%20model%0Aseries%2C%20Llama3-KALE-LM-Chem-8B%2C%20which%20has%20achieved%20outstanding%20performance%20in%0Atasks%20related%20to%20the%20field%20of%20chemistry.%20We%20hope%20that%20our%20work%20serves%20as%20a%0Astrong%20starting%20point%2C%20helping%20to%20realize%20more%20intelligent%20AI%20and%20promoting%20the%0Aadvancement%20of%20human%20science%20and%20technology%2C%20as%20well%20as%20societal%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKALE-LM%253A%2520Unleash%2520The%2520Power%2520Of%2520AI%2520For%2520Science%2520Via%2520Knowledge%2520And%2520Logic%250A%2520%2520Enhanced%2520Large%2520Model%26entry.906535625%3DWeichen%2520Dai%2520and%2520Yezeng%2520Chen%2520and%2520Zijie%2520Dai%2520and%2520Zhijie%2520Huang%2520and%2520Yubo%2520Liu%2520and%2520Yixuan%2520Pan%2520and%2520Baiyang%2520Song%2520and%2520Chengli%2520Zhong%2520and%2520Xinhe%2520Li%2520and%2520Zeyu%2520Wang%2520and%2520Zhuoying%2520Feng%2520and%2520Yi%2520Zhou%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520is%2520gradually%2520demonstrating%2520its%2520immense%2520potential%252C%2520and%250Aincreasing%2520attention%2520is%2520being%2520given%2520to%2520how%2520AI%2520can%2520be%2520harnessed%2520to%2520advance%250Ascientific%2520research.%2520In%2520this%2520vision%2520paper%252C%2520we%2520present%2520our%2520perspectives%2520on%2520how%250AAI%2520can%2520better%2520assist%2520scientific%2520inquiry%2520and%2520explore%2520corresponding%2520technical%250Aapproach.%2520We%2520have%2520proposed%2520and%2520open-sourced%2520a%2520large%2520model%2520of%2520our%2520KALE-LM%2520model%250Aseries%252C%2520Llama3-KALE-LM-Chem-8B%252C%2520which%2520has%2520achieved%2520outstanding%2520performance%2520in%250Atasks%2520related%2520to%2520the%2520field%2520of%2520chemistry.%2520We%2520hope%2520that%2520our%2520work%2520serves%2520as%2520a%250Astrong%2520starting%2520point%252C%2520helping%2520to%2520realize%2520more%2520intelligent%2520AI%2520and%2520promoting%2520the%250Aadvancement%2520of%2520human%2520science%2520and%2520technology%252C%2520as%2520well%2520as%2520societal%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KALE-LM%3A%20Unleash%20The%20Power%20Of%20AI%20For%20Science%20Via%20Knowledge%20And%20Logic%0A%20%20Enhanced%20Large%20Model&entry.906535625=Weichen%20Dai%20and%20Yezeng%20Chen%20and%20Zijie%20Dai%20and%20Zhijie%20Huang%20and%20Yubo%20Liu%20and%20Yixuan%20Pan%20and%20Baiyang%20Song%20and%20Chengli%20Zhong%20and%20Xinhe%20Li%20and%20Zeyu%20Wang%20and%20Zhuoying%20Feng%20and%20Yi%20Zhou&entry.1292438233=%20%20Artificial%20intelligence%20is%20gradually%20demonstrating%20its%20immense%20potential%2C%20and%0Aincreasing%20attention%20is%20being%20given%20to%20how%20AI%20can%20be%20harnessed%20to%20advance%0Ascientific%20research.%20In%20this%20vision%20paper%2C%20we%20present%20our%20perspectives%20on%20how%0AAI%20can%20better%20assist%20scientific%20inquiry%20and%20explore%20corresponding%20technical%0Aapproach.%20We%20have%20proposed%20and%20open-sourced%20a%20large%20model%20of%20our%20KALE-LM%20model%0Aseries%2C%20Llama3-KALE-LM-Chem-8B%2C%20which%20has%20achieved%20outstanding%20performance%20in%0Atasks%20related%20to%20the%20field%20of%20chemistry.%20We%20hope%20that%20our%20work%20serves%20as%20a%0Astrong%20starting%20point%2C%20helping%20to%20realize%20more%20intelligent%20AI%20and%20promoting%20the%0Aadvancement%20of%20human%20science%20and%20technology%2C%20as%20well%20as%20societal%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18695v1&entry.124074799=Read"},
{"title": "PIM-Opt: Demystifying Distributed Optimization Algorithms on a\n  Real-World Processing-In-Memory System", "author": "Steve Rhyner and Haocong Luo and Juan G\u00f3mez-Luna and Mohammad Sadrosadati and Jiawei Jiang and Ataberk Olgun and Harshita Gupta and Ce Zhang and Onur Mutlu", "abstract": "  Modern Machine Learning (ML) training on large-scale datasets is a very\ntime-consuming workload. It relies on the optimization algorithm Stochastic\nGradient Descent (SGD) due to its effectiveness, simplicity, and generalization\nperformance. Processor-centric architectures (e.g., CPUs, GPUs) commonly used\nfor modern ML training workloads based on SGD are bottlenecked by data movement\nbetween the processor and memory units due to the poor data locality in\naccessing large datasets. As a result, processor-centric architectures suffer\nfrom low performance and high energy consumption while executing ML training\nworkloads. Processing-In-Memory (PIM) is a promising solution to alleviate the\ndata movement bottleneck by placing the computation mechanisms inside or near\nmemory.\n  Our goal is to understand the capabilities of popular distributed SGD\nalgorithms on real-world PIM systems to accelerate data-intensive ML training\nworkloads. To this end, we 1) implement several representative centralized\nparallel SGD algorithms on the real-world UPMEM PIM system, 2) rigorously\nevaluate these algorithms for ML training on large-scale datasets in terms of\nperformance, accuracy, and scalability, 3) compare to conventional CPU and GPU\nbaselines, and 4) discuss implications for future PIM hardware and highlight\nthe need for a shift to an algorithm-hardware codesign.\n  Our results demonstrate three major findings: 1) The UPMEM PIM system can be\na viable alternative to state-of-the-art CPUs and GPUs for many memory-bound ML\ntraining workloads, especially when operations and datatypes are natively\nsupported by PIM hardware, 2) it is important to carefully choose the\noptimization algorithms that best fit PIM, and 3) the UPMEM PIM system does not\nscale approximately linearly with the number of nodes for many data-intensive\nML training workloads. We open source all our code to facilitate future\nresearch.\n", "link": "http://arxiv.org/abs/2404.07164v2", "date": "2024-09-27", "relevancy": 1.9356, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.52}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4828}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIM-Opt%3A%20Demystifying%20Distributed%20Optimization%20Algorithms%20on%20a%0A%20%20Real-World%20Processing-In-Memory%20System&body=Title%3A%20PIM-Opt%3A%20Demystifying%20Distributed%20Optimization%20Algorithms%20on%20a%0A%20%20Real-World%20Processing-In-Memory%20System%0AAuthor%3A%20Steve%20Rhyner%20and%20Haocong%20Luo%20and%20Juan%20G%C3%B3mez-Luna%20and%20Mohammad%20Sadrosadati%20and%20Jiawei%20Jiang%20and%20Ataberk%20Olgun%20and%20Harshita%20Gupta%20and%20Ce%20Zhang%20and%20Onur%20Mutlu%0AAbstract%3A%20%20%20Modern%20Machine%20Learning%20%28ML%29%20training%20on%20large-scale%20datasets%20is%20a%20very%0Atime-consuming%20workload.%20It%20relies%20on%20the%20optimization%20algorithm%20Stochastic%0AGradient%20Descent%20%28SGD%29%20due%20to%20its%20effectiveness%2C%20simplicity%2C%20and%20generalization%0Aperformance.%20Processor-centric%20architectures%20%28e.g.%2C%20CPUs%2C%20GPUs%29%20commonly%20used%0Afor%20modern%20ML%20training%20workloads%20based%20on%20SGD%20are%20bottlenecked%20by%20data%20movement%0Abetween%20the%20processor%20and%20memory%20units%20due%20to%20the%20poor%20data%20locality%20in%0Aaccessing%20large%20datasets.%20As%20a%20result%2C%20processor-centric%20architectures%20suffer%0Afrom%20low%20performance%20and%20high%20energy%20consumption%20while%20executing%20ML%20training%0Aworkloads.%20Processing-In-Memory%20%28PIM%29%20is%20a%20promising%20solution%20to%20alleviate%20the%0Adata%20movement%20bottleneck%20by%20placing%20the%20computation%20mechanisms%20inside%20or%20near%0Amemory.%0A%20%20Our%20goal%20is%20to%20understand%20the%20capabilities%20of%20popular%20distributed%20SGD%0Aalgorithms%20on%20real-world%20PIM%20systems%20to%20accelerate%20data-intensive%20ML%20training%0Aworkloads.%20To%20this%20end%2C%20we%201%29%20implement%20several%20representative%20centralized%0Aparallel%20SGD%20algorithms%20on%20the%20real-world%20UPMEM%20PIM%20system%2C%202%29%20rigorously%0Aevaluate%20these%20algorithms%20for%20ML%20training%20on%20large-scale%20datasets%20in%20terms%20of%0Aperformance%2C%20accuracy%2C%20and%20scalability%2C%203%29%20compare%20to%20conventional%20CPU%20and%20GPU%0Abaselines%2C%20and%204%29%20discuss%20implications%20for%20future%20PIM%20hardware%20and%20highlight%0Athe%20need%20for%20a%20shift%20to%20an%20algorithm-hardware%20codesign.%0A%20%20Our%20results%20demonstrate%20three%20major%20findings%3A%201%29%20The%20UPMEM%20PIM%20system%20can%20be%0Aa%20viable%20alternative%20to%20state-of-the-art%20CPUs%20and%20GPUs%20for%20many%20memory-bound%20ML%0Atraining%20workloads%2C%20especially%20when%20operations%20and%20datatypes%20are%20natively%0Asupported%20by%20PIM%20hardware%2C%202%29%20it%20is%20important%20to%20carefully%20choose%20the%0Aoptimization%20algorithms%20that%20best%20fit%20PIM%2C%20and%203%29%20the%20UPMEM%20PIM%20system%20does%20not%0Ascale%20approximately%20linearly%20with%20the%20number%20of%20nodes%20for%20many%20data-intensive%0AML%20training%20workloads.%20We%20open%20source%20all%20our%20code%20to%20facilitate%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07164v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIM-Opt%253A%2520Demystifying%2520Distributed%2520Optimization%2520Algorithms%2520on%2520a%250A%2520%2520Real-World%2520Processing-In-Memory%2520System%26entry.906535625%3DSteve%2520Rhyner%2520and%2520Haocong%2520Luo%2520and%2520Juan%2520G%25C3%25B3mez-Luna%2520and%2520Mohammad%2520Sadrosadati%2520and%2520Jiawei%2520Jiang%2520and%2520Ataberk%2520Olgun%2520and%2520Harshita%2520Gupta%2520and%2520Ce%2520Zhang%2520and%2520Onur%2520Mutlu%26entry.1292438233%3D%2520%2520Modern%2520Machine%2520Learning%2520%2528ML%2529%2520training%2520on%2520large-scale%2520datasets%2520is%2520a%2520very%250Atime-consuming%2520workload.%2520It%2520relies%2520on%2520the%2520optimization%2520algorithm%2520Stochastic%250AGradient%2520Descent%2520%2528SGD%2529%2520due%2520to%2520its%2520effectiveness%252C%2520simplicity%252C%2520and%2520generalization%250Aperformance.%2520Processor-centric%2520architectures%2520%2528e.g.%252C%2520CPUs%252C%2520GPUs%2529%2520commonly%2520used%250Afor%2520modern%2520ML%2520training%2520workloads%2520based%2520on%2520SGD%2520are%2520bottlenecked%2520by%2520data%2520movement%250Abetween%2520the%2520processor%2520and%2520memory%2520units%2520due%2520to%2520the%2520poor%2520data%2520locality%2520in%250Aaccessing%2520large%2520datasets.%2520As%2520a%2520result%252C%2520processor-centric%2520architectures%2520suffer%250Afrom%2520low%2520performance%2520and%2520high%2520energy%2520consumption%2520while%2520executing%2520ML%2520training%250Aworkloads.%2520Processing-In-Memory%2520%2528PIM%2529%2520is%2520a%2520promising%2520solution%2520to%2520alleviate%2520the%250Adata%2520movement%2520bottleneck%2520by%2520placing%2520the%2520computation%2520mechanisms%2520inside%2520or%2520near%250Amemory.%250A%2520%2520Our%2520goal%2520is%2520to%2520understand%2520the%2520capabilities%2520of%2520popular%2520distributed%2520SGD%250Aalgorithms%2520on%2520real-world%2520PIM%2520systems%2520to%2520accelerate%2520data-intensive%2520ML%2520training%250Aworkloads.%2520To%2520this%2520end%252C%2520we%25201%2529%2520implement%2520several%2520representative%2520centralized%250Aparallel%2520SGD%2520algorithms%2520on%2520the%2520real-world%2520UPMEM%2520PIM%2520system%252C%25202%2529%2520rigorously%250Aevaluate%2520these%2520algorithms%2520for%2520ML%2520training%2520on%2520large-scale%2520datasets%2520in%2520terms%2520of%250Aperformance%252C%2520accuracy%252C%2520and%2520scalability%252C%25203%2529%2520compare%2520to%2520conventional%2520CPU%2520and%2520GPU%250Abaselines%252C%2520and%25204%2529%2520discuss%2520implications%2520for%2520future%2520PIM%2520hardware%2520and%2520highlight%250Athe%2520need%2520for%2520a%2520shift%2520to%2520an%2520algorithm-hardware%2520codesign.%250A%2520%2520Our%2520results%2520demonstrate%2520three%2520major%2520findings%253A%25201%2529%2520The%2520UPMEM%2520PIM%2520system%2520can%2520be%250Aa%2520viable%2520alternative%2520to%2520state-of-the-art%2520CPUs%2520and%2520GPUs%2520for%2520many%2520memory-bound%2520ML%250Atraining%2520workloads%252C%2520especially%2520when%2520operations%2520and%2520datatypes%2520are%2520natively%250Asupported%2520by%2520PIM%2520hardware%252C%25202%2529%2520it%2520is%2520important%2520to%2520carefully%2520choose%2520the%250Aoptimization%2520algorithms%2520that%2520best%2520fit%2520PIM%252C%2520and%25203%2529%2520the%2520UPMEM%2520PIM%2520system%2520does%2520not%250Ascale%2520approximately%2520linearly%2520with%2520the%2520number%2520of%2520nodes%2520for%2520many%2520data-intensive%250AML%2520training%2520workloads.%2520We%2520open%2520source%2520all%2520our%2520code%2520to%2520facilitate%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07164v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIM-Opt%3A%20Demystifying%20Distributed%20Optimization%20Algorithms%20on%20a%0A%20%20Real-World%20Processing-In-Memory%20System&entry.906535625=Steve%20Rhyner%20and%20Haocong%20Luo%20and%20Juan%20G%C3%B3mez-Luna%20and%20Mohammad%20Sadrosadati%20and%20Jiawei%20Jiang%20and%20Ataberk%20Olgun%20and%20Harshita%20Gupta%20and%20Ce%20Zhang%20and%20Onur%20Mutlu&entry.1292438233=%20%20Modern%20Machine%20Learning%20%28ML%29%20training%20on%20large-scale%20datasets%20is%20a%20very%0Atime-consuming%20workload.%20It%20relies%20on%20the%20optimization%20algorithm%20Stochastic%0AGradient%20Descent%20%28SGD%29%20due%20to%20its%20effectiveness%2C%20simplicity%2C%20and%20generalization%0Aperformance.%20Processor-centric%20architectures%20%28e.g.%2C%20CPUs%2C%20GPUs%29%20commonly%20used%0Afor%20modern%20ML%20training%20workloads%20based%20on%20SGD%20are%20bottlenecked%20by%20data%20movement%0Abetween%20the%20processor%20and%20memory%20units%20due%20to%20the%20poor%20data%20locality%20in%0Aaccessing%20large%20datasets.%20As%20a%20result%2C%20processor-centric%20architectures%20suffer%0Afrom%20low%20performance%20and%20high%20energy%20consumption%20while%20executing%20ML%20training%0Aworkloads.%20Processing-In-Memory%20%28PIM%29%20is%20a%20promising%20solution%20to%20alleviate%20the%0Adata%20movement%20bottleneck%20by%20placing%20the%20computation%20mechanisms%20inside%20or%20near%0Amemory.%0A%20%20Our%20goal%20is%20to%20understand%20the%20capabilities%20of%20popular%20distributed%20SGD%0Aalgorithms%20on%20real-world%20PIM%20systems%20to%20accelerate%20data-intensive%20ML%20training%0Aworkloads.%20To%20this%20end%2C%20we%201%29%20implement%20several%20representative%20centralized%0Aparallel%20SGD%20algorithms%20on%20the%20real-world%20UPMEM%20PIM%20system%2C%202%29%20rigorously%0Aevaluate%20these%20algorithms%20for%20ML%20training%20on%20large-scale%20datasets%20in%20terms%20of%0Aperformance%2C%20accuracy%2C%20and%20scalability%2C%203%29%20compare%20to%20conventional%20CPU%20and%20GPU%0Abaselines%2C%20and%204%29%20discuss%20implications%20for%20future%20PIM%20hardware%20and%20highlight%0Athe%20need%20for%20a%20shift%20to%20an%20algorithm-hardware%20codesign.%0A%20%20Our%20results%20demonstrate%20three%20major%20findings%3A%201%29%20The%20UPMEM%20PIM%20system%20can%20be%0Aa%20viable%20alternative%20to%20state-of-the-art%20CPUs%20and%20GPUs%20for%20many%20memory-bound%20ML%0Atraining%20workloads%2C%20especially%20when%20operations%20and%20datatypes%20are%20natively%0Asupported%20by%20PIM%20hardware%2C%202%29%20it%20is%20important%20to%20carefully%20choose%20the%0Aoptimization%20algorithms%20that%20best%20fit%20PIM%2C%20and%203%29%20the%20UPMEM%20PIM%20system%20does%20not%0Ascale%20approximately%20linearly%20with%20the%20number%20of%20nodes%20for%20many%20data-intensive%0AML%20training%20workloads.%20We%20open%20source%20all%20our%20code%20to%20facilitate%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07164v2&entry.124074799=Read"},
{"title": "LW2G: Learning Whether to Grow for Prompt-based Continual Learning", "author": "Qian Feng and Dawei Zhou and Hanbin Zhao and Chao Zhang and Hui Qian", "abstract": "  Continual Learning (CL) aims to learn in non-stationary scenarios,\nprogressively acquiring and maintaining knowledge from sequential tasks. Recent\nPrompt-based Continual Learning (PCL) has achieved remarkable performance with\nPre-Trained Models (PTMs). These approaches grow a prompt sets pool by adding a\nnew set of prompts when learning each new task (\\emph{prompt learning}) and\nadopt a matching mechanism to select the correct set for each testing sample\n(\\emph{prompt retrieval}). Previous studies focus on the latter stage by\nimproving the matching mechanism to enhance Prompt Retrieval Accuracy (PRA). To\npromote cross-task knowledge facilitation and form an effective and efficient\nprompt sets pool, we propose a plug-in module in the former stage to\n\\textbf{Learn Whether to Grow (LW2G)} based on the disparities between tasks.\nSpecifically, a shared set of prompts is utilized when several tasks share\ncertain commonalities, and a new set is added when there are significant\ndifferences between the new task and previous tasks. Inspired by Gradient\nProjection Continual Learning, our LW2G develops a metric called Hinder Forward\nCapability (HFC) to measure the hindrance imposed on learning new tasks by\nsurgically modifying the original gradient onto the orthogonal complement of\nthe old feature space. With HFC, an automated scheme Dynamic Growing Approach\nadaptively learns whether to grow with a dynamic threshold. Furthermore, we\ndesign a gradient-based constraint to ensure the consistency between the\nupdating prompts and pre-trained knowledge, and a prompts weights reusing\nstrategy to enhance forward transfer. Extensive experiments show the\neffectiveness of our method. The source codes are available at\n\\url{https://github.com/RAIAN08/LW2G}.\n", "link": "http://arxiv.org/abs/2409.18860v1", "date": "2024-09-27", "relevancy": 1.9354, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4863}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4851}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LW2G%3A%20Learning%20Whether%20to%20Grow%20for%20Prompt-based%20Continual%20Learning&body=Title%3A%20LW2G%3A%20Learning%20Whether%20to%20Grow%20for%20Prompt-based%20Continual%20Learning%0AAuthor%3A%20Qian%20Feng%20and%20Dawei%20Zhou%20and%20Hanbin%20Zhao%20and%20Chao%20Zhang%20and%20Hui%20Qian%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20aims%20to%20learn%20in%20non-stationary%20scenarios%2C%0Aprogressively%20acquiring%20and%20maintaining%20knowledge%20from%20sequential%20tasks.%20Recent%0APrompt-based%20Continual%20Learning%20%28PCL%29%20has%20achieved%20remarkable%20performance%20with%0APre-Trained%20Models%20%28PTMs%29.%20These%20approaches%20grow%20a%20prompt%20sets%20pool%20by%20adding%20a%0Anew%20set%20of%20prompts%20when%20learning%20each%20new%20task%20%28%5Cemph%7Bprompt%20learning%7D%29%20and%0Aadopt%20a%20matching%20mechanism%20to%20select%20the%20correct%20set%20for%20each%20testing%20sample%0A%28%5Cemph%7Bprompt%20retrieval%7D%29.%20Previous%20studies%20focus%20on%20the%20latter%20stage%20by%0Aimproving%20the%20matching%20mechanism%20to%20enhance%20Prompt%20Retrieval%20Accuracy%20%28PRA%29.%20To%0Apromote%20cross-task%20knowledge%20facilitation%20and%20form%20an%20effective%20and%20efficient%0Aprompt%20sets%20pool%2C%20we%20propose%20a%20plug-in%20module%20in%20the%20former%20stage%20to%0A%5Ctextbf%7BLearn%20Whether%20to%20Grow%20%28LW2G%29%7D%20based%20on%20the%20disparities%20between%20tasks.%0ASpecifically%2C%20a%20shared%20set%20of%20prompts%20is%20utilized%20when%20several%20tasks%20share%0Acertain%20commonalities%2C%20and%20a%20new%20set%20is%20added%20when%20there%20are%20significant%0Adifferences%20between%20the%20new%20task%20and%20previous%20tasks.%20Inspired%20by%20Gradient%0AProjection%20Continual%20Learning%2C%20our%20LW2G%20develops%20a%20metric%20called%20Hinder%20Forward%0ACapability%20%28HFC%29%20to%20measure%20the%20hindrance%20imposed%20on%20learning%20new%20tasks%20by%0Asurgically%20modifying%20the%20original%20gradient%20onto%20the%20orthogonal%20complement%20of%0Athe%20old%20feature%20space.%20With%20HFC%2C%20an%20automated%20scheme%20Dynamic%20Growing%20Approach%0Aadaptively%20learns%20whether%20to%20grow%20with%20a%20dynamic%20threshold.%20Furthermore%2C%20we%0Adesign%20a%20gradient-based%20constraint%20to%20ensure%20the%20consistency%20between%20the%0Aupdating%20prompts%20and%20pre-trained%20knowledge%2C%20and%20a%20prompts%20weights%20reusing%0Astrategy%20to%20enhance%20forward%20transfer.%20Extensive%20experiments%20show%20the%0Aeffectiveness%20of%20our%20method.%20The%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/RAIAN08/LW2G%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLW2G%253A%2520Learning%2520Whether%2520to%2520Grow%2520for%2520Prompt-based%2520Continual%2520Learning%26entry.906535625%3DQian%2520Feng%2520and%2520Dawei%2520Zhou%2520and%2520Hanbin%2520Zhao%2520and%2520Chao%2520Zhang%2520and%2520Hui%2520Qian%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520aims%2520to%2520learn%2520in%2520non-stationary%2520scenarios%252C%250Aprogressively%2520acquiring%2520and%2520maintaining%2520knowledge%2520from%2520sequential%2520tasks.%2520Recent%250APrompt-based%2520Continual%2520Learning%2520%2528PCL%2529%2520has%2520achieved%2520remarkable%2520performance%2520with%250APre-Trained%2520Models%2520%2528PTMs%2529.%2520These%2520approaches%2520grow%2520a%2520prompt%2520sets%2520pool%2520by%2520adding%2520a%250Anew%2520set%2520of%2520prompts%2520when%2520learning%2520each%2520new%2520task%2520%2528%255Cemph%257Bprompt%2520learning%257D%2529%2520and%250Aadopt%2520a%2520matching%2520mechanism%2520to%2520select%2520the%2520correct%2520set%2520for%2520each%2520testing%2520sample%250A%2528%255Cemph%257Bprompt%2520retrieval%257D%2529.%2520Previous%2520studies%2520focus%2520on%2520the%2520latter%2520stage%2520by%250Aimproving%2520the%2520matching%2520mechanism%2520to%2520enhance%2520Prompt%2520Retrieval%2520Accuracy%2520%2528PRA%2529.%2520To%250Apromote%2520cross-task%2520knowledge%2520facilitation%2520and%2520form%2520an%2520effective%2520and%2520efficient%250Aprompt%2520sets%2520pool%252C%2520we%2520propose%2520a%2520plug-in%2520module%2520in%2520the%2520former%2520stage%2520to%250A%255Ctextbf%257BLearn%2520Whether%2520to%2520Grow%2520%2528LW2G%2529%257D%2520based%2520on%2520the%2520disparities%2520between%2520tasks.%250ASpecifically%252C%2520a%2520shared%2520set%2520of%2520prompts%2520is%2520utilized%2520when%2520several%2520tasks%2520share%250Acertain%2520commonalities%252C%2520and%2520a%2520new%2520set%2520is%2520added%2520when%2520there%2520are%2520significant%250Adifferences%2520between%2520the%2520new%2520task%2520and%2520previous%2520tasks.%2520Inspired%2520by%2520Gradient%250AProjection%2520Continual%2520Learning%252C%2520our%2520LW2G%2520develops%2520a%2520metric%2520called%2520Hinder%2520Forward%250ACapability%2520%2528HFC%2529%2520to%2520measure%2520the%2520hindrance%2520imposed%2520on%2520learning%2520new%2520tasks%2520by%250Asurgically%2520modifying%2520the%2520original%2520gradient%2520onto%2520the%2520orthogonal%2520complement%2520of%250Athe%2520old%2520feature%2520space.%2520With%2520HFC%252C%2520an%2520automated%2520scheme%2520Dynamic%2520Growing%2520Approach%250Aadaptively%2520learns%2520whether%2520to%2520grow%2520with%2520a%2520dynamic%2520threshold.%2520Furthermore%252C%2520we%250Adesign%2520a%2520gradient-based%2520constraint%2520to%2520ensure%2520the%2520consistency%2520between%2520the%250Aupdating%2520prompts%2520and%2520pre-trained%2520knowledge%252C%2520and%2520a%2520prompts%2520weights%2520reusing%250Astrategy%2520to%2520enhance%2520forward%2520transfer.%2520Extensive%2520experiments%2520show%2520the%250Aeffectiveness%2520of%2520our%2520method.%2520The%2520source%2520codes%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/RAIAN08/LW2G%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LW2G%3A%20Learning%20Whether%20to%20Grow%20for%20Prompt-based%20Continual%20Learning&entry.906535625=Qian%20Feng%20and%20Dawei%20Zhou%20and%20Hanbin%20Zhao%20and%20Chao%20Zhang%20and%20Hui%20Qian&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20aims%20to%20learn%20in%20non-stationary%20scenarios%2C%0Aprogressively%20acquiring%20and%20maintaining%20knowledge%20from%20sequential%20tasks.%20Recent%0APrompt-based%20Continual%20Learning%20%28PCL%29%20has%20achieved%20remarkable%20performance%20with%0APre-Trained%20Models%20%28PTMs%29.%20These%20approaches%20grow%20a%20prompt%20sets%20pool%20by%20adding%20a%0Anew%20set%20of%20prompts%20when%20learning%20each%20new%20task%20%28%5Cemph%7Bprompt%20learning%7D%29%20and%0Aadopt%20a%20matching%20mechanism%20to%20select%20the%20correct%20set%20for%20each%20testing%20sample%0A%28%5Cemph%7Bprompt%20retrieval%7D%29.%20Previous%20studies%20focus%20on%20the%20latter%20stage%20by%0Aimproving%20the%20matching%20mechanism%20to%20enhance%20Prompt%20Retrieval%20Accuracy%20%28PRA%29.%20To%0Apromote%20cross-task%20knowledge%20facilitation%20and%20form%20an%20effective%20and%20efficient%0Aprompt%20sets%20pool%2C%20we%20propose%20a%20plug-in%20module%20in%20the%20former%20stage%20to%0A%5Ctextbf%7BLearn%20Whether%20to%20Grow%20%28LW2G%29%7D%20based%20on%20the%20disparities%20between%20tasks.%0ASpecifically%2C%20a%20shared%20set%20of%20prompts%20is%20utilized%20when%20several%20tasks%20share%0Acertain%20commonalities%2C%20and%20a%20new%20set%20is%20added%20when%20there%20are%20significant%0Adifferences%20between%20the%20new%20task%20and%20previous%20tasks.%20Inspired%20by%20Gradient%0AProjection%20Continual%20Learning%2C%20our%20LW2G%20develops%20a%20metric%20called%20Hinder%20Forward%0ACapability%20%28HFC%29%20to%20measure%20the%20hindrance%20imposed%20on%20learning%20new%20tasks%20by%0Asurgically%20modifying%20the%20original%20gradient%20onto%20the%20orthogonal%20complement%20of%0Athe%20old%20feature%20space.%20With%20HFC%2C%20an%20automated%20scheme%20Dynamic%20Growing%20Approach%0Aadaptively%20learns%20whether%20to%20grow%20with%20a%20dynamic%20threshold.%20Furthermore%2C%20we%0Adesign%20a%20gradient-based%20constraint%20to%20ensure%20the%20consistency%20between%20the%0Aupdating%20prompts%20and%20pre-trained%20knowledge%2C%20and%20a%20prompts%20weights%20reusing%0Astrategy%20to%20enhance%20forward%20transfer.%20Extensive%20experiments%20show%20the%0Aeffectiveness%20of%20our%20method.%20The%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/RAIAN08/LW2G%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18860v1&entry.124074799=Read"},
{"title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific\n  Synthesis", "author": "Hamed Babaei Giglou and Jennifer D'Souza and S\u00f6ren Auer", "abstract": "  In response to the growing complexity and volume of scientific literature,\nthis paper introduces the LLMs4Synthesis framework, designed to enhance the\ncapabilities of Large Language Models (LLMs) in generating high-quality\nscientific syntheses. This framework addresses the need for rapid, coherent,\nand contextually rich integration of scientific insights, leveraging both\nopen-source and proprietary LLMs. It also examines the effectiveness of LLMs in\nevaluating the integrity and reliability of these syntheses, alleviating\ninadequacies in current quantitative metrics. Our study contributes to this\nfield by developing a novel methodology for processing scientific papers,\ndefining new synthesis types, and establishing nine detailed quality criteria\nfor evaluating syntheses. The integration of LLMs with reinforcement learning\nand AI feedback is proposed to optimize synthesis quality, ensuring alignment\nwith established criteria. The LLMs4Synthesis framework and its components are\nmade available, promising to enhance both the generation and evaluation\nprocesses in scientific research synthesis.\n", "link": "http://arxiv.org/abs/2409.18812v1", "date": "2024-09-27", "relevancy": 1.9325, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs4Synthesis%3A%20Leveraging%20Large%20Language%20Models%20for%20Scientific%0A%20%20Synthesis&body=Title%3A%20LLMs4Synthesis%3A%20Leveraging%20Large%20Language%20Models%20for%20Scientific%0A%20%20Synthesis%0AAuthor%3A%20Hamed%20Babaei%20Giglou%20and%20Jennifer%20D%27Souza%20and%20S%C3%B6ren%20Auer%0AAbstract%3A%20%20%20In%20response%20to%20the%20growing%20complexity%20and%20volume%20of%20scientific%20literature%2C%0Athis%20paper%20introduces%20the%20LLMs4Synthesis%20framework%2C%20designed%20to%20enhance%20the%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20generating%20high-quality%0Ascientific%20syntheses.%20This%20framework%20addresses%20the%20need%20for%20rapid%2C%20coherent%2C%0Aand%20contextually%20rich%20integration%20of%20scientific%20insights%2C%20leveraging%20both%0Aopen-source%20and%20proprietary%20LLMs.%20It%20also%20examines%20the%20effectiveness%20of%20LLMs%20in%0Aevaluating%20the%20integrity%20and%20reliability%20of%20these%20syntheses%2C%20alleviating%0Ainadequacies%20in%20current%20quantitative%20metrics.%20Our%20study%20contributes%20to%20this%0Afield%20by%20developing%20a%20novel%20methodology%20for%20processing%20scientific%20papers%2C%0Adefining%20new%20synthesis%20types%2C%20and%20establishing%20nine%20detailed%20quality%20criteria%0Afor%20evaluating%20syntheses.%20The%20integration%20of%20LLMs%20with%20reinforcement%20learning%0Aand%20AI%20feedback%20is%20proposed%20to%20optimize%20synthesis%20quality%2C%20ensuring%20alignment%0Awith%20established%20criteria.%20The%20LLMs4Synthesis%20framework%20and%20its%20components%20are%0Amade%20available%2C%20promising%20to%20enhance%20both%20the%20generation%20and%20evaluation%0Aprocesses%20in%20scientific%20research%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs4Synthesis%253A%2520Leveraging%2520Large%2520Language%2520Models%2520for%2520Scientific%250A%2520%2520Synthesis%26entry.906535625%3DHamed%2520Babaei%2520Giglou%2520and%2520Jennifer%2520D%2527Souza%2520and%2520S%25C3%25B6ren%2520Auer%26entry.1292438233%3D%2520%2520In%2520response%2520to%2520the%2520growing%2520complexity%2520and%2520volume%2520of%2520scientific%2520literature%252C%250Athis%2520paper%2520introduces%2520the%2520LLMs4Synthesis%2520framework%252C%2520designed%2520to%2520enhance%2520the%250Acapabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520generating%2520high-quality%250Ascientific%2520syntheses.%2520This%2520framework%2520addresses%2520the%2520need%2520for%2520rapid%252C%2520coherent%252C%250Aand%2520contextually%2520rich%2520integration%2520of%2520scientific%2520insights%252C%2520leveraging%2520both%250Aopen-source%2520and%2520proprietary%2520LLMs.%2520It%2520also%2520examines%2520the%2520effectiveness%2520of%2520LLMs%2520in%250Aevaluating%2520the%2520integrity%2520and%2520reliability%2520of%2520these%2520syntheses%252C%2520alleviating%250Ainadequacies%2520in%2520current%2520quantitative%2520metrics.%2520Our%2520study%2520contributes%2520to%2520this%250Afield%2520by%2520developing%2520a%2520novel%2520methodology%2520for%2520processing%2520scientific%2520papers%252C%250Adefining%2520new%2520synthesis%2520types%252C%2520and%2520establishing%2520nine%2520detailed%2520quality%2520criteria%250Afor%2520evaluating%2520syntheses.%2520The%2520integration%2520of%2520LLMs%2520with%2520reinforcement%2520learning%250Aand%2520AI%2520feedback%2520is%2520proposed%2520to%2520optimize%2520synthesis%2520quality%252C%2520ensuring%2520alignment%250Awith%2520established%2520criteria.%2520The%2520LLMs4Synthesis%2520framework%2520and%2520its%2520components%2520are%250Amade%2520available%252C%2520promising%2520to%2520enhance%2520both%2520the%2520generation%2520and%2520evaluation%250Aprocesses%2520in%2520scientific%2520research%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs4Synthesis%3A%20Leveraging%20Large%20Language%20Models%20for%20Scientific%0A%20%20Synthesis&entry.906535625=Hamed%20Babaei%20Giglou%20and%20Jennifer%20D%27Souza%20and%20S%C3%B6ren%20Auer&entry.1292438233=%20%20In%20response%20to%20the%20growing%20complexity%20and%20volume%20of%20scientific%20literature%2C%0Athis%20paper%20introduces%20the%20LLMs4Synthesis%20framework%2C%20designed%20to%20enhance%20the%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20generating%20high-quality%0Ascientific%20syntheses.%20This%20framework%20addresses%20the%20need%20for%20rapid%2C%20coherent%2C%0Aand%20contextually%20rich%20integration%20of%20scientific%20insights%2C%20leveraging%20both%0Aopen-source%20and%20proprietary%20LLMs.%20It%20also%20examines%20the%20effectiveness%20of%20LLMs%20in%0Aevaluating%20the%20integrity%20and%20reliability%20of%20these%20syntheses%2C%20alleviating%0Ainadequacies%20in%20current%20quantitative%20metrics.%20Our%20study%20contributes%20to%20this%0Afield%20by%20developing%20a%20novel%20methodology%20for%20processing%20scientific%20papers%2C%0Adefining%20new%20synthesis%20types%2C%20and%20establishing%20nine%20detailed%20quality%20criteria%0Afor%20evaluating%20syntheses.%20The%20integration%20of%20LLMs%20with%20reinforcement%20learning%0Aand%20AI%20feedback%20is%20proposed%20to%20optimize%20synthesis%20quality%2C%20ensuring%20alignment%0Awith%20established%20criteria.%20The%20LLMs4Synthesis%20framework%20and%20its%20components%20are%0Amade%20available%2C%20promising%20to%20enhance%20both%20the%20generation%20and%20evaluation%0Aprocesses%20in%20scientific%20research%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18812v1&entry.124074799=Read"},
{"title": "Early diagnosis of Alzheimer's disease from MRI images with deep\n  learning model", "author": "Sajjad Aghasi Javid and Mahmood Mohassel Feghhi", "abstract": "  It is acknowledged that the most common cause of dementia worldwide is\nAlzheimer's disease (AD). This condition progresses in severity from mild to\nsevere and interferes with people's everyday routines. Early diagnosis plays a\ncritical role in patient care and clinical trials. Convolutional neural\nnetworks (CNN) are used to create a framework for identifying specific disease\nfeatures from MRI scans Classification of dementia involves approaches such as\nmedical history review, neuropsychological tests, and magnetic resonance\nimaging (MRI). However, the image dataset obtained from Kaggle faces a\nsignificant issue of class imbalance, which requires equal distribution of\nsamples from each class to address. In this article, to address this imbalance,\nthe Synthetic Minority Oversampling Technique (SMOTE) is utilized. Furthermore,\na pre-trained convolutional neural network has been applied to the DEMNET\ndementia network to extract key features from AD images. The proposed model\nachieved an impressive accuracy of 98.67%.\n", "link": "http://arxiv.org/abs/2409.18814v1", "date": "2024-09-27", "relevancy": 1.9283, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4849}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4832}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20diagnosis%20of%20Alzheimer%27s%20disease%20from%20MRI%20images%20with%20deep%0A%20%20learning%20model&body=Title%3A%20Early%20diagnosis%20of%20Alzheimer%27s%20disease%20from%20MRI%20images%20with%20deep%0A%20%20learning%20model%0AAuthor%3A%20Sajjad%20Aghasi%20Javid%20and%20Mahmood%20Mohassel%20Feghhi%0AAbstract%3A%20%20%20It%20is%20acknowledged%20that%20the%20most%20common%20cause%20of%20dementia%20worldwide%20is%0AAlzheimer%27s%20disease%20%28AD%29.%20This%20condition%20progresses%20in%20severity%20from%20mild%20to%0Asevere%20and%20interferes%20with%20people%27s%20everyday%20routines.%20Early%20diagnosis%20plays%20a%0Acritical%20role%20in%20patient%20care%20and%20clinical%20trials.%20Convolutional%20neural%0Anetworks%20%28CNN%29%20are%20used%20to%20create%20a%20framework%20for%20identifying%20specific%20disease%0Afeatures%20from%20MRI%20scans%20Classification%20of%20dementia%20involves%20approaches%20such%20as%0Amedical%20history%20review%2C%20neuropsychological%20tests%2C%20and%20magnetic%20resonance%0Aimaging%20%28MRI%29.%20However%2C%20the%20image%20dataset%20obtained%20from%20Kaggle%20faces%20a%0Asignificant%20issue%20of%20class%20imbalance%2C%20which%20requires%20equal%20distribution%20of%0Asamples%20from%20each%20class%20to%20address.%20In%20this%20article%2C%20to%20address%20this%20imbalance%2C%0Athe%20Synthetic%20Minority%20Oversampling%20Technique%20%28SMOTE%29%20is%20utilized.%20Furthermore%2C%0Aa%20pre-trained%20convolutional%20neural%20network%20has%20been%20applied%20to%20the%20DEMNET%0Adementia%20network%20to%20extract%20key%20features%20from%20AD%20images.%20The%20proposed%20model%0Aachieved%20an%20impressive%20accuracy%20of%2098.67%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520diagnosis%2520of%2520Alzheimer%2527s%2520disease%2520from%2520MRI%2520images%2520with%2520deep%250A%2520%2520learning%2520model%26entry.906535625%3DSajjad%2520Aghasi%2520Javid%2520and%2520Mahmood%2520Mohassel%2520Feghhi%26entry.1292438233%3D%2520%2520It%2520is%2520acknowledged%2520that%2520the%2520most%2520common%2520cause%2520of%2520dementia%2520worldwide%2520is%250AAlzheimer%2527s%2520disease%2520%2528AD%2529.%2520This%2520condition%2520progresses%2520in%2520severity%2520from%2520mild%2520to%250Asevere%2520and%2520interferes%2520with%2520people%2527s%2520everyday%2520routines.%2520Early%2520diagnosis%2520plays%2520a%250Acritical%2520role%2520in%2520patient%2520care%2520and%2520clinical%2520trials.%2520Convolutional%2520neural%250Anetworks%2520%2528CNN%2529%2520are%2520used%2520to%2520create%2520a%2520framework%2520for%2520identifying%2520specific%2520disease%250Afeatures%2520from%2520MRI%2520scans%2520Classification%2520of%2520dementia%2520involves%2520approaches%2520such%2520as%250Amedical%2520history%2520review%252C%2520neuropsychological%2520tests%252C%2520and%2520magnetic%2520resonance%250Aimaging%2520%2528MRI%2529.%2520However%252C%2520the%2520image%2520dataset%2520obtained%2520from%2520Kaggle%2520faces%2520a%250Asignificant%2520issue%2520of%2520class%2520imbalance%252C%2520which%2520requires%2520equal%2520distribution%2520of%250Asamples%2520from%2520each%2520class%2520to%2520address.%2520In%2520this%2520article%252C%2520to%2520address%2520this%2520imbalance%252C%250Athe%2520Synthetic%2520Minority%2520Oversampling%2520Technique%2520%2528SMOTE%2529%2520is%2520utilized.%2520Furthermore%252C%250Aa%2520pre-trained%2520convolutional%2520neural%2520network%2520has%2520been%2520applied%2520to%2520the%2520DEMNET%250Adementia%2520network%2520to%2520extract%2520key%2520features%2520from%2520AD%2520images.%2520The%2520proposed%2520model%250Aachieved%2520an%2520impressive%2520accuracy%2520of%252098.67%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20diagnosis%20of%20Alzheimer%27s%20disease%20from%20MRI%20images%20with%20deep%0A%20%20learning%20model&entry.906535625=Sajjad%20Aghasi%20Javid%20and%20Mahmood%20Mohassel%20Feghhi&entry.1292438233=%20%20It%20is%20acknowledged%20that%20the%20most%20common%20cause%20of%20dementia%20worldwide%20is%0AAlzheimer%27s%20disease%20%28AD%29.%20This%20condition%20progresses%20in%20severity%20from%20mild%20to%0Asevere%20and%20interferes%20with%20people%27s%20everyday%20routines.%20Early%20diagnosis%20plays%20a%0Acritical%20role%20in%20patient%20care%20and%20clinical%20trials.%20Convolutional%20neural%0Anetworks%20%28CNN%29%20are%20used%20to%20create%20a%20framework%20for%20identifying%20specific%20disease%0Afeatures%20from%20MRI%20scans%20Classification%20of%20dementia%20involves%20approaches%20such%20as%0Amedical%20history%20review%2C%20neuropsychological%20tests%2C%20and%20magnetic%20resonance%0Aimaging%20%28MRI%29.%20However%2C%20the%20image%20dataset%20obtained%20from%20Kaggle%20faces%20a%0Asignificant%20issue%20of%20class%20imbalance%2C%20which%20requires%20equal%20distribution%20of%0Asamples%20from%20each%20class%20to%20address.%20In%20this%20article%2C%20to%20address%20this%20imbalance%2C%0Athe%20Synthetic%20Minority%20Oversampling%20Technique%20%28SMOTE%29%20is%20utilized.%20Furthermore%2C%0Aa%20pre-trained%20convolutional%20neural%20network%20has%20been%20applied%20to%20the%20DEMNET%0Adementia%20network%20to%20extract%20key%20features%20from%20AD%20images.%20The%20proposed%20model%0Aachieved%20an%20impressive%20accuracy%20of%2098.67%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18814v1&entry.124074799=Read"},
{"title": "In-depth Analysis of Privacy Threats in Federated Learning for Medical\n  Data", "author": "Badhan Chandra Das and M. Hadi Amini and Yanzhao Wu", "abstract": "  Federated learning is emerging as a promising machine learning technique in\nthe medical field for analyzing medical images, as it is considered an\neffective method to safeguard sensitive patient data and comply with privacy\nregulations. However, recent studies have revealed that the default settings of\nfederated learning may inadvertently expose private training data to privacy\nattacks. Thus, the intensity of such privacy risks and potential mitigation\nstrategies in the medical domain remain unclear. In this paper, we make three\noriginal contributions to privacy risk analysis and mitigation in federated\nlearning for medical data. First, we propose a holistic framework, MedPFL, for\nanalyzing privacy risks in processing medical data in the federated learning\nenvironment and developing effective mitigation strategies for protecting\nprivacy. Second, through our empirical analysis, we demonstrate the severe\nprivacy risks in federated learning to process medical images, where\nadversaries can accurately reconstruct private medical images by performing\nprivacy attacks. Third, we illustrate that the prevalent defense mechanism of\nadding random noises may not always be effective in protecting medical images\nagainst privacy attacks in federated learning, which poses unique and pressing\nchallenges related to protecting the privacy of medical data. Furthermore, the\npaper discusses several unique research questions related to the privacy\nprotection of medical data in the federated learning environment. We conduct\nextensive experiments on several benchmark medical image datasets to analyze\nand mitigate the privacy risks associated with federated learning for medical\ndata.\n", "link": "http://arxiv.org/abs/2409.18907v1", "date": "2024-09-27", "relevancy": 1.9146, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4884}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-depth%20Analysis%20of%20Privacy%20Threats%20in%20Federated%20Learning%20for%20Medical%0A%20%20Data&body=Title%3A%20In-depth%20Analysis%20of%20Privacy%20Threats%20in%20Federated%20Learning%20for%20Medical%0A%20%20Data%0AAuthor%3A%20Badhan%20Chandra%20Das%20and%20M.%20Hadi%20Amini%20and%20Yanzhao%20Wu%0AAbstract%3A%20%20%20Federated%20learning%20is%20emerging%20as%20a%20promising%20machine%20learning%20technique%20in%0Athe%20medical%20field%20for%20analyzing%20medical%20images%2C%20as%20it%20is%20considered%20an%0Aeffective%20method%20to%20safeguard%20sensitive%20patient%20data%20and%20comply%20with%20privacy%0Aregulations.%20However%2C%20recent%20studies%20have%20revealed%20that%20the%20default%20settings%20of%0Afederated%20learning%20may%20inadvertently%20expose%20private%20training%20data%20to%20privacy%0Aattacks.%20Thus%2C%20the%20intensity%20of%20such%20privacy%20risks%20and%20potential%20mitigation%0Astrategies%20in%20the%20medical%20domain%20remain%20unclear.%20In%20this%20paper%2C%20we%20make%20three%0Aoriginal%20contributions%20to%20privacy%20risk%20analysis%20and%20mitigation%20in%20federated%0Alearning%20for%20medical%20data.%20First%2C%20we%20propose%20a%20holistic%20framework%2C%20MedPFL%2C%20for%0Aanalyzing%20privacy%20risks%20in%20processing%20medical%20data%20in%20the%20federated%20learning%0Aenvironment%20and%20developing%20effective%20mitigation%20strategies%20for%20protecting%0Aprivacy.%20Second%2C%20through%20our%20empirical%20analysis%2C%20we%20demonstrate%20the%20severe%0Aprivacy%20risks%20in%20federated%20learning%20to%20process%20medical%20images%2C%20where%0Aadversaries%20can%20accurately%20reconstruct%20private%20medical%20images%20by%20performing%0Aprivacy%20attacks.%20Third%2C%20we%20illustrate%20that%20the%20prevalent%20defense%20mechanism%20of%0Aadding%20random%20noises%20may%20not%20always%20be%20effective%20in%20protecting%20medical%20images%0Aagainst%20privacy%20attacks%20in%20federated%20learning%2C%20which%20poses%20unique%20and%20pressing%0Achallenges%20related%20to%20protecting%20the%20privacy%20of%20medical%20data.%20Furthermore%2C%20the%0Apaper%20discusses%20several%20unique%20research%20questions%20related%20to%20the%20privacy%0Aprotection%20of%20medical%20data%20in%20the%20federated%20learning%20environment.%20We%20conduct%0Aextensive%20experiments%20on%20several%20benchmark%20medical%20image%20datasets%20to%20analyze%0Aand%20mitigate%20the%20privacy%20risks%20associated%20with%20federated%20learning%20for%20medical%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-depth%2520Analysis%2520of%2520Privacy%2520Threats%2520in%2520Federated%2520Learning%2520for%2520Medical%250A%2520%2520Data%26entry.906535625%3DBadhan%2520Chandra%2520Das%2520and%2520M.%2520Hadi%2520Amini%2520and%2520Yanzhao%2520Wu%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520emerging%2520as%2520a%2520promising%2520machine%2520learning%2520technique%2520in%250Athe%2520medical%2520field%2520for%2520analyzing%2520medical%2520images%252C%2520as%2520it%2520is%2520considered%2520an%250Aeffective%2520method%2520to%2520safeguard%2520sensitive%2520patient%2520data%2520and%2520comply%2520with%2520privacy%250Aregulations.%2520However%252C%2520recent%2520studies%2520have%2520revealed%2520that%2520the%2520default%2520settings%2520of%250Afederated%2520learning%2520may%2520inadvertently%2520expose%2520private%2520training%2520data%2520to%2520privacy%250Aattacks.%2520Thus%252C%2520the%2520intensity%2520of%2520such%2520privacy%2520risks%2520and%2520potential%2520mitigation%250Astrategies%2520in%2520the%2520medical%2520domain%2520remain%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520make%2520three%250Aoriginal%2520contributions%2520to%2520privacy%2520risk%2520analysis%2520and%2520mitigation%2520in%2520federated%250Alearning%2520for%2520medical%2520data.%2520First%252C%2520we%2520propose%2520a%2520holistic%2520framework%252C%2520MedPFL%252C%2520for%250Aanalyzing%2520privacy%2520risks%2520in%2520processing%2520medical%2520data%2520in%2520the%2520federated%2520learning%250Aenvironment%2520and%2520developing%2520effective%2520mitigation%2520strategies%2520for%2520protecting%250Aprivacy.%2520Second%252C%2520through%2520our%2520empirical%2520analysis%252C%2520we%2520demonstrate%2520the%2520severe%250Aprivacy%2520risks%2520in%2520federated%2520learning%2520to%2520process%2520medical%2520images%252C%2520where%250Aadversaries%2520can%2520accurately%2520reconstruct%2520private%2520medical%2520images%2520by%2520performing%250Aprivacy%2520attacks.%2520Third%252C%2520we%2520illustrate%2520that%2520the%2520prevalent%2520defense%2520mechanism%2520of%250Aadding%2520random%2520noises%2520may%2520not%2520always%2520be%2520effective%2520in%2520protecting%2520medical%2520images%250Aagainst%2520privacy%2520attacks%2520in%2520federated%2520learning%252C%2520which%2520poses%2520unique%2520and%2520pressing%250Achallenges%2520related%2520to%2520protecting%2520the%2520privacy%2520of%2520medical%2520data.%2520Furthermore%252C%2520the%250Apaper%2520discusses%2520several%2520unique%2520research%2520questions%2520related%2520to%2520the%2520privacy%250Aprotection%2520of%2520medical%2520data%2520in%2520the%2520federated%2520learning%2520environment.%2520We%2520conduct%250Aextensive%2520experiments%2520on%2520several%2520benchmark%2520medical%2520image%2520datasets%2520to%2520analyze%250Aand%2520mitigate%2520the%2520privacy%2520risks%2520associated%2520with%2520federated%2520learning%2520for%2520medical%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-depth%20Analysis%20of%20Privacy%20Threats%20in%20Federated%20Learning%20for%20Medical%0A%20%20Data&entry.906535625=Badhan%20Chandra%20Das%20and%20M.%20Hadi%20Amini%20and%20Yanzhao%20Wu&entry.1292438233=%20%20Federated%20learning%20is%20emerging%20as%20a%20promising%20machine%20learning%20technique%20in%0Athe%20medical%20field%20for%20analyzing%20medical%20images%2C%20as%20it%20is%20considered%20an%0Aeffective%20method%20to%20safeguard%20sensitive%20patient%20data%20and%20comply%20with%20privacy%0Aregulations.%20However%2C%20recent%20studies%20have%20revealed%20that%20the%20default%20settings%20of%0Afederated%20learning%20may%20inadvertently%20expose%20private%20training%20data%20to%20privacy%0Aattacks.%20Thus%2C%20the%20intensity%20of%20such%20privacy%20risks%20and%20potential%20mitigation%0Astrategies%20in%20the%20medical%20domain%20remain%20unclear.%20In%20this%20paper%2C%20we%20make%20three%0Aoriginal%20contributions%20to%20privacy%20risk%20analysis%20and%20mitigation%20in%20federated%0Alearning%20for%20medical%20data.%20First%2C%20we%20propose%20a%20holistic%20framework%2C%20MedPFL%2C%20for%0Aanalyzing%20privacy%20risks%20in%20processing%20medical%20data%20in%20the%20federated%20learning%0Aenvironment%20and%20developing%20effective%20mitigation%20strategies%20for%20protecting%0Aprivacy.%20Second%2C%20through%20our%20empirical%20analysis%2C%20we%20demonstrate%20the%20severe%0Aprivacy%20risks%20in%20federated%20learning%20to%20process%20medical%20images%2C%20where%0Aadversaries%20can%20accurately%20reconstruct%20private%20medical%20images%20by%20performing%0Aprivacy%20attacks.%20Third%2C%20we%20illustrate%20that%20the%20prevalent%20defense%20mechanism%20of%0Aadding%20random%20noises%20may%20not%20always%20be%20effective%20in%20protecting%20medical%20images%0Aagainst%20privacy%20attacks%20in%20federated%20learning%2C%20which%20poses%20unique%20and%20pressing%0Achallenges%20related%20to%20protecting%20the%20privacy%20of%20medical%20data.%20Furthermore%2C%20the%0Apaper%20discusses%20several%20unique%20research%20questions%20related%20to%20the%20privacy%0Aprotection%20of%20medical%20data%20in%20the%20federated%20learning%20environment.%20We%20conduct%0Aextensive%20experiments%20on%20several%20benchmark%20medical%20image%20datasets%20to%20analyze%0Aand%20mitigate%20the%20privacy%20risks%20associated%20with%20federated%20learning%20for%20medical%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18907v1&entry.124074799=Read"},
{"title": "Unsupervised Cognition", "author": "Alfredo Ibias and Hector Antona and Guillem Ramirez-Miranda and Enric Guinovart and Eduard Alarcon", "abstract": "  Unsupervised learning methods have a soft inspiration in cognition models. To\nthis day, the most successful unsupervised learning methods revolve around\nclustering samples in a mathematical space. In this paper we propose a\nstate-of-the-art primitive-based unsupervised learning approach for\ndecision-making inspired by novel cognition models. This representation-centric\napproach models the input space constructively as a distributed hierarchical\nstructure in an input-agnostic way. We compared our approach with current\nstate-of-the-art in unsupervised learning classification, and with current\nstate-of-the-art in cancer type classification. We show how our proposal\noutperforms previous state-of-the-art. We also evaluate some cognition-like\nproperties of our proposal where it not only outperforms the compared\nalgorithms (even supervised learning ones), but it also shows a different, more\ncognition-like, behaviour.\n", "link": "http://arxiv.org/abs/2409.18624v1", "date": "2024-09-27", "relevancy": 1.9107, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5172}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4501}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Cognition&body=Title%3A%20Unsupervised%20Cognition%0AAuthor%3A%20Alfredo%20Ibias%20and%20Hector%20Antona%20and%20Guillem%20Ramirez-Miranda%20and%20Enric%20Guinovart%20and%20Eduard%20Alarcon%0AAbstract%3A%20%20%20Unsupervised%20learning%20methods%20have%20a%20soft%20inspiration%20in%20cognition%20models.%20To%0Athis%20day%2C%20the%20most%20successful%20unsupervised%20learning%20methods%20revolve%20around%0Aclustering%20samples%20in%20a%20mathematical%20space.%20In%20this%20paper%20we%20propose%20a%0Astate-of-the-art%20primitive-based%20unsupervised%20learning%20approach%20for%0Adecision-making%20inspired%20by%20novel%20cognition%20models.%20This%20representation-centric%0Aapproach%20models%20the%20input%20space%20constructively%20as%20a%20distributed%20hierarchical%0Astructure%20in%20an%20input-agnostic%20way.%20We%20compared%20our%20approach%20with%20current%0Astate-of-the-art%20in%20unsupervised%20learning%20classification%2C%20and%20with%20current%0Astate-of-the-art%20in%20cancer%20type%20classification.%20We%20show%20how%20our%20proposal%0Aoutperforms%20previous%20state-of-the-art.%20We%20also%20evaluate%20some%20cognition-like%0Aproperties%20of%20our%20proposal%20where%20it%20not%20only%20outperforms%20the%20compared%0Aalgorithms%20%28even%20supervised%20learning%20ones%29%2C%20but%20it%20also%20shows%20a%20different%2C%20more%0Acognition-like%2C%20behaviour.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Cognition%26entry.906535625%3DAlfredo%2520Ibias%2520and%2520Hector%2520Antona%2520and%2520Guillem%2520Ramirez-Miranda%2520and%2520Enric%2520Guinovart%2520and%2520Eduard%2520Alarcon%26entry.1292438233%3D%2520%2520Unsupervised%2520learning%2520methods%2520have%2520a%2520soft%2520inspiration%2520in%2520cognition%2520models.%2520To%250Athis%2520day%252C%2520the%2520most%2520successful%2520unsupervised%2520learning%2520methods%2520revolve%2520around%250Aclustering%2520samples%2520in%2520a%2520mathematical%2520space.%2520In%2520this%2520paper%2520we%2520propose%2520a%250Astate-of-the-art%2520primitive-based%2520unsupervised%2520learning%2520approach%2520for%250Adecision-making%2520inspired%2520by%2520novel%2520cognition%2520models.%2520This%2520representation-centric%250Aapproach%2520models%2520the%2520input%2520space%2520constructively%2520as%2520a%2520distributed%2520hierarchical%250Astructure%2520in%2520an%2520input-agnostic%2520way.%2520We%2520compared%2520our%2520approach%2520with%2520current%250Astate-of-the-art%2520in%2520unsupervised%2520learning%2520classification%252C%2520and%2520with%2520current%250Astate-of-the-art%2520in%2520cancer%2520type%2520classification.%2520We%2520show%2520how%2520our%2520proposal%250Aoutperforms%2520previous%2520state-of-the-art.%2520We%2520also%2520evaluate%2520some%2520cognition-like%250Aproperties%2520of%2520our%2520proposal%2520where%2520it%2520not%2520only%2520outperforms%2520the%2520compared%250Aalgorithms%2520%2528even%2520supervised%2520learning%2520ones%2529%252C%2520but%2520it%2520also%2520shows%2520a%2520different%252C%2520more%250Acognition-like%252C%2520behaviour.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Cognition&entry.906535625=Alfredo%20Ibias%20and%20Hector%20Antona%20and%20Guillem%20Ramirez-Miranda%20and%20Enric%20Guinovart%20and%20Eduard%20Alarcon&entry.1292438233=%20%20Unsupervised%20learning%20methods%20have%20a%20soft%20inspiration%20in%20cognition%20models.%20To%0Athis%20day%2C%20the%20most%20successful%20unsupervised%20learning%20methods%20revolve%20around%0Aclustering%20samples%20in%20a%20mathematical%20space.%20In%20this%20paper%20we%20propose%20a%0Astate-of-the-art%20primitive-based%20unsupervised%20learning%20approach%20for%0Adecision-making%20inspired%20by%20novel%20cognition%20models.%20This%20representation-centric%0Aapproach%20models%20the%20input%20space%20constructively%20as%20a%20distributed%20hierarchical%0Astructure%20in%20an%20input-agnostic%20way.%20We%20compared%20our%20approach%20with%20current%0Astate-of-the-art%20in%20unsupervised%20learning%20classification%2C%20and%20with%20current%0Astate-of-the-art%20in%20cancer%20type%20classification.%20We%20show%20how%20our%20proposal%0Aoutperforms%20previous%20state-of-the-art.%20We%20also%20evaluate%20some%20cognition-like%0Aproperties%20of%20our%20proposal%20where%20it%20not%20only%20outperforms%20the%20compared%0Aalgorithms%20%28even%20supervised%20learning%20ones%29%2C%20but%20it%20also%20shows%20a%20different%2C%20more%0Acognition-like%2C%20behaviour.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18624v1&entry.124074799=Read"},
{"title": "HardCore Generation: Generating Hard UNSAT Problems for Data\n  Augmentation", "author": "Joseph Cotnareanu and Zhanguang Zhang and Hui-Ling Zhen and Yingxue Zhang and Mark Coates", "abstract": "  Efficiently determining the satisfiability of a boolean equation -- known as\nthe SAT problem for brevity -- is crucial in various industrial problems.\nRecently, the advent of deep learning methods has introduced significant\npotential for enhancing SAT solving. However, a major barrier to the\nadvancement of this field has been the scarcity of large, realistic datasets.\nThe majority of current public datasets are either randomly generated or\nextremely limited, containing only a few examples from unrelated problem\nfamilies. These datasets are inadequate for meaningful training of deep\nlearning methods. In light of this, researchers have started exploring\ngenerative techniques to create data that more accurately reflect SAT problems\nencountered in practical situations. These methods have so far suffered from\neither the inability to produce challenging SAT problems or time-scalability\nobstacles. In this paper we address both by identifying and manipulating the\nkey contributors to a problem's ``hardness'', known as cores. Although some\nprevious work has addressed cores, the time costs are unacceptably high due to\nthe expense of traditional heuristic core detection techniques. We introduce a\nfast core detection procedure that uses a graph neural network. Our empirical\nresults demonstrate that we can efficiently generate problems that remain hard\nto solve and retain key attributes of the original example problems. We show\nvia experiment that the generated synthetic SAT problems can be used in a data\naugmentation setting to provide improved prediction of solver runtimes.\n", "link": "http://arxiv.org/abs/2409.18778v1", "date": "2024-09-27", "relevancy": 1.91, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5134}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4763}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HardCore%20Generation%3A%20Generating%20Hard%20UNSAT%20Problems%20for%20Data%0A%20%20Augmentation&body=Title%3A%20HardCore%20Generation%3A%20Generating%20Hard%20UNSAT%20Problems%20for%20Data%0A%20%20Augmentation%0AAuthor%3A%20Joseph%20Cotnareanu%20and%20Zhanguang%20Zhang%20and%20Hui-Ling%20Zhen%20and%20Yingxue%20Zhang%20and%20Mark%20Coates%0AAbstract%3A%20%20%20Efficiently%20determining%20the%20satisfiability%20of%20a%20boolean%20equation%20--%20known%20as%0Athe%20SAT%20problem%20for%20brevity%20--%20is%20crucial%20in%20various%20industrial%20problems.%0ARecently%2C%20the%20advent%20of%20deep%20learning%20methods%20has%20introduced%20significant%0Apotential%20for%20enhancing%20SAT%20solving.%20However%2C%20a%20major%20barrier%20to%20the%0Aadvancement%20of%20this%20field%20has%20been%20the%20scarcity%20of%20large%2C%20realistic%20datasets.%0AThe%20majority%20of%20current%20public%20datasets%20are%20either%20randomly%20generated%20or%0Aextremely%20limited%2C%20containing%20only%20a%20few%20examples%20from%20unrelated%20problem%0Afamilies.%20These%20datasets%20are%20inadequate%20for%20meaningful%20training%20of%20deep%0Alearning%20methods.%20In%20light%20of%20this%2C%20researchers%20have%20started%20exploring%0Agenerative%20techniques%20to%20create%20data%20that%20more%20accurately%20reflect%20SAT%20problems%0Aencountered%20in%20practical%20situations.%20These%20methods%20have%20so%20far%20suffered%20from%0Aeither%20the%20inability%20to%20produce%20challenging%20SAT%20problems%20or%20time-scalability%0Aobstacles.%20In%20this%20paper%20we%20address%20both%20by%20identifying%20and%20manipulating%20the%0Akey%20contributors%20to%20a%20problem%27s%20%60%60hardness%27%27%2C%20known%20as%20cores.%20Although%20some%0Aprevious%20work%20has%20addressed%20cores%2C%20the%20time%20costs%20are%20unacceptably%20high%20due%20to%0Athe%20expense%20of%20traditional%20heuristic%20core%20detection%20techniques.%20We%20introduce%20a%0Afast%20core%20detection%20procedure%20that%20uses%20a%20graph%20neural%20network.%20Our%20empirical%0Aresults%20demonstrate%20that%20we%20can%20efficiently%20generate%20problems%20that%20remain%20hard%0Ato%20solve%20and%20retain%20key%20attributes%20of%20the%20original%20example%20problems.%20We%20show%0Avia%20experiment%20that%20the%20generated%20synthetic%20SAT%20problems%20can%20be%20used%20in%20a%20data%0Aaugmentation%20setting%20to%20provide%20improved%20prediction%20of%20solver%20runtimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHardCore%2520Generation%253A%2520Generating%2520Hard%2520UNSAT%2520Problems%2520for%2520Data%250A%2520%2520Augmentation%26entry.906535625%3DJoseph%2520Cotnareanu%2520and%2520Zhanguang%2520Zhang%2520and%2520Hui-Ling%2520Zhen%2520and%2520Yingxue%2520Zhang%2520and%2520Mark%2520Coates%26entry.1292438233%3D%2520%2520Efficiently%2520determining%2520the%2520satisfiability%2520of%2520a%2520boolean%2520equation%2520--%2520known%2520as%250Athe%2520SAT%2520problem%2520for%2520brevity%2520--%2520is%2520crucial%2520in%2520various%2520industrial%2520problems.%250ARecently%252C%2520the%2520advent%2520of%2520deep%2520learning%2520methods%2520has%2520introduced%2520significant%250Apotential%2520for%2520enhancing%2520SAT%2520solving.%2520However%252C%2520a%2520major%2520barrier%2520to%2520the%250Aadvancement%2520of%2520this%2520field%2520has%2520been%2520the%2520scarcity%2520of%2520large%252C%2520realistic%2520datasets.%250AThe%2520majority%2520of%2520current%2520public%2520datasets%2520are%2520either%2520randomly%2520generated%2520or%250Aextremely%2520limited%252C%2520containing%2520only%2520a%2520few%2520examples%2520from%2520unrelated%2520problem%250Afamilies.%2520These%2520datasets%2520are%2520inadequate%2520for%2520meaningful%2520training%2520of%2520deep%250Alearning%2520methods.%2520In%2520light%2520of%2520this%252C%2520researchers%2520have%2520started%2520exploring%250Agenerative%2520techniques%2520to%2520create%2520data%2520that%2520more%2520accurately%2520reflect%2520SAT%2520problems%250Aencountered%2520in%2520practical%2520situations.%2520These%2520methods%2520have%2520so%2520far%2520suffered%2520from%250Aeither%2520the%2520inability%2520to%2520produce%2520challenging%2520SAT%2520problems%2520or%2520time-scalability%250Aobstacles.%2520In%2520this%2520paper%2520we%2520address%2520both%2520by%2520identifying%2520and%2520manipulating%2520the%250Akey%2520contributors%2520to%2520a%2520problem%2527s%2520%2560%2560hardness%2527%2527%252C%2520known%2520as%2520cores.%2520Although%2520some%250Aprevious%2520work%2520has%2520addressed%2520cores%252C%2520the%2520time%2520costs%2520are%2520unacceptably%2520high%2520due%2520to%250Athe%2520expense%2520of%2520traditional%2520heuristic%2520core%2520detection%2520techniques.%2520We%2520introduce%2520a%250Afast%2520core%2520detection%2520procedure%2520that%2520uses%2520a%2520graph%2520neural%2520network.%2520Our%2520empirical%250Aresults%2520demonstrate%2520that%2520we%2520can%2520efficiently%2520generate%2520problems%2520that%2520remain%2520hard%250Ato%2520solve%2520and%2520retain%2520key%2520attributes%2520of%2520the%2520original%2520example%2520problems.%2520We%2520show%250Avia%2520experiment%2520that%2520the%2520generated%2520synthetic%2520SAT%2520problems%2520can%2520be%2520used%2520in%2520a%2520data%250Aaugmentation%2520setting%2520to%2520provide%2520improved%2520prediction%2520of%2520solver%2520runtimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HardCore%20Generation%3A%20Generating%20Hard%20UNSAT%20Problems%20for%20Data%0A%20%20Augmentation&entry.906535625=Joseph%20Cotnareanu%20and%20Zhanguang%20Zhang%20and%20Hui-Ling%20Zhen%20and%20Yingxue%20Zhang%20and%20Mark%20Coates&entry.1292438233=%20%20Efficiently%20determining%20the%20satisfiability%20of%20a%20boolean%20equation%20--%20known%20as%0Athe%20SAT%20problem%20for%20brevity%20--%20is%20crucial%20in%20various%20industrial%20problems.%0ARecently%2C%20the%20advent%20of%20deep%20learning%20methods%20has%20introduced%20significant%0Apotential%20for%20enhancing%20SAT%20solving.%20However%2C%20a%20major%20barrier%20to%20the%0Aadvancement%20of%20this%20field%20has%20been%20the%20scarcity%20of%20large%2C%20realistic%20datasets.%0AThe%20majority%20of%20current%20public%20datasets%20are%20either%20randomly%20generated%20or%0Aextremely%20limited%2C%20containing%20only%20a%20few%20examples%20from%20unrelated%20problem%0Afamilies.%20These%20datasets%20are%20inadequate%20for%20meaningful%20training%20of%20deep%0Alearning%20methods.%20In%20light%20of%20this%2C%20researchers%20have%20started%20exploring%0Agenerative%20techniques%20to%20create%20data%20that%20more%20accurately%20reflect%20SAT%20problems%0Aencountered%20in%20practical%20situations.%20These%20methods%20have%20so%20far%20suffered%20from%0Aeither%20the%20inability%20to%20produce%20challenging%20SAT%20problems%20or%20time-scalability%0Aobstacles.%20In%20this%20paper%20we%20address%20both%20by%20identifying%20and%20manipulating%20the%0Akey%20contributors%20to%20a%20problem%27s%20%60%60hardness%27%27%2C%20known%20as%20cores.%20Although%20some%0Aprevious%20work%20has%20addressed%20cores%2C%20the%20time%20costs%20are%20unacceptably%20high%20due%20to%0Athe%20expense%20of%20traditional%20heuristic%20core%20detection%20techniques.%20We%20introduce%20a%0Afast%20core%20detection%20procedure%20that%20uses%20a%20graph%20neural%20network.%20Our%20empirical%0Aresults%20demonstrate%20that%20we%20can%20efficiently%20generate%20problems%20that%20remain%20hard%0Ato%20solve%20and%20retain%20key%20attributes%20of%20the%20original%20example%20problems.%20We%20show%0Avia%20experiment%20that%20the%20generated%20synthetic%20SAT%20problems%20can%20be%20used%20in%20a%20data%0Aaugmentation%20setting%20to%20provide%20improved%20prediction%20of%20solver%20runtimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18778v1&entry.124074799=Read"},
{"title": "Soft Measures for Extracting Causal Collective Intelligence", "author": "Maryam Berijanian and Spencer Dork and Kuldeep Singh and Michael Riley Millikan and Ashlin Riggs and Aadarsh Swaminathan and Sarah L. Gibbs and Scott E. Friedman and Nathan Brugnone", "abstract": "  Understanding and modeling collective intelligence is essential for\naddressing complex social systems. Directed graphs called fuzzy cognitive maps\n(FCMs) offer a powerful tool for encoding causal mental models, but extracting\nhigh-integrity FCMs from text is challenging. This study presents an approach\nusing large language models (LLMs) to automate FCM extraction. We introduce\nnovel graph-based similarity measures and evaluate them by correlating their\noutputs with human judgments through the Elo rating system. Results show\npositive correlations with human evaluations, but even the best-performing\nmeasure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs\nimproves performance, but existing measures still fall short. This study\nhighlights the need for soft similarity measures tailored to FCM extraction,\nadvancing collective intelligence modeling with NLP.\n", "link": "http://arxiv.org/abs/2409.18911v1", "date": "2024-09-27", "relevancy": 1.9099, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4845}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Measures%20for%20Extracting%20Causal%20Collective%20Intelligence&body=Title%3A%20Soft%20Measures%20for%20Extracting%20Causal%20Collective%20Intelligence%0AAuthor%3A%20Maryam%20Berijanian%20and%20Spencer%20Dork%20and%20Kuldeep%20Singh%20and%20Michael%20Riley%20Millikan%20and%20Ashlin%20Riggs%20and%20Aadarsh%20Swaminathan%20and%20Sarah%20L.%20Gibbs%20and%20Scott%20E.%20Friedman%20and%20Nathan%20Brugnone%0AAbstract%3A%20%20%20Understanding%20and%20modeling%20collective%20intelligence%20is%20essential%20for%0Aaddressing%20complex%20social%20systems.%20Directed%20graphs%20called%20fuzzy%20cognitive%20maps%0A%28FCMs%29%20offer%20a%20powerful%20tool%20for%20encoding%20causal%20mental%20models%2C%20but%20extracting%0Ahigh-integrity%20FCMs%20from%20text%20is%20challenging.%20This%20study%20presents%20an%20approach%0Ausing%20large%20language%20models%20%28LLMs%29%20to%20automate%20FCM%20extraction.%20We%20introduce%0Anovel%20graph-based%20similarity%20measures%20and%20evaluate%20them%20by%20correlating%20their%0Aoutputs%20with%20human%20judgments%20through%20the%20Elo%20rating%20system.%20Results%20show%0Apositive%20correlations%20with%20human%20evaluations%2C%20but%20even%20the%20best-performing%0Ameasure%20exhibits%20limitations%20in%20capturing%20FCM%20nuances.%20Fine-tuning%20LLMs%0Aimproves%20performance%2C%20but%20existing%20measures%20still%20fall%20short.%20This%20study%0Ahighlights%20the%20need%20for%20soft%20similarity%20measures%20tailored%20to%20FCM%20extraction%2C%0Aadvancing%20collective%20intelligence%20modeling%20with%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Measures%2520for%2520Extracting%2520Causal%2520Collective%2520Intelligence%26entry.906535625%3DMaryam%2520Berijanian%2520and%2520Spencer%2520Dork%2520and%2520Kuldeep%2520Singh%2520and%2520Michael%2520Riley%2520Millikan%2520and%2520Ashlin%2520Riggs%2520and%2520Aadarsh%2520Swaminathan%2520and%2520Sarah%2520L.%2520Gibbs%2520and%2520Scott%2520E.%2520Friedman%2520and%2520Nathan%2520Brugnone%26entry.1292438233%3D%2520%2520Understanding%2520and%2520modeling%2520collective%2520intelligence%2520is%2520essential%2520for%250Aaddressing%2520complex%2520social%2520systems.%2520Directed%2520graphs%2520called%2520fuzzy%2520cognitive%2520maps%250A%2528FCMs%2529%2520offer%2520a%2520powerful%2520tool%2520for%2520encoding%2520causal%2520mental%2520models%252C%2520but%2520extracting%250Ahigh-integrity%2520FCMs%2520from%2520text%2520is%2520challenging.%2520This%2520study%2520presents%2520an%2520approach%250Ausing%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520automate%2520FCM%2520extraction.%2520We%2520introduce%250Anovel%2520graph-based%2520similarity%2520measures%2520and%2520evaluate%2520them%2520by%2520correlating%2520their%250Aoutputs%2520with%2520human%2520judgments%2520through%2520the%2520Elo%2520rating%2520system.%2520Results%2520show%250Apositive%2520correlations%2520with%2520human%2520evaluations%252C%2520but%2520even%2520the%2520best-performing%250Ameasure%2520exhibits%2520limitations%2520in%2520capturing%2520FCM%2520nuances.%2520Fine-tuning%2520LLMs%250Aimproves%2520performance%252C%2520but%2520existing%2520measures%2520still%2520fall%2520short.%2520This%2520study%250Ahighlights%2520the%2520need%2520for%2520soft%2520similarity%2520measures%2520tailored%2520to%2520FCM%2520extraction%252C%250Aadvancing%2520collective%2520intelligence%2520modeling%2520with%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Measures%20for%20Extracting%20Causal%20Collective%20Intelligence&entry.906535625=Maryam%20Berijanian%20and%20Spencer%20Dork%20and%20Kuldeep%20Singh%20and%20Michael%20Riley%20Millikan%20and%20Ashlin%20Riggs%20and%20Aadarsh%20Swaminathan%20and%20Sarah%20L.%20Gibbs%20and%20Scott%20E.%20Friedman%20and%20Nathan%20Brugnone&entry.1292438233=%20%20Understanding%20and%20modeling%20collective%20intelligence%20is%20essential%20for%0Aaddressing%20complex%20social%20systems.%20Directed%20graphs%20called%20fuzzy%20cognitive%20maps%0A%28FCMs%29%20offer%20a%20powerful%20tool%20for%20encoding%20causal%20mental%20models%2C%20but%20extracting%0Ahigh-integrity%20FCMs%20from%20text%20is%20challenging.%20This%20study%20presents%20an%20approach%0Ausing%20large%20language%20models%20%28LLMs%29%20to%20automate%20FCM%20extraction.%20We%20introduce%0Anovel%20graph-based%20similarity%20measures%20and%20evaluate%20them%20by%20correlating%20their%0Aoutputs%20with%20human%20judgments%20through%20the%20Elo%20rating%20system.%20Results%20show%0Apositive%20correlations%20with%20human%20evaluations%2C%20but%20even%20the%20best-performing%0Ameasure%20exhibits%20limitations%20in%20capturing%20FCM%20nuances.%20Fine-tuning%20LLMs%0Aimproves%20performance%2C%20but%20existing%20measures%20still%20fall%20short.%20This%20study%0Ahighlights%20the%20need%20for%20soft%20similarity%20measures%20tailored%20to%20FCM%20extraction%2C%0Aadvancing%20collective%20intelligence%20modeling%20with%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18911v1&entry.124074799=Read"},
{"title": "CausalBench: A Comprehensive Benchmark for Causal Learning Capability of\n  LLMs", "author": "Yu Zhou and Xingyu Wu and Beicheng Huang and Jibin Wu and Liang Feng and Kay Chen Tan", "abstract": "  The ability to understand causality significantly impacts the competence of\nlarge language models (LLMs) in output explanation and counterfactual\nreasoning, as causality reveals the underlying data distribution. However, the\nlack of a comprehensive benchmark currently limits the evaluation of LLMs'\ncausal learning capabilities. To fill this gap, this paper develops CausalBench\nbased on data from the causal research community, enabling comparative\nevaluations of LLMs against traditional causal learning algorithms. To provide\na comprehensive investigation, we offer three tasks of varying difficulties,\nincluding correlation, causal skeleton, and causality identification.\nEvaluations of 19 leading LLMs reveal that, while closed-source LLMs show\npotential for simple causal relationships, they significantly lag behind\ntraditional algorithms on larger-scale networks ($>50$ nodes). Specifically,\nLLMs struggle with collider structures but excel at chain structures,\nespecially at long-chain causality analogous to Chains-of-Thought techniques.\nThis supports the current prompt approaches while suggesting directions to\nenhance LLMs' causal reasoning capability. Furthermore, CausalBench\nincorporates background knowledge and training data into prompts to thoroughly\nunlock LLMs' text-comprehension ability during evaluation, whose findings\nindicate that, LLM understand causality through semantic associations with\ndistinct entities, rather than directly from contextual information or\nnumerical distributions.\n", "link": "http://arxiv.org/abs/2404.06349v2", "date": "2024-09-27", "relevancy": 1.9028, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausalBench%3A%20A%20Comprehensive%20Benchmark%20for%20Causal%20Learning%20Capability%20of%0A%20%20LLMs&body=Title%3A%20CausalBench%3A%20A%20Comprehensive%20Benchmark%20for%20Causal%20Learning%20Capability%20of%0A%20%20LLMs%0AAuthor%3A%20Yu%20Zhou%20and%20Xingyu%20Wu%20and%20Beicheng%20Huang%20and%20Jibin%20Wu%20and%20Liang%20Feng%20and%20Kay%20Chen%20Tan%0AAbstract%3A%20%20%20The%20ability%20to%20understand%20causality%20significantly%20impacts%20the%20competence%20of%0Alarge%20language%20models%20%28LLMs%29%20in%20output%20explanation%20and%20counterfactual%0Areasoning%2C%20as%20causality%20reveals%20the%20underlying%20data%20distribution.%20However%2C%20the%0Alack%20of%20a%20comprehensive%20benchmark%20currently%20limits%20the%20evaluation%20of%20LLMs%27%0Acausal%20learning%20capabilities.%20To%20fill%20this%20gap%2C%20this%20paper%20develops%20CausalBench%0Abased%20on%20data%20from%20the%20causal%20research%20community%2C%20enabling%20comparative%0Aevaluations%20of%20LLMs%20against%20traditional%20causal%20learning%20algorithms.%20To%20provide%0Aa%20comprehensive%20investigation%2C%20we%20offer%20three%20tasks%20of%20varying%20difficulties%2C%0Aincluding%20correlation%2C%20causal%20skeleton%2C%20and%20causality%20identification.%0AEvaluations%20of%2019%20leading%20LLMs%20reveal%20that%2C%20while%20closed-source%20LLMs%20show%0Apotential%20for%20simple%20causal%20relationships%2C%20they%20significantly%20lag%20behind%0Atraditional%20algorithms%20on%20larger-scale%20networks%20%28%24%3E50%24%20nodes%29.%20Specifically%2C%0ALLMs%20struggle%20with%20collider%20structures%20but%20excel%20at%20chain%20structures%2C%0Aespecially%20at%20long-chain%20causality%20analogous%20to%20Chains-of-Thought%20techniques.%0AThis%20supports%20the%20current%20prompt%20approaches%20while%20suggesting%20directions%20to%0Aenhance%20LLMs%27%20causal%20reasoning%20capability.%20Furthermore%2C%20CausalBench%0Aincorporates%20background%20knowledge%20and%20training%20data%20into%20prompts%20to%20thoroughly%0Aunlock%20LLMs%27%20text-comprehension%20ability%20during%20evaluation%2C%20whose%20findings%0Aindicate%20that%2C%20LLM%20understand%20causality%20through%20semantic%20associations%20with%0Adistinct%20entities%2C%20rather%20than%20directly%20from%20contextual%20information%20or%0Anumerical%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausalBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Causal%2520Learning%2520Capability%2520of%250A%2520%2520LLMs%26entry.906535625%3DYu%2520Zhou%2520and%2520Xingyu%2520Wu%2520and%2520Beicheng%2520Huang%2520and%2520Jibin%2520Wu%2520and%2520Liang%2520Feng%2520and%2520Kay%2520Chen%2520Tan%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520understand%2520causality%2520significantly%2520impacts%2520the%2520competence%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520in%2520output%2520explanation%2520and%2520counterfactual%250Areasoning%252C%2520as%2520causality%2520reveals%2520the%2520underlying%2520data%2520distribution.%2520However%252C%2520the%250Alack%2520of%2520a%2520comprehensive%2520benchmark%2520currently%2520limits%2520the%2520evaluation%2520of%2520LLMs%2527%250Acausal%2520learning%2520capabilities.%2520To%2520fill%2520this%2520gap%252C%2520this%2520paper%2520develops%2520CausalBench%250Abased%2520on%2520data%2520from%2520the%2520causal%2520research%2520community%252C%2520enabling%2520comparative%250Aevaluations%2520of%2520LLMs%2520against%2520traditional%2520causal%2520learning%2520algorithms.%2520To%2520provide%250Aa%2520comprehensive%2520investigation%252C%2520we%2520offer%2520three%2520tasks%2520of%2520varying%2520difficulties%252C%250Aincluding%2520correlation%252C%2520causal%2520skeleton%252C%2520and%2520causality%2520identification.%250AEvaluations%2520of%252019%2520leading%2520LLMs%2520reveal%2520that%252C%2520while%2520closed-source%2520LLMs%2520show%250Apotential%2520for%2520simple%2520causal%2520relationships%252C%2520they%2520significantly%2520lag%2520behind%250Atraditional%2520algorithms%2520on%2520larger-scale%2520networks%2520%2528%2524%253E50%2524%2520nodes%2529.%2520Specifically%252C%250ALLMs%2520struggle%2520with%2520collider%2520structures%2520but%2520excel%2520at%2520chain%2520structures%252C%250Aespecially%2520at%2520long-chain%2520causality%2520analogous%2520to%2520Chains-of-Thought%2520techniques.%250AThis%2520supports%2520the%2520current%2520prompt%2520approaches%2520while%2520suggesting%2520directions%2520to%250Aenhance%2520LLMs%2527%2520causal%2520reasoning%2520capability.%2520Furthermore%252C%2520CausalBench%250Aincorporates%2520background%2520knowledge%2520and%2520training%2520data%2520into%2520prompts%2520to%2520thoroughly%250Aunlock%2520LLMs%2527%2520text-comprehension%2520ability%2520during%2520evaluation%252C%2520whose%2520findings%250Aindicate%2520that%252C%2520LLM%2520understand%2520causality%2520through%2520semantic%2520associations%2520with%250Adistinct%2520entities%252C%2520rather%2520than%2520directly%2520from%2520contextual%2520information%2520or%250Anumerical%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausalBench%3A%20A%20Comprehensive%20Benchmark%20for%20Causal%20Learning%20Capability%20of%0A%20%20LLMs&entry.906535625=Yu%20Zhou%20and%20Xingyu%20Wu%20and%20Beicheng%20Huang%20and%20Jibin%20Wu%20and%20Liang%20Feng%20and%20Kay%20Chen%20Tan&entry.1292438233=%20%20The%20ability%20to%20understand%20causality%20significantly%20impacts%20the%20competence%20of%0Alarge%20language%20models%20%28LLMs%29%20in%20output%20explanation%20and%20counterfactual%0Areasoning%2C%20as%20causality%20reveals%20the%20underlying%20data%20distribution.%20However%2C%20the%0Alack%20of%20a%20comprehensive%20benchmark%20currently%20limits%20the%20evaluation%20of%20LLMs%27%0Acausal%20learning%20capabilities.%20To%20fill%20this%20gap%2C%20this%20paper%20develops%20CausalBench%0Abased%20on%20data%20from%20the%20causal%20research%20community%2C%20enabling%20comparative%0Aevaluations%20of%20LLMs%20against%20traditional%20causal%20learning%20algorithms.%20To%20provide%0Aa%20comprehensive%20investigation%2C%20we%20offer%20three%20tasks%20of%20varying%20difficulties%2C%0Aincluding%20correlation%2C%20causal%20skeleton%2C%20and%20causality%20identification.%0AEvaluations%20of%2019%20leading%20LLMs%20reveal%20that%2C%20while%20closed-source%20LLMs%20show%0Apotential%20for%20simple%20causal%20relationships%2C%20they%20significantly%20lag%20behind%0Atraditional%20algorithms%20on%20larger-scale%20networks%20%28%24%3E50%24%20nodes%29.%20Specifically%2C%0ALLMs%20struggle%20with%20collider%20structures%20but%20excel%20at%20chain%20structures%2C%0Aespecially%20at%20long-chain%20causality%20analogous%20to%20Chains-of-Thought%20techniques.%0AThis%20supports%20the%20current%20prompt%20approaches%20while%20suggesting%20directions%20to%0Aenhance%20LLMs%27%20causal%20reasoning%20capability.%20Furthermore%2C%20CausalBench%0Aincorporates%20background%20knowledge%20and%20training%20data%20into%20prompts%20to%20thoroughly%0Aunlock%20LLMs%27%20text-comprehension%20ability%20during%20evaluation%2C%20whose%20findings%0Aindicate%20that%2C%20LLM%20understand%20causality%20through%20semantic%20associations%20with%0Adistinct%20entities%2C%20rather%20than%20directly%20from%20contextual%20information%20or%0Anumerical%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06349v2&entry.124074799=Read"},
{"title": "ELiSe: Efficient Learning of Sequences in Structured Recurrent Networks", "author": "Laura Kriener and Kristin V\u00f6lk and Ben von H\u00fcnerbein and Federico Benitez and Walter Senn and Mihai A. Petrovici", "abstract": "  Behavior can be described as a temporal sequence of actions driven by neural\nactivity. To learn complex sequential patterns in neural networks, memories of\npast activities need to persist on significantly longer timescales than the\nrelaxation times of single-neuron activity. While recurrent networks can\nproduce such long transients, training these networks is a challenge. Learning\nvia error propagation confers models such as FORCE, RTRL or BPTT a significant\nfunctional advantage, but at the expense of biological plausibility. While\nreservoir computing circumvents this issue by learning only the readout\nweights, it does not scale well with problem complexity. We propose that two\nprominent structural features of cortical networks can alleviate these issues:\nthe presence of a certain network scaffold at the onset of learning and the\nexistence of dendritic compartments for enhancing neuronal information storage\nand computation. Our resulting model for Efficient Learning of Sequences\n(ELiSe) builds on these features to acquire and replay complex non-Markovian\nspatio-temporal patterns using only local, always-on and phase-free synaptic\nplasticity. We showcase the capabilities of ELiSe in a mock-up of birdsong\nlearning, and demonstrate its flexibility with respect to parametrization, as\nwell as its robustness to external disturbances.\n", "link": "http://arxiv.org/abs/2402.16763v2", "date": "2024-09-27", "relevancy": 1.9018, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4818}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4795}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELiSe%3A%20Efficient%20Learning%20of%20Sequences%20in%20Structured%20Recurrent%20Networks&body=Title%3A%20ELiSe%3A%20Efficient%20Learning%20of%20Sequences%20in%20Structured%20Recurrent%20Networks%0AAuthor%3A%20Laura%20Kriener%20and%20Kristin%20V%C3%B6lk%20and%20Ben%20von%20H%C3%BCnerbein%20and%20Federico%20Benitez%20and%20Walter%20Senn%20and%20Mihai%20A.%20Petrovici%0AAbstract%3A%20%20%20Behavior%20can%20be%20described%20as%20a%20temporal%20sequence%20of%20actions%20driven%20by%20neural%0Aactivity.%20To%20learn%20complex%20sequential%20patterns%20in%20neural%20networks%2C%20memories%20of%0Apast%20activities%20need%20to%20persist%20on%20significantly%20longer%20timescales%20than%20the%0Arelaxation%20times%20of%20single-neuron%20activity.%20While%20recurrent%20networks%20can%0Aproduce%20such%20long%20transients%2C%20training%20these%20networks%20is%20a%20challenge.%20Learning%0Avia%20error%20propagation%20confers%20models%20such%20as%20FORCE%2C%20RTRL%20or%20BPTT%20a%20significant%0Afunctional%20advantage%2C%20but%20at%20the%20expense%20of%20biological%20plausibility.%20While%0Areservoir%20computing%20circumvents%20this%20issue%20by%20learning%20only%20the%20readout%0Aweights%2C%20it%20does%20not%20scale%20well%20with%20problem%20complexity.%20We%20propose%20that%20two%0Aprominent%20structural%20features%20of%20cortical%20networks%20can%20alleviate%20these%20issues%3A%0Athe%20presence%20of%20a%20certain%20network%20scaffold%20at%20the%20onset%20of%20learning%20and%20the%0Aexistence%20of%20dendritic%20compartments%20for%20enhancing%20neuronal%20information%20storage%0Aand%20computation.%20Our%20resulting%20model%20for%20Efficient%20Learning%20of%20Sequences%0A%28ELiSe%29%20builds%20on%20these%20features%20to%20acquire%20and%20replay%20complex%20non-Markovian%0Aspatio-temporal%20patterns%20using%20only%20local%2C%20always-on%20and%20phase-free%20synaptic%0Aplasticity.%20We%20showcase%20the%20capabilities%20of%20ELiSe%20in%20a%20mock-up%20of%20birdsong%0Alearning%2C%20and%20demonstrate%20its%20flexibility%20with%20respect%20to%20parametrization%2C%20as%0Awell%20as%20its%20robustness%20to%20external%20disturbances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16763v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELiSe%253A%2520Efficient%2520Learning%2520of%2520Sequences%2520in%2520Structured%2520Recurrent%2520Networks%26entry.906535625%3DLaura%2520Kriener%2520and%2520Kristin%2520V%25C3%25B6lk%2520and%2520Ben%2520von%2520H%25C3%25BCnerbein%2520and%2520Federico%2520Benitez%2520and%2520Walter%2520Senn%2520and%2520Mihai%2520A.%2520Petrovici%26entry.1292438233%3D%2520%2520Behavior%2520can%2520be%2520described%2520as%2520a%2520temporal%2520sequence%2520of%2520actions%2520driven%2520by%2520neural%250Aactivity.%2520To%2520learn%2520complex%2520sequential%2520patterns%2520in%2520neural%2520networks%252C%2520memories%2520of%250Apast%2520activities%2520need%2520to%2520persist%2520on%2520significantly%2520longer%2520timescales%2520than%2520the%250Arelaxation%2520times%2520of%2520single-neuron%2520activity.%2520While%2520recurrent%2520networks%2520can%250Aproduce%2520such%2520long%2520transients%252C%2520training%2520these%2520networks%2520is%2520a%2520challenge.%2520Learning%250Avia%2520error%2520propagation%2520confers%2520models%2520such%2520as%2520FORCE%252C%2520RTRL%2520or%2520BPTT%2520a%2520significant%250Afunctional%2520advantage%252C%2520but%2520at%2520the%2520expense%2520of%2520biological%2520plausibility.%2520While%250Areservoir%2520computing%2520circumvents%2520this%2520issue%2520by%2520learning%2520only%2520the%2520readout%250Aweights%252C%2520it%2520does%2520not%2520scale%2520well%2520with%2520problem%2520complexity.%2520We%2520propose%2520that%2520two%250Aprominent%2520structural%2520features%2520of%2520cortical%2520networks%2520can%2520alleviate%2520these%2520issues%253A%250Athe%2520presence%2520of%2520a%2520certain%2520network%2520scaffold%2520at%2520the%2520onset%2520of%2520learning%2520and%2520the%250Aexistence%2520of%2520dendritic%2520compartments%2520for%2520enhancing%2520neuronal%2520information%2520storage%250Aand%2520computation.%2520Our%2520resulting%2520model%2520for%2520Efficient%2520Learning%2520of%2520Sequences%250A%2528ELiSe%2529%2520builds%2520on%2520these%2520features%2520to%2520acquire%2520and%2520replay%2520complex%2520non-Markovian%250Aspatio-temporal%2520patterns%2520using%2520only%2520local%252C%2520always-on%2520and%2520phase-free%2520synaptic%250Aplasticity.%2520We%2520showcase%2520the%2520capabilities%2520of%2520ELiSe%2520in%2520a%2520mock-up%2520of%2520birdsong%250Alearning%252C%2520and%2520demonstrate%2520its%2520flexibility%2520with%2520respect%2520to%2520parametrization%252C%2520as%250Awell%2520as%2520its%2520robustness%2520to%2520external%2520disturbances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16763v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELiSe%3A%20Efficient%20Learning%20of%20Sequences%20in%20Structured%20Recurrent%20Networks&entry.906535625=Laura%20Kriener%20and%20Kristin%20V%C3%B6lk%20and%20Ben%20von%20H%C3%BCnerbein%20and%20Federico%20Benitez%20and%20Walter%20Senn%20and%20Mihai%20A.%20Petrovici&entry.1292438233=%20%20Behavior%20can%20be%20described%20as%20a%20temporal%20sequence%20of%20actions%20driven%20by%20neural%0Aactivity.%20To%20learn%20complex%20sequential%20patterns%20in%20neural%20networks%2C%20memories%20of%0Apast%20activities%20need%20to%20persist%20on%20significantly%20longer%20timescales%20than%20the%0Arelaxation%20times%20of%20single-neuron%20activity.%20While%20recurrent%20networks%20can%0Aproduce%20such%20long%20transients%2C%20training%20these%20networks%20is%20a%20challenge.%20Learning%0Avia%20error%20propagation%20confers%20models%20such%20as%20FORCE%2C%20RTRL%20or%20BPTT%20a%20significant%0Afunctional%20advantage%2C%20but%20at%20the%20expense%20of%20biological%20plausibility.%20While%0Areservoir%20computing%20circumvents%20this%20issue%20by%20learning%20only%20the%20readout%0Aweights%2C%20it%20does%20not%20scale%20well%20with%20problem%20complexity.%20We%20propose%20that%20two%0Aprominent%20structural%20features%20of%20cortical%20networks%20can%20alleviate%20these%20issues%3A%0Athe%20presence%20of%20a%20certain%20network%20scaffold%20at%20the%20onset%20of%20learning%20and%20the%0Aexistence%20of%20dendritic%20compartments%20for%20enhancing%20neuronal%20information%20storage%0Aand%20computation.%20Our%20resulting%20model%20for%20Efficient%20Learning%20of%20Sequences%0A%28ELiSe%29%20builds%20on%20these%20features%20to%20acquire%20and%20replay%20complex%20non-Markovian%0Aspatio-temporal%20patterns%20using%20only%20local%2C%20always-on%20and%20phase-free%20synaptic%0Aplasticity.%20We%20showcase%20the%20capabilities%20of%20ELiSe%20in%20a%20mock-up%20of%20birdsong%0Alearning%2C%20and%20demonstrate%20its%20flexibility%20with%20respect%20to%20parametrization%2C%20as%0Awell%20as%20its%20robustness%20to%20external%20disturbances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16763v2&entry.124074799=Read"},
{"title": "On Rademacher Complexity-based Generalization Bounds for Deep Learning", "author": "Lan V. Truong", "abstract": "  We show that the Rademacher complexity-based approach can generate\nnon-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for\nclassifying a small number of classes of images. The development of new\nTalagrand's contraction lemmas for high-dimensional mappings between function\nspaces and CNNs for general Lipschitz activation functions is a key technical\ncontribution. Our results show that the Rademacher complexity does not depend\non the network length for CNNs with some special types of activation functions\nsuch as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.\n", "link": "http://arxiv.org/abs/2208.04284v3", "date": "2024-09-27", "relevancy": 1.8847, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.485}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4713}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Rademacher%20Complexity-based%20Generalization%20Bounds%20for%20Deep%20Learning&body=Title%3A%20On%20Rademacher%20Complexity-based%20Generalization%20Bounds%20for%20Deep%20Learning%0AAuthor%3A%20Lan%20V.%20Truong%0AAbstract%3A%20%20%20We%20show%20that%20the%20Rademacher%20complexity-based%20approach%20can%20generate%0Anon-vacuous%20generalisation%20bounds%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%0Aclassifying%20a%20small%20number%20of%20classes%20of%20images.%20The%20development%20of%20new%0ATalagrand%27s%20contraction%20lemmas%20for%20high-dimensional%20mappings%20between%20function%0Aspaces%20and%20CNNs%20for%20general%20Lipschitz%20activation%20functions%20is%20a%20key%20technical%0Acontribution.%20Our%20results%20show%20that%20the%20Rademacher%20complexity%20does%20not%20depend%0Aon%20the%20network%20length%20for%20CNNs%20with%20some%20special%20types%20of%20activation%20functions%0Asuch%20as%20ReLU%2C%20Leaky%20ReLU%2C%20Parametric%20Rectifier%20Linear%20Unit%2C%20Sigmoid%2C%20and%20Tanh.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.04284v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Rademacher%2520Complexity-based%2520Generalization%2520Bounds%2520for%2520Deep%2520Learning%26entry.906535625%3DLan%2520V.%2520Truong%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520the%2520Rademacher%2520complexity-based%2520approach%2520can%2520generate%250Anon-vacuous%2520generalisation%2520bounds%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520for%250Aclassifying%2520a%2520small%2520number%2520of%2520classes%2520of%2520images.%2520The%2520development%2520of%2520new%250ATalagrand%2527s%2520contraction%2520lemmas%2520for%2520high-dimensional%2520mappings%2520between%2520function%250Aspaces%2520and%2520CNNs%2520for%2520general%2520Lipschitz%2520activation%2520functions%2520is%2520a%2520key%2520technical%250Acontribution.%2520Our%2520results%2520show%2520that%2520the%2520Rademacher%2520complexity%2520does%2520not%2520depend%250Aon%2520the%2520network%2520length%2520for%2520CNNs%2520with%2520some%2520special%2520types%2520of%2520activation%2520functions%250Asuch%2520as%2520ReLU%252C%2520Leaky%2520ReLU%252C%2520Parametric%2520Rectifier%2520Linear%2520Unit%252C%2520Sigmoid%252C%2520and%2520Tanh.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.04284v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Rademacher%20Complexity-based%20Generalization%20Bounds%20for%20Deep%20Learning&entry.906535625=Lan%20V.%20Truong&entry.1292438233=%20%20We%20show%20that%20the%20Rademacher%20complexity-based%20approach%20can%20generate%0Anon-vacuous%20generalisation%20bounds%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%0Aclassifying%20a%20small%20number%20of%20classes%20of%20images.%20The%20development%20of%20new%0ATalagrand%27s%20contraction%20lemmas%20for%20high-dimensional%20mappings%20between%20function%0Aspaces%20and%20CNNs%20for%20general%20Lipschitz%20activation%20functions%20is%20a%20key%20technical%0Acontribution.%20Our%20results%20show%20that%20the%20Rademacher%20complexity%20does%20not%20depend%0Aon%20the%20network%20length%20for%20CNNs%20with%20some%20special%20types%20of%20activation%20functions%0Asuch%20as%20ReLU%2C%20Leaky%20ReLU%2C%20Parametric%20Rectifier%20Linear%20Unit%2C%20Sigmoid%2C%20and%20Tanh.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.04284v3&entry.124074799=Read"},
{"title": "ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse\n  Weather Conditions", "author": "Wenfeng Huang and Guoan Xu and Wenjing Jia and Stuart Perry and Guangwei Gao", "abstract": "  Images captured in challenging environments--such as nighttime, foggy, rainy\nweather, and underwater--often suffer from significant degradation, resulting\nin a substantial loss of visual quality. Effective restoration of these\ndegraded images is critical for the subsequent vision tasks. While many\nexisting approaches have successfully incorporated specific priors for\nindividual tasks, these tailored solutions limit their applicability to other\ndegradations. In this work, we propose a universal network architecture, dubbed\n\"ReviveDiff\", which can address a wide range of degradations and bring images\nback to life by enhancing and restoring their quality. Our approach is inspired\nby the observation that, unlike degradation caused by movement or electronic\nissues, quality degradation under adverse conditions primarily stems from\nnatural media (such as fog, water, and low luminance), which generally\npreserves the original structures of objects. To restore the quality of such\nimages, we leveraged the latest advancements in diffusion models and developed\nReviveDiff to restore image quality from both macro and micro levels across\nsome key factors determining image quality, such as sharpness, distortion,\nnoise level, dynamic range, and color accuracy. We rigorously evaluated\nReviveDiff on seven benchmark datasets covering five types of degrading\nconditions: Rainy, Underwater, Low-light, Smoke, and Nighttime Hazy. Our\nexperimental results demonstrate that ReviveDiff outperforms the\nstate-of-the-art methods both quantitatively and visually.\n", "link": "http://arxiv.org/abs/2409.18932v1", "date": "2024-09-27", "relevancy": 1.8602, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6531}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6208}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReviveDiff%3A%20A%20Universal%20Diffusion%20Model%20for%20Restoring%20Images%20in%20Adverse%0A%20%20Weather%20Conditions&body=Title%3A%20ReviveDiff%3A%20A%20Universal%20Diffusion%20Model%20for%20Restoring%20Images%20in%20Adverse%0A%20%20Weather%20Conditions%0AAuthor%3A%20Wenfeng%20Huang%20and%20Guoan%20Xu%20and%20Wenjing%20Jia%20and%20Stuart%20Perry%20and%20Guangwei%20Gao%0AAbstract%3A%20%20%20Images%20captured%20in%20challenging%20environments--such%20as%20nighttime%2C%20foggy%2C%20rainy%0Aweather%2C%20and%20underwater--often%20suffer%20from%20significant%20degradation%2C%20resulting%0Ain%20a%20substantial%20loss%20of%20visual%20quality.%20Effective%20restoration%20of%20these%0Adegraded%20images%20is%20critical%20for%20the%20subsequent%20vision%20tasks.%20While%20many%0Aexisting%20approaches%20have%20successfully%20incorporated%20specific%20priors%20for%0Aindividual%20tasks%2C%20these%20tailored%20solutions%20limit%20their%20applicability%20to%20other%0Adegradations.%20In%20this%20work%2C%20we%20propose%20a%20universal%20network%20architecture%2C%20dubbed%0A%22ReviveDiff%22%2C%20which%20can%20address%20a%20wide%20range%20of%20degradations%20and%20bring%20images%0Aback%20to%20life%20by%20enhancing%20and%20restoring%20their%20quality.%20Our%20approach%20is%20inspired%0Aby%20the%20observation%20that%2C%20unlike%20degradation%20caused%20by%20movement%20or%20electronic%0Aissues%2C%20quality%20degradation%20under%20adverse%20conditions%20primarily%20stems%20from%0Anatural%20media%20%28such%20as%20fog%2C%20water%2C%20and%20low%20luminance%29%2C%20which%20generally%0Apreserves%20the%20original%20structures%20of%20objects.%20To%20restore%20the%20quality%20of%20such%0Aimages%2C%20we%20leveraged%20the%20latest%20advancements%20in%20diffusion%20models%20and%20developed%0AReviveDiff%20to%20restore%20image%20quality%20from%20both%20macro%20and%20micro%20levels%20across%0Asome%20key%20factors%20determining%20image%20quality%2C%20such%20as%20sharpness%2C%20distortion%2C%0Anoise%20level%2C%20dynamic%20range%2C%20and%20color%20accuracy.%20We%20rigorously%20evaluated%0AReviveDiff%20on%20seven%20benchmark%20datasets%20covering%20five%20types%20of%20degrading%0Aconditions%3A%20Rainy%2C%20Underwater%2C%20Low-light%2C%20Smoke%2C%20and%20Nighttime%20Hazy.%20Our%0Aexperimental%20results%20demonstrate%20that%20ReviveDiff%20outperforms%20the%0Astate-of-the-art%20methods%20both%20quantitatively%20and%20visually.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReviveDiff%253A%2520A%2520Universal%2520Diffusion%2520Model%2520for%2520Restoring%2520Images%2520in%2520Adverse%250A%2520%2520Weather%2520Conditions%26entry.906535625%3DWenfeng%2520Huang%2520and%2520Guoan%2520Xu%2520and%2520Wenjing%2520Jia%2520and%2520Stuart%2520Perry%2520and%2520Guangwei%2520Gao%26entry.1292438233%3D%2520%2520Images%2520captured%2520in%2520challenging%2520environments--such%2520as%2520nighttime%252C%2520foggy%252C%2520rainy%250Aweather%252C%2520and%2520underwater--often%2520suffer%2520from%2520significant%2520degradation%252C%2520resulting%250Ain%2520a%2520substantial%2520loss%2520of%2520visual%2520quality.%2520Effective%2520restoration%2520of%2520these%250Adegraded%2520images%2520is%2520critical%2520for%2520the%2520subsequent%2520vision%2520tasks.%2520While%2520many%250Aexisting%2520approaches%2520have%2520successfully%2520incorporated%2520specific%2520priors%2520for%250Aindividual%2520tasks%252C%2520these%2520tailored%2520solutions%2520limit%2520their%2520applicability%2520to%2520other%250Adegradations.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520universal%2520network%2520architecture%252C%2520dubbed%250A%2522ReviveDiff%2522%252C%2520which%2520can%2520address%2520a%2520wide%2520range%2520of%2520degradations%2520and%2520bring%2520images%250Aback%2520to%2520life%2520by%2520enhancing%2520and%2520restoring%2520their%2520quality.%2520Our%2520approach%2520is%2520inspired%250Aby%2520the%2520observation%2520that%252C%2520unlike%2520degradation%2520caused%2520by%2520movement%2520or%2520electronic%250Aissues%252C%2520quality%2520degradation%2520under%2520adverse%2520conditions%2520primarily%2520stems%2520from%250Anatural%2520media%2520%2528such%2520as%2520fog%252C%2520water%252C%2520and%2520low%2520luminance%2529%252C%2520which%2520generally%250Apreserves%2520the%2520original%2520structures%2520of%2520objects.%2520To%2520restore%2520the%2520quality%2520of%2520such%250Aimages%252C%2520we%2520leveraged%2520the%2520latest%2520advancements%2520in%2520diffusion%2520models%2520and%2520developed%250AReviveDiff%2520to%2520restore%2520image%2520quality%2520from%2520both%2520macro%2520and%2520micro%2520levels%2520across%250Asome%2520key%2520factors%2520determining%2520image%2520quality%252C%2520such%2520as%2520sharpness%252C%2520distortion%252C%250Anoise%2520level%252C%2520dynamic%2520range%252C%2520and%2520color%2520accuracy.%2520We%2520rigorously%2520evaluated%250AReviveDiff%2520on%2520seven%2520benchmark%2520datasets%2520covering%2520five%2520types%2520of%2520degrading%250Aconditions%253A%2520Rainy%252C%2520Underwater%252C%2520Low-light%252C%2520Smoke%252C%2520and%2520Nighttime%2520Hazy.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520ReviveDiff%2520outperforms%2520the%250Astate-of-the-art%2520methods%2520both%2520quantitatively%2520and%2520visually.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReviveDiff%3A%20A%20Universal%20Diffusion%20Model%20for%20Restoring%20Images%20in%20Adverse%0A%20%20Weather%20Conditions&entry.906535625=Wenfeng%20Huang%20and%20Guoan%20Xu%20and%20Wenjing%20Jia%20and%20Stuart%20Perry%20and%20Guangwei%20Gao&entry.1292438233=%20%20Images%20captured%20in%20challenging%20environments--such%20as%20nighttime%2C%20foggy%2C%20rainy%0Aweather%2C%20and%20underwater--often%20suffer%20from%20significant%20degradation%2C%20resulting%0Ain%20a%20substantial%20loss%20of%20visual%20quality.%20Effective%20restoration%20of%20these%0Adegraded%20images%20is%20critical%20for%20the%20subsequent%20vision%20tasks.%20While%20many%0Aexisting%20approaches%20have%20successfully%20incorporated%20specific%20priors%20for%0Aindividual%20tasks%2C%20these%20tailored%20solutions%20limit%20their%20applicability%20to%20other%0Adegradations.%20In%20this%20work%2C%20we%20propose%20a%20universal%20network%20architecture%2C%20dubbed%0A%22ReviveDiff%22%2C%20which%20can%20address%20a%20wide%20range%20of%20degradations%20and%20bring%20images%0Aback%20to%20life%20by%20enhancing%20and%20restoring%20their%20quality.%20Our%20approach%20is%20inspired%0Aby%20the%20observation%20that%2C%20unlike%20degradation%20caused%20by%20movement%20or%20electronic%0Aissues%2C%20quality%20degradation%20under%20adverse%20conditions%20primarily%20stems%20from%0Anatural%20media%20%28such%20as%20fog%2C%20water%2C%20and%20low%20luminance%29%2C%20which%20generally%0Apreserves%20the%20original%20structures%20of%20objects.%20To%20restore%20the%20quality%20of%20such%0Aimages%2C%20we%20leveraged%20the%20latest%20advancements%20in%20diffusion%20models%20and%20developed%0AReviveDiff%20to%20restore%20image%20quality%20from%20both%20macro%20and%20micro%20levels%20across%0Asome%20key%20factors%20determining%20image%20quality%2C%20such%20as%20sharpness%2C%20distortion%2C%0Anoise%20level%2C%20dynamic%20range%2C%20and%20color%20accuracy.%20We%20rigorously%20evaluated%0AReviveDiff%20on%20seven%20benchmark%20datasets%20covering%20five%20types%20of%20degrading%0Aconditions%3A%20Rainy%2C%20Underwater%2C%20Low-light%2C%20Smoke%2C%20and%20Nighttime%20Hazy.%20Our%0Aexperimental%20results%20demonstrate%20that%20ReviveDiff%20outperforms%20the%0Astate-of-the-art%20methods%20both%20quantitatively%20and%20visually.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18932v1&entry.124074799=Read"},
{"title": "Challenges of Generating Structurally Diverse Graphs", "author": "Fedor Velikonivtsev and Mikhail Mironov and Liudmila Prokhorenkova", "abstract": "  For many graph-related problems, it can be essential to have a set of\nstructurally diverse graphs. For instance, such graphs can be used for testing\ngraph algorithms or their neural approximations. However, to the best of our\nknowledge, the problem of generating structurally diverse graphs has not been\nexplored in the literature. In this paper, we fill this gap. First, we discuss\nhow to define diversity for a set of graphs, why this task is non-trivial, and\nhow one can choose a proper diversity measure. Then, for a given diversity\nmeasure, we propose and compare several algorithms optimizing it: we consider\napproaches based on standard random graph models, local graph optimization,\ngenetic algorithms, and neural generative models. We show that it is possible\nto significantly improve diversity over basic random graph generators.\nAdditionally, our analysis of generated graphs allows us to better understand\nthe properties of graph distances: depending on which diversity measure is used\nfor optimization, the obtained graphs may possess very different structural\nproperties which gives insights about the sensitivity of the graph distance\nunderlying the diversity measure.\n", "link": "http://arxiv.org/abs/2409.18859v1", "date": "2024-09-27", "relevancy": 1.8492, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5075}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4714}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenges%20of%20Generating%20Structurally%20Diverse%20Graphs&body=Title%3A%20Challenges%20of%20Generating%20Structurally%20Diverse%20Graphs%0AAuthor%3A%20Fedor%20Velikonivtsev%20and%20Mikhail%20Mironov%20and%20Liudmila%20Prokhorenkova%0AAbstract%3A%20%20%20For%20many%20graph-related%20problems%2C%20it%20can%20be%20essential%20to%20have%20a%20set%20of%0Astructurally%20diverse%20graphs.%20For%20instance%2C%20such%20graphs%20can%20be%20used%20for%20testing%0Agraph%20algorithms%20or%20their%20neural%20approximations.%20However%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20problem%20of%20generating%20structurally%20diverse%20graphs%20has%20not%20been%0Aexplored%20in%20the%20literature.%20In%20this%20paper%2C%20we%20fill%20this%20gap.%20First%2C%20we%20discuss%0Ahow%20to%20define%20diversity%20for%20a%20set%20of%20graphs%2C%20why%20this%20task%20is%20non-trivial%2C%20and%0Ahow%20one%20can%20choose%20a%20proper%20diversity%20measure.%20Then%2C%20for%20a%20given%20diversity%0Ameasure%2C%20we%20propose%20and%20compare%20several%20algorithms%20optimizing%20it%3A%20we%20consider%0Aapproaches%20based%20on%20standard%20random%20graph%20models%2C%20local%20graph%20optimization%2C%0Agenetic%20algorithms%2C%20and%20neural%20generative%20models.%20We%20show%20that%20it%20is%20possible%0Ato%20significantly%20improve%20diversity%20over%20basic%20random%20graph%20generators.%0AAdditionally%2C%20our%20analysis%20of%20generated%20graphs%20allows%20us%20to%20better%20understand%0Athe%20properties%20of%20graph%20distances%3A%20depending%20on%20which%20diversity%20measure%20is%20used%0Afor%20optimization%2C%20the%20obtained%20graphs%20may%20possess%20very%20different%20structural%0Aproperties%20which%20gives%20insights%20about%20the%20sensitivity%20of%20the%20graph%20distance%0Aunderlying%20the%20diversity%20measure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenges%2520of%2520Generating%2520Structurally%2520Diverse%2520Graphs%26entry.906535625%3DFedor%2520Velikonivtsev%2520and%2520Mikhail%2520Mironov%2520and%2520Liudmila%2520Prokhorenkova%26entry.1292438233%3D%2520%2520For%2520many%2520graph-related%2520problems%252C%2520it%2520can%2520be%2520essential%2520to%2520have%2520a%2520set%2520of%250Astructurally%2520diverse%2520graphs.%2520For%2520instance%252C%2520such%2520graphs%2520can%2520be%2520used%2520for%2520testing%250Agraph%2520algorithms%2520or%2520their%2520neural%2520approximations.%2520However%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520problem%2520of%2520generating%2520structurally%2520diverse%2520graphs%2520has%2520not%2520been%250Aexplored%2520in%2520the%2520literature.%2520In%2520this%2520paper%252C%2520we%2520fill%2520this%2520gap.%2520First%252C%2520we%2520discuss%250Ahow%2520to%2520define%2520diversity%2520for%2520a%2520set%2520of%2520graphs%252C%2520why%2520this%2520task%2520is%2520non-trivial%252C%2520and%250Ahow%2520one%2520can%2520choose%2520a%2520proper%2520diversity%2520measure.%2520Then%252C%2520for%2520a%2520given%2520diversity%250Ameasure%252C%2520we%2520propose%2520and%2520compare%2520several%2520algorithms%2520optimizing%2520it%253A%2520we%2520consider%250Aapproaches%2520based%2520on%2520standard%2520random%2520graph%2520models%252C%2520local%2520graph%2520optimization%252C%250Agenetic%2520algorithms%252C%2520and%2520neural%2520generative%2520models.%2520We%2520show%2520that%2520it%2520is%2520possible%250Ato%2520significantly%2520improve%2520diversity%2520over%2520basic%2520random%2520graph%2520generators.%250AAdditionally%252C%2520our%2520analysis%2520of%2520generated%2520graphs%2520allows%2520us%2520to%2520better%2520understand%250Athe%2520properties%2520of%2520graph%2520distances%253A%2520depending%2520on%2520which%2520diversity%2520measure%2520is%2520used%250Afor%2520optimization%252C%2520the%2520obtained%2520graphs%2520may%2520possess%2520very%2520different%2520structural%250Aproperties%2520which%2520gives%2520insights%2520about%2520the%2520sensitivity%2520of%2520the%2520graph%2520distance%250Aunderlying%2520the%2520diversity%2520measure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenges%20of%20Generating%20Structurally%20Diverse%20Graphs&entry.906535625=Fedor%20Velikonivtsev%20and%20Mikhail%20Mironov%20and%20Liudmila%20Prokhorenkova&entry.1292438233=%20%20For%20many%20graph-related%20problems%2C%20it%20can%20be%20essential%20to%20have%20a%20set%20of%0Astructurally%20diverse%20graphs.%20For%20instance%2C%20such%20graphs%20can%20be%20used%20for%20testing%0Agraph%20algorithms%20or%20their%20neural%20approximations.%20However%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20problem%20of%20generating%20structurally%20diverse%20graphs%20has%20not%20been%0Aexplored%20in%20the%20literature.%20In%20this%20paper%2C%20we%20fill%20this%20gap.%20First%2C%20we%20discuss%0Ahow%20to%20define%20diversity%20for%20a%20set%20of%20graphs%2C%20why%20this%20task%20is%20non-trivial%2C%20and%0Ahow%20one%20can%20choose%20a%20proper%20diversity%20measure.%20Then%2C%20for%20a%20given%20diversity%0Ameasure%2C%20we%20propose%20and%20compare%20several%20algorithms%20optimizing%20it%3A%20we%20consider%0Aapproaches%20based%20on%20standard%20random%20graph%20models%2C%20local%20graph%20optimization%2C%0Agenetic%20algorithms%2C%20and%20neural%20generative%20models.%20We%20show%20that%20it%20is%20possible%0Ato%20significantly%20improve%20diversity%20over%20basic%20random%20graph%20generators.%0AAdditionally%2C%20our%20analysis%20of%20generated%20graphs%20allows%20us%20to%20better%20understand%0Athe%20properties%20of%20graph%20distances%3A%20depending%20on%20which%20diversity%20measure%20is%20used%0Afor%20optimization%2C%20the%20obtained%20graphs%20may%20possess%20very%20different%20structural%0Aproperties%20which%20gives%20insights%20about%20the%20sensitivity%20of%20the%20graph%20distance%0Aunderlying%20the%20diversity%20measure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18859v1&entry.124074799=Read"},
{"title": "Quantum Algorithms for Drone Mission Planning", "author": "Ethan Davies and Pranav Kalidindi", "abstract": "  Mission planning often involves optimising the use of ISR (Intelligence,\nSurveillance and Reconnaissance) assets in order to achieve a set of mission\nobjectives within allowed parameters subject to constraints. The missions of\ninterest here, involve routing multiple UAVs visiting multiple targets,\nutilising sensors to capture data relating to each target. Finding such\nsolutions is often an NP-Hard problem and cannot be solved efficiently on\nclassical computers. Furthermore, during the mission new constraints and\nobjectives may arise, requiring a new solution to be computed within a short\ntime period. To achieve this we investigate near term quantum algorithms that\nhave the potential to offer speed-ups against current classical methods. We\ndemonstrate how a large family of these problems can be formulated as a Mixed\nInteger Linear Program (MILP) and then converted to a Quadratic Unconstrained\nBinary Optimisation (QUBO). The formulation provided is versatile and can be\nadapted for many different constraints with clear qubit scaling provided. We\ndiscuss the results of solving the QUBO formulation using commercial quantum\nannealers and compare the solutions to current edge classical solvers. We also\nanalyse the results from solving the QUBO using Quantum Approximate\nOptimisation Algorithms (QAOA) and discuss their results. Finally, we also\nprovide efficient methods to encode to the problem into the Variational Quantum\nEigensolver (VQE) formalism, where we have tailored the ansatz to the problem\nmaking efficient use of the qubits available.\n", "link": "http://arxiv.org/abs/2409.18631v1", "date": "2024-09-27", "relevancy": 1.754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4558}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4362}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Algorithms%20for%20Drone%20Mission%20Planning&body=Title%3A%20Quantum%20Algorithms%20for%20Drone%20Mission%20Planning%0AAuthor%3A%20Ethan%20Davies%20and%20Pranav%20Kalidindi%0AAbstract%3A%20%20%20Mission%20planning%20often%20involves%20optimising%20the%20use%20of%20ISR%20%28Intelligence%2C%0ASurveillance%20and%20Reconnaissance%29%20assets%20in%20order%20to%20achieve%20a%20set%20of%20mission%0Aobjectives%20within%20allowed%20parameters%20subject%20to%20constraints.%20The%20missions%20of%0Ainterest%20here%2C%20involve%20routing%20multiple%20UAVs%20visiting%20multiple%20targets%2C%0Autilising%20sensors%20to%20capture%20data%20relating%20to%20each%20target.%20Finding%20such%0Asolutions%20is%20often%20an%20NP-Hard%20problem%20and%20cannot%20be%20solved%20efficiently%20on%0Aclassical%20computers.%20Furthermore%2C%20during%20the%20mission%20new%20constraints%20and%0Aobjectives%20may%20arise%2C%20requiring%20a%20new%20solution%20to%20be%20computed%20within%20a%20short%0Atime%20period.%20To%20achieve%20this%20we%20investigate%20near%20term%20quantum%20algorithms%20that%0Ahave%20the%20potential%20to%20offer%20speed-ups%20against%20current%20classical%20methods.%20We%0Ademonstrate%20how%20a%20large%20family%20of%20these%20problems%20can%20be%20formulated%20as%20a%20Mixed%0AInteger%20Linear%20Program%20%28MILP%29%20and%20then%20converted%20to%20a%20Quadratic%20Unconstrained%0ABinary%20Optimisation%20%28QUBO%29.%20The%20formulation%20provided%20is%20versatile%20and%20can%20be%0Aadapted%20for%20many%20different%20constraints%20with%20clear%20qubit%20scaling%20provided.%20We%0Adiscuss%20the%20results%20of%20solving%20the%20QUBO%20formulation%20using%20commercial%20quantum%0Aannealers%20and%20compare%20the%20solutions%20to%20current%20edge%20classical%20solvers.%20We%20also%0Aanalyse%20the%20results%20from%20solving%20the%20QUBO%20using%20Quantum%20Approximate%0AOptimisation%20Algorithms%20%28QAOA%29%20and%20discuss%20their%20results.%20Finally%2C%20we%20also%0Aprovide%20efficient%20methods%20to%20encode%20to%20the%20problem%20into%20the%20Variational%20Quantum%0AEigensolver%20%28VQE%29%20formalism%2C%20where%20we%20have%20tailored%20the%20ansatz%20to%20the%20problem%0Amaking%20efficient%20use%20of%20the%20qubits%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Algorithms%2520for%2520Drone%2520Mission%2520Planning%26entry.906535625%3DEthan%2520Davies%2520and%2520Pranav%2520Kalidindi%26entry.1292438233%3D%2520%2520Mission%2520planning%2520often%2520involves%2520optimising%2520the%2520use%2520of%2520ISR%2520%2528Intelligence%252C%250ASurveillance%2520and%2520Reconnaissance%2529%2520assets%2520in%2520order%2520to%2520achieve%2520a%2520set%2520of%2520mission%250Aobjectives%2520within%2520allowed%2520parameters%2520subject%2520to%2520constraints.%2520The%2520missions%2520of%250Ainterest%2520here%252C%2520involve%2520routing%2520multiple%2520UAVs%2520visiting%2520multiple%2520targets%252C%250Autilising%2520sensors%2520to%2520capture%2520data%2520relating%2520to%2520each%2520target.%2520Finding%2520such%250Asolutions%2520is%2520often%2520an%2520NP-Hard%2520problem%2520and%2520cannot%2520be%2520solved%2520efficiently%2520on%250Aclassical%2520computers.%2520Furthermore%252C%2520during%2520the%2520mission%2520new%2520constraints%2520and%250Aobjectives%2520may%2520arise%252C%2520requiring%2520a%2520new%2520solution%2520to%2520be%2520computed%2520within%2520a%2520short%250Atime%2520period.%2520To%2520achieve%2520this%2520we%2520investigate%2520near%2520term%2520quantum%2520algorithms%2520that%250Ahave%2520the%2520potential%2520to%2520offer%2520speed-ups%2520against%2520current%2520classical%2520methods.%2520We%250Ademonstrate%2520how%2520a%2520large%2520family%2520of%2520these%2520problems%2520can%2520be%2520formulated%2520as%2520a%2520Mixed%250AInteger%2520Linear%2520Program%2520%2528MILP%2529%2520and%2520then%2520converted%2520to%2520a%2520Quadratic%2520Unconstrained%250ABinary%2520Optimisation%2520%2528QUBO%2529.%2520The%2520formulation%2520provided%2520is%2520versatile%2520and%2520can%2520be%250Aadapted%2520for%2520many%2520different%2520constraints%2520with%2520clear%2520qubit%2520scaling%2520provided.%2520We%250Adiscuss%2520the%2520results%2520of%2520solving%2520the%2520QUBO%2520formulation%2520using%2520commercial%2520quantum%250Aannealers%2520and%2520compare%2520the%2520solutions%2520to%2520current%2520edge%2520classical%2520solvers.%2520We%2520also%250Aanalyse%2520the%2520results%2520from%2520solving%2520the%2520QUBO%2520using%2520Quantum%2520Approximate%250AOptimisation%2520Algorithms%2520%2528QAOA%2529%2520and%2520discuss%2520their%2520results.%2520Finally%252C%2520we%2520also%250Aprovide%2520efficient%2520methods%2520to%2520encode%2520to%2520the%2520problem%2520into%2520the%2520Variational%2520Quantum%250AEigensolver%2520%2528VQE%2529%2520formalism%252C%2520where%2520we%2520have%2520tailored%2520the%2520ansatz%2520to%2520the%2520problem%250Amaking%2520efficient%2520use%2520of%2520the%2520qubits%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Algorithms%20for%20Drone%20Mission%20Planning&entry.906535625=Ethan%20Davies%20and%20Pranav%20Kalidindi&entry.1292438233=%20%20Mission%20planning%20often%20involves%20optimising%20the%20use%20of%20ISR%20%28Intelligence%2C%0ASurveillance%20and%20Reconnaissance%29%20assets%20in%20order%20to%20achieve%20a%20set%20of%20mission%0Aobjectives%20within%20allowed%20parameters%20subject%20to%20constraints.%20The%20missions%20of%0Ainterest%20here%2C%20involve%20routing%20multiple%20UAVs%20visiting%20multiple%20targets%2C%0Autilising%20sensors%20to%20capture%20data%20relating%20to%20each%20target.%20Finding%20such%0Asolutions%20is%20often%20an%20NP-Hard%20problem%20and%20cannot%20be%20solved%20efficiently%20on%0Aclassical%20computers.%20Furthermore%2C%20during%20the%20mission%20new%20constraints%20and%0Aobjectives%20may%20arise%2C%20requiring%20a%20new%20solution%20to%20be%20computed%20within%20a%20short%0Atime%20period.%20To%20achieve%20this%20we%20investigate%20near%20term%20quantum%20algorithms%20that%0Ahave%20the%20potential%20to%20offer%20speed-ups%20against%20current%20classical%20methods.%20We%0Ademonstrate%20how%20a%20large%20family%20of%20these%20problems%20can%20be%20formulated%20as%20a%20Mixed%0AInteger%20Linear%20Program%20%28MILP%29%20and%20then%20converted%20to%20a%20Quadratic%20Unconstrained%0ABinary%20Optimisation%20%28QUBO%29.%20The%20formulation%20provided%20is%20versatile%20and%20can%20be%0Aadapted%20for%20many%20different%20constraints%20with%20clear%20qubit%20scaling%20provided.%20We%0Adiscuss%20the%20results%20of%20solving%20the%20QUBO%20formulation%20using%20commercial%20quantum%0Aannealers%20and%20compare%20the%20solutions%20to%20current%20edge%20classical%20solvers.%20We%20also%0Aanalyse%20the%20results%20from%20solving%20the%20QUBO%20using%20Quantum%20Approximate%0AOptimisation%20Algorithms%20%28QAOA%29%20and%20discuss%20their%20results.%20Finally%2C%20we%20also%0Aprovide%20efficient%20methods%20to%20encode%20to%20the%20problem%20into%20the%20Variational%20Quantum%0AEigensolver%20%28VQE%29%20formalism%2C%20where%20we%20have%20tailored%20the%20ansatz%20to%20the%20problem%0Amaking%20efficient%20use%20of%20the%20qubits%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18631v1&entry.124074799=Read"},
{"title": "EMR-Merging: Tuning-Free High-Performance Model Merging", "author": "Chenyu Huang and Peng Ye and Tao Chen and Tong He and Xiangyu Yue and Wanli Ouyang", "abstract": "  The success of pretrain-finetune paradigm brings about the release of\nnumerous model weights. In this case, merging models finetuned on different\ntasks to enable a single model with multi-task capabilities is gaining\nincreasing attention for its practicability. Existing model merging methods\nusually suffer from (1) significant performance degradation or (2) requiring\ntuning by additional data or training. In this paper, we rethink and analyze\nthe existing model merging paradigm. We discover that using a single model's\nweights can hardly simulate all the models' performance. To tackle this issue,\nwe propose Elect, Mask & Rescale-Merging (EMR-Merging). We first (a) elect a\nunified model from all the model weights and then (b) generate extremely\nlightweight task-specific modulators, including masks and rescalers, to align\nthe direction and magnitude between the unified model and each specific model,\nrespectively. EMR-Merging is tuning-free, thus requiring no data availability\nor any additional training while showing impressive performance. We find that\nEMR-Merging shows outstanding performance compared to existing merging methods\nunder different classical and newly-established settings, including merging\ndifferent numbers of vision models (up to 30), NLP models, PEFT models, and\nmulti-modal models.\n", "link": "http://arxiv.org/abs/2405.17461v2", "date": "2024-09-27", "relevancy": 0.9965, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5085}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5058}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMR-Merging%3A%20Tuning-Free%20High-Performance%20Model%20Merging&body=Title%3A%20EMR-Merging%3A%20Tuning-Free%20High-Performance%20Model%20Merging%0AAuthor%3A%20Chenyu%20Huang%20and%20Peng%20Ye%20and%20Tao%20Chen%20and%20Tong%20He%20and%20Xiangyu%20Yue%20and%20Wanli%20Ouyang%0AAbstract%3A%20%20%20The%20success%20of%20pretrain-finetune%20paradigm%20brings%20about%20the%20release%20of%0Anumerous%20model%20weights.%20In%20this%20case%2C%20merging%20models%20finetuned%20on%20different%0Atasks%20to%20enable%20a%20single%20model%20with%20multi-task%20capabilities%20is%20gaining%0Aincreasing%20attention%20for%20its%20practicability.%20Existing%20model%20merging%20methods%0Ausually%20suffer%20from%20%281%29%20significant%20performance%20degradation%20or%20%282%29%20requiring%0Atuning%20by%20additional%20data%20or%20training.%20In%20this%20paper%2C%20we%20rethink%20and%20analyze%0Athe%20existing%20model%20merging%20paradigm.%20We%20discover%20that%20using%20a%20single%20model%27s%0Aweights%20can%20hardly%20simulate%20all%20the%20models%27%20performance.%20To%20tackle%20this%20issue%2C%0Awe%20propose%20Elect%2C%20Mask%20%26%20Rescale-Merging%20%28EMR-Merging%29.%20We%20first%20%28a%29%20elect%20a%0Aunified%20model%20from%20all%20the%20model%20weights%20and%20then%20%28b%29%20generate%20extremely%0Alightweight%20task-specific%20modulators%2C%20including%20masks%20and%20rescalers%2C%20to%20align%0Athe%20direction%20and%20magnitude%20between%20the%20unified%20model%20and%20each%20specific%20model%2C%0Arespectively.%20EMR-Merging%20is%20tuning-free%2C%20thus%20requiring%20no%20data%20availability%0Aor%20any%20additional%20training%20while%20showing%20impressive%20performance.%20We%20find%20that%0AEMR-Merging%20shows%20outstanding%20performance%20compared%20to%20existing%20merging%20methods%0Aunder%20different%20classical%20and%20newly-established%20settings%2C%20including%20merging%0Adifferent%20numbers%20of%20vision%20models%20%28up%20to%2030%29%2C%20NLP%20models%2C%20PEFT%20models%2C%20and%0Amulti-modal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMR-Merging%253A%2520Tuning-Free%2520High-Performance%2520Model%2520Merging%26entry.906535625%3DChenyu%2520Huang%2520and%2520Peng%2520Ye%2520and%2520Tao%2520Chen%2520and%2520Tong%2520He%2520and%2520Xiangyu%2520Yue%2520and%2520Wanli%2520Ouyang%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520pretrain-finetune%2520paradigm%2520brings%2520about%2520the%2520release%2520of%250Anumerous%2520model%2520weights.%2520In%2520this%2520case%252C%2520merging%2520models%2520finetuned%2520on%2520different%250Atasks%2520to%2520enable%2520a%2520single%2520model%2520with%2520multi-task%2520capabilities%2520is%2520gaining%250Aincreasing%2520attention%2520for%2520its%2520practicability.%2520Existing%2520model%2520merging%2520methods%250Ausually%2520suffer%2520from%2520%25281%2529%2520significant%2520performance%2520degradation%2520or%2520%25282%2529%2520requiring%250Atuning%2520by%2520additional%2520data%2520or%2520training.%2520In%2520this%2520paper%252C%2520we%2520rethink%2520and%2520analyze%250Athe%2520existing%2520model%2520merging%2520paradigm.%2520We%2520discover%2520that%2520using%2520a%2520single%2520model%2527s%250Aweights%2520can%2520hardly%2520simulate%2520all%2520the%2520models%2527%2520performance.%2520To%2520tackle%2520this%2520issue%252C%250Awe%2520propose%2520Elect%252C%2520Mask%2520%2526%2520Rescale-Merging%2520%2528EMR-Merging%2529.%2520We%2520first%2520%2528a%2529%2520elect%2520a%250Aunified%2520model%2520from%2520all%2520the%2520model%2520weights%2520and%2520then%2520%2528b%2529%2520generate%2520extremely%250Alightweight%2520task-specific%2520modulators%252C%2520including%2520masks%2520and%2520rescalers%252C%2520to%2520align%250Athe%2520direction%2520and%2520magnitude%2520between%2520the%2520unified%2520model%2520and%2520each%2520specific%2520model%252C%250Arespectively.%2520EMR-Merging%2520is%2520tuning-free%252C%2520thus%2520requiring%2520no%2520data%2520availability%250Aor%2520any%2520additional%2520training%2520while%2520showing%2520impressive%2520performance.%2520We%2520find%2520that%250AEMR-Merging%2520shows%2520outstanding%2520performance%2520compared%2520to%2520existing%2520merging%2520methods%250Aunder%2520different%2520classical%2520and%2520newly-established%2520settings%252C%2520including%2520merging%250Adifferent%2520numbers%2520of%2520vision%2520models%2520%2528up%2520to%252030%2529%252C%2520NLP%2520models%252C%2520PEFT%2520models%252C%2520and%250Amulti-modal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMR-Merging%3A%20Tuning-Free%20High-Performance%20Model%20Merging&entry.906535625=Chenyu%20Huang%20and%20Peng%20Ye%20and%20Tao%20Chen%20and%20Tong%20He%20and%20Xiangyu%20Yue%20and%20Wanli%20Ouyang&entry.1292438233=%20%20The%20success%20of%20pretrain-finetune%20paradigm%20brings%20about%20the%20release%20of%0Anumerous%20model%20weights.%20In%20this%20case%2C%20merging%20models%20finetuned%20on%20different%0Atasks%20to%20enable%20a%20single%20model%20with%20multi-task%20capabilities%20is%20gaining%0Aincreasing%20attention%20for%20its%20practicability.%20Existing%20model%20merging%20methods%0Ausually%20suffer%20from%20%281%29%20significant%20performance%20degradation%20or%20%282%29%20requiring%0Atuning%20by%20additional%20data%20or%20training.%20In%20this%20paper%2C%20we%20rethink%20and%20analyze%0Athe%20existing%20model%20merging%20paradigm.%20We%20discover%20that%20using%20a%20single%20model%27s%0Aweights%20can%20hardly%20simulate%20all%20the%20models%27%20performance.%20To%20tackle%20this%20issue%2C%0Awe%20propose%20Elect%2C%20Mask%20%26%20Rescale-Merging%20%28EMR-Merging%29.%20We%20first%20%28a%29%20elect%20a%0Aunified%20model%20from%20all%20the%20model%20weights%20and%20then%20%28b%29%20generate%20extremely%0Alightweight%20task-specific%20modulators%2C%20including%20masks%20and%20rescalers%2C%20to%20align%0Athe%20direction%20and%20magnitude%20between%20the%20unified%20model%20and%20each%20specific%20model%2C%0Arespectively.%20EMR-Merging%20is%20tuning-free%2C%20thus%20requiring%20no%20data%20availability%0Aor%20any%20additional%20training%20while%20showing%20impressive%20performance.%20We%20find%20that%0AEMR-Merging%20shows%20outstanding%20performance%20compared%20to%20existing%20merging%20methods%0Aunder%20different%20classical%20and%20newly-established%20settings%2C%20including%20merging%0Adifferent%20numbers%20of%20vision%20models%20%28up%20to%2030%29%2C%20NLP%20models%2C%20PEFT%20models%2C%20and%0Amulti-modal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17461v2&entry.124074799=Read"},
{"title": "Safe Decentralized Multi-Agent Control using Black-Box Predictors,\n  Conformal Decision Policies, and Control Barrier Functions", "author": "Sacha Huriot and Hussein Sibai", "abstract": "  We address the challenge of safe control in decentralized multi-agent robotic\nsettings, where agents use uncertain black-box models to predict other agents'\ntrajectories. We use the recently proposed conformal decision theory to adapt\nthe restrictiveness of control barrier functions-based safety constraints based\non observed prediction errors. We use these constraints to synthesize\ncontrollers that balance between the objectives of safety and task\naccomplishment, despite the prediction errors. We provide an upper bound on the\naverage over time of the value of a monotonic function of the difference\nbetween the safety constraint based on the predicted trajectories and the\nconstraint based on the ground truth ones. We validate our theory through\nexperimental results showing the performance of our controllers when navigating\na robot in the multi-agent scenes in the Stanford Drone Dataset.\n", "link": "http://arxiv.org/abs/2409.18862v1", "date": "2024-09-27", "relevancy": 1.7794, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6171}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6141}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Decentralized%20Multi-Agent%20Control%20using%20Black-Box%20Predictors%2C%0A%20%20Conformal%20Decision%20Policies%2C%20and%20Control%20Barrier%20Functions&body=Title%3A%20Safe%20Decentralized%20Multi-Agent%20Control%20using%20Black-Box%20Predictors%2C%0A%20%20Conformal%20Decision%20Policies%2C%20and%20Control%20Barrier%20Functions%0AAuthor%3A%20Sacha%20Huriot%20and%20Hussein%20Sibai%0AAbstract%3A%20%20%20We%20address%20the%20challenge%20of%20safe%20control%20in%20decentralized%20multi-agent%20robotic%0Asettings%2C%20where%20agents%20use%20uncertain%20black-box%20models%20to%20predict%20other%20agents%27%0Atrajectories.%20We%20use%20the%20recently%20proposed%20conformal%20decision%20theory%20to%20adapt%0Athe%20restrictiveness%20of%20control%20barrier%20functions-based%20safety%20constraints%20based%0Aon%20observed%20prediction%20errors.%20We%20use%20these%20constraints%20to%20synthesize%0Acontrollers%20that%20balance%20between%20the%20objectives%20of%20safety%20and%20task%0Aaccomplishment%2C%20despite%20the%20prediction%20errors.%20We%20provide%20an%20upper%20bound%20on%20the%0Aaverage%20over%20time%20of%20the%20value%20of%20a%20monotonic%20function%20of%20the%20difference%0Abetween%20the%20safety%20constraint%20based%20on%20the%20predicted%20trajectories%20and%20the%0Aconstraint%20based%20on%20the%20ground%20truth%20ones.%20We%20validate%20our%20theory%20through%0Aexperimental%20results%20showing%20the%20performance%20of%20our%20controllers%20when%20navigating%0Aa%20robot%20in%20the%20multi-agent%20scenes%20in%20the%20Stanford%20Drone%20Dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Decentralized%2520Multi-Agent%2520Control%2520using%2520Black-Box%2520Predictors%252C%250A%2520%2520Conformal%2520Decision%2520Policies%252C%2520and%2520Control%2520Barrier%2520Functions%26entry.906535625%3DSacha%2520Huriot%2520and%2520Hussein%2520Sibai%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenge%2520of%2520safe%2520control%2520in%2520decentralized%2520multi-agent%2520robotic%250Asettings%252C%2520where%2520agents%2520use%2520uncertain%2520black-box%2520models%2520to%2520predict%2520other%2520agents%2527%250Atrajectories.%2520We%2520use%2520the%2520recently%2520proposed%2520conformal%2520decision%2520theory%2520to%2520adapt%250Athe%2520restrictiveness%2520of%2520control%2520barrier%2520functions-based%2520safety%2520constraints%2520based%250Aon%2520observed%2520prediction%2520errors.%2520We%2520use%2520these%2520constraints%2520to%2520synthesize%250Acontrollers%2520that%2520balance%2520between%2520the%2520objectives%2520of%2520safety%2520and%2520task%250Aaccomplishment%252C%2520despite%2520the%2520prediction%2520errors.%2520We%2520provide%2520an%2520upper%2520bound%2520on%2520the%250Aaverage%2520over%2520time%2520of%2520the%2520value%2520of%2520a%2520monotonic%2520function%2520of%2520the%2520difference%250Abetween%2520the%2520safety%2520constraint%2520based%2520on%2520the%2520predicted%2520trajectories%2520and%2520the%250Aconstraint%2520based%2520on%2520the%2520ground%2520truth%2520ones.%2520We%2520validate%2520our%2520theory%2520through%250Aexperimental%2520results%2520showing%2520the%2520performance%2520of%2520our%2520controllers%2520when%2520navigating%250Aa%2520robot%2520in%2520the%2520multi-agent%2520scenes%2520in%2520the%2520Stanford%2520Drone%2520Dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Decentralized%20Multi-Agent%20Control%20using%20Black-Box%20Predictors%2C%0A%20%20Conformal%20Decision%20Policies%2C%20and%20Control%20Barrier%20Functions&entry.906535625=Sacha%20Huriot%20and%20Hussein%20Sibai&entry.1292438233=%20%20We%20address%20the%20challenge%20of%20safe%20control%20in%20decentralized%20multi-agent%20robotic%0Asettings%2C%20where%20agents%20use%20uncertain%20black-box%20models%20to%20predict%20other%20agents%27%0Atrajectories.%20We%20use%20the%20recently%20proposed%20conformal%20decision%20theory%20to%20adapt%0Athe%20restrictiveness%20of%20control%20barrier%20functions-based%20safety%20constraints%20based%0Aon%20observed%20prediction%20errors.%20We%20use%20these%20constraints%20to%20synthesize%0Acontrollers%20that%20balance%20between%20the%20objectives%20of%20safety%20and%20task%0Aaccomplishment%2C%20despite%20the%20prediction%20errors.%20We%20provide%20an%20upper%20bound%20on%20the%0Aaverage%20over%20time%20of%20the%20value%20of%20a%20monotonic%20function%20of%20the%20difference%0Abetween%20the%20safety%20constraint%20based%20on%20the%20predicted%20trajectories%20and%20the%0Aconstraint%20based%20on%20the%20ground%20truth%20ones.%20We%20validate%20our%20theory%20through%0Aexperimental%20results%20showing%20the%20performance%20of%20our%20controllers%20when%20navigating%0Aa%20robot%20in%20the%20multi-agent%20scenes%20in%20the%20Stanford%20Drone%20Dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18862v1&entry.124074799=Read"},
{"title": "Reward-Robust RLHF in LLMs", "author": "Yuzi Yan and Xingzhou Lou and Jialian Li and Yiping Zhang and Jian Xie and Chao Yu and Yu Wang and Dong Yan and Yuan Shen", "abstract": "  As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.\n", "link": "http://arxiv.org/abs/2409.15360v2", "date": "2024-09-27", "relevancy": 1.4386, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.492}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward-Robust%20RLHF%20in%20LLMs&body=Title%3A%20Reward-Robust%20RLHF%20in%20LLMs%0AAuthor%3A%20Yuzi%20Yan%20and%20Xingzhou%20Lou%20and%20Jialian%20Li%20and%20Yiping%20Zhang%20and%20Jian%20Xie%20and%20Chao%20Yu%20and%20Yu%20Wang%20and%20Dong%20Yan%20and%20Yuan%20Shen%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20progress%20toward%20more%20advanced%0Aforms%20of%20intelligence%2C%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%0Aincreasingly%20seen%20as%20a%20key%20pathway%20toward%20achieving%20Artificial%20General%0AIntelligence%20%28AGI%29.%20However%2C%20the%20reliance%20on%20reward-model-based%20%28RM-based%29%0Aalignment%20methods%20introduces%20significant%20challenges%20due%20to%20the%20inherent%0Ainstability%20and%20imperfections%20of%20Reward%20Models%20%28RMs%29%2C%20which%20can%20lead%20to%0Acritical%20issues%20such%20as%20reward%20hacking%20and%20misalignment%20with%20human%20intentions.%0AIn%20this%20paper%2C%20we%20introduce%20a%20reward-robust%20RLHF%20framework%20aimed%20at%20addressing%0Athese%20fundamental%20challenges%2C%20paving%20the%20way%20for%20more%20reliable%20and%20resilient%0Alearning%20in%20LLMs.%20Our%20approach%20introduces%20a%20novel%20optimization%20objective%20that%0Acarefully%20balances%20performance%20and%20robustness%20by%20incorporating%20Bayesian%20Reward%0AModel%20Ensembles%20%28BRME%29%20to%20model%20the%20uncertainty%20set%20of%20reward%20functions.%20This%0Aallows%20the%20framework%20to%20integrate%20both%20nominal%20performance%20and%20minimum%20reward%0Asignals%2C%20ensuring%20more%20stable%20learning%20even%20with%20imperfect%20RMs.%20Empirical%0Aresults%20demonstrate%20that%20our%20framework%20consistently%20outperforms%20baselines%0Aacross%20diverse%20benchmarks%2C%20showing%20improved%20accuracy%20and%20long-term%20stability.%0AWe%20also%20provide%20a%20theoretical%20analysis%2C%20demonstrating%20that%20reward-robust%20RLHF%0Aapproaches%20the%20stability%20of%20constant%20reward%20settings%2C%20which%20proves%20to%20be%0Aacceptable%20even%20in%20a%20stochastic-case%20analysis.%20Together%2C%20these%20contributions%0Ahighlight%20the%20framework%20potential%20to%20enhance%20both%20the%20performance%20and%20stability%0Aof%20LLM%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15360v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward-Robust%2520RLHF%2520in%2520LLMs%26entry.906535625%3DYuzi%2520Yan%2520and%2520Xingzhou%2520Lou%2520and%2520Jialian%2520Li%2520and%2520Yiping%2520Zhang%2520and%2520Jian%2520Xie%2520and%2520Chao%2520Yu%2520and%2520Yu%2520Wang%2520and%2520Dong%2520Yan%2520and%2520Yuan%2520Shen%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520continue%2520to%2520progress%2520toward%2520more%2520advanced%250Aforms%2520of%2520intelligence%252C%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520is%250Aincreasingly%2520seen%2520as%2520a%2520key%2520pathway%2520toward%2520achieving%2520Artificial%2520General%250AIntelligence%2520%2528AGI%2529.%2520However%252C%2520the%2520reliance%2520on%2520reward-model-based%2520%2528RM-based%2529%250Aalignment%2520methods%2520introduces%2520significant%2520challenges%2520due%2520to%2520the%2520inherent%250Ainstability%2520and%2520imperfections%2520of%2520Reward%2520Models%2520%2528RMs%2529%252C%2520which%2520can%2520lead%2520to%250Acritical%2520issues%2520such%2520as%2520reward%2520hacking%2520and%2520misalignment%2520with%2520human%2520intentions.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520reward-robust%2520RLHF%2520framework%2520aimed%2520at%2520addressing%250Athese%2520fundamental%2520challenges%252C%2520paving%2520the%2520way%2520for%2520more%2520reliable%2520and%2520resilient%250Alearning%2520in%2520LLMs.%2520Our%2520approach%2520introduces%2520a%2520novel%2520optimization%2520objective%2520that%250Acarefully%2520balances%2520performance%2520and%2520robustness%2520by%2520incorporating%2520Bayesian%2520Reward%250AModel%2520Ensembles%2520%2528BRME%2529%2520to%2520model%2520the%2520uncertainty%2520set%2520of%2520reward%2520functions.%2520This%250Aallows%2520the%2520framework%2520to%2520integrate%2520both%2520nominal%2520performance%2520and%2520minimum%2520reward%250Asignals%252C%2520ensuring%2520more%2520stable%2520learning%2520even%2520with%2520imperfect%2520RMs.%2520Empirical%250Aresults%2520demonstrate%2520that%2520our%2520framework%2520consistently%2520outperforms%2520baselines%250Aacross%2520diverse%2520benchmarks%252C%2520showing%2520improved%2520accuracy%2520and%2520long-term%2520stability.%250AWe%2520also%2520provide%2520a%2520theoretical%2520analysis%252C%2520demonstrating%2520that%2520reward-robust%2520RLHF%250Aapproaches%2520the%2520stability%2520of%2520constant%2520reward%2520settings%252C%2520which%2520proves%2520to%2520be%250Aacceptable%2520even%2520in%2520a%2520stochastic-case%2520analysis.%2520Together%252C%2520these%2520contributions%250Ahighlight%2520the%2520framework%2520potential%2520to%2520enhance%2520both%2520the%2520performance%2520and%2520stability%250Aof%2520LLM%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15360v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward-Robust%20RLHF%20in%20LLMs&entry.906535625=Yuzi%20Yan%20and%20Xingzhou%20Lou%20and%20Jialian%20Li%20and%20Yiping%20Zhang%20and%20Jian%20Xie%20and%20Chao%20Yu%20and%20Yu%20Wang%20and%20Dong%20Yan%20and%20Yuan%20Shen&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20progress%20toward%20more%20advanced%0Aforms%20of%20intelligence%2C%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%0Aincreasingly%20seen%20as%20a%20key%20pathway%20toward%20achieving%20Artificial%20General%0AIntelligence%20%28AGI%29.%20However%2C%20the%20reliance%20on%20reward-model-based%20%28RM-based%29%0Aalignment%20methods%20introduces%20significant%20challenges%20due%20to%20the%20inherent%0Ainstability%20and%20imperfections%20of%20Reward%20Models%20%28RMs%29%2C%20which%20can%20lead%20to%0Acritical%20issues%20such%20as%20reward%20hacking%20and%20misalignment%20with%20human%20intentions.%0AIn%20this%20paper%2C%20we%20introduce%20a%20reward-robust%20RLHF%20framework%20aimed%20at%20addressing%0Athese%20fundamental%20challenges%2C%20paving%20the%20way%20for%20more%20reliable%20and%20resilient%0Alearning%20in%20LLMs.%20Our%20approach%20introduces%20a%20novel%20optimization%20objective%20that%0Acarefully%20balances%20performance%20and%20robustness%20by%20incorporating%20Bayesian%20Reward%0AModel%20Ensembles%20%28BRME%29%20to%20model%20the%20uncertainty%20set%20of%20reward%20functions.%20This%0Aallows%20the%20framework%20to%20integrate%20both%20nominal%20performance%20and%20minimum%20reward%0Asignals%2C%20ensuring%20more%20stable%20learning%20even%20with%20imperfect%20RMs.%20Empirical%0Aresults%20demonstrate%20that%20our%20framework%20consistently%20outperforms%20baselines%0Aacross%20diverse%20benchmarks%2C%20showing%20improved%20accuracy%20and%20long-term%20stability.%0AWe%20also%20provide%20a%20theoretical%20analysis%2C%20demonstrating%20that%20reward-robust%20RLHF%0Aapproaches%20the%20stability%20of%20constant%20reward%20settings%2C%20which%20proves%20to%20be%0Aacceptable%20even%20in%20a%20stochastic-case%20analysis.%20Together%2C%20these%20contributions%0Ahighlight%20the%20framework%20potential%20to%20enhance%20both%20the%20performance%20and%20stability%0Aof%20LLM%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15360v2&entry.124074799=Read"},
{"title": "CCFExp: Facial Image Synthesis with Cycle Cross-Fusion Diffusion Model\n  for Facial Paralysis Individuals", "author": "Weixiang Gao and Yifan Xia", "abstract": "  Facial paralysis is a debilitating condition that affects the movement of\nfacial muscles, leading to a significant loss of facial expressions. Currently,\nthe diagnosis of facial paralysis remains a challenging task, often relying\nheavily on the subjective judgment and experience of clinicians, which can\nintroduce variability and uncertainty in the assessment process. One promising\napplication in real-life situations is the automatic estimation of facial\nparalysis. However, the scarcity of facial paralysis datasets limits the\ndevelopment of robust machine learning models for automated diagnosis and\ntherapeutic interventions. To this end, this study aims to synthesize a\nhigh-quality facial paralysis dataset to address this gap, enabling more\naccurate and efficient algorithm training. Specifically, a novel Cycle\nCross-Fusion Expression Generative Model (CCFExp) based on the diffusion model\nis proposed to combine different features of facial information and enhance the\nvisual details of facial appearance and texture in facial regions, thus\ncreating synthetic facial images that accurately represent various degrees and\ntypes of facial paralysis. We have qualitatively and quantitatively evaluated\nthe proposed method on the commonly used public clinical datasets of facial\nparalysis to demonstrate its effectiveness. Experimental results indicate that\nthe proposed method surpasses state-of-the-art methods, generating more\nrealistic facial images and maintaining identity consistency.\n", "link": "http://arxiv.org/abs/2409.07271v2", "date": "2024-09-27", "relevancy": 1.672, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5611}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.558}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCFExp%3A%20Facial%20Image%20Synthesis%20with%20Cycle%20Cross-Fusion%20Diffusion%20Model%0A%20%20for%20Facial%20Paralysis%20Individuals&body=Title%3A%20CCFExp%3A%20Facial%20Image%20Synthesis%20with%20Cycle%20Cross-Fusion%20Diffusion%20Model%0A%20%20for%20Facial%20Paralysis%20Individuals%0AAuthor%3A%20Weixiang%20Gao%20and%20Yifan%20Xia%0AAbstract%3A%20%20%20Facial%20paralysis%20is%20a%20debilitating%20condition%20that%20affects%20the%20movement%20of%0Afacial%20muscles%2C%20leading%20to%20a%20significant%20loss%20of%20facial%20expressions.%20Currently%2C%0Athe%20diagnosis%20of%20facial%20paralysis%20remains%20a%20challenging%20task%2C%20often%20relying%0Aheavily%20on%20the%20subjective%20judgment%20and%20experience%20of%20clinicians%2C%20which%20can%0Aintroduce%20variability%20and%20uncertainty%20in%20the%20assessment%20process.%20One%20promising%0Aapplication%20in%20real-life%20situations%20is%20the%20automatic%20estimation%20of%20facial%0Aparalysis.%20However%2C%20the%20scarcity%20of%20facial%20paralysis%20datasets%20limits%20the%0Adevelopment%20of%20robust%20machine%20learning%20models%20for%20automated%20diagnosis%20and%0Atherapeutic%20interventions.%20To%20this%20end%2C%20this%20study%20aims%20to%20synthesize%20a%0Ahigh-quality%20facial%20paralysis%20dataset%20to%20address%20this%20gap%2C%20enabling%20more%0Aaccurate%20and%20efficient%20algorithm%20training.%20Specifically%2C%20a%20novel%20Cycle%0ACross-Fusion%20Expression%20Generative%20Model%20%28CCFExp%29%20based%20on%20the%20diffusion%20model%0Ais%20proposed%20to%20combine%20different%20features%20of%20facial%20information%20and%20enhance%20the%0Avisual%20details%20of%20facial%20appearance%20and%20texture%20in%20facial%20regions%2C%20thus%0Acreating%20synthetic%20facial%20images%20that%20accurately%20represent%20various%20degrees%20and%0Atypes%20of%20facial%20paralysis.%20We%20have%20qualitatively%20and%20quantitatively%20evaluated%0Athe%20proposed%20method%20on%20the%20commonly%20used%20public%20clinical%20datasets%20of%20facial%0Aparalysis%20to%20demonstrate%20its%20effectiveness.%20Experimental%20results%20indicate%20that%0Athe%20proposed%20method%20surpasses%20state-of-the-art%20methods%2C%20generating%20more%0Arealistic%20facial%20images%20and%20maintaining%20identity%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCFExp%253A%2520Facial%2520Image%2520Synthesis%2520with%2520Cycle%2520Cross-Fusion%2520Diffusion%2520Model%250A%2520%2520for%2520Facial%2520Paralysis%2520Individuals%26entry.906535625%3DWeixiang%2520Gao%2520and%2520Yifan%2520Xia%26entry.1292438233%3D%2520%2520Facial%2520paralysis%2520is%2520a%2520debilitating%2520condition%2520that%2520affects%2520the%2520movement%2520of%250Afacial%2520muscles%252C%2520leading%2520to%2520a%2520significant%2520loss%2520of%2520facial%2520expressions.%2520Currently%252C%250Athe%2520diagnosis%2520of%2520facial%2520paralysis%2520remains%2520a%2520challenging%2520task%252C%2520often%2520relying%250Aheavily%2520on%2520the%2520subjective%2520judgment%2520and%2520experience%2520of%2520clinicians%252C%2520which%2520can%250Aintroduce%2520variability%2520and%2520uncertainty%2520in%2520the%2520assessment%2520process.%2520One%2520promising%250Aapplication%2520in%2520real-life%2520situations%2520is%2520the%2520automatic%2520estimation%2520of%2520facial%250Aparalysis.%2520However%252C%2520the%2520scarcity%2520of%2520facial%2520paralysis%2520datasets%2520limits%2520the%250Adevelopment%2520of%2520robust%2520machine%2520learning%2520models%2520for%2520automated%2520diagnosis%2520and%250Atherapeutic%2520interventions.%2520To%2520this%2520end%252C%2520this%2520study%2520aims%2520to%2520synthesize%2520a%250Ahigh-quality%2520facial%2520paralysis%2520dataset%2520to%2520address%2520this%2520gap%252C%2520enabling%2520more%250Aaccurate%2520and%2520efficient%2520algorithm%2520training.%2520Specifically%252C%2520a%2520novel%2520Cycle%250ACross-Fusion%2520Expression%2520Generative%2520Model%2520%2528CCFExp%2529%2520based%2520on%2520the%2520diffusion%2520model%250Ais%2520proposed%2520to%2520combine%2520different%2520features%2520of%2520facial%2520information%2520and%2520enhance%2520the%250Avisual%2520details%2520of%2520facial%2520appearance%2520and%2520texture%2520in%2520facial%2520regions%252C%2520thus%250Acreating%2520synthetic%2520facial%2520images%2520that%2520accurately%2520represent%2520various%2520degrees%2520and%250Atypes%2520of%2520facial%2520paralysis.%2520We%2520have%2520qualitatively%2520and%2520quantitatively%2520evaluated%250Athe%2520proposed%2520method%2520on%2520the%2520commonly%2520used%2520public%2520clinical%2520datasets%2520of%2520facial%250Aparalysis%2520to%2520demonstrate%2520its%2520effectiveness.%2520Experimental%2520results%2520indicate%2520that%250Athe%2520proposed%2520method%2520surpasses%2520state-of-the-art%2520methods%252C%2520generating%2520more%250Arealistic%2520facial%2520images%2520and%2520maintaining%2520identity%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCFExp%3A%20Facial%20Image%20Synthesis%20with%20Cycle%20Cross-Fusion%20Diffusion%20Model%0A%20%20for%20Facial%20Paralysis%20Individuals&entry.906535625=Weixiang%20Gao%20and%20Yifan%20Xia&entry.1292438233=%20%20Facial%20paralysis%20is%20a%20debilitating%20condition%20that%20affects%20the%20movement%20of%0Afacial%20muscles%2C%20leading%20to%20a%20significant%20loss%20of%20facial%20expressions.%20Currently%2C%0Athe%20diagnosis%20of%20facial%20paralysis%20remains%20a%20challenging%20task%2C%20often%20relying%0Aheavily%20on%20the%20subjective%20judgment%20and%20experience%20of%20clinicians%2C%20which%20can%0Aintroduce%20variability%20and%20uncertainty%20in%20the%20assessment%20process.%20One%20promising%0Aapplication%20in%20real-life%20situations%20is%20the%20automatic%20estimation%20of%20facial%0Aparalysis.%20However%2C%20the%20scarcity%20of%20facial%20paralysis%20datasets%20limits%20the%0Adevelopment%20of%20robust%20machine%20learning%20models%20for%20automated%20diagnosis%20and%0Atherapeutic%20interventions.%20To%20this%20end%2C%20this%20study%20aims%20to%20synthesize%20a%0Ahigh-quality%20facial%20paralysis%20dataset%20to%20address%20this%20gap%2C%20enabling%20more%0Aaccurate%20and%20efficient%20algorithm%20training.%20Specifically%2C%20a%20novel%20Cycle%0ACross-Fusion%20Expression%20Generative%20Model%20%28CCFExp%29%20based%20on%20the%20diffusion%20model%0Ais%20proposed%20to%20combine%20different%20features%20of%20facial%20information%20and%20enhance%20the%0Avisual%20details%20of%20facial%20appearance%20and%20texture%20in%20facial%20regions%2C%20thus%0Acreating%20synthetic%20facial%20images%20that%20accurately%20represent%20various%20degrees%20and%0Atypes%20of%20facial%20paralysis.%20We%20have%20qualitatively%20and%20quantitatively%20evaluated%0Athe%20proposed%20method%20on%20the%20commonly%20used%20public%20clinical%20datasets%20of%20facial%0Aparalysis%20to%20demonstrate%20its%20effectiveness.%20Experimental%20results%20indicate%20that%0Athe%20proposed%20method%20surpasses%20state-of-the-art%20methods%2C%20generating%20more%0Arealistic%20facial%20images%20and%20maintaining%20identity%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07271v2&entry.124074799=Read"},
{"title": "Semi-Supervised Cognitive State Classification from Speech with\n  Multi-View Pseudo-Labeling", "author": "Yuanchao Li and Zixing Zhang and Jing Han and Peter Bell and Catherine Lai", "abstract": "  The lack of labeled data is a common challenge in speech classification\ntasks, particularly those requiring extensive subjective assessment, such as\ncognitive state classification. In this work, we propose a Semi-Supervised\nLearning (SSL) framework, introducing a novel multi-view pseudo-labeling method\nthat leverages both acoustic and linguistic characteristics to select the most\nconfident data for training the classification model. Acoustically, unlabeled\ndata are compared to labeled data using the Frechet audio distance, calculated\nfrom embeddings generated by multiple audio encoders. Linguistically, large\nlanguage models are prompted to revise automatic speech recognition\ntranscriptions and predict labels based on our proposed task-specific\nknowledge. High-confidence data are identified when pseudo-labels from both\nsources align, while mismatches are treated as low-confidence data. A bimodal\nclassifier is then trained to iteratively label the low-confidence data until a\npredefined criterion is met. We evaluate our SSL framework on emotion\nrecognition and dementia detection tasks. Experimental results demonstrate that\nour method achieves competitive performance compared to fully supervised\nlearning using only 30% of the labeled data and significantly outperforms two\nselected baselines.\n", "link": "http://arxiv.org/abs/2409.16937v2", "date": "2024-09-27", "relevancy": 1.5521, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5369}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5238}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Cognitive%20State%20Classification%20from%20Speech%20with%0A%20%20Multi-View%20Pseudo-Labeling&body=Title%3A%20Semi-Supervised%20Cognitive%20State%20Classification%20from%20Speech%20with%0A%20%20Multi-View%20Pseudo-Labeling%0AAuthor%3A%20Yuanchao%20Li%20and%20Zixing%20Zhang%20and%20Jing%20Han%20and%20Peter%20Bell%20and%20Catherine%20Lai%0AAbstract%3A%20%20%20The%20lack%20of%20labeled%20data%20is%20a%20common%20challenge%20in%20speech%20classification%0Atasks%2C%20particularly%20those%20requiring%20extensive%20subjective%20assessment%2C%20such%20as%0Acognitive%20state%20classification.%20In%20this%20work%2C%20we%20propose%20a%20Semi-Supervised%0ALearning%20%28SSL%29%20framework%2C%20introducing%20a%20novel%20multi-view%20pseudo-labeling%20method%0Athat%20leverages%20both%20acoustic%20and%20linguistic%20characteristics%20to%20select%20the%20most%0Aconfident%20data%20for%20training%20the%20classification%20model.%20Acoustically%2C%20unlabeled%0Adata%20are%20compared%20to%20labeled%20data%20using%20the%20Frechet%20audio%20distance%2C%20calculated%0Afrom%20embeddings%20generated%20by%20multiple%20audio%20encoders.%20Linguistically%2C%20large%0Alanguage%20models%20are%20prompted%20to%20revise%20automatic%20speech%20recognition%0Atranscriptions%20and%20predict%20labels%20based%20on%20our%20proposed%20task-specific%0Aknowledge.%20High-confidence%20data%20are%20identified%20when%20pseudo-labels%20from%20both%0Asources%20align%2C%20while%20mismatches%20are%20treated%20as%20low-confidence%20data.%20A%20bimodal%0Aclassifier%20is%20then%20trained%20to%20iteratively%20label%20the%20low-confidence%20data%20until%20a%0Apredefined%20criterion%20is%20met.%20We%20evaluate%20our%20SSL%20framework%20on%20emotion%0Arecognition%20and%20dementia%20detection%20tasks.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20achieves%20competitive%20performance%20compared%20to%20fully%20supervised%0Alearning%20using%20only%2030%25%20of%20the%20labeled%20data%20and%20significantly%20outperforms%20two%0Aselected%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Cognitive%2520State%2520Classification%2520from%2520Speech%2520with%250A%2520%2520Multi-View%2520Pseudo-Labeling%26entry.906535625%3DYuanchao%2520Li%2520and%2520Zixing%2520Zhang%2520and%2520Jing%2520Han%2520and%2520Peter%2520Bell%2520and%2520Catherine%2520Lai%26entry.1292438233%3D%2520%2520The%2520lack%2520of%2520labeled%2520data%2520is%2520a%2520common%2520challenge%2520in%2520speech%2520classification%250Atasks%252C%2520particularly%2520those%2520requiring%2520extensive%2520subjective%2520assessment%252C%2520such%2520as%250Acognitive%2520state%2520classification.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Semi-Supervised%250ALearning%2520%2528SSL%2529%2520framework%252C%2520introducing%2520a%2520novel%2520multi-view%2520pseudo-labeling%2520method%250Athat%2520leverages%2520both%2520acoustic%2520and%2520linguistic%2520characteristics%2520to%2520select%2520the%2520most%250Aconfident%2520data%2520for%2520training%2520the%2520classification%2520model.%2520Acoustically%252C%2520unlabeled%250Adata%2520are%2520compared%2520to%2520labeled%2520data%2520using%2520the%2520Frechet%2520audio%2520distance%252C%2520calculated%250Afrom%2520embeddings%2520generated%2520by%2520multiple%2520audio%2520encoders.%2520Linguistically%252C%2520large%250Alanguage%2520models%2520are%2520prompted%2520to%2520revise%2520automatic%2520speech%2520recognition%250Atranscriptions%2520and%2520predict%2520labels%2520based%2520on%2520our%2520proposed%2520task-specific%250Aknowledge.%2520High-confidence%2520data%2520are%2520identified%2520when%2520pseudo-labels%2520from%2520both%250Asources%2520align%252C%2520while%2520mismatches%2520are%2520treated%2520as%2520low-confidence%2520data.%2520A%2520bimodal%250Aclassifier%2520is%2520then%2520trained%2520to%2520iteratively%2520label%2520the%2520low-confidence%2520data%2520until%2520a%250Apredefined%2520criterion%2520is%2520met.%2520We%2520evaluate%2520our%2520SSL%2520framework%2520on%2520emotion%250Arecognition%2520and%2520dementia%2520detection%2520tasks.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520competitive%2520performance%2520compared%2520to%2520fully%2520supervised%250Alearning%2520using%2520only%252030%2525%2520of%2520the%2520labeled%2520data%2520and%2520significantly%2520outperforms%2520two%250Aselected%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Cognitive%20State%20Classification%20from%20Speech%20with%0A%20%20Multi-View%20Pseudo-Labeling&entry.906535625=Yuanchao%20Li%20and%20Zixing%20Zhang%20and%20Jing%20Han%20and%20Peter%20Bell%20and%20Catherine%20Lai&entry.1292438233=%20%20The%20lack%20of%20labeled%20data%20is%20a%20common%20challenge%20in%20speech%20classification%0Atasks%2C%20particularly%20those%20requiring%20extensive%20subjective%20assessment%2C%20such%20as%0Acognitive%20state%20classification.%20In%20this%20work%2C%20we%20propose%20a%20Semi-Supervised%0ALearning%20%28SSL%29%20framework%2C%20introducing%20a%20novel%20multi-view%20pseudo-labeling%20method%0Athat%20leverages%20both%20acoustic%20and%20linguistic%20characteristics%20to%20select%20the%20most%0Aconfident%20data%20for%20training%20the%20classification%20model.%20Acoustically%2C%20unlabeled%0Adata%20are%20compared%20to%20labeled%20data%20using%20the%20Frechet%20audio%20distance%2C%20calculated%0Afrom%20embeddings%20generated%20by%20multiple%20audio%20encoders.%20Linguistically%2C%20large%0Alanguage%20models%20are%20prompted%20to%20revise%20automatic%20speech%20recognition%0Atranscriptions%20and%20predict%20labels%20based%20on%20our%20proposed%20task-specific%0Aknowledge.%20High-confidence%20data%20are%20identified%20when%20pseudo-labels%20from%20both%0Asources%20align%2C%20while%20mismatches%20are%20treated%20as%20low-confidence%20data.%20A%20bimodal%0Aclassifier%20is%20then%20trained%20to%20iteratively%20label%20the%20low-confidence%20data%20until%20a%0Apredefined%20criterion%20is%20met.%20We%20evaluate%20our%20SSL%20framework%20on%20emotion%0Arecognition%20and%20dementia%20detection%20tasks.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20achieves%20competitive%20performance%20compared%20to%20fully%20supervised%0Alearning%20using%20only%2030%25%20of%20the%20labeled%20data%20and%20significantly%20outperforms%20two%0Aselected%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16937v2&entry.124074799=Read"},
{"title": "Implicit Image-to-Image Schrodinger Bridge for Image Restoration", "author": "Yuang Wang and Siyeop Yoon and Pengfei Jin and Matthew Tivnan and Sifan Song and Zhennong Chen and Rui Hu and Li Zhang and Quanzheng Li and Zhiqiang Chen and Dufan Wu", "abstract": "  Diffusion-based models are widely recognized for their effectiveness in image\nrestoration tasks; however, their iterative denoising process, which begins\nfrom Gaussian noise, often results in slow inference speeds. The Image-to-Image\nSchr\\\"odinger Bridge (I$^2$SB) presents a promising alternative by starting the\ngenerative process from corrupted images and leveraging training techniques\nfrom score-based diffusion models. In this paper, we introduce the Implicit\nImage-to-Image Schr\\\"odinger Bridge (I$^3$SB) to further accelerate the\ngenerative process of I$^2$SB. I$^3$SB reconfigures the generative process into\na non-Markovian framework by incorporating the initial corrupted image into\neach step, while ensuring that the marginal distribution aligns with that of\nI$^2$SB. This allows for the direct use of the pretrained network from I$^2$SB.\nExtensive experiments on natural images, human face images, and medical images\nvalidate the acceleration benefits of I$^3$SB. Compared to I$^2$SB, I$^3$SB\nachieves the same perceptual quality with fewer generative steps, while\nmaintaining equal or improved fidelity to the ground truth.\n", "link": "http://arxiv.org/abs/2403.06069v2", "date": "2024-09-27", "relevancy": 1.8226, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6125}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6056}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Image-to-Image%20Schrodinger%20Bridge%20for%20Image%20Restoration&body=Title%3A%20Implicit%20Image-to-Image%20Schrodinger%20Bridge%20for%20Image%20Restoration%0AAuthor%3A%20Yuang%20Wang%20and%20Siyeop%20Yoon%20and%20Pengfei%20Jin%20and%20Matthew%20Tivnan%20and%20Sifan%20Song%20and%20Zhennong%20Chen%20and%20Rui%20Hu%20and%20Li%20Zhang%20and%20Quanzheng%20Li%20and%20Zhiqiang%20Chen%20and%20Dufan%20Wu%0AAbstract%3A%20%20%20Diffusion-based%20models%20are%20widely%20recognized%20for%20their%20effectiveness%20in%20image%0Arestoration%20tasks%3B%20however%2C%20their%20iterative%20denoising%20process%2C%20which%20begins%0Afrom%20Gaussian%20noise%2C%20often%20results%20in%20slow%20inference%20speeds.%20The%20Image-to-Image%0ASchr%5C%22odinger%20Bridge%20%28I%24%5E2%24SB%29%20presents%20a%20promising%20alternative%20by%20starting%20the%0Agenerative%20process%20from%20corrupted%20images%20and%20leveraging%20training%20techniques%0Afrom%20score-based%20diffusion%20models.%20In%20this%20paper%2C%20we%20introduce%20the%20Implicit%0AImage-to-Image%20Schr%5C%22odinger%20Bridge%20%28I%24%5E3%24SB%29%20to%20further%20accelerate%20the%0Agenerative%20process%20of%20I%24%5E2%24SB.%20I%24%5E3%24SB%20reconfigures%20the%20generative%20process%20into%0Aa%20non-Markovian%20framework%20by%20incorporating%20the%20initial%20corrupted%20image%20into%0Aeach%20step%2C%20while%20ensuring%20that%20the%20marginal%20distribution%20aligns%20with%20that%20of%0AI%24%5E2%24SB.%20This%20allows%20for%20the%20direct%20use%20of%20the%20pretrained%20network%20from%20I%24%5E2%24SB.%0AExtensive%20experiments%20on%20natural%20images%2C%20human%20face%20images%2C%20and%20medical%20images%0Avalidate%20the%20acceleration%20benefits%20of%20I%24%5E3%24SB.%20Compared%20to%20I%24%5E2%24SB%2C%20I%24%5E3%24SB%0Aachieves%20the%20same%20perceptual%20quality%20with%20fewer%20generative%20steps%2C%20while%0Amaintaining%20equal%20or%20improved%20fidelity%20to%20the%20ground%20truth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Image-to-Image%2520Schrodinger%2520Bridge%2520for%2520Image%2520Restoration%26entry.906535625%3DYuang%2520Wang%2520and%2520Siyeop%2520Yoon%2520and%2520Pengfei%2520Jin%2520and%2520Matthew%2520Tivnan%2520and%2520Sifan%2520Song%2520and%2520Zhennong%2520Chen%2520and%2520Rui%2520Hu%2520and%2520Li%2520Zhang%2520and%2520Quanzheng%2520Li%2520and%2520Zhiqiang%2520Chen%2520and%2520Dufan%2520Wu%26entry.1292438233%3D%2520%2520Diffusion-based%2520models%2520are%2520widely%2520recognized%2520for%2520their%2520effectiveness%2520in%2520image%250Arestoration%2520tasks%253B%2520however%252C%2520their%2520iterative%2520denoising%2520process%252C%2520which%2520begins%250Afrom%2520Gaussian%2520noise%252C%2520often%2520results%2520in%2520slow%2520inference%2520speeds.%2520The%2520Image-to-Image%250ASchr%255C%2522odinger%2520Bridge%2520%2528I%2524%255E2%2524SB%2529%2520presents%2520a%2520promising%2520alternative%2520by%2520starting%2520the%250Agenerative%2520process%2520from%2520corrupted%2520images%2520and%2520leveraging%2520training%2520techniques%250Afrom%2520score-based%2520diffusion%2520models.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Implicit%250AImage-to-Image%2520Schr%255C%2522odinger%2520Bridge%2520%2528I%2524%255E3%2524SB%2529%2520to%2520further%2520accelerate%2520the%250Agenerative%2520process%2520of%2520I%2524%255E2%2524SB.%2520I%2524%255E3%2524SB%2520reconfigures%2520the%2520generative%2520process%2520into%250Aa%2520non-Markovian%2520framework%2520by%2520incorporating%2520the%2520initial%2520corrupted%2520image%2520into%250Aeach%2520step%252C%2520while%2520ensuring%2520that%2520the%2520marginal%2520distribution%2520aligns%2520with%2520that%2520of%250AI%2524%255E2%2524SB.%2520This%2520allows%2520for%2520the%2520direct%2520use%2520of%2520the%2520pretrained%2520network%2520from%2520I%2524%255E2%2524SB.%250AExtensive%2520experiments%2520on%2520natural%2520images%252C%2520human%2520face%2520images%252C%2520and%2520medical%2520images%250Avalidate%2520the%2520acceleration%2520benefits%2520of%2520I%2524%255E3%2524SB.%2520Compared%2520to%2520I%2524%255E2%2524SB%252C%2520I%2524%255E3%2524SB%250Aachieves%2520the%2520same%2520perceptual%2520quality%2520with%2520fewer%2520generative%2520steps%252C%2520while%250Amaintaining%2520equal%2520or%2520improved%2520fidelity%2520to%2520the%2520ground%2520truth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Image-to-Image%20Schrodinger%20Bridge%20for%20Image%20Restoration&entry.906535625=Yuang%20Wang%20and%20Siyeop%20Yoon%20and%20Pengfei%20Jin%20and%20Matthew%20Tivnan%20and%20Sifan%20Song%20and%20Zhennong%20Chen%20and%20Rui%20Hu%20and%20Li%20Zhang%20and%20Quanzheng%20Li%20and%20Zhiqiang%20Chen%20and%20Dufan%20Wu&entry.1292438233=%20%20Diffusion-based%20models%20are%20widely%20recognized%20for%20their%20effectiveness%20in%20image%0Arestoration%20tasks%3B%20however%2C%20their%20iterative%20denoising%20process%2C%20which%20begins%0Afrom%20Gaussian%20noise%2C%20often%20results%20in%20slow%20inference%20speeds.%20The%20Image-to-Image%0ASchr%5C%22odinger%20Bridge%20%28I%24%5E2%24SB%29%20presents%20a%20promising%20alternative%20by%20starting%20the%0Agenerative%20process%20from%20corrupted%20images%20and%20leveraging%20training%20techniques%0Afrom%20score-based%20diffusion%20models.%20In%20this%20paper%2C%20we%20introduce%20the%20Implicit%0AImage-to-Image%20Schr%5C%22odinger%20Bridge%20%28I%24%5E3%24SB%29%20to%20further%20accelerate%20the%0Agenerative%20process%20of%20I%24%5E2%24SB.%20I%24%5E3%24SB%20reconfigures%20the%20generative%20process%20into%0Aa%20non-Markovian%20framework%20by%20incorporating%20the%20initial%20corrupted%20image%20into%0Aeach%20step%2C%20while%20ensuring%20that%20the%20marginal%20distribution%20aligns%20with%20that%20of%0AI%24%5E2%24SB.%20This%20allows%20for%20the%20direct%20use%20of%20the%20pretrained%20network%20from%20I%24%5E2%24SB.%0AExtensive%20experiments%20on%20natural%20images%2C%20human%20face%20images%2C%20and%20medical%20images%0Avalidate%20the%20acceleration%20benefits%20of%20I%24%5E3%24SB.%20Compared%20to%20I%24%5E2%24SB%2C%20I%24%5E3%24SB%0Aachieves%20the%20same%20perceptual%20quality%20with%20fewer%20generative%20steps%2C%20while%0Amaintaining%20equal%20or%20improved%20fidelity%20to%20the%20ground%20truth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06069v2&entry.124074799=Read"},
{"title": "A New Dataset for Monocular Depth Estimation Under Viewpoint Shifts", "author": "Aurel Pjetri and Stefano Caprasecca and Leonardo Taccari and Matteo Simoncini and Henrique Pi\u00f1eiro Monteagudo and Walter Wallace and Douglas Coimbra de Andrade and Francesco Sambo and Andrew David Bagdanov", "abstract": "  Monocular depth estimation is a critical task for autonomous driving and many\nother computer vision applications. While significant progress has been made in\nthis field, the effects of viewpoint shifts on depth estimation models remain\nlargely underexplored. This paper introduces a novel dataset and evaluation\nmethodology to quantify the impact of different camera positions and\norientations on monocular depth estimation performance. We propose a ground\ntruth strategy based on homography estimation and object detection, eliminating\nthe need for expensive lidar sensors. We collect a diverse dataset of road\nscenes from multiple viewpoints and use it to assess the robustness of a modern\ndepth estimation model to geometric shifts. After assessing the validity of our\nstrategy on a public dataset, we provide valuable insights into the limitations\nof current models and highlight the importance of considering viewpoint\nvariations in real-world applications.\n", "link": "http://arxiv.org/abs/2409.17851v2", "date": "2024-09-27", "relevancy": 1.6327, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5557}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5381}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Dataset%20for%20Monocular%20Depth%20Estimation%20Under%20Viewpoint%20Shifts&body=Title%3A%20A%20New%20Dataset%20for%20Monocular%20Depth%20Estimation%20Under%20Viewpoint%20Shifts%0AAuthor%3A%20Aurel%20Pjetri%20and%20Stefano%20Caprasecca%20and%20Leonardo%20Taccari%20and%20Matteo%20Simoncini%20and%20Henrique%20Pi%C3%B1eiro%20Monteagudo%20and%20Walter%20Wallace%20and%20Douglas%20Coimbra%20de%20Andrade%20and%20Francesco%20Sambo%20and%20Andrew%20David%20Bagdanov%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20is%20a%20critical%20task%20for%20autonomous%20driving%20and%20many%0Aother%20computer%20vision%20applications.%20While%20significant%20progress%20has%20been%20made%20in%0Athis%20field%2C%20the%20effects%20of%20viewpoint%20shifts%20on%20depth%20estimation%20models%20remain%0Alargely%20underexplored.%20This%20paper%20introduces%20a%20novel%20dataset%20and%20evaluation%0Amethodology%20to%20quantify%20the%20impact%20of%20different%20camera%20positions%20and%0Aorientations%20on%20monocular%20depth%20estimation%20performance.%20We%20propose%20a%20ground%0Atruth%20strategy%20based%20on%20homography%20estimation%20and%20object%20detection%2C%20eliminating%0Athe%20need%20for%20expensive%20lidar%20sensors.%20We%20collect%20a%20diverse%20dataset%20of%20road%0Ascenes%20from%20multiple%20viewpoints%20and%20use%20it%20to%20assess%20the%20robustness%20of%20a%20modern%0Adepth%20estimation%20model%20to%20geometric%20shifts.%20After%20assessing%20the%20validity%20of%20our%0Astrategy%20on%20a%20public%20dataset%2C%20we%20provide%20valuable%20insights%20into%20the%20limitations%0Aof%20current%20models%20and%20highlight%20the%20importance%20of%20considering%20viewpoint%0Avariations%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Dataset%2520for%2520Monocular%2520Depth%2520Estimation%2520Under%2520Viewpoint%2520Shifts%26entry.906535625%3DAurel%2520Pjetri%2520and%2520Stefano%2520Caprasecca%2520and%2520Leonardo%2520Taccari%2520and%2520Matteo%2520Simoncini%2520and%2520Henrique%2520Pi%25C3%25B1eiro%2520Monteagudo%2520and%2520Walter%2520Wallace%2520and%2520Douglas%2520Coimbra%2520de%2520Andrade%2520and%2520Francesco%2520Sambo%2520and%2520Andrew%2520David%2520Bagdanov%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520is%2520a%2520critical%2520task%2520for%2520autonomous%2520driving%2520and%2520many%250Aother%2520computer%2520vision%2520applications.%2520While%2520significant%2520progress%2520has%2520been%2520made%2520in%250Athis%2520field%252C%2520the%2520effects%2520of%2520viewpoint%2520shifts%2520on%2520depth%2520estimation%2520models%2520remain%250Alargely%2520underexplored.%2520This%2520paper%2520introduces%2520a%2520novel%2520dataset%2520and%2520evaluation%250Amethodology%2520to%2520quantify%2520the%2520impact%2520of%2520different%2520camera%2520positions%2520and%250Aorientations%2520on%2520monocular%2520depth%2520estimation%2520performance.%2520We%2520propose%2520a%2520ground%250Atruth%2520strategy%2520based%2520on%2520homography%2520estimation%2520and%2520object%2520detection%252C%2520eliminating%250Athe%2520need%2520for%2520expensive%2520lidar%2520sensors.%2520We%2520collect%2520a%2520diverse%2520dataset%2520of%2520road%250Ascenes%2520from%2520multiple%2520viewpoints%2520and%2520use%2520it%2520to%2520assess%2520the%2520robustness%2520of%2520a%2520modern%250Adepth%2520estimation%2520model%2520to%2520geometric%2520shifts.%2520After%2520assessing%2520the%2520validity%2520of%2520our%250Astrategy%2520on%2520a%2520public%2520dataset%252C%2520we%2520provide%2520valuable%2520insights%2520into%2520the%2520limitations%250Aof%2520current%2520models%2520and%2520highlight%2520the%2520importance%2520of%2520considering%2520viewpoint%250Avariations%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Dataset%20for%20Monocular%20Depth%20Estimation%20Under%20Viewpoint%20Shifts&entry.906535625=Aurel%20Pjetri%20and%20Stefano%20Caprasecca%20and%20Leonardo%20Taccari%20and%20Matteo%20Simoncini%20and%20Henrique%20Pi%C3%B1eiro%20Monteagudo%20and%20Walter%20Wallace%20and%20Douglas%20Coimbra%20de%20Andrade%20and%20Francesco%20Sambo%20and%20Andrew%20David%20Bagdanov&entry.1292438233=%20%20Monocular%20depth%20estimation%20is%20a%20critical%20task%20for%20autonomous%20driving%20and%20many%0Aother%20computer%20vision%20applications.%20While%20significant%20progress%20has%20been%20made%20in%0Athis%20field%2C%20the%20effects%20of%20viewpoint%20shifts%20on%20depth%20estimation%20models%20remain%0Alargely%20underexplored.%20This%20paper%20introduces%20a%20novel%20dataset%20and%20evaluation%0Amethodology%20to%20quantify%20the%20impact%20of%20different%20camera%20positions%20and%0Aorientations%20on%20monocular%20depth%20estimation%20performance.%20We%20propose%20a%20ground%0Atruth%20strategy%20based%20on%20homography%20estimation%20and%20object%20detection%2C%20eliminating%0Athe%20need%20for%20expensive%20lidar%20sensors.%20We%20collect%20a%20diverse%20dataset%20of%20road%0Ascenes%20from%20multiple%20viewpoints%20and%20use%20it%20to%20assess%20the%20robustness%20of%20a%20modern%0Adepth%20estimation%20model%20to%20geometric%20shifts.%20After%20assessing%20the%20validity%20of%20our%0Astrategy%20on%20a%20public%20dataset%2C%20we%20provide%20valuable%20insights%20into%20the%20limitations%0Aof%20current%20models%20and%20highlight%20the%20importance%20of%20considering%20viewpoint%0Avariations%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17851v2&entry.124074799=Read"},
{"title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard\n  for Prompt Attacks", "author": "Giandomenico Cornacchia and Giulio Zizzo and Kieran Fraser and Muhammad Zaid Hamed and Ambrish Rawat and Mark Purcell", "abstract": "  The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks.\n", "link": "http://arxiv.org/abs/2409.17699v2", "date": "2024-09-27", "relevancy": 1.3718, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4688}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4542}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoJE%3A%20Mixture%20of%20Jailbreak%20Experts%2C%20Naive%20Tabular%20Classifiers%20as%20Guard%0A%20%20for%20Prompt%20Attacks&body=Title%3A%20MoJE%3A%20Mixture%20of%20Jailbreak%20Experts%2C%20Naive%20Tabular%20Classifiers%20as%20Guard%0A%20%20for%20Prompt%20Attacks%0AAuthor%3A%20Giandomenico%20Cornacchia%20and%20Giulio%20Zizzo%20and%20Kieran%20Fraser%20and%20Muhammad%20Zaid%20Hamed%20and%20Ambrish%20Rawat%20and%20Mark%20Purcell%0AAbstract%3A%20%20%20The%20proliferation%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20diverse%20applications%0Aunderscores%20the%20pressing%20need%20for%20robust%20security%20measures%20to%20thwart%20potential%0Ajailbreak%20attacks.%20These%20attacks%20exploit%20vulnerabilities%20within%20LLMs%2C%20endanger%0Adata%20integrity%20and%20user%20privacy.%20Guardrails%20serve%20as%20crucial%20protective%0Amechanisms%20against%20such%20threats%2C%20but%20existing%20models%20often%20fall%20short%20in%20terms%0Aof%20both%20detection%20accuracy%2C%20and%20computational%20efficiency.%20This%20paper%20advocates%0Afor%20the%20significance%20of%20jailbreak%20attack%20prevention%20on%20LLMs%2C%20and%20emphasises%20the%0Arole%20of%20input%20guardrails%20in%20safeguarding%20these%20models.%20We%20introduce%20MoJE%0A%28Mixture%20of%20Jailbreak%20Expert%29%2C%20a%20novel%20guardrail%20architecture%20designed%20to%0Asurpass%20current%20limitations%20in%20existing%20state-of-the-art%20guardrails.%20By%0Aemploying%20simple%20linguistic%20statistical%20techniques%2C%20MoJE%20excels%20in%20detecting%0Ajailbreak%20attacks%20while%20maintaining%20minimal%20computational%20overhead%20during%20model%0Ainference.%20Through%20rigorous%20experimentation%2C%20MoJE%20demonstrates%20superior%0Aperformance%20capable%20of%20detecting%2090%25%20of%20the%20attacks%20without%20compromising%20benign%0Aprompts%2C%20enhancing%20LLMs%20security%20against%20jailbreak%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17699v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoJE%253A%2520Mixture%2520of%2520Jailbreak%2520Experts%252C%2520Naive%2520Tabular%2520Classifiers%2520as%2520Guard%250A%2520%2520for%2520Prompt%2520Attacks%26entry.906535625%3DGiandomenico%2520Cornacchia%2520and%2520Giulio%2520Zizzo%2520and%2520Kieran%2520Fraser%2520and%2520Muhammad%2520Zaid%2520Hamed%2520and%2520Ambrish%2520Rawat%2520and%2520Mark%2520Purcell%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520diverse%2520applications%250Aunderscores%2520the%2520pressing%2520need%2520for%2520robust%2520security%2520measures%2520to%2520thwart%2520potential%250Ajailbreak%2520attacks.%2520These%2520attacks%2520exploit%2520vulnerabilities%2520within%2520LLMs%252C%2520endanger%250Adata%2520integrity%2520and%2520user%2520privacy.%2520Guardrails%2520serve%2520as%2520crucial%2520protective%250Amechanisms%2520against%2520such%2520threats%252C%2520but%2520existing%2520models%2520often%2520fall%2520short%2520in%2520terms%250Aof%2520both%2520detection%2520accuracy%252C%2520and%2520computational%2520efficiency.%2520This%2520paper%2520advocates%250Afor%2520the%2520significance%2520of%2520jailbreak%2520attack%2520prevention%2520on%2520LLMs%252C%2520and%2520emphasises%2520the%250Arole%2520of%2520input%2520guardrails%2520in%2520safeguarding%2520these%2520models.%2520We%2520introduce%2520MoJE%250A%2528Mixture%2520of%2520Jailbreak%2520Expert%2529%252C%2520a%2520novel%2520guardrail%2520architecture%2520designed%2520to%250Asurpass%2520current%2520limitations%2520in%2520existing%2520state-of-the-art%2520guardrails.%2520By%250Aemploying%2520simple%2520linguistic%2520statistical%2520techniques%252C%2520MoJE%2520excels%2520in%2520detecting%250Ajailbreak%2520attacks%2520while%2520maintaining%2520minimal%2520computational%2520overhead%2520during%2520model%250Ainference.%2520Through%2520rigorous%2520experimentation%252C%2520MoJE%2520demonstrates%2520superior%250Aperformance%2520capable%2520of%2520detecting%252090%2525%2520of%2520the%2520attacks%2520without%2520compromising%2520benign%250Aprompts%252C%2520enhancing%2520LLMs%2520security%2520against%2520jailbreak%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17699v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoJE%3A%20Mixture%20of%20Jailbreak%20Experts%2C%20Naive%20Tabular%20Classifiers%20as%20Guard%0A%20%20for%20Prompt%20Attacks&entry.906535625=Giandomenico%20Cornacchia%20and%20Giulio%20Zizzo%20and%20Kieran%20Fraser%20and%20Muhammad%20Zaid%20Hamed%20and%20Ambrish%20Rawat%20and%20Mark%20Purcell&entry.1292438233=%20%20The%20proliferation%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20diverse%20applications%0Aunderscores%20the%20pressing%20need%20for%20robust%20security%20measures%20to%20thwart%20potential%0Ajailbreak%20attacks.%20These%20attacks%20exploit%20vulnerabilities%20within%20LLMs%2C%20endanger%0Adata%20integrity%20and%20user%20privacy.%20Guardrails%20serve%20as%20crucial%20protective%0Amechanisms%20against%20such%20threats%2C%20but%20existing%20models%20often%20fall%20short%20in%20terms%0Aof%20both%20detection%20accuracy%2C%20and%20computational%20efficiency.%20This%20paper%20advocates%0Afor%20the%20significance%20of%20jailbreak%20attack%20prevention%20on%20LLMs%2C%20and%20emphasises%20the%0Arole%20of%20input%20guardrails%20in%20safeguarding%20these%20models.%20We%20introduce%20MoJE%0A%28Mixture%20of%20Jailbreak%20Expert%29%2C%20a%20novel%20guardrail%20architecture%20designed%20to%0Asurpass%20current%20limitations%20in%20existing%20state-of-the-art%20guardrails.%20By%0Aemploying%20simple%20linguistic%20statistical%20techniques%2C%20MoJE%20excels%20in%20detecting%0Ajailbreak%20attacks%20while%20maintaining%20minimal%20computational%20overhead%20during%20model%0Ainference.%20Through%20rigorous%20experimentation%2C%20MoJE%20demonstrates%20superior%0Aperformance%20capable%20of%20detecting%2090%25%20of%20the%20attacks%20without%20compromising%20benign%0Aprompts%2C%20enhancing%20LLMs%20security%20against%20jailbreak%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17699v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


