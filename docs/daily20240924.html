<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240922.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A Controlled Study on Long Context Extension and Generalization in LLMs", "author": "Yi Lu and Jing Nathan Yan and Songlin Yang and Justin T. Chiu and Siyu Ren and Fei Yuan and Wenting Zhao and Zhiyong Wu and Alexander M. Rush", "abstract": "  Broad textual understanding and in-context learning require language models\nthat utilize full document contexts. Due to the implementation challenges\nassociated with directly training long-context models, many methods have been\nproposed for extending models to handle long contexts. However, owing to\ndifferences in data and model classes, it has been challenging to compare these\napproaches, leading to uncertainty as to how to evaluate long-context\nperformance and whether it differs from standard evaluation. We implement a\ncontrolled protocol for extension methods with a standardized evaluation,\nutilizing consistent base models and extension data. Our study yields several\ninsights into long-context behavior. First, we reaffirm the critical role of\nperplexity as a general-purpose performance indicator even in longer-context\ntasks. Second, we find that current approximate attention methods\nsystematically underperform across long-context tasks. Finally, we confirm that\nexact fine-tuning based methods are generally effective within the range of\ntheir extension, whereas extrapolation remains challenging. All codebases,\nmodels, and checkpoints will be made available open-source, promoting\ntransparency and facilitating further research in this critical area of AI\ndevelopment.\n", "link": "http://arxiv.org/abs/2409.12181v2", "date": "2024-09-23", "relevancy": 2.726, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Controlled%20Study%20on%20Long%20Context%20Extension%20and%20Generalization%20in%20LLMs&body=Title%3A%20A%20Controlled%20Study%20on%20Long%20Context%20Extension%20and%20Generalization%20in%20LLMs%0AAuthor%3A%20Yi%20Lu%20and%20Jing%20Nathan%20Yan%20and%20Songlin%20Yang%20and%20Justin%20T.%20Chiu%20and%20Siyu%20Ren%20and%20Fei%20Yuan%20and%20Wenting%20Zhao%20and%20Zhiyong%20Wu%20and%20Alexander%20M.%20Rush%0AAbstract%3A%20%20%20Broad%20textual%20understanding%20and%20in-context%20learning%20require%20language%20models%0Athat%20utilize%20full%20document%20contexts.%20Due%20to%20the%20implementation%20challenges%0Aassociated%20with%20directly%20training%20long-context%20models%2C%20many%20methods%20have%20been%0Aproposed%20for%20extending%20models%20to%20handle%20long%20contexts.%20However%2C%20owing%20to%0Adifferences%20in%20data%20and%20model%20classes%2C%20it%20has%20been%20challenging%20to%20compare%20these%0Aapproaches%2C%20leading%20to%20uncertainty%20as%20to%20how%20to%20evaluate%20long-context%0Aperformance%20and%20whether%20it%20differs%20from%20standard%20evaluation.%20We%20implement%20a%0Acontrolled%20protocol%20for%20extension%20methods%20with%20a%20standardized%20evaluation%2C%0Autilizing%20consistent%20base%20models%20and%20extension%20data.%20Our%20study%20yields%20several%0Ainsights%20into%20long-context%20behavior.%20First%2C%20we%20reaffirm%20the%20critical%20role%20of%0Aperplexity%20as%20a%20general-purpose%20performance%20indicator%20even%20in%20longer-context%0Atasks.%20Second%2C%20we%20find%20that%20current%20approximate%20attention%20methods%0Asystematically%20underperform%20across%20long-context%20tasks.%20Finally%2C%20we%20confirm%20that%0Aexact%20fine-tuning%20based%20methods%20are%20generally%20effective%20within%20the%20range%20of%0Atheir%20extension%2C%20whereas%20extrapolation%20remains%20challenging.%20All%20codebases%2C%0Amodels%2C%20and%20checkpoints%20will%20be%20made%20available%20open-source%2C%20promoting%0Atransparency%20and%20facilitating%20further%20research%20in%20this%20critical%20area%20of%20AI%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12181v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Controlled%2520Study%2520on%2520Long%2520Context%2520Extension%2520and%2520Generalization%2520in%2520LLMs%26entry.906535625%3DYi%2520Lu%2520and%2520Jing%2520Nathan%2520Yan%2520and%2520Songlin%2520Yang%2520and%2520Justin%2520T.%2520Chiu%2520and%2520Siyu%2520Ren%2520and%2520Fei%2520Yuan%2520and%2520Wenting%2520Zhao%2520and%2520Zhiyong%2520Wu%2520and%2520Alexander%2520M.%2520Rush%26entry.1292438233%3D%2520%2520Broad%2520textual%2520understanding%2520and%2520in-context%2520learning%2520require%2520language%2520models%250Athat%2520utilize%2520full%2520document%2520contexts.%2520Due%2520to%2520the%2520implementation%2520challenges%250Aassociated%2520with%2520directly%2520training%2520long-context%2520models%252C%2520many%2520methods%2520have%2520been%250Aproposed%2520for%2520extending%2520models%2520to%2520handle%2520long%2520contexts.%2520However%252C%2520owing%2520to%250Adifferences%2520in%2520data%2520and%2520model%2520classes%252C%2520it%2520has%2520been%2520challenging%2520to%2520compare%2520these%250Aapproaches%252C%2520leading%2520to%2520uncertainty%2520as%2520to%2520how%2520to%2520evaluate%2520long-context%250Aperformance%2520and%2520whether%2520it%2520differs%2520from%2520standard%2520evaluation.%2520We%2520implement%2520a%250Acontrolled%2520protocol%2520for%2520extension%2520methods%2520with%2520a%2520standardized%2520evaluation%252C%250Autilizing%2520consistent%2520base%2520models%2520and%2520extension%2520data.%2520Our%2520study%2520yields%2520several%250Ainsights%2520into%2520long-context%2520behavior.%2520First%252C%2520we%2520reaffirm%2520the%2520critical%2520role%2520of%250Aperplexity%2520as%2520a%2520general-purpose%2520performance%2520indicator%2520even%2520in%2520longer-context%250Atasks.%2520Second%252C%2520we%2520find%2520that%2520current%2520approximate%2520attention%2520methods%250Asystematically%2520underperform%2520across%2520long-context%2520tasks.%2520Finally%252C%2520we%2520confirm%2520that%250Aexact%2520fine-tuning%2520based%2520methods%2520are%2520generally%2520effective%2520within%2520the%2520range%2520of%250Atheir%2520extension%252C%2520whereas%2520extrapolation%2520remains%2520challenging.%2520All%2520codebases%252C%250Amodels%252C%2520and%2520checkpoints%2520will%2520be%2520made%2520available%2520open-source%252C%2520promoting%250Atransparency%2520and%2520facilitating%2520further%2520research%2520in%2520this%2520critical%2520area%2520of%2520AI%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12181v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Controlled%20Study%20on%20Long%20Context%20Extension%20and%20Generalization%20in%20LLMs&entry.906535625=Yi%20Lu%20and%20Jing%20Nathan%20Yan%20and%20Songlin%20Yang%20and%20Justin%20T.%20Chiu%20and%20Siyu%20Ren%20and%20Fei%20Yuan%20and%20Wenting%20Zhao%20and%20Zhiyong%20Wu%20and%20Alexander%20M.%20Rush&entry.1292438233=%20%20Broad%20textual%20understanding%20and%20in-context%20learning%20require%20language%20models%0Athat%20utilize%20full%20document%20contexts.%20Due%20to%20the%20implementation%20challenges%0Aassociated%20with%20directly%20training%20long-context%20models%2C%20many%20methods%20have%20been%0Aproposed%20for%20extending%20models%20to%20handle%20long%20contexts.%20However%2C%20owing%20to%0Adifferences%20in%20data%20and%20model%20classes%2C%20it%20has%20been%20challenging%20to%20compare%20these%0Aapproaches%2C%20leading%20to%20uncertainty%20as%20to%20how%20to%20evaluate%20long-context%0Aperformance%20and%20whether%20it%20differs%20from%20standard%20evaluation.%20We%20implement%20a%0Acontrolled%20protocol%20for%20extension%20methods%20with%20a%20standardized%20evaluation%2C%0Autilizing%20consistent%20base%20models%20and%20extension%20data.%20Our%20study%20yields%20several%0Ainsights%20into%20long-context%20behavior.%20First%2C%20we%20reaffirm%20the%20critical%20role%20of%0Aperplexity%20as%20a%20general-purpose%20performance%20indicator%20even%20in%20longer-context%0Atasks.%20Second%2C%20we%20find%20that%20current%20approximate%20attention%20methods%0Asystematically%20underperform%20across%20long-context%20tasks.%20Finally%2C%20we%20confirm%20that%0Aexact%20fine-tuning%20based%20methods%20are%20generally%20effective%20within%20the%20range%20of%0Atheir%20extension%2C%20whereas%20extrapolation%20remains%20challenging.%20All%20codebases%2C%0Amodels%2C%20and%20checkpoints%20will%20be%20made%20available%20open-source%2C%20promoting%0Atransparency%20and%20facilitating%20further%20research%20in%20this%20critical%20area%20of%20AI%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12181v2&entry.124074799=Read"},
{"title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models", "author": " EverestAI and  : and Sijing Chen and Yuan Feng and Laipeng He and Tianwei He and Wendi He and Yanni Hu and Bin Lin and Yiting Lin and Yu Pan and Pengfei Tan and Chengwei Tian and Chen Wang and Zhicheng Wang and Ruoye Xie and Jixun Yao and Quanlei Yan and Yuguang Yang and Jianhao Ye and Jingjing Yin and Yanzhen Yu and Huimin Zhang and Xiang Zhang and Guangcheng Zhao and Hongbin Zhou and Pengpeng Zou", "abstract": "  With the advent of the big data and large language model era, zero-shot\npersonalized rapid customization has emerged as a significant trend. In this\nreport, we introduce Takin AudioLLM, a series of techniques and models, mainly\nincluding Takin TTS, Takin VC, and Takin Morphing, specifically designed for\naudiobook production. These models are capable of zero-shot speech production,\ngenerating high-quality speech that is nearly indistinguishable from real human\nspeech and facilitating individuals to customize the speech content according\nto their own needs. Specifically, we first introduce Takin TTS, a neural codec\nlanguage model that builds upon an enhanced neural speech codec and a\nmulti-task training framework, capable of generating high-fidelity natural\nspeech in a zero-shot way. For Takin VC, we advocate an effective content and\ntimbre joint modeling approach to improve the speaker similarity, while\nadvocating for a conditional flow matching based decoder to further enhance its\nnaturalness and expressiveness. Last, we propose the Takin Morphing system with\nhighly decoupled and advanced timbre and prosody modeling approaches, which\nenables individuals to customize speech production with their preferred timbre\nand prosody in a precise and controllable manner. Extensive experiments\nvalidate the effectiveness and robustness of our Takin AudioLLM series models.\nFor detailed demos, please refer to\nhttps://everest-ai.github.io/takinaudiollm/.\n", "link": "http://arxiv.org/abs/2409.12139v2", "date": "2024-09-23", "relevancy": 2.1542, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5862}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5429}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Takin%3A%20A%20Cohort%20of%20Superior%20Quality%20Zero-shot%20Speech%20Generation%20Models&body=Title%3A%20Takin%3A%20A%20Cohort%20of%20Superior%20Quality%20Zero-shot%20Speech%20Generation%20Models%0AAuthor%3A%20%20EverestAI%20and%20%20%3A%20and%20Sijing%20Chen%20and%20Yuan%20Feng%20and%20Laipeng%20He%20and%20Tianwei%20He%20and%20Wendi%20He%20and%20Yanni%20Hu%20and%20Bin%20Lin%20and%20Yiting%20Lin%20and%20Yu%20Pan%20and%20Pengfei%20Tan%20and%20Chengwei%20Tian%20and%20Chen%20Wang%20and%20Zhicheng%20Wang%20and%20Ruoye%20Xie%20and%20Jixun%20Yao%20and%20Quanlei%20Yan%20and%20Yuguang%20Yang%20and%20Jianhao%20Ye%20and%20Jingjing%20Yin%20and%20Yanzhen%20Yu%20and%20Huimin%20Zhang%20and%20Xiang%20Zhang%20and%20Guangcheng%20Zhao%20and%20Hongbin%20Zhou%20and%20Pengpeng%20Zou%0AAbstract%3A%20%20%20With%20the%20advent%20of%20the%20big%20data%20and%20large%20language%20model%20era%2C%20zero-shot%0Apersonalized%20rapid%20customization%20has%20emerged%20as%20a%20significant%20trend.%20In%20this%0Areport%2C%20we%20introduce%20Takin%20AudioLLM%2C%20a%20series%20of%20techniques%20and%20models%2C%20mainly%0Aincluding%20Takin%20TTS%2C%20Takin%20VC%2C%20and%20Takin%20Morphing%2C%20specifically%20designed%20for%0Aaudiobook%20production.%20These%20models%20are%20capable%20of%20zero-shot%20speech%20production%2C%0Agenerating%20high-quality%20speech%20that%20is%20nearly%20indistinguishable%20from%20real%20human%0Aspeech%20and%20facilitating%20individuals%20to%20customize%20the%20speech%20content%20according%0Ato%20their%20own%20needs.%20Specifically%2C%20we%20first%20introduce%20Takin%20TTS%2C%20a%20neural%20codec%0Alanguage%20model%20that%20builds%20upon%20an%20enhanced%20neural%20speech%20codec%20and%20a%0Amulti-task%20training%20framework%2C%20capable%20of%20generating%20high-fidelity%20natural%0Aspeech%20in%20a%20zero-shot%20way.%20For%20Takin%20VC%2C%20we%20advocate%20an%20effective%20content%20and%0Atimbre%20joint%20modeling%20approach%20to%20improve%20the%20speaker%20similarity%2C%20while%0Aadvocating%20for%20a%20conditional%20flow%20matching%20based%20decoder%20to%20further%20enhance%20its%0Anaturalness%20and%20expressiveness.%20Last%2C%20we%20propose%20the%20Takin%20Morphing%20system%20with%0Ahighly%20decoupled%20and%20advanced%20timbre%20and%20prosody%20modeling%20approaches%2C%20which%0Aenables%20individuals%20to%20customize%20speech%20production%20with%20their%20preferred%20timbre%0Aand%20prosody%20in%20a%20precise%20and%20controllable%20manner.%20Extensive%20experiments%0Avalidate%20the%20effectiveness%20and%20robustness%20of%20our%20Takin%20AudioLLM%20series%20models.%0AFor%20detailed%20demos%2C%20please%20refer%20to%0Ahttps%3A//everest-ai.github.io/takinaudiollm/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12139v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTakin%253A%2520A%2520Cohort%2520of%2520Superior%2520Quality%2520Zero-shot%2520Speech%2520Generation%2520Models%26entry.906535625%3D%2520EverestAI%2520and%2520%2520%253A%2520and%2520Sijing%2520Chen%2520and%2520Yuan%2520Feng%2520and%2520Laipeng%2520He%2520and%2520Tianwei%2520He%2520and%2520Wendi%2520He%2520and%2520Yanni%2520Hu%2520and%2520Bin%2520Lin%2520and%2520Yiting%2520Lin%2520and%2520Yu%2520Pan%2520and%2520Pengfei%2520Tan%2520and%2520Chengwei%2520Tian%2520and%2520Chen%2520Wang%2520and%2520Zhicheng%2520Wang%2520and%2520Ruoye%2520Xie%2520and%2520Jixun%2520Yao%2520and%2520Quanlei%2520Yan%2520and%2520Yuguang%2520Yang%2520and%2520Jianhao%2520Ye%2520and%2520Jingjing%2520Yin%2520and%2520Yanzhen%2520Yu%2520and%2520Huimin%2520Zhang%2520and%2520Xiang%2520Zhang%2520and%2520Guangcheng%2520Zhao%2520and%2520Hongbin%2520Zhou%2520and%2520Pengpeng%2520Zou%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520the%2520big%2520data%2520and%2520large%2520language%2520model%2520era%252C%2520zero-shot%250Apersonalized%2520rapid%2520customization%2520has%2520emerged%2520as%2520a%2520significant%2520trend.%2520In%2520this%250Areport%252C%2520we%2520introduce%2520Takin%2520AudioLLM%252C%2520a%2520series%2520of%2520techniques%2520and%2520models%252C%2520mainly%250Aincluding%2520Takin%2520TTS%252C%2520Takin%2520VC%252C%2520and%2520Takin%2520Morphing%252C%2520specifically%2520designed%2520for%250Aaudiobook%2520production.%2520These%2520models%2520are%2520capable%2520of%2520zero-shot%2520speech%2520production%252C%250Agenerating%2520high-quality%2520speech%2520that%2520is%2520nearly%2520indistinguishable%2520from%2520real%2520human%250Aspeech%2520and%2520facilitating%2520individuals%2520to%2520customize%2520the%2520speech%2520content%2520according%250Ato%2520their%2520own%2520needs.%2520Specifically%252C%2520we%2520first%2520introduce%2520Takin%2520TTS%252C%2520a%2520neural%2520codec%250Alanguage%2520model%2520that%2520builds%2520upon%2520an%2520enhanced%2520neural%2520speech%2520codec%2520and%2520a%250Amulti-task%2520training%2520framework%252C%2520capable%2520of%2520generating%2520high-fidelity%2520natural%250Aspeech%2520in%2520a%2520zero-shot%2520way.%2520For%2520Takin%2520VC%252C%2520we%2520advocate%2520an%2520effective%2520content%2520and%250Atimbre%2520joint%2520modeling%2520approach%2520to%2520improve%2520the%2520speaker%2520similarity%252C%2520while%250Aadvocating%2520for%2520a%2520conditional%2520flow%2520matching%2520based%2520decoder%2520to%2520further%2520enhance%2520its%250Anaturalness%2520and%2520expressiveness.%2520Last%252C%2520we%2520propose%2520the%2520Takin%2520Morphing%2520system%2520with%250Ahighly%2520decoupled%2520and%2520advanced%2520timbre%2520and%2520prosody%2520modeling%2520approaches%252C%2520which%250Aenables%2520individuals%2520to%2520customize%2520speech%2520production%2520with%2520their%2520preferred%2520timbre%250Aand%2520prosody%2520in%2520a%2520precise%2520and%2520controllable%2520manner.%2520Extensive%2520experiments%250Avalidate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520our%2520Takin%2520AudioLLM%2520series%2520models.%250AFor%2520detailed%2520demos%252C%2520please%2520refer%2520to%250Ahttps%253A//everest-ai.github.io/takinaudiollm/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12139v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Takin%3A%20A%20Cohort%20of%20Superior%20Quality%20Zero-shot%20Speech%20Generation%20Models&entry.906535625=%20EverestAI%20and%20%20%3A%20and%20Sijing%20Chen%20and%20Yuan%20Feng%20and%20Laipeng%20He%20and%20Tianwei%20He%20and%20Wendi%20He%20and%20Yanni%20Hu%20and%20Bin%20Lin%20and%20Yiting%20Lin%20and%20Yu%20Pan%20and%20Pengfei%20Tan%20and%20Chengwei%20Tian%20and%20Chen%20Wang%20and%20Zhicheng%20Wang%20and%20Ruoye%20Xie%20and%20Jixun%20Yao%20and%20Quanlei%20Yan%20and%20Yuguang%20Yang%20and%20Jianhao%20Ye%20and%20Jingjing%20Yin%20and%20Yanzhen%20Yu%20and%20Huimin%20Zhang%20and%20Xiang%20Zhang%20and%20Guangcheng%20Zhao%20and%20Hongbin%20Zhou%20and%20Pengpeng%20Zou&entry.1292438233=%20%20With%20the%20advent%20of%20the%20big%20data%20and%20large%20language%20model%20era%2C%20zero-shot%0Apersonalized%20rapid%20customization%20has%20emerged%20as%20a%20significant%20trend.%20In%20this%0Areport%2C%20we%20introduce%20Takin%20AudioLLM%2C%20a%20series%20of%20techniques%20and%20models%2C%20mainly%0Aincluding%20Takin%20TTS%2C%20Takin%20VC%2C%20and%20Takin%20Morphing%2C%20specifically%20designed%20for%0Aaudiobook%20production.%20These%20models%20are%20capable%20of%20zero-shot%20speech%20production%2C%0Agenerating%20high-quality%20speech%20that%20is%20nearly%20indistinguishable%20from%20real%20human%0Aspeech%20and%20facilitating%20individuals%20to%20customize%20the%20speech%20content%20according%0Ato%20their%20own%20needs.%20Specifically%2C%20we%20first%20introduce%20Takin%20TTS%2C%20a%20neural%20codec%0Alanguage%20model%20that%20builds%20upon%20an%20enhanced%20neural%20speech%20codec%20and%20a%0Amulti-task%20training%20framework%2C%20capable%20of%20generating%20high-fidelity%20natural%0Aspeech%20in%20a%20zero-shot%20way.%20For%20Takin%20VC%2C%20we%20advocate%20an%20effective%20content%20and%0Atimbre%20joint%20modeling%20approach%20to%20improve%20the%20speaker%20similarity%2C%20while%0Aadvocating%20for%20a%20conditional%20flow%20matching%20based%20decoder%20to%20further%20enhance%20its%0Anaturalness%20and%20expressiveness.%20Last%2C%20we%20propose%20the%20Takin%20Morphing%20system%20with%0Ahighly%20decoupled%20and%20advanced%20timbre%20and%20prosody%20modeling%20approaches%2C%20which%0Aenables%20individuals%20to%20customize%20speech%20production%20with%20their%20preferred%20timbre%0Aand%20prosody%20in%20a%20precise%20and%20controllable%20manner.%20Extensive%20experiments%0Avalidate%20the%20effectiveness%20and%20robustness%20of%20our%20Takin%20AudioLLM%20series%20models.%0AFor%20detailed%20demos%2C%20please%20refer%20to%0Ahttps%3A//everest-ai.github.io/takinaudiollm/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12139v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


