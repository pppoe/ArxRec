<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240714.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Instant 3D Human Avatar Generation using Image Diffusion Models", "author": "Nikos Kolotouros and Thiemo Alldieck and Enric Corona and Eduard Gabriel Bazavan and Cristian Sminchisescu", "abstract": "  We present AvatarPopUp, a method for fast, high quality 3D human avatar\ngeneration from different input modalities, such as images and text prompts and\nwith control over the generated pose and shape. The common theme is the use of\ndiffusion-based image generation networks that are specialized for each\nparticular task, followed by a 3D lifting network. We purposefully decouple the\ngeneration from the 3D modeling which allow us to leverage powerful image\nsynthesis priors, trained on billions of text-image pairs. We fine-tune latent\ndiffusion networks with additional image conditioning for image generation and\nback-view prediction, and to support qualitatively different multiple 3D\nhypotheses. Our partial fine-tuning approach allows to adapt the networks for\neach task without inducing catastrophic forgetting. In our experiments, we\ndemonstrate that our method produces accurate, high-quality 3D avatars with\ndiverse appearance that respect the multimodal text, image, and body control\nsignals. Our approach can produce a 3D model in as few as 2 seconds, a four\norders of magnitude speedup wrt the vast majority of existing methods, most of\nwhich solve only a subset of our tasks, and with fewer controls. AvatarPopUp\nenables applications that require the controlled 3D generation of human avatars\nat scale. The project website can be found at\nhttps://www.nikoskolot.com/avatarpopup/.\n", "link": "http://arxiv.org/abs/2406.07516v2", "date": "2024-07-12", "relevancy": 3.1912, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.639}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.639}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instant%203D%20Human%20Avatar%20Generation%20using%20Image%20Diffusion%20Models&body=Title%3A%20Instant%203D%20Human%20Avatar%20Generation%20using%20Image%20Diffusion%20Models%0AAuthor%3A%20Nikos%20Kolotouros%20and%20Thiemo%20Alldieck%20and%20Enric%20Corona%20and%20Eduard%20Gabriel%20Bazavan%20and%20Cristian%20Sminchisescu%0AAbstract%3A%20%20%20We%20present%20AvatarPopUp%2C%20a%20method%20for%20fast%2C%20high%20quality%203D%20human%20avatar%0Ageneration%20from%20different%20input%20modalities%2C%20such%20as%20images%20and%20text%20prompts%20and%0Awith%20control%20over%20the%20generated%20pose%20and%20shape.%20The%20common%20theme%20is%20the%20use%20of%0Adiffusion-based%20image%20generation%20networks%20that%20are%20specialized%20for%20each%0Aparticular%20task%2C%20followed%20by%20a%203D%20lifting%20network.%20We%20purposefully%20decouple%20the%0Ageneration%20from%20the%203D%20modeling%20which%20allow%20us%20to%20leverage%20powerful%20image%0Asynthesis%20priors%2C%20trained%20on%20billions%20of%20text-image%20pairs.%20We%20fine-tune%20latent%0Adiffusion%20networks%20with%20additional%20image%20conditioning%20for%20image%20generation%20and%0Aback-view%20prediction%2C%20and%20to%20support%20qualitatively%20different%20multiple%203D%0Ahypotheses.%20Our%20partial%20fine-tuning%20approach%20allows%20to%20adapt%20the%20networks%20for%0Aeach%20task%20without%20inducing%20catastrophic%20forgetting.%20In%20our%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20produces%20accurate%2C%20high-quality%203D%20avatars%20with%0Adiverse%20appearance%20that%20respect%20the%20multimodal%20text%2C%20image%2C%20and%20body%20control%0Asignals.%20Our%20approach%20can%20produce%20a%203D%20model%20in%20as%20few%20as%202%20seconds%2C%20a%20four%0Aorders%20of%20magnitude%20speedup%20wrt%20the%20vast%20majority%20of%20existing%20methods%2C%20most%20of%0Awhich%20solve%20only%20a%20subset%20of%20our%20tasks%2C%20and%20with%20fewer%20controls.%20AvatarPopUp%0Aenables%20applications%20that%20require%20the%20controlled%203D%20generation%20of%20human%20avatars%0Aat%20scale.%20The%20project%20website%20can%20be%20found%20at%0Ahttps%3A//www.nikoskolot.com/avatarpopup/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstant%25203D%2520Human%2520Avatar%2520Generation%2520using%2520Image%2520Diffusion%2520Models%26entry.906535625%3DNikos%2520Kolotouros%2520and%2520Thiemo%2520Alldieck%2520and%2520Enric%2520Corona%2520and%2520Eduard%2520Gabriel%2520Bazavan%2520and%2520Cristian%2520Sminchisescu%26entry.1292438233%3D%2520%2520We%2520present%2520AvatarPopUp%252C%2520a%2520method%2520for%2520fast%252C%2520high%2520quality%25203D%2520human%2520avatar%250Ageneration%2520from%2520different%2520input%2520modalities%252C%2520such%2520as%2520images%2520and%2520text%2520prompts%2520and%250Awith%2520control%2520over%2520the%2520generated%2520pose%2520and%2520shape.%2520The%2520common%2520theme%2520is%2520the%2520use%2520of%250Adiffusion-based%2520image%2520generation%2520networks%2520that%2520are%2520specialized%2520for%2520each%250Aparticular%2520task%252C%2520followed%2520by%2520a%25203D%2520lifting%2520network.%2520We%2520purposefully%2520decouple%2520the%250Ageneration%2520from%2520the%25203D%2520modeling%2520which%2520allow%2520us%2520to%2520leverage%2520powerful%2520image%250Asynthesis%2520priors%252C%2520trained%2520on%2520billions%2520of%2520text-image%2520pairs.%2520We%2520fine-tune%2520latent%250Adiffusion%2520networks%2520with%2520additional%2520image%2520conditioning%2520for%2520image%2520generation%2520and%250Aback-view%2520prediction%252C%2520and%2520to%2520support%2520qualitatively%2520different%2520multiple%25203D%250Ahypotheses.%2520Our%2520partial%2520fine-tuning%2520approach%2520allows%2520to%2520adapt%2520the%2520networks%2520for%250Aeach%2520task%2520without%2520inducing%2520catastrophic%2520forgetting.%2520In%2520our%2520experiments%252C%2520we%250Ademonstrate%2520that%2520our%2520method%2520produces%2520accurate%252C%2520high-quality%25203D%2520avatars%2520with%250Adiverse%2520appearance%2520that%2520respect%2520the%2520multimodal%2520text%252C%2520image%252C%2520and%2520body%2520control%250Asignals.%2520Our%2520approach%2520can%2520produce%2520a%25203D%2520model%2520in%2520as%2520few%2520as%25202%2520seconds%252C%2520a%2520four%250Aorders%2520of%2520magnitude%2520speedup%2520wrt%2520the%2520vast%2520majority%2520of%2520existing%2520methods%252C%2520most%2520of%250Awhich%2520solve%2520only%2520a%2520subset%2520of%2520our%2520tasks%252C%2520and%2520with%2520fewer%2520controls.%2520AvatarPopUp%250Aenables%2520applications%2520that%2520require%2520the%2520controlled%25203D%2520generation%2520of%2520human%2520avatars%250Aat%2520scale.%2520The%2520project%2520website%2520can%2520be%2520found%2520at%250Ahttps%253A//www.nikoskolot.com/avatarpopup/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instant%203D%20Human%20Avatar%20Generation%20using%20Image%20Diffusion%20Models&entry.906535625=Nikos%20Kolotouros%20and%20Thiemo%20Alldieck%20and%20Enric%20Corona%20and%20Eduard%20Gabriel%20Bazavan%20and%20Cristian%20Sminchisescu&entry.1292438233=%20%20We%20present%20AvatarPopUp%2C%20a%20method%20for%20fast%2C%20high%20quality%203D%20human%20avatar%0Ageneration%20from%20different%20input%20modalities%2C%20such%20as%20images%20and%20text%20prompts%20and%0Awith%20control%20over%20the%20generated%20pose%20and%20shape.%20The%20common%20theme%20is%20the%20use%20of%0Adiffusion-based%20image%20generation%20networks%20that%20are%20specialized%20for%20each%0Aparticular%20task%2C%20followed%20by%20a%203D%20lifting%20network.%20We%20purposefully%20decouple%20the%0Ageneration%20from%20the%203D%20modeling%20which%20allow%20us%20to%20leverage%20powerful%20image%0Asynthesis%20priors%2C%20trained%20on%20billions%20of%20text-image%20pairs.%20We%20fine-tune%20latent%0Adiffusion%20networks%20with%20additional%20image%20conditioning%20for%20image%20generation%20and%0Aback-view%20prediction%2C%20and%20to%20support%20qualitatively%20different%20multiple%203D%0Ahypotheses.%20Our%20partial%20fine-tuning%20approach%20allows%20to%20adapt%20the%20networks%20for%0Aeach%20task%20without%20inducing%20catastrophic%20forgetting.%20In%20our%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20produces%20accurate%2C%20high-quality%203D%20avatars%20with%0Adiverse%20appearance%20that%20respect%20the%20multimodal%20text%2C%20image%2C%20and%20body%20control%0Asignals.%20Our%20approach%20can%20produce%20a%203D%20model%20in%20as%20few%20as%202%20seconds%2C%20a%20four%0Aorders%20of%20magnitude%20speedup%20wrt%20the%20vast%20majority%20of%20existing%20methods%2C%20most%20of%0Awhich%20solve%20only%20a%20subset%20of%20our%20tasks%2C%20and%20with%20fewer%20controls.%20AvatarPopUp%0Aenables%20applications%20that%20require%20the%20controlled%203D%20generation%20of%20human%20avatars%0Aat%20scale.%20The%20project%20website%20can%20be%20found%20at%0Ahttps%3A//www.nikoskolot.com/avatarpopup/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07516v2&entry.124074799=Read"},
{"title": "D2S: Representing sparse descriptors and 3D coordinates for camera\n  relocalization", "author": "Bach-Thuan Bui and Huy-Hoang Bui and Dinh-Tuan Tran and Joo-Ho Lee", "abstract": "  State-of-the-art visual localization methods mostly rely on complex\nprocedures to match local descriptors and 3D point clouds. However, these\nprocedures can incur significant costs in terms of inference, storage, and\nupdates over time. In this study, we propose a direct learning-based approach\nthat utilizes a simple network named D2S to represent complex local descriptors\nand their scene coordinates. Our method is characterized by its simplicity and\ncost-effectiveness. It solely leverages a single RGB image for localization\nduring the testing phase and only requires a lightweight model to encode a\ncomplex sparse scene. The proposed D2S employs a combination of a simple loss\nfunction and graph attention to selectively focus on robust descriptors while\ndisregarding areas such as clouds, trees, and several dynamic objects. This\nselective attention enables D2S to effectively perform a binary-semantic\nclassification for sparse descriptors. Additionally, we propose a simple\noutdoor dataset to evaluate the capabilities of visual localization methods in\nscene-specific generalization and self-updating from unlabeled observations.\nOur approach outperforms the state-of-the-art CNN-based methods in scene\ncoordinate regression in indoor and outdoor environments. It demonstrates the\nability to generalize beyond training data, including scenarios involving\ntransitions from day to night and adapting to domain shifts, even in the\nabsence of the labeled data sources. The source code, trained models, dataset,\nand demo videos are available at the following link:\nhttps://thpjp.github.io/d2s.\n", "link": "http://arxiv.org/abs/2307.15250v3", "date": "2024-07-12", "relevancy": 3.1166, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6911}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5929}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D2S%3A%20Representing%20sparse%20descriptors%20and%203D%20coordinates%20for%20camera%0A%20%20relocalization&body=Title%3A%20D2S%3A%20Representing%20sparse%20descriptors%20and%203D%20coordinates%20for%20camera%0A%20%20relocalization%0AAuthor%3A%20Bach-Thuan%20Bui%20and%20Huy-Hoang%20Bui%20and%20Dinh-Tuan%20Tran%20and%20Joo-Ho%20Lee%0AAbstract%3A%20%20%20State-of-the-art%20visual%20localization%20methods%20mostly%20rely%20on%20complex%0Aprocedures%20to%20match%20local%20descriptors%20and%203D%20point%20clouds.%20However%2C%20these%0Aprocedures%20can%20incur%20significant%20costs%20in%20terms%20of%20inference%2C%20storage%2C%20and%0Aupdates%20over%20time.%20In%20this%20study%2C%20we%20propose%20a%20direct%20learning-based%20approach%0Athat%20utilizes%20a%20simple%20network%20named%20D2S%20to%20represent%20complex%20local%20descriptors%0Aand%20their%20scene%20coordinates.%20Our%20method%20is%20characterized%20by%20its%20simplicity%20and%0Acost-effectiveness.%20It%20solely%20leverages%20a%20single%20RGB%20image%20for%20localization%0Aduring%20the%20testing%20phase%20and%20only%20requires%20a%20lightweight%20model%20to%20encode%20a%0Acomplex%20sparse%20scene.%20The%20proposed%20D2S%20employs%20a%20combination%20of%20a%20simple%20loss%0Afunction%20and%20graph%20attention%20to%20selectively%20focus%20on%20robust%20descriptors%20while%0Adisregarding%20areas%20such%20as%20clouds%2C%20trees%2C%20and%20several%20dynamic%20objects.%20This%0Aselective%20attention%20enables%20D2S%20to%20effectively%20perform%20a%20binary-semantic%0Aclassification%20for%20sparse%20descriptors.%20Additionally%2C%20we%20propose%20a%20simple%0Aoutdoor%20dataset%20to%20evaluate%20the%20capabilities%20of%20visual%20localization%20methods%20in%0Ascene-specific%20generalization%20and%20self-updating%20from%20unlabeled%20observations.%0AOur%20approach%20outperforms%20the%20state-of-the-art%20CNN-based%20methods%20in%20scene%0Acoordinate%20regression%20in%20indoor%20and%20outdoor%20environments.%20It%20demonstrates%20the%0Aability%20to%20generalize%20beyond%20training%20data%2C%20including%20scenarios%20involving%0Atransitions%20from%20day%20to%20night%20and%20adapting%20to%20domain%20shifts%2C%20even%20in%20the%0Aabsence%20of%20the%20labeled%20data%20sources.%20The%20source%20code%2C%20trained%20models%2C%20dataset%2C%0Aand%20demo%20videos%20are%20available%20at%20the%20following%20link%3A%0Ahttps%3A//thpjp.github.io/d2s.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.15250v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD2S%253A%2520Representing%2520sparse%2520descriptors%2520and%25203D%2520coordinates%2520for%2520camera%250A%2520%2520relocalization%26entry.906535625%3DBach-Thuan%2520Bui%2520and%2520Huy-Hoang%2520Bui%2520and%2520Dinh-Tuan%2520Tran%2520and%2520Joo-Ho%2520Lee%26entry.1292438233%3D%2520%2520State-of-the-art%2520visual%2520localization%2520methods%2520mostly%2520rely%2520on%2520complex%250Aprocedures%2520to%2520match%2520local%2520descriptors%2520and%25203D%2520point%2520clouds.%2520However%252C%2520these%250Aprocedures%2520can%2520incur%2520significant%2520costs%2520in%2520terms%2520of%2520inference%252C%2520storage%252C%2520and%250Aupdates%2520over%2520time.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520direct%2520learning-based%2520approach%250Athat%2520utilizes%2520a%2520simple%2520network%2520named%2520D2S%2520to%2520represent%2520complex%2520local%2520descriptors%250Aand%2520their%2520scene%2520coordinates.%2520Our%2520method%2520is%2520characterized%2520by%2520its%2520simplicity%2520and%250Acost-effectiveness.%2520It%2520solely%2520leverages%2520a%2520single%2520RGB%2520image%2520for%2520localization%250Aduring%2520the%2520testing%2520phase%2520and%2520only%2520requires%2520a%2520lightweight%2520model%2520to%2520encode%2520a%250Acomplex%2520sparse%2520scene.%2520The%2520proposed%2520D2S%2520employs%2520a%2520combination%2520of%2520a%2520simple%2520loss%250Afunction%2520and%2520graph%2520attention%2520to%2520selectively%2520focus%2520on%2520robust%2520descriptors%2520while%250Adisregarding%2520areas%2520such%2520as%2520clouds%252C%2520trees%252C%2520and%2520several%2520dynamic%2520objects.%2520This%250Aselective%2520attention%2520enables%2520D2S%2520to%2520effectively%2520perform%2520a%2520binary-semantic%250Aclassification%2520for%2520sparse%2520descriptors.%2520Additionally%252C%2520we%2520propose%2520a%2520simple%250Aoutdoor%2520dataset%2520to%2520evaluate%2520the%2520capabilities%2520of%2520visual%2520localization%2520methods%2520in%250Ascene-specific%2520generalization%2520and%2520self-updating%2520from%2520unlabeled%2520observations.%250AOur%2520approach%2520outperforms%2520the%2520state-of-the-art%2520CNN-based%2520methods%2520in%2520scene%250Acoordinate%2520regression%2520in%2520indoor%2520and%2520outdoor%2520environments.%2520It%2520demonstrates%2520the%250Aability%2520to%2520generalize%2520beyond%2520training%2520data%252C%2520including%2520scenarios%2520involving%250Atransitions%2520from%2520day%2520to%2520night%2520and%2520adapting%2520to%2520domain%2520shifts%252C%2520even%2520in%2520the%250Aabsence%2520of%2520the%2520labeled%2520data%2520sources.%2520The%2520source%2520code%252C%2520trained%2520models%252C%2520dataset%252C%250Aand%2520demo%2520videos%2520are%2520available%2520at%2520the%2520following%2520link%253A%250Ahttps%253A//thpjp.github.io/d2s.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.15250v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D2S%3A%20Representing%20sparse%20descriptors%20and%203D%20coordinates%20for%20camera%0A%20%20relocalization&entry.906535625=Bach-Thuan%20Bui%20and%20Huy-Hoang%20Bui%20and%20Dinh-Tuan%20Tran%20and%20Joo-Ho%20Lee&entry.1292438233=%20%20State-of-the-art%20visual%20localization%20methods%20mostly%20rely%20on%20complex%0Aprocedures%20to%20match%20local%20descriptors%20and%203D%20point%20clouds.%20However%2C%20these%0Aprocedures%20can%20incur%20significant%20costs%20in%20terms%20of%20inference%2C%20storage%2C%20and%0Aupdates%20over%20time.%20In%20this%20study%2C%20we%20propose%20a%20direct%20learning-based%20approach%0Athat%20utilizes%20a%20simple%20network%20named%20D2S%20to%20represent%20complex%20local%20descriptors%0Aand%20their%20scene%20coordinates.%20Our%20method%20is%20characterized%20by%20its%20simplicity%20and%0Acost-effectiveness.%20It%20solely%20leverages%20a%20single%20RGB%20image%20for%20localization%0Aduring%20the%20testing%20phase%20and%20only%20requires%20a%20lightweight%20model%20to%20encode%20a%0Acomplex%20sparse%20scene.%20The%20proposed%20D2S%20employs%20a%20combination%20of%20a%20simple%20loss%0Afunction%20and%20graph%20attention%20to%20selectively%20focus%20on%20robust%20descriptors%20while%0Adisregarding%20areas%20such%20as%20clouds%2C%20trees%2C%20and%20several%20dynamic%20objects.%20This%0Aselective%20attention%20enables%20D2S%20to%20effectively%20perform%20a%20binary-semantic%0Aclassification%20for%20sparse%20descriptors.%20Additionally%2C%20we%20propose%20a%20simple%0Aoutdoor%20dataset%20to%20evaluate%20the%20capabilities%20of%20visual%20localization%20methods%20in%0Ascene-specific%20generalization%20and%20self-updating%20from%20unlabeled%20observations.%0AOur%20approach%20outperforms%20the%20state-of-the-art%20CNN-based%20methods%20in%20scene%0Acoordinate%20regression%20in%20indoor%20and%20outdoor%20environments.%20It%20demonstrates%20the%0Aability%20to%20generalize%20beyond%20training%20data%2C%20including%20scenarios%20involving%0Atransitions%20from%20day%20to%20night%20and%20adapting%20to%20domain%20shifts%2C%20even%20in%20the%0Aabsence%20of%20the%20labeled%20data%20sources.%20The%20source%20code%2C%20trained%20models%2C%20dataset%2C%0Aand%20demo%20videos%20are%20available%20at%20the%20following%20link%3A%0Ahttps%3A//thpjp.github.io/d2s.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.15250v3&entry.124074799=Read"},
{"title": "Segment Any 4D Gaussians", "author": "Shengxiang Ji and Guanjun Wu and Jiemin Fang and Jiazhong Cen and Taoran Yi and Wenyu Liu and Qi Tian and Xinggang Wang", "abstract": "  Modeling, understanding, and reconstructing the real world are crucial in\nXR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable\nsuccess in modeling and understanding 3D scenes. Similarly, various 4D\nrepresentations have demonstrated the ability to capture the dynamics of the 4D\nworld. However, there is a dearth of research focusing on segmentation within\n4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D),\none of the first frameworks to segment anything in the 4D digital world based\non 4D Gaussians. In SA4D, an efficient temporal identity feature field is\nintroduced to handle Gaussian drifting, with the potential to learn precise\nidentity features from noisy and sparse input. Additionally, a 4D segmentation\nrefinement process is proposed to remove artifacts. Our SA4D achieves precise,\nhigh-quality segmentation within seconds in 4D Gaussians and shows the ability\nto remove, recolor, compose, and render high-quality anything masks. More demos\nare available at: https://jsxzs.github.io/sa4d/.\n", "link": "http://arxiv.org/abs/2407.04504v2", "date": "2024-07-12", "relevancy": 3.1058, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.665}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6215}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Any%204D%20Gaussians&body=Title%3A%20Segment%20Any%204D%20Gaussians%0AAuthor%3A%20Shengxiang%20Ji%20and%20Guanjun%20Wu%20and%20Jiemin%20Fang%20and%20Jiazhong%20Cen%20and%20Taoran%20Yi%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Modeling%2C%20understanding%2C%20and%20reconstructing%20the%20real%20world%20are%20crucial%20in%0AXR/VR.%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20methods%20have%20shown%20remarkable%0Asuccess%20in%20modeling%20and%20understanding%203D%20scenes.%20Similarly%2C%20various%204D%0Arepresentations%20have%20demonstrated%20the%20ability%20to%20capture%20the%20dynamics%20of%20the%204D%0Aworld.%20However%2C%20there%20is%20a%20dearth%20of%20research%20focusing%20on%20segmentation%20within%0A4D%20representations.%20In%20this%20paper%2C%20we%20propose%20Segment%20Any%204D%20Gaussians%20%28SA4D%29%2C%0Aone%20of%20the%20first%20frameworks%20to%20segment%20anything%20in%20the%204D%20digital%20world%20based%0Aon%204D%20Gaussians.%20In%20SA4D%2C%20an%20efficient%20temporal%20identity%20feature%20field%20is%0Aintroduced%20to%20handle%20Gaussian%20drifting%2C%20with%20the%20potential%20to%20learn%20precise%0Aidentity%20features%20from%20noisy%20and%20sparse%20input.%20Additionally%2C%20a%204D%20segmentation%0Arefinement%20process%20is%20proposed%20to%20remove%20artifacts.%20Our%20SA4D%20achieves%20precise%2C%0Ahigh-quality%20segmentation%20within%20seconds%20in%204D%20Gaussians%20and%20shows%20the%20ability%0Ato%20remove%2C%20recolor%2C%20compose%2C%20and%20render%20high-quality%20anything%20masks.%20More%20demos%0Aare%20available%20at%3A%20https%3A//jsxzs.github.io/sa4d/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04504v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Any%25204D%2520Gaussians%26entry.906535625%3DShengxiang%2520Ji%2520and%2520Guanjun%2520Wu%2520and%2520Jiemin%2520Fang%2520and%2520Jiazhong%2520Cen%2520and%2520Taoran%2520Yi%2520and%2520Wenyu%2520Liu%2520and%2520Qi%2520Tian%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Modeling%252C%2520understanding%252C%2520and%2520reconstructing%2520the%2520real%2520world%2520are%2520crucial%2520in%250AXR/VR.%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520methods%2520have%2520shown%2520remarkable%250Asuccess%2520in%2520modeling%2520and%2520understanding%25203D%2520scenes.%2520Similarly%252C%2520various%25204D%250Arepresentations%2520have%2520demonstrated%2520the%2520ability%2520to%2520capture%2520the%2520dynamics%2520of%2520the%25204D%250Aworld.%2520However%252C%2520there%2520is%2520a%2520dearth%2520of%2520research%2520focusing%2520on%2520segmentation%2520within%250A4D%2520representations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Segment%2520Any%25204D%2520Gaussians%2520%2528SA4D%2529%252C%250Aone%2520of%2520the%2520first%2520frameworks%2520to%2520segment%2520anything%2520in%2520the%25204D%2520digital%2520world%2520based%250Aon%25204D%2520Gaussians.%2520In%2520SA4D%252C%2520an%2520efficient%2520temporal%2520identity%2520feature%2520field%2520is%250Aintroduced%2520to%2520handle%2520Gaussian%2520drifting%252C%2520with%2520the%2520potential%2520to%2520learn%2520precise%250Aidentity%2520features%2520from%2520noisy%2520and%2520sparse%2520input.%2520Additionally%252C%2520a%25204D%2520segmentation%250Arefinement%2520process%2520is%2520proposed%2520to%2520remove%2520artifacts.%2520Our%2520SA4D%2520achieves%2520precise%252C%250Ahigh-quality%2520segmentation%2520within%2520seconds%2520in%25204D%2520Gaussians%2520and%2520shows%2520the%2520ability%250Ato%2520remove%252C%2520recolor%252C%2520compose%252C%2520and%2520render%2520high-quality%2520anything%2520masks.%2520More%2520demos%250Aare%2520available%2520at%253A%2520https%253A//jsxzs.github.io/sa4d/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04504v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Any%204D%20Gaussians&entry.906535625=Shengxiang%20Ji%20and%20Guanjun%20Wu%20and%20Jiemin%20Fang%20and%20Jiazhong%20Cen%20and%20Taoran%20Yi%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang&entry.1292438233=%20%20Modeling%2C%20understanding%2C%20and%20reconstructing%20the%20real%20world%20are%20crucial%20in%0AXR/VR.%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20methods%20have%20shown%20remarkable%0Asuccess%20in%20modeling%20and%20understanding%203D%20scenes.%20Similarly%2C%20various%204D%0Arepresentations%20have%20demonstrated%20the%20ability%20to%20capture%20the%20dynamics%20of%20the%204D%0Aworld.%20However%2C%20there%20is%20a%20dearth%20of%20research%20focusing%20on%20segmentation%20within%0A4D%20representations.%20In%20this%20paper%2C%20we%20propose%20Segment%20Any%204D%20Gaussians%20%28SA4D%29%2C%0Aone%20of%20the%20first%20frameworks%20to%20segment%20anything%20in%20the%204D%20digital%20world%20based%0Aon%204D%20Gaussians.%20In%20SA4D%2C%20an%20efficient%20temporal%20identity%20feature%20field%20is%0Aintroduced%20to%20handle%20Gaussian%20drifting%2C%20with%20the%20potential%20to%20learn%20precise%0Aidentity%20features%20from%20noisy%20and%20sparse%20input.%20Additionally%2C%20a%204D%20segmentation%0Arefinement%20process%20is%20proposed%20to%20remove%20artifacts.%20Our%20SA4D%20achieves%20precise%2C%0Ahigh-quality%20segmentation%20within%20seconds%20in%204D%20Gaussians%20and%20shows%20the%20ability%0Ato%20remove%2C%20recolor%2C%20compose%2C%20and%20render%20high-quality%20anything%20masks.%20More%20demos%0Aare%20available%20at%3A%20https%3A//jsxzs.github.io/sa4d/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04504v2&entry.124074799=Read"},
{"title": "GenerateCT: Text-Conditional Generation of 3D Chest CT Volumes", "author": "Ibrahim Ethem Hamamci and Sezgin Er and Anjany Sekuboyina and Enis Simsar and Alperen Tezcan and Ayse Gulnihan Simsek and Sevval Nil Esirgun and Furkan Almas and Irem Dogan and Muhammed Furkan Dasdelen and Chinmay Prabhakar and Hadrien Reynaud and Sarthak Pati and Christian Bluethgen and Mehmet Kemal Ozdemir and Bjoern Menze", "abstract": "  GenerateCT, the first approach to generating 3D medical imaging conditioned\non free-form medical text prompts, incorporates a text encoder and three key\ncomponents: a novel causal vision transformer for encoding 3D CT volumes, a\ntext-image transformer for aligning CT and text tokens, and a text-conditional\nsuper-resolution diffusion model. Without directly comparable methods in 3D\nmedical imaging, we benchmarked GenerateCT against cutting-edge methods,\ndemonstrating its superiority across all key metrics. Importantly, we evaluated\nGenerateCT's clinical applications in a multi-abnormality classification task.\nFirst, we established a baseline by training a multi-abnormality classifier on\nour real dataset. To further assess the model's generalization to external data\nand performance with unseen prompts in a zero-shot scenario, we employed an\nexternal set to train the classifier, setting an additional benchmark. We\nconducted two experiments in which we doubled the training datasets by\nsynthesizing an equal number of volumes for each set using GenerateCT. The\nfirst experiment demonstrated an 11% improvement in the AP score when training\nthe classifier jointly on real and generated volumes. The second experiment\nshowed a 7% improvement when training on both real and generated volumes based\non unseen prompts. Moreover, GenerateCT enables the scaling of synthetic\ntraining datasets to arbitrary sizes. As an example, we generated 100,000 3D\nCTs, fivefold the number in our real set, and trained the classifier\nexclusively on these synthetic CTs. Impressively, this classifier surpassed the\nperformance of the one trained on all available real data by a margin of 8%.\nLast, domain experts evaluated the generated volumes, confirming a high degree\nof alignment with the text prompt. Access our code, model weights, training\ndata, and generated data at https://github.com/ibrahimethemhamamci/GenerateCT\n", "link": "http://arxiv.org/abs/2305.16037v5", "date": "2024-07-12", "relevancy": 2.9423, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6053}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6053}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenerateCT%3A%20Text-Conditional%20Generation%20of%203D%20Chest%20CT%20Volumes&body=Title%3A%20GenerateCT%3A%20Text-Conditional%20Generation%20of%203D%20Chest%20CT%20Volumes%0AAuthor%3A%20Ibrahim%20Ethem%20Hamamci%20and%20Sezgin%20Er%20and%20Anjany%20Sekuboyina%20and%20Enis%20Simsar%20and%20Alperen%20Tezcan%20and%20Ayse%20Gulnihan%20Simsek%20and%20Sevval%20Nil%20Esirgun%20and%20Furkan%20Almas%20and%20Irem%20Dogan%20and%20Muhammed%20Furkan%20Dasdelen%20and%20Chinmay%20Prabhakar%20and%20Hadrien%20Reynaud%20and%20Sarthak%20Pati%20and%20Christian%20Bluethgen%20and%20Mehmet%20Kemal%20Ozdemir%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20GenerateCT%2C%20the%20first%20approach%20to%20generating%203D%20medical%20imaging%20conditioned%0Aon%20free-form%20medical%20text%20prompts%2C%20incorporates%20a%20text%20encoder%20and%20three%20key%0Acomponents%3A%20a%20novel%20causal%20vision%20transformer%20for%20encoding%203D%20CT%20volumes%2C%20a%0Atext-image%20transformer%20for%20aligning%20CT%20and%20text%20tokens%2C%20and%20a%20text-conditional%0Asuper-resolution%20diffusion%20model.%20Without%20directly%20comparable%20methods%20in%203D%0Amedical%20imaging%2C%20we%20benchmarked%20GenerateCT%20against%20cutting-edge%20methods%2C%0Ademonstrating%20its%20superiority%20across%20all%20key%20metrics.%20Importantly%2C%20we%20evaluated%0AGenerateCT%27s%20clinical%20applications%20in%20a%20multi-abnormality%20classification%20task.%0AFirst%2C%20we%20established%20a%20baseline%20by%20training%20a%20multi-abnormality%20classifier%20on%0Aour%20real%20dataset.%20To%20further%20assess%20the%20model%27s%20generalization%20to%20external%20data%0Aand%20performance%20with%20unseen%20prompts%20in%20a%20zero-shot%20scenario%2C%20we%20employed%20an%0Aexternal%20set%20to%20train%20the%20classifier%2C%20setting%20an%20additional%20benchmark.%20We%0Aconducted%20two%20experiments%20in%20which%20we%20doubled%20the%20training%20datasets%20by%0Asynthesizing%20an%20equal%20number%20of%20volumes%20for%20each%20set%20using%20GenerateCT.%20The%0Afirst%20experiment%20demonstrated%20an%2011%25%20improvement%20in%20the%20AP%20score%20when%20training%0Athe%20classifier%20jointly%20on%20real%20and%20generated%20volumes.%20The%20second%20experiment%0Ashowed%20a%207%25%20improvement%20when%20training%20on%20both%20real%20and%20generated%20volumes%20based%0Aon%20unseen%20prompts.%20Moreover%2C%20GenerateCT%20enables%20the%20scaling%20of%20synthetic%0Atraining%20datasets%20to%20arbitrary%20sizes.%20As%20an%20example%2C%20we%20generated%20100%2C000%203D%0ACTs%2C%20fivefold%20the%20number%20in%20our%20real%20set%2C%20and%20trained%20the%20classifier%0Aexclusively%20on%20these%20synthetic%20CTs.%20Impressively%2C%20this%20classifier%20surpassed%20the%0Aperformance%20of%20the%20one%20trained%20on%20all%20available%20real%20data%20by%20a%20margin%20of%208%25.%0ALast%2C%20domain%20experts%20evaluated%20the%20generated%20volumes%2C%20confirming%20a%20high%20degree%0Aof%20alignment%20with%20the%20text%20prompt.%20Access%20our%20code%2C%20model%20weights%2C%20training%0Adata%2C%20and%20generated%20data%20at%20https%3A//github.com/ibrahimethemhamamci/GenerateCT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16037v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerateCT%253A%2520Text-Conditional%2520Generation%2520of%25203D%2520Chest%2520CT%2520Volumes%26entry.906535625%3DIbrahim%2520Ethem%2520Hamamci%2520and%2520Sezgin%2520Er%2520and%2520Anjany%2520Sekuboyina%2520and%2520Enis%2520Simsar%2520and%2520Alperen%2520Tezcan%2520and%2520Ayse%2520Gulnihan%2520Simsek%2520and%2520Sevval%2520Nil%2520Esirgun%2520and%2520Furkan%2520Almas%2520and%2520Irem%2520Dogan%2520and%2520Muhammed%2520Furkan%2520Dasdelen%2520and%2520Chinmay%2520Prabhakar%2520and%2520Hadrien%2520Reynaud%2520and%2520Sarthak%2520Pati%2520and%2520Christian%2520Bluethgen%2520and%2520Mehmet%2520Kemal%2520Ozdemir%2520and%2520Bjoern%2520Menze%26entry.1292438233%3D%2520%2520GenerateCT%252C%2520the%2520first%2520approach%2520to%2520generating%25203D%2520medical%2520imaging%2520conditioned%250Aon%2520free-form%2520medical%2520text%2520prompts%252C%2520incorporates%2520a%2520text%2520encoder%2520and%2520three%2520key%250Acomponents%253A%2520a%2520novel%2520causal%2520vision%2520transformer%2520for%2520encoding%25203D%2520CT%2520volumes%252C%2520a%250Atext-image%2520transformer%2520for%2520aligning%2520CT%2520and%2520text%2520tokens%252C%2520and%2520a%2520text-conditional%250Asuper-resolution%2520diffusion%2520model.%2520Without%2520directly%2520comparable%2520methods%2520in%25203D%250Amedical%2520imaging%252C%2520we%2520benchmarked%2520GenerateCT%2520against%2520cutting-edge%2520methods%252C%250Ademonstrating%2520its%2520superiority%2520across%2520all%2520key%2520metrics.%2520Importantly%252C%2520we%2520evaluated%250AGenerateCT%2527s%2520clinical%2520applications%2520in%2520a%2520multi-abnormality%2520classification%2520task.%250AFirst%252C%2520we%2520established%2520a%2520baseline%2520by%2520training%2520a%2520multi-abnormality%2520classifier%2520on%250Aour%2520real%2520dataset.%2520To%2520further%2520assess%2520the%2520model%2527s%2520generalization%2520to%2520external%2520data%250Aand%2520performance%2520with%2520unseen%2520prompts%2520in%2520a%2520zero-shot%2520scenario%252C%2520we%2520employed%2520an%250Aexternal%2520set%2520to%2520train%2520the%2520classifier%252C%2520setting%2520an%2520additional%2520benchmark.%2520We%250Aconducted%2520two%2520experiments%2520in%2520which%2520we%2520doubled%2520the%2520training%2520datasets%2520by%250Asynthesizing%2520an%2520equal%2520number%2520of%2520volumes%2520for%2520each%2520set%2520using%2520GenerateCT.%2520The%250Afirst%2520experiment%2520demonstrated%2520an%252011%2525%2520improvement%2520in%2520the%2520AP%2520score%2520when%2520training%250Athe%2520classifier%2520jointly%2520on%2520real%2520and%2520generated%2520volumes.%2520The%2520second%2520experiment%250Ashowed%2520a%25207%2525%2520improvement%2520when%2520training%2520on%2520both%2520real%2520and%2520generated%2520volumes%2520based%250Aon%2520unseen%2520prompts.%2520Moreover%252C%2520GenerateCT%2520enables%2520the%2520scaling%2520of%2520synthetic%250Atraining%2520datasets%2520to%2520arbitrary%2520sizes.%2520As%2520an%2520example%252C%2520we%2520generated%2520100%252C000%25203D%250ACTs%252C%2520fivefold%2520the%2520number%2520in%2520our%2520real%2520set%252C%2520and%2520trained%2520the%2520classifier%250Aexclusively%2520on%2520these%2520synthetic%2520CTs.%2520Impressively%252C%2520this%2520classifier%2520surpassed%2520the%250Aperformance%2520of%2520the%2520one%2520trained%2520on%2520all%2520available%2520real%2520data%2520by%2520a%2520margin%2520of%25208%2525.%250ALast%252C%2520domain%2520experts%2520evaluated%2520the%2520generated%2520volumes%252C%2520confirming%2520a%2520high%2520degree%250Aof%2520alignment%2520with%2520the%2520text%2520prompt.%2520Access%2520our%2520code%252C%2520model%2520weights%252C%2520training%250Adata%252C%2520and%2520generated%2520data%2520at%2520https%253A//github.com/ibrahimethemhamamci/GenerateCT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.16037v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenerateCT%3A%20Text-Conditional%20Generation%20of%203D%20Chest%20CT%20Volumes&entry.906535625=Ibrahim%20Ethem%20Hamamci%20and%20Sezgin%20Er%20and%20Anjany%20Sekuboyina%20and%20Enis%20Simsar%20and%20Alperen%20Tezcan%20and%20Ayse%20Gulnihan%20Simsek%20and%20Sevval%20Nil%20Esirgun%20and%20Furkan%20Almas%20and%20Irem%20Dogan%20and%20Muhammed%20Furkan%20Dasdelen%20and%20Chinmay%20Prabhakar%20and%20Hadrien%20Reynaud%20and%20Sarthak%20Pati%20and%20Christian%20Bluethgen%20and%20Mehmet%20Kemal%20Ozdemir%20and%20Bjoern%20Menze&entry.1292438233=%20%20GenerateCT%2C%20the%20first%20approach%20to%20generating%203D%20medical%20imaging%20conditioned%0Aon%20free-form%20medical%20text%20prompts%2C%20incorporates%20a%20text%20encoder%20and%20three%20key%0Acomponents%3A%20a%20novel%20causal%20vision%20transformer%20for%20encoding%203D%20CT%20volumes%2C%20a%0Atext-image%20transformer%20for%20aligning%20CT%20and%20text%20tokens%2C%20and%20a%20text-conditional%0Asuper-resolution%20diffusion%20model.%20Without%20directly%20comparable%20methods%20in%203D%0Amedical%20imaging%2C%20we%20benchmarked%20GenerateCT%20against%20cutting-edge%20methods%2C%0Ademonstrating%20its%20superiority%20across%20all%20key%20metrics.%20Importantly%2C%20we%20evaluated%0AGenerateCT%27s%20clinical%20applications%20in%20a%20multi-abnormality%20classification%20task.%0AFirst%2C%20we%20established%20a%20baseline%20by%20training%20a%20multi-abnormality%20classifier%20on%0Aour%20real%20dataset.%20To%20further%20assess%20the%20model%27s%20generalization%20to%20external%20data%0Aand%20performance%20with%20unseen%20prompts%20in%20a%20zero-shot%20scenario%2C%20we%20employed%20an%0Aexternal%20set%20to%20train%20the%20classifier%2C%20setting%20an%20additional%20benchmark.%20We%0Aconducted%20two%20experiments%20in%20which%20we%20doubled%20the%20training%20datasets%20by%0Asynthesizing%20an%20equal%20number%20of%20volumes%20for%20each%20set%20using%20GenerateCT.%20The%0Afirst%20experiment%20demonstrated%20an%2011%25%20improvement%20in%20the%20AP%20score%20when%20training%0Athe%20classifier%20jointly%20on%20real%20and%20generated%20volumes.%20The%20second%20experiment%0Ashowed%20a%207%25%20improvement%20when%20training%20on%20both%20real%20and%20generated%20volumes%20based%0Aon%20unseen%20prompts.%20Moreover%2C%20GenerateCT%20enables%20the%20scaling%20of%20synthetic%0Atraining%20datasets%20to%20arbitrary%20sizes.%20As%20an%20example%2C%20we%20generated%20100%2C000%203D%0ACTs%2C%20fivefold%20the%20number%20in%20our%20real%20set%2C%20and%20trained%20the%20classifier%0Aexclusively%20on%20these%20synthetic%20CTs.%20Impressively%2C%20this%20classifier%20surpassed%20the%0Aperformance%20of%20the%20one%20trained%20on%20all%20available%20real%20data%20by%20a%20margin%20of%208%25.%0ALast%2C%20domain%20experts%20evaluated%20the%20generated%20volumes%2C%20confirming%20a%20high%20degree%0Aof%20alignment%20with%20the%20text%20prompt.%20Access%20our%20code%2C%20model%20weights%2C%20training%0Adata%2C%20and%20generated%20data%20at%20https%3A//github.com/ibrahimethemhamamci/GenerateCT%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16037v5&entry.124074799=Read"},
{"title": "StyleSplat: 3D Object Style Transfer with Gaussian Splatting", "author": "Sahil Jain and Avik Kuthiala and Prabhdeep Singh Sethi and Prakanshul Saxena", "abstract": "  Recent advancements in radiance fields have opened new avenues for creating\nhigh-quality 3D assets and scenes. Style transfer can enhance these 3D assets\nwith diverse artistic styles, transforming creative expression. However,\nexisting techniques are often slow or unable to localize style transfer to\nspecific objects. We introduce StyleSplat, a lightweight method for stylizing\n3D objects in scenes represented by 3D Gaussians from reference style images.\nOur approach first learns a photorealistic representation of the scene using 3D\nGaussian splatting while jointly segmenting individual 3D objects. We then use\na nearest-neighbor feature matching loss to finetune the Gaussians of the\nselected objects, aligning their spherical harmonic coefficients with the style\nimage to ensure consistency and visual appeal. StyleSplat allows for quick,\ncustomizable style transfer and localized stylization of multiple objects\nwithin a scene, each with a different style. We demonstrate its effectiveness\nacross various 3D scenes and styles, showcasing enhanced control and\ncustomization in 3D creation.\n", "link": "http://arxiv.org/abs/2407.09473v1", "date": "2024-07-12", "relevancy": 2.8625, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6205}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5485}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StyleSplat%3A%203D%20Object%20Style%20Transfer%20with%20Gaussian%20Splatting&body=Title%3A%20StyleSplat%3A%203D%20Object%20Style%20Transfer%20with%20Gaussian%20Splatting%0AAuthor%3A%20Sahil%20Jain%20and%20Avik%20Kuthiala%20and%20Prabhdeep%20Singh%20Sethi%20and%20Prakanshul%20Saxena%0AAbstract%3A%20%20%20Recent%20advancements%20in%20radiance%20fields%20have%20opened%20new%20avenues%20for%20creating%0Ahigh-quality%203D%20assets%20and%20scenes.%20Style%20transfer%20can%20enhance%20these%203D%20assets%0Awith%20diverse%20artistic%20styles%2C%20transforming%20creative%20expression.%20However%2C%0Aexisting%20techniques%20are%20often%20slow%20or%20unable%20to%20localize%20style%20transfer%20to%0Aspecific%20objects.%20We%20introduce%20StyleSplat%2C%20a%20lightweight%20method%20for%20stylizing%0A3D%20objects%20in%20scenes%20represented%20by%203D%20Gaussians%20from%20reference%20style%20images.%0AOur%20approach%20first%20learns%20a%20photorealistic%20representation%20of%20the%20scene%20using%203D%0AGaussian%20splatting%20while%20jointly%20segmenting%20individual%203D%20objects.%20We%20then%20use%0Aa%20nearest-neighbor%20feature%20matching%20loss%20to%20finetune%20the%20Gaussians%20of%20the%0Aselected%20objects%2C%20aligning%20their%20spherical%20harmonic%20coefficients%20with%20the%20style%0Aimage%20to%20ensure%20consistency%20and%20visual%20appeal.%20StyleSplat%20allows%20for%20quick%2C%0Acustomizable%20style%20transfer%20and%20localized%20stylization%20of%20multiple%20objects%0Awithin%20a%20scene%2C%20each%20with%20a%20different%20style.%20We%20demonstrate%20its%20effectiveness%0Aacross%20various%203D%20scenes%20and%20styles%2C%20showcasing%20enhanced%20control%20and%0Acustomization%20in%203D%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyleSplat%253A%25203D%2520Object%2520Style%2520Transfer%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DSahil%2520Jain%2520and%2520Avik%2520Kuthiala%2520and%2520Prabhdeep%2520Singh%2520Sethi%2520and%2520Prakanshul%2520Saxena%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520radiance%2520fields%2520have%2520opened%2520new%2520avenues%2520for%2520creating%250Ahigh-quality%25203D%2520assets%2520and%2520scenes.%2520Style%2520transfer%2520can%2520enhance%2520these%25203D%2520assets%250Awith%2520diverse%2520artistic%2520styles%252C%2520transforming%2520creative%2520expression.%2520However%252C%250Aexisting%2520techniques%2520are%2520often%2520slow%2520or%2520unable%2520to%2520localize%2520style%2520transfer%2520to%250Aspecific%2520objects.%2520We%2520introduce%2520StyleSplat%252C%2520a%2520lightweight%2520method%2520for%2520stylizing%250A3D%2520objects%2520in%2520scenes%2520represented%2520by%25203D%2520Gaussians%2520from%2520reference%2520style%2520images.%250AOur%2520approach%2520first%2520learns%2520a%2520photorealistic%2520representation%2520of%2520the%2520scene%2520using%25203D%250AGaussian%2520splatting%2520while%2520jointly%2520segmenting%2520individual%25203D%2520objects.%2520We%2520then%2520use%250Aa%2520nearest-neighbor%2520feature%2520matching%2520loss%2520to%2520finetune%2520the%2520Gaussians%2520of%2520the%250Aselected%2520objects%252C%2520aligning%2520their%2520spherical%2520harmonic%2520coefficients%2520with%2520the%2520style%250Aimage%2520to%2520ensure%2520consistency%2520and%2520visual%2520appeal.%2520StyleSplat%2520allows%2520for%2520quick%252C%250Acustomizable%2520style%2520transfer%2520and%2520localized%2520stylization%2520of%2520multiple%2520objects%250Awithin%2520a%2520scene%252C%2520each%2520with%2520a%2520different%2520style.%2520We%2520demonstrate%2520its%2520effectiveness%250Aacross%2520various%25203D%2520scenes%2520and%2520styles%252C%2520showcasing%2520enhanced%2520control%2520and%250Acustomization%2520in%25203D%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StyleSplat%3A%203D%20Object%20Style%20Transfer%20with%20Gaussian%20Splatting&entry.906535625=Sahil%20Jain%20and%20Avik%20Kuthiala%20and%20Prabhdeep%20Singh%20Sethi%20and%20Prakanshul%20Saxena&entry.1292438233=%20%20Recent%20advancements%20in%20radiance%20fields%20have%20opened%20new%20avenues%20for%20creating%0Ahigh-quality%203D%20assets%20and%20scenes.%20Style%20transfer%20can%20enhance%20these%203D%20assets%0Awith%20diverse%20artistic%20styles%2C%20transforming%20creative%20expression.%20However%2C%0Aexisting%20techniques%20are%20often%20slow%20or%20unable%20to%20localize%20style%20transfer%20to%0Aspecific%20objects.%20We%20introduce%20StyleSplat%2C%20a%20lightweight%20method%20for%20stylizing%0A3D%20objects%20in%20scenes%20represented%20by%203D%20Gaussians%20from%20reference%20style%20images.%0AOur%20approach%20first%20learns%20a%20photorealistic%20representation%20of%20the%20scene%20using%203D%0AGaussian%20splatting%20while%20jointly%20segmenting%20individual%203D%20objects.%20We%20then%20use%0Aa%20nearest-neighbor%20feature%20matching%20loss%20to%20finetune%20the%20Gaussians%20of%20the%0Aselected%20objects%2C%20aligning%20their%20spherical%20harmonic%20coefficients%20with%20the%20style%0Aimage%20to%20ensure%20consistency%20and%20visual%20appeal.%20StyleSplat%20allows%20for%20quick%2C%0Acustomizable%20style%20transfer%20and%20localized%20stylization%20of%20multiple%20objects%0Awithin%20a%20scene%2C%20each%20with%20a%20different%20style.%20We%20demonstrate%20its%20effectiveness%0Aacross%20various%203D%20scenes%20and%20styles%2C%20showcasing%20enhanced%20control%20and%0Acustomization%20in%203D%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09473v1&entry.124074799=Read"},
{"title": "Reshaping the Online Data Buffering and Organizing Mechanism for\n  Continual Test-Time Adaptation", "author": "Zhilin Zhu and Xiaopeng Hong and Zhiheng Ma and Weijun Zhuang and Yaohui Ma and Dai Yong and Yaowei Wang", "abstract": "  Continual Test-Time Adaptation (CTTA) involves adapting a pre-trained source\nmodel to continually changing unsupervised target domains. In this paper, we\nsystematically analyze the challenges of this task: online environment,\nunsupervised nature, and the risks of error accumulation and catastrophic\nforgetting under continual domain shifts. To address these challenges, we\nreshape the online data buffering and organizing mechanism for CTTA. We propose\nan {uncertainty-aware buffering approach} to identify {and aggregate}\nsignificant samples with high certainty from the unsupervised, single-pass data\nstream. {Based on this}, we propose a graph-based class relation preservation\nconstraint to overcome catastrophic forgetting. Furthermore, a pseudo-target\nreplay objective is used to mitigate error accumulation. Extensive experiments\ndemonstrate the superiority of our method in both segmentation and\nclassification CTTA tasks. Code is available at\n\\href{https://github.com/z1358/OBAO}{this https URL}.\n", "link": "http://arxiv.org/abs/2407.09367v1", "date": "2024-07-12", "relevancy": 2.73, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5611}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5605}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reshaping%20the%20Online%20Data%20Buffering%20and%20Organizing%20Mechanism%20for%0A%20%20Continual%20Test-Time%20Adaptation&body=Title%3A%20Reshaping%20the%20Online%20Data%20Buffering%20and%20Organizing%20Mechanism%20for%0A%20%20Continual%20Test-Time%20Adaptation%0AAuthor%3A%20Zhilin%20Zhu%20and%20Xiaopeng%20Hong%20and%20Zhiheng%20Ma%20and%20Weijun%20Zhuang%20and%20Yaohui%20Ma%20and%20Dai%20Yong%20and%20Yaowei%20Wang%0AAbstract%3A%20%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20involves%20adapting%20a%20pre-trained%20source%0Amodel%20to%20continually%20changing%20unsupervised%20target%20domains.%20In%20this%20paper%2C%20we%0Asystematically%20analyze%20the%20challenges%20of%20this%20task%3A%20online%20environment%2C%0Aunsupervised%20nature%2C%20and%20the%20risks%20of%20error%20accumulation%20and%20catastrophic%0Aforgetting%20under%20continual%20domain%20shifts.%20To%20address%20these%20challenges%2C%20we%0Areshape%20the%20online%20data%20buffering%20and%20organizing%20mechanism%20for%20CTTA.%20We%20propose%0Aan%20%7Buncertainty-aware%20buffering%20approach%7D%20to%20identify%20%7Band%20aggregate%7D%0Asignificant%20samples%20with%20high%20certainty%20from%20the%20unsupervised%2C%20single-pass%20data%0Astream.%20%7BBased%20on%20this%7D%2C%20we%20propose%20a%20graph-based%20class%20relation%20preservation%0Aconstraint%20to%20overcome%20catastrophic%20forgetting.%20Furthermore%2C%20a%20pseudo-target%0Areplay%20objective%20is%20used%20to%20mitigate%20error%20accumulation.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20our%20method%20in%20both%20segmentation%20and%0Aclassification%20CTTA%20tasks.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/z1358/OBAO%7D%7Bthis%20https%20URL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReshaping%2520the%2520Online%2520Data%2520Buffering%2520and%2520Organizing%2520Mechanism%2520for%250A%2520%2520Continual%2520Test-Time%2520Adaptation%26entry.906535625%3DZhilin%2520Zhu%2520and%2520Xiaopeng%2520Hong%2520and%2520Zhiheng%2520Ma%2520and%2520Weijun%2520Zhuang%2520and%2520Yaohui%2520Ma%2520and%2520Dai%2520Yong%2520and%2520Yaowei%2520Wang%26entry.1292438233%3D%2520%2520Continual%2520Test-Time%2520Adaptation%2520%2528CTTA%2529%2520involves%2520adapting%2520a%2520pre-trained%2520source%250Amodel%2520to%2520continually%2520changing%2520unsupervised%2520target%2520domains.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520analyze%2520the%2520challenges%2520of%2520this%2520task%253A%2520online%2520environment%252C%250Aunsupervised%2520nature%252C%2520and%2520the%2520risks%2520of%2520error%2520accumulation%2520and%2520catastrophic%250Aforgetting%2520under%2520continual%2520domain%2520shifts.%2520To%2520address%2520these%2520challenges%252C%2520we%250Areshape%2520the%2520online%2520data%2520buffering%2520and%2520organizing%2520mechanism%2520for%2520CTTA.%2520We%2520propose%250Aan%2520%257Buncertainty-aware%2520buffering%2520approach%257D%2520to%2520identify%2520%257Band%2520aggregate%257D%250Asignificant%2520samples%2520with%2520high%2520certainty%2520from%2520the%2520unsupervised%252C%2520single-pass%2520data%250Astream.%2520%257BBased%2520on%2520this%257D%252C%2520we%2520propose%2520a%2520graph-based%2520class%2520relation%2520preservation%250Aconstraint%2520to%2520overcome%2520catastrophic%2520forgetting.%2520Furthermore%252C%2520a%2520pseudo-target%250Areplay%2520objective%2520is%2520used%2520to%2520mitigate%2520error%2520accumulation.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method%2520in%2520both%2520segmentation%2520and%250Aclassification%2520CTTA%2520tasks.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/z1358/OBAO%257D%257Bthis%2520https%2520URL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reshaping%20the%20Online%20Data%20Buffering%20and%20Organizing%20Mechanism%20for%0A%20%20Continual%20Test-Time%20Adaptation&entry.906535625=Zhilin%20Zhu%20and%20Xiaopeng%20Hong%20and%20Zhiheng%20Ma%20and%20Weijun%20Zhuang%20and%20Yaohui%20Ma%20and%20Dai%20Yong%20and%20Yaowei%20Wang&entry.1292438233=%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20involves%20adapting%20a%20pre-trained%20source%0Amodel%20to%20continually%20changing%20unsupervised%20target%20domains.%20In%20this%20paper%2C%20we%0Asystematically%20analyze%20the%20challenges%20of%20this%20task%3A%20online%20environment%2C%0Aunsupervised%20nature%2C%20and%20the%20risks%20of%20error%20accumulation%20and%20catastrophic%0Aforgetting%20under%20continual%20domain%20shifts.%20To%20address%20these%20challenges%2C%20we%0Areshape%20the%20online%20data%20buffering%20and%20organizing%20mechanism%20for%20CTTA.%20We%20propose%0Aan%20%7Buncertainty-aware%20buffering%20approach%7D%20to%20identify%20%7Band%20aggregate%7D%0Asignificant%20samples%20with%20high%20certainty%20from%20the%20unsupervised%2C%20single-pass%20data%0Astream.%20%7BBased%20on%20this%7D%2C%20we%20propose%20a%20graph-based%20class%20relation%20preservation%0Aconstraint%20to%20overcome%20catastrophic%20forgetting.%20Furthermore%2C%20a%20pseudo-target%0Areplay%20objective%20is%20used%20to%20mitigate%20error%20accumulation.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20our%20method%20in%20both%20segmentation%20and%0Aclassification%20CTTA%20tasks.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/z1358/OBAO%7D%7Bthis%20https%20URL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09367v1&entry.124074799=Read"},
{"title": "A Unified Anomaly Synthesis Strategy with Gradient Ascent for Industrial\n  Anomaly Detection and Localization", "author": "Qiyu Chen and Huiyuan Luo and Chengkan Lv and Zhengtao Zhang", "abstract": "  Anomaly synthesis strategies can effectively enhance unsupervised anomaly\ndetection. However, existing strategies have limitations in the coverage and\ncontrollability of anomaly synthesis, particularly for weak defects that are\nvery similar to normal regions. In this paper, we propose Global and Local\nAnomaly co-Synthesis Strategy (GLASS), a novel unified framework designed to\nsynthesize a broader coverage of anomalies under the manifold and hypersphere\ndistribution constraints of Global Anomaly Synthesis (GAS) at the feature level\nand Local Anomaly Synthesis (LAS) at the image level. Our method synthesizes\nnear-in-distribution anomalies in a controllable way using Gaussian noise\nguided by gradient ascent and truncated projection. GLASS achieves\nstate-of-the-art results on the MVTec AD (detection AUROC of 99.9\\%), VisA, and\nMPDD datasets and excels in weak defect detection. The effectiveness and\nefficiency have been further validated in industrial applications for woven\nfabric defect detection. The code and dataset are available at:\n\\url{https://github.com/cqylunlun/GLASS}.\n", "link": "http://arxiv.org/abs/2407.09359v1", "date": "2024-07-12", "relevancy": 2.6877, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5471}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5332}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Anomaly%20Synthesis%20Strategy%20with%20Gradient%20Ascent%20for%20Industrial%0A%20%20Anomaly%20Detection%20and%20Localization&body=Title%3A%20A%20Unified%20Anomaly%20Synthesis%20Strategy%20with%20Gradient%20Ascent%20for%20Industrial%0A%20%20Anomaly%20Detection%20and%20Localization%0AAuthor%3A%20Qiyu%20Chen%20and%20Huiyuan%20Luo%20and%20Chengkan%20Lv%20and%20Zhengtao%20Zhang%0AAbstract%3A%20%20%20Anomaly%20synthesis%20strategies%20can%20effectively%20enhance%20unsupervised%20anomaly%0Adetection.%20However%2C%20existing%20strategies%20have%20limitations%20in%20the%20coverage%20and%0Acontrollability%20of%20anomaly%20synthesis%2C%20particularly%20for%20weak%20defects%20that%20are%0Avery%20similar%20to%20normal%20regions.%20In%20this%20paper%2C%20we%20propose%20Global%20and%20Local%0AAnomaly%20co-Synthesis%20Strategy%20%28GLASS%29%2C%20a%20novel%20unified%20framework%20designed%20to%0Asynthesize%20a%20broader%20coverage%20of%20anomalies%20under%20the%20manifold%20and%20hypersphere%0Adistribution%20constraints%20of%20Global%20Anomaly%20Synthesis%20%28GAS%29%20at%20the%20feature%20level%0Aand%20Local%20Anomaly%20Synthesis%20%28LAS%29%20at%20the%20image%20level.%20Our%20method%20synthesizes%0Anear-in-distribution%20anomalies%20in%20a%20controllable%20way%20using%20Gaussian%20noise%0Aguided%20by%20gradient%20ascent%20and%20truncated%20projection.%20GLASS%20achieves%0Astate-of-the-art%20results%20on%20the%20MVTec%20AD%20%28detection%20AUROC%20of%2099.9%5C%25%29%2C%20VisA%2C%20and%0AMPDD%20datasets%20and%20excels%20in%20weak%20defect%20detection.%20The%20effectiveness%20and%0Aefficiency%20have%20been%20further%20validated%20in%20industrial%20applications%20for%20woven%0Afabric%20defect%20detection.%20The%20code%20and%20dataset%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/cqylunlun/GLASS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Anomaly%2520Synthesis%2520Strategy%2520with%2520Gradient%2520Ascent%2520for%2520Industrial%250A%2520%2520Anomaly%2520Detection%2520and%2520Localization%26entry.906535625%3DQiyu%2520Chen%2520and%2520Huiyuan%2520Luo%2520and%2520Chengkan%2520Lv%2520and%2520Zhengtao%2520Zhang%26entry.1292438233%3D%2520%2520Anomaly%2520synthesis%2520strategies%2520can%2520effectively%2520enhance%2520unsupervised%2520anomaly%250Adetection.%2520However%252C%2520existing%2520strategies%2520have%2520limitations%2520in%2520the%2520coverage%2520and%250Acontrollability%2520of%2520anomaly%2520synthesis%252C%2520particularly%2520for%2520weak%2520defects%2520that%2520are%250Avery%2520similar%2520to%2520normal%2520regions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Global%2520and%2520Local%250AAnomaly%2520co-Synthesis%2520Strategy%2520%2528GLASS%2529%252C%2520a%2520novel%2520unified%2520framework%2520designed%2520to%250Asynthesize%2520a%2520broader%2520coverage%2520of%2520anomalies%2520under%2520the%2520manifold%2520and%2520hypersphere%250Adistribution%2520constraints%2520of%2520Global%2520Anomaly%2520Synthesis%2520%2528GAS%2529%2520at%2520the%2520feature%2520level%250Aand%2520Local%2520Anomaly%2520Synthesis%2520%2528LAS%2529%2520at%2520the%2520image%2520level.%2520Our%2520method%2520synthesizes%250Anear-in-distribution%2520anomalies%2520in%2520a%2520controllable%2520way%2520using%2520Gaussian%2520noise%250Aguided%2520by%2520gradient%2520ascent%2520and%2520truncated%2520projection.%2520GLASS%2520achieves%250Astate-of-the-art%2520results%2520on%2520the%2520MVTec%2520AD%2520%2528detection%2520AUROC%2520of%252099.9%255C%2525%2529%252C%2520VisA%252C%2520and%250AMPDD%2520datasets%2520and%2520excels%2520in%2520weak%2520defect%2520detection.%2520The%2520effectiveness%2520and%250Aefficiency%2520have%2520been%2520further%2520validated%2520in%2520industrial%2520applications%2520for%2520woven%250Afabric%2520defect%2520detection.%2520The%2520code%2520and%2520dataset%2520are%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/cqylunlun/GLASS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Anomaly%20Synthesis%20Strategy%20with%20Gradient%20Ascent%20for%20Industrial%0A%20%20Anomaly%20Detection%20and%20Localization&entry.906535625=Qiyu%20Chen%20and%20Huiyuan%20Luo%20and%20Chengkan%20Lv%20and%20Zhengtao%20Zhang&entry.1292438233=%20%20Anomaly%20synthesis%20strategies%20can%20effectively%20enhance%20unsupervised%20anomaly%0Adetection.%20However%2C%20existing%20strategies%20have%20limitations%20in%20the%20coverage%20and%0Acontrollability%20of%20anomaly%20synthesis%2C%20particularly%20for%20weak%20defects%20that%20are%0Avery%20similar%20to%20normal%20regions.%20In%20this%20paper%2C%20we%20propose%20Global%20and%20Local%0AAnomaly%20co-Synthesis%20Strategy%20%28GLASS%29%2C%20a%20novel%20unified%20framework%20designed%20to%0Asynthesize%20a%20broader%20coverage%20of%20anomalies%20under%20the%20manifold%20and%20hypersphere%0Adistribution%20constraints%20of%20Global%20Anomaly%20Synthesis%20%28GAS%29%20at%20the%20feature%20level%0Aand%20Local%20Anomaly%20Synthesis%20%28LAS%29%20at%20the%20image%20level.%20Our%20method%20synthesizes%0Anear-in-distribution%20anomalies%20in%20a%20controllable%20way%20using%20Gaussian%20noise%0Aguided%20by%20gradient%20ascent%20and%20truncated%20projection.%20GLASS%20achieves%0Astate-of-the-art%20results%20on%20the%20MVTec%20AD%20%28detection%20AUROC%20of%2099.9%5C%25%29%2C%20VisA%2C%20and%0AMPDD%20datasets%20and%20excels%20in%20weak%20defect%20detection.%20The%20effectiveness%20and%0Aefficiency%20have%20been%20further%20validated%20in%20industrial%20applications%20for%20woven%0Afabric%20defect%20detection.%20The%20code%20and%20dataset%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/cqylunlun/GLASS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09359v1&entry.124074799=Read"},
{"title": "MetaFood CVPR 2024 Challenge on Physically Informed 3D Food\n  Reconstruction: Methods and Results", "author": "Jiangpeng He and Yuhao Chen and Gautham Vinod and Talha Ibn Mahmud and Fengqing Zhu and Edward Delp and Alexander Wong and Pengcheng Xi and Ahmad AlMughrabi and Umair Haroon and Ricardo Marques and Petia Radeva and Jiadong Tang and Dianyi Yang and Yu Gao and Zhaoxiang Liang and Yawei Jueluo and Chengyu Shi and Pengyu Wang", "abstract": "  The increasing interest in computer vision applications for nutrition and\ndietary monitoring has led to the development of advanced 3D reconstruction\ntechniques for food items. However, the scarcity of high-quality data and\nlimited collaboration between industry and academia have constrained progress\nin this field. Building on recent advancements in 3D reconstruction, we host\nthe MetaFood Workshop and its challenge for Physically Informed 3D Food\nReconstruction. This challenge focuses on reconstructing volume-accurate 3D\nmodels of food items from 2D images, using a visible checkerboard as a size\nreference. Participants were tasked with reconstructing 3D models for 20\nselected food items of varying difficulty levels: easy, medium, and hard. The\neasy level provides 200 images, the medium level provides 30 images, and the\nhard level provides only 1 image for reconstruction. In total, 16 teams\nsubmitted results in the final testing phase. The solutions developed in this\nchallenge achieved promising results in 3D food reconstruction, with\nsignificant potential for improving portion estimation for dietary assessment\nand nutritional monitoring. More details about this workshop challenge and\naccess to the dataset can be found at\nhttps://sites.google.com/view/cvpr-metafood-2024.\n", "link": "http://arxiv.org/abs/2407.09285v1", "date": "2024-07-12", "relevancy": 2.6641, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5578}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5578}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaFood%20CVPR%202024%20Challenge%20on%20Physically%20Informed%203D%20Food%0A%20%20Reconstruction%3A%20Methods%20and%20Results&body=Title%3A%20MetaFood%20CVPR%202024%20Challenge%20on%20Physically%20Informed%203D%20Food%0A%20%20Reconstruction%3A%20Methods%20and%20Results%0AAuthor%3A%20Jiangpeng%20He%20and%20Yuhao%20Chen%20and%20Gautham%20Vinod%20and%20Talha%20Ibn%20Mahmud%20and%20Fengqing%20Zhu%20and%20Edward%20Delp%20and%20Alexander%20Wong%20and%20Pengcheng%20Xi%20and%20Ahmad%20AlMughrabi%20and%20Umair%20Haroon%20and%20Ricardo%20Marques%20and%20Petia%20Radeva%20and%20Jiadong%20Tang%20and%20Dianyi%20Yang%20and%20Yu%20Gao%20and%20Zhaoxiang%20Liang%20and%20Yawei%20Jueluo%20and%20Chengyu%20Shi%20and%20Pengyu%20Wang%0AAbstract%3A%20%20%20The%20increasing%20interest%20in%20computer%20vision%20applications%20for%20nutrition%20and%0Adietary%20monitoring%20has%20led%20to%20the%20development%20of%20advanced%203D%20reconstruction%0Atechniques%20for%20food%20items.%20However%2C%20the%20scarcity%20of%20high-quality%20data%20and%0Alimited%20collaboration%20between%20industry%20and%20academia%20have%20constrained%20progress%0Ain%20this%20field.%20Building%20on%20recent%20advancements%20in%203D%20reconstruction%2C%20we%20host%0Athe%20MetaFood%20Workshop%20and%20its%20challenge%20for%20Physically%20Informed%203D%20Food%0AReconstruction.%20This%20challenge%20focuses%20on%20reconstructing%20volume-accurate%203D%0Amodels%20of%20food%20items%20from%202D%20images%2C%20using%20a%20visible%20checkerboard%20as%20a%20size%0Areference.%20Participants%20were%20tasked%20with%20reconstructing%203D%20models%20for%2020%0Aselected%20food%20items%20of%20varying%20difficulty%20levels%3A%20easy%2C%20medium%2C%20and%20hard.%20The%0Aeasy%20level%20provides%20200%20images%2C%20the%20medium%20level%20provides%2030%20images%2C%20and%20the%0Ahard%20level%20provides%20only%201%20image%20for%20reconstruction.%20In%20total%2C%2016%20teams%0Asubmitted%20results%20in%20the%20final%20testing%20phase.%20The%20solutions%20developed%20in%20this%0Achallenge%20achieved%20promising%20results%20in%203D%20food%20reconstruction%2C%20with%0Asignificant%20potential%20for%20improving%20portion%20estimation%20for%20dietary%20assessment%0Aand%20nutritional%20monitoring.%20More%20details%20about%20this%20workshop%20challenge%20and%0Aaccess%20to%20the%20dataset%20can%20be%20found%20at%0Ahttps%3A//sites.google.com/view/cvpr-metafood-2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaFood%2520CVPR%25202024%2520Challenge%2520on%2520Physically%2520Informed%25203D%2520Food%250A%2520%2520Reconstruction%253A%2520Methods%2520and%2520Results%26entry.906535625%3DJiangpeng%2520He%2520and%2520Yuhao%2520Chen%2520and%2520Gautham%2520Vinod%2520and%2520Talha%2520Ibn%2520Mahmud%2520and%2520Fengqing%2520Zhu%2520and%2520Edward%2520Delp%2520and%2520Alexander%2520Wong%2520and%2520Pengcheng%2520Xi%2520and%2520Ahmad%2520AlMughrabi%2520and%2520Umair%2520Haroon%2520and%2520Ricardo%2520Marques%2520and%2520Petia%2520Radeva%2520and%2520Jiadong%2520Tang%2520and%2520Dianyi%2520Yang%2520and%2520Yu%2520Gao%2520and%2520Zhaoxiang%2520Liang%2520and%2520Yawei%2520Jueluo%2520and%2520Chengyu%2520Shi%2520and%2520Pengyu%2520Wang%26entry.1292438233%3D%2520%2520The%2520increasing%2520interest%2520in%2520computer%2520vision%2520applications%2520for%2520nutrition%2520and%250Adietary%2520monitoring%2520has%2520led%2520to%2520the%2520development%2520of%2520advanced%25203D%2520reconstruction%250Atechniques%2520for%2520food%2520items.%2520However%252C%2520the%2520scarcity%2520of%2520high-quality%2520data%2520and%250Alimited%2520collaboration%2520between%2520industry%2520and%2520academia%2520have%2520constrained%2520progress%250Ain%2520this%2520field.%2520Building%2520on%2520recent%2520advancements%2520in%25203D%2520reconstruction%252C%2520we%2520host%250Athe%2520MetaFood%2520Workshop%2520and%2520its%2520challenge%2520for%2520Physically%2520Informed%25203D%2520Food%250AReconstruction.%2520This%2520challenge%2520focuses%2520on%2520reconstructing%2520volume-accurate%25203D%250Amodels%2520of%2520food%2520items%2520from%25202D%2520images%252C%2520using%2520a%2520visible%2520checkerboard%2520as%2520a%2520size%250Areference.%2520Participants%2520were%2520tasked%2520with%2520reconstructing%25203D%2520models%2520for%252020%250Aselected%2520food%2520items%2520of%2520varying%2520difficulty%2520levels%253A%2520easy%252C%2520medium%252C%2520and%2520hard.%2520The%250Aeasy%2520level%2520provides%2520200%2520images%252C%2520the%2520medium%2520level%2520provides%252030%2520images%252C%2520and%2520the%250Ahard%2520level%2520provides%2520only%25201%2520image%2520for%2520reconstruction.%2520In%2520total%252C%252016%2520teams%250Asubmitted%2520results%2520in%2520the%2520final%2520testing%2520phase.%2520The%2520solutions%2520developed%2520in%2520this%250Achallenge%2520achieved%2520promising%2520results%2520in%25203D%2520food%2520reconstruction%252C%2520with%250Asignificant%2520potential%2520for%2520improving%2520portion%2520estimation%2520for%2520dietary%2520assessment%250Aand%2520nutritional%2520monitoring.%2520More%2520details%2520about%2520this%2520workshop%2520challenge%2520and%250Aaccess%2520to%2520the%2520dataset%2520can%2520be%2520found%2520at%250Ahttps%253A//sites.google.com/view/cvpr-metafood-2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaFood%20CVPR%202024%20Challenge%20on%20Physically%20Informed%203D%20Food%0A%20%20Reconstruction%3A%20Methods%20and%20Results&entry.906535625=Jiangpeng%20He%20and%20Yuhao%20Chen%20and%20Gautham%20Vinod%20and%20Talha%20Ibn%20Mahmud%20and%20Fengqing%20Zhu%20and%20Edward%20Delp%20and%20Alexander%20Wong%20and%20Pengcheng%20Xi%20and%20Ahmad%20AlMughrabi%20and%20Umair%20Haroon%20and%20Ricardo%20Marques%20and%20Petia%20Radeva%20and%20Jiadong%20Tang%20and%20Dianyi%20Yang%20and%20Yu%20Gao%20and%20Zhaoxiang%20Liang%20and%20Yawei%20Jueluo%20and%20Chengyu%20Shi%20and%20Pengyu%20Wang&entry.1292438233=%20%20The%20increasing%20interest%20in%20computer%20vision%20applications%20for%20nutrition%20and%0Adietary%20monitoring%20has%20led%20to%20the%20development%20of%20advanced%203D%20reconstruction%0Atechniques%20for%20food%20items.%20However%2C%20the%20scarcity%20of%20high-quality%20data%20and%0Alimited%20collaboration%20between%20industry%20and%20academia%20have%20constrained%20progress%0Ain%20this%20field.%20Building%20on%20recent%20advancements%20in%203D%20reconstruction%2C%20we%20host%0Athe%20MetaFood%20Workshop%20and%20its%20challenge%20for%20Physically%20Informed%203D%20Food%0AReconstruction.%20This%20challenge%20focuses%20on%20reconstructing%20volume-accurate%203D%0Amodels%20of%20food%20items%20from%202D%20images%2C%20using%20a%20visible%20checkerboard%20as%20a%20size%0Areference.%20Participants%20were%20tasked%20with%20reconstructing%203D%20models%20for%2020%0Aselected%20food%20items%20of%20varying%20difficulty%20levels%3A%20easy%2C%20medium%2C%20and%20hard.%20The%0Aeasy%20level%20provides%20200%20images%2C%20the%20medium%20level%20provides%2030%20images%2C%20and%20the%0Ahard%20level%20provides%20only%201%20image%20for%20reconstruction.%20In%20total%2C%2016%20teams%0Asubmitted%20results%20in%20the%20final%20testing%20phase.%20The%20solutions%20developed%20in%20this%0Achallenge%20achieved%20promising%20results%20in%203D%20food%20reconstruction%2C%20with%0Asignificant%20potential%20for%20improving%20portion%20estimation%20for%20dietary%20assessment%0Aand%20nutritional%20monitoring.%20More%20details%20about%20this%20workshop%20challenge%20and%0Aaccess%20to%20the%20dataset%20can%20be%20found%20at%0Ahttps%3A//sites.google.com/view/cvpr-metafood-2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09285v1&entry.124074799=Read"},
{"title": "3DReact: Geometric deep learning for chemical reactions", "author": "Puck van Gerwen and Ksenia R. Briling and Charlotte Bunne and Vignesh Ram Somnath and Ruben Laplaza and Andreas Krause and Clemence Corminboeuf", "abstract": "  Geometric deep learning models, which incorporate the relevant molecular\nsymmetries within the neural network architecture, have considerably improved\nthe accuracy and data efficiency of predictions of molecular properties.\nBuilding on this success, we introduce 3DReact, a geometric deep learning model\nto predict reaction properties from three-dimensional structures of reactants\nand products. We demonstrate that the invariant version of the model is\nsufficient for existing reaction datasets. We illustrate its competitive\nperformance on the prediction of activation barriers on the GDB7-22-TS,\nCyclo-23-TS and Proparg-21-TS datasets in different atom-mapping regimes. We\nshow that, compared to existing models for reaction property prediction,\n3DReact offers a flexible framework that exploits atom-mapping information, if\navailable, as well as geometries of reactants and products (in an invariant or\nequivariant fashion). Accordingly, it performs systematically well across\ndifferent datasets, atom-mapping regimes, as well as both interpolation and\nextrapolation tasks.\n", "link": "http://arxiv.org/abs/2312.08307v2", "date": "2024-07-12", "relevancy": 2.6539, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5423}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5423}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DReact%3A%20Geometric%20deep%20learning%20for%20chemical%20reactions&body=Title%3A%203DReact%3A%20Geometric%20deep%20learning%20for%20chemical%20reactions%0AAuthor%3A%20Puck%20van%20Gerwen%20and%20Ksenia%20R.%20Briling%20and%20Charlotte%20Bunne%20and%20Vignesh%20Ram%20Somnath%20and%20Ruben%20Laplaza%20and%20Andreas%20Krause%20and%20Clemence%20Corminboeuf%0AAbstract%3A%20%20%20Geometric%20deep%20learning%20models%2C%20which%20incorporate%20the%20relevant%20molecular%0Asymmetries%20within%20the%20neural%20network%20architecture%2C%20have%20considerably%20improved%0Athe%20accuracy%20and%20data%20efficiency%20of%20predictions%20of%20molecular%20properties.%0ABuilding%20on%20this%20success%2C%20we%20introduce%203DReact%2C%20a%20geometric%20deep%20learning%20model%0Ato%20predict%20reaction%20properties%20from%20three-dimensional%20structures%20of%20reactants%0Aand%20products.%20We%20demonstrate%20that%20the%20invariant%20version%20of%20the%20model%20is%0Asufficient%20for%20existing%20reaction%20datasets.%20We%20illustrate%20its%20competitive%0Aperformance%20on%20the%20prediction%20of%20activation%20barriers%20on%20the%20GDB7-22-TS%2C%0ACyclo-23-TS%20and%20Proparg-21-TS%20datasets%20in%20different%20atom-mapping%20regimes.%20We%0Ashow%20that%2C%20compared%20to%20existing%20models%20for%20reaction%20property%20prediction%2C%0A3DReact%20offers%20a%20flexible%20framework%20that%20exploits%20atom-mapping%20information%2C%20if%0Aavailable%2C%20as%20well%20as%20geometries%20of%20reactants%20and%20products%20%28in%20an%20invariant%20or%0Aequivariant%20fashion%29.%20Accordingly%2C%20it%20performs%20systematically%20well%20across%0Adifferent%20datasets%2C%20atom-mapping%20regimes%2C%20as%20well%20as%20both%20interpolation%20and%0Aextrapolation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08307v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DReact%253A%2520Geometric%2520deep%2520learning%2520for%2520chemical%2520reactions%26entry.906535625%3DPuck%2520van%2520Gerwen%2520and%2520Ksenia%2520R.%2520Briling%2520and%2520Charlotte%2520Bunne%2520and%2520Vignesh%2520Ram%2520Somnath%2520and%2520Ruben%2520Laplaza%2520and%2520Andreas%2520Krause%2520and%2520Clemence%2520Corminboeuf%26entry.1292438233%3D%2520%2520Geometric%2520deep%2520learning%2520models%252C%2520which%2520incorporate%2520the%2520relevant%2520molecular%250Asymmetries%2520within%2520the%2520neural%2520network%2520architecture%252C%2520have%2520considerably%2520improved%250Athe%2520accuracy%2520and%2520data%2520efficiency%2520of%2520predictions%2520of%2520molecular%2520properties.%250ABuilding%2520on%2520this%2520success%252C%2520we%2520introduce%25203DReact%252C%2520a%2520geometric%2520deep%2520learning%2520model%250Ato%2520predict%2520reaction%2520properties%2520from%2520three-dimensional%2520structures%2520of%2520reactants%250Aand%2520products.%2520We%2520demonstrate%2520that%2520the%2520invariant%2520version%2520of%2520the%2520model%2520is%250Asufficient%2520for%2520existing%2520reaction%2520datasets.%2520We%2520illustrate%2520its%2520competitive%250Aperformance%2520on%2520the%2520prediction%2520of%2520activation%2520barriers%2520on%2520the%2520GDB7-22-TS%252C%250ACyclo-23-TS%2520and%2520Proparg-21-TS%2520datasets%2520in%2520different%2520atom-mapping%2520regimes.%2520We%250Ashow%2520that%252C%2520compared%2520to%2520existing%2520models%2520for%2520reaction%2520property%2520prediction%252C%250A3DReact%2520offers%2520a%2520flexible%2520framework%2520that%2520exploits%2520atom-mapping%2520information%252C%2520if%250Aavailable%252C%2520as%2520well%2520as%2520geometries%2520of%2520reactants%2520and%2520products%2520%2528in%2520an%2520invariant%2520or%250Aequivariant%2520fashion%2529.%2520Accordingly%252C%2520it%2520performs%2520systematically%2520well%2520across%250Adifferent%2520datasets%252C%2520atom-mapping%2520regimes%252C%2520as%2520well%2520as%2520both%2520interpolation%2520and%250Aextrapolation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08307v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DReact%3A%20Geometric%20deep%20learning%20for%20chemical%20reactions&entry.906535625=Puck%20van%20Gerwen%20and%20Ksenia%20R.%20Briling%20and%20Charlotte%20Bunne%20and%20Vignesh%20Ram%20Somnath%20and%20Ruben%20Laplaza%20and%20Andreas%20Krause%20and%20Clemence%20Corminboeuf&entry.1292438233=%20%20Geometric%20deep%20learning%20models%2C%20which%20incorporate%20the%20relevant%20molecular%0Asymmetries%20within%20the%20neural%20network%20architecture%2C%20have%20considerably%20improved%0Athe%20accuracy%20and%20data%20efficiency%20of%20predictions%20of%20molecular%20properties.%0ABuilding%20on%20this%20success%2C%20we%20introduce%203DReact%2C%20a%20geometric%20deep%20learning%20model%0Ato%20predict%20reaction%20properties%20from%20three-dimensional%20structures%20of%20reactants%0Aand%20products.%20We%20demonstrate%20that%20the%20invariant%20version%20of%20the%20model%20is%0Asufficient%20for%20existing%20reaction%20datasets.%20We%20illustrate%20its%20competitive%0Aperformance%20on%20the%20prediction%20of%20activation%20barriers%20on%20the%20GDB7-22-TS%2C%0ACyclo-23-TS%20and%20Proparg-21-TS%20datasets%20in%20different%20atom-mapping%20regimes.%20We%0Ashow%20that%2C%20compared%20to%20existing%20models%20for%20reaction%20property%20prediction%2C%0A3DReact%20offers%20a%20flexible%20framework%20that%20exploits%20atom-mapping%20information%2C%20if%0Aavailable%2C%20as%20well%20as%20geometries%20of%20reactants%20and%20products%20%28in%20an%20invariant%20or%0Aequivariant%20fashion%29.%20Accordingly%2C%20it%20performs%20systematically%20well%20across%0Adifferent%20datasets%2C%20atom-mapping%20regimes%2C%20as%20well%20as%20both%20interpolation%20and%0Aextrapolation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08307v2&entry.124074799=Read"},
{"title": "SS-SfP:Neural Inverse Rendering for Self Supervised Shape from (Mixed)\n  Polarization", "author": "Ashish Tiwari and Shanmuganathan Raman", "abstract": "  We present a novel inverse rendering-based framework to estimate the 3D shape\n(per-pixel surface normals and depth) of objects and scenes from single-view\npolarization images, the problem popularly known as Shape from Polarization\n(SfP). The existing physics-based and learning-based methods for SfP perform\nunder certain restrictions, i.e., (a) purely diffuse or purely specular\nreflections, which are seldom in the real surfaces, (b) availability of the\nground truth surface normals for direct supervision that are hard to acquire\nand are limited by the scanner's resolution, and (c) known refractive index. To\novercome these restrictions, we start by learning to separate the\npartially-polarized diffuse and specular reflection components, which we call\nreflectance cues, based on a modified polarization reflection model and then\nestimate shape under mixed polarization through an inverse-rendering based\nself-supervised deep learning framework called SS-SfP, guided by the\npolarization data and estimated reflectance cues. Furthermore, we also obtain\nthe refractive index as a non-linear least squares solution. Through extensive\nquantitative and qualitative evaluation, we establish the efficacy of the\nproposed framework over simple single-object scenes from DeepSfP dataset and\ncomplex in-the-wild scenes from SPW dataset in an entirely self-supervised\nsetting. To the best of our knowledge, this is the first learning-based\napproach to address SfP under mixed polarization in a completely\nself-supervised framework.\n", "link": "http://arxiv.org/abs/2407.09294v1", "date": "2024-07-12", "relevancy": 2.6522, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5399}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SS-SfP%3ANeural%20Inverse%20Rendering%20for%20Self%20Supervised%20Shape%20from%20%28Mixed%29%0A%20%20Polarization&body=Title%3A%20SS-SfP%3ANeural%20Inverse%20Rendering%20for%20Self%20Supervised%20Shape%20from%20%28Mixed%29%0A%20%20Polarization%0AAuthor%3A%20Ashish%20Tiwari%20and%20Shanmuganathan%20Raman%0AAbstract%3A%20%20%20We%20present%20a%20novel%20inverse%20rendering-based%20framework%20to%20estimate%20the%203D%20shape%0A%28per-pixel%20surface%20normals%20and%20depth%29%20of%20objects%20and%20scenes%20from%20single-view%0Apolarization%20images%2C%20the%20problem%20popularly%20known%20as%20Shape%20from%20Polarization%0A%28SfP%29.%20The%20existing%20physics-based%20and%20learning-based%20methods%20for%20SfP%20perform%0Aunder%20certain%20restrictions%2C%20i.e.%2C%20%28a%29%20purely%20diffuse%20or%20purely%20specular%0Areflections%2C%20which%20are%20seldom%20in%20the%20real%20surfaces%2C%20%28b%29%20availability%20of%20the%0Aground%20truth%20surface%20normals%20for%20direct%20supervision%20that%20are%20hard%20to%20acquire%0Aand%20are%20limited%20by%20the%20scanner%27s%20resolution%2C%20and%20%28c%29%20known%20refractive%20index.%20To%0Aovercome%20these%20restrictions%2C%20we%20start%20by%20learning%20to%20separate%20the%0Apartially-polarized%20diffuse%20and%20specular%20reflection%20components%2C%20which%20we%20call%0Areflectance%20cues%2C%20based%20on%20a%20modified%20polarization%20reflection%20model%20and%20then%0Aestimate%20shape%20under%20mixed%20polarization%20through%20an%20inverse-rendering%20based%0Aself-supervised%20deep%20learning%20framework%20called%20SS-SfP%2C%20guided%20by%20the%0Apolarization%20data%20and%20estimated%20reflectance%20cues.%20Furthermore%2C%20we%20also%20obtain%0Athe%20refractive%20index%20as%20a%20non-linear%20least%20squares%20solution.%20Through%20extensive%0Aquantitative%20and%20qualitative%20evaluation%2C%20we%20establish%20the%20efficacy%20of%20the%0Aproposed%20framework%20over%20simple%20single-object%20scenes%20from%20DeepSfP%20dataset%20and%0Acomplex%20in-the-wild%20scenes%20from%20SPW%20dataset%20in%20an%20entirely%20self-supervised%0Asetting.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20learning-based%0Aapproach%20to%20address%20SfP%20under%20mixed%20polarization%20in%20a%20completely%0Aself-supervised%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSS-SfP%253ANeural%2520Inverse%2520Rendering%2520for%2520Self%2520Supervised%2520Shape%2520from%2520%2528Mixed%2529%250A%2520%2520Polarization%26entry.906535625%3DAshish%2520Tiwari%2520and%2520Shanmuganathan%2520Raman%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520inverse%2520rendering-based%2520framework%2520to%2520estimate%2520the%25203D%2520shape%250A%2528per-pixel%2520surface%2520normals%2520and%2520depth%2529%2520of%2520objects%2520and%2520scenes%2520from%2520single-view%250Apolarization%2520images%252C%2520the%2520problem%2520popularly%2520known%2520as%2520Shape%2520from%2520Polarization%250A%2528SfP%2529.%2520The%2520existing%2520physics-based%2520and%2520learning-based%2520methods%2520for%2520SfP%2520perform%250Aunder%2520certain%2520restrictions%252C%2520i.e.%252C%2520%2528a%2529%2520purely%2520diffuse%2520or%2520purely%2520specular%250Areflections%252C%2520which%2520are%2520seldom%2520in%2520the%2520real%2520surfaces%252C%2520%2528b%2529%2520availability%2520of%2520the%250Aground%2520truth%2520surface%2520normals%2520for%2520direct%2520supervision%2520that%2520are%2520hard%2520to%2520acquire%250Aand%2520are%2520limited%2520by%2520the%2520scanner%2527s%2520resolution%252C%2520and%2520%2528c%2529%2520known%2520refractive%2520index.%2520To%250Aovercome%2520these%2520restrictions%252C%2520we%2520start%2520by%2520learning%2520to%2520separate%2520the%250Apartially-polarized%2520diffuse%2520and%2520specular%2520reflection%2520components%252C%2520which%2520we%2520call%250Areflectance%2520cues%252C%2520based%2520on%2520a%2520modified%2520polarization%2520reflection%2520model%2520and%2520then%250Aestimate%2520shape%2520under%2520mixed%2520polarization%2520through%2520an%2520inverse-rendering%2520based%250Aself-supervised%2520deep%2520learning%2520framework%2520called%2520SS-SfP%252C%2520guided%2520by%2520the%250Apolarization%2520data%2520and%2520estimated%2520reflectance%2520cues.%2520Furthermore%252C%2520we%2520also%2520obtain%250Athe%2520refractive%2520index%2520as%2520a%2520non-linear%2520least%2520squares%2520solution.%2520Through%2520extensive%250Aquantitative%2520and%2520qualitative%2520evaluation%252C%2520we%2520establish%2520the%2520efficacy%2520of%2520the%250Aproposed%2520framework%2520over%2520simple%2520single-object%2520scenes%2520from%2520DeepSfP%2520dataset%2520and%250Acomplex%2520in-the-wild%2520scenes%2520from%2520SPW%2520dataset%2520in%2520an%2520entirely%2520self-supervised%250Asetting.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520learning-based%250Aapproach%2520to%2520address%2520SfP%2520under%2520mixed%2520polarization%2520in%2520a%2520completely%250Aself-supervised%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SS-SfP%3ANeural%20Inverse%20Rendering%20for%20Self%20Supervised%20Shape%20from%20%28Mixed%29%0A%20%20Polarization&entry.906535625=Ashish%20Tiwari%20and%20Shanmuganathan%20Raman&entry.1292438233=%20%20We%20present%20a%20novel%20inverse%20rendering-based%20framework%20to%20estimate%20the%203D%20shape%0A%28per-pixel%20surface%20normals%20and%20depth%29%20of%20objects%20and%20scenes%20from%20single-view%0Apolarization%20images%2C%20the%20problem%20popularly%20known%20as%20Shape%20from%20Polarization%0A%28SfP%29.%20The%20existing%20physics-based%20and%20learning-based%20methods%20for%20SfP%20perform%0Aunder%20certain%20restrictions%2C%20i.e.%2C%20%28a%29%20purely%20diffuse%20or%20purely%20specular%0Areflections%2C%20which%20are%20seldom%20in%20the%20real%20surfaces%2C%20%28b%29%20availability%20of%20the%0Aground%20truth%20surface%20normals%20for%20direct%20supervision%20that%20are%20hard%20to%20acquire%0Aand%20are%20limited%20by%20the%20scanner%27s%20resolution%2C%20and%20%28c%29%20known%20refractive%20index.%20To%0Aovercome%20these%20restrictions%2C%20we%20start%20by%20learning%20to%20separate%20the%0Apartially-polarized%20diffuse%20and%20specular%20reflection%20components%2C%20which%20we%20call%0Areflectance%20cues%2C%20based%20on%20a%20modified%20polarization%20reflection%20model%20and%20then%0Aestimate%20shape%20under%20mixed%20polarization%20through%20an%20inverse-rendering%20based%0Aself-supervised%20deep%20learning%20framework%20called%20SS-SfP%2C%20guided%20by%20the%0Apolarization%20data%20and%20estimated%20reflectance%20cues.%20Furthermore%2C%20we%20also%20obtain%0Athe%20refractive%20index%20as%20a%20non-linear%20least%20squares%20solution.%20Through%20extensive%0Aquantitative%20and%20qualitative%20evaluation%2C%20we%20establish%20the%20efficacy%20of%20the%0Aproposed%20framework%20over%20simple%20single-object%20scenes%20from%20DeepSfP%20dataset%20and%0Acomplex%20in-the-wild%20scenes%20from%20SPW%20dataset%20in%20an%20entirely%20self-supervised%0Asetting.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20learning-based%0Aapproach%20to%20address%20SfP%20under%20mixed%20polarization%20in%20a%20completely%0Aself-supervised%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09294v1&entry.124074799=Read"},
{"title": "GAVEL: Generating Games Via Evolution and Language Models", "author": "Graham Todd and Alexander Padula and Matthew Stephenson and \u00c9ric Piette and Dennis J. N. J. Soemers and Julian Togelius", "abstract": "  Automatically generating novel and interesting games is a complex task.\nChallenges include representing game rules in a computationally workable form,\nsearching through the large space of potential games under most such\nrepresentations, and accurately evaluating the originality and quality of\npreviously unseen games. Prior work in automated game generation has largely\nfocused on relatively restricted rule representations and relied on\ndomain-specific heuristics. In this work, we explore the generation of novel\ngames in the comparatively expansive Ludii game description language, which\nencodes the rules of over 1000 board games in a variety of styles and modes of\nplay. We draw inspiration from recent advances in large language models and\nevolutionary computation in order to train a model that intelligently mutates\nand recombines games and mechanics expressed as code. We demonstrate both\nquantitatively and qualitatively that our approach is capable of generating new\nand interesting games, including in regions of the potential rules space not\ncovered by existing games in the Ludii dataset. A sample of the generated games\nare available to play online through the Ludii portal.\n", "link": "http://arxiv.org/abs/2407.09388v1", "date": "2024-07-12", "relevancy": 2.6186, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5642}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5093}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAVEL%3A%20Generating%20Games%20Via%20Evolution%20and%20Language%20Models&body=Title%3A%20GAVEL%3A%20Generating%20Games%20Via%20Evolution%20and%20Language%20Models%0AAuthor%3A%20Graham%20Todd%20and%20Alexander%20Padula%20and%20Matthew%20Stephenson%20and%20%C3%89ric%20Piette%20and%20Dennis%20J.%20N.%20J.%20Soemers%20and%20Julian%20Togelius%0AAbstract%3A%20%20%20Automatically%20generating%20novel%20and%20interesting%20games%20is%20a%20complex%20task.%0AChallenges%20include%20representing%20game%20rules%20in%20a%20computationally%20workable%20form%2C%0Asearching%20through%20the%20large%20space%20of%20potential%20games%20under%20most%20such%0Arepresentations%2C%20and%20accurately%20evaluating%20the%20originality%20and%20quality%20of%0Apreviously%20unseen%20games.%20Prior%20work%20in%20automated%20game%20generation%20has%20largely%0Afocused%20on%20relatively%20restricted%20rule%20representations%20and%20relied%20on%0Adomain-specific%20heuristics.%20In%20this%20work%2C%20we%20explore%20the%20generation%20of%20novel%0Agames%20in%20the%20comparatively%20expansive%20Ludii%20game%20description%20language%2C%20which%0Aencodes%20the%20rules%20of%20over%201000%20board%20games%20in%20a%20variety%20of%20styles%20and%20modes%20of%0Aplay.%20We%20draw%20inspiration%20from%20recent%20advances%20in%20large%20language%20models%20and%0Aevolutionary%20computation%20in%20order%20to%20train%20a%20model%20that%20intelligently%20mutates%0Aand%20recombines%20games%20and%20mechanics%20expressed%20as%20code.%20We%20demonstrate%20both%0Aquantitatively%20and%20qualitatively%20that%20our%20approach%20is%20capable%20of%20generating%20new%0Aand%20interesting%20games%2C%20including%20in%20regions%20of%20the%20potential%20rules%20space%20not%0Acovered%20by%20existing%20games%20in%20the%20Ludii%20dataset.%20A%20sample%20of%20the%20generated%20games%0Aare%20available%20to%20play%20online%20through%20the%20Ludii%20portal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAVEL%253A%2520Generating%2520Games%2520Via%2520Evolution%2520and%2520Language%2520Models%26entry.906535625%3DGraham%2520Todd%2520and%2520Alexander%2520Padula%2520and%2520Matthew%2520Stephenson%2520and%2520%25C3%2589ric%2520Piette%2520and%2520Dennis%2520J.%2520N.%2520J.%2520Soemers%2520and%2520Julian%2520Togelius%26entry.1292438233%3D%2520%2520Automatically%2520generating%2520novel%2520and%2520interesting%2520games%2520is%2520a%2520complex%2520task.%250AChallenges%2520include%2520representing%2520game%2520rules%2520in%2520a%2520computationally%2520workable%2520form%252C%250Asearching%2520through%2520the%2520large%2520space%2520of%2520potential%2520games%2520under%2520most%2520such%250Arepresentations%252C%2520and%2520accurately%2520evaluating%2520the%2520originality%2520and%2520quality%2520of%250Apreviously%2520unseen%2520games.%2520Prior%2520work%2520in%2520automated%2520game%2520generation%2520has%2520largely%250Afocused%2520on%2520relatively%2520restricted%2520rule%2520representations%2520and%2520relied%2520on%250Adomain-specific%2520heuristics.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520generation%2520of%2520novel%250Agames%2520in%2520the%2520comparatively%2520expansive%2520Ludii%2520game%2520description%2520language%252C%2520which%250Aencodes%2520the%2520rules%2520of%2520over%25201000%2520board%2520games%2520in%2520a%2520variety%2520of%2520styles%2520and%2520modes%2520of%250Aplay.%2520We%2520draw%2520inspiration%2520from%2520recent%2520advances%2520in%2520large%2520language%2520models%2520and%250Aevolutionary%2520computation%2520in%2520order%2520to%2520train%2520a%2520model%2520that%2520intelligently%2520mutates%250Aand%2520recombines%2520games%2520and%2520mechanics%2520expressed%2520as%2520code.%2520We%2520demonstrate%2520both%250Aquantitatively%2520and%2520qualitatively%2520that%2520our%2520approach%2520is%2520capable%2520of%2520generating%2520new%250Aand%2520interesting%2520games%252C%2520including%2520in%2520regions%2520of%2520the%2520potential%2520rules%2520space%2520not%250Acovered%2520by%2520existing%2520games%2520in%2520the%2520Ludii%2520dataset.%2520A%2520sample%2520of%2520the%2520generated%2520games%250Aare%2520available%2520to%2520play%2520online%2520through%2520the%2520Ludii%2520portal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAVEL%3A%20Generating%20Games%20Via%20Evolution%20and%20Language%20Models&entry.906535625=Graham%20Todd%20and%20Alexander%20Padula%20and%20Matthew%20Stephenson%20and%20%C3%89ric%20Piette%20and%20Dennis%20J.%20N.%20J.%20Soemers%20and%20Julian%20Togelius&entry.1292438233=%20%20Automatically%20generating%20novel%20and%20interesting%20games%20is%20a%20complex%20task.%0AChallenges%20include%20representing%20game%20rules%20in%20a%20computationally%20workable%20form%2C%0Asearching%20through%20the%20large%20space%20of%20potential%20games%20under%20most%20such%0Arepresentations%2C%20and%20accurately%20evaluating%20the%20originality%20and%20quality%20of%0Apreviously%20unseen%20games.%20Prior%20work%20in%20automated%20game%20generation%20has%20largely%0Afocused%20on%20relatively%20restricted%20rule%20representations%20and%20relied%20on%0Adomain-specific%20heuristics.%20In%20this%20work%2C%20we%20explore%20the%20generation%20of%20novel%0Agames%20in%20the%20comparatively%20expansive%20Ludii%20game%20description%20language%2C%20which%0Aencodes%20the%20rules%20of%20over%201000%20board%20games%20in%20a%20variety%20of%20styles%20and%20modes%20of%0Aplay.%20We%20draw%20inspiration%20from%20recent%20advances%20in%20large%20language%20models%20and%0Aevolutionary%20computation%20in%20order%20to%20train%20a%20model%20that%20intelligently%20mutates%0Aand%20recombines%20games%20and%20mechanics%20expressed%20as%20code.%20We%20demonstrate%20both%0Aquantitatively%20and%20qualitatively%20that%20our%20approach%20is%20capable%20of%20generating%20new%0Aand%20interesting%20games%2C%20including%20in%20regions%20of%20the%20potential%20rules%20space%20not%0Acovered%20by%20existing%20games%20in%20the%20Ludii%20dataset.%20A%20sample%20of%20the%20generated%20games%0Aare%20available%20to%20play%20online%20through%20the%20Ludii%20portal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09388v1&entry.124074799=Read"},
{"title": "SE(3)-bi-equivariant Transformers for Point Cloud Assembly", "author": "Ziming Wang and Rebecka J\u00f6rnsten", "abstract": "  Given a pair of point clouds, the goal of assembly is to recover a rigid\ntransformation that aligns one point cloud to the other. This task is\nchallenging because the point clouds may be non-overlapped, and they may have\narbitrary initial positions. To address these difficulties, we propose a\nmethod, called SE(3)-bi-equivariant transformer (BITR), based on the\nSE(3)-bi-equivariance prior of the task: it guarantees that when the inputs are\nrigidly perturbed, the output will transform accordingly. Due to its\nequivariance property, BITR can not only handle non-overlapped PCs, but also\nguarantee robustness against initial positions. Specifically, BITR first\nextracts features of the inputs using a novel $SE(3) \\times SE(3)$-transformer,\nand then projects the learned feature to group SE(3) as the output. Moreover,\nwe theoretically show that swap and scale equivariances can be incorporated\ninto BITR, thus it further guarantees stable performance under scaling and\nswapping the inputs. We experimentally show the effectiveness of BITR in\npractical tasks.\n", "link": "http://arxiv.org/abs/2407.09167v1", "date": "2024-07-12", "relevancy": 2.6156, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5351}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5213}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SE%283%29-bi-equivariant%20Transformers%20for%20Point%20Cloud%20Assembly&body=Title%3A%20SE%283%29-bi-equivariant%20Transformers%20for%20Point%20Cloud%20Assembly%0AAuthor%3A%20Ziming%20Wang%20and%20Rebecka%20J%C3%B6rnsten%0AAbstract%3A%20%20%20Given%20a%20pair%20of%20point%20clouds%2C%20the%20goal%20of%20assembly%20is%20to%20recover%20a%20rigid%0Atransformation%20that%20aligns%20one%20point%20cloud%20to%20the%20other.%20This%20task%20is%0Achallenging%20because%20the%20point%20clouds%20may%20be%20non-overlapped%2C%20and%20they%20may%20have%0Aarbitrary%20initial%20positions.%20To%20address%20these%20difficulties%2C%20we%20propose%20a%0Amethod%2C%20called%20SE%283%29-bi-equivariant%20transformer%20%28BITR%29%2C%20based%20on%20the%0ASE%283%29-bi-equivariance%20prior%20of%20the%20task%3A%20it%20guarantees%20that%20when%20the%20inputs%20are%0Arigidly%20perturbed%2C%20the%20output%20will%20transform%20accordingly.%20Due%20to%20its%0Aequivariance%20property%2C%20BITR%20can%20not%20only%20handle%20non-overlapped%20PCs%2C%20but%20also%0Aguarantee%20robustness%20against%20initial%20positions.%20Specifically%2C%20BITR%20first%0Aextracts%20features%20of%20the%20inputs%20using%20a%20novel%20%24SE%283%29%20%5Ctimes%20SE%283%29%24-transformer%2C%0Aand%20then%20projects%20the%20learned%20feature%20to%20group%20SE%283%29%20as%20the%20output.%20Moreover%2C%0Awe%20theoretically%20show%20that%20swap%20and%20scale%20equivariances%20can%20be%20incorporated%0Ainto%20BITR%2C%20thus%20it%20further%20guarantees%20stable%20performance%20under%20scaling%20and%0Aswapping%20the%20inputs.%20We%20experimentally%20show%20the%20effectiveness%20of%20BITR%20in%0Apractical%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSE%25283%2529-bi-equivariant%2520Transformers%2520for%2520Point%2520Cloud%2520Assembly%26entry.906535625%3DZiming%2520Wang%2520and%2520Rebecka%2520J%25C3%25B6rnsten%26entry.1292438233%3D%2520%2520Given%2520a%2520pair%2520of%2520point%2520clouds%252C%2520the%2520goal%2520of%2520assembly%2520is%2520to%2520recover%2520a%2520rigid%250Atransformation%2520that%2520aligns%2520one%2520point%2520cloud%2520to%2520the%2520other.%2520This%2520task%2520is%250Achallenging%2520because%2520the%2520point%2520clouds%2520may%2520be%2520non-overlapped%252C%2520and%2520they%2520may%2520have%250Aarbitrary%2520initial%2520positions.%2520To%2520address%2520these%2520difficulties%252C%2520we%2520propose%2520a%250Amethod%252C%2520called%2520SE%25283%2529-bi-equivariant%2520transformer%2520%2528BITR%2529%252C%2520based%2520on%2520the%250ASE%25283%2529-bi-equivariance%2520prior%2520of%2520the%2520task%253A%2520it%2520guarantees%2520that%2520when%2520the%2520inputs%2520are%250Arigidly%2520perturbed%252C%2520the%2520output%2520will%2520transform%2520accordingly.%2520Due%2520to%2520its%250Aequivariance%2520property%252C%2520BITR%2520can%2520not%2520only%2520handle%2520non-overlapped%2520PCs%252C%2520but%2520also%250Aguarantee%2520robustness%2520against%2520initial%2520positions.%2520Specifically%252C%2520BITR%2520first%250Aextracts%2520features%2520of%2520the%2520inputs%2520using%2520a%2520novel%2520%2524SE%25283%2529%2520%255Ctimes%2520SE%25283%2529%2524-transformer%252C%250Aand%2520then%2520projects%2520the%2520learned%2520feature%2520to%2520group%2520SE%25283%2529%2520as%2520the%2520output.%2520Moreover%252C%250Awe%2520theoretically%2520show%2520that%2520swap%2520and%2520scale%2520equivariances%2520can%2520be%2520incorporated%250Ainto%2520BITR%252C%2520thus%2520it%2520further%2520guarantees%2520stable%2520performance%2520under%2520scaling%2520and%250Aswapping%2520the%2520inputs.%2520We%2520experimentally%2520show%2520the%2520effectiveness%2520of%2520BITR%2520in%250Apractical%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SE%283%29-bi-equivariant%20Transformers%20for%20Point%20Cloud%20Assembly&entry.906535625=Ziming%20Wang%20and%20Rebecka%20J%C3%B6rnsten&entry.1292438233=%20%20Given%20a%20pair%20of%20point%20clouds%2C%20the%20goal%20of%20assembly%20is%20to%20recover%20a%20rigid%0Atransformation%20that%20aligns%20one%20point%20cloud%20to%20the%20other.%20This%20task%20is%0Achallenging%20because%20the%20point%20clouds%20may%20be%20non-overlapped%2C%20and%20they%20may%20have%0Aarbitrary%20initial%20positions.%20To%20address%20these%20difficulties%2C%20we%20propose%20a%0Amethod%2C%20called%20SE%283%29-bi-equivariant%20transformer%20%28BITR%29%2C%20based%20on%20the%0ASE%283%29-bi-equivariance%20prior%20of%20the%20task%3A%20it%20guarantees%20that%20when%20the%20inputs%20are%0Arigidly%20perturbed%2C%20the%20output%20will%20transform%20accordingly.%20Due%20to%20its%0Aequivariance%20property%2C%20BITR%20can%20not%20only%20handle%20non-overlapped%20PCs%2C%20but%20also%0Aguarantee%20robustness%20against%20initial%20positions.%20Specifically%2C%20BITR%20first%0Aextracts%20features%20of%20the%20inputs%20using%20a%20novel%20%24SE%283%29%20%5Ctimes%20SE%283%29%24-transformer%2C%0Aand%20then%20projects%20the%20learned%20feature%20to%20group%20SE%283%29%20as%20the%20output.%20Moreover%2C%0Awe%20theoretically%20show%20that%20swap%20and%20scale%20equivariances%20can%20be%20incorporated%0Ainto%20BITR%2C%20thus%20it%20further%20guarantees%20stable%20performance%20under%20scaling%20and%0Aswapping%20the%20inputs.%20We%20experimentally%20show%20the%20effectiveness%20of%20BITR%20in%0Apractical%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09167v1&entry.124074799=Read"},
{"title": "NeuFair: Neural Network Fairness Repair with Dropout", "author": "Vishnu Asutosh Dasu and Ashish Kumar and Saeid Tizpaz-Niari and Gang Tan", "abstract": "  This paper investigates neuron dropout as a post-processing bias mitigation\nfor deep neural networks (DNNs). Neural-driven software solutions are\nincreasingly applied in socially critical domains with significant fairness\nimplications. While neural networks are exceptionally good at finding\nstatistical patterns from data, they may encode and amplify existing biases\nfrom the historical data. Existing bias mitigation algorithms often require\nmodifying the input dataset or the learning algorithms. We posit that the\nprevalent dropout methods that prevent over-fitting during training by randomly\ndropping neurons may be an effective and less intrusive approach to improve the\nfairness of pre-trained DNNs. However, finding the ideal set of neurons to drop\nis a combinatorial problem. We propose NeuFair, a family of post-processing\nrandomized algorithms that mitigate unfairness in pre-trained DNNs via dropouts\nduring inference after training. Our randomized search is guided by an\nobjective to minimize discrimination while maintaining the model's utility. We\nshow that our design of randomized algorithms is effective and efficient in\nimproving fairness (up to 69%) with minimal or no model performance\ndegradation. We provide intuitive explanations of these phenomena and carefully\nexamine the influence of various hyperparameters of search algorithms on the\nresults. Finally, we empirically and conceptually compare NeuFair to different\nstate-of-the-art bias mitigators.\n", "link": "http://arxiv.org/abs/2407.04268v2", "date": "2024-07-12", "relevancy": 2.607, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5663}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuFair%3A%20Neural%20Network%20Fairness%20Repair%20with%20Dropout&body=Title%3A%20NeuFair%3A%20Neural%20Network%20Fairness%20Repair%20with%20Dropout%0AAuthor%3A%20Vishnu%20Asutosh%20Dasu%20and%20Ashish%20Kumar%20and%20Saeid%20Tizpaz-Niari%20and%20Gang%20Tan%0AAbstract%3A%20%20%20This%20paper%20investigates%20neuron%20dropout%20as%20a%20post-processing%20bias%20mitigation%0Afor%20deep%20neural%20networks%20%28DNNs%29.%20Neural-driven%20software%20solutions%20are%0Aincreasingly%20applied%20in%20socially%20critical%20domains%20with%20significant%20fairness%0Aimplications.%20While%20neural%20networks%20are%20exceptionally%20good%20at%20finding%0Astatistical%20patterns%20from%20data%2C%20they%20may%20encode%20and%20amplify%20existing%20biases%0Afrom%20the%20historical%20data.%20Existing%20bias%20mitigation%20algorithms%20often%20require%0Amodifying%20the%20input%20dataset%20or%20the%20learning%20algorithms.%20We%20posit%20that%20the%0Aprevalent%20dropout%20methods%20that%20prevent%20over-fitting%20during%20training%20by%20randomly%0Adropping%20neurons%20may%20be%20an%20effective%20and%20less%20intrusive%20approach%20to%20improve%20the%0Afairness%20of%20pre-trained%20DNNs.%20However%2C%20finding%20the%20ideal%20set%20of%20neurons%20to%20drop%0Ais%20a%20combinatorial%20problem.%20We%20propose%20NeuFair%2C%20a%20family%20of%20post-processing%0Arandomized%20algorithms%20that%20mitigate%20unfairness%20in%20pre-trained%20DNNs%20via%20dropouts%0Aduring%20inference%20after%20training.%20Our%20randomized%20search%20is%20guided%20by%20an%0Aobjective%20to%20minimize%20discrimination%20while%20maintaining%20the%20model%27s%20utility.%20We%0Ashow%20that%20our%20design%20of%20randomized%20algorithms%20is%20effective%20and%20efficient%20in%0Aimproving%20fairness%20%28up%20to%2069%25%29%20with%20minimal%20or%20no%20model%20performance%0Adegradation.%20We%20provide%20intuitive%20explanations%20of%20these%20phenomena%20and%20carefully%0Aexamine%20the%20influence%20of%20various%20hyperparameters%20of%20search%20algorithms%20on%20the%0Aresults.%20Finally%2C%20we%20empirically%20and%20conceptually%20compare%20NeuFair%20to%20different%0Astate-of-the-art%20bias%20mitigators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuFair%253A%2520Neural%2520Network%2520Fairness%2520Repair%2520with%2520Dropout%26entry.906535625%3DVishnu%2520Asutosh%2520Dasu%2520and%2520Ashish%2520Kumar%2520and%2520Saeid%2520Tizpaz-Niari%2520and%2520Gang%2520Tan%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520neuron%2520dropout%2520as%2520a%2520post-processing%2520bias%2520mitigation%250Afor%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520Neural-driven%2520software%2520solutions%2520are%250Aincreasingly%2520applied%2520in%2520socially%2520critical%2520domains%2520with%2520significant%2520fairness%250Aimplications.%2520While%2520neural%2520networks%2520are%2520exceptionally%2520good%2520at%2520finding%250Astatistical%2520patterns%2520from%2520data%252C%2520they%2520may%2520encode%2520and%2520amplify%2520existing%2520biases%250Afrom%2520the%2520historical%2520data.%2520Existing%2520bias%2520mitigation%2520algorithms%2520often%2520require%250Amodifying%2520the%2520input%2520dataset%2520or%2520the%2520learning%2520algorithms.%2520We%2520posit%2520that%2520the%250Aprevalent%2520dropout%2520methods%2520that%2520prevent%2520over-fitting%2520during%2520training%2520by%2520randomly%250Adropping%2520neurons%2520may%2520be%2520an%2520effective%2520and%2520less%2520intrusive%2520approach%2520to%2520improve%2520the%250Afairness%2520of%2520pre-trained%2520DNNs.%2520However%252C%2520finding%2520the%2520ideal%2520set%2520of%2520neurons%2520to%2520drop%250Ais%2520a%2520combinatorial%2520problem.%2520We%2520propose%2520NeuFair%252C%2520a%2520family%2520of%2520post-processing%250Arandomized%2520algorithms%2520that%2520mitigate%2520unfairness%2520in%2520pre-trained%2520DNNs%2520via%2520dropouts%250Aduring%2520inference%2520after%2520training.%2520Our%2520randomized%2520search%2520is%2520guided%2520by%2520an%250Aobjective%2520to%2520minimize%2520discrimination%2520while%2520maintaining%2520the%2520model%2527s%2520utility.%2520We%250Ashow%2520that%2520our%2520design%2520of%2520randomized%2520algorithms%2520is%2520effective%2520and%2520efficient%2520in%250Aimproving%2520fairness%2520%2528up%2520to%252069%2525%2529%2520with%2520minimal%2520or%2520no%2520model%2520performance%250Adegradation.%2520We%2520provide%2520intuitive%2520explanations%2520of%2520these%2520phenomena%2520and%2520carefully%250Aexamine%2520the%2520influence%2520of%2520various%2520hyperparameters%2520of%2520search%2520algorithms%2520on%2520the%250Aresults.%2520Finally%252C%2520we%2520empirically%2520and%2520conceptually%2520compare%2520NeuFair%2520to%2520different%250Astate-of-the-art%2520bias%2520mitigators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuFair%3A%20Neural%20Network%20Fairness%20Repair%20with%20Dropout&entry.906535625=Vishnu%20Asutosh%20Dasu%20and%20Ashish%20Kumar%20and%20Saeid%20Tizpaz-Niari%20and%20Gang%20Tan&entry.1292438233=%20%20This%20paper%20investigates%20neuron%20dropout%20as%20a%20post-processing%20bias%20mitigation%0Afor%20deep%20neural%20networks%20%28DNNs%29.%20Neural-driven%20software%20solutions%20are%0Aincreasingly%20applied%20in%20socially%20critical%20domains%20with%20significant%20fairness%0Aimplications.%20While%20neural%20networks%20are%20exceptionally%20good%20at%20finding%0Astatistical%20patterns%20from%20data%2C%20they%20may%20encode%20and%20amplify%20existing%20biases%0Afrom%20the%20historical%20data.%20Existing%20bias%20mitigation%20algorithms%20often%20require%0Amodifying%20the%20input%20dataset%20or%20the%20learning%20algorithms.%20We%20posit%20that%20the%0Aprevalent%20dropout%20methods%20that%20prevent%20over-fitting%20during%20training%20by%20randomly%0Adropping%20neurons%20may%20be%20an%20effective%20and%20less%20intrusive%20approach%20to%20improve%20the%0Afairness%20of%20pre-trained%20DNNs.%20However%2C%20finding%20the%20ideal%20set%20of%20neurons%20to%20drop%0Ais%20a%20combinatorial%20problem.%20We%20propose%20NeuFair%2C%20a%20family%20of%20post-processing%0Arandomized%20algorithms%20that%20mitigate%20unfairness%20in%20pre-trained%20DNNs%20via%20dropouts%0Aduring%20inference%20after%20training.%20Our%20randomized%20search%20is%20guided%20by%20an%0Aobjective%20to%20minimize%20discrimination%20while%20maintaining%20the%20model%27s%20utility.%20We%0Ashow%20that%20our%20design%20of%20randomized%20algorithms%20is%20effective%20and%20efficient%20in%0Aimproving%20fairness%20%28up%20to%2069%25%29%20with%20minimal%20or%20no%20model%20performance%0Adegradation.%20We%20provide%20intuitive%20explanations%20of%20these%20phenomena%20and%20carefully%0Aexamine%20the%20influence%20of%20various%20hyperparameters%20of%20search%20algorithms%20on%20the%0Aresults.%20Finally%2C%20we%20empirically%20and%20conceptually%20compare%20NeuFair%20to%20different%0Astate-of-the-art%20bias%20mitigators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04268v2&entry.124074799=Read"},
{"title": "FairyLandAI: Personalized Fairy Tales utilizing ChatGPT and DALLE-3", "author": "Georgios Makridis and Athanasios Oikonomou and Vasileios Koukos", "abstract": "  In the diverse world of AI-driven storytelling, there is a unique opportunity\nto engage young audiences with customized, and personalized narratives. This\npaper introduces FairyLandAI an innovative Large Language Model (LLM) developed\nthrough OpenAI's API, specifically crafted to create personalized fairytales\nfor children. The distinctive feature of FairyLandAI is its dual capability: it\nnot only generates stories that are engaging, age-appropriate, and reflective\nof various traditions but also autonomously produces imaginative prompts\nsuitable for advanced image generation tools like GenAI and Dalle-3, thereby\nenriching the storytelling experience. FairyLandAI is expertly tailored to\nresonate with the imaginative worlds of children, providing narratives that are\nboth educational and entertaining and in alignment with the moral values\ninherent in different ages. Its unique strength lies in customizing stories to\nmatch individual children's preferences and cultural backgrounds, heralding a\nnew era in personalized storytelling. Further, its integration with image\ngeneration technology offers a comprehensive narrative experience that\nstimulates both verbal and visual creativity. Empirical evaluations of\nFairyLandAI demonstrate its effectiveness in crafting captivating stories for\nchildren, which not only entertain but also embody the values and teachings of\ndiverse traditions. This model serves as an invaluable tool for parents and\neducators, supporting them in imparting meaningful moral lessons through\nengaging narratives. FairyLandAI represents a pioneering step in using LLMs,\nparticularly through OpenAI's API, for educational and cultural enrichment,\nmaking complex moral narratives accessible and enjoyable for young, imaginative\nminds.\n", "link": "http://arxiv.org/abs/2407.09467v1", "date": "2024-07-12", "relevancy": 2.6029, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5534}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5147}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairyLandAI%3A%20Personalized%20Fairy%20Tales%20utilizing%20ChatGPT%20and%20DALLE-3&body=Title%3A%20FairyLandAI%3A%20Personalized%20Fairy%20Tales%20utilizing%20ChatGPT%20and%20DALLE-3%0AAuthor%3A%20Georgios%20Makridis%20and%20Athanasios%20Oikonomou%20and%20Vasileios%20Koukos%0AAbstract%3A%20%20%20In%20the%20diverse%20world%20of%20AI-driven%20storytelling%2C%20there%20is%20a%20unique%20opportunity%0Ato%20engage%20young%20audiences%20with%20customized%2C%20and%20personalized%20narratives.%20This%0Apaper%20introduces%20FairyLandAI%20an%20innovative%20Large%20Language%20Model%20%28LLM%29%20developed%0Athrough%20OpenAI%27s%20API%2C%20specifically%20crafted%20to%20create%20personalized%20fairytales%0Afor%20children.%20The%20distinctive%20feature%20of%20FairyLandAI%20is%20its%20dual%20capability%3A%20it%0Anot%20only%20generates%20stories%20that%20are%20engaging%2C%20age-appropriate%2C%20and%20reflective%0Aof%20various%20traditions%20but%20also%20autonomously%20produces%20imaginative%20prompts%0Asuitable%20for%20advanced%20image%20generation%20tools%20like%20GenAI%20and%20Dalle-3%2C%20thereby%0Aenriching%20the%20storytelling%20experience.%20FairyLandAI%20is%20expertly%20tailored%20to%0Aresonate%20with%20the%20imaginative%20worlds%20of%20children%2C%20providing%20narratives%20that%20are%0Aboth%20educational%20and%20entertaining%20and%20in%20alignment%20with%20the%20moral%20values%0Ainherent%20in%20different%20ages.%20Its%20unique%20strength%20lies%20in%20customizing%20stories%20to%0Amatch%20individual%20children%27s%20preferences%20and%20cultural%20backgrounds%2C%20heralding%20a%0Anew%20era%20in%20personalized%20storytelling.%20Further%2C%20its%20integration%20with%20image%0Ageneration%20technology%20offers%20a%20comprehensive%20narrative%20experience%20that%0Astimulates%20both%20verbal%20and%20visual%20creativity.%20Empirical%20evaluations%20of%0AFairyLandAI%20demonstrate%20its%20effectiveness%20in%20crafting%20captivating%20stories%20for%0Achildren%2C%20which%20not%20only%20entertain%20but%20also%20embody%20the%20values%20and%20teachings%20of%0Adiverse%20traditions.%20This%20model%20serves%20as%20an%20invaluable%20tool%20for%20parents%20and%0Aeducators%2C%20supporting%20them%20in%20imparting%20meaningful%20moral%20lessons%20through%0Aengaging%20narratives.%20FairyLandAI%20represents%20a%20pioneering%20step%20in%20using%20LLMs%2C%0Aparticularly%20through%20OpenAI%27s%20API%2C%20for%20educational%20and%20cultural%20enrichment%2C%0Amaking%20complex%20moral%20narratives%20accessible%20and%20enjoyable%20for%20young%2C%20imaginative%0Aminds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairyLandAI%253A%2520Personalized%2520Fairy%2520Tales%2520utilizing%2520ChatGPT%2520and%2520DALLE-3%26entry.906535625%3DGeorgios%2520Makridis%2520and%2520Athanasios%2520Oikonomou%2520and%2520Vasileios%2520Koukos%26entry.1292438233%3D%2520%2520In%2520the%2520diverse%2520world%2520of%2520AI-driven%2520storytelling%252C%2520there%2520is%2520a%2520unique%2520opportunity%250Ato%2520engage%2520young%2520audiences%2520with%2520customized%252C%2520and%2520personalized%2520narratives.%2520This%250Apaper%2520introduces%2520FairyLandAI%2520an%2520innovative%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520developed%250Athrough%2520OpenAI%2527s%2520API%252C%2520specifically%2520crafted%2520to%2520create%2520personalized%2520fairytales%250Afor%2520children.%2520The%2520distinctive%2520feature%2520of%2520FairyLandAI%2520is%2520its%2520dual%2520capability%253A%2520it%250Anot%2520only%2520generates%2520stories%2520that%2520are%2520engaging%252C%2520age-appropriate%252C%2520and%2520reflective%250Aof%2520various%2520traditions%2520but%2520also%2520autonomously%2520produces%2520imaginative%2520prompts%250Asuitable%2520for%2520advanced%2520image%2520generation%2520tools%2520like%2520GenAI%2520and%2520Dalle-3%252C%2520thereby%250Aenriching%2520the%2520storytelling%2520experience.%2520FairyLandAI%2520is%2520expertly%2520tailored%2520to%250Aresonate%2520with%2520the%2520imaginative%2520worlds%2520of%2520children%252C%2520providing%2520narratives%2520that%2520are%250Aboth%2520educational%2520and%2520entertaining%2520and%2520in%2520alignment%2520with%2520the%2520moral%2520values%250Ainherent%2520in%2520different%2520ages.%2520Its%2520unique%2520strength%2520lies%2520in%2520customizing%2520stories%2520to%250Amatch%2520individual%2520children%2527s%2520preferences%2520and%2520cultural%2520backgrounds%252C%2520heralding%2520a%250Anew%2520era%2520in%2520personalized%2520storytelling.%2520Further%252C%2520its%2520integration%2520with%2520image%250Ageneration%2520technology%2520offers%2520a%2520comprehensive%2520narrative%2520experience%2520that%250Astimulates%2520both%2520verbal%2520and%2520visual%2520creativity.%2520Empirical%2520evaluations%2520of%250AFairyLandAI%2520demonstrate%2520its%2520effectiveness%2520in%2520crafting%2520captivating%2520stories%2520for%250Achildren%252C%2520which%2520not%2520only%2520entertain%2520but%2520also%2520embody%2520the%2520values%2520and%2520teachings%2520of%250Adiverse%2520traditions.%2520This%2520model%2520serves%2520as%2520an%2520invaluable%2520tool%2520for%2520parents%2520and%250Aeducators%252C%2520supporting%2520them%2520in%2520imparting%2520meaningful%2520moral%2520lessons%2520through%250Aengaging%2520narratives.%2520FairyLandAI%2520represents%2520a%2520pioneering%2520step%2520in%2520using%2520LLMs%252C%250Aparticularly%2520through%2520OpenAI%2527s%2520API%252C%2520for%2520educational%2520and%2520cultural%2520enrichment%252C%250Amaking%2520complex%2520moral%2520narratives%2520accessible%2520and%2520enjoyable%2520for%2520young%252C%2520imaginative%250Aminds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairyLandAI%3A%20Personalized%20Fairy%20Tales%20utilizing%20ChatGPT%20and%20DALLE-3&entry.906535625=Georgios%20Makridis%20and%20Athanasios%20Oikonomou%20and%20Vasileios%20Koukos&entry.1292438233=%20%20In%20the%20diverse%20world%20of%20AI-driven%20storytelling%2C%20there%20is%20a%20unique%20opportunity%0Ato%20engage%20young%20audiences%20with%20customized%2C%20and%20personalized%20narratives.%20This%0Apaper%20introduces%20FairyLandAI%20an%20innovative%20Large%20Language%20Model%20%28LLM%29%20developed%0Athrough%20OpenAI%27s%20API%2C%20specifically%20crafted%20to%20create%20personalized%20fairytales%0Afor%20children.%20The%20distinctive%20feature%20of%20FairyLandAI%20is%20its%20dual%20capability%3A%20it%0Anot%20only%20generates%20stories%20that%20are%20engaging%2C%20age-appropriate%2C%20and%20reflective%0Aof%20various%20traditions%20but%20also%20autonomously%20produces%20imaginative%20prompts%0Asuitable%20for%20advanced%20image%20generation%20tools%20like%20GenAI%20and%20Dalle-3%2C%20thereby%0Aenriching%20the%20storytelling%20experience.%20FairyLandAI%20is%20expertly%20tailored%20to%0Aresonate%20with%20the%20imaginative%20worlds%20of%20children%2C%20providing%20narratives%20that%20are%0Aboth%20educational%20and%20entertaining%20and%20in%20alignment%20with%20the%20moral%20values%0Ainherent%20in%20different%20ages.%20Its%20unique%20strength%20lies%20in%20customizing%20stories%20to%0Amatch%20individual%20children%27s%20preferences%20and%20cultural%20backgrounds%2C%20heralding%20a%0Anew%20era%20in%20personalized%20storytelling.%20Further%2C%20its%20integration%20with%20image%0Ageneration%20technology%20offers%20a%20comprehensive%20narrative%20experience%20that%0Astimulates%20both%20verbal%20and%20visual%20creativity.%20Empirical%20evaluations%20of%0AFairyLandAI%20demonstrate%20its%20effectiveness%20in%20crafting%20captivating%20stories%20for%0Achildren%2C%20which%20not%20only%20entertain%20but%20also%20embody%20the%20values%20and%20teachings%20of%0Adiverse%20traditions.%20This%20model%20serves%20as%20an%20invaluable%20tool%20for%20parents%20and%0Aeducators%2C%20supporting%20them%20in%20imparting%20meaningful%20moral%20lessons%20through%0Aengaging%20narratives.%20FairyLandAI%20represents%20a%20pioneering%20step%20in%20using%20LLMs%2C%0Aparticularly%20through%20OpenAI%27s%20API%2C%20for%20educational%20and%20cultural%20enrichment%2C%0Amaking%20complex%20moral%20narratives%20accessible%20and%20enjoyable%20for%20young%2C%20imaginative%0Aminds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09467v1&entry.124074799=Read"},
{"title": "Pre-training Point Cloud Compact Model with Partial-aware Reconstruction", "author": "Yaohua Zha and Yanzi Wang and Tao Dai and Shu-Tao Xia", "abstract": "  The pre-trained point cloud model based on Masked Point Modeling (MPM) has\nexhibited substantial improvements across various tasks. However, two drawbacks\nhinder their practical application. Firstly, the positional embedding of masked\npatches in the decoder results in the leakage of their central coordinates,\nleading to limited 3D representations. Secondly, the excessive model size of\nexisting MPM methods results in higher demands for devices. To address these,\nwe propose to pre-train Point cloud Compact Model with Partial-aware\n\\textbf{R}econstruction, named Point-CPR. Specifically, in the decoder, we\ncouple the vanilla masked tokens with their positional embeddings as randomly\nmasked queries and introduce a partial-aware prediction module before each\ndecoder layer to predict them from the unmasked partial. It prevents the\ndecoder from creating a shortcut between the central coordinates of masked\npatches and their reconstructed coordinates, enhancing the robustness of\nmodels. We also devise a compact encoder composed of local aggregation and\nMLPs, reducing the parameters and computational requirements compared to\nexisting Transformer-based encoders. Extensive experiments demonstrate that our\nmodel exhibits strong performance across various tasks, especially surpassing\nthe leading MPM-based model PointGPT-B with only 2% of its parameters.\n", "link": "http://arxiv.org/abs/2407.09344v1", "date": "2024-07-12", "relevancy": 2.5982, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5427}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5172}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-training%20Point%20Cloud%20Compact%20Model%20with%20Partial-aware%20Reconstruction&body=Title%3A%20Pre-training%20Point%20Cloud%20Compact%20Model%20with%20Partial-aware%20Reconstruction%0AAuthor%3A%20Yaohua%20Zha%20and%20Yanzi%20Wang%20and%20Tao%20Dai%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20The%20pre-trained%20point%20cloud%20model%20based%20on%20Masked%20Point%20Modeling%20%28MPM%29%20has%0Aexhibited%20substantial%20improvements%20across%20various%20tasks.%20However%2C%20two%20drawbacks%0Ahinder%20their%20practical%20application.%20Firstly%2C%20the%20positional%20embedding%20of%20masked%0Apatches%20in%20the%20decoder%20results%20in%20the%20leakage%20of%20their%20central%20coordinates%2C%0Aleading%20to%20limited%203D%20representations.%20Secondly%2C%20the%20excessive%20model%20size%20of%0Aexisting%20MPM%20methods%20results%20in%20higher%20demands%20for%20devices.%20To%20address%20these%2C%0Awe%20propose%20to%20pre-train%20Point%20cloud%20Compact%20Model%20with%20Partial-aware%0A%5Ctextbf%7BR%7Deconstruction%2C%20named%20Point-CPR.%20Specifically%2C%20in%20the%20decoder%2C%20we%0Acouple%20the%20vanilla%20masked%20tokens%20with%20their%20positional%20embeddings%20as%20randomly%0Amasked%20queries%20and%20introduce%20a%20partial-aware%20prediction%20module%20before%20each%0Adecoder%20layer%20to%20predict%20them%20from%20the%20unmasked%20partial.%20It%20prevents%20the%0Adecoder%20from%20creating%20a%20shortcut%20between%20the%20central%20coordinates%20of%20masked%0Apatches%20and%20their%20reconstructed%20coordinates%2C%20enhancing%20the%20robustness%20of%0Amodels.%20We%20also%20devise%20a%20compact%20encoder%20composed%20of%20local%20aggregation%20and%0AMLPs%2C%20reducing%20the%20parameters%20and%20computational%20requirements%20compared%20to%0Aexisting%20Transformer-based%20encoders.%20Extensive%20experiments%20demonstrate%20that%20our%0Amodel%20exhibits%20strong%20performance%20across%20various%20tasks%2C%20especially%20surpassing%0Athe%20leading%20MPM-based%20model%20PointGPT-B%20with%20only%202%25%20of%20its%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-training%2520Point%2520Cloud%2520Compact%2520Model%2520with%2520Partial-aware%2520Reconstruction%26entry.906535625%3DYaohua%2520Zha%2520and%2520Yanzi%2520Wang%2520and%2520Tao%2520Dai%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520The%2520pre-trained%2520point%2520cloud%2520model%2520based%2520on%2520Masked%2520Point%2520Modeling%2520%2528MPM%2529%2520has%250Aexhibited%2520substantial%2520improvements%2520across%2520various%2520tasks.%2520However%252C%2520two%2520drawbacks%250Ahinder%2520their%2520practical%2520application.%2520Firstly%252C%2520the%2520positional%2520embedding%2520of%2520masked%250Apatches%2520in%2520the%2520decoder%2520results%2520in%2520the%2520leakage%2520of%2520their%2520central%2520coordinates%252C%250Aleading%2520to%2520limited%25203D%2520representations.%2520Secondly%252C%2520the%2520excessive%2520model%2520size%2520of%250Aexisting%2520MPM%2520methods%2520results%2520in%2520higher%2520demands%2520for%2520devices.%2520To%2520address%2520these%252C%250Awe%2520propose%2520to%2520pre-train%2520Point%2520cloud%2520Compact%2520Model%2520with%2520Partial-aware%250A%255Ctextbf%257BR%257Deconstruction%252C%2520named%2520Point-CPR.%2520Specifically%252C%2520in%2520the%2520decoder%252C%2520we%250Acouple%2520the%2520vanilla%2520masked%2520tokens%2520with%2520their%2520positional%2520embeddings%2520as%2520randomly%250Amasked%2520queries%2520and%2520introduce%2520a%2520partial-aware%2520prediction%2520module%2520before%2520each%250Adecoder%2520layer%2520to%2520predict%2520them%2520from%2520the%2520unmasked%2520partial.%2520It%2520prevents%2520the%250Adecoder%2520from%2520creating%2520a%2520shortcut%2520between%2520the%2520central%2520coordinates%2520of%2520masked%250Apatches%2520and%2520their%2520reconstructed%2520coordinates%252C%2520enhancing%2520the%2520robustness%2520of%250Amodels.%2520We%2520also%2520devise%2520a%2520compact%2520encoder%2520composed%2520of%2520local%2520aggregation%2520and%250AMLPs%252C%2520reducing%2520the%2520parameters%2520and%2520computational%2520requirements%2520compared%2520to%250Aexisting%2520Transformer-based%2520encoders.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amodel%2520exhibits%2520strong%2520performance%2520across%2520various%2520tasks%252C%2520especially%2520surpassing%250Athe%2520leading%2520MPM-based%2520model%2520PointGPT-B%2520with%2520only%25202%2525%2520of%2520its%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-training%20Point%20Cloud%20Compact%20Model%20with%20Partial-aware%20Reconstruction&entry.906535625=Yaohua%20Zha%20and%20Yanzi%20Wang%20and%20Tao%20Dai%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20The%20pre-trained%20point%20cloud%20model%20based%20on%20Masked%20Point%20Modeling%20%28MPM%29%20has%0Aexhibited%20substantial%20improvements%20across%20various%20tasks.%20However%2C%20two%20drawbacks%0Ahinder%20their%20practical%20application.%20Firstly%2C%20the%20positional%20embedding%20of%20masked%0Apatches%20in%20the%20decoder%20results%20in%20the%20leakage%20of%20their%20central%20coordinates%2C%0Aleading%20to%20limited%203D%20representations.%20Secondly%2C%20the%20excessive%20model%20size%20of%0Aexisting%20MPM%20methods%20results%20in%20higher%20demands%20for%20devices.%20To%20address%20these%2C%0Awe%20propose%20to%20pre-train%20Point%20cloud%20Compact%20Model%20with%20Partial-aware%0A%5Ctextbf%7BR%7Deconstruction%2C%20named%20Point-CPR.%20Specifically%2C%20in%20the%20decoder%2C%20we%0Acouple%20the%20vanilla%20masked%20tokens%20with%20their%20positional%20embeddings%20as%20randomly%0Amasked%20queries%20and%20introduce%20a%20partial-aware%20prediction%20module%20before%20each%0Adecoder%20layer%20to%20predict%20them%20from%20the%20unmasked%20partial.%20It%20prevents%20the%0Adecoder%20from%20creating%20a%20shortcut%20between%20the%20central%20coordinates%20of%20masked%0Apatches%20and%20their%20reconstructed%20coordinates%2C%20enhancing%20the%20robustness%20of%0Amodels.%20We%20also%20devise%20a%20compact%20encoder%20composed%20of%20local%20aggregation%20and%0AMLPs%2C%20reducing%20the%20parameters%20and%20computational%20requirements%20compared%20to%0Aexisting%20Transformer-based%20encoders.%20Extensive%20experiments%20demonstrate%20that%20our%0Amodel%20exhibits%20strong%20performance%20across%20various%20tasks%2C%20especially%20surpassing%0Athe%20leading%20MPM-based%20model%20PointGPT-B%20with%20only%202%25%20of%20its%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09344v1&entry.124074799=Read"},
{"title": "Soft Prompt Generation for Domain Generalization", "author": "Shuanghao Bai and Yuedi Zhang and Wanqi Zhou and Zhirong Luan and Badong Chen", "abstract": "  Large pre-trained vision language models (VLMs) have shown impressive\nzero-shot ability on downstream tasks with manually designed prompt. To further\nadapt VLMs to downstream tasks, soft prompt is proposed to replace manually\ndesigned prompt, which undergoes fine-tuning based on specific domain data.\nPrior prompt learning methods primarily learn a fixed prompt or residuled\nprompt from training samples. However, the learned prompts lack diversity and\nignore information about unseen domains. In this paper, we reframe the prompt\nlearning framework from a generative perspective and propose a simple yet\nefficient method for the Domain Generalization (DG) task, namely Soft Prompt\nGeneration (SPG). Specifically, SPG consists of a two-stage training phase and\nan inference phase. During the training phase, we introduce soft prompt label\nfor each domain, aiming to incorporate the generative model domain knowledge.\nDuring the inference phase, the generator of the generative model is employed\nto obtain instance-specific soft prompts for the unseen target domain.\nExtensive experiments on five domain generalization benchmarks of three DG\ntasks demonstrate that SPG achieves state-of-the-art performance. The code is\navailable at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\n", "link": "http://arxiv.org/abs/2404.19286v2", "date": "2024-07-12", "relevancy": 2.5703, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5533}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5034}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Prompt%20Generation%20for%20Domain%20Generalization&body=Title%3A%20Soft%20Prompt%20Generation%20for%20Domain%20Generalization%0AAuthor%3A%20Shuanghao%20Bai%20and%20Yuedi%20Zhang%20and%20Wanqi%20Zhou%20and%20Zhirong%20Luan%20and%20Badong%20Chen%0AAbstract%3A%20%20%20Large%20pre-trained%20vision%20language%20models%20%28VLMs%29%20have%20shown%20impressive%0Azero-shot%20ability%20on%20downstream%20tasks%20with%20manually%20designed%20prompt.%20To%20further%0Aadapt%20VLMs%20to%20downstream%20tasks%2C%20soft%20prompt%20is%20proposed%20to%20replace%20manually%0Adesigned%20prompt%2C%20which%20undergoes%20fine-tuning%20based%20on%20specific%20domain%20data.%0APrior%20prompt%20learning%20methods%20primarily%20learn%20a%20fixed%20prompt%20or%20residuled%0Aprompt%20from%20training%20samples.%20However%2C%20the%20learned%20prompts%20lack%20diversity%20and%0Aignore%20information%20about%20unseen%20domains.%20In%20this%20paper%2C%20we%20reframe%20the%20prompt%0Alearning%20framework%20from%20a%20generative%20perspective%20and%20propose%20a%20simple%20yet%0Aefficient%20method%20for%20the%20Domain%20Generalization%20%28DG%29%20task%2C%20namely%20Soft%20Prompt%0AGeneration%20%28SPG%29.%20Specifically%2C%20SPG%20consists%20of%20a%20two-stage%20training%20phase%20and%0Aan%20inference%20phase.%20During%20the%20training%20phase%2C%20we%20introduce%20soft%20prompt%20label%0Afor%20each%20domain%2C%20aiming%20to%20incorporate%20the%20generative%20model%20domain%20knowledge.%0ADuring%20the%20inference%20phase%2C%20the%20generator%20of%20the%20generative%20model%20is%20employed%0Ato%20obtain%20instance-specific%20soft%20prompts%20for%20the%20unseen%20target%20domain.%0AExtensive%20experiments%20on%20five%20domain%20generalization%20benchmarks%20of%20three%20DG%0Atasks%20demonstrate%20that%20SPG%20achieves%20state-of-the-art%20performance.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/renytek13/Soft-Prompt-Generation-with-CGAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19286v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Prompt%2520Generation%2520for%2520Domain%2520Generalization%26entry.906535625%3DShuanghao%2520Bai%2520and%2520Yuedi%2520Zhang%2520and%2520Wanqi%2520Zhou%2520and%2520Zhirong%2520Luan%2520and%2520Badong%2520Chen%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520impressive%250Azero-shot%2520ability%2520on%2520downstream%2520tasks%2520with%2520manually%2520designed%2520prompt.%2520To%2520further%250Aadapt%2520VLMs%2520to%2520downstream%2520tasks%252C%2520soft%2520prompt%2520is%2520proposed%2520to%2520replace%2520manually%250Adesigned%2520prompt%252C%2520which%2520undergoes%2520fine-tuning%2520based%2520on%2520specific%2520domain%2520data.%250APrior%2520prompt%2520learning%2520methods%2520primarily%2520learn%2520a%2520fixed%2520prompt%2520or%2520residuled%250Aprompt%2520from%2520training%2520samples.%2520However%252C%2520the%2520learned%2520prompts%2520lack%2520diversity%2520and%250Aignore%2520information%2520about%2520unseen%2520domains.%2520In%2520this%2520paper%252C%2520we%2520reframe%2520the%2520prompt%250Alearning%2520framework%2520from%2520a%2520generative%2520perspective%2520and%2520propose%2520a%2520simple%2520yet%250Aefficient%2520method%2520for%2520the%2520Domain%2520Generalization%2520%2528DG%2529%2520task%252C%2520namely%2520Soft%2520Prompt%250AGeneration%2520%2528SPG%2529.%2520Specifically%252C%2520SPG%2520consists%2520of%2520a%2520two-stage%2520training%2520phase%2520and%250Aan%2520inference%2520phase.%2520During%2520the%2520training%2520phase%252C%2520we%2520introduce%2520soft%2520prompt%2520label%250Afor%2520each%2520domain%252C%2520aiming%2520to%2520incorporate%2520the%2520generative%2520model%2520domain%2520knowledge.%250ADuring%2520the%2520inference%2520phase%252C%2520the%2520generator%2520of%2520the%2520generative%2520model%2520is%2520employed%250Ato%2520obtain%2520instance-specific%2520soft%2520prompts%2520for%2520the%2520unseen%2520target%2520domain.%250AExtensive%2520experiments%2520on%2520five%2520domain%2520generalization%2520benchmarks%2520of%2520three%2520DG%250Atasks%2520demonstrate%2520that%2520SPG%2520achieves%2520state-of-the-art%2520performance.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/renytek13/Soft-Prompt-Generation-with-CGAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19286v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Prompt%20Generation%20for%20Domain%20Generalization&entry.906535625=Shuanghao%20Bai%20and%20Yuedi%20Zhang%20and%20Wanqi%20Zhou%20and%20Zhirong%20Luan%20and%20Badong%20Chen&entry.1292438233=%20%20Large%20pre-trained%20vision%20language%20models%20%28VLMs%29%20have%20shown%20impressive%0Azero-shot%20ability%20on%20downstream%20tasks%20with%20manually%20designed%20prompt.%20To%20further%0Aadapt%20VLMs%20to%20downstream%20tasks%2C%20soft%20prompt%20is%20proposed%20to%20replace%20manually%0Adesigned%20prompt%2C%20which%20undergoes%20fine-tuning%20based%20on%20specific%20domain%20data.%0APrior%20prompt%20learning%20methods%20primarily%20learn%20a%20fixed%20prompt%20or%20residuled%0Aprompt%20from%20training%20samples.%20However%2C%20the%20learned%20prompts%20lack%20diversity%20and%0Aignore%20information%20about%20unseen%20domains.%20In%20this%20paper%2C%20we%20reframe%20the%20prompt%0Alearning%20framework%20from%20a%20generative%20perspective%20and%20propose%20a%20simple%20yet%0Aefficient%20method%20for%20the%20Domain%20Generalization%20%28DG%29%20task%2C%20namely%20Soft%20Prompt%0AGeneration%20%28SPG%29.%20Specifically%2C%20SPG%20consists%20of%20a%20two-stage%20training%20phase%20and%0Aan%20inference%20phase.%20During%20the%20training%20phase%2C%20we%20introduce%20soft%20prompt%20label%0Afor%20each%20domain%2C%20aiming%20to%20incorporate%20the%20generative%20model%20domain%20knowledge.%0ADuring%20the%20inference%20phase%2C%20the%20generator%20of%20the%20generative%20model%20is%20employed%0Ato%20obtain%20instance-specific%20soft%20prompts%20for%20the%20unseen%20target%20domain.%0AExtensive%20experiments%20on%20five%20domain%20generalization%20benchmarks%20of%20three%20DG%0Atasks%20demonstrate%20that%20SPG%20achieves%20state-of-the-art%20performance.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/renytek13/Soft-Prompt-Generation-with-CGAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19286v2&entry.124074799=Read"},
{"title": "Machine Apophenia: The Kaleidoscopic Generation of Architectural Images", "author": "Alexey Tikhonov and Dmitry Sinyavin", "abstract": "  This study investigates the application of generative artificial intelligence\nin architectural design. We present a novel methodology that combines multiple\nneural networks to create an unsupervised and unmoderated stream of unique\narchitectural images. Our approach is grounded in the conceptual framework\ncalled machine apophenia. We hypothesize that neural networks, trained on\ndiverse human-generated data, internalize aesthetic preferences and tend to\nproduce coherent designs even from random inputs. The methodology involves an\niterative process of image generation, description, and refinement, resulting\nin captioned architectural postcards automatically shared on several social\nmedia platforms. Evaluation and ablation studies show the improvement both in\ntechnical and aesthetic metrics of resulting images on each step.\n", "link": "http://arxiv.org/abs/2407.09172v1", "date": "2024-07-12", "relevancy": 2.5517, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5305}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.503}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Apophenia%3A%20The%20Kaleidoscopic%20Generation%20of%20Architectural%20Images&body=Title%3A%20Machine%20Apophenia%3A%20The%20Kaleidoscopic%20Generation%20of%20Architectural%20Images%0AAuthor%3A%20Alexey%20Tikhonov%20and%20Dmitry%20Sinyavin%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20application%20of%20generative%20artificial%20intelligence%0Ain%20architectural%20design.%20We%20present%20a%20novel%20methodology%20that%20combines%20multiple%0Aneural%20networks%20to%20create%20an%20unsupervised%20and%20unmoderated%20stream%20of%20unique%0Aarchitectural%20images.%20Our%20approach%20is%20grounded%20in%20the%20conceptual%20framework%0Acalled%20machine%20apophenia.%20We%20hypothesize%20that%20neural%20networks%2C%20trained%20on%0Adiverse%20human-generated%20data%2C%20internalize%20aesthetic%20preferences%20and%20tend%20to%0Aproduce%20coherent%20designs%20even%20from%20random%20inputs.%20The%20methodology%20involves%20an%0Aiterative%20process%20of%20image%20generation%2C%20description%2C%20and%20refinement%2C%20resulting%0Ain%20captioned%20architectural%20postcards%20automatically%20shared%20on%20several%20social%0Amedia%20platforms.%20Evaluation%20and%20ablation%20studies%20show%20the%20improvement%20both%20in%0Atechnical%20and%20aesthetic%20metrics%20of%20resulting%20images%20on%20each%20step.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Apophenia%253A%2520The%2520Kaleidoscopic%2520Generation%2520of%2520Architectural%2520Images%26entry.906535625%3DAlexey%2520Tikhonov%2520and%2520Dmitry%2520Sinyavin%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520application%2520of%2520generative%2520artificial%2520intelligence%250Ain%2520architectural%2520design.%2520We%2520present%2520a%2520novel%2520methodology%2520that%2520combines%2520multiple%250Aneural%2520networks%2520to%2520create%2520an%2520unsupervised%2520and%2520unmoderated%2520stream%2520of%2520unique%250Aarchitectural%2520images.%2520Our%2520approach%2520is%2520grounded%2520in%2520the%2520conceptual%2520framework%250Acalled%2520machine%2520apophenia.%2520We%2520hypothesize%2520that%2520neural%2520networks%252C%2520trained%2520on%250Adiverse%2520human-generated%2520data%252C%2520internalize%2520aesthetic%2520preferences%2520and%2520tend%2520to%250Aproduce%2520coherent%2520designs%2520even%2520from%2520random%2520inputs.%2520The%2520methodology%2520involves%2520an%250Aiterative%2520process%2520of%2520image%2520generation%252C%2520description%252C%2520and%2520refinement%252C%2520resulting%250Ain%2520captioned%2520architectural%2520postcards%2520automatically%2520shared%2520on%2520several%2520social%250Amedia%2520platforms.%2520Evaluation%2520and%2520ablation%2520studies%2520show%2520the%2520improvement%2520both%2520in%250Atechnical%2520and%2520aesthetic%2520metrics%2520of%2520resulting%2520images%2520on%2520each%2520step.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Apophenia%3A%20The%20Kaleidoscopic%20Generation%20of%20Architectural%20Images&entry.906535625=Alexey%20Tikhonov%20and%20Dmitry%20Sinyavin&entry.1292438233=%20%20This%20study%20investigates%20the%20application%20of%20generative%20artificial%20intelligence%0Ain%20architectural%20design.%20We%20present%20a%20novel%20methodology%20that%20combines%20multiple%0Aneural%20networks%20to%20create%20an%20unsupervised%20and%20unmoderated%20stream%20of%20unique%0Aarchitectural%20images.%20Our%20approach%20is%20grounded%20in%20the%20conceptual%20framework%0Acalled%20machine%20apophenia.%20We%20hypothesize%20that%20neural%20networks%2C%20trained%20on%0Adiverse%20human-generated%20data%2C%20internalize%20aesthetic%20preferences%20and%20tend%20to%0Aproduce%20coherent%20designs%20even%20from%20random%20inputs.%20The%20methodology%20involves%20an%0Aiterative%20process%20of%20image%20generation%2C%20description%2C%20and%20refinement%2C%20resulting%0Ain%20captioned%20architectural%20postcards%20automatically%20shared%20on%20several%20social%0Amedia%20platforms.%20Evaluation%20and%20ablation%20studies%20show%20the%20improvement%20both%20in%0Atechnical%20and%20aesthetic%20metrics%20of%20resulting%20images%20on%20each%20step.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09172v1&entry.124074799=Read"},
{"title": "FANet: Feature Amplification Network for Semantic Segmentation in\n  Cluttered Background", "author": "Muhammad Ali and Mamoona Javaid and Mubashir Noman and Mustansar Fiaz and Salman Khan", "abstract": "  Existing deep learning approaches leave out the semantic cues that are\ncrucial in semantic segmentation present in complex scenarios including\ncluttered backgrounds and translucent objects, etc. To handle these challenges,\nwe propose a feature amplification network (FANet) as a backbone network that\nincorporates semantic information using a novel feature enhancement module at\nmulti-stages. To achieve this, we propose an adaptive feature enhancement (AFE)\nblock that benefits from both a spatial context module (SCM) and a feature\nrefinement module (FRM) in a parallel fashion. SCM aims to exploit larger\nkernel leverages for the increased receptive field to handle scale variations\nin the scene. Whereas our novel FRM is responsible for generating semantic cues\nthat can capture both low-frequency and high-frequency regions for better\nsegmentation tasks. We perform experiments over challenging real-world\nZeroWaste-f dataset which contains background-cluttered and translucent\nobjects. Our experimental results demonstrate the state-of-the-art performance\ncompared to existing methods.\n", "link": "http://arxiv.org/abs/2407.09379v1", "date": "2024-07-12", "relevancy": 2.511, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5044}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FANet%3A%20Feature%20Amplification%20Network%20for%20Semantic%20Segmentation%20in%0A%20%20Cluttered%20Background&body=Title%3A%20FANet%3A%20Feature%20Amplification%20Network%20for%20Semantic%20Segmentation%20in%0A%20%20Cluttered%20Background%0AAuthor%3A%20Muhammad%20Ali%20and%20Mamoona%20Javaid%20and%20Mubashir%20Noman%20and%20Mustansar%20Fiaz%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Existing%20deep%20learning%20approaches%20leave%20out%20the%20semantic%20cues%20that%20are%0Acrucial%20in%20semantic%20segmentation%20present%20in%20complex%20scenarios%20including%0Acluttered%20backgrounds%20and%20translucent%20objects%2C%20etc.%20To%20handle%20these%20challenges%2C%0Awe%20propose%20a%20feature%20amplification%20network%20%28FANet%29%20as%20a%20backbone%20network%20that%0Aincorporates%20semantic%20information%20using%20a%20novel%20feature%20enhancement%20module%20at%0Amulti-stages.%20To%20achieve%20this%2C%20we%20propose%20an%20adaptive%20feature%20enhancement%20%28AFE%29%0Ablock%20that%20benefits%20from%20both%20a%20spatial%20context%20module%20%28SCM%29%20and%20a%20feature%0Arefinement%20module%20%28FRM%29%20in%20a%20parallel%20fashion.%20SCM%20aims%20to%20exploit%20larger%0Akernel%20leverages%20for%20the%20increased%20receptive%20field%20to%20handle%20scale%20variations%0Ain%20the%20scene.%20Whereas%20our%20novel%20FRM%20is%20responsible%20for%20generating%20semantic%20cues%0Athat%20can%20capture%20both%20low-frequency%20and%20high-frequency%20regions%20for%20better%0Asegmentation%20tasks.%20We%20perform%20experiments%20over%20challenging%20real-world%0AZeroWaste-f%20dataset%20which%20contains%20background-cluttered%20and%20translucent%0Aobjects.%20Our%20experimental%20results%20demonstrate%20the%20state-of-the-art%20performance%0Acompared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFANet%253A%2520Feature%2520Amplification%2520Network%2520for%2520Semantic%2520Segmentation%2520in%250A%2520%2520Cluttered%2520Background%26entry.906535625%3DMuhammad%2520Ali%2520and%2520Mamoona%2520Javaid%2520and%2520Mubashir%2520Noman%2520and%2520Mustansar%2520Fiaz%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Existing%2520deep%2520learning%2520approaches%2520leave%2520out%2520the%2520semantic%2520cues%2520that%2520are%250Acrucial%2520in%2520semantic%2520segmentation%2520present%2520in%2520complex%2520scenarios%2520including%250Acluttered%2520backgrounds%2520and%2520translucent%2520objects%252C%2520etc.%2520To%2520handle%2520these%2520challenges%252C%250Awe%2520propose%2520a%2520feature%2520amplification%2520network%2520%2528FANet%2529%2520as%2520a%2520backbone%2520network%2520that%250Aincorporates%2520semantic%2520information%2520using%2520a%2520novel%2520feature%2520enhancement%2520module%2520at%250Amulti-stages.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520an%2520adaptive%2520feature%2520enhancement%2520%2528AFE%2529%250Ablock%2520that%2520benefits%2520from%2520both%2520a%2520spatial%2520context%2520module%2520%2528SCM%2529%2520and%2520a%2520feature%250Arefinement%2520module%2520%2528FRM%2529%2520in%2520a%2520parallel%2520fashion.%2520SCM%2520aims%2520to%2520exploit%2520larger%250Akernel%2520leverages%2520for%2520the%2520increased%2520receptive%2520field%2520to%2520handle%2520scale%2520variations%250Ain%2520the%2520scene.%2520Whereas%2520our%2520novel%2520FRM%2520is%2520responsible%2520for%2520generating%2520semantic%2520cues%250Athat%2520can%2520capture%2520both%2520low-frequency%2520and%2520high-frequency%2520regions%2520for%2520better%250Asegmentation%2520tasks.%2520We%2520perform%2520experiments%2520over%2520challenging%2520real-world%250AZeroWaste-f%2520dataset%2520which%2520contains%2520background-cluttered%2520and%2520translucent%250Aobjects.%2520Our%2520experimental%2520results%2520demonstrate%2520the%2520state-of-the-art%2520performance%250Acompared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FANet%3A%20Feature%20Amplification%20Network%20for%20Semantic%20Segmentation%20in%0A%20%20Cluttered%20Background&entry.906535625=Muhammad%20Ali%20and%20Mamoona%20Javaid%20and%20Mubashir%20Noman%20and%20Mustansar%20Fiaz%20and%20Salman%20Khan&entry.1292438233=%20%20Existing%20deep%20learning%20approaches%20leave%20out%20the%20semantic%20cues%20that%20are%0Acrucial%20in%20semantic%20segmentation%20present%20in%20complex%20scenarios%20including%0Acluttered%20backgrounds%20and%20translucent%20objects%2C%20etc.%20To%20handle%20these%20challenges%2C%0Awe%20propose%20a%20feature%20amplification%20network%20%28FANet%29%20as%20a%20backbone%20network%20that%0Aincorporates%20semantic%20information%20using%20a%20novel%20feature%20enhancement%20module%20at%0Amulti-stages.%20To%20achieve%20this%2C%20we%20propose%20an%20adaptive%20feature%20enhancement%20%28AFE%29%0Ablock%20that%20benefits%20from%20both%20a%20spatial%20context%20module%20%28SCM%29%20and%20a%20feature%0Arefinement%20module%20%28FRM%29%20in%20a%20parallel%20fashion.%20SCM%20aims%20to%20exploit%20larger%0Akernel%20leverages%20for%20the%20increased%20receptive%20field%20to%20handle%20scale%20variations%0Ain%20the%20scene.%20Whereas%20our%20novel%20FRM%20is%20responsible%20for%20generating%20semantic%20cues%0Athat%20can%20capture%20both%20low-frequency%20and%20high-frequency%20regions%20for%20better%0Asegmentation%20tasks.%20We%20perform%20experiments%20over%20challenging%20real-world%0AZeroWaste-f%20dataset%20which%20contains%20background-cluttered%20and%20translucent%0Aobjects.%20Our%20experimental%20results%20demonstrate%20the%20state-of-the-art%20performance%0Acompared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09379v1&entry.124074799=Read"},
{"title": "Beyond Euclid: An Illustrated Guide to Modern Machine Learning with\n  Geometric, Topological, and Algebraic Structures", "author": "Sophia Sanborn and Johan Mathe and Mathilde Papillon and Domas Buracas and Hansen J Lillemark and Christian Shewmake and Abby Bertics and Xavier Pennec and Nina Miolane", "abstract": "  The enduring legacy of Euclidean geometry underpins classical machine\nlearning, which, for decades, has been primarily developed for data lying in\nEuclidean space. Yet, modern machine learning increasingly encounters richly\nstructured data that is inherently nonEuclidean. This data can exhibit\nintricate geometric, topological and algebraic structure: from the geometry of\nthe curvature of space-time, to topologically complex interactions between\nneurons in the brain, to the algebraic transformations describing symmetries of\nphysical systems. Extracting knowledge from such non-Euclidean data\nnecessitates a broader mathematical perspective. Echoing the 19th-century\nrevolutions that gave rise to non-Euclidean geometry, an emerging line of\nresearch is redefining modern machine learning with non-Euclidean structures.\nIts goal: generalizing classical methods to unconventional data types with\ngeometry, topology, and algebra. In this review, we provide an accessible\ngateway to this fast-growing field and propose a graphical taxonomy that\nintegrates recent advances into an intuitive unified framework. We subsequently\nextract insights into current challenges and highlight exciting opportunities\nfor future development in this field.\n", "link": "http://arxiv.org/abs/2407.09468v1", "date": "2024-07-12", "relevancy": 2.4957, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4874}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Euclid%3A%20An%20Illustrated%20Guide%20to%20Modern%20Machine%20Learning%20with%0A%20%20Geometric%2C%20Topological%2C%20and%20Algebraic%20Structures&body=Title%3A%20Beyond%20Euclid%3A%20An%20Illustrated%20Guide%20to%20Modern%20Machine%20Learning%20with%0A%20%20Geometric%2C%20Topological%2C%20and%20Algebraic%20Structures%0AAuthor%3A%20Sophia%20Sanborn%20and%20Johan%20Mathe%20and%20Mathilde%20Papillon%20and%20Domas%20Buracas%20and%20Hansen%20J%20Lillemark%20and%20Christian%20Shewmake%20and%20Abby%20Bertics%20and%20Xavier%20Pennec%20and%20Nina%20Miolane%0AAbstract%3A%20%20%20The%20enduring%20legacy%20of%20Euclidean%20geometry%20underpins%20classical%20machine%0Alearning%2C%20which%2C%20for%20decades%2C%20has%20been%20primarily%20developed%20for%20data%20lying%20in%0AEuclidean%20space.%20Yet%2C%20modern%20machine%20learning%20increasingly%20encounters%20richly%0Astructured%20data%20that%20is%20inherently%20nonEuclidean.%20This%20data%20can%20exhibit%0Aintricate%20geometric%2C%20topological%20and%20algebraic%20structure%3A%20from%20the%20geometry%20of%0Athe%20curvature%20of%20space-time%2C%20to%20topologically%20complex%20interactions%20between%0Aneurons%20in%20the%20brain%2C%20to%20the%20algebraic%20transformations%20describing%20symmetries%20of%0Aphysical%20systems.%20Extracting%20knowledge%20from%20such%20non-Euclidean%20data%0Anecessitates%20a%20broader%20mathematical%20perspective.%20Echoing%20the%2019th-century%0Arevolutions%20that%20gave%20rise%20to%20non-Euclidean%20geometry%2C%20an%20emerging%20line%20of%0Aresearch%20is%20redefining%20modern%20machine%20learning%20with%20non-Euclidean%20structures.%0AIts%20goal%3A%20generalizing%20classical%20methods%20to%20unconventional%20data%20types%20with%0Ageometry%2C%20topology%2C%20and%20algebra.%20In%20this%20review%2C%20we%20provide%20an%20accessible%0Agateway%20to%20this%20fast-growing%20field%20and%20propose%20a%20graphical%20taxonomy%20that%0Aintegrates%20recent%20advances%20into%20an%20intuitive%20unified%20framework.%20We%20subsequently%0Aextract%20insights%20into%20current%20challenges%20and%20highlight%20exciting%20opportunities%0Afor%20future%20development%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Euclid%253A%2520An%2520Illustrated%2520Guide%2520to%2520Modern%2520Machine%2520Learning%2520with%250A%2520%2520Geometric%252C%2520Topological%252C%2520and%2520Algebraic%2520Structures%26entry.906535625%3DSophia%2520Sanborn%2520and%2520Johan%2520Mathe%2520and%2520Mathilde%2520Papillon%2520and%2520Domas%2520Buracas%2520and%2520Hansen%2520J%2520Lillemark%2520and%2520Christian%2520Shewmake%2520and%2520Abby%2520Bertics%2520and%2520Xavier%2520Pennec%2520and%2520Nina%2520Miolane%26entry.1292438233%3D%2520%2520The%2520enduring%2520legacy%2520of%2520Euclidean%2520geometry%2520underpins%2520classical%2520machine%250Alearning%252C%2520which%252C%2520for%2520decades%252C%2520has%2520been%2520primarily%2520developed%2520for%2520data%2520lying%2520in%250AEuclidean%2520space.%2520Yet%252C%2520modern%2520machine%2520learning%2520increasingly%2520encounters%2520richly%250Astructured%2520data%2520that%2520is%2520inherently%2520nonEuclidean.%2520This%2520data%2520can%2520exhibit%250Aintricate%2520geometric%252C%2520topological%2520and%2520algebraic%2520structure%253A%2520from%2520the%2520geometry%2520of%250Athe%2520curvature%2520of%2520space-time%252C%2520to%2520topologically%2520complex%2520interactions%2520between%250Aneurons%2520in%2520the%2520brain%252C%2520to%2520the%2520algebraic%2520transformations%2520describing%2520symmetries%2520of%250Aphysical%2520systems.%2520Extracting%2520knowledge%2520from%2520such%2520non-Euclidean%2520data%250Anecessitates%2520a%2520broader%2520mathematical%2520perspective.%2520Echoing%2520the%252019th-century%250Arevolutions%2520that%2520gave%2520rise%2520to%2520non-Euclidean%2520geometry%252C%2520an%2520emerging%2520line%2520of%250Aresearch%2520is%2520redefining%2520modern%2520machine%2520learning%2520with%2520non-Euclidean%2520structures.%250AIts%2520goal%253A%2520generalizing%2520classical%2520methods%2520to%2520unconventional%2520data%2520types%2520with%250Ageometry%252C%2520topology%252C%2520and%2520algebra.%2520In%2520this%2520review%252C%2520we%2520provide%2520an%2520accessible%250Agateway%2520to%2520this%2520fast-growing%2520field%2520and%2520propose%2520a%2520graphical%2520taxonomy%2520that%250Aintegrates%2520recent%2520advances%2520into%2520an%2520intuitive%2520unified%2520framework.%2520We%2520subsequently%250Aextract%2520insights%2520into%2520current%2520challenges%2520and%2520highlight%2520exciting%2520opportunities%250Afor%2520future%2520development%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Euclid%3A%20An%20Illustrated%20Guide%20to%20Modern%20Machine%20Learning%20with%0A%20%20Geometric%2C%20Topological%2C%20and%20Algebraic%20Structures&entry.906535625=Sophia%20Sanborn%20and%20Johan%20Mathe%20and%20Mathilde%20Papillon%20and%20Domas%20Buracas%20and%20Hansen%20J%20Lillemark%20and%20Christian%20Shewmake%20and%20Abby%20Bertics%20and%20Xavier%20Pennec%20and%20Nina%20Miolane&entry.1292438233=%20%20The%20enduring%20legacy%20of%20Euclidean%20geometry%20underpins%20classical%20machine%0Alearning%2C%20which%2C%20for%20decades%2C%20has%20been%20primarily%20developed%20for%20data%20lying%20in%0AEuclidean%20space.%20Yet%2C%20modern%20machine%20learning%20increasingly%20encounters%20richly%0Astructured%20data%20that%20is%20inherently%20nonEuclidean.%20This%20data%20can%20exhibit%0Aintricate%20geometric%2C%20topological%20and%20algebraic%20structure%3A%20from%20the%20geometry%20of%0Athe%20curvature%20of%20space-time%2C%20to%20topologically%20complex%20interactions%20between%0Aneurons%20in%20the%20brain%2C%20to%20the%20algebraic%20transformations%20describing%20symmetries%20of%0Aphysical%20systems.%20Extracting%20knowledge%20from%20such%20non-Euclidean%20data%0Anecessitates%20a%20broader%20mathematical%20perspective.%20Echoing%20the%2019th-century%0Arevolutions%20that%20gave%20rise%20to%20non-Euclidean%20geometry%2C%20an%20emerging%20line%20of%0Aresearch%20is%20redefining%20modern%20machine%20learning%20with%20non-Euclidean%20structures.%0AIts%20goal%3A%20generalizing%20classical%20methods%20to%20unconventional%20data%20types%20with%0Ageometry%2C%20topology%2C%20and%20algebra.%20In%20this%20review%2C%20we%20provide%20an%20accessible%0Agateway%20to%20this%20fast-growing%20field%20and%20propose%20a%20graphical%20taxonomy%20that%0Aintegrates%20recent%20advances%20into%20an%20intuitive%20unified%20framework.%20We%20subsequently%0Aextract%20insights%20into%20current%20challenges%20and%20highlight%20exciting%20opportunities%0Afor%20future%20development%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09468v1&entry.124074799=Read"},
{"title": "Any-Property-Conditional Molecule Generation with Self-Criticism using\n  Spanning Trees", "author": "Alexia Jolicoeur-Martineau and Aristide Baratin and Kisoo Kwon and Boris Knyazev and Yan Zhang", "abstract": "  Generating novel molecules is challenging, with most representations leading\nto generative models producing many invalid molecules. Spanning Tree-based\nGraph Generation (STGG) is a promising approach to ensure the generation of\nvalid molecules, outperforming state-of-the-art SMILES and graph diffusion\nmodels for unconditional generation. In the real world, we want to be able to\ngenerate molecules conditional on one or multiple desired properties rather\nthan unconditionally. Thus, in this work, we extend STGG to\nmulti-property-conditional generation. Our approach, STGG+, incorporates a\nmodern Transformer architecture, random masking of properties during training\n(enabling conditioning on any subset of properties and classifier-free\nguidance), an auxiliary property-prediction loss (allowing the model to\nself-criticize molecules and select the best ones), and other improvements. We\nshow that STGG+ achieves state-of-the-art performance on in-distribution and\nout-of-distribution conditional generation, and reward maximization.\n", "link": "http://arxiv.org/abs/2407.09357v1", "date": "2024-07-12", "relevancy": 2.4875, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.512}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4936}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Any-Property-Conditional%20Molecule%20Generation%20with%20Self-Criticism%20using%0A%20%20Spanning%20Trees&body=Title%3A%20Any-Property-Conditional%20Molecule%20Generation%20with%20Self-Criticism%20using%0A%20%20Spanning%20Trees%0AAuthor%3A%20Alexia%20Jolicoeur-Martineau%20and%20Aristide%20Baratin%20and%20Kisoo%20Kwon%20and%20Boris%20Knyazev%20and%20Yan%20Zhang%0AAbstract%3A%20%20%20Generating%20novel%20molecules%20is%20challenging%2C%20with%20most%20representations%20leading%0Ato%20generative%20models%20producing%20many%20invalid%20molecules.%20Spanning%20Tree-based%0AGraph%20Generation%20%28STGG%29%20is%20a%20promising%20approach%20to%20ensure%20the%20generation%20of%0Avalid%20molecules%2C%20outperforming%20state-of-the-art%20SMILES%20and%20graph%20diffusion%0Amodels%20for%20unconditional%20generation.%20In%20the%20real%20world%2C%20we%20want%20to%20be%20able%20to%0Agenerate%20molecules%20conditional%20on%20one%20or%20multiple%20desired%20properties%20rather%0Athan%20unconditionally.%20Thus%2C%20in%20this%20work%2C%20we%20extend%20STGG%20to%0Amulti-property-conditional%20generation.%20Our%20approach%2C%20STGG%2B%2C%20incorporates%20a%0Amodern%20Transformer%20architecture%2C%20random%20masking%20of%20properties%20during%20training%0A%28enabling%20conditioning%20on%20any%20subset%20of%20properties%20and%20classifier-free%0Aguidance%29%2C%20an%20auxiliary%20property-prediction%20loss%20%28allowing%20the%20model%20to%0Aself-criticize%20molecules%20and%20select%20the%20best%20ones%29%2C%20and%20other%20improvements.%20We%0Ashow%20that%20STGG%2B%20achieves%20state-of-the-art%20performance%20on%20in-distribution%20and%0Aout-of-distribution%20conditional%20generation%2C%20and%20reward%20maximization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAny-Property-Conditional%2520Molecule%2520Generation%2520with%2520Self-Criticism%2520using%250A%2520%2520Spanning%2520Trees%26entry.906535625%3DAlexia%2520Jolicoeur-Martineau%2520and%2520Aristide%2520Baratin%2520and%2520Kisoo%2520Kwon%2520and%2520Boris%2520Knyazev%2520and%2520Yan%2520Zhang%26entry.1292438233%3D%2520%2520Generating%2520novel%2520molecules%2520is%2520challenging%252C%2520with%2520most%2520representations%2520leading%250Ato%2520generative%2520models%2520producing%2520many%2520invalid%2520molecules.%2520Spanning%2520Tree-based%250AGraph%2520Generation%2520%2528STGG%2529%2520is%2520a%2520promising%2520approach%2520to%2520ensure%2520the%2520generation%2520of%250Avalid%2520molecules%252C%2520outperforming%2520state-of-the-art%2520SMILES%2520and%2520graph%2520diffusion%250Amodels%2520for%2520unconditional%2520generation.%2520In%2520the%2520real%2520world%252C%2520we%2520want%2520to%2520be%2520able%2520to%250Agenerate%2520molecules%2520conditional%2520on%2520one%2520or%2520multiple%2520desired%2520properties%2520rather%250Athan%2520unconditionally.%2520Thus%252C%2520in%2520this%2520work%252C%2520we%2520extend%2520STGG%2520to%250Amulti-property-conditional%2520generation.%2520Our%2520approach%252C%2520STGG%252B%252C%2520incorporates%2520a%250Amodern%2520Transformer%2520architecture%252C%2520random%2520masking%2520of%2520properties%2520during%2520training%250A%2528enabling%2520conditioning%2520on%2520any%2520subset%2520of%2520properties%2520and%2520classifier-free%250Aguidance%2529%252C%2520an%2520auxiliary%2520property-prediction%2520loss%2520%2528allowing%2520the%2520model%2520to%250Aself-criticize%2520molecules%2520and%2520select%2520the%2520best%2520ones%2529%252C%2520and%2520other%2520improvements.%2520We%250Ashow%2520that%2520STGG%252B%2520achieves%2520state-of-the-art%2520performance%2520on%2520in-distribution%2520and%250Aout-of-distribution%2520conditional%2520generation%252C%2520and%2520reward%2520maximization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any-Property-Conditional%20Molecule%20Generation%20with%20Self-Criticism%20using%0A%20%20Spanning%20Trees&entry.906535625=Alexia%20Jolicoeur-Martineau%20and%20Aristide%20Baratin%20and%20Kisoo%20Kwon%20and%20Boris%20Knyazev%20and%20Yan%20Zhang&entry.1292438233=%20%20Generating%20novel%20molecules%20is%20challenging%2C%20with%20most%20representations%20leading%0Ato%20generative%20models%20producing%20many%20invalid%20molecules.%20Spanning%20Tree-based%0AGraph%20Generation%20%28STGG%29%20is%20a%20promising%20approach%20to%20ensure%20the%20generation%20of%0Avalid%20molecules%2C%20outperforming%20state-of-the-art%20SMILES%20and%20graph%20diffusion%0Amodels%20for%20unconditional%20generation.%20In%20the%20real%20world%2C%20we%20want%20to%20be%20able%20to%0Agenerate%20molecules%20conditional%20on%20one%20or%20multiple%20desired%20properties%20rather%0Athan%20unconditionally.%20Thus%2C%20in%20this%20work%2C%20we%20extend%20STGG%20to%0Amulti-property-conditional%20generation.%20Our%20approach%2C%20STGG%2B%2C%20incorporates%20a%0Amodern%20Transformer%20architecture%2C%20random%20masking%20of%20properties%20during%20training%0A%28enabling%20conditioning%20on%20any%20subset%20of%20properties%20and%20classifier-free%0Aguidance%29%2C%20an%20auxiliary%20property-prediction%20loss%20%28allowing%20the%20model%20to%0Aself-criticize%20molecules%20and%20select%20the%20best%20ones%29%2C%20and%20other%20improvements.%20We%0Ashow%20that%20STGG%2B%20achieves%20state-of-the-art%20performance%20on%20in-distribution%20and%0Aout-of-distribution%20conditional%20generation%2C%20and%20reward%20maximization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09357v1&entry.124074799=Read"},
{"title": "EnvGen: Generating and Adapting Environments via LLMs for Training\n  Embodied Agents", "author": "Abhay Zala and Jaemin Cho and Han Lin and Jaehong Yoon and Mohit Bansal", "abstract": "  Recent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller RL agents learn useful skills that they are weak at? We propose EnvGen,\na novel framework to address this question. We first prompt an LLM to generate\ntraining environments by giving it the task description and simulator\nobjectives that the agents should learn and then asking it to generate a set of\nenvironment configurations (e.g., different terrains, items initially given to\nagents, etc.). Next, we train a small RL agent in a mixture of the original and\nLLM-generated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent is\nweak at, by providing feedback to the LLM in the form of the agent's\nperformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist environments. We find that a small RL agent\ntrained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and\nlearns long-horizon tasks significantly faster. We also show that using an LLM\nto adapt environments dynamically outperforms curriculum learning approaches\nand how the environments are adapted to help improve RL agents' weaker skills\nover time. Additionally, EnvGen is substantially more efficient as it only uses\na small number of LLM calls (e.g., 4 in total), whereas LLM agents require\nthousands of calls. Lastly, we present detailed ablation studies for EnvGen\ndesign choices.\n", "link": "http://arxiv.org/abs/2403.12014v2", "date": "2024-07-12", "relevancy": 2.48, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.694}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6095}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnvGen%3A%20Generating%20and%20Adapting%20Environments%20via%20LLMs%20for%20Training%0A%20%20Embodied%20Agents&body=Title%3A%20EnvGen%3A%20Generating%20and%20Adapting%20Environments%20via%20LLMs%20for%20Training%0A%20%20Embodied%20Agents%0AAuthor%3A%20Abhay%20Zala%20and%20Jaemin%20Cho%20and%20Han%20Lin%20and%20Jaehong%20Yoon%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Recent%20SOTA%20approaches%20for%20embodied%20learning%20via%20interaction%20directly%20employ%0Alarge%20language%20models%20%28LLMs%29%20as%20agents%20to%20determine%20the%20next%20steps%20in%20an%0Aenvironment.%20Due%20to%20their%20world%20knowledge%20and%20reasoning%20capabilities%2C%20LLM%0Aagents%20achieve%20stronger%20performance%20than%20previous%20smaller%20agents%20based%20on%0Areinforcement%20learning%20%28RL%29%3B%20however%2C%20frequently%20calling%20LLMs%20is%20slow%20and%0Aexpensive.%20Instead%20of%20directly%20employing%20LLMs%20as%20agents%2C%20can%20we%20use%20LLMs%27%0Areasoning%20capabilities%20to%20adaptively%20create%20training%20environments%20to%20help%0Asmaller%20RL%20agents%20learn%20useful%20skills%20that%20they%20are%20weak%20at%3F%20We%20propose%20EnvGen%2C%0Aa%20novel%20framework%20to%20address%20this%20question.%20We%20first%20prompt%20an%20LLM%20to%20generate%0Atraining%20environments%20by%20giving%20it%20the%20task%20description%20and%20simulator%0Aobjectives%20that%20the%20agents%20should%20learn%20and%20then%20asking%20it%20to%20generate%20a%20set%20of%0Aenvironment%20configurations%20%28e.g.%2C%20different%20terrains%2C%20items%20initially%20given%20to%0Aagents%2C%20etc.%29.%20Next%2C%20we%20train%20a%20small%20RL%20agent%20in%20a%20mixture%20of%20the%20original%20and%0ALLM-generated%20environments.%20Then%2C%20we%20enable%20the%20LLM%20to%20continuously%20adapt%20the%0Agenerated%20environments%20to%20progressively%20improve%20the%20skills%20that%20the%20agent%20is%0Aweak%20at%2C%20by%20providing%20feedback%20to%20the%20LLM%20in%20the%20form%20of%20the%20agent%27s%0Aperformance.%20We%20demonstrate%20the%20usefulness%20of%20EnvGen%20with%20comprehensive%0Aexperiments%20in%20Crafter%20and%20Heist%20environments.%20We%20find%20that%20a%20small%20RL%20agent%0Atrained%20with%20EnvGen%20can%20outperform%20SOTA%20methods%2C%20including%20a%20GPT-4%20agent%2C%20and%0Alearns%20long-horizon%20tasks%20significantly%20faster.%20We%20also%20show%20that%20using%20an%20LLM%0Ato%20adapt%20environments%20dynamically%20outperforms%20curriculum%20learning%20approaches%0Aand%20how%20the%20environments%20are%20adapted%20to%20help%20improve%20RL%20agents%27%20weaker%20skills%0Aover%20time.%20Additionally%2C%20EnvGen%20is%20substantially%20more%20efficient%20as%20it%20only%20uses%0Aa%20small%20number%20of%20LLM%20calls%20%28e.g.%2C%204%20in%20total%29%2C%20whereas%20LLM%20agents%20require%0Athousands%20of%20calls.%20Lastly%2C%20we%20present%20detailed%20ablation%20studies%20for%20EnvGen%0Adesign%20choices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12014v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvGen%253A%2520Generating%2520and%2520Adapting%2520Environments%2520via%2520LLMs%2520for%2520Training%250A%2520%2520Embodied%2520Agents%26entry.906535625%3DAbhay%2520Zala%2520and%2520Jaemin%2520Cho%2520and%2520Han%2520Lin%2520and%2520Jaehong%2520Yoon%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Recent%2520SOTA%2520approaches%2520for%2520embodied%2520learning%2520via%2520interaction%2520directly%2520employ%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520as%2520agents%2520to%2520determine%2520the%2520next%2520steps%2520in%2520an%250Aenvironment.%2520Due%2520to%2520their%2520world%2520knowledge%2520and%2520reasoning%2520capabilities%252C%2520LLM%250Aagents%2520achieve%2520stronger%2520performance%2520than%2520previous%2520smaller%2520agents%2520based%2520on%250Areinforcement%2520learning%2520%2528RL%2529%253B%2520however%252C%2520frequently%2520calling%2520LLMs%2520is%2520slow%2520and%250Aexpensive.%2520Instead%2520of%2520directly%2520employing%2520LLMs%2520as%2520agents%252C%2520can%2520we%2520use%2520LLMs%2527%250Areasoning%2520capabilities%2520to%2520adaptively%2520create%2520training%2520environments%2520to%2520help%250Asmaller%2520RL%2520agents%2520learn%2520useful%2520skills%2520that%2520they%2520are%2520weak%2520at%253F%2520We%2520propose%2520EnvGen%252C%250Aa%2520novel%2520framework%2520to%2520address%2520this%2520question.%2520We%2520first%2520prompt%2520an%2520LLM%2520to%2520generate%250Atraining%2520environments%2520by%2520giving%2520it%2520the%2520task%2520description%2520and%2520simulator%250Aobjectives%2520that%2520the%2520agents%2520should%2520learn%2520and%2520then%2520asking%2520it%2520to%2520generate%2520a%2520set%2520of%250Aenvironment%2520configurations%2520%2528e.g.%252C%2520different%2520terrains%252C%2520items%2520initially%2520given%2520to%250Aagents%252C%2520etc.%2529.%2520Next%252C%2520we%2520train%2520a%2520small%2520RL%2520agent%2520in%2520a%2520mixture%2520of%2520the%2520original%2520and%250ALLM-generated%2520environments.%2520Then%252C%2520we%2520enable%2520the%2520LLM%2520to%2520continuously%2520adapt%2520the%250Agenerated%2520environments%2520to%2520progressively%2520improve%2520the%2520skills%2520that%2520the%2520agent%2520is%250Aweak%2520at%252C%2520by%2520providing%2520feedback%2520to%2520the%2520LLM%2520in%2520the%2520form%2520of%2520the%2520agent%2527s%250Aperformance.%2520We%2520demonstrate%2520the%2520usefulness%2520of%2520EnvGen%2520with%2520comprehensive%250Aexperiments%2520in%2520Crafter%2520and%2520Heist%2520environments.%2520We%2520find%2520that%2520a%2520small%2520RL%2520agent%250Atrained%2520with%2520EnvGen%2520can%2520outperform%2520SOTA%2520methods%252C%2520including%2520a%2520GPT-4%2520agent%252C%2520and%250Alearns%2520long-horizon%2520tasks%2520significantly%2520faster.%2520We%2520also%2520show%2520that%2520using%2520an%2520LLM%250Ato%2520adapt%2520environments%2520dynamically%2520outperforms%2520curriculum%2520learning%2520approaches%250Aand%2520how%2520the%2520environments%2520are%2520adapted%2520to%2520help%2520improve%2520RL%2520agents%2527%2520weaker%2520skills%250Aover%2520time.%2520Additionally%252C%2520EnvGen%2520is%2520substantially%2520more%2520efficient%2520as%2520it%2520only%2520uses%250Aa%2520small%2520number%2520of%2520LLM%2520calls%2520%2528e.g.%252C%25204%2520in%2520total%2529%252C%2520whereas%2520LLM%2520agents%2520require%250Athousands%2520of%2520calls.%2520Lastly%252C%2520we%2520present%2520detailed%2520ablation%2520studies%2520for%2520EnvGen%250Adesign%2520choices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12014v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnvGen%3A%20Generating%20and%20Adapting%20Environments%20via%20LLMs%20for%20Training%0A%20%20Embodied%20Agents&entry.906535625=Abhay%20Zala%20and%20Jaemin%20Cho%20and%20Han%20Lin%20and%20Jaehong%20Yoon%20and%20Mohit%20Bansal&entry.1292438233=%20%20Recent%20SOTA%20approaches%20for%20embodied%20learning%20via%20interaction%20directly%20employ%0Alarge%20language%20models%20%28LLMs%29%20as%20agents%20to%20determine%20the%20next%20steps%20in%20an%0Aenvironment.%20Due%20to%20their%20world%20knowledge%20and%20reasoning%20capabilities%2C%20LLM%0Aagents%20achieve%20stronger%20performance%20than%20previous%20smaller%20agents%20based%20on%0Areinforcement%20learning%20%28RL%29%3B%20however%2C%20frequently%20calling%20LLMs%20is%20slow%20and%0Aexpensive.%20Instead%20of%20directly%20employing%20LLMs%20as%20agents%2C%20can%20we%20use%20LLMs%27%0Areasoning%20capabilities%20to%20adaptively%20create%20training%20environments%20to%20help%0Asmaller%20RL%20agents%20learn%20useful%20skills%20that%20they%20are%20weak%20at%3F%20We%20propose%20EnvGen%2C%0Aa%20novel%20framework%20to%20address%20this%20question.%20We%20first%20prompt%20an%20LLM%20to%20generate%0Atraining%20environments%20by%20giving%20it%20the%20task%20description%20and%20simulator%0Aobjectives%20that%20the%20agents%20should%20learn%20and%20then%20asking%20it%20to%20generate%20a%20set%20of%0Aenvironment%20configurations%20%28e.g.%2C%20different%20terrains%2C%20items%20initially%20given%20to%0Aagents%2C%20etc.%29.%20Next%2C%20we%20train%20a%20small%20RL%20agent%20in%20a%20mixture%20of%20the%20original%20and%0ALLM-generated%20environments.%20Then%2C%20we%20enable%20the%20LLM%20to%20continuously%20adapt%20the%0Agenerated%20environments%20to%20progressively%20improve%20the%20skills%20that%20the%20agent%20is%0Aweak%20at%2C%20by%20providing%20feedback%20to%20the%20LLM%20in%20the%20form%20of%20the%20agent%27s%0Aperformance.%20We%20demonstrate%20the%20usefulness%20of%20EnvGen%20with%20comprehensive%0Aexperiments%20in%20Crafter%20and%20Heist%20environments.%20We%20find%20that%20a%20small%20RL%20agent%0Atrained%20with%20EnvGen%20can%20outperform%20SOTA%20methods%2C%20including%20a%20GPT-4%20agent%2C%20and%0Alearns%20long-horizon%20tasks%20significantly%20faster.%20We%20also%20show%20that%20using%20an%20LLM%0Ato%20adapt%20environments%20dynamically%20outperforms%20curriculum%20learning%20approaches%0Aand%20how%20the%20environments%20are%20adapted%20to%20help%20improve%20RL%20agents%27%20weaker%20skills%0Aover%20time.%20Additionally%2C%20EnvGen%20is%20substantially%20more%20efficient%20as%20it%20only%20uses%0Aa%20small%20number%20of%20LLM%20calls%20%28e.g.%2C%204%20in%20total%29%2C%20whereas%20LLM%20agents%20require%0Athousands%20of%20calls.%20Lastly%2C%20we%20present%20detailed%20ablation%20studies%20for%20EnvGen%0Adesign%20choices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12014v2&entry.124074799=Read"},
{"title": "FedVAE: Trajectory privacy preserving based on Federated Variational\n  AutoEncoder", "author": "Yuchen Jiang and Ying Wu and Shiyao Zhang and James J. Q. Yu", "abstract": "  The use of trajectory data with abundant spatial-temporal information is\npivotal in Intelligent Transport Systems (ITS) and various traffic system\ntasks. Location-Based Services (LBS) capitalize on this trajectory data to\noffer users personalized services tailored to their location information.\nHowever, this trajectory data contains sensitive information about users'\nmovement patterns and habits, necessitating confidentiality and protection from\nunknown collectors. To address this challenge, privacy-preserving methods like\nK-anonymity and Differential Privacy have been proposed to safeguard private\ninformation in the dataset. Despite their effectiveness, these methods can\nimpact the original features by introducing perturbations or generating\nunrealistic trajectory data, leading to suboptimal performance in downstream\ntasks. To overcome these limitations, we propose a Federated Variational\nAutoEncoder (FedVAE) approach, which effectively generates a new trajectory\ndataset while preserving the confidentiality of private information and\nretaining the structure of the original features. In addition, FedVAE leverages\nVariational AutoEncoder (VAE) to maintain the original feature space and\ngenerate new trajectory data, and incorporates Federated Learning (FL) during\nthe training stage, ensuring that users' data remains locally stored to protect\ntheir personal information. The results demonstrate its superior performance\ncompared to other existing methods, affirming FedVAE as a promising solution\nfor enhancing data privacy and utility in location-based applications.\n", "link": "http://arxiv.org/abs/2407.09239v1", "date": "2024-07-12", "relevancy": 2.4788, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5028}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4932}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedVAE%3A%20Trajectory%20privacy%20preserving%20based%20on%20Federated%20Variational%0A%20%20AutoEncoder&body=Title%3A%20FedVAE%3A%20Trajectory%20privacy%20preserving%20based%20on%20Federated%20Variational%0A%20%20AutoEncoder%0AAuthor%3A%20Yuchen%20Jiang%20and%20Ying%20Wu%20and%20Shiyao%20Zhang%20and%20James%20J.%20Q.%20Yu%0AAbstract%3A%20%20%20The%20use%20of%20trajectory%20data%20with%20abundant%20spatial-temporal%20information%20is%0Apivotal%20in%20Intelligent%20Transport%20Systems%20%28ITS%29%20and%20various%20traffic%20system%0Atasks.%20Location-Based%20Services%20%28LBS%29%20capitalize%20on%20this%20trajectory%20data%20to%0Aoffer%20users%20personalized%20services%20tailored%20to%20their%20location%20information.%0AHowever%2C%20this%20trajectory%20data%20contains%20sensitive%20information%20about%20users%27%0Amovement%20patterns%20and%20habits%2C%20necessitating%20confidentiality%20and%20protection%20from%0Aunknown%20collectors.%20To%20address%20this%20challenge%2C%20privacy-preserving%20methods%20like%0AK-anonymity%20and%20Differential%20Privacy%20have%20been%20proposed%20to%20safeguard%20private%0Ainformation%20in%20the%20dataset.%20Despite%20their%20effectiveness%2C%20these%20methods%20can%0Aimpact%20the%20original%20features%20by%20introducing%20perturbations%20or%20generating%0Aunrealistic%20trajectory%20data%2C%20leading%20to%20suboptimal%20performance%20in%20downstream%0Atasks.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20Federated%20Variational%0AAutoEncoder%20%28FedVAE%29%20approach%2C%20which%20effectively%20generates%20a%20new%20trajectory%0Adataset%20while%20preserving%20the%20confidentiality%20of%20private%20information%20and%0Aretaining%20the%20structure%20of%20the%20original%20features.%20In%20addition%2C%20FedVAE%20leverages%0AVariational%20AutoEncoder%20%28VAE%29%20to%20maintain%20the%20original%20feature%20space%20and%0Agenerate%20new%20trajectory%20data%2C%20and%20incorporates%20Federated%20Learning%20%28FL%29%20during%0Athe%20training%20stage%2C%20ensuring%20that%20users%27%20data%20remains%20locally%20stored%20to%20protect%0Atheir%20personal%20information.%20The%20results%20demonstrate%20its%20superior%20performance%0Acompared%20to%20other%20existing%20methods%2C%20affirming%20FedVAE%20as%20a%20promising%20solution%0Afor%20enhancing%20data%20privacy%20and%20utility%20in%20location-based%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedVAE%253A%2520Trajectory%2520privacy%2520preserving%2520based%2520on%2520Federated%2520Variational%250A%2520%2520AutoEncoder%26entry.906535625%3DYuchen%2520Jiang%2520and%2520Ying%2520Wu%2520and%2520Shiyao%2520Zhang%2520and%2520James%2520J.%2520Q.%2520Yu%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520trajectory%2520data%2520with%2520abundant%2520spatial-temporal%2520information%2520is%250Apivotal%2520in%2520Intelligent%2520Transport%2520Systems%2520%2528ITS%2529%2520and%2520various%2520traffic%2520system%250Atasks.%2520Location-Based%2520Services%2520%2528LBS%2529%2520capitalize%2520on%2520this%2520trajectory%2520data%2520to%250Aoffer%2520users%2520personalized%2520services%2520tailored%2520to%2520their%2520location%2520information.%250AHowever%252C%2520this%2520trajectory%2520data%2520contains%2520sensitive%2520information%2520about%2520users%2527%250Amovement%2520patterns%2520and%2520habits%252C%2520necessitating%2520confidentiality%2520and%2520protection%2520from%250Aunknown%2520collectors.%2520To%2520address%2520this%2520challenge%252C%2520privacy-preserving%2520methods%2520like%250AK-anonymity%2520and%2520Differential%2520Privacy%2520have%2520been%2520proposed%2520to%2520safeguard%2520private%250Ainformation%2520in%2520the%2520dataset.%2520Despite%2520their%2520effectiveness%252C%2520these%2520methods%2520can%250Aimpact%2520the%2520original%2520features%2520by%2520introducing%2520perturbations%2520or%2520generating%250Aunrealistic%2520trajectory%2520data%252C%2520leading%2520to%2520suboptimal%2520performance%2520in%2520downstream%250Atasks.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520Federated%2520Variational%250AAutoEncoder%2520%2528FedVAE%2529%2520approach%252C%2520which%2520effectively%2520generates%2520a%2520new%2520trajectory%250Adataset%2520while%2520preserving%2520the%2520confidentiality%2520of%2520private%2520information%2520and%250Aretaining%2520the%2520structure%2520of%2520the%2520original%2520features.%2520In%2520addition%252C%2520FedVAE%2520leverages%250AVariational%2520AutoEncoder%2520%2528VAE%2529%2520to%2520maintain%2520the%2520original%2520feature%2520space%2520and%250Agenerate%2520new%2520trajectory%2520data%252C%2520and%2520incorporates%2520Federated%2520Learning%2520%2528FL%2529%2520during%250Athe%2520training%2520stage%252C%2520ensuring%2520that%2520users%2527%2520data%2520remains%2520locally%2520stored%2520to%2520protect%250Atheir%2520personal%2520information.%2520The%2520results%2520demonstrate%2520its%2520superior%2520performance%250Acompared%2520to%2520other%2520existing%2520methods%252C%2520affirming%2520FedVAE%2520as%2520a%2520promising%2520solution%250Afor%2520enhancing%2520data%2520privacy%2520and%2520utility%2520in%2520location-based%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedVAE%3A%20Trajectory%20privacy%20preserving%20based%20on%20Federated%20Variational%0A%20%20AutoEncoder&entry.906535625=Yuchen%20Jiang%20and%20Ying%20Wu%20and%20Shiyao%20Zhang%20and%20James%20J.%20Q.%20Yu&entry.1292438233=%20%20The%20use%20of%20trajectory%20data%20with%20abundant%20spatial-temporal%20information%20is%0Apivotal%20in%20Intelligent%20Transport%20Systems%20%28ITS%29%20and%20various%20traffic%20system%0Atasks.%20Location-Based%20Services%20%28LBS%29%20capitalize%20on%20this%20trajectory%20data%20to%0Aoffer%20users%20personalized%20services%20tailored%20to%20their%20location%20information.%0AHowever%2C%20this%20trajectory%20data%20contains%20sensitive%20information%20about%20users%27%0Amovement%20patterns%20and%20habits%2C%20necessitating%20confidentiality%20and%20protection%20from%0Aunknown%20collectors.%20To%20address%20this%20challenge%2C%20privacy-preserving%20methods%20like%0AK-anonymity%20and%20Differential%20Privacy%20have%20been%20proposed%20to%20safeguard%20private%0Ainformation%20in%20the%20dataset.%20Despite%20their%20effectiveness%2C%20these%20methods%20can%0Aimpact%20the%20original%20features%20by%20introducing%20perturbations%20or%20generating%0Aunrealistic%20trajectory%20data%2C%20leading%20to%20suboptimal%20performance%20in%20downstream%0Atasks.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20Federated%20Variational%0AAutoEncoder%20%28FedVAE%29%20approach%2C%20which%20effectively%20generates%20a%20new%20trajectory%0Adataset%20while%20preserving%20the%20confidentiality%20of%20private%20information%20and%0Aretaining%20the%20structure%20of%20the%20original%20features.%20In%20addition%2C%20FedVAE%20leverages%0AVariational%20AutoEncoder%20%28VAE%29%20to%20maintain%20the%20original%20feature%20space%20and%0Agenerate%20new%20trajectory%20data%2C%20and%20incorporates%20Federated%20Learning%20%28FL%29%20during%0Athe%20training%20stage%2C%20ensuring%20that%20users%27%20data%20remains%20locally%20stored%20to%20protect%0Atheir%20personal%20information.%20The%20results%20demonstrate%20its%20superior%20performance%0Acompared%20to%20other%20existing%20methods%2C%20affirming%20FedVAE%20as%20a%20promising%20solution%0Afor%20enhancing%20data%20privacy%20and%20utility%20in%20location-based%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09239v1&entry.124074799=Read"},
{"title": "Unifying Sequences, Structures, and Descriptions for Any-to-Any Protein\n  Generation with the Large Multimodal Model HelixProtX", "author": "Zhiyuan Chen and Tianhao Chen and Chenggang Xie and Yang Xue and Xiaonan Zhang and Jingbo Zhou and Xiaomin Fang", "abstract": "  Proteins are fundamental components of biological systems and can be\nrepresented through various modalities, including sequences, structures, and\ntextual descriptions. Despite the advances in deep learning and scientific\nlarge language models (LLMs) for protein research, current methodologies\npredominantly focus on limited specialized tasks -- often predicting one\nprotein modality from another. These approaches restrict the understanding and\ngeneration of multimodal protein data. In contrast, large multimodal models\nhave demonstrated potential capabilities in generating any-to-any content like\ntext, images, and videos, thus enriching user interactions across various\ndomains. Integrating these multimodal model technologies into protein research\noffers significant promise by potentially transforming how proteins are\nstudied. To this end, we introduce HelixProtX, a system built upon the large\nmultimodal model, aiming to offer a comprehensive solution to protein research\nby supporting any-to-any protein modality generation. Unlike existing methods,\nit allows for the transformation of any input protein modality into any desired\nprotein modality. The experimental results affirm the advanced capabilities of\nHelixProtX, not only in generating functional descriptions from amino acid\nsequences but also in executing critical tasks such as designing protein\nsequences and structures from textual descriptions. Preliminary findings\nindicate that HelixProtX consistently achieves superior accuracy across a range\nof protein-related tasks, outperforming existing state-of-the-art models. By\nintegrating multimodal large models into protein research, HelixProtX opens new\navenues for understanding protein biology, thereby promising to accelerate\nscientific discovery.\n", "link": "http://arxiv.org/abs/2407.09274v1", "date": "2024-07-12", "relevancy": 2.467, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4938}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20Sequences%2C%20Structures%2C%20and%20Descriptions%20for%20Any-to-Any%20Protein%0A%20%20Generation%20with%20the%20Large%20Multimodal%20Model%20HelixProtX&body=Title%3A%20Unifying%20Sequences%2C%20Structures%2C%20and%20Descriptions%20for%20Any-to-Any%20Protein%0A%20%20Generation%20with%20the%20Large%20Multimodal%20Model%20HelixProtX%0AAuthor%3A%20Zhiyuan%20Chen%20and%20Tianhao%20Chen%20and%20Chenggang%20Xie%20and%20Yang%20Xue%20and%20Xiaonan%20Zhang%20and%20Jingbo%20Zhou%20and%20Xiaomin%20Fang%0AAbstract%3A%20%20%20Proteins%20are%20fundamental%20components%20of%20biological%20systems%20and%20can%20be%0Arepresented%20through%20various%20modalities%2C%20including%20sequences%2C%20structures%2C%20and%0Atextual%20descriptions.%20Despite%20the%20advances%20in%20deep%20learning%20and%20scientific%0Alarge%20language%20models%20%28LLMs%29%20for%20protein%20research%2C%20current%20methodologies%0Apredominantly%20focus%20on%20limited%20specialized%20tasks%20--%20often%20predicting%20one%0Aprotein%20modality%20from%20another.%20These%20approaches%20restrict%20the%20understanding%20and%0Ageneration%20of%20multimodal%20protein%20data.%20In%20contrast%2C%20large%20multimodal%20models%0Ahave%20demonstrated%20potential%20capabilities%20in%20generating%20any-to-any%20content%20like%0Atext%2C%20images%2C%20and%20videos%2C%20thus%20enriching%20user%20interactions%20across%20various%0Adomains.%20Integrating%20these%20multimodal%20model%20technologies%20into%20protein%20research%0Aoffers%20significant%20promise%20by%20potentially%20transforming%20how%20proteins%20are%0Astudied.%20To%20this%20end%2C%20we%20introduce%20HelixProtX%2C%20a%20system%20built%20upon%20the%20large%0Amultimodal%20model%2C%20aiming%20to%20offer%20a%20comprehensive%20solution%20to%20protein%20research%0Aby%20supporting%20any-to-any%20protein%20modality%20generation.%20Unlike%20existing%20methods%2C%0Ait%20allows%20for%20the%20transformation%20of%20any%20input%20protein%20modality%20into%20any%20desired%0Aprotein%20modality.%20The%20experimental%20results%20affirm%20the%20advanced%20capabilities%20of%0AHelixProtX%2C%20not%20only%20in%20generating%20functional%20descriptions%20from%20amino%20acid%0Asequences%20but%20also%20in%20executing%20critical%20tasks%20such%20as%20designing%20protein%0Asequences%20and%20structures%20from%20textual%20descriptions.%20Preliminary%20findings%0Aindicate%20that%20HelixProtX%20consistently%20achieves%20superior%20accuracy%20across%20a%20range%0Aof%20protein-related%20tasks%2C%20outperforming%20existing%20state-of-the-art%20models.%20By%0Aintegrating%20multimodal%20large%20models%20into%20protein%20research%2C%20HelixProtX%20opens%20new%0Aavenues%20for%20understanding%20protein%20biology%2C%20thereby%20promising%20to%20accelerate%0Ascientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520Sequences%252C%2520Structures%252C%2520and%2520Descriptions%2520for%2520Any-to-Any%2520Protein%250A%2520%2520Generation%2520with%2520the%2520Large%2520Multimodal%2520Model%2520HelixProtX%26entry.906535625%3DZhiyuan%2520Chen%2520and%2520Tianhao%2520Chen%2520and%2520Chenggang%2520Xie%2520and%2520Yang%2520Xue%2520and%2520Xiaonan%2520Zhang%2520and%2520Jingbo%2520Zhou%2520and%2520Xiaomin%2520Fang%26entry.1292438233%3D%2520%2520Proteins%2520are%2520fundamental%2520components%2520of%2520biological%2520systems%2520and%2520can%2520be%250Arepresented%2520through%2520various%2520modalities%252C%2520including%2520sequences%252C%2520structures%252C%2520and%250Atextual%2520descriptions.%2520Despite%2520the%2520advances%2520in%2520deep%2520learning%2520and%2520scientific%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520for%2520protein%2520research%252C%2520current%2520methodologies%250Apredominantly%2520focus%2520on%2520limited%2520specialized%2520tasks%2520--%2520often%2520predicting%2520one%250Aprotein%2520modality%2520from%2520another.%2520These%2520approaches%2520restrict%2520the%2520understanding%2520and%250Ageneration%2520of%2520multimodal%2520protein%2520data.%2520In%2520contrast%252C%2520large%2520multimodal%2520models%250Ahave%2520demonstrated%2520potential%2520capabilities%2520in%2520generating%2520any-to-any%2520content%2520like%250Atext%252C%2520images%252C%2520and%2520videos%252C%2520thus%2520enriching%2520user%2520interactions%2520across%2520various%250Adomains.%2520Integrating%2520these%2520multimodal%2520model%2520technologies%2520into%2520protein%2520research%250Aoffers%2520significant%2520promise%2520by%2520potentially%2520transforming%2520how%2520proteins%2520are%250Astudied.%2520To%2520this%2520end%252C%2520we%2520introduce%2520HelixProtX%252C%2520a%2520system%2520built%2520upon%2520the%2520large%250Amultimodal%2520model%252C%2520aiming%2520to%2520offer%2520a%2520comprehensive%2520solution%2520to%2520protein%2520research%250Aby%2520supporting%2520any-to-any%2520protein%2520modality%2520generation.%2520Unlike%2520existing%2520methods%252C%250Ait%2520allows%2520for%2520the%2520transformation%2520of%2520any%2520input%2520protein%2520modality%2520into%2520any%2520desired%250Aprotein%2520modality.%2520The%2520experimental%2520results%2520affirm%2520the%2520advanced%2520capabilities%2520of%250AHelixProtX%252C%2520not%2520only%2520in%2520generating%2520functional%2520descriptions%2520from%2520amino%2520acid%250Asequences%2520but%2520also%2520in%2520executing%2520critical%2520tasks%2520such%2520as%2520designing%2520protein%250Asequences%2520and%2520structures%2520from%2520textual%2520descriptions.%2520Preliminary%2520findings%250Aindicate%2520that%2520HelixProtX%2520consistently%2520achieves%2520superior%2520accuracy%2520across%2520a%2520range%250Aof%2520protein-related%2520tasks%252C%2520outperforming%2520existing%2520state-of-the-art%2520models.%2520By%250Aintegrating%2520multimodal%2520large%2520models%2520into%2520protein%2520research%252C%2520HelixProtX%2520opens%2520new%250Aavenues%2520for%2520understanding%2520protein%2520biology%252C%2520thereby%2520promising%2520to%2520accelerate%250Ascientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Sequences%2C%20Structures%2C%20and%20Descriptions%20for%20Any-to-Any%20Protein%0A%20%20Generation%20with%20the%20Large%20Multimodal%20Model%20HelixProtX&entry.906535625=Zhiyuan%20Chen%20and%20Tianhao%20Chen%20and%20Chenggang%20Xie%20and%20Yang%20Xue%20and%20Xiaonan%20Zhang%20and%20Jingbo%20Zhou%20and%20Xiaomin%20Fang&entry.1292438233=%20%20Proteins%20are%20fundamental%20components%20of%20biological%20systems%20and%20can%20be%0Arepresented%20through%20various%20modalities%2C%20including%20sequences%2C%20structures%2C%20and%0Atextual%20descriptions.%20Despite%20the%20advances%20in%20deep%20learning%20and%20scientific%0Alarge%20language%20models%20%28LLMs%29%20for%20protein%20research%2C%20current%20methodologies%0Apredominantly%20focus%20on%20limited%20specialized%20tasks%20--%20often%20predicting%20one%0Aprotein%20modality%20from%20another.%20These%20approaches%20restrict%20the%20understanding%20and%0Ageneration%20of%20multimodal%20protein%20data.%20In%20contrast%2C%20large%20multimodal%20models%0Ahave%20demonstrated%20potential%20capabilities%20in%20generating%20any-to-any%20content%20like%0Atext%2C%20images%2C%20and%20videos%2C%20thus%20enriching%20user%20interactions%20across%20various%0Adomains.%20Integrating%20these%20multimodal%20model%20technologies%20into%20protein%20research%0Aoffers%20significant%20promise%20by%20potentially%20transforming%20how%20proteins%20are%0Astudied.%20To%20this%20end%2C%20we%20introduce%20HelixProtX%2C%20a%20system%20built%20upon%20the%20large%0Amultimodal%20model%2C%20aiming%20to%20offer%20a%20comprehensive%20solution%20to%20protein%20research%0Aby%20supporting%20any-to-any%20protein%20modality%20generation.%20Unlike%20existing%20methods%2C%0Ait%20allows%20for%20the%20transformation%20of%20any%20input%20protein%20modality%20into%20any%20desired%0Aprotein%20modality.%20The%20experimental%20results%20affirm%20the%20advanced%20capabilities%20of%0AHelixProtX%2C%20not%20only%20in%20generating%20functional%20descriptions%20from%20amino%20acid%0Asequences%20but%20also%20in%20executing%20critical%20tasks%20such%20as%20designing%20protein%0Asequences%20and%20structures%20from%20textual%20descriptions.%20Preliminary%20findings%0Aindicate%20that%20HelixProtX%20consistently%20achieves%20superior%20accuracy%20across%20a%20range%0Aof%20protein-related%20tasks%2C%20outperforming%20existing%20state-of-the-art%20models.%20By%0Aintegrating%20multimodal%20large%20models%20into%20protein%20research%2C%20HelixProtX%20opens%20new%0Aavenues%20for%20understanding%20protein%20biology%2C%20thereby%20promising%20to%20accelerate%0Ascientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09274v1&entry.124074799=Read"},
{"title": "GraspXL: Generating Grasping Motions for Diverse Objects at Scale", "author": "Hui Zhang and Sammy Christen and Zicong Fan and Otmar Hilliges and Jie Song", "abstract": "  Human hands possess the dexterity to interact with diverse objects such as\ngrasping specific parts of the objects and/or approaching them from desired\ndirections. More importantly, humans can grasp objects of any shape without\nobject-specific skills. Recent works synthesize grasping motions following\nsingle objectives such as a desired approach heading direction or a grasping\narea. Moreover, they usually rely on expensive 3D hand-object data during\ntraining and inference, which limits their capability to synthesize grasping\nmotions for unseen objects at scale. In this paper, we unify the generation of\nhand-object grasping motions across multiple motion objectives, diverse object\nshapes and dexterous hand morphologies in a policy learning framework GraspXL.\nThe objectives are composed of the graspable area, heading direction during\napproach, wrist rotation, and hand position. Without requiring any 3D\nhand-object interaction data, our policy trained with 58 objects can robustly\nsynthesize diverse grasping motions for more than 500k unseen objects with a\nsuccess rate of 82.2%. At the same time, the policy adheres to objectives,\nwhich enables the generation of diverse grasps per object. Moreover, we show\nthat our framework can be deployed to different dexterous hands and work with\nreconstructed or generated objects. We quantitatively and qualitatively\nevaluate our method to show the efficacy of our approach. Our model, code, and\nthe large-scale generated motions are available at\nhttps://eth-ait.github.io/graspxl/.\n", "link": "http://arxiv.org/abs/2403.19649v2", "date": "2024-07-12", "relevancy": 2.4311, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7254}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5678}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraspXL%3A%20Generating%20Grasping%20Motions%20for%20Diverse%20Objects%20at%20Scale&body=Title%3A%20GraspXL%3A%20Generating%20Grasping%20Motions%20for%20Diverse%20Objects%20at%20Scale%0AAuthor%3A%20Hui%20Zhang%20and%20Sammy%20Christen%20and%20Zicong%20Fan%20and%20Otmar%20Hilliges%20and%20Jie%20Song%0AAbstract%3A%20%20%20Human%20hands%20possess%20the%20dexterity%20to%20interact%20with%20diverse%20objects%20such%20as%0Agrasping%20specific%20parts%20of%20the%20objects%20and/or%20approaching%20them%20from%20desired%0Adirections.%20More%20importantly%2C%20humans%20can%20grasp%20objects%20of%20any%20shape%20without%0Aobject-specific%20skills.%20Recent%20works%20synthesize%20grasping%20motions%20following%0Asingle%20objectives%20such%20as%20a%20desired%20approach%20heading%20direction%20or%20a%20grasping%0Aarea.%20Moreover%2C%20they%20usually%20rely%20on%20expensive%203D%20hand-object%20data%20during%0Atraining%20and%20inference%2C%20which%20limits%20their%20capability%20to%20synthesize%20grasping%0Amotions%20for%20unseen%20objects%20at%20scale.%20In%20this%20paper%2C%20we%20unify%20the%20generation%20of%0Ahand-object%20grasping%20motions%20across%20multiple%20motion%20objectives%2C%20diverse%20object%0Ashapes%20and%20dexterous%20hand%20morphologies%20in%20a%20policy%20learning%20framework%20GraspXL.%0AThe%20objectives%20are%20composed%20of%20the%20graspable%20area%2C%20heading%20direction%20during%0Aapproach%2C%20wrist%20rotation%2C%20and%20hand%20position.%20Without%20requiring%20any%203D%0Ahand-object%20interaction%20data%2C%20our%20policy%20trained%20with%2058%20objects%20can%20robustly%0Asynthesize%20diverse%20grasping%20motions%20for%20more%20than%20500k%20unseen%20objects%20with%20a%0Asuccess%20rate%20of%2082.2%25.%20At%20the%20same%20time%2C%20the%20policy%20adheres%20to%20objectives%2C%0Awhich%20enables%20the%20generation%20of%20diverse%20grasps%20per%20object.%20Moreover%2C%20we%20show%0Athat%20our%20framework%20can%20be%20deployed%20to%20different%20dexterous%20hands%20and%20work%20with%0Areconstructed%20or%20generated%20objects.%20We%20quantitatively%20and%20qualitatively%0Aevaluate%20our%20method%20to%20show%20the%20efficacy%20of%20our%20approach.%20Our%20model%2C%20code%2C%20and%0Athe%20large-scale%20generated%20motions%20are%20available%20at%0Ahttps%3A//eth-ait.github.io/graspxl/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19649v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraspXL%253A%2520Generating%2520Grasping%2520Motions%2520for%2520Diverse%2520Objects%2520at%2520Scale%26entry.906535625%3DHui%2520Zhang%2520and%2520Sammy%2520Christen%2520and%2520Zicong%2520Fan%2520and%2520Otmar%2520Hilliges%2520and%2520Jie%2520Song%26entry.1292438233%3D%2520%2520Human%2520hands%2520possess%2520the%2520dexterity%2520to%2520interact%2520with%2520diverse%2520objects%2520such%2520as%250Agrasping%2520specific%2520parts%2520of%2520the%2520objects%2520and/or%2520approaching%2520them%2520from%2520desired%250Adirections.%2520More%2520importantly%252C%2520humans%2520can%2520grasp%2520objects%2520of%2520any%2520shape%2520without%250Aobject-specific%2520skills.%2520Recent%2520works%2520synthesize%2520grasping%2520motions%2520following%250Asingle%2520objectives%2520such%2520as%2520a%2520desired%2520approach%2520heading%2520direction%2520or%2520a%2520grasping%250Aarea.%2520Moreover%252C%2520they%2520usually%2520rely%2520on%2520expensive%25203D%2520hand-object%2520data%2520during%250Atraining%2520and%2520inference%252C%2520which%2520limits%2520their%2520capability%2520to%2520synthesize%2520grasping%250Amotions%2520for%2520unseen%2520objects%2520at%2520scale.%2520In%2520this%2520paper%252C%2520we%2520unify%2520the%2520generation%2520of%250Ahand-object%2520grasping%2520motions%2520across%2520multiple%2520motion%2520objectives%252C%2520diverse%2520object%250Ashapes%2520and%2520dexterous%2520hand%2520morphologies%2520in%2520a%2520policy%2520learning%2520framework%2520GraspXL.%250AThe%2520objectives%2520are%2520composed%2520of%2520the%2520graspable%2520area%252C%2520heading%2520direction%2520during%250Aapproach%252C%2520wrist%2520rotation%252C%2520and%2520hand%2520position.%2520Without%2520requiring%2520any%25203D%250Ahand-object%2520interaction%2520data%252C%2520our%2520policy%2520trained%2520with%252058%2520objects%2520can%2520robustly%250Asynthesize%2520diverse%2520grasping%2520motions%2520for%2520more%2520than%2520500k%2520unseen%2520objects%2520with%2520a%250Asuccess%2520rate%2520of%252082.2%2525.%2520At%2520the%2520same%2520time%252C%2520the%2520policy%2520adheres%2520to%2520objectives%252C%250Awhich%2520enables%2520the%2520generation%2520of%2520diverse%2520grasps%2520per%2520object.%2520Moreover%252C%2520we%2520show%250Athat%2520our%2520framework%2520can%2520be%2520deployed%2520to%2520different%2520dexterous%2520hands%2520and%2520work%2520with%250Areconstructed%2520or%2520generated%2520objects.%2520We%2520quantitatively%2520and%2520qualitatively%250Aevaluate%2520our%2520method%2520to%2520show%2520the%2520efficacy%2520of%2520our%2520approach.%2520Our%2520model%252C%2520code%252C%2520and%250Athe%2520large-scale%2520generated%2520motions%2520are%2520available%2520at%250Ahttps%253A//eth-ait.github.io/graspxl/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19649v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraspXL%3A%20Generating%20Grasping%20Motions%20for%20Diverse%20Objects%20at%20Scale&entry.906535625=Hui%20Zhang%20and%20Sammy%20Christen%20and%20Zicong%20Fan%20and%20Otmar%20Hilliges%20and%20Jie%20Song&entry.1292438233=%20%20Human%20hands%20possess%20the%20dexterity%20to%20interact%20with%20diverse%20objects%20such%20as%0Agrasping%20specific%20parts%20of%20the%20objects%20and/or%20approaching%20them%20from%20desired%0Adirections.%20More%20importantly%2C%20humans%20can%20grasp%20objects%20of%20any%20shape%20without%0Aobject-specific%20skills.%20Recent%20works%20synthesize%20grasping%20motions%20following%0Asingle%20objectives%20such%20as%20a%20desired%20approach%20heading%20direction%20or%20a%20grasping%0Aarea.%20Moreover%2C%20they%20usually%20rely%20on%20expensive%203D%20hand-object%20data%20during%0Atraining%20and%20inference%2C%20which%20limits%20their%20capability%20to%20synthesize%20grasping%0Amotions%20for%20unseen%20objects%20at%20scale.%20In%20this%20paper%2C%20we%20unify%20the%20generation%20of%0Ahand-object%20grasping%20motions%20across%20multiple%20motion%20objectives%2C%20diverse%20object%0Ashapes%20and%20dexterous%20hand%20morphologies%20in%20a%20policy%20learning%20framework%20GraspXL.%0AThe%20objectives%20are%20composed%20of%20the%20graspable%20area%2C%20heading%20direction%20during%0Aapproach%2C%20wrist%20rotation%2C%20and%20hand%20position.%20Without%20requiring%20any%203D%0Ahand-object%20interaction%20data%2C%20our%20policy%20trained%20with%2058%20objects%20can%20robustly%0Asynthesize%20diverse%20grasping%20motions%20for%20more%20than%20500k%20unseen%20objects%20with%20a%0Asuccess%20rate%20of%2082.2%25.%20At%20the%20same%20time%2C%20the%20policy%20adheres%20to%20objectives%2C%0Awhich%20enables%20the%20generation%20of%20diverse%20grasps%20per%20object.%20Moreover%2C%20we%20show%0Athat%20our%20framework%20can%20be%20deployed%20to%20different%20dexterous%20hands%20and%20work%20with%0Areconstructed%20or%20generated%20objects.%20We%20quantitatively%20and%20qualitatively%0Aevaluate%20our%20method%20to%20show%20the%20efficacy%20of%20our%20approach.%20Our%20model%2C%20code%2C%20and%0Athe%20large-scale%20generated%20motions%20are%20available%20at%0Ahttps%3A//eth-ait.github.io/graspxl/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19649v2&entry.124074799=Read"},
{"title": "ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class\n  Class-agnostic Counting", "author": "Michael A. Hobley and Victor A. Prisacariu", "abstract": "  Class-agnostic counting methods enumerate objects of an arbitrary class,\nproviding tremendous utility in many fields. Prior works have limited\nusefulness as they require either a set of examples of the type to be counted\nor that the query image contains only a single type of object. A significant\nfactor in these shortcomings is the lack of a dataset to properly address\ncounting in settings with more than one kind of object present. To address\nthese issues, we propose the first Multi-class, Class-Agnostic Counting dataset\n(MCAC) and A Blind Counter (ABC123), a method that can count multiple types of\nobjects simultaneously without using examples of type during training or\ninference. ABC123 introduces a new paradigm where instead of requiring\nexemplars to guide the enumeration, examples are found after the counting stage\nto help a user understand the generated outputs. We show that ABC123\noutperforms contemporary methods on MCAC without needing human in-the-loop\nannotations. We also show that this performance transfers to FSC-147, the\nstandard class-agnostic counting dataset. MCAC is available at\nMCAC.active.vision and ABC123 is available at ABC123.active.vision.\n", "link": "http://arxiv.org/abs/2309.04820v2", "date": "2024-07-12", "relevancy": 2.4084, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5058}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4802}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ABC%20Easy%20as%20123%3A%20A%20Blind%20Counter%20for%20Exemplar-Free%20Multi-Class%0A%20%20Class-agnostic%20Counting&body=Title%3A%20ABC%20Easy%20as%20123%3A%20A%20Blind%20Counter%20for%20Exemplar-Free%20Multi-Class%0A%20%20Class-agnostic%20Counting%0AAuthor%3A%20Michael%20A.%20Hobley%20and%20Victor%20A.%20Prisacariu%0AAbstract%3A%20%20%20Class-agnostic%20counting%20methods%20enumerate%20objects%20of%20an%20arbitrary%20class%2C%0Aproviding%20tremendous%20utility%20in%20many%20fields.%20Prior%20works%20have%20limited%0Ausefulness%20as%20they%20require%20either%20a%20set%20of%20examples%20of%20the%20type%20to%20be%20counted%0Aor%20that%20the%20query%20image%20contains%20only%20a%20single%20type%20of%20object.%20A%20significant%0Afactor%20in%20these%20shortcomings%20is%20the%20lack%20of%20a%20dataset%20to%20properly%20address%0Acounting%20in%20settings%20with%20more%20than%20one%20kind%20of%20object%20present.%20To%20address%0Athese%20issues%2C%20we%20propose%20the%20first%20Multi-class%2C%20Class-Agnostic%20Counting%20dataset%0A%28MCAC%29%20and%20A%20Blind%20Counter%20%28ABC123%29%2C%20a%20method%20that%20can%20count%20multiple%20types%20of%0Aobjects%20simultaneously%20without%20using%20examples%20of%20type%20during%20training%20or%0Ainference.%20ABC123%20introduces%20a%20new%20paradigm%20where%20instead%20of%20requiring%0Aexemplars%20to%20guide%20the%20enumeration%2C%20examples%20are%20found%20after%20the%20counting%20stage%0Ato%20help%20a%20user%20understand%20the%20generated%20outputs.%20We%20show%20that%20ABC123%0Aoutperforms%20contemporary%20methods%20on%20MCAC%20without%20needing%20human%20in-the-loop%0Aannotations.%20We%20also%20show%20that%20this%20performance%20transfers%20to%20FSC-147%2C%20the%0Astandard%20class-agnostic%20counting%20dataset.%20MCAC%20is%20available%20at%0AMCAC.active.vision%20and%20ABC123%20is%20available%20at%20ABC123.active.vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.04820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DABC%2520Easy%2520as%2520123%253A%2520A%2520Blind%2520Counter%2520for%2520Exemplar-Free%2520Multi-Class%250A%2520%2520Class-agnostic%2520Counting%26entry.906535625%3DMichael%2520A.%2520Hobley%2520and%2520Victor%2520A.%2520Prisacariu%26entry.1292438233%3D%2520%2520Class-agnostic%2520counting%2520methods%2520enumerate%2520objects%2520of%2520an%2520arbitrary%2520class%252C%250Aproviding%2520tremendous%2520utility%2520in%2520many%2520fields.%2520Prior%2520works%2520have%2520limited%250Ausefulness%2520as%2520they%2520require%2520either%2520a%2520set%2520of%2520examples%2520of%2520the%2520type%2520to%2520be%2520counted%250Aor%2520that%2520the%2520query%2520image%2520contains%2520only%2520a%2520single%2520type%2520of%2520object.%2520A%2520significant%250Afactor%2520in%2520these%2520shortcomings%2520is%2520the%2520lack%2520of%2520a%2520dataset%2520to%2520properly%2520address%250Acounting%2520in%2520settings%2520with%2520more%2520than%2520one%2520kind%2520of%2520object%2520present.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520the%2520first%2520Multi-class%252C%2520Class-Agnostic%2520Counting%2520dataset%250A%2528MCAC%2529%2520and%2520A%2520Blind%2520Counter%2520%2528ABC123%2529%252C%2520a%2520method%2520that%2520can%2520count%2520multiple%2520types%2520of%250Aobjects%2520simultaneously%2520without%2520using%2520examples%2520of%2520type%2520during%2520training%2520or%250Ainference.%2520ABC123%2520introduces%2520a%2520new%2520paradigm%2520where%2520instead%2520of%2520requiring%250Aexemplars%2520to%2520guide%2520the%2520enumeration%252C%2520examples%2520are%2520found%2520after%2520the%2520counting%2520stage%250Ato%2520help%2520a%2520user%2520understand%2520the%2520generated%2520outputs.%2520We%2520show%2520that%2520ABC123%250Aoutperforms%2520contemporary%2520methods%2520on%2520MCAC%2520without%2520needing%2520human%2520in-the-loop%250Aannotations.%2520We%2520also%2520show%2520that%2520this%2520performance%2520transfers%2520to%2520FSC-147%252C%2520the%250Astandard%2520class-agnostic%2520counting%2520dataset.%2520MCAC%2520is%2520available%2520at%250AMCAC.active.vision%2520and%2520ABC123%2520is%2520available%2520at%2520ABC123.active.vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.04820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ABC%20Easy%20as%20123%3A%20A%20Blind%20Counter%20for%20Exemplar-Free%20Multi-Class%0A%20%20Class-agnostic%20Counting&entry.906535625=Michael%20A.%20Hobley%20and%20Victor%20A.%20Prisacariu&entry.1292438233=%20%20Class-agnostic%20counting%20methods%20enumerate%20objects%20of%20an%20arbitrary%20class%2C%0Aproviding%20tremendous%20utility%20in%20many%20fields.%20Prior%20works%20have%20limited%0Ausefulness%20as%20they%20require%20either%20a%20set%20of%20examples%20of%20the%20type%20to%20be%20counted%0Aor%20that%20the%20query%20image%20contains%20only%20a%20single%20type%20of%20object.%20A%20significant%0Afactor%20in%20these%20shortcomings%20is%20the%20lack%20of%20a%20dataset%20to%20properly%20address%0Acounting%20in%20settings%20with%20more%20than%20one%20kind%20of%20object%20present.%20To%20address%0Athese%20issues%2C%20we%20propose%20the%20first%20Multi-class%2C%20Class-Agnostic%20Counting%20dataset%0A%28MCAC%29%20and%20A%20Blind%20Counter%20%28ABC123%29%2C%20a%20method%20that%20can%20count%20multiple%20types%20of%0Aobjects%20simultaneously%20without%20using%20examples%20of%20type%20during%20training%20or%0Ainference.%20ABC123%20introduces%20a%20new%20paradigm%20where%20instead%20of%20requiring%0Aexemplars%20to%20guide%20the%20enumeration%2C%20examples%20are%20found%20after%20the%20counting%20stage%0Ato%20help%20a%20user%20understand%20the%20generated%20outputs.%20We%20show%20that%20ABC123%0Aoutperforms%20contemporary%20methods%20on%20MCAC%20without%20needing%20human%20in-the-loop%0Aannotations.%20We%20also%20show%20that%20this%20performance%20transfers%20to%20FSC-147%2C%20the%0Astandard%20class-agnostic%20counting%20dataset.%20MCAC%20is%20available%20at%0AMCAC.active.vision%20and%20ABC123%20is%20available%20at%20ABC123.active.vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.04820v2&entry.124074799=Read"},
{"title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction", "author": "Zekun Qi and Runpei Dong and Shaochen Zhang and Haoran Geng and Chunrui Han and Zheng Ge and Li Yi and Kaisheng Ma", "abstract": "  This paper presents ShapeLLM, the first 3D Multimodal Large Language Model\n(LLM) designed for embodied interaction, exploring a universal 3D object\nunderstanding with 3D point clouds and languages. ShapeLLM is built upon an\nimproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view\nimage distillation for enhanced geometry understanding. By utilizing ReCon++ as\nthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed\ninstruction-following data and tested on our newly human-curated benchmark, 3D\nMM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D\ngeometry understanding and language-unified 3D interaction tasks, such as\nembodied visual grounding. Project page: https://qizekun.github.io/shapellm/\n", "link": "http://arxiv.org/abs/2402.17766v3", "date": "2024-07-12", "relevancy": 2.3455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6517}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5598}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapeLLM%3A%20Universal%203D%20Object%20Understanding%20for%20Embodied%20Interaction&body=Title%3A%20ShapeLLM%3A%20Universal%203D%20Object%20Understanding%20for%20Embodied%20Interaction%0AAuthor%3A%20Zekun%20Qi%20and%20Runpei%20Dong%20and%20Shaochen%20Zhang%20and%20Haoran%20Geng%20and%20Chunrui%20Han%20and%20Zheng%20Ge%20and%20Li%20Yi%20and%20Kaisheng%20Ma%0AAbstract%3A%20%20%20This%20paper%20presents%20ShapeLLM%2C%20the%20first%203D%20Multimodal%20Large%20Language%20Model%0A%28LLM%29%20designed%20for%20embodied%20interaction%2C%20exploring%20a%20universal%203D%20object%0Aunderstanding%20with%203D%20point%20clouds%20and%20languages.%20ShapeLLM%20is%20built%20upon%20an%0Aimproved%203D%20encoder%20by%20extending%20ReCon%20to%20ReCon%2B%2B%20that%20benefits%20from%20multi-view%0Aimage%20distillation%20for%20enhanced%20geometry%20understanding.%20By%20utilizing%20ReCon%2B%2B%20as%0Athe%203D%20point%20cloud%20input%20encoder%20for%20LLMs%2C%20ShapeLLM%20is%20trained%20on%20constructed%0Ainstruction-following%20data%20and%20tested%20on%20our%20newly%20human-curated%20benchmark%2C%203D%0AMM-Vet.%20ReCon%2B%2B%20and%20ShapeLLM%20achieve%20state-of-the-art%20performance%20in%203D%0Ageometry%20understanding%20and%20language-unified%203D%20interaction%20tasks%2C%20such%20as%0Aembodied%20visual%20grounding.%20Project%20page%3A%20https%3A//qizekun.github.io/shapellm/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17766v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapeLLM%253A%2520Universal%25203D%2520Object%2520Understanding%2520for%2520Embodied%2520Interaction%26entry.906535625%3DZekun%2520Qi%2520and%2520Runpei%2520Dong%2520and%2520Shaochen%2520Zhang%2520and%2520Haoran%2520Geng%2520and%2520Chunrui%2520Han%2520and%2520Zheng%2520Ge%2520and%2520Li%2520Yi%2520and%2520Kaisheng%2520Ma%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520ShapeLLM%252C%2520the%2520first%25203D%2520Multimodal%2520Large%2520Language%2520Model%250A%2528LLM%2529%2520designed%2520for%2520embodied%2520interaction%252C%2520exploring%2520a%2520universal%25203D%2520object%250Aunderstanding%2520with%25203D%2520point%2520clouds%2520and%2520languages.%2520ShapeLLM%2520is%2520built%2520upon%2520an%250Aimproved%25203D%2520encoder%2520by%2520extending%2520ReCon%2520to%2520ReCon%252B%252B%2520that%2520benefits%2520from%2520multi-view%250Aimage%2520distillation%2520for%2520enhanced%2520geometry%2520understanding.%2520By%2520utilizing%2520ReCon%252B%252B%2520as%250Athe%25203D%2520point%2520cloud%2520input%2520encoder%2520for%2520LLMs%252C%2520ShapeLLM%2520is%2520trained%2520on%2520constructed%250Ainstruction-following%2520data%2520and%2520tested%2520on%2520our%2520newly%2520human-curated%2520benchmark%252C%25203D%250AMM-Vet.%2520ReCon%252B%252B%2520and%2520ShapeLLM%2520achieve%2520state-of-the-art%2520performance%2520in%25203D%250Ageometry%2520understanding%2520and%2520language-unified%25203D%2520interaction%2520tasks%252C%2520such%2520as%250Aembodied%2520visual%2520grounding.%2520Project%2520page%253A%2520https%253A//qizekun.github.io/shapellm/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17766v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeLLM%3A%20Universal%203D%20Object%20Understanding%20for%20Embodied%20Interaction&entry.906535625=Zekun%20Qi%20and%20Runpei%20Dong%20and%20Shaochen%20Zhang%20and%20Haoran%20Geng%20and%20Chunrui%20Han%20and%20Zheng%20Ge%20and%20Li%20Yi%20and%20Kaisheng%20Ma&entry.1292438233=%20%20This%20paper%20presents%20ShapeLLM%2C%20the%20first%203D%20Multimodal%20Large%20Language%20Model%0A%28LLM%29%20designed%20for%20embodied%20interaction%2C%20exploring%20a%20universal%203D%20object%0Aunderstanding%20with%203D%20point%20clouds%20and%20languages.%20ShapeLLM%20is%20built%20upon%20an%0Aimproved%203D%20encoder%20by%20extending%20ReCon%20to%20ReCon%2B%2B%20that%20benefits%20from%20multi-view%0Aimage%20distillation%20for%20enhanced%20geometry%20understanding.%20By%20utilizing%20ReCon%2B%2B%20as%0Athe%203D%20point%20cloud%20input%20encoder%20for%20LLMs%2C%20ShapeLLM%20is%20trained%20on%20constructed%0Ainstruction-following%20data%20and%20tested%20on%20our%20newly%20human-curated%20benchmark%2C%203D%0AMM-Vet.%20ReCon%2B%2B%20and%20ShapeLLM%20achieve%20state-of-the-art%20performance%20in%203D%0Ageometry%20understanding%20and%20language-unified%203D%20interaction%20tasks%2C%20such%20as%0Aembodied%20visual%20grounding.%20Project%20page%3A%20https%3A//qizekun.github.io/shapellm/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17766v3&entry.124074799=Read"},
{"title": "HUP-3D: A 3D multi-view synthetic dataset for assisted-egocentric\n  hand-ultrasound pose estimation", "author": "Manuel Birlo and Razvan Caramalau and Philip J. \"Eddie\" Edwards and Brian Dromey and Matthew J. Clarkson and Danail Stoyanov", "abstract": "  We present HUP-3D, a 3D multi-view multi-modal synthetic dataset for\nhand-ultrasound (US) probe pose estimation in the context of obstetric\nultrasound. Egocentric markerless 3D joint pose estimation has potential\napplications in mixed reality based medical education. The ability to\nunderstand hand and probe movements programmatically opens the door to tailored\nguidance and mentoring applications. Our dataset consists of over 31k sets of\nRGB, depth and segmentation mask frames, including pose related ground truth\ndata, with a strong emphasis on image diversity and complexity. Adopting a\ncamera viewpoint-based sphere concept allows us to capture a variety of views\nand generate multiple hand grasp poses using a pre-trained network.\nAdditionally, our approach includes a software-based image rendering concept,\nenhancing diversity with various hand and arm textures, lighting conditions,\nand background images. Furthermore, we validated our proposed dataset with\nstate-of-the-art learning models and we obtained the lowest hand-object\nkeypoint errors. The dataset and other details are provided with the\nsupplementary material. The source code of our grasp generation and rendering\npipeline will be made publicly available.\n", "link": "http://arxiv.org/abs/2407.09215v1", "date": "2024-07-12", "relevancy": 2.3435, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6156}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5799}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HUP-3D%3A%20A%203D%20multi-view%20synthetic%20dataset%20for%20assisted-egocentric%0A%20%20hand-ultrasound%20pose%20estimation&body=Title%3A%20HUP-3D%3A%20A%203D%20multi-view%20synthetic%20dataset%20for%20assisted-egocentric%0A%20%20hand-ultrasound%20pose%20estimation%0AAuthor%3A%20Manuel%20Birlo%20and%20Razvan%20Caramalau%20and%20Philip%20J.%20%22Eddie%22%20Edwards%20and%20Brian%20Dromey%20and%20Matthew%20J.%20Clarkson%20and%20Danail%20Stoyanov%0AAbstract%3A%20%20%20We%20present%20HUP-3D%2C%20a%203D%20multi-view%20multi-modal%20synthetic%20dataset%20for%0Ahand-ultrasound%20%28US%29%20probe%20pose%20estimation%20in%20the%20context%20of%20obstetric%0Aultrasound.%20Egocentric%20markerless%203D%20joint%20pose%20estimation%20has%20potential%0Aapplications%20in%20mixed%20reality%20based%20medical%20education.%20The%20ability%20to%0Aunderstand%20hand%20and%20probe%20movements%20programmatically%20opens%20the%20door%20to%20tailored%0Aguidance%20and%20mentoring%20applications.%20Our%20dataset%20consists%20of%20over%2031k%20sets%20of%0ARGB%2C%20depth%20and%20segmentation%20mask%20frames%2C%20including%20pose%20related%20ground%20truth%0Adata%2C%20with%20a%20strong%20emphasis%20on%20image%20diversity%20and%20complexity.%20Adopting%20a%0Acamera%20viewpoint-based%20sphere%20concept%20allows%20us%20to%20capture%20a%20variety%20of%20views%0Aand%20generate%20multiple%20hand%20grasp%20poses%20using%20a%20pre-trained%20network.%0AAdditionally%2C%20our%20approach%20includes%20a%20software-based%20image%20rendering%20concept%2C%0Aenhancing%20diversity%20with%20various%20hand%20and%20arm%20textures%2C%20lighting%20conditions%2C%0Aand%20background%20images.%20Furthermore%2C%20we%20validated%20our%20proposed%20dataset%20with%0Astate-of-the-art%20learning%20models%20and%20we%20obtained%20the%20lowest%20hand-object%0Akeypoint%20errors.%20The%20dataset%20and%20other%20details%20are%20provided%20with%20the%0Asupplementary%20material.%20The%20source%20code%20of%20our%20grasp%20generation%20and%20rendering%0Apipeline%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHUP-3D%253A%2520A%25203D%2520multi-view%2520synthetic%2520dataset%2520for%2520assisted-egocentric%250A%2520%2520hand-ultrasound%2520pose%2520estimation%26entry.906535625%3DManuel%2520Birlo%2520and%2520Razvan%2520Caramalau%2520and%2520Philip%2520J.%2520%2522Eddie%2522%2520Edwards%2520and%2520Brian%2520Dromey%2520and%2520Matthew%2520J.%2520Clarkson%2520and%2520Danail%2520Stoyanov%26entry.1292438233%3D%2520%2520We%2520present%2520HUP-3D%252C%2520a%25203D%2520multi-view%2520multi-modal%2520synthetic%2520dataset%2520for%250Ahand-ultrasound%2520%2528US%2529%2520probe%2520pose%2520estimation%2520in%2520the%2520context%2520of%2520obstetric%250Aultrasound.%2520Egocentric%2520markerless%25203D%2520joint%2520pose%2520estimation%2520has%2520potential%250Aapplications%2520in%2520mixed%2520reality%2520based%2520medical%2520education.%2520The%2520ability%2520to%250Aunderstand%2520hand%2520and%2520probe%2520movements%2520programmatically%2520opens%2520the%2520door%2520to%2520tailored%250Aguidance%2520and%2520mentoring%2520applications.%2520Our%2520dataset%2520consists%2520of%2520over%252031k%2520sets%2520of%250ARGB%252C%2520depth%2520and%2520segmentation%2520mask%2520frames%252C%2520including%2520pose%2520related%2520ground%2520truth%250Adata%252C%2520with%2520a%2520strong%2520emphasis%2520on%2520image%2520diversity%2520and%2520complexity.%2520Adopting%2520a%250Acamera%2520viewpoint-based%2520sphere%2520concept%2520allows%2520us%2520to%2520capture%2520a%2520variety%2520of%2520views%250Aand%2520generate%2520multiple%2520hand%2520grasp%2520poses%2520using%2520a%2520pre-trained%2520network.%250AAdditionally%252C%2520our%2520approach%2520includes%2520a%2520software-based%2520image%2520rendering%2520concept%252C%250Aenhancing%2520diversity%2520with%2520various%2520hand%2520and%2520arm%2520textures%252C%2520lighting%2520conditions%252C%250Aand%2520background%2520images.%2520Furthermore%252C%2520we%2520validated%2520our%2520proposed%2520dataset%2520with%250Astate-of-the-art%2520learning%2520models%2520and%2520we%2520obtained%2520the%2520lowest%2520hand-object%250Akeypoint%2520errors.%2520The%2520dataset%2520and%2520other%2520details%2520are%2520provided%2520with%2520the%250Asupplementary%2520material.%2520The%2520source%2520code%2520of%2520our%2520grasp%2520generation%2520and%2520rendering%250Apipeline%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HUP-3D%3A%20A%203D%20multi-view%20synthetic%20dataset%20for%20assisted-egocentric%0A%20%20hand-ultrasound%20pose%20estimation&entry.906535625=Manuel%20Birlo%20and%20Razvan%20Caramalau%20and%20Philip%20J.%20%22Eddie%22%20Edwards%20and%20Brian%20Dromey%20and%20Matthew%20J.%20Clarkson%20and%20Danail%20Stoyanov&entry.1292438233=%20%20We%20present%20HUP-3D%2C%20a%203D%20multi-view%20multi-modal%20synthetic%20dataset%20for%0Ahand-ultrasound%20%28US%29%20probe%20pose%20estimation%20in%20the%20context%20of%20obstetric%0Aultrasound.%20Egocentric%20markerless%203D%20joint%20pose%20estimation%20has%20potential%0Aapplications%20in%20mixed%20reality%20based%20medical%20education.%20The%20ability%20to%0Aunderstand%20hand%20and%20probe%20movements%20programmatically%20opens%20the%20door%20to%20tailored%0Aguidance%20and%20mentoring%20applications.%20Our%20dataset%20consists%20of%20over%2031k%20sets%20of%0ARGB%2C%20depth%20and%20segmentation%20mask%20frames%2C%20including%20pose%20related%20ground%20truth%0Adata%2C%20with%20a%20strong%20emphasis%20on%20image%20diversity%20and%20complexity.%20Adopting%20a%0Acamera%20viewpoint-based%20sphere%20concept%20allows%20us%20to%20capture%20a%20variety%20of%20views%0Aand%20generate%20multiple%20hand%20grasp%20poses%20using%20a%20pre-trained%20network.%0AAdditionally%2C%20our%20approach%20includes%20a%20software-based%20image%20rendering%20concept%2C%0Aenhancing%20diversity%20with%20various%20hand%20and%20arm%20textures%2C%20lighting%20conditions%2C%0Aand%20background%20images.%20Furthermore%2C%20we%20validated%20our%20proposed%20dataset%20with%0Astate-of-the-art%20learning%20models%20and%20we%20obtained%20the%20lowest%20hand-object%0Akeypoint%20errors.%20The%20dataset%20and%20other%20details%20are%20provided%20with%20the%0Asupplementary%20material.%20The%20source%20code%20of%20our%20grasp%20generation%20and%20rendering%0Apipeline%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09215v1&entry.124074799=Read"},
{"title": "Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban\n  Environments", "author": "Djamahl Etchegaray and Zi Huang and Tatsuya Harada and Yadan Luo", "abstract": "  In this work, we tackle the limitations of current LiDAR-based 3D object\ndetection systems, which are hindered by a restricted class vocabulary and the\nhigh costs associated with annotating new object classes. Our exploration of\nopen-vocabulary (OV) learning in urban environments aims to capture novel\ninstances using pre-trained vision-language models (VLMs) with multi-sensor\ndata. We design and benchmark a set of four potential solutions as baselines,\ncategorizing them into either top-down or bottom-up approaches based on their\ninput data strategies. While effective, these methods exhibit certain\nlimitations, such as missing novel objects in 3D box estimation or applying\nrigorous priors, leading to biases towards objects near the camera or of\nrectangular geometries. To overcome these limitations, we introduce a universal\n\\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the\nrecall of novel objects and propagating this detection capability to more\ndistant areas thereby progressively capturing more. In particular, we utilize a\ngreedy box seeker to search against 3D novel boxes of varying orientations and\ndepth in each generated frustum and ensure the reliability of newly identified\nboxes by cross alignment and density ranker. Additionally, the inherent bias\ntowards camera-proximal objects is alleviated by the proposed remote simulator,\nwhich randomly diversifies pseudo-labeled novel instances in the self-training\nprocess, combined with the fusion of base samples in the memory bank. Extensive\nexperiments demonstrate a 53% improvement in novel recall across diverse OV\nsettings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold\nincrease in Average Precision (AP) for novel object classes. The source code is\nmade available at https://github.com/djamahl99/findnpropagate.\n", "link": "http://arxiv.org/abs/2403.13556v2", "date": "2024-07-12", "relevancy": 2.3433, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5874}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5854}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Find%20n%27%20Propagate%3A%20Open-Vocabulary%203D%20Object%20Detection%20in%20Urban%0A%20%20Environments&body=Title%3A%20Find%20n%27%20Propagate%3A%20Open-Vocabulary%203D%20Object%20Detection%20in%20Urban%0A%20%20Environments%0AAuthor%3A%20Djamahl%20Etchegaray%20and%20Zi%20Huang%20and%20Tatsuya%20Harada%20and%20Yadan%20Luo%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20tackle%20the%20limitations%20of%20current%20LiDAR-based%203D%20object%0Adetection%20systems%2C%20which%20are%20hindered%20by%20a%20restricted%20class%20vocabulary%20and%20the%0Ahigh%20costs%20associated%20with%20annotating%20new%20object%20classes.%20Our%20exploration%20of%0Aopen-vocabulary%20%28OV%29%20learning%20in%20urban%20environments%20aims%20to%20capture%20novel%0Ainstances%20using%20pre-trained%20vision-language%20models%20%28VLMs%29%20with%20multi-sensor%0Adata.%20We%20design%20and%20benchmark%20a%20set%20of%20four%20potential%20solutions%20as%20baselines%2C%0Acategorizing%20them%20into%20either%20top-down%20or%20bottom-up%20approaches%20based%20on%20their%0Ainput%20data%20strategies.%20While%20effective%2C%20these%20methods%20exhibit%20certain%0Alimitations%2C%20such%20as%20missing%20novel%20objects%20in%203D%20box%20estimation%20or%20applying%0Arigorous%20priors%2C%20leading%20to%20biases%20towards%20objects%20near%20the%20camera%20or%20of%0Arectangular%20geometries.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20a%20universal%0A%5Ctextsc%7BFind%20n%27%20Propagate%7D%20approach%20for%203D%20OV%20tasks%2C%20aimed%20at%20maximizing%20the%0Arecall%20of%20novel%20objects%20and%20propagating%20this%20detection%20capability%20to%20more%0Adistant%20areas%20thereby%20progressively%20capturing%20more.%20In%20particular%2C%20we%20utilize%20a%0Agreedy%20box%20seeker%20to%20search%20against%203D%20novel%20boxes%20of%20varying%20orientations%20and%0Adepth%20in%20each%20generated%20frustum%20and%20ensure%20the%20reliability%20of%20newly%20identified%0Aboxes%20by%20cross%20alignment%20and%20density%20ranker.%20Additionally%2C%20the%20inherent%20bias%0Atowards%20camera-proximal%20objects%20is%20alleviated%20by%20the%20proposed%20remote%20simulator%2C%0Awhich%20randomly%20diversifies%20pseudo-labeled%20novel%20instances%20in%20the%20self-training%0Aprocess%2C%20combined%20with%20the%20fusion%20of%20base%20samples%20in%20the%20memory%20bank.%20Extensive%0Aexperiments%20demonstrate%20a%2053%25%20improvement%20in%20novel%20recall%20across%20diverse%20OV%0Asettings%2C%20VLMs%2C%20and%203D%20detectors.%20Notably%2C%20we%20achieve%20up%20to%20a%203.97-fold%0Aincrease%20in%20Average%20Precision%20%28AP%29%20for%20novel%20object%20classes.%20The%20source%20code%20is%0Amade%20available%20at%20https%3A//github.com/djamahl99/findnpropagate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13556v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFind%2520n%2527%2520Propagate%253A%2520Open-Vocabulary%25203D%2520Object%2520Detection%2520in%2520Urban%250A%2520%2520Environments%26entry.906535625%3DDjamahl%2520Etchegaray%2520and%2520Zi%2520Huang%2520and%2520Tatsuya%2520Harada%2520and%2520Yadan%2520Luo%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520limitations%2520of%2520current%2520LiDAR-based%25203D%2520object%250Adetection%2520systems%252C%2520which%2520are%2520hindered%2520by%2520a%2520restricted%2520class%2520vocabulary%2520and%2520the%250Ahigh%2520costs%2520associated%2520with%2520annotating%2520new%2520object%2520classes.%2520Our%2520exploration%2520of%250Aopen-vocabulary%2520%2528OV%2529%2520learning%2520in%2520urban%2520environments%2520aims%2520to%2520capture%2520novel%250Ainstances%2520using%2520pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520with%2520multi-sensor%250Adata.%2520We%2520design%2520and%2520benchmark%2520a%2520set%2520of%2520four%2520potential%2520solutions%2520as%2520baselines%252C%250Acategorizing%2520them%2520into%2520either%2520top-down%2520or%2520bottom-up%2520approaches%2520based%2520on%2520their%250Ainput%2520data%2520strategies.%2520While%2520effective%252C%2520these%2520methods%2520exhibit%2520certain%250Alimitations%252C%2520such%2520as%2520missing%2520novel%2520objects%2520in%25203D%2520box%2520estimation%2520or%2520applying%250Arigorous%2520priors%252C%2520leading%2520to%2520biases%2520towards%2520objects%2520near%2520the%2520camera%2520or%2520of%250Arectangular%2520geometries.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520a%2520universal%250A%255Ctextsc%257BFind%2520n%2527%2520Propagate%257D%2520approach%2520for%25203D%2520OV%2520tasks%252C%2520aimed%2520at%2520maximizing%2520the%250Arecall%2520of%2520novel%2520objects%2520and%2520propagating%2520this%2520detection%2520capability%2520to%2520more%250Adistant%2520areas%2520thereby%2520progressively%2520capturing%2520more.%2520In%2520particular%252C%2520we%2520utilize%2520a%250Agreedy%2520box%2520seeker%2520to%2520search%2520against%25203D%2520novel%2520boxes%2520of%2520varying%2520orientations%2520and%250Adepth%2520in%2520each%2520generated%2520frustum%2520and%2520ensure%2520the%2520reliability%2520of%2520newly%2520identified%250Aboxes%2520by%2520cross%2520alignment%2520and%2520density%2520ranker.%2520Additionally%252C%2520the%2520inherent%2520bias%250Atowards%2520camera-proximal%2520objects%2520is%2520alleviated%2520by%2520the%2520proposed%2520remote%2520simulator%252C%250Awhich%2520randomly%2520diversifies%2520pseudo-labeled%2520novel%2520instances%2520in%2520the%2520self-training%250Aprocess%252C%2520combined%2520with%2520the%2520fusion%2520of%2520base%2520samples%2520in%2520the%2520memory%2520bank.%2520Extensive%250Aexperiments%2520demonstrate%2520a%252053%2525%2520improvement%2520in%2520novel%2520recall%2520across%2520diverse%2520OV%250Asettings%252C%2520VLMs%252C%2520and%25203D%2520detectors.%2520Notably%252C%2520we%2520achieve%2520up%2520to%2520a%25203.97-fold%250Aincrease%2520in%2520Average%2520Precision%2520%2528AP%2529%2520for%2520novel%2520object%2520classes.%2520The%2520source%2520code%2520is%250Amade%2520available%2520at%2520https%253A//github.com/djamahl99/findnpropagate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13556v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Find%20n%27%20Propagate%3A%20Open-Vocabulary%203D%20Object%20Detection%20in%20Urban%0A%20%20Environments&entry.906535625=Djamahl%20Etchegaray%20and%20Zi%20Huang%20and%20Tatsuya%20Harada%20and%20Yadan%20Luo&entry.1292438233=%20%20In%20this%20work%2C%20we%20tackle%20the%20limitations%20of%20current%20LiDAR-based%203D%20object%0Adetection%20systems%2C%20which%20are%20hindered%20by%20a%20restricted%20class%20vocabulary%20and%20the%0Ahigh%20costs%20associated%20with%20annotating%20new%20object%20classes.%20Our%20exploration%20of%0Aopen-vocabulary%20%28OV%29%20learning%20in%20urban%20environments%20aims%20to%20capture%20novel%0Ainstances%20using%20pre-trained%20vision-language%20models%20%28VLMs%29%20with%20multi-sensor%0Adata.%20We%20design%20and%20benchmark%20a%20set%20of%20four%20potential%20solutions%20as%20baselines%2C%0Acategorizing%20them%20into%20either%20top-down%20or%20bottom-up%20approaches%20based%20on%20their%0Ainput%20data%20strategies.%20While%20effective%2C%20these%20methods%20exhibit%20certain%0Alimitations%2C%20such%20as%20missing%20novel%20objects%20in%203D%20box%20estimation%20or%20applying%0Arigorous%20priors%2C%20leading%20to%20biases%20towards%20objects%20near%20the%20camera%20or%20of%0Arectangular%20geometries.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20a%20universal%0A%5Ctextsc%7BFind%20n%27%20Propagate%7D%20approach%20for%203D%20OV%20tasks%2C%20aimed%20at%20maximizing%20the%0Arecall%20of%20novel%20objects%20and%20propagating%20this%20detection%20capability%20to%20more%0Adistant%20areas%20thereby%20progressively%20capturing%20more.%20In%20particular%2C%20we%20utilize%20a%0Agreedy%20box%20seeker%20to%20search%20against%203D%20novel%20boxes%20of%20varying%20orientations%20and%0Adepth%20in%20each%20generated%20frustum%20and%20ensure%20the%20reliability%20of%20newly%20identified%0Aboxes%20by%20cross%20alignment%20and%20density%20ranker.%20Additionally%2C%20the%20inherent%20bias%0Atowards%20camera-proximal%20objects%20is%20alleviated%20by%20the%20proposed%20remote%20simulator%2C%0Awhich%20randomly%20diversifies%20pseudo-labeled%20novel%20instances%20in%20the%20self-training%0Aprocess%2C%20combined%20with%20the%20fusion%20of%20base%20samples%20in%20the%20memory%20bank.%20Extensive%0Aexperiments%20demonstrate%20a%2053%25%20improvement%20in%20novel%20recall%20across%20diverse%20OV%0Asettings%2C%20VLMs%2C%20and%203D%20detectors.%20Notably%2C%20we%20achieve%20up%20to%20a%203.97-fold%0Aincrease%20in%20Average%20Precision%20%28AP%29%20for%20novel%20object%20classes.%20The%20source%20code%20is%0Amade%20available%20at%20https%3A//github.com/djamahl99/findnpropagate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13556v2&entry.124074799=Read"},
{"title": "CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features\n  for a Disentangled, Interpretable, and Controllable Text-Guided Face\n  Manipulation", "author": "Chenliang Zhou and Fangcheng Zhong and Cengiz Oztireli", "abstract": "  Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges\nimages and text by embedding them into a joint latent space. This opens the\ndoor to ample literature that aims to manipulate an input image by providing a\ntextual explanation. However, due to the discrepancy between image and text\nembeddings in the joint space, using text embeddings as the optimization target\noften introduces undesired artifacts in the resulting images. Disentanglement,\ninterpretability, and controllability are also hard to guarantee for\nmanipulation. To alleviate these problems, we propose to define corpus\nsubspaces spanned by relevant prompts to capture specific image\ncharacteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as\nan optimization target to improve the performance of text-guided image\nmanipulation. Our method is a simple and general paradigm that can be easily\ncomputed and adapted, and smoothly incorporated into any CLIP-based image\nmanipulation algorithm. To demonstrate the effectiveness of our method, we\nconduct several theoretical and empirical studies. As a case study, we utilize\nthe method for text-guided semantic face editing. We quantitatively and\nqualitatively demonstrate that PAE facilitates a more disentangled,\ninterpretable, and controllable image manipulation with state-of-the-art\nquality and accuracy. Project page: https://chenliang-zhou.github.io/CLIP-PAE/.\n", "link": "http://arxiv.org/abs/2210.03919v5", "date": "2024-07-12", "relevancy": 2.339, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6128}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5715}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-PAE%3A%20Projection-Augmentation%20Embedding%20to%20Extract%20Relevant%20Features%0A%20%20for%20a%20Disentangled%2C%20Interpretable%2C%20and%20Controllable%20Text-Guided%20Face%0A%20%20Manipulation&body=Title%3A%20CLIP-PAE%3A%20Projection-Augmentation%20Embedding%20to%20Extract%20Relevant%20Features%0A%20%20for%20a%20Disentangled%2C%20Interpretable%2C%20and%20Controllable%20Text-Guided%20Face%0A%20%20Manipulation%0AAuthor%3A%20Chenliang%20Zhou%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli%0AAbstract%3A%20%20%20Recently%20introduced%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20bridges%0Aimages%20and%20text%20by%20embedding%20them%20into%20a%20joint%20latent%20space.%20This%20opens%20the%0Adoor%20to%20ample%20literature%20that%20aims%20to%20manipulate%20an%20input%20image%20by%20providing%20a%0Atextual%20explanation.%20However%2C%20due%20to%20the%20discrepancy%20between%20image%20and%20text%0Aembeddings%20in%20the%20joint%20space%2C%20using%20text%20embeddings%20as%20the%20optimization%20target%0Aoften%20introduces%20undesired%20artifacts%20in%20the%20resulting%20images.%20Disentanglement%2C%0Ainterpretability%2C%20and%20controllability%20are%20also%20hard%20to%20guarantee%20for%0Amanipulation.%20To%20alleviate%20these%20problems%2C%20we%20propose%20to%20define%20corpus%0Asubspaces%20spanned%20by%20relevant%20prompts%20to%20capture%20specific%20image%0Acharacteristics.%20We%20introduce%20CLIP%20Projection-Augmentation%20Embedding%20%28PAE%29%20as%0Aan%20optimization%20target%20to%20improve%20the%20performance%20of%20text-guided%20image%0Amanipulation.%20Our%20method%20is%20a%20simple%20and%20general%20paradigm%20that%20can%20be%20easily%0Acomputed%20and%20adapted%2C%20and%20smoothly%20incorporated%20into%20any%20CLIP-based%20image%0Amanipulation%20algorithm.%20To%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20we%0Aconduct%20several%20theoretical%20and%20empirical%20studies.%20As%20a%20case%20study%2C%20we%20utilize%0Athe%20method%20for%20text-guided%20semantic%20face%20editing.%20We%20quantitatively%20and%0Aqualitatively%20demonstrate%20that%20PAE%20facilitates%20a%20more%20disentangled%2C%0Ainterpretable%2C%20and%20controllable%20image%20manipulation%20with%20state-of-the-art%0Aquality%20and%20accuracy.%20Project%20page%3A%20https%3A//chenliang-zhou.github.io/CLIP-PAE/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.03919v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-PAE%253A%2520Projection-Augmentation%2520Embedding%2520to%2520Extract%2520Relevant%2520Features%250A%2520%2520for%2520a%2520Disentangled%252C%2520Interpretable%252C%2520and%2520Controllable%2520Text-Guided%2520Face%250A%2520%2520Manipulation%26entry.906535625%3DChenliang%2520Zhou%2520and%2520Fangcheng%2520Zhong%2520and%2520Cengiz%2520Oztireli%26entry.1292438233%3D%2520%2520Recently%2520introduced%2520Contrastive%2520Language-Image%2520Pre-Training%2520%2528CLIP%2529%2520bridges%250Aimages%2520and%2520text%2520by%2520embedding%2520them%2520into%2520a%2520joint%2520latent%2520space.%2520This%2520opens%2520the%250Adoor%2520to%2520ample%2520literature%2520that%2520aims%2520to%2520manipulate%2520an%2520input%2520image%2520by%2520providing%2520a%250Atextual%2520explanation.%2520However%252C%2520due%2520to%2520the%2520discrepancy%2520between%2520image%2520and%2520text%250Aembeddings%2520in%2520the%2520joint%2520space%252C%2520using%2520text%2520embeddings%2520as%2520the%2520optimization%2520target%250Aoften%2520introduces%2520undesired%2520artifacts%2520in%2520the%2520resulting%2520images.%2520Disentanglement%252C%250Ainterpretability%252C%2520and%2520controllability%2520are%2520also%2520hard%2520to%2520guarantee%2520for%250Amanipulation.%2520To%2520alleviate%2520these%2520problems%252C%2520we%2520propose%2520to%2520define%2520corpus%250Asubspaces%2520spanned%2520by%2520relevant%2520prompts%2520to%2520capture%2520specific%2520image%250Acharacteristics.%2520We%2520introduce%2520CLIP%2520Projection-Augmentation%2520Embedding%2520%2528PAE%2529%2520as%250Aan%2520optimization%2520target%2520to%2520improve%2520the%2520performance%2520of%2520text-guided%2520image%250Amanipulation.%2520Our%2520method%2520is%2520a%2520simple%2520and%2520general%2520paradigm%2520that%2520can%2520be%2520easily%250Acomputed%2520and%2520adapted%252C%2520and%2520smoothly%2520incorporated%2520into%2520any%2520CLIP-based%2520image%250Amanipulation%2520algorithm.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520we%250Aconduct%2520several%2520theoretical%2520and%2520empirical%2520studies.%2520As%2520a%2520case%2520study%252C%2520we%2520utilize%250Athe%2520method%2520for%2520text-guided%2520semantic%2520face%2520editing.%2520We%2520quantitatively%2520and%250Aqualitatively%2520demonstrate%2520that%2520PAE%2520facilitates%2520a%2520more%2520disentangled%252C%250Ainterpretable%252C%2520and%2520controllable%2520image%2520manipulation%2520with%2520state-of-the-art%250Aquality%2520and%2520accuracy.%2520Project%2520page%253A%2520https%253A//chenliang-zhou.github.io/CLIP-PAE/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.03919v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-PAE%3A%20Projection-Augmentation%20Embedding%20to%20Extract%20Relevant%20Features%0A%20%20for%20a%20Disentangled%2C%20Interpretable%2C%20and%20Controllable%20Text-Guided%20Face%0A%20%20Manipulation&entry.906535625=Chenliang%20Zhou%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli&entry.1292438233=%20%20Recently%20introduced%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20bridges%0Aimages%20and%20text%20by%20embedding%20them%20into%20a%20joint%20latent%20space.%20This%20opens%20the%0Adoor%20to%20ample%20literature%20that%20aims%20to%20manipulate%20an%20input%20image%20by%20providing%20a%0Atextual%20explanation.%20However%2C%20due%20to%20the%20discrepancy%20between%20image%20and%20text%0Aembeddings%20in%20the%20joint%20space%2C%20using%20text%20embeddings%20as%20the%20optimization%20target%0Aoften%20introduces%20undesired%20artifacts%20in%20the%20resulting%20images.%20Disentanglement%2C%0Ainterpretability%2C%20and%20controllability%20are%20also%20hard%20to%20guarantee%20for%0Amanipulation.%20To%20alleviate%20these%20problems%2C%20we%20propose%20to%20define%20corpus%0Asubspaces%20spanned%20by%20relevant%20prompts%20to%20capture%20specific%20image%0Acharacteristics.%20We%20introduce%20CLIP%20Projection-Augmentation%20Embedding%20%28PAE%29%20as%0Aan%20optimization%20target%20to%20improve%20the%20performance%20of%20text-guided%20image%0Amanipulation.%20Our%20method%20is%20a%20simple%20and%20general%20paradigm%20that%20can%20be%20easily%0Acomputed%20and%20adapted%2C%20and%20smoothly%20incorporated%20into%20any%20CLIP-based%20image%0Amanipulation%20algorithm.%20To%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20we%0Aconduct%20several%20theoretical%20and%20empirical%20studies.%20As%20a%20case%20study%2C%20we%20utilize%0Athe%20method%20for%20text-guided%20semantic%20face%20editing.%20We%20quantitatively%20and%0Aqualitatively%20demonstrate%20that%20PAE%20facilitates%20a%20more%20disentangled%2C%0Ainterpretable%2C%20and%20controllable%20image%20manipulation%20with%20state-of-the-art%0Aquality%20and%20accuracy.%20Project%20page%3A%20https%3A//chenliang-zhou.github.io/CLIP-PAE/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.03919v5&entry.124074799=Read"},
{"title": "Novel clustered federated learning based on local loss", "author": "Endong Gu and Yongxin Chen and Hao Wen and Xingju Cai and Deren Han", "abstract": "  This paper proposes LCFL, a novel clustering metric for evaluating clients'\ndata distributions in federated learning. LCFL aligns with federated learning\nrequirements, accurately assessing client-to-client variations in data\ndistribution. It offers advantages over existing clustered federated learning\nmethods, addressing privacy concerns, improving applicability to non-convex\nmodels, and providing more accurate classification results. LCFL does not\nrequire prior knowledge of clients' data distributions. We provide a rigorous\nmathematical analysis, demonstrating the correctness and feasibility of our\nframework. Numerical experiments with neural network instances highlight the\nsuperior performance of LCFL over baselines on several clustered federated\nlearning benchmarks.\n", "link": "http://arxiv.org/abs/2407.09360v1", "date": "2024-07-12", "relevancy": 2.3085, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4637}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20clustered%20federated%20learning%20based%20on%20local%20loss&body=Title%3A%20Novel%20clustered%20federated%20learning%20based%20on%20local%20loss%0AAuthor%3A%20Endong%20Gu%20and%20Yongxin%20Chen%20and%20Hao%20Wen%20and%20Xingju%20Cai%20and%20Deren%20Han%0AAbstract%3A%20%20%20This%20paper%20proposes%20LCFL%2C%20a%20novel%20clustering%20metric%20for%20evaluating%20clients%27%0Adata%20distributions%20in%20federated%20learning.%20LCFL%20aligns%20with%20federated%20learning%0Arequirements%2C%20accurately%20assessing%20client-to-client%20variations%20in%20data%0Adistribution.%20It%20offers%20advantages%20over%20existing%20clustered%20federated%20learning%0Amethods%2C%20addressing%20privacy%20concerns%2C%20improving%20applicability%20to%20non-convex%0Amodels%2C%20and%20providing%20more%20accurate%20classification%20results.%20LCFL%20does%20not%0Arequire%20prior%20knowledge%20of%20clients%27%20data%20distributions.%20We%20provide%20a%20rigorous%0Amathematical%20analysis%2C%20demonstrating%20the%20correctness%20and%20feasibility%20of%20our%0Aframework.%20Numerical%20experiments%20with%20neural%20network%20instances%20highlight%20the%0Asuperior%20performance%20of%20LCFL%20over%20baselines%20on%20several%20clustered%20federated%0Alearning%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520clustered%2520federated%2520learning%2520based%2520on%2520local%2520loss%26entry.906535625%3DEndong%2520Gu%2520and%2520Yongxin%2520Chen%2520and%2520Hao%2520Wen%2520and%2520Xingju%2520Cai%2520and%2520Deren%2520Han%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520LCFL%252C%2520a%2520novel%2520clustering%2520metric%2520for%2520evaluating%2520clients%2527%250Adata%2520distributions%2520in%2520federated%2520learning.%2520LCFL%2520aligns%2520with%2520federated%2520learning%250Arequirements%252C%2520accurately%2520assessing%2520client-to-client%2520variations%2520in%2520data%250Adistribution.%2520It%2520offers%2520advantages%2520over%2520existing%2520clustered%2520federated%2520learning%250Amethods%252C%2520addressing%2520privacy%2520concerns%252C%2520improving%2520applicability%2520to%2520non-convex%250Amodels%252C%2520and%2520providing%2520more%2520accurate%2520classification%2520results.%2520LCFL%2520does%2520not%250Arequire%2520prior%2520knowledge%2520of%2520clients%2527%2520data%2520distributions.%2520We%2520provide%2520a%2520rigorous%250Amathematical%2520analysis%252C%2520demonstrating%2520the%2520correctness%2520and%2520feasibility%2520of%2520our%250Aframework.%2520Numerical%2520experiments%2520with%2520neural%2520network%2520instances%2520highlight%2520the%250Asuperior%2520performance%2520of%2520LCFL%2520over%2520baselines%2520on%2520several%2520clustered%2520federated%250Alearning%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20clustered%20federated%20learning%20based%20on%20local%20loss&entry.906535625=Endong%20Gu%20and%20Yongxin%20Chen%20and%20Hao%20Wen%20and%20Xingju%20Cai%20and%20Deren%20Han&entry.1292438233=%20%20This%20paper%20proposes%20LCFL%2C%20a%20novel%20clustering%20metric%20for%20evaluating%20clients%27%0Adata%20distributions%20in%20federated%20learning.%20LCFL%20aligns%20with%20federated%20learning%0Arequirements%2C%20accurately%20assessing%20client-to-client%20variations%20in%20data%0Adistribution.%20It%20offers%20advantages%20over%20existing%20clustered%20federated%20learning%0Amethods%2C%20addressing%20privacy%20concerns%2C%20improving%20applicability%20to%20non-convex%0Amodels%2C%20and%20providing%20more%20accurate%20classification%20results.%20LCFL%20does%20not%0Arequire%20prior%20knowledge%20of%20clients%27%20data%20distributions.%20We%20provide%20a%20rigorous%0Amathematical%20analysis%2C%20demonstrating%20the%20correctness%20and%20feasibility%20of%20our%0Aframework.%20Numerical%20experiments%20with%20neural%20network%20instances%20highlight%20the%0Asuperior%20performance%20of%20LCFL%20over%20baselines%20on%20several%20clustered%20federated%0Alearning%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09360v1&entry.124074799=Read"},
{"title": "TAPI: Towards Target-Specific and Adversarial Prompt Injection against\n  Code LLMs", "author": "Yuchen Yang and Hongwei Yao and Bingrun Yang and Yiling He and Yiming Li and Tianwei Zhang and Zhan Qin and Kui Ren", "abstract": "  Recently, code-oriented large language models (Code LLMs) have been widely\nand successfully used to simplify and facilitate code programming. With these\ntools, developers can easily generate desired complete functional codes based\non incomplete code and natural language prompts. However, a few pioneering\nworks revealed that these Code LLMs are also vulnerable, e.g., against backdoor\nand adversarial attacks. The former could induce LLMs to respond to triggers to\ninsert malicious code snippets by poisoning the training data or model\nparameters, while the latter can craft malicious adversarial input codes to\nreduce the quality of generated codes. However, both attack methods have\nunderlying limitations: backdoor attacks rely on controlling the model training\nprocess, while adversarial attacks struggle with fulfilling specific malicious\npurposes.\n  To inherit the advantages of both backdoor and adversarial attacks, this\npaper proposes a new attack paradigm, i.e., target-specific and adversarial\nprompt injection (TAPI), against Code LLMs. TAPI generates unreadable comments\ncontaining information about malicious instructions and hides them as triggers\nin the external source code. When users exploit Code LLMs to complete codes\ncontaining the trigger, the models will generate attacker-specified malicious\ncode snippets at specific locations. We evaluate our TAPI attack on four\nrepresentative LLMs under three representative malicious objectives and seven\ncases. The results show that our method is highly threatening (achieving an\nattack success rate of up to 89.3\\%) and stealthy (saving an average of 53.1\\%\nof tokens in the trigger design). In particular, we successfully attack some\nfamous deployed code completion integrated applications, including CodeGeex and\nGithub Copilot. This further confirms the realistic threat of our attack.\n", "link": "http://arxiv.org/abs/2407.09164v1", "date": "2024-07-12", "relevancy": 2.2701, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4959}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4363}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAPI%3A%20Towards%20Target-Specific%20and%20Adversarial%20Prompt%20Injection%20against%0A%20%20Code%20LLMs&body=Title%3A%20TAPI%3A%20Towards%20Target-Specific%20and%20Adversarial%20Prompt%20Injection%20against%0A%20%20Code%20LLMs%0AAuthor%3A%20Yuchen%20Yang%20and%20Hongwei%20Yao%20and%20Bingrun%20Yang%20and%20Yiling%20He%20and%20Yiming%20Li%20and%20Tianwei%20Zhang%20and%20Zhan%20Qin%20and%20Kui%20Ren%0AAbstract%3A%20%20%20Recently%2C%20code-oriented%20large%20language%20models%20%28Code%20LLMs%29%20have%20been%20widely%0Aand%20successfully%20used%20to%20simplify%20and%20facilitate%20code%20programming.%20With%20these%0Atools%2C%20developers%20can%20easily%20generate%20desired%20complete%20functional%20codes%20based%0Aon%20incomplete%20code%20and%20natural%20language%20prompts.%20However%2C%20a%20few%20pioneering%0Aworks%20revealed%20that%20these%20Code%20LLMs%20are%20also%20vulnerable%2C%20e.g.%2C%20against%20backdoor%0Aand%20adversarial%20attacks.%20The%20former%20could%20induce%20LLMs%20to%20respond%20to%20triggers%20to%0Ainsert%20malicious%20code%20snippets%20by%20poisoning%20the%20training%20data%20or%20model%0Aparameters%2C%20while%20the%20latter%20can%20craft%20malicious%20adversarial%20input%20codes%20to%0Areduce%20the%20quality%20of%20generated%20codes.%20However%2C%20both%20attack%20methods%20have%0Aunderlying%20limitations%3A%20backdoor%20attacks%20rely%20on%20controlling%20the%20model%20training%0Aprocess%2C%20while%20adversarial%20attacks%20struggle%20with%20fulfilling%20specific%20malicious%0Apurposes.%0A%20%20To%20inherit%20the%20advantages%20of%20both%20backdoor%20and%20adversarial%20attacks%2C%20this%0Apaper%20proposes%20a%20new%20attack%20paradigm%2C%20i.e.%2C%20target-specific%20and%20adversarial%0Aprompt%20injection%20%28TAPI%29%2C%20against%20Code%20LLMs.%20TAPI%20generates%20unreadable%20comments%0Acontaining%20information%20about%20malicious%20instructions%20and%20hides%20them%20as%20triggers%0Ain%20the%20external%20source%20code.%20When%20users%20exploit%20Code%20LLMs%20to%20complete%20codes%0Acontaining%20the%20trigger%2C%20the%20models%20will%20generate%20attacker-specified%20malicious%0Acode%20snippets%20at%20specific%20locations.%20We%20evaluate%20our%20TAPI%20attack%20on%20four%0Arepresentative%20LLMs%20under%20three%20representative%20malicious%20objectives%20and%20seven%0Acases.%20The%20results%20show%20that%20our%20method%20is%20highly%20threatening%20%28achieving%20an%0Aattack%20success%20rate%20of%20up%20to%2089.3%5C%25%29%20and%20stealthy%20%28saving%20an%20average%20of%2053.1%5C%25%0Aof%20tokens%20in%20the%20trigger%20design%29.%20In%20particular%2C%20we%20successfully%20attack%20some%0Afamous%20deployed%20code%20completion%20integrated%20applications%2C%20including%20CodeGeex%20and%0AGithub%20Copilot.%20This%20further%20confirms%20the%20realistic%20threat%20of%20our%20attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAPI%253A%2520Towards%2520Target-Specific%2520and%2520Adversarial%2520Prompt%2520Injection%2520against%250A%2520%2520Code%2520LLMs%26entry.906535625%3DYuchen%2520Yang%2520and%2520Hongwei%2520Yao%2520and%2520Bingrun%2520Yang%2520and%2520Yiling%2520He%2520and%2520Yiming%2520Li%2520and%2520Tianwei%2520Zhang%2520and%2520Zhan%2520Qin%2520and%2520Kui%2520Ren%26entry.1292438233%3D%2520%2520Recently%252C%2520code-oriented%2520large%2520language%2520models%2520%2528Code%2520LLMs%2529%2520have%2520been%2520widely%250Aand%2520successfully%2520used%2520to%2520simplify%2520and%2520facilitate%2520code%2520programming.%2520With%2520these%250Atools%252C%2520developers%2520can%2520easily%2520generate%2520desired%2520complete%2520functional%2520codes%2520based%250Aon%2520incomplete%2520code%2520and%2520natural%2520language%2520prompts.%2520However%252C%2520a%2520few%2520pioneering%250Aworks%2520revealed%2520that%2520these%2520Code%2520LLMs%2520are%2520also%2520vulnerable%252C%2520e.g.%252C%2520against%2520backdoor%250Aand%2520adversarial%2520attacks.%2520The%2520former%2520could%2520induce%2520LLMs%2520to%2520respond%2520to%2520triggers%2520to%250Ainsert%2520malicious%2520code%2520snippets%2520by%2520poisoning%2520the%2520training%2520data%2520or%2520model%250Aparameters%252C%2520while%2520the%2520latter%2520can%2520craft%2520malicious%2520adversarial%2520input%2520codes%2520to%250Areduce%2520the%2520quality%2520of%2520generated%2520codes.%2520However%252C%2520both%2520attack%2520methods%2520have%250Aunderlying%2520limitations%253A%2520backdoor%2520attacks%2520rely%2520on%2520controlling%2520the%2520model%2520training%250Aprocess%252C%2520while%2520adversarial%2520attacks%2520struggle%2520with%2520fulfilling%2520specific%2520malicious%250Apurposes.%250A%2520%2520To%2520inherit%2520the%2520advantages%2520of%2520both%2520backdoor%2520and%2520adversarial%2520attacks%252C%2520this%250Apaper%2520proposes%2520a%2520new%2520attack%2520paradigm%252C%2520i.e.%252C%2520target-specific%2520and%2520adversarial%250Aprompt%2520injection%2520%2528TAPI%2529%252C%2520against%2520Code%2520LLMs.%2520TAPI%2520generates%2520unreadable%2520comments%250Acontaining%2520information%2520about%2520malicious%2520instructions%2520and%2520hides%2520them%2520as%2520triggers%250Ain%2520the%2520external%2520source%2520code.%2520When%2520users%2520exploit%2520Code%2520LLMs%2520to%2520complete%2520codes%250Acontaining%2520the%2520trigger%252C%2520the%2520models%2520will%2520generate%2520attacker-specified%2520malicious%250Acode%2520snippets%2520at%2520specific%2520locations.%2520We%2520evaluate%2520our%2520TAPI%2520attack%2520on%2520four%250Arepresentative%2520LLMs%2520under%2520three%2520representative%2520malicious%2520objectives%2520and%2520seven%250Acases.%2520The%2520results%2520show%2520that%2520our%2520method%2520is%2520highly%2520threatening%2520%2528achieving%2520an%250Aattack%2520success%2520rate%2520of%2520up%2520to%252089.3%255C%2525%2529%2520and%2520stealthy%2520%2528saving%2520an%2520average%2520of%252053.1%255C%2525%250Aof%2520tokens%2520in%2520the%2520trigger%2520design%2529.%2520In%2520particular%252C%2520we%2520successfully%2520attack%2520some%250Afamous%2520deployed%2520code%2520completion%2520integrated%2520applications%252C%2520including%2520CodeGeex%2520and%250AGithub%2520Copilot.%2520This%2520further%2520confirms%2520the%2520realistic%2520threat%2520of%2520our%2520attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAPI%3A%20Towards%20Target-Specific%20and%20Adversarial%20Prompt%20Injection%20against%0A%20%20Code%20LLMs&entry.906535625=Yuchen%20Yang%20and%20Hongwei%20Yao%20and%20Bingrun%20Yang%20and%20Yiling%20He%20and%20Yiming%20Li%20and%20Tianwei%20Zhang%20and%20Zhan%20Qin%20and%20Kui%20Ren&entry.1292438233=%20%20Recently%2C%20code-oriented%20large%20language%20models%20%28Code%20LLMs%29%20have%20been%20widely%0Aand%20successfully%20used%20to%20simplify%20and%20facilitate%20code%20programming.%20With%20these%0Atools%2C%20developers%20can%20easily%20generate%20desired%20complete%20functional%20codes%20based%0Aon%20incomplete%20code%20and%20natural%20language%20prompts.%20However%2C%20a%20few%20pioneering%0Aworks%20revealed%20that%20these%20Code%20LLMs%20are%20also%20vulnerable%2C%20e.g.%2C%20against%20backdoor%0Aand%20adversarial%20attacks.%20The%20former%20could%20induce%20LLMs%20to%20respond%20to%20triggers%20to%0Ainsert%20malicious%20code%20snippets%20by%20poisoning%20the%20training%20data%20or%20model%0Aparameters%2C%20while%20the%20latter%20can%20craft%20malicious%20adversarial%20input%20codes%20to%0Areduce%20the%20quality%20of%20generated%20codes.%20However%2C%20both%20attack%20methods%20have%0Aunderlying%20limitations%3A%20backdoor%20attacks%20rely%20on%20controlling%20the%20model%20training%0Aprocess%2C%20while%20adversarial%20attacks%20struggle%20with%20fulfilling%20specific%20malicious%0Apurposes.%0A%20%20To%20inherit%20the%20advantages%20of%20both%20backdoor%20and%20adversarial%20attacks%2C%20this%0Apaper%20proposes%20a%20new%20attack%20paradigm%2C%20i.e.%2C%20target-specific%20and%20adversarial%0Aprompt%20injection%20%28TAPI%29%2C%20against%20Code%20LLMs.%20TAPI%20generates%20unreadable%20comments%0Acontaining%20information%20about%20malicious%20instructions%20and%20hides%20them%20as%20triggers%0Ain%20the%20external%20source%20code.%20When%20users%20exploit%20Code%20LLMs%20to%20complete%20codes%0Acontaining%20the%20trigger%2C%20the%20models%20will%20generate%20attacker-specified%20malicious%0Acode%20snippets%20at%20specific%20locations.%20We%20evaluate%20our%20TAPI%20attack%20on%20four%0Arepresentative%20LLMs%20under%20three%20representative%20malicious%20objectives%20and%20seven%0Acases.%20The%20results%20show%20that%20our%20method%20is%20highly%20threatening%20%28achieving%20an%0Aattack%20success%20rate%20of%20up%20to%2089.3%5C%25%29%20and%20stealthy%20%28saving%20an%20average%20of%2053.1%5C%25%0Aof%20tokens%20in%20the%20trigger%20design%29.%20In%20particular%2C%20we%20successfully%20attack%20some%0Afamous%20deployed%20code%20completion%20integrated%20applications%2C%20including%20CodeGeex%20and%0AGithub%20Copilot.%20This%20further%20confirms%20the%20realistic%20threat%20of%20our%20attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09164v1&entry.124074799=Read"},
{"title": "GPT-4 Generated Narratives of Life Events using a Structured Narrative\n  Prompt: A Validation Study", "author": "Christopher J. Lynch and Erik Jensen and Madison H. Munro and Virginia Zamponi and Joseph Martinez and Kevin O'Brien and Brandon Feldhaus and Katherine Smith and Ann Marie Reinhold and Ross Gore", "abstract": "  Large Language Models (LLMs) play a pivotal role in generating vast arrays of\nnarratives, facilitating a systematic exploration of their effectiveness for\ncommunicating life events in narrative form. In this study, we employ a\nzero-shot structured narrative prompt to generate 24,000 narratives using\nOpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and\nevaluate their validity in conveying birth, death, hiring, and firing events.\nRemarkably, 87.43% of the narratives sufficiently convey the intention of the\nstructured prompt. To automate the identification of valid and invalid\nnarratives, we train and validate nine Machine Learning models on the\nclassified datasets. Leveraging these models, we extend our analysis to predict\nthe classifications of the remaining 21,120 narratives. All the ML models\nexcelled at classifying valid narratives as valid, but experienced challenges\nat simultaneously classifying invalid narratives as invalid. Our findings not\nonly advance the study of LLM capabilities, limitations, and validity but also\noffer practical insights for narrative generation and natural language\nprocessing applications.\n", "link": "http://arxiv.org/abs/2402.05435v2", "date": "2024-07-12", "relevancy": 2.2685, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4783}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4562}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT-4%20Generated%20Narratives%20of%20Life%20Events%20using%20a%20Structured%20Narrative%0A%20%20Prompt%3A%20A%20Validation%20Study&body=Title%3A%20GPT-4%20Generated%20Narratives%20of%20Life%20Events%20using%20a%20Structured%20Narrative%0A%20%20Prompt%3A%20A%20Validation%20Study%0AAuthor%3A%20Christopher%20J.%20Lynch%20and%20Erik%20Jensen%20and%20Madison%20H.%20Munro%20and%20Virginia%20Zamponi%20and%20Joseph%20Martinez%20and%20Kevin%20O%27Brien%20and%20Brandon%20Feldhaus%20and%20Katherine%20Smith%20and%20Ann%20Marie%20Reinhold%20and%20Ross%20Gore%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20play%20a%20pivotal%20role%20in%20generating%20vast%20arrays%20of%0Anarratives%2C%20facilitating%20a%20systematic%20exploration%20of%20their%20effectiveness%20for%0Acommunicating%20life%20events%20in%20narrative%20form.%20In%20this%20study%2C%20we%20employ%20a%0Azero-shot%20structured%20narrative%20prompt%20to%20generate%2024%2C000%20narratives%20using%0AOpenAI%27s%20GPT-4.%20From%20this%20dataset%2C%20we%20manually%20classify%202%2C880%20narratives%20and%0Aevaluate%20their%20validity%20in%20conveying%20birth%2C%20death%2C%20hiring%2C%20and%20firing%20events.%0ARemarkably%2C%2087.43%25%20of%20the%20narratives%20sufficiently%20convey%20the%20intention%20of%20the%0Astructured%20prompt.%20To%20automate%20the%20identification%20of%20valid%20and%20invalid%0Anarratives%2C%20we%20train%20and%20validate%20nine%20Machine%20Learning%20models%20on%20the%0Aclassified%20datasets.%20Leveraging%20these%20models%2C%20we%20extend%20our%20analysis%20to%20predict%0Athe%20classifications%20of%20the%20remaining%2021%2C120%20narratives.%20All%20the%20ML%20models%0Aexcelled%20at%20classifying%20valid%20narratives%20as%20valid%2C%20but%20experienced%20challenges%0Aat%20simultaneously%20classifying%20invalid%20narratives%20as%20invalid.%20Our%20findings%20not%0Aonly%20advance%20the%20study%20of%20LLM%20capabilities%2C%20limitations%2C%20and%20validity%20but%20also%0Aoffer%20practical%20insights%20for%20narrative%20generation%20and%20natural%20language%0Aprocessing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT-4%2520Generated%2520Narratives%2520of%2520Life%2520Events%2520using%2520a%2520Structured%2520Narrative%250A%2520%2520Prompt%253A%2520A%2520Validation%2520Study%26entry.906535625%3DChristopher%2520J.%2520Lynch%2520and%2520Erik%2520Jensen%2520and%2520Madison%2520H.%2520Munro%2520and%2520Virginia%2520Zamponi%2520and%2520Joseph%2520Martinez%2520and%2520Kevin%2520O%2527Brien%2520and%2520Brandon%2520Feldhaus%2520and%2520Katherine%2520Smith%2520and%2520Ann%2520Marie%2520Reinhold%2520and%2520Ross%2520Gore%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520play%2520a%2520pivotal%2520role%2520in%2520generating%2520vast%2520arrays%2520of%250Anarratives%252C%2520facilitating%2520a%2520systematic%2520exploration%2520of%2520their%2520effectiveness%2520for%250Acommunicating%2520life%2520events%2520in%2520narrative%2520form.%2520In%2520this%2520study%252C%2520we%2520employ%2520a%250Azero-shot%2520structured%2520narrative%2520prompt%2520to%2520generate%252024%252C000%2520narratives%2520using%250AOpenAI%2527s%2520GPT-4.%2520From%2520this%2520dataset%252C%2520we%2520manually%2520classify%25202%252C880%2520narratives%2520and%250Aevaluate%2520their%2520validity%2520in%2520conveying%2520birth%252C%2520death%252C%2520hiring%252C%2520and%2520firing%2520events.%250ARemarkably%252C%252087.43%2525%2520of%2520the%2520narratives%2520sufficiently%2520convey%2520the%2520intention%2520of%2520the%250Astructured%2520prompt.%2520To%2520automate%2520the%2520identification%2520of%2520valid%2520and%2520invalid%250Anarratives%252C%2520we%2520train%2520and%2520validate%2520nine%2520Machine%2520Learning%2520models%2520on%2520the%250Aclassified%2520datasets.%2520Leveraging%2520these%2520models%252C%2520we%2520extend%2520our%2520analysis%2520to%2520predict%250Athe%2520classifications%2520of%2520the%2520remaining%252021%252C120%2520narratives.%2520All%2520the%2520ML%2520models%250Aexcelled%2520at%2520classifying%2520valid%2520narratives%2520as%2520valid%252C%2520but%2520experienced%2520challenges%250Aat%2520simultaneously%2520classifying%2520invalid%2520narratives%2520as%2520invalid.%2520Our%2520findings%2520not%250Aonly%2520advance%2520the%2520study%2520of%2520LLM%2520capabilities%252C%2520limitations%252C%2520and%2520validity%2520but%2520also%250Aoffer%2520practical%2520insights%2520for%2520narrative%2520generation%2520and%2520natural%2520language%250Aprocessing%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT-4%20Generated%20Narratives%20of%20Life%20Events%20using%20a%20Structured%20Narrative%0A%20%20Prompt%3A%20A%20Validation%20Study&entry.906535625=Christopher%20J.%20Lynch%20and%20Erik%20Jensen%20and%20Madison%20H.%20Munro%20and%20Virginia%20Zamponi%20and%20Joseph%20Martinez%20and%20Kevin%20O%27Brien%20and%20Brandon%20Feldhaus%20and%20Katherine%20Smith%20and%20Ann%20Marie%20Reinhold%20and%20Ross%20Gore&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20play%20a%20pivotal%20role%20in%20generating%20vast%20arrays%20of%0Anarratives%2C%20facilitating%20a%20systematic%20exploration%20of%20their%20effectiveness%20for%0Acommunicating%20life%20events%20in%20narrative%20form.%20In%20this%20study%2C%20we%20employ%20a%0Azero-shot%20structured%20narrative%20prompt%20to%20generate%2024%2C000%20narratives%20using%0AOpenAI%27s%20GPT-4.%20From%20this%20dataset%2C%20we%20manually%20classify%202%2C880%20narratives%20and%0Aevaluate%20their%20validity%20in%20conveying%20birth%2C%20death%2C%20hiring%2C%20and%20firing%20events.%0ARemarkably%2C%2087.43%25%20of%20the%20narratives%20sufficiently%20convey%20the%20intention%20of%20the%0Astructured%20prompt.%20To%20automate%20the%20identification%20of%20valid%20and%20invalid%0Anarratives%2C%20we%20train%20and%20validate%20nine%20Machine%20Learning%20models%20on%20the%0Aclassified%20datasets.%20Leveraging%20these%20models%2C%20we%20extend%20our%20analysis%20to%20predict%0Athe%20classifications%20of%20the%20remaining%2021%2C120%20narratives.%20All%20the%20ML%20models%0Aexcelled%20at%20classifying%20valid%20narratives%20as%20valid%2C%20but%20experienced%20challenges%0Aat%20simultaneously%20classifying%20invalid%20narratives%20as%20invalid.%20Our%20findings%20not%0Aonly%20advance%20the%20study%20of%20LLM%20capabilities%2C%20limitations%2C%20and%20validity%20but%20also%0Aoffer%20practical%20insights%20for%20narrative%20generation%20and%20natural%20language%0Aprocessing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05435v2&entry.124074799=Read"},
{"title": "An Adaptive Indoor Localization Approach Using WiFi RSSI Fingerprinting\n  with SLAM-Enabled Robotic Platform and Deep Neural Networks", "author": "Seyed Alireza Rahimi Azghadi and Atah Nuh Mih and Asfia Kawnine and Francis Palma and Hung Cao", "abstract": "  Indoor localization plays a vital role in the era of the IoT and robotics,\nwith WiFi technology being a prominent choice due to its ubiquity. We present a\nmethod for creating WiFi fingerprinting datasets to enhance indoor localization\nsystems and address the gap in WiFi fingerprinting dataset creation. We used\nthe Simultaneous Localization And Mapping (SLAM) algorithm and employed a\nrobotic platform to construct precise maps and localize robots in indoor\nenvironments. We developed software applications to facilitate data\nacquisition, fingerprinting dataset collection, and accurate ground truth map\nbuilding. Subsequently, we aligned the spatial information generated via the\nSLAM with the WiFi scans to create a comprehensive WiFi fingerprinting dataset.\nThe created dataset was used to train a deep neural network (DNN) for indoor\nlocalization, which can prove the usefulness of grid density. We conducted\nexperimental validation within our office environment to demonstrate the\nproposed method's effectiveness, including a heatmap from the dataset\nshowcasing the spatial distribution of WiFi signal strengths for the testing\naccess points placed within the environment. Notably, our method offers\ndistinct advantages over existing approaches as it eliminates the need for a\npredefined map of the environment, requires no preparatory steps, lessens human\nintervention, creates a denser fingerprinting dataset, and reduces the WiFi\nfingerprinting dataset creation time. Our method achieves 26% more accurate\nlocalization than the other methods and can create a six times denser\nfingerprinting dataset in one-third of the time compared to the traditional\nmethod. In summary, using WiFi RSSI Fingerprinting data surveyed by the\nSLAM-Enabled Robotic Platform, we can adapt our trained DNN model to indoor\nlocalization in any dynamic environment and enhance its scalability and\napplicability in real-world scenarios.\n", "link": "http://arxiv.org/abs/2407.09242v1", "date": "2024-07-12", "relevancy": 2.2487, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.591}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5449}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Indoor%20Localization%20Approach%20Using%20WiFi%20RSSI%20Fingerprinting%0A%20%20with%20SLAM-Enabled%20Robotic%20Platform%20and%20Deep%20Neural%20Networks&body=Title%3A%20An%20Adaptive%20Indoor%20Localization%20Approach%20Using%20WiFi%20RSSI%20Fingerprinting%0A%20%20with%20SLAM-Enabled%20Robotic%20Platform%20and%20Deep%20Neural%20Networks%0AAuthor%3A%20Seyed%20Alireza%20Rahimi%20Azghadi%20and%20Atah%20Nuh%20Mih%20and%20Asfia%20Kawnine%20and%20Francis%20Palma%20and%20Hung%20Cao%0AAbstract%3A%20%20%20Indoor%20localization%20plays%20a%20vital%20role%20in%20the%20era%20of%20the%20IoT%20and%20robotics%2C%0Awith%20WiFi%20technology%20being%20a%20prominent%20choice%20due%20to%20its%20ubiquity.%20We%20present%20a%0Amethod%20for%20creating%20WiFi%20fingerprinting%20datasets%20to%20enhance%20indoor%20localization%0Asystems%20and%20address%20the%20gap%20in%20WiFi%20fingerprinting%20dataset%20creation.%20We%20used%0Athe%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29%20algorithm%20and%20employed%20a%0Arobotic%20platform%20to%20construct%20precise%20maps%20and%20localize%20robots%20in%20indoor%0Aenvironments.%20We%20developed%20software%20applications%20to%20facilitate%20data%0Aacquisition%2C%20fingerprinting%20dataset%20collection%2C%20and%20accurate%20ground%20truth%20map%0Abuilding.%20Subsequently%2C%20we%20aligned%20the%20spatial%20information%20generated%20via%20the%0ASLAM%20with%20the%20WiFi%20scans%20to%20create%20a%20comprehensive%20WiFi%20fingerprinting%20dataset.%0AThe%20created%20dataset%20was%20used%20to%20train%20a%20deep%20neural%20network%20%28DNN%29%20for%20indoor%0Alocalization%2C%20which%20can%20prove%20the%20usefulness%20of%20grid%20density.%20We%20conducted%0Aexperimental%20validation%20within%20our%20office%20environment%20to%20demonstrate%20the%0Aproposed%20method%27s%20effectiveness%2C%20including%20a%20heatmap%20from%20the%20dataset%0Ashowcasing%20the%20spatial%20distribution%20of%20WiFi%20signal%20strengths%20for%20the%20testing%0Aaccess%20points%20placed%20within%20the%20environment.%20Notably%2C%20our%20method%20offers%0Adistinct%20advantages%20over%20existing%20approaches%20as%20it%20eliminates%20the%20need%20for%20a%0Apredefined%20map%20of%20the%20environment%2C%20requires%20no%20preparatory%20steps%2C%20lessens%20human%0Aintervention%2C%20creates%20a%20denser%20fingerprinting%20dataset%2C%20and%20reduces%20the%20WiFi%0Afingerprinting%20dataset%20creation%20time.%20Our%20method%20achieves%2026%25%20more%20accurate%0Alocalization%20than%20the%20other%20methods%20and%20can%20create%20a%20six%20times%20denser%0Afingerprinting%20dataset%20in%20one-third%20of%20the%20time%20compared%20to%20the%20traditional%0Amethod.%20In%20summary%2C%20using%20WiFi%20RSSI%20Fingerprinting%20data%20surveyed%20by%20the%0ASLAM-Enabled%20Robotic%20Platform%2C%20we%20can%20adapt%20our%20trained%20DNN%20model%20to%20indoor%0Alocalization%20in%20any%20dynamic%20environment%20and%20enhance%20its%20scalability%20and%0Aapplicability%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Indoor%2520Localization%2520Approach%2520Using%2520WiFi%2520RSSI%2520Fingerprinting%250A%2520%2520with%2520SLAM-Enabled%2520Robotic%2520Platform%2520and%2520Deep%2520Neural%2520Networks%26entry.906535625%3DSeyed%2520Alireza%2520Rahimi%2520Azghadi%2520and%2520Atah%2520Nuh%2520Mih%2520and%2520Asfia%2520Kawnine%2520and%2520Francis%2520Palma%2520and%2520Hung%2520Cao%26entry.1292438233%3D%2520%2520Indoor%2520localization%2520plays%2520a%2520vital%2520role%2520in%2520the%2520era%2520of%2520the%2520IoT%2520and%2520robotics%252C%250Awith%2520WiFi%2520technology%2520being%2520a%2520prominent%2520choice%2520due%2520to%2520its%2520ubiquity.%2520We%2520present%2520a%250Amethod%2520for%2520creating%2520WiFi%2520fingerprinting%2520datasets%2520to%2520enhance%2520indoor%2520localization%250Asystems%2520and%2520address%2520the%2520gap%2520in%2520WiFi%2520fingerprinting%2520dataset%2520creation.%2520We%2520used%250Athe%2520Simultaneous%2520Localization%2520And%2520Mapping%2520%2528SLAM%2529%2520algorithm%2520and%2520employed%2520a%250Arobotic%2520platform%2520to%2520construct%2520precise%2520maps%2520and%2520localize%2520robots%2520in%2520indoor%250Aenvironments.%2520We%2520developed%2520software%2520applications%2520to%2520facilitate%2520data%250Aacquisition%252C%2520fingerprinting%2520dataset%2520collection%252C%2520and%2520accurate%2520ground%2520truth%2520map%250Abuilding.%2520Subsequently%252C%2520we%2520aligned%2520the%2520spatial%2520information%2520generated%2520via%2520the%250ASLAM%2520with%2520the%2520WiFi%2520scans%2520to%2520create%2520a%2520comprehensive%2520WiFi%2520fingerprinting%2520dataset.%250AThe%2520created%2520dataset%2520was%2520used%2520to%2520train%2520a%2520deep%2520neural%2520network%2520%2528DNN%2529%2520for%2520indoor%250Alocalization%252C%2520which%2520can%2520prove%2520the%2520usefulness%2520of%2520grid%2520density.%2520We%2520conducted%250Aexperimental%2520validation%2520within%2520our%2520office%2520environment%2520to%2520demonstrate%2520the%250Aproposed%2520method%2527s%2520effectiveness%252C%2520including%2520a%2520heatmap%2520from%2520the%2520dataset%250Ashowcasing%2520the%2520spatial%2520distribution%2520of%2520WiFi%2520signal%2520strengths%2520for%2520the%2520testing%250Aaccess%2520points%2520placed%2520within%2520the%2520environment.%2520Notably%252C%2520our%2520method%2520offers%250Adistinct%2520advantages%2520over%2520existing%2520approaches%2520as%2520it%2520eliminates%2520the%2520need%2520for%2520a%250Apredefined%2520map%2520of%2520the%2520environment%252C%2520requires%2520no%2520preparatory%2520steps%252C%2520lessens%2520human%250Aintervention%252C%2520creates%2520a%2520denser%2520fingerprinting%2520dataset%252C%2520and%2520reduces%2520the%2520WiFi%250Afingerprinting%2520dataset%2520creation%2520time.%2520Our%2520method%2520achieves%252026%2525%2520more%2520accurate%250Alocalization%2520than%2520the%2520other%2520methods%2520and%2520can%2520create%2520a%2520six%2520times%2520denser%250Afingerprinting%2520dataset%2520in%2520one-third%2520of%2520the%2520time%2520compared%2520to%2520the%2520traditional%250Amethod.%2520In%2520summary%252C%2520using%2520WiFi%2520RSSI%2520Fingerprinting%2520data%2520surveyed%2520by%2520the%250ASLAM-Enabled%2520Robotic%2520Platform%252C%2520we%2520can%2520adapt%2520our%2520trained%2520DNN%2520model%2520to%2520indoor%250Alocalization%2520in%2520any%2520dynamic%2520environment%2520and%2520enhance%2520its%2520scalability%2520and%250Aapplicability%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Indoor%20Localization%20Approach%20Using%20WiFi%20RSSI%20Fingerprinting%0A%20%20with%20SLAM-Enabled%20Robotic%20Platform%20and%20Deep%20Neural%20Networks&entry.906535625=Seyed%20Alireza%20Rahimi%20Azghadi%20and%20Atah%20Nuh%20Mih%20and%20Asfia%20Kawnine%20and%20Francis%20Palma%20and%20Hung%20Cao&entry.1292438233=%20%20Indoor%20localization%20plays%20a%20vital%20role%20in%20the%20era%20of%20the%20IoT%20and%20robotics%2C%0Awith%20WiFi%20technology%20being%20a%20prominent%20choice%20due%20to%20its%20ubiquity.%20We%20present%20a%0Amethod%20for%20creating%20WiFi%20fingerprinting%20datasets%20to%20enhance%20indoor%20localization%0Asystems%20and%20address%20the%20gap%20in%20WiFi%20fingerprinting%20dataset%20creation.%20We%20used%0Athe%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29%20algorithm%20and%20employed%20a%0Arobotic%20platform%20to%20construct%20precise%20maps%20and%20localize%20robots%20in%20indoor%0Aenvironments.%20We%20developed%20software%20applications%20to%20facilitate%20data%0Aacquisition%2C%20fingerprinting%20dataset%20collection%2C%20and%20accurate%20ground%20truth%20map%0Abuilding.%20Subsequently%2C%20we%20aligned%20the%20spatial%20information%20generated%20via%20the%0ASLAM%20with%20the%20WiFi%20scans%20to%20create%20a%20comprehensive%20WiFi%20fingerprinting%20dataset.%0AThe%20created%20dataset%20was%20used%20to%20train%20a%20deep%20neural%20network%20%28DNN%29%20for%20indoor%0Alocalization%2C%20which%20can%20prove%20the%20usefulness%20of%20grid%20density.%20We%20conducted%0Aexperimental%20validation%20within%20our%20office%20environment%20to%20demonstrate%20the%0Aproposed%20method%27s%20effectiveness%2C%20including%20a%20heatmap%20from%20the%20dataset%0Ashowcasing%20the%20spatial%20distribution%20of%20WiFi%20signal%20strengths%20for%20the%20testing%0Aaccess%20points%20placed%20within%20the%20environment.%20Notably%2C%20our%20method%20offers%0Adistinct%20advantages%20over%20existing%20approaches%20as%20it%20eliminates%20the%20need%20for%20a%0Apredefined%20map%20of%20the%20environment%2C%20requires%20no%20preparatory%20steps%2C%20lessens%20human%0Aintervention%2C%20creates%20a%20denser%20fingerprinting%20dataset%2C%20and%20reduces%20the%20WiFi%0Afingerprinting%20dataset%20creation%20time.%20Our%20method%20achieves%2026%25%20more%20accurate%0Alocalization%20than%20the%20other%20methods%20and%20can%20create%20a%20six%20times%20denser%0Afingerprinting%20dataset%20in%20one-third%20of%20the%20time%20compared%20to%20the%20traditional%0Amethod.%20In%20summary%2C%20using%20WiFi%20RSSI%20Fingerprinting%20data%20surveyed%20by%20the%0ASLAM-Enabled%20Robotic%20Platform%2C%20we%20can%20adapt%20our%20trained%20DNN%20model%20to%20indoor%0Alocalization%20in%20any%20dynamic%20environment%20and%20enhance%20its%20scalability%20and%0Aapplicability%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09242v1&entry.124074799=Read"},
{"title": "DART: An Automated End-to-End Object Detection Pipeline with Data\n  Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label\n  Review, and Model Training", "author": "Chen Xin and Andreas Hartel and Enkelejda Kasneci", "abstract": "  Swift and accurate detection of specified objects is crucial for many\nindustrial applications, such as safety monitoring on construction sites.\nHowever, traditional approaches rely heavily on arduous manual annotation and\ndata collection, which struggle to adapt to ever-changing environments and\nnovel target objects. To address these limitations, this paper presents DART,\nan automated end-to-end pipeline designed to streamline the entire workflow of\nan object detection application from data collection to model deployment. DART\neliminates the need for human labeling and extensive data collection while\nexcelling in diverse scenarios. It employs a subject-driven image generation\nmodule (DreamBooth with SDXL) for data diversification, followed by an\nannotation stage where open-vocabulary object detection (Grounding DINO)\ngenerates bounding box annotations for both generated and original images.\nThese pseudo-labels are then reviewed by a large multimodal model (GPT-4o) to\nguarantee credibility before serving as ground truth to train real-time object\ndetectors (YOLO). We apply DART to a self-collected dataset of construction\nmachines named Liebherr Product, which contains over 15K high-quality images\nacross 23 categories. The current implementation of DART significantly\nincreases average precision (AP) from 0.064 to 0.832. Furthermore, we adopt a\nmodular design for DART to ensure easy exchangeability and extensibility. This\nallows for a smooth transition to more advanced algorithms in the future,\nseamless integration of new object categories without manual labeling, and\nadaptability to customized environments without extra data collection. The code\nand dataset are released at https://github.com/chen-xin-94/DART.\n", "link": "http://arxiv.org/abs/2407.09174v1", "date": "2024-07-12", "relevancy": 2.2429, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.587}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5625}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DART%3A%20An%20Automated%20End-to-End%20Object%20Detection%20Pipeline%20with%20Data%0A%20%20Diversification%2C%20Open-Vocabulary%20Bounding%20Box%20Annotation%2C%20Pseudo-Label%0A%20%20Review%2C%20and%20Model%20Training&body=Title%3A%20DART%3A%20An%20Automated%20End-to-End%20Object%20Detection%20Pipeline%20with%20Data%0A%20%20Diversification%2C%20Open-Vocabulary%20Bounding%20Box%20Annotation%2C%20Pseudo-Label%0A%20%20Review%2C%20and%20Model%20Training%0AAuthor%3A%20Chen%20Xin%20and%20Andreas%20Hartel%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20%20%20Swift%20and%20accurate%20detection%20of%20specified%20objects%20is%20crucial%20for%20many%0Aindustrial%20applications%2C%20such%20as%20safety%20monitoring%20on%20construction%20sites.%0AHowever%2C%20traditional%20approaches%20rely%20heavily%20on%20arduous%20manual%20annotation%20and%0Adata%20collection%2C%20which%20struggle%20to%20adapt%20to%20ever-changing%20environments%20and%0Anovel%20target%20objects.%20To%20address%20these%20limitations%2C%20this%20paper%20presents%20DART%2C%0Aan%20automated%20end-to-end%20pipeline%20designed%20to%20streamline%20the%20entire%20workflow%20of%0Aan%20object%20detection%20application%20from%20data%20collection%20to%20model%20deployment.%20DART%0Aeliminates%20the%20need%20for%20human%20labeling%20and%20extensive%20data%20collection%20while%0Aexcelling%20in%20diverse%20scenarios.%20It%20employs%20a%20subject-driven%20image%20generation%0Amodule%20%28DreamBooth%20with%20SDXL%29%20for%20data%20diversification%2C%20followed%20by%20an%0Aannotation%20stage%20where%20open-vocabulary%20object%20detection%20%28Grounding%20DINO%29%0Agenerates%20bounding%20box%20annotations%20for%20both%20generated%20and%20original%20images.%0AThese%20pseudo-labels%20are%20then%20reviewed%20by%20a%20large%20multimodal%20model%20%28GPT-4o%29%20to%0Aguarantee%20credibility%20before%20serving%20as%20ground%20truth%20to%20train%20real-time%20object%0Adetectors%20%28YOLO%29.%20We%20apply%20DART%20to%20a%20self-collected%20dataset%20of%20construction%0Amachines%20named%20Liebherr%20Product%2C%20which%20contains%20over%2015K%20high-quality%20images%0Aacross%2023%20categories.%20The%20current%20implementation%20of%20DART%20significantly%0Aincreases%20average%20precision%20%28AP%29%20from%200.064%20to%200.832.%20Furthermore%2C%20we%20adopt%20a%0Amodular%20design%20for%20DART%20to%20ensure%20easy%20exchangeability%20and%20extensibility.%20This%0Aallows%20for%20a%20smooth%20transition%20to%20more%20advanced%20algorithms%20in%20the%20future%2C%0Aseamless%20integration%20of%20new%20object%20categories%20without%20manual%20labeling%2C%20and%0Aadaptability%20to%20customized%20environments%20without%20extra%20data%20collection.%20The%20code%0Aand%20dataset%20are%20released%20at%20https%3A//github.com/chen-xin-94/DART.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDART%253A%2520An%2520Automated%2520End-to-End%2520Object%2520Detection%2520Pipeline%2520with%2520Data%250A%2520%2520Diversification%252C%2520Open-Vocabulary%2520Bounding%2520Box%2520Annotation%252C%2520Pseudo-Label%250A%2520%2520Review%252C%2520and%2520Model%2520Training%26entry.906535625%3DChen%2520Xin%2520and%2520Andreas%2520Hartel%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3D%2520%2520Swift%2520and%2520accurate%2520detection%2520of%2520specified%2520objects%2520is%2520crucial%2520for%2520many%250Aindustrial%2520applications%252C%2520such%2520as%2520safety%2520monitoring%2520on%2520construction%2520sites.%250AHowever%252C%2520traditional%2520approaches%2520rely%2520heavily%2520on%2520arduous%2520manual%2520annotation%2520and%250Adata%2520collection%252C%2520which%2520struggle%2520to%2520adapt%2520to%2520ever-changing%2520environments%2520and%250Anovel%2520target%2520objects.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520presents%2520DART%252C%250Aan%2520automated%2520end-to-end%2520pipeline%2520designed%2520to%2520streamline%2520the%2520entire%2520workflow%2520of%250Aan%2520object%2520detection%2520application%2520from%2520data%2520collection%2520to%2520model%2520deployment.%2520DART%250Aeliminates%2520the%2520need%2520for%2520human%2520labeling%2520and%2520extensive%2520data%2520collection%2520while%250Aexcelling%2520in%2520diverse%2520scenarios.%2520It%2520employs%2520a%2520subject-driven%2520image%2520generation%250Amodule%2520%2528DreamBooth%2520with%2520SDXL%2529%2520for%2520data%2520diversification%252C%2520followed%2520by%2520an%250Aannotation%2520stage%2520where%2520open-vocabulary%2520object%2520detection%2520%2528Grounding%2520DINO%2529%250Agenerates%2520bounding%2520box%2520annotations%2520for%2520both%2520generated%2520and%2520original%2520images.%250AThese%2520pseudo-labels%2520are%2520then%2520reviewed%2520by%2520a%2520large%2520multimodal%2520model%2520%2528GPT-4o%2529%2520to%250Aguarantee%2520credibility%2520before%2520serving%2520as%2520ground%2520truth%2520to%2520train%2520real-time%2520object%250Adetectors%2520%2528YOLO%2529.%2520We%2520apply%2520DART%2520to%2520a%2520self-collected%2520dataset%2520of%2520construction%250Amachines%2520named%2520Liebherr%2520Product%252C%2520which%2520contains%2520over%252015K%2520high-quality%2520images%250Aacross%252023%2520categories.%2520The%2520current%2520implementation%2520of%2520DART%2520significantly%250Aincreases%2520average%2520precision%2520%2528AP%2529%2520from%25200.064%2520to%25200.832.%2520Furthermore%252C%2520we%2520adopt%2520a%250Amodular%2520design%2520for%2520DART%2520to%2520ensure%2520easy%2520exchangeability%2520and%2520extensibility.%2520This%250Aallows%2520for%2520a%2520smooth%2520transition%2520to%2520more%2520advanced%2520algorithms%2520in%2520the%2520future%252C%250Aseamless%2520integration%2520of%2520new%2520object%2520categories%2520without%2520manual%2520labeling%252C%2520and%250Aadaptability%2520to%2520customized%2520environments%2520without%2520extra%2520data%2520collection.%2520The%2520code%250Aand%2520dataset%2520are%2520released%2520at%2520https%253A//github.com/chen-xin-94/DART.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DART%3A%20An%20Automated%20End-to-End%20Object%20Detection%20Pipeline%20with%20Data%0A%20%20Diversification%2C%20Open-Vocabulary%20Bounding%20Box%20Annotation%2C%20Pseudo-Label%0A%20%20Review%2C%20and%20Model%20Training&entry.906535625=Chen%20Xin%20and%20Andreas%20Hartel%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20Swift%20and%20accurate%20detection%20of%20specified%20objects%20is%20crucial%20for%20many%0Aindustrial%20applications%2C%20such%20as%20safety%20monitoring%20on%20construction%20sites.%0AHowever%2C%20traditional%20approaches%20rely%20heavily%20on%20arduous%20manual%20annotation%20and%0Adata%20collection%2C%20which%20struggle%20to%20adapt%20to%20ever-changing%20environments%20and%0Anovel%20target%20objects.%20To%20address%20these%20limitations%2C%20this%20paper%20presents%20DART%2C%0Aan%20automated%20end-to-end%20pipeline%20designed%20to%20streamline%20the%20entire%20workflow%20of%0Aan%20object%20detection%20application%20from%20data%20collection%20to%20model%20deployment.%20DART%0Aeliminates%20the%20need%20for%20human%20labeling%20and%20extensive%20data%20collection%20while%0Aexcelling%20in%20diverse%20scenarios.%20It%20employs%20a%20subject-driven%20image%20generation%0Amodule%20%28DreamBooth%20with%20SDXL%29%20for%20data%20diversification%2C%20followed%20by%20an%0Aannotation%20stage%20where%20open-vocabulary%20object%20detection%20%28Grounding%20DINO%29%0Agenerates%20bounding%20box%20annotations%20for%20both%20generated%20and%20original%20images.%0AThese%20pseudo-labels%20are%20then%20reviewed%20by%20a%20large%20multimodal%20model%20%28GPT-4o%29%20to%0Aguarantee%20credibility%20before%20serving%20as%20ground%20truth%20to%20train%20real-time%20object%0Adetectors%20%28YOLO%29.%20We%20apply%20DART%20to%20a%20self-collected%20dataset%20of%20construction%0Amachines%20named%20Liebherr%20Product%2C%20which%20contains%20over%2015K%20high-quality%20images%0Aacross%2023%20categories.%20The%20current%20implementation%20of%20DART%20significantly%0Aincreases%20average%20precision%20%28AP%29%20from%200.064%20to%200.832.%20Furthermore%2C%20we%20adopt%20a%0Amodular%20design%20for%20DART%20to%20ensure%20easy%20exchangeability%20and%20extensibility.%20This%0Aallows%20for%20a%20smooth%20transition%20to%20more%20advanced%20algorithms%20in%20the%20future%2C%0Aseamless%20integration%20of%20new%20object%20categories%20without%20manual%20labeling%2C%20and%0Aadaptability%20to%20customized%20environments%20without%20extra%20data%20collection.%20The%20code%0Aand%20dataset%20are%20released%20at%20https%3A//github.com/chen-xin-94/DART.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09174v1&entry.124074799=Read"},
{"title": "iNeMo: Incremental Neural Mesh Models for Robust Class-Incremental\n  Learning", "author": "Tom Fischer and Yaoyao Liu and Artur Jesslen and Noor Ahmed and Prakhar Kaushik and Angtian Wang and Alan Yuille and Adam Kortylewski and Eddy Ilg", "abstract": "  Different from human nature, it is still common practice today for vision\ntasks to train deep learning models only initially and on fixed datasets. A\nvariety of approaches have recently addressed handling continual data streams.\nHowever, extending these methods to manage out-of-distribution (OOD) scenarios\nhas not effectively been investigated. On the other hand, it has recently been\nshown that non-continual neural mesh models exhibit strong performance in\ngeneralizing to such OOD scenarios. To leverage this decisive property in a\ncontinual learning setting, we propose incremental neural mesh models that can\nbe extended with new meshes over time. In addition, we present a latent space\ninitialization strategy that enables us to allocate feature space for future\nunseen classes in advance and a positional regularization term that forces the\nfeatures of the different classes to consistently stay in respective latent\nspace regions. We demonstrate the effectiveness of our method through extensive\nexperiments on the Pascal3D and ObjectNet3D datasets and show that our approach\noutperforms the baselines for classification by $2-6\\%$ in the in-domain and by\n$6-50\\%$ in the OOD setting. Our work also presents the first incremental\nlearning approach for pose estimation. Our code and model can be found at\nhttps://github.com/Fischer-Tom/iNeMo.\n", "link": "http://arxiv.org/abs/2407.09271v1", "date": "2024-07-12", "relevancy": 2.2319, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5703}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5526}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iNeMo%3A%20Incremental%20Neural%20Mesh%20Models%20for%20Robust%20Class-Incremental%0A%20%20Learning&body=Title%3A%20iNeMo%3A%20Incremental%20Neural%20Mesh%20Models%20for%20Robust%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Tom%20Fischer%20and%20Yaoyao%20Liu%20and%20Artur%20Jesslen%20and%20Noor%20Ahmed%20and%20Prakhar%20Kaushik%20and%20Angtian%20Wang%20and%20Alan%20Yuille%20and%20Adam%20Kortylewski%20and%20Eddy%20Ilg%0AAbstract%3A%20%20%20Different%20from%20human%20nature%2C%20it%20is%20still%20common%20practice%20today%20for%20vision%0Atasks%20to%20train%20deep%20learning%20models%20only%20initially%20and%20on%20fixed%20datasets.%20A%0Avariety%20of%20approaches%20have%20recently%20addressed%20handling%20continual%20data%20streams.%0AHowever%2C%20extending%20these%20methods%20to%20manage%20out-of-distribution%20%28OOD%29%20scenarios%0Ahas%20not%20effectively%20been%20investigated.%20On%20the%20other%20hand%2C%20it%20has%20recently%20been%0Ashown%20that%20non-continual%20neural%20mesh%20models%20exhibit%20strong%20performance%20in%0Ageneralizing%20to%20such%20OOD%20scenarios.%20To%20leverage%20this%20decisive%20property%20in%20a%0Acontinual%20learning%20setting%2C%20we%20propose%20incremental%20neural%20mesh%20models%20that%20can%0Abe%20extended%20with%20new%20meshes%20over%20time.%20In%20addition%2C%20we%20present%20a%20latent%20space%0Ainitialization%20strategy%20that%20enables%20us%20to%20allocate%20feature%20space%20for%20future%0Aunseen%20classes%20in%20advance%20and%20a%20positional%20regularization%20term%20that%20forces%20the%0Afeatures%20of%20the%20different%20classes%20to%20consistently%20stay%20in%20respective%20latent%0Aspace%20regions.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20through%20extensive%0Aexperiments%20on%20the%20Pascal3D%20and%20ObjectNet3D%20datasets%20and%20show%20that%20our%20approach%0Aoutperforms%20the%20baselines%20for%20classification%20by%20%242-6%5C%25%24%20in%20the%20in-domain%20and%20by%0A%246-50%5C%25%24%20in%20the%20OOD%20setting.%20Our%20work%20also%20presents%20the%20first%20incremental%0Alearning%20approach%20for%20pose%20estimation.%20Our%20code%20and%20model%20can%20be%20found%20at%0Ahttps%3A//github.com/Fischer-Tom/iNeMo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiNeMo%253A%2520Incremental%2520Neural%2520Mesh%2520Models%2520for%2520Robust%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DTom%2520Fischer%2520and%2520Yaoyao%2520Liu%2520and%2520Artur%2520Jesslen%2520and%2520Noor%2520Ahmed%2520and%2520Prakhar%2520Kaushik%2520and%2520Angtian%2520Wang%2520and%2520Alan%2520Yuille%2520and%2520Adam%2520Kortylewski%2520and%2520Eddy%2520Ilg%26entry.1292438233%3D%2520%2520Different%2520from%2520human%2520nature%252C%2520it%2520is%2520still%2520common%2520practice%2520today%2520for%2520vision%250Atasks%2520to%2520train%2520deep%2520learning%2520models%2520only%2520initially%2520and%2520on%2520fixed%2520datasets.%2520A%250Avariety%2520of%2520approaches%2520have%2520recently%2520addressed%2520handling%2520continual%2520data%2520streams.%250AHowever%252C%2520extending%2520these%2520methods%2520to%2520manage%2520out-of-distribution%2520%2528OOD%2529%2520scenarios%250Ahas%2520not%2520effectively%2520been%2520investigated.%2520On%2520the%2520other%2520hand%252C%2520it%2520has%2520recently%2520been%250Ashown%2520that%2520non-continual%2520neural%2520mesh%2520models%2520exhibit%2520strong%2520performance%2520in%250Ageneralizing%2520to%2520such%2520OOD%2520scenarios.%2520To%2520leverage%2520this%2520decisive%2520property%2520in%2520a%250Acontinual%2520learning%2520setting%252C%2520we%2520propose%2520incremental%2520neural%2520mesh%2520models%2520that%2520can%250Abe%2520extended%2520with%2520new%2520meshes%2520over%2520time.%2520In%2520addition%252C%2520we%2520present%2520a%2520latent%2520space%250Ainitialization%2520strategy%2520that%2520enables%2520us%2520to%2520allocate%2520feature%2520space%2520for%2520future%250Aunseen%2520classes%2520in%2520advance%2520and%2520a%2520positional%2520regularization%2520term%2520that%2520forces%2520the%250Afeatures%2520of%2520the%2520different%2520classes%2520to%2520consistently%2520stay%2520in%2520respective%2520latent%250Aspace%2520regions.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520through%2520extensive%250Aexperiments%2520on%2520the%2520Pascal3D%2520and%2520ObjectNet3D%2520datasets%2520and%2520show%2520that%2520our%2520approach%250Aoutperforms%2520the%2520baselines%2520for%2520classification%2520by%2520%25242-6%255C%2525%2524%2520in%2520the%2520in-domain%2520and%2520by%250A%25246-50%255C%2525%2524%2520in%2520the%2520OOD%2520setting.%2520Our%2520work%2520also%2520presents%2520the%2520first%2520incremental%250Alearning%2520approach%2520for%2520pose%2520estimation.%2520Our%2520code%2520and%2520model%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/Fischer-Tom/iNeMo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iNeMo%3A%20Incremental%20Neural%20Mesh%20Models%20for%20Robust%20Class-Incremental%0A%20%20Learning&entry.906535625=Tom%20Fischer%20and%20Yaoyao%20Liu%20and%20Artur%20Jesslen%20and%20Noor%20Ahmed%20and%20Prakhar%20Kaushik%20and%20Angtian%20Wang%20and%20Alan%20Yuille%20and%20Adam%20Kortylewski%20and%20Eddy%20Ilg&entry.1292438233=%20%20Different%20from%20human%20nature%2C%20it%20is%20still%20common%20practice%20today%20for%20vision%0Atasks%20to%20train%20deep%20learning%20models%20only%20initially%20and%20on%20fixed%20datasets.%20A%0Avariety%20of%20approaches%20have%20recently%20addressed%20handling%20continual%20data%20streams.%0AHowever%2C%20extending%20these%20methods%20to%20manage%20out-of-distribution%20%28OOD%29%20scenarios%0Ahas%20not%20effectively%20been%20investigated.%20On%20the%20other%20hand%2C%20it%20has%20recently%20been%0Ashown%20that%20non-continual%20neural%20mesh%20models%20exhibit%20strong%20performance%20in%0Ageneralizing%20to%20such%20OOD%20scenarios.%20To%20leverage%20this%20decisive%20property%20in%20a%0Acontinual%20learning%20setting%2C%20we%20propose%20incremental%20neural%20mesh%20models%20that%20can%0Abe%20extended%20with%20new%20meshes%20over%20time.%20In%20addition%2C%20we%20present%20a%20latent%20space%0Ainitialization%20strategy%20that%20enables%20us%20to%20allocate%20feature%20space%20for%20future%0Aunseen%20classes%20in%20advance%20and%20a%20positional%20regularization%20term%20that%20forces%20the%0Afeatures%20of%20the%20different%20classes%20to%20consistently%20stay%20in%20respective%20latent%0Aspace%20regions.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20through%20extensive%0Aexperiments%20on%20the%20Pascal3D%20and%20ObjectNet3D%20datasets%20and%20show%20that%20our%20approach%0Aoutperforms%20the%20baselines%20for%20classification%20by%20%242-6%5C%25%24%20in%20the%20in-domain%20and%20by%0A%246-50%5C%25%24%20in%20the%20OOD%20setting.%20Our%20work%20also%20presents%20the%20first%20incremental%0Alearning%20approach%20for%20pose%20estimation.%20Our%20code%20and%20model%20can%20be%20found%20at%0Ahttps%3A//github.com/Fischer-Tom/iNeMo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09271v1&entry.124074799=Read"},
{"title": "Radiance Fields from Photons", "author": "Sacha Jungerman and Mohit Gupta", "abstract": "  Neural radiance fields, or NeRFs, have become the de facto approach for\nhigh-quality view synthesis from a collection of images captured from multiple\nviewpoints. However, many issues remain when capturing images in-the-wild under\nchallenging conditions, such as low light, high dynamic range, or rapid motion\nleading to smeared reconstructions with noticeable artifacts. In this work, we\nintroduce quanta radiance fields, a novel class of neural radiance fields that\nare trained at the granularity of individual photons using single-photon\ncameras (SPCs). We develop theory and practical computational techniques for\nbuilding radiance fields and estimating dense camera poses from unconventional,\nstochastic, and high-speed binary frame sequences captured by SPCs. We\ndemonstrate, both via simulations and a SPC hardware prototype, high-fidelity\nreconstructions under high-speed motion, in low light, and for extreme dynamic\nrange settings.\n", "link": "http://arxiv.org/abs/2407.09386v1", "date": "2024-07-12", "relevancy": 2.2023, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5791}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5428}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radiance%20Fields%20from%20Photons&body=Title%3A%20Radiance%20Fields%20from%20Photons%0AAuthor%3A%20Sacha%20Jungerman%20and%20Mohit%20Gupta%0AAbstract%3A%20%20%20Neural%20radiance%20fields%2C%20or%20NeRFs%2C%20have%20become%20the%20de%20facto%20approach%20for%0Ahigh-quality%20view%20synthesis%20from%20a%20collection%20of%20images%20captured%20from%20multiple%0Aviewpoints.%20However%2C%20many%20issues%20remain%20when%20capturing%20images%20in-the-wild%20under%0Achallenging%20conditions%2C%20such%20as%20low%20light%2C%20high%20dynamic%20range%2C%20or%20rapid%20motion%0Aleading%20to%20smeared%20reconstructions%20with%20noticeable%20artifacts.%20In%20this%20work%2C%20we%0Aintroduce%20quanta%20radiance%20fields%2C%20a%20novel%20class%20of%20neural%20radiance%20fields%20that%0Aare%20trained%20at%20the%20granularity%20of%20individual%20photons%20using%20single-photon%0Acameras%20%28SPCs%29.%20We%20develop%20theory%20and%20practical%20computational%20techniques%20for%0Abuilding%20radiance%20fields%20and%20estimating%20dense%20camera%20poses%20from%20unconventional%2C%0Astochastic%2C%20and%20high-speed%20binary%20frame%20sequences%20captured%20by%20SPCs.%20We%0Ademonstrate%2C%20both%20via%20simulations%20and%20a%20SPC%20hardware%20prototype%2C%20high-fidelity%0Areconstructions%20under%20high-speed%20motion%2C%20in%20low%20light%2C%20and%20for%20extreme%20dynamic%0Arange%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadiance%2520Fields%2520from%2520Photons%26entry.906535625%3DSacha%2520Jungerman%2520and%2520Mohit%2520Gupta%26entry.1292438233%3D%2520%2520Neural%2520radiance%2520fields%252C%2520or%2520NeRFs%252C%2520have%2520become%2520the%2520de%2520facto%2520approach%2520for%250Ahigh-quality%2520view%2520synthesis%2520from%2520a%2520collection%2520of%2520images%2520captured%2520from%2520multiple%250Aviewpoints.%2520However%252C%2520many%2520issues%2520remain%2520when%2520capturing%2520images%2520in-the-wild%2520under%250Achallenging%2520conditions%252C%2520such%2520as%2520low%2520light%252C%2520high%2520dynamic%2520range%252C%2520or%2520rapid%2520motion%250Aleading%2520to%2520smeared%2520reconstructions%2520with%2520noticeable%2520artifacts.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520quanta%2520radiance%2520fields%252C%2520a%2520novel%2520class%2520of%2520neural%2520radiance%2520fields%2520that%250Aare%2520trained%2520at%2520the%2520granularity%2520of%2520individual%2520photons%2520using%2520single-photon%250Acameras%2520%2528SPCs%2529.%2520We%2520develop%2520theory%2520and%2520practical%2520computational%2520techniques%2520for%250Abuilding%2520radiance%2520fields%2520and%2520estimating%2520dense%2520camera%2520poses%2520from%2520unconventional%252C%250Astochastic%252C%2520and%2520high-speed%2520binary%2520frame%2520sequences%2520captured%2520by%2520SPCs.%2520We%250Ademonstrate%252C%2520both%2520via%2520simulations%2520and%2520a%2520SPC%2520hardware%2520prototype%252C%2520high-fidelity%250Areconstructions%2520under%2520high-speed%2520motion%252C%2520in%2520low%2520light%252C%2520and%2520for%2520extreme%2520dynamic%250Arange%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radiance%20Fields%20from%20Photons&entry.906535625=Sacha%20Jungerman%20and%20Mohit%20Gupta&entry.1292438233=%20%20Neural%20radiance%20fields%2C%20or%20NeRFs%2C%20have%20become%20the%20de%20facto%20approach%20for%0Ahigh-quality%20view%20synthesis%20from%20a%20collection%20of%20images%20captured%20from%20multiple%0Aviewpoints.%20However%2C%20many%20issues%20remain%20when%20capturing%20images%20in-the-wild%20under%0Achallenging%20conditions%2C%20such%20as%20low%20light%2C%20high%20dynamic%20range%2C%20or%20rapid%20motion%0Aleading%20to%20smeared%20reconstructions%20with%20noticeable%20artifacts.%20In%20this%20work%2C%20we%0Aintroduce%20quanta%20radiance%20fields%2C%20a%20novel%20class%20of%20neural%20radiance%20fields%20that%0Aare%20trained%20at%20the%20granularity%20of%20individual%20photons%20using%20single-photon%0Acameras%20%28SPCs%29.%20We%20develop%20theory%20and%20practical%20computational%20techniques%20for%0Abuilding%20radiance%20fields%20and%20estimating%20dense%20camera%20poses%20from%20unconventional%2C%0Astochastic%2C%20and%20high-speed%20binary%20frame%20sequences%20captured%20by%20SPCs.%20We%0Ademonstrate%2C%20both%20via%20simulations%20and%20a%20SPC%20hardware%20prototype%2C%20high-fidelity%0Areconstructions%20under%20high-speed%20motion%2C%20in%20low%20light%2C%20and%20for%20extreme%20dynamic%0Arange%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09386v1&entry.124074799=Read"},
{"title": "A Fair Ranking and New Model for Panoptic Scene Graph Generation", "author": "Julian Lorenz and Alexander Pest and Daniel Kienzle and Katja Ludwig and Rainer Lienhart", "abstract": "  In panoptic scene graph generation (PSGG), models retrieve interactions\nbetween objects in an image which are grounded by panoptic segmentation masks.\nPrevious evaluations on panoptic scene graphs have been subject to an erroneous\nevaluation protocol where multiple masks for the same object can lead to\nmultiple relation distributions per mask-mask pair. This can be exploited to\nincrease the final score. We correct this flaw and provide a fair ranking over\na wide range of existing PSGG models. The observed scores for existing methods\nincrease by up to 7.4 mR@50 for all two-stage methods, while dropping by up to\n19.3 mR@50 for all one-stage methods, highlighting the importance of a correct\nevaluation. Contrary to recent publications, we show that existing two-stage\nmethods are competitive to one-stage methods. Building on this, we introduce\nthe Decoupled SceneFormer (DSFormer), a novel two-stage model that outperforms\nall existing scene graph models by a large margin of +11 mR@50 and +10 mNgR@50\non the corrected evaluation, thus setting a new SOTA. As a core design\nprinciple, DSFormer encodes subject and object masks directly into feature\nspace.\n", "link": "http://arxiv.org/abs/2407.09216v1", "date": "2024-07-12", "relevancy": 2.1893, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5481}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.548}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Fair%20Ranking%20and%20New%20Model%20for%20Panoptic%20Scene%20Graph%20Generation&body=Title%3A%20A%20Fair%20Ranking%20and%20New%20Model%20for%20Panoptic%20Scene%20Graph%20Generation%0AAuthor%3A%20Julian%20Lorenz%20and%20Alexander%20Pest%20and%20Daniel%20Kienzle%20and%20Katja%20Ludwig%20and%20Rainer%20Lienhart%0AAbstract%3A%20%20%20In%20panoptic%20scene%20graph%20generation%20%28PSGG%29%2C%20models%20retrieve%20interactions%0Abetween%20objects%20in%20an%20image%20which%20are%20grounded%20by%20panoptic%20segmentation%20masks.%0APrevious%20evaluations%20on%20panoptic%20scene%20graphs%20have%20been%20subject%20to%20an%20erroneous%0Aevaluation%20protocol%20where%20multiple%20masks%20for%20the%20same%20object%20can%20lead%20to%0Amultiple%20relation%20distributions%20per%20mask-mask%20pair.%20This%20can%20be%20exploited%20to%0Aincrease%20the%20final%20score.%20We%20correct%20this%20flaw%20and%20provide%20a%20fair%20ranking%20over%0Aa%20wide%20range%20of%20existing%20PSGG%20models.%20The%20observed%20scores%20for%20existing%20methods%0Aincrease%20by%20up%20to%207.4%20mR%4050%20for%20all%20two-stage%20methods%2C%20while%20dropping%20by%20up%20to%0A19.3%20mR%4050%20for%20all%20one-stage%20methods%2C%20highlighting%20the%20importance%20of%20a%20correct%0Aevaluation.%20Contrary%20to%20recent%20publications%2C%20we%20show%20that%20existing%20two-stage%0Amethods%20are%20competitive%20to%20one-stage%20methods.%20Building%20on%20this%2C%20we%20introduce%0Athe%20Decoupled%20SceneFormer%20%28DSFormer%29%2C%20a%20novel%20two-stage%20model%20that%20outperforms%0Aall%20existing%20scene%20graph%20models%20by%20a%20large%20margin%20of%20%2B11%20mR%4050%20and%20%2B10%20mNgR%4050%0Aon%20the%20corrected%20evaluation%2C%20thus%20setting%20a%20new%20SOTA.%20As%20a%20core%20design%0Aprinciple%2C%20DSFormer%20encodes%20subject%20and%20object%20masks%20directly%20into%20feature%0Aspace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Fair%2520Ranking%2520and%2520New%2520Model%2520for%2520Panoptic%2520Scene%2520Graph%2520Generation%26entry.906535625%3DJulian%2520Lorenz%2520and%2520Alexander%2520Pest%2520and%2520Daniel%2520Kienzle%2520and%2520Katja%2520Ludwig%2520and%2520Rainer%2520Lienhart%26entry.1292438233%3D%2520%2520In%2520panoptic%2520scene%2520graph%2520generation%2520%2528PSGG%2529%252C%2520models%2520retrieve%2520interactions%250Abetween%2520objects%2520in%2520an%2520image%2520which%2520are%2520grounded%2520by%2520panoptic%2520segmentation%2520masks.%250APrevious%2520evaluations%2520on%2520panoptic%2520scene%2520graphs%2520have%2520been%2520subject%2520to%2520an%2520erroneous%250Aevaluation%2520protocol%2520where%2520multiple%2520masks%2520for%2520the%2520same%2520object%2520can%2520lead%2520to%250Amultiple%2520relation%2520distributions%2520per%2520mask-mask%2520pair.%2520This%2520can%2520be%2520exploited%2520to%250Aincrease%2520the%2520final%2520score.%2520We%2520correct%2520this%2520flaw%2520and%2520provide%2520a%2520fair%2520ranking%2520over%250Aa%2520wide%2520range%2520of%2520existing%2520PSGG%2520models.%2520The%2520observed%2520scores%2520for%2520existing%2520methods%250Aincrease%2520by%2520up%2520to%25207.4%2520mR%254050%2520for%2520all%2520two-stage%2520methods%252C%2520while%2520dropping%2520by%2520up%2520to%250A19.3%2520mR%254050%2520for%2520all%2520one-stage%2520methods%252C%2520highlighting%2520the%2520importance%2520of%2520a%2520correct%250Aevaluation.%2520Contrary%2520to%2520recent%2520publications%252C%2520we%2520show%2520that%2520existing%2520two-stage%250Amethods%2520are%2520competitive%2520to%2520one-stage%2520methods.%2520Building%2520on%2520this%252C%2520we%2520introduce%250Athe%2520Decoupled%2520SceneFormer%2520%2528DSFormer%2529%252C%2520a%2520novel%2520two-stage%2520model%2520that%2520outperforms%250Aall%2520existing%2520scene%2520graph%2520models%2520by%2520a%2520large%2520margin%2520of%2520%252B11%2520mR%254050%2520and%2520%252B10%2520mNgR%254050%250Aon%2520the%2520corrected%2520evaluation%252C%2520thus%2520setting%2520a%2520new%2520SOTA.%2520As%2520a%2520core%2520design%250Aprinciple%252C%2520DSFormer%2520encodes%2520subject%2520and%2520object%2520masks%2520directly%2520into%2520feature%250Aspace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fair%20Ranking%20and%20New%20Model%20for%20Panoptic%20Scene%20Graph%20Generation&entry.906535625=Julian%20Lorenz%20and%20Alexander%20Pest%20and%20Daniel%20Kienzle%20and%20Katja%20Ludwig%20and%20Rainer%20Lienhart&entry.1292438233=%20%20In%20panoptic%20scene%20graph%20generation%20%28PSGG%29%2C%20models%20retrieve%20interactions%0Abetween%20objects%20in%20an%20image%20which%20are%20grounded%20by%20panoptic%20segmentation%20masks.%0APrevious%20evaluations%20on%20panoptic%20scene%20graphs%20have%20been%20subject%20to%20an%20erroneous%0Aevaluation%20protocol%20where%20multiple%20masks%20for%20the%20same%20object%20can%20lead%20to%0Amultiple%20relation%20distributions%20per%20mask-mask%20pair.%20This%20can%20be%20exploited%20to%0Aincrease%20the%20final%20score.%20We%20correct%20this%20flaw%20and%20provide%20a%20fair%20ranking%20over%0Aa%20wide%20range%20of%20existing%20PSGG%20models.%20The%20observed%20scores%20for%20existing%20methods%0Aincrease%20by%20up%20to%207.4%20mR%4050%20for%20all%20two-stage%20methods%2C%20while%20dropping%20by%20up%20to%0A19.3%20mR%4050%20for%20all%20one-stage%20methods%2C%20highlighting%20the%20importance%20of%20a%20correct%0Aevaluation.%20Contrary%20to%20recent%20publications%2C%20we%20show%20that%20existing%20two-stage%0Amethods%20are%20competitive%20to%20one-stage%20methods.%20Building%20on%20this%2C%20we%20introduce%0Athe%20Decoupled%20SceneFormer%20%28DSFormer%29%2C%20a%20novel%20two-stage%20model%20that%20outperforms%0Aall%20existing%20scene%20graph%20models%20by%20a%20large%20margin%20of%20%2B11%20mR%4050%20and%20%2B10%20mNgR%4050%0Aon%20the%20corrected%20evaluation%2C%20thus%20setting%20a%20new%20SOTA.%20As%20a%20core%20design%0Aprinciple%2C%20DSFormer%20encodes%20subject%20and%20object%20masks%20directly%20into%20feature%0Aspace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09216v1&entry.124074799=Read"},
{"title": "Instruction Following with Goal-Conditioned Reinforcement Learning in\n  Virtual Environments", "author": "Zoya Volovikova and Alexey Skrynnik and Petr Kuderov and Aleksandr I. Panov", "abstract": "  In this study, we address the issue of enabling an artificial intelligence\nagent to execute complex language instructions within virtual environments. In\nour framework, we assume that these instructions involve intricate linguistic\nstructures and multiple interdependent tasks that must be navigated\nsuccessfully to achieve the desired outcomes. To effectively manage these\ncomplexities, we propose a hierarchical framework that combines the deep\nlanguage comprehension of large language models with the adaptive\naction-execution capabilities of reinforcement learning agents. The language\nmodule (based on LLM) translates the language instruction into a high-level\naction plan, which is then executed by a pre-trained reinforcement learning\nagent. We have demonstrated the effectiveness of our approach in two different\nenvironments: in IGLU, where agents are instructed to build structures, and in\nCrafter, where agents perform tasks and interact with objects in the\nsurrounding environment according to language commands.\n", "link": "http://arxiv.org/abs/2407.09287v1", "date": "2024-07-12", "relevancy": 2.1874, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5618}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.544}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction%20Following%20with%20Goal-Conditioned%20Reinforcement%20Learning%20in%0A%20%20Virtual%20Environments&body=Title%3A%20Instruction%20Following%20with%20Goal-Conditioned%20Reinforcement%20Learning%20in%0A%20%20Virtual%20Environments%0AAuthor%3A%20Zoya%20Volovikova%20and%20Alexey%20Skrynnik%20and%20Petr%20Kuderov%20and%20Aleksandr%20I.%20Panov%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20address%20the%20issue%20of%20enabling%20an%20artificial%20intelligence%0Aagent%20to%20execute%20complex%20language%20instructions%20within%20virtual%20environments.%20In%0Aour%20framework%2C%20we%20assume%20that%20these%20instructions%20involve%20intricate%20linguistic%0Astructures%20and%20multiple%20interdependent%20tasks%20that%20must%20be%20navigated%0Asuccessfully%20to%20achieve%20the%20desired%20outcomes.%20To%20effectively%20manage%20these%0Acomplexities%2C%20we%20propose%20a%20hierarchical%20framework%20that%20combines%20the%20deep%0Alanguage%20comprehension%20of%20large%20language%20models%20with%20the%20adaptive%0Aaction-execution%20capabilities%20of%20reinforcement%20learning%20agents.%20The%20language%0Amodule%20%28based%20on%20LLM%29%20translates%20the%20language%20instruction%20into%20a%20high-level%0Aaction%20plan%2C%20which%20is%20then%20executed%20by%20a%20pre-trained%20reinforcement%20learning%0Aagent.%20We%20have%20demonstrated%20the%20effectiveness%20of%20our%20approach%20in%20two%20different%0Aenvironments%3A%20in%20IGLU%2C%20where%20agents%20are%20instructed%20to%20build%20structures%2C%20and%20in%0ACrafter%2C%20where%20agents%20perform%20tasks%20and%20interact%20with%20objects%20in%20the%0Asurrounding%20environment%20according%20to%20language%20commands.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction%2520Following%2520with%2520Goal-Conditioned%2520Reinforcement%2520Learning%2520in%250A%2520%2520Virtual%2520Environments%26entry.906535625%3DZoya%2520Volovikova%2520and%2520Alexey%2520Skrynnik%2520and%2520Petr%2520Kuderov%2520and%2520Aleksandr%2520I.%2520Panov%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520address%2520the%2520issue%2520of%2520enabling%2520an%2520artificial%2520intelligence%250Aagent%2520to%2520execute%2520complex%2520language%2520instructions%2520within%2520virtual%2520environments.%2520In%250Aour%2520framework%252C%2520we%2520assume%2520that%2520these%2520instructions%2520involve%2520intricate%2520linguistic%250Astructures%2520and%2520multiple%2520interdependent%2520tasks%2520that%2520must%2520be%2520navigated%250Asuccessfully%2520to%2520achieve%2520the%2520desired%2520outcomes.%2520To%2520effectively%2520manage%2520these%250Acomplexities%252C%2520we%2520propose%2520a%2520hierarchical%2520framework%2520that%2520combines%2520the%2520deep%250Alanguage%2520comprehension%2520of%2520large%2520language%2520models%2520with%2520the%2520adaptive%250Aaction-execution%2520capabilities%2520of%2520reinforcement%2520learning%2520agents.%2520The%2520language%250Amodule%2520%2528based%2520on%2520LLM%2529%2520translates%2520the%2520language%2520instruction%2520into%2520a%2520high-level%250Aaction%2520plan%252C%2520which%2520is%2520then%2520executed%2520by%2520a%2520pre-trained%2520reinforcement%2520learning%250Aagent.%2520We%2520have%2520demonstrated%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520two%2520different%250Aenvironments%253A%2520in%2520IGLU%252C%2520where%2520agents%2520are%2520instructed%2520to%2520build%2520structures%252C%2520and%2520in%250ACrafter%252C%2520where%2520agents%2520perform%2520tasks%2520and%2520interact%2520with%2520objects%2520in%2520the%250Asurrounding%2520environment%2520according%2520to%2520language%2520commands.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction%20Following%20with%20Goal-Conditioned%20Reinforcement%20Learning%20in%0A%20%20Virtual%20Environments&entry.906535625=Zoya%20Volovikova%20and%20Alexey%20Skrynnik%20and%20Petr%20Kuderov%20and%20Aleksandr%20I.%20Panov&entry.1292438233=%20%20In%20this%20study%2C%20we%20address%20the%20issue%20of%20enabling%20an%20artificial%20intelligence%0Aagent%20to%20execute%20complex%20language%20instructions%20within%20virtual%20environments.%20In%0Aour%20framework%2C%20we%20assume%20that%20these%20instructions%20involve%20intricate%20linguistic%0Astructures%20and%20multiple%20interdependent%20tasks%20that%20must%20be%20navigated%0Asuccessfully%20to%20achieve%20the%20desired%20outcomes.%20To%20effectively%20manage%20these%0Acomplexities%2C%20we%20propose%20a%20hierarchical%20framework%20that%20combines%20the%20deep%0Alanguage%20comprehension%20of%20large%20language%20models%20with%20the%20adaptive%0Aaction-execution%20capabilities%20of%20reinforcement%20learning%20agents.%20The%20language%0Amodule%20%28based%20on%20LLM%29%20translates%20the%20language%20instruction%20into%20a%20high-level%0Aaction%20plan%2C%20which%20is%20then%20executed%20by%20a%20pre-trained%20reinforcement%20learning%0Aagent.%20We%20have%20demonstrated%20the%20effectiveness%20of%20our%20approach%20in%20two%20different%0Aenvironments%3A%20in%20IGLU%2C%20where%20agents%20are%20instructed%20to%20build%20structures%2C%20and%20in%0ACrafter%2C%20where%20agents%20perform%20tasks%20and%20interact%20with%20objects%20in%20the%0Asurrounding%20environment%20according%20to%20language%20commands.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09287v1&entry.124074799=Read"},
{"title": "Rethinking temporal self-similarity for repetitive action counting", "author": "Yanan Luo and Jinhui Yi and Yazan Abu Farha and Moritz Wolter and Juergen Gall", "abstract": "  Counting repetitive actions in long untrimmed videos is a challenging task\nthat has many applications such as rehabilitation. State-of-the-art methods\npredict action counts by first generating a temporal self-similarity matrix\n(TSM) from the sampled frames and then feeding the matrix to a predictor\nnetwork. The self-similarity matrix, however, is not an optimal input to a\nnetwork since it discards too much information from the frame-wise embeddings.\nWe thus rethink how a TSM can be utilized for counting repetitive actions and\npropose a framework that learns embeddings and predicts action start\nprobabilities at full temporal resolution. The number of repeated actions is\nthen inferred from the action start probabilities. In contrast to current\napproaches that have the TSM as an intermediate representation, we propose a\nnovel loss based on a generated reference TSM, which enforces that the\nself-similarity of the learned frame-wise embeddings is consistent with the\nself-similarity of repeated actions. The proposed framework achieves\nstate-of-the-art results on three datasets, i.e., RepCount, UCFRep, and\nCountix.\n", "link": "http://arxiv.org/abs/2407.09431v1", "date": "2024-07-12", "relevancy": 2.1811, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5897}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5346}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20temporal%20self-similarity%20for%20repetitive%20action%20counting&body=Title%3A%20Rethinking%20temporal%20self-similarity%20for%20repetitive%20action%20counting%0AAuthor%3A%20Yanan%20Luo%20and%20Jinhui%20Yi%20and%20Yazan%20Abu%20Farha%20and%20Moritz%20Wolter%20and%20Juergen%20Gall%0AAbstract%3A%20%20%20Counting%20repetitive%20actions%20in%20long%20untrimmed%20videos%20is%20a%20challenging%20task%0Athat%20has%20many%20applications%20such%20as%20rehabilitation.%20State-of-the-art%20methods%0Apredict%20action%20counts%20by%20first%20generating%20a%20temporal%20self-similarity%20matrix%0A%28TSM%29%20from%20the%20sampled%20frames%20and%20then%20feeding%20the%20matrix%20to%20a%20predictor%0Anetwork.%20The%20self-similarity%20matrix%2C%20however%2C%20is%20not%20an%20optimal%20input%20to%20a%0Anetwork%20since%20it%20discards%20too%20much%20information%20from%20the%20frame-wise%20embeddings.%0AWe%20thus%20rethink%20how%20a%20TSM%20can%20be%20utilized%20for%20counting%20repetitive%20actions%20and%0Apropose%20a%20framework%20that%20learns%20embeddings%20and%20predicts%20action%20start%0Aprobabilities%20at%20full%20temporal%20resolution.%20The%20number%20of%20repeated%20actions%20is%0Athen%20inferred%20from%20the%20action%20start%20probabilities.%20In%20contrast%20to%20current%0Aapproaches%20that%20have%20the%20TSM%20as%20an%20intermediate%20representation%2C%20we%20propose%20a%0Anovel%20loss%20based%20on%20a%20generated%20reference%20TSM%2C%20which%20enforces%20that%20the%0Aself-similarity%20of%20the%20learned%20frame-wise%20embeddings%20is%20consistent%20with%20the%0Aself-similarity%20of%20repeated%20actions.%20The%20proposed%20framework%20achieves%0Astate-of-the-art%20results%20on%20three%20datasets%2C%20i.e.%2C%20RepCount%2C%20UCFRep%2C%20and%0ACountix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520temporal%2520self-similarity%2520for%2520repetitive%2520action%2520counting%26entry.906535625%3DYanan%2520Luo%2520and%2520Jinhui%2520Yi%2520and%2520Yazan%2520Abu%2520Farha%2520and%2520Moritz%2520Wolter%2520and%2520Juergen%2520Gall%26entry.1292438233%3D%2520%2520Counting%2520repetitive%2520actions%2520in%2520long%2520untrimmed%2520videos%2520is%2520a%2520challenging%2520task%250Athat%2520has%2520many%2520applications%2520such%2520as%2520rehabilitation.%2520State-of-the-art%2520methods%250Apredict%2520action%2520counts%2520by%2520first%2520generating%2520a%2520temporal%2520self-similarity%2520matrix%250A%2528TSM%2529%2520from%2520the%2520sampled%2520frames%2520and%2520then%2520feeding%2520the%2520matrix%2520to%2520a%2520predictor%250Anetwork.%2520The%2520self-similarity%2520matrix%252C%2520however%252C%2520is%2520not%2520an%2520optimal%2520input%2520to%2520a%250Anetwork%2520since%2520it%2520discards%2520too%2520much%2520information%2520from%2520the%2520frame-wise%2520embeddings.%250AWe%2520thus%2520rethink%2520how%2520a%2520TSM%2520can%2520be%2520utilized%2520for%2520counting%2520repetitive%2520actions%2520and%250Apropose%2520a%2520framework%2520that%2520learns%2520embeddings%2520and%2520predicts%2520action%2520start%250Aprobabilities%2520at%2520full%2520temporal%2520resolution.%2520The%2520number%2520of%2520repeated%2520actions%2520is%250Athen%2520inferred%2520from%2520the%2520action%2520start%2520probabilities.%2520In%2520contrast%2520to%2520current%250Aapproaches%2520that%2520have%2520the%2520TSM%2520as%2520an%2520intermediate%2520representation%252C%2520we%2520propose%2520a%250Anovel%2520loss%2520based%2520on%2520a%2520generated%2520reference%2520TSM%252C%2520which%2520enforces%2520that%2520the%250Aself-similarity%2520of%2520the%2520learned%2520frame-wise%2520embeddings%2520is%2520consistent%2520with%2520the%250Aself-similarity%2520of%2520repeated%2520actions.%2520The%2520proposed%2520framework%2520achieves%250Astate-of-the-art%2520results%2520on%2520three%2520datasets%252C%2520i.e.%252C%2520RepCount%252C%2520UCFRep%252C%2520and%250ACountix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20temporal%20self-similarity%20for%20repetitive%20action%20counting&entry.906535625=Yanan%20Luo%20and%20Jinhui%20Yi%20and%20Yazan%20Abu%20Farha%20and%20Moritz%20Wolter%20and%20Juergen%20Gall&entry.1292438233=%20%20Counting%20repetitive%20actions%20in%20long%20untrimmed%20videos%20is%20a%20challenging%20task%0Athat%20has%20many%20applications%20such%20as%20rehabilitation.%20State-of-the-art%20methods%0Apredict%20action%20counts%20by%20first%20generating%20a%20temporal%20self-similarity%20matrix%0A%28TSM%29%20from%20the%20sampled%20frames%20and%20then%20feeding%20the%20matrix%20to%20a%20predictor%0Anetwork.%20The%20self-similarity%20matrix%2C%20however%2C%20is%20not%20an%20optimal%20input%20to%20a%0Anetwork%20since%20it%20discards%20too%20much%20information%20from%20the%20frame-wise%20embeddings.%0AWe%20thus%20rethink%20how%20a%20TSM%20can%20be%20utilized%20for%20counting%20repetitive%20actions%20and%0Apropose%20a%20framework%20that%20learns%20embeddings%20and%20predicts%20action%20start%0Aprobabilities%20at%20full%20temporal%20resolution.%20The%20number%20of%20repeated%20actions%20is%0Athen%20inferred%20from%20the%20action%20start%20probabilities.%20In%20contrast%20to%20current%0Aapproaches%20that%20have%20the%20TSM%20as%20an%20intermediate%20representation%2C%20we%20propose%20a%0Anovel%20loss%20based%20on%20a%20generated%20reference%20TSM%2C%20which%20enforces%20that%20the%0Aself-similarity%20of%20the%20learned%20frame-wise%20embeddings%20is%20consistent%20with%20the%0Aself-similarity%20of%20repeated%20actions.%20The%20proposed%20framework%20achieves%0Astate-of-the-art%20results%20on%20three%20datasets%2C%20i.e.%2C%20RepCount%2C%20UCFRep%2C%20and%0ACountix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09431v1&entry.124074799=Read"},
{"title": "Let Me DeCode You: Decoder Conditioning with Tabular Data", "author": "Tomasz Szczepa\u0144ski and Michal K. Grzeszczyk and Szymon P\u0142otka and Arleta Adamowicz and Piotr Fudalej and Przemys\u0142aw Korzeniowski and Tomasz Trzci\u0144ski and Arkadiusz Sitek", "abstract": "  Training deep neural networks for 3D segmentation tasks can be challenging,\noften requiring efficient and effective strategies to improve model\nperformance. In this study, we introduce a novel approach, DeCode, that\nutilizes label-derived features for model conditioning to support the decoder\nin the reconstruction process dynamically, aiming to enhance the efficiency of\nthe training process. DeCode focuses on improving 3D segmentation performance\nthrough the incorporation of conditioning embedding with learned numerical\nrepresentation of 3D-label shape features. Specifically, we develop an\napproach, where conditioning is applied during the training phase to guide the\nnetwork toward robust segmentation. When labels are not available during\ninference, our model infers the necessary conditioning embedding directly from\nthe input data, thanks to a feed-forward network learned during the training\nphase. This approach is tested using synthetic data and cone-beam computed\ntomography (CBCT) images of teeth. For CBCT, three datasets are used: one\npublicly available and two in-house. Our results show that DeCode significantly\noutperforms traditional, unconditioned models in terms of generalization to\nunseen data, achieving higher accuracy at a reduced computational cost. This\nwork represents the first of its kind to explore conditioning strategies in 3D\ndata segmentation, offering a novel and more efficient method for leveraging\nannotated data. Our code, pre-trained models are publicly available at\nhttps://github.com/SanoScience/DeCode .\n", "link": "http://arxiv.org/abs/2407.09437v1", "date": "2024-07-12", "relevancy": 2.1739, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5575}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5463}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%20Me%20DeCode%20You%3A%20Decoder%20Conditioning%20with%20Tabular%20Data&body=Title%3A%20Let%20Me%20DeCode%20You%3A%20Decoder%20Conditioning%20with%20Tabular%20Data%0AAuthor%3A%20Tomasz%20Szczepa%C5%84ski%20and%20Michal%20K.%20Grzeszczyk%20and%20Szymon%20P%C5%82otka%20and%20Arleta%20Adamowicz%20and%20Piotr%20Fudalej%20and%20Przemys%C5%82aw%20Korzeniowski%20and%20Tomasz%20Trzci%C5%84ski%20and%20Arkadiusz%20Sitek%0AAbstract%3A%20%20%20Training%20deep%20neural%20networks%20for%203D%20segmentation%20tasks%20can%20be%20challenging%2C%0Aoften%20requiring%20efficient%20and%20effective%20strategies%20to%20improve%20model%0Aperformance.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20approach%2C%20DeCode%2C%20that%0Autilizes%20label-derived%20features%20for%20model%20conditioning%20to%20support%20the%20decoder%0Ain%20the%20reconstruction%20process%20dynamically%2C%20aiming%20to%20enhance%20the%20efficiency%20of%0Athe%20training%20process.%20DeCode%20focuses%20on%20improving%203D%20segmentation%20performance%0Athrough%20the%20incorporation%20of%20conditioning%20embedding%20with%20learned%20numerical%0Arepresentation%20of%203D-label%20shape%20features.%20Specifically%2C%20we%20develop%20an%0Aapproach%2C%20where%20conditioning%20is%20applied%20during%20the%20training%20phase%20to%20guide%20the%0Anetwork%20toward%20robust%20segmentation.%20When%20labels%20are%20not%20available%20during%0Ainference%2C%20our%20model%20infers%20the%20necessary%20conditioning%20embedding%20directly%20from%0Athe%20input%20data%2C%20thanks%20to%20a%20feed-forward%20network%20learned%20during%20the%20training%0Aphase.%20This%20approach%20is%20tested%20using%20synthetic%20data%20and%20cone-beam%20computed%0Atomography%20%28CBCT%29%20images%20of%20teeth.%20For%20CBCT%2C%20three%20datasets%20are%20used%3A%20one%0Apublicly%20available%20and%20two%20in-house.%20Our%20results%20show%20that%20DeCode%20significantly%0Aoutperforms%20traditional%2C%20unconditioned%20models%20in%20terms%20of%20generalization%20to%0Aunseen%20data%2C%20achieving%20higher%20accuracy%20at%20a%20reduced%20computational%20cost.%20This%0Awork%20represents%20the%20first%20of%20its%20kind%20to%20explore%20conditioning%20strategies%20in%203D%0Adata%20segmentation%2C%20offering%20a%20novel%20and%20more%20efficient%20method%20for%20leveraging%0Aannotated%20data.%20Our%20code%2C%20pre-trained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/SanoScience/DeCode%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2520Me%2520DeCode%2520You%253A%2520Decoder%2520Conditioning%2520with%2520Tabular%2520Data%26entry.906535625%3DTomasz%2520Szczepa%25C5%2584ski%2520and%2520Michal%2520K.%2520Grzeszczyk%2520and%2520Szymon%2520P%25C5%2582otka%2520and%2520Arleta%2520Adamowicz%2520and%2520Piotr%2520Fudalej%2520and%2520Przemys%25C5%2582aw%2520Korzeniowski%2520and%2520Tomasz%2520Trzci%25C5%2584ski%2520and%2520Arkadiusz%2520Sitek%26entry.1292438233%3D%2520%2520Training%2520deep%2520neural%2520networks%2520for%25203D%2520segmentation%2520tasks%2520can%2520be%2520challenging%252C%250Aoften%2520requiring%2520efficient%2520and%2520effective%2520strategies%2520to%2520improve%2520model%250Aperformance.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520approach%252C%2520DeCode%252C%2520that%250Autilizes%2520label-derived%2520features%2520for%2520model%2520conditioning%2520to%2520support%2520the%2520decoder%250Ain%2520the%2520reconstruction%2520process%2520dynamically%252C%2520aiming%2520to%2520enhance%2520the%2520efficiency%2520of%250Athe%2520training%2520process.%2520DeCode%2520focuses%2520on%2520improving%25203D%2520segmentation%2520performance%250Athrough%2520the%2520incorporation%2520of%2520conditioning%2520embedding%2520with%2520learned%2520numerical%250Arepresentation%2520of%25203D-label%2520shape%2520features.%2520Specifically%252C%2520we%2520develop%2520an%250Aapproach%252C%2520where%2520conditioning%2520is%2520applied%2520during%2520the%2520training%2520phase%2520to%2520guide%2520the%250Anetwork%2520toward%2520robust%2520segmentation.%2520When%2520labels%2520are%2520not%2520available%2520during%250Ainference%252C%2520our%2520model%2520infers%2520the%2520necessary%2520conditioning%2520embedding%2520directly%2520from%250Athe%2520input%2520data%252C%2520thanks%2520to%2520a%2520feed-forward%2520network%2520learned%2520during%2520the%2520training%250Aphase.%2520This%2520approach%2520is%2520tested%2520using%2520synthetic%2520data%2520and%2520cone-beam%2520computed%250Atomography%2520%2528CBCT%2529%2520images%2520of%2520teeth.%2520For%2520CBCT%252C%2520three%2520datasets%2520are%2520used%253A%2520one%250Apublicly%2520available%2520and%2520two%2520in-house.%2520Our%2520results%2520show%2520that%2520DeCode%2520significantly%250Aoutperforms%2520traditional%252C%2520unconditioned%2520models%2520in%2520terms%2520of%2520generalization%2520to%250Aunseen%2520data%252C%2520achieving%2520higher%2520accuracy%2520at%2520a%2520reduced%2520computational%2520cost.%2520This%250Awork%2520represents%2520the%2520first%2520of%2520its%2520kind%2520to%2520explore%2520conditioning%2520strategies%2520in%25203D%250Adata%2520segmentation%252C%2520offering%2520a%2520novel%2520and%2520more%2520efficient%2520method%2520for%2520leveraging%250Aannotated%2520data.%2520Our%2520code%252C%2520pre-trained%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/SanoScience/DeCode%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%20Me%20DeCode%20You%3A%20Decoder%20Conditioning%20with%20Tabular%20Data&entry.906535625=Tomasz%20Szczepa%C5%84ski%20and%20Michal%20K.%20Grzeszczyk%20and%20Szymon%20P%C5%82otka%20and%20Arleta%20Adamowicz%20and%20Piotr%20Fudalej%20and%20Przemys%C5%82aw%20Korzeniowski%20and%20Tomasz%20Trzci%C5%84ski%20and%20Arkadiusz%20Sitek&entry.1292438233=%20%20Training%20deep%20neural%20networks%20for%203D%20segmentation%20tasks%20can%20be%20challenging%2C%0Aoften%20requiring%20efficient%20and%20effective%20strategies%20to%20improve%20model%0Aperformance.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20approach%2C%20DeCode%2C%20that%0Autilizes%20label-derived%20features%20for%20model%20conditioning%20to%20support%20the%20decoder%0Ain%20the%20reconstruction%20process%20dynamically%2C%20aiming%20to%20enhance%20the%20efficiency%20of%0Athe%20training%20process.%20DeCode%20focuses%20on%20improving%203D%20segmentation%20performance%0Athrough%20the%20incorporation%20of%20conditioning%20embedding%20with%20learned%20numerical%0Arepresentation%20of%203D-label%20shape%20features.%20Specifically%2C%20we%20develop%20an%0Aapproach%2C%20where%20conditioning%20is%20applied%20during%20the%20training%20phase%20to%20guide%20the%0Anetwork%20toward%20robust%20segmentation.%20When%20labels%20are%20not%20available%20during%0Ainference%2C%20our%20model%20infers%20the%20necessary%20conditioning%20embedding%20directly%20from%0Athe%20input%20data%2C%20thanks%20to%20a%20feed-forward%20network%20learned%20during%20the%20training%0Aphase.%20This%20approach%20is%20tested%20using%20synthetic%20data%20and%20cone-beam%20computed%0Atomography%20%28CBCT%29%20images%20of%20teeth.%20For%20CBCT%2C%20three%20datasets%20are%20used%3A%20one%0Apublicly%20available%20and%20two%20in-house.%20Our%20results%20show%20that%20DeCode%20significantly%0Aoutperforms%20traditional%2C%20unconditioned%20models%20in%20terms%20of%20generalization%20to%0Aunseen%20data%2C%20achieving%20higher%20accuracy%20at%20a%20reduced%20computational%20cost.%20This%0Awork%20represents%20the%20first%20of%20its%20kind%20to%20explore%20conditioning%20strategies%20in%203D%0Adata%20segmentation%2C%20offering%20a%20novel%20and%20more%20efficient%20method%20for%20leveraging%0Aannotated%20data.%20Our%20code%2C%20pre-trained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/SanoScience/DeCode%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09437v1&entry.124074799=Read"},
{"title": "Alternate Diverse Teaching for Semi-supervised Medical Image\n  Segmentation", "author": "Zhen Zhao and Zicheng Wang and Longyue Wang and Dian Yu and Yixuan Yuan and Luping Zhou", "abstract": "  Semi-supervised medical image segmentation studies have shown promise in\ntraining models with limited labeled data. However, current dominant\nteacher-student based approaches can suffer from the confirmation bias. To\naddress this challenge, we propose AD-MT, an alternate diverse teaching\napproach in a teacher-student framework. It involves a single student model and\ntwo non-trainable teacher models that are momentum-updated periodically and\nrandomly in an alternate fashion. To mitigate the confirmation bias from the\ndiverse supervision, the core of AD-MT lies in two proposed modules: the Random\nPeriodic Alternate (RPA) Updating Module and the Conflict-Combating Module\n(CCM). The RPA schedules the alternating diverse updating process with\ncomplementary data batches, distinct data augmentation, and random switching\nperiods to encourage diverse reasoning from different teaching perspectives.\nThe CCM employs an entropy-based ensembling strategy to encourage the model to\nlearn from both the consistent and conflicting predictions between the\nteachers. Experimental results demonstrate the effectiveness and superiority of\nour AD-MT on the 2D and 3D medical segmentation benchmarks across various\nsemi-supervised settings.\n", "link": "http://arxiv.org/abs/2311.17325v2", "date": "2024-07-12", "relevancy": 2.1732, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5681}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5443}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alternate%20Diverse%20Teaching%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Alternate%20Diverse%20Teaching%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Zhen%20Zhao%20and%20Zicheng%20Wang%20and%20Longyue%20Wang%20and%20Dian%20Yu%20and%20Yixuan%20Yuan%20and%20Luping%20Zhou%0AAbstract%3A%20%20%20Semi-supervised%20medical%20image%20segmentation%20studies%20have%20shown%20promise%20in%0Atraining%20models%20with%20limited%20labeled%20data.%20However%2C%20current%20dominant%0Ateacher-student%20based%20approaches%20can%20suffer%20from%20the%20confirmation%20bias.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20AD-MT%2C%20an%20alternate%20diverse%20teaching%0Aapproach%20in%20a%20teacher-student%20framework.%20It%20involves%20a%20single%20student%20model%20and%0Atwo%20non-trainable%20teacher%20models%20that%20are%20momentum-updated%20periodically%20and%0Arandomly%20in%20an%20alternate%20fashion.%20To%20mitigate%20the%20confirmation%20bias%20from%20the%0Adiverse%20supervision%2C%20the%20core%20of%20AD-MT%20lies%20in%20two%20proposed%20modules%3A%20the%20Random%0APeriodic%20Alternate%20%28RPA%29%20Updating%20Module%20and%20the%20Conflict-Combating%20Module%0A%28CCM%29.%20The%20RPA%20schedules%20the%20alternating%20diverse%20updating%20process%20with%0Acomplementary%20data%20batches%2C%20distinct%20data%20augmentation%2C%20and%20random%20switching%0Aperiods%20to%20encourage%20diverse%20reasoning%20from%20different%20teaching%20perspectives.%0AThe%20CCM%20employs%20an%20entropy-based%20ensembling%20strategy%20to%20encourage%20the%20model%20to%0Alearn%20from%20both%20the%20consistent%20and%20conflicting%20predictions%20between%20the%0Ateachers.%20Experimental%20results%20demonstrate%20the%20effectiveness%20and%20superiority%20of%0Aour%20AD-MT%20on%20the%202D%20and%203D%20medical%20segmentation%20benchmarks%20across%20various%0Asemi-supervised%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlternate%2520Diverse%2520Teaching%2520for%2520Semi-supervised%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DZhen%2520Zhao%2520and%2520Zicheng%2520Wang%2520and%2520Longyue%2520Wang%2520and%2520Dian%2520Yu%2520and%2520Yixuan%2520Yuan%2520and%2520Luping%2520Zhou%26entry.1292438233%3D%2520%2520Semi-supervised%2520medical%2520image%2520segmentation%2520studies%2520have%2520shown%2520promise%2520in%250Atraining%2520models%2520with%2520limited%2520labeled%2520data.%2520However%252C%2520current%2520dominant%250Ateacher-student%2520based%2520approaches%2520can%2520suffer%2520from%2520the%2520confirmation%2520bias.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520AD-MT%252C%2520an%2520alternate%2520diverse%2520teaching%250Aapproach%2520in%2520a%2520teacher-student%2520framework.%2520It%2520involves%2520a%2520single%2520student%2520model%2520and%250Atwo%2520non-trainable%2520teacher%2520models%2520that%2520are%2520momentum-updated%2520periodically%2520and%250Arandomly%2520in%2520an%2520alternate%2520fashion.%2520To%2520mitigate%2520the%2520confirmation%2520bias%2520from%2520the%250Adiverse%2520supervision%252C%2520the%2520core%2520of%2520AD-MT%2520lies%2520in%2520two%2520proposed%2520modules%253A%2520the%2520Random%250APeriodic%2520Alternate%2520%2528RPA%2529%2520Updating%2520Module%2520and%2520the%2520Conflict-Combating%2520Module%250A%2528CCM%2529.%2520The%2520RPA%2520schedules%2520the%2520alternating%2520diverse%2520updating%2520process%2520with%250Acomplementary%2520data%2520batches%252C%2520distinct%2520data%2520augmentation%252C%2520and%2520random%2520switching%250Aperiods%2520to%2520encourage%2520diverse%2520reasoning%2520from%2520different%2520teaching%2520perspectives.%250AThe%2520CCM%2520employs%2520an%2520entropy-based%2520ensembling%2520strategy%2520to%2520encourage%2520the%2520model%2520to%250Alearn%2520from%2520both%2520the%2520consistent%2520and%2520conflicting%2520predictions%2520between%2520the%250Ateachers.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520and%2520superiority%2520of%250Aour%2520AD-MT%2520on%2520the%25202D%2520and%25203D%2520medical%2520segmentation%2520benchmarks%2520across%2520various%250Asemi-supervised%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alternate%20Diverse%20Teaching%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Zhen%20Zhao%20and%20Zicheng%20Wang%20and%20Longyue%20Wang%20and%20Dian%20Yu%20and%20Yixuan%20Yuan%20and%20Luping%20Zhou&entry.1292438233=%20%20Semi-supervised%20medical%20image%20segmentation%20studies%20have%20shown%20promise%20in%0Atraining%20models%20with%20limited%20labeled%20data.%20However%2C%20current%20dominant%0Ateacher-student%20based%20approaches%20can%20suffer%20from%20the%20confirmation%20bias.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20AD-MT%2C%20an%20alternate%20diverse%20teaching%0Aapproach%20in%20a%20teacher-student%20framework.%20It%20involves%20a%20single%20student%20model%20and%0Atwo%20non-trainable%20teacher%20models%20that%20are%20momentum-updated%20periodically%20and%0Arandomly%20in%20an%20alternate%20fashion.%20To%20mitigate%20the%20confirmation%20bias%20from%20the%0Adiverse%20supervision%2C%20the%20core%20of%20AD-MT%20lies%20in%20two%20proposed%20modules%3A%20the%20Random%0APeriodic%20Alternate%20%28RPA%29%20Updating%20Module%20and%20the%20Conflict-Combating%20Module%0A%28CCM%29.%20The%20RPA%20schedules%20the%20alternating%20diverse%20updating%20process%20with%0Acomplementary%20data%20batches%2C%20distinct%20data%20augmentation%2C%20and%20random%20switching%0Aperiods%20to%20encourage%20diverse%20reasoning%20from%20different%20teaching%20perspectives.%0AThe%20CCM%20employs%20an%20entropy-based%20ensembling%20strategy%20to%20encourage%20the%20model%20to%0Alearn%20from%20both%20the%20consistent%20and%20conflicting%20predictions%20between%20the%0Ateachers.%20Experimental%20results%20demonstrate%20the%20effectiveness%20and%20superiority%20of%0Aour%20AD-MT%20on%20the%202D%20and%203D%20medical%20segmentation%20benchmarks%20across%20various%0Asemi-supervised%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17325v2&entry.124074799=Read"},
{"title": "Detect Closer Surfaces that can be Seen: New Modeling and Evaluation in\n  Cross-domain 3D Object Detection", "author": "Ruixiao Zhang and Yihong Wu and Juheon Lee and Adam Prugel-Bennett and Xiaohao Cai", "abstract": "  The performance of domain adaptation technologies has not yet reached an\nideal level in the current 3D object detection field for autonomous driving,\nwhich is mainly due to significant differences in the size of vehicles, as well\nas the environments they operate in when applied across domains. These factors\ntogether hinder the effective transfer and application of knowledge learned\nfrom specific datasets. Since the existing evaluation metrics are initially\ndesigned for evaluation on a single domain by calculating the 2D or 3D overlap\nbetween the prediction and ground-truth bounding boxes, they often suffer from\nthe overfitting problem caused by the size differences among datasets. This\nraises a fundamental question related to the evaluation of the 3D object\ndetection models' cross-domain performance: Do we really need models to\nmaintain excellent performance in their original 3D bounding boxes after being\napplied across domains? From a practical application perspective, one of our\nmain focuses is actually on preventing collisions between vehicles and other\nobstacles, especially in cross-domain scenarios where correctly predicting the\nsize of vehicles is much more difficult. In other words, as long as a model can\naccurately identify the closest surfaces to the ego vehicle, it is sufficient\nto effectively avoid obstacles. In this paper, we propose two metrics to\nmeasure 3D object detection models' ability of detecting the closer surfaces to\nthe sensor on the ego vehicle, which can be used to evaluate their cross-domain\nperformance more comprehensively and reasonably. Furthermore, we propose a\nrefinement head, named EdgeHead, to guide models to focus more on the learnable\ncloser surfaces, which can greatly improve the cross-domain performance of\nexisting models not only under our new metrics, but even also under the\noriginal BEV/3D metrics.\n", "link": "http://arxiv.org/abs/2407.04061v3", "date": "2024-07-12", "relevancy": 2.173, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detect%20Closer%20Surfaces%20that%20can%20be%20Seen%3A%20New%20Modeling%20and%20Evaluation%20in%0A%20%20Cross-domain%203D%20Object%20Detection&body=Title%3A%20Detect%20Closer%20Surfaces%20that%20can%20be%20Seen%3A%20New%20Modeling%20and%20Evaluation%20in%0A%20%20Cross-domain%203D%20Object%20Detection%0AAuthor%3A%20Ruixiao%20Zhang%20and%20Yihong%20Wu%20and%20Juheon%20Lee%20and%20Adam%20Prugel-Bennett%20and%20Xiaohao%20Cai%0AAbstract%3A%20%20%20The%20performance%20of%20domain%20adaptation%20technologies%20has%20not%20yet%20reached%20an%0Aideal%20level%20in%20the%20current%203D%20object%20detection%20field%20for%20autonomous%20driving%2C%0Awhich%20is%20mainly%20due%20to%20significant%20differences%20in%20the%20size%20of%20vehicles%2C%20as%20well%0Aas%20the%20environments%20they%20operate%20in%20when%20applied%20across%20domains.%20These%20factors%0Atogether%20hinder%20the%20effective%20transfer%20and%20application%20of%20knowledge%20learned%0Afrom%20specific%20datasets.%20Since%20the%20existing%20evaluation%20metrics%20are%20initially%0Adesigned%20for%20evaluation%20on%20a%20single%20domain%20by%20calculating%20the%202D%20or%203D%20overlap%0Abetween%20the%20prediction%20and%20ground-truth%20bounding%20boxes%2C%20they%20often%20suffer%20from%0Athe%20overfitting%20problem%20caused%20by%20the%20size%20differences%20among%20datasets.%20This%0Araises%20a%20fundamental%20question%20related%20to%20the%20evaluation%20of%20the%203D%20object%0Adetection%20models%27%20cross-domain%20performance%3A%20Do%20we%20really%20need%20models%20to%0Amaintain%20excellent%20performance%20in%20their%20original%203D%20bounding%20boxes%20after%20being%0Aapplied%20across%20domains%3F%20From%20a%20practical%20application%20perspective%2C%20one%20of%20our%0Amain%20focuses%20is%20actually%20on%20preventing%20collisions%20between%20vehicles%20and%20other%0Aobstacles%2C%20especially%20in%20cross-domain%20scenarios%20where%20correctly%20predicting%20the%0Asize%20of%20vehicles%20is%20much%20more%20difficult.%20In%20other%20words%2C%20as%20long%20as%20a%20model%20can%0Aaccurately%20identify%20the%20closest%20surfaces%20to%20the%20ego%20vehicle%2C%20it%20is%20sufficient%0Ato%20effectively%20avoid%20obstacles.%20In%20this%20paper%2C%20we%20propose%20two%20metrics%20to%0Ameasure%203D%20object%20detection%20models%27%20ability%20of%20detecting%20the%20closer%20surfaces%20to%0Athe%20sensor%20on%20the%20ego%20vehicle%2C%20which%20can%20be%20used%20to%20evaluate%20their%20cross-domain%0Aperformance%20more%20comprehensively%20and%20reasonably.%20Furthermore%2C%20we%20propose%20a%0Arefinement%20head%2C%20named%20EdgeHead%2C%20to%20guide%20models%20to%20focus%20more%20on%20the%20learnable%0Acloser%20surfaces%2C%20which%20can%20greatly%20improve%20the%20cross-domain%20performance%20of%0Aexisting%20models%20not%20only%20under%20our%20new%20metrics%2C%20but%20even%20also%20under%20the%0Aoriginal%20BEV/3D%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04061v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetect%2520Closer%2520Surfaces%2520that%2520can%2520be%2520Seen%253A%2520New%2520Modeling%2520and%2520Evaluation%2520in%250A%2520%2520Cross-domain%25203D%2520Object%2520Detection%26entry.906535625%3DRuixiao%2520Zhang%2520and%2520Yihong%2520Wu%2520and%2520Juheon%2520Lee%2520and%2520Adam%2520Prugel-Bennett%2520and%2520Xiaohao%2520Cai%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520domain%2520adaptation%2520technologies%2520has%2520not%2520yet%2520reached%2520an%250Aideal%2520level%2520in%2520the%2520current%25203D%2520object%2520detection%2520field%2520for%2520autonomous%2520driving%252C%250Awhich%2520is%2520mainly%2520due%2520to%2520significant%2520differences%2520in%2520the%2520size%2520of%2520vehicles%252C%2520as%2520well%250Aas%2520the%2520environments%2520they%2520operate%2520in%2520when%2520applied%2520across%2520domains.%2520These%2520factors%250Atogether%2520hinder%2520the%2520effective%2520transfer%2520and%2520application%2520of%2520knowledge%2520learned%250Afrom%2520specific%2520datasets.%2520Since%2520the%2520existing%2520evaluation%2520metrics%2520are%2520initially%250Adesigned%2520for%2520evaluation%2520on%2520a%2520single%2520domain%2520by%2520calculating%2520the%25202D%2520or%25203D%2520overlap%250Abetween%2520the%2520prediction%2520and%2520ground-truth%2520bounding%2520boxes%252C%2520they%2520often%2520suffer%2520from%250Athe%2520overfitting%2520problem%2520caused%2520by%2520the%2520size%2520differences%2520among%2520datasets.%2520This%250Araises%2520a%2520fundamental%2520question%2520related%2520to%2520the%2520evaluation%2520of%2520the%25203D%2520object%250Adetection%2520models%2527%2520cross-domain%2520performance%253A%2520Do%2520we%2520really%2520need%2520models%2520to%250Amaintain%2520excellent%2520performance%2520in%2520their%2520original%25203D%2520bounding%2520boxes%2520after%2520being%250Aapplied%2520across%2520domains%253F%2520From%2520a%2520practical%2520application%2520perspective%252C%2520one%2520of%2520our%250Amain%2520focuses%2520is%2520actually%2520on%2520preventing%2520collisions%2520between%2520vehicles%2520and%2520other%250Aobstacles%252C%2520especially%2520in%2520cross-domain%2520scenarios%2520where%2520correctly%2520predicting%2520the%250Asize%2520of%2520vehicles%2520is%2520much%2520more%2520difficult.%2520In%2520other%2520words%252C%2520as%2520long%2520as%2520a%2520model%2520can%250Aaccurately%2520identify%2520the%2520closest%2520surfaces%2520to%2520the%2520ego%2520vehicle%252C%2520it%2520is%2520sufficient%250Ato%2520effectively%2520avoid%2520obstacles.%2520In%2520this%2520paper%252C%2520we%2520propose%2520two%2520metrics%2520to%250Ameasure%25203D%2520object%2520detection%2520models%2527%2520ability%2520of%2520detecting%2520the%2520closer%2520surfaces%2520to%250Athe%2520sensor%2520on%2520the%2520ego%2520vehicle%252C%2520which%2520can%2520be%2520used%2520to%2520evaluate%2520their%2520cross-domain%250Aperformance%2520more%2520comprehensively%2520and%2520reasonably.%2520Furthermore%252C%2520we%2520propose%2520a%250Arefinement%2520head%252C%2520named%2520EdgeHead%252C%2520to%2520guide%2520models%2520to%2520focus%2520more%2520on%2520the%2520learnable%250Acloser%2520surfaces%252C%2520which%2520can%2520greatly%2520improve%2520the%2520cross-domain%2520performance%2520of%250Aexisting%2520models%2520not%2520only%2520under%2520our%2520new%2520metrics%252C%2520but%2520even%2520also%2520under%2520the%250Aoriginal%2520BEV/3D%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04061v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detect%20Closer%20Surfaces%20that%20can%20be%20Seen%3A%20New%20Modeling%20and%20Evaluation%20in%0A%20%20Cross-domain%203D%20Object%20Detection&entry.906535625=Ruixiao%20Zhang%20and%20Yihong%20Wu%20and%20Juheon%20Lee%20and%20Adam%20Prugel-Bennett%20and%20Xiaohao%20Cai&entry.1292438233=%20%20The%20performance%20of%20domain%20adaptation%20technologies%20has%20not%20yet%20reached%20an%0Aideal%20level%20in%20the%20current%203D%20object%20detection%20field%20for%20autonomous%20driving%2C%0Awhich%20is%20mainly%20due%20to%20significant%20differences%20in%20the%20size%20of%20vehicles%2C%20as%20well%0Aas%20the%20environments%20they%20operate%20in%20when%20applied%20across%20domains.%20These%20factors%0Atogether%20hinder%20the%20effective%20transfer%20and%20application%20of%20knowledge%20learned%0Afrom%20specific%20datasets.%20Since%20the%20existing%20evaluation%20metrics%20are%20initially%0Adesigned%20for%20evaluation%20on%20a%20single%20domain%20by%20calculating%20the%202D%20or%203D%20overlap%0Abetween%20the%20prediction%20and%20ground-truth%20bounding%20boxes%2C%20they%20often%20suffer%20from%0Athe%20overfitting%20problem%20caused%20by%20the%20size%20differences%20among%20datasets.%20This%0Araises%20a%20fundamental%20question%20related%20to%20the%20evaluation%20of%20the%203D%20object%0Adetection%20models%27%20cross-domain%20performance%3A%20Do%20we%20really%20need%20models%20to%0Amaintain%20excellent%20performance%20in%20their%20original%203D%20bounding%20boxes%20after%20being%0Aapplied%20across%20domains%3F%20From%20a%20practical%20application%20perspective%2C%20one%20of%20our%0Amain%20focuses%20is%20actually%20on%20preventing%20collisions%20between%20vehicles%20and%20other%0Aobstacles%2C%20especially%20in%20cross-domain%20scenarios%20where%20correctly%20predicting%20the%0Asize%20of%20vehicles%20is%20much%20more%20difficult.%20In%20other%20words%2C%20as%20long%20as%20a%20model%20can%0Aaccurately%20identify%20the%20closest%20surfaces%20to%20the%20ego%20vehicle%2C%20it%20is%20sufficient%0Ato%20effectively%20avoid%20obstacles.%20In%20this%20paper%2C%20we%20propose%20two%20metrics%20to%0Ameasure%203D%20object%20detection%20models%27%20ability%20of%20detecting%20the%20closer%20surfaces%20to%0Athe%20sensor%20on%20the%20ego%20vehicle%2C%20which%20can%20be%20used%20to%20evaluate%20their%20cross-domain%0Aperformance%20more%20comprehensively%20and%20reasonably.%20Furthermore%2C%20we%20propose%20a%0Arefinement%20head%2C%20named%20EdgeHead%2C%20to%20guide%20models%20to%20focus%20more%20on%20the%20learnable%0Acloser%20surfaces%2C%20which%20can%20greatly%20improve%20the%20cross-domain%20performance%20of%0Aexisting%20models%20not%20only%20under%20our%20new%20metrics%2C%20but%20even%20also%20under%20the%0Aoriginal%20BEV/3D%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04061v3&entry.124074799=Read"},
{"title": "Surgical Text-to-Image Generation", "author": "Chinedu Innocent Nwoye and Rupak Bose and Kareem Elgohary and Lorenzo Arboit and Giorgio Carlino and Jo\u00ebl L. Lavanchy and Pietro Mascagni and Nicolas Padoy", "abstract": "  Acquiring surgical data for research and development is significantly\nhindered by high annotation costs and practical and ethical constraints.\nUtilizing synthetically generated images could offer a valuable alternative. In\nthis work, we conduct an in-depth analysis on adapting text-to-image generative\nmodels for the surgical domain, leveraging the CholecT50 dataset, which\nprovides surgical images annotated with surgical action triplets (instrument,\nverb, target). We investigate various language models and find T5 to offer more\ndistinct features for differentiating surgical actions based on triplet-based\ntextual inputs. Our analysis demonstrates strong alignment between long and\ntriplet-based captions, supporting the use of triplet-based labels. We address\nthe challenges in training text-to-image models on triplet-based captions\nwithout additional input signals by uncovering that triplet text embeddings are\ninstrument-centric in the latent space and then, by designing an\ninstrument-based class balancing technique to counteract the imbalance and\nskewness in the surgical data, improving training convergence. Extending\nImagen, a diffusion-based generative model, we develop Surgical Imagen to\ngenerate photorealistic and activity-aligned surgical images from triplet-based\ntextual prompts. We evaluate our model using diverse metrics, including human\nexpert surveys and automated methods like FID and CLIP scores. We assess the\nmodel performance on key aspects: quality, alignment, reasoning, knowledge, and\nrobustness, demonstrating the effectiveness of our approach in providing a\nrealistic alternative to real data collection.\n", "link": "http://arxiv.org/abs/2407.09230v1", "date": "2024-07-12", "relevancy": 2.1708, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5374}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surgical%20Text-to-Image%20Generation&body=Title%3A%20Surgical%20Text-to-Image%20Generation%0AAuthor%3A%20Chinedu%20Innocent%20Nwoye%20and%20Rupak%20Bose%20and%20Kareem%20Elgohary%20and%20Lorenzo%20Arboit%20and%20Giorgio%20Carlino%20and%20Jo%C3%ABl%20L.%20Lavanchy%20and%20Pietro%20Mascagni%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Acquiring%20surgical%20data%20for%20research%20and%20development%20is%20significantly%0Ahindered%20by%20high%20annotation%20costs%20and%20practical%20and%20ethical%20constraints.%0AUtilizing%20synthetically%20generated%20images%20could%20offer%20a%20valuable%20alternative.%20In%0Athis%20work%2C%20we%20conduct%20an%20in-depth%20analysis%20on%20adapting%20text-to-image%20generative%0Amodels%20for%20the%20surgical%20domain%2C%20leveraging%20the%20CholecT50%20dataset%2C%20which%0Aprovides%20surgical%20images%20annotated%20with%20surgical%20action%20triplets%20%28instrument%2C%0Averb%2C%20target%29.%20We%20investigate%20various%20language%20models%20and%20find%20T5%20to%20offer%20more%0Adistinct%20features%20for%20differentiating%20surgical%20actions%20based%20on%20triplet-based%0Atextual%20inputs.%20Our%20analysis%20demonstrates%20strong%20alignment%20between%20long%20and%0Atriplet-based%20captions%2C%20supporting%20the%20use%20of%20triplet-based%20labels.%20We%20address%0Athe%20challenges%20in%20training%20text-to-image%20models%20on%20triplet-based%20captions%0Awithout%20additional%20input%20signals%20by%20uncovering%20that%20triplet%20text%20embeddings%20are%0Ainstrument-centric%20in%20the%20latent%20space%20and%20then%2C%20by%20designing%20an%0Ainstrument-based%20class%20balancing%20technique%20to%20counteract%20the%20imbalance%20and%0Askewness%20in%20the%20surgical%20data%2C%20improving%20training%20convergence.%20Extending%0AImagen%2C%20a%20diffusion-based%20generative%20model%2C%20we%20develop%20Surgical%20Imagen%20to%0Agenerate%20photorealistic%20and%20activity-aligned%20surgical%20images%20from%20triplet-based%0Atextual%20prompts.%20We%20evaluate%20our%20model%20using%20diverse%20metrics%2C%20including%20human%0Aexpert%20surveys%20and%20automated%20methods%20like%20FID%20and%20CLIP%20scores.%20We%20assess%20the%0Amodel%20performance%20on%20key%20aspects%3A%20quality%2C%20alignment%2C%20reasoning%2C%20knowledge%2C%20and%0Arobustness%2C%20demonstrating%20the%20effectiveness%20of%20our%20approach%20in%20providing%20a%0Arealistic%20alternative%20to%20real%20data%20collection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgical%2520Text-to-Image%2520Generation%26entry.906535625%3DChinedu%2520Innocent%2520Nwoye%2520and%2520Rupak%2520Bose%2520and%2520Kareem%2520Elgohary%2520and%2520Lorenzo%2520Arboit%2520and%2520Giorgio%2520Carlino%2520and%2520Jo%25C3%25ABl%2520L.%2520Lavanchy%2520and%2520Pietro%2520Mascagni%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Acquiring%2520surgical%2520data%2520for%2520research%2520and%2520development%2520is%2520significantly%250Ahindered%2520by%2520high%2520annotation%2520costs%2520and%2520practical%2520and%2520ethical%2520constraints.%250AUtilizing%2520synthetically%2520generated%2520images%2520could%2520offer%2520a%2520valuable%2520alternative.%2520In%250Athis%2520work%252C%2520we%2520conduct%2520an%2520in-depth%2520analysis%2520on%2520adapting%2520text-to-image%2520generative%250Amodels%2520for%2520the%2520surgical%2520domain%252C%2520leveraging%2520the%2520CholecT50%2520dataset%252C%2520which%250Aprovides%2520surgical%2520images%2520annotated%2520with%2520surgical%2520action%2520triplets%2520%2528instrument%252C%250Averb%252C%2520target%2529.%2520We%2520investigate%2520various%2520language%2520models%2520and%2520find%2520T5%2520to%2520offer%2520more%250Adistinct%2520features%2520for%2520differentiating%2520surgical%2520actions%2520based%2520on%2520triplet-based%250Atextual%2520inputs.%2520Our%2520analysis%2520demonstrates%2520strong%2520alignment%2520between%2520long%2520and%250Atriplet-based%2520captions%252C%2520supporting%2520the%2520use%2520of%2520triplet-based%2520labels.%2520We%2520address%250Athe%2520challenges%2520in%2520training%2520text-to-image%2520models%2520on%2520triplet-based%2520captions%250Awithout%2520additional%2520input%2520signals%2520by%2520uncovering%2520that%2520triplet%2520text%2520embeddings%2520are%250Ainstrument-centric%2520in%2520the%2520latent%2520space%2520and%2520then%252C%2520by%2520designing%2520an%250Ainstrument-based%2520class%2520balancing%2520technique%2520to%2520counteract%2520the%2520imbalance%2520and%250Askewness%2520in%2520the%2520surgical%2520data%252C%2520improving%2520training%2520convergence.%2520Extending%250AImagen%252C%2520a%2520diffusion-based%2520generative%2520model%252C%2520we%2520develop%2520Surgical%2520Imagen%2520to%250Agenerate%2520photorealistic%2520and%2520activity-aligned%2520surgical%2520images%2520from%2520triplet-based%250Atextual%2520prompts.%2520We%2520evaluate%2520our%2520model%2520using%2520diverse%2520metrics%252C%2520including%2520human%250Aexpert%2520surveys%2520and%2520automated%2520methods%2520like%2520FID%2520and%2520CLIP%2520scores.%2520We%2520assess%2520the%250Amodel%2520performance%2520on%2520key%2520aspects%253A%2520quality%252C%2520alignment%252C%2520reasoning%252C%2520knowledge%252C%2520and%250Arobustness%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520providing%2520a%250Arealistic%2520alternative%2520to%2520real%2520data%2520collection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surgical%20Text-to-Image%20Generation&entry.906535625=Chinedu%20Innocent%20Nwoye%20and%20Rupak%20Bose%20and%20Kareem%20Elgohary%20and%20Lorenzo%20Arboit%20and%20Giorgio%20Carlino%20and%20Jo%C3%ABl%20L.%20Lavanchy%20and%20Pietro%20Mascagni%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Acquiring%20surgical%20data%20for%20research%20and%20development%20is%20significantly%0Ahindered%20by%20high%20annotation%20costs%20and%20practical%20and%20ethical%20constraints.%0AUtilizing%20synthetically%20generated%20images%20could%20offer%20a%20valuable%20alternative.%20In%0Athis%20work%2C%20we%20conduct%20an%20in-depth%20analysis%20on%20adapting%20text-to-image%20generative%0Amodels%20for%20the%20surgical%20domain%2C%20leveraging%20the%20CholecT50%20dataset%2C%20which%0Aprovides%20surgical%20images%20annotated%20with%20surgical%20action%20triplets%20%28instrument%2C%0Averb%2C%20target%29.%20We%20investigate%20various%20language%20models%20and%20find%20T5%20to%20offer%20more%0Adistinct%20features%20for%20differentiating%20surgical%20actions%20based%20on%20triplet-based%0Atextual%20inputs.%20Our%20analysis%20demonstrates%20strong%20alignment%20between%20long%20and%0Atriplet-based%20captions%2C%20supporting%20the%20use%20of%20triplet-based%20labels.%20We%20address%0Athe%20challenges%20in%20training%20text-to-image%20models%20on%20triplet-based%20captions%0Awithout%20additional%20input%20signals%20by%20uncovering%20that%20triplet%20text%20embeddings%20are%0Ainstrument-centric%20in%20the%20latent%20space%20and%20then%2C%20by%20designing%20an%0Ainstrument-based%20class%20balancing%20technique%20to%20counteract%20the%20imbalance%20and%0Askewness%20in%20the%20surgical%20data%2C%20improving%20training%20convergence.%20Extending%0AImagen%2C%20a%20diffusion-based%20generative%20model%2C%20we%20develop%20Surgical%20Imagen%20to%0Agenerate%20photorealistic%20and%20activity-aligned%20surgical%20images%20from%20triplet-based%0Atextual%20prompts.%20We%20evaluate%20our%20model%20using%20diverse%20metrics%2C%20including%20human%0Aexpert%20surveys%20and%20automated%20methods%20like%20FID%20and%20CLIP%20scores.%20We%20assess%20the%0Amodel%20performance%20on%20key%20aspects%3A%20quality%2C%20alignment%2C%20reasoning%2C%20knowledge%2C%20and%0Arobustness%2C%20demonstrating%20the%20effectiveness%20of%20our%20approach%20in%20providing%20a%0Arealistic%20alternative%20to%20real%20data%20collection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09230v1&entry.124074799=Read"},
{"title": "Depth Information Assisted Collaborative Mutual Promotion Network for\n  Single Image Dehazing", "author": "Yafei Zhang and Shen Zhou and Huafeng Li", "abstract": "  Recovering a clear image from a single hazy image is an open inverse problem.\nAlthough significant research progress has been made, most existing methods\nignore the effect that downstream tasks play in promoting upstream dehazing.\nFrom the perspective of the haze generation mechanism, there is a potential\nrelationship between the depth information of the scene and the hazy image.\nBased on this, we propose a dual-task collaborative mutual promotion framework\nto achieve the dehazing of a single image. This framework integrates depth\nestimation and dehazing by a dual-task interaction mechanism and achieves\nmutual enhancement of their performance. To realize the joint optimization of\nthe two tasks, an alternative implementation mechanism with the difference\nperception is developed. On the one hand, the difference perception between the\ndepth maps of the dehazing result and the ideal image is proposed to promote\nthe dehazing network to pay attention to the non-ideal areas of the dehazing.\nOn the other hand, by improving the depth estimation performance in the\ndifficult-to-recover areas of the hazy image, the dehazing network can\nexplicitly use the depth information of the hazy image to assist the clear\nimage recovery. To promote the depth estimation, we propose to use the\ndifference between the dehazed image and the ground truth to guide the depth\nestimation network to focus on the dehazed unideal areas. It allows dehazing\nand depth estimation to leverage their strengths in a mutually reinforcing\nmanner. Experimental results show that the proposed method can achieve better\nperformance than that of the state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2403.01105v2", "date": "2024-07-12", "relevancy": 2.1687, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5661}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5406}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Information%20Assisted%20Collaborative%20Mutual%20Promotion%20Network%20for%0A%20%20Single%20Image%20Dehazing&body=Title%3A%20Depth%20Information%20Assisted%20Collaborative%20Mutual%20Promotion%20Network%20for%0A%20%20Single%20Image%20Dehazing%0AAuthor%3A%20Yafei%20Zhang%20and%20Shen%20Zhou%20and%20Huafeng%20Li%0AAbstract%3A%20%20%20Recovering%20a%20clear%20image%20from%20a%20single%20hazy%20image%20is%20an%20open%20inverse%20problem.%0AAlthough%20significant%20research%20progress%20has%20been%20made%2C%20most%20existing%20methods%0Aignore%20the%20effect%20that%20downstream%20tasks%20play%20in%20promoting%20upstream%20dehazing.%0AFrom%20the%20perspective%20of%20the%20haze%20generation%20mechanism%2C%20there%20is%20a%20potential%0Arelationship%20between%20the%20depth%20information%20of%20the%20scene%20and%20the%20hazy%20image.%0ABased%20on%20this%2C%20we%20propose%20a%20dual-task%20collaborative%20mutual%20promotion%20framework%0Ato%20achieve%20the%20dehazing%20of%20a%20single%20image.%20This%20framework%20integrates%20depth%0Aestimation%20and%20dehazing%20by%20a%20dual-task%20interaction%20mechanism%20and%20achieves%0Amutual%20enhancement%20of%20their%20performance.%20To%20realize%20the%20joint%20optimization%20of%0Athe%20two%20tasks%2C%20an%20alternative%20implementation%20mechanism%20with%20the%20difference%0Aperception%20is%20developed.%20On%20the%20one%20hand%2C%20the%20difference%20perception%20between%20the%0Adepth%20maps%20of%20the%20dehazing%20result%20and%20the%20ideal%20image%20is%20proposed%20to%20promote%0Athe%20dehazing%20network%20to%20pay%20attention%20to%20the%20non-ideal%20areas%20of%20the%20dehazing.%0AOn%20the%20other%20hand%2C%20by%20improving%20the%20depth%20estimation%20performance%20in%20the%0Adifficult-to-recover%20areas%20of%20the%20hazy%20image%2C%20the%20dehazing%20network%20can%0Aexplicitly%20use%20the%20depth%20information%20of%20the%20hazy%20image%20to%20assist%20the%20clear%0Aimage%20recovery.%20To%20promote%20the%20depth%20estimation%2C%20we%20propose%20to%20use%20the%0Adifference%20between%20the%20dehazed%20image%20and%20the%20ground%20truth%20to%20guide%20the%20depth%0Aestimation%20network%20to%20focus%20on%20the%20dehazed%20unideal%20areas.%20It%20allows%20dehazing%0Aand%20depth%20estimation%20to%20leverage%20their%20strengths%20in%20a%20mutually%20reinforcing%0Amanner.%20Experimental%20results%20show%20that%20the%20proposed%20method%20can%20achieve%20better%0Aperformance%20than%20that%20of%20the%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Information%2520Assisted%2520Collaborative%2520Mutual%2520Promotion%2520Network%2520for%250A%2520%2520Single%2520Image%2520Dehazing%26entry.906535625%3DYafei%2520Zhang%2520and%2520Shen%2520Zhou%2520and%2520Huafeng%2520Li%26entry.1292438233%3D%2520%2520Recovering%2520a%2520clear%2520image%2520from%2520a%2520single%2520hazy%2520image%2520is%2520an%2520open%2520inverse%2520problem.%250AAlthough%2520significant%2520research%2520progress%2520has%2520been%2520made%252C%2520most%2520existing%2520methods%250Aignore%2520the%2520effect%2520that%2520downstream%2520tasks%2520play%2520in%2520promoting%2520upstream%2520dehazing.%250AFrom%2520the%2520perspective%2520of%2520the%2520haze%2520generation%2520mechanism%252C%2520there%2520is%2520a%2520potential%250Arelationship%2520between%2520the%2520depth%2520information%2520of%2520the%2520scene%2520and%2520the%2520hazy%2520image.%250ABased%2520on%2520this%252C%2520we%2520propose%2520a%2520dual-task%2520collaborative%2520mutual%2520promotion%2520framework%250Ato%2520achieve%2520the%2520dehazing%2520of%2520a%2520single%2520image.%2520This%2520framework%2520integrates%2520depth%250Aestimation%2520and%2520dehazing%2520by%2520a%2520dual-task%2520interaction%2520mechanism%2520and%2520achieves%250Amutual%2520enhancement%2520of%2520their%2520performance.%2520To%2520realize%2520the%2520joint%2520optimization%2520of%250Athe%2520two%2520tasks%252C%2520an%2520alternative%2520implementation%2520mechanism%2520with%2520the%2520difference%250Aperception%2520is%2520developed.%2520On%2520the%2520one%2520hand%252C%2520the%2520difference%2520perception%2520between%2520the%250Adepth%2520maps%2520of%2520the%2520dehazing%2520result%2520and%2520the%2520ideal%2520image%2520is%2520proposed%2520to%2520promote%250Athe%2520dehazing%2520network%2520to%2520pay%2520attention%2520to%2520the%2520non-ideal%2520areas%2520of%2520the%2520dehazing.%250AOn%2520the%2520other%2520hand%252C%2520by%2520improving%2520the%2520depth%2520estimation%2520performance%2520in%2520the%250Adifficult-to-recover%2520areas%2520of%2520the%2520hazy%2520image%252C%2520the%2520dehazing%2520network%2520can%250Aexplicitly%2520use%2520the%2520depth%2520information%2520of%2520the%2520hazy%2520image%2520to%2520assist%2520the%2520clear%250Aimage%2520recovery.%2520To%2520promote%2520the%2520depth%2520estimation%252C%2520we%2520propose%2520to%2520use%2520the%250Adifference%2520between%2520the%2520dehazed%2520image%2520and%2520the%2520ground%2520truth%2520to%2520guide%2520the%2520depth%250Aestimation%2520network%2520to%2520focus%2520on%2520the%2520dehazed%2520unideal%2520areas.%2520It%2520allows%2520dehazing%250Aand%2520depth%2520estimation%2520to%2520leverage%2520their%2520strengths%2520in%2520a%2520mutually%2520reinforcing%250Amanner.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520method%2520can%2520achieve%2520better%250Aperformance%2520than%2520that%2520of%2520the%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Information%20Assisted%20Collaborative%20Mutual%20Promotion%20Network%20for%0A%20%20Single%20Image%20Dehazing&entry.906535625=Yafei%20Zhang%20and%20Shen%20Zhou%20and%20Huafeng%20Li&entry.1292438233=%20%20Recovering%20a%20clear%20image%20from%20a%20single%20hazy%20image%20is%20an%20open%20inverse%20problem.%0AAlthough%20significant%20research%20progress%20has%20been%20made%2C%20most%20existing%20methods%0Aignore%20the%20effect%20that%20downstream%20tasks%20play%20in%20promoting%20upstream%20dehazing.%0AFrom%20the%20perspective%20of%20the%20haze%20generation%20mechanism%2C%20there%20is%20a%20potential%0Arelationship%20between%20the%20depth%20information%20of%20the%20scene%20and%20the%20hazy%20image.%0ABased%20on%20this%2C%20we%20propose%20a%20dual-task%20collaborative%20mutual%20promotion%20framework%0Ato%20achieve%20the%20dehazing%20of%20a%20single%20image.%20This%20framework%20integrates%20depth%0Aestimation%20and%20dehazing%20by%20a%20dual-task%20interaction%20mechanism%20and%20achieves%0Amutual%20enhancement%20of%20their%20performance.%20To%20realize%20the%20joint%20optimization%20of%0Athe%20two%20tasks%2C%20an%20alternative%20implementation%20mechanism%20with%20the%20difference%0Aperception%20is%20developed.%20On%20the%20one%20hand%2C%20the%20difference%20perception%20between%20the%0Adepth%20maps%20of%20the%20dehazing%20result%20and%20the%20ideal%20image%20is%20proposed%20to%20promote%0Athe%20dehazing%20network%20to%20pay%20attention%20to%20the%20non-ideal%20areas%20of%20the%20dehazing.%0AOn%20the%20other%20hand%2C%20by%20improving%20the%20depth%20estimation%20performance%20in%20the%0Adifficult-to-recover%20areas%20of%20the%20hazy%20image%2C%20the%20dehazing%20network%20can%0Aexplicitly%20use%20the%20depth%20information%20of%20the%20hazy%20image%20to%20assist%20the%20clear%0Aimage%20recovery.%20To%20promote%20the%20depth%20estimation%2C%20we%20propose%20to%20use%20the%0Adifference%20between%20the%20dehazed%20image%20and%20the%20ground%20truth%20to%20guide%20the%20depth%0Aestimation%20network%20to%20focus%20on%20the%20dehazed%20unideal%20areas.%20It%20allows%20dehazing%0Aand%20depth%20estimation%20to%20leverage%20their%20strengths%20in%20a%20mutually%20reinforcing%0Amanner.%20Experimental%20results%20show%20that%20the%20proposed%20method%20can%20achieve%20better%0Aperformance%20than%20that%20of%20the%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01105v2&entry.124074799=Read"},
{"title": "From Easy to Hard: Learning Curricular Shape-aware Features for Robust\n  Panoptic Scene Graph Generation", "author": "Hanrong Shi and Lin Li and Jun Xiao and Yueting Zhuang and Long Chen", "abstract": "  Panoptic Scene Graph Generation (PSG) aims to generate a comprehensive\ngraph-structure representation based on panoptic segmentation masks. Despite\nremarkable progress in PSG, almost all existing methods neglect the importance\nof shape-aware features, which inherently focus on the contours and boundaries\nof objects. To bridge this gap, we propose a model-agnostic Curricular\nshApe-aware FEature (CAFE) learning strategy for PSG. Specifically, we\nincorporate shape-aware features (i.e., mask features and boundary features)\ninto PSG, moving beyond reliance solely on bbox features. Furthermore, drawing\ninspiration from human cognition, we propose to integrate shape-aware features\nin an easy-to-hard manner. To achieve this, we categorize the predicates into\nthree groups based on cognition learning difficulty and correspondingly divide\nthe training process into three stages. Each stage utilizes a specialized\nrelation classifier to distinguish specific groups of predicates. As the\nlearning difficulty of predicates increases, these classifiers are equipped\nwith features of ascending complexity. We also incorporate knowledge\ndistillation to retain knowledge acquired in earlier stages. Due to its\nmodel-agnostic nature, CAFE can be seamlessly incorporated into any PSG model.\nExtensive experiments and ablations on two PSG tasks under both robust and\nzero-shot PSG have attested to the superiority and robustness of our proposed\nCAFE, which outperforms existing state-of-the-art methods by a large margin.\n", "link": "http://arxiv.org/abs/2407.09191v1", "date": "2024-07-12", "relevancy": 2.1636, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5548}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5409}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Easy%20to%20Hard%3A%20Learning%20Curricular%20Shape-aware%20Features%20for%20Robust%0A%20%20Panoptic%20Scene%20Graph%20Generation&body=Title%3A%20From%20Easy%20to%20Hard%3A%20Learning%20Curricular%20Shape-aware%20Features%20for%20Robust%0A%20%20Panoptic%20Scene%20Graph%20Generation%0AAuthor%3A%20Hanrong%20Shi%20and%20Lin%20Li%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%20and%20Long%20Chen%0AAbstract%3A%20%20%20Panoptic%20Scene%20Graph%20Generation%20%28PSG%29%20aims%20to%20generate%20a%20comprehensive%0Agraph-structure%20representation%20based%20on%20panoptic%20segmentation%20masks.%20Despite%0Aremarkable%20progress%20in%20PSG%2C%20almost%20all%20existing%20methods%20neglect%20the%20importance%0Aof%20shape-aware%20features%2C%20which%20inherently%20focus%20on%20the%20contours%20and%20boundaries%0Aof%20objects.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20model-agnostic%20Curricular%0AshApe-aware%20FEature%20%28CAFE%29%20learning%20strategy%20for%20PSG.%20Specifically%2C%20we%0Aincorporate%20shape-aware%20features%20%28i.e.%2C%20mask%20features%20and%20boundary%20features%29%0Ainto%20PSG%2C%20moving%20beyond%20reliance%20solely%20on%20bbox%20features.%20Furthermore%2C%20drawing%0Ainspiration%20from%20human%20cognition%2C%20we%20propose%20to%20integrate%20shape-aware%20features%0Ain%20an%20easy-to-hard%20manner.%20To%20achieve%20this%2C%20we%20categorize%20the%20predicates%20into%0Athree%20groups%20based%20on%20cognition%20learning%20difficulty%20and%20correspondingly%20divide%0Athe%20training%20process%20into%20three%20stages.%20Each%20stage%20utilizes%20a%20specialized%0Arelation%20classifier%20to%20distinguish%20specific%20groups%20of%20predicates.%20As%20the%0Alearning%20difficulty%20of%20predicates%20increases%2C%20these%20classifiers%20are%20equipped%0Awith%20features%20of%20ascending%20complexity.%20We%20also%20incorporate%20knowledge%0Adistillation%20to%20retain%20knowledge%20acquired%20in%20earlier%20stages.%20Due%20to%20its%0Amodel-agnostic%20nature%2C%20CAFE%20can%20be%20seamlessly%20incorporated%20into%20any%20PSG%20model.%0AExtensive%20experiments%20and%20ablations%20on%20two%20PSG%20tasks%20under%20both%20robust%20and%0Azero-shot%20PSG%20have%20attested%20to%20the%20superiority%20and%20robustness%20of%20our%20proposed%0ACAFE%2C%20which%20outperforms%20existing%20state-of-the-art%20methods%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Easy%2520to%2520Hard%253A%2520Learning%2520Curricular%2520Shape-aware%2520Features%2520for%2520Robust%250A%2520%2520Panoptic%2520Scene%2520Graph%2520Generation%26entry.906535625%3DHanrong%2520Shi%2520and%2520Lin%2520Li%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Panoptic%2520Scene%2520Graph%2520Generation%2520%2528PSG%2529%2520aims%2520to%2520generate%2520a%2520comprehensive%250Agraph-structure%2520representation%2520based%2520on%2520panoptic%2520segmentation%2520masks.%2520Despite%250Aremarkable%2520progress%2520in%2520PSG%252C%2520almost%2520all%2520existing%2520methods%2520neglect%2520the%2520importance%250Aof%2520shape-aware%2520features%252C%2520which%2520inherently%2520focus%2520on%2520the%2520contours%2520and%2520boundaries%250Aof%2520objects.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520model-agnostic%2520Curricular%250AshApe-aware%2520FEature%2520%2528CAFE%2529%2520learning%2520strategy%2520for%2520PSG.%2520Specifically%252C%2520we%250Aincorporate%2520shape-aware%2520features%2520%2528i.e.%252C%2520mask%2520features%2520and%2520boundary%2520features%2529%250Ainto%2520PSG%252C%2520moving%2520beyond%2520reliance%2520solely%2520on%2520bbox%2520features.%2520Furthermore%252C%2520drawing%250Ainspiration%2520from%2520human%2520cognition%252C%2520we%2520propose%2520to%2520integrate%2520shape-aware%2520features%250Ain%2520an%2520easy-to-hard%2520manner.%2520To%2520achieve%2520this%252C%2520we%2520categorize%2520the%2520predicates%2520into%250Athree%2520groups%2520based%2520on%2520cognition%2520learning%2520difficulty%2520and%2520correspondingly%2520divide%250Athe%2520training%2520process%2520into%2520three%2520stages.%2520Each%2520stage%2520utilizes%2520a%2520specialized%250Arelation%2520classifier%2520to%2520distinguish%2520specific%2520groups%2520of%2520predicates.%2520As%2520the%250Alearning%2520difficulty%2520of%2520predicates%2520increases%252C%2520these%2520classifiers%2520are%2520equipped%250Awith%2520features%2520of%2520ascending%2520complexity.%2520We%2520also%2520incorporate%2520knowledge%250Adistillation%2520to%2520retain%2520knowledge%2520acquired%2520in%2520earlier%2520stages.%2520Due%2520to%2520its%250Amodel-agnostic%2520nature%252C%2520CAFE%2520can%2520be%2520seamlessly%2520incorporated%2520into%2520any%2520PSG%2520model.%250AExtensive%2520experiments%2520and%2520ablations%2520on%2520two%2520PSG%2520tasks%2520under%2520both%2520robust%2520and%250Azero-shot%2520PSG%2520have%2520attested%2520to%2520the%2520superiority%2520and%2520robustness%2520of%2520our%2520proposed%250ACAFE%252C%2520which%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520by%2520a%2520large%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Easy%20to%20Hard%3A%20Learning%20Curricular%20Shape-aware%20Features%20for%20Robust%0A%20%20Panoptic%20Scene%20Graph%20Generation&entry.906535625=Hanrong%20Shi%20and%20Lin%20Li%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%20and%20Long%20Chen&entry.1292438233=%20%20Panoptic%20Scene%20Graph%20Generation%20%28PSG%29%20aims%20to%20generate%20a%20comprehensive%0Agraph-structure%20representation%20based%20on%20panoptic%20segmentation%20masks.%20Despite%0Aremarkable%20progress%20in%20PSG%2C%20almost%20all%20existing%20methods%20neglect%20the%20importance%0Aof%20shape-aware%20features%2C%20which%20inherently%20focus%20on%20the%20contours%20and%20boundaries%0Aof%20objects.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20model-agnostic%20Curricular%0AshApe-aware%20FEature%20%28CAFE%29%20learning%20strategy%20for%20PSG.%20Specifically%2C%20we%0Aincorporate%20shape-aware%20features%20%28i.e.%2C%20mask%20features%20and%20boundary%20features%29%0Ainto%20PSG%2C%20moving%20beyond%20reliance%20solely%20on%20bbox%20features.%20Furthermore%2C%20drawing%0Ainspiration%20from%20human%20cognition%2C%20we%20propose%20to%20integrate%20shape-aware%20features%0Ain%20an%20easy-to-hard%20manner.%20To%20achieve%20this%2C%20we%20categorize%20the%20predicates%20into%0Athree%20groups%20based%20on%20cognition%20learning%20difficulty%20and%20correspondingly%20divide%0Athe%20training%20process%20into%20three%20stages.%20Each%20stage%20utilizes%20a%20specialized%0Arelation%20classifier%20to%20distinguish%20specific%20groups%20of%20predicates.%20As%20the%0Alearning%20difficulty%20of%20predicates%20increases%2C%20these%20classifiers%20are%20equipped%0Awith%20features%20of%20ascending%20complexity.%20We%20also%20incorporate%20knowledge%0Adistillation%20to%20retain%20knowledge%20acquired%20in%20earlier%20stages.%20Due%20to%20its%0Amodel-agnostic%20nature%2C%20CAFE%20can%20be%20seamlessly%20incorporated%20into%20any%20PSG%20model.%0AExtensive%20experiments%20and%20ablations%20on%20two%20PSG%20tasks%20under%20both%20robust%20and%0Azero-shot%20PSG%20have%20attested%20to%20the%20superiority%20and%20robustness%20of%20our%20proposed%0ACAFE%2C%20which%20outperforms%20existing%20state-of-the-art%20methods%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09191v1&entry.124074799=Read"},
{"title": "OmniSat: Self-Supervised Modality Fusion for Earth Observation", "author": "Guillaume Astruc and Nicolas Gonthier and Clement Mallet and Loic Landrieu", "abstract": "  The field of Earth Observations (EO) offers a wealth of data from diverse\nsensors, presenting a great opportunity for advancing self-supervised\nmultimodal learning. However, current multimodal EO datasets and models focus\non a single data type, either mono-date images or time series, which limits\ntheir expressivity. We introduce OmniSat, a novel architecture that exploits\nthe spatial alignment between multiple EO modalities to learn expressive\nmultimodal representations without labels. To demonstrate the advantages of\ncombining modalities of different natures, we augment two existing datasets\nwith new modalities. As demonstrated on three downstream tasks: forestry, land\ncover classification, and crop mapping. OmniSat can learn rich representations\nin an unsupervised manner, leading to improved performance in the semi- and\nfully-supervised settings, even when only one modality is available for\ninference. The code and dataset are available at\nhttps://github.com/gastruc/OmniSat.\n", "link": "http://arxiv.org/abs/2404.08351v2", "date": "2024-07-12", "relevancy": 2.1266, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5247}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniSat%3A%20Self-Supervised%20Modality%20Fusion%20for%20Earth%20Observation&body=Title%3A%20OmniSat%3A%20Self-Supervised%20Modality%20Fusion%20for%20Earth%20Observation%0AAuthor%3A%20Guillaume%20Astruc%20and%20Nicolas%20Gonthier%20and%20Clement%20Mallet%20and%20Loic%20Landrieu%0AAbstract%3A%20%20%20The%20field%20of%20Earth%20Observations%20%28EO%29%20offers%20a%20wealth%20of%20data%20from%20diverse%0Asensors%2C%20presenting%20a%20great%20opportunity%20for%20advancing%20self-supervised%0Amultimodal%20learning.%20However%2C%20current%20multimodal%20EO%20datasets%20and%20models%20focus%0Aon%20a%20single%20data%20type%2C%20either%20mono-date%20images%20or%20time%20series%2C%20which%20limits%0Atheir%20expressivity.%20We%20introduce%20OmniSat%2C%20a%20novel%20architecture%20that%20exploits%0Athe%20spatial%20alignment%20between%20multiple%20EO%20modalities%20to%20learn%20expressive%0Amultimodal%20representations%20without%20labels.%20To%20demonstrate%20the%20advantages%20of%0Acombining%20modalities%20of%20different%20natures%2C%20we%20augment%20two%20existing%20datasets%0Awith%20new%20modalities.%20As%20demonstrated%20on%20three%20downstream%20tasks%3A%20forestry%2C%20land%0Acover%20classification%2C%20and%20crop%20mapping.%20OmniSat%20can%20learn%20rich%20representations%0Ain%20an%20unsupervised%20manner%2C%20leading%20to%20improved%20performance%20in%20the%20semi-%20and%0Afully-supervised%20settings%2C%20even%20when%20only%20one%20modality%20is%20available%20for%0Ainference.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/gastruc/OmniSat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08351v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniSat%253A%2520Self-Supervised%2520Modality%2520Fusion%2520for%2520Earth%2520Observation%26entry.906535625%3DGuillaume%2520Astruc%2520and%2520Nicolas%2520Gonthier%2520and%2520Clement%2520Mallet%2520and%2520Loic%2520Landrieu%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520Earth%2520Observations%2520%2528EO%2529%2520offers%2520a%2520wealth%2520of%2520data%2520from%2520diverse%250Asensors%252C%2520presenting%2520a%2520great%2520opportunity%2520for%2520advancing%2520self-supervised%250Amultimodal%2520learning.%2520However%252C%2520current%2520multimodal%2520EO%2520datasets%2520and%2520models%2520focus%250Aon%2520a%2520single%2520data%2520type%252C%2520either%2520mono-date%2520images%2520or%2520time%2520series%252C%2520which%2520limits%250Atheir%2520expressivity.%2520We%2520introduce%2520OmniSat%252C%2520a%2520novel%2520architecture%2520that%2520exploits%250Athe%2520spatial%2520alignment%2520between%2520multiple%2520EO%2520modalities%2520to%2520learn%2520expressive%250Amultimodal%2520representations%2520without%2520labels.%2520To%2520demonstrate%2520the%2520advantages%2520of%250Acombining%2520modalities%2520of%2520different%2520natures%252C%2520we%2520augment%2520two%2520existing%2520datasets%250Awith%2520new%2520modalities.%2520As%2520demonstrated%2520on%2520three%2520downstream%2520tasks%253A%2520forestry%252C%2520land%250Acover%2520classification%252C%2520and%2520crop%2520mapping.%2520OmniSat%2520can%2520learn%2520rich%2520representations%250Ain%2520an%2520unsupervised%2520manner%252C%2520leading%2520to%2520improved%2520performance%2520in%2520the%2520semi-%2520and%250Afully-supervised%2520settings%252C%2520even%2520when%2520only%2520one%2520modality%2520is%2520available%2520for%250Ainference.%2520The%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/gastruc/OmniSat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08351v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniSat%3A%20Self-Supervised%20Modality%20Fusion%20for%20Earth%20Observation&entry.906535625=Guillaume%20Astruc%20and%20Nicolas%20Gonthier%20and%20Clement%20Mallet%20and%20Loic%20Landrieu&entry.1292438233=%20%20The%20field%20of%20Earth%20Observations%20%28EO%29%20offers%20a%20wealth%20of%20data%20from%20diverse%0Asensors%2C%20presenting%20a%20great%20opportunity%20for%20advancing%20self-supervised%0Amultimodal%20learning.%20However%2C%20current%20multimodal%20EO%20datasets%20and%20models%20focus%0Aon%20a%20single%20data%20type%2C%20either%20mono-date%20images%20or%20time%20series%2C%20which%20limits%0Atheir%20expressivity.%20We%20introduce%20OmniSat%2C%20a%20novel%20architecture%20that%20exploits%0Athe%20spatial%20alignment%20between%20multiple%20EO%20modalities%20to%20learn%20expressive%0Amultimodal%20representations%20without%20labels.%20To%20demonstrate%20the%20advantages%20of%0Acombining%20modalities%20of%20different%20natures%2C%20we%20augment%20two%20existing%20datasets%0Awith%20new%20modalities.%20As%20demonstrated%20on%20three%20downstream%20tasks%3A%20forestry%2C%20land%0Acover%20classification%2C%20and%20crop%20mapping.%20OmniSat%20can%20learn%20rich%20representations%0Ain%20an%20unsupervised%20manner%2C%20leading%20to%20improved%20performance%20in%20the%20semi-%20and%0Afully-supervised%20settings%2C%20even%20when%20only%20one%20modality%20is%20available%20for%0Ainference.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/gastruc/OmniSat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08351v2&entry.124074799=Read"},
{"title": "Adaptive Deep Iris Feature Extractor at Arbitrary Resolutions", "author": "Yuho Shoji and Yuka Ogino and Takahiro Toizumi and Atsushi Ito", "abstract": "  This paper proposes a deep feature extractor for iris recognition at\narbitrary resolutions. Resolution degradation reduces the recognition\nperformance of deep learning models trained by high-resolution images. Using\nvarious-resolution images for training can improve the model's robustness while\nsacrificing recognition performance for high-resolution images. To achieve\nhigher recognition performance at various resolutions, we propose a method of\nresolution-adaptive feature extraction with automatically switching networks.\nOur framework includes resolution expert modules specialized for different\nresolution degradations, including down-sampling and out-of-focus blurring. The\nframework automatically switches them depending on the degradation condition of\nan input image. Lower-resolution experts are trained by knowledge-distillation\nfrom the high-resolution expert in such a manner that both experts can extract\ncommon identity features. We applied our framework to three conventional neural\nnetwork models. The experimental results show that our method enhances the\nrecognition performance at low-resolution in the conventional methods and also\nmaintains their performance at high-resolution.\n", "link": "http://arxiv.org/abs/2407.08341v2", "date": "2024-07-12", "relevancy": 2.1192, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5393}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5363}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Deep%20Iris%20Feature%20Extractor%20at%20Arbitrary%20Resolutions&body=Title%3A%20Adaptive%20Deep%20Iris%20Feature%20Extractor%20at%20Arbitrary%20Resolutions%0AAuthor%3A%20Yuho%20Shoji%20and%20Yuka%20Ogino%20and%20Takahiro%20Toizumi%20and%20Atsushi%20Ito%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20deep%20feature%20extractor%20for%20iris%20recognition%20at%0Aarbitrary%20resolutions.%20Resolution%20degradation%20reduces%20the%20recognition%0Aperformance%20of%20deep%20learning%20models%20trained%20by%20high-resolution%20images.%20Using%0Avarious-resolution%20images%20for%20training%20can%20improve%20the%20model%27s%20robustness%20while%0Asacrificing%20recognition%20performance%20for%20high-resolution%20images.%20To%20achieve%0Ahigher%20recognition%20performance%20at%20various%20resolutions%2C%20we%20propose%20a%20method%20of%0Aresolution-adaptive%20feature%20extraction%20with%20automatically%20switching%20networks.%0AOur%20framework%20includes%20resolution%20expert%20modules%20specialized%20for%20different%0Aresolution%20degradations%2C%20including%20down-sampling%20and%20out-of-focus%20blurring.%20The%0Aframework%20automatically%20switches%20them%20depending%20on%20the%20degradation%20condition%20of%0Aan%20input%20image.%20Lower-resolution%20experts%20are%20trained%20by%20knowledge-distillation%0Afrom%20the%20high-resolution%20expert%20in%20such%20a%20manner%20that%20both%20experts%20can%20extract%0Acommon%20identity%20features.%20We%20applied%20our%20framework%20to%20three%20conventional%20neural%0Anetwork%20models.%20The%20experimental%20results%20show%20that%20our%20method%20enhances%20the%0Arecognition%20performance%20at%20low-resolution%20in%20the%20conventional%20methods%20and%20also%0Amaintains%20their%20performance%20at%20high-resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08341v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Deep%2520Iris%2520Feature%2520Extractor%2520at%2520Arbitrary%2520Resolutions%26entry.906535625%3DYuho%2520Shoji%2520and%2520Yuka%2520Ogino%2520and%2520Takahiro%2520Toizumi%2520and%2520Atsushi%2520Ito%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520deep%2520feature%2520extractor%2520for%2520iris%2520recognition%2520at%250Aarbitrary%2520resolutions.%2520Resolution%2520degradation%2520reduces%2520the%2520recognition%250Aperformance%2520of%2520deep%2520learning%2520models%2520trained%2520by%2520high-resolution%2520images.%2520Using%250Avarious-resolution%2520images%2520for%2520training%2520can%2520improve%2520the%2520model%2527s%2520robustness%2520while%250Asacrificing%2520recognition%2520performance%2520for%2520high-resolution%2520images.%2520To%2520achieve%250Ahigher%2520recognition%2520performance%2520at%2520various%2520resolutions%252C%2520we%2520propose%2520a%2520method%2520of%250Aresolution-adaptive%2520feature%2520extraction%2520with%2520automatically%2520switching%2520networks.%250AOur%2520framework%2520includes%2520resolution%2520expert%2520modules%2520specialized%2520for%2520different%250Aresolution%2520degradations%252C%2520including%2520down-sampling%2520and%2520out-of-focus%2520blurring.%2520The%250Aframework%2520automatically%2520switches%2520them%2520depending%2520on%2520the%2520degradation%2520condition%2520of%250Aan%2520input%2520image.%2520Lower-resolution%2520experts%2520are%2520trained%2520by%2520knowledge-distillation%250Afrom%2520the%2520high-resolution%2520expert%2520in%2520such%2520a%2520manner%2520that%2520both%2520experts%2520can%2520extract%250Acommon%2520identity%2520features.%2520We%2520applied%2520our%2520framework%2520to%2520three%2520conventional%2520neural%250Anetwork%2520models.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520method%2520enhances%2520the%250Arecognition%2520performance%2520at%2520low-resolution%2520in%2520the%2520conventional%2520methods%2520and%2520also%250Amaintains%2520their%2520performance%2520at%2520high-resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08341v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Deep%20Iris%20Feature%20Extractor%20at%20Arbitrary%20Resolutions&entry.906535625=Yuho%20Shoji%20and%20Yuka%20Ogino%20and%20Takahiro%20Toizumi%20and%20Atsushi%20Ito&entry.1292438233=%20%20This%20paper%20proposes%20a%20deep%20feature%20extractor%20for%20iris%20recognition%20at%0Aarbitrary%20resolutions.%20Resolution%20degradation%20reduces%20the%20recognition%0Aperformance%20of%20deep%20learning%20models%20trained%20by%20high-resolution%20images.%20Using%0Avarious-resolution%20images%20for%20training%20can%20improve%20the%20model%27s%20robustness%20while%0Asacrificing%20recognition%20performance%20for%20high-resolution%20images.%20To%20achieve%0Ahigher%20recognition%20performance%20at%20various%20resolutions%2C%20we%20propose%20a%20method%20of%0Aresolution-adaptive%20feature%20extraction%20with%20automatically%20switching%20networks.%0AOur%20framework%20includes%20resolution%20expert%20modules%20specialized%20for%20different%0Aresolution%20degradations%2C%20including%20down-sampling%20and%20out-of-focus%20blurring.%20The%0Aframework%20automatically%20switches%20them%20depending%20on%20the%20degradation%20condition%20of%0Aan%20input%20image.%20Lower-resolution%20experts%20are%20trained%20by%20knowledge-distillation%0Afrom%20the%20high-resolution%20expert%20in%20such%20a%20manner%20that%20both%20experts%20can%20extract%0Acommon%20identity%20features.%20We%20applied%20our%20framework%20to%20three%20conventional%20neural%0Anetwork%20models.%20The%20experimental%20results%20show%20that%20our%20method%20enhances%20the%0Arecognition%20performance%20at%20low-resolution%20in%20the%20conventional%20methods%20and%20also%0Amaintains%20their%20performance%20at%20high-resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08341v2&entry.124074799=Read"},
{"title": "Region Attention Transformer for Medical Image Restoration", "author": "Zhiwen Yang and Haowei Chen and Ziniu Qian and Yang Zhou and Hui Zhang and Dan Zhao and Bingzheng Wei and Yan Xu", "abstract": "  Transformer-based methods have demonstrated impressive results in medical\nimage restoration, attributed to the multi-head self-attention (MSA) mechanism\nin the spatial dimension. However, the majority of existing Transformers\nconduct attention within fixed and coarsely partitioned regions (\\text{e.g.}\nthe entire image or fixed patches), resulting in interference from irrelevant\nregions and fragmentation of continuous image content. To overcome these\nchallenges, we introduce a novel Region Attention Transformer (RAT) that\nutilizes a region-based multi-head self-attention mechanism (R-MSA). The R-MSA\ndynamically partitions the input image into non-overlapping semantic regions\nusing the robust Segment Anything Model (SAM) and then performs self-attention\nwithin these regions. This region partitioning is more flexible and\ninterpretable, ensuring that only pixels from similar semantic regions\ncomplement each other, thereby eliminating interference from irrelevant\nregions. Moreover, we introduce a focal region loss to guide our model to\nadaptively focus on recovering high-difficulty regions. Extensive experiments\ndemonstrate the effectiveness of RAT in various medical image restoration\ntasks, including PET image synthesis, CT image denoising, and pathological\nimage super-resolution. Code is available at\n\\href{https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git}{https://github.com/RAT}.\n", "link": "http://arxiv.org/abs/2407.09268v1", "date": "2024-07-12", "relevancy": 2.1156, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5679}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Region%20Attention%20Transformer%20for%20Medical%20Image%20Restoration&body=Title%3A%20Region%20Attention%20Transformer%20for%20Medical%20Image%20Restoration%0AAuthor%3A%20Zhiwen%20Yang%20and%20Haowei%20Chen%20and%20Ziniu%20Qian%20and%20Yang%20Zhou%20and%20Hui%20Zhang%20and%20Dan%20Zhao%20and%20Bingzheng%20Wei%20and%20Yan%20Xu%0AAbstract%3A%20%20%20Transformer-based%20methods%20have%20demonstrated%20impressive%20results%20in%20medical%0Aimage%20restoration%2C%20attributed%20to%20the%20multi-head%20self-attention%20%28MSA%29%20mechanism%0Ain%20the%20spatial%20dimension.%20However%2C%20the%20majority%20of%20existing%20Transformers%0Aconduct%20attention%20within%20fixed%20and%20coarsely%20partitioned%20regions%20%28%5Ctext%7Be.g.%7D%0Athe%20entire%20image%20or%20fixed%20patches%29%2C%20resulting%20in%20interference%20from%20irrelevant%0Aregions%20and%20fragmentation%20of%20continuous%20image%20content.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20Region%20Attention%20Transformer%20%28RAT%29%20that%0Autilizes%20a%20region-based%20multi-head%20self-attention%20mechanism%20%28R-MSA%29.%20The%20R-MSA%0Adynamically%20partitions%20the%20input%20image%20into%20non-overlapping%20semantic%20regions%0Ausing%20the%20robust%20Segment%20Anything%20Model%20%28SAM%29%20and%20then%20performs%20self-attention%0Awithin%20these%20regions.%20This%20region%20partitioning%20is%20more%20flexible%20and%0Ainterpretable%2C%20ensuring%20that%20only%20pixels%20from%20similar%20semantic%20regions%0Acomplement%20each%20other%2C%20thereby%20eliminating%20interference%20from%20irrelevant%0Aregions.%20Moreover%2C%20we%20introduce%20a%20focal%20region%20loss%20to%20guide%20our%20model%20to%0Aadaptively%20focus%20on%20recovering%20high-difficulty%20regions.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20RAT%20in%20various%20medical%20image%20restoration%0Atasks%2C%20including%20PET%20image%20synthesis%2C%20CT%20image%20denoising%2C%20and%20pathological%0Aimage%20super-resolution.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git%7D%7Bhttps%3A//github.com/RAT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegion%2520Attention%2520Transformer%2520for%2520Medical%2520Image%2520Restoration%26entry.906535625%3DZhiwen%2520Yang%2520and%2520Haowei%2520Chen%2520and%2520Ziniu%2520Qian%2520and%2520Yang%2520Zhou%2520and%2520Hui%2520Zhang%2520and%2520Dan%2520Zhao%2520and%2520Bingzheng%2520Wei%2520and%2520Yan%2520Xu%26entry.1292438233%3D%2520%2520Transformer-based%2520methods%2520have%2520demonstrated%2520impressive%2520results%2520in%2520medical%250Aimage%2520restoration%252C%2520attributed%2520to%2520the%2520multi-head%2520self-attention%2520%2528MSA%2529%2520mechanism%250Ain%2520the%2520spatial%2520dimension.%2520However%252C%2520the%2520majority%2520of%2520existing%2520Transformers%250Aconduct%2520attention%2520within%2520fixed%2520and%2520coarsely%2520partitioned%2520regions%2520%2528%255Ctext%257Be.g.%257D%250Athe%2520entire%2520image%2520or%2520fixed%2520patches%2529%252C%2520resulting%2520in%2520interference%2520from%2520irrelevant%250Aregions%2520and%2520fragmentation%2520of%2520continuous%2520image%2520content.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520introduce%2520a%2520novel%2520Region%2520Attention%2520Transformer%2520%2528RAT%2529%2520that%250Autilizes%2520a%2520region-based%2520multi-head%2520self-attention%2520mechanism%2520%2528R-MSA%2529.%2520The%2520R-MSA%250Adynamically%2520partitions%2520the%2520input%2520image%2520into%2520non-overlapping%2520semantic%2520regions%250Ausing%2520the%2520robust%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520and%2520then%2520performs%2520self-attention%250Awithin%2520these%2520regions.%2520This%2520region%2520partitioning%2520is%2520more%2520flexible%2520and%250Ainterpretable%252C%2520ensuring%2520that%2520only%2520pixels%2520from%2520similar%2520semantic%2520regions%250Acomplement%2520each%2520other%252C%2520thereby%2520eliminating%2520interference%2520from%2520irrelevant%250Aregions.%2520Moreover%252C%2520we%2520introduce%2520a%2520focal%2520region%2520loss%2520to%2520guide%2520our%2520model%2520to%250Aadaptively%2520focus%2520on%2520recovering%2520high-difficulty%2520regions.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520RAT%2520in%2520various%2520medical%2520image%2520restoration%250Atasks%252C%2520including%2520PET%2520image%2520synthesis%252C%2520CT%2520image%2520denoising%252C%2520and%2520pathological%250Aimage%2520super-resolution.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git%257D%257Bhttps%253A//github.com/RAT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Region%20Attention%20Transformer%20for%20Medical%20Image%20Restoration&entry.906535625=Zhiwen%20Yang%20and%20Haowei%20Chen%20and%20Ziniu%20Qian%20and%20Yang%20Zhou%20and%20Hui%20Zhang%20and%20Dan%20Zhao%20and%20Bingzheng%20Wei%20and%20Yan%20Xu&entry.1292438233=%20%20Transformer-based%20methods%20have%20demonstrated%20impressive%20results%20in%20medical%0Aimage%20restoration%2C%20attributed%20to%20the%20multi-head%20self-attention%20%28MSA%29%20mechanism%0Ain%20the%20spatial%20dimension.%20However%2C%20the%20majority%20of%20existing%20Transformers%0Aconduct%20attention%20within%20fixed%20and%20coarsely%20partitioned%20regions%20%28%5Ctext%7Be.g.%7D%0Athe%20entire%20image%20or%20fixed%20patches%29%2C%20resulting%20in%20interference%20from%20irrelevant%0Aregions%20and%20fragmentation%20of%20continuous%20image%20content.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20Region%20Attention%20Transformer%20%28RAT%29%20that%0Autilizes%20a%20region-based%20multi-head%20self-attention%20mechanism%20%28R-MSA%29.%20The%20R-MSA%0Adynamically%20partitions%20the%20input%20image%20into%20non-overlapping%20semantic%20regions%0Ausing%20the%20robust%20Segment%20Anything%20Model%20%28SAM%29%20and%20then%20performs%20self-attention%0Awithin%20these%20regions.%20This%20region%20partitioning%20is%20more%20flexible%20and%0Ainterpretable%2C%20ensuring%20that%20only%20pixels%20from%20similar%20semantic%20regions%0Acomplement%20each%20other%2C%20thereby%20eliminating%20interference%20from%20irrelevant%0Aregions.%20Moreover%2C%20we%20introduce%20a%20focal%20region%20loss%20to%20guide%20our%20model%20to%0Aadaptively%20focus%20on%20recovering%20high-difficulty%20regions.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20RAT%20in%20various%20medical%20image%20restoration%0Atasks%2C%20including%20PET%20image%20synthesis%2C%20CT%20image%20denoising%2C%20and%20pathological%0Aimage%20super-resolution.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git%7D%7Bhttps%3A//github.com/RAT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09268v1&entry.124074799=Read"},
{"title": "GazeRace: Revolutionizing Remote Piloting with Eye-Gaze Control", "author": "Issatay Tokmurziyev and Valerii Serpiva and Alexey Fedoseev and Miguel Altamirano Cabrera and Dzmitry Tsetserukou", "abstract": "  This paper introduces the GazeRace method for drone navigation, employing a\ncomputer vision interface facilitated by eye-tracking technology. This\ninterface is designed to be compatible with a single camera and uses a\nconvolutional neural network to convert eye movements into control commands for\nthe drone. Experimental validation demonstrates that users equipped with the\neye-tracking interface achieve comparable performance to a traditional remote\ncontrol interface when completing a drone racing task.\n  Ten participants completed flight tests in which they navigated a drone\nthrough a racing track in a Gazebo simulation environment. Users reduced drone\ntrajectory length by 18% (73.44 m vs. 89.29 m) using the eye-tracking interface\nto navigate racing gates effectively. The time taken to complete the route\nusing the eye-tracking method (average of 70.01 seconds) was only 3.5% slower\nthan using the remote control method (also average of 70.01 seconds),\nindicating the good efficiency of the interface. It is also worth mentioning\nthat four of the participants completed the race with an average time that was\n25.9% faster than the other participants. In addition, users evaluated highly\nthe performance (M = 34.0, SD = 14.2) and low frustration (M = 30.5, SD = 9.2)\nwith the eye-tracking interface compared to performance (M = 63.0, SD = 10.1)\nand frustration (M = 49.0, SD = 11.7) with the baseline remote controller. The\nhedonic quality (M = 1.65, SD = 0.45) was also evaluated high by the users in\nthe UEQ questionnaire.\n", "link": "http://arxiv.org/abs/2407.09459v1", "date": "2024-07-12", "relevancy": 2.11, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.592}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4818}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GazeRace%3A%20Revolutionizing%20Remote%20Piloting%20with%20Eye-Gaze%20Control&body=Title%3A%20GazeRace%3A%20Revolutionizing%20Remote%20Piloting%20with%20Eye-Gaze%20Control%0AAuthor%3A%20Issatay%20Tokmurziyev%20and%20Valerii%20Serpiva%20and%20Alexey%20Fedoseev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20GazeRace%20method%20for%20drone%20navigation%2C%20employing%20a%0Acomputer%20vision%20interface%20facilitated%20by%20eye-tracking%20technology.%20This%0Ainterface%20is%20designed%20to%20be%20compatible%20with%20a%20single%20camera%20and%20uses%20a%0Aconvolutional%20neural%20network%20to%20convert%20eye%20movements%20into%20control%20commands%20for%0Athe%20drone.%20Experimental%20validation%20demonstrates%20that%20users%20equipped%20with%20the%0Aeye-tracking%20interface%20achieve%20comparable%20performance%20to%20a%20traditional%20remote%0Acontrol%20interface%20when%20completing%20a%20drone%20racing%20task.%0A%20%20Ten%20participants%20completed%20flight%20tests%20in%20which%20they%20navigated%20a%20drone%0Athrough%20a%20racing%20track%20in%20a%20Gazebo%20simulation%20environment.%20Users%20reduced%20drone%0Atrajectory%20length%20by%2018%25%20%2873.44%20m%20vs.%2089.29%20m%29%20using%20the%20eye-tracking%20interface%0Ato%20navigate%20racing%20gates%20effectively.%20The%20time%20taken%20to%20complete%20the%20route%0Ausing%20the%20eye-tracking%20method%20%28average%20of%2070.01%20seconds%29%20was%20only%203.5%25%20slower%0Athan%20using%20the%20remote%20control%20method%20%28also%20average%20of%2070.01%20seconds%29%2C%0Aindicating%20the%20good%20efficiency%20of%20the%20interface.%20It%20is%20also%20worth%20mentioning%0Athat%20four%20of%20the%20participants%20completed%20the%20race%20with%20an%20average%20time%20that%20was%0A25.9%25%20faster%20than%20the%20other%20participants.%20In%20addition%2C%20users%20evaluated%20highly%0Athe%20performance%20%28M%20%3D%2034.0%2C%20SD%20%3D%2014.2%29%20and%20low%20frustration%20%28M%20%3D%2030.5%2C%20SD%20%3D%209.2%29%0Awith%20the%20eye-tracking%20interface%20compared%20to%20performance%20%28M%20%3D%2063.0%2C%20SD%20%3D%2010.1%29%0Aand%20frustration%20%28M%20%3D%2049.0%2C%20SD%20%3D%2011.7%29%20with%20the%20baseline%20remote%20controller.%20The%0Ahedonic%20quality%20%28M%20%3D%201.65%2C%20SD%20%3D%200.45%29%20was%20also%20evaluated%20high%20by%20the%20users%20in%0Athe%20UEQ%20questionnaire.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGazeRace%253A%2520Revolutionizing%2520Remote%2520Piloting%2520with%2520Eye-Gaze%2520Control%26entry.906535625%3DIssatay%2520Tokmurziyev%2520and%2520Valerii%2520Serpiva%2520and%2520Alexey%2520Fedoseev%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520GazeRace%2520method%2520for%2520drone%2520navigation%252C%2520employing%2520a%250Acomputer%2520vision%2520interface%2520facilitated%2520by%2520eye-tracking%2520technology.%2520This%250Ainterface%2520is%2520designed%2520to%2520be%2520compatible%2520with%2520a%2520single%2520camera%2520and%2520uses%2520a%250Aconvolutional%2520neural%2520network%2520to%2520convert%2520eye%2520movements%2520into%2520control%2520commands%2520for%250Athe%2520drone.%2520Experimental%2520validation%2520demonstrates%2520that%2520users%2520equipped%2520with%2520the%250Aeye-tracking%2520interface%2520achieve%2520comparable%2520performance%2520to%2520a%2520traditional%2520remote%250Acontrol%2520interface%2520when%2520completing%2520a%2520drone%2520racing%2520task.%250A%2520%2520Ten%2520participants%2520completed%2520flight%2520tests%2520in%2520which%2520they%2520navigated%2520a%2520drone%250Athrough%2520a%2520racing%2520track%2520in%2520a%2520Gazebo%2520simulation%2520environment.%2520Users%2520reduced%2520drone%250Atrajectory%2520length%2520by%252018%2525%2520%252873.44%2520m%2520vs.%252089.29%2520m%2529%2520using%2520the%2520eye-tracking%2520interface%250Ato%2520navigate%2520racing%2520gates%2520effectively.%2520The%2520time%2520taken%2520to%2520complete%2520the%2520route%250Ausing%2520the%2520eye-tracking%2520method%2520%2528average%2520of%252070.01%2520seconds%2529%2520was%2520only%25203.5%2525%2520slower%250Athan%2520using%2520the%2520remote%2520control%2520method%2520%2528also%2520average%2520of%252070.01%2520seconds%2529%252C%250Aindicating%2520the%2520good%2520efficiency%2520of%2520the%2520interface.%2520It%2520is%2520also%2520worth%2520mentioning%250Athat%2520four%2520of%2520the%2520participants%2520completed%2520the%2520race%2520with%2520an%2520average%2520time%2520that%2520was%250A25.9%2525%2520faster%2520than%2520the%2520other%2520participants.%2520In%2520addition%252C%2520users%2520evaluated%2520highly%250Athe%2520performance%2520%2528M%2520%253D%252034.0%252C%2520SD%2520%253D%252014.2%2529%2520and%2520low%2520frustration%2520%2528M%2520%253D%252030.5%252C%2520SD%2520%253D%25209.2%2529%250Awith%2520the%2520eye-tracking%2520interface%2520compared%2520to%2520performance%2520%2528M%2520%253D%252063.0%252C%2520SD%2520%253D%252010.1%2529%250Aand%2520frustration%2520%2528M%2520%253D%252049.0%252C%2520SD%2520%253D%252011.7%2529%2520with%2520the%2520baseline%2520remote%2520controller.%2520The%250Ahedonic%2520quality%2520%2528M%2520%253D%25201.65%252C%2520SD%2520%253D%25200.45%2529%2520was%2520also%2520evaluated%2520high%2520by%2520the%2520users%2520in%250Athe%2520UEQ%2520questionnaire.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GazeRace%3A%20Revolutionizing%20Remote%20Piloting%20with%20Eye-Gaze%20Control&entry.906535625=Issatay%20Tokmurziyev%20and%20Valerii%20Serpiva%20and%20Alexey%20Fedoseev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20This%20paper%20introduces%20the%20GazeRace%20method%20for%20drone%20navigation%2C%20employing%20a%0Acomputer%20vision%20interface%20facilitated%20by%20eye-tracking%20technology.%20This%0Ainterface%20is%20designed%20to%20be%20compatible%20with%20a%20single%20camera%20and%20uses%20a%0Aconvolutional%20neural%20network%20to%20convert%20eye%20movements%20into%20control%20commands%20for%0Athe%20drone.%20Experimental%20validation%20demonstrates%20that%20users%20equipped%20with%20the%0Aeye-tracking%20interface%20achieve%20comparable%20performance%20to%20a%20traditional%20remote%0Acontrol%20interface%20when%20completing%20a%20drone%20racing%20task.%0A%20%20Ten%20participants%20completed%20flight%20tests%20in%20which%20they%20navigated%20a%20drone%0Athrough%20a%20racing%20track%20in%20a%20Gazebo%20simulation%20environment.%20Users%20reduced%20drone%0Atrajectory%20length%20by%2018%25%20%2873.44%20m%20vs.%2089.29%20m%29%20using%20the%20eye-tracking%20interface%0Ato%20navigate%20racing%20gates%20effectively.%20The%20time%20taken%20to%20complete%20the%20route%0Ausing%20the%20eye-tracking%20method%20%28average%20of%2070.01%20seconds%29%20was%20only%203.5%25%20slower%0Athan%20using%20the%20remote%20control%20method%20%28also%20average%20of%2070.01%20seconds%29%2C%0Aindicating%20the%20good%20efficiency%20of%20the%20interface.%20It%20is%20also%20worth%20mentioning%0Athat%20four%20of%20the%20participants%20completed%20the%20race%20with%20an%20average%20time%20that%20was%0A25.9%25%20faster%20than%20the%20other%20participants.%20In%20addition%2C%20users%20evaluated%20highly%0Athe%20performance%20%28M%20%3D%2034.0%2C%20SD%20%3D%2014.2%29%20and%20low%20frustration%20%28M%20%3D%2030.5%2C%20SD%20%3D%209.2%29%0Awith%20the%20eye-tracking%20interface%20compared%20to%20performance%20%28M%20%3D%2063.0%2C%20SD%20%3D%2010.1%29%0Aand%20frustration%20%28M%20%3D%2049.0%2C%20SD%20%3D%2011.7%29%20with%20the%20baseline%20remote%20controller.%20The%0Ahedonic%20quality%20%28M%20%3D%201.65%2C%20SD%20%3D%200.45%29%20was%20also%20evaluated%20high%20by%20the%20users%20in%0Athe%20UEQ%20questionnaire.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09459v1&entry.124074799=Read"},
{"title": "The $\u03bc\\mathcal{G}$ Language for Programming Graph Neural Networks", "author": "Matteo Belenchia and Flavio Corradini and Michela Quadrini and Michele Loreti", "abstract": "  Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.\n", "link": "http://arxiv.org/abs/2407.09441v1", "date": "2024-07-12", "relevancy": 2.1049, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4373}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4285}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks&body=Title%3A%20The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks%0AAuthor%3A%20Matteo%20Belenchia%20and%20Flavio%20Corradini%20and%20Michela%20Quadrini%20and%20Michele%20Loreti%0AAbstract%3A%20%20%20Graph%20neural%20networks%20form%20a%20class%20of%20deep%20learning%20architectures%0Aspecifically%20designed%20to%20work%20with%20graph-structured%20data.%20As%20such%2C%20they%20share%0Athe%20inherent%20limitations%20and%20problems%20of%20deep%20learning%2C%20especially%20regarding%0Athe%20issues%20of%20explainability%20and%20trustworthiness.%20We%20propose%20%24%5Cmu%5Cmathcal%7BG%7D%24%2C%0Aan%20original%20domain-specific%20language%20for%20the%20specification%20of%20graph%20neural%0Anetworks%20that%20aims%20to%20overcome%20these%20issues.%20The%20language%27s%20syntax%20is%0Aintroduced%2C%20and%20its%20meaning%20is%20rigorously%20defined%20by%20a%20denotational%20semantics.%0AAn%20equivalent%20characterization%20in%20the%20form%20of%20an%20operational%20semantics%20is%20also%0Aprovided%20and%2C%20together%20with%20a%20type%20system%2C%20is%20used%20to%20prove%20the%20type%20soundness%0Aof%20%24%5Cmu%5Cmathcal%7BG%7D%24.%20We%20show%20how%20%24%5Cmu%5Cmathcal%7BG%7D%24%20programs%20can%20be%20represented%0Ain%20a%20more%20user-friendly%20graphical%20visualization%2C%20and%20provide%20examples%20of%20its%0Agenerality%20by%20showing%20how%20it%20can%20be%20used%20to%20define%20some%20of%20the%20most%20popular%0Agraph%20neural%20network%20models%2C%20or%20to%20develop%20any%20custom%20graph%20processing%0Aapplication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520%2524%25CE%25BC%255Cmathcal%257BG%257D%2524%2520Language%2520for%2520Programming%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMatteo%2520Belenchia%2520and%2520Flavio%2520Corradini%2520and%2520Michela%2520Quadrini%2520and%2520Michele%2520Loreti%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520form%2520a%2520class%2520of%2520deep%2520learning%2520architectures%250Aspecifically%2520designed%2520to%2520work%2520with%2520graph-structured%2520data.%2520As%2520such%252C%2520they%2520share%250Athe%2520inherent%2520limitations%2520and%2520problems%2520of%2520deep%2520learning%252C%2520especially%2520regarding%250Athe%2520issues%2520of%2520explainability%2520and%2520trustworthiness.%2520We%2520propose%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524%252C%250Aan%2520original%2520domain-specific%2520language%2520for%2520the%2520specification%2520of%2520graph%2520neural%250Anetworks%2520that%2520aims%2520to%2520overcome%2520these%2520issues.%2520The%2520language%2527s%2520syntax%2520is%250Aintroduced%252C%2520and%2520its%2520meaning%2520is%2520rigorously%2520defined%2520by%2520a%2520denotational%2520semantics.%250AAn%2520equivalent%2520characterization%2520in%2520the%2520form%2520of%2520an%2520operational%2520semantics%2520is%2520also%250Aprovided%2520and%252C%2520together%2520with%2520a%2520type%2520system%252C%2520is%2520used%2520to%2520prove%2520the%2520type%2520soundness%250Aof%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524.%2520We%2520show%2520how%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524%2520programs%2520can%2520be%2520represented%250Ain%2520a%2520more%2520user-friendly%2520graphical%2520visualization%252C%2520and%2520provide%2520examples%2520of%2520its%250Agenerality%2520by%2520showing%2520how%2520it%2520can%2520be%2520used%2520to%2520define%2520some%2520of%2520the%2520most%2520popular%250Agraph%2520neural%2520network%2520models%252C%2520or%2520to%2520develop%2520any%2520custom%2520graph%2520processing%250Aapplication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks&entry.906535625=Matteo%20Belenchia%20and%20Flavio%20Corradini%20and%20Michela%20Quadrini%20and%20Michele%20Loreti&entry.1292438233=%20%20Graph%20neural%20networks%20form%20a%20class%20of%20deep%20learning%20architectures%0Aspecifically%20designed%20to%20work%20with%20graph-structured%20data.%20As%20such%2C%20they%20share%0Athe%20inherent%20limitations%20and%20problems%20of%20deep%20learning%2C%20especially%20regarding%0Athe%20issues%20of%20explainability%20and%20trustworthiness.%20We%20propose%20%24%5Cmu%5Cmathcal%7BG%7D%24%2C%0Aan%20original%20domain-specific%20language%20for%20the%20specification%20of%20graph%20neural%0Anetworks%20that%20aims%20to%20overcome%20these%20issues.%20The%20language%27s%20syntax%20is%0Aintroduced%2C%20and%20its%20meaning%20is%20rigorously%20defined%20by%20a%20denotational%20semantics.%0AAn%20equivalent%20characterization%20in%20the%20form%20of%20an%20operational%20semantics%20is%20also%0Aprovided%20and%2C%20together%20with%20a%20type%20system%2C%20is%20used%20to%20prove%20the%20type%20soundness%0Aof%20%24%5Cmu%5Cmathcal%7BG%7D%24.%20We%20show%20how%20%24%5Cmu%5Cmathcal%7BG%7D%24%20programs%20can%20be%20represented%0Ain%20a%20more%20user-friendly%20graphical%20visualization%2C%20and%20provide%20examples%20of%20its%0Agenerality%20by%20showing%20how%20it%20can%20be%20used%20to%20define%20some%20of%20the%20most%20popular%0Agraph%20neural%20network%20models%2C%20or%20to%20develop%20any%20custom%20graph%20processing%0Aapplication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09441v1&entry.124074799=Read"},
{"title": "Leveraging Computer Vision in the Intensive Care Unit (ICU) for\n  Examining Visitation and Mobility", "author": "Scott Siegel and Jiaqing Zhang and Sabyasachi Bandyopadhyay and Subhash Nerella and Brandon Silva and Tezcan Baslanti and Azra Bihorac and Parisa Rashidi", "abstract": "  Despite the importance of closely monitoring patients in the Intensive Care\nUnit (ICU), many aspects are still assessed in a limited manner due to the time\nconstraints imposed on healthcare providers. For example, although excessive\nvisitations during rest hours can potentially exacerbate the risk of circadian\nrhythm disruption and delirium, it is not captured in the ICU. Likewise, while\nmobility can be an important indicator of recovery or deterioration in ICU\npatients, it is only captured sporadically or not captured at all. In the past\nfew years, the computer vision field has found application in many domains by\nreducing the human burden. Using computer vision systems in the ICU can also\npotentially enable non-existing assessments or enhance the frequency and\naccuracy of existing assessments while reducing the staff workload. In this\nstudy, we leverage a state-of-the-art noninvasive computer vision system based\non depth imaging to characterize ICU visitations and patients' mobility. We\nthen examine the relationship between visitation and several patient outcomes,\nsuch as pain, acuity, and delirium. We found an association between\ndeteriorating patient acuity and the incidence of delirium with increased\nvisitations. In contrast, self-reported pain, reported using the Defense and\nVeteran Pain Rating Scale (DVPRS), was correlated with decreased visitations.\nOur findings highlight the feasibility and potential of using noninvasive\nautonomous systems to monitor ICU patients.\n", "link": "http://arxiv.org/abs/2403.06322v2", "date": "2024-07-12", "relevancy": 2.0918, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5689}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5016}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Computer%20Vision%20in%20the%20Intensive%20Care%20Unit%20%28ICU%29%20for%0A%20%20Examining%20Visitation%20and%20Mobility&body=Title%3A%20Leveraging%20Computer%20Vision%20in%20the%20Intensive%20Care%20Unit%20%28ICU%29%20for%0A%20%20Examining%20Visitation%20and%20Mobility%0AAuthor%3A%20Scott%20Siegel%20and%20Jiaqing%20Zhang%20and%20Sabyasachi%20Bandyopadhyay%20and%20Subhash%20Nerella%20and%20Brandon%20Silva%20and%20Tezcan%20Baslanti%20and%20Azra%20Bihorac%20and%20Parisa%20Rashidi%0AAbstract%3A%20%20%20Despite%20the%20importance%20of%20closely%20monitoring%20patients%20in%20the%20Intensive%20Care%0AUnit%20%28ICU%29%2C%20many%20aspects%20are%20still%20assessed%20in%20a%20limited%20manner%20due%20to%20the%20time%0Aconstraints%20imposed%20on%20healthcare%20providers.%20For%20example%2C%20although%20excessive%0Avisitations%20during%20rest%20hours%20can%20potentially%20exacerbate%20the%20risk%20of%20circadian%0Arhythm%20disruption%20and%20delirium%2C%20it%20is%20not%20captured%20in%20the%20ICU.%20Likewise%2C%20while%0Amobility%20can%20be%20an%20important%20indicator%20of%20recovery%20or%20deterioration%20in%20ICU%0Apatients%2C%20it%20is%20only%20captured%20sporadically%20or%20not%20captured%20at%20all.%20In%20the%20past%0Afew%20years%2C%20the%20computer%20vision%20field%20has%20found%20application%20in%20many%20domains%20by%0Areducing%20the%20human%20burden.%20Using%20computer%20vision%20systems%20in%20the%20ICU%20can%20also%0Apotentially%20enable%20non-existing%20assessments%20or%20enhance%20the%20frequency%20and%0Aaccuracy%20of%20existing%20assessments%20while%20reducing%20the%20staff%20workload.%20In%20this%0Astudy%2C%20we%20leverage%20a%20state-of-the-art%20noninvasive%20computer%20vision%20system%20based%0Aon%20depth%20imaging%20to%20characterize%20ICU%20visitations%20and%20patients%27%20mobility.%20We%0Athen%20examine%20the%20relationship%20between%20visitation%20and%20several%20patient%20outcomes%2C%0Asuch%20as%20pain%2C%20acuity%2C%20and%20delirium.%20We%20found%20an%20association%20between%0Adeteriorating%20patient%20acuity%20and%20the%20incidence%20of%20delirium%20with%20increased%0Avisitations.%20In%20contrast%2C%20self-reported%20pain%2C%20reported%20using%20the%20Defense%20and%0AVeteran%20Pain%20Rating%20Scale%20%28DVPRS%29%2C%20was%20correlated%20with%20decreased%20visitations.%0AOur%20findings%20highlight%20the%20feasibility%20and%20potential%20of%20using%20noninvasive%0Aautonomous%20systems%20to%20monitor%20ICU%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Computer%2520Vision%2520in%2520the%2520Intensive%2520Care%2520Unit%2520%2528ICU%2529%2520for%250A%2520%2520Examining%2520Visitation%2520and%2520Mobility%26entry.906535625%3DScott%2520Siegel%2520and%2520Jiaqing%2520Zhang%2520and%2520Sabyasachi%2520Bandyopadhyay%2520and%2520Subhash%2520Nerella%2520and%2520Brandon%2520Silva%2520and%2520Tezcan%2520Baslanti%2520and%2520Azra%2520Bihorac%2520and%2520Parisa%2520Rashidi%26entry.1292438233%3D%2520%2520Despite%2520the%2520importance%2520of%2520closely%2520monitoring%2520patients%2520in%2520the%2520Intensive%2520Care%250AUnit%2520%2528ICU%2529%252C%2520many%2520aspects%2520are%2520still%2520assessed%2520in%2520a%2520limited%2520manner%2520due%2520to%2520the%2520time%250Aconstraints%2520imposed%2520on%2520healthcare%2520providers.%2520For%2520example%252C%2520although%2520excessive%250Avisitations%2520during%2520rest%2520hours%2520can%2520potentially%2520exacerbate%2520the%2520risk%2520of%2520circadian%250Arhythm%2520disruption%2520and%2520delirium%252C%2520it%2520is%2520not%2520captured%2520in%2520the%2520ICU.%2520Likewise%252C%2520while%250Amobility%2520can%2520be%2520an%2520important%2520indicator%2520of%2520recovery%2520or%2520deterioration%2520in%2520ICU%250Apatients%252C%2520it%2520is%2520only%2520captured%2520sporadically%2520or%2520not%2520captured%2520at%2520all.%2520In%2520the%2520past%250Afew%2520years%252C%2520the%2520computer%2520vision%2520field%2520has%2520found%2520application%2520in%2520many%2520domains%2520by%250Areducing%2520the%2520human%2520burden.%2520Using%2520computer%2520vision%2520systems%2520in%2520the%2520ICU%2520can%2520also%250Apotentially%2520enable%2520non-existing%2520assessments%2520or%2520enhance%2520the%2520frequency%2520and%250Aaccuracy%2520of%2520existing%2520assessments%2520while%2520reducing%2520the%2520staff%2520workload.%2520In%2520this%250Astudy%252C%2520we%2520leverage%2520a%2520state-of-the-art%2520noninvasive%2520computer%2520vision%2520system%2520based%250Aon%2520depth%2520imaging%2520to%2520characterize%2520ICU%2520visitations%2520and%2520patients%2527%2520mobility.%2520We%250Athen%2520examine%2520the%2520relationship%2520between%2520visitation%2520and%2520several%2520patient%2520outcomes%252C%250Asuch%2520as%2520pain%252C%2520acuity%252C%2520and%2520delirium.%2520We%2520found%2520an%2520association%2520between%250Adeteriorating%2520patient%2520acuity%2520and%2520the%2520incidence%2520of%2520delirium%2520with%2520increased%250Avisitations.%2520In%2520contrast%252C%2520self-reported%2520pain%252C%2520reported%2520using%2520the%2520Defense%2520and%250AVeteran%2520Pain%2520Rating%2520Scale%2520%2528DVPRS%2529%252C%2520was%2520correlated%2520with%2520decreased%2520visitations.%250AOur%2520findings%2520highlight%2520the%2520feasibility%2520and%2520potential%2520of%2520using%2520noninvasive%250Aautonomous%2520systems%2520to%2520monitor%2520ICU%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Computer%20Vision%20in%20the%20Intensive%20Care%20Unit%20%28ICU%29%20for%0A%20%20Examining%20Visitation%20and%20Mobility&entry.906535625=Scott%20Siegel%20and%20Jiaqing%20Zhang%20and%20Sabyasachi%20Bandyopadhyay%20and%20Subhash%20Nerella%20and%20Brandon%20Silva%20and%20Tezcan%20Baslanti%20and%20Azra%20Bihorac%20and%20Parisa%20Rashidi&entry.1292438233=%20%20Despite%20the%20importance%20of%20closely%20monitoring%20patients%20in%20the%20Intensive%20Care%0AUnit%20%28ICU%29%2C%20many%20aspects%20are%20still%20assessed%20in%20a%20limited%20manner%20due%20to%20the%20time%0Aconstraints%20imposed%20on%20healthcare%20providers.%20For%20example%2C%20although%20excessive%0Avisitations%20during%20rest%20hours%20can%20potentially%20exacerbate%20the%20risk%20of%20circadian%0Arhythm%20disruption%20and%20delirium%2C%20it%20is%20not%20captured%20in%20the%20ICU.%20Likewise%2C%20while%0Amobility%20can%20be%20an%20important%20indicator%20of%20recovery%20or%20deterioration%20in%20ICU%0Apatients%2C%20it%20is%20only%20captured%20sporadically%20or%20not%20captured%20at%20all.%20In%20the%20past%0Afew%20years%2C%20the%20computer%20vision%20field%20has%20found%20application%20in%20many%20domains%20by%0Areducing%20the%20human%20burden.%20Using%20computer%20vision%20systems%20in%20the%20ICU%20can%20also%0Apotentially%20enable%20non-existing%20assessments%20or%20enhance%20the%20frequency%20and%0Aaccuracy%20of%20existing%20assessments%20while%20reducing%20the%20staff%20workload.%20In%20this%0Astudy%2C%20we%20leverage%20a%20state-of-the-art%20noninvasive%20computer%20vision%20system%20based%0Aon%20depth%20imaging%20to%20characterize%20ICU%20visitations%20and%20patients%27%20mobility.%20We%0Athen%20examine%20the%20relationship%20between%20visitation%20and%20several%20patient%20outcomes%2C%0Asuch%20as%20pain%2C%20acuity%2C%20and%20delirium.%20We%20found%20an%20association%20between%0Adeteriorating%20patient%20acuity%20and%20the%20incidence%20of%20delirium%20with%20increased%0Avisitations.%20In%20contrast%2C%20self-reported%20pain%2C%20reported%20using%20the%20Defense%20and%0AVeteran%20Pain%20Rating%20Scale%20%28DVPRS%29%2C%20was%20correlated%20with%20decreased%20visitations.%0AOur%20findings%20highlight%20the%20feasibility%20and%20potential%20of%20using%20noninvasive%0Aautonomous%20systems%20to%20monitor%20ICU%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06322v2&entry.124074799=Read"},
{"title": "MIXED-SENSE: A Mixed Reality Sensor Emulation Framework for Test and\n  Evaluation of UAVs Against False Data Injection Attacks", "author": "Kartik A. Pant and Li-Yu Lin and Jaehyeok Kim and Worawis Sribunma and James M. Goppert and Inseok Hwang", "abstract": "  We present a high-fidelity Mixed Reality sensor emulation framework for\ntesting and evaluating the resilience of Unmanned Aerial Vehicles (UAVs)\nagainst false data injection (FDI) attacks. The proposed approach can be\nutilized to assess the impact of FDI attacks, benchmark attack detector\nperformance, and validate the effectiveness of mitigation/reconfiguration\nstrategies in single-UAV and UAV swarm operations. Our Mixed Reality framework\nleverages high-fidelity simulations of Gazebo and a Motion Capture system to\nemulate proprioceptive (e.g., GNSS) and exteroceptive (e.g., camera) sensor\nmeasurements in real-time. We propose an empirical approach to faithfully\nrecreate signal characteristics such as latency and noise in these\nmeasurements. Finally, we illustrate the efficacy of our proposed framework\nthrough a Mixed Reality experiment consisting of an emulated GNSS attack on an\nactual UAV, which (i) demonstrates the impact of false data injection attacks\non GNSS measurements and (ii) validates a mitigation strategy utilizing a\ndistributed camera network developed in our previous work. Our open-source\nimplementation is available at\n\\href{https://github.com/CogniPilot/mixed\\_sense}{\\texttt{https://github.com/CogniPilot/mixed\\_sense}}\n", "link": "http://arxiv.org/abs/2407.09342v1", "date": "2024-07-12", "relevancy": 2.0582, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5347}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIXED-SENSE%3A%20A%20Mixed%20Reality%20Sensor%20Emulation%20Framework%20for%20Test%20and%0A%20%20Evaluation%20of%20UAVs%20Against%20False%20Data%20Injection%20Attacks&body=Title%3A%20MIXED-SENSE%3A%20A%20Mixed%20Reality%20Sensor%20Emulation%20Framework%20for%20Test%20and%0A%20%20Evaluation%20of%20UAVs%20Against%20False%20Data%20Injection%20Attacks%0AAuthor%3A%20Kartik%20A.%20Pant%20and%20Li-Yu%20Lin%20and%20Jaehyeok%20Kim%20and%20Worawis%20Sribunma%20and%20James%20M.%20Goppert%20and%20Inseok%20Hwang%0AAbstract%3A%20%20%20We%20present%20a%20high-fidelity%20Mixed%20Reality%20sensor%20emulation%20framework%20for%0Atesting%20and%20evaluating%20the%20resilience%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%0Aagainst%20false%20data%20injection%20%28FDI%29%20attacks.%20The%20proposed%20approach%20can%20be%0Autilized%20to%20assess%20the%20impact%20of%20FDI%20attacks%2C%20benchmark%20attack%20detector%0Aperformance%2C%20and%20validate%20the%20effectiveness%20of%20mitigation/reconfiguration%0Astrategies%20in%20single-UAV%20and%20UAV%20swarm%20operations.%20Our%20Mixed%20Reality%20framework%0Aleverages%20high-fidelity%20simulations%20of%20Gazebo%20and%20a%20Motion%20Capture%20system%20to%0Aemulate%20proprioceptive%20%28e.g.%2C%20GNSS%29%20and%20exteroceptive%20%28e.g.%2C%20camera%29%20sensor%0Ameasurements%20in%20real-time.%20We%20propose%20an%20empirical%20approach%20to%20faithfully%0Arecreate%20signal%20characteristics%20such%20as%20latency%20and%20noise%20in%20these%0Ameasurements.%20Finally%2C%20we%20illustrate%20the%20efficacy%20of%20our%20proposed%20framework%0Athrough%20a%20Mixed%20Reality%20experiment%20consisting%20of%20an%20emulated%20GNSS%20attack%20on%20an%0Aactual%20UAV%2C%20which%20%28i%29%20demonstrates%20the%20impact%20of%20false%20data%20injection%20attacks%0Aon%20GNSS%20measurements%20and%20%28ii%29%20validates%20a%20mitigation%20strategy%20utilizing%20a%0Adistributed%20camera%20network%20developed%20in%20our%20previous%20work.%20Our%20open-source%0Aimplementation%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CogniPilot/mixed%5C_sense%7D%7B%5Ctexttt%7Bhttps%3A//github.com/CogniPilot/mixed%5C_sense%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIXED-SENSE%253A%2520A%2520Mixed%2520Reality%2520Sensor%2520Emulation%2520Framework%2520for%2520Test%2520and%250A%2520%2520Evaluation%2520of%2520UAVs%2520Against%2520False%2520Data%2520Injection%2520Attacks%26entry.906535625%3DKartik%2520A.%2520Pant%2520and%2520Li-Yu%2520Lin%2520and%2520Jaehyeok%2520Kim%2520and%2520Worawis%2520Sribunma%2520and%2520James%2520M.%2520Goppert%2520and%2520Inseok%2520Hwang%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520high-fidelity%2520Mixed%2520Reality%2520sensor%2520emulation%2520framework%2520for%250Atesting%2520and%2520evaluating%2520the%2520resilience%2520of%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%250Aagainst%2520false%2520data%2520injection%2520%2528FDI%2529%2520attacks.%2520The%2520proposed%2520approach%2520can%2520be%250Autilized%2520to%2520assess%2520the%2520impact%2520of%2520FDI%2520attacks%252C%2520benchmark%2520attack%2520detector%250Aperformance%252C%2520and%2520validate%2520the%2520effectiveness%2520of%2520mitigation/reconfiguration%250Astrategies%2520in%2520single-UAV%2520and%2520UAV%2520swarm%2520operations.%2520Our%2520Mixed%2520Reality%2520framework%250Aleverages%2520high-fidelity%2520simulations%2520of%2520Gazebo%2520and%2520a%2520Motion%2520Capture%2520system%2520to%250Aemulate%2520proprioceptive%2520%2528e.g.%252C%2520GNSS%2529%2520and%2520exteroceptive%2520%2528e.g.%252C%2520camera%2529%2520sensor%250Ameasurements%2520in%2520real-time.%2520We%2520propose%2520an%2520empirical%2520approach%2520to%2520faithfully%250Arecreate%2520signal%2520characteristics%2520such%2520as%2520latency%2520and%2520noise%2520in%2520these%250Ameasurements.%2520Finally%252C%2520we%2520illustrate%2520the%2520efficacy%2520of%2520our%2520proposed%2520framework%250Athrough%2520a%2520Mixed%2520Reality%2520experiment%2520consisting%2520of%2520an%2520emulated%2520GNSS%2520attack%2520on%2520an%250Aactual%2520UAV%252C%2520which%2520%2528i%2529%2520demonstrates%2520the%2520impact%2520of%2520false%2520data%2520injection%2520attacks%250Aon%2520GNSS%2520measurements%2520and%2520%2528ii%2529%2520validates%2520a%2520mitigation%2520strategy%2520utilizing%2520a%250Adistributed%2520camera%2520network%2520developed%2520in%2520our%2520previous%2520work.%2520Our%2520open-source%250Aimplementation%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/CogniPilot/mixed%255C_sense%257D%257B%255Ctexttt%257Bhttps%253A//github.com/CogniPilot/mixed%255C_sense%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIXED-SENSE%3A%20A%20Mixed%20Reality%20Sensor%20Emulation%20Framework%20for%20Test%20and%0A%20%20Evaluation%20of%20UAVs%20Against%20False%20Data%20Injection%20Attacks&entry.906535625=Kartik%20A.%20Pant%20and%20Li-Yu%20Lin%20and%20Jaehyeok%20Kim%20and%20Worawis%20Sribunma%20and%20James%20M.%20Goppert%20and%20Inseok%20Hwang&entry.1292438233=%20%20We%20present%20a%20high-fidelity%20Mixed%20Reality%20sensor%20emulation%20framework%20for%0Atesting%20and%20evaluating%20the%20resilience%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%0Aagainst%20false%20data%20injection%20%28FDI%29%20attacks.%20The%20proposed%20approach%20can%20be%0Autilized%20to%20assess%20the%20impact%20of%20FDI%20attacks%2C%20benchmark%20attack%20detector%0Aperformance%2C%20and%20validate%20the%20effectiveness%20of%20mitigation/reconfiguration%0Astrategies%20in%20single-UAV%20and%20UAV%20swarm%20operations.%20Our%20Mixed%20Reality%20framework%0Aleverages%20high-fidelity%20simulations%20of%20Gazebo%20and%20a%20Motion%20Capture%20system%20to%0Aemulate%20proprioceptive%20%28e.g.%2C%20GNSS%29%20and%20exteroceptive%20%28e.g.%2C%20camera%29%20sensor%0Ameasurements%20in%20real-time.%20We%20propose%20an%20empirical%20approach%20to%20faithfully%0Arecreate%20signal%20characteristics%20such%20as%20latency%20and%20noise%20in%20these%0Ameasurements.%20Finally%2C%20we%20illustrate%20the%20efficacy%20of%20our%20proposed%20framework%0Athrough%20a%20Mixed%20Reality%20experiment%20consisting%20of%20an%20emulated%20GNSS%20attack%20on%20an%0Aactual%20UAV%2C%20which%20%28i%29%20demonstrates%20the%20impact%20of%20false%20data%20injection%20attacks%0Aon%20GNSS%20measurements%20and%20%28ii%29%20validates%20a%20mitigation%20strategy%20utilizing%20a%0Adistributed%20camera%20network%20developed%20in%20our%20previous%20work.%20Our%20open-source%0Aimplementation%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CogniPilot/mixed%5C_sense%7D%7B%5Ctexttt%7Bhttps%3A//github.com/CogniPilot/mixed%5C_sense%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09342v1&entry.124074799=Read"},
{"title": "Bayesian Learning-driven Prototypical Contrastive Loss for\n  Class-Incremental Learning", "author": "Nisha L. Raichur and Lucas Heublein and Tobias Feigl and Alexander R\u00fcgamer and Christopher Mutschler and Felix Ott", "abstract": "  The primary objective of methods in continual learning is to learn tasks in a\nsequential manner over time from a stream of data, while mitigating the\ndetrimental phenomenon of catastrophic forgetting. In this paper, we focus on\nlearning an optimal representation between previous class prototypes and newly\nencountered ones. We propose a prototypical network with a Bayesian\nlearning-driven contrastive loss (BLCL) tailored specifically for\nclass-incremental learning scenarios. Therefore, we introduce a contrastive\nloss that incorporates new classes into the latent representation by reducing\nthe intra-class distance and increasing the inter-class distance. Our approach\ndynamically adapts the balance between the cross-entropy and contrastive loss\nfunctions with a Bayesian learning technique. Empirical evaluations conducted\non both the CIFAR-10 and CIFAR-100 dataset for image classification and images\nof a GNSS-based dataset for interference classification validate the efficacy\nof our method, showcasing its superiority over existing state-of-the-art\napproaches.\n", "link": "http://arxiv.org/abs/2405.11067v2", "date": "2024-07-12", "relevancy": 2.0504, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5162}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5161}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Learning-driven%20Prototypical%20Contrastive%20Loss%20for%0A%20%20Class-Incremental%20Learning&body=Title%3A%20Bayesian%20Learning-driven%20Prototypical%20Contrastive%20Loss%20for%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Nisha%20L.%20Raichur%20and%20Lucas%20Heublein%20and%20Tobias%20Feigl%20and%20Alexander%20R%C3%BCgamer%20and%20Christopher%20Mutschler%20and%20Felix%20Ott%0AAbstract%3A%20%20%20The%20primary%20objective%20of%20methods%20in%20continual%20learning%20is%20to%20learn%20tasks%20in%20a%0Asequential%20manner%20over%20time%20from%20a%20stream%20of%20data%2C%20while%20mitigating%20the%0Adetrimental%20phenomenon%20of%20catastrophic%20forgetting.%20In%20this%20paper%2C%20we%20focus%20on%0Alearning%20an%20optimal%20representation%20between%20previous%20class%20prototypes%20and%20newly%0Aencountered%20ones.%20We%20propose%20a%20prototypical%20network%20with%20a%20Bayesian%0Alearning-driven%20contrastive%20loss%20%28BLCL%29%20tailored%20specifically%20for%0Aclass-incremental%20learning%20scenarios.%20Therefore%2C%20we%20introduce%20a%20contrastive%0Aloss%20that%20incorporates%20new%20classes%20into%20the%20latent%20representation%20by%20reducing%0Athe%20intra-class%20distance%20and%20increasing%20the%20inter-class%20distance.%20Our%20approach%0Adynamically%20adapts%20the%20balance%20between%20the%20cross-entropy%20and%20contrastive%20loss%0Afunctions%20with%20a%20Bayesian%20learning%20technique.%20Empirical%20evaluations%20conducted%0Aon%20both%20the%20CIFAR-10%20and%20CIFAR-100%20dataset%20for%20image%20classification%20and%20images%0Aof%20a%20GNSS-based%20dataset%20for%20interference%20classification%20validate%20the%20efficacy%0Aof%20our%20method%2C%20showcasing%20its%20superiority%20over%20existing%20state-of-the-art%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Learning-driven%2520Prototypical%2520Contrastive%2520Loss%2520for%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DNisha%2520L.%2520Raichur%2520and%2520Lucas%2520Heublein%2520and%2520Tobias%2520Feigl%2520and%2520Alexander%2520R%25C3%25BCgamer%2520and%2520Christopher%2520Mutschler%2520and%2520Felix%2520Ott%26entry.1292438233%3D%2520%2520The%2520primary%2520objective%2520of%2520methods%2520in%2520continual%2520learning%2520is%2520to%2520learn%2520tasks%2520in%2520a%250Asequential%2520manner%2520over%2520time%2520from%2520a%2520stream%2520of%2520data%252C%2520while%2520mitigating%2520the%250Adetrimental%2520phenomenon%2520of%2520catastrophic%2520forgetting.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%250Alearning%2520an%2520optimal%2520representation%2520between%2520previous%2520class%2520prototypes%2520and%2520newly%250Aencountered%2520ones.%2520We%2520propose%2520a%2520prototypical%2520network%2520with%2520a%2520Bayesian%250Alearning-driven%2520contrastive%2520loss%2520%2528BLCL%2529%2520tailored%2520specifically%2520for%250Aclass-incremental%2520learning%2520scenarios.%2520Therefore%252C%2520we%2520introduce%2520a%2520contrastive%250Aloss%2520that%2520incorporates%2520new%2520classes%2520into%2520the%2520latent%2520representation%2520by%2520reducing%250Athe%2520intra-class%2520distance%2520and%2520increasing%2520the%2520inter-class%2520distance.%2520Our%2520approach%250Adynamically%2520adapts%2520the%2520balance%2520between%2520the%2520cross-entropy%2520and%2520contrastive%2520loss%250Afunctions%2520with%2520a%2520Bayesian%2520learning%2520technique.%2520Empirical%2520evaluations%2520conducted%250Aon%2520both%2520the%2520CIFAR-10%2520and%2520CIFAR-100%2520dataset%2520for%2520image%2520classification%2520and%2520images%250Aof%2520a%2520GNSS-based%2520dataset%2520for%2520interference%2520classification%2520validate%2520the%2520efficacy%250Aof%2520our%2520method%252C%2520showcasing%2520its%2520superiority%2520over%2520existing%2520state-of-the-art%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Learning-driven%20Prototypical%20Contrastive%20Loss%20for%0A%20%20Class-Incremental%20Learning&entry.906535625=Nisha%20L.%20Raichur%20and%20Lucas%20Heublein%20and%20Tobias%20Feigl%20and%20Alexander%20R%C3%BCgamer%20and%20Christopher%20Mutschler%20and%20Felix%20Ott&entry.1292438233=%20%20The%20primary%20objective%20of%20methods%20in%20continual%20learning%20is%20to%20learn%20tasks%20in%20a%0Asequential%20manner%20over%20time%20from%20a%20stream%20of%20data%2C%20while%20mitigating%20the%0Adetrimental%20phenomenon%20of%20catastrophic%20forgetting.%20In%20this%20paper%2C%20we%20focus%20on%0Alearning%20an%20optimal%20representation%20between%20previous%20class%20prototypes%20and%20newly%0Aencountered%20ones.%20We%20propose%20a%20prototypical%20network%20with%20a%20Bayesian%0Alearning-driven%20contrastive%20loss%20%28BLCL%29%20tailored%20specifically%20for%0Aclass-incremental%20learning%20scenarios.%20Therefore%2C%20we%20introduce%20a%20contrastive%0Aloss%20that%20incorporates%20new%20classes%20into%20the%20latent%20representation%20by%20reducing%0Athe%20intra-class%20distance%20and%20increasing%20the%20inter-class%20distance.%20Our%20approach%0Adynamically%20adapts%20the%20balance%20between%20the%20cross-entropy%20and%20contrastive%20loss%0Afunctions%20with%20a%20Bayesian%20learning%20technique.%20Empirical%20evaluations%20conducted%0Aon%20both%20the%20CIFAR-10%20and%20CIFAR-100%20dataset%20for%20image%20classification%20and%20images%0Aof%20a%20GNSS-based%20dataset%20for%20interference%20classification%20validate%20the%20efficacy%0Aof%20our%20method%2C%20showcasing%20its%20superiority%20over%20existing%20state-of-the-art%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11067v2&entry.124074799=Read"},
{"title": "Semi-Supervised Learning for Deep Causal Generative Models", "author": "Yasin Ibrahim and Hermione Warr and Konstantinos Kamnitsas", "abstract": "  Developing models that are capable of answering questions of the form \"How\nwould x change if y had been z?'\" is fundamental to advancing medical image\nanalysis. Training causal generative models that address such counterfactual\nquestions, though, currently requires that all relevant variables have been\nobserved and that the corresponding labels are available in the training data.\nHowever, clinical data may not have complete records for all patients and state\nof the art causal generative models are unable to take full advantage of this.\nWe thus develop, for the first time, a semi-supervised deep causal generative\nmodel that exploits the causal relationships between variables to maximise the\nuse of all available data. We explore this in the setting where each sample is\neither fully labelled or fully unlabelled, as well as the more clinically\nrealistic case of having different labels missing for each sample. We leverage\ntechniques from causal inference to infer missing values and subsequently\ngenerate realistic counterfactuals, even for samples with incomplete labels.\n", "link": "http://arxiv.org/abs/2403.18717v2", "date": "2024-07-12", "relevancy": 2.0483, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5386}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Learning%20for%20Deep%20Causal%20Generative%20Models&body=Title%3A%20Semi-Supervised%20Learning%20for%20Deep%20Causal%20Generative%20Models%0AAuthor%3A%20Yasin%20Ibrahim%20and%20Hermione%20Warr%20and%20Konstantinos%20Kamnitsas%0AAbstract%3A%20%20%20Developing%20models%20that%20are%20capable%20of%20answering%20questions%20of%20the%20form%20%22How%0Awould%20x%20change%20if%20y%20had%20been%20z%3F%27%22%20is%20fundamental%20to%20advancing%20medical%20image%0Aanalysis.%20Training%20causal%20generative%20models%20that%20address%20such%20counterfactual%0Aquestions%2C%20though%2C%20currently%20requires%20that%20all%20relevant%20variables%20have%20been%0Aobserved%20and%20that%20the%20corresponding%20labels%20are%20available%20in%20the%20training%20data.%0AHowever%2C%20clinical%20data%20may%20not%20have%20complete%20records%20for%20all%20patients%20and%20state%0Aof%20the%20art%20causal%20generative%20models%20are%20unable%20to%20take%20full%20advantage%20of%20this.%0AWe%20thus%20develop%2C%20for%20the%20first%20time%2C%20a%20semi-supervised%20deep%20causal%20generative%0Amodel%20that%20exploits%20the%20causal%20relationships%20between%20variables%20to%20maximise%20the%0Ause%20of%20all%20available%20data.%20We%20explore%20this%20in%20the%20setting%20where%20each%20sample%20is%0Aeither%20fully%20labelled%20or%20fully%20unlabelled%2C%20as%20well%20as%20the%20more%20clinically%0Arealistic%20case%20of%20having%20different%20labels%20missing%20for%20each%20sample.%20We%20leverage%0Atechniques%20from%20causal%20inference%20to%20infer%20missing%20values%20and%20subsequently%0Agenerate%20realistic%20counterfactuals%2C%20even%20for%20samples%20with%20incomplete%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Learning%2520for%2520Deep%2520Causal%2520Generative%2520Models%26entry.906535625%3DYasin%2520Ibrahim%2520and%2520Hermione%2520Warr%2520and%2520Konstantinos%2520Kamnitsas%26entry.1292438233%3D%2520%2520Developing%2520models%2520that%2520are%2520capable%2520of%2520answering%2520questions%2520of%2520the%2520form%2520%2522How%250Awould%2520x%2520change%2520if%2520y%2520had%2520been%2520z%253F%2527%2522%2520is%2520fundamental%2520to%2520advancing%2520medical%2520image%250Aanalysis.%2520Training%2520causal%2520generative%2520models%2520that%2520address%2520such%2520counterfactual%250Aquestions%252C%2520though%252C%2520currently%2520requires%2520that%2520all%2520relevant%2520variables%2520have%2520been%250Aobserved%2520and%2520that%2520the%2520corresponding%2520labels%2520are%2520available%2520in%2520the%2520training%2520data.%250AHowever%252C%2520clinical%2520data%2520may%2520not%2520have%2520complete%2520records%2520for%2520all%2520patients%2520and%2520state%250Aof%2520the%2520art%2520causal%2520generative%2520models%2520are%2520unable%2520to%2520take%2520full%2520advantage%2520of%2520this.%250AWe%2520thus%2520develop%252C%2520for%2520the%2520first%2520time%252C%2520a%2520semi-supervised%2520deep%2520causal%2520generative%250Amodel%2520that%2520exploits%2520the%2520causal%2520relationships%2520between%2520variables%2520to%2520maximise%2520the%250Ause%2520of%2520all%2520available%2520data.%2520We%2520explore%2520this%2520in%2520the%2520setting%2520where%2520each%2520sample%2520is%250Aeither%2520fully%2520labelled%2520or%2520fully%2520unlabelled%252C%2520as%2520well%2520as%2520the%2520more%2520clinically%250Arealistic%2520case%2520of%2520having%2520different%2520labels%2520missing%2520for%2520each%2520sample.%2520We%2520leverage%250Atechniques%2520from%2520causal%2520inference%2520to%2520infer%2520missing%2520values%2520and%2520subsequently%250Agenerate%2520realistic%2520counterfactuals%252C%2520even%2520for%2520samples%2520with%2520incomplete%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Learning%20for%20Deep%20Causal%20Generative%20Models&entry.906535625=Yasin%20Ibrahim%20and%20Hermione%20Warr%20and%20Konstantinos%20Kamnitsas&entry.1292438233=%20%20Developing%20models%20that%20are%20capable%20of%20answering%20questions%20of%20the%20form%20%22How%0Awould%20x%20change%20if%20y%20had%20been%20z%3F%27%22%20is%20fundamental%20to%20advancing%20medical%20image%0Aanalysis.%20Training%20causal%20generative%20models%20that%20address%20such%20counterfactual%0Aquestions%2C%20though%2C%20currently%20requires%20that%20all%20relevant%20variables%20have%20been%0Aobserved%20and%20that%20the%20corresponding%20labels%20are%20available%20in%20the%20training%20data.%0AHowever%2C%20clinical%20data%20may%20not%20have%20complete%20records%20for%20all%20patients%20and%20state%0Aof%20the%20art%20causal%20generative%20models%20are%20unable%20to%20take%20full%20advantage%20of%20this.%0AWe%20thus%20develop%2C%20for%20the%20first%20time%2C%20a%20semi-supervised%20deep%20causal%20generative%0Amodel%20that%20exploits%20the%20causal%20relationships%20between%20variables%20to%20maximise%20the%0Ause%20of%20all%20available%20data.%20We%20explore%20this%20in%20the%20setting%20where%20each%20sample%20is%0Aeither%20fully%20labelled%20or%20fully%20unlabelled%2C%20as%20well%20as%20the%20more%20clinically%0Arealistic%20case%20of%20having%20different%20labels%20missing%20for%20each%20sample.%20We%20leverage%0Atechniques%20from%20causal%20inference%20to%20infer%20missing%20values%20and%20subsequently%0Agenerate%20realistic%20counterfactuals%2C%20even%20for%20samples%20with%20incomplete%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18717v2&entry.124074799=Read"},
{"title": "Evaluating the Adversarial Robustness of Semantic Segmentation: Trying\n  Harder Pays Off", "author": "Levente Halmosi and B\u00e1lint Mohos and M\u00e1rk Jelasity", "abstract": "  Machine learning models are vulnerable to tiny adversarial input\nperturbations optimized to cause a very large output error. To measure this\nvulnerability, we need reliable methods that can find such adversarial\nperturbations. For image classification models, evaluation methodologies have\nemerged that have stood the test of time. However, we argue that in the area of\nsemantic segmentation, a good approximation of the sensitivity to adversarial\nperturbations requires significantly more effort than what is currently\nconsidered satisfactory. To support this claim, we re-evaluate a number of\nwell-known robust segmentation models in an extensive empirical study. We\npropose new attacks and combine them with the strongest attacks available in\nthe literature. We also analyze the sensitivity of the models in fine detail.\nThe results indicate that most of the state-of-the-art models have a\ndramatically larger sensitivity to adversarial perturbations than previously\nreported. We also demonstrate a size-bias: small objects are often more easily\nattacked, even if the large objects are robust, a phenomenon not revealed by\ncurrent evaluation metrics. Our results also demonstrate that a diverse set of\nstrong attacks is necessary, because different models are often vulnerable to\ndifferent attacks.\n", "link": "http://arxiv.org/abs/2407.09150v1", "date": "2024-07-12", "relevancy": 2.0475, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5329}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5093}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Adversarial%20Robustness%20of%20Semantic%20Segmentation%3A%20Trying%0A%20%20Harder%20Pays%20Off&body=Title%3A%20Evaluating%20the%20Adversarial%20Robustness%20of%20Semantic%20Segmentation%3A%20Trying%0A%20%20Harder%20Pays%20Off%0AAuthor%3A%20Levente%20Halmosi%20and%20B%C3%A1lint%20Mohos%20and%20M%C3%A1rk%20Jelasity%0AAbstract%3A%20%20%20Machine%20learning%20models%20are%20vulnerable%20to%20tiny%20adversarial%20input%0Aperturbations%20optimized%20to%20cause%20a%20very%20large%20output%20error.%20To%20measure%20this%0Avulnerability%2C%20we%20need%20reliable%20methods%20that%20can%20find%20such%20adversarial%0Aperturbations.%20For%20image%20classification%20models%2C%20evaluation%20methodologies%20have%0Aemerged%20that%20have%20stood%20the%20test%20of%20time.%20However%2C%20we%20argue%20that%20in%20the%20area%20of%0Asemantic%20segmentation%2C%20a%20good%20approximation%20of%20the%20sensitivity%20to%20adversarial%0Aperturbations%20requires%20significantly%20more%20effort%20than%20what%20is%20currently%0Aconsidered%20satisfactory.%20To%20support%20this%20claim%2C%20we%20re-evaluate%20a%20number%20of%0Awell-known%20robust%20segmentation%20models%20in%20an%20extensive%20empirical%20study.%20We%0Apropose%20new%20attacks%20and%20combine%20them%20with%20the%20strongest%20attacks%20available%20in%0Athe%20literature.%20We%20also%20analyze%20the%20sensitivity%20of%20the%20models%20in%20fine%20detail.%0AThe%20results%20indicate%20that%20most%20of%20the%20state-of-the-art%20models%20have%20a%0Adramatically%20larger%20sensitivity%20to%20adversarial%20perturbations%20than%20previously%0Areported.%20We%20also%20demonstrate%20a%20size-bias%3A%20small%20objects%20are%20often%20more%20easily%0Aattacked%2C%20even%20if%20the%20large%20objects%20are%20robust%2C%20a%20phenomenon%20not%20revealed%20by%0Acurrent%20evaluation%20metrics.%20Our%20results%20also%20demonstrate%20that%20a%20diverse%20set%20of%0Astrong%20attacks%20is%20necessary%2C%20because%20different%20models%20are%20often%20vulnerable%20to%0Adifferent%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Adversarial%2520Robustness%2520of%2520Semantic%2520Segmentation%253A%2520Trying%250A%2520%2520Harder%2520Pays%2520Off%26entry.906535625%3DLevente%2520Halmosi%2520and%2520B%25C3%25A1lint%2520Mohos%2520and%2520M%25C3%25A1rk%2520Jelasity%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520are%2520vulnerable%2520to%2520tiny%2520adversarial%2520input%250Aperturbations%2520optimized%2520to%2520cause%2520a%2520very%2520large%2520output%2520error.%2520To%2520measure%2520this%250Avulnerability%252C%2520we%2520need%2520reliable%2520methods%2520that%2520can%2520find%2520such%2520adversarial%250Aperturbations.%2520For%2520image%2520classification%2520models%252C%2520evaluation%2520methodologies%2520have%250Aemerged%2520that%2520have%2520stood%2520the%2520test%2520of%2520time.%2520However%252C%2520we%2520argue%2520that%2520in%2520the%2520area%2520of%250Asemantic%2520segmentation%252C%2520a%2520good%2520approximation%2520of%2520the%2520sensitivity%2520to%2520adversarial%250Aperturbations%2520requires%2520significantly%2520more%2520effort%2520than%2520what%2520is%2520currently%250Aconsidered%2520satisfactory.%2520To%2520support%2520this%2520claim%252C%2520we%2520re-evaluate%2520a%2520number%2520of%250Awell-known%2520robust%2520segmentation%2520models%2520in%2520an%2520extensive%2520empirical%2520study.%2520We%250Apropose%2520new%2520attacks%2520and%2520combine%2520them%2520with%2520the%2520strongest%2520attacks%2520available%2520in%250Athe%2520literature.%2520We%2520also%2520analyze%2520the%2520sensitivity%2520of%2520the%2520models%2520in%2520fine%2520detail.%250AThe%2520results%2520indicate%2520that%2520most%2520of%2520the%2520state-of-the-art%2520models%2520have%2520a%250Adramatically%2520larger%2520sensitivity%2520to%2520adversarial%2520perturbations%2520than%2520previously%250Areported.%2520We%2520also%2520demonstrate%2520a%2520size-bias%253A%2520small%2520objects%2520are%2520often%2520more%2520easily%250Aattacked%252C%2520even%2520if%2520the%2520large%2520objects%2520are%2520robust%252C%2520a%2520phenomenon%2520not%2520revealed%2520by%250Acurrent%2520evaluation%2520metrics.%2520Our%2520results%2520also%2520demonstrate%2520that%2520a%2520diverse%2520set%2520of%250Astrong%2520attacks%2520is%2520necessary%252C%2520because%2520different%2520models%2520are%2520often%2520vulnerable%2520to%250Adifferent%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Adversarial%20Robustness%20of%20Semantic%20Segmentation%3A%20Trying%0A%20%20Harder%20Pays%20Off&entry.906535625=Levente%20Halmosi%20and%20B%C3%A1lint%20Mohos%20and%20M%C3%A1rk%20Jelasity&entry.1292438233=%20%20Machine%20learning%20models%20are%20vulnerable%20to%20tiny%20adversarial%20input%0Aperturbations%20optimized%20to%20cause%20a%20very%20large%20output%20error.%20To%20measure%20this%0Avulnerability%2C%20we%20need%20reliable%20methods%20that%20can%20find%20such%20adversarial%0Aperturbations.%20For%20image%20classification%20models%2C%20evaluation%20methodologies%20have%0Aemerged%20that%20have%20stood%20the%20test%20of%20time.%20However%2C%20we%20argue%20that%20in%20the%20area%20of%0Asemantic%20segmentation%2C%20a%20good%20approximation%20of%20the%20sensitivity%20to%20adversarial%0Aperturbations%20requires%20significantly%20more%20effort%20than%20what%20is%20currently%0Aconsidered%20satisfactory.%20To%20support%20this%20claim%2C%20we%20re-evaluate%20a%20number%20of%0Awell-known%20robust%20segmentation%20models%20in%20an%20extensive%20empirical%20study.%20We%0Apropose%20new%20attacks%20and%20combine%20them%20with%20the%20strongest%20attacks%20available%20in%0Athe%20literature.%20We%20also%20analyze%20the%20sensitivity%20of%20the%20models%20in%20fine%20detail.%0AThe%20results%20indicate%20that%20most%20of%20the%20state-of-the-art%20models%20have%20a%0Adramatically%20larger%20sensitivity%20to%20adversarial%20perturbations%20than%20previously%0Areported.%20We%20also%20demonstrate%20a%20size-bias%3A%20small%20objects%20are%20often%20more%20easily%0Aattacked%2C%20even%20if%20the%20large%20objects%20are%20robust%2C%20a%20phenomenon%20not%20revealed%20by%0Acurrent%20evaluation%20metrics.%20Our%20results%20also%20demonstrate%20that%20a%20diverse%20set%20of%0Astrong%20attacks%20is%20necessary%2C%20because%20different%20models%20are%20often%20vulnerable%20to%0Adifferent%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09150v1&entry.124074799=Read"},
{"title": "Is Contrasting All You Need? Contrastive Learning for the Detection and\n  Attribution of AI-generated Text", "author": "Lucio La Cava and Davide Costa and Andrea Tagarelli", "abstract": "  The significant progress in the development of Large Language Models has\ncontributed to blurring the distinction between human and AI-generated text.\nThe increasing pervasiveness of AI-generated text and the difficulty in\ndetecting it poses new challenges for our society. In this paper, we tackle the\nproblem of detecting and attributing AI-generated text by proposing WhosAI, a\ntriplet-network contrastive learning framework designed to predict whether a\ngiven input text has been generated by humans or AI and to unveil the\nauthorship of the text. Unlike most existing approaches, our proposed framework\nis conceived to learn semantic similarity representations from multiple\ngenerators at once, thus equally handling both detection and attribution tasks.\nFurthermore, WhosAI is model-agnostic and scalable to the release of new AI\ntext-generation models by incorporating their generated instances into the\nembedding space learned by our framework. Experimental results on the\nTuringBench benchmark of 200K news articles show that our proposed framework\nachieves outstanding results in both the Turing Test and Authorship Attribution\ntasks, outperforming all the methods listed in the TuringBench benchmark\nleaderboards.\n", "link": "http://arxiv.org/abs/2407.09364v1", "date": "2024-07-12", "relevancy": 2.0404, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5158}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5117}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Contrasting%20All%20You%20Need%3F%20Contrastive%20Learning%20for%20the%20Detection%20and%0A%20%20Attribution%20of%20AI-generated%20Text&body=Title%3A%20Is%20Contrasting%20All%20You%20Need%3F%20Contrastive%20Learning%20for%20the%20Detection%20and%0A%20%20Attribution%20of%20AI-generated%20Text%0AAuthor%3A%20Lucio%20La%20Cava%20and%20Davide%20Costa%20and%20Andrea%20Tagarelli%0AAbstract%3A%20%20%20The%20significant%20progress%20in%20the%20development%20of%20Large%20Language%20Models%20has%0Acontributed%20to%20blurring%20the%20distinction%20between%20human%20and%20AI-generated%20text.%0AThe%20increasing%20pervasiveness%20of%20AI-generated%20text%20and%20the%20difficulty%20in%0Adetecting%20it%20poses%20new%20challenges%20for%20our%20society.%20In%20this%20paper%2C%20we%20tackle%20the%0Aproblem%20of%20detecting%20and%20attributing%20AI-generated%20text%20by%20proposing%20WhosAI%2C%20a%0Atriplet-network%20contrastive%20learning%20framework%20designed%20to%20predict%20whether%20a%0Agiven%20input%20text%20has%20been%20generated%20by%20humans%20or%20AI%20and%20to%20unveil%20the%0Aauthorship%20of%20the%20text.%20Unlike%20most%20existing%20approaches%2C%20our%20proposed%20framework%0Ais%20conceived%20to%20learn%20semantic%20similarity%20representations%20from%20multiple%0Agenerators%20at%20once%2C%20thus%20equally%20handling%20both%20detection%20and%20attribution%20tasks.%0AFurthermore%2C%20WhosAI%20is%20model-agnostic%20and%20scalable%20to%20the%20release%20of%20new%20AI%0Atext-generation%20models%20by%20incorporating%20their%20generated%20instances%20into%20the%0Aembedding%20space%20learned%20by%20our%20framework.%20Experimental%20results%20on%20the%0ATuringBench%20benchmark%20of%20200K%20news%20articles%20show%20that%20our%20proposed%20framework%0Aachieves%20outstanding%20results%20in%20both%20the%20Turing%20Test%20and%20Authorship%20Attribution%0Atasks%2C%20outperforming%20all%20the%20methods%20listed%20in%20the%20TuringBench%20benchmark%0Aleaderboards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Contrasting%2520All%2520You%2520Need%253F%2520Contrastive%2520Learning%2520for%2520the%2520Detection%2520and%250A%2520%2520Attribution%2520of%2520AI-generated%2520Text%26entry.906535625%3DLucio%2520La%2520Cava%2520and%2520Davide%2520Costa%2520and%2520Andrea%2520Tagarelli%26entry.1292438233%3D%2520%2520The%2520significant%2520progress%2520in%2520the%2520development%2520of%2520Large%2520Language%2520Models%2520has%250Acontributed%2520to%2520blurring%2520the%2520distinction%2520between%2520human%2520and%2520AI-generated%2520text.%250AThe%2520increasing%2520pervasiveness%2520of%2520AI-generated%2520text%2520and%2520the%2520difficulty%2520in%250Adetecting%2520it%2520poses%2520new%2520challenges%2520for%2520our%2520society.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%250Aproblem%2520of%2520detecting%2520and%2520attributing%2520AI-generated%2520text%2520by%2520proposing%2520WhosAI%252C%2520a%250Atriplet-network%2520contrastive%2520learning%2520framework%2520designed%2520to%2520predict%2520whether%2520a%250Agiven%2520input%2520text%2520has%2520been%2520generated%2520by%2520humans%2520or%2520AI%2520and%2520to%2520unveil%2520the%250Aauthorship%2520of%2520the%2520text.%2520Unlike%2520most%2520existing%2520approaches%252C%2520our%2520proposed%2520framework%250Ais%2520conceived%2520to%2520learn%2520semantic%2520similarity%2520representations%2520from%2520multiple%250Agenerators%2520at%2520once%252C%2520thus%2520equally%2520handling%2520both%2520detection%2520and%2520attribution%2520tasks.%250AFurthermore%252C%2520WhosAI%2520is%2520model-agnostic%2520and%2520scalable%2520to%2520the%2520release%2520of%2520new%2520AI%250Atext-generation%2520models%2520by%2520incorporating%2520their%2520generated%2520instances%2520into%2520the%250Aembedding%2520space%2520learned%2520by%2520our%2520framework.%2520Experimental%2520results%2520on%2520the%250ATuringBench%2520benchmark%2520of%2520200K%2520news%2520articles%2520show%2520that%2520our%2520proposed%2520framework%250Aachieves%2520outstanding%2520results%2520in%2520both%2520the%2520Turing%2520Test%2520and%2520Authorship%2520Attribution%250Atasks%252C%2520outperforming%2520all%2520the%2520methods%2520listed%2520in%2520the%2520TuringBench%2520benchmark%250Aleaderboards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Contrasting%20All%20You%20Need%3F%20Contrastive%20Learning%20for%20the%20Detection%20and%0A%20%20Attribution%20of%20AI-generated%20Text&entry.906535625=Lucio%20La%20Cava%20and%20Davide%20Costa%20and%20Andrea%20Tagarelli&entry.1292438233=%20%20The%20significant%20progress%20in%20the%20development%20of%20Large%20Language%20Models%20has%0Acontributed%20to%20blurring%20the%20distinction%20between%20human%20and%20AI-generated%20text.%0AThe%20increasing%20pervasiveness%20of%20AI-generated%20text%20and%20the%20difficulty%20in%0Adetecting%20it%20poses%20new%20challenges%20for%20our%20society.%20In%20this%20paper%2C%20we%20tackle%20the%0Aproblem%20of%20detecting%20and%20attributing%20AI-generated%20text%20by%20proposing%20WhosAI%2C%20a%0Atriplet-network%20contrastive%20learning%20framework%20designed%20to%20predict%20whether%20a%0Agiven%20input%20text%20has%20been%20generated%20by%20humans%20or%20AI%20and%20to%20unveil%20the%0Aauthorship%20of%20the%20text.%20Unlike%20most%20existing%20approaches%2C%20our%20proposed%20framework%0Ais%20conceived%20to%20learn%20semantic%20similarity%20representations%20from%20multiple%0Agenerators%20at%20once%2C%20thus%20equally%20handling%20both%20detection%20and%20attribution%20tasks.%0AFurthermore%2C%20WhosAI%20is%20model-agnostic%20and%20scalable%20to%20the%20release%20of%20new%20AI%0Atext-generation%20models%20by%20incorporating%20their%20generated%20instances%20into%20the%0Aembedding%20space%20learned%20by%20our%20framework.%20Experimental%20results%20on%20the%0ATuringBench%20benchmark%20of%20200K%20news%20articles%20show%20that%20our%20proposed%20framework%0Aachieves%20outstanding%20results%20in%20both%20the%20Turing%20Test%20and%20Authorship%20Attribution%0Atasks%2C%20outperforming%20all%20the%20methods%20listed%20in%20the%20TuringBench%20benchmark%0Aleaderboards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09364v1&entry.124074799=Read"},
{"title": "Comparing supervised learning dynamics: Deep neural networks match human\n  data efficiency but show a generalisation lag", "author": "Lukas S. Huber and Fred W. Mast and Felix A. Wichmann", "abstract": "  Recent research has seen many behavioral comparisons between humans and deep\nneural networks (DNNs) in the domain of image classification. Often, comparison\nstudies focus on the end-result of the learning process by measuring and\ncomparing the similarities in the representations of object categories once\nthey have been formed. However, the process of how these representations emerge\n-- that is, the behavioral changes and intermediate stages observed during the\nacquisition -- is less often directly and empirically compared. Here we report\na detailed investigation of the learning dynamics in human observers and\nvarious classic and state-of-the-art DNNs. We develop a constrained supervised\nlearning environment to align learning-relevant conditions such as starting\npoint, input modality, available input data and the feedback provided. Across\nthe whole learning process we evaluate and compare how well learned\nrepresentations can be generalized to previously unseen test data. Comparisons\nacross the entire learning process indicate that DNNs demonstrate a level of\ndata efficiency comparable to human learners, challenging some prevailing\nassumptions in the field. However, our results also reveal representational\ndifferences: while DNNs' learning is characterized by a pronounced\ngeneralisation lag, humans appear to immediately acquire generalizable\nrepresentations without a preliminary phase of learning training set-specific\ninformation that is only later transferred to novel data.\n", "link": "http://arxiv.org/abs/2402.09303v3", "date": "2024-07-12", "relevancy": 2.0322, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.551}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4778}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20supervised%20learning%20dynamics%3A%20Deep%20neural%20networks%20match%20human%0A%20%20data%20efficiency%20but%20show%20a%20generalisation%20lag&body=Title%3A%20Comparing%20supervised%20learning%20dynamics%3A%20Deep%20neural%20networks%20match%20human%0A%20%20data%20efficiency%20but%20show%20a%20generalisation%20lag%0AAuthor%3A%20Lukas%20S.%20Huber%20and%20Fred%20W.%20Mast%20and%20Felix%20A.%20Wichmann%0AAbstract%3A%20%20%20Recent%20research%20has%20seen%20many%20behavioral%20comparisons%20between%20humans%20and%20deep%0Aneural%20networks%20%28DNNs%29%20in%20the%20domain%20of%20image%20classification.%20Often%2C%20comparison%0Astudies%20focus%20on%20the%20end-result%20of%20the%20learning%20process%20by%20measuring%20and%0Acomparing%20the%20similarities%20in%20the%20representations%20of%20object%20categories%20once%0Athey%20have%20been%20formed.%20However%2C%20the%20process%20of%20how%20these%20representations%20emerge%0A--%20that%20is%2C%20the%20behavioral%20changes%20and%20intermediate%20stages%20observed%20during%20the%0Aacquisition%20--%20is%20less%20often%20directly%20and%20empirically%20compared.%20Here%20we%20report%0Aa%20detailed%20investigation%20of%20the%20learning%20dynamics%20in%20human%20observers%20and%0Avarious%20classic%20and%20state-of-the-art%20DNNs.%20We%20develop%20a%20constrained%20supervised%0Alearning%20environment%20to%20align%20learning-relevant%20conditions%20such%20as%20starting%0Apoint%2C%20input%20modality%2C%20available%20input%20data%20and%20the%20feedback%20provided.%20Across%0Athe%20whole%20learning%20process%20we%20evaluate%20and%20compare%20how%20well%20learned%0Arepresentations%20can%20be%20generalized%20to%20previously%20unseen%20test%20data.%20Comparisons%0Aacross%20the%20entire%20learning%20process%20indicate%20that%20DNNs%20demonstrate%20a%20level%20of%0Adata%20efficiency%20comparable%20to%20human%20learners%2C%20challenging%20some%20prevailing%0Aassumptions%20in%20the%20field.%20However%2C%20our%20results%20also%20reveal%20representational%0Adifferences%3A%20while%20DNNs%27%20learning%20is%20characterized%20by%20a%20pronounced%0Ageneralisation%20lag%2C%20humans%20appear%20to%20immediately%20acquire%20generalizable%0Arepresentations%20without%20a%20preliminary%20phase%20of%20learning%20training%20set-specific%0Ainformation%20that%20is%20only%20later%20transferred%20to%20novel%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09303v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520supervised%2520learning%2520dynamics%253A%2520Deep%2520neural%2520networks%2520match%2520human%250A%2520%2520data%2520efficiency%2520but%2520show%2520a%2520generalisation%2520lag%26entry.906535625%3DLukas%2520S.%2520Huber%2520and%2520Fred%2520W.%2520Mast%2520and%2520Felix%2520A.%2520Wichmann%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520seen%2520many%2520behavioral%2520comparisons%2520between%2520humans%2520and%2520deep%250Aneural%2520networks%2520%2528DNNs%2529%2520in%2520the%2520domain%2520of%2520image%2520classification.%2520Often%252C%2520comparison%250Astudies%2520focus%2520on%2520the%2520end-result%2520of%2520the%2520learning%2520process%2520by%2520measuring%2520and%250Acomparing%2520the%2520similarities%2520in%2520the%2520representations%2520of%2520object%2520categories%2520once%250Athey%2520have%2520been%2520formed.%2520However%252C%2520the%2520process%2520of%2520how%2520these%2520representations%2520emerge%250A--%2520that%2520is%252C%2520the%2520behavioral%2520changes%2520and%2520intermediate%2520stages%2520observed%2520during%2520the%250Aacquisition%2520--%2520is%2520less%2520often%2520directly%2520and%2520empirically%2520compared.%2520Here%2520we%2520report%250Aa%2520detailed%2520investigation%2520of%2520the%2520learning%2520dynamics%2520in%2520human%2520observers%2520and%250Avarious%2520classic%2520and%2520state-of-the-art%2520DNNs.%2520We%2520develop%2520a%2520constrained%2520supervised%250Alearning%2520environment%2520to%2520align%2520learning-relevant%2520conditions%2520such%2520as%2520starting%250Apoint%252C%2520input%2520modality%252C%2520available%2520input%2520data%2520and%2520the%2520feedback%2520provided.%2520Across%250Athe%2520whole%2520learning%2520process%2520we%2520evaluate%2520and%2520compare%2520how%2520well%2520learned%250Arepresentations%2520can%2520be%2520generalized%2520to%2520previously%2520unseen%2520test%2520data.%2520Comparisons%250Aacross%2520the%2520entire%2520learning%2520process%2520indicate%2520that%2520DNNs%2520demonstrate%2520a%2520level%2520of%250Adata%2520efficiency%2520comparable%2520to%2520human%2520learners%252C%2520challenging%2520some%2520prevailing%250Aassumptions%2520in%2520the%2520field.%2520However%252C%2520our%2520results%2520also%2520reveal%2520representational%250Adifferences%253A%2520while%2520DNNs%2527%2520learning%2520is%2520characterized%2520by%2520a%2520pronounced%250Ageneralisation%2520lag%252C%2520humans%2520appear%2520to%2520immediately%2520acquire%2520generalizable%250Arepresentations%2520without%2520a%2520preliminary%2520phase%2520of%2520learning%2520training%2520set-specific%250Ainformation%2520that%2520is%2520only%2520later%2520transferred%2520to%2520novel%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09303v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20supervised%20learning%20dynamics%3A%20Deep%20neural%20networks%20match%20human%0A%20%20data%20efficiency%20but%20show%20a%20generalisation%20lag&entry.906535625=Lukas%20S.%20Huber%20and%20Fred%20W.%20Mast%20and%20Felix%20A.%20Wichmann&entry.1292438233=%20%20Recent%20research%20has%20seen%20many%20behavioral%20comparisons%20between%20humans%20and%20deep%0Aneural%20networks%20%28DNNs%29%20in%20the%20domain%20of%20image%20classification.%20Often%2C%20comparison%0Astudies%20focus%20on%20the%20end-result%20of%20the%20learning%20process%20by%20measuring%20and%0Acomparing%20the%20similarities%20in%20the%20representations%20of%20object%20categories%20once%0Athey%20have%20been%20formed.%20However%2C%20the%20process%20of%20how%20these%20representations%20emerge%0A--%20that%20is%2C%20the%20behavioral%20changes%20and%20intermediate%20stages%20observed%20during%20the%0Aacquisition%20--%20is%20less%20often%20directly%20and%20empirically%20compared.%20Here%20we%20report%0Aa%20detailed%20investigation%20of%20the%20learning%20dynamics%20in%20human%20observers%20and%0Avarious%20classic%20and%20state-of-the-art%20DNNs.%20We%20develop%20a%20constrained%20supervised%0Alearning%20environment%20to%20align%20learning-relevant%20conditions%20such%20as%20starting%0Apoint%2C%20input%20modality%2C%20available%20input%20data%20and%20the%20feedback%20provided.%20Across%0Athe%20whole%20learning%20process%20we%20evaluate%20and%20compare%20how%20well%20learned%0Arepresentations%20can%20be%20generalized%20to%20previously%20unseen%20test%20data.%20Comparisons%0Aacross%20the%20entire%20learning%20process%20indicate%20that%20DNNs%20demonstrate%20a%20level%20of%0Adata%20efficiency%20comparable%20to%20human%20learners%2C%20challenging%20some%20prevailing%0Aassumptions%20in%20the%20field.%20However%2C%20our%20results%20also%20reveal%20representational%0Adifferences%3A%20while%20DNNs%27%20learning%20is%20characterized%20by%20a%20pronounced%0Ageneralisation%20lag%2C%20humans%20appear%20to%20immediately%20acquire%20generalizable%0Arepresentations%20without%20a%20preliminary%20phase%20of%20learning%20training%20set-specific%0Ainformation%20that%20is%20only%20later%20transferred%20to%20novel%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09303v3&entry.124074799=Read"},
{"title": "Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap\n  for Prompt-Based Large Language Models and Beyond", "author": "Xinyu Wang and Hainiu Xu and Lin Gui and Yulan He", "abstract": "  Task embedding, a meta-learning technique that captures task-specific\ninformation, has gained popularity, especially in areas such as multi-task\nlearning, model editing, and interpretability. However, it faces challenges\nwith the emergence of prompt-guided Large Language Models (LLMs) operating in a\ngradient-free manner. Existing task embedding methods rely on fine-tuned,\ntask-specific language models, which hinders the adaptability of task\nembeddings across diverse models, especially prompt-based LLMs. To hardness the\npotential of task embeddings in the era of LLMs, we propose a framework for\nunified task embeddings (FUTE), harmonizing task embeddings from various\nmodels, including smaller language models and LLMs with varied prompts, within\na single vector space. Such uniformity enables comparison and analysis of\nsimilarities amongst different models, broadening the scope and utility of\nexisting task embedding methods in multi-model scenarios, while maintaining\ntheir performance comparable to architecture-specific methods.\n", "link": "http://arxiv.org/abs/2402.14522v2", "date": "2024-07-12", "relevancy": 2.029, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5368}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5172}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Unified%20Task%20Embeddings%20Across%20Multiple%20Models%3A%20Bridging%20the%20Gap%0A%20%20for%20Prompt-Based%20Large%20Language%20Models%20and%20Beyond&body=Title%3A%20Towards%20Unified%20Task%20Embeddings%20Across%20Multiple%20Models%3A%20Bridging%20the%20Gap%0A%20%20for%20Prompt-Based%20Large%20Language%20Models%20and%20Beyond%0AAuthor%3A%20Xinyu%20Wang%20and%20Hainiu%20Xu%20and%20Lin%20Gui%20and%20Yulan%20He%0AAbstract%3A%20%20%20Task%20embedding%2C%20a%20meta-learning%20technique%20that%20captures%20task-specific%0Ainformation%2C%20has%20gained%20popularity%2C%20especially%20in%20areas%20such%20as%20multi-task%0Alearning%2C%20model%20editing%2C%20and%20interpretability.%20However%2C%20it%20faces%20challenges%0Awith%20the%20emergence%20of%20prompt-guided%20Large%20Language%20Models%20%28LLMs%29%20operating%20in%20a%0Agradient-free%20manner.%20Existing%20task%20embedding%20methods%20rely%20on%20fine-tuned%2C%0Atask-specific%20language%20models%2C%20which%20hinders%20the%20adaptability%20of%20task%0Aembeddings%20across%20diverse%20models%2C%20especially%20prompt-based%20LLMs.%20To%20hardness%20the%0Apotential%20of%20task%20embeddings%20in%20the%20era%20of%20LLMs%2C%20we%20propose%20a%20framework%20for%0Aunified%20task%20embeddings%20%28FUTE%29%2C%20harmonizing%20task%20embeddings%20from%20various%0Amodels%2C%20including%20smaller%20language%20models%20and%20LLMs%20with%20varied%20prompts%2C%20within%0Aa%20single%20vector%20space.%20Such%20uniformity%20enables%20comparison%20and%20analysis%20of%0Asimilarities%20amongst%20different%20models%2C%20broadening%20the%20scope%20and%20utility%20of%0Aexisting%20task%20embedding%20methods%20in%20multi-model%20scenarios%2C%20while%20maintaining%0Atheir%20performance%20comparable%20to%20architecture-specific%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Unified%2520Task%2520Embeddings%2520Across%2520Multiple%2520Models%253A%2520Bridging%2520the%2520Gap%250A%2520%2520for%2520Prompt-Based%2520Large%2520Language%2520Models%2520and%2520Beyond%26entry.906535625%3DXinyu%2520Wang%2520and%2520Hainiu%2520Xu%2520and%2520Lin%2520Gui%2520and%2520Yulan%2520He%26entry.1292438233%3D%2520%2520Task%2520embedding%252C%2520a%2520meta-learning%2520technique%2520that%2520captures%2520task-specific%250Ainformation%252C%2520has%2520gained%2520popularity%252C%2520especially%2520in%2520areas%2520such%2520as%2520multi-task%250Alearning%252C%2520model%2520editing%252C%2520and%2520interpretability.%2520However%252C%2520it%2520faces%2520challenges%250Awith%2520the%2520emergence%2520of%2520prompt-guided%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520operating%2520in%2520a%250Agradient-free%2520manner.%2520Existing%2520task%2520embedding%2520methods%2520rely%2520on%2520fine-tuned%252C%250Atask-specific%2520language%2520models%252C%2520which%2520hinders%2520the%2520adaptability%2520of%2520task%250Aembeddings%2520across%2520diverse%2520models%252C%2520especially%2520prompt-based%2520LLMs.%2520To%2520hardness%2520the%250Apotential%2520of%2520task%2520embeddings%2520in%2520the%2520era%2520of%2520LLMs%252C%2520we%2520propose%2520a%2520framework%2520for%250Aunified%2520task%2520embeddings%2520%2528FUTE%2529%252C%2520harmonizing%2520task%2520embeddings%2520from%2520various%250Amodels%252C%2520including%2520smaller%2520language%2520models%2520and%2520LLMs%2520with%2520varied%2520prompts%252C%2520within%250Aa%2520single%2520vector%2520space.%2520Such%2520uniformity%2520enables%2520comparison%2520and%2520analysis%2520of%250Asimilarities%2520amongst%2520different%2520models%252C%2520broadening%2520the%2520scope%2520and%2520utility%2520of%250Aexisting%2520task%2520embedding%2520methods%2520in%2520multi-model%2520scenarios%252C%2520while%2520maintaining%250Atheir%2520performance%2520comparable%2520to%2520architecture-specific%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unified%20Task%20Embeddings%20Across%20Multiple%20Models%3A%20Bridging%20the%20Gap%0A%20%20for%20Prompt-Based%20Large%20Language%20Models%20and%20Beyond&entry.906535625=Xinyu%20Wang%20and%20Hainiu%20Xu%20and%20Lin%20Gui%20and%20Yulan%20He&entry.1292438233=%20%20Task%20embedding%2C%20a%20meta-learning%20technique%20that%20captures%20task-specific%0Ainformation%2C%20has%20gained%20popularity%2C%20especially%20in%20areas%20such%20as%20multi-task%0Alearning%2C%20model%20editing%2C%20and%20interpretability.%20However%2C%20it%20faces%20challenges%0Awith%20the%20emergence%20of%20prompt-guided%20Large%20Language%20Models%20%28LLMs%29%20operating%20in%20a%0Agradient-free%20manner.%20Existing%20task%20embedding%20methods%20rely%20on%20fine-tuned%2C%0Atask-specific%20language%20models%2C%20which%20hinders%20the%20adaptability%20of%20task%0Aembeddings%20across%20diverse%20models%2C%20especially%20prompt-based%20LLMs.%20To%20hardness%20the%0Apotential%20of%20task%20embeddings%20in%20the%20era%20of%20LLMs%2C%20we%20propose%20a%20framework%20for%0Aunified%20task%20embeddings%20%28FUTE%29%2C%20harmonizing%20task%20embeddings%20from%20various%0Amodels%2C%20including%20smaller%20language%20models%20and%20LLMs%20with%20varied%20prompts%2C%20within%0Aa%20single%20vector%20space.%20Such%20uniformity%20enables%20comparison%20and%20analysis%20of%0Asimilarities%20amongst%20different%20models%2C%20broadening%20the%20scope%20and%20utility%20of%0Aexisting%20task%20embedding%20methods%20in%20multi-model%20scenarios%2C%20while%20maintaining%0Atheir%20performance%20comparable%20to%20architecture-specific%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14522v2&entry.124074799=Read"},
{"title": "ConRebSeg: A Segmentation Dataset for Reinforced Concrete Construction", "author": "Patrick Schmidt and Lazaros Nalpantidis", "abstract": "  The construction industry has been traditionally slow in adopting digital\ntechnologies. However, these are becoming increasingly necessary due to a\nplentitude of challenges, such as a shortage of skilled labor and decreasing\nproductivity levels compared to other industries. Autonomous robotic systems\ncan alleviate this problem, but the software development process for these\nsystems is heavily driven by data, a resource usually challenging to find in\nthe construction domain due to the lack of public availability. In our work, we\ntherefore provide a dataset of 14,805 RGB images with segmentation labels for\nreinforced concrete construction and make it publicly available. We conduct a\ndetailed analysis of our dataset and discuss how to deal with labeling\ninconsistencies. Furthermore, we establish baselines for the YOLOv8L-seg,\nDeepLabV3, and U-Net segmentation models and investigate the influence of data\navailability and label inconsistencies on the performance of these models. Our\nstudy showed that the models are precise in their predictions but would benefit\nfrom more data to increase the number of recalled instances. Label\ninconsistencies had a negligible effect on model performance, and we,\ntherefore, advocate for a crowd-sourced dataset to boost the development of\nautonomous robotic systems in the construction industry.\n", "link": "http://arxiv.org/abs/2407.09372v1", "date": "2024-07-12", "relevancy": 2.0277, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConRebSeg%3A%20A%20Segmentation%20Dataset%20for%20Reinforced%20Concrete%20Construction&body=Title%3A%20ConRebSeg%3A%20A%20Segmentation%20Dataset%20for%20Reinforced%20Concrete%20Construction%0AAuthor%3A%20Patrick%20Schmidt%20and%20Lazaros%20Nalpantidis%0AAbstract%3A%20%20%20The%20construction%20industry%20has%20been%20traditionally%20slow%20in%20adopting%20digital%0Atechnologies.%20However%2C%20these%20are%20becoming%20increasingly%20necessary%20due%20to%20a%0Aplentitude%20of%20challenges%2C%20such%20as%20a%20shortage%20of%20skilled%20labor%20and%20decreasing%0Aproductivity%20levels%20compared%20to%20other%20industries.%20Autonomous%20robotic%20systems%0Acan%20alleviate%20this%20problem%2C%20but%20the%20software%20development%20process%20for%20these%0Asystems%20is%20heavily%20driven%20by%20data%2C%20a%20resource%20usually%20challenging%20to%20find%20in%0Athe%20construction%20domain%20due%20to%20the%20lack%20of%20public%20availability.%20In%20our%20work%2C%20we%0Atherefore%20provide%20a%20dataset%20of%2014%2C805%20RGB%20images%20with%20segmentation%20labels%20for%0Areinforced%20concrete%20construction%20and%20make%20it%20publicly%20available.%20We%20conduct%20a%0Adetailed%20analysis%20of%20our%20dataset%20and%20discuss%20how%20to%20deal%20with%20labeling%0Ainconsistencies.%20Furthermore%2C%20we%20establish%20baselines%20for%20the%20YOLOv8L-seg%2C%0ADeepLabV3%2C%20and%20U-Net%20segmentation%20models%20and%20investigate%20the%20influence%20of%20data%0Aavailability%20and%20label%20inconsistencies%20on%20the%20performance%20of%20these%20models.%20Our%0Astudy%20showed%20that%20the%20models%20are%20precise%20in%20their%20predictions%20but%20would%20benefit%0Afrom%20more%20data%20to%20increase%20the%20number%20of%20recalled%20instances.%20Label%0Ainconsistencies%20had%20a%20negligible%20effect%20on%20model%20performance%2C%20and%20we%2C%0Atherefore%2C%20advocate%20for%20a%20crowd-sourced%20dataset%20to%20boost%20the%20development%20of%0Aautonomous%20robotic%20systems%20in%20the%20construction%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConRebSeg%253A%2520A%2520Segmentation%2520Dataset%2520for%2520Reinforced%2520Concrete%2520Construction%26entry.906535625%3DPatrick%2520Schmidt%2520and%2520Lazaros%2520Nalpantidis%26entry.1292438233%3D%2520%2520The%2520construction%2520industry%2520has%2520been%2520traditionally%2520slow%2520in%2520adopting%2520digital%250Atechnologies.%2520However%252C%2520these%2520are%2520becoming%2520increasingly%2520necessary%2520due%2520to%2520a%250Aplentitude%2520of%2520challenges%252C%2520such%2520as%2520a%2520shortage%2520of%2520skilled%2520labor%2520and%2520decreasing%250Aproductivity%2520levels%2520compared%2520to%2520other%2520industries.%2520Autonomous%2520robotic%2520systems%250Acan%2520alleviate%2520this%2520problem%252C%2520but%2520the%2520software%2520development%2520process%2520for%2520these%250Asystems%2520is%2520heavily%2520driven%2520by%2520data%252C%2520a%2520resource%2520usually%2520challenging%2520to%2520find%2520in%250Athe%2520construction%2520domain%2520due%2520to%2520the%2520lack%2520of%2520public%2520availability.%2520In%2520our%2520work%252C%2520we%250Atherefore%2520provide%2520a%2520dataset%2520of%252014%252C805%2520RGB%2520images%2520with%2520segmentation%2520labels%2520for%250Areinforced%2520concrete%2520construction%2520and%2520make%2520it%2520publicly%2520available.%2520We%2520conduct%2520a%250Adetailed%2520analysis%2520of%2520our%2520dataset%2520and%2520discuss%2520how%2520to%2520deal%2520with%2520labeling%250Ainconsistencies.%2520Furthermore%252C%2520we%2520establish%2520baselines%2520for%2520the%2520YOLOv8L-seg%252C%250ADeepLabV3%252C%2520and%2520U-Net%2520segmentation%2520models%2520and%2520investigate%2520the%2520influence%2520of%2520data%250Aavailability%2520and%2520label%2520inconsistencies%2520on%2520the%2520performance%2520of%2520these%2520models.%2520Our%250Astudy%2520showed%2520that%2520the%2520models%2520are%2520precise%2520in%2520their%2520predictions%2520but%2520would%2520benefit%250Afrom%2520more%2520data%2520to%2520increase%2520the%2520number%2520of%2520recalled%2520instances.%2520Label%250Ainconsistencies%2520had%2520a%2520negligible%2520effect%2520on%2520model%2520performance%252C%2520and%2520we%252C%250Atherefore%252C%2520advocate%2520for%2520a%2520crowd-sourced%2520dataset%2520to%2520boost%2520the%2520development%2520of%250Aautonomous%2520robotic%2520systems%2520in%2520the%2520construction%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConRebSeg%3A%20A%20Segmentation%20Dataset%20for%20Reinforced%20Concrete%20Construction&entry.906535625=Patrick%20Schmidt%20and%20Lazaros%20Nalpantidis&entry.1292438233=%20%20The%20construction%20industry%20has%20been%20traditionally%20slow%20in%20adopting%20digital%0Atechnologies.%20However%2C%20these%20are%20becoming%20increasingly%20necessary%20due%20to%20a%0Aplentitude%20of%20challenges%2C%20such%20as%20a%20shortage%20of%20skilled%20labor%20and%20decreasing%0Aproductivity%20levels%20compared%20to%20other%20industries.%20Autonomous%20robotic%20systems%0Acan%20alleviate%20this%20problem%2C%20but%20the%20software%20development%20process%20for%20these%0Asystems%20is%20heavily%20driven%20by%20data%2C%20a%20resource%20usually%20challenging%20to%20find%20in%0Athe%20construction%20domain%20due%20to%20the%20lack%20of%20public%20availability.%20In%20our%20work%2C%20we%0Atherefore%20provide%20a%20dataset%20of%2014%2C805%20RGB%20images%20with%20segmentation%20labels%20for%0Areinforced%20concrete%20construction%20and%20make%20it%20publicly%20available.%20We%20conduct%20a%0Adetailed%20analysis%20of%20our%20dataset%20and%20discuss%20how%20to%20deal%20with%20labeling%0Ainconsistencies.%20Furthermore%2C%20we%20establish%20baselines%20for%20the%20YOLOv8L-seg%2C%0ADeepLabV3%2C%20and%20U-Net%20segmentation%20models%20and%20investigate%20the%20influence%20of%20data%0Aavailability%20and%20label%20inconsistencies%20on%20the%20performance%20of%20these%20models.%20Our%0Astudy%20showed%20that%20the%20models%20are%20precise%20in%20their%20predictions%20but%20would%20benefit%0Afrom%20more%20data%20to%20increase%20the%20number%20of%20recalled%20instances.%20Label%0Ainconsistencies%20had%20a%20negligible%20effect%20on%20model%20performance%2C%20and%20we%2C%0Atherefore%2C%20advocate%20for%20a%20crowd-sourced%20dataset%20to%20boost%20the%20development%20of%0Aautonomous%20robotic%20systems%20in%20the%20construction%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09372v1&entry.124074799=Read"},
{"title": "Structural Design Through Reinforcement Learning", "author": "Thomas Rochefort-Beaudoin and Aurelian Vadean and Niels Aage and Sofiane Achiche", "abstract": "  This paper introduces the Structural Optimization gym (SOgym), a novel\nopen-source Reinforcement Learning (RL) environment designed to advance machine\nlearning in Topology Optimization (TO). SOgym enables RL agents to generate\nphysically viable and structurally robust designs by integrating the physics of\nTO into the reward function. To enhance scalability, SOgym leverages\nfeature-mapping methods as a mesh-independent interface between the environment\nand the agent, allowing efficient interaction with the design variables\nregardless of mesh resolution. Baseline results use a model-free Proximal\nPolicy Optimization agent and a model-based DreamerV3 agent. Three observation\nspace configurations were tested. The TopOpt game-inspired configuration, an\ninteractive educational tool that improves students' intuition in designing\nstructures to minimize compliance under volume constraints, performed best in\nterms of performance and sample efficiency. The 100M parameter version of\nDreamerV3 produced structures within 54% of the baseline compliance achieved by\ntraditional optimization methods and a 0% disconnection rate, an improvement\nover supervised learning approaches that often struggle with disconnected load\npaths. When comparing the learning rates of the agents to those of engineering\nstudents from the TopOpt game experiment, the DreamerV3-100M model shows a\nlearning rate approximately four orders of magnitude lower, an impressive feat\nfor a policy trained from scratch through trial and error. These results\nsuggest RL's potential to solve continuous TO problems and its capacity to\nexplore and learn from diverse design solutions. SOgym provides a platform for\ndeveloping RL agents for complex structural design challenges and is publicly\navailable to support further research in the field.\n", "link": "http://arxiv.org/abs/2407.07288v2", "date": "2024-07-12", "relevancy": 2.0262, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5533}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.499}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Design%20Through%20Reinforcement%20Learning&body=Title%3A%20Structural%20Design%20Through%20Reinforcement%20Learning%0AAuthor%3A%20Thomas%20Rochefort-Beaudoin%20and%20Aurelian%20Vadean%20and%20Niels%20Aage%20and%20Sofiane%20Achiche%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20Structural%20Optimization%20gym%20%28SOgym%29%2C%20a%20novel%0Aopen-source%20Reinforcement%20Learning%20%28RL%29%20environment%20designed%20to%20advance%20machine%0Alearning%20in%20Topology%20Optimization%20%28TO%29.%20SOgym%20enables%20RL%20agents%20to%20generate%0Aphysically%20viable%20and%20structurally%20robust%20designs%20by%20integrating%20the%20physics%20of%0ATO%20into%20the%20reward%20function.%20To%20enhance%20scalability%2C%20SOgym%20leverages%0Afeature-mapping%20methods%20as%20a%20mesh-independent%20interface%20between%20the%20environment%0Aand%20the%20agent%2C%20allowing%20efficient%20interaction%20with%20the%20design%20variables%0Aregardless%20of%20mesh%20resolution.%20Baseline%20results%20use%20a%20model-free%20Proximal%0APolicy%20Optimization%20agent%20and%20a%20model-based%20DreamerV3%20agent.%20Three%20observation%0Aspace%20configurations%20were%20tested.%20The%20TopOpt%20game-inspired%20configuration%2C%20an%0Ainteractive%20educational%20tool%20that%20improves%20students%27%20intuition%20in%20designing%0Astructures%20to%20minimize%20compliance%20under%20volume%20constraints%2C%20performed%20best%20in%0Aterms%20of%20performance%20and%20sample%20efficiency.%20The%20100M%20parameter%20version%20of%0ADreamerV3%20produced%20structures%20within%2054%25%20of%20the%20baseline%20compliance%20achieved%20by%0Atraditional%20optimization%20methods%20and%20a%200%25%20disconnection%20rate%2C%20an%20improvement%0Aover%20supervised%20learning%20approaches%20that%20often%20struggle%20with%20disconnected%20load%0Apaths.%20When%20comparing%20the%20learning%20rates%20of%20the%20agents%20to%20those%20of%20engineering%0Astudents%20from%20the%20TopOpt%20game%20experiment%2C%20the%20DreamerV3-100M%20model%20shows%20a%0Alearning%20rate%20approximately%20four%20orders%20of%20magnitude%20lower%2C%20an%20impressive%20feat%0Afor%20a%20policy%20trained%20from%20scratch%20through%20trial%20and%20error.%20These%20results%0Asuggest%20RL%27s%20potential%20to%20solve%20continuous%20TO%20problems%20and%20its%20capacity%20to%0Aexplore%20and%20learn%20from%20diverse%20design%20solutions.%20SOgym%20provides%20a%20platform%20for%0Adeveloping%20RL%20agents%20for%20complex%20structural%20design%20challenges%20and%20is%20publicly%0Aavailable%20to%20support%20further%20research%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Design%2520Through%2520Reinforcement%2520Learning%26entry.906535625%3DThomas%2520Rochefort-Beaudoin%2520and%2520Aurelian%2520Vadean%2520and%2520Niels%2520Aage%2520and%2520Sofiane%2520Achiche%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520Structural%2520Optimization%2520gym%2520%2528SOgym%2529%252C%2520a%2520novel%250Aopen-source%2520Reinforcement%2520Learning%2520%2528RL%2529%2520environment%2520designed%2520to%2520advance%2520machine%250Alearning%2520in%2520Topology%2520Optimization%2520%2528TO%2529.%2520SOgym%2520enables%2520RL%2520agents%2520to%2520generate%250Aphysically%2520viable%2520and%2520structurally%2520robust%2520designs%2520by%2520integrating%2520the%2520physics%2520of%250ATO%2520into%2520the%2520reward%2520function.%2520To%2520enhance%2520scalability%252C%2520SOgym%2520leverages%250Afeature-mapping%2520methods%2520as%2520a%2520mesh-independent%2520interface%2520between%2520the%2520environment%250Aand%2520the%2520agent%252C%2520allowing%2520efficient%2520interaction%2520with%2520the%2520design%2520variables%250Aregardless%2520of%2520mesh%2520resolution.%2520Baseline%2520results%2520use%2520a%2520model-free%2520Proximal%250APolicy%2520Optimization%2520agent%2520and%2520a%2520model-based%2520DreamerV3%2520agent.%2520Three%2520observation%250Aspace%2520configurations%2520were%2520tested.%2520The%2520TopOpt%2520game-inspired%2520configuration%252C%2520an%250Ainteractive%2520educational%2520tool%2520that%2520improves%2520students%2527%2520intuition%2520in%2520designing%250Astructures%2520to%2520minimize%2520compliance%2520under%2520volume%2520constraints%252C%2520performed%2520best%2520in%250Aterms%2520of%2520performance%2520and%2520sample%2520efficiency.%2520The%2520100M%2520parameter%2520version%2520of%250ADreamerV3%2520produced%2520structures%2520within%252054%2525%2520of%2520the%2520baseline%2520compliance%2520achieved%2520by%250Atraditional%2520optimization%2520methods%2520and%2520a%25200%2525%2520disconnection%2520rate%252C%2520an%2520improvement%250Aover%2520supervised%2520learning%2520approaches%2520that%2520often%2520struggle%2520with%2520disconnected%2520load%250Apaths.%2520When%2520comparing%2520the%2520learning%2520rates%2520of%2520the%2520agents%2520to%2520those%2520of%2520engineering%250Astudents%2520from%2520the%2520TopOpt%2520game%2520experiment%252C%2520the%2520DreamerV3-100M%2520model%2520shows%2520a%250Alearning%2520rate%2520approximately%2520four%2520orders%2520of%2520magnitude%2520lower%252C%2520an%2520impressive%2520feat%250Afor%2520a%2520policy%2520trained%2520from%2520scratch%2520through%2520trial%2520and%2520error.%2520These%2520results%250Asuggest%2520RL%2527s%2520potential%2520to%2520solve%2520continuous%2520TO%2520problems%2520and%2520its%2520capacity%2520to%250Aexplore%2520and%2520learn%2520from%2520diverse%2520design%2520solutions.%2520SOgym%2520provides%2520a%2520platform%2520for%250Adeveloping%2520RL%2520agents%2520for%2520complex%2520structural%2520design%2520challenges%2520and%2520is%2520publicly%250Aavailable%2520to%2520support%2520further%2520research%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Design%20Through%20Reinforcement%20Learning&entry.906535625=Thomas%20Rochefort-Beaudoin%20and%20Aurelian%20Vadean%20and%20Niels%20Aage%20and%20Sofiane%20Achiche&entry.1292438233=%20%20This%20paper%20introduces%20the%20Structural%20Optimization%20gym%20%28SOgym%29%2C%20a%20novel%0Aopen-source%20Reinforcement%20Learning%20%28RL%29%20environment%20designed%20to%20advance%20machine%0Alearning%20in%20Topology%20Optimization%20%28TO%29.%20SOgym%20enables%20RL%20agents%20to%20generate%0Aphysically%20viable%20and%20structurally%20robust%20designs%20by%20integrating%20the%20physics%20of%0ATO%20into%20the%20reward%20function.%20To%20enhance%20scalability%2C%20SOgym%20leverages%0Afeature-mapping%20methods%20as%20a%20mesh-independent%20interface%20between%20the%20environment%0Aand%20the%20agent%2C%20allowing%20efficient%20interaction%20with%20the%20design%20variables%0Aregardless%20of%20mesh%20resolution.%20Baseline%20results%20use%20a%20model-free%20Proximal%0APolicy%20Optimization%20agent%20and%20a%20model-based%20DreamerV3%20agent.%20Three%20observation%0Aspace%20configurations%20were%20tested.%20The%20TopOpt%20game-inspired%20configuration%2C%20an%0Ainteractive%20educational%20tool%20that%20improves%20students%27%20intuition%20in%20designing%0Astructures%20to%20minimize%20compliance%20under%20volume%20constraints%2C%20performed%20best%20in%0Aterms%20of%20performance%20and%20sample%20efficiency.%20The%20100M%20parameter%20version%20of%0ADreamerV3%20produced%20structures%20within%2054%25%20of%20the%20baseline%20compliance%20achieved%20by%0Atraditional%20optimization%20methods%20and%20a%200%25%20disconnection%20rate%2C%20an%20improvement%0Aover%20supervised%20learning%20approaches%20that%20often%20struggle%20with%20disconnected%20load%0Apaths.%20When%20comparing%20the%20learning%20rates%20of%20the%20agents%20to%20those%20of%20engineering%0Astudents%20from%20the%20TopOpt%20game%20experiment%2C%20the%20DreamerV3-100M%20model%20shows%20a%0Alearning%20rate%20approximately%20four%20orders%20of%20magnitude%20lower%2C%20an%20impressive%20feat%0Afor%20a%20policy%20trained%20from%20scratch%20through%20trial%20and%20error.%20These%20results%0Asuggest%20RL%27s%20potential%20to%20solve%20continuous%20TO%20problems%20and%20its%20capacity%20to%0Aexplore%20and%20learn%20from%20diverse%20design%20solutions.%20SOgym%20provides%20a%20platform%20for%0Adeveloping%20RL%20agents%20for%20complex%20structural%20design%20challenges%20and%20is%20publicly%0Aavailable%20to%20support%20further%20research%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07288v2&entry.124074799=Read"},
{"title": "Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators", "author": "Paolo D'Alberto and Taehee Jeong and Akshai Jain and Shreyas Manjunath and Mrinal Sarmah and Samuel Hsu Yaswanth Raparti and Nitesh Pipralia", "abstract": "  Nowadays, increasingly larger Deep Neural Networks (DNNs) are being\ndeveloped, trained, and utilized. These networks require significant\ncomputational resources, putting a strain on both advanced and limited devices.\nOur solution is to implement {\\em weight block sparsity}, which is a structured\nsparsity that is friendly to hardware. By zeroing certain sections of the\nconvolution and fully connected layers parameters of pre-trained DNN models, we\ncan efficiently speed up the DNN's inference process. This results in a smaller\nmemory footprint, faster communication, and fewer operations.\n  Our work presents a vertical system that allows for the training of\nconvolution and matrix multiplication weights to exploit 8x8 block sparsity on\na single GPU within a reasonable amount of time. Compilers recognize this\nsparsity and use it for both data compaction and computation splitting into\nthreads. Blocks like these take full advantage of both spatial and temporal\nlocality, paving the way for fast vector operations and memory reuse. By using\nthis system on a Resnet50 model, we were able to reduce the weight by half with\nminimal accuracy loss, resulting in a two-times faster inference speed. We will\npresent performance estimates using accurate and complete code generation for\nAIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, and\nVGG16 to demonstrate the necessary synergy between hardware overlay designs and\nsoftware stacks for compiling and executing machine learning applications.\n", "link": "http://arxiv.org/abs/2407.09453v1", "date": "2024-07-12", "relevancy": 2.0227, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5277}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.52}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weight%20Block%20Sparsity%3A%20Training%2C%20Compilation%2C%20and%20AI%20Engine%20Accelerators&body=Title%3A%20Weight%20Block%20Sparsity%3A%20Training%2C%20Compilation%2C%20and%20AI%20Engine%20Accelerators%0AAuthor%3A%20Paolo%20D%27Alberto%20and%20Taehee%20Jeong%20and%20Akshai%20Jain%20and%20Shreyas%20Manjunath%20and%20Mrinal%20Sarmah%20and%20Samuel%20Hsu%20Yaswanth%20Raparti%20and%20Nitesh%20Pipralia%0AAbstract%3A%20%20%20Nowadays%2C%20increasingly%20larger%20Deep%20Neural%20Networks%20%28DNNs%29%20are%20being%0Adeveloped%2C%20trained%2C%20and%20utilized.%20These%20networks%20require%20significant%0Acomputational%20resources%2C%20putting%20a%20strain%20on%20both%20advanced%20and%20limited%20devices.%0AOur%20solution%20is%20to%20implement%20%7B%5Cem%20weight%20block%20sparsity%7D%2C%20which%20is%20a%20structured%0Asparsity%20that%20is%20friendly%20to%20hardware.%20By%20zeroing%20certain%20sections%20of%20the%0Aconvolution%20and%20fully%20connected%20layers%20parameters%20of%20pre-trained%20DNN%20models%2C%20we%0Acan%20efficiently%20speed%20up%20the%20DNN%27s%20inference%20process.%20This%20results%20in%20a%20smaller%0Amemory%20footprint%2C%20faster%20communication%2C%20and%20fewer%20operations.%0A%20%20Our%20work%20presents%20a%20vertical%20system%20that%20allows%20for%20the%20training%20of%0Aconvolution%20and%20matrix%20multiplication%20weights%20to%20exploit%208x8%20block%20sparsity%20on%0Aa%20single%20GPU%20within%20a%20reasonable%20amount%20of%20time.%20Compilers%20recognize%20this%0Asparsity%20and%20use%20it%20for%20both%20data%20compaction%20and%20computation%20splitting%20into%0Athreads.%20Blocks%20like%20these%20take%20full%20advantage%20of%20both%20spatial%20and%20temporal%0Alocality%2C%20paving%20the%20way%20for%20fast%20vector%20operations%20and%20memory%20reuse.%20By%20using%0Athis%20system%20on%20a%20Resnet50%20model%2C%20we%20were%20able%20to%20reduce%20the%20weight%20by%20half%20with%0Aminimal%20accuracy%20loss%2C%20resulting%20in%20a%20two-times%20faster%20inference%20speed.%20We%20will%0Apresent%20performance%20estimates%20using%20accurate%20and%20complete%20code%20generation%20for%0AAIE2%20configuration%20sets%20%28AMD%20Versal%20FPGAs%29%20with%20Resnet50%2C%20Inception%20V3%2C%20and%0AVGG16%20to%20demonstrate%20the%20necessary%20synergy%20between%20hardware%20overlay%20designs%20and%0Asoftware%20stacks%20for%20compiling%20and%20executing%20machine%20learning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeight%2520Block%2520Sparsity%253A%2520Training%252C%2520Compilation%252C%2520and%2520AI%2520Engine%2520Accelerators%26entry.906535625%3DPaolo%2520D%2527Alberto%2520and%2520Taehee%2520Jeong%2520and%2520Akshai%2520Jain%2520and%2520Shreyas%2520Manjunath%2520and%2520Mrinal%2520Sarmah%2520and%2520Samuel%2520Hsu%2520Yaswanth%2520Raparti%2520and%2520Nitesh%2520Pipralia%26entry.1292438233%3D%2520%2520Nowadays%252C%2520increasingly%2520larger%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520are%2520being%250Adeveloped%252C%2520trained%252C%2520and%2520utilized.%2520These%2520networks%2520require%2520significant%250Acomputational%2520resources%252C%2520putting%2520a%2520strain%2520on%2520both%2520advanced%2520and%2520limited%2520devices.%250AOur%2520solution%2520is%2520to%2520implement%2520%257B%255Cem%2520weight%2520block%2520sparsity%257D%252C%2520which%2520is%2520a%2520structured%250Asparsity%2520that%2520is%2520friendly%2520to%2520hardware.%2520By%2520zeroing%2520certain%2520sections%2520of%2520the%250Aconvolution%2520and%2520fully%2520connected%2520layers%2520parameters%2520of%2520pre-trained%2520DNN%2520models%252C%2520we%250Acan%2520efficiently%2520speed%2520up%2520the%2520DNN%2527s%2520inference%2520process.%2520This%2520results%2520in%2520a%2520smaller%250Amemory%2520footprint%252C%2520faster%2520communication%252C%2520and%2520fewer%2520operations.%250A%2520%2520Our%2520work%2520presents%2520a%2520vertical%2520system%2520that%2520allows%2520for%2520the%2520training%2520of%250Aconvolution%2520and%2520matrix%2520multiplication%2520weights%2520to%2520exploit%25208x8%2520block%2520sparsity%2520on%250Aa%2520single%2520GPU%2520within%2520a%2520reasonable%2520amount%2520of%2520time.%2520Compilers%2520recognize%2520this%250Asparsity%2520and%2520use%2520it%2520for%2520both%2520data%2520compaction%2520and%2520computation%2520splitting%2520into%250Athreads.%2520Blocks%2520like%2520these%2520take%2520full%2520advantage%2520of%2520both%2520spatial%2520and%2520temporal%250Alocality%252C%2520paving%2520the%2520way%2520for%2520fast%2520vector%2520operations%2520and%2520memory%2520reuse.%2520By%2520using%250Athis%2520system%2520on%2520a%2520Resnet50%2520model%252C%2520we%2520were%2520able%2520to%2520reduce%2520the%2520weight%2520by%2520half%2520with%250Aminimal%2520accuracy%2520loss%252C%2520resulting%2520in%2520a%2520two-times%2520faster%2520inference%2520speed.%2520We%2520will%250Apresent%2520performance%2520estimates%2520using%2520accurate%2520and%2520complete%2520code%2520generation%2520for%250AAIE2%2520configuration%2520sets%2520%2528AMD%2520Versal%2520FPGAs%2529%2520with%2520Resnet50%252C%2520Inception%2520V3%252C%2520and%250AVGG16%2520to%2520demonstrate%2520the%2520necessary%2520synergy%2520between%2520hardware%2520overlay%2520designs%2520and%250Asoftware%2520stacks%2520for%2520compiling%2520and%2520executing%2520machine%2520learning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weight%20Block%20Sparsity%3A%20Training%2C%20Compilation%2C%20and%20AI%20Engine%20Accelerators&entry.906535625=Paolo%20D%27Alberto%20and%20Taehee%20Jeong%20and%20Akshai%20Jain%20and%20Shreyas%20Manjunath%20and%20Mrinal%20Sarmah%20and%20Samuel%20Hsu%20Yaswanth%20Raparti%20and%20Nitesh%20Pipralia&entry.1292438233=%20%20Nowadays%2C%20increasingly%20larger%20Deep%20Neural%20Networks%20%28DNNs%29%20are%20being%0Adeveloped%2C%20trained%2C%20and%20utilized.%20These%20networks%20require%20significant%0Acomputational%20resources%2C%20putting%20a%20strain%20on%20both%20advanced%20and%20limited%20devices.%0AOur%20solution%20is%20to%20implement%20%7B%5Cem%20weight%20block%20sparsity%7D%2C%20which%20is%20a%20structured%0Asparsity%20that%20is%20friendly%20to%20hardware.%20By%20zeroing%20certain%20sections%20of%20the%0Aconvolution%20and%20fully%20connected%20layers%20parameters%20of%20pre-trained%20DNN%20models%2C%20we%0Acan%20efficiently%20speed%20up%20the%20DNN%27s%20inference%20process.%20This%20results%20in%20a%20smaller%0Amemory%20footprint%2C%20faster%20communication%2C%20and%20fewer%20operations.%0A%20%20Our%20work%20presents%20a%20vertical%20system%20that%20allows%20for%20the%20training%20of%0Aconvolution%20and%20matrix%20multiplication%20weights%20to%20exploit%208x8%20block%20sparsity%20on%0Aa%20single%20GPU%20within%20a%20reasonable%20amount%20of%20time.%20Compilers%20recognize%20this%0Asparsity%20and%20use%20it%20for%20both%20data%20compaction%20and%20computation%20splitting%20into%0Athreads.%20Blocks%20like%20these%20take%20full%20advantage%20of%20both%20spatial%20and%20temporal%0Alocality%2C%20paving%20the%20way%20for%20fast%20vector%20operations%20and%20memory%20reuse.%20By%20using%0Athis%20system%20on%20a%20Resnet50%20model%2C%20we%20were%20able%20to%20reduce%20the%20weight%20by%20half%20with%0Aminimal%20accuracy%20loss%2C%20resulting%20in%20a%20two-times%20faster%20inference%20speed.%20We%20will%0Apresent%20performance%20estimates%20using%20accurate%20and%20complete%20code%20generation%20for%0AAIE2%20configuration%20sets%20%28AMD%20Versal%20FPGAs%29%20with%20Resnet50%2C%20Inception%20V3%2C%20and%0AVGG16%20to%20demonstrate%20the%20necessary%20synergy%20between%20hardware%20overlay%20designs%20and%0Asoftware%20stacks%20for%20compiling%20and%20executing%20machine%20learning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09453v1&entry.124074799=Read"},
{"title": "Guidelines for Augmentation Selection in Contrastive Learning for Time\n  Series Classification", "author": "Ziyu Liu and Azadeh Alavi and Minyi Li and Xiang Zhang", "abstract": "  Self-supervised contrastive learning has become a key technique in deep\nlearning, particularly in time series analysis, due to its ability to learn\nmeaningful representations without explicit supervision. Augmentation is a\ncritical component in contrastive learning, where different augmentations can\ndramatically impact performance, sometimes influencing accuracy by over 30%.\nHowever, the selection of augmentations is predominantly empirical which can be\nsuboptimal, or grid searching that is time-consuming. In this paper, we\nestablish a principled framework for selecting augmentations based on dataset\ncharacteristics such as trend and seasonality. Specifically, we construct 12\nsynthetic datasets incorporating trend, seasonality, and integration weights.\nWe then evaluate the effectiveness of 8 different augmentations across these\nsynthetic datasets, thereby inducing generalizable associations between time\nseries characteristics and augmentation efficiency. Additionally, we evaluated\nthe induced associations across 6 real-world datasets encompassing domains such\nas activity recognition, disease diagnosis, traffic monitoring, electricity\nusage, mechanical fault prognosis, and finance. These real-world datasets are\ndiverse, covering a range from 1 to 12 channels, 2 to 10 classes, sequence\nlengths of 14 to 1280, and data frequencies from 250 Hz to daily intervals. The\nexperimental results show that our proposed trend-seasonality-based\naugmentation recommendation algorithm can accurately identify the effective\naugmentations for a given time series dataset, achieving an average Recall@3 of\n0.667, outperforming baselines. Our work provides guidance for studies\nemploying contrastive learning in time series analysis, with wide-ranging\napplications. All the code, datasets, and analysis results will be released at\nhttps://github.com/DL4mHealth/TS-Contrastive-Augmentation-Recommendation.\n", "link": "http://arxiv.org/abs/2407.09336v1", "date": "2024-07-12", "relevancy": 2.019, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5098}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5066}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guidelines%20for%20Augmentation%20Selection%20in%20Contrastive%20Learning%20for%20Time%0A%20%20Series%20Classification&body=Title%3A%20Guidelines%20for%20Augmentation%20Selection%20in%20Contrastive%20Learning%20for%20Time%0A%20%20Series%20Classification%0AAuthor%3A%20Ziyu%20Liu%20and%20Azadeh%20Alavi%20and%20Minyi%20Li%20and%20Xiang%20Zhang%0AAbstract%3A%20%20%20Self-supervised%20contrastive%20learning%20has%20become%20a%20key%20technique%20in%20deep%0Alearning%2C%20particularly%20in%20time%20series%20analysis%2C%20due%20to%20its%20ability%20to%20learn%0Ameaningful%20representations%20without%20explicit%20supervision.%20Augmentation%20is%20a%0Acritical%20component%20in%20contrastive%20learning%2C%20where%20different%20augmentations%20can%0Adramatically%20impact%20performance%2C%20sometimes%20influencing%20accuracy%20by%20over%2030%25.%0AHowever%2C%20the%20selection%20of%20augmentations%20is%20predominantly%20empirical%20which%20can%20be%0Asuboptimal%2C%20or%20grid%20searching%20that%20is%20time-consuming.%20In%20this%20paper%2C%20we%0Aestablish%20a%20principled%20framework%20for%20selecting%20augmentations%20based%20on%20dataset%0Acharacteristics%20such%20as%20trend%20and%20seasonality.%20Specifically%2C%20we%20construct%2012%0Asynthetic%20datasets%20incorporating%20trend%2C%20seasonality%2C%20and%20integration%20weights.%0AWe%20then%20evaluate%20the%20effectiveness%20of%208%20different%20augmentations%20across%20these%0Asynthetic%20datasets%2C%20thereby%20inducing%20generalizable%20associations%20between%20time%0Aseries%20characteristics%20and%20augmentation%20efficiency.%20Additionally%2C%20we%20evaluated%0Athe%20induced%20associations%20across%206%20real-world%20datasets%20encompassing%20domains%20such%0Aas%20activity%20recognition%2C%20disease%20diagnosis%2C%20traffic%20monitoring%2C%20electricity%0Ausage%2C%20mechanical%20fault%20prognosis%2C%20and%20finance.%20These%20real-world%20datasets%20are%0Adiverse%2C%20covering%20a%20range%20from%201%20to%2012%20channels%2C%202%20to%2010%20classes%2C%20sequence%0Alengths%20of%2014%20to%201280%2C%20and%20data%20frequencies%20from%20250%20Hz%20to%20daily%20intervals.%20The%0Aexperimental%20results%20show%20that%20our%20proposed%20trend-seasonality-based%0Aaugmentation%20recommendation%20algorithm%20can%20accurately%20identify%20the%20effective%0Aaugmentations%20for%20a%20given%20time%20series%20dataset%2C%20achieving%20an%20average%20Recall%403%20of%0A0.667%2C%20outperforming%20baselines.%20Our%20work%20provides%20guidance%20for%20studies%0Aemploying%20contrastive%20learning%20in%20time%20series%20analysis%2C%20with%20wide-ranging%0Aapplications.%20All%20the%20code%2C%20datasets%2C%20and%20analysis%20results%20will%20be%20released%20at%0Ahttps%3A//github.com/DL4mHealth/TS-Contrastive-Augmentation-Recommendation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuidelines%2520for%2520Augmentation%2520Selection%2520in%2520Contrastive%2520Learning%2520for%2520Time%250A%2520%2520Series%2520Classification%26entry.906535625%3DZiyu%2520Liu%2520and%2520Azadeh%2520Alavi%2520and%2520Minyi%2520Li%2520and%2520Xiang%2520Zhang%26entry.1292438233%3D%2520%2520Self-supervised%2520contrastive%2520learning%2520has%2520become%2520a%2520key%2520technique%2520in%2520deep%250Alearning%252C%2520particularly%2520in%2520time%2520series%2520analysis%252C%2520due%2520to%2520its%2520ability%2520to%2520learn%250Ameaningful%2520representations%2520without%2520explicit%2520supervision.%2520Augmentation%2520is%2520a%250Acritical%2520component%2520in%2520contrastive%2520learning%252C%2520where%2520different%2520augmentations%2520can%250Adramatically%2520impact%2520performance%252C%2520sometimes%2520influencing%2520accuracy%2520by%2520over%252030%2525.%250AHowever%252C%2520the%2520selection%2520of%2520augmentations%2520is%2520predominantly%2520empirical%2520which%2520can%2520be%250Asuboptimal%252C%2520or%2520grid%2520searching%2520that%2520is%2520time-consuming.%2520In%2520this%2520paper%252C%2520we%250Aestablish%2520a%2520principled%2520framework%2520for%2520selecting%2520augmentations%2520based%2520on%2520dataset%250Acharacteristics%2520such%2520as%2520trend%2520and%2520seasonality.%2520Specifically%252C%2520we%2520construct%252012%250Asynthetic%2520datasets%2520incorporating%2520trend%252C%2520seasonality%252C%2520and%2520integration%2520weights.%250AWe%2520then%2520evaluate%2520the%2520effectiveness%2520of%25208%2520different%2520augmentations%2520across%2520these%250Asynthetic%2520datasets%252C%2520thereby%2520inducing%2520generalizable%2520associations%2520between%2520time%250Aseries%2520characteristics%2520and%2520augmentation%2520efficiency.%2520Additionally%252C%2520we%2520evaluated%250Athe%2520induced%2520associations%2520across%25206%2520real-world%2520datasets%2520encompassing%2520domains%2520such%250Aas%2520activity%2520recognition%252C%2520disease%2520diagnosis%252C%2520traffic%2520monitoring%252C%2520electricity%250Ausage%252C%2520mechanical%2520fault%2520prognosis%252C%2520and%2520finance.%2520These%2520real-world%2520datasets%2520are%250Adiverse%252C%2520covering%2520a%2520range%2520from%25201%2520to%252012%2520channels%252C%25202%2520to%252010%2520classes%252C%2520sequence%250Alengths%2520of%252014%2520to%25201280%252C%2520and%2520data%2520frequencies%2520from%2520250%2520Hz%2520to%2520daily%2520intervals.%2520The%250Aexperimental%2520results%2520show%2520that%2520our%2520proposed%2520trend-seasonality-based%250Aaugmentation%2520recommendation%2520algorithm%2520can%2520accurately%2520identify%2520the%2520effective%250Aaugmentations%2520for%2520a%2520given%2520time%2520series%2520dataset%252C%2520achieving%2520an%2520average%2520Recall%25403%2520of%250A0.667%252C%2520outperforming%2520baselines.%2520Our%2520work%2520provides%2520guidance%2520for%2520studies%250Aemploying%2520contrastive%2520learning%2520in%2520time%2520series%2520analysis%252C%2520with%2520wide-ranging%250Aapplications.%2520All%2520the%2520code%252C%2520datasets%252C%2520and%2520analysis%2520results%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/DL4mHealth/TS-Contrastive-Augmentation-Recommendation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guidelines%20for%20Augmentation%20Selection%20in%20Contrastive%20Learning%20for%20Time%0A%20%20Series%20Classification&entry.906535625=Ziyu%20Liu%20and%20Azadeh%20Alavi%20and%20Minyi%20Li%20and%20Xiang%20Zhang&entry.1292438233=%20%20Self-supervised%20contrastive%20learning%20has%20become%20a%20key%20technique%20in%20deep%0Alearning%2C%20particularly%20in%20time%20series%20analysis%2C%20due%20to%20its%20ability%20to%20learn%0Ameaningful%20representations%20without%20explicit%20supervision.%20Augmentation%20is%20a%0Acritical%20component%20in%20contrastive%20learning%2C%20where%20different%20augmentations%20can%0Adramatically%20impact%20performance%2C%20sometimes%20influencing%20accuracy%20by%20over%2030%25.%0AHowever%2C%20the%20selection%20of%20augmentations%20is%20predominantly%20empirical%20which%20can%20be%0Asuboptimal%2C%20or%20grid%20searching%20that%20is%20time-consuming.%20In%20this%20paper%2C%20we%0Aestablish%20a%20principled%20framework%20for%20selecting%20augmentations%20based%20on%20dataset%0Acharacteristics%20such%20as%20trend%20and%20seasonality.%20Specifically%2C%20we%20construct%2012%0Asynthetic%20datasets%20incorporating%20trend%2C%20seasonality%2C%20and%20integration%20weights.%0AWe%20then%20evaluate%20the%20effectiveness%20of%208%20different%20augmentations%20across%20these%0Asynthetic%20datasets%2C%20thereby%20inducing%20generalizable%20associations%20between%20time%0Aseries%20characteristics%20and%20augmentation%20efficiency.%20Additionally%2C%20we%20evaluated%0Athe%20induced%20associations%20across%206%20real-world%20datasets%20encompassing%20domains%20such%0Aas%20activity%20recognition%2C%20disease%20diagnosis%2C%20traffic%20monitoring%2C%20electricity%0Ausage%2C%20mechanical%20fault%20prognosis%2C%20and%20finance.%20These%20real-world%20datasets%20are%0Adiverse%2C%20covering%20a%20range%20from%201%20to%2012%20channels%2C%202%20to%2010%20classes%2C%20sequence%0Alengths%20of%2014%20to%201280%2C%20and%20data%20frequencies%20from%20250%20Hz%20to%20daily%20intervals.%20The%0Aexperimental%20results%20show%20that%20our%20proposed%20trend-seasonality-based%0Aaugmentation%20recommendation%20algorithm%20can%20accurately%20identify%20the%20effective%0Aaugmentations%20for%20a%20given%20time%20series%20dataset%2C%20achieving%20an%20average%20Recall%403%20of%0A0.667%2C%20outperforming%20baselines.%20Our%20work%20provides%20guidance%20for%20studies%0Aemploying%20contrastive%20learning%20in%20time%20series%20analysis%2C%20with%20wide-ranging%0Aapplications.%20All%20the%20code%2C%20datasets%2C%20and%20analysis%20results%20will%20be%20released%20at%0Ahttps%3A//github.com/DL4mHealth/TS-Contrastive-Augmentation-Recommendation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09336v1&entry.124074799=Read"},
{"title": "AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with\n  Pervasive Sensing", "author": "Subhash Nerella and Ziyuan Guan and Scott Siegel and Jiaqing Zhang and Kia Khezeli and Azra Bihorac and Parisa Rashidi", "abstract": "  The intensive care unit (ICU) is a specialized hospital space where\ncritically ill patients receive intensive care and monitoring. Comprehensive\nmonitoring is imperative in assessing patients conditions, in particular\nacuity, and ultimately the quality of care. However, the extent of patient\nmonitoring in the ICU is limited due to time constraints and the workload on\nhealthcare providers. Currently, visual assessments for acuity, including fine\ndetails such as facial expressions, posture, and mobility, are sporadically\ncaptured, or not captured at all. These manual observations are subjective to\nthe individual, prone to documentation errors, and overburden care providers\nwith the additional workload. Artificial Intelligence (AI) enabled systems has\nthe potential to augment the patient visual monitoring and assessment due to\ntheir exceptional learning capabilities. Such systems require robust annotated\ndata to train. To this end, we have developed pervasive sensing and data\nprocessing system which collects data from multiple modalities depth images,\ncolor RGB images, accelerometry, electromyography, sound pressure, and light\nlevels in ICU for developing intelligent monitoring systems for continuous and\ngranular acuity, delirium risk, pain, and mobility assessment. This paper\npresents the Intelligent Intensive Care Unit (I2CU) system architecture we\ndeveloped for real-time patient monitoring and visual assessment.\n", "link": "http://arxiv.org/abs/2303.06252v2", "date": "2024-07-12", "relevancy": 2.0174, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5246}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4952}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Enhanced%20Intensive%20Care%20Unit%3A%20Revolutionizing%20Patient%20Care%20with%0A%20%20Pervasive%20Sensing&body=Title%3A%20AI-Enhanced%20Intensive%20Care%20Unit%3A%20Revolutionizing%20Patient%20Care%20with%0A%20%20Pervasive%20Sensing%0AAuthor%3A%20Subhash%20Nerella%20and%20Ziyuan%20Guan%20and%20Scott%20Siegel%20and%20Jiaqing%20Zhang%20and%20Kia%20Khezeli%20and%20Azra%20Bihorac%20and%20Parisa%20Rashidi%0AAbstract%3A%20%20%20The%20intensive%20care%20unit%20%28ICU%29%20is%20a%20specialized%20hospital%20space%20where%0Acritically%20ill%20patients%20receive%20intensive%20care%20and%20monitoring.%20Comprehensive%0Amonitoring%20is%20imperative%20in%20assessing%20patients%20conditions%2C%20in%20particular%0Aacuity%2C%20and%20ultimately%20the%20quality%20of%20care.%20However%2C%20the%20extent%20of%20patient%0Amonitoring%20in%20the%20ICU%20is%20limited%20due%20to%20time%20constraints%20and%20the%20workload%20on%0Ahealthcare%20providers.%20Currently%2C%20visual%20assessments%20for%20acuity%2C%20including%20fine%0Adetails%20such%20as%20facial%20expressions%2C%20posture%2C%20and%20mobility%2C%20are%20sporadically%0Acaptured%2C%20or%20not%20captured%20at%20all.%20These%20manual%20observations%20are%20subjective%20to%0Athe%20individual%2C%20prone%20to%20documentation%20errors%2C%20and%20overburden%20care%20providers%0Awith%20the%20additional%20workload.%20Artificial%20Intelligence%20%28AI%29%20enabled%20systems%20has%0Athe%20potential%20to%20augment%20the%20patient%20visual%20monitoring%20and%20assessment%20due%20to%0Atheir%20exceptional%20learning%20capabilities.%20Such%20systems%20require%20robust%20annotated%0Adata%20to%20train.%20To%20this%20end%2C%20we%20have%20developed%20pervasive%20sensing%20and%20data%0Aprocessing%20system%20which%20collects%20data%20from%20multiple%20modalities%20depth%20images%2C%0Acolor%20RGB%20images%2C%20accelerometry%2C%20electromyography%2C%20sound%20pressure%2C%20and%20light%0Alevels%20in%20ICU%20for%20developing%20intelligent%20monitoring%20systems%20for%20continuous%20and%0Agranular%20acuity%2C%20delirium%20risk%2C%20pain%2C%20and%20mobility%20assessment.%20This%20paper%0Apresents%20the%20Intelligent%20Intensive%20Care%20Unit%20%28I2CU%29%20system%20architecture%20we%0Adeveloped%20for%20real-time%20patient%20monitoring%20and%20visual%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Enhanced%2520Intensive%2520Care%2520Unit%253A%2520Revolutionizing%2520Patient%2520Care%2520with%250A%2520%2520Pervasive%2520Sensing%26entry.906535625%3DSubhash%2520Nerella%2520and%2520Ziyuan%2520Guan%2520and%2520Scott%2520Siegel%2520and%2520Jiaqing%2520Zhang%2520and%2520Kia%2520Khezeli%2520and%2520Azra%2520Bihorac%2520and%2520Parisa%2520Rashidi%26entry.1292438233%3D%2520%2520The%2520intensive%2520care%2520unit%2520%2528ICU%2529%2520is%2520a%2520specialized%2520hospital%2520space%2520where%250Acritically%2520ill%2520patients%2520receive%2520intensive%2520care%2520and%2520monitoring.%2520Comprehensive%250Amonitoring%2520is%2520imperative%2520in%2520assessing%2520patients%2520conditions%252C%2520in%2520particular%250Aacuity%252C%2520and%2520ultimately%2520the%2520quality%2520of%2520care.%2520However%252C%2520the%2520extent%2520of%2520patient%250Amonitoring%2520in%2520the%2520ICU%2520is%2520limited%2520due%2520to%2520time%2520constraints%2520and%2520the%2520workload%2520on%250Ahealthcare%2520providers.%2520Currently%252C%2520visual%2520assessments%2520for%2520acuity%252C%2520including%2520fine%250Adetails%2520such%2520as%2520facial%2520expressions%252C%2520posture%252C%2520and%2520mobility%252C%2520are%2520sporadically%250Acaptured%252C%2520or%2520not%2520captured%2520at%2520all.%2520These%2520manual%2520observations%2520are%2520subjective%2520to%250Athe%2520individual%252C%2520prone%2520to%2520documentation%2520errors%252C%2520and%2520overburden%2520care%2520providers%250Awith%2520the%2520additional%2520workload.%2520Artificial%2520Intelligence%2520%2528AI%2529%2520enabled%2520systems%2520has%250Athe%2520potential%2520to%2520augment%2520the%2520patient%2520visual%2520monitoring%2520and%2520assessment%2520due%2520to%250Atheir%2520exceptional%2520learning%2520capabilities.%2520Such%2520systems%2520require%2520robust%2520annotated%250Adata%2520to%2520train.%2520To%2520this%2520end%252C%2520we%2520have%2520developed%2520pervasive%2520sensing%2520and%2520data%250Aprocessing%2520system%2520which%2520collects%2520data%2520from%2520multiple%2520modalities%2520depth%2520images%252C%250Acolor%2520RGB%2520images%252C%2520accelerometry%252C%2520electromyography%252C%2520sound%2520pressure%252C%2520and%2520light%250Alevels%2520in%2520ICU%2520for%2520developing%2520intelligent%2520monitoring%2520systems%2520for%2520continuous%2520and%250Agranular%2520acuity%252C%2520delirium%2520risk%252C%2520pain%252C%2520and%2520mobility%2520assessment.%2520This%2520paper%250Apresents%2520the%2520Intelligent%2520Intensive%2520Care%2520Unit%2520%2528I2CU%2529%2520system%2520architecture%2520we%250Adeveloped%2520for%2520real-time%2520patient%2520monitoring%2520and%2520visual%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.06252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Enhanced%20Intensive%20Care%20Unit%3A%20Revolutionizing%20Patient%20Care%20with%0A%20%20Pervasive%20Sensing&entry.906535625=Subhash%20Nerella%20and%20Ziyuan%20Guan%20and%20Scott%20Siegel%20and%20Jiaqing%20Zhang%20and%20Kia%20Khezeli%20and%20Azra%20Bihorac%20and%20Parisa%20Rashidi&entry.1292438233=%20%20The%20intensive%20care%20unit%20%28ICU%29%20is%20a%20specialized%20hospital%20space%20where%0Acritically%20ill%20patients%20receive%20intensive%20care%20and%20monitoring.%20Comprehensive%0Amonitoring%20is%20imperative%20in%20assessing%20patients%20conditions%2C%20in%20particular%0Aacuity%2C%20and%20ultimately%20the%20quality%20of%20care.%20However%2C%20the%20extent%20of%20patient%0Amonitoring%20in%20the%20ICU%20is%20limited%20due%20to%20time%20constraints%20and%20the%20workload%20on%0Ahealthcare%20providers.%20Currently%2C%20visual%20assessments%20for%20acuity%2C%20including%20fine%0Adetails%20such%20as%20facial%20expressions%2C%20posture%2C%20and%20mobility%2C%20are%20sporadically%0Acaptured%2C%20or%20not%20captured%20at%20all.%20These%20manual%20observations%20are%20subjective%20to%0Athe%20individual%2C%20prone%20to%20documentation%20errors%2C%20and%20overburden%20care%20providers%0Awith%20the%20additional%20workload.%20Artificial%20Intelligence%20%28AI%29%20enabled%20systems%20has%0Athe%20potential%20to%20augment%20the%20patient%20visual%20monitoring%20and%20assessment%20due%20to%0Atheir%20exceptional%20learning%20capabilities.%20Such%20systems%20require%20robust%20annotated%0Adata%20to%20train.%20To%20this%20end%2C%20we%20have%20developed%20pervasive%20sensing%20and%20data%0Aprocessing%20system%20which%20collects%20data%20from%20multiple%20modalities%20depth%20images%2C%0Acolor%20RGB%20images%2C%20accelerometry%2C%20electromyography%2C%20sound%20pressure%2C%20and%20light%0Alevels%20in%20ICU%20for%20developing%20intelligent%20monitoring%20systems%20for%20continuous%20and%0Agranular%20acuity%2C%20delirium%20risk%2C%20pain%2C%20and%20mobility%20assessment.%20This%20paper%0Apresents%20the%20Intelligent%20Intensive%20Care%20Unit%20%28I2CU%29%20system%20architecture%20we%0Adeveloped%20for%20real-time%20patient%20monitoring%20and%20visual%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06252v2&entry.124074799=Read"},
{"title": "Deep Adversarial Defense Against Multilevel-Lp Attacks", "author": "Ren Wang and Yuxuan Li and Alfred Hero", "abstract": "  Deep learning models have shown considerable vulnerability to adversarial\nattacks, particularly as attacker strategies become more sophisticated. While\ntraditional adversarial training (AT) techniques offer some resilience, they\noften focus on defending against a single type of attack, e.g., the\n$\\ell_\\infty$-norm attack, which can fail for other types. This paper\nintroduces a computationally efficient multilevel $\\ell_p$ defense, called the\nEfficient Robust Mode Connectivity (EMRC) method, which aims to enhance a deep\nlearning model's resilience against multiple $\\ell_p$-norm attacks. Similar to\nanalytical continuation approaches used in continuous optimization, the method\nblends two $p$-specific adversarially optimal models, the $\\ell_1$- and\n$\\ell_\\infty$-norm AT solutions, to provide good adversarial robustness for a\nrange of $p$. We present experiments demonstrating that our approach performs\nbetter on various attacks as compared to AT-$\\ell_\\infty$, E-AT, and MSD, for\ndatasets/architectures including: CIFAR-10, CIFAR-100 / PreResNet110,\nWideResNet, ViT-Base.\n", "link": "http://arxiv.org/abs/2407.09251v1", "date": "2024-07-12", "relevancy": 1.9932, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5048}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5033}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Adversarial%20Defense%20Against%20Multilevel-Lp%20Attacks&body=Title%3A%20Deep%20Adversarial%20Defense%20Against%20Multilevel-Lp%20Attacks%0AAuthor%3A%20Ren%20Wang%20and%20Yuxuan%20Li%20and%20Alfred%20Hero%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20shown%20considerable%20vulnerability%20to%20adversarial%0Aattacks%2C%20particularly%20as%20attacker%20strategies%20become%20more%20sophisticated.%20While%0Atraditional%20adversarial%20training%20%28AT%29%20techniques%20offer%20some%20resilience%2C%20they%0Aoften%20focus%20on%20defending%20against%20a%20single%20type%20of%20attack%2C%20e.g.%2C%20the%0A%24%5Cell_%5Cinfty%24-norm%20attack%2C%20which%20can%20fail%20for%20other%20types.%20This%20paper%0Aintroduces%20a%20computationally%20efficient%20multilevel%20%24%5Cell_p%24%20defense%2C%20called%20the%0AEfficient%20Robust%20Mode%20Connectivity%20%28EMRC%29%20method%2C%20which%20aims%20to%20enhance%20a%20deep%0Alearning%20model%27s%20resilience%20against%20multiple%20%24%5Cell_p%24-norm%20attacks.%20Similar%20to%0Aanalytical%20continuation%20approaches%20used%20in%20continuous%20optimization%2C%20the%20method%0Ablends%20two%20%24p%24-specific%20adversarially%20optimal%20models%2C%20the%20%24%5Cell_1%24-%20and%0A%24%5Cell_%5Cinfty%24-norm%20AT%20solutions%2C%20to%20provide%20good%20adversarial%20robustness%20for%20a%0Arange%20of%20%24p%24.%20We%20present%20experiments%20demonstrating%20that%20our%20approach%20performs%0Abetter%20on%20various%20attacks%20as%20compared%20to%20AT-%24%5Cell_%5Cinfty%24%2C%20E-AT%2C%20and%20MSD%2C%20for%0Adatasets/architectures%20including%3A%20CIFAR-10%2C%20CIFAR-100%20/%20PreResNet110%2C%0AWideResNet%2C%20ViT-Base.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Adversarial%2520Defense%2520Against%2520Multilevel-Lp%2520Attacks%26entry.906535625%3DRen%2520Wang%2520and%2520Yuxuan%2520Li%2520and%2520Alfred%2520Hero%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520have%2520shown%2520considerable%2520vulnerability%2520to%2520adversarial%250Aattacks%252C%2520particularly%2520as%2520attacker%2520strategies%2520become%2520more%2520sophisticated.%2520While%250Atraditional%2520adversarial%2520training%2520%2528AT%2529%2520techniques%2520offer%2520some%2520resilience%252C%2520they%250Aoften%2520focus%2520on%2520defending%2520against%2520a%2520single%2520type%2520of%2520attack%252C%2520e.g.%252C%2520the%250A%2524%255Cell_%255Cinfty%2524-norm%2520attack%252C%2520which%2520can%2520fail%2520for%2520other%2520types.%2520This%2520paper%250Aintroduces%2520a%2520computationally%2520efficient%2520multilevel%2520%2524%255Cell_p%2524%2520defense%252C%2520called%2520the%250AEfficient%2520Robust%2520Mode%2520Connectivity%2520%2528EMRC%2529%2520method%252C%2520which%2520aims%2520to%2520enhance%2520a%2520deep%250Alearning%2520model%2527s%2520resilience%2520against%2520multiple%2520%2524%255Cell_p%2524-norm%2520attacks.%2520Similar%2520to%250Aanalytical%2520continuation%2520approaches%2520used%2520in%2520continuous%2520optimization%252C%2520the%2520method%250Ablends%2520two%2520%2524p%2524-specific%2520adversarially%2520optimal%2520models%252C%2520the%2520%2524%255Cell_1%2524-%2520and%250A%2524%255Cell_%255Cinfty%2524-norm%2520AT%2520solutions%252C%2520to%2520provide%2520good%2520adversarial%2520robustness%2520for%2520a%250Arange%2520of%2520%2524p%2524.%2520We%2520present%2520experiments%2520demonstrating%2520that%2520our%2520approach%2520performs%250Abetter%2520on%2520various%2520attacks%2520as%2520compared%2520to%2520AT-%2524%255Cell_%255Cinfty%2524%252C%2520E-AT%252C%2520and%2520MSD%252C%2520for%250Adatasets/architectures%2520including%253A%2520CIFAR-10%252C%2520CIFAR-100%2520/%2520PreResNet110%252C%250AWideResNet%252C%2520ViT-Base.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Adversarial%20Defense%20Against%20Multilevel-Lp%20Attacks&entry.906535625=Ren%20Wang%20and%20Yuxuan%20Li%20and%20Alfred%20Hero&entry.1292438233=%20%20Deep%20learning%20models%20have%20shown%20considerable%20vulnerability%20to%20adversarial%0Aattacks%2C%20particularly%20as%20attacker%20strategies%20become%20more%20sophisticated.%20While%0Atraditional%20adversarial%20training%20%28AT%29%20techniques%20offer%20some%20resilience%2C%20they%0Aoften%20focus%20on%20defending%20against%20a%20single%20type%20of%20attack%2C%20e.g.%2C%20the%0A%24%5Cell_%5Cinfty%24-norm%20attack%2C%20which%20can%20fail%20for%20other%20types.%20This%20paper%0Aintroduces%20a%20computationally%20efficient%20multilevel%20%24%5Cell_p%24%20defense%2C%20called%20the%0AEfficient%20Robust%20Mode%20Connectivity%20%28EMRC%29%20method%2C%20which%20aims%20to%20enhance%20a%20deep%0Alearning%20model%27s%20resilience%20against%20multiple%20%24%5Cell_p%24-norm%20attacks.%20Similar%20to%0Aanalytical%20continuation%20approaches%20used%20in%20continuous%20optimization%2C%20the%20method%0Ablends%20two%20%24p%24-specific%20adversarially%20optimal%20models%2C%20the%20%24%5Cell_1%24-%20and%0A%24%5Cell_%5Cinfty%24-norm%20AT%20solutions%2C%20to%20provide%20good%20adversarial%20robustness%20for%20a%0Arange%20of%20%24p%24.%20We%20present%20experiments%20demonstrating%20that%20our%20approach%20performs%0Abetter%20on%20various%20attacks%20as%20compared%20to%20AT-%24%5Cell_%5Cinfty%24%2C%20E-AT%2C%20and%20MSD%2C%20for%0Adatasets/architectures%20including%3A%20CIFAR-10%2C%20CIFAR-100%20/%20PreResNet110%2C%0AWideResNet%2C%20ViT-Base.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09251v1&entry.124074799=Read"},
{"title": "Imaging Interiors: An Implicit Solution to Electromagnetic Inverse\n  Scattering Problems", "author": "Ziyuan Luo and Boxin Shi and Haoliang Li and Renjie Wan", "abstract": "  Electromagnetic Inverse Scattering Problems (EISP) have gained wide\napplications in computational imaging. By solving EISP, the internal relative\npermittivity of the scatterer can be non-invasively determined based on the\nscattered electromagnetic fields. Despite previous efforts to address EISP,\nachieving better solutions to this problem has remained elusive, due to the\nchallenges posed by inversion and discretization. This paper tackles those\nchallenges in EISP via an implicit approach. By representing the scatterer's\nrelative permittivity as a continuous implicit representation, our method is\nable to address the low-resolution problems arising from discretization.\nFurther, optimizing this implicit representation within a forward framework\nallows us to conveniently circumvent the challenges posed by inverse\nestimation. Our approach outperforms existing methods on standard benchmark\ndatasets. Project page: https://luo-ziyuan.github.io/Imaging-Interiors\n", "link": "http://arxiv.org/abs/2407.09352v1", "date": "2024-07-12", "relevancy": 1.9854, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5143}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5002}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imaging%20Interiors%3A%20An%20Implicit%20Solution%20to%20Electromagnetic%20Inverse%0A%20%20Scattering%20Problems&body=Title%3A%20Imaging%20Interiors%3A%20An%20Implicit%20Solution%20to%20Electromagnetic%20Inverse%0A%20%20Scattering%20Problems%0AAuthor%3A%20Ziyuan%20Luo%20and%20Boxin%20Shi%20and%20Haoliang%20Li%20and%20Renjie%20Wan%0AAbstract%3A%20%20%20Electromagnetic%20Inverse%20Scattering%20Problems%20%28EISP%29%20have%20gained%20wide%0Aapplications%20in%20computational%20imaging.%20By%20solving%20EISP%2C%20the%20internal%20relative%0Apermittivity%20of%20the%20scatterer%20can%20be%20non-invasively%20determined%20based%20on%20the%0Ascattered%20electromagnetic%20fields.%20Despite%20previous%20efforts%20to%20address%20EISP%2C%0Aachieving%20better%20solutions%20to%20this%20problem%20has%20remained%20elusive%2C%20due%20to%20the%0Achallenges%20posed%20by%20inversion%20and%20discretization.%20This%20paper%20tackles%20those%0Achallenges%20in%20EISP%20via%20an%20implicit%20approach.%20By%20representing%20the%20scatterer%27s%0Arelative%20permittivity%20as%20a%20continuous%20implicit%20representation%2C%20our%20method%20is%0Aable%20to%20address%20the%20low-resolution%20problems%20arising%20from%20discretization.%0AFurther%2C%20optimizing%20this%20implicit%20representation%20within%20a%20forward%20framework%0Aallows%20us%20to%20conveniently%20circumvent%20the%20challenges%20posed%20by%20inverse%0Aestimation.%20Our%20approach%20outperforms%20existing%20methods%20on%20standard%20benchmark%0Adatasets.%20Project%20page%3A%20https%3A//luo-ziyuan.github.io/Imaging-Interiors%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImaging%2520Interiors%253A%2520An%2520Implicit%2520Solution%2520to%2520Electromagnetic%2520Inverse%250A%2520%2520Scattering%2520Problems%26entry.906535625%3DZiyuan%2520Luo%2520and%2520Boxin%2520Shi%2520and%2520Haoliang%2520Li%2520and%2520Renjie%2520Wan%26entry.1292438233%3D%2520%2520Electromagnetic%2520Inverse%2520Scattering%2520Problems%2520%2528EISP%2529%2520have%2520gained%2520wide%250Aapplications%2520in%2520computational%2520imaging.%2520By%2520solving%2520EISP%252C%2520the%2520internal%2520relative%250Apermittivity%2520of%2520the%2520scatterer%2520can%2520be%2520non-invasively%2520determined%2520based%2520on%2520the%250Ascattered%2520electromagnetic%2520fields.%2520Despite%2520previous%2520efforts%2520to%2520address%2520EISP%252C%250Aachieving%2520better%2520solutions%2520to%2520this%2520problem%2520has%2520remained%2520elusive%252C%2520due%2520to%2520the%250Achallenges%2520posed%2520by%2520inversion%2520and%2520discretization.%2520This%2520paper%2520tackles%2520those%250Achallenges%2520in%2520EISP%2520via%2520an%2520implicit%2520approach.%2520By%2520representing%2520the%2520scatterer%2527s%250Arelative%2520permittivity%2520as%2520a%2520continuous%2520implicit%2520representation%252C%2520our%2520method%2520is%250Aable%2520to%2520address%2520the%2520low-resolution%2520problems%2520arising%2520from%2520discretization.%250AFurther%252C%2520optimizing%2520this%2520implicit%2520representation%2520within%2520a%2520forward%2520framework%250Aallows%2520us%2520to%2520conveniently%2520circumvent%2520the%2520challenges%2520posed%2520by%2520inverse%250Aestimation.%2520Our%2520approach%2520outperforms%2520existing%2520methods%2520on%2520standard%2520benchmark%250Adatasets.%2520Project%2520page%253A%2520https%253A//luo-ziyuan.github.io/Imaging-Interiors%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imaging%20Interiors%3A%20An%20Implicit%20Solution%20to%20Electromagnetic%20Inverse%0A%20%20Scattering%20Problems&entry.906535625=Ziyuan%20Luo%20and%20Boxin%20Shi%20and%20Haoliang%20Li%20and%20Renjie%20Wan&entry.1292438233=%20%20Electromagnetic%20Inverse%20Scattering%20Problems%20%28EISP%29%20have%20gained%20wide%0Aapplications%20in%20computational%20imaging.%20By%20solving%20EISP%2C%20the%20internal%20relative%0Apermittivity%20of%20the%20scatterer%20can%20be%20non-invasively%20determined%20based%20on%20the%0Ascattered%20electromagnetic%20fields.%20Despite%20previous%20efforts%20to%20address%20EISP%2C%0Aachieving%20better%20solutions%20to%20this%20problem%20has%20remained%20elusive%2C%20due%20to%20the%0Achallenges%20posed%20by%20inversion%20and%20discretization.%20This%20paper%20tackles%20those%0Achallenges%20in%20EISP%20via%20an%20implicit%20approach.%20By%20representing%20the%20scatterer%27s%0Arelative%20permittivity%20as%20a%20continuous%20implicit%20representation%2C%20our%20method%20is%0Aable%20to%20address%20the%20low-resolution%20problems%20arising%20from%20discretization.%0AFurther%2C%20optimizing%20this%20implicit%20representation%20within%20a%20forward%20framework%0Aallows%20us%20to%20conveniently%20circumvent%20the%20challenges%20posed%20by%20inverse%0Aestimation.%20Our%20approach%20outperforms%20existing%20methods%20on%20standard%20benchmark%0Adatasets.%20Project%20page%3A%20https%3A//luo-ziyuan.github.io/Imaging-Interiors%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09352v1&entry.124074799=Read"},
{"title": "TRAVERSE: Traffic-Responsive Autonomous Vehicle Experience & Rare-event\n  Simulation for Enhanced safety", "author": "Sandeep Thalapanane and Sandip Sharan Senthil Kumar and Guru Nandhan Appiya Dilipkumar Peethambari and Sourang SriHari and Laura Zheng and Julio Poveda and Ming C. Lin", "abstract": "  Data for training learning-enabled self-driving cars in the physical world\nare typically collected in a safe, normal environment. Such data distribution\noften engenders a strong bias towards safe driving, making self-driving cars\nunprepared when encountering adversarial scenarios like unexpected accidents.\nDue to a dearth of such adverse data that is unrealistic for drivers to\ncollect, autonomous vehicles can perform poorly when experiencing such rare\nevents. This work addresses much-needed research by having participants drive a\nVR vehicle simulator going through simulated traffic with various types of\naccidental scenarios. It aims to understand human responses and behaviors in\nsimulated accidents, contributing to our understanding of driving dynamics and\nsafety. The simulation framework adopts a robust traffic simulation and is\nrendered using the Unity Game Engine. Furthermore, the simulation framework is\nbuilt with portable, light-weight immersive driving simulator hardware,\nlowering the resource barrier for studies in autonomous driving research.\n  Keywords: Rare Events, Traffic Simulation, Autonomous Driving, Virtual\nReality, User Studies\n", "link": "http://arxiv.org/abs/2407.09466v1", "date": "2024-07-12", "relevancy": 1.9708, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.506}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4914}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRAVERSE%3A%20Traffic-Responsive%20Autonomous%20Vehicle%20Experience%20%26%20Rare-event%0A%20%20Simulation%20for%20Enhanced%20safety&body=Title%3A%20TRAVERSE%3A%20Traffic-Responsive%20Autonomous%20Vehicle%20Experience%20%26%20Rare-event%0A%20%20Simulation%20for%20Enhanced%20safety%0AAuthor%3A%20Sandeep%20Thalapanane%20and%20Sandip%20Sharan%20Senthil%20Kumar%20and%20Guru%20Nandhan%20Appiya%20Dilipkumar%20Peethambari%20and%20Sourang%20SriHari%20and%20Laura%20Zheng%20and%20Julio%20Poveda%20and%20Ming%20C.%20Lin%0AAbstract%3A%20%20%20Data%20for%20training%20learning-enabled%20self-driving%20cars%20in%20the%20physical%20world%0Aare%20typically%20collected%20in%20a%20safe%2C%20normal%20environment.%20Such%20data%20distribution%0Aoften%20engenders%20a%20strong%20bias%20towards%20safe%20driving%2C%20making%20self-driving%20cars%0Aunprepared%20when%20encountering%20adversarial%20scenarios%20like%20unexpected%20accidents.%0ADue%20to%20a%20dearth%20of%20such%20adverse%20data%20that%20is%20unrealistic%20for%20drivers%20to%0Acollect%2C%20autonomous%20vehicles%20can%20perform%20poorly%20when%20experiencing%20such%20rare%0Aevents.%20This%20work%20addresses%20much-needed%20research%20by%20having%20participants%20drive%20a%0AVR%20vehicle%20simulator%20going%20through%20simulated%20traffic%20with%20various%20types%20of%0Aaccidental%20scenarios.%20It%20aims%20to%20understand%20human%20responses%20and%20behaviors%20in%0Asimulated%20accidents%2C%20contributing%20to%20our%20understanding%20of%20driving%20dynamics%20and%0Asafety.%20The%20simulation%20framework%20adopts%20a%20robust%20traffic%20simulation%20and%20is%0Arendered%20using%20the%20Unity%20Game%20Engine.%20Furthermore%2C%20the%20simulation%20framework%20is%0Abuilt%20with%20portable%2C%20light-weight%20immersive%20driving%20simulator%20hardware%2C%0Alowering%20the%20resource%20barrier%20for%20studies%20in%20autonomous%20driving%20research.%0A%20%20Keywords%3A%20Rare%20Events%2C%20Traffic%20Simulation%2C%20Autonomous%20Driving%2C%20Virtual%0AReality%2C%20User%20Studies%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRAVERSE%253A%2520Traffic-Responsive%2520Autonomous%2520Vehicle%2520Experience%2520%2526%2520Rare-event%250A%2520%2520Simulation%2520for%2520Enhanced%2520safety%26entry.906535625%3DSandeep%2520Thalapanane%2520and%2520Sandip%2520Sharan%2520Senthil%2520Kumar%2520and%2520Guru%2520Nandhan%2520Appiya%2520Dilipkumar%2520Peethambari%2520and%2520Sourang%2520SriHari%2520and%2520Laura%2520Zheng%2520and%2520Julio%2520Poveda%2520and%2520Ming%2520C.%2520Lin%26entry.1292438233%3D%2520%2520Data%2520for%2520training%2520learning-enabled%2520self-driving%2520cars%2520in%2520the%2520physical%2520world%250Aare%2520typically%2520collected%2520in%2520a%2520safe%252C%2520normal%2520environment.%2520Such%2520data%2520distribution%250Aoften%2520engenders%2520a%2520strong%2520bias%2520towards%2520safe%2520driving%252C%2520making%2520self-driving%2520cars%250Aunprepared%2520when%2520encountering%2520adversarial%2520scenarios%2520like%2520unexpected%2520accidents.%250ADue%2520to%2520a%2520dearth%2520of%2520such%2520adverse%2520data%2520that%2520is%2520unrealistic%2520for%2520drivers%2520to%250Acollect%252C%2520autonomous%2520vehicles%2520can%2520perform%2520poorly%2520when%2520experiencing%2520such%2520rare%250Aevents.%2520This%2520work%2520addresses%2520much-needed%2520research%2520by%2520having%2520participants%2520drive%2520a%250AVR%2520vehicle%2520simulator%2520going%2520through%2520simulated%2520traffic%2520with%2520various%2520types%2520of%250Aaccidental%2520scenarios.%2520It%2520aims%2520to%2520understand%2520human%2520responses%2520and%2520behaviors%2520in%250Asimulated%2520accidents%252C%2520contributing%2520to%2520our%2520understanding%2520of%2520driving%2520dynamics%2520and%250Asafety.%2520The%2520simulation%2520framework%2520adopts%2520a%2520robust%2520traffic%2520simulation%2520and%2520is%250Arendered%2520using%2520the%2520Unity%2520Game%2520Engine.%2520Furthermore%252C%2520the%2520simulation%2520framework%2520is%250Abuilt%2520with%2520portable%252C%2520light-weight%2520immersive%2520driving%2520simulator%2520hardware%252C%250Alowering%2520the%2520resource%2520barrier%2520for%2520studies%2520in%2520autonomous%2520driving%2520research.%250A%2520%2520Keywords%253A%2520Rare%2520Events%252C%2520Traffic%2520Simulation%252C%2520Autonomous%2520Driving%252C%2520Virtual%250AReality%252C%2520User%2520Studies%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRAVERSE%3A%20Traffic-Responsive%20Autonomous%20Vehicle%20Experience%20%26%20Rare-event%0A%20%20Simulation%20for%20Enhanced%20safety&entry.906535625=Sandeep%20Thalapanane%20and%20Sandip%20Sharan%20Senthil%20Kumar%20and%20Guru%20Nandhan%20Appiya%20Dilipkumar%20Peethambari%20and%20Sourang%20SriHari%20and%20Laura%20Zheng%20and%20Julio%20Poveda%20and%20Ming%20C.%20Lin&entry.1292438233=%20%20Data%20for%20training%20learning-enabled%20self-driving%20cars%20in%20the%20physical%20world%0Aare%20typically%20collected%20in%20a%20safe%2C%20normal%20environment.%20Such%20data%20distribution%0Aoften%20engenders%20a%20strong%20bias%20towards%20safe%20driving%2C%20making%20self-driving%20cars%0Aunprepared%20when%20encountering%20adversarial%20scenarios%20like%20unexpected%20accidents.%0ADue%20to%20a%20dearth%20of%20such%20adverse%20data%20that%20is%20unrealistic%20for%20drivers%20to%0Acollect%2C%20autonomous%20vehicles%20can%20perform%20poorly%20when%20experiencing%20such%20rare%0Aevents.%20This%20work%20addresses%20much-needed%20research%20by%20having%20participants%20drive%20a%0AVR%20vehicle%20simulator%20going%20through%20simulated%20traffic%20with%20various%20types%20of%0Aaccidental%20scenarios.%20It%20aims%20to%20understand%20human%20responses%20and%20behaviors%20in%0Asimulated%20accidents%2C%20contributing%20to%20our%20understanding%20of%20driving%20dynamics%20and%0Asafety.%20The%20simulation%20framework%20adopts%20a%20robust%20traffic%20simulation%20and%20is%0Arendered%20using%20the%20Unity%20Game%20Engine.%20Furthermore%2C%20the%20simulation%20framework%20is%0Abuilt%20with%20portable%2C%20light-weight%20immersive%20driving%20simulator%20hardware%2C%0Alowering%20the%20resource%20barrier%20for%20studies%20in%20autonomous%20driving%20research.%0A%20%20Keywords%3A%20Rare%20Events%2C%20Traffic%20Simulation%2C%20Autonomous%20Driving%2C%20Virtual%0AReality%2C%20User%20Studies%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09466v1&entry.124074799=Read"},
{"title": "Learning Distances from Data with Normalizing Flows and Score Matching", "author": "Peter Sorrenson and Daniel Behrend-Uriarte and Christoph Schn\u00f6rr and Ullrich K\u00f6the", "abstract": "  Density-based distances (DBDs) offer an elegant solution to the problem of\nmetric learning. By defining a Riemannian metric which increases with\ndecreasing probability density, shortest paths naturally follow the data\nmanifold and points are clustered according to the modes of the data. We show\nthat existing methods to estimate Fermat distances, a particular choice of DBD,\nsuffer from poor convergence in both low and high dimensions due to i)\ninaccurate density estimates and ii) reliance on graph-based paths which are\nincreasingly rough in high dimensions. To address these issues, we propose\nlearning the densities using a normalizing flow, a generative model with\ntractable density estimation, and employing a smooth relaxation method using a\nscore model initialized from a graph-based proposal. Additionally, we introduce\na dimension-adapted Fermat distance that exhibits more intuitive behavior when\nscaled to high dimensions and offers better numerical properties. Our work\npaves the way for practical use of density-based distances, especially in\nhigh-dimensional spaces.\n", "link": "http://arxiv.org/abs/2407.09297v1", "date": "2024-07-12", "relevancy": 1.9669, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5144}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5068}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Distances%20from%20Data%20with%20Normalizing%20Flows%20and%20Score%20Matching&body=Title%3A%20Learning%20Distances%20from%20Data%20with%20Normalizing%20Flows%20and%20Score%20Matching%0AAuthor%3A%20Peter%20Sorrenson%20and%20Daniel%20Behrend-Uriarte%20and%20Christoph%20Schn%C3%B6rr%20and%20Ullrich%20K%C3%B6the%0AAbstract%3A%20%20%20Density-based%20distances%20%28DBDs%29%20offer%20an%20elegant%20solution%20to%20the%20problem%20of%0Ametric%20learning.%20By%20defining%20a%20Riemannian%20metric%20which%20increases%20with%0Adecreasing%20probability%20density%2C%20shortest%20paths%20naturally%20follow%20the%20data%0Amanifold%20and%20points%20are%20clustered%20according%20to%20the%20modes%20of%20the%20data.%20We%20show%0Athat%20existing%20methods%20to%20estimate%20Fermat%20distances%2C%20a%20particular%20choice%20of%20DBD%2C%0Asuffer%20from%20poor%20convergence%20in%20both%20low%20and%20high%20dimensions%20due%20to%20i%29%0Ainaccurate%20density%20estimates%20and%20ii%29%20reliance%20on%20graph-based%20paths%20which%20are%0Aincreasingly%20rough%20in%20high%20dimensions.%20To%20address%20these%20issues%2C%20we%20propose%0Alearning%20the%20densities%20using%20a%20normalizing%20flow%2C%20a%20generative%20model%20with%0Atractable%20density%20estimation%2C%20and%20employing%20a%20smooth%20relaxation%20method%20using%20a%0Ascore%20model%20initialized%20from%20a%20graph-based%20proposal.%20Additionally%2C%20we%20introduce%0Aa%20dimension-adapted%20Fermat%20distance%20that%20exhibits%20more%20intuitive%20behavior%20when%0Ascaled%20to%20high%20dimensions%20and%20offers%20better%20numerical%20properties.%20Our%20work%0Apaves%20the%20way%20for%20practical%20use%20of%20density-based%20distances%2C%20especially%20in%0Ahigh-dimensional%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Distances%2520from%2520Data%2520with%2520Normalizing%2520Flows%2520and%2520Score%2520Matching%26entry.906535625%3DPeter%2520Sorrenson%2520and%2520Daniel%2520Behrend-Uriarte%2520and%2520Christoph%2520Schn%25C3%25B6rr%2520and%2520Ullrich%2520K%25C3%25B6the%26entry.1292438233%3D%2520%2520Density-based%2520distances%2520%2528DBDs%2529%2520offer%2520an%2520elegant%2520solution%2520to%2520the%2520problem%2520of%250Ametric%2520learning.%2520By%2520defining%2520a%2520Riemannian%2520metric%2520which%2520increases%2520with%250Adecreasing%2520probability%2520density%252C%2520shortest%2520paths%2520naturally%2520follow%2520the%2520data%250Amanifold%2520and%2520points%2520are%2520clustered%2520according%2520to%2520the%2520modes%2520of%2520the%2520data.%2520We%2520show%250Athat%2520existing%2520methods%2520to%2520estimate%2520Fermat%2520distances%252C%2520a%2520particular%2520choice%2520of%2520DBD%252C%250Asuffer%2520from%2520poor%2520convergence%2520in%2520both%2520low%2520and%2520high%2520dimensions%2520due%2520to%2520i%2529%250Ainaccurate%2520density%2520estimates%2520and%2520ii%2529%2520reliance%2520on%2520graph-based%2520paths%2520which%2520are%250Aincreasingly%2520rough%2520in%2520high%2520dimensions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250Alearning%2520the%2520densities%2520using%2520a%2520normalizing%2520flow%252C%2520a%2520generative%2520model%2520with%250Atractable%2520density%2520estimation%252C%2520and%2520employing%2520a%2520smooth%2520relaxation%2520method%2520using%2520a%250Ascore%2520model%2520initialized%2520from%2520a%2520graph-based%2520proposal.%2520Additionally%252C%2520we%2520introduce%250Aa%2520dimension-adapted%2520Fermat%2520distance%2520that%2520exhibits%2520more%2520intuitive%2520behavior%2520when%250Ascaled%2520to%2520high%2520dimensions%2520and%2520offers%2520better%2520numerical%2520properties.%2520Our%2520work%250Apaves%2520the%2520way%2520for%2520practical%2520use%2520of%2520density-based%2520distances%252C%2520especially%2520in%250Ahigh-dimensional%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Distances%20from%20Data%20with%20Normalizing%20Flows%20and%20Score%20Matching&entry.906535625=Peter%20Sorrenson%20and%20Daniel%20Behrend-Uriarte%20and%20Christoph%20Schn%C3%B6rr%20and%20Ullrich%20K%C3%B6the&entry.1292438233=%20%20Density-based%20distances%20%28DBDs%29%20offer%20an%20elegant%20solution%20to%20the%20problem%20of%0Ametric%20learning.%20By%20defining%20a%20Riemannian%20metric%20which%20increases%20with%0Adecreasing%20probability%20density%2C%20shortest%20paths%20naturally%20follow%20the%20data%0Amanifold%20and%20points%20are%20clustered%20according%20to%20the%20modes%20of%20the%20data.%20We%20show%0Athat%20existing%20methods%20to%20estimate%20Fermat%20distances%2C%20a%20particular%20choice%20of%20DBD%2C%0Asuffer%20from%20poor%20convergence%20in%20both%20low%20and%20high%20dimensions%20due%20to%20i%29%0Ainaccurate%20density%20estimates%20and%20ii%29%20reliance%20on%20graph-based%20paths%20which%20are%0Aincreasingly%20rough%20in%20high%20dimensions.%20To%20address%20these%20issues%2C%20we%20propose%0Alearning%20the%20densities%20using%20a%20normalizing%20flow%2C%20a%20generative%20model%20with%0Atractable%20density%20estimation%2C%20and%20employing%20a%20smooth%20relaxation%20method%20using%20a%0Ascore%20model%20initialized%20from%20a%20graph-based%20proposal.%20Additionally%2C%20we%20introduce%0Aa%20dimension-adapted%20Fermat%20distance%20that%20exhibits%20more%20intuitive%20behavior%20when%0Ascaled%20to%20high%20dimensions%20and%20offers%20better%20numerical%20properties.%20Our%20work%0Apaves%20the%20way%20for%20practical%20use%20of%20density-based%20distances%2C%20especially%20in%0Ahigh-dimensional%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09297v1&entry.124074799=Read"},
{"title": "Can large language models explore in-context?", "author": "Akshay Krishnamurthy and Keegan Harris and Dylan J. Foster and Cyril Zhang and Aleksandrs Slivkins", "abstract": "  We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.\n", "link": "http://arxiv.org/abs/2403.15371v2", "date": "2024-07-12", "relevancy": 1.9569, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5555}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20large%20language%20models%20explore%20in-context%3F&body=Title%3A%20Can%20large%20language%20models%20explore%20in-context%3F%0AAuthor%3A%20Akshay%20Krishnamurthy%20and%20Keegan%20Harris%20and%20Dylan%20J.%20Foster%20and%20Cyril%20Zhang%20and%20Aleksandrs%20Slivkins%0AAbstract%3A%20%20%20We%20investigate%20the%20extent%20to%20which%20contemporary%20Large%20Language%20Models%20%28LLMs%29%0Acan%20engage%20in%20exploration%2C%20a%20core%20capability%20in%20reinforcement%20learning%20and%0Adecision%20making.%20We%20focus%20on%20native%20performance%20of%20existing%20LLMs%2C%20without%0Atraining%20interventions.%20We%20deploy%20LLMs%20as%20agents%20in%20simple%20multi-armed%20bandit%0Aenvironments%2C%20specifying%20the%20environment%20description%20and%20interaction%20history%0Aentirely%20in-context%2C%20i.e.%2C%20within%20the%20LLM%20prompt.%20We%20experiment%20with%20GPT-3.5%2C%0AGPT-4%2C%20and%20Llama2%2C%20using%20a%20variety%20of%20prompt%20designs%2C%20and%20find%20that%20the%20models%0Ado%20not%20robustly%20engage%20in%20exploration%20without%20substantial%20interventions%3A%20i%29%0AAcross%20all%20of%20our%20experiments%2C%20only%20one%20configuration%20resulted%20in%20satisfactory%0Aexploratory%20behavior%3A%20GPT-4%20with%20chain-of-thought%20reasoning%20and%20an%20externally%0Asummarized%20interaction%20history%2C%20presented%20as%20sufficient%20statistics%3B%20ii%29%20All%0Aother%20configurations%20did%20not%20result%20in%20robust%20exploratory%20behavior%2C%20including%0Athose%20with%20chain-of-thought%20reasoning%20but%20unsummarized%20history.%20Although%20these%0Afindings%20can%20be%20interpreted%20positively%2C%20they%20suggest%20that%20external%0Asummarization%20--%20which%20may%20not%20be%20possible%20in%20more%20complex%20settings%20--%20is%0Aimportant%20for%20obtaining%20desirable%20behavior%20from%20LLM%20agents.%20We%20conclude%20that%0Anon-trivial%20algorithmic%20interventions%2C%20such%20as%20fine-tuning%20or%20dataset%20curation%2C%0Amay%20be%20required%20to%20empower%20LLM-based%20decision%20making%20agents%20in%20complex%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15371v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520large%2520language%2520models%2520explore%2520in-context%253F%26entry.906535625%3DAkshay%2520Krishnamurthy%2520and%2520Keegan%2520Harris%2520and%2520Dylan%2520J.%2520Foster%2520and%2520Cyril%2520Zhang%2520and%2520Aleksandrs%2520Slivkins%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520extent%2520to%2520which%2520contemporary%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Acan%2520engage%2520in%2520exploration%252C%2520a%2520core%2520capability%2520in%2520reinforcement%2520learning%2520and%250Adecision%2520making.%2520We%2520focus%2520on%2520native%2520performance%2520of%2520existing%2520LLMs%252C%2520without%250Atraining%2520interventions.%2520We%2520deploy%2520LLMs%2520as%2520agents%2520in%2520simple%2520multi-armed%2520bandit%250Aenvironments%252C%2520specifying%2520the%2520environment%2520description%2520and%2520interaction%2520history%250Aentirely%2520in-context%252C%2520i.e.%252C%2520within%2520the%2520LLM%2520prompt.%2520We%2520experiment%2520with%2520GPT-3.5%252C%250AGPT-4%252C%2520and%2520Llama2%252C%2520using%2520a%2520variety%2520of%2520prompt%2520designs%252C%2520and%2520find%2520that%2520the%2520models%250Ado%2520not%2520robustly%2520engage%2520in%2520exploration%2520without%2520substantial%2520interventions%253A%2520i%2529%250AAcross%2520all%2520of%2520our%2520experiments%252C%2520only%2520one%2520configuration%2520resulted%2520in%2520satisfactory%250Aexploratory%2520behavior%253A%2520GPT-4%2520with%2520chain-of-thought%2520reasoning%2520and%2520an%2520externally%250Asummarized%2520interaction%2520history%252C%2520presented%2520as%2520sufficient%2520statistics%253B%2520ii%2529%2520All%250Aother%2520configurations%2520did%2520not%2520result%2520in%2520robust%2520exploratory%2520behavior%252C%2520including%250Athose%2520with%2520chain-of-thought%2520reasoning%2520but%2520unsummarized%2520history.%2520Although%2520these%250Afindings%2520can%2520be%2520interpreted%2520positively%252C%2520they%2520suggest%2520that%2520external%250Asummarization%2520--%2520which%2520may%2520not%2520be%2520possible%2520in%2520more%2520complex%2520settings%2520--%2520is%250Aimportant%2520for%2520obtaining%2520desirable%2520behavior%2520from%2520LLM%2520agents.%2520We%2520conclude%2520that%250Anon-trivial%2520algorithmic%2520interventions%252C%2520such%2520as%2520fine-tuning%2520or%2520dataset%2520curation%252C%250Amay%2520be%2520required%2520to%2520empower%2520LLM-based%2520decision%2520making%2520agents%2520in%2520complex%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15371v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20large%20language%20models%20explore%20in-context%3F&entry.906535625=Akshay%20Krishnamurthy%20and%20Keegan%20Harris%20and%20Dylan%20J.%20Foster%20and%20Cyril%20Zhang%20and%20Aleksandrs%20Slivkins&entry.1292438233=%20%20We%20investigate%20the%20extent%20to%20which%20contemporary%20Large%20Language%20Models%20%28LLMs%29%0Acan%20engage%20in%20exploration%2C%20a%20core%20capability%20in%20reinforcement%20learning%20and%0Adecision%20making.%20We%20focus%20on%20native%20performance%20of%20existing%20LLMs%2C%20without%0Atraining%20interventions.%20We%20deploy%20LLMs%20as%20agents%20in%20simple%20multi-armed%20bandit%0Aenvironments%2C%20specifying%20the%20environment%20description%20and%20interaction%20history%0Aentirely%20in-context%2C%20i.e.%2C%20within%20the%20LLM%20prompt.%20We%20experiment%20with%20GPT-3.5%2C%0AGPT-4%2C%20and%20Llama2%2C%20using%20a%20variety%20of%20prompt%20designs%2C%20and%20find%20that%20the%20models%0Ado%20not%20robustly%20engage%20in%20exploration%20without%20substantial%20interventions%3A%20i%29%0AAcross%20all%20of%20our%20experiments%2C%20only%20one%20configuration%20resulted%20in%20satisfactory%0Aexploratory%20behavior%3A%20GPT-4%20with%20chain-of-thought%20reasoning%20and%20an%20externally%0Asummarized%20interaction%20history%2C%20presented%20as%20sufficient%20statistics%3B%20ii%29%20All%0Aother%20configurations%20did%20not%20result%20in%20robust%20exploratory%20behavior%2C%20including%0Athose%20with%20chain-of-thought%20reasoning%20but%20unsummarized%20history.%20Although%20these%0Afindings%20can%20be%20interpreted%20positively%2C%20they%20suggest%20that%20external%0Asummarization%20--%20which%20may%20not%20be%20possible%20in%20more%20complex%20settings%20--%20is%0Aimportant%20for%20obtaining%20desirable%20behavior%20from%20LLM%20agents.%20We%20conclude%20that%0Anon-trivial%20algorithmic%20interventions%2C%20such%20as%20fine-tuning%20or%20dataset%20curation%2C%0Amay%20be%20required%20to%20empower%20LLM-based%20decision%20making%20agents%20in%20complex%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15371v2&entry.124074799=Read"},
{"title": "Semantic UV mapping to improve texture inpainting for indoor scenes", "author": "Jelle Vermandere and Maarten Bassier and Maarten Vergauwen", "abstract": "  This work aims to improve texture inpainting after clutter removal in scanned\nindoor meshes. This is achieved with a new UV mapping pre-processing step which\nleverages semantic information of indoor scenes to more accurately match the UV\nislands with the 3D representation of distinct structural elements like walls\nand floors. Semantic UV Mapping enriches classic UV unwrapping algorithms by\nnot only relying on geometric features but also visual features originating\nfrom the present texture. The segmentation improves the UV mapping and\nsimultaneously simplifies the 3D geometric reconstruction of the scene after\nthe removal of loose objects. Each segmented element can be reconstructed\nseparately using the boundary conditions of the adjacent elements. Because this\nis performed as a pre-processing step, other specialized methods for geometric\nand texture reconstruction can be used in the future to improve the results\neven further.\n", "link": "http://arxiv.org/abs/2407.09248v1", "date": "2024-07-12", "relevancy": 1.9449, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4872}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20UV%20mapping%20to%20improve%20texture%20inpainting%20for%20indoor%20scenes&body=Title%3A%20Semantic%20UV%20mapping%20to%20improve%20texture%20inpainting%20for%20indoor%20scenes%0AAuthor%3A%20Jelle%20Vermandere%20and%20Maarten%20Bassier%20and%20Maarten%20Vergauwen%0AAbstract%3A%20%20%20This%20work%20aims%20to%20improve%20texture%20inpainting%20after%20clutter%20removal%20in%20scanned%0Aindoor%20meshes.%20This%20is%20achieved%20with%20a%20new%20UV%20mapping%20pre-processing%20step%20which%0Aleverages%20semantic%20information%20of%20indoor%20scenes%20to%20more%20accurately%20match%20the%20UV%0Aislands%20with%20the%203D%20representation%20of%20distinct%20structural%20elements%20like%20walls%0Aand%20floors.%20Semantic%20UV%20Mapping%20enriches%20classic%20UV%20unwrapping%20algorithms%20by%0Anot%20only%20relying%20on%20geometric%20features%20but%20also%20visual%20features%20originating%0Afrom%20the%20present%20texture.%20The%20segmentation%20improves%20the%20UV%20mapping%20and%0Asimultaneously%20simplifies%20the%203D%20geometric%20reconstruction%20of%20the%20scene%20after%0Athe%20removal%20of%20loose%20objects.%20Each%20segmented%20element%20can%20be%20reconstructed%0Aseparately%20using%20the%20boundary%20conditions%20of%20the%20adjacent%20elements.%20Because%20this%0Ais%20performed%20as%20a%20pre-processing%20step%2C%20other%20specialized%20methods%20for%20geometric%0Aand%20texture%20reconstruction%20can%20be%20used%20in%20the%20future%20to%20improve%20the%20results%0Aeven%20further.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520UV%2520mapping%2520to%2520improve%2520texture%2520inpainting%2520for%2520indoor%2520scenes%26entry.906535625%3DJelle%2520Vermandere%2520and%2520Maarten%2520Bassier%2520and%2520Maarten%2520Vergauwen%26entry.1292438233%3D%2520%2520This%2520work%2520aims%2520to%2520improve%2520texture%2520inpainting%2520after%2520clutter%2520removal%2520in%2520scanned%250Aindoor%2520meshes.%2520This%2520is%2520achieved%2520with%2520a%2520new%2520UV%2520mapping%2520pre-processing%2520step%2520which%250Aleverages%2520semantic%2520information%2520of%2520indoor%2520scenes%2520to%2520more%2520accurately%2520match%2520the%2520UV%250Aislands%2520with%2520the%25203D%2520representation%2520of%2520distinct%2520structural%2520elements%2520like%2520walls%250Aand%2520floors.%2520Semantic%2520UV%2520Mapping%2520enriches%2520classic%2520UV%2520unwrapping%2520algorithms%2520by%250Anot%2520only%2520relying%2520on%2520geometric%2520features%2520but%2520also%2520visual%2520features%2520originating%250Afrom%2520the%2520present%2520texture.%2520The%2520segmentation%2520improves%2520the%2520UV%2520mapping%2520and%250Asimultaneously%2520simplifies%2520the%25203D%2520geometric%2520reconstruction%2520of%2520the%2520scene%2520after%250Athe%2520removal%2520of%2520loose%2520objects.%2520Each%2520segmented%2520element%2520can%2520be%2520reconstructed%250Aseparately%2520using%2520the%2520boundary%2520conditions%2520of%2520the%2520adjacent%2520elements.%2520Because%2520this%250Ais%2520performed%2520as%2520a%2520pre-processing%2520step%252C%2520other%2520specialized%2520methods%2520for%2520geometric%250Aand%2520texture%2520reconstruction%2520can%2520be%2520used%2520in%2520the%2520future%2520to%2520improve%2520the%2520results%250Aeven%2520further.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20UV%20mapping%20to%20improve%20texture%20inpainting%20for%20indoor%20scenes&entry.906535625=Jelle%20Vermandere%20and%20Maarten%20Bassier%20and%20Maarten%20Vergauwen&entry.1292438233=%20%20This%20work%20aims%20to%20improve%20texture%20inpainting%20after%20clutter%20removal%20in%20scanned%0Aindoor%20meshes.%20This%20is%20achieved%20with%20a%20new%20UV%20mapping%20pre-processing%20step%20which%0Aleverages%20semantic%20information%20of%20indoor%20scenes%20to%20more%20accurately%20match%20the%20UV%0Aislands%20with%20the%203D%20representation%20of%20distinct%20structural%20elements%20like%20walls%0Aand%20floors.%20Semantic%20UV%20Mapping%20enriches%20classic%20UV%20unwrapping%20algorithms%20by%0Anot%20only%20relying%20on%20geometric%20features%20but%20also%20visual%20features%20originating%0Afrom%20the%20present%20texture.%20The%20segmentation%20improves%20the%20UV%20mapping%20and%0Asimultaneously%20simplifies%20the%203D%20geometric%20reconstruction%20of%20the%20scene%20after%0Athe%20removal%20of%20loose%20objects.%20Each%20segmented%20element%20can%20be%20reconstructed%0Aseparately%20using%20the%20boundary%20conditions%20of%20the%20adjacent%20elements.%20Because%20this%0Ais%20performed%20as%20a%20pre-processing%20step%2C%20other%20specialized%20methods%20for%20geometric%0Aand%20texture%20reconstruction%20can%20be%20used%20in%20the%20future%20to%20improve%20the%20results%0Aeven%20further.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09248v1&entry.124074799=Read"},
{"title": "Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across\n  Language Models", "author": "Zijun Wu and Yongkang Wu and Lili Mou", "abstract": "  Prompt tuning in natural language processing (NLP) has become an increasingly\npopular method for adapting large language models to specific tasks. However,\nthe transferability of these prompts, especially continuous prompts, between\ndifferent models remains a challenge. In this work, we propose a zero-shot\ncontinuous prompt transfer method, where source prompts are encoded into\nrelative space and the corresponding target prompts are searched for\ntransferring to target models. Experimental results confirm the effectiveness\nof our method, showing that 'task semantics' in continuous prompts can be\ngeneralized across various language models. Moreover, we find that combining\n'task semantics' from multiple source models can further enhance the\ngeneralizability of transfer.\n", "link": "http://arxiv.org/abs/2310.01691v2", "date": "2024-07-12", "relevancy": 1.9361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4973}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4761}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Continuous%20Prompt%20Transfer%3A%20Generalizing%20Task%20Semantics%20Across%0A%20%20Language%20Models&body=Title%3A%20Zero-Shot%20Continuous%20Prompt%20Transfer%3A%20Generalizing%20Task%20Semantics%20Across%0A%20%20Language%20Models%0AAuthor%3A%20Zijun%20Wu%20and%20Yongkang%20Wu%20and%20Lili%20Mou%0AAbstract%3A%20%20%20Prompt%20tuning%20in%20natural%20language%20processing%20%28NLP%29%20has%20become%20an%20increasingly%0Apopular%20method%20for%20adapting%20large%20language%20models%20to%20specific%20tasks.%20However%2C%0Athe%20transferability%20of%20these%20prompts%2C%20especially%20continuous%20prompts%2C%20between%0Adifferent%20models%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20propose%20a%20zero-shot%0Acontinuous%20prompt%20transfer%20method%2C%20where%20source%20prompts%20are%20encoded%20into%0Arelative%20space%20and%20the%20corresponding%20target%20prompts%20are%20searched%20for%0Atransferring%20to%20target%20models.%20Experimental%20results%20confirm%20the%20effectiveness%0Aof%20our%20method%2C%20showing%20that%20%27task%20semantics%27%20in%20continuous%20prompts%20can%20be%0Ageneralized%20across%20various%20language%20models.%20Moreover%2C%20we%20find%20that%20combining%0A%27task%20semantics%27%20from%20multiple%20source%20models%20can%20further%20enhance%20the%0Ageneralizability%20of%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Continuous%2520Prompt%2520Transfer%253A%2520Generalizing%2520Task%2520Semantics%2520Across%250A%2520%2520Language%2520Models%26entry.906535625%3DZijun%2520Wu%2520and%2520Yongkang%2520Wu%2520and%2520Lili%2520Mou%26entry.1292438233%3D%2520%2520Prompt%2520tuning%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%2520has%2520become%2520an%2520increasingly%250Apopular%2520method%2520for%2520adapting%2520large%2520language%2520models%2520to%2520specific%2520tasks.%2520However%252C%250Athe%2520transferability%2520of%2520these%2520prompts%252C%2520especially%2520continuous%2520prompts%252C%2520between%250Adifferent%2520models%2520remains%2520a%2520challenge.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520zero-shot%250Acontinuous%2520prompt%2520transfer%2520method%252C%2520where%2520source%2520prompts%2520are%2520encoded%2520into%250Arelative%2520space%2520and%2520the%2520corresponding%2520target%2520prompts%2520are%2520searched%2520for%250Atransferring%2520to%2520target%2520models.%2520Experimental%2520results%2520confirm%2520the%2520effectiveness%250Aof%2520our%2520method%252C%2520showing%2520that%2520%2527task%2520semantics%2527%2520in%2520continuous%2520prompts%2520can%2520be%250Ageneralized%2520across%2520various%2520language%2520models.%2520Moreover%252C%2520we%2520find%2520that%2520combining%250A%2527task%2520semantics%2527%2520from%2520multiple%2520source%2520models%2520can%2520further%2520enhance%2520the%250Ageneralizability%2520of%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Continuous%20Prompt%20Transfer%3A%20Generalizing%20Task%20Semantics%20Across%0A%20%20Language%20Models&entry.906535625=Zijun%20Wu%20and%20Yongkang%20Wu%20and%20Lili%20Mou&entry.1292438233=%20%20Prompt%20tuning%20in%20natural%20language%20processing%20%28NLP%29%20has%20become%20an%20increasingly%0Apopular%20method%20for%20adapting%20large%20language%20models%20to%20specific%20tasks.%20However%2C%0Athe%20transferability%20of%20these%20prompts%2C%20especially%20continuous%20prompts%2C%20between%0Adifferent%20models%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20propose%20a%20zero-shot%0Acontinuous%20prompt%20transfer%20method%2C%20where%20source%20prompts%20are%20encoded%20into%0Arelative%20space%20and%20the%20corresponding%20target%20prompts%20are%20searched%20for%0Atransferring%20to%20target%20models.%20Experimental%20results%20confirm%20the%20effectiveness%0Aof%20our%20method%2C%20showing%20that%20%27task%20semantics%27%20in%20continuous%20prompts%20can%20be%0Ageneralized%20across%20various%20language%20models.%20Moreover%2C%20we%20find%20that%20combining%0A%27task%20semantics%27%20from%20multiple%20source%20models%20can%20further%20enhance%20the%0Ageneralizability%20of%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01691v2&entry.124074799=Read"},
{"title": "The Two Sides of the Coin: Hallucination Generation and Detection with\n  LLMs as Evaluators for LLMs", "author": "Anh Thu Maria Bui and Saskia Felizitas Brech and Natalie Hu\u00dffeldt and Tobias Jennert and Melanie Ullrich and Timo Breuer and Narjes Nikzad Khasmakhi and Philipp Schaer", "abstract": "  Hallucination detection in Large Language Models (LLMs) is crucial for\nensuring their reliability. This work presents our participation in the CLEF\nELOQUENT HalluciGen shared task, where the goal is to develop evaluators for\nboth generating and detecting hallucinated content. We explored the\ncapabilities of four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, for this\npurpose. We also employed ensemble majority voting to incorporate all four\nmodels for the detection task. The results provide valuable insights into the\nstrengths and weaknesses of these LLMs in handling hallucination generation and\ndetection tasks.\n", "link": "http://arxiv.org/abs/2407.09152v1", "date": "2024-07-12", "relevancy": 1.9123, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5248}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4805}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Two%20Sides%20of%20the%20Coin%3A%20Hallucination%20Generation%20and%20Detection%20with%0A%20%20LLMs%20as%20Evaluators%20for%20LLMs&body=Title%3A%20The%20Two%20Sides%20of%20the%20Coin%3A%20Hallucination%20Generation%20and%20Detection%20with%0A%20%20LLMs%20as%20Evaluators%20for%20LLMs%0AAuthor%3A%20Anh%20Thu%20Maria%20Bui%20and%20Saskia%20Felizitas%20Brech%20and%20Natalie%20Hu%C3%9Ffeldt%20and%20Tobias%20Jennert%20and%20Melanie%20Ullrich%20and%20Timo%20Breuer%20and%20Narjes%20Nikzad%20Khasmakhi%20and%20Philipp%20Schaer%0AAbstract%3A%20%20%20Hallucination%20detection%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20crucial%20for%0Aensuring%20their%20reliability.%20This%20work%20presents%20our%20participation%20in%20the%20CLEF%0AELOQUENT%20HalluciGen%20shared%20task%2C%20where%20the%20goal%20is%20to%20develop%20evaluators%20for%0Aboth%20generating%20and%20detecting%20hallucinated%20content.%20We%20explored%20the%0Acapabilities%20of%20four%20LLMs%3A%20Llama%203%2C%20Gemma%2C%20GPT-3.5%20Turbo%2C%20and%20GPT-4%2C%20for%20this%0Apurpose.%20We%20also%20employed%20ensemble%20majority%20voting%20to%20incorporate%20all%20four%0Amodels%20for%20the%20detection%20task.%20The%20results%20provide%20valuable%20insights%20into%20the%0Astrengths%20and%20weaknesses%20of%20these%20LLMs%20in%20handling%20hallucination%20generation%20and%0Adetection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Two%2520Sides%2520of%2520the%2520Coin%253A%2520Hallucination%2520Generation%2520and%2520Detection%2520with%250A%2520%2520LLMs%2520as%2520Evaluators%2520for%2520LLMs%26entry.906535625%3DAnh%2520Thu%2520Maria%2520Bui%2520and%2520Saskia%2520Felizitas%2520Brech%2520and%2520Natalie%2520Hu%25C3%259Ffeldt%2520and%2520Tobias%2520Jennert%2520and%2520Melanie%2520Ullrich%2520and%2520Timo%2520Breuer%2520and%2520Narjes%2520Nikzad%2520Khasmakhi%2520and%2520Philipp%2520Schaer%26entry.1292438233%3D%2520%2520Hallucination%2520detection%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520crucial%2520for%250Aensuring%2520their%2520reliability.%2520This%2520work%2520presents%2520our%2520participation%2520in%2520the%2520CLEF%250AELOQUENT%2520HalluciGen%2520shared%2520task%252C%2520where%2520the%2520goal%2520is%2520to%2520develop%2520evaluators%2520for%250Aboth%2520generating%2520and%2520detecting%2520hallucinated%2520content.%2520We%2520explored%2520the%250Acapabilities%2520of%2520four%2520LLMs%253A%2520Llama%25203%252C%2520Gemma%252C%2520GPT-3.5%2520Turbo%252C%2520and%2520GPT-4%252C%2520for%2520this%250Apurpose.%2520We%2520also%2520employed%2520ensemble%2520majority%2520voting%2520to%2520incorporate%2520all%2520four%250Amodels%2520for%2520the%2520detection%2520task.%2520The%2520results%2520provide%2520valuable%2520insights%2520into%2520the%250Astrengths%2520and%2520weaknesses%2520of%2520these%2520LLMs%2520in%2520handling%2520hallucination%2520generation%2520and%250Adetection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Two%20Sides%20of%20the%20Coin%3A%20Hallucination%20Generation%20and%20Detection%20with%0A%20%20LLMs%20as%20Evaluators%20for%20LLMs&entry.906535625=Anh%20Thu%20Maria%20Bui%20and%20Saskia%20Felizitas%20Brech%20and%20Natalie%20Hu%C3%9Ffeldt%20and%20Tobias%20Jennert%20and%20Melanie%20Ullrich%20and%20Timo%20Breuer%20and%20Narjes%20Nikzad%20Khasmakhi%20and%20Philipp%20Schaer&entry.1292438233=%20%20Hallucination%20detection%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20crucial%20for%0Aensuring%20their%20reliability.%20This%20work%20presents%20our%20participation%20in%20the%20CLEF%0AELOQUENT%20HalluciGen%20shared%20task%2C%20where%20the%20goal%20is%20to%20develop%20evaluators%20for%0Aboth%20generating%20and%20detecting%20hallucinated%20content.%20We%20explored%20the%0Acapabilities%20of%20four%20LLMs%3A%20Llama%203%2C%20Gemma%2C%20GPT-3.5%20Turbo%2C%20and%20GPT-4%2C%20for%20this%0Apurpose.%20We%20also%20employed%20ensemble%20majority%20voting%20to%20incorporate%20all%20four%0Amodels%20for%20the%20detection%20task.%20The%20results%20provide%20valuable%20insights%20into%20the%0Astrengths%20and%20weaknesses%20of%20these%20LLMs%20in%20handling%20hallucination%20generation%20and%0Adetection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09152v1&entry.124074799=Read"},
{"title": "The Effectiveness of Curvature-Based Rewiring and the Role of\n  Hyperparameters in GNNs Revisited", "author": "Floriano Tori and Vincent Holst and Vincent Ginis", "abstract": "  Message passing is the dominant paradigm in Graph Neural Networks (GNNs). The\nefficiency of message passing, however, can be limited by the topology of the\ngraph. This happens when information is lost during propagation due to being\noversquashed when travelling through bottlenecks. To remedy this, recent\nefforts have focused on graph rewiring techniques, which disconnect the input\ngraph originating from the data and the computational graph, on which message\npassing is performed. A prominent approach for this is to use discrete graph\ncurvature measures, of which several variants have been proposed, to identify\nand rewire around bottlenecks, facilitating information propagation. While\noversquashing has been demonstrated in synthetic datasets, in this work we\nreevaluate the performance gains that curvature-based rewiring brings to\nreal-world datasets. We show that in these datasets, edges selected during the\nrewiring process are not in line with theoretical criteria identifying\nbottlenecks. This implies they do not necessarily oversquash information during\nmessage passing. Subsequently, we demonstrate that SOTA accuracies on these\ndatasets are outliers originating from sweeps of hyperparameters -- both the\nones for training and dedicated ones related to the rewiring algorithm --\ninstead of consistent performance gains. In conclusion, our analysis nuances\nthe effectiveness of curvature-based rewiring in real-world datasets and brings\na new perspective on the methods to evaluate GNN accuracy improvements.\n", "link": "http://arxiv.org/abs/2407.09381v1", "date": "2024-07-12", "relevancy": 1.9104, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5014}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4792}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Effectiveness%20of%20Curvature-Based%20Rewiring%20and%20the%20Role%20of%0A%20%20Hyperparameters%20in%20GNNs%20Revisited&body=Title%3A%20The%20Effectiveness%20of%20Curvature-Based%20Rewiring%20and%20the%20Role%20of%0A%20%20Hyperparameters%20in%20GNNs%20Revisited%0AAuthor%3A%20Floriano%20Tori%20and%20Vincent%20Holst%20and%20Vincent%20Ginis%0AAbstract%3A%20%20%20Message%20passing%20is%20the%20dominant%20paradigm%20in%20Graph%20Neural%20Networks%20%28GNNs%29.%20The%0Aefficiency%20of%20message%20passing%2C%20however%2C%20can%20be%20limited%20by%20the%20topology%20of%20the%0Agraph.%20This%20happens%20when%20information%20is%20lost%20during%20propagation%20due%20to%20being%0Aoversquashed%20when%20travelling%20through%20bottlenecks.%20To%20remedy%20this%2C%20recent%0Aefforts%20have%20focused%20on%20graph%20rewiring%20techniques%2C%20which%20disconnect%20the%20input%0Agraph%20originating%20from%20the%20data%20and%20the%20computational%20graph%2C%20on%20which%20message%0Apassing%20is%20performed.%20A%20prominent%20approach%20for%20this%20is%20to%20use%20discrete%20graph%0Acurvature%20measures%2C%20of%20which%20several%20variants%20have%20been%20proposed%2C%20to%20identify%0Aand%20rewire%20around%20bottlenecks%2C%20facilitating%20information%20propagation.%20While%0Aoversquashing%20has%20been%20demonstrated%20in%20synthetic%20datasets%2C%20in%20this%20work%20we%0Areevaluate%20the%20performance%20gains%20that%20curvature-based%20rewiring%20brings%20to%0Areal-world%20datasets.%20We%20show%20that%20in%20these%20datasets%2C%20edges%20selected%20during%20the%0Arewiring%20process%20are%20not%20in%20line%20with%20theoretical%20criteria%20identifying%0Abottlenecks.%20This%20implies%20they%20do%20not%20necessarily%20oversquash%20information%20during%0Amessage%20passing.%20Subsequently%2C%20we%20demonstrate%20that%20SOTA%20accuracies%20on%20these%0Adatasets%20are%20outliers%20originating%20from%20sweeps%20of%20hyperparameters%20--%20both%20the%0Aones%20for%20training%20and%20dedicated%20ones%20related%20to%20the%20rewiring%20algorithm%20--%0Ainstead%20of%20consistent%20performance%20gains.%20In%20conclusion%2C%20our%20analysis%20nuances%0Athe%20effectiveness%20of%20curvature-based%20rewiring%20in%20real-world%20datasets%20and%20brings%0Aa%20new%20perspective%20on%20the%20methods%20to%20evaluate%20GNN%20accuracy%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Effectiveness%2520of%2520Curvature-Based%2520Rewiring%2520and%2520the%2520Role%2520of%250A%2520%2520Hyperparameters%2520in%2520GNNs%2520Revisited%26entry.906535625%3DFloriano%2520Tori%2520and%2520Vincent%2520Holst%2520and%2520Vincent%2520Ginis%26entry.1292438233%3D%2520%2520Message%2520passing%2520is%2520the%2520dominant%2520paradigm%2520in%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520The%250Aefficiency%2520of%2520message%2520passing%252C%2520however%252C%2520can%2520be%2520limited%2520by%2520the%2520topology%2520of%2520the%250Agraph.%2520This%2520happens%2520when%2520information%2520is%2520lost%2520during%2520propagation%2520due%2520to%2520being%250Aoversquashed%2520when%2520travelling%2520through%2520bottlenecks.%2520To%2520remedy%2520this%252C%2520recent%250Aefforts%2520have%2520focused%2520on%2520graph%2520rewiring%2520techniques%252C%2520which%2520disconnect%2520the%2520input%250Agraph%2520originating%2520from%2520the%2520data%2520and%2520the%2520computational%2520graph%252C%2520on%2520which%2520message%250Apassing%2520is%2520performed.%2520A%2520prominent%2520approach%2520for%2520this%2520is%2520to%2520use%2520discrete%2520graph%250Acurvature%2520measures%252C%2520of%2520which%2520several%2520variants%2520have%2520been%2520proposed%252C%2520to%2520identify%250Aand%2520rewire%2520around%2520bottlenecks%252C%2520facilitating%2520information%2520propagation.%2520While%250Aoversquashing%2520has%2520been%2520demonstrated%2520in%2520synthetic%2520datasets%252C%2520in%2520this%2520work%2520we%250Areevaluate%2520the%2520performance%2520gains%2520that%2520curvature-based%2520rewiring%2520brings%2520to%250Areal-world%2520datasets.%2520We%2520show%2520that%2520in%2520these%2520datasets%252C%2520edges%2520selected%2520during%2520the%250Arewiring%2520process%2520are%2520not%2520in%2520line%2520with%2520theoretical%2520criteria%2520identifying%250Abottlenecks.%2520This%2520implies%2520they%2520do%2520not%2520necessarily%2520oversquash%2520information%2520during%250Amessage%2520passing.%2520Subsequently%252C%2520we%2520demonstrate%2520that%2520SOTA%2520accuracies%2520on%2520these%250Adatasets%2520are%2520outliers%2520originating%2520from%2520sweeps%2520of%2520hyperparameters%2520--%2520both%2520the%250Aones%2520for%2520training%2520and%2520dedicated%2520ones%2520related%2520to%2520the%2520rewiring%2520algorithm%2520--%250Ainstead%2520of%2520consistent%2520performance%2520gains.%2520In%2520conclusion%252C%2520our%2520analysis%2520nuances%250Athe%2520effectiveness%2520of%2520curvature-based%2520rewiring%2520in%2520real-world%2520datasets%2520and%2520brings%250Aa%2520new%2520perspective%2520on%2520the%2520methods%2520to%2520evaluate%2520GNN%2520accuracy%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Effectiveness%20of%20Curvature-Based%20Rewiring%20and%20the%20Role%20of%0A%20%20Hyperparameters%20in%20GNNs%20Revisited&entry.906535625=Floriano%20Tori%20and%20Vincent%20Holst%20and%20Vincent%20Ginis&entry.1292438233=%20%20Message%20passing%20is%20the%20dominant%20paradigm%20in%20Graph%20Neural%20Networks%20%28GNNs%29.%20The%0Aefficiency%20of%20message%20passing%2C%20however%2C%20can%20be%20limited%20by%20the%20topology%20of%20the%0Agraph.%20This%20happens%20when%20information%20is%20lost%20during%20propagation%20due%20to%20being%0Aoversquashed%20when%20travelling%20through%20bottlenecks.%20To%20remedy%20this%2C%20recent%0Aefforts%20have%20focused%20on%20graph%20rewiring%20techniques%2C%20which%20disconnect%20the%20input%0Agraph%20originating%20from%20the%20data%20and%20the%20computational%20graph%2C%20on%20which%20message%0Apassing%20is%20performed.%20A%20prominent%20approach%20for%20this%20is%20to%20use%20discrete%20graph%0Acurvature%20measures%2C%20of%20which%20several%20variants%20have%20been%20proposed%2C%20to%20identify%0Aand%20rewire%20around%20bottlenecks%2C%20facilitating%20information%20propagation.%20While%0Aoversquashing%20has%20been%20demonstrated%20in%20synthetic%20datasets%2C%20in%20this%20work%20we%0Areevaluate%20the%20performance%20gains%20that%20curvature-based%20rewiring%20brings%20to%0Areal-world%20datasets.%20We%20show%20that%20in%20these%20datasets%2C%20edges%20selected%20during%20the%0Arewiring%20process%20are%20not%20in%20line%20with%20theoretical%20criteria%20identifying%0Abottlenecks.%20This%20implies%20they%20do%20not%20necessarily%20oversquash%20information%20during%0Amessage%20passing.%20Subsequently%2C%20we%20demonstrate%20that%20SOTA%20accuracies%20on%20these%0Adatasets%20are%20outliers%20originating%20from%20sweeps%20of%20hyperparameters%20--%20both%20the%0Aones%20for%20training%20and%20dedicated%20ones%20related%20to%20the%20rewiring%20algorithm%20--%0Ainstead%20of%20consistent%20performance%20gains.%20In%20conclusion%2C%20our%20analysis%20nuances%0Athe%20effectiveness%20of%20curvature-based%20rewiring%20in%20real-world%20datasets%20and%20brings%0Aa%20new%20perspective%20on%20the%20methods%20to%20evaluate%20GNN%20accuracy%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09381v1&entry.124074799=Read"},
{"title": "Motion-Guided Latent Diffusion for Temporally Consistent Real-world\n  Video Super-resolution", "author": "Xi Yang and Chenhang He and Jianqi Ma and Lei Zhang", "abstract": "  Real-world low-resolution (LR) videos have diverse and complex degradations,\nimposing great challenges on video super-resolution (VSR) algorithms to\nreproduce their high-resolution (HR) counterparts with high quality. Recently,\nthe diffusion models have shown compelling performance in generating realistic\ndetails for image restoration tasks. However, the diffusion process has\nrandomness, making it hard to control the contents of restored images. This\nissue becomes more serious when applying diffusion models to VSR tasks because\ntemporal consistency is crucial to the perceptual quality of videos. In this\npaper, we propose an effective real-world VSR algorithm by leveraging the\nstrength of pre-trained latent diffusion models. To ensure the content\nconsistency among adjacent frames, we exploit the temporal dynamics in LR\nvideos to guide the diffusion process by optimizing the latent sampling path\nwith a motion-guided loss, ensuring that the generated HR video maintains a\ncoherent and continuous visual flow. To further mitigate the discontinuity of\ngenerated details, we insert temporal module to the decoder and fine-tune it\nwith an innovative sequence-oriented loss. The proposed motion-guided latent\ndiffusion (MGLD) based VSR algorithm achieves significantly better perceptual\nquality than state-of-the-arts on real-world VSR benchmark datasets, validating\nthe effectiveness of the proposed model design and training strategies.\n", "link": "http://arxiv.org/abs/2312.00853v2", "date": "2024-07-12", "relevancy": 1.9045, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6745}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6551}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-Guided%20Latent%20Diffusion%20for%20Temporally%20Consistent%20Real-world%0A%20%20Video%20Super-resolution&body=Title%3A%20Motion-Guided%20Latent%20Diffusion%20for%20Temporally%20Consistent%20Real-world%0A%20%20Video%20Super-resolution%0AAuthor%3A%20Xi%20Yang%20and%20Chenhang%20He%20and%20Jianqi%20Ma%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Real-world%20low-resolution%20%28LR%29%20videos%20have%20diverse%20and%20complex%20degradations%2C%0Aimposing%20great%20challenges%20on%20video%20super-resolution%20%28VSR%29%20algorithms%20to%0Areproduce%20their%20high-resolution%20%28HR%29%20counterparts%20with%20high%20quality.%20Recently%2C%0Athe%20diffusion%20models%20have%20shown%20compelling%20performance%20in%20generating%20realistic%0Adetails%20for%20image%20restoration%20tasks.%20However%2C%20the%20diffusion%20process%20has%0Arandomness%2C%20making%20it%20hard%20to%20control%20the%20contents%20of%20restored%20images.%20This%0Aissue%20becomes%20more%20serious%20when%20applying%20diffusion%20models%20to%20VSR%20tasks%20because%0Atemporal%20consistency%20is%20crucial%20to%20the%20perceptual%20quality%20of%20videos.%20In%20this%0Apaper%2C%20we%20propose%20an%20effective%20real-world%20VSR%20algorithm%20by%20leveraging%20the%0Astrength%20of%20pre-trained%20latent%20diffusion%20models.%20To%20ensure%20the%20content%0Aconsistency%20among%20adjacent%20frames%2C%20we%20exploit%20the%20temporal%20dynamics%20in%20LR%0Avideos%20to%20guide%20the%20diffusion%20process%20by%20optimizing%20the%20latent%20sampling%20path%0Awith%20a%20motion-guided%20loss%2C%20ensuring%20that%20the%20generated%20HR%20video%20maintains%20a%0Acoherent%20and%20continuous%20visual%20flow.%20To%20further%20mitigate%20the%20discontinuity%20of%0Agenerated%20details%2C%20we%20insert%20temporal%20module%20to%20the%20decoder%20and%20fine-tune%20it%0Awith%20an%20innovative%20sequence-oriented%20loss.%20The%20proposed%20motion-guided%20latent%0Adiffusion%20%28MGLD%29%20based%20VSR%20algorithm%20achieves%20significantly%20better%20perceptual%0Aquality%20than%20state-of-the-arts%20on%20real-world%20VSR%20benchmark%20datasets%2C%20validating%0Athe%20effectiveness%20of%20the%20proposed%20model%20design%20and%20training%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-Guided%2520Latent%2520Diffusion%2520for%2520Temporally%2520Consistent%2520Real-world%250A%2520%2520Video%2520Super-resolution%26entry.906535625%3DXi%2520Yang%2520and%2520Chenhang%2520He%2520and%2520Jianqi%2520Ma%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Real-world%2520low-resolution%2520%2528LR%2529%2520videos%2520have%2520diverse%2520and%2520complex%2520degradations%252C%250Aimposing%2520great%2520challenges%2520on%2520video%2520super-resolution%2520%2528VSR%2529%2520algorithms%2520to%250Areproduce%2520their%2520high-resolution%2520%2528HR%2529%2520counterparts%2520with%2520high%2520quality.%2520Recently%252C%250Athe%2520diffusion%2520models%2520have%2520shown%2520compelling%2520performance%2520in%2520generating%2520realistic%250Adetails%2520for%2520image%2520restoration%2520tasks.%2520However%252C%2520the%2520diffusion%2520process%2520has%250Arandomness%252C%2520making%2520it%2520hard%2520to%2520control%2520the%2520contents%2520of%2520restored%2520images.%2520This%250Aissue%2520becomes%2520more%2520serious%2520when%2520applying%2520diffusion%2520models%2520to%2520VSR%2520tasks%2520because%250Atemporal%2520consistency%2520is%2520crucial%2520to%2520the%2520perceptual%2520quality%2520of%2520videos.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520an%2520effective%2520real-world%2520VSR%2520algorithm%2520by%2520leveraging%2520the%250Astrength%2520of%2520pre-trained%2520latent%2520diffusion%2520models.%2520To%2520ensure%2520the%2520content%250Aconsistency%2520among%2520adjacent%2520frames%252C%2520we%2520exploit%2520the%2520temporal%2520dynamics%2520in%2520LR%250Avideos%2520to%2520guide%2520the%2520diffusion%2520process%2520by%2520optimizing%2520the%2520latent%2520sampling%2520path%250Awith%2520a%2520motion-guided%2520loss%252C%2520ensuring%2520that%2520the%2520generated%2520HR%2520video%2520maintains%2520a%250Acoherent%2520and%2520continuous%2520visual%2520flow.%2520To%2520further%2520mitigate%2520the%2520discontinuity%2520of%250Agenerated%2520details%252C%2520we%2520insert%2520temporal%2520module%2520to%2520the%2520decoder%2520and%2520fine-tune%2520it%250Awith%2520an%2520innovative%2520sequence-oriented%2520loss.%2520The%2520proposed%2520motion-guided%2520latent%250Adiffusion%2520%2528MGLD%2529%2520based%2520VSR%2520algorithm%2520achieves%2520significantly%2520better%2520perceptual%250Aquality%2520than%2520state-of-the-arts%2520on%2520real-world%2520VSR%2520benchmark%2520datasets%252C%2520validating%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520model%2520design%2520and%2520training%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-Guided%20Latent%20Diffusion%20for%20Temporally%20Consistent%20Real-world%0A%20%20Video%20Super-resolution&entry.906535625=Xi%20Yang%20and%20Chenhang%20He%20and%20Jianqi%20Ma%20and%20Lei%20Zhang&entry.1292438233=%20%20Real-world%20low-resolution%20%28LR%29%20videos%20have%20diverse%20and%20complex%20degradations%2C%0Aimposing%20great%20challenges%20on%20video%20super-resolution%20%28VSR%29%20algorithms%20to%0Areproduce%20their%20high-resolution%20%28HR%29%20counterparts%20with%20high%20quality.%20Recently%2C%0Athe%20diffusion%20models%20have%20shown%20compelling%20performance%20in%20generating%20realistic%0Adetails%20for%20image%20restoration%20tasks.%20However%2C%20the%20diffusion%20process%20has%0Arandomness%2C%20making%20it%20hard%20to%20control%20the%20contents%20of%20restored%20images.%20This%0Aissue%20becomes%20more%20serious%20when%20applying%20diffusion%20models%20to%20VSR%20tasks%20because%0Atemporal%20consistency%20is%20crucial%20to%20the%20perceptual%20quality%20of%20videos.%20In%20this%0Apaper%2C%20we%20propose%20an%20effective%20real-world%20VSR%20algorithm%20by%20leveraging%20the%0Astrength%20of%20pre-trained%20latent%20diffusion%20models.%20To%20ensure%20the%20content%0Aconsistency%20among%20adjacent%20frames%2C%20we%20exploit%20the%20temporal%20dynamics%20in%20LR%0Avideos%20to%20guide%20the%20diffusion%20process%20by%20optimizing%20the%20latent%20sampling%20path%0Awith%20a%20motion-guided%20loss%2C%20ensuring%20that%20the%20generated%20HR%20video%20maintains%20a%0Acoherent%20and%20continuous%20visual%20flow.%20To%20further%20mitigate%20the%20discontinuity%20of%0Agenerated%20details%2C%20we%20insert%20temporal%20module%20to%20the%20decoder%20and%20fine-tune%20it%0Awith%20an%20innovative%20sequence-oriented%20loss.%20The%20proposed%20motion-guided%20latent%0Adiffusion%20%28MGLD%29%20based%20VSR%20algorithm%20achieves%20significantly%20better%20perceptual%0Aquality%20than%20state-of-the-arts%20on%20real-world%20VSR%20benchmark%20datasets%2C%20validating%0Athe%20effectiveness%20of%20the%20proposed%20model%20design%20and%20training%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00853v2&entry.124074799=Read"},
{"title": "Technique Report of CVPR 2024 PBDL Challenges", "author": "Ying Fu and Yu Li and Shaodi You and Boxin Shi and Linwei Chen and Yunhao Zou and Zichun Wang and Yichen Li and Yuze Han and Yingkai Zhang and Jianan Wang and Qinglin Liu and Wei Yu and Xiaoqian Lv and Jianing Li and Shengping Zhang and Xiangyang Ji and Yuanpei Chen and Yuhan Zhang and Weihang Peng and Liwen Zhang and Zhe Xu and Dingyong Gou and Cong Li and Senyan Xu and Yunkang Zhang and Siyuan Jiang and Xiaoqiang Lu and Licheng Jiao and Fang Liu and Xu Liu and Lingling Li and Wenping Ma and Shuyuan Yang and Haiyang Xie and Jian Zhao and Shihua Huang and Peng Cheng and Xi Shen and Zheng Wang and Shuai An and Caizhi Zhu and Xuelong Li and Tao Zhang and Liang Li and Yu Liu and Chenggang Yan and Gengchen Zhang and Linyan Jiang and Bingyi Song and Zhuoyu An and Haibo Lei and Qing Luo and Jie Song and Yuan Liu and Qihang Li and Haoyuan Zhang and Lingfeng Wang and Wei Chen and Aling Luo and Cheng Li and Jun Cao and Shu Chen and Zifei Dou and Xinyu Liu and Jing Zhang and Kexin Zhang and Yuting Yang and Xuejian Gou and Qinliang Wang and Yang Liu and Shizhan Zhao and Yanzhao Zhang and Libo Yan and Yuwei Guo and Guoxin Li and Qiong Gao and Chenyue Che and Long Sun and Xiang Chen and Hao Li and Jinshan Pan and Chuanlong Xie and Hongming Chen and Mingrui Li and Tianchen Deng and Jingwei Huang and Yufeng Li and Fei Wan and Bingxin Xu and Jian Cheng and Hongzhe Liu and Cheng Xu and Yuxiang Zou and Weiguo Pan and Songyin Dai and Sen Jia and Junpei Zhang and Puhua Chen and Qihang Li", "abstract": "  The intersection of physics-based vision and deep learning presents an\nexciting frontier for advancing computer vision technologies. By leveraging the\nprinciples of physics to inform and enhance deep learning models, we can\ndevelop more robust and accurate vision systems. Physics-based vision aims to\ninvert the processes to recover scene properties such as shape, reflectance,\nlight distribution, and medium properties from images. In recent years, deep\nlearning has shown promising improvements for various vision tasks, and when\ncombined with physics-based vision, these approaches can enhance the robustness\nand accuracy of vision systems. This technical report summarizes the outcomes\nof the Physics-Based Vision Meets Deep Learning (PBDL) 2024 challenge, held in\nCVPR 2024 workshop. The challenge consisted of eight tracks, focusing on\nLow-Light Enhancement and Detection as well as High Dynamic Range (HDR)\nImaging. This report details the objectives, methodologies, and results of each\ntrack, highlighting the top-performing solutions and their innovative\napproaches.\n", "link": "http://arxiv.org/abs/2406.10744v3", "date": "2024-07-12", "relevancy": 1.8991, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4887}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Technique%20Report%20of%20CVPR%202024%20PBDL%20Challenges&body=Title%3A%20Technique%20Report%20of%20CVPR%202024%20PBDL%20Challenges%0AAuthor%3A%20Ying%20Fu%20and%20Yu%20Li%20and%20Shaodi%20You%20and%20Boxin%20Shi%20and%20Linwei%20Chen%20and%20Yunhao%20Zou%20and%20Zichun%20Wang%20and%20Yichen%20Li%20and%20Yuze%20Han%20and%20Yingkai%20Zhang%20and%20Jianan%20Wang%20and%20Qinglin%20Liu%20and%20Wei%20Yu%20and%20Xiaoqian%20Lv%20and%20Jianing%20Li%20and%20Shengping%20Zhang%20and%20Xiangyang%20Ji%20and%20Yuanpei%20Chen%20and%20Yuhan%20Zhang%20and%20Weihang%20Peng%20and%20Liwen%20Zhang%20and%20Zhe%20Xu%20and%20Dingyong%20Gou%20and%20Cong%20Li%20and%20Senyan%20Xu%20and%20Yunkang%20Zhang%20and%20Siyuan%20Jiang%20and%20Xiaoqiang%20Lu%20and%20Licheng%20Jiao%20and%20Fang%20Liu%20and%20Xu%20Liu%20and%20Lingling%20Li%20and%20Wenping%20Ma%20and%20Shuyuan%20Yang%20and%20Haiyang%20Xie%20and%20Jian%20Zhao%20and%20Shihua%20Huang%20and%20Peng%20Cheng%20and%20Xi%20Shen%20and%20Zheng%20Wang%20and%20Shuai%20An%20and%20Caizhi%20Zhu%20and%20Xuelong%20Li%20and%20Tao%20Zhang%20and%20Liang%20Li%20and%20Yu%20Liu%20and%20Chenggang%20Yan%20and%20Gengchen%20Zhang%20and%20Linyan%20Jiang%20and%20Bingyi%20Song%20and%20Zhuoyu%20An%20and%20Haibo%20Lei%20and%20Qing%20Luo%20and%20Jie%20Song%20and%20Yuan%20Liu%20and%20Qihang%20Li%20and%20Haoyuan%20Zhang%20and%20Lingfeng%20Wang%20and%20Wei%20Chen%20and%20Aling%20Luo%20and%20Cheng%20Li%20and%20Jun%20Cao%20and%20Shu%20Chen%20and%20Zifei%20Dou%20and%20Xinyu%20Liu%20and%20Jing%20Zhang%20and%20Kexin%20Zhang%20and%20Yuting%20Yang%20and%20Xuejian%20Gou%20and%20Qinliang%20Wang%20and%20Yang%20Liu%20and%20Shizhan%20Zhao%20and%20Yanzhao%20Zhang%20and%20Libo%20Yan%20and%20Yuwei%20Guo%20and%20Guoxin%20Li%20and%20Qiong%20Gao%20and%20Chenyue%20Che%20and%20Long%20Sun%20and%20Xiang%20Chen%20and%20Hao%20Li%20and%20Jinshan%20Pan%20and%20Chuanlong%20Xie%20and%20Hongming%20Chen%20and%20Mingrui%20Li%20and%20Tianchen%20Deng%20and%20Jingwei%20Huang%20and%20Yufeng%20Li%20and%20Fei%20Wan%20and%20Bingxin%20Xu%20and%20Jian%20Cheng%20and%20Hongzhe%20Liu%20and%20Cheng%20Xu%20and%20Yuxiang%20Zou%20and%20Weiguo%20Pan%20and%20Songyin%20Dai%20and%20Sen%20Jia%20and%20Junpei%20Zhang%20and%20Puhua%20Chen%20and%20Qihang%20Li%0AAbstract%3A%20%20%20The%20intersection%20of%20physics-based%20vision%20and%20deep%20learning%20presents%20an%0Aexciting%20frontier%20for%20advancing%20computer%20vision%20technologies.%20By%20leveraging%20the%0Aprinciples%20of%20physics%20to%20inform%20and%20enhance%20deep%20learning%20models%2C%20we%20can%0Adevelop%20more%20robust%20and%20accurate%20vision%20systems.%20Physics-based%20vision%20aims%20to%0Ainvert%20the%20processes%20to%20recover%20scene%20properties%20such%20as%20shape%2C%20reflectance%2C%0Alight%20distribution%2C%20and%20medium%20properties%20from%20images.%20In%20recent%20years%2C%20deep%0Alearning%20has%20shown%20promising%20improvements%20for%20various%20vision%20tasks%2C%20and%20when%0Acombined%20with%20physics-based%20vision%2C%20these%20approaches%20can%20enhance%20the%20robustness%0Aand%20accuracy%20of%20vision%20systems.%20This%20technical%20report%20summarizes%20the%20outcomes%0Aof%20the%20Physics-Based%20Vision%20Meets%20Deep%20Learning%20%28PBDL%29%202024%20challenge%2C%20held%20in%0ACVPR%202024%20workshop.%20The%20challenge%20consisted%20of%20eight%20tracks%2C%20focusing%20on%0ALow-Light%20Enhancement%20and%20Detection%20as%20well%20as%20High%20Dynamic%20Range%20%28HDR%29%0AImaging.%20This%20report%20details%20the%20objectives%2C%20methodologies%2C%20and%20results%20of%20each%0Atrack%2C%20highlighting%20the%20top-performing%20solutions%20and%20their%20innovative%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10744v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTechnique%2520Report%2520of%2520CVPR%25202024%2520PBDL%2520Challenges%26entry.906535625%3DYing%2520Fu%2520and%2520Yu%2520Li%2520and%2520Shaodi%2520You%2520and%2520Boxin%2520Shi%2520and%2520Linwei%2520Chen%2520and%2520Yunhao%2520Zou%2520and%2520Zichun%2520Wang%2520and%2520Yichen%2520Li%2520and%2520Yuze%2520Han%2520and%2520Yingkai%2520Zhang%2520and%2520Jianan%2520Wang%2520and%2520Qinglin%2520Liu%2520and%2520Wei%2520Yu%2520and%2520Xiaoqian%2520Lv%2520and%2520Jianing%2520Li%2520and%2520Shengping%2520Zhang%2520and%2520Xiangyang%2520Ji%2520and%2520Yuanpei%2520Chen%2520and%2520Yuhan%2520Zhang%2520and%2520Weihang%2520Peng%2520and%2520Liwen%2520Zhang%2520and%2520Zhe%2520Xu%2520and%2520Dingyong%2520Gou%2520and%2520Cong%2520Li%2520and%2520Senyan%2520Xu%2520and%2520Yunkang%2520Zhang%2520and%2520Siyuan%2520Jiang%2520and%2520Xiaoqiang%2520Lu%2520and%2520Licheng%2520Jiao%2520and%2520Fang%2520Liu%2520and%2520Xu%2520Liu%2520and%2520Lingling%2520Li%2520and%2520Wenping%2520Ma%2520and%2520Shuyuan%2520Yang%2520and%2520Haiyang%2520Xie%2520and%2520Jian%2520Zhao%2520and%2520Shihua%2520Huang%2520and%2520Peng%2520Cheng%2520and%2520Xi%2520Shen%2520and%2520Zheng%2520Wang%2520and%2520Shuai%2520An%2520and%2520Caizhi%2520Zhu%2520and%2520Xuelong%2520Li%2520and%2520Tao%2520Zhang%2520and%2520Liang%2520Li%2520and%2520Yu%2520Liu%2520and%2520Chenggang%2520Yan%2520and%2520Gengchen%2520Zhang%2520and%2520Linyan%2520Jiang%2520and%2520Bingyi%2520Song%2520and%2520Zhuoyu%2520An%2520and%2520Haibo%2520Lei%2520and%2520Qing%2520Luo%2520and%2520Jie%2520Song%2520and%2520Yuan%2520Liu%2520and%2520Qihang%2520Li%2520and%2520Haoyuan%2520Zhang%2520and%2520Lingfeng%2520Wang%2520and%2520Wei%2520Chen%2520and%2520Aling%2520Luo%2520and%2520Cheng%2520Li%2520and%2520Jun%2520Cao%2520and%2520Shu%2520Chen%2520and%2520Zifei%2520Dou%2520and%2520Xinyu%2520Liu%2520and%2520Jing%2520Zhang%2520and%2520Kexin%2520Zhang%2520and%2520Yuting%2520Yang%2520and%2520Xuejian%2520Gou%2520and%2520Qinliang%2520Wang%2520and%2520Yang%2520Liu%2520and%2520Shizhan%2520Zhao%2520and%2520Yanzhao%2520Zhang%2520and%2520Libo%2520Yan%2520and%2520Yuwei%2520Guo%2520and%2520Guoxin%2520Li%2520and%2520Qiong%2520Gao%2520and%2520Chenyue%2520Che%2520and%2520Long%2520Sun%2520and%2520Xiang%2520Chen%2520and%2520Hao%2520Li%2520and%2520Jinshan%2520Pan%2520and%2520Chuanlong%2520Xie%2520and%2520Hongming%2520Chen%2520and%2520Mingrui%2520Li%2520and%2520Tianchen%2520Deng%2520and%2520Jingwei%2520Huang%2520and%2520Yufeng%2520Li%2520and%2520Fei%2520Wan%2520and%2520Bingxin%2520Xu%2520and%2520Jian%2520Cheng%2520and%2520Hongzhe%2520Liu%2520and%2520Cheng%2520Xu%2520and%2520Yuxiang%2520Zou%2520and%2520Weiguo%2520Pan%2520and%2520Songyin%2520Dai%2520and%2520Sen%2520Jia%2520and%2520Junpei%2520Zhang%2520and%2520Puhua%2520Chen%2520and%2520Qihang%2520Li%26entry.1292438233%3D%2520%2520The%2520intersection%2520of%2520physics-based%2520vision%2520and%2520deep%2520learning%2520presents%2520an%250Aexciting%2520frontier%2520for%2520advancing%2520computer%2520vision%2520technologies.%2520By%2520leveraging%2520the%250Aprinciples%2520of%2520physics%2520to%2520inform%2520and%2520enhance%2520deep%2520learning%2520models%252C%2520we%2520can%250Adevelop%2520more%2520robust%2520and%2520accurate%2520vision%2520systems.%2520Physics-based%2520vision%2520aims%2520to%250Ainvert%2520the%2520processes%2520to%2520recover%2520scene%2520properties%2520such%2520as%2520shape%252C%2520reflectance%252C%250Alight%2520distribution%252C%2520and%2520medium%2520properties%2520from%2520images.%2520In%2520recent%2520years%252C%2520deep%250Alearning%2520has%2520shown%2520promising%2520improvements%2520for%2520various%2520vision%2520tasks%252C%2520and%2520when%250Acombined%2520with%2520physics-based%2520vision%252C%2520these%2520approaches%2520can%2520enhance%2520the%2520robustness%250Aand%2520accuracy%2520of%2520vision%2520systems.%2520This%2520technical%2520report%2520summarizes%2520the%2520outcomes%250Aof%2520the%2520Physics-Based%2520Vision%2520Meets%2520Deep%2520Learning%2520%2528PBDL%2529%25202024%2520challenge%252C%2520held%2520in%250ACVPR%25202024%2520workshop.%2520The%2520challenge%2520consisted%2520of%2520eight%2520tracks%252C%2520focusing%2520on%250ALow-Light%2520Enhancement%2520and%2520Detection%2520as%2520well%2520as%2520High%2520Dynamic%2520Range%2520%2528HDR%2529%250AImaging.%2520This%2520report%2520details%2520the%2520objectives%252C%2520methodologies%252C%2520and%2520results%2520of%2520each%250Atrack%252C%2520highlighting%2520the%2520top-performing%2520solutions%2520and%2520their%2520innovative%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10744v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technique%20Report%20of%20CVPR%202024%20PBDL%20Challenges&entry.906535625=Ying%20Fu%20and%20Yu%20Li%20and%20Shaodi%20You%20and%20Boxin%20Shi%20and%20Linwei%20Chen%20and%20Yunhao%20Zou%20and%20Zichun%20Wang%20and%20Yichen%20Li%20and%20Yuze%20Han%20and%20Yingkai%20Zhang%20and%20Jianan%20Wang%20and%20Qinglin%20Liu%20and%20Wei%20Yu%20and%20Xiaoqian%20Lv%20and%20Jianing%20Li%20and%20Shengping%20Zhang%20and%20Xiangyang%20Ji%20and%20Yuanpei%20Chen%20and%20Yuhan%20Zhang%20and%20Weihang%20Peng%20and%20Liwen%20Zhang%20and%20Zhe%20Xu%20and%20Dingyong%20Gou%20and%20Cong%20Li%20and%20Senyan%20Xu%20and%20Yunkang%20Zhang%20and%20Siyuan%20Jiang%20and%20Xiaoqiang%20Lu%20and%20Licheng%20Jiao%20and%20Fang%20Liu%20and%20Xu%20Liu%20and%20Lingling%20Li%20and%20Wenping%20Ma%20and%20Shuyuan%20Yang%20and%20Haiyang%20Xie%20and%20Jian%20Zhao%20and%20Shihua%20Huang%20and%20Peng%20Cheng%20and%20Xi%20Shen%20and%20Zheng%20Wang%20and%20Shuai%20An%20and%20Caizhi%20Zhu%20and%20Xuelong%20Li%20and%20Tao%20Zhang%20and%20Liang%20Li%20and%20Yu%20Liu%20and%20Chenggang%20Yan%20and%20Gengchen%20Zhang%20and%20Linyan%20Jiang%20and%20Bingyi%20Song%20and%20Zhuoyu%20An%20and%20Haibo%20Lei%20and%20Qing%20Luo%20and%20Jie%20Song%20and%20Yuan%20Liu%20and%20Qihang%20Li%20and%20Haoyuan%20Zhang%20and%20Lingfeng%20Wang%20and%20Wei%20Chen%20and%20Aling%20Luo%20and%20Cheng%20Li%20and%20Jun%20Cao%20and%20Shu%20Chen%20and%20Zifei%20Dou%20and%20Xinyu%20Liu%20and%20Jing%20Zhang%20and%20Kexin%20Zhang%20and%20Yuting%20Yang%20and%20Xuejian%20Gou%20and%20Qinliang%20Wang%20and%20Yang%20Liu%20and%20Shizhan%20Zhao%20and%20Yanzhao%20Zhang%20and%20Libo%20Yan%20and%20Yuwei%20Guo%20and%20Guoxin%20Li%20and%20Qiong%20Gao%20and%20Chenyue%20Che%20and%20Long%20Sun%20and%20Xiang%20Chen%20and%20Hao%20Li%20and%20Jinshan%20Pan%20and%20Chuanlong%20Xie%20and%20Hongming%20Chen%20and%20Mingrui%20Li%20and%20Tianchen%20Deng%20and%20Jingwei%20Huang%20and%20Yufeng%20Li%20and%20Fei%20Wan%20and%20Bingxin%20Xu%20and%20Jian%20Cheng%20and%20Hongzhe%20Liu%20and%20Cheng%20Xu%20and%20Yuxiang%20Zou%20and%20Weiguo%20Pan%20and%20Songyin%20Dai%20and%20Sen%20Jia%20and%20Junpei%20Zhang%20and%20Puhua%20Chen%20and%20Qihang%20Li&entry.1292438233=%20%20The%20intersection%20of%20physics-based%20vision%20and%20deep%20learning%20presents%20an%0Aexciting%20frontier%20for%20advancing%20computer%20vision%20technologies.%20By%20leveraging%20the%0Aprinciples%20of%20physics%20to%20inform%20and%20enhance%20deep%20learning%20models%2C%20we%20can%0Adevelop%20more%20robust%20and%20accurate%20vision%20systems.%20Physics-based%20vision%20aims%20to%0Ainvert%20the%20processes%20to%20recover%20scene%20properties%20such%20as%20shape%2C%20reflectance%2C%0Alight%20distribution%2C%20and%20medium%20properties%20from%20images.%20In%20recent%20years%2C%20deep%0Alearning%20has%20shown%20promising%20improvements%20for%20various%20vision%20tasks%2C%20and%20when%0Acombined%20with%20physics-based%20vision%2C%20these%20approaches%20can%20enhance%20the%20robustness%0Aand%20accuracy%20of%20vision%20systems.%20This%20technical%20report%20summarizes%20the%20outcomes%0Aof%20the%20Physics-Based%20Vision%20Meets%20Deep%20Learning%20%28PBDL%29%202024%20challenge%2C%20held%20in%0ACVPR%202024%20workshop.%20The%20challenge%20consisted%20of%20eight%20tracks%2C%20focusing%20on%0ALow-Light%20Enhancement%20and%20Detection%20as%20well%20as%20High%20Dynamic%20Range%20%28HDR%29%0AImaging.%20This%20report%20details%20the%20objectives%2C%20methodologies%2C%20and%20results%20of%20each%0Atrack%2C%20highlighting%20the%20top-performing%20solutions%20and%20their%20innovative%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10744v3&entry.124074799=Read"},
{"title": "Long-term drought prediction using deep neural networks based on\n  geospatial weather data", "author": "Alexander Marusov and Vsevolod Grabar and Yury Maximov and Nazar Sotiriadi and Alexander Bulkin and Alexey Zaytsev", "abstract": "  The problem of high-quality drought forecasting up to a year in advance is\ncritical for agriculture planning and insurance. Yet, it is still unsolved with\nreasonable accuracy due to data complexity and aridity stochasticity. We tackle\ndrought data by introducing an end-to-end approach that adopts a\nspatio-temporal neural network model with accessible open monthly climate data\nas the input.\n  Our systematic research employs diverse proposed models and five distinct\nenvironmental regions as a testbed to evaluate the efficacy of the Palmer\nDrought Severity Index (PDSI) prediction. Key aggregated findings are the\nexceptional performance of a Transformer model, EarthFormer, in making accurate\nshort-term (up to six months) forecasts. At the same time, the Convolutional\nLSTM excels in longer-term forecasting.\n", "link": "http://arxiv.org/abs/2309.06212v6", "date": "2024-07-12", "relevancy": 1.8986, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5015}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4753}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-term%20drought%20prediction%20using%20deep%20neural%20networks%20based%20on%0A%20%20geospatial%20weather%20data&body=Title%3A%20Long-term%20drought%20prediction%20using%20deep%20neural%20networks%20based%20on%0A%20%20geospatial%20weather%20data%0AAuthor%3A%20Alexander%20Marusov%20and%20Vsevolod%20Grabar%20and%20Yury%20Maximov%20and%20Nazar%20Sotiriadi%20and%20Alexander%20Bulkin%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20The%20problem%20of%20high-quality%20drought%20forecasting%20up%20to%20a%20year%20in%20advance%20is%0Acritical%20for%20agriculture%20planning%20and%20insurance.%20Yet%2C%20it%20is%20still%20unsolved%20with%0Areasonable%20accuracy%20due%20to%20data%20complexity%20and%20aridity%20stochasticity.%20We%20tackle%0Adrought%20data%20by%20introducing%20an%20end-to-end%20approach%20that%20adopts%20a%0Aspatio-temporal%20neural%20network%20model%20with%20accessible%20open%20monthly%20climate%20data%0Aas%20the%20input.%0A%20%20Our%20systematic%20research%20employs%20diverse%20proposed%20models%20and%20five%20distinct%0Aenvironmental%20regions%20as%20a%20testbed%20to%20evaluate%20the%20efficacy%20of%20the%20Palmer%0ADrought%20Severity%20Index%20%28PDSI%29%20prediction.%20Key%20aggregated%20findings%20are%20the%0Aexceptional%20performance%20of%20a%20Transformer%20model%2C%20EarthFormer%2C%20in%20making%20accurate%0Ashort-term%20%28up%20to%20six%20months%29%20forecasts.%20At%20the%20same%20time%2C%20the%20Convolutional%0ALSTM%20excels%20in%20longer-term%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06212v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-term%2520drought%2520prediction%2520using%2520deep%2520neural%2520networks%2520based%2520on%250A%2520%2520geospatial%2520weather%2520data%26entry.906535625%3DAlexander%2520Marusov%2520and%2520Vsevolod%2520Grabar%2520and%2520Yury%2520Maximov%2520and%2520Nazar%2520Sotiriadi%2520and%2520Alexander%2520Bulkin%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520high-quality%2520drought%2520forecasting%2520up%2520to%2520a%2520year%2520in%2520advance%2520is%250Acritical%2520for%2520agriculture%2520planning%2520and%2520insurance.%2520Yet%252C%2520it%2520is%2520still%2520unsolved%2520with%250Areasonable%2520accuracy%2520due%2520to%2520data%2520complexity%2520and%2520aridity%2520stochasticity.%2520We%2520tackle%250Adrought%2520data%2520by%2520introducing%2520an%2520end-to-end%2520approach%2520that%2520adopts%2520a%250Aspatio-temporal%2520neural%2520network%2520model%2520with%2520accessible%2520open%2520monthly%2520climate%2520data%250Aas%2520the%2520input.%250A%2520%2520Our%2520systematic%2520research%2520employs%2520diverse%2520proposed%2520models%2520and%2520five%2520distinct%250Aenvironmental%2520regions%2520as%2520a%2520testbed%2520to%2520evaluate%2520the%2520efficacy%2520of%2520the%2520Palmer%250ADrought%2520Severity%2520Index%2520%2528PDSI%2529%2520prediction.%2520Key%2520aggregated%2520findings%2520are%2520the%250Aexceptional%2520performance%2520of%2520a%2520Transformer%2520model%252C%2520EarthFormer%252C%2520in%2520making%2520accurate%250Ashort-term%2520%2528up%2520to%2520six%2520months%2529%2520forecasts.%2520At%2520the%2520same%2520time%252C%2520the%2520Convolutional%250ALSTM%2520excels%2520in%2520longer-term%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06212v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-term%20drought%20prediction%20using%20deep%20neural%20networks%20based%20on%0A%20%20geospatial%20weather%20data&entry.906535625=Alexander%20Marusov%20and%20Vsevolod%20Grabar%20and%20Yury%20Maximov%20and%20Nazar%20Sotiriadi%20and%20Alexander%20Bulkin%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20The%20problem%20of%20high-quality%20drought%20forecasting%20up%20to%20a%20year%20in%20advance%20is%0Acritical%20for%20agriculture%20planning%20and%20insurance.%20Yet%2C%20it%20is%20still%20unsolved%20with%0Areasonable%20accuracy%20due%20to%20data%20complexity%20and%20aridity%20stochasticity.%20We%20tackle%0Adrought%20data%20by%20introducing%20an%20end-to-end%20approach%20that%20adopts%20a%0Aspatio-temporal%20neural%20network%20model%20with%20accessible%20open%20monthly%20climate%20data%0Aas%20the%20input.%0A%20%20Our%20systematic%20research%20employs%20diverse%20proposed%20models%20and%20five%20distinct%0Aenvironmental%20regions%20as%20a%20testbed%20to%20evaluate%20the%20efficacy%20of%20the%20Palmer%0ADrought%20Severity%20Index%20%28PDSI%29%20prediction.%20Key%20aggregated%20findings%20are%20the%0Aexceptional%20performance%20of%20a%20Transformer%20model%2C%20EarthFormer%2C%20in%20making%20accurate%0Ashort-term%20%28up%20to%20six%20months%29%20forecasts.%20At%20the%20same%20time%2C%20the%20Convolutional%0ALSTM%20excels%20in%20longer-term%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06212v6&entry.124074799=Read"},
{"title": "Learning High-Frequency Functions Made Easy with Sinusoidal Positional\n  Encoding", "author": "Chuanhao Sun and Zhihang Yuan and Kai Xu and Luo Mai and Siddharth N and Shuo Chen and Mahesh K. Marina", "abstract": "  Fourier features based positional encoding (PE) is commonly used in machine\nlearning tasks that involve learning high-frequency features from\nlow-dimensional inputs, such as 3D view synthesis and time series regression\nwith neural tangent kernels. Despite their effectiveness, existing PEs require\nmanual, empirical adjustment of crucial hyperparameters, specifically the\nFourier features, tailored to each unique task. Further, PEs face challenges in\nefficiently learning high-frequency functions, particularly in tasks with\nlimited data. In this paper, we introduce sinusoidal PE (SPE), designed to\nefficiently learn adaptive frequency features closely aligned with the true\nunderlying function. Our experiments demonstrate that SPE, without\nhyperparameter tuning, consistently achieves enhanced fidelity and faster\ntraining across various tasks, including 3D view synthesis, Text-to-Speech\ngeneration, and 1D regression. SPE is implemented as a direct replacement for\nexisting PEs. Its plug-and-play nature lets numerous tasks easily adopt and\nbenefit from SPE.\n", "link": "http://arxiv.org/abs/2407.09370v1", "date": "2024-07-12", "relevancy": 1.8977, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4876}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4816}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20High-Frequency%20Functions%20Made%20Easy%20with%20Sinusoidal%20Positional%0A%20%20Encoding&body=Title%3A%20Learning%20High-Frequency%20Functions%20Made%20Easy%20with%20Sinusoidal%20Positional%0A%20%20Encoding%0AAuthor%3A%20Chuanhao%20Sun%20and%20Zhihang%20Yuan%20and%20Kai%20Xu%20and%20Luo%20Mai%20and%20Siddharth%20N%20and%20Shuo%20Chen%20and%20Mahesh%20K.%20Marina%0AAbstract%3A%20%20%20Fourier%20features%20based%20positional%20encoding%20%28PE%29%20is%20commonly%20used%20in%20machine%0Alearning%20tasks%20that%20involve%20learning%20high-frequency%20features%20from%0Alow-dimensional%20inputs%2C%20such%20as%203D%20view%20synthesis%20and%20time%20series%20regression%0Awith%20neural%20tangent%20kernels.%20Despite%20their%20effectiveness%2C%20existing%20PEs%20require%0Amanual%2C%20empirical%20adjustment%20of%20crucial%20hyperparameters%2C%20specifically%20the%0AFourier%20features%2C%20tailored%20to%20each%20unique%20task.%20Further%2C%20PEs%20face%20challenges%20in%0Aefficiently%20learning%20high-frequency%20functions%2C%20particularly%20in%20tasks%20with%0Alimited%20data.%20In%20this%20paper%2C%20we%20introduce%20sinusoidal%20PE%20%28SPE%29%2C%20designed%20to%0Aefficiently%20learn%20adaptive%20frequency%20features%20closely%20aligned%20with%20the%20true%0Aunderlying%20function.%20Our%20experiments%20demonstrate%20that%20SPE%2C%20without%0Ahyperparameter%20tuning%2C%20consistently%20achieves%20enhanced%20fidelity%20and%20faster%0Atraining%20across%20various%20tasks%2C%20including%203D%20view%20synthesis%2C%20Text-to-Speech%0Ageneration%2C%20and%201D%20regression.%20SPE%20is%20implemented%20as%20a%20direct%20replacement%20for%0Aexisting%20PEs.%20Its%20plug-and-play%20nature%20lets%20numerous%20tasks%20easily%20adopt%20and%0Abenefit%20from%20SPE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520High-Frequency%2520Functions%2520Made%2520Easy%2520with%2520Sinusoidal%2520Positional%250A%2520%2520Encoding%26entry.906535625%3DChuanhao%2520Sun%2520and%2520Zhihang%2520Yuan%2520and%2520Kai%2520Xu%2520and%2520Luo%2520Mai%2520and%2520Siddharth%2520N%2520and%2520Shuo%2520Chen%2520and%2520Mahesh%2520K.%2520Marina%26entry.1292438233%3D%2520%2520Fourier%2520features%2520based%2520positional%2520encoding%2520%2528PE%2529%2520is%2520commonly%2520used%2520in%2520machine%250Alearning%2520tasks%2520that%2520involve%2520learning%2520high-frequency%2520features%2520from%250Alow-dimensional%2520inputs%252C%2520such%2520as%25203D%2520view%2520synthesis%2520and%2520time%2520series%2520regression%250Awith%2520neural%2520tangent%2520kernels.%2520Despite%2520their%2520effectiveness%252C%2520existing%2520PEs%2520require%250Amanual%252C%2520empirical%2520adjustment%2520of%2520crucial%2520hyperparameters%252C%2520specifically%2520the%250AFourier%2520features%252C%2520tailored%2520to%2520each%2520unique%2520task.%2520Further%252C%2520PEs%2520face%2520challenges%2520in%250Aefficiently%2520learning%2520high-frequency%2520functions%252C%2520particularly%2520in%2520tasks%2520with%250Alimited%2520data.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520sinusoidal%2520PE%2520%2528SPE%2529%252C%2520designed%2520to%250Aefficiently%2520learn%2520adaptive%2520frequency%2520features%2520closely%2520aligned%2520with%2520the%2520true%250Aunderlying%2520function.%2520Our%2520experiments%2520demonstrate%2520that%2520SPE%252C%2520without%250Ahyperparameter%2520tuning%252C%2520consistently%2520achieves%2520enhanced%2520fidelity%2520and%2520faster%250Atraining%2520across%2520various%2520tasks%252C%2520including%25203D%2520view%2520synthesis%252C%2520Text-to-Speech%250Ageneration%252C%2520and%25201D%2520regression.%2520SPE%2520is%2520implemented%2520as%2520a%2520direct%2520replacement%2520for%250Aexisting%2520PEs.%2520Its%2520plug-and-play%2520nature%2520lets%2520numerous%2520tasks%2520easily%2520adopt%2520and%250Abenefit%2520from%2520SPE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20High-Frequency%20Functions%20Made%20Easy%20with%20Sinusoidal%20Positional%0A%20%20Encoding&entry.906535625=Chuanhao%20Sun%20and%20Zhihang%20Yuan%20and%20Kai%20Xu%20and%20Luo%20Mai%20and%20Siddharth%20N%20and%20Shuo%20Chen%20and%20Mahesh%20K.%20Marina&entry.1292438233=%20%20Fourier%20features%20based%20positional%20encoding%20%28PE%29%20is%20commonly%20used%20in%20machine%0Alearning%20tasks%20that%20involve%20learning%20high-frequency%20features%20from%0Alow-dimensional%20inputs%2C%20such%20as%203D%20view%20synthesis%20and%20time%20series%20regression%0Awith%20neural%20tangent%20kernels.%20Despite%20their%20effectiveness%2C%20existing%20PEs%20require%0Amanual%2C%20empirical%20adjustment%20of%20crucial%20hyperparameters%2C%20specifically%20the%0AFourier%20features%2C%20tailored%20to%20each%20unique%20task.%20Further%2C%20PEs%20face%20challenges%20in%0Aefficiently%20learning%20high-frequency%20functions%2C%20particularly%20in%20tasks%20with%0Alimited%20data.%20In%20this%20paper%2C%20we%20introduce%20sinusoidal%20PE%20%28SPE%29%2C%20designed%20to%0Aefficiently%20learn%20adaptive%20frequency%20features%20closely%20aligned%20with%20the%20true%0Aunderlying%20function.%20Our%20experiments%20demonstrate%20that%20SPE%2C%20without%0Ahyperparameter%20tuning%2C%20consistently%20achieves%20enhanced%20fidelity%20and%20faster%0Atraining%20across%20various%20tasks%2C%20including%203D%20view%20synthesis%2C%20Text-to-Speech%0Ageneration%2C%20and%201D%20regression.%20SPE%20is%20implemented%20as%20a%20direct%20replacement%20for%0Aexisting%20PEs.%20Its%20plug-and-play%20nature%20lets%20numerous%20tasks%20easily%20adopt%20and%0Abenefit%20from%20SPE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09370v1&entry.124074799=Read"},
{"title": "Stepwise Verification and Remediation of Student Reasoning Errors with\n  Large Language Model Tutors", "author": "Nico Daheim and Jakub Macina and Manu Kapur and Iryna Gurevych and Mrinmaya Sachan", "abstract": "  Large language models (LLMs) present an opportunity to scale high-quality\npersonalized education to all. A promising approach towards this means is to\nbuild dialog tutoring models that scaffold students' problem-solving. However,\neven though existing LLMs perform well in solving reasoning questions, they\nstruggle to precisely detect student's errors and tailor their feedback to\nthese errors. Inspired by real-world teaching practice where teachers identify\nstudent errors and customize their response based on them, we focus on\nverifying student solutions and show how grounding to such verification\nimproves the overall quality of tutor response generation. We collect a dataset\nof 1K stepwise math reasoning chains with the first error step annotated by\nteachers. We show empirically that finding the mistake in a student solution is\nchallenging for current models. We propose and evaluate several verifiers for\ndetecting these errors. Using both automatic and human evaluation we show that\nthe student solution verifiers steer the generation model towards highly\ntargeted responses to student errors which are more often correct with less\nhallucinations compared to existing baselines.\n", "link": "http://arxiv.org/abs/2407.09136v1", "date": "2024-07-12", "relevancy": 1.8913, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4928}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.483}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stepwise%20Verification%20and%20Remediation%20of%20Student%20Reasoning%20Errors%20with%0A%20%20Large%20Language%20Model%20Tutors&body=Title%3A%20Stepwise%20Verification%20and%20Remediation%20of%20Student%20Reasoning%20Errors%20with%0A%20%20Large%20Language%20Model%20Tutors%0AAuthor%3A%20Nico%20Daheim%20and%20Jakub%20Macina%20and%20Manu%20Kapur%20and%20Iryna%20Gurevych%20and%20Mrinmaya%20Sachan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20present%20an%20opportunity%20to%20scale%20high-quality%0Apersonalized%20education%20to%20all.%20A%20promising%20approach%20towards%20this%20means%20is%20to%0Abuild%20dialog%20tutoring%20models%20that%20scaffold%20students%27%20problem-solving.%20However%2C%0Aeven%20though%20existing%20LLMs%20perform%20well%20in%20solving%20reasoning%20questions%2C%20they%0Astruggle%20to%20precisely%20detect%20student%27s%20errors%20and%20tailor%20their%20feedback%20to%0Athese%20errors.%20Inspired%20by%20real-world%20teaching%20practice%20where%20teachers%20identify%0Astudent%20errors%20and%20customize%20their%20response%20based%20on%20them%2C%20we%20focus%20on%0Averifying%20student%20solutions%20and%20show%20how%20grounding%20to%20such%20verification%0Aimproves%20the%20overall%20quality%20of%20tutor%20response%20generation.%20We%20collect%20a%20dataset%0Aof%201K%20stepwise%20math%20reasoning%20chains%20with%20the%20first%20error%20step%20annotated%20by%0Ateachers.%20We%20show%20empirically%20that%20finding%20the%20mistake%20in%20a%20student%20solution%20is%0Achallenging%20for%20current%20models.%20We%20propose%20and%20evaluate%20several%20verifiers%20for%0Adetecting%20these%20errors.%20Using%20both%20automatic%20and%20human%20evaluation%20we%20show%20that%0Athe%20student%20solution%20verifiers%20steer%20the%20generation%20model%20towards%20highly%0Atargeted%20responses%20to%20student%20errors%20which%20are%20more%20often%20correct%20with%20less%0Ahallucinations%20compared%20to%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepwise%2520Verification%2520and%2520Remediation%2520of%2520Student%2520Reasoning%2520Errors%2520with%250A%2520%2520Large%2520Language%2520Model%2520Tutors%26entry.906535625%3DNico%2520Daheim%2520and%2520Jakub%2520Macina%2520and%2520Manu%2520Kapur%2520and%2520Iryna%2520Gurevych%2520and%2520Mrinmaya%2520Sachan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520present%2520an%2520opportunity%2520to%2520scale%2520high-quality%250Apersonalized%2520education%2520to%2520all.%2520A%2520promising%2520approach%2520towards%2520this%2520means%2520is%2520to%250Abuild%2520dialog%2520tutoring%2520models%2520that%2520scaffold%2520students%2527%2520problem-solving.%2520However%252C%250Aeven%2520though%2520existing%2520LLMs%2520perform%2520well%2520in%2520solving%2520reasoning%2520questions%252C%2520they%250Astruggle%2520to%2520precisely%2520detect%2520student%2527s%2520errors%2520and%2520tailor%2520their%2520feedback%2520to%250Athese%2520errors.%2520Inspired%2520by%2520real-world%2520teaching%2520practice%2520where%2520teachers%2520identify%250Astudent%2520errors%2520and%2520customize%2520their%2520response%2520based%2520on%2520them%252C%2520we%2520focus%2520on%250Averifying%2520student%2520solutions%2520and%2520show%2520how%2520grounding%2520to%2520such%2520verification%250Aimproves%2520the%2520overall%2520quality%2520of%2520tutor%2520response%2520generation.%2520We%2520collect%2520a%2520dataset%250Aof%25201K%2520stepwise%2520math%2520reasoning%2520chains%2520with%2520the%2520first%2520error%2520step%2520annotated%2520by%250Ateachers.%2520We%2520show%2520empirically%2520that%2520finding%2520the%2520mistake%2520in%2520a%2520student%2520solution%2520is%250Achallenging%2520for%2520current%2520models.%2520We%2520propose%2520and%2520evaluate%2520several%2520verifiers%2520for%250Adetecting%2520these%2520errors.%2520Using%2520both%2520automatic%2520and%2520human%2520evaluation%2520we%2520show%2520that%250Athe%2520student%2520solution%2520verifiers%2520steer%2520the%2520generation%2520model%2520towards%2520highly%250Atargeted%2520responses%2520to%2520student%2520errors%2520which%2520are%2520more%2520often%2520correct%2520with%2520less%250Ahallucinations%2520compared%2520to%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stepwise%20Verification%20and%20Remediation%20of%20Student%20Reasoning%20Errors%20with%0A%20%20Large%20Language%20Model%20Tutors&entry.906535625=Nico%20Daheim%20and%20Jakub%20Macina%20and%20Manu%20Kapur%20and%20Iryna%20Gurevych%20and%20Mrinmaya%20Sachan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20present%20an%20opportunity%20to%20scale%20high-quality%0Apersonalized%20education%20to%20all.%20A%20promising%20approach%20towards%20this%20means%20is%20to%0Abuild%20dialog%20tutoring%20models%20that%20scaffold%20students%27%20problem-solving.%20However%2C%0Aeven%20though%20existing%20LLMs%20perform%20well%20in%20solving%20reasoning%20questions%2C%20they%0Astruggle%20to%20precisely%20detect%20student%27s%20errors%20and%20tailor%20their%20feedback%20to%0Athese%20errors.%20Inspired%20by%20real-world%20teaching%20practice%20where%20teachers%20identify%0Astudent%20errors%20and%20customize%20their%20response%20based%20on%20them%2C%20we%20focus%20on%0Averifying%20student%20solutions%20and%20show%20how%20grounding%20to%20such%20verification%0Aimproves%20the%20overall%20quality%20of%20tutor%20response%20generation.%20We%20collect%20a%20dataset%0Aof%201K%20stepwise%20math%20reasoning%20chains%20with%20the%20first%20error%20step%20annotated%20by%0Ateachers.%20We%20show%20empirically%20that%20finding%20the%20mistake%20in%20a%20student%20solution%20is%0Achallenging%20for%20current%20models.%20We%20propose%20and%20evaluate%20several%20verifiers%20for%0Adetecting%20these%20errors.%20Using%20both%20automatic%20and%20human%20evaluation%20we%20show%20that%0Athe%20student%20solution%20verifiers%20steer%20the%20generation%20model%20towards%20highly%0Atargeted%20responses%20to%20student%20errors%20which%20are%20more%20often%20correct%20with%20less%0Ahallucinations%20compared%20to%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09136v1&entry.124074799=Read"},
{"title": "Robust Yet Efficient Conformal Prediction Sets", "author": "Soroush H. Zargarbashi and Mohammad Sadegh Akhondzadeh and Aleksandar Bojchevski", "abstract": "  Conformal prediction (CP) can convert any model's output into prediction sets\nguaranteed to include the true label with any user-specified probability.\nHowever, same as the model itself, CP is vulnerable to adversarial test\nexamples (evasion) and perturbed calibration data (poisoning). We derive\nprovably robust sets by bounding the worst-case change in conformity scores.\nOur tighter bounds lead to more efficient sets. We cover both continuous and\ndiscrete (sparse) data and our guarantees work both for evasion and poisoning\nattacks (on both features and labels).\n", "link": "http://arxiv.org/abs/2407.09165v1", "date": "2024-07-12", "relevancy": 1.8883, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4752}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Yet%20Efficient%20Conformal%20Prediction%20Sets&body=Title%3A%20Robust%20Yet%20Efficient%20Conformal%20Prediction%20Sets%0AAuthor%3A%20Soroush%20H.%20Zargarbashi%20and%20Mohammad%20Sadegh%20Akhondzadeh%20and%20Aleksandar%20Bojchevski%0AAbstract%3A%20%20%20Conformal%20prediction%20%28CP%29%20can%20convert%20any%20model%27s%20output%20into%20prediction%20sets%0Aguaranteed%20to%20include%20the%20true%20label%20with%20any%20user-specified%20probability.%0AHowever%2C%20same%20as%20the%20model%20itself%2C%20CP%20is%20vulnerable%20to%20adversarial%20test%0Aexamples%20%28evasion%29%20and%20perturbed%20calibration%20data%20%28poisoning%29.%20We%20derive%0Aprovably%20robust%20sets%20by%20bounding%20the%20worst-case%20change%20in%20conformity%20scores.%0AOur%20tighter%20bounds%20lead%20to%20more%20efficient%20sets.%20We%20cover%20both%20continuous%20and%0Adiscrete%20%28sparse%29%20data%20and%20our%20guarantees%20work%20both%20for%20evasion%20and%20poisoning%0Aattacks%20%28on%20both%20features%20and%20labels%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Yet%2520Efficient%2520Conformal%2520Prediction%2520Sets%26entry.906535625%3DSoroush%2520H.%2520Zargarbashi%2520and%2520Mohammad%2520Sadegh%2520Akhondzadeh%2520and%2520Aleksandar%2520Bojchevski%26entry.1292438233%3D%2520%2520Conformal%2520prediction%2520%2528CP%2529%2520can%2520convert%2520any%2520model%2527s%2520output%2520into%2520prediction%2520sets%250Aguaranteed%2520to%2520include%2520the%2520true%2520label%2520with%2520any%2520user-specified%2520probability.%250AHowever%252C%2520same%2520as%2520the%2520model%2520itself%252C%2520CP%2520is%2520vulnerable%2520to%2520adversarial%2520test%250Aexamples%2520%2528evasion%2529%2520and%2520perturbed%2520calibration%2520data%2520%2528poisoning%2529.%2520We%2520derive%250Aprovably%2520robust%2520sets%2520by%2520bounding%2520the%2520worst-case%2520change%2520in%2520conformity%2520scores.%250AOur%2520tighter%2520bounds%2520lead%2520to%2520more%2520efficient%2520sets.%2520We%2520cover%2520both%2520continuous%2520and%250Adiscrete%2520%2528sparse%2529%2520data%2520and%2520our%2520guarantees%2520work%2520both%2520for%2520evasion%2520and%2520poisoning%250Aattacks%2520%2528on%2520both%2520features%2520and%2520labels%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Yet%20Efficient%20Conformal%20Prediction%20Sets&entry.906535625=Soroush%20H.%20Zargarbashi%20and%20Mohammad%20Sadegh%20Akhondzadeh%20and%20Aleksandar%20Bojchevski&entry.1292438233=%20%20Conformal%20prediction%20%28CP%29%20can%20convert%20any%20model%27s%20output%20into%20prediction%20sets%0Aguaranteed%20to%20include%20the%20true%20label%20with%20any%20user-specified%20probability.%0AHowever%2C%20same%20as%20the%20model%20itself%2C%20CP%20is%20vulnerable%20to%20adversarial%20test%0Aexamples%20%28evasion%29%20and%20perturbed%20calibration%20data%20%28poisoning%29.%20We%20derive%0Aprovably%20robust%20sets%20by%20bounding%20the%20worst-case%20change%20in%20conformity%20scores.%0AOur%20tighter%20bounds%20lead%20to%20more%20efficient%20sets.%20We%20cover%20both%20continuous%20and%0Adiscrete%20%28sparse%29%20data%20and%20our%20guarantees%20work%20both%20for%20evasion%20and%20poisoning%0Aattacks%20%28on%20both%20features%20and%20labels%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09165v1&entry.124074799=Read"},
{"title": "Improving Alignment and Robustness with Circuit Breakers", "author": "Andy Zou and Long Phan and Justin Wang and Derek Duenas and Maxwell Lin and Maksym Andriushchenko and Rowan Wang and Zico Kolter and Matt Fredrikson and Dan Hendrycks", "abstract": "  AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.\n", "link": "http://arxiv.org/abs/2406.04313v4", "date": "2024-07-12", "relevancy": 1.8824, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4711}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4709}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Alignment%20and%20Robustness%20with%20Circuit%20Breakers&body=Title%3A%20Improving%20Alignment%20and%20Robustness%20with%20Circuit%20Breakers%0AAuthor%3A%20Andy%20Zou%20and%20Long%20Phan%20and%20Justin%20Wang%20and%20Derek%20Duenas%20and%20Maxwell%20Lin%20and%20Maksym%20Andriushchenko%20and%20Rowan%20Wang%20and%20Zico%20Kolter%20and%20Matt%20Fredrikson%20and%20Dan%20Hendrycks%0AAbstract%3A%20%20%20AI%20systems%20can%20take%20harmful%20actions%20and%20are%20highly%20vulnerable%20to%20adversarial%0Aattacks.%20We%20present%20an%20approach%2C%20inspired%20by%20recent%20advances%20in%20representation%0Aengineering%2C%20that%20interrupts%20the%20models%20as%20they%20respond%20with%20harmful%20outputs%0Awith%20%22circuit%20breakers.%22%20Existing%20techniques%20aimed%20at%20improving%20alignment%2C%20such%0Aas%20refusal%20training%2C%20are%20often%20bypassed.%20Techniques%20such%20as%20adversarial%0Atraining%20try%20to%20plug%20these%20holes%20by%20countering%20specific%20attacks.%20As%20an%0Aalternative%20to%20refusal%20training%20and%20adversarial%20training%2C%20circuit-breaking%0Adirectly%20controls%20the%20representations%20that%20are%20responsible%20for%20harmful%20outputs%0Ain%20the%20first%20place.%20Our%20technique%20can%20be%20applied%20to%20both%20text-only%20and%0Amultimodal%20language%20models%20to%20prevent%20the%20generation%20of%20harmful%20outputs%20without%0Asacrificing%20utility%20--%20even%20in%20the%20presence%20of%20powerful%20unseen%20attacks.%0ANotably%2C%20while%20adversarial%20robustness%20in%20standalone%20image%20recognition%20remains%0Aan%20open%20challenge%2C%20circuit%20breakers%20allow%20the%20larger%20multimodal%20system%20to%0Areliably%20withstand%20image%20%22hijacks%22%20that%20aim%20to%20produce%20harmful%20content.%0AFinally%2C%20we%20extend%20our%20approach%20to%20AI%20agents%2C%20demonstrating%20considerable%0Areductions%20in%20the%20rate%20of%20harmful%20actions%20when%20they%20are%20under%20attack.%20Our%0Aapproach%20represents%20a%20significant%20step%20forward%20in%20the%20development%20of%20reliable%0Asafeguards%20to%20harmful%20behavior%20and%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04313v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Alignment%2520and%2520Robustness%2520with%2520Circuit%2520Breakers%26entry.906535625%3DAndy%2520Zou%2520and%2520Long%2520Phan%2520and%2520Justin%2520Wang%2520and%2520Derek%2520Duenas%2520and%2520Maxwell%2520Lin%2520and%2520Maksym%2520Andriushchenko%2520and%2520Rowan%2520Wang%2520and%2520Zico%2520Kolter%2520and%2520Matt%2520Fredrikson%2520and%2520Dan%2520Hendrycks%26entry.1292438233%3D%2520%2520AI%2520systems%2520can%2520take%2520harmful%2520actions%2520and%2520are%2520highly%2520vulnerable%2520to%2520adversarial%250Aattacks.%2520We%2520present%2520an%2520approach%252C%2520inspired%2520by%2520recent%2520advances%2520in%2520representation%250Aengineering%252C%2520that%2520interrupts%2520the%2520models%2520as%2520they%2520respond%2520with%2520harmful%2520outputs%250Awith%2520%2522circuit%2520breakers.%2522%2520Existing%2520techniques%2520aimed%2520at%2520improving%2520alignment%252C%2520such%250Aas%2520refusal%2520training%252C%2520are%2520often%2520bypassed.%2520Techniques%2520such%2520as%2520adversarial%250Atraining%2520try%2520to%2520plug%2520these%2520holes%2520by%2520countering%2520specific%2520attacks.%2520As%2520an%250Aalternative%2520to%2520refusal%2520training%2520and%2520adversarial%2520training%252C%2520circuit-breaking%250Adirectly%2520controls%2520the%2520representations%2520that%2520are%2520responsible%2520for%2520harmful%2520outputs%250Ain%2520the%2520first%2520place.%2520Our%2520technique%2520can%2520be%2520applied%2520to%2520both%2520text-only%2520and%250Amultimodal%2520language%2520models%2520to%2520prevent%2520the%2520generation%2520of%2520harmful%2520outputs%2520without%250Asacrificing%2520utility%2520--%2520even%2520in%2520the%2520presence%2520of%2520powerful%2520unseen%2520attacks.%250ANotably%252C%2520while%2520adversarial%2520robustness%2520in%2520standalone%2520image%2520recognition%2520remains%250Aan%2520open%2520challenge%252C%2520circuit%2520breakers%2520allow%2520the%2520larger%2520multimodal%2520system%2520to%250Areliably%2520withstand%2520image%2520%2522hijacks%2522%2520that%2520aim%2520to%2520produce%2520harmful%2520content.%250AFinally%252C%2520we%2520extend%2520our%2520approach%2520to%2520AI%2520agents%252C%2520demonstrating%2520considerable%250Areductions%2520in%2520the%2520rate%2520of%2520harmful%2520actions%2520when%2520they%2520are%2520under%2520attack.%2520Our%250Aapproach%2520represents%2520a%2520significant%2520step%2520forward%2520in%2520the%2520development%2520of%2520reliable%250Asafeguards%2520to%2520harmful%2520behavior%2520and%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04313v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Alignment%20and%20Robustness%20with%20Circuit%20Breakers&entry.906535625=Andy%20Zou%20and%20Long%20Phan%20and%20Justin%20Wang%20and%20Derek%20Duenas%20and%20Maxwell%20Lin%20and%20Maksym%20Andriushchenko%20and%20Rowan%20Wang%20and%20Zico%20Kolter%20and%20Matt%20Fredrikson%20and%20Dan%20Hendrycks&entry.1292438233=%20%20AI%20systems%20can%20take%20harmful%20actions%20and%20are%20highly%20vulnerable%20to%20adversarial%0Aattacks.%20We%20present%20an%20approach%2C%20inspired%20by%20recent%20advances%20in%20representation%0Aengineering%2C%20that%20interrupts%20the%20models%20as%20they%20respond%20with%20harmful%20outputs%0Awith%20%22circuit%20breakers.%22%20Existing%20techniques%20aimed%20at%20improving%20alignment%2C%20such%0Aas%20refusal%20training%2C%20are%20often%20bypassed.%20Techniques%20such%20as%20adversarial%0Atraining%20try%20to%20plug%20these%20holes%20by%20countering%20specific%20attacks.%20As%20an%0Aalternative%20to%20refusal%20training%20and%20adversarial%20training%2C%20circuit-breaking%0Adirectly%20controls%20the%20representations%20that%20are%20responsible%20for%20harmful%20outputs%0Ain%20the%20first%20place.%20Our%20technique%20can%20be%20applied%20to%20both%20text-only%20and%0Amultimodal%20language%20models%20to%20prevent%20the%20generation%20of%20harmful%20outputs%20without%0Asacrificing%20utility%20--%20even%20in%20the%20presence%20of%20powerful%20unseen%20attacks.%0ANotably%2C%20while%20adversarial%20robustness%20in%20standalone%20image%20recognition%20remains%0Aan%20open%20challenge%2C%20circuit%20breakers%20allow%20the%20larger%20multimodal%20system%20to%0Areliably%20withstand%20image%20%22hijacks%22%20that%20aim%20to%20produce%20harmful%20content.%0AFinally%2C%20we%20extend%20our%20approach%20to%20AI%20agents%2C%20demonstrating%20considerable%0Areductions%20in%20the%20rate%20of%20harmful%20actions%20when%20they%20are%20under%20attack.%20Our%0Aapproach%20represents%20a%20significant%20step%20forward%20in%20the%20development%20of%20reliable%0Asafeguards%20to%20harmful%20behavior%20and%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04313v4&entry.124074799=Read"},
{"title": "Early Classification of Time Series: Taxonomy and Benchmark", "author": "Aur\u00e9lien Renault and Alexis Bondu and Antoine Cornu\u00e9jols and Vincent Lemaire", "abstract": "  In many situations, the measurements of a studied phenomenon are provided\nsequentially, and the prediction of its class needs to be made as early as\npossible so as not to incur too high a time penalty, but not too early and risk\npaying the cost of misclassification. This problem has been particularly\nstudied in the case of time series, and is known as Early Classification of\nTime Series (ECTS). Although it has been the subject of a growing body of\nliterature, there is still a lack of a systematic, shared evaluation protocol\nto compare the relative merits of the various existing methods. This document\nbegins by situating these methods within a principle-based taxonomy. It defines\ndimensions for organizing their evaluation, and then reports the results of a\nvery extensive set of experiments along these dimensions involving nine\nstate-of-the art ECTS algorithms. In addition, these and other experiments can\nbe carried out using an open-source library in which most of the existing ECTS\nalgorithms have been implemented (see \\url{https://github.com/ML-EDM/ml_edm}).\n", "link": "http://arxiv.org/abs/2406.18332v2", "date": "2024-07-12", "relevancy": 1.8664, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4981}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.449}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Classification%20of%20Time%20Series%3A%20Taxonomy%20and%20Benchmark&body=Title%3A%20Early%20Classification%20of%20Time%20Series%3A%20Taxonomy%20and%20Benchmark%0AAuthor%3A%20Aur%C3%A9lien%20Renault%20and%20Alexis%20Bondu%20and%20Antoine%20Cornu%C3%A9jols%20and%20Vincent%20Lemaire%0AAbstract%3A%20%20%20In%20many%20situations%2C%20the%20measurements%20of%20a%20studied%20phenomenon%20are%20provided%0Asequentially%2C%20and%20the%20prediction%20of%20its%20class%20needs%20to%20be%20made%20as%20early%20as%0Apossible%20so%20as%20not%20to%20incur%20too%20high%20a%20time%20penalty%2C%20but%20not%20too%20early%20and%20risk%0Apaying%20the%20cost%20of%20misclassification.%20This%20problem%20has%20been%20particularly%0Astudied%20in%20the%20case%20of%20time%20series%2C%20and%20is%20known%20as%20Early%20Classification%20of%0ATime%20Series%20%28ECTS%29.%20Although%20it%20has%20been%20the%20subject%20of%20a%20growing%20body%20of%0Aliterature%2C%20there%20is%20still%20a%20lack%20of%20a%20systematic%2C%20shared%20evaluation%20protocol%0Ato%20compare%20the%20relative%20merits%20of%20the%20various%20existing%20methods.%20This%20document%0Abegins%20by%20situating%20these%20methods%20within%20a%20principle-based%20taxonomy.%20It%20defines%0Adimensions%20for%20organizing%20their%20evaluation%2C%20and%20then%20reports%20the%20results%20of%20a%0Avery%20extensive%20set%20of%20experiments%20along%20these%20dimensions%20involving%20nine%0Astate-of-the%20art%20ECTS%20algorithms.%20In%20addition%2C%20these%20and%20other%20experiments%20can%0Abe%20carried%20out%20using%20an%20open-source%20library%20in%20which%20most%20of%20the%20existing%20ECTS%0Aalgorithms%20have%20been%20implemented%20%28see%20%5Curl%7Bhttps%3A//github.com/ML-EDM/ml_edm%7D%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18332v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Classification%2520of%2520Time%2520Series%253A%2520Taxonomy%2520and%2520Benchmark%26entry.906535625%3DAur%25C3%25A9lien%2520Renault%2520and%2520Alexis%2520Bondu%2520and%2520Antoine%2520Cornu%25C3%25A9jols%2520and%2520Vincent%2520Lemaire%26entry.1292438233%3D%2520%2520In%2520many%2520situations%252C%2520the%2520measurements%2520of%2520a%2520studied%2520phenomenon%2520are%2520provided%250Asequentially%252C%2520and%2520the%2520prediction%2520of%2520its%2520class%2520needs%2520to%2520be%2520made%2520as%2520early%2520as%250Apossible%2520so%2520as%2520not%2520to%2520incur%2520too%2520high%2520a%2520time%2520penalty%252C%2520but%2520not%2520too%2520early%2520and%2520risk%250Apaying%2520the%2520cost%2520of%2520misclassification.%2520This%2520problem%2520has%2520been%2520particularly%250Astudied%2520in%2520the%2520case%2520of%2520time%2520series%252C%2520and%2520is%2520known%2520as%2520Early%2520Classification%2520of%250ATime%2520Series%2520%2528ECTS%2529.%2520Although%2520it%2520has%2520been%2520the%2520subject%2520of%2520a%2520growing%2520body%2520of%250Aliterature%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520a%2520systematic%252C%2520shared%2520evaluation%2520protocol%250Ato%2520compare%2520the%2520relative%2520merits%2520of%2520the%2520various%2520existing%2520methods.%2520This%2520document%250Abegins%2520by%2520situating%2520these%2520methods%2520within%2520a%2520principle-based%2520taxonomy.%2520It%2520defines%250Adimensions%2520for%2520organizing%2520their%2520evaluation%252C%2520and%2520then%2520reports%2520the%2520results%2520of%2520a%250Avery%2520extensive%2520set%2520of%2520experiments%2520along%2520these%2520dimensions%2520involving%2520nine%250Astate-of-the%2520art%2520ECTS%2520algorithms.%2520In%2520addition%252C%2520these%2520and%2520other%2520experiments%2520can%250Abe%2520carried%2520out%2520using%2520an%2520open-source%2520library%2520in%2520which%2520most%2520of%2520the%2520existing%2520ECTS%250Aalgorithms%2520have%2520been%2520implemented%2520%2528see%2520%255Curl%257Bhttps%253A//github.com/ML-EDM/ml_edm%257D%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18332v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Classification%20of%20Time%20Series%3A%20Taxonomy%20and%20Benchmark&entry.906535625=Aur%C3%A9lien%20Renault%20and%20Alexis%20Bondu%20and%20Antoine%20Cornu%C3%A9jols%20and%20Vincent%20Lemaire&entry.1292438233=%20%20In%20many%20situations%2C%20the%20measurements%20of%20a%20studied%20phenomenon%20are%20provided%0Asequentially%2C%20and%20the%20prediction%20of%20its%20class%20needs%20to%20be%20made%20as%20early%20as%0Apossible%20so%20as%20not%20to%20incur%20too%20high%20a%20time%20penalty%2C%20but%20not%20too%20early%20and%20risk%0Apaying%20the%20cost%20of%20misclassification.%20This%20problem%20has%20been%20particularly%0Astudied%20in%20the%20case%20of%20time%20series%2C%20and%20is%20known%20as%20Early%20Classification%20of%0ATime%20Series%20%28ECTS%29.%20Although%20it%20has%20been%20the%20subject%20of%20a%20growing%20body%20of%0Aliterature%2C%20there%20is%20still%20a%20lack%20of%20a%20systematic%2C%20shared%20evaluation%20protocol%0Ato%20compare%20the%20relative%20merits%20of%20the%20various%20existing%20methods.%20This%20document%0Abegins%20by%20situating%20these%20methods%20within%20a%20principle-based%20taxonomy.%20It%20defines%0Adimensions%20for%20organizing%20their%20evaluation%2C%20and%20then%20reports%20the%20results%20of%20a%0Avery%20extensive%20set%20of%20experiments%20along%20these%20dimensions%20involving%20nine%0Astate-of-the%20art%20ECTS%20algorithms.%20In%20addition%2C%20these%20and%20other%20experiments%20can%0Abe%20carried%20out%20using%20an%20open-source%20library%20in%20which%20most%20of%20the%20existing%20ECTS%0Aalgorithms%20have%20been%20implemented%20%28see%20%5Curl%7Bhttps%3A//github.com/ML-EDM/ml_edm%7D%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18332v2&entry.124074799=Read"},
{"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "author": "Zeyu Han and Chao Gao and Jinyang Liu and Jeff Zhang and Sai Qian Zhang", "abstract": "  Large models represent a groundbreaking advancement in multiple application\nfields, enabling remarkable achievements across various tasks. However, their\nunprecedented scale comes with significant computational costs. These models,\noften consisting of billions of parameters, require vast amounts of\ncomputational resources for execution. Especially, the expansive scale and\ncomputational demands pose considerable challenges when customizing them for\nparticular downstream tasks, particularly over the hardware platforms\nconstrained by computational capabilities. Parameter Efficient Fine-Tuning\n(PEFT) provides a practical solution by efficiently adjusting the large models\nover the various downstream tasks. In particular, PEFT refers to the process of\nadjusting the parameters of a pre-trained large models to adapt it to a\nspecific task or domain while minimizing the number of additional parameters\nintroduced or computational resources required. This approach is particularly\nimportant when dealing with large-scale language models with high parameter\ncounts, as fine-tuning these models from scratch can be computationally\nexpensive and resource-intensive, posing considerable challenges in the\nsupporting system platform design. In this survey, we present comprehensive\nstudies of various PEFT algorithms, examining their performance and\ncomputational overhead. Moreover, we provide an overview of applications\ndeveloped using different PEFT algorithms and discuss common techniques\nemployed to mitigate computation costs for PEFT. In addition to providing an\nextensive survey from an algorithmic standpoint, we also examine various\nreal-world system designs to investigate the implementation costs associated\nwith different PEFT approaches. This survey serves as an indispensable resource\nfor researchers aiming to understand both the PEFT algorithm and its system\nimplementation, offering detailed ......\n", "link": "http://arxiv.org/abs/2403.14608v6", "date": "2024-07-12", "relevancy": 1.8527, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.496}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4749}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20Fine-Tuning%20for%20Large%20Models%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Parameter-Efficient%20Fine-Tuning%20for%20Large%20Models%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Zeyu%20Han%20and%20Chao%20Gao%20and%20Jinyang%20Liu%20and%20Jeff%20Zhang%20and%20Sai%20Qian%20Zhang%0AAbstract%3A%20%20%20Large%20models%20represent%20a%20groundbreaking%20advancement%20in%20multiple%20application%0Afields%2C%20enabling%20remarkable%20achievements%20across%20various%20tasks.%20However%2C%20their%0Aunprecedented%20scale%20comes%20with%20significant%20computational%20costs.%20These%20models%2C%0Aoften%20consisting%20of%20billions%20of%20parameters%2C%20require%20vast%20amounts%20of%0Acomputational%20resources%20for%20execution.%20Especially%2C%20the%20expansive%20scale%20and%0Acomputational%20demands%20pose%20considerable%20challenges%20when%20customizing%20them%20for%0Aparticular%20downstream%20tasks%2C%20particularly%20over%20the%20hardware%20platforms%0Aconstrained%20by%20computational%20capabilities.%20Parameter%20Efficient%20Fine-Tuning%0A%28PEFT%29%20provides%20a%20practical%20solution%20by%20efficiently%20adjusting%20the%20large%20models%0Aover%20the%20various%20downstream%20tasks.%20In%20particular%2C%20PEFT%20refers%20to%20the%20process%20of%0Aadjusting%20the%20parameters%20of%20a%20pre-trained%20large%20models%20to%20adapt%20it%20to%20a%0Aspecific%20task%20or%20domain%20while%20minimizing%20the%20number%20of%20additional%20parameters%0Aintroduced%20or%20computational%20resources%20required.%20This%20approach%20is%20particularly%0Aimportant%20when%20dealing%20with%20large-scale%20language%20models%20with%20high%20parameter%0Acounts%2C%20as%20fine-tuning%20these%20models%20from%20scratch%20can%20be%20computationally%0Aexpensive%20and%20resource-intensive%2C%20posing%20considerable%20challenges%20in%20the%0Asupporting%20system%20platform%20design.%20In%20this%20survey%2C%20we%20present%20comprehensive%0Astudies%20of%20various%20PEFT%20algorithms%2C%20examining%20their%20performance%20and%0Acomputational%20overhead.%20Moreover%2C%20we%20provide%20an%20overview%20of%20applications%0Adeveloped%20using%20different%20PEFT%20algorithms%20and%20discuss%20common%20techniques%0Aemployed%20to%20mitigate%20computation%20costs%20for%20PEFT.%20In%20addition%20to%20providing%20an%0Aextensive%20survey%20from%20an%20algorithmic%20standpoint%2C%20we%20also%20examine%20various%0Areal-world%20system%20designs%20to%20investigate%20the%20implementation%20costs%20associated%0Awith%20different%20PEFT%20approaches.%20This%20survey%20serves%20as%20an%20indispensable%20resource%0Afor%20researchers%20aiming%20to%20understand%20both%20the%20PEFT%20algorithm%20and%20its%20system%0Aimplementation%2C%20offering%20detailed%20......%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14608v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520Fine-Tuning%2520for%2520Large%2520Models%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DZeyu%2520Han%2520and%2520Chao%2520Gao%2520and%2520Jinyang%2520Liu%2520and%2520Jeff%2520Zhang%2520and%2520Sai%2520Qian%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520models%2520represent%2520a%2520groundbreaking%2520advancement%2520in%2520multiple%2520application%250Afields%252C%2520enabling%2520remarkable%2520achievements%2520across%2520various%2520tasks.%2520However%252C%2520their%250Aunprecedented%2520scale%2520comes%2520with%2520significant%2520computational%2520costs.%2520These%2520models%252C%250Aoften%2520consisting%2520of%2520billions%2520of%2520parameters%252C%2520require%2520vast%2520amounts%2520of%250Acomputational%2520resources%2520for%2520execution.%2520Especially%252C%2520the%2520expansive%2520scale%2520and%250Acomputational%2520demands%2520pose%2520considerable%2520challenges%2520when%2520customizing%2520them%2520for%250Aparticular%2520downstream%2520tasks%252C%2520particularly%2520over%2520the%2520hardware%2520platforms%250Aconstrained%2520by%2520computational%2520capabilities.%2520Parameter%2520Efficient%2520Fine-Tuning%250A%2528PEFT%2529%2520provides%2520a%2520practical%2520solution%2520by%2520efficiently%2520adjusting%2520the%2520large%2520models%250Aover%2520the%2520various%2520downstream%2520tasks.%2520In%2520particular%252C%2520PEFT%2520refers%2520to%2520the%2520process%2520of%250Aadjusting%2520the%2520parameters%2520of%2520a%2520pre-trained%2520large%2520models%2520to%2520adapt%2520it%2520to%2520a%250Aspecific%2520task%2520or%2520domain%2520while%2520minimizing%2520the%2520number%2520of%2520additional%2520parameters%250Aintroduced%2520or%2520computational%2520resources%2520required.%2520This%2520approach%2520is%2520particularly%250Aimportant%2520when%2520dealing%2520with%2520large-scale%2520language%2520models%2520with%2520high%2520parameter%250Acounts%252C%2520as%2520fine-tuning%2520these%2520models%2520from%2520scratch%2520can%2520be%2520computationally%250Aexpensive%2520and%2520resource-intensive%252C%2520posing%2520considerable%2520challenges%2520in%2520the%250Asupporting%2520system%2520platform%2520design.%2520In%2520this%2520survey%252C%2520we%2520present%2520comprehensive%250Astudies%2520of%2520various%2520PEFT%2520algorithms%252C%2520examining%2520their%2520performance%2520and%250Acomputational%2520overhead.%2520Moreover%252C%2520we%2520provide%2520an%2520overview%2520of%2520applications%250Adeveloped%2520using%2520different%2520PEFT%2520algorithms%2520and%2520discuss%2520common%2520techniques%250Aemployed%2520to%2520mitigate%2520computation%2520costs%2520for%2520PEFT.%2520In%2520addition%2520to%2520providing%2520an%250Aextensive%2520survey%2520from%2520an%2520algorithmic%2520standpoint%252C%2520we%2520also%2520examine%2520various%250Areal-world%2520system%2520designs%2520to%2520investigate%2520the%2520implementation%2520costs%2520associated%250Awith%2520different%2520PEFT%2520approaches.%2520This%2520survey%2520serves%2520as%2520an%2520indispensable%2520resource%250Afor%2520researchers%2520aiming%2520to%2520understand%2520both%2520the%2520PEFT%2520algorithm%2520and%2520its%2520system%250Aimplementation%252C%2520offering%2520detailed%2520......%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14608v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20Fine-Tuning%20for%20Large%20Models%3A%20A%20Comprehensive%20Survey&entry.906535625=Zeyu%20Han%20and%20Chao%20Gao%20and%20Jinyang%20Liu%20and%20Jeff%20Zhang%20and%20Sai%20Qian%20Zhang&entry.1292438233=%20%20Large%20models%20represent%20a%20groundbreaking%20advancement%20in%20multiple%20application%0Afields%2C%20enabling%20remarkable%20achievements%20across%20various%20tasks.%20However%2C%20their%0Aunprecedented%20scale%20comes%20with%20significant%20computational%20costs.%20These%20models%2C%0Aoften%20consisting%20of%20billions%20of%20parameters%2C%20require%20vast%20amounts%20of%0Acomputational%20resources%20for%20execution.%20Especially%2C%20the%20expansive%20scale%20and%0Acomputational%20demands%20pose%20considerable%20challenges%20when%20customizing%20them%20for%0Aparticular%20downstream%20tasks%2C%20particularly%20over%20the%20hardware%20platforms%0Aconstrained%20by%20computational%20capabilities.%20Parameter%20Efficient%20Fine-Tuning%0A%28PEFT%29%20provides%20a%20practical%20solution%20by%20efficiently%20adjusting%20the%20large%20models%0Aover%20the%20various%20downstream%20tasks.%20In%20particular%2C%20PEFT%20refers%20to%20the%20process%20of%0Aadjusting%20the%20parameters%20of%20a%20pre-trained%20large%20models%20to%20adapt%20it%20to%20a%0Aspecific%20task%20or%20domain%20while%20minimizing%20the%20number%20of%20additional%20parameters%0Aintroduced%20or%20computational%20resources%20required.%20This%20approach%20is%20particularly%0Aimportant%20when%20dealing%20with%20large-scale%20language%20models%20with%20high%20parameter%0Acounts%2C%20as%20fine-tuning%20these%20models%20from%20scratch%20can%20be%20computationally%0Aexpensive%20and%20resource-intensive%2C%20posing%20considerable%20challenges%20in%20the%0Asupporting%20system%20platform%20design.%20In%20this%20survey%2C%20we%20present%20comprehensive%0Astudies%20of%20various%20PEFT%20algorithms%2C%20examining%20their%20performance%20and%0Acomputational%20overhead.%20Moreover%2C%20we%20provide%20an%20overview%20of%20applications%0Adeveloped%20using%20different%20PEFT%20algorithms%20and%20discuss%20common%20techniques%0Aemployed%20to%20mitigate%20computation%20costs%20for%20PEFT.%20In%20addition%20to%20providing%20an%0Aextensive%20survey%20from%20an%20algorithmic%20standpoint%2C%20we%20also%20examine%20various%0Areal-world%20system%20designs%20to%20investigate%20the%20implementation%20costs%20associated%0Awith%20different%20PEFT%20approaches.%20This%20survey%20serves%20as%20an%20indispensable%20resource%0Afor%20researchers%20aiming%20to%20understand%20both%20the%20PEFT%20algorithm%20and%20its%20system%0Aimplementation%2C%20offering%20detailed%20......%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14608v6&entry.124074799=Read"},
{"title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention", "author": "Ramya Prabhu and Ajay Nayak and Jayashree Mohan and Ramachandran Ramjee and Ashish Panwar", "abstract": "  Efficient management of GPU memory is essential for high throughput LLM\ninference. Prior systems used to reserve KV-cache memory ahead-of-time that\nresulted in wasted capacity due to internal fragmentation. Inspired by demand\npaging, vLLM proposed PagedAttention to enable dynamic memory allocation for\nKV-cache. This approach eliminates fragmentation and improves serving\nthroughout. However, to be able to allocate physical memory dynamically,\nPagedAttention changes the layout of KV-cache from contiguous virtual memory to\nnon-contiguous virtual memory. As a consequence, one needs to rewrite the\nattention kernels to support paging, and implement a memory manager in the\nserving framework. This results in both performance and programming overheads,\nas well as portability challenges in adopting state-of-the-art attention\nkernels.\n  In this paper, we propose vAttention, a new approach for dynamic KV-cache\nmemory management. In contrast to PagedAttention, vAttention stores KV-cache in\ncontiguous virtual memory and leverages OS support for on-demand allocation of\nphysical memory. vAttention thus enables one to use state-of-the art attention\nkernels out-of-the-box by adding support for dynamic allocation of physical\nmemory without having to re-write their code. We implement vAttention in the\nvLLM serving stack to show that it also helps improve decode throughput by up\nto 1.99x over vLLM, and the end-to-end serving throughput by up to 1.22x and\n1.29x, compared to using the state-of-the-art PagedAttention based kernels of\nFlashAttention and FlashInfer.\n", "link": "http://arxiv.org/abs/2405.04437v2", "date": "2024-07-12", "relevancy": 1.844, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4848}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4543}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20vAttention%3A%20Dynamic%20Memory%20Management%20for%20Serving%20LLMs%20without%0A%20%20PagedAttention&body=Title%3A%20vAttention%3A%20Dynamic%20Memory%20Management%20for%20Serving%20LLMs%20without%0A%20%20PagedAttention%0AAuthor%3A%20Ramya%20Prabhu%20and%20Ajay%20Nayak%20and%20Jayashree%20Mohan%20and%20Ramachandran%20Ramjee%20and%20Ashish%20Panwar%0AAbstract%3A%20%20%20Efficient%20management%20of%20GPU%20memory%20is%20essential%20for%20high%20throughput%20LLM%0Ainference.%20Prior%20systems%20used%20to%20reserve%20KV-cache%20memory%20ahead-of-time%20that%0Aresulted%20in%20wasted%20capacity%20due%20to%20internal%20fragmentation.%20Inspired%20by%20demand%0Apaging%2C%20vLLM%20proposed%20PagedAttention%20to%20enable%20dynamic%20memory%20allocation%20for%0AKV-cache.%20This%20approach%20eliminates%20fragmentation%20and%20improves%20serving%0Athroughout.%20However%2C%20to%20be%20able%20to%20allocate%20physical%20memory%20dynamically%2C%0APagedAttention%20changes%20the%20layout%20of%20KV-cache%20from%20contiguous%20virtual%20memory%20to%0Anon-contiguous%20virtual%20memory.%20As%20a%20consequence%2C%20one%20needs%20to%20rewrite%20the%0Aattention%20kernels%20to%20support%20paging%2C%20and%20implement%20a%20memory%20manager%20in%20the%0Aserving%20framework.%20This%20results%20in%20both%20performance%20and%20programming%20overheads%2C%0Aas%20well%20as%20portability%20challenges%20in%20adopting%20state-of-the-art%20attention%0Akernels.%0A%20%20In%20this%20paper%2C%20we%20propose%20vAttention%2C%20a%20new%20approach%20for%20dynamic%20KV-cache%0Amemory%20management.%20In%20contrast%20to%20PagedAttention%2C%20vAttention%20stores%20KV-cache%20in%0Acontiguous%20virtual%20memory%20and%20leverages%20OS%20support%20for%20on-demand%20allocation%20of%0Aphysical%20memory.%20vAttention%20thus%20enables%20one%20to%20use%20state-of-the%20art%20attention%0Akernels%20out-of-the-box%20by%20adding%20support%20for%20dynamic%20allocation%20of%20physical%0Amemory%20without%20having%20to%20re-write%20their%20code.%20We%20implement%20vAttention%20in%20the%0AvLLM%20serving%20stack%20to%20show%20that%20it%20also%20helps%20improve%20decode%20throughput%20by%20up%0Ato%201.99x%20over%20vLLM%2C%20and%20the%20end-to-end%20serving%20throughput%20by%20up%20to%201.22x%20and%0A1.29x%2C%20compared%20to%20using%20the%20state-of-the-art%20PagedAttention%20based%20kernels%20of%0AFlashAttention%20and%20FlashInfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04437v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DvAttention%253A%2520Dynamic%2520Memory%2520Management%2520for%2520Serving%2520LLMs%2520without%250A%2520%2520PagedAttention%26entry.906535625%3DRamya%2520Prabhu%2520and%2520Ajay%2520Nayak%2520and%2520Jayashree%2520Mohan%2520and%2520Ramachandran%2520Ramjee%2520and%2520Ashish%2520Panwar%26entry.1292438233%3D%2520%2520Efficient%2520management%2520of%2520GPU%2520memory%2520is%2520essential%2520for%2520high%2520throughput%2520LLM%250Ainference.%2520Prior%2520systems%2520used%2520to%2520reserve%2520KV-cache%2520memory%2520ahead-of-time%2520that%250Aresulted%2520in%2520wasted%2520capacity%2520due%2520to%2520internal%2520fragmentation.%2520Inspired%2520by%2520demand%250Apaging%252C%2520vLLM%2520proposed%2520PagedAttention%2520to%2520enable%2520dynamic%2520memory%2520allocation%2520for%250AKV-cache.%2520This%2520approach%2520eliminates%2520fragmentation%2520and%2520improves%2520serving%250Athroughout.%2520However%252C%2520to%2520be%2520able%2520to%2520allocate%2520physical%2520memory%2520dynamically%252C%250APagedAttention%2520changes%2520the%2520layout%2520of%2520KV-cache%2520from%2520contiguous%2520virtual%2520memory%2520to%250Anon-contiguous%2520virtual%2520memory.%2520As%2520a%2520consequence%252C%2520one%2520needs%2520to%2520rewrite%2520the%250Aattention%2520kernels%2520to%2520support%2520paging%252C%2520and%2520implement%2520a%2520memory%2520manager%2520in%2520the%250Aserving%2520framework.%2520This%2520results%2520in%2520both%2520performance%2520and%2520programming%2520overheads%252C%250Aas%2520well%2520as%2520portability%2520challenges%2520in%2520adopting%2520state-of-the-art%2520attention%250Akernels.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520vAttention%252C%2520a%2520new%2520approach%2520for%2520dynamic%2520KV-cache%250Amemory%2520management.%2520In%2520contrast%2520to%2520PagedAttention%252C%2520vAttention%2520stores%2520KV-cache%2520in%250Acontiguous%2520virtual%2520memory%2520and%2520leverages%2520OS%2520support%2520for%2520on-demand%2520allocation%2520of%250Aphysical%2520memory.%2520vAttention%2520thus%2520enables%2520one%2520to%2520use%2520state-of-the%2520art%2520attention%250Akernels%2520out-of-the-box%2520by%2520adding%2520support%2520for%2520dynamic%2520allocation%2520of%2520physical%250Amemory%2520without%2520having%2520to%2520re-write%2520their%2520code.%2520We%2520implement%2520vAttention%2520in%2520the%250AvLLM%2520serving%2520stack%2520to%2520show%2520that%2520it%2520also%2520helps%2520improve%2520decode%2520throughput%2520by%2520up%250Ato%25201.99x%2520over%2520vLLM%252C%2520and%2520the%2520end-to-end%2520serving%2520throughput%2520by%2520up%2520to%25201.22x%2520and%250A1.29x%252C%2520compared%2520to%2520using%2520the%2520state-of-the-art%2520PagedAttention%2520based%2520kernels%2520of%250AFlashAttention%2520and%2520FlashInfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04437v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=vAttention%3A%20Dynamic%20Memory%20Management%20for%20Serving%20LLMs%20without%0A%20%20PagedAttention&entry.906535625=Ramya%20Prabhu%20and%20Ajay%20Nayak%20and%20Jayashree%20Mohan%20and%20Ramachandran%20Ramjee%20and%20Ashish%20Panwar&entry.1292438233=%20%20Efficient%20management%20of%20GPU%20memory%20is%20essential%20for%20high%20throughput%20LLM%0Ainference.%20Prior%20systems%20used%20to%20reserve%20KV-cache%20memory%20ahead-of-time%20that%0Aresulted%20in%20wasted%20capacity%20due%20to%20internal%20fragmentation.%20Inspired%20by%20demand%0Apaging%2C%20vLLM%20proposed%20PagedAttention%20to%20enable%20dynamic%20memory%20allocation%20for%0AKV-cache.%20This%20approach%20eliminates%20fragmentation%20and%20improves%20serving%0Athroughout.%20However%2C%20to%20be%20able%20to%20allocate%20physical%20memory%20dynamically%2C%0APagedAttention%20changes%20the%20layout%20of%20KV-cache%20from%20contiguous%20virtual%20memory%20to%0Anon-contiguous%20virtual%20memory.%20As%20a%20consequence%2C%20one%20needs%20to%20rewrite%20the%0Aattention%20kernels%20to%20support%20paging%2C%20and%20implement%20a%20memory%20manager%20in%20the%0Aserving%20framework.%20This%20results%20in%20both%20performance%20and%20programming%20overheads%2C%0Aas%20well%20as%20portability%20challenges%20in%20adopting%20state-of-the-art%20attention%0Akernels.%0A%20%20In%20this%20paper%2C%20we%20propose%20vAttention%2C%20a%20new%20approach%20for%20dynamic%20KV-cache%0Amemory%20management.%20In%20contrast%20to%20PagedAttention%2C%20vAttention%20stores%20KV-cache%20in%0Acontiguous%20virtual%20memory%20and%20leverages%20OS%20support%20for%20on-demand%20allocation%20of%0Aphysical%20memory.%20vAttention%20thus%20enables%20one%20to%20use%20state-of-the%20art%20attention%0Akernels%20out-of-the-box%20by%20adding%20support%20for%20dynamic%20allocation%20of%20physical%0Amemory%20without%20having%20to%20re-write%20their%20code.%20We%20implement%20vAttention%20in%20the%0AvLLM%20serving%20stack%20to%20show%20that%20it%20also%20helps%20improve%20decode%20throughput%20by%20up%0Ato%201.99x%20over%20vLLM%2C%20and%20the%20end-to-end%20serving%20throughput%20by%20up%20to%201.22x%20and%0A1.29x%2C%20compared%20to%20using%20the%20state-of-the-art%20PagedAttention%20based%20kernels%20of%0AFlashAttention%20and%20FlashInfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04437v2&entry.124074799=Read"},
{"title": "Needle in the Haystack for Memory Based Large Language Models", "author": "Elliot Nelson and Georgios Kollias and Payel Das and Subhajit Chaudhury and Soham Dan", "abstract": "  Current large language models (LLMs) often perform poorly on simple fact\nretrieval tasks. Here we investigate if coupling a dynamically adaptable\nexternal memory to a LLM can alleviate this problem. For this purpose, we test\nLarimar, a recently proposed language model architecture which uses an external\nassociative memory, on long-context recall tasks including passkey and\nneedle-in-the-haystack tests. We demonstrate that the external memory of\nLarimar, which allows fast write and read of an episode of text samples, can be\nused at test time to handle contexts much longer than those seen during\ntraining. We further show that the latent readouts from the memory (to which\nlong contexts are written) control the decoder towards generating correct\noutputs, with the memory stored off of the GPU. Compared to existing\ntransformer-based LLM architectures for long-context recall tasks that use\nlarger parameter counts or modified attention mechanisms, a relatively smaller\nsize Larimar is able to maintain strong performance without any task-specific\ntraining or training on longer contexts.\n", "link": "http://arxiv.org/abs/2407.01437v2", "date": "2024-07-12", "relevancy": 1.8426, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4877}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4459}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Needle%20in%20the%20Haystack%20for%20Memory%20Based%20Large%20Language%20Models&body=Title%3A%20Needle%20in%20the%20Haystack%20for%20Memory%20Based%20Large%20Language%20Models%0AAuthor%3A%20Elliot%20Nelson%20and%20Georgios%20Kollias%20and%20Payel%20Das%20and%20Subhajit%20Chaudhury%20and%20Soham%20Dan%0AAbstract%3A%20%20%20Current%20large%20language%20models%20%28LLMs%29%20often%20perform%20poorly%20on%20simple%20fact%0Aretrieval%20tasks.%20Here%20we%20investigate%20if%20coupling%20a%20dynamically%20adaptable%0Aexternal%20memory%20to%20a%20LLM%20can%20alleviate%20this%20problem.%20For%20this%20purpose%2C%20we%20test%0ALarimar%2C%20a%20recently%20proposed%20language%20model%20architecture%20which%20uses%20an%20external%0Aassociative%20memory%2C%20on%20long-context%20recall%20tasks%20including%20passkey%20and%0Aneedle-in-the-haystack%20tests.%20We%20demonstrate%20that%20the%20external%20memory%20of%0ALarimar%2C%20which%20allows%20fast%20write%20and%20read%20of%20an%20episode%20of%20text%20samples%2C%20can%20be%0Aused%20at%20test%20time%20to%20handle%20contexts%20much%20longer%20than%20those%20seen%20during%0Atraining.%20We%20further%20show%20that%20the%20latent%20readouts%20from%20the%20memory%20%28to%20which%0Along%20contexts%20are%20written%29%20control%20the%20decoder%20towards%20generating%20correct%0Aoutputs%2C%20with%20the%20memory%20stored%20off%20of%20the%20GPU.%20Compared%20to%20existing%0Atransformer-based%20LLM%20architectures%20for%20long-context%20recall%20tasks%20that%20use%0Alarger%20parameter%20counts%20or%20modified%20attention%20mechanisms%2C%20a%20relatively%20smaller%0Asize%20Larimar%20is%20able%20to%20maintain%20strong%20performance%20without%20any%20task-specific%0Atraining%20or%20training%20on%20longer%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01437v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeedle%2520in%2520the%2520Haystack%2520for%2520Memory%2520Based%2520Large%2520Language%2520Models%26entry.906535625%3DElliot%2520Nelson%2520and%2520Georgios%2520Kollias%2520and%2520Payel%2520Das%2520and%2520Subhajit%2520Chaudhury%2520and%2520Soham%2520Dan%26entry.1292438233%3D%2520%2520Current%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520perform%2520poorly%2520on%2520simple%2520fact%250Aretrieval%2520tasks.%2520Here%2520we%2520investigate%2520if%2520coupling%2520a%2520dynamically%2520adaptable%250Aexternal%2520memory%2520to%2520a%2520LLM%2520can%2520alleviate%2520this%2520problem.%2520For%2520this%2520purpose%252C%2520we%2520test%250ALarimar%252C%2520a%2520recently%2520proposed%2520language%2520model%2520architecture%2520which%2520uses%2520an%2520external%250Aassociative%2520memory%252C%2520on%2520long-context%2520recall%2520tasks%2520including%2520passkey%2520and%250Aneedle-in-the-haystack%2520tests.%2520We%2520demonstrate%2520that%2520the%2520external%2520memory%2520of%250ALarimar%252C%2520which%2520allows%2520fast%2520write%2520and%2520read%2520of%2520an%2520episode%2520of%2520text%2520samples%252C%2520can%2520be%250Aused%2520at%2520test%2520time%2520to%2520handle%2520contexts%2520much%2520longer%2520than%2520those%2520seen%2520during%250Atraining.%2520We%2520further%2520show%2520that%2520the%2520latent%2520readouts%2520from%2520the%2520memory%2520%2528to%2520which%250Along%2520contexts%2520are%2520written%2529%2520control%2520the%2520decoder%2520towards%2520generating%2520correct%250Aoutputs%252C%2520with%2520the%2520memory%2520stored%2520off%2520of%2520the%2520GPU.%2520Compared%2520to%2520existing%250Atransformer-based%2520LLM%2520architectures%2520for%2520long-context%2520recall%2520tasks%2520that%2520use%250Alarger%2520parameter%2520counts%2520or%2520modified%2520attention%2520mechanisms%252C%2520a%2520relatively%2520smaller%250Asize%2520Larimar%2520is%2520able%2520to%2520maintain%2520strong%2520performance%2520without%2520any%2520task-specific%250Atraining%2520or%2520training%2520on%2520longer%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01437v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Needle%20in%20the%20Haystack%20for%20Memory%20Based%20Large%20Language%20Models&entry.906535625=Elliot%20Nelson%20and%20Georgios%20Kollias%20and%20Payel%20Das%20and%20Subhajit%20Chaudhury%20and%20Soham%20Dan&entry.1292438233=%20%20Current%20large%20language%20models%20%28LLMs%29%20often%20perform%20poorly%20on%20simple%20fact%0Aretrieval%20tasks.%20Here%20we%20investigate%20if%20coupling%20a%20dynamically%20adaptable%0Aexternal%20memory%20to%20a%20LLM%20can%20alleviate%20this%20problem.%20For%20this%20purpose%2C%20we%20test%0ALarimar%2C%20a%20recently%20proposed%20language%20model%20architecture%20which%20uses%20an%20external%0Aassociative%20memory%2C%20on%20long-context%20recall%20tasks%20including%20passkey%20and%0Aneedle-in-the-haystack%20tests.%20We%20demonstrate%20that%20the%20external%20memory%20of%0ALarimar%2C%20which%20allows%20fast%20write%20and%20read%20of%20an%20episode%20of%20text%20samples%2C%20can%20be%0Aused%20at%20test%20time%20to%20handle%20contexts%20much%20longer%20than%20those%20seen%20during%0Atraining.%20We%20further%20show%20that%20the%20latent%20readouts%20from%20the%20memory%20%28to%20which%0Along%20contexts%20are%20written%29%20control%20the%20decoder%20towards%20generating%20correct%0Aoutputs%2C%20with%20the%20memory%20stored%20off%20of%20the%20GPU.%20Compared%20to%20existing%0Atransformer-based%20LLM%20architectures%20for%20long-context%20recall%20tasks%20that%20use%0Alarger%20parameter%20counts%20or%20modified%20attention%20mechanisms%2C%20a%20relatively%20smaller%0Asize%20Larimar%20is%20able%20to%20maintain%20strong%20performance%20without%20any%20task-specific%0Atraining%20or%20training%20on%20longer%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01437v2&entry.124074799=Read"},
{"title": "OneActor: Consistent Character Generation via Cluster-Conditioned\n  Guidance", "author": "Jiahao Wang and Caixia Yan and Haonan Lin and Weizhan Zhang and Mengmeng Wang and Tieliang Gong and Guang Dai and Hao Sun", "abstract": "  Text-to-image diffusion models benefit artists with high-quality image\ngeneration. Yet their stochastic nature hinders artists from creating\nconsistent images of the same subject. Existing methods try to tackle this\nchallenge and generate consistent content in various ways. However, they either\ndepend on external restricted data or require expensive tuning of the diffusion\nmodel. For this issue, we propose a novel one-shot tuning paradigm, termed as\nOneActor. It efficiently performs consistent subject generation solely driven\nby prompts via a learned semantic guidance to bypass the laborious backbone\ntuning. We lead the way to formalize the objective of consistent subject\ngeneration from a clustering perspective, and thus design a cluster-conditioned\nmodel. To mitigate the overfitting challenge shared by one-shot tuning\npipelines, we augment the tuning with auxiliary samples and devise two\ninference strategies: semantic interpolation and cluster guidance. These\ntechniques are later verified to significantly enhance the generation quality.\nComprehensive experiments show that our method outperforms a variety of\nbaselines with satisfactory subject consistency, superior prompt conformity as\nwell as high image quality. Our method is capable of multi-subject generation\nand compatible with popular diffusion extensions. Besides, we achieve a 4 times\nfaster tuning speed than tuning-based baselines and, if desired, avoid\nincreasing inference time. Furthermore, to our best knowledge, we are the first\nto prove that the semantic space of the diffusion model has the same\ninterpolation property as the latent space does. This property can serve as\nanother promising tool for fine generation control.\n", "link": "http://arxiv.org/abs/2404.10267v2", "date": "2024-07-12", "relevancy": 1.8417, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6193}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6165}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneActor%3A%20Consistent%20Character%20Generation%20via%20Cluster-Conditioned%0A%20%20Guidance&body=Title%3A%20OneActor%3A%20Consistent%20Character%20Generation%20via%20Cluster-Conditioned%0A%20%20Guidance%0AAuthor%3A%20Jiahao%20Wang%20and%20Caixia%20Yan%20and%20Haonan%20Lin%20and%20Weizhan%20Zhang%20and%20Mengmeng%20Wang%20and%20Tieliang%20Gong%20and%20Guang%20Dai%20and%20Hao%20Sun%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20benefit%20artists%20with%20high-quality%20image%0Ageneration.%20Yet%20their%20stochastic%20nature%20hinders%20artists%20from%20creating%0Aconsistent%20images%20of%20the%20same%20subject.%20Existing%20methods%20try%20to%20tackle%20this%0Achallenge%20and%20generate%20consistent%20content%20in%20various%20ways.%20However%2C%20they%20either%0Adepend%20on%20external%20restricted%20data%20or%20require%20expensive%20tuning%20of%20the%20diffusion%0Amodel.%20For%20this%20issue%2C%20we%20propose%20a%20novel%20one-shot%20tuning%20paradigm%2C%20termed%20as%0AOneActor.%20It%20efficiently%20performs%20consistent%20subject%20generation%20solely%20driven%0Aby%20prompts%20via%20a%20learned%20semantic%20guidance%20to%20bypass%20the%20laborious%20backbone%0Atuning.%20We%20lead%20the%20way%20to%20formalize%20the%20objective%20of%20consistent%20subject%0Ageneration%20from%20a%20clustering%20perspective%2C%20and%20thus%20design%20a%20cluster-conditioned%0Amodel.%20To%20mitigate%20the%20overfitting%20challenge%20shared%20by%20one-shot%20tuning%0Apipelines%2C%20we%20augment%20the%20tuning%20with%20auxiliary%20samples%20and%20devise%20two%0Ainference%20strategies%3A%20semantic%20interpolation%20and%20cluster%20guidance.%20These%0Atechniques%20are%20later%20verified%20to%20significantly%20enhance%20the%20generation%20quality.%0AComprehensive%20experiments%20show%20that%20our%20method%20outperforms%20a%20variety%20of%0Abaselines%20with%20satisfactory%20subject%20consistency%2C%20superior%20prompt%20conformity%20as%0Awell%20as%20high%20image%20quality.%20Our%20method%20is%20capable%20of%20multi-subject%20generation%0Aand%20compatible%20with%20popular%20diffusion%20extensions.%20Besides%2C%20we%20achieve%20a%204%20times%0Afaster%20tuning%20speed%20than%20tuning-based%20baselines%20and%2C%20if%20desired%2C%20avoid%0Aincreasing%20inference%20time.%20Furthermore%2C%20to%20our%20best%20knowledge%2C%20we%20are%20the%20first%0Ato%20prove%20that%20the%20semantic%20space%20of%20the%20diffusion%20model%20has%20the%20same%0Ainterpolation%20property%20as%20the%20latent%20space%20does.%20This%20property%20can%20serve%20as%0Aanother%20promising%20tool%20for%20fine%20generation%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10267v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneActor%253A%2520Consistent%2520Character%2520Generation%2520via%2520Cluster-Conditioned%250A%2520%2520Guidance%26entry.906535625%3DJiahao%2520Wang%2520and%2520Caixia%2520Yan%2520and%2520Haonan%2520Lin%2520and%2520Weizhan%2520Zhang%2520and%2520Mengmeng%2520Wang%2520and%2520Tieliang%2520Gong%2520and%2520Guang%2520Dai%2520and%2520Hao%2520Sun%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520benefit%2520artists%2520with%2520high-quality%2520image%250Ageneration.%2520Yet%2520their%2520stochastic%2520nature%2520hinders%2520artists%2520from%2520creating%250Aconsistent%2520images%2520of%2520the%2520same%2520subject.%2520Existing%2520methods%2520try%2520to%2520tackle%2520this%250Achallenge%2520and%2520generate%2520consistent%2520content%2520in%2520various%2520ways.%2520However%252C%2520they%2520either%250Adepend%2520on%2520external%2520restricted%2520data%2520or%2520require%2520expensive%2520tuning%2520of%2520the%2520diffusion%250Amodel.%2520For%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520one-shot%2520tuning%2520paradigm%252C%2520termed%2520as%250AOneActor.%2520It%2520efficiently%2520performs%2520consistent%2520subject%2520generation%2520solely%2520driven%250Aby%2520prompts%2520via%2520a%2520learned%2520semantic%2520guidance%2520to%2520bypass%2520the%2520laborious%2520backbone%250Atuning.%2520We%2520lead%2520the%2520way%2520to%2520formalize%2520the%2520objective%2520of%2520consistent%2520subject%250Ageneration%2520from%2520a%2520clustering%2520perspective%252C%2520and%2520thus%2520design%2520a%2520cluster-conditioned%250Amodel.%2520To%2520mitigate%2520the%2520overfitting%2520challenge%2520shared%2520by%2520one-shot%2520tuning%250Apipelines%252C%2520we%2520augment%2520the%2520tuning%2520with%2520auxiliary%2520samples%2520and%2520devise%2520two%250Ainference%2520strategies%253A%2520semantic%2520interpolation%2520and%2520cluster%2520guidance.%2520These%250Atechniques%2520are%2520later%2520verified%2520to%2520significantly%2520enhance%2520the%2520generation%2520quality.%250AComprehensive%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520a%2520variety%2520of%250Abaselines%2520with%2520satisfactory%2520subject%2520consistency%252C%2520superior%2520prompt%2520conformity%2520as%250Awell%2520as%2520high%2520image%2520quality.%2520Our%2520method%2520is%2520capable%2520of%2520multi-subject%2520generation%250Aand%2520compatible%2520with%2520popular%2520diffusion%2520extensions.%2520Besides%252C%2520we%2520achieve%2520a%25204%2520times%250Afaster%2520tuning%2520speed%2520than%2520tuning-based%2520baselines%2520and%252C%2520if%2520desired%252C%2520avoid%250Aincreasing%2520inference%2520time.%2520Furthermore%252C%2520to%2520our%2520best%2520knowledge%252C%2520we%2520are%2520the%2520first%250Ato%2520prove%2520that%2520the%2520semantic%2520space%2520of%2520the%2520diffusion%2520model%2520has%2520the%2520same%250Ainterpolation%2520property%2520as%2520the%2520latent%2520space%2520does.%2520This%2520property%2520can%2520serve%2520as%250Aanother%2520promising%2520tool%2520for%2520fine%2520generation%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10267v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneActor%3A%20Consistent%20Character%20Generation%20via%20Cluster-Conditioned%0A%20%20Guidance&entry.906535625=Jiahao%20Wang%20and%20Caixia%20Yan%20and%20Haonan%20Lin%20and%20Weizhan%20Zhang%20and%20Mengmeng%20Wang%20and%20Tieliang%20Gong%20and%20Guang%20Dai%20and%20Hao%20Sun&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20benefit%20artists%20with%20high-quality%20image%0Ageneration.%20Yet%20their%20stochastic%20nature%20hinders%20artists%20from%20creating%0Aconsistent%20images%20of%20the%20same%20subject.%20Existing%20methods%20try%20to%20tackle%20this%0Achallenge%20and%20generate%20consistent%20content%20in%20various%20ways.%20However%2C%20they%20either%0Adepend%20on%20external%20restricted%20data%20or%20require%20expensive%20tuning%20of%20the%20diffusion%0Amodel.%20For%20this%20issue%2C%20we%20propose%20a%20novel%20one-shot%20tuning%20paradigm%2C%20termed%20as%0AOneActor.%20It%20efficiently%20performs%20consistent%20subject%20generation%20solely%20driven%0Aby%20prompts%20via%20a%20learned%20semantic%20guidance%20to%20bypass%20the%20laborious%20backbone%0Atuning.%20We%20lead%20the%20way%20to%20formalize%20the%20objective%20of%20consistent%20subject%0Ageneration%20from%20a%20clustering%20perspective%2C%20and%20thus%20design%20a%20cluster-conditioned%0Amodel.%20To%20mitigate%20the%20overfitting%20challenge%20shared%20by%20one-shot%20tuning%0Apipelines%2C%20we%20augment%20the%20tuning%20with%20auxiliary%20samples%20and%20devise%20two%0Ainference%20strategies%3A%20semantic%20interpolation%20and%20cluster%20guidance.%20These%0Atechniques%20are%20later%20verified%20to%20significantly%20enhance%20the%20generation%20quality.%0AComprehensive%20experiments%20show%20that%20our%20method%20outperforms%20a%20variety%20of%0Abaselines%20with%20satisfactory%20subject%20consistency%2C%20superior%20prompt%20conformity%20as%0Awell%20as%20high%20image%20quality.%20Our%20method%20is%20capable%20of%20multi-subject%20generation%0Aand%20compatible%20with%20popular%20diffusion%20extensions.%20Besides%2C%20we%20achieve%20a%204%20times%0Afaster%20tuning%20speed%20than%20tuning-based%20baselines%20and%2C%20if%20desired%2C%20avoid%0Aincreasing%20inference%20time.%20Furthermore%2C%20to%20our%20best%20knowledge%2C%20we%20are%20the%20first%0Ato%20prove%20that%20the%20semantic%20space%20of%20the%20diffusion%20model%20has%20the%20same%0Ainterpolation%20property%20as%20the%20latent%20space%20does.%20This%20property%20can%20serve%20as%0Aanother%20promising%20tool%20for%20fine%20generation%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10267v2&entry.124074799=Read"},
{"title": "Flow-Based Generative Emulation of Grids of Stellar Evolutionary Models", "author": "Marc Hon and Yaguang Li and Joel Ong", "abstract": "  We present a flow-based generative approach to emulate grids of stellar\nevolutionary models. By interpreting the input parameters and output properties\nof these models as multi-dimensional probability distributions, we train\nconditional normalizing flows to learn and predict the complex relationships\nbetween grid inputs and outputs in the form of conditional joint distributions.\nLeveraging the expressive power and versatility of these flows, we showcase\ntheir ability to emulate a variety of evolutionary tracks and isochrones across\na continuous range of input parameters. In addition, we describe a simple\nBayesian approach for estimating stellar parameters using these flows and\ndemonstrate its application to asteroseismic datasets of red giants observed by\nthe Kepler mission. By applying this approach to red giants in open clusters\nNGC 6791 and NGC 6819, we illustrate how large age uncertainties can arise when\nfitting only to global asteroseismic and spectroscopic parameters without prior\ninformation on initial helium abundances and mixing length parameter values. We\nalso conduct inference using the flow at a large scale by determining revised\nestimates of masses and radii for 15,388 field red giants. These estimates show\nimproved agreement with results from existing grid-based modelling, reveal\ndistinct population-level features in the red clump, and suggest that the\nmasses of Kepler red giants previously determined using the corrected\nasteroseismic scaling relations have been overestimated by 5-10%.\n", "link": "http://arxiv.org/abs/2407.09427v1", "date": "2024-07-12", "relevancy": 1.8396, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5518}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4507}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow-Based%20Generative%20Emulation%20of%20Grids%20of%20Stellar%20Evolutionary%20Models&body=Title%3A%20Flow-Based%20Generative%20Emulation%20of%20Grids%20of%20Stellar%20Evolutionary%20Models%0AAuthor%3A%20Marc%20Hon%20and%20Yaguang%20Li%20and%20Joel%20Ong%0AAbstract%3A%20%20%20We%20present%20a%20flow-based%20generative%20approach%20to%20emulate%20grids%20of%20stellar%0Aevolutionary%20models.%20By%20interpreting%20the%20input%20parameters%20and%20output%20properties%0Aof%20these%20models%20as%20multi-dimensional%20probability%20distributions%2C%20we%20train%0Aconditional%20normalizing%20flows%20to%20learn%20and%20predict%20the%20complex%20relationships%0Abetween%20grid%20inputs%20and%20outputs%20in%20the%20form%20of%20conditional%20joint%20distributions.%0ALeveraging%20the%20expressive%20power%20and%20versatility%20of%20these%20flows%2C%20we%20showcase%0Atheir%20ability%20to%20emulate%20a%20variety%20of%20evolutionary%20tracks%20and%20isochrones%20across%0Aa%20continuous%20range%20of%20input%20parameters.%20In%20addition%2C%20we%20describe%20a%20simple%0ABayesian%20approach%20for%20estimating%20stellar%20parameters%20using%20these%20flows%20and%0Ademonstrate%20its%20application%20to%20asteroseismic%20datasets%20of%20red%20giants%20observed%20by%0Athe%20Kepler%20mission.%20By%20applying%20this%20approach%20to%20red%20giants%20in%20open%20clusters%0ANGC%206791%20and%20NGC%206819%2C%20we%20illustrate%20how%20large%20age%20uncertainties%20can%20arise%20when%0Afitting%20only%20to%20global%20asteroseismic%20and%20spectroscopic%20parameters%20without%20prior%0Ainformation%20on%20initial%20helium%20abundances%20and%20mixing%20length%20parameter%20values.%20We%0Aalso%20conduct%20inference%20using%20the%20flow%20at%20a%20large%20scale%20by%20determining%20revised%0Aestimates%20of%20masses%20and%20radii%20for%2015%2C388%20field%20red%20giants.%20These%20estimates%20show%0Aimproved%20agreement%20with%20results%20from%20existing%20grid-based%20modelling%2C%20reveal%0Adistinct%20population-level%20features%20in%20the%20red%20clump%2C%20and%20suggest%20that%20the%0Amasses%20of%20Kepler%20red%20giants%20previously%20determined%20using%20the%20corrected%0Aasteroseismic%20scaling%20relations%20have%20been%20overestimated%20by%205-10%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow-Based%2520Generative%2520Emulation%2520of%2520Grids%2520of%2520Stellar%2520Evolutionary%2520Models%26entry.906535625%3DMarc%2520Hon%2520and%2520Yaguang%2520Li%2520and%2520Joel%2520Ong%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520flow-based%2520generative%2520approach%2520to%2520emulate%2520grids%2520of%2520stellar%250Aevolutionary%2520models.%2520By%2520interpreting%2520the%2520input%2520parameters%2520and%2520output%2520properties%250Aof%2520these%2520models%2520as%2520multi-dimensional%2520probability%2520distributions%252C%2520we%2520train%250Aconditional%2520normalizing%2520flows%2520to%2520learn%2520and%2520predict%2520the%2520complex%2520relationships%250Abetween%2520grid%2520inputs%2520and%2520outputs%2520in%2520the%2520form%2520of%2520conditional%2520joint%2520distributions.%250ALeveraging%2520the%2520expressive%2520power%2520and%2520versatility%2520of%2520these%2520flows%252C%2520we%2520showcase%250Atheir%2520ability%2520to%2520emulate%2520a%2520variety%2520of%2520evolutionary%2520tracks%2520and%2520isochrones%2520across%250Aa%2520continuous%2520range%2520of%2520input%2520parameters.%2520In%2520addition%252C%2520we%2520describe%2520a%2520simple%250ABayesian%2520approach%2520for%2520estimating%2520stellar%2520parameters%2520using%2520these%2520flows%2520and%250Ademonstrate%2520its%2520application%2520to%2520asteroseismic%2520datasets%2520of%2520red%2520giants%2520observed%2520by%250Athe%2520Kepler%2520mission.%2520By%2520applying%2520this%2520approach%2520to%2520red%2520giants%2520in%2520open%2520clusters%250ANGC%25206791%2520and%2520NGC%25206819%252C%2520we%2520illustrate%2520how%2520large%2520age%2520uncertainties%2520can%2520arise%2520when%250Afitting%2520only%2520to%2520global%2520asteroseismic%2520and%2520spectroscopic%2520parameters%2520without%2520prior%250Ainformation%2520on%2520initial%2520helium%2520abundances%2520and%2520mixing%2520length%2520parameter%2520values.%2520We%250Aalso%2520conduct%2520inference%2520using%2520the%2520flow%2520at%2520a%2520large%2520scale%2520by%2520determining%2520revised%250Aestimates%2520of%2520masses%2520and%2520radii%2520for%252015%252C388%2520field%2520red%2520giants.%2520These%2520estimates%2520show%250Aimproved%2520agreement%2520with%2520results%2520from%2520existing%2520grid-based%2520modelling%252C%2520reveal%250Adistinct%2520population-level%2520features%2520in%2520the%2520red%2520clump%252C%2520and%2520suggest%2520that%2520the%250Amasses%2520of%2520Kepler%2520red%2520giants%2520previously%2520determined%2520using%2520the%2520corrected%250Aasteroseismic%2520scaling%2520relations%2520have%2520been%2520overestimated%2520by%25205-10%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow-Based%20Generative%20Emulation%20of%20Grids%20of%20Stellar%20Evolutionary%20Models&entry.906535625=Marc%20Hon%20and%20Yaguang%20Li%20and%20Joel%20Ong&entry.1292438233=%20%20We%20present%20a%20flow-based%20generative%20approach%20to%20emulate%20grids%20of%20stellar%0Aevolutionary%20models.%20By%20interpreting%20the%20input%20parameters%20and%20output%20properties%0Aof%20these%20models%20as%20multi-dimensional%20probability%20distributions%2C%20we%20train%0Aconditional%20normalizing%20flows%20to%20learn%20and%20predict%20the%20complex%20relationships%0Abetween%20grid%20inputs%20and%20outputs%20in%20the%20form%20of%20conditional%20joint%20distributions.%0ALeveraging%20the%20expressive%20power%20and%20versatility%20of%20these%20flows%2C%20we%20showcase%0Atheir%20ability%20to%20emulate%20a%20variety%20of%20evolutionary%20tracks%20and%20isochrones%20across%0Aa%20continuous%20range%20of%20input%20parameters.%20In%20addition%2C%20we%20describe%20a%20simple%0ABayesian%20approach%20for%20estimating%20stellar%20parameters%20using%20these%20flows%20and%0Ademonstrate%20its%20application%20to%20asteroseismic%20datasets%20of%20red%20giants%20observed%20by%0Athe%20Kepler%20mission.%20By%20applying%20this%20approach%20to%20red%20giants%20in%20open%20clusters%0ANGC%206791%20and%20NGC%206819%2C%20we%20illustrate%20how%20large%20age%20uncertainties%20can%20arise%20when%0Afitting%20only%20to%20global%20asteroseismic%20and%20spectroscopic%20parameters%20without%20prior%0Ainformation%20on%20initial%20helium%20abundances%20and%20mixing%20length%20parameter%20values.%20We%0Aalso%20conduct%20inference%20using%20the%20flow%20at%20a%20large%20scale%20by%20determining%20revised%0Aestimates%20of%20masses%20and%20radii%20for%2015%2C388%20field%20red%20giants.%20These%20estimates%20show%0Aimproved%20agreement%20with%20results%20from%20existing%20grid-based%20modelling%2C%20reveal%0Adistinct%20population-level%20features%20in%20the%20red%20clump%2C%20and%20suggest%20that%20the%0Amasses%20of%20Kepler%20red%20giants%20previously%20determined%20using%20the%20corrected%0Aasteroseismic%20scaling%20relations%20have%20been%20overestimated%20by%205-10%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09427v1&entry.124074799=Read"},
{"title": "Open-Canopy: A Country-Scale Benchmark for Canopy Height Estimation at\n  Very High Resolution", "author": "Fajwel Fogel and Yohann Perron and Nikola Besic and Laurent Saint-Andr\u00e9 and Agn\u00e8s Pellissier-Tanon and Martin Schwartz and Thomas Boudras and Ibrahim Fayad and Alexandre d'Aspremont and Loic Landrieu and Phillipe Ciais", "abstract": "  Estimating canopy height and canopy height change at meter resolution from\nsatellite imagery has numerous applications, such as monitoring forest health,\nlogging activities, wood resources, and carbon stocks. However, many existing\nforest datasets are based on commercial or closed data sources, restricting the\nreproducibility and evaluation of new approaches. To address this gap, we\nintroduce Open-Canopy, the first open-access and country-scale benchmark for\nvery high resolution (1.5 m) canopy height estimation. Covering more than\n87,000 km$^2$ across France, Open-Canopy combines SPOT satellite imagery with\nhigh resolution aerial LiDAR data. We also propose Open-Canopy-$\\Delta$, the\nfirst benchmark for canopy height change detection between two images taken at\ndifferent years, a particularly challenging task even for recent models. To\nestablish a robust foundation for these benchmarks, we evaluate a comprehensive\nlist of state-of-the-art computer vision models for canopy height estimation.\nThe dataset and associated codes can be accessed at\nhttps://github.com/fajwel/Open-Canopy.\n", "link": "http://arxiv.org/abs/2407.09392v1", "date": "2024-07-12", "relevancy": 1.832, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.488}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4399}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Canopy%3A%20A%20Country-Scale%20Benchmark%20for%20Canopy%20Height%20Estimation%20at%0A%20%20Very%20High%20Resolution&body=Title%3A%20Open-Canopy%3A%20A%20Country-Scale%20Benchmark%20for%20Canopy%20Height%20Estimation%20at%0A%20%20Very%20High%20Resolution%0AAuthor%3A%20Fajwel%20Fogel%20and%20Yohann%20Perron%20and%20Nikola%20Besic%20and%20Laurent%20Saint-Andr%C3%A9%20and%20Agn%C3%A8s%20Pellissier-Tanon%20and%20Martin%20Schwartz%20and%20Thomas%20Boudras%20and%20Ibrahim%20Fayad%20and%20Alexandre%20d%27Aspremont%20and%20Loic%20Landrieu%20and%20Phillipe%20Ciais%0AAbstract%3A%20%20%20Estimating%20canopy%20height%20and%20canopy%20height%20change%20at%20meter%20resolution%20from%0Asatellite%20imagery%20has%20numerous%20applications%2C%20such%20as%20monitoring%20forest%20health%2C%0Alogging%20activities%2C%20wood%20resources%2C%20and%20carbon%20stocks.%20However%2C%20many%20existing%0Aforest%20datasets%20are%20based%20on%20commercial%20or%20closed%20data%20sources%2C%20restricting%20the%0Areproducibility%20and%20evaluation%20of%20new%20approaches.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20Open-Canopy%2C%20the%20first%20open-access%20and%20country-scale%20benchmark%20for%0Avery%20high%20resolution%20%281.5%20m%29%20canopy%20height%20estimation.%20Covering%20more%20than%0A87%2C000%20km%24%5E2%24%20across%20France%2C%20Open-Canopy%20combines%20SPOT%20satellite%20imagery%20with%0Ahigh%20resolution%20aerial%20LiDAR%20data.%20We%20also%20propose%20Open-Canopy-%24%5CDelta%24%2C%20the%0Afirst%20benchmark%20for%20canopy%20height%20change%20detection%20between%20two%20images%20taken%20at%0Adifferent%20years%2C%20a%20particularly%20challenging%20task%20even%20for%20recent%20models.%20To%0Aestablish%20a%20robust%20foundation%20for%20these%20benchmarks%2C%20we%20evaluate%20a%20comprehensive%0Alist%20of%20state-of-the-art%20computer%20vision%20models%20for%20canopy%20height%20estimation.%0AThe%20dataset%20and%20associated%20codes%20can%20be%20accessed%20at%0Ahttps%3A//github.com/fajwel/Open-Canopy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Canopy%253A%2520A%2520Country-Scale%2520Benchmark%2520for%2520Canopy%2520Height%2520Estimation%2520at%250A%2520%2520Very%2520High%2520Resolution%26entry.906535625%3DFajwel%2520Fogel%2520and%2520Yohann%2520Perron%2520and%2520Nikola%2520Besic%2520and%2520Laurent%2520Saint-Andr%25C3%25A9%2520and%2520Agn%25C3%25A8s%2520Pellissier-Tanon%2520and%2520Martin%2520Schwartz%2520and%2520Thomas%2520Boudras%2520and%2520Ibrahim%2520Fayad%2520and%2520Alexandre%2520d%2527Aspremont%2520and%2520Loic%2520Landrieu%2520and%2520Phillipe%2520Ciais%26entry.1292438233%3D%2520%2520Estimating%2520canopy%2520height%2520and%2520canopy%2520height%2520change%2520at%2520meter%2520resolution%2520from%250Asatellite%2520imagery%2520has%2520numerous%2520applications%252C%2520such%2520as%2520monitoring%2520forest%2520health%252C%250Alogging%2520activities%252C%2520wood%2520resources%252C%2520and%2520carbon%2520stocks.%2520However%252C%2520many%2520existing%250Aforest%2520datasets%2520are%2520based%2520on%2520commercial%2520or%2520closed%2520data%2520sources%252C%2520restricting%2520the%250Areproducibility%2520and%2520evaluation%2520of%2520new%2520approaches.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520Open-Canopy%252C%2520the%2520first%2520open-access%2520and%2520country-scale%2520benchmark%2520for%250Avery%2520high%2520resolution%2520%25281.5%2520m%2529%2520canopy%2520height%2520estimation.%2520Covering%2520more%2520than%250A87%252C000%2520km%2524%255E2%2524%2520across%2520France%252C%2520Open-Canopy%2520combines%2520SPOT%2520satellite%2520imagery%2520with%250Ahigh%2520resolution%2520aerial%2520LiDAR%2520data.%2520We%2520also%2520propose%2520Open-Canopy-%2524%255CDelta%2524%252C%2520the%250Afirst%2520benchmark%2520for%2520canopy%2520height%2520change%2520detection%2520between%2520two%2520images%2520taken%2520at%250Adifferent%2520years%252C%2520a%2520particularly%2520challenging%2520task%2520even%2520for%2520recent%2520models.%2520To%250Aestablish%2520a%2520robust%2520foundation%2520for%2520these%2520benchmarks%252C%2520we%2520evaluate%2520a%2520comprehensive%250Alist%2520of%2520state-of-the-art%2520computer%2520vision%2520models%2520for%2520canopy%2520height%2520estimation.%250AThe%2520dataset%2520and%2520associated%2520codes%2520can%2520be%2520accessed%2520at%250Ahttps%253A//github.com/fajwel/Open-Canopy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Canopy%3A%20A%20Country-Scale%20Benchmark%20for%20Canopy%20Height%20Estimation%20at%0A%20%20Very%20High%20Resolution&entry.906535625=Fajwel%20Fogel%20and%20Yohann%20Perron%20and%20Nikola%20Besic%20and%20Laurent%20Saint-Andr%C3%A9%20and%20Agn%C3%A8s%20Pellissier-Tanon%20and%20Martin%20Schwartz%20and%20Thomas%20Boudras%20and%20Ibrahim%20Fayad%20and%20Alexandre%20d%27Aspremont%20and%20Loic%20Landrieu%20and%20Phillipe%20Ciais&entry.1292438233=%20%20Estimating%20canopy%20height%20and%20canopy%20height%20change%20at%20meter%20resolution%20from%0Asatellite%20imagery%20has%20numerous%20applications%2C%20such%20as%20monitoring%20forest%20health%2C%0Alogging%20activities%2C%20wood%20resources%2C%20and%20carbon%20stocks.%20However%2C%20many%20existing%0Aforest%20datasets%20are%20based%20on%20commercial%20or%20closed%20data%20sources%2C%20restricting%20the%0Areproducibility%20and%20evaluation%20of%20new%20approaches.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20Open-Canopy%2C%20the%20first%20open-access%20and%20country-scale%20benchmark%20for%0Avery%20high%20resolution%20%281.5%20m%29%20canopy%20height%20estimation.%20Covering%20more%20than%0A87%2C000%20km%24%5E2%24%20across%20France%2C%20Open-Canopy%20combines%20SPOT%20satellite%20imagery%20with%0Ahigh%20resolution%20aerial%20LiDAR%20data.%20We%20also%20propose%20Open-Canopy-%24%5CDelta%24%2C%20the%0Afirst%20benchmark%20for%20canopy%20height%20change%20detection%20between%20two%20images%20taken%20at%0Adifferent%20years%2C%20a%20particularly%20challenging%20task%20even%20for%20recent%20models.%20To%0Aestablish%20a%20robust%20foundation%20for%20these%20benchmarks%2C%20we%20evaluate%20a%20comprehensive%0Alist%20of%20state-of-the-art%20computer%20vision%20models%20for%20canopy%20height%20estimation.%0AThe%20dataset%20and%20associated%20codes%20can%20be%20accessed%20at%0Ahttps%3A//github.com/fajwel/Open-Canopy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09392v1&entry.124074799=Read"},
{"title": "Conformal Inductive Graph Neural Networks", "author": "Soroush H. Zargarbashi and Aleksandar Bojchevski", "abstract": "  Conformal prediction (CP) transforms any model's output into prediction sets\nguaranteed to include (cover) the true label. CP requires exchangeability, a\nrelaxation of the i.i.d. assumption, to obtain a valid distribution-free\ncoverage guarantee. This makes it directly applicable to transductive\nnode-classification. However, conventional CP cannot be applied in inductive\nsettings due to the implicit shift in the (calibration) scores caused by\nmessage passing with the new nodes. We fix this issue for both cases of node\nand edge-exchangeable graphs, recovering the standard coverage guarantee\nwithout sacrificing statistical efficiency. We further prove that the guarantee\nholds independently of the prediction time, e.g. upon arrival of a new\nnode/edge or at any subsequent moment.\n", "link": "http://arxiv.org/abs/2407.09173v1", "date": "2024-07-12", "relevancy": 1.8299, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4792}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4472}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Inductive%20Graph%20Neural%20Networks&body=Title%3A%20Conformal%20Inductive%20Graph%20Neural%20Networks%0AAuthor%3A%20Soroush%20H.%20Zargarbashi%20and%20Aleksandar%20Bojchevski%0AAbstract%3A%20%20%20Conformal%20prediction%20%28CP%29%20transforms%20any%20model%27s%20output%20into%20prediction%20sets%0Aguaranteed%20to%20include%20%28cover%29%20the%20true%20label.%20CP%20requires%20exchangeability%2C%20a%0Arelaxation%20of%20the%20i.i.d.%20assumption%2C%20to%20obtain%20a%20valid%20distribution-free%0Acoverage%20guarantee.%20This%20makes%20it%20directly%20applicable%20to%20transductive%0Anode-classification.%20However%2C%20conventional%20CP%20cannot%20be%20applied%20in%20inductive%0Asettings%20due%20to%20the%20implicit%20shift%20in%20the%20%28calibration%29%20scores%20caused%20by%0Amessage%20passing%20with%20the%20new%20nodes.%20We%20fix%20this%20issue%20for%20both%20cases%20of%20node%0Aand%20edge-exchangeable%20graphs%2C%20recovering%20the%20standard%20coverage%20guarantee%0Awithout%20sacrificing%20statistical%20efficiency.%20We%20further%20prove%20that%20the%20guarantee%0Aholds%20independently%20of%20the%20prediction%20time%2C%20e.g.%20upon%20arrival%20of%20a%20new%0Anode/edge%20or%20at%20any%20subsequent%20moment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Inductive%2520Graph%2520Neural%2520Networks%26entry.906535625%3DSoroush%2520H.%2520Zargarbashi%2520and%2520Aleksandar%2520Bojchevski%26entry.1292438233%3D%2520%2520Conformal%2520prediction%2520%2528CP%2529%2520transforms%2520any%2520model%2527s%2520output%2520into%2520prediction%2520sets%250Aguaranteed%2520to%2520include%2520%2528cover%2529%2520the%2520true%2520label.%2520CP%2520requires%2520exchangeability%252C%2520a%250Arelaxation%2520of%2520the%2520i.i.d.%2520assumption%252C%2520to%2520obtain%2520a%2520valid%2520distribution-free%250Acoverage%2520guarantee.%2520This%2520makes%2520it%2520directly%2520applicable%2520to%2520transductive%250Anode-classification.%2520However%252C%2520conventional%2520CP%2520cannot%2520be%2520applied%2520in%2520inductive%250Asettings%2520due%2520to%2520the%2520implicit%2520shift%2520in%2520the%2520%2528calibration%2529%2520scores%2520caused%2520by%250Amessage%2520passing%2520with%2520the%2520new%2520nodes.%2520We%2520fix%2520this%2520issue%2520for%2520both%2520cases%2520of%2520node%250Aand%2520edge-exchangeable%2520graphs%252C%2520recovering%2520the%2520standard%2520coverage%2520guarantee%250Awithout%2520sacrificing%2520statistical%2520efficiency.%2520We%2520further%2520prove%2520that%2520the%2520guarantee%250Aholds%2520independently%2520of%2520the%2520prediction%2520time%252C%2520e.g.%2520upon%2520arrival%2520of%2520a%2520new%250Anode/edge%2520or%2520at%2520any%2520subsequent%2520moment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Inductive%20Graph%20Neural%20Networks&entry.906535625=Soroush%20H.%20Zargarbashi%20and%20Aleksandar%20Bojchevski&entry.1292438233=%20%20Conformal%20prediction%20%28CP%29%20transforms%20any%20model%27s%20output%20into%20prediction%20sets%0Aguaranteed%20to%20include%20%28cover%29%20the%20true%20label.%20CP%20requires%20exchangeability%2C%20a%0Arelaxation%20of%20the%20i.i.d.%20assumption%2C%20to%20obtain%20a%20valid%20distribution-free%0Acoverage%20guarantee.%20This%20makes%20it%20directly%20applicable%20to%20transductive%0Anode-classification.%20However%2C%20conventional%20CP%20cannot%20be%20applied%20in%20inductive%0Asettings%20due%20to%20the%20implicit%20shift%20in%20the%20%28calibration%29%20scores%20caused%20by%0Amessage%20passing%20with%20the%20new%20nodes.%20We%20fix%20this%20issue%20for%20both%20cases%20of%20node%0Aand%20edge-exchangeable%20graphs%2C%20recovering%20the%20standard%20coverage%20guarantee%0Awithout%20sacrificing%20statistical%20efficiency.%20We%20further%20prove%20that%20the%20guarantee%0Aholds%20independently%20of%20the%20prediction%20time%2C%20e.g.%20upon%20arrival%20of%20a%20new%0Anode/edge%20or%20at%20any%20subsequent%20moment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09173v1&entry.124074799=Read"},
{"title": "A Perspective on Foundation Models for the Electric Power Grid", "author": "Hendrik F. Hamann and Thomas Brunschwiler and Blazhe Gjorgiev and Leonardo S. A. Martins and Alban Puech and Anna Varbella and Jonas Weiss and Juan Bernabe-Moreno and Alexandre Blondin Mass\u00e9 and Seong Choi and Ian Foster and Bri-Mathias Hodge and Rishabh Jain and Kibaek Kim and Vincent Mai and Fran\u00e7ois Mirall\u00e8s and Martin De Montigny and Octavio Ramos-Lea\u00f1os and Hussein Supr\u00eame and Le Xie and El-Nasser S. Youssef and Arnaud Zinflou and Alexander J. Belvi and Ricardo J. Bessa and Bishnu Prasad Bhattari and Johannes Schmude and Stanislav Sobolevsky", "abstract": "  Foundation models (FMs) currently dominate news headlines. They employ\nadvanced deep learning architectures to extract structural information\nautonomously from vast datasets through self-supervision. The resulting rich\nrepresentations of complex systems and dynamics can be applied to many\ndownstream applications. Therefore, FMs can find uses in electric power grids,\nchallenged by the energy transition and climate change. In this paper, we call\nfor the development of, and state why we believe in, the potential of FMs for\nelectric grids. We highlight their strengths and weaknesses amidst the\nchallenges of a changing grid. We argue that an FM learning from diverse grid\ndata and topologies could unlock transformative capabilities, pioneering a new\napproach in leveraging AI to redefine how we manage complexity and uncertainty\nin the electric grid. Finally, we discuss a power grid FM concept, namely\nGridFM, based on graph neural networks and show how different downstream tasks\nbenefit.\n", "link": "http://arxiv.org/abs/2407.09434v1", "date": "2024-07-12", "relevancy": 1.8272, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5135}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4573}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Perspective%20on%20Foundation%20Models%20for%20the%20Electric%20Power%20Grid&body=Title%3A%20A%20Perspective%20on%20Foundation%20Models%20for%20the%20Electric%20Power%20Grid%0AAuthor%3A%20Hendrik%20F.%20Hamann%20and%20Thomas%20Brunschwiler%20and%20Blazhe%20Gjorgiev%20and%20Leonardo%20S.%20A.%20Martins%20and%20Alban%20Puech%20and%20Anna%20Varbella%20and%20Jonas%20Weiss%20and%20Juan%20Bernabe-Moreno%20and%20Alexandre%20Blondin%20Mass%C3%A9%20and%20Seong%20Choi%20and%20Ian%20Foster%20and%20Bri-Mathias%20Hodge%20and%20Rishabh%20Jain%20and%20Kibaek%20Kim%20and%20Vincent%20Mai%20and%20Fran%C3%A7ois%20Mirall%C3%A8s%20and%20Martin%20De%20Montigny%20and%20Octavio%20Ramos-Lea%C3%B1os%20and%20Hussein%20Supr%C3%AAme%20and%20Le%20Xie%20and%20El-Nasser%20S.%20Youssef%20and%20Arnaud%20Zinflou%20and%20Alexander%20J.%20Belvi%20and%20Ricardo%20J.%20Bessa%20and%20Bishnu%20Prasad%20Bhattari%20and%20Johannes%20Schmude%20and%20Stanislav%20Sobolevsky%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20currently%20dominate%20news%20headlines.%20They%20employ%0Aadvanced%20deep%20learning%20architectures%20to%20extract%20structural%20information%0Aautonomously%20from%20vast%20datasets%20through%20self-supervision.%20The%20resulting%20rich%0Arepresentations%20of%20complex%20systems%20and%20dynamics%20can%20be%20applied%20to%20many%0Adownstream%20applications.%20Therefore%2C%20FMs%20can%20find%20uses%20in%20electric%20power%20grids%2C%0Achallenged%20by%20the%20energy%20transition%20and%20climate%20change.%20In%20this%20paper%2C%20we%20call%0Afor%20the%20development%20of%2C%20and%20state%20why%20we%20believe%20in%2C%20the%20potential%20of%20FMs%20for%0Aelectric%20grids.%20We%20highlight%20their%20strengths%20and%20weaknesses%20amidst%20the%0Achallenges%20of%20a%20changing%20grid.%20We%20argue%20that%20an%20FM%20learning%20from%20diverse%20grid%0Adata%20and%20topologies%20could%20unlock%20transformative%20capabilities%2C%20pioneering%20a%20new%0Aapproach%20in%20leveraging%20AI%20to%20redefine%20how%20we%20manage%20complexity%20and%20uncertainty%0Ain%20the%20electric%20grid.%20Finally%2C%20we%20discuss%20a%20power%20grid%20FM%20concept%2C%20namely%0AGridFM%2C%20based%20on%20graph%20neural%20networks%20and%20show%20how%20different%20downstream%20tasks%0Abenefit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Perspective%2520on%2520Foundation%2520Models%2520for%2520the%2520Electric%2520Power%2520Grid%26entry.906535625%3DHendrik%2520F.%2520Hamann%2520and%2520Thomas%2520Brunschwiler%2520and%2520Blazhe%2520Gjorgiev%2520and%2520Leonardo%2520S.%2520A.%2520Martins%2520and%2520Alban%2520Puech%2520and%2520Anna%2520Varbella%2520and%2520Jonas%2520Weiss%2520and%2520Juan%2520Bernabe-Moreno%2520and%2520Alexandre%2520Blondin%2520Mass%25C3%25A9%2520and%2520Seong%2520Choi%2520and%2520Ian%2520Foster%2520and%2520Bri-Mathias%2520Hodge%2520and%2520Rishabh%2520Jain%2520and%2520Kibaek%2520Kim%2520and%2520Vincent%2520Mai%2520and%2520Fran%25C3%25A7ois%2520Mirall%25C3%25A8s%2520and%2520Martin%2520De%2520Montigny%2520and%2520Octavio%2520Ramos-Lea%25C3%25B1os%2520and%2520Hussein%2520Supr%25C3%25AAme%2520and%2520Le%2520Xie%2520and%2520El-Nasser%2520S.%2520Youssef%2520and%2520Arnaud%2520Zinflou%2520and%2520Alexander%2520J.%2520Belvi%2520and%2520Ricardo%2520J.%2520Bessa%2520and%2520Bishnu%2520Prasad%2520Bhattari%2520and%2520Johannes%2520Schmude%2520and%2520Stanislav%2520Sobolevsky%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520currently%2520dominate%2520news%2520headlines.%2520They%2520employ%250Aadvanced%2520deep%2520learning%2520architectures%2520to%2520extract%2520structural%2520information%250Aautonomously%2520from%2520vast%2520datasets%2520through%2520self-supervision.%2520The%2520resulting%2520rich%250Arepresentations%2520of%2520complex%2520systems%2520and%2520dynamics%2520can%2520be%2520applied%2520to%2520many%250Adownstream%2520applications.%2520Therefore%252C%2520FMs%2520can%2520find%2520uses%2520in%2520electric%2520power%2520grids%252C%250Achallenged%2520by%2520the%2520energy%2520transition%2520and%2520climate%2520change.%2520In%2520this%2520paper%252C%2520we%2520call%250Afor%2520the%2520development%2520of%252C%2520and%2520state%2520why%2520we%2520believe%2520in%252C%2520the%2520potential%2520of%2520FMs%2520for%250Aelectric%2520grids.%2520We%2520highlight%2520their%2520strengths%2520and%2520weaknesses%2520amidst%2520the%250Achallenges%2520of%2520a%2520changing%2520grid.%2520We%2520argue%2520that%2520an%2520FM%2520learning%2520from%2520diverse%2520grid%250Adata%2520and%2520topologies%2520could%2520unlock%2520transformative%2520capabilities%252C%2520pioneering%2520a%2520new%250Aapproach%2520in%2520leveraging%2520AI%2520to%2520redefine%2520how%2520we%2520manage%2520complexity%2520and%2520uncertainty%250Ain%2520the%2520electric%2520grid.%2520Finally%252C%2520we%2520discuss%2520a%2520power%2520grid%2520FM%2520concept%252C%2520namely%250AGridFM%252C%2520based%2520on%2520graph%2520neural%2520networks%2520and%2520show%2520how%2520different%2520downstream%2520tasks%250Abenefit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Perspective%20on%20Foundation%20Models%20for%20the%20Electric%20Power%20Grid&entry.906535625=Hendrik%20F.%20Hamann%20and%20Thomas%20Brunschwiler%20and%20Blazhe%20Gjorgiev%20and%20Leonardo%20S.%20A.%20Martins%20and%20Alban%20Puech%20and%20Anna%20Varbella%20and%20Jonas%20Weiss%20and%20Juan%20Bernabe-Moreno%20and%20Alexandre%20Blondin%20Mass%C3%A9%20and%20Seong%20Choi%20and%20Ian%20Foster%20and%20Bri-Mathias%20Hodge%20and%20Rishabh%20Jain%20and%20Kibaek%20Kim%20and%20Vincent%20Mai%20and%20Fran%C3%A7ois%20Mirall%C3%A8s%20and%20Martin%20De%20Montigny%20and%20Octavio%20Ramos-Lea%C3%B1os%20and%20Hussein%20Supr%C3%AAme%20and%20Le%20Xie%20and%20El-Nasser%20S.%20Youssef%20and%20Arnaud%20Zinflou%20and%20Alexander%20J.%20Belvi%20and%20Ricardo%20J.%20Bessa%20and%20Bishnu%20Prasad%20Bhattari%20and%20Johannes%20Schmude%20and%20Stanislav%20Sobolevsky&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20currently%20dominate%20news%20headlines.%20They%20employ%0Aadvanced%20deep%20learning%20architectures%20to%20extract%20structural%20information%0Aautonomously%20from%20vast%20datasets%20through%20self-supervision.%20The%20resulting%20rich%0Arepresentations%20of%20complex%20systems%20and%20dynamics%20can%20be%20applied%20to%20many%0Adownstream%20applications.%20Therefore%2C%20FMs%20can%20find%20uses%20in%20electric%20power%20grids%2C%0Achallenged%20by%20the%20energy%20transition%20and%20climate%20change.%20In%20this%20paper%2C%20we%20call%0Afor%20the%20development%20of%2C%20and%20state%20why%20we%20believe%20in%2C%20the%20potential%20of%20FMs%20for%0Aelectric%20grids.%20We%20highlight%20their%20strengths%20and%20weaknesses%20amidst%20the%0Achallenges%20of%20a%20changing%20grid.%20We%20argue%20that%20an%20FM%20learning%20from%20diverse%20grid%0Adata%20and%20topologies%20could%20unlock%20transformative%20capabilities%2C%20pioneering%20a%20new%0Aapproach%20in%20leveraging%20AI%20to%20redefine%20how%20we%20manage%20complexity%20and%20uncertainty%0Ain%20the%20electric%20grid.%20Finally%2C%20we%20discuss%20a%20power%20grid%20FM%20concept%2C%20namely%0AGridFM%2C%20based%20on%20graph%20neural%20networks%20and%20show%20how%20different%20downstream%20tasks%0Abenefit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09434v1&entry.124074799=Read"},
{"title": "ProDepth: Boosting Self-Supervised Multi-Frame Monocular Depth with\n  Probabilistic Fusion", "author": "Sungmin Woo and Wonjoon Lee and Woo Jin Kim and Dogyoon Lee and Sangyoun Lee", "abstract": "  Self-supervised multi-frame monocular depth estimation relies on the\ngeometric consistency between successive frames under the assumption of a\nstatic scene. However, the presence of moving objects in dynamic scenes\nintroduces inevitable inconsistencies, causing misaligned multi-frame feature\nmatching and misleading self-supervision during training. In this paper, we\npropose a novel framework called ProDepth, which effectively addresses the\nmismatch problem caused by dynamic objects using a probabilistic approach. We\ninitially deduce the uncertainty associated with static scene assumption by\nadopting an auxiliary decoder. This decoder analyzes inconsistencies embedded\nin the cost volume, inferring the probability of areas being dynamic. We then\ndirectly rectify the erroneous cost volume for dynamic areas through a\nProbabilistic Cost Volume Modulation (PCVM) module. Specifically, we derive\nprobability distributions of depth candidates from both single-frame and\nmulti-frame cues, modulating the cost volume by adaptively fusing those\ndistributions based on the inferred uncertainty. Additionally, we present a\nself-supervision loss reweighting strategy that not only masks out incorrect\nsupervision with high uncertainty but also mitigates the risks in remaining\npossible dynamic areas in accordance with the probability. Our proposed method\nexcels over state-of-the-art approaches in all metrics on both Cityscapes and\nKITTI datasets, and demonstrates superior generalization ability on the Waymo\nOpen dataset.\n", "link": "http://arxiv.org/abs/2407.09303v1", "date": "2024-07-12", "relevancy": 1.8237, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6277}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6058}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProDepth%3A%20Boosting%20Self-Supervised%20Multi-Frame%20Monocular%20Depth%20with%0A%20%20Probabilistic%20Fusion&body=Title%3A%20ProDepth%3A%20Boosting%20Self-Supervised%20Multi-Frame%20Monocular%20Depth%20with%0A%20%20Probabilistic%20Fusion%0AAuthor%3A%20Sungmin%20Woo%20and%20Wonjoon%20Lee%20and%20Woo%20Jin%20Kim%20and%20Dogyoon%20Lee%20and%20Sangyoun%20Lee%0AAbstract%3A%20%20%20Self-supervised%20multi-frame%20monocular%20depth%20estimation%20relies%20on%20the%0Ageometric%20consistency%20between%20successive%20frames%20under%20the%20assumption%20of%20a%0Astatic%20scene.%20However%2C%20the%20presence%20of%20moving%20objects%20in%20dynamic%20scenes%0Aintroduces%20inevitable%20inconsistencies%2C%20causing%20misaligned%20multi-frame%20feature%0Amatching%20and%20misleading%20self-supervision%20during%20training.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20framework%20called%20ProDepth%2C%20which%20effectively%20addresses%20the%0Amismatch%20problem%20caused%20by%20dynamic%20objects%20using%20a%20probabilistic%20approach.%20We%0Ainitially%20deduce%20the%20uncertainty%20associated%20with%20static%20scene%20assumption%20by%0Aadopting%20an%20auxiliary%20decoder.%20This%20decoder%20analyzes%20inconsistencies%20embedded%0Ain%20the%20cost%20volume%2C%20inferring%20the%20probability%20of%20areas%20being%20dynamic.%20We%20then%0Adirectly%20rectify%20the%20erroneous%20cost%20volume%20for%20dynamic%20areas%20through%20a%0AProbabilistic%20Cost%20Volume%20Modulation%20%28PCVM%29%20module.%20Specifically%2C%20we%20derive%0Aprobability%20distributions%20of%20depth%20candidates%20from%20both%20single-frame%20and%0Amulti-frame%20cues%2C%20modulating%20the%20cost%20volume%20by%20adaptively%20fusing%20those%0Adistributions%20based%20on%20the%20inferred%20uncertainty.%20Additionally%2C%20we%20present%20a%0Aself-supervision%20loss%20reweighting%20strategy%20that%20not%20only%20masks%20out%20incorrect%0Asupervision%20with%20high%20uncertainty%20but%20also%20mitigates%20the%20risks%20in%20remaining%0Apossible%20dynamic%20areas%20in%20accordance%20with%20the%20probability.%20Our%20proposed%20method%0Aexcels%20over%20state-of-the-art%20approaches%20in%20all%20metrics%20on%20both%20Cityscapes%20and%0AKITTI%20datasets%2C%20and%20demonstrates%20superior%20generalization%20ability%20on%20the%20Waymo%0AOpen%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09303v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProDepth%253A%2520Boosting%2520Self-Supervised%2520Multi-Frame%2520Monocular%2520Depth%2520with%250A%2520%2520Probabilistic%2520Fusion%26entry.906535625%3DSungmin%2520Woo%2520and%2520Wonjoon%2520Lee%2520and%2520Woo%2520Jin%2520Kim%2520and%2520Dogyoon%2520Lee%2520and%2520Sangyoun%2520Lee%26entry.1292438233%3D%2520%2520Self-supervised%2520multi-frame%2520monocular%2520depth%2520estimation%2520relies%2520on%2520the%250Ageometric%2520consistency%2520between%2520successive%2520frames%2520under%2520the%2520assumption%2520of%2520a%250Astatic%2520scene.%2520However%252C%2520the%2520presence%2520of%2520moving%2520objects%2520in%2520dynamic%2520scenes%250Aintroduces%2520inevitable%2520inconsistencies%252C%2520causing%2520misaligned%2520multi-frame%2520feature%250Amatching%2520and%2520misleading%2520self-supervision%2520during%2520training.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520called%2520ProDepth%252C%2520which%2520effectively%2520addresses%2520the%250Amismatch%2520problem%2520caused%2520by%2520dynamic%2520objects%2520using%2520a%2520probabilistic%2520approach.%2520We%250Ainitially%2520deduce%2520the%2520uncertainty%2520associated%2520with%2520static%2520scene%2520assumption%2520by%250Aadopting%2520an%2520auxiliary%2520decoder.%2520This%2520decoder%2520analyzes%2520inconsistencies%2520embedded%250Ain%2520the%2520cost%2520volume%252C%2520inferring%2520the%2520probability%2520of%2520areas%2520being%2520dynamic.%2520We%2520then%250Adirectly%2520rectify%2520the%2520erroneous%2520cost%2520volume%2520for%2520dynamic%2520areas%2520through%2520a%250AProbabilistic%2520Cost%2520Volume%2520Modulation%2520%2528PCVM%2529%2520module.%2520Specifically%252C%2520we%2520derive%250Aprobability%2520distributions%2520of%2520depth%2520candidates%2520from%2520both%2520single-frame%2520and%250Amulti-frame%2520cues%252C%2520modulating%2520the%2520cost%2520volume%2520by%2520adaptively%2520fusing%2520those%250Adistributions%2520based%2520on%2520the%2520inferred%2520uncertainty.%2520Additionally%252C%2520we%2520present%2520a%250Aself-supervision%2520loss%2520reweighting%2520strategy%2520that%2520not%2520only%2520masks%2520out%2520incorrect%250Asupervision%2520with%2520high%2520uncertainty%2520but%2520also%2520mitigates%2520the%2520risks%2520in%2520remaining%250Apossible%2520dynamic%2520areas%2520in%2520accordance%2520with%2520the%2520probability.%2520Our%2520proposed%2520method%250Aexcels%2520over%2520state-of-the-art%2520approaches%2520in%2520all%2520metrics%2520on%2520both%2520Cityscapes%2520and%250AKITTI%2520datasets%252C%2520and%2520demonstrates%2520superior%2520generalization%2520ability%2520on%2520the%2520Waymo%250AOpen%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09303v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProDepth%3A%20Boosting%20Self-Supervised%20Multi-Frame%20Monocular%20Depth%20with%0A%20%20Probabilistic%20Fusion&entry.906535625=Sungmin%20Woo%20and%20Wonjoon%20Lee%20and%20Woo%20Jin%20Kim%20and%20Dogyoon%20Lee%20and%20Sangyoun%20Lee&entry.1292438233=%20%20Self-supervised%20multi-frame%20monocular%20depth%20estimation%20relies%20on%20the%0Ageometric%20consistency%20between%20successive%20frames%20under%20the%20assumption%20of%20a%0Astatic%20scene.%20However%2C%20the%20presence%20of%20moving%20objects%20in%20dynamic%20scenes%0Aintroduces%20inevitable%20inconsistencies%2C%20causing%20misaligned%20multi-frame%20feature%0Amatching%20and%20misleading%20self-supervision%20during%20training.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20framework%20called%20ProDepth%2C%20which%20effectively%20addresses%20the%0Amismatch%20problem%20caused%20by%20dynamic%20objects%20using%20a%20probabilistic%20approach.%20We%0Ainitially%20deduce%20the%20uncertainty%20associated%20with%20static%20scene%20assumption%20by%0Aadopting%20an%20auxiliary%20decoder.%20This%20decoder%20analyzes%20inconsistencies%20embedded%0Ain%20the%20cost%20volume%2C%20inferring%20the%20probability%20of%20areas%20being%20dynamic.%20We%20then%0Adirectly%20rectify%20the%20erroneous%20cost%20volume%20for%20dynamic%20areas%20through%20a%0AProbabilistic%20Cost%20Volume%20Modulation%20%28PCVM%29%20module.%20Specifically%2C%20we%20derive%0Aprobability%20distributions%20of%20depth%20candidates%20from%20both%20single-frame%20and%0Amulti-frame%20cues%2C%20modulating%20the%20cost%20volume%20by%20adaptively%20fusing%20those%0Adistributions%20based%20on%20the%20inferred%20uncertainty.%20Additionally%2C%20we%20present%20a%0Aself-supervision%20loss%20reweighting%20strategy%20that%20not%20only%20masks%20out%20incorrect%0Asupervision%20with%20high%20uncertainty%20but%20also%20mitigates%20the%20risks%20in%20remaining%0Apossible%20dynamic%20areas%20in%20accordance%20with%20the%20probability.%20Our%20proposed%20method%0Aexcels%20over%20state-of-the-art%20approaches%20in%20all%20metrics%20on%20both%20Cityscapes%20and%0AKITTI%20datasets%2C%20and%20demonstrates%20superior%20generalization%20ability%20on%20the%20Waymo%0AOpen%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09303v1&entry.124074799=Read"},
{"title": "H2O-Danube3 Technical Report", "author": "Pascal Pfeiffer and Philipp Singer and Yauhen Babakhin and Gabor Fodor and Nischay Dhankhar and Sri Satish Ambati", "abstract": "  We present H2O-Danube3, a series of small language models consisting of\nH2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T\ntokens. Our models are pre-trained on high quality Web data consisting of\nprimarily English tokens in three stages with different data mixes before final\nsupervised tuning for chat version. The models exhibit highly competitive\nmetrics across a multitude of academic, chat, and fine-tuning benchmarks.\nThanks to its compact architecture, H2O-Danube3 can be efficiently run on a\nmodern smartphone, enabling local inference and rapid processing capabilities\neven on mobile devices. We make all models openly available under Apache 2.0\nlicense further democratizing LLMs to a wider audience economically.\n", "link": "http://arxiv.org/abs/2407.09276v1", "date": "2024-07-12", "relevancy": 1.8196, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4624}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4511}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H2O-Danube3%20Technical%20Report&body=Title%3A%20H2O-Danube3%20Technical%20Report%0AAuthor%3A%20Pascal%20Pfeiffer%20and%20Philipp%20Singer%20and%20Yauhen%20Babakhin%20and%20Gabor%20Fodor%20and%20Nischay%20Dhankhar%20and%20Sri%20Satish%20Ambati%0AAbstract%3A%20%20%20We%20present%20H2O-Danube3%2C%20a%20series%20of%20small%20language%20models%20consisting%20of%0AH2O-Danube3-4B%2C%20trained%20on%206T%20tokens%20and%20H2O-Danube3-500M%2C%20trained%20on%204T%0Atokens.%20Our%20models%20are%20pre-trained%20on%20high%20quality%20Web%20data%20consisting%20of%0Aprimarily%20English%20tokens%20in%20three%20stages%20with%20different%20data%20mixes%20before%20final%0Asupervised%20tuning%20for%20chat%20version.%20The%20models%20exhibit%20highly%20competitive%0Ametrics%20across%20a%20multitude%20of%20academic%2C%20chat%2C%20and%20fine-tuning%20benchmarks.%0AThanks%20to%20its%20compact%20architecture%2C%20H2O-Danube3%20can%20be%20efficiently%20run%20on%20a%0Amodern%20smartphone%2C%20enabling%20local%20inference%20and%20rapid%20processing%20capabilities%0Aeven%20on%20mobile%20devices.%20We%20make%20all%20models%20openly%20available%20under%20Apache%202.0%0Alicense%20further%20democratizing%20LLMs%20to%20a%20wider%20audience%20economically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH2O-Danube3%2520Technical%2520Report%26entry.906535625%3DPascal%2520Pfeiffer%2520and%2520Philipp%2520Singer%2520and%2520Yauhen%2520Babakhin%2520and%2520Gabor%2520Fodor%2520and%2520Nischay%2520Dhankhar%2520and%2520Sri%2520Satish%2520Ambati%26entry.1292438233%3D%2520%2520We%2520present%2520H2O-Danube3%252C%2520a%2520series%2520of%2520small%2520language%2520models%2520consisting%2520of%250AH2O-Danube3-4B%252C%2520trained%2520on%25206T%2520tokens%2520and%2520H2O-Danube3-500M%252C%2520trained%2520on%25204T%250Atokens.%2520Our%2520models%2520are%2520pre-trained%2520on%2520high%2520quality%2520Web%2520data%2520consisting%2520of%250Aprimarily%2520English%2520tokens%2520in%2520three%2520stages%2520with%2520different%2520data%2520mixes%2520before%2520final%250Asupervised%2520tuning%2520for%2520chat%2520version.%2520The%2520models%2520exhibit%2520highly%2520competitive%250Ametrics%2520across%2520a%2520multitude%2520of%2520academic%252C%2520chat%252C%2520and%2520fine-tuning%2520benchmarks.%250AThanks%2520to%2520its%2520compact%2520architecture%252C%2520H2O-Danube3%2520can%2520be%2520efficiently%2520run%2520on%2520a%250Amodern%2520smartphone%252C%2520enabling%2520local%2520inference%2520and%2520rapid%2520processing%2520capabilities%250Aeven%2520on%2520mobile%2520devices.%2520We%2520make%2520all%2520models%2520openly%2520available%2520under%2520Apache%25202.0%250Alicense%2520further%2520democratizing%2520LLMs%2520to%2520a%2520wider%2520audience%2520economically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H2O-Danube3%20Technical%20Report&entry.906535625=Pascal%20Pfeiffer%20and%20Philipp%20Singer%20and%20Yauhen%20Babakhin%20and%20Gabor%20Fodor%20and%20Nischay%20Dhankhar%20and%20Sri%20Satish%20Ambati&entry.1292438233=%20%20We%20present%20H2O-Danube3%2C%20a%20series%20of%20small%20language%20models%20consisting%20of%0AH2O-Danube3-4B%2C%20trained%20on%206T%20tokens%20and%20H2O-Danube3-500M%2C%20trained%20on%204T%0Atokens.%20Our%20models%20are%20pre-trained%20on%20high%20quality%20Web%20data%20consisting%20of%0Aprimarily%20English%20tokens%20in%20three%20stages%20with%20different%20data%20mixes%20before%20final%0Asupervised%20tuning%20for%20chat%20version.%20The%20models%20exhibit%20highly%20competitive%0Ametrics%20across%20a%20multitude%20of%20academic%2C%20chat%2C%20and%20fine-tuning%20benchmarks.%0AThanks%20to%20its%20compact%20architecture%2C%20H2O-Danube3%20can%20be%20efficiently%20run%20on%20a%0Amodern%20smartphone%2C%20enabling%20local%20inference%20and%20rapid%20processing%20capabilities%0Aeven%20on%20mobile%20devices.%20We%20make%20all%20models%20openly%20available%20under%20Apache%202.0%0Alicense%20further%20democratizing%20LLMs%20to%20a%20wider%20audience%20economically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09276v1&entry.124074799=Read"},
{"title": "Synthetic Cancer -- Augmenting Worms with LLMs", "author": "Benjamin Zimmerman and David Zollikofer", "abstract": "  With increasingly sophisticated large language models (LLMs), the potential\nfor abuse rises drastically. As a submission to the Swiss AI Safety Prize, we\npresent a novel type of metamorphic malware leveraging LLMs for two key\nprocesses. First, LLMs are used for automatic code rewriting to evade\nsignature-based detection by antimalware programs. The malware then spreads its\ncopies via email by utilizing an LLM to socially engineer email replies to\nencourage recipients to execute the attached malware. Our submission includes a\nfunctional minimal prototype, highlighting the risks that LLMs pose for\ncybersecurity and underscoring the need for further research into intelligent\nmalware.\n", "link": "http://arxiv.org/abs/2406.19570v2", "date": "2024-07-12", "relevancy": 1.6419, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4208}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4114}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Cancer%20--%20Augmenting%20Worms%20with%20LLMs&body=Title%3A%20Synthetic%20Cancer%20--%20Augmenting%20Worms%20with%20LLMs%0AAuthor%3A%20Benjamin%20Zimmerman%20and%20David%20Zollikofer%0AAbstract%3A%20%20%20With%20increasingly%20sophisticated%20large%20language%20models%20%28LLMs%29%2C%20the%20potential%0Afor%20abuse%20rises%20drastically.%20As%20a%20submission%20to%20the%20Swiss%20AI%20Safety%20Prize%2C%20we%0Apresent%20a%20novel%20type%20of%20metamorphic%20malware%20leveraging%20LLMs%20for%20two%20key%0Aprocesses.%20First%2C%20LLMs%20are%20used%20for%20automatic%20code%20rewriting%20to%20evade%0Asignature-based%20detection%20by%20antimalware%20programs.%20The%20malware%20then%20spreads%20its%0Acopies%20via%20email%20by%20utilizing%20an%20LLM%20to%20socially%20engineer%20email%20replies%20to%0Aencourage%20recipients%20to%20execute%20the%20attached%20malware.%20Our%20submission%20includes%20a%0Afunctional%20minimal%20prototype%2C%20highlighting%20the%20risks%20that%20LLMs%20pose%20for%0Acybersecurity%20and%20underscoring%20the%20need%20for%20further%20research%20into%20intelligent%0Amalware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Cancer%2520--%2520Augmenting%2520Worms%2520with%2520LLMs%26entry.906535625%3DBenjamin%2520Zimmerman%2520and%2520David%2520Zollikofer%26entry.1292438233%3D%2520%2520With%2520increasingly%2520sophisticated%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520the%2520potential%250Afor%2520abuse%2520rises%2520drastically.%2520As%2520a%2520submission%2520to%2520the%2520Swiss%2520AI%2520Safety%2520Prize%252C%2520we%250Apresent%2520a%2520novel%2520type%2520of%2520metamorphic%2520malware%2520leveraging%2520LLMs%2520for%2520two%2520key%250Aprocesses.%2520First%252C%2520LLMs%2520are%2520used%2520for%2520automatic%2520code%2520rewriting%2520to%2520evade%250Asignature-based%2520detection%2520by%2520antimalware%2520programs.%2520The%2520malware%2520then%2520spreads%2520its%250Acopies%2520via%2520email%2520by%2520utilizing%2520an%2520LLM%2520to%2520socially%2520engineer%2520email%2520replies%2520to%250Aencourage%2520recipients%2520to%2520execute%2520the%2520attached%2520malware.%2520Our%2520submission%2520includes%2520a%250Afunctional%2520minimal%2520prototype%252C%2520highlighting%2520the%2520risks%2520that%2520LLMs%2520pose%2520for%250Acybersecurity%2520and%2520underscoring%2520the%2520need%2520for%2520further%2520research%2520into%2520intelligent%250Amalware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Cancer%20--%20Augmenting%20Worms%20with%20LLMs&entry.906535625=Benjamin%20Zimmerman%20and%20David%20Zollikofer&entry.1292438233=%20%20With%20increasingly%20sophisticated%20large%20language%20models%20%28LLMs%29%2C%20the%20potential%0Afor%20abuse%20rises%20drastically.%20As%20a%20submission%20to%20the%20Swiss%20AI%20Safety%20Prize%2C%20we%0Apresent%20a%20novel%20type%20of%20metamorphic%20malware%20leveraging%20LLMs%20for%20two%20key%0Aprocesses.%20First%2C%20LLMs%20are%20used%20for%20automatic%20code%20rewriting%20to%20evade%0Asignature-based%20detection%20by%20antimalware%20programs.%20The%20malware%20then%20spreads%20its%0Acopies%20via%20email%20by%20utilizing%20an%20LLM%20to%20socially%20engineer%20email%20replies%20to%0Aencourage%20recipients%20to%20execute%20the%20attached%20malware.%20Our%20submission%20includes%20a%0Afunctional%20minimal%20prototype%2C%20highlighting%20the%20risks%20that%20LLMs%20pose%20for%0Acybersecurity%20and%20underscoring%20the%20need%20for%20further%20research%20into%20intelligent%0Amalware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19570v2&entry.124074799=Read"},
{"title": "Advancing Medical Image Segmentation with Mini-Net: A Lightweight\n  Solution Tailored for Efficient Segmentation of Medical Images", "author": "Syed Javed and Tariq M. Khan and Abdul Qayyum and Arcot Sowmya and Imran Razzak", "abstract": "  Accurate segmentation of anatomical structures and abnormalities in medical\nimages is crucial for computer-aided diagnosis and analysis. While deep\nlearning techniques excel at this task, their computational demands pose\nchallenges. Additionally, some cutting-edge segmentation methods, though\neffective for general object segmentation, may not be optimised for medical\nimages. To address these issues, we propose Mini-Net, a lightweight\nsegmentation network specifically designed for medical images. With fewer than\n38,000 parameters, Mini-Net efficiently captures both high- and low-frequency\nfeatures, enabling real-time applications in various medical imaging scenarios.\nWe evaluate Mini-Net on various datasets, including DRIVE, STARE, ISIC-2016,\nISIC-2018, and MoNuSeg, demonstrating its robustness and good performance\ncompared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.17520v3", "date": "2024-07-12", "relevancy": 1.4752, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5228}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4849}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Medical%20Image%20Segmentation%20with%20Mini-Net%3A%20A%20Lightweight%0A%20%20Solution%20Tailored%20for%20Efficient%20Segmentation%20of%20Medical%20Images&body=Title%3A%20Advancing%20Medical%20Image%20Segmentation%20with%20Mini-Net%3A%20A%20Lightweight%0A%20%20Solution%20Tailored%20for%20Efficient%20Segmentation%20of%20Medical%20Images%0AAuthor%3A%20Syed%20Javed%20and%20Tariq%20M.%20Khan%20and%20Abdul%20Qayyum%20and%20Arcot%20Sowmya%20and%20Imran%20Razzak%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20anatomical%20structures%20and%20abnormalities%20in%20medical%0Aimages%20is%20crucial%20for%20computer-aided%20diagnosis%20and%20analysis.%20While%20deep%0Alearning%20techniques%20excel%20at%20this%20task%2C%20their%20computational%20demands%20pose%0Achallenges.%20Additionally%2C%20some%20cutting-edge%20segmentation%20methods%2C%20though%0Aeffective%20for%20general%20object%20segmentation%2C%20may%20not%20be%20optimised%20for%20medical%0Aimages.%20To%20address%20these%20issues%2C%20we%20propose%20Mini-Net%2C%20a%20lightweight%0Asegmentation%20network%20specifically%20designed%20for%20medical%20images.%20With%20fewer%20than%0A38%2C000%20parameters%2C%20Mini-Net%20efficiently%20captures%20both%20high-%20and%20low-frequency%0Afeatures%2C%20enabling%20real-time%20applications%20in%20various%20medical%20imaging%20scenarios.%0AWe%20evaluate%20Mini-Net%20on%20various%20datasets%2C%20including%20DRIVE%2C%20STARE%2C%20ISIC-2016%2C%0AISIC-2018%2C%20and%20MoNuSeg%2C%20demonstrating%20its%20robustness%20and%20good%20performance%0Acompared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17520v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Medical%2520Image%2520Segmentation%2520with%2520Mini-Net%253A%2520A%2520Lightweight%250A%2520%2520Solution%2520Tailored%2520for%2520Efficient%2520Segmentation%2520of%2520Medical%2520Images%26entry.906535625%3DSyed%2520Javed%2520and%2520Tariq%2520M.%2520Khan%2520and%2520Abdul%2520Qayyum%2520and%2520Arcot%2520Sowmya%2520and%2520Imran%2520Razzak%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520anatomical%2520structures%2520and%2520abnormalities%2520in%2520medical%250Aimages%2520is%2520crucial%2520for%2520computer-aided%2520diagnosis%2520and%2520analysis.%2520While%2520deep%250Alearning%2520techniques%2520excel%2520at%2520this%2520task%252C%2520their%2520computational%2520demands%2520pose%250Achallenges.%2520Additionally%252C%2520some%2520cutting-edge%2520segmentation%2520methods%252C%2520though%250Aeffective%2520for%2520general%2520object%2520segmentation%252C%2520may%2520not%2520be%2520optimised%2520for%2520medical%250Aimages.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Mini-Net%252C%2520a%2520lightweight%250Asegmentation%2520network%2520specifically%2520designed%2520for%2520medical%2520images.%2520With%2520fewer%2520than%250A38%252C000%2520parameters%252C%2520Mini-Net%2520efficiently%2520captures%2520both%2520high-%2520and%2520low-frequency%250Afeatures%252C%2520enabling%2520real-time%2520applications%2520in%2520various%2520medical%2520imaging%2520scenarios.%250AWe%2520evaluate%2520Mini-Net%2520on%2520various%2520datasets%252C%2520including%2520DRIVE%252C%2520STARE%252C%2520ISIC-2016%252C%250AISIC-2018%252C%2520and%2520MoNuSeg%252C%2520demonstrating%2520its%2520robustness%2520and%2520good%2520performance%250Acompared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17520v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Medical%20Image%20Segmentation%20with%20Mini-Net%3A%20A%20Lightweight%0A%20%20Solution%20Tailored%20for%20Efficient%20Segmentation%20of%20Medical%20Images&entry.906535625=Syed%20Javed%20and%20Tariq%20M.%20Khan%20and%20Abdul%20Qayyum%20and%20Arcot%20Sowmya%20and%20Imran%20Razzak&entry.1292438233=%20%20Accurate%20segmentation%20of%20anatomical%20structures%20and%20abnormalities%20in%20medical%0Aimages%20is%20crucial%20for%20computer-aided%20diagnosis%20and%20analysis.%20While%20deep%0Alearning%20techniques%20excel%20at%20this%20task%2C%20their%20computational%20demands%20pose%0Achallenges.%20Additionally%2C%20some%20cutting-edge%20segmentation%20methods%2C%20though%0Aeffective%20for%20general%20object%20segmentation%2C%20may%20not%20be%20optimised%20for%20medical%0Aimages.%20To%20address%20these%20issues%2C%20we%20propose%20Mini-Net%2C%20a%20lightweight%0Asegmentation%20network%20specifically%20designed%20for%20medical%20images.%20With%20fewer%20than%0A38%2C000%20parameters%2C%20Mini-Net%20efficiently%20captures%20both%20high-%20and%20low-frequency%0Afeatures%2C%20enabling%20real-time%20applications%20in%20various%20medical%20imaging%20scenarios.%0AWe%20evaluate%20Mini-Net%20on%20various%20datasets%2C%20including%20DRIVE%2C%20STARE%2C%20ISIC-2016%2C%0AISIC-2018%2C%20and%20MoNuSeg%2C%20demonstrating%20its%20robustness%20and%20good%20performance%0Acompared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17520v3&entry.124074799=Read"},
{"title": "Feasibility Study on Active Learning of Smart Surrogates for Scientific\n  Simulations", "author": "Pradeep Bajracharya and Javier Quetzalc\u00f3atl Toledo-Mar\u00edn and Geoffrey Fox and Shantenu Jha and Linwei Wang", "abstract": "  High-performance scientific simulations, important for comprehension of\ncomplex systems, encounter computational challenges especially when exploring\nextensive parameter spaces. There has been an increasing interest in developing\ndeep neural networks (DNNs) as surrogate models capable of accelerating the\nsimulations. However, existing approaches for training these DNN surrogates\nrely on extensive simulation data which are heuristically selected and\ngenerated with expensive computation -- a challenge under-explored in the\nliterature. In this paper, we investigate the potential of incorporating active\nlearning into DNN surrogate training. This allows intelligent and objective\nselection of training simulations, reducing the need to generate extensive\nsimulation data as well as the dependency of the performance of DNN surrogates\non pre-defined training simulations. In the problem context of constructing DNN\nsurrogates for diffusion equations with sources, we examine the efficacy of\ndiversity- and uncertainty-based strategies for selecting training simulations,\nconsidering two different DNN architecture. The results set the groundwork for\ndeveloping the high-performance computing infrastructure for Smart Surrogates\nthat supports on-the-fly generation of simulation data steered by active\nlearning strategies to potentially improve the efficiency of scientific\nsimulations.\n", "link": "http://arxiv.org/abs/2407.07674v2", "date": "2024-07-12", "relevancy": 1.0046, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5181}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feasibility%20Study%20on%20Active%20Learning%20of%20Smart%20Surrogates%20for%20Scientific%0A%20%20Simulations&body=Title%3A%20Feasibility%20Study%20on%20Active%20Learning%20of%20Smart%20Surrogates%20for%20Scientific%0A%20%20Simulations%0AAuthor%3A%20Pradeep%20Bajracharya%20and%20Javier%20Quetzalc%C3%B3atl%20Toledo-Mar%C3%ADn%20and%20Geoffrey%20Fox%20and%20Shantenu%20Jha%20and%20Linwei%20Wang%0AAbstract%3A%20%20%20High-performance%20scientific%20simulations%2C%20important%20for%20comprehension%20of%0Acomplex%20systems%2C%20encounter%20computational%20challenges%20especially%20when%20exploring%0Aextensive%20parameter%20spaces.%20There%20has%20been%20an%20increasing%20interest%20in%20developing%0Adeep%20neural%20networks%20%28DNNs%29%20as%20surrogate%20models%20capable%20of%20accelerating%20the%0Asimulations.%20However%2C%20existing%20approaches%20for%20training%20these%20DNN%20surrogates%0Arely%20on%20extensive%20simulation%20data%20which%20are%20heuristically%20selected%20and%0Agenerated%20with%20expensive%20computation%20--%20a%20challenge%20under-explored%20in%20the%0Aliterature.%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20incorporating%20active%0Alearning%20into%20DNN%20surrogate%20training.%20This%20allows%20intelligent%20and%20objective%0Aselection%20of%20training%20simulations%2C%20reducing%20the%20need%20to%20generate%20extensive%0Asimulation%20data%20as%20well%20as%20the%20dependency%20of%20the%20performance%20of%20DNN%20surrogates%0Aon%20pre-defined%20training%20simulations.%20In%20the%20problem%20context%20of%20constructing%20DNN%0Asurrogates%20for%20diffusion%20equations%20with%20sources%2C%20we%20examine%20the%20efficacy%20of%0Adiversity-%20and%20uncertainty-based%20strategies%20for%20selecting%20training%20simulations%2C%0Aconsidering%20two%20different%20DNN%20architecture.%20The%20results%20set%20the%20groundwork%20for%0Adeveloping%20the%20high-performance%20computing%20infrastructure%20for%20Smart%20Surrogates%0Athat%20supports%20on-the-fly%20generation%20of%20simulation%20data%20steered%20by%20active%0Alearning%20strategies%20to%20potentially%20improve%20the%20efficiency%20of%20scientific%0Asimulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07674v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeasibility%2520Study%2520on%2520Active%2520Learning%2520of%2520Smart%2520Surrogates%2520for%2520Scientific%250A%2520%2520Simulations%26entry.906535625%3DPradeep%2520Bajracharya%2520and%2520Javier%2520Quetzalc%25C3%25B3atl%2520Toledo-Mar%25C3%25ADn%2520and%2520Geoffrey%2520Fox%2520and%2520Shantenu%2520Jha%2520and%2520Linwei%2520Wang%26entry.1292438233%3D%2520%2520High-performance%2520scientific%2520simulations%252C%2520important%2520for%2520comprehension%2520of%250Acomplex%2520systems%252C%2520encounter%2520computational%2520challenges%2520especially%2520when%2520exploring%250Aextensive%2520parameter%2520spaces.%2520There%2520has%2520been%2520an%2520increasing%2520interest%2520in%2520developing%250Adeep%2520neural%2520networks%2520%2528DNNs%2529%2520as%2520surrogate%2520models%2520capable%2520of%2520accelerating%2520the%250Asimulations.%2520However%252C%2520existing%2520approaches%2520for%2520training%2520these%2520DNN%2520surrogates%250Arely%2520on%2520extensive%2520simulation%2520data%2520which%2520are%2520heuristically%2520selected%2520and%250Agenerated%2520with%2520expensive%2520computation%2520--%2520a%2520challenge%2520under-explored%2520in%2520the%250Aliterature.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520potential%2520of%2520incorporating%2520active%250Alearning%2520into%2520DNN%2520surrogate%2520training.%2520This%2520allows%2520intelligent%2520and%2520objective%250Aselection%2520of%2520training%2520simulations%252C%2520reducing%2520the%2520need%2520to%2520generate%2520extensive%250Asimulation%2520data%2520as%2520well%2520as%2520the%2520dependency%2520of%2520the%2520performance%2520of%2520DNN%2520surrogates%250Aon%2520pre-defined%2520training%2520simulations.%2520In%2520the%2520problem%2520context%2520of%2520constructing%2520DNN%250Asurrogates%2520for%2520diffusion%2520equations%2520with%2520sources%252C%2520we%2520examine%2520the%2520efficacy%2520of%250Adiversity-%2520and%2520uncertainty-based%2520strategies%2520for%2520selecting%2520training%2520simulations%252C%250Aconsidering%2520two%2520different%2520DNN%2520architecture.%2520The%2520results%2520set%2520the%2520groundwork%2520for%250Adeveloping%2520the%2520high-performance%2520computing%2520infrastructure%2520for%2520Smart%2520Surrogates%250Athat%2520supports%2520on-the-fly%2520generation%2520of%2520simulation%2520data%2520steered%2520by%2520active%250Alearning%2520strategies%2520to%2520potentially%2520improve%2520the%2520efficiency%2520of%2520scientific%250Asimulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07674v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feasibility%20Study%20on%20Active%20Learning%20of%20Smart%20Surrogates%20for%20Scientific%0A%20%20Simulations&entry.906535625=Pradeep%20Bajracharya%20and%20Javier%20Quetzalc%C3%B3atl%20Toledo-Mar%C3%ADn%20and%20Geoffrey%20Fox%20and%20Shantenu%20Jha%20and%20Linwei%20Wang&entry.1292438233=%20%20High-performance%20scientific%20simulations%2C%20important%20for%20comprehension%20of%0Acomplex%20systems%2C%20encounter%20computational%20challenges%20especially%20when%20exploring%0Aextensive%20parameter%20spaces.%20There%20has%20been%20an%20increasing%20interest%20in%20developing%0Adeep%20neural%20networks%20%28DNNs%29%20as%20surrogate%20models%20capable%20of%20accelerating%20the%0Asimulations.%20However%2C%20existing%20approaches%20for%20training%20these%20DNN%20surrogates%0Arely%20on%20extensive%20simulation%20data%20which%20are%20heuristically%20selected%20and%0Agenerated%20with%20expensive%20computation%20--%20a%20challenge%20under-explored%20in%20the%0Aliterature.%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20incorporating%20active%0Alearning%20into%20DNN%20surrogate%20training.%20This%20allows%20intelligent%20and%20objective%0Aselection%20of%20training%20simulations%2C%20reducing%20the%20need%20to%20generate%20extensive%0Asimulation%20data%20as%20well%20as%20the%20dependency%20of%20the%20performance%20of%20DNN%20surrogates%0Aon%20pre-defined%20training%20simulations.%20In%20the%20problem%20context%20of%20constructing%20DNN%0Asurrogates%20for%20diffusion%20equations%20with%20sources%2C%20we%20examine%20the%20efficacy%20of%0Adiversity-%20and%20uncertainty-based%20strategies%20for%20selecting%20training%20simulations%2C%0Aconsidering%20two%20different%20DNN%20architecture.%20The%20results%20set%20the%20groundwork%20for%0Adeveloping%20the%20high-performance%20computing%20infrastructure%20for%20Smart%20Surrogates%0Athat%20supports%20on-the-fly%20generation%20of%20simulation%20data%20steered%20by%20active%0Alearning%20strategies%20to%20potentially%20improve%20the%20efficiency%20of%20scientific%0Asimulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07674v2&entry.124074799=Read"},
{"title": "Logical Characterizations of Recurrent Graph Neural Networks with Reals\n  and Floats", "author": "Veeti Ahvonen and Damian Heiman and Antti Kuusisto and Carsten Lutz", "abstract": "  In pioneering work from 2019, Barcel\\'o and coauthors identified logics that\nprecisely match the expressive power of constant iteration-depth graph neural\nnetworks (GNNs) relative to properties definable in first-order logic. In this\narticle, we give exact logical characterizations of recurrent GNNs in two\nscenarios: (1) in the setting with floating-point numbers and (2) with reals.\nFor floats, the formalism matching recurrent GNNs is a rule-based modal logic\nwith counting, while for reals we use a suitable infinitary modal logic, also\nwith counting. These results give exact matches between logics and GNNs in the\nrecurrent setting without relativising to a background logic in either case,\nbut using some natural assumptions about floating-point arithmetic. Applying\nour characterizations, we also prove that, relative to graph properties\ndefinable in monadic second-order logic (MSO), our infinitary and rule-based\nlogics are equally expressive. This implies that recurrent GNNs with reals and\nfloats have the same expressive power over MSO-definable properties and shows\nthat, for such properties, also recurrent GNNs with reals are characterized by\na (finitary!) rule-based modal logic. In the general case, in contrast, the\nexpressive power with floats is weaker than with reals. In addition to\nlogic-oriented results, we also characterize recurrent GNNs, with both reals\nand floats, via distributed automata, drawing links to distributed computing\nmodels.\n", "link": "http://arxiv.org/abs/2405.14606v2", "date": "2024-07-12", "relevancy": 1.2805, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4603}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4178}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logical%20Characterizations%20of%20Recurrent%20Graph%20Neural%20Networks%20with%20Reals%0A%20%20and%20Floats&body=Title%3A%20Logical%20Characterizations%20of%20Recurrent%20Graph%20Neural%20Networks%20with%20Reals%0A%20%20and%20Floats%0AAuthor%3A%20Veeti%20Ahvonen%20and%20Damian%20Heiman%20and%20Antti%20Kuusisto%20and%20Carsten%20Lutz%0AAbstract%3A%20%20%20In%20pioneering%20work%20from%202019%2C%20Barcel%5C%27o%20and%20coauthors%20identified%20logics%20that%0Aprecisely%20match%20the%20expressive%20power%20of%20constant%20iteration-depth%20graph%20neural%0Anetworks%20%28GNNs%29%20relative%20to%20properties%20definable%20in%20first-order%20logic.%20In%20this%0Aarticle%2C%20we%20give%20exact%20logical%20characterizations%20of%20recurrent%20GNNs%20in%20two%0Ascenarios%3A%20%281%29%20in%20the%20setting%20with%20floating-point%20numbers%20and%20%282%29%20with%20reals.%0AFor%20floats%2C%20the%20formalism%20matching%20recurrent%20GNNs%20is%20a%20rule-based%20modal%20logic%0Awith%20counting%2C%20while%20for%20reals%20we%20use%20a%20suitable%20infinitary%20modal%20logic%2C%20also%0Awith%20counting.%20These%20results%20give%20exact%20matches%20between%20logics%20and%20GNNs%20in%20the%0Arecurrent%20setting%20without%20relativising%20to%20a%20background%20logic%20in%20either%20case%2C%0Abut%20using%20some%20natural%20assumptions%20about%20floating-point%20arithmetic.%20Applying%0Aour%20characterizations%2C%20we%20also%20prove%20that%2C%20relative%20to%20graph%20properties%0Adefinable%20in%20monadic%20second-order%20logic%20%28MSO%29%2C%20our%20infinitary%20and%20rule-based%0Alogics%20are%20equally%20expressive.%20This%20implies%20that%20recurrent%20GNNs%20with%20reals%20and%0Afloats%20have%20the%20same%20expressive%20power%20over%20MSO-definable%20properties%20and%20shows%0Athat%2C%20for%20such%20properties%2C%20also%20recurrent%20GNNs%20with%20reals%20are%20characterized%20by%0Aa%20%28finitary%21%29%20rule-based%20modal%20logic.%20In%20the%20general%20case%2C%20in%20contrast%2C%20the%0Aexpressive%20power%20with%20floats%20is%20weaker%20than%20with%20reals.%20In%20addition%20to%0Alogic-oriented%20results%2C%20we%20also%20characterize%20recurrent%20GNNs%2C%20with%20both%20reals%0Aand%20floats%2C%20via%20distributed%20automata%2C%20drawing%20links%20to%20distributed%20computing%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogical%2520Characterizations%2520of%2520Recurrent%2520Graph%2520Neural%2520Networks%2520with%2520Reals%250A%2520%2520and%2520Floats%26entry.906535625%3DVeeti%2520Ahvonen%2520and%2520Damian%2520Heiman%2520and%2520Antti%2520Kuusisto%2520and%2520Carsten%2520Lutz%26entry.1292438233%3D%2520%2520In%2520pioneering%2520work%2520from%25202019%252C%2520Barcel%255C%2527o%2520and%2520coauthors%2520identified%2520logics%2520that%250Aprecisely%2520match%2520the%2520expressive%2520power%2520of%2520constant%2520iteration-depth%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%2520relative%2520to%2520properties%2520definable%2520in%2520first-order%2520logic.%2520In%2520this%250Aarticle%252C%2520we%2520give%2520exact%2520logical%2520characterizations%2520of%2520recurrent%2520GNNs%2520in%2520two%250Ascenarios%253A%2520%25281%2529%2520in%2520the%2520setting%2520with%2520floating-point%2520numbers%2520and%2520%25282%2529%2520with%2520reals.%250AFor%2520floats%252C%2520the%2520formalism%2520matching%2520recurrent%2520GNNs%2520is%2520a%2520rule-based%2520modal%2520logic%250Awith%2520counting%252C%2520while%2520for%2520reals%2520we%2520use%2520a%2520suitable%2520infinitary%2520modal%2520logic%252C%2520also%250Awith%2520counting.%2520These%2520results%2520give%2520exact%2520matches%2520between%2520logics%2520and%2520GNNs%2520in%2520the%250Arecurrent%2520setting%2520without%2520relativising%2520to%2520a%2520background%2520logic%2520in%2520either%2520case%252C%250Abut%2520using%2520some%2520natural%2520assumptions%2520about%2520floating-point%2520arithmetic.%2520Applying%250Aour%2520characterizations%252C%2520we%2520also%2520prove%2520that%252C%2520relative%2520to%2520graph%2520properties%250Adefinable%2520in%2520monadic%2520second-order%2520logic%2520%2528MSO%2529%252C%2520our%2520infinitary%2520and%2520rule-based%250Alogics%2520are%2520equally%2520expressive.%2520This%2520implies%2520that%2520recurrent%2520GNNs%2520with%2520reals%2520and%250Afloats%2520have%2520the%2520same%2520expressive%2520power%2520over%2520MSO-definable%2520properties%2520and%2520shows%250Athat%252C%2520for%2520such%2520properties%252C%2520also%2520recurrent%2520GNNs%2520with%2520reals%2520are%2520characterized%2520by%250Aa%2520%2528finitary%2521%2529%2520rule-based%2520modal%2520logic.%2520In%2520the%2520general%2520case%252C%2520in%2520contrast%252C%2520the%250Aexpressive%2520power%2520with%2520floats%2520is%2520weaker%2520than%2520with%2520reals.%2520In%2520addition%2520to%250Alogic-oriented%2520results%252C%2520we%2520also%2520characterize%2520recurrent%2520GNNs%252C%2520with%2520both%2520reals%250Aand%2520floats%252C%2520via%2520distributed%2520automata%252C%2520drawing%2520links%2520to%2520distributed%2520computing%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logical%20Characterizations%20of%20Recurrent%20Graph%20Neural%20Networks%20with%20Reals%0A%20%20and%20Floats&entry.906535625=Veeti%20Ahvonen%20and%20Damian%20Heiman%20and%20Antti%20Kuusisto%20and%20Carsten%20Lutz&entry.1292438233=%20%20In%20pioneering%20work%20from%202019%2C%20Barcel%5C%27o%20and%20coauthors%20identified%20logics%20that%0Aprecisely%20match%20the%20expressive%20power%20of%20constant%20iteration-depth%20graph%20neural%0Anetworks%20%28GNNs%29%20relative%20to%20properties%20definable%20in%20first-order%20logic.%20In%20this%0Aarticle%2C%20we%20give%20exact%20logical%20characterizations%20of%20recurrent%20GNNs%20in%20two%0Ascenarios%3A%20%281%29%20in%20the%20setting%20with%20floating-point%20numbers%20and%20%282%29%20with%20reals.%0AFor%20floats%2C%20the%20formalism%20matching%20recurrent%20GNNs%20is%20a%20rule-based%20modal%20logic%0Awith%20counting%2C%20while%20for%20reals%20we%20use%20a%20suitable%20infinitary%20modal%20logic%2C%20also%0Awith%20counting.%20These%20results%20give%20exact%20matches%20between%20logics%20and%20GNNs%20in%20the%0Arecurrent%20setting%20without%20relativising%20to%20a%20background%20logic%20in%20either%20case%2C%0Abut%20using%20some%20natural%20assumptions%20about%20floating-point%20arithmetic.%20Applying%0Aour%20characterizations%2C%20we%20also%20prove%20that%2C%20relative%20to%20graph%20properties%0Adefinable%20in%20monadic%20second-order%20logic%20%28MSO%29%2C%20our%20infinitary%20and%20rule-based%0Alogics%20are%20equally%20expressive.%20This%20implies%20that%20recurrent%20GNNs%20with%20reals%20and%0Afloats%20have%20the%20same%20expressive%20power%20over%20MSO-definable%20properties%20and%20shows%0Athat%2C%20for%20such%20properties%2C%20also%20recurrent%20GNNs%20with%20reals%20are%20characterized%20by%0Aa%20%28finitary%21%29%20rule-based%20modal%20logic.%20In%20the%20general%20case%2C%20in%20contrast%2C%20the%0Aexpressive%20power%20with%20floats%20is%20weaker%20than%20with%20reals.%20In%20addition%20to%0Alogic-oriented%20results%2C%20we%20also%20characterize%20recurrent%20GNNs%2C%20with%20both%20reals%0Aand%20floats%2C%20via%20distributed%20automata%2C%20drawing%20links%20to%20distributed%20computing%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14606v2&entry.124074799=Read"},
{"title": "LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge\n  Retrieval-Augmented Diffusion", "author": "Pancheng Zhao and Peng Xu and Pengda Qin and Deng-Ping Fan and Zhicheng Zhang and Guoli Jia and Bowen Zhou and Jufeng Yang", "abstract": "  Camouflaged vision perception is an important vision task with numerous\npractical applications. Due to the expensive collection and labeling costs,\nthis community struggles with a major bottleneck that the species category of\nits datasets is limited to a small number of object species. However, the\nexisting camouflaged generation methods require specifying the background\nmanually, thus failing to extend the camouflaged sample diversity in a low-cost\nmanner. In this paper, we propose a Latent Background Knowledge\nRetrieval-Augmented Diffusion (LAKE-RED) for camouflaged image generation. To\nour knowledge, our contributions mainly include: (1) For the first time, we\npropose a camouflaged generation paradigm that does not need to receive any\nbackground inputs. (2) Our LAKE-RED is the first knowledge retrieval-augmented\nmethod with interpretability for camouflaged generation, in which we propose an\nidea that knowledge retrieval and reasoning enhancement are separated\nexplicitly, to alleviate the task-specific challenges. Moreover, our method is\nnot restricted to specific foreground targets or backgrounds, offering a\npotential for extending camouflaged vision perception to more diverse domains.\n(3) Experimental results demonstrate that our method outperforms the existing\napproaches, generating more realistic camouflage images.\n", "link": "http://arxiv.org/abs/2404.00292v4", "date": "2024-07-12", "relevancy": 1.707, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5765}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5676}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAKE-RED%3A%20Camouflaged%20Images%20Generation%20by%20Latent%20Background%20Knowledge%0A%20%20Retrieval-Augmented%20Diffusion&body=Title%3A%20LAKE-RED%3A%20Camouflaged%20Images%20Generation%20by%20Latent%20Background%20Knowledge%0A%20%20Retrieval-Augmented%20Diffusion%0AAuthor%3A%20Pancheng%20Zhao%20and%20Peng%20Xu%20and%20Pengda%20Qin%20and%20Deng-Ping%20Fan%20and%20Zhicheng%20Zhang%20and%20Guoli%20Jia%20and%20Bowen%20Zhou%20and%20Jufeng%20Yang%0AAbstract%3A%20%20%20Camouflaged%20vision%20perception%20is%20an%20important%20vision%20task%20with%20numerous%0Apractical%20applications.%20Due%20to%20the%20expensive%20collection%20and%20labeling%20costs%2C%0Athis%20community%20struggles%20with%20a%20major%20bottleneck%20that%20the%20species%20category%20of%0Aits%20datasets%20is%20limited%20to%20a%20small%20number%20of%20object%20species.%20However%2C%20the%0Aexisting%20camouflaged%20generation%20methods%20require%20specifying%20the%20background%0Amanually%2C%20thus%20failing%20to%20extend%20the%20camouflaged%20sample%20diversity%20in%20a%20low-cost%0Amanner.%20In%20this%20paper%2C%20we%20propose%20a%20Latent%20Background%20Knowledge%0ARetrieval-Augmented%20Diffusion%20%28LAKE-RED%29%20for%20camouflaged%20image%20generation.%20To%0Aour%20knowledge%2C%20our%20contributions%20mainly%20include%3A%20%281%29%20For%20the%20first%20time%2C%20we%0Apropose%20a%20camouflaged%20generation%20paradigm%20that%20does%20not%20need%20to%20receive%20any%0Abackground%20inputs.%20%282%29%20Our%20LAKE-RED%20is%20the%20first%20knowledge%20retrieval-augmented%0Amethod%20with%20interpretability%20for%20camouflaged%20generation%2C%20in%20which%20we%20propose%20an%0Aidea%20that%20knowledge%20retrieval%20and%20reasoning%20enhancement%20are%20separated%0Aexplicitly%2C%20to%20alleviate%20the%20task-specific%20challenges.%20Moreover%2C%20our%20method%20is%0Anot%20restricted%20to%20specific%20foreground%20targets%20or%20backgrounds%2C%20offering%20a%0Apotential%20for%20extending%20camouflaged%20vision%20perception%20to%20more%20diverse%20domains.%0A%283%29%20Experimental%20results%20demonstrate%20that%20our%20method%20outperforms%20the%20existing%0Aapproaches%2C%20generating%20more%20realistic%20camouflage%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00292v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAKE-RED%253A%2520Camouflaged%2520Images%2520Generation%2520by%2520Latent%2520Background%2520Knowledge%250A%2520%2520Retrieval-Augmented%2520Diffusion%26entry.906535625%3DPancheng%2520Zhao%2520and%2520Peng%2520Xu%2520and%2520Pengda%2520Qin%2520and%2520Deng-Ping%2520Fan%2520and%2520Zhicheng%2520Zhang%2520and%2520Guoli%2520Jia%2520and%2520Bowen%2520Zhou%2520and%2520Jufeng%2520Yang%26entry.1292438233%3D%2520%2520Camouflaged%2520vision%2520perception%2520is%2520an%2520important%2520vision%2520task%2520with%2520numerous%250Apractical%2520applications.%2520Due%2520to%2520the%2520expensive%2520collection%2520and%2520labeling%2520costs%252C%250Athis%2520community%2520struggles%2520with%2520a%2520major%2520bottleneck%2520that%2520the%2520species%2520category%2520of%250Aits%2520datasets%2520is%2520limited%2520to%2520a%2520small%2520number%2520of%2520object%2520species.%2520However%252C%2520the%250Aexisting%2520camouflaged%2520generation%2520methods%2520require%2520specifying%2520the%2520background%250Amanually%252C%2520thus%2520failing%2520to%2520extend%2520the%2520camouflaged%2520sample%2520diversity%2520in%2520a%2520low-cost%250Amanner.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Latent%2520Background%2520Knowledge%250ARetrieval-Augmented%2520Diffusion%2520%2528LAKE-RED%2529%2520for%2520camouflaged%2520image%2520generation.%2520To%250Aour%2520knowledge%252C%2520our%2520contributions%2520mainly%2520include%253A%2520%25281%2529%2520For%2520the%2520first%2520time%252C%2520we%250Apropose%2520a%2520camouflaged%2520generation%2520paradigm%2520that%2520does%2520not%2520need%2520to%2520receive%2520any%250Abackground%2520inputs.%2520%25282%2529%2520Our%2520LAKE-RED%2520is%2520the%2520first%2520knowledge%2520retrieval-augmented%250Amethod%2520with%2520interpretability%2520for%2520camouflaged%2520generation%252C%2520in%2520which%2520we%2520propose%2520an%250Aidea%2520that%2520knowledge%2520retrieval%2520and%2520reasoning%2520enhancement%2520are%2520separated%250Aexplicitly%252C%2520to%2520alleviate%2520the%2520task-specific%2520challenges.%2520Moreover%252C%2520our%2520method%2520is%250Anot%2520restricted%2520to%2520specific%2520foreground%2520targets%2520or%2520backgrounds%252C%2520offering%2520a%250Apotential%2520for%2520extending%2520camouflaged%2520vision%2520perception%2520to%2520more%2520diverse%2520domains.%250A%25283%2529%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520the%2520existing%250Aapproaches%252C%2520generating%2520more%2520realistic%2520camouflage%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00292v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAKE-RED%3A%20Camouflaged%20Images%20Generation%20by%20Latent%20Background%20Knowledge%0A%20%20Retrieval-Augmented%20Diffusion&entry.906535625=Pancheng%20Zhao%20and%20Peng%20Xu%20and%20Pengda%20Qin%20and%20Deng-Ping%20Fan%20and%20Zhicheng%20Zhang%20and%20Guoli%20Jia%20and%20Bowen%20Zhou%20and%20Jufeng%20Yang&entry.1292438233=%20%20Camouflaged%20vision%20perception%20is%20an%20important%20vision%20task%20with%20numerous%0Apractical%20applications.%20Due%20to%20the%20expensive%20collection%20and%20labeling%20costs%2C%0Athis%20community%20struggles%20with%20a%20major%20bottleneck%20that%20the%20species%20category%20of%0Aits%20datasets%20is%20limited%20to%20a%20small%20number%20of%20object%20species.%20However%2C%20the%0Aexisting%20camouflaged%20generation%20methods%20require%20specifying%20the%20background%0Amanually%2C%20thus%20failing%20to%20extend%20the%20camouflaged%20sample%20diversity%20in%20a%20low-cost%0Amanner.%20In%20this%20paper%2C%20we%20propose%20a%20Latent%20Background%20Knowledge%0ARetrieval-Augmented%20Diffusion%20%28LAKE-RED%29%20for%20camouflaged%20image%20generation.%20To%0Aour%20knowledge%2C%20our%20contributions%20mainly%20include%3A%20%281%29%20For%20the%20first%20time%2C%20we%0Apropose%20a%20camouflaged%20generation%20paradigm%20that%20does%20not%20need%20to%20receive%20any%0Abackground%20inputs.%20%282%29%20Our%20LAKE-RED%20is%20the%20first%20knowledge%20retrieval-augmented%0Amethod%20with%20interpretability%20for%20camouflaged%20generation%2C%20in%20which%20we%20propose%20an%0Aidea%20that%20knowledge%20retrieval%20and%20reasoning%20enhancement%20are%20separated%0Aexplicitly%2C%20to%20alleviate%20the%20task-specific%20challenges.%20Moreover%2C%20our%20method%20is%0Anot%20restricted%20to%20specific%20foreground%20targets%20or%20backgrounds%2C%20offering%20a%0Apotential%20for%20extending%20camouflaged%20vision%20perception%20to%20more%20diverse%20domains.%0A%283%29%20Experimental%20results%20demonstrate%20that%20our%20method%20outperforms%20the%20existing%0Aapproaches%2C%20generating%20more%20realistic%20camouflage%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00292v4&entry.124074799=Read"},
{"title": "MS-TCRNet: Multi-Stage Temporal Convolutional Recurrent Networks for\n  Action Segmentation Using Sensor-Augmented Kinematics", "author": "Adam Goldbraikh and Omer Shubi and Or Rubin and Carla M Pugh and Shlomi Laufer", "abstract": "  Action segmentation is a challenging task in high-level process analysis,\ntypically performed on video or kinematic data obtained from various sensors.\nThis work presents two contributions related to action segmentation on\nkinematic data. Firstly, we introduce two versions of Multi-Stage Temporal\nConvolutional Recurrent Networks (MS-TCRNet), specifically designed for\nkinematic data. The architectures consist of a prediction generator with\nintra-stage regularization and Bidirectional LSTM or GRU-based refinement\nstages. Secondly, we propose two new data augmentation techniques, World Frame\nRotation and Hand Inversion, which utilize the strong geometric structure of\nkinematic data to improve algorithm performance and robustness. We evaluate our\nmodels on three datasets of surgical suturing tasks: the Variable Tissue\nSimulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)\nDataset, both of which are open surgery simulation datasets collected by us, as\nwell as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a\nwell-known benchmark in robotic surgery. Our methods achieved state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2303.07814v2", "date": "2024-07-12", "relevancy": 1.7025, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5832}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5656}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-TCRNet%3A%20Multi-Stage%20Temporal%20Convolutional%20Recurrent%20Networks%20for%0A%20%20Action%20Segmentation%20Using%20Sensor-Augmented%20Kinematics&body=Title%3A%20MS-TCRNet%3A%20Multi-Stage%20Temporal%20Convolutional%20Recurrent%20Networks%20for%0A%20%20Action%20Segmentation%20Using%20Sensor-Augmented%20Kinematics%0AAuthor%3A%20Adam%20Goldbraikh%20and%20Omer%20Shubi%20and%20Or%20Rubin%20and%20Carla%20M%20Pugh%20and%20Shlomi%20Laufer%0AAbstract%3A%20%20%20Action%20segmentation%20is%20a%20challenging%20task%20in%20high-level%20process%20analysis%2C%0Atypically%20performed%20on%20video%20or%20kinematic%20data%20obtained%20from%20various%20sensors.%0AThis%20work%20presents%20two%20contributions%20related%20to%20action%20segmentation%20on%0Akinematic%20data.%20Firstly%2C%20we%20introduce%20two%20versions%20of%20Multi-Stage%20Temporal%0AConvolutional%20Recurrent%20Networks%20%28MS-TCRNet%29%2C%20specifically%20designed%20for%0Akinematic%20data.%20The%20architectures%20consist%20of%20a%20prediction%20generator%20with%0Aintra-stage%20regularization%20and%20Bidirectional%20LSTM%20or%20GRU-based%20refinement%0Astages.%20Secondly%2C%20we%20propose%20two%20new%20data%20augmentation%20techniques%2C%20World%20Frame%0ARotation%20and%20Hand%20Inversion%2C%20which%20utilize%20the%20strong%20geometric%20structure%20of%0Akinematic%20data%20to%20improve%20algorithm%20performance%20and%20robustness.%20We%20evaluate%20our%0Amodels%20on%20three%20datasets%20of%20surgical%20suturing%20tasks%3A%20the%20Variable%20Tissue%0ASimulation%20%28VTS%29%20Dataset%20and%20the%20newly%20introduced%20Bowel%20Repair%20Simulation%20%28BRS%29%0ADataset%2C%20both%20of%20which%20are%20open%20surgery%20simulation%20datasets%20collected%20by%20us%2C%20as%0Awell%20as%20the%20JHU-ISI%20Gesture%20and%20Skill%20Assessment%20Working%20Set%20%28JIGSAWS%29%2C%20a%0Awell-known%20benchmark%20in%20robotic%20surgery.%20Our%20methods%20achieved%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.07814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-TCRNet%253A%2520Multi-Stage%2520Temporal%2520Convolutional%2520Recurrent%2520Networks%2520for%250A%2520%2520Action%2520Segmentation%2520Using%2520Sensor-Augmented%2520Kinematics%26entry.906535625%3DAdam%2520Goldbraikh%2520and%2520Omer%2520Shubi%2520and%2520Or%2520Rubin%2520and%2520Carla%2520M%2520Pugh%2520and%2520Shlomi%2520Laufer%26entry.1292438233%3D%2520%2520Action%2520segmentation%2520is%2520a%2520challenging%2520task%2520in%2520high-level%2520process%2520analysis%252C%250Atypically%2520performed%2520on%2520video%2520or%2520kinematic%2520data%2520obtained%2520from%2520various%2520sensors.%250AThis%2520work%2520presents%2520two%2520contributions%2520related%2520to%2520action%2520segmentation%2520on%250Akinematic%2520data.%2520Firstly%252C%2520we%2520introduce%2520two%2520versions%2520of%2520Multi-Stage%2520Temporal%250AConvolutional%2520Recurrent%2520Networks%2520%2528MS-TCRNet%2529%252C%2520specifically%2520designed%2520for%250Akinematic%2520data.%2520The%2520architectures%2520consist%2520of%2520a%2520prediction%2520generator%2520with%250Aintra-stage%2520regularization%2520and%2520Bidirectional%2520LSTM%2520or%2520GRU-based%2520refinement%250Astages.%2520Secondly%252C%2520we%2520propose%2520two%2520new%2520data%2520augmentation%2520techniques%252C%2520World%2520Frame%250ARotation%2520and%2520Hand%2520Inversion%252C%2520which%2520utilize%2520the%2520strong%2520geometric%2520structure%2520of%250Akinematic%2520data%2520to%2520improve%2520algorithm%2520performance%2520and%2520robustness.%2520We%2520evaluate%2520our%250Amodels%2520on%2520three%2520datasets%2520of%2520surgical%2520suturing%2520tasks%253A%2520the%2520Variable%2520Tissue%250ASimulation%2520%2528VTS%2529%2520Dataset%2520and%2520the%2520newly%2520introduced%2520Bowel%2520Repair%2520Simulation%2520%2528BRS%2529%250ADataset%252C%2520both%2520of%2520which%2520are%2520open%2520surgery%2520simulation%2520datasets%2520collected%2520by%2520us%252C%2520as%250Awell%2520as%2520the%2520JHU-ISI%2520Gesture%2520and%2520Skill%2520Assessment%2520Working%2520Set%2520%2528JIGSAWS%2529%252C%2520a%250Awell-known%2520benchmark%2520in%2520robotic%2520surgery.%2520Our%2520methods%2520achieved%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.07814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-TCRNet%3A%20Multi-Stage%20Temporal%20Convolutional%20Recurrent%20Networks%20for%0A%20%20Action%20Segmentation%20Using%20Sensor-Augmented%20Kinematics&entry.906535625=Adam%20Goldbraikh%20and%20Omer%20Shubi%20and%20Or%20Rubin%20and%20Carla%20M%20Pugh%20and%20Shlomi%20Laufer&entry.1292438233=%20%20Action%20segmentation%20is%20a%20challenging%20task%20in%20high-level%20process%20analysis%2C%0Atypically%20performed%20on%20video%20or%20kinematic%20data%20obtained%20from%20various%20sensors.%0AThis%20work%20presents%20two%20contributions%20related%20to%20action%20segmentation%20on%0Akinematic%20data.%20Firstly%2C%20we%20introduce%20two%20versions%20of%20Multi-Stage%20Temporal%0AConvolutional%20Recurrent%20Networks%20%28MS-TCRNet%29%2C%20specifically%20designed%20for%0Akinematic%20data.%20The%20architectures%20consist%20of%20a%20prediction%20generator%20with%0Aintra-stage%20regularization%20and%20Bidirectional%20LSTM%20or%20GRU-based%20refinement%0Astages.%20Secondly%2C%20we%20propose%20two%20new%20data%20augmentation%20techniques%2C%20World%20Frame%0ARotation%20and%20Hand%20Inversion%2C%20which%20utilize%20the%20strong%20geometric%20structure%20of%0Akinematic%20data%20to%20improve%20algorithm%20performance%20and%20robustness.%20We%20evaluate%20our%0Amodels%20on%20three%20datasets%20of%20surgical%20suturing%20tasks%3A%20the%20Variable%20Tissue%0ASimulation%20%28VTS%29%20Dataset%20and%20the%20newly%20introduced%20Bowel%20Repair%20Simulation%20%28BRS%29%0ADataset%2C%20both%20of%20which%20are%20open%20surgery%20simulation%20datasets%20collected%20by%20us%2C%20as%0Awell%20as%20the%20JHU-ISI%20Gesture%20and%20Skill%20Assessment%20Working%20Set%20%28JIGSAWS%29%2C%20a%0Awell-known%20benchmark%20in%20robotic%20surgery.%20Our%20methods%20achieved%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.07814v2&entry.124074799=Read"},
{"title": "HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems\n  in Context", "author": "Federico Arangath Joseph and Kilian Haefeli and Noah Liniger and Caglar Gulcehre", "abstract": "  This work explores the in-context learning capabilities of State Space Models\n(SSMs) and presents, to the best of our knowledge, the first theoretical\nexplanation of a possible underlying mechanism. We introduce a novel weight\nconstruction for SSMs, enabling them to predict the next state of any dynamical\nsystem after observing previous states without parameter fine-tuning. This is\naccomplished by extending the HiPPO framework to demonstrate that continuous\nSSMs can approximate the derivative of any input signal. Specifically, we find\nan explicit weight construction for continuous SSMs and provide an asymptotic\nerror bound on the derivative approximation. The discretization of this\ncontinuous SSM subsequently yields a discrete SSM that predicts the next state.\nFinally, we demonstrate the effectiveness of our parameterization empirically.\nThis work should be an initial step toward understanding how sequence models\nbased on SSMs learn in context.\n", "link": "http://arxiv.org/abs/2407.09375v1", "date": "2024-07-12", "relevancy": 1.4566, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5605}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4725}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiPPO-Prophecy%3A%20State-Space%20Models%20can%20Provably%20Learn%20Dynamical%20Systems%0A%20%20in%20Context&body=Title%3A%20HiPPO-Prophecy%3A%20State-Space%20Models%20can%20Provably%20Learn%20Dynamical%20Systems%0A%20%20in%20Context%0AAuthor%3A%20Federico%20Arangath%20Joseph%20and%20Kilian%20Haefeli%20and%20Noah%20Liniger%20and%20Caglar%20Gulcehre%0AAbstract%3A%20%20%20This%20work%20explores%20the%20in-context%20learning%20capabilities%20of%20State%20Space%20Models%0A%28SSMs%29%20and%20presents%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20theoretical%0Aexplanation%20of%20a%20possible%20underlying%20mechanism.%20We%20introduce%20a%20novel%20weight%0Aconstruction%20for%20SSMs%2C%20enabling%20them%20to%20predict%20the%20next%20state%20of%20any%20dynamical%0Asystem%20after%20observing%20previous%20states%20without%20parameter%20fine-tuning.%20This%20is%0Aaccomplished%20by%20extending%20the%20HiPPO%20framework%20to%20demonstrate%20that%20continuous%0ASSMs%20can%20approximate%20the%20derivative%20of%20any%20input%20signal.%20Specifically%2C%20we%20find%0Aan%20explicit%20weight%20construction%20for%20continuous%20SSMs%20and%20provide%20an%20asymptotic%0Aerror%20bound%20on%20the%20derivative%20approximation.%20The%20discretization%20of%20this%0Acontinuous%20SSM%20subsequently%20yields%20a%20discrete%20SSM%20that%20predicts%20the%20next%20state.%0AFinally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20parameterization%20empirically.%0AThis%20work%20should%20be%20an%20initial%20step%20toward%20understanding%20how%20sequence%20models%0Abased%20on%20SSMs%20learn%20in%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiPPO-Prophecy%253A%2520State-Space%2520Models%2520can%2520Provably%2520Learn%2520Dynamical%2520Systems%250A%2520%2520in%2520Context%26entry.906535625%3DFederico%2520Arangath%2520Joseph%2520and%2520Kilian%2520Haefeli%2520and%2520Noah%2520Liniger%2520and%2520Caglar%2520Gulcehre%26entry.1292438233%3D%2520%2520This%2520work%2520explores%2520the%2520in-context%2520learning%2520capabilities%2520of%2520State%2520Space%2520Models%250A%2528SSMs%2529%2520and%2520presents%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%2520theoretical%250Aexplanation%2520of%2520a%2520possible%2520underlying%2520mechanism.%2520We%2520introduce%2520a%2520novel%2520weight%250Aconstruction%2520for%2520SSMs%252C%2520enabling%2520them%2520to%2520predict%2520the%2520next%2520state%2520of%2520any%2520dynamical%250Asystem%2520after%2520observing%2520previous%2520states%2520without%2520parameter%2520fine-tuning.%2520This%2520is%250Aaccomplished%2520by%2520extending%2520the%2520HiPPO%2520framework%2520to%2520demonstrate%2520that%2520continuous%250ASSMs%2520can%2520approximate%2520the%2520derivative%2520of%2520any%2520input%2520signal.%2520Specifically%252C%2520we%2520find%250Aan%2520explicit%2520weight%2520construction%2520for%2520continuous%2520SSMs%2520and%2520provide%2520an%2520asymptotic%250Aerror%2520bound%2520on%2520the%2520derivative%2520approximation.%2520The%2520discretization%2520of%2520this%250Acontinuous%2520SSM%2520subsequently%2520yields%2520a%2520discrete%2520SSM%2520that%2520predicts%2520the%2520next%2520state.%250AFinally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520parameterization%2520empirically.%250AThis%2520work%2520should%2520be%2520an%2520initial%2520step%2520toward%2520understanding%2520how%2520sequence%2520models%250Abased%2520on%2520SSMs%2520learn%2520in%2520context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiPPO-Prophecy%3A%20State-Space%20Models%20can%20Provably%20Learn%20Dynamical%20Systems%0A%20%20in%20Context&entry.906535625=Federico%20Arangath%20Joseph%20and%20Kilian%20Haefeli%20and%20Noah%20Liniger%20and%20Caglar%20Gulcehre&entry.1292438233=%20%20This%20work%20explores%20the%20in-context%20learning%20capabilities%20of%20State%20Space%20Models%0A%28SSMs%29%20and%20presents%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20theoretical%0Aexplanation%20of%20a%20possible%20underlying%20mechanism.%20We%20introduce%20a%20novel%20weight%0Aconstruction%20for%20SSMs%2C%20enabling%20them%20to%20predict%20the%20next%20state%20of%20any%20dynamical%0Asystem%20after%20observing%20previous%20states%20without%20parameter%20fine-tuning.%20This%20is%0Aaccomplished%20by%20extending%20the%20HiPPO%20framework%20to%20demonstrate%20that%20continuous%0ASSMs%20can%20approximate%20the%20derivative%20of%20any%20input%20signal.%20Specifically%2C%20we%20find%0Aan%20explicit%20weight%20construction%20for%20continuous%20SSMs%20and%20provide%20an%20asymptotic%0Aerror%20bound%20on%20the%20derivative%20approximation.%20The%20discretization%20of%20this%0Acontinuous%20SSM%20subsequently%20yields%20a%20discrete%20SSM%20that%20predicts%20the%20next%20state.%0AFinally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20parameterization%20empirically.%0AThis%20work%20should%20be%20an%20initial%20step%20toward%20understanding%20how%20sequence%20models%0Abased%20on%20SSMs%20learn%20in%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09375v1&entry.124074799=Read"},
{"title": "Chasing Convex Functions with Long-term Constraints", "author": "Adam Lechowicz and Nicolas Christianson and Bo Sun and Noman Bashir and Mohammad Hajiesmaili and Adam Wierman and Prashant Shenoy", "abstract": "  We introduce and study a family of online metric problems with long-term\nconstraints. In these problems, an online player makes decisions $\\mathbf{x}_t$\nin a metric space $(X,d)$ to simultaneously minimize their hitting cost\n$f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric. Over the\ntime horizon $T$, the player must satisfy a long-term demand constraint\n$\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction\nof demand satisfied at time $t$. Such problems can find a wide array of\napplications to online resource allocation in sustainable energy/computing\nsystems. We devise optimal competitive and learning-augmented algorithms for\nthe case of bounded hitting cost gradients and weighted $\\ell_1$ metrics, and\nfurther show that our proposed algorithms perform well in numerical\nexperiments.\n", "link": "http://arxiv.org/abs/2402.14012v2", "date": "2024-07-12", "relevancy": 1.2208, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4359}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4016}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chasing%20Convex%20Functions%20with%20Long-term%20Constraints&body=Title%3A%20Chasing%20Convex%20Functions%20with%20Long-term%20Constraints%0AAuthor%3A%20Adam%20Lechowicz%20and%20Nicolas%20Christianson%20and%20Bo%20Sun%20and%20Noman%20Bashir%20and%20Mohammad%20Hajiesmaili%20and%20Adam%20Wierman%20and%20Prashant%20Shenoy%0AAbstract%3A%20%20%20We%20introduce%20and%20study%20a%20family%20of%20online%20metric%20problems%20with%20long-term%0Aconstraints.%20In%20these%20problems%2C%20an%20online%20player%20makes%20decisions%20%24%5Cmathbf%7Bx%7D_t%24%0Ain%20a%20metric%20space%20%24%28X%2Cd%29%24%20to%20simultaneously%20minimize%20their%20hitting%20cost%0A%24f_t%28%5Cmathbf%7Bx%7D_t%29%24%20and%20switching%20cost%20as%20determined%20by%20the%20metric.%20Over%20the%0Atime%20horizon%20%24T%24%2C%20the%20player%20must%20satisfy%20a%20long-term%20demand%20constraint%0A%24%5Csum_%7Bt%7D%20c%28%5Cmathbf%7Bx%7D_t%29%20%5Cgeq%201%24%2C%20where%20%24c%28%5Cmathbf%7Bx%7D_t%29%24%20denotes%20the%20fraction%0Aof%20demand%20satisfied%20at%20time%20%24t%24.%20Such%20problems%20can%20find%20a%20wide%20array%20of%0Aapplications%20to%20online%20resource%20allocation%20in%20sustainable%20energy/computing%0Asystems.%20We%20devise%20optimal%20competitive%20and%20learning-augmented%20algorithms%20for%0Athe%20case%20of%20bounded%20hitting%20cost%20gradients%20and%20weighted%20%24%5Cell_1%24%20metrics%2C%20and%0Afurther%20show%20that%20our%20proposed%20algorithms%20perform%20well%20in%20numerical%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChasing%2520Convex%2520Functions%2520with%2520Long-term%2520Constraints%26entry.906535625%3DAdam%2520Lechowicz%2520and%2520Nicolas%2520Christianson%2520and%2520Bo%2520Sun%2520and%2520Noman%2520Bashir%2520and%2520Mohammad%2520Hajiesmaili%2520and%2520Adam%2520Wierman%2520and%2520Prashant%2520Shenoy%26entry.1292438233%3D%2520%2520We%2520introduce%2520and%2520study%2520a%2520family%2520of%2520online%2520metric%2520problems%2520with%2520long-term%250Aconstraints.%2520In%2520these%2520problems%252C%2520an%2520online%2520player%2520makes%2520decisions%2520%2524%255Cmathbf%257Bx%257D_t%2524%250Ain%2520a%2520metric%2520space%2520%2524%2528X%252Cd%2529%2524%2520to%2520simultaneously%2520minimize%2520their%2520hitting%2520cost%250A%2524f_t%2528%255Cmathbf%257Bx%257D_t%2529%2524%2520and%2520switching%2520cost%2520as%2520determined%2520by%2520the%2520metric.%2520Over%2520the%250Atime%2520horizon%2520%2524T%2524%252C%2520the%2520player%2520must%2520satisfy%2520a%2520long-term%2520demand%2520constraint%250A%2524%255Csum_%257Bt%257D%2520c%2528%255Cmathbf%257Bx%257D_t%2529%2520%255Cgeq%25201%2524%252C%2520where%2520%2524c%2528%255Cmathbf%257Bx%257D_t%2529%2524%2520denotes%2520the%2520fraction%250Aof%2520demand%2520satisfied%2520at%2520time%2520%2524t%2524.%2520Such%2520problems%2520can%2520find%2520a%2520wide%2520array%2520of%250Aapplications%2520to%2520online%2520resource%2520allocation%2520in%2520sustainable%2520energy/computing%250Asystems.%2520We%2520devise%2520optimal%2520competitive%2520and%2520learning-augmented%2520algorithms%2520for%250Athe%2520case%2520of%2520bounded%2520hitting%2520cost%2520gradients%2520and%2520weighted%2520%2524%255Cell_1%2524%2520metrics%252C%2520and%250Afurther%2520show%2520that%2520our%2520proposed%2520algorithms%2520perform%2520well%2520in%2520numerical%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chasing%20Convex%20Functions%20with%20Long-term%20Constraints&entry.906535625=Adam%20Lechowicz%20and%20Nicolas%20Christianson%20and%20Bo%20Sun%20and%20Noman%20Bashir%20and%20Mohammad%20Hajiesmaili%20and%20Adam%20Wierman%20and%20Prashant%20Shenoy&entry.1292438233=%20%20We%20introduce%20and%20study%20a%20family%20of%20online%20metric%20problems%20with%20long-term%0Aconstraints.%20In%20these%20problems%2C%20an%20online%20player%20makes%20decisions%20%24%5Cmathbf%7Bx%7D_t%24%0Ain%20a%20metric%20space%20%24%28X%2Cd%29%24%20to%20simultaneously%20minimize%20their%20hitting%20cost%0A%24f_t%28%5Cmathbf%7Bx%7D_t%29%24%20and%20switching%20cost%20as%20determined%20by%20the%20metric.%20Over%20the%0Atime%20horizon%20%24T%24%2C%20the%20player%20must%20satisfy%20a%20long-term%20demand%20constraint%0A%24%5Csum_%7Bt%7D%20c%28%5Cmathbf%7Bx%7D_t%29%20%5Cgeq%201%24%2C%20where%20%24c%28%5Cmathbf%7Bx%7D_t%29%24%20denotes%20the%20fraction%0Aof%20demand%20satisfied%20at%20time%20%24t%24.%20Such%20problems%20can%20find%20a%20wide%20array%20of%0Aapplications%20to%20online%20resource%20allocation%20in%20sustainable%20energy/computing%0Asystems.%20We%20devise%20optimal%20competitive%20and%20learning-augmented%20algorithms%20for%0Athe%20case%20of%20bounded%20hitting%20cost%20gradients%20and%20weighted%20%24%5Cell_1%24%20metrics%2C%20and%0Afurther%20show%20that%20our%20proposed%20algorithms%20perform%20well%20in%20numerical%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14012v2&entry.124074799=Read"},
{"title": "Efficient Adaptation in Mixed-Motive Environments via Hierarchical\n  Opponent Modeling and Planning", "author": "Yizhe Huang and Anji Liu and Fanqi Kong and Yaodong Yang and Song-Chun Zhu and Xue Feng", "abstract": "  Despite the recent successes of multi-agent reinforcement learning (MARL)\nalgorithms, efficiently adapting to co-players in mixed-motive environments\nremains a significant challenge. One feasible approach is to hierarchically\nmodel co-players' behavior based on inferring their characteristics. However,\nthese methods often encounter difficulties in efficient reasoning and\nutilization of inferred information. To address these issues, we propose\nHierarchical Opponent modeling and Planning (HOP), a novel multi-agent\ndecision-making algorithm that enables few-shot adaptation to unseen policies\nin mixed-motive environments. HOP is hierarchically composed of two modules: an\nopponent modeling module that infers others' goals and learns corresponding\ngoal-conditioned policies, and a planning module that employs Monte Carlo Tree\nSearch (MCTS) to identify the best response. Our approach improves efficiency\nby updating beliefs about others' goals both across and within episodes and by\nusing information from the opponent modeling module to guide planning.\nExperimental results demonstrate that in mixed-motive environments, HOP\nexhibits superior few-shot adaptation capabilities when interacting with\nvarious unseen agents, and excels in self-play scenarios. Furthermore, the\nemergence of social intelligence during our experiments underscores the\npotential of our approach in complex multi-agent environments.\n", "link": "http://arxiv.org/abs/2406.08002v2", "date": "2024-07-12", "relevancy": 1.6719, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5914}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.567}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Adaptation%20in%20Mixed-Motive%20Environments%20via%20Hierarchical%0A%20%20Opponent%20Modeling%20and%20Planning&body=Title%3A%20Efficient%20Adaptation%20in%20Mixed-Motive%20Environments%20via%20Hierarchical%0A%20%20Opponent%20Modeling%20and%20Planning%0AAuthor%3A%20Yizhe%20Huang%20and%20Anji%20Liu%20and%20Fanqi%20Kong%20and%20Yaodong%20Yang%20and%20Song-Chun%20Zhu%20and%20Xue%20Feng%0AAbstract%3A%20%20%20Despite%20the%20recent%20successes%20of%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Aalgorithms%2C%20efficiently%20adapting%20to%20co-players%20in%20mixed-motive%20environments%0Aremains%20a%20significant%20challenge.%20One%20feasible%20approach%20is%20to%20hierarchically%0Amodel%20co-players%27%20behavior%20based%20on%20inferring%20their%20characteristics.%20However%2C%0Athese%20methods%20often%20encounter%20difficulties%20in%20efficient%20reasoning%20and%0Autilization%20of%20inferred%20information.%20To%20address%20these%20issues%2C%20we%20propose%0AHierarchical%20Opponent%20modeling%20and%20Planning%20%28HOP%29%2C%20a%20novel%20multi-agent%0Adecision-making%20algorithm%20that%20enables%20few-shot%20adaptation%20to%20unseen%20policies%0Ain%20mixed-motive%20environments.%20HOP%20is%20hierarchically%20composed%20of%20two%20modules%3A%20an%0Aopponent%20modeling%20module%20that%20infers%20others%27%20goals%20and%20learns%20corresponding%0Agoal-conditioned%20policies%2C%20and%20a%20planning%20module%20that%20employs%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20to%20identify%20the%20best%20response.%20Our%20approach%20improves%20efficiency%0Aby%20updating%20beliefs%20about%20others%27%20goals%20both%20across%20and%20within%20episodes%20and%20by%0Ausing%20information%20from%20the%20opponent%20modeling%20module%20to%20guide%20planning.%0AExperimental%20results%20demonstrate%20that%20in%20mixed-motive%20environments%2C%20HOP%0Aexhibits%20superior%20few-shot%20adaptation%20capabilities%20when%20interacting%20with%0Avarious%20unseen%20agents%2C%20and%20excels%20in%20self-play%20scenarios.%20Furthermore%2C%20the%0Aemergence%20of%20social%20intelligence%20during%20our%20experiments%20underscores%20the%0Apotential%20of%20our%20approach%20in%20complex%20multi-agent%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Adaptation%2520in%2520Mixed-Motive%2520Environments%2520via%2520Hierarchical%250A%2520%2520Opponent%2520Modeling%2520and%2520Planning%26entry.906535625%3DYizhe%2520Huang%2520and%2520Anji%2520Liu%2520and%2520Fanqi%2520Kong%2520and%2520Yaodong%2520Yang%2520and%2520Song-Chun%2520Zhu%2520and%2520Xue%2520Feng%26entry.1292438233%3D%2520%2520Despite%2520the%2520recent%2520successes%2520of%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%250Aalgorithms%252C%2520efficiently%2520adapting%2520to%2520co-players%2520in%2520mixed-motive%2520environments%250Aremains%2520a%2520significant%2520challenge.%2520One%2520feasible%2520approach%2520is%2520to%2520hierarchically%250Amodel%2520co-players%2527%2520behavior%2520based%2520on%2520inferring%2520their%2520characteristics.%2520However%252C%250Athese%2520methods%2520often%2520encounter%2520difficulties%2520in%2520efficient%2520reasoning%2520and%250Autilization%2520of%2520inferred%2520information.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250AHierarchical%2520Opponent%2520modeling%2520and%2520Planning%2520%2528HOP%2529%252C%2520a%2520novel%2520multi-agent%250Adecision-making%2520algorithm%2520that%2520enables%2520few-shot%2520adaptation%2520to%2520unseen%2520policies%250Ain%2520mixed-motive%2520environments.%2520HOP%2520is%2520hierarchically%2520composed%2520of%2520two%2520modules%253A%2520an%250Aopponent%2520modeling%2520module%2520that%2520infers%2520others%2527%2520goals%2520and%2520learns%2520corresponding%250Agoal-conditioned%2520policies%252C%2520and%2520a%2520planning%2520module%2520that%2520employs%2520Monte%2520Carlo%2520Tree%250ASearch%2520%2528MCTS%2529%2520to%2520identify%2520the%2520best%2520response.%2520Our%2520approach%2520improves%2520efficiency%250Aby%2520updating%2520beliefs%2520about%2520others%2527%2520goals%2520both%2520across%2520and%2520within%2520episodes%2520and%2520by%250Ausing%2520information%2520from%2520the%2520opponent%2520modeling%2520module%2520to%2520guide%2520planning.%250AExperimental%2520results%2520demonstrate%2520that%2520in%2520mixed-motive%2520environments%252C%2520HOP%250Aexhibits%2520superior%2520few-shot%2520adaptation%2520capabilities%2520when%2520interacting%2520with%250Avarious%2520unseen%2520agents%252C%2520and%2520excels%2520in%2520self-play%2520scenarios.%2520Furthermore%252C%2520the%250Aemergence%2520of%2520social%2520intelligence%2520during%2520our%2520experiments%2520underscores%2520the%250Apotential%2520of%2520our%2520approach%2520in%2520complex%2520multi-agent%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Adaptation%20in%20Mixed-Motive%20Environments%20via%20Hierarchical%0A%20%20Opponent%20Modeling%20and%20Planning&entry.906535625=Yizhe%20Huang%20and%20Anji%20Liu%20and%20Fanqi%20Kong%20and%20Yaodong%20Yang%20and%20Song-Chun%20Zhu%20and%20Xue%20Feng&entry.1292438233=%20%20Despite%20the%20recent%20successes%20of%20multi-agent%20reinforcement%20learning%20%28MARL%29%0Aalgorithms%2C%20efficiently%20adapting%20to%20co-players%20in%20mixed-motive%20environments%0Aremains%20a%20significant%20challenge.%20One%20feasible%20approach%20is%20to%20hierarchically%0Amodel%20co-players%27%20behavior%20based%20on%20inferring%20their%20characteristics.%20However%2C%0Athese%20methods%20often%20encounter%20difficulties%20in%20efficient%20reasoning%20and%0Autilization%20of%20inferred%20information.%20To%20address%20these%20issues%2C%20we%20propose%0AHierarchical%20Opponent%20modeling%20and%20Planning%20%28HOP%29%2C%20a%20novel%20multi-agent%0Adecision-making%20algorithm%20that%20enables%20few-shot%20adaptation%20to%20unseen%20policies%0Ain%20mixed-motive%20environments.%20HOP%20is%20hierarchically%20composed%20of%20two%20modules%3A%20an%0Aopponent%20modeling%20module%20that%20infers%20others%27%20goals%20and%20learns%20corresponding%0Agoal-conditioned%20policies%2C%20and%20a%20planning%20module%20that%20employs%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20to%20identify%20the%20best%20response.%20Our%20approach%20improves%20efficiency%0Aby%20updating%20beliefs%20about%20others%27%20goals%20both%20across%20and%20within%20episodes%20and%20by%0Ausing%20information%20from%20the%20opponent%20modeling%20module%20to%20guide%20planning.%0AExperimental%20results%20demonstrate%20that%20in%20mixed-motive%20environments%2C%20HOP%0Aexhibits%20superior%20few-shot%20adaptation%20capabilities%20when%20interacting%20with%0Avarious%20unseen%20agents%2C%20and%20excels%20in%20self-play%20scenarios.%20Furthermore%2C%20the%0Aemergence%20of%20social%20intelligence%20during%20our%20experiments%20underscores%20the%0Apotential%20of%20our%20approach%20in%20complex%20multi-agent%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08002v2&entry.124074799=Read"},
{"title": "Beyond static AI evaluations: advancing human interaction evaluations\n  for LLM harms and risks", "author": "Lujain Ibrahim and Saffron Huang and Lama Ahmad and Markus Anderljung", "abstract": "  Model evaluations are central to understanding the safety, risks, and\nsocietal impacts of AI systems. While most real-world AI applications involve\nhuman-AI interaction, most current evaluations (e.g., common benchmarks) of AI\nmodels do not. Instead, they incorporate human factors in limited ways,\nassessing the safety of models in isolation, thereby falling short of capturing\nthe complexity of human-model interactions. In this paper, we discuss and\noperationalize a definition of an emerging category of evaluations -- \"human\ninteraction evaluations\" (HIEs) -- which focus on the assessment of human-model\ninteractions or the process and the outcomes of humans using models. First, we\nargue that HIEs can be used to increase the validity of safety evaluations,\nassess direct human impact and interaction-specific harms, and guide future\nassessments of models' societal impact. Second, we propose a safety-focused HIE\ndesign framework -- containing a human-LLM interaction taxonomy -- with three\nstages: (1) identifying the risk or harm area, (2) characterizing the use\ncontext, and (3) choosing the evaluation parameters. Third, we apply our\nframework to two potential evaluations for overreliance and persuasion risks.\nFinally, we conclude with tangible recommendations for addressing concerns over\ncosts, replicability, and unrepresentativeness of HIEs.\n", "link": "http://arxiv.org/abs/2405.10632v5", "date": "2024-07-12", "relevancy": 1.4052, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4723}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4709}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20static%20AI%20evaluations%3A%20advancing%20human%20interaction%20evaluations%0A%20%20for%20LLM%20harms%20and%20risks&body=Title%3A%20Beyond%20static%20AI%20evaluations%3A%20advancing%20human%20interaction%20evaluations%0A%20%20for%20LLM%20harms%20and%20risks%0AAuthor%3A%20Lujain%20Ibrahim%20and%20Saffron%20Huang%20and%20Lama%20Ahmad%20and%20Markus%20Anderljung%0AAbstract%3A%20%20%20Model%20evaluations%20are%20central%20to%20understanding%20the%20safety%2C%20risks%2C%20and%0Asocietal%20impacts%20of%20AI%20systems.%20While%20most%20real-world%20AI%20applications%20involve%0Ahuman-AI%20interaction%2C%20most%20current%20evaluations%20%28e.g.%2C%20common%20benchmarks%29%20of%20AI%0Amodels%20do%20not.%20Instead%2C%20they%20incorporate%20human%20factors%20in%20limited%20ways%2C%0Aassessing%20the%20safety%20of%20models%20in%20isolation%2C%20thereby%20falling%20short%20of%20capturing%0Athe%20complexity%20of%20human-model%20interactions.%20In%20this%20paper%2C%20we%20discuss%20and%0Aoperationalize%20a%20definition%20of%20an%20emerging%20category%20of%20evaluations%20--%20%22human%0Ainteraction%20evaluations%22%20%28HIEs%29%20--%20which%20focus%20on%20the%20assessment%20of%20human-model%0Ainteractions%20or%20the%20process%20and%20the%20outcomes%20of%20humans%20using%20models.%20First%2C%20we%0Aargue%20that%20HIEs%20can%20be%20used%20to%20increase%20the%20validity%20of%20safety%20evaluations%2C%0Aassess%20direct%20human%20impact%20and%20interaction-specific%20harms%2C%20and%20guide%20future%0Aassessments%20of%20models%27%20societal%20impact.%20Second%2C%20we%20propose%20a%20safety-focused%20HIE%0Adesign%20framework%20--%20containing%20a%20human-LLM%20interaction%20taxonomy%20--%20with%20three%0Astages%3A%20%281%29%20identifying%20the%20risk%20or%20harm%20area%2C%20%282%29%20characterizing%20the%20use%0Acontext%2C%20and%20%283%29%20choosing%20the%20evaluation%20parameters.%20Third%2C%20we%20apply%20our%0Aframework%20to%20two%20potential%20evaluations%20for%20overreliance%20and%20persuasion%20risks.%0AFinally%2C%20we%20conclude%20with%20tangible%20recommendations%20for%20addressing%20concerns%20over%0Acosts%2C%20replicability%2C%20and%20unrepresentativeness%20of%20HIEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10632v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520static%2520AI%2520evaluations%253A%2520advancing%2520human%2520interaction%2520evaluations%250A%2520%2520for%2520LLM%2520harms%2520and%2520risks%26entry.906535625%3DLujain%2520Ibrahim%2520and%2520Saffron%2520Huang%2520and%2520Lama%2520Ahmad%2520and%2520Markus%2520Anderljung%26entry.1292438233%3D%2520%2520Model%2520evaluations%2520are%2520central%2520to%2520understanding%2520the%2520safety%252C%2520risks%252C%2520and%250Asocietal%2520impacts%2520of%2520AI%2520systems.%2520While%2520most%2520real-world%2520AI%2520applications%2520involve%250Ahuman-AI%2520interaction%252C%2520most%2520current%2520evaluations%2520%2528e.g.%252C%2520common%2520benchmarks%2529%2520of%2520AI%250Amodels%2520do%2520not.%2520Instead%252C%2520they%2520incorporate%2520human%2520factors%2520in%2520limited%2520ways%252C%250Aassessing%2520the%2520safety%2520of%2520models%2520in%2520isolation%252C%2520thereby%2520falling%2520short%2520of%2520capturing%250Athe%2520complexity%2520of%2520human-model%2520interactions.%2520In%2520this%2520paper%252C%2520we%2520discuss%2520and%250Aoperationalize%2520a%2520definition%2520of%2520an%2520emerging%2520category%2520of%2520evaluations%2520--%2520%2522human%250Ainteraction%2520evaluations%2522%2520%2528HIEs%2529%2520--%2520which%2520focus%2520on%2520the%2520assessment%2520of%2520human-model%250Ainteractions%2520or%2520the%2520process%2520and%2520the%2520outcomes%2520of%2520humans%2520using%2520models.%2520First%252C%2520we%250Aargue%2520that%2520HIEs%2520can%2520be%2520used%2520to%2520increase%2520the%2520validity%2520of%2520safety%2520evaluations%252C%250Aassess%2520direct%2520human%2520impact%2520and%2520interaction-specific%2520harms%252C%2520and%2520guide%2520future%250Aassessments%2520of%2520models%2527%2520societal%2520impact.%2520Second%252C%2520we%2520propose%2520a%2520safety-focused%2520HIE%250Adesign%2520framework%2520--%2520containing%2520a%2520human-LLM%2520interaction%2520taxonomy%2520--%2520with%2520three%250Astages%253A%2520%25281%2529%2520identifying%2520the%2520risk%2520or%2520harm%2520area%252C%2520%25282%2529%2520characterizing%2520the%2520use%250Acontext%252C%2520and%2520%25283%2529%2520choosing%2520the%2520evaluation%2520parameters.%2520Third%252C%2520we%2520apply%2520our%250Aframework%2520to%2520two%2520potential%2520evaluations%2520for%2520overreliance%2520and%2520persuasion%2520risks.%250AFinally%252C%2520we%2520conclude%2520with%2520tangible%2520recommendations%2520for%2520addressing%2520concerns%2520over%250Acosts%252C%2520replicability%252C%2520and%2520unrepresentativeness%2520of%2520HIEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10632v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20static%20AI%20evaluations%3A%20advancing%20human%20interaction%20evaluations%0A%20%20for%20LLM%20harms%20and%20risks&entry.906535625=Lujain%20Ibrahim%20and%20Saffron%20Huang%20and%20Lama%20Ahmad%20and%20Markus%20Anderljung&entry.1292438233=%20%20Model%20evaluations%20are%20central%20to%20understanding%20the%20safety%2C%20risks%2C%20and%0Asocietal%20impacts%20of%20AI%20systems.%20While%20most%20real-world%20AI%20applications%20involve%0Ahuman-AI%20interaction%2C%20most%20current%20evaluations%20%28e.g.%2C%20common%20benchmarks%29%20of%20AI%0Amodels%20do%20not.%20Instead%2C%20they%20incorporate%20human%20factors%20in%20limited%20ways%2C%0Aassessing%20the%20safety%20of%20models%20in%20isolation%2C%20thereby%20falling%20short%20of%20capturing%0Athe%20complexity%20of%20human-model%20interactions.%20In%20this%20paper%2C%20we%20discuss%20and%0Aoperationalize%20a%20definition%20of%20an%20emerging%20category%20of%20evaluations%20--%20%22human%0Ainteraction%20evaluations%22%20%28HIEs%29%20--%20which%20focus%20on%20the%20assessment%20of%20human-model%0Ainteractions%20or%20the%20process%20and%20the%20outcomes%20of%20humans%20using%20models.%20First%2C%20we%0Aargue%20that%20HIEs%20can%20be%20used%20to%20increase%20the%20validity%20of%20safety%20evaluations%2C%0Aassess%20direct%20human%20impact%20and%20interaction-specific%20harms%2C%20and%20guide%20future%0Aassessments%20of%20models%27%20societal%20impact.%20Second%2C%20we%20propose%20a%20safety-focused%20HIE%0Adesign%20framework%20--%20containing%20a%20human-LLM%20interaction%20taxonomy%20--%20with%20three%0Astages%3A%20%281%29%20identifying%20the%20risk%20or%20harm%20area%2C%20%282%29%20characterizing%20the%20use%0Acontext%2C%20and%20%283%29%20choosing%20the%20evaluation%20parameters.%20Third%2C%20we%20apply%20our%0Aframework%20to%20two%20potential%20evaluations%20for%20overreliance%20and%20persuasion%20risks.%0AFinally%2C%20we%20conclude%20with%20tangible%20recommendations%20for%20addressing%20concerns%20over%0Acosts%2C%20replicability%2C%20and%20unrepresentativeness%20of%20HIEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10632v5&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


