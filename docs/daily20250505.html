<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250504.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Compensating Spatiotemporally Inconsistent Observations for Online\n  Dynamic 3D Gaussian Splatting", "author": "Youngsik Yun and Jeongmin Bae and Hyunseung Son and Seoha Kim and Hahyun Lee and Gun Bang and Youngjung Uh", "abstract": "  Online reconstruction of dynamic scenes is significant as it enables learning\nscenes from live-streaming video inputs, while existing offline dynamic\nreconstruction methods rely on recorded video inputs. However, previous online\nreconstruction approaches have primarily focused on efficiency and rendering\nquality, overlooking the temporal consistency of their results, which often\ncontain noticeable artifacts in static regions. This paper identifies that\nerrors such as noise in real-world recordings affect temporal inconsistency in\nonline reconstruction. We propose a method that enhances temporal consistency\nin online reconstruction from observations with temporal inconsistency which is\ninevitable in cameras. We show that our method restores the ideal observation\nby subtracting the learned error. We demonstrate that applying our method to\nvarious baselines significantly enhances both temporal consistency and\nrendering quality across datasets. Code, video results, and checkpoints are\navailable at https://bbangsik13.github.io/OR2.\n", "link": "http://arxiv.org/abs/2505.01235v1", "date": "2025-05-02", "relevancy": 3.2146, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.668}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.662}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compensating%20Spatiotemporally%20Inconsistent%20Observations%20for%20Online%0A%20%20Dynamic%203D%20Gaussian%20Splatting&body=Title%3A%20Compensating%20Spatiotemporally%20Inconsistent%20Observations%20for%20Online%0A%20%20Dynamic%203D%20Gaussian%20Splatting%0AAuthor%3A%20Youngsik%20Yun%20and%20Jeongmin%20Bae%20and%20Hyunseung%20Son%20and%20Seoha%20Kim%20and%20Hahyun%20Lee%20and%20Gun%20Bang%20and%20Youngjung%20Uh%0AAbstract%3A%20%20%20Online%20reconstruction%20of%20dynamic%20scenes%20is%20significant%20as%20it%20enables%20learning%0Ascenes%20from%20live-streaming%20video%20inputs%2C%20while%20existing%20offline%20dynamic%0Areconstruction%20methods%20rely%20on%20recorded%20video%20inputs.%20However%2C%20previous%20online%0Areconstruction%20approaches%20have%20primarily%20focused%20on%20efficiency%20and%20rendering%0Aquality%2C%20overlooking%20the%20temporal%20consistency%20of%20their%20results%2C%20which%20often%0Acontain%20noticeable%20artifacts%20in%20static%20regions.%20This%20paper%20identifies%20that%0Aerrors%20such%20as%20noise%20in%20real-world%20recordings%20affect%20temporal%20inconsistency%20in%0Aonline%20reconstruction.%20We%20propose%20a%20method%20that%20enhances%20temporal%20consistency%0Ain%20online%20reconstruction%20from%20observations%20with%20temporal%20inconsistency%20which%20is%0Ainevitable%20in%20cameras.%20We%20show%20that%20our%20method%20restores%20the%20ideal%20observation%0Aby%20subtracting%20the%20learned%20error.%20We%20demonstrate%20that%20applying%20our%20method%20to%0Avarious%20baselines%20significantly%20enhances%20both%20temporal%20consistency%20and%0Arendering%20quality%20across%20datasets.%20Code%2C%20video%20results%2C%20and%20checkpoints%20are%0Aavailable%20at%20https%3A//bbangsik13.github.io/OR2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompensating%2520Spatiotemporally%2520Inconsistent%2520Observations%2520for%2520Online%250A%2520%2520Dynamic%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYoungsik%2520Yun%2520and%2520Jeongmin%2520Bae%2520and%2520Hyunseung%2520Son%2520and%2520Seoha%2520Kim%2520and%2520Hahyun%2520Lee%2520and%2520Gun%2520Bang%2520and%2520Youngjung%2520Uh%26entry.1292438233%3D%2520%2520Online%2520reconstruction%2520of%2520dynamic%2520scenes%2520is%2520significant%2520as%2520it%2520enables%2520learning%250Ascenes%2520from%2520live-streaming%2520video%2520inputs%252C%2520while%2520existing%2520offline%2520dynamic%250Areconstruction%2520methods%2520rely%2520on%2520recorded%2520video%2520inputs.%2520However%252C%2520previous%2520online%250Areconstruction%2520approaches%2520have%2520primarily%2520focused%2520on%2520efficiency%2520and%2520rendering%250Aquality%252C%2520overlooking%2520the%2520temporal%2520consistency%2520of%2520their%2520results%252C%2520which%2520often%250Acontain%2520noticeable%2520artifacts%2520in%2520static%2520regions.%2520This%2520paper%2520identifies%2520that%250Aerrors%2520such%2520as%2520noise%2520in%2520real-world%2520recordings%2520affect%2520temporal%2520inconsistency%2520in%250Aonline%2520reconstruction.%2520We%2520propose%2520a%2520method%2520that%2520enhances%2520temporal%2520consistency%250Ain%2520online%2520reconstruction%2520from%2520observations%2520with%2520temporal%2520inconsistency%2520which%2520is%250Ainevitable%2520in%2520cameras.%2520We%2520show%2520that%2520our%2520method%2520restores%2520the%2520ideal%2520observation%250Aby%2520subtracting%2520the%2520learned%2520error.%2520We%2520demonstrate%2520that%2520applying%2520our%2520method%2520to%250Avarious%2520baselines%2520significantly%2520enhances%2520both%2520temporal%2520consistency%2520and%250Arendering%2520quality%2520across%2520datasets.%2520Code%252C%2520video%2520results%252C%2520and%2520checkpoints%2520are%250Aavailable%2520at%2520https%253A//bbangsik13.github.io/OR2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compensating%20Spatiotemporally%20Inconsistent%20Observations%20for%20Online%0A%20%20Dynamic%203D%20Gaussian%20Splatting&entry.906535625=Youngsik%20Yun%20and%20Jeongmin%20Bae%20and%20Hyunseung%20Son%20and%20Seoha%20Kim%20and%20Hahyun%20Lee%20and%20Gun%20Bang%20and%20Youngjung%20Uh&entry.1292438233=%20%20Online%20reconstruction%20of%20dynamic%20scenes%20is%20significant%20as%20it%20enables%20learning%0Ascenes%20from%20live-streaming%20video%20inputs%2C%20while%20existing%20offline%20dynamic%0Areconstruction%20methods%20rely%20on%20recorded%20video%20inputs.%20However%2C%20previous%20online%0Areconstruction%20approaches%20have%20primarily%20focused%20on%20efficiency%20and%20rendering%0Aquality%2C%20overlooking%20the%20temporal%20consistency%20of%20their%20results%2C%20which%20often%0Acontain%20noticeable%20artifacts%20in%20static%20regions.%20This%20paper%20identifies%20that%0Aerrors%20such%20as%20noise%20in%20real-world%20recordings%20affect%20temporal%20inconsistency%20in%0Aonline%20reconstruction.%20We%20propose%20a%20method%20that%20enhances%20temporal%20consistency%0Ain%20online%20reconstruction%20from%20observations%20with%20temporal%20inconsistency%20which%20is%0Ainevitable%20in%20cameras.%20We%20show%20that%20our%20method%20restores%20the%20ideal%20observation%0Aby%20subtracting%20the%20learned%20error.%20We%20demonstrate%20that%20applying%20our%20method%20to%0Avarious%20baselines%20significantly%20enhances%20both%20temporal%20consistency%20and%0Arendering%20quality%20across%20datasets.%20Code%2C%20video%20results%2C%20and%20checkpoints%20are%0Aavailable%20at%20https%3A//bbangsik13.github.io/OR2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01235v1&entry.124074799=Read"},
{"title": "GENMO: A GENeralist Model for Human MOtion", "author": "Jiefeng Li and Jinkun Cao and Haotian Zhang and Davis Rempe and Jan Kautz and Umar Iqbal and Ye Yuan", "abstract": "  Human motion modeling traditionally separates motion generation and\nestimation into distinct tasks with specialized models. Motion generation\nmodels focus on creating diverse, realistic motions from inputs like text,\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\nmotion trajectories from observations like videos. Despite sharing underlying\nrepresentations of temporal dynamics and kinematics, this separation limits\nknowledge transfer between tasks and requires maintaining separate models. We\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\nestimation and generation in a single framework. Our key insight is to\nreformulate motion estimation as constrained motion generation, where the\noutput motion must precisely satisfy observed conditioning signals. Leveraging\nthe synergy between regression and diffusion, GENMO achieves accurate global\nmotion estimation while enabling diverse motion generation. We also introduce\nan estimation-guided training objective that exploits in-the-wild videos with\n2D annotations and text descriptions to enhance generative diversity.\nFurthermore, our novel architecture handles variable-length motions and mixed\nmultimodal conditions (text, audio, video) at different time intervals,\noffering flexible control. This unified approach creates synergistic benefits:\ngenerative priors improve estimated motions under challenging conditions like\nocclusions, while diverse video data enhances generation capabilities.\nExtensive experiments demonstrate GENMO's effectiveness as a generalist\nframework that successfully handles multiple human motion tasks within a single\nmodel.\n", "link": "http://arxiv.org/abs/2505.01425v1", "date": "2025-05-02", "relevancy": 3.1274, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.634}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6247}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GENMO%3A%20A%20GENeralist%20Model%20for%20Human%20MOtion&body=Title%3A%20GENMO%3A%20A%20GENeralist%20Model%20for%20Human%20MOtion%0AAuthor%3A%20Jiefeng%20Li%20and%20Jinkun%20Cao%20and%20Haotian%20Zhang%20and%20Davis%20Rempe%20and%20Jan%20Kautz%20and%20Umar%20Iqbal%20and%20Ye%20Yuan%0AAbstract%3A%20%20%20Human%20motion%20modeling%20traditionally%20separates%20motion%20generation%20and%0Aestimation%20into%20distinct%20tasks%20with%20specialized%20models.%20Motion%20generation%0Amodels%20focus%20on%20creating%20diverse%2C%20realistic%20motions%20from%20inputs%20like%20text%2C%0Aaudio%2C%20or%20keyframes%2C%20while%20motion%20estimation%20models%20aim%20to%20reconstruct%20accurate%0Amotion%20trajectories%20from%20observations%20like%20videos.%20Despite%20sharing%20underlying%0Arepresentations%20of%20temporal%20dynamics%20and%20kinematics%2C%20this%20separation%20limits%0Aknowledge%20transfer%20between%20tasks%20and%20requires%20maintaining%20separate%20models.%20We%0Apresent%20GENMO%2C%20a%20unified%20Generalist%20Model%20for%20Human%20Motion%20that%20bridges%20motion%0Aestimation%20and%20generation%20in%20a%20single%20framework.%20Our%20key%20insight%20is%20to%0Areformulate%20motion%20estimation%20as%20constrained%20motion%20generation%2C%20where%20the%0Aoutput%20motion%20must%20precisely%20satisfy%20observed%20conditioning%20signals.%20Leveraging%0Athe%20synergy%20between%20regression%20and%20diffusion%2C%20GENMO%20achieves%20accurate%20global%0Amotion%20estimation%20while%20enabling%20diverse%20motion%20generation.%20We%20also%20introduce%0Aan%20estimation-guided%20training%20objective%20that%20exploits%20in-the-wild%20videos%20with%0A2D%20annotations%20and%20text%20descriptions%20to%20enhance%20generative%20diversity.%0AFurthermore%2C%20our%20novel%20architecture%20handles%20variable-length%20motions%20and%20mixed%0Amultimodal%20conditions%20%28text%2C%20audio%2C%20video%29%20at%20different%20time%20intervals%2C%0Aoffering%20flexible%20control.%20This%20unified%20approach%20creates%20synergistic%20benefits%3A%0Agenerative%20priors%20improve%20estimated%20motions%20under%20challenging%20conditions%20like%0Aocclusions%2C%20while%20diverse%20video%20data%20enhances%20generation%20capabilities.%0AExtensive%20experiments%20demonstrate%20GENMO%27s%20effectiveness%20as%20a%20generalist%0Aframework%20that%20successfully%20handles%20multiple%20human%20motion%20tasks%20within%20a%20single%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGENMO%253A%2520A%2520GENeralist%2520Model%2520for%2520Human%2520MOtion%26entry.906535625%3DJiefeng%2520Li%2520and%2520Jinkun%2520Cao%2520and%2520Haotian%2520Zhang%2520and%2520Davis%2520Rempe%2520and%2520Jan%2520Kautz%2520and%2520Umar%2520Iqbal%2520and%2520Ye%2520Yuan%26entry.1292438233%3D%2520%2520Human%2520motion%2520modeling%2520traditionally%2520separates%2520motion%2520generation%2520and%250Aestimation%2520into%2520distinct%2520tasks%2520with%2520specialized%2520models.%2520Motion%2520generation%250Amodels%2520focus%2520on%2520creating%2520diverse%252C%2520realistic%2520motions%2520from%2520inputs%2520like%2520text%252C%250Aaudio%252C%2520or%2520keyframes%252C%2520while%2520motion%2520estimation%2520models%2520aim%2520to%2520reconstruct%2520accurate%250Amotion%2520trajectories%2520from%2520observations%2520like%2520videos.%2520Despite%2520sharing%2520underlying%250Arepresentations%2520of%2520temporal%2520dynamics%2520and%2520kinematics%252C%2520this%2520separation%2520limits%250Aknowledge%2520transfer%2520between%2520tasks%2520and%2520requires%2520maintaining%2520separate%2520models.%2520We%250Apresent%2520GENMO%252C%2520a%2520unified%2520Generalist%2520Model%2520for%2520Human%2520Motion%2520that%2520bridges%2520motion%250Aestimation%2520and%2520generation%2520in%2520a%2520single%2520framework.%2520Our%2520key%2520insight%2520is%2520to%250Areformulate%2520motion%2520estimation%2520as%2520constrained%2520motion%2520generation%252C%2520where%2520the%250Aoutput%2520motion%2520must%2520precisely%2520satisfy%2520observed%2520conditioning%2520signals.%2520Leveraging%250Athe%2520synergy%2520between%2520regression%2520and%2520diffusion%252C%2520GENMO%2520achieves%2520accurate%2520global%250Amotion%2520estimation%2520while%2520enabling%2520diverse%2520motion%2520generation.%2520We%2520also%2520introduce%250Aan%2520estimation-guided%2520training%2520objective%2520that%2520exploits%2520in-the-wild%2520videos%2520with%250A2D%2520annotations%2520and%2520text%2520descriptions%2520to%2520enhance%2520generative%2520diversity.%250AFurthermore%252C%2520our%2520novel%2520architecture%2520handles%2520variable-length%2520motions%2520and%2520mixed%250Amultimodal%2520conditions%2520%2528text%252C%2520audio%252C%2520video%2529%2520at%2520different%2520time%2520intervals%252C%250Aoffering%2520flexible%2520control.%2520This%2520unified%2520approach%2520creates%2520synergistic%2520benefits%253A%250Agenerative%2520priors%2520improve%2520estimated%2520motions%2520under%2520challenging%2520conditions%2520like%250Aocclusions%252C%2520while%2520diverse%2520video%2520data%2520enhances%2520generation%2520capabilities.%250AExtensive%2520experiments%2520demonstrate%2520GENMO%2527s%2520effectiveness%2520as%2520a%2520generalist%250Aframework%2520that%2520successfully%2520handles%2520multiple%2520human%2520motion%2520tasks%2520within%2520a%2520single%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GENMO%3A%20A%20GENeralist%20Model%20for%20Human%20MOtion&entry.906535625=Jiefeng%20Li%20and%20Jinkun%20Cao%20and%20Haotian%20Zhang%20and%20Davis%20Rempe%20and%20Jan%20Kautz%20and%20Umar%20Iqbal%20and%20Ye%20Yuan&entry.1292438233=%20%20Human%20motion%20modeling%20traditionally%20separates%20motion%20generation%20and%0Aestimation%20into%20distinct%20tasks%20with%20specialized%20models.%20Motion%20generation%0Amodels%20focus%20on%20creating%20diverse%2C%20realistic%20motions%20from%20inputs%20like%20text%2C%0Aaudio%2C%20or%20keyframes%2C%20while%20motion%20estimation%20models%20aim%20to%20reconstruct%20accurate%0Amotion%20trajectories%20from%20observations%20like%20videos.%20Despite%20sharing%20underlying%0Arepresentations%20of%20temporal%20dynamics%20and%20kinematics%2C%20this%20separation%20limits%0Aknowledge%20transfer%20between%20tasks%20and%20requires%20maintaining%20separate%20models.%20We%0Apresent%20GENMO%2C%20a%20unified%20Generalist%20Model%20for%20Human%20Motion%20that%20bridges%20motion%0Aestimation%20and%20generation%20in%20a%20single%20framework.%20Our%20key%20insight%20is%20to%0Areformulate%20motion%20estimation%20as%20constrained%20motion%20generation%2C%20where%20the%0Aoutput%20motion%20must%20precisely%20satisfy%20observed%20conditioning%20signals.%20Leveraging%0Athe%20synergy%20between%20regression%20and%20diffusion%2C%20GENMO%20achieves%20accurate%20global%0Amotion%20estimation%20while%20enabling%20diverse%20motion%20generation.%20We%20also%20introduce%0Aan%20estimation-guided%20training%20objective%20that%20exploits%20in-the-wild%20videos%20with%0A2D%20annotations%20and%20text%20descriptions%20to%20enhance%20generative%20diversity.%0AFurthermore%2C%20our%20novel%20architecture%20handles%20variable-length%20motions%20and%20mixed%0Amultimodal%20conditions%20%28text%2C%20audio%2C%20video%29%20at%20different%20time%20intervals%2C%0Aoffering%20flexible%20control.%20This%20unified%20approach%20creates%20synergistic%20benefits%3A%0Agenerative%20priors%20improve%20estimated%20motions%20under%20challenging%20conditions%20like%0Aocclusions%2C%20while%20diverse%20video%20data%20enhances%20generation%20capabilities.%0AExtensive%20experiments%20demonstrate%20GENMO%27s%20effectiveness%20as%20a%20generalist%0Aframework%20that%20successfully%20handles%20multiple%20human%20motion%20tasks%20within%20a%20single%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01425v1&entry.124074799=Read"},
{"title": "Model See Model Do: Speech-Driven Facial Animation with Style Control", "author": "Yifang Pan and Karan Singh and Luiz Gustavo Hafemann", "abstract": "  Speech-driven 3D facial animation plays a key role in applications such as\nvirtual avatars, gaming, and digital content creation. While existing methods\nhave made significant progress in achieving accurate lip synchronization and\ngenerating basic emotional expressions, they often struggle to capture and\neffectively transfer nuanced performance styles. We propose a novel\nexample-based generation framework that conditions a latent diffusion model on\na reference style clip to produce highly expressive and temporally coherent\nfacial animations. To address the challenge of accurately adhering to the style\nreference, we introduce a novel conditioning mechanism called style basis,\nwhich extracts key poses from the reference and additively guides the diffusion\ngeneration process to fit the style without compromising lip synchronization\nquality. This approach enables the model to capture subtle stylistic cues while\nensuring that the generated animations align closely with the input speech.\nExtensive qualitative, quantitative, and perceptual evaluations demonstrate the\neffectiveness of our method in faithfully reproducing the desired style while\nachieving superior lip synchronization across various speech scenarios.\n", "link": "http://arxiv.org/abs/2505.01319v1", "date": "2025-05-02", "relevancy": 3.1, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6601}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5999}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20See%20Model%20Do%3A%20Speech-Driven%20Facial%20Animation%20with%20Style%20Control&body=Title%3A%20Model%20See%20Model%20Do%3A%20Speech-Driven%20Facial%20Animation%20with%20Style%20Control%0AAuthor%3A%20Yifang%20Pan%20and%20Karan%20Singh%20and%20Luiz%20Gustavo%20Hafemann%0AAbstract%3A%20%20%20Speech-driven%203D%20facial%20animation%20plays%20a%20key%20role%20in%20applications%20such%20as%0Avirtual%20avatars%2C%20gaming%2C%20and%20digital%20content%20creation.%20While%20existing%20methods%0Ahave%20made%20significant%20progress%20in%20achieving%20accurate%20lip%20synchronization%20and%0Agenerating%20basic%20emotional%20expressions%2C%20they%20often%20struggle%20to%20capture%20and%0Aeffectively%20transfer%20nuanced%20performance%20styles.%20We%20propose%20a%20novel%0Aexample-based%20generation%20framework%20that%20conditions%20a%20latent%20diffusion%20model%20on%0Aa%20reference%20style%20clip%20to%20produce%20highly%20expressive%20and%20temporally%20coherent%0Afacial%20animations.%20To%20address%20the%20challenge%20of%20accurately%20adhering%20to%20the%20style%0Areference%2C%20we%20introduce%20a%20novel%20conditioning%20mechanism%20called%20style%20basis%2C%0Awhich%20extracts%20key%20poses%20from%20the%20reference%20and%20additively%20guides%20the%20diffusion%0Ageneration%20process%20to%20fit%20the%20style%20without%20compromising%20lip%20synchronization%0Aquality.%20This%20approach%20enables%20the%20model%20to%20capture%20subtle%20stylistic%20cues%20while%0Aensuring%20that%20the%20generated%20animations%20align%20closely%20with%20the%20input%20speech.%0AExtensive%20qualitative%2C%20quantitative%2C%20and%20perceptual%20evaluations%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20faithfully%20reproducing%20the%20desired%20style%20while%0Aachieving%20superior%20lip%20synchronization%20across%20various%20speech%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520See%2520Model%2520Do%253A%2520Speech-Driven%2520Facial%2520Animation%2520with%2520Style%2520Control%26entry.906535625%3DYifang%2520Pan%2520and%2520Karan%2520Singh%2520and%2520Luiz%2520Gustavo%2520Hafemann%26entry.1292438233%3D%2520%2520Speech-driven%25203D%2520facial%2520animation%2520plays%2520a%2520key%2520role%2520in%2520applications%2520such%2520as%250Avirtual%2520avatars%252C%2520gaming%252C%2520and%2520digital%2520content%2520creation.%2520While%2520existing%2520methods%250Ahave%2520made%2520significant%2520progress%2520in%2520achieving%2520accurate%2520lip%2520synchronization%2520and%250Agenerating%2520basic%2520emotional%2520expressions%252C%2520they%2520often%2520struggle%2520to%2520capture%2520and%250Aeffectively%2520transfer%2520nuanced%2520performance%2520styles.%2520We%2520propose%2520a%2520novel%250Aexample-based%2520generation%2520framework%2520that%2520conditions%2520a%2520latent%2520diffusion%2520model%2520on%250Aa%2520reference%2520style%2520clip%2520to%2520produce%2520highly%2520expressive%2520and%2520temporally%2520coherent%250Afacial%2520animations.%2520To%2520address%2520the%2520challenge%2520of%2520accurately%2520adhering%2520to%2520the%2520style%250Areference%252C%2520we%2520introduce%2520a%2520novel%2520conditioning%2520mechanism%2520called%2520style%2520basis%252C%250Awhich%2520extracts%2520key%2520poses%2520from%2520the%2520reference%2520and%2520additively%2520guides%2520the%2520diffusion%250Ageneration%2520process%2520to%2520fit%2520the%2520style%2520without%2520compromising%2520lip%2520synchronization%250Aquality.%2520This%2520approach%2520enables%2520the%2520model%2520to%2520capture%2520subtle%2520stylistic%2520cues%2520while%250Aensuring%2520that%2520the%2520generated%2520animations%2520align%2520closely%2520with%2520the%2520input%2520speech.%250AExtensive%2520qualitative%252C%2520quantitative%252C%2520and%2520perceptual%2520evaluations%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520faithfully%2520reproducing%2520the%2520desired%2520style%2520while%250Aachieving%2520superior%2520lip%2520synchronization%2520across%2520various%2520speech%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20See%20Model%20Do%3A%20Speech-Driven%20Facial%20Animation%20with%20Style%20Control&entry.906535625=Yifang%20Pan%20and%20Karan%20Singh%20and%20Luiz%20Gustavo%20Hafemann&entry.1292438233=%20%20Speech-driven%203D%20facial%20animation%20plays%20a%20key%20role%20in%20applications%20such%20as%0Avirtual%20avatars%2C%20gaming%2C%20and%20digital%20content%20creation.%20While%20existing%20methods%0Ahave%20made%20significant%20progress%20in%20achieving%20accurate%20lip%20synchronization%20and%0Agenerating%20basic%20emotional%20expressions%2C%20they%20often%20struggle%20to%20capture%20and%0Aeffectively%20transfer%20nuanced%20performance%20styles.%20We%20propose%20a%20novel%0Aexample-based%20generation%20framework%20that%20conditions%20a%20latent%20diffusion%20model%20on%0Aa%20reference%20style%20clip%20to%20produce%20highly%20expressive%20and%20temporally%20coherent%0Afacial%20animations.%20To%20address%20the%20challenge%20of%20accurately%20adhering%20to%20the%20style%0Areference%2C%20we%20introduce%20a%20novel%20conditioning%20mechanism%20called%20style%20basis%2C%0Awhich%20extracts%20key%20poses%20from%20the%20reference%20and%20additively%20guides%20the%20diffusion%0Ageneration%20process%20to%20fit%20the%20style%20without%20compromising%20lip%20synchronization%0Aquality.%20This%20approach%20enables%20the%20model%20to%20capture%20subtle%20stylistic%20cues%20while%0Aensuring%20that%20the%20generated%20animations%20align%20closely%20with%20the%20input%20speech.%0AExtensive%20qualitative%2C%20quantitative%2C%20and%20perceptual%20evaluations%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20faithfully%20reproducing%20the%20desired%20style%20while%0Aachieving%20superior%20lip%20synchronization%20across%20various%20speech%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01319v1&entry.124074799=Read"},
{"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models", "author": "Yuhao Dong and Zuyan Liu and Hai-Long Sun and Jingkang Yang and Winston Hu and Yongming Rao and Ziwei Liu", "abstract": "  Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks.\n", "link": "http://arxiv.org/abs/2411.14432v2", "date": "2025-05-02", "relevancy": 3.0438, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6321}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insight-V%3A%20Exploring%20Long-Chain%20Visual%20Reasoning%20with%20Multimodal%20Large%0A%20%20Language%20Models&body=Title%3A%20Insight-V%3A%20Exploring%20Long-Chain%20Visual%20Reasoning%20with%20Multimodal%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yuhao%20Dong%20and%20Zuyan%20Liu%20and%20Hai-Long%20Sun%20and%20Jingkang%20Yang%20and%20Winston%20Hu%20and%20Yongming%20Rao%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20enhanced%20capabilities%20and%0Areliability%20by%20reasoning%20more%2C%20evolving%20from%20Chain-of-Thought%20prompting%20to%0Aproduct-level%20solutions%20like%20OpenAI%20o1.%20Despite%20various%20efforts%20to%20improve%20LLM%0Areasoning%2C%20high-quality%20long-chain%20reasoning%20data%20and%20optimized%20training%0Apipelines%20still%20remain%20inadequately%20explored%20in%20vision-language%20tasks.%20In%20this%0Apaper%2C%20we%20present%20Insight-V%2C%20an%20early%20effort%20to%201%29%20scalably%20produce%20long%20and%0Arobust%20reasoning%20data%20for%20complex%20multi-modal%20tasks%2C%20and%202%29%20an%20effective%0Atraining%20pipeline%20to%20enhance%20the%20reasoning%20capabilities%20of%20multi-modal%20large%0Alanguage%20models%20%28MLLMs%29.%20Specifically%2C%20to%20create%20long%20and%20structured%20reasoning%0Adata%20without%20human%20labor%2C%20we%20design%20a%20two-step%20pipeline%20with%20a%20progressive%0Astrategy%20to%20generate%20sufficiently%20long%20and%20diverse%20reasoning%20paths%20and%20a%0Amulti-granularity%20assessment%20method%20to%20ensure%20data%20quality.%20We%20observe%20that%0Adirectly%20supervising%20MLLMs%20with%20such%20long%20and%20complex%20reasoning%20data%20will%20not%0Ayield%20ideal%20reasoning%20ability.%20To%20tackle%20this%20problem%2C%20we%20design%20a%20multi-agent%0Asystem%20consisting%20of%20a%20reasoning%20agent%20dedicated%20to%20performing%20long-chain%0Areasoning%20and%20a%20summary%20agent%20trained%20to%20judge%20and%20summarize%20reasoning%20results.%0AWe%20further%20incorporate%20an%20iterative%20DPO%20algorithm%20to%20enhance%20the%20reasoning%0Aagent%27s%20generation%20stability%20and%20quality.%20Based%20on%20the%20popular%20LLaVA-NeXT%20model%0Aand%20our%20stronger%20base%20MLLM%2C%20we%20demonstrate%20significant%20performance%20gains%20across%0Achallenging%20multi-modal%20benchmarks%20requiring%20visual%20reasoning.%20Benefiting%20from%0Aour%20multi-agent%20system%2C%20Insight-V%20can%20also%20easily%20maintain%20or%20improve%0Aperformance%20on%20perception-focused%20multi-modal%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsight-V%253A%2520Exploring%2520Long-Chain%2520Visual%2520Reasoning%2520with%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYuhao%2520Dong%2520and%2520Zuyan%2520Liu%2520and%2520Hai-Long%2520Sun%2520and%2520Jingkang%2520Yang%2520and%2520Winston%2520Hu%2520and%2520Yongming%2520Rao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520enhanced%2520capabilities%2520and%250Areliability%2520by%2520reasoning%2520more%252C%2520evolving%2520from%2520Chain-of-Thought%2520prompting%2520to%250Aproduct-level%2520solutions%2520like%2520OpenAI%2520o1.%2520Despite%2520various%2520efforts%2520to%2520improve%2520LLM%250Areasoning%252C%2520high-quality%2520long-chain%2520reasoning%2520data%2520and%2520optimized%2520training%250Apipelines%2520still%2520remain%2520inadequately%2520explored%2520in%2520vision-language%2520tasks.%2520In%2520this%250Apaper%252C%2520we%2520present%2520Insight-V%252C%2520an%2520early%2520effort%2520to%25201%2529%2520scalably%2520produce%2520long%2520and%250Arobust%2520reasoning%2520data%2520for%2520complex%2520multi-modal%2520tasks%252C%2520and%25202%2529%2520an%2520effective%250Atraining%2520pipeline%2520to%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520multi-modal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529.%2520Specifically%252C%2520to%2520create%2520long%2520and%2520structured%2520reasoning%250Adata%2520without%2520human%2520labor%252C%2520we%2520design%2520a%2520two-step%2520pipeline%2520with%2520a%2520progressive%250Astrategy%2520to%2520generate%2520sufficiently%2520long%2520and%2520diverse%2520reasoning%2520paths%2520and%2520a%250Amulti-granularity%2520assessment%2520method%2520to%2520ensure%2520data%2520quality.%2520We%2520observe%2520that%250Adirectly%2520supervising%2520MLLMs%2520with%2520such%2520long%2520and%2520complex%2520reasoning%2520data%2520will%2520not%250Ayield%2520ideal%2520reasoning%2520ability.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520design%2520a%2520multi-agent%250Asystem%2520consisting%2520of%2520a%2520reasoning%2520agent%2520dedicated%2520to%2520performing%2520long-chain%250Areasoning%2520and%2520a%2520summary%2520agent%2520trained%2520to%2520judge%2520and%2520summarize%2520reasoning%2520results.%250AWe%2520further%2520incorporate%2520an%2520iterative%2520DPO%2520algorithm%2520to%2520enhance%2520the%2520reasoning%250Aagent%2527s%2520generation%2520stability%2520and%2520quality.%2520Based%2520on%2520the%2520popular%2520LLaVA-NeXT%2520model%250Aand%2520our%2520stronger%2520base%2520MLLM%252C%2520we%2520demonstrate%2520significant%2520performance%2520gains%2520across%250Achallenging%2520multi-modal%2520benchmarks%2520requiring%2520visual%2520reasoning.%2520Benefiting%2520from%250Aour%2520multi-agent%2520system%252C%2520Insight-V%2520can%2520also%2520easily%2520maintain%2520or%2520improve%250Aperformance%2520on%2520perception-focused%2520multi-modal%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insight-V%3A%20Exploring%20Long-Chain%20Visual%20Reasoning%20with%20Multimodal%20Large%0A%20%20Language%20Models&entry.906535625=Yuhao%20Dong%20and%20Zuyan%20Liu%20and%20Hai-Long%20Sun%20and%20Jingkang%20Yang%20and%20Winston%20Hu%20and%20Yongming%20Rao%20and%20Ziwei%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20enhanced%20capabilities%20and%0Areliability%20by%20reasoning%20more%2C%20evolving%20from%20Chain-of-Thought%20prompting%20to%0Aproduct-level%20solutions%20like%20OpenAI%20o1.%20Despite%20various%20efforts%20to%20improve%20LLM%0Areasoning%2C%20high-quality%20long-chain%20reasoning%20data%20and%20optimized%20training%0Apipelines%20still%20remain%20inadequately%20explored%20in%20vision-language%20tasks.%20In%20this%0Apaper%2C%20we%20present%20Insight-V%2C%20an%20early%20effort%20to%201%29%20scalably%20produce%20long%20and%0Arobust%20reasoning%20data%20for%20complex%20multi-modal%20tasks%2C%20and%202%29%20an%20effective%0Atraining%20pipeline%20to%20enhance%20the%20reasoning%20capabilities%20of%20multi-modal%20large%0Alanguage%20models%20%28MLLMs%29.%20Specifically%2C%20to%20create%20long%20and%20structured%20reasoning%0Adata%20without%20human%20labor%2C%20we%20design%20a%20two-step%20pipeline%20with%20a%20progressive%0Astrategy%20to%20generate%20sufficiently%20long%20and%20diverse%20reasoning%20paths%20and%20a%0Amulti-granularity%20assessment%20method%20to%20ensure%20data%20quality.%20We%20observe%20that%0Adirectly%20supervising%20MLLMs%20with%20such%20long%20and%20complex%20reasoning%20data%20will%20not%0Ayield%20ideal%20reasoning%20ability.%20To%20tackle%20this%20problem%2C%20we%20design%20a%20multi-agent%0Asystem%20consisting%20of%20a%20reasoning%20agent%20dedicated%20to%20performing%20long-chain%0Areasoning%20and%20a%20summary%20agent%20trained%20to%20judge%20and%20summarize%20reasoning%20results.%0AWe%20further%20incorporate%20an%20iterative%20DPO%20algorithm%20to%20enhance%20the%20reasoning%0Aagent%27s%20generation%20stability%20and%20quality.%20Based%20on%20the%20popular%20LLaVA-NeXT%20model%0Aand%20our%20stronger%20base%20MLLM%2C%20we%20demonstrate%20significant%20performance%20gains%20across%0Achallenging%20multi-modal%20benchmarks%20requiring%20visual%20reasoning.%20Benefiting%20from%0Aour%20multi-agent%20system%2C%20Insight-V%20can%20also%20easily%20maintain%20or%20improve%0Aperformance%20on%20perception-focused%20multi-modal%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14432v2&entry.124074799=Read"},
{"title": "MASH: Masked Anchored SpHerical Distances for 3D Shape Representation\n  and Generation", "author": "Changhao Li and Yu Xin and Xiaowei Zhou and Ariel Shamir and Hao Zhang and Ligang Liu and Ruizhen Hu", "abstract": "  We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view\nand parametrized representation of 3D shapes. Inspired by multi-view geometry\nand motivated by the importance of perceptual shape understanding for learning\n3D shapes, MASH represents a 3D shape as a collection of observable local\nsurface patches, each defined by a spherical distance function emanating from\nan anchor point. We further leverage the compactness of spherical harmonics to\nencode the MASH functions, combined with a generalized view cone with a\nparameterized base that masks the spatial extent of the spherical function to\nattain locality. We develop a differentiable optimization algorithm capable of\nconverting any point cloud into a MASH representation accurately approximating\nground-truth surfaces with arbitrary geometry and topology. Extensive\nexperiments demonstrate that MASH is versatile for multiple applications\nincluding surface reconstruction, shape generation, completion, and blending,\nachieving superior performance thanks to its unique representation encompassing\nboth implicit and explicit features.\n", "link": "http://arxiv.org/abs/2504.09149v3", "date": "2025-05-02", "relevancy": 2.888, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5931}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.575}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MASH%3A%20Masked%20Anchored%20SpHerical%20Distances%20for%203D%20Shape%20Representation%0A%20%20and%20Generation&body=Title%3A%20MASH%3A%20Masked%20Anchored%20SpHerical%20Distances%20for%203D%20Shape%20Representation%0A%20%20and%20Generation%0AAuthor%3A%20Changhao%20Li%20and%20Yu%20Xin%20and%20Xiaowei%20Zhou%20and%20Ariel%20Shamir%20and%20Hao%20Zhang%20and%20Ligang%20Liu%20and%20Ruizhen%20Hu%0AAbstract%3A%20%20%20We%20introduce%20Masked%20Anchored%20SpHerical%20Distances%20%28MASH%29%2C%20a%20novel%20multi-view%0Aand%20parametrized%20representation%20of%203D%20shapes.%20Inspired%20by%20multi-view%20geometry%0Aand%20motivated%20by%20the%20importance%20of%20perceptual%20shape%20understanding%20for%20learning%0A3D%20shapes%2C%20MASH%20represents%20a%203D%20shape%20as%20a%20collection%20of%20observable%20local%0Asurface%20patches%2C%20each%20defined%20by%20a%20spherical%20distance%20function%20emanating%20from%0Aan%20anchor%20point.%20We%20further%20leverage%20the%20compactness%20of%20spherical%20harmonics%20to%0Aencode%20the%20MASH%20functions%2C%20combined%20with%20a%20generalized%20view%20cone%20with%20a%0Aparameterized%20base%20that%20masks%20the%20spatial%20extent%20of%20the%20spherical%20function%20to%0Aattain%20locality.%20We%20develop%20a%20differentiable%20optimization%20algorithm%20capable%20of%0Aconverting%20any%20point%20cloud%20into%20a%20MASH%20representation%20accurately%20approximating%0Aground-truth%20surfaces%20with%20arbitrary%20geometry%20and%20topology.%20Extensive%0Aexperiments%20demonstrate%20that%20MASH%20is%20versatile%20for%20multiple%20applications%0Aincluding%20surface%20reconstruction%2C%20shape%20generation%2C%20completion%2C%20and%20blending%2C%0Aachieving%20superior%20performance%20thanks%20to%20its%20unique%20representation%20encompassing%0Aboth%20implicit%20and%20explicit%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09149v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMASH%253A%2520Masked%2520Anchored%2520SpHerical%2520Distances%2520for%25203D%2520Shape%2520Representation%250A%2520%2520and%2520Generation%26entry.906535625%3DChanghao%2520Li%2520and%2520Yu%2520Xin%2520and%2520Xiaowei%2520Zhou%2520and%2520Ariel%2520Shamir%2520and%2520Hao%2520Zhang%2520and%2520Ligang%2520Liu%2520and%2520Ruizhen%2520Hu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Masked%2520Anchored%2520SpHerical%2520Distances%2520%2528MASH%2529%252C%2520a%2520novel%2520multi-view%250Aand%2520parametrized%2520representation%2520of%25203D%2520shapes.%2520Inspired%2520by%2520multi-view%2520geometry%250Aand%2520motivated%2520by%2520the%2520importance%2520of%2520perceptual%2520shape%2520understanding%2520for%2520learning%250A3D%2520shapes%252C%2520MASH%2520represents%2520a%25203D%2520shape%2520as%2520a%2520collection%2520of%2520observable%2520local%250Asurface%2520patches%252C%2520each%2520defined%2520by%2520a%2520spherical%2520distance%2520function%2520emanating%2520from%250Aan%2520anchor%2520point.%2520We%2520further%2520leverage%2520the%2520compactness%2520of%2520spherical%2520harmonics%2520to%250Aencode%2520the%2520MASH%2520functions%252C%2520combined%2520with%2520a%2520generalized%2520view%2520cone%2520with%2520a%250Aparameterized%2520base%2520that%2520masks%2520the%2520spatial%2520extent%2520of%2520the%2520spherical%2520function%2520to%250Aattain%2520locality.%2520We%2520develop%2520a%2520differentiable%2520optimization%2520algorithm%2520capable%2520of%250Aconverting%2520any%2520point%2520cloud%2520into%2520a%2520MASH%2520representation%2520accurately%2520approximating%250Aground-truth%2520surfaces%2520with%2520arbitrary%2520geometry%2520and%2520topology.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520MASH%2520is%2520versatile%2520for%2520multiple%2520applications%250Aincluding%2520surface%2520reconstruction%252C%2520shape%2520generation%252C%2520completion%252C%2520and%2520blending%252C%250Aachieving%2520superior%2520performance%2520thanks%2520to%2520its%2520unique%2520representation%2520encompassing%250Aboth%2520implicit%2520and%2520explicit%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09149v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MASH%3A%20Masked%20Anchored%20SpHerical%20Distances%20for%203D%20Shape%20Representation%0A%20%20and%20Generation&entry.906535625=Changhao%20Li%20and%20Yu%20Xin%20and%20Xiaowei%20Zhou%20and%20Ariel%20Shamir%20and%20Hao%20Zhang%20and%20Ligang%20Liu%20and%20Ruizhen%20Hu&entry.1292438233=%20%20We%20introduce%20Masked%20Anchored%20SpHerical%20Distances%20%28MASH%29%2C%20a%20novel%20multi-view%0Aand%20parametrized%20representation%20of%203D%20shapes.%20Inspired%20by%20multi-view%20geometry%0Aand%20motivated%20by%20the%20importance%20of%20perceptual%20shape%20understanding%20for%20learning%0A3D%20shapes%2C%20MASH%20represents%20a%203D%20shape%20as%20a%20collection%20of%20observable%20local%0Asurface%20patches%2C%20each%20defined%20by%20a%20spherical%20distance%20function%20emanating%20from%0Aan%20anchor%20point.%20We%20further%20leverage%20the%20compactness%20of%20spherical%20harmonics%20to%0Aencode%20the%20MASH%20functions%2C%20combined%20with%20a%20generalized%20view%20cone%20with%20a%0Aparameterized%20base%20that%20masks%20the%20spatial%20extent%20of%20the%20spherical%20function%20to%0Aattain%20locality.%20We%20develop%20a%20differentiable%20optimization%20algorithm%20capable%20of%0Aconverting%20any%20point%20cloud%20into%20a%20MASH%20representation%20accurately%20approximating%0Aground-truth%20surfaces%20with%20arbitrary%20geometry%20and%20topology.%20Extensive%0Aexperiments%20demonstrate%20that%20MASH%20is%20versatile%20for%20multiple%20applications%0Aincluding%20surface%20reconstruction%2C%20shape%20generation%2C%20completion%2C%20and%20blending%2C%0Aachieving%20superior%20performance%20thanks%20to%20its%20unique%20representation%20encompassing%0Aboth%20implicit%20and%20explicit%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09149v3&entry.124074799=Read"},
{"title": "Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph\n  Convolution Networks", "author": "Xiaoyan Jiang and Bohan Wang and Xinlong Wan and Shanshan Chen and Hamido Fujita and Hanan Abd. Al Juaid", "abstract": "  Most existing RGB-D semantic segmentation methods focus on the feature level\nfusion, including complex cross-modality and cross-scale fusion modules.\nHowever, these methods may cause misalignment problem in the feature fusion\nprocess and counter-intuitive patches in the segmentation results. Inspired by\nthe popular pixel-node-pixel pipeline, we propose to 1) fuse features from two\nmodalities in a late fusion style, during which the geometric feature injection\nis guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on\nthe fused feature to alleviate the emergence of irregular patches by inferring\npatch relationship. At the 3D feature extraction stage, we argue that\ntraditional CNNs are not efficient enough for depth maps. So, we encode depth\nmap into normal map, after which CNNs can easily extract object surface\ntendencies.At projection matrix generation stage, we find the existence of\nBiased-Assignment and Ambiguous-Locality issues in the original pipeline.\nTherefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no\nmissing important pixel features, which can be viewed as hard pixel mining\nprocess; 2) connect regions that are close to each other in the Euclidean space\nas well as in the semantic space with larger edge weights so that location\ninformations can been considered. Extensive experiments on two public datasets,\nNYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost\nthe performance of RGB-D semantic segmentation task.\n", "link": "http://arxiv.org/abs/2501.18851v3", "date": "2025-05-02", "relevancy": 2.7795, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5877}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5437}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Project-and-Fuse%3A%20Improving%20RGB-D%20Semantic%20Segmentation%20via%20Graph%0A%20%20Convolution%20Networks&body=Title%3A%20Project-and-Fuse%3A%20Improving%20RGB-D%20Semantic%20Segmentation%20via%20Graph%0A%20%20Convolution%20Networks%0AAuthor%3A%20Xiaoyan%20Jiang%20and%20Bohan%20Wang%20and%20Xinlong%20Wan%20and%20Shanshan%20Chen%20and%20Hamido%20Fujita%20and%20Hanan%20Abd.%20Al%20Juaid%0AAbstract%3A%20%20%20Most%20existing%20RGB-D%20semantic%20segmentation%20methods%20focus%20on%20the%20feature%20level%0Afusion%2C%20including%20complex%20cross-modality%20and%20cross-scale%20fusion%20modules.%0AHowever%2C%20these%20methods%20may%20cause%20misalignment%20problem%20in%20the%20feature%20fusion%0Aprocess%20and%20counter-intuitive%20patches%20in%20the%20segmentation%20results.%20Inspired%20by%0Athe%20popular%20pixel-node-pixel%20pipeline%2C%20we%20propose%20to%201%29%20fuse%20features%20from%20two%0Amodalities%20in%20a%20late%20fusion%20style%2C%20during%20which%20the%20geometric%20feature%20injection%0Ais%20guided%20by%20texture%20feature%20prior%3B%202%29%20employ%20Graph%20Neural%20Networks%20%28GNNs%29%20on%0Athe%20fused%20feature%20to%20alleviate%20the%20emergence%20of%20irregular%20patches%20by%20inferring%0Apatch%20relationship.%20At%20the%203D%20feature%20extraction%20stage%2C%20we%20argue%20that%0Atraditional%20CNNs%20are%20not%20efficient%20enough%20for%20depth%20maps.%20So%2C%20we%20encode%20depth%0Amap%20into%20normal%20map%2C%20after%20which%20CNNs%20can%20easily%20extract%20object%20surface%0Atendencies.At%20projection%20matrix%20generation%20stage%2C%20we%20find%20the%20existence%20of%0ABiased-Assignment%20and%20Ambiguous-Locality%20issues%20in%20the%20original%20pipeline.%0ATherefore%2C%20we%20propose%20to%201%29%20adopt%20the%20Kullback-Leibler%20Loss%20to%20ensure%20no%0Amissing%20important%20pixel%20features%2C%20which%20can%20be%20viewed%20as%20hard%20pixel%20mining%0Aprocess%3B%202%29%20connect%20regions%20that%20are%20close%20to%20each%20other%20in%20the%20Euclidean%20space%0Aas%20well%20as%20in%20the%20semantic%20space%20with%20larger%20edge%20weights%20so%20that%20location%0Ainformations%20can%20been%20considered.%20Extensive%20experiments%20on%20two%20public%20datasets%2C%0ANYU-DepthV2%20and%20SUN%20RGB-D%2C%20have%20shown%20that%20our%20approach%20can%20consistently%20boost%0Athe%20performance%20of%20RGB-D%20semantic%20segmentation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18851v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProject-and-Fuse%253A%2520Improving%2520RGB-D%2520Semantic%2520Segmentation%2520via%2520Graph%250A%2520%2520Convolution%2520Networks%26entry.906535625%3DXiaoyan%2520Jiang%2520and%2520Bohan%2520Wang%2520and%2520Xinlong%2520Wan%2520and%2520Shanshan%2520Chen%2520and%2520Hamido%2520Fujita%2520and%2520Hanan%2520Abd.%2520Al%2520Juaid%26entry.1292438233%3D%2520%2520Most%2520existing%2520RGB-D%2520semantic%2520segmentation%2520methods%2520focus%2520on%2520the%2520feature%2520level%250Afusion%252C%2520including%2520complex%2520cross-modality%2520and%2520cross-scale%2520fusion%2520modules.%250AHowever%252C%2520these%2520methods%2520may%2520cause%2520misalignment%2520problem%2520in%2520the%2520feature%2520fusion%250Aprocess%2520and%2520counter-intuitive%2520patches%2520in%2520the%2520segmentation%2520results.%2520Inspired%2520by%250Athe%2520popular%2520pixel-node-pixel%2520pipeline%252C%2520we%2520propose%2520to%25201%2529%2520fuse%2520features%2520from%2520two%250Amodalities%2520in%2520a%2520late%2520fusion%2520style%252C%2520during%2520which%2520the%2520geometric%2520feature%2520injection%250Ais%2520guided%2520by%2520texture%2520feature%2520prior%253B%25202%2529%2520employ%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520on%250Athe%2520fused%2520feature%2520to%2520alleviate%2520the%2520emergence%2520of%2520irregular%2520patches%2520by%2520inferring%250Apatch%2520relationship.%2520At%2520the%25203D%2520feature%2520extraction%2520stage%252C%2520we%2520argue%2520that%250Atraditional%2520CNNs%2520are%2520not%2520efficient%2520enough%2520for%2520depth%2520maps.%2520So%252C%2520we%2520encode%2520depth%250Amap%2520into%2520normal%2520map%252C%2520after%2520which%2520CNNs%2520can%2520easily%2520extract%2520object%2520surface%250Atendencies.At%2520projection%2520matrix%2520generation%2520stage%252C%2520we%2520find%2520the%2520existence%2520of%250ABiased-Assignment%2520and%2520Ambiguous-Locality%2520issues%2520in%2520the%2520original%2520pipeline.%250ATherefore%252C%2520we%2520propose%2520to%25201%2529%2520adopt%2520the%2520Kullback-Leibler%2520Loss%2520to%2520ensure%2520no%250Amissing%2520important%2520pixel%2520features%252C%2520which%2520can%2520be%2520viewed%2520as%2520hard%2520pixel%2520mining%250Aprocess%253B%25202%2529%2520connect%2520regions%2520that%2520are%2520close%2520to%2520each%2520other%2520in%2520the%2520Euclidean%2520space%250Aas%2520well%2520as%2520in%2520the%2520semantic%2520space%2520with%2520larger%2520edge%2520weights%2520so%2520that%2520location%250Ainformations%2520can%2520been%2520considered.%2520Extensive%2520experiments%2520on%2520two%2520public%2520datasets%252C%250ANYU-DepthV2%2520and%2520SUN%2520RGB-D%252C%2520have%2520shown%2520that%2520our%2520approach%2520can%2520consistently%2520boost%250Athe%2520performance%2520of%2520RGB-D%2520semantic%2520segmentation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18851v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Project-and-Fuse%3A%20Improving%20RGB-D%20Semantic%20Segmentation%20via%20Graph%0A%20%20Convolution%20Networks&entry.906535625=Xiaoyan%20Jiang%20and%20Bohan%20Wang%20and%20Xinlong%20Wan%20and%20Shanshan%20Chen%20and%20Hamido%20Fujita%20and%20Hanan%20Abd.%20Al%20Juaid&entry.1292438233=%20%20Most%20existing%20RGB-D%20semantic%20segmentation%20methods%20focus%20on%20the%20feature%20level%0Afusion%2C%20including%20complex%20cross-modality%20and%20cross-scale%20fusion%20modules.%0AHowever%2C%20these%20methods%20may%20cause%20misalignment%20problem%20in%20the%20feature%20fusion%0Aprocess%20and%20counter-intuitive%20patches%20in%20the%20segmentation%20results.%20Inspired%20by%0Athe%20popular%20pixel-node-pixel%20pipeline%2C%20we%20propose%20to%201%29%20fuse%20features%20from%20two%0Amodalities%20in%20a%20late%20fusion%20style%2C%20during%20which%20the%20geometric%20feature%20injection%0Ais%20guided%20by%20texture%20feature%20prior%3B%202%29%20employ%20Graph%20Neural%20Networks%20%28GNNs%29%20on%0Athe%20fused%20feature%20to%20alleviate%20the%20emergence%20of%20irregular%20patches%20by%20inferring%0Apatch%20relationship.%20At%20the%203D%20feature%20extraction%20stage%2C%20we%20argue%20that%0Atraditional%20CNNs%20are%20not%20efficient%20enough%20for%20depth%20maps.%20So%2C%20we%20encode%20depth%0Amap%20into%20normal%20map%2C%20after%20which%20CNNs%20can%20easily%20extract%20object%20surface%0Atendencies.At%20projection%20matrix%20generation%20stage%2C%20we%20find%20the%20existence%20of%0ABiased-Assignment%20and%20Ambiguous-Locality%20issues%20in%20the%20original%20pipeline.%0ATherefore%2C%20we%20propose%20to%201%29%20adopt%20the%20Kullback-Leibler%20Loss%20to%20ensure%20no%0Amissing%20important%20pixel%20features%2C%20which%20can%20be%20viewed%20as%20hard%20pixel%20mining%0Aprocess%3B%202%29%20connect%20regions%20that%20are%20close%20to%20each%20other%20in%20the%20Euclidean%20space%0Aas%20well%20as%20in%20the%20semantic%20space%20with%20larger%20edge%20weights%20so%20that%20location%0Ainformations%20can%20been%20considered.%20Extensive%20experiments%20on%20two%20public%20datasets%2C%0ANYU-DepthV2%20and%20SUN%20RGB-D%2C%20have%20shown%20that%20our%20approach%20can%20consistently%20boost%0Athe%20performance%20of%20RGB-D%20semantic%20segmentation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18851v3&entry.124074799=Read"},
{"title": "T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise\n  Translation Graph", "author": "Qingyu Xian and Weiqin Jiao and Hao Cheng and Berend Jan van der Zwaag and Yanqiu Huang", "abstract": "  Sparse-view camera pose estimation, which aims to estimate the\n6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from\ndifferent viewpoints, is a fundamental yet challenging problem in remote\nsensing applications. Existing methods often overlook the translation\ninformation between each pair of viewpoints, leading to suboptimal performance\nin sparse-view scenarios. To address this limitation, we introduce T-Graph, a\nlightweight, plug-and-play module to enhance camera pose estimation in\nsparse-view settings. T-graph takes paired image features as input and maps\nthem through a Multilayer Perceptron (MLP). It then constructs a fully\nconnected translation graph, where nodes represent cameras and edges encode\ntheir translation relationships. It can be seamlessly integrated into existing\nmodels as an additional branch in parallel with the original prediction,\nmaintaining efficiency and ease of use. Furthermore, we introduce two pairwise\ntranslation representations, relative-t and pair-t, formulated under different\nlocal coordinate systems. While relative-t captures intuitive spatial\nrelationships, pair-t offers a rotation-disentangled alternative. The two\nrepresentations contribute to enhanced adaptability across diverse application\nscenarios, further improving our module's robustness. Extensive experiments on\ntwo state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D\nand IMC PhotoTourism) validate both the effectiveness and generalizability of\nT-Graph. The results demonstrate consistent improvements across various\nmetrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8\nviewpoints.\n", "link": "http://arxiv.org/abs/2505.01207v1", "date": "2025-05-02", "relevancy": 2.7546, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5552}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5531}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-Graph%3A%20Enhancing%20Sparse-view%20Camera%20Pose%20Estimation%20by%20Pairwise%0A%20%20Translation%20Graph&body=Title%3A%20T-Graph%3A%20Enhancing%20Sparse-view%20Camera%20Pose%20Estimation%20by%20Pairwise%0A%20%20Translation%20Graph%0AAuthor%3A%20Qingyu%20Xian%20and%20Weiqin%20Jiao%20and%20Hao%20Cheng%20and%20Berend%20Jan%20van%20der%20Zwaag%20and%20Yanqiu%20Huang%0AAbstract%3A%20%20%20Sparse-view%20camera%20pose%20estimation%2C%20which%20aims%20to%20estimate%20the%0A6-Degree-of-Freedom%20%286-DoF%29%20poses%20from%20a%20limited%20number%20of%20images%20captured%20from%0Adifferent%20viewpoints%2C%20is%20a%20fundamental%20yet%20challenging%20problem%20in%20remote%0Asensing%20applications.%20Existing%20methods%20often%20overlook%20the%20translation%0Ainformation%20between%20each%20pair%20of%20viewpoints%2C%20leading%20to%20suboptimal%20performance%0Ain%20sparse-view%20scenarios.%20To%20address%20this%20limitation%2C%20we%20introduce%20T-Graph%2C%20a%0Alightweight%2C%20plug-and-play%20module%20to%20enhance%20camera%20pose%20estimation%20in%0Asparse-view%20settings.%20T-graph%20takes%20paired%20image%20features%20as%20input%20and%20maps%0Athem%20through%20a%20Multilayer%20Perceptron%20%28MLP%29.%20It%20then%20constructs%20a%20fully%0Aconnected%20translation%20graph%2C%20where%20nodes%20represent%20cameras%20and%20edges%20encode%0Atheir%20translation%20relationships.%20It%20can%20be%20seamlessly%20integrated%20into%20existing%0Amodels%20as%20an%20additional%20branch%20in%20parallel%20with%20the%20original%20prediction%2C%0Amaintaining%20efficiency%20and%20ease%20of%20use.%20Furthermore%2C%20we%20introduce%20two%20pairwise%0Atranslation%20representations%2C%20relative-t%20and%20pair-t%2C%20formulated%20under%20different%0Alocal%20coordinate%20systems.%20While%20relative-t%20captures%20intuitive%20spatial%0Arelationships%2C%20pair-t%20offers%20a%20rotation-disentangled%20alternative.%20The%20two%0Arepresentations%20contribute%20to%20enhanced%20adaptability%20across%20diverse%20application%0Ascenarios%2C%20further%20improving%20our%20module%27s%20robustness.%20Extensive%20experiments%20on%0Atwo%20state-of-the-art%20methods%20%28RelPose%2B%2B%20and%20Forge%29%20using%20public%20datasets%20%28C03D%0Aand%20IMC%20PhotoTourism%29%20validate%20both%20the%20effectiveness%20and%20generalizability%20of%0AT-Graph.%20The%20results%20demonstrate%20consistent%20improvements%20across%20various%0Ametrics%2C%20notably%20camera%20center%20accuracy%2C%20which%20improves%20by%201%25%20to%206%25%20from%202%20to%208%0Aviewpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-Graph%253A%2520Enhancing%2520Sparse-view%2520Camera%2520Pose%2520Estimation%2520by%2520Pairwise%250A%2520%2520Translation%2520Graph%26entry.906535625%3DQingyu%2520Xian%2520and%2520Weiqin%2520Jiao%2520and%2520Hao%2520Cheng%2520and%2520Berend%2520Jan%2520van%2520der%2520Zwaag%2520and%2520Yanqiu%2520Huang%26entry.1292438233%3D%2520%2520Sparse-view%2520camera%2520pose%2520estimation%252C%2520which%2520aims%2520to%2520estimate%2520the%250A6-Degree-of-Freedom%2520%25286-DoF%2529%2520poses%2520from%2520a%2520limited%2520number%2520of%2520images%2520captured%2520from%250Adifferent%2520viewpoints%252C%2520is%2520a%2520fundamental%2520yet%2520challenging%2520problem%2520in%2520remote%250Asensing%2520applications.%2520Existing%2520methods%2520often%2520overlook%2520the%2520translation%250Ainformation%2520between%2520each%2520pair%2520of%2520viewpoints%252C%2520leading%2520to%2520suboptimal%2520performance%250Ain%2520sparse-view%2520scenarios.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520T-Graph%252C%2520a%250Alightweight%252C%2520plug-and-play%2520module%2520to%2520enhance%2520camera%2520pose%2520estimation%2520in%250Asparse-view%2520settings.%2520T-graph%2520takes%2520paired%2520image%2520features%2520as%2520input%2520and%2520maps%250Athem%2520through%2520a%2520Multilayer%2520Perceptron%2520%2528MLP%2529.%2520It%2520then%2520constructs%2520a%2520fully%250Aconnected%2520translation%2520graph%252C%2520where%2520nodes%2520represent%2520cameras%2520and%2520edges%2520encode%250Atheir%2520translation%2520relationships.%2520It%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%250Amodels%2520as%2520an%2520additional%2520branch%2520in%2520parallel%2520with%2520the%2520original%2520prediction%252C%250Amaintaining%2520efficiency%2520and%2520ease%2520of%2520use.%2520Furthermore%252C%2520we%2520introduce%2520two%2520pairwise%250Atranslation%2520representations%252C%2520relative-t%2520and%2520pair-t%252C%2520formulated%2520under%2520different%250Alocal%2520coordinate%2520systems.%2520While%2520relative-t%2520captures%2520intuitive%2520spatial%250Arelationships%252C%2520pair-t%2520offers%2520a%2520rotation-disentangled%2520alternative.%2520The%2520two%250Arepresentations%2520contribute%2520to%2520enhanced%2520adaptability%2520across%2520diverse%2520application%250Ascenarios%252C%2520further%2520improving%2520our%2520module%2527s%2520robustness.%2520Extensive%2520experiments%2520on%250Atwo%2520state-of-the-art%2520methods%2520%2528RelPose%252B%252B%2520and%2520Forge%2529%2520using%2520public%2520datasets%2520%2528C03D%250Aand%2520IMC%2520PhotoTourism%2529%2520validate%2520both%2520the%2520effectiveness%2520and%2520generalizability%2520of%250AT-Graph.%2520The%2520results%2520demonstrate%2520consistent%2520improvements%2520across%2520various%250Ametrics%252C%2520notably%2520camera%2520center%2520accuracy%252C%2520which%2520improves%2520by%25201%2525%2520to%25206%2525%2520from%25202%2520to%25208%250Aviewpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-Graph%3A%20Enhancing%20Sparse-view%20Camera%20Pose%20Estimation%20by%20Pairwise%0A%20%20Translation%20Graph&entry.906535625=Qingyu%20Xian%20and%20Weiqin%20Jiao%20and%20Hao%20Cheng%20and%20Berend%20Jan%20van%20der%20Zwaag%20and%20Yanqiu%20Huang&entry.1292438233=%20%20Sparse-view%20camera%20pose%20estimation%2C%20which%20aims%20to%20estimate%20the%0A6-Degree-of-Freedom%20%286-DoF%29%20poses%20from%20a%20limited%20number%20of%20images%20captured%20from%0Adifferent%20viewpoints%2C%20is%20a%20fundamental%20yet%20challenging%20problem%20in%20remote%0Asensing%20applications.%20Existing%20methods%20often%20overlook%20the%20translation%0Ainformation%20between%20each%20pair%20of%20viewpoints%2C%20leading%20to%20suboptimal%20performance%0Ain%20sparse-view%20scenarios.%20To%20address%20this%20limitation%2C%20we%20introduce%20T-Graph%2C%20a%0Alightweight%2C%20plug-and-play%20module%20to%20enhance%20camera%20pose%20estimation%20in%0Asparse-view%20settings.%20T-graph%20takes%20paired%20image%20features%20as%20input%20and%20maps%0Athem%20through%20a%20Multilayer%20Perceptron%20%28MLP%29.%20It%20then%20constructs%20a%20fully%0Aconnected%20translation%20graph%2C%20where%20nodes%20represent%20cameras%20and%20edges%20encode%0Atheir%20translation%20relationships.%20It%20can%20be%20seamlessly%20integrated%20into%20existing%0Amodels%20as%20an%20additional%20branch%20in%20parallel%20with%20the%20original%20prediction%2C%0Amaintaining%20efficiency%20and%20ease%20of%20use.%20Furthermore%2C%20we%20introduce%20two%20pairwise%0Atranslation%20representations%2C%20relative-t%20and%20pair-t%2C%20formulated%20under%20different%0Alocal%20coordinate%20systems.%20While%20relative-t%20captures%20intuitive%20spatial%0Arelationships%2C%20pair-t%20offers%20a%20rotation-disentangled%20alternative.%20The%20two%0Arepresentations%20contribute%20to%20enhanced%20adaptability%20across%20diverse%20application%0Ascenarios%2C%20further%20improving%20our%20module%27s%20robustness.%20Extensive%20experiments%20on%0Atwo%20state-of-the-art%20methods%20%28RelPose%2B%2B%20and%20Forge%29%20using%20public%20datasets%20%28C03D%0Aand%20IMC%20PhotoTourism%29%20validate%20both%20the%20effectiveness%20and%20generalizability%20of%0AT-Graph.%20The%20results%20demonstrate%20consistent%20improvements%20across%20various%0Ametrics%2C%20notably%20camera%20center%20accuracy%2C%20which%20improves%20by%201%25%20to%206%25%20from%202%20to%208%0Aviewpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01207v1&entry.124074799=Read"},
{"title": "CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via\n  Fine-Grained Alignment", "author": "Edson Araujo and Andrew Rouditchenko and Yuan Gong and Saurabhchand Bhati and Samuel Thomas and Brian Kingsbury and Leonid Karlinsky and Rogerio Feris and James R. Glass", "abstract": "  Recent advances in audio-visual learning have shown promising results in\nlearning representations across modalities. However, most approaches rely on\nglobal audio representations that fail to capture fine-grained temporal\ncorrespondences with visual frames. Additionally, existing methods often\nstruggle with conflicting optimization objectives when trying to jointly learn\nreconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync\nas a simple yet effective extension of the original CAV-MAE framework for\nself-supervised audio-visual learning. We address three key challenges: First,\nwe tackle the granularity mismatch between modalities by treating audio as a\ntemporal sequence aligned with video frames, rather than using global\nrepresentations. Second, we resolve conflicting optimization goals by\nseparating contrastive and reconstruction objectives through dedicated global\ntokens. Third, we improve spatial localization by introducing learnable\nregister tokens that reduce semantic load on patch tokens. We evaluate the\nproposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on\nzero-shot retrieval, classification and localization tasks demonstrating\nstate-of-the-art performance and outperforming more complex architectures.\n", "link": "http://arxiv.org/abs/2505.01237v1", "date": "2025-05-02", "relevancy": 2.7326, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5517}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5488}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAV-MAE%20Sync%3A%20Improving%20Contrastive%20Audio-Visual%20Mask%20Autoencoders%20via%0A%20%20Fine-Grained%20Alignment&body=Title%3A%20CAV-MAE%20Sync%3A%20Improving%20Contrastive%20Audio-Visual%20Mask%20Autoencoders%20via%0A%20%20Fine-Grained%20Alignment%0AAuthor%3A%20Edson%20Araujo%20and%20Andrew%20Rouditchenko%20and%20Yuan%20Gong%20and%20Saurabhchand%20Bhati%20and%20Samuel%20Thomas%20and%20Brian%20Kingsbury%20and%20Leonid%20Karlinsky%20and%20Rogerio%20Feris%20and%20James%20R.%20Glass%0AAbstract%3A%20%20%20Recent%20advances%20in%20audio-visual%20learning%20have%20shown%20promising%20results%20in%0Alearning%20representations%20across%20modalities.%20However%2C%20most%20approaches%20rely%20on%0Aglobal%20audio%20representations%20that%20fail%20to%20capture%20fine-grained%20temporal%0Acorrespondences%20with%20visual%20frames.%20Additionally%2C%20existing%20methods%20often%0Astruggle%20with%20conflicting%20optimization%20objectives%20when%20trying%20to%20jointly%20learn%0Areconstruction%20and%20cross-modal%20alignment.%20In%20this%20work%2C%20we%20propose%20CAV-MAE%20Sync%0Aas%20a%20simple%20yet%20effective%20extension%20of%20the%20original%20CAV-MAE%20framework%20for%0Aself-supervised%20audio-visual%20learning.%20We%20address%20three%20key%20challenges%3A%20First%2C%0Awe%20tackle%20the%20granularity%20mismatch%20between%20modalities%20by%20treating%20audio%20as%20a%0Atemporal%20sequence%20aligned%20with%20video%20frames%2C%20rather%20than%20using%20global%0Arepresentations.%20Second%2C%20we%20resolve%20conflicting%20optimization%20goals%20by%0Aseparating%20contrastive%20and%20reconstruction%20objectives%20through%20dedicated%20global%0Atokens.%20Third%2C%20we%20improve%20spatial%20localization%20by%20introducing%20learnable%0Aregister%20tokens%20that%20reduce%20semantic%20load%20on%20patch%20tokens.%20We%20evaluate%20the%0Aproposed%20approach%20on%20AudioSet%2C%20VGG%20Sound%2C%20and%20the%20ADE20K%20Sound%20dataset%20on%0Azero-shot%20retrieval%2C%20classification%20and%20localization%20tasks%20demonstrating%0Astate-of-the-art%20performance%20and%20outperforming%20more%20complex%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAV-MAE%2520Sync%253A%2520Improving%2520Contrastive%2520Audio-Visual%2520Mask%2520Autoencoders%2520via%250A%2520%2520Fine-Grained%2520Alignment%26entry.906535625%3DEdson%2520Araujo%2520and%2520Andrew%2520Rouditchenko%2520and%2520Yuan%2520Gong%2520and%2520Saurabhchand%2520Bhati%2520and%2520Samuel%2520Thomas%2520and%2520Brian%2520Kingsbury%2520and%2520Leonid%2520Karlinsky%2520and%2520Rogerio%2520Feris%2520and%2520James%2520R.%2520Glass%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520audio-visual%2520learning%2520have%2520shown%2520promising%2520results%2520in%250Alearning%2520representations%2520across%2520modalities.%2520However%252C%2520most%2520approaches%2520rely%2520on%250Aglobal%2520audio%2520representations%2520that%2520fail%2520to%2520capture%2520fine-grained%2520temporal%250Acorrespondences%2520with%2520visual%2520frames.%2520Additionally%252C%2520existing%2520methods%2520often%250Astruggle%2520with%2520conflicting%2520optimization%2520objectives%2520when%2520trying%2520to%2520jointly%2520learn%250Areconstruction%2520and%2520cross-modal%2520alignment.%2520In%2520this%2520work%252C%2520we%2520propose%2520CAV-MAE%2520Sync%250Aas%2520a%2520simple%2520yet%2520effective%2520extension%2520of%2520the%2520original%2520CAV-MAE%2520framework%2520for%250Aself-supervised%2520audio-visual%2520learning.%2520We%2520address%2520three%2520key%2520challenges%253A%2520First%252C%250Awe%2520tackle%2520the%2520granularity%2520mismatch%2520between%2520modalities%2520by%2520treating%2520audio%2520as%2520a%250Atemporal%2520sequence%2520aligned%2520with%2520video%2520frames%252C%2520rather%2520than%2520using%2520global%250Arepresentations.%2520Second%252C%2520we%2520resolve%2520conflicting%2520optimization%2520goals%2520by%250Aseparating%2520contrastive%2520and%2520reconstruction%2520objectives%2520through%2520dedicated%2520global%250Atokens.%2520Third%252C%2520we%2520improve%2520spatial%2520localization%2520by%2520introducing%2520learnable%250Aregister%2520tokens%2520that%2520reduce%2520semantic%2520load%2520on%2520patch%2520tokens.%2520We%2520evaluate%2520the%250Aproposed%2520approach%2520on%2520AudioSet%252C%2520VGG%2520Sound%252C%2520and%2520the%2520ADE20K%2520Sound%2520dataset%2520on%250Azero-shot%2520retrieval%252C%2520classification%2520and%2520localization%2520tasks%2520demonstrating%250Astate-of-the-art%2520performance%2520and%2520outperforming%2520more%2520complex%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAV-MAE%20Sync%3A%20Improving%20Contrastive%20Audio-Visual%20Mask%20Autoencoders%20via%0A%20%20Fine-Grained%20Alignment&entry.906535625=Edson%20Araujo%20and%20Andrew%20Rouditchenko%20and%20Yuan%20Gong%20and%20Saurabhchand%20Bhati%20and%20Samuel%20Thomas%20and%20Brian%20Kingsbury%20and%20Leonid%20Karlinsky%20and%20Rogerio%20Feris%20and%20James%20R.%20Glass&entry.1292438233=%20%20Recent%20advances%20in%20audio-visual%20learning%20have%20shown%20promising%20results%20in%0Alearning%20representations%20across%20modalities.%20However%2C%20most%20approaches%20rely%20on%0Aglobal%20audio%20representations%20that%20fail%20to%20capture%20fine-grained%20temporal%0Acorrespondences%20with%20visual%20frames.%20Additionally%2C%20existing%20methods%20often%0Astruggle%20with%20conflicting%20optimization%20objectives%20when%20trying%20to%20jointly%20learn%0Areconstruction%20and%20cross-modal%20alignment.%20In%20this%20work%2C%20we%20propose%20CAV-MAE%20Sync%0Aas%20a%20simple%20yet%20effective%20extension%20of%20the%20original%20CAV-MAE%20framework%20for%0Aself-supervised%20audio-visual%20learning.%20We%20address%20three%20key%20challenges%3A%20First%2C%0Awe%20tackle%20the%20granularity%20mismatch%20between%20modalities%20by%20treating%20audio%20as%20a%0Atemporal%20sequence%20aligned%20with%20video%20frames%2C%20rather%20than%20using%20global%0Arepresentations.%20Second%2C%20we%20resolve%20conflicting%20optimization%20goals%20by%0Aseparating%20contrastive%20and%20reconstruction%20objectives%20through%20dedicated%20global%0Atokens.%20Third%2C%20we%20improve%20spatial%20localization%20by%20introducing%20learnable%0Aregister%20tokens%20that%20reduce%20semantic%20load%20on%20patch%20tokens.%20We%20evaluate%20the%0Aproposed%20approach%20on%20AudioSet%2C%20VGG%20Sound%2C%20and%20the%20ADE20K%20Sound%20dataset%20on%0Azero-shot%20retrieval%2C%20classification%20and%20localization%20tasks%20demonstrating%0Astate-of-the-art%20performance%20and%20outperforming%20more%20complex%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01237v1&entry.124074799=Read"},
{"title": "Negative Stepsizes Make Gradient-Descent-Ascent Converge", "author": "Henry Shugart and Jason M. Altschuler", "abstract": "  Efficient computation of min-max problems is a central question in\noptimization, learning, games, and controls. Arguably the most natural\nalgorithm is gradient-descent-ascent (GDA). However, since the 1970s,\nconventional wisdom has argued that GDA fails to converge even on simple\nproblems. This failure spurred an extensive literature on modifying GDA with\nadditional building blocks such as extragradients, optimism, momentum,\nanchoring, etc. In contrast, we show that GDA converges in its original form by\nsimply using a judicious choice of stepsizes.\n  The key innovation is the proposal of unconventional stepsize schedules\n(dubbed slingshot stepsize schedules) that are time-varying, asymmetric, and\nperiodically negative. We show that all three properties are necessary for\nconvergence, and that altogether this enables GDA to converge on the classical\ncounterexamples (e.g., unconstrained convex-concave problems). All of our\nresults apply to the last iterate of GDA, as is typically desired in practice.\n  The core algorithmic intuition is that although negative stepsizes make\nbackward progress, they de-synchronize the min and max variables (overcoming\nthe cycling issue of GDA), and lead to a slingshot phenomenon in which the\nforward progress in the other iterations is overwhelmingly larger. This results\nin fast overall convergence. Geometrically, the slingshot dynamics leverage the\nnon-reversibility of gradient flow: positive/negative steps cancel to first\norder, yielding a second-order net movement in a new direction that leads to\nconvergence and is otherwise impossible for GDA to move in. We interpret this\nas a second-order finite-differencing algorithm and show that, intriguingly, it\napproximately implements consensus optimization, an empirically popular\nalgorithm for min-max problems involving deep neural networks (e.g., training\nGANs).\n", "link": "http://arxiv.org/abs/2505.01423v1", "date": "2025-05-02", "relevancy": 2.5292, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5138}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.509}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Negative%20Stepsizes%20Make%20Gradient-Descent-Ascent%20Converge&body=Title%3A%20Negative%20Stepsizes%20Make%20Gradient-Descent-Ascent%20Converge%0AAuthor%3A%20Henry%20Shugart%20and%20Jason%20M.%20Altschuler%0AAbstract%3A%20%20%20Efficient%20computation%20of%20min-max%20problems%20is%20a%20central%20question%20in%0Aoptimization%2C%20learning%2C%20games%2C%20and%20controls.%20Arguably%20the%20most%20natural%0Aalgorithm%20is%20gradient-descent-ascent%20%28GDA%29.%20However%2C%20since%20the%201970s%2C%0Aconventional%20wisdom%20has%20argued%20that%20GDA%20fails%20to%20converge%20even%20on%20simple%0Aproblems.%20This%20failure%20spurred%20an%20extensive%20literature%20on%20modifying%20GDA%20with%0Aadditional%20building%20blocks%20such%20as%20extragradients%2C%20optimism%2C%20momentum%2C%0Aanchoring%2C%20etc.%20In%20contrast%2C%20we%20show%20that%20GDA%20converges%20in%20its%20original%20form%20by%0Asimply%20using%20a%20judicious%20choice%20of%20stepsizes.%0A%20%20The%20key%20innovation%20is%20the%20proposal%20of%20unconventional%20stepsize%20schedules%0A%28dubbed%20slingshot%20stepsize%20schedules%29%20that%20are%20time-varying%2C%20asymmetric%2C%20and%0Aperiodically%20negative.%20We%20show%20that%20all%20three%20properties%20are%20necessary%20for%0Aconvergence%2C%20and%20that%20altogether%20this%20enables%20GDA%20to%20converge%20on%20the%20classical%0Acounterexamples%20%28e.g.%2C%20unconstrained%20convex-concave%20problems%29.%20All%20of%20our%0Aresults%20apply%20to%20the%20last%20iterate%20of%20GDA%2C%20as%20is%20typically%20desired%20in%20practice.%0A%20%20The%20core%20algorithmic%20intuition%20is%20that%20although%20negative%20stepsizes%20make%0Abackward%20progress%2C%20they%20de-synchronize%20the%20min%20and%20max%20variables%20%28overcoming%0Athe%20cycling%20issue%20of%20GDA%29%2C%20and%20lead%20to%20a%20slingshot%20phenomenon%20in%20which%20the%0Aforward%20progress%20in%20the%20other%20iterations%20is%20overwhelmingly%20larger.%20This%20results%0Ain%20fast%20overall%20convergence.%20Geometrically%2C%20the%20slingshot%20dynamics%20leverage%20the%0Anon-reversibility%20of%20gradient%20flow%3A%20positive/negative%20steps%20cancel%20to%20first%0Aorder%2C%20yielding%20a%20second-order%20net%20movement%20in%20a%20new%20direction%20that%20leads%20to%0Aconvergence%20and%20is%20otherwise%20impossible%20for%20GDA%20to%20move%20in.%20We%20interpret%20this%0Aas%20a%20second-order%20finite-differencing%20algorithm%20and%20show%20that%2C%20intriguingly%2C%20it%0Aapproximately%20implements%20consensus%20optimization%2C%20an%20empirically%20popular%0Aalgorithm%20for%20min-max%20problems%20involving%20deep%20neural%20networks%20%28e.g.%2C%20training%0AGANs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNegative%2520Stepsizes%2520Make%2520Gradient-Descent-Ascent%2520Converge%26entry.906535625%3DHenry%2520Shugart%2520and%2520Jason%2520M.%2520Altschuler%26entry.1292438233%3D%2520%2520Efficient%2520computation%2520of%2520min-max%2520problems%2520is%2520a%2520central%2520question%2520in%250Aoptimization%252C%2520learning%252C%2520games%252C%2520and%2520controls.%2520Arguably%2520the%2520most%2520natural%250Aalgorithm%2520is%2520gradient-descent-ascent%2520%2528GDA%2529.%2520However%252C%2520since%2520the%25201970s%252C%250Aconventional%2520wisdom%2520has%2520argued%2520that%2520GDA%2520fails%2520to%2520converge%2520even%2520on%2520simple%250Aproblems.%2520This%2520failure%2520spurred%2520an%2520extensive%2520literature%2520on%2520modifying%2520GDA%2520with%250Aadditional%2520building%2520blocks%2520such%2520as%2520extragradients%252C%2520optimism%252C%2520momentum%252C%250Aanchoring%252C%2520etc.%2520In%2520contrast%252C%2520we%2520show%2520that%2520GDA%2520converges%2520in%2520its%2520original%2520form%2520by%250Asimply%2520using%2520a%2520judicious%2520choice%2520of%2520stepsizes.%250A%2520%2520The%2520key%2520innovation%2520is%2520the%2520proposal%2520of%2520unconventional%2520stepsize%2520schedules%250A%2528dubbed%2520slingshot%2520stepsize%2520schedules%2529%2520that%2520are%2520time-varying%252C%2520asymmetric%252C%2520and%250Aperiodically%2520negative.%2520We%2520show%2520that%2520all%2520three%2520properties%2520are%2520necessary%2520for%250Aconvergence%252C%2520and%2520that%2520altogether%2520this%2520enables%2520GDA%2520to%2520converge%2520on%2520the%2520classical%250Acounterexamples%2520%2528e.g.%252C%2520unconstrained%2520convex-concave%2520problems%2529.%2520All%2520of%2520our%250Aresults%2520apply%2520to%2520the%2520last%2520iterate%2520of%2520GDA%252C%2520as%2520is%2520typically%2520desired%2520in%2520practice.%250A%2520%2520The%2520core%2520algorithmic%2520intuition%2520is%2520that%2520although%2520negative%2520stepsizes%2520make%250Abackward%2520progress%252C%2520they%2520de-synchronize%2520the%2520min%2520and%2520max%2520variables%2520%2528overcoming%250Athe%2520cycling%2520issue%2520of%2520GDA%2529%252C%2520and%2520lead%2520to%2520a%2520slingshot%2520phenomenon%2520in%2520which%2520the%250Aforward%2520progress%2520in%2520the%2520other%2520iterations%2520is%2520overwhelmingly%2520larger.%2520This%2520results%250Ain%2520fast%2520overall%2520convergence.%2520Geometrically%252C%2520the%2520slingshot%2520dynamics%2520leverage%2520the%250Anon-reversibility%2520of%2520gradient%2520flow%253A%2520positive/negative%2520steps%2520cancel%2520to%2520first%250Aorder%252C%2520yielding%2520a%2520second-order%2520net%2520movement%2520in%2520a%2520new%2520direction%2520that%2520leads%2520to%250Aconvergence%2520and%2520is%2520otherwise%2520impossible%2520for%2520GDA%2520to%2520move%2520in.%2520We%2520interpret%2520this%250Aas%2520a%2520second-order%2520finite-differencing%2520algorithm%2520and%2520show%2520that%252C%2520intriguingly%252C%2520it%250Aapproximately%2520implements%2520consensus%2520optimization%252C%2520an%2520empirically%2520popular%250Aalgorithm%2520for%2520min-max%2520problems%2520involving%2520deep%2520neural%2520networks%2520%2528e.g.%252C%2520training%250AGANs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Negative%20Stepsizes%20Make%20Gradient-Descent-Ascent%20Converge&entry.906535625=Henry%20Shugart%20and%20Jason%20M.%20Altschuler&entry.1292438233=%20%20Efficient%20computation%20of%20min-max%20problems%20is%20a%20central%20question%20in%0Aoptimization%2C%20learning%2C%20games%2C%20and%20controls.%20Arguably%20the%20most%20natural%0Aalgorithm%20is%20gradient-descent-ascent%20%28GDA%29.%20However%2C%20since%20the%201970s%2C%0Aconventional%20wisdom%20has%20argued%20that%20GDA%20fails%20to%20converge%20even%20on%20simple%0Aproblems.%20This%20failure%20spurred%20an%20extensive%20literature%20on%20modifying%20GDA%20with%0Aadditional%20building%20blocks%20such%20as%20extragradients%2C%20optimism%2C%20momentum%2C%0Aanchoring%2C%20etc.%20In%20contrast%2C%20we%20show%20that%20GDA%20converges%20in%20its%20original%20form%20by%0Asimply%20using%20a%20judicious%20choice%20of%20stepsizes.%0A%20%20The%20key%20innovation%20is%20the%20proposal%20of%20unconventional%20stepsize%20schedules%0A%28dubbed%20slingshot%20stepsize%20schedules%29%20that%20are%20time-varying%2C%20asymmetric%2C%20and%0Aperiodically%20negative.%20We%20show%20that%20all%20three%20properties%20are%20necessary%20for%0Aconvergence%2C%20and%20that%20altogether%20this%20enables%20GDA%20to%20converge%20on%20the%20classical%0Acounterexamples%20%28e.g.%2C%20unconstrained%20convex-concave%20problems%29.%20All%20of%20our%0Aresults%20apply%20to%20the%20last%20iterate%20of%20GDA%2C%20as%20is%20typically%20desired%20in%20practice.%0A%20%20The%20core%20algorithmic%20intuition%20is%20that%20although%20negative%20stepsizes%20make%0Abackward%20progress%2C%20they%20de-synchronize%20the%20min%20and%20max%20variables%20%28overcoming%0Athe%20cycling%20issue%20of%20GDA%29%2C%20and%20lead%20to%20a%20slingshot%20phenomenon%20in%20which%20the%0Aforward%20progress%20in%20the%20other%20iterations%20is%20overwhelmingly%20larger.%20This%20results%0Ain%20fast%20overall%20convergence.%20Geometrically%2C%20the%20slingshot%20dynamics%20leverage%20the%0Anon-reversibility%20of%20gradient%20flow%3A%20positive/negative%20steps%20cancel%20to%20first%0Aorder%2C%20yielding%20a%20second-order%20net%20movement%20in%20a%20new%20direction%20that%20leads%20to%0Aconvergence%20and%20is%20otherwise%20impossible%20for%20GDA%20to%20move%20in.%20We%20interpret%20this%0Aas%20a%20second-order%20finite-differencing%20algorithm%20and%20show%20that%2C%20intriguingly%2C%20it%0Aapproximately%20implements%20consensus%20optimization%2C%20an%20empirically%20popular%0Aalgorithm%20for%20min-max%20problems%20involving%20deep%20neural%20networks%20%28e.g.%2C%20training%0AGANs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01423v1&entry.124074799=Read"},
{"title": "Improving Continual Learning Performance and Efficiency with Auxiliary\n  Classifiers", "author": "Filip Szatkowski and Yaoyue Zheng and Fei Yang and Bart\u0142omiej Twardowski and Tomasz Trzci\u0144ski and Joost van de Weijer", "abstract": "  Continual learning is crucial for applying machine learning in challenging,\ndynamic, and often resource-constrained environments. However, catastrophic\nforgetting - overwriting previously learned knowledge when new information is\nacquired - remains a major challenge. In this work, we examine the intermediate\nrepresentations in neural network layers during continual learning and find\nthat such representations are less prone to forgetting, highlighting their\npotential to accelerate computation. Motivated by these findings, we propose to\nuse auxiliary classifiers(ACs) to enhance performance and demonstrate that\nintegrating ACs into various continual learning methods consistently improves\naccuracy across diverse evaluation settings, yielding an average 10% relative\ngain. We also leverage the ACs to reduce the average cost of the inference by\n10-60% without compromising accuracy, enabling the model to return the\npredictions before computing all the layers. Our approach provides a scalable\nand efficient solution for continual learning.\n", "link": "http://arxiv.org/abs/2403.07404v3", "date": "2025-05-02", "relevancy": 2.519, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Continual%20Learning%20Performance%20and%20Efficiency%20with%20Auxiliary%0A%20%20Classifiers&body=Title%3A%20Improving%20Continual%20Learning%20Performance%20and%20Efficiency%20with%20Auxiliary%0A%20%20Classifiers%0AAuthor%3A%20Filip%20Szatkowski%20and%20Yaoyue%20Zheng%20and%20Fei%20Yang%20and%20Bart%C5%82omiej%20Twardowski%20and%20Tomasz%20Trzci%C5%84ski%20and%20Joost%20van%20de%20Weijer%0AAbstract%3A%20%20%20Continual%20learning%20is%20crucial%20for%20applying%20machine%20learning%20in%20challenging%2C%0Adynamic%2C%20and%20often%20resource-constrained%20environments.%20However%2C%20catastrophic%0Aforgetting%20-%20overwriting%20previously%20learned%20knowledge%20when%20new%20information%20is%0Aacquired%20-%20remains%20a%20major%20challenge.%20In%20this%20work%2C%20we%20examine%20the%20intermediate%0Arepresentations%20in%20neural%20network%20layers%20during%20continual%20learning%20and%20find%0Athat%20such%20representations%20are%20less%20prone%20to%20forgetting%2C%20highlighting%20their%0Apotential%20to%20accelerate%20computation.%20Motivated%20by%20these%20findings%2C%20we%20propose%20to%0Ause%20auxiliary%20classifiers%28ACs%29%20to%20enhance%20performance%20and%20demonstrate%20that%0Aintegrating%20ACs%20into%20various%20continual%20learning%20methods%20consistently%20improves%0Aaccuracy%20across%20diverse%20evaluation%20settings%2C%20yielding%20an%20average%2010%25%20relative%0Again.%20We%20also%20leverage%20the%20ACs%20to%20reduce%20the%20average%20cost%20of%20the%20inference%20by%0A10-60%25%20without%20compromising%20accuracy%2C%20enabling%20the%20model%20to%20return%20the%0Apredictions%20before%20computing%20all%20the%20layers.%20Our%20approach%20provides%20a%20scalable%0Aand%20efficient%20solution%20for%20continual%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07404v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Continual%2520Learning%2520Performance%2520and%2520Efficiency%2520with%2520Auxiliary%250A%2520%2520Classifiers%26entry.906535625%3DFilip%2520Szatkowski%2520and%2520Yaoyue%2520Zheng%2520and%2520Fei%2520Yang%2520and%2520Bart%25C5%2582omiej%2520Twardowski%2520and%2520Tomasz%2520Trzci%25C5%2584ski%2520and%2520Joost%2520van%2520de%2520Weijer%26entry.1292438233%3D%2520%2520Continual%2520learning%2520is%2520crucial%2520for%2520applying%2520machine%2520learning%2520in%2520challenging%252C%250Adynamic%252C%2520and%2520often%2520resource-constrained%2520environments.%2520However%252C%2520catastrophic%250Aforgetting%2520-%2520overwriting%2520previously%2520learned%2520knowledge%2520when%2520new%2520information%2520is%250Aacquired%2520-%2520remains%2520a%2520major%2520challenge.%2520In%2520this%2520work%252C%2520we%2520examine%2520the%2520intermediate%250Arepresentations%2520in%2520neural%2520network%2520layers%2520during%2520continual%2520learning%2520and%2520find%250Athat%2520such%2520representations%2520are%2520less%2520prone%2520to%2520forgetting%252C%2520highlighting%2520their%250Apotential%2520to%2520accelerate%2520computation.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520propose%2520to%250Ause%2520auxiliary%2520classifiers%2528ACs%2529%2520to%2520enhance%2520performance%2520and%2520demonstrate%2520that%250Aintegrating%2520ACs%2520into%2520various%2520continual%2520learning%2520methods%2520consistently%2520improves%250Aaccuracy%2520across%2520diverse%2520evaluation%2520settings%252C%2520yielding%2520an%2520average%252010%2525%2520relative%250Again.%2520We%2520also%2520leverage%2520the%2520ACs%2520to%2520reduce%2520the%2520average%2520cost%2520of%2520the%2520inference%2520by%250A10-60%2525%2520without%2520compromising%2520accuracy%252C%2520enabling%2520the%2520model%2520to%2520return%2520the%250Apredictions%2520before%2520computing%2520all%2520the%2520layers.%2520Our%2520approach%2520provides%2520a%2520scalable%250Aand%2520efficient%2520solution%2520for%2520continual%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07404v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Continual%20Learning%20Performance%20and%20Efficiency%20with%20Auxiliary%0A%20%20Classifiers&entry.906535625=Filip%20Szatkowski%20and%20Yaoyue%20Zheng%20and%20Fei%20Yang%20and%20Bart%C5%82omiej%20Twardowski%20and%20Tomasz%20Trzci%C5%84ski%20and%20Joost%20van%20de%20Weijer&entry.1292438233=%20%20Continual%20learning%20is%20crucial%20for%20applying%20machine%20learning%20in%20challenging%2C%0Adynamic%2C%20and%20often%20resource-constrained%20environments.%20However%2C%20catastrophic%0Aforgetting%20-%20overwriting%20previously%20learned%20knowledge%20when%20new%20information%20is%0Aacquired%20-%20remains%20a%20major%20challenge.%20In%20this%20work%2C%20we%20examine%20the%20intermediate%0Arepresentations%20in%20neural%20network%20layers%20during%20continual%20learning%20and%20find%0Athat%20such%20representations%20are%20less%20prone%20to%20forgetting%2C%20highlighting%20their%0Apotential%20to%20accelerate%20computation.%20Motivated%20by%20these%20findings%2C%20we%20propose%20to%0Ause%20auxiliary%20classifiers%28ACs%29%20to%20enhance%20performance%20and%20demonstrate%20that%0Aintegrating%20ACs%20into%20various%20continual%20learning%20methods%20consistently%20improves%0Aaccuracy%20across%20diverse%20evaluation%20settings%2C%20yielding%20an%20average%2010%25%20relative%0Again.%20We%20also%20leverage%20the%20ACs%20to%20reduce%20the%20average%20cost%20of%20the%20inference%20by%0A10-60%25%20without%20compromising%20accuracy%2C%20enabling%20the%20model%20to%20return%20the%0Apredictions%20before%20computing%20all%20the%20layers.%20Our%20approach%20provides%20a%20scalable%0Aand%20efficient%20solution%20for%20continual%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07404v3&entry.124074799=Read"},
{"title": "Core-Set Selection for Data-efficient Land Cover Segmentation", "author": "Keiller Nogueira and Akram Zaytar and Wanli Ma and Ribana Roscher and Ronny H\u00e4nsch and Caleb Robinson and Anthony Ortiz and Simone Nsutezo and Rahul Dodhia and Juan M. Lavista Ferres and Oktay Karaku\u015f and Paul L. Rosin", "abstract": "  The increasing accessibility of remotely sensed data and the potential of\nsuch data to inform large-scale decision-making has driven the development of\ndeep learning models for many Earth Observation tasks. Traditionally, such\nmodels must be trained on large datasets. However, the common assumption that\nbroadly larger datasets lead to better outcomes tends to overlook the\ncomplexities of the data distribution, the potential for introducing biases and\nnoise, and the computational resources required for processing and storing vast\ndatasets. Therefore, effective solutions should consider both the quantity and\nquality of data. In this paper, we propose six novel core-set selection methods\nfor selecting important subsets of samples from remote sensing image\nsegmentation datasets that rely on imagery only, labels only, and a combination\nof each. We benchmark these approaches against a random-selection baseline on\nthree commonly used land cover classification datasets: DFC2022, Vaihingen, and\nPotsdam. In each of the datasets, we demonstrate that training on a subset of\nsamples outperforms the random baseline, and some approaches outperform\ntraining on all available data. This result shows the importance and potential\nof data-centric learning for the remote sensing domain. The code is available\nat https://github.com/keillernogueira/data-centric-rs-classification/.\n", "link": "http://arxiv.org/abs/2505.01225v1", "date": "2025-05-02", "relevancy": 2.493, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Core-Set%20Selection%20for%20Data-efficient%20Land%20Cover%20Segmentation&body=Title%3A%20Core-Set%20Selection%20for%20Data-efficient%20Land%20Cover%20Segmentation%0AAuthor%3A%20Keiller%20Nogueira%20and%20Akram%20Zaytar%20and%20Wanli%20Ma%20and%20Ribana%20Roscher%20and%20Ronny%20H%C3%A4nsch%20and%20Caleb%20Robinson%20and%20Anthony%20Ortiz%20and%20Simone%20Nsutezo%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres%20and%20Oktay%20Karaku%C5%9F%20and%20Paul%20L.%20Rosin%0AAbstract%3A%20%20%20The%20increasing%20accessibility%20of%20remotely%20sensed%20data%20and%20the%20potential%20of%0Asuch%20data%20to%20inform%20large-scale%20decision-making%20has%20driven%20the%20development%20of%0Adeep%20learning%20models%20for%20many%20Earth%20Observation%20tasks.%20Traditionally%2C%20such%0Amodels%20must%20be%20trained%20on%20large%20datasets.%20However%2C%20the%20common%20assumption%20that%0Abroadly%20larger%20datasets%20lead%20to%20better%20outcomes%20tends%20to%20overlook%20the%0Acomplexities%20of%20the%20data%20distribution%2C%20the%20potential%20for%20introducing%20biases%20and%0Anoise%2C%20and%20the%20computational%20resources%20required%20for%20processing%20and%20storing%20vast%0Adatasets.%20Therefore%2C%20effective%20solutions%20should%20consider%20both%20the%20quantity%20and%0Aquality%20of%20data.%20In%20this%20paper%2C%20we%20propose%20six%20novel%20core-set%20selection%20methods%0Afor%20selecting%20important%20subsets%20of%20samples%20from%20remote%20sensing%20image%0Asegmentation%20datasets%20that%20rely%20on%20imagery%20only%2C%20labels%20only%2C%20and%20a%20combination%0Aof%20each.%20We%20benchmark%20these%20approaches%20against%20a%20random-selection%20baseline%20on%0Athree%20commonly%20used%20land%20cover%20classification%20datasets%3A%20DFC2022%2C%20Vaihingen%2C%20and%0APotsdam.%20In%20each%20of%20the%20datasets%2C%20we%20demonstrate%20that%20training%20on%20a%20subset%20of%0Asamples%20outperforms%20the%20random%20baseline%2C%20and%20some%20approaches%20outperform%0Atraining%20on%20all%20available%20data.%20This%20result%20shows%20the%20importance%20and%20potential%0Aof%20data-centric%20learning%20for%20the%20remote%20sensing%20domain.%20The%20code%20is%20available%0Aat%20https%3A//github.com/keillernogueira/data-centric-rs-classification/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCore-Set%2520Selection%2520for%2520Data-efficient%2520Land%2520Cover%2520Segmentation%26entry.906535625%3DKeiller%2520Nogueira%2520and%2520Akram%2520Zaytar%2520and%2520Wanli%2520Ma%2520and%2520Ribana%2520Roscher%2520and%2520Ronny%2520H%25C3%25A4nsch%2520and%2520Caleb%2520Robinson%2520and%2520Anthony%2520Ortiz%2520and%2520Simone%2520Nsutezo%2520and%2520Rahul%2520Dodhia%2520and%2520Juan%2520M.%2520Lavista%2520Ferres%2520and%2520Oktay%2520Karaku%25C5%259F%2520and%2520Paul%2520L.%2520Rosin%26entry.1292438233%3D%2520%2520The%2520increasing%2520accessibility%2520of%2520remotely%2520sensed%2520data%2520and%2520the%2520potential%2520of%250Asuch%2520data%2520to%2520inform%2520large-scale%2520decision-making%2520has%2520driven%2520the%2520development%2520of%250Adeep%2520learning%2520models%2520for%2520many%2520Earth%2520Observation%2520tasks.%2520Traditionally%252C%2520such%250Amodels%2520must%2520be%2520trained%2520on%2520large%2520datasets.%2520However%252C%2520the%2520common%2520assumption%2520that%250Abroadly%2520larger%2520datasets%2520lead%2520to%2520better%2520outcomes%2520tends%2520to%2520overlook%2520the%250Acomplexities%2520of%2520the%2520data%2520distribution%252C%2520the%2520potential%2520for%2520introducing%2520biases%2520and%250Anoise%252C%2520and%2520the%2520computational%2520resources%2520required%2520for%2520processing%2520and%2520storing%2520vast%250Adatasets.%2520Therefore%252C%2520effective%2520solutions%2520should%2520consider%2520both%2520the%2520quantity%2520and%250Aquality%2520of%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520six%2520novel%2520core-set%2520selection%2520methods%250Afor%2520selecting%2520important%2520subsets%2520of%2520samples%2520from%2520remote%2520sensing%2520image%250Asegmentation%2520datasets%2520that%2520rely%2520on%2520imagery%2520only%252C%2520labels%2520only%252C%2520and%2520a%2520combination%250Aof%2520each.%2520We%2520benchmark%2520these%2520approaches%2520against%2520a%2520random-selection%2520baseline%2520on%250Athree%2520commonly%2520used%2520land%2520cover%2520classification%2520datasets%253A%2520DFC2022%252C%2520Vaihingen%252C%2520and%250APotsdam.%2520In%2520each%2520of%2520the%2520datasets%252C%2520we%2520demonstrate%2520that%2520training%2520on%2520a%2520subset%2520of%250Asamples%2520outperforms%2520the%2520random%2520baseline%252C%2520and%2520some%2520approaches%2520outperform%250Atraining%2520on%2520all%2520available%2520data.%2520This%2520result%2520shows%2520the%2520importance%2520and%2520potential%250Aof%2520data-centric%2520learning%2520for%2520the%2520remote%2520sensing%2520domain.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/keillernogueira/data-centric-rs-classification/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Core-Set%20Selection%20for%20Data-efficient%20Land%20Cover%20Segmentation&entry.906535625=Keiller%20Nogueira%20and%20Akram%20Zaytar%20and%20Wanli%20Ma%20and%20Ribana%20Roscher%20and%20Ronny%20H%C3%A4nsch%20and%20Caleb%20Robinson%20and%20Anthony%20Ortiz%20and%20Simone%20Nsutezo%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres%20and%20Oktay%20Karaku%C5%9F%20and%20Paul%20L.%20Rosin&entry.1292438233=%20%20The%20increasing%20accessibility%20of%20remotely%20sensed%20data%20and%20the%20potential%20of%0Asuch%20data%20to%20inform%20large-scale%20decision-making%20has%20driven%20the%20development%20of%0Adeep%20learning%20models%20for%20many%20Earth%20Observation%20tasks.%20Traditionally%2C%20such%0Amodels%20must%20be%20trained%20on%20large%20datasets.%20However%2C%20the%20common%20assumption%20that%0Abroadly%20larger%20datasets%20lead%20to%20better%20outcomes%20tends%20to%20overlook%20the%0Acomplexities%20of%20the%20data%20distribution%2C%20the%20potential%20for%20introducing%20biases%20and%0Anoise%2C%20and%20the%20computational%20resources%20required%20for%20processing%20and%20storing%20vast%0Adatasets.%20Therefore%2C%20effective%20solutions%20should%20consider%20both%20the%20quantity%20and%0Aquality%20of%20data.%20In%20this%20paper%2C%20we%20propose%20six%20novel%20core-set%20selection%20methods%0Afor%20selecting%20important%20subsets%20of%20samples%20from%20remote%20sensing%20image%0Asegmentation%20datasets%20that%20rely%20on%20imagery%20only%2C%20labels%20only%2C%20and%20a%20combination%0Aof%20each.%20We%20benchmark%20these%20approaches%20against%20a%20random-selection%20baseline%20on%0Athree%20commonly%20used%20land%20cover%20classification%20datasets%3A%20DFC2022%2C%20Vaihingen%2C%20and%0APotsdam.%20In%20each%20of%20the%20datasets%2C%20we%20demonstrate%20that%20training%20on%20a%20subset%20of%0Asamples%20outperforms%20the%20random%20baseline%2C%20and%20some%20approaches%20outperform%0Atraining%20on%20all%20available%20data.%20This%20result%20shows%20the%20importance%20and%20potential%0Aof%20data-centric%20learning%20for%20the%20remote%20sensing%20domain.%20The%20code%20is%20available%0Aat%20https%3A//github.com/keillernogueira/data-centric-rs-classification/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01225v1&entry.124074799=Read"},
{"title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System", "author": "Sheikh Samit Muhaimin and Spyridon Mastorakis", "abstract": "  The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.\n", "link": "http://arxiv.org/abs/2505.01315v1", "date": "2025-05-02", "relevancy": 2.4789, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Helping%20Big%20Language%20Models%20Protect%20Themselves%3A%20An%20Enhanced%20Filtering%0A%20%20and%20Summarization%20System&body=Title%3A%20Helping%20Big%20Language%20Models%20Protect%20Themselves%3A%20An%20Enhanced%20Filtering%0A%20%20and%20Summarization%20System%0AAuthor%3A%20Sheikh%20Samit%20Muhaimin%20and%20Spyridon%20Mastorakis%0AAbstract%3A%20%20%20The%20recent%20growth%20in%20the%20use%20of%20Large%20Language%20Models%20has%20made%20them%0Avulnerable%20to%20sophisticated%20adversarial%20assaults%2C%20manipulative%20prompts%2C%20and%0Aencoded%20malicious%20inputs.%20Existing%20countermeasures%20frequently%20necessitate%0Aretraining%20models%2C%20which%20is%20computationally%20costly%20and%20impracticable%20for%0Adeployment.%20Without%20the%20need%20for%20retraining%20or%20fine-tuning%2C%20this%20study%20presents%0Aa%20unique%20defense%20paradigm%20that%20allows%20LLMs%20to%20recognize%2C%20filter%2C%20and%20defend%0Aagainst%20adversarial%20or%20malicious%20inputs%20on%20their%20own.%20There%20are%20two%20main%20parts%0Ato%20the%20suggested%20framework%3A%20%281%29%20A%20prompt%20filtering%20module%20that%20uses%0Asophisticated%20Natural%20Language%20Processing%20%28NLP%29%20techniques%2C%20including%20zero-shot%0Aclassification%2C%20keyword%20analysis%2C%20and%20encoded%20content%20detection%20%28e.g.%20base64%2C%0Ahexadecimal%2C%20URL%20encoding%29%2C%20to%20detect%2C%20decode%2C%20and%20classify%20harmful%20inputs%3B%20and%0A%282%29%20A%20summarization%20module%20that%20processes%20and%20summarizes%20adversarial%20research%0Aliterature%20to%20give%20the%20LLM%20context-aware%20defense%20knowledge.%20This%20approach%0Astrengthens%20LLMs%27%20resistance%20to%20adversarial%20exploitation%20by%20fusing%20text%0Aextraction%2C%20summarization%2C%20and%20harmful%20prompt%20analysis.%20According%20to%0Aexperimental%20results%2C%20this%20integrated%20technique%20has%20a%2098.71%25%20success%20rate%20in%0Aidentifying%20harmful%20patterns%2C%20manipulative%20language%20structures%2C%20and%20encoded%0Aprompts.%20By%20employing%20a%20modest%20amount%20of%20adversarial%20research%20literature%20as%0Acontext%2C%20the%20methodology%20also%20allows%20the%20model%20to%20react%20correctly%20to%20harmful%0Ainputs%20with%20a%20larger%20percentage%20of%20jailbreak%20resistance%20and%20refusal%20rate.%20While%0Amaintaining%20the%20quality%20of%20LLM%20responses%2C%20the%20framework%20dramatically%20increases%0ALLM%27s%20resistance%20to%20hostile%20misuse%2C%20demonstrating%20its%20efficacy%20as%20a%20quick%20and%0Aeasy%20substitute%20for%20time-consuming%2C%20retraining-based%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelping%2520Big%2520Language%2520Models%2520Protect%2520Themselves%253A%2520An%2520Enhanced%2520Filtering%250A%2520%2520and%2520Summarization%2520System%26entry.906535625%3DSheikh%2520Samit%2520Muhaimin%2520and%2520Spyridon%2520Mastorakis%26entry.1292438233%3D%2520%2520The%2520recent%2520growth%2520in%2520the%2520use%2520of%2520Large%2520Language%2520Models%2520has%2520made%2520them%250Avulnerable%2520to%2520sophisticated%2520adversarial%2520assaults%252C%2520manipulative%2520prompts%252C%2520and%250Aencoded%2520malicious%2520inputs.%2520Existing%2520countermeasures%2520frequently%2520necessitate%250Aretraining%2520models%252C%2520which%2520is%2520computationally%2520costly%2520and%2520impracticable%2520for%250Adeployment.%2520Without%2520the%2520need%2520for%2520retraining%2520or%2520fine-tuning%252C%2520this%2520study%2520presents%250Aa%2520unique%2520defense%2520paradigm%2520that%2520allows%2520LLMs%2520to%2520recognize%252C%2520filter%252C%2520and%2520defend%250Aagainst%2520adversarial%2520or%2520malicious%2520inputs%2520on%2520their%2520own.%2520There%2520are%2520two%2520main%2520parts%250Ato%2520the%2520suggested%2520framework%253A%2520%25281%2529%2520A%2520prompt%2520filtering%2520module%2520that%2520uses%250Asophisticated%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520techniques%252C%2520including%2520zero-shot%250Aclassification%252C%2520keyword%2520analysis%252C%2520and%2520encoded%2520content%2520detection%2520%2528e.g.%2520base64%252C%250Ahexadecimal%252C%2520URL%2520encoding%2529%252C%2520to%2520detect%252C%2520decode%252C%2520and%2520classify%2520harmful%2520inputs%253B%2520and%250A%25282%2529%2520A%2520summarization%2520module%2520that%2520processes%2520and%2520summarizes%2520adversarial%2520research%250Aliterature%2520to%2520give%2520the%2520LLM%2520context-aware%2520defense%2520knowledge.%2520This%2520approach%250Astrengthens%2520LLMs%2527%2520resistance%2520to%2520adversarial%2520exploitation%2520by%2520fusing%2520text%250Aextraction%252C%2520summarization%252C%2520and%2520harmful%2520prompt%2520analysis.%2520According%2520to%250Aexperimental%2520results%252C%2520this%2520integrated%2520technique%2520has%2520a%252098.71%2525%2520success%2520rate%2520in%250Aidentifying%2520harmful%2520patterns%252C%2520manipulative%2520language%2520structures%252C%2520and%2520encoded%250Aprompts.%2520By%2520employing%2520a%2520modest%2520amount%2520of%2520adversarial%2520research%2520literature%2520as%250Acontext%252C%2520the%2520methodology%2520also%2520allows%2520the%2520model%2520to%2520react%2520correctly%2520to%2520harmful%250Ainputs%2520with%2520a%2520larger%2520percentage%2520of%2520jailbreak%2520resistance%2520and%2520refusal%2520rate.%2520While%250Amaintaining%2520the%2520quality%2520of%2520LLM%2520responses%252C%2520the%2520framework%2520dramatically%2520increases%250ALLM%2527s%2520resistance%2520to%2520hostile%2520misuse%252C%2520demonstrating%2520its%2520efficacy%2520as%2520a%2520quick%2520and%250Aeasy%2520substitute%2520for%2520time-consuming%252C%2520retraining-based%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Helping%20Big%20Language%20Models%20Protect%20Themselves%3A%20An%20Enhanced%20Filtering%0A%20%20and%20Summarization%20System&entry.906535625=Sheikh%20Samit%20Muhaimin%20and%20Spyridon%20Mastorakis&entry.1292438233=%20%20The%20recent%20growth%20in%20the%20use%20of%20Large%20Language%20Models%20has%20made%20them%0Avulnerable%20to%20sophisticated%20adversarial%20assaults%2C%20manipulative%20prompts%2C%20and%0Aencoded%20malicious%20inputs.%20Existing%20countermeasures%20frequently%20necessitate%0Aretraining%20models%2C%20which%20is%20computationally%20costly%20and%20impracticable%20for%0Adeployment.%20Without%20the%20need%20for%20retraining%20or%20fine-tuning%2C%20this%20study%20presents%0Aa%20unique%20defense%20paradigm%20that%20allows%20LLMs%20to%20recognize%2C%20filter%2C%20and%20defend%0Aagainst%20adversarial%20or%20malicious%20inputs%20on%20their%20own.%20There%20are%20two%20main%20parts%0Ato%20the%20suggested%20framework%3A%20%281%29%20A%20prompt%20filtering%20module%20that%20uses%0Asophisticated%20Natural%20Language%20Processing%20%28NLP%29%20techniques%2C%20including%20zero-shot%0Aclassification%2C%20keyword%20analysis%2C%20and%20encoded%20content%20detection%20%28e.g.%20base64%2C%0Ahexadecimal%2C%20URL%20encoding%29%2C%20to%20detect%2C%20decode%2C%20and%20classify%20harmful%20inputs%3B%20and%0A%282%29%20A%20summarization%20module%20that%20processes%20and%20summarizes%20adversarial%20research%0Aliterature%20to%20give%20the%20LLM%20context-aware%20defense%20knowledge.%20This%20approach%0Astrengthens%20LLMs%27%20resistance%20to%20adversarial%20exploitation%20by%20fusing%20text%0Aextraction%2C%20summarization%2C%20and%20harmful%20prompt%20analysis.%20According%20to%0Aexperimental%20results%2C%20this%20integrated%20technique%20has%20a%2098.71%25%20success%20rate%20in%0Aidentifying%20harmful%20patterns%2C%20manipulative%20language%20structures%2C%20and%20encoded%0Aprompts.%20By%20employing%20a%20modest%20amount%20of%20adversarial%20research%20literature%20as%0Acontext%2C%20the%20methodology%20also%20allows%20the%20model%20to%20react%20correctly%20to%20harmful%0Ainputs%20with%20a%20larger%20percentage%20of%20jailbreak%20resistance%20and%20refusal%20rate.%20While%0Amaintaining%20the%20quality%20of%20LLM%20responses%2C%20the%20framework%20dramatically%20increases%0ALLM%27s%20resistance%20to%20hostile%20misuse%2C%20demonstrating%20its%20efficacy%20as%20a%20quick%20and%0Aeasy%20substitute%20for%20time-consuming%2C%20retraining-based%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01315v1&entry.124074799=Read"},
{"title": "On the Limitations of Steering in Language Model Alignment", "author": "Chebrolu Niranjan and Kokil Jaidka and Gerard Christopher Yeo", "abstract": "  Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models.\n", "link": "http://arxiv.org/abs/2505.01162v1", "date": "2025-05-02", "relevancy": 2.4583, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Limitations%20of%20Steering%20in%20Language%20Model%20Alignment&body=Title%3A%20On%20the%20Limitations%20of%20Steering%20in%20Language%20Model%20Alignment%0AAuthor%3A%20Chebrolu%20Niranjan%20and%20Kokil%20Jaidka%20and%20Gerard%20Christopher%20Yeo%0AAbstract%3A%20%20%20Steering%20vectors%20are%20a%20promising%20approach%20to%20aligning%20language%20model%20behavior%0Aat%20inference%20time.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20to%20assess%20the%0Alimitations%20of%20steering%20vectors%20as%20alignment%20mechanisms.%20Using%20a%20framework%20of%0Atransformer%20hook%20interventions%20and%20antonym-based%20function%20vectors%2C%20we%20evaluate%0Athe%20role%20of%20prompt%20structure%20and%20context%20complexity%20in%20steering%20effectiveness.%0AOur%20findings%20indicate%20that%20steering%20vectors%20are%20promising%20for%20specific%0Aalignment%20tasks%2C%20such%20as%20value%20alignment%2C%20but%20may%20not%20provide%20a%20robust%0Afoundation%20for%20general-purpose%20alignment%20in%20LLMs%2C%20particularly%20in%20complex%0Ascenarios.%20We%20establish%20a%20methodological%20foundation%20for%20future%20investigations%0Ainto%20steering%20capabilities%20of%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Limitations%2520of%2520Steering%2520in%2520Language%2520Model%2520Alignment%26entry.906535625%3DChebrolu%2520Niranjan%2520and%2520Kokil%2520Jaidka%2520and%2520Gerard%2520Christopher%2520Yeo%26entry.1292438233%3D%2520%2520Steering%2520vectors%2520are%2520a%2520promising%2520approach%2520to%2520aligning%2520language%2520model%2520behavior%250Aat%2520inference%2520time.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520framework%2520to%2520assess%2520the%250Alimitations%2520of%2520steering%2520vectors%2520as%2520alignment%2520mechanisms.%2520Using%2520a%2520framework%2520of%250Atransformer%2520hook%2520interventions%2520and%2520antonym-based%2520function%2520vectors%252C%2520we%2520evaluate%250Athe%2520role%2520of%2520prompt%2520structure%2520and%2520context%2520complexity%2520in%2520steering%2520effectiveness.%250AOur%2520findings%2520indicate%2520that%2520steering%2520vectors%2520are%2520promising%2520for%2520specific%250Aalignment%2520tasks%252C%2520such%2520as%2520value%2520alignment%252C%2520but%2520may%2520not%2520provide%2520a%2520robust%250Afoundation%2520for%2520general-purpose%2520alignment%2520in%2520LLMs%252C%2520particularly%2520in%2520complex%250Ascenarios.%2520We%2520establish%2520a%2520methodological%2520foundation%2520for%2520future%2520investigations%250Ainto%2520steering%2520capabilities%2520of%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Limitations%20of%20Steering%20in%20Language%20Model%20Alignment&entry.906535625=Chebrolu%20Niranjan%20and%20Kokil%20Jaidka%20and%20Gerard%20Christopher%20Yeo&entry.1292438233=%20%20Steering%20vectors%20are%20a%20promising%20approach%20to%20aligning%20language%20model%20behavior%0Aat%20inference%20time.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20to%20assess%20the%0Alimitations%20of%20steering%20vectors%20as%20alignment%20mechanisms.%20Using%20a%20framework%20of%0Atransformer%20hook%20interventions%20and%20antonym-based%20function%20vectors%2C%20we%20evaluate%0Athe%20role%20of%20prompt%20structure%20and%20context%20complexity%20in%20steering%20effectiveness.%0AOur%20findings%20indicate%20that%20steering%20vectors%20are%20promising%20for%20specific%0Aalignment%20tasks%2C%20such%20as%20value%20alignment%2C%20but%20may%20not%20provide%20a%20robust%0Afoundation%20for%20general-purpose%20alignment%20in%20LLMs%2C%20particularly%20in%20complex%0Ascenarios.%20We%20establish%20a%20methodological%20foundation%20for%20future%20investigations%0Ainto%20steering%20capabilities%20of%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01162v1&entry.124074799=Read"},
{"title": "VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in\n  Video Diffusion Models", "author": "Mohammadreza Teymoorianfard and Shiqing Ma and Amir Houmansadr", "abstract": "  The rapid rise of video diffusion models has enabled the generation of highly\nrealistic and temporally coherent videos, raising critical concerns about\ncontent authenticity, provenance, and misuse. Existing watermarking approaches,\nwhether passive, post-hoc, or adapted from image-based techniques, often\nstruggle to withstand video-specific manipulations such as frame insertion,\ndropping, or reordering, and typically degrade visual quality. In this work, we\nintroduce VIDSTAMP, a watermarking framework that embeds per-frame or\nper-segment messages directly into the latent space of temporally-aware video\ndiffusion models. By fine-tuning the model's decoder through a two-stage\npipeline, first on static image datasets to promote spatial message separation,\nand then on synthesized video sequences to restore temporal consistency,\nVIDSTAMP learns to embed high-capacity, flexible watermarks with minimal\nperceptual impact. Leveraging architectural components such as 3D convolutions\nand temporal attention, our method imposes no additional inference cost and\noffers better perceptual quality than prior methods, while maintaining\ncomparable robustness against common distortions and tampering. VIDSTAMP embeds\n768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a\nlog P-value of -166.65 (lower is better), and maintains a video quality score\nof 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior\nmethods in capacity-quality tradeoffs. Code: Code:\n\\url{https://github.com/SPIN-UMass/VidStamp}\n", "link": "http://arxiv.org/abs/2505.01406v1", "date": "2025-05-02", "relevancy": 2.4257, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6308}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6114}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIDSTAMP%3A%20A%20Temporally-Aware%20Watermark%20for%20Ownership%20and%20Integrity%20in%0A%20%20Video%20Diffusion%20Models&body=Title%3A%20VIDSTAMP%3A%20A%20Temporally-Aware%20Watermark%20for%20Ownership%20and%20Integrity%20in%0A%20%20Video%20Diffusion%20Models%0AAuthor%3A%20Mohammadreza%20Teymoorianfard%20and%20Shiqing%20Ma%20and%20Amir%20Houmansadr%0AAbstract%3A%20%20%20The%20rapid%20rise%20of%20video%20diffusion%20models%20has%20enabled%20the%20generation%20of%20highly%0Arealistic%20and%20temporally%20coherent%20videos%2C%20raising%20critical%20concerns%20about%0Acontent%20authenticity%2C%20provenance%2C%20and%20misuse.%20Existing%20watermarking%20approaches%2C%0Awhether%20passive%2C%20post-hoc%2C%20or%20adapted%20from%20image-based%20techniques%2C%20often%0Astruggle%20to%20withstand%20video-specific%20manipulations%20such%20as%20frame%20insertion%2C%0Adropping%2C%20or%20reordering%2C%20and%20typically%20degrade%20visual%20quality.%20In%20this%20work%2C%20we%0Aintroduce%20VIDSTAMP%2C%20a%20watermarking%20framework%20that%20embeds%20per-frame%20or%0Aper-segment%20messages%20directly%20into%20the%20latent%20space%20of%20temporally-aware%20video%0Adiffusion%20models.%20By%20fine-tuning%20the%20model%27s%20decoder%20through%20a%20two-stage%0Apipeline%2C%20first%20on%20static%20image%20datasets%20to%20promote%20spatial%20message%20separation%2C%0Aand%20then%20on%20synthesized%20video%20sequences%20to%20restore%20temporal%20consistency%2C%0AVIDSTAMP%20learns%20to%20embed%20high-capacity%2C%20flexible%20watermarks%20with%20minimal%0Aperceptual%20impact.%20Leveraging%20architectural%20components%20such%20as%203D%20convolutions%0Aand%20temporal%20attention%2C%20our%20method%20imposes%20no%20additional%20inference%20cost%20and%0Aoffers%20better%20perceptual%20quality%20than%20prior%20methods%2C%20while%20maintaining%0Acomparable%20robustness%20against%20common%20distortions%20and%20tampering.%20VIDSTAMP%20embeds%0A768%20bits%20per%20video%20%2848%20bits%20per%20frame%29%20with%20a%20bit%20accuracy%20of%2095.0%25%2C%20achieves%20a%0Alog%20P-value%20of%20-166.65%20%28lower%20is%20better%29%2C%20and%20maintains%20a%20video%20quality%20score%0Aof%200.836%2C%20comparable%20to%20unwatermarked%20outputs%20%280.838%29%20and%20surpassing%20prior%0Amethods%20in%20capacity-quality%20tradeoffs.%20Code%3A%20Code%3A%0A%5Curl%7Bhttps%3A//github.com/SPIN-UMass/VidStamp%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIDSTAMP%253A%2520A%2520Temporally-Aware%2520Watermark%2520for%2520Ownership%2520and%2520Integrity%2520in%250A%2520%2520Video%2520Diffusion%2520Models%26entry.906535625%3DMohammadreza%2520Teymoorianfard%2520and%2520Shiqing%2520Ma%2520and%2520Amir%2520Houmansadr%26entry.1292438233%3D%2520%2520The%2520rapid%2520rise%2520of%2520video%2520diffusion%2520models%2520has%2520enabled%2520the%2520generation%2520of%2520highly%250Arealistic%2520and%2520temporally%2520coherent%2520videos%252C%2520raising%2520critical%2520concerns%2520about%250Acontent%2520authenticity%252C%2520provenance%252C%2520and%2520misuse.%2520Existing%2520watermarking%2520approaches%252C%250Awhether%2520passive%252C%2520post-hoc%252C%2520or%2520adapted%2520from%2520image-based%2520techniques%252C%2520often%250Astruggle%2520to%2520withstand%2520video-specific%2520manipulations%2520such%2520as%2520frame%2520insertion%252C%250Adropping%252C%2520or%2520reordering%252C%2520and%2520typically%2520degrade%2520visual%2520quality.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520VIDSTAMP%252C%2520a%2520watermarking%2520framework%2520that%2520embeds%2520per-frame%2520or%250Aper-segment%2520messages%2520directly%2520into%2520the%2520latent%2520space%2520of%2520temporally-aware%2520video%250Adiffusion%2520models.%2520By%2520fine-tuning%2520the%2520model%2527s%2520decoder%2520through%2520a%2520two-stage%250Apipeline%252C%2520first%2520on%2520static%2520image%2520datasets%2520to%2520promote%2520spatial%2520message%2520separation%252C%250Aand%2520then%2520on%2520synthesized%2520video%2520sequences%2520to%2520restore%2520temporal%2520consistency%252C%250AVIDSTAMP%2520learns%2520to%2520embed%2520high-capacity%252C%2520flexible%2520watermarks%2520with%2520minimal%250Aperceptual%2520impact.%2520Leveraging%2520architectural%2520components%2520such%2520as%25203D%2520convolutions%250Aand%2520temporal%2520attention%252C%2520our%2520method%2520imposes%2520no%2520additional%2520inference%2520cost%2520and%250Aoffers%2520better%2520perceptual%2520quality%2520than%2520prior%2520methods%252C%2520while%2520maintaining%250Acomparable%2520robustness%2520against%2520common%2520distortions%2520and%2520tampering.%2520VIDSTAMP%2520embeds%250A768%2520bits%2520per%2520video%2520%252848%2520bits%2520per%2520frame%2529%2520with%2520a%2520bit%2520accuracy%2520of%252095.0%2525%252C%2520achieves%2520a%250Alog%2520P-value%2520of%2520-166.65%2520%2528lower%2520is%2520better%2529%252C%2520and%2520maintains%2520a%2520video%2520quality%2520score%250Aof%25200.836%252C%2520comparable%2520to%2520unwatermarked%2520outputs%2520%25280.838%2529%2520and%2520surpassing%2520prior%250Amethods%2520in%2520capacity-quality%2520tradeoffs.%2520Code%253A%2520Code%253A%250A%255Curl%257Bhttps%253A//github.com/SPIN-UMass/VidStamp%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIDSTAMP%3A%20A%20Temporally-Aware%20Watermark%20for%20Ownership%20and%20Integrity%20in%0A%20%20Video%20Diffusion%20Models&entry.906535625=Mohammadreza%20Teymoorianfard%20and%20Shiqing%20Ma%20and%20Amir%20Houmansadr&entry.1292438233=%20%20The%20rapid%20rise%20of%20video%20diffusion%20models%20has%20enabled%20the%20generation%20of%20highly%0Arealistic%20and%20temporally%20coherent%20videos%2C%20raising%20critical%20concerns%20about%0Acontent%20authenticity%2C%20provenance%2C%20and%20misuse.%20Existing%20watermarking%20approaches%2C%0Awhether%20passive%2C%20post-hoc%2C%20or%20adapted%20from%20image-based%20techniques%2C%20often%0Astruggle%20to%20withstand%20video-specific%20manipulations%20such%20as%20frame%20insertion%2C%0Adropping%2C%20or%20reordering%2C%20and%20typically%20degrade%20visual%20quality.%20In%20this%20work%2C%20we%0Aintroduce%20VIDSTAMP%2C%20a%20watermarking%20framework%20that%20embeds%20per-frame%20or%0Aper-segment%20messages%20directly%20into%20the%20latent%20space%20of%20temporally-aware%20video%0Adiffusion%20models.%20By%20fine-tuning%20the%20model%27s%20decoder%20through%20a%20two-stage%0Apipeline%2C%20first%20on%20static%20image%20datasets%20to%20promote%20spatial%20message%20separation%2C%0Aand%20then%20on%20synthesized%20video%20sequences%20to%20restore%20temporal%20consistency%2C%0AVIDSTAMP%20learns%20to%20embed%20high-capacity%2C%20flexible%20watermarks%20with%20minimal%0Aperceptual%20impact.%20Leveraging%20architectural%20components%20such%20as%203D%20convolutions%0Aand%20temporal%20attention%2C%20our%20method%20imposes%20no%20additional%20inference%20cost%20and%0Aoffers%20better%20perceptual%20quality%20than%20prior%20methods%2C%20while%20maintaining%0Acomparable%20robustness%20against%20common%20distortions%20and%20tampering.%20VIDSTAMP%20embeds%0A768%20bits%20per%20video%20%2848%20bits%20per%20frame%29%20with%20a%20bit%20accuracy%20of%2095.0%25%2C%20achieves%20a%0Alog%20P-value%20of%20-166.65%20%28lower%20is%20better%29%2C%20and%20maintains%20a%20video%20quality%20score%0Aof%200.836%2C%20comparable%20to%20unwatermarked%20outputs%20%280.838%29%20and%20surpassing%20prior%0Amethods%20in%20capacity-quality%20tradeoffs.%20Code%3A%20Code%3A%0A%5Curl%7Bhttps%3A//github.com/SPIN-UMass/VidStamp%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01406v1&entry.124074799=Read"},
{"title": "Early Detection of Patient Deterioration from Real-Time Wearable\n  Monitoring System", "author": "Lo Pang-Yun Ting and Hong-Pei Chen and An-Shan Liu and Chun-Yin Yeh and Po-Lin Chen and Kun-Ta Chuang", "abstract": "  Early detection of patient deterioration is crucial for reducing mortality\nrates. Heart rate data has shown promise in assessing patient health, and\nwearable devices offer a cost-effective solution for real-time monitoring.\nHowever, extracting meaningful insights from diverse heart rate data and\nhandling missing values in wearable device data remain key challenges. To\naddress these challenges, we propose TARL, an innovative approach that models\nthe structural relationships of representative subsequences, known as\nshapelets, in heart rate time series. TARL creates a shapelet-transition\nknowledge graph to model shapelet dynamics in heart rate time series,\nindicating illness progression and potential future changes. We further\nintroduce a transition-aware knowledge embedding to reinforce relationships\namong shapelets and quantify the impact of missing values, enabling the\nformulation of comprehensive heart rate representations. These representations\ncapture explanatory structures and predict future heart rate trends, aiding\nearly illness detection. We collaborate with physicians and nurses to gather\nICU patient heart rate data from wearables and diagnostic metrics assessing\nillness severity for evaluating deterioration. Experiments on real-world ICU\ndata demonstrate that TARL achieves both high reliability and early detection.\nA case study further showcases TARL's explainable detection process,\nhighlighting its potential as an AI-driven tool to assist clinicians in\nrecognizing early signs of patient deterioration.\n", "link": "http://arxiv.org/abs/2505.01305v1", "date": "2025-05-02", "relevancy": 2.4184, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.499}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4802}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Detection%20of%20Patient%20Deterioration%20from%20Real-Time%20Wearable%0A%20%20Monitoring%20System&body=Title%3A%20Early%20Detection%20of%20Patient%20Deterioration%20from%20Real-Time%20Wearable%0A%20%20Monitoring%20System%0AAuthor%3A%20Lo%20Pang-Yun%20Ting%20and%20Hong-Pei%20Chen%20and%20An-Shan%20Liu%20and%20Chun-Yin%20Yeh%20and%20Po-Lin%20Chen%20and%20Kun-Ta%20Chuang%0AAbstract%3A%20%20%20Early%20detection%20of%20patient%20deterioration%20is%20crucial%20for%20reducing%20mortality%0Arates.%20Heart%20rate%20data%20has%20shown%20promise%20in%20assessing%20patient%20health%2C%20and%0Awearable%20devices%20offer%20a%20cost-effective%20solution%20for%20real-time%20monitoring.%0AHowever%2C%20extracting%20meaningful%20insights%20from%20diverse%20heart%20rate%20data%20and%0Ahandling%20missing%20values%20in%20wearable%20device%20data%20remain%20key%20challenges.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20TARL%2C%20an%20innovative%20approach%20that%20models%0Athe%20structural%20relationships%20of%20representative%20subsequences%2C%20known%20as%0Ashapelets%2C%20in%20heart%20rate%20time%20series.%20TARL%20creates%20a%20shapelet-transition%0Aknowledge%20graph%20to%20model%20shapelet%20dynamics%20in%20heart%20rate%20time%20series%2C%0Aindicating%20illness%20progression%20and%20potential%20future%20changes.%20We%20further%0Aintroduce%20a%20transition-aware%20knowledge%20embedding%20to%20reinforce%20relationships%0Aamong%20shapelets%20and%20quantify%20the%20impact%20of%20missing%20values%2C%20enabling%20the%0Aformulation%20of%20comprehensive%20heart%20rate%20representations.%20These%20representations%0Acapture%20explanatory%20structures%20and%20predict%20future%20heart%20rate%20trends%2C%20aiding%0Aearly%20illness%20detection.%20We%20collaborate%20with%20physicians%20and%20nurses%20to%20gather%0AICU%20patient%20heart%20rate%20data%20from%20wearables%20and%20diagnostic%20metrics%20assessing%0Aillness%20severity%20for%20evaluating%20deterioration.%20Experiments%20on%20real-world%20ICU%0Adata%20demonstrate%20that%20TARL%20achieves%20both%20high%20reliability%20and%20early%20detection.%0AA%20case%20study%20further%20showcases%20TARL%27s%20explainable%20detection%20process%2C%0Ahighlighting%20its%20potential%20as%20an%20AI-driven%20tool%20to%20assist%20clinicians%20in%0Arecognizing%20early%20signs%20of%20patient%20deterioration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Detection%2520of%2520Patient%2520Deterioration%2520from%2520Real-Time%2520Wearable%250A%2520%2520Monitoring%2520System%26entry.906535625%3DLo%2520Pang-Yun%2520Ting%2520and%2520Hong-Pei%2520Chen%2520and%2520An-Shan%2520Liu%2520and%2520Chun-Yin%2520Yeh%2520and%2520Po-Lin%2520Chen%2520and%2520Kun-Ta%2520Chuang%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520patient%2520deterioration%2520is%2520crucial%2520for%2520reducing%2520mortality%250Arates.%2520Heart%2520rate%2520data%2520has%2520shown%2520promise%2520in%2520assessing%2520patient%2520health%252C%2520and%250Awearable%2520devices%2520offer%2520a%2520cost-effective%2520solution%2520for%2520real-time%2520monitoring.%250AHowever%252C%2520extracting%2520meaningful%2520insights%2520from%2520diverse%2520heart%2520rate%2520data%2520and%250Ahandling%2520missing%2520values%2520in%2520wearable%2520device%2520data%2520remain%2520key%2520challenges.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520TARL%252C%2520an%2520innovative%2520approach%2520that%2520models%250Athe%2520structural%2520relationships%2520of%2520representative%2520subsequences%252C%2520known%2520as%250Ashapelets%252C%2520in%2520heart%2520rate%2520time%2520series.%2520TARL%2520creates%2520a%2520shapelet-transition%250Aknowledge%2520graph%2520to%2520model%2520shapelet%2520dynamics%2520in%2520heart%2520rate%2520time%2520series%252C%250Aindicating%2520illness%2520progression%2520and%2520potential%2520future%2520changes.%2520We%2520further%250Aintroduce%2520a%2520transition-aware%2520knowledge%2520embedding%2520to%2520reinforce%2520relationships%250Aamong%2520shapelets%2520and%2520quantify%2520the%2520impact%2520of%2520missing%2520values%252C%2520enabling%2520the%250Aformulation%2520of%2520comprehensive%2520heart%2520rate%2520representations.%2520These%2520representations%250Acapture%2520explanatory%2520structures%2520and%2520predict%2520future%2520heart%2520rate%2520trends%252C%2520aiding%250Aearly%2520illness%2520detection.%2520We%2520collaborate%2520with%2520physicians%2520and%2520nurses%2520to%2520gather%250AICU%2520patient%2520heart%2520rate%2520data%2520from%2520wearables%2520and%2520diagnostic%2520metrics%2520assessing%250Aillness%2520severity%2520for%2520evaluating%2520deterioration.%2520Experiments%2520on%2520real-world%2520ICU%250Adata%2520demonstrate%2520that%2520TARL%2520achieves%2520both%2520high%2520reliability%2520and%2520early%2520detection.%250AA%2520case%2520study%2520further%2520showcases%2520TARL%2527s%2520explainable%2520detection%2520process%252C%250Ahighlighting%2520its%2520potential%2520as%2520an%2520AI-driven%2520tool%2520to%2520assist%2520clinicians%2520in%250Arecognizing%2520early%2520signs%2520of%2520patient%2520deterioration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Detection%20of%20Patient%20Deterioration%20from%20Real-Time%20Wearable%0A%20%20Monitoring%20System&entry.906535625=Lo%20Pang-Yun%20Ting%20and%20Hong-Pei%20Chen%20and%20An-Shan%20Liu%20and%20Chun-Yin%20Yeh%20and%20Po-Lin%20Chen%20and%20Kun-Ta%20Chuang&entry.1292438233=%20%20Early%20detection%20of%20patient%20deterioration%20is%20crucial%20for%20reducing%20mortality%0Arates.%20Heart%20rate%20data%20has%20shown%20promise%20in%20assessing%20patient%20health%2C%20and%0Awearable%20devices%20offer%20a%20cost-effective%20solution%20for%20real-time%20monitoring.%0AHowever%2C%20extracting%20meaningful%20insights%20from%20diverse%20heart%20rate%20data%20and%0Ahandling%20missing%20values%20in%20wearable%20device%20data%20remain%20key%20challenges.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20TARL%2C%20an%20innovative%20approach%20that%20models%0Athe%20structural%20relationships%20of%20representative%20subsequences%2C%20known%20as%0Ashapelets%2C%20in%20heart%20rate%20time%20series.%20TARL%20creates%20a%20shapelet-transition%0Aknowledge%20graph%20to%20model%20shapelet%20dynamics%20in%20heart%20rate%20time%20series%2C%0Aindicating%20illness%20progression%20and%20potential%20future%20changes.%20We%20further%0Aintroduce%20a%20transition-aware%20knowledge%20embedding%20to%20reinforce%20relationships%0Aamong%20shapelets%20and%20quantify%20the%20impact%20of%20missing%20values%2C%20enabling%20the%0Aformulation%20of%20comprehensive%20heart%20rate%20representations.%20These%20representations%0Acapture%20explanatory%20structures%20and%20predict%20future%20heart%20rate%20trends%2C%20aiding%0Aearly%20illness%20detection.%20We%20collaborate%20with%20physicians%20and%20nurses%20to%20gather%0AICU%20patient%20heart%20rate%20data%20from%20wearables%20and%20diagnostic%20metrics%20assessing%0Aillness%20severity%20for%20evaluating%20deterioration.%20Experiments%20on%20real-world%20ICU%0Adata%20demonstrate%20that%20TARL%20achieves%20both%20high%20reliability%20and%20early%20detection.%0AA%20case%20study%20further%20showcases%20TARL%27s%20explainable%20detection%20process%2C%0Ahighlighting%20its%20potential%20as%20an%20AI-driven%20tool%20to%20assist%20clinicians%20in%0Arecognizing%20early%20signs%20of%20patient%20deterioration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01305v1&entry.124074799=Read"},
{"title": "Deciphering scrolls with tomography: A training experiment", "author": "Sonia Foschiatti and Axel Kittenberger and Otmar Scherzer", "abstract": "  The recovery of severely damaged ancient written documents has proven to be a\nmajor challenge for many scientists, mainly due to the impracticality of\nphysical unwrapping them. Non-destructive techniques, such as X-ray computed\ntomography (CT), combined with computer vision algorithms, have emerged as a\nmeans of facilitating the virtual reading of the hidden contents of the damaged\ndocuments. This paper proposes an educational laboratory aimed at simulating\nthe entire process of acquisition and virtual recovery of the ancient works. We\nhave developed an experimental setup that uses visible light to replace the\ndetrimental X-rays, and a didactic software pipeline that allows students to\nvirtually reconstruct a transparent rolled sheet with printed text on it, the\nwrapped scroll.\n", "link": "http://arxiv.org/abs/2504.11485v2", "date": "2025-05-02", "relevancy": 2.4059, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4992}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deciphering%20scrolls%20with%20tomography%3A%20A%20training%20experiment&body=Title%3A%20Deciphering%20scrolls%20with%20tomography%3A%20A%20training%20experiment%0AAuthor%3A%20Sonia%20Foschiatti%20and%20Axel%20Kittenberger%20and%20Otmar%20Scherzer%0AAbstract%3A%20%20%20The%20recovery%20of%20severely%20damaged%20ancient%20written%20documents%20has%20proven%20to%20be%20a%0Amajor%20challenge%20for%20many%20scientists%2C%20mainly%20due%20to%20the%20impracticality%20of%0Aphysical%20unwrapping%20them.%20Non-destructive%20techniques%2C%20such%20as%20X-ray%20computed%0Atomography%20%28CT%29%2C%20combined%20with%20computer%20vision%20algorithms%2C%20have%20emerged%20as%20a%0Ameans%20of%20facilitating%20the%20virtual%20reading%20of%20the%20hidden%20contents%20of%20the%20damaged%0Adocuments.%20This%20paper%20proposes%20an%20educational%20laboratory%20aimed%20at%20simulating%0Athe%20entire%20process%20of%20acquisition%20and%20virtual%20recovery%20of%20the%20ancient%20works.%20We%0Ahave%20developed%20an%20experimental%20setup%20that%20uses%20visible%20light%20to%20replace%20the%0Adetrimental%20X-rays%2C%20and%20a%20didactic%20software%20pipeline%20that%20allows%20students%20to%0Avirtually%20reconstruct%20a%20transparent%20rolled%20sheet%20with%20printed%20text%20on%20it%2C%20the%0Awrapped%20scroll.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeciphering%2520scrolls%2520with%2520tomography%253A%2520A%2520training%2520experiment%26entry.906535625%3DSonia%2520Foschiatti%2520and%2520Axel%2520Kittenberger%2520and%2520Otmar%2520Scherzer%26entry.1292438233%3D%2520%2520The%2520recovery%2520of%2520severely%2520damaged%2520ancient%2520written%2520documents%2520has%2520proven%2520to%2520be%2520a%250Amajor%2520challenge%2520for%2520many%2520scientists%252C%2520mainly%2520due%2520to%2520the%2520impracticality%2520of%250Aphysical%2520unwrapping%2520them.%2520Non-destructive%2520techniques%252C%2520such%2520as%2520X-ray%2520computed%250Atomography%2520%2528CT%2529%252C%2520combined%2520with%2520computer%2520vision%2520algorithms%252C%2520have%2520emerged%2520as%2520a%250Ameans%2520of%2520facilitating%2520the%2520virtual%2520reading%2520of%2520the%2520hidden%2520contents%2520of%2520the%2520damaged%250Adocuments.%2520This%2520paper%2520proposes%2520an%2520educational%2520laboratory%2520aimed%2520at%2520simulating%250Athe%2520entire%2520process%2520of%2520acquisition%2520and%2520virtual%2520recovery%2520of%2520the%2520ancient%2520works.%2520We%250Ahave%2520developed%2520an%2520experimental%2520setup%2520that%2520uses%2520visible%2520light%2520to%2520replace%2520the%250Adetrimental%2520X-rays%252C%2520and%2520a%2520didactic%2520software%2520pipeline%2520that%2520allows%2520students%2520to%250Avirtually%2520reconstruct%2520a%2520transparent%2520rolled%2520sheet%2520with%2520printed%2520text%2520on%2520it%252C%2520the%250Awrapped%2520scroll.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deciphering%20scrolls%20with%20tomography%3A%20A%20training%20experiment&entry.906535625=Sonia%20Foschiatti%20and%20Axel%20Kittenberger%20and%20Otmar%20Scherzer&entry.1292438233=%20%20The%20recovery%20of%20severely%20damaged%20ancient%20written%20documents%20has%20proven%20to%20be%20a%0Amajor%20challenge%20for%20many%20scientists%2C%20mainly%20due%20to%20the%20impracticality%20of%0Aphysical%20unwrapping%20them.%20Non-destructive%20techniques%2C%20such%20as%20X-ray%20computed%0Atomography%20%28CT%29%2C%20combined%20with%20computer%20vision%20algorithms%2C%20have%20emerged%20as%20a%0Ameans%20of%20facilitating%20the%20virtual%20reading%20of%20the%20hidden%20contents%20of%20the%20damaged%0Adocuments.%20This%20paper%20proposes%20an%20educational%20laboratory%20aimed%20at%20simulating%0Athe%20entire%20process%20of%20acquisition%20and%20virtual%20recovery%20of%20the%20ancient%20works.%20We%0Ahave%20developed%20an%20experimental%20setup%20that%20uses%20visible%20light%20to%20replace%20the%0Adetrimental%20X-rays%2C%20and%20a%20didactic%20software%20pipeline%20that%20allows%20students%20to%0Avirtually%20reconstruct%20a%20transparent%20rolled%20sheet%20with%20printed%20text%20on%20it%2C%20the%0Awrapped%20scroll.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11485v2&entry.124074799=Read"},
{"title": "Deterministic Nonsmooth Nonconvex Optimization", "author": "Michael I. Jordan and Guy Kornowski and Tianyi Lin and Ohad Shamir and Manolis Zampetakis", "abstract": "  We study the complexity of optimizing nonsmooth nonconvex Lipschitz functions\nby producing $(\\delta,\\epsilon)$-stationary points. Several recent works have\npresented randomized algorithms that produce such points using $\\tilde\nO(\\delta^{-1}\\epsilon^{-3})$ first-order oracle calls, independent of the\ndimension $d$. It has been an open problem as to whether a similar result can\nbe obtained via a deterministic algorithm. We resolve this open problem,\nshowing that randomization is necessary to obtain a dimension-free rate. In\nparticular, we prove a lower bound of $\\Omega(d)$ for any deterministic\nalgorithm. Moreover, we show that unlike smooth or convex optimization, access\nto function values is required for any deterministic algorithm to halt within\nany finite time.\n  On the other hand, we prove that if the function is even slightly smooth,\nthen the dimension-free rate of $\\tilde O(\\delta^{-1}\\epsilon^{-3})$ can be\nobtained by a deterministic algorithm with merely a logarithmic dependence on\nthe smoothness parameter. Motivated by these findings, we turn to study the\ncomplexity of deterministically smoothing Lipschitz functions. Though there are\nefficient black-box randomized smoothings, we start by showing that no such\ndeterministic procedure can smooth functions in a meaningful manner, resolving\nan open question. We then bypass this impossibility result for the structured\ncase of ReLU neural networks. To that end, in a practical white-box setting in\nwhich the optimizer is granted access to the network's architecture, we propose\na simple, dimension-free, deterministic smoothing that provably preserves\n$(\\delta,\\epsilon)$-stationary points. Our method applies to a variety of\narchitectures of arbitrary depth, including ResNets and ConvNets. Combined with\nour algorithm, this yields the first deterministic dimension-free algorithm for\noptimizing ReLU networks, circumventing our lower bound.\n", "link": "http://arxiv.org/abs/2302.08300v2", "date": "2025-05-02", "relevancy": 2.3545, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4781}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4744}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deterministic%20Nonsmooth%20Nonconvex%20Optimization&body=Title%3A%20Deterministic%20Nonsmooth%20Nonconvex%20Optimization%0AAuthor%3A%20Michael%20I.%20Jordan%20and%20Guy%20Kornowski%20and%20Tianyi%20Lin%20and%20Ohad%20Shamir%20and%20Manolis%20Zampetakis%0AAbstract%3A%20%20%20We%20study%20the%20complexity%20of%20optimizing%20nonsmooth%20nonconvex%20Lipschitz%20functions%0Aby%20producing%20%24%28%5Cdelta%2C%5Cepsilon%29%24-stationary%20points.%20Several%20recent%20works%20have%0Apresented%20randomized%20algorithms%20that%20produce%20such%20points%20using%20%24%5Ctilde%0AO%28%5Cdelta%5E%7B-1%7D%5Cepsilon%5E%7B-3%7D%29%24%20first-order%20oracle%20calls%2C%20independent%20of%20the%0Adimension%20%24d%24.%20It%20has%20been%20an%20open%20problem%20as%20to%20whether%20a%20similar%20result%20can%0Abe%20obtained%20via%20a%20deterministic%20algorithm.%20We%20resolve%20this%20open%20problem%2C%0Ashowing%20that%20randomization%20is%20necessary%20to%20obtain%20a%20dimension-free%20rate.%20In%0Aparticular%2C%20we%20prove%20a%20lower%20bound%20of%20%24%5COmega%28d%29%24%20for%20any%20deterministic%0Aalgorithm.%20Moreover%2C%20we%20show%20that%20unlike%20smooth%20or%20convex%20optimization%2C%20access%0Ato%20function%20values%20is%20required%20for%20any%20deterministic%20algorithm%20to%20halt%20within%0Aany%20finite%20time.%0A%20%20On%20the%20other%20hand%2C%20we%20prove%20that%20if%20the%20function%20is%20even%20slightly%20smooth%2C%0Athen%20the%20dimension-free%20rate%20of%20%24%5Ctilde%20O%28%5Cdelta%5E%7B-1%7D%5Cepsilon%5E%7B-3%7D%29%24%20can%20be%0Aobtained%20by%20a%20deterministic%20algorithm%20with%20merely%20a%20logarithmic%20dependence%20on%0Athe%20smoothness%20parameter.%20Motivated%20by%20these%20findings%2C%20we%20turn%20to%20study%20the%0Acomplexity%20of%20deterministically%20smoothing%20Lipschitz%20functions.%20Though%20there%20are%0Aefficient%20black-box%20randomized%20smoothings%2C%20we%20start%20by%20showing%20that%20no%20such%0Adeterministic%20procedure%20can%20smooth%20functions%20in%20a%20meaningful%20manner%2C%20resolving%0Aan%20open%20question.%20We%20then%20bypass%20this%20impossibility%20result%20for%20the%20structured%0Acase%20of%20ReLU%20neural%20networks.%20To%20that%20end%2C%20in%20a%20practical%20white-box%20setting%20in%0Awhich%20the%20optimizer%20is%20granted%20access%20to%20the%20network%27s%20architecture%2C%20we%20propose%0Aa%20simple%2C%20dimension-free%2C%20deterministic%20smoothing%20that%20provably%20preserves%0A%24%28%5Cdelta%2C%5Cepsilon%29%24-stationary%20points.%20Our%20method%20applies%20to%20a%20variety%20of%0Aarchitectures%20of%20arbitrary%20depth%2C%20including%20ResNets%20and%20ConvNets.%20Combined%20with%0Aour%20algorithm%2C%20this%20yields%20the%20first%20deterministic%20dimension-free%20algorithm%20for%0Aoptimizing%20ReLU%20networks%2C%20circumventing%20our%20lower%20bound.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.08300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeterministic%2520Nonsmooth%2520Nonconvex%2520Optimization%26entry.906535625%3DMichael%2520I.%2520Jordan%2520and%2520Guy%2520Kornowski%2520and%2520Tianyi%2520Lin%2520and%2520Ohad%2520Shamir%2520and%2520Manolis%2520Zampetakis%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520complexity%2520of%2520optimizing%2520nonsmooth%2520nonconvex%2520Lipschitz%2520functions%250Aby%2520producing%2520%2524%2528%255Cdelta%252C%255Cepsilon%2529%2524-stationary%2520points.%2520Several%2520recent%2520works%2520have%250Apresented%2520randomized%2520algorithms%2520that%2520produce%2520such%2520points%2520using%2520%2524%255Ctilde%250AO%2528%255Cdelta%255E%257B-1%257D%255Cepsilon%255E%257B-3%257D%2529%2524%2520first-order%2520oracle%2520calls%252C%2520independent%2520of%2520the%250Adimension%2520%2524d%2524.%2520It%2520has%2520been%2520an%2520open%2520problem%2520as%2520to%2520whether%2520a%2520similar%2520result%2520can%250Abe%2520obtained%2520via%2520a%2520deterministic%2520algorithm.%2520We%2520resolve%2520this%2520open%2520problem%252C%250Ashowing%2520that%2520randomization%2520is%2520necessary%2520to%2520obtain%2520a%2520dimension-free%2520rate.%2520In%250Aparticular%252C%2520we%2520prove%2520a%2520lower%2520bound%2520of%2520%2524%255COmega%2528d%2529%2524%2520for%2520any%2520deterministic%250Aalgorithm.%2520Moreover%252C%2520we%2520show%2520that%2520unlike%2520smooth%2520or%2520convex%2520optimization%252C%2520access%250Ato%2520function%2520values%2520is%2520required%2520for%2520any%2520deterministic%2520algorithm%2520to%2520halt%2520within%250Aany%2520finite%2520time.%250A%2520%2520On%2520the%2520other%2520hand%252C%2520we%2520prove%2520that%2520if%2520the%2520function%2520is%2520even%2520slightly%2520smooth%252C%250Athen%2520the%2520dimension-free%2520rate%2520of%2520%2524%255Ctilde%2520O%2528%255Cdelta%255E%257B-1%257D%255Cepsilon%255E%257B-3%257D%2529%2524%2520can%2520be%250Aobtained%2520by%2520a%2520deterministic%2520algorithm%2520with%2520merely%2520a%2520logarithmic%2520dependence%2520on%250Athe%2520smoothness%2520parameter.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520turn%2520to%2520study%2520the%250Acomplexity%2520of%2520deterministically%2520smoothing%2520Lipschitz%2520functions.%2520Though%2520there%2520are%250Aefficient%2520black-box%2520randomized%2520smoothings%252C%2520we%2520start%2520by%2520showing%2520that%2520no%2520such%250Adeterministic%2520procedure%2520can%2520smooth%2520functions%2520in%2520a%2520meaningful%2520manner%252C%2520resolving%250Aan%2520open%2520question.%2520We%2520then%2520bypass%2520this%2520impossibility%2520result%2520for%2520the%2520structured%250Acase%2520of%2520ReLU%2520neural%2520networks.%2520To%2520that%2520end%252C%2520in%2520a%2520practical%2520white-box%2520setting%2520in%250Awhich%2520the%2520optimizer%2520is%2520granted%2520access%2520to%2520the%2520network%2527s%2520architecture%252C%2520we%2520propose%250Aa%2520simple%252C%2520dimension-free%252C%2520deterministic%2520smoothing%2520that%2520provably%2520preserves%250A%2524%2528%255Cdelta%252C%255Cepsilon%2529%2524-stationary%2520points.%2520Our%2520method%2520applies%2520to%2520a%2520variety%2520of%250Aarchitectures%2520of%2520arbitrary%2520depth%252C%2520including%2520ResNets%2520and%2520ConvNets.%2520Combined%2520with%250Aour%2520algorithm%252C%2520this%2520yields%2520the%2520first%2520deterministic%2520dimension-free%2520algorithm%2520for%250Aoptimizing%2520ReLU%2520networks%252C%2520circumventing%2520our%2520lower%2520bound.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.08300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deterministic%20Nonsmooth%20Nonconvex%20Optimization&entry.906535625=Michael%20I.%20Jordan%20and%20Guy%20Kornowski%20and%20Tianyi%20Lin%20and%20Ohad%20Shamir%20and%20Manolis%20Zampetakis&entry.1292438233=%20%20We%20study%20the%20complexity%20of%20optimizing%20nonsmooth%20nonconvex%20Lipschitz%20functions%0Aby%20producing%20%24%28%5Cdelta%2C%5Cepsilon%29%24-stationary%20points.%20Several%20recent%20works%20have%0Apresented%20randomized%20algorithms%20that%20produce%20such%20points%20using%20%24%5Ctilde%0AO%28%5Cdelta%5E%7B-1%7D%5Cepsilon%5E%7B-3%7D%29%24%20first-order%20oracle%20calls%2C%20independent%20of%20the%0Adimension%20%24d%24.%20It%20has%20been%20an%20open%20problem%20as%20to%20whether%20a%20similar%20result%20can%0Abe%20obtained%20via%20a%20deterministic%20algorithm.%20We%20resolve%20this%20open%20problem%2C%0Ashowing%20that%20randomization%20is%20necessary%20to%20obtain%20a%20dimension-free%20rate.%20In%0Aparticular%2C%20we%20prove%20a%20lower%20bound%20of%20%24%5COmega%28d%29%24%20for%20any%20deterministic%0Aalgorithm.%20Moreover%2C%20we%20show%20that%20unlike%20smooth%20or%20convex%20optimization%2C%20access%0Ato%20function%20values%20is%20required%20for%20any%20deterministic%20algorithm%20to%20halt%20within%0Aany%20finite%20time.%0A%20%20On%20the%20other%20hand%2C%20we%20prove%20that%20if%20the%20function%20is%20even%20slightly%20smooth%2C%0Athen%20the%20dimension-free%20rate%20of%20%24%5Ctilde%20O%28%5Cdelta%5E%7B-1%7D%5Cepsilon%5E%7B-3%7D%29%24%20can%20be%0Aobtained%20by%20a%20deterministic%20algorithm%20with%20merely%20a%20logarithmic%20dependence%20on%0Athe%20smoothness%20parameter.%20Motivated%20by%20these%20findings%2C%20we%20turn%20to%20study%20the%0Acomplexity%20of%20deterministically%20smoothing%20Lipschitz%20functions.%20Though%20there%20are%0Aefficient%20black-box%20randomized%20smoothings%2C%20we%20start%20by%20showing%20that%20no%20such%0Adeterministic%20procedure%20can%20smooth%20functions%20in%20a%20meaningful%20manner%2C%20resolving%0Aan%20open%20question.%20We%20then%20bypass%20this%20impossibility%20result%20for%20the%20structured%0Acase%20of%20ReLU%20neural%20networks.%20To%20that%20end%2C%20in%20a%20practical%20white-box%20setting%20in%0Awhich%20the%20optimizer%20is%20granted%20access%20to%20the%20network%27s%20architecture%2C%20we%20propose%0Aa%20simple%2C%20dimension-free%2C%20deterministic%20smoothing%20that%20provably%20preserves%0A%24%28%5Cdelta%2C%5Cepsilon%29%24-stationary%20points.%20Our%20method%20applies%20to%20a%20variety%20of%0Aarchitectures%20of%20arbitrary%20depth%2C%20including%20ResNets%20and%20ConvNets.%20Combined%20with%0Aour%20algorithm%2C%20this%20yields%20the%20first%20deterministic%20dimension-free%20algorithm%20for%0Aoptimizing%20ReLU%20networks%2C%20circumventing%20our%20lower%20bound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.08300v2&entry.124074799=Read"},
{"title": "FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian\n  Scene without Spatial Priors", "author": "Chenxi Li and Weijie Wang and Qiang Li and Bruno Lepri and Nicu Sebe and Weizhi Nie", "abstract": "  Text-driven object insertion in 3D scenes is an emerging task that enables\nintuitive scene editing through natural language. However, existing 2D\nediting-based methods often rely on spatial priors such as 2D masks or 3D\nbounding boxes, and they struggle to ensure consistency of the inserted object.\nThese limitations hinder flexibility and scalability in real-world\napplications. In this paper, we propose FreeInsert, a novel framework that\nleverages foundation models including MLLMs, LGMs, and diffusion models to\ndisentangle object generation from spatial placement. This enables unsupervised\nand flexible object insertion in 3D scenes without spatial priors. FreeInsert\nstarts with an MLLM-based parser that extracts structured semantics, including\nobject types, spatial relationships, and attachment regions, from user\ninstructions. These semantics guide both the reconstruction of the inserted\nobject for 3D consistency and the learning of its degrees of freedom. We\nleverage the spatial reasoning capabilities of MLLMs to initialize object pose\nand scale. A hierarchical, spatially aware refinement stage further integrates\nspatial semantics and MLLM-inferred priors to enhance placement. Finally, the\nappearance of the object is improved using the inserted-object image to enhance\nvisual fidelity. Experimental results demonstrate that FreeInsert achieves\nsemantically coherent, spatially precise, and visually realistic 3D insertions\nwithout relying on spatial priors, offering a user-friendly and flexible\nediting experience.\n", "link": "http://arxiv.org/abs/2505.01322v1", "date": "2025-05-02", "relevancy": 2.3465, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5989}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5874}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeInsert%3A%20Disentangled%20Text-Guided%20Object%20Insertion%20in%203D%20Gaussian%0A%20%20Scene%20without%20Spatial%20Priors&body=Title%3A%20FreeInsert%3A%20Disentangled%20Text-Guided%20Object%20Insertion%20in%203D%20Gaussian%0A%20%20Scene%20without%20Spatial%20Priors%0AAuthor%3A%20Chenxi%20Li%20and%20Weijie%20Wang%20and%20Qiang%20Li%20and%20Bruno%20Lepri%20and%20Nicu%20Sebe%20and%20Weizhi%20Nie%0AAbstract%3A%20%20%20Text-driven%20object%20insertion%20in%203D%20scenes%20is%20an%20emerging%20task%20that%20enables%0Aintuitive%20scene%20editing%20through%20natural%20language.%20However%2C%20existing%202D%0Aediting-based%20methods%20often%20rely%20on%20spatial%20priors%20such%20as%202D%20masks%20or%203D%0Abounding%20boxes%2C%20and%20they%20struggle%20to%20ensure%20consistency%20of%20the%20inserted%20object.%0AThese%20limitations%20hinder%20flexibility%20and%20scalability%20in%20real-world%0Aapplications.%20In%20this%20paper%2C%20we%20propose%20FreeInsert%2C%20a%20novel%20framework%20that%0Aleverages%20foundation%20models%20including%20MLLMs%2C%20LGMs%2C%20and%20diffusion%20models%20to%0Adisentangle%20object%20generation%20from%20spatial%20placement.%20This%20enables%20unsupervised%0Aand%20flexible%20object%20insertion%20in%203D%20scenes%20without%20spatial%20priors.%20FreeInsert%0Astarts%20with%20an%20MLLM-based%20parser%20that%20extracts%20structured%20semantics%2C%20including%0Aobject%20types%2C%20spatial%20relationships%2C%20and%20attachment%20regions%2C%20from%20user%0Ainstructions.%20These%20semantics%20guide%20both%20the%20reconstruction%20of%20the%20inserted%0Aobject%20for%203D%20consistency%20and%20the%20learning%20of%20its%20degrees%20of%20freedom.%20We%0Aleverage%20the%20spatial%20reasoning%20capabilities%20of%20MLLMs%20to%20initialize%20object%20pose%0Aand%20scale.%20A%20hierarchical%2C%20spatially%20aware%20refinement%20stage%20further%20integrates%0Aspatial%20semantics%20and%20MLLM-inferred%20priors%20to%20enhance%20placement.%20Finally%2C%20the%0Aappearance%20of%20the%20object%20is%20improved%20using%20the%20inserted-object%20image%20to%20enhance%0Avisual%20fidelity.%20Experimental%20results%20demonstrate%20that%20FreeInsert%20achieves%0Asemantically%20coherent%2C%20spatially%20precise%2C%20and%20visually%20realistic%203D%20insertions%0Awithout%20relying%20on%20spatial%20priors%2C%20offering%20a%20user-friendly%20and%20flexible%0Aediting%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeInsert%253A%2520Disentangled%2520Text-Guided%2520Object%2520Insertion%2520in%25203D%2520Gaussian%250A%2520%2520Scene%2520without%2520Spatial%2520Priors%26entry.906535625%3DChenxi%2520Li%2520and%2520Weijie%2520Wang%2520and%2520Qiang%2520Li%2520and%2520Bruno%2520Lepri%2520and%2520Nicu%2520Sebe%2520and%2520Weizhi%2520Nie%26entry.1292438233%3D%2520%2520Text-driven%2520object%2520insertion%2520in%25203D%2520scenes%2520is%2520an%2520emerging%2520task%2520that%2520enables%250Aintuitive%2520scene%2520editing%2520through%2520natural%2520language.%2520However%252C%2520existing%25202D%250Aediting-based%2520methods%2520often%2520rely%2520on%2520spatial%2520priors%2520such%2520as%25202D%2520masks%2520or%25203D%250Abounding%2520boxes%252C%2520and%2520they%2520struggle%2520to%2520ensure%2520consistency%2520of%2520the%2520inserted%2520object.%250AThese%2520limitations%2520hinder%2520flexibility%2520and%2520scalability%2520in%2520real-world%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FreeInsert%252C%2520a%2520novel%2520framework%2520that%250Aleverages%2520foundation%2520models%2520including%2520MLLMs%252C%2520LGMs%252C%2520and%2520diffusion%2520models%2520to%250Adisentangle%2520object%2520generation%2520from%2520spatial%2520placement.%2520This%2520enables%2520unsupervised%250Aand%2520flexible%2520object%2520insertion%2520in%25203D%2520scenes%2520without%2520spatial%2520priors.%2520FreeInsert%250Astarts%2520with%2520an%2520MLLM-based%2520parser%2520that%2520extracts%2520structured%2520semantics%252C%2520including%250Aobject%2520types%252C%2520spatial%2520relationships%252C%2520and%2520attachment%2520regions%252C%2520from%2520user%250Ainstructions.%2520These%2520semantics%2520guide%2520both%2520the%2520reconstruction%2520of%2520the%2520inserted%250Aobject%2520for%25203D%2520consistency%2520and%2520the%2520learning%2520of%2520its%2520degrees%2520of%2520freedom.%2520We%250Aleverage%2520the%2520spatial%2520reasoning%2520capabilities%2520of%2520MLLMs%2520to%2520initialize%2520object%2520pose%250Aand%2520scale.%2520A%2520hierarchical%252C%2520spatially%2520aware%2520refinement%2520stage%2520further%2520integrates%250Aspatial%2520semantics%2520and%2520MLLM-inferred%2520priors%2520to%2520enhance%2520placement.%2520Finally%252C%2520the%250Aappearance%2520of%2520the%2520object%2520is%2520improved%2520using%2520the%2520inserted-object%2520image%2520to%2520enhance%250Avisual%2520fidelity.%2520Experimental%2520results%2520demonstrate%2520that%2520FreeInsert%2520achieves%250Asemantically%2520coherent%252C%2520spatially%2520precise%252C%2520and%2520visually%2520realistic%25203D%2520insertions%250Awithout%2520relying%2520on%2520spatial%2520priors%252C%2520offering%2520a%2520user-friendly%2520and%2520flexible%250Aediting%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeInsert%3A%20Disentangled%20Text-Guided%20Object%20Insertion%20in%203D%20Gaussian%0A%20%20Scene%20without%20Spatial%20Priors&entry.906535625=Chenxi%20Li%20and%20Weijie%20Wang%20and%20Qiang%20Li%20and%20Bruno%20Lepri%20and%20Nicu%20Sebe%20and%20Weizhi%20Nie&entry.1292438233=%20%20Text-driven%20object%20insertion%20in%203D%20scenes%20is%20an%20emerging%20task%20that%20enables%0Aintuitive%20scene%20editing%20through%20natural%20language.%20However%2C%20existing%202D%0Aediting-based%20methods%20often%20rely%20on%20spatial%20priors%20such%20as%202D%20masks%20or%203D%0Abounding%20boxes%2C%20and%20they%20struggle%20to%20ensure%20consistency%20of%20the%20inserted%20object.%0AThese%20limitations%20hinder%20flexibility%20and%20scalability%20in%20real-world%0Aapplications.%20In%20this%20paper%2C%20we%20propose%20FreeInsert%2C%20a%20novel%20framework%20that%0Aleverages%20foundation%20models%20including%20MLLMs%2C%20LGMs%2C%20and%20diffusion%20models%20to%0Adisentangle%20object%20generation%20from%20spatial%20placement.%20This%20enables%20unsupervised%0Aand%20flexible%20object%20insertion%20in%203D%20scenes%20without%20spatial%20priors.%20FreeInsert%0Astarts%20with%20an%20MLLM-based%20parser%20that%20extracts%20structured%20semantics%2C%20including%0Aobject%20types%2C%20spatial%20relationships%2C%20and%20attachment%20regions%2C%20from%20user%0Ainstructions.%20These%20semantics%20guide%20both%20the%20reconstruction%20of%20the%20inserted%0Aobject%20for%203D%20consistency%20and%20the%20learning%20of%20its%20degrees%20of%20freedom.%20We%0Aleverage%20the%20spatial%20reasoning%20capabilities%20of%20MLLMs%20to%20initialize%20object%20pose%0Aand%20scale.%20A%20hierarchical%2C%20spatially%20aware%20refinement%20stage%20further%20integrates%0Aspatial%20semantics%20and%20MLLM-inferred%20priors%20to%20enhance%20placement.%20Finally%2C%20the%0Aappearance%20of%20the%20object%20is%20improved%20using%20the%20inserted-object%20image%20to%20enhance%0Avisual%20fidelity.%20Experimental%20results%20demonstrate%20that%20FreeInsert%20achieves%0Asemantically%20coherent%2C%20spatially%20precise%2C%20and%20visually%20realistic%203D%20insertions%0Awithout%20relying%20on%20spatial%20priors%2C%20offering%20a%20user-friendly%20and%20flexible%0Aediting%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01322v1&entry.124074799=Read"},
{"title": "An Automated Pipeline for Few-Shot Bird Call Classification: A Case\n  Study with the Tooth-Billed Pigeon", "author": "Abhishek Jana and Moeumu Uili and James Atherton and Mark O'Brien and Joe Wood and Leandra Brickson", "abstract": "  This paper presents an automated one-shot bird call classification pipeline\ndesigned for rare species absent from large publicly available classifiers like\nBirdNET and Perch. While these models excel at detecting common birds with\nabundant training data, they lack options for species with only 1-3 known\nrecordings-a critical limitation for conservationists monitoring the last\nremaining individuals of endangered birds. To address this, we leverage the\nembedding space of large bird classification networks and develop a classifier\nusing cosine similarity, combined with filtering and denoising preprocessing\ntechniques, to optimize detection with minimal training data. We evaluate\nvarious embedding spaces using clustering metrics and validate our approach in\nboth a simulated scenario with Xeno-Canto recordings and a real-world test on\nthe critically endangered tooth-billed pigeon (Didunculus strigirostris), which\nhas no existing classifiers and only three confirmed recordings. The final\nmodel achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon\ncalls, making it practical for use in the field. This open-source system\nprovides a practical tool for conservationists seeking to detect and monitor\nrare species on the brink of extinction.\n", "link": "http://arxiv.org/abs/2504.16276v2", "date": "2025-05-02", "relevancy": 2.3096, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4846}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4548}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Automated%20Pipeline%20for%20Few-Shot%20Bird%20Call%20Classification%3A%20A%20Case%0A%20%20Study%20with%20the%20Tooth-Billed%20Pigeon&body=Title%3A%20An%20Automated%20Pipeline%20for%20Few-Shot%20Bird%20Call%20Classification%3A%20A%20Case%0A%20%20Study%20with%20the%20Tooth-Billed%20Pigeon%0AAuthor%3A%20Abhishek%20Jana%20and%20Moeumu%20Uili%20and%20James%20Atherton%20and%20Mark%20O%27Brien%20and%20Joe%20Wood%20and%20Leandra%20Brickson%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20automated%20one-shot%20bird%20call%20classification%20pipeline%0Adesigned%20for%20rare%20species%20absent%20from%20large%20publicly%20available%20classifiers%20like%0ABirdNET%20and%20Perch.%20While%20these%20models%20excel%20at%20detecting%20common%20birds%20with%0Aabundant%20training%20data%2C%20they%20lack%20options%20for%20species%20with%20only%201-3%20known%0Arecordings-a%20critical%20limitation%20for%20conservationists%20monitoring%20the%20last%0Aremaining%20individuals%20of%20endangered%20birds.%20To%20address%20this%2C%20we%20leverage%20the%0Aembedding%20space%20of%20large%20bird%20classification%20networks%20and%20develop%20a%20classifier%0Ausing%20cosine%20similarity%2C%20combined%20with%20filtering%20and%20denoising%20preprocessing%0Atechniques%2C%20to%20optimize%20detection%20with%20minimal%20training%20data.%20We%20evaluate%0Avarious%20embedding%20spaces%20using%20clustering%20metrics%20and%20validate%20our%20approach%20in%0Aboth%20a%20simulated%20scenario%20with%20Xeno-Canto%20recordings%20and%20a%20real-world%20test%20on%0Athe%20critically%20endangered%20tooth-billed%20pigeon%20%28Didunculus%20strigirostris%29%2C%20which%0Ahas%20no%20existing%20classifiers%20and%20only%20three%20confirmed%20recordings.%20The%20final%0Amodel%20achieved%201.0%20recall%20and%200.95%20accuracy%20in%20detecting%20tooth-billed%20pigeon%0Acalls%2C%20making%20it%20practical%20for%20use%20in%20the%20field.%20This%20open-source%20system%0Aprovides%20a%20practical%20tool%20for%20conservationists%20seeking%20to%20detect%20and%20monitor%0Arare%20species%20on%20the%20brink%20of%20extinction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16276v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Automated%2520Pipeline%2520for%2520Few-Shot%2520Bird%2520Call%2520Classification%253A%2520A%2520Case%250A%2520%2520Study%2520with%2520the%2520Tooth-Billed%2520Pigeon%26entry.906535625%3DAbhishek%2520Jana%2520and%2520Moeumu%2520Uili%2520and%2520James%2520Atherton%2520and%2520Mark%2520O%2527Brien%2520and%2520Joe%2520Wood%2520and%2520Leandra%2520Brickson%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520automated%2520one-shot%2520bird%2520call%2520classification%2520pipeline%250Adesigned%2520for%2520rare%2520species%2520absent%2520from%2520large%2520publicly%2520available%2520classifiers%2520like%250ABirdNET%2520and%2520Perch.%2520While%2520these%2520models%2520excel%2520at%2520detecting%2520common%2520birds%2520with%250Aabundant%2520training%2520data%252C%2520they%2520lack%2520options%2520for%2520species%2520with%2520only%25201-3%2520known%250Arecordings-a%2520critical%2520limitation%2520for%2520conservationists%2520monitoring%2520the%2520last%250Aremaining%2520individuals%2520of%2520endangered%2520birds.%2520To%2520address%2520this%252C%2520we%2520leverage%2520the%250Aembedding%2520space%2520of%2520large%2520bird%2520classification%2520networks%2520and%2520develop%2520a%2520classifier%250Ausing%2520cosine%2520similarity%252C%2520combined%2520with%2520filtering%2520and%2520denoising%2520preprocessing%250Atechniques%252C%2520to%2520optimize%2520detection%2520with%2520minimal%2520training%2520data.%2520We%2520evaluate%250Avarious%2520embedding%2520spaces%2520using%2520clustering%2520metrics%2520and%2520validate%2520our%2520approach%2520in%250Aboth%2520a%2520simulated%2520scenario%2520with%2520Xeno-Canto%2520recordings%2520and%2520a%2520real-world%2520test%2520on%250Athe%2520critically%2520endangered%2520tooth-billed%2520pigeon%2520%2528Didunculus%2520strigirostris%2529%252C%2520which%250Ahas%2520no%2520existing%2520classifiers%2520and%2520only%2520three%2520confirmed%2520recordings.%2520The%2520final%250Amodel%2520achieved%25201.0%2520recall%2520and%25200.95%2520accuracy%2520in%2520detecting%2520tooth-billed%2520pigeon%250Acalls%252C%2520making%2520it%2520practical%2520for%2520use%2520in%2520the%2520field.%2520This%2520open-source%2520system%250Aprovides%2520a%2520practical%2520tool%2520for%2520conservationists%2520seeking%2520to%2520detect%2520and%2520monitor%250Arare%2520species%2520on%2520the%2520brink%2520of%2520extinction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16276v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Automated%20Pipeline%20for%20Few-Shot%20Bird%20Call%20Classification%3A%20A%20Case%0A%20%20Study%20with%20the%20Tooth-Billed%20Pigeon&entry.906535625=Abhishek%20Jana%20and%20Moeumu%20Uili%20and%20James%20Atherton%20and%20Mark%20O%27Brien%20and%20Joe%20Wood%20and%20Leandra%20Brickson&entry.1292438233=%20%20This%20paper%20presents%20an%20automated%20one-shot%20bird%20call%20classification%20pipeline%0Adesigned%20for%20rare%20species%20absent%20from%20large%20publicly%20available%20classifiers%20like%0ABirdNET%20and%20Perch.%20While%20these%20models%20excel%20at%20detecting%20common%20birds%20with%0Aabundant%20training%20data%2C%20they%20lack%20options%20for%20species%20with%20only%201-3%20known%0Arecordings-a%20critical%20limitation%20for%20conservationists%20monitoring%20the%20last%0Aremaining%20individuals%20of%20endangered%20birds.%20To%20address%20this%2C%20we%20leverage%20the%0Aembedding%20space%20of%20large%20bird%20classification%20networks%20and%20develop%20a%20classifier%0Ausing%20cosine%20similarity%2C%20combined%20with%20filtering%20and%20denoising%20preprocessing%0Atechniques%2C%20to%20optimize%20detection%20with%20minimal%20training%20data.%20We%20evaluate%0Avarious%20embedding%20spaces%20using%20clustering%20metrics%20and%20validate%20our%20approach%20in%0Aboth%20a%20simulated%20scenario%20with%20Xeno-Canto%20recordings%20and%20a%20real-world%20test%20on%0Athe%20critically%20endangered%20tooth-billed%20pigeon%20%28Didunculus%20strigirostris%29%2C%20which%0Ahas%20no%20existing%20classifiers%20and%20only%20three%20confirmed%20recordings.%20The%20final%0Amodel%20achieved%201.0%20recall%20and%200.95%20accuracy%20in%20detecting%20tooth-billed%20pigeon%0Acalls%2C%20making%20it%20practical%20for%20use%20in%20the%20field.%20This%20open-source%20system%0Aprovides%20a%20practical%20tool%20for%20conservationists%20seeking%20to%20detect%20and%20monitor%0Arare%20species%20on%20the%20brink%20of%20extinction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16276v2&entry.124074799=Read"},
{"title": "High Dynamic Range Novel View Synthesis with Single Exposure", "author": "Kaixuan Zhang and Hu Wang and Minxian Li and Mingwu Ren and Mao Ye and Xiatian Zhu", "abstract": "  High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D\nscene HDR model from Low Dynamic Range (LDR) imagery. Typically,\nmultiple-exposure LDR images are employed to capture a wider range of\nbrightness levels in a scene, as a single LDR image cannot represent both the\nbrightest and darkest regions simultaneously. While effective, this\nmultiple-exposure HDR-NVS approach has significant limitations, including\nsusceptibility to motion artifacts (e.g., ghosting and blurring), high capture\nand storage costs. To overcome these challenges, we introduce, for the first\ntime, the single-exposure HDR-NVS problem, where only single exposure LDR\nimages are available during training. We further introduce a novel approach,\nMono-HDR-3D, featuring two dedicated modules formulated by the LDR image\nformation principles, one for converting LDR colors to HDR counterparts, and\nthe other for transforming HDR images to LDR format so that unsupervised\nlearning is enabled in a closed loop. Designed as a meta-algorithm, our\napproach can be seamlessly integrated with existing NVS models. Extensive\nexperiments show that Mono-HDR-3D significantly outperforms previous methods.\nSource code will be released.\n", "link": "http://arxiv.org/abs/2505.01212v1", "date": "2025-05-02", "relevancy": 2.3089, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5848}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5763}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20with%20Single%20Exposure&body=Title%3A%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20with%20Single%20Exposure%0AAuthor%3A%20Kaixuan%20Zhang%20and%20Hu%20Wang%20and%20Minxian%20Li%20and%20Mingwu%20Ren%20and%20Mao%20Ye%20and%20Xiatian%20Zhu%0AAbstract%3A%20%20%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20%28HDR-NVS%29%20aims%20to%20establish%20a%203D%0Ascene%20HDR%20model%20from%20Low%20Dynamic%20Range%20%28LDR%29%20imagery.%20Typically%2C%0Amultiple-exposure%20LDR%20images%20are%20employed%20to%20capture%20a%20wider%20range%20of%0Abrightness%20levels%20in%20a%20scene%2C%20as%20a%20single%20LDR%20image%20cannot%20represent%20both%20the%0Abrightest%20and%20darkest%20regions%20simultaneously.%20While%20effective%2C%20this%0Amultiple-exposure%20HDR-NVS%20approach%20has%20significant%20limitations%2C%20including%0Asusceptibility%20to%20motion%20artifacts%20%28e.g.%2C%20ghosting%20and%20blurring%29%2C%20high%20capture%0Aand%20storage%20costs.%20To%20overcome%20these%20challenges%2C%20we%20introduce%2C%20for%20the%20first%0Atime%2C%20the%20single-exposure%20HDR-NVS%20problem%2C%20where%20only%20single%20exposure%20LDR%0Aimages%20are%20available%20during%20training.%20We%20further%20introduce%20a%20novel%20approach%2C%0AMono-HDR-3D%2C%20featuring%20two%20dedicated%20modules%20formulated%20by%20the%20LDR%20image%0Aformation%20principles%2C%20one%20for%20converting%20LDR%20colors%20to%20HDR%20counterparts%2C%20and%0Athe%20other%20for%20transforming%20HDR%20images%20to%20LDR%20format%20so%20that%20unsupervised%0Alearning%20is%20enabled%20in%20a%20closed%20loop.%20Designed%20as%20a%20meta-algorithm%2C%20our%0Aapproach%20can%20be%20seamlessly%20integrated%20with%20existing%20NVS%20models.%20Extensive%0Aexperiments%20show%20that%20Mono-HDR-3D%20significantly%20outperforms%20previous%20methods.%0ASource%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh%2520Dynamic%2520Range%2520Novel%2520View%2520Synthesis%2520with%2520Single%2520Exposure%26entry.906535625%3DKaixuan%2520Zhang%2520and%2520Hu%2520Wang%2520and%2520Minxian%2520Li%2520and%2520Mingwu%2520Ren%2520and%2520Mao%2520Ye%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3D%2520%2520High%2520Dynamic%2520Range%2520Novel%2520View%2520Synthesis%2520%2528HDR-NVS%2529%2520aims%2520to%2520establish%2520a%25203D%250Ascene%2520HDR%2520model%2520from%2520Low%2520Dynamic%2520Range%2520%2528LDR%2529%2520imagery.%2520Typically%252C%250Amultiple-exposure%2520LDR%2520images%2520are%2520employed%2520to%2520capture%2520a%2520wider%2520range%2520of%250Abrightness%2520levels%2520in%2520a%2520scene%252C%2520as%2520a%2520single%2520LDR%2520image%2520cannot%2520represent%2520both%2520the%250Abrightest%2520and%2520darkest%2520regions%2520simultaneously.%2520While%2520effective%252C%2520this%250Amultiple-exposure%2520HDR-NVS%2520approach%2520has%2520significant%2520limitations%252C%2520including%250Asusceptibility%2520to%2520motion%2520artifacts%2520%2528e.g.%252C%2520ghosting%2520and%2520blurring%2529%252C%2520high%2520capture%250Aand%2520storage%2520costs.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%252C%2520for%2520the%2520first%250Atime%252C%2520the%2520single-exposure%2520HDR-NVS%2520problem%252C%2520where%2520only%2520single%2520exposure%2520LDR%250Aimages%2520are%2520available%2520during%2520training.%2520We%2520further%2520introduce%2520a%2520novel%2520approach%252C%250AMono-HDR-3D%252C%2520featuring%2520two%2520dedicated%2520modules%2520formulated%2520by%2520the%2520LDR%2520image%250Aformation%2520principles%252C%2520one%2520for%2520converting%2520LDR%2520colors%2520to%2520HDR%2520counterparts%252C%2520and%250Athe%2520other%2520for%2520transforming%2520HDR%2520images%2520to%2520LDR%2520format%2520so%2520that%2520unsupervised%250Alearning%2520is%2520enabled%2520in%2520a%2520closed%2520loop.%2520Designed%2520as%2520a%2520meta-algorithm%252C%2520our%250Aapproach%2520can%2520be%2520seamlessly%2520integrated%2520with%2520existing%2520NVS%2520models.%2520Extensive%250Aexperiments%2520show%2520that%2520Mono-HDR-3D%2520significantly%2520outperforms%2520previous%2520methods.%250ASource%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High%20Dynamic%20Range%20Novel%20View%20Synthesis%20with%20Single%20Exposure&entry.906535625=Kaixuan%20Zhang%20and%20Hu%20Wang%20and%20Minxian%20Li%20and%20Mingwu%20Ren%20and%20Mao%20Ye%20and%20Xiatian%20Zhu&entry.1292438233=%20%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20%28HDR-NVS%29%20aims%20to%20establish%20a%203D%0Ascene%20HDR%20model%20from%20Low%20Dynamic%20Range%20%28LDR%29%20imagery.%20Typically%2C%0Amultiple-exposure%20LDR%20images%20are%20employed%20to%20capture%20a%20wider%20range%20of%0Abrightness%20levels%20in%20a%20scene%2C%20as%20a%20single%20LDR%20image%20cannot%20represent%20both%20the%0Abrightest%20and%20darkest%20regions%20simultaneously.%20While%20effective%2C%20this%0Amultiple-exposure%20HDR-NVS%20approach%20has%20significant%20limitations%2C%20including%0Asusceptibility%20to%20motion%20artifacts%20%28e.g.%2C%20ghosting%20and%20blurring%29%2C%20high%20capture%0Aand%20storage%20costs.%20To%20overcome%20these%20challenges%2C%20we%20introduce%2C%20for%20the%20first%0Atime%2C%20the%20single-exposure%20HDR-NVS%20problem%2C%20where%20only%20single%20exposure%20LDR%0Aimages%20are%20available%20during%20training.%20We%20further%20introduce%20a%20novel%20approach%2C%0AMono-HDR-3D%2C%20featuring%20two%20dedicated%20modules%20formulated%20by%20the%20LDR%20image%0Aformation%20principles%2C%20one%20for%20converting%20LDR%20colors%20to%20HDR%20counterparts%2C%20and%0Athe%20other%20for%20transforming%20HDR%20images%20to%20LDR%20format%20so%20that%20unsupervised%0Alearning%20is%20enabled%20in%20a%20closed%20loop.%20Designed%20as%20a%20meta-algorithm%2C%20our%0Aapproach%20can%20be%20seamlessly%20integrated%20with%20existing%20NVS%20models.%20Extensive%0Aexperiments%20show%20that%20Mono-HDR-3D%20significantly%20outperforms%20previous%20methods.%0ASource%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01212v1&entry.124074799=Read"},
{"title": "FreePCA: Integrating Consistency Information across Long-short Frames in\n  Training-free Long Video Generation via Principal Component Analysis", "author": "Jiangtong Tan and Hu Yu and Jie Huang and Jie Xiao and Feng Zhao", "abstract": "  Long video generation involves generating extended videos using models\ntrained on short videos, suffering from distribution shifts due to varying\nframe counts. It necessitates the use of local information from the original\nshort frames to enhance visual and motion quality, and global information from\nthe entire long frames to ensure appearance consistency. Existing training-free\nmethods struggle to effectively integrate the benefits of both, as appearance\nand motion in videos are closely coupled, leading to motion inconsistency and\nvisual quality. In this paper, we reveal that global and local information can\nbe precisely decoupled into consistent appearance and motion intensity\ninformation by applying Principal Component Analysis (PCA), allowing for\nrefined complementary integration of global consistency and local quality. With\nthis insight, we propose FreePCA, a training-free long video generation\nparadigm based on PCA that simultaneously achieves high consistency and\nquality. Concretely, we decouple consistent appearance and motion intensity\nfeatures by measuring cosine similarity in the principal component space.\nCritically, we progressively integrate these features to preserve original\nquality and ensure smooth transitions, while further enhancing consistency by\nreusing the mean statistics of the initial noise. Experiments demonstrate that\nFreePCA can be applied to various video diffusion models without requiring\ntraining, leading to substantial improvements. Code is available at\nhttps://github.com/JosephTiTan/FreePCA.\n", "link": "http://arxiv.org/abs/2505.01172v1", "date": "2025-05-02", "relevancy": 2.3047, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.59}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5746}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreePCA%3A%20Integrating%20Consistency%20Information%20across%20Long-short%20Frames%20in%0A%20%20Training-free%20Long%20Video%20Generation%20via%20Principal%20Component%20Analysis&body=Title%3A%20FreePCA%3A%20Integrating%20Consistency%20Information%20across%20Long-short%20Frames%20in%0A%20%20Training-free%20Long%20Video%20Generation%20via%20Principal%20Component%20Analysis%0AAuthor%3A%20Jiangtong%20Tan%20and%20Hu%20Yu%20and%20Jie%20Huang%20and%20Jie%20Xiao%20and%20Feng%20Zhao%0AAbstract%3A%20%20%20Long%20video%20generation%20involves%20generating%20extended%20videos%20using%20models%0Atrained%20on%20short%20videos%2C%20suffering%20from%20distribution%20shifts%20due%20to%20varying%0Aframe%20counts.%20It%20necessitates%20the%20use%20of%20local%20information%20from%20the%20original%0Ashort%20frames%20to%20enhance%20visual%20and%20motion%20quality%2C%20and%20global%20information%20from%0Athe%20entire%20long%20frames%20to%20ensure%20appearance%20consistency.%20Existing%20training-free%0Amethods%20struggle%20to%20effectively%20integrate%20the%20benefits%20of%20both%2C%20as%20appearance%0Aand%20motion%20in%20videos%20are%20closely%20coupled%2C%20leading%20to%20motion%20inconsistency%20and%0Avisual%20quality.%20In%20this%20paper%2C%20we%20reveal%20that%20global%20and%20local%20information%20can%0Abe%20precisely%20decoupled%20into%20consistent%20appearance%20and%20motion%20intensity%0Ainformation%20by%20applying%20Principal%20Component%20Analysis%20%28PCA%29%2C%20allowing%20for%0Arefined%20complementary%20integration%20of%20global%20consistency%20and%20local%20quality.%20With%0Athis%20insight%2C%20we%20propose%20FreePCA%2C%20a%20training-free%20long%20video%20generation%0Aparadigm%20based%20on%20PCA%20that%20simultaneously%20achieves%20high%20consistency%20and%0Aquality.%20Concretely%2C%20we%20decouple%20consistent%20appearance%20and%20motion%20intensity%0Afeatures%20by%20measuring%20cosine%20similarity%20in%20the%20principal%20component%20space.%0ACritically%2C%20we%20progressively%20integrate%20these%20features%20to%20preserve%20original%0Aquality%20and%20ensure%20smooth%20transitions%2C%20while%20further%20enhancing%20consistency%20by%0Areusing%20the%20mean%20statistics%20of%20the%20initial%20noise.%20Experiments%20demonstrate%20that%0AFreePCA%20can%20be%20applied%20to%20various%20video%20diffusion%20models%20without%20requiring%0Atraining%2C%20leading%20to%20substantial%20improvements.%20Code%20is%20available%20at%0Ahttps%3A//github.com/JosephTiTan/FreePCA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreePCA%253A%2520Integrating%2520Consistency%2520Information%2520across%2520Long-short%2520Frames%2520in%250A%2520%2520Training-free%2520Long%2520Video%2520Generation%2520via%2520Principal%2520Component%2520Analysis%26entry.906535625%3DJiangtong%2520Tan%2520and%2520Hu%2520Yu%2520and%2520Jie%2520Huang%2520and%2520Jie%2520Xiao%2520and%2520Feng%2520Zhao%26entry.1292438233%3D%2520%2520Long%2520video%2520generation%2520involves%2520generating%2520extended%2520videos%2520using%2520models%250Atrained%2520on%2520short%2520videos%252C%2520suffering%2520from%2520distribution%2520shifts%2520due%2520to%2520varying%250Aframe%2520counts.%2520It%2520necessitates%2520the%2520use%2520of%2520local%2520information%2520from%2520the%2520original%250Ashort%2520frames%2520to%2520enhance%2520visual%2520and%2520motion%2520quality%252C%2520and%2520global%2520information%2520from%250Athe%2520entire%2520long%2520frames%2520to%2520ensure%2520appearance%2520consistency.%2520Existing%2520training-free%250Amethods%2520struggle%2520to%2520effectively%2520integrate%2520the%2520benefits%2520of%2520both%252C%2520as%2520appearance%250Aand%2520motion%2520in%2520videos%2520are%2520closely%2520coupled%252C%2520leading%2520to%2520motion%2520inconsistency%2520and%250Avisual%2520quality.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520that%2520global%2520and%2520local%2520information%2520can%250Abe%2520precisely%2520decoupled%2520into%2520consistent%2520appearance%2520and%2520motion%2520intensity%250Ainformation%2520by%2520applying%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%252C%2520allowing%2520for%250Arefined%2520complementary%2520integration%2520of%2520global%2520consistency%2520and%2520local%2520quality.%2520With%250Athis%2520insight%252C%2520we%2520propose%2520FreePCA%252C%2520a%2520training-free%2520long%2520video%2520generation%250Aparadigm%2520based%2520on%2520PCA%2520that%2520simultaneously%2520achieves%2520high%2520consistency%2520and%250Aquality.%2520Concretely%252C%2520we%2520decouple%2520consistent%2520appearance%2520and%2520motion%2520intensity%250Afeatures%2520by%2520measuring%2520cosine%2520similarity%2520in%2520the%2520principal%2520component%2520space.%250ACritically%252C%2520we%2520progressively%2520integrate%2520these%2520features%2520to%2520preserve%2520original%250Aquality%2520and%2520ensure%2520smooth%2520transitions%252C%2520while%2520further%2520enhancing%2520consistency%2520by%250Areusing%2520the%2520mean%2520statistics%2520of%2520the%2520initial%2520noise.%2520Experiments%2520demonstrate%2520that%250AFreePCA%2520can%2520be%2520applied%2520to%2520various%2520video%2520diffusion%2520models%2520without%2520requiring%250Atraining%252C%2520leading%2520to%2520substantial%2520improvements.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/JosephTiTan/FreePCA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreePCA%3A%20Integrating%20Consistency%20Information%20across%20Long-short%20Frames%20in%0A%20%20Training-free%20Long%20Video%20Generation%20via%20Principal%20Component%20Analysis&entry.906535625=Jiangtong%20Tan%20and%20Hu%20Yu%20and%20Jie%20Huang%20and%20Jie%20Xiao%20and%20Feng%20Zhao&entry.1292438233=%20%20Long%20video%20generation%20involves%20generating%20extended%20videos%20using%20models%0Atrained%20on%20short%20videos%2C%20suffering%20from%20distribution%20shifts%20due%20to%20varying%0Aframe%20counts.%20It%20necessitates%20the%20use%20of%20local%20information%20from%20the%20original%0Ashort%20frames%20to%20enhance%20visual%20and%20motion%20quality%2C%20and%20global%20information%20from%0Athe%20entire%20long%20frames%20to%20ensure%20appearance%20consistency.%20Existing%20training-free%0Amethods%20struggle%20to%20effectively%20integrate%20the%20benefits%20of%20both%2C%20as%20appearance%0Aand%20motion%20in%20videos%20are%20closely%20coupled%2C%20leading%20to%20motion%20inconsistency%20and%0Avisual%20quality.%20In%20this%20paper%2C%20we%20reveal%20that%20global%20and%20local%20information%20can%0Abe%20precisely%20decoupled%20into%20consistent%20appearance%20and%20motion%20intensity%0Ainformation%20by%20applying%20Principal%20Component%20Analysis%20%28PCA%29%2C%20allowing%20for%0Arefined%20complementary%20integration%20of%20global%20consistency%20and%20local%20quality.%20With%0Athis%20insight%2C%20we%20propose%20FreePCA%2C%20a%20training-free%20long%20video%20generation%0Aparadigm%20based%20on%20PCA%20that%20simultaneously%20achieves%20high%20consistency%20and%0Aquality.%20Concretely%2C%20we%20decouple%20consistent%20appearance%20and%20motion%20intensity%0Afeatures%20by%20measuring%20cosine%20similarity%20in%20the%20principal%20component%20space.%0ACritically%2C%20we%20progressively%20integrate%20these%20features%20to%20preserve%20original%0Aquality%20and%20ensure%20smooth%20transitions%2C%20while%20further%20enhancing%20consistency%20by%0Areusing%20the%20mean%20statistics%20of%20the%20initial%20noise.%20Experiments%20demonstrate%20that%0AFreePCA%20can%20be%20applied%20to%20various%20video%20diffusion%20models%20without%20requiring%0Atraining%2C%20leading%20to%20substantial%20improvements.%20Code%20is%20available%20at%0Ahttps%3A//github.com/JosephTiTan/FreePCA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01172v1&entry.124074799=Read"},
{"title": "Global Collinearity-aware Polygonizer for Polygonal Building Mapping in\n  Remote Sensing", "author": "Fahong Zhang and Yilei Shi and Xiao Xiang Zhu", "abstract": "  This paper addresses the challenge of mapping polygonal buildings from remote\nsensing images and introduces a novel algorithm, the Global Collinearity-aware\nPolygonizer (GCP). GCP, built upon an instance segmentation framework,\nprocesses binary masks produced by any instance segmentation model. The\nalgorithm begins by collecting polylines sampled along the contours of the\nbinary masks. These polylines undergo a refinement process using a\ntransformer-based regression module to ensure they accurately fit the contours\nof the targeted building instances. Subsequently, a collinearity-aware polygon\nsimplification module simplifies these refined polylines and generate the final\npolygon representation. This module employs dynamic programming technique to\noptimize an objective function that balances the simplicity and fidelity of the\npolygons, achieving globally optimal solutions. Furthermore, the optimized\ncollinearity-aware objective is seamlessly integrated into network training,\nenhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has\nbeen validated on two public benchmarks for polygonal building mapping. Further\nexperiments reveal that applying the collinearity-aware polygon simplification\nmodule to arbitrary polylines, without prior knowledge, enhances accuracy over\ntraditional methods such as the Douglas-Peucker algorithm. This finding\nunderscores the broad applicability of GCP. The code for the proposed method\nwill be made available at https://github.com/zhu-xlab.\n", "link": "http://arxiv.org/abs/2505.01385v1", "date": "2025-05-02", "relevancy": 2.2386, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5916}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5448}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Collinearity-aware%20Polygonizer%20for%20Polygonal%20Building%20Mapping%20in%0A%20%20Remote%20Sensing&body=Title%3A%20Global%20Collinearity-aware%20Polygonizer%20for%20Polygonal%20Building%20Mapping%20in%0A%20%20Remote%20Sensing%0AAuthor%3A%20Fahong%20Zhang%20and%20Yilei%20Shi%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20mapping%20polygonal%20buildings%20from%20remote%0Asensing%20images%20and%20introduces%20a%20novel%20algorithm%2C%20the%20Global%20Collinearity-aware%0APolygonizer%20%28GCP%29.%20GCP%2C%20built%20upon%20an%20instance%20segmentation%20framework%2C%0Aprocesses%20binary%20masks%20produced%20by%20any%20instance%20segmentation%20model.%20The%0Aalgorithm%20begins%20by%20collecting%20polylines%20sampled%20along%20the%20contours%20of%20the%0Abinary%20masks.%20These%20polylines%20undergo%20a%20refinement%20process%20using%20a%0Atransformer-based%20regression%20module%20to%20ensure%20they%20accurately%20fit%20the%20contours%0Aof%20the%20targeted%20building%20instances.%20Subsequently%2C%20a%20collinearity-aware%20polygon%0Asimplification%20module%20simplifies%20these%20refined%20polylines%20and%20generate%20the%20final%0Apolygon%20representation.%20This%20module%20employs%20dynamic%20programming%20technique%20to%0Aoptimize%20an%20objective%20function%20that%20balances%20the%20simplicity%20and%20fidelity%20of%20the%0Apolygons%2C%20achieving%20globally%20optimal%20solutions.%20Furthermore%2C%20the%20optimized%0Acollinearity-aware%20objective%20is%20seamlessly%20integrated%20into%20network%20training%2C%0Aenhancing%20the%20cohesiveness%20of%20the%20entire%20pipeline.%20The%20effectiveness%20of%20GCP%20has%0Abeen%20validated%20on%20two%20public%20benchmarks%20for%20polygonal%20building%20mapping.%20Further%0Aexperiments%20reveal%20that%20applying%20the%20collinearity-aware%20polygon%20simplification%0Amodule%20to%20arbitrary%20polylines%2C%20without%20prior%20knowledge%2C%20enhances%20accuracy%20over%0Atraditional%20methods%20such%20as%20the%20Douglas-Peucker%20algorithm.%20This%20finding%0Aunderscores%20the%20broad%20applicability%20of%20GCP.%20The%20code%20for%20the%20proposed%20method%0Awill%20be%20made%20available%20at%20https%3A//github.com/zhu-xlab.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Collinearity-aware%2520Polygonizer%2520for%2520Polygonal%2520Building%2520Mapping%2520in%250A%2520%2520Remote%2520Sensing%26entry.906535625%3DFahong%2520Zhang%2520and%2520Yilei%2520Shi%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520mapping%2520polygonal%2520buildings%2520from%2520remote%250Asensing%2520images%2520and%2520introduces%2520a%2520novel%2520algorithm%252C%2520the%2520Global%2520Collinearity-aware%250APolygonizer%2520%2528GCP%2529.%2520GCP%252C%2520built%2520upon%2520an%2520instance%2520segmentation%2520framework%252C%250Aprocesses%2520binary%2520masks%2520produced%2520by%2520any%2520instance%2520segmentation%2520model.%2520The%250Aalgorithm%2520begins%2520by%2520collecting%2520polylines%2520sampled%2520along%2520the%2520contours%2520of%2520the%250Abinary%2520masks.%2520These%2520polylines%2520undergo%2520a%2520refinement%2520process%2520using%2520a%250Atransformer-based%2520regression%2520module%2520to%2520ensure%2520they%2520accurately%2520fit%2520the%2520contours%250Aof%2520the%2520targeted%2520building%2520instances.%2520Subsequently%252C%2520a%2520collinearity-aware%2520polygon%250Asimplification%2520module%2520simplifies%2520these%2520refined%2520polylines%2520and%2520generate%2520the%2520final%250Apolygon%2520representation.%2520This%2520module%2520employs%2520dynamic%2520programming%2520technique%2520to%250Aoptimize%2520an%2520objective%2520function%2520that%2520balances%2520the%2520simplicity%2520and%2520fidelity%2520of%2520the%250Apolygons%252C%2520achieving%2520globally%2520optimal%2520solutions.%2520Furthermore%252C%2520the%2520optimized%250Acollinearity-aware%2520objective%2520is%2520seamlessly%2520integrated%2520into%2520network%2520training%252C%250Aenhancing%2520the%2520cohesiveness%2520of%2520the%2520entire%2520pipeline.%2520The%2520effectiveness%2520of%2520GCP%2520has%250Abeen%2520validated%2520on%2520two%2520public%2520benchmarks%2520for%2520polygonal%2520building%2520mapping.%2520Further%250Aexperiments%2520reveal%2520that%2520applying%2520the%2520collinearity-aware%2520polygon%2520simplification%250Amodule%2520to%2520arbitrary%2520polylines%252C%2520without%2520prior%2520knowledge%252C%2520enhances%2520accuracy%2520over%250Atraditional%2520methods%2520such%2520as%2520the%2520Douglas-Peucker%2520algorithm.%2520This%2520finding%250Aunderscores%2520the%2520broad%2520applicability%2520of%2520GCP.%2520The%2520code%2520for%2520the%2520proposed%2520method%250Awill%2520be%2520made%2520available%2520at%2520https%253A//github.com/zhu-xlab.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Collinearity-aware%20Polygonizer%20for%20Polygonal%20Building%20Mapping%20in%0A%20%20Remote%20Sensing&entry.906535625=Fahong%20Zhang%20and%20Yilei%20Shi%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20mapping%20polygonal%20buildings%20from%20remote%0Asensing%20images%20and%20introduces%20a%20novel%20algorithm%2C%20the%20Global%20Collinearity-aware%0APolygonizer%20%28GCP%29.%20GCP%2C%20built%20upon%20an%20instance%20segmentation%20framework%2C%0Aprocesses%20binary%20masks%20produced%20by%20any%20instance%20segmentation%20model.%20The%0Aalgorithm%20begins%20by%20collecting%20polylines%20sampled%20along%20the%20contours%20of%20the%0Abinary%20masks.%20These%20polylines%20undergo%20a%20refinement%20process%20using%20a%0Atransformer-based%20regression%20module%20to%20ensure%20they%20accurately%20fit%20the%20contours%0Aof%20the%20targeted%20building%20instances.%20Subsequently%2C%20a%20collinearity-aware%20polygon%0Asimplification%20module%20simplifies%20these%20refined%20polylines%20and%20generate%20the%20final%0Apolygon%20representation.%20This%20module%20employs%20dynamic%20programming%20technique%20to%0Aoptimize%20an%20objective%20function%20that%20balances%20the%20simplicity%20and%20fidelity%20of%20the%0Apolygons%2C%20achieving%20globally%20optimal%20solutions.%20Furthermore%2C%20the%20optimized%0Acollinearity-aware%20objective%20is%20seamlessly%20integrated%20into%20network%20training%2C%0Aenhancing%20the%20cohesiveness%20of%20the%20entire%20pipeline.%20The%20effectiveness%20of%20GCP%20has%0Abeen%20validated%20on%20two%20public%20benchmarks%20for%20polygonal%20building%20mapping.%20Further%0Aexperiments%20reveal%20that%20applying%20the%20collinearity-aware%20polygon%20simplification%0Amodule%20to%20arbitrary%20polylines%2C%20without%20prior%20knowledge%2C%20enhances%20accuracy%20over%0Atraditional%20methods%20such%20as%20the%20Douglas-Peucker%20algorithm.%20This%20finding%0Aunderscores%20the%20broad%20applicability%20of%20GCP.%20The%20code%20for%20the%20proposed%20method%0Awill%20be%20made%20available%20at%20https%3A//github.com/zhu-xlab.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01385v1&entry.124074799=Read"},
{"title": "Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in\n  Lung CT Imaging", "author": "Elena Mulero Ayll\u00f3n and Massimiliano Mantegna and Linlin Shen and Paolo Soda and Valerio Guarrasi and Matteo Tortora", "abstract": "  Accurate lung tumor segmentation is crucial for improving diagnosis,\ntreatment planning, and patient outcomes in oncology. However, the complexity\nof tumor morphology, size, and location poses significant challenges for\nautomated segmentation. This study presents a comprehensive benchmarking\nanalysis of deep learning-based segmentation models, comparing traditional\narchitectures such as U-Net and DeepLabV3, self-configuring models like nnUNet,\nand foundation models like MedSAM, and MedSAM~2. Evaluating performance across\ntwo lung tumor segmentation datasets, we assess segmentation accuracy and\ncomputational efficiency under various learning paradigms, including few-shot\nlearning and fine-tuning. The results reveal that while traditional models\nstruggle with tumor delineation, foundation models, particularly MedSAM~2,\noutperform them in both accuracy and computational efficiency. These findings\nunderscore the potential of foundation models for lung tumor segmentation,\nhighlighting their applicability in improving clinical workflows and patient\noutcomes.\n", "link": "http://arxiv.org/abs/2505.01239v1", "date": "2025-05-02", "relevancy": 2.218, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Foundation%20Models%20Really%20Segment%20Tumors%3F%20A%20Benchmarking%20Odyssey%20in%0A%20%20Lung%20CT%20Imaging&body=Title%3A%20Can%20Foundation%20Models%20Really%20Segment%20Tumors%3F%20A%20Benchmarking%20Odyssey%20in%0A%20%20Lung%20CT%20Imaging%0AAuthor%3A%20Elena%20Mulero%20Ayll%C3%B3n%20and%20Massimiliano%20Mantegna%20and%20Linlin%20Shen%20and%20Paolo%20Soda%20and%20Valerio%20Guarrasi%20and%20Matteo%20Tortora%0AAbstract%3A%20%20%20Accurate%20lung%20tumor%20segmentation%20is%20crucial%20for%20improving%20diagnosis%2C%0Atreatment%20planning%2C%20and%20patient%20outcomes%20in%20oncology.%20However%2C%20the%20complexity%0Aof%20tumor%20morphology%2C%20size%2C%20and%20location%20poses%20significant%20challenges%20for%0Aautomated%20segmentation.%20This%20study%20presents%20a%20comprehensive%20benchmarking%0Aanalysis%20of%20deep%20learning-based%20segmentation%20models%2C%20comparing%20traditional%0Aarchitectures%20such%20as%20U-Net%20and%20DeepLabV3%2C%20self-configuring%20models%20like%20nnUNet%2C%0Aand%20foundation%20models%20like%20MedSAM%2C%20and%20MedSAM~2.%20Evaluating%20performance%20across%0Atwo%20lung%20tumor%20segmentation%20datasets%2C%20we%20assess%20segmentation%20accuracy%20and%0Acomputational%20efficiency%20under%20various%20learning%20paradigms%2C%20including%20few-shot%0Alearning%20and%20fine-tuning.%20The%20results%20reveal%20that%20while%20traditional%20models%0Astruggle%20with%20tumor%20delineation%2C%20foundation%20models%2C%20particularly%20MedSAM~2%2C%0Aoutperform%20them%20in%20both%20accuracy%20and%20computational%20efficiency.%20These%20findings%0Aunderscore%20the%20potential%20of%20foundation%20models%20for%20lung%20tumor%20segmentation%2C%0Ahighlighting%20their%20applicability%20in%20improving%20clinical%20workflows%20and%20patient%0Aoutcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Foundation%2520Models%2520Really%2520Segment%2520Tumors%253F%2520A%2520Benchmarking%2520Odyssey%2520in%250A%2520%2520Lung%2520CT%2520Imaging%26entry.906535625%3DElena%2520Mulero%2520Ayll%25C3%25B3n%2520and%2520Massimiliano%2520Mantegna%2520and%2520Linlin%2520Shen%2520and%2520Paolo%2520Soda%2520and%2520Valerio%2520Guarrasi%2520and%2520Matteo%2520Tortora%26entry.1292438233%3D%2520%2520Accurate%2520lung%2520tumor%2520segmentation%2520is%2520crucial%2520for%2520improving%2520diagnosis%252C%250Atreatment%2520planning%252C%2520and%2520patient%2520outcomes%2520in%2520oncology.%2520However%252C%2520the%2520complexity%250Aof%2520tumor%2520morphology%252C%2520size%252C%2520and%2520location%2520poses%2520significant%2520challenges%2520for%250Aautomated%2520segmentation.%2520This%2520study%2520presents%2520a%2520comprehensive%2520benchmarking%250Aanalysis%2520of%2520deep%2520learning-based%2520segmentation%2520models%252C%2520comparing%2520traditional%250Aarchitectures%2520such%2520as%2520U-Net%2520and%2520DeepLabV3%252C%2520self-configuring%2520models%2520like%2520nnUNet%252C%250Aand%2520foundation%2520models%2520like%2520MedSAM%252C%2520and%2520MedSAM~2.%2520Evaluating%2520performance%2520across%250Atwo%2520lung%2520tumor%2520segmentation%2520datasets%252C%2520we%2520assess%2520segmentation%2520accuracy%2520and%250Acomputational%2520efficiency%2520under%2520various%2520learning%2520paradigms%252C%2520including%2520few-shot%250Alearning%2520and%2520fine-tuning.%2520The%2520results%2520reveal%2520that%2520while%2520traditional%2520models%250Astruggle%2520with%2520tumor%2520delineation%252C%2520foundation%2520models%252C%2520particularly%2520MedSAM~2%252C%250Aoutperform%2520them%2520in%2520both%2520accuracy%2520and%2520computational%2520efficiency.%2520These%2520findings%250Aunderscore%2520the%2520potential%2520of%2520foundation%2520models%2520for%2520lung%2520tumor%2520segmentation%252C%250Ahighlighting%2520their%2520applicability%2520in%2520improving%2520clinical%2520workflows%2520and%2520patient%250Aoutcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Foundation%20Models%20Really%20Segment%20Tumors%3F%20A%20Benchmarking%20Odyssey%20in%0A%20%20Lung%20CT%20Imaging&entry.906535625=Elena%20Mulero%20Ayll%C3%B3n%20and%20Massimiliano%20Mantegna%20and%20Linlin%20Shen%20and%20Paolo%20Soda%20and%20Valerio%20Guarrasi%20and%20Matteo%20Tortora&entry.1292438233=%20%20Accurate%20lung%20tumor%20segmentation%20is%20crucial%20for%20improving%20diagnosis%2C%0Atreatment%20planning%2C%20and%20patient%20outcomes%20in%20oncology.%20However%2C%20the%20complexity%0Aof%20tumor%20morphology%2C%20size%2C%20and%20location%20poses%20significant%20challenges%20for%0Aautomated%20segmentation.%20This%20study%20presents%20a%20comprehensive%20benchmarking%0Aanalysis%20of%20deep%20learning-based%20segmentation%20models%2C%20comparing%20traditional%0Aarchitectures%20such%20as%20U-Net%20and%20DeepLabV3%2C%20self-configuring%20models%20like%20nnUNet%2C%0Aand%20foundation%20models%20like%20MedSAM%2C%20and%20MedSAM~2.%20Evaluating%20performance%20across%0Atwo%20lung%20tumor%20segmentation%20datasets%2C%20we%20assess%20segmentation%20accuracy%20and%0Acomputational%20efficiency%20under%20various%20learning%20paradigms%2C%20including%20few-shot%0Alearning%20and%20fine-tuning.%20The%20results%20reveal%20that%20while%20traditional%20models%0Astruggle%20with%20tumor%20delineation%2C%20foundation%20models%2C%20particularly%20MedSAM~2%2C%0Aoutperform%20them%20in%20both%20accuracy%20and%20computational%20efficiency.%20These%20findings%0Aunderscore%20the%20potential%20of%20foundation%20models%20for%20lung%20tumor%20segmentation%2C%0Ahighlighting%20their%20applicability%20in%20improving%20clinical%20workflows%20and%20patient%0Aoutcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01239v1&entry.124074799=Read"},
{"title": "Gaussian Differential Private Bootstrap by Subsampling", "author": "Holger Dette and Carina Graw", "abstract": "  Bootstrap is a common tool for quantifying uncertainty in data analysis.\nHowever, besides additional computational costs in the application of the\nbootstrap on massive data, a challenging problem in bootstrap based inference\nunder Differential Privacy consists in the fact that it requires repeated\naccess to the data. As a consequence, bootstrap based differentially private\ninference requires a significant increase of the privacy budget, which on the\nother hand comes with a substantial loss in statistical accuracy.\n  A potential solution to reconcile the conflicting goals of statistical\naccuracy and privacy is to analyze the data under parametric model assumptions\nand in the last decade, several parametric bootstrap methods for inference\nunder privacy have been investigated. However, uncertainty quantification by\nparametric bootstrap is only valid if the the quantities of interest can be\nidentified as the parameters of a statistical model and the imposed model\nassumptions are (at least approximately) satisfied. An alternative to\nparametric methods is the empirical bootstrap that is a widely used tool for\nnon-parametric inference and well studied in the non-private regime. However,\nunder privacy, less insight is available. In this paper, we propose a private\nempirical $m$ out of $n$ bootstrap and validate its consistency and privacy\nguarantees under Gaussian Differential Privacy. Compared to the the private $n$\nout of $n$ bootstrap, our approach has several advantages. First, it comes with\nless computational costs, in particular for massive data. Second, the proposed\nprocedure needs less additional noise in the bootstrap iterations, which leads\nto an improved statistical accuracy while asymptotically guaranteeing the same\nlevel of privacy. Third, we demonstrate much better finite sample properties\ncompared to the currently available procedures.\n", "link": "http://arxiv.org/abs/2505.01197v1", "date": "2025-05-02", "relevancy": 2.193, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4507}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4494}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Differential%20Private%20Bootstrap%20by%20Subsampling&body=Title%3A%20Gaussian%20Differential%20Private%20Bootstrap%20by%20Subsampling%0AAuthor%3A%20Holger%20Dette%20and%20Carina%20Graw%0AAbstract%3A%20%20%20Bootstrap%20is%20a%20common%20tool%20for%20quantifying%20uncertainty%20in%20data%20analysis.%0AHowever%2C%20besides%20additional%20computational%20costs%20in%20the%20application%20of%20the%0Abootstrap%20on%20massive%20data%2C%20a%20challenging%20problem%20in%20bootstrap%20based%20inference%0Aunder%20Differential%20Privacy%20consists%20in%20the%20fact%20that%20it%20requires%20repeated%0Aaccess%20to%20the%20data.%20As%20a%20consequence%2C%20bootstrap%20based%20differentially%20private%0Ainference%20requires%20a%20significant%20increase%20of%20the%20privacy%20budget%2C%20which%20on%20the%0Aother%20hand%20comes%20with%20a%20substantial%20loss%20in%20statistical%20accuracy.%0A%20%20A%20potential%20solution%20to%20reconcile%20the%20conflicting%20goals%20of%20statistical%0Aaccuracy%20and%20privacy%20is%20to%20analyze%20the%20data%20under%20parametric%20model%20assumptions%0Aand%20in%20the%20last%20decade%2C%20several%20parametric%20bootstrap%20methods%20for%20inference%0Aunder%20privacy%20have%20been%20investigated.%20However%2C%20uncertainty%20quantification%20by%0Aparametric%20bootstrap%20is%20only%20valid%20if%20the%20the%20quantities%20of%20interest%20can%20be%0Aidentified%20as%20the%20parameters%20of%20a%20statistical%20model%20and%20the%20imposed%20model%0Aassumptions%20are%20%28at%20least%20approximately%29%20satisfied.%20An%20alternative%20to%0Aparametric%20methods%20is%20the%20empirical%20bootstrap%20that%20is%20a%20widely%20used%20tool%20for%0Anon-parametric%20inference%20and%20well%20studied%20in%20the%20non-private%20regime.%20However%2C%0Aunder%20privacy%2C%20less%20insight%20is%20available.%20In%20this%20paper%2C%20we%20propose%20a%20private%0Aempirical%20%24m%24%20out%20of%20%24n%24%20bootstrap%20and%20validate%20its%20consistency%20and%20privacy%0Aguarantees%20under%20Gaussian%20Differential%20Privacy.%20Compared%20to%20the%20the%20private%20%24n%24%0Aout%20of%20%24n%24%20bootstrap%2C%20our%20approach%20has%20several%20advantages.%20First%2C%20it%20comes%20with%0Aless%20computational%20costs%2C%20in%20particular%20for%20massive%20data.%20Second%2C%20the%20proposed%0Aprocedure%20needs%20less%20additional%20noise%20in%20the%20bootstrap%20iterations%2C%20which%20leads%0Ato%20an%20improved%20statistical%20accuracy%20while%20asymptotically%20guaranteeing%20the%20same%0Alevel%20of%20privacy.%20Third%2C%20we%20demonstrate%20much%20better%20finite%20sample%20properties%0Acompared%20to%20the%20currently%20available%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Differential%2520Private%2520Bootstrap%2520by%2520Subsampling%26entry.906535625%3DHolger%2520Dette%2520and%2520Carina%2520Graw%26entry.1292438233%3D%2520%2520Bootstrap%2520is%2520a%2520common%2520tool%2520for%2520quantifying%2520uncertainty%2520in%2520data%2520analysis.%250AHowever%252C%2520besides%2520additional%2520computational%2520costs%2520in%2520the%2520application%2520of%2520the%250Abootstrap%2520on%2520massive%2520data%252C%2520a%2520challenging%2520problem%2520in%2520bootstrap%2520based%2520inference%250Aunder%2520Differential%2520Privacy%2520consists%2520in%2520the%2520fact%2520that%2520it%2520requires%2520repeated%250Aaccess%2520to%2520the%2520data.%2520As%2520a%2520consequence%252C%2520bootstrap%2520based%2520differentially%2520private%250Ainference%2520requires%2520a%2520significant%2520increase%2520of%2520the%2520privacy%2520budget%252C%2520which%2520on%2520the%250Aother%2520hand%2520comes%2520with%2520a%2520substantial%2520loss%2520in%2520statistical%2520accuracy.%250A%2520%2520A%2520potential%2520solution%2520to%2520reconcile%2520the%2520conflicting%2520goals%2520of%2520statistical%250Aaccuracy%2520and%2520privacy%2520is%2520to%2520analyze%2520the%2520data%2520under%2520parametric%2520model%2520assumptions%250Aand%2520in%2520the%2520last%2520decade%252C%2520several%2520parametric%2520bootstrap%2520methods%2520for%2520inference%250Aunder%2520privacy%2520have%2520been%2520investigated.%2520However%252C%2520uncertainty%2520quantification%2520by%250Aparametric%2520bootstrap%2520is%2520only%2520valid%2520if%2520the%2520the%2520quantities%2520of%2520interest%2520can%2520be%250Aidentified%2520as%2520the%2520parameters%2520of%2520a%2520statistical%2520model%2520and%2520the%2520imposed%2520model%250Aassumptions%2520are%2520%2528at%2520least%2520approximately%2529%2520satisfied.%2520An%2520alternative%2520to%250Aparametric%2520methods%2520is%2520the%2520empirical%2520bootstrap%2520that%2520is%2520a%2520widely%2520used%2520tool%2520for%250Anon-parametric%2520inference%2520and%2520well%2520studied%2520in%2520the%2520non-private%2520regime.%2520However%252C%250Aunder%2520privacy%252C%2520less%2520insight%2520is%2520available.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520private%250Aempirical%2520%2524m%2524%2520out%2520of%2520%2524n%2524%2520bootstrap%2520and%2520validate%2520its%2520consistency%2520and%2520privacy%250Aguarantees%2520under%2520Gaussian%2520Differential%2520Privacy.%2520Compared%2520to%2520the%2520the%2520private%2520%2524n%2524%250Aout%2520of%2520%2524n%2524%2520bootstrap%252C%2520our%2520approach%2520has%2520several%2520advantages.%2520First%252C%2520it%2520comes%2520with%250Aless%2520computational%2520costs%252C%2520in%2520particular%2520for%2520massive%2520data.%2520Second%252C%2520the%2520proposed%250Aprocedure%2520needs%2520less%2520additional%2520noise%2520in%2520the%2520bootstrap%2520iterations%252C%2520which%2520leads%250Ato%2520an%2520improved%2520statistical%2520accuracy%2520while%2520asymptotically%2520guaranteeing%2520the%2520same%250Alevel%2520of%2520privacy.%2520Third%252C%2520we%2520demonstrate%2520much%2520better%2520finite%2520sample%2520properties%250Acompared%2520to%2520the%2520currently%2520available%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Differential%20Private%20Bootstrap%20by%20Subsampling&entry.906535625=Holger%20Dette%20and%20Carina%20Graw&entry.1292438233=%20%20Bootstrap%20is%20a%20common%20tool%20for%20quantifying%20uncertainty%20in%20data%20analysis.%0AHowever%2C%20besides%20additional%20computational%20costs%20in%20the%20application%20of%20the%0Abootstrap%20on%20massive%20data%2C%20a%20challenging%20problem%20in%20bootstrap%20based%20inference%0Aunder%20Differential%20Privacy%20consists%20in%20the%20fact%20that%20it%20requires%20repeated%0Aaccess%20to%20the%20data.%20As%20a%20consequence%2C%20bootstrap%20based%20differentially%20private%0Ainference%20requires%20a%20significant%20increase%20of%20the%20privacy%20budget%2C%20which%20on%20the%0Aother%20hand%20comes%20with%20a%20substantial%20loss%20in%20statistical%20accuracy.%0A%20%20A%20potential%20solution%20to%20reconcile%20the%20conflicting%20goals%20of%20statistical%0Aaccuracy%20and%20privacy%20is%20to%20analyze%20the%20data%20under%20parametric%20model%20assumptions%0Aand%20in%20the%20last%20decade%2C%20several%20parametric%20bootstrap%20methods%20for%20inference%0Aunder%20privacy%20have%20been%20investigated.%20However%2C%20uncertainty%20quantification%20by%0Aparametric%20bootstrap%20is%20only%20valid%20if%20the%20the%20quantities%20of%20interest%20can%20be%0Aidentified%20as%20the%20parameters%20of%20a%20statistical%20model%20and%20the%20imposed%20model%0Aassumptions%20are%20%28at%20least%20approximately%29%20satisfied.%20An%20alternative%20to%0Aparametric%20methods%20is%20the%20empirical%20bootstrap%20that%20is%20a%20widely%20used%20tool%20for%0Anon-parametric%20inference%20and%20well%20studied%20in%20the%20non-private%20regime.%20However%2C%0Aunder%20privacy%2C%20less%20insight%20is%20available.%20In%20this%20paper%2C%20we%20propose%20a%20private%0Aempirical%20%24m%24%20out%20of%20%24n%24%20bootstrap%20and%20validate%20its%20consistency%20and%20privacy%0Aguarantees%20under%20Gaussian%20Differential%20Privacy.%20Compared%20to%20the%20the%20private%20%24n%24%0Aout%20of%20%24n%24%20bootstrap%2C%20our%20approach%20has%20several%20advantages.%20First%2C%20it%20comes%20with%0Aless%20computational%20costs%2C%20in%20particular%20for%20massive%20data.%20Second%2C%20the%20proposed%0Aprocedure%20needs%20less%20additional%20noise%20in%20the%20bootstrap%20iterations%2C%20which%20leads%0Ato%20an%20improved%20statistical%20accuracy%20while%20asymptotically%20guaranteeing%20the%20same%0Alevel%20of%20privacy.%20Third%2C%20we%20demonstrate%20much%20better%20finite%20sample%20properties%0Acompared%20to%20the%20currently%20available%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01197v1&entry.124074799=Read"},
{"title": "AGRO: An Autonomous AI Rover for Precision Agriculture", "author": "Simar Ghumman and Fabio Di Troia and William Andreopoulos and Mark Stamp and Sanjit Rai", "abstract": "  Unmanned Ground Vehicles (UGVs) are emerging as a crucial tool in the world\nof precision agriculture. The combination of UGVs with machine learning allows\nus to find solutions for a range of complex agricultural problems. This\nresearch focuses on developing a UGV capable of autonomously traversing\nagricultural fields and capturing data. The project, known as AGRO (Autonomous\nGround Rover Observer) leverages machine learning, computer vision and other\nsensor technologies. AGRO uses its capabilities to determine pistachio yields,\nperforming self-localization and real-time environmental mapping while avoiding\nobstacles. The main objective of this research work is to automate\nresource-consuming operations so that AGRO can support farmers in making\ndata-driven decisions. Furthermore, AGRO provides a foundation for advanced\nmachine learning techniques as it captures the world around it.\n", "link": "http://arxiv.org/abs/2505.01200v1", "date": "2025-05-02", "relevancy": 2.1573, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5615}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5425}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGRO%3A%20An%20Autonomous%20AI%20Rover%20for%20Precision%20Agriculture&body=Title%3A%20AGRO%3A%20An%20Autonomous%20AI%20Rover%20for%20Precision%20Agriculture%0AAuthor%3A%20Simar%20Ghumman%20and%20Fabio%20Di%20Troia%20and%20William%20Andreopoulos%20and%20Mark%20Stamp%20and%20Sanjit%20Rai%0AAbstract%3A%20%20%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20are%20emerging%20as%20a%20crucial%20tool%20in%20the%20world%0Aof%20precision%20agriculture.%20The%20combination%20of%20UGVs%20with%20machine%20learning%20allows%0Aus%20to%20find%20solutions%20for%20a%20range%20of%20complex%20agricultural%20problems.%20This%0Aresearch%20focuses%20on%20developing%20a%20UGV%20capable%20of%20autonomously%20traversing%0Aagricultural%20fields%20and%20capturing%20data.%20The%20project%2C%20known%20as%20AGRO%20%28Autonomous%0AGround%20Rover%20Observer%29%20leverages%20machine%20learning%2C%20computer%20vision%20and%20other%0Asensor%20technologies.%20AGRO%20uses%20its%20capabilities%20to%20determine%20pistachio%20yields%2C%0Aperforming%20self-localization%20and%20real-time%20environmental%20mapping%20while%20avoiding%0Aobstacles.%20The%20main%20objective%20of%20this%20research%20work%20is%20to%20automate%0Aresource-consuming%20operations%20so%20that%20AGRO%20can%20support%20farmers%20in%20making%0Adata-driven%20decisions.%20Furthermore%2C%20AGRO%20provides%20a%20foundation%20for%20advanced%0Amachine%20learning%20techniques%20as%20it%20captures%20the%20world%20around%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGRO%253A%2520An%2520Autonomous%2520AI%2520Rover%2520for%2520Precision%2520Agriculture%26entry.906535625%3DSimar%2520Ghumman%2520and%2520Fabio%2520Di%2520Troia%2520and%2520William%2520Andreopoulos%2520and%2520Mark%2520Stamp%2520and%2520Sanjit%2520Rai%26entry.1292438233%3D%2520%2520Unmanned%2520Ground%2520Vehicles%2520%2528UGVs%2529%2520are%2520emerging%2520as%2520a%2520crucial%2520tool%2520in%2520the%2520world%250Aof%2520precision%2520agriculture.%2520The%2520combination%2520of%2520UGVs%2520with%2520machine%2520learning%2520allows%250Aus%2520to%2520find%2520solutions%2520for%2520a%2520range%2520of%2520complex%2520agricultural%2520problems.%2520This%250Aresearch%2520focuses%2520on%2520developing%2520a%2520UGV%2520capable%2520of%2520autonomously%2520traversing%250Aagricultural%2520fields%2520and%2520capturing%2520data.%2520The%2520project%252C%2520known%2520as%2520AGRO%2520%2528Autonomous%250AGround%2520Rover%2520Observer%2529%2520leverages%2520machine%2520learning%252C%2520computer%2520vision%2520and%2520other%250Asensor%2520technologies.%2520AGRO%2520uses%2520its%2520capabilities%2520to%2520determine%2520pistachio%2520yields%252C%250Aperforming%2520self-localization%2520and%2520real-time%2520environmental%2520mapping%2520while%2520avoiding%250Aobstacles.%2520The%2520main%2520objective%2520of%2520this%2520research%2520work%2520is%2520to%2520automate%250Aresource-consuming%2520operations%2520so%2520that%2520AGRO%2520can%2520support%2520farmers%2520in%2520making%250Adata-driven%2520decisions.%2520Furthermore%252C%2520AGRO%2520provides%2520a%2520foundation%2520for%2520advanced%250Amachine%2520learning%2520techniques%2520as%2520it%2520captures%2520the%2520world%2520around%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGRO%3A%20An%20Autonomous%20AI%20Rover%20for%20Precision%20Agriculture&entry.906535625=Simar%20Ghumman%20and%20Fabio%20Di%20Troia%20and%20William%20Andreopoulos%20and%20Mark%20Stamp%20and%20Sanjit%20Rai&entry.1292438233=%20%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20are%20emerging%20as%20a%20crucial%20tool%20in%20the%20world%0Aof%20precision%20agriculture.%20The%20combination%20of%20UGVs%20with%20machine%20learning%20allows%0Aus%20to%20find%20solutions%20for%20a%20range%20of%20complex%20agricultural%20problems.%20This%0Aresearch%20focuses%20on%20developing%20a%20UGV%20capable%20of%20autonomously%20traversing%0Aagricultural%20fields%20and%20capturing%20data.%20The%20project%2C%20known%20as%20AGRO%20%28Autonomous%0AGround%20Rover%20Observer%29%20leverages%20machine%20learning%2C%20computer%20vision%20and%20other%0Asensor%20technologies.%20AGRO%20uses%20its%20capabilities%20to%20determine%20pistachio%20yields%2C%0Aperforming%20self-localization%20and%20real-time%20environmental%20mapping%20while%20avoiding%0Aobstacles.%20The%20main%20objective%20of%20this%20research%20work%20is%20to%20automate%0Aresource-consuming%20operations%20so%20that%20AGRO%20can%20support%20farmers%20in%20making%0Adata-driven%20decisions.%20Furthermore%2C%20AGRO%20provides%20a%20foundation%20for%20advanced%0Amachine%20learning%20techniques%20as%20it%20captures%20the%20world%20around%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01200v1&entry.124074799=Read"},
{"title": "RD-UIE: Relation-Driven State Space Modeling for Underwater Image\n  Enhancement", "author": "Kui Jiang and Yan Luo and Junjun Jiang and Xin Xu and Fei Ma and Fei Yu", "abstract": "  Underwater image enhancement (UIE) is a critical preprocessing step for\nmarine vision applications, where wavelength-dependent attenuation causes\nsevere content degradation and color distortion. While recent state space\nmodels like Mamba show potential for long-range dependency modeling, their\nunfolding operations and fixed scan paths on 1D sequences fail to adapt to\nlocal object semantics and global relation modeling, limiting their efficacy in\ncomplex underwater environments. To address this, we enhance conventional Mamba\nwith the sorting-based scanning mechanism that dynamically reorders scanning\nsequences based on statistical distribution of spatial correlation of all\npixels. In this way, it encourages the network to prioritize the most\ninformative components--structural and semantic features. Upon building this\nmechanism, we devise a Visually Self-adaptive State Block (VSSB) that\nharmonizes dynamic sorting of Mamba with input-dependent dynamic convolution,\nenabling coherent integration of global context and local relational cues. This\nexquisite design helps eliminate global focus bias, especially for widely\ndistributed contents, which greatly weakens the statistical frequency. For\nrobust feature extraction and refinement, we design a cross-feature bridge\n(CFB) to adaptively fuse multi-scale representations. These efforts compose the\nnovel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive\nexperiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms\nthe state-of-the-art approach WMamba in both quantitative metrics and visual\nfidelity, averagely achieving 0.55 dB performance gain on the three benchmarks.\nOur code is available at https://github.com/kkoucy/RD-UIE/tree/main\n", "link": "http://arxiv.org/abs/2505.01224v1", "date": "2025-05-02", "relevancy": 2.1133, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5259}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RD-UIE%3A%20Relation-Driven%20State%20Space%20Modeling%20for%20Underwater%20Image%0A%20%20Enhancement&body=Title%3A%20RD-UIE%3A%20Relation-Driven%20State%20Space%20Modeling%20for%20Underwater%20Image%0A%20%20Enhancement%0AAuthor%3A%20Kui%20Jiang%20and%20Yan%20Luo%20and%20Junjun%20Jiang%20and%20Xin%20Xu%20and%20Fei%20Ma%20and%20Fei%20Yu%0AAbstract%3A%20%20%20Underwater%20image%20enhancement%20%28UIE%29%20is%20a%20critical%20preprocessing%20step%20for%0Amarine%20vision%20applications%2C%20where%20wavelength-dependent%20attenuation%20causes%0Asevere%20content%20degradation%20and%20color%20distortion.%20While%20recent%20state%20space%0Amodels%20like%20Mamba%20show%20potential%20for%20long-range%20dependency%20modeling%2C%20their%0Aunfolding%20operations%20and%20fixed%20scan%20paths%20on%201D%20sequences%20fail%20to%20adapt%20to%0Alocal%20object%20semantics%20and%20global%20relation%20modeling%2C%20limiting%20their%20efficacy%20in%0Acomplex%20underwater%20environments.%20To%20address%20this%2C%20we%20enhance%20conventional%20Mamba%0Awith%20the%20sorting-based%20scanning%20mechanism%20that%20dynamically%20reorders%20scanning%0Asequences%20based%20on%20statistical%20distribution%20of%20spatial%20correlation%20of%20all%0Apixels.%20In%20this%20way%2C%20it%20encourages%20the%20network%20to%20prioritize%20the%20most%0Ainformative%20components--structural%20and%20semantic%20features.%20Upon%20building%20this%0Amechanism%2C%20we%20devise%20a%20Visually%20Self-adaptive%20State%20Block%20%28VSSB%29%20that%0Aharmonizes%20dynamic%20sorting%20of%20Mamba%20with%20input-dependent%20dynamic%20convolution%2C%0Aenabling%20coherent%20integration%20of%20global%20context%20and%20local%20relational%20cues.%20This%0Aexquisite%20design%20helps%20eliminate%20global%20focus%20bias%2C%20especially%20for%20widely%0Adistributed%20contents%2C%20which%20greatly%20weakens%20the%20statistical%20frequency.%20For%0Arobust%20feature%20extraction%20and%20refinement%2C%20we%20design%20a%20cross-feature%20bridge%0A%28CFB%29%20to%20adaptively%20fuse%20multi-scale%20representations.%20These%20efforts%20compose%20the%0Anovel%20relation-driven%20Mamba%20framework%20for%20effective%20UIE%20%28RD-UIE%29.%20Extensive%0Aexperiments%20on%20underwater%20enhancement%20benchmarks%20demonstrate%20RD-UIE%20outperforms%0Athe%20state-of-the-art%20approach%20WMamba%20in%20both%20quantitative%20metrics%20and%20visual%0Afidelity%2C%20averagely%20achieving%200.55%20dB%20performance%20gain%20on%20the%20three%20benchmarks.%0AOur%20code%20is%20available%20at%20https%3A//github.com/kkoucy/RD-UIE/tree/main%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRD-UIE%253A%2520Relation-Driven%2520State%2520Space%2520Modeling%2520for%2520Underwater%2520Image%250A%2520%2520Enhancement%26entry.906535625%3DKui%2520Jiang%2520and%2520Yan%2520Luo%2520and%2520Junjun%2520Jiang%2520and%2520Xin%2520Xu%2520and%2520Fei%2520Ma%2520and%2520Fei%2520Yu%26entry.1292438233%3D%2520%2520Underwater%2520image%2520enhancement%2520%2528UIE%2529%2520is%2520a%2520critical%2520preprocessing%2520step%2520for%250Amarine%2520vision%2520applications%252C%2520where%2520wavelength-dependent%2520attenuation%2520causes%250Asevere%2520content%2520degradation%2520and%2520color%2520distortion.%2520While%2520recent%2520state%2520space%250Amodels%2520like%2520Mamba%2520show%2520potential%2520for%2520long-range%2520dependency%2520modeling%252C%2520their%250Aunfolding%2520operations%2520and%2520fixed%2520scan%2520paths%2520on%25201D%2520sequences%2520fail%2520to%2520adapt%2520to%250Alocal%2520object%2520semantics%2520and%2520global%2520relation%2520modeling%252C%2520limiting%2520their%2520efficacy%2520in%250Acomplex%2520underwater%2520environments.%2520To%2520address%2520this%252C%2520we%2520enhance%2520conventional%2520Mamba%250Awith%2520the%2520sorting-based%2520scanning%2520mechanism%2520that%2520dynamically%2520reorders%2520scanning%250Asequences%2520based%2520on%2520statistical%2520distribution%2520of%2520spatial%2520correlation%2520of%2520all%250Apixels.%2520In%2520this%2520way%252C%2520it%2520encourages%2520the%2520network%2520to%2520prioritize%2520the%2520most%250Ainformative%2520components--structural%2520and%2520semantic%2520features.%2520Upon%2520building%2520this%250Amechanism%252C%2520we%2520devise%2520a%2520Visually%2520Self-adaptive%2520State%2520Block%2520%2528VSSB%2529%2520that%250Aharmonizes%2520dynamic%2520sorting%2520of%2520Mamba%2520with%2520input-dependent%2520dynamic%2520convolution%252C%250Aenabling%2520coherent%2520integration%2520of%2520global%2520context%2520and%2520local%2520relational%2520cues.%2520This%250Aexquisite%2520design%2520helps%2520eliminate%2520global%2520focus%2520bias%252C%2520especially%2520for%2520widely%250Adistributed%2520contents%252C%2520which%2520greatly%2520weakens%2520the%2520statistical%2520frequency.%2520For%250Arobust%2520feature%2520extraction%2520and%2520refinement%252C%2520we%2520design%2520a%2520cross-feature%2520bridge%250A%2528CFB%2529%2520to%2520adaptively%2520fuse%2520multi-scale%2520representations.%2520These%2520efforts%2520compose%2520the%250Anovel%2520relation-driven%2520Mamba%2520framework%2520for%2520effective%2520UIE%2520%2528RD-UIE%2529.%2520Extensive%250Aexperiments%2520on%2520underwater%2520enhancement%2520benchmarks%2520demonstrate%2520RD-UIE%2520outperforms%250Athe%2520state-of-the-art%2520approach%2520WMamba%2520in%2520both%2520quantitative%2520metrics%2520and%2520visual%250Afidelity%252C%2520averagely%2520achieving%25200.55%2520dB%2520performance%2520gain%2520on%2520the%2520three%2520benchmarks.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/kkoucy/RD-UIE/tree/main%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RD-UIE%3A%20Relation-Driven%20State%20Space%20Modeling%20for%20Underwater%20Image%0A%20%20Enhancement&entry.906535625=Kui%20Jiang%20and%20Yan%20Luo%20and%20Junjun%20Jiang%20and%20Xin%20Xu%20and%20Fei%20Ma%20and%20Fei%20Yu&entry.1292438233=%20%20Underwater%20image%20enhancement%20%28UIE%29%20is%20a%20critical%20preprocessing%20step%20for%0Amarine%20vision%20applications%2C%20where%20wavelength-dependent%20attenuation%20causes%0Asevere%20content%20degradation%20and%20color%20distortion.%20While%20recent%20state%20space%0Amodels%20like%20Mamba%20show%20potential%20for%20long-range%20dependency%20modeling%2C%20their%0Aunfolding%20operations%20and%20fixed%20scan%20paths%20on%201D%20sequences%20fail%20to%20adapt%20to%0Alocal%20object%20semantics%20and%20global%20relation%20modeling%2C%20limiting%20their%20efficacy%20in%0Acomplex%20underwater%20environments.%20To%20address%20this%2C%20we%20enhance%20conventional%20Mamba%0Awith%20the%20sorting-based%20scanning%20mechanism%20that%20dynamically%20reorders%20scanning%0Asequences%20based%20on%20statistical%20distribution%20of%20spatial%20correlation%20of%20all%0Apixels.%20In%20this%20way%2C%20it%20encourages%20the%20network%20to%20prioritize%20the%20most%0Ainformative%20components--structural%20and%20semantic%20features.%20Upon%20building%20this%0Amechanism%2C%20we%20devise%20a%20Visually%20Self-adaptive%20State%20Block%20%28VSSB%29%20that%0Aharmonizes%20dynamic%20sorting%20of%20Mamba%20with%20input-dependent%20dynamic%20convolution%2C%0Aenabling%20coherent%20integration%20of%20global%20context%20and%20local%20relational%20cues.%20This%0Aexquisite%20design%20helps%20eliminate%20global%20focus%20bias%2C%20especially%20for%20widely%0Adistributed%20contents%2C%20which%20greatly%20weakens%20the%20statistical%20frequency.%20For%0Arobust%20feature%20extraction%20and%20refinement%2C%20we%20design%20a%20cross-feature%20bridge%0A%28CFB%29%20to%20adaptively%20fuse%20multi-scale%20representations.%20These%20efforts%20compose%20the%0Anovel%20relation-driven%20Mamba%20framework%20for%20effective%20UIE%20%28RD-UIE%29.%20Extensive%0Aexperiments%20on%20underwater%20enhancement%20benchmarks%20demonstrate%20RD-UIE%20outperforms%0Athe%20state-of-the-art%20approach%20WMamba%20in%20both%20quantitative%20metrics%20and%20visual%0Afidelity%2C%20averagely%20achieving%200.55%20dB%20performance%20gain%20on%20the%20three%20benchmarks.%0AOur%20code%20is%20available%20at%20https%3A//github.com/kkoucy/RD-UIE/tree/main%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01224v1&entry.124074799=Read"},
{"title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks", "author": "Vishnu Sarukkai and Zhiqiang Xie and Kayvon Fatahalian", "abstract": "  Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering.\n", "link": "http://arxiv.org/abs/2505.00234v2", "date": "2025-05-02", "relevancy": 2.1075, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5738}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5189}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Generated%20In-Context%20Examples%20Improve%20LLM%20Agents%20for%20Sequential%0A%20%20Decision-Making%20Tasks&body=Title%3A%20Self-Generated%20In-Context%20Examples%20Improve%20LLM%20Agents%20for%20Sequential%0A%20%20Decision-Making%20Tasks%0AAuthor%3A%20Vishnu%20Sarukkai%20and%20Zhiqiang%20Xie%20and%20Kayvon%20Fatahalian%0AAbstract%3A%20%20%20Many%20methods%20for%20improving%20Large%20Language%20Model%20%28LLM%29%20agents%20for%20sequential%0Adecision-making%20tasks%20depend%20on%20task-specific%20knowledge%20engineering--such%20as%0Aprompt%20tuning%2C%20curated%20in-context%20examples%2C%20or%20customized%20observation%20and%0Aaction%20spaces.%20Using%20these%20approaches%2C%20agent%20performance%20improves%20with%20the%0Aquality%20or%20amount%20of%20knowledge%20engineering%20invested.%20Instead%2C%20we%20investigate%0Ahow%20LLM%20agents%20can%20automatically%20improve%20their%20performance%20by%20learning%0Ain-context%20from%20their%20own%20successful%20experiences%20on%20similar%20tasks.%20Rather%20than%0Arelying%20on%20task-specific%20knowledge%20engineering%2C%20we%20focus%20on%20constructing%20and%0Arefining%20a%20database%20of%20self-generated%20examples.%20We%20demonstrate%20that%20even%20a%0Anaive%20accumulation%20of%20successful%20trajectories%20across%20training%20tasks%20boosts%20test%0Aperformance%20on%20three%20benchmarks%3A%20ALFWorld%20%2873%25%20to%2089%25%29%2C%20Wordcraft%20%2855%25%20to%2064%25%29%2C%0Aand%20InterCode-SQL%20%2875%25%20to%2079%25%29--matching%20the%20performance%20the%20initial%20agent%0Aachieves%20if%20allowed%20two%20to%20three%20attempts%20per%20task.%20We%20then%20introduce%20two%0Aextensions%3A%20%281%29%20database-level%20selection%20through%20population-based%20training%20to%0Aidentify%20high-performing%20example%20collections%2C%20and%20%282%29%20exemplar-level%20selection%0Athat%20retains%20individual%20trajectories%20based%20on%20their%20empirical%20utility%20as%0Ain-context%20examples.%20These%20extensions%20further%20enhance%20performance%2C%20achieving%0A91%25%20on%20ALFWorld--matching%20more%20complex%20approaches%20that%20employ%20task-specific%0Acomponents%20and%20prompts.%20Our%20results%20demonstrate%20that%20automatic%20trajectory%0Adatabase%20construction%20offers%20a%20compelling%20alternative%20to%20labor-intensive%0Aknowledge%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Generated%2520In-Context%2520Examples%2520Improve%2520LLM%2520Agents%2520for%2520Sequential%250A%2520%2520Decision-Making%2520Tasks%26entry.906535625%3DVishnu%2520Sarukkai%2520and%2520Zhiqiang%2520Xie%2520and%2520Kayvon%2520Fatahalian%26entry.1292438233%3D%2520%2520Many%2520methods%2520for%2520improving%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520for%2520sequential%250Adecision-making%2520tasks%2520depend%2520on%2520task-specific%2520knowledge%2520engineering--such%2520as%250Aprompt%2520tuning%252C%2520curated%2520in-context%2520examples%252C%2520or%2520customized%2520observation%2520and%250Aaction%2520spaces.%2520Using%2520these%2520approaches%252C%2520agent%2520performance%2520improves%2520with%2520the%250Aquality%2520or%2520amount%2520of%2520knowledge%2520engineering%2520invested.%2520Instead%252C%2520we%2520investigate%250Ahow%2520LLM%2520agents%2520can%2520automatically%2520improve%2520their%2520performance%2520by%2520learning%250Ain-context%2520from%2520their%2520own%2520successful%2520experiences%2520on%2520similar%2520tasks.%2520Rather%2520than%250Arelying%2520on%2520task-specific%2520knowledge%2520engineering%252C%2520we%2520focus%2520on%2520constructing%2520and%250Arefining%2520a%2520database%2520of%2520self-generated%2520examples.%2520We%2520demonstrate%2520that%2520even%2520a%250Anaive%2520accumulation%2520of%2520successful%2520trajectories%2520across%2520training%2520tasks%2520boosts%2520test%250Aperformance%2520on%2520three%2520benchmarks%253A%2520ALFWorld%2520%252873%2525%2520to%252089%2525%2529%252C%2520Wordcraft%2520%252855%2525%2520to%252064%2525%2529%252C%250Aand%2520InterCode-SQL%2520%252875%2525%2520to%252079%2525%2529--matching%2520the%2520performance%2520the%2520initial%2520agent%250Aachieves%2520if%2520allowed%2520two%2520to%2520three%2520attempts%2520per%2520task.%2520We%2520then%2520introduce%2520two%250Aextensions%253A%2520%25281%2529%2520database-level%2520selection%2520through%2520population-based%2520training%2520to%250Aidentify%2520high-performing%2520example%2520collections%252C%2520and%2520%25282%2529%2520exemplar-level%2520selection%250Athat%2520retains%2520individual%2520trajectories%2520based%2520on%2520their%2520empirical%2520utility%2520as%250Ain-context%2520examples.%2520These%2520extensions%2520further%2520enhance%2520performance%252C%2520achieving%250A91%2525%2520on%2520ALFWorld--matching%2520more%2520complex%2520approaches%2520that%2520employ%2520task-specific%250Acomponents%2520and%2520prompts.%2520Our%2520results%2520demonstrate%2520that%2520automatic%2520trajectory%250Adatabase%2520construction%2520offers%2520a%2520compelling%2520alternative%2520to%2520labor-intensive%250Aknowledge%2520engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Generated%20In-Context%20Examples%20Improve%20LLM%20Agents%20for%20Sequential%0A%20%20Decision-Making%20Tasks&entry.906535625=Vishnu%20Sarukkai%20and%20Zhiqiang%20Xie%20and%20Kayvon%20Fatahalian&entry.1292438233=%20%20Many%20methods%20for%20improving%20Large%20Language%20Model%20%28LLM%29%20agents%20for%20sequential%0Adecision-making%20tasks%20depend%20on%20task-specific%20knowledge%20engineering--such%20as%0Aprompt%20tuning%2C%20curated%20in-context%20examples%2C%20or%20customized%20observation%20and%0Aaction%20spaces.%20Using%20these%20approaches%2C%20agent%20performance%20improves%20with%20the%0Aquality%20or%20amount%20of%20knowledge%20engineering%20invested.%20Instead%2C%20we%20investigate%0Ahow%20LLM%20agents%20can%20automatically%20improve%20their%20performance%20by%20learning%0Ain-context%20from%20their%20own%20successful%20experiences%20on%20similar%20tasks.%20Rather%20than%0Arelying%20on%20task-specific%20knowledge%20engineering%2C%20we%20focus%20on%20constructing%20and%0Arefining%20a%20database%20of%20self-generated%20examples.%20We%20demonstrate%20that%20even%20a%0Anaive%20accumulation%20of%20successful%20trajectories%20across%20training%20tasks%20boosts%20test%0Aperformance%20on%20three%20benchmarks%3A%20ALFWorld%20%2873%25%20to%2089%25%29%2C%20Wordcraft%20%2855%25%20to%2064%25%29%2C%0Aand%20InterCode-SQL%20%2875%25%20to%2079%25%29--matching%20the%20performance%20the%20initial%20agent%0Aachieves%20if%20allowed%20two%20to%20three%20attempts%20per%20task.%20We%20then%20introduce%20two%0Aextensions%3A%20%281%29%20database-level%20selection%20through%20population-based%20training%20to%0Aidentify%20high-performing%20example%20collections%2C%20and%20%282%29%20exemplar-level%20selection%0Athat%20retains%20individual%20trajectories%20based%20on%20their%20empirical%20utility%20as%0Ain-context%20examples.%20These%20extensions%20further%20enhance%20performance%2C%20achieving%0A91%25%20on%20ALFWorld--matching%20more%20complex%20approaches%20that%20employ%20task-specific%0Acomponents%20and%20prompts.%20Our%20results%20demonstrate%20that%20automatic%20trajectory%0Adatabase%20construction%20offers%20a%20compelling%20alternative%20to%20labor-intensive%0Aknowledge%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00234v2&entry.124074799=Read"},
{"title": "FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft\n  Research", "author": "Yan Miao and Will Shen and Hang Cui and Sayan Mitra", "abstract": "  We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing\nplatform for autonomy research. The hardware platform integrates a small\ncamera, a standard airframe, offboard computation, and radio communication for\nmanual overrides. We demonstrate FalconWing's capabilities by developing and\ndeploying a purely vision-based control policy for autonomous landing (without\nIMU or motion capture) using a novel real-to-sim-to-real learning approach. Our\nlearning approach: (1) constructs a photorealistic simulation environment via\n3D Gaussian splatting trained on real-world images; (2) identifies nonlinear\ndynamics from vision-estimated real-flight data; and (3) trains a multi-modal\nVision Transformer (ViT) policy through simulation-only imitation learning. The\nViT architecture fuses single RGB image with the history of control actions via\nself-attention, preserving temporal context while maintaining real-time 20 Hz\ninference. When deployed zero-shot on the hardware platform, this policy\nachieves an 80% success rate in vision-based autonomous landings. Together with\nthe hardware specifications, we also open-source the system dynamics, the\nsoftware for photorealistic simulator and the learning approach.\n", "link": "http://arxiv.org/abs/2505.01383v1", "date": "2025-05-02", "relevancy": 2.0901, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5332}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5258}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FalconWing%3A%20An%20Open-Source%20Platform%20for%20Ultra-Light%20Fixed-Wing%20Aircraft%0A%20%20Research&body=Title%3A%20FalconWing%3A%20An%20Open-Source%20Platform%20for%20Ultra-Light%20Fixed-Wing%20Aircraft%0A%20%20Research%0AAuthor%3A%20Yan%20Miao%20and%20Will%20Shen%20and%20Hang%20Cui%20and%20Sayan%20Mitra%0AAbstract%3A%20%20%20We%20present%20FalconWing%20--%20an%20open-source%2C%20ultra-lightweight%20%28150%20g%29%20fixed-wing%0Aplatform%20for%20autonomy%20research.%20The%20hardware%20platform%20integrates%20a%20small%0Acamera%2C%20a%20standard%20airframe%2C%20offboard%20computation%2C%20and%20radio%20communication%20for%0Amanual%20overrides.%20We%20demonstrate%20FalconWing%27s%20capabilities%20by%20developing%20and%0Adeploying%20a%20purely%20vision-based%20control%20policy%20for%20autonomous%20landing%20%28without%0AIMU%20or%20motion%20capture%29%20using%20a%20novel%20real-to-sim-to-real%20learning%20approach.%20Our%0Alearning%20approach%3A%20%281%29%20constructs%20a%20photorealistic%20simulation%20environment%20via%0A3D%20Gaussian%20splatting%20trained%20on%20real-world%20images%3B%20%282%29%20identifies%20nonlinear%0Adynamics%20from%20vision-estimated%20real-flight%20data%3B%20and%20%283%29%20trains%20a%20multi-modal%0AVision%20Transformer%20%28ViT%29%20policy%20through%20simulation-only%20imitation%20learning.%20The%0AViT%20architecture%20fuses%20single%20RGB%20image%20with%20the%20history%20of%20control%20actions%20via%0Aself-attention%2C%20preserving%20temporal%20context%20while%20maintaining%20real-time%2020%20Hz%0Ainference.%20When%20deployed%20zero-shot%20on%20the%20hardware%20platform%2C%20this%20policy%0Aachieves%20an%2080%25%20success%20rate%20in%20vision-based%20autonomous%20landings.%20Together%20with%0Athe%20hardware%20specifications%2C%20we%20also%20open-source%20the%20system%20dynamics%2C%20the%0Asoftware%20for%20photorealistic%20simulator%20and%20the%20learning%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFalconWing%253A%2520An%2520Open-Source%2520Platform%2520for%2520Ultra-Light%2520Fixed-Wing%2520Aircraft%250A%2520%2520Research%26entry.906535625%3DYan%2520Miao%2520and%2520Will%2520Shen%2520and%2520Hang%2520Cui%2520and%2520Sayan%2520Mitra%26entry.1292438233%3D%2520%2520We%2520present%2520FalconWing%2520--%2520an%2520open-source%252C%2520ultra-lightweight%2520%2528150%2520g%2529%2520fixed-wing%250Aplatform%2520for%2520autonomy%2520research.%2520The%2520hardware%2520platform%2520integrates%2520a%2520small%250Acamera%252C%2520a%2520standard%2520airframe%252C%2520offboard%2520computation%252C%2520and%2520radio%2520communication%2520for%250Amanual%2520overrides.%2520We%2520demonstrate%2520FalconWing%2527s%2520capabilities%2520by%2520developing%2520and%250Adeploying%2520a%2520purely%2520vision-based%2520control%2520policy%2520for%2520autonomous%2520landing%2520%2528without%250AIMU%2520or%2520motion%2520capture%2529%2520using%2520a%2520novel%2520real-to-sim-to-real%2520learning%2520approach.%2520Our%250Alearning%2520approach%253A%2520%25281%2529%2520constructs%2520a%2520photorealistic%2520simulation%2520environment%2520via%250A3D%2520Gaussian%2520splatting%2520trained%2520on%2520real-world%2520images%253B%2520%25282%2529%2520identifies%2520nonlinear%250Adynamics%2520from%2520vision-estimated%2520real-flight%2520data%253B%2520and%2520%25283%2529%2520trains%2520a%2520multi-modal%250AVision%2520Transformer%2520%2528ViT%2529%2520policy%2520through%2520simulation-only%2520imitation%2520learning.%2520The%250AViT%2520architecture%2520fuses%2520single%2520RGB%2520image%2520with%2520the%2520history%2520of%2520control%2520actions%2520via%250Aself-attention%252C%2520preserving%2520temporal%2520context%2520while%2520maintaining%2520real-time%252020%2520Hz%250Ainference.%2520When%2520deployed%2520zero-shot%2520on%2520the%2520hardware%2520platform%252C%2520this%2520policy%250Aachieves%2520an%252080%2525%2520success%2520rate%2520in%2520vision-based%2520autonomous%2520landings.%2520Together%2520with%250Athe%2520hardware%2520specifications%252C%2520we%2520also%2520open-source%2520the%2520system%2520dynamics%252C%2520the%250Asoftware%2520for%2520photorealistic%2520simulator%2520and%2520the%2520learning%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FalconWing%3A%20An%20Open-Source%20Platform%20for%20Ultra-Light%20Fixed-Wing%20Aircraft%0A%20%20Research&entry.906535625=Yan%20Miao%20and%20Will%20Shen%20and%20Hang%20Cui%20and%20Sayan%20Mitra&entry.1292438233=%20%20We%20present%20FalconWing%20--%20an%20open-source%2C%20ultra-lightweight%20%28150%20g%29%20fixed-wing%0Aplatform%20for%20autonomy%20research.%20The%20hardware%20platform%20integrates%20a%20small%0Acamera%2C%20a%20standard%20airframe%2C%20offboard%20computation%2C%20and%20radio%20communication%20for%0Amanual%20overrides.%20We%20demonstrate%20FalconWing%27s%20capabilities%20by%20developing%20and%0Adeploying%20a%20purely%20vision-based%20control%20policy%20for%20autonomous%20landing%20%28without%0AIMU%20or%20motion%20capture%29%20using%20a%20novel%20real-to-sim-to-real%20learning%20approach.%20Our%0Alearning%20approach%3A%20%281%29%20constructs%20a%20photorealistic%20simulation%20environment%20via%0A3D%20Gaussian%20splatting%20trained%20on%20real-world%20images%3B%20%282%29%20identifies%20nonlinear%0Adynamics%20from%20vision-estimated%20real-flight%20data%3B%20and%20%283%29%20trains%20a%20multi-modal%0AVision%20Transformer%20%28ViT%29%20policy%20through%20simulation-only%20imitation%20learning.%20The%0AViT%20architecture%20fuses%20single%20RGB%20image%20with%20the%20history%20of%20control%20actions%20via%0Aself-attention%2C%20preserving%20temporal%20context%20while%20maintaining%20real-time%2020%20Hz%0Ainference.%20When%20deployed%20zero-shot%20on%20the%20hardware%20platform%2C%20this%20policy%0Aachieves%20an%2080%25%20success%20rate%20in%20vision-based%20autonomous%20landings.%20Together%20with%0Athe%20hardware%20specifications%2C%20we%20also%20open-source%20the%20system%20dynamics%2C%20the%0Asoftware%20for%20photorealistic%20simulator%20and%20the%20learning%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01383v1&entry.124074799=Read"},
{"title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods\n  on NLP Models", "author": "Mahdi Dhaini and Kafaite Zahra Hussain and Efstratios Zaradoukas and Gjergji Kasneci", "abstract": "  As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP.\n", "link": "http://arxiv.org/abs/2505.01238v1", "date": "2025-05-02", "relevancy": 2.0873, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5277}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5277}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvalxNLP%3A%20A%20Framework%20for%20Benchmarking%20Post-Hoc%20Explainability%20Methods%0A%20%20on%20NLP%20Models&body=Title%3A%20EvalxNLP%3A%20A%20Framework%20for%20Benchmarking%20Post-Hoc%20Explainability%20Methods%0A%20%20on%20NLP%20Models%0AAuthor%3A%20Mahdi%20Dhaini%20and%20Kafaite%20Zahra%20Hussain%20and%20Efstratios%20Zaradoukas%20and%20Gjergji%20Kasneci%0AAbstract%3A%20%20%20As%20Natural%20Language%20Processing%20%28NLP%29%20models%20continue%20to%20evolve%20and%20become%0Aintegral%20to%20high-stakes%20applications%2C%20ensuring%20their%20interpretability%20remains%20a%0Acritical%20challenge.%20Given%20the%20growing%20variety%20of%20explainability%20methods%20and%0Adiverse%20stakeholder%20requirements%2C%20frameworks%20that%20help%20stakeholders%20select%0Aappropriate%20explanations%20tailored%20to%20their%20specific%20use%20cases%20are%20increasingly%0Aimportant.%20To%20address%20this%20need%2C%20we%20introduce%20EvalxNLP%2C%20a%20Python%20framework%20for%0Abenchmarking%20state-of-the-art%20feature%20attribution%20methods%20for%20transformer-based%0ANLP%20models.%20EvalxNLP%20integrates%20eight%20widely%20recognized%20explainability%0Atechniques%20from%20the%20Explainable%20AI%20%28XAI%29%20literature%2C%20enabling%20users%20to%20generate%0Aand%20evaluate%20explanations%20based%20on%20key%20properties%20such%20as%20faithfulness%2C%0Aplausibility%2C%20and%20complexity.%20Our%20framework%20also%20provides%20interactive%2C%0ALLM-based%20textual%20explanations%2C%20facilitating%20user%20understanding%20of%20the%0Agenerated%20explanations%20and%20evaluation%20outcomes.%20Human%20evaluation%20results%0Aindicate%20high%20user%20satisfaction%20with%20EvalxNLP%2C%20suggesting%20it%20is%20a%20promising%0Aframework%20for%20benchmarking%20explanation%20methods%20across%20diverse%20user%20groups.%20By%0Aoffering%20a%20user-friendly%20and%20extensible%20platform%2C%20EvalxNLP%20aims%20at%0Ademocratizing%20explainability%20tools%20and%20supporting%20the%20systematic%20comparison%20and%0Aadvancement%20of%20XAI%20techniques%20in%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvalxNLP%253A%2520A%2520Framework%2520for%2520Benchmarking%2520Post-Hoc%2520Explainability%2520Methods%250A%2520%2520on%2520NLP%2520Models%26entry.906535625%3DMahdi%2520Dhaini%2520and%2520Kafaite%2520Zahra%2520Hussain%2520and%2520Efstratios%2520Zaradoukas%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3D%2520%2520As%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520models%2520continue%2520to%2520evolve%2520and%2520become%250Aintegral%2520to%2520high-stakes%2520applications%252C%2520ensuring%2520their%2520interpretability%2520remains%2520a%250Acritical%2520challenge.%2520Given%2520the%2520growing%2520variety%2520of%2520explainability%2520methods%2520and%250Adiverse%2520stakeholder%2520requirements%252C%2520frameworks%2520that%2520help%2520stakeholders%2520select%250Aappropriate%2520explanations%2520tailored%2520to%2520their%2520specific%2520use%2520cases%2520are%2520increasingly%250Aimportant.%2520To%2520address%2520this%2520need%252C%2520we%2520introduce%2520EvalxNLP%252C%2520a%2520Python%2520framework%2520for%250Abenchmarking%2520state-of-the-art%2520feature%2520attribution%2520methods%2520for%2520transformer-based%250ANLP%2520models.%2520EvalxNLP%2520integrates%2520eight%2520widely%2520recognized%2520explainability%250Atechniques%2520from%2520the%2520Explainable%2520AI%2520%2528XAI%2529%2520literature%252C%2520enabling%2520users%2520to%2520generate%250Aand%2520evaluate%2520explanations%2520based%2520on%2520key%2520properties%2520such%2520as%2520faithfulness%252C%250Aplausibility%252C%2520and%2520complexity.%2520Our%2520framework%2520also%2520provides%2520interactive%252C%250ALLM-based%2520textual%2520explanations%252C%2520facilitating%2520user%2520understanding%2520of%2520the%250Agenerated%2520explanations%2520and%2520evaluation%2520outcomes.%2520Human%2520evaluation%2520results%250Aindicate%2520high%2520user%2520satisfaction%2520with%2520EvalxNLP%252C%2520suggesting%2520it%2520is%2520a%2520promising%250Aframework%2520for%2520benchmarking%2520explanation%2520methods%2520across%2520diverse%2520user%2520groups.%2520By%250Aoffering%2520a%2520user-friendly%2520and%2520extensible%2520platform%252C%2520EvalxNLP%2520aims%2520at%250Ademocratizing%2520explainability%2520tools%2520and%2520supporting%2520the%2520systematic%2520comparison%2520and%250Aadvancement%2520of%2520XAI%2520techniques%2520in%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvalxNLP%3A%20A%20Framework%20for%20Benchmarking%20Post-Hoc%20Explainability%20Methods%0A%20%20on%20NLP%20Models&entry.906535625=Mahdi%20Dhaini%20and%20Kafaite%20Zahra%20Hussain%20and%20Efstratios%20Zaradoukas%20and%20Gjergji%20Kasneci&entry.1292438233=%20%20As%20Natural%20Language%20Processing%20%28NLP%29%20models%20continue%20to%20evolve%20and%20become%0Aintegral%20to%20high-stakes%20applications%2C%20ensuring%20their%20interpretability%20remains%20a%0Acritical%20challenge.%20Given%20the%20growing%20variety%20of%20explainability%20methods%20and%0Adiverse%20stakeholder%20requirements%2C%20frameworks%20that%20help%20stakeholders%20select%0Aappropriate%20explanations%20tailored%20to%20their%20specific%20use%20cases%20are%20increasingly%0Aimportant.%20To%20address%20this%20need%2C%20we%20introduce%20EvalxNLP%2C%20a%20Python%20framework%20for%0Abenchmarking%20state-of-the-art%20feature%20attribution%20methods%20for%20transformer-based%0ANLP%20models.%20EvalxNLP%20integrates%20eight%20widely%20recognized%20explainability%0Atechniques%20from%20the%20Explainable%20AI%20%28XAI%29%20literature%2C%20enabling%20users%20to%20generate%0Aand%20evaluate%20explanations%20based%20on%20key%20properties%20such%20as%20faithfulness%2C%0Aplausibility%2C%20and%20complexity.%20Our%20framework%20also%20provides%20interactive%2C%0ALLM-based%20textual%20explanations%2C%20facilitating%20user%20understanding%20of%20the%0Agenerated%20explanations%20and%20evaluation%20outcomes.%20Human%20evaluation%20results%0Aindicate%20high%20user%20satisfaction%20with%20EvalxNLP%2C%20suggesting%20it%20is%20a%20promising%0Aframework%20for%20benchmarking%20explanation%20methods%20across%20diverse%20user%20groups.%20By%0Aoffering%20a%20user-friendly%20and%20extensible%20platform%2C%20EvalxNLP%20aims%20at%0Ademocratizing%20explainability%20tools%20and%20supporting%20the%20systematic%20comparison%20and%0Aadvancement%20of%20XAI%20techniques%20in%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01238v1&entry.124074799=Read"},
{"title": "A Causal World Model Underlying Next Token Prediction: Exploring GPT in\n  a Controlled Environment", "author": "Raanan Y. Rohekar and Yaniv Gurwicz and Sungduk Yu and Estelle Aflalo and Vasudev Lal", "abstract": "  Do generative pre-trained transformer (GPT) models, trained only to predict\nthe next token, implicitly learn a world model from which a sequence is\ngenerated one token at a time? We address this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that GPT\nmodels, at inference time, can be utilized for zero-shot causal structure\nlearning for input sequences and present a confidence score. Empirical\nevaluation is conducted in a controlled environment using the setup and rules\nof the Othello and Chess strategy games. A GPT, pre-trained on real-world games\nplayed with the intention of winning, is tested on out-of-distribution\nsynthetic data consisting of sequences of random legal moves. We find that the\nGPT model is likely to generate legal next moves for out-of-distribution\nsequences for which a causal structure is encoded in the attention mechanism\nwith high confidence. In cases for which the GPT model generates illegal moves\nit also fails to capture any causal structure.\n", "link": "http://arxiv.org/abs/2412.07446v3", "date": "2025-05-02", "relevancy": 2.0861, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5772}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Causal%20World%20Model%20Underlying%20Next%20Token%20Prediction%3A%20Exploring%20GPT%20in%0A%20%20a%20Controlled%20Environment&body=Title%3A%20A%20Causal%20World%20Model%20Underlying%20Next%20Token%20Prediction%3A%20Exploring%20GPT%20in%0A%20%20a%20Controlled%20Environment%0AAuthor%3A%20Raanan%20Y.%20Rohekar%20and%20Yaniv%20Gurwicz%20and%20Sungduk%20Yu%20and%20Estelle%20Aflalo%20and%20Vasudev%20Lal%0AAbstract%3A%20%20%20Do%20generative%20pre-trained%20transformer%20%28GPT%29%20models%2C%20trained%20only%20to%20predict%0Athe%20next%20token%2C%20implicitly%20learn%20a%20world%20model%20from%20which%20a%20sequence%20is%0Agenerated%20one%20token%20at%20a%20time%3F%20We%20address%20this%20question%20by%20deriving%20a%20causal%0Ainterpretation%20of%20the%20attention%20mechanism%20in%20GPT%2C%20and%20suggesting%20a%20causal%20world%0Amodel%20that%20arises%20from%20this%20interpretation.%20Furthermore%2C%20we%20propose%20that%20GPT%0Amodels%2C%20at%20inference%20time%2C%20can%20be%20utilized%20for%20zero-shot%20causal%20structure%0Alearning%20for%20input%20sequences%20and%20present%20a%20confidence%20score.%20Empirical%0Aevaluation%20is%20conducted%20in%20a%20controlled%20environment%20using%20the%20setup%20and%20rules%0Aof%20the%20Othello%20and%20Chess%20strategy%20games.%20A%20GPT%2C%20pre-trained%20on%20real-world%20games%0Aplayed%20with%20the%20intention%20of%20winning%2C%20is%20tested%20on%20out-of-distribution%0Asynthetic%20data%20consisting%20of%20sequences%20of%20random%20legal%20moves.%20We%20find%20that%20the%0AGPT%20model%20is%20likely%20to%20generate%20legal%20next%20moves%20for%20out-of-distribution%0Asequences%20for%20which%20a%20causal%20structure%20is%20encoded%20in%20the%20attention%20mechanism%0Awith%20high%20confidence.%20In%20cases%20for%20which%20the%20GPT%20model%20generates%20illegal%20moves%0Ait%20also%20fails%20to%20capture%20any%20causal%20structure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07446v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Causal%2520World%2520Model%2520Underlying%2520Next%2520Token%2520Prediction%253A%2520Exploring%2520GPT%2520in%250A%2520%2520a%2520Controlled%2520Environment%26entry.906535625%3DRaanan%2520Y.%2520Rohekar%2520and%2520Yaniv%2520Gurwicz%2520and%2520Sungduk%2520Yu%2520and%2520Estelle%2520Aflalo%2520and%2520Vasudev%2520Lal%26entry.1292438233%3D%2520%2520Do%2520generative%2520pre-trained%2520transformer%2520%2528GPT%2529%2520models%252C%2520trained%2520only%2520to%2520predict%250Athe%2520next%2520token%252C%2520implicitly%2520learn%2520a%2520world%2520model%2520from%2520which%2520a%2520sequence%2520is%250Agenerated%2520one%2520token%2520at%2520a%2520time%253F%2520We%2520address%2520this%2520question%2520by%2520deriving%2520a%2520causal%250Ainterpretation%2520of%2520the%2520attention%2520mechanism%2520in%2520GPT%252C%2520and%2520suggesting%2520a%2520causal%2520world%250Amodel%2520that%2520arises%2520from%2520this%2520interpretation.%2520Furthermore%252C%2520we%2520propose%2520that%2520GPT%250Amodels%252C%2520at%2520inference%2520time%252C%2520can%2520be%2520utilized%2520for%2520zero-shot%2520causal%2520structure%250Alearning%2520for%2520input%2520sequences%2520and%2520present%2520a%2520confidence%2520score.%2520Empirical%250Aevaluation%2520is%2520conducted%2520in%2520a%2520controlled%2520environment%2520using%2520the%2520setup%2520and%2520rules%250Aof%2520the%2520Othello%2520and%2520Chess%2520strategy%2520games.%2520A%2520GPT%252C%2520pre-trained%2520on%2520real-world%2520games%250Aplayed%2520with%2520the%2520intention%2520of%2520winning%252C%2520is%2520tested%2520on%2520out-of-distribution%250Asynthetic%2520data%2520consisting%2520of%2520sequences%2520of%2520random%2520legal%2520moves.%2520We%2520find%2520that%2520the%250AGPT%2520model%2520is%2520likely%2520to%2520generate%2520legal%2520next%2520moves%2520for%2520out-of-distribution%250Asequences%2520for%2520which%2520a%2520causal%2520structure%2520is%2520encoded%2520in%2520the%2520attention%2520mechanism%250Awith%2520high%2520confidence.%2520In%2520cases%2520for%2520which%2520the%2520GPT%2520model%2520generates%2520illegal%2520moves%250Ait%2520also%2520fails%2520to%2520capture%2520any%2520causal%2520structure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07446v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Causal%20World%20Model%20Underlying%20Next%20Token%20Prediction%3A%20Exploring%20GPT%20in%0A%20%20a%20Controlled%20Environment&entry.906535625=Raanan%20Y.%20Rohekar%20and%20Yaniv%20Gurwicz%20and%20Sungduk%20Yu%20and%20Estelle%20Aflalo%20and%20Vasudev%20Lal&entry.1292438233=%20%20Do%20generative%20pre-trained%20transformer%20%28GPT%29%20models%2C%20trained%20only%20to%20predict%0Athe%20next%20token%2C%20implicitly%20learn%20a%20world%20model%20from%20which%20a%20sequence%20is%0Agenerated%20one%20token%20at%20a%20time%3F%20We%20address%20this%20question%20by%20deriving%20a%20causal%0Ainterpretation%20of%20the%20attention%20mechanism%20in%20GPT%2C%20and%20suggesting%20a%20causal%20world%0Amodel%20that%20arises%20from%20this%20interpretation.%20Furthermore%2C%20we%20propose%20that%20GPT%0Amodels%2C%20at%20inference%20time%2C%20can%20be%20utilized%20for%20zero-shot%20causal%20structure%0Alearning%20for%20input%20sequences%20and%20present%20a%20confidence%20score.%20Empirical%0Aevaluation%20is%20conducted%20in%20a%20controlled%20environment%20using%20the%20setup%20and%20rules%0Aof%20the%20Othello%20and%20Chess%20strategy%20games.%20A%20GPT%2C%20pre-trained%20on%20real-world%20games%0Aplayed%20with%20the%20intention%20of%20winning%2C%20is%20tested%20on%20out-of-distribution%0Asynthetic%20data%20consisting%20of%20sequences%20of%20random%20legal%20moves.%20We%20find%20that%20the%0AGPT%20model%20is%20likely%20to%20generate%20legal%20next%20moves%20for%20out-of-distribution%0Asequences%20for%20which%20a%20causal%20structure%20is%20encoded%20in%20the%20attention%20mechanism%0Awith%20high%20confidence.%20In%20cases%20for%20which%20the%20GPT%20model%20generates%20illegal%20moves%0Ait%20also%20fails%20to%20capture%20any%20causal%20structure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07446v3&entry.124074799=Read"},
{"title": "FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and\n  Flow Matching based Voice Enhancing", "author": "Gaoxiang Cong and Liang Li and Jiadong Pan and Zhedong Zhang and Amin Beheshti and Anton van den Hengel and Yuankai Qi and Qingming Huang", "abstract": "  Movie Dubbing aims to convert scripts into speeches that align with the given\nmovie clip in both temporal and emotional aspects while preserving the vocal\ntimbre of a given brief reference audio. Existing methods focus primarily on\nreducing the word error rate while ignoring the importance of lip-sync and\nacoustic quality. To address these issues, we propose a large language model\n(LLM) based flow matching architecture for dubbing, named FlowDubber, which\nachieves high-quality audio-visual sync and pronunciation by incorporating a\nlarge speech language model and dual contrastive aligning while achieving\nbetter acoustic quality via the proposed voice-enhanced flow matching than\nprevious works. First, we introduce Qwen2.5 as the backbone of LLM to learn the\nin-context sequence from movie scripts and reference audio. Then, the proposed\nsemantic-aware learning focuses on capturing LLM semantic knowledge at the\nphoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment\nwith lip movement, reducing ambiguities where similar phonemes might be\nconfused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves\nacoustic quality in two aspects, which introduces an LLM-based acoustics flow\nmatching guidance to strengthen clarity and uses affine style prior to enhance\nidentity when recovering noise into mel-spectrograms via gradient vector field\nprediction. Extensive experiments demonstrate that our method outperforms\nseveral state-of-the-art methods on two primary benchmarks. The demos are\navailable at\n{\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.\n", "link": "http://arxiv.org/abs/2505.01263v1", "date": "2025-05-02", "relevancy": 2.0493, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5328}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlowDubber%3A%20Movie%20Dubbing%20with%20LLM-based%20Semantic-aware%20Learning%20and%0A%20%20Flow%20Matching%20based%20Voice%20Enhancing&body=Title%3A%20FlowDubber%3A%20Movie%20Dubbing%20with%20LLM-based%20Semantic-aware%20Learning%20and%0A%20%20Flow%20Matching%20based%20Voice%20Enhancing%0AAuthor%3A%20Gaoxiang%20Cong%20and%20Liang%20Li%20and%20Jiadong%20Pan%20and%20Zhedong%20Zhang%20and%20Amin%20Beheshti%20and%20Anton%20van%20den%20Hengel%20and%20Yuankai%20Qi%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20Movie%20Dubbing%20aims%20to%20convert%20scripts%20into%20speeches%20that%20align%20with%20the%20given%0Amovie%20clip%20in%20both%20temporal%20and%20emotional%20aspects%20while%20preserving%20the%20vocal%0Atimbre%20of%20a%20given%20brief%20reference%20audio.%20Existing%20methods%20focus%20primarily%20on%0Areducing%20the%20word%20error%20rate%20while%20ignoring%20the%20importance%20of%20lip-sync%20and%0Aacoustic%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20a%20large%20language%20model%0A%28LLM%29%20based%20flow%20matching%20architecture%20for%20dubbing%2C%20named%20FlowDubber%2C%20which%0Aachieves%20high-quality%20audio-visual%20sync%20and%20pronunciation%20by%20incorporating%20a%0Alarge%20speech%20language%20model%20and%20dual%20contrastive%20aligning%20while%20achieving%0Abetter%20acoustic%20quality%20via%20the%20proposed%20voice-enhanced%20flow%20matching%20than%0Aprevious%20works.%20First%2C%20we%20introduce%20Qwen2.5%20as%20the%20backbone%20of%20LLM%20to%20learn%20the%0Ain-context%20sequence%20from%20movie%20scripts%20and%20reference%20audio.%20Then%2C%20the%20proposed%0Asemantic-aware%20learning%20focuses%20on%20capturing%20LLM%20semantic%20knowledge%20at%20the%0Aphoneme%20level.%20Next%2C%20dual%20contrastive%20aligning%20%28DCA%29%20boosts%20mutual%20alignment%0Awith%20lip%20movement%2C%20reducing%20ambiguities%20where%20similar%20phonemes%20might%20be%0Aconfused.%20Finally%2C%20the%20proposed%20Flow-based%20Voice%20Enhancing%20%28FVE%29%20improves%0Aacoustic%20quality%20in%20two%20aspects%2C%20which%20introduces%20an%20LLM-based%20acoustics%20flow%0Amatching%20guidance%20to%20strengthen%20clarity%20and%20uses%20affine%20style%20prior%20to%20enhance%0Aidentity%20when%20recovering%20noise%20into%20mel-spectrograms%20via%20gradient%20vector%20field%0Aprediction.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Aseveral%20state-of-the-art%20methods%20on%20two%20primary%20benchmarks.%20The%20demos%20are%0Aavailable%20at%0A%7B%5Chref%7Bhttps%3A//galaxycong.github.io/LLM-Flow-Dubber/%7D%7B%5Ctextcolor%7Bred%7D%7Bhttps%3A//galaxycong.github.io/LLM-Flow-Dubber/%7D%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowDubber%253A%2520Movie%2520Dubbing%2520with%2520LLM-based%2520Semantic-aware%2520Learning%2520and%250A%2520%2520Flow%2520Matching%2520based%2520Voice%2520Enhancing%26entry.906535625%3DGaoxiang%2520Cong%2520and%2520Liang%2520Li%2520and%2520Jiadong%2520Pan%2520and%2520Zhedong%2520Zhang%2520and%2520Amin%2520Beheshti%2520and%2520Anton%2520van%2520den%2520Hengel%2520and%2520Yuankai%2520Qi%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520Movie%2520Dubbing%2520aims%2520to%2520convert%2520scripts%2520into%2520speeches%2520that%2520align%2520with%2520the%2520given%250Amovie%2520clip%2520in%2520both%2520temporal%2520and%2520emotional%2520aspects%2520while%2520preserving%2520the%2520vocal%250Atimbre%2520of%2520a%2520given%2520brief%2520reference%2520audio.%2520Existing%2520methods%2520focus%2520primarily%2520on%250Areducing%2520the%2520word%2520error%2520rate%2520while%2520ignoring%2520the%2520importance%2520of%2520lip-sync%2520and%250Aacoustic%2520quality.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520large%2520language%2520model%250A%2528LLM%2529%2520based%2520flow%2520matching%2520architecture%2520for%2520dubbing%252C%2520named%2520FlowDubber%252C%2520which%250Aachieves%2520high-quality%2520audio-visual%2520sync%2520and%2520pronunciation%2520by%2520incorporating%2520a%250Alarge%2520speech%2520language%2520model%2520and%2520dual%2520contrastive%2520aligning%2520while%2520achieving%250Abetter%2520acoustic%2520quality%2520via%2520the%2520proposed%2520voice-enhanced%2520flow%2520matching%2520than%250Aprevious%2520works.%2520First%252C%2520we%2520introduce%2520Qwen2.5%2520as%2520the%2520backbone%2520of%2520LLM%2520to%2520learn%2520the%250Ain-context%2520sequence%2520from%2520movie%2520scripts%2520and%2520reference%2520audio.%2520Then%252C%2520the%2520proposed%250Asemantic-aware%2520learning%2520focuses%2520on%2520capturing%2520LLM%2520semantic%2520knowledge%2520at%2520the%250Aphoneme%2520level.%2520Next%252C%2520dual%2520contrastive%2520aligning%2520%2528DCA%2529%2520boosts%2520mutual%2520alignment%250Awith%2520lip%2520movement%252C%2520reducing%2520ambiguities%2520where%2520similar%2520phonemes%2520might%2520be%250Aconfused.%2520Finally%252C%2520the%2520proposed%2520Flow-based%2520Voice%2520Enhancing%2520%2528FVE%2529%2520improves%250Aacoustic%2520quality%2520in%2520two%2520aspects%252C%2520which%2520introduces%2520an%2520LLM-based%2520acoustics%2520flow%250Amatching%2520guidance%2520to%2520strengthen%2520clarity%2520and%2520uses%2520affine%2520style%2520prior%2520to%2520enhance%250Aidentity%2520when%2520recovering%2520noise%2520into%2520mel-spectrograms%2520via%2520gradient%2520vector%2520field%250Aprediction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Aseveral%2520state-of-the-art%2520methods%2520on%2520two%2520primary%2520benchmarks.%2520The%2520demos%2520are%250Aavailable%2520at%250A%257B%255Chref%257Bhttps%253A//galaxycong.github.io/LLM-Flow-Dubber/%257D%257B%255Ctextcolor%257Bred%257D%257Bhttps%253A//galaxycong.github.io/LLM-Flow-Dubber/%257D%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowDubber%3A%20Movie%20Dubbing%20with%20LLM-based%20Semantic-aware%20Learning%20and%0A%20%20Flow%20Matching%20based%20Voice%20Enhancing&entry.906535625=Gaoxiang%20Cong%20and%20Liang%20Li%20and%20Jiadong%20Pan%20and%20Zhedong%20Zhang%20and%20Amin%20Beheshti%20and%20Anton%20van%20den%20Hengel%20and%20Yuankai%20Qi%20and%20Qingming%20Huang&entry.1292438233=%20%20Movie%20Dubbing%20aims%20to%20convert%20scripts%20into%20speeches%20that%20align%20with%20the%20given%0Amovie%20clip%20in%20both%20temporal%20and%20emotional%20aspects%20while%20preserving%20the%20vocal%0Atimbre%20of%20a%20given%20brief%20reference%20audio.%20Existing%20methods%20focus%20primarily%20on%0Areducing%20the%20word%20error%20rate%20while%20ignoring%20the%20importance%20of%20lip-sync%20and%0Aacoustic%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20a%20large%20language%20model%0A%28LLM%29%20based%20flow%20matching%20architecture%20for%20dubbing%2C%20named%20FlowDubber%2C%20which%0Aachieves%20high-quality%20audio-visual%20sync%20and%20pronunciation%20by%20incorporating%20a%0Alarge%20speech%20language%20model%20and%20dual%20contrastive%20aligning%20while%20achieving%0Abetter%20acoustic%20quality%20via%20the%20proposed%20voice-enhanced%20flow%20matching%20than%0Aprevious%20works.%20First%2C%20we%20introduce%20Qwen2.5%20as%20the%20backbone%20of%20LLM%20to%20learn%20the%0Ain-context%20sequence%20from%20movie%20scripts%20and%20reference%20audio.%20Then%2C%20the%20proposed%0Asemantic-aware%20learning%20focuses%20on%20capturing%20LLM%20semantic%20knowledge%20at%20the%0Aphoneme%20level.%20Next%2C%20dual%20contrastive%20aligning%20%28DCA%29%20boosts%20mutual%20alignment%0Awith%20lip%20movement%2C%20reducing%20ambiguities%20where%20similar%20phonemes%20might%20be%0Aconfused.%20Finally%2C%20the%20proposed%20Flow-based%20Voice%20Enhancing%20%28FVE%29%20improves%0Aacoustic%20quality%20in%20two%20aspects%2C%20which%20introduces%20an%20LLM-based%20acoustics%20flow%0Amatching%20guidance%20to%20strengthen%20clarity%20and%20uses%20affine%20style%20prior%20to%20enhance%0Aidentity%20when%20recovering%20noise%20into%20mel-spectrograms%20via%20gradient%20vector%20field%0Aprediction.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Aseveral%20state-of-the-art%20methods%20on%20two%20primary%20benchmarks.%20The%20demos%20are%0Aavailable%20at%0A%7B%5Chref%7Bhttps%3A//galaxycong.github.io/LLM-Flow-Dubber/%7D%7B%5Ctextcolor%7Bred%7D%7Bhttps%3A//galaxycong.github.io/LLM-Flow-Dubber/%7D%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01263v1&entry.124074799=Read"},
{"title": "EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an\n  Environmental-Aware Path Loss and Adaptive RSSI Smoothing", "author": "Nahshon Mokua Obiri and Kristof Van Laerhoven", "abstract": "  LoRaWAN technology's extensive coverage positions it as a strong contender\nfor large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor\nlocalization remains challenging due to complex environmental conditions,\nmultipath fading, and transient obstructions. This paper proposes a lightweight\nbut robust approach combining adaptive filtering with an extended log-distance,\nmulti-wall path loss and shadowing (PLS) model. Our methodology augments\nconventional models with critical LoRaWAN parameters (received signal strength\nindicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic\nenvironmental indicators (temperature, humidity, carbon dioxide, particulate\nmatter, and barometric pressure). An adaptive Kalman filter reduces RSSI\nfluctuations, isolating persistent trends from momentary noise. Using a\nsix-month dataset of 1,328,334 field measurements, we evaluate three models:\nthe baseline COST 231 multi-wall model (MWM), the baseline model augmented with\nenvironmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered\nRSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF\nachieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP\n(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation\nreduces systematic errors by 41.22%, while Kalman filtering significantly\nenhances robustness under high RSSI volatility by 42.63%, on average across all\ndevices. These findings present an interpretable, efficient solution for\nprecise indoor LoRaWAN localization in dynamically changing environments.\n", "link": "http://arxiv.org/abs/2505.01185v1", "date": "2025-05-02", "relevancy": 2.0414, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5492}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4863}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnviKal-Loc%3A%20Sub-10m%20Indoor%20LoRaWAN%20Localization%20using%20an%0A%20%20Environmental-Aware%20Path%20Loss%20and%20Adaptive%20RSSI%20Smoothing&body=Title%3A%20EnviKal-Loc%3A%20Sub-10m%20Indoor%20LoRaWAN%20Localization%20using%20an%0A%20%20Environmental-Aware%20Path%20Loss%20and%20Adaptive%20RSSI%20Smoothing%0AAuthor%3A%20Nahshon%20Mokua%20Obiri%20and%20Kristof%20Van%20Laerhoven%0AAbstract%3A%20%20%20LoRaWAN%20technology%27s%20extensive%20coverage%20positions%20it%20as%20a%20strong%20contender%0Afor%20large-scale%20IoT%20deployments.%20However%2C%20achieving%20sub-10%20m%20accuracy%20in%20indoor%0Alocalization%20remains%20challenging%20due%20to%20complex%20environmental%20conditions%2C%0Amultipath%20fading%2C%20and%20transient%20obstructions.%20This%20paper%20proposes%20a%20lightweight%0Abut%20robust%20approach%20combining%20adaptive%20filtering%20with%20an%20extended%20log-distance%2C%0Amulti-wall%20path%20loss%20and%20shadowing%20%28PLS%29%20model.%20Our%20methodology%20augments%0Aconventional%20models%20with%20critical%20LoRaWAN%20parameters%20%28received%20signal%20strength%0Aindicator%20%28RSSI%29%2C%20frequency%2C%20and%20signal-to-noise%20ratio%20%28SNR%29%29%20and%20dynamic%0Aenvironmental%20indicators%20%28temperature%2C%20humidity%2C%20carbon%20dioxide%2C%20particulate%0Amatter%2C%20and%20barometric%20pressure%29.%20An%20adaptive%20Kalman%20filter%20reduces%20RSSI%0Afluctuations%2C%20isolating%20persistent%20trends%20from%20momentary%20noise.%20Using%20a%0Asix-month%20dataset%20of%201%2C328%2C334%20field%20measurements%2C%20we%20evaluate%20three%20models%3A%0Athe%20baseline%20COST%20231%20multi-wall%20model%20%28MWM%29%2C%20the%20baseline%20model%20augmented%20with%0Aenvironmental%20parameters%20%28MWM-EP%29%2C%20and%20a%20forward-only%20adaptive%20Kalman-filtered%0ARSSI%20version%20of%20the%20latter%20%28MWM-EP-KF%29.%20Results%20confirm%20that%20the%20MWM-EP-KF%0Aachieves%20a%20mean%20absolute%20error%20%28MAE%29%20of%205.81%20m%2C%20outperforming%20both%20the%20MWM-EP%0A%2810.56%20m%29%20and%20the%20baseline%20MWM%20framework%20%2817.98%20m%29.%20Environmental%20augmentation%0Areduces%20systematic%20errors%20by%2041.22%25%2C%20while%20Kalman%20filtering%20significantly%0Aenhances%20robustness%20under%20high%20RSSI%20volatility%20by%2042.63%25%2C%20on%20average%20across%20all%0Adevices.%20These%20findings%20present%20an%20interpretable%2C%20efficient%20solution%20for%0Aprecise%20indoor%20LoRaWAN%20localization%20in%20dynamically%20changing%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnviKal-Loc%253A%2520Sub-10m%2520Indoor%2520LoRaWAN%2520Localization%2520using%2520an%250A%2520%2520Environmental-Aware%2520Path%2520Loss%2520and%2520Adaptive%2520RSSI%2520Smoothing%26entry.906535625%3DNahshon%2520Mokua%2520Obiri%2520and%2520Kristof%2520Van%2520Laerhoven%26entry.1292438233%3D%2520%2520LoRaWAN%2520technology%2527s%2520extensive%2520coverage%2520positions%2520it%2520as%2520a%2520strong%2520contender%250Afor%2520large-scale%2520IoT%2520deployments.%2520However%252C%2520achieving%2520sub-10%2520m%2520accuracy%2520in%2520indoor%250Alocalization%2520remains%2520challenging%2520due%2520to%2520complex%2520environmental%2520conditions%252C%250Amultipath%2520fading%252C%2520and%2520transient%2520obstructions.%2520This%2520paper%2520proposes%2520a%2520lightweight%250Abut%2520robust%2520approach%2520combining%2520adaptive%2520filtering%2520with%2520an%2520extended%2520log-distance%252C%250Amulti-wall%2520path%2520loss%2520and%2520shadowing%2520%2528PLS%2529%2520model.%2520Our%2520methodology%2520augments%250Aconventional%2520models%2520with%2520critical%2520LoRaWAN%2520parameters%2520%2528received%2520signal%2520strength%250Aindicator%2520%2528RSSI%2529%252C%2520frequency%252C%2520and%2520signal-to-noise%2520ratio%2520%2528SNR%2529%2529%2520and%2520dynamic%250Aenvironmental%2520indicators%2520%2528temperature%252C%2520humidity%252C%2520carbon%2520dioxide%252C%2520particulate%250Amatter%252C%2520and%2520barometric%2520pressure%2529.%2520An%2520adaptive%2520Kalman%2520filter%2520reduces%2520RSSI%250Afluctuations%252C%2520isolating%2520persistent%2520trends%2520from%2520momentary%2520noise.%2520Using%2520a%250Asix-month%2520dataset%2520of%25201%252C328%252C334%2520field%2520measurements%252C%2520we%2520evaluate%2520three%2520models%253A%250Athe%2520baseline%2520COST%2520231%2520multi-wall%2520model%2520%2528MWM%2529%252C%2520the%2520baseline%2520model%2520augmented%2520with%250Aenvironmental%2520parameters%2520%2528MWM-EP%2529%252C%2520and%2520a%2520forward-only%2520adaptive%2520Kalman-filtered%250ARSSI%2520version%2520of%2520the%2520latter%2520%2528MWM-EP-KF%2529.%2520Results%2520confirm%2520that%2520the%2520MWM-EP-KF%250Aachieves%2520a%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25205.81%2520m%252C%2520outperforming%2520both%2520the%2520MWM-EP%250A%252810.56%2520m%2529%2520and%2520the%2520baseline%2520MWM%2520framework%2520%252817.98%2520m%2529.%2520Environmental%2520augmentation%250Areduces%2520systematic%2520errors%2520by%252041.22%2525%252C%2520while%2520Kalman%2520filtering%2520significantly%250Aenhances%2520robustness%2520under%2520high%2520RSSI%2520volatility%2520by%252042.63%2525%252C%2520on%2520average%2520across%2520all%250Adevices.%2520These%2520findings%2520present%2520an%2520interpretable%252C%2520efficient%2520solution%2520for%250Aprecise%2520indoor%2520LoRaWAN%2520localization%2520in%2520dynamically%2520changing%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnviKal-Loc%3A%20Sub-10m%20Indoor%20LoRaWAN%20Localization%20using%20an%0A%20%20Environmental-Aware%20Path%20Loss%20and%20Adaptive%20RSSI%20Smoothing&entry.906535625=Nahshon%20Mokua%20Obiri%20and%20Kristof%20Van%20Laerhoven&entry.1292438233=%20%20LoRaWAN%20technology%27s%20extensive%20coverage%20positions%20it%20as%20a%20strong%20contender%0Afor%20large-scale%20IoT%20deployments.%20However%2C%20achieving%20sub-10%20m%20accuracy%20in%20indoor%0Alocalization%20remains%20challenging%20due%20to%20complex%20environmental%20conditions%2C%0Amultipath%20fading%2C%20and%20transient%20obstructions.%20This%20paper%20proposes%20a%20lightweight%0Abut%20robust%20approach%20combining%20adaptive%20filtering%20with%20an%20extended%20log-distance%2C%0Amulti-wall%20path%20loss%20and%20shadowing%20%28PLS%29%20model.%20Our%20methodology%20augments%0Aconventional%20models%20with%20critical%20LoRaWAN%20parameters%20%28received%20signal%20strength%0Aindicator%20%28RSSI%29%2C%20frequency%2C%20and%20signal-to-noise%20ratio%20%28SNR%29%29%20and%20dynamic%0Aenvironmental%20indicators%20%28temperature%2C%20humidity%2C%20carbon%20dioxide%2C%20particulate%0Amatter%2C%20and%20barometric%20pressure%29.%20An%20adaptive%20Kalman%20filter%20reduces%20RSSI%0Afluctuations%2C%20isolating%20persistent%20trends%20from%20momentary%20noise.%20Using%20a%0Asix-month%20dataset%20of%201%2C328%2C334%20field%20measurements%2C%20we%20evaluate%20three%20models%3A%0Athe%20baseline%20COST%20231%20multi-wall%20model%20%28MWM%29%2C%20the%20baseline%20model%20augmented%20with%0Aenvironmental%20parameters%20%28MWM-EP%29%2C%20and%20a%20forward-only%20adaptive%20Kalman-filtered%0ARSSI%20version%20of%20the%20latter%20%28MWM-EP-KF%29.%20Results%20confirm%20that%20the%20MWM-EP-KF%0Aachieves%20a%20mean%20absolute%20error%20%28MAE%29%20of%205.81%20m%2C%20outperforming%20both%20the%20MWM-EP%0A%2810.56%20m%29%20and%20the%20baseline%20MWM%20framework%20%2817.98%20m%29.%20Environmental%20augmentation%0Areduces%20systematic%20errors%20by%2041.22%25%2C%20while%20Kalman%20filtering%20significantly%0Aenhances%20robustness%20under%20high%20RSSI%20volatility%20by%2042.63%25%2C%20on%20average%20across%20all%0Adevices.%20These%20findings%20present%20an%20interpretable%2C%20efficient%20solution%20for%0Aprecise%20indoor%20LoRaWAN%20localization%20in%20dynamically%20changing%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01185v1&entry.124074799=Read"},
{"title": "DConAD: A Differencing-based Contrastive Representation Learning\n  Framework for Time Series Anomaly Detection", "author": "Wenxin Zhang and Xiaojian Lin and Wenjun Yu and Guangzhen Yao and jingxiang Zhong and Yu Li and Renda Han and Songcheng Xu and Hao Shi and Cuicui Luo", "abstract": "  Time series anomaly detection holds notable importance for risk\nidentification and fault detection across diverse application domains.\nUnsupervised learning methods have become popular because they have no\nrequirement for labels. However, due to the challenges posed by the\nmultiplicity of abnormal patterns, the sparsity of anomalies, and the growth of\ndata scale and complexity, these methods often fail to capture robust and\nrepresentative dependencies within the time series for identifying anomalies.\nTo enhance the ability of models to capture normal patterns of time series and\navoid the retrogression of modeling ability triggered by the dependencies on\nhigh-quality prior knowledge, we propose a differencing-based contrastive\nrepresentation learning framework for time series anomaly detection (DConAD).\nSpecifically, DConAD generates differential data to provide additional\ninformation about time series and utilizes transformer-based architecture to\ncapture spatiotemporal dependencies, which enhances the robustness of unbiased\nrepresentation learning ability. Furthermore, DConAD implements a novel KL\ndivergence-based contrastive learning paradigm that only uses positive samples\nto avoid deviation from reconstruction and deploys the stop-gradient strategy\nto compel convergence. Extensive experiments on five public datasets show the\nsuperiority and effectiveness of DConAD compared with nine baselines. The code\nis available at https://github.com/shaieesss/DConAD.\n", "link": "http://arxiv.org/abs/2504.14204v2", "date": "2025-05-02", "relevancy": 2.0103, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5113}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4977}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DConAD%3A%20A%20Differencing-based%20Contrastive%20Representation%20Learning%0A%20%20Framework%20for%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20DConAD%3A%20A%20Differencing-based%20Contrastive%20Representation%20Learning%0A%20%20Framework%20for%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Wenxin%20Zhang%20and%20Xiaojian%20Lin%20and%20Wenjun%20Yu%20and%20Guangzhen%20Yao%20and%20jingxiang%20Zhong%20and%20Yu%20Li%20and%20Renda%20Han%20and%20Songcheng%20Xu%20and%20Hao%20Shi%20and%20Cuicui%20Luo%0AAbstract%3A%20%20%20Time%20series%20anomaly%20detection%20holds%20notable%20importance%20for%20risk%0Aidentification%20and%20fault%20detection%20across%20diverse%20application%20domains.%0AUnsupervised%20learning%20methods%20have%20become%20popular%20because%20they%20have%20no%0Arequirement%20for%20labels.%20However%2C%20due%20to%20the%20challenges%20posed%20by%20the%0Amultiplicity%20of%20abnormal%20patterns%2C%20the%20sparsity%20of%20anomalies%2C%20and%20the%20growth%20of%0Adata%20scale%20and%20complexity%2C%20these%20methods%20often%20fail%20to%20capture%20robust%20and%0Arepresentative%20dependencies%20within%20the%20time%20series%20for%20identifying%20anomalies.%0ATo%20enhance%20the%20ability%20of%20models%20to%20capture%20normal%20patterns%20of%20time%20series%20and%0Aavoid%20the%20retrogression%20of%20modeling%20ability%20triggered%20by%20the%20dependencies%20on%0Ahigh-quality%20prior%20knowledge%2C%20we%20propose%20a%20differencing-based%20contrastive%0Arepresentation%20learning%20framework%20for%20time%20series%20anomaly%20detection%20%28DConAD%29.%0ASpecifically%2C%20DConAD%20generates%20differential%20data%20to%20provide%20additional%0Ainformation%20about%20time%20series%20and%20utilizes%20transformer-based%20architecture%20to%0Acapture%20spatiotemporal%20dependencies%2C%20which%20enhances%20the%20robustness%20of%20unbiased%0Arepresentation%20learning%20ability.%20Furthermore%2C%20DConAD%20implements%20a%20novel%20KL%0Adivergence-based%20contrastive%20learning%20paradigm%20that%20only%20uses%20positive%20samples%0Ato%20avoid%20deviation%20from%20reconstruction%20and%20deploys%20the%20stop-gradient%20strategy%0Ato%20compel%20convergence.%20Extensive%20experiments%20on%20five%20public%20datasets%20show%20the%0Asuperiority%20and%20effectiveness%20of%20DConAD%20compared%20with%20nine%20baselines.%20The%20code%0Ais%20available%20at%20https%3A//github.com/shaieesss/DConAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDConAD%253A%2520A%2520Differencing-based%2520Contrastive%2520Representation%2520Learning%250A%2520%2520Framework%2520for%2520Time%2520Series%2520Anomaly%2520Detection%26entry.906535625%3DWenxin%2520Zhang%2520and%2520Xiaojian%2520Lin%2520and%2520Wenjun%2520Yu%2520and%2520Guangzhen%2520Yao%2520and%2520jingxiang%2520Zhong%2520and%2520Yu%2520Li%2520and%2520Renda%2520Han%2520and%2520Songcheng%2520Xu%2520and%2520Hao%2520Shi%2520and%2520Cuicui%2520Luo%26entry.1292438233%3D%2520%2520Time%2520series%2520anomaly%2520detection%2520holds%2520notable%2520importance%2520for%2520risk%250Aidentification%2520and%2520fault%2520detection%2520across%2520diverse%2520application%2520domains.%250AUnsupervised%2520learning%2520methods%2520have%2520become%2520popular%2520because%2520they%2520have%2520no%250Arequirement%2520for%2520labels.%2520However%252C%2520due%2520to%2520the%2520challenges%2520posed%2520by%2520the%250Amultiplicity%2520of%2520abnormal%2520patterns%252C%2520the%2520sparsity%2520of%2520anomalies%252C%2520and%2520the%2520growth%2520of%250Adata%2520scale%2520and%2520complexity%252C%2520these%2520methods%2520often%2520fail%2520to%2520capture%2520robust%2520and%250Arepresentative%2520dependencies%2520within%2520the%2520time%2520series%2520for%2520identifying%2520anomalies.%250ATo%2520enhance%2520the%2520ability%2520of%2520models%2520to%2520capture%2520normal%2520patterns%2520of%2520time%2520series%2520and%250Aavoid%2520the%2520retrogression%2520of%2520modeling%2520ability%2520triggered%2520by%2520the%2520dependencies%2520on%250Ahigh-quality%2520prior%2520knowledge%252C%2520we%2520propose%2520a%2520differencing-based%2520contrastive%250Arepresentation%2520learning%2520framework%2520for%2520time%2520series%2520anomaly%2520detection%2520%2528DConAD%2529.%250ASpecifically%252C%2520DConAD%2520generates%2520differential%2520data%2520to%2520provide%2520additional%250Ainformation%2520about%2520time%2520series%2520and%2520utilizes%2520transformer-based%2520architecture%2520to%250Acapture%2520spatiotemporal%2520dependencies%252C%2520which%2520enhances%2520the%2520robustness%2520of%2520unbiased%250Arepresentation%2520learning%2520ability.%2520Furthermore%252C%2520DConAD%2520implements%2520a%2520novel%2520KL%250Adivergence-based%2520contrastive%2520learning%2520paradigm%2520that%2520only%2520uses%2520positive%2520samples%250Ato%2520avoid%2520deviation%2520from%2520reconstruction%2520and%2520deploys%2520the%2520stop-gradient%2520strategy%250Ato%2520compel%2520convergence.%2520Extensive%2520experiments%2520on%2520five%2520public%2520datasets%2520show%2520the%250Asuperiority%2520and%2520effectiveness%2520of%2520DConAD%2520compared%2520with%2520nine%2520baselines.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/shaieesss/DConAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DConAD%3A%20A%20Differencing-based%20Contrastive%20Representation%20Learning%0A%20%20Framework%20for%20Time%20Series%20Anomaly%20Detection&entry.906535625=Wenxin%20Zhang%20and%20Xiaojian%20Lin%20and%20Wenjun%20Yu%20and%20Guangzhen%20Yao%20and%20jingxiang%20Zhong%20and%20Yu%20Li%20and%20Renda%20Han%20and%20Songcheng%20Xu%20and%20Hao%20Shi%20and%20Cuicui%20Luo&entry.1292438233=%20%20Time%20series%20anomaly%20detection%20holds%20notable%20importance%20for%20risk%0Aidentification%20and%20fault%20detection%20across%20diverse%20application%20domains.%0AUnsupervised%20learning%20methods%20have%20become%20popular%20because%20they%20have%20no%0Arequirement%20for%20labels.%20However%2C%20due%20to%20the%20challenges%20posed%20by%20the%0Amultiplicity%20of%20abnormal%20patterns%2C%20the%20sparsity%20of%20anomalies%2C%20and%20the%20growth%20of%0Adata%20scale%20and%20complexity%2C%20these%20methods%20often%20fail%20to%20capture%20robust%20and%0Arepresentative%20dependencies%20within%20the%20time%20series%20for%20identifying%20anomalies.%0ATo%20enhance%20the%20ability%20of%20models%20to%20capture%20normal%20patterns%20of%20time%20series%20and%0Aavoid%20the%20retrogression%20of%20modeling%20ability%20triggered%20by%20the%20dependencies%20on%0Ahigh-quality%20prior%20knowledge%2C%20we%20propose%20a%20differencing-based%20contrastive%0Arepresentation%20learning%20framework%20for%20time%20series%20anomaly%20detection%20%28DConAD%29.%0ASpecifically%2C%20DConAD%20generates%20differential%20data%20to%20provide%20additional%0Ainformation%20about%20time%20series%20and%20utilizes%20transformer-based%20architecture%20to%0Acapture%20spatiotemporal%20dependencies%2C%20which%20enhances%20the%20robustness%20of%20unbiased%0Arepresentation%20learning%20ability.%20Furthermore%2C%20DConAD%20implements%20a%20novel%20KL%0Adivergence-based%20contrastive%20learning%20paradigm%20that%20only%20uses%20positive%20samples%0Ato%20avoid%20deviation%20from%20reconstruction%20and%20deploys%20the%20stop-gradient%20strategy%0Ato%20compel%20convergence.%20Extensive%20experiments%20on%20five%20public%20datasets%20show%20the%0Asuperiority%20and%20effectiveness%20of%20DConAD%20compared%20with%20nine%20baselines.%20The%20code%0Ais%20available%20at%20https%3A//github.com/shaieesss/DConAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14204v2&entry.124074799=Read"},
{"title": "Enhancing Obsolescence Forecasting with Deep Generative Data\n  Augmentation: A Semi-Supervised Framework for Low-Data Industrial\n  Applications", "author": "Elie Saad and Mariem Besbes and Marc Zolghadri and Victor Czmil and Claude Baron and Vincent Bourgeois", "abstract": "  The challenge of electronic component obsolescence is particularly critical\nin systems with long life cycles. Various obsolescence management methods are\nemployed to mitigate its impact, with obsolescence forecasting being a highly\nsought-after and prominent approach. As a result, numerous machine\nlearning-based forecasting methods have been proposed. However, machine\nlearning models require a substantial amount of relevant data to achieve high\nprecision, which is lacking in the current obsolescence landscape in some\nsituations. This work introduces a novel framework for obsolescence forecasting\nbased on deep learning. The proposed framework solves the lack of available\ndata through deep generative modeling, where new obsolescence cases are\ngenerated and used to augment the training dataset. The augmented dataset is\nthen used to train a classical machine learning-based obsolescence forecasting\nmodel. To train classical forecasting models using augmented datasets, existing\nclassical supervised-learning classifiers are adapted for semi-supervised\nlearning within this framework. The proposed framework demonstrates\nstate-of-the-art results on benchmarking datasets.\n", "link": "http://arxiv.org/abs/2505.01261v1", "date": "2025-05-02", "relevancy": 2.0027, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5387}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.504}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Obsolescence%20Forecasting%20with%20Deep%20Generative%20Data%0A%20%20Augmentation%3A%20A%20Semi-Supervised%20Framework%20for%20Low-Data%20Industrial%0A%20%20Applications&body=Title%3A%20Enhancing%20Obsolescence%20Forecasting%20with%20Deep%20Generative%20Data%0A%20%20Augmentation%3A%20A%20Semi-Supervised%20Framework%20for%20Low-Data%20Industrial%0A%20%20Applications%0AAuthor%3A%20Elie%20Saad%20and%20Mariem%20Besbes%20and%20Marc%20Zolghadri%20and%20Victor%20Czmil%20and%20Claude%20Baron%20and%20Vincent%20Bourgeois%0AAbstract%3A%20%20%20The%20challenge%20of%20electronic%20component%20obsolescence%20is%20particularly%20critical%0Ain%20systems%20with%20long%20life%20cycles.%20Various%20obsolescence%20management%20methods%20are%0Aemployed%20to%20mitigate%20its%20impact%2C%20with%20obsolescence%20forecasting%20being%20a%20highly%0Asought-after%20and%20prominent%20approach.%20As%20a%20result%2C%20numerous%20machine%0Alearning-based%20forecasting%20methods%20have%20been%20proposed.%20However%2C%20machine%0Alearning%20models%20require%20a%20substantial%20amount%20of%20relevant%20data%20to%20achieve%20high%0Aprecision%2C%20which%20is%20lacking%20in%20the%20current%20obsolescence%20landscape%20in%20some%0Asituations.%20This%20work%20introduces%20a%20novel%20framework%20for%20obsolescence%20forecasting%0Abased%20on%20deep%20learning.%20The%20proposed%20framework%20solves%20the%20lack%20of%20available%0Adata%20through%20deep%20generative%20modeling%2C%20where%20new%20obsolescence%20cases%20are%0Agenerated%20and%20used%20to%20augment%20the%20training%20dataset.%20The%20augmented%20dataset%20is%0Athen%20used%20to%20train%20a%20classical%20machine%20learning-based%20obsolescence%20forecasting%0Amodel.%20To%20train%20classical%20forecasting%20models%20using%20augmented%20datasets%2C%20existing%0Aclassical%20supervised-learning%20classifiers%20are%20adapted%20for%20semi-supervised%0Alearning%20within%20this%20framework.%20The%20proposed%20framework%20demonstrates%0Astate-of-the-art%20results%20on%20benchmarking%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Obsolescence%2520Forecasting%2520with%2520Deep%2520Generative%2520Data%250A%2520%2520Augmentation%253A%2520A%2520Semi-Supervised%2520Framework%2520for%2520Low-Data%2520Industrial%250A%2520%2520Applications%26entry.906535625%3DElie%2520Saad%2520and%2520Mariem%2520Besbes%2520and%2520Marc%2520Zolghadri%2520and%2520Victor%2520Czmil%2520and%2520Claude%2520Baron%2520and%2520Vincent%2520Bourgeois%26entry.1292438233%3D%2520%2520The%2520challenge%2520of%2520electronic%2520component%2520obsolescence%2520is%2520particularly%2520critical%250Ain%2520systems%2520with%2520long%2520life%2520cycles.%2520Various%2520obsolescence%2520management%2520methods%2520are%250Aemployed%2520to%2520mitigate%2520its%2520impact%252C%2520with%2520obsolescence%2520forecasting%2520being%2520a%2520highly%250Asought-after%2520and%2520prominent%2520approach.%2520As%2520a%2520result%252C%2520numerous%2520machine%250Alearning-based%2520forecasting%2520methods%2520have%2520been%2520proposed.%2520However%252C%2520machine%250Alearning%2520models%2520require%2520a%2520substantial%2520amount%2520of%2520relevant%2520data%2520to%2520achieve%2520high%250Aprecision%252C%2520which%2520is%2520lacking%2520in%2520the%2520current%2520obsolescence%2520landscape%2520in%2520some%250Asituations.%2520This%2520work%2520introduces%2520a%2520novel%2520framework%2520for%2520obsolescence%2520forecasting%250Abased%2520on%2520deep%2520learning.%2520The%2520proposed%2520framework%2520solves%2520the%2520lack%2520of%2520available%250Adata%2520through%2520deep%2520generative%2520modeling%252C%2520where%2520new%2520obsolescence%2520cases%2520are%250Agenerated%2520and%2520used%2520to%2520augment%2520the%2520training%2520dataset.%2520The%2520augmented%2520dataset%2520is%250Athen%2520used%2520to%2520train%2520a%2520classical%2520machine%2520learning-based%2520obsolescence%2520forecasting%250Amodel.%2520To%2520train%2520classical%2520forecasting%2520models%2520using%2520augmented%2520datasets%252C%2520existing%250Aclassical%2520supervised-learning%2520classifiers%2520are%2520adapted%2520for%2520semi-supervised%250Alearning%2520within%2520this%2520framework.%2520The%2520proposed%2520framework%2520demonstrates%250Astate-of-the-art%2520results%2520on%2520benchmarking%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Obsolescence%20Forecasting%20with%20Deep%20Generative%20Data%0A%20%20Augmentation%3A%20A%20Semi-Supervised%20Framework%20for%20Low-Data%20Industrial%0A%20%20Applications&entry.906535625=Elie%20Saad%20and%20Mariem%20Besbes%20and%20Marc%20Zolghadri%20and%20Victor%20Czmil%20and%20Claude%20Baron%20and%20Vincent%20Bourgeois&entry.1292438233=%20%20The%20challenge%20of%20electronic%20component%20obsolescence%20is%20particularly%20critical%0Ain%20systems%20with%20long%20life%20cycles.%20Various%20obsolescence%20management%20methods%20are%0Aemployed%20to%20mitigate%20its%20impact%2C%20with%20obsolescence%20forecasting%20being%20a%20highly%0Asought-after%20and%20prominent%20approach.%20As%20a%20result%2C%20numerous%20machine%0Alearning-based%20forecasting%20methods%20have%20been%20proposed.%20However%2C%20machine%0Alearning%20models%20require%20a%20substantial%20amount%20of%20relevant%20data%20to%20achieve%20high%0Aprecision%2C%20which%20is%20lacking%20in%20the%20current%20obsolescence%20landscape%20in%20some%0Asituations.%20This%20work%20introduces%20a%20novel%20framework%20for%20obsolescence%20forecasting%0Abased%20on%20deep%20learning.%20The%20proposed%20framework%20solves%20the%20lack%20of%20available%0Adata%20through%20deep%20generative%20modeling%2C%20where%20new%20obsolescence%20cases%20are%0Agenerated%20and%20used%20to%20augment%20the%20training%20dataset.%20The%20augmented%20dataset%20is%0Athen%20used%20to%20train%20a%20classical%20machine%20learning-based%20obsolescence%20forecasting%0Amodel.%20To%20train%20classical%20forecasting%20models%20using%20augmented%20datasets%2C%20existing%0Aclassical%20supervised-learning%20classifiers%20are%20adapted%20for%20semi-supervised%0Alearning%20within%20this%20framework.%20The%20proposed%20framework%20demonstrates%0Astate-of-the-art%20results%20on%20benchmarking%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01261v1&entry.124074799=Read"},
{"title": "MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for\n  Multi-Granular Spatiotemporal Traffic Forecasting", "author": "Zhaoyan Wang and Xiangchi Song and In-Young Ko", "abstract": "  Accurate traffic forecasting and swift inference provision are essential for\nintelligent transportation systems. However, the present Graph Convolutional\nNetwork (GCN)-based approaches cannot extract and fuse multi-granular\nspatiotemporal features across various spatial and temporal scales\nsufficiently, proven to yield less accurate forecasts. Besides, additional\nfeature extraction branches introduced in prior studies critically increased\nmodel complexity and extended inference time, making it challenging to provide\nfast inference for traffic forecasting. In this paper, we propose\nMultiGran-STGCNFog, an efficient fog distributed inference system with a novel\ntraffic forecasting model that employs multi-granular spatiotemporal feature\nfusion on generated dynamic traffic graphs to fully capture interdependent\ntraffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer\nexecution order and layer-device scheduling scheme simultaneously, contributes\nto considerable inference throughput improvement by leveraging heterogeneous\nfog devices in a pipelined manner. Extensive experiments on real-world datasets\ndemonstrate the superiority of the proposed method over selected baselines.\n", "link": "http://arxiv.org/abs/2505.01279v1", "date": "2025-05-02", "relevancy": 1.998, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5063}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.495}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiGran-STGCNFog%3A%20Towards%20Accurate%20and%20High-Throughput%20Inference%20for%0A%20%20Multi-Granular%20Spatiotemporal%20Traffic%20Forecasting&body=Title%3A%20MultiGran-STGCNFog%3A%20Towards%20Accurate%20and%20High-Throughput%20Inference%20for%0A%20%20Multi-Granular%20Spatiotemporal%20Traffic%20Forecasting%0AAuthor%3A%20Zhaoyan%20Wang%20and%20Xiangchi%20Song%20and%20In-Young%20Ko%0AAbstract%3A%20%20%20Accurate%20traffic%20forecasting%20and%20swift%20inference%20provision%20are%20essential%20for%0Aintelligent%20transportation%20systems.%20However%2C%20the%20present%20Graph%20Convolutional%0ANetwork%20%28GCN%29-based%20approaches%20cannot%20extract%20and%20fuse%20multi-granular%0Aspatiotemporal%20features%20across%20various%20spatial%20and%20temporal%20scales%0Asufficiently%2C%20proven%20to%20yield%20less%20accurate%20forecasts.%20Besides%2C%20additional%0Afeature%20extraction%20branches%20introduced%20in%20prior%20studies%20critically%20increased%0Amodel%20complexity%20and%20extended%20inference%20time%2C%20making%20it%20challenging%20to%20provide%0Afast%20inference%20for%20traffic%20forecasting.%20In%20this%20paper%2C%20we%20propose%0AMultiGran-STGCNFog%2C%20an%20efficient%20fog%20distributed%20inference%20system%20with%20a%20novel%0Atraffic%20forecasting%20model%20that%20employs%20multi-granular%20spatiotemporal%20feature%0Afusion%20on%20generated%20dynamic%20traffic%20graphs%20to%20fully%20capture%20interdependent%0Atraffic%20dynamics.%20The%20proposed%20scheduling%20algorithm%20GA-DPHDS%2C%20optimizing%20layer%0Aexecution%20order%20and%20layer-device%20scheduling%20scheme%20simultaneously%2C%20contributes%0Ato%20considerable%20inference%20throughput%20improvement%20by%20leveraging%20heterogeneous%0Afog%20devices%20in%20a%20pipelined%20manner.%20Extensive%20experiments%20on%20real-world%20datasets%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20method%20over%20selected%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiGran-STGCNFog%253A%2520Towards%2520Accurate%2520and%2520High-Throughput%2520Inference%2520for%250A%2520%2520Multi-Granular%2520Spatiotemporal%2520Traffic%2520Forecasting%26entry.906535625%3DZhaoyan%2520Wang%2520and%2520Xiangchi%2520Song%2520and%2520In-Young%2520Ko%26entry.1292438233%3D%2520%2520Accurate%2520traffic%2520forecasting%2520and%2520swift%2520inference%2520provision%2520are%2520essential%2520for%250Aintelligent%2520transportation%2520systems.%2520However%252C%2520the%2520present%2520Graph%2520Convolutional%250ANetwork%2520%2528GCN%2529-based%2520approaches%2520cannot%2520extract%2520and%2520fuse%2520multi-granular%250Aspatiotemporal%2520features%2520across%2520various%2520spatial%2520and%2520temporal%2520scales%250Asufficiently%252C%2520proven%2520to%2520yield%2520less%2520accurate%2520forecasts.%2520Besides%252C%2520additional%250Afeature%2520extraction%2520branches%2520introduced%2520in%2520prior%2520studies%2520critically%2520increased%250Amodel%2520complexity%2520and%2520extended%2520inference%2520time%252C%2520making%2520it%2520challenging%2520to%2520provide%250Afast%2520inference%2520for%2520traffic%2520forecasting.%2520In%2520this%2520paper%252C%2520we%2520propose%250AMultiGran-STGCNFog%252C%2520an%2520efficient%2520fog%2520distributed%2520inference%2520system%2520with%2520a%2520novel%250Atraffic%2520forecasting%2520model%2520that%2520employs%2520multi-granular%2520spatiotemporal%2520feature%250Afusion%2520on%2520generated%2520dynamic%2520traffic%2520graphs%2520to%2520fully%2520capture%2520interdependent%250Atraffic%2520dynamics.%2520The%2520proposed%2520scheduling%2520algorithm%2520GA-DPHDS%252C%2520optimizing%2520layer%250Aexecution%2520order%2520and%2520layer-device%2520scheduling%2520scheme%2520simultaneously%252C%2520contributes%250Ato%2520considerable%2520inference%2520throughput%2520improvement%2520by%2520leveraging%2520heterogeneous%250Afog%2520devices%2520in%2520a%2520pipelined%2520manner.%2520Extensive%2520experiments%2520on%2520real-world%2520datasets%250Ademonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520method%2520over%2520selected%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiGran-STGCNFog%3A%20Towards%20Accurate%20and%20High-Throughput%20Inference%20for%0A%20%20Multi-Granular%20Spatiotemporal%20Traffic%20Forecasting&entry.906535625=Zhaoyan%20Wang%20and%20Xiangchi%20Song%20and%20In-Young%20Ko&entry.1292438233=%20%20Accurate%20traffic%20forecasting%20and%20swift%20inference%20provision%20are%20essential%20for%0Aintelligent%20transportation%20systems.%20However%2C%20the%20present%20Graph%20Convolutional%0ANetwork%20%28GCN%29-based%20approaches%20cannot%20extract%20and%20fuse%20multi-granular%0Aspatiotemporal%20features%20across%20various%20spatial%20and%20temporal%20scales%0Asufficiently%2C%20proven%20to%20yield%20less%20accurate%20forecasts.%20Besides%2C%20additional%0Afeature%20extraction%20branches%20introduced%20in%20prior%20studies%20critically%20increased%0Amodel%20complexity%20and%20extended%20inference%20time%2C%20making%20it%20challenging%20to%20provide%0Afast%20inference%20for%20traffic%20forecasting.%20In%20this%20paper%2C%20we%20propose%0AMultiGran-STGCNFog%2C%20an%20efficient%20fog%20distributed%20inference%20system%20with%20a%20novel%0Atraffic%20forecasting%20model%20that%20employs%20multi-granular%20spatiotemporal%20feature%0Afusion%20on%20generated%20dynamic%20traffic%20graphs%20to%20fully%20capture%20interdependent%0Atraffic%20dynamics.%20The%20proposed%20scheduling%20algorithm%20GA-DPHDS%2C%20optimizing%20layer%0Aexecution%20order%20and%20layer-device%20scheduling%20scheme%20simultaneously%2C%20contributes%0Ato%20considerable%20inference%20throughput%20improvement%20by%20leveraging%20heterogeneous%0Afog%20devices%20in%20a%20pipelined%20manner.%20Extensive%20experiments%20on%20real-world%20datasets%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20method%20over%20selected%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01279v1&entry.124074799=Read"},
{"title": "Secure Cluster-Based Hierarchical Federated Learning in Vehicular\n  Networks", "author": "M. Saeid HaghighiFard and Sinem Coleri", "abstract": "  Hierarchical Federated Learning (HFL) has recently emerged as a promising\nsolution for intelligent decision-making in vehicular networks, helping to\naddress challenges such as limited communication resources, high vehicle\nmobility, and data heterogeneity. However, HFL remains vulnerable to\nadversarial and unreliable vehicles, whose misleading updates can significantly\ncompromise the integrity and convergence of the global model. To address these\nchallenges, we propose a novel defense framework that integrates dynamic\nvehicle selection with robust anomaly detection within a cluster-based HFL\narchitecture, specifically designed to counter Gaussian noise and gradient\nascent attacks. The framework performs a comprehensive reliability assessment\nfor each vehicle by evaluating historical accuracy, contribution frequency, and\nanomaly records. Anomaly detection combines Z-score and cosine similarity\nanalyses on model updates to identify both statistical outliers and directional\ndeviations in model updates. To further refine detection, an adaptive\nthresholding mechanism is incorporated into the cosine similarity metric,\ndynamically adjusting the threshold based on the historical accuracy of each\nvehicle to enforce stricter standards for consistently high-performing\nvehicles. In addition, a weighted gradient averaging mechanism is implemented,\nwhich assigns higher weights to gradient updates from more trustworthy\nvehicles. To defend against coordinated attacks, a cross-cluster consistency\ncheck is applied to identify collaborative attacks in which multiple\ncompromised clusters coordinate misleading updates. Together, these mechanisms\nform a multi-level defense strategy to filter out malicious contributions\neffectively. Simulation results show that the proposed algorithm significantly\nreduces convergence time compared to benchmark methods across both 1-hop and\n3-hop topologies.\n", "link": "http://arxiv.org/abs/2505.01186v1", "date": "2025-05-02", "relevancy": 1.9906, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4791}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Secure%20Cluster-Based%20Hierarchical%20Federated%20Learning%20in%20Vehicular%0A%20%20Networks&body=Title%3A%20Secure%20Cluster-Based%20Hierarchical%20Federated%20Learning%20in%20Vehicular%0A%20%20Networks%0AAuthor%3A%20M.%20Saeid%20HaghighiFard%20and%20Sinem%20Coleri%0AAbstract%3A%20%20%20Hierarchical%20Federated%20Learning%20%28HFL%29%20has%20recently%20emerged%20as%20a%20promising%0Asolution%20for%20intelligent%20decision-making%20in%20vehicular%20networks%2C%20helping%20to%0Aaddress%20challenges%20such%20as%20limited%20communication%20resources%2C%20high%20vehicle%0Amobility%2C%20and%20data%20heterogeneity.%20However%2C%20HFL%20remains%20vulnerable%20to%0Aadversarial%20and%20unreliable%20vehicles%2C%20whose%20misleading%20updates%20can%20significantly%0Acompromise%20the%20integrity%20and%20convergence%20of%20the%20global%20model.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20defense%20framework%20that%20integrates%20dynamic%0Avehicle%20selection%20with%20robust%20anomaly%20detection%20within%20a%20cluster-based%20HFL%0Aarchitecture%2C%20specifically%20designed%20to%20counter%20Gaussian%20noise%20and%20gradient%0Aascent%20attacks.%20The%20framework%20performs%20a%20comprehensive%20reliability%20assessment%0Afor%20each%20vehicle%20by%20evaluating%20historical%20accuracy%2C%20contribution%20frequency%2C%20and%0Aanomaly%20records.%20Anomaly%20detection%20combines%20Z-score%20and%20cosine%20similarity%0Aanalyses%20on%20model%20updates%20to%20identify%20both%20statistical%20outliers%20and%20directional%0Adeviations%20in%20model%20updates.%20To%20further%20refine%20detection%2C%20an%20adaptive%0Athresholding%20mechanism%20is%20incorporated%20into%20the%20cosine%20similarity%20metric%2C%0Adynamically%20adjusting%20the%20threshold%20based%20on%20the%20historical%20accuracy%20of%20each%0Avehicle%20to%20enforce%20stricter%20standards%20for%20consistently%20high-performing%0Avehicles.%20In%20addition%2C%20a%20weighted%20gradient%20averaging%20mechanism%20is%20implemented%2C%0Awhich%20assigns%20higher%20weights%20to%20gradient%20updates%20from%20more%20trustworthy%0Avehicles.%20To%20defend%20against%20coordinated%20attacks%2C%20a%20cross-cluster%20consistency%0Acheck%20is%20applied%20to%20identify%20collaborative%20attacks%20in%20which%20multiple%0Acompromised%20clusters%20coordinate%20misleading%20updates.%20Together%2C%20these%20mechanisms%0Aform%20a%20multi-level%20defense%20strategy%20to%20filter%20out%20malicious%20contributions%0Aeffectively.%20Simulation%20results%20show%20that%20the%20proposed%20algorithm%20significantly%0Areduces%20convergence%20time%20compared%20to%20benchmark%20methods%20across%20both%201-hop%20and%0A3-hop%20topologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecure%2520Cluster-Based%2520Hierarchical%2520Federated%2520Learning%2520in%2520Vehicular%250A%2520%2520Networks%26entry.906535625%3DM.%2520Saeid%2520HaghighiFard%2520and%2520Sinem%2520Coleri%26entry.1292438233%3D%2520%2520Hierarchical%2520Federated%2520Learning%2520%2528HFL%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%250Asolution%2520for%2520intelligent%2520decision-making%2520in%2520vehicular%2520networks%252C%2520helping%2520to%250Aaddress%2520challenges%2520such%2520as%2520limited%2520communication%2520resources%252C%2520high%2520vehicle%250Amobility%252C%2520and%2520data%2520heterogeneity.%2520However%252C%2520HFL%2520remains%2520vulnerable%2520to%250Aadversarial%2520and%2520unreliable%2520vehicles%252C%2520whose%2520misleading%2520updates%2520can%2520significantly%250Acompromise%2520the%2520integrity%2520and%2520convergence%2520of%2520the%2520global%2520model.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520defense%2520framework%2520that%2520integrates%2520dynamic%250Avehicle%2520selection%2520with%2520robust%2520anomaly%2520detection%2520within%2520a%2520cluster-based%2520HFL%250Aarchitecture%252C%2520specifically%2520designed%2520to%2520counter%2520Gaussian%2520noise%2520and%2520gradient%250Aascent%2520attacks.%2520The%2520framework%2520performs%2520a%2520comprehensive%2520reliability%2520assessment%250Afor%2520each%2520vehicle%2520by%2520evaluating%2520historical%2520accuracy%252C%2520contribution%2520frequency%252C%2520and%250Aanomaly%2520records.%2520Anomaly%2520detection%2520combines%2520Z-score%2520and%2520cosine%2520similarity%250Aanalyses%2520on%2520model%2520updates%2520to%2520identify%2520both%2520statistical%2520outliers%2520and%2520directional%250Adeviations%2520in%2520model%2520updates.%2520To%2520further%2520refine%2520detection%252C%2520an%2520adaptive%250Athresholding%2520mechanism%2520is%2520incorporated%2520into%2520the%2520cosine%2520similarity%2520metric%252C%250Adynamically%2520adjusting%2520the%2520threshold%2520based%2520on%2520the%2520historical%2520accuracy%2520of%2520each%250Avehicle%2520to%2520enforce%2520stricter%2520standards%2520for%2520consistently%2520high-performing%250Avehicles.%2520In%2520addition%252C%2520a%2520weighted%2520gradient%2520averaging%2520mechanism%2520is%2520implemented%252C%250Awhich%2520assigns%2520higher%2520weights%2520to%2520gradient%2520updates%2520from%2520more%2520trustworthy%250Avehicles.%2520To%2520defend%2520against%2520coordinated%2520attacks%252C%2520a%2520cross-cluster%2520consistency%250Acheck%2520is%2520applied%2520to%2520identify%2520collaborative%2520attacks%2520in%2520which%2520multiple%250Acompromised%2520clusters%2520coordinate%2520misleading%2520updates.%2520Together%252C%2520these%2520mechanisms%250Aform%2520a%2520multi-level%2520defense%2520strategy%2520to%2520filter%2520out%2520malicious%2520contributions%250Aeffectively.%2520Simulation%2520results%2520show%2520that%2520the%2520proposed%2520algorithm%2520significantly%250Areduces%2520convergence%2520time%2520compared%2520to%2520benchmark%2520methods%2520across%2520both%25201-hop%2520and%250A3-hop%2520topologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Secure%20Cluster-Based%20Hierarchical%20Federated%20Learning%20in%20Vehicular%0A%20%20Networks&entry.906535625=M.%20Saeid%20HaghighiFard%20and%20Sinem%20Coleri&entry.1292438233=%20%20Hierarchical%20Federated%20Learning%20%28HFL%29%20has%20recently%20emerged%20as%20a%20promising%0Asolution%20for%20intelligent%20decision-making%20in%20vehicular%20networks%2C%20helping%20to%0Aaddress%20challenges%20such%20as%20limited%20communication%20resources%2C%20high%20vehicle%0Amobility%2C%20and%20data%20heterogeneity.%20However%2C%20HFL%20remains%20vulnerable%20to%0Aadversarial%20and%20unreliable%20vehicles%2C%20whose%20misleading%20updates%20can%20significantly%0Acompromise%20the%20integrity%20and%20convergence%20of%20the%20global%20model.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20defense%20framework%20that%20integrates%20dynamic%0Avehicle%20selection%20with%20robust%20anomaly%20detection%20within%20a%20cluster-based%20HFL%0Aarchitecture%2C%20specifically%20designed%20to%20counter%20Gaussian%20noise%20and%20gradient%0Aascent%20attacks.%20The%20framework%20performs%20a%20comprehensive%20reliability%20assessment%0Afor%20each%20vehicle%20by%20evaluating%20historical%20accuracy%2C%20contribution%20frequency%2C%20and%0Aanomaly%20records.%20Anomaly%20detection%20combines%20Z-score%20and%20cosine%20similarity%0Aanalyses%20on%20model%20updates%20to%20identify%20both%20statistical%20outliers%20and%20directional%0Adeviations%20in%20model%20updates.%20To%20further%20refine%20detection%2C%20an%20adaptive%0Athresholding%20mechanism%20is%20incorporated%20into%20the%20cosine%20similarity%20metric%2C%0Adynamically%20adjusting%20the%20threshold%20based%20on%20the%20historical%20accuracy%20of%20each%0Avehicle%20to%20enforce%20stricter%20standards%20for%20consistently%20high-performing%0Avehicles.%20In%20addition%2C%20a%20weighted%20gradient%20averaging%20mechanism%20is%20implemented%2C%0Awhich%20assigns%20higher%20weights%20to%20gradient%20updates%20from%20more%20trustworthy%0Avehicles.%20To%20defend%20against%20coordinated%20attacks%2C%20a%20cross-cluster%20consistency%0Acheck%20is%20applied%20to%20identify%20collaborative%20attacks%20in%20which%20multiple%0Acompromised%20clusters%20coordinate%20misleading%20updates.%20Together%2C%20these%20mechanisms%0Aform%20a%20multi-level%20defense%20strategy%20to%20filter%20out%20malicious%20contributions%0Aeffectively.%20Simulation%20results%20show%20that%20the%20proposed%20algorithm%20significantly%0Areduces%20convergence%20time%20compared%20to%20benchmark%20methods%20across%20both%201-hop%20and%0A3-hop%20topologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01186v1&entry.124074799=Read"},
{"title": "Monitoring morphometric drift in lifelong learning segmentation of the\n  spinal cord", "author": "Enamundram Naga Karthik and Sandrine B\u00e9dard and Jan Valo\u0161ek and Christoph S. Aigner and Elise Bannier and Josef Bedna\u0159\u00edk and Virginie Callot and Anna Combes and Armin Curt and Gergely David and Falk Eippert and Lynn Farner and Michael G Fehlings and Patrick Freund and Tobias Granberg and Cristina Granziera and RHSCIR Network Imaging Group and Ulrike Horn and Tom\u00e1\u0161 Hor\u00e1k and Suzanne Humphreys and Markus Hupp and Anne Kerbrat and Nawal Kinany and Shannon Kolind and Petr Kudli\u010dka and Anna Lebret and Lisa Eunyoung Lee and Caterina Mainero and Allan R. Martin and Megan McGrath and Govind Nair and Kristin P. O'Grady and Jiwon Oh and Russell Ouellette and Nikolai Pfender and Dario Pfyffer and Pierre-Fran\u00e7ois Pradat and Alexandre Prat and Emanuele Pravat\u00e0 and Daniel S. Reich and Ilaria Ricchi and Naama Rotem-Kohavi and Simon Schading-Sassenhausen and Maryam Seif and Andrew Smith and Seth A Smith and Grace Sweeney and Roger Tam and Anthony Traboulsee and Constantina Andrada Treaba and Charidimos Tsagkas and Zachary Vavasour and Dimitri Van De Ville and Kenneth Arnold Weber II and Sarath Chandar and Julien Cohen-Adad", "abstract": "  Morphometric measures derived from spinal cord segmentations can serve as\ndiagnostic and prognostic biomarkers in neurological diseases and injuries\naffecting the spinal cord. While robust, automatic segmentation methods to a\nwide variety of contrasts and pathologies have been developed over the past few\nyears, whether their predictions are stable as the model is updated using new\ndatasets has not been assessed. This is particularly important for deriving\nnormative values from healthy participants. In this study, we present a spinal\ncord segmentation model trained on a multisite $(n=75)$ dataset, including 9\ndifferent MRI contrasts and several spinal cord pathologies. We also introduce\na lifelong learning framework to automatically monitor the morphometric drift\nas the model is updated using additional datasets. The framework is triggered\nby an automatic GitHub Actions workflow every time a new model is created,\nrecording the morphometric values derived from the model's predictions over\ntime. As a real-world application of the proposed framework, we employed the\nspinal cord segmentation model to update a recently-introduced normative\ndatabase of healthy participants containing commonly used measures of spinal\ncord morphometry. Results showed that: (i) our model outperforms previous\nversions and pathology-specific models on challenging lumbar spinal cord cases,\nachieving an average Dice score of $0.95 \\pm 0.03$; (ii) the automatic workflow\nfor monitoring morphometric drift provides a quick feedback loop for developing\nfuture segmentation models; and (iii) the scaling factor required to update the\ndatabase of morphometric measures is nearly constant among slices across the\ngiven vertebral levels, showing minimum drift between the current and previous\nversions of the model monitored by the framework. The model is freely available\nin Spinal Cord Toolbox v7.0.\n", "link": "http://arxiv.org/abs/2505.01364v1", "date": "2025-05-02", "relevancy": 1.9877, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monitoring%20morphometric%20drift%20in%20lifelong%20learning%20segmentation%20of%20the%0A%20%20spinal%20cord&body=Title%3A%20Monitoring%20morphometric%20drift%20in%20lifelong%20learning%20segmentation%20of%20the%0A%20%20spinal%20cord%0AAuthor%3A%20Enamundram%20Naga%20Karthik%20and%20Sandrine%20B%C3%A9dard%20and%20Jan%20Valo%C5%A1ek%20and%20Christoph%20S.%20Aigner%20and%20Elise%20Bannier%20and%20Josef%20Bedna%C5%99%C3%ADk%20and%20Virginie%20Callot%20and%20Anna%20Combes%20and%20Armin%20Curt%20and%20Gergely%20David%20and%20Falk%20Eippert%20and%20Lynn%20Farner%20and%20Michael%20G%20Fehlings%20and%20Patrick%20Freund%20and%20Tobias%20Granberg%20and%20Cristina%20Granziera%20and%20RHSCIR%20Network%20Imaging%20Group%20and%20Ulrike%20Horn%20and%20Tom%C3%A1%C5%A1%20Hor%C3%A1k%20and%20Suzanne%20Humphreys%20and%20Markus%20Hupp%20and%20Anne%20Kerbrat%20and%20Nawal%20Kinany%20and%20Shannon%20Kolind%20and%20Petr%20Kudli%C4%8Dka%20and%20Anna%20Lebret%20and%20Lisa%20Eunyoung%20Lee%20and%20Caterina%20Mainero%20and%20Allan%20R.%20Martin%20and%20Megan%20McGrath%20and%20Govind%20Nair%20and%20Kristin%20P.%20O%27Grady%20and%20Jiwon%20Oh%20and%20Russell%20Ouellette%20and%20Nikolai%20Pfender%20and%20Dario%20Pfyffer%20and%20Pierre-Fran%C3%A7ois%20Pradat%20and%20Alexandre%20Prat%20and%20Emanuele%20Pravat%C3%A0%20and%20Daniel%20S.%20Reich%20and%20Ilaria%20Ricchi%20and%20Naama%20Rotem-Kohavi%20and%20Simon%20Schading-Sassenhausen%20and%20Maryam%20Seif%20and%20Andrew%20Smith%20and%20Seth%20A%20Smith%20and%20Grace%20Sweeney%20and%20Roger%20Tam%20and%20Anthony%20Traboulsee%20and%20Constantina%20Andrada%20Treaba%20and%20Charidimos%20Tsagkas%20and%20Zachary%20Vavasour%20and%20Dimitri%20Van%20De%20Ville%20and%20Kenneth%20Arnold%20Weber%20II%20and%20Sarath%20Chandar%20and%20Julien%20Cohen-Adad%0AAbstract%3A%20%20%20Morphometric%20measures%20derived%20from%20spinal%20cord%20segmentations%20can%20serve%20as%0Adiagnostic%20and%20prognostic%20biomarkers%20in%20neurological%20diseases%20and%20injuries%0Aaffecting%20the%20spinal%20cord.%20While%20robust%2C%20automatic%20segmentation%20methods%20to%20a%0Awide%20variety%20of%20contrasts%20and%20pathologies%20have%20been%20developed%20over%20the%20past%20few%0Ayears%2C%20whether%20their%20predictions%20are%20stable%20as%20the%20model%20is%20updated%20using%20new%0Adatasets%20has%20not%20been%20assessed.%20This%20is%20particularly%20important%20for%20deriving%0Anormative%20values%20from%20healthy%20participants.%20In%20this%20study%2C%20we%20present%20a%20spinal%0Acord%20segmentation%20model%20trained%20on%20a%20multisite%20%24%28n%3D75%29%24%20dataset%2C%20including%209%0Adifferent%20MRI%20contrasts%20and%20several%20spinal%20cord%20pathologies.%20We%20also%20introduce%0Aa%20lifelong%20learning%20framework%20to%20automatically%20monitor%20the%20morphometric%20drift%0Aas%20the%20model%20is%20updated%20using%20additional%20datasets.%20The%20framework%20is%20triggered%0Aby%20an%20automatic%20GitHub%20Actions%20workflow%20every%20time%20a%20new%20model%20is%20created%2C%0Arecording%20the%20morphometric%20values%20derived%20from%20the%20model%27s%20predictions%20over%0Atime.%20As%20a%20real-world%20application%20of%20the%20proposed%20framework%2C%20we%20employed%20the%0Aspinal%20cord%20segmentation%20model%20to%20update%20a%20recently-introduced%20normative%0Adatabase%20of%20healthy%20participants%20containing%20commonly%20used%20measures%20of%20spinal%0Acord%20morphometry.%20Results%20showed%20that%3A%20%28i%29%20our%20model%20outperforms%20previous%0Aversions%20and%20pathology-specific%20models%20on%20challenging%20lumbar%20spinal%20cord%20cases%2C%0Aachieving%20an%20average%20Dice%20score%20of%20%240.95%20%5Cpm%200.03%24%3B%20%28ii%29%20the%20automatic%20workflow%0Afor%20monitoring%20morphometric%20drift%20provides%20a%20quick%20feedback%20loop%20for%20developing%0Afuture%20segmentation%20models%3B%20and%20%28iii%29%20the%20scaling%20factor%20required%20to%20update%20the%0Adatabase%20of%20morphometric%20measures%20is%20nearly%20constant%20among%20slices%20across%20the%0Agiven%20vertebral%20levels%2C%20showing%20minimum%20drift%20between%20the%20current%20and%20previous%0Aversions%20of%20the%20model%20monitored%20by%20the%20framework.%20The%20model%20is%20freely%20available%0Ain%20Spinal%20Cord%20Toolbox%20v7.0.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonitoring%2520morphometric%2520drift%2520in%2520lifelong%2520learning%2520segmentation%2520of%2520the%250A%2520%2520spinal%2520cord%26entry.906535625%3DEnamundram%2520Naga%2520Karthik%2520and%2520Sandrine%2520B%25C3%25A9dard%2520and%2520Jan%2520Valo%25C5%25A1ek%2520and%2520Christoph%2520S.%2520Aigner%2520and%2520Elise%2520Bannier%2520and%2520Josef%2520Bedna%25C5%2599%25C3%25ADk%2520and%2520Virginie%2520Callot%2520and%2520Anna%2520Combes%2520and%2520Armin%2520Curt%2520and%2520Gergely%2520David%2520and%2520Falk%2520Eippert%2520and%2520Lynn%2520Farner%2520and%2520Michael%2520G%2520Fehlings%2520and%2520Patrick%2520Freund%2520and%2520Tobias%2520Granberg%2520and%2520Cristina%2520Granziera%2520and%2520RHSCIR%2520Network%2520Imaging%2520Group%2520and%2520Ulrike%2520Horn%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Hor%25C3%25A1k%2520and%2520Suzanne%2520Humphreys%2520and%2520Markus%2520Hupp%2520and%2520Anne%2520Kerbrat%2520and%2520Nawal%2520Kinany%2520and%2520Shannon%2520Kolind%2520and%2520Petr%2520Kudli%25C4%258Dka%2520and%2520Anna%2520Lebret%2520and%2520Lisa%2520Eunyoung%2520Lee%2520and%2520Caterina%2520Mainero%2520and%2520Allan%2520R.%2520Martin%2520and%2520Megan%2520McGrath%2520and%2520Govind%2520Nair%2520and%2520Kristin%2520P.%2520O%2527Grady%2520and%2520Jiwon%2520Oh%2520and%2520Russell%2520Ouellette%2520and%2520Nikolai%2520Pfender%2520and%2520Dario%2520Pfyffer%2520and%2520Pierre-Fran%25C3%25A7ois%2520Pradat%2520and%2520Alexandre%2520Prat%2520and%2520Emanuele%2520Pravat%25C3%25A0%2520and%2520Daniel%2520S.%2520Reich%2520and%2520Ilaria%2520Ricchi%2520and%2520Naama%2520Rotem-Kohavi%2520and%2520Simon%2520Schading-Sassenhausen%2520and%2520Maryam%2520Seif%2520and%2520Andrew%2520Smith%2520and%2520Seth%2520A%2520Smith%2520and%2520Grace%2520Sweeney%2520and%2520Roger%2520Tam%2520and%2520Anthony%2520Traboulsee%2520and%2520Constantina%2520Andrada%2520Treaba%2520and%2520Charidimos%2520Tsagkas%2520and%2520Zachary%2520Vavasour%2520and%2520Dimitri%2520Van%2520De%2520Ville%2520and%2520Kenneth%2520Arnold%2520Weber%2520II%2520and%2520Sarath%2520Chandar%2520and%2520Julien%2520Cohen-Adad%26entry.1292438233%3D%2520%2520Morphometric%2520measures%2520derived%2520from%2520spinal%2520cord%2520segmentations%2520can%2520serve%2520as%250Adiagnostic%2520and%2520prognostic%2520biomarkers%2520in%2520neurological%2520diseases%2520and%2520injuries%250Aaffecting%2520the%2520spinal%2520cord.%2520While%2520robust%252C%2520automatic%2520segmentation%2520methods%2520to%2520a%250Awide%2520variety%2520of%2520contrasts%2520and%2520pathologies%2520have%2520been%2520developed%2520over%2520the%2520past%2520few%250Ayears%252C%2520whether%2520their%2520predictions%2520are%2520stable%2520as%2520the%2520model%2520is%2520updated%2520using%2520new%250Adatasets%2520has%2520not%2520been%2520assessed.%2520This%2520is%2520particularly%2520important%2520for%2520deriving%250Anormative%2520values%2520from%2520healthy%2520participants.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520spinal%250Acord%2520segmentation%2520model%2520trained%2520on%2520a%2520multisite%2520%2524%2528n%253D75%2529%2524%2520dataset%252C%2520including%25209%250Adifferent%2520MRI%2520contrasts%2520and%2520several%2520spinal%2520cord%2520pathologies.%2520We%2520also%2520introduce%250Aa%2520lifelong%2520learning%2520framework%2520to%2520automatically%2520monitor%2520the%2520morphometric%2520drift%250Aas%2520the%2520model%2520is%2520updated%2520using%2520additional%2520datasets.%2520The%2520framework%2520is%2520triggered%250Aby%2520an%2520automatic%2520GitHub%2520Actions%2520workflow%2520every%2520time%2520a%2520new%2520model%2520is%2520created%252C%250Arecording%2520the%2520morphometric%2520values%2520derived%2520from%2520the%2520model%2527s%2520predictions%2520over%250Atime.%2520As%2520a%2520real-world%2520application%2520of%2520the%2520proposed%2520framework%252C%2520we%2520employed%2520the%250Aspinal%2520cord%2520segmentation%2520model%2520to%2520update%2520a%2520recently-introduced%2520normative%250Adatabase%2520of%2520healthy%2520participants%2520containing%2520commonly%2520used%2520measures%2520of%2520spinal%250Acord%2520morphometry.%2520Results%2520showed%2520that%253A%2520%2528i%2529%2520our%2520model%2520outperforms%2520previous%250Aversions%2520and%2520pathology-specific%2520models%2520on%2520challenging%2520lumbar%2520spinal%2520cord%2520cases%252C%250Aachieving%2520an%2520average%2520Dice%2520score%2520of%2520%25240.95%2520%255Cpm%25200.03%2524%253B%2520%2528ii%2529%2520the%2520automatic%2520workflow%250Afor%2520monitoring%2520morphometric%2520drift%2520provides%2520a%2520quick%2520feedback%2520loop%2520for%2520developing%250Afuture%2520segmentation%2520models%253B%2520and%2520%2528iii%2529%2520the%2520scaling%2520factor%2520required%2520to%2520update%2520the%250Adatabase%2520of%2520morphometric%2520measures%2520is%2520nearly%2520constant%2520among%2520slices%2520across%2520the%250Agiven%2520vertebral%2520levels%252C%2520showing%2520minimum%2520drift%2520between%2520the%2520current%2520and%2520previous%250Aversions%2520of%2520the%2520model%2520monitored%2520by%2520the%2520framework.%2520The%2520model%2520is%2520freely%2520available%250Ain%2520Spinal%2520Cord%2520Toolbox%2520v7.0.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monitoring%20morphometric%20drift%20in%20lifelong%20learning%20segmentation%20of%20the%0A%20%20spinal%20cord&entry.906535625=Enamundram%20Naga%20Karthik%20and%20Sandrine%20B%C3%A9dard%20and%20Jan%20Valo%C5%A1ek%20and%20Christoph%20S.%20Aigner%20and%20Elise%20Bannier%20and%20Josef%20Bedna%C5%99%C3%ADk%20and%20Virginie%20Callot%20and%20Anna%20Combes%20and%20Armin%20Curt%20and%20Gergely%20David%20and%20Falk%20Eippert%20and%20Lynn%20Farner%20and%20Michael%20G%20Fehlings%20and%20Patrick%20Freund%20and%20Tobias%20Granberg%20and%20Cristina%20Granziera%20and%20RHSCIR%20Network%20Imaging%20Group%20and%20Ulrike%20Horn%20and%20Tom%C3%A1%C5%A1%20Hor%C3%A1k%20and%20Suzanne%20Humphreys%20and%20Markus%20Hupp%20and%20Anne%20Kerbrat%20and%20Nawal%20Kinany%20and%20Shannon%20Kolind%20and%20Petr%20Kudli%C4%8Dka%20and%20Anna%20Lebret%20and%20Lisa%20Eunyoung%20Lee%20and%20Caterina%20Mainero%20and%20Allan%20R.%20Martin%20and%20Megan%20McGrath%20and%20Govind%20Nair%20and%20Kristin%20P.%20O%27Grady%20and%20Jiwon%20Oh%20and%20Russell%20Ouellette%20and%20Nikolai%20Pfender%20and%20Dario%20Pfyffer%20and%20Pierre-Fran%C3%A7ois%20Pradat%20and%20Alexandre%20Prat%20and%20Emanuele%20Pravat%C3%A0%20and%20Daniel%20S.%20Reich%20and%20Ilaria%20Ricchi%20and%20Naama%20Rotem-Kohavi%20and%20Simon%20Schading-Sassenhausen%20and%20Maryam%20Seif%20and%20Andrew%20Smith%20and%20Seth%20A%20Smith%20and%20Grace%20Sweeney%20and%20Roger%20Tam%20and%20Anthony%20Traboulsee%20and%20Constantina%20Andrada%20Treaba%20and%20Charidimos%20Tsagkas%20and%20Zachary%20Vavasour%20and%20Dimitri%20Van%20De%20Ville%20and%20Kenneth%20Arnold%20Weber%20II%20and%20Sarath%20Chandar%20and%20Julien%20Cohen-Adad&entry.1292438233=%20%20Morphometric%20measures%20derived%20from%20spinal%20cord%20segmentations%20can%20serve%20as%0Adiagnostic%20and%20prognostic%20biomarkers%20in%20neurological%20diseases%20and%20injuries%0Aaffecting%20the%20spinal%20cord.%20While%20robust%2C%20automatic%20segmentation%20methods%20to%20a%0Awide%20variety%20of%20contrasts%20and%20pathologies%20have%20been%20developed%20over%20the%20past%20few%0Ayears%2C%20whether%20their%20predictions%20are%20stable%20as%20the%20model%20is%20updated%20using%20new%0Adatasets%20has%20not%20been%20assessed.%20This%20is%20particularly%20important%20for%20deriving%0Anormative%20values%20from%20healthy%20participants.%20In%20this%20study%2C%20we%20present%20a%20spinal%0Acord%20segmentation%20model%20trained%20on%20a%20multisite%20%24%28n%3D75%29%24%20dataset%2C%20including%209%0Adifferent%20MRI%20contrasts%20and%20several%20spinal%20cord%20pathologies.%20We%20also%20introduce%0Aa%20lifelong%20learning%20framework%20to%20automatically%20monitor%20the%20morphometric%20drift%0Aas%20the%20model%20is%20updated%20using%20additional%20datasets.%20The%20framework%20is%20triggered%0Aby%20an%20automatic%20GitHub%20Actions%20workflow%20every%20time%20a%20new%20model%20is%20created%2C%0Arecording%20the%20morphometric%20values%20derived%20from%20the%20model%27s%20predictions%20over%0Atime.%20As%20a%20real-world%20application%20of%20the%20proposed%20framework%2C%20we%20employed%20the%0Aspinal%20cord%20segmentation%20model%20to%20update%20a%20recently-introduced%20normative%0Adatabase%20of%20healthy%20participants%20containing%20commonly%20used%20measures%20of%20spinal%0Acord%20morphometry.%20Results%20showed%20that%3A%20%28i%29%20our%20model%20outperforms%20previous%0Aversions%20and%20pathology-specific%20models%20on%20challenging%20lumbar%20spinal%20cord%20cases%2C%0Aachieving%20an%20average%20Dice%20score%20of%20%240.95%20%5Cpm%200.03%24%3B%20%28ii%29%20the%20automatic%20workflow%0Afor%20monitoring%20morphometric%20drift%20provides%20a%20quick%20feedback%20loop%20for%20developing%0Afuture%20segmentation%20models%3B%20and%20%28iii%29%20the%20scaling%20factor%20required%20to%20update%20the%0Adatabase%20of%20morphometric%20measures%20is%20nearly%20constant%20among%20slices%20across%20the%0Agiven%20vertebral%20levels%2C%20showing%20minimum%20drift%20between%20the%20current%20and%20previous%0Aversions%20of%20the%20model%20monitored%20by%20the%20framework.%20The%20model%20is%20freely%20available%0Ain%20Spinal%20Cord%20Toolbox%20v7.0.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01364v1&entry.124074799=Read"},
{"title": "Towards One Model for Classical Dimensionality Reduction: A\n  Probabilistic Perspective on UMAP and t-SNE", "author": "Aditya Ravuri and Neil D. Lawrence", "abstract": "  This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in Ravuri et al. (2023), that describes the graph Laplacian\n(an estimate of the data precision matrix) using a Wishart distribution, with a\nmean given by a non-linear covariance function evaluated on the latents. This\ninterpretation offers deeper theoretical and semantic insights into such\nalgorithms, and forging a connection to Gaussian process latent variable models\nby showing that well-known kernels can be used to describe covariances implied\nby graph Laplacians. We also introduce tools with which similar dimensionality\nreduction methods can be studied.\n", "link": "http://arxiv.org/abs/2405.17412v4", "date": "2025-05-02", "relevancy": 1.9779, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5216}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4931}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20One%20Model%20for%20Classical%20Dimensionality%20Reduction%3A%20A%0A%20%20Probabilistic%20Perspective%20on%20UMAP%20and%20t-SNE&body=Title%3A%20Towards%20One%20Model%20for%20Classical%20Dimensionality%20Reduction%3A%20A%0A%20%20Probabilistic%20Perspective%20on%20UMAP%20and%20t-SNE%0AAuthor%3A%20Aditya%20Ravuri%20and%20Neil%20D.%20Lawrence%0AAbstract%3A%20%20%20This%20paper%20shows%20that%20dimensionality%20reduction%20methods%20such%20as%20UMAP%20and%0At-SNE%2C%20can%20be%20approximately%20recast%20as%20MAP%20inference%20methods%20corresponding%20to%20a%0Amodel%20introduced%20in%20Ravuri%20et%20al.%20%282023%29%2C%20that%20describes%20the%20graph%20Laplacian%0A%28an%20estimate%20of%20the%20data%20precision%20matrix%29%20using%20a%20Wishart%20distribution%2C%20with%20a%0Amean%20given%20by%20a%20non-linear%20covariance%20function%20evaluated%20on%20the%20latents.%20This%0Ainterpretation%20offers%20deeper%20theoretical%20and%20semantic%20insights%20into%20such%0Aalgorithms%2C%20and%20forging%20a%20connection%20to%20Gaussian%20process%20latent%20variable%20models%0Aby%20showing%20that%20well-known%20kernels%20can%20be%20used%20to%20describe%20covariances%20implied%0Aby%20graph%20Laplacians.%20We%20also%20introduce%20tools%20with%20which%20similar%20dimensionality%0Areduction%20methods%20can%20be%20studied.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17412v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520One%2520Model%2520for%2520Classical%2520Dimensionality%2520Reduction%253A%2520A%250A%2520%2520Probabilistic%2520Perspective%2520on%2520UMAP%2520and%2520t-SNE%26entry.906535625%3DAditya%2520Ravuri%2520and%2520Neil%2520D.%2520Lawrence%26entry.1292438233%3D%2520%2520This%2520paper%2520shows%2520that%2520dimensionality%2520reduction%2520methods%2520such%2520as%2520UMAP%2520and%250At-SNE%252C%2520can%2520be%2520approximately%2520recast%2520as%2520MAP%2520inference%2520methods%2520corresponding%2520to%2520a%250Amodel%2520introduced%2520in%2520Ravuri%2520et%2520al.%2520%25282023%2529%252C%2520that%2520describes%2520the%2520graph%2520Laplacian%250A%2528an%2520estimate%2520of%2520the%2520data%2520precision%2520matrix%2529%2520using%2520a%2520Wishart%2520distribution%252C%2520with%2520a%250Amean%2520given%2520by%2520a%2520non-linear%2520covariance%2520function%2520evaluated%2520on%2520the%2520latents.%2520This%250Ainterpretation%2520offers%2520deeper%2520theoretical%2520and%2520semantic%2520insights%2520into%2520such%250Aalgorithms%252C%2520and%2520forging%2520a%2520connection%2520to%2520Gaussian%2520process%2520latent%2520variable%2520models%250Aby%2520showing%2520that%2520well-known%2520kernels%2520can%2520be%2520used%2520to%2520describe%2520covariances%2520implied%250Aby%2520graph%2520Laplacians.%2520We%2520also%2520introduce%2520tools%2520with%2520which%2520similar%2520dimensionality%250Areduction%2520methods%2520can%2520be%2520studied.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17412v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20One%20Model%20for%20Classical%20Dimensionality%20Reduction%3A%20A%0A%20%20Probabilistic%20Perspective%20on%20UMAP%20and%20t-SNE&entry.906535625=Aditya%20Ravuri%20and%20Neil%20D.%20Lawrence&entry.1292438233=%20%20This%20paper%20shows%20that%20dimensionality%20reduction%20methods%20such%20as%20UMAP%20and%0At-SNE%2C%20can%20be%20approximately%20recast%20as%20MAP%20inference%20methods%20corresponding%20to%20a%0Amodel%20introduced%20in%20Ravuri%20et%20al.%20%282023%29%2C%20that%20describes%20the%20graph%20Laplacian%0A%28an%20estimate%20of%20the%20data%20precision%20matrix%29%20using%20a%20Wishart%20distribution%2C%20with%20a%0Amean%20given%20by%20a%20non-linear%20covariance%20function%20evaluated%20on%20the%20latents.%20This%0Ainterpretation%20offers%20deeper%20theoretical%20and%20semantic%20insights%20into%20such%0Aalgorithms%2C%20and%20forging%20a%20connection%20to%20Gaussian%20process%20latent%20variable%20models%0Aby%20showing%20that%20well-known%20kernels%20can%20be%20used%20to%20describe%20covariances%20implied%0Aby%20graph%20Laplacians.%20We%20also%20introduce%20tools%20with%20which%20similar%20dimensionality%0Areduction%20methods%20can%20be%20studied.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17412v4&entry.124074799=Read"},
{"title": "TActiLE: Tiny Active LEarning for wearable devices", "author": "Massimo Pavan and Claudio Galimberti and Manuel Roveri", "abstract": "  Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent\nyears, enabling wearable devices to be not only connected but also genuinely\nintelligent by running machine learning (ML) computations directly on-device.\nAmong such devices, smart glasses have particularly benefited from TinyML\nadvancements. TinyML facilitates the on-device execution of the inference phase\nof ML algorithms on embedded and wearable devices, and more recently, it has\nexpanded into On-device Learning (ODL), which allows both inference and\nlearning phases to occur directly on the device. The application of ODL\ntechniques to wearable devices is particularly compelling, as it enables the\ndevelopment of more personalized models that adapt based on the data of the\nuser. However, one of the major challenges of ODL algorithms is the scarcity of\nlabeled data collected on-device. In smart wearable contexts, requiring users\nto manually label large amounts of data is often impractical and could lead to\nuser disengagement with the technology. To address this issue, this paper\nexplores the application of Active Learning (AL) techniques, i.e., techniques\nthat aim at minimizing the labeling effort, by actively selecting from a large\nquantity of unlabeled data only a small subset to be labeled and added to the\ntraining set of the algorithm. In particular, we propose TActiLE, a novel AL\nalgorithm that selects from the stream of on-device sensor data the ones that\nwould help the ML algorithm improve the most once coupled with labels provided\nby the user. TActiLE is the first Active Learning technique specifically\ndesigned for the TinyML context. We evaluate its effectiveness and efficiency\nthrough experiments on multiple image classification datasets. The results\ndemonstrate its suitability for tiny and wearable devices.\n", "link": "http://arxiv.org/abs/2505.01160v1", "date": "2025-05-02", "relevancy": 1.9711, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4991}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4967}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TActiLE%3A%20Tiny%20Active%20LEarning%20for%20wearable%20devices&body=Title%3A%20TActiLE%3A%20Tiny%20Active%20LEarning%20for%20wearable%20devices%0AAuthor%3A%20Massimo%20Pavan%20and%20Claudio%20Galimberti%20and%20Manuel%20Roveri%0AAbstract%3A%20%20%20Tiny%20Machine%20Learning%20%28TinyML%29%20algorithms%20have%20seen%20extensive%20use%20in%20recent%0Ayears%2C%20enabling%20wearable%20devices%20to%20be%20not%20only%20connected%20but%20also%20genuinely%0Aintelligent%20by%20running%20machine%20learning%20%28ML%29%20computations%20directly%20on-device.%0AAmong%20such%20devices%2C%20smart%20glasses%20have%20particularly%20benefited%20from%20TinyML%0Aadvancements.%20TinyML%20facilitates%20the%20on-device%20execution%20of%20the%20inference%20phase%0Aof%20ML%20algorithms%20on%20embedded%20and%20wearable%20devices%2C%20and%20more%20recently%2C%20it%20has%0Aexpanded%20into%20On-device%20Learning%20%28ODL%29%2C%20which%20allows%20both%20inference%20and%0Alearning%20phases%20to%20occur%20directly%20on%20the%20device.%20The%20application%20of%20ODL%0Atechniques%20to%20wearable%20devices%20is%20particularly%20compelling%2C%20as%20it%20enables%20the%0Adevelopment%20of%20more%20personalized%20models%20that%20adapt%20based%20on%20the%20data%20of%20the%0Auser.%20However%2C%20one%20of%20the%20major%20challenges%20of%20ODL%20algorithms%20is%20the%20scarcity%20of%0Alabeled%20data%20collected%20on-device.%20In%20smart%20wearable%20contexts%2C%20requiring%20users%0Ato%20manually%20label%20large%20amounts%20of%20data%20is%20often%20impractical%20and%20could%20lead%20to%0Auser%20disengagement%20with%20the%20technology.%20To%20address%20this%20issue%2C%20this%20paper%0Aexplores%20the%20application%20of%20Active%20Learning%20%28AL%29%20techniques%2C%20i.e.%2C%20techniques%0Athat%20aim%20at%20minimizing%20the%20labeling%20effort%2C%20by%20actively%20selecting%20from%20a%20large%0Aquantity%20of%20unlabeled%20data%20only%20a%20small%20subset%20to%20be%20labeled%20and%20added%20to%20the%0Atraining%20set%20of%20the%20algorithm.%20In%20particular%2C%20we%20propose%20TActiLE%2C%20a%20novel%20AL%0Aalgorithm%20that%20selects%20from%20the%20stream%20of%20on-device%20sensor%20data%20the%20ones%20that%0Awould%20help%20the%20ML%20algorithm%20improve%20the%20most%20once%20coupled%20with%20labels%20provided%0Aby%20the%20user.%20TActiLE%20is%20the%20first%20Active%20Learning%20technique%20specifically%0Adesigned%20for%20the%20TinyML%20context.%20We%20evaluate%20its%20effectiveness%20and%20efficiency%0Athrough%20experiments%20on%20multiple%20image%20classification%20datasets.%20The%20results%0Ademonstrate%20its%20suitability%20for%20tiny%20and%20wearable%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTActiLE%253A%2520Tiny%2520Active%2520LEarning%2520for%2520wearable%2520devices%26entry.906535625%3DMassimo%2520Pavan%2520and%2520Claudio%2520Galimberti%2520and%2520Manuel%2520Roveri%26entry.1292438233%3D%2520%2520Tiny%2520Machine%2520Learning%2520%2528TinyML%2529%2520algorithms%2520have%2520seen%2520extensive%2520use%2520in%2520recent%250Ayears%252C%2520enabling%2520wearable%2520devices%2520to%2520be%2520not%2520only%2520connected%2520but%2520also%2520genuinely%250Aintelligent%2520by%2520running%2520machine%2520learning%2520%2528ML%2529%2520computations%2520directly%2520on-device.%250AAmong%2520such%2520devices%252C%2520smart%2520glasses%2520have%2520particularly%2520benefited%2520from%2520TinyML%250Aadvancements.%2520TinyML%2520facilitates%2520the%2520on-device%2520execution%2520of%2520the%2520inference%2520phase%250Aof%2520ML%2520algorithms%2520on%2520embedded%2520and%2520wearable%2520devices%252C%2520and%2520more%2520recently%252C%2520it%2520has%250Aexpanded%2520into%2520On-device%2520Learning%2520%2528ODL%2529%252C%2520which%2520allows%2520both%2520inference%2520and%250Alearning%2520phases%2520to%2520occur%2520directly%2520on%2520the%2520device.%2520The%2520application%2520of%2520ODL%250Atechniques%2520to%2520wearable%2520devices%2520is%2520particularly%2520compelling%252C%2520as%2520it%2520enables%2520the%250Adevelopment%2520of%2520more%2520personalized%2520models%2520that%2520adapt%2520based%2520on%2520the%2520data%2520of%2520the%250Auser.%2520However%252C%2520one%2520of%2520the%2520major%2520challenges%2520of%2520ODL%2520algorithms%2520is%2520the%2520scarcity%2520of%250Alabeled%2520data%2520collected%2520on-device.%2520In%2520smart%2520wearable%2520contexts%252C%2520requiring%2520users%250Ato%2520manually%2520label%2520large%2520amounts%2520of%2520data%2520is%2520often%2520impractical%2520and%2520could%2520lead%2520to%250Auser%2520disengagement%2520with%2520the%2520technology.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%250Aexplores%2520the%2520application%2520of%2520Active%2520Learning%2520%2528AL%2529%2520techniques%252C%2520i.e.%252C%2520techniques%250Athat%2520aim%2520at%2520minimizing%2520the%2520labeling%2520effort%252C%2520by%2520actively%2520selecting%2520from%2520a%2520large%250Aquantity%2520of%2520unlabeled%2520data%2520only%2520a%2520small%2520subset%2520to%2520be%2520labeled%2520and%2520added%2520to%2520the%250Atraining%2520set%2520of%2520the%2520algorithm.%2520In%2520particular%252C%2520we%2520propose%2520TActiLE%252C%2520a%2520novel%2520AL%250Aalgorithm%2520that%2520selects%2520from%2520the%2520stream%2520of%2520on-device%2520sensor%2520data%2520the%2520ones%2520that%250Awould%2520help%2520the%2520ML%2520algorithm%2520improve%2520the%2520most%2520once%2520coupled%2520with%2520labels%2520provided%250Aby%2520the%2520user.%2520TActiLE%2520is%2520the%2520first%2520Active%2520Learning%2520technique%2520specifically%250Adesigned%2520for%2520the%2520TinyML%2520context.%2520We%2520evaluate%2520its%2520effectiveness%2520and%2520efficiency%250Athrough%2520experiments%2520on%2520multiple%2520image%2520classification%2520datasets.%2520The%2520results%250Ademonstrate%2520its%2520suitability%2520for%2520tiny%2520and%2520wearable%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TActiLE%3A%20Tiny%20Active%20LEarning%20for%20wearable%20devices&entry.906535625=Massimo%20Pavan%20and%20Claudio%20Galimberti%20and%20Manuel%20Roveri&entry.1292438233=%20%20Tiny%20Machine%20Learning%20%28TinyML%29%20algorithms%20have%20seen%20extensive%20use%20in%20recent%0Ayears%2C%20enabling%20wearable%20devices%20to%20be%20not%20only%20connected%20but%20also%20genuinely%0Aintelligent%20by%20running%20machine%20learning%20%28ML%29%20computations%20directly%20on-device.%0AAmong%20such%20devices%2C%20smart%20glasses%20have%20particularly%20benefited%20from%20TinyML%0Aadvancements.%20TinyML%20facilitates%20the%20on-device%20execution%20of%20the%20inference%20phase%0Aof%20ML%20algorithms%20on%20embedded%20and%20wearable%20devices%2C%20and%20more%20recently%2C%20it%20has%0Aexpanded%20into%20On-device%20Learning%20%28ODL%29%2C%20which%20allows%20both%20inference%20and%0Alearning%20phases%20to%20occur%20directly%20on%20the%20device.%20The%20application%20of%20ODL%0Atechniques%20to%20wearable%20devices%20is%20particularly%20compelling%2C%20as%20it%20enables%20the%0Adevelopment%20of%20more%20personalized%20models%20that%20adapt%20based%20on%20the%20data%20of%20the%0Auser.%20However%2C%20one%20of%20the%20major%20challenges%20of%20ODL%20algorithms%20is%20the%20scarcity%20of%0Alabeled%20data%20collected%20on-device.%20In%20smart%20wearable%20contexts%2C%20requiring%20users%0Ato%20manually%20label%20large%20amounts%20of%20data%20is%20often%20impractical%20and%20could%20lead%20to%0Auser%20disengagement%20with%20the%20technology.%20To%20address%20this%20issue%2C%20this%20paper%0Aexplores%20the%20application%20of%20Active%20Learning%20%28AL%29%20techniques%2C%20i.e.%2C%20techniques%0Athat%20aim%20at%20minimizing%20the%20labeling%20effort%2C%20by%20actively%20selecting%20from%20a%20large%0Aquantity%20of%20unlabeled%20data%20only%20a%20small%20subset%20to%20be%20labeled%20and%20added%20to%20the%0Atraining%20set%20of%20the%20algorithm.%20In%20particular%2C%20we%20propose%20TActiLE%2C%20a%20novel%20AL%0Aalgorithm%20that%20selects%20from%20the%20stream%20of%20on-device%20sensor%20data%20the%20ones%20that%0Awould%20help%20the%20ML%20algorithm%20improve%20the%20most%20once%20coupled%20with%20labels%20provided%0Aby%20the%20user.%20TActiLE%20is%20the%20first%20Active%20Learning%20technique%20specifically%0Adesigned%20for%20the%20TinyML%20context.%20We%20evaluate%20its%20effectiveness%20and%20efficiency%0Athrough%20experiments%20on%20multiple%20image%20classification%20datasets.%20The%20results%0Ademonstrate%20its%20suitability%20for%20tiny%20and%20wearable%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01160v1&entry.124074799=Read"},
{"title": "CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for\n  Open-Ended Diagnostic Reasoning", "author": "Tsai-Ning Wang and Lin-Lin Chen and Neil Zeghidour and Aaqib Saeed", "abstract": "  Medical audio signals, such as heart and lung sounds, play a crucial role in\nclinical diagnosis. However, analyzing these signals remains challenging:\ntraditional methods rely on handcrafted features or supervised deep learning\nmodels that demand extensive labeled datasets, limiting their scalability and\napplicability. To address these issues, we propose CaReAQA, an audio-language\nmodel that integrates a foundation audio model with the reasoning capabilities\nof large language models, enabling clinically relevant, open-ended diagnostic\nresponses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of\nannotated medical audio recordings enriched with metadata and paired\nquestion-answer examples, intended to drive progress in diagnostic reasoning\nresearch. Evaluation results show that CaReAQA achieves 86.2% accuracy on\nopen-ended diagnostic reasoning tasks, outperforming baseline models. It also\ngeneralizes well to closed-ended classification tasks, achieving an average\naccuracy of 56.9% on unseen datasets. Our findings show how audio-language\nintegration and reasoning advances medical diagnostics, enabling efficient AI\nsystems for clinical decision support.\n", "link": "http://arxiv.org/abs/2505.01199v1", "date": "2025-05-02", "relevancy": 1.9664, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaReAQA%3A%20A%20Cardiac%20and%20Respiratory%20Audio%20Question%20Answering%20Model%20for%0A%20%20Open-Ended%20Diagnostic%20Reasoning&body=Title%3A%20CaReAQA%3A%20A%20Cardiac%20and%20Respiratory%20Audio%20Question%20Answering%20Model%20for%0A%20%20Open-Ended%20Diagnostic%20Reasoning%0AAuthor%3A%20Tsai-Ning%20Wang%20and%20Lin-Lin%20Chen%20and%20Neil%20Zeghidour%20and%20Aaqib%20Saeed%0AAbstract%3A%20%20%20Medical%20audio%20signals%2C%20such%20as%20heart%20and%20lung%20sounds%2C%20play%20a%20crucial%20role%20in%0Aclinical%20diagnosis.%20However%2C%20analyzing%20these%20signals%20remains%20challenging%3A%0Atraditional%20methods%20rely%20on%20handcrafted%20features%20or%20supervised%20deep%20learning%0Amodels%20that%20demand%20extensive%20labeled%20datasets%2C%20limiting%20their%20scalability%20and%0Aapplicability.%20To%20address%20these%20issues%2C%20we%20propose%20CaReAQA%2C%20an%20audio-language%0Amodel%20that%20integrates%20a%20foundation%20audio%20model%20with%20the%20reasoning%20capabilities%0Aof%20large%20language%20models%2C%20enabling%20clinically%20relevant%2C%20open-ended%20diagnostic%0Aresponses.%20Alongside%20CaReAQA%2C%20we%20introduce%20CaReSound%2C%20a%20benchmark%20dataset%20of%0Aannotated%20medical%20audio%20recordings%20enriched%20with%20metadata%20and%20paired%0Aquestion-answer%20examples%2C%20intended%20to%20drive%20progress%20in%20diagnostic%20reasoning%0Aresearch.%20Evaluation%20results%20show%20that%20CaReAQA%20achieves%2086.2%25%20accuracy%20on%0Aopen-ended%20diagnostic%20reasoning%20tasks%2C%20outperforming%20baseline%20models.%20It%20also%0Ageneralizes%20well%20to%20closed-ended%20classification%20tasks%2C%20achieving%20an%20average%0Aaccuracy%20of%2056.9%25%20on%20unseen%20datasets.%20Our%20findings%20show%20how%20audio-language%0Aintegration%20and%20reasoning%20advances%20medical%20diagnostics%2C%20enabling%20efficient%20AI%0Asystems%20for%20clinical%20decision%20support.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaReAQA%253A%2520A%2520Cardiac%2520and%2520Respiratory%2520Audio%2520Question%2520Answering%2520Model%2520for%250A%2520%2520Open-Ended%2520Diagnostic%2520Reasoning%26entry.906535625%3DTsai-Ning%2520Wang%2520and%2520Lin-Lin%2520Chen%2520and%2520Neil%2520Zeghidour%2520and%2520Aaqib%2520Saeed%26entry.1292438233%3D%2520%2520Medical%2520audio%2520signals%252C%2520such%2520as%2520heart%2520and%2520lung%2520sounds%252C%2520play%2520a%2520crucial%2520role%2520in%250Aclinical%2520diagnosis.%2520However%252C%2520analyzing%2520these%2520signals%2520remains%2520challenging%253A%250Atraditional%2520methods%2520rely%2520on%2520handcrafted%2520features%2520or%2520supervised%2520deep%2520learning%250Amodels%2520that%2520demand%2520extensive%2520labeled%2520datasets%252C%2520limiting%2520their%2520scalability%2520and%250Aapplicability.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520CaReAQA%252C%2520an%2520audio-language%250Amodel%2520that%2520integrates%2520a%2520foundation%2520audio%2520model%2520with%2520the%2520reasoning%2520capabilities%250Aof%2520large%2520language%2520models%252C%2520enabling%2520clinically%2520relevant%252C%2520open-ended%2520diagnostic%250Aresponses.%2520Alongside%2520CaReAQA%252C%2520we%2520introduce%2520CaReSound%252C%2520a%2520benchmark%2520dataset%2520of%250Aannotated%2520medical%2520audio%2520recordings%2520enriched%2520with%2520metadata%2520and%2520paired%250Aquestion-answer%2520examples%252C%2520intended%2520to%2520drive%2520progress%2520in%2520diagnostic%2520reasoning%250Aresearch.%2520Evaluation%2520results%2520show%2520that%2520CaReAQA%2520achieves%252086.2%2525%2520accuracy%2520on%250Aopen-ended%2520diagnostic%2520reasoning%2520tasks%252C%2520outperforming%2520baseline%2520models.%2520It%2520also%250Ageneralizes%2520well%2520to%2520closed-ended%2520classification%2520tasks%252C%2520achieving%2520an%2520average%250Aaccuracy%2520of%252056.9%2525%2520on%2520unseen%2520datasets.%2520Our%2520findings%2520show%2520how%2520audio-language%250Aintegration%2520and%2520reasoning%2520advances%2520medical%2520diagnostics%252C%2520enabling%2520efficient%2520AI%250Asystems%2520for%2520clinical%2520decision%2520support.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaReAQA%3A%20A%20Cardiac%20and%20Respiratory%20Audio%20Question%20Answering%20Model%20for%0A%20%20Open-Ended%20Diagnostic%20Reasoning&entry.906535625=Tsai-Ning%20Wang%20and%20Lin-Lin%20Chen%20and%20Neil%20Zeghidour%20and%20Aaqib%20Saeed&entry.1292438233=%20%20Medical%20audio%20signals%2C%20such%20as%20heart%20and%20lung%20sounds%2C%20play%20a%20crucial%20role%20in%0Aclinical%20diagnosis.%20However%2C%20analyzing%20these%20signals%20remains%20challenging%3A%0Atraditional%20methods%20rely%20on%20handcrafted%20features%20or%20supervised%20deep%20learning%0Amodels%20that%20demand%20extensive%20labeled%20datasets%2C%20limiting%20their%20scalability%20and%0Aapplicability.%20To%20address%20these%20issues%2C%20we%20propose%20CaReAQA%2C%20an%20audio-language%0Amodel%20that%20integrates%20a%20foundation%20audio%20model%20with%20the%20reasoning%20capabilities%0Aof%20large%20language%20models%2C%20enabling%20clinically%20relevant%2C%20open-ended%20diagnostic%0Aresponses.%20Alongside%20CaReAQA%2C%20we%20introduce%20CaReSound%2C%20a%20benchmark%20dataset%20of%0Aannotated%20medical%20audio%20recordings%20enriched%20with%20metadata%20and%20paired%0Aquestion-answer%20examples%2C%20intended%20to%20drive%20progress%20in%20diagnostic%20reasoning%0Aresearch.%20Evaluation%20results%20show%20that%20CaReAQA%20achieves%2086.2%25%20accuracy%20on%0Aopen-ended%20diagnostic%20reasoning%20tasks%2C%20outperforming%20baseline%20models.%20It%20also%0Ageneralizes%20well%20to%20closed-ended%20classification%20tasks%2C%20achieving%20an%20average%0Aaccuracy%20of%2056.9%25%20on%20unseen%20datasets.%20Our%20findings%20show%20how%20audio-language%0Aintegration%20and%20reasoning%20advances%20medical%20diagnostics%2C%20enabling%20efficient%20AI%0Asystems%20for%20clinical%20decision%20support.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01199v1&entry.124074799=Read"},
{"title": "YARE-GAN: Yet Another Resting State EEG-GAN", "author": "Yeganeh Farahzadi and Morteza Ansarinia and Zoltan Kekecs", "abstract": "  In this study, we implement a Wasserstein GAN with Gradient Penalty (WGAN-GP)\nto generate multi-channel resting-state EEG data and assess the quality of the\nsynthesized signals through both visual and feature-based evaluations. Our\nresults indicate that the model effectively captures the statistical and\nspectral characteristics of real EEG data, although challenges remain in\nreplicating high-frequency oscillations in the frontal region. Additionally, we\ndemonstrate that the Critic's learned representations can be reused for gender\nclassification task, achieving an out-of-sample accuracy, significantly better\nthan a shuffled-label baseline and a model trained directly on EEG data. These\nfindings suggest that generative models can serve not only as EEG data\ngenerators but also as unsupervised feature extractors, reducing the need for\nmanual feature engineering. This study highlights the potential of GAN-based\nunsupervised learning for EEG analysis, suggesting avenues for more\ndata-efficient deep learning applications in neuroscience.\n", "link": "http://arxiv.org/abs/2503.02636v2", "date": "2025-05-02", "relevancy": 1.9658, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.508}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4833}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YARE-GAN%3A%20Yet%20Another%20Resting%20State%20EEG-GAN&body=Title%3A%20YARE-GAN%3A%20Yet%20Another%20Resting%20State%20EEG-GAN%0AAuthor%3A%20Yeganeh%20Farahzadi%20and%20Morteza%20Ansarinia%20and%20Zoltan%20Kekecs%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20implement%20a%20Wasserstein%20GAN%20with%20Gradient%20Penalty%20%28WGAN-GP%29%0Ato%20generate%20multi-channel%20resting-state%20EEG%20data%20and%20assess%20the%20quality%20of%20the%0Asynthesized%20signals%20through%20both%20visual%20and%20feature-based%20evaluations.%20Our%0Aresults%20indicate%20that%20the%20model%20effectively%20captures%20the%20statistical%20and%0Aspectral%20characteristics%20of%20real%20EEG%20data%2C%20although%20challenges%20remain%20in%0Areplicating%20high-frequency%20oscillations%20in%20the%20frontal%20region.%20Additionally%2C%20we%0Ademonstrate%20that%20the%20Critic%27s%20learned%20representations%20can%20be%20reused%20for%20gender%0Aclassification%20task%2C%20achieving%20an%20out-of-sample%20accuracy%2C%20significantly%20better%0Athan%20a%20shuffled-label%20baseline%20and%20a%20model%20trained%20directly%20on%20EEG%20data.%20These%0Afindings%20suggest%20that%20generative%20models%20can%20serve%20not%20only%20as%20EEG%20data%0Agenerators%20but%20also%20as%20unsupervised%20feature%20extractors%2C%20reducing%20the%20need%20for%0Amanual%20feature%20engineering.%20This%20study%20highlights%20the%20potential%20of%20GAN-based%0Aunsupervised%20learning%20for%20EEG%20analysis%2C%20suggesting%20avenues%20for%20more%0Adata-efficient%20deep%20learning%20applications%20in%20neuroscience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02636v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYARE-GAN%253A%2520Yet%2520Another%2520Resting%2520State%2520EEG-GAN%26entry.906535625%3DYeganeh%2520Farahzadi%2520and%2520Morteza%2520Ansarinia%2520and%2520Zoltan%2520Kekecs%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520implement%2520a%2520Wasserstein%2520GAN%2520with%2520Gradient%2520Penalty%2520%2528WGAN-GP%2529%250Ato%2520generate%2520multi-channel%2520resting-state%2520EEG%2520data%2520and%2520assess%2520the%2520quality%2520of%2520the%250Asynthesized%2520signals%2520through%2520both%2520visual%2520and%2520feature-based%2520evaluations.%2520Our%250Aresults%2520indicate%2520that%2520the%2520model%2520effectively%2520captures%2520the%2520statistical%2520and%250Aspectral%2520characteristics%2520of%2520real%2520EEG%2520data%252C%2520although%2520challenges%2520remain%2520in%250Areplicating%2520high-frequency%2520oscillations%2520in%2520the%2520frontal%2520region.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520the%2520Critic%2527s%2520learned%2520representations%2520can%2520be%2520reused%2520for%2520gender%250Aclassification%2520task%252C%2520achieving%2520an%2520out-of-sample%2520accuracy%252C%2520significantly%2520better%250Athan%2520a%2520shuffled-label%2520baseline%2520and%2520a%2520model%2520trained%2520directly%2520on%2520EEG%2520data.%2520These%250Afindings%2520suggest%2520that%2520generative%2520models%2520can%2520serve%2520not%2520only%2520as%2520EEG%2520data%250Agenerators%2520but%2520also%2520as%2520unsupervised%2520feature%2520extractors%252C%2520reducing%2520the%2520need%2520for%250Amanual%2520feature%2520engineering.%2520This%2520study%2520highlights%2520the%2520potential%2520of%2520GAN-based%250Aunsupervised%2520learning%2520for%2520EEG%2520analysis%252C%2520suggesting%2520avenues%2520for%2520more%250Adata-efficient%2520deep%2520learning%2520applications%2520in%2520neuroscience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02636v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YARE-GAN%3A%20Yet%20Another%20Resting%20State%20EEG-GAN&entry.906535625=Yeganeh%20Farahzadi%20and%20Morteza%20Ansarinia%20and%20Zoltan%20Kekecs&entry.1292438233=%20%20In%20this%20study%2C%20we%20implement%20a%20Wasserstein%20GAN%20with%20Gradient%20Penalty%20%28WGAN-GP%29%0Ato%20generate%20multi-channel%20resting-state%20EEG%20data%20and%20assess%20the%20quality%20of%20the%0Asynthesized%20signals%20through%20both%20visual%20and%20feature-based%20evaluations.%20Our%0Aresults%20indicate%20that%20the%20model%20effectively%20captures%20the%20statistical%20and%0Aspectral%20characteristics%20of%20real%20EEG%20data%2C%20although%20challenges%20remain%20in%0Areplicating%20high-frequency%20oscillations%20in%20the%20frontal%20region.%20Additionally%2C%20we%0Ademonstrate%20that%20the%20Critic%27s%20learned%20representations%20can%20be%20reused%20for%20gender%0Aclassification%20task%2C%20achieving%20an%20out-of-sample%20accuracy%2C%20significantly%20better%0Athan%20a%20shuffled-label%20baseline%20and%20a%20model%20trained%20directly%20on%20EEG%20data.%20These%0Afindings%20suggest%20that%20generative%20models%20can%20serve%20not%20only%20as%20EEG%20data%0Agenerators%20but%20also%20as%20unsupervised%20feature%20extractors%2C%20reducing%20the%20need%20for%0Amanual%20feature%20engineering.%20This%20study%20highlights%20the%20potential%20of%20GAN-based%0Aunsupervised%20learning%20for%20EEG%20analysis%2C%20suggesting%20avenues%20for%20more%0Adata-efficient%20deep%20learning%20applications%20in%20neuroscience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02636v2&entry.124074799=Read"},
{"title": "Evaluating Explanations: An Explanatory Virtues Framework for\n  Mechanistic Interpretability -- The Strange Science Part I.ii", "author": "Kola Ayonrinde and Louis Jaburi", "abstract": "  Mechanistic Interpretability (MI) aims to understand neural networks through\ncausal explanations. Though MI has many explanation-generating methods,\nprogress has been limited by the lack of a universal approach to evaluating\nexplanations. Here we analyse the fundamental question \"What makes a good\nexplanation?\" We introduce a pluralist Explanatory Virtues Framework drawing on\nfour perspectives from the Philosophy of Science - the Bayesian, Kuhnian,\nDeutschian, and Nomological - to systematically evaluate and improve\nexplanations in MI. We find that Compact Proofs consider many explanatory\nvirtues and are hence a promising approach. Fruitful research directions\nimplied by our framework include (1) clearly defining explanatory simplicity,\n(2) focusing on unifying explanations and (3) deriving universal principles for\nneural networks. Improved MI methods enhance our ability to monitor, predict,\nand steer AI systems.\n", "link": "http://arxiv.org/abs/2505.01372v1", "date": "2025-05-02", "relevancy": 1.9621, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Explanations%3A%20An%20Explanatory%20Virtues%20Framework%20for%0A%20%20Mechanistic%20Interpretability%20--%20The%20Strange%20Science%20Part%20I.ii&body=Title%3A%20Evaluating%20Explanations%3A%20An%20Explanatory%20Virtues%20Framework%20for%0A%20%20Mechanistic%20Interpretability%20--%20The%20Strange%20Science%20Part%20I.ii%0AAuthor%3A%20Kola%20Ayonrinde%20and%20Louis%20Jaburi%0AAbstract%3A%20%20%20Mechanistic%20Interpretability%20%28MI%29%20aims%20to%20understand%20neural%20networks%20through%0Acausal%20explanations.%20Though%20MI%20has%20many%20explanation-generating%20methods%2C%0Aprogress%20has%20been%20limited%20by%20the%20lack%20of%20a%20universal%20approach%20to%20evaluating%0Aexplanations.%20Here%20we%20analyse%20the%20fundamental%20question%20%22What%20makes%20a%20good%0Aexplanation%3F%22%20We%20introduce%20a%20pluralist%20Explanatory%20Virtues%20Framework%20drawing%20on%0Afour%20perspectives%20from%20the%20Philosophy%20of%20Science%20-%20the%20Bayesian%2C%20Kuhnian%2C%0ADeutschian%2C%20and%20Nomological%20-%20to%20systematically%20evaluate%20and%20improve%0Aexplanations%20in%20MI.%20We%20find%20that%20Compact%20Proofs%20consider%20many%20explanatory%0Avirtues%20and%20are%20hence%20a%20promising%20approach.%20Fruitful%20research%20directions%0Aimplied%20by%20our%20framework%20include%20%281%29%20clearly%20defining%20explanatory%20simplicity%2C%0A%282%29%20focusing%20on%20unifying%20explanations%20and%20%283%29%20deriving%20universal%20principles%20for%0Aneural%20networks.%20Improved%20MI%20methods%20enhance%20our%20ability%20to%20monitor%2C%20predict%2C%0Aand%20steer%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Explanations%253A%2520An%2520Explanatory%2520Virtues%2520Framework%2520for%250A%2520%2520Mechanistic%2520Interpretability%2520--%2520The%2520Strange%2520Science%2520Part%2520I.ii%26entry.906535625%3DKola%2520Ayonrinde%2520and%2520Louis%2520Jaburi%26entry.1292438233%3D%2520%2520Mechanistic%2520Interpretability%2520%2528MI%2529%2520aims%2520to%2520understand%2520neural%2520networks%2520through%250Acausal%2520explanations.%2520Though%2520MI%2520has%2520many%2520explanation-generating%2520methods%252C%250Aprogress%2520has%2520been%2520limited%2520by%2520the%2520lack%2520of%2520a%2520universal%2520approach%2520to%2520evaluating%250Aexplanations.%2520Here%2520we%2520analyse%2520the%2520fundamental%2520question%2520%2522What%2520makes%2520a%2520good%250Aexplanation%253F%2522%2520We%2520introduce%2520a%2520pluralist%2520Explanatory%2520Virtues%2520Framework%2520drawing%2520on%250Afour%2520perspectives%2520from%2520the%2520Philosophy%2520of%2520Science%2520-%2520the%2520Bayesian%252C%2520Kuhnian%252C%250ADeutschian%252C%2520and%2520Nomological%2520-%2520to%2520systematically%2520evaluate%2520and%2520improve%250Aexplanations%2520in%2520MI.%2520We%2520find%2520that%2520Compact%2520Proofs%2520consider%2520many%2520explanatory%250Avirtues%2520and%2520are%2520hence%2520a%2520promising%2520approach.%2520Fruitful%2520research%2520directions%250Aimplied%2520by%2520our%2520framework%2520include%2520%25281%2529%2520clearly%2520defining%2520explanatory%2520simplicity%252C%250A%25282%2529%2520focusing%2520on%2520unifying%2520explanations%2520and%2520%25283%2529%2520deriving%2520universal%2520principles%2520for%250Aneural%2520networks.%2520Improved%2520MI%2520methods%2520enhance%2520our%2520ability%2520to%2520monitor%252C%2520predict%252C%250Aand%2520steer%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Explanations%3A%20An%20Explanatory%20Virtues%20Framework%20for%0A%20%20Mechanistic%20Interpretability%20--%20The%20Strange%20Science%20Part%20I.ii&entry.906535625=Kola%20Ayonrinde%20and%20Louis%20Jaburi&entry.1292438233=%20%20Mechanistic%20Interpretability%20%28MI%29%20aims%20to%20understand%20neural%20networks%20through%0Acausal%20explanations.%20Though%20MI%20has%20many%20explanation-generating%20methods%2C%0Aprogress%20has%20been%20limited%20by%20the%20lack%20of%20a%20universal%20approach%20to%20evaluating%0Aexplanations.%20Here%20we%20analyse%20the%20fundamental%20question%20%22What%20makes%20a%20good%0Aexplanation%3F%22%20We%20introduce%20a%20pluralist%20Explanatory%20Virtues%20Framework%20drawing%20on%0Afour%20perspectives%20from%20the%20Philosophy%20of%20Science%20-%20the%20Bayesian%2C%20Kuhnian%2C%0ADeutschian%2C%20and%20Nomological%20-%20to%20systematically%20evaluate%20and%20improve%0Aexplanations%20in%20MI.%20We%20find%20that%20Compact%20Proofs%20consider%20many%20explanatory%0Avirtues%20and%20are%20hence%20a%20promising%20approach.%20Fruitful%20research%20directions%0Aimplied%20by%20our%20framework%20include%20%281%29%20clearly%20defining%20explanatory%20simplicity%2C%0A%282%29%20focusing%20on%20unifying%20explanations%20and%20%283%29%20deriving%20universal%20principles%20for%0Aneural%20networks.%20Improved%20MI%20methods%20enhance%20our%20ability%20to%20monitor%2C%20predict%2C%0Aand%20steer%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01372v1&entry.124074799=Read"},
{"title": "An Efficient Matrix Multiplication Algorithm for Accelerating Inference\n  in Binary and Ternary Neural Networks", "author": "Mohsen Dehghankar and Mahdi Erfanian and Abolfazl Asudeh", "abstract": "  Despite their tremendous success and versatility, Deep Neural Networks (DNNs)\nsuch as Large Language Models (LLMs) suffer from inference inefficiency and\nrely on advanced computational infrastructure. To address these challenges and\nmake these models more accessible and cost-effective, in this paper, we propose\nalgorithms to improve the inference time and memory efficiency of DNNs with\nbinary and ternary weight matrices. Particularly focusing on matrix\nmultiplication as the bottleneck operation of inference, we observe that, once\ntrained, the weight matrices of a model no longer change. This allows us to\npreprocess these matrices and create indices that help reduce the storage\nrequirements by a logarithmic factor while enabling our efficient inference\nalgorithms. Specifically, for a $n\\times n$ weight matrix, our efficient\nalgorithm guarantees a time complexity of $O(\\frac{n^2}{\\log n})$, a\nlogarithmic factor improvement over the standard vector-matrix multiplication.\nBesides theoretical analysis, we conduct extensive experiments to evaluate the\npractical efficiency of our algorithms. Our results confirm the superiority of\nour approach both with respect to time and memory, as we observed a reduction\nin the multiplication time up to 29x and memory usage up to 6x. When applied to\nLLMs, our experiments show up to a 5.24x speedup in the inference time.\n", "link": "http://arxiv.org/abs/2411.06360v3", "date": "2025-05-02", "relevancy": 1.957, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5089}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5062}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Matrix%20Multiplication%20Algorithm%20for%20Accelerating%20Inference%0A%20%20in%20Binary%20and%20Ternary%20Neural%20Networks&body=Title%3A%20An%20Efficient%20Matrix%20Multiplication%20Algorithm%20for%20Accelerating%20Inference%0A%20%20in%20Binary%20and%20Ternary%20Neural%20Networks%0AAuthor%3A%20Mohsen%20Dehghankar%20and%20Mahdi%20Erfanian%20and%20Abolfazl%20Asudeh%0AAbstract%3A%20%20%20Despite%20their%20tremendous%20success%20and%20versatility%2C%20Deep%20Neural%20Networks%20%28DNNs%29%0Asuch%20as%20Large%20Language%20Models%20%28LLMs%29%20suffer%20from%20inference%20inefficiency%20and%0Arely%20on%20advanced%20computational%20infrastructure.%20To%20address%20these%20challenges%20and%0Amake%20these%20models%20more%20accessible%20and%20cost-effective%2C%20in%20this%20paper%2C%20we%20propose%0Aalgorithms%20to%20improve%20the%20inference%20time%20and%20memory%20efficiency%20of%20DNNs%20with%0Abinary%20and%20ternary%20weight%20matrices.%20Particularly%20focusing%20on%20matrix%0Amultiplication%20as%20the%20bottleneck%20operation%20of%20inference%2C%20we%20observe%20that%2C%20once%0Atrained%2C%20the%20weight%20matrices%20of%20a%20model%20no%20longer%20change.%20This%20allows%20us%20to%0Apreprocess%20these%20matrices%20and%20create%20indices%20that%20help%20reduce%20the%20storage%0Arequirements%20by%20a%20logarithmic%20factor%20while%20enabling%20our%20efficient%20inference%0Aalgorithms.%20Specifically%2C%20for%20a%20%24n%5Ctimes%20n%24%20weight%20matrix%2C%20our%20efficient%0Aalgorithm%20guarantees%20a%20time%20complexity%20of%20%24O%28%5Cfrac%7Bn%5E2%7D%7B%5Clog%20n%7D%29%24%2C%20a%0Alogarithmic%20factor%20improvement%20over%20the%20standard%20vector-matrix%20multiplication.%0ABesides%20theoretical%20analysis%2C%20we%20conduct%20extensive%20experiments%20to%20evaluate%20the%0Apractical%20efficiency%20of%20our%20algorithms.%20Our%20results%20confirm%20the%20superiority%20of%0Aour%20approach%20both%20with%20respect%20to%20time%20and%20memory%2C%20as%20we%20observed%20a%20reduction%0Ain%20the%20multiplication%20time%20up%20to%2029x%20and%20memory%20usage%20up%20to%206x.%20When%20applied%20to%0ALLMs%2C%20our%20experiments%20show%20up%20to%20a%205.24x%20speedup%20in%20the%20inference%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06360v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Matrix%2520Multiplication%2520Algorithm%2520for%2520Accelerating%2520Inference%250A%2520%2520in%2520Binary%2520and%2520Ternary%2520Neural%2520Networks%26entry.906535625%3DMohsen%2520Dehghankar%2520and%2520Mahdi%2520Erfanian%2520and%2520Abolfazl%2520Asudeh%26entry.1292438233%3D%2520%2520Despite%2520their%2520tremendous%2520success%2520and%2520versatility%252C%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%250Asuch%2520as%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520suffer%2520from%2520inference%2520inefficiency%2520and%250Arely%2520on%2520advanced%2520computational%2520infrastructure.%2520To%2520address%2520these%2520challenges%2520and%250Amake%2520these%2520models%2520more%2520accessible%2520and%2520cost-effective%252C%2520in%2520this%2520paper%252C%2520we%2520propose%250Aalgorithms%2520to%2520improve%2520the%2520inference%2520time%2520and%2520memory%2520efficiency%2520of%2520DNNs%2520with%250Abinary%2520and%2520ternary%2520weight%2520matrices.%2520Particularly%2520focusing%2520on%2520matrix%250Amultiplication%2520as%2520the%2520bottleneck%2520operation%2520of%2520inference%252C%2520we%2520observe%2520that%252C%2520once%250Atrained%252C%2520the%2520weight%2520matrices%2520of%2520a%2520model%2520no%2520longer%2520change.%2520This%2520allows%2520us%2520to%250Apreprocess%2520these%2520matrices%2520and%2520create%2520indices%2520that%2520help%2520reduce%2520the%2520storage%250Arequirements%2520by%2520a%2520logarithmic%2520factor%2520while%2520enabling%2520our%2520efficient%2520inference%250Aalgorithms.%2520Specifically%252C%2520for%2520a%2520%2524n%255Ctimes%2520n%2524%2520weight%2520matrix%252C%2520our%2520efficient%250Aalgorithm%2520guarantees%2520a%2520time%2520complexity%2520of%2520%2524O%2528%255Cfrac%257Bn%255E2%257D%257B%255Clog%2520n%257D%2529%2524%252C%2520a%250Alogarithmic%2520factor%2520improvement%2520over%2520the%2520standard%2520vector-matrix%2520multiplication.%250ABesides%2520theoretical%2520analysis%252C%2520we%2520conduct%2520extensive%2520experiments%2520to%2520evaluate%2520the%250Apractical%2520efficiency%2520of%2520our%2520algorithms.%2520Our%2520results%2520confirm%2520the%2520superiority%2520of%250Aour%2520approach%2520both%2520with%2520respect%2520to%2520time%2520and%2520memory%252C%2520as%2520we%2520observed%2520a%2520reduction%250Ain%2520the%2520multiplication%2520time%2520up%2520to%252029x%2520and%2520memory%2520usage%2520up%2520to%25206x.%2520When%2520applied%2520to%250ALLMs%252C%2520our%2520experiments%2520show%2520up%2520to%2520a%25205.24x%2520speedup%2520in%2520the%2520inference%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06360v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Matrix%20Multiplication%20Algorithm%20for%20Accelerating%20Inference%0A%20%20in%20Binary%20and%20Ternary%20Neural%20Networks&entry.906535625=Mohsen%20Dehghankar%20and%20Mahdi%20Erfanian%20and%20Abolfazl%20Asudeh&entry.1292438233=%20%20Despite%20their%20tremendous%20success%20and%20versatility%2C%20Deep%20Neural%20Networks%20%28DNNs%29%0Asuch%20as%20Large%20Language%20Models%20%28LLMs%29%20suffer%20from%20inference%20inefficiency%20and%0Arely%20on%20advanced%20computational%20infrastructure.%20To%20address%20these%20challenges%20and%0Amake%20these%20models%20more%20accessible%20and%20cost-effective%2C%20in%20this%20paper%2C%20we%20propose%0Aalgorithms%20to%20improve%20the%20inference%20time%20and%20memory%20efficiency%20of%20DNNs%20with%0Abinary%20and%20ternary%20weight%20matrices.%20Particularly%20focusing%20on%20matrix%0Amultiplication%20as%20the%20bottleneck%20operation%20of%20inference%2C%20we%20observe%20that%2C%20once%0Atrained%2C%20the%20weight%20matrices%20of%20a%20model%20no%20longer%20change.%20This%20allows%20us%20to%0Apreprocess%20these%20matrices%20and%20create%20indices%20that%20help%20reduce%20the%20storage%0Arequirements%20by%20a%20logarithmic%20factor%20while%20enabling%20our%20efficient%20inference%0Aalgorithms.%20Specifically%2C%20for%20a%20%24n%5Ctimes%20n%24%20weight%20matrix%2C%20our%20efficient%0Aalgorithm%20guarantees%20a%20time%20complexity%20of%20%24O%28%5Cfrac%7Bn%5E2%7D%7B%5Clog%20n%7D%29%24%2C%20a%0Alogarithmic%20factor%20improvement%20over%20the%20standard%20vector-matrix%20multiplication.%0ABesides%20theoretical%20analysis%2C%20we%20conduct%20extensive%20experiments%20to%20evaluate%20the%0Apractical%20efficiency%20of%20our%20algorithms.%20Our%20results%20confirm%20the%20superiority%20of%0Aour%20approach%20both%20with%20respect%20to%20time%20and%20memory%2C%20as%20we%20observed%20a%20reduction%0Ain%20the%20multiplication%20time%20up%20to%2029x%20and%20memory%20usage%20up%20to%206x.%20When%20applied%20to%0ALLMs%2C%20our%20experiments%20show%20up%20to%20a%205.24x%20speedup%20in%20the%20inference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06360v3&entry.124074799=Read"},
{"title": "A Neural Architecture Search Method using Auxiliary Evaluation Metric\n  based on ResNet Architecture", "author": "Shang Wang and Huanrong Tang and Jianquan Ouyang", "abstract": "  This paper proposes a neural architecture search space using ResNet as a\nframework, with search objectives including parameters for convolution,\npooling, fully connected layers, and connectivity of the residual network. In\naddition to recognition accuracy, this paper uses the loss value on the\nvalidation set as a secondary objective for optimization. The experimental\nresults demonstrate that the search space of this paper together with the\noptimisation approach can find competitive network architectures on the MNIST,\nFashion-MNIST and CIFAR100 datasets.\n", "link": "http://arxiv.org/abs/2505.01313v1", "date": "2025-05-02", "relevancy": 1.9446, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5039}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4815}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Neural%20Architecture%20Search%20Method%20using%20Auxiliary%20Evaluation%20Metric%0A%20%20based%20on%20ResNet%20Architecture&body=Title%3A%20A%20Neural%20Architecture%20Search%20Method%20using%20Auxiliary%20Evaluation%20Metric%0A%20%20based%20on%20ResNet%20Architecture%0AAuthor%3A%20Shang%20Wang%20and%20Huanrong%20Tang%20and%20Jianquan%20Ouyang%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20neural%20architecture%20search%20space%20using%20ResNet%20as%20a%0Aframework%2C%20with%20search%20objectives%20including%20parameters%20for%20convolution%2C%0Apooling%2C%20fully%20connected%20layers%2C%20and%20connectivity%20of%20the%20residual%20network.%20In%0Aaddition%20to%20recognition%20accuracy%2C%20this%20paper%20uses%20the%20loss%20value%20on%20the%0Avalidation%20set%20as%20a%20secondary%20objective%20for%20optimization.%20The%20experimental%0Aresults%20demonstrate%20that%20the%20search%20space%20of%20this%20paper%20together%20with%20the%0Aoptimisation%20approach%20can%20find%20competitive%20network%20architectures%20on%20the%20MNIST%2C%0AFashion-MNIST%20and%20CIFAR100%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Neural%2520Architecture%2520Search%2520Method%2520using%2520Auxiliary%2520Evaluation%2520Metric%250A%2520%2520based%2520on%2520ResNet%2520Architecture%26entry.906535625%3DShang%2520Wang%2520and%2520Huanrong%2520Tang%2520and%2520Jianquan%2520Ouyang%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520neural%2520architecture%2520search%2520space%2520using%2520ResNet%2520as%2520a%250Aframework%252C%2520with%2520search%2520objectives%2520including%2520parameters%2520for%2520convolution%252C%250Apooling%252C%2520fully%2520connected%2520layers%252C%2520and%2520connectivity%2520of%2520the%2520residual%2520network.%2520In%250Aaddition%2520to%2520recognition%2520accuracy%252C%2520this%2520paper%2520uses%2520the%2520loss%2520value%2520on%2520the%250Avalidation%2520set%2520as%2520a%2520secondary%2520objective%2520for%2520optimization.%2520The%2520experimental%250Aresults%2520demonstrate%2520that%2520the%2520search%2520space%2520of%2520this%2520paper%2520together%2520with%2520the%250Aoptimisation%2520approach%2520can%2520find%2520competitive%2520network%2520architectures%2520on%2520the%2520MNIST%252C%250AFashion-MNIST%2520and%2520CIFAR100%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Neural%20Architecture%20Search%20Method%20using%20Auxiliary%20Evaluation%20Metric%0A%20%20based%20on%20ResNet%20Architecture&entry.906535625=Shang%20Wang%20and%20Huanrong%20Tang%20and%20Jianquan%20Ouyang&entry.1292438233=%20%20This%20paper%20proposes%20a%20neural%20architecture%20search%20space%20using%20ResNet%20as%20a%0Aframework%2C%20with%20search%20objectives%20including%20parameters%20for%20convolution%2C%0Apooling%2C%20fully%20connected%20layers%2C%20and%20connectivity%20of%20the%20residual%20network.%20In%0Aaddition%20to%20recognition%20accuracy%2C%20this%20paper%20uses%20the%20loss%20value%20on%20the%0Avalidation%20set%20as%20a%20secondary%20objective%20for%20optimization.%20The%20experimental%0Aresults%20demonstrate%20that%20the%20search%20space%20of%20this%20paper%20together%20with%20the%0Aoptimisation%20approach%20can%20find%20competitive%20network%20architectures%20on%20the%20MNIST%2C%0AFashion-MNIST%20and%20CIFAR100%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01313v1&entry.124074799=Read"},
{"title": "chebgreen: Learning and Interpolating Continuous Empirical Green's\n  Functions from Data", "author": "Harshwardhan Praveen and Jacob Brown and Christopher Earls", "abstract": "  In this work, we present a mesh-independent, data-driven library, chebgreen,\nto mathematically model one-dimensional systems, possessing an associated\ncontrol parameter, and whose governing partial differential equation is\nunknown. The proposed method learns an Empirical Green's Function for the\nassociated, but hidden, boundary value problem, in the form of a Rational\nNeural Network from which we subsequently construct a bivariate representation\nin a Chebyshev basis. We uncover the Green's function, at an unseen control\nparameter value, by interpolating the left and right singular functions within\na suitable library, expressed as points on a manifold of Quasimatrices, while\nthe associated singular values are interpolated with Lagrange polynomials.\n", "link": "http://arxiv.org/abs/2501.18715v3", "date": "2025-05-02", "relevancy": 1.9268, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4996}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4736}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20chebgreen%3A%20Learning%20and%20Interpolating%20Continuous%20Empirical%20Green%27s%0A%20%20Functions%20from%20Data&body=Title%3A%20chebgreen%3A%20Learning%20and%20Interpolating%20Continuous%20Empirical%20Green%27s%0A%20%20Functions%20from%20Data%0AAuthor%3A%20Harshwardhan%20Praveen%20and%20Jacob%20Brown%20and%20Christopher%20Earls%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20mesh-independent%2C%20data-driven%20library%2C%20chebgreen%2C%0Ato%20mathematically%20model%20one-dimensional%20systems%2C%20possessing%20an%20associated%0Acontrol%20parameter%2C%20and%20whose%20governing%20partial%20differential%20equation%20is%0Aunknown.%20The%20proposed%20method%20learns%20an%20Empirical%20Green%27s%20Function%20for%20the%0Aassociated%2C%20but%20hidden%2C%20boundary%20value%20problem%2C%20in%20the%20form%20of%20a%20Rational%0ANeural%20Network%20from%20which%20we%20subsequently%20construct%20a%20bivariate%20representation%0Ain%20a%20Chebyshev%20basis.%20We%20uncover%20the%20Green%27s%20function%2C%20at%20an%20unseen%20control%0Aparameter%20value%2C%20by%20interpolating%20the%20left%20and%20right%20singular%20functions%20within%0Aa%20suitable%20library%2C%20expressed%20as%20points%20on%20a%20manifold%20of%20Quasimatrices%2C%20while%0Athe%20associated%20singular%20values%20are%20interpolated%20with%20Lagrange%20polynomials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18715v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dchebgreen%253A%2520Learning%2520and%2520Interpolating%2520Continuous%2520Empirical%2520Green%2527s%250A%2520%2520Functions%2520from%2520Data%26entry.906535625%3DHarshwardhan%2520Praveen%2520and%2520Jacob%2520Brown%2520and%2520Christopher%2520Earls%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520mesh-independent%252C%2520data-driven%2520library%252C%2520chebgreen%252C%250Ato%2520mathematically%2520model%2520one-dimensional%2520systems%252C%2520possessing%2520an%2520associated%250Acontrol%2520parameter%252C%2520and%2520whose%2520governing%2520partial%2520differential%2520equation%2520is%250Aunknown.%2520The%2520proposed%2520method%2520learns%2520an%2520Empirical%2520Green%2527s%2520Function%2520for%2520the%250Aassociated%252C%2520but%2520hidden%252C%2520boundary%2520value%2520problem%252C%2520in%2520the%2520form%2520of%2520a%2520Rational%250ANeural%2520Network%2520from%2520which%2520we%2520subsequently%2520construct%2520a%2520bivariate%2520representation%250Ain%2520a%2520Chebyshev%2520basis.%2520We%2520uncover%2520the%2520Green%2527s%2520function%252C%2520at%2520an%2520unseen%2520control%250Aparameter%2520value%252C%2520by%2520interpolating%2520the%2520left%2520and%2520right%2520singular%2520functions%2520within%250Aa%2520suitable%2520library%252C%2520expressed%2520as%2520points%2520on%2520a%2520manifold%2520of%2520Quasimatrices%252C%2520while%250Athe%2520associated%2520singular%2520values%2520are%2520interpolated%2520with%2520Lagrange%2520polynomials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18715v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=chebgreen%3A%20Learning%20and%20Interpolating%20Continuous%20Empirical%20Green%27s%0A%20%20Functions%20from%20Data&entry.906535625=Harshwardhan%20Praveen%20and%20Jacob%20Brown%20and%20Christopher%20Earls&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20mesh-independent%2C%20data-driven%20library%2C%20chebgreen%2C%0Ato%20mathematically%20model%20one-dimensional%20systems%2C%20possessing%20an%20associated%0Acontrol%20parameter%2C%20and%20whose%20governing%20partial%20differential%20equation%20is%0Aunknown.%20The%20proposed%20method%20learns%20an%20Empirical%20Green%27s%20Function%20for%20the%0Aassociated%2C%20but%20hidden%2C%20boundary%20value%20problem%2C%20in%20the%20form%20of%20a%20Rational%0ANeural%20Network%20from%20which%20we%20subsequently%20construct%20a%20bivariate%20representation%0Ain%20a%20Chebyshev%20basis.%20We%20uncover%20the%20Green%27s%20function%2C%20at%20an%20unseen%20control%0Aparameter%20value%2C%20by%20interpolating%20the%20left%20and%20right%20singular%20functions%20within%0Aa%20suitable%20library%2C%20expressed%20as%20points%20on%20a%20manifold%20of%20Quasimatrices%2C%20while%0Athe%20associated%20singular%20values%20are%20interpolated%20with%20Lagrange%20polynomials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18715v3&entry.124074799=Read"},
{"title": "Toward Teach and Repeat Across Seasonal Deep Snow Accumulation", "author": "Mat\u011bj Boxan and Alexander Krawciw and Timothy D. Barfoot and Fran\u00e7ois Pomerleau", "abstract": "  Teach and repeat is a rapid way to achieve autonomy in challenging terrain\nand off-road environments. A human operator pilots the vehicles to create a\nnetwork of paths that are mapped and associated with odometry. Immediately\nafter teaching, the system can drive autonomously within its tracks. This\nprecision lets operators remain confident that the robot will follow a\ntraversable route. However, this operational paradigm has rarely been explored\nin off-road environments that change significantly through seasonal variation.\nThis paper presents preliminary field trials using lidar and radar\nimplementations of teach and repeat. Using a subset of the data from the\nupcoming FoMo dataset, we attempted to repeat routes that were 4 days, 44 days,\nand 113 days old. Lidar teach and repeat demonstrated a stronger ability to\nlocalize when the ground points were removed. FMCW radar was often able to\nlocalize on older maps, but only with small deviations from the taught path.\nAdditionally, we highlight specific cases where radar localization failed with\nrecent maps due to the high pitch or roll of the vehicle. We highlight lessons\nlearned during the field deployment and highlight areas to improve to achieve\nreliable teach and repeat with seasonal changes in the environment. Please\nfollow the dataset at https://norlab-ulaval.github.io/FoMo-website for updates\nand information on the data release.\n", "link": "http://arxiv.org/abs/2505.01339v1", "date": "2025-05-02", "relevancy": 1.9191, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4939}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4905}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Teach%20and%20Repeat%20Across%20Seasonal%20Deep%20Snow%20Accumulation&body=Title%3A%20Toward%20Teach%20and%20Repeat%20Across%20Seasonal%20Deep%20Snow%20Accumulation%0AAuthor%3A%20Mat%C4%9Bj%20Boxan%20and%20Alexander%20Krawciw%20and%20Timothy%20D.%20Barfoot%20and%20Fran%C3%A7ois%20Pomerleau%0AAbstract%3A%20%20%20Teach%20and%20repeat%20is%20a%20rapid%20way%20to%20achieve%20autonomy%20in%20challenging%20terrain%0Aand%20off-road%20environments.%20A%20human%20operator%20pilots%20the%20vehicles%20to%20create%20a%0Anetwork%20of%20paths%20that%20are%20mapped%20and%20associated%20with%20odometry.%20Immediately%0Aafter%20teaching%2C%20the%20system%20can%20drive%20autonomously%20within%20its%20tracks.%20This%0Aprecision%20lets%20operators%20remain%20confident%20that%20the%20robot%20will%20follow%20a%0Atraversable%20route.%20However%2C%20this%20operational%20paradigm%20has%20rarely%20been%20explored%0Ain%20off-road%20environments%20that%20change%20significantly%20through%20seasonal%20variation.%0AThis%20paper%20presents%20preliminary%20field%20trials%20using%20lidar%20and%20radar%0Aimplementations%20of%20teach%20and%20repeat.%20Using%20a%20subset%20of%20the%20data%20from%20the%0Aupcoming%20FoMo%20dataset%2C%20we%20attempted%20to%20repeat%20routes%20that%20were%204%20days%2C%2044%20days%2C%0Aand%20113%20days%20old.%20Lidar%20teach%20and%20repeat%20demonstrated%20a%20stronger%20ability%20to%0Alocalize%20when%20the%20ground%20points%20were%20removed.%20FMCW%20radar%20was%20often%20able%20to%0Alocalize%20on%20older%20maps%2C%20but%20only%20with%20small%20deviations%20from%20the%20taught%20path.%0AAdditionally%2C%20we%20highlight%20specific%20cases%20where%20radar%20localization%20failed%20with%0Arecent%20maps%20due%20to%20the%20high%20pitch%20or%20roll%20of%20the%20vehicle.%20We%20highlight%20lessons%0Alearned%20during%20the%20field%20deployment%20and%20highlight%20areas%20to%20improve%20to%20achieve%0Areliable%20teach%20and%20repeat%20with%20seasonal%20changes%20in%20the%20environment.%20Please%0Afollow%20the%20dataset%20at%20https%3A//norlab-ulaval.github.io/FoMo-website%20for%20updates%0Aand%20information%20on%20the%20data%20release.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Teach%2520and%2520Repeat%2520Across%2520Seasonal%2520Deep%2520Snow%2520Accumulation%26entry.906535625%3DMat%25C4%259Bj%2520Boxan%2520and%2520Alexander%2520Krawciw%2520and%2520Timothy%2520D.%2520Barfoot%2520and%2520Fran%25C3%25A7ois%2520Pomerleau%26entry.1292438233%3D%2520%2520Teach%2520and%2520repeat%2520is%2520a%2520rapid%2520way%2520to%2520achieve%2520autonomy%2520in%2520challenging%2520terrain%250Aand%2520off-road%2520environments.%2520A%2520human%2520operator%2520pilots%2520the%2520vehicles%2520to%2520create%2520a%250Anetwork%2520of%2520paths%2520that%2520are%2520mapped%2520and%2520associated%2520with%2520odometry.%2520Immediately%250Aafter%2520teaching%252C%2520the%2520system%2520can%2520drive%2520autonomously%2520within%2520its%2520tracks.%2520This%250Aprecision%2520lets%2520operators%2520remain%2520confident%2520that%2520the%2520robot%2520will%2520follow%2520a%250Atraversable%2520route.%2520However%252C%2520this%2520operational%2520paradigm%2520has%2520rarely%2520been%2520explored%250Ain%2520off-road%2520environments%2520that%2520change%2520significantly%2520through%2520seasonal%2520variation.%250AThis%2520paper%2520presents%2520preliminary%2520field%2520trials%2520using%2520lidar%2520and%2520radar%250Aimplementations%2520of%2520teach%2520and%2520repeat.%2520Using%2520a%2520subset%2520of%2520the%2520data%2520from%2520the%250Aupcoming%2520FoMo%2520dataset%252C%2520we%2520attempted%2520to%2520repeat%2520routes%2520that%2520were%25204%2520days%252C%252044%2520days%252C%250Aand%2520113%2520days%2520old.%2520Lidar%2520teach%2520and%2520repeat%2520demonstrated%2520a%2520stronger%2520ability%2520to%250Alocalize%2520when%2520the%2520ground%2520points%2520were%2520removed.%2520FMCW%2520radar%2520was%2520often%2520able%2520to%250Alocalize%2520on%2520older%2520maps%252C%2520but%2520only%2520with%2520small%2520deviations%2520from%2520the%2520taught%2520path.%250AAdditionally%252C%2520we%2520highlight%2520specific%2520cases%2520where%2520radar%2520localization%2520failed%2520with%250Arecent%2520maps%2520due%2520to%2520the%2520high%2520pitch%2520or%2520roll%2520of%2520the%2520vehicle.%2520We%2520highlight%2520lessons%250Alearned%2520during%2520the%2520field%2520deployment%2520and%2520highlight%2520areas%2520to%2520improve%2520to%2520achieve%250Areliable%2520teach%2520and%2520repeat%2520with%2520seasonal%2520changes%2520in%2520the%2520environment.%2520Please%250Afollow%2520the%2520dataset%2520at%2520https%253A//norlab-ulaval.github.io/FoMo-website%2520for%2520updates%250Aand%2520information%2520on%2520the%2520data%2520release.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Teach%20and%20Repeat%20Across%20Seasonal%20Deep%20Snow%20Accumulation&entry.906535625=Mat%C4%9Bj%20Boxan%20and%20Alexander%20Krawciw%20and%20Timothy%20D.%20Barfoot%20and%20Fran%C3%A7ois%20Pomerleau&entry.1292438233=%20%20Teach%20and%20repeat%20is%20a%20rapid%20way%20to%20achieve%20autonomy%20in%20challenging%20terrain%0Aand%20off-road%20environments.%20A%20human%20operator%20pilots%20the%20vehicles%20to%20create%20a%0Anetwork%20of%20paths%20that%20are%20mapped%20and%20associated%20with%20odometry.%20Immediately%0Aafter%20teaching%2C%20the%20system%20can%20drive%20autonomously%20within%20its%20tracks.%20This%0Aprecision%20lets%20operators%20remain%20confident%20that%20the%20robot%20will%20follow%20a%0Atraversable%20route.%20However%2C%20this%20operational%20paradigm%20has%20rarely%20been%20explored%0Ain%20off-road%20environments%20that%20change%20significantly%20through%20seasonal%20variation.%0AThis%20paper%20presents%20preliminary%20field%20trials%20using%20lidar%20and%20radar%0Aimplementations%20of%20teach%20and%20repeat.%20Using%20a%20subset%20of%20the%20data%20from%20the%0Aupcoming%20FoMo%20dataset%2C%20we%20attempted%20to%20repeat%20routes%20that%20were%204%20days%2C%2044%20days%2C%0Aand%20113%20days%20old.%20Lidar%20teach%20and%20repeat%20demonstrated%20a%20stronger%20ability%20to%0Alocalize%20when%20the%20ground%20points%20were%20removed.%20FMCW%20radar%20was%20often%20able%20to%0Alocalize%20on%20older%20maps%2C%20but%20only%20with%20small%20deviations%20from%20the%20taught%20path.%0AAdditionally%2C%20we%20highlight%20specific%20cases%20where%20radar%20localization%20failed%20with%0Arecent%20maps%20due%20to%20the%20high%20pitch%20or%20roll%20of%20the%20vehicle.%20We%20highlight%20lessons%0Alearned%20during%20the%20field%20deployment%20and%20highlight%20areas%20to%20improve%20to%20achieve%0Areliable%20teach%20and%20repeat%20with%20seasonal%20changes%20in%20the%20environment.%20Please%0Afollow%20the%20dataset%20at%20https%3A//norlab-ulaval.github.io/FoMo-website%20for%20updates%0Aand%20information%20on%20the%20data%20release.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01339v1&entry.124074799=Read"},
{"title": "Learning Lifted STRIPS Models from Action Traces Alone: A Simple,\n  General, and Scalable Solution", "author": "Jonas G\u00f6sgens and Niklas Jansen and Hector Geffner", "abstract": "  Learning STRIPS action models from action traces alone is a challenging\nproblem as it involves learning the domain predicates as well. In this work, a\nnovel approach is introduced which, like the well-known LOCM systems, is\nscalable, but like SAT approaches, is sound and complete. Furthermore, the\napproach is general and imposes no restrictions on the hidden domain or the\nnumber or arity of the predicates. The new learning method is based on an\n\\emph{efficient, novel test} that checks whether the assumption that a\npredicate is affected by a set of action patterns, namely, actions with\nspecific argument positions, is consistent with the traces. The predicates and\naction patterns that pass the test provide the basis for the learned domain\nthat is then easily completed with preconditions and static predicates. The new\nmethod is studied theoretically and experimentally. For the latter, the method\nis evaluated on traces and graphs obtained from standard classical domains like\nthe 8-puzzle, which involve hundreds of thousands of states and transitions.\nThe learned representations are then verified on larger instances.\n", "link": "http://arxiv.org/abs/2411.14995v2", "date": "2025-05-02", "relevancy": 1.9175, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5195}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4737}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Lifted%20STRIPS%20Models%20from%20Action%20Traces%20Alone%3A%20A%20Simple%2C%0A%20%20General%2C%20and%20Scalable%20Solution&body=Title%3A%20Learning%20Lifted%20STRIPS%20Models%20from%20Action%20Traces%20Alone%3A%20A%20Simple%2C%0A%20%20General%2C%20and%20Scalable%20Solution%0AAuthor%3A%20Jonas%20G%C3%B6sgens%20and%20Niklas%20Jansen%20and%20Hector%20Geffner%0AAbstract%3A%20%20%20Learning%20STRIPS%20action%20models%20from%20action%20traces%20alone%20is%20a%20challenging%0Aproblem%20as%20it%20involves%20learning%20the%20domain%20predicates%20as%20well.%20In%20this%20work%2C%20a%0Anovel%20approach%20is%20introduced%20which%2C%20like%20the%20well-known%20LOCM%20systems%2C%20is%0Ascalable%2C%20but%20like%20SAT%20approaches%2C%20is%20sound%20and%20complete.%20Furthermore%2C%20the%0Aapproach%20is%20general%20and%20imposes%20no%20restrictions%20on%20the%20hidden%20domain%20or%20the%0Anumber%20or%20arity%20of%20the%20predicates.%20The%20new%20learning%20method%20is%20based%20on%20an%0A%5Cemph%7Befficient%2C%20novel%20test%7D%20that%20checks%20whether%20the%20assumption%20that%20a%0Apredicate%20is%20affected%20by%20a%20set%20of%20action%20patterns%2C%20namely%2C%20actions%20with%0Aspecific%20argument%20positions%2C%20is%20consistent%20with%20the%20traces.%20The%20predicates%20and%0Aaction%20patterns%20that%20pass%20the%20test%20provide%20the%20basis%20for%20the%20learned%20domain%0Athat%20is%20then%20easily%20completed%20with%20preconditions%20and%20static%20predicates.%20The%20new%0Amethod%20is%20studied%20theoretically%20and%20experimentally.%20For%20the%20latter%2C%20the%20method%0Ais%20evaluated%20on%20traces%20and%20graphs%20obtained%20from%20standard%20classical%20domains%20like%0Athe%208-puzzle%2C%20which%20involve%20hundreds%20of%20thousands%20of%20states%20and%20transitions.%0AThe%20learned%20representations%20are%20then%20verified%20on%20larger%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14995v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Lifted%2520STRIPS%2520Models%2520from%2520Action%2520Traces%2520Alone%253A%2520A%2520Simple%252C%250A%2520%2520General%252C%2520and%2520Scalable%2520Solution%26entry.906535625%3DJonas%2520G%25C3%25B6sgens%2520and%2520Niklas%2520Jansen%2520and%2520Hector%2520Geffner%26entry.1292438233%3D%2520%2520Learning%2520STRIPS%2520action%2520models%2520from%2520action%2520traces%2520alone%2520is%2520a%2520challenging%250Aproblem%2520as%2520it%2520involves%2520learning%2520the%2520domain%2520predicates%2520as%2520well.%2520In%2520this%2520work%252C%2520a%250Anovel%2520approach%2520is%2520introduced%2520which%252C%2520like%2520the%2520well-known%2520LOCM%2520systems%252C%2520is%250Ascalable%252C%2520but%2520like%2520SAT%2520approaches%252C%2520is%2520sound%2520and%2520complete.%2520Furthermore%252C%2520the%250Aapproach%2520is%2520general%2520and%2520imposes%2520no%2520restrictions%2520on%2520the%2520hidden%2520domain%2520or%2520the%250Anumber%2520or%2520arity%2520of%2520the%2520predicates.%2520The%2520new%2520learning%2520method%2520is%2520based%2520on%2520an%250A%255Cemph%257Befficient%252C%2520novel%2520test%257D%2520that%2520checks%2520whether%2520the%2520assumption%2520that%2520a%250Apredicate%2520is%2520affected%2520by%2520a%2520set%2520of%2520action%2520patterns%252C%2520namely%252C%2520actions%2520with%250Aspecific%2520argument%2520positions%252C%2520is%2520consistent%2520with%2520the%2520traces.%2520The%2520predicates%2520and%250Aaction%2520patterns%2520that%2520pass%2520the%2520test%2520provide%2520the%2520basis%2520for%2520the%2520learned%2520domain%250Athat%2520is%2520then%2520easily%2520completed%2520with%2520preconditions%2520and%2520static%2520predicates.%2520The%2520new%250Amethod%2520is%2520studied%2520theoretically%2520and%2520experimentally.%2520For%2520the%2520latter%252C%2520the%2520method%250Ais%2520evaluated%2520on%2520traces%2520and%2520graphs%2520obtained%2520from%2520standard%2520classical%2520domains%2520like%250Athe%25208-puzzle%252C%2520which%2520involve%2520hundreds%2520of%2520thousands%2520of%2520states%2520and%2520transitions.%250AThe%2520learned%2520representations%2520are%2520then%2520verified%2520on%2520larger%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14995v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Lifted%20STRIPS%20Models%20from%20Action%20Traces%20Alone%3A%20A%20Simple%2C%0A%20%20General%2C%20and%20Scalable%20Solution&entry.906535625=Jonas%20G%C3%B6sgens%20and%20Niklas%20Jansen%20and%20Hector%20Geffner&entry.1292438233=%20%20Learning%20STRIPS%20action%20models%20from%20action%20traces%20alone%20is%20a%20challenging%0Aproblem%20as%20it%20involves%20learning%20the%20domain%20predicates%20as%20well.%20In%20this%20work%2C%20a%0Anovel%20approach%20is%20introduced%20which%2C%20like%20the%20well-known%20LOCM%20systems%2C%20is%0Ascalable%2C%20but%20like%20SAT%20approaches%2C%20is%20sound%20and%20complete.%20Furthermore%2C%20the%0Aapproach%20is%20general%20and%20imposes%20no%20restrictions%20on%20the%20hidden%20domain%20or%20the%0Anumber%20or%20arity%20of%20the%20predicates.%20The%20new%20learning%20method%20is%20based%20on%20an%0A%5Cemph%7Befficient%2C%20novel%20test%7D%20that%20checks%20whether%20the%20assumption%20that%20a%0Apredicate%20is%20affected%20by%20a%20set%20of%20action%20patterns%2C%20namely%2C%20actions%20with%0Aspecific%20argument%20positions%2C%20is%20consistent%20with%20the%20traces.%20The%20predicates%20and%0Aaction%20patterns%20that%20pass%20the%20test%20provide%20the%20basis%20for%20the%20learned%20domain%0Athat%20is%20then%20easily%20completed%20with%20preconditions%20and%20static%20predicates.%20The%20new%0Amethod%20is%20studied%20theoretically%20and%20experimentally.%20For%20the%20latter%2C%20the%20method%0Ais%20evaluated%20on%20traces%20and%20graphs%20obtained%20from%20standard%20classical%20domains%20like%0Athe%208-puzzle%2C%20which%20involve%20hundreds%20of%20thousands%20of%20states%20and%20transitions.%0AThe%20learned%20representations%20are%20then%20verified%20on%20larger%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14995v2&entry.124074799=Read"},
{"title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration", "author": "Yushi Huang and Zining Wang and Ruihao Gong and Jing Liu and Xinjie Zhang and Jinyang Guo and Xianglong Liu and Jun Zhang", "abstract": "  Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.\n", "link": "http://arxiv.org/abs/2410.01723v4", "date": "2025-05-02", "relevancy": 1.9175, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6751}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6307}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HarmoniCa%3A%20Harmonizing%20Training%20and%20Inference%20for%20Better%20Feature%20Caching%0A%20%20in%20Diffusion%20Transformer%20Acceleration&body=Title%3A%20HarmoniCa%3A%20Harmonizing%20Training%20and%20Inference%20for%20Better%20Feature%20Caching%0A%20%20in%20Diffusion%20Transformer%20Acceleration%0AAuthor%3A%20Yushi%20Huang%20and%20Zining%20Wang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Xinjie%20Zhang%20and%20Jinyang%20Guo%20and%20Xianglong%20Liu%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiTs%29%20excel%20in%20generative%20tasks%20but%20face%20practical%0Adeployment%20challenges%20due%20to%20high%20inference%20costs.%20Feature%20caching%2C%20which%0Astores%20and%20retrieves%20redundant%20computations%2C%20offers%20the%20potential%20for%0Aacceleration.%20Existing%20learning-based%20caching%2C%20though%20adaptive%2C%20overlooks%20the%0Aimpact%20of%20the%20prior%20timestep.%20It%20also%20suffers%20from%20misaligned%0Aobjectives--aligned%20predicted%20noise%20vs.%20high-quality%20images--between%20training%0Aand%20inference.%20These%20two%20discrepancies%20compromise%20both%20performance%20and%0Aefficiency.%20To%20this%20end%2C%20we%20harmonize%20training%20and%20inference%20with%20a%20novel%0Alearning-based%20caching%20framework%20dubbed%20HarmoniCa.%20It%20first%20incorporates%0AStep-Wise%20Denoising%20Training%20%28SDT%29%20to%20ensure%20the%20continuity%20of%20the%20denoising%0Aprocess%2C%20where%20prior%20steps%20can%20be%20leveraged.%20In%20addition%2C%20an%20Image%20Error%0AProxy-Guided%20Objective%20%28IEPO%29%20is%20applied%20to%20balance%20image%20quality%20against%20cache%0Autilization%20through%20an%20efficient%20proxy%20to%20approximate%20the%20image%20error.%0AExtensive%20experiments%20across%20%248%24%20models%2C%20%244%24%20samplers%2C%20and%20resolutions%20from%0A%24256%5Ctimes256%24%20to%20%242K%24%20demonstrate%20superior%20performance%20and%20speedup%20of%20our%0Aframework.%20For%20instance%2C%20it%20achieves%20over%20%2440%5C%25%24%20latency%20reduction%20%28i.e.%2C%0A%242.07%5Ctimes%24%20theoretical%20speedup%29%20and%20improved%20performance%20on%20PixArt-%24%5Calpha%24.%0ARemarkably%2C%20our%20image-free%20approach%20reduces%20training%20time%20by%20%2425%5C%25%24%20compared%0Awith%20the%20previous%20method.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ModelTC/HarmoniCa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01723v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmoniCa%253A%2520Harmonizing%2520Training%2520and%2520Inference%2520for%2520Better%2520Feature%2520Caching%250A%2520%2520in%2520Diffusion%2520Transformer%2520Acceleration%26entry.906535625%3DYushi%2520Huang%2520and%2520Zining%2520Wang%2520and%2520Ruihao%2520Gong%2520and%2520Jing%2520Liu%2520and%2520Xinjie%2520Zhang%2520and%2520Jinyang%2520Guo%2520and%2520Xianglong%2520Liu%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520excel%2520in%2520generative%2520tasks%2520but%2520face%2520practical%250Adeployment%2520challenges%2520due%2520to%2520high%2520inference%2520costs.%2520Feature%2520caching%252C%2520which%250Astores%2520and%2520retrieves%2520redundant%2520computations%252C%2520offers%2520the%2520potential%2520for%250Aacceleration.%2520Existing%2520learning-based%2520caching%252C%2520though%2520adaptive%252C%2520overlooks%2520the%250Aimpact%2520of%2520the%2520prior%2520timestep.%2520It%2520also%2520suffers%2520from%2520misaligned%250Aobjectives--aligned%2520predicted%2520noise%2520vs.%2520high-quality%2520images--between%2520training%250Aand%2520inference.%2520These%2520two%2520discrepancies%2520compromise%2520both%2520performance%2520and%250Aefficiency.%2520To%2520this%2520end%252C%2520we%2520harmonize%2520training%2520and%2520inference%2520with%2520a%2520novel%250Alearning-based%2520caching%2520framework%2520dubbed%2520HarmoniCa.%2520It%2520first%2520incorporates%250AStep-Wise%2520Denoising%2520Training%2520%2528SDT%2529%2520to%2520ensure%2520the%2520continuity%2520of%2520the%2520denoising%250Aprocess%252C%2520where%2520prior%2520steps%2520can%2520be%2520leveraged.%2520In%2520addition%252C%2520an%2520Image%2520Error%250AProxy-Guided%2520Objective%2520%2528IEPO%2529%2520is%2520applied%2520to%2520balance%2520image%2520quality%2520against%2520cache%250Autilization%2520through%2520an%2520efficient%2520proxy%2520to%2520approximate%2520the%2520image%2520error.%250AExtensive%2520experiments%2520across%2520%25248%2524%2520models%252C%2520%25244%2524%2520samplers%252C%2520and%2520resolutions%2520from%250A%2524256%255Ctimes256%2524%2520to%2520%25242K%2524%2520demonstrate%2520superior%2520performance%2520and%2520speedup%2520of%2520our%250Aframework.%2520For%2520instance%252C%2520it%2520achieves%2520over%2520%252440%255C%2525%2524%2520latency%2520reduction%2520%2528i.e.%252C%250A%25242.07%255Ctimes%2524%2520theoretical%2520speedup%2529%2520and%2520improved%2520performance%2520on%2520PixArt-%2524%255Calpha%2524.%250ARemarkably%252C%2520our%2520image-free%2520approach%2520reduces%2520training%2520time%2520by%2520%252425%255C%2525%2524%2520compared%250Awith%2520the%2520previous%2520method.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ModelTC/HarmoniCa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01723v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HarmoniCa%3A%20Harmonizing%20Training%20and%20Inference%20for%20Better%20Feature%20Caching%0A%20%20in%20Diffusion%20Transformer%20Acceleration&entry.906535625=Yushi%20Huang%20and%20Zining%20Wang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Xinjie%20Zhang%20and%20Jinyang%20Guo%20and%20Xianglong%20Liu%20and%20Jun%20Zhang&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiTs%29%20excel%20in%20generative%20tasks%20but%20face%20practical%0Adeployment%20challenges%20due%20to%20high%20inference%20costs.%20Feature%20caching%2C%20which%0Astores%20and%20retrieves%20redundant%20computations%2C%20offers%20the%20potential%20for%0Aacceleration.%20Existing%20learning-based%20caching%2C%20though%20adaptive%2C%20overlooks%20the%0Aimpact%20of%20the%20prior%20timestep.%20It%20also%20suffers%20from%20misaligned%0Aobjectives--aligned%20predicted%20noise%20vs.%20high-quality%20images--between%20training%0Aand%20inference.%20These%20two%20discrepancies%20compromise%20both%20performance%20and%0Aefficiency.%20To%20this%20end%2C%20we%20harmonize%20training%20and%20inference%20with%20a%20novel%0Alearning-based%20caching%20framework%20dubbed%20HarmoniCa.%20It%20first%20incorporates%0AStep-Wise%20Denoising%20Training%20%28SDT%29%20to%20ensure%20the%20continuity%20of%20the%20denoising%0Aprocess%2C%20where%20prior%20steps%20can%20be%20leveraged.%20In%20addition%2C%20an%20Image%20Error%0AProxy-Guided%20Objective%20%28IEPO%29%20is%20applied%20to%20balance%20image%20quality%20against%20cache%0Autilization%20through%20an%20efficient%20proxy%20to%20approximate%20the%20image%20error.%0AExtensive%20experiments%20across%20%248%24%20models%2C%20%244%24%20samplers%2C%20and%20resolutions%20from%0A%24256%5Ctimes256%24%20to%20%242K%24%20demonstrate%20superior%20performance%20and%20speedup%20of%20our%0Aframework.%20For%20instance%2C%20it%20achieves%20over%20%2440%5C%25%24%20latency%20reduction%20%28i.e.%2C%0A%242.07%5Ctimes%24%20theoretical%20speedup%29%20and%20improved%20performance%20on%20PixArt-%24%5Calpha%24.%0ARemarkably%2C%20our%20image-free%20approach%20reduces%20training%20time%20by%20%2425%5C%25%24%20compared%0Awith%20the%20previous%20method.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ModelTC/HarmoniCa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01723v4&entry.124074799=Read"},
{"title": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning", "author": "Katharina Stein and Daniel Fi\u0161er and J\u00f6rg Hoffmann and Alexander Koller", "abstract": "  Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL. Our NL prompts yield better performance than PDDL\nprompts and simple template-based NL prompts. Compared to symbolic planners,\nLLM planning lags far behind; but in some domains, our best LLM configuration\nscales up further than A$^\\star$ using LM-cut.\n", "link": "http://arxiv.org/abs/2311.09830v4", "date": "2025-05-02", "relevancy": 1.9173, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20the%20Generation%20of%20Prompts%20for%20LLM-based%20Action%20Choice%20in%20PDDL%0A%20%20Planning&body=Title%3A%20Automating%20the%20Generation%20of%20Prompts%20for%20LLM-based%20Action%20Choice%20in%20PDDL%0A%20%20Planning%0AAuthor%3A%20Katharina%20Stein%20and%20Daniel%20Fi%C5%A1er%20and%20J%C3%B6rg%20Hoffmann%20and%20Alexander%20Koller%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20a%20large%20variety%20of%20NLP%0Atasks.%20An%20active%20debate%20is%20to%20what%20extent%20they%20can%20do%20reasoning%20and%20planning.%0APrior%20work%20has%20assessed%20the%20latter%20in%20the%20specific%20context%20of%20PDDL%20planning%2C%0Abased%20on%20manually%20converting%20three%20PDDL%20domains%20into%20natural%20language%20%28NL%29%0Aprompts.%20Here%20we%20automate%20this%20conversion%20step%2C%20showing%20how%20to%20leverage%20an%20LLM%0Ato%20automatically%20generate%20NL%20prompts%20from%20PDDL%20input.%20Our%20automatically%0Agenerated%20NL%20prompts%20result%20in%20similar%20LLM-planning%20performance%20as%20the%20previous%0Amanually%20generated%20ones.%20Beyond%20this%2C%20the%20automation%20enables%20us%20to%20run%20much%0Alarger%20experiments%2C%20providing%20for%20the%20first%20time%20a%20broad%20evaluation%20of%20LLM%0Aplanning%20performance%20in%20PDDL.%20Our%20NL%20prompts%20yield%20better%20performance%20than%20PDDL%0Aprompts%20and%20simple%20template-based%20NL%20prompts.%20Compared%20to%20symbolic%20planners%2C%0ALLM%20planning%20lags%20far%20behind%3B%20but%20in%20some%20domains%2C%20our%20best%20LLM%20configuration%0Ascales%20up%20further%20than%20A%24%5E%5Cstar%24%20using%20LM-cut.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09830v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520the%2520Generation%2520of%2520Prompts%2520for%2520LLM-based%2520Action%2520Choice%2520in%2520PDDL%250A%2520%2520Planning%26entry.906535625%3DKatharina%2520Stein%2520and%2520Daniel%2520Fi%25C5%25A1er%2520and%2520J%25C3%25B6rg%2520Hoffmann%2520and%2520Alexander%2520Koller%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520a%2520large%2520variety%2520of%2520NLP%250Atasks.%2520An%2520active%2520debate%2520is%2520to%2520what%2520extent%2520they%2520can%2520do%2520reasoning%2520and%2520planning.%250APrior%2520work%2520has%2520assessed%2520the%2520latter%2520in%2520the%2520specific%2520context%2520of%2520PDDL%2520planning%252C%250Abased%2520on%2520manually%2520converting%2520three%2520PDDL%2520domains%2520into%2520natural%2520language%2520%2528NL%2529%250Aprompts.%2520Here%2520we%2520automate%2520this%2520conversion%2520step%252C%2520showing%2520how%2520to%2520leverage%2520an%2520LLM%250Ato%2520automatically%2520generate%2520NL%2520prompts%2520from%2520PDDL%2520input.%2520Our%2520automatically%250Agenerated%2520NL%2520prompts%2520result%2520in%2520similar%2520LLM-planning%2520performance%2520as%2520the%2520previous%250Amanually%2520generated%2520ones.%2520Beyond%2520this%252C%2520the%2520automation%2520enables%2520us%2520to%2520run%2520much%250Alarger%2520experiments%252C%2520providing%2520for%2520the%2520first%2520time%2520a%2520broad%2520evaluation%2520of%2520LLM%250Aplanning%2520performance%2520in%2520PDDL.%2520Our%2520NL%2520prompts%2520yield%2520better%2520performance%2520than%2520PDDL%250Aprompts%2520and%2520simple%2520template-based%2520NL%2520prompts.%2520Compared%2520to%2520symbolic%2520planners%252C%250ALLM%2520planning%2520lags%2520far%2520behind%253B%2520but%2520in%2520some%2520domains%252C%2520our%2520best%2520LLM%2520configuration%250Ascales%2520up%2520further%2520than%2520A%2524%255E%255Cstar%2524%2520using%2520LM-cut.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09830v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20the%20Generation%20of%20Prompts%20for%20LLM-based%20Action%20Choice%20in%20PDDL%0A%20%20Planning&entry.906535625=Katharina%20Stein%20and%20Daniel%20Fi%C5%A1er%20and%20J%C3%B6rg%20Hoffmann%20and%20Alexander%20Koller&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20a%20large%20variety%20of%20NLP%0Atasks.%20An%20active%20debate%20is%20to%20what%20extent%20they%20can%20do%20reasoning%20and%20planning.%0APrior%20work%20has%20assessed%20the%20latter%20in%20the%20specific%20context%20of%20PDDL%20planning%2C%0Abased%20on%20manually%20converting%20three%20PDDL%20domains%20into%20natural%20language%20%28NL%29%0Aprompts.%20Here%20we%20automate%20this%20conversion%20step%2C%20showing%20how%20to%20leverage%20an%20LLM%0Ato%20automatically%20generate%20NL%20prompts%20from%20PDDL%20input.%20Our%20automatically%0Agenerated%20NL%20prompts%20result%20in%20similar%20LLM-planning%20performance%20as%20the%20previous%0Amanually%20generated%20ones.%20Beyond%20this%2C%20the%20automation%20enables%20us%20to%20run%20much%0Alarger%20experiments%2C%20providing%20for%20the%20first%20time%20a%20broad%20evaluation%20of%20LLM%0Aplanning%20performance%20in%20PDDL.%20Our%20NL%20prompts%20yield%20better%20performance%20than%20PDDL%0Aprompts%20and%20simple%20template-based%20NL%20prompts.%20Compared%20to%20symbolic%20planners%2C%0ALLM%20planning%20lags%20far%20behind%3B%20but%20in%20some%20domains%2C%20our%20best%20LLM%20configuration%0Ascales%20up%20further%20than%20A%24%5E%5Cstar%24%20using%20LM-cut.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09830v4&entry.124074799=Read"},
{"title": "A Physics-preserved Transfer Learning Method for Differential Equations", "author": "Hao-Ran Yang and Chuan-Xian Ren", "abstract": "  While data-driven methods such as neural operator have achieved great success\nin solving differential equations (DEs), they suffer from domain shift problems\ncaused by different learning environments (with data bias or equation changes),\nwhich can be alleviated by transfer learning (TL). However, existing TL methods\nadopted in DEs problems lack either generalizability in general DEs problems or\nphysics preservation during training. In this work, we focus on a general\ntransfer learning method that adaptively correct the domain shift and preserve\nphysical information. Mathematically, we characterize the data domain as\nproduct distribution and the essential problems as distribution bias and\noperator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that\nsimultaneously admits generalizability to common DEs and physics preservation\nof specific problem is proposed to adapt the data-driven model to target domain\nutilizing the push-forward distribution induced by the POTT map. Extensive\nexperiments demonstrate the superior performance, generalizability and physics\npreservation of the proposed POTT method.\n", "link": "http://arxiv.org/abs/2505.01281v1", "date": "2025-05-02", "relevancy": 1.9133, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4935}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4788}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Physics-preserved%20Transfer%20Learning%20Method%20for%20Differential%20Equations&body=Title%3A%20A%20Physics-preserved%20Transfer%20Learning%20Method%20for%20Differential%20Equations%0AAuthor%3A%20Hao-Ran%20Yang%20and%20Chuan-Xian%20Ren%0AAbstract%3A%20%20%20While%20data-driven%20methods%20such%20as%20neural%20operator%20have%20achieved%20great%20success%0Ain%20solving%20differential%20equations%20%28DEs%29%2C%20they%20suffer%20from%20domain%20shift%20problems%0Acaused%20by%20different%20learning%20environments%20%28with%20data%20bias%20or%20equation%20changes%29%2C%0Awhich%20can%20be%20alleviated%20by%20transfer%20learning%20%28TL%29.%20However%2C%20existing%20TL%20methods%0Aadopted%20in%20DEs%20problems%20lack%20either%20generalizability%20in%20general%20DEs%20problems%20or%0Aphysics%20preservation%20during%20training.%20In%20this%20work%2C%20we%20focus%20on%20a%20general%0Atransfer%20learning%20method%20that%20adaptively%20correct%20the%20domain%20shift%20and%20preserve%0Aphysical%20information.%20Mathematically%2C%20we%20characterize%20the%20data%20domain%20as%0Aproduct%20distribution%20and%20the%20essential%20problems%20as%20distribution%20bias%20and%0Aoperator%20bias.%20A%20Physics-preserved%20Optimal%20Tensor%20Transport%20%28POTT%29%20method%20that%0Asimultaneously%20admits%20generalizability%20to%20common%20DEs%20and%20physics%20preservation%0Aof%20specific%20problem%20is%20proposed%20to%20adapt%20the%20data-driven%20model%20to%20target%20domain%0Autilizing%20the%20push-forward%20distribution%20induced%20by%20the%20POTT%20map.%20Extensive%0Aexperiments%20demonstrate%20the%20superior%20performance%2C%20generalizability%20and%20physics%0Apreservation%20of%20the%20proposed%20POTT%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Physics-preserved%2520Transfer%2520Learning%2520Method%2520for%2520Differential%2520Equations%26entry.906535625%3DHao-Ran%2520Yang%2520and%2520Chuan-Xian%2520Ren%26entry.1292438233%3D%2520%2520While%2520data-driven%2520methods%2520such%2520as%2520neural%2520operator%2520have%2520achieved%2520great%2520success%250Ain%2520solving%2520differential%2520equations%2520%2528DEs%2529%252C%2520they%2520suffer%2520from%2520domain%2520shift%2520problems%250Acaused%2520by%2520different%2520learning%2520environments%2520%2528with%2520data%2520bias%2520or%2520equation%2520changes%2529%252C%250Awhich%2520can%2520be%2520alleviated%2520by%2520transfer%2520learning%2520%2528TL%2529.%2520However%252C%2520existing%2520TL%2520methods%250Aadopted%2520in%2520DEs%2520problems%2520lack%2520either%2520generalizability%2520in%2520general%2520DEs%2520problems%2520or%250Aphysics%2520preservation%2520during%2520training.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520a%2520general%250Atransfer%2520learning%2520method%2520that%2520adaptively%2520correct%2520the%2520domain%2520shift%2520and%2520preserve%250Aphysical%2520information.%2520Mathematically%252C%2520we%2520characterize%2520the%2520data%2520domain%2520as%250Aproduct%2520distribution%2520and%2520the%2520essential%2520problems%2520as%2520distribution%2520bias%2520and%250Aoperator%2520bias.%2520A%2520Physics-preserved%2520Optimal%2520Tensor%2520Transport%2520%2528POTT%2529%2520method%2520that%250Asimultaneously%2520admits%2520generalizability%2520to%2520common%2520DEs%2520and%2520physics%2520preservation%250Aof%2520specific%2520problem%2520is%2520proposed%2520to%2520adapt%2520the%2520data-driven%2520model%2520to%2520target%2520domain%250Autilizing%2520the%2520push-forward%2520distribution%2520induced%2520by%2520the%2520POTT%2520map.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520superior%2520performance%252C%2520generalizability%2520and%2520physics%250Apreservation%2520of%2520the%2520proposed%2520POTT%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Physics-preserved%20Transfer%20Learning%20Method%20for%20Differential%20Equations&entry.906535625=Hao-Ran%20Yang%20and%20Chuan-Xian%20Ren&entry.1292438233=%20%20While%20data-driven%20methods%20such%20as%20neural%20operator%20have%20achieved%20great%20success%0Ain%20solving%20differential%20equations%20%28DEs%29%2C%20they%20suffer%20from%20domain%20shift%20problems%0Acaused%20by%20different%20learning%20environments%20%28with%20data%20bias%20or%20equation%20changes%29%2C%0Awhich%20can%20be%20alleviated%20by%20transfer%20learning%20%28TL%29.%20However%2C%20existing%20TL%20methods%0Aadopted%20in%20DEs%20problems%20lack%20either%20generalizability%20in%20general%20DEs%20problems%20or%0Aphysics%20preservation%20during%20training.%20In%20this%20work%2C%20we%20focus%20on%20a%20general%0Atransfer%20learning%20method%20that%20adaptively%20correct%20the%20domain%20shift%20and%20preserve%0Aphysical%20information.%20Mathematically%2C%20we%20characterize%20the%20data%20domain%20as%0Aproduct%20distribution%20and%20the%20essential%20problems%20as%20distribution%20bias%20and%0Aoperator%20bias.%20A%20Physics-preserved%20Optimal%20Tensor%20Transport%20%28POTT%29%20method%20that%0Asimultaneously%20admits%20generalizability%20to%20common%20DEs%20and%20physics%20preservation%0Aof%20specific%20problem%20is%20proposed%20to%20adapt%20the%20data-driven%20model%20to%20target%20domain%0Autilizing%20the%20push-forward%20distribution%20induced%20by%20the%20POTT%20map.%20Extensive%0Aexperiments%20demonstrate%20the%20superior%20performance%2C%20generalizability%20and%20physics%0Apreservation%20of%20the%20proposed%20POTT%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01281v1&entry.124074799=Read"},
{"title": "Randomized Approach to Matrix Completion: Applications in Collaborative\n  Filtering and Image Inpainting", "author": "Antonina Krajewska and Ewa Niewiadomska-Szynkiewicz", "abstract": "  We present a novel method for matrix completion, specifically designed for\nmatrices where one dimension significantly exceeds the other. Our Columns\nSelected Matrix Completion (CSMC) method combines Column Subset Selection and\nLow-Rank Matrix Completion to efficiently reconstruct incomplete datasets. In\neach step, CSMC solves a convex optimization problem. We introduce two\nalgorithms to implement CSMC, each tailored to problems of different sizes. A\nformal analysis is provided, outlining the necessary assumptions and the\nprobability of obtaining a correct solution. To assess the impact of matrix\nsize, rank, and the ratio of missing entries on solution quality and\ncomputation time, we conducted experiments on synthetic data. The method was\nalso applied to two real-world problems: recommendation systems and image\ninpainting. Our results show that CSMC provides solutions of the same quality\nas state-of-the-art matrix completion algorithms based on convex optimization,\nwhile achieving significant reductions in computational runtime.\n", "link": "http://arxiv.org/abs/2403.01919v6", "date": "2025-05-02", "relevancy": 1.912, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4832}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4819}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Randomized%20Approach%20to%20Matrix%20Completion%3A%20Applications%20in%20Collaborative%0A%20%20Filtering%20and%20Image%20Inpainting&body=Title%3A%20Randomized%20Approach%20to%20Matrix%20Completion%3A%20Applications%20in%20Collaborative%0A%20%20Filtering%20and%20Image%20Inpainting%0AAuthor%3A%20Antonina%20Krajewska%20and%20Ewa%20Niewiadomska-Szynkiewicz%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20for%20matrix%20completion%2C%20specifically%20designed%20for%0Amatrices%20where%20one%20dimension%20significantly%20exceeds%20the%20other.%20Our%20Columns%0ASelected%20Matrix%20Completion%20%28CSMC%29%20method%20combines%20Column%20Subset%20Selection%20and%0ALow-Rank%20Matrix%20Completion%20to%20efficiently%20reconstruct%20incomplete%20datasets.%20In%0Aeach%20step%2C%20CSMC%20solves%20a%20convex%20optimization%20problem.%20We%20introduce%20two%0Aalgorithms%20to%20implement%20CSMC%2C%20each%20tailored%20to%20problems%20of%20different%20sizes.%20A%0Aformal%20analysis%20is%20provided%2C%20outlining%20the%20necessary%20assumptions%20and%20the%0Aprobability%20of%20obtaining%20a%20correct%20solution.%20To%20assess%20the%20impact%20of%20matrix%0Asize%2C%20rank%2C%20and%20the%20ratio%20of%20missing%20entries%20on%20solution%20quality%20and%0Acomputation%20time%2C%20we%20conducted%20experiments%20on%20synthetic%20data.%20The%20method%20was%0Aalso%20applied%20to%20two%20real-world%20problems%3A%20recommendation%20systems%20and%20image%0Ainpainting.%20Our%20results%20show%20that%20CSMC%20provides%20solutions%20of%20the%20same%20quality%0Aas%20state-of-the-art%20matrix%20completion%20algorithms%20based%20on%20convex%20optimization%2C%0Awhile%20achieving%20significant%20reductions%20in%20computational%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01919v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandomized%2520Approach%2520to%2520Matrix%2520Completion%253A%2520Applications%2520in%2520Collaborative%250A%2520%2520Filtering%2520and%2520Image%2520Inpainting%26entry.906535625%3DAntonina%2520Krajewska%2520and%2520Ewa%2520Niewiadomska-Szynkiewicz%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520method%2520for%2520matrix%2520completion%252C%2520specifically%2520designed%2520for%250Amatrices%2520where%2520one%2520dimension%2520significantly%2520exceeds%2520the%2520other.%2520Our%2520Columns%250ASelected%2520Matrix%2520Completion%2520%2528CSMC%2529%2520method%2520combines%2520Column%2520Subset%2520Selection%2520and%250ALow-Rank%2520Matrix%2520Completion%2520to%2520efficiently%2520reconstruct%2520incomplete%2520datasets.%2520In%250Aeach%2520step%252C%2520CSMC%2520solves%2520a%2520convex%2520optimization%2520problem.%2520We%2520introduce%2520two%250Aalgorithms%2520to%2520implement%2520CSMC%252C%2520each%2520tailored%2520to%2520problems%2520of%2520different%2520sizes.%2520A%250Aformal%2520analysis%2520is%2520provided%252C%2520outlining%2520the%2520necessary%2520assumptions%2520and%2520the%250Aprobability%2520of%2520obtaining%2520a%2520correct%2520solution.%2520To%2520assess%2520the%2520impact%2520of%2520matrix%250Asize%252C%2520rank%252C%2520and%2520the%2520ratio%2520of%2520missing%2520entries%2520on%2520solution%2520quality%2520and%250Acomputation%2520time%252C%2520we%2520conducted%2520experiments%2520on%2520synthetic%2520data.%2520The%2520method%2520was%250Aalso%2520applied%2520to%2520two%2520real-world%2520problems%253A%2520recommendation%2520systems%2520and%2520image%250Ainpainting.%2520Our%2520results%2520show%2520that%2520CSMC%2520provides%2520solutions%2520of%2520the%2520same%2520quality%250Aas%2520state-of-the-art%2520matrix%2520completion%2520algorithms%2520based%2520on%2520convex%2520optimization%252C%250Awhile%2520achieving%2520significant%2520reductions%2520in%2520computational%2520runtime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01919v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Randomized%20Approach%20to%20Matrix%20Completion%3A%20Applications%20in%20Collaborative%0A%20%20Filtering%20and%20Image%20Inpainting&entry.906535625=Antonina%20Krajewska%20and%20Ewa%20Niewiadomska-Szynkiewicz&entry.1292438233=%20%20We%20present%20a%20novel%20method%20for%20matrix%20completion%2C%20specifically%20designed%20for%0Amatrices%20where%20one%20dimension%20significantly%20exceeds%20the%20other.%20Our%20Columns%0ASelected%20Matrix%20Completion%20%28CSMC%29%20method%20combines%20Column%20Subset%20Selection%20and%0ALow-Rank%20Matrix%20Completion%20to%20efficiently%20reconstruct%20incomplete%20datasets.%20In%0Aeach%20step%2C%20CSMC%20solves%20a%20convex%20optimization%20problem.%20We%20introduce%20two%0Aalgorithms%20to%20implement%20CSMC%2C%20each%20tailored%20to%20problems%20of%20different%20sizes.%20A%0Aformal%20analysis%20is%20provided%2C%20outlining%20the%20necessary%20assumptions%20and%20the%0Aprobability%20of%20obtaining%20a%20correct%20solution.%20To%20assess%20the%20impact%20of%20matrix%0Asize%2C%20rank%2C%20and%20the%20ratio%20of%20missing%20entries%20on%20solution%20quality%20and%0Acomputation%20time%2C%20we%20conducted%20experiments%20on%20synthetic%20data.%20The%20method%20was%0Aalso%20applied%20to%20two%20real-world%20problems%3A%20recommendation%20systems%20and%20image%0Ainpainting.%20Our%20results%20show%20that%20CSMC%20provides%20solutions%20of%20the%20same%20quality%0Aas%20state-of-the-art%20matrix%20completion%20algorithms%20based%20on%20convex%20optimization%2C%0Awhile%20achieving%20significant%20reductions%20in%20computational%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01919v6&entry.124074799=Read"},
{"title": "Deep Kernel Posterior Learning under Infinite Variance Prior Weights", "author": "Jorge Lor\u00eda and Anindya Bhadra", "abstract": "  Neal (1996) proved that infinitely wide shallow Bayesian neural networks\n(BNN) converge to Gaussian processes (GP), when the network weights have\nbounded prior variance. Cho & Saul (2009) provided a useful recursive formula\nfor deep kernel processes for relating the covariance kernel of each layer to\nthe layer immediately below. Moreover, they worked out the form of the\nlayer-wise covariance kernel in an explicit manner for several common\nactivation functions. Recent works, including Aitchison et al. (2021), have\nhighlighted that the covariance kernels obtained in this manner are\ndeterministic and hence, precludes any possibility of representation learning,\nwhich amounts to learning a non-degenerate posterior of a random kernel given\nthe data. To address this, they propose adding artificial noise to the kernel\nto retain stochasticity, and develop deep kernel inverse Wishart processes.\nNonetheless, this artificial noise injection could be critiqued in that it\nwould not naturally emerge in a classic BNN architecture under an\ninfinite-width limit. To address this, we show that a Bayesian deep neural\nnetwork, where each layer width approaches infinity, and all network weights\nare elliptically distributed with infinite variance, converges to a process\nwith $\\alpha$-stable marginals in each layer that has a conditionally Gaussian\nrepresentation. These conditional random covariance kernels could be\nrecursively linked in the manner of Cho & Saul (2009), even though marginally\nthe process exhibits stable behavior, and hence covariances are not even\nnecessarily defined. We also provide useful generalizations of the recent\nresults of Lor\\'ia & Bhadra (2024) on shallow networks to multi-layer networks,\nand remedy the computational burden of their approach. The computational and\nstatistical benefits over competing approaches stand out in simulations and in\ndemonstrations on benchmark data sets.\n", "link": "http://arxiv.org/abs/2410.01284v2", "date": "2025-05-02", "relevancy": 1.9088, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4985}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Kernel%20Posterior%20Learning%20under%20Infinite%20Variance%20Prior%20Weights&body=Title%3A%20Deep%20Kernel%20Posterior%20Learning%20under%20Infinite%20Variance%20Prior%20Weights%0AAuthor%3A%20Jorge%20Lor%C3%ADa%20and%20Anindya%20Bhadra%0AAbstract%3A%20%20%20Neal%20%281996%29%20proved%20that%20infinitely%20wide%20shallow%20Bayesian%20neural%20networks%0A%28BNN%29%20converge%20to%20Gaussian%20processes%20%28GP%29%2C%20when%20the%20network%20weights%20have%0Abounded%20prior%20variance.%20Cho%20%26%20Saul%20%282009%29%20provided%20a%20useful%20recursive%20formula%0Afor%20deep%20kernel%20processes%20for%20relating%20the%20covariance%20kernel%20of%20each%20layer%20to%0Athe%20layer%20immediately%20below.%20Moreover%2C%20they%20worked%20out%20the%20form%20of%20the%0Alayer-wise%20covariance%20kernel%20in%20an%20explicit%20manner%20for%20several%20common%0Aactivation%20functions.%20Recent%20works%2C%20including%20Aitchison%20et%20al.%20%282021%29%2C%20have%0Ahighlighted%20that%20the%20covariance%20kernels%20obtained%20in%20this%20manner%20are%0Adeterministic%20and%20hence%2C%20precludes%20any%20possibility%20of%20representation%20learning%2C%0Awhich%20amounts%20to%20learning%20a%20non-degenerate%20posterior%20of%20a%20random%20kernel%20given%0Athe%20data.%20To%20address%20this%2C%20they%20propose%20adding%20artificial%20noise%20to%20the%20kernel%0Ato%20retain%20stochasticity%2C%20and%20develop%20deep%20kernel%20inverse%20Wishart%20processes.%0ANonetheless%2C%20this%20artificial%20noise%20injection%20could%20be%20critiqued%20in%20that%20it%0Awould%20not%20naturally%20emerge%20in%20a%20classic%20BNN%20architecture%20under%20an%0Ainfinite-width%20limit.%20To%20address%20this%2C%20we%20show%20that%20a%20Bayesian%20deep%20neural%0Anetwork%2C%20where%20each%20layer%20width%20approaches%20infinity%2C%20and%20all%20network%20weights%0Aare%20elliptically%20distributed%20with%20infinite%20variance%2C%20converges%20to%20a%20process%0Awith%20%24%5Calpha%24-stable%20marginals%20in%20each%20layer%20that%20has%20a%20conditionally%20Gaussian%0Arepresentation.%20These%20conditional%20random%20covariance%20kernels%20could%20be%0Arecursively%20linked%20in%20the%20manner%20of%20Cho%20%26%20Saul%20%282009%29%2C%20even%20though%20marginally%0Athe%20process%20exhibits%20stable%20behavior%2C%20and%20hence%20covariances%20are%20not%20even%0Anecessarily%20defined.%20We%20also%20provide%20useful%20generalizations%20of%20the%20recent%0Aresults%20of%20Lor%5C%27ia%20%26%20Bhadra%20%282024%29%20on%20shallow%20networks%20to%20multi-layer%20networks%2C%0Aand%20remedy%20the%20computational%20burden%20of%20their%20approach.%20The%20computational%20and%0Astatistical%20benefits%20over%20competing%20approaches%20stand%20out%20in%20simulations%20and%20in%0Ademonstrations%20on%20benchmark%20data%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01284v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Kernel%2520Posterior%2520Learning%2520under%2520Infinite%2520Variance%2520Prior%2520Weights%26entry.906535625%3DJorge%2520Lor%25C3%25ADa%2520and%2520Anindya%2520Bhadra%26entry.1292438233%3D%2520%2520Neal%2520%25281996%2529%2520proved%2520that%2520infinitely%2520wide%2520shallow%2520Bayesian%2520neural%2520networks%250A%2528BNN%2529%2520converge%2520to%2520Gaussian%2520processes%2520%2528GP%2529%252C%2520when%2520the%2520network%2520weights%2520have%250Abounded%2520prior%2520variance.%2520Cho%2520%2526%2520Saul%2520%25282009%2529%2520provided%2520a%2520useful%2520recursive%2520formula%250Afor%2520deep%2520kernel%2520processes%2520for%2520relating%2520the%2520covariance%2520kernel%2520of%2520each%2520layer%2520to%250Athe%2520layer%2520immediately%2520below.%2520Moreover%252C%2520they%2520worked%2520out%2520the%2520form%2520of%2520the%250Alayer-wise%2520covariance%2520kernel%2520in%2520an%2520explicit%2520manner%2520for%2520several%2520common%250Aactivation%2520functions.%2520Recent%2520works%252C%2520including%2520Aitchison%2520et%2520al.%2520%25282021%2529%252C%2520have%250Ahighlighted%2520that%2520the%2520covariance%2520kernels%2520obtained%2520in%2520this%2520manner%2520are%250Adeterministic%2520and%2520hence%252C%2520precludes%2520any%2520possibility%2520of%2520representation%2520learning%252C%250Awhich%2520amounts%2520to%2520learning%2520a%2520non-degenerate%2520posterior%2520of%2520a%2520random%2520kernel%2520given%250Athe%2520data.%2520To%2520address%2520this%252C%2520they%2520propose%2520adding%2520artificial%2520noise%2520to%2520the%2520kernel%250Ato%2520retain%2520stochasticity%252C%2520and%2520develop%2520deep%2520kernel%2520inverse%2520Wishart%2520processes.%250ANonetheless%252C%2520this%2520artificial%2520noise%2520injection%2520could%2520be%2520critiqued%2520in%2520that%2520it%250Awould%2520not%2520naturally%2520emerge%2520in%2520a%2520classic%2520BNN%2520architecture%2520under%2520an%250Ainfinite-width%2520limit.%2520To%2520address%2520this%252C%2520we%2520show%2520that%2520a%2520Bayesian%2520deep%2520neural%250Anetwork%252C%2520where%2520each%2520layer%2520width%2520approaches%2520infinity%252C%2520and%2520all%2520network%2520weights%250Aare%2520elliptically%2520distributed%2520with%2520infinite%2520variance%252C%2520converges%2520to%2520a%2520process%250Awith%2520%2524%255Calpha%2524-stable%2520marginals%2520in%2520each%2520layer%2520that%2520has%2520a%2520conditionally%2520Gaussian%250Arepresentation.%2520These%2520conditional%2520random%2520covariance%2520kernels%2520could%2520be%250Arecursively%2520linked%2520in%2520the%2520manner%2520of%2520Cho%2520%2526%2520Saul%2520%25282009%2529%252C%2520even%2520though%2520marginally%250Athe%2520process%2520exhibits%2520stable%2520behavior%252C%2520and%2520hence%2520covariances%2520are%2520not%2520even%250Anecessarily%2520defined.%2520We%2520also%2520provide%2520useful%2520generalizations%2520of%2520the%2520recent%250Aresults%2520of%2520Lor%255C%2527ia%2520%2526%2520Bhadra%2520%25282024%2529%2520on%2520shallow%2520networks%2520to%2520multi-layer%2520networks%252C%250Aand%2520remedy%2520the%2520computational%2520burden%2520of%2520their%2520approach.%2520The%2520computational%2520and%250Astatistical%2520benefits%2520over%2520competing%2520approaches%2520stand%2520out%2520in%2520simulations%2520and%2520in%250Ademonstrations%2520on%2520benchmark%2520data%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01284v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Kernel%20Posterior%20Learning%20under%20Infinite%20Variance%20Prior%20Weights&entry.906535625=Jorge%20Lor%C3%ADa%20and%20Anindya%20Bhadra&entry.1292438233=%20%20Neal%20%281996%29%20proved%20that%20infinitely%20wide%20shallow%20Bayesian%20neural%20networks%0A%28BNN%29%20converge%20to%20Gaussian%20processes%20%28GP%29%2C%20when%20the%20network%20weights%20have%0Abounded%20prior%20variance.%20Cho%20%26%20Saul%20%282009%29%20provided%20a%20useful%20recursive%20formula%0Afor%20deep%20kernel%20processes%20for%20relating%20the%20covariance%20kernel%20of%20each%20layer%20to%0Athe%20layer%20immediately%20below.%20Moreover%2C%20they%20worked%20out%20the%20form%20of%20the%0Alayer-wise%20covariance%20kernel%20in%20an%20explicit%20manner%20for%20several%20common%0Aactivation%20functions.%20Recent%20works%2C%20including%20Aitchison%20et%20al.%20%282021%29%2C%20have%0Ahighlighted%20that%20the%20covariance%20kernels%20obtained%20in%20this%20manner%20are%0Adeterministic%20and%20hence%2C%20precludes%20any%20possibility%20of%20representation%20learning%2C%0Awhich%20amounts%20to%20learning%20a%20non-degenerate%20posterior%20of%20a%20random%20kernel%20given%0Athe%20data.%20To%20address%20this%2C%20they%20propose%20adding%20artificial%20noise%20to%20the%20kernel%0Ato%20retain%20stochasticity%2C%20and%20develop%20deep%20kernel%20inverse%20Wishart%20processes.%0ANonetheless%2C%20this%20artificial%20noise%20injection%20could%20be%20critiqued%20in%20that%20it%0Awould%20not%20naturally%20emerge%20in%20a%20classic%20BNN%20architecture%20under%20an%0Ainfinite-width%20limit.%20To%20address%20this%2C%20we%20show%20that%20a%20Bayesian%20deep%20neural%0Anetwork%2C%20where%20each%20layer%20width%20approaches%20infinity%2C%20and%20all%20network%20weights%0Aare%20elliptically%20distributed%20with%20infinite%20variance%2C%20converges%20to%20a%20process%0Awith%20%24%5Calpha%24-stable%20marginals%20in%20each%20layer%20that%20has%20a%20conditionally%20Gaussian%0Arepresentation.%20These%20conditional%20random%20covariance%20kernels%20could%20be%0Arecursively%20linked%20in%20the%20manner%20of%20Cho%20%26%20Saul%20%282009%29%2C%20even%20though%20marginally%0Athe%20process%20exhibits%20stable%20behavior%2C%20and%20hence%20covariances%20are%20not%20even%0Anecessarily%20defined.%20We%20also%20provide%20useful%20generalizations%20of%20the%20recent%0Aresults%20of%20Lor%5C%27ia%20%26%20Bhadra%20%282024%29%20on%20shallow%20networks%20to%20multi-layer%20networks%2C%0Aand%20remedy%20the%20computational%20burden%20of%20their%20approach.%20The%20computational%20and%0Astatistical%20benefits%20over%20competing%20approaches%20stand%20out%20in%20simulations%20and%20in%0Ademonstrations%20on%20benchmark%20data%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01284v2&entry.124074799=Read"},
{"title": "Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating\n  MobileNetV2 and GraphSAGE with Cross-Modal Attention", "author": "Md Abrar Jahin and Soudeep Shahriar and M. F. Mridha and Md. Jakir Hossen and Nilanjan Dey", "abstract": "  Soybean leaf disease detection is critical for agricultural productivity but\nfaces challenges due to visually similar symptoms and limited interpretability\nin conventional methods. While Convolutional Neural Networks (CNNs) excel in\nspatial feature extraction, they often neglect inter-image relational\ndependencies, leading to misclassifications. This paper proposes an\ninterpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that\nsynergizes MobileNetV2 for localized feature extraction and GraphSAGE for\nrelational modeling. The framework constructs a graph where nodes represent\nleaf images, with edges defined by cosine similarity-based adjacency matrices\nand adaptive neighborhood sampling. This design captures fine-grained lesion\nfeatures and global symptom patterns, addressing inter-class similarity\nchallenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM\nvisualizations, generating heatmaps to highlight disease-influential regions.\nEvaluated on a dataset of ten soybean leaf diseases, the model achieves\n$97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional\nmachine learning models ($\\le77.05\\%$). Ablation studies validate the\nsequential architecture's superiority over parallel or single-model\nconfigurations. With only 2.3 million parameters, the lightweight\nMobileNetV2-GraphSAGE combination ensures computational efficiency, enabling\nreal-time deployment in resource-constrained environments. The proposed\napproach bridges the gap between accurate classification and practical\napplicability, offering a robust, interpretable tool for agricultural\ndiagnostics while advancing CNN-GNN integration in plant pathology research.\n", "link": "http://arxiv.org/abs/2503.01284v3", "date": "2025-05-02", "relevancy": 1.8979, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.485}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soybean%20Disease%20Detection%20via%20Interpretable%20Hybrid%20CNN-GNN%3A%20Integrating%0A%20%20MobileNetV2%20and%20GraphSAGE%20with%20Cross-Modal%20Attention&body=Title%3A%20Soybean%20Disease%20Detection%20via%20Interpretable%20Hybrid%20CNN-GNN%3A%20Integrating%0A%20%20MobileNetV2%20and%20GraphSAGE%20with%20Cross-Modal%20Attention%0AAuthor%3A%20Md%20Abrar%20Jahin%20and%20Soudeep%20Shahriar%20and%20M.%20F.%20Mridha%20and%20Md.%20Jakir%20Hossen%20and%20Nilanjan%20Dey%0AAbstract%3A%20%20%20Soybean%20leaf%20disease%20detection%20is%20critical%20for%20agricultural%20productivity%20but%0Afaces%20challenges%20due%20to%20visually%20similar%20symptoms%20and%20limited%20interpretability%0Ain%20conventional%20methods.%20While%20Convolutional%20Neural%20Networks%20%28CNNs%29%20excel%20in%0Aspatial%20feature%20extraction%2C%20they%20often%20neglect%20inter-image%20relational%0Adependencies%2C%20leading%20to%20misclassifications.%20This%20paper%20proposes%20an%0Ainterpretable%20hybrid%20Sequential%20CNN-Graph%20Neural%20Network%20%28GNN%29%20framework%20that%0Asynergizes%20MobileNetV2%20for%20localized%20feature%20extraction%20and%20GraphSAGE%20for%0Arelational%20modeling.%20The%20framework%20constructs%20a%20graph%20where%20nodes%20represent%0Aleaf%20images%2C%20with%20edges%20defined%20by%20cosine%20similarity-based%20adjacency%20matrices%0Aand%20adaptive%20neighborhood%20sampling.%20This%20design%20captures%20fine-grained%20lesion%0Afeatures%20and%20global%20symptom%20patterns%2C%20addressing%20inter-class%20similarity%0Achallenges.%20Cross-modal%20interpretability%20is%20achieved%20via%20Grad-CAM%20and%20Eigen-CAM%0Avisualizations%2C%20generating%20heatmaps%20to%20highlight%20disease-influential%20regions.%0AEvaluated%20on%20a%20dataset%20of%20ten%20soybean%20leaf%20diseases%2C%20the%20model%20achieves%0A%2497.16%5C%25%24%20accuracy%2C%20surpassing%20standalone%20CNNs%20%28%24%5Cle95.04%5C%25%24%29%20and%20traditional%0Amachine%20learning%20models%20%28%24%5Cle77.05%5C%25%24%29.%20Ablation%20studies%20validate%20the%0Asequential%20architecture%27s%20superiority%20over%20parallel%20or%20single-model%0Aconfigurations.%20With%20only%202.3%20million%20parameters%2C%20the%20lightweight%0AMobileNetV2-GraphSAGE%20combination%20ensures%20computational%20efficiency%2C%20enabling%0Areal-time%20deployment%20in%20resource-constrained%20environments.%20The%20proposed%0Aapproach%20bridges%20the%20gap%20between%20accurate%20classification%20and%20practical%0Aapplicability%2C%20offering%20a%20robust%2C%20interpretable%20tool%20for%20agricultural%0Adiagnostics%20while%20advancing%20CNN-GNN%20integration%20in%20plant%20pathology%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01284v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoybean%2520Disease%2520Detection%2520via%2520Interpretable%2520Hybrid%2520CNN-GNN%253A%2520Integrating%250A%2520%2520MobileNetV2%2520and%2520GraphSAGE%2520with%2520Cross-Modal%2520Attention%26entry.906535625%3DMd%2520Abrar%2520Jahin%2520and%2520Soudeep%2520Shahriar%2520and%2520M.%2520F.%2520Mridha%2520and%2520Md.%2520Jakir%2520Hossen%2520and%2520Nilanjan%2520Dey%26entry.1292438233%3D%2520%2520Soybean%2520leaf%2520disease%2520detection%2520is%2520critical%2520for%2520agricultural%2520productivity%2520but%250Afaces%2520challenges%2520due%2520to%2520visually%2520similar%2520symptoms%2520and%2520limited%2520interpretability%250Ain%2520conventional%2520methods.%2520While%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520excel%2520in%250Aspatial%2520feature%2520extraction%252C%2520they%2520often%2520neglect%2520inter-image%2520relational%250Adependencies%252C%2520leading%2520to%2520misclassifications.%2520This%2520paper%2520proposes%2520an%250Ainterpretable%2520hybrid%2520Sequential%2520CNN-Graph%2520Neural%2520Network%2520%2528GNN%2529%2520framework%2520that%250Asynergizes%2520MobileNetV2%2520for%2520localized%2520feature%2520extraction%2520and%2520GraphSAGE%2520for%250Arelational%2520modeling.%2520The%2520framework%2520constructs%2520a%2520graph%2520where%2520nodes%2520represent%250Aleaf%2520images%252C%2520with%2520edges%2520defined%2520by%2520cosine%2520similarity-based%2520adjacency%2520matrices%250Aand%2520adaptive%2520neighborhood%2520sampling.%2520This%2520design%2520captures%2520fine-grained%2520lesion%250Afeatures%2520and%2520global%2520symptom%2520patterns%252C%2520addressing%2520inter-class%2520similarity%250Achallenges.%2520Cross-modal%2520interpretability%2520is%2520achieved%2520via%2520Grad-CAM%2520and%2520Eigen-CAM%250Avisualizations%252C%2520generating%2520heatmaps%2520to%2520highlight%2520disease-influential%2520regions.%250AEvaluated%2520on%2520a%2520dataset%2520of%2520ten%2520soybean%2520leaf%2520diseases%252C%2520the%2520model%2520achieves%250A%252497.16%255C%2525%2524%2520accuracy%252C%2520surpassing%2520standalone%2520CNNs%2520%2528%2524%255Cle95.04%255C%2525%2524%2529%2520and%2520traditional%250Amachine%2520learning%2520models%2520%2528%2524%255Cle77.05%255C%2525%2524%2529.%2520Ablation%2520studies%2520validate%2520the%250Asequential%2520architecture%2527s%2520superiority%2520over%2520parallel%2520or%2520single-model%250Aconfigurations.%2520With%2520only%25202.3%2520million%2520parameters%252C%2520the%2520lightweight%250AMobileNetV2-GraphSAGE%2520combination%2520ensures%2520computational%2520efficiency%252C%2520enabling%250Areal-time%2520deployment%2520in%2520resource-constrained%2520environments.%2520The%2520proposed%250Aapproach%2520bridges%2520the%2520gap%2520between%2520accurate%2520classification%2520and%2520practical%250Aapplicability%252C%2520offering%2520a%2520robust%252C%2520interpretable%2520tool%2520for%2520agricultural%250Adiagnostics%2520while%2520advancing%2520CNN-GNN%2520integration%2520in%2520plant%2520pathology%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01284v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soybean%20Disease%20Detection%20via%20Interpretable%20Hybrid%20CNN-GNN%3A%20Integrating%0A%20%20MobileNetV2%20and%20GraphSAGE%20with%20Cross-Modal%20Attention&entry.906535625=Md%20Abrar%20Jahin%20and%20Soudeep%20Shahriar%20and%20M.%20F.%20Mridha%20and%20Md.%20Jakir%20Hossen%20and%20Nilanjan%20Dey&entry.1292438233=%20%20Soybean%20leaf%20disease%20detection%20is%20critical%20for%20agricultural%20productivity%20but%0Afaces%20challenges%20due%20to%20visually%20similar%20symptoms%20and%20limited%20interpretability%0Ain%20conventional%20methods.%20While%20Convolutional%20Neural%20Networks%20%28CNNs%29%20excel%20in%0Aspatial%20feature%20extraction%2C%20they%20often%20neglect%20inter-image%20relational%0Adependencies%2C%20leading%20to%20misclassifications.%20This%20paper%20proposes%20an%0Ainterpretable%20hybrid%20Sequential%20CNN-Graph%20Neural%20Network%20%28GNN%29%20framework%20that%0Asynergizes%20MobileNetV2%20for%20localized%20feature%20extraction%20and%20GraphSAGE%20for%0Arelational%20modeling.%20The%20framework%20constructs%20a%20graph%20where%20nodes%20represent%0Aleaf%20images%2C%20with%20edges%20defined%20by%20cosine%20similarity-based%20adjacency%20matrices%0Aand%20adaptive%20neighborhood%20sampling.%20This%20design%20captures%20fine-grained%20lesion%0Afeatures%20and%20global%20symptom%20patterns%2C%20addressing%20inter-class%20similarity%0Achallenges.%20Cross-modal%20interpretability%20is%20achieved%20via%20Grad-CAM%20and%20Eigen-CAM%0Avisualizations%2C%20generating%20heatmaps%20to%20highlight%20disease-influential%20regions.%0AEvaluated%20on%20a%20dataset%20of%20ten%20soybean%20leaf%20diseases%2C%20the%20model%20achieves%0A%2497.16%5C%25%24%20accuracy%2C%20surpassing%20standalone%20CNNs%20%28%24%5Cle95.04%5C%25%24%29%20and%20traditional%0Amachine%20learning%20models%20%28%24%5Cle77.05%5C%25%24%29.%20Ablation%20studies%20validate%20the%0Asequential%20architecture%27s%20superiority%20over%20parallel%20or%20single-model%0Aconfigurations.%20With%20only%202.3%20million%20parameters%2C%20the%20lightweight%0AMobileNetV2-GraphSAGE%20combination%20ensures%20computational%20efficiency%2C%20enabling%0Areal-time%20deployment%20in%20resource-constrained%20environments.%20The%20proposed%0Aapproach%20bridges%20the%20gap%20between%20accurate%20classification%20and%20practical%0Aapplicability%2C%20offering%20a%20robust%2C%20interpretable%20tool%20for%20agricultural%0Adiagnostics%20while%20advancing%20CNN-GNN%20integration%20in%20plant%20pathology%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01284v3&entry.124074799=Read"},
{"title": "Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for\n  Adversarial Transferability", "author": "Zhaoyang Ma and Zhihao Wu and Wang Lu and Xin Gao and Jinghang Yue and Taolin Zhang and Lipo Wang and Youfang Lin and Jing Wang", "abstract": "  The development of model ensemble attacks has significantly improved the\ntransferability of adversarial examples, but this progress also poses severe\nthreats to the security of deep neural networks. Existing methods, however,\nface two critical challenges: insufficient capture of shared gradient\ndirections across models and a lack of adaptive weight allocation mechanisms.\nTo address these issues, we propose a novel method Harmonized Ensemble for\nAdversarial Transferability (HEAT), which introduces domain generalization into\nadversarial example generation for the first time. HEAT consists of two key\nmodules: Consensus Gradient Direction Synthesizer, which uses Singular Value\nDecomposition to synthesize shared gradient directions; and Dual-Harmony Weight\nOrchestrator which dynamically balances intra-domain coherence, stabilizing\ngradients within individual models, and inter-domain diversity, enhancing\ntransferability across models. Experimental results demonstrate that HEAT\nsignificantly outperforms existing methods across various datasets and\nsettings, offering a new perspective and direction for adversarial attack\nresearch.\n", "link": "http://arxiv.org/abs/2505.01168v1", "date": "2025-05-02", "relevancy": 1.8894, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4747}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4714}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmonizing%20Intra-coherence%20and%20Inter-divergence%20in%20Ensemble%20Attacks%20for%0A%20%20Adversarial%20Transferability&body=Title%3A%20Harmonizing%20Intra-coherence%20and%20Inter-divergence%20in%20Ensemble%20Attacks%20for%0A%20%20Adversarial%20Transferability%0AAuthor%3A%20Zhaoyang%20Ma%20and%20Zhihao%20Wu%20and%20Wang%20Lu%20and%20Xin%20Gao%20and%20Jinghang%20Yue%20and%20Taolin%20Zhang%20and%20Lipo%20Wang%20and%20Youfang%20Lin%20and%20Jing%20Wang%0AAbstract%3A%20%20%20The%20development%20of%20model%20ensemble%20attacks%20has%20significantly%20improved%20the%0Atransferability%20of%20adversarial%20examples%2C%20but%20this%20progress%20also%20poses%20severe%0Athreats%20to%20the%20security%20of%20deep%20neural%20networks.%20Existing%20methods%2C%20however%2C%0Aface%20two%20critical%20challenges%3A%20insufficient%20capture%20of%20shared%20gradient%0Adirections%20across%20models%20and%20a%20lack%20of%20adaptive%20weight%20allocation%20mechanisms.%0ATo%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%20Harmonized%20Ensemble%20for%0AAdversarial%20Transferability%20%28HEAT%29%2C%20which%20introduces%20domain%20generalization%20into%0Aadversarial%20example%20generation%20for%20the%20first%20time.%20HEAT%20consists%20of%20two%20key%0Amodules%3A%20Consensus%20Gradient%20Direction%20Synthesizer%2C%20which%20uses%20Singular%20Value%0ADecomposition%20to%20synthesize%20shared%20gradient%20directions%3B%20and%20Dual-Harmony%20Weight%0AOrchestrator%20which%20dynamically%20balances%20intra-domain%20coherence%2C%20stabilizing%0Agradients%20within%20individual%20models%2C%20and%20inter-domain%20diversity%2C%20enhancing%0Atransferability%20across%20models.%20Experimental%20results%20demonstrate%20that%20HEAT%0Asignificantly%20outperforms%20existing%20methods%20across%20various%20datasets%20and%0Asettings%2C%20offering%20a%20new%20perspective%20and%20direction%20for%20adversarial%20attack%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonizing%2520Intra-coherence%2520and%2520Inter-divergence%2520in%2520Ensemble%2520Attacks%2520for%250A%2520%2520Adversarial%2520Transferability%26entry.906535625%3DZhaoyang%2520Ma%2520and%2520Zhihao%2520Wu%2520and%2520Wang%2520Lu%2520and%2520Xin%2520Gao%2520and%2520Jinghang%2520Yue%2520and%2520Taolin%2520Zhang%2520and%2520Lipo%2520Wang%2520and%2520Youfang%2520Lin%2520and%2520Jing%2520Wang%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520model%2520ensemble%2520attacks%2520has%2520significantly%2520improved%2520the%250Atransferability%2520of%2520adversarial%2520examples%252C%2520but%2520this%2520progress%2520also%2520poses%2520severe%250Athreats%2520to%2520the%2520security%2520of%2520deep%2520neural%2520networks.%2520Existing%2520methods%252C%2520however%252C%250Aface%2520two%2520critical%2520challenges%253A%2520insufficient%2520capture%2520of%2520shared%2520gradient%250Adirections%2520across%2520models%2520and%2520a%2520lack%2520of%2520adaptive%2520weight%2520allocation%2520mechanisms.%250ATo%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520method%2520Harmonized%2520Ensemble%2520for%250AAdversarial%2520Transferability%2520%2528HEAT%2529%252C%2520which%2520introduces%2520domain%2520generalization%2520into%250Aadversarial%2520example%2520generation%2520for%2520the%2520first%2520time.%2520HEAT%2520consists%2520of%2520two%2520key%250Amodules%253A%2520Consensus%2520Gradient%2520Direction%2520Synthesizer%252C%2520which%2520uses%2520Singular%2520Value%250ADecomposition%2520to%2520synthesize%2520shared%2520gradient%2520directions%253B%2520and%2520Dual-Harmony%2520Weight%250AOrchestrator%2520which%2520dynamically%2520balances%2520intra-domain%2520coherence%252C%2520stabilizing%250Agradients%2520within%2520individual%2520models%252C%2520and%2520inter-domain%2520diversity%252C%2520enhancing%250Atransferability%2520across%2520models.%2520Experimental%2520results%2520demonstrate%2520that%2520HEAT%250Asignificantly%2520outperforms%2520existing%2520methods%2520across%2520various%2520datasets%2520and%250Asettings%252C%2520offering%2520a%2520new%2520perspective%2520and%2520direction%2520for%2520adversarial%2520attack%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonizing%20Intra-coherence%20and%20Inter-divergence%20in%20Ensemble%20Attacks%20for%0A%20%20Adversarial%20Transferability&entry.906535625=Zhaoyang%20Ma%20and%20Zhihao%20Wu%20and%20Wang%20Lu%20and%20Xin%20Gao%20and%20Jinghang%20Yue%20and%20Taolin%20Zhang%20and%20Lipo%20Wang%20and%20Youfang%20Lin%20and%20Jing%20Wang&entry.1292438233=%20%20The%20development%20of%20model%20ensemble%20attacks%20has%20significantly%20improved%20the%0Atransferability%20of%20adversarial%20examples%2C%20but%20this%20progress%20also%20poses%20severe%0Athreats%20to%20the%20security%20of%20deep%20neural%20networks.%20Existing%20methods%2C%20however%2C%0Aface%20two%20critical%20challenges%3A%20insufficient%20capture%20of%20shared%20gradient%0Adirections%20across%20models%20and%20a%20lack%20of%20adaptive%20weight%20allocation%20mechanisms.%0ATo%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%20Harmonized%20Ensemble%20for%0AAdversarial%20Transferability%20%28HEAT%29%2C%20which%20introduces%20domain%20generalization%20into%0Aadversarial%20example%20generation%20for%20the%20first%20time.%20HEAT%20consists%20of%20two%20key%0Amodules%3A%20Consensus%20Gradient%20Direction%20Synthesizer%2C%20which%20uses%20Singular%20Value%0ADecomposition%20to%20synthesize%20shared%20gradient%20directions%3B%20and%20Dual-Harmony%20Weight%0AOrchestrator%20which%20dynamically%20balances%20intra-domain%20coherence%2C%20stabilizing%0Agradients%20within%20individual%20models%2C%20and%20inter-domain%20diversity%2C%20enhancing%0Atransferability%20across%20models.%20Experimental%20results%20demonstrate%20that%20HEAT%0Asignificantly%20outperforms%20existing%20methods%20across%20various%20datasets%20and%0Asettings%2C%20offering%20a%20new%20perspective%20and%20direction%20for%20adversarial%20attack%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01168v1&entry.124074799=Read"},
{"title": "Reduced-order structure-property linkages for stochastic metamaterials", "author": "Hooman Danesh and Maruthi Annamaraju and Tim Brepols and Stefanie Reese and Surya R. Kalidindi", "abstract": "  The capabilities of additive manufacturing have facilitated the design and\nproduction of mechanical metamaterials with diverse unit cell geometries.\nEstablishing linkages between the vast design space of unit cells and their\neffective mechanical properties is critical for the efficient design and\nperformance evaluation of such metamaterials. However, physics-based\nsimulations of metamaterial unit cells across the entire design space are\ncomputationally expensive, necessitating a materials informatics framework to\nefficiently capture complex structure-property relationships. In this work,\nprincipal component analysis of 2-point correlation functions is performed to\nextract the salient features from a large dataset of randomly generated 2D\nmetamaterials. Physics-based simulations are performed using a fast Fourier\ntransform (FFT)-based homogenization approach to efficiently compute the\nhomogenized effective elastic stiffness across the extensive unit cell designs.\nSubsequently, Gaussian process regression is used to generate reduced-order\nsurrogates, mapping unit cell designs to their homogenized effective elastic\nconstant. It is demonstrated that the adopted workflow enables a high-value\nlow-dimensional representation of the voluminous stochastic metamaterial\ndataset, facilitating the construction of robust structure-property maps.\nFinally, an uncertainty-based active learning framework is utilized to train a\nsurrogate model with a significantly smaller number of data points compared to\nthe original full dataset. It is shown that a dataset as small as $0.61\\%$ of\nthe entire dataset is sufficient to generate accurate and robust\nstructure-property maps.\n", "link": "http://arxiv.org/abs/2505.01283v1", "date": "2025-05-02", "relevancy": 1.8849, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4981}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4819}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reduced-order%20structure-property%20linkages%20for%20stochastic%20metamaterials&body=Title%3A%20Reduced-order%20structure-property%20linkages%20for%20stochastic%20metamaterials%0AAuthor%3A%20Hooman%20Danesh%20and%20Maruthi%20Annamaraju%20and%20Tim%20Brepols%20and%20Stefanie%20Reese%20and%20Surya%20R.%20Kalidindi%0AAbstract%3A%20%20%20The%20capabilities%20of%20additive%20manufacturing%20have%20facilitated%20the%20design%20and%0Aproduction%20of%20mechanical%20metamaterials%20with%20diverse%20unit%20cell%20geometries.%0AEstablishing%20linkages%20between%20the%20vast%20design%20space%20of%20unit%20cells%20and%20their%0Aeffective%20mechanical%20properties%20is%20critical%20for%20the%20efficient%20design%20and%0Aperformance%20evaluation%20of%20such%20metamaterials.%20However%2C%20physics-based%0Asimulations%20of%20metamaterial%20unit%20cells%20across%20the%20entire%20design%20space%20are%0Acomputationally%20expensive%2C%20necessitating%20a%20materials%20informatics%20framework%20to%0Aefficiently%20capture%20complex%20structure-property%20relationships.%20In%20this%20work%2C%0Aprincipal%20component%20analysis%20of%202-point%20correlation%20functions%20is%20performed%20to%0Aextract%20the%20salient%20features%20from%20a%20large%20dataset%20of%20randomly%20generated%202D%0Ametamaterials.%20Physics-based%20simulations%20are%20performed%20using%20a%20fast%20Fourier%0Atransform%20%28FFT%29-based%20homogenization%20approach%20to%20efficiently%20compute%20the%0Ahomogenized%20effective%20elastic%20stiffness%20across%20the%20extensive%20unit%20cell%20designs.%0ASubsequently%2C%20Gaussian%20process%20regression%20is%20used%20to%20generate%20reduced-order%0Asurrogates%2C%20mapping%20unit%20cell%20designs%20to%20their%20homogenized%20effective%20elastic%0Aconstant.%20It%20is%20demonstrated%20that%20the%20adopted%20workflow%20enables%20a%20high-value%0Alow-dimensional%20representation%20of%20the%20voluminous%20stochastic%20metamaterial%0Adataset%2C%20facilitating%20the%20construction%20of%20robust%20structure-property%20maps.%0AFinally%2C%20an%20uncertainty-based%20active%20learning%20framework%20is%20utilized%20to%20train%20a%0Asurrogate%20model%20with%20a%20significantly%20smaller%20number%20of%20data%20points%20compared%20to%0Athe%20original%20full%20dataset.%20It%20is%20shown%20that%20a%20dataset%20as%20small%20as%20%240.61%5C%25%24%20of%0Athe%20entire%20dataset%20is%20sufficient%20to%20generate%20accurate%20and%20robust%0Astructure-property%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReduced-order%2520structure-property%2520linkages%2520for%2520stochastic%2520metamaterials%26entry.906535625%3DHooman%2520Danesh%2520and%2520Maruthi%2520Annamaraju%2520and%2520Tim%2520Brepols%2520and%2520Stefanie%2520Reese%2520and%2520Surya%2520R.%2520Kalidindi%26entry.1292438233%3D%2520%2520The%2520capabilities%2520of%2520additive%2520manufacturing%2520have%2520facilitated%2520the%2520design%2520and%250Aproduction%2520of%2520mechanical%2520metamaterials%2520with%2520diverse%2520unit%2520cell%2520geometries.%250AEstablishing%2520linkages%2520between%2520the%2520vast%2520design%2520space%2520of%2520unit%2520cells%2520and%2520their%250Aeffective%2520mechanical%2520properties%2520is%2520critical%2520for%2520the%2520efficient%2520design%2520and%250Aperformance%2520evaluation%2520of%2520such%2520metamaterials.%2520However%252C%2520physics-based%250Asimulations%2520of%2520metamaterial%2520unit%2520cells%2520across%2520the%2520entire%2520design%2520space%2520are%250Acomputationally%2520expensive%252C%2520necessitating%2520a%2520materials%2520informatics%2520framework%2520to%250Aefficiently%2520capture%2520complex%2520structure-property%2520relationships.%2520In%2520this%2520work%252C%250Aprincipal%2520component%2520analysis%2520of%25202-point%2520correlation%2520functions%2520is%2520performed%2520to%250Aextract%2520the%2520salient%2520features%2520from%2520a%2520large%2520dataset%2520of%2520randomly%2520generated%25202D%250Ametamaterials.%2520Physics-based%2520simulations%2520are%2520performed%2520using%2520a%2520fast%2520Fourier%250Atransform%2520%2528FFT%2529-based%2520homogenization%2520approach%2520to%2520efficiently%2520compute%2520the%250Ahomogenized%2520effective%2520elastic%2520stiffness%2520across%2520the%2520extensive%2520unit%2520cell%2520designs.%250ASubsequently%252C%2520Gaussian%2520process%2520regression%2520is%2520used%2520to%2520generate%2520reduced-order%250Asurrogates%252C%2520mapping%2520unit%2520cell%2520designs%2520to%2520their%2520homogenized%2520effective%2520elastic%250Aconstant.%2520It%2520is%2520demonstrated%2520that%2520the%2520adopted%2520workflow%2520enables%2520a%2520high-value%250Alow-dimensional%2520representation%2520of%2520the%2520voluminous%2520stochastic%2520metamaterial%250Adataset%252C%2520facilitating%2520the%2520construction%2520of%2520robust%2520structure-property%2520maps.%250AFinally%252C%2520an%2520uncertainty-based%2520active%2520learning%2520framework%2520is%2520utilized%2520to%2520train%2520a%250Asurrogate%2520model%2520with%2520a%2520significantly%2520smaller%2520number%2520of%2520data%2520points%2520compared%2520to%250Athe%2520original%2520full%2520dataset.%2520It%2520is%2520shown%2520that%2520a%2520dataset%2520as%2520small%2520as%2520%25240.61%255C%2525%2524%2520of%250Athe%2520entire%2520dataset%2520is%2520sufficient%2520to%2520generate%2520accurate%2520and%2520robust%250Astructure-property%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reduced-order%20structure-property%20linkages%20for%20stochastic%20metamaterials&entry.906535625=Hooman%20Danesh%20and%20Maruthi%20Annamaraju%20and%20Tim%20Brepols%20and%20Stefanie%20Reese%20and%20Surya%20R.%20Kalidindi&entry.1292438233=%20%20The%20capabilities%20of%20additive%20manufacturing%20have%20facilitated%20the%20design%20and%0Aproduction%20of%20mechanical%20metamaterials%20with%20diverse%20unit%20cell%20geometries.%0AEstablishing%20linkages%20between%20the%20vast%20design%20space%20of%20unit%20cells%20and%20their%0Aeffective%20mechanical%20properties%20is%20critical%20for%20the%20efficient%20design%20and%0Aperformance%20evaluation%20of%20such%20metamaterials.%20However%2C%20physics-based%0Asimulations%20of%20metamaterial%20unit%20cells%20across%20the%20entire%20design%20space%20are%0Acomputationally%20expensive%2C%20necessitating%20a%20materials%20informatics%20framework%20to%0Aefficiently%20capture%20complex%20structure-property%20relationships.%20In%20this%20work%2C%0Aprincipal%20component%20analysis%20of%202-point%20correlation%20functions%20is%20performed%20to%0Aextract%20the%20salient%20features%20from%20a%20large%20dataset%20of%20randomly%20generated%202D%0Ametamaterials.%20Physics-based%20simulations%20are%20performed%20using%20a%20fast%20Fourier%0Atransform%20%28FFT%29-based%20homogenization%20approach%20to%20efficiently%20compute%20the%0Ahomogenized%20effective%20elastic%20stiffness%20across%20the%20extensive%20unit%20cell%20designs.%0ASubsequently%2C%20Gaussian%20process%20regression%20is%20used%20to%20generate%20reduced-order%0Asurrogates%2C%20mapping%20unit%20cell%20designs%20to%20their%20homogenized%20effective%20elastic%0Aconstant.%20It%20is%20demonstrated%20that%20the%20adopted%20workflow%20enables%20a%20high-value%0Alow-dimensional%20representation%20of%20the%20voluminous%20stochastic%20metamaterial%0Adataset%2C%20facilitating%20the%20construction%20of%20robust%20structure-property%20maps.%0AFinally%2C%20an%20uncertainty-based%20active%20learning%20framework%20is%20utilized%20to%20train%20a%0Asurrogate%20model%20with%20a%20significantly%20smaller%20number%20of%20data%20points%20compared%20to%0Athe%20original%20full%20dataset.%20It%20is%20shown%20that%20a%20dataset%20as%20small%20as%20%240.61%5C%25%24%20of%0Athe%20entire%20dataset%20is%20sufficient%20to%20generate%20accurate%20and%20robust%0Astructure-property%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01283v1&entry.124074799=Read"},
{"title": "TSTMotion: Training-free Scene-awarenText-to-motion Generation", "author": "Ziyan Guo and Haoxuan Qu and Hossein Rahmani and Dewen Soh and Ping Hu and Qiuhong Ke and Jun Liu", "abstract": "  Text-to-motion generation has recently garnered significant research\ninterest, primarily focusing on generating human motion sequences in blank\nbackgrounds. However, human motions commonly occur within diverse 3D scenes,\nwhich has prompted exploration into scene-aware text-to-motion generation\nmethods. Yet, existing scene-aware methods often rely on large-scale\nground-truth motion sequences in diverse 3D scenes, which poses practical\nchallenges due to the expensive cost. To mitigate this challenge, we are the\nfirst to propose a \\textbf{T}raining-free \\textbf{S}cene-aware\n\\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that\nefficiently empowers pre-trained blank-background motion generators with the\nscene-aware capability. Specifically, conditioned on the given 3D scene and\ntext description, we adopt foundation models together to reason, predict and\nvalidate a scene-aware motion guidance. Then, the motion guidance is\nincorporated into the blank-background motion generators with two\nmodifications, resulting in scene-aware text-driven motion sequences. Extensive\nexperiments demonstrate the efficacy and generalizability of our proposed\nframework. We release our code in \\href{https://tstmotion.github.io/}{Project\nPage}.\n", "link": "http://arxiv.org/abs/2505.01182v1", "date": "2025-05-02", "relevancy": 1.8806, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6401}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6386}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSTMotion%3A%20Training-free%20Scene-awarenText-to-motion%20Generation&body=Title%3A%20TSTMotion%3A%20Training-free%20Scene-awarenText-to-motion%20Generation%0AAuthor%3A%20Ziyan%20Guo%20and%20Haoxuan%20Qu%20and%20Hossein%20Rahmani%20and%20Dewen%20Soh%20and%20Ping%20Hu%20and%20Qiuhong%20Ke%20and%20Jun%20Liu%0AAbstract%3A%20%20%20Text-to-motion%20generation%20has%20recently%20garnered%20significant%20research%0Ainterest%2C%20primarily%20focusing%20on%20generating%20human%20motion%20sequences%20in%20blank%0Abackgrounds.%20However%2C%20human%20motions%20commonly%20occur%20within%20diverse%203D%20scenes%2C%0Awhich%20has%20prompted%20exploration%20into%20scene-aware%20text-to-motion%20generation%0Amethods.%20Yet%2C%20existing%20scene-aware%20methods%20often%20rely%20on%20large-scale%0Aground-truth%20motion%20sequences%20in%20diverse%203D%20scenes%2C%20which%20poses%20practical%0Achallenges%20due%20to%20the%20expensive%20cost.%20To%20mitigate%20this%20challenge%2C%20we%20are%20the%0Afirst%20to%20propose%20a%20%5Ctextbf%7BT%7Draining-free%20%5Ctextbf%7BS%7Dcene-aware%0A%5Ctextbf%7BT%7Dext-to-%5Ctextbf%7BMotion%7D%20framework%2C%20dubbed%20as%20%5Ctextbf%7BTSTMotion%7D%2C%20that%0Aefficiently%20empowers%20pre-trained%20blank-background%20motion%20generators%20with%20the%0Ascene-aware%20capability.%20Specifically%2C%20conditioned%20on%20the%20given%203D%20scene%20and%0Atext%20description%2C%20we%20adopt%20foundation%20models%20together%20to%20reason%2C%20predict%20and%0Avalidate%20a%20scene-aware%20motion%20guidance.%20Then%2C%20the%20motion%20guidance%20is%0Aincorporated%20into%20the%20blank-background%20motion%20generators%20with%20two%0Amodifications%2C%20resulting%20in%20scene-aware%20text-driven%20motion%20sequences.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20and%20generalizability%20of%20our%20proposed%0Aframework.%20We%20release%20our%20code%20in%20%5Chref%7Bhttps%3A//tstmotion.github.io/%7D%7BProject%0APage%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSTMotion%253A%2520Training-free%2520Scene-awarenText-to-motion%2520Generation%26entry.906535625%3DZiyan%2520Guo%2520and%2520Haoxuan%2520Qu%2520and%2520Hossein%2520Rahmani%2520and%2520Dewen%2520Soh%2520and%2520Ping%2520Hu%2520and%2520Qiuhong%2520Ke%2520and%2520Jun%2520Liu%26entry.1292438233%3D%2520%2520Text-to-motion%2520generation%2520has%2520recently%2520garnered%2520significant%2520research%250Ainterest%252C%2520primarily%2520focusing%2520on%2520generating%2520human%2520motion%2520sequences%2520in%2520blank%250Abackgrounds.%2520However%252C%2520human%2520motions%2520commonly%2520occur%2520within%2520diverse%25203D%2520scenes%252C%250Awhich%2520has%2520prompted%2520exploration%2520into%2520scene-aware%2520text-to-motion%2520generation%250Amethods.%2520Yet%252C%2520existing%2520scene-aware%2520methods%2520often%2520rely%2520on%2520large-scale%250Aground-truth%2520motion%2520sequences%2520in%2520diverse%25203D%2520scenes%252C%2520which%2520poses%2520practical%250Achallenges%2520due%2520to%2520the%2520expensive%2520cost.%2520To%2520mitigate%2520this%2520challenge%252C%2520we%2520are%2520the%250Afirst%2520to%2520propose%2520a%2520%255Ctextbf%257BT%257Draining-free%2520%255Ctextbf%257BS%257Dcene-aware%250A%255Ctextbf%257BT%257Dext-to-%255Ctextbf%257BMotion%257D%2520framework%252C%2520dubbed%2520as%2520%255Ctextbf%257BTSTMotion%257D%252C%2520that%250Aefficiently%2520empowers%2520pre-trained%2520blank-background%2520motion%2520generators%2520with%2520the%250Ascene-aware%2520capability.%2520Specifically%252C%2520conditioned%2520on%2520the%2520given%25203D%2520scene%2520and%250Atext%2520description%252C%2520we%2520adopt%2520foundation%2520models%2520together%2520to%2520reason%252C%2520predict%2520and%250Avalidate%2520a%2520scene-aware%2520motion%2520guidance.%2520Then%252C%2520the%2520motion%2520guidance%2520is%250Aincorporated%2520into%2520the%2520blank-background%2520motion%2520generators%2520with%2520two%250Amodifications%252C%2520resulting%2520in%2520scene-aware%2520text-driven%2520motion%2520sequences.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520efficacy%2520and%2520generalizability%2520of%2520our%2520proposed%250Aframework.%2520We%2520release%2520our%2520code%2520in%2520%255Chref%257Bhttps%253A//tstmotion.github.io/%257D%257BProject%250APage%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSTMotion%3A%20Training-free%20Scene-awarenText-to-motion%20Generation&entry.906535625=Ziyan%20Guo%20and%20Haoxuan%20Qu%20and%20Hossein%20Rahmani%20and%20Dewen%20Soh%20and%20Ping%20Hu%20and%20Qiuhong%20Ke%20and%20Jun%20Liu&entry.1292438233=%20%20Text-to-motion%20generation%20has%20recently%20garnered%20significant%20research%0Ainterest%2C%20primarily%20focusing%20on%20generating%20human%20motion%20sequences%20in%20blank%0Abackgrounds.%20However%2C%20human%20motions%20commonly%20occur%20within%20diverse%203D%20scenes%2C%0Awhich%20has%20prompted%20exploration%20into%20scene-aware%20text-to-motion%20generation%0Amethods.%20Yet%2C%20existing%20scene-aware%20methods%20often%20rely%20on%20large-scale%0Aground-truth%20motion%20sequences%20in%20diverse%203D%20scenes%2C%20which%20poses%20practical%0Achallenges%20due%20to%20the%20expensive%20cost.%20To%20mitigate%20this%20challenge%2C%20we%20are%20the%0Afirst%20to%20propose%20a%20%5Ctextbf%7BT%7Draining-free%20%5Ctextbf%7BS%7Dcene-aware%0A%5Ctextbf%7BT%7Dext-to-%5Ctextbf%7BMotion%7D%20framework%2C%20dubbed%20as%20%5Ctextbf%7BTSTMotion%7D%2C%20that%0Aefficiently%20empowers%20pre-trained%20blank-background%20motion%20generators%20with%20the%0Ascene-aware%20capability.%20Specifically%2C%20conditioned%20on%20the%20given%203D%20scene%20and%0Atext%20description%2C%20we%20adopt%20foundation%20models%20together%20to%20reason%2C%20predict%20and%0Avalidate%20a%20scene-aware%20motion%20guidance.%20Then%2C%20the%20motion%20guidance%20is%0Aincorporated%20into%20the%20blank-background%20motion%20generators%20with%20two%0Amodifications%2C%20resulting%20in%20scene-aware%20text-driven%20motion%20sequences.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20and%20generalizability%20of%20our%20proposed%0Aframework.%20We%20release%20our%20code%20in%20%5Chref%7Bhttps%3A//tstmotion.github.io/%7D%7BProject%0APage%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01182v1&entry.124074799=Read"},
{"title": "On Expressive Power of Looped Transformers: Theoretical Analysis and\n  Enhancement via Timestep Encoding", "author": "Kevin Xu and Issei Sato", "abstract": "  Looped Transformers provide advantages in parameter efficiency, computational\ncapabilities, and generalization for reasoning tasks. However, their expressive\npower regarding function approximation remains underexplored. In this paper, we\nestablish the approximation rate of Looped Transformers by defining the modulus\nof continuity for sequence-to-sequence functions. This reveals a limitation\nspecific to the looped architecture. That is, the analysis prompts the\nincorporation of scaling parameters for each loop, conditioned on timestep\nencoding. Experiments validate the theoretical results, showing that increasing\nthe number of loops enhances performance, with further gains achieved through\nthe timestep encoding.\n", "link": "http://arxiv.org/abs/2410.01405v6", "date": "2025-05-02", "relevancy": 1.8788, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5378}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4617}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Expressive%20Power%20of%20Looped%20Transformers%3A%20Theoretical%20Analysis%20and%0A%20%20Enhancement%20via%20Timestep%20Encoding&body=Title%3A%20On%20Expressive%20Power%20of%20Looped%20Transformers%3A%20Theoretical%20Analysis%20and%0A%20%20Enhancement%20via%20Timestep%20Encoding%0AAuthor%3A%20Kevin%20Xu%20and%20Issei%20Sato%0AAbstract%3A%20%20%20Looped%20Transformers%20provide%20advantages%20in%20parameter%20efficiency%2C%20computational%0Acapabilities%2C%20and%20generalization%20for%20reasoning%20tasks.%20However%2C%20their%20expressive%0Apower%20regarding%20function%20approximation%20remains%20underexplored.%20In%20this%20paper%2C%20we%0Aestablish%20the%20approximation%20rate%20of%20Looped%20Transformers%20by%20defining%20the%20modulus%0Aof%20continuity%20for%20sequence-to-sequence%20functions.%20This%20reveals%20a%20limitation%0Aspecific%20to%20the%20looped%20architecture.%20That%20is%2C%20the%20analysis%20prompts%20the%0Aincorporation%20of%20scaling%20parameters%20for%20each%20loop%2C%20conditioned%20on%20timestep%0Aencoding.%20Experiments%20validate%20the%20theoretical%20results%2C%20showing%20that%20increasing%0Athe%20number%20of%20loops%20enhances%20performance%2C%20with%20further%20gains%20achieved%20through%0Athe%20timestep%20encoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01405v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Expressive%2520Power%2520of%2520Looped%2520Transformers%253A%2520Theoretical%2520Analysis%2520and%250A%2520%2520Enhancement%2520via%2520Timestep%2520Encoding%26entry.906535625%3DKevin%2520Xu%2520and%2520Issei%2520Sato%26entry.1292438233%3D%2520%2520Looped%2520Transformers%2520provide%2520advantages%2520in%2520parameter%2520efficiency%252C%2520computational%250Acapabilities%252C%2520and%2520generalization%2520for%2520reasoning%2520tasks.%2520However%252C%2520their%2520expressive%250Apower%2520regarding%2520function%2520approximation%2520remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%250Aestablish%2520the%2520approximation%2520rate%2520of%2520Looped%2520Transformers%2520by%2520defining%2520the%2520modulus%250Aof%2520continuity%2520for%2520sequence-to-sequence%2520functions.%2520This%2520reveals%2520a%2520limitation%250Aspecific%2520to%2520the%2520looped%2520architecture.%2520That%2520is%252C%2520the%2520analysis%2520prompts%2520the%250Aincorporation%2520of%2520scaling%2520parameters%2520for%2520each%2520loop%252C%2520conditioned%2520on%2520timestep%250Aencoding.%2520Experiments%2520validate%2520the%2520theoretical%2520results%252C%2520showing%2520that%2520increasing%250Athe%2520number%2520of%2520loops%2520enhances%2520performance%252C%2520with%2520further%2520gains%2520achieved%2520through%250Athe%2520timestep%2520encoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01405v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Expressive%20Power%20of%20Looped%20Transformers%3A%20Theoretical%20Analysis%20and%0A%20%20Enhancement%20via%20Timestep%20Encoding&entry.906535625=Kevin%20Xu%20and%20Issei%20Sato&entry.1292438233=%20%20Looped%20Transformers%20provide%20advantages%20in%20parameter%20efficiency%2C%20computational%0Acapabilities%2C%20and%20generalization%20for%20reasoning%20tasks.%20However%2C%20their%20expressive%0Apower%20regarding%20function%20approximation%20remains%20underexplored.%20In%20this%20paper%2C%20we%0Aestablish%20the%20approximation%20rate%20of%20Looped%20Transformers%20by%20defining%20the%20modulus%0Aof%20continuity%20for%20sequence-to-sequence%20functions.%20This%20reveals%20a%20limitation%0Aspecific%20to%20the%20looped%20architecture.%20That%20is%2C%20the%20analysis%20prompts%20the%0Aincorporation%20of%20scaling%20parameters%20for%20each%20loop%2C%20conditioned%20on%20timestep%0Aencoding.%20Experiments%20validate%20the%20theoretical%20results%2C%20showing%20that%20increasing%0Athe%20number%20of%20loops%20enhances%20performance%2C%20with%20further%20gains%20achieved%20through%0Athe%20timestep%20encoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01405v6&entry.124074799=Read"},
{"title": "Quantitative Attractor Analysis of High-Capacity Kernel Logistic\n  Regression Hopfield Networks", "author": "Akira Tamamori", "abstract": "  Traditional Hopfield networks, using Hebbian learning, face severe storage\ncapacity limits ($\\approx 0.14$ P/N) and spurious attractors. Kernel Logistic\nRegression (KLR) offers a non-linear approach, mapping patterns to\nhigh-dimensional feature spaces for improved separability. Our previous work\nshowed KLR dramatically improves capacity and noise robustness over\nconventional methods. This paper quantitatively analyzes the attractor\nstructures in KLR-trained networks via extensive simulations. We evaluated\nrecall from diverse initial states across wide storage loads (up to 4.0 P/N)\nand noise levels. We quantified convergence rates and speed. Our analysis\nconfirms KLR's superior performance: high capacity (up to 4.0 P/N) and\nrobustness. The attractor landscape is remarkably \"clean,\" with near-zero\nspurious fixed points. Recall failures under high load/noise are primarily due\nto convergence to other learned patterns, not spurious ones. Dynamics are\nexceptionally fast (typically 1-2 steps for high-similarity states). This\ncharacterization reveals how KLR reshapes dynamics for high-capacity\nassociative memory, highlighting its effectiveness and contributing to AM\nunderstanding.\n", "link": "http://arxiv.org/abs/2505.01218v1", "date": "2025-05-02", "relevancy": 1.8733, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4857}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantitative%20Attractor%20Analysis%20of%20High-Capacity%20Kernel%20Logistic%0A%20%20Regression%20Hopfield%20Networks&body=Title%3A%20Quantitative%20Attractor%20Analysis%20of%20High-Capacity%20Kernel%20Logistic%0A%20%20Regression%20Hopfield%20Networks%0AAuthor%3A%20Akira%20Tamamori%0AAbstract%3A%20%20%20Traditional%20Hopfield%20networks%2C%20using%20Hebbian%20learning%2C%20face%20severe%20storage%0Acapacity%20limits%20%28%24%5Capprox%200.14%24%20P/N%29%20and%20spurious%20attractors.%20Kernel%20Logistic%0ARegression%20%28KLR%29%20offers%20a%20non-linear%20approach%2C%20mapping%20patterns%20to%0Ahigh-dimensional%20feature%20spaces%20for%20improved%20separability.%20Our%20previous%20work%0Ashowed%20KLR%20dramatically%20improves%20capacity%20and%20noise%20robustness%20over%0Aconventional%20methods.%20This%20paper%20quantitatively%20analyzes%20the%20attractor%0Astructures%20in%20KLR-trained%20networks%20via%20extensive%20simulations.%20We%20evaluated%0Arecall%20from%20diverse%20initial%20states%20across%20wide%20storage%20loads%20%28up%20to%204.0%20P/N%29%0Aand%20noise%20levels.%20We%20quantified%20convergence%20rates%20and%20speed.%20Our%20analysis%0Aconfirms%20KLR%27s%20superior%20performance%3A%20high%20capacity%20%28up%20to%204.0%20P/N%29%20and%0Arobustness.%20The%20attractor%20landscape%20is%20remarkably%20%22clean%2C%22%20with%20near-zero%0Aspurious%20fixed%20points.%20Recall%20failures%20under%20high%20load/noise%20are%20primarily%20due%0Ato%20convergence%20to%20other%20learned%20patterns%2C%20not%20spurious%20ones.%20Dynamics%20are%0Aexceptionally%20fast%20%28typically%201-2%20steps%20for%20high-similarity%20states%29.%20This%0Acharacterization%20reveals%20how%20KLR%20reshapes%20dynamics%20for%20high-capacity%0Aassociative%20memory%2C%20highlighting%20its%20effectiveness%20and%20contributing%20to%20AM%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantitative%2520Attractor%2520Analysis%2520of%2520High-Capacity%2520Kernel%2520Logistic%250A%2520%2520Regression%2520Hopfield%2520Networks%26entry.906535625%3DAkira%2520Tamamori%26entry.1292438233%3D%2520%2520Traditional%2520Hopfield%2520networks%252C%2520using%2520Hebbian%2520learning%252C%2520face%2520severe%2520storage%250Acapacity%2520limits%2520%2528%2524%255Capprox%25200.14%2524%2520P/N%2529%2520and%2520spurious%2520attractors.%2520Kernel%2520Logistic%250ARegression%2520%2528KLR%2529%2520offers%2520a%2520non-linear%2520approach%252C%2520mapping%2520patterns%2520to%250Ahigh-dimensional%2520feature%2520spaces%2520for%2520improved%2520separability.%2520Our%2520previous%2520work%250Ashowed%2520KLR%2520dramatically%2520improves%2520capacity%2520and%2520noise%2520robustness%2520over%250Aconventional%2520methods.%2520This%2520paper%2520quantitatively%2520analyzes%2520the%2520attractor%250Astructures%2520in%2520KLR-trained%2520networks%2520via%2520extensive%2520simulations.%2520We%2520evaluated%250Arecall%2520from%2520diverse%2520initial%2520states%2520across%2520wide%2520storage%2520loads%2520%2528up%2520to%25204.0%2520P/N%2529%250Aand%2520noise%2520levels.%2520We%2520quantified%2520convergence%2520rates%2520and%2520speed.%2520Our%2520analysis%250Aconfirms%2520KLR%2527s%2520superior%2520performance%253A%2520high%2520capacity%2520%2528up%2520to%25204.0%2520P/N%2529%2520and%250Arobustness.%2520The%2520attractor%2520landscape%2520is%2520remarkably%2520%2522clean%252C%2522%2520with%2520near-zero%250Aspurious%2520fixed%2520points.%2520Recall%2520failures%2520under%2520high%2520load/noise%2520are%2520primarily%2520due%250Ato%2520convergence%2520to%2520other%2520learned%2520patterns%252C%2520not%2520spurious%2520ones.%2520Dynamics%2520are%250Aexceptionally%2520fast%2520%2528typically%25201-2%2520steps%2520for%2520high-similarity%2520states%2529.%2520This%250Acharacterization%2520reveals%2520how%2520KLR%2520reshapes%2520dynamics%2520for%2520high-capacity%250Aassociative%2520memory%252C%2520highlighting%2520its%2520effectiveness%2520and%2520contributing%2520to%2520AM%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantitative%20Attractor%20Analysis%20of%20High-Capacity%20Kernel%20Logistic%0A%20%20Regression%20Hopfield%20Networks&entry.906535625=Akira%20Tamamori&entry.1292438233=%20%20Traditional%20Hopfield%20networks%2C%20using%20Hebbian%20learning%2C%20face%20severe%20storage%0Acapacity%20limits%20%28%24%5Capprox%200.14%24%20P/N%29%20and%20spurious%20attractors.%20Kernel%20Logistic%0ARegression%20%28KLR%29%20offers%20a%20non-linear%20approach%2C%20mapping%20patterns%20to%0Ahigh-dimensional%20feature%20spaces%20for%20improved%20separability.%20Our%20previous%20work%0Ashowed%20KLR%20dramatically%20improves%20capacity%20and%20noise%20robustness%20over%0Aconventional%20methods.%20This%20paper%20quantitatively%20analyzes%20the%20attractor%0Astructures%20in%20KLR-trained%20networks%20via%20extensive%20simulations.%20We%20evaluated%0Arecall%20from%20diverse%20initial%20states%20across%20wide%20storage%20loads%20%28up%20to%204.0%20P/N%29%0Aand%20noise%20levels.%20We%20quantified%20convergence%20rates%20and%20speed.%20Our%20analysis%0Aconfirms%20KLR%27s%20superior%20performance%3A%20high%20capacity%20%28up%20to%204.0%20P/N%29%20and%0Arobustness.%20The%20attractor%20landscape%20is%20remarkably%20%22clean%2C%22%20with%20near-zero%0Aspurious%20fixed%20points.%20Recall%20failures%20under%20high%20load/noise%20are%20primarily%20due%0Ato%20convergence%20to%20other%20learned%20patterns%2C%20not%20spurious%20ones.%20Dynamics%20are%0Aexceptionally%20fast%20%28typically%201-2%20steps%20for%20high-similarity%20states%29.%20This%0Acharacterization%20reveals%20how%20KLR%20reshapes%20dynamics%20for%20high-capacity%0Aassociative%20memory%2C%20highlighting%20its%20effectiveness%20and%20contributing%20to%20AM%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01218v1&entry.124074799=Read"},
{"title": "Differentiable Nonlinear Model Predictive Control", "author": "Jonathan Frey and Katrin Baumg\u00e4rtner and Gianluca Frison and Dirk Reinhardt and Jasper Hoffmann and Leonard Fichtner and Sebastien Gros and Moritz Diehl", "abstract": "  The efficient computation of parametric solution sensitivities is a key\nchallenge in the integration of learning-enhanced methods with nonlinear model\npredictive control (MPC), as their availability is crucial for many learning\nalgorithms. While approaches presented in the machine learning community are\nlimited to convex or unconstrained formulations, this paper discusses the\ncomputation of solution sensitivities of general nonlinear programs (NLPs)\nusing the implicit function theorem (IFT) and smoothed optimality conditions\ntreated in interior-point methods (IPM). We detail sensitivity computation\nwithin a sequential quadratic programming (SQP) method which employs an IPM for\nthe quadratic subproblems. The publication is accompanied by an efficient\nopen-source implementation within the framework, providing both forward and\nadjoint sensitivities for general optimal control problems, achieving speedups\nexceeding 3x over the state-of-the-art solver mpc.pytorch.\n", "link": "http://arxiv.org/abs/2505.01353v1", "date": "2025-05-02", "relevancy": 1.8719, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4921}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4683}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Nonlinear%20Model%20Predictive%20Control&body=Title%3A%20Differentiable%20Nonlinear%20Model%20Predictive%20Control%0AAuthor%3A%20Jonathan%20Frey%20and%20Katrin%20Baumg%C3%A4rtner%20and%20Gianluca%20Frison%20and%20Dirk%20Reinhardt%20and%20Jasper%20Hoffmann%20and%20Leonard%20Fichtner%20and%20Sebastien%20Gros%20and%20Moritz%20Diehl%0AAbstract%3A%20%20%20The%20efficient%20computation%20of%20parametric%20solution%20sensitivities%20is%20a%20key%0Achallenge%20in%20the%20integration%20of%20learning-enhanced%20methods%20with%20nonlinear%20model%0Apredictive%20control%20%28MPC%29%2C%20as%20their%20availability%20is%20crucial%20for%20many%20learning%0Aalgorithms.%20While%20approaches%20presented%20in%20the%20machine%20learning%20community%20are%0Alimited%20to%20convex%20or%20unconstrained%20formulations%2C%20this%20paper%20discusses%20the%0Acomputation%20of%20solution%20sensitivities%20of%20general%20nonlinear%20programs%20%28NLPs%29%0Ausing%20the%20implicit%20function%20theorem%20%28IFT%29%20and%20smoothed%20optimality%20conditions%0Atreated%20in%20interior-point%20methods%20%28IPM%29.%20We%20detail%20sensitivity%20computation%0Awithin%20a%20sequential%20quadratic%20programming%20%28SQP%29%20method%20which%20employs%20an%20IPM%20for%0Athe%20quadratic%20subproblems.%20The%20publication%20is%20accompanied%20by%20an%20efficient%0Aopen-source%20implementation%20within%20the%20framework%2C%20providing%20both%20forward%20and%0Aadjoint%20sensitivities%20for%20general%20optimal%20control%20problems%2C%20achieving%20speedups%0Aexceeding%203x%20over%20the%20state-of-the-art%20solver%20mpc.pytorch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Nonlinear%2520Model%2520Predictive%2520Control%26entry.906535625%3DJonathan%2520Frey%2520and%2520Katrin%2520Baumg%25C3%25A4rtner%2520and%2520Gianluca%2520Frison%2520and%2520Dirk%2520Reinhardt%2520and%2520Jasper%2520Hoffmann%2520and%2520Leonard%2520Fichtner%2520and%2520Sebastien%2520Gros%2520and%2520Moritz%2520Diehl%26entry.1292438233%3D%2520%2520The%2520efficient%2520computation%2520of%2520parametric%2520solution%2520sensitivities%2520is%2520a%2520key%250Achallenge%2520in%2520the%2520integration%2520of%2520learning-enhanced%2520methods%2520with%2520nonlinear%2520model%250Apredictive%2520control%2520%2528MPC%2529%252C%2520as%2520their%2520availability%2520is%2520crucial%2520for%2520many%2520learning%250Aalgorithms.%2520While%2520approaches%2520presented%2520in%2520the%2520machine%2520learning%2520community%2520are%250Alimited%2520to%2520convex%2520or%2520unconstrained%2520formulations%252C%2520this%2520paper%2520discusses%2520the%250Acomputation%2520of%2520solution%2520sensitivities%2520of%2520general%2520nonlinear%2520programs%2520%2528NLPs%2529%250Ausing%2520the%2520implicit%2520function%2520theorem%2520%2528IFT%2529%2520and%2520smoothed%2520optimality%2520conditions%250Atreated%2520in%2520interior-point%2520methods%2520%2528IPM%2529.%2520We%2520detail%2520sensitivity%2520computation%250Awithin%2520a%2520sequential%2520quadratic%2520programming%2520%2528SQP%2529%2520method%2520which%2520employs%2520an%2520IPM%2520for%250Athe%2520quadratic%2520subproblems.%2520The%2520publication%2520is%2520accompanied%2520by%2520an%2520efficient%250Aopen-source%2520implementation%2520within%2520the%2520framework%252C%2520providing%2520both%2520forward%2520and%250Aadjoint%2520sensitivities%2520for%2520general%2520optimal%2520control%2520problems%252C%2520achieving%2520speedups%250Aexceeding%25203x%2520over%2520the%2520state-of-the-art%2520solver%2520mpc.pytorch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Nonlinear%20Model%20Predictive%20Control&entry.906535625=Jonathan%20Frey%20and%20Katrin%20Baumg%C3%A4rtner%20and%20Gianluca%20Frison%20and%20Dirk%20Reinhardt%20and%20Jasper%20Hoffmann%20and%20Leonard%20Fichtner%20and%20Sebastien%20Gros%20and%20Moritz%20Diehl&entry.1292438233=%20%20The%20efficient%20computation%20of%20parametric%20solution%20sensitivities%20is%20a%20key%0Achallenge%20in%20the%20integration%20of%20learning-enhanced%20methods%20with%20nonlinear%20model%0Apredictive%20control%20%28MPC%29%2C%20as%20their%20availability%20is%20crucial%20for%20many%20learning%0Aalgorithms.%20While%20approaches%20presented%20in%20the%20machine%20learning%20community%20are%0Alimited%20to%20convex%20or%20unconstrained%20formulations%2C%20this%20paper%20discusses%20the%0Acomputation%20of%20solution%20sensitivities%20of%20general%20nonlinear%20programs%20%28NLPs%29%0Ausing%20the%20implicit%20function%20theorem%20%28IFT%29%20and%20smoothed%20optimality%20conditions%0Atreated%20in%20interior-point%20methods%20%28IPM%29.%20We%20detail%20sensitivity%20computation%0Awithin%20a%20sequential%20quadratic%20programming%20%28SQP%29%20method%20which%20employs%20an%20IPM%20for%0Athe%20quadratic%20subproblems.%20The%20publication%20is%20accompanied%20by%20an%20efficient%0Aopen-source%20implementation%20within%20the%20framework%2C%20providing%20both%20forward%20and%0Aadjoint%20sensitivities%20for%20general%20optimal%20control%20problems%2C%20achieving%20speedups%0Aexceeding%203x%20over%20the%20state-of-the-art%20solver%20mpc.pytorch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01353v1&entry.124074799=Read"},
{"title": "Learning Stabilizing Policies via an Unstable Subspace Representation", "author": "Leonardo F. Toso and Lintao Ye and James Anderson", "abstract": "  We study the problem of learning to stabilize (LTS) a linear time-invariant\n(LTI) system. Policy gradient (PG) methods for control assume access to an\ninitial stabilizing policy. However, designing such a policy for an unknown\nsystem is one of the most fundamental problems in control, and it may be as\nhard as learning the optimal policy itself. Existing work on the LTS problem\nrequires large data as it scales quadratically with the ambient dimension. We\npropose a two-phase approach that first learns the left unstable subspace of\nthe system and then solves a series of discounted linear quadratic regulator\n(LQR) problems on the learned unstable subspace, targeting to stabilize only\nthe system's unstable dynamics and reduce the effective dimension of the\ncontrol space. We provide non-asymptotic guarantees for both phases and\ndemonstrate that operating on the unstable subspace reduces sample complexity.\nIn particular, when the number of unstable modes is much smaller than the state\ndimension, our analysis reveals that LTS on the unstable subspace substantially\nspeeds up the stabilization process. Numerical experiments are provided to\nsupport this sample complexity reduction achieved by our approach.\n", "link": "http://arxiv.org/abs/2505.01348v1", "date": "2025-05-02", "relevancy": 1.8688, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4958}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4491}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Stabilizing%20Policies%20via%20an%20Unstable%20Subspace%20Representation&body=Title%3A%20Learning%20Stabilizing%20Policies%20via%20an%20Unstable%20Subspace%20Representation%0AAuthor%3A%20Leonardo%20F.%20Toso%20and%20Lintao%20Ye%20and%20James%20Anderson%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20to%20stabilize%20%28LTS%29%20a%20linear%20time-invariant%0A%28LTI%29%20system.%20Policy%20gradient%20%28PG%29%20methods%20for%20control%20assume%20access%20to%20an%0Ainitial%20stabilizing%20policy.%20However%2C%20designing%20such%20a%20policy%20for%20an%20unknown%0Asystem%20is%20one%20of%20the%20most%20fundamental%20problems%20in%20control%2C%20and%20it%20may%20be%20as%0Ahard%20as%20learning%20the%20optimal%20policy%20itself.%20Existing%20work%20on%20the%20LTS%20problem%0Arequires%20large%20data%20as%20it%20scales%20quadratically%20with%20the%20ambient%20dimension.%20We%0Apropose%20a%20two-phase%20approach%20that%20first%20learns%20the%20left%20unstable%20subspace%20of%0Athe%20system%20and%20then%20solves%20a%20series%20of%20discounted%20linear%20quadratic%20regulator%0A%28LQR%29%20problems%20on%20the%20learned%20unstable%20subspace%2C%20targeting%20to%20stabilize%20only%0Athe%20system%27s%20unstable%20dynamics%20and%20reduce%20the%20effective%20dimension%20of%20the%0Acontrol%20space.%20We%20provide%20non-asymptotic%20guarantees%20for%20both%20phases%20and%0Ademonstrate%20that%20operating%20on%20the%20unstable%20subspace%20reduces%20sample%20complexity.%0AIn%20particular%2C%20when%20the%20number%20of%20unstable%20modes%20is%20much%20smaller%20than%20the%20state%0Adimension%2C%20our%20analysis%20reveals%20that%20LTS%20on%20the%20unstable%20subspace%20substantially%0Aspeeds%20up%20the%20stabilization%20process.%20Numerical%20experiments%20are%20provided%20to%0Asupport%20this%20sample%20complexity%20reduction%20achieved%20by%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Stabilizing%2520Policies%2520via%2520an%2520Unstable%2520Subspace%2520Representation%26entry.906535625%3DLeonardo%2520F.%2520Toso%2520and%2520Lintao%2520Ye%2520and%2520James%2520Anderson%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520learning%2520to%2520stabilize%2520%2528LTS%2529%2520a%2520linear%2520time-invariant%250A%2528LTI%2529%2520system.%2520Policy%2520gradient%2520%2528PG%2529%2520methods%2520for%2520control%2520assume%2520access%2520to%2520an%250Ainitial%2520stabilizing%2520policy.%2520However%252C%2520designing%2520such%2520a%2520policy%2520for%2520an%2520unknown%250Asystem%2520is%2520one%2520of%2520the%2520most%2520fundamental%2520problems%2520in%2520control%252C%2520and%2520it%2520may%2520be%2520as%250Ahard%2520as%2520learning%2520the%2520optimal%2520policy%2520itself.%2520Existing%2520work%2520on%2520the%2520LTS%2520problem%250Arequires%2520large%2520data%2520as%2520it%2520scales%2520quadratically%2520with%2520the%2520ambient%2520dimension.%2520We%250Apropose%2520a%2520two-phase%2520approach%2520that%2520first%2520learns%2520the%2520left%2520unstable%2520subspace%2520of%250Athe%2520system%2520and%2520then%2520solves%2520a%2520series%2520of%2520discounted%2520linear%2520quadratic%2520regulator%250A%2528LQR%2529%2520problems%2520on%2520the%2520learned%2520unstable%2520subspace%252C%2520targeting%2520to%2520stabilize%2520only%250Athe%2520system%2527s%2520unstable%2520dynamics%2520and%2520reduce%2520the%2520effective%2520dimension%2520of%2520the%250Acontrol%2520space.%2520We%2520provide%2520non-asymptotic%2520guarantees%2520for%2520both%2520phases%2520and%250Ademonstrate%2520that%2520operating%2520on%2520the%2520unstable%2520subspace%2520reduces%2520sample%2520complexity.%250AIn%2520particular%252C%2520when%2520the%2520number%2520of%2520unstable%2520modes%2520is%2520much%2520smaller%2520than%2520the%2520state%250Adimension%252C%2520our%2520analysis%2520reveals%2520that%2520LTS%2520on%2520the%2520unstable%2520subspace%2520substantially%250Aspeeds%2520up%2520the%2520stabilization%2520process.%2520Numerical%2520experiments%2520are%2520provided%2520to%250Asupport%2520this%2520sample%2520complexity%2520reduction%2520achieved%2520by%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Stabilizing%20Policies%20via%20an%20Unstable%20Subspace%20Representation&entry.906535625=Leonardo%20F.%20Toso%20and%20Lintao%20Ye%20and%20James%20Anderson&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20to%20stabilize%20%28LTS%29%20a%20linear%20time-invariant%0A%28LTI%29%20system.%20Policy%20gradient%20%28PG%29%20methods%20for%20control%20assume%20access%20to%20an%0Ainitial%20stabilizing%20policy.%20However%2C%20designing%20such%20a%20policy%20for%20an%20unknown%0Asystem%20is%20one%20of%20the%20most%20fundamental%20problems%20in%20control%2C%20and%20it%20may%20be%20as%0Ahard%20as%20learning%20the%20optimal%20policy%20itself.%20Existing%20work%20on%20the%20LTS%20problem%0Arequires%20large%20data%20as%20it%20scales%20quadratically%20with%20the%20ambient%20dimension.%20We%0Apropose%20a%20two-phase%20approach%20that%20first%20learns%20the%20left%20unstable%20subspace%20of%0Athe%20system%20and%20then%20solves%20a%20series%20of%20discounted%20linear%20quadratic%20regulator%0A%28LQR%29%20problems%20on%20the%20learned%20unstable%20subspace%2C%20targeting%20to%20stabilize%20only%0Athe%20system%27s%20unstable%20dynamics%20and%20reduce%20the%20effective%20dimension%20of%20the%0Acontrol%20space.%20We%20provide%20non-asymptotic%20guarantees%20for%20both%20phases%20and%0Ademonstrate%20that%20operating%20on%20the%20unstable%20subspace%20reduces%20sample%20complexity.%0AIn%20particular%2C%20when%20the%20number%20of%20unstable%20modes%20is%20much%20smaller%20than%20the%20state%0Adimension%2C%20our%20analysis%20reveals%20that%20LTS%20on%20the%20unstable%20subspace%20substantially%0Aspeeds%20up%20the%20stabilization%20process.%20Numerical%20experiments%20are%20provided%20to%0Asupport%20this%20sample%20complexity%20reduction%20achieved%20by%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01348v1&entry.124074799=Read"},
{"title": "A Provably Convergent Plug-and-Play Framework for Stochastic Bilevel\n  Optimization", "author": "Tianshu Chu and Dachuan Xu and Wei Yao and Chengming Yu and Jin Zhang", "abstract": "  Bilevel optimization has recently attracted significant attention in machine\nlearning due to its wide range of applications and advanced hierarchical\noptimization capabilities. In this paper, we propose a plug-and-play framework,\nnamed PnPBO, for developing and analyzing stochastic bilevel optimization\nmethods. This framework integrates both modern unbiased and biased stochastic\nestimators into the single-loop bilevel optimization framework introduced in\n[9], with several improvements. In the implementation of PnPBO, all stochastic\nestimators for different variables can be independently incorporated, and an\nadditional moving average technique is applied when using an unbiased estimator\nfor the upper-level variable. In the theoretical analysis, we provide a unified\nconvergence and complexity analysis for PnPBO, demonstrating that the\nadaptation of various stochastic estimators (including PAGE, ZeroSARAH, and\nmixed strategies) within the PnPBO framework achieves optimal sample\ncomplexity, comparable to that of single-level optimization. This resolves the\nopen question of whether the optimal complexity bounds for solving bilevel\noptimization are identical to those for single-level optimization. Finally, we\nempirically validate our framework, demonstrating its effectiveness on several\nbenchmark problems and confirming our theoretical findings.\n", "link": "http://arxiv.org/abs/2505.01258v1", "date": "2025-05-02", "relevancy": 1.8432, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.463}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Provably%20Convergent%20Plug-and-Play%20Framework%20for%20Stochastic%20Bilevel%0A%20%20Optimization&body=Title%3A%20A%20Provably%20Convergent%20Plug-and-Play%20Framework%20for%20Stochastic%20Bilevel%0A%20%20Optimization%0AAuthor%3A%20Tianshu%20Chu%20and%20Dachuan%20Xu%20and%20Wei%20Yao%20and%20Chengming%20Yu%20and%20Jin%20Zhang%0AAbstract%3A%20%20%20Bilevel%20optimization%20has%20recently%20attracted%20significant%20attention%20in%20machine%0Alearning%20due%20to%20its%20wide%20range%20of%20applications%20and%20advanced%20hierarchical%0Aoptimization%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%20plug-and-play%20framework%2C%0Anamed%20PnPBO%2C%20for%20developing%20and%20analyzing%20stochastic%20bilevel%20optimization%0Amethods.%20This%20framework%20integrates%20both%20modern%20unbiased%20and%20biased%20stochastic%0Aestimators%20into%20the%20single-loop%20bilevel%20optimization%20framework%20introduced%20in%0A%5B9%5D%2C%20with%20several%20improvements.%20In%20the%20implementation%20of%20PnPBO%2C%20all%20stochastic%0Aestimators%20for%20different%20variables%20can%20be%20independently%20incorporated%2C%20and%20an%0Aadditional%20moving%20average%20technique%20is%20applied%20when%20using%20an%20unbiased%20estimator%0Afor%20the%20upper-level%20variable.%20In%20the%20theoretical%20analysis%2C%20we%20provide%20a%20unified%0Aconvergence%20and%20complexity%20analysis%20for%20PnPBO%2C%20demonstrating%20that%20the%0Aadaptation%20of%20various%20stochastic%20estimators%20%28including%20PAGE%2C%20ZeroSARAH%2C%20and%0Amixed%20strategies%29%20within%20the%20PnPBO%20framework%20achieves%20optimal%20sample%0Acomplexity%2C%20comparable%20to%20that%20of%20single-level%20optimization.%20This%20resolves%20the%0Aopen%20question%20of%20whether%20the%20optimal%20complexity%20bounds%20for%20solving%20bilevel%0Aoptimization%20are%20identical%20to%20those%20for%20single-level%20optimization.%20Finally%2C%20we%0Aempirically%20validate%20our%20framework%2C%20demonstrating%20its%20effectiveness%20on%20several%0Abenchmark%20problems%20and%20confirming%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Provably%2520Convergent%2520Plug-and-Play%2520Framework%2520for%2520Stochastic%2520Bilevel%250A%2520%2520Optimization%26entry.906535625%3DTianshu%2520Chu%2520and%2520Dachuan%2520Xu%2520and%2520Wei%2520Yao%2520and%2520Chengming%2520Yu%2520and%2520Jin%2520Zhang%26entry.1292438233%3D%2520%2520Bilevel%2520optimization%2520has%2520recently%2520attracted%2520significant%2520attention%2520in%2520machine%250Alearning%2520due%2520to%2520its%2520wide%2520range%2520of%2520applications%2520and%2520advanced%2520hierarchical%250Aoptimization%2520capabilities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520plug-and-play%2520framework%252C%250Anamed%2520PnPBO%252C%2520for%2520developing%2520and%2520analyzing%2520stochastic%2520bilevel%2520optimization%250Amethods.%2520This%2520framework%2520integrates%2520both%2520modern%2520unbiased%2520and%2520biased%2520stochastic%250Aestimators%2520into%2520the%2520single-loop%2520bilevel%2520optimization%2520framework%2520introduced%2520in%250A%255B9%255D%252C%2520with%2520several%2520improvements.%2520In%2520the%2520implementation%2520of%2520PnPBO%252C%2520all%2520stochastic%250Aestimators%2520for%2520different%2520variables%2520can%2520be%2520independently%2520incorporated%252C%2520and%2520an%250Aadditional%2520moving%2520average%2520technique%2520is%2520applied%2520when%2520using%2520an%2520unbiased%2520estimator%250Afor%2520the%2520upper-level%2520variable.%2520In%2520the%2520theoretical%2520analysis%252C%2520we%2520provide%2520a%2520unified%250Aconvergence%2520and%2520complexity%2520analysis%2520for%2520PnPBO%252C%2520demonstrating%2520that%2520the%250Aadaptation%2520of%2520various%2520stochastic%2520estimators%2520%2528including%2520PAGE%252C%2520ZeroSARAH%252C%2520and%250Amixed%2520strategies%2529%2520within%2520the%2520PnPBO%2520framework%2520achieves%2520optimal%2520sample%250Acomplexity%252C%2520comparable%2520to%2520that%2520of%2520single-level%2520optimization.%2520This%2520resolves%2520the%250Aopen%2520question%2520of%2520whether%2520the%2520optimal%2520complexity%2520bounds%2520for%2520solving%2520bilevel%250Aoptimization%2520are%2520identical%2520to%2520those%2520for%2520single-level%2520optimization.%2520Finally%252C%2520we%250Aempirically%2520validate%2520our%2520framework%252C%2520demonstrating%2520its%2520effectiveness%2520on%2520several%250Abenchmark%2520problems%2520and%2520confirming%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Provably%20Convergent%20Plug-and-Play%20Framework%20for%20Stochastic%20Bilevel%0A%20%20Optimization&entry.906535625=Tianshu%20Chu%20and%20Dachuan%20Xu%20and%20Wei%20Yao%20and%20Chengming%20Yu%20and%20Jin%20Zhang&entry.1292438233=%20%20Bilevel%20optimization%20has%20recently%20attracted%20significant%20attention%20in%20machine%0Alearning%20due%20to%20its%20wide%20range%20of%20applications%20and%20advanced%20hierarchical%0Aoptimization%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%20plug-and-play%20framework%2C%0Anamed%20PnPBO%2C%20for%20developing%20and%20analyzing%20stochastic%20bilevel%20optimization%0Amethods.%20This%20framework%20integrates%20both%20modern%20unbiased%20and%20biased%20stochastic%0Aestimators%20into%20the%20single-loop%20bilevel%20optimization%20framework%20introduced%20in%0A%5B9%5D%2C%20with%20several%20improvements.%20In%20the%20implementation%20of%20PnPBO%2C%20all%20stochastic%0Aestimators%20for%20different%20variables%20can%20be%20independently%20incorporated%2C%20and%20an%0Aadditional%20moving%20average%20technique%20is%20applied%20when%20using%20an%20unbiased%20estimator%0Afor%20the%20upper-level%20variable.%20In%20the%20theoretical%20analysis%2C%20we%20provide%20a%20unified%0Aconvergence%20and%20complexity%20analysis%20for%20PnPBO%2C%20demonstrating%20that%20the%0Aadaptation%20of%20various%20stochastic%20estimators%20%28including%20PAGE%2C%20ZeroSARAH%2C%20and%0Amixed%20strategies%29%20within%20the%20PnPBO%20framework%20achieves%20optimal%20sample%0Acomplexity%2C%20comparable%20to%20that%20of%20single-level%20optimization.%20This%20resolves%20the%0Aopen%20question%20of%20whether%20the%20optimal%20complexity%20bounds%20for%20solving%20bilevel%0Aoptimization%20are%20identical%20to%20those%20for%20single-level%20optimization.%20Finally%2C%20we%0Aempirically%20validate%20our%20framework%2C%20demonstrating%20its%20effectiveness%20on%20several%0Abenchmark%20problems%20and%20confirming%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01258v1&entry.124074799=Read"},
{"title": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures", "author": "Francisco Aguilera-Mart\u00ednez and Fernando Berzal", "abstract": "  As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges.\n", "link": "http://arxiv.org/abs/2505.01177v1", "date": "2025-05-02", "relevancy": 1.8367, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Security%3A%20Vulnerabilities%2C%20Attacks%2C%20Defenses%2C%20and%20Countermeasures&body=Title%3A%20LLM%20Security%3A%20Vulnerabilities%2C%20Attacks%2C%20Defenses%2C%20and%20Countermeasures%0AAuthor%3A%20Francisco%20Aguilera-Mart%C3%ADnez%20and%20Fernando%20Berzal%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20evolve%2C%20it%20is%20critical%20to%20assess%0Athe%20security%20threats%20and%20vulnerabilities%20that%20may%20arise%20both%20during%20their%0Atraining%20phase%20and%20after%20models%20have%20been%20deployed.%20This%20survey%20seeks%20to%20define%0Aand%20categorize%20the%20various%20attacks%20targeting%20LLMs%2C%20distinguishing%20between%20those%0Athat%20occur%20during%20the%20training%20phase%20and%20those%20that%20affect%20already%20trained%0Amodels.%20A%20thorough%20analysis%20of%20these%20attacks%20is%20presented%2C%20alongside%20an%0Aexploration%20of%20defense%20mechanisms%20designed%20to%20mitigate%20such%20threats.%20Defenses%0Aare%20classified%20into%20two%20primary%20categories%3A%20prevention-based%20and%0Adetection-based%20defenses.%20Furthermore%2C%20our%20survey%20summarizes%20possible%20attacks%0Aand%20their%20corresponding%20defense%20strategies.%20It%20also%20provides%20an%20evaluation%20of%0Athe%20effectiveness%20of%20the%20known%20defense%20mechanisms%20for%20the%20different%20security%0Athreats.%20Our%20survey%20aims%20to%20offer%20a%20structured%20framework%20for%20securing%20LLMs%2C%0Awhile%20also%20identifying%20areas%20that%20require%20further%20research%20to%20improve%20and%0Astrengthen%20defenses%20against%20emerging%20security%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Security%253A%2520Vulnerabilities%252C%2520Attacks%252C%2520Defenses%252C%2520and%2520Countermeasures%26entry.906535625%3DFrancisco%2520Aguilera-Mart%25C3%25ADnez%2520and%2520Fernando%2520Berzal%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520continue%2520to%2520evolve%252C%2520it%2520is%2520critical%2520to%2520assess%250Athe%2520security%2520threats%2520and%2520vulnerabilities%2520that%2520may%2520arise%2520both%2520during%2520their%250Atraining%2520phase%2520and%2520after%2520models%2520have%2520been%2520deployed.%2520This%2520survey%2520seeks%2520to%2520define%250Aand%2520categorize%2520the%2520various%2520attacks%2520targeting%2520LLMs%252C%2520distinguishing%2520between%2520those%250Athat%2520occur%2520during%2520the%2520training%2520phase%2520and%2520those%2520that%2520affect%2520already%2520trained%250Amodels.%2520A%2520thorough%2520analysis%2520of%2520these%2520attacks%2520is%2520presented%252C%2520alongside%2520an%250Aexploration%2520of%2520defense%2520mechanisms%2520designed%2520to%2520mitigate%2520such%2520threats.%2520Defenses%250Aare%2520classified%2520into%2520two%2520primary%2520categories%253A%2520prevention-based%2520and%250Adetection-based%2520defenses.%2520Furthermore%252C%2520our%2520survey%2520summarizes%2520possible%2520attacks%250Aand%2520their%2520corresponding%2520defense%2520strategies.%2520It%2520also%2520provides%2520an%2520evaluation%2520of%250Athe%2520effectiveness%2520of%2520the%2520known%2520defense%2520mechanisms%2520for%2520the%2520different%2520security%250Athreats.%2520Our%2520survey%2520aims%2520to%2520offer%2520a%2520structured%2520framework%2520for%2520securing%2520LLMs%252C%250Awhile%2520also%2520identifying%2520areas%2520that%2520require%2520further%2520research%2520to%2520improve%2520and%250Astrengthen%2520defenses%2520against%2520emerging%2520security%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Security%3A%20Vulnerabilities%2C%20Attacks%2C%20Defenses%2C%20and%20Countermeasures&entry.906535625=Francisco%20Aguilera-Mart%C3%ADnez%20and%20Fernando%20Berzal&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20evolve%2C%20it%20is%20critical%20to%20assess%0Athe%20security%20threats%20and%20vulnerabilities%20that%20may%20arise%20both%20during%20their%0Atraining%20phase%20and%20after%20models%20have%20been%20deployed.%20This%20survey%20seeks%20to%20define%0Aand%20categorize%20the%20various%20attacks%20targeting%20LLMs%2C%20distinguishing%20between%20those%0Athat%20occur%20during%20the%20training%20phase%20and%20those%20that%20affect%20already%20trained%0Amodels.%20A%20thorough%20analysis%20of%20these%20attacks%20is%20presented%2C%20alongside%20an%0Aexploration%20of%20defense%20mechanisms%20designed%20to%20mitigate%20such%20threats.%20Defenses%0Aare%20classified%20into%20two%20primary%20categories%3A%20prevention-based%20and%0Adetection-based%20defenses.%20Furthermore%2C%20our%20survey%20summarizes%20possible%20attacks%0Aand%20their%20corresponding%20defense%20strategies.%20It%20also%20provides%20an%20evaluation%20of%0Athe%20effectiveness%20of%20the%20known%20defense%20mechanisms%20for%20the%20different%20security%0Athreats.%20Our%20survey%20aims%20to%20offer%20a%20structured%20framework%20for%20securing%20LLMs%2C%0Awhile%20also%20identifying%20areas%20that%20require%20further%20research%20to%20improve%20and%0Astrengthen%20defenses%20against%20emerging%20security%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01177v1&entry.124074799=Read"},
{"title": "Client Selection for Federated Policy Optimization with Environment\n  Heterogeneity", "author": "Zhijie Xie and S. H. Song", "abstract": "  The development of Policy Iteration (PI) has inspired many recent algorithms\nfor Reinforcement Learning (RL), including several policy gradient methods that\ngained both theoretical soundness and empirical success on a variety of tasks.\nThe theory of PI is rich in the context of centralized learning, but its study\nunder the federated setting is still in the infant stage. This paper\ninvestigates the federated version of Approximate PI (API) and derives its\nerror bound, taking into account the approximation error introduced by\nenvironment heterogeneity. We theoretically prove that a proper client\nselection scheme can reduce this error bound. Based on the theoretical result,\nwe propose a client selection algorithm to alleviate the additional\napproximation error caused by environment heterogeneity. Experiment results\nshow that the proposed algorithm outperforms other biased and unbiased client\nselection methods on the federated mountain car problem, the Mujoco Hopper\nproblem, and the SUMO-based autonomous vehicle training problem by effectively\nselecting clients with a lower level of heterogeneity from the population\ndistribution.\n", "link": "http://arxiv.org/abs/2305.10978v6", "date": "2025-05-02", "relevancy": 1.8227, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4689}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4566}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Client%20Selection%20for%20Federated%20Policy%20Optimization%20with%20Environment%0A%20%20Heterogeneity&body=Title%3A%20Client%20Selection%20for%20Federated%20Policy%20Optimization%20with%20Environment%0A%20%20Heterogeneity%0AAuthor%3A%20Zhijie%20Xie%20and%20S.%20H.%20Song%0AAbstract%3A%20%20%20The%20development%20of%20Policy%20Iteration%20%28PI%29%20has%20inspired%20many%20recent%20algorithms%0Afor%20Reinforcement%20Learning%20%28RL%29%2C%20including%20several%20policy%20gradient%20methods%20that%0Agained%20both%20theoretical%20soundness%20and%20empirical%20success%20on%20a%20variety%20of%20tasks.%0AThe%20theory%20of%20PI%20is%20rich%20in%20the%20context%20of%20centralized%20learning%2C%20but%20its%20study%0Aunder%20the%20federated%20setting%20is%20still%20in%20the%20infant%20stage.%20This%20paper%0Ainvestigates%20the%20federated%20version%20of%20Approximate%20PI%20%28API%29%20and%20derives%20its%0Aerror%20bound%2C%20taking%20into%20account%20the%20approximation%20error%20introduced%20by%0Aenvironment%20heterogeneity.%20We%20theoretically%20prove%20that%20a%20proper%20client%0Aselection%20scheme%20can%20reduce%20this%20error%20bound.%20Based%20on%20the%20theoretical%20result%2C%0Awe%20propose%20a%20client%20selection%20algorithm%20to%20alleviate%20the%20additional%0Aapproximation%20error%20caused%20by%20environment%20heterogeneity.%20Experiment%20results%0Ashow%20that%20the%20proposed%20algorithm%20outperforms%20other%20biased%20and%20unbiased%20client%0Aselection%20methods%20on%20the%20federated%20mountain%20car%20problem%2C%20the%20Mujoco%20Hopper%0Aproblem%2C%20and%20the%20SUMO-based%20autonomous%20vehicle%20training%20problem%20by%20effectively%0Aselecting%20clients%20with%20a%20lower%20level%20of%20heterogeneity%20from%20the%20population%0Adistribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10978v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClient%2520Selection%2520for%2520Federated%2520Policy%2520Optimization%2520with%2520Environment%250A%2520%2520Heterogeneity%26entry.906535625%3DZhijie%2520Xie%2520and%2520S.%2520H.%2520Song%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520Policy%2520Iteration%2520%2528PI%2529%2520has%2520inspired%2520many%2520recent%2520algorithms%250Afor%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520including%2520several%2520policy%2520gradient%2520methods%2520that%250Agained%2520both%2520theoretical%2520soundness%2520and%2520empirical%2520success%2520on%2520a%2520variety%2520of%2520tasks.%250AThe%2520theory%2520of%2520PI%2520is%2520rich%2520in%2520the%2520context%2520of%2520centralized%2520learning%252C%2520but%2520its%2520study%250Aunder%2520the%2520federated%2520setting%2520is%2520still%2520in%2520the%2520infant%2520stage.%2520This%2520paper%250Ainvestigates%2520the%2520federated%2520version%2520of%2520Approximate%2520PI%2520%2528API%2529%2520and%2520derives%2520its%250Aerror%2520bound%252C%2520taking%2520into%2520account%2520the%2520approximation%2520error%2520introduced%2520by%250Aenvironment%2520heterogeneity.%2520We%2520theoretically%2520prove%2520that%2520a%2520proper%2520client%250Aselection%2520scheme%2520can%2520reduce%2520this%2520error%2520bound.%2520Based%2520on%2520the%2520theoretical%2520result%252C%250Awe%2520propose%2520a%2520client%2520selection%2520algorithm%2520to%2520alleviate%2520the%2520additional%250Aapproximation%2520error%2520caused%2520by%2520environment%2520heterogeneity.%2520Experiment%2520results%250Ashow%2520that%2520the%2520proposed%2520algorithm%2520outperforms%2520other%2520biased%2520and%2520unbiased%2520client%250Aselection%2520methods%2520on%2520the%2520federated%2520mountain%2520car%2520problem%252C%2520the%2520Mujoco%2520Hopper%250Aproblem%252C%2520and%2520the%2520SUMO-based%2520autonomous%2520vehicle%2520training%2520problem%2520by%2520effectively%250Aselecting%2520clients%2520with%2520a%2520lower%2520level%2520of%2520heterogeneity%2520from%2520the%2520population%250Adistribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.10978v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Client%20Selection%20for%20Federated%20Policy%20Optimization%20with%20Environment%0A%20%20Heterogeneity&entry.906535625=Zhijie%20Xie%20and%20S.%20H.%20Song&entry.1292438233=%20%20The%20development%20of%20Policy%20Iteration%20%28PI%29%20has%20inspired%20many%20recent%20algorithms%0Afor%20Reinforcement%20Learning%20%28RL%29%2C%20including%20several%20policy%20gradient%20methods%20that%0Agained%20both%20theoretical%20soundness%20and%20empirical%20success%20on%20a%20variety%20of%20tasks.%0AThe%20theory%20of%20PI%20is%20rich%20in%20the%20context%20of%20centralized%20learning%2C%20but%20its%20study%0Aunder%20the%20federated%20setting%20is%20still%20in%20the%20infant%20stage.%20This%20paper%0Ainvestigates%20the%20federated%20version%20of%20Approximate%20PI%20%28API%29%20and%20derives%20its%0Aerror%20bound%2C%20taking%20into%20account%20the%20approximation%20error%20introduced%20by%0Aenvironment%20heterogeneity.%20We%20theoretically%20prove%20that%20a%20proper%20client%0Aselection%20scheme%20can%20reduce%20this%20error%20bound.%20Based%20on%20the%20theoretical%20result%2C%0Awe%20propose%20a%20client%20selection%20algorithm%20to%20alleviate%20the%20additional%0Aapproximation%20error%20caused%20by%20environment%20heterogeneity.%20Experiment%20results%0Ashow%20that%20the%20proposed%20algorithm%20outperforms%20other%20biased%20and%20unbiased%20client%0Aselection%20methods%20on%20the%20federated%20mountain%20car%20problem%2C%20the%20Mujoco%20Hopper%0Aproblem%2C%20and%20the%20SUMO-based%20autonomous%20vehicle%20training%20problem%20by%20effectively%0Aselecting%20clients%20with%20a%20lower%20level%20of%20heterogeneity%20from%20the%20population%0Adistribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10978v6&entry.124074799=Read"},
{"title": "Enhancing SPARQL Query Rewriting for Complex Ontology Alignments", "author": "Anicet Lepetit Ondo and Laurence Capus and Mamadou Bousso", "abstract": "  SPARQL query rewriting is a fundamental mechanism for uniformly querying\nheterogeneous ontologies in the Linked Data Web. However, the complexity of\nontology alignments, particularly rich correspondences (c : c), makes this\nprocess challenging. Existing approaches primarily focus on simple (s : s) and\npartially complex ( s : c) alignments, thereby overlooking the challenges posed\nby more expressive alignments. Moreover, the intricate syntax of SPARQL\npresents a barrier for non-expert users seeking to fully exploit the knowledge\nencapsulated in ontologies. This article proposes an innovative approach for\nthe automatic rewriting of SPARQL queries from a source ontology to a target\nontology, based on a user's need expressed in natural language. It leverages\nthe principles of equivalence transitivity as well as the advanced capabilities\nof large language models such as GPT-4. By integrating these elements, this\napproach stands out for its ability to efficiently handle complex alignments,\nparticularly (c : c) correspondences , by fully exploiting their\nexpressiveness. Additionally, it facilitates access to aligned ontologies for\nusers unfamiliar with SPARQL, providing a flexible solution for querying\nheterogeneous data.\n", "link": "http://arxiv.org/abs/2505.01309v1", "date": "2025-05-02", "relevancy": 1.813, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20SPARQL%20Query%20Rewriting%20for%20Complex%20Ontology%20Alignments&body=Title%3A%20Enhancing%20SPARQL%20Query%20Rewriting%20for%20Complex%20Ontology%20Alignments%0AAuthor%3A%20Anicet%20Lepetit%20Ondo%20and%20Laurence%20Capus%20and%20Mamadou%20Bousso%0AAbstract%3A%20%20%20SPARQL%20query%20rewriting%20is%20a%20fundamental%20mechanism%20for%20uniformly%20querying%0Aheterogeneous%20ontologies%20in%20the%20Linked%20Data%20Web.%20However%2C%20the%20complexity%20of%0Aontology%20alignments%2C%20particularly%20rich%20correspondences%20%28c%20%3A%20c%29%2C%20makes%20this%0Aprocess%20challenging.%20Existing%20approaches%20primarily%20focus%20on%20simple%20%28s%20%3A%20s%29%20and%0Apartially%20complex%20%28%20s%20%3A%20c%29%20alignments%2C%20thereby%20overlooking%20the%20challenges%20posed%0Aby%20more%20expressive%20alignments.%20Moreover%2C%20the%20intricate%20syntax%20of%20SPARQL%0Apresents%20a%20barrier%20for%20non-expert%20users%20seeking%20to%20fully%20exploit%20the%20knowledge%0Aencapsulated%20in%20ontologies.%20This%20article%20proposes%20an%20innovative%20approach%20for%0Athe%20automatic%20rewriting%20of%20SPARQL%20queries%20from%20a%20source%20ontology%20to%20a%20target%0Aontology%2C%20based%20on%20a%20user%27s%20need%20expressed%20in%20natural%20language.%20It%20leverages%0Athe%20principles%20of%20equivalence%20transitivity%20as%20well%20as%20the%20advanced%20capabilities%0Aof%20large%20language%20models%20such%20as%20GPT-4.%20By%20integrating%20these%20elements%2C%20this%0Aapproach%20stands%20out%20for%20its%20ability%20to%20efficiently%20handle%20complex%20alignments%2C%0Aparticularly%20%28c%20%3A%20c%29%20correspondences%20%2C%20by%20fully%20exploiting%20their%0Aexpressiveness.%20Additionally%2C%20it%20facilitates%20access%20to%20aligned%20ontologies%20for%0Ausers%20unfamiliar%20with%20SPARQL%2C%20providing%20a%20flexible%20solution%20for%20querying%0Aheterogeneous%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520SPARQL%2520Query%2520Rewriting%2520for%2520Complex%2520Ontology%2520Alignments%26entry.906535625%3DAnicet%2520Lepetit%2520Ondo%2520and%2520Laurence%2520Capus%2520and%2520Mamadou%2520Bousso%26entry.1292438233%3D%2520%2520SPARQL%2520query%2520rewriting%2520is%2520a%2520fundamental%2520mechanism%2520for%2520uniformly%2520querying%250Aheterogeneous%2520ontologies%2520in%2520the%2520Linked%2520Data%2520Web.%2520However%252C%2520the%2520complexity%2520of%250Aontology%2520alignments%252C%2520particularly%2520rich%2520correspondences%2520%2528c%2520%253A%2520c%2529%252C%2520makes%2520this%250Aprocess%2520challenging.%2520Existing%2520approaches%2520primarily%2520focus%2520on%2520simple%2520%2528s%2520%253A%2520s%2529%2520and%250Apartially%2520complex%2520%2528%2520s%2520%253A%2520c%2529%2520alignments%252C%2520thereby%2520overlooking%2520the%2520challenges%2520posed%250Aby%2520more%2520expressive%2520alignments.%2520Moreover%252C%2520the%2520intricate%2520syntax%2520of%2520SPARQL%250Apresents%2520a%2520barrier%2520for%2520non-expert%2520users%2520seeking%2520to%2520fully%2520exploit%2520the%2520knowledge%250Aencapsulated%2520in%2520ontologies.%2520This%2520article%2520proposes%2520an%2520innovative%2520approach%2520for%250Athe%2520automatic%2520rewriting%2520of%2520SPARQL%2520queries%2520from%2520a%2520source%2520ontology%2520to%2520a%2520target%250Aontology%252C%2520based%2520on%2520a%2520user%2527s%2520need%2520expressed%2520in%2520natural%2520language.%2520It%2520leverages%250Athe%2520principles%2520of%2520equivalence%2520transitivity%2520as%2520well%2520as%2520the%2520advanced%2520capabilities%250Aof%2520large%2520language%2520models%2520such%2520as%2520GPT-4.%2520By%2520integrating%2520these%2520elements%252C%2520this%250Aapproach%2520stands%2520out%2520for%2520its%2520ability%2520to%2520efficiently%2520handle%2520complex%2520alignments%252C%250Aparticularly%2520%2528c%2520%253A%2520c%2529%2520correspondences%2520%252C%2520by%2520fully%2520exploiting%2520their%250Aexpressiveness.%2520Additionally%252C%2520it%2520facilitates%2520access%2520to%2520aligned%2520ontologies%2520for%250Ausers%2520unfamiliar%2520with%2520SPARQL%252C%2520providing%2520a%2520flexible%2520solution%2520for%2520querying%250Aheterogeneous%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20SPARQL%20Query%20Rewriting%20for%20Complex%20Ontology%20Alignments&entry.906535625=Anicet%20Lepetit%20Ondo%20and%20Laurence%20Capus%20and%20Mamadou%20Bousso&entry.1292438233=%20%20SPARQL%20query%20rewriting%20is%20a%20fundamental%20mechanism%20for%20uniformly%20querying%0Aheterogeneous%20ontologies%20in%20the%20Linked%20Data%20Web.%20However%2C%20the%20complexity%20of%0Aontology%20alignments%2C%20particularly%20rich%20correspondences%20%28c%20%3A%20c%29%2C%20makes%20this%0Aprocess%20challenging.%20Existing%20approaches%20primarily%20focus%20on%20simple%20%28s%20%3A%20s%29%20and%0Apartially%20complex%20%28%20s%20%3A%20c%29%20alignments%2C%20thereby%20overlooking%20the%20challenges%20posed%0Aby%20more%20expressive%20alignments.%20Moreover%2C%20the%20intricate%20syntax%20of%20SPARQL%0Apresents%20a%20barrier%20for%20non-expert%20users%20seeking%20to%20fully%20exploit%20the%20knowledge%0Aencapsulated%20in%20ontologies.%20This%20article%20proposes%20an%20innovative%20approach%20for%0Athe%20automatic%20rewriting%20of%20SPARQL%20queries%20from%20a%20source%20ontology%20to%20a%20target%0Aontology%2C%20based%20on%20a%20user%27s%20need%20expressed%20in%20natural%20language.%20It%20leverages%0Athe%20principles%20of%20equivalence%20transitivity%20as%20well%20as%20the%20advanced%20capabilities%0Aof%20large%20language%20models%20such%20as%20GPT-4.%20By%20integrating%20these%20elements%2C%20this%0Aapproach%20stands%20out%20for%20its%20ability%20to%20efficiently%20handle%20complex%20alignments%2C%0Aparticularly%20%28c%20%3A%20c%29%20correspondences%20%2C%20by%20fully%20exploiting%20their%0Aexpressiveness.%20Additionally%2C%20it%20facilitates%20access%20to%20aligned%20ontologies%20for%0Ausers%20unfamiliar%20with%20SPARQL%2C%20providing%20a%20flexible%20solution%20for%20querying%0Aheterogeneous%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01309v1&entry.124074799=Read"},
{"title": "How Effective are Large Time Series Models in Hydrology? A Study on\n  Water Level Forecasting in Everglades", "author": "Rahuul Rangaraj and Jimeng Shi and Azam Shirali and Rajendra Paudel and Yanzhao Wu and Giri Narasimhan", "abstract": "  The Everglades play a crucial role in flood and drought regulation, water\nresource planning, and ecosystem management in the surrounding regions.\nHowever, traditional physics-based and statistical methods for predicting water\nlevels often face significant challenges, including high computational costs\nand limited adaptability to diverse or unforeseen conditions. Recent\nadvancements in large time series models have demonstrated the potential to\naddress these limitations, with state-of-the-art deep learning and foundation\nmodels achieving remarkable success in time series forecasting across various\ndomains. Despite this progress, their application to critical environmental\nsystems, such as the Everglades, remains underexplored. In this study, we fill\nthe gap by investigating twelve task-specific models and five time series\nfoundation models across six categories for a real-world application focused on\nwater level prediction in the Everglades. Our primary results show that the\nfoundation model, Chronos, significantly outperforms all other models while the\nremaining foundation models exhibit relatively poor performance. Moreover, the\nperformance of task-specific models varies with the model architectures.\nLastly, we discuss the possible reasons for the varying performance of models.\n", "link": "http://arxiv.org/abs/2505.01415v1", "date": "2025-05-02", "relevancy": 1.81, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Effective%20are%20Large%20Time%20Series%20Models%20in%20Hydrology%3F%20A%20Study%20on%0A%20%20Water%20Level%20Forecasting%20in%20Everglades&body=Title%3A%20How%20Effective%20are%20Large%20Time%20Series%20Models%20in%20Hydrology%3F%20A%20Study%20on%0A%20%20Water%20Level%20Forecasting%20in%20Everglades%0AAuthor%3A%20Rahuul%20Rangaraj%20and%20Jimeng%20Shi%20and%20Azam%20Shirali%20and%20Rajendra%20Paudel%20and%20Yanzhao%20Wu%20and%20Giri%20Narasimhan%0AAbstract%3A%20%20%20The%20Everglades%20play%20a%20crucial%20role%20in%20flood%20and%20drought%20regulation%2C%20water%0Aresource%20planning%2C%20and%20ecosystem%20management%20in%20the%20surrounding%20regions.%0AHowever%2C%20traditional%20physics-based%20and%20statistical%20methods%20for%20predicting%20water%0Alevels%20often%20face%20significant%20challenges%2C%20including%20high%20computational%20costs%0Aand%20limited%20adaptability%20to%20diverse%20or%20unforeseen%20conditions.%20Recent%0Aadvancements%20in%20large%20time%20series%20models%20have%20demonstrated%20the%20potential%20to%0Aaddress%20these%20limitations%2C%20with%20state-of-the-art%20deep%20learning%20and%20foundation%0Amodels%20achieving%20remarkable%20success%20in%20time%20series%20forecasting%20across%20various%0Adomains.%20Despite%20this%20progress%2C%20their%20application%20to%20critical%20environmental%0Asystems%2C%20such%20as%20the%20Everglades%2C%20remains%20underexplored.%20In%20this%20study%2C%20we%20fill%0Athe%20gap%20by%20investigating%20twelve%20task-specific%20models%20and%20five%20time%20series%0Afoundation%20models%20across%20six%20categories%20for%20a%20real-world%20application%20focused%20on%0Awater%20level%20prediction%20in%20the%20Everglades.%20Our%20primary%20results%20show%20that%20the%0Afoundation%20model%2C%20Chronos%2C%20significantly%20outperforms%20all%20other%20models%20while%20the%0Aremaining%20foundation%20models%20exhibit%20relatively%20poor%20performance.%20Moreover%2C%20the%0Aperformance%20of%20task-specific%20models%20varies%20with%20the%20model%20architectures.%0ALastly%2C%20we%20discuss%20the%20possible%20reasons%20for%20the%20varying%20performance%20of%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Effective%2520are%2520Large%2520Time%2520Series%2520Models%2520in%2520Hydrology%253F%2520A%2520Study%2520on%250A%2520%2520Water%2520Level%2520Forecasting%2520in%2520Everglades%26entry.906535625%3DRahuul%2520Rangaraj%2520and%2520Jimeng%2520Shi%2520and%2520Azam%2520Shirali%2520and%2520Rajendra%2520Paudel%2520and%2520Yanzhao%2520Wu%2520and%2520Giri%2520Narasimhan%26entry.1292438233%3D%2520%2520The%2520Everglades%2520play%2520a%2520crucial%2520role%2520in%2520flood%2520and%2520drought%2520regulation%252C%2520water%250Aresource%2520planning%252C%2520and%2520ecosystem%2520management%2520in%2520the%2520surrounding%2520regions.%250AHowever%252C%2520traditional%2520physics-based%2520and%2520statistical%2520methods%2520for%2520predicting%2520water%250Alevels%2520often%2520face%2520significant%2520challenges%252C%2520including%2520high%2520computational%2520costs%250Aand%2520limited%2520adaptability%2520to%2520diverse%2520or%2520unforeseen%2520conditions.%2520Recent%250Aadvancements%2520in%2520large%2520time%2520series%2520models%2520have%2520demonstrated%2520the%2520potential%2520to%250Aaddress%2520these%2520limitations%252C%2520with%2520state-of-the-art%2520deep%2520learning%2520and%2520foundation%250Amodels%2520achieving%2520remarkable%2520success%2520in%2520time%2520series%2520forecasting%2520across%2520various%250Adomains.%2520Despite%2520this%2520progress%252C%2520their%2520application%2520to%2520critical%2520environmental%250Asystems%252C%2520such%2520as%2520the%2520Everglades%252C%2520remains%2520underexplored.%2520In%2520this%2520study%252C%2520we%2520fill%250Athe%2520gap%2520by%2520investigating%2520twelve%2520task-specific%2520models%2520and%2520five%2520time%2520series%250Afoundation%2520models%2520across%2520six%2520categories%2520for%2520a%2520real-world%2520application%2520focused%2520on%250Awater%2520level%2520prediction%2520in%2520the%2520Everglades.%2520Our%2520primary%2520results%2520show%2520that%2520the%250Afoundation%2520model%252C%2520Chronos%252C%2520significantly%2520outperforms%2520all%2520other%2520models%2520while%2520the%250Aremaining%2520foundation%2520models%2520exhibit%2520relatively%2520poor%2520performance.%2520Moreover%252C%2520the%250Aperformance%2520of%2520task-specific%2520models%2520varies%2520with%2520the%2520model%2520architectures.%250ALastly%252C%2520we%2520discuss%2520the%2520possible%2520reasons%2520for%2520the%2520varying%2520performance%2520of%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Effective%20are%20Large%20Time%20Series%20Models%20in%20Hydrology%3F%20A%20Study%20on%0A%20%20Water%20Level%20Forecasting%20in%20Everglades&entry.906535625=Rahuul%20Rangaraj%20and%20Jimeng%20Shi%20and%20Azam%20Shirali%20and%20Rajendra%20Paudel%20and%20Yanzhao%20Wu%20and%20Giri%20Narasimhan&entry.1292438233=%20%20The%20Everglades%20play%20a%20crucial%20role%20in%20flood%20and%20drought%20regulation%2C%20water%0Aresource%20planning%2C%20and%20ecosystem%20management%20in%20the%20surrounding%20regions.%0AHowever%2C%20traditional%20physics-based%20and%20statistical%20methods%20for%20predicting%20water%0Alevels%20often%20face%20significant%20challenges%2C%20including%20high%20computational%20costs%0Aand%20limited%20adaptability%20to%20diverse%20or%20unforeseen%20conditions.%20Recent%0Aadvancements%20in%20large%20time%20series%20models%20have%20demonstrated%20the%20potential%20to%0Aaddress%20these%20limitations%2C%20with%20state-of-the-art%20deep%20learning%20and%20foundation%0Amodels%20achieving%20remarkable%20success%20in%20time%20series%20forecasting%20across%20various%0Adomains.%20Despite%20this%20progress%2C%20their%20application%20to%20critical%20environmental%0Asystems%2C%20such%20as%20the%20Everglades%2C%20remains%20underexplored.%20In%20this%20study%2C%20we%20fill%0Athe%20gap%20by%20investigating%20twelve%20task-specific%20models%20and%20five%20time%20series%0Afoundation%20models%20across%20six%20categories%20for%20a%20real-world%20application%20focused%20on%0Awater%20level%20prediction%20in%20the%20Everglades.%20Our%20primary%20results%20show%20that%20the%0Afoundation%20model%2C%20Chronos%2C%20significantly%20outperforms%20all%20other%20models%20while%20the%0Aremaining%20foundation%20models%20exhibit%20relatively%20poor%20performance.%20Moreover%2C%20the%0Aperformance%20of%20task-specific%20models%20varies%20with%20the%20model%20architectures.%0ALastly%2C%20we%20discuss%20the%20possible%20reasons%20for%20the%20varying%20performance%20of%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01415v1&entry.124074799=Read"},
{"title": "Gender Bias in Explainability: Investigating Performance Disparity in\n  Post-hoc Methods", "author": "Mahdi Dhaini and Ege Erdogan and Nils Feldhus and Gjergji Kasneci", "abstract": "  While research on applications and evaluations of explanation methods\ncontinues to expand, fairness of the explanation methods concerning disparities\nin their performance across subgroups remains an often overlooked aspect. In\nthis paper, we address this gap by showing that, across three tasks and five\nlanguage models, widely used post-hoc feature attribution methods exhibit\nsignificant gender disparity with respect to their faithfulness, robustness,\nand complexity. These disparities persist even when the models are pre-trained\nor fine-tuned on particularly unbiased datasets, indicating that the\ndisparities we observe are not merely consequences of biased training data. Our\nresults highlight the importance of addressing disparities in explanations when\ndeveloping and applying explainability methods, as these can lead to biased\noutcomes against certain subgroups, with particularly critical implications in\nhigh-stakes contexts. Furthermore, our findings underscore the importance of\nincorporating the fairness of explanations, alongside overall model fairness\nand explainability, as a requirement in regulatory frameworks.\n", "link": "http://arxiv.org/abs/2505.01198v1", "date": "2025-05-02", "relevancy": 1.7727, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.446}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gender%20Bias%20in%20Explainability%3A%20Investigating%20Performance%20Disparity%20in%0A%20%20Post-hoc%20Methods&body=Title%3A%20Gender%20Bias%20in%20Explainability%3A%20Investigating%20Performance%20Disparity%20in%0A%20%20Post-hoc%20Methods%0AAuthor%3A%20Mahdi%20Dhaini%20and%20Ege%20Erdogan%20and%20Nils%20Feldhus%20and%20Gjergji%20Kasneci%0AAbstract%3A%20%20%20While%20research%20on%20applications%20and%20evaluations%20of%20explanation%20methods%0Acontinues%20to%20expand%2C%20fairness%20of%20the%20explanation%20methods%20concerning%20disparities%0Ain%20their%20performance%20across%20subgroups%20remains%20an%20often%20overlooked%20aspect.%20In%0Athis%20paper%2C%20we%20address%20this%20gap%20by%20showing%20that%2C%20across%20three%20tasks%20and%20five%0Alanguage%20models%2C%20widely%20used%20post-hoc%20feature%20attribution%20methods%20exhibit%0Asignificant%20gender%20disparity%20with%20respect%20to%20their%20faithfulness%2C%20robustness%2C%0Aand%20complexity.%20These%20disparities%20persist%20even%20when%20the%20models%20are%20pre-trained%0Aor%20fine-tuned%20on%20particularly%20unbiased%20datasets%2C%20indicating%20that%20the%0Adisparities%20we%20observe%20are%20not%20merely%20consequences%20of%20biased%20training%20data.%20Our%0Aresults%20highlight%20the%20importance%20of%20addressing%20disparities%20in%20explanations%20when%0Adeveloping%20and%20applying%20explainability%20methods%2C%20as%20these%20can%20lead%20to%20biased%0Aoutcomes%20against%20certain%20subgroups%2C%20with%20particularly%20critical%20implications%20in%0Ahigh-stakes%20contexts.%20Furthermore%2C%20our%20findings%20underscore%20the%20importance%20of%0Aincorporating%20the%20fairness%20of%20explanations%2C%20alongside%20overall%20model%20fairness%0Aand%20explainability%2C%20as%20a%20requirement%20in%20regulatory%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGender%2520Bias%2520in%2520Explainability%253A%2520Investigating%2520Performance%2520Disparity%2520in%250A%2520%2520Post-hoc%2520Methods%26entry.906535625%3DMahdi%2520Dhaini%2520and%2520Ege%2520Erdogan%2520and%2520Nils%2520Feldhus%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3D%2520%2520While%2520research%2520on%2520applications%2520and%2520evaluations%2520of%2520explanation%2520methods%250Acontinues%2520to%2520expand%252C%2520fairness%2520of%2520the%2520explanation%2520methods%2520concerning%2520disparities%250Ain%2520their%2520performance%2520across%2520subgroups%2520remains%2520an%2520often%2520overlooked%2520aspect.%2520In%250Athis%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520showing%2520that%252C%2520across%2520three%2520tasks%2520and%2520five%250Alanguage%2520models%252C%2520widely%2520used%2520post-hoc%2520feature%2520attribution%2520methods%2520exhibit%250Asignificant%2520gender%2520disparity%2520with%2520respect%2520to%2520their%2520faithfulness%252C%2520robustness%252C%250Aand%2520complexity.%2520These%2520disparities%2520persist%2520even%2520when%2520the%2520models%2520are%2520pre-trained%250Aor%2520fine-tuned%2520on%2520particularly%2520unbiased%2520datasets%252C%2520indicating%2520that%2520the%250Adisparities%2520we%2520observe%2520are%2520not%2520merely%2520consequences%2520of%2520biased%2520training%2520data.%2520Our%250Aresults%2520highlight%2520the%2520importance%2520of%2520addressing%2520disparities%2520in%2520explanations%2520when%250Adeveloping%2520and%2520applying%2520explainability%2520methods%252C%2520as%2520these%2520can%2520lead%2520to%2520biased%250Aoutcomes%2520against%2520certain%2520subgroups%252C%2520with%2520particularly%2520critical%2520implications%2520in%250Ahigh-stakes%2520contexts.%2520Furthermore%252C%2520our%2520findings%2520underscore%2520the%2520importance%2520of%250Aincorporating%2520the%2520fairness%2520of%2520explanations%252C%2520alongside%2520overall%2520model%2520fairness%250Aand%2520explainability%252C%2520as%2520a%2520requirement%2520in%2520regulatory%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gender%20Bias%20in%20Explainability%3A%20Investigating%20Performance%20Disparity%20in%0A%20%20Post-hoc%20Methods&entry.906535625=Mahdi%20Dhaini%20and%20Ege%20Erdogan%20and%20Nils%20Feldhus%20and%20Gjergji%20Kasneci&entry.1292438233=%20%20While%20research%20on%20applications%20and%20evaluations%20of%20explanation%20methods%0Acontinues%20to%20expand%2C%20fairness%20of%20the%20explanation%20methods%20concerning%20disparities%0Ain%20their%20performance%20across%20subgroups%20remains%20an%20often%20overlooked%20aspect.%20In%0Athis%20paper%2C%20we%20address%20this%20gap%20by%20showing%20that%2C%20across%20three%20tasks%20and%20five%0Alanguage%20models%2C%20widely%20used%20post-hoc%20feature%20attribution%20methods%20exhibit%0Asignificant%20gender%20disparity%20with%20respect%20to%20their%20faithfulness%2C%20robustness%2C%0Aand%20complexity.%20These%20disparities%20persist%20even%20when%20the%20models%20are%20pre-trained%0Aor%20fine-tuned%20on%20particularly%20unbiased%20datasets%2C%20indicating%20that%20the%0Adisparities%20we%20observe%20are%20not%20merely%20consequences%20of%20biased%20training%20data.%20Our%0Aresults%20highlight%20the%20importance%20of%20addressing%20disparities%20in%20explanations%20when%0Adeveloping%20and%20applying%20explainability%20methods%2C%20as%20these%20can%20lead%20to%20biased%0Aoutcomes%20against%20certain%20subgroups%2C%20with%20particularly%20critical%20implications%20in%0Ahigh-stakes%20contexts.%20Furthermore%2C%20our%20findings%20underscore%20the%20importance%20of%0Aincorporating%20the%20fairness%20of%20explanations%2C%20alongside%20overall%20model%20fairness%0Aand%20explainability%2C%20as%20a%20requirement%20in%20regulatory%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01198v1&entry.124074799=Read"},
{"title": "How to Learn a Star: Binary Classification with Starshaped Polyhedral\n  Sets", "author": "Marie-Charlotte Brandenburg and Katharina Jochemko", "abstract": "  We consider binary classification restricted to a class of continuous\npiecewise linear functions whose decision boundaries are (possibly nonconvex)\nstarshaped polyhedral sets, supported on a fixed polyhedral simplicial fan. We\ninvestigate the expressivity of these function classes and describe the\ncombinatorial and geometric structure of the loss landscape, most prominently\nthe sublevel sets, for two loss-functions: the 0/1-loss (discrete loss) and an\nexponential loss function. In particular, we give explicit bounds on the VC\ndimension of this model, and concretely describe the sublevel sets of the\ndiscrete loss as chambers in a hyperplane arrangement. For the exponential\nloss, we give sufficient conditions for the optimum to be unique, and describe\nthe geometry of the optimum when varying the rate parameter of the underlying\nexponential probability distribution.\n", "link": "http://arxiv.org/abs/2505.01346v1", "date": "2025-05-02", "relevancy": 1.7578, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Learn%20a%20Star%3A%20Binary%20Classification%20with%20Starshaped%20Polyhedral%0A%20%20Sets&body=Title%3A%20How%20to%20Learn%20a%20Star%3A%20Binary%20Classification%20with%20Starshaped%20Polyhedral%0A%20%20Sets%0AAuthor%3A%20Marie-Charlotte%20Brandenburg%20and%20Katharina%20Jochemko%0AAbstract%3A%20%20%20We%20consider%20binary%20classification%20restricted%20to%20a%20class%20of%20continuous%0Apiecewise%20linear%20functions%20whose%20decision%20boundaries%20are%20%28possibly%20nonconvex%29%0Astarshaped%20polyhedral%20sets%2C%20supported%20on%20a%20fixed%20polyhedral%20simplicial%20fan.%20We%0Ainvestigate%20the%20expressivity%20of%20these%20function%20classes%20and%20describe%20the%0Acombinatorial%20and%20geometric%20structure%20of%20the%20loss%20landscape%2C%20most%20prominently%0Athe%20sublevel%20sets%2C%20for%20two%20loss-functions%3A%20the%200/1-loss%20%28discrete%20loss%29%20and%20an%0Aexponential%20loss%20function.%20In%20particular%2C%20we%20give%20explicit%20bounds%20on%20the%20VC%0Adimension%20of%20this%20model%2C%20and%20concretely%20describe%20the%20sublevel%20sets%20of%20the%0Adiscrete%20loss%20as%20chambers%20in%20a%20hyperplane%20arrangement.%20For%20the%20exponential%0Aloss%2C%20we%20give%20sufficient%20conditions%20for%20the%20optimum%20to%20be%20unique%2C%20and%20describe%0Athe%20geometry%20of%20the%20optimum%20when%20varying%20the%20rate%20parameter%20of%20the%20underlying%0Aexponential%20probability%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Learn%2520a%2520Star%253A%2520Binary%2520Classification%2520with%2520Starshaped%2520Polyhedral%250A%2520%2520Sets%26entry.906535625%3DMarie-Charlotte%2520Brandenburg%2520and%2520Katharina%2520Jochemko%26entry.1292438233%3D%2520%2520We%2520consider%2520binary%2520classification%2520restricted%2520to%2520a%2520class%2520of%2520continuous%250Apiecewise%2520linear%2520functions%2520whose%2520decision%2520boundaries%2520are%2520%2528possibly%2520nonconvex%2529%250Astarshaped%2520polyhedral%2520sets%252C%2520supported%2520on%2520a%2520fixed%2520polyhedral%2520simplicial%2520fan.%2520We%250Ainvestigate%2520the%2520expressivity%2520of%2520these%2520function%2520classes%2520and%2520describe%2520the%250Acombinatorial%2520and%2520geometric%2520structure%2520of%2520the%2520loss%2520landscape%252C%2520most%2520prominently%250Athe%2520sublevel%2520sets%252C%2520for%2520two%2520loss-functions%253A%2520the%25200/1-loss%2520%2528discrete%2520loss%2529%2520and%2520an%250Aexponential%2520loss%2520function.%2520In%2520particular%252C%2520we%2520give%2520explicit%2520bounds%2520on%2520the%2520VC%250Adimension%2520of%2520this%2520model%252C%2520and%2520concretely%2520describe%2520the%2520sublevel%2520sets%2520of%2520the%250Adiscrete%2520loss%2520as%2520chambers%2520in%2520a%2520hyperplane%2520arrangement.%2520For%2520the%2520exponential%250Aloss%252C%2520we%2520give%2520sufficient%2520conditions%2520for%2520the%2520optimum%2520to%2520be%2520unique%252C%2520and%2520describe%250Athe%2520geometry%2520of%2520the%2520optimum%2520when%2520varying%2520the%2520rate%2520parameter%2520of%2520the%2520underlying%250Aexponential%2520probability%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Learn%20a%20Star%3A%20Binary%20Classification%20with%20Starshaped%20Polyhedral%0A%20%20Sets&entry.906535625=Marie-Charlotte%20Brandenburg%20and%20Katharina%20Jochemko&entry.1292438233=%20%20We%20consider%20binary%20classification%20restricted%20to%20a%20class%20of%20continuous%0Apiecewise%20linear%20functions%20whose%20decision%20boundaries%20are%20%28possibly%20nonconvex%29%0Astarshaped%20polyhedral%20sets%2C%20supported%20on%20a%20fixed%20polyhedral%20simplicial%20fan.%20We%0Ainvestigate%20the%20expressivity%20of%20these%20function%20classes%20and%20describe%20the%0Acombinatorial%20and%20geometric%20structure%20of%20the%20loss%20landscape%2C%20most%20prominently%0Athe%20sublevel%20sets%2C%20for%20two%20loss-functions%3A%20the%200/1-loss%20%28discrete%20loss%29%20and%20an%0Aexponential%20loss%20function.%20In%20particular%2C%20we%20give%20explicit%20bounds%20on%20the%20VC%0Adimension%20of%20this%20model%2C%20and%20concretely%20describe%20the%20sublevel%20sets%20of%20the%0Adiscrete%20loss%20as%20chambers%20in%20a%20hyperplane%20arrangement.%20For%20the%20exponential%0Aloss%2C%20we%20give%20sufficient%20conditions%20for%20the%20optimum%20to%20be%20unique%2C%20and%20describe%0Athe%20geometry%20of%20the%20optimum%20when%20varying%20the%20rate%20parameter%20of%20the%20underlying%0Aexponential%20probability%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01346v1&entry.124074799=Read"},
{"title": "RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through\n  Embedding Predictive Pre-Training", "author": "Raktim Gautam Goswami and Prashanth Krishnamurthy and Yann LeCun and Farshad Khorrami", "abstract": "  Vision-based pose estimation of articulated robots with unknown joint angles\nhas applications in collaborative robotics and human-robot interaction tasks.\nCurrent frameworks use neural network encoders to extract image features and\ndownstream layers to predict joint angles and robot pose. While images of\nrobots inherently contain rich information about the robot's physical\nstructures, existing methods often fail to leverage it fully; therefore,\nlimiting performance under occlusions and truncations. To address this, we\nintroduce RoboPEPP, a method that fuses information about the robot's physical\nmodel into the encoder using a masking-based self-supervised\nembedding-predictive architecture. Specifically, we mask the robot's joints and\npre-train an encoder-predictor model to infer the joints' embeddings from\nsurrounding unmasked regions, enhancing the encoder's understanding of the\nrobot's physical model. The pre-trained encoder-predictor pair, along with\njoint angle and keypoint prediction networks, is then fine-tuned for pose and\njoint angle estimation. Random masking of input during fine-tuning and keypoint\nfiltering during evaluation further improves robustness. Our method, evaluated\non several datasets, achieves the best results in robot pose and joint angle\nestimation while being the least sensitive to occlusions and requiring the\nlowest execution time.\n", "link": "http://arxiv.org/abs/2411.17662v2", "date": "2025-05-02", "relevancy": 1.7249, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.625}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5651}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboPEPP%3A%20Vision-Based%20Robot%20Pose%20and%20Joint%20Angle%20Estimation%20through%0A%20%20Embedding%20Predictive%20Pre-Training&body=Title%3A%20RoboPEPP%3A%20Vision-Based%20Robot%20Pose%20and%20Joint%20Angle%20Estimation%20through%0A%20%20Embedding%20Predictive%20Pre-Training%0AAuthor%3A%20Raktim%20Gautam%20Goswami%20and%20Prashanth%20Krishnamurthy%20and%20Yann%20LeCun%20and%20Farshad%20Khorrami%0AAbstract%3A%20%20%20Vision-based%20pose%20estimation%20of%20articulated%20robots%20with%20unknown%20joint%20angles%0Ahas%20applications%20in%20collaborative%20robotics%20and%20human-robot%20interaction%20tasks.%0ACurrent%20frameworks%20use%20neural%20network%20encoders%20to%20extract%20image%20features%20and%0Adownstream%20layers%20to%20predict%20joint%20angles%20and%20robot%20pose.%20While%20images%20of%0Arobots%20inherently%20contain%20rich%20information%20about%20the%20robot%27s%20physical%0Astructures%2C%20existing%20methods%20often%20fail%20to%20leverage%20it%20fully%3B%20therefore%2C%0Alimiting%20performance%20under%20occlusions%20and%20truncations.%20To%20address%20this%2C%20we%0Aintroduce%20RoboPEPP%2C%20a%20method%20that%20fuses%20information%20about%20the%20robot%27s%20physical%0Amodel%20into%20the%20encoder%20using%20a%20masking-based%20self-supervised%0Aembedding-predictive%20architecture.%20Specifically%2C%20we%20mask%20the%20robot%27s%20joints%20and%0Apre-train%20an%20encoder-predictor%20model%20to%20infer%20the%20joints%27%20embeddings%20from%0Asurrounding%20unmasked%20regions%2C%20enhancing%20the%20encoder%27s%20understanding%20of%20the%0Arobot%27s%20physical%20model.%20The%20pre-trained%20encoder-predictor%20pair%2C%20along%20with%0Ajoint%20angle%20and%20keypoint%20prediction%20networks%2C%20is%20then%20fine-tuned%20for%20pose%20and%0Ajoint%20angle%20estimation.%20Random%20masking%20of%20input%20during%20fine-tuning%20and%20keypoint%0Afiltering%20during%20evaluation%20further%20improves%20robustness.%20Our%20method%2C%20evaluated%0Aon%20several%20datasets%2C%20achieves%20the%20best%20results%20in%20robot%20pose%20and%20joint%20angle%0Aestimation%20while%20being%20the%20least%20sensitive%20to%20occlusions%20and%20requiring%20the%0Alowest%20execution%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboPEPP%253A%2520Vision-Based%2520Robot%2520Pose%2520and%2520Joint%2520Angle%2520Estimation%2520through%250A%2520%2520Embedding%2520Predictive%2520Pre-Training%26entry.906535625%3DRaktim%2520Gautam%2520Goswami%2520and%2520Prashanth%2520Krishnamurthy%2520and%2520Yann%2520LeCun%2520and%2520Farshad%2520Khorrami%26entry.1292438233%3D%2520%2520Vision-based%2520pose%2520estimation%2520of%2520articulated%2520robots%2520with%2520unknown%2520joint%2520angles%250Ahas%2520applications%2520in%2520collaborative%2520robotics%2520and%2520human-robot%2520interaction%2520tasks.%250ACurrent%2520frameworks%2520use%2520neural%2520network%2520encoders%2520to%2520extract%2520image%2520features%2520and%250Adownstream%2520layers%2520to%2520predict%2520joint%2520angles%2520and%2520robot%2520pose.%2520While%2520images%2520of%250Arobots%2520inherently%2520contain%2520rich%2520information%2520about%2520the%2520robot%2527s%2520physical%250Astructures%252C%2520existing%2520methods%2520often%2520fail%2520to%2520leverage%2520it%2520fully%253B%2520therefore%252C%250Alimiting%2520performance%2520under%2520occlusions%2520and%2520truncations.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520RoboPEPP%252C%2520a%2520method%2520that%2520fuses%2520information%2520about%2520the%2520robot%2527s%2520physical%250Amodel%2520into%2520the%2520encoder%2520using%2520a%2520masking-based%2520self-supervised%250Aembedding-predictive%2520architecture.%2520Specifically%252C%2520we%2520mask%2520the%2520robot%2527s%2520joints%2520and%250Apre-train%2520an%2520encoder-predictor%2520model%2520to%2520infer%2520the%2520joints%2527%2520embeddings%2520from%250Asurrounding%2520unmasked%2520regions%252C%2520enhancing%2520the%2520encoder%2527s%2520understanding%2520of%2520the%250Arobot%2527s%2520physical%2520model.%2520The%2520pre-trained%2520encoder-predictor%2520pair%252C%2520along%2520with%250Ajoint%2520angle%2520and%2520keypoint%2520prediction%2520networks%252C%2520is%2520then%2520fine-tuned%2520for%2520pose%2520and%250Ajoint%2520angle%2520estimation.%2520Random%2520masking%2520of%2520input%2520during%2520fine-tuning%2520and%2520keypoint%250Afiltering%2520during%2520evaluation%2520further%2520improves%2520robustness.%2520Our%2520method%252C%2520evaluated%250Aon%2520several%2520datasets%252C%2520achieves%2520the%2520best%2520results%2520in%2520robot%2520pose%2520and%2520joint%2520angle%250Aestimation%2520while%2520being%2520the%2520least%2520sensitive%2520to%2520occlusions%2520and%2520requiring%2520the%250Alowest%2520execution%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboPEPP%3A%20Vision-Based%20Robot%20Pose%20and%20Joint%20Angle%20Estimation%20through%0A%20%20Embedding%20Predictive%20Pre-Training&entry.906535625=Raktim%20Gautam%20Goswami%20and%20Prashanth%20Krishnamurthy%20and%20Yann%20LeCun%20and%20Farshad%20Khorrami&entry.1292438233=%20%20Vision-based%20pose%20estimation%20of%20articulated%20robots%20with%20unknown%20joint%20angles%0Ahas%20applications%20in%20collaborative%20robotics%20and%20human-robot%20interaction%20tasks.%0ACurrent%20frameworks%20use%20neural%20network%20encoders%20to%20extract%20image%20features%20and%0Adownstream%20layers%20to%20predict%20joint%20angles%20and%20robot%20pose.%20While%20images%20of%0Arobots%20inherently%20contain%20rich%20information%20about%20the%20robot%27s%20physical%0Astructures%2C%20existing%20methods%20often%20fail%20to%20leverage%20it%20fully%3B%20therefore%2C%0Alimiting%20performance%20under%20occlusions%20and%20truncations.%20To%20address%20this%2C%20we%0Aintroduce%20RoboPEPP%2C%20a%20method%20that%20fuses%20information%20about%20the%20robot%27s%20physical%0Amodel%20into%20the%20encoder%20using%20a%20masking-based%20self-supervised%0Aembedding-predictive%20architecture.%20Specifically%2C%20we%20mask%20the%20robot%27s%20joints%20and%0Apre-train%20an%20encoder-predictor%20model%20to%20infer%20the%20joints%27%20embeddings%20from%0Asurrounding%20unmasked%20regions%2C%20enhancing%20the%20encoder%27s%20understanding%20of%20the%0Arobot%27s%20physical%20model.%20The%20pre-trained%20encoder-predictor%20pair%2C%20along%20with%0Ajoint%20angle%20and%20keypoint%20prediction%20networks%2C%20is%20then%20fine-tuned%20for%20pose%20and%0Ajoint%20angle%20estimation.%20Random%20masking%20of%20input%20during%20fine-tuning%20and%20keypoint%0Afiltering%20during%20evaluation%20further%20improves%20robustness.%20Our%20method%2C%20evaluated%0Aon%20several%20datasets%2C%20achieves%20the%20best%20results%20in%20robot%20pose%20and%20joint%20angle%0Aestimation%20while%20being%20the%20least%20sensitive%20to%20occlusions%20and%20requiring%20the%0Alowest%20execution%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17662v2&entry.124074799=Read"},
{"title": "Minimum mean-squared error estimation with bandit feedback", "author": "Ayon Ghosh and L. A. Prashanth and Dipayan Sen and Aditya Gopalan", "abstract": "  We consider the problem of sequentially learning to estimate, in the mean\nsquared error (MSE) sense, a Gaussian $K$-vector of unknown covariance by\nobserving only $m < K$ of its entries in each round. We propose two MSE\nestimators, and analyze their concentration properties. The first estimator is\nnon-adaptive, as it is tied to a predetermined $m$-subset and lacks the\nflexibility to transition to alternative subsets. The second estimator, which\nis derived using a regression framework, is adaptive and exhibits better\nconcentration bounds in comparison to the first estimator. We frame the MSE\nestimation problem with bandit feedback, where the objective is to find the\nMSE-optimal subset with high confidence. We propose a variant of the successive\nelimination algorithm to solve this problem. We also derive a minimax lower\nbound to understand the fundamental limit on the sample complexity of this\nproblem.\n", "link": "http://arxiv.org/abs/2203.16810v4", "date": "2025-05-02", "relevancy": 1.7226, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4602}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4403}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimum%20mean-squared%20error%20estimation%20with%20bandit%20feedback&body=Title%3A%20Minimum%20mean-squared%20error%20estimation%20with%20bandit%20feedback%0AAuthor%3A%20Ayon%20Ghosh%20and%20L.%20A.%20Prashanth%20and%20Dipayan%20Sen%20and%20Aditya%20Gopalan%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20sequentially%20learning%20to%20estimate%2C%20in%20the%20mean%0Asquared%20error%20%28MSE%29%20sense%2C%20a%20Gaussian%20%24K%24-vector%20of%20unknown%20covariance%20by%0Aobserving%20only%20%24m%20%3C%20K%24%20of%20its%20entries%20in%20each%20round.%20We%20propose%20two%20MSE%0Aestimators%2C%20and%20analyze%20their%20concentration%20properties.%20The%20first%20estimator%20is%0Anon-adaptive%2C%20as%20it%20is%20tied%20to%20a%20predetermined%20%24m%24-subset%20and%20lacks%20the%0Aflexibility%20to%20transition%20to%20alternative%20subsets.%20The%20second%20estimator%2C%20which%0Ais%20derived%20using%20a%20regression%20framework%2C%20is%20adaptive%20and%20exhibits%20better%0Aconcentration%20bounds%20in%20comparison%20to%20the%20first%20estimator.%20We%20frame%20the%20MSE%0Aestimation%20problem%20with%20bandit%20feedback%2C%20where%20the%20objective%20is%20to%20find%20the%0AMSE-optimal%20subset%20with%20high%20confidence.%20We%20propose%20a%20variant%20of%20the%20successive%0Aelimination%20algorithm%20to%20solve%20this%20problem.%20We%20also%20derive%20a%20minimax%20lower%0Abound%20to%20understand%20the%20fundamental%20limit%20on%20the%20sample%20complexity%20of%20this%0Aproblem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.16810v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimum%2520mean-squared%2520error%2520estimation%2520with%2520bandit%2520feedback%26entry.906535625%3DAyon%2520Ghosh%2520and%2520L.%2520A.%2520Prashanth%2520and%2520Dipayan%2520Sen%2520and%2520Aditya%2520Gopalan%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520sequentially%2520learning%2520to%2520estimate%252C%2520in%2520the%2520mean%250Asquared%2520error%2520%2528MSE%2529%2520sense%252C%2520a%2520Gaussian%2520%2524K%2524-vector%2520of%2520unknown%2520covariance%2520by%250Aobserving%2520only%2520%2524m%2520%253C%2520K%2524%2520of%2520its%2520entries%2520in%2520each%2520round.%2520We%2520propose%2520two%2520MSE%250Aestimators%252C%2520and%2520analyze%2520their%2520concentration%2520properties.%2520The%2520first%2520estimator%2520is%250Anon-adaptive%252C%2520as%2520it%2520is%2520tied%2520to%2520a%2520predetermined%2520%2524m%2524-subset%2520and%2520lacks%2520the%250Aflexibility%2520to%2520transition%2520to%2520alternative%2520subsets.%2520The%2520second%2520estimator%252C%2520which%250Ais%2520derived%2520using%2520a%2520regression%2520framework%252C%2520is%2520adaptive%2520and%2520exhibits%2520better%250Aconcentration%2520bounds%2520in%2520comparison%2520to%2520the%2520first%2520estimator.%2520We%2520frame%2520the%2520MSE%250Aestimation%2520problem%2520with%2520bandit%2520feedback%252C%2520where%2520the%2520objective%2520is%2520to%2520find%2520the%250AMSE-optimal%2520subset%2520with%2520high%2520confidence.%2520We%2520propose%2520a%2520variant%2520of%2520the%2520successive%250Aelimination%2520algorithm%2520to%2520solve%2520this%2520problem.%2520We%2520also%2520derive%2520a%2520minimax%2520lower%250Abound%2520to%2520understand%2520the%2520fundamental%2520limit%2520on%2520the%2520sample%2520complexity%2520of%2520this%250Aproblem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.16810v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimum%20mean-squared%20error%20estimation%20with%20bandit%20feedback&entry.906535625=Ayon%20Ghosh%20and%20L.%20A.%20Prashanth%20and%20Dipayan%20Sen%20and%20Aditya%20Gopalan&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20sequentially%20learning%20to%20estimate%2C%20in%20the%20mean%0Asquared%20error%20%28MSE%29%20sense%2C%20a%20Gaussian%20%24K%24-vector%20of%20unknown%20covariance%20by%0Aobserving%20only%20%24m%20%3C%20K%24%20of%20its%20entries%20in%20each%20round.%20We%20propose%20two%20MSE%0Aestimators%2C%20and%20analyze%20their%20concentration%20properties.%20The%20first%20estimator%20is%0Anon-adaptive%2C%20as%20it%20is%20tied%20to%20a%20predetermined%20%24m%24-subset%20and%20lacks%20the%0Aflexibility%20to%20transition%20to%20alternative%20subsets.%20The%20second%20estimator%2C%20which%0Ais%20derived%20using%20a%20regression%20framework%2C%20is%20adaptive%20and%20exhibits%20better%0Aconcentration%20bounds%20in%20comparison%20to%20the%20first%20estimator.%20We%20frame%20the%20MSE%0Aestimation%20problem%20with%20bandit%20feedback%2C%20where%20the%20objective%20is%20to%20find%20the%0AMSE-optimal%20subset%20with%20high%20confidence.%20We%20propose%20a%20variant%20of%20the%20successive%0Aelimination%20algorithm%20to%20solve%20this%20problem.%20We%20also%20derive%20a%20minimax%20lower%0Abound%20to%20understand%20the%20fundamental%20limit%20on%20the%20sample%20complexity%20of%20this%0Aproblem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.16810v4&entry.124074799=Read"},
{"title": "CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object\n  Tracking", "author": "Vladimir Somers and Baptiste Standaert and Victor Joos and Alexandre Alahi and Christophe De Vleeschouwer", "abstract": "  Online multi-object tracking has been recently dominated by\ntracking-by-detection (TbD) methods, where recent advances rely on increasingly\nsophisticated heuristics for tracklet representation, feature fusion, and\nmulti-stage matching. The key strength of TbD lies in its modular design,\nenabling the integration of specialized off-the-shelf models like motion\npredictors and re-identification. However, the extensive usage of human-crafted\nrules for temporal associations makes these methods inherently limited in their\nability to capture the complex interplay between various tracking cues. In this\nwork, we introduce CAMEL, a novel association module for Context-Aware\nMulti-Cue ExpLoitation, that learns resilient association strategies directly\nfrom data, breaking free from hand-crafted heuristics while maintaining TbD's\nvaluable modularity. At its core, CAMEL employs two transformer-based modules\nand relies on a novel association-centric training scheme to effectively model\nthe complex interactions between tracked targets and their various association\ncues. Unlike end-to-end detection-by-tracking approaches, our method remains\nlightweight and fast to train while being able to leverage external\noff-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,\nachieves state-of-the-art performance on multiple tracking benchmarks. Our code\nis available at https://github.com/TrackingLaboratory/CAMELTrack.\n", "link": "http://arxiv.org/abs/2505.01257v1", "date": "2025-05-02", "relevancy": 1.7043, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5788}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMELTrack%3A%20Context-Aware%20Multi-cue%20ExpLoitation%20for%20Online%20Multi-Object%0A%20%20Tracking&body=Title%3A%20CAMELTrack%3A%20Context-Aware%20Multi-cue%20ExpLoitation%20for%20Online%20Multi-Object%0A%20%20Tracking%0AAuthor%3A%20Vladimir%20Somers%20and%20Baptiste%20Standaert%20and%20Victor%20Joos%20and%20Alexandre%20Alahi%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20Online%20multi-object%20tracking%20has%20been%20recently%20dominated%20by%0Atracking-by-detection%20%28TbD%29%20methods%2C%20where%20recent%20advances%20rely%20on%20increasingly%0Asophisticated%20heuristics%20for%20tracklet%20representation%2C%20feature%20fusion%2C%20and%0Amulti-stage%20matching.%20The%20key%20strength%20of%20TbD%20lies%20in%20its%20modular%20design%2C%0Aenabling%20the%20integration%20of%20specialized%20off-the-shelf%20models%20like%20motion%0Apredictors%20and%20re-identification.%20However%2C%20the%20extensive%20usage%20of%20human-crafted%0Arules%20for%20temporal%20associations%20makes%20these%20methods%20inherently%20limited%20in%20their%0Aability%20to%20capture%20the%20complex%20interplay%20between%20various%20tracking%20cues.%20In%20this%0Awork%2C%20we%20introduce%20CAMEL%2C%20a%20novel%20association%20module%20for%20Context-Aware%0AMulti-Cue%20ExpLoitation%2C%20that%20learns%20resilient%20association%20strategies%20directly%0Afrom%20data%2C%20breaking%20free%20from%20hand-crafted%20heuristics%20while%20maintaining%20TbD%27s%0Avaluable%20modularity.%20At%20its%20core%2C%20CAMEL%20employs%20two%20transformer-based%20modules%0Aand%20relies%20on%20a%20novel%20association-centric%20training%20scheme%20to%20effectively%20model%0Athe%20complex%20interactions%20between%20tracked%20targets%20and%20their%20various%20association%0Acues.%20Unlike%20end-to-end%20detection-by-tracking%20approaches%2C%20our%20method%20remains%0Alightweight%20and%20fast%20to%20train%20while%20being%20able%20to%20leverage%20external%0Aoff-the-shelf%20models.%20Our%20proposed%20online%20tracking%20pipeline%2C%20CAMELTrack%2C%0Aachieves%20state-of-the-art%20performance%20on%20multiple%20tracking%20benchmarks.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/TrackingLaboratory/CAMELTrack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMELTrack%253A%2520Context-Aware%2520Multi-cue%2520ExpLoitation%2520for%2520Online%2520Multi-Object%250A%2520%2520Tracking%26entry.906535625%3DVladimir%2520Somers%2520and%2520Baptiste%2520Standaert%2520and%2520Victor%2520Joos%2520and%2520Alexandre%2520Alahi%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520Online%2520multi-object%2520tracking%2520has%2520been%2520recently%2520dominated%2520by%250Atracking-by-detection%2520%2528TbD%2529%2520methods%252C%2520where%2520recent%2520advances%2520rely%2520on%2520increasingly%250Asophisticated%2520heuristics%2520for%2520tracklet%2520representation%252C%2520feature%2520fusion%252C%2520and%250Amulti-stage%2520matching.%2520The%2520key%2520strength%2520of%2520TbD%2520lies%2520in%2520its%2520modular%2520design%252C%250Aenabling%2520the%2520integration%2520of%2520specialized%2520off-the-shelf%2520models%2520like%2520motion%250Apredictors%2520and%2520re-identification.%2520However%252C%2520the%2520extensive%2520usage%2520of%2520human-crafted%250Arules%2520for%2520temporal%2520associations%2520makes%2520these%2520methods%2520inherently%2520limited%2520in%2520their%250Aability%2520to%2520capture%2520the%2520complex%2520interplay%2520between%2520various%2520tracking%2520cues.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520CAMEL%252C%2520a%2520novel%2520association%2520module%2520for%2520Context-Aware%250AMulti-Cue%2520ExpLoitation%252C%2520that%2520learns%2520resilient%2520association%2520strategies%2520directly%250Afrom%2520data%252C%2520breaking%2520free%2520from%2520hand-crafted%2520heuristics%2520while%2520maintaining%2520TbD%2527s%250Avaluable%2520modularity.%2520At%2520its%2520core%252C%2520CAMEL%2520employs%2520two%2520transformer-based%2520modules%250Aand%2520relies%2520on%2520a%2520novel%2520association-centric%2520training%2520scheme%2520to%2520effectively%2520model%250Athe%2520complex%2520interactions%2520between%2520tracked%2520targets%2520and%2520their%2520various%2520association%250Acues.%2520Unlike%2520end-to-end%2520detection-by-tracking%2520approaches%252C%2520our%2520method%2520remains%250Alightweight%2520and%2520fast%2520to%2520train%2520while%2520being%2520able%2520to%2520leverage%2520external%250Aoff-the-shelf%2520models.%2520Our%2520proposed%2520online%2520tracking%2520pipeline%252C%2520CAMELTrack%252C%250Aachieves%2520state-of-the-art%2520performance%2520on%2520multiple%2520tracking%2520benchmarks.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/TrackingLaboratory/CAMELTrack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMELTrack%3A%20Context-Aware%20Multi-cue%20ExpLoitation%20for%20Online%20Multi-Object%0A%20%20Tracking&entry.906535625=Vladimir%20Somers%20and%20Baptiste%20Standaert%20and%20Victor%20Joos%20and%20Alexandre%20Alahi%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20Online%20multi-object%20tracking%20has%20been%20recently%20dominated%20by%0Atracking-by-detection%20%28TbD%29%20methods%2C%20where%20recent%20advances%20rely%20on%20increasingly%0Asophisticated%20heuristics%20for%20tracklet%20representation%2C%20feature%20fusion%2C%20and%0Amulti-stage%20matching.%20The%20key%20strength%20of%20TbD%20lies%20in%20its%20modular%20design%2C%0Aenabling%20the%20integration%20of%20specialized%20off-the-shelf%20models%20like%20motion%0Apredictors%20and%20re-identification.%20However%2C%20the%20extensive%20usage%20of%20human-crafted%0Arules%20for%20temporal%20associations%20makes%20these%20methods%20inherently%20limited%20in%20their%0Aability%20to%20capture%20the%20complex%20interplay%20between%20various%20tracking%20cues.%20In%20this%0Awork%2C%20we%20introduce%20CAMEL%2C%20a%20novel%20association%20module%20for%20Context-Aware%0AMulti-Cue%20ExpLoitation%2C%20that%20learns%20resilient%20association%20strategies%20directly%0Afrom%20data%2C%20breaking%20free%20from%20hand-crafted%20heuristics%20while%20maintaining%20TbD%27s%0Avaluable%20modularity.%20At%20its%20core%2C%20CAMEL%20employs%20two%20transformer-based%20modules%0Aand%20relies%20on%20a%20novel%20association-centric%20training%20scheme%20to%20effectively%20model%0Athe%20complex%20interactions%20between%20tracked%20targets%20and%20their%20various%20association%0Acues.%20Unlike%20end-to-end%20detection-by-tracking%20approaches%2C%20our%20method%20remains%0Alightweight%20and%20fast%20to%20train%20while%20being%20able%20to%20leverage%20external%0Aoff-the-shelf%20models.%20Our%20proposed%20online%20tracking%20pipeline%2C%20CAMELTrack%2C%0Aachieves%20state-of-the-art%20performance%20on%20multiple%20tracking%20benchmarks.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/TrackingLaboratory/CAMELTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01257v1&entry.124074799=Read"},
{"title": "Logical Characterizations of Recurrent Graph Neural Networks with Reals\n  and Floats", "author": "Veeti Ahvonen and Damian Heiman and Antti Kuusisto and Carsten Lutz", "abstract": "  In pioneering work from 2019, Barcel\\'o and coauthors identified logics that\nprecisely match the expressive power of constant iteration-depth graph neural\nnetworks (GNNs) relative to properties definable in first-order logic. In this\narticle, we give exact logical characterizations of recurrent GNNs in two\nscenarios: (1) in the setting with floating-point numbers and (2) with reals.\nFor floats, the formalism matching recurrent GNNs is a rule-based modal logic\nwith counting, while for reals we use a suitable infinitary modal logic, also\nwith counting. These results give exact matches between logics and GNNs in the\nrecurrent setting without relativising to a background logic in either case,\nbut using some natural assumptions about floating-point arithmetic. Applying\nour characterizations, we also prove that, relative to graph properties\ndefinable in monadic second-order logic (MSO), our infinitary and rule-based\nlogics are equally expressive. This implies that recurrent GNNs with reals and\nfloats have the same expressive power over MSO-definable properties and shows\nthat, for such properties, also recurrent GNNs with reals are characterized by\na (finitary!) rule-based modal logic. In the general case, in contrast, the\nexpressive power with floats is weaker than with reals. In addition to\nlogic-oriented results, we also characterize recurrent GNNs, with both reals\nand floats, via distributed automata, drawing links to distributed computing\nmodels.\n", "link": "http://arxiv.org/abs/2405.14606v4", "date": "2025-05-02", "relevancy": 1.6995, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4603}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4178}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logical%20Characterizations%20of%20Recurrent%20Graph%20Neural%20Networks%20with%20Reals%0A%20%20and%20Floats&body=Title%3A%20Logical%20Characterizations%20of%20Recurrent%20Graph%20Neural%20Networks%20with%20Reals%0A%20%20and%20Floats%0AAuthor%3A%20Veeti%20Ahvonen%20and%20Damian%20Heiman%20and%20Antti%20Kuusisto%20and%20Carsten%20Lutz%0AAbstract%3A%20%20%20In%20pioneering%20work%20from%202019%2C%20Barcel%5C%27o%20and%20coauthors%20identified%20logics%20that%0Aprecisely%20match%20the%20expressive%20power%20of%20constant%20iteration-depth%20graph%20neural%0Anetworks%20%28GNNs%29%20relative%20to%20properties%20definable%20in%20first-order%20logic.%20In%20this%0Aarticle%2C%20we%20give%20exact%20logical%20characterizations%20of%20recurrent%20GNNs%20in%20two%0Ascenarios%3A%20%281%29%20in%20the%20setting%20with%20floating-point%20numbers%20and%20%282%29%20with%20reals.%0AFor%20floats%2C%20the%20formalism%20matching%20recurrent%20GNNs%20is%20a%20rule-based%20modal%20logic%0Awith%20counting%2C%20while%20for%20reals%20we%20use%20a%20suitable%20infinitary%20modal%20logic%2C%20also%0Awith%20counting.%20These%20results%20give%20exact%20matches%20between%20logics%20and%20GNNs%20in%20the%0Arecurrent%20setting%20without%20relativising%20to%20a%20background%20logic%20in%20either%20case%2C%0Abut%20using%20some%20natural%20assumptions%20about%20floating-point%20arithmetic.%20Applying%0Aour%20characterizations%2C%20we%20also%20prove%20that%2C%20relative%20to%20graph%20properties%0Adefinable%20in%20monadic%20second-order%20logic%20%28MSO%29%2C%20our%20infinitary%20and%20rule-based%0Alogics%20are%20equally%20expressive.%20This%20implies%20that%20recurrent%20GNNs%20with%20reals%20and%0Afloats%20have%20the%20same%20expressive%20power%20over%20MSO-definable%20properties%20and%20shows%0Athat%2C%20for%20such%20properties%2C%20also%20recurrent%20GNNs%20with%20reals%20are%20characterized%20by%0Aa%20%28finitary%21%29%20rule-based%20modal%20logic.%20In%20the%20general%20case%2C%20in%20contrast%2C%20the%0Aexpressive%20power%20with%20floats%20is%20weaker%20than%20with%20reals.%20In%20addition%20to%0Alogic-oriented%20results%2C%20we%20also%20characterize%20recurrent%20GNNs%2C%20with%20both%20reals%0Aand%20floats%2C%20via%20distributed%20automata%2C%20drawing%20links%20to%20distributed%20computing%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14606v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogical%2520Characterizations%2520of%2520Recurrent%2520Graph%2520Neural%2520Networks%2520with%2520Reals%250A%2520%2520and%2520Floats%26entry.906535625%3DVeeti%2520Ahvonen%2520and%2520Damian%2520Heiman%2520and%2520Antti%2520Kuusisto%2520and%2520Carsten%2520Lutz%26entry.1292438233%3D%2520%2520In%2520pioneering%2520work%2520from%25202019%252C%2520Barcel%255C%2527o%2520and%2520coauthors%2520identified%2520logics%2520that%250Aprecisely%2520match%2520the%2520expressive%2520power%2520of%2520constant%2520iteration-depth%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%2520relative%2520to%2520properties%2520definable%2520in%2520first-order%2520logic.%2520In%2520this%250Aarticle%252C%2520we%2520give%2520exact%2520logical%2520characterizations%2520of%2520recurrent%2520GNNs%2520in%2520two%250Ascenarios%253A%2520%25281%2529%2520in%2520the%2520setting%2520with%2520floating-point%2520numbers%2520and%2520%25282%2529%2520with%2520reals.%250AFor%2520floats%252C%2520the%2520formalism%2520matching%2520recurrent%2520GNNs%2520is%2520a%2520rule-based%2520modal%2520logic%250Awith%2520counting%252C%2520while%2520for%2520reals%2520we%2520use%2520a%2520suitable%2520infinitary%2520modal%2520logic%252C%2520also%250Awith%2520counting.%2520These%2520results%2520give%2520exact%2520matches%2520between%2520logics%2520and%2520GNNs%2520in%2520the%250Arecurrent%2520setting%2520without%2520relativising%2520to%2520a%2520background%2520logic%2520in%2520either%2520case%252C%250Abut%2520using%2520some%2520natural%2520assumptions%2520about%2520floating-point%2520arithmetic.%2520Applying%250Aour%2520characterizations%252C%2520we%2520also%2520prove%2520that%252C%2520relative%2520to%2520graph%2520properties%250Adefinable%2520in%2520monadic%2520second-order%2520logic%2520%2528MSO%2529%252C%2520our%2520infinitary%2520and%2520rule-based%250Alogics%2520are%2520equally%2520expressive.%2520This%2520implies%2520that%2520recurrent%2520GNNs%2520with%2520reals%2520and%250Afloats%2520have%2520the%2520same%2520expressive%2520power%2520over%2520MSO-definable%2520properties%2520and%2520shows%250Athat%252C%2520for%2520such%2520properties%252C%2520also%2520recurrent%2520GNNs%2520with%2520reals%2520are%2520characterized%2520by%250Aa%2520%2528finitary%2521%2529%2520rule-based%2520modal%2520logic.%2520In%2520the%2520general%2520case%252C%2520in%2520contrast%252C%2520the%250Aexpressive%2520power%2520with%2520floats%2520is%2520weaker%2520than%2520with%2520reals.%2520In%2520addition%2520to%250Alogic-oriented%2520results%252C%2520we%2520also%2520characterize%2520recurrent%2520GNNs%252C%2520with%2520both%2520reals%250Aand%2520floats%252C%2520via%2520distributed%2520automata%252C%2520drawing%2520links%2520to%2520distributed%2520computing%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14606v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logical%20Characterizations%20of%20Recurrent%20Graph%20Neural%20Networks%20with%20Reals%0A%20%20and%20Floats&entry.906535625=Veeti%20Ahvonen%20and%20Damian%20Heiman%20and%20Antti%20Kuusisto%20and%20Carsten%20Lutz&entry.1292438233=%20%20In%20pioneering%20work%20from%202019%2C%20Barcel%5C%27o%20and%20coauthors%20identified%20logics%20that%0Aprecisely%20match%20the%20expressive%20power%20of%20constant%20iteration-depth%20graph%20neural%0Anetworks%20%28GNNs%29%20relative%20to%20properties%20definable%20in%20first-order%20logic.%20In%20this%0Aarticle%2C%20we%20give%20exact%20logical%20characterizations%20of%20recurrent%20GNNs%20in%20two%0Ascenarios%3A%20%281%29%20in%20the%20setting%20with%20floating-point%20numbers%20and%20%282%29%20with%20reals.%0AFor%20floats%2C%20the%20formalism%20matching%20recurrent%20GNNs%20is%20a%20rule-based%20modal%20logic%0Awith%20counting%2C%20while%20for%20reals%20we%20use%20a%20suitable%20infinitary%20modal%20logic%2C%20also%0Awith%20counting.%20These%20results%20give%20exact%20matches%20between%20logics%20and%20GNNs%20in%20the%0Arecurrent%20setting%20without%20relativising%20to%20a%20background%20logic%20in%20either%20case%2C%0Abut%20using%20some%20natural%20assumptions%20about%20floating-point%20arithmetic.%20Applying%0Aour%20characterizations%2C%20we%20also%20prove%20that%2C%20relative%20to%20graph%20properties%0Adefinable%20in%20monadic%20second-order%20logic%20%28MSO%29%2C%20our%20infinitary%20and%20rule-based%0Alogics%20are%20equally%20expressive.%20This%20implies%20that%20recurrent%20GNNs%20with%20reals%20and%0Afloats%20have%20the%20same%20expressive%20power%20over%20MSO-definable%20properties%20and%20shows%0Athat%2C%20for%20such%20properties%2C%20also%20recurrent%20GNNs%20with%20reals%20are%20characterized%20by%0Aa%20%28finitary%21%29%20rule-based%20modal%20logic.%20In%20the%20general%20case%2C%20in%20contrast%2C%20the%0Aexpressive%20power%20with%20floats%20is%20weaker%20than%20with%20reals.%20In%20addition%20to%0Alogic-oriented%20results%2C%20we%20also%20characterize%20recurrent%20GNNs%2C%20with%20both%20reals%0Aand%20floats%2C%20via%20distributed%20automata%2C%20drawing%20links%20to%20distributed%20computing%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14606v4&entry.124074799=Read"},
{"title": "Dynamic Robot Tool Use with Vision Language Models", "author": "Noah Trupin and Zixing Wang and Ahmed H. Qureshi", "abstract": "  Tool use enhances a robot's task capabilities. Recent advances in\nvision-language models (VLMs) have equipped robots with sophisticated cognitive\ncapabilities for tool-use applications. However, existing methodologies focus\non elementary quasi-static tool manipulations or high-level tool selection\nwhile neglecting the critical aspect of task-appropriate tool grasping. To\naddress this limitation, we introduce inverse Tool-Use Planning (iTUP), a novel\nVLM-driven framework that enables grounded fine-grained planning for versatile\nrobotic tool use. Through an integrated pipeline of VLM-based tool and contact\npoint grounding, position-velocity trajectory planning, and physics-informed\ngrasp generation and selection, iTUP demonstrates versatility across (1)\nquasi-static and more challenging (2) dynamic and (3) cluster tool-use tasks.\nTo ensure robust planning, our framework integrates stable and safe task-aware\ngrasping by reasoning over semantic affordances and physical constraints. We\nevaluate iTUP and baselines on a comprehensive range of realistic tool use\ntasks including precision hammering, object scooping, and cluster sweeping.\nExperimental results demonstrate that iTUP ensures a thorough grounding of\ncognition and planning for challenging robot tool use across diverse\nenvironments.\n", "link": "http://arxiv.org/abs/2505.01399v1", "date": "2025-05-02", "relevancy": 1.6931, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5951}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5706}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Robot%20Tool%20Use%20with%20Vision%20Language%20Models&body=Title%3A%20Dynamic%20Robot%20Tool%20Use%20with%20Vision%20Language%20Models%0AAuthor%3A%20Noah%20Trupin%20and%20Zixing%20Wang%20and%20Ahmed%20H.%20Qureshi%0AAbstract%3A%20%20%20Tool%20use%20enhances%20a%20robot%27s%20task%20capabilities.%20Recent%20advances%20in%0Avision-language%20models%20%28VLMs%29%20have%20equipped%20robots%20with%20sophisticated%20cognitive%0Acapabilities%20for%20tool-use%20applications.%20However%2C%20existing%20methodologies%20focus%0Aon%20elementary%20quasi-static%20tool%20manipulations%20or%20high-level%20tool%20selection%0Awhile%20neglecting%20the%20critical%20aspect%20of%20task-appropriate%20tool%20grasping.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20inverse%20Tool-Use%20Planning%20%28iTUP%29%2C%20a%20novel%0AVLM-driven%20framework%20that%20enables%20grounded%20fine-grained%20planning%20for%20versatile%0Arobotic%20tool%20use.%20Through%20an%20integrated%20pipeline%20of%20VLM-based%20tool%20and%20contact%0Apoint%20grounding%2C%20position-velocity%20trajectory%20planning%2C%20and%20physics-informed%0Agrasp%20generation%20and%20selection%2C%20iTUP%20demonstrates%20versatility%20across%20%281%29%0Aquasi-static%20and%20more%20challenging%20%282%29%20dynamic%20and%20%283%29%20cluster%20tool-use%20tasks.%0ATo%20ensure%20robust%20planning%2C%20our%20framework%20integrates%20stable%20and%20safe%20task-aware%0Agrasping%20by%20reasoning%20over%20semantic%20affordances%20and%20physical%20constraints.%20We%0Aevaluate%20iTUP%20and%20baselines%20on%20a%20comprehensive%20range%20of%20realistic%20tool%20use%0Atasks%20including%20precision%20hammering%2C%20object%20scooping%2C%20and%20cluster%20sweeping.%0AExperimental%20results%20demonstrate%20that%20iTUP%20ensures%20a%20thorough%20grounding%20of%0Acognition%20and%20planning%20for%20challenging%20robot%20tool%20use%20across%20diverse%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Robot%2520Tool%2520Use%2520with%2520Vision%2520Language%2520Models%26entry.906535625%3DNoah%2520Trupin%2520and%2520Zixing%2520Wang%2520and%2520Ahmed%2520H.%2520Qureshi%26entry.1292438233%3D%2520%2520Tool%2520use%2520enhances%2520a%2520robot%2527s%2520task%2520capabilities.%2520Recent%2520advances%2520in%250Avision-language%2520models%2520%2528VLMs%2529%2520have%2520equipped%2520robots%2520with%2520sophisticated%2520cognitive%250Acapabilities%2520for%2520tool-use%2520applications.%2520However%252C%2520existing%2520methodologies%2520focus%250Aon%2520elementary%2520quasi-static%2520tool%2520manipulations%2520or%2520high-level%2520tool%2520selection%250Awhile%2520neglecting%2520the%2520critical%2520aspect%2520of%2520task-appropriate%2520tool%2520grasping.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520inverse%2520Tool-Use%2520Planning%2520%2528iTUP%2529%252C%2520a%2520novel%250AVLM-driven%2520framework%2520that%2520enables%2520grounded%2520fine-grained%2520planning%2520for%2520versatile%250Arobotic%2520tool%2520use.%2520Through%2520an%2520integrated%2520pipeline%2520of%2520VLM-based%2520tool%2520and%2520contact%250Apoint%2520grounding%252C%2520position-velocity%2520trajectory%2520planning%252C%2520and%2520physics-informed%250Agrasp%2520generation%2520and%2520selection%252C%2520iTUP%2520demonstrates%2520versatility%2520across%2520%25281%2529%250Aquasi-static%2520and%2520more%2520challenging%2520%25282%2529%2520dynamic%2520and%2520%25283%2529%2520cluster%2520tool-use%2520tasks.%250ATo%2520ensure%2520robust%2520planning%252C%2520our%2520framework%2520integrates%2520stable%2520and%2520safe%2520task-aware%250Agrasping%2520by%2520reasoning%2520over%2520semantic%2520affordances%2520and%2520physical%2520constraints.%2520We%250Aevaluate%2520iTUP%2520and%2520baselines%2520on%2520a%2520comprehensive%2520range%2520of%2520realistic%2520tool%2520use%250Atasks%2520including%2520precision%2520hammering%252C%2520object%2520scooping%252C%2520and%2520cluster%2520sweeping.%250AExperimental%2520results%2520demonstrate%2520that%2520iTUP%2520ensures%2520a%2520thorough%2520grounding%2520of%250Acognition%2520and%2520planning%2520for%2520challenging%2520robot%2520tool%2520use%2520across%2520diverse%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Robot%20Tool%20Use%20with%20Vision%20Language%20Models&entry.906535625=Noah%20Trupin%20and%20Zixing%20Wang%20and%20Ahmed%20H.%20Qureshi&entry.1292438233=%20%20Tool%20use%20enhances%20a%20robot%27s%20task%20capabilities.%20Recent%20advances%20in%0Avision-language%20models%20%28VLMs%29%20have%20equipped%20robots%20with%20sophisticated%20cognitive%0Acapabilities%20for%20tool-use%20applications.%20However%2C%20existing%20methodologies%20focus%0Aon%20elementary%20quasi-static%20tool%20manipulations%20or%20high-level%20tool%20selection%0Awhile%20neglecting%20the%20critical%20aspect%20of%20task-appropriate%20tool%20grasping.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20inverse%20Tool-Use%20Planning%20%28iTUP%29%2C%20a%20novel%0AVLM-driven%20framework%20that%20enables%20grounded%20fine-grained%20planning%20for%20versatile%0Arobotic%20tool%20use.%20Through%20an%20integrated%20pipeline%20of%20VLM-based%20tool%20and%20contact%0Apoint%20grounding%2C%20position-velocity%20trajectory%20planning%2C%20and%20physics-informed%0Agrasp%20generation%20and%20selection%2C%20iTUP%20demonstrates%20versatility%20across%20%281%29%0Aquasi-static%20and%20more%20challenging%20%282%29%20dynamic%20and%20%283%29%20cluster%20tool-use%20tasks.%0ATo%20ensure%20robust%20planning%2C%20our%20framework%20integrates%20stable%20and%20safe%20task-aware%0Agrasping%20by%20reasoning%20over%20semantic%20affordances%20and%20physical%20constraints.%20We%0Aevaluate%20iTUP%20and%20baselines%20on%20a%20comprehensive%20range%20of%20realistic%20tool%20use%0Atasks%20including%20precision%20hammering%2C%20object%20scooping%2C%20and%20cluster%20sweeping.%0AExperimental%20results%20demonstrate%20that%20iTUP%20ensures%20a%20thorough%20grounding%20of%0Acognition%20and%20planning%20for%20challenging%20robot%20tool%20use%20across%20diverse%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01399v1&entry.124074799=Read"},
{"title": "How much to Dereverberate? Low-Latency Single-Channel Speech Enhancement\n  in Distant Microphone Scenarios", "author": "Satvik Venkatesh and Philip Coleman and Arthur Benilov and Simon Brown and Selim Sheta and Frederic Roskam", "abstract": "  Dereverberation is an important sub-task of Speech Enhancement (SE) to\nimprove the signal's intelligibility and quality. However, it remains\nchallenging because the reverberation is highly correlated with the signal.\nFurthermore, the single-channel SE literature has predominantly focused on\nrooms with short reverb times (typically under 1 second), smaller rooms (under\nvolumes of 1000 cubic meters) and relatively short distances (up to 2 meters).\nIn this paper, we explore real-time low-latency single-channel SE under distant\nmicrophone scenarios, such as 5 to 10 meters, and focus on conference rooms and\ntheatres, with larger room dimensions and reverberation times. Such a setup is\nuseful for applications such as lecture demonstrations, drama, and to enhance\nstage acoustics. First, we show that single-channel SE in such challenging\nscenarios is feasible. Second, we investigate the relationship between room\nvolume and reverberation time, and demonstrate its importance when randomly\nsimulating room impulse responses. Lastly, we show that for dereverberation\nwith short decay times, preserving early reflections before decaying the\ntransfer function of the room improves overall signal quality.\n", "link": "http://arxiv.org/abs/2505.01338v1", "date": "2025-05-02", "relevancy": 1.6838, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20much%20to%20Dereverberate%3F%20Low-Latency%20Single-Channel%20Speech%20Enhancement%0A%20%20in%20Distant%20Microphone%20Scenarios&body=Title%3A%20How%20much%20to%20Dereverberate%3F%20Low-Latency%20Single-Channel%20Speech%20Enhancement%0A%20%20in%20Distant%20Microphone%20Scenarios%0AAuthor%3A%20Satvik%20Venkatesh%20and%20Philip%20Coleman%20and%20Arthur%20Benilov%20and%20Simon%20Brown%20and%20Selim%20Sheta%20and%20Frederic%20Roskam%0AAbstract%3A%20%20%20Dereverberation%20is%20an%20important%20sub-task%20of%20Speech%20Enhancement%20%28SE%29%20to%0Aimprove%20the%20signal%27s%20intelligibility%20and%20quality.%20However%2C%20it%20remains%0Achallenging%20because%20the%20reverberation%20is%20highly%20correlated%20with%20the%20signal.%0AFurthermore%2C%20the%20single-channel%20SE%20literature%20has%20predominantly%20focused%20on%0Arooms%20with%20short%20reverb%20times%20%28typically%20under%201%20second%29%2C%20smaller%20rooms%20%28under%0Avolumes%20of%201000%20cubic%20meters%29%20and%20relatively%20short%20distances%20%28up%20to%202%20meters%29.%0AIn%20this%20paper%2C%20we%20explore%20real-time%20low-latency%20single-channel%20SE%20under%20distant%0Amicrophone%20scenarios%2C%20such%20as%205%20to%2010%20meters%2C%20and%20focus%20on%20conference%20rooms%20and%0Atheatres%2C%20with%20larger%20room%20dimensions%20and%20reverberation%20times.%20Such%20a%20setup%20is%0Auseful%20for%20applications%20such%20as%20lecture%20demonstrations%2C%20drama%2C%20and%20to%20enhance%0Astage%20acoustics.%20First%2C%20we%20show%20that%20single-channel%20SE%20in%20such%20challenging%0Ascenarios%20is%20feasible.%20Second%2C%20we%20investigate%20the%20relationship%20between%20room%0Avolume%20and%20reverberation%20time%2C%20and%20demonstrate%20its%20importance%20when%20randomly%0Asimulating%20room%20impulse%20responses.%20Lastly%2C%20we%20show%20that%20for%20dereverberation%0Awith%20short%20decay%20times%2C%20preserving%20early%20reflections%20before%20decaying%20the%0Atransfer%20function%20of%20the%20room%20improves%20overall%20signal%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520much%2520to%2520Dereverberate%253F%2520Low-Latency%2520Single-Channel%2520Speech%2520Enhancement%250A%2520%2520in%2520Distant%2520Microphone%2520Scenarios%26entry.906535625%3DSatvik%2520Venkatesh%2520and%2520Philip%2520Coleman%2520and%2520Arthur%2520Benilov%2520and%2520Simon%2520Brown%2520and%2520Selim%2520Sheta%2520and%2520Frederic%2520Roskam%26entry.1292438233%3D%2520%2520Dereverberation%2520is%2520an%2520important%2520sub-task%2520of%2520Speech%2520Enhancement%2520%2528SE%2529%2520to%250Aimprove%2520the%2520signal%2527s%2520intelligibility%2520and%2520quality.%2520However%252C%2520it%2520remains%250Achallenging%2520because%2520the%2520reverberation%2520is%2520highly%2520correlated%2520with%2520the%2520signal.%250AFurthermore%252C%2520the%2520single-channel%2520SE%2520literature%2520has%2520predominantly%2520focused%2520on%250Arooms%2520with%2520short%2520reverb%2520times%2520%2528typically%2520under%25201%2520second%2529%252C%2520smaller%2520rooms%2520%2528under%250Avolumes%2520of%25201000%2520cubic%2520meters%2529%2520and%2520relatively%2520short%2520distances%2520%2528up%2520to%25202%2520meters%2529.%250AIn%2520this%2520paper%252C%2520we%2520explore%2520real-time%2520low-latency%2520single-channel%2520SE%2520under%2520distant%250Amicrophone%2520scenarios%252C%2520such%2520as%25205%2520to%252010%2520meters%252C%2520and%2520focus%2520on%2520conference%2520rooms%2520and%250Atheatres%252C%2520with%2520larger%2520room%2520dimensions%2520and%2520reverberation%2520times.%2520Such%2520a%2520setup%2520is%250Auseful%2520for%2520applications%2520such%2520as%2520lecture%2520demonstrations%252C%2520drama%252C%2520and%2520to%2520enhance%250Astage%2520acoustics.%2520First%252C%2520we%2520show%2520that%2520single-channel%2520SE%2520in%2520such%2520challenging%250Ascenarios%2520is%2520feasible.%2520Second%252C%2520we%2520investigate%2520the%2520relationship%2520between%2520room%250Avolume%2520and%2520reverberation%2520time%252C%2520and%2520demonstrate%2520its%2520importance%2520when%2520randomly%250Asimulating%2520room%2520impulse%2520responses.%2520Lastly%252C%2520we%2520show%2520that%2520for%2520dereverberation%250Awith%2520short%2520decay%2520times%252C%2520preserving%2520early%2520reflections%2520before%2520decaying%2520the%250Atransfer%2520function%2520of%2520the%2520room%2520improves%2520overall%2520signal%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20much%20to%20Dereverberate%3F%20Low-Latency%20Single-Channel%20Speech%20Enhancement%0A%20%20in%20Distant%20Microphone%20Scenarios&entry.906535625=Satvik%20Venkatesh%20and%20Philip%20Coleman%20and%20Arthur%20Benilov%20and%20Simon%20Brown%20and%20Selim%20Sheta%20and%20Frederic%20Roskam&entry.1292438233=%20%20Dereverberation%20is%20an%20important%20sub-task%20of%20Speech%20Enhancement%20%28SE%29%20to%0Aimprove%20the%20signal%27s%20intelligibility%20and%20quality.%20However%2C%20it%20remains%0Achallenging%20because%20the%20reverberation%20is%20highly%20correlated%20with%20the%20signal.%0AFurthermore%2C%20the%20single-channel%20SE%20literature%20has%20predominantly%20focused%20on%0Arooms%20with%20short%20reverb%20times%20%28typically%20under%201%20second%29%2C%20smaller%20rooms%20%28under%0Avolumes%20of%201000%20cubic%20meters%29%20and%20relatively%20short%20distances%20%28up%20to%202%20meters%29.%0AIn%20this%20paper%2C%20we%20explore%20real-time%20low-latency%20single-channel%20SE%20under%20distant%0Amicrophone%20scenarios%2C%20such%20as%205%20to%2010%20meters%2C%20and%20focus%20on%20conference%20rooms%20and%0Atheatres%2C%20with%20larger%20room%20dimensions%20and%20reverberation%20times.%20Such%20a%20setup%20is%0Auseful%20for%20applications%20such%20as%20lecture%20demonstrations%2C%20drama%2C%20and%20to%20enhance%0Astage%20acoustics.%20First%2C%20we%20show%20that%20single-channel%20SE%20in%20such%20challenging%0Ascenarios%20is%20feasible.%20Second%2C%20we%20investigate%20the%20relationship%20between%20room%0Avolume%20and%20reverberation%20time%2C%20and%20demonstrate%20its%20importance%20when%20randomly%0Asimulating%20room%20impulse%20responses.%20Lastly%2C%20we%20show%20that%20for%20dereverberation%0Awith%20short%20decay%20times%2C%20preserving%20early%20reflections%20before%20decaying%20the%0Atransfer%20function%20of%20the%20room%20improves%20overall%20signal%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01338v1&entry.124074799=Read"},
{"title": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond\n  Surface-level Privacy Leakage", "author": "Rui Xin and Niloofar Mireshghallah and Shuyue Stella Li and Michael Duan and Hyunwoo Kim and Yejin Choi and Yulia Tsvetkov and Sewoong Oh and Pang Wei Koh", "abstract": "  Sanitizing sensitive text data typically involves removing personally\nidentifiable information (PII) or generating synthetic data under the\nassumption that these methods adequately protect privacy; however, their\neffectiveness is often only assessed by measuring the leakage of explicit\nidentifiers but ignoring nuanced textual markers that can lead to\nre-identification. We challenge the above illusion of privacy by proposing a\nnew framework that evaluates re-identification attacks to quantify individual\nprivacy risks upon data release. Our approach shows that seemingly innocuous\nauxiliary information -- such as routine social activities -- can be used to\ninfer sensitive attributes like age or substance use history from sanitized\ndata. For instance, we demonstrate that Azure's commercial PII removal tool\nfails to protect 74\\% of information in the MedQA dataset. Although\ndifferential privacy mitigates these risks to some extent, it significantly\nreduces the utility of the sanitized text for downstream tasks. Our findings\nindicate that current sanitization techniques offer a \\textit{false sense of\nprivacy}, highlighting the need for more robust methods that protect against\nsemantic-level information leakage.\n", "link": "http://arxiv.org/abs/2504.21035v2", "date": "2025-05-02", "relevancy": 1.6732, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4549}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4153}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20False%20Sense%20of%20Privacy%3A%20Evaluating%20Textual%20Data%20Sanitization%20Beyond%0A%20%20Surface-level%20Privacy%20Leakage&body=Title%3A%20A%20False%20Sense%20of%20Privacy%3A%20Evaluating%20Textual%20Data%20Sanitization%20Beyond%0A%20%20Surface-level%20Privacy%20Leakage%0AAuthor%3A%20Rui%20Xin%20and%20Niloofar%20Mireshghallah%20and%20Shuyue%20Stella%20Li%20and%20Michael%20Duan%20and%20Hyunwoo%20Kim%20and%20Yejin%20Choi%20and%20Yulia%20Tsvetkov%20and%20Sewoong%20Oh%20and%20Pang%20Wei%20Koh%0AAbstract%3A%20%20%20Sanitizing%20sensitive%20text%20data%20typically%20involves%20removing%20personally%0Aidentifiable%20information%20%28PII%29%20or%20generating%20synthetic%20data%20under%20the%0Aassumption%20that%20these%20methods%20adequately%20protect%20privacy%3B%20however%2C%20their%0Aeffectiveness%20is%20often%20only%20assessed%20by%20measuring%20the%20leakage%20of%20explicit%0Aidentifiers%20but%20ignoring%20nuanced%20textual%20markers%20that%20can%20lead%20to%0Are-identification.%20We%20challenge%20the%20above%20illusion%20of%20privacy%20by%20proposing%20a%0Anew%20framework%20that%20evaluates%20re-identification%20attacks%20to%20quantify%20individual%0Aprivacy%20risks%20upon%20data%20release.%20Our%20approach%20shows%20that%20seemingly%20innocuous%0Aauxiliary%20information%20--%20such%20as%20routine%20social%20activities%20--%20can%20be%20used%20to%0Ainfer%20sensitive%20attributes%20like%20age%20or%20substance%20use%20history%20from%20sanitized%0Adata.%20For%20instance%2C%20we%20demonstrate%20that%20Azure%27s%20commercial%20PII%20removal%20tool%0Afails%20to%20protect%2074%5C%25%20of%20information%20in%20the%20MedQA%20dataset.%20Although%0Adifferential%20privacy%20mitigates%20these%20risks%20to%20some%20extent%2C%20it%20significantly%0Areduces%20the%20utility%20of%20the%20sanitized%20text%20for%20downstream%20tasks.%20Our%20findings%0Aindicate%20that%20current%20sanitization%20techniques%20offer%20a%20%5Ctextit%7Bfalse%20sense%20of%0Aprivacy%7D%2C%20highlighting%20the%20need%20for%20more%20robust%20methods%20that%20protect%20against%0Asemantic-level%20information%20leakage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520False%2520Sense%2520of%2520Privacy%253A%2520Evaluating%2520Textual%2520Data%2520Sanitization%2520Beyond%250A%2520%2520Surface-level%2520Privacy%2520Leakage%26entry.906535625%3DRui%2520Xin%2520and%2520Niloofar%2520Mireshghallah%2520and%2520Shuyue%2520Stella%2520Li%2520and%2520Michael%2520Duan%2520and%2520Hyunwoo%2520Kim%2520and%2520Yejin%2520Choi%2520and%2520Yulia%2520Tsvetkov%2520and%2520Sewoong%2520Oh%2520and%2520Pang%2520Wei%2520Koh%26entry.1292438233%3D%2520%2520Sanitizing%2520sensitive%2520text%2520data%2520typically%2520involves%2520removing%2520personally%250Aidentifiable%2520information%2520%2528PII%2529%2520or%2520generating%2520synthetic%2520data%2520under%2520the%250Aassumption%2520that%2520these%2520methods%2520adequately%2520protect%2520privacy%253B%2520however%252C%2520their%250Aeffectiveness%2520is%2520often%2520only%2520assessed%2520by%2520measuring%2520the%2520leakage%2520of%2520explicit%250Aidentifiers%2520but%2520ignoring%2520nuanced%2520textual%2520markers%2520that%2520can%2520lead%2520to%250Are-identification.%2520We%2520challenge%2520the%2520above%2520illusion%2520of%2520privacy%2520by%2520proposing%2520a%250Anew%2520framework%2520that%2520evaluates%2520re-identification%2520attacks%2520to%2520quantify%2520individual%250Aprivacy%2520risks%2520upon%2520data%2520release.%2520Our%2520approach%2520shows%2520that%2520seemingly%2520innocuous%250Aauxiliary%2520information%2520--%2520such%2520as%2520routine%2520social%2520activities%2520--%2520can%2520be%2520used%2520to%250Ainfer%2520sensitive%2520attributes%2520like%2520age%2520or%2520substance%2520use%2520history%2520from%2520sanitized%250Adata.%2520For%2520instance%252C%2520we%2520demonstrate%2520that%2520Azure%2527s%2520commercial%2520PII%2520removal%2520tool%250Afails%2520to%2520protect%252074%255C%2525%2520of%2520information%2520in%2520the%2520MedQA%2520dataset.%2520Although%250Adifferential%2520privacy%2520mitigates%2520these%2520risks%2520to%2520some%2520extent%252C%2520it%2520significantly%250Areduces%2520the%2520utility%2520of%2520the%2520sanitized%2520text%2520for%2520downstream%2520tasks.%2520Our%2520findings%250Aindicate%2520that%2520current%2520sanitization%2520techniques%2520offer%2520a%2520%255Ctextit%257Bfalse%2520sense%2520of%250Aprivacy%257D%252C%2520highlighting%2520the%2520need%2520for%2520more%2520robust%2520methods%2520that%2520protect%2520against%250Asemantic-level%2520information%2520leakage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20False%20Sense%20of%20Privacy%3A%20Evaluating%20Textual%20Data%20Sanitization%20Beyond%0A%20%20Surface-level%20Privacy%20Leakage&entry.906535625=Rui%20Xin%20and%20Niloofar%20Mireshghallah%20and%20Shuyue%20Stella%20Li%20and%20Michael%20Duan%20and%20Hyunwoo%20Kim%20and%20Yejin%20Choi%20and%20Yulia%20Tsvetkov%20and%20Sewoong%20Oh%20and%20Pang%20Wei%20Koh&entry.1292438233=%20%20Sanitizing%20sensitive%20text%20data%20typically%20involves%20removing%20personally%0Aidentifiable%20information%20%28PII%29%20or%20generating%20synthetic%20data%20under%20the%0Aassumption%20that%20these%20methods%20adequately%20protect%20privacy%3B%20however%2C%20their%0Aeffectiveness%20is%20often%20only%20assessed%20by%20measuring%20the%20leakage%20of%20explicit%0Aidentifiers%20but%20ignoring%20nuanced%20textual%20markers%20that%20can%20lead%20to%0Are-identification.%20We%20challenge%20the%20above%20illusion%20of%20privacy%20by%20proposing%20a%0Anew%20framework%20that%20evaluates%20re-identification%20attacks%20to%20quantify%20individual%0Aprivacy%20risks%20upon%20data%20release.%20Our%20approach%20shows%20that%20seemingly%20innocuous%0Aauxiliary%20information%20--%20such%20as%20routine%20social%20activities%20--%20can%20be%20used%20to%0Ainfer%20sensitive%20attributes%20like%20age%20or%20substance%20use%20history%20from%20sanitized%0Adata.%20For%20instance%2C%20we%20demonstrate%20that%20Azure%27s%20commercial%20PII%20removal%20tool%0Afails%20to%20protect%2074%5C%25%20of%20information%20in%20the%20MedQA%20dataset.%20Although%0Adifferential%20privacy%20mitigates%20these%20risks%20to%20some%20extent%2C%20it%20significantly%0Areduces%20the%20utility%20of%20the%20sanitized%20text%20for%20downstream%20tasks.%20Our%20findings%0Aindicate%20that%20current%20sanitization%20techniques%20offer%20a%20%5Ctextit%7Bfalse%20sense%20of%0Aprivacy%7D%2C%20highlighting%20the%20need%20for%20more%20robust%20methods%20that%20protect%20against%0Asemantic-level%20information%20leakage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21035v2&entry.124074799=Read"},
{"title": "ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video\n  Semantic Action Flow", "author": "Changhe Chen and Quantao Yang and Xiaohao Xu and Nima Fazeli and Olov Andersson", "abstract": "  One of the central challenges preventing robots from acquiring complex\nmanipulation skills is the prohibitive cost of collecting large-scale robot\ndemonstrations. In contrast, humans are able to learn efficiently by watching\nothers interact with their environment. To bridge this gap, we introduce\nsemantic action flow as a core intermediate representation capturing the\nessential spatio-temporal manipulator-object interactions, invariant to\nsuperficial visual differences. We present ViSA-Flow, a framework that learns\nthis representation self-supervised from unlabeled large-scale video data.\nFirst, a generative model is pre-trained on semantic action flows automatically\nextracted from large-scale human-object interaction video data, learning a\nrobust prior over manipulation structure. Second, this prior is efficiently\nadapted to a target robot by fine-tuning on a small set of robot demonstrations\nprocessed through the same semantic abstraction pipeline. We demonstrate\nthrough extensive experiments on the CALVIN benchmark and real-world tasks that\nViSA-Flow achieves state-of-the-art performance, particularly in low-data\nregimes, outperforming prior methods by effectively transferring knowledge from\nhuman video observation to robotic execution. Videos are available at\nhttps://visaflow-web.github.io/ViSAFLOW.\n", "link": "http://arxiv.org/abs/2505.01288v1", "date": "2025-05-02", "relevancy": 1.6708, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6107}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5747}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViSA-Flow%3A%20Accelerating%20Robot%20Skill%20Learning%20via%20Large-Scale%20Video%0A%20%20Semantic%20Action%20Flow&body=Title%3A%20ViSA-Flow%3A%20Accelerating%20Robot%20Skill%20Learning%20via%20Large-Scale%20Video%0A%20%20Semantic%20Action%20Flow%0AAuthor%3A%20Changhe%20Chen%20and%20Quantao%20Yang%20and%20Xiaohao%20Xu%20and%20Nima%20Fazeli%20and%20Olov%20Andersson%0AAbstract%3A%20%20%20One%20of%20the%20central%20challenges%20preventing%20robots%20from%20acquiring%20complex%0Amanipulation%20skills%20is%20the%20prohibitive%20cost%20of%20collecting%20large-scale%20robot%0Ademonstrations.%20In%20contrast%2C%20humans%20are%20able%20to%20learn%20efficiently%20by%20watching%0Aothers%20interact%20with%20their%20environment.%20To%20bridge%20this%20gap%2C%20we%20introduce%0Asemantic%20action%20flow%20as%20a%20core%20intermediate%20representation%20capturing%20the%0Aessential%20spatio-temporal%20manipulator-object%20interactions%2C%20invariant%20to%0Asuperficial%20visual%20differences.%20We%20present%20ViSA-Flow%2C%20a%20framework%20that%20learns%0Athis%20representation%20self-supervised%20from%20unlabeled%20large-scale%20video%20data.%0AFirst%2C%20a%20generative%20model%20is%20pre-trained%20on%20semantic%20action%20flows%20automatically%0Aextracted%20from%20large-scale%20human-object%20interaction%20video%20data%2C%20learning%20a%0Arobust%20prior%20over%20manipulation%20structure.%20Second%2C%20this%20prior%20is%20efficiently%0Aadapted%20to%20a%20target%20robot%20by%20fine-tuning%20on%20a%20small%20set%20of%20robot%20demonstrations%0Aprocessed%20through%20the%20same%20semantic%20abstraction%20pipeline.%20We%20demonstrate%0Athrough%20extensive%20experiments%20on%20the%20CALVIN%20benchmark%20and%20real-world%20tasks%20that%0AViSA-Flow%20achieves%20state-of-the-art%20performance%2C%20particularly%20in%20low-data%0Aregimes%2C%20outperforming%20prior%20methods%20by%20effectively%20transferring%20knowledge%20from%0Ahuman%20video%20observation%20to%20robotic%20execution.%20Videos%20are%20available%20at%0Ahttps%3A//visaflow-web.github.io/ViSAFLOW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViSA-Flow%253A%2520Accelerating%2520Robot%2520Skill%2520Learning%2520via%2520Large-Scale%2520Video%250A%2520%2520Semantic%2520Action%2520Flow%26entry.906535625%3DChanghe%2520Chen%2520and%2520Quantao%2520Yang%2520and%2520Xiaohao%2520Xu%2520and%2520Nima%2520Fazeli%2520and%2520Olov%2520Andersson%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520central%2520challenges%2520preventing%2520robots%2520from%2520acquiring%2520complex%250Amanipulation%2520skills%2520is%2520the%2520prohibitive%2520cost%2520of%2520collecting%2520large-scale%2520robot%250Ademonstrations.%2520In%2520contrast%252C%2520humans%2520are%2520able%2520to%2520learn%2520efficiently%2520by%2520watching%250Aothers%2520interact%2520with%2520their%2520environment.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%250Asemantic%2520action%2520flow%2520as%2520a%2520core%2520intermediate%2520representation%2520capturing%2520the%250Aessential%2520spatio-temporal%2520manipulator-object%2520interactions%252C%2520invariant%2520to%250Asuperficial%2520visual%2520differences.%2520We%2520present%2520ViSA-Flow%252C%2520a%2520framework%2520that%2520learns%250Athis%2520representation%2520self-supervised%2520from%2520unlabeled%2520large-scale%2520video%2520data.%250AFirst%252C%2520a%2520generative%2520model%2520is%2520pre-trained%2520on%2520semantic%2520action%2520flows%2520automatically%250Aextracted%2520from%2520large-scale%2520human-object%2520interaction%2520video%2520data%252C%2520learning%2520a%250Arobust%2520prior%2520over%2520manipulation%2520structure.%2520Second%252C%2520this%2520prior%2520is%2520efficiently%250Aadapted%2520to%2520a%2520target%2520robot%2520by%2520fine-tuning%2520on%2520a%2520small%2520set%2520of%2520robot%2520demonstrations%250Aprocessed%2520through%2520the%2520same%2520semantic%2520abstraction%2520pipeline.%2520We%2520demonstrate%250Athrough%2520extensive%2520experiments%2520on%2520the%2520CALVIN%2520benchmark%2520and%2520real-world%2520tasks%2520that%250AViSA-Flow%2520achieves%2520state-of-the-art%2520performance%252C%2520particularly%2520in%2520low-data%250Aregimes%252C%2520outperforming%2520prior%2520methods%2520by%2520effectively%2520transferring%2520knowledge%2520from%250Ahuman%2520video%2520observation%2520to%2520robotic%2520execution.%2520Videos%2520are%2520available%2520at%250Ahttps%253A//visaflow-web.github.io/ViSAFLOW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViSA-Flow%3A%20Accelerating%20Robot%20Skill%20Learning%20via%20Large-Scale%20Video%0A%20%20Semantic%20Action%20Flow&entry.906535625=Changhe%20Chen%20and%20Quantao%20Yang%20and%20Xiaohao%20Xu%20and%20Nima%20Fazeli%20and%20Olov%20Andersson&entry.1292438233=%20%20One%20of%20the%20central%20challenges%20preventing%20robots%20from%20acquiring%20complex%0Amanipulation%20skills%20is%20the%20prohibitive%20cost%20of%20collecting%20large-scale%20robot%0Ademonstrations.%20In%20contrast%2C%20humans%20are%20able%20to%20learn%20efficiently%20by%20watching%0Aothers%20interact%20with%20their%20environment.%20To%20bridge%20this%20gap%2C%20we%20introduce%0Asemantic%20action%20flow%20as%20a%20core%20intermediate%20representation%20capturing%20the%0Aessential%20spatio-temporal%20manipulator-object%20interactions%2C%20invariant%20to%0Asuperficial%20visual%20differences.%20We%20present%20ViSA-Flow%2C%20a%20framework%20that%20learns%0Athis%20representation%20self-supervised%20from%20unlabeled%20large-scale%20video%20data.%0AFirst%2C%20a%20generative%20model%20is%20pre-trained%20on%20semantic%20action%20flows%20automatically%0Aextracted%20from%20large-scale%20human-object%20interaction%20video%20data%2C%20learning%20a%0Arobust%20prior%20over%20manipulation%20structure.%20Second%2C%20this%20prior%20is%20efficiently%0Aadapted%20to%20a%20target%20robot%20by%20fine-tuning%20on%20a%20small%20set%20of%20robot%20demonstrations%0Aprocessed%20through%20the%20same%20semantic%20abstraction%20pipeline.%20We%20demonstrate%0Athrough%20extensive%20experiments%20on%20the%20CALVIN%20benchmark%20and%20real-world%20tasks%20that%0AViSA-Flow%20achieves%20state-of-the-art%20performance%2C%20particularly%20in%20low-data%0Aregimes%2C%20outperforming%20prior%20methods%20by%20effectively%20transferring%20knowledge%20from%0Ahuman%20video%20observation%20to%20robotic%20execution.%20Videos%20are%20available%20at%0Ahttps%3A//visaflow-web.github.io/ViSAFLOW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01288v1&entry.124074799=Read"},
{"title": "A Secured Triad of IoT, Machine Learning, and Blockchain for Crop\n  Forecasting in Agriculture", "author": "Najmus Sakib Sizan and Md. Abu Layek and Khondokar Fida Hasan", "abstract": "  To improve crop forecasting and provide farmers with actionable data-driven\ninsights, we propose a novel approach integrating IoT, machine learning, and\nblockchain technologies. Using IoT, real-time data from sensor networks\ncontinuously monitor environmental conditions and soil nutrient levels,\nsignificantly improving our understanding of crop growth dynamics. Our study\ndemonstrates the exceptional accuracy of the Random Forest model, achieving a\n99.45\\% accuracy rate in predicting optimal crop types and yields, thereby\noffering precise crop projections and customized recommendations. To ensure the\nsecurity and integrity of the sensor data used for these forecasts, we\nintegrate the Ethereum blockchain, which provides a robust and secure platform.\nThis ensures that the forecasted data remain tamper-proof and reliable.\nStakeholders can access real-time and historical crop projections through an\nintuitive online interface, enhancing transparency and facilitating informed\ndecision-making. By presenting multiple predicted crop scenarios, our system\nenables farmers to optimize production strategies effectively. This integrated\napproach promises significant advances in precision agriculture, making crop\nforecasting more accurate, secure, and user-friendly.\n", "link": "http://arxiv.org/abs/2505.01196v1", "date": "2025-05-02", "relevancy": 1.6659, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4425}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4146}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Secured%20Triad%20of%20IoT%2C%20Machine%20Learning%2C%20and%20Blockchain%20for%20Crop%0A%20%20Forecasting%20in%20Agriculture&body=Title%3A%20A%20Secured%20Triad%20of%20IoT%2C%20Machine%20Learning%2C%20and%20Blockchain%20for%20Crop%0A%20%20Forecasting%20in%20Agriculture%0AAuthor%3A%20Najmus%20Sakib%20Sizan%20and%20Md.%20Abu%20Layek%20and%20Khondokar%20Fida%20Hasan%0AAbstract%3A%20%20%20To%20improve%20crop%20forecasting%20and%20provide%20farmers%20with%20actionable%20data-driven%0Ainsights%2C%20we%20propose%20a%20novel%20approach%20integrating%20IoT%2C%20machine%20learning%2C%20and%0Ablockchain%20technologies.%20Using%20IoT%2C%20real-time%20data%20from%20sensor%20networks%0Acontinuously%20monitor%20environmental%20conditions%20and%20soil%20nutrient%20levels%2C%0Asignificantly%20improving%20our%20understanding%20of%20crop%20growth%20dynamics.%20Our%20study%0Ademonstrates%20the%20exceptional%20accuracy%20of%20the%20Random%20Forest%20model%2C%20achieving%20a%0A99.45%5C%25%20accuracy%20rate%20in%20predicting%20optimal%20crop%20types%20and%20yields%2C%20thereby%0Aoffering%20precise%20crop%20projections%20and%20customized%20recommendations.%20To%20ensure%20the%0Asecurity%20and%20integrity%20of%20the%20sensor%20data%20used%20for%20these%20forecasts%2C%20we%0Aintegrate%20the%20Ethereum%20blockchain%2C%20which%20provides%20a%20robust%20and%20secure%20platform.%0AThis%20ensures%20that%20the%20forecasted%20data%20remain%20tamper-proof%20and%20reliable.%0AStakeholders%20can%20access%20real-time%20and%20historical%20crop%20projections%20through%20an%0Aintuitive%20online%20interface%2C%20enhancing%20transparency%20and%20facilitating%20informed%0Adecision-making.%20By%20presenting%20multiple%20predicted%20crop%20scenarios%2C%20our%20system%0Aenables%20farmers%20to%20optimize%20production%20strategies%20effectively.%20This%20integrated%0Aapproach%20promises%20significant%20advances%20in%20precision%20agriculture%2C%20making%20crop%0Aforecasting%20more%20accurate%2C%20secure%2C%20and%20user-friendly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Secured%2520Triad%2520of%2520IoT%252C%2520Machine%2520Learning%252C%2520and%2520Blockchain%2520for%2520Crop%250A%2520%2520Forecasting%2520in%2520Agriculture%26entry.906535625%3DNajmus%2520Sakib%2520Sizan%2520and%2520Md.%2520Abu%2520Layek%2520and%2520Khondokar%2520Fida%2520Hasan%26entry.1292438233%3D%2520%2520To%2520improve%2520crop%2520forecasting%2520and%2520provide%2520farmers%2520with%2520actionable%2520data-driven%250Ainsights%252C%2520we%2520propose%2520a%2520novel%2520approach%2520integrating%2520IoT%252C%2520machine%2520learning%252C%2520and%250Ablockchain%2520technologies.%2520Using%2520IoT%252C%2520real-time%2520data%2520from%2520sensor%2520networks%250Acontinuously%2520monitor%2520environmental%2520conditions%2520and%2520soil%2520nutrient%2520levels%252C%250Asignificantly%2520improving%2520our%2520understanding%2520of%2520crop%2520growth%2520dynamics.%2520Our%2520study%250Ademonstrates%2520the%2520exceptional%2520accuracy%2520of%2520the%2520Random%2520Forest%2520model%252C%2520achieving%2520a%250A99.45%255C%2525%2520accuracy%2520rate%2520in%2520predicting%2520optimal%2520crop%2520types%2520and%2520yields%252C%2520thereby%250Aoffering%2520precise%2520crop%2520projections%2520and%2520customized%2520recommendations.%2520To%2520ensure%2520the%250Asecurity%2520and%2520integrity%2520of%2520the%2520sensor%2520data%2520used%2520for%2520these%2520forecasts%252C%2520we%250Aintegrate%2520the%2520Ethereum%2520blockchain%252C%2520which%2520provides%2520a%2520robust%2520and%2520secure%2520platform.%250AThis%2520ensures%2520that%2520the%2520forecasted%2520data%2520remain%2520tamper-proof%2520and%2520reliable.%250AStakeholders%2520can%2520access%2520real-time%2520and%2520historical%2520crop%2520projections%2520through%2520an%250Aintuitive%2520online%2520interface%252C%2520enhancing%2520transparency%2520and%2520facilitating%2520informed%250Adecision-making.%2520By%2520presenting%2520multiple%2520predicted%2520crop%2520scenarios%252C%2520our%2520system%250Aenables%2520farmers%2520to%2520optimize%2520production%2520strategies%2520effectively.%2520This%2520integrated%250Aapproach%2520promises%2520significant%2520advances%2520in%2520precision%2520agriculture%252C%2520making%2520crop%250Aforecasting%2520more%2520accurate%252C%2520secure%252C%2520and%2520user-friendly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Secured%20Triad%20of%20IoT%2C%20Machine%20Learning%2C%20and%20Blockchain%20for%20Crop%0A%20%20Forecasting%20in%20Agriculture&entry.906535625=Najmus%20Sakib%20Sizan%20and%20Md.%20Abu%20Layek%20and%20Khondokar%20Fida%20Hasan&entry.1292438233=%20%20To%20improve%20crop%20forecasting%20and%20provide%20farmers%20with%20actionable%20data-driven%0Ainsights%2C%20we%20propose%20a%20novel%20approach%20integrating%20IoT%2C%20machine%20learning%2C%20and%0Ablockchain%20technologies.%20Using%20IoT%2C%20real-time%20data%20from%20sensor%20networks%0Acontinuously%20monitor%20environmental%20conditions%20and%20soil%20nutrient%20levels%2C%0Asignificantly%20improving%20our%20understanding%20of%20crop%20growth%20dynamics.%20Our%20study%0Ademonstrates%20the%20exceptional%20accuracy%20of%20the%20Random%20Forest%20model%2C%20achieving%20a%0A99.45%5C%25%20accuracy%20rate%20in%20predicting%20optimal%20crop%20types%20and%20yields%2C%20thereby%0Aoffering%20precise%20crop%20projections%20and%20customized%20recommendations.%20To%20ensure%20the%0Asecurity%20and%20integrity%20of%20the%20sensor%20data%20used%20for%20these%20forecasts%2C%20we%0Aintegrate%20the%20Ethereum%20blockchain%2C%20which%20provides%20a%20robust%20and%20secure%20platform.%0AThis%20ensures%20that%20the%20forecasted%20data%20remain%20tamper-proof%20and%20reliable.%0AStakeholders%20can%20access%20real-time%20and%20historical%20crop%20projections%20through%20an%0Aintuitive%20online%20interface%2C%20enhancing%20transparency%20and%20facilitating%20informed%0Adecision-making.%20By%20presenting%20multiple%20predicted%20crop%20scenarios%2C%20our%20system%0Aenables%20farmers%20to%20optimize%20production%20strategies%20effectively.%20This%20integrated%0Aapproach%20promises%20significant%20advances%20in%20precision%20agriculture%2C%20making%20crop%0Aforecasting%20more%20accurate%2C%20secure%2C%20and%20user-friendly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01196v1&entry.124074799=Read"},
{"title": "Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables", "author": "Aneta Koleva and Martin Ringsquandl and Ahmed Hatem and Thomas Runkler and Volker Tresp", "abstract": "  Interest in solving table interpretation tasks has grown over the years, yet\nit still relies on existing datasets that may be overly simplified. This is\npotentially reducing the effectiveness of the dataset for thorough evaluation\nand failing to accurately represent tables as they appear in the real-world. To\nenrich the existing benchmark datasets, we extract and annotate a new, more\nchallenging dataset. The proposed Wiki-TabNER dataset features complex tables\ncontaining several entities per cell, with named entities labeled using DBpedia\nclasses. This dataset is specifically designed to address named entity\nrecognition (NER) task within tables, but it can also be used as a more\nchallenging dataset for evaluating the entity linking task. In this paper we\ndescribe the distinguishing features of the Wiki-TabNER dataset and the\nlabeling process. In addition, we propose a prompting framework for evaluating\nthe new large language models on the within tables NER task. Finally, we\nperform qualitative analysis to gain insights into the challenges encountered\nby the models and to understand the limitations of the proposed~dataset.\n", "link": "http://arxiv.org/abs/2403.04577v2", "date": "2025-05-02", "relevancy": 1.6631, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wiki-TabNER%3A%20Integrating%20Named%20Entity%20Recognition%20into%20Wikipedia%20Tables&body=Title%3A%20Wiki-TabNER%3A%20Integrating%20Named%20Entity%20Recognition%20into%20Wikipedia%20Tables%0AAuthor%3A%20Aneta%20Koleva%20and%20Martin%20Ringsquandl%20and%20Ahmed%20Hatem%20and%20Thomas%20Runkler%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20Interest%20in%20solving%20table%20interpretation%20tasks%20has%20grown%20over%20the%20years%2C%20yet%0Ait%20still%20relies%20on%20existing%20datasets%20that%20may%20be%20overly%20simplified.%20This%20is%0Apotentially%20reducing%20the%20effectiveness%20of%20the%20dataset%20for%20thorough%20evaluation%0Aand%20failing%20to%20accurately%20represent%20tables%20as%20they%20appear%20in%20the%20real-world.%20To%0Aenrich%20the%20existing%20benchmark%20datasets%2C%20we%20extract%20and%20annotate%20a%20new%2C%20more%0Achallenging%20dataset.%20The%20proposed%20Wiki-TabNER%20dataset%20features%20complex%20tables%0Acontaining%20several%20entities%20per%20cell%2C%20with%20named%20entities%20labeled%20using%20DBpedia%0Aclasses.%20This%20dataset%20is%20specifically%20designed%20to%20address%20named%20entity%0Arecognition%20%28NER%29%20task%20within%20tables%2C%20but%20it%20can%20also%20be%20used%20as%20a%20more%0Achallenging%20dataset%20for%20evaluating%20the%20entity%20linking%20task.%20In%20this%20paper%20we%0Adescribe%20the%20distinguishing%20features%20of%20the%20Wiki-TabNER%20dataset%20and%20the%0Alabeling%20process.%20In%20addition%2C%20we%20propose%20a%20prompting%20framework%20for%20evaluating%0Athe%20new%20large%20language%20models%20on%20the%20within%20tables%20NER%20task.%20Finally%2C%20we%0Aperform%20qualitative%20analysis%20to%20gain%20insights%20into%20the%20challenges%20encountered%0Aby%20the%20models%20and%20to%20understand%20the%20limitations%20of%20the%20proposed~dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWiki-TabNER%253A%2520Integrating%2520Named%2520Entity%2520Recognition%2520into%2520Wikipedia%2520Tables%26entry.906535625%3DAneta%2520Koleva%2520and%2520Martin%2520Ringsquandl%2520and%2520Ahmed%2520Hatem%2520and%2520Thomas%2520Runkler%2520and%2520Volker%2520Tresp%26entry.1292438233%3D%2520%2520Interest%2520in%2520solving%2520table%2520interpretation%2520tasks%2520has%2520grown%2520over%2520the%2520years%252C%2520yet%250Ait%2520still%2520relies%2520on%2520existing%2520datasets%2520that%2520may%2520be%2520overly%2520simplified.%2520This%2520is%250Apotentially%2520reducing%2520the%2520effectiveness%2520of%2520the%2520dataset%2520for%2520thorough%2520evaluation%250Aand%2520failing%2520to%2520accurately%2520represent%2520tables%2520as%2520they%2520appear%2520in%2520the%2520real-world.%2520To%250Aenrich%2520the%2520existing%2520benchmark%2520datasets%252C%2520we%2520extract%2520and%2520annotate%2520a%2520new%252C%2520more%250Achallenging%2520dataset.%2520The%2520proposed%2520Wiki-TabNER%2520dataset%2520features%2520complex%2520tables%250Acontaining%2520several%2520entities%2520per%2520cell%252C%2520with%2520named%2520entities%2520labeled%2520using%2520DBpedia%250Aclasses.%2520This%2520dataset%2520is%2520specifically%2520designed%2520to%2520address%2520named%2520entity%250Arecognition%2520%2528NER%2529%2520task%2520within%2520tables%252C%2520but%2520it%2520can%2520also%2520be%2520used%2520as%2520a%2520more%250Achallenging%2520dataset%2520for%2520evaluating%2520the%2520entity%2520linking%2520task.%2520In%2520this%2520paper%2520we%250Adescribe%2520the%2520distinguishing%2520features%2520of%2520the%2520Wiki-TabNER%2520dataset%2520and%2520the%250Alabeling%2520process.%2520In%2520addition%252C%2520we%2520propose%2520a%2520prompting%2520framework%2520for%2520evaluating%250Athe%2520new%2520large%2520language%2520models%2520on%2520the%2520within%2520tables%2520NER%2520task.%2520Finally%252C%2520we%250Aperform%2520qualitative%2520analysis%2520to%2520gain%2520insights%2520into%2520the%2520challenges%2520encountered%250Aby%2520the%2520models%2520and%2520to%2520understand%2520the%2520limitations%2520of%2520the%2520proposed~dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wiki-TabNER%3A%20Integrating%20Named%20Entity%20Recognition%20into%20Wikipedia%20Tables&entry.906535625=Aneta%20Koleva%20and%20Martin%20Ringsquandl%20and%20Ahmed%20Hatem%20and%20Thomas%20Runkler%20and%20Volker%20Tresp&entry.1292438233=%20%20Interest%20in%20solving%20table%20interpretation%20tasks%20has%20grown%20over%20the%20years%2C%20yet%0Ait%20still%20relies%20on%20existing%20datasets%20that%20may%20be%20overly%20simplified.%20This%20is%0Apotentially%20reducing%20the%20effectiveness%20of%20the%20dataset%20for%20thorough%20evaluation%0Aand%20failing%20to%20accurately%20represent%20tables%20as%20they%20appear%20in%20the%20real-world.%20To%0Aenrich%20the%20existing%20benchmark%20datasets%2C%20we%20extract%20and%20annotate%20a%20new%2C%20more%0Achallenging%20dataset.%20The%20proposed%20Wiki-TabNER%20dataset%20features%20complex%20tables%0Acontaining%20several%20entities%20per%20cell%2C%20with%20named%20entities%20labeled%20using%20DBpedia%0Aclasses.%20This%20dataset%20is%20specifically%20designed%20to%20address%20named%20entity%0Arecognition%20%28NER%29%20task%20within%20tables%2C%20but%20it%20can%20also%20be%20used%20as%20a%20more%0Achallenging%20dataset%20for%20evaluating%20the%20entity%20linking%20task.%20In%20this%20paper%20we%0Adescribe%20the%20distinguishing%20features%20of%20the%20Wiki-TabNER%20dataset%20and%20the%0Alabeling%20process.%20In%20addition%2C%20we%20propose%20a%20prompting%20framework%20for%20evaluating%0Athe%20new%20large%20language%20models%20on%20the%20within%20tables%20NER%20task.%20Finally%2C%20we%0Aperform%20qualitative%20analysis%20to%20gain%20insights%20into%20the%20challenges%20encountered%0Aby%20the%20models%20and%20to%20understand%20the%20limitations%20of%20the%20proposed~dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04577v2&entry.124074799=Read"},
{"title": "mwBTFreddy: A Dataset for Flash Flood Damage Assessment in Urban Malawi", "author": "Evelyn Chapuma and Grey Mengezi and Lewis Msasa and Amelia Taylor", "abstract": "  This paper describes the mwBTFreddy dataset, a resource developed to support\nflash flood damage assessment in urban Malawi, specifically focusing on the\nimpacts of Cyclone Freddy in 2023. The dataset comprises paired pre- and\npost-disaster satellite images sourced from Google Earth Pro, accompanied by\nJSON files containing labelled building annotations with geographic coordinates\nand damage levels (no damage, minor, major, or destroyed). Developed by the\nKuyesera AI Lab at the Malawi University of Business and Applied Sciences, this\ndataset is intended to facilitate the development of machine learning models\ntailored to building detection and damage classification in African urban\ncontexts. It also supports flood damage visualisation and spatial analysis to\ninform decisions on relocation, infrastructure planning, and emergency response\nin climate-vulnerable regions.\n", "link": "http://arxiv.org/abs/2505.01242v1", "date": "2025-05-02", "relevancy": 1.6261, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4228}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.3952}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mwBTFreddy%3A%20A%20Dataset%20for%20Flash%20Flood%20Damage%20Assessment%20in%20Urban%20Malawi&body=Title%3A%20mwBTFreddy%3A%20A%20Dataset%20for%20Flash%20Flood%20Damage%20Assessment%20in%20Urban%20Malawi%0AAuthor%3A%20Evelyn%20Chapuma%20and%20Grey%20Mengezi%20and%20Lewis%20Msasa%20and%20Amelia%20Taylor%0AAbstract%3A%20%20%20This%20paper%20describes%20the%20mwBTFreddy%20dataset%2C%20a%20resource%20developed%20to%20support%0Aflash%20flood%20damage%20assessment%20in%20urban%20Malawi%2C%20specifically%20focusing%20on%20the%0Aimpacts%20of%20Cyclone%20Freddy%20in%202023.%20The%20dataset%20comprises%20paired%20pre-%20and%0Apost-disaster%20satellite%20images%20sourced%20from%20Google%20Earth%20Pro%2C%20accompanied%20by%0AJSON%20files%20containing%20labelled%20building%20annotations%20with%20geographic%20coordinates%0Aand%20damage%20levels%20%28no%20damage%2C%20minor%2C%20major%2C%20or%20destroyed%29.%20Developed%20by%20the%0AKuyesera%20AI%20Lab%20at%20the%20Malawi%20University%20of%20Business%20and%20Applied%20Sciences%2C%20this%0Adataset%20is%20intended%20to%20facilitate%20the%20development%20of%20machine%20learning%20models%0Atailored%20to%20building%20detection%20and%20damage%20classification%20in%20African%20urban%0Acontexts.%20It%20also%20supports%20flood%20damage%20visualisation%20and%20spatial%20analysis%20to%0Ainform%20decisions%20on%20relocation%2C%20infrastructure%20planning%2C%20and%20emergency%20response%0Ain%20climate-vulnerable%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmwBTFreddy%253A%2520A%2520Dataset%2520for%2520Flash%2520Flood%2520Damage%2520Assessment%2520in%2520Urban%2520Malawi%26entry.906535625%3DEvelyn%2520Chapuma%2520and%2520Grey%2520Mengezi%2520and%2520Lewis%2520Msasa%2520and%2520Amelia%2520Taylor%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520the%2520mwBTFreddy%2520dataset%252C%2520a%2520resource%2520developed%2520to%2520support%250Aflash%2520flood%2520damage%2520assessment%2520in%2520urban%2520Malawi%252C%2520specifically%2520focusing%2520on%2520the%250Aimpacts%2520of%2520Cyclone%2520Freddy%2520in%25202023.%2520The%2520dataset%2520comprises%2520paired%2520pre-%2520and%250Apost-disaster%2520satellite%2520images%2520sourced%2520from%2520Google%2520Earth%2520Pro%252C%2520accompanied%2520by%250AJSON%2520files%2520containing%2520labelled%2520building%2520annotations%2520with%2520geographic%2520coordinates%250Aand%2520damage%2520levels%2520%2528no%2520damage%252C%2520minor%252C%2520major%252C%2520or%2520destroyed%2529.%2520Developed%2520by%2520the%250AKuyesera%2520AI%2520Lab%2520at%2520the%2520Malawi%2520University%2520of%2520Business%2520and%2520Applied%2520Sciences%252C%2520this%250Adataset%2520is%2520intended%2520to%2520facilitate%2520the%2520development%2520of%2520machine%2520learning%2520models%250Atailored%2520to%2520building%2520detection%2520and%2520damage%2520classification%2520in%2520African%2520urban%250Acontexts.%2520It%2520also%2520supports%2520flood%2520damage%2520visualisation%2520and%2520spatial%2520analysis%2520to%250Ainform%2520decisions%2520on%2520relocation%252C%2520infrastructure%2520planning%252C%2520and%2520emergency%2520response%250Ain%2520climate-vulnerable%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mwBTFreddy%3A%20A%20Dataset%20for%20Flash%20Flood%20Damage%20Assessment%20in%20Urban%20Malawi&entry.906535625=Evelyn%20Chapuma%20and%20Grey%20Mengezi%20and%20Lewis%20Msasa%20and%20Amelia%20Taylor&entry.1292438233=%20%20This%20paper%20describes%20the%20mwBTFreddy%20dataset%2C%20a%20resource%20developed%20to%20support%0Aflash%20flood%20damage%20assessment%20in%20urban%20Malawi%2C%20specifically%20focusing%20on%20the%0Aimpacts%20of%20Cyclone%20Freddy%20in%202023.%20The%20dataset%20comprises%20paired%20pre-%20and%0Apost-disaster%20satellite%20images%20sourced%20from%20Google%20Earth%20Pro%2C%20accompanied%20by%0AJSON%20files%20containing%20labelled%20building%20annotations%20with%20geographic%20coordinates%0Aand%20damage%20levels%20%28no%20damage%2C%20minor%2C%20major%2C%20or%20destroyed%29.%20Developed%20by%20the%0AKuyesera%20AI%20Lab%20at%20the%20Malawi%20University%20of%20Business%20and%20Applied%20Sciences%2C%20this%0Adataset%20is%20intended%20to%20facilitate%20the%20development%20of%20machine%20learning%20models%0Atailored%20to%20building%20detection%20and%20damage%20classification%20in%20African%20urban%0Acontexts.%20It%20also%20supports%20flood%20damage%20visualisation%20and%20spatial%20analysis%20to%0Ainform%20decisions%20on%20relocation%2C%20infrastructure%20planning%2C%20and%20emergency%20response%0Ain%20climate-vulnerable%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01242v1&entry.124074799=Read"},
{"title": "AirExo-2: Scaling up Generalizable Robotic Imitation Learning with\n  Low-Cost Exoskeletons", "author": "Hongjie Fang and Chenxi Wang and Yiming Wang and Jingjing Chen and Shangning Xia and Jun Lv and Zihao He and Xiyan Yi and Yunhan Guo and Xinyu Zhan and Lixin Yang and Weiming Wang and Cewu Lu and Hao-Shu Fang", "abstract": "  Scaling up robotic imitation learning for real-world applications requires\nefficient and scalable demonstration collection methods. While teleoperation is\neffective, it depends on costly and inflexible robot platforms. In-the-wild\ndemonstrations offer a promising alternative, but existing collection devices\nhave key limitations: handheld setups offer limited observational coverage, and\nwhole-body systems often require fine-tuning with robot data due to domain\ngaps. To address these challenges, we present AirExo-2, a low-cost exoskeleton\nsystem for large-scale in-the-wild data collection, along with several adaptors\nthat transform collected data into pseudo-robot demonstrations suitable for\npolicy learning. We further introduce RISE-2, a generalizable imitation\nlearning policy that fuses 3D spatial and 2D semantic perception for robust\nmanipulations. Experiments show that RISE-2 outperforms prior state-of-the-art\nmethods on both in-domain and generalization evaluations. Trained solely on\nadapted in-the-wild data produced by AirExo-2, the RISE-2 policy achieves\ncomparable performance to the policy trained with teleoperated data,\nhighlighting the effectiveness and potential of AirExo-2 for scalable and\ngeneralizable imitation learning.\n", "link": "http://arxiv.org/abs/2503.03081v2", "date": "2025-05-02", "relevancy": 1.6245, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6152}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5434}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AirExo-2%3A%20Scaling%20up%20Generalizable%20Robotic%20Imitation%20Learning%20with%0A%20%20Low-Cost%20Exoskeletons&body=Title%3A%20AirExo-2%3A%20Scaling%20up%20Generalizable%20Robotic%20Imitation%20Learning%20with%0A%20%20Low-Cost%20Exoskeletons%0AAuthor%3A%20Hongjie%20Fang%20and%20Chenxi%20Wang%20and%20Yiming%20Wang%20and%20Jingjing%20Chen%20and%20Shangning%20Xia%20and%20Jun%20Lv%20and%20Zihao%20He%20and%20Xiyan%20Yi%20and%20Yunhan%20Guo%20and%20Xinyu%20Zhan%20and%20Lixin%20Yang%20and%20Weiming%20Wang%20and%20Cewu%20Lu%20and%20Hao-Shu%20Fang%0AAbstract%3A%20%20%20Scaling%20up%20robotic%20imitation%20learning%20for%20real-world%20applications%20requires%0Aefficient%20and%20scalable%20demonstration%20collection%20methods.%20While%20teleoperation%20is%0Aeffective%2C%20it%20depends%20on%20costly%20and%20inflexible%20robot%20platforms.%20In-the-wild%0Ademonstrations%20offer%20a%20promising%20alternative%2C%20but%20existing%20collection%20devices%0Ahave%20key%20limitations%3A%20handheld%20setups%20offer%20limited%20observational%20coverage%2C%20and%0Awhole-body%20systems%20often%20require%20fine-tuning%20with%20robot%20data%20due%20to%20domain%0Agaps.%20To%20address%20these%20challenges%2C%20we%20present%20AirExo-2%2C%20a%20low-cost%20exoskeleton%0Asystem%20for%20large-scale%20in-the-wild%20data%20collection%2C%20along%20with%20several%20adaptors%0Athat%20transform%20collected%20data%20into%20pseudo-robot%20demonstrations%20suitable%20for%0Apolicy%20learning.%20We%20further%20introduce%20RISE-2%2C%20a%20generalizable%20imitation%0Alearning%20policy%20that%20fuses%203D%20spatial%20and%202D%20semantic%20perception%20for%20robust%0Amanipulations.%20Experiments%20show%20that%20RISE-2%20outperforms%20prior%20state-of-the-art%0Amethods%20on%20both%20in-domain%20and%20generalization%20evaluations.%20Trained%20solely%20on%0Aadapted%20in-the-wild%20data%20produced%20by%20AirExo-2%2C%20the%20RISE-2%20policy%20achieves%0Acomparable%20performance%20to%20the%20policy%20trained%20with%20teleoperated%20data%2C%0Ahighlighting%20the%20effectiveness%20and%20potential%20of%20AirExo-2%20for%20scalable%20and%0Ageneralizable%20imitation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirExo-2%253A%2520Scaling%2520up%2520Generalizable%2520Robotic%2520Imitation%2520Learning%2520with%250A%2520%2520Low-Cost%2520Exoskeletons%26entry.906535625%3DHongjie%2520Fang%2520and%2520Chenxi%2520Wang%2520and%2520Yiming%2520Wang%2520and%2520Jingjing%2520Chen%2520and%2520Shangning%2520Xia%2520and%2520Jun%2520Lv%2520and%2520Zihao%2520He%2520and%2520Xiyan%2520Yi%2520and%2520Yunhan%2520Guo%2520and%2520Xinyu%2520Zhan%2520and%2520Lixin%2520Yang%2520and%2520Weiming%2520Wang%2520and%2520Cewu%2520Lu%2520and%2520Hao-Shu%2520Fang%26entry.1292438233%3D%2520%2520Scaling%2520up%2520robotic%2520imitation%2520learning%2520for%2520real-world%2520applications%2520requires%250Aefficient%2520and%2520scalable%2520demonstration%2520collection%2520methods.%2520While%2520teleoperation%2520is%250Aeffective%252C%2520it%2520depends%2520on%2520costly%2520and%2520inflexible%2520robot%2520platforms.%2520In-the-wild%250Ademonstrations%2520offer%2520a%2520promising%2520alternative%252C%2520but%2520existing%2520collection%2520devices%250Ahave%2520key%2520limitations%253A%2520handheld%2520setups%2520offer%2520limited%2520observational%2520coverage%252C%2520and%250Awhole-body%2520systems%2520often%2520require%2520fine-tuning%2520with%2520robot%2520data%2520due%2520to%2520domain%250Agaps.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520AirExo-2%252C%2520a%2520low-cost%2520exoskeleton%250Asystem%2520for%2520large-scale%2520in-the-wild%2520data%2520collection%252C%2520along%2520with%2520several%2520adaptors%250Athat%2520transform%2520collected%2520data%2520into%2520pseudo-robot%2520demonstrations%2520suitable%2520for%250Apolicy%2520learning.%2520We%2520further%2520introduce%2520RISE-2%252C%2520a%2520generalizable%2520imitation%250Alearning%2520policy%2520that%2520fuses%25203D%2520spatial%2520and%25202D%2520semantic%2520perception%2520for%2520robust%250Amanipulations.%2520Experiments%2520show%2520that%2520RISE-2%2520outperforms%2520prior%2520state-of-the-art%250Amethods%2520on%2520both%2520in-domain%2520and%2520generalization%2520evaluations.%2520Trained%2520solely%2520on%250Aadapted%2520in-the-wild%2520data%2520produced%2520by%2520AirExo-2%252C%2520the%2520RISE-2%2520policy%2520achieves%250Acomparable%2520performance%2520to%2520the%2520policy%2520trained%2520with%2520teleoperated%2520data%252C%250Ahighlighting%2520the%2520effectiveness%2520and%2520potential%2520of%2520AirExo-2%2520for%2520scalable%2520and%250Ageneralizable%2520imitation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AirExo-2%3A%20Scaling%20up%20Generalizable%20Robotic%20Imitation%20Learning%20with%0A%20%20Low-Cost%20Exoskeletons&entry.906535625=Hongjie%20Fang%20and%20Chenxi%20Wang%20and%20Yiming%20Wang%20and%20Jingjing%20Chen%20and%20Shangning%20Xia%20and%20Jun%20Lv%20and%20Zihao%20He%20and%20Xiyan%20Yi%20and%20Yunhan%20Guo%20and%20Xinyu%20Zhan%20and%20Lixin%20Yang%20and%20Weiming%20Wang%20and%20Cewu%20Lu%20and%20Hao-Shu%20Fang&entry.1292438233=%20%20Scaling%20up%20robotic%20imitation%20learning%20for%20real-world%20applications%20requires%0Aefficient%20and%20scalable%20demonstration%20collection%20methods.%20While%20teleoperation%20is%0Aeffective%2C%20it%20depends%20on%20costly%20and%20inflexible%20robot%20platforms.%20In-the-wild%0Ademonstrations%20offer%20a%20promising%20alternative%2C%20but%20existing%20collection%20devices%0Ahave%20key%20limitations%3A%20handheld%20setups%20offer%20limited%20observational%20coverage%2C%20and%0Awhole-body%20systems%20often%20require%20fine-tuning%20with%20robot%20data%20due%20to%20domain%0Agaps.%20To%20address%20these%20challenges%2C%20we%20present%20AirExo-2%2C%20a%20low-cost%20exoskeleton%0Asystem%20for%20large-scale%20in-the-wild%20data%20collection%2C%20along%20with%20several%20adaptors%0Athat%20transform%20collected%20data%20into%20pseudo-robot%20demonstrations%20suitable%20for%0Apolicy%20learning.%20We%20further%20introduce%20RISE-2%2C%20a%20generalizable%20imitation%0Alearning%20policy%20that%20fuses%203D%20spatial%20and%202D%20semantic%20perception%20for%20robust%0Amanipulations.%20Experiments%20show%20that%20RISE-2%20outperforms%20prior%20state-of-the-art%0Amethods%20on%20both%20in-domain%20and%20generalization%20evaluations.%20Trained%20solely%20on%0Aadapted%20in-the-wild%20data%20produced%20by%20AirExo-2%2C%20the%20RISE-2%20policy%20achieves%0Acomparable%20performance%20to%20the%20policy%20trained%20with%20teleoperated%20data%2C%0Ahighlighting%20the%20effectiveness%20and%20potential%20of%20AirExo-2%20for%20scalable%20and%0Ageneralizable%20imitation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03081v2&entry.124074799=Read"},
{"title": "SIME: Enhancing Policy Self-Improvement with Modal-level Exploration", "author": "Yang Jin and Jun Lv and Wenye Yu and Hongjie Fang and Yong-Lu Li and Cewu Lu", "abstract": "  Self-improvement requires robotic systems to initially learn from\nhuman-provided data and then gradually enhance their capabilities through\ninteraction with the environment. This is similar to how humans improve their\nskills through continuous practice. However, achieving effective\nself-improvement is challenging, primarily because robots tend to repeat their\nexisting abilities during interactions, often failing to generate new, valuable\ndata for learning. In this paper, we identify the key to successful\nself-improvement: modal-level exploration and data selection. By incorporating\na modal-level exploration mechanism during policy execution, the robot can\nproduce more diverse and multi-modal interactions. At the same time, we select\nthe most valuable trials and high-quality segments from these interactions for\nlearning. We successfully demonstrate effective robot self-improvement on both\nsimulation benchmarks and real-world experiments. The capability for\nself-improvement will enable us to develop more robust and high-success-rate\nrobotic control strategies at a lower cost. Our code and experiment scripts are\navailable at https://ericjin2002.github.io/SIME/\n", "link": "http://arxiv.org/abs/2505.01396v1", "date": "2025-05-02", "relevancy": 1.6119, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5925}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5708}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIME%3A%20Enhancing%20Policy%20Self-Improvement%20with%20Modal-level%20Exploration&body=Title%3A%20SIME%3A%20Enhancing%20Policy%20Self-Improvement%20with%20Modal-level%20Exploration%0AAuthor%3A%20Yang%20Jin%20and%20Jun%20Lv%20and%20Wenye%20Yu%20and%20Hongjie%20Fang%20and%20Yong-Lu%20Li%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20Self-improvement%20requires%20robotic%20systems%20to%20initially%20learn%20from%0Ahuman-provided%20data%20and%20then%20gradually%20enhance%20their%20capabilities%20through%0Ainteraction%20with%20the%20environment.%20This%20is%20similar%20to%20how%20humans%20improve%20their%0Askills%20through%20continuous%20practice.%20However%2C%20achieving%20effective%0Aself-improvement%20is%20challenging%2C%20primarily%20because%20robots%20tend%20to%20repeat%20their%0Aexisting%20abilities%20during%20interactions%2C%20often%20failing%20to%20generate%20new%2C%20valuable%0Adata%20for%20learning.%20In%20this%20paper%2C%20we%20identify%20the%20key%20to%20successful%0Aself-improvement%3A%20modal-level%20exploration%20and%20data%20selection.%20By%20incorporating%0Aa%20modal-level%20exploration%20mechanism%20during%20policy%20execution%2C%20the%20robot%20can%0Aproduce%20more%20diverse%20and%20multi-modal%20interactions.%20At%20the%20same%20time%2C%20we%20select%0Athe%20most%20valuable%20trials%20and%20high-quality%20segments%20from%20these%20interactions%20for%0Alearning.%20We%20successfully%20demonstrate%20effective%20robot%20self-improvement%20on%20both%0Asimulation%20benchmarks%20and%20real-world%20experiments.%20The%20capability%20for%0Aself-improvement%20will%20enable%20us%20to%20develop%20more%20robust%20and%20high-success-rate%0Arobotic%20control%20strategies%20at%20a%20lower%20cost.%20Our%20code%20and%20experiment%20scripts%20are%0Aavailable%20at%20https%3A//ericjin2002.github.io/SIME/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIME%253A%2520Enhancing%2520Policy%2520Self-Improvement%2520with%2520Modal-level%2520Exploration%26entry.906535625%3DYang%2520Jin%2520and%2520Jun%2520Lv%2520and%2520Wenye%2520Yu%2520and%2520Hongjie%2520Fang%2520and%2520Yong-Lu%2520Li%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520Self-improvement%2520requires%2520robotic%2520systems%2520to%2520initially%2520learn%2520from%250Ahuman-provided%2520data%2520and%2520then%2520gradually%2520enhance%2520their%2520capabilities%2520through%250Ainteraction%2520with%2520the%2520environment.%2520This%2520is%2520similar%2520to%2520how%2520humans%2520improve%2520their%250Askills%2520through%2520continuous%2520practice.%2520However%252C%2520achieving%2520effective%250Aself-improvement%2520is%2520challenging%252C%2520primarily%2520because%2520robots%2520tend%2520to%2520repeat%2520their%250Aexisting%2520abilities%2520during%2520interactions%252C%2520often%2520failing%2520to%2520generate%2520new%252C%2520valuable%250Adata%2520for%2520learning.%2520In%2520this%2520paper%252C%2520we%2520identify%2520the%2520key%2520to%2520successful%250Aself-improvement%253A%2520modal-level%2520exploration%2520and%2520data%2520selection.%2520By%2520incorporating%250Aa%2520modal-level%2520exploration%2520mechanism%2520during%2520policy%2520execution%252C%2520the%2520robot%2520can%250Aproduce%2520more%2520diverse%2520and%2520multi-modal%2520interactions.%2520At%2520the%2520same%2520time%252C%2520we%2520select%250Athe%2520most%2520valuable%2520trials%2520and%2520high-quality%2520segments%2520from%2520these%2520interactions%2520for%250Alearning.%2520We%2520successfully%2520demonstrate%2520effective%2520robot%2520self-improvement%2520on%2520both%250Asimulation%2520benchmarks%2520and%2520real-world%2520experiments.%2520The%2520capability%2520for%250Aself-improvement%2520will%2520enable%2520us%2520to%2520develop%2520more%2520robust%2520and%2520high-success-rate%250Arobotic%2520control%2520strategies%2520at%2520a%2520lower%2520cost.%2520Our%2520code%2520and%2520experiment%2520scripts%2520are%250Aavailable%2520at%2520https%253A//ericjin2002.github.io/SIME/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIME%3A%20Enhancing%20Policy%20Self-Improvement%20with%20Modal-level%20Exploration&entry.906535625=Yang%20Jin%20and%20Jun%20Lv%20and%20Wenye%20Yu%20and%20Hongjie%20Fang%20and%20Yong-Lu%20Li%20and%20Cewu%20Lu&entry.1292438233=%20%20Self-improvement%20requires%20robotic%20systems%20to%20initially%20learn%20from%0Ahuman-provided%20data%20and%20then%20gradually%20enhance%20their%20capabilities%20through%0Ainteraction%20with%20the%20environment.%20This%20is%20similar%20to%20how%20humans%20improve%20their%0Askills%20through%20continuous%20practice.%20However%2C%20achieving%20effective%0Aself-improvement%20is%20challenging%2C%20primarily%20because%20robots%20tend%20to%20repeat%20their%0Aexisting%20abilities%20during%20interactions%2C%20often%20failing%20to%20generate%20new%2C%20valuable%0Adata%20for%20learning.%20In%20this%20paper%2C%20we%20identify%20the%20key%20to%20successful%0Aself-improvement%3A%20modal-level%20exploration%20and%20data%20selection.%20By%20incorporating%0Aa%20modal-level%20exploration%20mechanism%20during%20policy%20execution%2C%20the%20robot%20can%0Aproduce%20more%20diverse%20and%20multi-modal%20interactions.%20At%20the%20same%20time%2C%20we%20select%0Athe%20most%20valuable%20trials%20and%20high-quality%20segments%20from%20these%20interactions%20for%0Alearning.%20We%20successfully%20demonstrate%20effective%20robot%20self-improvement%20on%20both%0Asimulation%20benchmarks%20and%20real-world%20experiments.%20The%20capability%20for%0Aself-improvement%20will%20enable%20us%20to%20develop%20more%20robust%20and%20high-success-rate%0Arobotic%20control%20strategies%20at%20a%20lower%20cost.%20Our%20code%20and%20experiment%20scripts%20are%0Aavailable%20at%20https%3A//ericjin2002.github.io/SIME/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01396v1&entry.124074799=Read"},
{"title": "Fast Flow-based Visuomotor Policies via Conditional Optimal Transport\n  Couplings", "author": "Andreas Sochopoulos and Nikolay Malkin and Nikolaos Tsagkas and Jo\u00e3o Moura and Michael Gienger and Sethu Vijayakumar", "abstract": "  Diffusion and flow matching policies have recently demonstrated remarkable\nperformance in robotic applications by accurately capturing multimodal robot\ntrajectory distributions. However, their computationally expensive inference,\ndue to the numerical integration of an ODE or SDE, limits their applicability\nas real-time controllers for robots. We introduce a methodology that utilizes\nconditional Optimal Transport couplings between noise and samples to enforce\nstraight solutions in the flow ODE for robot action generation tasks. We show\nthat naively coupling noise and samples fails in conditional tasks and propose\nincorporating condition variables into the coupling process to improve few-step\nperformance. The proposed few-step policy achieves a 4% higher success rate\nwith a 10x speed-up compared to Diffusion Policy on a diverse set of simulation\ntasks. Moreover, it produces high-quality and diverse action trajectories\nwithin 1-2 steps on a set of real-world robot tasks. Our method also retains\nthe same training complexity as Diffusion Policy and vanilla Flow Matching, in\ncontrast to distillation-based approaches.\n", "link": "http://arxiv.org/abs/2505.01179v1", "date": "2025-05-02", "relevancy": 1.6055, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5731}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5285}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Flow-based%20Visuomotor%20Policies%20via%20Conditional%20Optimal%20Transport%0A%20%20Couplings&body=Title%3A%20Fast%20Flow-based%20Visuomotor%20Policies%20via%20Conditional%20Optimal%20Transport%0A%20%20Couplings%0AAuthor%3A%20Andreas%20Sochopoulos%20and%20Nikolay%20Malkin%20and%20Nikolaos%20Tsagkas%20and%20Jo%C3%A3o%20Moura%20and%20Michael%20Gienger%20and%20Sethu%20Vijayakumar%0AAbstract%3A%20%20%20Diffusion%20and%20flow%20matching%20policies%20have%20recently%20demonstrated%20remarkable%0Aperformance%20in%20robotic%20applications%20by%20accurately%20capturing%20multimodal%20robot%0Atrajectory%20distributions.%20However%2C%20their%20computationally%20expensive%20inference%2C%0Adue%20to%20the%20numerical%20integration%20of%20an%20ODE%20or%20SDE%2C%20limits%20their%20applicability%0Aas%20real-time%20controllers%20for%20robots.%20We%20introduce%20a%20methodology%20that%20utilizes%0Aconditional%20Optimal%20Transport%20couplings%20between%20noise%20and%20samples%20to%20enforce%0Astraight%20solutions%20in%20the%20flow%20ODE%20for%20robot%20action%20generation%20tasks.%20We%20show%0Athat%20naively%20coupling%20noise%20and%20samples%20fails%20in%20conditional%20tasks%20and%20propose%0Aincorporating%20condition%20variables%20into%20the%20coupling%20process%20to%20improve%20few-step%0Aperformance.%20The%20proposed%20few-step%20policy%20achieves%20a%204%25%20higher%20success%20rate%0Awith%20a%2010x%20speed-up%20compared%20to%20Diffusion%20Policy%20on%20a%20diverse%20set%20of%20simulation%0Atasks.%20Moreover%2C%20it%20produces%20high-quality%20and%20diverse%20action%20trajectories%0Awithin%201-2%20steps%20on%20a%20set%20of%20real-world%20robot%20tasks.%20Our%20method%20also%20retains%0Athe%20same%20training%20complexity%20as%20Diffusion%20Policy%20and%20vanilla%20Flow%20Matching%2C%20in%0Acontrast%20to%20distillation-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Flow-based%2520Visuomotor%2520Policies%2520via%2520Conditional%2520Optimal%2520Transport%250A%2520%2520Couplings%26entry.906535625%3DAndreas%2520Sochopoulos%2520and%2520Nikolay%2520Malkin%2520and%2520Nikolaos%2520Tsagkas%2520and%2520Jo%25C3%25A3o%2520Moura%2520and%2520Michael%2520Gienger%2520and%2520Sethu%2520Vijayakumar%26entry.1292438233%3D%2520%2520Diffusion%2520and%2520flow%2520matching%2520policies%2520have%2520recently%2520demonstrated%2520remarkable%250Aperformance%2520in%2520robotic%2520applications%2520by%2520accurately%2520capturing%2520multimodal%2520robot%250Atrajectory%2520distributions.%2520However%252C%2520their%2520computationally%2520expensive%2520inference%252C%250Adue%2520to%2520the%2520numerical%2520integration%2520of%2520an%2520ODE%2520or%2520SDE%252C%2520limits%2520their%2520applicability%250Aas%2520real-time%2520controllers%2520for%2520robots.%2520We%2520introduce%2520a%2520methodology%2520that%2520utilizes%250Aconditional%2520Optimal%2520Transport%2520couplings%2520between%2520noise%2520and%2520samples%2520to%2520enforce%250Astraight%2520solutions%2520in%2520the%2520flow%2520ODE%2520for%2520robot%2520action%2520generation%2520tasks.%2520We%2520show%250Athat%2520naively%2520coupling%2520noise%2520and%2520samples%2520fails%2520in%2520conditional%2520tasks%2520and%2520propose%250Aincorporating%2520condition%2520variables%2520into%2520the%2520coupling%2520process%2520to%2520improve%2520few-step%250Aperformance.%2520The%2520proposed%2520few-step%2520policy%2520achieves%2520a%25204%2525%2520higher%2520success%2520rate%250Awith%2520a%252010x%2520speed-up%2520compared%2520to%2520Diffusion%2520Policy%2520on%2520a%2520diverse%2520set%2520of%2520simulation%250Atasks.%2520Moreover%252C%2520it%2520produces%2520high-quality%2520and%2520diverse%2520action%2520trajectories%250Awithin%25201-2%2520steps%2520on%2520a%2520set%2520of%2520real-world%2520robot%2520tasks.%2520Our%2520method%2520also%2520retains%250Athe%2520same%2520training%2520complexity%2520as%2520Diffusion%2520Policy%2520and%2520vanilla%2520Flow%2520Matching%252C%2520in%250Acontrast%2520to%2520distillation-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Flow-based%20Visuomotor%20Policies%20via%20Conditional%20Optimal%20Transport%0A%20%20Couplings&entry.906535625=Andreas%20Sochopoulos%20and%20Nikolay%20Malkin%20and%20Nikolaos%20Tsagkas%20and%20Jo%C3%A3o%20Moura%20and%20Michael%20Gienger%20and%20Sethu%20Vijayakumar&entry.1292438233=%20%20Diffusion%20and%20flow%20matching%20policies%20have%20recently%20demonstrated%20remarkable%0Aperformance%20in%20robotic%20applications%20by%20accurately%20capturing%20multimodal%20robot%0Atrajectory%20distributions.%20However%2C%20their%20computationally%20expensive%20inference%2C%0Adue%20to%20the%20numerical%20integration%20of%20an%20ODE%20or%20SDE%2C%20limits%20their%20applicability%0Aas%20real-time%20controllers%20for%20robots.%20We%20introduce%20a%20methodology%20that%20utilizes%0Aconditional%20Optimal%20Transport%20couplings%20between%20noise%20and%20samples%20to%20enforce%0Astraight%20solutions%20in%20the%20flow%20ODE%20for%20robot%20action%20generation%20tasks.%20We%20show%0Athat%20naively%20coupling%20noise%20and%20samples%20fails%20in%20conditional%20tasks%20and%20propose%0Aincorporating%20condition%20variables%20into%20the%20coupling%20process%20to%20improve%20few-step%0Aperformance.%20The%20proposed%20few-step%20policy%20achieves%20a%204%25%20higher%20success%20rate%0Awith%20a%2010x%20speed-up%20compared%20to%20Diffusion%20Policy%20on%20a%20diverse%20set%20of%20simulation%0Atasks.%20Moreover%2C%20it%20produces%20high-quality%20and%20diverse%20action%20trajectories%0Awithin%201-2%20steps%20on%20a%20set%20of%20real-world%20robot%20tasks.%20Our%20method%20also%20retains%0Athe%20same%20training%20complexity%20as%20Diffusion%20Policy%20and%20vanilla%20Flow%20Matching%2C%20in%0Acontrast%20to%20distillation-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01179v1&entry.124074799=Read"},
{"title": "Fusing Foveal Fixations Using Linear Retinal Transformations and\n  Bayesian Experimental Design", "author": "Christopher K. I. Williams", "abstract": "  Humans (and many vertebrates) face the problem of fusing together multiple\nfixations of a scene in order to obtain a representation of the whole, where\neach fixation uses a high-resolution fovea and decreasing resolution in the\nperiphery. In this paper we explicitly represent the retinal transformation of\na fixation as a linear downsampling of a high-resolution latent image of the\nscene, exploiting the known geometry. This linear transformation allows us to\ncarry out exact inference for the latent variables in factor analysis (FA) and\nmixtures of FA models of the scene. Further, this allows us to formulate and\nsolve the choice of \"where to look next\" as a Bayesian experimental design\nproblem using the Expected Information Gain criterion. Experiments on the Frey\nfaces and MNIST datasets demonstrate the effectiveness of our models.\n", "link": "http://arxiv.org/abs/2505.01249v1", "date": "2025-05-02", "relevancy": 1.6049, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5436}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5384}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusing%20Foveal%20Fixations%20Using%20Linear%20Retinal%20Transformations%20and%0A%20%20Bayesian%20Experimental%20Design&body=Title%3A%20Fusing%20Foveal%20Fixations%20Using%20Linear%20Retinal%20Transformations%20and%0A%20%20Bayesian%20Experimental%20Design%0AAuthor%3A%20Christopher%20K.%20I.%20Williams%0AAbstract%3A%20%20%20Humans%20%28and%20many%20vertebrates%29%20face%20the%20problem%20of%20fusing%20together%20multiple%0Afixations%20of%20a%20scene%20in%20order%20to%20obtain%20a%20representation%20of%20the%20whole%2C%20where%0Aeach%20fixation%20uses%20a%20high-resolution%20fovea%20and%20decreasing%20resolution%20in%20the%0Aperiphery.%20In%20this%20paper%20we%20explicitly%20represent%20the%20retinal%20transformation%20of%0Aa%20fixation%20as%20a%20linear%20downsampling%20of%20a%20high-resolution%20latent%20image%20of%20the%0Ascene%2C%20exploiting%20the%20known%20geometry.%20This%20linear%20transformation%20allows%20us%20to%0Acarry%20out%20exact%20inference%20for%20the%20latent%20variables%20in%20factor%20analysis%20%28FA%29%20and%0Amixtures%20of%20FA%20models%20of%20the%20scene.%20Further%2C%20this%20allows%20us%20to%20formulate%20and%0Asolve%20the%20choice%20of%20%22where%20to%20look%20next%22%20as%20a%20Bayesian%20experimental%20design%0Aproblem%20using%20the%20Expected%20Information%20Gain%20criterion.%20Experiments%20on%20the%20Frey%0Afaces%20and%20MNIST%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusing%2520Foveal%2520Fixations%2520Using%2520Linear%2520Retinal%2520Transformations%2520and%250A%2520%2520Bayesian%2520Experimental%2520Design%26entry.906535625%3DChristopher%2520K.%2520I.%2520Williams%26entry.1292438233%3D%2520%2520Humans%2520%2528and%2520many%2520vertebrates%2529%2520face%2520the%2520problem%2520of%2520fusing%2520together%2520multiple%250Afixations%2520of%2520a%2520scene%2520in%2520order%2520to%2520obtain%2520a%2520representation%2520of%2520the%2520whole%252C%2520where%250Aeach%2520fixation%2520uses%2520a%2520high-resolution%2520fovea%2520and%2520decreasing%2520resolution%2520in%2520the%250Aperiphery.%2520In%2520this%2520paper%2520we%2520explicitly%2520represent%2520the%2520retinal%2520transformation%2520of%250Aa%2520fixation%2520as%2520a%2520linear%2520downsampling%2520of%2520a%2520high-resolution%2520latent%2520image%2520of%2520the%250Ascene%252C%2520exploiting%2520the%2520known%2520geometry.%2520This%2520linear%2520transformation%2520allows%2520us%2520to%250Acarry%2520out%2520exact%2520inference%2520for%2520the%2520latent%2520variables%2520in%2520factor%2520analysis%2520%2528FA%2529%2520and%250Amixtures%2520of%2520FA%2520models%2520of%2520the%2520scene.%2520Further%252C%2520this%2520allows%2520us%2520to%2520formulate%2520and%250Asolve%2520the%2520choice%2520of%2520%2522where%2520to%2520look%2520next%2522%2520as%2520a%2520Bayesian%2520experimental%2520design%250Aproblem%2520using%2520the%2520Expected%2520Information%2520Gain%2520criterion.%2520Experiments%2520on%2520the%2520Frey%250Afaces%2520and%2520MNIST%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusing%20Foveal%20Fixations%20Using%20Linear%20Retinal%20Transformations%20and%0A%20%20Bayesian%20Experimental%20Design&entry.906535625=Christopher%20K.%20I.%20Williams&entry.1292438233=%20%20Humans%20%28and%20many%20vertebrates%29%20face%20the%20problem%20of%20fusing%20together%20multiple%0Afixations%20of%20a%20scene%20in%20order%20to%20obtain%20a%20representation%20of%20the%20whole%2C%20where%0Aeach%20fixation%20uses%20a%20high-resolution%20fovea%20and%20decreasing%20resolution%20in%20the%0Aperiphery.%20In%20this%20paper%20we%20explicitly%20represent%20the%20retinal%20transformation%20of%0Aa%20fixation%20as%20a%20linear%20downsampling%20of%20a%20high-resolution%20latent%20image%20of%20the%0Ascene%2C%20exploiting%20the%20known%20geometry.%20This%20linear%20transformation%20allows%20us%20to%0Acarry%20out%20exact%20inference%20for%20the%20latent%20variables%20in%20factor%20analysis%20%28FA%29%20and%0Amixtures%20of%20FA%20models%20of%20the%20scene.%20Further%2C%20this%20allows%20us%20to%20formulate%20and%0Asolve%20the%20choice%20of%20%22where%20to%20look%20next%22%20as%20a%20Bayesian%20experimental%20design%0Aproblem%20using%20the%20Expected%20Information%20Gain%20criterion.%20Experiments%20on%20the%20Frey%0Afaces%20and%20MNIST%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01249v1&entry.124074799=Read"},
{"title": "Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework\n  for Predicting Pathological Response in Non-Small Cell Lung Cancer", "author": "Alice Natalina Caragliano and Claudia Tacconi and Carlo Greco and Lorenzo Nibid and Edy Ippolito and Michele Fiore and Giuseppe Perrone and Sara Ramella and Paolo Soda and Valerio Guarrasi", "abstract": "  This study proposes a novel approach combining Multimodal Deep Learning with\nintrinsic eXplainable Artificial Intelligence techniques to predict\npathological response in non-small cell lung cancer patients undergoing\nneoadjuvant therapy. Due to the limitations of existing radiomics and unimodal\ndeep learning approaches, we introduce an intermediate fusion strategy that\nintegrates imaging and clinical data, enabling efficient interaction between\ndata modalities. The proposed Multimodal Doctor-in-the-Loop method further\nenhances clinical relevance by embedding clinicians' domain knowledge directly\ninto the training process, guiding the model's focus gradually from broader\nlung regions to specific lesions. Results demonstrate improved predictive\naccuracy and explainability, providing insights into optimal data integration\nstrategies for clinical applications.\n", "link": "http://arxiv.org/abs/2505.01390v1", "date": "2025-05-02", "relevancy": 1.5983, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5376}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5316}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Doctor-in-the-Loop%3A%20A%20Clinically-Guided%20Explainable%20Framework%0A%20%20for%20Predicting%20Pathological%20Response%20in%20Non-Small%20Cell%20Lung%20Cancer&body=Title%3A%20Multimodal%20Doctor-in-the-Loop%3A%20A%20Clinically-Guided%20Explainable%20Framework%0A%20%20for%20Predicting%20Pathological%20Response%20in%20Non-Small%20Cell%20Lung%20Cancer%0AAuthor%3A%20Alice%20Natalina%20Caragliano%20and%20Claudia%20Tacconi%20and%20Carlo%20Greco%20and%20Lorenzo%20Nibid%20and%20Edy%20Ippolito%20and%20Michele%20Fiore%20and%20Giuseppe%20Perrone%20and%20Sara%20Ramella%20and%20Paolo%20Soda%20and%20Valerio%20Guarrasi%0AAbstract%3A%20%20%20This%20study%20proposes%20a%20novel%20approach%20combining%20Multimodal%20Deep%20Learning%20with%0Aintrinsic%20eXplainable%20Artificial%20Intelligence%20techniques%20to%20predict%0Apathological%20response%20in%20non-small%20cell%20lung%20cancer%20patients%20undergoing%0Aneoadjuvant%20therapy.%20Due%20to%20the%20limitations%20of%20existing%20radiomics%20and%20unimodal%0Adeep%20learning%20approaches%2C%20we%20introduce%20an%20intermediate%20fusion%20strategy%20that%0Aintegrates%20imaging%20and%20clinical%20data%2C%20enabling%20efficient%20interaction%20between%0Adata%20modalities.%20The%20proposed%20Multimodal%20Doctor-in-the-Loop%20method%20further%0Aenhances%20clinical%20relevance%20by%20embedding%20clinicians%27%20domain%20knowledge%20directly%0Ainto%20the%20training%20process%2C%20guiding%20the%20model%27s%20focus%20gradually%20from%20broader%0Alung%20regions%20to%20specific%20lesions.%20Results%20demonstrate%20improved%20predictive%0Aaccuracy%20and%20explainability%2C%20providing%20insights%20into%20optimal%20data%20integration%0Astrategies%20for%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Doctor-in-the-Loop%253A%2520A%2520Clinically-Guided%2520Explainable%2520Framework%250A%2520%2520for%2520Predicting%2520Pathological%2520Response%2520in%2520Non-Small%2520Cell%2520Lung%2520Cancer%26entry.906535625%3DAlice%2520Natalina%2520Caragliano%2520and%2520Claudia%2520Tacconi%2520and%2520Carlo%2520Greco%2520and%2520Lorenzo%2520Nibid%2520and%2520Edy%2520Ippolito%2520and%2520Michele%2520Fiore%2520and%2520Giuseppe%2520Perrone%2520and%2520Sara%2520Ramella%2520and%2520Paolo%2520Soda%2520and%2520Valerio%2520Guarrasi%26entry.1292438233%3D%2520%2520This%2520study%2520proposes%2520a%2520novel%2520approach%2520combining%2520Multimodal%2520Deep%2520Learning%2520with%250Aintrinsic%2520eXplainable%2520Artificial%2520Intelligence%2520techniques%2520to%2520predict%250Apathological%2520response%2520in%2520non-small%2520cell%2520lung%2520cancer%2520patients%2520undergoing%250Aneoadjuvant%2520therapy.%2520Due%2520to%2520the%2520limitations%2520of%2520existing%2520radiomics%2520and%2520unimodal%250Adeep%2520learning%2520approaches%252C%2520we%2520introduce%2520an%2520intermediate%2520fusion%2520strategy%2520that%250Aintegrates%2520imaging%2520and%2520clinical%2520data%252C%2520enabling%2520efficient%2520interaction%2520between%250Adata%2520modalities.%2520The%2520proposed%2520Multimodal%2520Doctor-in-the-Loop%2520method%2520further%250Aenhances%2520clinical%2520relevance%2520by%2520embedding%2520clinicians%2527%2520domain%2520knowledge%2520directly%250Ainto%2520the%2520training%2520process%252C%2520guiding%2520the%2520model%2527s%2520focus%2520gradually%2520from%2520broader%250Alung%2520regions%2520to%2520specific%2520lesions.%2520Results%2520demonstrate%2520improved%2520predictive%250Aaccuracy%2520and%2520explainability%252C%2520providing%2520insights%2520into%2520optimal%2520data%2520integration%250Astrategies%2520for%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Doctor-in-the-Loop%3A%20A%20Clinically-Guided%20Explainable%20Framework%0A%20%20for%20Predicting%20Pathological%20Response%20in%20Non-Small%20Cell%20Lung%20Cancer&entry.906535625=Alice%20Natalina%20Caragliano%20and%20Claudia%20Tacconi%20and%20Carlo%20Greco%20and%20Lorenzo%20Nibid%20and%20Edy%20Ippolito%20and%20Michele%20Fiore%20and%20Giuseppe%20Perrone%20and%20Sara%20Ramella%20and%20Paolo%20Soda%20and%20Valerio%20Guarrasi&entry.1292438233=%20%20This%20study%20proposes%20a%20novel%20approach%20combining%20Multimodal%20Deep%20Learning%20with%0Aintrinsic%20eXplainable%20Artificial%20Intelligence%20techniques%20to%20predict%0Apathological%20response%20in%20non-small%20cell%20lung%20cancer%20patients%20undergoing%0Aneoadjuvant%20therapy.%20Due%20to%20the%20limitations%20of%20existing%20radiomics%20and%20unimodal%0Adeep%20learning%20approaches%2C%20we%20introduce%20an%20intermediate%20fusion%20strategy%20that%0Aintegrates%20imaging%20and%20clinical%20data%2C%20enabling%20efficient%20interaction%20between%0Adata%20modalities.%20The%20proposed%20Multimodal%20Doctor-in-the-Loop%20method%20further%0Aenhances%20clinical%20relevance%20by%20embedding%20clinicians%27%20domain%20knowledge%20directly%0Ainto%20the%20training%20process%2C%20guiding%20the%20model%27s%20focus%20gradually%20from%20broader%0Alung%20regions%20to%20specific%20lesions.%20Results%20demonstrate%20improved%20predictive%0Aaccuracy%20and%20explainability%2C%20providing%20insights%20into%20optimal%20data%20integration%0Astrategies%20for%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01390v1&entry.124074799=Read"},
{"title": "From Foresight to Forethought: VLM-In-the-Loop Policy Steering via\n  Latent Alignment", "author": "Yilin Wu and Ran Tian and Gokul Swamy and Andrea Bajcsy", "abstract": "  While generative robot policies have demonstrated significant potential in\nlearning complex, multimodal behaviors from demonstrations, they still exhibit\ndiverse failures at deployment-time. Policy steering offers an elegant solution\nto reducing the chance of failure by using an external verifier to select from\nlow-level actions proposed by an imperfect generative policy. Here, one might\nhope to use a Vision Language Model (VLM) as a verifier, leveraging its\nopen-world reasoning capabilities. However, off-the-shelf VLMs struggle to\nunderstand the consequences of low-level robot actions as they are represented\nfundamentally differently than the text and images the VLM was trained on. In\nresponse, we propose FOREWARN, a novel framework to unlock the potential of\nVLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is\nto decouple the VLM's burden of predicting action outcomes (foresight) from\nevaluation (forethought). For foresight, we leverage a latent world model to\nimagine future latent states given diverse low-level action plans. For\nforethought, we align the VLM with these predicted latent states to reason\nabout the consequences of actions in its native representation--natural\nlanguage--and effectively filter proposed plans. We validate our framework\nacross diverse robotic manipulation tasks, demonstrating its ability to bridge\nrepresentational gaps and provide robust, generalizable policy steering. Videos\ncan be found on the project website: https://yilin-wu98.github.io/forewarn/.\n", "link": "http://arxiv.org/abs/2502.01828v3", "date": "2025-05-02", "relevancy": 1.5947, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5712}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5263}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Foresight%20to%20Forethought%3A%20VLM-In-the-Loop%20Policy%20Steering%20via%0A%20%20Latent%20Alignment&body=Title%3A%20From%20Foresight%20to%20Forethought%3A%20VLM-In-the-Loop%20Policy%20Steering%20via%0A%20%20Latent%20Alignment%0AAuthor%3A%20Yilin%20Wu%20and%20Ran%20Tian%20and%20Gokul%20Swamy%20and%20Andrea%20Bajcsy%0AAbstract%3A%20%20%20While%20generative%20robot%20policies%20have%20demonstrated%20significant%20potential%20in%0Alearning%20complex%2C%20multimodal%20behaviors%20from%20demonstrations%2C%20they%20still%20exhibit%0Adiverse%20failures%20at%20deployment-time.%20Policy%20steering%20offers%20an%20elegant%20solution%0Ato%20reducing%20the%20chance%20of%20failure%20by%20using%20an%20external%20verifier%20to%20select%20from%0Alow-level%20actions%20proposed%20by%20an%20imperfect%20generative%20policy.%20Here%2C%20one%20might%0Ahope%20to%20use%20a%20Vision%20Language%20Model%20%28VLM%29%20as%20a%20verifier%2C%20leveraging%20its%0Aopen-world%20reasoning%20capabilities.%20However%2C%20off-the-shelf%20VLMs%20struggle%20to%0Aunderstand%20the%20consequences%20of%20low-level%20robot%20actions%20as%20they%20are%20represented%0Afundamentally%20differently%20than%20the%20text%20and%20images%20the%20VLM%20was%20trained%20on.%20In%0Aresponse%2C%20we%20propose%20FOREWARN%2C%20a%20novel%20framework%20to%20unlock%20the%20potential%20of%0AVLMs%20as%20open-vocabulary%20verifiers%20for%20runtime%20policy%20steering.%20Our%20key%20idea%20is%0Ato%20decouple%20the%20VLM%27s%20burden%20of%20predicting%20action%20outcomes%20%28foresight%29%20from%0Aevaluation%20%28forethought%29.%20For%20foresight%2C%20we%20leverage%20a%20latent%20world%20model%20to%0Aimagine%20future%20latent%20states%20given%20diverse%20low-level%20action%20plans.%20For%0Aforethought%2C%20we%20align%20the%20VLM%20with%20these%20predicted%20latent%20states%20to%20reason%0Aabout%20the%20consequences%20of%20actions%20in%20its%20native%20representation--natural%0Alanguage--and%20effectively%20filter%20proposed%20plans.%20We%20validate%20our%20framework%0Aacross%20diverse%20robotic%20manipulation%20tasks%2C%20demonstrating%20its%20ability%20to%20bridge%0Arepresentational%20gaps%20and%20provide%20robust%2C%20generalizable%20policy%20steering.%20Videos%0Acan%20be%20found%20on%20the%20project%20website%3A%20https%3A//yilin-wu98.github.io/forewarn/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01828v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Foresight%2520to%2520Forethought%253A%2520VLM-In-the-Loop%2520Policy%2520Steering%2520via%250A%2520%2520Latent%2520Alignment%26entry.906535625%3DYilin%2520Wu%2520and%2520Ran%2520Tian%2520and%2520Gokul%2520Swamy%2520and%2520Andrea%2520Bajcsy%26entry.1292438233%3D%2520%2520While%2520generative%2520robot%2520policies%2520have%2520demonstrated%2520significant%2520potential%2520in%250Alearning%2520complex%252C%2520multimodal%2520behaviors%2520from%2520demonstrations%252C%2520they%2520still%2520exhibit%250Adiverse%2520failures%2520at%2520deployment-time.%2520Policy%2520steering%2520offers%2520an%2520elegant%2520solution%250Ato%2520reducing%2520the%2520chance%2520of%2520failure%2520by%2520using%2520an%2520external%2520verifier%2520to%2520select%2520from%250Alow-level%2520actions%2520proposed%2520by%2520an%2520imperfect%2520generative%2520policy.%2520Here%252C%2520one%2520might%250Ahope%2520to%2520use%2520a%2520Vision%2520Language%2520Model%2520%2528VLM%2529%2520as%2520a%2520verifier%252C%2520leveraging%2520its%250Aopen-world%2520reasoning%2520capabilities.%2520However%252C%2520off-the-shelf%2520VLMs%2520struggle%2520to%250Aunderstand%2520the%2520consequences%2520of%2520low-level%2520robot%2520actions%2520as%2520they%2520are%2520represented%250Afundamentally%2520differently%2520than%2520the%2520text%2520and%2520images%2520the%2520VLM%2520was%2520trained%2520on.%2520In%250Aresponse%252C%2520we%2520propose%2520FOREWARN%252C%2520a%2520novel%2520framework%2520to%2520unlock%2520the%2520potential%2520of%250AVLMs%2520as%2520open-vocabulary%2520verifiers%2520for%2520runtime%2520policy%2520steering.%2520Our%2520key%2520idea%2520is%250Ato%2520decouple%2520the%2520VLM%2527s%2520burden%2520of%2520predicting%2520action%2520outcomes%2520%2528foresight%2529%2520from%250Aevaluation%2520%2528forethought%2529.%2520For%2520foresight%252C%2520we%2520leverage%2520a%2520latent%2520world%2520model%2520to%250Aimagine%2520future%2520latent%2520states%2520given%2520diverse%2520low-level%2520action%2520plans.%2520For%250Aforethought%252C%2520we%2520align%2520the%2520VLM%2520with%2520these%2520predicted%2520latent%2520states%2520to%2520reason%250Aabout%2520the%2520consequences%2520of%2520actions%2520in%2520its%2520native%2520representation--natural%250Alanguage--and%2520effectively%2520filter%2520proposed%2520plans.%2520We%2520validate%2520our%2520framework%250Aacross%2520diverse%2520robotic%2520manipulation%2520tasks%252C%2520demonstrating%2520its%2520ability%2520to%2520bridge%250Arepresentational%2520gaps%2520and%2520provide%2520robust%252C%2520generalizable%2520policy%2520steering.%2520Videos%250Acan%2520be%2520found%2520on%2520the%2520project%2520website%253A%2520https%253A//yilin-wu98.github.io/forewarn/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01828v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Foresight%20to%20Forethought%3A%20VLM-In-the-Loop%20Policy%20Steering%20via%0A%20%20Latent%20Alignment&entry.906535625=Yilin%20Wu%20and%20Ran%20Tian%20and%20Gokul%20Swamy%20and%20Andrea%20Bajcsy&entry.1292438233=%20%20While%20generative%20robot%20policies%20have%20demonstrated%20significant%20potential%20in%0Alearning%20complex%2C%20multimodal%20behaviors%20from%20demonstrations%2C%20they%20still%20exhibit%0Adiverse%20failures%20at%20deployment-time.%20Policy%20steering%20offers%20an%20elegant%20solution%0Ato%20reducing%20the%20chance%20of%20failure%20by%20using%20an%20external%20verifier%20to%20select%20from%0Alow-level%20actions%20proposed%20by%20an%20imperfect%20generative%20policy.%20Here%2C%20one%20might%0Ahope%20to%20use%20a%20Vision%20Language%20Model%20%28VLM%29%20as%20a%20verifier%2C%20leveraging%20its%0Aopen-world%20reasoning%20capabilities.%20However%2C%20off-the-shelf%20VLMs%20struggle%20to%0Aunderstand%20the%20consequences%20of%20low-level%20robot%20actions%20as%20they%20are%20represented%0Afundamentally%20differently%20than%20the%20text%20and%20images%20the%20VLM%20was%20trained%20on.%20In%0Aresponse%2C%20we%20propose%20FOREWARN%2C%20a%20novel%20framework%20to%20unlock%20the%20potential%20of%0AVLMs%20as%20open-vocabulary%20verifiers%20for%20runtime%20policy%20steering.%20Our%20key%20idea%20is%0Ato%20decouple%20the%20VLM%27s%20burden%20of%20predicting%20action%20outcomes%20%28foresight%29%20from%0Aevaluation%20%28forethought%29.%20For%20foresight%2C%20we%20leverage%20a%20latent%20world%20model%20to%0Aimagine%20future%20latent%20states%20given%20diverse%20low-level%20action%20plans.%20For%0Aforethought%2C%20we%20align%20the%20VLM%20with%20these%20predicted%20latent%20states%20to%20reason%0Aabout%20the%20consequences%20of%20actions%20in%20its%20native%20representation--natural%0Alanguage--and%20effectively%20filter%20proposed%20plans.%20We%20validate%20our%20framework%0Aacross%20diverse%20robotic%20manipulation%20tasks%2C%20demonstrating%20its%20ability%20to%20bridge%0Arepresentational%20gaps%20and%20provide%20robust%2C%20generalizable%20policy%20steering.%20Videos%0Acan%20be%20found%20on%20the%20project%20website%3A%20https%3A//yilin-wu98.github.io/forewarn/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01828v3&entry.124074799=Read"},
{"title": "Diffusion-based Adversarial Purification from the Perspective of the\n  Frequency Domain", "author": "Gaozheng Pei and Ke Ma and Yingfei Sun and Qianqian Xu and Qingming Huang", "abstract": "  The diffusion-based adversarial purification methods attempt to drown\nadversarial perturbations into a part of isotropic noise through the forward\nprocess, and then recover the clean images through the reverse process. Due to\nthe lack of distribution information about adversarial perturbations in the\npixel domain, it is often unavoidable to damage normal semantics. We turn to\nthe frequency domain perspective, decomposing the image into amplitude spectrum\nand phase spectrum. We find that for both spectra, the damage caused by\nadversarial perturbations tends to increase monotonically with frequency. This\nmeans that we can extract the content and structural information of the\noriginal clean sample from the frequency components that are less damaged.\nMeanwhile, theoretical analysis indicates that existing purification methods\nindiscriminately damage all frequency components, leading to excessive damage\nto the image. Therefore, we propose a purification method that can eliminate\nadversarial perturbations while maximizing the preservation of the content and\nstructure of the original image. Specifically, at each time step during the\nreverse process, for the amplitude spectrum, we replace the low-frequency\ncomponents of the estimated image's amplitude spectrum with the corresponding\nparts of the adversarial image. For the phase spectrum, we project the phase of\nthe estimated image into a designated range of the adversarial image's phase\nspectrum, focusing on the low frequencies. Empirical evidence from extensive\nexperiments demonstrates that our method significantly outperforms most current\ndefense methods.\n", "link": "http://arxiv.org/abs/2505.01267v1", "date": "2025-05-02", "relevancy": 1.5915, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5492}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5127}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Adversarial%20Purification%20from%20the%20Perspective%20of%20the%0A%20%20Frequency%20Domain&body=Title%3A%20Diffusion-based%20Adversarial%20Purification%20from%20the%20Perspective%20of%20the%0A%20%20Frequency%20Domain%0AAuthor%3A%20Gaozheng%20Pei%20and%20Ke%20Ma%20and%20Yingfei%20Sun%20and%20Qianqian%20Xu%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20The%20diffusion-based%20adversarial%20purification%20methods%20attempt%20to%20drown%0Aadversarial%20perturbations%20into%20a%20part%20of%20isotropic%20noise%20through%20the%20forward%0Aprocess%2C%20and%20then%20recover%20the%20clean%20images%20through%20the%20reverse%20process.%20Due%20to%0Athe%20lack%20of%20distribution%20information%20about%20adversarial%20perturbations%20in%20the%0Apixel%20domain%2C%20it%20is%20often%20unavoidable%20to%20damage%20normal%20semantics.%20We%20turn%20to%0Athe%20frequency%20domain%20perspective%2C%20decomposing%20the%20image%20into%20amplitude%20spectrum%0Aand%20phase%20spectrum.%20We%20find%20that%20for%20both%20spectra%2C%20the%20damage%20caused%20by%0Aadversarial%20perturbations%20tends%20to%20increase%20monotonically%20with%20frequency.%20This%0Ameans%20that%20we%20can%20extract%20the%20content%20and%20structural%20information%20of%20the%0Aoriginal%20clean%20sample%20from%20the%20frequency%20components%20that%20are%20less%20damaged.%0AMeanwhile%2C%20theoretical%20analysis%20indicates%20that%20existing%20purification%20methods%0Aindiscriminately%20damage%20all%20frequency%20components%2C%20leading%20to%20excessive%20damage%0Ato%20the%20image.%20Therefore%2C%20we%20propose%20a%20purification%20method%20that%20can%20eliminate%0Aadversarial%20perturbations%20while%20maximizing%20the%20preservation%20of%20the%20content%20and%0Astructure%20of%20the%20original%20image.%20Specifically%2C%20at%20each%20time%20step%20during%20the%0Areverse%20process%2C%20for%20the%20amplitude%20spectrum%2C%20we%20replace%20the%20low-frequency%0Acomponents%20of%20the%20estimated%20image%27s%20amplitude%20spectrum%20with%20the%20corresponding%0Aparts%20of%20the%20adversarial%20image.%20For%20the%20phase%20spectrum%2C%20we%20project%20the%20phase%20of%0Athe%20estimated%20image%20into%20a%20designated%20range%20of%20the%20adversarial%20image%27s%20phase%0Aspectrum%2C%20focusing%20on%20the%20low%20frequencies.%20Empirical%20evidence%20from%20extensive%0Aexperiments%20demonstrates%20that%20our%20method%20significantly%20outperforms%20most%20current%0Adefense%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-based%2520Adversarial%2520Purification%2520from%2520the%2520Perspective%2520of%2520the%250A%2520%2520Frequency%2520Domain%26entry.906535625%3DGaozheng%2520Pei%2520and%2520Ke%2520Ma%2520and%2520Yingfei%2520Sun%2520and%2520Qianqian%2520Xu%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520The%2520diffusion-based%2520adversarial%2520purification%2520methods%2520attempt%2520to%2520drown%250Aadversarial%2520perturbations%2520into%2520a%2520part%2520of%2520isotropic%2520noise%2520through%2520the%2520forward%250Aprocess%252C%2520and%2520then%2520recover%2520the%2520clean%2520images%2520through%2520the%2520reverse%2520process.%2520Due%2520to%250Athe%2520lack%2520of%2520distribution%2520information%2520about%2520adversarial%2520perturbations%2520in%2520the%250Apixel%2520domain%252C%2520it%2520is%2520often%2520unavoidable%2520to%2520damage%2520normal%2520semantics.%2520We%2520turn%2520to%250Athe%2520frequency%2520domain%2520perspective%252C%2520decomposing%2520the%2520image%2520into%2520amplitude%2520spectrum%250Aand%2520phase%2520spectrum.%2520We%2520find%2520that%2520for%2520both%2520spectra%252C%2520the%2520damage%2520caused%2520by%250Aadversarial%2520perturbations%2520tends%2520to%2520increase%2520monotonically%2520with%2520frequency.%2520This%250Ameans%2520that%2520we%2520can%2520extract%2520the%2520content%2520and%2520structural%2520information%2520of%2520the%250Aoriginal%2520clean%2520sample%2520from%2520the%2520frequency%2520components%2520that%2520are%2520less%2520damaged.%250AMeanwhile%252C%2520theoretical%2520analysis%2520indicates%2520that%2520existing%2520purification%2520methods%250Aindiscriminately%2520damage%2520all%2520frequency%2520components%252C%2520leading%2520to%2520excessive%2520damage%250Ato%2520the%2520image.%2520Therefore%252C%2520we%2520propose%2520a%2520purification%2520method%2520that%2520can%2520eliminate%250Aadversarial%2520perturbations%2520while%2520maximizing%2520the%2520preservation%2520of%2520the%2520content%2520and%250Astructure%2520of%2520the%2520original%2520image.%2520Specifically%252C%2520at%2520each%2520time%2520step%2520during%2520the%250Areverse%2520process%252C%2520for%2520the%2520amplitude%2520spectrum%252C%2520we%2520replace%2520the%2520low-frequency%250Acomponents%2520of%2520the%2520estimated%2520image%2527s%2520amplitude%2520spectrum%2520with%2520the%2520corresponding%250Aparts%2520of%2520the%2520adversarial%2520image.%2520For%2520the%2520phase%2520spectrum%252C%2520we%2520project%2520the%2520phase%2520of%250Athe%2520estimated%2520image%2520into%2520a%2520designated%2520range%2520of%2520the%2520adversarial%2520image%2527s%2520phase%250Aspectrum%252C%2520focusing%2520on%2520the%2520low%2520frequencies.%2520Empirical%2520evidence%2520from%2520extensive%250Aexperiments%2520demonstrates%2520that%2520our%2520method%2520significantly%2520outperforms%2520most%2520current%250Adefense%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Adversarial%20Purification%20from%20the%20Perspective%20of%20the%0A%20%20Frequency%20Domain&entry.906535625=Gaozheng%20Pei%20and%20Ke%20Ma%20and%20Yingfei%20Sun%20and%20Qianqian%20Xu%20and%20Qingming%20Huang&entry.1292438233=%20%20The%20diffusion-based%20adversarial%20purification%20methods%20attempt%20to%20drown%0Aadversarial%20perturbations%20into%20a%20part%20of%20isotropic%20noise%20through%20the%20forward%0Aprocess%2C%20and%20then%20recover%20the%20clean%20images%20through%20the%20reverse%20process.%20Due%20to%0Athe%20lack%20of%20distribution%20information%20about%20adversarial%20perturbations%20in%20the%0Apixel%20domain%2C%20it%20is%20often%20unavoidable%20to%20damage%20normal%20semantics.%20We%20turn%20to%0Athe%20frequency%20domain%20perspective%2C%20decomposing%20the%20image%20into%20amplitude%20spectrum%0Aand%20phase%20spectrum.%20We%20find%20that%20for%20both%20spectra%2C%20the%20damage%20caused%20by%0Aadversarial%20perturbations%20tends%20to%20increase%20monotonically%20with%20frequency.%20This%0Ameans%20that%20we%20can%20extract%20the%20content%20and%20structural%20information%20of%20the%0Aoriginal%20clean%20sample%20from%20the%20frequency%20components%20that%20are%20less%20damaged.%0AMeanwhile%2C%20theoretical%20analysis%20indicates%20that%20existing%20purification%20methods%0Aindiscriminately%20damage%20all%20frequency%20components%2C%20leading%20to%20excessive%20damage%0Ato%20the%20image.%20Therefore%2C%20we%20propose%20a%20purification%20method%20that%20can%20eliminate%0Aadversarial%20perturbations%20while%20maximizing%20the%20preservation%20of%20the%20content%20and%0Astructure%20of%20the%20original%20image.%20Specifically%2C%20at%20each%20time%20step%20during%20the%0Areverse%20process%2C%20for%20the%20amplitude%20spectrum%2C%20we%20replace%20the%20low-frequency%0Acomponents%20of%20the%20estimated%20image%27s%20amplitude%20spectrum%20with%20the%20corresponding%0Aparts%20of%20the%20adversarial%20image.%20For%20the%20phase%20spectrum%2C%20we%20project%20the%20phase%20of%0Athe%20estimated%20image%20into%20a%20designated%20range%20of%20the%20adversarial%20image%27s%20phase%0Aspectrum%2C%20focusing%20on%20the%20low%20frequencies.%20Empirical%20evidence%20from%20extensive%0Aexperiments%20demonstrates%20that%20our%20method%20significantly%20outperforms%20most%20current%0Adefense%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01267v1&entry.124074799=Read"},
{"title": "Efficient Vision-based Vehicle Speed Estimation", "author": "Andrej Macko and Luk\u00e1\u0161 Gajdo\u0161ech and Viktor Kocur", "abstract": "  This paper presents a computationally efficient method for vehicle speed\nestimation from traffic camera footage. Building upon previous work that\nutilizes 3D bounding boxes derived from 2D detections and vanishing point\ngeometry, we introduce several improvements to enhance real-time performance.\nWe evaluate our method in several variants on the BrnoCompSpeed dataset in\nterms of vehicle detection and speed estimation accuracy. Our extensive\nevaluation across various hardware platforms, including edge devices,\ndemonstrates significant gains in frames per second (FPS) compared to the prior\nstate-of-the-art, while maintaining comparable or improved speed estimation\naccuracy. We analyze the trade-off between accuracy and computational cost,\nshowing that smaller models utilizing post-training quantization offer the best\nbalance for real-world deployment. Our best performing model beats previous\nstate-of-the-art in terms of median vehicle speed estimation error (0.58 km/h\nvs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.\n83.32%) while also being 5.5 times faster.\n", "link": "http://arxiv.org/abs/2505.01203v1", "date": "2025-05-02", "relevancy": 1.5879, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.534}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Vision-based%20Vehicle%20Speed%20Estimation&body=Title%3A%20Efficient%20Vision-based%20Vehicle%20Speed%20Estimation%0AAuthor%3A%20Andrej%20Macko%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Viktor%20Kocur%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20computationally%20efficient%20method%20for%20vehicle%20speed%0Aestimation%20from%20traffic%20camera%20footage.%20Building%20upon%20previous%20work%20that%0Autilizes%203D%20bounding%20boxes%20derived%20from%202D%20detections%20and%20vanishing%20point%0Ageometry%2C%20we%20introduce%20several%20improvements%20to%20enhance%20real-time%20performance.%0AWe%20evaluate%20our%20method%20in%20several%20variants%20on%20the%20BrnoCompSpeed%20dataset%20in%0Aterms%20of%20vehicle%20detection%20and%20speed%20estimation%20accuracy.%20Our%20extensive%0Aevaluation%20across%20various%20hardware%20platforms%2C%20including%20edge%20devices%2C%0Ademonstrates%20significant%20gains%20in%20frames%20per%20second%20%28FPS%29%20compared%20to%20the%20prior%0Astate-of-the-art%2C%20while%20maintaining%20comparable%20or%20improved%20speed%20estimation%0Aaccuracy.%20We%20analyze%20the%20trade-off%20between%20accuracy%20and%20computational%20cost%2C%0Ashowing%20that%20smaller%20models%20utilizing%20post-training%20quantization%20offer%20the%20best%0Abalance%20for%20real-world%20deployment.%20Our%20best%20performing%20model%20beats%20previous%0Astate-of-the-art%20in%20terms%20of%20median%20vehicle%20speed%20estimation%20error%20%280.58%20km/h%0Avs.%200.60%20km/h%29%2C%20detection%20precision%20%2891.02%25%20vs%2087.08%25%29%20and%20recall%20%2891.14%25%20vs.%0A83.32%25%29%20while%20also%20being%205.5%20times%20faster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Vision-based%2520Vehicle%2520Speed%2520Estimation%26entry.906535625%3DAndrej%2520Macko%2520and%2520Luk%25C3%25A1%25C5%25A1%2520Gajdo%25C5%25A1ech%2520and%2520Viktor%2520Kocur%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520computationally%2520efficient%2520method%2520for%2520vehicle%2520speed%250Aestimation%2520from%2520traffic%2520camera%2520footage.%2520Building%2520upon%2520previous%2520work%2520that%250Autilizes%25203D%2520bounding%2520boxes%2520derived%2520from%25202D%2520detections%2520and%2520vanishing%2520point%250Ageometry%252C%2520we%2520introduce%2520several%2520improvements%2520to%2520enhance%2520real-time%2520performance.%250AWe%2520evaluate%2520our%2520method%2520in%2520several%2520variants%2520on%2520the%2520BrnoCompSpeed%2520dataset%2520in%250Aterms%2520of%2520vehicle%2520detection%2520and%2520speed%2520estimation%2520accuracy.%2520Our%2520extensive%250Aevaluation%2520across%2520various%2520hardware%2520platforms%252C%2520including%2520edge%2520devices%252C%250Ademonstrates%2520significant%2520gains%2520in%2520frames%2520per%2520second%2520%2528FPS%2529%2520compared%2520to%2520the%2520prior%250Astate-of-the-art%252C%2520while%2520maintaining%2520comparable%2520or%2520improved%2520speed%2520estimation%250Aaccuracy.%2520We%2520analyze%2520the%2520trade-off%2520between%2520accuracy%2520and%2520computational%2520cost%252C%250Ashowing%2520that%2520smaller%2520models%2520utilizing%2520post-training%2520quantization%2520offer%2520the%2520best%250Abalance%2520for%2520real-world%2520deployment.%2520Our%2520best%2520performing%2520model%2520beats%2520previous%250Astate-of-the-art%2520in%2520terms%2520of%2520median%2520vehicle%2520speed%2520estimation%2520error%2520%25280.58%2520km/h%250Avs.%25200.60%2520km/h%2529%252C%2520detection%2520precision%2520%252891.02%2525%2520vs%252087.08%2525%2529%2520and%2520recall%2520%252891.14%2525%2520vs.%250A83.32%2525%2529%2520while%2520also%2520being%25205.5%2520times%2520faster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Vision-based%20Vehicle%20Speed%20Estimation&entry.906535625=Andrej%20Macko%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Viktor%20Kocur&entry.1292438233=%20%20This%20paper%20presents%20a%20computationally%20efficient%20method%20for%20vehicle%20speed%0Aestimation%20from%20traffic%20camera%20footage.%20Building%20upon%20previous%20work%20that%0Autilizes%203D%20bounding%20boxes%20derived%20from%202D%20detections%20and%20vanishing%20point%0Ageometry%2C%20we%20introduce%20several%20improvements%20to%20enhance%20real-time%20performance.%0AWe%20evaluate%20our%20method%20in%20several%20variants%20on%20the%20BrnoCompSpeed%20dataset%20in%0Aterms%20of%20vehicle%20detection%20and%20speed%20estimation%20accuracy.%20Our%20extensive%0Aevaluation%20across%20various%20hardware%20platforms%2C%20including%20edge%20devices%2C%0Ademonstrates%20significant%20gains%20in%20frames%20per%20second%20%28FPS%29%20compared%20to%20the%20prior%0Astate-of-the-art%2C%20while%20maintaining%20comparable%20or%20improved%20speed%20estimation%0Aaccuracy.%20We%20analyze%20the%20trade-off%20between%20accuracy%20and%20computational%20cost%2C%0Ashowing%20that%20smaller%20models%20utilizing%20post-training%20quantization%20offer%20the%20best%0Abalance%20for%20real-world%20deployment.%20Our%20best%20performing%20model%20beats%20previous%0Astate-of-the-art%20in%20terms%20of%20median%20vehicle%20speed%20estimation%20error%20%280.58%20km/h%0Avs.%200.60%20km/h%29%2C%20detection%20precision%20%2891.02%25%20vs%2087.08%25%29%20and%20recall%20%2891.14%25%20vs.%0A83.32%25%29%20while%20also%20being%205.5%20times%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01203v1&entry.124074799=Read"},
{"title": "Carbon Aware Transformers Through Joint Model-Hardware Optimization", "author": "Irene Wang and Newsha Ardalani and Mostafa Elhoushi and Daniel Jiang and Samuel Hsia and Ekin Sumbul and Divya Mahajan and Carole-Jean Wu and Bilge Acun", "abstract": "  The rapid growth of machine learning (ML) systems necessitates a more\ncomprehensive evaluation of their environmental impact, particularly their\ncarbon footprint, which comprises operational carbon from training and\ninference execution and embodied carbon from hardware manufacturing and its\nentire life-cycle. Despite the increasing importance of embodied emissions,\nthere is a lack of tools and frameworks to holistically quantify and optimize\nthe total carbon footprint of ML systems. To address this, we propose\nCATransformers, a carbon-aware architecture search framework that enables\nsustainability-driven co-optimization of ML models and hardware architectures.\nBy incorporating both operational and embodied carbon metrics into early design\nspace exploration of domain-specific hardware accelerators, CATransformers\ndemonstrates that optimizing for carbon yields design choices distinct from\nthose optimized solely for latency or energy efficiency. We apply our framework\nto multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models\nachieving up to 17% reduction in total carbon emissions while maintaining\naccuracy and latency compared to state-of-the-art edge small CLIP baselines.\nThis work underscores the need for holistic optimization methods to design\nhigh-performance, environmentally sustainable AI systems.\n", "link": "http://arxiv.org/abs/2505.01386v1", "date": "2025-05-02", "relevancy": 1.5817, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5518}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5272}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Carbon%20Aware%20Transformers%20Through%20Joint%20Model-Hardware%20Optimization&body=Title%3A%20Carbon%20Aware%20Transformers%20Through%20Joint%20Model-Hardware%20Optimization%0AAuthor%3A%20Irene%20Wang%20and%20Newsha%20Ardalani%20and%20Mostafa%20Elhoushi%20and%20Daniel%20Jiang%20and%20Samuel%20Hsia%20and%20Ekin%20Sumbul%20and%20Divya%20Mahajan%20and%20Carole-Jean%20Wu%20and%20Bilge%20Acun%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20machine%20learning%20%28ML%29%20systems%20necessitates%20a%20more%0Acomprehensive%20evaluation%20of%20their%20environmental%20impact%2C%20particularly%20their%0Acarbon%20footprint%2C%20which%20comprises%20operational%20carbon%20from%20training%20and%0Ainference%20execution%20and%20embodied%20carbon%20from%20hardware%20manufacturing%20and%20its%0Aentire%20life-cycle.%20Despite%20the%20increasing%20importance%20of%20embodied%20emissions%2C%0Athere%20is%20a%20lack%20of%20tools%20and%20frameworks%20to%20holistically%20quantify%20and%20optimize%0Athe%20total%20carbon%20footprint%20of%20ML%20systems.%20To%20address%20this%2C%20we%20propose%0ACATransformers%2C%20a%20carbon-aware%20architecture%20search%20framework%20that%20enables%0Asustainability-driven%20co-optimization%20of%20ML%20models%20and%20hardware%20architectures.%0ABy%20incorporating%20both%20operational%20and%20embodied%20carbon%20metrics%20into%20early%20design%0Aspace%20exploration%20of%20domain-specific%20hardware%20accelerators%2C%20CATransformers%0Ademonstrates%20that%20optimizing%20for%20carbon%20yields%20design%20choices%20distinct%20from%0Athose%20optimized%20solely%20for%20latency%20or%20energy%20efficiency.%20We%20apply%20our%20framework%0Ato%20multi-modal%20CLIP-based%20models%2C%20producing%20CarbonCLIP%2C%20a%20family%20of%20CLIP%20models%0Aachieving%20up%20to%2017%25%20reduction%20in%20total%20carbon%20emissions%20while%20maintaining%0Aaccuracy%20and%20latency%20compared%20to%20state-of-the-art%20edge%20small%20CLIP%20baselines.%0AThis%20work%20underscores%20the%20need%20for%20holistic%20optimization%20methods%20to%20design%0Ahigh-performance%2C%20environmentally%20sustainable%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCarbon%2520Aware%2520Transformers%2520Through%2520Joint%2520Model-Hardware%2520Optimization%26entry.906535625%3DIrene%2520Wang%2520and%2520Newsha%2520Ardalani%2520and%2520Mostafa%2520Elhoushi%2520and%2520Daniel%2520Jiang%2520and%2520Samuel%2520Hsia%2520and%2520Ekin%2520Sumbul%2520and%2520Divya%2520Mahajan%2520and%2520Carole-Jean%2520Wu%2520and%2520Bilge%2520Acun%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520machine%2520learning%2520%2528ML%2529%2520systems%2520necessitates%2520a%2520more%250Acomprehensive%2520evaluation%2520of%2520their%2520environmental%2520impact%252C%2520particularly%2520their%250Acarbon%2520footprint%252C%2520which%2520comprises%2520operational%2520carbon%2520from%2520training%2520and%250Ainference%2520execution%2520and%2520embodied%2520carbon%2520from%2520hardware%2520manufacturing%2520and%2520its%250Aentire%2520life-cycle.%2520Despite%2520the%2520increasing%2520importance%2520of%2520embodied%2520emissions%252C%250Athere%2520is%2520a%2520lack%2520of%2520tools%2520and%2520frameworks%2520to%2520holistically%2520quantify%2520and%2520optimize%250Athe%2520total%2520carbon%2520footprint%2520of%2520ML%2520systems.%2520To%2520address%2520this%252C%2520we%2520propose%250ACATransformers%252C%2520a%2520carbon-aware%2520architecture%2520search%2520framework%2520that%2520enables%250Asustainability-driven%2520co-optimization%2520of%2520ML%2520models%2520and%2520hardware%2520architectures.%250ABy%2520incorporating%2520both%2520operational%2520and%2520embodied%2520carbon%2520metrics%2520into%2520early%2520design%250Aspace%2520exploration%2520of%2520domain-specific%2520hardware%2520accelerators%252C%2520CATransformers%250Ademonstrates%2520that%2520optimizing%2520for%2520carbon%2520yields%2520design%2520choices%2520distinct%2520from%250Athose%2520optimized%2520solely%2520for%2520latency%2520or%2520energy%2520efficiency.%2520We%2520apply%2520our%2520framework%250Ato%2520multi-modal%2520CLIP-based%2520models%252C%2520producing%2520CarbonCLIP%252C%2520a%2520family%2520of%2520CLIP%2520models%250Aachieving%2520up%2520to%252017%2525%2520reduction%2520in%2520total%2520carbon%2520emissions%2520while%2520maintaining%250Aaccuracy%2520and%2520latency%2520compared%2520to%2520state-of-the-art%2520edge%2520small%2520CLIP%2520baselines.%250AThis%2520work%2520underscores%2520the%2520need%2520for%2520holistic%2520optimization%2520methods%2520to%2520design%250Ahigh-performance%252C%2520environmentally%2520sustainable%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Carbon%20Aware%20Transformers%20Through%20Joint%20Model-Hardware%20Optimization&entry.906535625=Irene%20Wang%20and%20Newsha%20Ardalani%20and%20Mostafa%20Elhoushi%20and%20Daniel%20Jiang%20and%20Samuel%20Hsia%20and%20Ekin%20Sumbul%20and%20Divya%20Mahajan%20and%20Carole-Jean%20Wu%20and%20Bilge%20Acun&entry.1292438233=%20%20The%20rapid%20growth%20of%20machine%20learning%20%28ML%29%20systems%20necessitates%20a%20more%0Acomprehensive%20evaluation%20of%20their%20environmental%20impact%2C%20particularly%20their%0Acarbon%20footprint%2C%20which%20comprises%20operational%20carbon%20from%20training%20and%0Ainference%20execution%20and%20embodied%20carbon%20from%20hardware%20manufacturing%20and%20its%0Aentire%20life-cycle.%20Despite%20the%20increasing%20importance%20of%20embodied%20emissions%2C%0Athere%20is%20a%20lack%20of%20tools%20and%20frameworks%20to%20holistically%20quantify%20and%20optimize%0Athe%20total%20carbon%20footprint%20of%20ML%20systems.%20To%20address%20this%2C%20we%20propose%0ACATransformers%2C%20a%20carbon-aware%20architecture%20search%20framework%20that%20enables%0Asustainability-driven%20co-optimization%20of%20ML%20models%20and%20hardware%20architectures.%0ABy%20incorporating%20both%20operational%20and%20embodied%20carbon%20metrics%20into%20early%20design%0Aspace%20exploration%20of%20domain-specific%20hardware%20accelerators%2C%20CATransformers%0Ademonstrates%20that%20optimizing%20for%20carbon%20yields%20design%20choices%20distinct%20from%0Athose%20optimized%20solely%20for%20latency%20or%20energy%20efficiency.%20We%20apply%20our%20framework%0Ato%20multi-modal%20CLIP-based%20models%2C%20producing%20CarbonCLIP%2C%20a%20family%20of%20CLIP%20models%0Aachieving%20up%20to%2017%25%20reduction%20in%20total%20carbon%20emissions%20while%20maintaining%0Aaccuracy%20and%20latency%20compared%20to%20state-of-the-art%20edge%20small%20CLIP%20baselines.%0AThis%20work%20underscores%20the%20need%20for%20holistic%20optimization%20methods%20to%20design%0Ahigh-performance%2C%20environmentally%20sustainable%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01386v1&entry.124074799=Read"},
{"title": "A flexible Bayesian non-parametric mixture model reveals multiple\n  dependencies of swap errors in visual working memory", "author": "Puria Radmard and Paul M. Bays and M\u00e1t\u00e9 Lengyel", "abstract": "  Human behavioural data in psychophysics has been used to elucidate the\nunderlying mechanisms of many cognitive processes, such as attention,\nsensorimotor integration, and perceptual decision making. Visual working memory\nhas particularly benefited from this approach: analyses of VWM errors have\nproven crucial for understanding VWM capacity and coding schemes, in turn\nconstraining neural models of both. One poorly understood class of VWM errors\nare swap errors, whereby participants recall an uncued item from memory. Swap\nerrors could arise from erroneous memory encoding, noisy storage, or errors at\nretrieval time - previous research has mostly implicated the latter two.\nHowever, these studies made strong a priori assumptions on the detailed\nmechanisms and/or parametric form of errors contributed by these sources. Here,\nwe pursue a data-driven approach instead, introducing a Bayesian non-parametric\nmixture model of swap errors (BNS) which provides a flexible descriptive model\nof swapping behaviour, such that swaps are allowed to depend on both the probed\nand reported features of every stimulus item. We fit BNS to the trial-by-trial\nbehaviour of human participants and show that it recapitulates the strong\ndependence of swaps on cue similarity in multiple datasets. Critically, BNS\nreveals that this dependence coexists with a non-monotonic modulation in the\nreport feature dimension for a random dot motion direction-cued,\nlocation-reported dataset. The form of the modulation inferred by BNS opens new\nquestions about the importance of memory encoding in causing swap errors in\nVWM, a distinct source to the previously suggested binding and cueing errors.\nOur analyses, combining qualitative comparisons of the highly interpretable BNS\nparameter structure with rigorous quantitative model comparison and recovery\nmethods, show that previous interpretations of swap errors may have been\nincomplete.\n", "link": "http://arxiv.org/abs/2505.01178v1", "date": "2025-05-02", "relevancy": 1.5585, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5655}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5159}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20flexible%20Bayesian%20non-parametric%20mixture%20model%20reveals%20multiple%0A%20%20dependencies%20of%20swap%20errors%20in%20visual%20working%20memory&body=Title%3A%20A%20flexible%20Bayesian%20non-parametric%20mixture%20model%20reveals%20multiple%0A%20%20dependencies%20of%20swap%20errors%20in%20visual%20working%20memory%0AAuthor%3A%20Puria%20Radmard%20and%20Paul%20M.%20Bays%20and%20M%C3%A1t%C3%A9%20Lengyel%0AAbstract%3A%20%20%20Human%20behavioural%20data%20in%20psychophysics%20has%20been%20used%20to%20elucidate%20the%0Aunderlying%20mechanisms%20of%20many%20cognitive%20processes%2C%20such%20as%20attention%2C%0Asensorimotor%20integration%2C%20and%20perceptual%20decision%20making.%20Visual%20working%20memory%0Ahas%20particularly%20benefited%20from%20this%20approach%3A%20analyses%20of%20VWM%20errors%20have%0Aproven%20crucial%20for%20understanding%20VWM%20capacity%20and%20coding%20schemes%2C%20in%20turn%0Aconstraining%20neural%20models%20of%20both.%20One%20poorly%20understood%20class%20of%20VWM%20errors%0Aare%20swap%20errors%2C%20whereby%20participants%20recall%20an%20uncued%20item%20from%20memory.%20Swap%0Aerrors%20could%20arise%20from%20erroneous%20memory%20encoding%2C%20noisy%20storage%2C%20or%20errors%20at%0Aretrieval%20time%20-%20previous%20research%20has%20mostly%20implicated%20the%20latter%20two.%0AHowever%2C%20these%20studies%20made%20strong%20a%20priori%20assumptions%20on%20the%20detailed%0Amechanisms%20and/or%20parametric%20form%20of%20errors%20contributed%20by%20these%20sources.%20Here%2C%0Awe%20pursue%20a%20data-driven%20approach%20instead%2C%20introducing%20a%20Bayesian%20non-parametric%0Amixture%20model%20of%20swap%20errors%20%28BNS%29%20which%20provides%20a%20flexible%20descriptive%20model%0Aof%20swapping%20behaviour%2C%20such%20that%20swaps%20are%20allowed%20to%20depend%20on%20both%20the%20probed%0Aand%20reported%20features%20of%20every%20stimulus%20item.%20We%20fit%20BNS%20to%20the%20trial-by-trial%0Abehaviour%20of%20human%20participants%20and%20show%20that%20it%20recapitulates%20the%20strong%0Adependence%20of%20swaps%20on%20cue%20similarity%20in%20multiple%20datasets.%20Critically%2C%20BNS%0Areveals%20that%20this%20dependence%20coexists%20with%20a%20non-monotonic%20modulation%20in%20the%0Areport%20feature%20dimension%20for%20a%20random%20dot%20motion%20direction-cued%2C%0Alocation-reported%20dataset.%20The%20form%20of%20the%20modulation%20inferred%20by%20BNS%20opens%20new%0Aquestions%20about%20the%20importance%20of%20memory%20encoding%20in%20causing%20swap%20errors%20in%0AVWM%2C%20a%20distinct%20source%20to%20the%20previously%20suggested%20binding%20and%20cueing%20errors.%0AOur%20analyses%2C%20combining%20qualitative%20comparisons%20of%20the%20highly%20interpretable%20BNS%0Aparameter%20structure%20with%20rigorous%20quantitative%20model%20comparison%20and%20recovery%0Amethods%2C%20show%20that%20previous%20interpretations%20of%20swap%20errors%20may%20have%20been%0Aincomplete.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520flexible%2520Bayesian%2520non-parametric%2520mixture%2520model%2520reveals%2520multiple%250A%2520%2520dependencies%2520of%2520swap%2520errors%2520in%2520visual%2520working%2520memory%26entry.906535625%3DPuria%2520Radmard%2520and%2520Paul%2520M.%2520Bays%2520and%2520M%25C3%25A1t%25C3%25A9%2520Lengyel%26entry.1292438233%3D%2520%2520Human%2520behavioural%2520data%2520in%2520psychophysics%2520has%2520been%2520used%2520to%2520elucidate%2520the%250Aunderlying%2520mechanisms%2520of%2520many%2520cognitive%2520processes%252C%2520such%2520as%2520attention%252C%250Asensorimotor%2520integration%252C%2520and%2520perceptual%2520decision%2520making.%2520Visual%2520working%2520memory%250Ahas%2520particularly%2520benefited%2520from%2520this%2520approach%253A%2520analyses%2520of%2520VWM%2520errors%2520have%250Aproven%2520crucial%2520for%2520understanding%2520VWM%2520capacity%2520and%2520coding%2520schemes%252C%2520in%2520turn%250Aconstraining%2520neural%2520models%2520of%2520both.%2520One%2520poorly%2520understood%2520class%2520of%2520VWM%2520errors%250Aare%2520swap%2520errors%252C%2520whereby%2520participants%2520recall%2520an%2520uncued%2520item%2520from%2520memory.%2520Swap%250Aerrors%2520could%2520arise%2520from%2520erroneous%2520memory%2520encoding%252C%2520noisy%2520storage%252C%2520or%2520errors%2520at%250Aretrieval%2520time%2520-%2520previous%2520research%2520has%2520mostly%2520implicated%2520the%2520latter%2520two.%250AHowever%252C%2520these%2520studies%2520made%2520strong%2520a%2520priori%2520assumptions%2520on%2520the%2520detailed%250Amechanisms%2520and/or%2520parametric%2520form%2520of%2520errors%2520contributed%2520by%2520these%2520sources.%2520Here%252C%250Awe%2520pursue%2520a%2520data-driven%2520approach%2520instead%252C%2520introducing%2520a%2520Bayesian%2520non-parametric%250Amixture%2520model%2520of%2520swap%2520errors%2520%2528BNS%2529%2520which%2520provides%2520a%2520flexible%2520descriptive%2520model%250Aof%2520swapping%2520behaviour%252C%2520such%2520that%2520swaps%2520are%2520allowed%2520to%2520depend%2520on%2520both%2520the%2520probed%250Aand%2520reported%2520features%2520of%2520every%2520stimulus%2520item.%2520We%2520fit%2520BNS%2520to%2520the%2520trial-by-trial%250Abehaviour%2520of%2520human%2520participants%2520and%2520show%2520that%2520it%2520recapitulates%2520the%2520strong%250Adependence%2520of%2520swaps%2520on%2520cue%2520similarity%2520in%2520multiple%2520datasets.%2520Critically%252C%2520BNS%250Areveals%2520that%2520this%2520dependence%2520coexists%2520with%2520a%2520non-monotonic%2520modulation%2520in%2520the%250Areport%2520feature%2520dimension%2520for%2520a%2520random%2520dot%2520motion%2520direction-cued%252C%250Alocation-reported%2520dataset.%2520The%2520form%2520of%2520the%2520modulation%2520inferred%2520by%2520BNS%2520opens%2520new%250Aquestions%2520about%2520the%2520importance%2520of%2520memory%2520encoding%2520in%2520causing%2520swap%2520errors%2520in%250AVWM%252C%2520a%2520distinct%2520source%2520to%2520the%2520previously%2520suggested%2520binding%2520and%2520cueing%2520errors.%250AOur%2520analyses%252C%2520combining%2520qualitative%2520comparisons%2520of%2520the%2520highly%2520interpretable%2520BNS%250Aparameter%2520structure%2520with%2520rigorous%2520quantitative%2520model%2520comparison%2520and%2520recovery%250Amethods%252C%2520show%2520that%2520previous%2520interpretations%2520of%2520swap%2520errors%2520may%2520have%2520been%250Aincomplete.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20flexible%20Bayesian%20non-parametric%20mixture%20model%20reveals%20multiple%0A%20%20dependencies%20of%20swap%20errors%20in%20visual%20working%20memory&entry.906535625=Puria%20Radmard%20and%20Paul%20M.%20Bays%20and%20M%C3%A1t%C3%A9%20Lengyel&entry.1292438233=%20%20Human%20behavioural%20data%20in%20psychophysics%20has%20been%20used%20to%20elucidate%20the%0Aunderlying%20mechanisms%20of%20many%20cognitive%20processes%2C%20such%20as%20attention%2C%0Asensorimotor%20integration%2C%20and%20perceptual%20decision%20making.%20Visual%20working%20memory%0Ahas%20particularly%20benefited%20from%20this%20approach%3A%20analyses%20of%20VWM%20errors%20have%0Aproven%20crucial%20for%20understanding%20VWM%20capacity%20and%20coding%20schemes%2C%20in%20turn%0Aconstraining%20neural%20models%20of%20both.%20One%20poorly%20understood%20class%20of%20VWM%20errors%0Aare%20swap%20errors%2C%20whereby%20participants%20recall%20an%20uncued%20item%20from%20memory.%20Swap%0Aerrors%20could%20arise%20from%20erroneous%20memory%20encoding%2C%20noisy%20storage%2C%20or%20errors%20at%0Aretrieval%20time%20-%20previous%20research%20has%20mostly%20implicated%20the%20latter%20two.%0AHowever%2C%20these%20studies%20made%20strong%20a%20priori%20assumptions%20on%20the%20detailed%0Amechanisms%20and/or%20parametric%20form%20of%20errors%20contributed%20by%20these%20sources.%20Here%2C%0Awe%20pursue%20a%20data-driven%20approach%20instead%2C%20introducing%20a%20Bayesian%20non-parametric%0Amixture%20model%20of%20swap%20errors%20%28BNS%29%20which%20provides%20a%20flexible%20descriptive%20model%0Aof%20swapping%20behaviour%2C%20such%20that%20swaps%20are%20allowed%20to%20depend%20on%20both%20the%20probed%0Aand%20reported%20features%20of%20every%20stimulus%20item.%20We%20fit%20BNS%20to%20the%20trial-by-trial%0Abehaviour%20of%20human%20participants%20and%20show%20that%20it%20recapitulates%20the%20strong%0Adependence%20of%20swaps%20on%20cue%20similarity%20in%20multiple%20datasets.%20Critically%2C%20BNS%0Areveals%20that%20this%20dependence%20coexists%20with%20a%20non-monotonic%20modulation%20in%20the%0Areport%20feature%20dimension%20for%20a%20random%20dot%20motion%20direction-cued%2C%0Alocation-reported%20dataset.%20The%20form%20of%20the%20modulation%20inferred%20by%20BNS%20opens%20new%0Aquestions%20about%20the%20importance%20of%20memory%20encoding%20in%20causing%20swap%20errors%20in%0AVWM%2C%20a%20distinct%20source%20to%20the%20previously%20suggested%20binding%20and%20cueing%20errors.%0AOur%20analyses%2C%20combining%20qualitative%20comparisons%20of%20the%20highly%20interpretable%20BNS%0Aparameter%20structure%20with%20rigorous%20quantitative%20model%20comparison%20and%20recovery%0Amethods%2C%20show%20that%20previous%20interpretations%20of%20swap%20errors%20may%20have%20been%0Aincomplete.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01178v1&entry.124074799=Read"},
{"title": "MoDeGPT: Modular Decomposition for Large Language Model Compression", "author": "Chi-Heng Lin and Shangqian Gao and James Seale Smith and Abhishek Patel and Shikhar Tuli and Yilin Shen and Hongxia Jin and Yen-Chang Hsu", "abstract": "  Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.\n", "link": "http://arxiv.org/abs/2408.09632v5", "date": "2025-05-02", "relevancy": 1.5535, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5145}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoDeGPT%3A%20Modular%20Decomposition%20for%20Large%20Language%20Model%20Compression&body=Title%3A%20MoDeGPT%3A%20Modular%20Decomposition%20for%20Large%20Language%20Model%20Compression%0AAuthor%3A%20Chi-Heng%20Lin%20and%20Shangqian%20Gao%20and%20James%20Seale%20Smith%20and%20Abhishek%20Patel%20and%20Shikhar%20Tuli%20and%20Yilin%20Shen%20and%20Hongxia%20Jin%20and%20Yen-Chang%20Hsu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20reshaped%20the%20landscape%20of%20artificial%0Aintelligence%20by%20demonstrating%20exceptional%20performance%20across%20various%20tasks.%0AHowever%2C%20substantial%20computational%20requirements%20make%20their%20deployment%0Achallenging%20on%20devices%20with%20limited%20resources.%20Recently%2C%20compression%20methods%0Ausing%20low-rank%20matrix%20techniques%20have%20shown%20promise%2C%20yet%20these%20often%20lead%20to%0Adegraded%20accuracy%20or%20introduce%20significant%20overhead%20in%20parameters%20and%20inference%0Alatency.%20This%20paper%20introduces%20%5Ctextbf%7BMo%7Ddular%20%5Ctextbf%7BDe%7Dcomposition%0A%28MoDeGPT%29%2C%20a%20novel%20structured%20compression%20framework%20that%20does%20not%20need%20recovery%0Afine-tuning%20while%20resolving%20the%20above%20drawbacks.%20MoDeGPT%20partitions%20the%0ATransformer%20block%20into%20modules%20comprised%20of%20matrix%20pairs%20and%20reduces%20the%20hidden%0Adimensions%20via%20reconstructing%20the%20module-level%20outputs.%20MoDeGPT%20is%20developed%0Abased%20on%20a%20theoretical%20framework%20that%20utilizes%20three%20well-established%20matrix%0Adecomposition%20algorithms%20--%20Nystr%5C%22om%20approximation%2C%20CR%20decomposition%2C%20and%20SVD%0A--%20and%20applies%20them%20to%20our%20redefined%20transformer%20modules.%20Our%20comprehensive%0Aexperiments%20show%20MoDeGPT%2C%20without%20backward%20propagation%2C%20matches%20or%20surpasses%0Aprevious%20structured%20compression%20methods%20that%20rely%20on%20gradient%20information%2C%20and%0Asaves%2098%25%20of%20compute%20costs%20on%20compressing%20a%2013B%20model.%20On%20%5Ctextsc%7BLlama%7D-2/3%0Aand%20OPT%20models%2C%20MoDeGPT%20maintains%2090-95%25%20zero-shot%20performance%20with%2025-30%25%0Acompression%20rates.%20Moreover%2C%20the%20compression%20can%20be%20done%20on%20a%20single%20GPU%20within%0Aa%20few%20hours%20and%20increases%20the%20inference%20throughput%20by%20up%20to%2046%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09632v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoDeGPT%253A%2520Modular%2520Decomposition%2520for%2520Large%2520Language%2520Model%2520Compression%26entry.906535625%3DChi-Heng%2520Lin%2520and%2520Shangqian%2520Gao%2520and%2520James%2520Seale%2520Smith%2520and%2520Abhishek%2520Patel%2520and%2520Shikhar%2520Tuli%2520and%2520Yilin%2520Shen%2520and%2520Hongxia%2520Jin%2520and%2520Yen-Chang%2520Hsu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520reshaped%2520the%2520landscape%2520of%2520artificial%250Aintelligence%2520by%2520demonstrating%2520exceptional%2520performance%2520across%2520various%2520tasks.%250AHowever%252C%2520substantial%2520computational%2520requirements%2520make%2520their%2520deployment%250Achallenging%2520on%2520devices%2520with%2520limited%2520resources.%2520Recently%252C%2520compression%2520methods%250Ausing%2520low-rank%2520matrix%2520techniques%2520have%2520shown%2520promise%252C%2520yet%2520these%2520often%2520lead%2520to%250Adegraded%2520accuracy%2520or%2520introduce%2520significant%2520overhead%2520in%2520parameters%2520and%2520inference%250Alatency.%2520This%2520paper%2520introduces%2520%255Ctextbf%257BMo%257Ddular%2520%255Ctextbf%257BDe%257Dcomposition%250A%2528MoDeGPT%2529%252C%2520a%2520novel%2520structured%2520compression%2520framework%2520that%2520does%2520not%2520need%2520recovery%250Afine-tuning%2520while%2520resolving%2520the%2520above%2520drawbacks.%2520MoDeGPT%2520partitions%2520the%250ATransformer%2520block%2520into%2520modules%2520comprised%2520of%2520matrix%2520pairs%2520and%2520reduces%2520the%2520hidden%250Adimensions%2520via%2520reconstructing%2520the%2520module-level%2520outputs.%2520MoDeGPT%2520is%2520developed%250Abased%2520on%2520a%2520theoretical%2520framework%2520that%2520utilizes%2520three%2520well-established%2520matrix%250Adecomposition%2520algorithms%2520--%2520Nystr%255C%2522om%2520approximation%252C%2520CR%2520decomposition%252C%2520and%2520SVD%250A--%2520and%2520applies%2520them%2520to%2520our%2520redefined%2520transformer%2520modules.%2520Our%2520comprehensive%250Aexperiments%2520show%2520MoDeGPT%252C%2520without%2520backward%2520propagation%252C%2520matches%2520or%2520surpasses%250Aprevious%2520structured%2520compression%2520methods%2520that%2520rely%2520on%2520gradient%2520information%252C%2520and%250Asaves%252098%2525%2520of%2520compute%2520costs%2520on%2520compressing%2520a%252013B%2520model.%2520On%2520%255Ctextsc%257BLlama%257D-2/3%250Aand%2520OPT%2520models%252C%2520MoDeGPT%2520maintains%252090-95%2525%2520zero-shot%2520performance%2520with%252025-30%2525%250Acompression%2520rates.%2520Moreover%252C%2520the%2520compression%2520can%2520be%2520done%2520on%2520a%2520single%2520GPU%2520within%250Aa%2520few%2520hours%2520and%2520increases%2520the%2520inference%2520throughput%2520by%2520up%2520to%252046%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09632v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoDeGPT%3A%20Modular%20Decomposition%20for%20Large%20Language%20Model%20Compression&entry.906535625=Chi-Heng%20Lin%20and%20Shangqian%20Gao%20and%20James%20Seale%20Smith%20and%20Abhishek%20Patel%20and%20Shikhar%20Tuli%20and%20Yilin%20Shen%20and%20Hongxia%20Jin%20and%20Yen-Chang%20Hsu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20reshaped%20the%20landscape%20of%20artificial%0Aintelligence%20by%20demonstrating%20exceptional%20performance%20across%20various%20tasks.%0AHowever%2C%20substantial%20computational%20requirements%20make%20their%20deployment%0Achallenging%20on%20devices%20with%20limited%20resources.%20Recently%2C%20compression%20methods%0Ausing%20low-rank%20matrix%20techniques%20have%20shown%20promise%2C%20yet%20these%20often%20lead%20to%0Adegraded%20accuracy%20or%20introduce%20significant%20overhead%20in%20parameters%20and%20inference%0Alatency.%20This%20paper%20introduces%20%5Ctextbf%7BMo%7Ddular%20%5Ctextbf%7BDe%7Dcomposition%0A%28MoDeGPT%29%2C%20a%20novel%20structured%20compression%20framework%20that%20does%20not%20need%20recovery%0Afine-tuning%20while%20resolving%20the%20above%20drawbacks.%20MoDeGPT%20partitions%20the%0ATransformer%20block%20into%20modules%20comprised%20of%20matrix%20pairs%20and%20reduces%20the%20hidden%0Adimensions%20via%20reconstructing%20the%20module-level%20outputs.%20MoDeGPT%20is%20developed%0Abased%20on%20a%20theoretical%20framework%20that%20utilizes%20three%20well-established%20matrix%0Adecomposition%20algorithms%20--%20Nystr%5C%22om%20approximation%2C%20CR%20decomposition%2C%20and%20SVD%0A--%20and%20applies%20them%20to%20our%20redefined%20transformer%20modules.%20Our%20comprehensive%0Aexperiments%20show%20MoDeGPT%2C%20without%20backward%20propagation%2C%20matches%20or%20surpasses%0Aprevious%20structured%20compression%20methods%20that%20rely%20on%20gradient%20information%2C%20and%0Asaves%2098%25%20of%20compute%20costs%20on%20compressing%20a%2013B%20model.%20On%20%5Ctextsc%7BLlama%7D-2/3%0Aand%20OPT%20models%2C%20MoDeGPT%20maintains%2090-95%25%20zero-shot%20performance%20with%2025-30%25%0Acompression%20rates.%20Moreover%2C%20the%20compression%20can%20be%20done%20on%20a%20single%20GPU%20within%0Aa%20few%20hours%20and%20increases%20the%20inference%20throughput%20by%20up%20to%2046%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09632v5&entry.124074799=Read"},
{"title": "Learning and Transferring Physical Models through Derivatives", "author": "Alessandro Trenta and Andrea Cossu and Davide Bacciu", "abstract": "  We propose Derivative Learning (DERL), a supervised approach that models\nphysical systems by learning their partial derivatives. We also leverage DERL\nto build physical models incrementally, by designing a distillation protocol\nthat effectively transfers knowledge from a pre-trained to a student model. We\nprovide theoretical guarantees that our approach can learn the true physical\nsystem, being consistent with the underlying physical laws, even when using\nempirical derivatives. DERL outperforms state-of-the-art methods in\ngeneralizing an ODE to unseen initial conditions and a parametric PDE to unseen\nparameters. We finally propose a method based on DERL to transfer physical\nknowledge across models by extending them to new portions of the physical\ndomain and new range of PDE parameters. We believe this is the first attempt at\nbuilding physical models incrementally in multiple stages.\n", "link": "http://arxiv.org/abs/2505.01391v1", "date": "2025-05-02", "relevancy": 1.5486, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5164}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20and%20Transferring%20Physical%20Models%20through%20Derivatives&body=Title%3A%20Learning%20and%20Transferring%20Physical%20Models%20through%20Derivatives%0AAuthor%3A%20Alessandro%20Trenta%20and%20Andrea%20Cossu%20and%20Davide%20Bacciu%0AAbstract%3A%20%20%20We%20propose%20Derivative%20Learning%20%28DERL%29%2C%20a%20supervised%20approach%20that%20models%0Aphysical%20systems%20by%20learning%20their%20partial%20derivatives.%20We%20also%20leverage%20DERL%0Ato%20build%20physical%20models%20incrementally%2C%20by%20designing%20a%20distillation%20protocol%0Athat%20effectively%20transfers%20knowledge%20from%20a%20pre-trained%20to%20a%20student%20model.%20We%0Aprovide%20theoretical%20guarantees%20that%20our%20approach%20can%20learn%20the%20true%20physical%0Asystem%2C%20being%20consistent%20with%20the%20underlying%20physical%20laws%2C%20even%20when%20using%0Aempirical%20derivatives.%20DERL%20outperforms%20state-of-the-art%20methods%20in%0Ageneralizing%20an%20ODE%20to%20unseen%20initial%20conditions%20and%20a%20parametric%20PDE%20to%20unseen%0Aparameters.%20We%20finally%20propose%20a%20method%20based%20on%20DERL%20to%20transfer%20physical%0Aknowledge%20across%20models%20by%20extending%20them%20to%20new%20portions%20of%20the%20physical%0Adomain%20and%20new%20range%20of%20PDE%20parameters.%20We%20believe%20this%20is%20the%20first%20attempt%20at%0Abuilding%20physical%20models%20incrementally%20in%20multiple%20stages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520and%2520Transferring%2520Physical%2520Models%2520through%2520Derivatives%26entry.906535625%3DAlessandro%2520Trenta%2520and%2520Andrea%2520Cossu%2520and%2520Davide%2520Bacciu%26entry.1292438233%3D%2520%2520We%2520propose%2520Derivative%2520Learning%2520%2528DERL%2529%252C%2520a%2520supervised%2520approach%2520that%2520models%250Aphysical%2520systems%2520by%2520learning%2520their%2520partial%2520derivatives.%2520We%2520also%2520leverage%2520DERL%250Ato%2520build%2520physical%2520models%2520incrementally%252C%2520by%2520designing%2520a%2520distillation%2520protocol%250Athat%2520effectively%2520transfers%2520knowledge%2520from%2520a%2520pre-trained%2520to%2520a%2520student%2520model.%2520We%250Aprovide%2520theoretical%2520guarantees%2520that%2520our%2520approach%2520can%2520learn%2520the%2520true%2520physical%250Asystem%252C%2520being%2520consistent%2520with%2520the%2520underlying%2520physical%2520laws%252C%2520even%2520when%2520using%250Aempirical%2520derivatives.%2520DERL%2520outperforms%2520state-of-the-art%2520methods%2520in%250Ageneralizing%2520an%2520ODE%2520to%2520unseen%2520initial%2520conditions%2520and%2520a%2520parametric%2520PDE%2520to%2520unseen%250Aparameters.%2520We%2520finally%2520propose%2520a%2520method%2520based%2520on%2520DERL%2520to%2520transfer%2520physical%250Aknowledge%2520across%2520models%2520by%2520extending%2520them%2520to%2520new%2520portions%2520of%2520the%2520physical%250Adomain%2520and%2520new%2520range%2520of%2520PDE%2520parameters.%2520We%2520believe%2520this%2520is%2520the%2520first%2520attempt%2520at%250Abuilding%2520physical%2520models%2520incrementally%2520in%2520multiple%2520stages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20and%20Transferring%20Physical%20Models%20through%20Derivatives&entry.906535625=Alessandro%20Trenta%20and%20Andrea%20Cossu%20and%20Davide%20Bacciu&entry.1292438233=%20%20We%20propose%20Derivative%20Learning%20%28DERL%29%2C%20a%20supervised%20approach%20that%20models%0Aphysical%20systems%20by%20learning%20their%20partial%20derivatives.%20We%20also%20leverage%20DERL%0Ato%20build%20physical%20models%20incrementally%2C%20by%20designing%20a%20distillation%20protocol%0Athat%20effectively%20transfers%20knowledge%20from%20a%20pre-trained%20to%20a%20student%20model.%20We%0Aprovide%20theoretical%20guarantees%20that%20our%20approach%20can%20learn%20the%20true%20physical%0Asystem%2C%20being%20consistent%20with%20the%20underlying%20physical%20laws%2C%20even%20when%20using%0Aempirical%20derivatives.%20DERL%20outperforms%20state-of-the-art%20methods%20in%0Ageneralizing%20an%20ODE%20to%20unseen%20initial%20conditions%20and%20a%20parametric%20PDE%20to%20unseen%0Aparameters.%20We%20finally%20propose%20a%20method%20based%20on%20DERL%20to%20transfer%20physical%0Aknowledge%20across%20models%20by%20extending%20them%20to%20new%20portions%20of%20the%20physical%0Adomain%20and%20new%20range%20of%20PDE%20parameters.%20We%20believe%20this%20is%20the%20first%20attempt%20at%0Abuilding%20physical%20models%20incrementally%20in%20multiple%20stages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01391v1&entry.124074799=Read"},
{"title": "Desired Impedance Allocation for Robotic Systems", "author": "Mahdi Hejrati and Jouni Mattila", "abstract": "  Virtual Decomposition Control (VDC) has emerged as a powerful modular\nframework for real-world robotic control, particularly in contact-rich tasks.\nDespite its widespread use, VDC has been fundamentally limited to first-order\nimpedance allocation, inherently neglecting the desired inertia due to the\nmathematical complexity of second-order behavior allocation. However, inertia\nis crucial, not only for shaping dynamic responses during contact phases, but\nalso for enabling smooth acceleration and deceleration in trajectory tracking.\nMotivated by the growing demand for high-fidelity interaction control, this\nwork introduces, for the first time in the VDC framework, a method to realize\nsecond-order impedance behavior. By redefining the required end-effector\nvelocity and introducing a required acceleration and a pseudo-impedance term,\nwe achieve second-order impedance control while preserving the modularity of\nVDC. Rigorous stability analysis confirms the robustness of the proposed\ncontroller. Experimental validation on a 7-degree-of-freedom haptic exoskeleton\ndemonstrates superior tracking and contact performance compared to first-order\nmethods. Notably, incorporating inertia enables stable interaction with\nenvironments up to 70% stiffer, highlighting the effectiveness of the approach\nin real-world contact-rich scenarios.\n", "link": "http://arxiv.org/abs/2505.01308v1", "date": "2025-05-02", "relevancy": 1.4624, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4993}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4884}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Desired%20Impedance%20Allocation%20for%20Robotic%20Systems&body=Title%3A%20Desired%20Impedance%20Allocation%20for%20Robotic%20Systems%0AAuthor%3A%20Mahdi%20Hejrati%20and%20Jouni%20Mattila%0AAbstract%3A%20%20%20Virtual%20Decomposition%20Control%20%28VDC%29%20has%20emerged%20as%20a%20powerful%20modular%0Aframework%20for%20real-world%20robotic%20control%2C%20particularly%20in%20contact-rich%20tasks.%0ADespite%20its%20widespread%20use%2C%20VDC%20has%20been%20fundamentally%20limited%20to%20first-order%0Aimpedance%20allocation%2C%20inherently%20neglecting%20the%20desired%20inertia%20due%20to%20the%0Amathematical%20complexity%20of%20second-order%20behavior%20allocation.%20However%2C%20inertia%0Ais%20crucial%2C%20not%20only%20for%20shaping%20dynamic%20responses%20during%20contact%20phases%2C%20but%0Aalso%20for%20enabling%20smooth%20acceleration%20and%20deceleration%20in%20trajectory%20tracking.%0AMotivated%20by%20the%20growing%20demand%20for%20high-fidelity%20interaction%20control%2C%20this%0Awork%20introduces%2C%20for%20the%20first%20time%20in%20the%20VDC%20framework%2C%20a%20method%20to%20realize%0Asecond-order%20impedance%20behavior.%20By%20redefining%20the%20required%20end-effector%0Avelocity%20and%20introducing%20a%20required%20acceleration%20and%20a%20pseudo-impedance%20term%2C%0Awe%20achieve%20second-order%20impedance%20control%20while%20preserving%20the%20modularity%20of%0AVDC.%20Rigorous%20stability%20analysis%20confirms%20the%20robustness%20of%20the%20proposed%0Acontroller.%20Experimental%20validation%20on%20a%207-degree-of-freedom%20haptic%20exoskeleton%0Ademonstrates%20superior%20tracking%20and%20contact%20performance%20compared%20to%20first-order%0Amethods.%20Notably%2C%20incorporating%20inertia%20enables%20stable%20interaction%20with%0Aenvironments%20up%20to%2070%25%20stiffer%2C%20highlighting%20the%20effectiveness%20of%20the%20approach%0Ain%20real-world%20contact-rich%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesired%2520Impedance%2520Allocation%2520for%2520Robotic%2520Systems%26entry.906535625%3DMahdi%2520Hejrati%2520and%2520Jouni%2520Mattila%26entry.1292438233%3D%2520%2520Virtual%2520Decomposition%2520Control%2520%2528VDC%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520modular%250Aframework%2520for%2520real-world%2520robotic%2520control%252C%2520particularly%2520in%2520contact-rich%2520tasks.%250ADespite%2520its%2520widespread%2520use%252C%2520VDC%2520has%2520been%2520fundamentally%2520limited%2520to%2520first-order%250Aimpedance%2520allocation%252C%2520inherently%2520neglecting%2520the%2520desired%2520inertia%2520due%2520to%2520the%250Amathematical%2520complexity%2520of%2520second-order%2520behavior%2520allocation.%2520However%252C%2520inertia%250Ais%2520crucial%252C%2520not%2520only%2520for%2520shaping%2520dynamic%2520responses%2520during%2520contact%2520phases%252C%2520but%250Aalso%2520for%2520enabling%2520smooth%2520acceleration%2520and%2520deceleration%2520in%2520trajectory%2520tracking.%250AMotivated%2520by%2520the%2520growing%2520demand%2520for%2520high-fidelity%2520interaction%2520control%252C%2520this%250Awork%2520introduces%252C%2520for%2520the%2520first%2520time%2520in%2520the%2520VDC%2520framework%252C%2520a%2520method%2520to%2520realize%250Asecond-order%2520impedance%2520behavior.%2520By%2520redefining%2520the%2520required%2520end-effector%250Avelocity%2520and%2520introducing%2520a%2520required%2520acceleration%2520and%2520a%2520pseudo-impedance%2520term%252C%250Awe%2520achieve%2520second-order%2520impedance%2520control%2520while%2520preserving%2520the%2520modularity%2520of%250AVDC.%2520Rigorous%2520stability%2520analysis%2520confirms%2520the%2520robustness%2520of%2520the%2520proposed%250Acontroller.%2520Experimental%2520validation%2520on%2520a%25207-degree-of-freedom%2520haptic%2520exoskeleton%250Ademonstrates%2520superior%2520tracking%2520and%2520contact%2520performance%2520compared%2520to%2520first-order%250Amethods.%2520Notably%252C%2520incorporating%2520inertia%2520enables%2520stable%2520interaction%2520with%250Aenvironments%2520up%2520to%252070%2525%2520stiffer%252C%2520highlighting%2520the%2520effectiveness%2520of%2520the%2520approach%250Ain%2520real-world%2520contact-rich%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Desired%20Impedance%20Allocation%20for%20Robotic%20Systems&entry.906535625=Mahdi%20Hejrati%20and%20Jouni%20Mattila&entry.1292438233=%20%20Virtual%20Decomposition%20Control%20%28VDC%29%20has%20emerged%20as%20a%20powerful%20modular%0Aframework%20for%20real-world%20robotic%20control%2C%20particularly%20in%20contact-rich%20tasks.%0ADespite%20its%20widespread%20use%2C%20VDC%20has%20been%20fundamentally%20limited%20to%20first-order%0Aimpedance%20allocation%2C%20inherently%20neglecting%20the%20desired%20inertia%20due%20to%20the%0Amathematical%20complexity%20of%20second-order%20behavior%20allocation.%20However%2C%20inertia%0Ais%20crucial%2C%20not%20only%20for%20shaping%20dynamic%20responses%20during%20contact%20phases%2C%20but%0Aalso%20for%20enabling%20smooth%20acceleration%20and%20deceleration%20in%20trajectory%20tracking.%0AMotivated%20by%20the%20growing%20demand%20for%20high-fidelity%20interaction%20control%2C%20this%0Awork%20introduces%2C%20for%20the%20first%20time%20in%20the%20VDC%20framework%2C%20a%20method%20to%20realize%0Asecond-order%20impedance%20behavior.%20By%20redefining%20the%20required%20end-effector%0Avelocity%20and%20introducing%20a%20required%20acceleration%20and%20a%20pseudo-impedance%20term%2C%0Awe%20achieve%20second-order%20impedance%20control%20while%20preserving%20the%20modularity%20of%0AVDC.%20Rigorous%20stability%20analysis%20confirms%20the%20robustness%20of%20the%20proposed%0Acontroller.%20Experimental%20validation%20on%20a%207-degree-of-freedom%20haptic%20exoskeleton%0Ademonstrates%20superior%20tracking%20and%20contact%20performance%20compared%20to%20first-order%0Amethods.%20Notably%2C%20incorporating%20inertia%20enables%20stable%20interaction%20with%0Aenvironments%20up%20to%2070%25%20stiffer%2C%20highlighting%20the%20effectiveness%20of%20the%20approach%0Ain%20real-world%20contact-rich%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01308v1&entry.124074799=Read"},
{"title": "Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical\n  software assessments", "author": "Regan Bolton and Mohammadreza Sheikhfathollahi and Simon Parkinson and Vanessa Vulovic and Gary Bamford and Dan Basher and Howard Parkinson", "abstract": "  Safety critical software assessment requires robust assessment against\ncomplex regulatory frameworks, a process traditionally limited by manual\nevaluation. This paper presents Document Retrieval-Augmented Fine-Tuning\n(DRAFT), a novel approach that enhances the capabilities of a large language\nmodel (LLM) for safety-critical compliance assessment. DRAFT builds upon\nexisting Retrieval-Augmented Generation (RAG) techniques by introducing a novel\nfine-tuning framework that accommodates our dual-retrieval architecture, which\nsimultaneously accesses both software documentation and applicable reference\nstandards. To fine-tune DRAFT, we develop a semi-automated dataset generation\nmethodology that incorporates variable numbers of relevant documents with\nmeaningful distractors, closely mirroring real-world assessment scenarios.\nExperiments with GPT-4o-mini demonstrate a 7% improvement in correctness over\nthe baseline model, with qualitative improvements in evidence handling,\nresponse structure, and domain-specific reasoning. DRAFT represents a practical\napproach to improving compliance assessment systems while maintaining the\ntransparency and evidence-based reasoning essential in regulatory domains.\n", "link": "http://arxiv.org/abs/2505.01307v1", "date": "2025-05-02", "relevancy": 1.5028, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5506}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Document%20Retrieval%20Augmented%20Fine-Tuning%20%28DRAFT%29%20for%20safety-critical%0A%20%20software%20assessments&body=Title%3A%20Document%20Retrieval%20Augmented%20Fine-Tuning%20%28DRAFT%29%20for%20safety-critical%0A%20%20software%20assessments%0AAuthor%3A%20Regan%20Bolton%20and%20Mohammadreza%20Sheikhfathollahi%20and%20Simon%20Parkinson%20and%20Vanessa%20Vulovic%20and%20Gary%20Bamford%20and%20Dan%20Basher%20and%20Howard%20Parkinson%0AAbstract%3A%20%20%20Safety%20critical%20software%20assessment%20requires%20robust%20assessment%20against%0Acomplex%20regulatory%20frameworks%2C%20a%20process%20traditionally%20limited%20by%20manual%0Aevaluation.%20This%20paper%20presents%20Document%20Retrieval-Augmented%20Fine-Tuning%0A%28DRAFT%29%2C%20a%20novel%20approach%20that%20enhances%20the%20capabilities%20of%20a%20large%20language%0Amodel%20%28LLM%29%20for%20safety-critical%20compliance%20assessment.%20DRAFT%20builds%20upon%0Aexisting%20Retrieval-Augmented%20Generation%20%28RAG%29%20techniques%20by%20introducing%20a%20novel%0Afine-tuning%20framework%20that%20accommodates%20our%20dual-retrieval%20architecture%2C%20which%0Asimultaneously%20accesses%20both%20software%20documentation%20and%20applicable%20reference%0Astandards.%20To%20fine-tune%20DRAFT%2C%20we%20develop%20a%20semi-automated%20dataset%20generation%0Amethodology%20that%20incorporates%20variable%20numbers%20of%20relevant%20documents%20with%0Ameaningful%20distractors%2C%20closely%20mirroring%20real-world%20assessment%20scenarios.%0AExperiments%20with%20GPT-4o-mini%20demonstrate%20a%207%25%20improvement%20in%20correctness%20over%0Athe%20baseline%20model%2C%20with%20qualitative%20improvements%20in%20evidence%20handling%2C%0Aresponse%20structure%2C%20and%20domain-specific%20reasoning.%20DRAFT%20represents%20a%20practical%0Aapproach%20to%20improving%20compliance%20assessment%20systems%20while%20maintaining%20the%0Atransparency%20and%20evidence-based%20reasoning%20essential%20in%20regulatory%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocument%2520Retrieval%2520Augmented%2520Fine-Tuning%2520%2528DRAFT%2529%2520for%2520safety-critical%250A%2520%2520software%2520assessments%26entry.906535625%3DRegan%2520Bolton%2520and%2520Mohammadreza%2520Sheikhfathollahi%2520and%2520Simon%2520Parkinson%2520and%2520Vanessa%2520Vulovic%2520and%2520Gary%2520Bamford%2520and%2520Dan%2520Basher%2520and%2520Howard%2520Parkinson%26entry.1292438233%3D%2520%2520Safety%2520critical%2520software%2520assessment%2520requires%2520robust%2520assessment%2520against%250Acomplex%2520regulatory%2520frameworks%252C%2520a%2520process%2520traditionally%2520limited%2520by%2520manual%250Aevaluation.%2520This%2520paper%2520presents%2520Document%2520Retrieval-Augmented%2520Fine-Tuning%250A%2528DRAFT%2529%252C%2520a%2520novel%2520approach%2520that%2520enhances%2520the%2520capabilities%2520of%2520a%2520large%2520language%250Amodel%2520%2528LLM%2529%2520for%2520safety-critical%2520compliance%2520assessment.%2520DRAFT%2520builds%2520upon%250Aexisting%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520techniques%2520by%2520introducing%2520a%2520novel%250Afine-tuning%2520framework%2520that%2520accommodates%2520our%2520dual-retrieval%2520architecture%252C%2520which%250Asimultaneously%2520accesses%2520both%2520software%2520documentation%2520and%2520applicable%2520reference%250Astandards.%2520To%2520fine-tune%2520DRAFT%252C%2520we%2520develop%2520a%2520semi-automated%2520dataset%2520generation%250Amethodology%2520that%2520incorporates%2520variable%2520numbers%2520of%2520relevant%2520documents%2520with%250Ameaningful%2520distractors%252C%2520closely%2520mirroring%2520real-world%2520assessment%2520scenarios.%250AExperiments%2520with%2520GPT-4o-mini%2520demonstrate%2520a%25207%2525%2520improvement%2520in%2520correctness%2520over%250Athe%2520baseline%2520model%252C%2520with%2520qualitative%2520improvements%2520in%2520evidence%2520handling%252C%250Aresponse%2520structure%252C%2520and%2520domain-specific%2520reasoning.%2520DRAFT%2520represents%2520a%2520practical%250Aapproach%2520to%2520improving%2520compliance%2520assessment%2520systems%2520while%2520maintaining%2520the%250Atransparency%2520and%2520evidence-based%2520reasoning%2520essential%2520in%2520regulatory%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Document%20Retrieval%20Augmented%20Fine-Tuning%20%28DRAFT%29%20for%20safety-critical%0A%20%20software%20assessments&entry.906535625=Regan%20Bolton%20and%20Mohammadreza%20Sheikhfathollahi%20and%20Simon%20Parkinson%20and%20Vanessa%20Vulovic%20and%20Gary%20Bamford%20and%20Dan%20Basher%20and%20Howard%20Parkinson&entry.1292438233=%20%20Safety%20critical%20software%20assessment%20requires%20robust%20assessment%20against%0Acomplex%20regulatory%20frameworks%2C%20a%20process%20traditionally%20limited%20by%20manual%0Aevaluation.%20This%20paper%20presents%20Document%20Retrieval-Augmented%20Fine-Tuning%0A%28DRAFT%29%2C%20a%20novel%20approach%20that%20enhances%20the%20capabilities%20of%20a%20large%20language%0Amodel%20%28LLM%29%20for%20safety-critical%20compliance%20assessment.%20DRAFT%20builds%20upon%0Aexisting%20Retrieval-Augmented%20Generation%20%28RAG%29%20techniques%20by%20introducing%20a%20novel%0Afine-tuning%20framework%20that%20accommodates%20our%20dual-retrieval%20architecture%2C%20which%0Asimultaneously%20accesses%20both%20software%20documentation%20and%20applicable%20reference%0Astandards.%20To%20fine-tune%20DRAFT%2C%20we%20develop%20a%20semi-automated%20dataset%20generation%0Amethodology%20that%20incorporates%20variable%20numbers%20of%20relevant%20documents%20with%0Ameaningful%20distractors%2C%20closely%20mirroring%20real-world%20assessment%20scenarios.%0AExperiments%20with%20GPT-4o-mini%20demonstrate%20a%207%25%20improvement%20in%20correctness%20over%0Athe%20baseline%20model%2C%20with%20qualitative%20improvements%20in%20evidence%20handling%2C%0Aresponse%20structure%2C%20and%20domain-specific%20reasoning.%20DRAFT%20represents%20a%20practical%0Aapproach%20to%20improving%20compliance%20assessment%20systems%20while%20maintaining%20the%0Atransparency%20and%20evidence-based%20reasoning%20essential%20in%20regulatory%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01307v1&entry.124074799=Read"},
{"title": "Integration of Multi-Mode Preference into Home Energy Management System\n  Using Deep Reinforcement Learning", "author": "Mohammed Sumayli and Olugbenga Moses Anubi", "abstract": "  Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the\nsmart home ecosystem, aiming to enhance energy efficiency, reduce costs, and\nimprove user comfort. By enabling intelligent control and optimization of\nhousehold energy consumption, HEMS plays a significant role in bridging the gap\nbetween consumer needs and energy utility objectives. However, much of the\nexisting literature construes consumer comfort as a mere deviation from the\nstandard appliance settings. Such deviations are typically incorporated into\noptimization objectives via static weighting factors. These factors often\noverlook the dynamic nature of consumer behaviors and preferences. Addressing\nthis oversight, our paper introduces a multi-mode Deep Reinforcement\nLearning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize\nbased on dynamic, consumer-defined preferences. Our primary goal is to augment\nconsumer involvement in Demand Response (DR) programs by embedding dynamic\nmulti-mode preferences tailored to individual appliances. In this study, we\nleverage a model-free, single-agent DRL algorithm to deliver a HEMS framework\nthat is not only dynamic but also user-friendly. To validate its efficacy, we\nemployed real-world data at 15-minute intervals, including metrics such as\nelectricity price, ambient temperature, and appliances' power consumption. Our\nresults show that the model performs exceptionally well in optimizing energy\nconsumption within different preference modes. Furthermore, when compared to\ntraditional algorithms based on Mixed-Integer Linear Programming (MILP), our\nmodel achieves nearly optimal performance while outperforming in computational\nefficiency.\n", "link": "http://arxiv.org/abs/2505.01332v1", "date": "2025-05-02", "relevancy": 1.3615, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4954}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4617}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integration%20of%20Multi-Mode%20Preference%20into%20Home%20Energy%20Management%20System%0A%20%20Using%20Deep%20Reinforcement%20Learning&body=Title%3A%20Integration%20of%20Multi-Mode%20Preference%20into%20Home%20Energy%20Management%20System%0A%20%20Using%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Mohammed%20Sumayli%20and%20Olugbenga%20Moses%20Anubi%0AAbstract%3A%20%20%20Home%20Energy%20Management%20Systems%20%28HEMS%29%20have%20emerged%20as%20a%20pivotal%20tool%20in%20the%0Asmart%20home%20ecosystem%2C%20aiming%20to%20enhance%20energy%20efficiency%2C%20reduce%20costs%2C%20and%0Aimprove%20user%20comfort.%20By%20enabling%20intelligent%20control%20and%20optimization%20of%0Ahousehold%20energy%20consumption%2C%20HEMS%20plays%20a%20significant%20role%20in%20bridging%20the%20gap%0Abetween%20consumer%20needs%20and%20energy%20utility%20objectives.%20However%2C%20much%20of%20the%0Aexisting%20literature%20construes%20consumer%20comfort%20as%20a%20mere%20deviation%20from%20the%0Astandard%20appliance%20settings.%20Such%20deviations%20are%20typically%20incorporated%20into%0Aoptimization%20objectives%20via%20static%20weighting%20factors.%20These%20factors%20often%0Aoverlook%20the%20dynamic%20nature%20of%20consumer%20behaviors%20and%20preferences.%20Addressing%0Athis%20oversight%2C%20our%20paper%20introduces%20a%20multi-mode%20Deep%20Reinforcement%0ALearning-based%20HEMS%20%28DRL-HEMS%29%20framework%2C%20meticulously%20designed%20to%20optimize%0Abased%20on%20dynamic%2C%20consumer-defined%20preferences.%20Our%20primary%20goal%20is%20to%20augment%0Aconsumer%20involvement%20in%20Demand%20Response%20%28DR%29%20programs%20by%20embedding%20dynamic%0Amulti-mode%20preferences%20tailored%20to%20individual%20appliances.%20In%20this%20study%2C%20we%0Aleverage%20a%20model-free%2C%20single-agent%20DRL%20algorithm%20to%20deliver%20a%20HEMS%20framework%0Athat%20is%20not%20only%20dynamic%20but%20also%20user-friendly.%20To%20validate%20its%20efficacy%2C%20we%0Aemployed%20real-world%20data%20at%2015-minute%20intervals%2C%20including%20metrics%20such%20as%0Aelectricity%20price%2C%20ambient%20temperature%2C%20and%20appliances%27%20power%20consumption.%20Our%0Aresults%20show%20that%20the%20model%20performs%20exceptionally%20well%20in%20optimizing%20energy%0Aconsumption%20within%20different%20preference%20modes.%20Furthermore%2C%20when%20compared%20to%0Atraditional%20algorithms%20based%20on%20Mixed-Integer%20Linear%20Programming%20%28MILP%29%2C%20our%0Amodel%20achieves%20nearly%20optimal%20performance%20while%20outperforming%20in%20computational%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegration%2520of%2520Multi-Mode%2520Preference%2520into%2520Home%2520Energy%2520Management%2520System%250A%2520%2520Using%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DMohammed%2520Sumayli%2520and%2520Olugbenga%2520Moses%2520Anubi%26entry.1292438233%3D%2520%2520Home%2520Energy%2520Management%2520Systems%2520%2528HEMS%2529%2520have%2520emerged%2520as%2520a%2520pivotal%2520tool%2520in%2520the%250Asmart%2520home%2520ecosystem%252C%2520aiming%2520to%2520enhance%2520energy%2520efficiency%252C%2520reduce%2520costs%252C%2520and%250Aimprove%2520user%2520comfort.%2520By%2520enabling%2520intelligent%2520control%2520and%2520optimization%2520of%250Ahousehold%2520energy%2520consumption%252C%2520HEMS%2520plays%2520a%2520significant%2520role%2520in%2520bridging%2520the%2520gap%250Abetween%2520consumer%2520needs%2520and%2520energy%2520utility%2520objectives.%2520However%252C%2520much%2520of%2520the%250Aexisting%2520literature%2520construes%2520consumer%2520comfort%2520as%2520a%2520mere%2520deviation%2520from%2520the%250Astandard%2520appliance%2520settings.%2520Such%2520deviations%2520are%2520typically%2520incorporated%2520into%250Aoptimization%2520objectives%2520via%2520static%2520weighting%2520factors.%2520These%2520factors%2520often%250Aoverlook%2520the%2520dynamic%2520nature%2520of%2520consumer%2520behaviors%2520and%2520preferences.%2520Addressing%250Athis%2520oversight%252C%2520our%2520paper%2520introduces%2520a%2520multi-mode%2520Deep%2520Reinforcement%250ALearning-based%2520HEMS%2520%2528DRL-HEMS%2529%2520framework%252C%2520meticulously%2520designed%2520to%2520optimize%250Abased%2520on%2520dynamic%252C%2520consumer-defined%2520preferences.%2520Our%2520primary%2520goal%2520is%2520to%2520augment%250Aconsumer%2520involvement%2520in%2520Demand%2520Response%2520%2528DR%2529%2520programs%2520by%2520embedding%2520dynamic%250Amulti-mode%2520preferences%2520tailored%2520to%2520individual%2520appliances.%2520In%2520this%2520study%252C%2520we%250Aleverage%2520a%2520model-free%252C%2520single-agent%2520DRL%2520algorithm%2520to%2520deliver%2520a%2520HEMS%2520framework%250Athat%2520is%2520not%2520only%2520dynamic%2520but%2520also%2520user-friendly.%2520To%2520validate%2520its%2520efficacy%252C%2520we%250Aemployed%2520real-world%2520data%2520at%252015-minute%2520intervals%252C%2520including%2520metrics%2520such%2520as%250Aelectricity%2520price%252C%2520ambient%2520temperature%252C%2520and%2520appliances%2527%2520power%2520consumption.%2520Our%250Aresults%2520show%2520that%2520the%2520model%2520performs%2520exceptionally%2520well%2520in%2520optimizing%2520energy%250Aconsumption%2520within%2520different%2520preference%2520modes.%2520Furthermore%252C%2520when%2520compared%2520to%250Atraditional%2520algorithms%2520based%2520on%2520Mixed-Integer%2520Linear%2520Programming%2520%2528MILP%2529%252C%2520our%250Amodel%2520achieves%2520nearly%2520optimal%2520performance%2520while%2520outperforming%2520in%2520computational%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integration%20of%20Multi-Mode%20Preference%20into%20Home%20Energy%20Management%20System%0A%20%20Using%20Deep%20Reinforcement%20Learning&entry.906535625=Mohammed%20Sumayli%20and%20Olugbenga%20Moses%20Anubi&entry.1292438233=%20%20Home%20Energy%20Management%20Systems%20%28HEMS%29%20have%20emerged%20as%20a%20pivotal%20tool%20in%20the%0Asmart%20home%20ecosystem%2C%20aiming%20to%20enhance%20energy%20efficiency%2C%20reduce%20costs%2C%20and%0Aimprove%20user%20comfort.%20By%20enabling%20intelligent%20control%20and%20optimization%20of%0Ahousehold%20energy%20consumption%2C%20HEMS%20plays%20a%20significant%20role%20in%20bridging%20the%20gap%0Abetween%20consumer%20needs%20and%20energy%20utility%20objectives.%20However%2C%20much%20of%20the%0Aexisting%20literature%20construes%20consumer%20comfort%20as%20a%20mere%20deviation%20from%20the%0Astandard%20appliance%20settings.%20Such%20deviations%20are%20typically%20incorporated%20into%0Aoptimization%20objectives%20via%20static%20weighting%20factors.%20These%20factors%20often%0Aoverlook%20the%20dynamic%20nature%20of%20consumer%20behaviors%20and%20preferences.%20Addressing%0Athis%20oversight%2C%20our%20paper%20introduces%20a%20multi-mode%20Deep%20Reinforcement%0ALearning-based%20HEMS%20%28DRL-HEMS%29%20framework%2C%20meticulously%20designed%20to%20optimize%0Abased%20on%20dynamic%2C%20consumer-defined%20preferences.%20Our%20primary%20goal%20is%20to%20augment%0Aconsumer%20involvement%20in%20Demand%20Response%20%28DR%29%20programs%20by%20embedding%20dynamic%0Amulti-mode%20preferences%20tailored%20to%20individual%20appliances.%20In%20this%20study%2C%20we%0Aleverage%20a%20model-free%2C%20single-agent%20DRL%20algorithm%20to%20deliver%20a%20HEMS%20framework%0Athat%20is%20not%20only%20dynamic%20but%20also%20user-friendly.%20To%20validate%20its%20efficacy%2C%20we%0Aemployed%20real-world%20data%20at%2015-minute%20intervals%2C%20including%20metrics%20such%20as%0Aelectricity%20price%2C%20ambient%20temperature%2C%20and%20appliances%27%20power%20consumption.%20Our%0Aresults%20show%20that%20the%20model%20performs%20exceptionally%20well%20in%20optimizing%20energy%0Aconsumption%20within%20different%20preference%20modes.%20Furthermore%2C%20when%20compared%20to%0Atraditional%20algorithms%20based%20on%20Mixed-Integer%20Linear%20Programming%20%28MILP%29%2C%20our%0Amodel%20achieves%20nearly%20optimal%20performance%20while%20outperforming%20in%20computational%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01332v1&entry.124074799=Read"},
{"title": "Predicting the Price of Gold in the Financial Markets Using Hybrid\n  Models", "author": "Mohammadhossein Rashidi and Mohammad Modarres", "abstract": "  Predicting the price that has the least error and can provide the best and\nhighest accuracy has been one of the most challenging issues and one of the\nmost critical concerns among capital market activists and researchers.\nTherefore, a model that can solve problems and provide results with high\naccuracy is one of the topics of interest among researchers. In this project,\nusing time series prediction models such as ARIMA to estimate the price,\nvariables, and indicators related to technical analysis show the behavior of\ntraders involved in involving psychological factors for the model. By linking\nall of these variables to stepwise regression, we identify the best variables\ninfluencing the prediction of the variable. Finally, we enter the selected\nvariables as inputs to the artificial neural network. In other words, we want\nto call this whole prediction process the \"ARIMA_Stepwise Regression_Neural\nNetwork\" model and try to predict the price of gold in international financial\nmarkets. This approach is expected to be able to be used to predict the types\nof stocks, commodities, currency pairs, financial market indicators, and other\nitems used in local and international financial markets. Moreover, a comparison\nbetween the results of this method and time series methods is also expressed.\nFinally, based on the results, it can be seen that the resulting hybrid model\nhas the highest accuracy compared to the time series method, regression, and\nstepwise regression.\n", "link": "http://arxiv.org/abs/2505.01402v1", "date": "2025-05-02", "relevancy": 1.1929, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4061}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3967}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20the%20Price%20of%20Gold%20in%20the%20Financial%20Markets%20Using%20Hybrid%0A%20%20Models&body=Title%3A%20Predicting%20the%20Price%20of%20Gold%20in%20the%20Financial%20Markets%20Using%20Hybrid%0A%20%20Models%0AAuthor%3A%20Mohammadhossein%20Rashidi%20and%20Mohammad%20Modarres%0AAbstract%3A%20%20%20Predicting%20the%20price%20that%20has%20the%20least%20error%20and%20can%20provide%20the%20best%20and%0Ahighest%20accuracy%20has%20been%20one%20of%20the%20most%20challenging%20issues%20and%20one%20of%20the%0Amost%20critical%20concerns%20among%20capital%20market%20activists%20and%20researchers.%0ATherefore%2C%20a%20model%20that%20can%20solve%20problems%20and%20provide%20results%20with%20high%0Aaccuracy%20is%20one%20of%20the%20topics%20of%20interest%20among%20researchers.%20In%20this%20project%2C%0Ausing%20time%20series%20prediction%20models%20such%20as%20ARIMA%20to%20estimate%20the%20price%2C%0Avariables%2C%20and%20indicators%20related%20to%20technical%20analysis%20show%20the%20behavior%20of%0Atraders%20involved%20in%20involving%20psychological%20factors%20for%20the%20model.%20By%20linking%0Aall%20of%20these%20variables%20to%20stepwise%20regression%2C%20we%20identify%20the%20best%20variables%0Ainfluencing%20the%20prediction%20of%20the%20variable.%20Finally%2C%20we%20enter%20the%20selected%0Avariables%20as%20inputs%20to%20the%20artificial%20neural%20network.%20In%20other%20words%2C%20we%20want%0Ato%20call%20this%20whole%20prediction%20process%20the%20%22ARIMA_Stepwise%20Regression_Neural%0ANetwork%22%20model%20and%20try%20to%20predict%20the%20price%20of%20gold%20in%20international%20financial%0Amarkets.%20This%20approach%20is%20expected%20to%20be%20able%20to%20be%20used%20to%20predict%20the%20types%0Aof%20stocks%2C%20commodities%2C%20currency%20pairs%2C%20financial%20market%20indicators%2C%20and%20other%0Aitems%20used%20in%20local%20and%20international%20financial%20markets.%20Moreover%2C%20a%20comparison%0Abetween%20the%20results%20of%20this%20method%20and%20time%20series%20methods%20is%20also%20expressed.%0AFinally%2C%20based%20on%20the%20results%2C%20it%20can%20be%20seen%20that%20the%20resulting%20hybrid%20model%0Ahas%20the%20highest%20accuracy%20compared%20to%20the%20time%20series%20method%2C%20regression%2C%20and%0Astepwise%20regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520the%2520Price%2520of%2520Gold%2520in%2520the%2520Financial%2520Markets%2520Using%2520Hybrid%250A%2520%2520Models%26entry.906535625%3DMohammadhossein%2520Rashidi%2520and%2520Mohammad%2520Modarres%26entry.1292438233%3D%2520%2520Predicting%2520the%2520price%2520that%2520has%2520the%2520least%2520error%2520and%2520can%2520provide%2520the%2520best%2520and%250Ahighest%2520accuracy%2520has%2520been%2520one%2520of%2520the%2520most%2520challenging%2520issues%2520and%2520one%2520of%2520the%250Amost%2520critical%2520concerns%2520among%2520capital%2520market%2520activists%2520and%2520researchers.%250ATherefore%252C%2520a%2520model%2520that%2520can%2520solve%2520problems%2520and%2520provide%2520results%2520with%2520high%250Aaccuracy%2520is%2520one%2520of%2520the%2520topics%2520of%2520interest%2520among%2520researchers.%2520In%2520this%2520project%252C%250Ausing%2520time%2520series%2520prediction%2520models%2520such%2520as%2520ARIMA%2520to%2520estimate%2520the%2520price%252C%250Avariables%252C%2520and%2520indicators%2520related%2520to%2520technical%2520analysis%2520show%2520the%2520behavior%2520of%250Atraders%2520involved%2520in%2520involving%2520psychological%2520factors%2520for%2520the%2520model.%2520By%2520linking%250Aall%2520of%2520these%2520variables%2520to%2520stepwise%2520regression%252C%2520we%2520identify%2520the%2520best%2520variables%250Ainfluencing%2520the%2520prediction%2520of%2520the%2520variable.%2520Finally%252C%2520we%2520enter%2520the%2520selected%250Avariables%2520as%2520inputs%2520to%2520the%2520artificial%2520neural%2520network.%2520In%2520other%2520words%252C%2520we%2520want%250Ato%2520call%2520this%2520whole%2520prediction%2520process%2520the%2520%2522ARIMA_Stepwise%2520Regression_Neural%250ANetwork%2522%2520model%2520and%2520try%2520to%2520predict%2520the%2520price%2520of%2520gold%2520in%2520international%2520financial%250Amarkets.%2520This%2520approach%2520is%2520expected%2520to%2520be%2520able%2520to%2520be%2520used%2520to%2520predict%2520the%2520types%250Aof%2520stocks%252C%2520commodities%252C%2520currency%2520pairs%252C%2520financial%2520market%2520indicators%252C%2520and%2520other%250Aitems%2520used%2520in%2520local%2520and%2520international%2520financial%2520markets.%2520Moreover%252C%2520a%2520comparison%250Abetween%2520the%2520results%2520of%2520this%2520method%2520and%2520time%2520series%2520methods%2520is%2520also%2520expressed.%250AFinally%252C%2520based%2520on%2520the%2520results%252C%2520it%2520can%2520be%2520seen%2520that%2520the%2520resulting%2520hybrid%2520model%250Ahas%2520the%2520highest%2520accuracy%2520compared%2520to%2520the%2520time%2520series%2520method%252C%2520regression%252C%2520and%250Astepwise%2520regression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20the%20Price%20of%20Gold%20in%20the%20Financial%20Markets%20Using%20Hybrid%0A%20%20Models&entry.906535625=Mohammadhossein%20Rashidi%20and%20Mohammad%20Modarres&entry.1292438233=%20%20Predicting%20the%20price%20that%20has%20the%20least%20error%20and%20can%20provide%20the%20best%20and%0Ahighest%20accuracy%20has%20been%20one%20of%20the%20most%20challenging%20issues%20and%20one%20of%20the%0Amost%20critical%20concerns%20among%20capital%20market%20activists%20and%20researchers.%0ATherefore%2C%20a%20model%20that%20can%20solve%20problems%20and%20provide%20results%20with%20high%0Aaccuracy%20is%20one%20of%20the%20topics%20of%20interest%20among%20researchers.%20In%20this%20project%2C%0Ausing%20time%20series%20prediction%20models%20such%20as%20ARIMA%20to%20estimate%20the%20price%2C%0Avariables%2C%20and%20indicators%20related%20to%20technical%20analysis%20show%20the%20behavior%20of%0Atraders%20involved%20in%20involving%20psychological%20factors%20for%20the%20model.%20By%20linking%0Aall%20of%20these%20variables%20to%20stepwise%20regression%2C%20we%20identify%20the%20best%20variables%0Ainfluencing%20the%20prediction%20of%20the%20variable.%20Finally%2C%20we%20enter%20the%20selected%0Avariables%20as%20inputs%20to%20the%20artificial%20neural%20network.%20In%20other%20words%2C%20we%20want%0Ato%20call%20this%20whole%20prediction%20process%20the%20%22ARIMA_Stepwise%20Regression_Neural%0ANetwork%22%20model%20and%20try%20to%20predict%20the%20price%20of%20gold%20in%20international%20financial%0Amarkets.%20This%20approach%20is%20expected%20to%20be%20able%20to%20be%20used%20to%20predict%20the%20types%0Aof%20stocks%2C%20commodities%2C%20currency%20pairs%2C%20financial%20market%20indicators%2C%20and%20other%0Aitems%20used%20in%20local%20and%20international%20financial%20markets.%20Moreover%2C%20a%20comparison%0Abetween%20the%20results%20of%20this%20method%20and%20time%20series%20methods%20is%20also%20expressed.%0AFinally%2C%20based%20on%20the%20results%2C%20it%20can%20be%20seen%20that%20the%20resulting%20hybrid%20model%0Ahas%20the%20highest%20accuracy%20compared%20to%20the%20time%20series%20method%2C%20regression%2C%20and%0Astepwise%20regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01402v1&entry.124074799=Read"},
{"title": "Distilling Two-Timed Flow Models by Separately Matching Initial and\n  Terminal Velocities", "author": "Pramook Khungurn and Pratch Piyawongwisal and Sira Sriswadi and Supasorn Suwajanakorn", "abstract": "  A flow matching model learns a time-dependent vector field $v_t(x)$ that\ngenerates a probability path $\\{ p_t \\}_{0 \\leq t \\leq 1}$ that interpolates\nbetween a well-known noise distribution ($p_0$) and the data distribution\n($p_1$). It can be distilled into a \\emph{two-timed flow model} (TTFM)\n$\\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an\ninitial time $s$ to another belonging to the distribution at a terminal time\n$t$ in one function evaluation. We present a new loss function for TTFM\ndistillation called the \\emph{initial/terminal velocity matching} (ITVM) loss\nthat extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi\net al. by adding redundant terms to match the initial velocities at time $s$,\nremoving the derivative from the terminal velocity term at time $t$, and using\na version of the model under training, stabilized by exponential moving\naveraging (EMA), to compute the target terminal average velocity. Preliminary\nexperiments show that our loss leads to better few-step generation performance\non multiple types of datasets and model architectures over baselines.\n", "link": "http://arxiv.org/abs/2505.01169v1", "date": "2025-05-02", "relevancy": 0.9892, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5388}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4743}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Two-Timed%20Flow%20Models%20by%20Separately%20Matching%20Initial%20and%0A%20%20Terminal%20Velocities&body=Title%3A%20Distilling%20Two-Timed%20Flow%20Models%20by%20Separately%20Matching%20Initial%20and%0A%20%20Terminal%20Velocities%0AAuthor%3A%20Pramook%20Khungurn%20and%20Pratch%20Piyawongwisal%20and%20Sira%20Sriswadi%20and%20Supasorn%20Suwajanakorn%0AAbstract%3A%20%20%20A%20flow%20matching%20model%20learns%20a%20time-dependent%20vector%20field%20%24v_t%28x%29%24%20that%0Agenerates%20a%20probability%20path%20%24%5C%7B%20p_t%20%5C%7D_%7B0%20%5Cleq%20t%20%5Cleq%201%7D%24%20that%20interpolates%0Abetween%20a%20well-known%20noise%20distribution%20%28%24p_0%24%29%20and%20the%20data%20distribution%0A%28%24p_1%24%29.%20It%20can%20be%20distilled%20into%20a%20%5Cemph%7Btwo-timed%20flow%20model%7D%20%28TTFM%29%0A%24%5Cphi_%7Bs%2Cx%7D%28t%29%24%20that%20can%20transform%20a%20sample%20belonging%20to%20the%20distribution%20at%20an%0Ainitial%20time%20%24s%24%20to%20another%20belonging%20to%20the%20distribution%20at%20a%20terminal%20time%0A%24t%24%20in%20one%20function%20evaluation.%20We%20present%20a%20new%20loss%20function%20for%20TTFM%0Adistillation%20called%20the%20%5Cemph%7Binitial/terminal%20velocity%20matching%7D%20%28ITVM%29%20loss%0Athat%20extends%20the%20Lagrangian%20Flow%20Map%20Distillation%20%28LFMD%29%20loss%20proposed%20by%20Boffi%0Aet%20al.%20by%20adding%20redundant%20terms%20to%20match%20the%20initial%20velocities%20at%20time%20%24s%24%2C%0Aremoving%20the%20derivative%20from%20the%20terminal%20velocity%20term%20at%20time%20%24t%24%2C%20and%20using%0Aa%20version%20of%20the%20model%20under%20training%2C%20stabilized%20by%20exponential%20moving%0Aaveraging%20%28EMA%29%2C%20to%20compute%20the%20target%20terminal%20average%20velocity.%20Preliminary%0Aexperiments%20show%20that%20our%20loss%20leads%20to%20better%20few-step%20generation%20performance%0Aon%20multiple%20types%20of%20datasets%20and%20model%20architectures%20over%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Two-Timed%2520Flow%2520Models%2520by%2520Separately%2520Matching%2520Initial%2520and%250A%2520%2520Terminal%2520Velocities%26entry.906535625%3DPramook%2520Khungurn%2520and%2520Pratch%2520Piyawongwisal%2520and%2520Sira%2520Sriswadi%2520and%2520Supasorn%2520Suwajanakorn%26entry.1292438233%3D%2520%2520A%2520flow%2520matching%2520model%2520learns%2520a%2520time-dependent%2520vector%2520field%2520%2524v_t%2528x%2529%2524%2520that%250Agenerates%2520a%2520probability%2520path%2520%2524%255C%257B%2520p_t%2520%255C%257D_%257B0%2520%255Cleq%2520t%2520%255Cleq%25201%257D%2524%2520that%2520interpolates%250Abetween%2520a%2520well-known%2520noise%2520distribution%2520%2528%2524p_0%2524%2529%2520and%2520the%2520data%2520distribution%250A%2528%2524p_1%2524%2529.%2520It%2520can%2520be%2520distilled%2520into%2520a%2520%255Cemph%257Btwo-timed%2520flow%2520model%257D%2520%2528TTFM%2529%250A%2524%255Cphi_%257Bs%252Cx%257D%2528t%2529%2524%2520that%2520can%2520transform%2520a%2520sample%2520belonging%2520to%2520the%2520distribution%2520at%2520an%250Ainitial%2520time%2520%2524s%2524%2520to%2520another%2520belonging%2520to%2520the%2520distribution%2520at%2520a%2520terminal%2520time%250A%2524t%2524%2520in%2520one%2520function%2520evaluation.%2520We%2520present%2520a%2520new%2520loss%2520function%2520for%2520TTFM%250Adistillation%2520called%2520the%2520%255Cemph%257Binitial/terminal%2520velocity%2520matching%257D%2520%2528ITVM%2529%2520loss%250Athat%2520extends%2520the%2520Lagrangian%2520Flow%2520Map%2520Distillation%2520%2528LFMD%2529%2520loss%2520proposed%2520by%2520Boffi%250Aet%2520al.%2520by%2520adding%2520redundant%2520terms%2520to%2520match%2520the%2520initial%2520velocities%2520at%2520time%2520%2524s%2524%252C%250Aremoving%2520the%2520derivative%2520from%2520the%2520terminal%2520velocity%2520term%2520at%2520time%2520%2524t%2524%252C%2520and%2520using%250Aa%2520version%2520of%2520the%2520model%2520under%2520training%252C%2520stabilized%2520by%2520exponential%2520moving%250Aaveraging%2520%2528EMA%2529%252C%2520to%2520compute%2520the%2520target%2520terminal%2520average%2520velocity.%2520Preliminary%250Aexperiments%2520show%2520that%2520our%2520loss%2520leads%2520to%2520better%2520few-step%2520generation%2520performance%250Aon%2520multiple%2520types%2520of%2520datasets%2520and%2520model%2520architectures%2520over%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Two-Timed%20Flow%20Models%20by%20Separately%20Matching%20Initial%20and%0A%20%20Terminal%20Velocities&entry.906535625=Pramook%20Khungurn%20and%20Pratch%20Piyawongwisal%20and%20Sira%20Sriswadi%20and%20Supasorn%20Suwajanakorn&entry.1292438233=%20%20A%20flow%20matching%20model%20learns%20a%20time-dependent%20vector%20field%20%24v_t%28x%29%24%20that%0Agenerates%20a%20probability%20path%20%24%5C%7B%20p_t%20%5C%7D_%7B0%20%5Cleq%20t%20%5Cleq%201%7D%24%20that%20interpolates%0Abetween%20a%20well-known%20noise%20distribution%20%28%24p_0%24%29%20and%20the%20data%20distribution%0A%28%24p_1%24%29.%20It%20can%20be%20distilled%20into%20a%20%5Cemph%7Btwo-timed%20flow%20model%7D%20%28TTFM%29%0A%24%5Cphi_%7Bs%2Cx%7D%28t%29%24%20that%20can%20transform%20a%20sample%20belonging%20to%20the%20distribution%20at%20an%0Ainitial%20time%20%24s%24%20to%20another%20belonging%20to%20the%20distribution%20at%20a%20terminal%20time%0A%24t%24%20in%20one%20function%20evaluation.%20We%20present%20a%20new%20loss%20function%20for%20TTFM%0Adistillation%20called%20the%20%5Cemph%7Binitial/terminal%20velocity%20matching%7D%20%28ITVM%29%20loss%0Athat%20extends%20the%20Lagrangian%20Flow%20Map%20Distillation%20%28LFMD%29%20loss%20proposed%20by%20Boffi%0Aet%20al.%20by%20adding%20redundant%20terms%20to%20match%20the%20initial%20velocities%20at%20time%20%24s%24%2C%0Aremoving%20the%20derivative%20from%20the%20terminal%20velocity%20term%20at%20time%20%24t%24%2C%20and%20using%0Aa%20version%20of%20the%20model%20under%20training%2C%20stabilized%20by%20exponential%20moving%0Aaveraging%20%28EMA%29%2C%20to%20compute%20the%20target%20terminal%20average%20velocity.%20Preliminary%0Aexperiments%20show%20that%20our%20loss%20leads%20to%20better%20few-step%20generation%20performance%0Aon%20multiple%20types%20of%20datasets%20and%20model%20architectures%20over%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01169v1&entry.124074799=Read"},
{"title": "A Rate-Distortion Framework for Summarization", "author": "Enes Arda and Aylin Yener", "abstract": "  This paper introduces an information-theoretic framework for text\nsummarization. We define the summarizer rate-distortion function and show that\nit provides a fundamental lower bound on summarizer performance. We describe an\niterative procedure, similar to Blahut-Arimoto algorithm, for computing this\nfunction. To handle real-world text datasets, we also propose a practical\nmethod that can calculate the summarizer rate-distortion function with limited\ndata. Finally, we empirically confirm our theoretical results by comparing the\nsummarizer rate-distortion function with the performances of different\nsummarizers used in practice.\n", "link": "http://arxiv.org/abs/2501.13100v2", "date": "2025-05-02", "relevancy": 1.3926, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4799}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4721}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Rate-Distortion%20Framework%20for%20Summarization&body=Title%3A%20A%20Rate-Distortion%20Framework%20for%20Summarization%0AAuthor%3A%20Enes%20Arda%20and%20Aylin%20Yener%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20information-theoretic%20framework%20for%20text%0Asummarization.%20We%20define%20the%20summarizer%20rate-distortion%20function%20and%20show%20that%0Ait%20provides%20a%20fundamental%20lower%20bound%20on%20summarizer%20performance.%20We%20describe%20an%0Aiterative%20procedure%2C%20similar%20to%20Blahut-Arimoto%20algorithm%2C%20for%20computing%20this%0Afunction.%20To%20handle%20real-world%20text%20datasets%2C%20we%20also%20propose%20a%20practical%0Amethod%20that%20can%20calculate%20the%20summarizer%20rate-distortion%20function%20with%20limited%0Adata.%20Finally%2C%20we%20empirically%20confirm%20our%20theoretical%20results%20by%20comparing%20the%0Asummarizer%20rate-distortion%20function%20with%20the%20performances%20of%20different%0Asummarizers%20used%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Rate-Distortion%2520Framework%2520for%2520Summarization%26entry.906535625%3DEnes%2520Arda%2520and%2520Aylin%2520Yener%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520information-theoretic%2520framework%2520for%2520text%250Asummarization.%2520We%2520define%2520the%2520summarizer%2520rate-distortion%2520function%2520and%2520show%2520that%250Ait%2520provides%2520a%2520fundamental%2520lower%2520bound%2520on%2520summarizer%2520performance.%2520We%2520describe%2520an%250Aiterative%2520procedure%252C%2520similar%2520to%2520Blahut-Arimoto%2520algorithm%252C%2520for%2520computing%2520this%250Afunction.%2520To%2520handle%2520real-world%2520text%2520datasets%252C%2520we%2520also%2520propose%2520a%2520practical%250Amethod%2520that%2520can%2520calculate%2520the%2520summarizer%2520rate-distortion%2520function%2520with%2520limited%250Adata.%2520Finally%252C%2520we%2520empirically%2520confirm%2520our%2520theoretical%2520results%2520by%2520comparing%2520the%250Asummarizer%2520rate-distortion%2520function%2520with%2520the%2520performances%2520of%2520different%250Asummarizers%2520used%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Rate-Distortion%20Framework%20for%20Summarization&entry.906535625=Enes%20Arda%20and%20Aylin%20Yener&entry.1292438233=%20%20This%20paper%20introduces%20an%20information-theoretic%20framework%20for%20text%0Asummarization.%20We%20define%20the%20summarizer%20rate-distortion%20function%20and%20show%20that%0Ait%20provides%20a%20fundamental%20lower%20bound%20on%20summarizer%20performance.%20We%20describe%20an%0Aiterative%20procedure%2C%20similar%20to%20Blahut-Arimoto%20algorithm%2C%20for%20computing%20this%0Afunction.%20To%20handle%20real-world%20text%20datasets%2C%20we%20also%20propose%20a%20practical%0Amethod%20that%20can%20calculate%20the%20summarizer%20rate-distortion%20function%20with%20limited%0Adata.%20Finally%2C%20we%20empirically%20confirm%20our%20theoretical%20results%20by%20comparing%20the%0Asummarizer%20rate-distortion%20function%20with%20the%20performances%20of%20different%0Asummarizers%20used%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13100v2&entry.124074799=Read"},
{"title": "FlexLLM: A System for Co-Serving Large Language Model Inference and\n  Parameter-Efficient Finetuning", "author": "Gabriele Oliaro and Xupeng Miao and Xinhao Cheng and Vineeth Kada and Ruohan Gao and Yingyi Huang and Remi Delacourt and April Yang and Yingcheng Wang and Mengdi Wu and Colin Unger and Zhihao Jia", "abstract": "  Finetuning large language models (LLMs) is essential for task adaptation, yet\nserving stacks today isolate inference and finetuning on separate GPU clusters\n-- wasting resources and under-utilizing hardware. We introduce FlexLLM, the\nfirst system to co-serve LLM inference and PEFT-based finetuning on shared GPUs\nby fusing computation at the token level. The static compilation optimizations\nin FlexLLM -- dependent parallelization and graph pruning significantly shrink\nactivation memory, leading to end-to-end GPU memory savings by up to 80%. At\nruntime, a novel token-level finetuning mechanism paired with a hybrid token\nscheduler dynamically interleaves inference and training tokens within each\nco-serving iteration, meeting strict latency SLOs while maximizing utilization.\nIn end-to-end benchmarks on LLaMA-3.1-8B, Qwen-2.5-14B, and Qwen-2.5-32B,\nFlexLLM sustains the inference SLO requirements up to 20 req/s, and improves\nfinetuning throughput by 1.9-4.8x under heavy inference workloads and 2.5-6.8x\nunder light loads, preserving over 76% of peak finetuning progress even at peak\ndemand. The source code of FlexLLM is publicly available at\nhttps://github.com/flexflow/FlexFlow/.\n", "link": "http://arxiv.org/abs/2402.18789v2", "date": "2025-05-02", "relevancy": 1.4018, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4791}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4752}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexLLM%3A%20A%20System%20for%20Co-Serving%20Large%20Language%20Model%20Inference%20and%0A%20%20Parameter-Efficient%20Finetuning&body=Title%3A%20FlexLLM%3A%20A%20System%20for%20Co-Serving%20Large%20Language%20Model%20Inference%20and%0A%20%20Parameter-Efficient%20Finetuning%0AAuthor%3A%20Gabriele%20Oliaro%20and%20Xupeng%20Miao%20and%20Xinhao%20Cheng%20and%20Vineeth%20Kada%20and%20Ruohan%20Gao%20and%20Yingyi%20Huang%20and%20Remi%20Delacourt%20and%20April%20Yang%20and%20Yingcheng%20Wang%20and%20Mengdi%20Wu%20and%20Colin%20Unger%20and%20Zhihao%20Jia%0AAbstract%3A%20%20%20Finetuning%20large%20language%20models%20%28LLMs%29%20is%20essential%20for%20task%20adaptation%2C%20yet%0Aserving%20stacks%20today%20isolate%20inference%20and%20finetuning%20on%20separate%20GPU%20clusters%0A--%20wasting%20resources%20and%20under-utilizing%20hardware.%20We%20introduce%20FlexLLM%2C%20the%0Afirst%20system%20to%20co-serve%20LLM%20inference%20and%20PEFT-based%20finetuning%20on%20shared%20GPUs%0Aby%20fusing%20computation%20at%20the%20token%20level.%20The%20static%20compilation%20optimizations%0Ain%20FlexLLM%20--%20dependent%20parallelization%20and%20graph%20pruning%20significantly%20shrink%0Aactivation%20memory%2C%20leading%20to%20end-to-end%20GPU%20memory%20savings%20by%20up%20to%2080%25.%20At%0Aruntime%2C%20a%20novel%20token-level%20finetuning%20mechanism%20paired%20with%20a%20hybrid%20token%0Ascheduler%20dynamically%20interleaves%20inference%20and%20training%20tokens%20within%20each%0Aco-serving%20iteration%2C%20meeting%20strict%20latency%20SLOs%20while%20maximizing%20utilization.%0AIn%20end-to-end%20benchmarks%20on%20LLaMA-3.1-8B%2C%20Qwen-2.5-14B%2C%20and%20Qwen-2.5-32B%2C%0AFlexLLM%20sustains%20the%20inference%20SLO%20requirements%20up%20to%2020%20req/s%2C%20and%20improves%0Afinetuning%20throughput%20by%201.9-4.8x%20under%20heavy%20inference%20workloads%20and%202.5-6.8x%0Aunder%20light%20loads%2C%20preserving%20over%2076%25%20of%20peak%20finetuning%20progress%20even%20at%20peak%0Ademand.%20The%20source%20code%20of%20FlexLLM%20is%20publicly%20available%20at%0Ahttps%3A//github.com/flexflow/FlexFlow/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18789v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexLLM%253A%2520A%2520System%2520for%2520Co-Serving%2520Large%2520Language%2520Model%2520Inference%2520and%250A%2520%2520Parameter-Efficient%2520Finetuning%26entry.906535625%3DGabriele%2520Oliaro%2520and%2520Xupeng%2520Miao%2520and%2520Xinhao%2520Cheng%2520and%2520Vineeth%2520Kada%2520and%2520Ruohan%2520Gao%2520and%2520Yingyi%2520Huang%2520and%2520Remi%2520Delacourt%2520and%2520April%2520Yang%2520and%2520Yingcheng%2520Wang%2520and%2520Mengdi%2520Wu%2520and%2520Colin%2520Unger%2520and%2520Zhihao%2520Jia%26entry.1292438233%3D%2520%2520Finetuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520essential%2520for%2520task%2520adaptation%252C%2520yet%250Aserving%2520stacks%2520today%2520isolate%2520inference%2520and%2520finetuning%2520on%2520separate%2520GPU%2520clusters%250A--%2520wasting%2520resources%2520and%2520under-utilizing%2520hardware.%2520We%2520introduce%2520FlexLLM%252C%2520the%250Afirst%2520system%2520to%2520co-serve%2520LLM%2520inference%2520and%2520PEFT-based%2520finetuning%2520on%2520shared%2520GPUs%250Aby%2520fusing%2520computation%2520at%2520the%2520token%2520level.%2520The%2520static%2520compilation%2520optimizations%250Ain%2520FlexLLM%2520--%2520dependent%2520parallelization%2520and%2520graph%2520pruning%2520significantly%2520shrink%250Aactivation%2520memory%252C%2520leading%2520to%2520end-to-end%2520GPU%2520memory%2520savings%2520by%2520up%2520to%252080%2525.%2520At%250Aruntime%252C%2520a%2520novel%2520token-level%2520finetuning%2520mechanism%2520paired%2520with%2520a%2520hybrid%2520token%250Ascheduler%2520dynamically%2520interleaves%2520inference%2520and%2520training%2520tokens%2520within%2520each%250Aco-serving%2520iteration%252C%2520meeting%2520strict%2520latency%2520SLOs%2520while%2520maximizing%2520utilization.%250AIn%2520end-to-end%2520benchmarks%2520on%2520LLaMA-3.1-8B%252C%2520Qwen-2.5-14B%252C%2520and%2520Qwen-2.5-32B%252C%250AFlexLLM%2520sustains%2520the%2520inference%2520SLO%2520requirements%2520up%2520to%252020%2520req/s%252C%2520and%2520improves%250Afinetuning%2520throughput%2520by%25201.9-4.8x%2520under%2520heavy%2520inference%2520workloads%2520and%25202.5-6.8x%250Aunder%2520light%2520loads%252C%2520preserving%2520over%252076%2525%2520of%2520peak%2520finetuning%2520progress%2520even%2520at%2520peak%250Ademand.%2520The%2520source%2520code%2520of%2520FlexLLM%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/flexflow/FlexFlow/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18789v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexLLM%3A%20A%20System%20for%20Co-Serving%20Large%20Language%20Model%20Inference%20and%0A%20%20Parameter-Efficient%20Finetuning&entry.906535625=Gabriele%20Oliaro%20and%20Xupeng%20Miao%20and%20Xinhao%20Cheng%20and%20Vineeth%20Kada%20and%20Ruohan%20Gao%20and%20Yingyi%20Huang%20and%20Remi%20Delacourt%20and%20April%20Yang%20and%20Yingcheng%20Wang%20and%20Mengdi%20Wu%20and%20Colin%20Unger%20and%20Zhihao%20Jia&entry.1292438233=%20%20Finetuning%20large%20language%20models%20%28LLMs%29%20is%20essential%20for%20task%20adaptation%2C%20yet%0Aserving%20stacks%20today%20isolate%20inference%20and%20finetuning%20on%20separate%20GPU%20clusters%0A--%20wasting%20resources%20and%20under-utilizing%20hardware.%20We%20introduce%20FlexLLM%2C%20the%0Afirst%20system%20to%20co-serve%20LLM%20inference%20and%20PEFT-based%20finetuning%20on%20shared%20GPUs%0Aby%20fusing%20computation%20at%20the%20token%20level.%20The%20static%20compilation%20optimizations%0Ain%20FlexLLM%20--%20dependent%20parallelization%20and%20graph%20pruning%20significantly%20shrink%0Aactivation%20memory%2C%20leading%20to%20end-to-end%20GPU%20memory%20savings%20by%20up%20to%2080%25.%20At%0Aruntime%2C%20a%20novel%20token-level%20finetuning%20mechanism%20paired%20with%20a%20hybrid%20token%0Ascheduler%20dynamically%20interleaves%20inference%20and%20training%20tokens%20within%20each%0Aco-serving%20iteration%2C%20meeting%20strict%20latency%20SLOs%20while%20maximizing%20utilization.%0AIn%20end-to-end%20benchmarks%20on%20LLaMA-3.1-8B%2C%20Qwen-2.5-14B%2C%20and%20Qwen-2.5-32B%2C%0AFlexLLM%20sustains%20the%20inference%20SLO%20requirements%20up%20to%2020%20req/s%2C%20and%20improves%0Afinetuning%20throughput%20by%201.9-4.8x%20under%20heavy%20inference%20workloads%20and%202.5-6.8x%0Aunder%20light%20loads%2C%20preserving%20over%2076%25%20of%20peak%20finetuning%20progress%20even%20at%20peak%0Ademand.%20The%20source%20code%20of%20FlexLLM%20is%20publicly%20available%20at%0Ahttps%3A//github.com/flexflow/FlexFlow/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18789v2&entry.124074799=Read"},
{"title": "Offline Model-Based Optimization by Learning to Rank", "author": "Rong-Xi Tan and Ke Xue and Shen-Huan Lyu and Haopu Shang and Yao Wang and Yaoyuan Wang and Sheng Fu and Chao Qian", "abstract": "  Offline model-based optimization (MBO) aims to identify a design that\nmaximizes a black-box function using only a fixed, pre-collected dataset of\ndesigns and their corresponding scores. A common approach in offline MBO is to\ntrain a regression-based surrogate model by minimizing mean squared error (MSE)\nand then find the best design within this surrogate model by different\noptimizers (e.g., gradient ascent). However, a critical challenge is the risk\nof out-of-distribution errors, i.e., the surrogate model may typically\noverestimate the scores and mislead the optimizers into suboptimal regions.\nPrior works have attempted to address this issue in various ways, such as using\nregularization techniques and ensemble learning to enhance the robustness of\nthe model, but it still remains. In this paper, we argue that regression models\ntrained with MSE are not well-aligned with the primary goal of offline MBO,\nwhich is to select promising designs rather than to predict their scores\nprecisely. Notably, if a surrogate model can maintain the order of candidate\ndesigns based on their relative score relationships, it can produce the best\ndesigns even without precise predictions. To validate it, we conduct\nexperiments to compare the relationship between the quality of the final\ndesigns and MSE, finding that the correlation is really very weak. In contrast,\na metric that measures order-maintaining quality shows a significantly stronger\ncorrelation. Based on this observation, we propose learning a ranking-based\nmodel that leverages learning to rank techniques to prioritize promising\ndesigns based on their relative scores. We show that the generalization error\non ranking loss can be well bounded. Empirical results across diverse tasks\ndemonstrate the superior performance of our proposed ranking-based models than\ntwenty existing methods.\n", "link": "http://arxiv.org/abs/2410.11502v3", "date": "2025-05-02", "relevancy": 1.4318, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4971}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4688}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Offline%20Model-Based%20Optimization%20by%20Learning%20to%20Rank&body=Title%3A%20Offline%20Model-Based%20Optimization%20by%20Learning%20to%20Rank%0AAuthor%3A%20Rong-Xi%20Tan%20and%20Ke%20Xue%20and%20Shen-Huan%20Lyu%20and%20Haopu%20Shang%20and%20Yao%20Wang%20and%20Yaoyuan%20Wang%20and%20Sheng%20Fu%20and%20Chao%20Qian%0AAbstract%3A%20%20%20Offline%20model-based%20optimization%20%28MBO%29%20aims%20to%20identify%20a%20design%20that%0Amaximizes%20a%20black-box%20function%20using%20only%20a%20fixed%2C%20pre-collected%20dataset%20of%0Adesigns%20and%20their%20corresponding%20scores.%20A%20common%20approach%20in%20offline%20MBO%20is%20to%0Atrain%20a%20regression-based%20surrogate%20model%20by%20minimizing%20mean%20squared%20error%20%28MSE%29%0Aand%20then%20find%20the%20best%20design%20within%20this%20surrogate%20model%20by%20different%0Aoptimizers%20%28e.g.%2C%20gradient%20ascent%29.%20However%2C%20a%20critical%20challenge%20is%20the%20risk%0Aof%20out-of-distribution%20errors%2C%20i.e.%2C%20the%20surrogate%20model%20may%20typically%0Aoverestimate%20the%20scores%20and%20mislead%20the%20optimizers%20into%20suboptimal%20regions.%0APrior%20works%20have%20attempted%20to%20address%20this%20issue%20in%20various%20ways%2C%20such%20as%20using%0Aregularization%20techniques%20and%20ensemble%20learning%20to%20enhance%20the%20robustness%20of%0Athe%20model%2C%20but%20it%20still%20remains.%20In%20this%20paper%2C%20we%20argue%20that%20regression%20models%0Atrained%20with%20MSE%20are%20not%20well-aligned%20with%20the%20primary%20goal%20of%20offline%20MBO%2C%0Awhich%20is%20to%20select%20promising%20designs%20rather%20than%20to%20predict%20their%20scores%0Aprecisely.%20Notably%2C%20if%20a%20surrogate%20model%20can%20maintain%20the%20order%20of%20candidate%0Adesigns%20based%20on%20their%20relative%20score%20relationships%2C%20it%20can%20produce%20the%20best%0Adesigns%20even%20without%20precise%20predictions.%20To%20validate%20it%2C%20we%20conduct%0Aexperiments%20to%20compare%20the%20relationship%20between%20the%20quality%20of%20the%20final%0Adesigns%20and%20MSE%2C%20finding%20that%20the%20correlation%20is%20really%20very%20weak.%20In%20contrast%2C%0Aa%20metric%20that%20measures%20order-maintaining%20quality%20shows%20a%20significantly%20stronger%0Acorrelation.%20Based%20on%20this%20observation%2C%20we%20propose%20learning%20a%20ranking-based%0Amodel%20that%20leverages%20learning%20to%20rank%20techniques%20to%20prioritize%20promising%0Adesigns%20based%20on%20their%20relative%20scores.%20We%20show%20that%20the%20generalization%20error%0Aon%20ranking%20loss%20can%20be%20well%20bounded.%20Empirical%20results%20across%20diverse%20tasks%0Ademonstrate%20the%20superior%20performance%20of%20our%20proposed%20ranking-based%20models%20than%0Atwenty%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11502v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOffline%2520Model-Based%2520Optimization%2520by%2520Learning%2520to%2520Rank%26entry.906535625%3DRong-Xi%2520Tan%2520and%2520Ke%2520Xue%2520and%2520Shen-Huan%2520Lyu%2520and%2520Haopu%2520Shang%2520and%2520Yao%2520Wang%2520and%2520Yaoyuan%2520Wang%2520and%2520Sheng%2520Fu%2520and%2520Chao%2520Qian%26entry.1292438233%3D%2520%2520Offline%2520model-based%2520optimization%2520%2528MBO%2529%2520aims%2520to%2520identify%2520a%2520design%2520that%250Amaximizes%2520a%2520black-box%2520function%2520using%2520only%2520a%2520fixed%252C%2520pre-collected%2520dataset%2520of%250Adesigns%2520and%2520their%2520corresponding%2520scores.%2520A%2520common%2520approach%2520in%2520offline%2520MBO%2520is%2520to%250Atrain%2520a%2520regression-based%2520surrogate%2520model%2520by%2520minimizing%2520mean%2520squared%2520error%2520%2528MSE%2529%250Aand%2520then%2520find%2520the%2520best%2520design%2520within%2520this%2520surrogate%2520model%2520by%2520different%250Aoptimizers%2520%2528e.g.%252C%2520gradient%2520ascent%2529.%2520However%252C%2520a%2520critical%2520challenge%2520is%2520the%2520risk%250Aof%2520out-of-distribution%2520errors%252C%2520i.e.%252C%2520the%2520surrogate%2520model%2520may%2520typically%250Aoverestimate%2520the%2520scores%2520and%2520mislead%2520the%2520optimizers%2520into%2520suboptimal%2520regions.%250APrior%2520works%2520have%2520attempted%2520to%2520address%2520this%2520issue%2520in%2520various%2520ways%252C%2520such%2520as%2520using%250Aregularization%2520techniques%2520and%2520ensemble%2520learning%2520to%2520enhance%2520the%2520robustness%2520of%250Athe%2520model%252C%2520but%2520it%2520still%2520remains.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520regression%2520models%250Atrained%2520with%2520MSE%2520are%2520not%2520well-aligned%2520with%2520the%2520primary%2520goal%2520of%2520offline%2520MBO%252C%250Awhich%2520is%2520to%2520select%2520promising%2520designs%2520rather%2520than%2520to%2520predict%2520their%2520scores%250Aprecisely.%2520Notably%252C%2520if%2520a%2520surrogate%2520model%2520can%2520maintain%2520the%2520order%2520of%2520candidate%250Adesigns%2520based%2520on%2520their%2520relative%2520score%2520relationships%252C%2520it%2520can%2520produce%2520the%2520best%250Adesigns%2520even%2520without%2520precise%2520predictions.%2520To%2520validate%2520it%252C%2520we%2520conduct%250Aexperiments%2520to%2520compare%2520the%2520relationship%2520between%2520the%2520quality%2520of%2520the%2520final%250Adesigns%2520and%2520MSE%252C%2520finding%2520that%2520the%2520correlation%2520is%2520really%2520very%2520weak.%2520In%2520contrast%252C%250Aa%2520metric%2520that%2520measures%2520order-maintaining%2520quality%2520shows%2520a%2520significantly%2520stronger%250Acorrelation.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520learning%2520a%2520ranking-based%250Amodel%2520that%2520leverages%2520learning%2520to%2520rank%2520techniques%2520to%2520prioritize%2520promising%250Adesigns%2520based%2520on%2520their%2520relative%2520scores.%2520We%2520show%2520that%2520the%2520generalization%2520error%250Aon%2520ranking%2520loss%2520can%2520be%2520well%2520bounded.%2520Empirical%2520results%2520across%2520diverse%2520tasks%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520our%2520proposed%2520ranking-based%2520models%2520than%250Atwenty%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11502v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Offline%20Model-Based%20Optimization%20by%20Learning%20to%20Rank&entry.906535625=Rong-Xi%20Tan%20and%20Ke%20Xue%20and%20Shen-Huan%20Lyu%20and%20Haopu%20Shang%20and%20Yao%20Wang%20and%20Yaoyuan%20Wang%20and%20Sheng%20Fu%20and%20Chao%20Qian&entry.1292438233=%20%20Offline%20model-based%20optimization%20%28MBO%29%20aims%20to%20identify%20a%20design%20that%0Amaximizes%20a%20black-box%20function%20using%20only%20a%20fixed%2C%20pre-collected%20dataset%20of%0Adesigns%20and%20their%20corresponding%20scores.%20A%20common%20approach%20in%20offline%20MBO%20is%20to%0Atrain%20a%20regression-based%20surrogate%20model%20by%20minimizing%20mean%20squared%20error%20%28MSE%29%0Aand%20then%20find%20the%20best%20design%20within%20this%20surrogate%20model%20by%20different%0Aoptimizers%20%28e.g.%2C%20gradient%20ascent%29.%20However%2C%20a%20critical%20challenge%20is%20the%20risk%0Aof%20out-of-distribution%20errors%2C%20i.e.%2C%20the%20surrogate%20model%20may%20typically%0Aoverestimate%20the%20scores%20and%20mislead%20the%20optimizers%20into%20suboptimal%20regions.%0APrior%20works%20have%20attempted%20to%20address%20this%20issue%20in%20various%20ways%2C%20such%20as%20using%0Aregularization%20techniques%20and%20ensemble%20learning%20to%20enhance%20the%20robustness%20of%0Athe%20model%2C%20but%20it%20still%20remains.%20In%20this%20paper%2C%20we%20argue%20that%20regression%20models%0Atrained%20with%20MSE%20are%20not%20well-aligned%20with%20the%20primary%20goal%20of%20offline%20MBO%2C%0Awhich%20is%20to%20select%20promising%20designs%20rather%20than%20to%20predict%20their%20scores%0Aprecisely.%20Notably%2C%20if%20a%20surrogate%20model%20can%20maintain%20the%20order%20of%20candidate%0Adesigns%20based%20on%20their%20relative%20score%20relationships%2C%20it%20can%20produce%20the%20best%0Adesigns%20even%20without%20precise%20predictions.%20To%20validate%20it%2C%20we%20conduct%0Aexperiments%20to%20compare%20the%20relationship%20between%20the%20quality%20of%20the%20final%0Adesigns%20and%20MSE%2C%20finding%20that%20the%20correlation%20is%20really%20very%20weak.%20In%20contrast%2C%0Aa%20metric%20that%20measures%20order-maintaining%20quality%20shows%20a%20significantly%20stronger%0Acorrelation.%20Based%20on%20this%20observation%2C%20we%20propose%20learning%20a%20ranking-based%0Amodel%20that%20leverages%20learning%20to%20rank%20techniques%20to%20prioritize%20promising%0Adesigns%20based%20on%20their%20relative%20scores.%20We%20show%20that%20the%20generalization%20error%0Aon%20ranking%20loss%20can%20be%20well%20bounded.%20Empirical%20results%20across%20diverse%20tasks%0Ademonstrate%20the%20superior%20performance%20of%20our%20proposed%20ranking-based%20models%20than%0Atwenty%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11502v3&entry.124074799=Read"},
{"title": "Computational, Data-Driven, and Physics-Informed Machine Learning\n  Approaches for Microstructure Modeling in Metal Additive Manufacturing", "author": "D. Patel and R. Sharma and Y. B. Guo", "abstract": "  Metal additive manufacturing enables unprecedented design freedom and the\nproduction of customized, complex components. However, the rapid melting and\nsolidification dynamics inherent to metal AM processes generate heterogeneous,\nnon-equilibrium microstructures that significantly impact mechanical properties\nand subsequent functionality. Predicting microstructure and its evolution\nacross spatial and temporal scales remains a central challenge for process\noptimization and defect mitigation. While conventional experimental techniques\nand physics-based simulations provide a physical foundation and valuable\ninsights, they face critical limitations. In contrast, data-driven machine\nlearning offers an alternative prediction approach and powerful pattern\nrecognition but often operate as black-box, lacking generalizability and\nphysical consistency. To overcome these limitations, physics-informed machine\nlearning, including physics-informed neural networks, has emerged as a\npromising paradigm by embedding governing physical laws into neural network\narchitectures, thereby enhancing accuracy, transparency, data efficiency, and\nextrapolation capabilities. This work presents a comprehensive evaluation of\nmodeling strategies for microstructure prediction in metal AM. The strengths\nand limitations of experimental, computational, and data-driven methods are\nanalyzed in depth, and highlight recent advances in hybrid PIML frameworks that\nintegrate physical knowledge with ML. Key challenges, such as data scarcity,\nmulti-scale coupling, and uncertainty quantification, are discussed alongside\nfuture directions. Ultimately, this assessment underscores the importance of\nPIML-based hybrid approaches in enabling predictive, scalable, and physically\nconsistent microstructure modeling for site-specific, microstructure-aware\nprocess control and the reliable production of high-performance AM components.\n", "link": "http://arxiv.org/abs/2505.01424v1", "date": "2025-05-02", "relevancy": 1.4432, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5034}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4917}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computational%2C%20Data-Driven%2C%20and%20Physics-Informed%20Machine%20Learning%0A%20%20Approaches%20for%20Microstructure%20Modeling%20in%20Metal%20Additive%20Manufacturing&body=Title%3A%20Computational%2C%20Data-Driven%2C%20and%20Physics-Informed%20Machine%20Learning%0A%20%20Approaches%20for%20Microstructure%20Modeling%20in%20Metal%20Additive%20Manufacturing%0AAuthor%3A%20D.%20Patel%20and%20R.%20Sharma%20and%20Y.%20B.%20Guo%0AAbstract%3A%20%20%20Metal%20additive%20manufacturing%20enables%20unprecedented%20design%20freedom%20and%20the%0Aproduction%20of%20customized%2C%20complex%20components.%20However%2C%20the%20rapid%20melting%20and%0Asolidification%20dynamics%20inherent%20to%20metal%20AM%20processes%20generate%20heterogeneous%2C%0Anon-equilibrium%20microstructures%20that%20significantly%20impact%20mechanical%20properties%0Aand%20subsequent%20functionality.%20Predicting%20microstructure%20and%20its%20evolution%0Aacross%20spatial%20and%20temporal%20scales%20remains%20a%20central%20challenge%20for%20process%0Aoptimization%20and%20defect%20mitigation.%20While%20conventional%20experimental%20techniques%0Aand%20physics-based%20simulations%20provide%20a%20physical%20foundation%20and%20valuable%0Ainsights%2C%20they%20face%20critical%20limitations.%20In%20contrast%2C%20data-driven%20machine%0Alearning%20offers%20an%20alternative%20prediction%20approach%20and%20powerful%20pattern%0Arecognition%20but%20often%20operate%20as%20black-box%2C%20lacking%20generalizability%20and%0Aphysical%20consistency.%20To%20overcome%20these%20limitations%2C%20physics-informed%20machine%0Alearning%2C%20including%20physics-informed%20neural%20networks%2C%20has%20emerged%20as%20a%0Apromising%20paradigm%20by%20embedding%20governing%20physical%20laws%20into%20neural%20network%0Aarchitectures%2C%20thereby%20enhancing%20accuracy%2C%20transparency%2C%20data%20efficiency%2C%20and%0Aextrapolation%20capabilities.%20This%20work%20presents%20a%20comprehensive%20evaluation%20of%0Amodeling%20strategies%20for%20microstructure%20prediction%20in%20metal%20AM.%20The%20strengths%0Aand%20limitations%20of%20experimental%2C%20computational%2C%20and%20data-driven%20methods%20are%0Aanalyzed%20in%20depth%2C%20and%20highlight%20recent%20advances%20in%20hybrid%20PIML%20frameworks%20that%0Aintegrate%20physical%20knowledge%20with%20ML.%20Key%20challenges%2C%20such%20as%20data%20scarcity%2C%0Amulti-scale%20coupling%2C%20and%20uncertainty%20quantification%2C%20are%20discussed%20alongside%0Afuture%20directions.%20Ultimately%2C%20this%20assessment%20underscores%20the%20importance%20of%0APIML-based%20hybrid%20approaches%20in%20enabling%20predictive%2C%20scalable%2C%20and%20physically%0Aconsistent%20microstructure%20modeling%20for%20site-specific%2C%20microstructure-aware%0Aprocess%20control%20and%20the%20reliable%20production%20of%20high-performance%20AM%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputational%252C%2520Data-Driven%252C%2520and%2520Physics-Informed%2520Machine%2520Learning%250A%2520%2520Approaches%2520for%2520Microstructure%2520Modeling%2520in%2520Metal%2520Additive%2520Manufacturing%26entry.906535625%3DD.%2520Patel%2520and%2520R.%2520Sharma%2520and%2520Y.%2520B.%2520Guo%26entry.1292438233%3D%2520%2520Metal%2520additive%2520manufacturing%2520enables%2520unprecedented%2520design%2520freedom%2520and%2520the%250Aproduction%2520of%2520customized%252C%2520complex%2520components.%2520However%252C%2520the%2520rapid%2520melting%2520and%250Asolidification%2520dynamics%2520inherent%2520to%2520metal%2520AM%2520processes%2520generate%2520heterogeneous%252C%250Anon-equilibrium%2520microstructures%2520that%2520significantly%2520impact%2520mechanical%2520properties%250Aand%2520subsequent%2520functionality.%2520Predicting%2520microstructure%2520and%2520its%2520evolution%250Aacross%2520spatial%2520and%2520temporal%2520scales%2520remains%2520a%2520central%2520challenge%2520for%2520process%250Aoptimization%2520and%2520defect%2520mitigation.%2520While%2520conventional%2520experimental%2520techniques%250Aand%2520physics-based%2520simulations%2520provide%2520a%2520physical%2520foundation%2520and%2520valuable%250Ainsights%252C%2520they%2520face%2520critical%2520limitations.%2520In%2520contrast%252C%2520data-driven%2520machine%250Alearning%2520offers%2520an%2520alternative%2520prediction%2520approach%2520and%2520powerful%2520pattern%250Arecognition%2520but%2520often%2520operate%2520as%2520black-box%252C%2520lacking%2520generalizability%2520and%250Aphysical%2520consistency.%2520To%2520overcome%2520these%2520limitations%252C%2520physics-informed%2520machine%250Alearning%252C%2520including%2520physics-informed%2520neural%2520networks%252C%2520has%2520emerged%2520as%2520a%250Apromising%2520paradigm%2520by%2520embedding%2520governing%2520physical%2520laws%2520into%2520neural%2520network%250Aarchitectures%252C%2520thereby%2520enhancing%2520accuracy%252C%2520transparency%252C%2520data%2520efficiency%252C%2520and%250Aextrapolation%2520capabilities.%2520This%2520work%2520presents%2520a%2520comprehensive%2520evaluation%2520of%250Amodeling%2520strategies%2520for%2520microstructure%2520prediction%2520in%2520metal%2520AM.%2520The%2520strengths%250Aand%2520limitations%2520of%2520experimental%252C%2520computational%252C%2520and%2520data-driven%2520methods%2520are%250Aanalyzed%2520in%2520depth%252C%2520and%2520highlight%2520recent%2520advances%2520in%2520hybrid%2520PIML%2520frameworks%2520that%250Aintegrate%2520physical%2520knowledge%2520with%2520ML.%2520Key%2520challenges%252C%2520such%2520as%2520data%2520scarcity%252C%250Amulti-scale%2520coupling%252C%2520and%2520uncertainty%2520quantification%252C%2520are%2520discussed%2520alongside%250Afuture%2520directions.%2520Ultimately%252C%2520this%2520assessment%2520underscores%2520the%2520importance%2520of%250APIML-based%2520hybrid%2520approaches%2520in%2520enabling%2520predictive%252C%2520scalable%252C%2520and%2520physically%250Aconsistent%2520microstructure%2520modeling%2520for%2520site-specific%252C%2520microstructure-aware%250Aprocess%2520control%2520and%2520the%2520reliable%2520production%2520of%2520high-performance%2520AM%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%2C%20Data-Driven%2C%20and%20Physics-Informed%20Machine%20Learning%0A%20%20Approaches%20for%20Microstructure%20Modeling%20in%20Metal%20Additive%20Manufacturing&entry.906535625=D.%20Patel%20and%20R.%20Sharma%20and%20Y.%20B.%20Guo&entry.1292438233=%20%20Metal%20additive%20manufacturing%20enables%20unprecedented%20design%20freedom%20and%20the%0Aproduction%20of%20customized%2C%20complex%20components.%20However%2C%20the%20rapid%20melting%20and%0Asolidification%20dynamics%20inherent%20to%20metal%20AM%20processes%20generate%20heterogeneous%2C%0Anon-equilibrium%20microstructures%20that%20significantly%20impact%20mechanical%20properties%0Aand%20subsequent%20functionality.%20Predicting%20microstructure%20and%20its%20evolution%0Aacross%20spatial%20and%20temporal%20scales%20remains%20a%20central%20challenge%20for%20process%0Aoptimization%20and%20defect%20mitigation.%20While%20conventional%20experimental%20techniques%0Aand%20physics-based%20simulations%20provide%20a%20physical%20foundation%20and%20valuable%0Ainsights%2C%20they%20face%20critical%20limitations.%20In%20contrast%2C%20data-driven%20machine%0Alearning%20offers%20an%20alternative%20prediction%20approach%20and%20powerful%20pattern%0Arecognition%20but%20often%20operate%20as%20black-box%2C%20lacking%20generalizability%20and%0Aphysical%20consistency.%20To%20overcome%20these%20limitations%2C%20physics-informed%20machine%0Alearning%2C%20including%20physics-informed%20neural%20networks%2C%20has%20emerged%20as%20a%0Apromising%20paradigm%20by%20embedding%20governing%20physical%20laws%20into%20neural%20network%0Aarchitectures%2C%20thereby%20enhancing%20accuracy%2C%20transparency%2C%20data%20efficiency%2C%20and%0Aextrapolation%20capabilities.%20This%20work%20presents%20a%20comprehensive%20evaluation%20of%0Amodeling%20strategies%20for%20microstructure%20prediction%20in%20metal%20AM.%20The%20strengths%0Aand%20limitations%20of%20experimental%2C%20computational%2C%20and%20data-driven%20methods%20are%0Aanalyzed%20in%20depth%2C%20and%20highlight%20recent%20advances%20in%20hybrid%20PIML%20frameworks%20that%0Aintegrate%20physical%20knowledge%20with%20ML.%20Key%20challenges%2C%20such%20as%20data%20scarcity%2C%0Amulti-scale%20coupling%2C%20and%20uncertainty%20quantification%2C%20are%20discussed%20alongside%0Afuture%20directions.%20Ultimately%2C%20this%20assessment%20underscores%20the%20importance%20of%0APIML-based%20hybrid%20approaches%20in%20enabling%20predictive%2C%20scalable%2C%20and%20physically%0Aconsistent%20microstructure%20modeling%20for%20site-specific%2C%20microstructure-aware%0Aprocess%20control%20and%20the%20reliable%20production%20of%20high-performance%20AM%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01424v1&entry.124074799=Read"},
{"title": "BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in\n  Multi-modal Model Editing", "author": "Dongliang Guo and Mengxuan Hu and Zihan Guan and Thomas Hartvigsen and Sheng Li", "abstract": "  Large multi-modal models inevitably decay over time as facts change and\npreviously learned information becomes outdated. Traditional approaches such as\nfine-tuning are often impractical for updating these models due to their size\nand complexity. Instead, direct knowledge editing within the models presents a\nmore viable solution. Current model editing techniques, however, typically\noverlook the unique influence ranges of different facts, leading to compromised\nmodel performance in terms of both generality and locality. To address this\nissue, we introduce the concept of the generality-locality trade-off in\nmulti-modal model editing. We develop a new model editing dataset named OKEDIT,\nspecifically designed to effectively evaluate this trade-off. Building on this\nfoundation, we propose BalancEdit, a novel method for balanced model editing\nthat dynamically achieves an optimal balance between generality and locality.\nBalancEdit utilizes a unique mechanism that generates both positive and\nnegative samples for each fact to accurately determine its influence scope and\nincorporates these insights into the model's latent space using a discrete,\nlocalized codebook of edits, without modifying the underlying model weights. To\nour knowledge, this is the first approach explicitly addressing the\ngenerality-locality trade-off in multi-modal model editing. Our comprehensive\nresults confirm the effectiveness of BalancEdit, demonstrating minimal\ntrade-offs while maintaining robust editing capabilities. Our code and dataset\nwill be available.\n", "link": "http://arxiv.org/abs/2505.01343v1", "date": "2025-05-02", "relevancy": 1.0895, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5801}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BalancEdit%3A%20Dynamically%20Balancing%20the%20Generality-Locality%20Trade-off%20in%0A%20%20Multi-modal%20Model%20Editing&body=Title%3A%20BalancEdit%3A%20Dynamically%20Balancing%20the%20Generality-Locality%20Trade-off%20in%0A%20%20Multi-modal%20Model%20Editing%0AAuthor%3A%20Dongliang%20Guo%20and%20Mengxuan%20Hu%20and%20Zihan%20Guan%20and%20Thomas%20Hartvigsen%20and%20Sheng%20Li%0AAbstract%3A%20%20%20Large%20multi-modal%20models%20inevitably%20decay%20over%20time%20as%20facts%20change%20and%0Apreviously%20learned%20information%20becomes%20outdated.%20Traditional%20approaches%20such%20as%0Afine-tuning%20are%20often%20impractical%20for%20updating%20these%20models%20due%20to%20their%20size%0Aand%20complexity.%20Instead%2C%20direct%20knowledge%20editing%20within%20the%20models%20presents%20a%0Amore%20viable%20solution.%20Current%20model%20editing%20techniques%2C%20however%2C%20typically%0Aoverlook%20the%20unique%20influence%20ranges%20of%20different%20facts%2C%20leading%20to%20compromised%0Amodel%20performance%20in%20terms%20of%20both%20generality%20and%20locality.%20To%20address%20this%0Aissue%2C%20we%20introduce%20the%20concept%20of%20the%20generality-locality%20trade-off%20in%0Amulti-modal%20model%20editing.%20We%20develop%20a%20new%20model%20editing%20dataset%20named%20OKEDIT%2C%0Aspecifically%20designed%20to%20effectively%20evaluate%20this%20trade-off.%20Building%20on%20this%0Afoundation%2C%20we%20propose%20BalancEdit%2C%20a%20novel%20method%20for%20balanced%20model%20editing%0Athat%20dynamically%20achieves%20an%20optimal%20balance%20between%20generality%20and%20locality.%0ABalancEdit%20utilizes%20a%20unique%20mechanism%20that%20generates%20both%20positive%20and%0Anegative%20samples%20for%20each%20fact%20to%20accurately%20determine%20its%20influence%20scope%20and%0Aincorporates%20these%20insights%20into%20the%20model%27s%20latent%20space%20using%20a%20discrete%2C%0Alocalized%20codebook%20of%20edits%2C%20without%20modifying%20the%20underlying%20model%20weights.%20To%0Aour%20knowledge%2C%20this%20is%20the%20first%20approach%20explicitly%20addressing%20the%0Agenerality-locality%20trade-off%20in%20multi-modal%20model%20editing.%20Our%20comprehensive%0Aresults%20confirm%20the%20effectiveness%20of%20BalancEdit%2C%20demonstrating%20minimal%0Atrade-offs%20while%20maintaining%20robust%20editing%20capabilities.%20Our%20code%20and%20dataset%0Awill%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancEdit%253A%2520Dynamically%2520Balancing%2520the%2520Generality-Locality%2520Trade-off%2520in%250A%2520%2520Multi-modal%2520Model%2520Editing%26entry.906535625%3DDongliang%2520Guo%2520and%2520Mengxuan%2520Hu%2520and%2520Zihan%2520Guan%2520and%2520Thomas%2520Hartvigsen%2520and%2520Sheng%2520Li%26entry.1292438233%3D%2520%2520Large%2520multi-modal%2520models%2520inevitably%2520decay%2520over%2520time%2520as%2520facts%2520change%2520and%250Apreviously%2520learned%2520information%2520becomes%2520outdated.%2520Traditional%2520approaches%2520such%2520as%250Afine-tuning%2520are%2520often%2520impractical%2520for%2520updating%2520these%2520models%2520due%2520to%2520their%2520size%250Aand%2520complexity.%2520Instead%252C%2520direct%2520knowledge%2520editing%2520within%2520the%2520models%2520presents%2520a%250Amore%2520viable%2520solution.%2520Current%2520model%2520editing%2520techniques%252C%2520however%252C%2520typically%250Aoverlook%2520the%2520unique%2520influence%2520ranges%2520of%2520different%2520facts%252C%2520leading%2520to%2520compromised%250Amodel%2520performance%2520in%2520terms%2520of%2520both%2520generality%2520and%2520locality.%2520To%2520address%2520this%250Aissue%252C%2520we%2520introduce%2520the%2520concept%2520of%2520the%2520generality-locality%2520trade-off%2520in%250Amulti-modal%2520model%2520editing.%2520We%2520develop%2520a%2520new%2520model%2520editing%2520dataset%2520named%2520OKEDIT%252C%250Aspecifically%2520designed%2520to%2520effectively%2520evaluate%2520this%2520trade-off.%2520Building%2520on%2520this%250Afoundation%252C%2520we%2520propose%2520BalancEdit%252C%2520a%2520novel%2520method%2520for%2520balanced%2520model%2520editing%250Athat%2520dynamically%2520achieves%2520an%2520optimal%2520balance%2520between%2520generality%2520and%2520locality.%250ABalancEdit%2520utilizes%2520a%2520unique%2520mechanism%2520that%2520generates%2520both%2520positive%2520and%250Anegative%2520samples%2520for%2520each%2520fact%2520to%2520accurately%2520determine%2520its%2520influence%2520scope%2520and%250Aincorporates%2520these%2520insights%2520into%2520the%2520model%2527s%2520latent%2520space%2520using%2520a%2520discrete%252C%250Alocalized%2520codebook%2520of%2520edits%252C%2520without%2520modifying%2520the%2520underlying%2520model%2520weights.%2520To%250Aour%2520knowledge%252C%2520this%2520is%2520the%2520first%2520approach%2520explicitly%2520addressing%2520the%250Agenerality-locality%2520trade-off%2520in%2520multi-modal%2520model%2520editing.%2520Our%2520comprehensive%250Aresults%2520confirm%2520the%2520effectiveness%2520of%2520BalancEdit%252C%2520demonstrating%2520minimal%250Atrade-offs%2520while%2520maintaining%2520robust%2520editing%2520capabilities.%2520Our%2520code%2520and%2520dataset%250Awill%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BalancEdit%3A%20Dynamically%20Balancing%20the%20Generality-Locality%20Trade-off%20in%0A%20%20Multi-modal%20Model%20Editing&entry.906535625=Dongliang%20Guo%20and%20Mengxuan%20Hu%20and%20Zihan%20Guan%20and%20Thomas%20Hartvigsen%20and%20Sheng%20Li&entry.1292438233=%20%20Large%20multi-modal%20models%20inevitably%20decay%20over%20time%20as%20facts%20change%20and%0Apreviously%20learned%20information%20becomes%20outdated.%20Traditional%20approaches%20such%20as%0Afine-tuning%20are%20often%20impractical%20for%20updating%20these%20models%20due%20to%20their%20size%0Aand%20complexity.%20Instead%2C%20direct%20knowledge%20editing%20within%20the%20models%20presents%20a%0Amore%20viable%20solution.%20Current%20model%20editing%20techniques%2C%20however%2C%20typically%0Aoverlook%20the%20unique%20influence%20ranges%20of%20different%20facts%2C%20leading%20to%20compromised%0Amodel%20performance%20in%20terms%20of%20both%20generality%20and%20locality.%20To%20address%20this%0Aissue%2C%20we%20introduce%20the%20concept%20of%20the%20generality-locality%20trade-off%20in%0Amulti-modal%20model%20editing.%20We%20develop%20a%20new%20model%20editing%20dataset%20named%20OKEDIT%2C%0Aspecifically%20designed%20to%20effectively%20evaluate%20this%20trade-off.%20Building%20on%20this%0Afoundation%2C%20we%20propose%20BalancEdit%2C%20a%20novel%20method%20for%20balanced%20model%20editing%0Athat%20dynamically%20achieves%20an%20optimal%20balance%20between%20generality%20and%20locality.%0ABalancEdit%20utilizes%20a%20unique%20mechanism%20that%20generates%20both%20positive%20and%0Anegative%20samples%20for%20each%20fact%20to%20accurately%20determine%20its%20influence%20scope%20and%0Aincorporates%20these%20insights%20into%20the%20model%27s%20latent%20space%20using%20a%20discrete%2C%0Alocalized%20codebook%20of%20edits%2C%20without%20modifying%20the%20underlying%20model%20weights.%20To%0Aour%20knowledge%2C%20this%20is%20the%20first%20approach%20explicitly%20addressing%20the%0Agenerality-locality%20trade-off%20in%20multi-modal%20model%20editing.%20Our%20comprehensive%0Aresults%20confirm%20the%20effectiveness%20of%20BalancEdit%2C%20demonstrating%20minimal%0Atrade-offs%20while%20maintaining%20robust%20editing%20capabilities.%20Our%20code%20and%20dataset%0Awill%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01343v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


