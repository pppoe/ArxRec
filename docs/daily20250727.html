<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250724.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head\n  Avatars", "author": "Rui-Yang Ju and Sheng-Yen Huang and Yi-Ping Hung", "abstract": "  The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based method, has become widely used for facial image stylization. To\nextend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian\nblendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1\n(stylized video generation), we adopt an improved StyleGAN to generate the\nstylized video from the input video frames, which overcomes the limitation of\ncropping aligned faces at a fixed resolution as preprocessing for normal\nStyleGAN. This process provides a more stable stylized video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, facilitating the synthesis of high-quality animations in the next\nstage. In Stage 2 (Gaussian blendshapes synthesis), our method learns a\nstylized neutral head model and a set of expression blendshapes from the\ngenerated stylized video. By combining the neutral head model with expression\nblendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary\nexpressions. We validate the effectiveness of ToonifyGB on benchmark datasets\nusing two representative styles: Arcane and Pixar.\n", "link": "http://arxiv.org/abs/2505.10072v2", "date": "2025-07-24", "relevancy": 3.6069, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7882}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7882}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToonifyGB%3A%20StyleGAN-based%20Gaussian%20Blendshapes%20for%203D%20Stylized%20Head%0A%20%20Avatars&body=Title%3A%20ToonifyGB%3A%20StyleGAN-based%20Gaussian%20Blendshapes%20for%203D%20Stylized%20Head%0A%20%20Avatars%0AAuthor%3A%20Rui-Yang%20Ju%20and%20Sheng-Yen%20Huang%20and%20Yi-Ping%20Hung%0AAbstract%3A%20%20%20The%20introduction%20of%203D%20Gaussian%20blendshapes%20has%20enabled%20the%20real-time%0Areconstruction%20of%20animatable%20head%20avatars%20from%20monocular%20video.%20Toonify%2C%20a%0AStyleGAN-based%20method%2C%20has%20become%20widely%20used%20for%20facial%20image%20stylization.%20To%0Aextend%20Toonify%20for%20synthesizing%20diverse%20stylized%203D%20head%20avatars%20using%20Gaussian%0Ablendshapes%2C%20we%20propose%20an%20efficient%20two-stage%20framework%2C%20ToonifyGB.%20In%20Stage%201%0A%28stylized%20video%20generation%29%2C%20we%20adopt%20an%20improved%20StyleGAN%20to%20generate%20the%0Astylized%20video%20from%20the%20input%20video%20frames%2C%20which%20overcomes%20the%20limitation%20of%0Acropping%20aligned%20faces%20at%20a%20fixed%20resolution%20as%20preprocessing%20for%20normal%0AStyleGAN.%20This%20process%20provides%20a%20more%20stable%20stylized%20video%2C%20which%20enables%0AGaussian%20blendshapes%20to%20better%20capture%20the%20high-frequency%20details%20of%20the%20video%0Aframes%2C%20facilitating%20the%20synthesis%20of%20high-quality%20animations%20in%20the%20next%0Astage.%20In%20Stage%202%20%28Gaussian%20blendshapes%20synthesis%29%2C%20our%20method%20learns%20a%0Astylized%20neutral%20head%20model%20and%20a%20set%20of%20expression%20blendshapes%20from%20the%0Agenerated%20stylized%20video.%20By%20combining%20the%20neutral%20head%20model%20with%20expression%0Ablendshapes%2C%20ToonifyGB%20can%20efficiently%20render%20stylized%20avatars%20with%20arbitrary%0Aexpressions.%20We%20validate%20the%20effectiveness%20of%20ToonifyGB%20on%20benchmark%20datasets%0Ausing%20two%20representative%20styles%3A%20Arcane%20and%20Pixar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10072v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToonifyGB%253A%2520StyleGAN-based%2520Gaussian%2520Blendshapes%2520for%25203D%2520Stylized%2520Head%250A%2520%2520Avatars%26entry.906535625%3DRui-Yang%2520Ju%2520and%2520Sheng-Yen%2520Huang%2520and%2520Yi-Ping%2520Hung%26entry.1292438233%3D%2520%2520The%2520introduction%2520of%25203D%2520Gaussian%2520blendshapes%2520has%2520enabled%2520the%2520real-time%250Areconstruction%2520of%2520animatable%2520head%2520avatars%2520from%2520monocular%2520video.%2520Toonify%252C%2520a%250AStyleGAN-based%2520method%252C%2520has%2520become%2520widely%2520used%2520for%2520facial%2520image%2520stylization.%2520To%250Aextend%2520Toonify%2520for%2520synthesizing%2520diverse%2520stylized%25203D%2520head%2520avatars%2520using%2520Gaussian%250Ablendshapes%252C%2520we%2520propose%2520an%2520efficient%2520two-stage%2520framework%252C%2520ToonifyGB.%2520In%2520Stage%25201%250A%2528stylized%2520video%2520generation%2529%252C%2520we%2520adopt%2520an%2520improved%2520StyleGAN%2520to%2520generate%2520the%250Astylized%2520video%2520from%2520the%2520input%2520video%2520frames%252C%2520which%2520overcomes%2520the%2520limitation%2520of%250Acropping%2520aligned%2520faces%2520at%2520a%2520fixed%2520resolution%2520as%2520preprocessing%2520for%2520normal%250AStyleGAN.%2520This%2520process%2520provides%2520a%2520more%2520stable%2520stylized%2520video%252C%2520which%2520enables%250AGaussian%2520blendshapes%2520to%2520better%2520capture%2520the%2520high-frequency%2520details%2520of%2520the%2520video%250Aframes%252C%2520facilitating%2520the%2520synthesis%2520of%2520high-quality%2520animations%2520in%2520the%2520next%250Astage.%2520In%2520Stage%25202%2520%2528Gaussian%2520blendshapes%2520synthesis%2529%252C%2520our%2520method%2520learns%2520a%250Astylized%2520neutral%2520head%2520model%2520and%2520a%2520set%2520of%2520expression%2520blendshapes%2520from%2520the%250Agenerated%2520stylized%2520video.%2520By%2520combining%2520the%2520neutral%2520head%2520model%2520with%2520expression%250Ablendshapes%252C%2520ToonifyGB%2520can%2520efficiently%2520render%2520stylized%2520avatars%2520with%2520arbitrary%250Aexpressions.%2520We%2520validate%2520the%2520effectiveness%2520of%2520ToonifyGB%2520on%2520benchmark%2520datasets%250Ausing%2520two%2520representative%2520styles%253A%2520Arcane%2520and%2520Pixar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10072v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToonifyGB%3A%20StyleGAN-based%20Gaussian%20Blendshapes%20for%203D%20Stylized%20Head%0A%20%20Avatars&entry.906535625=Rui-Yang%20Ju%20and%20Sheng-Yen%20Huang%20and%20Yi-Ping%20Hung&entry.1292438233=%20%20The%20introduction%20of%203D%20Gaussian%20blendshapes%20has%20enabled%20the%20real-time%0Areconstruction%20of%20animatable%20head%20avatars%20from%20monocular%20video.%20Toonify%2C%20a%0AStyleGAN-based%20method%2C%20has%20become%20widely%20used%20for%20facial%20image%20stylization.%20To%0Aextend%20Toonify%20for%20synthesizing%20diverse%20stylized%203D%20head%20avatars%20using%20Gaussian%0Ablendshapes%2C%20we%20propose%20an%20efficient%20two-stage%20framework%2C%20ToonifyGB.%20In%20Stage%201%0A%28stylized%20video%20generation%29%2C%20we%20adopt%20an%20improved%20StyleGAN%20to%20generate%20the%0Astylized%20video%20from%20the%20input%20video%20frames%2C%20which%20overcomes%20the%20limitation%20of%0Acropping%20aligned%20faces%20at%20a%20fixed%20resolution%20as%20preprocessing%20for%20normal%0AStyleGAN.%20This%20process%20provides%20a%20more%20stable%20stylized%20video%2C%20which%20enables%0AGaussian%20blendshapes%20to%20better%20capture%20the%20high-frequency%20details%20of%20the%20video%0Aframes%2C%20facilitating%20the%20synthesis%20of%20high-quality%20animations%20in%20the%20next%0Astage.%20In%20Stage%202%20%28Gaussian%20blendshapes%20synthesis%29%2C%20our%20method%20learns%20a%0Astylized%20neutral%20head%20model%20and%20a%20set%20of%20expression%20blendshapes%20from%20the%0Agenerated%20stylized%20video.%20By%20combining%20the%20neutral%20head%20model%20with%20expression%0Ablendshapes%2C%20ToonifyGB%20can%20efficiently%20render%20stylized%20avatars%20with%20arbitrary%0Aexpressions.%20We%20validate%20the%20effectiveness%20of%20ToonifyGB%20on%20benchmark%20datasets%0Ausing%20two%20representative%20styles%3A%20Arcane%20and%20Pixar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10072v2&entry.124074799=Read"},
{"title": "Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping", "author": "Chong Cheng and Zijian Wang and Sicheng Yu and Yu Hu and Nanjie Yao and Hao Wang", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D\nrepresentation. Its effectiveness largely depends on precise camera poses and\naccurate point cloud initialization, which are often derived from pretrained\nMulti-View Stereo (MVS) models. However, in unposed reconstruction task from\nhundreds of outdoor images, existing MVS models may struggle with memory limits\nand lose accuracy as the number of input images grows. To address this\nlimitation, we propose a novel unposed 3DGS reconstruction framework that\nintegrates pretrained MVS priors with the probabilistic Procrustes mapping\nstrategy. The method partitions input images into subsets, maps submaps into a\nglobal space, and jointly optimizes geometry and poses with 3DGS. Technically,\nwe formulate the mapping of tens of millions of point clouds as a probabilistic\nProcrustes problem and solve a closed-form alignment. By employing\nprobabilistic coupling along with a soft dustbin mechanism to reject uncertain\ncorrespondences, our method globally aligns point clouds and poses within\nminutes across hundreds of images. Moreover, we propose a joint optimization\nframework for 3DGS and camera poses. It constructs Gaussians from\nconfidence-aware anchor points and integrates 3DGS differentiable rendering\nwith an analytical Jacobian to jointly refine scene and poses, enabling\naccurate reconstruction and pose estimation. Experiments on Waymo and KITTI\ndatasets show that our method achieves accurate reconstruction from unposed\nimage sequences, setting a new state of the art for unposed 3DGS\nreconstruction.\n", "link": "http://arxiv.org/abs/2507.18541v1", "date": "2025-07-24", "relevancy": 3.5274, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7308}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7015}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unposed%203DGS%20Reconstruction%20with%20Probabilistic%20Procrustes%20Mapping&body=Title%3A%20Unposed%203DGS%20Reconstruction%20with%20Probabilistic%20Procrustes%20Mapping%0AAuthor%3A%20Chong%20Cheng%20and%20Zijian%20Wang%20and%20Sicheng%20Yu%20and%20Yu%20Hu%20and%20Nanjie%20Yao%20and%20Hao%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20core%20technique%20for%203D%0Arepresentation.%20Its%20effectiveness%20largely%20depends%20on%20precise%20camera%20poses%20and%0Aaccurate%20point%20cloud%20initialization%2C%20which%20are%20often%20derived%20from%20pretrained%0AMulti-View%20Stereo%20%28MVS%29%20models.%20However%2C%20in%20unposed%20reconstruction%20task%20from%0Ahundreds%20of%20outdoor%20images%2C%20existing%20MVS%20models%20may%20struggle%20with%20memory%20limits%0Aand%20lose%20accuracy%20as%20the%20number%20of%20input%20images%20grows.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20unposed%203DGS%20reconstruction%20framework%20that%0Aintegrates%20pretrained%20MVS%20priors%20with%20the%20probabilistic%20Procrustes%20mapping%0Astrategy.%20The%20method%20partitions%20input%20images%20into%20subsets%2C%20maps%20submaps%20into%20a%0Aglobal%20space%2C%20and%20jointly%20optimizes%20geometry%20and%20poses%20with%203DGS.%20Technically%2C%0Awe%20formulate%20the%20mapping%20of%20tens%20of%20millions%20of%20point%20clouds%20as%20a%20probabilistic%0AProcrustes%20problem%20and%20solve%20a%20closed-form%20alignment.%20By%20employing%0Aprobabilistic%20coupling%20along%20with%20a%20soft%20dustbin%20mechanism%20to%20reject%20uncertain%0Acorrespondences%2C%20our%20method%20globally%20aligns%20point%20clouds%20and%20poses%20within%0Aminutes%20across%20hundreds%20of%20images.%20Moreover%2C%20we%20propose%20a%20joint%20optimization%0Aframework%20for%203DGS%20and%20camera%20poses.%20It%20constructs%20Gaussians%20from%0Aconfidence-aware%20anchor%20points%20and%20integrates%203DGS%20differentiable%20rendering%0Awith%20an%20analytical%20Jacobian%20to%20jointly%20refine%20scene%20and%20poses%2C%20enabling%0Aaccurate%20reconstruction%20and%20pose%20estimation.%20Experiments%20on%20Waymo%20and%20KITTI%0Adatasets%20show%20that%20our%20method%20achieves%20accurate%20reconstruction%20from%20unposed%0Aimage%20sequences%2C%20setting%20a%20new%20state%20of%20the%20art%20for%20unposed%203DGS%0Areconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnposed%25203DGS%2520Reconstruction%2520with%2520Probabilistic%2520Procrustes%2520Mapping%26entry.906535625%3DChong%2520Cheng%2520and%2520Zijian%2520Wang%2520and%2520Sicheng%2520Yu%2520and%2520Yu%2520Hu%2520and%2520Nanjie%2520Yao%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520core%2520technique%2520for%25203D%250Arepresentation.%2520Its%2520effectiveness%2520largely%2520depends%2520on%2520precise%2520camera%2520poses%2520and%250Aaccurate%2520point%2520cloud%2520initialization%252C%2520which%2520are%2520often%2520derived%2520from%2520pretrained%250AMulti-View%2520Stereo%2520%2528MVS%2529%2520models.%2520However%252C%2520in%2520unposed%2520reconstruction%2520task%2520from%250Ahundreds%2520of%2520outdoor%2520images%252C%2520existing%2520MVS%2520models%2520may%2520struggle%2520with%2520memory%2520limits%250Aand%2520lose%2520accuracy%2520as%2520the%2520number%2520of%2520input%2520images%2520grows.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520novel%2520unposed%25203DGS%2520reconstruction%2520framework%2520that%250Aintegrates%2520pretrained%2520MVS%2520priors%2520with%2520the%2520probabilistic%2520Procrustes%2520mapping%250Astrategy.%2520The%2520method%2520partitions%2520input%2520images%2520into%2520subsets%252C%2520maps%2520submaps%2520into%2520a%250Aglobal%2520space%252C%2520and%2520jointly%2520optimizes%2520geometry%2520and%2520poses%2520with%25203DGS.%2520Technically%252C%250Awe%2520formulate%2520the%2520mapping%2520of%2520tens%2520of%2520millions%2520of%2520point%2520clouds%2520as%2520a%2520probabilistic%250AProcrustes%2520problem%2520and%2520solve%2520a%2520closed-form%2520alignment.%2520By%2520employing%250Aprobabilistic%2520coupling%2520along%2520with%2520a%2520soft%2520dustbin%2520mechanism%2520to%2520reject%2520uncertain%250Acorrespondences%252C%2520our%2520method%2520globally%2520aligns%2520point%2520clouds%2520and%2520poses%2520within%250Aminutes%2520across%2520hundreds%2520of%2520images.%2520Moreover%252C%2520we%2520propose%2520a%2520joint%2520optimization%250Aframework%2520for%25203DGS%2520and%2520camera%2520poses.%2520It%2520constructs%2520Gaussians%2520from%250Aconfidence-aware%2520anchor%2520points%2520and%2520integrates%25203DGS%2520differentiable%2520rendering%250Awith%2520an%2520analytical%2520Jacobian%2520to%2520jointly%2520refine%2520scene%2520and%2520poses%252C%2520enabling%250Aaccurate%2520reconstruction%2520and%2520pose%2520estimation.%2520Experiments%2520on%2520Waymo%2520and%2520KITTI%250Adatasets%2520show%2520that%2520our%2520method%2520achieves%2520accurate%2520reconstruction%2520from%2520unposed%250Aimage%2520sequences%252C%2520setting%2520a%2520new%2520state%2520of%2520the%2520art%2520for%2520unposed%25203DGS%250Areconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unposed%203DGS%20Reconstruction%20with%20Probabilistic%20Procrustes%20Mapping&entry.906535625=Chong%20Cheng%20and%20Zijian%20Wang%20and%20Sicheng%20Yu%20and%20Yu%20Hu%20and%20Nanjie%20Yao%20and%20Hao%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20core%20technique%20for%203D%0Arepresentation.%20Its%20effectiveness%20largely%20depends%20on%20precise%20camera%20poses%20and%0Aaccurate%20point%20cloud%20initialization%2C%20which%20are%20often%20derived%20from%20pretrained%0AMulti-View%20Stereo%20%28MVS%29%20models.%20However%2C%20in%20unposed%20reconstruction%20task%20from%0Ahundreds%20of%20outdoor%20images%2C%20existing%20MVS%20models%20may%20struggle%20with%20memory%20limits%0Aand%20lose%20accuracy%20as%20the%20number%20of%20input%20images%20grows.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20unposed%203DGS%20reconstruction%20framework%20that%0Aintegrates%20pretrained%20MVS%20priors%20with%20the%20probabilistic%20Procrustes%20mapping%0Astrategy.%20The%20method%20partitions%20input%20images%20into%20subsets%2C%20maps%20submaps%20into%20a%0Aglobal%20space%2C%20and%20jointly%20optimizes%20geometry%20and%20poses%20with%203DGS.%20Technically%2C%0Awe%20formulate%20the%20mapping%20of%20tens%20of%20millions%20of%20point%20clouds%20as%20a%20probabilistic%0AProcrustes%20problem%20and%20solve%20a%20closed-form%20alignment.%20By%20employing%0Aprobabilistic%20coupling%20along%20with%20a%20soft%20dustbin%20mechanism%20to%20reject%20uncertain%0Acorrespondences%2C%20our%20method%20globally%20aligns%20point%20clouds%20and%20poses%20within%0Aminutes%20across%20hundreds%20of%20images.%20Moreover%2C%20we%20propose%20a%20joint%20optimization%0Aframework%20for%203DGS%20and%20camera%20poses.%20It%20constructs%20Gaussians%20from%0Aconfidence-aware%20anchor%20points%20and%20integrates%203DGS%20differentiable%20rendering%0Awith%20an%20analytical%20Jacobian%20to%20jointly%20refine%20scene%20and%20poses%2C%20enabling%0Aaccurate%20reconstruction%20and%20pose%20estimation.%20Experiments%20on%20Waymo%20and%20KITTI%0Adatasets%20show%20that%20our%20method%20achieves%20accurate%20reconstruction%20from%20unposed%0Aimage%20sequences%2C%20setting%20a%20new%20state%20of%20the%20art%20for%20unposed%203DGS%0Areconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18541v1&entry.124074799=Read"},
{"title": "Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian\n  Pointmaps", "author": "Chong Cheng and Sicheng Yu and Zijian Wang and Yifan Zhou and Hao Wang", "abstract": "  3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its\nhigh-fidelity and real-time novel view synthesis performance. However, some\nprevious 3DGS SLAM methods employ a differentiable rendering pipeline for\ntracking, lack geometric priors in outdoor scenes. Other approaches introduce\nseparate tracking modules, but they accumulate errors with significant camera\nmovement, leading to scale drift. To address these challenges, we propose a\nrobust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a\nself-consistent tracking module anchored in the 3DGS pointmap, which avoids\ncumulative scale drift and achieves more precise and robust tracking with fewer\niterations. Additionally, we design a patch-based pointmap dynamic mapping\nmodule, which introduces geometric priors while avoiding scale ambiguity. This\nsignificantly enhances tracking accuracy and the quality of scene\nreconstruction, making it particularly suitable for complex outdoor\nenvironments. Our experiments on the Waymo, KITTI, and DL3DV datasets\ndemonstrate that S3PO-GS achieves state-of-the-art results in novel view\nsynthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project\npage: https://3dagentworld.github.io/S3PO-GS/.\n", "link": "http://arxiv.org/abs/2507.03737v2", "date": "2025-07-24", "relevancy": 3.4594, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7626}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6677}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Outdoor%20Monocular%20SLAM%20with%20Global%20Scale-Consistent%203D%20Gaussian%0A%20%20Pointmaps&body=Title%3A%20Outdoor%20Monocular%20SLAM%20with%20Global%20Scale-Consistent%203D%20Gaussian%0A%20%20Pointmaps%0AAuthor%3A%20Chong%20Cheng%20and%20Sicheng%20Yu%20and%20Zijian%20Wang%20and%20Yifan%20Zhou%20and%20Hao%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20a%20popular%20solution%20in%20SLAM%20due%20to%20its%0Ahigh-fidelity%20and%20real-time%20novel%20view%20synthesis%20performance.%20However%2C%20some%0Aprevious%203DGS%20SLAM%20methods%20employ%20a%20differentiable%20rendering%20pipeline%20for%0Atracking%2C%20lack%20geometric%20priors%20in%20outdoor%20scenes.%20Other%20approaches%20introduce%0Aseparate%20tracking%20modules%2C%20but%20they%20accumulate%20errors%20with%20significant%20camera%0Amovement%2C%20leading%20to%20scale%20drift.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Arobust%20RGB-only%20outdoor%203DGS%20SLAM%20method%3A%20S3PO-GS.%20Technically%2C%20we%20establish%20a%0Aself-consistent%20tracking%20module%20anchored%20in%20the%203DGS%20pointmap%2C%20which%20avoids%0Acumulative%20scale%20drift%20and%20achieves%20more%20precise%20and%20robust%20tracking%20with%20fewer%0Aiterations.%20Additionally%2C%20we%20design%20a%20patch-based%20pointmap%20dynamic%20mapping%0Amodule%2C%20which%20introduces%20geometric%20priors%20while%20avoiding%20scale%20ambiguity.%20This%0Asignificantly%20enhances%20tracking%20accuracy%20and%20the%20quality%20of%20scene%0Areconstruction%2C%20making%20it%20particularly%20suitable%20for%20complex%20outdoor%0Aenvironments.%20Our%20experiments%20on%20the%20Waymo%2C%20KITTI%2C%20and%20DL3DV%20datasets%0Ademonstrate%20that%20S3PO-GS%20achieves%20state-of-the-art%20results%20in%20novel%20view%0Asynthesis%20and%20outperforms%20other%203DGS%20SLAM%20methods%20in%20tracking%20accuracy.%20Project%0Apage%3A%20https%3A//3dagentworld.github.io/S3PO-GS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOutdoor%2520Monocular%2520SLAM%2520with%2520Global%2520Scale-Consistent%25203D%2520Gaussian%250A%2520%2520Pointmaps%26entry.906535625%3DChong%2520Cheng%2520and%2520Sicheng%2520Yu%2520and%2520Zijian%2520Wang%2520and%2520Yifan%2520Zhou%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520become%2520a%2520popular%2520solution%2520in%2520SLAM%2520due%2520to%2520its%250Ahigh-fidelity%2520and%2520real-time%2520novel%2520view%2520synthesis%2520performance.%2520However%252C%2520some%250Aprevious%25203DGS%2520SLAM%2520methods%2520employ%2520a%2520differentiable%2520rendering%2520pipeline%2520for%250Atracking%252C%2520lack%2520geometric%2520priors%2520in%2520outdoor%2520scenes.%2520Other%2520approaches%2520introduce%250Aseparate%2520tracking%2520modules%252C%2520but%2520they%2520accumulate%2520errors%2520with%2520significant%2520camera%250Amovement%252C%2520leading%2520to%2520scale%2520drift.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Arobust%2520RGB-only%2520outdoor%25203DGS%2520SLAM%2520method%253A%2520S3PO-GS.%2520Technically%252C%2520we%2520establish%2520a%250Aself-consistent%2520tracking%2520module%2520anchored%2520in%2520the%25203DGS%2520pointmap%252C%2520which%2520avoids%250Acumulative%2520scale%2520drift%2520and%2520achieves%2520more%2520precise%2520and%2520robust%2520tracking%2520with%2520fewer%250Aiterations.%2520Additionally%252C%2520we%2520design%2520a%2520patch-based%2520pointmap%2520dynamic%2520mapping%250Amodule%252C%2520which%2520introduces%2520geometric%2520priors%2520while%2520avoiding%2520scale%2520ambiguity.%2520This%250Asignificantly%2520enhances%2520tracking%2520accuracy%2520and%2520the%2520quality%2520of%2520scene%250Areconstruction%252C%2520making%2520it%2520particularly%2520suitable%2520for%2520complex%2520outdoor%250Aenvironments.%2520Our%2520experiments%2520on%2520the%2520Waymo%252C%2520KITTI%252C%2520and%2520DL3DV%2520datasets%250Ademonstrate%2520that%2520S3PO-GS%2520achieves%2520state-of-the-art%2520results%2520in%2520novel%2520view%250Asynthesis%2520and%2520outperforms%2520other%25203DGS%2520SLAM%2520methods%2520in%2520tracking%2520accuracy.%2520Project%250Apage%253A%2520https%253A//3dagentworld.github.io/S3PO-GS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Outdoor%20Monocular%20SLAM%20with%20Global%20Scale-Consistent%203D%20Gaussian%0A%20%20Pointmaps&entry.906535625=Chong%20Cheng%20and%20Sicheng%20Yu%20and%20Zijian%20Wang%20and%20Yifan%20Zhou%20and%20Hao%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20a%20popular%20solution%20in%20SLAM%20due%20to%20its%0Ahigh-fidelity%20and%20real-time%20novel%20view%20synthesis%20performance.%20However%2C%20some%0Aprevious%203DGS%20SLAM%20methods%20employ%20a%20differentiable%20rendering%20pipeline%20for%0Atracking%2C%20lack%20geometric%20priors%20in%20outdoor%20scenes.%20Other%20approaches%20introduce%0Aseparate%20tracking%20modules%2C%20but%20they%20accumulate%20errors%20with%20significant%20camera%0Amovement%2C%20leading%20to%20scale%20drift.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Arobust%20RGB-only%20outdoor%203DGS%20SLAM%20method%3A%20S3PO-GS.%20Technically%2C%20we%20establish%20a%0Aself-consistent%20tracking%20module%20anchored%20in%20the%203DGS%20pointmap%2C%20which%20avoids%0Acumulative%20scale%20drift%20and%20achieves%20more%20precise%20and%20robust%20tracking%20with%20fewer%0Aiterations.%20Additionally%2C%20we%20design%20a%20patch-based%20pointmap%20dynamic%20mapping%0Amodule%2C%20which%20introduces%20geometric%20priors%20while%20avoiding%20scale%20ambiguity.%20This%0Asignificantly%20enhances%20tracking%20accuracy%20and%20the%20quality%20of%20scene%0Areconstruction%2C%20making%20it%20particularly%20suitable%20for%20complex%20outdoor%0Aenvironments.%20Our%20experiments%20on%20the%20Waymo%2C%20KITTI%2C%20and%20DL3DV%20datasets%0Ademonstrate%20that%20S3PO-GS%20achieves%20state-of-the-art%20results%20in%20novel%20view%0Asynthesis%20and%20outperforms%20other%203DGS%20SLAM%20methods%20in%20tracking%20accuracy.%20Project%0Apage%3A%20https%3A//3dagentworld.github.io/S3PO-GS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03737v2&entry.124074799=Read"},
{"title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D\n  Content Creation from a Single Image", "author": "Xiaotian Chen and DongFu Yin and Fei Richard Yu and Xuanchen Li and Xinhao Zhang", "abstract": "  Advances in generative modeling have significantly enhanced digital content\ncreation, extending from 2D images to complex 3D and 4D scenes. Despite\nsubstantial progress, producing high-fidelity and temporally consistent dynamic\n4D content remains a challenge. In this paper, we propose MVG4D, a novel\nframework that generates dynamic 4D content from a single still image by\ncombining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,\nMVG4D employs an image matrix module that synthesizes temporally coherent and\nspatially diverse multi-view images, providing rich supervisory signals for\ndownstream 3D and 4D reconstruction. These multi-view images are used to\noptimize a 3D Gaussian point cloud, which is further extended into the temporal\ndomain via a lightweight deformation network. Our method effectively enhances\ntemporal consistency, geometric fidelity, and visual realism, addressing key\nchallenges in motion discontinuity and background degradation that affect prior\n4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate\nthat MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and\ntime efficiency. Notably, it reduces flickering artifacts and sharpens\nstructural details across views and time, enabling more immersive AR/VR\nexperiences. MVG4D sets a new direction for efficient and controllable 4D\ngeneration from minimal inputs.\n", "link": "http://arxiv.org/abs/2507.18371v1", "date": "2025-07-24", "relevancy": 3.4163, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6976}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVG4D%3A%20Image%20Matrix-Based%20Multi-View%20and%20Motion%20Generation%20for%204D%0A%20%20Content%20Creation%20from%20a%20Single%20Image&body=Title%3A%20MVG4D%3A%20Image%20Matrix-Based%20Multi-View%20and%20Motion%20Generation%20for%204D%0A%20%20Content%20Creation%20from%20a%20Single%20Image%0AAuthor%3A%20Xiaotian%20Chen%20and%20DongFu%20Yin%20and%20Fei%20Richard%20Yu%20and%20Xuanchen%20Li%20and%20Xinhao%20Zhang%0AAbstract%3A%20%20%20Advances%20in%20generative%20modeling%20have%20significantly%20enhanced%20digital%20content%0Acreation%2C%20extending%20from%202D%20images%20to%20complex%203D%20and%204D%20scenes.%20Despite%0Asubstantial%20progress%2C%20producing%20high-fidelity%20and%20temporally%20consistent%20dynamic%0A4D%20content%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20propose%20MVG4D%2C%20a%20novel%0Aframework%20that%20generates%20dynamic%204D%20content%20from%20a%20single%20still%20image%20by%0Acombining%20multi-view%20synthesis%20with%204D%20Gaussian%20Splatting%20%284D%20GS%29.%20At%20its%20core%2C%0AMVG4D%20employs%20an%20image%20matrix%20module%20that%20synthesizes%20temporally%20coherent%20and%0Aspatially%20diverse%20multi-view%20images%2C%20providing%20rich%20supervisory%20signals%20for%0Adownstream%203D%20and%204D%20reconstruction.%20These%20multi-view%20images%20are%20used%20to%0Aoptimize%20a%203D%20Gaussian%20point%20cloud%2C%20which%20is%20further%20extended%20into%20the%20temporal%0Adomain%20via%20a%20lightweight%20deformation%20network.%20Our%20method%20effectively%20enhances%0Atemporal%20consistency%2C%20geometric%20fidelity%2C%20and%20visual%20realism%2C%20addressing%20key%0Achallenges%20in%20motion%20discontinuity%20and%20background%20degradation%20that%20affect%20prior%0A4D%20GS-based%20methods.%20Extensive%20experiments%20on%20the%20Objaverse%20dataset%20demonstrate%0Athat%20MVG4D%20outperforms%20state-of-the-art%20baselines%20in%20CLIP-I%2C%20PSNR%2C%20FVD%2C%20and%0Atime%20efficiency.%20Notably%2C%20it%20reduces%20flickering%20artifacts%20and%20sharpens%0Astructural%20details%20across%20views%20and%20time%2C%20enabling%20more%20immersive%20AR/VR%0Aexperiences.%20MVG4D%20sets%20a%20new%20direction%20for%20efficient%20and%20controllable%204D%0Ageneration%20from%20minimal%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVG4D%253A%2520Image%2520Matrix-Based%2520Multi-View%2520and%2520Motion%2520Generation%2520for%25204D%250A%2520%2520Content%2520Creation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DXiaotian%2520Chen%2520and%2520DongFu%2520Yin%2520and%2520Fei%2520Richard%2520Yu%2520and%2520Xuanchen%2520Li%2520and%2520Xinhao%2520Zhang%26entry.1292438233%3D%2520%2520Advances%2520in%2520generative%2520modeling%2520have%2520significantly%2520enhanced%2520digital%2520content%250Acreation%252C%2520extending%2520from%25202D%2520images%2520to%2520complex%25203D%2520and%25204D%2520scenes.%2520Despite%250Asubstantial%2520progress%252C%2520producing%2520high-fidelity%2520and%2520temporally%2520consistent%2520dynamic%250A4D%2520content%2520remains%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MVG4D%252C%2520a%2520novel%250Aframework%2520that%2520generates%2520dynamic%25204D%2520content%2520from%2520a%2520single%2520still%2520image%2520by%250Acombining%2520multi-view%2520synthesis%2520with%25204D%2520Gaussian%2520Splatting%2520%25284D%2520GS%2529.%2520At%2520its%2520core%252C%250AMVG4D%2520employs%2520an%2520image%2520matrix%2520module%2520that%2520synthesizes%2520temporally%2520coherent%2520and%250Aspatially%2520diverse%2520multi-view%2520images%252C%2520providing%2520rich%2520supervisory%2520signals%2520for%250Adownstream%25203D%2520and%25204D%2520reconstruction.%2520These%2520multi-view%2520images%2520are%2520used%2520to%250Aoptimize%2520a%25203D%2520Gaussian%2520point%2520cloud%252C%2520which%2520is%2520further%2520extended%2520into%2520the%2520temporal%250Adomain%2520via%2520a%2520lightweight%2520deformation%2520network.%2520Our%2520method%2520effectively%2520enhances%250Atemporal%2520consistency%252C%2520geometric%2520fidelity%252C%2520and%2520visual%2520realism%252C%2520addressing%2520key%250Achallenges%2520in%2520motion%2520discontinuity%2520and%2520background%2520degradation%2520that%2520affect%2520prior%250A4D%2520GS-based%2520methods.%2520Extensive%2520experiments%2520on%2520the%2520Objaverse%2520dataset%2520demonstrate%250Athat%2520MVG4D%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520CLIP-I%252C%2520PSNR%252C%2520FVD%252C%2520and%250Atime%2520efficiency.%2520Notably%252C%2520it%2520reduces%2520flickering%2520artifacts%2520and%2520sharpens%250Astructural%2520details%2520across%2520views%2520and%2520time%252C%2520enabling%2520more%2520immersive%2520AR/VR%250Aexperiences.%2520MVG4D%2520sets%2520a%2520new%2520direction%2520for%2520efficient%2520and%2520controllable%25204D%250Ageneration%2520from%2520minimal%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVG4D%3A%20Image%20Matrix-Based%20Multi-View%20and%20Motion%20Generation%20for%204D%0A%20%20Content%20Creation%20from%20a%20Single%20Image&entry.906535625=Xiaotian%20Chen%20and%20DongFu%20Yin%20and%20Fei%20Richard%20Yu%20and%20Xuanchen%20Li%20and%20Xinhao%20Zhang&entry.1292438233=%20%20Advances%20in%20generative%20modeling%20have%20significantly%20enhanced%20digital%20content%0Acreation%2C%20extending%20from%202D%20images%20to%20complex%203D%20and%204D%20scenes.%20Despite%0Asubstantial%20progress%2C%20producing%20high-fidelity%20and%20temporally%20consistent%20dynamic%0A4D%20content%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20propose%20MVG4D%2C%20a%20novel%0Aframework%20that%20generates%20dynamic%204D%20content%20from%20a%20single%20still%20image%20by%0Acombining%20multi-view%20synthesis%20with%204D%20Gaussian%20Splatting%20%284D%20GS%29.%20At%20its%20core%2C%0AMVG4D%20employs%20an%20image%20matrix%20module%20that%20synthesizes%20temporally%20coherent%20and%0Aspatially%20diverse%20multi-view%20images%2C%20providing%20rich%20supervisory%20signals%20for%0Adownstream%203D%20and%204D%20reconstruction.%20These%20multi-view%20images%20are%20used%20to%0Aoptimize%20a%203D%20Gaussian%20point%20cloud%2C%20which%20is%20further%20extended%20into%20the%20temporal%0Adomain%20via%20a%20lightweight%20deformation%20network.%20Our%20method%20effectively%20enhances%0Atemporal%20consistency%2C%20geometric%20fidelity%2C%20and%20visual%20realism%2C%20addressing%20key%0Achallenges%20in%20motion%20discontinuity%20and%20background%20degradation%20that%20affect%20prior%0A4D%20GS-based%20methods.%20Extensive%20experiments%20on%20the%20Objaverse%20dataset%20demonstrate%0Athat%20MVG4D%20outperforms%20state-of-the-art%20baselines%20in%20CLIP-I%2C%20PSNR%2C%20FVD%2C%20and%0Atime%20efficiency.%20Notably%2C%20it%20reduces%20flickering%20artifacts%20and%20sharpens%0Astructural%20details%20across%20views%20and%20time%2C%20enabling%20more%20immersive%20AR/VR%0Aexperiences.%20MVG4D%20sets%20a%20new%20direction%20for%20efficient%20and%20controllable%204D%0Ageneration%20from%20minimal%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18371v1&entry.124074799=Read"},
{"title": "G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM", "author": "Gyuhyeon Pak and Hae Min Cho and Euntai Kim", "abstract": "  In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting\nSLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D\nreconstruction and robust camera pose tracking in real-time by representing\neach scene element using a Gaussian distribution constrained to the local\ntangent plane. This effectively models the local surface as a 2D Gaussian disk\naligned with the underlying geometry, leading to more consistent depth\ninterpretation across multiple viewpoints compared to conventional 3D\nellipsoid-based representations with isotropic uncertainty. To integrate this\nrepresentation into the SLAM pipeline, we embed the surface-aligned Gaussian\ndisks into a Generalized ICP framework by introducing anisotropic covariance\nprior without altering the underlying registration formulation. Furthermore we\npropose a geometry-aware loss that supervises photometric, depth, and normal\nconsistency. Our system achieves real-time operation while preserving both\nvisual and geometric fidelity. Extensive experiments on the Replica and\nTUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems\nin terms of localization accuracy, reconstruction completeness, while\nmaintaining the rendering quality.\n", "link": "http://arxiv.org/abs/2507.18344v1", "date": "2025-07-24", "relevancy": 3.3883, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7905}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6366}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G2S-ICP%20SLAM%3A%20Geometry-aware%20Gaussian%20Splatting%20ICP%20SLAM&body=Title%3A%20G2S-ICP%20SLAM%3A%20Geometry-aware%20Gaussian%20Splatting%20ICP%20SLAM%0AAuthor%3A%20Gyuhyeon%20Pak%20and%20Hae%20Min%20Cho%20and%20Euntai%20Kim%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20geometry-aware%20RGB-D%20Gaussian%20Splatting%0ASLAM%20system%2C%20named%20G2S-ICP%20SLAM.%20The%20proposed%20method%20performs%20high-fidelity%203D%0Areconstruction%20and%20robust%20camera%20pose%20tracking%20in%20real-time%20by%20representing%0Aeach%20scene%20element%20using%20a%20Gaussian%20distribution%20constrained%20to%20the%20local%0Atangent%20plane.%20This%20effectively%20models%20the%20local%20surface%20as%20a%202D%20Gaussian%20disk%0Aaligned%20with%20the%20underlying%20geometry%2C%20leading%20to%20more%20consistent%20depth%0Ainterpretation%20across%20multiple%20viewpoints%20compared%20to%20conventional%203D%0Aellipsoid-based%20representations%20with%20isotropic%20uncertainty.%20To%20integrate%20this%0Arepresentation%20into%20the%20SLAM%20pipeline%2C%20we%20embed%20the%20surface-aligned%20Gaussian%0Adisks%20into%20a%20Generalized%20ICP%20framework%20by%20introducing%20anisotropic%20covariance%0Aprior%20without%20altering%20the%20underlying%20registration%20formulation.%20Furthermore%20we%0Apropose%20a%20geometry-aware%20loss%20that%20supervises%20photometric%2C%20depth%2C%20and%20normal%0Aconsistency.%20Our%20system%20achieves%20real-time%20operation%20while%20preserving%20both%0Avisual%20and%20geometric%20fidelity.%20Extensive%20experiments%20on%20the%20Replica%20and%0ATUM-RGBD%20datasets%20demonstrate%20that%20G2S-ICP%20SLAM%20outperforms%20prior%20SLAM%20systems%0Ain%20terms%20of%20localization%20accuracy%2C%20reconstruction%20completeness%2C%20while%0Amaintaining%20the%20rendering%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG2S-ICP%2520SLAM%253A%2520Geometry-aware%2520Gaussian%2520Splatting%2520ICP%2520SLAM%26entry.906535625%3DGyuhyeon%2520Pak%2520and%2520Hae%2520Min%2520Cho%2520and%2520Euntai%2520Kim%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520geometry-aware%2520RGB-D%2520Gaussian%2520Splatting%250ASLAM%2520system%252C%2520named%2520G2S-ICP%2520SLAM.%2520The%2520proposed%2520method%2520performs%2520high-fidelity%25203D%250Areconstruction%2520and%2520robust%2520camera%2520pose%2520tracking%2520in%2520real-time%2520by%2520representing%250Aeach%2520scene%2520element%2520using%2520a%2520Gaussian%2520distribution%2520constrained%2520to%2520the%2520local%250Atangent%2520plane.%2520This%2520effectively%2520models%2520the%2520local%2520surface%2520as%2520a%25202D%2520Gaussian%2520disk%250Aaligned%2520with%2520the%2520underlying%2520geometry%252C%2520leading%2520to%2520more%2520consistent%2520depth%250Ainterpretation%2520across%2520multiple%2520viewpoints%2520compared%2520to%2520conventional%25203D%250Aellipsoid-based%2520representations%2520with%2520isotropic%2520uncertainty.%2520To%2520integrate%2520this%250Arepresentation%2520into%2520the%2520SLAM%2520pipeline%252C%2520we%2520embed%2520the%2520surface-aligned%2520Gaussian%250Adisks%2520into%2520a%2520Generalized%2520ICP%2520framework%2520by%2520introducing%2520anisotropic%2520covariance%250Aprior%2520without%2520altering%2520the%2520underlying%2520registration%2520formulation.%2520Furthermore%2520we%250Apropose%2520a%2520geometry-aware%2520loss%2520that%2520supervises%2520photometric%252C%2520depth%252C%2520and%2520normal%250Aconsistency.%2520Our%2520system%2520achieves%2520real-time%2520operation%2520while%2520preserving%2520both%250Avisual%2520and%2520geometric%2520fidelity.%2520Extensive%2520experiments%2520on%2520the%2520Replica%2520and%250ATUM-RGBD%2520datasets%2520demonstrate%2520that%2520G2S-ICP%2520SLAM%2520outperforms%2520prior%2520SLAM%2520systems%250Ain%2520terms%2520of%2520localization%2520accuracy%252C%2520reconstruction%2520completeness%252C%2520while%250Amaintaining%2520the%2520rendering%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G2S-ICP%20SLAM%3A%20Geometry-aware%20Gaussian%20Splatting%20ICP%20SLAM&entry.906535625=Gyuhyeon%20Pak%20and%20Hae%20Min%20Cho%20and%20Euntai%20Kim&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20geometry-aware%20RGB-D%20Gaussian%20Splatting%0ASLAM%20system%2C%20named%20G2S-ICP%20SLAM.%20The%20proposed%20method%20performs%20high-fidelity%203D%0Areconstruction%20and%20robust%20camera%20pose%20tracking%20in%20real-time%20by%20representing%0Aeach%20scene%20element%20using%20a%20Gaussian%20distribution%20constrained%20to%20the%20local%0Atangent%20plane.%20This%20effectively%20models%20the%20local%20surface%20as%20a%202D%20Gaussian%20disk%0Aaligned%20with%20the%20underlying%20geometry%2C%20leading%20to%20more%20consistent%20depth%0Ainterpretation%20across%20multiple%20viewpoints%20compared%20to%20conventional%203D%0Aellipsoid-based%20representations%20with%20isotropic%20uncertainty.%20To%20integrate%20this%0Arepresentation%20into%20the%20SLAM%20pipeline%2C%20we%20embed%20the%20surface-aligned%20Gaussian%0Adisks%20into%20a%20Generalized%20ICP%20framework%20by%20introducing%20anisotropic%20covariance%0Aprior%20without%20altering%20the%20underlying%20registration%20formulation.%20Furthermore%20we%0Apropose%20a%20geometry-aware%20loss%20that%20supervises%20photometric%2C%20depth%2C%20and%20normal%0Aconsistency.%20Our%20system%20achieves%20real-time%20operation%20while%20preserving%20both%0Avisual%20and%20geometric%20fidelity.%20Extensive%20experiments%20on%20the%20Replica%20and%0ATUM-RGBD%20datasets%20demonstrate%20that%20G2S-ICP%20SLAM%20outperforms%20prior%20SLAM%20systems%0Ain%20terms%20of%20localization%20accuracy%2C%20reconstruction%20completeness%2C%20while%0Amaintaining%20the%20rendering%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18344v1&entry.124074799=Read"},
{"title": "LONG3R: Long Sequence Streaming 3D Reconstruction", "author": "Zhuoguang Chen and Minghui Qin and Tianyuan Yuan and Zhe Liu and Hang Zhao", "abstract": "  Recent advancements in multi-view scene reconstruction have been significant,\nyet existing methods face limitations when processing streams of input images.\nThese methods either rely on time-consuming offline optimization or are\nrestricted to shorter sequences, hindering their applicability in real-time\nscenarios. In this work, we propose LONG3R (LOng sequence streaming 3D\nReconstruction), a novel model designed for streaming multi-view 3D scene\nreconstruction over longer sequences. Our model achieves real-time processing\nby operating recurrently, maintaining and updating memory with each new\nobservation. We first employ a memory gating mechanism to filter relevant\nmemory, which, together with a new observation, is fed into a dual-source\nrefined decoder for coarse-to-fine interaction. To effectively capture\nlong-sequence memory, we propose a 3D spatio-temporal memory that dynamically\nprunes redundant spatial information while adaptively adjusting resolution\nalong the scene. To enhance our model's performance on long sequences while\nmaintaining training efficiency, we employ a two-stage curriculum training\nstrategy, each stage targeting specific capabilities. Experiments demonstrate\nthat LONG3R outperforms state-of-the-art streaming methods, particularly for\nlonger sequences, while maintaining real-time inference speed. Project page:\nhttps://zgchen33.github.io/LONG3R/.\n", "link": "http://arxiv.org/abs/2507.18255v1", "date": "2025-07-24", "relevancy": 3.1552, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6443}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6244}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LONG3R%3A%20Long%20Sequence%20Streaming%203D%20Reconstruction&body=Title%3A%20LONG3R%3A%20Long%20Sequence%20Streaming%203D%20Reconstruction%0AAuthor%3A%20Zhuoguang%20Chen%20and%20Minghui%20Qin%20and%20Tianyuan%20Yuan%20and%20Zhe%20Liu%20and%20Hang%20Zhao%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multi-view%20scene%20reconstruction%20have%20been%20significant%2C%0Ayet%20existing%20methods%20face%20limitations%20when%20processing%20streams%20of%20input%20images.%0AThese%20methods%20either%20rely%20on%20time-consuming%20offline%20optimization%20or%20are%0Arestricted%20to%20shorter%20sequences%2C%20hindering%20their%20applicability%20in%20real-time%0Ascenarios.%20In%20this%20work%2C%20we%20propose%20LONG3R%20%28LOng%20sequence%20streaming%203D%0AReconstruction%29%2C%20a%20novel%20model%20designed%20for%20streaming%20multi-view%203D%20scene%0Areconstruction%20over%20longer%20sequences.%20Our%20model%20achieves%20real-time%20processing%0Aby%20operating%20recurrently%2C%20maintaining%20and%20updating%20memory%20with%20each%20new%0Aobservation.%20We%20first%20employ%20a%20memory%20gating%20mechanism%20to%20filter%20relevant%0Amemory%2C%20which%2C%20together%20with%20a%20new%20observation%2C%20is%20fed%20into%20a%20dual-source%0Arefined%20decoder%20for%20coarse-to-fine%20interaction.%20To%20effectively%20capture%0Along-sequence%20memory%2C%20we%20propose%20a%203D%20spatio-temporal%20memory%20that%20dynamically%0Aprunes%20redundant%20spatial%20information%20while%20adaptively%20adjusting%20resolution%0Aalong%20the%20scene.%20To%20enhance%20our%20model%27s%20performance%20on%20long%20sequences%20while%0Amaintaining%20training%20efficiency%2C%20we%20employ%20a%20two-stage%20curriculum%20training%0Astrategy%2C%20each%20stage%20targeting%20specific%20capabilities.%20Experiments%20demonstrate%0Athat%20LONG3R%20outperforms%20state-of-the-art%20streaming%20methods%2C%20particularly%20for%0Alonger%20sequences%2C%20while%20maintaining%20real-time%20inference%20speed.%20Project%20page%3A%0Ahttps%3A//zgchen33.github.io/LONG3R/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLONG3R%253A%2520Long%2520Sequence%2520Streaming%25203D%2520Reconstruction%26entry.906535625%3DZhuoguang%2520Chen%2520and%2520Minghui%2520Qin%2520and%2520Tianyuan%2520Yuan%2520and%2520Zhe%2520Liu%2520and%2520Hang%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multi-view%2520scene%2520reconstruction%2520have%2520been%2520significant%252C%250Ayet%2520existing%2520methods%2520face%2520limitations%2520when%2520processing%2520streams%2520of%2520input%2520images.%250AThese%2520methods%2520either%2520rely%2520on%2520time-consuming%2520offline%2520optimization%2520or%2520are%250Arestricted%2520to%2520shorter%2520sequences%252C%2520hindering%2520their%2520applicability%2520in%2520real-time%250Ascenarios.%2520In%2520this%2520work%252C%2520we%2520propose%2520LONG3R%2520%2528LOng%2520sequence%2520streaming%25203D%250AReconstruction%2529%252C%2520a%2520novel%2520model%2520designed%2520for%2520streaming%2520multi-view%25203D%2520scene%250Areconstruction%2520over%2520longer%2520sequences.%2520Our%2520model%2520achieves%2520real-time%2520processing%250Aby%2520operating%2520recurrently%252C%2520maintaining%2520and%2520updating%2520memory%2520with%2520each%2520new%250Aobservation.%2520We%2520first%2520employ%2520a%2520memory%2520gating%2520mechanism%2520to%2520filter%2520relevant%250Amemory%252C%2520which%252C%2520together%2520with%2520a%2520new%2520observation%252C%2520is%2520fed%2520into%2520a%2520dual-source%250Arefined%2520decoder%2520for%2520coarse-to-fine%2520interaction.%2520To%2520effectively%2520capture%250Along-sequence%2520memory%252C%2520we%2520propose%2520a%25203D%2520spatio-temporal%2520memory%2520that%2520dynamically%250Aprunes%2520redundant%2520spatial%2520information%2520while%2520adaptively%2520adjusting%2520resolution%250Aalong%2520the%2520scene.%2520To%2520enhance%2520our%2520model%2527s%2520performance%2520on%2520long%2520sequences%2520while%250Amaintaining%2520training%2520efficiency%252C%2520we%2520employ%2520a%2520two-stage%2520curriculum%2520training%250Astrategy%252C%2520each%2520stage%2520targeting%2520specific%2520capabilities.%2520Experiments%2520demonstrate%250Athat%2520LONG3R%2520outperforms%2520state-of-the-art%2520streaming%2520methods%252C%2520particularly%2520for%250Alonger%2520sequences%252C%2520while%2520maintaining%2520real-time%2520inference%2520speed.%2520Project%2520page%253A%250Ahttps%253A//zgchen33.github.io/LONG3R/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LONG3R%3A%20Long%20Sequence%20Streaming%203D%20Reconstruction&entry.906535625=Zhuoguang%20Chen%20and%20Minghui%20Qin%20and%20Tianyuan%20Yuan%20and%20Zhe%20Liu%20and%20Hang%20Zhao&entry.1292438233=%20%20Recent%20advancements%20in%20multi-view%20scene%20reconstruction%20have%20been%20significant%2C%0Ayet%20existing%20methods%20face%20limitations%20when%20processing%20streams%20of%20input%20images.%0AThese%20methods%20either%20rely%20on%20time-consuming%20offline%20optimization%20or%20are%0Arestricted%20to%20shorter%20sequences%2C%20hindering%20their%20applicability%20in%20real-time%0Ascenarios.%20In%20this%20work%2C%20we%20propose%20LONG3R%20%28LOng%20sequence%20streaming%203D%0AReconstruction%29%2C%20a%20novel%20model%20designed%20for%20streaming%20multi-view%203D%20scene%0Areconstruction%20over%20longer%20sequences.%20Our%20model%20achieves%20real-time%20processing%0Aby%20operating%20recurrently%2C%20maintaining%20and%20updating%20memory%20with%20each%20new%0Aobservation.%20We%20first%20employ%20a%20memory%20gating%20mechanism%20to%20filter%20relevant%0Amemory%2C%20which%2C%20together%20with%20a%20new%20observation%2C%20is%20fed%20into%20a%20dual-source%0Arefined%20decoder%20for%20coarse-to-fine%20interaction.%20To%20effectively%20capture%0Along-sequence%20memory%2C%20we%20propose%20a%203D%20spatio-temporal%20memory%20that%20dynamically%0Aprunes%20redundant%20spatial%20information%20while%20adaptively%20adjusting%20resolution%0Aalong%20the%20scene.%20To%20enhance%20our%20model%27s%20performance%20on%20long%20sequences%20while%0Amaintaining%20training%20efficiency%2C%20we%20employ%20a%20two-stage%20curriculum%20training%0Astrategy%2C%20each%20stage%20targeting%20specific%20capabilities.%20Experiments%20demonstrate%0Athat%20LONG3R%20outperforms%20state-of-the-art%20streaming%20methods%2C%20particularly%20for%0Alonger%20sequences%2C%20while%20maintaining%20real-time%20inference%20speed.%20Project%20page%3A%0Ahttps%3A//zgchen33.github.io/LONG3R/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18255v1&entry.124074799=Read"},
{"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "author": "Haiwen Diao and Xiaotong Li and Yufeng Cui and Yueze Wang and Haoge Deng and Ting Pan and Wenxuan Wang and Huchuan Lu and Xinlong Wang", "abstract": "  Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.\n", "link": "http://arxiv.org/abs/2502.06788v2", "date": "2025-07-24", "relevancy": 3.0231, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVEv2%3A%20Improved%20Baselines%20for%20Encoder-Free%20Vision-Language%20Models&body=Title%3A%20EVEv2%3A%20Improved%20Baselines%20for%20Encoder-Free%20Vision-Language%20Models%0AAuthor%3A%20Haiwen%20Diao%20and%20Xiaotong%20Li%20and%20Yufeng%20Cui%20and%20Yueze%20Wang%20and%20Haoge%20Deng%20and%20Ting%20Pan%20and%20Wenxuan%20Wang%20and%20Huchuan%20Lu%20and%20Xinlong%20Wang%0AAbstract%3A%20%20%20Existing%20encoder-free%20vision-language%20models%20%28VLMs%29%20are%20rapidly%20narrowing%20the%0Aperformance%20gap%20with%20their%20encoder-based%20counterparts%2C%20highlighting%20the%0Apromising%20potential%20for%20unified%20multimodal%20systems%20with%20structural%20simplicity%0Aand%20efficient%20deployment.%20We%20systematically%20clarify%20the%20performance%20gap%20between%0AVLMs%20using%20pre-trained%20vision%20encoders%2C%20discrete%20tokenizers%2C%20and%20minimalist%0Avisual%20layers%20from%20scratch%2C%20deeply%20excavating%20the%20under-examined%0Acharacteristics%20of%20encoder-free%20VLMs.%20We%20develop%20efficient%20strategies%20for%0Aencoder-free%20VLMs%20that%20rival%20mainstream%20encoder-based%20ones.%20After%20an%20in-depth%0Ainvestigation%2C%20we%20launch%20EVEv2.0%2C%20a%20new%20and%20improved%20family%20of%20encoder-free%0AVLMs.%20We%20show%20that%3A%20%28i%29%20Properly%20decomposing%20and%20hierarchically%20associating%0Avision%20and%20language%20within%20a%20unified%20model%20reduces%20interference%20between%0Amodalities.%20%28ii%29%20A%20well-designed%20training%20strategy%20enables%20effective%0Aoptimization%20for%20encoder-free%20VLMs.%20Through%20extensive%20evaluation%2C%20our%20EVEv2.0%0Arepresents%20a%20thorough%20study%20for%20developing%20a%20decoder-only%20architecture%20across%0Amodalities%2C%20demonstrating%20superior%20data%20efficiency%20and%20strong%20vision-reasoning%0Acapability.%20Code%20is%20publicly%20available%20at%3A%20https%3A//github.com/baaivision/EVE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVEv2%253A%2520Improved%2520Baselines%2520for%2520Encoder-Free%2520Vision-Language%2520Models%26entry.906535625%3DHaiwen%2520Diao%2520and%2520Xiaotong%2520Li%2520and%2520Yufeng%2520Cui%2520and%2520Yueze%2520Wang%2520and%2520Haoge%2520Deng%2520and%2520Ting%2520Pan%2520and%2520Wenxuan%2520Wang%2520and%2520Huchuan%2520Lu%2520and%2520Xinlong%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520encoder-free%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520rapidly%2520narrowing%2520the%250Aperformance%2520gap%2520with%2520their%2520encoder-based%2520counterparts%252C%2520highlighting%2520the%250Apromising%2520potential%2520for%2520unified%2520multimodal%2520systems%2520with%2520structural%2520simplicity%250Aand%2520efficient%2520deployment.%2520We%2520systematically%2520clarify%2520the%2520performance%2520gap%2520between%250AVLMs%2520using%2520pre-trained%2520vision%2520encoders%252C%2520discrete%2520tokenizers%252C%2520and%2520minimalist%250Avisual%2520layers%2520from%2520scratch%252C%2520deeply%2520excavating%2520the%2520under-examined%250Acharacteristics%2520of%2520encoder-free%2520VLMs.%2520We%2520develop%2520efficient%2520strategies%2520for%250Aencoder-free%2520VLMs%2520that%2520rival%2520mainstream%2520encoder-based%2520ones.%2520After%2520an%2520in-depth%250Ainvestigation%252C%2520we%2520launch%2520EVEv2.0%252C%2520a%2520new%2520and%2520improved%2520family%2520of%2520encoder-free%250AVLMs.%2520We%2520show%2520that%253A%2520%2528i%2529%2520Properly%2520decomposing%2520and%2520hierarchically%2520associating%250Avision%2520and%2520language%2520within%2520a%2520unified%2520model%2520reduces%2520interference%2520between%250Amodalities.%2520%2528ii%2529%2520A%2520well-designed%2520training%2520strategy%2520enables%2520effective%250Aoptimization%2520for%2520encoder-free%2520VLMs.%2520Through%2520extensive%2520evaluation%252C%2520our%2520EVEv2.0%250Arepresents%2520a%2520thorough%2520study%2520for%2520developing%2520a%2520decoder-only%2520architecture%2520across%250Amodalities%252C%2520demonstrating%2520superior%2520data%2520efficiency%2520and%2520strong%2520vision-reasoning%250Acapability.%2520Code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/baaivision/EVE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVEv2%3A%20Improved%20Baselines%20for%20Encoder-Free%20Vision-Language%20Models&entry.906535625=Haiwen%20Diao%20and%20Xiaotong%20Li%20and%20Yufeng%20Cui%20and%20Yueze%20Wang%20and%20Haoge%20Deng%20and%20Ting%20Pan%20and%20Wenxuan%20Wang%20and%20Huchuan%20Lu%20and%20Xinlong%20Wang&entry.1292438233=%20%20Existing%20encoder-free%20vision-language%20models%20%28VLMs%29%20are%20rapidly%20narrowing%20the%0Aperformance%20gap%20with%20their%20encoder-based%20counterparts%2C%20highlighting%20the%0Apromising%20potential%20for%20unified%20multimodal%20systems%20with%20structural%20simplicity%0Aand%20efficient%20deployment.%20We%20systematically%20clarify%20the%20performance%20gap%20between%0AVLMs%20using%20pre-trained%20vision%20encoders%2C%20discrete%20tokenizers%2C%20and%20minimalist%0Avisual%20layers%20from%20scratch%2C%20deeply%20excavating%20the%20under-examined%0Acharacteristics%20of%20encoder-free%20VLMs.%20We%20develop%20efficient%20strategies%20for%0Aencoder-free%20VLMs%20that%20rival%20mainstream%20encoder-based%20ones.%20After%20an%20in-depth%0Ainvestigation%2C%20we%20launch%20EVEv2.0%2C%20a%20new%20and%20improved%20family%20of%20encoder-free%0AVLMs.%20We%20show%20that%3A%20%28i%29%20Properly%20decomposing%20and%20hierarchically%20associating%0Avision%20and%20language%20within%20a%20unified%20model%20reduces%20interference%20between%0Amodalities.%20%28ii%29%20A%20well-designed%20training%20strategy%20enables%20effective%0Aoptimization%20for%20encoder-free%20VLMs.%20Through%20extensive%20evaluation%2C%20our%20EVEv2.0%0Arepresents%20a%20thorough%20study%20for%20developing%20a%20decoder-only%20architecture%20across%0Amodalities%2C%20demonstrating%20superior%20data%20efficiency%20and%20strong%20vision-reasoning%0Acapability.%20Code%20is%20publicly%20available%20at%3A%20https%3A//github.com/baaivision/EVE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06788v2&entry.124074799=Read"},
{"title": "Improving Large Vision-Language Models' Understanding for Field Data", "author": "Xiaomei Zhang and Hanyu Zheng and Xiangyu Zhu and Jinghuan Wei and Junhong Zou and Zhen Lei and Zhaoxiang Zhang", "abstract": "  Large Vision-Language Models (LVLMs) have shown impressive capabilities\nacross a range of tasks that integrate visual and textual understanding, such\nas image captioning and visual question answering. These models are trained on\nlarge-scale image and video datasets paired with text, enabling them to bridge\nvisual perception and natural language processing. However, their application\nto scientific domains, especially in interpreting complex field data commonly\nused in the natural sciences, remains underexplored. In this work, we introduce\nFieldLVLM, a novel framework designed to improve large vision-language models'\nunderstanding of field data. FieldLVLM consists of two main components: a\nfield-aware language generation strategy and a data-compressed multimodal model\ntuning. The field-aware language generation strategy leverages a\nspecial-purpose machine learning pipeline to extract key physical features from\nfield data, such as flow classification, Reynolds number, and vortex patterns.\nThis information is then converted into structured textual descriptions that\nserve as a dataset. The data-compressed multimodal model tuning focuses on\nLVLMs with these generated datasets, using a data compression strategy to\nreduce the complexity of field inputs and retain only the most informative\nvalues. This ensures compatibility with the models language decoder and guides\nits learning more effectively. Experimental results on newly proposed benchmark\ndatasets demonstrate that FieldLVLM significantly outperforms existing methods\nin tasks involving scientific field data. Our findings suggest that this\napproach opens up new possibilities for applying large vision-language models\nto scientific research, helping bridge the gap between large models and\ndomain-specific discovery.\n", "link": "http://arxiv.org/abs/2507.18311v1", "date": "2025-07-24", "relevancy": 3.0095, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6314}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Large%20Vision-Language%20Models%27%20Understanding%20for%20Field%20Data&body=Title%3A%20Improving%20Large%20Vision-Language%20Models%27%20Understanding%20for%20Field%20Data%0AAuthor%3A%20Xiaomei%20Zhang%20and%20Hanyu%20Zheng%20and%20Xiangyu%20Zhu%20and%20Jinghuan%20Wei%20and%20Junhong%20Zou%20and%20Zhen%20Lei%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20shown%20impressive%20capabilities%0Aacross%20a%20range%20of%20tasks%20that%20integrate%20visual%20and%20textual%20understanding%2C%20such%0Aas%20image%20captioning%20and%20visual%20question%20answering.%20These%20models%20are%20trained%20on%0Alarge-scale%20image%20and%20video%20datasets%20paired%20with%20text%2C%20enabling%20them%20to%20bridge%0Avisual%20perception%20and%20natural%20language%20processing.%20However%2C%20their%20application%0Ato%20scientific%20domains%2C%20especially%20in%20interpreting%20complex%20field%20data%20commonly%0Aused%20in%20the%20natural%20sciences%2C%20remains%20underexplored.%20In%20this%20work%2C%20we%20introduce%0AFieldLVLM%2C%20a%20novel%20framework%20designed%20to%20improve%20large%20vision-language%20models%27%0Aunderstanding%20of%20field%20data.%20FieldLVLM%20consists%20of%20two%20main%20components%3A%20a%0Afield-aware%20language%20generation%20strategy%20and%20a%20data-compressed%20multimodal%20model%0Atuning.%20The%20field-aware%20language%20generation%20strategy%20leverages%20a%0Aspecial-purpose%20machine%20learning%20pipeline%20to%20extract%20key%20physical%20features%20from%0Afield%20data%2C%20such%20as%20flow%20classification%2C%20Reynolds%20number%2C%20and%20vortex%20patterns.%0AThis%20information%20is%20then%20converted%20into%20structured%20textual%20descriptions%20that%0Aserve%20as%20a%20dataset.%20The%20data-compressed%20multimodal%20model%20tuning%20focuses%20on%0ALVLMs%20with%20these%20generated%20datasets%2C%20using%20a%20data%20compression%20strategy%20to%0Areduce%20the%20complexity%20of%20field%20inputs%20and%20retain%20only%20the%20most%20informative%0Avalues.%20This%20ensures%20compatibility%20with%20the%20models%20language%20decoder%20and%20guides%0Aits%20learning%20more%20effectively.%20Experimental%20results%20on%20newly%20proposed%20benchmark%0Adatasets%20demonstrate%20that%20FieldLVLM%20significantly%20outperforms%20existing%20methods%0Ain%20tasks%20involving%20scientific%20field%20data.%20Our%20findings%20suggest%20that%20this%0Aapproach%20opens%20up%20new%20possibilities%20for%20applying%20large%20vision-language%20models%0Ato%20scientific%20research%2C%20helping%20bridge%20the%20gap%20between%20large%20models%20and%0Adomain-specific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Large%2520Vision-Language%2520Models%2527%2520Understanding%2520for%2520Field%2520Data%26entry.906535625%3DXiaomei%2520Zhang%2520and%2520Hanyu%2520Zheng%2520and%2520Xiangyu%2520Zhu%2520and%2520Jinghuan%2520Wei%2520and%2520Junhong%2520Zou%2520and%2520Zhen%2520Lei%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520shown%2520impressive%2520capabilities%250Aacross%2520a%2520range%2520of%2520tasks%2520that%2520integrate%2520visual%2520and%2520textual%2520understanding%252C%2520such%250Aas%2520image%2520captioning%2520and%2520visual%2520question%2520answering.%2520These%2520models%2520are%2520trained%2520on%250Alarge-scale%2520image%2520and%2520video%2520datasets%2520paired%2520with%2520text%252C%2520enabling%2520them%2520to%2520bridge%250Avisual%2520perception%2520and%2520natural%2520language%2520processing.%2520However%252C%2520their%2520application%250Ato%2520scientific%2520domains%252C%2520especially%2520in%2520interpreting%2520complex%2520field%2520data%2520commonly%250Aused%2520in%2520the%2520natural%2520sciences%252C%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520introduce%250AFieldLVLM%252C%2520a%2520novel%2520framework%2520designed%2520to%2520improve%2520large%2520vision-language%2520models%2527%250Aunderstanding%2520of%2520field%2520data.%2520FieldLVLM%2520consists%2520of%2520two%2520main%2520components%253A%2520a%250Afield-aware%2520language%2520generation%2520strategy%2520and%2520a%2520data-compressed%2520multimodal%2520model%250Atuning.%2520The%2520field-aware%2520language%2520generation%2520strategy%2520leverages%2520a%250Aspecial-purpose%2520machine%2520learning%2520pipeline%2520to%2520extract%2520key%2520physical%2520features%2520from%250Afield%2520data%252C%2520such%2520as%2520flow%2520classification%252C%2520Reynolds%2520number%252C%2520and%2520vortex%2520patterns.%250AThis%2520information%2520is%2520then%2520converted%2520into%2520structured%2520textual%2520descriptions%2520that%250Aserve%2520as%2520a%2520dataset.%2520The%2520data-compressed%2520multimodal%2520model%2520tuning%2520focuses%2520on%250ALVLMs%2520with%2520these%2520generated%2520datasets%252C%2520using%2520a%2520data%2520compression%2520strategy%2520to%250Areduce%2520the%2520complexity%2520of%2520field%2520inputs%2520and%2520retain%2520only%2520the%2520most%2520informative%250Avalues.%2520This%2520ensures%2520compatibility%2520with%2520the%2520models%2520language%2520decoder%2520and%2520guides%250Aits%2520learning%2520more%2520effectively.%2520Experimental%2520results%2520on%2520newly%2520proposed%2520benchmark%250Adatasets%2520demonstrate%2520that%2520FieldLVLM%2520significantly%2520outperforms%2520existing%2520methods%250Ain%2520tasks%2520involving%2520scientific%2520field%2520data.%2520Our%2520findings%2520suggest%2520that%2520this%250Aapproach%2520opens%2520up%2520new%2520possibilities%2520for%2520applying%2520large%2520vision-language%2520models%250Ato%2520scientific%2520research%252C%2520helping%2520bridge%2520the%2520gap%2520between%2520large%2520models%2520and%250Adomain-specific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Large%20Vision-Language%20Models%27%20Understanding%20for%20Field%20Data&entry.906535625=Xiaomei%20Zhang%20and%20Hanyu%20Zheng%20and%20Xiangyu%20Zhu%20and%20Jinghuan%20Wei%20and%20Junhong%20Zou%20and%20Zhen%20Lei%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20shown%20impressive%20capabilities%0Aacross%20a%20range%20of%20tasks%20that%20integrate%20visual%20and%20textual%20understanding%2C%20such%0Aas%20image%20captioning%20and%20visual%20question%20answering.%20These%20models%20are%20trained%20on%0Alarge-scale%20image%20and%20video%20datasets%20paired%20with%20text%2C%20enabling%20them%20to%20bridge%0Avisual%20perception%20and%20natural%20language%20processing.%20However%2C%20their%20application%0Ato%20scientific%20domains%2C%20especially%20in%20interpreting%20complex%20field%20data%20commonly%0Aused%20in%20the%20natural%20sciences%2C%20remains%20underexplored.%20In%20this%20work%2C%20we%20introduce%0AFieldLVLM%2C%20a%20novel%20framework%20designed%20to%20improve%20large%20vision-language%20models%27%0Aunderstanding%20of%20field%20data.%20FieldLVLM%20consists%20of%20two%20main%20components%3A%20a%0Afield-aware%20language%20generation%20strategy%20and%20a%20data-compressed%20multimodal%20model%0Atuning.%20The%20field-aware%20language%20generation%20strategy%20leverages%20a%0Aspecial-purpose%20machine%20learning%20pipeline%20to%20extract%20key%20physical%20features%20from%0Afield%20data%2C%20such%20as%20flow%20classification%2C%20Reynolds%20number%2C%20and%20vortex%20patterns.%0AThis%20information%20is%20then%20converted%20into%20structured%20textual%20descriptions%20that%0Aserve%20as%20a%20dataset.%20The%20data-compressed%20multimodal%20model%20tuning%20focuses%20on%0ALVLMs%20with%20these%20generated%20datasets%2C%20using%20a%20data%20compression%20strategy%20to%0Areduce%20the%20complexity%20of%20field%20inputs%20and%20retain%20only%20the%20most%20informative%0Avalues.%20This%20ensures%20compatibility%20with%20the%20models%20language%20decoder%20and%20guides%0Aits%20learning%20more%20effectively.%20Experimental%20results%20on%20newly%20proposed%20benchmark%0Adatasets%20demonstrate%20that%20FieldLVLM%20significantly%20outperforms%20existing%20methods%0Ain%20tasks%20involving%20scientific%20field%20data.%20Our%20findings%20suggest%20that%20this%0Aapproach%20opens%20up%20new%20possibilities%20for%20applying%20large%20vision-language%20models%0Ato%20scientific%20research%2C%20helping%20bridge%20the%20gap%20between%20large%20models%20and%0Adomain-specific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18311v1&entry.124074799=Read"},
{"title": "MC3D-AD: A Unified Geometry-aware Reconstruction Model for\n  Multi-category 3D Anomaly Detection", "author": "Jiayi Cheng and Can Gao and Jie Zhou and Jiajun Wen and Tao Dai and Jinbao Wang", "abstract": "  3D Anomaly Detection (AD) is a promising means of controlling the quality of\nmanufactured products. However, existing methods typically require carefully\ntraining a task-specific model for each category independently, leading to high\ncost, low efficiency, and weak generalization. Therefore, this paper presents a\nnovel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims\nto utilize both local and global geometry-aware information to reconstruct\nnormal representations of all categories. First, to learn robust and\ngeneralized features of different categories, we propose an adaptive\ngeometry-aware masked attention module that extracts geometry variation\ninformation to guide mask attention. Then, we introduce a local geometry-aware\nencoder reinforced by the improved mask attention to encode group-level feature\ntokens. Finally, we design a global query decoder that utilizes point cloud\nposition embeddings to improve the decoding process and reconstruction ability.\nThis leads to local and global geometry-aware reconstructed feature tokens for\nthe AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and\nAnomaly-ShapeNet datasets, and exhibits significant superiority over current\nstate-of-the-art single-category methods, achieving 3.1\\% and 9.3\\% improvement\nin object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The\ncode is available at https://github.com/iCAN-SZU/MC3D-AD.\n", "link": "http://arxiv.org/abs/2505.01969v2", "date": "2025-07-24", "relevancy": 2.9518, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6012}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5872}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MC3D-AD%3A%20A%20Unified%20Geometry-aware%20Reconstruction%20Model%20for%0A%20%20Multi-category%203D%20Anomaly%20Detection&body=Title%3A%20MC3D-AD%3A%20A%20Unified%20Geometry-aware%20Reconstruction%20Model%20for%0A%20%20Multi-category%203D%20Anomaly%20Detection%0AAuthor%3A%20Jiayi%20Cheng%20and%20Can%20Gao%20and%20Jie%20Zhou%20and%20Jiajun%20Wen%20and%20Tao%20Dai%20and%20Jinbao%20Wang%0AAbstract%3A%20%20%203D%20Anomaly%20Detection%20%28AD%29%20is%20a%20promising%20means%20of%20controlling%20the%20quality%20of%0Amanufactured%20products.%20However%2C%20existing%20methods%20typically%20require%20carefully%0Atraining%20a%20task-specific%20model%20for%20each%20category%20independently%2C%20leading%20to%20high%0Acost%2C%20low%20efficiency%2C%20and%20weak%20generalization.%20Therefore%2C%20this%20paper%20presents%20a%0Anovel%20unified%20model%20for%20Multi-Category%203D%20Anomaly%20Detection%20%28MC3D-AD%29%20that%20aims%0Ato%20utilize%20both%20local%20and%20global%20geometry-aware%20information%20to%20reconstruct%0Anormal%20representations%20of%20all%20categories.%20First%2C%20to%20learn%20robust%20and%0Ageneralized%20features%20of%20different%20categories%2C%20we%20propose%20an%20adaptive%0Ageometry-aware%20masked%20attention%20module%20that%20extracts%20geometry%20variation%0Ainformation%20to%20guide%20mask%20attention.%20Then%2C%20we%20introduce%20a%20local%20geometry-aware%0Aencoder%20reinforced%20by%20the%20improved%20mask%20attention%20to%20encode%20group-level%20feature%0Atokens.%20Finally%2C%20we%20design%20a%20global%20query%20decoder%20that%20utilizes%20point%20cloud%0Aposition%20embeddings%20to%20improve%20the%20decoding%20process%20and%20reconstruction%20ability.%0AThis%20leads%20to%20local%20and%20global%20geometry-aware%20reconstructed%20feature%20tokens%20for%0Athe%20AD%20task.%20MC3D-AD%20is%20evaluated%20on%20two%20publicly%20available%20Real3D-AD%20and%0AAnomaly-ShapeNet%20datasets%2C%20and%20exhibits%20significant%20superiority%20over%20current%0Astate-of-the-art%20single-category%20methods%2C%20achieving%203.1%5C%25%20and%209.3%5C%25%20improvement%0Ain%20object-level%20AUROC%20over%20Real3D-AD%20and%20Anomaly-ShapeNet%2C%20respectively.%20The%0Acode%20is%20available%20at%20https%3A//github.com/iCAN-SZU/MC3D-AD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMC3D-AD%253A%2520A%2520Unified%2520Geometry-aware%2520Reconstruction%2520Model%2520for%250A%2520%2520Multi-category%25203D%2520Anomaly%2520Detection%26entry.906535625%3DJiayi%2520Cheng%2520and%2520Can%2520Gao%2520and%2520Jie%2520Zhou%2520and%2520Jiajun%2520Wen%2520and%2520Tao%2520Dai%2520and%2520Jinbao%2520Wang%26entry.1292438233%3D%2520%25203D%2520Anomaly%2520Detection%2520%2528AD%2529%2520is%2520a%2520promising%2520means%2520of%2520controlling%2520the%2520quality%2520of%250Amanufactured%2520products.%2520However%252C%2520existing%2520methods%2520typically%2520require%2520carefully%250Atraining%2520a%2520task-specific%2520model%2520for%2520each%2520category%2520independently%252C%2520leading%2520to%2520high%250Acost%252C%2520low%2520efficiency%252C%2520and%2520weak%2520generalization.%2520Therefore%252C%2520this%2520paper%2520presents%2520a%250Anovel%2520unified%2520model%2520for%2520Multi-Category%25203D%2520Anomaly%2520Detection%2520%2528MC3D-AD%2529%2520that%2520aims%250Ato%2520utilize%2520both%2520local%2520and%2520global%2520geometry-aware%2520information%2520to%2520reconstruct%250Anormal%2520representations%2520of%2520all%2520categories.%2520First%252C%2520to%2520learn%2520robust%2520and%250Ageneralized%2520features%2520of%2520different%2520categories%252C%2520we%2520propose%2520an%2520adaptive%250Ageometry-aware%2520masked%2520attention%2520module%2520that%2520extracts%2520geometry%2520variation%250Ainformation%2520to%2520guide%2520mask%2520attention.%2520Then%252C%2520we%2520introduce%2520a%2520local%2520geometry-aware%250Aencoder%2520reinforced%2520by%2520the%2520improved%2520mask%2520attention%2520to%2520encode%2520group-level%2520feature%250Atokens.%2520Finally%252C%2520we%2520design%2520a%2520global%2520query%2520decoder%2520that%2520utilizes%2520point%2520cloud%250Aposition%2520embeddings%2520to%2520improve%2520the%2520decoding%2520process%2520and%2520reconstruction%2520ability.%250AThis%2520leads%2520to%2520local%2520and%2520global%2520geometry-aware%2520reconstructed%2520feature%2520tokens%2520for%250Athe%2520AD%2520task.%2520MC3D-AD%2520is%2520evaluated%2520on%2520two%2520publicly%2520available%2520Real3D-AD%2520and%250AAnomaly-ShapeNet%2520datasets%252C%2520and%2520exhibits%2520significant%2520superiority%2520over%2520current%250Astate-of-the-art%2520single-category%2520methods%252C%2520achieving%25203.1%255C%2525%2520and%25209.3%255C%2525%2520improvement%250Ain%2520object-level%2520AUROC%2520over%2520Real3D-AD%2520and%2520Anomaly-ShapeNet%252C%2520respectively.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/iCAN-SZU/MC3D-AD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MC3D-AD%3A%20A%20Unified%20Geometry-aware%20Reconstruction%20Model%20for%0A%20%20Multi-category%203D%20Anomaly%20Detection&entry.906535625=Jiayi%20Cheng%20and%20Can%20Gao%20and%20Jie%20Zhou%20and%20Jiajun%20Wen%20and%20Tao%20Dai%20and%20Jinbao%20Wang&entry.1292438233=%20%203D%20Anomaly%20Detection%20%28AD%29%20is%20a%20promising%20means%20of%20controlling%20the%20quality%20of%0Amanufactured%20products.%20However%2C%20existing%20methods%20typically%20require%20carefully%0Atraining%20a%20task-specific%20model%20for%20each%20category%20independently%2C%20leading%20to%20high%0Acost%2C%20low%20efficiency%2C%20and%20weak%20generalization.%20Therefore%2C%20this%20paper%20presents%20a%0Anovel%20unified%20model%20for%20Multi-Category%203D%20Anomaly%20Detection%20%28MC3D-AD%29%20that%20aims%0Ato%20utilize%20both%20local%20and%20global%20geometry-aware%20information%20to%20reconstruct%0Anormal%20representations%20of%20all%20categories.%20First%2C%20to%20learn%20robust%20and%0Ageneralized%20features%20of%20different%20categories%2C%20we%20propose%20an%20adaptive%0Ageometry-aware%20masked%20attention%20module%20that%20extracts%20geometry%20variation%0Ainformation%20to%20guide%20mask%20attention.%20Then%2C%20we%20introduce%20a%20local%20geometry-aware%0Aencoder%20reinforced%20by%20the%20improved%20mask%20attention%20to%20encode%20group-level%20feature%0Atokens.%20Finally%2C%20we%20design%20a%20global%20query%20decoder%20that%20utilizes%20point%20cloud%0Aposition%20embeddings%20to%20improve%20the%20decoding%20process%20and%20reconstruction%20ability.%0AThis%20leads%20to%20local%20and%20global%20geometry-aware%20reconstructed%20feature%20tokens%20for%0Athe%20AD%20task.%20MC3D-AD%20is%20evaluated%20on%20two%20publicly%20available%20Real3D-AD%20and%0AAnomaly-ShapeNet%20datasets%2C%20and%20exhibits%20significant%20superiority%20over%20current%0Astate-of-the-art%20single-category%20methods%2C%20achieving%203.1%5C%25%20and%209.3%5C%25%20improvement%0Ain%20object-level%20AUROC%20over%20Real3D-AD%20and%20Anomaly-ShapeNet%2C%20respectively.%20The%0Acode%20is%20available%20at%20https%3A//github.com/iCAN-SZU/MC3D-AD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01969v2&entry.124074799=Read"},
{"title": "CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using\n  Gaussian Splatting", "author": "Haoran Xu and Saining Zhang and Peishuo Li and Baijun Ye and Xiaoxue Chen and Huan-ang Gao and Jv Zheng and Xiaowei Song and Ziqiao Peng and Run Miao and Jinrang Jia and Yifeng Shi and Guangqi Yi and Hang Zhao and Hao Tang and Hongyang Li and Kaicheng Yu and Hao Zhao", "abstract": "  Vehicle-to-everything (V2X) communication plays a crucial role in autonomous\ndriving, enabling cooperation between vehicles and infrastructure. While\nsimulation has significantly contributed to various autonomous driving tasks,\nits potential for data generation and augmentation in V2X scenarios remains\nunderexplored. In this paper, we introduce CRUISE, a comprehensive\nreconstruction-and-synthesis framework designed for V2X driving environments.\nCRUISE employs decomposed Gaussian Splatting to accurately reconstruct\nreal-world scenes while supporting flexible editing. By decomposing dynamic\ntraffic participants into editable Gaussian representations, CRUISE allows for\nseamless modification and augmentation of driving scenes. Furthermore, the\nframework renders images from both ego-vehicle and infrastructure views,\nenabling large-scale V2X dataset augmentation for training and evaluation. Our\nexperimental results demonstrate that: 1) CRUISE reconstructs real-world V2X\ndriving scenes with high fidelity; 2) using CRUISE improves 3D detection across\nego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D\ntracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates\nchallenging corner cases.\n", "link": "http://arxiv.org/abs/2507.18473v1", "date": "2025-07-24", "relevancy": 2.9344, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6267}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5695}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRUISE%3A%20Cooperative%20Reconstruction%20and%20Editing%20in%20V2X%20Scenarios%20using%0A%20%20Gaussian%20Splatting&body=Title%3A%20CRUISE%3A%20Cooperative%20Reconstruction%20and%20Editing%20in%20V2X%20Scenarios%20using%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Haoran%20Xu%20and%20Saining%20Zhang%20and%20Peishuo%20Li%20and%20Baijun%20Ye%20and%20Xiaoxue%20Chen%20and%20Huan-ang%20Gao%20and%20Jv%20Zheng%20and%20Xiaowei%20Song%20and%20Ziqiao%20Peng%20and%20Run%20Miao%20and%20Jinrang%20Jia%20and%20Yifeng%20Shi%20and%20Guangqi%20Yi%20and%20Hang%20Zhao%20and%20Hao%20Tang%20and%20Hongyang%20Li%20and%20Kaicheng%20Yu%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Vehicle-to-everything%20%28V2X%29%20communication%20plays%20a%20crucial%20role%20in%20autonomous%0Adriving%2C%20enabling%20cooperation%20between%20vehicles%20and%20infrastructure.%20While%0Asimulation%20has%20significantly%20contributed%20to%20various%20autonomous%20driving%20tasks%2C%0Aits%20potential%20for%20data%20generation%20and%20augmentation%20in%20V2X%20scenarios%20remains%0Aunderexplored.%20In%20this%20paper%2C%20we%20introduce%20CRUISE%2C%20a%20comprehensive%0Areconstruction-and-synthesis%20framework%20designed%20for%20V2X%20driving%20environments.%0ACRUISE%20employs%20decomposed%20Gaussian%20Splatting%20to%20accurately%20reconstruct%0Areal-world%20scenes%20while%20supporting%20flexible%20editing.%20By%20decomposing%20dynamic%0Atraffic%20participants%20into%20editable%20Gaussian%20representations%2C%20CRUISE%20allows%20for%0Aseamless%20modification%20and%20augmentation%20of%20driving%20scenes.%20Furthermore%2C%20the%0Aframework%20renders%20images%20from%20both%20ego-vehicle%20and%20infrastructure%20views%2C%0Aenabling%20large-scale%20V2X%20dataset%20augmentation%20for%20training%20and%20evaluation.%20Our%0Aexperimental%20results%20demonstrate%20that%3A%201%29%20CRUISE%20reconstructs%20real-world%20V2X%0Adriving%20scenes%20with%20high%20fidelity%3B%202%29%20using%20CRUISE%20improves%203D%20detection%20across%0Aego-vehicle%2C%20infrastructure%2C%20and%20cooperative%20views%2C%20as%20well%20as%20cooperative%203D%0Atracking%20on%20the%20V2X-Seq%20benchmark%3B%20and%203%29%20CRUISE%20effectively%20generates%0Achallenging%20corner%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRUISE%253A%2520Cooperative%2520Reconstruction%2520and%2520Editing%2520in%2520V2X%2520Scenarios%2520using%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DHaoran%2520Xu%2520and%2520Saining%2520Zhang%2520and%2520Peishuo%2520Li%2520and%2520Baijun%2520Ye%2520and%2520Xiaoxue%2520Chen%2520and%2520Huan-ang%2520Gao%2520and%2520Jv%2520Zheng%2520and%2520Xiaowei%2520Song%2520and%2520Ziqiao%2520Peng%2520and%2520Run%2520Miao%2520and%2520Jinrang%2520Jia%2520and%2520Yifeng%2520Shi%2520and%2520Guangqi%2520Yi%2520and%2520Hang%2520Zhao%2520and%2520Hao%2520Tang%2520and%2520Hongyang%2520Li%2520and%2520Kaicheng%2520Yu%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Vehicle-to-everything%2520%2528V2X%2529%2520communication%2520plays%2520a%2520crucial%2520role%2520in%2520autonomous%250Adriving%252C%2520enabling%2520cooperation%2520between%2520vehicles%2520and%2520infrastructure.%2520While%250Asimulation%2520has%2520significantly%2520contributed%2520to%2520various%2520autonomous%2520driving%2520tasks%252C%250Aits%2520potential%2520for%2520data%2520generation%2520and%2520augmentation%2520in%2520V2X%2520scenarios%2520remains%250Aunderexplored.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CRUISE%252C%2520a%2520comprehensive%250Areconstruction-and-synthesis%2520framework%2520designed%2520for%2520V2X%2520driving%2520environments.%250ACRUISE%2520employs%2520decomposed%2520Gaussian%2520Splatting%2520to%2520accurately%2520reconstruct%250Areal-world%2520scenes%2520while%2520supporting%2520flexible%2520editing.%2520By%2520decomposing%2520dynamic%250Atraffic%2520participants%2520into%2520editable%2520Gaussian%2520representations%252C%2520CRUISE%2520allows%2520for%250Aseamless%2520modification%2520and%2520augmentation%2520of%2520driving%2520scenes.%2520Furthermore%252C%2520the%250Aframework%2520renders%2520images%2520from%2520both%2520ego-vehicle%2520and%2520infrastructure%2520views%252C%250Aenabling%2520large-scale%2520V2X%2520dataset%2520augmentation%2520for%2520training%2520and%2520evaluation.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%253A%25201%2529%2520CRUISE%2520reconstructs%2520real-world%2520V2X%250Adriving%2520scenes%2520with%2520high%2520fidelity%253B%25202%2529%2520using%2520CRUISE%2520improves%25203D%2520detection%2520across%250Aego-vehicle%252C%2520infrastructure%252C%2520and%2520cooperative%2520views%252C%2520as%2520well%2520as%2520cooperative%25203D%250Atracking%2520on%2520the%2520V2X-Seq%2520benchmark%253B%2520and%25203%2529%2520CRUISE%2520effectively%2520generates%250Achallenging%2520corner%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRUISE%3A%20Cooperative%20Reconstruction%20and%20Editing%20in%20V2X%20Scenarios%20using%0A%20%20Gaussian%20Splatting&entry.906535625=Haoran%20Xu%20and%20Saining%20Zhang%20and%20Peishuo%20Li%20and%20Baijun%20Ye%20and%20Xiaoxue%20Chen%20and%20Huan-ang%20Gao%20and%20Jv%20Zheng%20and%20Xiaowei%20Song%20and%20Ziqiao%20Peng%20and%20Run%20Miao%20and%20Jinrang%20Jia%20and%20Yifeng%20Shi%20and%20Guangqi%20Yi%20and%20Hang%20Zhao%20and%20Hao%20Tang%20and%20Hongyang%20Li%20and%20Kaicheng%20Yu%20and%20Hao%20Zhao&entry.1292438233=%20%20Vehicle-to-everything%20%28V2X%29%20communication%20plays%20a%20crucial%20role%20in%20autonomous%0Adriving%2C%20enabling%20cooperation%20between%20vehicles%20and%20infrastructure.%20While%0Asimulation%20has%20significantly%20contributed%20to%20various%20autonomous%20driving%20tasks%2C%0Aits%20potential%20for%20data%20generation%20and%20augmentation%20in%20V2X%20scenarios%20remains%0Aunderexplored.%20In%20this%20paper%2C%20we%20introduce%20CRUISE%2C%20a%20comprehensive%0Areconstruction-and-synthesis%20framework%20designed%20for%20V2X%20driving%20environments.%0ACRUISE%20employs%20decomposed%20Gaussian%20Splatting%20to%20accurately%20reconstruct%0Areal-world%20scenes%20while%20supporting%20flexible%20editing.%20By%20decomposing%20dynamic%0Atraffic%20participants%20into%20editable%20Gaussian%20representations%2C%20CRUISE%20allows%20for%0Aseamless%20modification%20and%20augmentation%20of%20driving%20scenes.%20Furthermore%2C%20the%0Aframework%20renders%20images%20from%20both%20ego-vehicle%20and%20infrastructure%20views%2C%0Aenabling%20large-scale%20V2X%20dataset%20augmentation%20for%20training%20and%20evaluation.%20Our%0Aexperimental%20results%20demonstrate%20that%3A%201%29%20CRUISE%20reconstructs%20real-world%20V2X%0Adriving%20scenes%20with%20high%20fidelity%3B%202%29%20using%20CRUISE%20improves%203D%20detection%20across%0Aego-vehicle%2C%20infrastructure%2C%20and%20cooperative%20views%2C%20as%20well%20as%20cooperative%203D%0Atracking%20on%20the%20V2X-Seq%20benchmark%3B%20and%203%29%20CRUISE%20effectively%20generates%0Achallenging%20corner%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18473v1&entry.124074799=Read"},
{"title": "Explaining How Visual, Textual and Multimodal Encoders Share Concepts", "author": "Cl\u00e9ment Cornet and Romaric Besan\u00e7on and Herv\u00e9 Le Borgne", "abstract": "  Sparse autoencoders (SAEs) have emerged as a powerful technique for\nextracting human-interpretable features from neural networks activations.\nPrevious works compared different models based on SAE-derived features but\nthose comparisons have been restricted to models within the same modality. We\npropose a novel indicator allowing quantitative comparison of models across SAE\nfeatures, and use it to conduct a comparative study of visual, textual and\nmultimodal encoders. We also propose to quantify the Comparative Sharedness of\nindividual features between different classes of models. With these two new\ntools, we conduct several studies on 21 encoders of the three types, with two\nsignificantly different sizes, and considering generalist and domain specific\ndatasets. The results allow to revisit previous studies at the light of\nencoders trained in a multimodal context and to quantify to which extent all\nthese models share some representations or features. They also suggest that\nvisual features that are specific to VLMs among vision encoders are shared with\ntext encoders, highlighting the impact of text pretraining. The code is\navailable at https://github.com/CEA-LIST/SAEshareConcepts\n", "link": "http://arxiv.org/abs/2507.18512v1", "date": "2025-07-24", "relevancy": 2.934, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6081}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20How%20Visual%2C%20Textual%20and%20Multimodal%20Encoders%20Share%20Concepts&body=Title%3A%20Explaining%20How%20Visual%2C%20Textual%20and%20Multimodal%20Encoders%20Share%20Concepts%0AAuthor%3A%20Cl%C3%A9ment%20Cornet%20and%20Romaric%20Besan%C3%A7on%20and%20Herv%C3%A9%20Le%20Borgne%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20powerful%20technique%20for%0Aextracting%20human-interpretable%20features%20from%20neural%20networks%20activations.%0APrevious%20works%20compared%20different%20models%20based%20on%20SAE-derived%20features%20but%0Athose%20comparisons%20have%20been%20restricted%20to%20models%20within%20the%20same%20modality.%20We%0Apropose%20a%20novel%20indicator%20allowing%20quantitative%20comparison%20of%20models%20across%20SAE%0Afeatures%2C%20and%20use%20it%20to%20conduct%20a%20comparative%20study%20of%20visual%2C%20textual%20and%0Amultimodal%20encoders.%20We%20also%20propose%20to%20quantify%20the%20Comparative%20Sharedness%20of%0Aindividual%20features%20between%20different%20classes%20of%20models.%20With%20these%20two%20new%0Atools%2C%20we%20conduct%20several%20studies%20on%2021%20encoders%20of%20the%20three%20types%2C%20with%20two%0Asignificantly%20different%20sizes%2C%20and%20considering%20generalist%20and%20domain%20specific%0Adatasets.%20The%20results%20allow%20to%20revisit%20previous%20studies%20at%20the%20light%20of%0Aencoders%20trained%20in%20a%20multimodal%20context%20and%20to%20quantify%20to%20which%20extent%20all%0Athese%20models%20share%20some%20representations%20or%20features.%20They%20also%20suggest%20that%0Avisual%20features%20that%20are%20specific%20to%20VLMs%20among%20vision%20encoders%20are%20shared%20with%0Atext%20encoders%2C%20highlighting%20the%20impact%20of%20text%20pretraining.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/CEA-LIST/SAEshareConcepts%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520How%2520Visual%252C%2520Textual%2520and%2520Multimodal%2520Encoders%2520Share%2520Concepts%26entry.906535625%3DCl%25C3%25A9ment%2520Cornet%2520and%2520Romaric%2520Besan%25C3%25A7on%2520and%2520Herv%25C3%25A9%2520Le%2520Borgne%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520technique%2520for%250Aextracting%2520human-interpretable%2520features%2520from%2520neural%2520networks%2520activations.%250APrevious%2520works%2520compared%2520different%2520models%2520based%2520on%2520SAE-derived%2520features%2520but%250Athose%2520comparisons%2520have%2520been%2520restricted%2520to%2520models%2520within%2520the%2520same%2520modality.%2520We%250Apropose%2520a%2520novel%2520indicator%2520allowing%2520quantitative%2520comparison%2520of%2520models%2520across%2520SAE%250Afeatures%252C%2520and%2520use%2520it%2520to%2520conduct%2520a%2520comparative%2520study%2520of%2520visual%252C%2520textual%2520and%250Amultimodal%2520encoders.%2520We%2520also%2520propose%2520to%2520quantify%2520the%2520Comparative%2520Sharedness%2520of%250Aindividual%2520features%2520between%2520different%2520classes%2520of%2520models.%2520With%2520these%2520two%2520new%250Atools%252C%2520we%2520conduct%2520several%2520studies%2520on%252021%2520encoders%2520of%2520the%2520three%2520types%252C%2520with%2520two%250Asignificantly%2520different%2520sizes%252C%2520and%2520considering%2520generalist%2520and%2520domain%2520specific%250Adatasets.%2520The%2520results%2520allow%2520to%2520revisit%2520previous%2520studies%2520at%2520the%2520light%2520of%250Aencoders%2520trained%2520in%2520a%2520multimodal%2520context%2520and%2520to%2520quantify%2520to%2520which%2520extent%2520all%250Athese%2520models%2520share%2520some%2520representations%2520or%2520features.%2520They%2520also%2520suggest%2520that%250Avisual%2520features%2520that%2520are%2520specific%2520to%2520VLMs%2520among%2520vision%2520encoders%2520are%2520shared%2520with%250Atext%2520encoders%252C%2520highlighting%2520the%2520impact%2520of%2520text%2520pretraining.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/CEA-LIST/SAEshareConcepts%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20How%20Visual%2C%20Textual%20and%20Multimodal%20Encoders%20Share%20Concepts&entry.906535625=Cl%C3%A9ment%20Cornet%20and%20Romaric%20Besan%C3%A7on%20and%20Herv%C3%A9%20Le%20Borgne&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20powerful%20technique%20for%0Aextracting%20human-interpretable%20features%20from%20neural%20networks%20activations.%0APrevious%20works%20compared%20different%20models%20based%20on%20SAE-derived%20features%20but%0Athose%20comparisons%20have%20been%20restricted%20to%20models%20within%20the%20same%20modality.%20We%0Apropose%20a%20novel%20indicator%20allowing%20quantitative%20comparison%20of%20models%20across%20SAE%0Afeatures%2C%20and%20use%20it%20to%20conduct%20a%20comparative%20study%20of%20visual%2C%20textual%20and%0Amultimodal%20encoders.%20We%20also%20propose%20to%20quantify%20the%20Comparative%20Sharedness%20of%0Aindividual%20features%20between%20different%20classes%20of%20models.%20With%20these%20two%20new%0Atools%2C%20we%20conduct%20several%20studies%20on%2021%20encoders%20of%20the%20three%20types%2C%20with%20two%0Asignificantly%20different%20sizes%2C%20and%20considering%20generalist%20and%20domain%20specific%0Adatasets.%20The%20results%20allow%20to%20revisit%20previous%20studies%20at%20the%20light%20of%0Aencoders%20trained%20in%20a%20multimodal%20context%20and%20to%20quantify%20to%20which%20extent%20all%0Athese%20models%20share%20some%20representations%20or%20features.%20They%20also%20suggest%20that%0Avisual%20features%20that%20are%20specific%20to%20VLMs%20among%20vision%20encoders%20are%20shared%20with%0Atext%20encoders%2C%20highlighting%20the%20impact%20of%20text%20pretraining.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/CEA-LIST/SAEshareConcepts%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18512v1&entry.124074799=Read"},
{"title": "VideoMind: An Omni-Modal Video Dataset with Intent Grounding for\n  Deep-Cognitive Video Understanding", "author": "Baoyao Yang and Wanyun Li and Dixin Chen and Junxiang Chen and Wenbin Yao and Haifeng Lin", "abstract": "  This paper introduces VideoMind, a video-centric omni-modal dataset designed\nfor deep video content cognition and enhanced multi-modal feature\nrepresentation. The dataset comprises 103K video samples (3K reserved for\ntesting), each paired with audio and systematically detailed textual\ndescriptions. Specifically, every video and its audio is described across three\nhierarchical layers (factual, abstract, and intent), progressing from surface\nto depth. It contains over 22 million words, averaging ~225 words per sample.\nVideoMind's key distinction from existing datasets is its provision of intent\nexpressions, which require contextual integration across the entire video and\nare not directly observable. These deep-cognitive expressions are generated\nusing a Chain-of-Thought (COT) approach, prompting the mLLM through\nstep-by-step reasoning. Each description includes annotations for subject,\nplace, time, event, action, and intent, supporting downstream recognition\ntasks. Crucially, we establish a gold-standard benchmark with 3,000 manually\nvalidated samples for evaluating deep-cognitive video understanding. We design\nhybrid-cognitive retrieval experiments, scored by multi-level retrieval\nmetrics, to appropriately assess deep video comprehension. Evaluation results\nfor models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a\npowerful benchmark for fine-grained cross-modal alignment and advances fields\nrequiring in-depth video understanding, such as emotion and intent recognition.\nThe data is publicly available on GitHub, HuggingFace, and OpenDataLab,\nhttps://github.com/cdx-cindy/VideoMind.\n", "link": "http://arxiv.org/abs/2507.18552v1", "date": "2025-07-24", "relevancy": 2.8816, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoMind%3A%20An%20Omni-Modal%20Video%20Dataset%20with%20Intent%20Grounding%20for%0A%20%20Deep-Cognitive%20Video%20Understanding&body=Title%3A%20VideoMind%3A%20An%20Omni-Modal%20Video%20Dataset%20with%20Intent%20Grounding%20for%0A%20%20Deep-Cognitive%20Video%20Understanding%0AAuthor%3A%20Baoyao%20Yang%20and%20Wanyun%20Li%20and%20Dixin%20Chen%20and%20Junxiang%20Chen%20and%20Wenbin%20Yao%20and%20Haifeng%20Lin%0AAbstract%3A%20%20%20This%20paper%20introduces%20VideoMind%2C%20a%20video-centric%20omni-modal%20dataset%20designed%0Afor%20deep%20video%20content%20cognition%20and%20enhanced%20multi-modal%20feature%0Arepresentation.%20The%20dataset%20comprises%20103K%20video%20samples%20%283K%20reserved%20for%0Atesting%29%2C%20each%20paired%20with%20audio%20and%20systematically%20detailed%20textual%0Adescriptions.%20Specifically%2C%20every%20video%20and%20its%20audio%20is%20described%20across%20three%0Ahierarchical%20layers%20%28factual%2C%20abstract%2C%20and%20intent%29%2C%20progressing%20from%20surface%0Ato%20depth.%20It%20contains%20over%2022%20million%20words%2C%20averaging%20~225%20words%20per%20sample.%0AVideoMind%27s%20key%20distinction%20from%20existing%20datasets%20is%20its%20provision%20of%20intent%0Aexpressions%2C%20which%20require%20contextual%20integration%20across%20the%20entire%20video%20and%0Aare%20not%20directly%20observable.%20These%20deep-cognitive%20expressions%20are%20generated%0Ausing%20a%20Chain-of-Thought%20%28COT%29%20approach%2C%20prompting%20the%20mLLM%20through%0Astep-by-step%20reasoning.%20Each%20description%20includes%20annotations%20for%20subject%2C%0Aplace%2C%20time%2C%20event%2C%20action%2C%20and%20intent%2C%20supporting%20downstream%20recognition%0Atasks.%20Crucially%2C%20we%20establish%20a%20gold-standard%20benchmark%20with%203%2C000%20manually%0Avalidated%20samples%20for%20evaluating%20deep-cognitive%20video%20understanding.%20We%20design%0Ahybrid-cognitive%20retrieval%20experiments%2C%20scored%20by%20multi-level%20retrieval%0Ametrics%2C%20to%20appropriately%20assess%20deep%20video%20comprehension.%20Evaluation%20results%0Afor%20models%20%28e.g.%2C%20InternVideo%2C%20VAST%2C%20UMT-L%29%20are%20released.%20VideoMind%20serves%20as%20a%0Apowerful%20benchmark%20for%20fine-grained%20cross-modal%20alignment%20and%20advances%20fields%0Arequiring%20in-depth%20video%20understanding%2C%20such%20as%20emotion%20and%20intent%20recognition.%0AThe%20data%20is%20publicly%20available%20on%20GitHub%2C%20HuggingFace%2C%20and%20OpenDataLab%2C%0Ahttps%3A//github.com/cdx-cindy/VideoMind.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoMind%253A%2520An%2520Omni-Modal%2520Video%2520Dataset%2520with%2520Intent%2520Grounding%2520for%250A%2520%2520Deep-Cognitive%2520Video%2520Understanding%26entry.906535625%3DBaoyao%2520Yang%2520and%2520Wanyun%2520Li%2520and%2520Dixin%2520Chen%2520and%2520Junxiang%2520Chen%2520and%2520Wenbin%2520Yao%2520and%2520Haifeng%2520Lin%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520VideoMind%252C%2520a%2520video-centric%2520omni-modal%2520dataset%2520designed%250Afor%2520deep%2520video%2520content%2520cognition%2520and%2520enhanced%2520multi-modal%2520feature%250Arepresentation.%2520The%2520dataset%2520comprises%2520103K%2520video%2520samples%2520%25283K%2520reserved%2520for%250Atesting%2529%252C%2520each%2520paired%2520with%2520audio%2520and%2520systematically%2520detailed%2520textual%250Adescriptions.%2520Specifically%252C%2520every%2520video%2520and%2520its%2520audio%2520is%2520described%2520across%2520three%250Ahierarchical%2520layers%2520%2528factual%252C%2520abstract%252C%2520and%2520intent%2529%252C%2520progressing%2520from%2520surface%250Ato%2520depth.%2520It%2520contains%2520over%252022%2520million%2520words%252C%2520averaging%2520~225%2520words%2520per%2520sample.%250AVideoMind%2527s%2520key%2520distinction%2520from%2520existing%2520datasets%2520is%2520its%2520provision%2520of%2520intent%250Aexpressions%252C%2520which%2520require%2520contextual%2520integration%2520across%2520the%2520entire%2520video%2520and%250Aare%2520not%2520directly%2520observable.%2520These%2520deep-cognitive%2520expressions%2520are%2520generated%250Ausing%2520a%2520Chain-of-Thought%2520%2528COT%2529%2520approach%252C%2520prompting%2520the%2520mLLM%2520through%250Astep-by-step%2520reasoning.%2520Each%2520description%2520includes%2520annotations%2520for%2520subject%252C%250Aplace%252C%2520time%252C%2520event%252C%2520action%252C%2520and%2520intent%252C%2520supporting%2520downstream%2520recognition%250Atasks.%2520Crucially%252C%2520we%2520establish%2520a%2520gold-standard%2520benchmark%2520with%25203%252C000%2520manually%250Avalidated%2520samples%2520for%2520evaluating%2520deep-cognitive%2520video%2520understanding.%2520We%2520design%250Ahybrid-cognitive%2520retrieval%2520experiments%252C%2520scored%2520by%2520multi-level%2520retrieval%250Ametrics%252C%2520to%2520appropriately%2520assess%2520deep%2520video%2520comprehension.%2520Evaluation%2520results%250Afor%2520models%2520%2528e.g.%252C%2520InternVideo%252C%2520VAST%252C%2520UMT-L%2529%2520are%2520released.%2520VideoMind%2520serves%2520as%2520a%250Apowerful%2520benchmark%2520for%2520fine-grained%2520cross-modal%2520alignment%2520and%2520advances%2520fields%250Arequiring%2520in-depth%2520video%2520understanding%252C%2520such%2520as%2520emotion%2520and%2520intent%2520recognition.%250AThe%2520data%2520is%2520publicly%2520available%2520on%2520GitHub%252C%2520HuggingFace%252C%2520and%2520OpenDataLab%252C%250Ahttps%253A//github.com/cdx-cindy/VideoMind.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoMind%3A%20An%20Omni-Modal%20Video%20Dataset%20with%20Intent%20Grounding%20for%0A%20%20Deep-Cognitive%20Video%20Understanding&entry.906535625=Baoyao%20Yang%20and%20Wanyun%20Li%20and%20Dixin%20Chen%20and%20Junxiang%20Chen%20and%20Wenbin%20Yao%20and%20Haifeng%20Lin&entry.1292438233=%20%20This%20paper%20introduces%20VideoMind%2C%20a%20video-centric%20omni-modal%20dataset%20designed%0Afor%20deep%20video%20content%20cognition%20and%20enhanced%20multi-modal%20feature%0Arepresentation.%20The%20dataset%20comprises%20103K%20video%20samples%20%283K%20reserved%20for%0Atesting%29%2C%20each%20paired%20with%20audio%20and%20systematically%20detailed%20textual%0Adescriptions.%20Specifically%2C%20every%20video%20and%20its%20audio%20is%20described%20across%20three%0Ahierarchical%20layers%20%28factual%2C%20abstract%2C%20and%20intent%29%2C%20progressing%20from%20surface%0Ato%20depth.%20It%20contains%20over%2022%20million%20words%2C%20averaging%20~225%20words%20per%20sample.%0AVideoMind%27s%20key%20distinction%20from%20existing%20datasets%20is%20its%20provision%20of%20intent%0Aexpressions%2C%20which%20require%20contextual%20integration%20across%20the%20entire%20video%20and%0Aare%20not%20directly%20observable.%20These%20deep-cognitive%20expressions%20are%20generated%0Ausing%20a%20Chain-of-Thought%20%28COT%29%20approach%2C%20prompting%20the%20mLLM%20through%0Astep-by-step%20reasoning.%20Each%20description%20includes%20annotations%20for%20subject%2C%0Aplace%2C%20time%2C%20event%2C%20action%2C%20and%20intent%2C%20supporting%20downstream%20recognition%0Atasks.%20Crucially%2C%20we%20establish%20a%20gold-standard%20benchmark%20with%203%2C000%20manually%0Avalidated%20samples%20for%20evaluating%20deep-cognitive%20video%20understanding.%20We%20design%0Ahybrid-cognitive%20retrieval%20experiments%2C%20scored%20by%20multi-level%20retrieval%0Ametrics%2C%20to%20appropriately%20assess%20deep%20video%20comprehension.%20Evaluation%20results%0Afor%20models%20%28e.g.%2C%20InternVideo%2C%20VAST%2C%20UMT-L%29%20are%20released.%20VideoMind%20serves%20as%20a%0Apowerful%20benchmark%20for%20fine-grained%20cross-modal%20alignment%20and%20advances%20fields%0Arequiring%20in-depth%20video%20understanding%2C%20such%20as%20emotion%20and%20intent%20recognition.%0AThe%20data%20is%20publicly%20available%20on%20GitHub%2C%20HuggingFace%2C%20and%20OpenDataLab%2C%0Ahttps%3A//github.com/cdx-cindy/VideoMind.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18552v1&entry.124074799=Read"},
{"title": "DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place\n  Recognition", "author": "Haiyang Jiang and Songhao Piao and Chao Gao and Lei Yu and Liguo Chen", "abstract": "  Visual Place Recognition (VPR) is crucial for robust mobile robot\nlocalization, yet it faces significant challenges in maintaining reliable\nperformance under varying environmental conditions and viewpoints. To address\nthis, we propose a novel framework that integrates Dual-Scale-Former\n(DSFormer), a Transformer-based cross-learning module, with an innovative block\nclustering strategy. DSFormer enhances feature representation by enabling\nbidirectional information transfer between dual-scale features extracted from\nthe final two CNN layers, capturing both semantic richness and spatial details\nthrough self-attention for long-range dependencies within each scale and shared\ncross-attention for cross-scale learning. Complementing this, our block\nclustering strategy repartitions the widely used San Francisco eXtra Large\n(SF-XL) training dataset from multiple distinct perspectives, optimizing data\norganization to further bolster robustness against viewpoint variations.\nTogether, these innovations not only yield a robust global embedding adaptable\nto environmental changes but also reduce the required training data volume by\napproximately 30\\% compared to previous partitioning methods. Comprehensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nacross most benchmark datasets, surpassing advanced reranking methods like\nDELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution\nusing 512-dim global descriptors, while significantly improving computational\nefficiency.\n", "link": "http://arxiv.org/abs/2507.18444v1", "date": "2025-07-24", "relevancy": 2.8585, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSFormer%3A%20A%20Dual-Scale%20Cross-Learning%20Transformer%20for%20Visual%20Place%0A%20%20Recognition&body=Title%3A%20DSFormer%3A%20A%20Dual-Scale%20Cross-Learning%20Transformer%20for%20Visual%20Place%0A%20%20Recognition%0AAuthor%3A%20Haiyang%20Jiang%20and%20Songhao%20Piao%20and%20Chao%20Gao%20and%20Lei%20Yu%20and%20Liguo%20Chen%0AAbstract%3A%20%20%20Visual%20Place%20Recognition%20%28VPR%29%20is%20crucial%20for%20robust%20mobile%20robot%0Alocalization%2C%20yet%20it%20faces%20significant%20challenges%20in%20maintaining%20reliable%0Aperformance%20under%20varying%20environmental%20conditions%20and%20viewpoints.%20To%20address%0Athis%2C%20we%20propose%20a%20novel%20framework%20that%20integrates%20Dual-Scale-Former%0A%28DSFormer%29%2C%20a%20Transformer-based%20cross-learning%20module%2C%20with%20an%20innovative%20block%0Aclustering%20strategy.%20DSFormer%20enhances%20feature%20representation%20by%20enabling%0Abidirectional%20information%20transfer%20between%20dual-scale%20features%20extracted%20from%0Athe%20final%20two%20CNN%20layers%2C%20capturing%20both%20semantic%20richness%20and%20spatial%20details%0Athrough%20self-attention%20for%20long-range%20dependencies%20within%20each%20scale%20and%20shared%0Across-attention%20for%20cross-scale%20learning.%20Complementing%20this%2C%20our%20block%0Aclustering%20strategy%20repartitions%20the%20widely%20used%20San%20Francisco%20eXtra%20Large%0A%28SF-XL%29%20training%20dataset%20from%20multiple%20distinct%20perspectives%2C%20optimizing%20data%0Aorganization%20to%20further%20bolster%20robustness%20against%20viewpoint%20variations.%0ATogether%2C%20these%20innovations%20not%20only%20yield%20a%20robust%20global%20embedding%20adaptable%0Ato%20environmental%20changes%20but%20also%20reduce%20the%20required%20training%20data%20volume%20by%0Aapproximately%2030%5C%25%20compared%20to%20previous%20partitioning%20methods.%20Comprehensive%0Aexperiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%0Aacross%20most%20benchmark%20datasets%2C%20surpassing%20advanced%20reranking%20methods%20like%0ADELG%2C%20Patch-NetVLAD%2C%20TransVPR%2C%20and%20R2Former%20as%20a%20global%20retrieval%20solution%0Ausing%20512-dim%20global%20descriptors%2C%20while%20significantly%20improving%20computational%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSFormer%253A%2520A%2520Dual-Scale%2520Cross-Learning%2520Transformer%2520for%2520Visual%2520Place%250A%2520%2520Recognition%26entry.906535625%3DHaiyang%2520Jiang%2520and%2520Songhao%2520Piao%2520and%2520Chao%2520Gao%2520and%2520Lei%2520Yu%2520and%2520Liguo%2520Chen%26entry.1292438233%3D%2520%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529%2520is%2520crucial%2520for%2520robust%2520mobile%2520robot%250Alocalization%252C%2520yet%2520it%2520faces%2520significant%2520challenges%2520in%2520maintaining%2520reliable%250Aperformance%2520under%2520varying%2520environmental%2520conditions%2520and%2520viewpoints.%2520To%2520address%250Athis%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520integrates%2520Dual-Scale-Former%250A%2528DSFormer%2529%252C%2520a%2520Transformer-based%2520cross-learning%2520module%252C%2520with%2520an%2520innovative%2520block%250Aclustering%2520strategy.%2520DSFormer%2520enhances%2520feature%2520representation%2520by%2520enabling%250Abidirectional%2520information%2520transfer%2520between%2520dual-scale%2520features%2520extracted%2520from%250Athe%2520final%2520two%2520CNN%2520layers%252C%2520capturing%2520both%2520semantic%2520richness%2520and%2520spatial%2520details%250Athrough%2520self-attention%2520for%2520long-range%2520dependencies%2520within%2520each%2520scale%2520and%2520shared%250Across-attention%2520for%2520cross-scale%2520learning.%2520Complementing%2520this%252C%2520our%2520block%250Aclustering%2520strategy%2520repartitions%2520the%2520widely%2520used%2520San%2520Francisco%2520eXtra%2520Large%250A%2528SF-XL%2529%2520training%2520dataset%2520from%2520multiple%2520distinct%2520perspectives%252C%2520optimizing%2520data%250Aorganization%2520to%2520further%2520bolster%2520robustness%2520against%2520viewpoint%2520variations.%250ATogether%252C%2520these%2520innovations%2520not%2520only%2520yield%2520a%2520robust%2520global%2520embedding%2520adaptable%250Ato%2520environmental%2520changes%2520but%2520also%2520reduce%2520the%2520required%2520training%2520data%2520volume%2520by%250Aapproximately%252030%255C%2525%2520compared%2520to%2520previous%2520partitioning%2520methods.%2520Comprehensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520most%2520benchmark%2520datasets%252C%2520surpassing%2520advanced%2520reranking%2520methods%2520like%250ADELG%252C%2520Patch-NetVLAD%252C%2520TransVPR%252C%2520and%2520R2Former%2520as%2520a%2520global%2520retrieval%2520solution%250Ausing%2520512-dim%2520global%2520descriptors%252C%2520while%2520significantly%2520improving%2520computational%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSFormer%3A%20A%20Dual-Scale%20Cross-Learning%20Transformer%20for%20Visual%20Place%0A%20%20Recognition&entry.906535625=Haiyang%20Jiang%20and%20Songhao%20Piao%20and%20Chao%20Gao%20and%20Lei%20Yu%20and%20Liguo%20Chen&entry.1292438233=%20%20Visual%20Place%20Recognition%20%28VPR%29%20is%20crucial%20for%20robust%20mobile%20robot%0Alocalization%2C%20yet%20it%20faces%20significant%20challenges%20in%20maintaining%20reliable%0Aperformance%20under%20varying%20environmental%20conditions%20and%20viewpoints.%20To%20address%0Athis%2C%20we%20propose%20a%20novel%20framework%20that%20integrates%20Dual-Scale-Former%0A%28DSFormer%29%2C%20a%20Transformer-based%20cross-learning%20module%2C%20with%20an%20innovative%20block%0Aclustering%20strategy.%20DSFormer%20enhances%20feature%20representation%20by%20enabling%0Abidirectional%20information%20transfer%20between%20dual-scale%20features%20extracted%20from%0Athe%20final%20two%20CNN%20layers%2C%20capturing%20both%20semantic%20richness%20and%20spatial%20details%0Athrough%20self-attention%20for%20long-range%20dependencies%20within%20each%20scale%20and%20shared%0Across-attention%20for%20cross-scale%20learning.%20Complementing%20this%2C%20our%20block%0Aclustering%20strategy%20repartitions%20the%20widely%20used%20San%20Francisco%20eXtra%20Large%0A%28SF-XL%29%20training%20dataset%20from%20multiple%20distinct%20perspectives%2C%20optimizing%20data%0Aorganization%20to%20further%20bolster%20robustness%20against%20viewpoint%20variations.%0ATogether%2C%20these%20innovations%20not%20only%20yield%20a%20robust%20global%20embedding%20adaptable%0Ato%20environmental%20changes%20but%20also%20reduce%20the%20required%20training%20data%20volume%20by%0Aapproximately%2030%5C%25%20compared%20to%20previous%20partitioning%20methods.%20Comprehensive%0Aexperiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%0Aacross%20most%20benchmark%20datasets%2C%20surpassing%20advanced%20reranking%20methods%20like%0ADELG%2C%20Patch-NetVLAD%2C%20TransVPR%2C%20and%20R2Former%20as%20a%20global%20retrieval%20solution%0Ausing%20512-dim%20global%20descriptors%2C%20while%20significantly%20improving%20computational%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18444v1&entry.124074799=Read"},
{"title": "Tiny is not small enough: High-quality, low-resource facial animation\n  models through hybrid knowledge distillation", "author": "Zhen Han and Mattias Teye and Derek Yadgaroff and Judith B\u00fctepage", "abstract": "  The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters.\n", "link": "http://arxiv.org/abs/2507.18352v1", "date": "2025-07-24", "relevancy": 2.7943, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5855}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5477}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiny%20is%20not%20small%20enough%3A%20High-quality%2C%20low-resource%20facial%20animation%0A%20%20models%20through%20hybrid%20knowledge%20distillation&body=Title%3A%20Tiny%20is%20not%20small%20enough%3A%20High-quality%2C%20low-resource%20facial%20animation%0A%20%20models%20through%20hybrid%20knowledge%20distillation%0AAuthor%3A%20Zhen%20Han%20and%20Mattias%20Teye%20and%20Derek%20Yadgaroff%20and%20Judith%20B%C3%BCtepage%0AAbstract%3A%20%20%20The%20training%20of%20high-quality%2C%20robust%20machine%20learning%20models%20for%0Aspeech-driven%203D%20facial%20animation%20requires%20a%20large%2C%20diverse%20dataset%20of%0Ahigh-quality%20audio-animation%20pairs.%20To%20overcome%20the%20lack%20of%20such%20a%20dataset%2C%0Arecent%20work%20has%20introduced%20large%20pre-trained%20speech%20encoders%20that%20are%20robust%20to%0Avariations%20in%20the%20input%20audio%20and%2C%20therefore%2C%20enable%20the%20facial%20animation%20model%0Ato%20generalize%20across%20speakers%2C%20audio%20quality%2C%20and%20languages.%20However%2C%20the%0Aresulting%20facial%20animation%20models%20are%20prohibitively%20large%20and%20lend%20themselves%0Aonly%20to%20offline%20inference%20on%20a%20dedicated%20machine.%20In%20this%20work%2C%20we%20explore%0Aon-device%2C%20real-time%20facial%20animation%20models%20in%20the%20context%20of%20game%0Adevelopment.%20We%20overcome%20the%20lack%20of%20large%20datasets%20by%20using%20hybrid%20knowledge%0Adistillation%20with%20pseudo-labeling.%20Given%20a%20large%20audio%20dataset%2C%20we%20employ%20a%0Ahigh-performing%20teacher%20model%20to%20train%20very%20small%20student%20models.%20In%20contrast%0Ato%20the%20pre-trained%20speech%20encoders%2C%20our%20student%20models%20only%20consist%20of%0Aconvolutional%20and%20fully-connected%20layers%2C%20removing%20the%20need%20for%20attention%0Acontext%20or%20recurrent%20updates.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20we%20can%0Areduce%20the%20memory%20footprint%20to%20up%20to%203.4%20MB%20and%20required%20future%20audio%20context%0Ato%20up%20to%2081%20ms%20while%20maintaining%20high-quality%20animations.%20This%20paves%20the%20way%0Afor%20on-device%20inference%2C%20an%20important%20step%20towards%20realistic%2C%20model-driven%0Adigital%20characters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiny%2520is%2520not%2520small%2520enough%253A%2520High-quality%252C%2520low-resource%2520facial%2520animation%250A%2520%2520models%2520through%2520hybrid%2520knowledge%2520distillation%26entry.906535625%3DZhen%2520Han%2520and%2520Mattias%2520Teye%2520and%2520Derek%2520Yadgaroff%2520and%2520Judith%2520B%25C3%25BCtepage%26entry.1292438233%3D%2520%2520The%2520training%2520of%2520high-quality%252C%2520robust%2520machine%2520learning%2520models%2520for%250Aspeech-driven%25203D%2520facial%2520animation%2520requires%2520a%2520large%252C%2520diverse%2520dataset%2520of%250Ahigh-quality%2520audio-animation%2520pairs.%2520To%2520overcome%2520the%2520lack%2520of%2520such%2520a%2520dataset%252C%250Arecent%2520work%2520has%2520introduced%2520large%2520pre-trained%2520speech%2520encoders%2520that%2520are%2520robust%2520to%250Avariations%2520in%2520the%2520input%2520audio%2520and%252C%2520therefore%252C%2520enable%2520the%2520facial%2520animation%2520model%250Ato%2520generalize%2520across%2520speakers%252C%2520audio%2520quality%252C%2520and%2520languages.%2520However%252C%2520the%250Aresulting%2520facial%2520animation%2520models%2520are%2520prohibitively%2520large%2520and%2520lend%2520themselves%250Aonly%2520to%2520offline%2520inference%2520on%2520a%2520dedicated%2520machine.%2520In%2520this%2520work%252C%2520we%2520explore%250Aon-device%252C%2520real-time%2520facial%2520animation%2520models%2520in%2520the%2520context%2520of%2520game%250Adevelopment.%2520We%2520overcome%2520the%2520lack%2520of%2520large%2520datasets%2520by%2520using%2520hybrid%2520knowledge%250Adistillation%2520with%2520pseudo-labeling.%2520Given%2520a%2520large%2520audio%2520dataset%252C%2520we%2520employ%2520a%250Ahigh-performing%2520teacher%2520model%2520to%2520train%2520very%2520small%2520student%2520models.%2520In%2520contrast%250Ato%2520the%2520pre-trained%2520speech%2520encoders%252C%2520our%2520student%2520models%2520only%2520consist%2520of%250Aconvolutional%2520and%2520fully-connected%2520layers%252C%2520removing%2520the%2520need%2520for%2520attention%250Acontext%2520or%2520recurrent%2520updates.%2520In%2520our%2520experiments%252C%2520we%2520demonstrate%2520that%2520we%2520can%250Areduce%2520the%2520memory%2520footprint%2520to%2520up%2520to%25203.4%2520MB%2520and%2520required%2520future%2520audio%2520context%250Ato%2520up%2520to%252081%2520ms%2520while%2520maintaining%2520high-quality%2520animations.%2520This%2520paves%2520the%2520way%250Afor%2520on-device%2520inference%252C%2520an%2520important%2520step%2520towards%2520realistic%252C%2520model-driven%250Adigital%2520characters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiny%20is%20not%20small%20enough%3A%20High-quality%2C%20low-resource%20facial%20animation%0A%20%20models%20through%20hybrid%20knowledge%20distillation&entry.906535625=Zhen%20Han%20and%20Mattias%20Teye%20and%20Derek%20Yadgaroff%20and%20Judith%20B%C3%BCtepage&entry.1292438233=%20%20The%20training%20of%20high-quality%2C%20robust%20machine%20learning%20models%20for%0Aspeech-driven%203D%20facial%20animation%20requires%20a%20large%2C%20diverse%20dataset%20of%0Ahigh-quality%20audio-animation%20pairs.%20To%20overcome%20the%20lack%20of%20such%20a%20dataset%2C%0Arecent%20work%20has%20introduced%20large%20pre-trained%20speech%20encoders%20that%20are%20robust%20to%0Avariations%20in%20the%20input%20audio%20and%2C%20therefore%2C%20enable%20the%20facial%20animation%20model%0Ato%20generalize%20across%20speakers%2C%20audio%20quality%2C%20and%20languages.%20However%2C%20the%0Aresulting%20facial%20animation%20models%20are%20prohibitively%20large%20and%20lend%20themselves%0Aonly%20to%20offline%20inference%20on%20a%20dedicated%20machine.%20In%20this%20work%2C%20we%20explore%0Aon-device%2C%20real-time%20facial%20animation%20models%20in%20the%20context%20of%20game%0Adevelopment.%20We%20overcome%20the%20lack%20of%20large%20datasets%20by%20using%20hybrid%20knowledge%0Adistillation%20with%20pseudo-labeling.%20Given%20a%20large%20audio%20dataset%2C%20we%20employ%20a%0Ahigh-performing%20teacher%20model%20to%20train%20very%20small%20student%20models.%20In%20contrast%0Ato%20the%20pre-trained%20speech%20encoders%2C%20our%20student%20models%20only%20consist%20of%0Aconvolutional%20and%20fully-connected%20layers%2C%20removing%20the%20need%20for%20attention%0Acontext%20or%20recurrent%20updates.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20we%20can%0Areduce%20the%20memory%20footprint%20to%20up%20to%203.4%20MB%20and%20required%20future%20audio%20context%0Ato%20up%20to%2081%20ms%20while%20maintaining%20high-quality%20animations.%20This%20paves%20the%20way%0Afor%20on-device%20inference%2C%20an%20important%20step%20towards%20realistic%2C%20model-driven%0Adigital%20characters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18352v1&entry.124074799=Read"},
{"title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis", "author": "Alexander Mai and Peter Hedman and George Kopanas and Dor Verbin and David Futschik and Qiangeng Xu and Falko Kuester and Jonathan T. Barron and Yinda Zhang", "abstract": "  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.\n", "link": "http://arxiv.org/abs/2410.01804v6", "date": "2025-07-24", "relevancy": 2.7838, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6072}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5315}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVER%3A%20Exact%20Volumetric%20Ellipsoid%20Rendering%20for%20Real-time%20View%20Synthesis&body=Title%3A%20EVER%3A%20Exact%20Volumetric%20Ellipsoid%20Rendering%20for%20Real-time%20View%20Synthesis%0AAuthor%3A%20Alexander%20Mai%20and%20Peter%20Hedman%20and%20George%20Kopanas%20and%20Dor%20Verbin%20and%20David%20Futschik%20and%20Qiangeng%20Xu%20and%20Falko%20Kuester%20and%20Jonathan%20T.%20Barron%20and%20Yinda%20Zhang%0AAbstract%3A%20%20%20We%20present%20Exact%20Volumetric%20Ellipsoid%20Rendering%20%28EVER%29%2C%20a%20method%20for%0Areal-time%20differentiable%20emission-only%20volume%20rendering.%20Unlike%20recent%0Arasterization%20based%20approach%20by%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20our%20primitive%0Abased%20representation%20allows%20for%20exact%20volume%20rendering%2C%20rather%20than%20alpha%0Acompositing%203D%20Gaussian%20billboards.%20As%20such%2C%20unlike%203DGS%20our%20formulation%20does%0Anot%20suffer%20from%20popping%20artifacts%20and%20view%20dependent%20density%2C%20but%20still%0Aachieves%20frame%20rates%20of%20%24%5Csim%5C%2130%24%20FPS%20at%20720p%20on%20an%20NVIDIA%20RTX4090.%20Since%20our%0Aapproach%20is%20built%20upon%20ray%20tracing%20it%20enables%20effects%20such%20as%20defocus%20blur%20and%0Acamera%20distortion%20%28e.g.%20such%20as%20from%20fisheye%20cameras%29%2C%20which%20are%20difficult%20to%0Aachieve%20by%20rasterization.%20We%20show%20that%20our%20method%20is%20more%20accurate%20with%20fewer%0Ablending%20issues%20than%203DGS%20and%20follow-up%20work%20on%20view-consistent%20rendering%2C%0Aespecially%20on%20the%20challenging%20large-scale%20scenes%20from%20the%20Zip-NeRF%20dataset%0Awhere%20it%20achieves%20sharpest%20results%20among%20real-time%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01804v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVER%253A%2520Exact%2520Volumetric%2520Ellipsoid%2520Rendering%2520for%2520Real-time%2520View%2520Synthesis%26entry.906535625%3DAlexander%2520Mai%2520and%2520Peter%2520Hedman%2520and%2520George%2520Kopanas%2520and%2520Dor%2520Verbin%2520and%2520David%2520Futschik%2520and%2520Qiangeng%2520Xu%2520and%2520Falko%2520Kuester%2520and%2520Jonathan%2520T.%2520Barron%2520and%2520Yinda%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520Exact%2520Volumetric%2520Ellipsoid%2520Rendering%2520%2528EVER%2529%252C%2520a%2520method%2520for%250Areal-time%2520differentiable%2520emission-only%2520volume%2520rendering.%2520Unlike%2520recent%250Arasterization%2520based%2520approach%2520by%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520our%2520primitive%250Abased%2520representation%2520allows%2520for%2520exact%2520volume%2520rendering%252C%2520rather%2520than%2520alpha%250Acompositing%25203D%2520Gaussian%2520billboards.%2520As%2520such%252C%2520unlike%25203DGS%2520our%2520formulation%2520does%250Anot%2520suffer%2520from%2520popping%2520artifacts%2520and%2520view%2520dependent%2520density%252C%2520but%2520still%250Aachieves%2520frame%2520rates%2520of%2520%2524%255Csim%255C%252130%2524%2520FPS%2520at%2520720p%2520on%2520an%2520NVIDIA%2520RTX4090.%2520Since%2520our%250Aapproach%2520is%2520built%2520upon%2520ray%2520tracing%2520it%2520enables%2520effects%2520such%2520as%2520defocus%2520blur%2520and%250Acamera%2520distortion%2520%2528e.g.%2520such%2520as%2520from%2520fisheye%2520cameras%2529%252C%2520which%2520are%2520difficult%2520to%250Aachieve%2520by%2520rasterization.%2520We%2520show%2520that%2520our%2520method%2520is%2520more%2520accurate%2520with%2520fewer%250Ablending%2520issues%2520than%25203DGS%2520and%2520follow-up%2520work%2520on%2520view-consistent%2520rendering%252C%250Aespecially%2520on%2520the%2520challenging%2520large-scale%2520scenes%2520from%2520the%2520Zip-NeRF%2520dataset%250Awhere%2520it%2520achieves%2520sharpest%2520results%2520among%2520real-time%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01804v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVER%3A%20Exact%20Volumetric%20Ellipsoid%20Rendering%20for%20Real-time%20View%20Synthesis&entry.906535625=Alexander%20Mai%20and%20Peter%20Hedman%20and%20George%20Kopanas%20and%20Dor%20Verbin%20and%20David%20Futschik%20and%20Qiangeng%20Xu%20and%20Falko%20Kuester%20and%20Jonathan%20T.%20Barron%20and%20Yinda%20Zhang&entry.1292438233=%20%20We%20present%20Exact%20Volumetric%20Ellipsoid%20Rendering%20%28EVER%29%2C%20a%20method%20for%0Areal-time%20differentiable%20emission-only%20volume%20rendering.%20Unlike%20recent%0Arasterization%20based%20approach%20by%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20our%20primitive%0Abased%20representation%20allows%20for%20exact%20volume%20rendering%2C%20rather%20than%20alpha%0Acompositing%203D%20Gaussian%20billboards.%20As%20such%2C%20unlike%203DGS%20our%20formulation%20does%0Anot%20suffer%20from%20popping%20artifacts%20and%20view%20dependent%20density%2C%20but%20still%0Aachieves%20frame%20rates%20of%20%24%5Csim%5C%2130%24%20FPS%20at%20720p%20on%20an%20NVIDIA%20RTX4090.%20Since%20our%0Aapproach%20is%20built%20upon%20ray%20tracing%20it%20enables%20effects%20such%20as%20defocus%20blur%20and%0Acamera%20distortion%20%28e.g.%20such%20as%20from%20fisheye%20cameras%29%2C%20which%20are%20difficult%20to%0Aachieve%20by%20rasterization.%20We%20show%20that%20our%20method%20is%20more%20accurate%20with%20fewer%0Ablending%20issues%20than%203DGS%20and%20follow-up%20work%20on%20view-consistent%20rendering%2C%0Aespecially%20on%20the%20challenging%20large-scale%20scenes%20from%20the%20Zip-NeRF%20dataset%0Awhere%20it%20achieves%20sharpest%20results%20among%20real-time%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01804v6&entry.124074799=Read"},
{"title": "BiECVC: Gated Diversification of Bidirectional Contexts for Learned\n  Video Compression", "author": "Wei Jiang and Junru Li and Kai Zhang and Li Zhang", "abstract": "  Recent forward prediction-based learned video compression (LVC) methods have\nachieved impressive results, even surpassing VVC reference software VTM under\nthe Low Delay B (LDB) configuration. In contrast, learned bidirectional video\ncompression (BVC) remains underexplored and still lags behind its forward-only\ncounterparts. This performance gap is mainly due to the limited ability to\nextract diverse and accurate contexts: most existing BVCs primarily exploit\ntemporal motion while neglecting non-local correlations across frames.\nMoreover, they lack the adaptability to dynamically suppress harmful contexts\narising from fast motion or occlusion. To tackle these challenges, we propose\nBiECVC, a BVC framework that incorporates diversified local and non-local\ncontext modeling along with adaptive context gating. For local context\nenhancement, BiECVC reuses high-quality features from lower layers and aligns\nthem using decoded motion vectors without introducing extra motion overhead. To\nmodel non-local dependencies efficiently, we adopt a linear attention mechanism\nthat balances performance and complexity. To further mitigate the impact of\ninaccurate context prediction, we introduce Bidirectional Context Gating,\ninspired by data-dependent decay in recent autoregressive language models, to\ndynamically filter contextual information based on conditional coding results.\nExtensive experiments demonstrate that BiECVC achieves state-of-the-art\nperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2\nunder the Random Access (RA) configuration with intra periods of 32 and 64,\nrespectively. To our knowledge, BiECVC is the first learned video codec to\nsurpass VTM 13.2 RA across all standard test datasets.\n", "link": "http://arxiv.org/abs/2505.09193v4", "date": "2025-07-24", "relevancy": 2.75, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiECVC%3A%20Gated%20Diversification%20of%20Bidirectional%20Contexts%20for%20Learned%0A%20%20Video%20Compression&body=Title%3A%20BiECVC%3A%20Gated%20Diversification%20of%20Bidirectional%20Contexts%20for%20Learned%0A%20%20Video%20Compression%0AAuthor%3A%20Wei%20Jiang%20and%20Junru%20Li%20and%20Kai%20Zhang%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Recent%20forward%20prediction-based%20learned%20video%20compression%20%28LVC%29%20methods%20have%0Aachieved%20impressive%20results%2C%20even%20surpassing%20VVC%20reference%20software%20VTM%20under%0Athe%20Low%20Delay%20B%20%28LDB%29%20configuration.%20In%20contrast%2C%20learned%20bidirectional%20video%0Acompression%20%28BVC%29%20remains%20underexplored%20and%20still%20lags%20behind%20its%20forward-only%0Acounterparts.%20This%20performance%20gap%20is%20mainly%20due%20to%20the%20limited%20ability%20to%0Aextract%20diverse%20and%20accurate%20contexts%3A%20most%20existing%20BVCs%20primarily%20exploit%0Atemporal%20motion%20while%20neglecting%20non-local%20correlations%20across%20frames.%0AMoreover%2C%20they%20lack%20the%20adaptability%20to%20dynamically%20suppress%20harmful%20contexts%0Aarising%20from%20fast%20motion%20or%20occlusion.%20To%20tackle%20these%20challenges%2C%20we%20propose%0ABiECVC%2C%20a%20BVC%20framework%20that%20incorporates%20diversified%20local%20and%20non-local%0Acontext%20modeling%20along%20with%20adaptive%20context%20gating.%20For%20local%20context%0Aenhancement%2C%20BiECVC%20reuses%20high-quality%20features%20from%20lower%20layers%20and%20aligns%0Athem%20using%20decoded%20motion%20vectors%20without%20introducing%20extra%20motion%20overhead.%20To%0Amodel%20non-local%20dependencies%20efficiently%2C%20we%20adopt%20a%20linear%20attention%20mechanism%0Athat%20balances%20performance%20and%20complexity.%20To%20further%20mitigate%20the%20impact%20of%0Ainaccurate%20context%20prediction%2C%20we%20introduce%20Bidirectional%20Context%20Gating%2C%0Ainspired%20by%20data-dependent%20decay%20in%20recent%20autoregressive%20language%20models%2C%20to%0Adynamically%20filter%20contextual%20information%20based%20on%20conditional%20coding%20results.%0AExtensive%20experiments%20demonstrate%20that%20BiECVC%20achieves%20state-of-the-art%0Aperformance%2C%20reducing%20the%20bit-rate%20by%2013.4%25%20and%2015.7%25%20compared%20to%20VTM%2013.2%0Aunder%20the%20Random%20Access%20%28RA%29%20configuration%20with%20intra%20periods%20of%2032%20and%2064%2C%0Arespectively.%20To%20our%20knowledge%2C%20BiECVC%20is%20the%20first%20learned%20video%20codec%20to%0Asurpass%20VTM%2013.2%20RA%20across%20all%20standard%20test%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09193v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiECVC%253A%2520Gated%2520Diversification%2520of%2520Bidirectional%2520Contexts%2520for%2520Learned%250A%2520%2520Video%2520Compression%26entry.906535625%3DWei%2520Jiang%2520and%2520Junru%2520Li%2520and%2520Kai%2520Zhang%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520forward%2520prediction-based%2520learned%2520video%2520compression%2520%2528LVC%2529%2520methods%2520have%250Aachieved%2520impressive%2520results%252C%2520even%2520surpassing%2520VVC%2520reference%2520software%2520VTM%2520under%250Athe%2520Low%2520Delay%2520B%2520%2528LDB%2529%2520configuration.%2520In%2520contrast%252C%2520learned%2520bidirectional%2520video%250Acompression%2520%2528BVC%2529%2520remains%2520underexplored%2520and%2520still%2520lags%2520behind%2520its%2520forward-only%250Acounterparts.%2520This%2520performance%2520gap%2520is%2520mainly%2520due%2520to%2520the%2520limited%2520ability%2520to%250Aextract%2520diverse%2520and%2520accurate%2520contexts%253A%2520most%2520existing%2520BVCs%2520primarily%2520exploit%250Atemporal%2520motion%2520while%2520neglecting%2520non-local%2520correlations%2520across%2520frames.%250AMoreover%252C%2520they%2520lack%2520the%2520adaptability%2520to%2520dynamically%2520suppress%2520harmful%2520contexts%250Aarising%2520from%2520fast%2520motion%2520or%2520occlusion.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%250ABiECVC%252C%2520a%2520BVC%2520framework%2520that%2520incorporates%2520diversified%2520local%2520and%2520non-local%250Acontext%2520modeling%2520along%2520with%2520adaptive%2520context%2520gating.%2520For%2520local%2520context%250Aenhancement%252C%2520BiECVC%2520reuses%2520high-quality%2520features%2520from%2520lower%2520layers%2520and%2520aligns%250Athem%2520using%2520decoded%2520motion%2520vectors%2520without%2520introducing%2520extra%2520motion%2520overhead.%2520To%250Amodel%2520non-local%2520dependencies%2520efficiently%252C%2520we%2520adopt%2520a%2520linear%2520attention%2520mechanism%250Athat%2520balances%2520performance%2520and%2520complexity.%2520To%2520further%2520mitigate%2520the%2520impact%2520of%250Ainaccurate%2520context%2520prediction%252C%2520we%2520introduce%2520Bidirectional%2520Context%2520Gating%252C%250Ainspired%2520by%2520data-dependent%2520decay%2520in%2520recent%2520autoregressive%2520language%2520models%252C%2520to%250Adynamically%2520filter%2520contextual%2520information%2520based%2520on%2520conditional%2520coding%2520results.%250AExtensive%2520experiments%2520demonstrate%2520that%2520BiECVC%2520achieves%2520state-of-the-art%250Aperformance%252C%2520reducing%2520the%2520bit-rate%2520by%252013.4%2525%2520and%252015.7%2525%2520compared%2520to%2520VTM%252013.2%250Aunder%2520the%2520Random%2520Access%2520%2528RA%2529%2520configuration%2520with%2520intra%2520periods%2520of%252032%2520and%252064%252C%250Arespectively.%2520To%2520our%2520knowledge%252C%2520BiECVC%2520is%2520the%2520first%2520learned%2520video%2520codec%2520to%250Asurpass%2520VTM%252013.2%2520RA%2520across%2520all%2520standard%2520test%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09193v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiECVC%3A%20Gated%20Diversification%20of%20Bidirectional%20Contexts%20for%20Learned%0A%20%20Video%20Compression&entry.906535625=Wei%20Jiang%20and%20Junru%20Li%20and%20Kai%20Zhang%20and%20Li%20Zhang&entry.1292438233=%20%20Recent%20forward%20prediction-based%20learned%20video%20compression%20%28LVC%29%20methods%20have%0Aachieved%20impressive%20results%2C%20even%20surpassing%20VVC%20reference%20software%20VTM%20under%0Athe%20Low%20Delay%20B%20%28LDB%29%20configuration.%20In%20contrast%2C%20learned%20bidirectional%20video%0Acompression%20%28BVC%29%20remains%20underexplored%20and%20still%20lags%20behind%20its%20forward-only%0Acounterparts.%20This%20performance%20gap%20is%20mainly%20due%20to%20the%20limited%20ability%20to%0Aextract%20diverse%20and%20accurate%20contexts%3A%20most%20existing%20BVCs%20primarily%20exploit%0Atemporal%20motion%20while%20neglecting%20non-local%20correlations%20across%20frames.%0AMoreover%2C%20they%20lack%20the%20adaptability%20to%20dynamically%20suppress%20harmful%20contexts%0Aarising%20from%20fast%20motion%20or%20occlusion.%20To%20tackle%20these%20challenges%2C%20we%20propose%0ABiECVC%2C%20a%20BVC%20framework%20that%20incorporates%20diversified%20local%20and%20non-local%0Acontext%20modeling%20along%20with%20adaptive%20context%20gating.%20For%20local%20context%0Aenhancement%2C%20BiECVC%20reuses%20high-quality%20features%20from%20lower%20layers%20and%20aligns%0Athem%20using%20decoded%20motion%20vectors%20without%20introducing%20extra%20motion%20overhead.%20To%0Amodel%20non-local%20dependencies%20efficiently%2C%20we%20adopt%20a%20linear%20attention%20mechanism%0Athat%20balances%20performance%20and%20complexity.%20To%20further%20mitigate%20the%20impact%20of%0Ainaccurate%20context%20prediction%2C%20we%20introduce%20Bidirectional%20Context%20Gating%2C%0Ainspired%20by%20data-dependent%20decay%20in%20recent%20autoregressive%20language%20models%2C%20to%0Adynamically%20filter%20contextual%20information%20based%20on%20conditional%20coding%20results.%0AExtensive%20experiments%20demonstrate%20that%20BiECVC%20achieves%20state-of-the-art%0Aperformance%2C%20reducing%20the%20bit-rate%20by%2013.4%25%20and%2015.7%25%20compared%20to%20VTM%2013.2%0Aunder%20the%20Random%20Access%20%28RA%29%20configuration%20with%20intra%20periods%20of%2032%20and%2064%2C%0Arespectively.%20To%20our%20knowledge%2C%20BiECVC%20is%20the%20first%20learned%20video%20codec%20to%0Asurpass%20VTM%2013.2%20RA%20across%20all%20standard%20test%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09193v4&entry.124074799=Read"},
{"title": "SR-NeRV: Improving Embedding Efficiency of Neural Video Representation\n  via Super-Resolution", "author": "Taiga Hayami and Kakeru Koizumi and Hiroshi Watanabe", "abstract": "  Implicit Neural Representations (INRs) have garnered significant attention\nfor their ability to model complex signals in various domains. Recently,\nINR-based frameworks have shown promise in neural video compression by\nembedding video content into compact neural networks. However, these methods\noften struggle to reconstruct high-frequency details under stringent\nconstraints on model size, which are critical in practical compression\nscenarios. To address this limitation, we propose an INR-based video\nrepresentation framework that integrates a general-purpose super-resolution\n(SR) network. This design is motivated by the observation that high-frequency\ncomponents tend to exhibit low temporal redundancy across frames. By offloading\nthe reconstruction of fine details to a dedicated SR network pre-trained on\nnatural images, the proposed method improves visual fidelity. Experimental\nresults demonstrate that the proposed method outperforms conventional INR-based\nbaselines in reconstruction quality, while maintaining a comparable model size.\n", "link": "http://arxiv.org/abs/2505.00046v2", "date": "2025-07-24", "relevancy": 2.7089, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5656}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SR-NeRV%3A%20Improving%20Embedding%20Efficiency%20of%20Neural%20Video%20Representation%0A%20%20via%20Super-Resolution&body=Title%3A%20SR-NeRV%3A%20Improving%20Embedding%20Efficiency%20of%20Neural%20Video%20Representation%0A%20%20via%20Super-Resolution%0AAuthor%3A%20Taiga%20Hayami%20and%20Kakeru%20Koizumi%20and%20Hiroshi%20Watanabe%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20%28INRs%29%20have%20garnered%20significant%20attention%0Afor%20their%20ability%20to%20model%20complex%20signals%20in%20various%20domains.%20Recently%2C%0AINR-based%20frameworks%20have%20shown%20promise%20in%20neural%20video%20compression%20by%0Aembedding%20video%20content%20into%20compact%20neural%20networks.%20However%2C%20these%20methods%0Aoften%20struggle%20to%20reconstruct%20high-frequency%20details%20under%20stringent%0Aconstraints%20on%20model%20size%2C%20which%20are%20critical%20in%20practical%20compression%0Ascenarios.%20To%20address%20this%20limitation%2C%20we%20propose%20an%20INR-based%20video%0Arepresentation%20framework%20that%20integrates%20a%20general-purpose%20super-resolution%0A%28SR%29%20network.%20This%20design%20is%20motivated%20by%20the%20observation%20that%20high-frequency%0Acomponents%20tend%20to%20exhibit%20low%20temporal%20redundancy%20across%20frames.%20By%20offloading%0Athe%20reconstruction%20of%20fine%20details%20to%20a%20dedicated%20SR%20network%20pre-trained%20on%0Anatural%20images%2C%20the%20proposed%20method%20improves%20visual%20fidelity.%20Experimental%0Aresults%20demonstrate%20that%20the%20proposed%20method%20outperforms%20conventional%20INR-based%0Abaselines%20in%20reconstruction%20quality%2C%20while%20maintaining%20a%20comparable%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSR-NeRV%253A%2520Improving%2520Embedding%2520Efficiency%2520of%2520Neural%2520Video%2520Representation%250A%2520%2520via%2520Super-Resolution%26entry.906535625%3DTaiga%2520Hayami%2520and%2520Kakeru%2520Koizumi%2520and%2520Hiroshi%2520Watanabe%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%2520have%2520garnered%2520significant%2520attention%250Afor%2520their%2520ability%2520to%2520model%2520complex%2520signals%2520in%2520various%2520domains.%2520Recently%252C%250AINR-based%2520frameworks%2520have%2520shown%2520promise%2520in%2520neural%2520video%2520compression%2520by%250Aembedding%2520video%2520content%2520into%2520compact%2520neural%2520networks.%2520However%252C%2520these%2520methods%250Aoften%2520struggle%2520to%2520reconstruct%2520high-frequency%2520details%2520under%2520stringent%250Aconstraints%2520on%2520model%2520size%252C%2520which%2520are%2520critical%2520in%2520practical%2520compression%250Ascenarios.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520an%2520INR-based%2520video%250Arepresentation%2520framework%2520that%2520integrates%2520a%2520general-purpose%2520super-resolution%250A%2528SR%2529%2520network.%2520This%2520design%2520is%2520motivated%2520by%2520the%2520observation%2520that%2520high-frequency%250Acomponents%2520tend%2520to%2520exhibit%2520low%2520temporal%2520redundancy%2520across%2520frames.%2520By%2520offloading%250Athe%2520reconstruction%2520of%2520fine%2520details%2520to%2520a%2520dedicated%2520SR%2520network%2520pre-trained%2520on%250Anatural%2520images%252C%2520the%2520proposed%2520method%2520improves%2520visual%2520fidelity.%2520Experimental%250Aresults%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520conventional%2520INR-based%250Abaselines%2520in%2520reconstruction%2520quality%252C%2520while%2520maintaining%2520a%2520comparable%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SR-NeRV%3A%20Improving%20Embedding%20Efficiency%20of%20Neural%20Video%20Representation%0A%20%20via%20Super-Resolution&entry.906535625=Taiga%20Hayami%20and%20Kakeru%20Koizumi%20and%20Hiroshi%20Watanabe&entry.1292438233=%20%20Implicit%20Neural%20Representations%20%28INRs%29%20have%20garnered%20significant%20attention%0Afor%20their%20ability%20to%20model%20complex%20signals%20in%20various%20domains.%20Recently%2C%0AINR-based%20frameworks%20have%20shown%20promise%20in%20neural%20video%20compression%20by%0Aembedding%20video%20content%20into%20compact%20neural%20networks.%20However%2C%20these%20methods%0Aoften%20struggle%20to%20reconstruct%20high-frequency%20details%20under%20stringent%0Aconstraints%20on%20model%20size%2C%20which%20are%20critical%20in%20practical%20compression%0Ascenarios.%20To%20address%20this%20limitation%2C%20we%20propose%20an%20INR-based%20video%0Arepresentation%20framework%20that%20integrates%20a%20general-purpose%20super-resolution%0A%28SR%29%20network.%20This%20design%20is%20motivated%20by%20the%20observation%20that%20high-frequency%0Acomponents%20tend%20to%20exhibit%20low%20temporal%20redundancy%20across%20frames.%20By%20offloading%0Athe%20reconstruction%20of%20fine%20details%20to%20a%20dedicated%20SR%20network%20pre-trained%20on%0Anatural%20images%2C%20the%20proposed%20method%20improves%20visual%20fidelity.%20Experimental%0Aresults%20demonstrate%20that%20the%20proposed%20method%20outperforms%20conventional%20INR-based%0Abaselines%20in%20reconstruction%20quality%2C%20while%20maintaining%20a%20comparable%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00046v2&entry.124074799=Read"},
{"title": "Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning", "author": "Junming Liu and Siyuan Meng and Yanting Gao and Song Mao and Pinlong Cai and Guohang Yan and Yirong Chen and Zilin Bian and Ding Wang and Botian Shi", "abstract": "  Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.\n", "link": "http://arxiv.org/abs/2503.12972v2", "date": "2025-07-24", "relevancy": 2.7054, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Vision%20to%20Language%3A%20Annotation-Free%20Multimodal%20Knowledge%20Graph%0A%20%20Construction%20for%20Enhanced%20LLMs%20Reasoning&body=Title%3A%20Aligning%20Vision%20to%20Language%3A%20Annotation-Free%20Multimodal%20Knowledge%20Graph%0A%20%20Construction%20for%20Enhanced%20LLMs%20Reasoning%0AAuthor%3A%20Junming%20Liu%20and%20Siyuan%20Meng%20and%20Yanting%20Gao%20and%20Song%20Mao%20and%20Pinlong%20Cai%20and%20Guohang%20Yan%20and%20Yirong%20Chen%20and%20Zilin%20Bian%20and%20Ding%20Wang%20and%20Botian%20Shi%0AAbstract%3A%20%20%20Multimodal%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29%20struggles%20with%0Aincomplete%20knowledge%20and%20hallucination%20artifacts%2C%20challenges%20that%20textual%0AKnowledge%20Graphs%20%28KGs%29%20only%20partially%20mitigate%20due%20to%20their%20modality%20isolation.%0AWhile%20Multimodal%20Knowledge%20Graphs%20%28MMKGs%29%20promise%20enhanced%20cross-modal%0Aunderstanding%2C%20their%20practical%20construction%20is%20impeded%20by%20semantic%20narrowness%0Aof%20manual%20text%20annotations%20and%20inherent%20noise%20in%20visual-semantic%20entity%0Alinkages.%20In%20this%20paper%2C%20we%20propose%20Vision-align-to-Language%20integrated%0AKnowledge%20Graph%20%28VaLiK%29%2C%20a%20novel%20approach%20for%20constructing%20MMKGs%20that%20enhances%0ALLMs%20reasoning%20through%20cross-modal%20information%20supplementation.%20Specifically%2C%0Awe%20cascade%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20to%20align%20image%20features%0Awith%20text%2C%20transforming%20them%20into%20descriptions%20that%20encapsulate%20image-specific%0Ainformation.%20Furthermore%2C%20we%20developed%20a%20cross-modal%20similarity%20verification%0Amechanism%20to%20quantify%20semantic%20consistency%2C%20effectively%20filtering%20out%20noise%0Aintroduced%20during%20feature%20alignment.%20Even%20without%20manually%20annotated%20image%0Acaptions%2C%20the%20refined%20descriptions%20alone%20suffice%20to%20construct%20the%20MMKG.%0ACompared%20to%20conventional%20MMKGs%20construction%20paradigms%2C%20our%20approach%20achieves%0Asubstantial%20storage%20efficiency%20gains%20while%20maintaining%20direct%20entity-to-image%0Alinkage%20capability.%20Experimental%20results%20on%20multimodal%20reasoning%20tasks%0Ademonstrate%20that%20LLMs%20augmented%20with%20VaLiK%20outperform%20previous%20state-of-the-art%0Amodels.%20Our%20code%20is%20published%20at%20https%3A//github.com/Wings-Of-Disaster/VaLiK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12972v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Vision%2520to%2520Language%253A%2520Annotation-Free%2520Multimodal%2520Knowledge%2520Graph%250A%2520%2520Construction%2520for%2520Enhanced%2520LLMs%2520Reasoning%26entry.906535625%3DJunming%2520Liu%2520and%2520Siyuan%2520Meng%2520and%2520Yanting%2520Gao%2520and%2520Song%2520Mao%2520and%2520Pinlong%2520Cai%2520and%2520Guohang%2520Yan%2520and%2520Yirong%2520Chen%2520and%2520Zilin%2520Bian%2520and%2520Ding%2520Wang%2520and%2520Botian%2520Shi%26entry.1292438233%3D%2520%2520Multimodal%2520reasoning%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520struggles%2520with%250Aincomplete%2520knowledge%2520and%2520hallucination%2520artifacts%252C%2520challenges%2520that%2520textual%250AKnowledge%2520Graphs%2520%2528KGs%2529%2520only%2520partially%2520mitigate%2520due%2520to%2520their%2520modality%2520isolation.%250AWhile%2520Multimodal%2520Knowledge%2520Graphs%2520%2528MMKGs%2529%2520promise%2520enhanced%2520cross-modal%250Aunderstanding%252C%2520their%2520practical%2520construction%2520is%2520impeded%2520by%2520semantic%2520narrowness%250Aof%2520manual%2520text%2520annotations%2520and%2520inherent%2520noise%2520in%2520visual-semantic%2520entity%250Alinkages.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Vision-align-to-Language%2520integrated%250AKnowledge%2520Graph%2520%2528VaLiK%2529%252C%2520a%2520novel%2520approach%2520for%2520constructing%2520MMKGs%2520that%2520enhances%250ALLMs%2520reasoning%2520through%2520cross-modal%2520information%2520supplementation.%2520Specifically%252C%250Awe%2520cascade%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520align%2520image%2520features%250Awith%2520text%252C%2520transforming%2520them%2520into%2520descriptions%2520that%2520encapsulate%2520image-specific%250Ainformation.%2520Furthermore%252C%2520we%2520developed%2520a%2520cross-modal%2520similarity%2520verification%250Amechanism%2520to%2520quantify%2520semantic%2520consistency%252C%2520effectively%2520filtering%2520out%2520noise%250Aintroduced%2520during%2520feature%2520alignment.%2520Even%2520without%2520manually%2520annotated%2520image%250Acaptions%252C%2520the%2520refined%2520descriptions%2520alone%2520suffice%2520to%2520construct%2520the%2520MMKG.%250ACompared%2520to%2520conventional%2520MMKGs%2520construction%2520paradigms%252C%2520our%2520approach%2520achieves%250Asubstantial%2520storage%2520efficiency%2520gains%2520while%2520maintaining%2520direct%2520entity-to-image%250Alinkage%2520capability.%2520Experimental%2520results%2520on%2520multimodal%2520reasoning%2520tasks%250Ademonstrate%2520that%2520LLMs%2520augmented%2520with%2520VaLiK%2520outperform%2520previous%2520state-of-the-art%250Amodels.%2520Our%2520code%2520is%2520published%2520at%2520https%253A//github.com/Wings-Of-Disaster/VaLiK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12972v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Vision%20to%20Language%3A%20Annotation-Free%20Multimodal%20Knowledge%20Graph%0A%20%20Construction%20for%20Enhanced%20LLMs%20Reasoning&entry.906535625=Junming%20Liu%20and%20Siyuan%20Meng%20and%20Yanting%20Gao%20and%20Song%20Mao%20and%20Pinlong%20Cai%20and%20Guohang%20Yan%20and%20Yirong%20Chen%20and%20Zilin%20Bian%20and%20Ding%20Wang%20and%20Botian%20Shi&entry.1292438233=%20%20Multimodal%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29%20struggles%20with%0Aincomplete%20knowledge%20and%20hallucination%20artifacts%2C%20challenges%20that%20textual%0AKnowledge%20Graphs%20%28KGs%29%20only%20partially%20mitigate%20due%20to%20their%20modality%20isolation.%0AWhile%20Multimodal%20Knowledge%20Graphs%20%28MMKGs%29%20promise%20enhanced%20cross-modal%0Aunderstanding%2C%20their%20practical%20construction%20is%20impeded%20by%20semantic%20narrowness%0Aof%20manual%20text%20annotations%20and%20inherent%20noise%20in%20visual-semantic%20entity%0Alinkages.%20In%20this%20paper%2C%20we%20propose%20Vision-align-to-Language%20integrated%0AKnowledge%20Graph%20%28VaLiK%29%2C%20a%20novel%20approach%20for%20constructing%20MMKGs%20that%20enhances%0ALLMs%20reasoning%20through%20cross-modal%20information%20supplementation.%20Specifically%2C%0Awe%20cascade%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20to%20align%20image%20features%0Awith%20text%2C%20transforming%20them%20into%20descriptions%20that%20encapsulate%20image-specific%0Ainformation.%20Furthermore%2C%20we%20developed%20a%20cross-modal%20similarity%20verification%0Amechanism%20to%20quantify%20semantic%20consistency%2C%20effectively%20filtering%20out%20noise%0Aintroduced%20during%20feature%20alignment.%20Even%20without%20manually%20annotated%20image%0Acaptions%2C%20the%20refined%20descriptions%20alone%20suffice%20to%20construct%20the%20MMKG.%0ACompared%20to%20conventional%20MMKGs%20construction%20paradigms%2C%20our%20approach%20achieves%0Asubstantial%20storage%20efficiency%20gains%20while%20maintaining%20direct%20entity-to-image%0Alinkage%20capability.%20Experimental%20results%20on%20multimodal%20reasoning%20tasks%0Ademonstrate%20that%20LLMs%20augmented%20with%20VaLiK%20outperform%20previous%20state-of-the-art%0Amodels.%20Our%20code%20is%20published%20at%20https%3A//github.com/Wings-Of-Disaster/VaLiK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12972v2&entry.124074799=Read"},
{"title": "NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning", "author": "Mahdi Ghafourian and Federico M. Sukno", "abstract": "  Head pose estimation (HPE) plays a critical role in various computer vision\napplications such as human-computer interaction and facial recognition. In this\npaper, we propose a novel deep learning approach for head pose estimation with\nlimited training data via non-linear manifold learning called NLML-HPE. This\nmethod is based on the combination of tensor decomposition (i.e., Tucker\ndecomposition) and feed forward neural networks. Unlike traditional\nclassification-based approaches, our method formulates head pose estimation as\na regression problem, mapping input landmarks into a continuous representation\nof pose angles. To this end, our method uses tensor decomposition to split each\nEuler angle (yaw, pitch, roll) to separate subspaces and models each dimension\nof the underlying manifold as a cosine curve. We address two key challenges: 1.\nAlmost all HPE datasets suffer from incorrect and inaccurate pose annotations.\nHence, we generated a precise and consistent 2D head pose dataset for our\ntraining set by rotating 3D head models for a fixed set of poses and rendering\nthe corresponding 2D images. 2. We achieved real-time performance with limited\ntraining data as our method accurately captures the nature of rotation of an\nobject from facial landmarks. Once the underlying manifold for rotation around\neach axis is learned, the model is very fast in predicting unseen data. Our\ntraining and testing code is available online along with our trained models:\nhttps: //github.com/MahdiGhafoorian/NLML_HPE.\n", "link": "http://arxiv.org/abs/2507.18429v1", "date": "2025-07-24", "relevancy": 2.6834, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5428}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5425}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NLML-HPE%3A%20Head%20Pose%20Estimation%20with%20Limited%20Data%20via%20Manifold%20Learning&body=Title%3A%20NLML-HPE%3A%20Head%20Pose%20Estimation%20with%20Limited%20Data%20via%20Manifold%20Learning%0AAuthor%3A%20Mahdi%20Ghafourian%20and%20Federico%20M.%20Sukno%0AAbstract%3A%20%20%20Head%20pose%20estimation%20%28HPE%29%20plays%20a%20critical%20role%20in%20various%20computer%20vision%0Aapplications%20such%20as%20human-computer%20interaction%20and%20facial%20recognition.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20deep%20learning%20approach%20for%20head%20pose%20estimation%20with%0Alimited%20training%20data%20via%20non-linear%20manifold%20learning%20called%20NLML-HPE.%20This%0Amethod%20is%20based%20on%20the%20combination%20of%20tensor%20decomposition%20%28i.e.%2C%20Tucker%0Adecomposition%29%20and%20feed%20forward%20neural%20networks.%20Unlike%20traditional%0Aclassification-based%20approaches%2C%20our%20method%20formulates%20head%20pose%20estimation%20as%0Aa%20regression%20problem%2C%20mapping%20input%20landmarks%20into%20a%20continuous%20representation%0Aof%20pose%20angles.%20To%20this%20end%2C%20our%20method%20uses%20tensor%20decomposition%20to%20split%20each%0AEuler%20angle%20%28yaw%2C%20pitch%2C%20roll%29%20to%20separate%20subspaces%20and%20models%20each%20dimension%0Aof%20the%20underlying%20manifold%20as%20a%20cosine%20curve.%20We%20address%20two%20key%20challenges%3A%201.%0AAlmost%20all%20HPE%20datasets%20suffer%20from%20incorrect%20and%20inaccurate%20pose%20annotations.%0AHence%2C%20we%20generated%20a%20precise%20and%20consistent%202D%20head%20pose%20dataset%20for%20our%0Atraining%20set%20by%20rotating%203D%20head%20models%20for%20a%20fixed%20set%20of%20poses%20and%20rendering%0Athe%20corresponding%202D%20images.%202.%20We%20achieved%20real-time%20performance%20with%20limited%0Atraining%20data%20as%20our%20method%20accurately%20captures%20the%20nature%20of%20rotation%20of%20an%0Aobject%20from%20facial%20landmarks.%20Once%20the%20underlying%20manifold%20for%20rotation%20around%0Aeach%20axis%20is%20learned%2C%20the%20model%20is%20very%20fast%20in%20predicting%20unseen%20data.%20Our%0Atraining%20and%20testing%20code%20is%20available%20online%20along%20with%20our%20trained%20models%3A%0Ahttps%3A%20//github.com/MahdiGhafoorian/NLML_HPE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNLML-HPE%253A%2520Head%2520Pose%2520Estimation%2520with%2520Limited%2520Data%2520via%2520Manifold%2520Learning%26entry.906535625%3DMahdi%2520Ghafourian%2520and%2520Federico%2520M.%2520Sukno%26entry.1292438233%3D%2520%2520Head%2520pose%2520estimation%2520%2528HPE%2529%2520plays%2520a%2520critical%2520role%2520in%2520various%2520computer%2520vision%250Aapplications%2520such%2520as%2520human-computer%2520interaction%2520and%2520facial%2520recognition.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520deep%2520learning%2520approach%2520for%2520head%2520pose%2520estimation%2520with%250Alimited%2520training%2520data%2520via%2520non-linear%2520manifold%2520learning%2520called%2520NLML-HPE.%2520This%250Amethod%2520is%2520based%2520on%2520the%2520combination%2520of%2520tensor%2520decomposition%2520%2528i.e.%252C%2520Tucker%250Adecomposition%2529%2520and%2520feed%2520forward%2520neural%2520networks.%2520Unlike%2520traditional%250Aclassification-based%2520approaches%252C%2520our%2520method%2520formulates%2520head%2520pose%2520estimation%2520as%250Aa%2520regression%2520problem%252C%2520mapping%2520input%2520landmarks%2520into%2520a%2520continuous%2520representation%250Aof%2520pose%2520angles.%2520To%2520this%2520end%252C%2520our%2520method%2520uses%2520tensor%2520decomposition%2520to%2520split%2520each%250AEuler%2520angle%2520%2528yaw%252C%2520pitch%252C%2520roll%2529%2520to%2520separate%2520subspaces%2520and%2520models%2520each%2520dimension%250Aof%2520the%2520underlying%2520manifold%2520as%2520a%2520cosine%2520curve.%2520We%2520address%2520two%2520key%2520challenges%253A%25201.%250AAlmost%2520all%2520HPE%2520datasets%2520suffer%2520from%2520incorrect%2520and%2520inaccurate%2520pose%2520annotations.%250AHence%252C%2520we%2520generated%2520a%2520precise%2520and%2520consistent%25202D%2520head%2520pose%2520dataset%2520for%2520our%250Atraining%2520set%2520by%2520rotating%25203D%2520head%2520models%2520for%2520a%2520fixed%2520set%2520of%2520poses%2520and%2520rendering%250Athe%2520corresponding%25202D%2520images.%25202.%2520We%2520achieved%2520real-time%2520performance%2520with%2520limited%250Atraining%2520data%2520as%2520our%2520method%2520accurately%2520captures%2520the%2520nature%2520of%2520rotation%2520of%2520an%250Aobject%2520from%2520facial%2520landmarks.%2520Once%2520the%2520underlying%2520manifold%2520for%2520rotation%2520around%250Aeach%2520axis%2520is%2520learned%252C%2520the%2520model%2520is%2520very%2520fast%2520in%2520predicting%2520unseen%2520data.%2520Our%250Atraining%2520and%2520testing%2520code%2520is%2520available%2520online%2520along%2520with%2520our%2520trained%2520models%253A%250Ahttps%253A%2520//github.com/MahdiGhafoorian/NLML_HPE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NLML-HPE%3A%20Head%20Pose%20Estimation%20with%20Limited%20Data%20via%20Manifold%20Learning&entry.906535625=Mahdi%20Ghafourian%20and%20Federico%20M.%20Sukno&entry.1292438233=%20%20Head%20pose%20estimation%20%28HPE%29%20plays%20a%20critical%20role%20in%20various%20computer%20vision%0Aapplications%20such%20as%20human-computer%20interaction%20and%20facial%20recognition.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20deep%20learning%20approach%20for%20head%20pose%20estimation%20with%0Alimited%20training%20data%20via%20non-linear%20manifold%20learning%20called%20NLML-HPE.%20This%0Amethod%20is%20based%20on%20the%20combination%20of%20tensor%20decomposition%20%28i.e.%2C%20Tucker%0Adecomposition%29%20and%20feed%20forward%20neural%20networks.%20Unlike%20traditional%0Aclassification-based%20approaches%2C%20our%20method%20formulates%20head%20pose%20estimation%20as%0Aa%20regression%20problem%2C%20mapping%20input%20landmarks%20into%20a%20continuous%20representation%0Aof%20pose%20angles.%20To%20this%20end%2C%20our%20method%20uses%20tensor%20decomposition%20to%20split%20each%0AEuler%20angle%20%28yaw%2C%20pitch%2C%20roll%29%20to%20separate%20subspaces%20and%20models%20each%20dimension%0Aof%20the%20underlying%20manifold%20as%20a%20cosine%20curve.%20We%20address%20two%20key%20challenges%3A%201.%0AAlmost%20all%20HPE%20datasets%20suffer%20from%20incorrect%20and%20inaccurate%20pose%20annotations.%0AHence%2C%20we%20generated%20a%20precise%20and%20consistent%202D%20head%20pose%20dataset%20for%20our%0Atraining%20set%20by%20rotating%203D%20head%20models%20for%20a%20fixed%20set%20of%20poses%20and%20rendering%0Athe%20corresponding%202D%20images.%202.%20We%20achieved%20real-time%20performance%20with%20limited%0Atraining%20data%20as%20our%20method%20accurately%20captures%20the%20nature%20of%20rotation%20of%20an%0Aobject%20from%20facial%20landmarks.%20Once%20the%20underlying%20manifold%20for%20rotation%20around%0Aeach%20axis%20is%20learned%2C%20the%20model%20is%20very%20fast%20in%20predicting%20unseen%20data.%20Our%0Atraining%20and%20testing%20code%20is%20available%20online%20along%20with%20our%20trained%20models%3A%0Ahttps%3A%20//github.com/MahdiGhafoorian/NLML_HPE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18429v1&entry.124074799=Read"},
{"title": "Residual Prior-driven Frequency-aware Network for Image Fusion", "author": "Guan Zheng and Xue Wang and Wenhua Qian and Peng Liu and Runzhuo Ma", "abstract": "  Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.\n", "link": "http://arxiv.org/abs/2507.06735v2", "date": "2025-07-24", "relevancy": 2.6711, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5414}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Prior-driven%20Frequency-aware%20Network%20for%20Image%20Fusion&body=Title%3A%20Residual%20Prior-driven%20Frequency-aware%20Network%20for%20Image%20Fusion%0AAuthor%3A%20Guan%20Zheng%20and%20Xue%20Wang%20and%20Wenhua%20Qian%20and%20Peng%20Liu%20and%20Runzhuo%20Ma%0AAbstract%3A%20%20%20Image%20fusion%20aims%20to%20integrate%20complementary%20information%20across%20modalities%20to%0Agenerate%20high-quality%20fused%20images%2C%20thereby%20enhancing%20the%20performance%20of%0Ahigh-level%20vision%20tasks.%20While%20global%20spatial%20modeling%20mechanisms%20show%0Apromising%20results%2C%20constructing%20long-range%20feature%20dependencies%20in%20the%20spatial%0Adomain%20incurs%20substantial%20computational%20costs.%20Additionally%2C%20the%20absence%20of%0Aground-truth%20exacerbates%20the%20difficulty%20of%20capturing%20complementary%20features%0Aeffectively.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20Residual%20Prior-driven%0AFrequency-aware%20Network%2C%20termed%20as%20RPFNet.%20Specifically%2C%20RPFNet%20employs%20a%0Adual-branch%20feature%20extraction%20framework%3A%20the%20Residual%20Prior%20Module%20%28RPM%29%0Aextracts%20modality-specific%20difference%20information%20from%20residual%20maps%2C%20thereby%0Aproviding%20complementary%20priors%20for%20fusion%3B%20the%20Frequency%20Domain%20Fusion%20Module%0A%28FDFM%29%20achieves%20efficient%20global%20feature%20modeling%20and%20integration%20through%0Afrequency-domain%20convolution.%20Additionally%2C%20the%20Cross%20Promotion%20Module%20%28CPM%29%0Aenhances%20the%20synergistic%20perception%20of%20local%20details%20and%20global%20structures%0Athrough%20bidirectional%20feature%20interaction.%20During%20training%2C%20we%20incorporate%20an%0Aauxiliary%20decoder%20and%20saliency%20structure%20loss%20to%20strengthen%20the%20model%27s%0Asensitivity%20to%20modality-specific%20differences.%20Furthermore%2C%20a%20combination%20of%0Aadaptive%20weight-based%20frequency%20contrastive%20loss%20and%20SSIM%20loss%20effectively%0Aconstrains%20the%20solution%20space%2C%20facilitating%20the%20joint%20capture%20of%20local%20details%0Aand%20global%20features%20while%20ensuring%20the%20retention%20of%20complementary%20information.%0AExtensive%20experiments%20validate%20the%20fusion%20performance%20of%20RPFNet%2C%20which%0Aeffectively%20integrates%20discriminative%20features%2C%20enhances%20texture%20details%20and%0Asalient%20objects%2C%20and%20can%20effectively%20facilitate%20the%20deployment%20of%20the%0Ahigh-level%20vision%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06735v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Prior-driven%2520Frequency-aware%2520Network%2520for%2520Image%2520Fusion%26entry.906535625%3DGuan%2520Zheng%2520and%2520Xue%2520Wang%2520and%2520Wenhua%2520Qian%2520and%2520Peng%2520Liu%2520and%2520Runzhuo%2520Ma%26entry.1292438233%3D%2520%2520Image%2520fusion%2520aims%2520to%2520integrate%2520complementary%2520information%2520across%2520modalities%2520to%250Agenerate%2520high-quality%2520fused%2520images%252C%2520thereby%2520enhancing%2520the%2520performance%2520of%250Ahigh-level%2520vision%2520tasks.%2520While%2520global%2520spatial%2520modeling%2520mechanisms%2520show%250Apromising%2520results%252C%2520constructing%2520long-range%2520feature%2520dependencies%2520in%2520the%2520spatial%250Adomain%2520incurs%2520substantial%2520computational%2520costs.%2520Additionally%252C%2520the%2520absence%2520of%250Aground-truth%2520exacerbates%2520the%2520difficulty%2520of%2520capturing%2520complementary%2520features%250Aeffectively.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Residual%2520Prior-driven%250AFrequency-aware%2520Network%252C%2520termed%2520as%2520RPFNet.%2520Specifically%252C%2520RPFNet%2520employs%2520a%250Adual-branch%2520feature%2520extraction%2520framework%253A%2520the%2520Residual%2520Prior%2520Module%2520%2528RPM%2529%250Aextracts%2520modality-specific%2520difference%2520information%2520from%2520residual%2520maps%252C%2520thereby%250Aproviding%2520complementary%2520priors%2520for%2520fusion%253B%2520the%2520Frequency%2520Domain%2520Fusion%2520Module%250A%2528FDFM%2529%2520achieves%2520efficient%2520global%2520feature%2520modeling%2520and%2520integration%2520through%250Afrequency-domain%2520convolution.%2520Additionally%252C%2520the%2520Cross%2520Promotion%2520Module%2520%2528CPM%2529%250Aenhances%2520the%2520synergistic%2520perception%2520of%2520local%2520details%2520and%2520global%2520structures%250Athrough%2520bidirectional%2520feature%2520interaction.%2520During%2520training%252C%2520we%2520incorporate%2520an%250Aauxiliary%2520decoder%2520and%2520saliency%2520structure%2520loss%2520to%2520strengthen%2520the%2520model%2527s%250Asensitivity%2520to%2520modality-specific%2520differences.%2520Furthermore%252C%2520a%2520combination%2520of%250Aadaptive%2520weight-based%2520frequency%2520contrastive%2520loss%2520and%2520SSIM%2520loss%2520effectively%250Aconstrains%2520the%2520solution%2520space%252C%2520facilitating%2520the%2520joint%2520capture%2520of%2520local%2520details%250Aand%2520global%2520features%2520while%2520ensuring%2520the%2520retention%2520of%2520complementary%2520information.%250AExtensive%2520experiments%2520validate%2520the%2520fusion%2520performance%2520of%2520RPFNet%252C%2520which%250Aeffectively%2520integrates%2520discriminative%2520features%252C%2520enhances%2520texture%2520details%2520and%250Asalient%2520objects%252C%2520and%2520can%2520effectively%2520facilitate%2520the%2520deployment%2520of%2520the%250Ahigh-level%2520vision%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06735v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Prior-driven%20Frequency-aware%20Network%20for%20Image%20Fusion&entry.906535625=Guan%20Zheng%20and%20Xue%20Wang%20and%20Wenhua%20Qian%20and%20Peng%20Liu%20and%20Runzhuo%20Ma&entry.1292438233=%20%20Image%20fusion%20aims%20to%20integrate%20complementary%20information%20across%20modalities%20to%0Agenerate%20high-quality%20fused%20images%2C%20thereby%20enhancing%20the%20performance%20of%0Ahigh-level%20vision%20tasks.%20While%20global%20spatial%20modeling%20mechanisms%20show%0Apromising%20results%2C%20constructing%20long-range%20feature%20dependencies%20in%20the%20spatial%0Adomain%20incurs%20substantial%20computational%20costs.%20Additionally%2C%20the%20absence%20of%0Aground-truth%20exacerbates%20the%20difficulty%20of%20capturing%20complementary%20features%0Aeffectively.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20Residual%20Prior-driven%0AFrequency-aware%20Network%2C%20termed%20as%20RPFNet.%20Specifically%2C%20RPFNet%20employs%20a%0Adual-branch%20feature%20extraction%20framework%3A%20the%20Residual%20Prior%20Module%20%28RPM%29%0Aextracts%20modality-specific%20difference%20information%20from%20residual%20maps%2C%20thereby%0Aproviding%20complementary%20priors%20for%20fusion%3B%20the%20Frequency%20Domain%20Fusion%20Module%0A%28FDFM%29%20achieves%20efficient%20global%20feature%20modeling%20and%20integration%20through%0Afrequency-domain%20convolution.%20Additionally%2C%20the%20Cross%20Promotion%20Module%20%28CPM%29%0Aenhances%20the%20synergistic%20perception%20of%20local%20details%20and%20global%20structures%0Athrough%20bidirectional%20feature%20interaction.%20During%20training%2C%20we%20incorporate%20an%0Aauxiliary%20decoder%20and%20saliency%20structure%20loss%20to%20strengthen%20the%20model%27s%0Asensitivity%20to%20modality-specific%20differences.%20Furthermore%2C%20a%20combination%20of%0Aadaptive%20weight-based%20frequency%20contrastive%20loss%20and%20SSIM%20loss%20effectively%0Aconstrains%20the%20solution%20space%2C%20facilitating%20the%20joint%20capture%20of%20local%20details%0Aand%20global%20features%20while%20ensuring%20the%20retention%20of%20complementary%20information.%0AExtensive%20experiments%20validate%20the%20fusion%20performance%20of%20RPFNet%2C%20which%0Aeffectively%20integrates%20discriminative%20features%2C%20enhances%20texture%20details%20and%0Asalient%20objects%2C%20and%20can%20effectively%20facilitate%20the%20deployment%20of%20the%0Ahigh-level%20vision%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06735v2&entry.124074799=Read"},
{"title": "Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task\n  RL with Hybrid Rewards", "author": "Derek Li and Jiaming Zhou and Amirreza Kazemi and Qianyi Sun and Abbas Ghaddar and Mohammad Ali Alomrani and Liheng Ma and Yu Luo and Dong Li and Feng Wen and Jianye Hao and Mark Coates and Yingxue Zhang", "abstract": "  The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce\nOmni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM\nperformance across diverse tasks by combining rule-based verifiable rewards\nwith generative preference signals via LLM-as-a-Judge evaluations. Our approach\nenables consistent optimization across task types and scales RL-based training\nto subjective domains. We further investigate training strategies,\ndemonstrating that a curriculum-based progression that orders tasks from\nstructured to open-ended improves performance and reduces forgetting.\nExperimental results across four domains reveal that curriculum learning\nimproves performance by 5.2% over joint training and 9.1% over model merging.\nThese results highlight the importance of task-aware sampling and hybrid\nsupervision in scaling RL-based post-training for general-purpose LLMs.\n", "link": "http://arxiv.org/abs/2507.14783v2", "date": "2025-07-24", "relevancy": 2.6312, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Thinker%3A%20Scaling%20Cross-Domain%20Generalization%20in%20LLMs%20via%20Multi-Task%0A%20%20RL%20with%20Hybrid%20Rewards&body=Title%3A%20Omni-Thinker%3A%20Scaling%20Cross-Domain%20Generalization%20in%20LLMs%20via%20Multi-Task%0A%20%20RL%20with%20Hybrid%20Rewards%0AAuthor%3A%20Derek%20Li%20and%20Jiaming%20Zhou%20and%20Amirreza%20Kazemi%20and%20Qianyi%20Sun%20and%20Abbas%20Ghaddar%20and%20Mohammad%20Ali%20Alomrani%20and%20Liheng%20Ma%20and%20Yu%20Luo%20and%20Dong%20Li%20and%20Feng%20Wen%20and%20Jianye%20Hao%20and%20Mark%20Coates%20and%20Yingxue%20Zhang%0AAbstract%3A%20%20%20The%20advancement%20of%20general-purpose%20artificial%20intelligence%20relies%20on%20large%0Alanguage%20models%20%28LLMs%29%20that%20excel%20across%20a%20wide%20range%20of%20tasks%2C%20from%20structured%0Areasoning%20to%20creative%20generation.%20However%2C%20post-training%20methods%20like%0ASupervised%20Fine-Tuning%20%28SFT%29%20often%20struggle%20with%20generalization%2C%20favoring%0Amemorization%20over%20transferable%20learning.%20In%20this%20work%2C%20we%20introduce%0AOmni-Thinker%2C%20a%20unified%20reinforcement%20learning%20%28RL%29%20framework%20that%20enhances%20LLM%0Aperformance%20across%20diverse%20tasks%20by%20combining%20rule-based%20verifiable%20rewards%0Awith%20generative%20preference%20signals%20via%20LLM-as-a-Judge%20evaluations.%20Our%20approach%0Aenables%20consistent%20optimization%20across%20task%20types%20and%20scales%20RL-based%20training%0Ato%20subjective%20domains.%20We%20further%20investigate%20training%20strategies%2C%0Ademonstrating%20that%20a%20curriculum-based%20progression%20that%20orders%20tasks%20from%0Astructured%20to%20open-ended%20improves%20performance%20and%20reduces%20forgetting.%0AExperimental%20results%20across%20four%20domains%20reveal%20that%20curriculum%20learning%0Aimproves%20performance%20by%205.2%25%20over%20joint%20training%20and%209.1%25%20over%20model%20merging.%0AThese%20results%20highlight%20the%20importance%20of%20task-aware%20sampling%20and%20hybrid%0Asupervision%20in%20scaling%20RL-based%20post-training%20for%20general-purpose%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Thinker%253A%2520Scaling%2520Cross-Domain%2520Generalization%2520in%2520LLMs%2520via%2520Multi-Task%250A%2520%2520RL%2520with%2520Hybrid%2520Rewards%26entry.906535625%3DDerek%2520Li%2520and%2520Jiaming%2520Zhou%2520and%2520Amirreza%2520Kazemi%2520and%2520Qianyi%2520Sun%2520and%2520Abbas%2520Ghaddar%2520and%2520Mohammad%2520Ali%2520Alomrani%2520and%2520Liheng%2520Ma%2520and%2520Yu%2520Luo%2520and%2520Dong%2520Li%2520and%2520Feng%2520Wen%2520and%2520Jianye%2520Hao%2520and%2520Mark%2520Coates%2520and%2520Yingxue%2520Zhang%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520general-purpose%2520artificial%2520intelligence%2520relies%2520on%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520that%2520excel%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520from%2520structured%250Areasoning%2520to%2520creative%2520generation.%2520However%252C%2520post-training%2520methods%2520like%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520often%2520struggle%2520with%2520generalization%252C%2520favoring%250Amemorization%2520over%2520transferable%2520learning.%2520In%2520this%2520work%252C%2520we%2520introduce%250AOmni-Thinker%252C%2520a%2520unified%2520reinforcement%2520learning%2520%2528RL%2529%2520framework%2520that%2520enhances%2520LLM%250Aperformance%2520across%2520diverse%2520tasks%2520by%2520combining%2520rule-based%2520verifiable%2520rewards%250Awith%2520generative%2520preference%2520signals%2520via%2520LLM-as-a-Judge%2520evaluations.%2520Our%2520approach%250Aenables%2520consistent%2520optimization%2520across%2520task%2520types%2520and%2520scales%2520RL-based%2520training%250Ato%2520subjective%2520domains.%2520We%2520further%2520investigate%2520training%2520strategies%252C%250Ademonstrating%2520that%2520a%2520curriculum-based%2520progression%2520that%2520orders%2520tasks%2520from%250Astructured%2520to%2520open-ended%2520improves%2520performance%2520and%2520reduces%2520forgetting.%250AExperimental%2520results%2520across%2520four%2520domains%2520reveal%2520that%2520curriculum%2520learning%250Aimproves%2520performance%2520by%25205.2%2525%2520over%2520joint%2520training%2520and%25209.1%2525%2520over%2520model%2520merging.%250AThese%2520results%2520highlight%2520the%2520importance%2520of%2520task-aware%2520sampling%2520and%2520hybrid%250Asupervision%2520in%2520scaling%2520RL-based%2520post-training%2520for%2520general-purpose%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Thinker%3A%20Scaling%20Cross-Domain%20Generalization%20in%20LLMs%20via%20Multi-Task%0A%20%20RL%20with%20Hybrid%20Rewards&entry.906535625=Derek%20Li%20and%20Jiaming%20Zhou%20and%20Amirreza%20Kazemi%20and%20Qianyi%20Sun%20and%20Abbas%20Ghaddar%20and%20Mohammad%20Ali%20Alomrani%20and%20Liheng%20Ma%20and%20Yu%20Luo%20and%20Dong%20Li%20and%20Feng%20Wen%20and%20Jianye%20Hao%20and%20Mark%20Coates%20and%20Yingxue%20Zhang&entry.1292438233=%20%20The%20advancement%20of%20general-purpose%20artificial%20intelligence%20relies%20on%20large%0Alanguage%20models%20%28LLMs%29%20that%20excel%20across%20a%20wide%20range%20of%20tasks%2C%20from%20structured%0Areasoning%20to%20creative%20generation.%20However%2C%20post-training%20methods%20like%0ASupervised%20Fine-Tuning%20%28SFT%29%20often%20struggle%20with%20generalization%2C%20favoring%0Amemorization%20over%20transferable%20learning.%20In%20this%20work%2C%20we%20introduce%0AOmni-Thinker%2C%20a%20unified%20reinforcement%20learning%20%28RL%29%20framework%20that%20enhances%20LLM%0Aperformance%20across%20diverse%20tasks%20by%20combining%20rule-based%20verifiable%20rewards%0Awith%20generative%20preference%20signals%20via%20LLM-as-a-Judge%20evaluations.%20Our%20approach%0Aenables%20consistent%20optimization%20across%20task%20types%20and%20scales%20RL-based%20training%0Ato%20subjective%20domains.%20We%20further%20investigate%20training%20strategies%2C%0Ademonstrating%20that%20a%20curriculum-based%20progression%20that%20orders%20tasks%20from%0Astructured%20to%20open-ended%20improves%20performance%20and%20reduces%20forgetting.%0AExperimental%20results%20across%20four%20domains%20reveal%20that%20curriculum%20learning%0Aimproves%20performance%20by%205.2%25%20over%20joint%20training%20and%209.1%25%20over%20model%20merging.%0AThese%20results%20highlight%20the%20importance%20of%20task-aware%20sampling%20and%20hybrid%0Asupervision%20in%20scaling%20RL-based%20post-training%20for%20general-purpose%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14783v2&entry.124074799=Read"},
{"title": "Scaling RL to Long Videos", "author": "Yukang Chen and Wei Huang and Baifeng Shi and Qinghao Hu and Hanrong Ye and Ligeng Zhu and Zhijian Liu and Pavlo Molchanov and Jan Kautz and Xiaojuan Qi and Sifei Liu and Hongxu Yin and Yao Lu and Song Han", "abstract": "  We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens).\n", "link": "http://arxiv.org/abs/2507.07966v2", "date": "2025-07-24", "relevancy": 2.6276, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20RL%20to%20Long%20Videos&body=Title%3A%20Scaling%20RL%20to%20Long%20Videos%0AAuthor%3A%20Yukang%20Chen%20and%20Wei%20Huang%20and%20Baifeng%20Shi%20and%20Qinghao%20Hu%20and%20Hanrong%20Ye%20and%20Ligeng%20Zhu%20and%20Zhijian%20Liu%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Xiaojuan%20Qi%20and%20Sifei%20Liu%20and%20Hongxu%20Yin%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20We%20introduce%20a%20full-stack%20framework%20that%20scales%20up%20reasoning%20in%0Avision-language%20models%20%28VLMs%29%20to%20long%20videos%2C%20leveraging%20reinforcement%0Alearning.%20We%20address%20the%20unique%20challenges%20of%20long%20video%20reasoning%20by%0Aintegrating%20three%20critical%20components%3A%20%281%29%20a%20large-scale%20dataset%2C%0ALongVideo-Reason%2C%20comprising%20104K%20long%20video%20QA%20pairs%20with%20high-quality%0Areasoning%20annotations%20across%20diverse%20domains%20such%20as%20sports%2C%20games%2C%20and%20vlogs%3B%0A%282%29%20a%20two-stage%20training%20pipeline%20that%20extends%20VLMs%20with%20chain-of-thought%0Asupervised%20fine-tuning%20%28CoT-SFT%29%20and%20reinforcement%20learning%20%28RL%29%3B%20and%20%283%29%20a%0Atraining%20infrastructure%20for%20long%20video%20RL%2C%20named%20Multi-modal%20Reinforcement%0ASequence%20Parallelism%20%28MR-SP%29%2C%20which%20incorporates%20sequence%20parallelism%20and%20a%0AvLLM-based%20engine%20tailored%20for%20long%20video%2C%20using%20cached%20video%20embeddings%20for%0Aefficient%20rollout%20and%20prefilling.%20In%20our%20experiments%2C%20LongVILA-R1-7B%20achieves%0Astrong%20performance%20on%20video%20benchmarks%2C%20reaching%2065.0%25%20and%2070.7%25%20accuracy%20on%0AVideoMME%20without%20and%20with%20subtitles%2C%20respectively%2C%20and%20consistently%0Aoutperforming%20LongVILA-R1%20across%20multiple%20benchmarks.%20Moreover%2C%20LongVILA-R1%0Ashows%20steady%20performance%20improvements%20as%20the%20number%20of%20input%20video%20frames%0Aincreases.%20Notably%2C%20our%20MR-SP%20system%20achieves%20up%20to%202.1x%20speedup%20on%20long%20video%0ARL%20training.%20In%20addition%2C%20we%20release%20our%20training%20system%20for%20public%0Aavailability%20that%20supports%20RL%20training%20on%20various%20modalities%20%28video%2C%20text%2C%20and%0Aaudio%29%2C%20various%20models%20%28VILA%20and%20Qwen%20series%29%2C%20and%20even%20image%20and%20video%0Ageneration%20models.%20On%20a%20single%20A100%20node%20%288%20GPUs%29%2C%20it%20supports%20RL%20training%20on%0Ahour-long%20videos%20%28e.g.%2C%203%2C600%20frames%20/%20around%20256k%20tokens%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07966v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520RL%2520to%2520Long%2520Videos%26entry.906535625%3DYukang%2520Chen%2520and%2520Wei%2520Huang%2520and%2520Baifeng%2520Shi%2520and%2520Qinghao%2520Hu%2520and%2520Hanrong%2520Ye%2520and%2520Ligeng%2520Zhu%2520and%2520Zhijian%2520Liu%2520and%2520Pavlo%2520Molchanov%2520and%2520Jan%2520Kautz%2520and%2520Xiaojuan%2520Qi%2520and%2520Sifei%2520Liu%2520and%2520Hongxu%2520Yin%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520full-stack%2520framework%2520that%2520scales%2520up%2520reasoning%2520in%250Avision-language%2520models%2520%2528VLMs%2529%2520to%2520long%2520videos%252C%2520leveraging%2520reinforcement%250Alearning.%2520We%2520address%2520the%2520unique%2520challenges%2520of%2520long%2520video%2520reasoning%2520by%250Aintegrating%2520three%2520critical%2520components%253A%2520%25281%2529%2520a%2520large-scale%2520dataset%252C%250ALongVideo-Reason%252C%2520comprising%2520104K%2520long%2520video%2520QA%2520pairs%2520with%2520high-quality%250Areasoning%2520annotations%2520across%2520diverse%2520domains%2520such%2520as%2520sports%252C%2520games%252C%2520and%2520vlogs%253B%250A%25282%2529%2520a%2520two-stage%2520training%2520pipeline%2520that%2520extends%2520VLMs%2520with%2520chain-of-thought%250Asupervised%2520fine-tuning%2520%2528CoT-SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529%253B%2520and%2520%25283%2529%2520a%250Atraining%2520infrastructure%2520for%2520long%2520video%2520RL%252C%2520named%2520Multi-modal%2520Reinforcement%250ASequence%2520Parallelism%2520%2528MR-SP%2529%252C%2520which%2520incorporates%2520sequence%2520parallelism%2520and%2520a%250AvLLM-based%2520engine%2520tailored%2520for%2520long%2520video%252C%2520using%2520cached%2520video%2520embeddings%2520for%250Aefficient%2520rollout%2520and%2520prefilling.%2520In%2520our%2520experiments%252C%2520LongVILA-R1-7B%2520achieves%250Astrong%2520performance%2520on%2520video%2520benchmarks%252C%2520reaching%252065.0%2525%2520and%252070.7%2525%2520accuracy%2520on%250AVideoMME%2520without%2520and%2520with%2520subtitles%252C%2520respectively%252C%2520and%2520consistently%250Aoutperforming%2520LongVILA-R1%2520across%2520multiple%2520benchmarks.%2520Moreover%252C%2520LongVILA-R1%250Ashows%2520steady%2520performance%2520improvements%2520as%2520the%2520number%2520of%2520input%2520video%2520frames%250Aincreases.%2520Notably%252C%2520our%2520MR-SP%2520system%2520achieves%2520up%2520to%25202.1x%2520speedup%2520on%2520long%2520video%250ARL%2520training.%2520In%2520addition%252C%2520we%2520release%2520our%2520training%2520system%2520for%2520public%250Aavailability%2520that%2520supports%2520RL%2520training%2520on%2520various%2520modalities%2520%2528video%252C%2520text%252C%2520and%250Aaudio%2529%252C%2520various%2520models%2520%2528VILA%2520and%2520Qwen%2520series%2529%252C%2520and%2520even%2520image%2520and%2520video%250Ageneration%2520models.%2520On%2520a%2520single%2520A100%2520node%2520%25288%2520GPUs%2529%252C%2520it%2520supports%2520RL%2520training%2520on%250Ahour-long%2520videos%2520%2528e.g.%252C%25203%252C600%2520frames%2520/%2520around%2520256k%2520tokens%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07966v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20RL%20to%20Long%20Videos&entry.906535625=Yukang%20Chen%20and%20Wei%20Huang%20and%20Baifeng%20Shi%20and%20Qinghao%20Hu%20and%20Hanrong%20Ye%20and%20Ligeng%20Zhu%20and%20Zhijian%20Liu%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Xiaojuan%20Qi%20and%20Sifei%20Liu%20and%20Hongxu%20Yin%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20We%20introduce%20a%20full-stack%20framework%20that%20scales%20up%20reasoning%20in%0Avision-language%20models%20%28VLMs%29%20to%20long%20videos%2C%20leveraging%20reinforcement%0Alearning.%20We%20address%20the%20unique%20challenges%20of%20long%20video%20reasoning%20by%0Aintegrating%20three%20critical%20components%3A%20%281%29%20a%20large-scale%20dataset%2C%0ALongVideo-Reason%2C%20comprising%20104K%20long%20video%20QA%20pairs%20with%20high-quality%0Areasoning%20annotations%20across%20diverse%20domains%20such%20as%20sports%2C%20games%2C%20and%20vlogs%3B%0A%282%29%20a%20two-stage%20training%20pipeline%20that%20extends%20VLMs%20with%20chain-of-thought%0Asupervised%20fine-tuning%20%28CoT-SFT%29%20and%20reinforcement%20learning%20%28RL%29%3B%20and%20%283%29%20a%0Atraining%20infrastructure%20for%20long%20video%20RL%2C%20named%20Multi-modal%20Reinforcement%0ASequence%20Parallelism%20%28MR-SP%29%2C%20which%20incorporates%20sequence%20parallelism%20and%20a%0AvLLM-based%20engine%20tailored%20for%20long%20video%2C%20using%20cached%20video%20embeddings%20for%0Aefficient%20rollout%20and%20prefilling.%20In%20our%20experiments%2C%20LongVILA-R1-7B%20achieves%0Astrong%20performance%20on%20video%20benchmarks%2C%20reaching%2065.0%25%20and%2070.7%25%20accuracy%20on%0AVideoMME%20without%20and%20with%20subtitles%2C%20respectively%2C%20and%20consistently%0Aoutperforming%20LongVILA-R1%20across%20multiple%20benchmarks.%20Moreover%2C%20LongVILA-R1%0Ashows%20steady%20performance%20improvements%20as%20the%20number%20of%20input%20video%20frames%0Aincreases.%20Notably%2C%20our%20MR-SP%20system%20achieves%20up%20to%202.1x%20speedup%20on%20long%20video%0ARL%20training.%20In%20addition%2C%20we%20release%20our%20training%20system%20for%20public%0Aavailability%20that%20supports%20RL%20training%20on%20various%20modalities%20%28video%2C%20text%2C%20and%0Aaudio%29%2C%20various%20models%20%28VILA%20and%20Qwen%20series%29%2C%20and%20even%20image%20and%20video%0Ageneration%20models.%20On%20a%20single%20A100%20node%20%288%20GPUs%29%2C%20it%20supports%20RL%20training%20on%0Ahour-long%20videos%20%28e.g.%2C%203%2C600%20frames%20/%20around%20256k%20tokens%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07966v2&entry.124074799=Read"},
{"title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many\n  Mapping for Zero-shot Image Captioning", "author": "Si-Woo Kim and MinJu Jeon and Ye-Chan Kim and Soeun Lee and Taewhan Kim and Dong-Jin Kim", "abstract": "  Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets\ngenerated by text-to-image (T2I) models to mitigate the need for costly manual\nannotation. However, these T2I models often produce images that exhibit\nsemantic misalignments with their corresponding input captions (e.g., missing\nobjects, incorrect attributes), resulting in noisy synthetic image-caption\npairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these\nmethods are ill-suited for the distinct challenges of synthetic data, where\ncaptions are typically well-formed, but images may be inaccurate\nrepresentations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC.\nInstead of conventional filtering or regeneration, SynC focuses on reassigning\ncaptions to the most semantically aligned images already present within the\nsynthetic image pool. Our approach employs a one-to-many mapping strategy by\ninitially retrieving multiple relevant candidate images for each caption. We\nthen apply a cycle-consistency-inspired alignment scorer that selects the best\nimage by verifying its ability to retrieve the original caption via\nimage-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models\non standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art\nresults in several scenarios. SynC offers an effective strategy for curating\nrefined synthetic data to enhance ZIC.\n", "link": "http://arxiv.org/abs/2507.18616v1", "date": "2025-07-24", "relevancy": 2.5993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5258}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5202}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynC%3A%20Synthetic%20Image%20Caption%20Dataset%20Refinement%20with%20One-to-many%0A%20%20Mapping%20for%20Zero-shot%20Image%20Captioning&body=Title%3A%20SynC%3A%20Synthetic%20Image%20Caption%20Dataset%20Refinement%20with%20One-to-many%0A%20%20Mapping%20for%20Zero-shot%20Image%20Captioning%0AAuthor%3A%20Si-Woo%20Kim%20and%20MinJu%20Jeon%20and%20Ye-Chan%20Kim%20and%20Soeun%20Lee%20and%20Taewhan%20Kim%20and%20Dong-Jin%20Kim%0AAbstract%3A%20%20%20Zero-shot%20Image%20Captioning%20%28ZIC%29%20increasingly%20utilizes%20synthetic%20datasets%0Agenerated%20by%20text-to-image%20%28T2I%29%20models%20to%20mitigate%20the%20need%20for%20costly%20manual%0Aannotation.%20However%2C%20these%20T2I%20models%20often%20produce%20images%20that%20exhibit%0Asemantic%20misalignments%20with%20their%20corresponding%20input%20captions%20%28e.g.%2C%20missing%0Aobjects%2C%20incorrect%20attributes%29%2C%20resulting%20in%20noisy%20synthetic%20image-caption%0Apairs%20that%20can%20hinder%20model%20training.%20Existing%20dataset%20pruning%20techniques%20are%0Alargely%20designed%20for%20removing%20noisy%20text%20in%20web-crawled%20data.%20However%2C%20these%0Amethods%20are%20ill-suited%20for%20the%20distinct%20challenges%20of%20synthetic%20data%2C%20where%0Acaptions%20are%20typically%20well-formed%2C%20but%20images%20may%20be%20inaccurate%0Arepresentations.%20To%20address%20this%20gap%2C%20we%20introduce%20SynC%2C%20a%20novel%20framework%0Aspecifically%20designed%20to%20refine%20synthetic%20image-caption%20datasets%20for%20ZIC.%0AInstead%20of%20conventional%20filtering%20or%20regeneration%2C%20SynC%20focuses%20on%20reassigning%0Acaptions%20to%20the%20most%20semantically%20aligned%20images%20already%20present%20within%20the%0Asynthetic%20image%20pool.%20Our%20approach%20employs%20a%20one-to-many%20mapping%20strategy%20by%0Ainitially%20retrieving%20multiple%20relevant%20candidate%20images%20for%20each%20caption.%20We%0Athen%20apply%20a%20cycle-consistency-inspired%20alignment%20scorer%20that%20selects%20the%20best%0Aimage%20by%20verifying%20its%20ability%20to%20retrieve%20the%20original%20caption%20via%0Aimage-to-text%20retrieval.%20Extensive%20evaluations%20demonstrate%20that%20SynC%0Aconsistently%20and%20significantly%20improves%20performance%20across%20various%20ZIC%20models%0Aon%20standard%20benchmarks%20%28MS-COCO%2C%20Flickr30k%2C%20NoCaps%29%2C%20achieving%20state-of-the-art%0Aresults%20in%20several%20scenarios.%20SynC%20offers%20an%20effective%20strategy%20for%20curating%0Arefined%20synthetic%20data%20to%20enhance%20ZIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynC%253A%2520Synthetic%2520Image%2520Caption%2520Dataset%2520Refinement%2520with%2520One-to-many%250A%2520%2520Mapping%2520for%2520Zero-shot%2520Image%2520Captioning%26entry.906535625%3DSi-Woo%2520Kim%2520and%2520MinJu%2520Jeon%2520and%2520Ye-Chan%2520Kim%2520and%2520Soeun%2520Lee%2520and%2520Taewhan%2520Kim%2520and%2520Dong-Jin%2520Kim%26entry.1292438233%3D%2520%2520Zero-shot%2520Image%2520Captioning%2520%2528ZIC%2529%2520increasingly%2520utilizes%2520synthetic%2520datasets%250Agenerated%2520by%2520text-to-image%2520%2528T2I%2529%2520models%2520to%2520mitigate%2520the%2520need%2520for%2520costly%2520manual%250Aannotation.%2520However%252C%2520these%2520T2I%2520models%2520often%2520produce%2520images%2520that%2520exhibit%250Asemantic%2520misalignments%2520with%2520their%2520corresponding%2520input%2520captions%2520%2528e.g.%252C%2520missing%250Aobjects%252C%2520incorrect%2520attributes%2529%252C%2520resulting%2520in%2520noisy%2520synthetic%2520image-caption%250Apairs%2520that%2520can%2520hinder%2520model%2520training.%2520Existing%2520dataset%2520pruning%2520techniques%2520are%250Alargely%2520designed%2520for%2520removing%2520noisy%2520text%2520in%2520web-crawled%2520data.%2520However%252C%2520these%250Amethods%2520are%2520ill-suited%2520for%2520the%2520distinct%2520challenges%2520of%2520synthetic%2520data%252C%2520where%250Acaptions%2520are%2520typically%2520well-formed%252C%2520but%2520images%2520may%2520be%2520inaccurate%250Arepresentations.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520SynC%252C%2520a%2520novel%2520framework%250Aspecifically%2520designed%2520to%2520refine%2520synthetic%2520image-caption%2520datasets%2520for%2520ZIC.%250AInstead%2520of%2520conventional%2520filtering%2520or%2520regeneration%252C%2520SynC%2520focuses%2520on%2520reassigning%250Acaptions%2520to%2520the%2520most%2520semantically%2520aligned%2520images%2520already%2520present%2520within%2520the%250Asynthetic%2520image%2520pool.%2520Our%2520approach%2520employs%2520a%2520one-to-many%2520mapping%2520strategy%2520by%250Ainitially%2520retrieving%2520multiple%2520relevant%2520candidate%2520images%2520for%2520each%2520caption.%2520We%250Athen%2520apply%2520a%2520cycle-consistency-inspired%2520alignment%2520scorer%2520that%2520selects%2520the%2520best%250Aimage%2520by%2520verifying%2520its%2520ability%2520to%2520retrieve%2520the%2520original%2520caption%2520via%250Aimage-to-text%2520retrieval.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520SynC%250Aconsistently%2520and%2520significantly%2520improves%2520performance%2520across%2520various%2520ZIC%2520models%250Aon%2520standard%2520benchmarks%2520%2528MS-COCO%252C%2520Flickr30k%252C%2520NoCaps%2529%252C%2520achieving%2520state-of-the-art%250Aresults%2520in%2520several%2520scenarios.%2520SynC%2520offers%2520an%2520effective%2520strategy%2520for%2520curating%250Arefined%2520synthetic%2520data%2520to%2520enhance%2520ZIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynC%3A%20Synthetic%20Image%20Caption%20Dataset%20Refinement%20with%20One-to-many%0A%20%20Mapping%20for%20Zero-shot%20Image%20Captioning&entry.906535625=Si-Woo%20Kim%20and%20MinJu%20Jeon%20and%20Ye-Chan%20Kim%20and%20Soeun%20Lee%20and%20Taewhan%20Kim%20and%20Dong-Jin%20Kim&entry.1292438233=%20%20Zero-shot%20Image%20Captioning%20%28ZIC%29%20increasingly%20utilizes%20synthetic%20datasets%0Agenerated%20by%20text-to-image%20%28T2I%29%20models%20to%20mitigate%20the%20need%20for%20costly%20manual%0Aannotation.%20However%2C%20these%20T2I%20models%20often%20produce%20images%20that%20exhibit%0Asemantic%20misalignments%20with%20their%20corresponding%20input%20captions%20%28e.g.%2C%20missing%0Aobjects%2C%20incorrect%20attributes%29%2C%20resulting%20in%20noisy%20synthetic%20image-caption%0Apairs%20that%20can%20hinder%20model%20training.%20Existing%20dataset%20pruning%20techniques%20are%0Alargely%20designed%20for%20removing%20noisy%20text%20in%20web-crawled%20data.%20However%2C%20these%0Amethods%20are%20ill-suited%20for%20the%20distinct%20challenges%20of%20synthetic%20data%2C%20where%0Acaptions%20are%20typically%20well-formed%2C%20but%20images%20may%20be%20inaccurate%0Arepresentations.%20To%20address%20this%20gap%2C%20we%20introduce%20SynC%2C%20a%20novel%20framework%0Aspecifically%20designed%20to%20refine%20synthetic%20image-caption%20datasets%20for%20ZIC.%0AInstead%20of%20conventional%20filtering%20or%20regeneration%2C%20SynC%20focuses%20on%20reassigning%0Acaptions%20to%20the%20most%20semantically%20aligned%20images%20already%20present%20within%20the%0Asynthetic%20image%20pool.%20Our%20approach%20employs%20a%20one-to-many%20mapping%20strategy%20by%0Ainitially%20retrieving%20multiple%20relevant%20candidate%20images%20for%20each%20caption.%20We%0Athen%20apply%20a%20cycle-consistency-inspired%20alignment%20scorer%20that%20selects%20the%20best%0Aimage%20by%20verifying%20its%20ability%20to%20retrieve%20the%20original%20caption%20via%0Aimage-to-text%20retrieval.%20Extensive%20evaluations%20demonstrate%20that%20SynC%0Aconsistently%20and%20significantly%20improves%20performance%20across%20various%20ZIC%20models%0Aon%20standard%20benchmarks%20%28MS-COCO%2C%20Flickr30k%2C%20NoCaps%29%2C%20achieving%20state-of-the-art%0Aresults%20in%20several%20scenarios.%20SynC%20offers%20an%20effective%20strategy%20for%20curating%0Arefined%20synthetic%20data%20to%20enhance%20ZIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18616v1&entry.124074799=Read"},
{"title": "External Knowledge Injection for CLIP-Based Class-Incremental Learning", "author": "Da-Wei Zhou and Kai-Wen Li and Jingyi Ning and Han-Jia Ye and Lijun Zhang and De-Chuan Zhan", "abstract": "  Class-Incremental Learning (CIL) enables learning systems to continuously\nadapt to evolving data streams. With the advancement of pre-training,\nleveraging pre-trained vision-language models (e.g., CLIP) offers a promising\nstarting point for CIL. However, CLIP makes decisions by matching visual\nembeddings to class names, overlooking the rich contextual information conveyed\nthrough language. For instance, the concept of ``cat'' can be decomposed into\nfeatures like tail, fur, and face for recognition. Besides, since the model is\ncontinually updated, these detailed features are overwritten in CIL, requiring\nexternal knowledge for compensation. In this paper, we introduce ExterNal\nknowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer\nfrom outside the dataset, we propose a dual-branch injection tuning framework\nthat encodes informative knowledge from both visual and textual modalities. The\nvisual branch is enhanced with data augmentation to enrich the visual features,\nwhile the textual branch leverages GPT-4 to rewrite discriminative descriptors.\nIn addition to this on-the-fly knowledge injection, we also implement\npost-tuning knowledge by re-ranking the prediction results during inference.\nWith the injected knowledge, the model can better capture informative features\nfor downstream tasks as data evolves. Extensive experiments demonstrate the\nstate-of-the-art performance of ENGINE. Code is available at:\nhttps://github.com/LAMDA-CL/ICCV25-ENGINE\n", "link": "http://arxiv.org/abs/2503.08510v2", "date": "2025-07-24", "relevancy": 2.5985, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5461}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20External%20Knowledge%20Injection%20for%20CLIP-Based%20Class-Incremental%20Learning&body=Title%3A%20External%20Knowledge%20Injection%20for%20CLIP-Based%20Class-Incremental%20Learning%0AAuthor%3A%20Da-Wei%20Zhou%20and%20Kai-Wen%20Li%20and%20Jingyi%20Ning%20and%20Han-Jia%20Ye%20and%20Lijun%20Zhang%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20Class-Incremental%20Learning%20%28CIL%29%20enables%20learning%20systems%20to%20continuously%0Aadapt%20to%20evolving%20data%20streams.%20With%20the%20advancement%20of%20pre-training%2C%0Aleveraging%20pre-trained%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20offers%20a%20promising%0Astarting%20point%20for%20CIL.%20However%2C%20CLIP%20makes%20decisions%20by%20matching%20visual%0Aembeddings%20to%20class%20names%2C%20overlooking%20the%20rich%20contextual%20information%20conveyed%0Athrough%20language.%20For%20instance%2C%20the%20concept%20of%20%60%60cat%27%27%20can%20be%20decomposed%20into%0Afeatures%20like%20tail%2C%20fur%2C%20and%20face%20for%20recognition.%20Besides%2C%20since%20the%20model%20is%0Acontinually%20updated%2C%20these%20detailed%20features%20are%20overwritten%20in%20CIL%2C%20requiring%0Aexternal%20knowledge%20for%20compensation.%20In%20this%20paper%2C%20we%20introduce%20ExterNal%0AknowledGe%20INjEction%20%28ENGINE%29%20for%20CLIP-based%20CIL.%20To%20enhance%20knowledge%20transfer%0Afrom%20outside%20the%20dataset%2C%20we%20propose%20a%20dual-branch%20injection%20tuning%20framework%0Athat%20encodes%20informative%20knowledge%20from%20both%20visual%20and%20textual%20modalities.%20The%0Avisual%20branch%20is%20enhanced%20with%20data%20augmentation%20to%20enrich%20the%20visual%20features%2C%0Awhile%20the%20textual%20branch%20leverages%20GPT-4%20to%20rewrite%20discriminative%20descriptors.%0AIn%20addition%20to%20this%20on-the-fly%20knowledge%20injection%2C%20we%20also%20implement%0Apost-tuning%20knowledge%20by%20re-ranking%20the%20prediction%20results%20during%20inference.%0AWith%20the%20injected%20knowledge%2C%20the%20model%20can%20better%20capture%20informative%20features%0Afor%20downstream%20tasks%20as%20data%20evolves.%20Extensive%20experiments%20demonstrate%20the%0Astate-of-the-art%20performance%20of%20ENGINE.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/LAMDA-CL/ICCV25-ENGINE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08510v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExternal%2520Knowledge%2520Injection%2520for%2520CLIP-Based%2520Class-Incremental%2520Learning%26entry.906535625%3DDa-Wei%2520Zhou%2520and%2520Kai-Wen%2520Li%2520and%2520Jingyi%2520Ning%2520and%2520Han-Jia%2520Ye%2520and%2520Lijun%2520Zhang%2520and%2520De-Chuan%2520Zhan%26entry.1292438233%3D%2520%2520Class-Incremental%2520Learning%2520%2528CIL%2529%2520enables%2520learning%2520systems%2520to%2520continuously%250Aadapt%2520to%2520evolving%2520data%2520streams.%2520With%2520the%2520advancement%2520of%2520pre-training%252C%250Aleveraging%2520pre-trained%2520vision-language%2520models%2520%2528e.g.%252C%2520CLIP%2529%2520offers%2520a%2520promising%250Astarting%2520point%2520for%2520CIL.%2520However%252C%2520CLIP%2520makes%2520decisions%2520by%2520matching%2520visual%250Aembeddings%2520to%2520class%2520names%252C%2520overlooking%2520the%2520rich%2520contextual%2520information%2520conveyed%250Athrough%2520language.%2520For%2520instance%252C%2520the%2520concept%2520of%2520%2560%2560cat%2527%2527%2520can%2520be%2520decomposed%2520into%250Afeatures%2520like%2520tail%252C%2520fur%252C%2520and%2520face%2520for%2520recognition.%2520Besides%252C%2520since%2520the%2520model%2520is%250Acontinually%2520updated%252C%2520these%2520detailed%2520features%2520are%2520overwritten%2520in%2520CIL%252C%2520requiring%250Aexternal%2520knowledge%2520for%2520compensation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520ExterNal%250AknowledGe%2520INjEction%2520%2528ENGINE%2529%2520for%2520CLIP-based%2520CIL.%2520To%2520enhance%2520knowledge%2520transfer%250Afrom%2520outside%2520the%2520dataset%252C%2520we%2520propose%2520a%2520dual-branch%2520injection%2520tuning%2520framework%250Athat%2520encodes%2520informative%2520knowledge%2520from%2520both%2520visual%2520and%2520textual%2520modalities.%2520The%250Avisual%2520branch%2520is%2520enhanced%2520with%2520data%2520augmentation%2520to%2520enrich%2520the%2520visual%2520features%252C%250Awhile%2520the%2520textual%2520branch%2520leverages%2520GPT-4%2520to%2520rewrite%2520discriminative%2520descriptors.%250AIn%2520addition%2520to%2520this%2520on-the-fly%2520knowledge%2520injection%252C%2520we%2520also%2520implement%250Apost-tuning%2520knowledge%2520by%2520re-ranking%2520the%2520prediction%2520results%2520during%2520inference.%250AWith%2520the%2520injected%2520knowledge%252C%2520the%2520model%2520can%2520better%2520capture%2520informative%2520features%250Afor%2520downstream%2520tasks%2520as%2520data%2520evolves.%2520Extensive%2520experiments%2520demonstrate%2520the%250Astate-of-the-art%2520performance%2520of%2520ENGINE.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/LAMDA-CL/ICCV25-ENGINE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08510v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=External%20Knowledge%20Injection%20for%20CLIP-Based%20Class-Incremental%20Learning&entry.906535625=Da-Wei%20Zhou%20and%20Kai-Wen%20Li%20and%20Jingyi%20Ning%20and%20Han-Jia%20Ye%20and%20Lijun%20Zhang%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20Class-Incremental%20Learning%20%28CIL%29%20enables%20learning%20systems%20to%20continuously%0Aadapt%20to%20evolving%20data%20streams.%20With%20the%20advancement%20of%20pre-training%2C%0Aleveraging%20pre-trained%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20offers%20a%20promising%0Astarting%20point%20for%20CIL.%20However%2C%20CLIP%20makes%20decisions%20by%20matching%20visual%0Aembeddings%20to%20class%20names%2C%20overlooking%20the%20rich%20contextual%20information%20conveyed%0Athrough%20language.%20For%20instance%2C%20the%20concept%20of%20%60%60cat%27%27%20can%20be%20decomposed%20into%0Afeatures%20like%20tail%2C%20fur%2C%20and%20face%20for%20recognition.%20Besides%2C%20since%20the%20model%20is%0Acontinually%20updated%2C%20these%20detailed%20features%20are%20overwritten%20in%20CIL%2C%20requiring%0Aexternal%20knowledge%20for%20compensation.%20In%20this%20paper%2C%20we%20introduce%20ExterNal%0AknowledGe%20INjEction%20%28ENGINE%29%20for%20CLIP-based%20CIL.%20To%20enhance%20knowledge%20transfer%0Afrom%20outside%20the%20dataset%2C%20we%20propose%20a%20dual-branch%20injection%20tuning%20framework%0Athat%20encodes%20informative%20knowledge%20from%20both%20visual%20and%20textual%20modalities.%20The%0Avisual%20branch%20is%20enhanced%20with%20data%20augmentation%20to%20enrich%20the%20visual%20features%2C%0Awhile%20the%20textual%20branch%20leverages%20GPT-4%20to%20rewrite%20discriminative%20descriptors.%0AIn%20addition%20to%20this%20on-the-fly%20knowledge%20injection%2C%20we%20also%20implement%0Apost-tuning%20knowledge%20by%20re-ranking%20the%20prediction%20results%20during%20inference.%0AWith%20the%20injected%20knowledge%2C%20the%20model%20can%20better%20capture%20informative%20features%0Afor%20downstream%20tasks%20as%20data%20evolves.%20Extensive%20experiments%20demonstrate%20the%0Astate-of-the-art%20performance%20of%20ENGINE.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/LAMDA-CL/ICCV25-ENGINE%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08510v2&entry.124074799=Read"},
{"title": "Dynamic mapping from static labels: remote sensing dynamic sample\n  generation with temporal-spectral embedding", "author": "Shuai Yuan and Shuang Chen and Tianwu Lin and Jincheng Yuan and Geng Tian and Yang Xu and Jie Wang and Peng Gong", "abstract": "  Accurate remote sensing geographic mapping requires timely and representative\nsamples. However, rapid land surface changes often render static samples\nobsolete within months, making manual sample updates labor-intensive and\nunsustainable. To address this challenge, we propose TasGen, a two-stage\nTemporal spectral-aware Automatic Sample Generation method for generating\ndynamic training samples from single-date static labels without human\nintervention. Land surface dynamics often manifest as anomalies in\ntemporal-spectral sequences. %These anomalies are multivariate yet unified:\ntemporal, spectral, or joint anomalies stem from different mechanisms and\ncannot be naively coupled, as this may obscure the nature of changes. Yet, any\nland surface state corresponds to a coherent temporal-spectral signature, which\nwould be lost if the two dimensions are modeled separately. To effectively\ncapture these dynamics, TasGen first disentangles temporal and spectral\nfeatures to isolate their individual contributions, and then couples them to\nmodel their synergistic interactions. In the first stage, we introduce a\nhierarchical temporal-spectral variational autoencoder (HTS-VAE) with a\ndual-dimension embedding to learn low-dimensional latent patterns of normal\nsamples by first disentangling and then jointly embedding temporal and spectral\ninformation. This temporal-spectral embedding enables robust anomaly detection\nby identifying deviations from learned joint patterns. In the second stage, a\nclassifier trained on stable samples relabels change points across time to\ngenerate dynamic samples. To not only detect but also explain surface dynamics,\nwe further propose an anomaly interpretation method based on Gibbs sampling,\nwhich attributes changes to specific spectral-temporal dimensions.\n", "link": "http://arxiv.org/abs/2506.02574v2", "date": "2025-07-24", "relevancy": 2.5919, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5266}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5159}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20mapping%20from%20static%20labels%3A%20remote%20sensing%20dynamic%20sample%0A%20%20generation%20with%20temporal-spectral%20embedding&body=Title%3A%20Dynamic%20mapping%20from%20static%20labels%3A%20remote%20sensing%20dynamic%20sample%0A%20%20generation%20with%20temporal-spectral%20embedding%0AAuthor%3A%20Shuai%20Yuan%20and%20Shuang%20Chen%20and%20Tianwu%20Lin%20and%20Jincheng%20Yuan%20and%20Geng%20Tian%20and%20Yang%20Xu%20and%20Jie%20Wang%20and%20Peng%20Gong%0AAbstract%3A%20%20%20Accurate%20remote%20sensing%20geographic%20mapping%20requires%20timely%20and%20representative%0Asamples.%20However%2C%20rapid%20land%20surface%20changes%20often%20render%20static%20samples%0Aobsolete%20within%20months%2C%20making%20manual%20sample%20updates%20labor-intensive%20and%0Aunsustainable.%20To%20address%20this%20challenge%2C%20we%20propose%20TasGen%2C%20a%20two-stage%0ATemporal%20spectral-aware%20Automatic%20Sample%20Generation%20method%20for%20generating%0Adynamic%20training%20samples%20from%20single-date%20static%20labels%20without%20human%0Aintervention.%20Land%20surface%20dynamics%20often%20manifest%20as%20anomalies%20in%0Atemporal-spectral%20sequences.%20%25These%20anomalies%20are%20multivariate%20yet%20unified%3A%0Atemporal%2C%20spectral%2C%20or%20joint%20anomalies%20stem%20from%20different%20mechanisms%20and%0Acannot%20be%20naively%20coupled%2C%20as%20this%20may%20obscure%20the%20nature%20of%20changes.%20Yet%2C%20any%0Aland%20surface%20state%20corresponds%20to%20a%20coherent%20temporal-spectral%20signature%2C%20which%0Awould%20be%20lost%20if%20the%20two%20dimensions%20are%20modeled%20separately.%20To%20effectively%0Acapture%20these%20dynamics%2C%20TasGen%20first%20disentangles%20temporal%20and%20spectral%0Afeatures%20to%20isolate%20their%20individual%20contributions%2C%20and%20then%20couples%20them%20to%0Amodel%20their%20synergistic%20interactions.%20In%20the%20first%20stage%2C%20we%20introduce%20a%0Ahierarchical%20temporal-spectral%20variational%20autoencoder%20%28HTS-VAE%29%20with%20a%0Adual-dimension%20embedding%20to%20learn%20low-dimensional%20latent%20patterns%20of%20normal%0Asamples%20by%20first%20disentangling%20and%20then%20jointly%20embedding%20temporal%20and%20spectral%0Ainformation.%20This%20temporal-spectral%20embedding%20enables%20robust%20anomaly%20detection%0Aby%20identifying%20deviations%20from%20learned%20joint%20patterns.%20In%20the%20second%20stage%2C%20a%0Aclassifier%20trained%20on%20stable%20samples%20relabels%20change%20points%20across%20time%20to%0Agenerate%20dynamic%20samples.%20To%20not%20only%20detect%20but%20also%20explain%20surface%20dynamics%2C%0Awe%20further%20propose%20an%20anomaly%20interpretation%20method%20based%20on%20Gibbs%20sampling%2C%0Awhich%20attributes%20changes%20to%20specific%20spectral-temporal%20dimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02574v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520mapping%2520from%2520static%2520labels%253A%2520remote%2520sensing%2520dynamic%2520sample%250A%2520%2520generation%2520with%2520temporal-spectral%2520embedding%26entry.906535625%3DShuai%2520Yuan%2520and%2520Shuang%2520Chen%2520and%2520Tianwu%2520Lin%2520and%2520Jincheng%2520Yuan%2520and%2520Geng%2520Tian%2520and%2520Yang%2520Xu%2520and%2520Jie%2520Wang%2520and%2520Peng%2520Gong%26entry.1292438233%3D%2520%2520Accurate%2520remote%2520sensing%2520geographic%2520mapping%2520requires%2520timely%2520and%2520representative%250Asamples.%2520However%252C%2520rapid%2520land%2520surface%2520changes%2520often%2520render%2520static%2520samples%250Aobsolete%2520within%2520months%252C%2520making%2520manual%2520sample%2520updates%2520labor-intensive%2520and%250Aunsustainable.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520TasGen%252C%2520a%2520two-stage%250ATemporal%2520spectral-aware%2520Automatic%2520Sample%2520Generation%2520method%2520for%2520generating%250Adynamic%2520training%2520samples%2520from%2520single-date%2520static%2520labels%2520without%2520human%250Aintervention.%2520Land%2520surface%2520dynamics%2520often%2520manifest%2520as%2520anomalies%2520in%250Atemporal-spectral%2520sequences.%2520%2525These%2520anomalies%2520are%2520multivariate%2520yet%2520unified%253A%250Atemporal%252C%2520spectral%252C%2520or%2520joint%2520anomalies%2520stem%2520from%2520different%2520mechanisms%2520and%250Acannot%2520be%2520naively%2520coupled%252C%2520as%2520this%2520may%2520obscure%2520the%2520nature%2520of%2520changes.%2520Yet%252C%2520any%250Aland%2520surface%2520state%2520corresponds%2520to%2520a%2520coherent%2520temporal-spectral%2520signature%252C%2520which%250Awould%2520be%2520lost%2520if%2520the%2520two%2520dimensions%2520are%2520modeled%2520separately.%2520To%2520effectively%250Acapture%2520these%2520dynamics%252C%2520TasGen%2520first%2520disentangles%2520temporal%2520and%2520spectral%250Afeatures%2520to%2520isolate%2520their%2520individual%2520contributions%252C%2520and%2520then%2520couples%2520them%2520to%250Amodel%2520their%2520synergistic%2520interactions.%2520In%2520the%2520first%2520stage%252C%2520we%2520introduce%2520a%250Ahierarchical%2520temporal-spectral%2520variational%2520autoencoder%2520%2528HTS-VAE%2529%2520with%2520a%250Adual-dimension%2520embedding%2520to%2520learn%2520low-dimensional%2520latent%2520patterns%2520of%2520normal%250Asamples%2520by%2520first%2520disentangling%2520and%2520then%2520jointly%2520embedding%2520temporal%2520and%2520spectral%250Ainformation.%2520This%2520temporal-spectral%2520embedding%2520enables%2520robust%2520anomaly%2520detection%250Aby%2520identifying%2520deviations%2520from%2520learned%2520joint%2520patterns.%2520In%2520the%2520second%2520stage%252C%2520a%250Aclassifier%2520trained%2520on%2520stable%2520samples%2520relabels%2520change%2520points%2520across%2520time%2520to%250Agenerate%2520dynamic%2520samples.%2520To%2520not%2520only%2520detect%2520but%2520also%2520explain%2520surface%2520dynamics%252C%250Awe%2520further%2520propose%2520an%2520anomaly%2520interpretation%2520method%2520based%2520on%2520Gibbs%2520sampling%252C%250Awhich%2520attributes%2520changes%2520to%2520specific%2520spectral-temporal%2520dimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02574v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20mapping%20from%20static%20labels%3A%20remote%20sensing%20dynamic%20sample%0A%20%20generation%20with%20temporal-spectral%20embedding&entry.906535625=Shuai%20Yuan%20and%20Shuang%20Chen%20and%20Tianwu%20Lin%20and%20Jincheng%20Yuan%20and%20Geng%20Tian%20and%20Yang%20Xu%20and%20Jie%20Wang%20and%20Peng%20Gong&entry.1292438233=%20%20Accurate%20remote%20sensing%20geographic%20mapping%20requires%20timely%20and%20representative%0Asamples.%20However%2C%20rapid%20land%20surface%20changes%20often%20render%20static%20samples%0Aobsolete%20within%20months%2C%20making%20manual%20sample%20updates%20labor-intensive%20and%0Aunsustainable.%20To%20address%20this%20challenge%2C%20we%20propose%20TasGen%2C%20a%20two-stage%0ATemporal%20spectral-aware%20Automatic%20Sample%20Generation%20method%20for%20generating%0Adynamic%20training%20samples%20from%20single-date%20static%20labels%20without%20human%0Aintervention.%20Land%20surface%20dynamics%20often%20manifest%20as%20anomalies%20in%0Atemporal-spectral%20sequences.%20%25These%20anomalies%20are%20multivariate%20yet%20unified%3A%0Atemporal%2C%20spectral%2C%20or%20joint%20anomalies%20stem%20from%20different%20mechanisms%20and%0Acannot%20be%20naively%20coupled%2C%20as%20this%20may%20obscure%20the%20nature%20of%20changes.%20Yet%2C%20any%0Aland%20surface%20state%20corresponds%20to%20a%20coherent%20temporal-spectral%20signature%2C%20which%0Awould%20be%20lost%20if%20the%20two%20dimensions%20are%20modeled%20separately.%20To%20effectively%0Acapture%20these%20dynamics%2C%20TasGen%20first%20disentangles%20temporal%20and%20spectral%0Afeatures%20to%20isolate%20their%20individual%20contributions%2C%20and%20then%20couples%20them%20to%0Amodel%20their%20synergistic%20interactions.%20In%20the%20first%20stage%2C%20we%20introduce%20a%0Ahierarchical%20temporal-spectral%20variational%20autoencoder%20%28HTS-VAE%29%20with%20a%0Adual-dimension%20embedding%20to%20learn%20low-dimensional%20latent%20patterns%20of%20normal%0Asamples%20by%20first%20disentangling%20and%20then%20jointly%20embedding%20temporal%20and%20spectral%0Ainformation.%20This%20temporal-spectral%20embedding%20enables%20robust%20anomaly%20detection%0Aby%20identifying%20deviations%20from%20learned%20joint%20patterns.%20In%20the%20second%20stage%2C%20a%0Aclassifier%20trained%20on%20stable%20samples%20relabels%20change%20points%20across%20time%20to%0Agenerate%20dynamic%20samples.%20To%20not%20only%20detect%20but%20also%20explain%20surface%20dynamics%2C%0Awe%20further%20propose%20an%20anomaly%20interpretation%20method%20based%20on%20Gibbs%20sampling%2C%0Awhich%20attributes%20changes%20to%20specific%20spectral-temporal%20dimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02574v2&entry.124074799=Read"},
{"title": "GLANCE: Graph Logic Attention Network with Cluster Enhancement for\n  Heterophilous Graph Representation Learning", "author": "Zhongtian Sun and Anoushka Harit and Alexandra Cristea and Christl A. Donnelly and Pietro Li\u00f2", "abstract": "  Graph Neural Networks (GNNs) have demonstrated significant success in\nlearning from graph-structured data but often struggle on heterophilous graphs,\nwhere connected nodes differ in features or class labels. This limitation\narises from indiscriminate neighbor aggregation and insufficient incorporation\nof higher-order structural patterns. To address these challenges, we propose\nGLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel\nframework that integrates logic-guided reasoning, dynamic graph refinement, and\nadaptive clustering to enhance graph representation learning. GLANCE combines a\nlogic layer for interpretable and structured embeddings, multi-head\nattention-based edge pruning for denoising graph structures, and clustering\nmechanisms for capturing global patterns. Experimental results in benchmark\ndatasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE\nachieves competitive performance, offering robust and interpretable solutions\nfor heterophilous graph scenarios. The proposed framework is lightweight,\nadaptable, and uniquely suited to the challenges of heterophilous graphs.\n", "link": "http://arxiv.org/abs/2507.18521v1", "date": "2025-07-24", "relevancy": 2.5903, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5423}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5071}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLANCE%3A%20Graph%20Logic%20Attention%20Network%20with%20Cluster%20Enhancement%20for%0A%20%20Heterophilous%20Graph%20Representation%20Learning&body=Title%3A%20GLANCE%3A%20Graph%20Logic%20Attention%20Network%20with%20Cluster%20Enhancement%20for%0A%20%20Heterophilous%20Graph%20Representation%20Learning%0AAuthor%3A%20Zhongtian%20Sun%20and%20Anoushka%20Harit%20and%20Alexandra%20Cristea%20and%20Christl%20A.%20Donnelly%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20significant%20success%20in%0Alearning%20from%20graph-structured%20data%20but%20often%20struggle%20on%20heterophilous%20graphs%2C%0Awhere%20connected%20nodes%20differ%20in%20features%20or%20class%20labels.%20This%20limitation%0Aarises%20from%20indiscriminate%20neighbor%20aggregation%20and%20insufficient%20incorporation%0Aof%20higher-order%20structural%20patterns.%20To%20address%20these%20challenges%2C%20we%20propose%0AGLANCE%20%28Graph%20Logic%20Attention%20Network%20with%20Cluster%20Enhancement%29%2C%20a%20novel%0Aframework%20that%20integrates%20logic-guided%20reasoning%2C%20dynamic%20graph%20refinement%2C%20and%0Aadaptive%20clustering%20to%20enhance%20graph%20representation%20learning.%20GLANCE%20combines%20a%0Alogic%20layer%20for%20interpretable%20and%20structured%20embeddings%2C%20multi-head%0Aattention-based%20edge%20pruning%20for%20denoising%20graph%20structures%2C%20and%20clustering%0Amechanisms%20for%20capturing%20global%20patterns.%20Experimental%20results%20in%20benchmark%0Adatasets%2C%20including%20Cornell%2C%20Texas%2C%20and%20Wisconsin%2C%20demonstrate%20that%20GLANCE%0Aachieves%20competitive%20performance%2C%20offering%20robust%20and%20interpretable%20solutions%0Afor%20heterophilous%20graph%20scenarios.%20The%20proposed%20framework%20is%20lightweight%2C%0Aadaptable%2C%20and%20uniquely%20suited%20to%20the%20challenges%20of%20heterophilous%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLANCE%253A%2520Graph%2520Logic%2520Attention%2520Network%2520with%2520Cluster%2520Enhancement%2520for%250A%2520%2520Heterophilous%2520Graph%2520Representation%2520Learning%26entry.906535625%3DZhongtian%2520Sun%2520and%2520Anoushka%2520Harit%2520and%2520Alexandra%2520Cristea%2520and%2520Christl%2520A.%2520Donnelly%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520significant%2520success%2520in%250Alearning%2520from%2520graph-structured%2520data%2520but%2520often%2520struggle%2520on%2520heterophilous%2520graphs%252C%250Awhere%2520connected%2520nodes%2520differ%2520in%2520features%2520or%2520class%2520labels.%2520This%2520limitation%250Aarises%2520from%2520indiscriminate%2520neighbor%2520aggregation%2520and%2520insufficient%2520incorporation%250Aof%2520higher-order%2520structural%2520patterns.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AGLANCE%2520%2528Graph%2520Logic%2520Attention%2520Network%2520with%2520Cluster%2520Enhancement%2529%252C%2520a%2520novel%250Aframework%2520that%2520integrates%2520logic-guided%2520reasoning%252C%2520dynamic%2520graph%2520refinement%252C%2520and%250Aadaptive%2520clustering%2520to%2520enhance%2520graph%2520representation%2520learning.%2520GLANCE%2520combines%2520a%250Alogic%2520layer%2520for%2520interpretable%2520and%2520structured%2520embeddings%252C%2520multi-head%250Aattention-based%2520edge%2520pruning%2520for%2520denoising%2520graph%2520structures%252C%2520and%2520clustering%250Amechanisms%2520for%2520capturing%2520global%2520patterns.%2520Experimental%2520results%2520in%2520benchmark%250Adatasets%252C%2520including%2520Cornell%252C%2520Texas%252C%2520and%2520Wisconsin%252C%2520demonstrate%2520that%2520GLANCE%250Aachieves%2520competitive%2520performance%252C%2520offering%2520robust%2520and%2520interpretable%2520solutions%250Afor%2520heterophilous%2520graph%2520scenarios.%2520The%2520proposed%2520framework%2520is%2520lightweight%252C%250Aadaptable%252C%2520and%2520uniquely%2520suited%2520to%2520the%2520challenges%2520of%2520heterophilous%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLANCE%3A%20Graph%20Logic%20Attention%20Network%20with%20Cluster%20Enhancement%20for%0A%20%20Heterophilous%20Graph%20Representation%20Learning&entry.906535625=Zhongtian%20Sun%20and%20Anoushka%20Harit%20and%20Alexandra%20Cristea%20and%20Christl%20A.%20Donnelly%20and%20Pietro%20Li%C3%B2&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20significant%20success%20in%0Alearning%20from%20graph-structured%20data%20but%20often%20struggle%20on%20heterophilous%20graphs%2C%0Awhere%20connected%20nodes%20differ%20in%20features%20or%20class%20labels.%20This%20limitation%0Aarises%20from%20indiscriminate%20neighbor%20aggregation%20and%20insufficient%20incorporation%0Aof%20higher-order%20structural%20patterns.%20To%20address%20these%20challenges%2C%20we%20propose%0AGLANCE%20%28Graph%20Logic%20Attention%20Network%20with%20Cluster%20Enhancement%29%2C%20a%20novel%0Aframework%20that%20integrates%20logic-guided%20reasoning%2C%20dynamic%20graph%20refinement%2C%20and%0Aadaptive%20clustering%20to%20enhance%20graph%20representation%20learning.%20GLANCE%20combines%20a%0Alogic%20layer%20for%20interpretable%20and%20structured%20embeddings%2C%20multi-head%0Aattention-based%20edge%20pruning%20for%20denoising%20graph%20structures%2C%20and%20clustering%0Amechanisms%20for%20capturing%20global%20patterns.%20Experimental%20results%20in%20benchmark%0Adatasets%2C%20including%20Cornell%2C%20Texas%2C%20and%20Wisconsin%2C%20demonstrate%20that%20GLANCE%0Aachieves%20competitive%20performance%2C%20offering%20robust%20and%20interpretable%20solutions%0Afor%20heterophilous%20graph%20scenarios.%20The%20proposed%20framework%20is%20lightweight%2C%0Aadaptable%2C%20and%20uniquely%20suited%20to%20the%20challenges%20of%20heterophilous%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18521v1&entry.124074799=Read"},
{"title": "Locate-and-Focus: Enhancing Terminology Translation in Speech Language\n  Models", "author": "Suhang Wu and Jialong Tang and Chengyi Yang and Pei Zhang and Baosong Yang and Junhui Li and Junfeng Yao and Min Zhang and Jinsong Su", "abstract": "  Direct speech translation (ST) has garnered increasing attention nowadays,\nyet the accurate translation of terminology within utterances remains a great\nchallenge. In this regard, current studies mainly concentrate on leveraging\nvarious translation knowledge into ST models. However, these methods often\nstruggle with interference from irrelevant noise and can not fully utilize the\ntranslation knowledge. To address these issues, in this paper, we propose a\nnovel Locate-and-Focus method for terminology translation. It first effectively\nlocates the speech clips containing terminologies within the utterance to\nconstruct translation knowledge, minimizing irrelevant information for the ST\nmodel. Subsequently, it associates the translation knowledge with the utterance\nand hypothesis from both audio and textual modalities, allowing the ST model to\nbetter focus on translation knowledge during translation. Experimental results\nacross various datasets demonstrate that our method effectively locates\nterminologies within utterances and enhances the success rate of terminology\ntranslation, while maintaining robust general translation performance.\n", "link": "http://arxiv.org/abs/2507.18263v1", "date": "2025-07-24", "relevancy": 2.5854, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locate-and-Focus%3A%20Enhancing%20Terminology%20Translation%20in%20Speech%20Language%0A%20%20Models&body=Title%3A%20Locate-and-Focus%3A%20Enhancing%20Terminology%20Translation%20in%20Speech%20Language%0A%20%20Models%0AAuthor%3A%20Suhang%20Wu%20and%20Jialong%20Tang%20and%20Chengyi%20Yang%20and%20Pei%20Zhang%20and%20Baosong%20Yang%20and%20Junhui%20Li%20and%20Junfeng%20Yao%20and%20Min%20Zhang%20and%20Jinsong%20Su%0AAbstract%3A%20%20%20Direct%20speech%20translation%20%28ST%29%20has%20garnered%20increasing%20attention%20nowadays%2C%0Ayet%20the%20accurate%20translation%20of%20terminology%20within%20utterances%20remains%20a%20great%0Achallenge.%20In%20this%20regard%2C%20current%20studies%20mainly%20concentrate%20on%20leveraging%0Avarious%20translation%20knowledge%20into%20ST%20models.%20However%2C%20these%20methods%20often%0Astruggle%20with%20interference%20from%20irrelevant%20noise%20and%20can%20not%20fully%20utilize%20the%0Atranslation%20knowledge.%20To%20address%20these%20issues%2C%20in%20this%20paper%2C%20we%20propose%20a%0Anovel%20Locate-and-Focus%20method%20for%20terminology%20translation.%20It%20first%20effectively%0Alocates%20the%20speech%20clips%20containing%20terminologies%20within%20the%20utterance%20to%0Aconstruct%20translation%20knowledge%2C%20minimizing%20irrelevant%20information%20for%20the%20ST%0Amodel.%20Subsequently%2C%20it%20associates%20the%20translation%20knowledge%20with%20the%20utterance%0Aand%20hypothesis%20from%20both%20audio%20and%20textual%20modalities%2C%20allowing%20the%20ST%20model%20to%0Abetter%20focus%20on%20translation%20knowledge%20during%20translation.%20Experimental%20results%0Aacross%20various%20datasets%20demonstrate%20that%20our%20method%20effectively%20locates%0Aterminologies%20within%20utterances%20and%20enhances%20the%20success%20rate%20of%20terminology%0Atranslation%2C%20while%20maintaining%20robust%20general%20translation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocate-and-Focus%253A%2520Enhancing%2520Terminology%2520Translation%2520in%2520Speech%2520Language%250A%2520%2520Models%26entry.906535625%3DSuhang%2520Wu%2520and%2520Jialong%2520Tang%2520and%2520Chengyi%2520Yang%2520and%2520Pei%2520Zhang%2520and%2520Baosong%2520Yang%2520and%2520Junhui%2520Li%2520and%2520Junfeng%2520Yao%2520and%2520Min%2520Zhang%2520and%2520Jinsong%2520Su%26entry.1292438233%3D%2520%2520Direct%2520speech%2520translation%2520%2528ST%2529%2520has%2520garnered%2520increasing%2520attention%2520nowadays%252C%250Ayet%2520the%2520accurate%2520translation%2520of%2520terminology%2520within%2520utterances%2520remains%2520a%2520great%250Achallenge.%2520In%2520this%2520regard%252C%2520current%2520studies%2520mainly%2520concentrate%2520on%2520leveraging%250Avarious%2520translation%2520knowledge%2520into%2520ST%2520models.%2520However%252C%2520these%2520methods%2520often%250Astruggle%2520with%2520interference%2520from%2520irrelevant%2520noise%2520and%2520can%2520not%2520fully%2520utilize%2520the%250Atranslation%2520knowledge.%2520To%2520address%2520these%2520issues%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520Locate-and-Focus%2520method%2520for%2520terminology%2520translation.%2520It%2520first%2520effectively%250Alocates%2520the%2520speech%2520clips%2520containing%2520terminologies%2520within%2520the%2520utterance%2520to%250Aconstruct%2520translation%2520knowledge%252C%2520minimizing%2520irrelevant%2520information%2520for%2520the%2520ST%250Amodel.%2520Subsequently%252C%2520it%2520associates%2520the%2520translation%2520knowledge%2520with%2520the%2520utterance%250Aand%2520hypothesis%2520from%2520both%2520audio%2520and%2520textual%2520modalities%252C%2520allowing%2520the%2520ST%2520model%2520to%250Abetter%2520focus%2520on%2520translation%2520knowledge%2520during%2520translation.%2520Experimental%2520results%250Aacross%2520various%2520datasets%2520demonstrate%2520that%2520our%2520method%2520effectively%2520locates%250Aterminologies%2520within%2520utterances%2520and%2520enhances%2520the%2520success%2520rate%2520of%2520terminology%250Atranslation%252C%2520while%2520maintaining%2520robust%2520general%2520translation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locate-and-Focus%3A%20Enhancing%20Terminology%20Translation%20in%20Speech%20Language%0A%20%20Models&entry.906535625=Suhang%20Wu%20and%20Jialong%20Tang%20and%20Chengyi%20Yang%20and%20Pei%20Zhang%20and%20Baosong%20Yang%20and%20Junhui%20Li%20and%20Junfeng%20Yao%20and%20Min%20Zhang%20and%20Jinsong%20Su&entry.1292438233=%20%20Direct%20speech%20translation%20%28ST%29%20has%20garnered%20increasing%20attention%20nowadays%2C%0Ayet%20the%20accurate%20translation%20of%20terminology%20within%20utterances%20remains%20a%20great%0Achallenge.%20In%20this%20regard%2C%20current%20studies%20mainly%20concentrate%20on%20leveraging%0Avarious%20translation%20knowledge%20into%20ST%20models.%20However%2C%20these%20methods%20often%0Astruggle%20with%20interference%20from%20irrelevant%20noise%20and%20can%20not%20fully%20utilize%20the%0Atranslation%20knowledge.%20To%20address%20these%20issues%2C%20in%20this%20paper%2C%20we%20propose%20a%0Anovel%20Locate-and-Focus%20method%20for%20terminology%20translation.%20It%20first%20effectively%0Alocates%20the%20speech%20clips%20containing%20terminologies%20within%20the%20utterance%20to%0Aconstruct%20translation%20knowledge%2C%20minimizing%20irrelevant%20information%20for%20the%20ST%0Amodel.%20Subsequently%2C%20it%20associates%20the%20translation%20knowledge%20with%20the%20utterance%0Aand%20hypothesis%20from%20both%20audio%20and%20textual%20modalities%2C%20allowing%20the%20ST%20model%20to%0Abetter%20focus%20on%20translation%20knowledge%20during%20translation.%20Experimental%20results%0Aacross%20various%20datasets%20demonstrate%20that%20our%20method%20effectively%20locates%0Aterminologies%20within%20utterances%20and%20enhances%20the%20success%20rate%20of%20terminology%0Atranslation%2C%20while%20maintaining%20robust%20general%20translation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18263v1&entry.124074799=Read"},
{"title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation", "author": "Jiafeng Xiong and Yuting Zhao", "abstract": "  Multimodal Machine Translation (MMT) has demonstrated the significant help of\nvisual information in machine translation. However, existing MMT methods face\nchallenges in leveraging the modality gap by enforcing rigid visual-linguistic\nalignment whilst being confined to inference within their trained multimodal\ndomains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage\nGraph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph\nAttention Network adapter to learn multimodal knowledge in a unified fused\nspace and inductively generalize it to broader image-free translation domains.\nExperimental results on the Multi30K dataset of English-to-French and\nEnglish-to-German tasks demonstrate that our GIIFT surpasses existing\napproaches and achieves the state-of-the-art, even without images during\ninference. Results on the WMT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards\ninductive image-free inference.\n", "link": "http://arxiv.org/abs/2507.18562v1", "date": "2025-07-24", "relevancy": 2.5591, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5407}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5005}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIIFT%3A%20Graph-guided%20Inductive%20Image-free%20Multimodal%20Machine%20Translation&body=Title%3A%20GIIFT%3A%20Graph-guided%20Inductive%20Image-free%20Multimodal%20Machine%20Translation%0AAuthor%3A%20Jiafeng%20Xiong%20and%20Yuting%20Zhao%0AAbstract%3A%20%20%20Multimodal%20Machine%20Translation%20%28MMT%29%20has%20demonstrated%20the%20significant%20help%20of%0Avisual%20information%20in%20machine%20translation.%20However%2C%20existing%20MMT%20methods%20face%0Achallenges%20in%20leveraging%20the%20modality%20gap%20by%20enforcing%20rigid%20visual-linguistic%0Aalignment%20whilst%20being%20confined%20to%20inference%20within%20their%20trained%20multimodal%0Adomains.%20In%20this%20work%2C%20we%20construct%20novel%20multimodal%20scene%20graphs%20to%20preserve%0Aand%20integrate%20modality-specific%20information%20and%20introduce%20GIIFT%2C%20a%20two-stage%0AGraph-guided%20Inductive%20Image-Free%20MMT%20framework%20that%20uses%20a%20cross-modal%20Graph%0AAttention%20Network%20adapter%20to%20learn%20multimodal%20knowledge%20in%20a%20unified%20fused%0Aspace%20and%20inductively%20generalize%20it%20to%20broader%20image-free%20translation%20domains.%0AExperimental%20results%20on%20the%20Multi30K%20dataset%20of%20English-to-French%20and%0AEnglish-to-German%20tasks%20demonstrate%20that%20our%20GIIFT%20surpasses%20existing%0Aapproaches%20and%20achieves%20the%20state-of-the-art%2C%20even%20without%20images%20during%0Ainference.%20Results%20on%20the%20WMT%20benchmark%20show%20significant%20improvements%20over%20the%0Aimage-free%20translation%20baselines%2C%20demonstrating%20the%20strength%20of%20GIIFT%20towards%0Ainductive%20image-free%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIIFT%253A%2520Graph-guided%2520Inductive%2520Image-free%2520Multimodal%2520Machine%2520Translation%26entry.906535625%3DJiafeng%2520Xiong%2520and%2520Yuting%2520Zhao%26entry.1292438233%3D%2520%2520Multimodal%2520Machine%2520Translation%2520%2528MMT%2529%2520has%2520demonstrated%2520the%2520significant%2520help%2520of%250Avisual%2520information%2520in%2520machine%2520translation.%2520However%252C%2520existing%2520MMT%2520methods%2520face%250Achallenges%2520in%2520leveraging%2520the%2520modality%2520gap%2520by%2520enforcing%2520rigid%2520visual-linguistic%250Aalignment%2520whilst%2520being%2520confined%2520to%2520inference%2520within%2520their%2520trained%2520multimodal%250Adomains.%2520In%2520this%2520work%252C%2520we%2520construct%2520novel%2520multimodal%2520scene%2520graphs%2520to%2520preserve%250Aand%2520integrate%2520modality-specific%2520information%2520and%2520introduce%2520GIIFT%252C%2520a%2520two-stage%250AGraph-guided%2520Inductive%2520Image-Free%2520MMT%2520framework%2520that%2520uses%2520a%2520cross-modal%2520Graph%250AAttention%2520Network%2520adapter%2520to%2520learn%2520multimodal%2520knowledge%2520in%2520a%2520unified%2520fused%250Aspace%2520and%2520inductively%2520generalize%2520it%2520to%2520broader%2520image-free%2520translation%2520domains.%250AExperimental%2520results%2520on%2520the%2520Multi30K%2520dataset%2520of%2520English-to-French%2520and%250AEnglish-to-German%2520tasks%2520demonstrate%2520that%2520our%2520GIIFT%2520surpasses%2520existing%250Aapproaches%2520and%2520achieves%2520the%2520state-of-the-art%252C%2520even%2520without%2520images%2520during%250Ainference.%2520Results%2520on%2520the%2520WMT%2520benchmark%2520show%2520significant%2520improvements%2520over%2520the%250Aimage-free%2520translation%2520baselines%252C%2520demonstrating%2520the%2520strength%2520of%2520GIIFT%2520towards%250Ainductive%2520image-free%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIIFT%3A%20Graph-guided%20Inductive%20Image-free%20Multimodal%20Machine%20Translation&entry.906535625=Jiafeng%20Xiong%20and%20Yuting%20Zhao&entry.1292438233=%20%20Multimodal%20Machine%20Translation%20%28MMT%29%20has%20demonstrated%20the%20significant%20help%20of%0Avisual%20information%20in%20machine%20translation.%20However%2C%20existing%20MMT%20methods%20face%0Achallenges%20in%20leveraging%20the%20modality%20gap%20by%20enforcing%20rigid%20visual-linguistic%0Aalignment%20whilst%20being%20confined%20to%20inference%20within%20their%20trained%20multimodal%0Adomains.%20In%20this%20work%2C%20we%20construct%20novel%20multimodal%20scene%20graphs%20to%20preserve%0Aand%20integrate%20modality-specific%20information%20and%20introduce%20GIIFT%2C%20a%20two-stage%0AGraph-guided%20Inductive%20Image-Free%20MMT%20framework%20that%20uses%20a%20cross-modal%20Graph%0AAttention%20Network%20adapter%20to%20learn%20multimodal%20knowledge%20in%20a%20unified%20fused%0Aspace%20and%20inductively%20generalize%20it%20to%20broader%20image-free%20translation%20domains.%0AExperimental%20results%20on%20the%20Multi30K%20dataset%20of%20English-to-French%20and%0AEnglish-to-German%20tasks%20demonstrate%20that%20our%20GIIFT%20surpasses%20existing%0Aapproaches%20and%20achieves%20the%20state-of-the-art%2C%20even%20without%20images%20during%0Ainference.%20Results%20on%20the%20WMT%20benchmark%20show%20significant%20improvements%20over%20the%0Aimage-free%20translation%20baselines%2C%20demonstrating%20the%20strength%20of%20GIIFT%20towards%0Ainductive%20image-free%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18562v1&entry.124074799=Read"},
{"title": "crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation\n  Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to\n  2023", "author": "Navodini Wijethilake and Reuben Dorent and Marina Ivory and Aaron Kujawa and Stefan Cornelissen and Patrick Langenhuizen and Mohamed Okasha and Anna Oviedova and Hexin Dong and Bogyeong Kang and Guillaume Sall\u00e9 and Luyi Han and Ziyuan Zhao and Han Liu and Yubo Fan and Tao Yang and Shahad Hardan and Hussain Alasmawi and Santosh Sanjeev and Yuzhou Zhuang and Satoshi Kondo and Maria Baldeon Calisto and Shaikh Muhammad Uzair Noman and Cancan Chen and Ipek Oguz and Rongguo Zhang and Mina Rezaei and Susana K. Lai-Yuen and Satoshi Kasai and Yunzhi Huang and Chih-Cheng Hung and Mohammad Yaqub and Lisheng Wang and Benoit M. Dawant and Cuntai Guan and Ritse Mann and Vincent Jaouen and Tae-Eui Kam and Li Zhang and Jonathan Shapey and Tom Vercauteren", "abstract": "  The cross-Modality Domain Adaptation (crossMoDA) challenge series, initiated\nin 2021 in conjunction with the International Conference on Medical Image\nComputing and Computer Assisted Intervention (MICCAI), focuses on unsupervised\ncross-modality segmentation, learning from contrast-enhanced T1 (ceT1) and\ntransferring to T2 MRI. The task is an extreme example of domain shift chosen\nto serve as a meaningful and illustrative benchmark. From a clinical\napplication perspective, it aims to automate Vestibular Schwannoma (VS) and\ncochlea segmentation on T2 scans for more cost-effective VS management. Over\ntime, the challenge objectives have evolved to enhance its clinical relevance.\nThe challenge evolved from using single-institutional data and basic\nsegmentation in 2021 to incorporating multi-institutional data and Koos grading\nin 2022, and by 2023, it included heterogeneous routine data and\nsub-segmentation of intra- and extra-meatal tumour components. In this work, we\nreport the findings of the 2022 and 2023 editions and perform a retrospective\nanalysis of the challenge progression over the years. The observations from the\nsuccessive challenge contributions indicate that the number of outliers\ndecreases with an expanding dataset. This is notable since the diversity of\nscanning protocols of the datasets concurrently increased. The winning approach\nof the 2023 edition reduced the number of outliers on the 2021 and 2022 testing\ndata, demonstrating how increased data heterogeneity can enhance segmentation\nperformance even on homogeneous data. However, the cochlea Dice score declined\nin 2023, likely due to the added complexity from tumour sub-annotations\naffecting overall segmentation performance. While progress is still needed for\nclinically acceptable VS segmentation, the plateauing performance suggests that\na more challenging cross-modal task may better serve future benchmarking.\n", "link": "http://arxiv.org/abs/2506.12006v3", "date": "2025-07-24", "relevancy": 2.5429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20crossMoDA%20Challenge%3A%20Evolution%20of%20Cross-Modality%20Domain%20Adaptation%0A%20%20Techniques%20for%20Vestibular%20Schwannoma%20and%20Cochlea%20Segmentation%20from%202021%20to%0A%20%202023&body=Title%3A%20crossMoDA%20Challenge%3A%20Evolution%20of%20Cross-Modality%20Domain%20Adaptation%0A%20%20Techniques%20for%20Vestibular%20Schwannoma%20and%20Cochlea%20Segmentation%20from%202021%20to%0A%20%202023%0AAuthor%3A%20Navodini%20Wijethilake%20and%20Reuben%20Dorent%20and%20Marina%20Ivory%20and%20Aaron%20Kujawa%20and%20Stefan%20Cornelissen%20and%20Patrick%20Langenhuizen%20and%20Mohamed%20Okasha%20and%20Anna%20Oviedova%20and%20Hexin%20Dong%20and%20Bogyeong%20Kang%20and%20Guillaume%20Sall%C3%A9%20and%20Luyi%20Han%20and%20Ziyuan%20Zhao%20and%20Han%20Liu%20and%20Yubo%20Fan%20and%20Tao%20Yang%20and%20Shahad%20Hardan%20and%20Hussain%20Alasmawi%20and%20Santosh%20Sanjeev%20and%20Yuzhou%20Zhuang%20and%20Satoshi%20Kondo%20and%20Maria%20Baldeon%20Calisto%20and%20Shaikh%20Muhammad%20Uzair%20Noman%20and%20Cancan%20Chen%20and%20Ipek%20Oguz%20and%20Rongguo%20Zhang%20and%20Mina%20Rezaei%20and%20Susana%20K.%20Lai-Yuen%20and%20Satoshi%20Kasai%20and%20Yunzhi%20Huang%20and%20Chih-Cheng%20Hung%20and%20Mohammad%20Yaqub%20and%20Lisheng%20Wang%20and%20Benoit%20M.%20Dawant%20and%20Cuntai%20Guan%20and%20Ritse%20Mann%20and%20Vincent%20Jaouen%20and%20Tae-Eui%20Kam%20and%20Li%20Zhang%20and%20Jonathan%20Shapey%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20The%20cross-Modality%20Domain%20Adaptation%20%28crossMoDA%29%20challenge%20series%2C%20initiated%0Ain%202021%20in%20conjunction%20with%20the%20International%20Conference%20on%20Medical%20Image%0AComputing%20and%20Computer%20Assisted%20Intervention%20%28MICCAI%29%2C%20focuses%20on%20unsupervised%0Across-modality%20segmentation%2C%20learning%20from%20contrast-enhanced%20T1%20%28ceT1%29%20and%0Atransferring%20to%20T2%20MRI.%20The%20task%20is%20an%20extreme%20example%20of%20domain%20shift%20chosen%0Ato%20serve%20as%20a%20meaningful%20and%20illustrative%20benchmark.%20From%20a%20clinical%0Aapplication%20perspective%2C%20it%20aims%20to%20automate%20Vestibular%20Schwannoma%20%28VS%29%20and%0Acochlea%20segmentation%20on%20T2%20scans%20for%20more%20cost-effective%20VS%20management.%20Over%0Atime%2C%20the%20challenge%20objectives%20have%20evolved%20to%20enhance%20its%20clinical%20relevance.%0AThe%20challenge%20evolved%20from%20using%20single-institutional%20data%20and%20basic%0Asegmentation%20in%202021%20to%20incorporating%20multi-institutional%20data%20and%20Koos%20grading%0Ain%202022%2C%20and%20by%202023%2C%20it%20included%20heterogeneous%20routine%20data%20and%0Asub-segmentation%20of%20intra-%20and%20extra-meatal%20tumour%20components.%20In%20this%20work%2C%20we%0Areport%20the%20findings%20of%20the%202022%20and%202023%20editions%20and%20perform%20a%20retrospective%0Aanalysis%20of%20the%20challenge%20progression%20over%20the%20years.%20The%20observations%20from%20the%0Asuccessive%20challenge%20contributions%20indicate%20that%20the%20number%20of%20outliers%0Adecreases%20with%20an%20expanding%20dataset.%20This%20is%20notable%20since%20the%20diversity%20of%0Ascanning%20protocols%20of%20the%20datasets%20concurrently%20increased.%20The%20winning%20approach%0Aof%20the%202023%20edition%20reduced%20the%20number%20of%20outliers%20on%20the%202021%20and%202022%20testing%0Adata%2C%20demonstrating%20how%20increased%20data%20heterogeneity%20can%20enhance%20segmentation%0Aperformance%20even%20on%20homogeneous%20data.%20However%2C%20the%20cochlea%20Dice%20score%20declined%0Ain%202023%2C%20likely%20due%20to%20the%20added%20complexity%20from%20tumour%20sub-annotations%0Aaffecting%20overall%20segmentation%20performance.%20While%20progress%20is%20still%20needed%20for%0Aclinically%20acceptable%20VS%20segmentation%2C%20the%20plateauing%20performance%20suggests%20that%0Aa%20more%20challenging%20cross-modal%20task%20may%20better%20serve%20future%20benchmarking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12006v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DcrossMoDA%2520Challenge%253A%2520Evolution%2520of%2520Cross-Modality%2520Domain%2520Adaptation%250A%2520%2520Techniques%2520for%2520Vestibular%2520Schwannoma%2520and%2520Cochlea%2520Segmentation%2520from%25202021%2520to%250A%2520%25202023%26entry.906535625%3DNavodini%2520Wijethilake%2520and%2520Reuben%2520Dorent%2520and%2520Marina%2520Ivory%2520and%2520Aaron%2520Kujawa%2520and%2520Stefan%2520Cornelissen%2520and%2520Patrick%2520Langenhuizen%2520and%2520Mohamed%2520Okasha%2520and%2520Anna%2520Oviedova%2520and%2520Hexin%2520Dong%2520and%2520Bogyeong%2520Kang%2520and%2520Guillaume%2520Sall%25C3%25A9%2520and%2520Luyi%2520Han%2520and%2520Ziyuan%2520Zhao%2520and%2520Han%2520Liu%2520and%2520Yubo%2520Fan%2520and%2520Tao%2520Yang%2520and%2520Shahad%2520Hardan%2520and%2520Hussain%2520Alasmawi%2520and%2520Santosh%2520Sanjeev%2520and%2520Yuzhou%2520Zhuang%2520and%2520Satoshi%2520Kondo%2520and%2520Maria%2520Baldeon%2520Calisto%2520and%2520Shaikh%2520Muhammad%2520Uzair%2520Noman%2520and%2520Cancan%2520Chen%2520and%2520Ipek%2520Oguz%2520and%2520Rongguo%2520Zhang%2520and%2520Mina%2520Rezaei%2520and%2520Susana%2520K.%2520Lai-Yuen%2520and%2520Satoshi%2520Kasai%2520and%2520Yunzhi%2520Huang%2520and%2520Chih-Cheng%2520Hung%2520and%2520Mohammad%2520Yaqub%2520and%2520Lisheng%2520Wang%2520and%2520Benoit%2520M.%2520Dawant%2520and%2520Cuntai%2520Guan%2520and%2520Ritse%2520Mann%2520and%2520Vincent%2520Jaouen%2520and%2520Tae-Eui%2520Kam%2520and%2520Li%2520Zhang%2520and%2520Jonathan%2520Shapey%2520and%2520Tom%2520Vercauteren%26entry.1292438233%3D%2520%2520The%2520cross-Modality%2520Domain%2520Adaptation%2520%2528crossMoDA%2529%2520challenge%2520series%252C%2520initiated%250Ain%25202021%2520in%2520conjunction%2520with%2520the%2520International%2520Conference%2520on%2520Medical%2520Image%250AComputing%2520and%2520Computer%2520Assisted%2520Intervention%2520%2528MICCAI%2529%252C%2520focuses%2520on%2520unsupervised%250Across-modality%2520segmentation%252C%2520learning%2520from%2520contrast-enhanced%2520T1%2520%2528ceT1%2529%2520and%250Atransferring%2520to%2520T2%2520MRI.%2520The%2520task%2520is%2520an%2520extreme%2520example%2520of%2520domain%2520shift%2520chosen%250Ato%2520serve%2520as%2520a%2520meaningful%2520and%2520illustrative%2520benchmark.%2520From%2520a%2520clinical%250Aapplication%2520perspective%252C%2520it%2520aims%2520to%2520automate%2520Vestibular%2520Schwannoma%2520%2528VS%2529%2520and%250Acochlea%2520segmentation%2520on%2520T2%2520scans%2520for%2520more%2520cost-effective%2520VS%2520management.%2520Over%250Atime%252C%2520the%2520challenge%2520objectives%2520have%2520evolved%2520to%2520enhance%2520its%2520clinical%2520relevance.%250AThe%2520challenge%2520evolved%2520from%2520using%2520single-institutional%2520data%2520and%2520basic%250Asegmentation%2520in%25202021%2520to%2520incorporating%2520multi-institutional%2520data%2520and%2520Koos%2520grading%250Ain%25202022%252C%2520and%2520by%25202023%252C%2520it%2520included%2520heterogeneous%2520routine%2520data%2520and%250Asub-segmentation%2520of%2520intra-%2520and%2520extra-meatal%2520tumour%2520components.%2520In%2520this%2520work%252C%2520we%250Areport%2520the%2520findings%2520of%2520the%25202022%2520and%25202023%2520editions%2520and%2520perform%2520a%2520retrospective%250Aanalysis%2520of%2520the%2520challenge%2520progression%2520over%2520the%2520years.%2520The%2520observations%2520from%2520the%250Asuccessive%2520challenge%2520contributions%2520indicate%2520that%2520the%2520number%2520of%2520outliers%250Adecreases%2520with%2520an%2520expanding%2520dataset.%2520This%2520is%2520notable%2520since%2520the%2520diversity%2520of%250Ascanning%2520protocols%2520of%2520the%2520datasets%2520concurrently%2520increased.%2520The%2520winning%2520approach%250Aof%2520the%25202023%2520edition%2520reduced%2520the%2520number%2520of%2520outliers%2520on%2520the%25202021%2520and%25202022%2520testing%250Adata%252C%2520demonstrating%2520how%2520increased%2520data%2520heterogeneity%2520can%2520enhance%2520segmentation%250Aperformance%2520even%2520on%2520homogeneous%2520data.%2520However%252C%2520the%2520cochlea%2520Dice%2520score%2520declined%250Ain%25202023%252C%2520likely%2520due%2520to%2520the%2520added%2520complexity%2520from%2520tumour%2520sub-annotations%250Aaffecting%2520overall%2520segmentation%2520performance.%2520While%2520progress%2520is%2520still%2520needed%2520for%250Aclinically%2520acceptable%2520VS%2520segmentation%252C%2520the%2520plateauing%2520performance%2520suggests%2520that%250Aa%2520more%2520challenging%2520cross-modal%2520task%2520may%2520better%2520serve%2520future%2520benchmarking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12006v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=crossMoDA%20Challenge%3A%20Evolution%20of%20Cross-Modality%20Domain%20Adaptation%0A%20%20Techniques%20for%20Vestibular%20Schwannoma%20and%20Cochlea%20Segmentation%20from%202021%20to%0A%20%202023&entry.906535625=Navodini%20Wijethilake%20and%20Reuben%20Dorent%20and%20Marina%20Ivory%20and%20Aaron%20Kujawa%20and%20Stefan%20Cornelissen%20and%20Patrick%20Langenhuizen%20and%20Mohamed%20Okasha%20and%20Anna%20Oviedova%20and%20Hexin%20Dong%20and%20Bogyeong%20Kang%20and%20Guillaume%20Sall%C3%A9%20and%20Luyi%20Han%20and%20Ziyuan%20Zhao%20and%20Han%20Liu%20and%20Yubo%20Fan%20and%20Tao%20Yang%20and%20Shahad%20Hardan%20and%20Hussain%20Alasmawi%20and%20Santosh%20Sanjeev%20and%20Yuzhou%20Zhuang%20and%20Satoshi%20Kondo%20and%20Maria%20Baldeon%20Calisto%20and%20Shaikh%20Muhammad%20Uzair%20Noman%20and%20Cancan%20Chen%20and%20Ipek%20Oguz%20and%20Rongguo%20Zhang%20and%20Mina%20Rezaei%20and%20Susana%20K.%20Lai-Yuen%20and%20Satoshi%20Kasai%20and%20Yunzhi%20Huang%20and%20Chih-Cheng%20Hung%20and%20Mohammad%20Yaqub%20and%20Lisheng%20Wang%20and%20Benoit%20M.%20Dawant%20and%20Cuntai%20Guan%20and%20Ritse%20Mann%20and%20Vincent%20Jaouen%20and%20Tae-Eui%20Kam%20and%20Li%20Zhang%20and%20Jonathan%20Shapey%20and%20Tom%20Vercauteren&entry.1292438233=%20%20The%20cross-Modality%20Domain%20Adaptation%20%28crossMoDA%29%20challenge%20series%2C%20initiated%0Ain%202021%20in%20conjunction%20with%20the%20International%20Conference%20on%20Medical%20Image%0AComputing%20and%20Computer%20Assisted%20Intervention%20%28MICCAI%29%2C%20focuses%20on%20unsupervised%0Across-modality%20segmentation%2C%20learning%20from%20contrast-enhanced%20T1%20%28ceT1%29%20and%0Atransferring%20to%20T2%20MRI.%20The%20task%20is%20an%20extreme%20example%20of%20domain%20shift%20chosen%0Ato%20serve%20as%20a%20meaningful%20and%20illustrative%20benchmark.%20From%20a%20clinical%0Aapplication%20perspective%2C%20it%20aims%20to%20automate%20Vestibular%20Schwannoma%20%28VS%29%20and%0Acochlea%20segmentation%20on%20T2%20scans%20for%20more%20cost-effective%20VS%20management.%20Over%0Atime%2C%20the%20challenge%20objectives%20have%20evolved%20to%20enhance%20its%20clinical%20relevance.%0AThe%20challenge%20evolved%20from%20using%20single-institutional%20data%20and%20basic%0Asegmentation%20in%202021%20to%20incorporating%20multi-institutional%20data%20and%20Koos%20grading%0Ain%202022%2C%20and%20by%202023%2C%20it%20included%20heterogeneous%20routine%20data%20and%0Asub-segmentation%20of%20intra-%20and%20extra-meatal%20tumour%20components.%20In%20this%20work%2C%20we%0Areport%20the%20findings%20of%20the%202022%20and%202023%20editions%20and%20perform%20a%20retrospective%0Aanalysis%20of%20the%20challenge%20progression%20over%20the%20years.%20The%20observations%20from%20the%0Asuccessive%20challenge%20contributions%20indicate%20that%20the%20number%20of%20outliers%0Adecreases%20with%20an%20expanding%20dataset.%20This%20is%20notable%20since%20the%20diversity%20of%0Ascanning%20protocols%20of%20the%20datasets%20concurrently%20increased.%20The%20winning%20approach%0Aof%20the%202023%20edition%20reduced%20the%20number%20of%20outliers%20on%20the%202021%20and%202022%20testing%0Adata%2C%20demonstrating%20how%20increased%20data%20heterogeneity%20can%20enhance%20segmentation%0Aperformance%20even%20on%20homogeneous%20data.%20However%2C%20the%20cochlea%20Dice%20score%20declined%0Ain%202023%2C%20likely%20due%20to%20the%20added%20complexity%20from%20tumour%20sub-annotations%0Aaffecting%20overall%20segmentation%20performance.%20While%20progress%20is%20still%20needed%20for%0Aclinically%20acceptable%20VS%20segmentation%2C%20the%20plateauing%20performance%20suggests%20that%0Aa%20more%20challenging%20cross-modal%20task%20may%20better%20serve%20future%20benchmarking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12006v3&entry.124074799=Read"},
{"title": "Towards Consistent Long-Term Pose Generation", "author": "Yayuan Li and Filippos Bellos and Jason Corso", "abstract": "  Current approaches to pose generation rely heavily on intermediate\nrepresentations, either through two-stage pipelines with quantization or\nautoregressive models that accumulate errors during inference. This fundamental\nlimitation leads to degraded performance, particularly in long-term pose\ngeneration where maintaining temporal coherence is crucial. We propose a novel\none-stage architecture that directly generates poses in continuous coordinate\nspace from minimal context - a single RGB image and text description - while\nmaintaining consistent distributions between training and inference. Our key\ninnovation is eliminating the need for intermediate representations or\ntoken-based generation by operating directly on pose coordinates through a\nrelative movement prediction mechanism that preserves spatial relationships,\nand a unified placeholder token approach that enables single-forward generation\nwith identical behavior during training and inference. Through extensive\nexperiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)\ndatasets, we demonstrate that our approach significantly outperforms existing\nquantization-based and autoregressive methods, especially in long-term\ngeneration scenarios.\n", "link": "http://arxiv.org/abs/2507.18382v1", "date": "2025-07-24", "relevancy": 2.5421, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.679}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6058}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Consistent%20Long-Term%20Pose%20Generation&body=Title%3A%20Towards%20Consistent%20Long-Term%20Pose%20Generation%0AAuthor%3A%20Yayuan%20Li%20and%20Filippos%20Bellos%20and%20Jason%20Corso%0AAbstract%3A%20%20%20Current%20approaches%20to%20pose%20generation%20rely%20heavily%20on%20intermediate%0Arepresentations%2C%20either%20through%20two-stage%20pipelines%20with%20quantization%20or%0Aautoregressive%20models%20that%20accumulate%20errors%20during%20inference.%20This%20fundamental%0Alimitation%20leads%20to%20degraded%20performance%2C%20particularly%20in%20long-term%20pose%0Ageneration%20where%20maintaining%20temporal%20coherence%20is%20crucial.%20We%20propose%20a%20novel%0Aone-stage%20architecture%20that%20directly%20generates%20poses%20in%20continuous%20coordinate%0Aspace%20from%20minimal%20context%20-%20a%20single%20RGB%20image%20and%20text%20description%20-%20while%0Amaintaining%20consistent%20distributions%20between%20training%20and%20inference.%20Our%20key%0Ainnovation%20is%20eliminating%20the%20need%20for%20intermediate%20representations%20or%0Atoken-based%20generation%20by%20operating%20directly%20on%20pose%20coordinates%20through%20a%0Arelative%20movement%20prediction%20mechanism%20that%20preserves%20spatial%20relationships%2C%0Aand%20a%20unified%20placeholder%20token%20approach%20that%20enables%20single-forward%20generation%0Awith%20identical%20behavior%20during%20training%20and%20inference.%20Through%20extensive%0Aexperiments%20on%20Penn%20Action%20and%20First-Person%20Hand%20Action%20Benchmark%20%28F-PHAB%29%0Adatasets%2C%20we%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%0Aquantization-based%20and%20autoregressive%20methods%2C%20especially%20in%20long-term%0Ageneration%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Consistent%2520Long-Term%2520Pose%2520Generation%26entry.906535625%3DYayuan%2520Li%2520and%2520Filippos%2520Bellos%2520and%2520Jason%2520Corso%26entry.1292438233%3D%2520%2520Current%2520approaches%2520to%2520pose%2520generation%2520rely%2520heavily%2520on%2520intermediate%250Arepresentations%252C%2520either%2520through%2520two-stage%2520pipelines%2520with%2520quantization%2520or%250Aautoregressive%2520models%2520that%2520accumulate%2520errors%2520during%2520inference.%2520This%2520fundamental%250Alimitation%2520leads%2520to%2520degraded%2520performance%252C%2520particularly%2520in%2520long-term%2520pose%250Ageneration%2520where%2520maintaining%2520temporal%2520coherence%2520is%2520crucial.%2520We%2520propose%2520a%2520novel%250Aone-stage%2520architecture%2520that%2520directly%2520generates%2520poses%2520in%2520continuous%2520coordinate%250Aspace%2520from%2520minimal%2520context%2520-%2520a%2520single%2520RGB%2520image%2520and%2520text%2520description%2520-%2520while%250Amaintaining%2520consistent%2520distributions%2520between%2520training%2520and%2520inference.%2520Our%2520key%250Ainnovation%2520is%2520eliminating%2520the%2520need%2520for%2520intermediate%2520representations%2520or%250Atoken-based%2520generation%2520by%2520operating%2520directly%2520on%2520pose%2520coordinates%2520through%2520a%250Arelative%2520movement%2520prediction%2520mechanism%2520that%2520preserves%2520spatial%2520relationships%252C%250Aand%2520a%2520unified%2520placeholder%2520token%2520approach%2520that%2520enables%2520single-forward%2520generation%250Awith%2520identical%2520behavior%2520during%2520training%2520and%2520inference.%2520Through%2520extensive%250Aexperiments%2520on%2520Penn%2520Action%2520and%2520First-Person%2520Hand%2520Action%2520Benchmark%2520%2528F-PHAB%2529%250Adatasets%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%250Aquantization-based%2520and%2520autoregressive%2520methods%252C%2520especially%2520in%2520long-term%250Ageneration%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Consistent%20Long-Term%20Pose%20Generation&entry.906535625=Yayuan%20Li%20and%20Filippos%20Bellos%20and%20Jason%20Corso&entry.1292438233=%20%20Current%20approaches%20to%20pose%20generation%20rely%20heavily%20on%20intermediate%0Arepresentations%2C%20either%20through%20two-stage%20pipelines%20with%20quantization%20or%0Aautoregressive%20models%20that%20accumulate%20errors%20during%20inference.%20This%20fundamental%0Alimitation%20leads%20to%20degraded%20performance%2C%20particularly%20in%20long-term%20pose%0Ageneration%20where%20maintaining%20temporal%20coherence%20is%20crucial.%20We%20propose%20a%20novel%0Aone-stage%20architecture%20that%20directly%20generates%20poses%20in%20continuous%20coordinate%0Aspace%20from%20minimal%20context%20-%20a%20single%20RGB%20image%20and%20text%20description%20-%20while%0Amaintaining%20consistent%20distributions%20between%20training%20and%20inference.%20Our%20key%0Ainnovation%20is%20eliminating%20the%20need%20for%20intermediate%20representations%20or%0Atoken-based%20generation%20by%20operating%20directly%20on%20pose%20coordinates%20through%20a%0Arelative%20movement%20prediction%20mechanism%20that%20preserves%20spatial%20relationships%2C%0Aand%20a%20unified%20placeholder%20token%20approach%20that%20enables%20single-forward%20generation%0Awith%20identical%20behavior%20during%20training%20and%20inference.%20Through%20extensive%0Aexperiments%20on%20Penn%20Action%20and%20First-Person%20Hand%20Action%20Benchmark%20%28F-PHAB%29%0Adatasets%2C%20we%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%0Aquantization-based%20and%20autoregressive%20methods%2C%20especially%20in%20long-term%0Ageneration%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18382v1&entry.124074799=Read"},
{"title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models", "author": "Zheyu Zhang and Shuo Yang and Bardh Prenkaj and Gjergji Kasneci", "abstract": "  Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.\n", "link": "http://arxiv.org/abs/2507.18504v1", "date": "2025-07-24", "relevancy": 2.5412, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5245}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5016}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Features%20Deserve%20Attention%3A%20Graph-Guided%20Dependency%20Learning%20for%0A%20%20Tabular%20Data%20Generation%20with%20Language%20Models&body=Title%3A%20Not%20All%20Features%20Deserve%20Attention%3A%20Graph-Guided%20Dependency%20Learning%20for%0A%20%20Tabular%20Data%20Generation%20with%20Language%20Models%0AAuthor%3A%20Zheyu%20Zhang%20and%20Shuo%20Yang%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20strong%20potential%20for%20tabular%20data%0Ageneration%20by%20modeling%20textualized%20feature-value%20pairs.%20However%2C%20tabular%20data%0Ainherently%20exhibits%20sparse%20feature-level%20dependencies%2C%20where%20many%20feature%0Ainteractions%20are%20structurally%20insignificant.%20This%20creates%20a%20fundamental%0Amismatch%20as%20LLMs%27%20self-attention%20mechanism%20inevitably%20distributes%20focus%20across%0Aall%20pairs%2C%20diluting%20attention%20on%20critical%20relationships%2C%20particularly%20in%0Adatasets%20with%20complex%20dependencies%20or%20semantically%20ambiguous%20features.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20GraDe%20%28Graph-Guided%20Dependency%20Learning%29%2C%20a%0Anovel%20method%20that%20explicitly%20integrates%20sparse%20dependency%20graphs%20into%20LLMs%27%0Aattention%20mechanism.%20GraDe%20employs%20a%20lightweight%20dynamic%20graph%20learning%20module%0Aguided%20by%20externally%20extracted%20functional%20dependencies%2C%20prioritizing%20key%0Afeature%20interactions%20while%20suppressing%20irrelevant%20ones.%20Our%20experiments%20across%0Adiverse%20real-world%20datasets%20demonstrate%20that%20GraDe%20outperforms%20existing%0ALLM-based%20approaches%20by%20up%20to%2012%25%20on%20complex%20datasets%20while%20achieving%0Acompetitive%20results%20with%20state-of-the-art%20approaches%20in%20synthetic%20data%20quality.%0AOur%20method%20is%20minimally%20intrusive%20yet%20effective%2C%20offering%20a%20practical%20solution%0Afor%20structure-aware%20tabular%20data%20modeling%20with%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Features%2520Deserve%2520Attention%253A%2520Graph-Guided%2520Dependency%2520Learning%2520for%250A%2520%2520Tabular%2520Data%2520Generation%2520with%2520Language%2520Models%26entry.906535625%3DZheyu%2520Zhang%2520and%2520Shuo%2520Yang%2520and%2520Bardh%2520Prenkaj%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520strong%2520potential%2520for%2520tabular%2520data%250Ageneration%2520by%2520modeling%2520textualized%2520feature-value%2520pairs.%2520However%252C%2520tabular%2520data%250Ainherently%2520exhibits%2520sparse%2520feature-level%2520dependencies%252C%2520where%2520many%2520feature%250Ainteractions%2520are%2520structurally%2520insignificant.%2520This%2520creates%2520a%2520fundamental%250Amismatch%2520as%2520LLMs%2527%2520self-attention%2520mechanism%2520inevitably%2520distributes%2520focus%2520across%250Aall%2520pairs%252C%2520diluting%2520attention%2520on%2520critical%2520relationships%252C%2520particularly%2520in%250Adatasets%2520with%2520complex%2520dependencies%2520or%2520semantically%2520ambiguous%2520features.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520GraDe%2520%2528Graph-Guided%2520Dependency%2520Learning%2529%252C%2520a%250Anovel%2520method%2520that%2520explicitly%2520integrates%2520sparse%2520dependency%2520graphs%2520into%2520LLMs%2527%250Aattention%2520mechanism.%2520GraDe%2520employs%2520a%2520lightweight%2520dynamic%2520graph%2520learning%2520module%250Aguided%2520by%2520externally%2520extracted%2520functional%2520dependencies%252C%2520prioritizing%2520key%250Afeature%2520interactions%2520while%2520suppressing%2520irrelevant%2520ones.%2520Our%2520experiments%2520across%250Adiverse%2520real-world%2520datasets%2520demonstrate%2520that%2520GraDe%2520outperforms%2520existing%250ALLM-based%2520approaches%2520by%2520up%2520to%252012%2525%2520on%2520complex%2520datasets%2520while%2520achieving%250Acompetitive%2520results%2520with%2520state-of-the-art%2520approaches%2520in%2520synthetic%2520data%2520quality.%250AOur%2520method%2520is%2520minimally%2520intrusive%2520yet%2520effective%252C%2520offering%2520a%2520practical%2520solution%250Afor%2520structure-aware%2520tabular%2520data%2520modeling%2520with%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Features%20Deserve%20Attention%3A%20Graph-Guided%20Dependency%20Learning%20for%0A%20%20Tabular%20Data%20Generation%20with%20Language%20Models&entry.906535625=Zheyu%20Zhang%20and%20Shuo%20Yang%20and%20Bardh%20Prenkaj%20and%20Gjergji%20Kasneci&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20strong%20potential%20for%20tabular%20data%0Ageneration%20by%20modeling%20textualized%20feature-value%20pairs.%20However%2C%20tabular%20data%0Ainherently%20exhibits%20sparse%20feature-level%20dependencies%2C%20where%20many%20feature%0Ainteractions%20are%20structurally%20insignificant.%20This%20creates%20a%20fundamental%0Amismatch%20as%20LLMs%27%20self-attention%20mechanism%20inevitably%20distributes%20focus%20across%0Aall%20pairs%2C%20diluting%20attention%20on%20critical%20relationships%2C%20particularly%20in%0Adatasets%20with%20complex%20dependencies%20or%20semantically%20ambiguous%20features.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20GraDe%20%28Graph-Guided%20Dependency%20Learning%29%2C%20a%0Anovel%20method%20that%20explicitly%20integrates%20sparse%20dependency%20graphs%20into%20LLMs%27%0Aattention%20mechanism.%20GraDe%20employs%20a%20lightweight%20dynamic%20graph%20learning%20module%0Aguided%20by%20externally%20extracted%20functional%20dependencies%2C%20prioritizing%20key%0Afeature%20interactions%20while%20suppressing%20irrelevant%20ones.%20Our%20experiments%20across%0Adiverse%20real-world%20datasets%20demonstrate%20that%20GraDe%20outperforms%20existing%0ALLM-based%20approaches%20by%20up%20to%2012%25%20on%20complex%20datasets%20while%20achieving%0Acompetitive%20results%20with%20state-of-the-art%20approaches%20in%20synthetic%20data%20quality.%0AOur%20method%20is%20minimally%20intrusive%20yet%20effective%2C%20offering%20a%20practical%20solution%0Afor%20structure-aware%20tabular%20data%20modeling%20with%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18504v1&entry.124074799=Read"},
{"title": "Beyond Euclid: An Illustrated Guide to Modern Machine Learning with\n  Geometric, Topological, and Algebraic Structures", "author": "Mathilde Papillon and Sophia Sanborn and Johan Mathe and Louisa Cornelis and Abby Bertics and Domas Buracas and Hansen J Lillemark and Christian Shewmake and Fatih Dinc and Xavier Pennec and Nina Miolane", "abstract": "  The enduring legacy of Euclidean geometry underpins classical machine\nlearning, which, for decades, has been primarily developed for data lying in\nEuclidean space. Yet, modern machine learning increasingly encounters richly\nstructured data that is inherently nonEuclidean. This data can exhibit\nintricate geometric, topological and algebraic structure: from the geometry of\nthe curvature of space-time, to topologically complex interactions between\nneurons in the brain, to the algebraic transformations describing symmetries of\nphysical systems. Extracting knowledge from such non-Euclidean data\nnecessitates a broader mathematical perspective. Echoing the 19th-century\nrevolutions that gave rise to non-Euclidean geometry, an emerging line of\nresearch is redefining modern machine learning with non-Euclidean structures.\nIts goal: generalizing classical methods to unconventional data types with\ngeometry, topology, and algebra. In this review, we provide an accessible\ngateway to this fast-growing field and propose a graphical taxonomy that\nintegrates recent advances into an intuitive unified framework. We subsequently\nextract insights into current challenges and highlight exciting opportunities\nfor future development in this field.\n", "link": "http://arxiv.org/abs/2407.09468v2", "date": "2025-07-24", "relevancy": 2.5387, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.529}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5069}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Euclid%3A%20An%20Illustrated%20Guide%20to%20Modern%20Machine%20Learning%20with%0A%20%20Geometric%2C%20Topological%2C%20and%20Algebraic%20Structures&body=Title%3A%20Beyond%20Euclid%3A%20An%20Illustrated%20Guide%20to%20Modern%20Machine%20Learning%20with%0A%20%20Geometric%2C%20Topological%2C%20and%20Algebraic%20Structures%0AAuthor%3A%20Mathilde%20Papillon%20and%20Sophia%20Sanborn%20and%20Johan%20Mathe%20and%20Louisa%20Cornelis%20and%20Abby%20Bertics%20and%20Domas%20Buracas%20and%20Hansen%20J%20Lillemark%20and%20Christian%20Shewmake%20and%20Fatih%20Dinc%20and%20Xavier%20Pennec%20and%20Nina%20Miolane%0AAbstract%3A%20%20%20The%20enduring%20legacy%20of%20Euclidean%20geometry%20underpins%20classical%20machine%0Alearning%2C%20which%2C%20for%20decades%2C%20has%20been%20primarily%20developed%20for%20data%20lying%20in%0AEuclidean%20space.%20Yet%2C%20modern%20machine%20learning%20increasingly%20encounters%20richly%0Astructured%20data%20that%20is%20inherently%20nonEuclidean.%20This%20data%20can%20exhibit%0Aintricate%20geometric%2C%20topological%20and%20algebraic%20structure%3A%20from%20the%20geometry%20of%0Athe%20curvature%20of%20space-time%2C%20to%20topologically%20complex%20interactions%20between%0Aneurons%20in%20the%20brain%2C%20to%20the%20algebraic%20transformations%20describing%20symmetries%20of%0Aphysical%20systems.%20Extracting%20knowledge%20from%20such%20non-Euclidean%20data%0Anecessitates%20a%20broader%20mathematical%20perspective.%20Echoing%20the%2019th-century%0Arevolutions%20that%20gave%20rise%20to%20non-Euclidean%20geometry%2C%20an%20emerging%20line%20of%0Aresearch%20is%20redefining%20modern%20machine%20learning%20with%20non-Euclidean%20structures.%0AIts%20goal%3A%20generalizing%20classical%20methods%20to%20unconventional%20data%20types%20with%0Ageometry%2C%20topology%2C%20and%20algebra.%20In%20this%20review%2C%20we%20provide%20an%20accessible%0Agateway%20to%20this%20fast-growing%20field%20and%20propose%20a%20graphical%20taxonomy%20that%0Aintegrates%20recent%20advances%20into%20an%20intuitive%20unified%20framework.%20We%20subsequently%0Aextract%20insights%20into%20current%20challenges%20and%20highlight%20exciting%20opportunities%0Afor%20future%20development%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Euclid%253A%2520An%2520Illustrated%2520Guide%2520to%2520Modern%2520Machine%2520Learning%2520with%250A%2520%2520Geometric%252C%2520Topological%252C%2520and%2520Algebraic%2520Structures%26entry.906535625%3DMathilde%2520Papillon%2520and%2520Sophia%2520Sanborn%2520and%2520Johan%2520Mathe%2520and%2520Louisa%2520Cornelis%2520and%2520Abby%2520Bertics%2520and%2520Domas%2520Buracas%2520and%2520Hansen%2520J%2520Lillemark%2520and%2520Christian%2520Shewmake%2520and%2520Fatih%2520Dinc%2520and%2520Xavier%2520Pennec%2520and%2520Nina%2520Miolane%26entry.1292438233%3D%2520%2520The%2520enduring%2520legacy%2520of%2520Euclidean%2520geometry%2520underpins%2520classical%2520machine%250Alearning%252C%2520which%252C%2520for%2520decades%252C%2520has%2520been%2520primarily%2520developed%2520for%2520data%2520lying%2520in%250AEuclidean%2520space.%2520Yet%252C%2520modern%2520machine%2520learning%2520increasingly%2520encounters%2520richly%250Astructured%2520data%2520that%2520is%2520inherently%2520nonEuclidean.%2520This%2520data%2520can%2520exhibit%250Aintricate%2520geometric%252C%2520topological%2520and%2520algebraic%2520structure%253A%2520from%2520the%2520geometry%2520of%250Athe%2520curvature%2520of%2520space-time%252C%2520to%2520topologically%2520complex%2520interactions%2520between%250Aneurons%2520in%2520the%2520brain%252C%2520to%2520the%2520algebraic%2520transformations%2520describing%2520symmetries%2520of%250Aphysical%2520systems.%2520Extracting%2520knowledge%2520from%2520such%2520non-Euclidean%2520data%250Anecessitates%2520a%2520broader%2520mathematical%2520perspective.%2520Echoing%2520the%252019th-century%250Arevolutions%2520that%2520gave%2520rise%2520to%2520non-Euclidean%2520geometry%252C%2520an%2520emerging%2520line%2520of%250Aresearch%2520is%2520redefining%2520modern%2520machine%2520learning%2520with%2520non-Euclidean%2520structures.%250AIts%2520goal%253A%2520generalizing%2520classical%2520methods%2520to%2520unconventional%2520data%2520types%2520with%250Ageometry%252C%2520topology%252C%2520and%2520algebra.%2520In%2520this%2520review%252C%2520we%2520provide%2520an%2520accessible%250Agateway%2520to%2520this%2520fast-growing%2520field%2520and%2520propose%2520a%2520graphical%2520taxonomy%2520that%250Aintegrates%2520recent%2520advances%2520into%2520an%2520intuitive%2520unified%2520framework.%2520We%2520subsequently%250Aextract%2520insights%2520into%2520current%2520challenges%2520and%2520highlight%2520exciting%2520opportunities%250Afor%2520future%2520development%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Euclid%3A%20An%20Illustrated%20Guide%20to%20Modern%20Machine%20Learning%20with%0A%20%20Geometric%2C%20Topological%2C%20and%20Algebraic%20Structures&entry.906535625=Mathilde%20Papillon%20and%20Sophia%20Sanborn%20and%20Johan%20Mathe%20and%20Louisa%20Cornelis%20and%20Abby%20Bertics%20and%20Domas%20Buracas%20and%20Hansen%20J%20Lillemark%20and%20Christian%20Shewmake%20and%20Fatih%20Dinc%20and%20Xavier%20Pennec%20and%20Nina%20Miolane&entry.1292438233=%20%20The%20enduring%20legacy%20of%20Euclidean%20geometry%20underpins%20classical%20machine%0Alearning%2C%20which%2C%20for%20decades%2C%20has%20been%20primarily%20developed%20for%20data%20lying%20in%0AEuclidean%20space.%20Yet%2C%20modern%20machine%20learning%20increasingly%20encounters%20richly%0Astructured%20data%20that%20is%20inherently%20nonEuclidean.%20This%20data%20can%20exhibit%0Aintricate%20geometric%2C%20topological%20and%20algebraic%20structure%3A%20from%20the%20geometry%20of%0Athe%20curvature%20of%20space-time%2C%20to%20topologically%20complex%20interactions%20between%0Aneurons%20in%20the%20brain%2C%20to%20the%20algebraic%20transformations%20describing%20symmetries%20of%0Aphysical%20systems.%20Extracting%20knowledge%20from%20such%20non-Euclidean%20data%0Anecessitates%20a%20broader%20mathematical%20perspective.%20Echoing%20the%2019th-century%0Arevolutions%20that%20gave%20rise%20to%20non-Euclidean%20geometry%2C%20an%20emerging%20line%20of%0Aresearch%20is%20redefining%20modern%20machine%20learning%20with%20non-Euclidean%20structures.%0AIts%20goal%3A%20generalizing%20classical%20methods%20to%20unconventional%20data%20types%20with%0Ageometry%2C%20topology%2C%20and%20algebra.%20In%20this%20review%2C%20we%20provide%20an%20accessible%0Agateway%20to%20this%20fast-growing%20field%20and%20propose%20a%20graphical%20taxonomy%20that%0Aintegrates%20recent%20advances%20into%20an%20intuitive%20unified%20framework.%20We%20subsequently%0Aextract%20insights%20into%20current%20challenges%20and%20highlight%20exciting%20opportunities%0Afor%20future%20development%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09468v2&entry.124074799=Read"},
{"title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance\n  Data Synthesis for Specialist LLMs", "author": "Xiaopeng Ke and Hexuan Deng and Xuebo Liu and Jun Rao and Zhenxi Song and Jun Yu and Min Zhang", "abstract": "  Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt.\n", "link": "http://arxiv.org/abs/2507.18584v1", "date": "2025-07-24", "relevancy": 2.5252, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AQuilt%3A%20Weaving%20Logic%20and%20Self-Inspection%20into%20Low-Cost%2C%20High-Relevance%0A%20%20Data%20Synthesis%20for%20Specialist%20LLMs&body=Title%3A%20AQuilt%3A%20Weaving%20Logic%20and%20Self-Inspection%20into%20Low-Cost%2C%20High-Relevance%0A%20%20Data%20Synthesis%20for%20Specialist%20LLMs%0AAuthor%3A%20Xiaopeng%20Ke%20and%20Hexuan%20Deng%20and%20Xuebo%20Liu%20and%20Jun%20Rao%20and%20Zhenxi%20Song%20and%20Jun%20Yu%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Despite%20the%20impressive%20performance%20of%20large%20language%20models%20%28LLMs%29%20in%20general%0Adomains%2C%20they%20often%20underperform%20in%20specialized%20domains.%20Existing%20approaches%0Atypically%20rely%20on%20data%20synthesis%20methods%20and%20yield%20promising%20results%20by%20using%0Aunlabeled%20data%20to%20capture%20domain-specific%20features.%20However%2C%20these%20methods%0Aeither%20incur%20high%20computational%20costs%20or%20suffer%20from%20performance%20limitations%2C%0Awhile%20also%20demonstrating%20insufficient%20generalization%20across%20different%20tasks.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20AQuilt%2C%20a%20framework%20for%20constructing%0Ainstruction-tuning%20data%20for%20any%20specialized%20domains%20from%20corresponding%0Aunlabeled%20data%2C%20including%20Answer%2C%20Question%2C%20Unlabeled%20data%2C%20Inspection%2C%20Logic%2C%0Aand%20Task%20type.%20By%20incorporating%20logic%20and%20inspection%2C%20we%20encourage%20reasoning%0Aprocesses%20and%20self-inspection%20to%20enhance%20model%20performance.%20Moreover%2C%0Acustomizable%20task%20instructions%20enable%20high-quality%20data%20generation%20for%20any%0Atask.%20As%20a%20result%2C%20we%20construct%20a%20dataset%20of%20703k%20examples%20to%20train%20a%20powerful%0Adata%20synthesis%20model.%20Experiments%20show%20that%20AQuilt%20is%20comparable%20to%20DeepSeek-V3%0Awhile%20utilizing%20just%2017%25%20of%20the%20production%20cost.%20Further%20analysis%20demonstrates%0Athat%20our%20generated%20data%20exhibits%20higher%20relevance%20to%20downstream%20tasks.%20Source%0Acode%2C%20models%2C%20and%20scripts%20are%20available%20at%20https%3A//github.com/Krueske/AQuilt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAQuilt%253A%2520Weaving%2520Logic%2520and%2520Self-Inspection%2520into%2520Low-Cost%252C%2520High-Relevance%250A%2520%2520Data%2520Synthesis%2520for%2520Specialist%2520LLMs%26entry.906535625%3DXiaopeng%2520Ke%2520and%2520Hexuan%2520Deng%2520and%2520Xuebo%2520Liu%2520and%2520Jun%2520Rao%2520and%2520Zhenxi%2520Song%2520and%2520Jun%2520Yu%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520general%250Adomains%252C%2520they%2520often%2520underperform%2520in%2520specialized%2520domains.%2520Existing%2520approaches%250Atypically%2520rely%2520on%2520data%2520synthesis%2520methods%2520and%2520yield%2520promising%2520results%2520by%2520using%250Aunlabeled%2520data%2520to%2520capture%2520domain-specific%2520features.%2520However%252C%2520these%2520methods%250Aeither%2520incur%2520high%2520computational%2520costs%2520or%2520suffer%2520from%2520performance%2520limitations%252C%250Awhile%2520also%2520demonstrating%2520insufficient%2520generalization%2520across%2520different%2520tasks.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520AQuilt%252C%2520a%2520framework%2520for%2520constructing%250Ainstruction-tuning%2520data%2520for%2520any%2520specialized%2520domains%2520from%2520corresponding%250Aunlabeled%2520data%252C%2520including%2520Answer%252C%2520Question%252C%2520Unlabeled%2520data%252C%2520Inspection%252C%2520Logic%252C%250Aand%2520Task%2520type.%2520By%2520incorporating%2520logic%2520and%2520inspection%252C%2520we%2520encourage%2520reasoning%250Aprocesses%2520and%2520self-inspection%2520to%2520enhance%2520model%2520performance.%2520Moreover%252C%250Acustomizable%2520task%2520instructions%2520enable%2520high-quality%2520data%2520generation%2520for%2520any%250Atask.%2520As%2520a%2520result%252C%2520we%2520construct%2520a%2520dataset%2520of%2520703k%2520examples%2520to%2520train%2520a%2520powerful%250Adata%2520synthesis%2520model.%2520Experiments%2520show%2520that%2520AQuilt%2520is%2520comparable%2520to%2520DeepSeek-V3%250Awhile%2520utilizing%2520just%252017%2525%2520of%2520the%2520production%2520cost.%2520Further%2520analysis%2520demonstrates%250Athat%2520our%2520generated%2520data%2520exhibits%2520higher%2520relevance%2520to%2520downstream%2520tasks.%2520Source%250Acode%252C%2520models%252C%2520and%2520scripts%2520are%2520available%2520at%2520https%253A//github.com/Krueske/AQuilt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AQuilt%3A%20Weaving%20Logic%20and%20Self-Inspection%20into%20Low-Cost%2C%20High-Relevance%0A%20%20Data%20Synthesis%20for%20Specialist%20LLMs&entry.906535625=Xiaopeng%20Ke%20and%20Hexuan%20Deng%20and%20Xuebo%20Liu%20and%20Jun%20Rao%20and%20Zhenxi%20Song%20and%20Jun%20Yu%20and%20Min%20Zhang&entry.1292438233=%20%20Despite%20the%20impressive%20performance%20of%20large%20language%20models%20%28LLMs%29%20in%20general%0Adomains%2C%20they%20often%20underperform%20in%20specialized%20domains.%20Existing%20approaches%0Atypically%20rely%20on%20data%20synthesis%20methods%20and%20yield%20promising%20results%20by%20using%0Aunlabeled%20data%20to%20capture%20domain-specific%20features.%20However%2C%20these%20methods%0Aeither%20incur%20high%20computational%20costs%20or%20suffer%20from%20performance%20limitations%2C%0Awhile%20also%20demonstrating%20insufficient%20generalization%20across%20different%20tasks.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20AQuilt%2C%20a%20framework%20for%20constructing%0Ainstruction-tuning%20data%20for%20any%20specialized%20domains%20from%20corresponding%0Aunlabeled%20data%2C%20including%20Answer%2C%20Question%2C%20Unlabeled%20data%2C%20Inspection%2C%20Logic%2C%0Aand%20Task%20type.%20By%20incorporating%20logic%20and%20inspection%2C%20we%20encourage%20reasoning%0Aprocesses%20and%20self-inspection%20to%20enhance%20model%20performance.%20Moreover%2C%0Acustomizable%20task%20instructions%20enable%20high-quality%20data%20generation%20for%20any%0Atask.%20As%20a%20result%2C%20we%20construct%20a%20dataset%20of%20703k%20examples%20to%20train%20a%20powerful%0Adata%20synthesis%20model.%20Experiments%20show%20that%20AQuilt%20is%20comparable%20to%20DeepSeek-V3%0Awhile%20utilizing%20just%2017%25%20of%20the%20production%20cost.%20Further%20analysis%20demonstrates%0Athat%20our%20generated%20data%20exhibits%20higher%20relevance%20to%20downstream%20tasks.%20Source%0Acode%2C%20models%2C%20and%20scripts%20are%20available%20at%20https%3A//github.com/Krueske/AQuilt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18584v1&entry.124074799=Read"},
{"title": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent\n  Thinking in LLMs for Financial Scenarios", "author": "Zhuang Qiang Bok and Watson Wei Khong Chua", "abstract": "  Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance.\n", "link": "http://arxiv.org/abs/2507.18368v1", "date": "2025-07-24", "relevancy": 2.5218, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5194}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5194}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Beyond%20the%20Obvious%3A%20Evaluating%20Divergent%20and%20Convergent%0A%20%20Thinking%20in%20LLMs%20for%20Financial%20Scenarios&body=Title%3A%20Reasoning%20Beyond%20the%20Obvious%3A%20Evaluating%20Divergent%20and%20Convergent%0A%20%20Thinking%20in%20LLMs%20for%20Financial%20Scenarios%0AAuthor%3A%20Zhuang%20Qiang%20Bok%20and%20Watson%20Wei%20Khong%20Chua%0AAbstract%3A%20%20%20Most%20reasoning%20benchmarks%20for%20LLMs%20emphasize%20factual%20accuracy%20or%20step-by-step%0Alogic.%20In%20finance%2C%20however%2C%20professionals%20must%20not%20only%20converge%20on%20optimal%0Adecisions%20but%20also%20generate%20creative%2C%20plausible%20futures%20under%20uncertainty.%20We%0Aintroduce%20ConDiFi%2C%20a%20benchmark%20that%20jointly%20evaluates%20divergent%20and%20convergent%0Athinking%20in%20LLMs%20for%20financial%20tasks.%0A%20%20ConDiFi%20features%20607%20macro-financial%20prompts%20for%20divergent%20reasoning%20and%20990%0Amulti-hop%20adversarial%20MCQs%20for%20convergent%20reasoning.%20Using%20this%20benchmark%2C%20we%0Aevaluated%2014%20leading%20models%20and%20uncovered%20striking%20differences.%20Despite%20high%0Afluency%2C%20GPT-4o%20underperforms%20on%20Novelty%20and%20Actionability.%20In%20contrast%2C%20models%0Alike%20DeepSeek-R1%20and%20Cohere%20Command%20R%2B%20rank%20among%20the%20top%20for%20generating%0Aactionable%2C%20insights%20suitable%20for%20investment%20decisions.%20ConDiFi%20provides%20a%20new%0Aperspective%20to%20assess%20reasoning%20capabilities%20essential%20to%20safe%20and%20strategic%0Adeployment%20of%20LLMs%20in%20finance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Beyond%2520the%2520Obvious%253A%2520Evaluating%2520Divergent%2520and%2520Convergent%250A%2520%2520Thinking%2520in%2520LLMs%2520for%2520Financial%2520Scenarios%26entry.906535625%3DZhuang%2520Qiang%2520Bok%2520and%2520Watson%2520Wei%2520Khong%2520Chua%26entry.1292438233%3D%2520%2520Most%2520reasoning%2520benchmarks%2520for%2520LLMs%2520emphasize%2520factual%2520accuracy%2520or%2520step-by-step%250Alogic.%2520In%2520finance%252C%2520however%252C%2520professionals%2520must%2520not%2520only%2520converge%2520on%2520optimal%250Adecisions%2520but%2520also%2520generate%2520creative%252C%2520plausible%2520futures%2520under%2520uncertainty.%2520We%250Aintroduce%2520ConDiFi%252C%2520a%2520benchmark%2520that%2520jointly%2520evaluates%2520divergent%2520and%2520convergent%250Athinking%2520in%2520LLMs%2520for%2520financial%2520tasks.%250A%2520%2520ConDiFi%2520features%2520607%2520macro-financial%2520prompts%2520for%2520divergent%2520reasoning%2520and%2520990%250Amulti-hop%2520adversarial%2520MCQs%2520for%2520convergent%2520reasoning.%2520Using%2520this%2520benchmark%252C%2520we%250Aevaluated%252014%2520leading%2520models%2520and%2520uncovered%2520striking%2520differences.%2520Despite%2520high%250Afluency%252C%2520GPT-4o%2520underperforms%2520on%2520Novelty%2520and%2520Actionability.%2520In%2520contrast%252C%2520models%250Alike%2520DeepSeek-R1%2520and%2520Cohere%2520Command%2520R%252B%2520rank%2520among%2520the%2520top%2520for%2520generating%250Aactionable%252C%2520insights%2520suitable%2520for%2520investment%2520decisions.%2520ConDiFi%2520provides%2520a%2520new%250Aperspective%2520to%2520assess%2520reasoning%2520capabilities%2520essential%2520to%2520safe%2520and%2520strategic%250Adeployment%2520of%2520LLMs%2520in%2520finance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Beyond%20the%20Obvious%3A%20Evaluating%20Divergent%20and%20Convergent%0A%20%20Thinking%20in%20LLMs%20for%20Financial%20Scenarios&entry.906535625=Zhuang%20Qiang%20Bok%20and%20Watson%20Wei%20Khong%20Chua&entry.1292438233=%20%20Most%20reasoning%20benchmarks%20for%20LLMs%20emphasize%20factual%20accuracy%20or%20step-by-step%0Alogic.%20In%20finance%2C%20however%2C%20professionals%20must%20not%20only%20converge%20on%20optimal%0Adecisions%20but%20also%20generate%20creative%2C%20plausible%20futures%20under%20uncertainty.%20We%0Aintroduce%20ConDiFi%2C%20a%20benchmark%20that%20jointly%20evaluates%20divergent%20and%20convergent%0Athinking%20in%20LLMs%20for%20financial%20tasks.%0A%20%20ConDiFi%20features%20607%20macro-financial%20prompts%20for%20divergent%20reasoning%20and%20990%0Amulti-hop%20adversarial%20MCQs%20for%20convergent%20reasoning.%20Using%20this%20benchmark%2C%20we%0Aevaluated%2014%20leading%20models%20and%20uncovered%20striking%20differences.%20Despite%20high%0Afluency%2C%20GPT-4o%20underperforms%20on%20Novelty%20and%20Actionability.%20In%20contrast%2C%20models%0Alike%20DeepSeek-R1%20and%20Cohere%20Command%20R%2B%20rank%20among%20the%20top%20for%20generating%0Aactionable%2C%20insights%20suitable%20for%20investment%20decisions.%20ConDiFi%20provides%20a%20new%0Aperspective%20to%20assess%20reasoning%20capabilities%20essential%20to%20safe%20and%20strategic%0Adeployment%20of%20LLMs%20in%20finance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18368v1&entry.124074799=Read"},
{"title": "Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume\n  Construction", "author": "Runmin Zhang and Zhu Yu and Si-Yuan Cao and Lingyu Zhu and Guangyi Zhang and Xiaokai Bai and Hui-Liang Shen", "abstract": "  This work presents SGCDet, a novel multi-view indoor 3D object detection\nframework based on adaptive 3D volume construction. Unlike previous approaches\nthat restrict the receptive field of voxels to fixed locations on images, we\nintroduce a geometry and context aware aggregation module to integrate\ngeometric and contextual information within adaptive regions in each image and\ndynamically adjust the contributions from different views, enhancing the\nrepresentation capability of voxel features. Furthermore, we propose a sparse\nvolume construction strategy that adaptively identifies and selects voxels with\nhigh occupancy probabilities for feature refinement, minimizing redundant\ncomputation in free space. Benefiting from the above designs, our framework\nachieves effective and efficient volume construction in an adaptive way. Better\nstill, our network can be supervised using only 3D bounding boxes, eliminating\nthe dependence on ground-truth scene geometry. Experimental results demonstrate\nthat SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200\nand ARKitScenes datasets. The source code is available at\nhttps://github.com/RM-Zhang/SGCDet.\n", "link": "http://arxiv.org/abs/2507.18331v1", "date": "2025-07-24", "relevancy": 2.4786, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6245}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6245}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Multi-View%20Indoor%203D%20Object%20Detection%20via%20Adaptive%203D%20Volume%0A%20%20Construction&body=Title%3A%20Boosting%20Multi-View%20Indoor%203D%20Object%20Detection%20via%20Adaptive%203D%20Volume%0A%20%20Construction%0AAuthor%3A%20Runmin%20Zhang%20and%20Zhu%20Yu%20and%20Si-Yuan%20Cao%20and%20Lingyu%20Zhu%20and%20Guangyi%20Zhang%20and%20Xiaokai%20Bai%20and%20Hui-Liang%20Shen%0AAbstract%3A%20%20%20This%20work%20presents%20SGCDet%2C%20a%20novel%20multi-view%20indoor%203D%20object%20detection%0Aframework%20based%20on%20adaptive%203D%20volume%20construction.%20Unlike%20previous%20approaches%0Athat%20restrict%20the%20receptive%20field%20of%20voxels%20to%20fixed%20locations%20on%20images%2C%20we%0Aintroduce%20a%20geometry%20and%20context%20aware%20aggregation%20module%20to%20integrate%0Ageometric%20and%20contextual%20information%20within%20adaptive%20regions%20in%20each%20image%20and%0Adynamically%20adjust%20the%20contributions%20from%20different%20views%2C%20enhancing%20the%0Arepresentation%20capability%20of%20voxel%20features.%20Furthermore%2C%20we%20propose%20a%20sparse%0Avolume%20construction%20strategy%20that%20adaptively%20identifies%20and%20selects%20voxels%20with%0Ahigh%20occupancy%20probabilities%20for%20feature%20refinement%2C%20minimizing%20redundant%0Acomputation%20in%20free%20space.%20Benefiting%20from%20the%20above%20designs%2C%20our%20framework%0Aachieves%20effective%20and%20efficient%20volume%20construction%20in%20an%20adaptive%20way.%20Better%0Astill%2C%20our%20network%20can%20be%20supervised%20using%20only%203D%20bounding%20boxes%2C%20eliminating%0Athe%20dependence%20on%20ground-truth%20scene%20geometry.%20Experimental%20results%20demonstrate%0Athat%20SGCDet%20achieves%20state-of-the-art%20performance%20on%20the%20ScanNet%2C%20ScanNet200%0Aand%20ARKitScenes%20datasets.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/RM-Zhang/SGCDet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Multi-View%2520Indoor%25203D%2520Object%2520Detection%2520via%2520Adaptive%25203D%2520Volume%250A%2520%2520Construction%26entry.906535625%3DRunmin%2520Zhang%2520and%2520Zhu%2520Yu%2520and%2520Si-Yuan%2520Cao%2520and%2520Lingyu%2520Zhu%2520and%2520Guangyi%2520Zhang%2520and%2520Xiaokai%2520Bai%2520and%2520Hui-Liang%2520Shen%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520SGCDet%252C%2520a%2520novel%2520multi-view%2520indoor%25203D%2520object%2520detection%250Aframework%2520based%2520on%2520adaptive%25203D%2520volume%2520construction.%2520Unlike%2520previous%2520approaches%250Athat%2520restrict%2520the%2520receptive%2520field%2520of%2520voxels%2520to%2520fixed%2520locations%2520on%2520images%252C%2520we%250Aintroduce%2520a%2520geometry%2520and%2520context%2520aware%2520aggregation%2520module%2520to%2520integrate%250Ageometric%2520and%2520contextual%2520information%2520within%2520adaptive%2520regions%2520in%2520each%2520image%2520and%250Adynamically%2520adjust%2520the%2520contributions%2520from%2520different%2520views%252C%2520enhancing%2520the%250Arepresentation%2520capability%2520of%2520voxel%2520features.%2520Furthermore%252C%2520we%2520propose%2520a%2520sparse%250Avolume%2520construction%2520strategy%2520that%2520adaptively%2520identifies%2520and%2520selects%2520voxels%2520with%250Ahigh%2520occupancy%2520probabilities%2520for%2520feature%2520refinement%252C%2520minimizing%2520redundant%250Acomputation%2520in%2520free%2520space.%2520Benefiting%2520from%2520the%2520above%2520designs%252C%2520our%2520framework%250Aachieves%2520effective%2520and%2520efficient%2520volume%2520construction%2520in%2520an%2520adaptive%2520way.%2520Better%250Astill%252C%2520our%2520network%2520can%2520be%2520supervised%2520using%2520only%25203D%2520bounding%2520boxes%252C%2520eliminating%250Athe%2520dependence%2520on%2520ground-truth%2520scene%2520geometry.%2520Experimental%2520results%2520demonstrate%250Athat%2520SGCDet%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520ScanNet%252C%2520ScanNet200%250Aand%2520ARKitScenes%2520datasets.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/RM-Zhang/SGCDet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Multi-View%20Indoor%203D%20Object%20Detection%20via%20Adaptive%203D%20Volume%0A%20%20Construction&entry.906535625=Runmin%20Zhang%20and%20Zhu%20Yu%20and%20Si-Yuan%20Cao%20and%20Lingyu%20Zhu%20and%20Guangyi%20Zhang%20and%20Xiaokai%20Bai%20and%20Hui-Liang%20Shen&entry.1292438233=%20%20This%20work%20presents%20SGCDet%2C%20a%20novel%20multi-view%20indoor%203D%20object%20detection%0Aframework%20based%20on%20adaptive%203D%20volume%20construction.%20Unlike%20previous%20approaches%0Athat%20restrict%20the%20receptive%20field%20of%20voxels%20to%20fixed%20locations%20on%20images%2C%20we%0Aintroduce%20a%20geometry%20and%20context%20aware%20aggregation%20module%20to%20integrate%0Ageometric%20and%20contextual%20information%20within%20adaptive%20regions%20in%20each%20image%20and%0Adynamically%20adjust%20the%20contributions%20from%20different%20views%2C%20enhancing%20the%0Arepresentation%20capability%20of%20voxel%20features.%20Furthermore%2C%20we%20propose%20a%20sparse%0Avolume%20construction%20strategy%20that%20adaptively%20identifies%20and%20selects%20voxels%20with%0Ahigh%20occupancy%20probabilities%20for%20feature%20refinement%2C%20minimizing%20redundant%0Acomputation%20in%20free%20space.%20Benefiting%20from%20the%20above%20designs%2C%20our%20framework%0Aachieves%20effective%20and%20efficient%20volume%20construction%20in%20an%20adaptive%20way.%20Better%0Astill%2C%20our%20network%20can%20be%20supervised%20using%20only%203D%20bounding%20boxes%2C%20eliminating%0Athe%20dependence%20on%20ground-truth%20scene%20geometry.%20Experimental%20results%20demonstrate%0Athat%20SGCDet%20achieves%20state-of-the-art%20performance%20on%20the%20ScanNet%2C%20ScanNet200%0Aand%20ARKitScenes%20datasets.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/RM-Zhang/SGCDet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18331v1&entry.124074799=Read"},
{"title": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1", "author": "Marcin Pietro\u0144 and Rafa\u0142 Olszowski and Jakub Gomu\u0142ka and Filip Gampel and Andrzej Tomski", "abstract": "  Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.\n", "link": "http://arxiv.org/abs/2507.08621v2", "date": "2025-07-24", "relevancy": 2.4756, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20comprehensive%20study%20of%20LLM-based%20argument%20classification%3A%20from%20LLAMA%0A%20%20through%20GPT-4o%20to%20Deepseek-R1&body=Title%3A%20A%20comprehensive%20study%20of%20LLM-based%20argument%20classification%3A%20from%20LLAMA%0A%20%20through%20GPT-4o%20to%20Deepseek-R1%0AAuthor%3A%20Marcin%20Pietro%C5%84%20and%20Rafa%C5%82%20Olszowski%20and%20Jakub%20Gomu%C5%82ka%20and%20Filip%20Gampel%20and%20Andrzej%20Tomski%0AAbstract%3A%20%20%20Argument%20mining%20%28AM%29%20is%20an%20interdisciplinary%20research%20field%20that%20integrates%0Ainsights%20from%20logic%2C%20philosophy%2C%20linguistics%2C%20rhetoric%2C%20law%2C%20psychology%2C%20and%0Acomputer%20science.%20It%20involves%20the%20automatic%20identification%20and%20extraction%20of%0Aargumentative%20components%2C%20such%20as%20premises%20and%20claims%2C%20and%20the%20detection%20of%0Arelationships%20between%20them%2C%20such%20as%20support%2C%20attack%2C%20or%20neutrality.%20Recently%2C%0Athe%20field%20has%20advanced%20significantly%2C%20especially%20with%20the%20advent%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20which%20have%20enhanced%20the%20efficiency%20of%20analyzing%20and%0Aextracting%20argument%20semantics%20compared%20to%20traditional%20methods%20and%20other%20deep%0Alearning%20models.%20There%20are%20many%20benchmarks%20for%20testing%20and%20verifying%20the%0Aquality%20of%20LLM%2C%20but%20there%20is%20still%20a%20lack%20of%20research%20and%20results%20on%20the%0Aoperation%20of%20these%20models%20in%20publicly%20available%20argument%20classification%0Adatabases.%20This%20paper%20presents%20a%20study%20of%20a%20selection%20of%20LLM%27s%2C%20using%20diverse%0Adatasets%20such%20as%20Args.me%20and%20UKP.%20The%20models%20tested%20include%20versions%20of%20GPT%2C%0ALlama%2C%20and%20DeepSeek%2C%20along%20with%20reasoning-enhanced%20variants%20incorporating%20the%0AChain-of-Thoughts%20algorithm.%20The%20results%20indicate%20that%20ChatGPT-4o%20outperforms%0Athe%20others%20in%20the%20argument%20classification%20benchmarks.%20In%20case%20of%20models%0Aincorporated%20with%20reasoning%20capabilities%2C%20the%20Deepseek-R1%20shows%20its%0Asuperiority.%20However%2C%20despite%20their%20superiority%2C%20GPT-4o%20and%20Deepseek-R1%20still%0Amake%20errors.%20The%20most%20common%20errors%20are%20discussed%20for%20all%20models.%20To%20our%0Aknowledge%2C%20the%20presented%20work%20is%20the%20first%20broader%20analysis%20of%20the%20mentioned%0Adatasets%20using%20LLM%20and%20prompt%20algorithms.%20The%20work%20also%20shows%20some%20weaknesses%0Aof%20known%20prompt%20algorithms%20in%20argument%20analysis%2C%20while%20indicating%20directions%0Afor%20their%20improvement.%20The%20added%20value%20of%20the%20work%20is%20the%20in-depth%20analysis%20of%0Athe%20available%20argument%20datasets%20and%20the%20demonstration%20of%20their%20shortcomings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520comprehensive%2520study%2520of%2520LLM-based%2520argument%2520classification%253A%2520from%2520LLAMA%250A%2520%2520through%2520GPT-4o%2520to%2520Deepseek-R1%26entry.906535625%3DMarcin%2520Pietro%25C5%2584%2520and%2520Rafa%25C5%2582%2520Olszowski%2520and%2520Jakub%2520Gomu%25C5%2582ka%2520and%2520Filip%2520Gampel%2520and%2520Andrzej%2520Tomski%26entry.1292438233%3D%2520%2520Argument%2520mining%2520%2528AM%2529%2520is%2520an%2520interdisciplinary%2520research%2520field%2520that%2520integrates%250Ainsights%2520from%2520logic%252C%2520philosophy%252C%2520linguistics%252C%2520rhetoric%252C%2520law%252C%2520psychology%252C%2520and%250Acomputer%2520science.%2520It%2520involves%2520the%2520automatic%2520identification%2520and%2520extraction%2520of%250Aargumentative%2520components%252C%2520such%2520as%2520premises%2520and%2520claims%252C%2520and%2520the%2520detection%2520of%250Arelationships%2520between%2520them%252C%2520such%2520as%2520support%252C%2520attack%252C%2520or%2520neutrality.%2520Recently%252C%250Athe%2520field%2520has%2520advanced%2520significantly%252C%2520especially%2520with%2520the%2520advent%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520which%2520have%2520enhanced%2520the%2520efficiency%2520of%2520analyzing%2520and%250Aextracting%2520argument%2520semantics%2520compared%2520to%2520traditional%2520methods%2520and%2520other%2520deep%250Alearning%2520models.%2520There%2520are%2520many%2520benchmarks%2520for%2520testing%2520and%2520verifying%2520the%250Aquality%2520of%2520LLM%252C%2520but%2520there%2520is%2520still%2520a%2520lack%2520of%2520research%2520and%2520results%2520on%2520the%250Aoperation%2520of%2520these%2520models%2520in%2520publicly%2520available%2520argument%2520classification%250Adatabases.%2520This%2520paper%2520presents%2520a%2520study%2520of%2520a%2520selection%2520of%2520LLM%2527s%252C%2520using%2520diverse%250Adatasets%2520such%2520as%2520Args.me%2520and%2520UKP.%2520The%2520models%2520tested%2520include%2520versions%2520of%2520GPT%252C%250ALlama%252C%2520and%2520DeepSeek%252C%2520along%2520with%2520reasoning-enhanced%2520variants%2520incorporating%2520the%250AChain-of-Thoughts%2520algorithm.%2520The%2520results%2520indicate%2520that%2520ChatGPT-4o%2520outperforms%250Athe%2520others%2520in%2520the%2520argument%2520classification%2520benchmarks.%2520In%2520case%2520of%2520models%250Aincorporated%2520with%2520reasoning%2520capabilities%252C%2520the%2520Deepseek-R1%2520shows%2520its%250Asuperiority.%2520However%252C%2520despite%2520their%2520superiority%252C%2520GPT-4o%2520and%2520Deepseek-R1%2520still%250Amake%2520errors.%2520The%2520most%2520common%2520errors%2520are%2520discussed%2520for%2520all%2520models.%2520To%2520our%250Aknowledge%252C%2520the%2520presented%2520work%2520is%2520the%2520first%2520broader%2520analysis%2520of%2520the%2520mentioned%250Adatasets%2520using%2520LLM%2520and%2520prompt%2520algorithms.%2520The%2520work%2520also%2520shows%2520some%2520weaknesses%250Aof%2520known%2520prompt%2520algorithms%2520in%2520argument%2520analysis%252C%2520while%2520indicating%2520directions%250Afor%2520their%2520improvement.%2520The%2520added%2520value%2520of%2520the%2520work%2520is%2520the%2520in-depth%2520analysis%2520of%250Athe%2520available%2520argument%2520datasets%2520and%2520the%2520demonstration%2520of%2520their%2520shortcomings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comprehensive%20study%20of%20LLM-based%20argument%20classification%3A%20from%20LLAMA%0A%20%20through%20GPT-4o%20to%20Deepseek-R1&entry.906535625=Marcin%20Pietro%C5%84%20and%20Rafa%C5%82%20Olszowski%20and%20Jakub%20Gomu%C5%82ka%20and%20Filip%20Gampel%20and%20Andrzej%20Tomski&entry.1292438233=%20%20Argument%20mining%20%28AM%29%20is%20an%20interdisciplinary%20research%20field%20that%20integrates%0Ainsights%20from%20logic%2C%20philosophy%2C%20linguistics%2C%20rhetoric%2C%20law%2C%20psychology%2C%20and%0Acomputer%20science.%20It%20involves%20the%20automatic%20identification%20and%20extraction%20of%0Aargumentative%20components%2C%20such%20as%20premises%20and%20claims%2C%20and%20the%20detection%20of%0Arelationships%20between%20them%2C%20such%20as%20support%2C%20attack%2C%20or%20neutrality.%20Recently%2C%0Athe%20field%20has%20advanced%20significantly%2C%20especially%20with%20the%20advent%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20which%20have%20enhanced%20the%20efficiency%20of%20analyzing%20and%0Aextracting%20argument%20semantics%20compared%20to%20traditional%20methods%20and%20other%20deep%0Alearning%20models.%20There%20are%20many%20benchmarks%20for%20testing%20and%20verifying%20the%0Aquality%20of%20LLM%2C%20but%20there%20is%20still%20a%20lack%20of%20research%20and%20results%20on%20the%0Aoperation%20of%20these%20models%20in%20publicly%20available%20argument%20classification%0Adatabases.%20This%20paper%20presents%20a%20study%20of%20a%20selection%20of%20LLM%27s%2C%20using%20diverse%0Adatasets%20such%20as%20Args.me%20and%20UKP.%20The%20models%20tested%20include%20versions%20of%20GPT%2C%0ALlama%2C%20and%20DeepSeek%2C%20along%20with%20reasoning-enhanced%20variants%20incorporating%20the%0AChain-of-Thoughts%20algorithm.%20The%20results%20indicate%20that%20ChatGPT-4o%20outperforms%0Athe%20others%20in%20the%20argument%20classification%20benchmarks.%20In%20case%20of%20models%0Aincorporated%20with%20reasoning%20capabilities%2C%20the%20Deepseek-R1%20shows%20its%0Asuperiority.%20However%2C%20despite%20their%20superiority%2C%20GPT-4o%20and%20Deepseek-R1%20still%0Amake%20errors.%20The%20most%20common%20errors%20are%20discussed%20for%20all%20models.%20To%20our%0Aknowledge%2C%20the%20presented%20work%20is%20the%20first%20broader%20analysis%20of%20the%20mentioned%0Adatasets%20using%20LLM%20and%20prompt%20algorithms.%20The%20work%20also%20shows%20some%20weaknesses%0Aof%20known%20prompt%20algorithms%20in%20argument%20analysis%2C%20while%20indicating%20directions%0Afor%20their%20improvement.%20The%20added%20value%20of%20the%20work%20is%20the%20in-depth%20analysis%20of%0Athe%20available%20argument%20datasets%20and%20the%20demonstration%20of%20their%20shortcomings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08621v2&entry.124074799=Read"},
{"title": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface", "author": "Urchade Zaratiana and Gil Pasternak and Oliver Boyd and George Hurn-Maloney and Ash Lewis", "abstract": "  Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.\n", "link": "http://arxiv.org/abs/2507.18546v1", "date": "2025-07-24", "relevancy": 2.4609, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLiNER2%3A%20An%20Efficient%20Multi-Task%20Information%20Extraction%20System%20with%0A%20%20Schema-Driven%20Interface&body=Title%3A%20GLiNER2%3A%20An%20Efficient%20Multi-Task%20Information%20Extraction%20System%20with%0A%20%20Schema-Driven%20Interface%0AAuthor%3A%20Urchade%20Zaratiana%20and%20Gil%20Pasternak%20and%20Oliver%20Boyd%20and%20George%20Hurn-Maloney%20and%20Ash%20Lewis%0AAbstract%3A%20%20%20Information%20extraction%20%28IE%29%20is%20fundamental%20to%20numerous%20NLP%20applications%2C%20yet%0Aexisting%20solutions%20often%20require%20specialized%20models%20for%20different%20tasks%20or%20rely%0Aon%20computationally%20expensive%20large%20language%20models.%20We%20present%20GLiNER2%2C%20a%0Aunified%20framework%20that%20enhances%20the%20original%20GLiNER%20architecture%20to%20support%0Anamed%20entity%20recognition%2C%20text%20classification%2C%20and%20hierarchical%20structured%20data%0Aextraction%20within%20a%20single%20efficient%20model.%20Built%20pretrained%20transformer%0Aencoder%20architecture%2C%20GLiNER2%20maintains%20CPU%20efficiency%20and%20compact%20size%20while%0Aintroducing%20multi-task%20composition%20through%20an%20intuitive%20schema-based%20interface.%0AOur%20experiments%20demonstrate%20competitive%20performance%20across%20extraction%20and%0Aclassification%20tasks%20with%20substantial%20improvements%20in%20deployment%20accessibility%0Acompared%20to%20LLM-based%20alternatives.%20We%20release%20GLiNER2%20as%20an%20open-source%0Apip-installable%20library%20with%20pre-trained%20models%20and%20documentation%20at%0Ahttps%3A//github.com/fastino-ai/GLiNER2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLiNER2%253A%2520An%2520Efficient%2520Multi-Task%2520Information%2520Extraction%2520System%2520with%250A%2520%2520Schema-Driven%2520Interface%26entry.906535625%3DUrchade%2520Zaratiana%2520and%2520Gil%2520Pasternak%2520and%2520Oliver%2520Boyd%2520and%2520George%2520Hurn-Maloney%2520and%2520Ash%2520Lewis%26entry.1292438233%3D%2520%2520Information%2520extraction%2520%2528IE%2529%2520is%2520fundamental%2520to%2520numerous%2520NLP%2520applications%252C%2520yet%250Aexisting%2520solutions%2520often%2520require%2520specialized%2520models%2520for%2520different%2520tasks%2520or%2520rely%250Aon%2520computationally%2520expensive%2520large%2520language%2520models.%2520We%2520present%2520GLiNER2%252C%2520a%250Aunified%2520framework%2520that%2520enhances%2520the%2520original%2520GLiNER%2520architecture%2520to%2520support%250Anamed%2520entity%2520recognition%252C%2520text%2520classification%252C%2520and%2520hierarchical%2520structured%2520data%250Aextraction%2520within%2520a%2520single%2520efficient%2520model.%2520Built%2520pretrained%2520transformer%250Aencoder%2520architecture%252C%2520GLiNER2%2520maintains%2520CPU%2520efficiency%2520and%2520compact%2520size%2520while%250Aintroducing%2520multi-task%2520composition%2520through%2520an%2520intuitive%2520schema-based%2520interface.%250AOur%2520experiments%2520demonstrate%2520competitive%2520performance%2520across%2520extraction%2520and%250Aclassification%2520tasks%2520with%2520substantial%2520improvements%2520in%2520deployment%2520accessibility%250Acompared%2520to%2520LLM-based%2520alternatives.%2520We%2520release%2520GLiNER2%2520as%2520an%2520open-source%250Apip-installable%2520library%2520with%2520pre-trained%2520models%2520and%2520documentation%2520at%250Ahttps%253A//github.com/fastino-ai/GLiNER2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLiNER2%3A%20An%20Efficient%20Multi-Task%20Information%20Extraction%20System%20with%0A%20%20Schema-Driven%20Interface&entry.906535625=Urchade%20Zaratiana%20and%20Gil%20Pasternak%20and%20Oliver%20Boyd%20and%20George%20Hurn-Maloney%20and%20Ash%20Lewis&entry.1292438233=%20%20Information%20extraction%20%28IE%29%20is%20fundamental%20to%20numerous%20NLP%20applications%2C%20yet%0Aexisting%20solutions%20often%20require%20specialized%20models%20for%20different%20tasks%20or%20rely%0Aon%20computationally%20expensive%20large%20language%20models.%20We%20present%20GLiNER2%2C%20a%0Aunified%20framework%20that%20enhances%20the%20original%20GLiNER%20architecture%20to%20support%0Anamed%20entity%20recognition%2C%20text%20classification%2C%20and%20hierarchical%20structured%20data%0Aextraction%20within%20a%20single%20efficient%20model.%20Built%20pretrained%20transformer%0Aencoder%20architecture%2C%20GLiNER2%20maintains%20CPU%20efficiency%20and%20compact%20size%20while%0Aintroducing%20multi-task%20composition%20through%20an%20intuitive%20schema-based%20interface.%0AOur%20experiments%20demonstrate%20competitive%20performance%20across%20extraction%20and%0Aclassification%20tasks%20with%20substantial%20improvements%20in%20deployment%20accessibility%0Acompared%20to%20LLM-based%20alternatives.%20We%20release%20GLiNER2%20as%20an%20open-source%0Apip-installable%20library%20with%20pre-trained%20models%20and%20documentation%20at%0Ahttps%3A//github.com/fastino-ai/GLiNER2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18546v1&entry.124074799=Read"},
{"title": "Object segmentation in the wild with foundation models: application to\n  vision assisted neuro-prostheses for upper limbs", "author": "Bolutife Atoki and Jenny Benois-Pineau and Renaud P\u00e9teri and Fabien Baldacci and Aymar de Rugy", "abstract": "  In this work, we address the problem of semantic object segmentation using\nfoundation models. We investigate whether foundation models, trained on a large\nnumber and variety of objects, can perform object segmentation without\nfine-tuning on specific images containing everyday objects, but in highly\ncluttered visual scenes. The ''in the wild'' context is driven by the target\napplication of vision guided upper limb neuroprostheses. We propose a method\nfor generating prompts based on gaze fixations to guide the Segment Anything\nModel (SAM) in our segmentation scenario, and fine-tune it on egocentric visual\ndata. Evaluation results of our approach show an improvement of the IoU\nsegmentation quality metric by up to 0.51 points on real-world challenging data\nof Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform\n(https://universe.roboflow.com/iwrist/grasping-in-the-wild)\n", "link": "http://arxiv.org/abs/2507.18517v1", "date": "2025-07-24", "relevancy": 2.4593, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6199}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6199}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20segmentation%20in%20the%20wild%20with%20foundation%20models%3A%20application%20to%0A%20%20vision%20assisted%20neuro-prostheses%20for%20upper%20limbs&body=Title%3A%20Object%20segmentation%20in%20the%20wild%20with%20foundation%20models%3A%20application%20to%0A%20%20vision%20assisted%20neuro-prostheses%20for%20upper%20limbs%0AAuthor%3A%20Bolutife%20Atoki%20and%20Jenny%20Benois-Pineau%20and%20Renaud%20P%C3%A9teri%20and%20Fabien%20Baldacci%20and%20Aymar%20de%20Rugy%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20semantic%20object%20segmentation%20using%0Afoundation%20models.%20We%20investigate%20whether%20foundation%20models%2C%20trained%20on%20a%20large%0Anumber%20and%20variety%20of%20objects%2C%20can%20perform%20object%20segmentation%20without%0Afine-tuning%20on%20specific%20images%20containing%20everyday%20objects%2C%20but%20in%20highly%0Acluttered%20visual%20scenes.%20The%20%27%27in%20the%20wild%27%27%20context%20is%20driven%20by%20the%20target%0Aapplication%20of%20vision%20guided%20upper%20limb%20neuroprostheses.%20We%20propose%20a%20method%0Afor%20generating%20prompts%20based%20on%20gaze%20fixations%20to%20guide%20the%20Segment%20Anything%0AModel%20%28SAM%29%20in%20our%20segmentation%20scenario%2C%20and%20fine-tune%20it%20on%20egocentric%20visual%0Adata.%20Evaluation%20results%20of%20our%20approach%20show%20an%20improvement%20of%20the%20IoU%0Asegmentation%20quality%20metric%20by%20up%20to%200.51%20points%20on%20real-world%20challenging%20data%0Aof%20Grasping-in-the-Wild%20corpus%20which%20is%20made%20available%20on%20the%20RoboFlow%20Platform%0A%28https%3A//universe.roboflow.com/iwrist/grasping-in-the-wild%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520segmentation%2520in%2520the%2520wild%2520with%2520foundation%2520models%253A%2520application%2520to%250A%2520%2520vision%2520assisted%2520neuro-prostheses%2520for%2520upper%2520limbs%26entry.906535625%3DBolutife%2520Atoki%2520and%2520Jenny%2520Benois-Pineau%2520and%2520Renaud%2520P%25C3%25A9teri%2520and%2520Fabien%2520Baldacci%2520and%2520Aymar%2520de%2520Rugy%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520problem%2520of%2520semantic%2520object%2520segmentation%2520using%250Afoundation%2520models.%2520We%2520investigate%2520whether%2520foundation%2520models%252C%2520trained%2520on%2520a%2520large%250Anumber%2520and%2520variety%2520of%2520objects%252C%2520can%2520perform%2520object%2520segmentation%2520without%250Afine-tuning%2520on%2520specific%2520images%2520containing%2520everyday%2520objects%252C%2520but%2520in%2520highly%250Acluttered%2520visual%2520scenes.%2520The%2520%2527%2527in%2520the%2520wild%2527%2527%2520context%2520is%2520driven%2520by%2520the%2520target%250Aapplication%2520of%2520vision%2520guided%2520upper%2520limb%2520neuroprostheses.%2520We%2520propose%2520a%2520method%250Afor%2520generating%2520prompts%2520based%2520on%2520gaze%2520fixations%2520to%2520guide%2520the%2520Segment%2520Anything%250AModel%2520%2528SAM%2529%2520in%2520our%2520segmentation%2520scenario%252C%2520and%2520fine-tune%2520it%2520on%2520egocentric%2520visual%250Adata.%2520Evaluation%2520results%2520of%2520our%2520approach%2520show%2520an%2520improvement%2520of%2520the%2520IoU%250Asegmentation%2520quality%2520metric%2520by%2520up%2520to%25200.51%2520points%2520on%2520real-world%2520challenging%2520data%250Aof%2520Grasping-in-the-Wild%2520corpus%2520which%2520is%2520made%2520available%2520on%2520the%2520RoboFlow%2520Platform%250A%2528https%253A//universe.roboflow.com/iwrist/grasping-in-the-wild%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20segmentation%20in%20the%20wild%20with%20foundation%20models%3A%20application%20to%0A%20%20vision%20assisted%20neuro-prostheses%20for%20upper%20limbs&entry.906535625=Bolutife%20Atoki%20and%20Jenny%20Benois-Pineau%20and%20Renaud%20P%C3%A9teri%20and%20Fabien%20Baldacci%20and%20Aymar%20de%20Rugy&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20semantic%20object%20segmentation%20using%0Afoundation%20models.%20We%20investigate%20whether%20foundation%20models%2C%20trained%20on%20a%20large%0Anumber%20and%20variety%20of%20objects%2C%20can%20perform%20object%20segmentation%20without%0Afine-tuning%20on%20specific%20images%20containing%20everyday%20objects%2C%20but%20in%20highly%0Acluttered%20visual%20scenes.%20The%20%27%27in%20the%20wild%27%27%20context%20is%20driven%20by%20the%20target%0Aapplication%20of%20vision%20guided%20upper%20limb%20neuroprostheses.%20We%20propose%20a%20method%0Afor%20generating%20prompts%20based%20on%20gaze%20fixations%20to%20guide%20the%20Segment%20Anything%0AModel%20%28SAM%29%20in%20our%20segmentation%20scenario%2C%20and%20fine-tune%20it%20on%20egocentric%20visual%0Adata.%20Evaluation%20results%20of%20our%20approach%20show%20an%20improvement%20of%20the%20IoU%0Asegmentation%20quality%20metric%20by%20up%20to%200.51%20points%20on%20real-world%20challenging%20data%0Aof%20Grasping-in-the-Wild%20corpus%20which%20is%20made%20available%20on%20the%20RoboFlow%20Platform%0A%28https%3A//universe.roboflow.com/iwrist/grasping-in-the-wild%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18517v1&entry.124074799=Read"},
{"title": "GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy\n  Prediction Using 3D Gaussians", "author": "Tomislav Pavkovi\u0107 and Mohammad-Ali Nikouei Mahani and Johannes Niedermayer and Johannes Betz", "abstract": "  3D semantic occupancy prediction is one of the crucial tasks of autonomous\ndriving. It enables precise and safe interpretation and navigation in complex\nenvironments. Reliable predictions rely on effective sensor fusion, as\ndifferent modalities can contain complementary information. Unlike conventional\nmethods that depend on dense grid representations, our approach,\nGaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor\nfusion mechanism. Seamless integration of data from camera, LiDAR, and radar\nsensors enables more precise and scalable occupancy prediction, while 3D\nGaussian representation significantly improves memory efficiency and inference\nspeed. GaussianFusionOcc employs modality-agnostic deformable attention to\nextract essential features from each sensor type, which are then used to refine\nGaussian properties, resulting in a more accurate representation of the\nenvironment. Extensive testing with various sensor combinations demonstrates\nthe versatility of our approach. By leveraging the robustness of multi-modal\nfusion and the efficiency of Gaussian representation, GaussianFusionOcc\noutperforms current state-of-the-art models.\n", "link": "http://arxiv.org/abs/2507.18522v1", "date": "2025-07-24", "relevancy": 2.4514, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6184}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6152}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianFusionOcc%3A%20A%20Seamless%20Sensor%20Fusion%20Approach%20for%203D%20Occupancy%0A%20%20Prediction%20Using%203D%20Gaussians&body=Title%3A%20GaussianFusionOcc%3A%20A%20Seamless%20Sensor%20Fusion%20Approach%20for%203D%20Occupancy%0A%20%20Prediction%20Using%203D%20Gaussians%0AAuthor%3A%20Tomislav%20Pavkovi%C4%87%20and%20Mohammad-Ali%20Nikouei%20Mahani%20and%20Johannes%20Niedermayer%20and%20Johannes%20Betz%0AAbstract%3A%20%20%203D%20semantic%20occupancy%20prediction%20is%20one%20of%20the%20crucial%20tasks%20of%20autonomous%0Adriving.%20It%20enables%20precise%20and%20safe%20interpretation%20and%20navigation%20in%20complex%0Aenvironments.%20Reliable%20predictions%20rely%20on%20effective%20sensor%20fusion%2C%20as%0Adifferent%20modalities%20can%20contain%20complementary%20information.%20Unlike%20conventional%0Amethods%20that%20depend%20on%20dense%20grid%20representations%2C%20our%20approach%2C%0AGaussianFusionOcc%2C%20uses%20semantic%203D%20Gaussians%20alongside%20an%20innovative%20sensor%0Afusion%20mechanism.%20Seamless%20integration%20of%20data%20from%20camera%2C%20LiDAR%2C%20and%20radar%0Asensors%20enables%20more%20precise%20and%20scalable%20occupancy%20prediction%2C%20while%203D%0AGaussian%20representation%20significantly%20improves%20memory%20efficiency%20and%20inference%0Aspeed.%20GaussianFusionOcc%20employs%20modality-agnostic%20deformable%20attention%20to%0Aextract%20essential%20features%20from%20each%20sensor%20type%2C%20which%20are%20then%20used%20to%20refine%0AGaussian%20properties%2C%20resulting%20in%20a%20more%20accurate%20representation%20of%20the%0Aenvironment.%20Extensive%20testing%20with%20various%20sensor%20combinations%20demonstrates%0Athe%20versatility%20of%20our%20approach.%20By%20leveraging%20the%20robustness%20of%20multi-modal%0Afusion%20and%20the%20efficiency%20of%20Gaussian%20representation%2C%20GaussianFusionOcc%0Aoutperforms%20current%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianFusionOcc%253A%2520A%2520Seamless%2520Sensor%2520Fusion%2520Approach%2520for%25203D%2520Occupancy%250A%2520%2520Prediction%2520Using%25203D%2520Gaussians%26entry.906535625%3DTomislav%2520Pavkovi%25C4%2587%2520and%2520Mohammad-Ali%2520Nikouei%2520Mahani%2520and%2520Johannes%2520Niedermayer%2520and%2520Johannes%2520Betz%26entry.1292438233%3D%2520%25203D%2520semantic%2520occupancy%2520prediction%2520is%2520one%2520of%2520the%2520crucial%2520tasks%2520of%2520autonomous%250Adriving.%2520It%2520enables%2520precise%2520and%2520safe%2520interpretation%2520and%2520navigation%2520in%2520complex%250Aenvironments.%2520Reliable%2520predictions%2520rely%2520on%2520effective%2520sensor%2520fusion%252C%2520as%250Adifferent%2520modalities%2520can%2520contain%2520complementary%2520information.%2520Unlike%2520conventional%250Amethods%2520that%2520depend%2520on%2520dense%2520grid%2520representations%252C%2520our%2520approach%252C%250AGaussianFusionOcc%252C%2520uses%2520semantic%25203D%2520Gaussians%2520alongside%2520an%2520innovative%2520sensor%250Afusion%2520mechanism.%2520Seamless%2520integration%2520of%2520data%2520from%2520camera%252C%2520LiDAR%252C%2520and%2520radar%250Asensors%2520enables%2520more%2520precise%2520and%2520scalable%2520occupancy%2520prediction%252C%2520while%25203D%250AGaussian%2520representation%2520significantly%2520improves%2520memory%2520efficiency%2520and%2520inference%250Aspeed.%2520GaussianFusionOcc%2520employs%2520modality-agnostic%2520deformable%2520attention%2520to%250Aextract%2520essential%2520features%2520from%2520each%2520sensor%2520type%252C%2520which%2520are%2520then%2520used%2520to%2520refine%250AGaussian%2520properties%252C%2520resulting%2520in%2520a%2520more%2520accurate%2520representation%2520of%2520the%250Aenvironment.%2520Extensive%2520testing%2520with%2520various%2520sensor%2520combinations%2520demonstrates%250Athe%2520versatility%2520of%2520our%2520approach.%2520By%2520leveraging%2520the%2520robustness%2520of%2520multi-modal%250Afusion%2520and%2520the%2520efficiency%2520of%2520Gaussian%2520representation%252C%2520GaussianFusionOcc%250Aoutperforms%2520current%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianFusionOcc%3A%20A%20Seamless%20Sensor%20Fusion%20Approach%20for%203D%20Occupancy%0A%20%20Prediction%20Using%203D%20Gaussians&entry.906535625=Tomislav%20Pavkovi%C4%87%20and%20Mohammad-Ali%20Nikouei%20Mahani%20and%20Johannes%20Niedermayer%20and%20Johannes%20Betz&entry.1292438233=%20%203D%20semantic%20occupancy%20prediction%20is%20one%20of%20the%20crucial%20tasks%20of%20autonomous%0Adriving.%20It%20enables%20precise%20and%20safe%20interpretation%20and%20navigation%20in%20complex%0Aenvironments.%20Reliable%20predictions%20rely%20on%20effective%20sensor%20fusion%2C%20as%0Adifferent%20modalities%20can%20contain%20complementary%20information.%20Unlike%20conventional%0Amethods%20that%20depend%20on%20dense%20grid%20representations%2C%20our%20approach%2C%0AGaussianFusionOcc%2C%20uses%20semantic%203D%20Gaussians%20alongside%20an%20innovative%20sensor%0Afusion%20mechanism.%20Seamless%20integration%20of%20data%20from%20camera%2C%20LiDAR%2C%20and%20radar%0Asensors%20enables%20more%20precise%20and%20scalable%20occupancy%20prediction%2C%20while%203D%0AGaussian%20representation%20significantly%20improves%20memory%20efficiency%20and%20inference%0Aspeed.%20GaussianFusionOcc%20employs%20modality-agnostic%20deformable%20attention%20to%0Aextract%20essential%20features%20from%20each%20sensor%20type%2C%20which%20are%20then%20used%20to%20refine%0AGaussian%20properties%2C%20resulting%20in%20a%20more%20accurate%20representation%20of%20the%0Aenvironment.%20Extensive%20testing%20with%20various%20sensor%20combinations%20demonstrates%0Athe%20versatility%20of%20our%20approach.%20By%20leveraging%20the%20robustness%20of%20multi-modal%0Afusion%20and%20the%20efficiency%20of%20Gaussian%20representation%2C%20GaussianFusionOcc%0Aoutperforms%20current%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18522v1&entry.124074799=Read"},
{"title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character\n  Similarity Networks", "author": "Zhijie Wang and Zixin Xu and Zhiyuan Pan", "abstract": "  The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples.\n", "link": "http://arxiv.org/abs/2507.14679v2", "date": "2025-07-24", "relevancy": 2.4231, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4986}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4831}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GCC-Spam%3A%20Spam%20Detection%20via%20GAN%2C%20Contrastive%20Learning%2C%20and%20Character%0A%20%20Similarity%20Networks&body=Title%3A%20GCC-Spam%3A%20Spam%20Detection%20via%20GAN%2C%20Contrastive%20Learning%2C%20and%20Character%0A%20%20Similarity%20Networks%0AAuthor%3A%20Zhijie%20Wang%20and%20Zixin%20Xu%20and%20Zhiyuan%20Pan%0AAbstract%3A%20%20%20The%20exponential%20growth%20of%20spam%20text%20on%20the%20Internet%20necessitates%20robust%0Adetection%20mechanisms%20to%20mitigate%20risks%20such%20as%20information%20leakage%20and%20social%0Ainstability.%20This%20work%20addresses%20two%20principal%20challenges%3A%20adversarial%0Astrategies%20employed%20by%20spammers%20and%20the%20scarcity%20of%20labeled%20data.%20We%20propose%20a%0Anovel%20spam-text%20detection%20framework%20GCC-Spam%2C%20which%20integrates%20three%20core%0Ainnovations.%20First%2C%20a%20character%20similarity%20network%20captures%20orthographic%20and%0Aphonetic%20features%20to%20counter%20character-obfuscation%20attacks%20and%20furthermore%0Aproduces%20sentence%20embeddings%20for%20downstream%20classification.%20Second%2C%20contrastive%0Alearning%20enhances%20discriminability%20by%20optimizing%20the%20latent-space%20distance%0Abetween%20spam%20and%20normal%20texts.%20Third%2C%20a%20Generative%20Adversarial%20Network%20%28GAN%29%0Agenerates%20realistic%20pseudo-spam%20samples%20to%20alleviate%20data%20scarcity%20while%0Aimproving%20model%20robustness%20and%20classification%20accuracy.%20Extensive%20experiments%0Aon%20real-world%20datasets%20demonstrate%20that%20our%20model%20outperforms%20baseline%0Aapproaches%2C%20achieving%20higher%20detection%20rates%20with%20significantly%20fewer%20labeled%0Aexamples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGCC-Spam%253A%2520Spam%2520Detection%2520via%2520GAN%252C%2520Contrastive%2520Learning%252C%2520and%2520Character%250A%2520%2520Similarity%2520Networks%26entry.906535625%3DZhijie%2520Wang%2520and%2520Zixin%2520Xu%2520and%2520Zhiyuan%2520Pan%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520of%2520spam%2520text%2520on%2520the%2520Internet%2520necessitates%2520robust%250Adetection%2520mechanisms%2520to%2520mitigate%2520risks%2520such%2520as%2520information%2520leakage%2520and%2520social%250Ainstability.%2520This%2520work%2520addresses%2520two%2520principal%2520challenges%253A%2520adversarial%250Astrategies%2520employed%2520by%2520spammers%2520and%2520the%2520scarcity%2520of%2520labeled%2520data.%2520We%2520propose%2520a%250Anovel%2520spam-text%2520detection%2520framework%2520GCC-Spam%252C%2520which%2520integrates%2520three%2520core%250Ainnovations.%2520First%252C%2520a%2520character%2520similarity%2520network%2520captures%2520orthographic%2520and%250Aphonetic%2520features%2520to%2520counter%2520character-obfuscation%2520attacks%2520and%2520furthermore%250Aproduces%2520sentence%2520embeddings%2520for%2520downstream%2520classification.%2520Second%252C%2520contrastive%250Alearning%2520enhances%2520discriminability%2520by%2520optimizing%2520the%2520latent-space%2520distance%250Abetween%2520spam%2520and%2520normal%2520texts.%2520Third%252C%2520a%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%250Agenerates%2520realistic%2520pseudo-spam%2520samples%2520to%2520alleviate%2520data%2520scarcity%2520while%250Aimproving%2520model%2520robustness%2520and%2520classification%2520accuracy.%2520Extensive%2520experiments%250Aon%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520model%2520outperforms%2520baseline%250Aapproaches%252C%2520achieving%2520higher%2520detection%2520rates%2520with%2520significantly%2520fewer%2520labeled%250Aexamples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCC-Spam%3A%20Spam%20Detection%20via%20GAN%2C%20Contrastive%20Learning%2C%20and%20Character%0A%20%20Similarity%20Networks&entry.906535625=Zhijie%20Wang%20and%20Zixin%20Xu%20and%20Zhiyuan%20Pan&entry.1292438233=%20%20The%20exponential%20growth%20of%20spam%20text%20on%20the%20Internet%20necessitates%20robust%0Adetection%20mechanisms%20to%20mitigate%20risks%20such%20as%20information%20leakage%20and%20social%0Ainstability.%20This%20work%20addresses%20two%20principal%20challenges%3A%20adversarial%0Astrategies%20employed%20by%20spammers%20and%20the%20scarcity%20of%20labeled%20data.%20We%20propose%20a%0Anovel%20spam-text%20detection%20framework%20GCC-Spam%2C%20which%20integrates%20three%20core%0Ainnovations.%20First%2C%20a%20character%20similarity%20network%20captures%20orthographic%20and%0Aphonetic%20features%20to%20counter%20character-obfuscation%20attacks%20and%20furthermore%0Aproduces%20sentence%20embeddings%20for%20downstream%20classification.%20Second%2C%20contrastive%0Alearning%20enhances%20discriminability%20by%20optimizing%20the%20latent-space%20distance%0Abetween%20spam%20and%20normal%20texts.%20Third%2C%20a%20Generative%20Adversarial%20Network%20%28GAN%29%0Agenerates%20realistic%20pseudo-spam%20samples%20to%20alleviate%20data%20scarcity%20while%0Aimproving%20model%20robustness%20and%20classification%20accuracy.%20Extensive%20experiments%0Aon%20real-world%20datasets%20demonstrate%20that%20our%20model%20outperforms%20baseline%0Aapproaches%2C%20achieving%20higher%20detection%20rates%20with%20significantly%20fewer%20labeled%0Aexamples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14679v2&entry.124074799=Read"},
{"title": "Synthetic Data Augmentation for Enhanced Chicken Carcass Instance\n  Segmentation", "author": "Yihong Feng and Chaitanya Pallerla and Xiaomin Lin and Pouya Sohrabipour Sr and Philip Crandall and Wan Shou and Yu She and Dongyi Wang", "abstract": "  The poultry industry has been driven by broiler chicken production and has\ngrown into the world's largest animal protein sector. Automated detection of\nchicken carcasses on processing lines is vital for quality control, food\nsafety, and operational efficiency in slaughterhouses and poultry processing\nplants. However, developing robust deep learning models for tasks like instance\nsegmentation in these fast-paced industrial environments is often hampered by\nthe need for laborious acquisition and annotation of large-scale real-world\nimage datasets. We present the first pipeline generating photo-realistic,\nautomatically labeled synthetic images of chicken carcasses. We also introduce\na new benchmark dataset containing 300 annotated real-world images, curated\nspecifically for poultry segmentation research. Using these datasets, this\nstudy investigates the efficacy of synthetic data and automatic data annotation\nto enhance the instance segmentation of chicken carcasses, particularly when\nreal annotated data from the processing line is scarce. A small real dataset\nwith varying proportions of synthetic images was evaluated in prominent\ninstance segmentation models. Results show that synthetic data significantly\nboosts segmentation performance for chicken carcasses across all models. This\nresearch underscores the value of synthetic data augmentation as a viable and\neffective strategy to mitigate data scarcity, reduce manual annotation efforts,\nand advance the development of robust AI-driven automated detection systems for\nchicken carcasses in the poultry processing industry.\n", "link": "http://arxiv.org/abs/2507.18558v1", "date": "2025-07-24", "relevancy": 2.4156, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.496}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4767}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Data%20Augmentation%20for%20Enhanced%20Chicken%20Carcass%20Instance%0A%20%20Segmentation&body=Title%3A%20Synthetic%20Data%20Augmentation%20for%20Enhanced%20Chicken%20Carcass%20Instance%0A%20%20Segmentation%0AAuthor%3A%20Yihong%20Feng%20and%20Chaitanya%20Pallerla%20and%20Xiaomin%20Lin%20and%20Pouya%20Sohrabipour%20Sr%20and%20Philip%20Crandall%20and%20Wan%20Shou%20and%20Yu%20She%20and%20Dongyi%20Wang%0AAbstract%3A%20%20%20The%20poultry%20industry%20has%20been%20driven%20by%20broiler%20chicken%20production%20and%20has%0Agrown%20into%20the%20world%27s%20largest%20animal%20protein%20sector.%20Automated%20detection%20of%0Achicken%20carcasses%20on%20processing%20lines%20is%20vital%20for%20quality%20control%2C%20food%0Asafety%2C%20and%20operational%20efficiency%20in%20slaughterhouses%20and%20poultry%20processing%0Aplants.%20However%2C%20developing%20robust%20deep%20learning%20models%20for%20tasks%20like%20instance%0Asegmentation%20in%20these%20fast-paced%20industrial%20environments%20is%20often%20hampered%20by%0Athe%20need%20for%20laborious%20acquisition%20and%20annotation%20of%20large-scale%20real-world%0Aimage%20datasets.%20We%20present%20the%20first%20pipeline%20generating%20photo-realistic%2C%0Aautomatically%20labeled%20synthetic%20images%20of%20chicken%20carcasses.%20We%20also%20introduce%0Aa%20new%20benchmark%20dataset%20containing%20300%20annotated%20real-world%20images%2C%20curated%0Aspecifically%20for%20poultry%20segmentation%20research.%20Using%20these%20datasets%2C%20this%0Astudy%20investigates%20the%20efficacy%20of%20synthetic%20data%20and%20automatic%20data%20annotation%0Ato%20enhance%20the%20instance%20segmentation%20of%20chicken%20carcasses%2C%20particularly%20when%0Areal%20annotated%20data%20from%20the%20processing%20line%20is%20scarce.%20A%20small%20real%20dataset%0Awith%20varying%20proportions%20of%20synthetic%20images%20was%20evaluated%20in%20prominent%0Ainstance%20segmentation%20models.%20Results%20show%20that%20synthetic%20data%20significantly%0Aboosts%20segmentation%20performance%20for%20chicken%20carcasses%20across%20all%20models.%20This%0Aresearch%20underscores%20the%20value%20of%20synthetic%20data%20augmentation%20as%20a%20viable%20and%0Aeffective%20strategy%20to%20mitigate%20data%20scarcity%2C%20reduce%20manual%20annotation%20efforts%2C%0Aand%20advance%20the%20development%20of%20robust%20AI-driven%20automated%20detection%20systems%20for%0Achicken%20carcasses%20in%20the%20poultry%20processing%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Data%2520Augmentation%2520for%2520Enhanced%2520Chicken%2520Carcass%2520Instance%250A%2520%2520Segmentation%26entry.906535625%3DYihong%2520Feng%2520and%2520Chaitanya%2520Pallerla%2520and%2520Xiaomin%2520Lin%2520and%2520Pouya%2520Sohrabipour%2520Sr%2520and%2520Philip%2520Crandall%2520and%2520Wan%2520Shou%2520and%2520Yu%2520She%2520and%2520Dongyi%2520Wang%26entry.1292438233%3D%2520%2520The%2520poultry%2520industry%2520has%2520been%2520driven%2520by%2520broiler%2520chicken%2520production%2520and%2520has%250Agrown%2520into%2520the%2520world%2527s%2520largest%2520animal%2520protein%2520sector.%2520Automated%2520detection%2520of%250Achicken%2520carcasses%2520on%2520processing%2520lines%2520is%2520vital%2520for%2520quality%2520control%252C%2520food%250Asafety%252C%2520and%2520operational%2520efficiency%2520in%2520slaughterhouses%2520and%2520poultry%2520processing%250Aplants.%2520However%252C%2520developing%2520robust%2520deep%2520learning%2520models%2520for%2520tasks%2520like%2520instance%250Asegmentation%2520in%2520these%2520fast-paced%2520industrial%2520environments%2520is%2520often%2520hampered%2520by%250Athe%2520need%2520for%2520laborious%2520acquisition%2520and%2520annotation%2520of%2520large-scale%2520real-world%250Aimage%2520datasets.%2520We%2520present%2520the%2520first%2520pipeline%2520generating%2520photo-realistic%252C%250Aautomatically%2520labeled%2520synthetic%2520images%2520of%2520chicken%2520carcasses.%2520We%2520also%2520introduce%250Aa%2520new%2520benchmark%2520dataset%2520containing%2520300%2520annotated%2520real-world%2520images%252C%2520curated%250Aspecifically%2520for%2520poultry%2520segmentation%2520research.%2520Using%2520these%2520datasets%252C%2520this%250Astudy%2520investigates%2520the%2520efficacy%2520of%2520synthetic%2520data%2520and%2520automatic%2520data%2520annotation%250Ato%2520enhance%2520the%2520instance%2520segmentation%2520of%2520chicken%2520carcasses%252C%2520particularly%2520when%250Areal%2520annotated%2520data%2520from%2520the%2520processing%2520line%2520is%2520scarce.%2520A%2520small%2520real%2520dataset%250Awith%2520varying%2520proportions%2520of%2520synthetic%2520images%2520was%2520evaluated%2520in%2520prominent%250Ainstance%2520segmentation%2520models.%2520Results%2520show%2520that%2520synthetic%2520data%2520significantly%250Aboosts%2520segmentation%2520performance%2520for%2520chicken%2520carcasses%2520across%2520all%2520models.%2520This%250Aresearch%2520underscores%2520the%2520value%2520of%2520synthetic%2520data%2520augmentation%2520as%2520a%2520viable%2520and%250Aeffective%2520strategy%2520to%2520mitigate%2520data%2520scarcity%252C%2520reduce%2520manual%2520annotation%2520efforts%252C%250Aand%2520advance%2520the%2520development%2520of%2520robust%2520AI-driven%2520automated%2520detection%2520systems%2520for%250Achicken%2520carcasses%2520in%2520the%2520poultry%2520processing%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Data%20Augmentation%20for%20Enhanced%20Chicken%20Carcass%20Instance%0A%20%20Segmentation&entry.906535625=Yihong%20Feng%20and%20Chaitanya%20Pallerla%20and%20Xiaomin%20Lin%20and%20Pouya%20Sohrabipour%20Sr%20and%20Philip%20Crandall%20and%20Wan%20Shou%20and%20Yu%20She%20and%20Dongyi%20Wang&entry.1292438233=%20%20The%20poultry%20industry%20has%20been%20driven%20by%20broiler%20chicken%20production%20and%20has%0Agrown%20into%20the%20world%27s%20largest%20animal%20protein%20sector.%20Automated%20detection%20of%0Achicken%20carcasses%20on%20processing%20lines%20is%20vital%20for%20quality%20control%2C%20food%0Asafety%2C%20and%20operational%20efficiency%20in%20slaughterhouses%20and%20poultry%20processing%0Aplants.%20However%2C%20developing%20robust%20deep%20learning%20models%20for%20tasks%20like%20instance%0Asegmentation%20in%20these%20fast-paced%20industrial%20environments%20is%20often%20hampered%20by%0Athe%20need%20for%20laborious%20acquisition%20and%20annotation%20of%20large-scale%20real-world%0Aimage%20datasets.%20We%20present%20the%20first%20pipeline%20generating%20photo-realistic%2C%0Aautomatically%20labeled%20synthetic%20images%20of%20chicken%20carcasses.%20We%20also%20introduce%0Aa%20new%20benchmark%20dataset%20containing%20300%20annotated%20real-world%20images%2C%20curated%0Aspecifically%20for%20poultry%20segmentation%20research.%20Using%20these%20datasets%2C%20this%0Astudy%20investigates%20the%20efficacy%20of%20synthetic%20data%20and%20automatic%20data%20annotation%0Ato%20enhance%20the%20instance%20segmentation%20of%20chicken%20carcasses%2C%20particularly%20when%0Areal%20annotated%20data%20from%20the%20processing%20line%20is%20scarce.%20A%20small%20real%20dataset%0Awith%20varying%20proportions%20of%20synthetic%20images%20was%20evaluated%20in%20prominent%0Ainstance%20segmentation%20models.%20Results%20show%20that%20synthetic%20data%20significantly%0Aboosts%20segmentation%20performance%20for%20chicken%20carcasses%20across%20all%20models.%20This%0Aresearch%20underscores%20the%20value%20of%20synthetic%20data%20augmentation%20as%20a%20viable%20and%0Aeffective%20strategy%20to%20mitigate%20data%20scarcity%2C%20reduce%20manual%20annotation%20efforts%2C%0Aand%20advance%20the%20development%20of%20robust%20AI-driven%20automated%20detection%20systems%20for%0Achicken%20carcasses%20in%20the%20poultry%20processing%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18558v1&entry.124074799=Read"},
{"title": "EgoExoBench: A Benchmark for First- and Third-person View Video\n  Understanding in MLLMs", "author": "Yuping He and Yifei Huang and Guo Chen and Baoqi Pei and Jilan Xu and Tong Lu and Jiangmiao Pang", "abstract": "  Transferring and integrating knowledge across first-person (egocentric) and\nthird-person (exocentric) viewpoints is intrinsic to human intelligence,\nenabling humans to learn from others and convey insights from their own\nexperiences. Despite rapid progress in multimodal large language models\n(MLLMs), their ability to perform such cross-view reasoning remains unexplored.\nTo address this, we introduce EgoExoBench, the first benchmark for\negocentric-exocentric video understanding and reasoning. Built from publicly\navailable datasets, EgoExoBench comprises over 7,300 question-answer pairs\nspanning eleven sub-tasks organized into three core challenges: semantic\nalignment, viewpoint association, and temporal reasoning. We evaluate 13\nstate-of-the-art MLLMs and find that while these models excel on single-view\ntasks, they struggle to align semantics across perspectives, accurately\nassociate views, and infer temporal dynamics in the ego-exo context. We hope\nEgoExoBench can serve as a valuable resource for research on embodied agents\nand intelligent assistants seeking human-like cross-view intelligence.\n", "link": "http://arxiv.org/abs/2507.18342v1", "date": "2025-07-24", "relevancy": 2.3867, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6077}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6077}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoExoBench%3A%20A%20Benchmark%20for%20First-%20and%20Third-person%20View%20Video%0A%20%20Understanding%20in%20MLLMs&body=Title%3A%20EgoExoBench%3A%20A%20Benchmark%20for%20First-%20and%20Third-person%20View%20Video%0A%20%20Understanding%20in%20MLLMs%0AAuthor%3A%20Yuping%20He%20and%20Yifei%20Huang%20and%20Guo%20Chen%20and%20Baoqi%20Pei%20and%20Jilan%20Xu%20and%20Tong%20Lu%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Transferring%20and%20integrating%20knowledge%20across%20first-person%20%28egocentric%29%20and%0Athird-person%20%28exocentric%29%20viewpoints%20is%20intrinsic%20to%20human%20intelligence%2C%0Aenabling%20humans%20to%20learn%20from%20others%20and%20convey%20insights%20from%20their%20own%0Aexperiences.%20Despite%20rapid%20progress%20in%20multimodal%20large%20language%20models%0A%28MLLMs%29%2C%20their%20ability%20to%20perform%20such%20cross-view%20reasoning%20remains%20unexplored.%0ATo%20address%20this%2C%20we%20introduce%20EgoExoBench%2C%20the%20first%20benchmark%20for%0Aegocentric-exocentric%20video%20understanding%20and%20reasoning.%20Built%20from%20publicly%0Aavailable%20datasets%2C%20EgoExoBench%20comprises%20over%207%2C300%20question-answer%20pairs%0Aspanning%20eleven%20sub-tasks%20organized%20into%20three%20core%20challenges%3A%20semantic%0Aalignment%2C%20viewpoint%20association%2C%20and%20temporal%20reasoning.%20We%20evaluate%2013%0Astate-of-the-art%20MLLMs%20and%20find%20that%20while%20these%20models%20excel%20on%20single-view%0Atasks%2C%20they%20struggle%20to%20align%20semantics%20across%20perspectives%2C%20accurately%0Aassociate%20views%2C%20and%20infer%20temporal%20dynamics%20in%20the%20ego-exo%20context.%20We%20hope%0AEgoExoBench%20can%20serve%20as%20a%20valuable%20resource%20for%20research%20on%20embodied%20agents%0Aand%20intelligent%20assistants%20seeking%20human-like%20cross-view%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoExoBench%253A%2520A%2520Benchmark%2520for%2520First-%2520and%2520Third-person%2520View%2520Video%250A%2520%2520Understanding%2520in%2520MLLMs%26entry.906535625%3DYuping%2520He%2520and%2520Yifei%2520Huang%2520and%2520Guo%2520Chen%2520and%2520Baoqi%2520Pei%2520and%2520Jilan%2520Xu%2520and%2520Tong%2520Lu%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Transferring%2520and%2520integrating%2520knowledge%2520across%2520first-person%2520%2528egocentric%2529%2520and%250Athird-person%2520%2528exocentric%2529%2520viewpoints%2520is%2520intrinsic%2520to%2520human%2520intelligence%252C%250Aenabling%2520humans%2520to%2520learn%2520from%2520others%2520and%2520convey%2520insights%2520from%2520their%2520own%250Aexperiences.%2520Despite%2520rapid%2520progress%2520in%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%252C%2520their%2520ability%2520to%2520perform%2520such%2520cross-view%2520reasoning%2520remains%2520unexplored.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520EgoExoBench%252C%2520the%2520first%2520benchmark%2520for%250Aegocentric-exocentric%2520video%2520understanding%2520and%2520reasoning.%2520Built%2520from%2520publicly%250Aavailable%2520datasets%252C%2520EgoExoBench%2520comprises%2520over%25207%252C300%2520question-answer%2520pairs%250Aspanning%2520eleven%2520sub-tasks%2520organized%2520into%2520three%2520core%2520challenges%253A%2520semantic%250Aalignment%252C%2520viewpoint%2520association%252C%2520and%2520temporal%2520reasoning.%2520We%2520evaluate%252013%250Astate-of-the-art%2520MLLMs%2520and%2520find%2520that%2520while%2520these%2520models%2520excel%2520on%2520single-view%250Atasks%252C%2520they%2520struggle%2520to%2520align%2520semantics%2520across%2520perspectives%252C%2520accurately%250Aassociate%2520views%252C%2520and%2520infer%2520temporal%2520dynamics%2520in%2520the%2520ego-exo%2520context.%2520We%2520hope%250AEgoExoBench%2520can%2520serve%2520as%2520a%2520valuable%2520resource%2520for%2520research%2520on%2520embodied%2520agents%250Aand%2520intelligent%2520assistants%2520seeking%2520human-like%2520cross-view%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoExoBench%3A%20A%20Benchmark%20for%20First-%20and%20Third-person%20View%20Video%0A%20%20Understanding%20in%20MLLMs&entry.906535625=Yuping%20He%20and%20Yifei%20Huang%20and%20Guo%20Chen%20and%20Baoqi%20Pei%20and%20Jilan%20Xu%20and%20Tong%20Lu%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Transferring%20and%20integrating%20knowledge%20across%20first-person%20%28egocentric%29%20and%0Athird-person%20%28exocentric%29%20viewpoints%20is%20intrinsic%20to%20human%20intelligence%2C%0Aenabling%20humans%20to%20learn%20from%20others%20and%20convey%20insights%20from%20their%20own%0Aexperiences.%20Despite%20rapid%20progress%20in%20multimodal%20large%20language%20models%0A%28MLLMs%29%2C%20their%20ability%20to%20perform%20such%20cross-view%20reasoning%20remains%20unexplored.%0ATo%20address%20this%2C%20we%20introduce%20EgoExoBench%2C%20the%20first%20benchmark%20for%0Aegocentric-exocentric%20video%20understanding%20and%20reasoning.%20Built%20from%20publicly%0Aavailable%20datasets%2C%20EgoExoBench%20comprises%20over%207%2C300%20question-answer%20pairs%0Aspanning%20eleven%20sub-tasks%20organized%20into%20three%20core%20challenges%3A%20semantic%0Aalignment%2C%20viewpoint%20association%2C%20and%20temporal%20reasoning.%20We%20evaluate%2013%0Astate-of-the-art%20MLLMs%20and%20find%20that%20while%20these%20models%20excel%20on%20single-view%0Atasks%2C%20they%20struggle%20to%20align%20semantics%20across%20perspectives%2C%20accurately%0Aassociate%20views%2C%20and%20infer%20temporal%20dynamics%20in%20the%20ego-exo%20context.%20We%20hope%0AEgoExoBench%20can%20serve%20as%20a%20valuable%20resource%20for%20research%20on%20embodied%20agents%0Aand%20intelligent%20assistants%20seeking%20human-like%20cross-view%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18342v1&entry.124074799=Read"},
{"title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion", "author": "Shengyuan Zhang and An Zhao and Ling Yang and Zejian Li and Chenye Meng and Haoran Xu and Tianrun Chen and AnYang Wei and Perry Pengyun GU and Lingyun Sun", "abstract": "  Diffusion models have been applied to 3D LiDAR scene completion due to their\nstrong training stability and high completion quality. However, the slow\nsampling speed limits the practical application of diffusion-based scene\ncompletion models since autonomous vehicles require an efficient perception of\nsurrounding environments. This paper proposes a novel distillation method\ntailored for 3D Li- DAR scene completion models, dubbed ScoreLiDAR, which\nachieves efficient yet high-quality scene completion. Score- LiDAR enables the\ndistilled model to sample in significantly fewer steps after distillation. To\nimprove completion quality, we also introduce a novel Structural Loss, which\nencourages the distilled model to capture the geometric structure of the 3D\nLiDAR scene. The loss contains a scene-wise term constraining the holistic\nstructure and a point-wise term constraining the key landmark points and their\nrelative configuration. Extensive experiments demonstrate that ScoreLiDAR\nsignificantly accelerates the completion time from 30.55 to 5.37 seconds per\nframe (>5x) on SemanticKITTI and achieves superior performance compared to\nstate-of-the-art 3D LiDAR scene completion models. Our model and code are\npublicly available on https: //github.com/happyw1nd/ScoreLiDAR.\n", "link": "http://arxiv.org/abs/2412.03515v2", "date": "2025-07-24", "relevancy": 2.3794, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5975}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5975}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Diffusion%20Models%20to%20Efficient%203D%20LiDAR%20Scene%20Completion&body=Title%3A%20Distilling%20Diffusion%20Models%20to%20Efficient%203D%20LiDAR%20Scene%20Completion%0AAuthor%3A%20Shengyuan%20Zhang%20and%20An%20Zhao%20and%20Ling%20Yang%20and%20Zejian%20Li%20and%20Chenye%20Meng%20and%20Haoran%20Xu%20and%20Tianrun%20Chen%20and%20AnYang%20Wei%20and%20Perry%20Pengyun%20GU%20and%20Lingyun%20Sun%0AAbstract%3A%20%20%20Diffusion%20models%20have%20been%20applied%20to%203D%20LiDAR%20scene%20completion%20due%20to%20their%0Astrong%20training%20stability%20and%20high%20completion%20quality.%20However%2C%20the%20slow%0Asampling%20speed%20limits%20the%20practical%20application%20of%20diffusion-based%20scene%0Acompletion%20models%20since%20autonomous%20vehicles%20require%20an%20efficient%20perception%20of%0Asurrounding%20environments.%20This%20paper%20proposes%20a%20novel%20distillation%20method%0Atailored%20for%203D%20Li-%20DAR%20scene%20completion%20models%2C%20dubbed%20ScoreLiDAR%2C%20which%0Aachieves%20efficient%20yet%20high-quality%20scene%20completion.%20Score-%20LiDAR%20enables%20the%0Adistilled%20model%20to%20sample%20in%20significantly%20fewer%20steps%20after%20distillation.%20To%0Aimprove%20completion%20quality%2C%20we%20also%20introduce%20a%20novel%20Structural%20Loss%2C%20which%0Aencourages%20the%20distilled%20model%20to%20capture%20the%20geometric%20structure%20of%20the%203D%0ALiDAR%20scene.%20The%20loss%20contains%20a%20scene-wise%20term%20constraining%20the%20holistic%0Astructure%20and%20a%20point-wise%20term%20constraining%20the%20key%20landmark%20points%20and%20their%0Arelative%20configuration.%20Extensive%20experiments%20demonstrate%20that%20ScoreLiDAR%0Asignificantly%20accelerates%20the%20completion%20time%20from%2030.55%20to%205.37%20seconds%20per%0Aframe%20%28%3E5x%29%20on%20SemanticKITTI%20and%20achieves%20superior%20performance%20compared%20to%0Astate-of-the-art%203D%20LiDAR%20scene%20completion%20models.%20Our%20model%20and%20code%20are%0Apublicly%20available%20on%20https%3A%20//github.com/happyw1nd/ScoreLiDAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Diffusion%2520Models%2520to%2520Efficient%25203D%2520LiDAR%2520Scene%2520Completion%26entry.906535625%3DShengyuan%2520Zhang%2520and%2520An%2520Zhao%2520and%2520Ling%2520Yang%2520and%2520Zejian%2520Li%2520and%2520Chenye%2520Meng%2520and%2520Haoran%2520Xu%2520and%2520Tianrun%2520Chen%2520and%2520AnYang%2520Wei%2520and%2520Perry%2520Pengyun%2520GU%2520and%2520Lingyun%2520Sun%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520been%2520applied%2520to%25203D%2520LiDAR%2520scene%2520completion%2520due%2520to%2520their%250Astrong%2520training%2520stability%2520and%2520high%2520completion%2520quality.%2520However%252C%2520the%2520slow%250Asampling%2520speed%2520limits%2520the%2520practical%2520application%2520of%2520diffusion-based%2520scene%250Acompletion%2520models%2520since%2520autonomous%2520vehicles%2520require%2520an%2520efficient%2520perception%2520of%250Asurrounding%2520environments.%2520This%2520paper%2520proposes%2520a%2520novel%2520distillation%2520method%250Atailored%2520for%25203D%2520Li-%2520DAR%2520scene%2520completion%2520models%252C%2520dubbed%2520ScoreLiDAR%252C%2520which%250Aachieves%2520efficient%2520yet%2520high-quality%2520scene%2520completion.%2520Score-%2520LiDAR%2520enables%2520the%250Adistilled%2520model%2520to%2520sample%2520in%2520significantly%2520fewer%2520steps%2520after%2520distillation.%2520To%250Aimprove%2520completion%2520quality%252C%2520we%2520also%2520introduce%2520a%2520novel%2520Structural%2520Loss%252C%2520which%250Aencourages%2520the%2520distilled%2520model%2520to%2520capture%2520the%2520geometric%2520structure%2520of%2520the%25203D%250ALiDAR%2520scene.%2520The%2520loss%2520contains%2520a%2520scene-wise%2520term%2520constraining%2520the%2520holistic%250Astructure%2520and%2520a%2520point-wise%2520term%2520constraining%2520the%2520key%2520landmark%2520points%2520and%2520their%250Arelative%2520configuration.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ScoreLiDAR%250Asignificantly%2520accelerates%2520the%2520completion%2520time%2520from%252030.55%2520to%25205.37%2520seconds%2520per%250Aframe%2520%2528%253E5x%2529%2520on%2520SemanticKITTI%2520and%2520achieves%2520superior%2520performance%2520compared%2520to%250Astate-of-the-art%25203D%2520LiDAR%2520scene%2520completion%2520models.%2520Our%2520model%2520and%2520code%2520are%250Apublicly%2520available%2520on%2520https%253A%2520//github.com/happyw1nd/ScoreLiDAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Diffusion%20Models%20to%20Efficient%203D%20LiDAR%20Scene%20Completion&entry.906535625=Shengyuan%20Zhang%20and%20An%20Zhao%20and%20Ling%20Yang%20and%20Zejian%20Li%20and%20Chenye%20Meng%20and%20Haoran%20Xu%20and%20Tianrun%20Chen%20and%20AnYang%20Wei%20and%20Perry%20Pengyun%20GU%20and%20Lingyun%20Sun&entry.1292438233=%20%20Diffusion%20models%20have%20been%20applied%20to%203D%20LiDAR%20scene%20completion%20due%20to%20their%0Astrong%20training%20stability%20and%20high%20completion%20quality.%20However%2C%20the%20slow%0Asampling%20speed%20limits%20the%20practical%20application%20of%20diffusion-based%20scene%0Acompletion%20models%20since%20autonomous%20vehicles%20require%20an%20efficient%20perception%20of%0Asurrounding%20environments.%20This%20paper%20proposes%20a%20novel%20distillation%20method%0Atailored%20for%203D%20Li-%20DAR%20scene%20completion%20models%2C%20dubbed%20ScoreLiDAR%2C%20which%0Aachieves%20efficient%20yet%20high-quality%20scene%20completion.%20Score-%20LiDAR%20enables%20the%0Adistilled%20model%20to%20sample%20in%20significantly%20fewer%20steps%20after%20distillation.%20To%0Aimprove%20completion%20quality%2C%20we%20also%20introduce%20a%20novel%20Structural%20Loss%2C%20which%0Aencourages%20the%20distilled%20model%20to%20capture%20the%20geometric%20structure%20of%20the%203D%0ALiDAR%20scene.%20The%20loss%20contains%20a%20scene-wise%20term%20constraining%20the%20holistic%0Astructure%20and%20a%20point-wise%20term%20constraining%20the%20key%20landmark%20points%20and%20their%0Arelative%20configuration.%20Extensive%20experiments%20demonstrate%20that%20ScoreLiDAR%0Asignificantly%20accelerates%20the%20completion%20time%20from%2030.55%20to%205.37%20seconds%20per%0Aframe%20%28%3E5x%29%20on%20SemanticKITTI%20and%20achieves%20superior%20performance%20compared%20to%0Astate-of-the-art%203D%20LiDAR%20scene%20completion%20models.%20Our%20model%20and%20code%20are%0Apublicly%20available%20on%20https%3A%20//github.com/happyw1nd/ScoreLiDAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03515v2&entry.124074799=Read"},
{"title": "Zeroth-Order Fine-Tuning of LLMs in Random Subspaces", "author": "Ziming Yu and Pan Zhou and Sike Wang and Jia Li and Mi Tian and Hua Huang", "abstract": "  Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks. Code is available at https://github.com/zimingyy/SubZero.\n", "link": "http://arxiv.org/abs/2410.08989v3", "date": "2025-07-24", "relevancy": 2.3719, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4745}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zeroth-Order%20Fine-Tuning%20of%20LLMs%20in%20Random%20Subspaces&body=Title%3A%20Zeroth-Order%20Fine-Tuning%20of%20LLMs%20in%20Random%20Subspaces%0AAuthor%3A%20Ziming%20Yu%20and%20Pan%20Zhou%20and%20Sike%20Wang%20and%20Jia%20Li%20and%20Mi%20Tian%20and%20Hua%20Huang%0AAbstract%3A%20%20%20Fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20has%20proven%20effective%20for%20a%20variety%0Aof%20downstream%20tasks.%20However%2C%20as%20LLMs%20grow%20in%20size%2C%20the%20memory%20demands%20for%0Abackpropagation%20become%20increasingly%20prohibitive.%20Zeroth-order%20%28ZO%29%20optimization%0Amethods%20offer%20a%20memory-efficient%20alternative%20by%20using%20forward%20passes%20to%0Aestimate%20gradients%2C%20but%20the%20variance%20of%20gradient%20estimates%20typically%20scales%0Alinearly%20with%20the%20model%27s%20parameter%20dimension%24%5Cunicode%7Bx2013%7D%24a%20significant%0Aissue%20for%20LLMs.%20In%20this%20paper%2C%20we%20propose%20the%20random%20Subspace%20Zeroth-order%0A%28SubZero%29%20optimization%20to%20address%20the%20challenges%20posed%20by%20LLMs%27%20high%0Adimensionality.%20We%20introduce%20a%20low-rank%20perturbation%20tailored%20for%20LLMs%20that%0Asignificantly%20reduces%20memory%20consumption%20while%20improving%20training%20performance.%0AAdditionally%2C%20we%20prove%20that%20our%20gradient%20estimation%20closely%20approximates%20the%0Abackpropagation%20gradient%2C%20exhibits%20lower%20variance%20than%20traditional%20ZO%20methods%2C%0Aand%20ensures%20convergence%20when%20combined%20with%20SGD.%20Experimental%20results%20show%20that%0ASubZero%20enhances%20fine-tuning%20performance%20and%20achieves%20faster%20convergence%0Acompared%20to%20standard%20ZO%20approaches%20like%20MeZO%20across%20various%20language%20modeling%0Atasks.%20Code%20is%20available%20at%20https%3A//github.com/zimingyy/SubZero.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08989v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroth-Order%2520Fine-Tuning%2520of%2520LLMs%2520in%2520Random%2520Subspaces%26entry.906535625%3DZiming%2520Yu%2520and%2520Pan%2520Zhou%2520and%2520Sike%2520Wang%2520and%2520Jia%2520Li%2520and%2520Mi%2520Tian%2520and%2520Hua%2520Huang%26entry.1292438233%3D%2520%2520Fine-tuning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520proven%2520effective%2520for%2520a%2520variety%250Aof%2520downstream%2520tasks.%2520However%252C%2520as%2520LLMs%2520grow%2520in%2520size%252C%2520the%2520memory%2520demands%2520for%250Abackpropagation%2520become%2520increasingly%2520prohibitive.%2520Zeroth-order%2520%2528ZO%2529%2520optimization%250Amethods%2520offer%2520a%2520memory-efficient%2520alternative%2520by%2520using%2520forward%2520passes%2520to%250Aestimate%2520gradients%252C%2520but%2520the%2520variance%2520of%2520gradient%2520estimates%2520typically%2520scales%250Alinearly%2520with%2520the%2520model%2527s%2520parameter%2520dimension%2524%255Cunicode%257Bx2013%257D%2524a%2520significant%250Aissue%2520for%2520LLMs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520random%2520Subspace%2520Zeroth-order%250A%2528SubZero%2529%2520optimization%2520to%2520address%2520the%2520challenges%2520posed%2520by%2520LLMs%2527%2520high%250Adimensionality.%2520We%2520introduce%2520a%2520low-rank%2520perturbation%2520tailored%2520for%2520LLMs%2520that%250Asignificantly%2520reduces%2520memory%2520consumption%2520while%2520improving%2520training%2520performance.%250AAdditionally%252C%2520we%2520prove%2520that%2520our%2520gradient%2520estimation%2520closely%2520approximates%2520the%250Abackpropagation%2520gradient%252C%2520exhibits%2520lower%2520variance%2520than%2520traditional%2520ZO%2520methods%252C%250Aand%2520ensures%2520convergence%2520when%2520combined%2520with%2520SGD.%2520Experimental%2520results%2520show%2520that%250ASubZero%2520enhances%2520fine-tuning%2520performance%2520and%2520achieves%2520faster%2520convergence%250Acompared%2520to%2520standard%2520ZO%2520approaches%2520like%2520MeZO%2520across%2520various%2520language%2520modeling%250Atasks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/zimingyy/SubZero.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08989v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zeroth-Order%20Fine-Tuning%20of%20LLMs%20in%20Random%20Subspaces&entry.906535625=Ziming%20Yu%20and%20Pan%20Zhou%20and%20Sike%20Wang%20and%20Jia%20Li%20and%20Mi%20Tian%20and%20Hua%20Huang&entry.1292438233=%20%20Fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20has%20proven%20effective%20for%20a%20variety%0Aof%20downstream%20tasks.%20However%2C%20as%20LLMs%20grow%20in%20size%2C%20the%20memory%20demands%20for%0Abackpropagation%20become%20increasingly%20prohibitive.%20Zeroth-order%20%28ZO%29%20optimization%0Amethods%20offer%20a%20memory-efficient%20alternative%20by%20using%20forward%20passes%20to%0Aestimate%20gradients%2C%20but%20the%20variance%20of%20gradient%20estimates%20typically%20scales%0Alinearly%20with%20the%20model%27s%20parameter%20dimension%24%5Cunicode%7Bx2013%7D%24a%20significant%0Aissue%20for%20LLMs.%20In%20this%20paper%2C%20we%20propose%20the%20random%20Subspace%20Zeroth-order%0A%28SubZero%29%20optimization%20to%20address%20the%20challenges%20posed%20by%20LLMs%27%20high%0Adimensionality.%20We%20introduce%20a%20low-rank%20perturbation%20tailored%20for%20LLMs%20that%0Asignificantly%20reduces%20memory%20consumption%20while%20improving%20training%20performance.%0AAdditionally%2C%20we%20prove%20that%20our%20gradient%20estimation%20closely%20approximates%20the%0Abackpropagation%20gradient%2C%20exhibits%20lower%20variance%20than%20traditional%20ZO%20methods%2C%0Aand%20ensures%20convergence%20when%20combined%20with%20SGD.%20Experimental%20results%20show%20that%0ASubZero%20enhances%20fine-tuning%20performance%20and%20achieves%20faster%20convergence%0Acompared%20to%20standard%20ZO%20approaches%20like%20MeZO%20across%20various%20language%20modeling%0Atasks.%20Code%20is%20available%20at%20https%3A//github.com/zimingyy/SubZero.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08989v3&entry.124074799=Read"},
{"title": "Differentiable Motion Manifold Primitives for Reactive Motion Generation\n  under Kinodynamic Constraints", "author": "Yonghyeon Lee", "abstract": "  Real-time motion generation -- which is essential for achieving reactive and\nadaptive behavior -- under kinodynamic constraints for high-dimensional systems\nis a crucial yet challenging problem. We address this with a two-step approach:\noffline learning of a lower-dimensional trajectory manifold of task-relevant,\nconstraint-satisfying trajectories, followed by rapid online search within this\nmanifold. Extending the discrete-time Motion Manifold Primitives (MMP)\nframework, we propose Differentiable Motion Manifold Primitives (DMMP), a novel\nneural network architecture that encodes and generates continuous-time,\ndifferentiable trajectories, trained using data collected offline through\ntrajectory optimizations, with a strategy that ensures constraint satisfaction\n-- absent in existing methods. Experiments on dynamic throwing with a 7-DoF\nrobot arm demonstrate that DMMP outperforms prior methods in planning speed,\ntask success, and constraint satisfaction.\n", "link": "http://arxiv.org/abs/2410.12193v2", "date": "2025-07-24", "relevancy": 2.3675, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6294}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5746}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Motion%20Manifold%20Primitives%20for%20Reactive%20Motion%20Generation%0A%20%20under%20Kinodynamic%20Constraints&body=Title%3A%20Differentiable%20Motion%20Manifold%20Primitives%20for%20Reactive%20Motion%20Generation%0A%20%20under%20Kinodynamic%20Constraints%0AAuthor%3A%20Yonghyeon%20Lee%0AAbstract%3A%20%20%20Real-time%20motion%20generation%20--%20which%20is%20essential%20for%20achieving%20reactive%20and%0Aadaptive%20behavior%20--%20under%20kinodynamic%20constraints%20for%20high-dimensional%20systems%0Ais%20a%20crucial%20yet%20challenging%20problem.%20We%20address%20this%20with%20a%20two-step%20approach%3A%0Aoffline%20learning%20of%20a%20lower-dimensional%20trajectory%20manifold%20of%20task-relevant%2C%0Aconstraint-satisfying%20trajectories%2C%20followed%20by%20rapid%20online%20search%20within%20this%0Amanifold.%20Extending%20the%20discrete-time%20Motion%20Manifold%20Primitives%20%28MMP%29%0Aframework%2C%20we%20propose%20Differentiable%20Motion%20Manifold%20Primitives%20%28DMMP%29%2C%20a%20novel%0Aneural%20network%20architecture%20that%20encodes%20and%20generates%20continuous-time%2C%0Adifferentiable%20trajectories%2C%20trained%20using%20data%20collected%20offline%20through%0Atrajectory%20optimizations%2C%20with%20a%20strategy%20that%20ensures%20constraint%20satisfaction%0A--%20absent%20in%20existing%20methods.%20Experiments%20on%20dynamic%20throwing%20with%20a%207-DoF%0Arobot%20arm%20demonstrate%20that%20DMMP%20outperforms%20prior%20methods%20in%20planning%20speed%2C%0Atask%20success%2C%20and%20constraint%20satisfaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Motion%2520Manifold%2520Primitives%2520for%2520Reactive%2520Motion%2520Generation%250A%2520%2520under%2520Kinodynamic%2520Constraints%26entry.906535625%3DYonghyeon%2520Lee%26entry.1292438233%3D%2520%2520Real-time%2520motion%2520generation%2520--%2520which%2520is%2520essential%2520for%2520achieving%2520reactive%2520and%250Aadaptive%2520behavior%2520--%2520under%2520kinodynamic%2520constraints%2520for%2520high-dimensional%2520systems%250Ais%2520a%2520crucial%2520yet%2520challenging%2520problem.%2520We%2520address%2520this%2520with%2520a%2520two-step%2520approach%253A%250Aoffline%2520learning%2520of%2520a%2520lower-dimensional%2520trajectory%2520manifold%2520of%2520task-relevant%252C%250Aconstraint-satisfying%2520trajectories%252C%2520followed%2520by%2520rapid%2520online%2520search%2520within%2520this%250Amanifold.%2520Extending%2520the%2520discrete-time%2520Motion%2520Manifold%2520Primitives%2520%2528MMP%2529%250Aframework%252C%2520we%2520propose%2520Differentiable%2520Motion%2520Manifold%2520Primitives%2520%2528DMMP%2529%252C%2520a%2520novel%250Aneural%2520network%2520architecture%2520that%2520encodes%2520and%2520generates%2520continuous-time%252C%250Adifferentiable%2520trajectories%252C%2520trained%2520using%2520data%2520collected%2520offline%2520through%250Atrajectory%2520optimizations%252C%2520with%2520a%2520strategy%2520that%2520ensures%2520constraint%2520satisfaction%250A--%2520absent%2520in%2520existing%2520methods.%2520Experiments%2520on%2520dynamic%2520throwing%2520with%2520a%25207-DoF%250Arobot%2520arm%2520demonstrate%2520that%2520DMMP%2520outperforms%2520prior%2520methods%2520in%2520planning%2520speed%252C%250Atask%2520success%252C%2520and%2520constraint%2520satisfaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Motion%20Manifold%20Primitives%20for%20Reactive%20Motion%20Generation%0A%20%20under%20Kinodynamic%20Constraints&entry.906535625=Yonghyeon%20Lee&entry.1292438233=%20%20Real-time%20motion%20generation%20--%20which%20is%20essential%20for%20achieving%20reactive%20and%0Aadaptive%20behavior%20--%20under%20kinodynamic%20constraints%20for%20high-dimensional%20systems%0Ais%20a%20crucial%20yet%20challenging%20problem.%20We%20address%20this%20with%20a%20two-step%20approach%3A%0Aoffline%20learning%20of%20a%20lower-dimensional%20trajectory%20manifold%20of%20task-relevant%2C%0Aconstraint-satisfying%20trajectories%2C%20followed%20by%20rapid%20online%20search%20within%20this%0Amanifold.%20Extending%20the%20discrete-time%20Motion%20Manifold%20Primitives%20%28MMP%29%0Aframework%2C%20we%20propose%20Differentiable%20Motion%20Manifold%20Primitives%20%28DMMP%29%2C%20a%20novel%0Aneural%20network%20architecture%20that%20encodes%20and%20generates%20continuous-time%2C%0Adifferentiable%20trajectories%2C%20trained%20using%20data%20collected%20offline%20through%0Atrajectory%20optimizations%2C%20with%20a%20strategy%20that%20ensures%20constraint%20satisfaction%0A--%20absent%20in%20existing%20methods.%20Experiments%20on%20dynamic%20throwing%20with%20a%207-DoF%0Arobot%20arm%20demonstrate%20that%20DMMP%20outperforms%20prior%20methods%20in%20planning%20speed%2C%0Atask%20success%2C%20and%20constraint%20satisfaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12193v2&entry.124074799=Read"},
{"title": "Frequency-Dynamic Attention Modulation for Dense Prediction", "author": "Linwei Chen and Lin Gu and Ying Fu", "abstract": "  Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\nhttps://github.com/Linwei-Chen/FDAM.\n", "link": "http://arxiv.org/abs/2507.12006v3", "date": "2025-07-24", "relevancy": 2.3599, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6133}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5931}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Dynamic%20Attention%20Modulation%20for%20Dense%20Prediction&body=Title%3A%20Frequency-Dynamic%20Attention%20Modulation%20for%20Dense%20Prediction%0AAuthor%3A%20Linwei%20Chen%20and%20Lin%20Gu%20and%20Ying%20Fu%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20significantly%20advanced%20computer%20vision%2C%0Ademonstrating%20strong%20performance%20across%20various%20tasks.%20However%2C%20the%20attention%0Amechanism%20in%20ViTs%20makes%20each%20layer%20function%20as%20a%20low-pass%20filter%2C%20and%20the%0Astacked-layer%20architecture%20in%20existing%20transformers%20suffers%20from%20frequency%0Avanishing.%20This%20leads%20to%20the%20loss%20of%20critical%20details%20and%20textures.%20We%20propose%0Aa%20novel%2C%20circuit-theory-inspired%20strategy%20called%20Frequency-Dynamic%20Attention%0AModulation%20%28FDAM%29%2C%20which%20can%20be%20easily%20plugged%20into%20ViTs.%20FDAM%20directly%0Amodulates%20the%20overall%20frequency%20response%20of%20ViTs%20and%20consists%20of%20two%0Atechniques%3A%20Attention%20Inversion%20%28AttInv%29%20and%20Frequency%20Dynamic%20Scaling%0A%28FreqScale%29.%20Since%20circuit%20theory%20uses%20low-pass%20filters%20as%20fundamental%0Aelements%2C%20we%20introduce%20AttInv%2C%20a%20method%20that%20generates%20complementary%20high-pass%0Afiltering%20by%20inverting%20the%20low-pass%20filter%20in%20the%20attention%20matrix%2C%20and%0Adynamically%20combining%20the%20two.%20We%20further%20design%20FreqScale%20to%20weight%20different%0Afrequency%20components%20for%20fine-grained%20adjustments%20to%20the%20target%20response%0Afunction.%20Through%20feature%20similarity%20analysis%20and%20effective%20rank%20evaluation%2C%20we%0Ademonstrate%20that%20our%20approach%20avoids%20representation%20collapse%2C%20leading%20to%0Aconsistent%20performance%20improvements%20across%20various%20models%2C%20including%20SegFormer%2C%0ADeiT%2C%20and%20MaskDINO.%20These%20improvements%20are%20evident%20in%20tasks%20such%20as%20semantic%0Asegmentation%2C%20object%20detection%2C%20and%20instance%20segmentation.%20Additionally%2C%20we%0Aapply%20our%20method%20to%20remote%20sensing%20detection%2C%20achieving%20state-of-the-art%0Aresults%20in%20single-scale%20settings.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Linwei-Chen/FDAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12006v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Dynamic%2520Attention%2520Modulation%2520for%2520Dense%2520Prediction%26entry.906535625%3DLinwei%2520Chen%2520and%2520Lin%2520Gu%2520and%2520Ying%2520Fu%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520significantly%2520advanced%2520computer%2520vision%252C%250Ademonstrating%2520strong%2520performance%2520across%2520various%2520tasks.%2520However%252C%2520the%2520attention%250Amechanism%2520in%2520ViTs%2520makes%2520each%2520layer%2520function%2520as%2520a%2520low-pass%2520filter%252C%2520and%2520the%250Astacked-layer%2520architecture%2520in%2520existing%2520transformers%2520suffers%2520from%2520frequency%250Avanishing.%2520This%2520leads%2520to%2520the%2520loss%2520of%2520critical%2520details%2520and%2520textures.%2520We%2520propose%250Aa%2520novel%252C%2520circuit-theory-inspired%2520strategy%2520called%2520Frequency-Dynamic%2520Attention%250AModulation%2520%2528FDAM%2529%252C%2520which%2520can%2520be%2520easily%2520plugged%2520into%2520ViTs.%2520FDAM%2520directly%250Amodulates%2520the%2520overall%2520frequency%2520response%2520of%2520ViTs%2520and%2520consists%2520of%2520two%250Atechniques%253A%2520Attention%2520Inversion%2520%2528AttInv%2529%2520and%2520Frequency%2520Dynamic%2520Scaling%250A%2528FreqScale%2529.%2520Since%2520circuit%2520theory%2520uses%2520low-pass%2520filters%2520as%2520fundamental%250Aelements%252C%2520we%2520introduce%2520AttInv%252C%2520a%2520method%2520that%2520generates%2520complementary%2520high-pass%250Afiltering%2520by%2520inverting%2520the%2520low-pass%2520filter%2520in%2520the%2520attention%2520matrix%252C%2520and%250Adynamically%2520combining%2520the%2520two.%2520We%2520further%2520design%2520FreqScale%2520to%2520weight%2520different%250Afrequency%2520components%2520for%2520fine-grained%2520adjustments%2520to%2520the%2520target%2520response%250Afunction.%2520Through%2520feature%2520similarity%2520analysis%2520and%2520effective%2520rank%2520evaluation%252C%2520we%250Ademonstrate%2520that%2520our%2520approach%2520avoids%2520representation%2520collapse%252C%2520leading%2520to%250Aconsistent%2520performance%2520improvements%2520across%2520various%2520models%252C%2520including%2520SegFormer%252C%250ADeiT%252C%2520and%2520MaskDINO.%2520These%2520improvements%2520are%2520evident%2520in%2520tasks%2520such%2520as%2520semantic%250Asegmentation%252C%2520object%2520detection%252C%2520and%2520instance%2520segmentation.%2520Additionally%252C%2520we%250Aapply%2520our%2520method%2520to%2520remote%2520sensing%2520detection%252C%2520achieving%2520state-of-the-art%250Aresults%2520in%2520single-scale%2520settings.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Linwei-Chen/FDAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12006v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Dynamic%20Attention%20Modulation%20for%20Dense%20Prediction&entry.906535625=Linwei%20Chen%20and%20Lin%20Gu%20and%20Ying%20Fu&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20significantly%20advanced%20computer%20vision%2C%0Ademonstrating%20strong%20performance%20across%20various%20tasks.%20However%2C%20the%20attention%0Amechanism%20in%20ViTs%20makes%20each%20layer%20function%20as%20a%20low-pass%20filter%2C%20and%20the%0Astacked-layer%20architecture%20in%20existing%20transformers%20suffers%20from%20frequency%0Avanishing.%20This%20leads%20to%20the%20loss%20of%20critical%20details%20and%20textures.%20We%20propose%0Aa%20novel%2C%20circuit-theory-inspired%20strategy%20called%20Frequency-Dynamic%20Attention%0AModulation%20%28FDAM%29%2C%20which%20can%20be%20easily%20plugged%20into%20ViTs.%20FDAM%20directly%0Amodulates%20the%20overall%20frequency%20response%20of%20ViTs%20and%20consists%20of%20two%0Atechniques%3A%20Attention%20Inversion%20%28AttInv%29%20and%20Frequency%20Dynamic%20Scaling%0A%28FreqScale%29.%20Since%20circuit%20theory%20uses%20low-pass%20filters%20as%20fundamental%0Aelements%2C%20we%20introduce%20AttInv%2C%20a%20method%20that%20generates%20complementary%20high-pass%0Afiltering%20by%20inverting%20the%20low-pass%20filter%20in%20the%20attention%20matrix%2C%20and%0Adynamically%20combining%20the%20two.%20We%20further%20design%20FreqScale%20to%20weight%20different%0Afrequency%20components%20for%20fine-grained%20adjustments%20to%20the%20target%20response%0Afunction.%20Through%20feature%20similarity%20analysis%20and%20effective%20rank%20evaluation%2C%20we%0Ademonstrate%20that%20our%20approach%20avoids%20representation%20collapse%2C%20leading%20to%0Aconsistent%20performance%20improvements%20across%20various%20models%2C%20including%20SegFormer%2C%0ADeiT%2C%20and%20MaskDINO.%20These%20improvements%20are%20evident%20in%20tasks%20such%20as%20semantic%0Asegmentation%2C%20object%20detection%2C%20and%20instance%20segmentation.%20Additionally%2C%20we%0Aapply%20our%20method%20to%20remote%20sensing%20detection%2C%20achieving%20state-of-the-art%0Aresults%20in%20single-scale%20settings.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Linwei-Chen/FDAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12006v3&entry.124074799=Read"},
{"title": "Position: An Empirically Grounded Identifiability Theory Will Accelerate\n  Self-Supervised Learning Research", "author": "Patrik Reizinger and Randall Balestriero and David Klindt and Wieland Brendel", "abstract": "  Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.\n", "link": "http://arxiv.org/abs/2504.13101v3", "date": "2025-07-24", "relevancy": 2.356, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4928}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4631}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20An%20Empirically%20Grounded%20Identifiability%20Theory%20Will%20Accelerate%0A%20%20Self-Supervised%20Learning%20Research&body=Title%3A%20Position%3A%20An%20Empirically%20Grounded%20Identifiability%20Theory%20Will%20Accelerate%0A%20%20Self-Supervised%20Learning%20Research%0AAuthor%3A%20Patrik%20Reizinger%20and%20Randall%20Balestriero%20and%20David%20Klindt%20and%20Wieland%20Brendel%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20powers%20many%20current%20AI%20systems.%20As%20research%0Ainterest%20and%20investment%20grow%2C%20the%20SSL%20design%20space%20continues%20to%20expand.%20The%0APlatonic%20view%20of%20SSL%2C%20following%20the%20Platonic%20Representation%20Hypothesis%20%28PRH%29%2C%0Asuggests%20that%20despite%20different%20methods%20and%20engineering%20approaches%2C%20all%0Arepresentations%20converge%20to%20the%20same%20Platonic%20ideal.%20However%2C%20this%20phenomenon%0Alacks%20precise%20theoretical%20explanation.%20By%20synthesizing%20evidence%20from%0AIdentifiability%20Theory%20%28IT%29%2C%20we%20show%20that%20the%20PRH%20can%20emerge%20in%20SSL.%20However%2C%0Acurrent%20IT%20cannot%20explain%20SSL%27s%20empirical%20success.%20To%20bridge%20the%20gap%20between%0Atheory%20and%20practice%2C%20we%20propose%20expanding%20IT%20into%20what%20we%20term%20Singular%0AIdentifiability%20Theory%20%28SITh%29%2C%20a%20broader%20theoretical%20framework%20encompassing%20the%0Aentire%20SSL%20pipeline.%20SITh%20would%20allow%20deeper%20insights%20into%20the%20implicit%20data%0Aassumptions%20in%20SSL%20and%20advance%20the%20field%20towards%20learning%20more%20interpretable%0Aand%20generalizable%20representations.%20We%20highlight%20three%20critical%20directions%20for%0Afuture%20research%3A%201%29%20training%20dynamics%20and%20convergence%20properties%20of%20SSL%3B%202%29%20the%0Aimpact%20of%20finite%20samples%2C%20batch%20size%2C%20and%20data%20diversity%3B%20and%203%29%20the%20role%20of%0Ainductive%20biases%20in%20architecture%2C%20augmentations%2C%20initialization%20schemes%2C%20and%0Aoptimizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13101v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520An%2520Empirically%2520Grounded%2520Identifiability%2520Theory%2520Will%2520Accelerate%250A%2520%2520Self-Supervised%2520Learning%2520Research%26entry.906535625%3DPatrik%2520Reizinger%2520and%2520Randall%2520Balestriero%2520and%2520David%2520Klindt%2520and%2520Wieland%2520Brendel%26entry.1292438233%3D%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520powers%2520many%2520current%2520AI%2520systems.%2520As%2520research%250Ainterest%2520and%2520investment%2520grow%252C%2520the%2520SSL%2520design%2520space%2520continues%2520to%2520expand.%2520The%250APlatonic%2520view%2520of%2520SSL%252C%2520following%2520the%2520Platonic%2520Representation%2520Hypothesis%2520%2528PRH%2529%252C%250Asuggests%2520that%2520despite%2520different%2520methods%2520and%2520engineering%2520approaches%252C%2520all%250Arepresentations%2520converge%2520to%2520the%2520same%2520Platonic%2520ideal.%2520However%252C%2520this%2520phenomenon%250Alacks%2520precise%2520theoretical%2520explanation.%2520By%2520synthesizing%2520evidence%2520from%250AIdentifiability%2520Theory%2520%2528IT%2529%252C%2520we%2520show%2520that%2520the%2520PRH%2520can%2520emerge%2520in%2520SSL.%2520However%252C%250Acurrent%2520IT%2520cannot%2520explain%2520SSL%2527s%2520empirical%2520success.%2520To%2520bridge%2520the%2520gap%2520between%250Atheory%2520and%2520practice%252C%2520we%2520propose%2520expanding%2520IT%2520into%2520what%2520we%2520term%2520Singular%250AIdentifiability%2520Theory%2520%2528SITh%2529%252C%2520a%2520broader%2520theoretical%2520framework%2520encompassing%2520the%250Aentire%2520SSL%2520pipeline.%2520SITh%2520would%2520allow%2520deeper%2520insights%2520into%2520the%2520implicit%2520data%250Aassumptions%2520in%2520SSL%2520and%2520advance%2520the%2520field%2520towards%2520learning%2520more%2520interpretable%250Aand%2520generalizable%2520representations.%2520We%2520highlight%2520three%2520critical%2520directions%2520for%250Afuture%2520research%253A%25201%2529%2520training%2520dynamics%2520and%2520convergence%2520properties%2520of%2520SSL%253B%25202%2529%2520the%250Aimpact%2520of%2520finite%2520samples%252C%2520batch%2520size%252C%2520and%2520data%2520diversity%253B%2520and%25203%2529%2520the%2520role%2520of%250Ainductive%2520biases%2520in%2520architecture%252C%2520augmentations%252C%2520initialization%2520schemes%252C%2520and%250Aoptimizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13101v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20An%20Empirically%20Grounded%20Identifiability%20Theory%20Will%20Accelerate%0A%20%20Self-Supervised%20Learning%20Research&entry.906535625=Patrik%20Reizinger%20and%20Randall%20Balestriero%20and%20David%20Klindt%20and%20Wieland%20Brendel&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20powers%20many%20current%20AI%20systems.%20As%20research%0Ainterest%20and%20investment%20grow%2C%20the%20SSL%20design%20space%20continues%20to%20expand.%20The%0APlatonic%20view%20of%20SSL%2C%20following%20the%20Platonic%20Representation%20Hypothesis%20%28PRH%29%2C%0Asuggests%20that%20despite%20different%20methods%20and%20engineering%20approaches%2C%20all%0Arepresentations%20converge%20to%20the%20same%20Platonic%20ideal.%20However%2C%20this%20phenomenon%0Alacks%20precise%20theoretical%20explanation.%20By%20synthesizing%20evidence%20from%0AIdentifiability%20Theory%20%28IT%29%2C%20we%20show%20that%20the%20PRH%20can%20emerge%20in%20SSL.%20However%2C%0Acurrent%20IT%20cannot%20explain%20SSL%27s%20empirical%20success.%20To%20bridge%20the%20gap%20between%0Atheory%20and%20practice%2C%20we%20propose%20expanding%20IT%20into%20what%20we%20term%20Singular%0AIdentifiability%20Theory%20%28SITh%29%2C%20a%20broader%20theoretical%20framework%20encompassing%20the%0Aentire%20SSL%20pipeline.%20SITh%20would%20allow%20deeper%20insights%20into%20the%20implicit%20data%0Aassumptions%20in%20SSL%20and%20advance%20the%20field%20towards%20learning%20more%20interpretable%0Aand%20generalizable%20representations.%20We%20highlight%20three%20critical%20directions%20for%0Afuture%20research%3A%201%29%20training%20dynamics%20and%20convergence%20properties%20of%20SSL%3B%202%29%20the%0Aimpact%20of%20finite%20samples%2C%20batch%20size%2C%20and%20data%20diversity%3B%20and%203%29%20the%20role%20of%0Ainductive%20biases%20in%20architecture%2C%20augmentations%2C%20initialization%20schemes%2C%20and%0Aoptimizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13101v3&entry.124074799=Read"},
{"title": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction", "author": "Guanyuan Pan and Tiansheng Zhou and Bingtao Ma and Yaqi Wang and Jianxiang Zhao and Zhi Li and Yugui Lin and Pietro Lio and Shuai Wang", "abstract": "  Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in analog circuit design automation. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a graph neural\nnetworks (GNNs) based method featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings,\nand Attributes for Link prediction) framework and achieve port-level accuracy\nin circuit link prediction. Second, we propose Netlist Babel Fish, a netlist\nformat conversion tool leveraging retrieval-augmented generation (RAG) with a\nlarge language model (LLM) to improve the compatibility of netlist formats.\nFinally, we construct SpiceNetlist, a comprehensive dataset that contains 775\nannotated circuits across 10 different component classes. Experiments\ndemonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on\nImage2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset\nevaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset\nevaluation, exhibiting robust feature transfer capabilities.\n", "link": "http://arxiv.org/abs/2504.10240v4", "date": "2025-07-24", "relevancy": 2.3502, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4984}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4759}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNN-ACLP%3A%20Graph%20Neural%20Networks%20Based%20Analog%20Circuit%20Link%20Prediction&body=Title%3A%20GNN-ACLP%3A%20Graph%20Neural%20Networks%20Based%20Analog%20Circuit%20Link%20Prediction%0AAuthor%3A%20Guanyuan%20Pan%20and%20Tiansheng%20Zhou%20and%20Bingtao%20Ma%20and%20Yaqi%20Wang%20and%20Jianxiang%20Zhao%20and%20Zhi%20Li%20and%20Yugui%20Lin%20and%20Pietro%20Lio%20and%20Shuai%20Wang%0AAbstract%3A%20%20%20Circuit%20link%20prediction%20identifying%20missing%20component%20connections%20from%0Aincomplete%20netlists%20is%20crucial%20in%20analog%20circuit%20design%20automation.%20However%2C%0Aexisting%20methods%20face%20three%20main%20challenges%3A%201%29%20Insufficient%20use%20of%20topological%0Apatterns%20in%20circuit%20graphs%20reduces%20prediction%20accuracy%3B%202%29%20Data%20scarcity%20due%20to%0Athe%20complexity%20of%20annotations%20hinders%20model%20generalization%3B%203%29%20Limited%0Aadaptability%20to%20various%20netlist%20formats.%20We%20propose%20GNN-ACLP%2C%20a%20graph%20neural%0Anetworks%20%28GNNs%29%20based%20method%20featuring%20three%20innovations%20to%20tackle%20these%0Achallenges.%20First%2C%20we%20introduce%20the%20SEAL%20%28learning%20from%20Subgraphs%2C%20Embeddings%2C%0Aand%20Attributes%20for%20Link%20prediction%29%20framework%20and%20achieve%20port-level%20accuracy%0Ain%20circuit%20link%20prediction.%20Second%2C%20we%20propose%20Netlist%20Babel%20Fish%2C%20a%20netlist%0Aformat%20conversion%20tool%20leveraging%20retrieval-augmented%20generation%20%28RAG%29%20with%20a%0Alarge%20language%20model%20%28LLM%29%20to%20improve%20the%20compatibility%20of%20netlist%20formats.%0AFinally%2C%20we%20construct%20SpiceNetlist%2C%20a%20comprehensive%20dataset%20that%20contains%20775%0Aannotated%20circuits%20across%2010%20different%20component%20classes.%20Experiments%0Ademonstrate%20accuracy%20improvements%20of%2016.08%25%20on%20SpiceNetlist%2C%2011.38%25%20on%0AImage2Net%2C%20and%2016.01%25%20on%20Masala-CHAI%20compared%20to%20the%20baseline%20in%20intra-dataset%0Aevaluation%2C%20while%20maintaining%20accuracy%20from%2092.05%25%20to%2099.07%25%20in%20cross-dataset%0Aevaluation%2C%20exhibiting%20robust%20feature%20transfer%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10240v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNN-ACLP%253A%2520Graph%2520Neural%2520Networks%2520Based%2520Analog%2520Circuit%2520Link%2520Prediction%26entry.906535625%3DGuanyuan%2520Pan%2520and%2520Tiansheng%2520Zhou%2520and%2520Bingtao%2520Ma%2520and%2520Yaqi%2520Wang%2520and%2520Jianxiang%2520Zhao%2520and%2520Zhi%2520Li%2520and%2520Yugui%2520Lin%2520and%2520Pietro%2520Lio%2520and%2520Shuai%2520Wang%26entry.1292438233%3D%2520%2520Circuit%2520link%2520prediction%2520identifying%2520missing%2520component%2520connections%2520from%250Aincomplete%2520netlists%2520is%2520crucial%2520in%2520analog%2520circuit%2520design%2520automation.%2520However%252C%250Aexisting%2520methods%2520face%2520three%2520main%2520challenges%253A%25201%2529%2520Insufficient%2520use%2520of%2520topological%250Apatterns%2520in%2520circuit%2520graphs%2520reduces%2520prediction%2520accuracy%253B%25202%2529%2520Data%2520scarcity%2520due%2520to%250Athe%2520complexity%2520of%2520annotations%2520hinders%2520model%2520generalization%253B%25203%2529%2520Limited%250Aadaptability%2520to%2520various%2520netlist%2520formats.%2520We%2520propose%2520GNN-ACLP%252C%2520a%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%2520based%2520method%2520featuring%2520three%2520innovations%2520to%2520tackle%2520these%250Achallenges.%2520First%252C%2520we%2520introduce%2520the%2520SEAL%2520%2528learning%2520from%2520Subgraphs%252C%2520Embeddings%252C%250Aand%2520Attributes%2520for%2520Link%2520prediction%2529%2520framework%2520and%2520achieve%2520port-level%2520accuracy%250Ain%2520circuit%2520link%2520prediction.%2520Second%252C%2520we%2520propose%2520Netlist%2520Babel%2520Fish%252C%2520a%2520netlist%250Aformat%2520conversion%2520tool%2520leveraging%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520with%2520a%250Alarge%2520language%2520model%2520%2528LLM%2529%2520to%2520improve%2520the%2520compatibility%2520of%2520netlist%2520formats.%250AFinally%252C%2520we%2520construct%2520SpiceNetlist%252C%2520a%2520comprehensive%2520dataset%2520that%2520contains%2520775%250Aannotated%2520circuits%2520across%252010%2520different%2520component%2520classes.%2520Experiments%250Ademonstrate%2520accuracy%2520improvements%2520of%252016.08%2525%2520on%2520SpiceNetlist%252C%252011.38%2525%2520on%250AImage2Net%252C%2520and%252016.01%2525%2520on%2520Masala-CHAI%2520compared%2520to%2520the%2520baseline%2520in%2520intra-dataset%250Aevaluation%252C%2520while%2520maintaining%2520accuracy%2520from%252092.05%2525%2520to%252099.07%2525%2520in%2520cross-dataset%250Aevaluation%252C%2520exhibiting%2520robust%2520feature%2520transfer%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10240v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNN-ACLP%3A%20Graph%20Neural%20Networks%20Based%20Analog%20Circuit%20Link%20Prediction&entry.906535625=Guanyuan%20Pan%20and%20Tiansheng%20Zhou%20and%20Bingtao%20Ma%20and%20Yaqi%20Wang%20and%20Jianxiang%20Zhao%20and%20Zhi%20Li%20and%20Yugui%20Lin%20and%20Pietro%20Lio%20and%20Shuai%20Wang&entry.1292438233=%20%20Circuit%20link%20prediction%20identifying%20missing%20component%20connections%20from%0Aincomplete%20netlists%20is%20crucial%20in%20analog%20circuit%20design%20automation.%20However%2C%0Aexisting%20methods%20face%20three%20main%20challenges%3A%201%29%20Insufficient%20use%20of%20topological%0Apatterns%20in%20circuit%20graphs%20reduces%20prediction%20accuracy%3B%202%29%20Data%20scarcity%20due%20to%0Athe%20complexity%20of%20annotations%20hinders%20model%20generalization%3B%203%29%20Limited%0Aadaptability%20to%20various%20netlist%20formats.%20We%20propose%20GNN-ACLP%2C%20a%20graph%20neural%0Anetworks%20%28GNNs%29%20based%20method%20featuring%20three%20innovations%20to%20tackle%20these%0Achallenges.%20First%2C%20we%20introduce%20the%20SEAL%20%28learning%20from%20Subgraphs%2C%20Embeddings%2C%0Aand%20Attributes%20for%20Link%20prediction%29%20framework%20and%20achieve%20port-level%20accuracy%0Ain%20circuit%20link%20prediction.%20Second%2C%20we%20propose%20Netlist%20Babel%20Fish%2C%20a%20netlist%0Aformat%20conversion%20tool%20leveraging%20retrieval-augmented%20generation%20%28RAG%29%20with%20a%0Alarge%20language%20model%20%28LLM%29%20to%20improve%20the%20compatibility%20of%20netlist%20formats.%0AFinally%2C%20we%20construct%20SpiceNetlist%2C%20a%20comprehensive%20dataset%20that%20contains%20775%0Aannotated%20circuits%20across%2010%20different%20component%20classes.%20Experiments%0Ademonstrate%20accuracy%20improvements%20of%2016.08%25%20on%20SpiceNetlist%2C%2011.38%25%20on%0AImage2Net%2C%20and%2016.01%25%20on%20Masala-CHAI%20compared%20to%20the%20baseline%20in%20intra-dataset%0Aevaluation%2C%20while%20maintaining%20accuracy%20from%2092.05%25%20to%2099.07%25%20in%20cross-dataset%0Aevaluation%2C%20exhibiting%20robust%20feature%20transfer%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10240v4&entry.124074799=Read"},
{"title": "HumanMaterial: Human Material Estimation from a Single Image via\n  Progressive Training", "author": "Yu Jiang and Jiahao Xia and Jiongming Qin and Yusen Wang and Tuo Cao and Chunxia Xiao", "abstract": "  Full-body Human inverse rendering based on physically-based rendering aims to\nacquire high-quality materials, which helps achieve photo-realistic rendering\nunder arbitrary illuminations. This task requires estimating multiple material\nmaps and usually relies on the constraint of rendering result. The absence of\nconstraints on the material maps makes inverse rendering an ill-posed task.\nPrevious works alleviated this problem by building material dataset for\ntraining, but their simplified material data and rendering equation lead to\nrendering results with limited realism, especially that of skin. To further\nalleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF)\nbased on scanned real data and statistical material data. In addition to the\nnormal, diffuse albedo, roughness, specular albedo, we produce displacement and\nsubsurface scattering to enhance the realism of rendering results, especially\nfor the skin. With the increase in prediction tasks for more materials, using\nan end-to-end model as in the previous work struggles to balance the importance\namong various material maps, and leads to model underfitting. Therefore, we\ndesign a model (HumanMaterial) with progressive training strategy to make full\nuse of the supervision information of the material maps and improve the\nperformance of material estimation. HumanMaterial first obtain the initial\nmaterial results via three prior models, and then refine the results by a\nfinetuning model. Prior models estimate different material maps, and each map\nhas different significance for rendering results. Thus, we design a Controlled\nPBR Rendering (CPR) loss, which enhances the importance of the materials to be\noptimized during the training of prior models. Extensive experiments on\nOpenHumanBRDF dataset and real data demonstrate that our method achieves\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2507.18385v1", "date": "2025-07-24", "relevancy": 2.3472, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6094}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5778}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanMaterial%3A%20Human%20Material%20Estimation%20from%20a%20Single%20Image%20via%0A%20%20Progressive%20Training&body=Title%3A%20HumanMaterial%3A%20Human%20Material%20Estimation%20from%20a%20Single%20Image%20via%0A%20%20Progressive%20Training%0AAuthor%3A%20Yu%20Jiang%20and%20Jiahao%20Xia%20and%20Jiongming%20Qin%20and%20Yusen%20Wang%20and%20Tuo%20Cao%20and%20Chunxia%20Xiao%0AAbstract%3A%20%20%20Full-body%20Human%20inverse%20rendering%20based%20on%20physically-based%20rendering%20aims%20to%0Aacquire%20high-quality%20materials%2C%20which%20helps%20achieve%20photo-realistic%20rendering%0Aunder%20arbitrary%20illuminations.%20This%20task%20requires%20estimating%20multiple%20material%0Amaps%20and%20usually%20relies%20on%20the%20constraint%20of%20rendering%20result.%20The%20absence%20of%0Aconstraints%20on%20the%20material%20maps%20makes%20inverse%20rendering%20an%20ill-posed%20task.%0APrevious%20works%20alleviated%20this%20problem%20by%20building%20material%20dataset%20for%0Atraining%2C%20but%20their%20simplified%20material%20data%20and%20rendering%20equation%20lead%20to%0Arendering%20results%20with%20limited%20realism%2C%20especially%20that%20of%20skin.%20To%20further%0Aalleviate%20this%20problem%2C%20we%20construct%20a%20higher-quality%20dataset%20%28OpenHumanBRDF%29%0Abased%20on%20scanned%20real%20data%20and%20statistical%20material%20data.%20In%20addition%20to%20the%0Anormal%2C%20diffuse%20albedo%2C%20roughness%2C%20specular%20albedo%2C%20we%20produce%20displacement%20and%0Asubsurface%20scattering%20to%20enhance%20the%20realism%20of%20rendering%20results%2C%20especially%0Afor%20the%20skin.%20With%20the%20increase%20in%20prediction%20tasks%20for%20more%20materials%2C%20using%0Aan%20end-to-end%20model%20as%20in%20the%20previous%20work%20struggles%20to%20balance%20the%20importance%0Aamong%20various%20material%20maps%2C%20and%20leads%20to%20model%20underfitting.%20Therefore%2C%20we%0Adesign%20a%20model%20%28HumanMaterial%29%20with%20progressive%20training%20strategy%20to%20make%20full%0Ause%20of%20the%20supervision%20information%20of%20the%20material%20maps%20and%20improve%20the%0Aperformance%20of%20material%20estimation.%20HumanMaterial%20first%20obtain%20the%20initial%0Amaterial%20results%20via%20three%20prior%20models%2C%20and%20then%20refine%20the%20results%20by%20a%0Afinetuning%20model.%20Prior%20models%20estimate%20different%20material%20maps%2C%20and%20each%20map%0Ahas%20different%20significance%20for%20rendering%20results.%20Thus%2C%20we%20design%20a%20Controlled%0APBR%20Rendering%20%28CPR%29%20loss%2C%20which%20enhances%20the%20importance%20of%20the%20materials%20to%20be%0Aoptimized%20during%20the%20training%20of%20prior%20models.%20Extensive%20experiments%20on%0AOpenHumanBRDF%20dataset%20and%20real%20data%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanMaterial%253A%2520Human%2520Material%2520Estimation%2520from%2520a%2520Single%2520Image%2520via%250A%2520%2520Progressive%2520Training%26entry.906535625%3DYu%2520Jiang%2520and%2520Jiahao%2520Xia%2520and%2520Jiongming%2520Qin%2520and%2520Yusen%2520Wang%2520and%2520Tuo%2520Cao%2520and%2520Chunxia%2520Xiao%26entry.1292438233%3D%2520%2520Full-body%2520Human%2520inverse%2520rendering%2520based%2520on%2520physically-based%2520rendering%2520aims%2520to%250Aacquire%2520high-quality%2520materials%252C%2520which%2520helps%2520achieve%2520photo-realistic%2520rendering%250Aunder%2520arbitrary%2520illuminations.%2520This%2520task%2520requires%2520estimating%2520multiple%2520material%250Amaps%2520and%2520usually%2520relies%2520on%2520the%2520constraint%2520of%2520rendering%2520result.%2520The%2520absence%2520of%250Aconstraints%2520on%2520the%2520material%2520maps%2520makes%2520inverse%2520rendering%2520an%2520ill-posed%2520task.%250APrevious%2520works%2520alleviated%2520this%2520problem%2520by%2520building%2520material%2520dataset%2520for%250Atraining%252C%2520but%2520their%2520simplified%2520material%2520data%2520and%2520rendering%2520equation%2520lead%2520to%250Arendering%2520results%2520with%2520limited%2520realism%252C%2520especially%2520that%2520of%2520skin.%2520To%2520further%250Aalleviate%2520this%2520problem%252C%2520we%2520construct%2520a%2520higher-quality%2520dataset%2520%2528OpenHumanBRDF%2529%250Abased%2520on%2520scanned%2520real%2520data%2520and%2520statistical%2520material%2520data.%2520In%2520addition%2520to%2520the%250Anormal%252C%2520diffuse%2520albedo%252C%2520roughness%252C%2520specular%2520albedo%252C%2520we%2520produce%2520displacement%2520and%250Asubsurface%2520scattering%2520to%2520enhance%2520the%2520realism%2520of%2520rendering%2520results%252C%2520especially%250Afor%2520the%2520skin.%2520With%2520the%2520increase%2520in%2520prediction%2520tasks%2520for%2520more%2520materials%252C%2520using%250Aan%2520end-to-end%2520model%2520as%2520in%2520the%2520previous%2520work%2520struggles%2520to%2520balance%2520the%2520importance%250Aamong%2520various%2520material%2520maps%252C%2520and%2520leads%2520to%2520model%2520underfitting.%2520Therefore%252C%2520we%250Adesign%2520a%2520model%2520%2528HumanMaterial%2529%2520with%2520progressive%2520training%2520strategy%2520to%2520make%2520full%250Ause%2520of%2520the%2520supervision%2520information%2520of%2520the%2520material%2520maps%2520and%2520improve%2520the%250Aperformance%2520of%2520material%2520estimation.%2520HumanMaterial%2520first%2520obtain%2520the%2520initial%250Amaterial%2520results%2520via%2520three%2520prior%2520models%252C%2520and%2520then%2520refine%2520the%2520results%2520by%2520a%250Afinetuning%2520model.%2520Prior%2520models%2520estimate%2520different%2520material%2520maps%252C%2520and%2520each%2520map%250Ahas%2520different%2520significance%2520for%2520rendering%2520results.%2520Thus%252C%2520we%2520design%2520a%2520Controlled%250APBR%2520Rendering%2520%2528CPR%2529%2520loss%252C%2520which%2520enhances%2520the%2520importance%2520of%2520the%2520materials%2520to%2520be%250Aoptimized%2520during%2520the%2520training%2520of%2520prior%2520models.%2520Extensive%2520experiments%2520on%250AOpenHumanBRDF%2520dataset%2520and%2520real%2520data%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanMaterial%3A%20Human%20Material%20Estimation%20from%20a%20Single%20Image%20via%0A%20%20Progressive%20Training&entry.906535625=Yu%20Jiang%20and%20Jiahao%20Xia%20and%20Jiongming%20Qin%20and%20Yusen%20Wang%20and%20Tuo%20Cao%20and%20Chunxia%20Xiao&entry.1292438233=%20%20Full-body%20Human%20inverse%20rendering%20based%20on%20physically-based%20rendering%20aims%20to%0Aacquire%20high-quality%20materials%2C%20which%20helps%20achieve%20photo-realistic%20rendering%0Aunder%20arbitrary%20illuminations.%20This%20task%20requires%20estimating%20multiple%20material%0Amaps%20and%20usually%20relies%20on%20the%20constraint%20of%20rendering%20result.%20The%20absence%20of%0Aconstraints%20on%20the%20material%20maps%20makes%20inverse%20rendering%20an%20ill-posed%20task.%0APrevious%20works%20alleviated%20this%20problem%20by%20building%20material%20dataset%20for%0Atraining%2C%20but%20their%20simplified%20material%20data%20and%20rendering%20equation%20lead%20to%0Arendering%20results%20with%20limited%20realism%2C%20especially%20that%20of%20skin.%20To%20further%0Aalleviate%20this%20problem%2C%20we%20construct%20a%20higher-quality%20dataset%20%28OpenHumanBRDF%29%0Abased%20on%20scanned%20real%20data%20and%20statistical%20material%20data.%20In%20addition%20to%20the%0Anormal%2C%20diffuse%20albedo%2C%20roughness%2C%20specular%20albedo%2C%20we%20produce%20displacement%20and%0Asubsurface%20scattering%20to%20enhance%20the%20realism%20of%20rendering%20results%2C%20especially%0Afor%20the%20skin.%20With%20the%20increase%20in%20prediction%20tasks%20for%20more%20materials%2C%20using%0Aan%20end-to-end%20model%20as%20in%20the%20previous%20work%20struggles%20to%20balance%20the%20importance%0Aamong%20various%20material%20maps%2C%20and%20leads%20to%20model%20underfitting.%20Therefore%2C%20we%0Adesign%20a%20model%20%28HumanMaterial%29%20with%20progressive%20training%20strategy%20to%20make%20full%0Ause%20of%20the%20supervision%20information%20of%20the%20material%20maps%20and%20improve%20the%0Aperformance%20of%20material%20estimation.%20HumanMaterial%20first%20obtain%20the%20initial%0Amaterial%20results%20via%20three%20prior%20models%2C%20and%20then%20refine%20the%20results%20by%20a%0Afinetuning%20model.%20Prior%20models%20estimate%20different%20material%20maps%2C%20and%20each%20map%0Ahas%20different%20significance%20for%20rendering%20results.%20Thus%2C%20we%20design%20a%20Controlled%0APBR%20Rendering%20%28CPR%29%20loss%2C%20which%20enhances%20the%20importance%20of%20the%20materials%20to%20be%0Aoptimized%20during%20the%20training%20of%20prior%20models.%20Extensive%20experiments%20on%0AOpenHumanBRDF%20dataset%20and%20real%20data%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18385v1&entry.124074799=Read"},
{"title": "Exploiting Gaussian Agnostic Representation Learning with Diffusion\n  Priors for Enhanced Infrared Small Target Detection", "author": "Junyao Li and Yahao Lu and Xingyuan Guo and Xiaoyu Xian and Tiantian Wang and Yukai Shi", "abstract": "  Infrared small target detection (ISTD) plays a vital role in numerous\npractical applications. In pursuit of determining the performance boundaries,\nresearchers employ large and expensive manual-labeling data for representation\nlearning. Nevertheless, this approach renders the state-of-the-art ISTD methods\nhighly fragile in real-world challenges. In this paper, we first study the\nvariation in detection performance across several mainstream methods under\nvarious scarcity -- namely, the absence of high-quality infrared data -- that\nchallenge the prevailing theories about practical ISTD. To address this\nconcern, we introduce the Gaussian Agnostic Representation Learning.\nSpecifically, we propose the Gaussian Group Squeezer, leveraging Gaussian\nsampling and compression for non-uniform quantization. By exploiting a diverse\narray of training samples, we enhance the resilience of ISTD models against\nvarious challenges. Then, we introduce two-stage diffusion models for\nreal-world reconstruction. By aligning quantized signals closely with\nreal-world distributions, we significantly elevate the quality and fidelity of\nthe synthetic samples. Comparative evaluations against state-of-the-art\ndetection methods in various scarcity scenarios demonstrate the efficacy of the\nproposed approach.\n", "link": "http://arxiv.org/abs/2507.18260v1", "date": "2025-07-24", "relevancy": 2.3467, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6018}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5838}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Gaussian%20Agnostic%20Representation%20Learning%20with%20Diffusion%0A%20%20Priors%20for%20Enhanced%20Infrared%20Small%20Target%20Detection&body=Title%3A%20Exploiting%20Gaussian%20Agnostic%20Representation%20Learning%20with%20Diffusion%0A%20%20Priors%20for%20Enhanced%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Junyao%20Li%20and%20Yahao%20Lu%20and%20Xingyuan%20Guo%20and%20Xiaoyu%20Xian%20and%20Tiantian%20Wang%20and%20Yukai%20Shi%0AAbstract%3A%20%20%20Infrared%20small%20target%20detection%20%28ISTD%29%20plays%20a%20vital%20role%20in%20numerous%0Apractical%20applications.%20In%20pursuit%20of%20determining%20the%20performance%20boundaries%2C%0Aresearchers%20employ%20large%20and%20expensive%20manual-labeling%20data%20for%20representation%0Alearning.%20Nevertheless%2C%20this%20approach%20renders%20the%20state-of-the-art%20ISTD%20methods%0Ahighly%20fragile%20in%20real-world%20challenges.%20In%20this%20paper%2C%20we%20first%20study%20the%0Avariation%20in%20detection%20performance%20across%20several%20mainstream%20methods%20under%0Avarious%20scarcity%20--%20namely%2C%20the%20absence%20of%20high-quality%20infrared%20data%20--%20that%0Achallenge%20the%20prevailing%20theories%20about%20practical%20ISTD.%20To%20address%20this%0Aconcern%2C%20we%20introduce%20the%20Gaussian%20Agnostic%20Representation%20Learning.%0ASpecifically%2C%20we%20propose%20the%20Gaussian%20Group%20Squeezer%2C%20leveraging%20Gaussian%0Asampling%20and%20compression%20for%20non-uniform%20quantization.%20By%20exploiting%20a%20diverse%0Aarray%20of%20training%20samples%2C%20we%20enhance%20the%20resilience%20of%20ISTD%20models%20against%0Avarious%20challenges.%20Then%2C%20we%20introduce%20two-stage%20diffusion%20models%20for%0Areal-world%20reconstruction.%20By%20aligning%20quantized%20signals%20closely%20with%0Areal-world%20distributions%2C%20we%20significantly%20elevate%20the%20quality%20and%20fidelity%20of%0Athe%20synthetic%20samples.%20Comparative%20evaluations%20against%20state-of-the-art%0Adetection%20methods%20in%20various%20scarcity%20scenarios%20demonstrate%20the%20efficacy%20of%20the%0Aproposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Gaussian%2520Agnostic%2520Representation%2520Learning%2520with%2520Diffusion%250A%2520%2520Priors%2520for%2520Enhanced%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DJunyao%2520Li%2520and%2520Yahao%2520Lu%2520and%2520Xingyuan%2520Guo%2520and%2520Xiaoyu%2520Xian%2520and%2520Tiantian%2520Wang%2520and%2520Yukai%2520Shi%26entry.1292438233%3D%2520%2520Infrared%2520small%2520target%2520detection%2520%2528ISTD%2529%2520plays%2520a%2520vital%2520role%2520in%2520numerous%250Apractical%2520applications.%2520In%2520pursuit%2520of%2520determining%2520the%2520performance%2520boundaries%252C%250Aresearchers%2520employ%2520large%2520and%2520expensive%2520manual-labeling%2520data%2520for%2520representation%250Alearning.%2520Nevertheless%252C%2520this%2520approach%2520renders%2520the%2520state-of-the-art%2520ISTD%2520methods%250Ahighly%2520fragile%2520in%2520real-world%2520challenges.%2520In%2520this%2520paper%252C%2520we%2520first%2520study%2520the%250Avariation%2520in%2520detection%2520performance%2520across%2520several%2520mainstream%2520methods%2520under%250Avarious%2520scarcity%2520--%2520namely%252C%2520the%2520absence%2520of%2520high-quality%2520infrared%2520data%2520--%2520that%250Achallenge%2520the%2520prevailing%2520theories%2520about%2520practical%2520ISTD.%2520To%2520address%2520this%250Aconcern%252C%2520we%2520introduce%2520the%2520Gaussian%2520Agnostic%2520Representation%2520Learning.%250ASpecifically%252C%2520we%2520propose%2520the%2520Gaussian%2520Group%2520Squeezer%252C%2520leveraging%2520Gaussian%250Asampling%2520and%2520compression%2520for%2520non-uniform%2520quantization.%2520By%2520exploiting%2520a%2520diverse%250Aarray%2520of%2520training%2520samples%252C%2520we%2520enhance%2520the%2520resilience%2520of%2520ISTD%2520models%2520against%250Avarious%2520challenges.%2520Then%252C%2520we%2520introduce%2520two-stage%2520diffusion%2520models%2520for%250Areal-world%2520reconstruction.%2520By%2520aligning%2520quantized%2520signals%2520closely%2520with%250Areal-world%2520distributions%252C%2520we%2520significantly%2520elevate%2520the%2520quality%2520and%2520fidelity%2520of%250Athe%2520synthetic%2520samples.%2520Comparative%2520evaluations%2520against%2520state-of-the-art%250Adetection%2520methods%2520in%2520various%2520scarcity%2520scenarios%2520demonstrate%2520the%2520efficacy%2520of%2520the%250Aproposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Gaussian%20Agnostic%20Representation%20Learning%20with%20Diffusion%0A%20%20Priors%20for%20Enhanced%20Infrared%20Small%20Target%20Detection&entry.906535625=Junyao%20Li%20and%20Yahao%20Lu%20and%20Xingyuan%20Guo%20and%20Xiaoyu%20Xian%20and%20Tiantian%20Wang%20and%20Yukai%20Shi&entry.1292438233=%20%20Infrared%20small%20target%20detection%20%28ISTD%29%20plays%20a%20vital%20role%20in%20numerous%0Apractical%20applications.%20In%20pursuit%20of%20determining%20the%20performance%20boundaries%2C%0Aresearchers%20employ%20large%20and%20expensive%20manual-labeling%20data%20for%20representation%0Alearning.%20Nevertheless%2C%20this%20approach%20renders%20the%20state-of-the-art%20ISTD%20methods%0Ahighly%20fragile%20in%20real-world%20challenges.%20In%20this%20paper%2C%20we%20first%20study%20the%0Avariation%20in%20detection%20performance%20across%20several%20mainstream%20methods%20under%0Avarious%20scarcity%20--%20namely%2C%20the%20absence%20of%20high-quality%20infrared%20data%20--%20that%0Achallenge%20the%20prevailing%20theories%20about%20practical%20ISTD.%20To%20address%20this%0Aconcern%2C%20we%20introduce%20the%20Gaussian%20Agnostic%20Representation%20Learning.%0ASpecifically%2C%20we%20propose%20the%20Gaussian%20Group%20Squeezer%2C%20leveraging%20Gaussian%0Asampling%20and%20compression%20for%20non-uniform%20quantization.%20By%20exploiting%20a%20diverse%0Aarray%20of%20training%20samples%2C%20we%20enhance%20the%20resilience%20of%20ISTD%20models%20against%0Avarious%20challenges.%20Then%2C%20we%20introduce%20two-stage%20diffusion%20models%20for%0Areal-world%20reconstruction.%20By%20aligning%20quantized%20signals%20closely%20with%0Areal-world%20distributions%2C%20we%20significantly%20elevate%20the%20quality%20and%20fidelity%20of%0Athe%20synthetic%20samples.%20Comparative%20evaluations%20against%20state-of-the-art%0Adetection%20methods%20in%20various%20scarcity%20scenarios%20demonstrate%20the%20efficacy%20of%20the%0Aproposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18260v1&entry.124074799=Read"},
{"title": "Learning Gentle Grasping Using Vision, Sound, and Touch", "author": "Ken Nakahara and Roberto Calandra", "abstract": "  In our daily life, we often encounter objects that are fragile and can be\ndamaged by excessive grasping force, such as fruits. For these objects, it is\nparamount to grasp gently -- not using the maximum amount of force possible,\nbut rather the minimum amount of force necessary. This paper proposes using\nvisual, tactile, and auditory signals to learn to grasp and regrasp objects\nstably and gently. Specifically, we use audio signals as an indicator of\ngentleness during the grasping, and then train an end-to-end action-conditional\nmodel from raw visuo-tactile inputs that predicts both the stability and the\ngentleness of future grasping candidates, thus allowing the selection and\nexecution of the most promising action. Experimental results on a\nmulti-fingered hand over 1,500 grasping trials demonstrated that our model is\nuseful for gentle grasping by validating the predictive performance (3.27%\nhigher accuracy than the vision-only variant) and providing interpretations of\ntheir behavior. Finally, real-world experiments confirmed that the grasping\nperformance with the trained multi-modal model outperformed other baselines\n(17% higher rate for stable and gentle grasps than vision-only). Our approach\nrequires neither tactile sensor calibration nor analytical force modeling,\ndrastically reducing the engineering effort to grasp fragile objects. Dataset\nand videos are available at https://lasr.org/research/gentle-grasping.\n", "link": "http://arxiv.org/abs/2503.07926v2", "date": "2025-07-24", "relevancy": 2.3427, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.645}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5704}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Gentle%20Grasping%20Using%20Vision%2C%20Sound%2C%20and%20Touch&body=Title%3A%20Learning%20Gentle%20Grasping%20Using%20Vision%2C%20Sound%2C%20and%20Touch%0AAuthor%3A%20Ken%20Nakahara%20and%20Roberto%20Calandra%0AAbstract%3A%20%20%20In%20our%20daily%20life%2C%20we%20often%20encounter%20objects%20that%20are%20fragile%20and%20can%20be%0Adamaged%20by%20excessive%20grasping%20force%2C%20such%20as%20fruits.%20For%20these%20objects%2C%20it%20is%0Aparamount%20to%20grasp%20gently%20--%20not%20using%20the%20maximum%20amount%20of%20force%20possible%2C%0Abut%20rather%20the%20minimum%20amount%20of%20force%20necessary.%20This%20paper%20proposes%20using%0Avisual%2C%20tactile%2C%20and%20auditory%20signals%20to%20learn%20to%20grasp%20and%20regrasp%20objects%0Astably%20and%20gently.%20Specifically%2C%20we%20use%20audio%20signals%20as%20an%20indicator%20of%0Agentleness%20during%20the%20grasping%2C%20and%20then%20train%20an%20end-to-end%20action-conditional%0Amodel%20from%20raw%20visuo-tactile%20inputs%20that%20predicts%20both%20the%20stability%20and%20the%0Agentleness%20of%20future%20grasping%20candidates%2C%20thus%20allowing%20the%20selection%20and%0Aexecution%20of%20the%20most%20promising%20action.%20Experimental%20results%20on%20a%0Amulti-fingered%20hand%20over%201%2C500%20grasping%20trials%20demonstrated%20that%20our%20model%20is%0Auseful%20for%20gentle%20grasping%20by%20validating%20the%20predictive%20performance%20%283.27%25%0Ahigher%20accuracy%20than%20the%20vision-only%20variant%29%20and%20providing%20interpretations%20of%0Atheir%20behavior.%20Finally%2C%20real-world%20experiments%20confirmed%20that%20the%20grasping%0Aperformance%20with%20the%20trained%20multi-modal%20model%20outperformed%20other%20baselines%0A%2817%25%20higher%20rate%20for%20stable%20and%20gentle%20grasps%20than%20vision-only%29.%20Our%20approach%0Arequires%20neither%20tactile%20sensor%20calibration%20nor%20analytical%20force%20modeling%2C%0Adrastically%20reducing%20the%20engineering%20effort%20to%20grasp%20fragile%20objects.%20Dataset%0Aand%20videos%20are%20available%20at%20https%3A//lasr.org/research/gentle-grasping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Gentle%2520Grasping%2520Using%2520Vision%252C%2520Sound%252C%2520and%2520Touch%26entry.906535625%3DKen%2520Nakahara%2520and%2520Roberto%2520Calandra%26entry.1292438233%3D%2520%2520In%2520our%2520daily%2520life%252C%2520we%2520often%2520encounter%2520objects%2520that%2520are%2520fragile%2520and%2520can%2520be%250Adamaged%2520by%2520excessive%2520grasping%2520force%252C%2520such%2520as%2520fruits.%2520For%2520these%2520objects%252C%2520it%2520is%250Aparamount%2520to%2520grasp%2520gently%2520--%2520not%2520using%2520the%2520maximum%2520amount%2520of%2520force%2520possible%252C%250Abut%2520rather%2520the%2520minimum%2520amount%2520of%2520force%2520necessary.%2520This%2520paper%2520proposes%2520using%250Avisual%252C%2520tactile%252C%2520and%2520auditory%2520signals%2520to%2520learn%2520to%2520grasp%2520and%2520regrasp%2520objects%250Astably%2520and%2520gently.%2520Specifically%252C%2520we%2520use%2520audio%2520signals%2520as%2520an%2520indicator%2520of%250Agentleness%2520during%2520the%2520grasping%252C%2520and%2520then%2520train%2520an%2520end-to-end%2520action-conditional%250Amodel%2520from%2520raw%2520visuo-tactile%2520inputs%2520that%2520predicts%2520both%2520the%2520stability%2520and%2520the%250Agentleness%2520of%2520future%2520grasping%2520candidates%252C%2520thus%2520allowing%2520the%2520selection%2520and%250Aexecution%2520of%2520the%2520most%2520promising%2520action.%2520Experimental%2520results%2520on%2520a%250Amulti-fingered%2520hand%2520over%25201%252C500%2520grasping%2520trials%2520demonstrated%2520that%2520our%2520model%2520is%250Auseful%2520for%2520gentle%2520grasping%2520by%2520validating%2520the%2520predictive%2520performance%2520%25283.27%2525%250Ahigher%2520accuracy%2520than%2520the%2520vision-only%2520variant%2529%2520and%2520providing%2520interpretations%2520of%250Atheir%2520behavior.%2520Finally%252C%2520real-world%2520experiments%2520confirmed%2520that%2520the%2520grasping%250Aperformance%2520with%2520the%2520trained%2520multi-modal%2520model%2520outperformed%2520other%2520baselines%250A%252817%2525%2520higher%2520rate%2520for%2520stable%2520and%2520gentle%2520grasps%2520than%2520vision-only%2529.%2520Our%2520approach%250Arequires%2520neither%2520tactile%2520sensor%2520calibration%2520nor%2520analytical%2520force%2520modeling%252C%250Adrastically%2520reducing%2520the%2520engineering%2520effort%2520to%2520grasp%2520fragile%2520objects.%2520Dataset%250Aand%2520videos%2520are%2520available%2520at%2520https%253A//lasr.org/research/gentle-grasping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Gentle%20Grasping%20Using%20Vision%2C%20Sound%2C%20and%20Touch&entry.906535625=Ken%20Nakahara%20and%20Roberto%20Calandra&entry.1292438233=%20%20In%20our%20daily%20life%2C%20we%20often%20encounter%20objects%20that%20are%20fragile%20and%20can%20be%0Adamaged%20by%20excessive%20grasping%20force%2C%20such%20as%20fruits.%20For%20these%20objects%2C%20it%20is%0Aparamount%20to%20grasp%20gently%20--%20not%20using%20the%20maximum%20amount%20of%20force%20possible%2C%0Abut%20rather%20the%20minimum%20amount%20of%20force%20necessary.%20This%20paper%20proposes%20using%0Avisual%2C%20tactile%2C%20and%20auditory%20signals%20to%20learn%20to%20grasp%20and%20regrasp%20objects%0Astably%20and%20gently.%20Specifically%2C%20we%20use%20audio%20signals%20as%20an%20indicator%20of%0Agentleness%20during%20the%20grasping%2C%20and%20then%20train%20an%20end-to-end%20action-conditional%0Amodel%20from%20raw%20visuo-tactile%20inputs%20that%20predicts%20both%20the%20stability%20and%20the%0Agentleness%20of%20future%20grasping%20candidates%2C%20thus%20allowing%20the%20selection%20and%0Aexecution%20of%20the%20most%20promising%20action.%20Experimental%20results%20on%20a%0Amulti-fingered%20hand%20over%201%2C500%20grasping%20trials%20demonstrated%20that%20our%20model%20is%0Auseful%20for%20gentle%20grasping%20by%20validating%20the%20predictive%20performance%20%283.27%25%0Ahigher%20accuracy%20than%20the%20vision-only%20variant%29%20and%20providing%20interpretations%20of%0Atheir%20behavior.%20Finally%2C%20real-world%20experiments%20confirmed%20that%20the%20grasping%0Aperformance%20with%20the%20trained%20multi-modal%20model%20outperformed%20other%20baselines%0A%2817%25%20higher%20rate%20for%20stable%20and%20gentle%20grasps%20than%20vision-only%29.%20Our%20approach%0Arequires%20neither%20tactile%20sensor%20calibration%20nor%20analytical%20force%20modeling%2C%0Adrastically%20reducing%20the%20engineering%20effort%20to%20grasp%20fragile%20objects.%20Dataset%0Aand%20videos%20are%20available%20at%20https%3A//lasr.org/research/gentle-grasping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07926v2&entry.124074799=Read"},
{"title": "HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation", "author": "Xinyu Wang and Jinghua Hou and Zhe Liu and Yingying Zhu", "abstract": "  Transformer-based methods have demonstrated remarkable capabilities in 3D\nsemantic segmentation through their powerful attention mechanisms, but the\nquadratic complexity limits their modeling of long-range dependencies in\nlarge-scale point clouds. While recent Mamba-based approaches offer efficient\nprocessing with linear complexity, they struggle with feature representation\nwhen extracting 3D features. However, effectively combining these complementary\nstrengths remains an open challenge in this field. In this paper, we propose\nHybridTM, the first hybrid architecture that integrates Transformer and Mamba\nfor 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid\nStrategy, which combines attention and Mamba at a finer granularity, enabling\nsimultaneous capture of long-range dependencies and fine-grained local\nfeatures. Extensive experiments demonstrate the effectiveness and\ngeneralization of our HybridTM on diverse indoor and outdoor datasets.\nFurthermore, our HybridTM achieves state-of-the-art performance on ScanNet,\nScanNet200, and nuScenes benchmarks. The code will be made available at\nhttps://github.com/deepinact/HybridTM.\n", "link": "http://arxiv.org/abs/2507.18575v1", "date": "2025-07-24", "relevancy": 2.3385, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6079}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5747}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HybridTM%3A%20Combining%20Transformer%20and%20Mamba%20for%203D%20Semantic%20Segmentation&body=Title%3A%20HybridTM%3A%20Combining%20Transformer%20and%20Mamba%20for%203D%20Semantic%20Segmentation%0AAuthor%3A%20Xinyu%20Wang%20and%20Jinghua%20Hou%20and%20Zhe%20Liu%20and%20Yingying%20Zhu%0AAbstract%3A%20%20%20Transformer-based%20methods%20have%20demonstrated%20remarkable%20capabilities%20in%203D%0Asemantic%20segmentation%20through%20their%20powerful%20attention%20mechanisms%2C%20but%20the%0Aquadratic%20complexity%20limits%20their%20modeling%20of%20long-range%20dependencies%20in%0Alarge-scale%20point%20clouds.%20While%20recent%20Mamba-based%20approaches%20offer%20efficient%0Aprocessing%20with%20linear%20complexity%2C%20they%20struggle%20with%20feature%20representation%0Awhen%20extracting%203D%20features.%20However%2C%20effectively%20combining%20these%20complementary%0Astrengths%20remains%20an%20open%20challenge%20in%20this%20field.%20In%20this%20paper%2C%20we%20propose%0AHybridTM%2C%20the%20first%20hybrid%20architecture%20that%20integrates%20Transformer%20and%20Mamba%0Afor%203D%20semantic%20segmentation.%20In%20addition%2C%20we%20propose%20the%20Inner%20Layer%20Hybrid%0AStrategy%2C%20which%20combines%20attention%20and%20Mamba%20at%20a%20finer%20granularity%2C%20enabling%0Asimultaneous%20capture%20of%20long-range%20dependencies%20and%20fine-grained%20local%0Afeatures.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%0Ageneralization%20of%20our%20HybridTM%20on%20diverse%20indoor%20and%20outdoor%20datasets.%0AFurthermore%2C%20our%20HybridTM%20achieves%20state-of-the-art%20performance%20on%20ScanNet%2C%0AScanNet200%2C%20and%20nuScenes%20benchmarks.%20The%20code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/deepinact/HybridTM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridTM%253A%2520Combining%2520Transformer%2520and%2520Mamba%2520for%25203D%2520Semantic%2520Segmentation%26entry.906535625%3DXinyu%2520Wang%2520and%2520Jinghua%2520Hou%2520and%2520Zhe%2520Liu%2520and%2520Yingying%2520Zhu%26entry.1292438233%3D%2520%2520Transformer-based%2520methods%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%25203D%250Asemantic%2520segmentation%2520through%2520their%2520powerful%2520attention%2520mechanisms%252C%2520but%2520the%250Aquadratic%2520complexity%2520limits%2520their%2520modeling%2520of%2520long-range%2520dependencies%2520in%250Alarge-scale%2520point%2520clouds.%2520While%2520recent%2520Mamba-based%2520approaches%2520offer%2520efficient%250Aprocessing%2520with%2520linear%2520complexity%252C%2520they%2520struggle%2520with%2520feature%2520representation%250Awhen%2520extracting%25203D%2520features.%2520However%252C%2520effectively%2520combining%2520these%2520complementary%250Astrengths%2520remains%2520an%2520open%2520challenge%2520in%2520this%2520field.%2520In%2520this%2520paper%252C%2520we%2520propose%250AHybridTM%252C%2520the%2520first%2520hybrid%2520architecture%2520that%2520integrates%2520Transformer%2520and%2520Mamba%250Afor%25203D%2520semantic%2520segmentation.%2520In%2520addition%252C%2520we%2520propose%2520the%2520Inner%2520Layer%2520Hybrid%250AStrategy%252C%2520which%2520combines%2520attention%2520and%2520Mamba%2520at%2520a%2520finer%2520granularity%252C%2520enabling%250Asimultaneous%2520capture%2520of%2520long-range%2520dependencies%2520and%2520fine-grained%2520local%250Afeatures.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520and%250Ageneralization%2520of%2520our%2520HybridTM%2520on%2520diverse%2520indoor%2520and%2520outdoor%2520datasets.%250AFurthermore%252C%2520our%2520HybridTM%2520achieves%2520state-of-the-art%2520performance%2520on%2520ScanNet%252C%250AScanNet200%252C%2520and%2520nuScenes%2520benchmarks.%2520The%2520code%2520will%2520be%2520made%2520available%2520at%250Ahttps%253A//github.com/deepinact/HybridTM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HybridTM%3A%20Combining%20Transformer%20and%20Mamba%20for%203D%20Semantic%20Segmentation&entry.906535625=Xinyu%20Wang%20and%20Jinghua%20Hou%20and%20Zhe%20Liu%20and%20Yingying%20Zhu&entry.1292438233=%20%20Transformer-based%20methods%20have%20demonstrated%20remarkable%20capabilities%20in%203D%0Asemantic%20segmentation%20through%20their%20powerful%20attention%20mechanisms%2C%20but%20the%0Aquadratic%20complexity%20limits%20their%20modeling%20of%20long-range%20dependencies%20in%0Alarge-scale%20point%20clouds.%20While%20recent%20Mamba-based%20approaches%20offer%20efficient%0Aprocessing%20with%20linear%20complexity%2C%20they%20struggle%20with%20feature%20representation%0Awhen%20extracting%203D%20features.%20However%2C%20effectively%20combining%20these%20complementary%0Astrengths%20remains%20an%20open%20challenge%20in%20this%20field.%20In%20this%20paper%2C%20we%20propose%0AHybridTM%2C%20the%20first%20hybrid%20architecture%20that%20integrates%20Transformer%20and%20Mamba%0Afor%203D%20semantic%20segmentation.%20In%20addition%2C%20we%20propose%20the%20Inner%20Layer%20Hybrid%0AStrategy%2C%20which%20combines%20attention%20and%20Mamba%20at%20a%20finer%20granularity%2C%20enabling%0Asimultaneous%20capture%20of%20long-range%20dependencies%20and%20fine-grained%20local%0Afeatures.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%0Ageneralization%20of%20our%20HybridTM%20on%20diverse%20indoor%20and%20outdoor%20datasets.%0AFurthermore%2C%20our%20HybridTM%20achieves%20state-of-the-art%20performance%20on%20ScanNet%2C%0AScanNet200%2C%20and%20nuScenes%20benchmarks.%20The%20code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/deepinact/HybridTM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18575v1&entry.124074799=Read"},
{"title": "Self-Supervised Coarsening of Unstructured Grid with Automatic\n  Differentiation", "author": "Sergei Shumilin and Alexander Ryabov and Nikolay Yavich and Evgeny Burnaev and Vladimir Vanovskiy", "abstract": "  Due to the high computational load of modern numerical simulation, there is a\ndemand for approaches that would reduce the size of discrete problems while\nkeeping the accuracy reasonable. In this work, we present an original algorithm\nto coarsen an unstructured grid based on the concepts of differentiable\nphysics. We achieve this by employing k-means clustering, autodifferentiation\nand stochastic minimization algorithms. We demonstrate performance of the\ndesigned algorithm on two PDEs: a linear parabolic equation which governs\nslightly compressible fluid flow in porous media and the wave equation. Our\nresults show that in the considered scenarios, we reduced the number of grid\npoints up to 10 times while preserving the modeled variable dynamics in the\npoints of interest. The proposed approach can be applied to the simulation of\nan arbitrary system described by evolutionary partial differential equations.\n", "link": "http://arxiv.org/abs/2507.18297v1", "date": "2025-07-24", "relevancy": 2.3339, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.488}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.46}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Coarsening%20of%20Unstructured%20Grid%20with%20Automatic%0A%20%20Differentiation&body=Title%3A%20Self-Supervised%20Coarsening%20of%20Unstructured%20Grid%20with%20Automatic%0A%20%20Differentiation%0AAuthor%3A%20Sergei%20Shumilin%20and%20Alexander%20Ryabov%20and%20Nikolay%20Yavich%20and%20Evgeny%20Burnaev%20and%20Vladimir%20Vanovskiy%0AAbstract%3A%20%20%20Due%20to%20the%20high%20computational%20load%20of%20modern%20numerical%20simulation%2C%20there%20is%20a%0Ademand%20for%20approaches%20that%20would%20reduce%20the%20size%20of%20discrete%20problems%20while%0Akeeping%20the%20accuracy%20reasonable.%20In%20this%20work%2C%20we%20present%20an%20original%20algorithm%0Ato%20coarsen%20an%20unstructured%20grid%20based%20on%20the%20concepts%20of%20differentiable%0Aphysics.%20We%20achieve%20this%20by%20employing%20k-means%20clustering%2C%20autodifferentiation%0Aand%20stochastic%20minimization%20algorithms.%20We%20demonstrate%20performance%20of%20the%0Adesigned%20algorithm%20on%20two%20PDEs%3A%20a%20linear%20parabolic%20equation%20which%20governs%0Aslightly%20compressible%20fluid%20flow%20in%20porous%20media%20and%20the%20wave%20equation.%20Our%0Aresults%20show%20that%20in%20the%20considered%20scenarios%2C%20we%20reduced%20the%20number%20of%20grid%0Apoints%20up%20to%2010%20times%20while%20preserving%20the%20modeled%20variable%20dynamics%20in%20the%0Apoints%20of%20interest.%20The%20proposed%20approach%20can%20be%20applied%20to%20the%20simulation%20of%0Aan%20arbitrary%20system%20described%20by%20evolutionary%20partial%20differential%20equations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Coarsening%2520of%2520Unstructured%2520Grid%2520with%2520Automatic%250A%2520%2520Differentiation%26entry.906535625%3DSergei%2520Shumilin%2520and%2520Alexander%2520Ryabov%2520and%2520Nikolay%2520Yavich%2520and%2520Evgeny%2520Burnaev%2520and%2520Vladimir%2520Vanovskiy%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520high%2520computational%2520load%2520of%2520modern%2520numerical%2520simulation%252C%2520there%2520is%2520a%250Ademand%2520for%2520approaches%2520that%2520would%2520reduce%2520the%2520size%2520of%2520discrete%2520problems%2520while%250Akeeping%2520the%2520accuracy%2520reasonable.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520original%2520algorithm%250Ato%2520coarsen%2520an%2520unstructured%2520grid%2520based%2520on%2520the%2520concepts%2520of%2520differentiable%250Aphysics.%2520We%2520achieve%2520this%2520by%2520employing%2520k-means%2520clustering%252C%2520autodifferentiation%250Aand%2520stochastic%2520minimization%2520algorithms.%2520We%2520demonstrate%2520performance%2520of%2520the%250Adesigned%2520algorithm%2520on%2520two%2520PDEs%253A%2520a%2520linear%2520parabolic%2520equation%2520which%2520governs%250Aslightly%2520compressible%2520fluid%2520flow%2520in%2520porous%2520media%2520and%2520the%2520wave%2520equation.%2520Our%250Aresults%2520show%2520that%2520in%2520the%2520considered%2520scenarios%252C%2520we%2520reduced%2520the%2520number%2520of%2520grid%250Apoints%2520up%2520to%252010%2520times%2520while%2520preserving%2520the%2520modeled%2520variable%2520dynamics%2520in%2520the%250Apoints%2520of%2520interest.%2520The%2520proposed%2520approach%2520can%2520be%2520applied%2520to%2520the%2520simulation%2520of%250Aan%2520arbitrary%2520system%2520described%2520by%2520evolutionary%2520partial%2520differential%2520equations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Coarsening%20of%20Unstructured%20Grid%20with%20Automatic%0A%20%20Differentiation&entry.906535625=Sergei%20Shumilin%20and%20Alexander%20Ryabov%20and%20Nikolay%20Yavich%20and%20Evgeny%20Burnaev%20and%20Vladimir%20Vanovskiy&entry.1292438233=%20%20Due%20to%20the%20high%20computational%20load%20of%20modern%20numerical%20simulation%2C%20there%20is%20a%0Ademand%20for%20approaches%20that%20would%20reduce%20the%20size%20of%20discrete%20problems%20while%0Akeeping%20the%20accuracy%20reasonable.%20In%20this%20work%2C%20we%20present%20an%20original%20algorithm%0Ato%20coarsen%20an%20unstructured%20grid%20based%20on%20the%20concepts%20of%20differentiable%0Aphysics.%20We%20achieve%20this%20by%20employing%20k-means%20clustering%2C%20autodifferentiation%0Aand%20stochastic%20minimization%20algorithms.%20We%20demonstrate%20performance%20of%20the%0Adesigned%20algorithm%20on%20two%20PDEs%3A%20a%20linear%20parabolic%20equation%20which%20governs%0Aslightly%20compressible%20fluid%20flow%20in%20porous%20media%20and%20the%20wave%20equation.%20Our%0Aresults%20show%20that%20in%20the%20considered%20scenarios%2C%20we%20reduced%20the%20number%20of%20grid%0Apoints%20up%20to%2010%20times%20while%20preserving%20the%20modeled%20variable%20dynamics%20in%20the%0Apoints%20of%20interest.%20The%20proposed%20approach%20can%20be%20applied%20to%20the%20simulation%20of%0Aan%20arbitrary%20system%20described%20by%20evolutionary%20partial%20differential%20equations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18297v1&entry.124074799=Read"},
{"title": "Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via\n  Imitation Learning", "author": "David Blanco-Mulero and J\u00falia Borr\u00e0s and Carme Torras", "abstract": "  Robotic-assisted dressing has the potential to significantly aid both\npatients as well as healthcare personnel, reducing the workload and improving\nthe efficiency in clinical settings. While substantial progress has been made\nin robotic dressing assistance, prior works typically assume that garments are\nalready unfolded and ready for use. However, in medical applications gowns and\naprons are often stored in a folded configuration, requiring an additional\nunfolding step. In this paper, we introduce the pre-dressing step, the process\nof unfolding garments prior to assisted dressing. We leverage imitation\nlearning for learning three manipulation primitives, including both high and\nlow acceleration motions. In addition, we employ a visual classifier to\ncategorise the garment state as closed, partly opened, and fully opened. We\nconduct an empirical evaluation of the learned manipulation primitives as well\nas their combinations. Our results show that highly dynamic motions are not\neffective for unfolding freshly unpacked garments, where the combination of\nmotions can efficiently enhance the opening configuration.\n", "link": "http://arxiv.org/abs/2507.18436v1", "date": "2025-07-24", "relevancy": 2.3143, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6065}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5649}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Pre-Dressing%20Step%3A%20Unfolding%20Medical%20Garments%20Via%0A%20%20Imitation%20Learning&body=Title%3A%20Evaluating%20the%20Pre-Dressing%20Step%3A%20Unfolding%20Medical%20Garments%20Via%0A%20%20Imitation%20Learning%0AAuthor%3A%20David%20Blanco-Mulero%20and%20J%C3%BAlia%20Borr%C3%A0s%20and%20Carme%20Torras%0AAbstract%3A%20%20%20Robotic-assisted%20dressing%20has%20the%20potential%20to%20significantly%20aid%20both%0Apatients%20as%20well%20as%20healthcare%20personnel%2C%20reducing%20the%20workload%20and%20improving%0Athe%20efficiency%20in%20clinical%20settings.%20While%20substantial%20progress%20has%20been%20made%0Ain%20robotic%20dressing%20assistance%2C%20prior%20works%20typically%20assume%20that%20garments%20are%0Aalready%20unfolded%20and%20ready%20for%20use.%20However%2C%20in%20medical%20applications%20gowns%20and%0Aaprons%20are%20often%20stored%20in%20a%20folded%20configuration%2C%20requiring%20an%20additional%0Aunfolding%20step.%20In%20this%20paper%2C%20we%20introduce%20the%20pre-dressing%20step%2C%20the%20process%0Aof%20unfolding%20garments%20prior%20to%20assisted%20dressing.%20We%20leverage%20imitation%0Alearning%20for%20learning%20three%20manipulation%20primitives%2C%20including%20both%20high%20and%0Alow%20acceleration%20motions.%20In%20addition%2C%20we%20employ%20a%20visual%20classifier%20to%0Acategorise%20the%20garment%20state%20as%20closed%2C%20partly%20opened%2C%20and%20fully%20opened.%20We%0Aconduct%20an%20empirical%20evaluation%20of%20the%20learned%20manipulation%20primitives%20as%20well%0Aas%20their%20combinations.%20Our%20results%20show%20that%20highly%20dynamic%20motions%20are%20not%0Aeffective%20for%20unfolding%20freshly%20unpacked%20garments%2C%20where%20the%20combination%20of%0Amotions%20can%20efficiently%20enhance%20the%20opening%20configuration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Pre-Dressing%2520Step%253A%2520Unfolding%2520Medical%2520Garments%2520Via%250A%2520%2520Imitation%2520Learning%26entry.906535625%3DDavid%2520Blanco-Mulero%2520and%2520J%25C3%25BAlia%2520Borr%25C3%25A0s%2520and%2520Carme%2520Torras%26entry.1292438233%3D%2520%2520Robotic-assisted%2520dressing%2520has%2520the%2520potential%2520to%2520significantly%2520aid%2520both%250Apatients%2520as%2520well%2520as%2520healthcare%2520personnel%252C%2520reducing%2520the%2520workload%2520and%2520improving%250Athe%2520efficiency%2520in%2520clinical%2520settings.%2520While%2520substantial%2520progress%2520has%2520been%2520made%250Ain%2520robotic%2520dressing%2520assistance%252C%2520prior%2520works%2520typically%2520assume%2520that%2520garments%2520are%250Aalready%2520unfolded%2520and%2520ready%2520for%2520use.%2520However%252C%2520in%2520medical%2520applications%2520gowns%2520and%250Aaprons%2520are%2520often%2520stored%2520in%2520a%2520folded%2520configuration%252C%2520requiring%2520an%2520additional%250Aunfolding%2520step.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520pre-dressing%2520step%252C%2520the%2520process%250Aof%2520unfolding%2520garments%2520prior%2520to%2520assisted%2520dressing.%2520We%2520leverage%2520imitation%250Alearning%2520for%2520learning%2520three%2520manipulation%2520primitives%252C%2520including%2520both%2520high%2520and%250Alow%2520acceleration%2520motions.%2520In%2520addition%252C%2520we%2520employ%2520a%2520visual%2520classifier%2520to%250Acategorise%2520the%2520garment%2520state%2520as%2520closed%252C%2520partly%2520opened%252C%2520and%2520fully%2520opened.%2520We%250Aconduct%2520an%2520empirical%2520evaluation%2520of%2520the%2520learned%2520manipulation%2520primitives%2520as%2520well%250Aas%2520their%2520combinations.%2520Our%2520results%2520show%2520that%2520highly%2520dynamic%2520motions%2520are%2520not%250Aeffective%2520for%2520unfolding%2520freshly%2520unpacked%2520garments%252C%2520where%2520the%2520combination%2520of%250Amotions%2520can%2520efficiently%2520enhance%2520the%2520opening%2520configuration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Pre-Dressing%20Step%3A%20Unfolding%20Medical%20Garments%20Via%0A%20%20Imitation%20Learning&entry.906535625=David%20Blanco-Mulero%20and%20J%C3%BAlia%20Borr%C3%A0s%20and%20Carme%20Torras&entry.1292438233=%20%20Robotic-assisted%20dressing%20has%20the%20potential%20to%20significantly%20aid%20both%0Apatients%20as%20well%20as%20healthcare%20personnel%2C%20reducing%20the%20workload%20and%20improving%0Athe%20efficiency%20in%20clinical%20settings.%20While%20substantial%20progress%20has%20been%20made%0Ain%20robotic%20dressing%20assistance%2C%20prior%20works%20typically%20assume%20that%20garments%20are%0Aalready%20unfolded%20and%20ready%20for%20use.%20However%2C%20in%20medical%20applications%20gowns%20and%0Aaprons%20are%20often%20stored%20in%20a%20folded%20configuration%2C%20requiring%20an%20additional%0Aunfolding%20step.%20In%20this%20paper%2C%20we%20introduce%20the%20pre-dressing%20step%2C%20the%20process%0Aof%20unfolding%20garments%20prior%20to%20assisted%20dressing.%20We%20leverage%20imitation%0Alearning%20for%20learning%20three%20manipulation%20primitives%2C%20including%20both%20high%20and%0Alow%20acceleration%20motions.%20In%20addition%2C%20we%20employ%20a%20visual%20classifier%20to%0Acategorise%20the%20garment%20state%20as%20closed%2C%20partly%20opened%2C%20and%20fully%20opened.%20We%0Aconduct%20an%20empirical%20evaluation%20of%20the%20learned%20manipulation%20primitives%20as%20well%0Aas%20their%20combinations.%20Our%20results%20show%20that%20highly%20dynamic%20motions%20are%20not%0Aeffective%20for%20unfolding%20freshly%20unpacked%20garments%2C%20where%20the%20combination%20of%0Amotions%20can%20efficiently%20enhance%20the%20opening%20configuration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18436v1&entry.124074799=Read"},
{"title": "A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration", "author": "Daniil Morozov and Reuben Dorent and Nazim Haouchine", "abstract": "  Intraoperative registration of real-time ultrasound (iUS) to preoperative\nMagnetic Resonance Imaging (MRI) remains an unsolved problem due to severe\nmodality-specific differences in appearance, resolution, and field-of-view. To\naddress this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS\nmatching and registration. Our approach employs a patient-specific\nmatching-by-synthesis approach, generating synthetic iUS volumes from\npreoperative MRI. This enables supervised contrastive training to learn a\nshared descriptor space.\n  A probabilistic keypoint detection strategy is then employed to identify\nanatomically salient and modality-consistent locations. During training, a\ncurriculum-based triplet loss with dynamic hard negative mining is used to\nlearn descriptors that are i) robust to iUS artifacts such as speckle noise and\nlimited coverage, and ii) rotation-invariant . At inference, the method detects\nkeypoints in MR and real iUS images and identifies sparse matches, which are\nthen used to perform rigid registration. Our approach is evaluated using 3D\nMRI-iUS pairs from the ReMIND dataset. Experiments show that our approach\noutperforms state-of-the-art keypoint matching methods across 11 patients, with\nan average precision of $69.8\\%$. For image registration, our method achieves a\ncompetitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg\nbenchmark.\n  Compared to existing iUS-MR registration approach, our framework is\ninterpretable, requires no manual initialization, and shows robustness to iUS\nfield-of-view variation. Code is available at\nhttps://github.com/morozovdd/CrossKEY.\n", "link": "http://arxiv.org/abs/2507.18551v1", "date": "2025-07-24", "relevancy": 2.3129, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5387}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%203D%20Cross-modal%20Keypoint%20Descriptor%20for%20MR-US%20Matching%20and%20Registration&body=Title%3A%20A%203D%20Cross-modal%20Keypoint%20Descriptor%20for%20MR-US%20Matching%20and%20Registration%0AAuthor%3A%20Daniil%20Morozov%20and%20Reuben%20Dorent%20and%20Nazim%20Haouchine%0AAbstract%3A%20%20%20Intraoperative%20registration%20of%20real-time%20ultrasound%20%28iUS%29%20to%20preoperative%0AMagnetic%20Resonance%20Imaging%20%28MRI%29%20remains%20an%20unsolved%20problem%20due%20to%20severe%0Amodality-specific%20differences%20in%20appearance%2C%20resolution%2C%20and%20field-of-view.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%203D%20cross-modal%20keypoint%20descriptor%20for%20MRI-iUS%0Amatching%20and%20registration.%20Our%20approach%20employs%20a%20patient-specific%0Amatching-by-synthesis%20approach%2C%20generating%20synthetic%20iUS%20volumes%20from%0Apreoperative%20MRI.%20This%20enables%20supervised%20contrastive%20training%20to%20learn%20a%0Ashared%20descriptor%20space.%0A%20%20A%20probabilistic%20keypoint%20detection%20strategy%20is%20then%20employed%20to%20identify%0Aanatomically%20salient%20and%20modality-consistent%20locations.%20During%20training%2C%20a%0Acurriculum-based%20triplet%20loss%20with%20dynamic%20hard%20negative%20mining%20is%20used%20to%0Alearn%20descriptors%20that%20are%20i%29%20robust%20to%20iUS%20artifacts%20such%20as%20speckle%20noise%20and%0Alimited%20coverage%2C%20and%20ii%29%20rotation-invariant%20.%20At%20inference%2C%20the%20method%20detects%0Akeypoints%20in%20MR%20and%20real%20iUS%20images%20and%20identifies%20sparse%20matches%2C%20which%20are%0Athen%20used%20to%20perform%20rigid%20registration.%20Our%20approach%20is%20evaluated%20using%203D%0AMRI-iUS%20pairs%20from%20the%20ReMIND%20dataset.%20Experiments%20show%20that%20our%20approach%0Aoutperforms%20state-of-the-art%20keypoint%20matching%20methods%20across%2011%20patients%2C%20with%0Aan%20average%20precision%20of%20%2469.8%5C%25%24.%20For%20image%20registration%2C%20our%20method%20achieves%20a%0Acompetitive%20mean%20Target%20Registration%20Error%20of%202.39%20mm%20on%20the%20ReMIND2Reg%0Abenchmark.%0A%20%20Compared%20to%20existing%20iUS-MR%20registration%20approach%2C%20our%20framework%20is%0Ainterpretable%2C%20requires%20no%20manual%20initialization%2C%20and%20shows%20robustness%20to%20iUS%0Afield-of-view%20variation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/morozovdd/CrossKEY.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%25203D%2520Cross-modal%2520Keypoint%2520Descriptor%2520for%2520MR-US%2520Matching%2520and%2520Registration%26entry.906535625%3DDaniil%2520Morozov%2520and%2520Reuben%2520Dorent%2520and%2520Nazim%2520Haouchine%26entry.1292438233%3D%2520%2520Intraoperative%2520registration%2520of%2520real-time%2520ultrasound%2520%2528iUS%2529%2520to%2520preoperative%250AMagnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520remains%2520an%2520unsolved%2520problem%2520due%2520to%2520severe%250Amodality-specific%2520differences%2520in%2520appearance%252C%2520resolution%252C%2520and%2520field-of-view.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520novel%25203D%2520cross-modal%2520keypoint%2520descriptor%2520for%2520MRI-iUS%250Amatching%2520and%2520registration.%2520Our%2520approach%2520employs%2520a%2520patient-specific%250Amatching-by-synthesis%2520approach%252C%2520generating%2520synthetic%2520iUS%2520volumes%2520from%250Apreoperative%2520MRI.%2520This%2520enables%2520supervised%2520contrastive%2520training%2520to%2520learn%2520a%250Ashared%2520descriptor%2520space.%250A%2520%2520A%2520probabilistic%2520keypoint%2520detection%2520strategy%2520is%2520then%2520employed%2520to%2520identify%250Aanatomically%2520salient%2520and%2520modality-consistent%2520locations.%2520During%2520training%252C%2520a%250Acurriculum-based%2520triplet%2520loss%2520with%2520dynamic%2520hard%2520negative%2520mining%2520is%2520used%2520to%250Alearn%2520descriptors%2520that%2520are%2520i%2529%2520robust%2520to%2520iUS%2520artifacts%2520such%2520as%2520speckle%2520noise%2520and%250Alimited%2520coverage%252C%2520and%2520ii%2529%2520rotation-invariant%2520.%2520At%2520inference%252C%2520the%2520method%2520detects%250Akeypoints%2520in%2520MR%2520and%2520real%2520iUS%2520images%2520and%2520identifies%2520sparse%2520matches%252C%2520which%2520are%250Athen%2520used%2520to%2520perform%2520rigid%2520registration.%2520Our%2520approach%2520is%2520evaluated%2520using%25203D%250AMRI-iUS%2520pairs%2520from%2520the%2520ReMIND%2520dataset.%2520Experiments%2520show%2520that%2520our%2520approach%250Aoutperforms%2520state-of-the-art%2520keypoint%2520matching%2520methods%2520across%252011%2520patients%252C%2520with%250Aan%2520average%2520precision%2520of%2520%252469.8%255C%2525%2524.%2520For%2520image%2520registration%252C%2520our%2520method%2520achieves%2520a%250Acompetitive%2520mean%2520Target%2520Registration%2520Error%2520of%25202.39%2520mm%2520on%2520the%2520ReMIND2Reg%250Abenchmark.%250A%2520%2520Compared%2520to%2520existing%2520iUS-MR%2520registration%2520approach%252C%2520our%2520framework%2520is%250Ainterpretable%252C%2520requires%2520no%2520manual%2520initialization%252C%2520and%2520shows%2520robustness%2520to%2520iUS%250Afield-of-view%2520variation.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/morozovdd/CrossKEY.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%203D%20Cross-modal%20Keypoint%20Descriptor%20for%20MR-US%20Matching%20and%20Registration&entry.906535625=Daniil%20Morozov%20and%20Reuben%20Dorent%20and%20Nazim%20Haouchine&entry.1292438233=%20%20Intraoperative%20registration%20of%20real-time%20ultrasound%20%28iUS%29%20to%20preoperative%0AMagnetic%20Resonance%20Imaging%20%28MRI%29%20remains%20an%20unsolved%20problem%20due%20to%20severe%0Amodality-specific%20differences%20in%20appearance%2C%20resolution%2C%20and%20field-of-view.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%203D%20cross-modal%20keypoint%20descriptor%20for%20MRI-iUS%0Amatching%20and%20registration.%20Our%20approach%20employs%20a%20patient-specific%0Amatching-by-synthesis%20approach%2C%20generating%20synthetic%20iUS%20volumes%20from%0Apreoperative%20MRI.%20This%20enables%20supervised%20contrastive%20training%20to%20learn%20a%0Ashared%20descriptor%20space.%0A%20%20A%20probabilistic%20keypoint%20detection%20strategy%20is%20then%20employed%20to%20identify%0Aanatomically%20salient%20and%20modality-consistent%20locations.%20During%20training%2C%20a%0Acurriculum-based%20triplet%20loss%20with%20dynamic%20hard%20negative%20mining%20is%20used%20to%0Alearn%20descriptors%20that%20are%20i%29%20robust%20to%20iUS%20artifacts%20such%20as%20speckle%20noise%20and%0Alimited%20coverage%2C%20and%20ii%29%20rotation-invariant%20.%20At%20inference%2C%20the%20method%20detects%0Akeypoints%20in%20MR%20and%20real%20iUS%20images%20and%20identifies%20sparse%20matches%2C%20which%20are%0Athen%20used%20to%20perform%20rigid%20registration.%20Our%20approach%20is%20evaluated%20using%203D%0AMRI-iUS%20pairs%20from%20the%20ReMIND%20dataset.%20Experiments%20show%20that%20our%20approach%0Aoutperforms%20state-of-the-art%20keypoint%20matching%20methods%20across%2011%20patients%2C%20with%0Aan%20average%20precision%20of%20%2469.8%5C%25%24.%20For%20image%20registration%2C%20our%20method%20achieves%20a%0Acompetitive%20mean%20Target%20Registration%20Error%20of%202.39%20mm%20on%20the%20ReMIND2Reg%0Abenchmark.%0A%20%20Compared%20to%20existing%20iUS-MR%20registration%20approach%2C%20our%20framework%20is%0Ainterpretable%2C%20requires%20no%20manual%20initialization%2C%20and%20shows%20robustness%20to%20iUS%0Afield-of-view%20variation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/morozovdd/CrossKEY.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18551v1&entry.124074799=Read"},
{"title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "author": "Maciej K. Wozniak and Lianhang Liu and Yixi Cai and Patric Jensfelt", "abstract": "  While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.\n", "link": "http://arxiv.org/abs/2507.17596v2", "date": "2025-07-24", "relevancy": 2.301, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.574}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIX%3A%20Learning%20to%20Plan%20from%20Raw%20Pixels%20for%20End-to-End%20Autonomous%20Driving&body=Title%3A%20PRIX%3A%20Learning%20to%20Plan%20from%20Raw%20Pixels%20for%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Maciej%20K.%20Wozniak%20and%20Lianhang%20Liu%20and%20Yixi%20Cai%20and%20Patric%20Jensfelt%0AAbstract%3A%20%20%20While%20end-to-end%20autonomous%20driving%20models%20show%20promising%20results%2C%20their%0Apractical%20deployment%20is%20often%20hindered%20by%20large%20model%20sizes%2C%20a%20reliance%20on%0Aexpensive%20LiDAR%20sensors%20and%20computationally%20intensive%20BEV%20feature%0Arepresentations.%20This%20limits%20their%20scalability%2C%20especially%20for%20mass-market%0Avehicles%20equipped%20only%20with%20cameras.%20To%20address%20these%20challenges%2C%20we%20propose%0APRIX%20%28Plan%20from%20Raw%20Pixels%29.%20Our%20novel%20and%20efficient%20end-to-end%20driving%0Aarchitecture%20operates%20using%20only%20camera%20data%2C%20without%20explicit%20BEV%0Arepresentation%20and%20forgoing%20the%20need%20for%20LiDAR.%20PRIX%20leverages%20a%20visual%20feature%0Aextractor%20coupled%20with%20a%20generative%20planning%20head%20to%20predict%20safe%20trajectories%0Afrom%20raw%20pixel%20inputs%20directly.%20A%20core%20component%20of%20our%20architecture%20is%20the%0AContext-aware%20Recalibration%20Transformer%20%28CaRT%29%2C%20a%20novel%20module%20designed%20to%0Aeffectively%20enhance%20multi-level%20visual%20features%20for%20more%20robust%20planning.%20We%0Ademonstrate%20through%20comprehensive%20experiments%20that%20PRIX%20achieves%0Astate-of-the-art%20performance%20on%20the%20NavSim%20and%20nuScenes%20benchmarks%2C%20matching%0Athe%20capabilities%20of%20larger%2C%20multimodal%20diffusion%20planners%20while%20being%0Asignificantly%20more%20efficient%20in%20terms%20of%20inference%20speed%20and%20model%20size%2C%20making%0Ait%20a%20practical%20solution%20for%20real-world%20deployment.%20Our%20work%20is%20open-source%20and%0Athe%20code%20will%20be%20at%20https%3A//maxiuw.github.io/prix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIX%253A%2520Learning%2520to%2520Plan%2520from%2520Raw%2520Pixels%2520for%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DMaciej%2520K.%2520Wozniak%2520and%2520Lianhang%2520Liu%2520and%2520Yixi%2520Cai%2520and%2520Patric%2520Jensfelt%26entry.1292438233%3D%2520%2520While%2520end-to-end%2520autonomous%2520driving%2520models%2520show%2520promising%2520results%252C%2520their%250Apractical%2520deployment%2520is%2520often%2520hindered%2520by%2520large%2520model%2520sizes%252C%2520a%2520reliance%2520on%250Aexpensive%2520LiDAR%2520sensors%2520and%2520computationally%2520intensive%2520BEV%2520feature%250Arepresentations.%2520This%2520limits%2520their%2520scalability%252C%2520especially%2520for%2520mass-market%250Avehicles%2520equipped%2520only%2520with%2520cameras.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250APRIX%2520%2528Plan%2520from%2520Raw%2520Pixels%2529.%2520Our%2520novel%2520and%2520efficient%2520end-to-end%2520driving%250Aarchitecture%2520operates%2520using%2520only%2520camera%2520data%252C%2520without%2520explicit%2520BEV%250Arepresentation%2520and%2520forgoing%2520the%2520need%2520for%2520LiDAR.%2520PRIX%2520leverages%2520a%2520visual%2520feature%250Aextractor%2520coupled%2520with%2520a%2520generative%2520planning%2520head%2520to%2520predict%2520safe%2520trajectories%250Afrom%2520raw%2520pixel%2520inputs%2520directly.%2520A%2520core%2520component%2520of%2520our%2520architecture%2520is%2520the%250AContext-aware%2520Recalibration%2520Transformer%2520%2528CaRT%2529%252C%2520a%2520novel%2520module%2520designed%2520to%250Aeffectively%2520enhance%2520multi-level%2520visual%2520features%2520for%2520more%2520robust%2520planning.%2520We%250Ademonstrate%2520through%2520comprehensive%2520experiments%2520that%2520PRIX%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520NavSim%2520and%2520nuScenes%2520benchmarks%252C%2520matching%250Athe%2520capabilities%2520of%2520larger%252C%2520multimodal%2520diffusion%2520planners%2520while%2520being%250Asignificantly%2520more%2520efficient%2520in%2520terms%2520of%2520inference%2520speed%2520and%2520model%2520size%252C%2520making%250Ait%2520a%2520practical%2520solution%2520for%2520real-world%2520deployment.%2520Our%2520work%2520is%2520open-source%2520and%250Athe%2520code%2520will%2520be%2520at%2520https%253A//maxiuw.github.io/prix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIX%3A%20Learning%20to%20Plan%20from%20Raw%20Pixels%20for%20End-to-End%20Autonomous%20Driving&entry.906535625=Maciej%20K.%20Wozniak%20and%20Lianhang%20Liu%20and%20Yixi%20Cai%20and%20Patric%20Jensfelt&entry.1292438233=%20%20While%20end-to-end%20autonomous%20driving%20models%20show%20promising%20results%2C%20their%0Apractical%20deployment%20is%20often%20hindered%20by%20large%20model%20sizes%2C%20a%20reliance%20on%0Aexpensive%20LiDAR%20sensors%20and%20computationally%20intensive%20BEV%20feature%0Arepresentations.%20This%20limits%20their%20scalability%2C%20especially%20for%20mass-market%0Avehicles%20equipped%20only%20with%20cameras.%20To%20address%20these%20challenges%2C%20we%20propose%0APRIX%20%28Plan%20from%20Raw%20Pixels%29.%20Our%20novel%20and%20efficient%20end-to-end%20driving%0Aarchitecture%20operates%20using%20only%20camera%20data%2C%20without%20explicit%20BEV%0Arepresentation%20and%20forgoing%20the%20need%20for%20LiDAR.%20PRIX%20leverages%20a%20visual%20feature%0Aextractor%20coupled%20with%20a%20generative%20planning%20head%20to%20predict%20safe%20trajectories%0Afrom%20raw%20pixel%20inputs%20directly.%20A%20core%20component%20of%20our%20architecture%20is%20the%0AContext-aware%20Recalibration%20Transformer%20%28CaRT%29%2C%20a%20novel%20module%20designed%20to%0Aeffectively%20enhance%20multi-level%20visual%20features%20for%20more%20robust%20planning.%20We%0Ademonstrate%20through%20comprehensive%20experiments%20that%20PRIX%20achieves%0Astate-of-the-art%20performance%20on%20the%20NavSim%20and%20nuScenes%20benchmarks%2C%20matching%0Athe%20capabilities%20of%20larger%2C%20multimodal%20diffusion%20planners%20while%20being%0Asignificantly%20more%20efficient%20in%20terms%20of%20inference%20speed%20and%20model%20size%2C%20making%0Ait%20a%20practical%20solution%20for%20real-world%20deployment.%20Our%20work%20is%20open-source%20and%0Athe%20code%20will%20be%20at%20https%3A//maxiuw.github.io/prix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17596v2&entry.124074799=Read"},
{"title": "Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual\n  Prompts", "author": "Pasquale De Marinis and Nicola Fanelli and Raffaele Scaringi and Emanuele Colonna and Giuseppe Fiameni and Gennaro Vessio and Giovanna Castellano", "abstract": "  We present Label Anything, an innovative neural network architecture designed\nfor few-shot semantic segmentation (FSS) that demonstrates remarkable\ngeneralizability across multiple classes with minimal examples required per\nclass. Diverging from traditional FSS methods that predominantly rely on masks\nfor annotating support images, Label Anything introduces varied visual prompts\n-- points, bounding boxes, and masks -- thereby enhancing the framework's\nversatility and adaptability. Unique to our approach, Label Anything is\nengineered for end-to-end training across multi-class FSS scenarios,\nefficiently learning from diverse support set configurations without\nretraining. This approach enables a \"universal\" application to various FSS\nchallenges, ranging from $1$-way $1$-shot to complex $N$-way $K$-shot\nconfigurations while remaining agnostic to the specific number of class\nexamples. This innovative training strategy reduces computational requirements\nand substantially improves the model's adaptability and generalization across\ndiverse segmentation tasks. Our comprehensive experimental validation,\nparticularly achieving state-of-the-art results on the COCO-$20^i$ benchmark,\nunderscores Label Anything's robust generalization and flexibility. The source\ncode is publicly available at: https://github.com/pasqualedem/LabelAnything.\n", "link": "http://arxiv.org/abs/2407.02075v2", "date": "2025-07-24", "relevancy": 2.2944, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5615}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label%20Anything%3A%20Multi-Class%20Few-Shot%20Semantic%20Segmentation%20with%20Visual%0A%20%20Prompts&body=Title%3A%20Label%20Anything%3A%20Multi-Class%20Few-Shot%20Semantic%20Segmentation%20with%20Visual%0A%20%20Prompts%0AAuthor%3A%20Pasquale%20De%20Marinis%20and%20Nicola%20Fanelli%20and%20Raffaele%20Scaringi%20and%20Emanuele%20Colonna%20and%20Giuseppe%20Fiameni%20and%20Gennaro%20Vessio%20and%20Giovanna%20Castellano%0AAbstract%3A%20%20%20We%20present%20Label%20Anything%2C%20an%20innovative%20neural%20network%20architecture%20designed%0Afor%20few-shot%20semantic%20segmentation%20%28FSS%29%20that%20demonstrates%20remarkable%0Ageneralizability%20across%20multiple%20classes%20with%20minimal%20examples%20required%20per%0Aclass.%20Diverging%20from%20traditional%20FSS%20methods%20that%20predominantly%20rely%20on%20masks%0Afor%20annotating%20support%20images%2C%20Label%20Anything%20introduces%20varied%20visual%20prompts%0A--%20points%2C%20bounding%20boxes%2C%20and%20masks%20--%20thereby%20enhancing%20the%20framework%27s%0Aversatility%20and%20adaptability.%20Unique%20to%20our%20approach%2C%20Label%20Anything%20is%0Aengineered%20for%20end-to-end%20training%20across%20multi-class%20FSS%20scenarios%2C%0Aefficiently%20learning%20from%20diverse%20support%20set%20configurations%20without%0Aretraining.%20This%20approach%20enables%20a%20%22universal%22%20application%20to%20various%20FSS%0Achallenges%2C%20ranging%20from%20%241%24-way%20%241%24-shot%20to%20complex%20%24N%24-way%20%24K%24-shot%0Aconfigurations%20while%20remaining%20agnostic%20to%20the%20specific%20number%20of%20class%0Aexamples.%20This%20innovative%20training%20strategy%20reduces%20computational%20requirements%0Aand%20substantially%20improves%20the%20model%27s%20adaptability%20and%20generalization%20across%0Adiverse%20segmentation%20tasks.%20Our%20comprehensive%20experimental%20validation%2C%0Aparticularly%20achieving%20state-of-the-art%20results%20on%20the%20COCO-%2420%5Ei%24%20benchmark%2C%0Aunderscores%20Label%20Anything%27s%20robust%20generalization%20and%20flexibility.%20The%20source%0Acode%20is%20publicly%20available%20at%3A%20https%3A//github.com/pasqualedem/LabelAnything.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel%2520Anything%253A%2520Multi-Class%2520Few-Shot%2520Semantic%2520Segmentation%2520with%2520Visual%250A%2520%2520Prompts%26entry.906535625%3DPasquale%2520De%2520Marinis%2520and%2520Nicola%2520Fanelli%2520and%2520Raffaele%2520Scaringi%2520and%2520Emanuele%2520Colonna%2520and%2520Giuseppe%2520Fiameni%2520and%2520Gennaro%2520Vessio%2520and%2520Giovanna%2520Castellano%26entry.1292438233%3D%2520%2520We%2520present%2520Label%2520Anything%252C%2520an%2520innovative%2520neural%2520network%2520architecture%2520designed%250Afor%2520few-shot%2520semantic%2520segmentation%2520%2528FSS%2529%2520that%2520demonstrates%2520remarkable%250Ageneralizability%2520across%2520multiple%2520classes%2520with%2520minimal%2520examples%2520required%2520per%250Aclass.%2520Diverging%2520from%2520traditional%2520FSS%2520methods%2520that%2520predominantly%2520rely%2520on%2520masks%250Afor%2520annotating%2520support%2520images%252C%2520Label%2520Anything%2520introduces%2520varied%2520visual%2520prompts%250A--%2520points%252C%2520bounding%2520boxes%252C%2520and%2520masks%2520--%2520thereby%2520enhancing%2520the%2520framework%2527s%250Aversatility%2520and%2520adaptability.%2520Unique%2520to%2520our%2520approach%252C%2520Label%2520Anything%2520is%250Aengineered%2520for%2520end-to-end%2520training%2520across%2520multi-class%2520FSS%2520scenarios%252C%250Aefficiently%2520learning%2520from%2520diverse%2520support%2520set%2520configurations%2520without%250Aretraining.%2520This%2520approach%2520enables%2520a%2520%2522universal%2522%2520application%2520to%2520various%2520FSS%250Achallenges%252C%2520ranging%2520from%2520%25241%2524-way%2520%25241%2524-shot%2520to%2520complex%2520%2524N%2524-way%2520%2524K%2524-shot%250Aconfigurations%2520while%2520remaining%2520agnostic%2520to%2520the%2520specific%2520number%2520of%2520class%250Aexamples.%2520This%2520innovative%2520training%2520strategy%2520reduces%2520computational%2520requirements%250Aand%2520substantially%2520improves%2520the%2520model%2527s%2520adaptability%2520and%2520generalization%2520across%250Adiverse%2520segmentation%2520tasks.%2520Our%2520comprehensive%2520experimental%2520validation%252C%250Aparticularly%2520achieving%2520state-of-the-art%2520results%2520on%2520the%2520COCO-%252420%255Ei%2524%2520benchmark%252C%250Aunderscores%2520Label%2520Anything%2527s%2520robust%2520generalization%2520and%2520flexibility.%2520The%2520source%250Acode%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/pasqualedem/LabelAnything.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Anything%3A%20Multi-Class%20Few-Shot%20Semantic%20Segmentation%20with%20Visual%0A%20%20Prompts&entry.906535625=Pasquale%20De%20Marinis%20and%20Nicola%20Fanelli%20and%20Raffaele%20Scaringi%20and%20Emanuele%20Colonna%20and%20Giuseppe%20Fiameni%20and%20Gennaro%20Vessio%20and%20Giovanna%20Castellano&entry.1292438233=%20%20We%20present%20Label%20Anything%2C%20an%20innovative%20neural%20network%20architecture%20designed%0Afor%20few-shot%20semantic%20segmentation%20%28FSS%29%20that%20demonstrates%20remarkable%0Ageneralizability%20across%20multiple%20classes%20with%20minimal%20examples%20required%20per%0Aclass.%20Diverging%20from%20traditional%20FSS%20methods%20that%20predominantly%20rely%20on%20masks%0Afor%20annotating%20support%20images%2C%20Label%20Anything%20introduces%20varied%20visual%20prompts%0A--%20points%2C%20bounding%20boxes%2C%20and%20masks%20--%20thereby%20enhancing%20the%20framework%27s%0Aversatility%20and%20adaptability.%20Unique%20to%20our%20approach%2C%20Label%20Anything%20is%0Aengineered%20for%20end-to-end%20training%20across%20multi-class%20FSS%20scenarios%2C%0Aefficiently%20learning%20from%20diverse%20support%20set%20configurations%20without%0Aretraining.%20This%20approach%20enables%20a%20%22universal%22%20application%20to%20various%20FSS%0Achallenges%2C%20ranging%20from%20%241%24-way%20%241%24-shot%20to%20complex%20%24N%24-way%20%24K%24-shot%0Aconfigurations%20while%20remaining%20agnostic%20to%20the%20specific%20number%20of%20class%0Aexamples.%20This%20innovative%20training%20strategy%20reduces%20computational%20requirements%0Aand%20substantially%20improves%20the%20model%27s%20adaptability%20and%20generalization%20across%0Adiverse%20segmentation%20tasks.%20Our%20comprehensive%20experimental%20validation%2C%0Aparticularly%20achieving%20state-of-the-art%20results%20on%20the%20COCO-%2420%5Ei%24%20benchmark%2C%0Aunderscores%20Label%20Anything%27s%20robust%20generalization%20and%20flexibility.%20The%20source%0Acode%20is%20publicly%20available%20at%3A%20https%3A//github.com/pasqualedem/LabelAnything.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02075v2&entry.124074799=Read"},
{"title": "Personalization Toolkit: Training Free Personalization of Large Vision\n  Language Models", "author": "Soroush Seifi and Vaggelis Dorovatas and Daniel Olmeda Reino and Rahaf Aljundi", "abstract": "  Personalization of Large Vision-Language Models (LVLMs) involves customizing\nmodels to recognize specific users and object instances, and to generate\ncontextually tailored responses. Existing approaches typically rely on\ntime-consuming test-time training for each user or object, making them\nimpractical for real-world deployment, a limitation reflected in current\npersonalization benchmarks, which are focused on object-centric, single-concept\nevaluations. In this paper, we present a novel training-free approach to LVLM\npersonalization and introduce a comprehensive real-world benchmark designed to\nrigorously evaluate various aspects of the personalization task. Our method\nleverages pre-trained vision foundation models to extract distinctive features,\napplies retrieval-augmented generation (RAG) techniques to identify instances\nwithin visual inputs, and employs visual prompting strategies to guide model\noutputs. Our model-agnostic vision toolkit enables efficient and flexible\nmulti-concept personalization across both images and videos, without any\nadditional training. We achieve state-of-the-art results, surpassing existing\ntraining-based methods.\n", "link": "http://arxiv.org/abs/2502.02452v3", "date": "2025-07-24", "relevancy": 2.2868, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalization%20Toolkit%3A%20Training%20Free%20Personalization%20of%20Large%20Vision%0A%20%20Language%20Models&body=Title%3A%20Personalization%20Toolkit%3A%20Training%20Free%20Personalization%20of%20Large%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Soroush%20Seifi%20and%20Vaggelis%20Dorovatas%20and%20Daniel%20Olmeda%20Reino%20and%20Rahaf%20Aljundi%0AAbstract%3A%20%20%20Personalization%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20involves%20customizing%0Amodels%20to%20recognize%20specific%20users%20and%20object%20instances%2C%20and%20to%20generate%0Acontextually%20tailored%20responses.%20Existing%20approaches%20typically%20rely%20on%0Atime-consuming%20test-time%20training%20for%20each%20user%20or%20object%2C%20making%20them%0Aimpractical%20for%20real-world%20deployment%2C%20a%20limitation%20reflected%20in%20current%0Apersonalization%20benchmarks%2C%20which%20are%20focused%20on%20object-centric%2C%20single-concept%0Aevaluations.%20In%20this%20paper%2C%20we%20present%20a%20novel%20training-free%20approach%20to%20LVLM%0Apersonalization%20and%20introduce%20a%20comprehensive%20real-world%20benchmark%20designed%20to%0Arigorously%20evaluate%20various%20aspects%20of%20the%20personalization%20task.%20Our%20method%0Aleverages%20pre-trained%20vision%20foundation%20models%20to%20extract%20distinctive%20features%2C%0Aapplies%20retrieval-augmented%20generation%20%28RAG%29%20techniques%20to%20identify%20instances%0Awithin%20visual%20inputs%2C%20and%20employs%20visual%20prompting%20strategies%20to%20guide%20model%0Aoutputs.%20Our%20model-agnostic%20vision%20toolkit%20enables%20efficient%20and%20flexible%0Amulti-concept%20personalization%20across%20both%20images%20and%20videos%2C%20without%20any%0Aadditional%20training.%20We%20achieve%20state-of-the-art%20results%2C%20surpassing%20existing%0Atraining-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02452v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalization%2520Toolkit%253A%2520Training%2520Free%2520Personalization%2520of%2520Large%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DSoroush%2520Seifi%2520and%2520Vaggelis%2520Dorovatas%2520and%2520Daniel%2520Olmeda%2520Reino%2520and%2520Rahaf%2520Aljundi%26entry.1292438233%3D%2520%2520Personalization%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520involves%2520customizing%250Amodels%2520to%2520recognize%2520specific%2520users%2520and%2520object%2520instances%252C%2520and%2520to%2520generate%250Acontextually%2520tailored%2520responses.%2520Existing%2520approaches%2520typically%2520rely%2520on%250Atime-consuming%2520test-time%2520training%2520for%2520each%2520user%2520or%2520object%252C%2520making%2520them%250Aimpractical%2520for%2520real-world%2520deployment%252C%2520a%2520limitation%2520reflected%2520in%2520current%250Apersonalization%2520benchmarks%252C%2520which%2520are%2520focused%2520on%2520object-centric%252C%2520single-concept%250Aevaluations.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520training-free%2520approach%2520to%2520LVLM%250Apersonalization%2520and%2520introduce%2520a%2520comprehensive%2520real-world%2520benchmark%2520designed%2520to%250Arigorously%2520evaluate%2520various%2520aspects%2520of%2520the%2520personalization%2520task.%2520Our%2520method%250Aleverages%2520pre-trained%2520vision%2520foundation%2520models%2520to%2520extract%2520distinctive%2520features%252C%250Aapplies%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520techniques%2520to%2520identify%2520instances%250Awithin%2520visual%2520inputs%252C%2520and%2520employs%2520visual%2520prompting%2520strategies%2520to%2520guide%2520model%250Aoutputs.%2520Our%2520model-agnostic%2520vision%2520toolkit%2520enables%2520efficient%2520and%2520flexible%250Amulti-concept%2520personalization%2520across%2520both%2520images%2520and%2520videos%252C%2520without%2520any%250Aadditional%2520training.%2520We%2520achieve%2520state-of-the-art%2520results%252C%2520surpassing%2520existing%250Atraining-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02452v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalization%20Toolkit%3A%20Training%20Free%20Personalization%20of%20Large%20Vision%0A%20%20Language%20Models&entry.906535625=Soroush%20Seifi%20and%20Vaggelis%20Dorovatas%20and%20Daniel%20Olmeda%20Reino%20and%20Rahaf%20Aljundi&entry.1292438233=%20%20Personalization%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20involves%20customizing%0Amodels%20to%20recognize%20specific%20users%20and%20object%20instances%2C%20and%20to%20generate%0Acontextually%20tailored%20responses.%20Existing%20approaches%20typically%20rely%20on%0Atime-consuming%20test-time%20training%20for%20each%20user%20or%20object%2C%20making%20them%0Aimpractical%20for%20real-world%20deployment%2C%20a%20limitation%20reflected%20in%20current%0Apersonalization%20benchmarks%2C%20which%20are%20focused%20on%20object-centric%2C%20single-concept%0Aevaluations.%20In%20this%20paper%2C%20we%20present%20a%20novel%20training-free%20approach%20to%20LVLM%0Apersonalization%20and%20introduce%20a%20comprehensive%20real-world%20benchmark%20designed%20to%0Arigorously%20evaluate%20various%20aspects%20of%20the%20personalization%20task.%20Our%20method%0Aleverages%20pre-trained%20vision%20foundation%20models%20to%20extract%20distinctive%20features%2C%0Aapplies%20retrieval-augmented%20generation%20%28RAG%29%20techniques%20to%20identify%20instances%0Awithin%20visual%20inputs%2C%20and%20employs%20visual%20prompting%20strategies%20to%20guide%20model%0Aoutputs.%20Our%20model-agnostic%20vision%20toolkit%20enables%20efficient%20and%20flexible%0Amulti-concept%20personalization%20across%20both%20images%20and%20videos%2C%20without%20any%0Aadditional%20training.%20We%20achieve%20state-of-the-art%20results%2C%20surpassing%20existing%0Atraining-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02452v3&entry.124074799=Read"},
{"title": "DiagR1: A Vision-Language Model Trained via Reinforcement Learning for\n  Digestive Pathology Diagnosis", "author": "Minxi Ouyang and Lianghui Zhu and Yaqing Bao and Qiang Huang and Jingli Ouyang and Tian Guan and Xitong Ling and Jiawen Li and Song Duan and Wenbin Dai and Li Zheng and Xuemei Zhang and Yonghong He", "abstract": "  Multimodal large models have shown great potential in automating pathology\nimage analysis. However, current multimodal models for gastrointestinal\npathology are constrained by both data quality and reasoning transparency:\npervasive noise and incomplete annotations in public datasets predispose vision\nlanguage models to factual hallucinations when generating diagnostic text,\nwhile the absence of explicit intermediate reasoning chains renders the outputs\ndifficult to audit and thus less trustworthy in clinical practice. To address\nthese issues, we construct a large scale gastrointestinal pathology dataset\ncontaining both microscopic descriptions and diagnostic conclusions, and\npropose a prompt argumentation strategy that incorporates lesion classification\nand anatomical site information. This design guides the model to better capture\nimage specific features and maintain semantic consistency in generation.\nFurthermore, we employ a post training pipeline that combines supervised fine\ntuning with Group Relative Policy Optimization (GRPO) to improve reasoning\nquality and output structure. Experimental results on real world pathology\nreport generation tasks demonstrate that our approach significantly outperforms\nstate of the art open source and proprietary baselines in terms of generation\nquality, structural completeness, and clinical relevance. Our solution\noutperforms state of the art models with 18.7% higher clinical relevance, 32.4%\nimproved structural completeness, and 41.2% fewer diagnostic errors,\ndemonstrating superior accuracy and clinical utility compared to existing\nsolutions.\n", "link": "http://arxiv.org/abs/2507.18433v1", "date": "2025-07-24", "relevancy": 2.2808, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5767}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiagR1%3A%20A%20Vision-Language%20Model%20Trained%20via%20Reinforcement%20Learning%20for%0A%20%20Digestive%20Pathology%20Diagnosis&body=Title%3A%20DiagR1%3A%20A%20Vision-Language%20Model%20Trained%20via%20Reinforcement%20Learning%20for%0A%20%20Digestive%20Pathology%20Diagnosis%0AAuthor%3A%20Minxi%20Ouyang%20and%20Lianghui%20Zhu%20and%20Yaqing%20Bao%20and%20Qiang%20Huang%20and%20Jingli%20Ouyang%20and%20Tian%20Guan%20and%20Xitong%20Ling%20and%20Jiawen%20Li%20and%20Song%20Duan%20and%20Wenbin%20Dai%20and%20Li%20Zheng%20and%20Xuemei%20Zhang%20and%20Yonghong%20He%0AAbstract%3A%20%20%20Multimodal%20large%20models%20have%20shown%20great%20potential%20in%20automating%20pathology%0Aimage%20analysis.%20However%2C%20current%20multimodal%20models%20for%20gastrointestinal%0Apathology%20are%20constrained%20by%20both%20data%20quality%20and%20reasoning%20transparency%3A%0Apervasive%20noise%20and%20incomplete%20annotations%20in%20public%20datasets%20predispose%20vision%0Alanguage%20models%20to%20factual%20hallucinations%20when%20generating%20diagnostic%20text%2C%0Awhile%20the%20absence%20of%20explicit%20intermediate%20reasoning%20chains%20renders%20the%20outputs%0Adifficult%20to%20audit%20and%20thus%20less%20trustworthy%20in%20clinical%20practice.%20To%20address%0Athese%20issues%2C%20we%20construct%20a%20large%20scale%20gastrointestinal%20pathology%20dataset%0Acontaining%20both%20microscopic%20descriptions%20and%20diagnostic%20conclusions%2C%20and%0Apropose%20a%20prompt%20argumentation%20strategy%20that%20incorporates%20lesion%20classification%0Aand%20anatomical%20site%20information.%20This%20design%20guides%20the%20model%20to%20better%20capture%0Aimage%20specific%20features%20and%20maintain%20semantic%20consistency%20in%20generation.%0AFurthermore%2C%20we%20employ%20a%20post%20training%20pipeline%20that%20combines%20supervised%20fine%0Atuning%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20improve%20reasoning%0Aquality%20and%20output%20structure.%20Experimental%20results%20on%20real%20world%20pathology%0Areport%20generation%20tasks%20demonstrate%20that%20our%20approach%20significantly%20outperforms%0Astate%20of%20the%20art%20open%20source%20and%20proprietary%20baselines%20in%20terms%20of%20generation%0Aquality%2C%20structural%20completeness%2C%20and%20clinical%20relevance.%20Our%20solution%0Aoutperforms%20state%20of%20the%20art%20models%20with%2018.7%25%20higher%20clinical%20relevance%2C%2032.4%25%0Aimproved%20structural%20completeness%2C%20and%2041.2%25%20fewer%20diagnostic%20errors%2C%0Ademonstrating%20superior%20accuracy%20and%20clinical%20utility%20compared%20to%20existing%0Asolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagR1%253A%2520A%2520Vision-Language%2520Model%2520Trained%2520via%2520Reinforcement%2520Learning%2520for%250A%2520%2520Digestive%2520Pathology%2520Diagnosis%26entry.906535625%3DMinxi%2520Ouyang%2520and%2520Lianghui%2520Zhu%2520and%2520Yaqing%2520Bao%2520and%2520Qiang%2520Huang%2520and%2520Jingli%2520Ouyang%2520and%2520Tian%2520Guan%2520and%2520Xitong%2520Ling%2520and%2520Jiawen%2520Li%2520and%2520Song%2520Duan%2520and%2520Wenbin%2520Dai%2520and%2520Li%2520Zheng%2520and%2520Xuemei%2520Zhang%2520and%2520Yonghong%2520He%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520models%2520have%2520shown%2520great%2520potential%2520in%2520automating%2520pathology%250Aimage%2520analysis.%2520However%252C%2520current%2520multimodal%2520models%2520for%2520gastrointestinal%250Apathology%2520are%2520constrained%2520by%2520both%2520data%2520quality%2520and%2520reasoning%2520transparency%253A%250Apervasive%2520noise%2520and%2520incomplete%2520annotations%2520in%2520public%2520datasets%2520predispose%2520vision%250Alanguage%2520models%2520to%2520factual%2520hallucinations%2520when%2520generating%2520diagnostic%2520text%252C%250Awhile%2520the%2520absence%2520of%2520explicit%2520intermediate%2520reasoning%2520chains%2520renders%2520the%2520outputs%250Adifficult%2520to%2520audit%2520and%2520thus%2520less%2520trustworthy%2520in%2520clinical%2520practice.%2520To%2520address%250Athese%2520issues%252C%2520we%2520construct%2520a%2520large%2520scale%2520gastrointestinal%2520pathology%2520dataset%250Acontaining%2520both%2520microscopic%2520descriptions%2520and%2520diagnostic%2520conclusions%252C%2520and%250Apropose%2520a%2520prompt%2520argumentation%2520strategy%2520that%2520incorporates%2520lesion%2520classification%250Aand%2520anatomical%2520site%2520information.%2520This%2520design%2520guides%2520the%2520model%2520to%2520better%2520capture%250Aimage%2520specific%2520features%2520and%2520maintain%2520semantic%2520consistency%2520in%2520generation.%250AFurthermore%252C%2520we%2520employ%2520a%2520post%2520training%2520pipeline%2520that%2520combines%2520supervised%2520fine%250Atuning%2520with%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520improve%2520reasoning%250Aquality%2520and%2520output%2520structure.%2520Experimental%2520results%2520on%2520real%2520world%2520pathology%250Areport%2520generation%2520tasks%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%250Astate%2520of%2520the%2520art%2520open%2520source%2520and%2520proprietary%2520baselines%2520in%2520terms%2520of%2520generation%250Aquality%252C%2520structural%2520completeness%252C%2520and%2520clinical%2520relevance.%2520Our%2520solution%250Aoutperforms%2520state%2520of%2520the%2520art%2520models%2520with%252018.7%2525%2520higher%2520clinical%2520relevance%252C%252032.4%2525%250Aimproved%2520structural%2520completeness%252C%2520and%252041.2%2525%2520fewer%2520diagnostic%2520errors%252C%250Ademonstrating%2520superior%2520accuracy%2520and%2520clinical%2520utility%2520compared%2520to%2520existing%250Asolutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiagR1%3A%20A%20Vision-Language%20Model%20Trained%20via%20Reinforcement%20Learning%20for%0A%20%20Digestive%20Pathology%20Diagnosis&entry.906535625=Minxi%20Ouyang%20and%20Lianghui%20Zhu%20and%20Yaqing%20Bao%20and%20Qiang%20Huang%20and%20Jingli%20Ouyang%20and%20Tian%20Guan%20and%20Xitong%20Ling%20and%20Jiawen%20Li%20and%20Song%20Duan%20and%20Wenbin%20Dai%20and%20Li%20Zheng%20and%20Xuemei%20Zhang%20and%20Yonghong%20He&entry.1292438233=%20%20Multimodal%20large%20models%20have%20shown%20great%20potential%20in%20automating%20pathology%0Aimage%20analysis.%20However%2C%20current%20multimodal%20models%20for%20gastrointestinal%0Apathology%20are%20constrained%20by%20both%20data%20quality%20and%20reasoning%20transparency%3A%0Apervasive%20noise%20and%20incomplete%20annotations%20in%20public%20datasets%20predispose%20vision%0Alanguage%20models%20to%20factual%20hallucinations%20when%20generating%20diagnostic%20text%2C%0Awhile%20the%20absence%20of%20explicit%20intermediate%20reasoning%20chains%20renders%20the%20outputs%0Adifficult%20to%20audit%20and%20thus%20less%20trustworthy%20in%20clinical%20practice.%20To%20address%0Athese%20issues%2C%20we%20construct%20a%20large%20scale%20gastrointestinal%20pathology%20dataset%0Acontaining%20both%20microscopic%20descriptions%20and%20diagnostic%20conclusions%2C%20and%0Apropose%20a%20prompt%20argumentation%20strategy%20that%20incorporates%20lesion%20classification%0Aand%20anatomical%20site%20information.%20This%20design%20guides%20the%20model%20to%20better%20capture%0Aimage%20specific%20features%20and%20maintain%20semantic%20consistency%20in%20generation.%0AFurthermore%2C%20we%20employ%20a%20post%20training%20pipeline%20that%20combines%20supervised%20fine%0Atuning%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20improve%20reasoning%0Aquality%20and%20output%20structure.%20Experimental%20results%20on%20real%20world%20pathology%0Areport%20generation%20tasks%20demonstrate%20that%20our%20approach%20significantly%20outperforms%0Astate%20of%20the%20art%20open%20source%20and%20proprietary%20baselines%20in%20terms%20of%20generation%0Aquality%2C%20structural%20completeness%2C%20and%20clinical%20relevance.%20Our%20solution%0Aoutperforms%20state%20of%20the%20art%20models%20with%2018.7%25%20higher%20clinical%20relevance%2C%2032.4%25%0Aimproved%20structural%20completeness%2C%20and%2041.2%25%20fewer%20diagnostic%20errors%2C%0Ademonstrating%20superior%20accuracy%20and%20clinical%20utility%20compared%20to%20existing%0Asolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18433v1&entry.124074799=Read"},
{"title": "Human Scanpath Prediction in Target-Present Visual Search with\n  Semantic-Foveal Bayesian Attention", "author": "Jo\u00e3o Luzio and Alexandre Bernardino and Plinio Moreno", "abstract": "  In goal-directed visual tasks, human perception is guided by both top-down\nand bottom-up cues. At the same time, foveal vision plays a crucial role in\ndirecting attention efficiently. Modern research on bio-inspired computational\nattention models has taken advantage of advancements in deep learning by\nutilizing human scanpath data to achieve new state-of-the-art performance. In\nthis work, we assess the performance of SemBA-FAST, i.e. Semantic-based\nBayesian Attention for Foveal Active visual Search Tasks, a top-down framework\ndesigned for predicting human visual attention in target-present visual search.\nSemBA-FAST integrates deep object detection with a probabilistic semantic\nfusion mechanism to generate attention maps dynamically, leveraging pre-trained\ndetectors and artificial foveation to update top-down knowledge and improve\nfixation prediction sequentially. We evaluate SemBA-FAST on the COCO-Search18\nbenchmark dataset, comparing its performance against other scanpath prediction\nmodels. Our methodology achieves fixation sequences that closely match human\nground-truth scanpaths. Notably, it surpasses baseline and other top-down\napproaches and competes, in some cases, with scanpath-informed models. These\nfindings provide valuable insights into the capabilities of semantic-foveal\nprobabilistic frameworks for human-like attention modelling, with implications\nfor real-time cognitive computing and robotics.\n", "link": "http://arxiv.org/abs/2507.18503v1", "date": "2025-07-24", "relevancy": 2.2776, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6309}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Scanpath%20Prediction%20in%20Target-Present%20Visual%20Search%20with%0A%20%20Semantic-Foveal%20Bayesian%20Attention&body=Title%3A%20Human%20Scanpath%20Prediction%20in%20Target-Present%20Visual%20Search%20with%0A%20%20Semantic-Foveal%20Bayesian%20Attention%0AAuthor%3A%20Jo%C3%A3o%20Luzio%20and%20Alexandre%20Bernardino%20and%20Plinio%20Moreno%0AAbstract%3A%20%20%20In%20goal-directed%20visual%20tasks%2C%20human%20perception%20is%20guided%20by%20both%20top-down%0Aand%20bottom-up%20cues.%20At%20the%20same%20time%2C%20foveal%20vision%20plays%20a%20crucial%20role%20in%0Adirecting%20attention%20efficiently.%20Modern%20research%20on%20bio-inspired%20computational%0Aattention%20models%20has%20taken%20advantage%20of%20advancements%20in%20deep%20learning%20by%0Autilizing%20human%20scanpath%20data%20to%20achieve%20new%20state-of-the-art%20performance.%20In%0Athis%20work%2C%20we%20assess%20the%20performance%20of%20SemBA-FAST%2C%20i.e.%20Semantic-based%0ABayesian%20Attention%20for%20Foveal%20Active%20visual%20Search%20Tasks%2C%20a%20top-down%20framework%0Adesigned%20for%20predicting%20human%20visual%20attention%20in%20target-present%20visual%20search.%0ASemBA-FAST%20integrates%20deep%20object%20detection%20with%20a%20probabilistic%20semantic%0Afusion%20mechanism%20to%20generate%20attention%20maps%20dynamically%2C%20leveraging%20pre-trained%0Adetectors%20and%20artificial%20foveation%20to%20update%20top-down%20knowledge%20and%20improve%0Afixation%20prediction%20sequentially.%20We%20evaluate%20SemBA-FAST%20on%20the%20COCO-Search18%0Abenchmark%20dataset%2C%20comparing%20its%20performance%20against%20other%20scanpath%20prediction%0Amodels.%20Our%20methodology%20achieves%20fixation%20sequences%20that%20closely%20match%20human%0Aground-truth%20scanpaths.%20Notably%2C%20it%20surpasses%20baseline%20and%20other%20top-down%0Aapproaches%20and%20competes%2C%20in%20some%20cases%2C%20with%20scanpath-informed%20models.%20These%0Afindings%20provide%20valuable%20insights%20into%20the%20capabilities%20of%20semantic-foveal%0Aprobabilistic%20frameworks%20for%20human-like%20attention%20modelling%2C%20with%20implications%0Afor%20real-time%20cognitive%20computing%20and%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Scanpath%2520Prediction%2520in%2520Target-Present%2520Visual%2520Search%2520with%250A%2520%2520Semantic-Foveal%2520Bayesian%2520Attention%26entry.906535625%3DJo%25C3%25A3o%2520Luzio%2520and%2520Alexandre%2520Bernardino%2520and%2520Plinio%2520Moreno%26entry.1292438233%3D%2520%2520In%2520goal-directed%2520visual%2520tasks%252C%2520human%2520perception%2520is%2520guided%2520by%2520both%2520top-down%250Aand%2520bottom-up%2520cues.%2520At%2520the%2520same%2520time%252C%2520foveal%2520vision%2520plays%2520a%2520crucial%2520role%2520in%250Adirecting%2520attention%2520efficiently.%2520Modern%2520research%2520on%2520bio-inspired%2520computational%250Aattention%2520models%2520has%2520taken%2520advantage%2520of%2520advancements%2520in%2520deep%2520learning%2520by%250Autilizing%2520human%2520scanpath%2520data%2520to%2520achieve%2520new%2520state-of-the-art%2520performance.%2520In%250Athis%2520work%252C%2520we%2520assess%2520the%2520performance%2520of%2520SemBA-FAST%252C%2520i.e.%2520Semantic-based%250ABayesian%2520Attention%2520for%2520Foveal%2520Active%2520visual%2520Search%2520Tasks%252C%2520a%2520top-down%2520framework%250Adesigned%2520for%2520predicting%2520human%2520visual%2520attention%2520in%2520target-present%2520visual%2520search.%250ASemBA-FAST%2520integrates%2520deep%2520object%2520detection%2520with%2520a%2520probabilistic%2520semantic%250Afusion%2520mechanism%2520to%2520generate%2520attention%2520maps%2520dynamically%252C%2520leveraging%2520pre-trained%250Adetectors%2520and%2520artificial%2520foveation%2520to%2520update%2520top-down%2520knowledge%2520and%2520improve%250Afixation%2520prediction%2520sequentially.%2520We%2520evaluate%2520SemBA-FAST%2520on%2520the%2520COCO-Search18%250Abenchmark%2520dataset%252C%2520comparing%2520its%2520performance%2520against%2520other%2520scanpath%2520prediction%250Amodels.%2520Our%2520methodology%2520achieves%2520fixation%2520sequences%2520that%2520closely%2520match%2520human%250Aground-truth%2520scanpaths.%2520Notably%252C%2520it%2520surpasses%2520baseline%2520and%2520other%2520top-down%250Aapproaches%2520and%2520competes%252C%2520in%2520some%2520cases%252C%2520with%2520scanpath-informed%2520models.%2520These%250Afindings%2520provide%2520valuable%2520insights%2520into%2520the%2520capabilities%2520of%2520semantic-foveal%250Aprobabilistic%2520frameworks%2520for%2520human-like%2520attention%2520modelling%252C%2520with%2520implications%250Afor%2520real-time%2520cognitive%2520computing%2520and%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Scanpath%20Prediction%20in%20Target-Present%20Visual%20Search%20with%0A%20%20Semantic-Foveal%20Bayesian%20Attention&entry.906535625=Jo%C3%A3o%20Luzio%20and%20Alexandre%20Bernardino%20and%20Plinio%20Moreno&entry.1292438233=%20%20In%20goal-directed%20visual%20tasks%2C%20human%20perception%20is%20guided%20by%20both%20top-down%0Aand%20bottom-up%20cues.%20At%20the%20same%20time%2C%20foveal%20vision%20plays%20a%20crucial%20role%20in%0Adirecting%20attention%20efficiently.%20Modern%20research%20on%20bio-inspired%20computational%0Aattention%20models%20has%20taken%20advantage%20of%20advancements%20in%20deep%20learning%20by%0Autilizing%20human%20scanpath%20data%20to%20achieve%20new%20state-of-the-art%20performance.%20In%0Athis%20work%2C%20we%20assess%20the%20performance%20of%20SemBA-FAST%2C%20i.e.%20Semantic-based%0ABayesian%20Attention%20for%20Foveal%20Active%20visual%20Search%20Tasks%2C%20a%20top-down%20framework%0Adesigned%20for%20predicting%20human%20visual%20attention%20in%20target-present%20visual%20search.%0ASemBA-FAST%20integrates%20deep%20object%20detection%20with%20a%20probabilistic%20semantic%0Afusion%20mechanism%20to%20generate%20attention%20maps%20dynamically%2C%20leveraging%20pre-trained%0Adetectors%20and%20artificial%20foveation%20to%20update%20top-down%20knowledge%20and%20improve%0Afixation%20prediction%20sequentially.%20We%20evaluate%20SemBA-FAST%20on%20the%20COCO-Search18%0Abenchmark%20dataset%2C%20comparing%20its%20performance%20against%20other%20scanpath%20prediction%0Amodels.%20Our%20methodology%20achieves%20fixation%20sequences%20that%20closely%20match%20human%0Aground-truth%20scanpaths.%20Notably%2C%20it%20surpasses%20baseline%20and%20other%20top-down%0Aapproaches%20and%20competes%2C%20in%20some%20cases%2C%20with%20scanpath-informed%20models.%20These%0Afindings%20provide%20valuable%20insights%20into%20the%20capabilities%20of%20semantic-foveal%0Aprobabilistic%20frameworks%20for%20human-like%20attention%20modelling%2C%20with%20implications%0Afor%20real-time%20cognitive%20computing%20and%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18503v1&entry.124074799=Read"},
{"title": "Spatio-Temporal Motion Retargeting for Quadruped Robots", "author": "Taerim Yoon and Dongho Kang and Seungmin Kim and Jin Cheng and Minsung Ahn and Stelian Coros and Sungjoon Choi", "abstract": "  This work presents a motion retargeting approach for legged robots, aimed at\ntransferring the dynamic and agile movements to robots from source motions. In\nparticular, we guide the imitation learning procedures by transferring motions\nfrom source to target, effectively bridging the morphological disparities while\nensuring the physical feasibility of the target system. In the first stage, we\nfocus on motion retargeting at the kinematic level by generating kinematically\nfeasible whole-body motions from keypoint trajectories. Following this, we\nrefine the motion at the dynamic level by adjusting it in the temporal domain\nwhile adhering to physical constraints. This process facilitates policy\ntraining via reinforcement learning, enabling precise and robust motion\ntracking. We demonstrate that our approach successfully transforms noisy motion\nsources, such as hand-held camera videos, into robot-specific motions that\nalign with the morphology and physical properties of the target robots.\nMoreover, we demonstrate terrain-aware motion retargeting to perform BackFlip\non top of a box. We successfully deployed these skills to four robots with\ndifferent dimensions and physical properties in the real world through hardware\nexperiments.\n", "link": "http://arxiv.org/abs/2404.11557v3", "date": "2025-07-24", "relevancy": 2.2766, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5938}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.592}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Temporal%20Motion%20Retargeting%20for%20Quadruped%20Robots&body=Title%3A%20Spatio-Temporal%20Motion%20Retargeting%20for%20Quadruped%20Robots%0AAuthor%3A%20Taerim%20Yoon%20and%20Dongho%20Kang%20and%20Seungmin%20Kim%20and%20Jin%20Cheng%20and%20Minsung%20Ahn%20and%20Stelian%20Coros%20and%20Sungjoon%20Choi%0AAbstract%3A%20%20%20This%20work%20presents%20a%20motion%20retargeting%20approach%20for%20legged%20robots%2C%20aimed%20at%0Atransferring%20the%20dynamic%20and%20agile%20movements%20to%20robots%20from%20source%20motions.%20In%0Aparticular%2C%20we%20guide%20the%20imitation%20learning%20procedures%20by%20transferring%20motions%0Afrom%20source%20to%20target%2C%20effectively%20bridging%20the%20morphological%20disparities%20while%0Aensuring%20the%20physical%20feasibility%20of%20the%20target%20system.%20In%20the%20first%20stage%2C%20we%0Afocus%20on%20motion%20retargeting%20at%20the%20kinematic%20level%20by%20generating%20kinematically%0Afeasible%20whole-body%20motions%20from%20keypoint%20trajectories.%20Following%20this%2C%20we%0Arefine%20the%20motion%20at%20the%20dynamic%20level%20by%20adjusting%20it%20in%20the%20temporal%20domain%0Awhile%20adhering%20to%20physical%20constraints.%20This%20process%20facilitates%20policy%0Atraining%20via%20reinforcement%20learning%2C%20enabling%20precise%20and%20robust%20motion%0Atracking.%20We%20demonstrate%20that%20our%20approach%20successfully%20transforms%20noisy%20motion%0Asources%2C%20such%20as%20hand-held%20camera%20videos%2C%20into%20robot-specific%20motions%20that%0Aalign%20with%20the%20morphology%20and%20physical%20properties%20of%20the%20target%20robots.%0AMoreover%2C%20we%20demonstrate%20terrain-aware%20motion%20retargeting%20to%20perform%20BackFlip%0Aon%20top%20of%20a%20box.%20We%20successfully%20deployed%20these%20skills%20to%20four%20robots%20with%0Adifferent%20dimensions%20and%20physical%20properties%20in%20the%20real%20world%20through%20hardware%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11557v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Temporal%2520Motion%2520Retargeting%2520for%2520Quadruped%2520Robots%26entry.906535625%3DTaerim%2520Yoon%2520and%2520Dongho%2520Kang%2520and%2520Seungmin%2520Kim%2520and%2520Jin%2520Cheng%2520and%2520Minsung%2520Ahn%2520and%2520Stelian%2520Coros%2520and%2520Sungjoon%2520Choi%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520motion%2520retargeting%2520approach%2520for%2520legged%2520robots%252C%2520aimed%2520at%250Atransferring%2520the%2520dynamic%2520and%2520agile%2520movements%2520to%2520robots%2520from%2520source%2520motions.%2520In%250Aparticular%252C%2520we%2520guide%2520the%2520imitation%2520learning%2520procedures%2520by%2520transferring%2520motions%250Afrom%2520source%2520to%2520target%252C%2520effectively%2520bridging%2520the%2520morphological%2520disparities%2520while%250Aensuring%2520the%2520physical%2520feasibility%2520of%2520the%2520target%2520system.%2520In%2520the%2520first%2520stage%252C%2520we%250Afocus%2520on%2520motion%2520retargeting%2520at%2520the%2520kinematic%2520level%2520by%2520generating%2520kinematically%250Afeasible%2520whole-body%2520motions%2520from%2520keypoint%2520trajectories.%2520Following%2520this%252C%2520we%250Arefine%2520the%2520motion%2520at%2520the%2520dynamic%2520level%2520by%2520adjusting%2520it%2520in%2520the%2520temporal%2520domain%250Awhile%2520adhering%2520to%2520physical%2520constraints.%2520This%2520process%2520facilitates%2520policy%250Atraining%2520via%2520reinforcement%2520learning%252C%2520enabling%2520precise%2520and%2520robust%2520motion%250Atracking.%2520We%2520demonstrate%2520that%2520our%2520approach%2520successfully%2520transforms%2520noisy%2520motion%250Asources%252C%2520such%2520as%2520hand-held%2520camera%2520videos%252C%2520into%2520robot-specific%2520motions%2520that%250Aalign%2520with%2520the%2520morphology%2520and%2520physical%2520properties%2520of%2520the%2520target%2520robots.%250AMoreover%252C%2520we%2520demonstrate%2520terrain-aware%2520motion%2520retargeting%2520to%2520perform%2520BackFlip%250Aon%2520top%2520of%2520a%2520box.%2520We%2520successfully%2520deployed%2520these%2520skills%2520to%2520four%2520robots%2520with%250Adifferent%2520dimensions%2520and%2520physical%2520properties%2520in%2520the%2520real%2520world%2520through%2520hardware%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11557v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Temporal%20Motion%20Retargeting%20for%20Quadruped%20Robots&entry.906535625=Taerim%20Yoon%20and%20Dongho%20Kang%20and%20Seungmin%20Kim%20and%20Jin%20Cheng%20and%20Minsung%20Ahn%20and%20Stelian%20Coros%20and%20Sungjoon%20Choi&entry.1292438233=%20%20This%20work%20presents%20a%20motion%20retargeting%20approach%20for%20legged%20robots%2C%20aimed%20at%0Atransferring%20the%20dynamic%20and%20agile%20movements%20to%20robots%20from%20source%20motions.%20In%0Aparticular%2C%20we%20guide%20the%20imitation%20learning%20procedures%20by%20transferring%20motions%0Afrom%20source%20to%20target%2C%20effectively%20bridging%20the%20morphological%20disparities%20while%0Aensuring%20the%20physical%20feasibility%20of%20the%20target%20system.%20In%20the%20first%20stage%2C%20we%0Afocus%20on%20motion%20retargeting%20at%20the%20kinematic%20level%20by%20generating%20kinematically%0Afeasible%20whole-body%20motions%20from%20keypoint%20trajectories.%20Following%20this%2C%20we%0Arefine%20the%20motion%20at%20the%20dynamic%20level%20by%20adjusting%20it%20in%20the%20temporal%20domain%0Awhile%20adhering%20to%20physical%20constraints.%20This%20process%20facilitates%20policy%0Atraining%20via%20reinforcement%20learning%2C%20enabling%20precise%20and%20robust%20motion%0Atracking.%20We%20demonstrate%20that%20our%20approach%20successfully%20transforms%20noisy%20motion%0Asources%2C%20such%20as%20hand-held%20camera%20videos%2C%20into%20robot-specific%20motions%20that%0Aalign%20with%20the%20morphology%20and%20physical%20properties%20of%20the%20target%20robots.%0AMoreover%2C%20we%20demonstrate%20terrain-aware%20motion%20retargeting%20to%20perform%20BackFlip%0Aon%20top%20of%20a%20box.%20We%20successfully%20deployed%20these%20skills%20to%20four%20robots%20with%0Adifferent%20dimensions%20and%20physical%20properties%20in%20the%20real%20world%20through%20hardware%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11557v3&entry.124074799=Read"},
{"title": "Improving Bird Classification with Primary Color Additives", "author": "Ezhini Rasendiran R and Chandresh Kumar Maurya", "abstract": "  We address the problem of classifying bird species using their song\nrecordings, a challenging task due to environmental noise, overlapping\nvocalizations, and missing labels. Existing models struggle with low-SNR or\nmulti-species recordings. We hypothesize that birds can be classified by\nvisualizing their pitch pattern, speed, and repetition, collectively called\nmotifs. Deep learning models applied to spectrogram images help, but similar\nmotifs across species cause confusion. To mitigate this, we embed frequency\ninformation into spectrograms using primary color additives. This enhances\nspecies distinction and improves classification accuracy. Our experiments show\nthat the proposed approach achieves statistically significant gains over models\nwithout colorization and surpasses the BirdCLEF 2024 winner, improving F1 by\n7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the\neffectiveness of incorporating frequency information via colorization.\n", "link": "http://arxiv.org/abs/2507.18334v1", "date": "2025-07-24", "relevancy": 2.2763, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4687}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Bird%20Classification%20with%20Primary%20Color%20Additives&body=Title%3A%20Improving%20Bird%20Classification%20with%20Primary%20Color%20Additives%0AAuthor%3A%20Ezhini%20Rasendiran%20R%20and%20Chandresh%20Kumar%20Maurya%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20classifying%20bird%20species%20using%20their%20song%0Arecordings%2C%20a%20challenging%20task%20due%20to%20environmental%20noise%2C%20overlapping%0Avocalizations%2C%20and%20missing%20labels.%20Existing%20models%20struggle%20with%20low-SNR%20or%0Amulti-species%20recordings.%20We%20hypothesize%20that%20birds%20can%20be%20classified%20by%0Avisualizing%20their%20pitch%20pattern%2C%20speed%2C%20and%20repetition%2C%20collectively%20called%0Amotifs.%20Deep%20learning%20models%20applied%20to%20spectrogram%20images%20help%2C%20but%20similar%0Amotifs%20across%20species%20cause%20confusion.%20To%20mitigate%20this%2C%20we%20embed%20frequency%0Ainformation%20into%20spectrograms%20using%20primary%20color%20additives.%20This%20enhances%0Aspecies%20distinction%20and%20improves%20classification%20accuracy.%20Our%20experiments%20show%0Athat%20the%20proposed%20approach%20achieves%20statistically%20significant%20gains%20over%20models%0Awithout%20colorization%20and%20surpasses%20the%20BirdCLEF%202024%20winner%2C%20improving%20F1%20by%0A7.3%25%2C%20ROC-AUC%20by%206.2%25%2C%20and%20CMAP%20by%206.6%25.%20These%20results%20demonstrate%20the%0Aeffectiveness%20of%20incorporating%20frequency%20information%20via%20colorization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Bird%2520Classification%2520with%2520Primary%2520Color%2520Additives%26entry.906535625%3DEzhini%2520Rasendiran%2520R%2520and%2520Chandresh%2520Kumar%2520Maurya%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520classifying%2520bird%2520species%2520using%2520their%2520song%250Arecordings%252C%2520a%2520challenging%2520task%2520due%2520to%2520environmental%2520noise%252C%2520overlapping%250Avocalizations%252C%2520and%2520missing%2520labels.%2520Existing%2520models%2520struggle%2520with%2520low-SNR%2520or%250Amulti-species%2520recordings.%2520We%2520hypothesize%2520that%2520birds%2520can%2520be%2520classified%2520by%250Avisualizing%2520their%2520pitch%2520pattern%252C%2520speed%252C%2520and%2520repetition%252C%2520collectively%2520called%250Amotifs.%2520Deep%2520learning%2520models%2520applied%2520to%2520spectrogram%2520images%2520help%252C%2520but%2520similar%250Amotifs%2520across%2520species%2520cause%2520confusion.%2520To%2520mitigate%2520this%252C%2520we%2520embed%2520frequency%250Ainformation%2520into%2520spectrograms%2520using%2520primary%2520color%2520additives.%2520This%2520enhances%250Aspecies%2520distinction%2520and%2520improves%2520classification%2520accuracy.%2520Our%2520experiments%2520show%250Athat%2520the%2520proposed%2520approach%2520achieves%2520statistically%2520significant%2520gains%2520over%2520models%250Awithout%2520colorization%2520and%2520surpasses%2520the%2520BirdCLEF%25202024%2520winner%252C%2520improving%2520F1%2520by%250A7.3%2525%252C%2520ROC-AUC%2520by%25206.2%2525%252C%2520and%2520CMAP%2520by%25206.6%2525.%2520These%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520incorporating%2520frequency%2520information%2520via%2520colorization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Bird%20Classification%20with%20Primary%20Color%20Additives&entry.906535625=Ezhini%20Rasendiran%20R%20and%20Chandresh%20Kumar%20Maurya&entry.1292438233=%20%20We%20address%20the%20problem%20of%20classifying%20bird%20species%20using%20their%20song%0Arecordings%2C%20a%20challenging%20task%20due%20to%20environmental%20noise%2C%20overlapping%0Avocalizations%2C%20and%20missing%20labels.%20Existing%20models%20struggle%20with%20low-SNR%20or%0Amulti-species%20recordings.%20We%20hypothesize%20that%20birds%20can%20be%20classified%20by%0Avisualizing%20their%20pitch%20pattern%2C%20speed%2C%20and%20repetition%2C%20collectively%20called%0Amotifs.%20Deep%20learning%20models%20applied%20to%20spectrogram%20images%20help%2C%20but%20similar%0Amotifs%20across%20species%20cause%20confusion.%20To%20mitigate%20this%2C%20we%20embed%20frequency%0Ainformation%20into%20spectrograms%20using%20primary%20color%20additives.%20This%20enhances%0Aspecies%20distinction%20and%20improves%20classification%20accuracy.%20Our%20experiments%20show%0Athat%20the%20proposed%20approach%20achieves%20statistically%20significant%20gains%20over%20models%0Awithout%20colorization%20and%20surpasses%20the%20BirdCLEF%202024%20winner%2C%20improving%20F1%20by%0A7.3%25%2C%20ROC-AUC%20by%206.2%25%2C%20and%20CMAP%20by%206.6%25.%20These%20results%20demonstrate%20the%0Aeffectiveness%20of%20incorporating%20frequency%20information%20via%20colorization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18334v1&entry.124074799=Read"},
{"title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual\n  Rewards", "author": "Andreea Nica and Ivan Zakazov and Nicolas Mario Baldwin and Saibo Geng and Robert West", "abstract": "  Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH.\n", "link": "http://arxiv.org/abs/2507.18618v1", "date": "2025-07-24", "relevancy": 2.2758, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRPrompt%3A%20Bootstrapping%20Query-Aware%20Prompt%20Optimization%20from%20Textual%0A%20%20Rewards&body=Title%3A%20TRPrompt%3A%20Bootstrapping%20Query-Aware%20Prompt%20Optimization%20from%20Textual%0A%20%20Rewards%0AAuthor%3A%20Andreea%20Nica%20and%20Ivan%20Zakazov%20and%20Nicolas%20Mario%20Baldwin%20and%20Saibo%20Geng%20and%20Robert%20West%0AAbstract%3A%20%20%20Prompt%20optimization%20improves%20the%20reasoning%20abilities%20of%20large%20language%20models%0A%28LLMs%29%20without%20requiring%20parameter%20updates%20to%20the%20target%20model.%20Following%0Aheuristic-based%20%22Think%20step%20by%20step%22%20approaches%2C%20the%20field%20has%20evolved%20in%20two%0Amain%20directions%3A%20while%20one%20group%20of%20methods%20uses%20textual%20feedback%20to%20elicit%0Aimproved%20prompts%20from%20general-purpose%20LLMs%20in%20a%20training-free%20way%2C%20a%20concurrent%0Aline%20of%20research%20relies%20on%20numerical%20rewards%20to%20train%20a%20special%20prompt%20model%2C%0Atailored%20for%20providing%20optimal%20prompts%20to%20the%20target%20model.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20Textual%20Reward%20Prompt%20framework%20%28TRPrompt%29%2C%20which%20unifies%20these%0Aapproaches%20by%20directly%20incorporating%20textual%20feedback%20into%20training%20of%20the%0Aprompt%20model.%20Our%20framework%20does%20not%20require%20prior%20dataset%20collection%20and%20is%0Abeing%20iteratively%20improved%20with%20the%20feedback%20on%20the%20generated%20prompts.%20When%0Acoupled%20with%20the%20capacity%20of%20an%20LLM%20to%20internalize%20the%20notion%20of%20what%20a%20%22good%22%0Aprompt%20is%2C%20the%20high-resolution%20signal%20provided%20by%20the%20textual%20rewards%20allows%20us%0Ato%20train%20a%20prompt%20model%20yielding%20state-of-the-art%20query-specific%20prompts%20for%0Athe%20problems%20from%20the%20challenging%20math%20datasets%20GSMHard%20and%20MATH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRPrompt%253A%2520Bootstrapping%2520Query-Aware%2520Prompt%2520Optimization%2520from%2520Textual%250A%2520%2520Rewards%26entry.906535625%3DAndreea%2520Nica%2520and%2520Ivan%2520Zakazov%2520and%2520Nicolas%2520Mario%2520Baldwin%2520and%2520Saibo%2520Geng%2520and%2520Robert%2520West%26entry.1292438233%3D%2520%2520Prompt%2520optimization%2520improves%2520the%2520reasoning%2520abilities%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520without%2520requiring%2520parameter%2520updates%2520to%2520the%2520target%2520model.%2520Following%250Aheuristic-based%2520%2522Think%2520step%2520by%2520step%2522%2520approaches%252C%2520the%2520field%2520has%2520evolved%2520in%2520two%250Amain%2520directions%253A%2520while%2520one%2520group%2520of%2520methods%2520uses%2520textual%2520feedback%2520to%2520elicit%250Aimproved%2520prompts%2520from%2520general-purpose%2520LLMs%2520in%2520a%2520training-free%2520way%252C%2520a%2520concurrent%250Aline%2520of%2520research%2520relies%2520on%2520numerical%2520rewards%2520to%2520train%2520a%2520special%2520prompt%2520model%252C%250Atailored%2520for%2520providing%2520optimal%2520prompts%2520to%2520the%2520target%2520model.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520Textual%2520Reward%2520Prompt%2520framework%2520%2528TRPrompt%2529%252C%2520which%2520unifies%2520these%250Aapproaches%2520by%2520directly%2520incorporating%2520textual%2520feedback%2520into%2520training%2520of%2520the%250Aprompt%2520model.%2520Our%2520framework%2520does%2520not%2520require%2520prior%2520dataset%2520collection%2520and%2520is%250Abeing%2520iteratively%2520improved%2520with%2520the%2520feedback%2520on%2520the%2520generated%2520prompts.%2520When%250Acoupled%2520with%2520the%2520capacity%2520of%2520an%2520LLM%2520to%2520internalize%2520the%2520notion%2520of%2520what%2520a%2520%2522good%2522%250Aprompt%2520is%252C%2520the%2520high-resolution%2520signal%2520provided%2520by%2520the%2520textual%2520rewards%2520allows%2520us%250Ato%2520train%2520a%2520prompt%2520model%2520yielding%2520state-of-the-art%2520query-specific%2520prompts%2520for%250Athe%2520problems%2520from%2520the%2520challenging%2520math%2520datasets%2520GSMHard%2520and%2520MATH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRPrompt%3A%20Bootstrapping%20Query-Aware%20Prompt%20Optimization%20from%20Textual%0A%20%20Rewards&entry.906535625=Andreea%20Nica%20and%20Ivan%20Zakazov%20and%20Nicolas%20Mario%20Baldwin%20and%20Saibo%20Geng%20and%20Robert%20West&entry.1292438233=%20%20Prompt%20optimization%20improves%20the%20reasoning%20abilities%20of%20large%20language%20models%0A%28LLMs%29%20without%20requiring%20parameter%20updates%20to%20the%20target%20model.%20Following%0Aheuristic-based%20%22Think%20step%20by%20step%22%20approaches%2C%20the%20field%20has%20evolved%20in%20two%0Amain%20directions%3A%20while%20one%20group%20of%20methods%20uses%20textual%20feedback%20to%20elicit%0Aimproved%20prompts%20from%20general-purpose%20LLMs%20in%20a%20training-free%20way%2C%20a%20concurrent%0Aline%20of%20research%20relies%20on%20numerical%20rewards%20to%20train%20a%20special%20prompt%20model%2C%0Atailored%20for%20providing%20optimal%20prompts%20to%20the%20target%20model.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20Textual%20Reward%20Prompt%20framework%20%28TRPrompt%29%2C%20which%20unifies%20these%0Aapproaches%20by%20directly%20incorporating%20textual%20feedback%20into%20training%20of%20the%0Aprompt%20model.%20Our%20framework%20does%20not%20require%20prior%20dataset%20collection%20and%20is%0Abeing%20iteratively%20improved%20with%20the%20feedback%20on%20the%20generated%20prompts.%20When%0Acoupled%20with%20the%20capacity%20of%20an%20LLM%20to%20internalize%20the%20notion%20of%20what%20a%20%22good%22%0Aprompt%20is%2C%20the%20high-resolution%20signal%20provided%20by%20the%20textual%20rewards%20allows%20us%0Ato%20train%20a%20prompt%20model%20yielding%20state-of-the-art%20query-specific%20prompts%20for%0Athe%20problems%20from%20the%20challenging%20math%20datasets%20GSMHard%20and%20MATH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18618v1&entry.124074799=Read"},
{"title": "Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear\n  Norm", "author": "Jiangjun Peng and Yisi Luo and Xiangyong Cao and Shuang Xu and Deyu Meng", "abstract": "  The nuclear norm (NN) has been widely explored in matrix recovery problems,\nsuch as Robust PCA and matrix completion, leveraging the inherent global\nlow-rank structure of the data. In this study, we introduce a new modified\nnuclear norm (MNN) framework, where the MNN family norms are defined by\nadopting suitable transformations and performing the NN on the transformed\nmatrix. The MNN framework offers two main advantages: (1) it jointly captures\nboth local information and global low-rankness without requiring trade-off\nparameter tuning; (2) Under mild assumptions on the transformation, we provided\nexact theoretical recovery guarantees for both Robust PCA and MC tasks-an\nachievement not shared by existing methods that combine local and global\ninformation. Thanks to its general and flexible design, MNN can accommodate\nvarious proven transformations, enabling a unified and effective approach to\nstructured low-rank recovery. Extensive experiments demonstrate the\neffectiveness of our method. Code and supplementary material are available at\nhttps://github.com/andrew-pengjj/modified_nuclear_norm.\n", "link": "http://arxiv.org/abs/2507.18327v1", "date": "2025-07-24", "relevancy": 2.2645, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4625}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4511}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Low-rankness%3A%20Guaranteed%20Matrix%20Recovery%20via%20Modified%20Nuclear%0A%20%20Norm&body=Title%3A%20Beyond%20Low-rankness%3A%20Guaranteed%20Matrix%20Recovery%20via%20Modified%20Nuclear%0A%20%20Norm%0AAuthor%3A%20Jiangjun%20Peng%20and%20Yisi%20Luo%20and%20Xiangyong%20Cao%20and%20Shuang%20Xu%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20The%20nuclear%20norm%20%28NN%29%20has%20been%20widely%20explored%20in%20matrix%20recovery%20problems%2C%0Asuch%20as%20Robust%20PCA%20and%20matrix%20completion%2C%20leveraging%20the%20inherent%20global%0Alow-rank%20structure%20of%20the%20data.%20In%20this%20study%2C%20we%20introduce%20a%20new%20modified%0Anuclear%20norm%20%28MNN%29%20framework%2C%20where%20the%20MNN%20family%20norms%20are%20defined%20by%0Aadopting%20suitable%20transformations%20and%20performing%20the%20NN%20on%20the%20transformed%0Amatrix.%20The%20MNN%20framework%20offers%20two%20main%20advantages%3A%20%281%29%20it%20jointly%20captures%0Aboth%20local%20information%20and%20global%20low-rankness%20without%20requiring%20trade-off%0Aparameter%20tuning%3B%20%282%29%20Under%20mild%20assumptions%20on%20the%20transformation%2C%20we%20provided%0Aexact%20theoretical%20recovery%20guarantees%20for%20both%20Robust%20PCA%20and%20MC%20tasks-an%0Aachievement%20not%20shared%20by%20existing%20methods%20that%20combine%20local%20and%20global%0Ainformation.%20Thanks%20to%20its%20general%20and%20flexible%20design%2C%20MNN%20can%20accommodate%0Avarious%20proven%20transformations%2C%20enabling%20a%20unified%20and%20effective%20approach%20to%0Astructured%20low-rank%20recovery.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%20Code%20and%20supplementary%20material%20are%20available%20at%0Ahttps%3A//github.com/andrew-pengjj/modified_nuclear_norm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Low-rankness%253A%2520Guaranteed%2520Matrix%2520Recovery%2520via%2520Modified%2520Nuclear%250A%2520%2520Norm%26entry.906535625%3DJiangjun%2520Peng%2520and%2520Yisi%2520Luo%2520and%2520Xiangyong%2520Cao%2520and%2520Shuang%2520Xu%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520The%2520nuclear%2520norm%2520%2528NN%2529%2520has%2520been%2520widely%2520explored%2520in%2520matrix%2520recovery%2520problems%252C%250Asuch%2520as%2520Robust%2520PCA%2520and%2520matrix%2520completion%252C%2520leveraging%2520the%2520inherent%2520global%250Alow-rank%2520structure%2520of%2520the%2520data.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520new%2520modified%250Anuclear%2520norm%2520%2528MNN%2529%2520framework%252C%2520where%2520the%2520MNN%2520family%2520norms%2520are%2520defined%2520by%250Aadopting%2520suitable%2520transformations%2520and%2520performing%2520the%2520NN%2520on%2520the%2520transformed%250Amatrix.%2520The%2520MNN%2520framework%2520offers%2520two%2520main%2520advantages%253A%2520%25281%2529%2520it%2520jointly%2520captures%250Aboth%2520local%2520information%2520and%2520global%2520low-rankness%2520without%2520requiring%2520trade-off%250Aparameter%2520tuning%253B%2520%25282%2529%2520Under%2520mild%2520assumptions%2520on%2520the%2520transformation%252C%2520we%2520provided%250Aexact%2520theoretical%2520recovery%2520guarantees%2520for%2520both%2520Robust%2520PCA%2520and%2520MC%2520tasks-an%250Aachievement%2520not%2520shared%2520by%2520existing%2520methods%2520that%2520combine%2520local%2520and%2520global%250Ainformation.%2520Thanks%2520to%2520its%2520general%2520and%2520flexible%2520design%252C%2520MNN%2520can%2520accommodate%250Avarious%2520proven%2520transformations%252C%2520enabling%2520a%2520unified%2520and%2520effective%2520approach%2520to%250Astructured%2520low-rank%2520recovery.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method.%2520Code%2520and%2520supplementary%2520material%2520are%2520available%2520at%250Ahttps%253A//github.com/andrew-pengjj/modified_nuclear_norm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Low-rankness%3A%20Guaranteed%20Matrix%20Recovery%20via%20Modified%20Nuclear%0A%20%20Norm&entry.906535625=Jiangjun%20Peng%20and%20Yisi%20Luo%20and%20Xiangyong%20Cao%20and%20Shuang%20Xu%20and%20Deyu%20Meng&entry.1292438233=%20%20The%20nuclear%20norm%20%28NN%29%20has%20been%20widely%20explored%20in%20matrix%20recovery%20problems%2C%0Asuch%20as%20Robust%20PCA%20and%20matrix%20completion%2C%20leveraging%20the%20inherent%20global%0Alow-rank%20structure%20of%20the%20data.%20In%20this%20study%2C%20we%20introduce%20a%20new%20modified%0Anuclear%20norm%20%28MNN%29%20framework%2C%20where%20the%20MNN%20family%20norms%20are%20defined%20by%0Aadopting%20suitable%20transformations%20and%20performing%20the%20NN%20on%20the%20transformed%0Amatrix.%20The%20MNN%20framework%20offers%20two%20main%20advantages%3A%20%281%29%20it%20jointly%20captures%0Aboth%20local%20information%20and%20global%20low-rankness%20without%20requiring%20trade-off%0Aparameter%20tuning%3B%20%282%29%20Under%20mild%20assumptions%20on%20the%20transformation%2C%20we%20provided%0Aexact%20theoretical%20recovery%20guarantees%20for%20both%20Robust%20PCA%20and%20MC%20tasks-an%0Aachievement%20not%20shared%20by%20existing%20methods%20that%20combine%20local%20and%20global%0Ainformation.%20Thanks%20to%20its%20general%20and%20flexible%20design%2C%20MNN%20can%20accommodate%0Avarious%20proven%20transformations%2C%20enabling%20a%20unified%20and%20effective%20approach%20to%0Astructured%20low-rank%20recovery.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%20Code%20and%20supplementary%20material%20are%20available%20at%0Ahttps%3A//github.com/andrew-pengjj/modified_nuclear_norm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18327v1&entry.124074799=Read"},
{"title": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented\n  Controllable Video Captioning", "author": "Tianheng Qiu and Jingchun Gao and Jingyu Li and Huiyi Leong and Xuan Huang and Xi Wang and Xiaocheng Zhang and Kele Xu and Lan Zhang", "abstract": "  Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet.\n", "link": "http://arxiv.org/abs/2507.18531v1", "date": "2025-07-24", "relevancy": 2.2588, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5775}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IntentVCNet%3A%20Bridging%20Spatio-Temporal%20Gaps%20for%20Intention-Oriented%0A%20%20Controllable%20Video%20Captioning&body=Title%3A%20IntentVCNet%3A%20Bridging%20Spatio-Temporal%20Gaps%20for%20Intention-Oriented%0A%20%20Controllable%20Video%20Captioning%0AAuthor%3A%20Tianheng%20Qiu%20and%20Jingchun%20Gao%20and%20Jingyu%20Li%20and%20Huiyi%20Leong%20and%20Xuan%20Huang%20and%20Xi%20Wang%20and%20Xiaocheng%20Zhang%20and%20Kele%20Xu%20and%20Lan%20Zhang%0AAbstract%3A%20%20%20Intent-oriented%20controlled%20video%20captioning%20aims%20to%20generate%20targeted%0Adescriptions%20for%20specific%20targets%20in%20a%20video%20based%20on%20customized%20user%20intent.%0ACurrent%20Large%20Visual%20Language%20Models%20%28LVLMs%29%20have%20gained%20strong%20instruction%0Afollowing%20and%20visual%20comprehension%20capabilities.%20Although%20the%20LVLMs%0Ademonstrated%20proficiency%20in%20spatial%20and%20temporal%20understanding%20respectively%2C%20it%0Awas%20not%20able%20to%20perform%20fine-grained%20spatial%20control%20in%20time%20sequences%20in%0Adirect%20response%20to%20instructions.%20This%20substantial%20spatio-temporal%20gap%0Acomplicates%20efforts%20to%20achieve%20fine-grained%20intention-oriented%20control%20in%0Avideo.%20Towards%20this%20end%2C%20we%20propose%20a%20novel%20IntentVCNet%20that%20unifies%20the%0Atemporal%20and%20spatial%20understanding%20knowledge%20inherent%20in%20LVLMs%20to%20bridge%20the%0Aspatio-temporal%20gap%20from%20both%20prompting%20and%20model%20perspectives.%20Specifically%2C%0Awe%20first%20propose%20a%20prompt%20combination%20strategy%20designed%20to%20enable%20LLM%20to%20model%0Athe%20implicit%20relationship%20between%20prompts%20that%20characterize%20user%20intent%20and%0Avideo%20sequences.%20We%20then%20propose%20a%20parameter%20efficient%20box%20adapter%20that%0Aaugments%20the%20object%20semantic%20information%20in%20the%20global%20visual%20context%20so%20that%0Athe%20visual%20token%20has%20a%20priori%20information%20about%20the%20user%20intent.%20The%20final%0Aexperiment%20proves%20that%20the%20combination%20of%20the%20two%20strategies%20can%20further%0Aenhance%20the%20LVLM%27s%20ability%20to%20model%20spatial%20details%20in%20video%20sequences%2C%20and%0Afacilitate%20the%20LVLMs%20to%20accurately%20generate%20controlled%20intent-oriented%0Acaptions.%20Our%20proposed%20method%20achieved%20state-of-the-art%20results%20in%20several%20open%0Asource%20LVLMs%20and%20was%20the%20runner-up%20in%20the%20IntentVC%20challenge.%20Our%20code%20is%0Aavailable%20on%20https%3A//github.com/thqiu0419/IntentVCNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntentVCNet%253A%2520Bridging%2520Spatio-Temporal%2520Gaps%2520for%2520Intention-Oriented%250A%2520%2520Controllable%2520Video%2520Captioning%26entry.906535625%3DTianheng%2520Qiu%2520and%2520Jingchun%2520Gao%2520and%2520Jingyu%2520Li%2520and%2520Huiyi%2520Leong%2520and%2520Xuan%2520Huang%2520and%2520Xi%2520Wang%2520and%2520Xiaocheng%2520Zhang%2520and%2520Kele%2520Xu%2520and%2520Lan%2520Zhang%26entry.1292438233%3D%2520%2520Intent-oriented%2520controlled%2520video%2520captioning%2520aims%2520to%2520generate%2520targeted%250Adescriptions%2520for%2520specific%2520targets%2520in%2520a%2520video%2520based%2520on%2520customized%2520user%2520intent.%250ACurrent%2520Large%2520Visual%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520gained%2520strong%2520instruction%250Afollowing%2520and%2520visual%2520comprehension%2520capabilities.%2520Although%2520the%2520LVLMs%250Ademonstrated%2520proficiency%2520in%2520spatial%2520and%2520temporal%2520understanding%2520respectively%252C%2520it%250Awas%2520not%2520able%2520to%2520perform%2520fine-grained%2520spatial%2520control%2520in%2520time%2520sequences%2520in%250Adirect%2520response%2520to%2520instructions.%2520This%2520substantial%2520spatio-temporal%2520gap%250Acomplicates%2520efforts%2520to%2520achieve%2520fine-grained%2520intention-oriented%2520control%2520in%250Avideo.%2520Towards%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520IntentVCNet%2520that%2520unifies%2520the%250Atemporal%2520and%2520spatial%2520understanding%2520knowledge%2520inherent%2520in%2520LVLMs%2520to%2520bridge%2520the%250Aspatio-temporal%2520gap%2520from%2520both%2520prompting%2520and%2520model%2520perspectives.%2520Specifically%252C%250Awe%2520first%2520propose%2520a%2520prompt%2520combination%2520strategy%2520designed%2520to%2520enable%2520LLM%2520to%2520model%250Athe%2520implicit%2520relationship%2520between%2520prompts%2520that%2520characterize%2520user%2520intent%2520and%250Avideo%2520sequences.%2520We%2520then%2520propose%2520a%2520parameter%2520efficient%2520box%2520adapter%2520that%250Aaugments%2520the%2520object%2520semantic%2520information%2520in%2520the%2520global%2520visual%2520context%2520so%2520that%250Athe%2520visual%2520token%2520has%2520a%2520priori%2520information%2520about%2520the%2520user%2520intent.%2520The%2520final%250Aexperiment%2520proves%2520that%2520the%2520combination%2520of%2520the%2520two%2520strategies%2520can%2520further%250Aenhance%2520the%2520LVLM%2527s%2520ability%2520to%2520model%2520spatial%2520details%2520in%2520video%2520sequences%252C%2520and%250Afacilitate%2520the%2520LVLMs%2520to%2520accurately%2520generate%2520controlled%2520intent-oriented%250Acaptions.%2520Our%2520proposed%2520method%2520achieved%2520state-of-the-art%2520results%2520in%2520several%2520open%250Asource%2520LVLMs%2520and%2520was%2520the%2520runner-up%2520in%2520the%2520IntentVC%2520challenge.%2520Our%2520code%2520is%250Aavailable%2520on%2520https%253A//github.com/thqiu0419/IntentVCNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntentVCNet%3A%20Bridging%20Spatio-Temporal%20Gaps%20for%20Intention-Oriented%0A%20%20Controllable%20Video%20Captioning&entry.906535625=Tianheng%20Qiu%20and%20Jingchun%20Gao%20and%20Jingyu%20Li%20and%20Huiyi%20Leong%20and%20Xuan%20Huang%20and%20Xi%20Wang%20and%20Xiaocheng%20Zhang%20and%20Kele%20Xu%20and%20Lan%20Zhang&entry.1292438233=%20%20Intent-oriented%20controlled%20video%20captioning%20aims%20to%20generate%20targeted%0Adescriptions%20for%20specific%20targets%20in%20a%20video%20based%20on%20customized%20user%20intent.%0ACurrent%20Large%20Visual%20Language%20Models%20%28LVLMs%29%20have%20gained%20strong%20instruction%0Afollowing%20and%20visual%20comprehension%20capabilities.%20Although%20the%20LVLMs%0Ademonstrated%20proficiency%20in%20spatial%20and%20temporal%20understanding%20respectively%2C%20it%0Awas%20not%20able%20to%20perform%20fine-grained%20spatial%20control%20in%20time%20sequences%20in%0Adirect%20response%20to%20instructions.%20This%20substantial%20spatio-temporal%20gap%0Acomplicates%20efforts%20to%20achieve%20fine-grained%20intention-oriented%20control%20in%0Avideo.%20Towards%20this%20end%2C%20we%20propose%20a%20novel%20IntentVCNet%20that%20unifies%20the%0Atemporal%20and%20spatial%20understanding%20knowledge%20inherent%20in%20LVLMs%20to%20bridge%20the%0Aspatio-temporal%20gap%20from%20both%20prompting%20and%20model%20perspectives.%20Specifically%2C%0Awe%20first%20propose%20a%20prompt%20combination%20strategy%20designed%20to%20enable%20LLM%20to%20model%0Athe%20implicit%20relationship%20between%20prompts%20that%20characterize%20user%20intent%20and%0Avideo%20sequences.%20We%20then%20propose%20a%20parameter%20efficient%20box%20adapter%20that%0Aaugments%20the%20object%20semantic%20information%20in%20the%20global%20visual%20context%20so%20that%0Athe%20visual%20token%20has%20a%20priori%20information%20about%20the%20user%20intent.%20The%20final%0Aexperiment%20proves%20that%20the%20combination%20of%20the%20two%20strategies%20can%20further%0Aenhance%20the%20LVLM%27s%20ability%20to%20model%20spatial%20details%20in%20video%20sequences%2C%20and%0Afacilitate%20the%20LVLMs%20to%20accurately%20generate%20controlled%20intent-oriented%0Acaptions.%20Our%20proposed%20method%20achieved%20state-of-the-art%20results%20in%20several%20open%0Asource%20LVLMs%20and%20was%20the%20runner-up%20in%20the%20IntentVC%20challenge.%20Our%20code%20is%0Aavailable%20on%20https%3A//github.com/thqiu0419/IntentVCNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18531v1&entry.124074799=Read"},
{"title": "Self-Supervised Ultrasound-Video Segmentation with Feature Prediction\n  and 3D Localised Loss", "author": "Edward Ellis and Robert Mendel and Andrew Bulpitt and Nasim Parsa and Michael F Byrne and Sharib Ali", "abstract": "  Acquiring and annotating large datasets in ultrasound imaging is challenging\ndue to low contrast, high noise, and susceptibility to artefacts. This process\nrequires significant time and clinical expertise. Self-supervised learning\n(SSL) offers a promising solution by leveraging unlabelled data to learn useful\nrepresentations, enabling improved segmentation performance when annotated data\nis limited. Recent state-of-the-art developments in SSL for video data include\nV-JEPA, a framework solely based on feature prediction, avoiding pixel level\nreconstruction or negative samples. We hypothesise that V-JEPA is well-suited\nto ultrasound imaging, as it is less sensitive to noisy pixel-level detail\nwhile effectively leveraging temporal information. To the best of our\nknowledge, this is the first study to adopt V-JEPA for ultrasound video data.\nSimilar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is\nwell-suited to ViT-based models. However, ViTs can underperform on small\nmedical datasets due to lack of inductive biases, limited spatial locality and\nabsence of hierarchical feature learning. To improve locality understanding, we\npropose a novel 3D localisation auxiliary task to improve locality in ViT\nrepresentations during V-JEPA pre-training. Our results show V-JEPA with our\nauxiliary task improves segmentation performance significantly across various\nfrozen encoder configurations, with gains up to 3.4\\% using 100\\% and up to\n8.35\\% using only 10\\% of the training data.\n", "link": "http://arxiv.org/abs/2507.18424v1", "date": "2025-07-24", "relevancy": 2.2566, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6075}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Ultrasound-Video%20Segmentation%20with%20Feature%20Prediction%0A%20%20and%203D%20Localised%20Loss&body=Title%3A%20Self-Supervised%20Ultrasound-Video%20Segmentation%20with%20Feature%20Prediction%0A%20%20and%203D%20Localised%20Loss%0AAuthor%3A%20Edward%20Ellis%20and%20Robert%20Mendel%20and%20Andrew%20Bulpitt%20and%20Nasim%20Parsa%20and%20Michael%20F%20Byrne%20and%20Sharib%20Ali%0AAbstract%3A%20%20%20Acquiring%20and%20annotating%20large%20datasets%20in%20ultrasound%20imaging%20is%20challenging%0Adue%20to%20low%20contrast%2C%20high%20noise%2C%20and%20susceptibility%20to%20artefacts.%20This%20process%0Arequires%20significant%20time%20and%20clinical%20expertise.%20Self-supervised%20learning%0A%28SSL%29%20offers%20a%20promising%20solution%20by%20leveraging%20unlabelled%20data%20to%20learn%20useful%0Arepresentations%2C%20enabling%20improved%20segmentation%20performance%20when%20annotated%20data%0Ais%20limited.%20Recent%20state-of-the-art%20developments%20in%20SSL%20for%20video%20data%20include%0AV-JEPA%2C%20a%20framework%20solely%20based%20on%20feature%20prediction%2C%20avoiding%20pixel%20level%0Areconstruction%20or%20negative%20samples.%20We%20hypothesise%20that%20V-JEPA%20is%20well-suited%0Ato%20ultrasound%20imaging%2C%20as%20it%20is%20less%20sensitive%20to%20noisy%20pixel-level%20detail%0Awhile%20effectively%20leveraging%20temporal%20information.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20study%20to%20adopt%20V-JEPA%20for%20ultrasound%20video%20data.%0ASimilar%20to%20other%20patch-based%20masking%20SSL%20techniques%20such%20as%20VideoMAE%2C%20V-JEPA%20is%0Awell-suited%20to%20ViT-based%20models.%20However%2C%20ViTs%20can%20underperform%20on%20small%0Amedical%20datasets%20due%20to%20lack%20of%20inductive%20biases%2C%20limited%20spatial%20locality%20and%0Aabsence%20of%20hierarchical%20feature%20learning.%20To%20improve%20locality%20understanding%2C%20we%0Apropose%20a%20novel%203D%20localisation%20auxiliary%20task%20to%20improve%20locality%20in%20ViT%0Arepresentations%20during%20V-JEPA%20pre-training.%20Our%20results%20show%20V-JEPA%20with%20our%0Aauxiliary%20task%20improves%20segmentation%20performance%20significantly%20across%20various%0Afrozen%20encoder%20configurations%2C%20with%20gains%20up%20to%203.4%5C%25%20using%20100%5C%25%20and%20up%20to%0A8.35%5C%25%20using%20only%2010%5C%25%20of%20the%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Ultrasound-Video%2520Segmentation%2520with%2520Feature%2520Prediction%250A%2520%2520and%25203D%2520Localised%2520Loss%26entry.906535625%3DEdward%2520Ellis%2520and%2520Robert%2520Mendel%2520and%2520Andrew%2520Bulpitt%2520and%2520Nasim%2520Parsa%2520and%2520Michael%2520F%2520Byrne%2520and%2520Sharib%2520Ali%26entry.1292438233%3D%2520%2520Acquiring%2520and%2520annotating%2520large%2520datasets%2520in%2520ultrasound%2520imaging%2520is%2520challenging%250Adue%2520to%2520low%2520contrast%252C%2520high%2520noise%252C%2520and%2520susceptibility%2520to%2520artefacts.%2520This%2520process%250Arequires%2520significant%2520time%2520and%2520clinical%2520expertise.%2520Self-supervised%2520learning%250A%2528SSL%2529%2520offers%2520a%2520promising%2520solution%2520by%2520leveraging%2520unlabelled%2520data%2520to%2520learn%2520useful%250Arepresentations%252C%2520enabling%2520improved%2520segmentation%2520performance%2520when%2520annotated%2520data%250Ais%2520limited.%2520Recent%2520state-of-the-art%2520developments%2520in%2520SSL%2520for%2520video%2520data%2520include%250AV-JEPA%252C%2520a%2520framework%2520solely%2520based%2520on%2520feature%2520prediction%252C%2520avoiding%2520pixel%2520level%250Areconstruction%2520or%2520negative%2520samples.%2520We%2520hypothesise%2520that%2520V-JEPA%2520is%2520well-suited%250Ato%2520ultrasound%2520imaging%252C%2520as%2520it%2520is%2520less%2520sensitive%2520to%2520noisy%2520pixel-level%2520detail%250Awhile%2520effectively%2520leveraging%2520temporal%2520information.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%2520adopt%2520V-JEPA%2520for%2520ultrasound%2520video%2520data.%250ASimilar%2520to%2520other%2520patch-based%2520masking%2520SSL%2520techniques%2520such%2520as%2520VideoMAE%252C%2520V-JEPA%2520is%250Awell-suited%2520to%2520ViT-based%2520models.%2520However%252C%2520ViTs%2520can%2520underperform%2520on%2520small%250Amedical%2520datasets%2520due%2520to%2520lack%2520of%2520inductive%2520biases%252C%2520limited%2520spatial%2520locality%2520and%250Aabsence%2520of%2520hierarchical%2520feature%2520learning.%2520To%2520improve%2520locality%2520understanding%252C%2520we%250Apropose%2520a%2520novel%25203D%2520localisation%2520auxiliary%2520task%2520to%2520improve%2520locality%2520in%2520ViT%250Arepresentations%2520during%2520V-JEPA%2520pre-training.%2520Our%2520results%2520show%2520V-JEPA%2520with%2520our%250Aauxiliary%2520task%2520improves%2520segmentation%2520performance%2520significantly%2520across%2520various%250Afrozen%2520encoder%2520configurations%252C%2520with%2520gains%2520up%2520to%25203.4%255C%2525%2520using%2520100%255C%2525%2520and%2520up%2520to%250A8.35%255C%2525%2520using%2520only%252010%255C%2525%2520of%2520the%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Ultrasound-Video%20Segmentation%20with%20Feature%20Prediction%0A%20%20and%203D%20Localised%20Loss&entry.906535625=Edward%20Ellis%20and%20Robert%20Mendel%20and%20Andrew%20Bulpitt%20and%20Nasim%20Parsa%20and%20Michael%20F%20Byrne%20and%20Sharib%20Ali&entry.1292438233=%20%20Acquiring%20and%20annotating%20large%20datasets%20in%20ultrasound%20imaging%20is%20challenging%0Adue%20to%20low%20contrast%2C%20high%20noise%2C%20and%20susceptibility%20to%20artefacts.%20This%20process%0Arequires%20significant%20time%20and%20clinical%20expertise.%20Self-supervised%20learning%0A%28SSL%29%20offers%20a%20promising%20solution%20by%20leveraging%20unlabelled%20data%20to%20learn%20useful%0Arepresentations%2C%20enabling%20improved%20segmentation%20performance%20when%20annotated%20data%0Ais%20limited.%20Recent%20state-of-the-art%20developments%20in%20SSL%20for%20video%20data%20include%0AV-JEPA%2C%20a%20framework%20solely%20based%20on%20feature%20prediction%2C%20avoiding%20pixel%20level%0Areconstruction%20or%20negative%20samples.%20We%20hypothesise%20that%20V-JEPA%20is%20well-suited%0Ato%20ultrasound%20imaging%2C%20as%20it%20is%20less%20sensitive%20to%20noisy%20pixel-level%20detail%0Awhile%20effectively%20leveraging%20temporal%20information.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20study%20to%20adopt%20V-JEPA%20for%20ultrasound%20video%20data.%0ASimilar%20to%20other%20patch-based%20masking%20SSL%20techniques%20such%20as%20VideoMAE%2C%20V-JEPA%20is%0Awell-suited%20to%20ViT-based%20models.%20However%2C%20ViTs%20can%20underperform%20on%20small%0Amedical%20datasets%20due%20to%20lack%20of%20inductive%20biases%2C%20limited%20spatial%20locality%20and%0Aabsence%20of%20hierarchical%20feature%20learning.%20To%20improve%20locality%20understanding%2C%20we%0Apropose%20a%20novel%203D%20localisation%20auxiliary%20task%20to%20improve%20locality%20in%20ViT%0Arepresentations%20during%20V-JEPA%20pre-training.%20Our%20results%20show%20V-JEPA%20with%20our%0Aauxiliary%20task%20improves%20segmentation%20performance%20significantly%20across%20various%0Afrozen%20encoder%20configurations%2C%20with%20gains%20up%20to%203.4%5C%25%20using%20100%5C%25%20and%20up%20to%0A8.35%5C%25%20using%20only%2010%5C%25%20of%20the%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18424v1&entry.124074799=Read"},
{"title": "Leveraging the Structure of Medical Data for Improved Representation\n  Learning", "author": "Andrea Agostini and Sonia Laguna and Alain Ryser and Samuel Ruiperez-Campillo and Moritz Vandenhirtz and Nicolas Deperrois and Farhad Nooralahzadeh and Michael Krauthammer and Thomas M. Sutter and Julia E. Vogt", "abstract": "  Building generalizable medical AI systems requires pretraining strategies\nthat are data-efficient and domain-aware. Unlike internet-scale corpora,\nclinical datasets such as MIMIC-CXR offer limited image counts and scarce\nannotations, but exhibit rich internal structure through multi-view imaging. We\npropose a self-supervised framework that leverages the inherent structure of\nmedical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and\nlateral views) as natural positive pairs, learning to reconstruct each view\nfrom sparse patches while aligning their latent embeddings. Our method requires\nno textual supervision and produces informative representations. Evaluated on\nMIMIC-CXR, we show strong performance compared to supervised objectives and\nbaselines being trained without leveraging structure. This work provides a\nlightweight, modality-agnostic blueprint for domain-specific pretraining where\ndata is structured but scarce\n", "link": "http://arxiv.org/abs/2507.02987v3", "date": "2025-07-24", "relevancy": 2.2446, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5717}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5681}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20the%20Structure%20of%20Medical%20Data%20for%20Improved%20Representation%0A%20%20Learning&body=Title%3A%20Leveraging%20the%20Structure%20of%20Medical%20Data%20for%20Improved%20Representation%0A%20%20Learning%0AAuthor%3A%20Andrea%20Agostini%20and%20Sonia%20Laguna%20and%20Alain%20Ryser%20and%20Samuel%20Ruiperez-Campillo%20and%20Moritz%20Vandenhirtz%20and%20Nicolas%20Deperrois%20and%20Farhad%20Nooralahzadeh%20and%20Michael%20Krauthammer%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt%0AAbstract%3A%20%20%20Building%20generalizable%20medical%20AI%20systems%20requires%20pretraining%20strategies%0Athat%20are%20data-efficient%20and%20domain-aware.%20Unlike%20internet-scale%20corpora%2C%0Aclinical%20datasets%20such%20as%20MIMIC-CXR%20offer%20limited%20image%20counts%20and%20scarce%0Aannotations%2C%20but%20exhibit%20rich%20internal%20structure%20through%20multi-view%20imaging.%20We%0Apropose%20a%20self-supervised%20framework%20that%20leverages%20the%20inherent%20structure%20of%0Amedical%20datasets.%20Specifically%2C%20we%20treat%20paired%20chest%20X-rays%20%28i.e.%2C%20frontal%20and%0Alateral%20views%29%20as%20natural%20positive%20pairs%2C%20learning%20to%20reconstruct%20each%20view%0Afrom%20sparse%20patches%20while%20aligning%20their%20latent%20embeddings.%20Our%20method%20requires%0Ano%20textual%20supervision%20and%20produces%20informative%20representations.%20Evaluated%20on%0AMIMIC-CXR%2C%20we%20show%20strong%20performance%20compared%20to%20supervised%20objectives%20and%0Abaselines%20being%20trained%20without%20leveraging%20structure.%20This%20work%20provides%20a%0Alightweight%2C%20modality-agnostic%20blueprint%20for%20domain-specific%20pretraining%20where%0Adata%20is%20structured%20but%20scarce%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02987v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520the%2520Structure%2520of%2520Medical%2520Data%2520for%2520Improved%2520Representation%250A%2520%2520Learning%26entry.906535625%3DAndrea%2520Agostini%2520and%2520Sonia%2520Laguna%2520and%2520Alain%2520Ryser%2520and%2520Samuel%2520Ruiperez-Campillo%2520and%2520Moritz%2520Vandenhirtz%2520and%2520Nicolas%2520Deperrois%2520and%2520Farhad%2520Nooralahzadeh%2520and%2520Michael%2520Krauthammer%2520and%2520Thomas%2520M.%2520Sutter%2520and%2520Julia%2520E.%2520Vogt%26entry.1292438233%3D%2520%2520Building%2520generalizable%2520medical%2520AI%2520systems%2520requires%2520pretraining%2520strategies%250Athat%2520are%2520data-efficient%2520and%2520domain-aware.%2520Unlike%2520internet-scale%2520corpora%252C%250Aclinical%2520datasets%2520such%2520as%2520MIMIC-CXR%2520offer%2520limited%2520image%2520counts%2520and%2520scarce%250Aannotations%252C%2520but%2520exhibit%2520rich%2520internal%2520structure%2520through%2520multi-view%2520imaging.%2520We%250Apropose%2520a%2520self-supervised%2520framework%2520that%2520leverages%2520the%2520inherent%2520structure%2520of%250Amedical%2520datasets.%2520Specifically%252C%2520we%2520treat%2520paired%2520chest%2520X-rays%2520%2528i.e.%252C%2520frontal%2520and%250Alateral%2520views%2529%2520as%2520natural%2520positive%2520pairs%252C%2520learning%2520to%2520reconstruct%2520each%2520view%250Afrom%2520sparse%2520patches%2520while%2520aligning%2520their%2520latent%2520embeddings.%2520Our%2520method%2520requires%250Ano%2520textual%2520supervision%2520and%2520produces%2520informative%2520representations.%2520Evaluated%2520on%250AMIMIC-CXR%252C%2520we%2520show%2520strong%2520performance%2520compared%2520to%2520supervised%2520objectives%2520and%250Abaselines%2520being%2520trained%2520without%2520leveraging%2520structure.%2520This%2520work%2520provides%2520a%250Alightweight%252C%2520modality-agnostic%2520blueprint%2520for%2520domain-specific%2520pretraining%2520where%250Adata%2520is%2520structured%2520but%2520scarce%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02987v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20the%20Structure%20of%20Medical%20Data%20for%20Improved%20Representation%0A%20%20Learning&entry.906535625=Andrea%20Agostini%20and%20Sonia%20Laguna%20and%20Alain%20Ryser%20and%20Samuel%20Ruiperez-Campillo%20and%20Moritz%20Vandenhirtz%20and%20Nicolas%20Deperrois%20and%20Farhad%20Nooralahzadeh%20and%20Michael%20Krauthammer%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt&entry.1292438233=%20%20Building%20generalizable%20medical%20AI%20systems%20requires%20pretraining%20strategies%0Athat%20are%20data-efficient%20and%20domain-aware.%20Unlike%20internet-scale%20corpora%2C%0Aclinical%20datasets%20such%20as%20MIMIC-CXR%20offer%20limited%20image%20counts%20and%20scarce%0Aannotations%2C%20but%20exhibit%20rich%20internal%20structure%20through%20multi-view%20imaging.%20We%0Apropose%20a%20self-supervised%20framework%20that%20leverages%20the%20inherent%20structure%20of%0Amedical%20datasets.%20Specifically%2C%20we%20treat%20paired%20chest%20X-rays%20%28i.e.%2C%20frontal%20and%0Alateral%20views%29%20as%20natural%20positive%20pairs%2C%20learning%20to%20reconstruct%20each%20view%0Afrom%20sparse%20patches%20while%20aligning%20their%20latent%20embeddings.%20Our%20method%20requires%0Ano%20textual%20supervision%20and%20produces%20informative%20representations.%20Evaluated%20on%0AMIMIC-CXR%2C%20we%20show%20strong%20performance%20compared%20to%20supervised%20objectives%20and%0Abaselines%20being%20trained%20without%20leveraging%20structure.%20This%20work%20provides%20a%0Alightweight%2C%20modality-agnostic%20blueprint%20for%20domain-specific%20pretraining%20where%0Adata%20is%20structured%20but%20scarce%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02987v3&entry.124074799=Read"},
{"title": "PLOT-TAL: Prompt Learning with Optimal Transport for Few-Shot Temporal\n  Action Localization", "author": "Edward Fish and Andrew Gilbert", "abstract": "  Few-shot temporal action localization (TAL) methods that adapt large models\nvia single-prompt tuning often fail to produce precise temporal boundaries.\nThis stems from the model learning a non-discriminative mean representation of\nan action from sparse data, which compromises generalization. We address this\nby proposing a new paradigm based on multi-prompt ensembles, where a set of\ndiverse, learnable prompts for each action is encouraged to specialize on\ncompositional sub-events. To enforce this specialization, we introduce\nPLOT-TAL, a framework that leverages Optimal Transport (OT) to find a globally\noptimal alignment between the prompt ensemble and the video's temporal\nfeatures. Our method establishes a new state-of-the-art on the challenging\nfew-shot benchmarks of THUMOS'14 and EPIC-Kitchens, without requiring complex\nmeta-learning. The significant performance gains, particularly at high IoU\nthresholds, validate our hypothesis and demonstrate the superiority of learning\ndistributed, compositional representations for precise temporal localization.\n", "link": "http://arxiv.org/abs/2403.18915v2", "date": "2025-07-24", "relevancy": 2.2418, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.567}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.556}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLOT-TAL%3A%20Prompt%20Learning%20with%20Optimal%20Transport%20for%20Few-Shot%20Temporal%0A%20%20Action%20Localization&body=Title%3A%20PLOT-TAL%3A%20Prompt%20Learning%20with%20Optimal%20Transport%20for%20Few-Shot%20Temporal%0A%20%20Action%20Localization%0AAuthor%3A%20Edward%20Fish%20and%20Andrew%20Gilbert%0AAbstract%3A%20%20%20Few-shot%20temporal%20action%20localization%20%28TAL%29%20methods%20that%20adapt%20large%20models%0Avia%20single-prompt%20tuning%20often%20fail%20to%20produce%20precise%20temporal%20boundaries.%0AThis%20stems%20from%20the%20model%20learning%20a%20non-discriminative%20mean%20representation%20of%0Aan%20action%20from%20sparse%20data%2C%20which%20compromises%20generalization.%20We%20address%20this%0Aby%20proposing%20a%20new%20paradigm%20based%20on%20multi-prompt%20ensembles%2C%20where%20a%20set%20of%0Adiverse%2C%20learnable%20prompts%20for%20each%20action%20is%20encouraged%20to%20specialize%20on%0Acompositional%20sub-events.%20To%20enforce%20this%20specialization%2C%20we%20introduce%0APLOT-TAL%2C%20a%20framework%20that%20leverages%20Optimal%20Transport%20%28OT%29%20to%20find%20a%20globally%0Aoptimal%20alignment%20between%20the%20prompt%20ensemble%20and%20the%20video%27s%20temporal%0Afeatures.%20Our%20method%20establishes%20a%20new%20state-of-the-art%20on%20the%20challenging%0Afew-shot%20benchmarks%20of%20THUMOS%2714%20and%20EPIC-Kitchens%2C%20without%20requiring%20complex%0Ameta-learning.%20The%20significant%20performance%20gains%2C%20particularly%20at%20high%20IoU%0Athresholds%2C%20validate%20our%20hypothesis%20and%20demonstrate%20the%20superiority%20of%20learning%0Adistributed%2C%20compositional%20representations%20for%20precise%20temporal%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18915v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLOT-TAL%253A%2520Prompt%2520Learning%2520with%2520Optimal%2520Transport%2520for%2520Few-Shot%2520Temporal%250A%2520%2520Action%2520Localization%26entry.906535625%3DEdward%2520Fish%2520and%2520Andrew%2520Gilbert%26entry.1292438233%3D%2520%2520Few-shot%2520temporal%2520action%2520localization%2520%2528TAL%2529%2520methods%2520that%2520adapt%2520large%2520models%250Avia%2520single-prompt%2520tuning%2520often%2520fail%2520to%2520produce%2520precise%2520temporal%2520boundaries.%250AThis%2520stems%2520from%2520the%2520model%2520learning%2520a%2520non-discriminative%2520mean%2520representation%2520of%250Aan%2520action%2520from%2520sparse%2520data%252C%2520which%2520compromises%2520generalization.%2520We%2520address%2520this%250Aby%2520proposing%2520a%2520new%2520paradigm%2520based%2520on%2520multi-prompt%2520ensembles%252C%2520where%2520a%2520set%2520of%250Adiverse%252C%2520learnable%2520prompts%2520for%2520each%2520action%2520is%2520encouraged%2520to%2520specialize%2520on%250Acompositional%2520sub-events.%2520To%2520enforce%2520this%2520specialization%252C%2520we%2520introduce%250APLOT-TAL%252C%2520a%2520framework%2520that%2520leverages%2520Optimal%2520Transport%2520%2528OT%2529%2520to%2520find%2520a%2520globally%250Aoptimal%2520alignment%2520between%2520the%2520prompt%2520ensemble%2520and%2520the%2520video%2527s%2520temporal%250Afeatures.%2520Our%2520method%2520establishes%2520a%2520new%2520state-of-the-art%2520on%2520the%2520challenging%250Afew-shot%2520benchmarks%2520of%2520THUMOS%252714%2520and%2520EPIC-Kitchens%252C%2520without%2520requiring%2520complex%250Ameta-learning.%2520The%2520significant%2520performance%2520gains%252C%2520particularly%2520at%2520high%2520IoU%250Athresholds%252C%2520validate%2520our%2520hypothesis%2520and%2520demonstrate%2520the%2520superiority%2520of%2520learning%250Adistributed%252C%2520compositional%2520representations%2520for%2520precise%2520temporal%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18915v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLOT-TAL%3A%20Prompt%20Learning%20with%20Optimal%20Transport%20for%20Few-Shot%20Temporal%0A%20%20Action%20Localization&entry.906535625=Edward%20Fish%20and%20Andrew%20Gilbert&entry.1292438233=%20%20Few-shot%20temporal%20action%20localization%20%28TAL%29%20methods%20that%20adapt%20large%20models%0Avia%20single-prompt%20tuning%20often%20fail%20to%20produce%20precise%20temporal%20boundaries.%0AThis%20stems%20from%20the%20model%20learning%20a%20non-discriminative%20mean%20representation%20of%0Aan%20action%20from%20sparse%20data%2C%20which%20compromises%20generalization.%20We%20address%20this%0Aby%20proposing%20a%20new%20paradigm%20based%20on%20multi-prompt%20ensembles%2C%20where%20a%20set%20of%0Adiverse%2C%20learnable%20prompts%20for%20each%20action%20is%20encouraged%20to%20specialize%20on%0Acompositional%20sub-events.%20To%20enforce%20this%20specialization%2C%20we%20introduce%0APLOT-TAL%2C%20a%20framework%20that%20leverages%20Optimal%20Transport%20%28OT%29%20to%20find%20a%20globally%0Aoptimal%20alignment%20between%20the%20prompt%20ensemble%20and%20the%20video%27s%20temporal%0Afeatures.%20Our%20method%20establishes%20a%20new%20state-of-the-art%20on%20the%20challenging%0Afew-shot%20benchmarks%20of%20THUMOS%2714%20and%20EPIC-Kitchens%2C%20without%20requiring%20complex%0Ameta-learning.%20The%20significant%20performance%20gains%2C%20particularly%20at%20high%20IoU%0Athresholds%2C%20validate%20our%20hypothesis%20and%20demonstrate%20the%20superiority%20of%20learning%0Adistributed%2C%20compositional%20representations%20for%20precise%20temporal%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18915v2&entry.124074799=Read"},
{"title": "IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level\n  Generation", "author": "In-Chang Baek and Sung-Hyun Kim and Seo-Young Lee and Dong-Hyeon Kim and Kyung-Joong Kim", "abstract": "  Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.\n", "link": "http://arxiv.org/abs/2503.12358v4", "date": "2025-07-24", "relevancy": 2.2395, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.588}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5439}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IPCGRL%3A%20Language-Instructed%20Reinforcement%20Learning%20for%20Procedural%20Level%0A%20%20Generation&body=Title%3A%20IPCGRL%3A%20Language-Instructed%20Reinforcement%20Learning%20for%20Procedural%20Level%0A%20%20Generation%0AAuthor%3A%20In-Chang%20Baek%20and%20Sung-Hyun%20Kim%20and%20Seo-Young%20Lee%20and%20Dong-Hyeon%20Kim%20and%20Kyung-Joong%20Kim%0AAbstract%3A%20%20%20Recent%20research%20has%20highlighted%20the%20significance%20of%20natural%20language%20in%0Aenhancing%20the%20controllability%20of%20generative%20models.%20While%20various%20efforts%20have%0Abeen%20made%20to%20leverage%20natural%20language%20for%20content%20generation%2C%20research%20on%20deep%0Areinforcement%20learning%20%28DRL%29%20agents%20utilizing%20text-based%20instructions%20for%0Aprocedural%20content%20generation%20remains%20limited.%20In%20this%20paper%2C%20we%20propose%0AIPCGRL%2C%20an%20instruction-based%20procedural%20content%20generation%20method%20via%0Areinforcement%20learning%2C%20which%20incorporates%20a%20sentence%20embedding%20model.%20IPCGRL%0Afine-tunes%20task-specific%20embedding%20representations%20to%20effectively%20compress%0Agame-level%20conditions.%20We%20evaluate%20IPCGRL%20in%20a%20two-dimensional%20level%20generation%0Atask%20and%20compare%20its%20performance%20with%20a%20general-purpose%20embedding%20method.%20The%0Aresults%20indicate%20that%20IPCGRL%20achieves%20up%20to%20a%2021.4%25%20improvement%20in%0Acontrollability%20and%20a%2017.2%25%20improvement%20in%20generalizability%20for%20unseen%0Ainstructions.%20Furthermore%2C%20the%20proposed%20method%20extends%20the%20modality%20of%0Aconditional%20input%2C%20enabling%20a%20more%20flexible%20and%20expressive%20interaction%0Aframework%20for%20procedural%20content%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12358v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIPCGRL%253A%2520Language-Instructed%2520Reinforcement%2520Learning%2520for%2520Procedural%2520Level%250A%2520%2520Generation%26entry.906535625%3DIn-Chang%2520Baek%2520and%2520Sung-Hyun%2520Kim%2520and%2520Seo-Young%2520Lee%2520and%2520Dong-Hyeon%2520Kim%2520and%2520Kyung-Joong%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520highlighted%2520the%2520significance%2520of%2520natural%2520language%2520in%250Aenhancing%2520the%2520controllability%2520of%2520generative%2520models.%2520While%2520various%2520efforts%2520have%250Abeen%2520made%2520to%2520leverage%2520natural%2520language%2520for%2520content%2520generation%252C%2520research%2520on%2520deep%250Areinforcement%2520learning%2520%2528DRL%2529%2520agents%2520utilizing%2520text-based%2520instructions%2520for%250Aprocedural%2520content%2520generation%2520remains%2520limited.%2520In%2520this%2520paper%252C%2520we%2520propose%250AIPCGRL%252C%2520an%2520instruction-based%2520procedural%2520content%2520generation%2520method%2520via%250Areinforcement%2520learning%252C%2520which%2520incorporates%2520a%2520sentence%2520embedding%2520model.%2520IPCGRL%250Afine-tunes%2520task-specific%2520embedding%2520representations%2520to%2520effectively%2520compress%250Agame-level%2520conditions.%2520We%2520evaluate%2520IPCGRL%2520in%2520a%2520two-dimensional%2520level%2520generation%250Atask%2520and%2520compare%2520its%2520performance%2520with%2520a%2520general-purpose%2520embedding%2520method.%2520The%250Aresults%2520indicate%2520that%2520IPCGRL%2520achieves%2520up%2520to%2520a%252021.4%2525%2520improvement%2520in%250Acontrollability%2520and%2520a%252017.2%2525%2520improvement%2520in%2520generalizability%2520for%2520unseen%250Ainstructions.%2520Furthermore%252C%2520the%2520proposed%2520method%2520extends%2520the%2520modality%2520of%250Aconditional%2520input%252C%2520enabling%2520a%2520more%2520flexible%2520and%2520expressive%2520interaction%250Aframework%2520for%2520procedural%2520content%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12358v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPCGRL%3A%20Language-Instructed%20Reinforcement%20Learning%20for%20Procedural%20Level%0A%20%20Generation&entry.906535625=In-Chang%20Baek%20and%20Sung-Hyun%20Kim%20and%20Seo-Young%20Lee%20and%20Dong-Hyeon%20Kim%20and%20Kyung-Joong%20Kim&entry.1292438233=%20%20Recent%20research%20has%20highlighted%20the%20significance%20of%20natural%20language%20in%0Aenhancing%20the%20controllability%20of%20generative%20models.%20While%20various%20efforts%20have%0Abeen%20made%20to%20leverage%20natural%20language%20for%20content%20generation%2C%20research%20on%20deep%0Areinforcement%20learning%20%28DRL%29%20agents%20utilizing%20text-based%20instructions%20for%0Aprocedural%20content%20generation%20remains%20limited.%20In%20this%20paper%2C%20we%20propose%0AIPCGRL%2C%20an%20instruction-based%20procedural%20content%20generation%20method%20via%0Areinforcement%20learning%2C%20which%20incorporates%20a%20sentence%20embedding%20model.%20IPCGRL%0Afine-tunes%20task-specific%20embedding%20representations%20to%20effectively%20compress%0Agame-level%20conditions.%20We%20evaluate%20IPCGRL%20in%20a%20two-dimensional%20level%20generation%0Atask%20and%20compare%20its%20performance%20with%20a%20general-purpose%20embedding%20method.%20The%0Aresults%20indicate%20that%20IPCGRL%20achieves%20up%20to%20a%2021.4%25%20improvement%20in%0Acontrollability%20and%20a%2017.2%25%20improvement%20in%20generalizability%20for%20unseen%0Ainstructions.%20Furthermore%2C%20the%20proposed%20method%20extends%20the%20modality%20of%0Aconditional%20input%2C%20enabling%20a%20more%20flexible%20and%20expressive%20interaction%0Aframework%20for%20procedural%20content%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12358v4&entry.124074799=Read"},
{"title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive\n  Generation", "author": "Zhekai Chen and Ruihang Chu and Yukang Chen and Shiwei Zhang and Yujie Wei and Yingya Zhang and Xihui Liu", "abstract": "  Scaling visual generation models is essential for real-world content\ncreation, yet requires substantial training and computational expenses.\nAlternatively, test-time scaling has garnered growing attention due to resource\nefficiency and promising performance. In this work, we present TTS-VAR, the\nfirst general test-time scaling framework for visual auto-regressive (VAR)\nmodels, modeling the generation process as a path searching problem. To\ndynamically balance computational efficiency with exploration capacity, we\nfirst introduce an adaptive descending batch size schedule throughout the\ncausal generation process. Besides, inspired by VAR's hierarchical\ncoarse-to-fine multi-scale generation, our framework integrates two key\ncomponents: (i) At coarse scales, we observe that generated tokens are hard for\nevaluation, possibly leading to erroneous acceptance of inferior samples or\nrejection of superior samples. Noticing that the coarse scales contain\nsufficient structural information, we propose clustering-based diversity\nsearch. It preserves structural variety through semantic feature clustering,\nenabling later selection on samples with higher potential. (ii) In fine scales,\nresampling-based potential selection prioritizes promising candidates using\npotential scores, which are defined as reward functions incorporating\nmulti-scale generation history. Experiments on the powerful VAR model Infinity\nshow a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights\nreveal that early-stage structural features effectively influence final\nquality, and resampling efficacy varies across generation scales. Code is\navailable at https://github.com/ali-vilab/TTS-VAR.\n", "link": "http://arxiv.org/abs/2507.18537v1", "date": "2025-07-24", "relevancy": 2.2276, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5904}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5634}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTS-VAR%3A%20A%20Test-Time%20Scaling%20Framework%20for%20Visual%20Auto-Regressive%0A%20%20Generation&body=Title%3A%20TTS-VAR%3A%20A%20Test-Time%20Scaling%20Framework%20for%20Visual%20Auto-Regressive%0A%20%20Generation%0AAuthor%3A%20Zhekai%20Chen%20and%20Ruihang%20Chu%20and%20Yukang%20Chen%20and%20Shiwei%20Zhang%20and%20Yujie%20Wei%20and%20Yingya%20Zhang%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Scaling%20visual%20generation%20models%20is%20essential%20for%20real-world%20content%0Acreation%2C%20yet%20requires%20substantial%20training%20and%20computational%20expenses.%0AAlternatively%2C%20test-time%20scaling%20has%20garnered%20growing%20attention%20due%20to%20resource%0Aefficiency%20and%20promising%20performance.%20In%20this%20work%2C%20we%20present%20TTS-VAR%2C%20the%0Afirst%20general%20test-time%20scaling%20framework%20for%20visual%20auto-regressive%20%28VAR%29%0Amodels%2C%20modeling%20the%20generation%20process%20as%20a%20path%20searching%20problem.%20To%0Adynamically%20balance%20computational%20efficiency%20with%20exploration%20capacity%2C%20we%0Afirst%20introduce%20an%20adaptive%20descending%20batch%20size%20schedule%20throughout%20the%0Acausal%20generation%20process.%20Besides%2C%20inspired%20by%20VAR%27s%20hierarchical%0Acoarse-to-fine%20multi-scale%20generation%2C%20our%20framework%20integrates%20two%20key%0Acomponents%3A%20%28i%29%20At%20coarse%20scales%2C%20we%20observe%20that%20generated%20tokens%20are%20hard%20for%0Aevaluation%2C%20possibly%20leading%20to%20erroneous%20acceptance%20of%20inferior%20samples%20or%0Arejection%20of%20superior%20samples.%20Noticing%20that%20the%20coarse%20scales%20contain%0Asufficient%20structural%20information%2C%20we%20propose%20clustering-based%20diversity%0Asearch.%20It%20preserves%20structural%20variety%20through%20semantic%20feature%20clustering%2C%0Aenabling%20later%20selection%20on%20samples%20with%20higher%20potential.%20%28ii%29%20In%20fine%20scales%2C%0Aresampling-based%20potential%20selection%20prioritizes%20promising%20candidates%20using%0Apotential%20scores%2C%20which%20are%20defined%20as%20reward%20functions%20incorporating%0Amulti-scale%20generation%20history.%20Experiments%20on%20the%20powerful%20VAR%20model%20Infinity%0Ashow%20a%20notable%208.7%25%20GenEval%20score%20improvement%20%28from%200.69%20to%200.75%29.%20Key%20insights%0Areveal%20that%20early-stage%20structural%20features%20effectively%20influence%20final%0Aquality%2C%20and%20resampling%20efficacy%20varies%20across%20generation%20scales.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ali-vilab/TTS-VAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTS-VAR%253A%2520A%2520Test-Time%2520Scaling%2520Framework%2520for%2520Visual%2520Auto-Regressive%250A%2520%2520Generation%26entry.906535625%3DZhekai%2520Chen%2520and%2520Ruihang%2520Chu%2520and%2520Yukang%2520Chen%2520and%2520Shiwei%2520Zhang%2520and%2520Yujie%2520Wei%2520and%2520Yingya%2520Zhang%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Scaling%2520visual%2520generation%2520models%2520is%2520essential%2520for%2520real-world%2520content%250Acreation%252C%2520yet%2520requires%2520substantial%2520training%2520and%2520computational%2520expenses.%250AAlternatively%252C%2520test-time%2520scaling%2520has%2520garnered%2520growing%2520attention%2520due%2520to%2520resource%250Aefficiency%2520and%2520promising%2520performance.%2520In%2520this%2520work%252C%2520we%2520present%2520TTS-VAR%252C%2520the%250Afirst%2520general%2520test-time%2520scaling%2520framework%2520for%2520visual%2520auto-regressive%2520%2528VAR%2529%250Amodels%252C%2520modeling%2520the%2520generation%2520process%2520as%2520a%2520path%2520searching%2520problem.%2520To%250Adynamically%2520balance%2520computational%2520efficiency%2520with%2520exploration%2520capacity%252C%2520we%250Afirst%2520introduce%2520an%2520adaptive%2520descending%2520batch%2520size%2520schedule%2520throughout%2520the%250Acausal%2520generation%2520process.%2520Besides%252C%2520inspired%2520by%2520VAR%2527s%2520hierarchical%250Acoarse-to-fine%2520multi-scale%2520generation%252C%2520our%2520framework%2520integrates%2520two%2520key%250Acomponents%253A%2520%2528i%2529%2520At%2520coarse%2520scales%252C%2520we%2520observe%2520that%2520generated%2520tokens%2520are%2520hard%2520for%250Aevaluation%252C%2520possibly%2520leading%2520to%2520erroneous%2520acceptance%2520of%2520inferior%2520samples%2520or%250Arejection%2520of%2520superior%2520samples.%2520Noticing%2520that%2520the%2520coarse%2520scales%2520contain%250Asufficient%2520structural%2520information%252C%2520we%2520propose%2520clustering-based%2520diversity%250Asearch.%2520It%2520preserves%2520structural%2520variety%2520through%2520semantic%2520feature%2520clustering%252C%250Aenabling%2520later%2520selection%2520on%2520samples%2520with%2520higher%2520potential.%2520%2528ii%2529%2520In%2520fine%2520scales%252C%250Aresampling-based%2520potential%2520selection%2520prioritizes%2520promising%2520candidates%2520using%250Apotential%2520scores%252C%2520which%2520are%2520defined%2520as%2520reward%2520functions%2520incorporating%250Amulti-scale%2520generation%2520history.%2520Experiments%2520on%2520the%2520powerful%2520VAR%2520model%2520Infinity%250Ashow%2520a%2520notable%25208.7%2525%2520GenEval%2520score%2520improvement%2520%2528from%25200.69%2520to%25200.75%2529.%2520Key%2520insights%250Areveal%2520that%2520early-stage%2520structural%2520features%2520effectively%2520influence%2520final%250Aquality%252C%2520and%2520resampling%2520efficacy%2520varies%2520across%2520generation%2520scales.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/ali-vilab/TTS-VAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTS-VAR%3A%20A%20Test-Time%20Scaling%20Framework%20for%20Visual%20Auto-Regressive%0A%20%20Generation&entry.906535625=Zhekai%20Chen%20and%20Ruihang%20Chu%20and%20Yukang%20Chen%20and%20Shiwei%20Zhang%20and%20Yujie%20Wei%20and%20Yingya%20Zhang%20and%20Xihui%20Liu&entry.1292438233=%20%20Scaling%20visual%20generation%20models%20is%20essential%20for%20real-world%20content%0Acreation%2C%20yet%20requires%20substantial%20training%20and%20computational%20expenses.%0AAlternatively%2C%20test-time%20scaling%20has%20garnered%20growing%20attention%20due%20to%20resource%0Aefficiency%20and%20promising%20performance.%20In%20this%20work%2C%20we%20present%20TTS-VAR%2C%20the%0Afirst%20general%20test-time%20scaling%20framework%20for%20visual%20auto-regressive%20%28VAR%29%0Amodels%2C%20modeling%20the%20generation%20process%20as%20a%20path%20searching%20problem.%20To%0Adynamically%20balance%20computational%20efficiency%20with%20exploration%20capacity%2C%20we%0Afirst%20introduce%20an%20adaptive%20descending%20batch%20size%20schedule%20throughout%20the%0Acausal%20generation%20process.%20Besides%2C%20inspired%20by%20VAR%27s%20hierarchical%0Acoarse-to-fine%20multi-scale%20generation%2C%20our%20framework%20integrates%20two%20key%0Acomponents%3A%20%28i%29%20At%20coarse%20scales%2C%20we%20observe%20that%20generated%20tokens%20are%20hard%20for%0Aevaluation%2C%20possibly%20leading%20to%20erroneous%20acceptance%20of%20inferior%20samples%20or%0Arejection%20of%20superior%20samples.%20Noticing%20that%20the%20coarse%20scales%20contain%0Asufficient%20structural%20information%2C%20we%20propose%20clustering-based%20diversity%0Asearch.%20It%20preserves%20structural%20variety%20through%20semantic%20feature%20clustering%2C%0Aenabling%20later%20selection%20on%20samples%20with%20higher%20potential.%20%28ii%29%20In%20fine%20scales%2C%0Aresampling-based%20potential%20selection%20prioritizes%20promising%20candidates%20using%0Apotential%20scores%2C%20which%20are%20defined%20as%20reward%20functions%20incorporating%0Amulti-scale%20generation%20history.%20Experiments%20on%20the%20powerful%20VAR%20model%20Infinity%0Ashow%20a%20notable%208.7%25%20GenEval%20score%20improvement%20%28from%200.69%20to%200.75%29.%20Key%20insights%0Areveal%20that%20early-stage%20structural%20features%20effectively%20influence%20final%0Aquality%2C%20and%20resampling%20efficacy%20varies%20across%20generation%20scales.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ali-vilab/TTS-VAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18537v1&entry.124074799=Read"},
{"title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate\n  Representation", "author": "Shuqing Li and Anson Y. Lam and Yun Peng and Wenxuan Wang and Michael R. Lyu", "abstract": "  Graphical user interface (UI) software has undergone a fundamental\ntransformation from traditional two-dimensional (2D) desktop/web/mobile\ninterfaces to spatial three-dimensional (3D) environments. While existing work\nhas made remarkable success in automated 2D software generation, such as\nHTML/CSS and mobile app interface code synthesis, the generation of 3D software\nstill remains under-explored. Current methods for 3D software generation\nusually generate the 3D environments as a whole and cannot modify or control\nspecific elements in the software. Furthermore, these methods struggle to\nhandle the complex spatial and semantic constraints inherent in the real world.\nTo address the challenges, we present Scenethesis, a novel\nrequirement-sensitive 3D software synthesis approach that maintains formal\ntraceability between user specifications and generated 3D software. Scenethesis\nis built upon ScenethesisLang, a domain-specific language that serves as a\ngranular constraint-aware intermediate representation (IR) to bridge natural\nlanguage requirements and executable 3D software. It serves both as a\ncomprehensive scene description language enabling fine-grained modification of\n3D software elements and as a formal constraint-expressive specification\nlanguage capable of expressing complex spatial constraints. By decomposing 3D\nsoftware synthesis into stages operating on ScenethesisLang, Scenethesis\nenables independent verification, targeted modification, and systematic\nconstraint satisfaction. Our evaluation demonstrates that Scenethesis\naccurately captures over 80% of user requirements and satisfies more than 90%\nof hard constraints while handling over 100 constraints simultaneously.\nFurthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual\nevaluation scores compared to the state-of-the-art method.\n", "link": "http://arxiv.org/abs/2507.18625v1", "date": "2025-07-24", "relevancy": 2.2248, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5568}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Software%20Synthesis%20Guided%20by%20Constraint-Expressive%20Intermediate%0A%20%20Representation&body=Title%3A%203D%20Software%20Synthesis%20Guided%20by%20Constraint-Expressive%20Intermediate%0A%20%20Representation%0AAuthor%3A%20Shuqing%20Li%20and%20Anson%20Y.%20Lam%20and%20Yun%20Peng%20and%20Wenxuan%20Wang%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Graphical%20user%20interface%20%28UI%29%20software%20has%20undergone%20a%20fundamental%0Atransformation%20from%20traditional%20two-dimensional%20%282D%29%20desktop/web/mobile%0Ainterfaces%20to%20spatial%20three-dimensional%20%283D%29%20environments.%20While%20existing%20work%0Ahas%20made%20remarkable%20success%20in%20automated%202D%20software%20generation%2C%20such%20as%0AHTML/CSS%20and%20mobile%20app%20interface%20code%20synthesis%2C%20the%20generation%20of%203D%20software%0Astill%20remains%20under-explored.%20Current%20methods%20for%203D%20software%20generation%0Ausually%20generate%20the%203D%20environments%20as%20a%20whole%20and%20cannot%20modify%20or%20control%0Aspecific%20elements%20in%20the%20software.%20Furthermore%2C%20these%20methods%20struggle%20to%0Ahandle%20the%20complex%20spatial%20and%20semantic%20constraints%20inherent%20in%20the%20real%20world.%0ATo%20address%20the%20challenges%2C%20we%20present%20Scenethesis%2C%20a%20novel%0Arequirement-sensitive%203D%20software%20synthesis%20approach%20that%20maintains%20formal%0Atraceability%20between%20user%20specifications%20and%20generated%203D%20software.%20Scenethesis%0Ais%20built%20upon%20ScenethesisLang%2C%20a%20domain-specific%20language%20that%20serves%20as%20a%0Agranular%20constraint-aware%20intermediate%20representation%20%28IR%29%20to%20bridge%20natural%0Alanguage%20requirements%20and%20executable%203D%20software.%20It%20serves%20both%20as%20a%0Acomprehensive%20scene%20description%20language%20enabling%20fine-grained%20modification%20of%0A3D%20software%20elements%20and%20as%20a%20formal%20constraint-expressive%20specification%0Alanguage%20capable%20of%20expressing%20complex%20spatial%20constraints.%20By%20decomposing%203D%0Asoftware%20synthesis%20into%20stages%20operating%20on%20ScenethesisLang%2C%20Scenethesis%0Aenables%20independent%20verification%2C%20targeted%20modification%2C%20and%20systematic%0Aconstraint%20satisfaction.%20Our%20evaluation%20demonstrates%20that%20Scenethesis%0Aaccurately%20captures%20over%2080%25%20of%20user%20requirements%20and%20satisfies%20more%20than%2090%25%0Aof%20hard%20constraints%20while%20handling%20over%20100%20constraints%20simultaneously.%0AFurthermore%2C%20Scenethesis%20achieves%20a%2042.8%25%20improvement%20in%20BLIP-2%20visual%0Aevaluation%20scores%20compared%20to%20the%20state-of-the-art%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Software%2520Synthesis%2520Guided%2520by%2520Constraint-Expressive%2520Intermediate%250A%2520%2520Representation%26entry.906535625%3DShuqing%2520Li%2520and%2520Anson%2520Y.%2520Lam%2520and%2520Yun%2520Peng%2520and%2520Wenxuan%2520Wang%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Graphical%2520user%2520interface%2520%2528UI%2529%2520software%2520has%2520undergone%2520a%2520fundamental%250Atransformation%2520from%2520traditional%2520two-dimensional%2520%25282D%2529%2520desktop/web/mobile%250Ainterfaces%2520to%2520spatial%2520three-dimensional%2520%25283D%2529%2520environments.%2520While%2520existing%2520work%250Ahas%2520made%2520remarkable%2520success%2520in%2520automated%25202D%2520software%2520generation%252C%2520such%2520as%250AHTML/CSS%2520and%2520mobile%2520app%2520interface%2520code%2520synthesis%252C%2520the%2520generation%2520of%25203D%2520software%250Astill%2520remains%2520under-explored.%2520Current%2520methods%2520for%25203D%2520software%2520generation%250Ausually%2520generate%2520the%25203D%2520environments%2520as%2520a%2520whole%2520and%2520cannot%2520modify%2520or%2520control%250Aspecific%2520elements%2520in%2520the%2520software.%2520Furthermore%252C%2520these%2520methods%2520struggle%2520to%250Ahandle%2520the%2520complex%2520spatial%2520and%2520semantic%2520constraints%2520inherent%2520in%2520the%2520real%2520world.%250ATo%2520address%2520the%2520challenges%252C%2520we%2520present%2520Scenethesis%252C%2520a%2520novel%250Arequirement-sensitive%25203D%2520software%2520synthesis%2520approach%2520that%2520maintains%2520formal%250Atraceability%2520between%2520user%2520specifications%2520and%2520generated%25203D%2520software.%2520Scenethesis%250Ais%2520built%2520upon%2520ScenethesisLang%252C%2520a%2520domain-specific%2520language%2520that%2520serves%2520as%2520a%250Agranular%2520constraint-aware%2520intermediate%2520representation%2520%2528IR%2529%2520to%2520bridge%2520natural%250Alanguage%2520requirements%2520and%2520executable%25203D%2520software.%2520It%2520serves%2520both%2520as%2520a%250Acomprehensive%2520scene%2520description%2520language%2520enabling%2520fine-grained%2520modification%2520of%250A3D%2520software%2520elements%2520and%2520as%2520a%2520formal%2520constraint-expressive%2520specification%250Alanguage%2520capable%2520of%2520expressing%2520complex%2520spatial%2520constraints.%2520By%2520decomposing%25203D%250Asoftware%2520synthesis%2520into%2520stages%2520operating%2520on%2520ScenethesisLang%252C%2520Scenethesis%250Aenables%2520independent%2520verification%252C%2520targeted%2520modification%252C%2520and%2520systematic%250Aconstraint%2520satisfaction.%2520Our%2520evaluation%2520demonstrates%2520that%2520Scenethesis%250Aaccurately%2520captures%2520over%252080%2525%2520of%2520user%2520requirements%2520and%2520satisfies%2520more%2520than%252090%2525%250Aof%2520hard%2520constraints%2520while%2520handling%2520over%2520100%2520constraints%2520simultaneously.%250AFurthermore%252C%2520Scenethesis%2520achieves%2520a%252042.8%2525%2520improvement%2520in%2520BLIP-2%2520visual%250Aevaluation%2520scores%2520compared%2520to%2520the%2520state-of-the-art%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Software%20Synthesis%20Guided%20by%20Constraint-Expressive%20Intermediate%0A%20%20Representation&entry.906535625=Shuqing%20Li%20and%20Anson%20Y.%20Lam%20and%20Yun%20Peng%20and%20Wenxuan%20Wang%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Graphical%20user%20interface%20%28UI%29%20software%20has%20undergone%20a%20fundamental%0Atransformation%20from%20traditional%20two-dimensional%20%282D%29%20desktop/web/mobile%0Ainterfaces%20to%20spatial%20three-dimensional%20%283D%29%20environments.%20While%20existing%20work%0Ahas%20made%20remarkable%20success%20in%20automated%202D%20software%20generation%2C%20such%20as%0AHTML/CSS%20and%20mobile%20app%20interface%20code%20synthesis%2C%20the%20generation%20of%203D%20software%0Astill%20remains%20under-explored.%20Current%20methods%20for%203D%20software%20generation%0Ausually%20generate%20the%203D%20environments%20as%20a%20whole%20and%20cannot%20modify%20or%20control%0Aspecific%20elements%20in%20the%20software.%20Furthermore%2C%20these%20methods%20struggle%20to%0Ahandle%20the%20complex%20spatial%20and%20semantic%20constraints%20inherent%20in%20the%20real%20world.%0ATo%20address%20the%20challenges%2C%20we%20present%20Scenethesis%2C%20a%20novel%0Arequirement-sensitive%203D%20software%20synthesis%20approach%20that%20maintains%20formal%0Atraceability%20between%20user%20specifications%20and%20generated%203D%20software.%20Scenethesis%0Ais%20built%20upon%20ScenethesisLang%2C%20a%20domain-specific%20language%20that%20serves%20as%20a%0Agranular%20constraint-aware%20intermediate%20representation%20%28IR%29%20to%20bridge%20natural%0Alanguage%20requirements%20and%20executable%203D%20software.%20It%20serves%20both%20as%20a%0Acomprehensive%20scene%20description%20language%20enabling%20fine-grained%20modification%20of%0A3D%20software%20elements%20and%20as%20a%20formal%20constraint-expressive%20specification%0Alanguage%20capable%20of%20expressing%20complex%20spatial%20constraints.%20By%20decomposing%203D%0Asoftware%20synthesis%20into%20stages%20operating%20on%20ScenethesisLang%2C%20Scenethesis%0Aenables%20independent%20verification%2C%20targeted%20modification%2C%20and%20systematic%0Aconstraint%20satisfaction.%20Our%20evaluation%20demonstrates%20that%20Scenethesis%0Aaccurately%20captures%20over%2080%25%20of%20user%20requirements%20and%20satisfies%20more%20than%2090%25%0Aof%20hard%20constraints%20while%20handling%20over%20100%20constraints%20simultaneously.%0AFurthermore%2C%20Scenethesis%20achieves%20a%2042.8%25%20improvement%20in%20BLIP-2%20visual%0Aevaluation%20scores%20compared%20to%20the%20state-of-the-art%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18625v1&entry.124074799=Read"},
{"title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with\n  Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control", "author": "An Wang and Rulin Zhou and Mengya Xu and Yiru Ye and Longfei Gou and Yiting Chang and Hao Chen and Chwee Ming Lim and Jiankun Wang and Hongliang Ren", "abstract": "  Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.\n", "link": "http://arxiv.org/abs/2507.15292v4", "date": "2025-07-24", "relevancy": 2.2244, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5818}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5523}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EndoControlMag%3A%20Robust%20Endoscopic%20Vascular%20Motion%20Magnification%20with%0A%20%20Periodic%20Reference%20Resetting%20and%20Hierarchical%20Tissue-aware%20Dual-Mask%20Control&body=Title%3A%20EndoControlMag%3A%20Robust%20Endoscopic%20Vascular%20Motion%20Magnification%20with%0A%20%20Periodic%20Reference%20Resetting%20and%20Hierarchical%20Tissue-aware%20Dual-Mask%20Control%0AAuthor%3A%20An%20Wang%20and%20Rulin%20Zhou%20and%20Mengya%20Xu%20and%20Yiru%20Ye%20and%20Longfei%20Gou%20and%20Yiting%20Chang%20and%20Hao%20Chen%20and%20Chwee%20Ming%20Lim%20and%20Jiankun%20Wang%20and%20Hongliang%20Ren%0AAbstract%3A%20%20%20Visualizing%20subtle%20vascular%20motions%20in%20endoscopic%20surgery%20is%20crucial%20for%0Asurgical%20precision%20and%20decision-making%2C%20yet%20remains%20challenging%20due%20to%20the%0Acomplex%20and%20dynamic%20nature%20of%20surgical%20scenes.%20To%20address%20this%2C%20we%20introduce%0AEndoControlMag%2C%20a%20training-free%2C%20Lagrangian-based%20framework%20with%0Amask-conditioned%20vascular%20motion%20magnification%20tailored%20to%20endoscopic%0Aenvironments.%20Our%20approach%20features%20two%20key%20modules%3A%20a%20Periodic%20Reference%0AResetting%20%28PRR%29%20scheme%20that%20divides%20videos%20into%20short%20overlapping%20clips%20with%0Adynamically%20updated%20reference%20frames%20to%20prevent%20error%20accumulation%20while%0Amaintaining%20temporal%20coherence%2C%20and%20a%20Hierarchical%20Tissue-aware%20Magnification%0A%28HTM%29%20framework%20with%20dual-mode%20mask%20dilation.%20HTM%20first%20tracks%20vessel%20cores%0Ausing%20a%20pretrained%20visual%20tracking%20model%20to%20maintain%20accurate%20localization%0Adespite%20occlusions%20and%20view%20changes.%20It%20then%20applies%20one%20of%20two%20adaptive%0Asoftening%20strategies%20to%20surrounding%20tissues%3A%20motion-based%20softening%20that%0Amodulates%20magnification%20strength%20proportional%20to%20observed%20tissue%20displacement%2C%0Aor%20distance-based%20exponential%20decay%20that%20simulates%20biomechanical%20force%0Aattenuation.%20This%20dual-mode%20approach%20accommodates%20diverse%20surgical%0Ascenarios-motion-based%20softening%20excels%20with%20complex%20tissue%20deformations%20while%0Adistance-based%20softening%20provides%20stability%20during%20unreliable%20optical%20flow%0Aconditions.%20We%20evaluate%20EndoControlMag%20on%20our%20EndoVMM24%20dataset%20spanning%20four%0Adifferent%20surgery%20types%20and%20various%20challenging%20scenarios%2C%20including%0Aocclusions%2C%20instrument%20disturbance%2C%20view%20changes%2C%20and%20vessel%20deformations.%0AQuantitative%20metrics%2C%20visual%20assessments%2C%20and%20expert%20surgeon%20evaluations%0Ademonstrate%20that%20EndoControlMag%20significantly%20outperforms%20existing%20methods%20in%0Aboth%20magnification%20accuracy%20and%20visual%20quality%20while%20maintaining%20robustness%0Aacross%20challenging%20surgical%20conditions.%20The%20code%2C%20dataset%2C%20and%20video%20results%0Aare%20available%20at%20https%3A//szupc.github.io/EndoControlMag/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15292v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndoControlMag%253A%2520Robust%2520Endoscopic%2520Vascular%2520Motion%2520Magnification%2520with%250A%2520%2520Periodic%2520Reference%2520Resetting%2520and%2520Hierarchical%2520Tissue-aware%2520Dual-Mask%2520Control%26entry.906535625%3DAn%2520Wang%2520and%2520Rulin%2520Zhou%2520and%2520Mengya%2520Xu%2520and%2520Yiru%2520Ye%2520and%2520Longfei%2520Gou%2520and%2520Yiting%2520Chang%2520and%2520Hao%2520Chen%2520and%2520Chwee%2520Ming%2520Lim%2520and%2520Jiankun%2520Wang%2520and%2520Hongliang%2520Ren%26entry.1292438233%3D%2520%2520Visualizing%2520subtle%2520vascular%2520motions%2520in%2520endoscopic%2520surgery%2520is%2520crucial%2520for%250Asurgical%2520precision%2520and%2520decision-making%252C%2520yet%2520remains%2520challenging%2520due%2520to%2520the%250Acomplex%2520and%2520dynamic%2520nature%2520of%2520surgical%2520scenes.%2520To%2520address%2520this%252C%2520we%2520introduce%250AEndoControlMag%252C%2520a%2520training-free%252C%2520Lagrangian-based%2520framework%2520with%250Amask-conditioned%2520vascular%2520motion%2520magnification%2520tailored%2520to%2520endoscopic%250Aenvironments.%2520Our%2520approach%2520features%2520two%2520key%2520modules%253A%2520a%2520Periodic%2520Reference%250AResetting%2520%2528PRR%2529%2520scheme%2520that%2520divides%2520videos%2520into%2520short%2520overlapping%2520clips%2520with%250Adynamically%2520updated%2520reference%2520frames%2520to%2520prevent%2520error%2520accumulation%2520while%250Amaintaining%2520temporal%2520coherence%252C%2520and%2520a%2520Hierarchical%2520Tissue-aware%2520Magnification%250A%2528HTM%2529%2520framework%2520with%2520dual-mode%2520mask%2520dilation.%2520HTM%2520first%2520tracks%2520vessel%2520cores%250Ausing%2520a%2520pretrained%2520visual%2520tracking%2520model%2520to%2520maintain%2520accurate%2520localization%250Adespite%2520occlusions%2520and%2520view%2520changes.%2520It%2520then%2520applies%2520one%2520of%2520two%2520adaptive%250Asoftening%2520strategies%2520to%2520surrounding%2520tissues%253A%2520motion-based%2520softening%2520that%250Amodulates%2520magnification%2520strength%2520proportional%2520to%2520observed%2520tissue%2520displacement%252C%250Aor%2520distance-based%2520exponential%2520decay%2520that%2520simulates%2520biomechanical%2520force%250Aattenuation.%2520This%2520dual-mode%2520approach%2520accommodates%2520diverse%2520surgical%250Ascenarios-motion-based%2520softening%2520excels%2520with%2520complex%2520tissue%2520deformations%2520while%250Adistance-based%2520softening%2520provides%2520stability%2520during%2520unreliable%2520optical%2520flow%250Aconditions.%2520We%2520evaluate%2520EndoControlMag%2520on%2520our%2520EndoVMM24%2520dataset%2520spanning%2520four%250Adifferent%2520surgery%2520types%2520and%2520various%2520challenging%2520scenarios%252C%2520including%250Aocclusions%252C%2520instrument%2520disturbance%252C%2520view%2520changes%252C%2520and%2520vessel%2520deformations.%250AQuantitative%2520metrics%252C%2520visual%2520assessments%252C%2520and%2520expert%2520surgeon%2520evaluations%250Ademonstrate%2520that%2520EndoControlMag%2520significantly%2520outperforms%2520existing%2520methods%2520in%250Aboth%2520magnification%2520accuracy%2520and%2520visual%2520quality%2520while%2520maintaining%2520robustness%250Aacross%2520challenging%2520surgical%2520conditions.%2520The%2520code%252C%2520dataset%252C%2520and%2520video%2520results%250Aare%2520available%2520at%2520https%253A//szupc.github.io/EndoControlMag/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15292v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EndoControlMag%3A%20Robust%20Endoscopic%20Vascular%20Motion%20Magnification%20with%0A%20%20Periodic%20Reference%20Resetting%20and%20Hierarchical%20Tissue-aware%20Dual-Mask%20Control&entry.906535625=An%20Wang%20and%20Rulin%20Zhou%20and%20Mengya%20Xu%20and%20Yiru%20Ye%20and%20Longfei%20Gou%20and%20Yiting%20Chang%20and%20Hao%20Chen%20and%20Chwee%20Ming%20Lim%20and%20Jiankun%20Wang%20and%20Hongliang%20Ren&entry.1292438233=%20%20Visualizing%20subtle%20vascular%20motions%20in%20endoscopic%20surgery%20is%20crucial%20for%0Asurgical%20precision%20and%20decision-making%2C%20yet%20remains%20challenging%20due%20to%20the%0Acomplex%20and%20dynamic%20nature%20of%20surgical%20scenes.%20To%20address%20this%2C%20we%20introduce%0AEndoControlMag%2C%20a%20training-free%2C%20Lagrangian-based%20framework%20with%0Amask-conditioned%20vascular%20motion%20magnification%20tailored%20to%20endoscopic%0Aenvironments.%20Our%20approach%20features%20two%20key%20modules%3A%20a%20Periodic%20Reference%0AResetting%20%28PRR%29%20scheme%20that%20divides%20videos%20into%20short%20overlapping%20clips%20with%0Adynamically%20updated%20reference%20frames%20to%20prevent%20error%20accumulation%20while%0Amaintaining%20temporal%20coherence%2C%20and%20a%20Hierarchical%20Tissue-aware%20Magnification%0A%28HTM%29%20framework%20with%20dual-mode%20mask%20dilation.%20HTM%20first%20tracks%20vessel%20cores%0Ausing%20a%20pretrained%20visual%20tracking%20model%20to%20maintain%20accurate%20localization%0Adespite%20occlusions%20and%20view%20changes.%20It%20then%20applies%20one%20of%20two%20adaptive%0Asoftening%20strategies%20to%20surrounding%20tissues%3A%20motion-based%20softening%20that%0Amodulates%20magnification%20strength%20proportional%20to%20observed%20tissue%20displacement%2C%0Aor%20distance-based%20exponential%20decay%20that%20simulates%20biomechanical%20force%0Aattenuation.%20This%20dual-mode%20approach%20accommodates%20diverse%20surgical%0Ascenarios-motion-based%20softening%20excels%20with%20complex%20tissue%20deformations%20while%0Adistance-based%20softening%20provides%20stability%20during%20unreliable%20optical%20flow%0Aconditions.%20We%20evaluate%20EndoControlMag%20on%20our%20EndoVMM24%20dataset%20spanning%20four%0Adifferent%20surgery%20types%20and%20various%20challenging%20scenarios%2C%20including%0Aocclusions%2C%20instrument%20disturbance%2C%20view%20changes%2C%20and%20vessel%20deformations.%0AQuantitative%20metrics%2C%20visual%20assessments%2C%20and%20expert%20surgeon%20evaluations%0Ademonstrate%20that%20EndoControlMag%20significantly%20outperforms%20existing%20methods%20in%0Aboth%20magnification%20accuracy%20and%20visual%20quality%20while%20maintaining%20robustness%0Aacross%20challenging%20surgical%20conditions.%20The%20code%2C%20dataset%2C%20and%20video%20results%0Aare%20available%20at%20https%3A//szupc.github.io/EndoControlMag/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15292v4&entry.124074799=Read"},
{"title": "On the Performance of Concept Probing: The Influence of the Data\n  (Extended Version)", "author": "Manuel de Sousa Ribeiro and Afonso Leote and Jo\u00e3o Leite", "abstract": "  Concept probing has recently garnered increasing interest as a way to help\ninterpret artificial neural networks, dealing both with their typically large\nsize and their subsymbolic nature, which ultimately renders them unfeasible for\ndirect human interpretation. Concept probing works by training additional\nclassifiers to map the internal representations of a model into human-defined\nconcepts of interest, thus allowing humans to peek inside artificial neural\nnetworks. Research on concept probing has mainly focused on the model being\nprobed or the probing model itself, paying limited attention to the data\nrequired to train such probing models. In this paper, we address this gap.\nFocusing on concept probing in the context of image classification tasks, we\ninvestigate the effect of the data used to train probing models on their\nperformance. We also make available concept labels for two widely used\ndatasets.\n", "link": "http://arxiv.org/abs/2507.18550v1", "date": "2025-07-24", "relevancy": 2.2233, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Performance%20of%20Concept%20Probing%3A%20The%20Influence%20of%20the%20Data%0A%20%20%28Extended%20Version%29&body=Title%3A%20On%20the%20Performance%20of%20Concept%20Probing%3A%20The%20Influence%20of%20the%20Data%0A%20%20%28Extended%20Version%29%0AAuthor%3A%20Manuel%20de%20Sousa%20Ribeiro%20and%20Afonso%20Leote%20and%20Jo%C3%A3o%20Leite%0AAbstract%3A%20%20%20Concept%20probing%20has%20recently%20garnered%20increasing%20interest%20as%20a%20way%20to%20help%0Ainterpret%20artificial%20neural%20networks%2C%20dealing%20both%20with%20their%20typically%20large%0Asize%20and%20their%20subsymbolic%20nature%2C%20which%20ultimately%20renders%20them%20unfeasible%20for%0Adirect%20human%20interpretation.%20Concept%20probing%20works%20by%20training%20additional%0Aclassifiers%20to%20map%20the%20internal%20representations%20of%20a%20model%20into%20human-defined%0Aconcepts%20of%20interest%2C%20thus%20allowing%20humans%20to%20peek%20inside%20artificial%20neural%0Anetworks.%20Research%20on%20concept%20probing%20has%20mainly%20focused%20on%20the%20model%20being%0Aprobed%20or%20the%20probing%20model%20itself%2C%20paying%20limited%20attention%20to%20the%20data%0Arequired%20to%20train%20such%20probing%20models.%20In%20this%20paper%2C%20we%20address%20this%20gap.%0AFocusing%20on%20concept%20probing%20in%20the%20context%20of%20image%20classification%20tasks%2C%20we%0Ainvestigate%20the%20effect%20of%20the%20data%20used%20to%20train%20probing%20models%20on%20their%0Aperformance.%20We%20also%20make%20available%20concept%20labels%20for%20two%20widely%20used%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Performance%2520of%2520Concept%2520Probing%253A%2520The%2520Influence%2520of%2520the%2520Data%250A%2520%2520%2528Extended%2520Version%2529%26entry.906535625%3DManuel%2520de%2520Sousa%2520Ribeiro%2520and%2520Afonso%2520Leote%2520and%2520Jo%25C3%25A3o%2520Leite%26entry.1292438233%3D%2520%2520Concept%2520probing%2520has%2520recently%2520garnered%2520increasing%2520interest%2520as%2520a%2520way%2520to%2520help%250Ainterpret%2520artificial%2520neural%2520networks%252C%2520dealing%2520both%2520with%2520their%2520typically%2520large%250Asize%2520and%2520their%2520subsymbolic%2520nature%252C%2520which%2520ultimately%2520renders%2520them%2520unfeasible%2520for%250Adirect%2520human%2520interpretation.%2520Concept%2520probing%2520works%2520by%2520training%2520additional%250Aclassifiers%2520to%2520map%2520the%2520internal%2520representations%2520of%2520a%2520model%2520into%2520human-defined%250Aconcepts%2520of%2520interest%252C%2520thus%2520allowing%2520humans%2520to%2520peek%2520inside%2520artificial%2520neural%250Anetworks.%2520Research%2520on%2520concept%2520probing%2520has%2520mainly%2520focused%2520on%2520the%2520model%2520being%250Aprobed%2520or%2520the%2520probing%2520model%2520itself%252C%2520paying%2520limited%2520attention%2520to%2520the%2520data%250Arequired%2520to%2520train%2520such%2520probing%2520models.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520gap.%250AFocusing%2520on%2520concept%2520probing%2520in%2520the%2520context%2520of%2520image%2520classification%2520tasks%252C%2520we%250Ainvestigate%2520the%2520effect%2520of%2520the%2520data%2520used%2520to%2520train%2520probing%2520models%2520on%2520their%250Aperformance.%2520We%2520also%2520make%2520available%2520concept%2520labels%2520for%2520two%2520widely%2520used%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Performance%20of%20Concept%20Probing%3A%20The%20Influence%20of%20the%20Data%0A%20%20%28Extended%20Version%29&entry.906535625=Manuel%20de%20Sousa%20Ribeiro%20and%20Afonso%20Leote%20and%20Jo%C3%A3o%20Leite&entry.1292438233=%20%20Concept%20probing%20has%20recently%20garnered%20increasing%20interest%20as%20a%20way%20to%20help%0Ainterpret%20artificial%20neural%20networks%2C%20dealing%20both%20with%20their%20typically%20large%0Asize%20and%20their%20subsymbolic%20nature%2C%20which%20ultimately%20renders%20them%20unfeasible%20for%0Adirect%20human%20interpretation.%20Concept%20probing%20works%20by%20training%20additional%0Aclassifiers%20to%20map%20the%20internal%20representations%20of%20a%20model%20into%20human-defined%0Aconcepts%20of%20interest%2C%20thus%20allowing%20humans%20to%20peek%20inside%20artificial%20neural%0Anetworks.%20Research%20on%20concept%20probing%20has%20mainly%20focused%20on%20the%20model%20being%0Aprobed%20or%20the%20probing%20model%20itself%2C%20paying%20limited%20attention%20to%20the%20data%0Arequired%20to%20train%20such%20probing%20models.%20In%20this%20paper%2C%20we%20address%20this%20gap.%0AFocusing%20on%20concept%20probing%20in%20the%20context%20of%20image%20classification%20tasks%2C%20we%0Ainvestigate%20the%20effect%20of%20the%20data%20used%20to%20train%20probing%20models%20on%20their%0Aperformance.%20We%20also%20make%20available%20concept%20labels%20for%20two%20widely%20used%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18550v1&entry.124074799=Read"},
{"title": "SyncMapV2: Robust and Adaptive Unsupervised Segmentation", "author": "Heng Zhang and Zikang Wan and Danilo Vasconcellos Vargas", "abstract": "  Human vision excels at segmenting visual cues without the need for explicit\ntraining, and it remains remarkably robust even as noise severity increases. In\ncontrast, existing AI algorithms struggle to maintain accuracy under similar\nconditions. Here, we present SyncMapV2, the first to solve unsupervised\nsegmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal\ndrop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop\nobserved in SOTA methods. This superior performance extends across various\ntypes of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur\n(7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust\ntraining, supervision, or loss functions. It is based on a learning paradigm\nthat uses self-organizing dynamical equations combined with concepts from\nrandom networks. Moreover, unlike conventional methods that require\nre-initialization for each new input, SyncMapV2 adapts online, mimicking the\ncontinuous adaptability of human vision. Thus, we go beyond the accurate and\nrobust results, and present the first algorithm that can do all the above\nonline, adapting to input rather than re-initializing. In adaptability tests,\nSyncMapV2 demonstrates near-zero performance degradation, which motivates and\nfosters a new generation of robust and adaptive intelligence in the near\nfuture.\n", "link": "http://arxiv.org/abs/2506.16297v3", "date": "2025-07-24", "relevancy": 2.221, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5966}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5312}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyncMapV2%3A%20Robust%20and%20Adaptive%20Unsupervised%20Segmentation&body=Title%3A%20SyncMapV2%3A%20Robust%20and%20Adaptive%20Unsupervised%20Segmentation%0AAuthor%3A%20Heng%20Zhang%20and%20Zikang%20Wan%20and%20Danilo%20Vasconcellos%20Vargas%0AAbstract%3A%20%20%20Human%20vision%20excels%20at%20segmenting%20visual%20cues%20without%20the%20need%20for%20explicit%0Atraining%2C%20and%20it%20remains%20remarkably%20robust%20even%20as%20noise%20severity%20increases.%20In%0Acontrast%2C%20existing%20AI%20algorithms%20struggle%20to%20maintain%20accuracy%20under%20similar%0Aconditions.%20Here%2C%20we%20present%20SyncMapV2%2C%20the%20first%20to%20solve%20unsupervised%0Asegmentation%20with%20state-of-the-art%20robustness.%20SyncMapV2%20exhibits%20a%20minimal%0Adrop%20in%20mIoU%2C%20only%200.01%25%2C%20under%20digital%20corruption%2C%20compared%20to%20a%2023.8%25%20drop%0Aobserved%20in%20SOTA%20methods.%20This%20superior%20performance%20extends%20across%20various%0Atypes%20of%20corruption%3A%20noise%20%287.3%25%20vs.%2037.7%25%29%2C%20weather%20%287.5%25%20vs.%2033.8%25%29%2C%20and%20blur%0A%287.0%25%20vs.%2029.5%25%29.%20Notably%2C%20SyncMapV2%20accomplishes%20this%20without%20any%20robust%0Atraining%2C%20supervision%2C%20or%20loss%20functions.%20It%20is%20based%20on%20a%20learning%20paradigm%0Athat%20uses%20self-organizing%20dynamical%20equations%20combined%20with%20concepts%20from%0Arandom%20networks.%20Moreover%2C%20unlike%20conventional%20methods%20that%20require%0Are-initialization%20for%20each%20new%20input%2C%20SyncMapV2%20adapts%20online%2C%20mimicking%20the%0Acontinuous%20adaptability%20of%20human%20vision.%20Thus%2C%20we%20go%20beyond%20the%20accurate%20and%0Arobust%20results%2C%20and%20present%20the%20first%20algorithm%20that%20can%20do%20all%20the%20above%0Aonline%2C%20adapting%20to%20input%20rather%20than%20re-initializing.%20In%20adaptability%20tests%2C%0ASyncMapV2%20demonstrates%20near-zero%20performance%20degradation%2C%20which%20motivates%20and%0Afosters%20a%20new%20generation%20of%20robust%20and%20adaptive%20intelligence%20in%20the%20near%0Afuture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16297v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyncMapV2%253A%2520Robust%2520and%2520Adaptive%2520Unsupervised%2520Segmentation%26entry.906535625%3DHeng%2520Zhang%2520and%2520Zikang%2520Wan%2520and%2520Danilo%2520Vasconcellos%2520Vargas%26entry.1292438233%3D%2520%2520Human%2520vision%2520excels%2520at%2520segmenting%2520visual%2520cues%2520without%2520the%2520need%2520for%2520explicit%250Atraining%252C%2520and%2520it%2520remains%2520remarkably%2520robust%2520even%2520as%2520noise%2520severity%2520increases.%2520In%250Acontrast%252C%2520existing%2520AI%2520algorithms%2520struggle%2520to%2520maintain%2520accuracy%2520under%2520similar%250Aconditions.%2520Here%252C%2520we%2520present%2520SyncMapV2%252C%2520the%2520first%2520to%2520solve%2520unsupervised%250Asegmentation%2520with%2520state-of-the-art%2520robustness.%2520SyncMapV2%2520exhibits%2520a%2520minimal%250Adrop%2520in%2520mIoU%252C%2520only%25200.01%2525%252C%2520under%2520digital%2520corruption%252C%2520compared%2520to%2520a%252023.8%2525%2520drop%250Aobserved%2520in%2520SOTA%2520methods.%2520This%2520superior%2520performance%2520extends%2520across%2520various%250Atypes%2520of%2520corruption%253A%2520noise%2520%25287.3%2525%2520vs.%252037.7%2525%2529%252C%2520weather%2520%25287.5%2525%2520vs.%252033.8%2525%2529%252C%2520and%2520blur%250A%25287.0%2525%2520vs.%252029.5%2525%2529.%2520Notably%252C%2520SyncMapV2%2520accomplishes%2520this%2520without%2520any%2520robust%250Atraining%252C%2520supervision%252C%2520or%2520loss%2520functions.%2520It%2520is%2520based%2520on%2520a%2520learning%2520paradigm%250Athat%2520uses%2520self-organizing%2520dynamical%2520equations%2520combined%2520with%2520concepts%2520from%250Arandom%2520networks.%2520Moreover%252C%2520unlike%2520conventional%2520methods%2520that%2520require%250Are-initialization%2520for%2520each%2520new%2520input%252C%2520SyncMapV2%2520adapts%2520online%252C%2520mimicking%2520the%250Acontinuous%2520adaptability%2520of%2520human%2520vision.%2520Thus%252C%2520we%2520go%2520beyond%2520the%2520accurate%2520and%250Arobust%2520results%252C%2520and%2520present%2520the%2520first%2520algorithm%2520that%2520can%2520do%2520all%2520the%2520above%250Aonline%252C%2520adapting%2520to%2520input%2520rather%2520than%2520re-initializing.%2520In%2520adaptability%2520tests%252C%250ASyncMapV2%2520demonstrates%2520near-zero%2520performance%2520degradation%252C%2520which%2520motivates%2520and%250Afosters%2520a%2520new%2520generation%2520of%2520robust%2520and%2520adaptive%2520intelligence%2520in%2520the%2520near%250Afuture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16297v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyncMapV2%3A%20Robust%20and%20Adaptive%20Unsupervised%20Segmentation&entry.906535625=Heng%20Zhang%20and%20Zikang%20Wan%20and%20Danilo%20Vasconcellos%20Vargas&entry.1292438233=%20%20Human%20vision%20excels%20at%20segmenting%20visual%20cues%20without%20the%20need%20for%20explicit%0Atraining%2C%20and%20it%20remains%20remarkably%20robust%20even%20as%20noise%20severity%20increases.%20In%0Acontrast%2C%20existing%20AI%20algorithms%20struggle%20to%20maintain%20accuracy%20under%20similar%0Aconditions.%20Here%2C%20we%20present%20SyncMapV2%2C%20the%20first%20to%20solve%20unsupervised%0Asegmentation%20with%20state-of-the-art%20robustness.%20SyncMapV2%20exhibits%20a%20minimal%0Adrop%20in%20mIoU%2C%20only%200.01%25%2C%20under%20digital%20corruption%2C%20compared%20to%20a%2023.8%25%20drop%0Aobserved%20in%20SOTA%20methods.%20This%20superior%20performance%20extends%20across%20various%0Atypes%20of%20corruption%3A%20noise%20%287.3%25%20vs.%2037.7%25%29%2C%20weather%20%287.5%25%20vs.%2033.8%25%29%2C%20and%20blur%0A%287.0%25%20vs.%2029.5%25%29.%20Notably%2C%20SyncMapV2%20accomplishes%20this%20without%20any%20robust%0Atraining%2C%20supervision%2C%20or%20loss%20functions.%20It%20is%20based%20on%20a%20learning%20paradigm%0Athat%20uses%20self-organizing%20dynamical%20equations%20combined%20with%20concepts%20from%0Arandom%20networks.%20Moreover%2C%20unlike%20conventional%20methods%20that%20require%0Are-initialization%20for%20each%20new%20input%2C%20SyncMapV2%20adapts%20online%2C%20mimicking%20the%0Acontinuous%20adaptability%20of%20human%20vision.%20Thus%2C%20we%20go%20beyond%20the%20accurate%20and%0Arobust%20results%2C%20and%20present%20the%20first%20algorithm%20that%20can%20do%20all%20the%20above%0Aonline%2C%20adapting%20to%20input%20rather%20than%20re-initializing.%20In%20adaptability%20tests%2C%0ASyncMapV2%20demonstrates%20near-zero%20performance%20degradation%2C%20which%20motivates%20and%0Afosters%20a%20new%20generation%20of%20robust%20and%20adaptive%20intelligence%20in%20the%20near%0Afuture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16297v3&entry.124074799=Read"},
{"title": "DRWKV: Focusing on Object Edges for Low-Light Image Enhancement", "author": "Xuecheng Bai and Yuxiang Wang and Boyu Hu and Qinyuan Jie and Chuanzhi Xu and Hongru Xiao and Kechen Li and Vera Chung", "abstract": "  Low-light image enhancement remains a challenging task, particularly in\npreserving object edge continuity and fine structural details under extreme\nillumination degradation. In this paper, we propose a novel model, DRWKV\n(Detailed Receptance Weighted Key Value), which integrates our proposed Global\nEdge Retinex (GER) theory, enabling effective decoupling of illumination and\nedge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV\nAttention, a spiral-scanning mechanism that captures spatial edge continuity\nand models irregular structures more effectively. Thirdly, we design the\nBilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align\nluminance and chrominance features, improving visual naturalness and mitigating\nartifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV\nachieves leading performance in PSNR, SSIM, and NIQE while maintaining low\ncomputational complexity. Furthermore, DRWKV enhances downstream performance in\nlow-light multi-object tracking tasks, validating its generalization\ncapabilities.\n", "link": "http://arxiv.org/abs/2507.18594v1", "date": "2025-07-24", "relevancy": 2.2146, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5566}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5532}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRWKV%3A%20Focusing%20on%20Object%20Edges%20for%20Low-Light%20Image%20Enhancement&body=Title%3A%20DRWKV%3A%20Focusing%20on%20Object%20Edges%20for%20Low-Light%20Image%20Enhancement%0AAuthor%3A%20Xuecheng%20Bai%20and%20Yuxiang%20Wang%20and%20Boyu%20Hu%20and%20Qinyuan%20Jie%20and%20Chuanzhi%20Xu%20and%20Hongru%20Xiao%20and%20Kechen%20Li%20and%20Vera%20Chung%0AAbstract%3A%20%20%20Low-light%20image%20enhancement%20remains%20a%20challenging%20task%2C%20particularly%20in%0Apreserving%20object%20edge%20continuity%20and%20fine%20structural%20details%20under%20extreme%0Aillumination%20degradation.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20model%2C%20DRWKV%0A%28Detailed%20Receptance%20Weighted%20Key%20Value%29%2C%20which%20integrates%20our%20proposed%20Global%0AEdge%20Retinex%20%28GER%29%20theory%2C%20enabling%20effective%20decoupling%20of%20illumination%20and%0Aedge%20structures%20for%20enhanced%20edge%20fidelity.%20Secondly%2C%20we%20introduce%20Evolving%20WKV%0AAttention%2C%20a%20spiral-scanning%20mechanism%20that%20captures%20spatial%20edge%20continuity%0Aand%20models%20irregular%20structures%20more%20effectively.%20Thirdly%2C%20we%20design%20the%0ABilateral%20Spectrum%20Aligner%20%28Bi-SAB%29%20and%20a%20tailored%20MS2-Loss%20to%20jointly%20align%0Aluminance%20and%20chrominance%20features%2C%20improving%20visual%20naturalness%20and%20mitigating%0Aartifacts.%20Extensive%20experiments%20on%20five%20LLIE%20benchmarks%20demonstrate%20that%20DRWKV%0Aachieves%20leading%20performance%20in%20PSNR%2C%20SSIM%2C%20and%20NIQE%20while%20maintaining%20low%0Acomputational%20complexity.%20Furthermore%2C%20DRWKV%20enhances%20downstream%20performance%20in%0Alow-light%20multi-object%20tracking%20tasks%2C%20validating%20its%20generalization%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRWKV%253A%2520Focusing%2520on%2520Object%2520Edges%2520for%2520Low-Light%2520Image%2520Enhancement%26entry.906535625%3DXuecheng%2520Bai%2520and%2520Yuxiang%2520Wang%2520and%2520Boyu%2520Hu%2520and%2520Qinyuan%2520Jie%2520and%2520Chuanzhi%2520Xu%2520and%2520Hongru%2520Xiao%2520and%2520Kechen%2520Li%2520and%2520Vera%2520Chung%26entry.1292438233%3D%2520%2520Low-light%2520image%2520enhancement%2520remains%2520a%2520challenging%2520task%252C%2520particularly%2520in%250Apreserving%2520object%2520edge%2520continuity%2520and%2520fine%2520structural%2520details%2520under%2520extreme%250Aillumination%2520degradation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520model%252C%2520DRWKV%250A%2528Detailed%2520Receptance%2520Weighted%2520Key%2520Value%2529%252C%2520which%2520integrates%2520our%2520proposed%2520Global%250AEdge%2520Retinex%2520%2528GER%2529%2520theory%252C%2520enabling%2520effective%2520decoupling%2520of%2520illumination%2520and%250Aedge%2520structures%2520for%2520enhanced%2520edge%2520fidelity.%2520Secondly%252C%2520we%2520introduce%2520Evolving%2520WKV%250AAttention%252C%2520a%2520spiral-scanning%2520mechanism%2520that%2520captures%2520spatial%2520edge%2520continuity%250Aand%2520models%2520irregular%2520structures%2520more%2520effectively.%2520Thirdly%252C%2520we%2520design%2520the%250ABilateral%2520Spectrum%2520Aligner%2520%2528Bi-SAB%2529%2520and%2520a%2520tailored%2520MS2-Loss%2520to%2520jointly%2520align%250Aluminance%2520and%2520chrominance%2520features%252C%2520improving%2520visual%2520naturalness%2520and%2520mitigating%250Aartifacts.%2520Extensive%2520experiments%2520on%2520five%2520LLIE%2520benchmarks%2520demonstrate%2520that%2520DRWKV%250Aachieves%2520leading%2520performance%2520in%2520PSNR%252C%2520SSIM%252C%2520and%2520NIQE%2520while%2520maintaining%2520low%250Acomputational%2520complexity.%2520Furthermore%252C%2520DRWKV%2520enhances%2520downstream%2520performance%2520in%250Alow-light%2520multi-object%2520tracking%2520tasks%252C%2520validating%2520its%2520generalization%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRWKV%3A%20Focusing%20on%20Object%20Edges%20for%20Low-Light%20Image%20Enhancement&entry.906535625=Xuecheng%20Bai%20and%20Yuxiang%20Wang%20and%20Boyu%20Hu%20and%20Qinyuan%20Jie%20and%20Chuanzhi%20Xu%20and%20Hongru%20Xiao%20and%20Kechen%20Li%20and%20Vera%20Chung&entry.1292438233=%20%20Low-light%20image%20enhancement%20remains%20a%20challenging%20task%2C%20particularly%20in%0Apreserving%20object%20edge%20continuity%20and%20fine%20structural%20details%20under%20extreme%0Aillumination%20degradation.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20model%2C%20DRWKV%0A%28Detailed%20Receptance%20Weighted%20Key%20Value%29%2C%20which%20integrates%20our%20proposed%20Global%0AEdge%20Retinex%20%28GER%29%20theory%2C%20enabling%20effective%20decoupling%20of%20illumination%20and%0Aedge%20structures%20for%20enhanced%20edge%20fidelity.%20Secondly%2C%20we%20introduce%20Evolving%20WKV%0AAttention%2C%20a%20spiral-scanning%20mechanism%20that%20captures%20spatial%20edge%20continuity%0Aand%20models%20irregular%20structures%20more%20effectively.%20Thirdly%2C%20we%20design%20the%0ABilateral%20Spectrum%20Aligner%20%28Bi-SAB%29%20and%20a%20tailored%20MS2-Loss%20to%20jointly%20align%0Aluminance%20and%20chrominance%20features%2C%20improving%20visual%20naturalness%20and%20mitigating%0Aartifacts.%20Extensive%20experiments%20on%20five%20LLIE%20benchmarks%20demonstrate%20that%20DRWKV%0Aachieves%20leading%20performance%20in%20PSNR%2C%20SSIM%2C%20and%20NIQE%20while%20maintaining%20low%0Acomputational%20complexity.%20Furthermore%2C%20DRWKV%20enhances%20downstream%20performance%20in%0Alow-light%20multi-object%20tracking%20tasks%2C%20validating%20its%20generalization%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18594v1&entry.124074799=Read"},
{"title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation", "author": "Ye-Chan Kim and SeungJu Cha and Si-Woo Kim and Taewhan Kim and Dong-Jin Kim", "abstract": "  Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time.\n", "link": "http://arxiv.org/abs/2507.18632v1", "date": "2025-07-24", "relevancy": 2.207, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5583}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5519}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIDA%3A%20Synthetic%20Image%20Driven%20Zero-shot%20Domain%20Adaptation&body=Title%3A%20SIDA%3A%20Synthetic%20Image%20Driven%20Zero-shot%20Domain%20Adaptation%0AAuthor%3A%20Ye-Chan%20Kim%20and%20SeungJu%20Cha%20and%20Si-Woo%20Kim%20and%20Taewhan%20Kim%20and%20Dong-Jin%20Kim%0AAbstract%3A%20%20%20Zero-shot%20domain%20adaptation%20is%20a%20method%20for%20adapting%20a%20model%20to%20a%20target%0Adomain%20without%20utilizing%20target%20domain%20image%20data.%20To%20enable%20adaptation%20without%0Atarget%20images%2C%20existing%20studies%20utilize%20CLIP%27s%20embedding%20space%20and%20text%0Adescription%20to%20simulate%20target-like%20style%20features.%20Despite%20the%20previous%0Aachievements%20in%20zero-shot%20domain%20adaptation%2C%20we%20observe%20that%20these%20text-driven%0Amethods%20struggle%20to%20capture%20complex%20real-world%20variations%20and%20significantly%0Aincrease%20adaptation%20time%20due%20to%20their%20alignment%20process.%20Instead%20of%20relying%20on%0Atext%20descriptions%2C%20we%20explore%20solutions%20leveraging%20image%20data%2C%20which%20provides%0Adiverse%20and%20more%20fine-grained%20style%20cues.%20In%20this%20work%2C%20we%20propose%20SIDA%2C%20a%0Anovel%20and%20efficient%20zero-shot%20domain%20adaptation%20method%20leveraging%20synthetic%0Aimages.%20To%20generate%20synthetic%20images%2C%20we%20first%20create%20detailed%2C%20source-like%0Aimages%20and%20apply%20image%20translation%20to%20reflect%20the%20style%20of%20the%20target%20domain.%0AWe%20then%20utilize%20the%20style%20features%20of%20these%20synthetic%20images%20as%20a%20proxy%20for%20the%0Atarget%20domain.%20Based%20on%20these%20features%2C%20we%20introduce%20Domain%20Mix%20and%20Patch%20Style%0ATransfer%20modules%2C%20which%20enable%20effective%20modeling%20of%20real-world%20variations.%20In%0Aparticular%2C%20Domain%20Mix%20blends%20multiple%20styles%20to%20expand%20the%20intra-domain%0Arepresentations%2C%20and%20Patch%20Style%20Transfer%20assigns%20different%20styles%20to%0Aindividual%20patches.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20by%20showing%0Astate-of-the-art%20performance%20in%20diverse%20zero-shot%20adaptation%20scenarios%2C%0Aparticularly%20in%20challenging%20domains.%20Moreover%2C%20our%20approach%20achieves%20high%0Aefficiency%20by%20significantly%20reducing%20the%20overall%20adaptation%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIDA%253A%2520Synthetic%2520Image%2520Driven%2520Zero-shot%2520Domain%2520Adaptation%26entry.906535625%3DYe-Chan%2520Kim%2520and%2520SeungJu%2520Cha%2520and%2520Si-Woo%2520Kim%2520and%2520Taewhan%2520Kim%2520and%2520Dong-Jin%2520Kim%26entry.1292438233%3D%2520%2520Zero-shot%2520domain%2520adaptation%2520is%2520a%2520method%2520for%2520adapting%2520a%2520model%2520to%2520a%2520target%250Adomain%2520without%2520utilizing%2520target%2520domain%2520image%2520data.%2520To%2520enable%2520adaptation%2520without%250Atarget%2520images%252C%2520existing%2520studies%2520utilize%2520CLIP%2527s%2520embedding%2520space%2520and%2520text%250Adescription%2520to%2520simulate%2520target-like%2520style%2520features.%2520Despite%2520the%2520previous%250Aachievements%2520in%2520zero-shot%2520domain%2520adaptation%252C%2520we%2520observe%2520that%2520these%2520text-driven%250Amethods%2520struggle%2520to%2520capture%2520complex%2520real-world%2520variations%2520and%2520significantly%250Aincrease%2520adaptation%2520time%2520due%2520to%2520their%2520alignment%2520process.%2520Instead%2520of%2520relying%2520on%250Atext%2520descriptions%252C%2520we%2520explore%2520solutions%2520leveraging%2520image%2520data%252C%2520which%2520provides%250Adiverse%2520and%2520more%2520fine-grained%2520style%2520cues.%2520In%2520this%2520work%252C%2520we%2520propose%2520SIDA%252C%2520a%250Anovel%2520and%2520efficient%2520zero-shot%2520domain%2520adaptation%2520method%2520leveraging%2520synthetic%250Aimages.%2520To%2520generate%2520synthetic%2520images%252C%2520we%2520first%2520create%2520detailed%252C%2520source-like%250Aimages%2520and%2520apply%2520image%2520translation%2520to%2520reflect%2520the%2520style%2520of%2520the%2520target%2520domain.%250AWe%2520then%2520utilize%2520the%2520style%2520features%2520of%2520these%2520synthetic%2520images%2520as%2520a%2520proxy%2520for%2520the%250Atarget%2520domain.%2520Based%2520on%2520these%2520features%252C%2520we%2520introduce%2520Domain%2520Mix%2520and%2520Patch%2520Style%250ATransfer%2520modules%252C%2520which%2520enable%2520effective%2520modeling%2520of%2520real-world%2520variations.%2520In%250Aparticular%252C%2520Domain%2520Mix%2520blends%2520multiple%2520styles%2520to%2520expand%2520the%2520intra-domain%250Arepresentations%252C%2520and%2520Patch%2520Style%2520Transfer%2520assigns%2520different%2520styles%2520to%250Aindividual%2520patches.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520by%2520showing%250Astate-of-the-art%2520performance%2520in%2520diverse%2520zero-shot%2520adaptation%2520scenarios%252C%250Aparticularly%2520in%2520challenging%2520domains.%2520Moreover%252C%2520our%2520approach%2520achieves%2520high%250Aefficiency%2520by%2520significantly%2520reducing%2520the%2520overall%2520adaptation%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIDA%3A%20Synthetic%20Image%20Driven%20Zero-shot%20Domain%20Adaptation&entry.906535625=Ye-Chan%20Kim%20and%20SeungJu%20Cha%20and%20Si-Woo%20Kim%20and%20Taewhan%20Kim%20and%20Dong-Jin%20Kim&entry.1292438233=%20%20Zero-shot%20domain%20adaptation%20is%20a%20method%20for%20adapting%20a%20model%20to%20a%20target%0Adomain%20without%20utilizing%20target%20domain%20image%20data.%20To%20enable%20adaptation%20without%0Atarget%20images%2C%20existing%20studies%20utilize%20CLIP%27s%20embedding%20space%20and%20text%0Adescription%20to%20simulate%20target-like%20style%20features.%20Despite%20the%20previous%0Aachievements%20in%20zero-shot%20domain%20adaptation%2C%20we%20observe%20that%20these%20text-driven%0Amethods%20struggle%20to%20capture%20complex%20real-world%20variations%20and%20significantly%0Aincrease%20adaptation%20time%20due%20to%20their%20alignment%20process.%20Instead%20of%20relying%20on%0Atext%20descriptions%2C%20we%20explore%20solutions%20leveraging%20image%20data%2C%20which%20provides%0Adiverse%20and%20more%20fine-grained%20style%20cues.%20In%20this%20work%2C%20we%20propose%20SIDA%2C%20a%0Anovel%20and%20efficient%20zero-shot%20domain%20adaptation%20method%20leveraging%20synthetic%0Aimages.%20To%20generate%20synthetic%20images%2C%20we%20first%20create%20detailed%2C%20source-like%0Aimages%20and%20apply%20image%20translation%20to%20reflect%20the%20style%20of%20the%20target%20domain.%0AWe%20then%20utilize%20the%20style%20features%20of%20these%20synthetic%20images%20as%20a%20proxy%20for%20the%0Atarget%20domain.%20Based%20on%20these%20features%2C%20we%20introduce%20Domain%20Mix%20and%20Patch%20Style%0ATransfer%20modules%2C%20which%20enable%20effective%20modeling%20of%20real-world%20variations.%20In%0Aparticular%2C%20Domain%20Mix%20blends%20multiple%20styles%20to%20expand%20the%20intra-domain%0Arepresentations%2C%20and%20Patch%20Style%20Transfer%20assigns%20different%20styles%20to%0Aindividual%20patches.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20by%20showing%0Astate-of-the-art%20performance%20in%20diverse%20zero-shot%20adaptation%20scenarios%2C%0Aparticularly%20in%20challenging%20domains.%20Moreover%2C%20our%20approach%20achieves%20high%0Aefficiency%20by%20significantly%20reducing%20the%20overall%20adaptation%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18632v1&entry.124074799=Read"},
{"title": "Efficient Uncertainty in LLMs through Evidential Knowledge Distillation", "author": "Lakshmana Sri Harsha Nemani and P. K. Srijith and Tomasz Ku\u015bmierczyk", "abstract": "  Accurate uncertainty quantification remains a key challenge for standard\nLLMs, prompting the adoption of Bayesian and ensemble-based methods. However,\nsuch methods typically necessitate computationally expensive sampling,\ninvolving multiple forward passes to effectively estimate predictive\nuncertainty.\n  In this paper, we introduce a novel approach enabling efficient and effective\nuncertainty estimation in LLMs without sacrificing performance. Specifically,\nwe distill uncertainty-aware teacher models - originally requiring multiple\nforward passes - into compact student models sharing the same architecture but\nfine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct\ndistillation strategies: one in which the student employs traditional\nsoftmax-based outputs, and another in which the student leverages\nDirichlet-distributed outputs to explicitly model epistemic uncertainty via\nevidential learning.\n  Empirical evaluations on classification datasets demonstrate that such\nstudents can achieve comparable or superior predictive and uncertainty\nquantification performance relative to their teacher models, while critically\nrequiring only a single forward pass. To our knowledge, this is the first\ndemonstration that immediate and robust uncertainty quantification can be\nachieved in LLMs through evidential distillation.\n", "link": "http://arxiv.org/abs/2507.18366v1", "date": "2025-07-24", "relevancy": 2.2, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6348}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5855}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Uncertainty%20in%20LLMs%20through%20Evidential%20Knowledge%20Distillation&body=Title%3A%20Efficient%20Uncertainty%20in%20LLMs%20through%20Evidential%20Knowledge%20Distillation%0AAuthor%3A%20Lakshmana%20Sri%20Harsha%20Nemani%20and%20P.%20K.%20Srijith%20and%20Tomasz%20Ku%C5%9Bmierczyk%0AAbstract%3A%20%20%20Accurate%20uncertainty%20quantification%20remains%20a%20key%20challenge%20for%20standard%0ALLMs%2C%20prompting%20the%20adoption%20of%20Bayesian%20and%20ensemble-based%20methods.%20However%2C%0Asuch%20methods%20typically%20necessitate%20computationally%20expensive%20sampling%2C%0Ainvolving%20multiple%20forward%20passes%20to%20effectively%20estimate%20predictive%0Auncertainty.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20enabling%20efficient%20and%20effective%0Auncertainty%20estimation%20in%20LLMs%20without%20sacrificing%20performance.%20Specifically%2C%0Awe%20distill%20uncertainty-aware%20teacher%20models%20-%20originally%20requiring%20multiple%0Aforward%20passes%20-%20into%20compact%20student%20models%20sharing%20the%20same%20architecture%20but%0Afine-tuned%20using%20Low-Rank%20Adaptation%20%28LoRA%29.%20We%20compare%20two%20distinct%0Adistillation%20strategies%3A%20one%20in%20which%20the%20student%20employs%20traditional%0Asoftmax-based%20outputs%2C%20and%20another%20in%20which%20the%20student%20leverages%0ADirichlet-distributed%20outputs%20to%20explicitly%20model%20epistemic%20uncertainty%20via%0Aevidential%20learning.%0A%20%20Empirical%20evaluations%20on%20classification%20datasets%20demonstrate%20that%20such%0Astudents%20can%20achieve%20comparable%20or%20superior%20predictive%20and%20uncertainty%0Aquantification%20performance%20relative%20to%20their%20teacher%20models%2C%20while%20critically%0Arequiring%20only%20a%20single%20forward%20pass.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Ademonstration%20that%20immediate%20and%20robust%20uncertainty%20quantification%20can%20be%0Aachieved%20in%20LLMs%20through%20evidential%20distillation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Uncertainty%2520in%2520LLMs%2520through%2520Evidential%2520Knowledge%2520Distillation%26entry.906535625%3DLakshmana%2520Sri%2520Harsha%2520Nemani%2520and%2520P.%2520K.%2520Srijith%2520and%2520Tomasz%2520Ku%25C5%259Bmierczyk%26entry.1292438233%3D%2520%2520Accurate%2520uncertainty%2520quantification%2520remains%2520a%2520key%2520challenge%2520for%2520standard%250ALLMs%252C%2520prompting%2520the%2520adoption%2520of%2520Bayesian%2520and%2520ensemble-based%2520methods.%2520However%252C%250Asuch%2520methods%2520typically%2520necessitate%2520computationally%2520expensive%2520sampling%252C%250Ainvolving%2520multiple%2520forward%2520passes%2520to%2520effectively%2520estimate%2520predictive%250Auncertainty.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520enabling%2520efficient%2520and%2520effective%250Auncertainty%2520estimation%2520in%2520LLMs%2520without%2520sacrificing%2520performance.%2520Specifically%252C%250Awe%2520distill%2520uncertainty-aware%2520teacher%2520models%2520-%2520originally%2520requiring%2520multiple%250Aforward%2520passes%2520-%2520into%2520compact%2520student%2520models%2520sharing%2520the%2520same%2520architecture%2520but%250Afine-tuned%2520using%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529.%2520We%2520compare%2520two%2520distinct%250Adistillation%2520strategies%253A%2520one%2520in%2520which%2520the%2520student%2520employs%2520traditional%250Asoftmax-based%2520outputs%252C%2520and%2520another%2520in%2520which%2520the%2520student%2520leverages%250ADirichlet-distributed%2520outputs%2520to%2520explicitly%2520model%2520epistemic%2520uncertainty%2520via%250Aevidential%2520learning.%250A%2520%2520Empirical%2520evaluations%2520on%2520classification%2520datasets%2520demonstrate%2520that%2520such%250Astudents%2520can%2520achieve%2520comparable%2520or%2520superior%2520predictive%2520and%2520uncertainty%250Aquantification%2520performance%2520relative%2520to%2520their%2520teacher%2520models%252C%2520while%2520critically%250Arequiring%2520only%2520a%2520single%2520forward%2520pass.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Ademonstration%2520that%2520immediate%2520and%2520robust%2520uncertainty%2520quantification%2520can%2520be%250Aachieved%2520in%2520LLMs%2520through%2520evidential%2520distillation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Uncertainty%20in%20LLMs%20through%20Evidential%20Knowledge%20Distillation&entry.906535625=Lakshmana%20Sri%20Harsha%20Nemani%20and%20P.%20K.%20Srijith%20and%20Tomasz%20Ku%C5%9Bmierczyk&entry.1292438233=%20%20Accurate%20uncertainty%20quantification%20remains%20a%20key%20challenge%20for%20standard%0ALLMs%2C%20prompting%20the%20adoption%20of%20Bayesian%20and%20ensemble-based%20methods.%20However%2C%0Asuch%20methods%20typically%20necessitate%20computationally%20expensive%20sampling%2C%0Ainvolving%20multiple%20forward%20passes%20to%20effectively%20estimate%20predictive%0Auncertainty.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20enabling%20efficient%20and%20effective%0Auncertainty%20estimation%20in%20LLMs%20without%20sacrificing%20performance.%20Specifically%2C%0Awe%20distill%20uncertainty-aware%20teacher%20models%20-%20originally%20requiring%20multiple%0Aforward%20passes%20-%20into%20compact%20student%20models%20sharing%20the%20same%20architecture%20but%0Afine-tuned%20using%20Low-Rank%20Adaptation%20%28LoRA%29.%20We%20compare%20two%20distinct%0Adistillation%20strategies%3A%20one%20in%20which%20the%20student%20employs%20traditional%0Asoftmax-based%20outputs%2C%20and%20another%20in%20which%20the%20student%20leverages%0ADirichlet-distributed%20outputs%20to%20explicitly%20model%20epistemic%20uncertainty%20via%0Aevidential%20learning.%0A%20%20Empirical%20evaluations%20on%20classification%20datasets%20demonstrate%20that%20such%0Astudents%20can%20achieve%20comparable%20or%20superior%20predictive%20and%20uncertainty%0Aquantification%20performance%20relative%20to%20their%20teacher%20models%2C%20while%20critically%0Arequiring%20only%20a%20single%20forward%20pass.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Ademonstration%20that%20immediate%20and%20robust%20uncertainty%20quantification%20can%20be%0Aachieved%20in%20LLMs%20through%20evidential%20distillation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18366v1&entry.124074799=Read"},
{"title": "Target Tracking via LiDAR-RADAR Sensor Fusion for Autonomous Racing", "author": "Marcello Cellina and Matteo Corno and Sergio Matteo Savaresi", "abstract": "  High Speed multi-vehicle Autonomous Racing will increase the safety and\nperformance of road-going Autonomous Vehicles. Precise vehicle detection and\ndynamics estimation from a moving platform is a key requirement for planning\nand executing complex autonomous overtaking maneuvers. To address this\nrequirement, we have developed a Latency-Aware EKF-based Multi Target Tracking\nalgorithm fusing LiDAR and RADAR measurements. The algorithm explots the\ndifferent sensor characteristics by explicitly integrating the Range Rate in\nthe EKF Measurement Function, as well as a-priori knowledge of the racetrack\nduring state prediction. It can handle Out-Of-Sequence Measurements via\nReprocessing using a double State and Measurement Buffer, ensuring sensor delay\ncompensation with no information loss. This algorithm has been implemented on\nTeam PoliMOVE's autonomous racecar, and was proved experimentally by completing\na number of fully autonomous overtaking maneuvers at speeds up to 275 km/h.\n", "link": "http://arxiv.org/abs/2505.20043v2", "date": "2025-07-24", "relevancy": 2.1674, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5496}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5459}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Target%20Tracking%20via%20LiDAR-RADAR%20Sensor%20Fusion%20for%20Autonomous%20Racing&body=Title%3A%20Target%20Tracking%20via%20LiDAR-RADAR%20Sensor%20Fusion%20for%20Autonomous%20Racing%0AAuthor%3A%20Marcello%20Cellina%20and%20Matteo%20Corno%20and%20Sergio%20Matteo%20Savaresi%0AAbstract%3A%20%20%20High%20Speed%20multi-vehicle%20Autonomous%20Racing%20will%20increase%20the%20safety%20and%0Aperformance%20of%20road-going%20Autonomous%20Vehicles.%20Precise%20vehicle%20detection%20and%0Adynamics%20estimation%20from%20a%20moving%20platform%20is%20a%20key%20requirement%20for%20planning%0Aand%20executing%20complex%20autonomous%20overtaking%20maneuvers.%20To%20address%20this%0Arequirement%2C%20we%20have%20developed%20a%20Latency-Aware%20EKF-based%20Multi%20Target%20Tracking%0Aalgorithm%20fusing%20LiDAR%20and%20RADAR%20measurements.%20The%20algorithm%20explots%20the%0Adifferent%20sensor%20characteristics%20by%20explicitly%20integrating%20the%20Range%20Rate%20in%0Athe%20EKF%20Measurement%20Function%2C%20as%20well%20as%20a-priori%20knowledge%20of%20the%20racetrack%0Aduring%20state%20prediction.%20It%20can%20handle%20Out-Of-Sequence%20Measurements%20via%0AReprocessing%20using%20a%20double%20State%20and%20Measurement%20Buffer%2C%20ensuring%20sensor%20delay%0Acompensation%20with%20no%20information%20loss.%20This%20algorithm%20has%20been%20implemented%20on%0ATeam%20PoliMOVE%27s%20autonomous%20racecar%2C%20and%20was%20proved%20experimentally%20by%20completing%0Aa%20number%20of%20fully%20autonomous%20overtaking%20maneuvers%20at%20speeds%20up%20to%20275%20km/h.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTarget%2520Tracking%2520via%2520LiDAR-RADAR%2520Sensor%2520Fusion%2520for%2520Autonomous%2520Racing%26entry.906535625%3DMarcello%2520Cellina%2520and%2520Matteo%2520Corno%2520and%2520Sergio%2520Matteo%2520Savaresi%26entry.1292438233%3D%2520%2520High%2520Speed%2520multi-vehicle%2520Autonomous%2520Racing%2520will%2520increase%2520the%2520safety%2520and%250Aperformance%2520of%2520road-going%2520Autonomous%2520Vehicles.%2520Precise%2520vehicle%2520detection%2520and%250Adynamics%2520estimation%2520from%2520a%2520moving%2520platform%2520is%2520a%2520key%2520requirement%2520for%2520planning%250Aand%2520executing%2520complex%2520autonomous%2520overtaking%2520maneuvers.%2520To%2520address%2520this%250Arequirement%252C%2520we%2520have%2520developed%2520a%2520Latency-Aware%2520EKF-based%2520Multi%2520Target%2520Tracking%250Aalgorithm%2520fusing%2520LiDAR%2520and%2520RADAR%2520measurements.%2520The%2520algorithm%2520explots%2520the%250Adifferent%2520sensor%2520characteristics%2520by%2520explicitly%2520integrating%2520the%2520Range%2520Rate%2520in%250Athe%2520EKF%2520Measurement%2520Function%252C%2520as%2520well%2520as%2520a-priori%2520knowledge%2520of%2520the%2520racetrack%250Aduring%2520state%2520prediction.%2520It%2520can%2520handle%2520Out-Of-Sequence%2520Measurements%2520via%250AReprocessing%2520using%2520a%2520double%2520State%2520and%2520Measurement%2520Buffer%252C%2520ensuring%2520sensor%2520delay%250Acompensation%2520with%2520no%2520information%2520loss.%2520This%2520algorithm%2520has%2520been%2520implemented%2520on%250ATeam%2520PoliMOVE%2527s%2520autonomous%2520racecar%252C%2520and%2520was%2520proved%2520experimentally%2520by%2520completing%250Aa%2520number%2520of%2520fully%2520autonomous%2520overtaking%2520maneuvers%2520at%2520speeds%2520up%2520to%2520275%2520km/h.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Target%20Tracking%20via%20LiDAR-RADAR%20Sensor%20Fusion%20for%20Autonomous%20Racing&entry.906535625=Marcello%20Cellina%20and%20Matteo%20Corno%20and%20Sergio%20Matteo%20Savaresi&entry.1292438233=%20%20High%20Speed%20multi-vehicle%20Autonomous%20Racing%20will%20increase%20the%20safety%20and%0Aperformance%20of%20road-going%20Autonomous%20Vehicles.%20Precise%20vehicle%20detection%20and%0Adynamics%20estimation%20from%20a%20moving%20platform%20is%20a%20key%20requirement%20for%20planning%0Aand%20executing%20complex%20autonomous%20overtaking%20maneuvers.%20To%20address%20this%0Arequirement%2C%20we%20have%20developed%20a%20Latency-Aware%20EKF-based%20Multi%20Target%20Tracking%0Aalgorithm%20fusing%20LiDAR%20and%20RADAR%20measurements.%20The%20algorithm%20explots%20the%0Adifferent%20sensor%20characteristics%20by%20explicitly%20integrating%20the%20Range%20Rate%20in%0Athe%20EKF%20Measurement%20Function%2C%20as%20well%20as%20a-priori%20knowledge%20of%20the%20racetrack%0Aduring%20state%20prediction.%20It%20can%20handle%20Out-Of-Sequence%20Measurements%20via%0AReprocessing%20using%20a%20double%20State%20and%20Measurement%20Buffer%2C%20ensuring%20sensor%20delay%0Acompensation%20with%20no%20information%20loss.%20This%20algorithm%20has%20been%20implemented%20on%0ATeam%20PoliMOVE%27s%20autonomous%20racecar%2C%20and%20was%20proved%20experimentally%20by%20completing%0Aa%20number%20of%20fully%20autonomous%20overtaking%20maneuvers%20at%20speeds%20up%20to%20275%20km/h.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20043v2&entry.124074799=Read"},
{"title": "Mechanistic Indicators of Understanding in Large Language Models", "author": "Pierre Beckmann and Matthieu Queloz", "abstract": "  Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.\n", "link": "http://arxiv.org/abs/2507.08017v3", "date": "2025-07-24", "relevancy": 2.1477, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanistic%20Indicators%20of%20Understanding%20in%20Large%20Language%20Models&body=Title%3A%20Mechanistic%20Indicators%20of%20Understanding%20in%20Large%20Language%20Models%0AAuthor%3A%20Pierre%20Beckmann%20and%20Matthieu%20Queloz%0AAbstract%3A%20%20%20Recent%20findings%20in%20mechanistic%20interpretability%20%28MI%29%2C%20the%20field%20probing%20the%0Ainner%20workings%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20challenge%20the%20view%20that%20these%0Amodels%20rely%20solely%20on%20superficial%20statistics.%20We%20offer%20an%20accessible%20synthesis%0Aof%20these%20findings%20that%20doubles%20as%20an%20introduction%20to%20MI%20while%20integrating%20these%0Afindings%20within%20a%20novel%20theoretical%20framework%20for%20thinking%20about%20machine%0Aunderstanding.%20We%20argue%20that%20LLMs%20develop%20internal%20structures%20that%20are%0Afunctionally%20analogous%20to%20the%20kind%20of%20understanding%20that%20consists%20in%20seeing%0Aconnections.%20To%20sharpen%20this%20idea%2C%20we%20propose%20a%20three-tiered%20conception%20of%0Aunderstanding.%20First%2C%20conceptual%20understanding%20emerges%20when%20a%20model%20forms%0A%22features%22%20as%20directions%20in%20latent%20space%2C%20learning%20the%20connections%20between%0Adiverse%20manifestations%20of%20something.%20Second%2C%20state-of-the-world%20understanding%0Aemerges%20when%20a%20model%20learns%20contingent%20factual%20connections%20between%20features%20and%0Adynamically%20tracks%20changes%20in%20the%20world.%20Third%2C%20principled%20understanding%0Aemerges%20when%20a%20model%20ceases%20to%20rely%20on%20a%20collection%20of%20memorized%20facts%20and%0Adiscovers%20a%20%22circuit%22%20connecting%20these%20facts.%20However%2C%20these%20forms%20of%0Aunderstanding%20remain%20radically%20different%20from%20human%20understanding%2C%20as%20the%0Aphenomenon%20of%20%22parallel%20mechanisms%22%20shows.%20We%20conclude%20that%20the%20debate%20should%0Amove%20beyond%20the%20yes-or-no%20question%20of%20whether%20LLMs%20understand%20to%20investigate%0Ahow%20their%20strange%20minds%20work%20and%20forge%20conceptions%20that%20fit%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08017v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanistic%2520Indicators%2520of%2520Understanding%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DPierre%2520Beckmann%2520and%2520Matthieu%2520Queloz%26entry.1292438233%3D%2520%2520Recent%2520findings%2520in%2520mechanistic%2520interpretability%2520%2528MI%2529%252C%2520the%2520field%2520probing%2520the%250Ainner%2520workings%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520challenge%2520the%2520view%2520that%2520these%250Amodels%2520rely%2520solely%2520on%2520superficial%2520statistics.%2520We%2520offer%2520an%2520accessible%2520synthesis%250Aof%2520these%2520findings%2520that%2520doubles%2520as%2520an%2520introduction%2520to%2520MI%2520while%2520integrating%2520these%250Afindings%2520within%2520a%2520novel%2520theoretical%2520framework%2520for%2520thinking%2520about%2520machine%250Aunderstanding.%2520We%2520argue%2520that%2520LLMs%2520develop%2520internal%2520structures%2520that%2520are%250Afunctionally%2520analogous%2520to%2520the%2520kind%2520of%2520understanding%2520that%2520consists%2520in%2520seeing%250Aconnections.%2520To%2520sharpen%2520this%2520idea%252C%2520we%2520propose%2520a%2520three-tiered%2520conception%2520of%250Aunderstanding.%2520First%252C%2520conceptual%2520understanding%2520emerges%2520when%2520a%2520model%2520forms%250A%2522features%2522%2520as%2520directions%2520in%2520latent%2520space%252C%2520learning%2520the%2520connections%2520between%250Adiverse%2520manifestations%2520of%2520something.%2520Second%252C%2520state-of-the-world%2520understanding%250Aemerges%2520when%2520a%2520model%2520learns%2520contingent%2520factual%2520connections%2520between%2520features%2520and%250Adynamically%2520tracks%2520changes%2520in%2520the%2520world.%2520Third%252C%2520principled%2520understanding%250Aemerges%2520when%2520a%2520model%2520ceases%2520to%2520rely%2520on%2520a%2520collection%2520of%2520memorized%2520facts%2520and%250Adiscovers%2520a%2520%2522circuit%2522%2520connecting%2520these%2520facts.%2520However%252C%2520these%2520forms%2520of%250Aunderstanding%2520remain%2520radically%2520different%2520from%2520human%2520understanding%252C%2520as%2520the%250Aphenomenon%2520of%2520%2522parallel%2520mechanisms%2522%2520shows.%2520We%2520conclude%2520that%2520the%2520debate%2520should%250Amove%2520beyond%2520the%2520yes-or-no%2520question%2520of%2520whether%2520LLMs%2520understand%2520to%2520investigate%250Ahow%2520their%2520strange%2520minds%2520work%2520and%2520forge%2520conceptions%2520that%2520fit%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08017v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanistic%20Indicators%20of%20Understanding%20in%20Large%20Language%20Models&entry.906535625=Pierre%20Beckmann%20and%20Matthieu%20Queloz&entry.1292438233=%20%20Recent%20findings%20in%20mechanistic%20interpretability%20%28MI%29%2C%20the%20field%20probing%20the%0Ainner%20workings%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20challenge%20the%20view%20that%20these%0Amodels%20rely%20solely%20on%20superficial%20statistics.%20We%20offer%20an%20accessible%20synthesis%0Aof%20these%20findings%20that%20doubles%20as%20an%20introduction%20to%20MI%20while%20integrating%20these%0Afindings%20within%20a%20novel%20theoretical%20framework%20for%20thinking%20about%20machine%0Aunderstanding.%20We%20argue%20that%20LLMs%20develop%20internal%20structures%20that%20are%0Afunctionally%20analogous%20to%20the%20kind%20of%20understanding%20that%20consists%20in%20seeing%0Aconnections.%20To%20sharpen%20this%20idea%2C%20we%20propose%20a%20three-tiered%20conception%20of%0Aunderstanding.%20First%2C%20conceptual%20understanding%20emerges%20when%20a%20model%20forms%0A%22features%22%20as%20directions%20in%20latent%20space%2C%20learning%20the%20connections%20between%0Adiverse%20manifestations%20of%20something.%20Second%2C%20state-of-the-world%20understanding%0Aemerges%20when%20a%20model%20learns%20contingent%20factual%20connections%20between%20features%20and%0Adynamically%20tracks%20changes%20in%20the%20world.%20Third%2C%20principled%20understanding%0Aemerges%20when%20a%20model%20ceases%20to%20rely%20on%20a%20collection%20of%20memorized%20facts%20and%0Adiscovers%20a%20%22circuit%22%20connecting%20these%20facts.%20However%2C%20these%20forms%20of%0Aunderstanding%20remain%20radically%20different%20from%20human%20understanding%2C%20as%20the%0Aphenomenon%20of%20%22parallel%20mechanisms%22%20shows.%20We%20conclude%20that%20the%20debate%20should%0Amove%20beyond%20the%20yes-or-no%20question%20of%20whether%20LLMs%20understand%20to%20investigate%0Ahow%20their%20strange%20minds%20work%20and%20forge%20conceptions%20that%20fit%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08017v3&entry.124074799=Read"},
{"title": "Demystify Protein Generation with Hierarchical Conditional Diffusion\n  Models", "author": "Zinan Ling and Yi Shi and Da Yan and Yang Zhou and Bo Hui", "abstract": "  Generating novel and functional protein sequences is critical to a wide range\nof applications in biology. Recent advancements in conditional diffusion models\nhave shown impressive empirical performance in protein generation tasks.\nHowever, reliable generations of protein remain an open research question in de\nnovo protein design, especially when it comes to conditional diffusion models.\nConsidering the biological function of a protein is determined by multi-level\nstructures, we propose a novel multi-level conditional diffusion model that\nintegrates both sequence-based and structure-based information for efficient\nend-to-end protein design guided by specified functions. By generating\nrepresentations at different levels simultaneously, our framework can\neffectively model the inherent hierarchical relations between different levels,\nresulting in an informative and discriminative representation of the generated\nprotein. We also propose a Protein-MMD, a new reliable evaluation metric, to\nevaluate the quality of generated protein with conditional diffusion models.\nOur new metric is able to capture both distributional and functional\nsimilarities between real and generated protein sequences while ensuring\nconditional consistency. We experiment with the benchmark datasets, and the\nresults on conditional protein generation tasks demonstrate the efficacy of the\nproposed generation framework and evaluation metric.\n", "link": "http://arxiv.org/abs/2507.18603v1", "date": "2025-07-24", "relevancy": 2.1368, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5388}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5315}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystify%20Protein%20Generation%20with%20Hierarchical%20Conditional%20Diffusion%0A%20%20Models&body=Title%3A%20Demystify%20Protein%20Generation%20with%20Hierarchical%20Conditional%20Diffusion%0A%20%20Models%0AAuthor%3A%20Zinan%20Ling%20and%20Yi%20Shi%20and%20Da%20Yan%20and%20Yang%20Zhou%20and%20Bo%20Hui%0AAbstract%3A%20%20%20Generating%20novel%20and%20functional%20protein%20sequences%20is%20critical%20to%20a%20wide%20range%0Aof%20applications%20in%20biology.%20Recent%20advancements%20in%20conditional%20diffusion%20models%0Ahave%20shown%20impressive%20empirical%20performance%20in%20protein%20generation%20tasks.%0AHowever%2C%20reliable%20generations%20of%20protein%20remain%20an%20open%20research%20question%20in%20de%0Anovo%20protein%20design%2C%20especially%20when%20it%20comes%20to%20conditional%20diffusion%20models.%0AConsidering%20the%20biological%20function%20of%20a%20protein%20is%20determined%20by%20multi-level%0Astructures%2C%20we%20propose%20a%20novel%20multi-level%20conditional%20diffusion%20model%20that%0Aintegrates%20both%20sequence-based%20and%20structure-based%20information%20for%20efficient%0Aend-to-end%20protein%20design%20guided%20by%20specified%20functions.%20By%20generating%0Arepresentations%20at%20different%20levels%20simultaneously%2C%20our%20framework%20can%0Aeffectively%20model%20the%20inherent%20hierarchical%20relations%20between%20different%20levels%2C%0Aresulting%20in%20an%20informative%20and%20discriminative%20representation%20of%20the%20generated%0Aprotein.%20We%20also%20propose%20a%20Protein-MMD%2C%20a%20new%20reliable%20evaluation%20metric%2C%20to%0Aevaluate%20the%20quality%20of%20generated%20protein%20with%20conditional%20diffusion%20models.%0AOur%20new%20metric%20is%20able%20to%20capture%20both%20distributional%20and%20functional%0Asimilarities%20between%20real%20and%20generated%20protein%20sequences%20while%20ensuring%0Aconditional%20consistency.%20We%20experiment%20with%20the%20benchmark%20datasets%2C%20and%20the%0Aresults%20on%20conditional%20protein%20generation%20tasks%20demonstrate%20the%20efficacy%20of%20the%0Aproposed%20generation%20framework%20and%20evaluation%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystify%2520Protein%2520Generation%2520with%2520Hierarchical%2520Conditional%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DZinan%2520Ling%2520and%2520Yi%2520Shi%2520and%2520Da%2520Yan%2520and%2520Yang%2520Zhou%2520and%2520Bo%2520Hui%26entry.1292438233%3D%2520%2520Generating%2520novel%2520and%2520functional%2520protein%2520sequences%2520is%2520critical%2520to%2520a%2520wide%2520range%250Aof%2520applications%2520in%2520biology.%2520Recent%2520advancements%2520in%2520conditional%2520diffusion%2520models%250Ahave%2520shown%2520impressive%2520empirical%2520performance%2520in%2520protein%2520generation%2520tasks.%250AHowever%252C%2520reliable%2520generations%2520of%2520protein%2520remain%2520an%2520open%2520research%2520question%2520in%2520de%250Anovo%2520protein%2520design%252C%2520especially%2520when%2520it%2520comes%2520to%2520conditional%2520diffusion%2520models.%250AConsidering%2520the%2520biological%2520function%2520of%2520a%2520protein%2520is%2520determined%2520by%2520multi-level%250Astructures%252C%2520we%2520propose%2520a%2520novel%2520multi-level%2520conditional%2520diffusion%2520model%2520that%250Aintegrates%2520both%2520sequence-based%2520and%2520structure-based%2520information%2520for%2520efficient%250Aend-to-end%2520protein%2520design%2520guided%2520by%2520specified%2520functions.%2520By%2520generating%250Arepresentations%2520at%2520different%2520levels%2520simultaneously%252C%2520our%2520framework%2520can%250Aeffectively%2520model%2520the%2520inherent%2520hierarchical%2520relations%2520between%2520different%2520levels%252C%250Aresulting%2520in%2520an%2520informative%2520and%2520discriminative%2520representation%2520of%2520the%2520generated%250Aprotein.%2520We%2520also%2520propose%2520a%2520Protein-MMD%252C%2520a%2520new%2520reliable%2520evaluation%2520metric%252C%2520to%250Aevaluate%2520the%2520quality%2520of%2520generated%2520protein%2520with%2520conditional%2520diffusion%2520models.%250AOur%2520new%2520metric%2520is%2520able%2520to%2520capture%2520both%2520distributional%2520and%2520functional%250Asimilarities%2520between%2520real%2520and%2520generated%2520protein%2520sequences%2520while%2520ensuring%250Aconditional%2520consistency.%2520We%2520experiment%2520with%2520the%2520benchmark%2520datasets%252C%2520and%2520the%250Aresults%2520on%2520conditional%2520protein%2520generation%2520tasks%2520demonstrate%2520the%2520efficacy%2520of%2520the%250Aproposed%2520generation%2520framework%2520and%2520evaluation%2520metric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystify%20Protein%20Generation%20with%20Hierarchical%20Conditional%20Diffusion%0A%20%20Models&entry.906535625=Zinan%20Ling%20and%20Yi%20Shi%20and%20Da%20Yan%20and%20Yang%20Zhou%20and%20Bo%20Hui&entry.1292438233=%20%20Generating%20novel%20and%20functional%20protein%20sequences%20is%20critical%20to%20a%20wide%20range%0Aof%20applications%20in%20biology.%20Recent%20advancements%20in%20conditional%20diffusion%20models%0Ahave%20shown%20impressive%20empirical%20performance%20in%20protein%20generation%20tasks.%0AHowever%2C%20reliable%20generations%20of%20protein%20remain%20an%20open%20research%20question%20in%20de%0Anovo%20protein%20design%2C%20especially%20when%20it%20comes%20to%20conditional%20diffusion%20models.%0AConsidering%20the%20biological%20function%20of%20a%20protein%20is%20determined%20by%20multi-level%0Astructures%2C%20we%20propose%20a%20novel%20multi-level%20conditional%20diffusion%20model%20that%0Aintegrates%20both%20sequence-based%20and%20structure-based%20information%20for%20efficient%0Aend-to-end%20protein%20design%20guided%20by%20specified%20functions.%20By%20generating%0Arepresentations%20at%20different%20levels%20simultaneously%2C%20our%20framework%20can%0Aeffectively%20model%20the%20inherent%20hierarchical%20relations%20between%20different%20levels%2C%0Aresulting%20in%20an%20informative%20and%20discriminative%20representation%20of%20the%20generated%0Aprotein.%20We%20also%20propose%20a%20Protein-MMD%2C%20a%20new%20reliable%20evaluation%20metric%2C%20to%0Aevaluate%20the%20quality%20of%20generated%20protein%20with%20conditional%20diffusion%20models.%0AOur%20new%20metric%20is%20able%20to%20capture%20both%20distributional%20and%20functional%0Asimilarities%20between%20real%20and%20generated%20protein%20sequences%20while%20ensuring%0Aconditional%20consistency.%20We%20experiment%20with%20the%20benchmark%20datasets%2C%20and%20the%0Aresults%20on%20conditional%20protein%20generation%20tasks%20demonstrate%20the%20efficacy%20of%20the%0Aproposed%20generation%20framework%20and%20evaluation%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18603v1&entry.124074799=Read"},
{"title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in\n  Vision-Language Tasks", "author": "Juhwan Choi and Junehyoung Kwon and JungMin Yun and Seunguk Yu and YoungBin Kim", "abstract": "  Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.\n", "link": "http://arxiv.org/abs/2407.19795v2", "date": "2025-07-24", "relevancy": 2.1293, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VolDoGer%3A%20LLM-assisted%20Datasets%20for%20Domain%20Generalization%20in%0A%20%20Vision-Language%20Tasks&body=Title%3A%20VolDoGer%3A%20LLM-assisted%20Datasets%20for%20Domain%20Generalization%20in%0A%20%20Vision-Language%20Tasks%0AAuthor%3A%20Juhwan%20Choi%20and%20Junehyoung%20Kwon%20and%20JungMin%20Yun%20and%20Seunguk%20Yu%20and%20YoungBin%20Kim%0AAbstract%3A%20%20%20Domain%20generalizability%20is%20a%20crucial%20aspect%20of%20a%20deep%20learning%20model%20since%20it%0Adetermines%20the%20capability%20of%20the%20model%20to%20perform%20well%20on%20data%20from%20unseen%0Adomains.%20However%2C%20research%20on%20the%20domain%20generalizability%20of%20deep%20learning%0Amodels%20for%20vision-language%20tasks%20remains%20limited%2C%20primarily%20because%20of%20the%20lack%0Aof%20required%20datasets.%20To%20address%20these%20challenges%2C%20we%20propose%20VolDoGer%3A%0AVision-Language%20Dataset%20for%20Domain%20Generalization%2C%20a%20dedicated%20dataset%20designed%0Afor%20domain%20generalization%20that%20addresses%20three%20vision-language%20tasks%3A%20image%0Acaptioning%2C%20visual%20question%20answering%2C%20and%20visual%20entailment.%20We%20constructed%0AVolDoGer%20by%20extending%20LLM-based%20data%20annotation%20techniques%20to%20vision-language%0Atasks%2C%20thereby%20alleviating%20the%20burden%20of%20recruiting%20human%20annotators.%20We%0Aevaluated%20the%20domain%20generalizability%20of%20various%20models%2C%20ranging%20from%0Afine-tuned%20models%20to%20a%20recent%20multimodal%20large%20language%20model%2C%20through%0AVolDoGer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolDoGer%253A%2520LLM-assisted%2520Datasets%2520for%2520Domain%2520Generalization%2520in%250A%2520%2520Vision-Language%2520Tasks%26entry.906535625%3DJuhwan%2520Choi%2520and%2520Junehyoung%2520Kwon%2520and%2520JungMin%2520Yun%2520and%2520Seunguk%2520Yu%2520and%2520YoungBin%2520Kim%26entry.1292438233%3D%2520%2520Domain%2520generalizability%2520is%2520a%2520crucial%2520aspect%2520of%2520a%2520deep%2520learning%2520model%2520since%2520it%250Adetermines%2520the%2520capability%2520of%2520the%2520model%2520to%2520perform%2520well%2520on%2520data%2520from%2520unseen%250Adomains.%2520However%252C%2520research%2520on%2520the%2520domain%2520generalizability%2520of%2520deep%2520learning%250Amodels%2520for%2520vision-language%2520tasks%2520remains%2520limited%252C%2520primarily%2520because%2520of%2520the%2520lack%250Aof%2520required%2520datasets.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520VolDoGer%253A%250AVision-Language%2520Dataset%2520for%2520Domain%2520Generalization%252C%2520a%2520dedicated%2520dataset%2520designed%250Afor%2520domain%2520generalization%2520that%2520addresses%2520three%2520vision-language%2520tasks%253A%2520image%250Acaptioning%252C%2520visual%2520question%2520answering%252C%2520and%2520visual%2520entailment.%2520We%2520constructed%250AVolDoGer%2520by%2520extending%2520LLM-based%2520data%2520annotation%2520techniques%2520to%2520vision-language%250Atasks%252C%2520thereby%2520alleviating%2520the%2520burden%2520of%2520recruiting%2520human%2520annotators.%2520We%250Aevaluated%2520the%2520domain%2520generalizability%2520of%2520various%2520models%252C%2520ranging%2520from%250Afine-tuned%2520models%2520to%2520a%2520recent%2520multimodal%2520large%2520language%2520model%252C%2520through%250AVolDoGer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VolDoGer%3A%20LLM-assisted%20Datasets%20for%20Domain%20Generalization%20in%0A%20%20Vision-Language%20Tasks&entry.906535625=Juhwan%20Choi%20and%20Junehyoung%20Kwon%20and%20JungMin%20Yun%20and%20Seunguk%20Yu%20and%20YoungBin%20Kim&entry.1292438233=%20%20Domain%20generalizability%20is%20a%20crucial%20aspect%20of%20a%20deep%20learning%20model%20since%20it%0Adetermines%20the%20capability%20of%20the%20model%20to%20perform%20well%20on%20data%20from%20unseen%0Adomains.%20However%2C%20research%20on%20the%20domain%20generalizability%20of%20deep%20learning%0Amodels%20for%20vision-language%20tasks%20remains%20limited%2C%20primarily%20because%20of%20the%20lack%0Aof%20required%20datasets.%20To%20address%20these%20challenges%2C%20we%20propose%20VolDoGer%3A%0AVision-Language%20Dataset%20for%20Domain%20Generalization%2C%20a%20dedicated%20dataset%20designed%0Afor%20domain%20generalization%20that%20addresses%20three%20vision-language%20tasks%3A%20image%0Acaptioning%2C%20visual%20question%20answering%2C%20and%20visual%20entailment.%20We%20constructed%0AVolDoGer%20by%20extending%20LLM-based%20data%20annotation%20techniques%20to%20vision-language%0Atasks%2C%20thereby%20alleviating%20the%20burden%20of%20recruiting%20human%20annotators.%20We%0Aevaluated%20the%20domain%20generalizability%20of%20various%20models%2C%20ranging%20from%0Afine-tuned%20models%20to%20a%20recent%20multimodal%20large%20language%20model%2C%20through%0AVolDoGer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19795v2&entry.124074799=Read"},
{"title": "Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics\n  with Small On-Track Data Input", "author": "Yonghao Fu and Cheng Hu and Haokun Xiong and Zhangpeng Bao and Wenyuan Du and Edoardo Ghignone and Michele Magno and Lei Xie and Hongye Su", "abstract": "  In vehicle trajectory tracking tasks, the simplest approach is the Pure\nPursuit (PP) Control. However, this single-point preview tracking strategy\nfails to consider vehicle model constraints, compromising driving safety. Model\nPredictive Control (MPC) as a widely adopted control method, optimizes control\nactions by incorporating mechanistic models and physical constraints. While its\ncontrol performance critically depends on the accuracy of vehicle modeling.\nTraditional vehicle modeling approaches face inherent trade-offs between\ncapturing nonlinear dynamics and maintaining computational efficiency, often\nresulting in reduced control performance. To address these challenges, this\npaper proposes Residual Koopman Model Predictive Control (RKMPC) framework.\nThis method uses two linear MPC architecture to calculate control inputs: a\nLinear Model Predictive Control (LMPC) computes the baseline control input\nbased on the vehicle kinematic model, and a neural network-based RKMPC\ncalculates the compensation input. The final control command is obtained by\nadding these two components. This design preserves the reliability and\ninterpretability of traditional mechanistic model while achieving performance\noptimization through residual modeling. This method has been validated on the\nCarsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH\nracing car. Experimental results show that RKMPC requires only 20% of the\ntraining data needed by traditional Koopman Model Predictive Control (KMPC)\nwhile delivering superior tracking performance. Compared to traditional LMPC,\nRKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by\n8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The\nimplementation code is available at: https://github.com/ZJU-DDRX/Residual\nKoopman.\n", "link": "http://arxiv.org/abs/2507.18396v1", "date": "2025-07-24", "relevancy": 2.1158, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5751}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5263}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Koopman%20Model%20Predictive%20Control%20for%20Enhanced%20Vehicle%20Dynamics%0A%20%20with%20Small%20On-Track%20Data%20Input&body=Title%3A%20Residual%20Koopman%20Model%20Predictive%20Control%20for%20Enhanced%20Vehicle%20Dynamics%0A%20%20with%20Small%20On-Track%20Data%20Input%0AAuthor%3A%20Yonghao%20Fu%20and%20Cheng%20Hu%20and%20Haokun%20Xiong%20and%20Zhangpeng%20Bao%20and%20Wenyuan%20Du%20and%20Edoardo%20Ghignone%20and%20Michele%20Magno%20and%20Lei%20Xie%20and%20Hongye%20Su%0AAbstract%3A%20%20%20In%20vehicle%20trajectory%20tracking%20tasks%2C%20the%20simplest%20approach%20is%20the%20Pure%0APursuit%20%28PP%29%20Control.%20However%2C%20this%20single-point%20preview%20tracking%20strategy%0Afails%20to%20consider%20vehicle%20model%20constraints%2C%20compromising%20driving%20safety.%20Model%0APredictive%20Control%20%28MPC%29%20as%20a%20widely%20adopted%20control%20method%2C%20optimizes%20control%0Aactions%20by%20incorporating%20mechanistic%20models%20and%20physical%20constraints.%20While%20its%0Acontrol%20performance%20critically%20depends%20on%20the%20accuracy%20of%20vehicle%20modeling.%0ATraditional%20vehicle%20modeling%20approaches%20face%20inherent%20trade-offs%20between%0Acapturing%20nonlinear%20dynamics%20and%20maintaining%20computational%20efficiency%2C%20often%0Aresulting%20in%20reduced%20control%20performance.%20To%20address%20these%20challenges%2C%20this%0Apaper%20proposes%20Residual%20Koopman%20Model%20Predictive%20Control%20%28RKMPC%29%20framework.%0AThis%20method%20uses%20two%20linear%20MPC%20architecture%20to%20calculate%20control%20inputs%3A%20a%0ALinear%20Model%20Predictive%20Control%20%28LMPC%29%20computes%20the%20baseline%20control%20input%0Abased%20on%20the%20vehicle%20kinematic%20model%2C%20and%20a%20neural%20network-based%20RKMPC%0Acalculates%20the%20compensation%20input.%20The%20final%20control%20command%20is%20obtained%20by%0Aadding%20these%20two%20components.%20This%20design%20preserves%20the%20reliability%20and%0Ainterpretability%20of%20traditional%20mechanistic%20model%20while%20achieving%20performance%0Aoptimization%20through%20residual%20modeling.%20This%20method%20has%20been%20validated%20on%20the%0ACarsim-Matlab%20joint%20simulation%20platform%20and%20a%20physical%201%3A10%20scale%20F1TENTH%0Aracing%20car.%20Experimental%20results%20show%20that%20RKMPC%20requires%20only%2020%25%20of%20the%0Atraining%20data%20needed%20by%20traditional%20Koopman%20Model%20Predictive%20Control%20%28KMPC%29%0Awhile%20delivering%20superior%20tracking%20performance.%20Compared%20to%20traditional%20LMPC%2C%0ARKMPC%20reduces%20lateral%20error%20by%2011.7%25-22.1%25%2C%20decreases%20heading%20error%20by%0A8.9%25-15.8%25%2C%20and%20improves%20front-wheel%20steering%20stability%20by%20up%20to%2027.6%25.%20The%0Aimplementation%20code%20is%20available%20at%3A%20https%3A//github.com/ZJU-DDRX/Residual%0AKoopman.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Koopman%2520Model%2520Predictive%2520Control%2520for%2520Enhanced%2520Vehicle%2520Dynamics%250A%2520%2520with%2520Small%2520On-Track%2520Data%2520Input%26entry.906535625%3DYonghao%2520Fu%2520and%2520Cheng%2520Hu%2520and%2520Haokun%2520Xiong%2520and%2520Zhangpeng%2520Bao%2520and%2520Wenyuan%2520Du%2520and%2520Edoardo%2520Ghignone%2520and%2520Michele%2520Magno%2520and%2520Lei%2520Xie%2520and%2520Hongye%2520Su%26entry.1292438233%3D%2520%2520In%2520vehicle%2520trajectory%2520tracking%2520tasks%252C%2520the%2520simplest%2520approach%2520is%2520the%2520Pure%250APursuit%2520%2528PP%2529%2520Control.%2520However%252C%2520this%2520single-point%2520preview%2520tracking%2520strategy%250Afails%2520to%2520consider%2520vehicle%2520model%2520constraints%252C%2520compromising%2520driving%2520safety.%2520Model%250APredictive%2520Control%2520%2528MPC%2529%2520as%2520a%2520widely%2520adopted%2520control%2520method%252C%2520optimizes%2520control%250Aactions%2520by%2520incorporating%2520mechanistic%2520models%2520and%2520physical%2520constraints.%2520While%2520its%250Acontrol%2520performance%2520critically%2520depends%2520on%2520the%2520accuracy%2520of%2520vehicle%2520modeling.%250ATraditional%2520vehicle%2520modeling%2520approaches%2520face%2520inherent%2520trade-offs%2520between%250Acapturing%2520nonlinear%2520dynamics%2520and%2520maintaining%2520computational%2520efficiency%252C%2520often%250Aresulting%2520in%2520reduced%2520control%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520this%250Apaper%2520proposes%2520Residual%2520Koopman%2520Model%2520Predictive%2520Control%2520%2528RKMPC%2529%2520framework.%250AThis%2520method%2520uses%2520two%2520linear%2520MPC%2520architecture%2520to%2520calculate%2520control%2520inputs%253A%2520a%250ALinear%2520Model%2520Predictive%2520Control%2520%2528LMPC%2529%2520computes%2520the%2520baseline%2520control%2520input%250Abased%2520on%2520the%2520vehicle%2520kinematic%2520model%252C%2520and%2520a%2520neural%2520network-based%2520RKMPC%250Acalculates%2520the%2520compensation%2520input.%2520The%2520final%2520control%2520command%2520is%2520obtained%2520by%250Aadding%2520these%2520two%2520components.%2520This%2520design%2520preserves%2520the%2520reliability%2520and%250Ainterpretability%2520of%2520traditional%2520mechanistic%2520model%2520while%2520achieving%2520performance%250Aoptimization%2520through%2520residual%2520modeling.%2520This%2520method%2520has%2520been%2520validated%2520on%2520the%250ACarsim-Matlab%2520joint%2520simulation%2520platform%2520and%2520a%2520physical%25201%253A10%2520scale%2520F1TENTH%250Aracing%2520car.%2520Experimental%2520results%2520show%2520that%2520RKMPC%2520requires%2520only%252020%2525%2520of%2520the%250Atraining%2520data%2520needed%2520by%2520traditional%2520Koopman%2520Model%2520Predictive%2520Control%2520%2528KMPC%2529%250Awhile%2520delivering%2520superior%2520tracking%2520performance.%2520Compared%2520to%2520traditional%2520LMPC%252C%250ARKMPC%2520reduces%2520lateral%2520error%2520by%252011.7%2525-22.1%2525%252C%2520decreases%2520heading%2520error%2520by%250A8.9%2525-15.8%2525%252C%2520and%2520improves%2520front-wheel%2520steering%2520stability%2520by%2520up%2520to%252027.6%2525.%2520The%250Aimplementation%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/ZJU-DDRX/Residual%250AKoopman.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Koopman%20Model%20Predictive%20Control%20for%20Enhanced%20Vehicle%20Dynamics%0A%20%20with%20Small%20On-Track%20Data%20Input&entry.906535625=Yonghao%20Fu%20and%20Cheng%20Hu%20and%20Haokun%20Xiong%20and%20Zhangpeng%20Bao%20and%20Wenyuan%20Du%20and%20Edoardo%20Ghignone%20and%20Michele%20Magno%20and%20Lei%20Xie%20and%20Hongye%20Su&entry.1292438233=%20%20In%20vehicle%20trajectory%20tracking%20tasks%2C%20the%20simplest%20approach%20is%20the%20Pure%0APursuit%20%28PP%29%20Control.%20However%2C%20this%20single-point%20preview%20tracking%20strategy%0Afails%20to%20consider%20vehicle%20model%20constraints%2C%20compromising%20driving%20safety.%20Model%0APredictive%20Control%20%28MPC%29%20as%20a%20widely%20adopted%20control%20method%2C%20optimizes%20control%0Aactions%20by%20incorporating%20mechanistic%20models%20and%20physical%20constraints.%20While%20its%0Acontrol%20performance%20critically%20depends%20on%20the%20accuracy%20of%20vehicle%20modeling.%0ATraditional%20vehicle%20modeling%20approaches%20face%20inherent%20trade-offs%20between%0Acapturing%20nonlinear%20dynamics%20and%20maintaining%20computational%20efficiency%2C%20often%0Aresulting%20in%20reduced%20control%20performance.%20To%20address%20these%20challenges%2C%20this%0Apaper%20proposes%20Residual%20Koopman%20Model%20Predictive%20Control%20%28RKMPC%29%20framework.%0AThis%20method%20uses%20two%20linear%20MPC%20architecture%20to%20calculate%20control%20inputs%3A%20a%0ALinear%20Model%20Predictive%20Control%20%28LMPC%29%20computes%20the%20baseline%20control%20input%0Abased%20on%20the%20vehicle%20kinematic%20model%2C%20and%20a%20neural%20network-based%20RKMPC%0Acalculates%20the%20compensation%20input.%20The%20final%20control%20command%20is%20obtained%20by%0Aadding%20these%20two%20components.%20This%20design%20preserves%20the%20reliability%20and%0Ainterpretability%20of%20traditional%20mechanistic%20model%20while%20achieving%20performance%0Aoptimization%20through%20residual%20modeling.%20This%20method%20has%20been%20validated%20on%20the%0ACarsim-Matlab%20joint%20simulation%20platform%20and%20a%20physical%201%3A10%20scale%20F1TENTH%0Aracing%20car.%20Experimental%20results%20show%20that%20RKMPC%20requires%20only%2020%25%20of%20the%0Atraining%20data%20needed%20by%20traditional%20Koopman%20Model%20Predictive%20Control%20%28KMPC%29%0Awhile%20delivering%20superior%20tracking%20performance.%20Compared%20to%20traditional%20LMPC%2C%0ARKMPC%20reduces%20lateral%20error%20by%2011.7%25-22.1%25%2C%20decreases%20heading%20error%20by%0A8.9%25-15.8%25%2C%20and%20improves%20front-wheel%20steering%20stability%20by%20up%20to%2027.6%25.%20The%0Aimplementation%20code%20is%20available%20at%3A%20https%3A//github.com/ZJU-DDRX/Residual%0AKoopman.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18396v1&entry.124074799=Read"},
{"title": "Linear Memory SE(2) Invariant Attention", "author": "Ethan Pronovost and Neha Boloor and Peter Schleede and Noureldin Hendy and Andres Morales and Nicholas Roy", "abstract": "  Processing spatial data is a key component in many learning tasks for\nautonomous driving such as motion forecasting, multi-agent simulation, and\nplanning. Prior works have demonstrated the value in using SE(2) invariant\nnetwork architectures that consider only the relative poses between objects\n(e.g. other agents, scene features such as traffic lanes). However, these\nmethods compute the relative poses for all pairs of objects explicitly,\nrequiring quadratic memory. In this work, we propose a mechanism for SE(2)\ninvariant scaled dot-product attention that requires linear memory relative to\nthe number of objects in the scene. Our SE(2) invariant transformer\narchitecture enjoys the same scaling properties that have benefited large\nlanguage models in recent years. We demonstrate experimentally that our\napproach is practical to implement and improves performance compared to\ncomparable non-invariant architectures.\n", "link": "http://arxiv.org/abs/2507.18597v1", "date": "2025-07-24", "relevancy": 2.1126, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5312}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5269}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Memory%20SE%282%29%20Invariant%20Attention&body=Title%3A%20Linear%20Memory%20SE%282%29%20Invariant%20Attention%0AAuthor%3A%20Ethan%20Pronovost%20and%20Neha%20Boloor%20and%20Peter%20Schleede%20and%20Noureldin%20Hendy%20and%20Andres%20Morales%20and%20Nicholas%20Roy%0AAbstract%3A%20%20%20Processing%20spatial%20data%20is%20a%20key%20component%20in%20many%20learning%20tasks%20for%0Aautonomous%20driving%20such%20as%20motion%20forecasting%2C%20multi-agent%20simulation%2C%20and%0Aplanning.%20Prior%20works%20have%20demonstrated%20the%20value%20in%20using%20SE%282%29%20invariant%0Anetwork%20architectures%20that%20consider%20only%20the%20relative%20poses%20between%20objects%0A%28e.g.%20other%20agents%2C%20scene%20features%20such%20as%20traffic%20lanes%29.%20However%2C%20these%0Amethods%20compute%20the%20relative%20poses%20for%20all%20pairs%20of%20objects%20explicitly%2C%0Arequiring%20quadratic%20memory.%20In%20this%20work%2C%20we%20propose%20a%20mechanism%20for%20SE%282%29%0Ainvariant%20scaled%20dot-product%20attention%20that%20requires%20linear%20memory%20relative%20to%0Athe%20number%20of%20objects%20in%20the%20scene.%20Our%20SE%282%29%20invariant%20transformer%0Aarchitecture%20enjoys%20the%20same%20scaling%20properties%20that%20have%20benefited%20large%0Alanguage%20models%20in%20recent%20years.%20We%20demonstrate%20experimentally%20that%20our%0Aapproach%20is%20practical%20to%20implement%20and%20improves%20performance%20compared%20to%0Acomparable%20non-invariant%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Memory%2520SE%25282%2529%2520Invariant%2520Attention%26entry.906535625%3DEthan%2520Pronovost%2520and%2520Neha%2520Boloor%2520and%2520Peter%2520Schleede%2520and%2520Noureldin%2520Hendy%2520and%2520Andres%2520Morales%2520and%2520Nicholas%2520Roy%26entry.1292438233%3D%2520%2520Processing%2520spatial%2520data%2520is%2520a%2520key%2520component%2520in%2520many%2520learning%2520tasks%2520for%250Aautonomous%2520driving%2520such%2520as%2520motion%2520forecasting%252C%2520multi-agent%2520simulation%252C%2520and%250Aplanning.%2520Prior%2520works%2520have%2520demonstrated%2520the%2520value%2520in%2520using%2520SE%25282%2529%2520invariant%250Anetwork%2520architectures%2520that%2520consider%2520only%2520the%2520relative%2520poses%2520between%2520objects%250A%2528e.g.%2520other%2520agents%252C%2520scene%2520features%2520such%2520as%2520traffic%2520lanes%2529.%2520However%252C%2520these%250Amethods%2520compute%2520the%2520relative%2520poses%2520for%2520all%2520pairs%2520of%2520objects%2520explicitly%252C%250Arequiring%2520quadratic%2520memory.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520mechanism%2520for%2520SE%25282%2529%250Ainvariant%2520scaled%2520dot-product%2520attention%2520that%2520requires%2520linear%2520memory%2520relative%2520to%250Athe%2520number%2520of%2520objects%2520in%2520the%2520scene.%2520Our%2520SE%25282%2529%2520invariant%2520transformer%250Aarchitecture%2520enjoys%2520the%2520same%2520scaling%2520properties%2520that%2520have%2520benefited%2520large%250Alanguage%2520models%2520in%2520recent%2520years.%2520We%2520demonstrate%2520experimentally%2520that%2520our%250Aapproach%2520is%2520practical%2520to%2520implement%2520and%2520improves%2520performance%2520compared%2520to%250Acomparable%2520non-invariant%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Memory%20SE%282%29%20Invariant%20Attention&entry.906535625=Ethan%20Pronovost%20and%20Neha%20Boloor%20and%20Peter%20Schleede%20and%20Noureldin%20Hendy%20and%20Andres%20Morales%20and%20Nicholas%20Roy&entry.1292438233=%20%20Processing%20spatial%20data%20is%20a%20key%20component%20in%20many%20learning%20tasks%20for%0Aautonomous%20driving%20such%20as%20motion%20forecasting%2C%20multi-agent%20simulation%2C%20and%0Aplanning.%20Prior%20works%20have%20demonstrated%20the%20value%20in%20using%20SE%282%29%20invariant%0Anetwork%20architectures%20that%20consider%20only%20the%20relative%20poses%20between%20objects%0A%28e.g.%20other%20agents%2C%20scene%20features%20such%20as%20traffic%20lanes%29.%20However%2C%20these%0Amethods%20compute%20the%20relative%20poses%20for%20all%20pairs%20of%20objects%20explicitly%2C%0Arequiring%20quadratic%20memory.%20In%20this%20work%2C%20we%20propose%20a%20mechanism%20for%20SE%282%29%0Ainvariant%20scaled%20dot-product%20attention%20that%20requires%20linear%20memory%20relative%20to%0Athe%20number%20of%20objects%20in%20the%20scene.%20Our%20SE%282%29%20invariant%20transformer%0Aarchitecture%20enjoys%20the%20same%20scaling%20properties%20that%20have%20benefited%20large%0Alanguage%20models%20in%20recent%20years.%20We%20demonstrate%20experimentally%20that%20our%0Aapproach%20is%20practical%20to%20implement%20and%20improves%20performance%20compared%20to%0Acomparable%20non-invariant%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18597v1&entry.124074799=Read"},
{"title": "Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation", "author": "Haotian Chen and Zhiyong Xiao", "abstract": "  In the field of food image processing, efficient semantic segmentation\ntechniques are crucial for industrial applications. However, existing\nlarge-scale Transformer-based models (such as FoodSAM) face challenges in\nmeeting practical deploymentrequirements due to their massive parameter counts\nand high computational resource demands. This paper introduces TUNable Adapter\nmodule (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that\nintegrates multiscale trainable adapters into the Swin Transformer\narchitecture, achieving high-performance food image segmentation by updating\nonly 4% of the parameters. The core innovation of Swin-TUNA lies in its\nhierarchical feature adaptation mechanism: it designs separable convolutions in\ndepth and dimensional mappings of varying scales to address the differences in\nfeatures between shallow and deep networks, combined with a dynamic balancing\nstrategy for tasks-agnostic and task-specific features. Experiments demonstrate\nthat this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and\nUECFoodPix Complete datasets, respectively, surpassing the fully parameterized\nFoodSAM model while reducing the parameter count by 98.7% (to only 8.13M).\nFurthermore, Swin-TUNA exhibits faster convergence and stronger generalization\ncapabilities in low-data scenarios, providing an efficient solution for\nassembling lightweight food image.\n", "link": "http://arxiv.org/abs/2507.17347v2", "date": "2025-07-24", "relevancy": 2.0952, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5345}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Swin-TUNA%20%3A%20A%20Novel%20PEFT%20Approach%20for%20Accurate%20Food%20Image%20Segmentation&body=Title%3A%20Swin-TUNA%20%3A%20A%20Novel%20PEFT%20Approach%20for%20Accurate%20Food%20Image%20Segmentation%0AAuthor%3A%20Haotian%20Chen%20and%20Zhiyong%20Xiao%0AAbstract%3A%20%20%20In%20the%20field%20of%20food%20image%20processing%2C%20efficient%20semantic%20segmentation%0Atechniques%20are%20crucial%20for%20industrial%20applications.%20However%2C%20existing%0Alarge-scale%20Transformer-based%20models%20%28such%20as%20FoodSAM%29%20face%20challenges%20in%0Ameeting%20practical%20deploymentrequirements%20due%20to%20their%20massive%20parameter%20counts%0Aand%20high%20computational%20resource%20demands.%20This%20paper%20introduces%20TUNable%20Adapter%0Amodule%20%28Swin-TUNA%29%2C%20a%20Parameter%20Efficient%20Fine-Tuning%20%28PEFT%29%20method%20that%0Aintegrates%20multiscale%20trainable%20adapters%20into%20the%20Swin%20Transformer%0Aarchitecture%2C%20achieving%20high-performance%20food%20image%20segmentation%20by%20updating%0Aonly%204%25%20of%20the%20parameters.%20The%20core%20innovation%20of%20Swin-TUNA%20lies%20in%20its%0Ahierarchical%20feature%20adaptation%20mechanism%3A%20it%20designs%20separable%20convolutions%20in%0Adepth%20and%20dimensional%20mappings%20of%20varying%20scales%20to%20address%20the%20differences%20in%0Afeatures%20between%20shallow%20and%20deep%20networks%2C%20combined%20with%20a%20dynamic%20balancing%0Astrategy%20for%20tasks-agnostic%20and%20task-specific%20features.%20Experiments%20demonstrate%0Athat%20this%20method%20achieves%20mIoU%20of%2050.56%25%20and%2074.94%25%20on%20the%20FoodSeg103%20and%0AUECFoodPix%20Complete%20datasets%2C%20respectively%2C%20surpassing%20the%20fully%20parameterized%0AFoodSAM%20model%20while%20reducing%20the%20parameter%20count%20by%2098.7%25%20%28to%20only%208.13M%29.%0AFurthermore%2C%20Swin-TUNA%20exhibits%20faster%20convergence%20and%20stronger%20generalization%0Acapabilities%20in%20low-data%20scenarios%2C%20providing%20an%20efficient%20solution%20for%0Aassembling%20lightweight%20food%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwin-TUNA%2520%253A%2520A%2520Novel%2520PEFT%2520Approach%2520for%2520Accurate%2520Food%2520Image%2520Segmentation%26entry.906535625%3DHaotian%2520Chen%2520and%2520Zhiyong%2520Xiao%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520food%2520image%2520processing%252C%2520efficient%2520semantic%2520segmentation%250Atechniques%2520are%2520crucial%2520for%2520industrial%2520applications.%2520However%252C%2520existing%250Alarge-scale%2520Transformer-based%2520models%2520%2528such%2520as%2520FoodSAM%2529%2520face%2520challenges%2520in%250Ameeting%2520practical%2520deploymentrequirements%2520due%2520to%2520their%2520massive%2520parameter%2520counts%250Aand%2520high%2520computational%2520resource%2520demands.%2520This%2520paper%2520introduces%2520TUNable%2520Adapter%250Amodule%2520%2528Swin-TUNA%2529%252C%2520a%2520Parameter%2520Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520method%2520that%250Aintegrates%2520multiscale%2520trainable%2520adapters%2520into%2520the%2520Swin%2520Transformer%250Aarchitecture%252C%2520achieving%2520high-performance%2520food%2520image%2520segmentation%2520by%2520updating%250Aonly%25204%2525%2520of%2520the%2520parameters.%2520The%2520core%2520innovation%2520of%2520Swin-TUNA%2520lies%2520in%2520its%250Ahierarchical%2520feature%2520adaptation%2520mechanism%253A%2520it%2520designs%2520separable%2520convolutions%2520in%250Adepth%2520and%2520dimensional%2520mappings%2520of%2520varying%2520scales%2520to%2520address%2520the%2520differences%2520in%250Afeatures%2520between%2520shallow%2520and%2520deep%2520networks%252C%2520combined%2520with%2520a%2520dynamic%2520balancing%250Astrategy%2520for%2520tasks-agnostic%2520and%2520task-specific%2520features.%2520Experiments%2520demonstrate%250Athat%2520this%2520method%2520achieves%2520mIoU%2520of%252050.56%2525%2520and%252074.94%2525%2520on%2520the%2520FoodSeg103%2520and%250AUECFoodPix%2520Complete%2520datasets%252C%2520respectively%252C%2520surpassing%2520the%2520fully%2520parameterized%250AFoodSAM%2520model%2520while%2520reducing%2520the%2520parameter%2520count%2520by%252098.7%2525%2520%2528to%2520only%25208.13M%2529.%250AFurthermore%252C%2520Swin-TUNA%2520exhibits%2520faster%2520convergence%2520and%2520stronger%2520generalization%250Acapabilities%2520in%2520low-data%2520scenarios%252C%2520providing%2520an%2520efficient%2520solution%2520for%250Aassembling%2520lightweight%2520food%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swin-TUNA%20%3A%20A%20Novel%20PEFT%20Approach%20for%20Accurate%20Food%20Image%20Segmentation&entry.906535625=Haotian%20Chen%20and%20Zhiyong%20Xiao&entry.1292438233=%20%20In%20the%20field%20of%20food%20image%20processing%2C%20efficient%20semantic%20segmentation%0Atechniques%20are%20crucial%20for%20industrial%20applications.%20However%2C%20existing%0Alarge-scale%20Transformer-based%20models%20%28such%20as%20FoodSAM%29%20face%20challenges%20in%0Ameeting%20practical%20deploymentrequirements%20due%20to%20their%20massive%20parameter%20counts%0Aand%20high%20computational%20resource%20demands.%20This%20paper%20introduces%20TUNable%20Adapter%0Amodule%20%28Swin-TUNA%29%2C%20a%20Parameter%20Efficient%20Fine-Tuning%20%28PEFT%29%20method%20that%0Aintegrates%20multiscale%20trainable%20adapters%20into%20the%20Swin%20Transformer%0Aarchitecture%2C%20achieving%20high-performance%20food%20image%20segmentation%20by%20updating%0Aonly%204%25%20of%20the%20parameters.%20The%20core%20innovation%20of%20Swin-TUNA%20lies%20in%20its%0Ahierarchical%20feature%20adaptation%20mechanism%3A%20it%20designs%20separable%20convolutions%20in%0Adepth%20and%20dimensional%20mappings%20of%20varying%20scales%20to%20address%20the%20differences%20in%0Afeatures%20between%20shallow%20and%20deep%20networks%2C%20combined%20with%20a%20dynamic%20balancing%0Astrategy%20for%20tasks-agnostic%20and%20task-specific%20features.%20Experiments%20demonstrate%0Athat%20this%20method%20achieves%20mIoU%20of%2050.56%25%20and%2074.94%25%20on%20the%20FoodSeg103%20and%0AUECFoodPix%20Complete%20datasets%2C%20respectively%2C%20surpassing%20the%20fully%20parameterized%0AFoodSAM%20model%20while%20reducing%20the%20parameter%20count%20by%2098.7%25%20%28to%20only%208.13M%29.%0AFurthermore%2C%20Swin-TUNA%20exhibits%20faster%20convergence%20and%20stronger%20generalization%0Acapabilities%20in%20low-data%20scenarios%2C%20providing%20an%20efficient%20solution%20for%0Aassembling%20lightweight%20food%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17347v2&entry.124074799=Read"},
{"title": "DCFFSNet: Deep Connectivity Feature Fusion Separation Network for\n  Medical Image Segmentation", "author": "Xun Ye and Ruixiang Tang and Mingda Zhang and Jianglong Qin", "abstract": "  Medical image segmentation leverages topological connectivity theory to\nenhance edge precision and regional consistency. However, existing deep\nnetworks integrating connectivity often forcibly inject it as an additional\nfeature module, resulting in coupled feature spaces with no standardized\nmechanism to quantify different feature strengths. To address these issues, we\npropose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It\nintroduces an innovative feature space decoupling strategy. This strategy\nquantifies the relative strength between connectivity features and other\nfeatures. It then builds a deep connectivity feature fusion-separation\narchitecture. This architecture dynamically balances multi-scale feature\nexpression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg\ndatasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by\n1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice)\nand 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU).\nThe results demonstrate that DCFFSNet exceeds existing mainstream methods\nacross all metrics. It effectively resolves segmentation fragmentation and\nachieves smooth edge transitions. This significantly enhances clinical\nusability.\n", "link": "http://arxiv.org/abs/2507.18407v1", "date": "2025-07-24", "relevancy": 2.0874, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5335}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5242}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCFFSNet%3A%20Deep%20Connectivity%20Feature%20Fusion%20Separation%20Network%20for%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20DCFFSNet%3A%20Deep%20Connectivity%20Feature%20Fusion%20Separation%20Network%20for%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Xun%20Ye%20and%20Ruixiang%20Tang%20and%20Mingda%20Zhang%20and%20Jianglong%20Qin%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20leverages%20topological%20connectivity%20theory%20to%0Aenhance%20edge%20precision%20and%20regional%20consistency.%20However%2C%20existing%20deep%0Anetworks%20integrating%20connectivity%20often%20forcibly%20inject%20it%20as%20an%20additional%0Afeature%20module%2C%20resulting%20in%20coupled%20feature%20spaces%20with%20no%20standardized%0Amechanism%20to%20quantify%20different%20feature%20strengths.%20To%20address%20these%20issues%2C%20we%0Apropose%20DCFFSNet%20%28Dual-Connectivity%20Feature%20Fusion-Separation%20Network%29.%20It%0Aintroduces%20an%20innovative%20feature%20space%20decoupling%20strategy.%20This%20strategy%0Aquantifies%20the%20relative%20strength%20between%20connectivity%20features%20and%20other%0Afeatures.%20It%20then%20builds%20a%20deep%20connectivity%20feature%20fusion-separation%0Aarchitecture.%20This%20architecture%20dynamically%20balances%20multi-scale%20feature%0Aexpression.%20Experiments%20were%20conducted%20on%20the%20ISIC2018%2C%20DSB2018%2C%20and%20MoNuSeg%0Adatasets.%20On%20ISIC2018%2C%20DCFFSNet%20outperformed%20the%20next%20best%20model%20%28CMUNet%29%20by%0A1.3%25%20%28Dice%29%20and%201.2%25%20%28IoU%29.%20On%20DSB2018%2C%20it%20surpassed%20TransUNet%20by%200.7%25%20%28Dice%29%0Aand%200.9%25%20%28IoU%29.%20On%20MoNuSeg%2C%20it%20exceeded%20CSCAUNet%20by%200.8%25%20%28Dice%29%20and%200.9%25%20%28IoU%29.%0AThe%20results%20demonstrate%20that%20DCFFSNet%20exceeds%20existing%20mainstream%20methods%0Aacross%20all%20metrics.%20It%20effectively%20resolves%20segmentation%20fragmentation%20and%0Aachieves%20smooth%20edge%20transitions.%20This%20significantly%20enhances%20clinical%0Ausability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCFFSNet%253A%2520Deep%2520Connectivity%2520Feature%2520Fusion%2520Separation%2520Network%2520for%250A%2520%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DXun%2520Ye%2520and%2520Ruixiang%2520Tang%2520and%2520Mingda%2520Zhang%2520and%2520Jianglong%2520Qin%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520leverages%2520topological%2520connectivity%2520theory%2520to%250Aenhance%2520edge%2520precision%2520and%2520regional%2520consistency.%2520However%252C%2520existing%2520deep%250Anetworks%2520integrating%2520connectivity%2520often%2520forcibly%2520inject%2520it%2520as%2520an%2520additional%250Afeature%2520module%252C%2520resulting%2520in%2520coupled%2520feature%2520spaces%2520with%2520no%2520standardized%250Amechanism%2520to%2520quantify%2520different%2520feature%2520strengths.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520DCFFSNet%2520%2528Dual-Connectivity%2520Feature%2520Fusion-Separation%2520Network%2529.%2520It%250Aintroduces%2520an%2520innovative%2520feature%2520space%2520decoupling%2520strategy.%2520This%2520strategy%250Aquantifies%2520the%2520relative%2520strength%2520between%2520connectivity%2520features%2520and%2520other%250Afeatures.%2520It%2520then%2520builds%2520a%2520deep%2520connectivity%2520feature%2520fusion-separation%250Aarchitecture.%2520This%2520architecture%2520dynamically%2520balances%2520multi-scale%2520feature%250Aexpression.%2520Experiments%2520were%2520conducted%2520on%2520the%2520ISIC2018%252C%2520DSB2018%252C%2520and%2520MoNuSeg%250Adatasets.%2520On%2520ISIC2018%252C%2520DCFFSNet%2520outperformed%2520the%2520next%2520best%2520model%2520%2528CMUNet%2529%2520by%250A1.3%2525%2520%2528Dice%2529%2520and%25201.2%2525%2520%2528IoU%2529.%2520On%2520DSB2018%252C%2520it%2520surpassed%2520TransUNet%2520by%25200.7%2525%2520%2528Dice%2529%250Aand%25200.9%2525%2520%2528IoU%2529.%2520On%2520MoNuSeg%252C%2520it%2520exceeded%2520CSCAUNet%2520by%25200.8%2525%2520%2528Dice%2529%2520and%25200.9%2525%2520%2528IoU%2529.%250AThe%2520results%2520demonstrate%2520that%2520DCFFSNet%2520exceeds%2520existing%2520mainstream%2520methods%250Aacross%2520all%2520metrics.%2520It%2520effectively%2520resolves%2520segmentation%2520fragmentation%2520and%250Aachieves%2520smooth%2520edge%2520transitions.%2520This%2520significantly%2520enhances%2520clinical%250Ausability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCFFSNet%3A%20Deep%20Connectivity%20Feature%20Fusion%20Separation%20Network%20for%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Xun%20Ye%20and%20Ruixiang%20Tang%20and%20Mingda%20Zhang%20and%20Jianglong%20Qin&entry.1292438233=%20%20Medical%20image%20segmentation%20leverages%20topological%20connectivity%20theory%20to%0Aenhance%20edge%20precision%20and%20regional%20consistency.%20However%2C%20existing%20deep%0Anetworks%20integrating%20connectivity%20often%20forcibly%20inject%20it%20as%20an%20additional%0Afeature%20module%2C%20resulting%20in%20coupled%20feature%20spaces%20with%20no%20standardized%0Amechanism%20to%20quantify%20different%20feature%20strengths.%20To%20address%20these%20issues%2C%20we%0Apropose%20DCFFSNet%20%28Dual-Connectivity%20Feature%20Fusion-Separation%20Network%29.%20It%0Aintroduces%20an%20innovative%20feature%20space%20decoupling%20strategy.%20This%20strategy%0Aquantifies%20the%20relative%20strength%20between%20connectivity%20features%20and%20other%0Afeatures.%20It%20then%20builds%20a%20deep%20connectivity%20feature%20fusion-separation%0Aarchitecture.%20This%20architecture%20dynamically%20balances%20multi-scale%20feature%0Aexpression.%20Experiments%20were%20conducted%20on%20the%20ISIC2018%2C%20DSB2018%2C%20and%20MoNuSeg%0Adatasets.%20On%20ISIC2018%2C%20DCFFSNet%20outperformed%20the%20next%20best%20model%20%28CMUNet%29%20by%0A1.3%25%20%28Dice%29%20and%201.2%25%20%28IoU%29.%20On%20DSB2018%2C%20it%20surpassed%20TransUNet%20by%200.7%25%20%28Dice%29%0Aand%200.9%25%20%28IoU%29.%20On%20MoNuSeg%2C%20it%20exceeded%20CSCAUNet%20by%200.8%25%20%28Dice%29%20and%200.9%25%20%28IoU%29.%0AThe%20results%20demonstrate%20that%20DCFFSNet%20exceeds%20existing%20mainstream%20methods%0Aacross%20all%20metrics.%20It%20effectively%20resolves%20segmentation%20fragmentation%20and%0Aachieves%20smooth%20edge%20transitions.%20This%20significantly%20enhances%20clinical%0Ausability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18407v1&entry.124074799=Read"},
{"title": "Realtime Limb Trajectory Optimization for Humanoid Running Through\n  Centroidal Angular Momentum Dynamics", "author": "Sait Sovukluk and Robert Schuller and Johannes Englsberger and Christian Ott", "abstract": "  One of the essential aspects of humanoid robot running is determining the\nlimb-swinging trajectories. During the flight phases, where the ground reaction\nforces are not available for regulation, the limb swinging trajectories are\nsignificant for the stability of the next stance phase. Due to the conservation\nof angular momentum, improper leg and arm swinging results in highly tilted and\nunsustainable body configurations at the next stance phase landing. In such\ncases, the robotic system fails to maintain locomotion independent of the\nstability of the center of mass trajectories. This problem is more apparent for\nfast and high flight time trajectories. This paper proposes a real-time\nnonlinear limb trajectory optimization problem for humanoid running. The\noptimization problem is tested on two different humanoid robot models, and the\ngenerated trajectories are verified using a running algorithm for both robots\nin a simulation environment.\n", "link": "http://arxiv.org/abs/2501.17351v3", "date": "2025-07-24", "relevancy": 2.0856, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5329}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realtime%20Limb%20Trajectory%20Optimization%20for%20Humanoid%20Running%20Through%0A%20%20Centroidal%20Angular%20Momentum%20Dynamics&body=Title%3A%20Realtime%20Limb%20Trajectory%20Optimization%20for%20Humanoid%20Running%20Through%0A%20%20Centroidal%20Angular%20Momentum%20Dynamics%0AAuthor%3A%20Sait%20Sovukluk%20and%20Robert%20Schuller%20and%20Johannes%20Englsberger%20and%20Christian%20Ott%0AAbstract%3A%20%20%20One%20of%20the%20essential%20aspects%20of%20humanoid%20robot%20running%20is%20determining%20the%0Alimb-swinging%20trajectories.%20During%20the%20flight%20phases%2C%20where%20the%20ground%20reaction%0Aforces%20are%20not%20available%20for%20regulation%2C%20the%20limb%20swinging%20trajectories%20are%0Asignificant%20for%20the%20stability%20of%20the%20next%20stance%20phase.%20Due%20to%20the%20conservation%0Aof%20angular%20momentum%2C%20improper%20leg%20and%20arm%20swinging%20results%20in%20highly%20tilted%20and%0Aunsustainable%20body%20configurations%20at%20the%20next%20stance%20phase%20landing.%20In%20such%0Acases%2C%20the%20robotic%20system%20fails%20to%20maintain%20locomotion%20independent%20of%20the%0Astability%20of%20the%20center%20of%20mass%20trajectories.%20This%20problem%20is%20more%20apparent%20for%0Afast%20and%20high%20flight%20time%20trajectories.%20This%20paper%20proposes%20a%20real-time%0Anonlinear%20limb%20trajectory%20optimization%20problem%20for%20humanoid%20running.%20The%0Aoptimization%20problem%20is%20tested%20on%20two%20different%20humanoid%20robot%20models%2C%20and%20the%0Agenerated%20trajectories%20are%20verified%20using%20a%20running%20algorithm%20for%20both%20robots%0Ain%20a%20simulation%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17351v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealtime%2520Limb%2520Trajectory%2520Optimization%2520for%2520Humanoid%2520Running%2520Through%250A%2520%2520Centroidal%2520Angular%2520Momentum%2520Dynamics%26entry.906535625%3DSait%2520Sovukluk%2520and%2520Robert%2520Schuller%2520and%2520Johannes%2520Englsberger%2520and%2520Christian%2520Ott%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520essential%2520aspects%2520of%2520humanoid%2520robot%2520running%2520is%2520determining%2520the%250Alimb-swinging%2520trajectories.%2520During%2520the%2520flight%2520phases%252C%2520where%2520the%2520ground%2520reaction%250Aforces%2520are%2520not%2520available%2520for%2520regulation%252C%2520the%2520limb%2520swinging%2520trajectories%2520are%250Asignificant%2520for%2520the%2520stability%2520of%2520the%2520next%2520stance%2520phase.%2520Due%2520to%2520the%2520conservation%250Aof%2520angular%2520momentum%252C%2520improper%2520leg%2520and%2520arm%2520swinging%2520results%2520in%2520highly%2520tilted%2520and%250Aunsustainable%2520body%2520configurations%2520at%2520the%2520next%2520stance%2520phase%2520landing.%2520In%2520such%250Acases%252C%2520the%2520robotic%2520system%2520fails%2520to%2520maintain%2520locomotion%2520independent%2520of%2520the%250Astability%2520of%2520the%2520center%2520of%2520mass%2520trajectories.%2520This%2520problem%2520is%2520more%2520apparent%2520for%250Afast%2520and%2520high%2520flight%2520time%2520trajectories.%2520This%2520paper%2520proposes%2520a%2520real-time%250Anonlinear%2520limb%2520trajectory%2520optimization%2520problem%2520for%2520humanoid%2520running.%2520The%250Aoptimization%2520problem%2520is%2520tested%2520on%2520two%2520different%2520humanoid%2520robot%2520models%252C%2520and%2520the%250Agenerated%2520trajectories%2520are%2520verified%2520using%2520a%2520running%2520algorithm%2520for%2520both%2520robots%250Ain%2520a%2520simulation%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17351v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realtime%20Limb%20Trajectory%20Optimization%20for%20Humanoid%20Running%20Through%0A%20%20Centroidal%20Angular%20Momentum%20Dynamics&entry.906535625=Sait%20Sovukluk%20and%20Robert%20Schuller%20and%20Johannes%20Englsberger%20and%20Christian%20Ott&entry.1292438233=%20%20One%20of%20the%20essential%20aspects%20of%20humanoid%20robot%20running%20is%20determining%20the%0Alimb-swinging%20trajectories.%20During%20the%20flight%20phases%2C%20where%20the%20ground%20reaction%0Aforces%20are%20not%20available%20for%20regulation%2C%20the%20limb%20swinging%20trajectories%20are%0Asignificant%20for%20the%20stability%20of%20the%20next%20stance%20phase.%20Due%20to%20the%20conservation%0Aof%20angular%20momentum%2C%20improper%20leg%20and%20arm%20swinging%20results%20in%20highly%20tilted%20and%0Aunsustainable%20body%20configurations%20at%20the%20next%20stance%20phase%20landing.%20In%20such%0Acases%2C%20the%20robotic%20system%20fails%20to%20maintain%20locomotion%20independent%20of%20the%0Astability%20of%20the%20center%20of%20mass%20trajectories.%20This%20problem%20is%20more%20apparent%20for%0Afast%20and%20high%20flight%20time%20trajectories.%20This%20paper%20proposes%20a%20real-time%0Anonlinear%20limb%20trajectory%20optimization%20problem%20for%20humanoid%20running.%20The%0Aoptimization%20problem%20is%20tested%20on%20two%20different%20humanoid%20robot%20models%2C%20and%20the%0Agenerated%20trajectories%20are%20verified%20using%20a%20running%20algorithm%20for%20both%20robots%0Ain%20a%20simulation%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17351v3&entry.124074799=Read"},
{"title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning", "author": "Yanjun Zheng and Xiyang Du and Longfei Liao and Xiaoke Zhao and Zhaowen Zhou and Jingze Song and Bo Zhang and Jiawei Liu and Xiang Qi and Zhe Li and Zhiqiang Zhang and Wei Wang and Peng Zhang", "abstract": "  Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.\n", "link": "http://arxiv.org/abs/2507.16802v3", "date": "2025-07-24", "relevancy": 2.0841, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentar-Fin-R1%3A%20Enhancing%20Financial%20Intelligence%20through%20Domain%0A%20%20Expertise%2C%20Training%20Efficiency%2C%20and%20Advanced%20Reasoning&body=Title%3A%20Agentar-Fin-R1%3A%20Enhancing%20Financial%20Intelligence%20through%20Domain%0A%20%20Expertise%2C%20Training%20Efficiency%2C%20and%20Advanced%20Reasoning%0AAuthor%3A%20Yanjun%20Zheng%20and%20Xiyang%20Du%20and%20Longfei%20Liao%20and%20Xiaoke%20Zhao%20and%20Zhaowen%20Zhou%20and%20Jingze%20Song%20and%20Bo%20Zhang%20and%20Jiawei%20Liu%20and%20Xiang%20Qi%20and%20Zhe%20Li%20and%20Zhiqiang%20Zhang%20and%20Wei%20Wang%20and%20Peng%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20considerable%20promise%20in%20financial%0Aapplications%3B%20however%2C%20prevailing%20models%20frequently%20demonstrate%20limitations%0Awhen%20confronted%20with%20scenarios%20that%20necessitate%20sophisticated%20reasoning%0Acapabilities%2C%20stringent%20trustworthiness%20criteria%2C%20and%20efficient%20adaptation%20to%0Adomain-specific%20requirements.%20We%20introduce%20the%20Agentar-Fin-R1%20series%20of%0Afinancial%20large%20language%20models%20%288B%20and%2032B%20parameters%29%2C%20specifically%0Aengineered%20based%20on%20the%20Qwen3%20foundation%20model%20to%20enhance%20reasoning%0Acapabilities%2C%20reliability%2C%20and%20domain%20specialization%20for%20financial%0Aapplications.%20Our%20optimization%20approach%20integrates%20a%20high-quality%2C%20systematic%0Afinancial%20task%20label%20system%20with%20a%20comprehensive%20multi-layered%20trustworthiness%0Aassurance%20framework.%20This%20framework%20encompasses%20high-quality%20trustworthy%0Aknowledge%20engineering%2C%20multi-agent%20trustworthy%20data%20synthesis%2C%20and%20rigorous%0Adata%20validation%20governance.%20Through%20label-guided%20automated%20difficulty-aware%0Aoptimization%2C%20tow-stage%20training%20pipeline%2C%20and%20dynamic%20attribution%20systems%2C%20we%0Aachieve%20substantial%20improvements%20in%20training%20efficiency.%20Our%20models%20undergo%0Acomprehensive%20evaluation%20on%20mainstream%20financial%20benchmarks%20including%20Fineva%2C%0AFinEval%2C%20and%20FinanceIQ%2C%20as%20well%20as%20general%20reasoning%20datasets%20such%20as%20MATH-500%0Aand%20GPQA-diamond.%20To%20thoroughly%20assess%20real-world%20deployment%20capabilities%2C%20we%0Ainnovatively%20propose%20the%20Finova%20evaluation%20benchmark%2C%20which%20focuses%20on%0Aagent-level%20financial%20reasoning%20and%20compliance%20verification.%20Experimental%0Aresults%20demonstrate%20that%20Agentar-Fin-R1%20not%20only%20achieves%20state-of-the-art%0Aperformance%20on%20financial%20tasks%20but%20also%20exhibits%20exceptional%20general%20reasoning%0Acapabilities%2C%20validating%20its%20effectiveness%20as%20a%20trustworthy%20solution%20for%0Ahigh-stakes%20financial%20applications.%20The%20Finova%20bench%20is%20available%20at%0Ahttps%3A//github.com/antgroup/Finova.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16802v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentar-Fin-R1%253A%2520Enhancing%2520Financial%2520Intelligence%2520through%2520Domain%250A%2520%2520Expertise%252C%2520Training%2520Efficiency%252C%2520and%2520Advanced%2520Reasoning%26entry.906535625%3DYanjun%2520Zheng%2520and%2520Xiyang%2520Du%2520and%2520Longfei%2520Liao%2520and%2520Xiaoke%2520Zhao%2520and%2520Zhaowen%2520Zhou%2520and%2520Jingze%2520Song%2520and%2520Bo%2520Zhang%2520and%2520Jiawei%2520Liu%2520and%2520Xiang%2520Qi%2520and%2520Zhe%2520Li%2520and%2520Zhiqiang%2520Zhang%2520and%2520Wei%2520Wang%2520and%2520Peng%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520considerable%2520promise%2520in%2520financial%250Aapplications%253B%2520however%252C%2520prevailing%2520models%2520frequently%2520demonstrate%2520limitations%250Awhen%2520confronted%2520with%2520scenarios%2520that%2520necessitate%2520sophisticated%2520reasoning%250Acapabilities%252C%2520stringent%2520trustworthiness%2520criteria%252C%2520and%2520efficient%2520adaptation%2520to%250Adomain-specific%2520requirements.%2520We%2520introduce%2520the%2520Agentar-Fin-R1%2520series%2520of%250Afinancial%2520large%2520language%2520models%2520%25288B%2520and%252032B%2520parameters%2529%252C%2520specifically%250Aengineered%2520based%2520on%2520the%2520Qwen3%2520foundation%2520model%2520to%2520enhance%2520reasoning%250Acapabilities%252C%2520reliability%252C%2520and%2520domain%2520specialization%2520for%2520financial%250Aapplications.%2520Our%2520optimization%2520approach%2520integrates%2520a%2520high-quality%252C%2520systematic%250Afinancial%2520task%2520label%2520system%2520with%2520a%2520comprehensive%2520multi-layered%2520trustworthiness%250Aassurance%2520framework.%2520This%2520framework%2520encompasses%2520high-quality%2520trustworthy%250Aknowledge%2520engineering%252C%2520multi-agent%2520trustworthy%2520data%2520synthesis%252C%2520and%2520rigorous%250Adata%2520validation%2520governance.%2520Through%2520label-guided%2520automated%2520difficulty-aware%250Aoptimization%252C%2520tow-stage%2520training%2520pipeline%252C%2520and%2520dynamic%2520attribution%2520systems%252C%2520we%250Aachieve%2520substantial%2520improvements%2520in%2520training%2520efficiency.%2520Our%2520models%2520undergo%250Acomprehensive%2520evaluation%2520on%2520mainstream%2520financial%2520benchmarks%2520including%2520Fineva%252C%250AFinEval%252C%2520and%2520FinanceIQ%252C%2520as%2520well%2520as%2520general%2520reasoning%2520datasets%2520such%2520as%2520MATH-500%250Aand%2520GPQA-diamond.%2520To%2520thoroughly%2520assess%2520real-world%2520deployment%2520capabilities%252C%2520we%250Ainnovatively%2520propose%2520the%2520Finova%2520evaluation%2520benchmark%252C%2520which%2520focuses%2520on%250Aagent-level%2520financial%2520reasoning%2520and%2520compliance%2520verification.%2520Experimental%250Aresults%2520demonstrate%2520that%2520Agentar-Fin-R1%2520not%2520only%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520financial%2520tasks%2520but%2520also%2520exhibits%2520exceptional%2520general%2520reasoning%250Acapabilities%252C%2520validating%2520its%2520effectiveness%2520as%2520a%2520trustworthy%2520solution%2520for%250Ahigh-stakes%2520financial%2520applications.%2520The%2520Finova%2520bench%2520is%2520available%2520at%250Ahttps%253A//github.com/antgroup/Finova.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16802v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentar-Fin-R1%3A%20Enhancing%20Financial%20Intelligence%20through%20Domain%0A%20%20Expertise%2C%20Training%20Efficiency%2C%20and%20Advanced%20Reasoning&entry.906535625=Yanjun%20Zheng%20and%20Xiyang%20Du%20and%20Longfei%20Liao%20and%20Xiaoke%20Zhao%20and%20Zhaowen%20Zhou%20and%20Jingze%20Song%20and%20Bo%20Zhang%20and%20Jiawei%20Liu%20and%20Xiang%20Qi%20and%20Zhe%20Li%20and%20Zhiqiang%20Zhang%20and%20Wei%20Wang%20and%20Peng%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20considerable%20promise%20in%20financial%0Aapplications%3B%20however%2C%20prevailing%20models%20frequently%20demonstrate%20limitations%0Awhen%20confronted%20with%20scenarios%20that%20necessitate%20sophisticated%20reasoning%0Acapabilities%2C%20stringent%20trustworthiness%20criteria%2C%20and%20efficient%20adaptation%20to%0Adomain-specific%20requirements.%20We%20introduce%20the%20Agentar-Fin-R1%20series%20of%0Afinancial%20large%20language%20models%20%288B%20and%2032B%20parameters%29%2C%20specifically%0Aengineered%20based%20on%20the%20Qwen3%20foundation%20model%20to%20enhance%20reasoning%0Acapabilities%2C%20reliability%2C%20and%20domain%20specialization%20for%20financial%0Aapplications.%20Our%20optimization%20approach%20integrates%20a%20high-quality%2C%20systematic%0Afinancial%20task%20label%20system%20with%20a%20comprehensive%20multi-layered%20trustworthiness%0Aassurance%20framework.%20This%20framework%20encompasses%20high-quality%20trustworthy%0Aknowledge%20engineering%2C%20multi-agent%20trustworthy%20data%20synthesis%2C%20and%20rigorous%0Adata%20validation%20governance.%20Through%20label-guided%20automated%20difficulty-aware%0Aoptimization%2C%20tow-stage%20training%20pipeline%2C%20and%20dynamic%20attribution%20systems%2C%20we%0Aachieve%20substantial%20improvements%20in%20training%20efficiency.%20Our%20models%20undergo%0Acomprehensive%20evaluation%20on%20mainstream%20financial%20benchmarks%20including%20Fineva%2C%0AFinEval%2C%20and%20FinanceIQ%2C%20as%20well%20as%20general%20reasoning%20datasets%20such%20as%20MATH-500%0Aand%20GPQA-diamond.%20To%20thoroughly%20assess%20real-world%20deployment%20capabilities%2C%20we%0Ainnovatively%20propose%20the%20Finova%20evaluation%20benchmark%2C%20which%20focuses%20on%0Aagent-level%20financial%20reasoning%20and%20compliance%20verification.%20Experimental%0Aresults%20demonstrate%20that%20Agentar-Fin-R1%20not%20only%20achieves%20state-of-the-art%0Aperformance%20on%20financial%20tasks%20but%20also%20exhibits%20exceptional%20general%20reasoning%0Acapabilities%2C%20validating%20its%20effectiveness%20as%20a%20trustworthy%20solution%20for%0Ahigh-stakes%20financial%20applications.%20The%20Finova%20bench%20is%20available%20at%0Ahttps%3A//github.com/antgroup/Finova.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16802v3&entry.124074799=Read"},
{"title": "Towards Large Scale Geostatistical Methane Monitoring with Part-based\n  Object Detection", "author": "Adhemar de Senneville and Xavier Bou and Thibaud Ehret and Rafael Grompone and Jean Louis Bonne and Nicolas Dumelie and Thomas Lauvaux and Gabriele Facciolo", "abstract": "  Object detection is one of the main applications of computer vision in remote\nsensing imagery. Despite its increasing availability, the sheer volume of\nremote sensing data poses a challenge when detecting rare objects across large\ngeographic areas. Paradoxically, this common challenge is crucial to many\napplications, such as estimating environmental impact of certain human\nactivities at scale. In this paper, we propose to address the problem by\ninvestigating the methane production and emissions of bio-digesters in France.\nWe first introduce a novel dataset containing bio-digesters, with small\ntraining and validation sets, and a large test set with a high imbalance\ntowards observations without objects since such sites are rare. We develop a\npart-based method that considers essential bio-digester sub-elements to boost\ninitial detections. To this end, we apply our method to new, unseen regions to\nbuild an inventory of bio-digesters. We then compute geostatistical estimates\nof the quantity of methane produced that can be attributed to these\ninfrastructures in a given area at a given time.\n", "link": "http://arxiv.org/abs/2507.18513v1", "date": "2025-07-24", "relevancy": 2.0762, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5422}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5206}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Large%20Scale%20Geostatistical%20Methane%20Monitoring%20with%20Part-based%0A%20%20Object%20Detection&body=Title%3A%20Towards%20Large%20Scale%20Geostatistical%20Methane%20Monitoring%20with%20Part-based%0A%20%20Object%20Detection%0AAuthor%3A%20Adhemar%20de%20Senneville%20and%20Xavier%20Bou%20and%20Thibaud%20Ehret%20and%20Rafael%20Grompone%20and%20Jean%20Louis%20Bonne%20and%20Nicolas%20Dumelie%20and%20Thomas%20Lauvaux%20and%20Gabriele%20Facciolo%0AAbstract%3A%20%20%20Object%20detection%20is%20one%20of%20the%20main%20applications%20of%20computer%20vision%20in%20remote%0Asensing%20imagery.%20Despite%20its%20increasing%20availability%2C%20the%20sheer%20volume%20of%0Aremote%20sensing%20data%20poses%20a%20challenge%20when%20detecting%20rare%20objects%20across%20large%0Ageographic%20areas.%20Paradoxically%2C%20this%20common%20challenge%20is%20crucial%20to%20many%0Aapplications%2C%20such%20as%20estimating%20environmental%20impact%20of%20certain%20human%0Aactivities%20at%20scale.%20In%20this%20paper%2C%20we%20propose%20to%20address%20the%20problem%20by%0Ainvestigating%20the%20methane%20production%20and%20emissions%20of%20bio-digesters%20in%20France.%0AWe%20first%20introduce%20a%20novel%20dataset%20containing%20bio-digesters%2C%20with%20small%0Atraining%20and%20validation%20sets%2C%20and%20a%20large%20test%20set%20with%20a%20high%20imbalance%0Atowards%20observations%20without%20objects%20since%20such%20sites%20are%20rare.%20We%20develop%20a%0Apart-based%20method%20that%20considers%20essential%20bio-digester%20sub-elements%20to%20boost%0Ainitial%20detections.%20To%20this%20end%2C%20we%20apply%20our%20method%20to%20new%2C%20unseen%20regions%20to%0Abuild%20an%20inventory%20of%20bio-digesters.%20We%20then%20compute%20geostatistical%20estimates%0Aof%20the%20quantity%20of%20methane%20produced%20that%20can%20be%20attributed%20to%20these%0Ainfrastructures%20in%20a%20given%20area%20at%20a%20given%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Large%2520Scale%2520Geostatistical%2520Methane%2520Monitoring%2520with%2520Part-based%250A%2520%2520Object%2520Detection%26entry.906535625%3DAdhemar%2520de%2520Senneville%2520and%2520Xavier%2520Bou%2520and%2520Thibaud%2520Ehret%2520and%2520Rafael%2520Grompone%2520and%2520Jean%2520Louis%2520Bonne%2520and%2520Nicolas%2520Dumelie%2520and%2520Thomas%2520Lauvaux%2520and%2520Gabriele%2520Facciolo%26entry.1292438233%3D%2520%2520Object%2520detection%2520is%2520one%2520of%2520the%2520main%2520applications%2520of%2520computer%2520vision%2520in%2520remote%250Asensing%2520imagery.%2520Despite%2520its%2520increasing%2520availability%252C%2520the%2520sheer%2520volume%2520of%250Aremote%2520sensing%2520data%2520poses%2520a%2520challenge%2520when%2520detecting%2520rare%2520objects%2520across%2520large%250Ageographic%2520areas.%2520Paradoxically%252C%2520this%2520common%2520challenge%2520is%2520crucial%2520to%2520many%250Aapplications%252C%2520such%2520as%2520estimating%2520environmental%2520impact%2520of%2520certain%2520human%250Aactivities%2520at%2520scale.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520address%2520the%2520problem%2520by%250Ainvestigating%2520the%2520methane%2520production%2520and%2520emissions%2520of%2520bio-digesters%2520in%2520France.%250AWe%2520first%2520introduce%2520a%2520novel%2520dataset%2520containing%2520bio-digesters%252C%2520with%2520small%250Atraining%2520and%2520validation%2520sets%252C%2520and%2520a%2520large%2520test%2520set%2520with%2520a%2520high%2520imbalance%250Atowards%2520observations%2520without%2520objects%2520since%2520such%2520sites%2520are%2520rare.%2520We%2520develop%2520a%250Apart-based%2520method%2520that%2520considers%2520essential%2520bio-digester%2520sub-elements%2520to%2520boost%250Ainitial%2520detections.%2520To%2520this%2520end%252C%2520we%2520apply%2520our%2520method%2520to%2520new%252C%2520unseen%2520regions%2520to%250Abuild%2520an%2520inventory%2520of%2520bio-digesters.%2520We%2520then%2520compute%2520geostatistical%2520estimates%250Aof%2520the%2520quantity%2520of%2520methane%2520produced%2520that%2520can%2520be%2520attributed%2520to%2520these%250Ainfrastructures%2520in%2520a%2520given%2520area%2520at%2520a%2520given%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Large%20Scale%20Geostatistical%20Methane%20Monitoring%20with%20Part-based%0A%20%20Object%20Detection&entry.906535625=Adhemar%20de%20Senneville%20and%20Xavier%20Bou%20and%20Thibaud%20Ehret%20and%20Rafael%20Grompone%20and%20Jean%20Louis%20Bonne%20and%20Nicolas%20Dumelie%20and%20Thomas%20Lauvaux%20and%20Gabriele%20Facciolo&entry.1292438233=%20%20Object%20detection%20is%20one%20of%20the%20main%20applications%20of%20computer%20vision%20in%20remote%0Asensing%20imagery.%20Despite%20its%20increasing%20availability%2C%20the%20sheer%20volume%20of%0Aremote%20sensing%20data%20poses%20a%20challenge%20when%20detecting%20rare%20objects%20across%20large%0Ageographic%20areas.%20Paradoxically%2C%20this%20common%20challenge%20is%20crucial%20to%20many%0Aapplications%2C%20such%20as%20estimating%20environmental%20impact%20of%20certain%20human%0Aactivities%20at%20scale.%20In%20this%20paper%2C%20we%20propose%20to%20address%20the%20problem%20by%0Ainvestigating%20the%20methane%20production%20and%20emissions%20of%20bio-digesters%20in%20France.%0AWe%20first%20introduce%20a%20novel%20dataset%20containing%20bio-digesters%2C%20with%20small%0Atraining%20and%20validation%20sets%2C%20and%20a%20large%20test%20set%20with%20a%20high%20imbalance%0Atowards%20observations%20without%20objects%20since%20such%20sites%20are%20rare.%20We%20develop%20a%0Apart-based%20method%20that%20considers%20essential%20bio-digester%20sub-elements%20to%20boost%0Ainitial%20detections.%20To%20this%20end%2C%20we%20apply%20our%20method%20to%20new%2C%20unseen%20regions%20to%0Abuild%20an%20inventory%20of%20bio-digesters.%20We%20then%20compute%20geostatistical%20estimates%0Aof%20the%20quantity%20of%20methane%20produced%20that%20can%20be%20attributed%20to%20these%0Ainfrastructures%20in%20a%20given%20area%20at%20a%20given%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18513v1&entry.124074799=Read"},
{"title": "Automated Code Review Using Large Language Models with Symbolic\n  Reasoning", "author": "Busra Icoz and Goksel Biricik", "abstract": "  Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.\n", "link": "http://arxiv.org/abs/2507.18476v1", "date": "2025-07-24", "relevancy": 2.0758, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Code%20Review%20Using%20Large%20Language%20Models%20with%20Symbolic%0A%20%20Reasoning&body=Title%3A%20Automated%20Code%20Review%20Using%20Large%20Language%20Models%20with%20Symbolic%0A%20%20Reasoning%0AAuthor%3A%20Busra%20Icoz%20and%20Goksel%20Biricik%0AAbstract%3A%20%20%20Code%20review%20is%20one%20of%20the%20key%20processes%20in%20the%20software%20development%20lifecycle%0Aand%20is%20essential%20to%20maintain%20code%20quality.%20However%2C%20manual%20code%20review%20is%0Asubjective%20and%20time%20consuming.%20Given%20its%20rule-based%20nature%2C%20code%20review%20is%20well%0Asuited%20for%20automation.%20In%20recent%20years%2C%20significant%20efforts%20have%20been%20made%20to%0Aautomate%20this%20process%20with%20the%20help%20of%20artificial%20intelligence.%20Recent%0Adevelopments%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20also%20emerged%20as%20a%20promising%0Atool%20in%20this%20area%2C%20but%20these%20models%20often%20lack%20the%20logical%20reasoning%0Acapabilities%20needed%20to%20fully%20understand%20and%20evaluate%20code.%20To%20overcome%20this%0Alimitation%2C%20this%20study%20proposes%20a%20hybrid%20approach%20that%20integrates%20symbolic%0Areasoning%20techniques%20with%20LLMs%20to%20automate%20the%20code%20review%20process.%20We%20tested%0Aour%20approach%20using%20the%20CodexGlue%20dataset%2C%20comparing%20several%20models%2C%20including%0ACodeT5%2C%20CodeBERT%2C%20and%20GraphCodeBERT%2C%20to%20assess%20the%20effectiveness%20of%20combining%0Asymbolic%20reasoning%20and%20prompting%20techniques%20with%20LLMs.%20Our%20results%20show%20that%0Athis%20approach%20improves%20the%20accuracy%20and%20efficiency%20of%20automated%20code%20review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Code%2520Review%2520Using%2520Large%2520Language%2520Models%2520with%2520Symbolic%250A%2520%2520Reasoning%26entry.906535625%3DBusra%2520Icoz%2520and%2520Goksel%2520Biricik%26entry.1292438233%3D%2520%2520Code%2520review%2520is%2520one%2520of%2520the%2520key%2520processes%2520in%2520the%2520software%2520development%2520lifecycle%250Aand%2520is%2520essential%2520to%2520maintain%2520code%2520quality.%2520However%252C%2520manual%2520code%2520review%2520is%250Asubjective%2520and%2520time%2520consuming.%2520Given%2520its%2520rule-based%2520nature%252C%2520code%2520review%2520is%2520well%250Asuited%2520for%2520automation.%2520In%2520recent%2520years%252C%2520significant%2520efforts%2520have%2520been%2520made%2520to%250Aautomate%2520this%2520process%2520with%2520the%2520help%2520of%2520artificial%2520intelligence.%2520Recent%250Adevelopments%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520also%2520emerged%2520as%2520a%2520promising%250Atool%2520in%2520this%2520area%252C%2520but%2520these%2520models%2520often%2520lack%2520the%2520logical%2520reasoning%250Acapabilities%2520needed%2520to%2520fully%2520understand%2520and%2520evaluate%2520code.%2520To%2520overcome%2520this%250Alimitation%252C%2520this%2520study%2520proposes%2520a%2520hybrid%2520approach%2520that%2520integrates%2520symbolic%250Areasoning%2520techniques%2520with%2520LLMs%2520to%2520automate%2520the%2520code%2520review%2520process.%2520We%2520tested%250Aour%2520approach%2520using%2520the%2520CodexGlue%2520dataset%252C%2520comparing%2520several%2520models%252C%2520including%250ACodeT5%252C%2520CodeBERT%252C%2520and%2520GraphCodeBERT%252C%2520to%2520assess%2520the%2520effectiveness%2520of%2520combining%250Asymbolic%2520reasoning%2520and%2520prompting%2520techniques%2520with%2520LLMs.%2520Our%2520results%2520show%2520that%250Athis%2520approach%2520improves%2520the%2520accuracy%2520and%2520efficiency%2520of%2520automated%2520code%2520review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Code%20Review%20Using%20Large%20Language%20Models%20with%20Symbolic%0A%20%20Reasoning&entry.906535625=Busra%20Icoz%20and%20Goksel%20Biricik&entry.1292438233=%20%20Code%20review%20is%20one%20of%20the%20key%20processes%20in%20the%20software%20development%20lifecycle%0Aand%20is%20essential%20to%20maintain%20code%20quality.%20However%2C%20manual%20code%20review%20is%0Asubjective%20and%20time%20consuming.%20Given%20its%20rule-based%20nature%2C%20code%20review%20is%20well%0Asuited%20for%20automation.%20In%20recent%20years%2C%20significant%20efforts%20have%20been%20made%20to%0Aautomate%20this%20process%20with%20the%20help%20of%20artificial%20intelligence.%20Recent%0Adevelopments%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20also%20emerged%20as%20a%20promising%0Atool%20in%20this%20area%2C%20but%20these%20models%20often%20lack%20the%20logical%20reasoning%0Acapabilities%20needed%20to%20fully%20understand%20and%20evaluate%20code.%20To%20overcome%20this%0Alimitation%2C%20this%20study%20proposes%20a%20hybrid%20approach%20that%20integrates%20symbolic%0Areasoning%20techniques%20with%20LLMs%20to%20automate%20the%20code%20review%20process.%20We%20tested%0Aour%20approach%20using%20the%20CodexGlue%20dataset%2C%20comparing%20several%20models%2C%20including%0ACodeT5%2C%20CodeBERT%2C%20and%20GraphCodeBERT%2C%20to%20assess%20the%20effectiveness%20of%20combining%0Asymbolic%20reasoning%20and%20prompting%20techniques%20with%20LLMs.%20Our%20results%20show%20that%0Athis%20approach%20improves%20the%20accuracy%20and%20efficiency%20of%20automated%20code%20review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18476v1&entry.124074799=Read"},
{"title": "Automated Code Review Using Large Language Models with Symbolic\n  Reasoning", "author": "Busra Icoz and Goksel Biricik", "abstract": "  Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.\n", "link": "http://arxiv.org/abs/2507.18476v1", "date": "2025-07-24", "relevancy": 2.0758, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Code%20Review%20Using%20Large%20Language%20Models%20with%20Symbolic%0A%20%20Reasoning&body=Title%3A%20Automated%20Code%20Review%20Using%20Large%20Language%20Models%20with%20Symbolic%0A%20%20Reasoning%0AAuthor%3A%20Busra%20Icoz%20and%20Goksel%20Biricik%0AAbstract%3A%20%20%20Code%20review%20is%20one%20of%20the%20key%20processes%20in%20the%20software%20development%20lifecycle%0Aand%20is%20essential%20to%20maintain%20code%20quality.%20However%2C%20manual%20code%20review%20is%0Asubjective%20and%20time%20consuming.%20Given%20its%20rule-based%20nature%2C%20code%20review%20is%20well%0Asuited%20for%20automation.%20In%20recent%20years%2C%20significant%20efforts%20have%20been%20made%20to%0Aautomate%20this%20process%20with%20the%20help%20of%20artificial%20intelligence.%20Recent%0Adevelopments%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20also%20emerged%20as%20a%20promising%0Atool%20in%20this%20area%2C%20but%20these%20models%20often%20lack%20the%20logical%20reasoning%0Acapabilities%20needed%20to%20fully%20understand%20and%20evaluate%20code.%20To%20overcome%20this%0Alimitation%2C%20this%20study%20proposes%20a%20hybrid%20approach%20that%20integrates%20symbolic%0Areasoning%20techniques%20with%20LLMs%20to%20automate%20the%20code%20review%20process.%20We%20tested%0Aour%20approach%20using%20the%20CodexGlue%20dataset%2C%20comparing%20several%20models%2C%20including%0ACodeT5%2C%20CodeBERT%2C%20and%20GraphCodeBERT%2C%20to%20assess%20the%20effectiveness%20of%20combining%0Asymbolic%20reasoning%20and%20prompting%20techniques%20with%20LLMs.%20Our%20results%20show%20that%0Athis%20approach%20improves%20the%20accuracy%20and%20efficiency%20of%20automated%20code%20review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Code%2520Review%2520Using%2520Large%2520Language%2520Models%2520with%2520Symbolic%250A%2520%2520Reasoning%26entry.906535625%3DBusra%2520Icoz%2520and%2520Goksel%2520Biricik%26entry.1292438233%3D%2520%2520Code%2520review%2520is%2520one%2520of%2520the%2520key%2520processes%2520in%2520the%2520software%2520development%2520lifecycle%250Aand%2520is%2520essential%2520to%2520maintain%2520code%2520quality.%2520However%252C%2520manual%2520code%2520review%2520is%250Asubjective%2520and%2520time%2520consuming.%2520Given%2520its%2520rule-based%2520nature%252C%2520code%2520review%2520is%2520well%250Asuited%2520for%2520automation.%2520In%2520recent%2520years%252C%2520significant%2520efforts%2520have%2520been%2520made%2520to%250Aautomate%2520this%2520process%2520with%2520the%2520help%2520of%2520artificial%2520intelligence.%2520Recent%250Adevelopments%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520also%2520emerged%2520as%2520a%2520promising%250Atool%2520in%2520this%2520area%252C%2520but%2520these%2520models%2520often%2520lack%2520the%2520logical%2520reasoning%250Acapabilities%2520needed%2520to%2520fully%2520understand%2520and%2520evaluate%2520code.%2520To%2520overcome%2520this%250Alimitation%252C%2520this%2520study%2520proposes%2520a%2520hybrid%2520approach%2520that%2520integrates%2520symbolic%250Areasoning%2520techniques%2520with%2520LLMs%2520to%2520automate%2520the%2520code%2520review%2520process.%2520We%2520tested%250Aour%2520approach%2520using%2520the%2520CodexGlue%2520dataset%252C%2520comparing%2520several%2520models%252C%2520including%250ACodeT5%252C%2520CodeBERT%252C%2520and%2520GraphCodeBERT%252C%2520to%2520assess%2520the%2520effectiveness%2520of%2520combining%250Asymbolic%2520reasoning%2520and%2520prompting%2520techniques%2520with%2520LLMs.%2520Our%2520results%2520show%2520that%250Athis%2520approach%2520improves%2520the%2520accuracy%2520and%2520efficiency%2520of%2520automated%2520code%2520review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Code%20Review%20Using%20Large%20Language%20Models%20with%20Symbolic%0A%20%20Reasoning&entry.906535625=Busra%20Icoz%20and%20Goksel%20Biricik&entry.1292438233=%20%20Code%20review%20is%20one%20of%20the%20key%20processes%20in%20the%20software%20development%20lifecycle%0Aand%20is%20essential%20to%20maintain%20code%20quality.%20However%2C%20manual%20code%20review%20is%0Asubjective%20and%20time%20consuming.%20Given%20its%20rule-based%20nature%2C%20code%20review%20is%20well%0Asuited%20for%20automation.%20In%20recent%20years%2C%20significant%20efforts%20have%20been%20made%20to%0Aautomate%20this%20process%20with%20the%20help%20of%20artificial%20intelligence.%20Recent%0Adevelopments%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20also%20emerged%20as%20a%20promising%0Atool%20in%20this%20area%2C%20but%20these%20models%20often%20lack%20the%20logical%20reasoning%0Acapabilities%20needed%20to%20fully%20understand%20and%20evaluate%20code.%20To%20overcome%20this%0Alimitation%2C%20this%20study%20proposes%20a%20hybrid%20approach%20that%20integrates%20symbolic%0Areasoning%20techniques%20with%20LLMs%20to%20automate%20the%20code%20review%20process.%20We%20tested%0Aour%20approach%20using%20the%20CodexGlue%20dataset%2C%20comparing%20several%20models%2C%20including%0ACodeT5%2C%20CodeBERT%2C%20and%20GraphCodeBERT%2C%20to%20assess%20the%20effectiveness%20of%20combining%0Asymbolic%20reasoning%20and%20prompting%20techniques%20with%20LLMs.%20Our%20results%20show%20that%0Athis%20approach%20improves%20the%20accuracy%20and%20efficiency%20of%20automated%20code%20review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18476v1&entry.124074799=Read"},
{"title": "AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust\n  Odometry in Challenging Environments", "author": "Chenglong Qian and Yang Xu and Xiufang Shi and Jiming Chen and Liang Li", "abstract": "  In robotic navigation, maintaining precise pose estimation and navigation in\ncomplex and dynamic environments is crucial. However, environmental challenges\nsuch as smoke, tunnels, and adverse weather can significantly degrade the\nperformance of single-sensor systems like LiDAR or GPS, compromising the\noverall stability and safety of autonomous robots. To address these challenges,\nwe propose AF-RLIO: an adaptive fusion approach that integrates 4D\nmillimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to\nleverage the complementary strengths of these sensors for robust odometry\nestimation in complex environments. Our method consists of three key modules.\nFirstly, the pre-processing module utilizes radar data to assist LiDAR in\nremoving dynamic points and determining when environmental conditions are\ndegraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects\nappropriate point cloud data for scan-to-map matching and tightly couples it\nwith the IMU using the Iterative Error State Kalman Filter. Lastly, the factor\ngraph optimization module balances weights between odometry and GPS data,\nconstructing a pose graph for optimization. The proposed approach has been\nevaluated on datasets and tested in real-world robotic environments,\ndemonstrating its effectiveness and advantages over existing methods in\nchallenging conditions such as smoke and tunnels.\n", "link": "http://arxiv.org/abs/2507.18317v1", "date": "2025-07-24", "relevancy": 1.8487, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.637}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6206}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AF-RLIO%3A%20Adaptive%20Fusion%20of%20Radar-LiDAR-Inertial%20Information%20for%20Robust%0A%20%20Odometry%20in%20Challenging%20Environments&body=Title%3A%20AF-RLIO%3A%20Adaptive%20Fusion%20of%20Radar-LiDAR-Inertial%20Information%20for%20Robust%0A%20%20Odometry%20in%20Challenging%20Environments%0AAuthor%3A%20Chenglong%20Qian%20and%20Yang%20Xu%20and%20Xiufang%20Shi%20and%20Jiming%20Chen%20and%20Liang%20Li%0AAbstract%3A%20%20%20In%20robotic%20navigation%2C%20maintaining%20precise%20pose%20estimation%20and%20navigation%20in%0Acomplex%20and%20dynamic%20environments%20is%20crucial.%20However%2C%20environmental%20challenges%0Asuch%20as%20smoke%2C%20tunnels%2C%20and%20adverse%20weather%20can%20significantly%20degrade%20the%0Aperformance%20of%20single-sensor%20systems%20like%20LiDAR%20or%20GPS%2C%20compromising%20the%0Aoverall%20stability%20and%20safety%20of%20autonomous%20robots.%20To%20address%20these%20challenges%2C%0Awe%20propose%20AF-RLIO%3A%20an%20adaptive%20fusion%20approach%20that%20integrates%204D%0Amillimeter-wave%20radar%2C%20LiDAR%2C%20inertial%20measurement%20unit%20%28IMU%29%2C%20and%20GPS%20to%0Aleverage%20the%20complementary%20strengths%20of%20these%20sensors%20for%20robust%20odometry%0Aestimation%20in%20complex%20environments.%20Our%20method%20consists%20of%20three%20key%20modules.%0AFirstly%2C%20the%20pre-processing%20module%20utilizes%20radar%20data%20to%20assist%20LiDAR%20in%0Aremoving%20dynamic%20points%20and%20determining%20when%20environmental%20conditions%20are%0Adegraded%20for%20LiDAR.%20Secondly%2C%20the%20dynamic-aware%20multimodal%20odometry%20selects%0Aappropriate%20point%20cloud%20data%20for%20scan-to-map%20matching%20and%20tightly%20couples%20it%0Awith%20the%20IMU%20using%20the%20Iterative%20Error%20State%20Kalman%20Filter.%20Lastly%2C%20the%20factor%0Agraph%20optimization%20module%20balances%20weights%20between%20odometry%20and%20GPS%20data%2C%0Aconstructing%20a%20pose%20graph%20for%20optimization.%20The%20proposed%20approach%20has%20been%0Aevaluated%20on%20datasets%20and%20tested%20in%20real-world%20robotic%20environments%2C%0Ademonstrating%20its%20effectiveness%20and%20advantages%20over%20existing%20methods%20in%0Achallenging%20conditions%20such%20as%20smoke%20and%20tunnels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAF-RLIO%253A%2520Adaptive%2520Fusion%2520of%2520Radar-LiDAR-Inertial%2520Information%2520for%2520Robust%250A%2520%2520Odometry%2520in%2520Challenging%2520Environments%26entry.906535625%3DChenglong%2520Qian%2520and%2520Yang%2520Xu%2520and%2520Xiufang%2520Shi%2520and%2520Jiming%2520Chen%2520and%2520Liang%2520Li%26entry.1292438233%3D%2520%2520In%2520robotic%2520navigation%252C%2520maintaining%2520precise%2520pose%2520estimation%2520and%2520navigation%2520in%250Acomplex%2520and%2520dynamic%2520environments%2520is%2520crucial.%2520However%252C%2520environmental%2520challenges%250Asuch%2520as%2520smoke%252C%2520tunnels%252C%2520and%2520adverse%2520weather%2520can%2520significantly%2520degrade%2520the%250Aperformance%2520of%2520single-sensor%2520systems%2520like%2520LiDAR%2520or%2520GPS%252C%2520compromising%2520the%250Aoverall%2520stability%2520and%2520safety%2520of%2520autonomous%2520robots.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520AF-RLIO%253A%2520an%2520adaptive%2520fusion%2520approach%2520that%2520integrates%25204D%250Amillimeter-wave%2520radar%252C%2520LiDAR%252C%2520inertial%2520measurement%2520unit%2520%2528IMU%2529%252C%2520and%2520GPS%2520to%250Aleverage%2520the%2520complementary%2520strengths%2520of%2520these%2520sensors%2520for%2520robust%2520odometry%250Aestimation%2520in%2520complex%2520environments.%2520Our%2520method%2520consists%2520of%2520three%2520key%2520modules.%250AFirstly%252C%2520the%2520pre-processing%2520module%2520utilizes%2520radar%2520data%2520to%2520assist%2520LiDAR%2520in%250Aremoving%2520dynamic%2520points%2520and%2520determining%2520when%2520environmental%2520conditions%2520are%250Adegraded%2520for%2520LiDAR.%2520Secondly%252C%2520the%2520dynamic-aware%2520multimodal%2520odometry%2520selects%250Aappropriate%2520point%2520cloud%2520data%2520for%2520scan-to-map%2520matching%2520and%2520tightly%2520couples%2520it%250Awith%2520the%2520IMU%2520using%2520the%2520Iterative%2520Error%2520State%2520Kalman%2520Filter.%2520Lastly%252C%2520the%2520factor%250Agraph%2520optimization%2520module%2520balances%2520weights%2520between%2520odometry%2520and%2520GPS%2520data%252C%250Aconstructing%2520a%2520pose%2520graph%2520for%2520optimization.%2520The%2520proposed%2520approach%2520has%2520been%250Aevaluated%2520on%2520datasets%2520and%2520tested%2520in%2520real-world%2520robotic%2520environments%252C%250Ademonstrating%2520its%2520effectiveness%2520and%2520advantages%2520over%2520existing%2520methods%2520in%250Achallenging%2520conditions%2520such%2520as%2520smoke%2520and%2520tunnels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AF-RLIO%3A%20Adaptive%20Fusion%20of%20Radar-LiDAR-Inertial%20Information%20for%20Robust%0A%20%20Odometry%20in%20Challenging%20Environments&entry.906535625=Chenglong%20Qian%20and%20Yang%20Xu%20and%20Xiufang%20Shi%20and%20Jiming%20Chen%20and%20Liang%20Li&entry.1292438233=%20%20In%20robotic%20navigation%2C%20maintaining%20precise%20pose%20estimation%20and%20navigation%20in%0Acomplex%20and%20dynamic%20environments%20is%20crucial.%20However%2C%20environmental%20challenges%0Asuch%20as%20smoke%2C%20tunnels%2C%20and%20adverse%20weather%20can%20significantly%20degrade%20the%0Aperformance%20of%20single-sensor%20systems%20like%20LiDAR%20or%20GPS%2C%20compromising%20the%0Aoverall%20stability%20and%20safety%20of%20autonomous%20robots.%20To%20address%20these%20challenges%2C%0Awe%20propose%20AF-RLIO%3A%20an%20adaptive%20fusion%20approach%20that%20integrates%204D%0Amillimeter-wave%20radar%2C%20LiDAR%2C%20inertial%20measurement%20unit%20%28IMU%29%2C%20and%20GPS%20to%0Aleverage%20the%20complementary%20strengths%20of%20these%20sensors%20for%20robust%20odometry%0Aestimation%20in%20complex%20environments.%20Our%20method%20consists%20of%20three%20key%20modules.%0AFirstly%2C%20the%20pre-processing%20module%20utilizes%20radar%20data%20to%20assist%20LiDAR%20in%0Aremoving%20dynamic%20points%20and%20determining%20when%20environmental%20conditions%20are%0Adegraded%20for%20LiDAR.%20Secondly%2C%20the%20dynamic-aware%20multimodal%20odometry%20selects%0Aappropriate%20point%20cloud%20data%20for%20scan-to-map%20matching%20and%20tightly%20couples%20it%0Awith%20the%20IMU%20using%20the%20Iterative%20Error%20State%20Kalman%20Filter.%20Lastly%2C%20the%20factor%0Agraph%20optimization%20module%20balances%20weights%20between%20odometry%20and%20GPS%20data%2C%0Aconstructing%20a%20pose%20graph%20for%20optimization.%20The%20proposed%20approach%20has%20been%0Aevaluated%20on%20datasets%20and%20tested%20in%20real-world%20robotic%20environments%2C%0Ademonstrating%20its%20effectiveness%20and%20advantages%20over%20existing%20methods%20in%0Achallenging%20conditions%20such%20as%20smoke%20and%20tunnels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18317v1&entry.124074799=Read"},
{"title": "A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum\n  Detection in Giemsa-Stained Blood Smears", "author": "Frauke Wilm and Luis Carlos Rivera Monroy and Mathias \u00d6ttl and Lukas M\u00fcrdter and Leonid Mill and Andreas Maier", "abstract": "  Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is\nan essential component of reliable malaria diagnosis, especially in developing\ncountries. Deep learning-based object detection methods have demonstrated\nstrong potential for automated Malaria diagnosis, but their adoption is limited\nby the scarcity of datasets with detailed instance-level annotations. In this\nwork, we present an enhanced version of the publicly available NIH malaria\ndataset, with detailed bounding box annotations in COCO format to support\nobject detection training. We validated the revised annotations by training a\nFaster R-CNN model to detect infected and non-infected red blood cells, as well\nas white blood cells. Cross-validation on the original dataset yielded F1\nscores of up to 0.88 for infected cell detection. These results underscore the\nimportance of annotation volume and consistency, and demonstrate that automated\nannotation refinement combined with targeted manual correction can produce\ntraining data of sufficient quality for robust detection performance. The\nupdated annotations set is publicly available via GitHub:\nhttps://github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.\n", "link": "http://arxiv.org/abs/2507.18483v1", "date": "2025-07-24", "relevancy": 1.7023, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4374}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4179}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20COCO-Formatted%20Instance-Level%20Dataset%20for%20Plasmodium%20Falciparum%0A%20%20Detection%20in%20Giemsa-Stained%20Blood%20Smears&body=Title%3A%20A%20COCO-Formatted%20Instance-Level%20Dataset%20for%20Plasmodium%20Falciparum%0A%20%20Detection%20in%20Giemsa-Stained%20Blood%20Smears%0AAuthor%3A%20Frauke%20Wilm%20and%20Luis%20Carlos%20Rivera%20Monroy%20and%20Mathias%20%C3%96ttl%20and%20Lukas%20M%C3%BCrdter%20and%20Leonid%20Mill%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Accurate%20detection%20of%20Plasmodium%20falciparum%20in%20Giemsa-stained%20blood%20smears%20is%0Aan%20essential%20component%20of%20reliable%20malaria%20diagnosis%2C%20especially%20in%20developing%0Acountries.%20Deep%20learning-based%20object%20detection%20methods%20have%20demonstrated%0Astrong%20potential%20for%20automated%20Malaria%20diagnosis%2C%20but%20their%20adoption%20is%20limited%0Aby%20the%20scarcity%20of%20datasets%20with%20detailed%20instance-level%20annotations.%20In%20this%0Awork%2C%20we%20present%20an%20enhanced%20version%20of%20the%20publicly%20available%20NIH%20malaria%0Adataset%2C%20with%20detailed%20bounding%20box%20annotations%20in%20COCO%20format%20to%20support%0Aobject%20detection%20training.%20We%20validated%20the%20revised%20annotations%20by%20training%20a%0AFaster%20R-CNN%20model%20to%20detect%20infected%20and%20non-infected%20red%20blood%20cells%2C%20as%20well%0Aas%20white%20blood%20cells.%20Cross-validation%20on%20the%20original%20dataset%20yielded%20F1%0Ascores%20of%20up%20to%200.88%20for%20infected%20cell%20detection.%20These%20results%20underscore%20the%0Aimportance%20of%20annotation%20volume%20and%20consistency%2C%20and%20demonstrate%20that%20automated%0Aannotation%20refinement%20combined%20with%20targeted%20manual%20correction%20can%20produce%0Atraining%20data%20of%20sufficient%20quality%20for%20robust%20detection%20performance.%20The%0Aupdated%20annotations%20set%20is%20publicly%20available%20via%20GitHub%3A%0Ahttps%3A//github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520COCO-Formatted%2520Instance-Level%2520Dataset%2520for%2520Plasmodium%2520Falciparum%250A%2520%2520Detection%2520in%2520Giemsa-Stained%2520Blood%2520Smears%26entry.906535625%3DFrauke%2520Wilm%2520and%2520Luis%2520Carlos%2520Rivera%2520Monroy%2520and%2520Mathias%2520%25C3%2596ttl%2520and%2520Lukas%2520M%25C3%25BCrdter%2520and%2520Leonid%2520Mill%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Accurate%2520detection%2520of%2520Plasmodium%2520falciparum%2520in%2520Giemsa-stained%2520blood%2520smears%2520is%250Aan%2520essential%2520component%2520of%2520reliable%2520malaria%2520diagnosis%252C%2520especially%2520in%2520developing%250Acountries.%2520Deep%2520learning-based%2520object%2520detection%2520methods%2520have%2520demonstrated%250Astrong%2520potential%2520for%2520automated%2520Malaria%2520diagnosis%252C%2520but%2520their%2520adoption%2520is%2520limited%250Aby%2520the%2520scarcity%2520of%2520datasets%2520with%2520detailed%2520instance-level%2520annotations.%2520In%2520this%250Awork%252C%2520we%2520present%2520an%2520enhanced%2520version%2520of%2520the%2520publicly%2520available%2520NIH%2520malaria%250Adataset%252C%2520with%2520detailed%2520bounding%2520box%2520annotations%2520in%2520COCO%2520format%2520to%2520support%250Aobject%2520detection%2520training.%2520We%2520validated%2520the%2520revised%2520annotations%2520by%2520training%2520a%250AFaster%2520R-CNN%2520model%2520to%2520detect%2520infected%2520and%2520non-infected%2520red%2520blood%2520cells%252C%2520as%2520well%250Aas%2520white%2520blood%2520cells.%2520Cross-validation%2520on%2520the%2520original%2520dataset%2520yielded%2520F1%250Ascores%2520of%2520up%2520to%25200.88%2520for%2520infected%2520cell%2520detection.%2520These%2520results%2520underscore%2520the%250Aimportance%2520of%2520annotation%2520volume%2520and%2520consistency%252C%2520and%2520demonstrate%2520that%2520automated%250Aannotation%2520refinement%2520combined%2520with%2520targeted%2520manual%2520correction%2520can%2520produce%250Atraining%2520data%2520of%2520sufficient%2520quality%2520for%2520robust%2520detection%2520performance.%2520The%250Aupdated%2520annotations%2520set%2520is%2520publicly%2520available%2520via%2520GitHub%253A%250Ahttps%253A//github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20COCO-Formatted%20Instance-Level%20Dataset%20for%20Plasmodium%20Falciparum%0A%20%20Detection%20in%20Giemsa-Stained%20Blood%20Smears&entry.906535625=Frauke%20Wilm%20and%20Luis%20Carlos%20Rivera%20Monroy%20and%20Mathias%20%C3%96ttl%20and%20Lukas%20M%C3%BCrdter%20and%20Leonid%20Mill%20and%20Andreas%20Maier&entry.1292438233=%20%20Accurate%20detection%20of%20Plasmodium%20falciparum%20in%20Giemsa-stained%20blood%20smears%20is%0Aan%20essential%20component%20of%20reliable%20malaria%20diagnosis%2C%20especially%20in%20developing%0Acountries.%20Deep%20learning-based%20object%20detection%20methods%20have%20demonstrated%0Astrong%20potential%20for%20automated%20Malaria%20diagnosis%2C%20but%20their%20adoption%20is%20limited%0Aby%20the%20scarcity%20of%20datasets%20with%20detailed%20instance-level%20annotations.%20In%20this%0Awork%2C%20we%20present%20an%20enhanced%20version%20of%20the%20publicly%20available%20NIH%20malaria%0Adataset%2C%20with%20detailed%20bounding%20box%20annotations%20in%20COCO%20format%20to%20support%0Aobject%20detection%20training.%20We%20validated%20the%20revised%20annotations%20by%20training%20a%0AFaster%20R-CNN%20model%20to%20detect%20infected%20and%20non-infected%20red%20blood%20cells%2C%20as%20well%0Aas%20white%20blood%20cells.%20Cross-validation%20on%20the%20original%20dataset%20yielded%20F1%0Ascores%20of%20up%20to%200.88%20for%20infected%20cell%20detection.%20These%20results%20underscore%20the%0Aimportance%20of%20annotation%20volume%20and%20consistency%2C%20and%20demonstrate%20that%20automated%0Aannotation%20refinement%20combined%20with%20targeted%20manual%20correction%20can%20produce%0Atraining%20data%20of%20sufficient%20quality%20for%20robust%20detection%20performance.%20The%0Aupdated%20annotations%20set%20is%20publicly%20available%20via%20GitHub%3A%0Ahttps%3A//github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18483v1&entry.124074799=Read"},
{"title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs", "author": " Anshumann and Mohd Abbas Zaidi and Akhil Kedia and Jinwoo Ahn and Taehwak Kwon and Kangwook Lee and Haejun Lee and Joohyung Lee", "abstract": "  Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.\n", "link": "http://arxiv.org/abs/2503.16870v2", "date": "2025-07-24", "relevancy": 1.9061, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4811}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.475}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Logit%20Sampling%3A%20Accelerating%20Knowledge%20Distillation%20in%20LLMs&body=Title%3A%20Sparse%20Logit%20Sampling%3A%20Accelerating%20Knowledge%20Distillation%20in%20LLMs%0AAuthor%3A%20%20Anshumann%20and%20Mohd%20Abbas%20Zaidi%20and%20Akhil%20Kedia%20and%20Jinwoo%20Ahn%20and%20Taehwak%20Kwon%20and%20Kangwook%20Lee%20and%20Haejun%20Lee%20and%20Joohyung%20Lee%0AAbstract%3A%20%20%20Knowledge%20distillation%20can%20be%20a%20cost-effective%20technique%20to%20distill%20knowledge%0Ain%20Large%20Language%20Models%2C%20if%20the%20teacher%20output%20logits%20can%20be%20pre-computed%20and%0Acached.%20However%2C%20successfully%20applying%20this%20to%20pre-training%20remains%20largely%0Aunexplored.%20In%20this%20work%2C%20we%20prove%20that%20naive%20approaches%20for%20sparse%20knowledge%0Adistillation%20such%20as%20caching%20Top-K%20probabilities%2C%20while%20intuitive%2C%20provide%0Abiased%20estimates%20of%20teacher%20probability%20distribution%20to%20the%20student%2C%20resulting%0Ain%20suboptimal%20performance%20and%20calibration.%20We%20propose%20an%0Aimportance-sampling-based%20method%20%60Random%20Sampling%20Knowledge%20Distillation%27%2C%0Awhich%20provides%20unbiased%20estimates%2C%20preserves%20the%20gradient%20in%20expectation%2C%20and%0Arequires%20storing%20significantly%20sparser%20logits.%20Our%20method%20enables%20faster%0Atraining%20of%20student%20models%20with%20marginal%20overhead%20%28%3C10%25%29%20compared%20to%0Across-entropy%20based%20training%2C%20while%20maintaining%20competitive%20performance%0Acompared%20to%20full%20distillation%2C%20across%20a%20range%20of%20model%20sizes%20from%20300M%20to%203B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.16870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Logit%2520Sampling%253A%2520Accelerating%2520Knowledge%2520Distillation%2520in%2520LLMs%26entry.906535625%3D%2520Anshumann%2520and%2520Mohd%2520Abbas%2520Zaidi%2520and%2520Akhil%2520Kedia%2520and%2520Jinwoo%2520Ahn%2520and%2520Taehwak%2520Kwon%2520and%2520Kangwook%2520Lee%2520and%2520Haejun%2520Lee%2520and%2520Joohyung%2520Lee%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520can%2520be%2520a%2520cost-effective%2520technique%2520to%2520distill%2520knowledge%250Ain%2520Large%2520Language%2520Models%252C%2520if%2520the%2520teacher%2520output%2520logits%2520can%2520be%2520pre-computed%2520and%250Acached.%2520However%252C%2520successfully%2520applying%2520this%2520to%2520pre-training%2520remains%2520largely%250Aunexplored.%2520In%2520this%2520work%252C%2520we%2520prove%2520that%2520naive%2520approaches%2520for%2520sparse%2520knowledge%250Adistillation%2520such%2520as%2520caching%2520Top-K%2520probabilities%252C%2520while%2520intuitive%252C%2520provide%250Abiased%2520estimates%2520of%2520teacher%2520probability%2520distribution%2520to%2520the%2520student%252C%2520resulting%250Ain%2520suboptimal%2520performance%2520and%2520calibration.%2520We%2520propose%2520an%250Aimportance-sampling-based%2520method%2520%2560Random%2520Sampling%2520Knowledge%2520Distillation%2527%252C%250Awhich%2520provides%2520unbiased%2520estimates%252C%2520preserves%2520the%2520gradient%2520in%2520expectation%252C%2520and%250Arequires%2520storing%2520significantly%2520sparser%2520logits.%2520Our%2520method%2520enables%2520faster%250Atraining%2520of%2520student%2520models%2520with%2520marginal%2520overhead%2520%2528%253C10%2525%2529%2520compared%2520to%250Across-entropy%2520based%2520training%252C%2520while%2520maintaining%2520competitive%2520performance%250Acompared%2520to%2520full%2520distillation%252C%2520across%2520a%2520range%2520of%2520model%2520sizes%2520from%2520300M%2520to%25203B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Logit%20Sampling%3A%20Accelerating%20Knowledge%20Distillation%20in%20LLMs&entry.906535625=%20Anshumann%20and%20Mohd%20Abbas%20Zaidi%20and%20Akhil%20Kedia%20and%20Jinwoo%20Ahn%20and%20Taehwak%20Kwon%20and%20Kangwook%20Lee%20and%20Haejun%20Lee%20and%20Joohyung%20Lee&entry.1292438233=%20%20Knowledge%20distillation%20can%20be%20a%20cost-effective%20technique%20to%20distill%20knowledge%0Ain%20Large%20Language%20Models%2C%20if%20the%20teacher%20output%20logits%20can%20be%20pre-computed%20and%0Acached.%20However%2C%20successfully%20applying%20this%20to%20pre-training%20remains%20largely%0Aunexplored.%20In%20this%20work%2C%20we%20prove%20that%20naive%20approaches%20for%20sparse%20knowledge%0Adistillation%20such%20as%20caching%20Top-K%20probabilities%2C%20while%20intuitive%2C%20provide%0Abiased%20estimates%20of%20teacher%20probability%20distribution%20to%20the%20student%2C%20resulting%0Ain%20suboptimal%20performance%20and%20calibration.%20We%20propose%20an%0Aimportance-sampling-based%20method%20%60Random%20Sampling%20Knowledge%20Distillation%27%2C%0Awhich%20provides%20unbiased%20estimates%2C%20preserves%20the%20gradient%20in%20expectation%2C%20and%0Arequires%20storing%20significantly%20sparser%20logits.%20Our%20method%20enables%20faster%0Atraining%20of%20student%20models%20with%20marginal%20overhead%20%28%3C10%25%29%20compared%20to%0Across-entropy%20based%20training%2C%20while%20maintaining%20competitive%20performance%0Acompared%20to%20full%20distillation%2C%20across%20a%20range%20of%20model%20sizes%20from%20300M%20to%203B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.16870v2&entry.124074799=Read"},
{"title": "How do language models learn facts? Dynamics, curricula and\n  hallucinations", "author": "Nicolas Zucchet and J\u00f6rg Bornschein and Stephanie Chan and Andrew Lampinen and Razvan Pascanu and Soham De", "abstract": "  Large language models accumulate vast knowledge during pre-training, yet the\ndynamics governing this acquisition remain poorly understood. This work\ninvestigates the learning dynamics of language models on a synthetic factual\nrecall task, uncovering three key findings: First, language models learn in\nthree phases, exhibiting a performance plateau before acquiring precise factual\nknowledge. Mechanistically, this plateau coincides with the formation of\nattention-based circuits that support recall. Second, the training data\ndistribution significantly impacts learning dynamics, as imbalanced\ndistributions lead to shorter plateaus. Finally, hallucinations emerge\nsimultaneously with knowledge, and integrating new knowledge into the model\nthrough fine-tuning is challenging, as it quickly corrupts its existing\nparametric memories. Our results emphasize the importance of data distribution\nin knowledge acquisition and suggest novel data scheduling strategies to\naccelerate neural network training.\n", "link": "http://arxiv.org/abs/2503.21676v2", "date": "2025-07-24", "relevancy": 2.0004, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20do%20language%20models%20learn%20facts%3F%20Dynamics%2C%20curricula%20and%0A%20%20hallucinations&body=Title%3A%20How%20do%20language%20models%20learn%20facts%3F%20Dynamics%2C%20curricula%20and%0A%20%20hallucinations%0AAuthor%3A%20Nicolas%20Zucchet%20and%20J%C3%B6rg%20Bornschein%20and%20Stephanie%20Chan%20and%20Andrew%20Lampinen%20and%20Razvan%20Pascanu%20and%20Soham%20De%0AAbstract%3A%20%20%20Large%20language%20models%20accumulate%20vast%20knowledge%20during%20pre-training%2C%20yet%20the%0Adynamics%20governing%20this%20acquisition%20remain%20poorly%20understood.%20This%20work%0Ainvestigates%20the%20learning%20dynamics%20of%20language%20models%20on%20a%20synthetic%20factual%0Arecall%20task%2C%20uncovering%20three%20key%20findings%3A%20First%2C%20language%20models%20learn%20in%0Athree%20phases%2C%20exhibiting%20a%20performance%20plateau%20before%20acquiring%20precise%20factual%0Aknowledge.%20Mechanistically%2C%20this%20plateau%20coincides%20with%20the%20formation%20of%0Aattention-based%20circuits%20that%20support%20recall.%20Second%2C%20the%20training%20data%0Adistribution%20significantly%20impacts%20learning%20dynamics%2C%20as%20imbalanced%0Adistributions%20lead%20to%20shorter%20plateaus.%20Finally%2C%20hallucinations%20emerge%0Asimultaneously%20with%20knowledge%2C%20and%20integrating%20new%20knowledge%20into%20the%20model%0Athrough%20fine-tuning%20is%20challenging%2C%20as%20it%20quickly%20corrupts%20its%20existing%0Aparametric%20memories.%20Our%20results%20emphasize%20the%20importance%20of%20data%20distribution%0Ain%20knowledge%20acquisition%20and%20suggest%20novel%20data%20scheduling%20strategies%20to%0Aaccelerate%20neural%20network%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21676v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520do%2520language%2520models%2520learn%2520facts%253F%2520Dynamics%252C%2520curricula%2520and%250A%2520%2520hallucinations%26entry.906535625%3DNicolas%2520Zucchet%2520and%2520J%25C3%25B6rg%2520Bornschein%2520and%2520Stephanie%2520Chan%2520and%2520Andrew%2520Lampinen%2520and%2520Razvan%2520Pascanu%2520and%2520Soham%2520De%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520accumulate%2520vast%2520knowledge%2520during%2520pre-training%252C%2520yet%2520the%250Adynamics%2520governing%2520this%2520acquisition%2520remain%2520poorly%2520understood.%2520This%2520work%250Ainvestigates%2520the%2520learning%2520dynamics%2520of%2520language%2520models%2520on%2520a%2520synthetic%2520factual%250Arecall%2520task%252C%2520uncovering%2520three%2520key%2520findings%253A%2520First%252C%2520language%2520models%2520learn%2520in%250Athree%2520phases%252C%2520exhibiting%2520a%2520performance%2520plateau%2520before%2520acquiring%2520precise%2520factual%250Aknowledge.%2520Mechanistically%252C%2520this%2520plateau%2520coincides%2520with%2520the%2520formation%2520of%250Aattention-based%2520circuits%2520that%2520support%2520recall.%2520Second%252C%2520the%2520training%2520data%250Adistribution%2520significantly%2520impacts%2520learning%2520dynamics%252C%2520as%2520imbalanced%250Adistributions%2520lead%2520to%2520shorter%2520plateaus.%2520Finally%252C%2520hallucinations%2520emerge%250Asimultaneously%2520with%2520knowledge%252C%2520and%2520integrating%2520new%2520knowledge%2520into%2520the%2520model%250Athrough%2520fine-tuning%2520is%2520challenging%252C%2520as%2520it%2520quickly%2520corrupts%2520its%2520existing%250Aparametric%2520memories.%2520Our%2520results%2520emphasize%2520the%2520importance%2520of%2520data%2520distribution%250Ain%2520knowledge%2520acquisition%2520and%2520suggest%2520novel%2520data%2520scheduling%2520strategies%2520to%250Aaccelerate%2520neural%2520network%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21676v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20do%20language%20models%20learn%20facts%3F%20Dynamics%2C%20curricula%20and%0A%20%20hallucinations&entry.906535625=Nicolas%20Zucchet%20and%20J%C3%B6rg%20Bornschein%20and%20Stephanie%20Chan%20and%20Andrew%20Lampinen%20and%20Razvan%20Pascanu%20and%20Soham%20De&entry.1292438233=%20%20Large%20language%20models%20accumulate%20vast%20knowledge%20during%20pre-training%2C%20yet%20the%0Adynamics%20governing%20this%20acquisition%20remain%20poorly%20understood.%20This%20work%0Ainvestigates%20the%20learning%20dynamics%20of%20language%20models%20on%20a%20synthetic%20factual%0Arecall%20task%2C%20uncovering%20three%20key%20findings%3A%20First%2C%20language%20models%20learn%20in%0Athree%20phases%2C%20exhibiting%20a%20performance%20plateau%20before%20acquiring%20precise%20factual%0Aknowledge.%20Mechanistically%2C%20this%20plateau%20coincides%20with%20the%20formation%20of%0Aattention-based%20circuits%20that%20support%20recall.%20Second%2C%20the%20training%20data%0Adistribution%20significantly%20impacts%20learning%20dynamics%2C%20as%20imbalanced%0Adistributions%20lead%20to%20shorter%20plateaus.%20Finally%2C%20hallucinations%20emerge%0Asimultaneously%20with%20knowledge%2C%20and%20integrating%20new%20knowledge%20into%20the%20model%0Athrough%20fine-tuning%20is%20challenging%2C%20as%20it%20quickly%20corrupts%20its%20existing%0Aparametric%20memories.%20Our%20results%20emphasize%20the%20importance%20of%20data%20distribution%0Ain%20knowledge%20acquisition%20and%20suggest%20novel%20data%20scheduling%20strategies%20to%0Aaccelerate%20neural%20network%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21676v2&entry.124074799=Read"},
{"title": "C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation", "author": "Magnus Bengtsson and Kenneth \u00d6stberg", "abstract": "  We introduce C2G-KD, a data-free knowledge distillation framework where a\nclass-conditional generator is trained to produce synthetic samples guided by a\nfrozen teacher model and geometric constraints derived from PCA. The generator\nnever observes real training data but instead learns to activate the teacher's\noutput through a combination of semantic and structural losses. By constraining\ngenerated samples to lie within class-specific PCA subspaces estimated from as\nfew as two real examples per class, we preserve topological consistency and\ndiversity. Experiments on MNIST show that even minimal class structure is\nsufficient to bootstrap useful synthetic training pipelines.\n", "link": "http://arxiv.org/abs/2507.18533v1", "date": "2025-07-24", "relevancy": 2.0067, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5163}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5061}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C2G-KD%3A%20PCA-Constrained%20Generator%20for%20Data-Free%20Knowledge%20Distillation&body=Title%3A%20C2G-KD%3A%20PCA-Constrained%20Generator%20for%20Data-Free%20Knowledge%20Distillation%0AAuthor%3A%20Magnus%20Bengtsson%20and%20Kenneth%20%C3%96stberg%0AAbstract%3A%20%20%20We%20introduce%20C2G-KD%2C%20a%20data-free%20knowledge%20distillation%20framework%20where%20a%0Aclass-conditional%20generator%20is%20trained%20to%20produce%20synthetic%20samples%20guided%20by%20a%0Afrozen%20teacher%20model%20and%20geometric%20constraints%20derived%20from%20PCA.%20The%20generator%0Anever%20observes%20real%20training%20data%20but%20instead%20learns%20to%20activate%20the%20teacher%27s%0Aoutput%20through%20a%20combination%20of%20semantic%20and%20structural%20losses.%20By%20constraining%0Agenerated%20samples%20to%20lie%20within%20class-specific%20PCA%20subspaces%20estimated%20from%20as%0Afew%20as%20two%20real%20examples%20per%20class%2C%20we%20preserve%20topological%20consistency%20and%0Adiversity.%20Experiments%20on%20MNIST%20show%20that%20even%20minimal%20class%20structure%20is%0Asufficient%20to%20bootstrap%20useful%20synthetic%20training%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC2G-KD%253A%2520PCA-Constrained%2520Generator%2520for%2520Data-Free%2520Knowledge%2520Distillation%26entry.906535625%3DMagnus%2520Bengtsson%2520and%2520Kenneth%2520%25C3%2596stberg%26entry.1292438233%3D%2520%2520We%2520introduce%2520C2G-KD%252C%2520a%2520data-free%2520knowledge%2520distillation%2520framework%2520where%2520a%250Aclass-conditional%2520generator%2520is%2520trained%2520to%2520produce%2520synthetic%2520samples%2520guided%2520by%2520a%250Afrozen%2520teacher%2520model%2520and%2520geometric%2520constraints%2520derived%2520from%2520PCA.%2520The%2520generator%250Anever%2520observes%2520real%2520training%2520data%2520but%2520instead%2520learns%2520to%2520activate%2520the%2520teacher%2527s%250Aoutput%2520through%2520a%2520combination%2520of%2520semantic%2520and%2520structural%2520losses.%2520By%2520constraining%250Agenerated%2520samples%2520to%2520lie%2520within%2520class-specific%2520PCA%2520subspaces%2520estimated%2520from%2520as%250Afew%2520as%2520two%2520real%2520examples%2520per%2520class%252C%2520we%2520preserve%2520topological%2520consistency%2520and%250Adiversity.%2520Experiments%2520on%2520MNIST%2520show%2520that%2520even%2520minimal%2520class%2520structure%2520is%250Asufficient%2520to%2520bootstrap%2520useful%2520synthetic%2520training%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C2G-KD%3A%20PCA-Constrained%20Generator%20for%20Data-Free%20Knowledge%20Distillation&entry.906535625=Magnus%20Bengtsson%20and%20Kenneth%20%C3%96stberg&entry.1292438233=%20%20We%20introduce%20C2G-KD%2C%20a%20data-free%20knowledge%20distillation%20framework%20where%20a%0Aclass-conditional%20generator%20is%20trained%20to%20produce%20synthetic%20samples%20guided%20by%20a%0Afrozen%20teacher%20model%20and%20geometric%20constraints%20derived%20from%20PCA.%20The%20generator%0Anever%20observes%20real%20training%20data%20but%20instead%20learns%20to%20activate%20the%20teacher%27s%0Aoutput%20through%20a%20combination%20of%20semantic%20and%20structural%20losses.%20By%20constraining%0Agenerated%20samples%20to%20lie%20within%20class-specific%20PCA%20subspaces%20estimated%20from%20as%0Afew%20as%20two%20real%20examples%20per%20class%2C%20we%20preserve%20topological%20consistency%20and%0Adiversity.%20Experiments%20on%20MNIST%20show%20that%20even%20minimal%20class%20structure%20is%0Asufficient%20to%20bootstrap%20useful%20synthetic%20training%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18533v1&entry.124074799=Read"},
{"title": "Learning Concepts Definable in First-Order Logic with Counting", "author": "Steffen van Bergerem", "abstract": "  We study Boolean classification problems over relational background\nstructures in the logical framework introduced by Grohe and Tur\\'an (TOCS\n2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in\nfirst-order logic over structures of polylogarithmic degree can be learned in\nsublinear time, where the degree of the structure and the running time are\nmeasured in terms of the size of the structure. We generalise the results to\nthe first-order logic with counting FOCN, which was introduced by Kuske and\nSchweikardt (LICS 2017) as an expressive logic generalising various other\ncounting logics. Specifically, we prove that classifiers definable in FOCN over\nclasses of structures of polylogarithmic degree can be consistently learned in\nsublinear time. This can be seen as a first step towards extending the learning\nframework to include numerical aspects of machine learning. We extend the\nresult to agnostic probably approximately correct (PAC) learning for classes of\nstructures of degree at most $(\\log \\log n)^c$ for some constant $c$. Moreover,\nwe show that bounding the degree is crucial to obtain sublinear-time learning\nalgorithms. That is, we prove that, for structures of unbounded degree,\nlearning is not possible in sublinear time, even for classifiers definable in\nplain first-order logic.\n", "link": "http://arxiv.org/abs/1909.03820v5", "date": "2025-07-24", "relevancy": 1.7715, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4632}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4316}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Concepts%20Definable%20in%20First-Order%20Logic%20with%20Counting&body=Title%3A%20Learning%20Concepts%20Definable%20in%20First-Order%20Logic%20with%20Counting%0AAuthor%3A%20Steffen%20van%20Bergerem%0AAbstract%3A%20%20%20We%20study%20Boolean%20classification%20problems%20over%20relational%20background%0Astructures%20in%20the%20logical%20framework%20introduced%20by%20Grohe%20and%20Tur%5C%27an%20%28TOCS%0A2004%29.%20It%20is%20known%20%28Grohe%20and%20Ritzert%2C%20LICS%202017%29%20that%20classifiers%20definable%20in%0Afirst-order%20logic%20over%20structures%20of%20polylogarithmic%20degree%20can%20be%20learned%20in%0Asublinear%20time%2C%20where%20the%20degree%20of%20the%20structure%20and%20the%20running%20time%20are%0Ameasured%20in%20terms%20of%20the%20size%20of%20the%20structure.%20We%20generalise%20the%20results%20to%0Athe%20first-order%20logic%20with%20counting%20FOCN%2C%20which%20was%20introduced%20by%20Kuske%20and%0ASchweikardt%20%28LICS%202017%29%20as%20an%20expressive%20logic%20generalising%20various%20other%0Acounting%20logics.%20Specifically%2C%20we%20prove%20that%20classifiers%20definable%20in%20FOCN%20over%0Aclasses%20of%20structures%20of%20polylogarithmic%20degree%20can%20be%20consistently%20learned%20in%0Asublinear%20time.%20This%20can%20be%20seen%20as%20a%20first%20step%20towards%20extending%20the%20learning%0Aframework%20to%20include%20numerical%20aspects%20of%20machine%20learning.%20We%20extend%20the%0Aresult%20to%20agnostic%20probably%20approximately%20correct%20%28PAC%29%20learning%20for%20classes%20of%0Astructures%20of%20degree%20at%20most%20%24%28%5Clog%20%5Clog%20n%29%5Ec%24%20for%20some%20constant%20%24c%24.%20Moreover%2C%0Awe%20show%20that%20bounding%20the%20degree%20is%20crucial%20to%20obtain%20sublinear-time%20learning%0Aalgorithms.%20That%20is%2C%20we%20prove%20that%2C%20for%20structures%20of%20unbounded%20degree%2C%0Alearning%20is%20not%20possible%20in%20sublinear%20time%2C%20even%20for%20classifiers%20definable%20in%0Aplain%20first-order%20logic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1909.03820v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Concepts%2520Definable%2520in%2520First-Order%2520Logic%2520with%2520Counting%26entry.906535625%3DSteffen%2520van%2520Bergerem%26entry.1292438233%3D%2520%2520We%2520study%2520Boolean%2520classification%2520problems%2520over%2520relational%2520background%250Astructures%2520in%2520the%2520logical%2520framework%2520introduced%2520by%2520Grohe%2520and%2520Tur%255C%2527an%2520%2528TOCS%250A2004%2529.%2520It%2520is%2520known%2520%2528Grohe%2520and%2520Ritzert%252C%2520LICS%25202017%2529%2520that%2520classifiers%2520definable%2520in%250Afirst-order%2520logic%2520over%2520structures%2520of%2520polylogarithmic%2520degree%2520can%2520be%2520learned%2520in%250Asublinear%2520time%252C%2520where%2520the%2520degree%2520of%2520the%2520structure%2520and%2520the%2520running%2520time%2520are%250Ameasured%2520in%2520terms%2520of%2520the%2520size%2520of%2520the%2520structure.%2520We%2520generalise%2520the%2520results%2520to%250Athe%2520first-order%2520logic%2520with%2520counting%2520FOCN%252C%2520which%2520was%2520introduced%2520by%2520Kuske%2520and%250ASchweikardt%2520%2528LICS%25202017%2529%2520as%2520an%2520expressive%2520logic%2520generalising%2520various%2520other%250Acounting%2520logics.%2520Specifically%252C%2520we%2520prove%2520that%2520classifiers%2520definable%2520in%2520FOCN%2520over%250Aclasses%2520of%2520structures%2520of%2520polylogarithmic%2520degree%2520can%2520be%2520consistently%2520learned%2520in%250Asublinear%2520time.%2520This%2520can%2520be%2520seen%2520as%2520a%2520first%2520step%2520towards%2520extending%2520the%2520learning%250Aframework%2520to%2520include%2520numerical%2520aspects%2520of%2520machine%2520learning.%2520We%2520extend%2520the%250Aresult%2520to%2520agnostic%2520probably%2520approximately%2520correct%2520%2528PAC%2529%2520learning%2520for%2520classes%2520of%250Astructures%2520of%2520degree%2520at%2520most%2520%2524%2528%255Clog%2520%255Clog%2520n%2529%255Ec%2524%2520for%2520some%2520constant%2520%2524c%2524.%2520Moreover%252C%250Awe%2520show%2520that%2520bounding%2520the%2520degree%2520is%2520crucial%2520to%2520obtain%2520sublinear-time%2520learning%250Aalgorithms.%2520That%2520is%252C%2520we%2520prove%2520that%252C%2520for%2520structures%2520of%2520unbounded%2520degree%252C%250Alearning%2520is%2520not%2520possible%2520in%2520sublinear%2520time%252C%2520even%2520for%2520classifiers%2520definable%2520in%250Aplain%2520first-order%2520logic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1909.03820v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Concepts%20Definable%20in%20First-Order%20Logic%20with%20Counting&entry.906535625=Steffen%20van%20Bergerem&entry.1292438233=%20%20We%20study%20Boolean%20classification%20problems%20over%20relational%20background%0Astructures%20in%20the%20logical%20framework%20introduced%20by%20Grohe%20and%20Tur%5C%27an%20%28TOCS%0A2004%29.%20It%20is%20known%20%28Grohe%20and%20Ritzert%2C%20LICS%202017%29%20that%20classifiers%20definable%20in%0Afirst-order%20logic%20over%20structures%20of%20polylogarithmic%20degree%20can%20be%20learned%20in%0Asublinear%20time%2C%20where%20the%20degree%20of%20the%20structure%20and%20the%20running%20time%20are%0Ameasured%20in%20terms%20of%20the%20size%20of%20the%20structure.%20We%20generalise%20the%20results%20to%0Athe%20first-order%20logic%20with%20counting%20FOCN%2C%20which%20was%20introduced%20by%20Kuske%20and%0ASchweikardt%20%28LICS%202017%29%20as%20an%20expressive%20logic%20generalising%20various%20other%0Acounting%20logics.%20Specifically%2C%20we%20prove%20that%20classifiers%20definable%20in%20FOCN%20over%0Aclasses%20of%20structures%20of%20polylogarithmic%20degree%20can%20be%20consistently%20learned%20in%0Asublinear%20time.%20This%20can%20be%20seen%20as%20a%20first%20step%20towards%20extending%20the%20learning%0Aframework%20to%20include%20numerical%20aspects%20of%20machine%20learning.%20We%20extend%20the%0Aresult%20to%20agnostic%20probably%20approximately%20correct%20%28PAC%29%20learning%20for%20classes%20of%0Astructures%20of%20degree%20at%20most%20%24%28%5Clog%20%5Clog%20n%29%5Ec%24%20for%20some%20constant%20%24c%24.%20Moreover%2C%0Awe%20show%20that%20bounding%20the%20degree%20is%20crucial%20to%20obtain%20sublinear-time%20learning%0Aalgorithms.%20That%20is%2C%20we%20prove%20that%2C%20for%20structures%20of%20unbounded%20degree%2C%0Alearning%20is%20not%20possible%20in%20sublinear%20time%2C%20even%20for%20classifiers%20definable%20in%0Aplain%20first-order%20logic.%0A&entry.1838667208=http%3A//arxiv.org/abs/1909.03820v5&entry.124074799=Read"},
{"title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in\n  Public Goods Games", "author": "David Guzman Piedrahita and Yongjin Yang and Mrinmaya Sachan and Giorgia Ramponi and Bernhard Sch\u00f6lkopf and Zhijing Jin", "abstract": "  As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim\n", "link": "http://arxiv.org/abs/2506.23276v2", "date": "2025-07-24", "relevancy": 1.8411, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Corrupted%20by%20Reasoning%3A%20Reasoning%20Language%20Models%20Become%20Free-Riders%20in%0A%20%20Public%20Goods%20Games&body=Title%3A%20Corrupted%20by%20Reasoning%3A%20Reasoning%20Language%20Models%20Become%20Free-Riders%20in%0A%20%20Public%20Goods%20Games%0AAuthor%3A%20David%20Guzman%20Piedrahita%20and%20Yongjin%20Yang%20and%20Mrinmaya%20Sachan%20and%20Giorgia%20Ramponi%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Zhijing%20Jin%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20as%20autonomous%0Aagents%2C%20understanding%20their%20cooperation%20and%20social%20mechanisms%20is%20becoming%0Aincreasingly%20important.%20In%20particular%2C%20how%20LLMs%20balance%20self-interest%20and%0Acollective%20well-being%20is%20a%20critical%20challenge%20for%20ensuring%20alignment%2C%0Arobustness%2C%20and%20safe%20deployment.%20In%20this%20paper%2C%20we%20examine%20the%20challenge%20of%0Acostly%20sanctioning%20in%20multi-agent%20LLM%20systems%2C%20where%20an%20agent%20must%20decide%0Awhether%20to%20invest%20its%20own%20resources%20to%20incentivize%20cooperation%20or%20penalize%0Adefection.%20To%20study%20this%2C%20we%20adapt%20a%20public%20goods%20game%20with%20institutional%0Achoice%20from%20behavioral%20economics%2C%20allowing%20us%20to%20observe%20how%20different%20LLMs%0Anavigate%20social%20dilemmas%20over%20repeated%20interactions.%20Our%20analysis%20reveals%20four%0Adistinct%20behavioral%20patterns%20among%20models%3A%20some%20consistently%20establish%20and%0Asustain%20high%20levels%20of%20cooperation%2C%20others%20fluctuate%20between%20engagement%20and%0Adisengagement%2C%20some%20gradually%20decline%20in%20cooperative%20behavior%20over%20time%2C%20and%0Aothers%20rigidly%20follow%20fixed%20strategies%20regardless%20of%20outcomes.%20Surprisingly%2C%20we%0Afind%20that%20reasoning%20LLMs%2C%20such%20as%20the%20o1%20series%2C%20struggle%20significantly%20with%0Acooperation%2C%20whereas%20some%20traditional%20LLMs%20consistently%20achieve%20high%20levels%20of%0Acooperation.%20These%20findings%20suggest%20that%20the%20current%20approach%20to%20improving%0ALLMs%2C%20which%20focuses%20on%20enhancing%20their%20reasoning%20capabilities%2C%20does%20not%0Anecessarily%20lead%20to%20cooperation%2C%20providing%20valuable%20insights%20for%20deploying%20LLM%0Aagents%20in%20environments%20that%20require%20sustained%20collaboration.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/davidguzmanp/SanctSim%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23276v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrupted%2520by%2520Reasoning%253A%2520Reasoning%2520Language%2520Models%2520Become%2520Free-Riders%2520in%250A%2520%2520Public%2520Goods%2520Games%26entry.906535625%3DDavid%2520Guzman%2520Piedrahita%2520and%2520Yongjin%2520Yang%2520and%2520Mrinmaya%2520Sachan%2520and%2520Giorgia%2520Ramponi%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Zhijing%2520Jin%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520as%2520autonomous%250Aagents%252C%2520understanding%2520their%2520cooperation%2520and%2520social%2520mechanisms%2520is%2520becoming%250Aincreasingly%2520important.%2520In%2520particular%252C%2520how%2520LLMs%2520balance%2520self-interest%2520and%250Acollective%2520well-being%2520is%2520a%2520critical%2520challenge%2520for%2520ensuring%2520alignment%252C%250Arobustness%252C%2520and%2520safe%2520deployment.%2520In%2520this%2520paper%252C%2520we%2520examine%2520the%2520challenge%2520of%250Acostly%2520sanctioning%2520in%2520multi-agent%2520LLM%2520systems%252C%2520where%2520an%2520agent%2520must%2520decide%250Awhether%2520to%2520invest%2520its%2520own%2520resources%2520to%2520incentivize%2520cooperation%2520or%2520penalize%250Adefection.%2520To%2520study%2520this%252C%2520we%2520adapt%2520a%2520public%2520goods%2520game%2520with%2520institutional%250Achoice%2520from%2520behavioral%2520economics%252C%2520allowing%2520us%2520to%2520observe%2520how%2520different%2520LLMs%250Anavigate%2520social%2520dilemmas%2520over%2520repeated%2520interactions.%2520Our%2520analysis%2520reveals%2520four%250Adistinct%2520behavioral%2520patterns%2520among%2520models%253A%2520some%2520consistently%2520establish%2520and%250Asustain%2520high%2520levels%2520of%2520cooperation%252C%2520others%2520fluctuate%2520between%2520engagement%2520and%250Adisengagement%252C%2520some%2520gradually%2520decline%2520in%2520cooperative%2520behavior%2520over%2520time%252C%2520and%250Aothers%2520rigidly%2520follow%2520fixed%2520strategies%2520regardless%2520of%2520outcomes.%2520Surprisingly%252C%2520we%250Afind%2520that%2520reasoning%2520LLMs%252C%2520such%2520as%2520the%2520o1%2520series%252C%2520struggle%2520significantly%2520with%250Acooperation%252C%2520whereas%2520some%2520traditional%2520LLMs%2520consistently%2520achieve%2520high%2520levels%2520of%250Acooperation.%2520These%2520findings%2520suggest%2520that%2520the%2520current%2520approach%2520to%2520improving%250ALLMs%252C%2520which%2520focuses%2520on%2520enhancing%2520their%2520reasoning%2520capabilities%252C%2520does%2520not%250Anecessarily%2520lead%2520to%2520cooperation%252C%2520providing%2520valuable%2520insights%2520for%2520deploying%2520LLM%250Aagents%2520in%2520environments%2520that%2520require%2520sustained%2520collaboration.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/davidguzmanp/SanctSim%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23276v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Corrupted%20by%20Reasoning%3A%20Reasoning%20Language%20Models%20Become%20Free-Riders%20in%0A%20%20Public%20Goods%20Games&entry.906535625=David%20Guzman%20Piedrahita%20and%20Yongjin%20Yang%20and%20Mrinmaya%20Sachan%20and%20Giorgia%20Ramponi%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Zhijing%20Jin&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20as%20autonomous%0Aagents%2C%20understanding%20their%20cooperation%20and%20social%20mechanisms%20is%20becoming%0Aincreasingly%20important.%20In%20particular%2C%20how%20LLMs%20balance%20self-interest%20and%0Acollective%20well-being%20is%20a%20critical%20challenge%20for%20ensuring%20alignment%2C%0Arobustness%2C%20and%20safe%20deployment.%20In%20this%20paper%2C%20we%20examine%20the%20challenge%20of%0Acostly%20sanctioning%20in%20multi-agent%20LLM%20systems%2C%20where%20an%20agent%20must%20decide%0Awhether%20to%20invest%20its%20own%20resources%20to%20incentivize%20cooperation%20or%20penalize%0Adefection.%20To%20study%20this%2C%20we%20adapt%20a%20public%20goods%20game%20with%20institutional%0Achoice%20from%20behavioral%20economics%2C%20allowing%20us%20to%20observe%20how%20different%20LLMs%0Anavigate%20social%20dilemmas%20over%20repeated%20interactions.%20Our%20analysis%20reveals%20four%0Adistinct%20behavioral%20patterns%20among%20models%3A%20some%20consistently%20establish%20and%0Asustain%20high%20levels%20of%20cooperation%2C%20others%20fluctuate%20between%20engagement%20and%0Adisengagement%2C%20some%20gradually%20decline%20in%20cooperative%20behavior%20over%20time%2C%20and%0Aothers%20rigidly%20follow%20fixed%20strategies%20regardless%20of%20outcomes.%20Surprisingly%2C%20we%0Afind%20that%20reasoning%20LLMs%2C%20such%20as%20the%20o1%20series%2C%20struggle%20significantly%20with%0Acooperation%2C%20whereas%20some%20traditional%20LLMs%20consistently%20achieve%20high%20levels%20of%0Acooperation.%20These%20findings%20suggest%20that%20the%20current%20approach%20to%20improving%0ALLMs%2C%20which%20focuses%20on%20enhancing%20their%20reasoning%20capabilities%2C%20does%20not%0Anecessarily%20lead%20to%20cooperation%2C%20providing%20valuable%20insights%20for%20deploying%20LLM%0Aagents%20in%20environments%20that%20require%20sustained%20collaboration.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/davidguzmanp/SanctSim%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23276v2&entry.124074799=Read"},
{"title": "Foundations for Risk Assessment of AI in Protecting Fundamental Rights", "author": "Antonino Rotolo and Beatrice Ferrigno and Jose Miguel Angel Garcia Godinez and Claudio Novelli and Giovanni Sartor", "abstract": "  This chapter introduces a conceptual framework for qualitative risk\nassessment of AI, particularly in the context of the EU AI Act. The framework\naddresses the complexities of legal compliance and fundamental rights\nprotection by itegrating definitional balancing and defeasible reasoning.\nDefinitional balancing employs proportionality analysis to resolve conflicts\nbetween competing rights, while defeasible reasoning accommodates the dynamic\nnature of legal decision-making. Our approach stresses the need for an analysis\nof AI deployment scenarios and for identifying potential legal violations and\nmulti-layered impacts on fundamental rights. On the basis of this analysis, we\nprovide philosophical foundations for a logical account of AI risk analysis. In\nparticular, we consider the basic building blocks for conceptually grasping the\ninteraction between AI deployment scenarios and fundamental rights,\nincorporating in defeasible reasoning definitional balancing and arguments\nabout the contextual promotion or demotion of rights. This layered approach\nallows for more operative models of assessment of both high-risk AI systems and\nGeneral Purpose AI (GPAI) systems, emphasizing the broader applicability of the\nlatter. Future work aims to develop a formal model and effective algorithms to\nenhance AI risk assessment, bridging theoretical insights with practical\napplications to support responsible AI governance.\n", "link": "http://arxiv.org/abs/2507.18290v1", "date": "2025-07-24", "relevancy": 1.2504, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4419}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4107}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundations%20for%20Risk%20Assessment%20of%20AI%20in%20Protecting%20Fundamental%20Rights&body=Title%3A%20Foundations%20for%20Risk%20Assessment%20of%20AI%20in%20Protecting%20Fundamental%20Rights%0AAuthor%3A%20Antonino%20Rotolo%20and%20Beatrice%20Ferrigno%20and%20Jose%20Miguel%20Angel%20Garcia%20Godinez%20and%20Claudio%20Novelli%20and%20Giovanni%20Sartor%0AAbstract%3A%20%20%20This%20chapter%20introduces%20a%20conceptual%20framework%20for%20qualitative%20risk%0Aassessment%20of%20AI%2C%20particularly%20in%20the%20context%20of%20the%20EU%20AI%20Act.%20The%20framework%0Aaddresses%20the%20complexities%20of%20legal%20compliance%20and%20fundamental%20rights%0Aprotection%20by%20itegrating%20definitional%20balancing%20and%20defeasible%20reasoning.%0ADefinitional%20balancing%20employs%20proportionality%20analysis%20to%20resolve%20conflicts%0Abetween%20competing%20rights%2C%20while%20defeasible%20reasoning%20accommodates%20the%20dynamic%0Anature%20of%20legal%20decision-making.%20Our%20approach%20stresses%20the%20need%20for%20an%20analysis%0Aof%20AI%20deployment%20scenarios%20and%20for%20identifying%20potential%20legal%20violations%20and%0Amulti-layered%20impacts%20on%20fundamental%20rights.%20On%20the%20basis%20of%20this%20analysis%2C%20we%0Aprovide%20philosophical%20foundations%20for%20a%20logical%20account%20of%20AI%20risk%20analysis.%20In%0Aparticular%2C%20we%20consider%20the%20basic%20building%20blocks%20for%20conceptually%20grasping%20the%0Ainteraction%20between%20AI%20deployment%20scenarios%20and%20fundamental%20rights%2C%0Aincorporating%20in%20defeasible%20reasoning%20definitional%20balancing%20and%20arguments%0Aabout%20the%20contextual%20promotion%20or%20demotion%20of%20rights.%20This%20layered%20approach%0Aallows%20for%20more%20operative%20models%20of%20assessment%20of%20both%20high-risk%20AI%20systems%20and%0AGeneral%20Purpose%20AI%20%28GPAI%29%20systems%2C%20emphasizing%20the%20broader%20applicability%20of%20the%0Alatter.%20Future%20work%20aims%20to%20develop%20a%20formal%20model%20and%20effective%20algorithms%20to%0Aenhance%20AI%20risk%20assessment%2C%20bridging%20theoretical%20insights%20with%20practical%0Aapplications%20to%20support%20responsible%20AI%20governance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundations%2520for%2520Risk%2520Assessment%2520of%2520AI%2520in%2520Protecting%2520Fundamental%2520Rights%26entry.906535625%3DAntonino%2520Rotolo%2520and%2520Beatrice%2520Ferrigno%2520and%2520Jose%2520Miguel%2520Angel%2520Garcia%2520Godinez%2520and%2520Claudio%2520Novelli%2520and%2520Giovanni%2520Sartor%26entry.1292438233%3D%2520%2520This%2520chapter%2520introduces%2520a%2520conceptual%2520framework%2520for%2520qualitative%2520risk%250Aassessment%2520of%2520AI%252C%2520particularly%2520in%2520the%2520context%2520of%2520the%2520EU%2520AI%2520Act.%2520The%2520framework%250Aaddresses%2520the%2520complexities%2520of%2520legal%2520compliance%2520and%2520fundamental%2520rights%250Aprotection%2520by%2520itegrating%2520definitional%2520balancing%2520and%2520defeasible%2520reasoning.%250ADefinitional%2520balancing%2520employs%2520proportionality%2520analysis%2520to%2520resolve%2520conflicts%250Abetween%2520competing%2520rights%252C%2520while%2520defeasible%2520reasoning%2520accommodates%2520the%2520dynamic%250Anature%2520of%2520legal%2520decision-making.%2520Our%2520approach%2520stresses%2520the%2520need%2520for%2520an%2520analysis%250Aof%2520AI%2520deployment%2520scenarios%2520and%2520for%2520identifying%2520potential%2520legal%2520violations%2520and%250Amulti-layered%2520impacts%2520on%2520fundamental%2520rights.%2520On%2520the%2520basis%2520of%2520this%2520analysis%252C%2520we%250Aprovide%2520philosophical%2520foundations%2520for%2520a%2520logical%2520account%2520of%2520AI%2520risk%2520analysis.%2520In%250Aparticular%252C%2520we%2520consider%2520the%2520basic%2520building%2520blocks%2520for%2520conceptually%2520grasping%2520the%250Ainteraction%2520between%2520AI%2520deployment%2520scenarios%2520and%2520fundamental%2520rights%252C%250Aincorporating%2520in%2520defeasible%2520reasoning%2520definitional%2520balancing%2520and%2520arguments%250Aabout%2520the%2520contextual%2520promotion%2520or%2520demotion%2520of%2520rights.%2520This%2520layered%2520approach%250Aallows%2520for%2520more%2520operative%2520models%2520of%2520assessment%2520of%2520both%2520high-risk%2520AI%2520systems%2520and%250AGeneral%2520Purpose%2520AI%2520%2528GPAI%2529%2520systems%252C%2520emphasizing%2520the%2520broader%2520applicability%2520of%2520the%250Alatter.%2520Future%2520work%2520aims%2520to%2520develop%2520a%2520formal%2520model%2520and%2520effective%2520algorithms%2520to%250Aenhance%2520AI%2520risk%2520assessment%252C%2520bridging%2520theoretical%2520insights%2520with%2520practical%250Aapplications%2520to%2520support%2520responsible%2520AI%2520governance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundations%20for%20Risk%20Assessment%20of%20AI%20in%20Protecting%20Fundamental%20Rights&entry.906535625=Antonino%20Rotolo%20and%20Beatrice%20Ferrigno%20and%20Jose%20Miguel%20Angel%20Garcia%20Godinez%20and%20Claudio%20Novelli%20and%20Giovanni%20Sartor&entry.1292438233=%20%20This%20chapter%20introduces%20a%20conceptual%20framework%20for%20qualitative%20risk%0Aassessment%20of%20AI%2C%20particularly%20in%20the%20context%20of%20the%20EU%20AI%20Act.%20The%20framework%0Aaddresses%20the%20complexities%20of%20legal%20compliance%20and%20fundamental%20rights%0Aprotection%20by%20itegrating%20definitional%20balancing%20and%20defeasible%20reasoning.%0ADefinitional%20balancing%20employs%20proportionality%20analysis%20to%20resolve%20conflicts%0Abetween%20competing%20rights%2C%20while%20defeasible%20reasoning%20accommodates%20the%20dynamic%0Anature%20of%20legal%20decision-making.%20Our%20approach%20stresses%20the%20need%20for%20an%20analysis%0Aof%20AI%20deployment%20scenarios%20and%20for%20identifying%20potential%20legal%20violations%20and%0Amulti-layered%20impacts%20on%20fundamental%20rights.%20On%20the%20basis%20of%20this%20analysis%2C%20we%0Aprovide%20philosophical%20foundations%20for%20a%20logical%20account%20of%20AI%20risk%20analysis.%20In%0Aparticular%2C%20we%20consider%20the%20basic%20building%20blocks%20for%20conceptually%20grasping%20the%0Ainteraction%20between%20AI%20deployment%20scenarios%20and%20fundamental%20rights%2C%0Aincorporating%20in%20defeasible%20reasoning%20definitional%20balancing%20and%20arguments%0Aabout%20the%20contextual%20promotion%20or%20demotion%20of%20rights.%20This%20layered%20approach%0Aallows%20for%20more%20operative%20models%20of%20assessment%20of%20both%20high-risk%20AI%20systems%20and%0AGeneral%20Purpose%20AI%20%28GPAI%29%20systems%2C%20emphasizing%20the%20broader%20applicability%20of%20the%0Alatter.%20Future%20work%20aims%20to%20develop%20a%20formal%20model%20and%20effective%20algorithms%20to%0Aenhance%20AI%20risk%20assessment%2C%20bridging%20theoretical%20insights%20with%20practical%0Aapplications%20to%20support%20responsible%20AI%20governance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18290v1&entry.124074799=Read"},
{"title": "State of Health Estimation of Batteries Using a Time-Informed Dynamic\n  Sequence-Inverted Transformer", "author": "Janak M. Patel and Milad Ramezankhani and Anirudh Deodhar and Dagnachew Birru", "abstract": "  The rapid adoption of battery-powered vehicles and energy storage systems\nover the past decade has made battery health monitoring increasingly critical.\nBatteries play a central role in the efficiency and safety of these systems,\nyet they inevitably degrade over time due to repeated charge-discharge cycles.\nThis degradation leads to reduced energy efficiency and potential overheating,\nposing significant safety concerns. Accurate estimation of a State of Health\n(SoH) of battery is therefore essential for ensuring operational reliability\nand safety. Several machine learning architectures, such as LSTMs,\ntransformers, and encoder-based models, have been proposed to estimate SoH from\ndischarge cycle data. However, these models struggle with the irregularities\ninherent in real-world measurements: discharge readings are often recorded at\nnon-uniform intervals, and the lengths of discharge cycles vary significantly.\nTo address this, most existing approaches extract features from the sequences\nrather than processing them in full, which introduces information loss and\ncompromises accuracy. To overcome these challenges, we propose a novel\narchitecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).\nTIDSIT incorporates continuous time embeddings to effectively represent\nirregularly sampled data and utilizes padded sequences with temporal attention\nmechanisms to manage variable-length inputs without discarding sequence\ninformation. Experimental results on the NASA battery degradation dataset show\nthat TIDSIT significantly outperforms existing models, achieving over 50%\nreduction in prediction error and maintaining an SoH prediction error below\n0.58%. Furthermore, the architecture is generalizable and holds promise for\nbroader applications in health monitoring tasks involving irregular time-series\ndata.\n", "link": "http://arxiv.org/abs/2507.18320v1", "date": "2025-07-24", "relevancy": 0.964, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4961}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4838}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20State%20of%20Health%20Estimation%20of%20Batteries%20Using%20a%20Time-Informed%20Dynamic%0A%20%20Sequence-Inverted%20Transformer&body=Title%3A%20State%20of%20Health%20Estimation%20of%20Batteries%20Using%20a%20Time-Informed%20Dynamic%0A%20%20Sequence-Inverted%20Transformer%0AAuthor%3A%20Janak%20M.%20Patel%20and%20Milad%20Ramezankhani%20and%20Anirudh%20Deodhar%20and%20Dagnachew%20Birru%0AAbstract%3A%20%20%20The%20rapid%20adoption%20of%20battery-powered%20vehicles%20and%20energy%20storage%20systems%0Aover%20the%20past%20decade%20has%20made%20battery%20health%20monitoring%20increasingly%20critical.%0ABatteries%20play%20a%20central%20role%20in%20the%20efficiency%20and%20safety%20of%20these%20systems%2C%0Ayet%20they%20inevitably%20degrade%20over%20time%20due%20to%20repeated%20charge-discharge%20cycles.%0AThis%20degradation%20leads%20to%20reduced%20energy%20efficiency%20and%20potential%20overheating%2C%0Aposing%20significant%20safety%20concerns.%20Accurate%20estimation%20of%20a%20State%20of%20Health%0A%28SoH%29%20of%20battery%20is%20therefore%20essential%20for%20ensuring%20operational%20reliability%0Aand%20safety.%20Several%20machine%20learning%20architectures%2C%20such%20as%20LSTMs%2C%0Atransformers%2C%20and%20encoder-based%20models%2C%20have%20been%20proposed%20to%20estimate%20SoH%20from%0Adischarge%20cycle%20data.%20However%2C%20these%20models%20struggle%20with%20the%20irregularities%0Ainherent%20in%20real-world%20measurements%3A%20discharge%20readings%20are%20often%20recorded%20at%0Anon-uniform%20intervals%2C%20and%20the%20lengths%20of%20discharge%20cycles%20vary%20significantly.%0ATo%20address%20this%2C%20most%20existing%20approaches%20extract%20features%20from%20the%20sequences%0Arather%20than%20processing%20them%20in%20full%2C%20which%20introduces%20information%20loss%20and%0Acompromises%20accuracy.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%0Aarchitecture%3A%20Time-Informed%20Dynamic%20Sequence%20Inverted%20Transformer%20%28TIDSIT%29.%0ATIDSIT%20incorporates%20continuous%20time%20embeddings%20to%20effectively%20represent%0Airregularly%20sampled%20data%20and%20utilizes%20padded%20sequences%20with%20temporal%20attention%0Amechanisms%20to%20manage%20variable-length%20inputs%20without%20discarding%20sequence%0Ainformation.%20Experimental%20results%20on%20the%20NASA%20battery%20degradation%20dataset%20show%0Athat%20TIDSIT%20significantly%20outperforms%20existing%20models%2C%20achieving%20over%2050%25%0Areduction%20in%20prediction%20error%20and%20maintaining%20an%20SoH%20prediction%20error%20below%0A0.58%25.%20Furthermore%2C%20the%20architecture%20is%20generalizable%20and%20holds%20promise%20for%0Abroader%20applications%20in%20health%20monitoring%20tasks%20involving%20irregular%20time-series%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DState%2520of%2520Health%2520Estimation%2520of%2520Batteries%2520Using%2520a%2520Time-Informed%2520Dynamic%250A%2520%2520Sequence-Inverted%2520Transformer%26entry.906535625%3DJanak%2520M.%2520Patel%2520and%2520Milad%2520Ramezankhani%2520and%2520Anirudh%2520Deodhar%2520and%2520Dagnachew%2520Birru%26entry.1292438233%3D%2520%2520The%2520rapid%2520adoption%2520of%2520battery-powered%2520vehicles%2520and%2520energy%2520storage%2520systems%250Aover%2520the%2520past%2520decade%2520has%2520made%2520battery%2520health%2520monitoring%2520increasingly%2520critical.%250ABatteries%2520play%2520a%2520central%2520role%2520in%2520the%2520efficiency%2520and%2520safety%2520of%2520these%2520systems%252C%250Ayet%2520they%2520inevitably%2520degrade%2520over%2520time%2520due%2520to%2520repeated%2520charge-discharge%2520cycles.%250AThis%2520degradation%2520leads%2520to%2520reduced%2520energy%2520efficiency%2520and%2520potential%2520overheating%252C%250Aposing%2520significant%2520safety%2520concerns.%2520Accurate%2520estimation%2520of%2520a%2520State%2520of%2520Health%250A%2528SoH%2529%2520of%2520battery%2520is%2520therefore%2520essential%2520for%2520ensuring%2520operational%2520reliability%250Aand%2520safety.%2520Several%2520machine%2520learning%2520architectures%252C%2520such%2520as%2520LSTMs%252C%250Atransformers%252C%2520and%2520encoder-based%2520models%252C%2520have%2520been%2520proposed%2520to%2520estimate%2520SoH%2520from%250Adischarge%2520cycle%2520data.%2520However%252C%2520these%2520models%2520struggle%2520with%2520the%2520irregularities%250Ainherent%2520in%2520real-world%2520measurements%253A%2520discharge%2520readings%2520are%2520often%2520recorded%2520at%250Anon-uniform%2520intervals%252C%2520and%2520the%2520lengths%2520of%2520discharge%2520cycles%2520vary%2520significantly.%250ATo%2520address%2520this%252C%2520most%2520existing%2520approaches%2520extract%2520features%2520from%2520the%2520sequences%250Arather%2520than%2520processing%2520them%2520in%2520full%252C%2520which%2520introduces%2520information%2520loss%2520and%250Acompromises%2520accuracy.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%250Aarchitecture%253A%2520Time-Informed%2520Dynamic%2520Sequence%2520Inverted%2520Transformer%2520%2528TIDSIT%2529.%250ATIDSIT%2520incorporates%2520continuous%2520time%2520embeddings%2520to%2520effectively%2520represent%250Airregularly%2520sampled%2520data%2520and%2520utilizes%2520padded%2520sequences%2520with%2520temporal%2520attention%250Amechanisms%2520to%2520manage%2520variable-length%2520inputs%2520without%2520discarding%2520sequence%250Ainformation.%2520Experimental%2520results%2520on%2520the%2520NASA%2520battery%2520degradation%2520dataset%2520show%250Athat%2520TIDSIT%2520significantly%2520outperforms%2520existing%2520models%252C%2520achieving%2520over%252050%2525%250Areduction%2520in%2520prediction%2520error%2520and%2520maintaining%2520an%2520SoH%2520prediction%2520error%2520below%250A0.58%2525.%2520Furthermore%252C%2520the%2520architecture%2520is%2520generalizable%2520and%2520holds%2520promise%2520for%250Abroader%2520applications%2520in%2520health%2520monitoring%2520tasks%2520involving%2520irregular%2520time-series%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=State%20of%20Health%20Estimation%20of%20Batteries%20Using%20a%20Time-Informed%20Dynamic%0A%20%20Sequence-Inverted%20Transformer&entry.906535625=Janak%20M.%20Patel%20and%20Milad%20Ramezankhani%20and%20Anirudh%20Deodhar%20and%20Dagnachew%20Birru&entry.1292438233=%20%20The%20rapid%20adoption%20of%20battery-powered%20vehicles%20and%20energy%20storage%20systems%0Aover%20the%20past%20decade%20has%20made%20battery%20health%20monitoring%20increasingly%20critical.%0ABatteries%20play%20a%20central%20role%20in%20the%20efficiency%20and%20safety%20of%20these%20systems%2C%0Ayet%20they%20inevitably%20degrade%20over%20time%20due%20to%20repeated%20charge-discharge%20cycles.%0AThis%20degradation%20leads%20to%20reduced%20energy%20efficiency%20and%20potential%20overheating%2C%0Aposing%20significant%20safety%20concerns.%20Accurate%20estimation%20of%20a%20State%20of%20Health%0A%28SoH%29%20of%20battery%20is%20therefore%20essential%20for%20ensuring%20operational%20reliability%0Aand%20safety.%20Several%20machine%20learning%20architectures%2C%20such%20as%20LSTMs%2C%0Atransformers%2C%20and%20encoder-based%20models%2C%20have%20been%20proposed%20to%20estimate%20SoH%20from%0Adischarge%20cycle%20data.%20However%2C%20these%20models%20struggle%20with%20the%20irregularities%0Ainherent%20in%20real-world%20measurements%3A%20discharge%20readings%20are%20often%20recorded%20at%0Anon-uniform%20intervals%2C%20and%20the%20lengths%20of%20discharge%20cycles%20vary%20significantly.%0ATo%20address%20this%2C%20most%20existing%20approaches%20extract%20features%20from%20the%20sequences%0Arather%20than%20processing%20them%20in%20full%2C%20which%20introduces%20information%20loss%20and%0Acompromises%20accuracy.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%0Aarchitecture%3A%20Time-Informed%20Dynamic%20Sequence%20Inverted%20Transformer%20%28TIDSIT%29.%0ATIDSIT%20incorporates%20continuous%20time%20embeddings%20to%20effectively%20represent%0Airregularly%20sampled%20data%20and%20utilizes%20padded%20sequences%20with%20temporal%20attention%0Amechanisms%20to%20manage%20variable-length%20inputs%20without%20discarding%20sequence%0Ainformation.%20Experimental%20results%20on%20the%20NASA%20battery%20degradation%20dataset%20show%0Athat%20TIDSIT%20significantly%20outperforms%20existing%20models%2C%20achieving%20over%2050%25%0Areduction%20in%20prediction%20error%20and%20maintaining%20an%20SoH%20prediction%20error%20below%0A0.58%25.%20Furthermore%2C%20the%20architecture%20is%20generalizable%20and%20holds%20promise%20for%0Abroader%20applications%20in%20health%20monitoring%20tasks%20involving%20irregular%20time-series%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18320v1&entry.124074799=Read"},
{"title": "Low-rank adaptive physics-informed HyperDeepONets for solving\n  differential equations", "author": "Etienne Zeudong and Elsa Cardoso-Bihlo and Alex Bihlo", "abstract": "  HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an\nalternative architecture for operator learning, in which a hypernetwork\ngenerates the weights for the trunk net of a DeepONet. While this improves\nexpressivity, it incurs high memory and computational costs due to the large\nnumber of output parameters required. In this work we introduce, in the\nphysics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,\nwhich leverage low-rank adaptation (LoRA) to reduce complexity by decomposing\nthe hypernetwork's output layer weight matrix into two smaller low-rank\nmatrices. This reduces the number of trainable parameters while introducing an\nextra regularization of the trunk networks' weights. Through extensive\nexperiments on both ordinary and partial differential equations we show that\nPI-LoRA-HyperDeepONets achieve up to 70\\% reduction in parameters and\nconsistently outperform regular HyperDeepONets in terms of predictive accuracy\nand generalization.\n", "link": "http://arxiv.org/abs/2507.18346v1", "date": "2025-07-24", "relevancy": 1.8199, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4711}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4595}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-rank%20adaptive%20physics-informed%20HyperDeepONets%20for%20solving%0A%20%20differential%20equations&body=Title%3A%20Low-rank%20adaptive%20physics-informed%20HyperDeepONets%20for%20solving%0A%20%20differential%20equations%0AAuthor%3A%20Etienne%20Zeudong%20and%20Elsa%20Cardoso-Bihlo%20and%20Alex%20Bihlo%0AAbstract%3A%20%20%20HyperDeepONets%20were%20introduced%20in%20Lee%2C%20Cho%20and%20Hwang%20%5BICLR%2C%202023%5D%20as%20an%0Aalternative%20architecture%20for%20operator%20learning%2C%20in%20which%20a%20hypernetwork%0Agenerates%20the%20weights%20for%20the%20trunk%20net%20of%20a%20DeepONet.%20While%20this%20improves%0Aexpressivity%2C%20it%20incurs%20high%20memory%20and%20computational%20costs%20due%20to%20the%20large%0Anumber%20of%20output%20parameters%20required.%20In%20this%20work%20we%20introduce%2C%20in%20the%0Aphysics-informed%20machine%20learning%20setting%2C%20a%20variation%2C%20PI-LoRA-HyperDeepONets%2C%0Awhich%20leverage%20low-rank%20adaptation%20%28LoRA%29%20to%20reduce%20complexity%20by%20decomposing%0Athe%20hypernetwork%27s%20output%20layer%20weight%20matrix%20into%20two%20smaller%20low-rank%0Amatrices.%20This%20reduces%20the%20number%20of%20trainable%20parameters%20while%20introducing%20an%0Aextra%20regularization%20of%20the%20trunk%20networks%27%20weights.%20Through%20extensive%0Aexperiments%20on%20both%20ordinary%20and%20partial%20differential%20equations%20we%20show%20that%0API-LoRA-HyperDeepONets%20achieve%20up%20to%2070%5C%25%20reduction%20in%20parameters%20and%0Aconsistently%20outperform%20regular%20HyperDeepONets%20in%20terms%20of%20predictive%20accuracy%0Aand%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-rank%2520adaptive%2520physics-informed%2520HyperDeepONets%2520for%2520solving%250A%2520%2520differential%2520equations%26entry.906535625%3DEtienne%2520Zeudong%2520and%2520Elsa%2520Cardoso-Bihlo%2520and%2520Alex%2520Bihlo%26entry.1292438233%3D%2520%2520HyperDeepONets%2520were%2520introduced%2520in%2520Lee%252C%2520Cho%2520and%2520Hwang%2520%255BICLR%252C%25202023%255D%2520as%2520an%250Aalternative%2520architecture%2520for%2520operator%2520learning%252C%2520in%2520which%2520a%2520hypernetwork%250Agenerates%2520the%2520weights%2520for%2520the%2520trunk%2520net%2520of%2520a%2520DeepONet.%2520While%2520this%2520improves%250Aexpressivity%252C%2520it%2520incurs%2520high%2520memory%2520and%2520computational%2520costs%2520due%2520to%2520the%2520large%250Anumber%2520of%2520output%2520parameters%2520required.%2520In%2520this%2520work%2520we%2520introduce%252C%2520in%2520the%250Aphysics-informed%2520machine%2520learning%2520setting%252C%2520a%2520variation%252C%2520PI-LoRA-HyperDeepONets%252C%250Awhich%2520leverage%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520to%2520reduce%2520complexity%2520by%2520decomposing%250Athe%2520hypernetwork%2527s%2520output%2520layer%2520weight%2520matrix%2520into%2520two%2520smaller%2520low-rank%250Amatrices.%2520This%2520reduces%2520the%2520number%2520of%2520trainable%2520parameters%2520while%2520introducing%2520an%250Aextra%2520regularization%2520of%2520the%2520trunk%2520networks%2527%2520weights.%2520Through%2520extensive%250Aexperiments%2520on%2520both%2520ordinary%2520and%2520partial%2520differential%2520equations%2520we%2520show%2520that%250API-LoRA-HyperDeepONets%2520achieve%2520up%2520to%252070%255C%2525%2520reduction%2520in%2520parameters%2520and%250Aconsistently%2520outperform%2520regular%2520HyperDeepONets%2520in%2520terms%2520of%2520predictive%2520accuracy%250Aand%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-rank%20adaptive%20physics-informed%20HyperDeepONets%20for%20solving%0A%20%20differential%20equations&entry.906535625=Etienne%20Zeudong%20and%20Elsa%20Cardoso-Bihlo%20and%20Alex%20Bihlo&entry.1292438233=%20%20HyperDeepONets%20were%20introduced%20in%20Lee%2C%20Cho%20and%20Hwang%20%5BICLR%2C%202023%5D%20as%20an%0Aalternative%20architecture%20for%20operator%20learning%2C%20in%20which%20a%20hypernetwork%0Agenerates%20the%20weights%20for%20the%20trunk%20net%20of%20a%20DeepONet.%20While%20this%20improves%0Aexpressivity%2C%20it%20incurs%20high%20memory%20and%20computational%20costs%20due%20to%20the%20large%0Anumber%20of%20output%20parameters%20required.%20In%20this%20work%20we%20introduce%2C%20in%20the%0Aphysics-informed%20machine%20learning%20setting%2C%20a%20variation%2C%20PI-LoRA-HyperDeepONets%2C%0Awhich%20leverage%20low-rank%20adaptation%20%28LoRA%29%20to%20reduce%20complexity%20by%20decomposing%0Athe%20hypernetwork%27s%20output%20layer%20weight%20matrix%20into%20two%20smaller%20low-rank%0Amatrices.%20This%20reduces%20the%20number%20of%20trainable%20parameters%20while%20introducing%20an%0Aextra%20regularization%20of%20the%20trunk%20networks%27%20weights.%20Through%20extensive%0Aexperiments%20on%20both%20ordinary%20and%20partial%20differential%20equations%20we%20show%20that%0API-LoRA-HyperDeepONets%20achieve%20up%20to%2070%5C%25%20reduction%20in%20parameters%20and%0Aconsistently%20outperform%20regular%20HyperDeepONets%20in%20terms%20of%20predictive%20accuracy%0Aand%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18346v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


