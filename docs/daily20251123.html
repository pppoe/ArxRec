<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251120.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing", "author": "Eddie Pokming Sheung and Qihao Liu and Wufei Ma and Prakhar Kaushik and Jianwen Xie and Alan Yuille", "abstract": "With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.", "link": "http://arxiv.org/abs/2511.16662v1", "date": "2025-11-20", "relevancy": 3.3962, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7152}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6612}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TriDiff-4D%3A%20Fast%204D%20Generation%20through%20Diffusion-based%20Triplane%20Re-posing&body=Title%3A%20TriDiff-4D%3A%20Fast%204D%20Generation%20through%20Diffusion-based%20Triplane%20Re-posing%0AAuthor%3A%20Eddie%20Pokming%20Sheung%20and%20Qihao%20Liu%20and%20Wufei%20Ma%20and%20Prakhar%20Kaushik%20and%20Jianwen%20Xie%20and%20Alan%20Yuille%0AAbstract%3A%20With%20the%20increasing%20demand%20for%203D%20animation%2C%20generating%20high-fidelity%2C%20controllable%204D%20avatars%20from%20textual%20descriptions%20remains%20a%20significant%20challenge.%20Despite%20notable%20efforts%20in%204D%20generative%20modeling%2C%20existing%20methods%20exhibit%20fundamental%20limitations%20that%20impede%20their%20broader%20applicability%2C%20including%20temporal%20and%20geometric%20inconsistencies%2C%20perceptual%20artifacts%2C%20motion%20irregularities%2C%20high%20computational%20costs%2C%20and%20limited%20control%20over%20dynamics.%20To%20address%20these%20challenges%2C%20we%20propose%20TriDiff-4D%2C%20a%20novel%204D%20generative%20pipeline%20that%20employs%20diffusion-based%20triplane%20re-posing%20to%20produce%20high-quality%2C%20temporally%20coherent%204D%20avatars.%20Our%20model%20adopts%20an%20auto-regressive%20strategy%20to%20generate%204D%20sequences%20of%20arbitrary%20length%2C%20synthesizing%20each%203D%20frame%20with%20a%20single%20diffusion%20process.%20By%20explicitly%20learning%203D%20structure%20and%20motion%20priors%20from%20large-scale%203D%20and%20motion%20datasets%2C%20TriDiff-4D%20enables%20skeleton-driven%204D%20generation%20that%20excels%20in%20temporal%20consistency%2C%20motion%20accuracy%2C%20computational%20efficiency%2C%20and%20visual%20fidelity.%20Specifically%2C%20TriDiff-4D%20first%20generates%20a%20canonical%203D%20avatar%20and%20a%20corresponding%20motion%20sequence%20from%20a%20text%20prompt%2C%20then%20uses%20a%20second%20diffusion%20model%20to%20animate%20the%20avatar%20according%20to%20the%20motion%20sequence%2C%20supporting%20arbitrarily%20long%204D%20generation.%20Experimental%20results%20demonstrate%20that%20TriDiff-4D%20significantly%20outperforms%20existing%20methods%2C%20reducing%20generation%20time%20from%20hours%20to%20seconds%20by%20eliminating%20the%20optimization%20process%2C%20while%20substantially%20improving%20the%20generation%20of%20complex%20motions%20with%20high-fidelity%20appearance%20and%20accurate%203D%20geometry.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriDiff-4D%253A%2520Fast%25204D%2520Generation%2520through%2520Diffusion-based%2520Triplane%2520Re-posing%26entry.906535625%3DEddie%2520Pokming%2520Sheung%2520and%2520Qihao%2520Liu%2520and%2520Wufei%2520Ma%2520and%2520Prakhar%2520Kaushik%2520and%2520Jianwen%2520Xie%2520and%2520Alan%2520Yuille%26entry.1292438233%3DWith%2520the%2520increasing%2520demand%2520for%25203D%2520animation%252C%2520generating%2520high-fidelity%252C%2520controllable%25204D%2520avatars%2520from%2520textual%2520descriptions%2520remains%2520a%2520significant%2520challenge.%2520Despite%2520notable%2520efforts%2520in%25204D%2520generative%2520modeling%252C%2520existing%2520methods%2520exhibit%2520fundamental%2520limitations%2520that%2520impede%2520their%2520broader%2520applicability%252C%2520including%2520temporal%2520and%2520geometric%2520inconsistencies%252C%2520perceptual%2520artifacts%252C%2520motion%2520irregularities%252C%2520high%2520computational%2520costs%252C%2520and%2520limited%2520control%2520over%2520dynamics.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520TriDiff-4D%252C%2520a%2520novel%25204D%2520generative%2520pipeline%2520that%2520employs%2520diffusion-based%2520triplane%2520re-posing%2520to%2520produce%2520high-quality%252C%2520temporally%2520coherent%25204D%2520avatars.%2520Our%2520model%2520adopts%2520an%2520auto-regressive%2520strategy%2520to%2520generate%25204D%2520sequences%2520of%2520arbitrary%2520length%252C%2520synthesizing%2520each%25203D%2520frame%2520with%2520a%2520single%2520diffusion%2520process.%2520By%2520explicitly%2520learning%25203D%2520structure%2520and%2520motion%2520priors%2520from%2520large-scale%25203D%2520and%2520motion%2520datasets%252C%2520TriDiff-4D%2520enables%2520skeleton-driven%25204D%2520generation%2520that%2520excels%2520in%2520temporal%2520consistency%252C%2520motion%2520accuracy%252C%2520computational%2520efficiency%252C%2520and%2520visual%2520fidelity.%2520Specifically%252C%2520TriDiff-4D%2520first%2520generates%2520a%2520canonical%25203D%2520avatar%2520and%2520a%2520corresponding%2520motion%2520sequence%2520from%2520a%2520text%2520prompt%252C%2520then%2520uses%2520a%2520second%2520diffusion%2520model%2520to%2520animate%2520the%2520avatar%2520according%2520to%2520the%2520motion%2520sequence%252C%2520supporting%2520arbitrarily%2520long%25204D%2520generation.%2520Experimental%2520results%2520demonstrate%2520that%2520TriDiff-4D%2520significantly%2520outperforms%2520existing%2520methods%252C%2520reducing%2520generation%2520time%2520from%2520hours%2520to%2520seconds%2520by%2520eliminating%2520the%2520optimization%2520process%252C%2520while%2520substantially%2520improving%2520the%2520generation%2520of%2520complex%2520motions%2520with%2520high-fidelity%2520appearance%2520and%2520accurate%25203D%2520geometry.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TriDiff-4D%3A%20Fast%204D%20Generation%20through%20Diffusion-based%20Triplane%20Re-posing&entry.906535625=Eddie%20Pokming%20Sheung%20and%20Qihao%20Liu%20and%20Wufei%20Ma%20and%20Prakhar%20Kaushik%20and%20Jianwen%20Xie%20and%20Alan%20Yuille&entry.1292438233=With%20the%20increasing%20demand%20for%203D%20animation%2C%20generating%20high-fidelity%2C%20controllable%204D%20avatars%20from%20textual%20descriptions%20remains%20a%20significant%20challenge.%20Despite%20notable%20efforts%20in%204D%20generative%20modeling%2C%20existing%20methods%20exhibit%20fundamental%20limitations%20that%20impede%20their%20broader%20applicability%2C%20including%20temporal%20and%20geometric%20inconsistencies%2C%20perceptual%20artifacts%2C%20motion%20irregularities%2C%20high%20computational%20costs%2C%20and%20limited%20control%20over%20dynamics.%20To%20address%20these%20challenges%2C%20we%20propose%20TriDiff-4D%2C%20a%20novel%204D%20generative%20pipeline%20that%20employs%20diffusion-based%20triplane%20re-posing%20to%20produce%20high-quality%2C%20temporally%20coherent%204D%20avatars.%20Our%20model%20adopts%20an%20auto-regressive%20strategy%20to%20generate%204D%20sequences%20of%20arbitrary%20length%2C%20synthesizing%20each%203D%20frame%20with%20a%20single%20diffusion%20process.%20By%20explicitly%20learning%203D%20structure%20and%20motion%20priors%20from%20large-scale%203D%20and%20motion%20datasets%2C%20TriDiff-4D%20enables%20skeleton-driven%204D%20generation%20that%20excels%20in%20temporal%20consistency%2C%20motion%20accuracy%2C%20computational%20efficiency%2C%20and%20visual%20fidelity.%20Specifically%2C%20TriDiff-4D%20first%20generates%20a%20canonical%203D%20avatar%20and%20a%20corresponding%20motion%20sequence%20from%20a%20text%20prompt%2C%20then%20uses%20a%20second%20diffusion%20model%20to%20animate%20the%20avatar%20according%20to%20the%20motion%20sequence%2C%20supporting%20arbitrarily%20long%204D%20generation.%20Experimental%20results%20demonstrate%20that%20TriDiff-4D%20significantly%20outperforms%20existing%20methods%2C%20reducing%20generation%20time%20from%20hours%20to%20seconds%20by%20eliminating%20the%20optimization%20process%2C%20while%20substantially%20improving%20the%20generation%20of%20complex%20motions%20with%20high-fidelity%20appearance%20and%20accurate%203D%20geometry.&entry.1838667208=http%3A//arxiv.org/abs/2511.16662v1&entry.124074799=Read"},
{"title": "SAM 3D: 3Dfy Anything in Images", "author": " SAM 3D Team and Xingyu Chen and Fu-Jen Chu and Pierre Gleize and Kevin J Liang and Alexander Sax and Hao Tang and Weiyao Wang and Michelle Guo and Thibaut Hardin and Xiang Li and Aohan Lin and Jiawei Liu and Ziqi Ma and Anushka Sagar and Bowen Song and Xiaodong Wang and Jianing Yang and Bowen Zhang and Piotr Doll\u00e1r and Georgia Gkioxari and Matt Feiszli and Jitendra Malik", "abstract": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.", "link": "http://arxiv.org/abs/2511.16624v1", "date": "2025-11-20", "relevancy": 3.2871, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6658}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6658}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM%203D%3A%203Dfy%20Anything%20in%20Images&body=Title%3A%20SAM%203D%3A%203Dfy%20Anything%20in%20Images%0AAuthor%3A%20%20SAM%203D%20Team%20and%20Xingyu%20Chen%20and%20Fu-Jen%20Chu%20and%20Pierre%20Gleize%20and%20Kevin%20J%20Liang%20and%20Alexander%20Sax%20and%20Hao%20Tang%20and%20Weiyao%20Wang%20and%20Michelle%20Guo%20and%20Thibaut%20Hardin%20and%20Xiang%20Li%20and%20Aohan%20Lin%20and%20Jiawei%20Liu%20and%20Ziqi%20Ma%20and%20Anushka%20Sagar%20and%20Bowen%20Song%20and%20Xiaodong%20Wang%20and%20Jianing%20Yang%20and%20Bowen%20Zhang%20and%20Piotr%20Doll%C3%A1r%20and%20Georgia%20Gkioxari%20and%20Matt%20Feiszli%20and%20Jitendra%20Malik%0AAbstract%3A%20We%20present%20SAM%203D%2C%20a%20generative%20model%20for%20visually%20grounded%203D%20object%20reconstruction%2C%20predicting%20geometry%2C%20texture%2C%20and%20layout%20from%20a%20single%20image.%20SAM%203D%20excels%20in%20natural%20images%2C%20where%20occlusion%20and%20scene%20clutter%20are%20common%20and%20visual%20recognition%20cues%20from%20context%20play%20a%20larger%20role.%20We%20achieve%20this%20with%20a%20human-%20and%20model-in-the-loop%20pipeline%20for%20annotating%20object%20shape%2C%20texture%2C%20and%20pose%2C%20providing%20visually%20grounded%203D%20reconstruction%20data%20at%20unprecedented%20scale.%20We%20learn%20from%20this%20data%20in%20a%20modern%2C%20multi-stage%20training%20framework%20that%20combines%20synthetic%20pretraining%20with%20real-world%20alignment%2C%20breaking%20the%203D%20%22data%20barrier%22.%20We%20obtain%20significant%20gains%20over%20recent%20work%2C%20with%20at%20least%20a%205%3A1%20win%20rate%20in%20human%20preference%20tests%20on%20real-world%20objects%20and%20scenes.%20We%20will%20release%20our%20code%20and%20model%20weights%2C%20an%20online%20demo%2C%20and%20a%20new%20challenging%20benchmark%20for%20in-the-wild%203D%20object%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM%25203D%253A%25203Dfy%2520Anything%2520in%2520Images%26entry.906535625%3D%2520SAM%25203D%2520Team%2520and%2520Xingyu%2520Chen%2520and%2520Fu-Jen%2520Chu%2520and%2520Pierre%2520Gleize%2520and%2520Kevin%2520J%2520Liang%2520and%2520Alexander%2520Sax%2520and%2520Hao%2520Tang%2520and%2520Weiyao%2520Wang%2520and%2520Michelle%2520Guo%2520and%2520Thibaut%2520Hardin%2520and%2520Xiang%2520Li%2520and%2520Aohan%2520Lin%2520and%2520Jiawei%2520Liu%2520and%2520Ziqi%2520Ma%2520and%2520Anushka%2520Sagar%2520and%2520Bowen%2520Song%2520and%2520Xiaodong%2520Wang%2520and%2520Jianing%2520Yang%2520and%2520Bowen%2520Zhang%2520and%2520Piotr%2520Doll%25C3%25A1r%2520and%2520Georgia%2520Gkioxari%2520and%2520Matt%2520Feiszli%2520and%2520Jitendra%2520Malik%26entry.1292438233%3DWe%2520present%2520SAM%25203D%252C%2520a%2520generative%2520model%2520for%2520visually%2520grounded%25203D%2520object%2520reconstruction%252C%2520predicting%2520geometry%252C%2520texture%252C%2520and%2520layout%2520from%2520a%2520single%2520image.%2520SAM%25203D%2520excels%2520in%2520natural%2520images%252C%2520where%2520occlusion%2520and%2520scene%2520clutter%2520are%2520common%2520and%2520visual%2520recognition%2520cues%2520from%2520context%2520play%2520a%2520larger%2520role.%2520We%2520achieve%2520this%2520with%2520a%2520human-%2520and%2520model-in-the-loop%2520pipeline%2520for%2520annotating%2520object%2520shape%252C%2520texture%252C%2520and%2520pose%252C%2520providing%2520visually%2520grounded%25203D%2520reconstruction%2520data%2520at%2520unprecedented%2520scale.%2520We%2520learn%2520from%2520this%2520data%2520in%2520a%2520modern%252C%2520multi-stage%2520training%2520framework%2520that%2520combines%2520synthetic%2520pretraining%2520with%2520real-world%2520alignment%252C%2520breaking%2520the%25203D%2520%2522data%2520barrier%2522.%2520We%2520obtain%2520significant%2520gains%2520over%2520recent%2520work%252C%2520with%2520at%2520least%2520a%25205%253A1%2520win%2520rate%2520in%2520human%2520preference%2520tests%2520on%2520real-world%2520objects%2520and%2520scenes.%2520We%2520will%2520release%2520our%2520code%2520and%2520model%2520weights%252C%2520an%2520online%2520demo%252C%2520and%2520a%2520new%2520challenging%2520benchmark%2520for%2520in-the-wild%25203D%2520object%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM%203D%3A%203Dfy%20Anything%20in%20Images&entry.906535625=%20SAM%203D%20Team%20and%20Xingyu%20Chen%20and%20Fu-Jen%20Chu%20and%20Pierre%20Gleize%20and%20Kevin%20J%20Liang%20and%20Alexander%20Sax%20and%20Hao%20Tang%20and%20Weiyao%20Wang%20and%20Michelle%20Guo%20and%20Thibaut%20Hardin%20and%20Xiang%20Li%20and%20Aohan%20Lin%20and%20Jiawei%20Liu%20and%20Ziqi%20Ma%20and%20Anushka%20Sagar%20and%20Bowen%20Song%20and%20Xiaodong%20Wang%20and%20Jianing%20Yang%20and%20Bowen%20Zhang%20and%20Piotr%20Doll%C3%A1r%20and%20Georgia%20Gkioxari%20and%20Matt%20Feiszli%20and%20Jitendra%20Malik&entry.1292438233=We%20present%20SAM%203D%2C%20a%20generative%20model%20for%20visually%20grounded%203D%20object%20reconstruction%2C%20predicting%20geometry%2C%20texture%2C%20and%20layout%20from%20a%20single%20image.%20SAM%203D%20excels%20in%20natural%20images%2C%20where%20occlusion%20and%20scene%20clutter%20are%20common%20and%20visual%20recognition%20cues%20from%20context%20play%20a%20larger%20role.%20We%20achieve%20this%20with%20a%20human-%20and%20model-in-the-loop%20pipeline%20for%20annotating%20object%20shape%2C%20texture%2C%20and%20pose%2C%20providing%20visually%20grounded%203D%20reconstruction%20data%20at%20unprecedented%20scale.%20We%20learn%20from%20this%20data%20in%20a%20modern%2C%20multi-stage%20training%20framework%20that%20combines%20synthetic%20pretraining%20with%20real-world%20alignment%2C%20breaking%20the%203D%20%22data%20barrier%22.%20We%20obtain%20significant%20gains%20over%20recent%20work%2C%20with%20at%20least%20a%205%3A1%20win%20rate%20in%20human%20preference%20tests%20on%20real-world%20objects%20and%20scenes.%20We%20will%20release%20our%20code%20and%20model%20weights%2C%20an%20online%20demo%2C%20and%20a%20new%20challenging%20benchmark%20for%20in-the-wild%203D%20object%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2511.16624v1&entry.124074799=Read"},
{"title": "Optimizing 3D Gaussian Splattering for Mobile GPUs", "author": "Md Musfiqur Rahman Sanim and Zhihao Shu and Bahram Afsharmanesh and AmirAli Mirian and Jiexiong Guan and Wei Niu and Bin Ren and Gagan Agrawal", "abstract": "Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\\times$ and 1.7$\\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.", "link": "http://arxiv.org/abs/2511.16298v1", "date": "2025-11-20", "relevancy": 3.2447, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6906}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6291}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%203D%20Gaussian%20Splattering%20for%20Mobile%20GPUs&body=Title%3A%20Optimizing%203D%20Gaussian%20Splattering%20for%20Mobile%20GPUs%0AAuthor%3A%20Md%20Musfiqur%20Rahman%20Sanim%20and%20Zhihao%20Shu%20and%20Bahram%20Afsharmanesh%20and%20AmirAli%20Mirian%20and%20Jiexiong%20Guan%20and%20Wei%20Niu%20and%20Bin%20Ren%20and%20Gagan%20Agrawal%0AAbstract%3A%20Image-based%203D%20scene%20reconstruction%2C%20which%20transforms%20multi-view%20images%20into%20a%20structured%203D%20representation%20of%20the%20surrounding%20environment%2C%20is%20a%20common%20task%20across%20many%20modern%20applications.%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20new%20paradigm%20to%20address%20this%20problem%20and%20offers%20considerable%20efficiency%20as%20compared%20to%20the%20previous%20methods.%20Motivated%20by%20this%2C%20and%20considering%20various%20benefits%20of%20mobile%20device%20deployment%20%28data%20privacy%2C%20operating%20without%20internet%20connectivity%2C%20and%20potentially%20faster%20responses%29%2C%20this%20paper%20develops%20Texture3dgs%2C%20an%20optimized%20mapping%20of%203DGS%20for%20a%20mobile%20GPU.%20A%20critical%20challenge%20in%20this%20area%20turns%20out%20to%20be%20optimizing%20for%20the%20two-dimensional%20%282D%29%20texture%20cache%2C%20which%20needs%20to%20be%20exploited%20for%20faster%20executions%20on%20mobile%20GPUs.%20As%20a%20sorting%20method%20dominates%20the%20computations%20in%203DGS%20on%20mobile%20platforms%2C%20the%20core%20of%20Texture3dgs%20is%20a%20novel%20sorting%20algorithm%20where%20the%20processing%2C%20data%20movement%2C%20and%20placement%20are%20highly%20optimized%20for%202D%20memory.%20The%20properties%20of%20this%20algorithm%20are%20analyzed%20in%20view%20of%20a%20cost%20model%20for%20the%20texture%20cache.%20In%20addition%2C%20we%20accelerate%20other%20steps%20of%20the%203DGS%20algorithm%20through%20improved%20variable%20layout%20design%20and%20other%20optimizations.%20End-to-end%20evaluation%20shows%20that%20Texture3dgs%20delivers%20up%20to%204.1%24%5Ctimes%24%20and%201.7%24%5Ctimes%24%20speedup%20for%20the%20sorting%20and%20overall%203D%20scene%20reconstruction%2C%20respectively%20--%20while%20also%20reducing%20memory%20usage%20by%20up%20to%201.6%24%5Ctimes%24%20--%20demonstrating%20the%20effectiveness%20of%20our%20design%20for%20efficient%20mobile%203D%20scene%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%25203D%2520Gaussian%2520Splattering%2520for%2520Mobile%2520GPUs%26entry.906535625%3DMd%2520Musfiqur%2520Rahman%2520Sanim%2520and%2520Zhihao%2520Shu%2520and%2520Bahram%2520Afsharmanesh%2520and%2520AmirAli%2520Mirian%2520and%2520Jiexiong%2520Guan%2520and%2520Wei%2520Niu%2520and%2520Bin%2520Ren%2520and%2520Gagan%2520Agrawal%26entry.1292438233%3DImage-based%25203D%2520scene%2520reconstruction%252C%2520which%2520transforms%2520multi-view%2520images%2520into%2520a%2520structured%25203D%2520representation%2520of%2520the%2520surrounding%2520environment%252C%2520is%2520a%2520common%2520task%2520across%2520many%2520modern%2520applications.%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520a%2520new%2520paradigm%2520to%2520address%2520this%2520problem%2520and%2520offers%2520considerable%2520efficiency%2520as%2520compared%2520to%2520the%2520previous%2520methods.%2520Motivated%2520by%2520this%252C%2520and%2520considering%2520various%2520benefits%2520of%2520mobile%2520device%2520deployment%2520%2528data%2520privacy%252C%2520operating%2520without%2520internet%2520connectivity%252C%2520and%2520potentially%2520faster%2520responses%2529%252C%2520this%2520paper%2520develops%2520Texture3dgs%252C%2520an%2520optimized%2520mapping%2520of%25203DGS%2520for%2520a%2520mobile%2520GPU.%2520A%2520critical%2520challenge%2520in%2520this%2520area%2520turns%2520out%2520to%2520be%2520optimizing%2520for%2520the%2520two-dimensional%2520%25282D%2529%2520texture%2520cache%252C%2520which%2520needs%2520to%2520be%2520exploited%2520for%2520faster%2520executions%2520on%2520mobile%2520GPUs.%2520As%2520a%2520sorting%2520method%2520dominates%2520the%2520computations%2520in%25203DGS%2520on%2520mobile%2520platforms%252C%2520the%2520core%2520of%2520Texture3dgs%2520is%2520a%2520novel%2520sorting%2520algorithm%2520where%2520the%2520processing%252C%2520data%2520movement%252C%2520and%2520placement%2520are%2520highly%2520optimized%2520for%25202D%2520memory.%2520The%2520properties%2520of%2520this%2520algorithm%2520are%2520analyzed%2520in%2520view%2520of%2520a%2520cost%2520model%2520for%2520the%2520texture%2520cache.%2520In%2520addition%252C%2520we%2520accelerate%2520other%2520steps%2520of%2520the%25203DGS%2520algorithm%2520through%2520improved%2520variable%2520layout%2520design%2520and%2520other%2520optimizations.%2520End-to-end%2520evaluation%2520shows%2520that%2520Texture3dgs%2520delivers%2520up%2520to%25204.1%2524%255Ctimes%2524%2520and%25201.7%2524%255Ctimes%2524%2520speedup%2520for%2520the%2520sorting%2520and%2520overall%25203D%2520scene%2520reconstruction%252C%2520respectively%2520--%2520while%2520also%2520reducing%2520memory%2520usage%2520by%2520up%2520to%25201.6%2524%255Ctimes%2524%2520--%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520design%2520for%2520efficient%2520mobile%25203D%2520scene%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%203D%20Gaussian%20Splattering%20for%20Mobile%20GPUs&entry.906535625=Md%20Musfiqur%20Rahman%20Sanim%20and%20Zhihao%20Shu%20and%20Bahram%20Afsharmanesh%20and%20AmirAli%20Mirian%20and%20Jiexiong%20Guan%20and%20Wei%20Niu%20and%20Bin%20Ren%20and%20Gagan%20Agrawal&entry.1292438233=Image-based%203D%20scene%20reconstruction%2C%20which%20transforms%20multi-view%20images%20into%20a%20structured%203D%20representation%20of%20the%20surrounding%20environment%2C%20is%20a%20common%20task%20across%20many%20modern%20applications.%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20new%20paradigm%20to%20address%20this%20problem%20and%20offers%20considerable%20efficiency%20as%20compared%20to%20the%20previous%20methods.%20Motivated%20by%20this%2C%20and%20considering%20various%20benefits%20of%20mobile%20device%20deployment%20%28data%20privacy%2C%20operating%20without%20internet%20connectivity%2C%20and%20potentially%20faster%20responses%29%2C%20this%20paper%20develops%20Texture3dgs%2C%20an%20optimized%20mapping%20of%203DGS%20for%20a%20mobile%20GPU.%20A%20critical%20challenge%20in%20this%20area%20turns%20out%20to%20be%20optimizing%20for%20the%20two-dimensional%20%282D%29%20texture%20cache%2C%20which%20needs%20to%20be%20exploited%20for%20faster%20executions%20on%20mobile%20GPUs.%20As%20a%20sorting%20method%20dominates%20the%20computations%20in%203DGS%20on%20mobile%20platforms%2C%20the%20core%20of%20Texture3dgs%20is%20a%20novel%20sorting%20algorithm%20where%20the%20processing%2C%20data%20movement%2C%20and%20placement%20are%20highly%20optimized%20for%202D%20memory.%20The%20properties%20of%20this%20algorithm%20are%20analyzed%20in%20view%20of%20a%20cost%20model%20for%20the%20texture%20cache.%20In%20addition%2C%20we%20accelerate%20other%20steps%20of%20the%203DGS%20algorithm%20through%20improved%20variable%20layout%20design%20and%20other%20optimizations.%20End-to-end%20evaluation%20shows%20that%20Texture3dgs%20delivers%20up%20to%204.1%24%5Ctimes%24%20and%201.7%24%5Ctimes%24%20speedup%20for%20the%20sorting%20and%20overall%203D%20scene%20reconstruction%2C%20respectively%20--%20while%20also%20reducing%20memory%20usage%20by%20up%20to%201.6%24%5Ctimes%24%20--%20demonstrating%20the%20effectiveness%20of%20our%20design%20for%20efficient%20mobile%203D%20scene%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2511.16298v1&entry.124074799=Read"},
{"title": "LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs", "author": "Doriand Petit and Steve Bourgeois and Vincent Gay-Bellile and Florian Chabot and Lo\u00efc Barthe", "abstract": "Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.", "link": "http://arxiv.org/abs/2511.16454v1", "date": "2025-11-20", "relevancy": 3.2431, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6603}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA%24%5E3%24%3A%20Representing%203D%20Scenes%20like%20a%20Cubist%20Painter%20to%20Boost%203D%20Scene%20Understanding%20of%20VLMs&body=Title%3A%20LLaVA%24%5E3%24%3A%20Representing%203D%20Scenes%20like%20a%20Cubist%20Painter%20to%20Boost%203D%20Scene%20Understanding%20of%20VLMs%0AAuthor%3A%20Doriand%20Petit%20and%20Steve%20Bourgeois%20and%20Vincent%20Gay-Bellile%20and%20Florian%20Chabot%20and%20Lo%C3%AFc%20Barthe%0AAbstract%3A%20Developing%20a%20multi-modal%20language%20model%20capable%20of%20understanding%203D%20scenes%20remains%20challenging%20due%20to%20the%20limited%20availability%20of%203D%20training%20data%2C%20in%20contrast%20to%20the%20abundance%20of%202D%20datasets%20used%20for%20vision-language%20models%20%28VLM%29.%20As%20an%20alternative%2C%20we%20introduce%20LLaVA%24%5E3%24%20%28pronounced%20LLaVA-Cube%29%2C%20a%20novel%20method%20that%20improves%20the%203D%20scene%20understanding%20capabilities%20of%20VLM%20using%20only%20multi-view%202D%20images%20and%20without%20any%20fine-tuning.%20Inspired%20by%20Cubist%20painters%2C%20who%20represented%20multiple%20viewpoints%20of%20a%203D%20object%20within%20a%20single%20picture%2C%20we%20propose%20to%20describe%20the%203D%20scene%20for%20the%20VLM%20through%20omnidirectional%20visual%20representations%20of%20each%20object.%20These%20representations%20are%20derived%20from%20an%20intermediate%20multi-view%203D%20reconstruction%20of%20the%20scene.%20Extensive%20experiments%20on%203D%20VQA%20and%203D%20language%20grounding%20show%20that%20our%20approach%20outperforms%20previous%202D-based%20VLM%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA%2524%255E3%2524%253A%2520Representing%25203D%2520Scenes%2520like%2520a%2520Cubist%2520Painter%2520to%2520Boost%25203D%2520Scene%2520Understanding%2520of%2520VLMs%26entry.906535625%3DDoriand%2520Petit%2520and%2520Steve%2520Bourgeois%2520and%2520Vincent%2520Gay-Bellile%2520and%2520Florian%2520Chabot%2520and%2520Lo%25C3%25AFc%2520Barthe%26entry.1292438233%3DDeveloping%2520a%2520multi-modal%2520language%2520model%2520capable%2520of%2520understanding%25203D%2520scenes%2520remains%2520challenging%2520due%2520to%2520the%2520limited%2520availability%2520of%25203D%2520training%2520data%252C%2520in%2520contrast%2520to%2520the%2520abundance%2520of%25202D%2520datasets%2520used%2520for%2520vision-language%2520models%2520%2528VLM%2529.%2520As%2520an%2520alternative%252C%2520we%2520introduce%2520LLaVA%2524%255E3%2524%2520%2528pronounced%2520LLaVA-Cube%2529%252C%2520a%2520novel%2520method%2520that%2520improves%2520the%25203D%2520scene%2520understanding%2520capabilities%2520of%2520VLM%2520using%2520only%2520multi-view%25202D%2520images%2520and%2520without%2520any%2520fine-tuning.%2520Inspired%2520by%2520Cubist%2520painters%252C%2520who%2520represented%2520multiple%2520viewpoints%2520of%2520a%25203D%2520object%2520within%2520a%2520single%2520picture%252C%2520we%2520propose%2520to%2520describe%2520the%25203D%2520scene%2520for%2520the%2520VLM%2520through%2520omnidirectional%2520visual%2520representations%2520of%2520each%2520object.%2520These%2520representations%2520are%2520derived%2520from%2520an%2520intermediate%2520multi-view%25203D%2520reconstruction%2520of%2520the%2520scene.%2520Extensive%2520experiments%2520on%25203D%2520VQA%2520and%25203D%2520language%2520grounding%2520show%2520that%2520our%2520approach%2520outperforms%2520previous%25202D-based%2520VLM%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA%24%5E3%24%3A%20Representing%203D%20Scenes%20like%20a%20Cubist%20Painter%20to%20Boost%203D%20Scene%20Understanding%20of%20VLMs&entry.906535625=Doriand%20Petit%20and%20Steve%20Bourgeois%20and%20Vincent%20Gay-Bellile%20and%20Florian%20Chabot%20and%20Lo%C3%AFc%20Barthe&entry.1292438233=Developing%20a%20multi-modal%20language%20model%20capable%20of%20understanding%203D%20scenes%20remains%20challenging%20due%20to%20the%20limited%20availability%20of%203D%20training%20data%2C%20in%20contrast%20to%20the%20abundance%20of%202D%20datasets%20used%20for%20vision-language%20models%20%28VLM%29.%20As%20an%20alternative%2C%20we%20introduce%20LLaVA%24%5E3%24%20%28pronounced%20LLaVA-Cube%29%2C%20a%20novel%20method%20that%20improves%20the%203D%20scene%20understanding%20capabilities%20of%20VLM%20using%20only%20multi-view%202D%20images%20and%20without%20any%20fine-tuning.%20Inspired%20by%20Cubist%20painters%2C%20who%20represented%20multiple%20viewpoints%20of%20a%203D%20object%20within%20a%20single%20picture%2C%20we%20propose%20to%20describe%20the%203D%20scene%20for%20the%20VLM%20through%20omnidirectional%20visual%20representations%20of%20each%20object.%20These%20representations%20are%20derived%20from%20an%20intermediate%20multi-view%203D%20reconstruction%20of%20the%20scene.%20Extensive%20experiments%20on%203D%20VQA%20and%203D%20language%20grounding%20show%20that%20our%20approach%20outperforms%20previous%202D-based%20VLM%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2511.16454v1&entry.124074799=Read"},
{"title": "EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering", "author": "Pierrick Bournez and Luca Savant Aira and Thibaud Ehret and Gabriele Facciolo", "abstract": "Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models", "link": "http://arxiv.org/abs/2511.16542v1", "date": "2025-11-20", "relevancy": 3.2022, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7011}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6153}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EOGS%2B%2B%3A%20Earth%20Observation%20Gaussian%20Splatting%20with%20Internal%20Camera%20Refinement%20and%20Direct%20Panchromatic%20Rendering&body=Title%3A%20EOGS%2B%2B%3A%20Earth%20Observation%20Gaussian%20Splatting%20with%20Internal%20Camera%20Refinement%20and%20Direct%20Panchromatic%20Rendering%0AAuthor%3A%20Pierrick%20Bournez%20and%20Luca%20Savant%20Aira%20and%20Thibaud%20Ehret%20and%20Gabriele%20Facciolo%0AAbstract%3A%20Recently%2C%203D%20Gaussian%20Splatting%20has%20been%20introduced%20as%20a%20compelling%20alternative%20to%20NeRF%20for%20Earth%20observation%2C%20offering%20com-%20petitive%20reconstruction%20quality%20with%20significantly%20reduced%20training%20times.%20In%20this%20work%2C%20we%20extend%20the%20Earth%20Observation%20Gaussian%20Splatting%20%28EOGS%29%20framework%20to%20propose%20EOGS%2B%2B%2C%20a%20novel%20method%20tailored%20for%20satellite%20imagery%20that%20directly%20operates%20on%20raw%20high-resolution%20panchromatic%20data%20without%20requiring%20external%20preprocessing.%20Furthermore%2C%20leveraging%20optical%20flow%20techniques%20we%20embed%20bundle%20adjustment%20directly%20within%20the%20training%20process%2C%20avoiding%20reliance%20on%20external%20optimization%20tools%20while%20improving%20camera%20pose%20estimation.%20We%20also%20introduce%20several%20improvements%20to%20the%20original%20implementation%2C%20including%20early%20stopping%20and%20TSDF%20post-processing%2C%20all%20contributing%20to%20sharper%20reconstructions%20and%20better%20geometric%20accuracy.%20Experiments%20on%20the%20IARPA%202016%20and%20DFC2019%20datasets%20demonstrate%20that%20EOGS%2B%2B%20achieves%20state-of-the-art%20performance%20in%20terms%20of%20reconstruction%20quality%20and%20effi-%20ciency%2C%20outperforming%20the%20original%20EOGS%20method%20and%20other%20NeRF-based%20methods%20while%20maintaining%20the%20computational%20advantages%20of%20Gaussian%20Splatting.%20Our%20model%20demonstrates%20an%20improvement%20from%201.33%20to%201.19%20mean%20MAE%20errors%20on%20buildings%20compared%20to%20the%20original%20EOGS%20models%0ALink%3A%20http%3A//arxiv.org/abs/2511.16542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEOGS%252B%252B%253A%2520Earth%2520Observation%2520Gaussian%2520Splatting%2520with%2520Internal%2520Camera%2520Refinement%2520and%2520Direct%2520Panchromatic%2520Rendering%26entry.906535625%3DPierrick%2520Bournez%2520and%2520Luca%2520Savant%2520Aira%2520and%2520Thibaud%2520Ehret%2520and%2520Gabriele%2520Facciolo%26entry.1292438233%3DRecently%252C%25203D%2520Gaussian%2520Splatting%2520has%2520been%2520introduced%2520as%2520a%2520compelling%2520alternative%2520to%2520NeRF%2520for%2520Earth%2520observation%252C%2520offering%2520com-%2520petitive%2520reconstruction%2520quality%2520with%2520significantly%2520reduced%2520training%2520times.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%2520Earth%2520Observation%2520Gaussian%2520Splatting%2520%2528EOGS%2529%2520framework%2520to%2520propose%2520EOGS%252B%252B%252C%2520a%2520novel%2520method%2520tailored%2520for%2520satellite%2520imagery%2520that%2520directly%2520operates%2520on%2520raw%2520high-resolution%2520panchromatic%2520data%2520without%2520requiring%2520external%2520preprocessing.%2520Furthermore%252C%2520leveraging%2520optical%2520flow%2520techniques%2520we%2520embed%2520bundle%2520adjustment%2520directly%2520within%2520the%2520training%2520process%252C%2520avoiding%2520reliance%2520on%2520external%2520optimization%2520tools%2520while%2520improving%2520camera%2520pose%2520estimation.%2520We%2520also%2520introduce%2520several%2520improvements%2520to%2520the%2520original%2520implementation%252C%2520including%2520early%2520stopping%2520and%2520TSDF%2520post-processing%252C%2520all%2520contributing%2520to%2520sharper%2520reconstructions%2520and%2520better%2520geometric%2520accuracy.%2520Experiments%2520on%2520the%2520IARPA%25202016%2520and%2520DFC2019%2520datasets%2520demonstrate%2520that%2520EOGS%252B%252B%2520achieves%2520state-of-the-art%2520performance%2520in%2520terms%2520of%2520reconstruction%2520quality%2520and%2520effi-%2520ciency%252C%2520outperforming%2520the%2520original%2520EOGS%2520method%2520and%2520other%2520NeRF-based%2520methods%2520while%2520maintaining%2520the%2520computational%2520advantages%2520of%2520Gaussian%2520Splatting.%2520Our%2520model%2520demonstrates%2520an%2520improvement%2520from%25201.33%2520to%25201.19%2520mean%2520MAE%2520errors%2520on%2520buildings%2520compared%2520to%2520the%2520original%2520EOGS%2520models%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EOGS%2B%2B%3A%20Earth%20Observation%20Gaussian%20Splatting%20with%20Internal%20Camera%20Refinement%20and%20Direct%20Panchromatic%20Rendering&entry.906535625=Pierrick%20Bournez%20and%20Luca%20Savant%20Aira%20and%20Thibaud%20Ehret%20and%20Gabriele%20Facciolo&entry.1292438233=Recently%2C%203D%20Gaussian%20Splatting%20has%20been%20introduced%20as%20a%20compelling%20alternative%20to%20NeRF%20for%20Earth%20observation%2C%20offering%20com-%20petitive%20reconstruction%20quality%20with%20significantly%20reduced%20training%20times.%20In%20this%20work%2C%20we%20extend%20the%20Earth%20Observation%20Gaussian%20Splatting%20%28EOGS%29%20framework%20to%20propose%20EOGS%2B%2B%2C%20a%20novel%20method%20tailored%20for%20satellite%20imagery%20that%20directly%20operates%20on%20raw%20high-resolution%20panchromatic%20data%20without%20requiring%20external%20preprocessing.%20Furthermore%2C%20leveraging%20optical%20flow%20techniques%20we%20embed%20bundle%20adjustment%20directly%20within%20the%20training%20process%2C%20avoiding%20reliance%20on%20external%20optimization%20tools%20while%20improving%20camera%20pose%20estimation.%20We%20also%20introduce%20several%20improvements%20to%20the%20original%20implementation%2C%20including%20early%20stopping%20and%20TSDF%20post-processing%2C%20all%20contributing%20to%20sharper%20reconstructions%20and%20better%20geometric%20accuracy.%20Experiments%20on%20the%20IARPA%202016%20and%20DFC2019%20datasets%20demonstrate%20that%20EOGS%2B%2B%20achieves%20state-of-the-art%20performance%20in%20terms%20of%20reconstruction%20quality%20and%20effi-%20ciency%2C%20outperforming%20the%20original%20EOGS%20method%20and%20other%20NeRF-based%20methods%20while%20maintaining%20the%20computational%20advantages%20of%20Gaussian%20Splatting.%20Our%20model%20demonstrates%20an%20improvement%20from%201.33%20to%201.19%20mean%20MAE%20errors%20on%20buildings%20compared%20to%20the%20original%20EOGS%20models&entry.1838667208=http%3A//arxiv.org/abs/2511.16542v1&entry.124074799=Read"},
{"title": "DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation", "author": "Karim Abou Zeid and Kadir Yilmaz and Daan de Geus and Alexander Hermans and David Adrian and Timm Linder and Bastian Leibe", "abstract": "Vision foundation models (VFMs) trained on large-scale image datasets provide high-quality features that have significantly advanced 2D visual recognition. However, their potential in 3D scene segmentation remains largely untapped, despite the common availability of 2D images alongside 3D point cloud datasets. While significant research has been dedicated to 2D-3D fusion, recent state-of-the-art 3D methods predominantly focus on 3D data, leaving the integration of VFMs into 3D models underexplored. In this work, we challenge this trend by introducing DITR, a generally applicable approach that extracts 2D foundation model features, projects them to 3D, and finally injects them into a 3D point cloud segmentation model. DITR achieves state-of-the-art results on both indoor and outdoor 3D semantic segmentation benchmarks. To enable the use of VFMs even when images are unavailable during inference, we additionally propose to pretrain 3D models by distilling 2D foundation models. By initializing the 3D backbone with knowledge distilled from 2D VFMs, we create a strong basis for downstream 3D segmentation tasks, ultimately boosting performance across various datasets.", "link": "http://arxiv.org/abs/2503.18944v2", "date": "2025-11-20", "relevancy": 3.1396, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6477}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO%20in%20the%20Room%3A%20Leveraging%202D%20Foundation%20Models%20for%203D%20Segmentation&body=Title%3A%20DINO%20in%20the%20Room%3A%20Leveraging%202D%20Foundation%20Models%20for%203D%20Segmentation%0AAuthor%3A%20Karim%20Abou%20Zeid%20and%20Kadir%20Yilmaz%20and%20Daan%20de%20Geus%20and%20Alexander%20Hermans%20and%20David%20Adrian%20and%20Timm%20Linder%20and%20Bastian%20Leibe%0AAbstract%3A%20Vision%20foundation%20models%20%28VFMs%29%20trained%20on%20large-scale%20image%20datasets%20provide%20high-quality%20features%20that%20have%20significantly%20advanced%202D%20visual%20recognition.%20However%2C%20their%20potential%20in%203D%20scene%20segmentation%20remains%20largely%20untapped%2C%20despite%20the%20common%20availability%20of%202D%20images%20alongside%203D%20point%20cloud%20datasets.%20While%20significant%20research%20has%20been%20dedicated%20to%202D-3D%20fusion%2C%20recent%20state-of-the-art%203D%20methods%20predominantly%20focus%20on%203D%20data%2C%20leaving%20the%20integration%20of%20VFMs%20into%203D%20models%20underexplored.%20In%20this%20work%2C%20we%20challenge%20this%20trend%20by%20introducing%20DITR%2C%20a%20generally%20applicable%20approach%20that%20extracts%202D%20foundation%20model%20features%2C%20projects%20them%20to%203D%2C%20and%20finally%20injects%20them%20into%20a%203D%20point%20cloud%20segmentation%20model.%20DITR%20achieves%20state-of-the-art%20results%20on%20both%20indoor%20and%20outdoor%203D%20semantic%20segmentation%20benchmarks.%20To%20enable%20the%20use%20of%20VFMs%20even%20when%20images%20are%20unavailable%20during%20inference%2C%20we%20additionally%20propose%20to%20pretrain%203D%20models%20by%20distilling%202D%20foundation%20models.%20By%20initializing%20the%203D%20backbone%20with%20knowledge%20distilled%20from%202D%20VFMs%2C%20we%20create%20a%20strong%20basis%20for%20downstream%203D%20segmentation%20tasks%2C%20ultimately%20boosting%20performance%20across%20various%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2503.18944v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO%2520in%2520the%2520Room%253A%2520Leveraging%25202D%2520Foundation%2520Models%2520for%25203D%2520Segmentation%26entry.906535625%3DKarim%2520Abou%2520Zeid%2520and%2520Kadir%2520Yilmaz%2520and%2520Daan%2520de%2520Geus%2520and%2520Alexander%2520Hermans%2520and%2520David%2520Adrian%2520and%2520Timm%2520Linder%2520and%2520Bastian%2520Leibe%26entry.1292438233%3DVision%2520foundation%2520models%2520%2528VFMs%2529%2520trained%2520on%2520large-scale%2520image%2520datasets%2520provide%2520high-quality%2520features%2520that%2520have%2520significantly%2520advanced%25202D%2520visual%2520recognition.%2520However%252C%2520their%2520potential%2520in%25203D%2520scene%2520segmentation%2520remains%2520largely%2520untapped%252C%2520despite%2520the%2520common%2520availability%2520of%25202D%2520images%2520alongside%25203D%2520point%2520cloud%2520datasets.%2520While%2520significant%2520research%2520has%2520been%2520dedicated%2520to%25202D-3D%2520fusion%252C%2520recent%2520state-of-the-art%25203D%2520methods%2520predominantly%2520focus%2520on%25203D%2520data%252C%2520leaving%2520the%2520integration%2520of%2520VFMs%2520into%25203D%2520models%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520challenge%2520this%2520trend%2520by%2520introducing%2520DITR%252C%2520a%2520generally%2520applicable%2520approach%2520that%2520extracts%25202D%2520foundation%2520model%2520features%252C%2520projects%2520them%2520to%25203D%252C%2520and%2520finally%2520injects%2520them%2520into%2520a%25203D%2520point%2520cloud%2520segmentation%2520model.%2520DITR%2520achieves%2520state-of-the-art%2520results%2520on%2520both%2520indoor%2520and%2520outdoor%25203D%2520semantic%2520segmentation%2520benchmarks.%2520To%2520enable%2520the%2520use%2520of%2520VFMs%2520even%2520when%2520images%2520are%2520unavailable%2520during%2520inference%252C%2520we%2520additionally%2520propose%2520to%2520pretrain%25203D%2520models%2520by%2520distilling%25202D%2520foundation%2520models.%2520By%2520initializing%2520the%25203D%2520backbone%2520with%2520knowledge%2520distilled%2520from%25202D%2520VFMs%252C%2520we%2520create%2520a%2520strong%2520basis%2520for%2520downstream%25203D%2520segmentation%2520tasks%252C%2520ultimately%2520boosting%2520performance%2520across%2520various%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18944v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO%20in%20the%20Room%3A%20Leveraging%202D%20Foundation%20Models%20for%203D%20Segmentation&entry.906535625=Karim%20Abou%20Zeid%20and%20Kadir%20Yilmaz%20and%20Daan%20de%20Geus%20and%20Alexander%20Hermans%20and%20David%20Adrian%20and%20Timm%20Linder%20and%20Bastian%20Leibe&entry.1292438233=Vision%20foundation%20models%20%28VFMs%29%20trained%20on%20large-scale%20image%20datasets%20provide%20high-quality%20features%20that%20have%20significantly%20advanced%202D%20visual%20recognition.%20However%2C%20their%20potential%20in%203D%20scene%20segmentation%20remains%20largely%20untapped%2C%20despite%20the%20common%20availability%20of%202D%20images%20alongside%203D%20point%20cloud%20datasets.%20While%20significant%20research%20has%20been%20dedicated%20to%202D-3D%20fusion%2C%20recent%20state-of-the-art%203D%20methods%20predominantly%20focus%20on%203D%20data%2C%20leaving%20the%20integration%20of%20VFMs%20into%203D%20models%20underexplored.%20In%20this%20work%2C%20we%20challenge%20this%20trend%20by%20introducing%20DITR%2C%20a%20generally%20applicable%20approach%20that%20extracts%202D%20foundation%20model%20features%2C%20projects%20them%20to%203D%2C%20and%20finally%20injects%20them%20into%20a%203D%20point%20cloud%20segmentation%20model.%20DITR%20achieves%20state-of-the-art%20results%20on%20both%20indoor%20and%20outdoor%203D%20semantic%20segmentation%20benchmarks.%20To%20enable%20the%20use%20of%20VFMs%20even%20when%20images%20are%20unavailable%20during%20inference%2C%20we%20additionally%20propose%20to%20pretrain%203D%20models%20by%20distilling%202D%20foundation%20models.%20By%20initializing%20the%203D%20backbone%20with%20knowledge%20distilled%20from%202D%20VFMs%2C%20we%20create%20a%20strong%20basis%20for%20downstream%203D%20segmentation%20tasks%2C%20ultimately%20boosting%20performance%20across%20various%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2503.18944v2&entry.124074799=Read"},
{"title": "NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses", "author": "Jing Wen and Alexander G. Schwing and Shenlong Wang", "abstract": "We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate \"ground-truth\" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).", "link": "http://arxiv.org/abs/2511.16673v1", "date": "2025-11-20", "relevancy": 2.9968, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.603}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6025}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoPo-Avatar%3A%20Generalizable%20and%20Animatable%20Avatars%20from%20Sparse%20Inputs%20without%20Human%20Poses&body=Title%3A%20NoPo-Avatar%3A%20Generalizable%20and%20Animatable%20Avatars%20from%20Sparse%20Inputs%20without%20Human%20Poses%0AAuthor%3A%20Jing%20Wen%20and%20Alexander%20G.%20Schwing%20and%20Shenlong%20Wang%0AAbstract%3A%20We%20tackle%20the%20task%20of%20recovering%20an%20animatable%203D%20human%20avatar%20from%20a%20single%20or%20a%20sparse%20set%20of%20images.%20For%20this%20task%2C%20beyond%20a%20set%20of%20images%2C%20many%20prior%20state-of-the-art%20methods%20use%20accurate%20%22ground-truth%22%20camera%20poses%20and%20human%20poses%20as%20input%20to%20guide%20reconstruction%20at%20test-time.%20We%20show%20that%20pose-dependent%20reconstruction%20degrades%20results%20significantly%20if%20pose%20estimates%20are%20noisy.%20To%20overcome%20this%2C%20we%20introduce%20NoPo-Avatar%2C%20which%20reconstructs%20avatars%20solely%20from%20images%2C%20without%20any%20pose%20input.%20By%20removing%20the%20dependence%20of%20test-time%20reconstruction%20on%20human%20poses%2C%20NoPo-Avatar%20is%20not%20affected%20by%20noisy%20human%20pose%20estimates%2C%20making%20it%20more%20widely%20applicable.%20Experiments%20on%20challenging%20THuman2.0%2C%20XHuman%2C%20and%20HuGe100K%20data%20show%20that%20NoPo-Avatar%20outperforms%20existing%20baselines%20in%20practical%20settings%20%28without%20ground-truth%20poses%29%20and%20delivers%20comparable%20results%20in%20lab%20settings%20%28with%20ground-truth%20poses%29.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoPo-Avatar%253A%2520Generalizable%2520and%2520Animatable%2520Avatars%2520from%2520Sparse%2520Inputs%2520without%2520Human%2520Poses%26entry.906535625%3DJing%2520Wen%2520and%2520Alexander%2520G.%2520Schwing%2520and%2520Shenlong%2520Wang%26entry.1292438233%3DWe%2520tackle%2520the%2520task%2520of%2520recovering%2520an%2520animatable%25203D%2520human%2520avatar%2520from%2520a%2520single%2520or%2520a%2520sparse%2520set%2520of%2520images.%2520For%2520this%2520task%252C%2520beyond%2520a%2520set%2520of%2520images%252C%2520many%2520prior%2520state-of-the-art%2520methods%2520use%2520accurate%2520%2522ground-truth%2522%2520camera%2520poses%2520and%2520human%2520poses%2520as%2520input%2520to%2520guide%2520reconstruction%2520at%2520test-time.%2520We%2520show%2520that%2520pose-dependent%2520reconstruction%2520degrades%2520results%2520significantly%2520if%2520pose%2520estimates%2520are%2520noisy.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520NoPo-Avatar%252C%2520which%2520reconstructs%2520avatars%2520solely%2520from%2520images%252C%2520without%2520any%2520pose%2520input.%2520By%2520removing%2520the%2520dependence%2520of%2520test-time%2520reconstruction%2520on%2520human%2520poses%252C%2520NoPo-Avatar%2520is%2520not%2520affected%2520by%2520noisy%2520human%2520pose%2520estimates%252C%2520making%2520it%2520more%2520widely%2520applicable.%2520Experiments%2520on%2520challenging%2520THuman2.0%252C%2520XHuman%252C%2520and%2520HuGe100K%2520data%2520show%2520that%2520NoPo-Avatar%2520outperforms%2520existing%2520baselines%2520in%2520practical%2520settings%2520%2528without%2520ground-truth%2520poses%2529%2520and%2520delivers%2520comparable%2520results%2520in%2520lab%2520settings%2520%2528with%2520ground-truth%2520poses%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoPo-Avatar%3A%20Generalizable%20and%20Animatable%20Avatars%20from%20Sparse%20Inputs%20without%20Human%20Poses&entry.906535625=Jing%20Wen%20and%20Alexander%20G.%20Schwing%20and%20Shenlong%20Wang&entry.1292438233=We%20tackle%20the%20task%20of%20recovering%20an%20animatable%203D%20human%20avatar%20from%20a%20single%20or%20a%20sparse%20set%20of%20images.%20For%20this%20task%2C%20beyond%20a%20set%20of%20images%2C%20many%20prior%20state-of-the-art%20methods%20use%20accurate%20%22ground-truth%22%20camera%20poses%20and%20human%20poses%20as%20input%20to%20guide%20reconstruction%20at%20test-time.%20We%20show%20that%20pose-dependent%20reconstruction%20degrades%20results%20significantly%20if%20pose%20estimates%20are%20noisy.%20To%20overcome%20this%2C%20we%20introduce%20NoPo-Avatar%2C%20which%20reconstructs%20avatars%20solely%20from%20images%2C%20without%20any%20pose%20input.%20By%20removing%20the%20dependence%20of%20test-time%20reconstruction%20on%20human%20poses%2C%20NoPo-Avatar%20is%20not%20affected%20by%20noisy%20human%20pose%20estimates%2C%20making%20it%20more%20widely%20applicable.%20Experiments%20on%20challenging%20THuman2.0%2C%20XHuman%2C%20and%20HuGe100K%20data%20show%20that%20NoPo-Avatar%20outperforms%20existing%20baselines%20in%20practical%20settings%20%28without%20ground-truth%20poses%29%20and%20delivers%20comparable%20results%20in%20lab%20settings%20%28with%20ground-truth%20poses%29.&entry.1838667208=http%3A//arxiv.org/abs/2511.16673v1&entry.124074799=Read"},
{"title": "POMA-3D: The Point Map Way to 3D Scene Understanding", "author": "Ye Mao and Weixun Luo and Ranran Huang and Junpeng Jing and Krystian Mikolajczyk", "abstract": "In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/", "link": "http://arxiv.org/abs/2511.16567v1", "date": "2025-11-20", "relevancy": 2.967, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POMA-3D%3A%20The%20Point%20Map%20Way%20to%203D%20Scene%20Understanding&body=Title%3A%20POMA-3D%3A%20The%20Point%20Map%20Way%20to%203D%20Scene%20Understanding%0AAuthor%3A%20Ye%20Mao%20and%20Weixun%20Luo%20and%20Ranran%20Huang%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk%0AAbstract%3A%20In%20this%20paper%2C%20we%20introduce%20POMA-3D%2C%20the%20first%20self-supervised%203D%20representation%20model%20learned%20from%20point%20maps.%20Point%20maps%20encode%20explicit%203D%20coordinates%20on%20a%20structured%202D%20grid%2C%20preserving%20global%203D%20geometry%20while%20remaining%20compatible%20with%20the%20input%20format%20of%202D%20foundation%20models.%20To%20transfer%20rich%202D%20priors%20into%20POMA-3D%2C%20a%20view-to-scene%20alignment%20strategy%20is%20designed.%20Moreover%2C%20as%20point%20maps%20are%20view-dependent%20with%20respect%20to%20a%20canonical%20space%2C%20we%20introduce%20POMA-JEPA%2C%20a%20joint%20embedding-predictive%20architecture%20that%20enforces%20geometrically%20consistent%20point%20map%20features%20across%20multiple%20views.%20Additionally%2C%20we%20introduce%20ScenePoint%2C%20a%20point%20map%20dataset%20constructed%20from%206.5K%20room-level%20RGB-D%20scenes%20and%201M%202D%20image%20scenes%20to%20facilitate%20large-scale%20POMA-3D%20pretraining.%20Experiments%20show%20that%20POMA-3D%20serves%20as%20a%20strong%20backbone%20for%20both%20specialist%20and%20generalist%203D%20understanding.%20It%20benefits%20diverse%20tasks%2C%20including%203D%20question%20answering%2C%20embodied%20navigation%2C%20scene%20retrieval%2C%20and%20embodied%20localization%2C%20all%20achieved%20using%20only%20geometric%20inputs%20%28i.e.%2C%203D%20coordinates%29.%20Overall%2C%20our%20POMA-3D%20explores%20a%20point%20map%20way%20to%203D%20scene%20understanding%2C%20addressing%20the%20scarcity%20of%20pretrained%20priors%20and%20limited%20data%20in%203D%20representation%20learning.%20Project%20Page%3A%20https%3A//matchlab-imperial.github.io/poma3d/%0ALink%3A%20http%3A//arxiv.org/abs/2511.16567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOMA-3D%253A%2520The%2520Point%2520Map%2520Way%2520to%25203D%2520Scene%2520Understanding%26entry.906535625%3DYe%2520Mao%2520and%2520Weixun%2520Luo%2520and%2520Ranran%2520Huang%2520and%2520Junpeng%2520Jing%2520and%2520Krystian%2520Mikolajczyk%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520introduce%2520POMA-3D%252C%2520the%2520first%2520self-supervised%25203D%2520representation%2520model%2520learned%2520from%2520point%2520maps.%2520Point%2520maps%2520encode%2520explicit%25203D%2520coordinates%2520on%2520a%2520structured%25202D%2520grid%252C%2520preserving%2520global%25203D%2520geometry%2520while%2520remaining%2520compatible%2520with%2520the%2520input%2520format%2520of%25202D%2520foundation%2520models.%2520To%2520transfer%2520rich%25202D%2520priors%2520into%2520POMA-3D%252C%2520a%2520view-to-scene%2520alignment%2520strategy%2520is%2520designed.%2520Moreover%252C%2520as%2520point%2520maps%2520are%2520view-dependent%2520with%2520respect%2520to%2520a%2520canonical%2520space%252C%2520we%2520introduce%2520POMA-JEPA%252C%2520a%2520joint%2520embedding-predictive%2520architecture%2520that%2520enforces%2520geometrically%2520consistent%2520point%2520map%2520features%2520across%2520multiple%2520views.%2520Additionally%252C%2520we%2520introduce%2520ScenePoint%252C%2520a%2520point%2520map%2520dataset%2520constructed%2520from%25206.5K%2520room-level%2520RGB-D%2520scenes%2520and%25201M%25202D%2520image%2520scenes%2520to%2520facilitate%2520large-scale%2520POMA-3D%2520pretraining.%2520Experiments%2520show%2520that%2520POMA-3D%2520serves%2520as%2520a%2520strong%2520backbone%2520for%2520both%2520specialist%2520and%2520generalist%25203D%2520understanding.%2520It%2520benefits%2520diverse%2520tasks%252C%2520including%25203D%2520question%2520answering%252C%2520embodied%2520navigation%252C%2520scene%2520retrieval%252C%2520and%2520embodied%2520localization%252C%2520all%2520achieved%2520using%2520only%2520geometric%2520inputs%2520%2528i.e.%252C%25203D%2520coordinates%2529.%2520Overall%252C%2520our%2520POMA-3D%2520explores%2520a%2520point%2520map%2520way%2520to%25203D%2520scene%2520understanding%252C%2520addressing%2520the%2520scarcity%2520of%2520pretrained%2520priors%2520and%2520limited%2520data%2520in%25203D%2520representation%2520learning.%2520Project%2520Page%253A%2520https%253A//matchlab-imperial.github.io/poma3d/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POMA-3D%3A%20The%20Point%20Map%20Way%20to%203D%20Scene%20Understanding&entry.906535625=Ye%20Mao%20and%20Weixun%20Luo%20and%20Ranran%20Huang%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk&entry.1292438233=In%20this%20paper%2C%20we%20introduce%20POMA-3D%2C%20the%20first%20self-supervised%203D%20representation%20model%20learned%20from%20point%20maps.%20Point%20maps%20encode%20explicit%203D%20coordinates%20on%20a%20structured%202D%20grid%2C%20preserving%20global%203D%20geometry%20while%20remaining%20compatible%20with%20the%20input%20format%20of%202D%20foundation%20models.%20To%20transfer%20rich%202D%20priors%20into%20POMA-3D%2C%20a%20view-to-scene%20alignment%20strategy%20is%20designed.%20Moreover%2C%20as%20point%20maps%20are%20view-dependent%20with%20respect%20to%20a%20canonical%20space%2C%20we%20introduce%20POMA-JEPA%2C%20a%20joint%20embedding-predictive%20architecture%20that%20enforces%20geometrically%20consistent%20point%20map%20features%20across%20multiple%20views.%20Additionally%2C%20we%20introduce%20ScenePoint%2C%20a%20point%20map%20dataset%20constructed%20from%206.5K%20room-level%20RGB-D%20scenes%20and%201M%202D%20image%20scenes%20to%20facilitate%20large-scale%20POMA-3D%20pretraining.%20Experiments%20show%20that%20POMA-3D%20serves%20as%20a%20strong%20backbone%20for%20both%20specialist%20and%20generalist%203D%20understanding.%20It%20benefits%20diverse%20tasks%2C%20including%203D%20question%20answering%2C%20embodied%20navigation%2C%20scene%20retrieval%2C%20and%20embodied%20localization%2C%20all%20achieved%20using%20only%20geometric%20inputs%20%28i.e.%2C%203D%20coordinates%29.%20Overall%2C%20our%20POMA-3D%20explores%20a%20point%20map%20way%20to%203D%20scene%20understanding%2C%20addressing%20the%20scarcity%20of%20pretrained%20priors%20and%20limited%20data%20in%203D%20representation%20learning.%20Project%20Page%3A%20https%3A//matchlab-imperial.github.io/poma3d/&entry.1838667208=http%3A//arxiv.org/abs/2511.16567v1&entry.124074799=Read"},
{"title": "Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation", "author": "Jin Wang and Bingfeng Zhang and Jian Pang and Mengyu Liu and Honglong Chen and Weifeng Liu", "abstract": "Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.", "link": "http://arxiv.org/abs/2511.16435v1", "date": "2025-11-20", "relevancy": 2.96, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Visual%20Cues%3A%20Leveraging%20General%20Semantics%20as%20Support%20for%20Few-Shot%20Segmentation&body=Title%3A%20Beyond%20Visual%20Cues%3A%20Leveraging%20General%20Semantics%20as%20Support%20for%20Few-Shot%20Segmentation%0AAuthor%3A%20Jin%20Wang%20and%20Bingfeng%20Zhang%20and%20Jian%20Pang%20and%20Mengyu%20Liu%20and%20Honglong%20Chen%20and%20Weifeng%20Liu%0AAbstract%3A%20Few-shot%20segmentation%20%28FSS%29%20aims%20to%20segment%20novel%20classes%20under%20the%20guidance%20of%20limited%20support%20samples%20by%20a%20meta-learning%20paradigm.%20Existing%20methods%20mainly%20mine%20references%20from%20support%20images%20as%20meta%20guidance.%20However%2C%20due%20to%20intra-class%20variations%20among%20visual%20representations%2C%20the%20meta%20information%20extracted%20from%20support%20images%20cannot%20produce%20accurate%20guidance%20to%20segment%20untrained%20classes.%20In%20this%20paper%2C%20we%20argue%20that%20the%20references%20from%20support%20images%20may%20not%20be%20essential%2C%20the%20key%20to%20the%20support%20role%20is%20to%20provide%20unbiased%20meta%20guidance%20for%20both%20trained%20and%20untrained%20classes.%20We%20then%20introduce%20a%20Language-Driven%20Attribute%20Generalization%20%28LDAG%29%20architecture%20to%20utilize%20inherent%20target%20property%20language%20descriptions%20to%20build%20robust%20support%20strategy.%20Specifically%2C%20to%20obtain%20an%20unbiased%20support%20representation%2C%20we%20design%20a%20Multi-attribute%20Enhancement%20%28MaE%29%20module%2C%20which%20produces%20multiple%20detailed%20attribute%20descriptions%20of%20the%20target%20class%20through%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20then%20builds%20refined%20visual-text%20prior%20guidance%20utilizing%20multi-modal%20matching.%20Meanwhile%2C%20due%20to%20text-vision%20modal%20shift%2C%20attribute%20text%20struggles%20to%20promote%20visual%20feature%20representation%2C%20we%20design%20a%20Multi-modal%20Attribute%20Alignment%20%28MaA%29%20to%20achieve%20cross-modal%20interaction%20between%20attribute%20texts%20and%20visual%20feature.%20Experiments%20show%20that%20our%20proposed%20method%20outperforms%20existing%20approaches%20by%20a%20clear%20margin%20and%20achieves%20the%20new%20state-of-the%20art%20performance.%20The%20code%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Visual%2520Cues%253A%2520Leveraging%2520General%2520Semantics%2520as%2520Support%2520for%2520Few-Shot%2520Segmentation%26entry.906535625%3DJin%2520Wang%2520and%2520Bingfeng%2520Zhang%2520and%2520Jian%2520Pang%2520and%2520Mengyu%2520Liu%2520and%2520Honglong%2520Chen%2520and%2520Weifeng%2520Liu%26entry.1292438233%3DFew-shot%2520segmentation%2520%2528FSS%2529%2520aims%2520to%2520segment%2520novel%2520classes%2520under%2520the%2520guidance%2520of%2520limited%2520support%2520samples%2520by%2520a%2520meta-learning%2520paradigm.%2520Existing%2520methods%2520mainly%2520mine%2520references%2520from%2520support%2520images%2520as%2520meta%2520guidance.%2520However%252C%2520due%2520to%2520intra-class%2520variations%2520among%2520visual%2520representations%252C%2520the%2520meta%2520information%2520extracted%2520from%2520support%2520images%2520cannot%2520produce%2520accurate%2520guidance%2520to%2520segment%2520untrained%2520classes.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520the%2520references%2520from%2520support%2520images%2520may%2520not%2520be%2520essential%252C%2520the%2520key%2520to%2520the%2520support%2520role%2520is%2520to%2520provide%2520unbiased%2520meta%2520guidance%2520for%2520both%2520trained%2520and%2520untrained%2520classes.%2520We%2520then%2520introduce%2520a%2520Language-Driven%2520Attribute%2520Generalization%2520%2528LDAG%2529%2520architecture%2520to%2520utilize%2520inherent%2520target%2520property%2520language%2520descriptions%2520to%2520build%2520robust%2520support%2520strategy.%2520Specifically%252C%2520to%2520obtain%2520an%2520unbiased%2520support%2520representation%252C%2520we%2520design%2520a%2520Multi-attribute%2520Enhancement%2520%2528MaE%2529%2520module%252C%2520which%2520produces%2520multiple%2520detailed%2520attribute%2520descriptions%2520of%2520the%2520target%2520class%2520through%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520and%2520then%2520builds%2520refined%2520visual-text%2520prior%2520guidance%2520utilizing%2520multi-modal%2520matching.%2520Meanwhile%252C%2520due%2520to%2520text-vision%2520modal%2520shift%252C%2520attribute%2520text%2520struggles%2520to%2520promote%2520visual%2520feature%2520representation%252C%2520we%2520design%2520a%2520Multi-modal%2520Attribute%2520Alignment%2520%2528MaA%2529%2520to%2520achieve%2520cross-modal%2520interaction%2520between%2520attribute%2520texts%2520and%2520visual%2520feature.%2520Experiments%2520show%2520that%2520our%2520proposed%2520method%2520outperforms%2520existing%2520approaches%2520by%2520a%2520clear%2520margin%2520and%2520achieves%2520the%2520new%2520state-of-the%2520art%2520performance.%2520The%2520code%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Visual%20Cues%3A%20Leveraging%20General%20Semantics%20as%20Support%20for%20Few-Shot%20Segmentation&entry.906535625=Jin%20Wang%20and%20Bingfeng%20Zhang%20and%20Jian%20Pang%20and%20Mengyu%20Liu%20and%20Honglong%20Chen%20and%20Weifeng%20Liu&entry.1292438233=Few-shot%20segmentation%20%28FSS%29%20aims%20to%20segment%20novel%20classes%20under%20the%20guidance%20of%20limited%20support%20samples%20by%20a%20meta-learning%20paradigm.%20Existing%20methods%20mainly%20mine%20references%20from%20support%20images%20as%20meta%20guidance.%20However%2C%20due%20to%20intra-class%20variations%20among%20visual%20representations%2C%20the%20meta%20information%20extracted%20from%20support%20images%20cannot%20produce%20accurate%20guidance%20to%20segment%20untrained%20classes.%20In%20this%20paper%2C%20we%20argue%20that%20the%20references%20from%20support%20images%20may%20not%20be%20essential%2C%20the%20key%20to%20the%20support%20role%20is%20to%20provide%20unbiased%20meta%20guidance%20for%20both%20trained%20and%20untrained%20classes.%20We%20then%20introduce%20a%20Language-Driven%20Attribute%20Generalization%20%28LDAG%29%20architecture%20to%20utilize%20inherent%20target%20property%20language%20descriptions%20to%20build%20robust%20support%20strategy.%20Specifically%2C%20to%20obtain%20an%20unbiased%20support%20representation%2C%20we%20design%20a%20Multi-attribute%20Enhancement%20%28MaE%29%20module%2C%20which%20produces%20multiple%20detailed%20attribute%20descriptions%20of%20the%20target%20class%20through%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20then%20builds%20refined%20visual-text%20prior%20guidance%20utilizing%20multi-modal%20matching.%20Meanwhile%2C%20due%20to%20text-vision%20modal%20shift%2C%20attribute%20text%20struggles%20to%20promote%20visual%20feature%20representation%2C%20we%20design%20a%20Multi-modal%20Attribute%20Alignment%20%28MaA%29%20to%20achieve%20cross-modal%20interaction%20between%20attribute%20texts%20and%20visual%20feature.%20Experiments%20show%20that%20our%20proposed%20method%20outperforms%20existing%20approaches%20by%20a%20clear%20margin%20and%20achieves%20the%20new%20state-of-the%20art%20performance.%20The%20code%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2511.16435v1&entry.124074799=Read"},
{"title": "Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision", "author": "Shuyu Cao and Chongshou Li and Jie Xu and Tianrui Li and Na Zhao", "abstract": "3D hierarchical semantic segmentation (3DHS) is crucial for embodied intelligence applications that demand a multi-grained and multi-hierarchy understanding of 3D scenes. Despite the progress, previous 3DHS methods have overlooked following two challenges: I) multi-label learning with a parameter-sharing model can lead to multi-hierarchy conflicts in cross-hierarchy optimization, and II) the class imbalance issue is inevitable across multiple hierarchies of 3D scenes, which makes the model performance become dominated by major classes. To address these issues, we propose a novel framework with a primary 3DHS branch and an auxiliary discrimination branch. Specifically, to alleviate the multi-hierarchy conflicts, we propose a late-decoupled 3DHS framework which employs multiple decoders with the coarse-to-fine hierarchical guidance and consistency. The late-decoupled architecture can mitigate the underfitting and overfitting conflicts among multiple hierarchies and can also constrain the class imbalance problem in each individual hierarchy. Moreover, we introduce a 3DHS-oriented semantic prototype based bi-branch supervision mechanism, which additionally learns class-wise discriminative point cloud features and performs mutual supervision between the auxiliary and 3DHS branches, to enhance the class-imbalance segmentation. Extensive experiments on multiple datasets and backbones demonstrate that our approach achieves state-of-the-art 3DHS performance, and its core components can also be used as a plug-and-play enhancement to improve previous methods.", "link": "http://arxiv.org/abs/2511.16650v1", "date": "2025-11-20", "relevancy": 2.9376, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Late-decoupled%203D%20Hierarchical%20Semantic%20Segmentation%20with%20Semantic%20Prototype%20Discrimination%20based%20Bi-branch%20Supervision&body=Title%3A%20Late-decoupled%203D%20Hierarchical%20Semantic%20Segmentation%20with%20Semantic%20Prototype%20Discrimination%20based%20Bi-branch%20Supervision%0AAuthor%3A%20Shuyu%20Cao%20and%20Chongshou%20Li%20and%20Jie%20Xu%20and%20Tianrui%20Li%20and%20Na%20Zhao%0AAbstract%3A%203D%20hierarchical%20semantic%20segmentation%20%283DHS%29%20is%20crucial%20for%20embodied%20intelligence%20applications%20that%20demand%20a%20multi-grained%20and%20multi-hierarchy%20understanding%20of%203D%20scenes.%20Despite%20the%20progress%2C%20previous%203DHS%20methods%20have%20overlooked%20following%20two%20challenges%3A%20I%29%20multi-label%20learning%20with%20a%20parameter-sharing%20model%20can%20lead%20to%20multi-hierarchy%20conflicts%20in%20cross-hierarchy%20optimization%2C%20and%20II%29%20the%20class%20imbalance%20issue%20is%20inevitable%20across%20multiple%20hierarchies%20of%203D%20scenes%2C%20which%20makes%20the%20model%20performance%20become%20dominated%20by%20major%20classes.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20framework%20with%20a%20primary%203DHS%20branch%20and%20an%20auxiliary%20discrimination%20branch.%20Specifically%2C%20to%20alleviate%20the%20multi-hierarchy%20conflicts%2C%20we%20propose%20a%20late-decoupled%203DHS%20framework%20which%20employs%20multiple%20decoders%20with%20the%20coarse-to-fine%20hierarchical%20guidance%20and%20consistency.%20The%20late-decoupled%20architecture%20can%20mitigate%20the%20underfitting%20and%20overfitting%20conflicts%20among%20multiple%20hierarchies%20and%20can%20also%20constrain%20the%20class%20imbalance%20problem%20in%20each%20individual%20hierarchy.%20Moreover%2C%20we%20introduce%20a%203DHS-oriented%20semantic%20prototype%20based%20bi-branch%20supervision%20mechanism%2C%20which%20additionally%20learns%20class-wise%20discriminative%20point%20cloud%20features%20and%20performs%20mutual%20supervision%20between%20the%20auxiliary%20and%203DHS%20branches%2C%20to%20enhance%20the%20class-imbalance%20segmentation.%20Extensive%20experiments%20on%20multiple%20datasets%20and%20backbones%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%203DHS%20performance%2C%20and%20its%20core%20components%20can%20also%20be%20used%20as%20a%20plug-and-play%20enhancement%20to%20improve%20previous%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLate-decoupled%25203D%2520Hierarchical%2520Semantic%2520Segmentation%2520with%2520Semantic%2520Prototype%2520Discrimination%2520based%2520Bi-branch%2520Supervision%26entry.906535625%3DShuyu%2520Cao%2520and%2520Chongshou%2520Li%2520and%2520Jie%2520Xu%2520and%2520Tianrui%2520Li%2520and%2520Na%2520Zhao%26entry.1292438233%3D3D%2520hierarchical%2520semantic%2520segmentation%2520%25283DHS%2529%2520is%2520crucial%2520for%2520embodied%2520intelligence%2520applications%2520that%2520demand%2520a%2520multi-grained%2520and%2520multi-hierarchy%2520understanding%2520of%25203D%2520scenes.%2520Despite%2520the%2520progress%252C%2520previous%25203DHS%2520methods%2520have%2520overlooked%2520following%2520two%2520challenges%253A%2520I%2529%2520multi-label%2520learning%2520with%2520a%2520parameter-sharing%2520model%2520can%2520lead%2520to%2520multi-hierarchy%2520conflicts%2520in%2520cross-hierarchy%2520optimization%252C%2520and%2520II%2529%2520the%2520class%2520imbalance%2520issue%2520is%2520inevitable%2520across%2520multiple%2520hierarchies%2520of%25203D%2520scenes%252C%2520which%2520makes%2520the%2520model%2520performance%2520become%2520dominated%2520by%2520major%2520classes.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520framework%2520with%2520a%2520primary%25203DHS%2520branch%2520and%2520an%2520auxiliary%2520discrimination%2520branch.%2520Specifically%252C%2520to%2520alleviate%2520the%2520multi-hierarchy%2520conflicts%252C%2520we%2520propose%2520a%2520late-decoupled%25203DHS%2520framework%2520which%2520employs%2520multiple%2520decoders%2520with%2520the%2520coarse-to-fine%2520hierarchical%2520guidance%2520and%2520consistency.%2520The%2520late-decoupled%2520architecture%2520can%2520mitigate%2520the%2520underfitting%2520and%2520overfitting%2520conflicts%2520among%2520multiple%2520hierarchies%2520and%2520can%2520also%2520constrain%2520the%2520class%2520imbalance%2520problem%2520in%2520each%2520individual%2520hierarchy.%2520Moreover%252C%2520we%2520introduce%2520a%25203DHS-oriented%2520semantic%2520prototype%2520based%2520bi-branch%2520supervision%2520mechanism%252C%2520which%2520additionally%2520learns%2520class-wise%2520discriminative%2520point%2520cloud%2520features%2520and%2520performs%2520mutual%2520supervision%2520between%2520the%2520auxiliary%2520and%25203DHS%2520branches%252C%2520to%2520enhance%2520the%2520class-imbalance%2520segmentation.%2520Extensive%2520experiments%2520on%2520multiple%2520datasets%2520and%2520backbones%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%25203DHS%2520performance%252C%2520and%2520its%2520core%2520components%2520can%2520also%2520be%2520used%2520as%2520a%2520plug-and-play%2520enhancement%2520to%2520improve%2520previous%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Late-decoupled%203D%20Hierarchical%20Semantic%20Segmentation%20with%20Semantic%20Prototype%20Discrimination%20based%20Bi-branch%20Supervision&entry.906535625=Shuyu%20Cao%20and%20Chongshou%20Li%20and%20Jie%20Xu%20and%20Tianrui%20Li%20and%20Na%20Zhao&entry.1292438233=3D%20hierarchical%20semantic%20segmentation%20%283DHS%29%20is%20crucial%20for%20embodied%20intelligence%20applications%20that%20demand%20a%20multi-grained%20and%20multi-hierarchy%20understanding%20of%203D%20scenes.%20Despite%20the%20progress%2C%20previous%203DHS%20methods%20have%20overlooked%20following%20two%20challenges%3A%20I%29%20multi-label%20learning%20with%20a%20parameter-sharing%20model%20can%20lead%20to%20multi-hierarchy%20conflicts%20in%20cross-hierarchy%20optimization%2C%20and%20II%29%20the%20class%20imbalance%20issue%20is%20inevitable%20across%20multiple%20hierarchies%20of%203D%20scenes%2C%20which%20makes%20the%20model%20performance%20become%20dominated%20by%20major%20classes.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20framework%20with%20a%20primary%203DHS%20branch%20and%20an%20auxiliary%20discrimination%20branch.%20Specifically%2C%20to%20alleviate%20the%20multi-hierarchy%20conflicts%2C%20we%20propose%20a%20late-decoupled%203DHS%20framework%20which%20employs%20multiple%20decoders%20with%20the%20coarse-to-fine%20hierarchical%20guidance%20and%20consistency.%20The%20late-decoupled%20architecture%20can%20mitigate%20the%20underfitting%20and%20overfitting%20conflicts%20among%20multiple%20hierarchies%20and%20can%20also%20constrain%20the%20class%20imbalance%20problem%20in%20each%20individual%20hierarchy.%20Moreover%2C%20we%20introduce%20a%203DHS-oriented%20semantic%20prototype%20based%20bi-branch%20supervision%20mechanism%2C%20which%20additionally%20learns%20class-wise%20discriminative%20point%20cloud%20features%20and%20performs%20mutual%20supervision%20between%20the%20auxiliary%20and%203DHS%20branches%2C%20to%20enhance%20the%20class-imbalance%20segmentation.%20Extensive%20experiments%20on%20multiple%20datasets%20and%20backbones%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%203DHS%20performance%2C%20and%20its%20core%20components%20can%20also%20be%20used%20as%20a%20plug-and-play%20enhancement%20to%20improve%20previous%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.16650v1&entry.124074799=Read"},
{"title": "VisPlay: Self-Evolving Vision-Language Models from Images", "author": "Yicheng He and Chengsong Huang and Zongxia Li and Jiaxin Huang and Yonghui Yang", "abstract": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/", "link": "http://arxiv.org/abs/2511.15661v2", "date": "2025-11-20", "relevancy": 2.8804, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5875}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisPlay%3A%20Self-Evolving%20Vision-Language%20Models%20from%20Images&body=Title%3A%20VisPlay%3A%20Self-Evolving%20Vision-Language%20Models%20from%20Images%0AAuthor%3A%20Yicheng%20He%20and%20Chengsong%20Huang%20and%20Zongxia%20Li%20and%20Jiaxin%20Huang%20and%20Yonghui%20Yang%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20provides%20a%20principled%20framework%20for%20improving%20Vision-Language%20Models%20%28VLMs%29%20on%20complex%20reasoning%20tasks.%20However%2C%20existing%20RL%20approaches%20often%20rely%20on%20human-annotated%20labels%20or%20task-specific%20heuristics%20to%20define%20verifiable%20rewards%2C%20both%20of%20which%20are%20costly%20and%20difficult%20to%20scale.%20We%20introduce%20VisPlay%2C%20a%20self-evolving%20RL%20framework%20that%20enables%20VLMs%20to%20autonomously%20improve%20their%20reasoning%20abilities%20using%20large%20amounts%20of%20unlabeled%20image%20data.%20Starting%20from%20a%20single%20base%20VLM%2C%20VisPlay%20assigns%20the%20model%20into%20two%20interacting%20roles%3A%20an%20Image-Conditioned%20Questioner%20that%20formulates%20challenging%20yet%20answerable%20visual%20questions%2C%20and%20a%20Multimodal%20Reasoner%20that%20generates%20silver%20responses.%20These%20roles%20are%20jointly%20trained%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20which%20incorporates%20diversity%20and%20difficulty%20rewards%20to%20balance%20the%20complexity%20of%20generated%20questions%20with%20the%20quality%20of%20the%20silver%20answers.%20VisPlay%20scales%20efficiently%20across%20two%20model%20families.%20When%20trained%20on%20Qwen2.5-VL%20and%20MiMo-VL%2C%20VisPlay%20achieves%20consistent%20improvements%20in%20visual%20reasoning%2C%20compositional%20generalization%2C%20and%20hallucination%20reduction%20across%20eight%20benchmarks%2C%20including%20MM-Vet%20and%20MMMU%2C%20demonstrating%20a%20scalable%20path%20toward%20self-evolving%20multimodal%20intelligence.%20The%20project%20page%20is%20available%20at%20https%3A//bruno686.github.io/VisPlay/%0ALink%3A%20http%3A//arxiv.org/abs/2511.15661v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisPlay%253A%2520Self-Evolving%2520Vision-Language%2520Models%2520from%2520Images%26entry.906535625%3DYicheng%2520He%2520and%2520Chengsong%2520Huang%2520and%2520Zongxia%2520Li%2520and%2520Jiaxin%2520Huang%2520and%2520Yonghui%2520Yang%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520provides%2520a%2520principled%2520framework%2520for%2520improving%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520on%2520complex%2520reasoning%2520tasks.%2520However%252C%2520existing%2520RL%2520approaches%2520often%2520rely%2520on%2520human-annotated%2520labels%2520or%2520task-specific%2520heuristics%2520to%2520define%2520verifiable%2520rewards%252C%2520both%2520of%2520which%2520are%2520costly%2520and%2520difficult%2520to%2520scale.%2520We%2520introduce%2520VisPlay%252C%2520a%2520self-evolving%2520RL%2520framework%2520that%2520enables%2520VLMs%2520to%2520autonomously%2520improve%2520their%2520reasoning%2520abilities%2520using%2520large%2520amounts%2520of%2520unlabeled%2520image%2520data.%2520Starting%2520from%2520a%2520single%2520base%2520VLM%252C%2520VisPlay%2520assigns%2520the%2520model%2520into%2520two%2520interacting%2520roles%253A%2520an%2520Image-Conditioned%2520Questioner%2520that%2520formulates%2520challenging%2520yet%2520answerable%2520visual%2520questions%252C%2520and%2520a%2520Multimodal%2520Reasoner%2520that%2520generates%2520silver%2520responses.%2520These%2520roles%2520are%2520jointly%2520trained%2520with%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520which%2520incorporates%2520diversity%2520and%2520difficulty%2520rewards%2520to%2520balance%2520the%2520complexity%2520of%2520generated%2520questions%2520with%2520the%2520quality%2520of%2520the%2520silver%2520answers.%2520VisPlay%2520scales%2520efficiently%2520across%2520two%2520model%2520families.%2520When%2520trained%2520on%2520Qwen2.5-VL%2520and%2520MiMo-VL%252C%2520VisPlay%2520achieves%2520consistent%2520improvements%2520in%2520visual%2520reasoning%252C%2520compositional%2520generalization%252C%2520and%2520hallucination%2520reduction%2520across%2520eight%2520benchmarks%252C%2520including%2520MM-Vet%2520and%2520MMMU%252C%2520demonstrating%2520a%2520scalable%2520path%2520toward%2520self-evolving%2520multimodal%2520intelligence.%2520The%2520project%2520page%2520is%2520available%2520at%2520https%253A//bruno686.github.io/VisPlay/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15661v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisPlay%3A%20Self-Evolving%20Vision-Language%20Models%20from%20Images&entry.906535625=Yicheng%20He%20and%20Chengsong%20Huang%20and%20Zongxia%20Li%20and%20Jiaxin%20Huang%20and%20Yonghui%20Yang&entry.1292438233=Reinforcement%20learning%20%28RL%29%20provides%20a%20principled%20framework%20for%20improving%20Vision-Language%20Models%20%28VLMs%29%20on%20complex%20reasoning%20tasks.%20However%2C%20existing%20RL%20approaches%20often%20rely%20on%20human-annotated%20labels%20or%20task-specific%20heuristics%20to%20define%20verifiable%20rewards%2C%20both%20of%20which%20are%20costly%20and%20difficult%20to%20scale.%20We%20introduce%20VisPlay%2C%20a%20self-evolving%20RL%20framework%20that%20enables%20VLMs%20to%20autonomously%20improve%20their%20reasoning%20abilities%20using%20large%20amounts%20of%20unlabeled%20image%20data.%20Starting%20from%20a%20single%20base%20VLM%2C%20VisPlay%20assigns%20the%20model%20into%20two%20interacting%20roles%3A%20an%20Image-Conditioned%20Questioner%20that%20formulates%20challenging%20yet%20answerable%20visual%20questions%2C%20and%20a%20Multimodal%20Reasoner%20that%20generates%20silver%20responses.%20These%20roles%20are%20jointly%20trained%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20which%20incorporates%20diversity%20and%20difficulty%20rewards%20to%20balance%20the%20complexity%20of%20generated%20questions%20with%20the%20quality%20of%20the%20silver%20answers.%20VisPlay%20scales%20efficiently%20across%20two%20model%20families.%20When%20trained%20on%20Qwen2.5-VL%20and%20MiMo-VL%2C%20VisPlay%20achieves%20consistent%20improvements%20in%20visual%20reasoning%2C%20compositional%20generalization%2C%20and%20hallucination%20reduction%20across%20eight%20benchmarks%2C%20including%20MM-Vet%20and%20MMMU%2C%20demonstrating%20a%20scalable%20path%20toward%20self-evolving%20multimodal%20intelligence.%20The%20project%20page%20is%20available%20at%20https%3A//bruno686.github.io/VisPlay/&entry.1838667208=http%3A//arxiv.org/abs/2511.15661v2&entry.124074799=Read"},
{"title": "vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs", "author": "Minye Shao and Sihan Guo and Xinrun Li and Xingyu Miao and Haoran Duan and Yang Long", "abstract": "Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.", "link": "http://arxiv.org/abs/2511.09540v3", "date": "2025-11-20", "relevancy": 2.8776, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20vMFCoOp%3A%20Towards%20Equilibrium%20on%20a%20Unified%20Hyperspherical%20Manifold%20for%20Prompting%20Biomedical%20VLMs&body=Title%3A%20vMFCoOp%3A%20Towards%20Equilibrium%20on%20a%20Unified%20Hyperspherical%20Manifold%20for%20Prompting%20Biomedical%20VLMs%0AAuthor%3A%20Minye%20Shao%20and%20Sihan%20Guo%20and%20Xinrun%20Li%20and%20Xingyu%20Miao%20and%20Haoran%20Duan%20and%20Yang%20Long%0AAbstract%3A%20Recent%20advances%20in%20context%20optimization%20%28CoOp%29%20guided%20by%20large%20language%20model%20%28LLM%29-distilled%20medical%20semantic%20priors%20offer%20a%20scalable%20alternative%20to%20manual%20prompt%20engineering%20and%20full%20fine-tuning%20for%20adapting%20biomedical%20CLIP-based%20vision-language%20models%20%28VLMs%29.%20However%2C%20prompt%20learning%20in%20this%20context%20is%20challenged%20by%20semantic%20misalignment%20between%20LLMs%20and%20CLIP%20variants%20due%20to%20divergent%20training%20corpora%20and%20model%20architectures%3B%20it%20further%20lacks%20scalability%20across%20continuously%20evolving%20families%20of%20foundation%20models.%20More%20critically%2C%20pairwise%20multimodal%20alignment%20via%20conventional%20Euclidean-space%20optimization%20lacks%20the%20capacity%20to%20model%20unified%20representations%20or%20apply%20localized%20geometric%20constraints%2C%20which%20tends%20to%20amplify%20modality%20gaps%20in%20complex%20biomedical%20imaging%20and%20destabilize%20few-shot%20adaptation.%20In%20this%20work%2C%20we%20propose%20vMFCoOp%2C%20a%20framework%20that%20inversely%20estimates%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20a%20shared%20Hyperspherical%20Manifold%2C%20aligning%20semantic%20biases%20between%20arbitrary%20LLMs%20and%20CLIP%20backbones%20via%20Unified%20Semantic%20Anchors%20to%20achieve%20robust%20biomedical%20prompting%20and%20superior%20few-shot%20classification.%20Grounded%20in%20three%20complementary%20constraints%2C%20vMFCoOp%20demonstrates%20consistent%20improvements%20across%2014%20medical%20datasets%2C%2012%20medical%20imaging%20modalities%2C%20and%2013%20anatomical%20regions%2C%20outperforming%20state-of-the-art%20methods%20in%20accuracy%2C%20generalization%2C%20and%20clinical%20applicability.%20This%20work%20aims%20to%20continuously%20expand%20to%20encompass%20more%20downstream%20applications%2C%20and%20the%20corresponding%20resources%20are%20intended%20to%20be%20shared%20through%20https%3A//github.com/VinyehShaw/UniEqui.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09540v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DvMFCoOp%253A%2520Towards%2520Equilibrium%2520on%2520a%2520Unified%2520Hyperspherical%2520Manifold%2520for%2520Prompting%2520Biomedical%2520VLMs%26entry.906535625%3DMinye%2520Shao%2520and%2520Sihan%2520Guo%2520and%2520Xinrun%2520Li%2520and%2520Xingyu%2520Miao%2520and%2520Haoran%2520Duan%2520and%2520Yang%2520Long%26entry.1292438233%3DRecent%2520advances%2520in%2520context%2520optimization%2520%2528CoOp%2529%2520guided%2520by%2520large%2520language%2520model%2520%2528LLM%2529-distilled%2520medical%2520semantic%2520priors%2520offer%2520a%2520scalable%2520alternative%2520to%2520manual%2520prompt%2520engineering%2520and%2520full%2520fine-tuning%2520for%2520adapting%2520biomedical%2520CLIP-based%2520vision-language%2520models%2520%2528VLMs%2529.%2520However%252C%2520prompt%2520learning%2520in%2520this%2520context%2520is%2520challenged%2520by%2520semantic%2520misalignment%2520between%2520LLMs%2520and%2520CLIP%2520variants%2520due%2520to%2520divergent%2520training%2520corpora%2520and%2520model%2520architectures%253B%2520it%2520further%2520lacks%2520scalability%2520across%2520continuously%2520evolving%2520families%2520of%2520foundation%2520models.%2520More%2520critically%252C%2520pairwise%2520multimodal%2520alignment%2520via%2520conventional%2520Euclidean-space%2520optimization%2520lacks%2520the%2520capacity%2520to%2520model%2520unified%2520representations%2520or%2520apply%2520localized%2520geometric%2520constraints%252C%2520which%2520tends%2520to%2520amplify%2520modality%2520gaps%2520in%2520complex%2520biomedical%2520imaging%2520and%2520destabilize%2520few-shot%2520adaptation.%2520In%2520this%2520work%252C%2520we%2520propose%2520vMFCoOp%252C%2520a%2520framework%2520that%2520inversely%2520estimates%2520von%2520Mises-Fisher%2520%2528vMF%2529%2520distributions%2520on%2520a%2520shared%2520Hyperspherical%2520Manifold%252C%2520aligning%2520semantic%2520biases%2520between%2520arbitrary%2520LLMs%2520and%2520CLIP%2520backbones%2520via%2520Unified%2520Semantic%2520Anchors%2520to%2520achieve%2520robust%2520biomedical%2520prompting%2520and%2520superior%2520few-shot%2520classification.%2520Grounded%2520in%2520three%2520complementary%2520constraints%252C%2520vMFCoOp%2520demonstrates%2520consistent%2520improvements%2520across%252014%2520medical%2520datasets%252C%252012%2520medical%2520imaging%2520modalities%252C%2520and%252013%2520anatomical%2520regions%252C%2520outperforming%2520state-of-the-art%2520methods%2520in%2520accuracy%252C%2520generalization%252C%2520and%2520clinical%2520applicability.%2520This%2520work%2520aims%2520to%2520continuously%2520expand%2520to%2520encompass%2520more%2520downstream%2520applications%252C%2520and%2520the%2520corresponding%2520resources%2520are%2520intended%2520to%2520be%2520shared%2520through%2520https%253A//github.com/VinyehShaw/UniEqui.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09540v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=vMFCoOp%3A%20Towards%20Equilibrium%20on%20a%20Unified%20Hyperspherical%20Manifold%20for%20Prompting%20Biomedical%20VLMs&entry.906535625=Minye%20Shao%20and%20Sihan%20Guo%20and%20Xinrun%20Li%20and%20Xingyu%20Miao%20and%20Haoran%20Duan%20and%20Yang%20Long&entry.1292438233=Recent%20advances%20in%20context%20optimization%20%28CoOp%29%20guided%20by%20large%20language%20model%20%28LLM%29-distilled%20medical%20semantic%20priors%20offer%20a%20scalable%20alternative%20to%20manual%20prompt%20engineering%20and%20full%20fine-tuning%20for%20adapting%20biomedical%20CLIP-based%20vision-language%20models%20%28VLMs%29.%20However%2C%20prompt%20learning%20in%20this%20context%20is%20challenged%20by%20semantic%20misalignment%20between%20LLMs%20and%20CLIP%20variants%20due%20to%20divergent%20training%20corpora%20and%20model%20architectures%3B%20it%20further%20lacks%20scalability%20across%20continuously%20evolving%20families%20of%20foundation%20models.%20More%20critically%2C%20pairwise%20multimodal%20alignment%20via%20conventional%20Euclidean-space%20optimization%20lacks%20the%20capacity%20to%20model%20unified%20representations%20or%20apply%20localized%20geometric%20constraints%2C%20which%20tends%20to%20amplify%20modality%20gaps%20in%20complex%20biomedical%20imaging%20and%20destabilize%20few-shot%20adaptation.%20In%20this%20work%2C%20we%20propose%20vMFCoOp%2C%20a%20framework%20that%20inversely%20estimates%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20a%20shared%20Hyperspherical%20Manifold%2C%20aligning%20semantic%20biases%20between%20arbitrary%20LLMs%20and%20CLIP%20backbones%20via%20Unified%20Semantic%20Anchors%20to%20achieve%20robust%20biomedical%20prompting%20and%20superior%20few-shot%20classification.%20Grounded%20in%20three%20complementary%20constraints%2C%20vMFCoOp%20demonstrates%20consistent%20improvements%20across%2014%20medical%20datasets%2C%2012%20medical%20imaging%20modalities%2C%20and%2013%20anatomical%20regions%2C%20outperforming%20state-of-the-art%20methods%20in%20accuracy%2C%20generalization%2C%20and%20clinical%20applicability.%20This%20work%20aims%20to%20continuously%20expand%20to%20encompass%20more%20downstream%20applications%2C%20and%20the%20corresponding%20resources%20are%20intended%20to%20be%20shared%20through%20https%3A//github.com/VinyehShaw/UniEqui.&entry.1838667208=http%3A//arxiv.org/abs/2511.09540v3&entry.124074799=Read"},
{"title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding", "author": "Boshen Xu and Zihan Xiao and Jiaze Li and Jianzhong Ju and Zhenbo Luo and Jian Luan and Qin Jin", "abstract": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.", "link": "http://arxiv.org/abs/2511.16595v1", "date": "2025-11-20", "relevancy": 2.8734, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeViper%3A%20A%20Hybrid%20Mamba-Transformer%20Vision-Language%20Model%20for%20Efficient%20Long%20Video%20Understanding&body=Title%3A%20TimeViper%3A%20A%20Hybrid%20Mamba-Transformer%20Vision-Language%20Model%20for%20Efficient%20Long%20Video%20Understanding%0AAuthor%3A%20Boshen%20Xu%20and%20Zihan%20Xiao%20and%20Jiaze%20Li%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan%20and%20Qin%20Jin%0AAbstract%3A%20We%20introduce%20TimeViper%2C%20a%20hybrid%20vision-language%20model%20designed%20to%20tackle%20challenges%20of%20long%20video%20understanding.%20Processing%20long%20videos%20demands%20both%20an%20efficient%20model%20architecture%20and%20an%20effective%20mechanism%20for%20handling%20extended%20temporal%20contexts.%20To%20this%20end%2C%20TimeViper%20adopts%20a%20hybrid%20Mamba-Transformer%20backbone%20that%20combines%20the%20efficiency%20of%20state-space%20models%20with%20the%20expressivity%20of%20attention%20mechanisms.%20Through%20this%20hybrid%20design%2C%20we%20reveal%20the%20vision-to-text%20information%20aggregation%20phenomenon%2C%20where%20information%20progressively%20flows%20from%20vision%20tokens%20to%20text%20tokens%20across%20increasing%20LLM%20depth%2C%20resulting%20in%20severe%20vision%20token%20redundancy.%20Motivated%20by%20this%20observation%2C%20we%20propose%20TransV%2C%20a%20token%20information%20transfer%20module%20that%20transfers%20and%20compresses%20vision%20tokens%20into%20instruction%20tokens%20while%20maintaining%20multimodal%20understanding%20capabilities.%20This%20design%20enables%20TimeViper%20to%20process%20hour-long%20videos%20exceeding%2010%2C000%20frames.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20TimeViper%20competes%20with%20state-of-the-art%20models%20while%20extending%20frame%20numbers.%20We%20further%20analyze%20attention%20behaviors%20of%20both%20Mamba%20and%20Transformer%20layers%2C%20offering%20new%20insights%20into%20hybrid%20model%20interpretability.%20This%20work%20represents%20an%20initial%20step%20towards%20developing%2C%20interpreting%2C%20and%20compressing%20hybrid%20Mamba-Transformer%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeViper%253A%2520A%2520Hybrid%2520Mamba-Transformer%2520Vision-Language%2520Model%2520for%2520Efficient%2520Long%2520Video%2520Understanding%26entry.906535625%3DBoshen%2520Xu%2520and%2520Zihan%2520Xiao%2520and%2520Jiaze%2520Li%2520and%2520Jianzhong%2520Ju%2520and%2520Zhenbo%2520Luo%2520and%2520Jian%2520Luan%2520and%2520Qin%2520Jin%26entry.1292438233%3DWe%2520introduce%2520TimeViper%252C%2520a%2520hybrid%2520vision-language%2520model%2520designed%2520to%2520tackle%2520challenges%2520of%2520long%2520video%2520understanding.%2520Processing%2520long%2520videos%2520demands%2520both%2520an%2520efficient%2520model%2520architecture%2520and%2520an%2520effective%2520mechanism%2520for%2520handling%2520extended%2520temporal%2520contexts.%2520To%2520this%2520end%252C%2520TimeViper%2520adopts%2520a%2520hybrid%2520Mamba-Transformer%2520backbone%2520that%2520combines%2520the%2520efficiency%2520of%2520state-space%2520models%2520with%2520the%2520expressivity%2520of%2520attention%2520mechanisms.%2520Through%2520this%2520hybrid%2520design%252C%2520we%2520reveal%2520the%2520vision-to-text%2520information%2520aggregation%2520phenomenon%252C%2520where%2520information%2520progressively%2520flows%2520from%2520vision%2520tokens%2520to%2520text%2520tokens%2520across%2520increasing%2520LLM%2520depth%252C%2520resulting%2520in%2520severe%2520vision%2520token%2520redundancy.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520TransV%252C%2520a%2520token%2520information%2520transfer%2520module%2520that%2520transfers%2520and%2520compresses%2520vision%2520tokens%2520into%2520instruction%2520tokens%2520while%2520maintaining%2520multimodal%2520understanding%2520capabilities.%2520This%2520design%2520enables%2520TimeViper%2520to%2520process%2520hour-long%2520videos%2520exceeding%252010%252C000%2520frames.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520TimeViper%2520competes%2520with%2520state-of-the-art%2520models%2520while%2520extending%2520frame%2520numbers.%2520We%2520further%2520analyze%2520attention%2520behaviors%2520of%2520both%2520Mamba%2520and%2520Transformer%2520layers%252C%2520offering%2520new%2520insights%2520into%2520hybrid%2520model%2520interpretability.%2520This%2520work%2520represents%2520an%2520initial%2520step%2520towards%2520developing%252C%2520interpreting%252C%2520and%2520compressing%2520hybrid%2520Mamba-Transformer%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeViper%3A%20A%20Hybrid%20Mamba-Transformer%20Vision-Language%20Model%20for%20Efficient%20Long%20Video%20Understanding&entry.906535625=Boshen%20Xu%20and%20Zihan%20Xiao%20and%20Jiaze%20Li%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan%20and%20Qin%20Jin&entry.1292438233=We%20introduce%20TimeViper%2C%20a%20hybrid%20vision-language%20model%20designed%20to%20tackle%20challenges%20of%20long%20video%20understanding.%20Processing%20long%20videos%20demands%20both%20an%20efficient%20model%20architecture%20and%20an%20effective%20mechanism%20for%20handling%20extended%20temporal%20contexts.%20To%20this%20end%2C%20TimeViper%20adopts%20a%20hybrid%20Mamba-Transformer%20backbone%20that%20combines%20the%20efficiency%20of%20state-space%20models%20with%20the%20expressivity%20of%20attention%20mechanisms.%20Through%20this%20hybrid%20design%2C%20we%20reveal%20the%20vision-to-text%20information%20aggregation%20phenomenon%2C%20where%20information%20progressively%20flows%20from%20vision%20tokens%20to%20text%20tokens%20across%20increasing%20LLM%20depth%2C%20resulting%20in%20severe%20vision%20token%20redundancy.%20Motivated%20by%20this%20observation%2C%20we%20propose%20TransV%2C%20a%20token%20information%20transfer%20module%20that%20transfers%20and%20compresses%20vision%20tokens%20into%20instruction%20tokens%20while%20maintaining%20multimodal%20understanding%20capabilities.%20This%20design%20enables%20TimeViper%20to%20process%20hour-long%20videos%20exceeding%2010%2C000%20frames.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20TimeViper%20competes%20with%20state-of-the-art%20models%20while%20extending%20frame%20numbers.%20We%20further%20analyze%20attention%20behaviors%20of%20both%20Mamba%20and%20Transformer%20layers%2C%20offering%20new%20insights%20into%20hybrid%20model%20interpretability.%20This%20work%20represents%20an%20initial%20step%20towards%20developing%2C%20interpreting%2C%20and%20compressing%20hybrid%20Mamba-Transformer%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2511.16595v1&entry.124074799=Read"},
{"title": "Multi-Order Matching Network for Alignment-Free Depth Super-Resolution", "author": "Zhengxue Wang and Zhiqiang Yan and Yuan Wu and Guangwei Gao and Xiang Li and Jian Yang", "abstract": "Recent guided depth super-resolution methods are premised on the assumption of strictly spatial alignment between depth and RGB, achieving high-quality depth reconstruction. However, in real-world scenarios, the acquisition of strictly aligned RGB-D is hindered by inherent hardware limitations (e.g., physically separate RGB-D sensors) and unavoidable calibration drift induced by mechanical vibrations or temperature variations. Consequently, existing approaches often suffer inevitable performance degradation when applied to misaligned real-world scenes. In this paper, we propose the Multi-Order Matching Network (MOMNet), a novel alignment-free framework that adaptively retrieves and selects the most relevant information from misaligned RGB. Specifically, our method begins with a multi-order matching mechanism, which jointly performs zero-order, first-order, and second-order matching to comprehensively identify RGB information consistent with depth across multi-order feature spaces. To effectively integrate the retrieved RGB and depth, we further introduce a multi-order aggregation composed of multiple structure detectors. This strategy uses multi-order priors as prompts to facilitate the selective feature transfer from RGB to depth. Extensive experiments demonstrate that MOMNet achieves state-of-the-art performance and exhibits outstanding robustness.", "link": "http://arxiv.org/abs/2511.16361v1", "date": "2025-11-20", "relevancy": 2.8687, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6202}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5654}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Order%20Matching%20Network%20for%20Alignment-Free%20Depth%20Super-Resolution&body=Title%3A%20Multi-Order%20Matching%20Network%20for%20Alignment-Free%20Depth%20Super-Resolution%0AAuthor%3A%20Zhengxue%20Wang%20and%20Zhiqiang%20Yan%20and%20Yuan%20Wu%20and%20Guangwei%20Gao%20and%20Xiang%20Li%20and%20Jian%20Yang%0AAbstract%3A%20Recent%20guided%20depth%20super-resolution%20methods%20are%20premised%20on%20the%20assumption%20of%20strictly%20spatial%20alignment%20between%20depth%20and%20RGB%2C%20achieving%20high-quality%20depth%20reconstruction.%20However%2C%20in%20real-world%20scenarios%2C%20the%20acquisition%20of%20strictly%20aligned%20RGB-D%20is%20hindered%20by%20inherent%20hardware%20limitations%20%28e.g.%2C%20physically%20separate%20RGB-D%20sensors%29%20and%20unavoidable%20calibration%20drift%20induced%20by%20mechanical%20vibrations%20or%20temperature%20variations.%20Consequently%2C%20existing%20approaches%20often%20suffer%20inevitable%20performance%20degradation%20when%20applied%20to%20misaligned%20real-world%20scenes.%20In%20this%20paper%2C%20we%20propose%20the%20Multi-Order%20Matching%20Network%20%28MOMNet%29%2C%20a%20novel%20alignment-free%20framework%20that%20adaptively%20retrieves%20and%20selects%20the%20most%20relevant%20information%20from%20misaligned%20RGB.%20Specifically%2C%20our%20method%20begins%20with%20a%20multi-order%20matching%20mechanism%2C%20which%20jointly%20performs%20zero-order%2C%20first-order%2C%20and%20second-order%20matching%20to%20comprehensively%20identify%20RGB%20information%20consistent%20with%20depth%20across%20multi-order%20feature%20spaces.%20To%20effectively%20integrate%20the%20retrieved%20RGB%20and%20depth%2C%20we%20further%20introduce%20a%20multi-order%20aggregation%20composed%20of%20multiple%20structure%20detectors.%20This%20strategy%20uses%20multi-order%20priors%20as%20prompts%20to%20facilitate%20the%20selective%20feature%20transfer%20from%20RGB%20to%20depth.%20Extensive%20experiments%20demonstrate%20that%20MOMNet%20achieves%20state-of-the-art%20performance%20and%20exhibits%20outstanding%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Order%2520Matching%2520Network%2520for%2520Alignment-Free%2520Depth%2520Super-Resolution%26entry.906535625%3DZhengxue%2520Wang%2520and%2520Zhiqiang%2520Yan%2520and%2520Yuan%2520Wu%2520and%2520Guangwei%2520Gao%2520and%2520Xiang%2520Li%2520and%2520Jian%2520Yang%26entry.1292438233%3DRecent%2520guided%2520depth%2520super-resolution%2520methods%2520are%2520premised%2520on%2520the%2520assumption%2520of%2520strictly%2520spatial%2520alignment%2520between%2520depth%2520and%2520RGB%252C%2520achieving%2520high-quality%2520depth%2520reconstruction.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520the%2520acquisition%2520of%2520strictly%2520aligned%2520RGB-D%2520is%2520hindered%2520by%2520inherent%2520hardware%2520limitations%2520%2528e.g.%252C%2520physically%2520separate%2520RGB-D%2520sensors%2529%2520and%2520unavoidable%2520calibration%2520drift%2520induced%2520by%2520mechanical%2520vibrations%2520or%2520temperature%2520variations.%2520Consequently%252C%2520existing%2520approaches%2520often%2520suffer%2520inevitable%2520performance%2520degradation%2520when%2520applied%2520to%2520misaligned%2520real-world%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Multi-Order%2520Matching%2520Network%2520%2528MOMNet%2529%252C%2520a%2520novel%2520alignment-free%2520framework%2520that%2520adaptively%2520retrieves%2520and%2520selects%2520the%2520most%2520relevant%2520information%2520from%2520misaligned%2520RGB.%2520Specifically%252C%2520our%2520method%2520begins%2520with%2520a%2520multi-order%2520matching%2520mechanism%252C%2520which%2520jointly%2520performs%2520zero-order%252C%2520first-order%252C%2520and%2520second-order%2520matching%2520to%2520comprehensively%2520identify%2520RGB%2520information%2520consistent%2520with%2520depth%2520across%2520multi-order%2520feature%2520spaces.%2520To%2520effectively%2520integrate%2520the%2520retrieved%2520RGB%2520and%2520depth%252C%2520we%2520further%2520introduce%2520a%2520multi-order%2520aggregation%2520composed%2520of%2520multiple%2520structure%2520detectors.%2520This%2520strategy%2520uses%2520multi-order%2520priors%2520as%2520prompts%2520to%2520facilitate%2520the%2520selective%2520feature%2520transfer%2520from%2520RGB%2520to%2520depth.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MOMNet%2520achieves%2520state-of-the-art%2520performance%2520and%2520exhibits%2520outstanding%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Order%20Matching%20Network%20for%20Alignment-Free%20Depth%20Super-Resolution&entry.906535625=Zhengxue%20Wang%20and%20Zhiqiang%20Yan%20and%20Yuan%20Wu%20and%20Guangwei%20Gao%20and%20Xiang%20Li%20and%20Jian%20Yang&entry.1292438233=Recent%20guided%20depth%20super-resolution%20methods%20are%20premised%20on%20the%20assumption%20of%20strictly%20spatial%20alignment%20between%20depth%20and%20RGB%2C%20achieving%20high-quality%20depth%20reconstruction.%20However%2C%20in%20real-world%20scenarios%2C%20the%20acquisition%20of%20strictly%20aligned%20RGB-D%20is%20hindered%20by%20inherent%20hardware%20limitations%20%28e.g.%2C%20physically%20separate%20RGB-D%20sensors%29%20and%20unavoidable%20calibration%20drift%20induced%20by%20mechanical%20vibrations%20or%20temperature%20variations.%20Consequently%2C%20existing%20approaches%20often%20suffer%20inevitable%20performance%20degradation%20when%20applied%20to%20misaligned%20real-world%20scenes.%20In%20this%20paper%2C%20we%20propose%20the%20Multi-Order%20Matching%20Network%20%28MOMNet%29%2C%20a%20novel%20alignment-free%20framework%20that%20adaptively%20retrieves%20and%20selects%20the%20most%20relevant%20information%20from%20misaligned%20RGB.%20Specifically%2C%20our%20method%20begins%20with%20a%20multi-order%20matching%20mechanism%2C%20which%20jointly%20performs%20zero-order%2C%20first-order%2C%20and%20second-order%20matching%20to%20comprehensively%20identify%20RGB%20information%20consistent%20with%20depth%20across%20multi-order%20feature%20spaces.%20To%20effectively%20integrate%20the%20retrieved%20RGB%20and%20depth%2C%20we%20further%20introduce%20a%20multi-order%20aggregation%20composed%20of%20multiple%20structure%20detectors.%20This%20strategy%20uses%20multi-order%20priors%20as%20prompts%20to%20facilitate%20the%20selective%20feature%20transfer%20from%20RGB%20to%20depth.%20Extensive%20experiments%20demonstrate%20that%20MOMNet%20achieves%20state-of-the-art%20performance%20and%20exhibits%20outstanding%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2511.16361v1&entry.124074799=Read"},
{"title": "RoMa v2: Harder Better Faster Denser Feature Matching", "author": "Johan Edstedt and David Nordstr\u00f6m and Yushan Zhang and Georg B\u00f6kman and Jonathan Astermark and Viktor Larsson and Anders Heyden and Fredrik Kahl and M\u00e5rten Wadenb\u00e4ck and Michael Felsberg", "abstract": "Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2", "link": "http://arxiv.org/abs/2511.15706v2", "date": "2025-11-20", "relevancy": 2.8206, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6002}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoMa%20v2%3A%20Harder%20Better%20Faster%20Denser%20Feature%20Matching&body=Title%3A%20RoMa%20v2%3A%20Harder%20Better%20Faster%20Denser%20Feature%20Matching%0AAuthor%3A%20Johan%20Edstedt%20and%20David%20Nordstr%C3%B6m%20and%20Yushan%20Zhang%20and%20Georg%20B%C3%B6kman%20and%20Jonathan%20Astermark%20and%20Viktor%20Larsson%20and%20Anders%20Heyden%20and%20Fredrik%20Kahl%20and%20M%C3%A5rten%20Wadenb%C3%A4ck%20and%20Michael%20Felsberg%0AAbstract%3A%20Dense%20feature%20matching%20aims%20to%20estimate%20all%20correspondences%20between%20two%20images%20of%20a%203D%20scene%20and%20has%20recently%20been%20established%20as%20the%20gold-standard%20due%20to%20its%20high%20accuracy%20and%20robustness.%20However%2C%20existing%20dense%20matchers%20still%20fail%20or%20perform%20poorly%20for%20many%20hard%20real-world%20scenarios%2C%20and%20high-precision%20models%20are%20often%20slow%2C%20limiting%20their%20applicability.%20In%20this%20paper%2C%20we%20attack%20these%20weaknesses%20on%20a%20wide%20front%20through%20a%20series%20of%20systematic%20improvements%20that%20together%20yield%20a%20significantly%20better%20model.%20In%20particular%2C%20we%20construct%20a%20novel%20matching%20architecture%20and%20loss%2C%20which%2C%20combined%20with%20a%20curated%20diverse%20training%20distribution%2C%20enables%20our%20model%20to%20solve%20many%20complex%20matching%20tasks.%20We%20further%20make%20training%20faster%20through%20a%20decoupled%20two-stage%20matching-then-refinement%20pipeline%2C%20and%20at%20the%20same%20time%2C%20significantly%20reduce%20refinement%20memory%20usage%20through%20a%20custom%20CUDA%20kernel.%20Finally%2C%20we%20leverage%20the%20recent%20DINOv3%20foundation%20model%20along%20with%20multiple%20other%20insights%20to%20make%20the%20model%20more%20robust%20and%20unbiased.%20In%20our%20extensive%20set%20of%20experiments%20we%20show%20that%20the%20resulting%20novel%20matcher%20sets%20a%20new%20state-of-the-art%2C%20being%20significantly%20more%20accurate%20than%20its%20predecessors.%20Code%20is%20available%20at%20https%3A//github.com/Parskatt/romav2%0ALink%3A%20http%3A//arxiv.org/abs/2511.15706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoMa%2520v2%253A%2520Harder%2520Better%2520Faster%2520Denser%2520Feature%2520Matching%26entry.906535625%3DJohan%2520Edstedt%2520and%2520David%2520Nordstr%25C3%25B6m%2520and%2520Yushan%2520Zhang%2520and%2520Georg%2520B%25C3%25B6kman%2520and%2520Jonathan%2520Astermark%2520and%2520Viktor%2520Larsson%2520and%2520Anders%2520Heyden%2520and%2520Fredrik%2520Kahl%2520and%2520M%25C3%25A5rten%2520Wadenb%25C3%25A4ck%2520and%2520Michael%2520Felsberg%26entry.1292438233%3DDense%2520feature%2520matching%2520aims%2520to%2520estimate%2520all%2520correspondences%2520between%2520two%2520images%2520of%2520a%25203D%2520scene%2520and%2520has%2520recently%2520been%2520established%2520as%2520the%2520gold-standard%2520due%2520to%2520its%2520high%2520accuracy%2520and%2520robustness.%2520However%252C%2520existing%2520dense%2520matchers%2520still%2520fail%2520or%2520perform%2520poorly%2520for%2520many%2520hard%2520real-world%2520scenarios%252C%2520and%2520high-precision%2520models%2520are%2520often%2520slow%252C%2520limiting%2520their%2520applicability.%2520In%2520this%2520paper%252C%2520we%2520attack%2520these%2520weaknesses%2520on%2520a%2520wide%2520front%2520through%2520a%2520series%2520of%2520systematic%2520improvements%2520that%2520together%2520yield%2520a%2520significantly%2520better%2520model.%2520In%2520particular%252C%2520we%2520construct%2520a%2520novel%2520matching%2520architecture%2520and%2520loss%252C%2520which%252C%2520combined%2520with%2520a%2520curated%2520diverse%2520training%2520distribution%252C%2520enables%2520our%2520model%2520to%2520solve%2520many%2520complex%2520matching%2520tasks.%2520We%2520further%2520make%2520training%2520faster%2520through%2520a%2520decoupled%2520two-stage%2520matching-then-refinement%2520pipeline%252C%2520and%2520at%2520the%2520same%2520time%252C%2520significantly%2520reduce%2520refinement%2520memory%2520usage%2520through%2520a%2520custom%2520CUDA%2520kernel.%2520Finally%252C%2520we%2520leverage%2520the%2520recent%2520DINOv3%2520foundation%2520model%2520along%2520with%2520multiple%2520other%2520insights%2520to%2520make%2520the%2520model%2520more%2520robust%2520and%2520unbiased.%2520In%2520our%2520extensive%2520set%2520of%2520experiments%2520we%2520show%2520that%2520the%2520resulting%2520novel%2520matcher%2520sets%2520a%2520new%2520state-of-the-art%252C%2520being%2520significantly%2520more%2520accurate%2520than%2520its%2520predecessors.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Parskatt/romav2%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoMa%20v2%3A%20Harder%20Better%20Faster%20Denser%20Feature%20Matching&entry.906535625=Johan%20Edstedt%20and%20David%20Nordstr%C3%B6m%20and%20Yushan%20Zhang%20and%20Georg%20B%C3%B6kman%20and%20Jonathan%20Astermark%20and%20Viktor%20Larsson%20and%20Anders%20Heyden%20and%20Fredrik%20Kahl%20and%20M%C3%A5rten%20Wadenb%C3%A4ck%20and%20Michael%20Felsberg&entry.1292438233=Dense%20feature%20matching%20aims%20to%20estimate%20all%20correspondences%20between%20two%20images%20of%20a%203D%20scene%20and%20has%20recently%20been%20established%20as%20the%20gold-standard%20due%20to%20its%20high%20accuracy%20and%20robustness.%20However%2C%20existing%20dense%20matchers%20still%20fail%20or%20perform%20poorly%20for%20many%20hard%20real-world%20scenarios%2C%20and%20high-precision%20models%20are%20often%20slow%2C%20limiting%20their%20applicability.%20In%20this%20paper%2C%20we%20attack%20these%20weaknesses%20on%20a%20wide%20front%20through%20a%20series%20of%20systematic%20improvements%20that%20together%20yield%20a%20significantly%20better%20model.%20In%20particular%2C%20we%20construct%20a%20novel%20matching%20architecture%20and%20loss%2C%20which%2C%20combined%20with%20a%20curated%20diverse%20training%20distribution%2C%20enables%20our%20model%20to%20solve%20many%20complex%20matching%20tasks.%20We%20further%20make%20training%20faster%20through%20a%20decoupled%20two-stage%20matching-then-refinement%20pipeline%2C%20and%20at%20the%20same%20time%2C%20significantly%20reduce%20refinement%20memory%20usage%20through%20a%20custom%20CUDA%20kernel.%20Finally%2C%20we%20leverage%20the%20recent%20DINOv3%20foundation%20model%20along%20with%20multiple%20other%20insights%20to%20make%20the%20model%20more%20robust%20and%20unbiased.%20In%20our%20extensive%20set%20of%20experiments%20we%20show%20that%20the%20resulting%20novel%20matcher%20sets%20a%20new%20state-of-the-art%2C%20being%20significantly%20more%20accurate%20than%20its%20predecessors.%20Code%20is%20available%20at%20https%3A//github.com/Parskatt/romav2&entry.1838667208=http%3A//arxiv.org/abs/2511.15706v2&entry.124074799=Read"},
{"title": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models", "author": "Yang Luo and Xuanlei Zhao and Baijiong Lin and Lingting Zhu and Liyao Tang and Yuqi Liu and Ying-Cong Chen and Shengju Qian and Xin Wang and Yang You", "abstract": "Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.", "link": "http://arxiv.org/abs/2511.16668v1", "date": "2025-11-20", "relevancy": 2.8174, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-ReasonBench%3A%20Toward%20Unified%20Reasoning%20Benchmark%20Suite%20for%20Video%20Generation%20Models&body=Title%3A%20V-ReasonBench%3A%20Toward%20Unified%20Reasoning%20Benchmark%20Suite%20for%20Video%20Generation%20Models%0AAuthor%3A%20Yang%20Luo%20and%20Xuanlei%20Zhao%20and%20Baijiong%20Lin%20and%20Lingting%20Zhu%20and%20Liyao%20Tang%20and%20Yuqi%20Liu%20and%20Ying-Cong%20Chen%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Yang%20You%0AAbstract%3A%20Recent%20progress%20in%20generative%20video%20models%2C%20such%20as%20Veo-3%2C%20has%20shown%20surprising%20zero-shot%20reasoning%20abilities%2C%20creating%20a%20growing%20need%20for%20systematic%20and%20reliable%20evaluation.%20We%20introduce%20V-ReasonBench%2C%20a%20benchmark%20designed%20to%20assess%20video%20reasoning%20across%20four%20key%20dimensions%3A%20structured%20problem-solving%2C%20spatial%20cognition%2C%20pattern-based%20inference%2C%20and%20physical%20dynamics.%20The%20benchmark%20is%20built%20from%20both%20synthetic%20and%20real-world%20image%20sequences%20and%20provides%20a%20diverse%20set%20of%20answer-verifiable%20tasks%20that%20are%20reproducible%2C%20scalable%2C%20and%20unambiguous.%20Evaluations%20of%20six%20state-of-the-art%20video%20models%20reveal%20clear%20dimension-wise%20differences%2C%20with%20strong%20variation%20in%20structured%2C%20spatial%2C%20pattern-based%2C%20and%20physical%20reasoning.%20We%20further%20compare%20video%20models%20with%20strong%20image%20models%2C%20analyze%20common%20hallucination%20behaviors%2C%20and%20study%20how%20video%20duration%20affects%20Chain-of-Frames%20reasoning.%20Overall%2C%20V-ReasonBench%20offers%20a%20unified%20and%20reproducible%20framework%20for%20measuring%20video%20reasoning%20and%20aims%20to%20support%20the%20development%20of%20models%20with%20more%20reliable%2C%20human-aligned%20reasoning%20skills.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-ReasonBench%253A%2520Toward%2520Unified%2520Reasoning%2520Benchmark%2520Suite%2520for%2520Video%2520Generation%2520Models%26entry.906535625%3DYang%2520Luo%2520and%2520Xuanlei%2520Zhao%2520and%2520Baijiong%2520Lin%2520and%2520Lingting%2520Zhu%2520and%2520Liyao%2520Tang%2520and%2520Yuqi%2520Liu%2520and%2520Ying-Cong%2520Chen%2520and%2520Shengju%2520Qian%2520and%2520Xin%2520Wang%2520and%2520Yang%2520You%26entry.1292438233%3DRecent%2520progress%2520in%2520generative%2520video%2520models%252C%2520such%2520as%2520Veo-3%252C%2520has%2520shown%2520surprising%2520zero-shot%2520reasoning%2520abilities%252C%2520creating%2520a%2520growing%2520need%2520for%2520systematic%2520and%2520reliable%2520evaluation.%2520We%2520introduce%2520V-ReasonBench%252C%2520a%2520benchmark%2520designed%2520to%2520assess%2520video%2520reasoning%2520across%2520four%2520key%2520dimensions%253A%2520structured%2520problem-solving%252C%2520spatial%2520cognition%252C%2520pattern-based%2520inference%252C%2520and%2520physical%2520dynamics.%2520The%2520benchmark%2520is%2520built%2520from%2520both%2520synthetic%2520and%2520real-world%2520image%2520sequences%2520and%2520provides%2520a%2520diverse%2520set%2520of%2520answer-verifiable%2520tasks%2520that%2520are%2520reproducible%252C%2520scalable%252C%2520and%2520unambiguous.%2520Evaluations%2520of%2520six%2520state-of-the-art%2520video%2520models%2520reveal%2520clear%2520dimension-wise%2520differences%252C%2520with%2520strong%2520variation%2520in%2520structured%252C%2520spatial%252C%2520pattern-based%252C%2520and%2520physical%2520reasoning.%2520We%2520further%2520compare%2520video%2520models%2520with%2520strong%2520image%2520models%252C%2520analyze%2520common%2520hallucination%2520behaviors%252C%2520and%2520study%2520how%2520video%2520duration%2520affects%2520Chain-of-Frames%2520reasoning.%2520Overall%252C%2520V-ReasonBench%2520offers%2520a%2520unified%2520and%2520reproducible%2520framework%2520for%2520measuring%2520video%2520reasoning%2520and%2520aims%2520to%2520support%2520the%2520development%2520of%2520models%2520with%2520more%2520reliable%252C%2520human-aligned%2520reasoning%2520skills.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-ReasonBench%3A%20Toward%20Unified%20Reasoning%20Benchmark%20Suite%20for%20Video%20Generation%20Models&entry.906535625=Yang%20Luo%20and%20Xuanlei%20Zhao%20and%20Baijiong%20Lin%20and%20Lingting%20Zhu%20and%20Liyao%20Tang%20and%20Yuqi%20Liu%20and%20Ying-Cong%20Chen%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Yang%20You&entry.1292438233=Recent%20progress%20in%20generative%20video%20models%2C%20such%20as%20Veo-3%2C%20has%20shown%20surprising%20zero-shot%20reasoning%20abilities%2C%20creating%20a%20growing%20need%20for%20systematic%20and%20reliable%20evaluation.%20We%20introduce%20V-ReasonBench%2C%20a%20benchmark%20designed%20to%20assess%20video%20reasoning%20across%20four%20key%20dimensions%3A%20structured%20problem-solving%2C%20spatial%20cognition%2C%20pattern-based%20inference%2C%20and%20physical%20dynamics.%20The%20benchmark%20is%20built%20from%20both%20synthetic%20and%20real-world%20image%20sequences%20and%20provides%20a%20diverse%20set%20of%20answer-verifiable%20tasks%20that%20are%20reproducible%2C%20scalable%2C%20and%20unambiguous.%20Evaluations%20of%20six%20state-of-the-art%20video%20models%20reveal%20clear%20dimension-wise%20differences%2C%20with%20strong%20variation%20in%20structured%2C%20spatial%2C%20pattern-based%2C%20and%20physical%20reasoning.%20We%20further%20compare%20video%20models%20with%20strong%20image%20models%2C%20analyze%20common%20hallucination%20behaviors%2C%20and%20study%20how%20video%20duration%20affects%20Chain-of-Frames%20reasoning.%20Overall%2C%20V-ReasonBench%20offers%20a%20unified%20and%20reproducible%20framework%20for%20measuring%20video%20reasoning%20and%20aims%20to%20support%20the%20development%20of%20models%20with%20more%20reliable%2C%20human-aligned%20reasoning%20skills.&entry.1838667208=http%3A//arxiv.org/abs/2511.16668v1&entry.124074799=Read"},
{"title": "On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation", "author": "Liyao Tang and Zhe Chen and Dacheng Tao", "abstract": "The emergence of large-scale pre-trained point cloud models has significantly advanced 3D scene understanding, but adapting these models to specific downstream tasks typically demands full fine-tuning, incurring high computational and storage costs. Parameter-efficient fine-tuning (PEFT) techniques, successful in natural language processing and 2D vision tasks, would underperform when naively applied to 3D point cloud models due to significant geometric and spatial distribution shifts. Existing PEFT methods commonly treat points as orderless tokens, neglecting important local spatial structures and global geometric contexts in 3D modeling. To bridge this gap, we introduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT module specifically designed for 3D point cloud transformers. GEM explicitly integrates fine-grained local positional encodings with a lightweight latent attention mechanism to capture comprehensive global context, thereby effectively addressing the spatial and geometric distribution mismatch. Extensive experiments demonstrate that GEM achieves performance comparable to or sometimes even exceeding full fine-tuning, while only updating 1.6% of the model's parameters, fewer than other PEFT methods. With significantly reduced training time and memory requirements, our approach thus sets a new benchmark for efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point cloud models. Code is available at https://github.com/LiyaoTang/GEM.", "link": "http://arxiv.org/abs/2505.22444v2", "date": "2025-11-20", "relevancy": 2.7944, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5727}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5675}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Geometry-Enhanced%20Parameter-Efficient%20Fine-Tuning%20for%203D%20Scene%20Segmentation&body=Title%3A%20On%20Geometry-Enhanced%20Parameter-Efficient%20Fine-Tuning%20for%203D%20Scene%20Segmentation%0AAuthor%3A%20Liyao%20Tang%20and%20Zhe%20Chen%20and%20Dacheng%20Tao%0AAbstract%3A%20The%20emergence%20of%20large-scale%20pre-trained%20point%20cloud%20models%20has%20significantly%20advanced%203D%20scene%20understanding%2C%20but%20adapting%20these%20models%20to%20specific%20downstream%20tasks%20typically%20demands%20full%20fine-tuning%2C%20incurring%20high%20computational%20and%20storage%20costs.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%2C%20successful%20in%20natural%20language%20processing%20and%202D%20vision%20tasks%2C%20would%20underperform%20when%20naively%20applied%20to%203D%20point%20cloud%20models%20due%20to%20significant%20geometric%20and%20spatial%20distribution%20shifts.%20Existing%20PEFT%20methods%20commonly%20treat%20points%20as%20orderless%20tokens%2C%20neglecting%20important%20local%20spatial%20structures%20and%20global%20geometric%20contexts%20in%203D%20modeling.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20Geometric%20Encoding%20Mixer%20%28GEM%29%2C%20a%20novel%20geometry-aware%20PEFT%20module%20specifically%20designed%20for%203D%20point%20cloud%20transformers.%20GEM%20explicitly%20integrates%20fine-grained%20local%20positional%20encodings%20with%20a%20lightweight%20latent%20attention%20mechanism%20to%20capture%20comprehensive%20global%20context%2C%20thereby%20effectively%20addressing%20the%20spatial%20and%20geometric%20distribution%20mismatch.%20Extensive%20experiments%20demonstrate%20that%20GEM%20achieves%20performance%20comparable%20to%20or%20sometimes%20even%20exceeding%20full%20fine-tuning%2C%20while%20only%20updating%201.6%25%20of%20the%20model%27s%20parameters%2C%20fewer%20than%20other%20PEFT%20methods.%20With%20significantly%20reduced%20training%20time%20and%20memory%20requirements%2C%20our%20approach%20thus%20sets%20a%20new%20benchmark%20for%20efficient%2C%20scalable%2C%20and%20geometry-aware%20fine-tuning%20of%20large-scale%203D%20point%20cloud%20models.%20Code%20is%20available%20at%20https%3A//github.com/LiyaoTang/GEM.%0ALink%3A%20http%3A//arxiv.org/abs/2505.22444v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Geometry-Enhanced%2520Parameter-Efficient%2520Fine-Tuning%2520for%25203D%2520Scene%2520Segmentation%26entry.906535625%3DLiyao%2520Tang%2520and%2520Zhe%2520Chen%2520and%2520Dacheng%2520Tao%26entry.1292438233%3DThe%2520emergence%2520of%2520large-scale%2520pre-trained%2520point%2520cloud%2520models%2520has%2520significantly%2520advanced%25203D%2520scene%2520understanding%252C%2520but%2520adapting%2520these%2520models%2520to%2520specific%2520downstream%2520tasks%2520typically%2520demands%2520full%2520fine-tuning%252C%2520incurring%2520high%2520computational%2520and%2520storage%2520costs.%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520techniques%252C%2520successful%2520in%2520natural%2520language%2520processing%2520and%25202D%2520vision%2520tasks%252C%2520would%2520underperform%2520when%2520naively%2520applied%2520to%25203D%2520point%2520cloud%2520models%2520due%2520to%2520significant%2520geometric%2520and%2520spatial%2520distribution%2520shifts.%2520Existing%2520PEFT%2520methods%2520commonly%2520treat%2520points%2520as%2520orderless%2520tokens%252C%2520neglecting%2520important%2520local%2520spatial%2520structures%2520and%2520global%2520geometric%2520contexts%2520in%25203D%2520modeling.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520the%2520Geometric%2520Encoding%2520Mixer%2520%2528GEM%2529%252C%2520a%2520novel%2520geometry-aware%2520PEFT%2520module%2520specifically%2520designed%2520for%25203D%2520point%2520cloud%2520transformers.%2520GEM%2520explicitly%2520integrates%2520fine-grained%2520local%2520positional%2520encodings%2520with%2520a%2520lightweight%2520latent%2520attention%2520mechanism%2520to%2520capture%2520comprehensive%2520global%2520context%252C%2520thereby%2520effectively%2520addressing%2520the%2520spatial%2520and%2520geometric%2520distribution%2520mismatch.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GEM%2520achieves%2520performance%2520comparable%2520to%2520or%2520sometimes%2520even%2520exceeding%2520full%2520fine-tuning%252C%2520while%2520only%2520updating%25201.6%2525%2520of%2520the%2520model%2527s%2520parameters%252C%2520fewer%2520than%2520other%2520PEFT%2520methods.%2520With%2520significantly%2520reduced%2520training%2520time%2520and%2520memory%2520requirements%252C%2520our%2520approach%2520thus%2520sets%2520a%2520new%2520benchmark%2520for%2520efficient%252C%2520scalable%252C%2520and%2520geometry-aware%2520fine-tuning%2520of%2520large-scale%25203D%2520point%2520cloud%2520models.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/LiyaoTang/GEM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22444v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Geometry-Enhanced%20Parameter-Efficient%20Fine-Tuning%20for%203D%20Scene%20Segmentation&entry.906535625=Liyao%20Tang%20and%20Zhe%20Chen%20and%20Dacheng%20Tao&entry.1292438233=The%20emergence%20of%20large-scale%20pre-trained%20point%20cloud%20models%20has%20significantly%20advanced%203D%20scene%20understanding%2C%20but%20adapting%20these%20models%20to%20specific%20downstream%20tasks%20typically%20demands%20full%20fine-tuning%2C%20incurring%20high%20computational%20and%20storage%20costs.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%2C%20successful%20in%20natural%20language%20processing%20and%202D%20vision%20tasks%2C%20would%20underperform%20when%20naively%20applied%20to%203D%20point%20cloud%20models%20due%20to%20significant%20geometric%20and%20spatial%20distribution%20shifts.%20Existing%20PEFT%20methods%20commonly%20treat%20points%20as%20orderless%20tokens%2C%20neglecting%20important%20local%20spatial%20structures%20and%20global%20geometric%20contexts%20in%203D%20modeling.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20Geometric%20Encoding%20Mixer%20%28GEM%29%2C%20a%20novel%20geometry-aware%20PEFT%20module%20specifically%20designed%20for%203D%20point%20cloud%20transformers.%20GEM%20explicitly%20integrates%20fine-grained%20local%20positional%20encodings%20with%20a%20lightweight%20latent%20attention%20mechanism%20to%20capture%20comprehensive%20global%20context%2C%20thereby%20effectively%20addressing%20the%20spatial%20and%20geometric%20distribution%20mismatch.%20Extensive%20experiments%20demonstrate%20that%20GEM%20achieves%20performance%20comparable%20to%20or%20sometimes%20even%20exceeding%20full%20fine-tuning%2C%20while%20only%20updating%201.6%25%20of%20the%20model%27s%20parameters%2C%20fewer%20than%20other%20PEFT%20methods.%20With%20significantly%20reduced%20training%20time%20and%20memory%20requirements%2C%20our%20approach%20thus%20sets%20a%20new%20benchmark%20for%20efficient%2C%20scalable%2C%20and%20geometry-aware%20fine-tuning%20of%20large-scale%203D%20point%20cloud%20models.%20Code%20is%20available%20at%20https%3A//github.com/LiyaoTang/GEM.&entry.1838667208=http%3A//arxiv.org/abs/2505.22444v2&entry.124074799=Read"},
{"title": "Self-Supervised Discriminative Feature Learning for Deep Multi-View Clustering", "author": "Jie Xu and Yazhou Ren and Huayi Tang and Zhimeng Yang and Lili Pan and Yang Yang and Xiaorong Pu and Philip S. Yu and Lifang He", "abstract": "Multi-view clustering is an important research topic due to its capability to utilize complementary information from multiple views. However, there are few methods to consider the negative impact caused by certain views with unclear clustering structures, resulting in poor multi-view clustering performance. To address this drawback, we propose self-supervised discriminative feature learning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders are applied to learn embedded features for each view independently. To leverage the multi-view complementary information, we concatenate all views' embedded features to form the global features, which can overcome the negative impact of some views' unclear clustering structures. In a self-supervised manner, pseudo-labels are obtained to build a unified target distribution to perform multi-view discriminative feature learning. During this process, global discriminative information can be mined to supervise all views to learn more discriminative features, which in turn are used to update the target distribution. Besides, this unified target distribution can make SDMVC learn consistent cluster assignments, which accomplishes the clustering consistency of multiple views while preserving their features' diversity. Experiments on various types of multi-view datasets show that SDMVC outperforms 14 competitors including classic and state-of-the-art methods. The code is available at https://github.com/SubmissionsIn/SDMVC.", "link": "http://arxiv.org/abs/2103.15069v3", "date": "2025-11-20", "relevancy": 2.7712, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.561}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5602}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Discriminative%20Feature%20Learning%20for%20Deep%20Multi-View%20Clustering&body=Title%3A%20Self-Supervised%20Discriminative%20Feature%20Learning%20for%20Deep%20Multi-View%20Clustering%0AAuthor%3A%20Jie%20Xu%20and%20Yazhou%20Ren%20and%20Huayi%20Tang%20and%20Zhimeng%20Yang%20and%20Lili%20Pan%20and%20Yang%20Yang%20and%20Xiaorong%20Pu%20and%20Philip%20S.%20Yu%20and%20Lifang%20He%0AAbstract%3A%20Multi-view%20clustering%20is%20an%20important%20research%20topic%20due%20to%20its%20capability%20to%20utilize%20complementary%20information%20from%20multiple%20views.%20However%2C%20there%20are%20few%20methods%20to%20consider%20the%20negative%20impact%20caused%20by%20certain%20views%20with%20unclear%20clustering%20structures%2C%20resulting%20in%20poor%20multi-view%20clustering%20performance.%20To%20address%20this%20drawback%2C%20we%20propose%20self-supervised%20discriminative%20feature%20learning%20for%20deep%20multi-view%20clustering%20%28SDMVC%29.%20Concretely%2C%20deep%20autoencoders%20are%20applied%20to%20learn%20embedded%20features%20for%20each%20view%20independently.%20To%20leverage%20the%20multi-view%20complementary%20information%2C%20we%20concatenate%20all%20views%27%20embedded%20features%20to%20form%20the%20global%20features%2C%20which%20can%20overcome%20the%20negative%20impact%20of%20some%20views%27%20unclear%20clustering%20structures.%20In%20a%20self-supervised%20manner%2C%20pseudo-labels%20are%20obtained%20to%20build%20a%20unified%20target%20distribution%20to%20perform%20multi-view%20discriminative%20feature%20learning.%20During%20this%20process%2C%20global%20discriminative%20information%20can%20be%20mined%20to%20supervise%20all%20views%20to%20learn%20more%20discriminative%20features%2C%20which%20in%20turn%20are%20used%20to%20update%20the%20target%20distribution.%20Besides%2C%20this%20unified%20target%20distribution%20can%20make%20SDMVC%20learn%20consistent%20cluster%20assignments%2C%20which%20accomplishes%20the%20clustering%20consistency%20of%20multiple%20views%20while%20preserving%20their%20features%27%20diversity.%20Experiments%20on%20various%20types%20of%20multi-view%20datasets%20show%20that%20SDMVC%20outperforms%2014%20competitors%20including%20classic%20and%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%20https%3A//github.com/SubmissionsIn/SDMVC.%0ALink%3A%20http%3A//arxiv.org/abs/2103.15069v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Discriminative%2520Feature%2520Learning%2520for%2520Deep%2520Multi-View%2520Clustering%26entry.906535625%3DJie%2520Xu%2520and%2520Yazhou%2520Ren%2520and%2520Huayi%2520Tang%2520and%2520Zhimeng%2520Yang%2520and%2520Lili%2520Pan%2520and%2520Yang%2520Yang%2520and%2520Xiaorong%2520Pu%2520and%2520Philip%2520S.%2520Yu%2520and%2520Lifang%2520He%26entry.1292438233%3DMulti-view%2520clustering%2520is%2520an%2520important%2520research%2520topic%2520due%2520to%2520its%2520capability%2520to%2520utilize%2520complementary%2520information%2520from%2520multiple%2520views.%2520However%252C%2520there%2520are%2520few%2520methods%2520to%2520consider%2520the%2520negative%2520impact%2520caused%2520by%2520certain%2520views%2520with%2520unclear%2520clustering%2520structures%252C%2520resulting%2520in%2520poor%2520multi-view%2520clustering%2520performance.%2520To%2520address%2520this%2520drawback%252C%2520we%2520propose%2520self-supervised%2520discriminative%2520feature%2520learning%2520for%2520deep%2520multi-view%2520clustering%2520%2528SDMVC%2529.%2520Concretely%252C%2520deep%2520autoencoders%2520are%2520applied%2520to%2520learn%2520embedded%2520features%2520for%2520each%2520view%2520independently.%2520To%2520leverage%2520the%2520multi-view%2520complementary%2520information%252C%2520we%2520concatenate%2520all%2520views%2527%2520embedded%2520features%2520to%2520form%2520the%2520global%2520features%252C%2520which%2520can%2520overcome%2520the%2520negative%2520impact%2520of%2520some%2520views%2527%2520unclear%2520clustering%2520structures.%2520In%2520a%2520self-supervised%2520manner%252C%2520pseudo-labels%2520are%2520obtained%2520to%2520build%2520a%2520unified%2520target%2520distribution%2520to%2520perform%2520multi-view%2520discriminative%2520feature%2520learning.%2520During%2520this%2520process%252C%2520global%2520discriminative%2520information%2520can%2520be%2520mined%2520to%2520supervise%2520all%2520views%2520to%2520learn%2520more%2520discriminative%2520features%252C%2520which%2520in%2520turn%2520are%2520used%2520to%2520update%2520the%2520target%2520distribution.%2520Besides%252C%2520this%2520unified%2520target%2520distribution%2520can%2520make%2520SDMVC%2520learn%2520consistent%2520cluster%2520assignments%252C%2520which%2520accomplishes%2520the%2520clustering%2520consistency%2520of%2520multiple%2520views%2520while%2520preserving%2520their%2520features%2527%2520diversity.%2520Experiments%2520on%2520various%2520types%2520of%2520multi-view%2520datasets%2520show%2520that%2520SDMVC%2520outperforms%252014%2520competitors%2520including%2520classic%2520and%2520state-of-the-art%2520methods.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/SubmissionsIn/SDMVC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2103.15069v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Discriminative%20Feature%20Learning%20for%20Deep%20Multi-View%20Clustering&entry.906535625=Jie%20Xu%20and%20Yazhou%20Ren%20and%20Huayi%20Tang%20and%20Zhimeng%20Yang%20and%20Lili%20Pan%20and%20Yang%20Yang%20and%20Xiaorong%20Pu%20and%20Philip%20S.%20Yu%20and%20Lifang%20He&entry.1292438233=Multi-view%20clustering%20is%20an%20important%20research%20topic%20due%20to%20its%20capability%20to%20utilize%20complementary%20information%20from%20multiple%20views.%20However%2C%20there%20are%20few%20methods%20to%20consider%20the%20negative%20impact%20caused%20by%20certain%20views%20with%20unclear%20clustering%20structures%2C%20resulting%20in%20poor%20multi-view%20clustering%20performance.%20To%20address%20this%20drawback%2C%20we%20propose%20self-supervised%20discriminative%20feature%20learning%20for%20deep%20multi-view%20clustering%20%28SDMVC%29.%20Concretely%2C%20deep%20autoencoders%20are%20applied%20to%20learn%20embedded%20features%20for%20each%20view%20independently.%20To%20leverage%20the%20multi-view%20complementary%20information%2C%20we%20concatenate%20all%20views%27%20embedded%20features%20to%20form%20the%20global%20features%2C%20which%20can%20overcome%20the%20negative%20impact%20of%20some%20views%27%20unclear%20clustering%20structures.%20In%20a%20self-supervised%20manner%2C%20pseudo-labels%20are%20obtained%20to%20build%20a%20unified%20target%20distribution%20to%20perform%20multi-view%20discriminative%20feature%20learning.%20During%20this%20process%2C%20global%20discriminative%20information%20can%20be%20mined%20to%20supervise%20all%20views%20to%20learn%20more%20discriminative%20features%2C%20which%20in%20turn%20are%20used%20to%20update%20the%20target%20distribution.%20Besides%2C%20this%20unified%20target%20distribution%20can%20make%20SDMVC%20learn%20consistent%20cluster%20assignments%2C%20which%20accomplishes%20the%20clustering%20consistency%20of%20multiple%20views%20while%20preserving%20their%20features%27%20diversity.%20Experiments%20on%20various%20types%20of%20multi-view%20datasets%20show%20that%20SDMVC%20outperforms%2014%20competitors%20including%20classic%20and%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%20https%3A//github.com/SubmissionsIn/SDMVC.&entry.1838667208=http%3A//arxiv.org/abs/2103.15069v3&entry.124074799=Read"},
{"title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy", "author": "Yang Tian and Yuyin Yang and Yiman Xie and Zetao Cai and Xu Shi and Ning Gao and Hangxu Liu and Xuekun Jiang and Zherui Qiu and Feng Yuan and Yaping Li and Ping Wang and Junhao Cai and Jia Zeng and Hao Dong and Jiangmiao Pang", "abstract": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $\u03c0$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $\u03c0_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $\u03c0_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.", "link": "http://arxiv.org/abs/2511.16651v1", "date": "2025-11-20", "relevancy": 2.7589, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5583}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5517}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternData-A1%3A%20Pioneering%20High-Fidelity%20Synthetic%20Data%20for%20Pre-training%20Generalist%20Policy&body=Title%3A%20InternData-A1%3A%20Pioneering%20High-Fidelity%20Synthetic%20Data%20for%20Pre-training%20Generalist%20Policy%0AAuthor%3A%20Yang%20Tian%20and%20Yuyin%20Yang%20and%20Yiman%20Xie%20and%20Zetao%20Cai%20and%20Xu%20Shi%20and%20Ning%20Gao%20and%20Hangxu%20Liu%20and%20Xuekun%20Jiang%20and%20Zherui%20Qiu%20and%20Feng%20Yuan%20and%20Yaping%20Li%20and%20Ping%20Wang%20and%20Junhao%20Cai%20and%20Jia%20Zeng%20and%20Hao%20Dong%20and%20Jiangmiao%20Pang%0AAbstract%3A%20Recent%20works%20explore%20how%20real%20and%20synthetic%20data%20contribute%20to%20Vision-Language-Action%20%28VLA%29%20models%27%20generalization.%20While%20current%20VLA%20models%20have%20shown%20the%20strong%20effectiveness%20of%20large-scale%20real-robot%20pre-training%2C%20synthetic%20data%20has%20not%20previously%20demonstrated%20comparable%20capability%20at%20scale.%20This%20paper%20provides%20the%20first%20evidence%20that%20synthetic%20data%20alone%20can%20match%20the%20performance%20of%20the%20strongest%20%24%CF%80%24-dataset%20in%20pre-training%20a%20VLA%20model%2C%20revealing%20the%20substantial%20value%20of%20large-scale%20simulation.%20The%20resulting%20model%20also%20exhibits%20surprisingly%20zero-shot%20sim-to-real%20transfer%20on%20several%20challenging%20tasks.%20Our%20synthetic%20dataset%2C%20InternData-A1%2C%20contains%20over%20630k%20trajectories%20and%207%2C433%20hours%20across%204%20embodiments%2C%2018%20skills%2C%2070%20tasks%2C%20and%20227%20scenes%2C%20covering%20rigid%2C%20articulated%2C%20deformable%2C%20and%20fluid-object%20manipulation.%20It%20is%20generated%20through%20a%20highly%20autonomous%2C%20fully%20decoupled%2C%20and%20compositional%20simulation%20pipeline%20that%20enables%20long-horizon%20skill%20composition%2C%20flexible%20task%20assembly%2C%20and%20heterogeneous%20embodiments%20with%20minimal%20manual%20tuning.%20Using%20the%20same%20architecture%20as%20%24%CF%80_0%24%2C%20we%20pre-train%20a%20model%20entirely%20on%20InternData-A1%20and%20find%20that%20it%20matches%20the%20official%20%24%CF%80_0%24%20across%2049%20simulation%20tasks%2C%205%20real-world%20tasks%2C%20and%204%20long-horizon%20dexterous%20tasks.%20We%20release%20the%20dataset%20and%20will%20open-source%20the%20generation%20pipeline%20to%20broaden%20access%20to%20large-scale%20robotic%20data%20and%20to%20lower%20the%20barrier%20to%20scalable%20data%20creation%20for%20embodied%20AI%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternData-A1%253A%2520Pioneering%2520High-Fidelity%2520Synthetic%2520Data%2520for%2520Pre-training%2520Generalist%2520Policy%26entry.906535625%3DYang%2520Tian%2520and%2520Yuyin%2520Yang%2520and%2520Yiman%2520Xie%2520and%2520Zetao%2520Cai%2520and%2520Xu%2520Shi%2520and%2520Ning%2520Gao%2520and%2520Hangxu%2520Liu%2520and%2520Xuekun%2520Jiang%2520and%2520Zherui%2520Qiu%2520and%2520Feng%2520Yuan%2520and%2520Yaping%2520Li%2520and%2520Ping%2520Wang%2520and%2520Junhao%2520Cai%2520and%2520Jia%2520Zeng%2520and%2520Hao%2520Dong%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DRecent%2520works%2520explore%2520how%2520real%2520and%2520synthetic%2520data%2520contribute%2520to%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2527%2520generalization.%2520While%2520current%2520VLA%2520models%2520have%2520shown%2520the%2520strong%2520effectiveness%2520of%2520large-scale%2520real-robot%2520pre-training%252C%2520synthetic%2520data%2520has%2520not%2520previously%2520demonstrated%2520comparable%2520capability%2520at%2520scale.%2520This%2520paper%2520provides%2520the%2520first%2520evidence%2520that%2520synthetic%2520data%2520alone%2520can%2520match%2520the%2520performance%2520of%2520the%2520strongest%2520%2524%25CF%2580%2524-dataset%2520in%2520pre-training%2520a%2520VLA%2520model%252C%2520revealing%2520the%2520substantial%2520value%2520of%2520large-scale%2520simulation.%2520The%2520resulting%2520model%2520also%2520exhibits%2520surprisingly%2520zero-shot%2520sim-to-real%2520transfer%2520on%2520several%2520challenging%2520tasks.%2520Our%2520synthetic%2520dataset%252C%2520InternData-A1%252C%2520contains%2520over%2520630k%2520trajectories%2520and%25207%252C433%2520hours%2520across%25204%2520embodiments%252C%252018%2520skills%252C%252070%2520tasks%252C%2520and%2520227%2520scenes%252C%2520covering%2520rigid%252C%2520articulated%252C%2520deformable%252C%2520and%2520fluid-object%2520manipulation.%2520It%2520is%2520generated%2520through%2520a%2520highly%2520autonomous%252C%2520fully%2520decoupled%252C%2520and%2520compositional%2520simulation%2520pipeline%2520that%2520enables%2520long-horizon%2520skill%2520composition%252C%2520flexible%2520task%2520assembly%252C%2520and%2520heterogeneous%2520embodiments%2520with%2520minimal%2520manual%2520tuning.%2520Using%2520the%2520same%2520architecture%2520as%2520%2524%25CF%2580_0%2524%252C%2520we%2520pre-train%2520a%2520model%2520entirely%2520on%2520InternData-A1%2520and%2520find%2520that%2520it%2520matches%2520the%2520official%2520%2524%25CF%2580_0%2524%2520across%252049%2520simulation%2520tasks%252C%25205%2520real-world%2520tasks%252C%2520and%25204%2520long-horizon%2520dexterous%2520tasks.%2520We%2520release%2520the%2520dataset%2520and%2520will%2520open-source%2520the%2520generation%2520pipeline%2520to%2520broaden%2520access%2520to%2520large-scale%2520robotic%2520data%2520and%2520to%2520lower%2520the%2520barrier%2520to%2520scalable%2520data%2520creation%2520for%2520embodied%2520AI%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternData-A1%3A%20Pioneering%20High-Fidelity%20Synthetic%20Data%20for%20Pre-training%20Generalist%20Policy&entry.906535625=Yang%20Tian%20and%20Yuyin%20Yang%20and%20Yiman%20Xie%20and%20Zetao%20Cai%20and%20Xu%20Shi%20and%20Ning%20Gao%20and%20Hangxu%20Liu%20and%20Xuekun%20Jiang%20and%20Zherui%20Qiu%20and%20Feng%20Yuan%20and%20Yaping%20Li%20and%20Ping%20Wang%20and%20Junhao%20Cai%20and%20Jia%20Zeng%20and%20Hao%20Dong%20and%20Jiangmiao%20Pang&entry.1292438233=Recent%20works%20explore%20how%20real%20and%20synthetic%20data%20contribute%20to%20Vision-Language-Action%20%28VLA%29%20models%27%20generalization.%20While%20current%20VLA%20models%20have%20shown%20the%20strong%20effectiveness%20of%20large-scale%20real-robot%20pre-training%2C%20synthetic%20data%20has%20not%20previously%20demonstrated%20comparable%20capability%20at%20scale.%20This%20paper%20provides%20the%20first%20evidence%20that%20synthetic%20data%20alone%20can%20match%20the%20performance%20of%20the%20strongest%20%24%CF%80%24-dataset%20in%20pre-training%20a%20VLA%20model%2C%20revealing%20the%20substantial%20value%20of%20large-scale%20simulation.%20The%20resulting%20model%20also%20exhibits%20surprisingly%20zero-shot%20sim-to-real%20transfer%20on%20several%20challenging%20tasks.%20Our%20synthetic%20dataset%2C%20InternData-A1%2C%20contains%20over%20630k%20trajectories%20and%207%2C433%20hours%20across%204%20embodiments%2C%2018%20skills%2C%2070%20tasks%2C%20and%20227%20scenes%2C%20covering%20rigid%2C%20articulated%2C%20deformable%2C%20and%20fluid-object%20manipulation.%20It%20is%20generated%20through%20a%20highly%20autonomous%2C%20fully%20decoupled%2C%20and%20compositional%20simulation%20pipeline%20that%20enables%20long-horizon%20skill%20composition%2C%20flexible%20task%20assembly%2C%20and%20heterogeneous%20embodiments%20with%20minimal%20manual%20tuning.%20Using%20the%20same%20architecture%20as%20%24%CF%80_0%24%2C%20we%20pre-train%20a%20model%20entirely%20on%20InternData-A1%20and%20find%20that%20it%20matches%20the%20official%20%24%CF%80_0%24%20across%2049%20simulation%20tasks%2C%205%20real-world%20tasks%2C%20and%204%20long-horizon%20dexterous%20tasks.%20We%20release%20the%20dataset%20and%20will%20open-source%20the%20generation%20pipeline%20to%20broaden%20access%20to%20large-scale%20robotic%20data%20and%20to%20lower%20the%20barrier%20to%20scalable%20data%20creation%20for%20embodied%20AI%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.16651v1&entry.124074799=Read"},
{"title": "Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental Modelling", "author": "Julia Peters and Karin Mora and Miguel D. Mahecha and Chaonan Ji and David Montero and Clemens Mosig and Guido Kraemer", "abstract": "Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.", "link": "http://arxiv.org/abs/2511.11706v3", "date": "2025-11-20", "relevancy": 2.7549, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Multimodal%20Representation%20Learning%20for%20Spatio-Temporally%20Explicit%20Environmental%20Modelling&body=Title%3A%20Context-Aware%20Multimodal%20Representation%20Learning%20for%20Spatio-Temporally%20Explicit%20Environmental%20Modelling%0AAuthor%3A%20Julia%20Peters%20and%20Karin%20Mora%20and%20Miguel%20D.%20Mahecha%20and%20Chaonan%20Ji%20and%20David%20Montero%20and%20Clemens%20Mosig%20and%20Guido%20Kraemer%0AAbstract%3A%20Earth%20observation%20%28EO%29%20foundation%20models%20have%20emerged%20as%20an%20effective%20approach%20to%20derive%20latent%20representations%20of%20the%20Earth%20system%20from%20various%20remote%20sensing%20sensors.%20These%20models%20produce%20embeddings%20that%20can%20be%20used%20as%20analysis-ready%20datasets%2C%20enabling%20the%20modelling%20of%20ecosystem%20dynamics%20without%20extensive%20sensor-specific%20preprocessing.%20However%2C%20existing%20models%20typically%20operate%20at%20fixed%20spatial%20or%20temporal%20scales%2C%20limiting%20their%20use%20for%20ecological%20analyses%20that%20require%20both%20fine%20spatial%20detail%20and%20high%20temporal%20fidelity.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20representation%20learning%20framework%20that%20integrates%20different%20EO%20modalities%20into%20a%20unified%20feature%20space%20at%20high%20spatio-temporal%20resolution.%20We%20introduce%20the%20framework%20using%20Sentinel-1%20and%20Sentinel-2%20data%20as%20representative%20modalities.%20Our%20approach%20produces%20a%20latent%20space%20at%20native%2010%20m%20resolution%20and%20the%20temporal%20frequency%20of%20cloud-free%20Sentinel-2%20acquisitions.%20Each%20sensor%20is%20first%20modeled%20independently%20to%20capture%20its%20sensor-specific%20characteristics.%20Their%20representations%20are%20then%20combined%20into%20a%20shared%20model.%20This%20two-stage%20design%20enables%20modality-specific%20optimisation%20and%20easy%20extension%20to%20new%20sensors%2C%20retaining%20pretrained%20encoders%20while%20retraining%20only%20fusion%20layers.%20This%20enables%20the%20model%20to%20capture%20complementary%20remote%20sensing%20data%20and%20to%20preserve%20coherence%20across%20space%20and%20time.%20Qualitative%20analyses%20reveal%20that%20the%20learned%20embeddings%20exhibit%20high%20spatial%20and%20semantic%20consistency%20across%20heterogeneous%20landscapes.%20Quantitative%20evaluation%20in%20modelling%20Gross%20Primary%20Production%20reveals%20that%20they%20encode%20ecologically%20meaningful%20patterns%20and%20retain%20sufficient%20temporal%20fidelity%20to%20support%20fine-scale%20analyses.%20Overall%2C%20the%20proposed%20framework%20provides%20a%20flexible%2C%20analysis-ready%20representation%20learning%20approach%20for%20environmental%20applications%20requiring%20diverse%20spatial%20and%20temporal%20resolutions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11706v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Multimodal%2520Representation%2520Learning%2520for%2520Spatio-Temporally%2520Explicit%2520Environmental%2520Modelling%26entry.906535625%3DJulia%2520Peters%2520and%2520Karin%2520Mora%2520and%2520Miguel%2520D.%2520Mahecha%2520and%2520Chaonan%2520Ji%2520and%2520David%2520Montero%2520and%2520Clemens%2520Mosig%2520and%2520Guido%2520Kraemer%26entry.1292438233%3DEarth%2520observation%2520%2528EO%2529%2520foundation%2520models%2520have%2520emerged%2520as%2520an%2520effective%2520approach%2520to%2520derive%2520latent%2520representations%2520of%2520the%2520Earth%2520system%2520from%2520various%2520remote%2520sensing%2520sensors.%2520These%2520models%2520produce%2520embeddings%2520that%2520can%2520be%2520used%2520as%2520analysis-ready%2520datasets%252C%2520enabling%2520the%2520modelling%2520of%2520ecosystem%2520dynamics%2520without%2520extensive%2520sensor-specific%2520preprocessing.%2520However%252C%2520existing%2520models%2520typically%2520operate%2520at%2520fixed%2520spatial%2520or%2520temporal%2520scales%252C%2520limiting%2520their%2520use%2520for%2520ecological%2520analyses%2520that%2520require%2520both%2520fine%2520spatial%2520detail%2520and%2520high%2520temporal%2520fidelity.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520representation%2520learning%2520framework%2520that%2520integrates%2520different%2520EO%2520modalities%2520into%2520a%2520unified%2520feature%2520space%2520at%2520high%2520spatio-temporal%2520resolution.%2520We%2520introduce%2520the%2520framework%2520using%2520Sentinel-1%2520and%2520Sentinel-2%2520data%2520as%2520representative%2520modalities.%2520Our%2520approach%2520produces%2520a%2520latent%2520space%2520at%2520native%252010%2520m%2520resolution%2520and%2520the%2520temporal%2520frequency%2520of%2520cloud-free%2520Sentinel-2%2520acquisitions.%2520Each%2520sensor%2520is%2520first%2520modeled%2520independently%2520to%2520capture%2520its%2520sensor-specific%2520characteristics.%2520Their%2520representations%2520are%2520then%2520combined%2520into%2520a%2520shared%2520model.%2520This%2520two-stage%2520design%2520enables%2520modality-specific%2520optimisation%2520and%2520easy%2520extension%2520to%2520new%2520sensors%252C%2520retaining%2520pretrained%2520encoders%2520while%2520retraining%2520only%2520fusion%2520layers.%2520This%2520enables%2520the%2520model%2520to%2520capture%2520complementary%2520remote%2520sensing%2520data%2520and%2520to%2520preserve%2520coherence%2520across%2520space%2520and%2520time.%2520Qualitative%2520analyses%2520reveal%2520that%2520the%2520learned%2520embeddings%2520exhibit%2520high%2520spatial%2520and%2520semantic%2520consistency%2520across%2520heterogeneous%2520landscapes.%2520Quantitative%2520evaluation%2520in%2520modelling%2520Gross%2520Primary%2520Production%2520reveals%2520that%2520they%2520encode%2520ecologically%2520meaningful%2520patterns%2520and%2520retain%2520sufficient%2520temporal%2520fidelity%2520to%2520support%2520fine-scale%2520analyses.%2520Overall%252C%2520the%2520proposed%2520framework%2520provides%2520a%2520flexible%252C%2520analysis-ready%2520representation%2520learning%2520approach%2520for%2520environmental%2520applications%2520requiring%2520diverse%2520spatial%2520and%2520temporal%2520resolutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11706v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Multimodal%20Representation%20Learning%20for%20Spatio-Temporally%20Explicit%20Environmental%20Modelling&entry.906535625=Julia%20Peters%20and%20Karin%20Mora%20and%20Miguel%20D.%20Mahecha%20and%20Chaonan%20Ji%20and%20David%20Montero%20and%20Clemens%20Mosig%20and%20Guido%20Kraemer&entry.1292438233=Earth%20observation%20%28EO%29%20foundation%20models%20have%20emerged%20as%20an%20effective%20approach%20to%20derive%20latent%20representations%20of%20the%20Earth%20system%20from%20various%20remote%20sensing%20sensors.%20These%20models%20produce%20embeddings%20that%20can%20be%20used%20as%20analysis-ready%20datasets%2C%20enabling%20the%20modelling%20of%20ecosystem%20dynamics%20without%20extensive%20sensor-specific%20preprocessing.%20However%2C%20existing%20models%20typically%20operate%20at%20fixed%20spatial%20or%20temporal%20scales%2C%20limiting%20their%20use%20for%20ecological%20analyses%20that%20require%20both%20fine%20spatial%20detail%20and%20high%20temporal%20fidelity.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20representation%20learning%20framework%20that%20integrates%20different%20EO%20modalities%20into%20a%20unified%20feature%20space%20at%20high%20spatio-temporal%20resolution.%20We%20introduce%20the%20framework%20using%20Sentinel-1%20and%20Sentinel-2%20data%20as%20representative%20modalities.%20Our%20approach%20produces%20a%20latent%20space%20at%20native%2010%20m%20resolution%20and%20the%20temporal%20frequency%20of%20cloud-free%20Sentinel-2%20acquisitions.%20Each%20sensor%20is%20first%20modeled%20independently%20to%20capture%20its%20sensor-specific%20characteristics.%20Their%20representations%20are%20then%20combined%20into%20a%20shared%20model.%20This%20two-stage%20design%20enables%20modality-specific%20optimisation%20and%20easy%20extension%20to%20new%20sensors%2C%20retaining%20pretrained%20encoders%20while%20retraining%20only%20fusion%20layers.%20This%20enables%20the%20model%20to%20capture%20complementary%20remote%20sensing%20data%20and%20to%20preserve%20coherence%20across%20space%20and%20time.%20Qualitative%20analyses%20reveal%20that%20the%20learned%20embeddings%20exhibit%20high%20spatial%20and%20semantic%20consistency%20across%20heterogeneous%20landscapes.%20Quantitative%20evaluation%20in%20modelling%20Gross%20Primary%20Production%20reveals%20that%20they%20encode%20ecologically%20meaningful%20patterns%20and%20retain%20sufficient%20temporal%20fidelity%20to%20support%20fine-scale%20analyses.%20Overall%2C%20the%20proposed%20framework%20provides%20a%20flexible%2C%20analysis-ready%20representation%20learning%20approach%20for%20environmental%20applications%20requiring%20diverse%20spatial%20and%20temporal%20resolutions.&entry.1838667208=http%3A//arxiv.org/abs/2511.11706v3&entry.124074799=Read"},
{"title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation", "author": "Ziyu Guo and Renrui Zhang and Hongyu Li and Manyuan Zhang and Xinyan Chen and Sifan Wang and Yan Feng and Peng Pei and Pheng-Ann Heng", "abstract": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.", "link": "http://arxiv.org/abs/2511.16671v1", "date": "2025-11-20", "relevancy": 2.7412, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5761}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5368}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking-while-Generating%3A%20Interleaving%20Textual%20Reasoning%20throughout%20Visual%20Generation&body=Title%3A%20Thinking-while-Generating%3A%20Interleaving%20Textual%20Reasoning%20throughout%20Visual%20Generation%0AAuthor%3A%20Ziyu%20Guo%20and%20Renrui%20Zhang%20and%20Hongyu%20Li%20and%20Manyuan%20Zhang%20and%20Xinyan%20Chen%20and%20Sifan%20Wang%20and%20Yan%20Feng%20and%20Peng%20Pei%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20Recent%20advances%20in%20visual%20generation%20have%20increasingly%20explored%20the%20integration%20of%20reasoning%20capabilities.%20They%20incorporate%20textual%20reasoning%2C%20i.e.%2C%20think%2C%20either%20before%20%28as%20pre-planning%29%20or%20after%20%28as%20post-refinement%29%20the%20generation%20process%2C%20yet%20they%20lack%20on-the-fly%20multimodal%20interaction%20during%20the%20generation%20itself.%20In%20this%20preliminary%20study%2C%20we%20introduce%20Thinking-while-Generating%20%28TwiG%29%2C%20the%20first%20interleaved%20framework%20that%20enables%20co-evolving%20textual%20reasoning%20throughout%20the%20visual%20generation%20process.%20As%20visual%20content%20is%20progressively%20generating%2C%20textual%20reasoning%20is%20interleaved%20to%20both%20guide%20upcoming%20local%20regions%20and%20reflect%20on%20previously%20synthesized%20ones.%20This%20dynamic%20interplay%20produces%20more%20context-aware%20and%20semantically%20rich%20visual%20outputs.%20To%20unveil%20the%20potential%20of%20this%20framework%2C%20we%20investigate%20three%20candidate%20strategies%2C%20zero-shot%20prompting%2C%20supervised%20fine-tuning%20%28SFT%29%20on%20our%20curated%20TwiG-50K%20dataset%2C%20and%20reinforcement%20learning%20%28RL%29%20via%20a%20customized%20TwiG-GRPO%20strategy%2C%20each%20offering%20unique%20insights%20into%20the%20dynamics%20of%20interleaved%20reasoning.%20We%20hope%20this%20work%20inspires%20further%20research%20into%20interleaving%20textual%20reasoning%20for%20enhanced%20visual%20generation.%20Code%20will%20be%20released%20at%3A%20https%3A//github.com/ZiyuGuo99/Thinking-while-Generating.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking-while-Generating%253A%2520Interleaving%2520Textual%2520Reasoning%2520throughout%2520Visual%2520Generation%26entry.906535625%3DZiyu%2520Guo%2520and%2520Renrui%2520Zhang%2520and%2520Hongyu%2520Li%2520and%2520Manyuan%2520Zhang%2520and%2520Xinyan%2520Chen%2520and%2520Sifan%2520Wang%2520and%2520Yan%2520Feng%2520and%2520Peng%2520Pei%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3DRecent%2520advances%2520in%2520visual%2520generation%2520have%2520increasingly%2520explored%2520the%2520integration%2520of%2520reasoning%2520capabilities.%2520They%2520incorporate%2520textual%2520reasoning%252C%2520i.e.%252C%2520think%252C%2520either%2520before%2520%2528as%2520pre-planning%2529%2520or%2520after%2520%2528as%2520post-refinement%2529%2520the%2520generation%2520process%252C%2520yet%2520they%2520lack%2520on-the-fly%2520multimodal%2520interaction%2520during%2520the%2520generation%2520itself.%2520In%2520this%2520preliminary%2520study%252C%2520we%2520introduce%2520Thinking-while-Generating%2520%2528TwiG%2529%252C%2520the%2520first%2520interleaved%2520framework%2520that%2520enables%2520co-evolving%2520textual%2520reasoning%2520throughout%2520the%2520visual%2520generation%2520process.%2520As%2520visual%2520content%2520is%2520progressively%2520generating%252C%2520textual%2520reasoning%2520is%2520interleaved%2520to%2520both%2520guide%2520upcoming%2520local%2520regions%2520and%2520reflect%2520on%2520previously%2520synthesized%2520ones.%2520This%2520dynamic%2520interplay%2520produces%2520more%2520context-aware%2520and%2520semantically%2520rich%2520visual%2520outputs.%2520To%2520unveil%2520the%2520potential%2520of%2520this%2520framework%252C%2520we%2520investigate%2520three%2520candidate%2520strategies%252C%2520zero-shot%2520prompting%252C%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520on%2520our%2520curated%2520TwiG-50K%2520dataset%252C%2520and%2520reinforcement%2520learning%2520%2528RL%2529%2520via%2520a%2520customized%2520TwiG-GRPO%2520strategy%252C%2520each%2520offering%2520unique%2520insights%2520into%2520the%2520dynamics%2520of%2520interleaved%2520reasoning.%2520We%2520hope%2520this%2520work%2520inspires%2520further%2520research%2520into%2520interleaving%2520textual%2520reasoning%2520for%2520enhanced%2520visual%2520generation.%2520Code%2520will%2520be%2520released%2520at%253A%2520https%253A//github.com/ZiyuGuo99/Thinking-while-Generating.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking-while-Generating%3A%20Interleaving%20Textual%20Reasoning%20throughout%20Visual%20Generation&entry.906535625=Ziyu%20Guo%20and%20Renrui%20Zhang%20and%20Hongyu%20Li%20and%20Manyuan%20Zhang%20and%20Xinyan%20Chen%20and%20Sifan%20Wang%20and%20Yan%20Feng%20and%20Peng%20Pei%20and%20Pheng-Ann%20Heng&entry.1292438233=Recent%20advances%20in%20visual%20generation%20have%20increasingly%20explored%20the%20integration%20of%20reasoning%20capabilities.%20They%20incorporate%20textual%20reasoning%2C%20i.e.%2C%20think%2C%20either%20before%20%28as%20pre-planning%29%20or%20after%20%28as%20post-refinement%29%20the%20generation%20process%2C%20yet%20they%20lack%20on-the-fly%20multimodal%20interaction%20during%20the%20generation%20itself.%20In%20this%20preliminary%20study%2C%20we%20introduce%20Thinking-while-Generating%20%28TwiG%29%2C%20the%20first%20interleaved%20framework%20that%20enables%20co-evolving%20textual%20reasoning%20throughout%20the%20visual%20generation%20process.%20As%20visual%20content%20is%20progressively%20generating%2C%20textual%20reasoning%20is%20interleaved%20to%20both%20guide%20upcoming%20local%20regions%20and%20reflect%20on%20previously%20synthesized%20ones.%20This%20dynamic%20interplay%20produces%20more%20context-aware%20and%20semantically%20rich%20visual%20outputs.%20To%20unveil%20the%20potential%20of%20this%20framework%2C%20we%20investigate%20three%20candidate%20strategies%2C%20zero-shot%20prompting%2C%20supervised%20fine-tuning%20%28SFT%29%20on%20our%20curated%20TwiG-50K%20dataset%2C%20and%20reinforcement%20learning%20%28RL%29%20via%20a%20customized%20TwiG-GRPO%20strategy%2C%20each%20offering%20unique%20insights%20into%20the%20dynamics%20of%20interleaved%20reasoning.%20We%20hope%20this%20work%20inspires%20further%20research%20into%20interleaving%20textual%20reasoning%20for%20enhanced%20visual%20generation.%20Code%20will%20be%20released%20at%3A%20https%3A//github.com/ZiyuGuo99/Thinking-while-Generating.&entry.1838667208=http%3A//arxiv.org/abs/2511.16671v1&entry.124074799=Read"},
{"title": "PartUV: Part-Based UV Unwrapping of 3D Meshes", "author": "Zhaoning Wang and Xinyue Wei and Ruoxi Shi and Xiaoshuai Zhang and Hao Su and Minghua Liu", "abstract": "UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.", "link": "http://arxiv.org/abs/2511.16659v1", "date": "2025-11-20", "relevancy": 2.7399, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5611}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5414}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartUV%3A%20Part-Based%20UV%20Unwrapping%20of%203D%20Meshes&body=Title%3A%20PartUV%3A%20Part-Based%20UV%20Unwrapping%20of%203D%20Meshes%0AAuthor%3A%20Zhaoning%20Wang%20and%20Xinyue%20Wei%20and%20Ruoxi%20Shi%20and%20Xiaoshuai%20Zhang%20and%20Hao%20Su%20and%20Minghua%20Liu%0AAbstract%3A%20UV%20unwrapping%20flattens%203D%20surfaces%20to%202D%20with%20minimal%20distortion%2C%20often%20requiring%20the%20complex%20surface%20to%20be%20decomposed%20into%20multiple%20charts.%20Although%20extensively%20studied%2C%20existing%20UV%20unwrapping%20methods%20frequently%20struggle%20with%20AI-generated%20meshes%2C%20which%20are%20typically%20noisy%2C%20bumpy%2C%20and%20poorly%20conditioned.%20These%20methods%20often%20produce%20highly%20fragmented%20charts%20and%20suboptimal%20boundaries%2C%20introducing%20artifacts%20and%20hindering%20downstream%20tasks.%20We%20introduce%20PartUV%2C%20a%20part-based%20UV%20unwrapping%20pipeline%20that%20generates%20significantly%20fewer%2C%20part-aligned%20charts%20while%20maintaining%20low%20distortion.%20Built%20on%20top%20of%20a%20recent%20learning-based%20part%20decomposition%20method%20PartField%2C%20PartUV%20combines%20high-level%20semantic%20part%20decomposition%20with%20novel%20geometric%20heuristics%20in%20a%20top-down%20recursive%20framework.%20It%20ensures%20each%20chart%27s%20distortion%20remains%20below%20a%20user-specified%20threshold%20while%20minimizing%20the%20total%20number%20of%20charts.%20The%20pipeline%20integrates%20and%20extends%20parameterization%20and%20packing%20algorithms%2C%20incorporates%20dedicated%20handling%20of%20non-manifold%20and%20degenerate%20meshes%2C%20and%20is%20extensively%20parallelized%20for%20efficiency.%20Evaluated%20across%20four%20diverse%20datasets%2C%20including%20man-made%2C%20CAD%2C%20AI-generated%2C%20and%20Common%20Shapes%2C%20PartUV%20outperforms%20existing%20tools%20and%20recent%20neural%20methods%20in%20chart%20count%20and%20seam%20length%2C%20achieves%20comparable%20distortion%2C%20exhibits%20high%20success%20rates%20on%20challenging%20meshes%2C%20and%20enables%20new%20applications%20like%20part-specific%20multi-tiles%20packing.%20Our%20project%20page%20is%20at%20https%3A//www.zhaoningwang.com/PartUV.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartUV%253A%2520Part-Based%2520UV%2520Unwrapping%2520of%25203D%2520Meshes%26entry.906535625%3DZhaoning%2520Wang%2520and%2520Xinyue%2520Wei%2520and%2520Ruoxi%2520Shi%2520and%2520Xiaoshuai%2520Zhang%2520and%2520Hao%2520Su%2520and%2520Minghua%2520Liu%26entry.1292438233%3DUV%2520unwrapping%2520flattens%25203D%2520surfaces%2520to%25202D%2520with%2520minimal%2520distortion%252C%2520often%2520requiring%2520the%2520complex%2520surface%2520to%2520be%2520decomposed%2520into%2520multiple%2520charts.%2520Although%2520extensively%2520studied%252C%2520existing%2520UV%2520unwrapping%2520methods%2520frequently%2520struggle%2520with%2520AI-generated%2520meshes%252C%2520which%2520are%2520typically%2520noisy%252C%2520bumpy%252C%2520and%2520poorly%2520conditioned.%2520These%2520methods%2520often%2520produce%2520highly%2520fragmented%2520charts%2520and%2520suboptimal%2520boundaries%252C%2520introducing%2520artifacts%2520and%2520hindering%2520downstream%2520tasks.%2520We%2520introduce%2520PartUV%252C%2520a%2520part-based%2520UV%2520unwrapping%2520pipeline%2520that%2520generates%2520significantly%2520fewer%252C%2520part-aligned%2520charts%2520while%2520maintaining%2520low%2520distortion.%2520Built%2520on%2520top%2520of%2520a%2520recent%2520learning-based%2520part%2520decomposition%2520method%2520PartField%252C%2520PartUV%2520combines%2520high-level%2520semantic%2520part%2520decomposition%2520with%2520novel%2520geometric%2520heuristics%2520in%2520a%2520top-down%2520recursive%2520framework.%2520It%2520ensures%2520each%2520chart%2527s%2520distortion%2520remains%2520below%2520a%2520user-specified%2520threshold%2520while%2520minimizing%2520the%2520total%2520number%2520of%2520charts.%2520The%2520pipeline%2520integrates%2520and%2520extends%2520parameterization%2520and%2520packing%2520algorithms%252C%2520incorporates%2520dedicated%2520handling%2520of%2520non-manifold%2520and%2520degenerate%2520meshes%252C%2520and%2520is%2520extensively%2520parallelized%2520for%2520efficiency.%2520Evaluated%2520across%2520four%2520diverse%2520datasets%252C%2520including%2520man-made%252C%2520CAD%252C%2520AI-generated%252C%2520and%2520Common%2520Shapes%252C%2520PartUV%2520outperforms%2520existing%2520tools%2520and%2520recent%2520neural%2520methods%2520in%2520chart%2520count%2520and%2520seam%2520length%252C%2520achieves%2520comparable%2520distortion%252C%2520exhibits%2520high%2520success%2520rates%2520on%2520challenging%2520meshes%252C%2520and%2520enables%2520new%2520applications%2520like%2520part-specific%2520multi-tiles%2520packing.%2520Our%2520project%2520page%2520is%2520at%2520https%253A//www.zhaoningwang.com/PartUV.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartUV%3A%20Part-Based%20UV%20Unwrapping%20of%203D%20Meshes&entry.906535625=Zhaoning%20Wang%20and%20Xinyue%20Wei%20and%20Ruoxi%20Shi%20and%20Xiaoshuai%20Zhang%20and%20Hao%20Su%20and%20Minghua%20Liu&entry.1292438233=UV%20unwrapping%20flattens%203D%20surfaces%20to%202D%20with%20minimal%20distortion%2C%20often%20requiring%20the%20complex%20surface%20to%20be%20decomposed%20into%20multiple%20charts.%20Although%20extensively%20studied%2C%20existing%20UV%20unwrapping%20methods%20frequently%20struggle%20with%20AI-generated%20meshes%2C%20which%20are%20typically%20noisy%2C%20bumpy%2C%20and%20poorly%20conditioned.%20These%20methods%20often%20produce%20highly%20fragmented%20charts%20and%20suboptimal%20boundaries%2C%20introducing%20artifacts%20and%20hindering%20downstream%20tasks.%20We%20introduce%20PartUV%2C%20a%20part-based%20UV%20unwrapping%20pipeline%20that%20generates%20significantly%20fewer%2C%20part-aligned%20charts%20while%20maintaining%20low%20distortion.%20Built%20on%20top%20of%20a%20recent%20learning-based%20part%20decomposition%20method%20PartField%2C%20PartUV%20combines%20high-level%20semantic%20part%20decomposition%20with%20novel%20geometric%20heuristics%20in%20a%20top-down%20recursive%20framework.%20It%20ensures%20each%20chart%27s%20distortion%20remains%20below%20a%20user-specified%20threshold%20while%20minimizing%20the%20total%20number%20of%20charts.%20The%20pipeline%20integrates%20and%20extends%20parameterization%20and%20packing%20algorithms%2C%20incorporates%20dedicated%20handling%20of%20non-manifold%20and%20degenerate%20meshes%2C%20and%20is%20extensively%20parallelized%20for%20efficiency.%20Evaluated%20across%20four%20diverse%20datasets%2C%20including%20man-made%2C%20CAD%2C%20AI-generated%2C%20and%20Common%20Shapes%2C%20PartUV%20outperforms%20existing%20tools%20and%20recent%20neural%20methods%20in%20chart%20count%20and%20seam%20length%2C%20achieves%20comparable%20distortion%2C%20exhibits%20high%20success%20rates%20on%20challenging%20meshes%2C%20and%20enables%20new%20applications%20like%20part-specific%20multi-tiles%20packing.%20Our%20project%20page%20is%20at%20https%3A//www.zhaoningwang.com/PartUV.&entry.1838667208=http%3A//arxiv.org/abs/2511.16659v1&entry.124074799=Read"},
{"title": "ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery", "author": "Ching-Heng Cheng and Chih-Chung Hsu", "abstract": "Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.", "link": "http://arxiv.org/abs/2511.16322v1", "date": "2025-11-20", "relevancy": 2.7381, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChangeDINO%3A%20DINOv3-Driven%20Building%20Change%20Detection%20in%20Optical%20Remote%20Sensing%20Imagery&body=Title%3A%20ChangeDINO%3A%20DINOv3-Driven%20Building%20Change%20Detection%20in%20Optical%20Remote%20Sensing%20Imagery%0AAuthor%3A%20Ching-Heng%20Cheng%20and%20Chih-Chung%20Hsu%0AAbstract%3A%20Remote%20sensing%20change%20detection%20%28RSCD%29%20aims%20to%20identify%20surface%20changes%20from%20co-registered%20bi-temporal%20images.%20However%2C%20many%20deep%20learning-based%20RSCD%20methods%20rely%20solely%20on%20change-map%20annotations%20and%20underuse%20the%20semantic%20information%20in%20non-changing%20regions%2C%20which%20limits%20robustness%20under%20illumination%20variation%2C%20off-nadir%20views%2C%20and%20scarce%20labels.%20This%20article%20introduces%20ChangeDINO%2C%20an%20end-to-end%20multiscale%20Siamese%20framework%20for%20optical%20building%20change%20detection.%20The%20model%20fuses%20a%20lightweight%20backbone%20stream%20with%20features%20transferred%20from%20a%20frozen%20DINOv3%2C%20yielding%20semantic-%20and%20context-rich%20pyramids%20even%20on%20small%20datasets.%20A%20spatial-spectral%20differential%20transformer%20decoder%20then%20exploits%20multi-scale%20absolute%20differences%20as%20change%20priors%20to%20highlight%20true%20building%20changes%20and%20suppress%20irrelevant%20responses.%20Finally%2C%20a%20learnable%20morphology%20module%20refines%20the%20upsampled%20logits%20to%20recover%20clean%20boundaries.%20Experiments%20on%20four%20public%20benchmarks%20show%20that%20ChangeDINO%20consistently%20outperforms%20recent%20state-of-the-art%20methods%20in%20IoU%20and%20F1%2C%20and%20ablation%20studies%20confirm%20the%20effectiveness%20of%20each%20component.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/chingheng0808/ChangeDINO.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChangeDINO%253A%2520DINOv3-Driven%2520Building%2520Change%2520Detection%2520in%2520Optical%2520Remote%2520Sensing%2520Imagery%26entry.906535625%3DChing-Heng%2520Cheng%2520and%2520Chih-Chung%2520Hsu%26entry.1292438233%3DRemote%2520sensing%2520change%2520detection%2520%2528RSCD%2529%2520aims%2520to%2520identify%2520surface%2520changes%2520from%2520co-registered%2520bi-temporal%2520images.%2520However%252C%2520many%2520deep%2520learning-based%2520RSCD%2520methods%2520rely%2520solely%2520on%2520change-map%2520annotations%2520and%2520underuse%2520the%2520semantic%2520information%2520in%2520non-changing%2520regions%252C%2520which%2520limits%2520robustness%2520under%2520illumination%2520variation%252C%2520off-nadir%2520views%252C%2520and%2520scarce%2520labels.%2520This%2520article%2520introduces%2520ChangeDINO%252C%2520an%2520end-to-end%2520multiscale%2520Siamese%2520framework%2520for%2520optical%2520building%2520change%2520detection.%2520The%2520model%2520fuses%2520a%2520lightweight%2520backbone%2520stream%2520with%2520features%2520transferred%2520from%2520a%2520frozen%2520DINOv3%252C%2520yielding%2520semantic-%2520and%2520context-rich%2520pyramids%2520even%2520on%2520small%2520datasets.%2520A%2520spatial-spectral%2520differential%2520transformer%2520decoder%2520then%2520exploits%2520multi-scale%2520absolute%2520differences%2520as%2520change%2520priors%2520to%2520highlight%2520true%2520building%2520changes%2520and%2520suppress%2520irrelevant%2520responses.%2520Finally%252C%2520a%2520learnable%2520morphology%2520module%2520refines%2520the%2520upsampled%2520logits%2520to%2520recover%2520clean%2520boundaries.%2520Experiments%2520on%2520four%2520public%2520benchmarks%2520show%2520that%2520ChangeDINO%2520consistently%2520outperforms%2520recent%2520state-of-the-art%2520methods%2520in%2520IoU%2520and%2520F1%252C%2520and%2520ablation%2520studies%2520confirm%2520the%2520effectiveness%2520of%2520each%2520component.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/chingheng0808/ChangeDINO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChangeDINO%3A%20DINOv3-Driven%20Building%20Change%20Detection%20in%20Optical%20Remote%20Sensing%20Imagery&entry.906535625=Ching-Heng%20Cheng%20and%20Chih-Chung%20Hsu&entry.1292438233=Remote%20sensing%20change%20detection%20%28RSCD%29%20aims%20to%20identify%20surface%20changes%20from%20co-registered%20bi-temporal%20images.%20However%2C%20many%20deep%20learning-based%20RSCD%20methods%20rely%20solely%20on%20change-map%20annotations%20and%20underuse%20the%20semantic%20information%20in%20non-changing%20regions%2C%20which%20limits%20robustness%20under%20illumination%20variation%2C%20off-nadir%20views%2C%20and%20scarce%20labels.%20This%20article%20introduces%20ChangeDINO%2C%20an%20end-to-end%20multiscale%20Siamese%20framework%20for%20optical%20building%20change%20detection.%20The%20model%20fuses%20a%20lightweight%20backbone%20stream%20with%20features%20transferred%20from%20a%20frozen%20DINOv3%2C%20yielding%20semantic-%20and%20context-rich%20pyramids%20even%20on%20small%20datasets.%20A%20spatial-spectral%20differential%20transformer%20decoder%20then%20exploits%20multi-scale%20absolute%20differences%20as%20change%20priors%20to%20highlight%20true%20building%20changes%20and%20suppress%20irrelevant%20responses.%20Finally%2C%20a%20learnable%20morphology%20module%20refines%20the%20upsampled%20logits%20to%20recover%20clean%20boundaries.%20Experiments%20on%20four%20public%20benchmarks%20show%20that%20ChangeDINO%20consistently%20outperforms%20recent%20state-of-the-art%20methods%20in%20IoU%20and%20F1%2C%20and%20ablation%20studies%20confirm%20the%20effectiveness%20of%20each%20component.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/chingheng0808/ChangeDINO.&entry.1838667208=http%3A//arxiv.org/abs/2511.16322v1&entry.124074799=Read"},
{"title": "Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding", "author": "Muxin Pu and Mei Kuan Lim and Chun Yong Chong and Chen Change Loy", "abstract": "Pre-training has proven effective for learning transferable features in sign language understanding (SLU) tasks. Recently, skeleton-based methods have gained increasing attention because they can robustly handle variations in subjects and backgrounds without being affected by appearance or environmental factors. Current SLU methods continue to face three key limitations: 1) weak semantic grounding, as models often capture low-level motion patterns from skeletal data but struggle to relate them to linguistic meaning; 2) imbalance between local details and global context, with models either focusing too narrowly on fine-grained cues or overlooking them for broader context; and 3) inefficient cross-modal learning, as constructing semantically aligned representations across modalities remains difficult. To address these, we propose Sigma, a unified skeleton-based SLU framework featuring: 1) a sign-aware early fusion mechanism that facilitates deep interaction between visual and textual modalities, enriching visual features with linguistic context; 2) a hierarchical alignment learning strategy that jointly maximises agreements across different levels of paired features from different modalities, effectively capturing both fine-grained details and high-level semantic relationships; and 3) a unified pre-training framework that combines contrastive learning, text matching and language modelling to promote semantic consistency and generalisation. Sigma achieves new state-of-the-art results on isolated sign language recognition, continuous sign language recognition, and gloss-free sign language translation on multiple benchmarks spanning different sign and spoken languages, demonstrating the impact of semantically informative pre-training and the effectiveness of skeletal data as a stand-alone solution for SLU.", "link": "http://arxiv.org/abs/2509.21223v2", "date": "2025-11-20", "relevancy": 2.7229, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sigma%3A%20Semantically%20Informative%20Pre-training%20for%20Skeleton-based%20Sign%20Language%20Understanding&body=Title%3A%20Sigma%3A%20Semantically%20Informative%20Pre-training%20for%20Skeleton-based%20Sign%20Language%20Understanding%0AAuthor%3A%20Muxin%20Pu%20and%20Mei%20Kuan%20Lim%20and%20Chun%20Yong%20Chong%20and%20Chen%20Change%20Loy%0AAbstract%3A%20Pre-training%20has%20proven%20effective%20for%20learning%20transferable%20features%20in%20sign%20language%20understanding%20%28SLU%29%20tasks.%20Recently%2C%20skeleton-based%20methods%20have%20gained%20increasing%20attention%20because%20they%20can%20robustly%20handle%20variations%20in%20subjects%20and%20backgrounds%20without%20being%20affected%20by%20appearance%20or%20environmental%20factors.%20Current%20SLU%20methods%20continue%20to%20face%20three%20key%20limitations%3A%201%29%20weak%20semantic%20grounding%2C%20as%20models%20often%20capture%20low-level%20motion%20patterns%20from%20skeletal%20data%20but%20struggle%20to%20relate%20them%20to%20linguistic%20meaning%3B%202%29%20imbalance%20between%20local%20details%20and%20global%20context%2C%20with%20models%20either%20focusing%20too%20narrowly%20on%20fine-grained%20cues%20or%20overlooking%20them%20for%20broader%20context%3B%20and%203%29%20inefficient%20cross-modal%20learning%2C%20as%20constructing%20semantically%20aligned%20representations%20across%20modalities%20remains%20difficult.%20To%20address%20these%2C%20we%20propose%20Sigma%2C%20a%20unified%20skeleton-based%20SLU%20framework%20featuring%3A%201%29%20a%20sign-aware%20early%20fusion%20mechanism%20that%20facilitates%20deep%20interaction%20between%20visual%20and%20textual%20modalities%2C%20enriching%20visual%20features%20with%20linguistic%20context%3B%202%29%20a%20hierarchical%20alignment%20learning%20strategy%20that%20jointly%20maximises%20agreements%20across%20different%20levels%20of%20paired%20features%20from%20different%20modalities%2C%20effectively%20capturing%20both%20fine-grained%20details%20and%20high-level%20semantic%20relationships%3B%20and%203%29%20a%20unified%20pre-training%20framework%20that%20combines%20contrastive%20learning%2C%20text%20matching%20and%20language%20modelling%20to%20promote%20semantic%20consistency%20and%20generalisation.%20Sigma%20achieves%20new%20state-of-the-art%20results%20on%20isolated%20sign%20language%20recognition%2C%20continuous%20sign%20language%20recognition%2C%20and%20gloss-free%20sign%20language%20translation%20on%20multiple%20benchmarks%20spanning%20different%20sign%20and%20spoken%20languages%2C%20demonstrating%20the%20impact%20of%20semantically%20informative%20pre-training%20and%20the%20effectiveness%20of%20skeletal%20data%20as%20a%20stand-alone%20solution%20for%20SLU.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21223v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSigma%253A%2520Semantically%2520Informative%2520Pre-training%2520for%2520Skeleton-based%2520Sign%2520Language%2520Understanding%26entry.906535625%3DMuxin%2520Pu%2520and%2520Mei%2520Kuan%2520Lim%2520and%2520Chun%2520Yong%2520Chong%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3DPre-training%2520has%2520proven%2520effective%2520for%2520learning%2520transferable%2520features%2520in%2520sign%2520language%2520understanding%2520%2528SLU%2529%2520tasks.%2520Recently%252C%2520skeleton-based%2520methods%2520have%2520gained%2520increasing%2520attention%2520because%2520they%2520can%2520robustly%2520handle%2520variations%2520in%2520subjects%2520and%2520backgrounds%2520without%2520being%2520affected%2520by%2520appearance%2520or%2520environmental%2520factors.%2520Current%2520SLU%2520methods%2520continue%2520to%2520face%2520three%2520key%2520limitations%253A%25201%2529%2520weak%2520semantic%2520grounding%252C%2520as%2520models%2520often%2520capture%2520low-level%2520motion%2520patterns%2520from%2520skeletal%2520data%2520but%2520struggle%2520to%2520relate%2520them%2520to%2520linguistic%2520meaning%253B%25202%2529%2520imbalance%2520between%2520local%2520details%2520and%2520global%2520context%252C%2520with%2520models%2520either%2520focusing%2520too%2520narrowly%2520on%2520fine-grained%2520cues%2520or%2520overlooking%2520them%2520for%2520broader%2520context%253B%2520and%25203%2529%2520inefficient%2520cross-modal%2520learning%252C%2520as%2520constructing%2520semantically%2520aligned%2520representations%2520across%2520modalities%2520remains%2520difficult.%2520To%2520address%2520these%252C%2520we%2520propose%2520Sigma%252C%2520a%2520unified%2520skeleton-based%2520SLU%2520framework%2520featuring%253A%25201%2529%2520a%2520sign-aware%2520early%2520fusion%2520mechanism%2520that%2520facilitates%2520deep%2520interaction%2520between%2520visual%2520and%2520textual%2520modalities%252C%2520enriching%2520visual%2520features%2520with%2520linguistic%2520context%253B%25202%2529%2520a%2520hierarchical%2520alignment%2520learning%2520strategy%2520that%2520jointly%2520maximises%2520agreements%2520across%2520different%2520levels%2520of%2520paired%2520features%2520from%2520different%2520modalities%252C%2520effectively%2520capturing%2520both%2520fine-grained%2520details%2520and%2520high-level%2520semantic%2520relationships%253B%2520and%25203%2529%2520a%2520unified%2520pre-training%2520framework%2520that%2520combines%2520contrastive%2520learning%252C%2520text%2520matching%2520and%2520language%2520modelling%2520to%2520promote%2520semantic%2520consistency%2520and%2520generalisation.%2520Sigma%2520achieves%2520new%2520state-of-the-art%2520results%2520on%2520isolated%2520sign%2520language%2520recognition%252C%2520continuous%2520sign%2520language%2520recognition%252C%2520and%2520gloss-free%2520sign%2520language%2520translation%2520on%2520multiple%2520benchmarks%2520spanning%2520different%2520sign%2520and%2520spoken%2520languages%252C%2520demonstrating%2520the%2520impact%2520of%2520semantically%2520informative%2520pre-training%2520and%2520the%2520effectiveness%2520of%2520skeletal%2520data%2520as%2520a%2520stand-alone%2520solution%2520for%2520SLU.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21223v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sigma%3A%20Semantically%20Informative%20Pre-training%20for%20Skeleton-based%20Sign%20Language%20Understanding&entry.906535625=Muxin%20Pu%20and%20Mei%20Kuan%20Lim%20and%20Chun%20Yong%20Chong%20and%20Chen%20Change%20Loy&entry.1292438233=Pre-training%20has%20proven%20effective%20for%20learning%20transferable%20features%20in%20sign%20language%20understanding%20%28SLU%29%20tasks.%20Recently%2C%20skeleton-based%20methods%20have%20gained%20increasing%20attention%20because%20they%20can%20robustly%20handle%20variations%20in%20subjects%20and%20backgrounds%20without%20being%20affected%20by%20appearance%20or%20environmental%20factors.%20Current%20SLU%20methods%20continue%20to%20face%20three%20key%20limitations%3A%201%29%20weak%20semantic%20grounding%2C%20as%20models%20often%20capture%20low-level%20motion%20patterns%20from%20skeletal%20data%20but%20struggle%20to%20relate%20them%20to%20linguistic%20meaning%3B%202%29%20imbalance%20between%20local%20details%20and%20global%20context%2C%20with%20models%20either%20focusing%20too%20narrowly%20on%20fine-grained%20cues%20or%20overlooking%20them%20for%20broader%20context%3B%20and%203%29%20inefficient%20cross-modal%20learning%2C%20as%20constructing%20semantically%20aligned%20representations%20across%20modalities%20remains%20difficult.%20To%20address%20these%2C%20we%20propose%20Sigma%2C%20a%20unified%20skeleton-based%20SLU%20framework%20featuring%3A%201%29%20a%20sign-aware%20early%20fusion%20mechanism%20that%20facilitates%20deep%20interaction%20between%20visual%20and%20textual%20modalities%2C%20enriching%20visual%20features%20with%20linguistic%20context%3B%202%29%20a%20hierarchical%20alignment%20learning%20strategy%20that%20jointly%20maximises%20agreements%20across%20different%20levels%20of%20paired%20features%20from%20different%20modalities%2C%20effectively%20capturing%20both%20fine-grained%20details%20and%20high-level%20semantic%20relationships%3B%20and%203%29%20a%20unified%20pre-training%20framework%20that%20combines%20contrastive%20learning%2C%20text%20matching%20and%20language%20modelling%20to%20promote%20semantic%20consistency%20and%20generalisation.%20Sigma%20achieves%20new%20state-of-the-art%20results%20on%20isolated%20sign%20language%20recognition%2C%20continuous%20sign%20language%20recognition%2C%20and%20gloss-free%20sign%20language%20translation%20on%20multiple%20benchmarks%20spanning%20different%20sign%20and%20spoken%20languages%2C%20demonstrating%20the%20impact%20of%20semantically%20informative%20pre-training%20and%20the%20effectiveness%20of%20skeletal%20data%20as%20a%20stand-alone%20solution%20for%20SLU.&entry.1838667208=http%3A//arxiv.org/abs/2509.21223v2&entry.124074799=Read"},
{"title": "Unsupervised learning of spatially varying regularization for diffeomorphic image registration", "author": "Junyu Chen and Shuwen Wei and Yihao Liu and Zhangxing Bian and Yufan He and Aaron Carass and Harrison Bai and Yong Du", "abstract": "Spatially varying regularization accommodates the deformation variations that may be necessary for different anatomical regions during deformable image registration. Historically, optimization-based registration models have harnessed spatially varying regularization to address anatomical subtleties. However, most modern deep learning-based models tend to gravitate towards spatially invariant regularization, wherein a homogenous regularization strength is applied across the entire image, potentially disregarding localized variations. In this paper, we propose a hierarchical probabilistic model that integrates a prior distribution on the deformation regularization strength, enabling the end-to-end learning of a spatially varying deformation regularizer directly from the data. The proposed method is straightforward to implement and easily integrates with various registration network architectures. Additionally, automatic tuning of hyperparameters is achieved through Bayesian optimization, allowing efficient identification of optimal hyperparameters for any given registration task. Comprehensive evaluations on publicly available datasets demonstrate that the proposed method significantly improves registration performance and enhances the interpretability of deep learning-based registration, all while maintaining smooth deformations.", "link": "http://arxiv.org/abs/2412.17982v2", "date": "2025-11-20", "relevancy": 2.7063, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5891}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5212}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20learning%20of%20spatially%20varying%20regularization%20for%20diffeomorphic%20image%20registration&body=Title%3A%20Unsupervised%20learning%20of%20spatially%20varying%20regularization%20for%20diffeomorphic%20image%20registration%0AAuthor%3A%20Junyu%20Chen%20and%20Shuwen%20Wei%20and%20Yihao%20Liu%20and%20Zhangxing%20Bian%20and%20Yufan%20He%20and%20Aaron%20Carass%20and%20Harrison%20Bai%20and%20Yong%20Du%0AAbstract%3A%20Spatially%20varying%20regularization%20accommodates%20the%20deformation%20variations%20that%20may%20be%20necessary%20for%20different%20anatomical%20regions%20during%20deformable%20image%20registration.%20Historically%2C%20optimization-based%20registration%20models%20have%20harnessed%20spatially%20varying%20regularization%20to%20address%20anatomical%20subtleties.%20However%2C%20most%20modern%20deep%20learning-based%20models%20tend%20to%20gravitate%20towards%20spatially%20invariant%20regularization%2C%20wherein%20a%20homogenous%20regularization%20strength%20is%20applied%20across%20the%20entire%20image%2C%20potentially%20disregarding%20localized%20variations.%20In%20this%20paper%2C%20we%20propose%20a%20hierarchical%20probabilistic%20model%20that%20integrates%20a%20prior%20distribution%20on%20the%20deformation%20regularization%20strength%2C%20enabling%20the%20end-to-end%20learning%20of%20a%20spatially%20varying%20deformation%20regularizer%20directly%20from%20the%20data.%20The%20proposed%20method%20is%20straightforward%20to%20implement%20and%20easily%20integrates%20with%20various%20registration%20network%20architectures.%20Additionally%2C%20automatic%20tuning%20of%20hyperparameters%20is%20achieved%20through%20Bayesian%20optimization%2C%20allowing%20efficient%20identification%20of%20optimal%20hyperparameters%20for%20any%20given%20registration%20task.%20Comprehensive%20evaluations%20on%20publicly%20available%20datasets%20demonstrate%20that%20the%20proposed%20method%20significantly%20improves%20registration%20performance%20and%20enhances%20the%20interpretability%20of%20deep%20learning-based%20registration%2C%20all%20while%20maintaining%20smooth%20deformations.%0ALink%3A%20http%3A//arxiv.org/abs/2412.17982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520learning%2520of%2520spatially%2520varying%2520regularization%2520for%2520diffeomorphic%2520image%2520registration%26entry.906535625%3DJunyu%2520Chen%2520and%2520Shuwen%2520Wei%2520and%2520Yihao%2520Liu%2520and%2520Zhangxing%2520Bian%2520and%2520Yufan%2520He%2520and%2520Aaron%2520Carass%2520and%2520Harrison%2520Bai%2520and%2520Yong%2520Du%26entry.1292438233%3DSpatially%2520varying%2520regularization%2520accommodates%2520the%2520deformation%2520variations%2520that%2520may%2520be%2520necessary%2520for%2520different%2520anatomical%2520regions%2520during%2520deformable%2520image%2520registration.%2520Historically%252C%2520optimization-based%2520registration%2520models%2520have%2520harnessed%2520spatially%2520varying%2520regularization%2520to%2520address%2520anatomical%2520subtleties.%2520However%252C%2520most%2520modern%2520deep%2520learning-based%2520models%2520tend%2520to%2520gravitate%2520towards%2520spatially%2520invariant%2520regularization%252C%2520wherein%2520a%2520homogenous%2520regularization%2520strength%2520is%2520applied%2520across%2520the%2520entire%2520image%252C%2520potentially%2520disregarding%2520localized%2520variations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520hierarchical%2520probabilistic%2520model%2520that%2520integrates%2520a%2520prior%2520distribution%2520on%2520the%2520deformation%2520regularization%2520strength%252C%2520enabling%2520the%2520end-to-end%2520learning%2520of%2520a%2520spatially%2520varying%2520deformation%2520regularizer%2520directly%2520from%2520the%2520data.%2520The%2520proposed%2520method%2520is%2520straightforward%2520to%2520implement%2520and%2520easily%2520integrates%2520with%2520various%2520registration%2520network%2520architectures.%2520Additionally%252C%2520automatic%2520tuning%2520of%2520hyperparameters%2520is%2520achieved%2520through%2520Bayesian%2520optimization%252C%2520allowing%2520efficient%2520identification%2520of%2520optimal%2520hyperparameters%2520for%2520any%2520given%2520registration%2520task.%2520Comprehensive%2520evaluations%2520on%2520publicly%2520available%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520method%2520significantly%2520improves%2520registration%2520performance%2520and%2520enhances%2520the%2520interpretability%2520of%2520deep%2520learning-based%2520registration%252C%2520all%2520while%2520maintaining%2520smooth%2520deformations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20learning%20of%20spatially%20varying%20regularization%20for%20diffeomorphic%20image%20registration&entry.906535625=Junyu%20Chen%20and%20Shuwen%20Wei%20and%20Yihao%20Liu%20and%20Zhangxing%20Bian%20and%20Yufan%20He%20and%20Aaron%20Carass%20and%20Harrison%20Bai%20and%20Yong%20Du&entry.1292438233=Spatially%20varying%20regularization%20accommodates%20the%20deformation%20variations%20that%20may%20be%20necessary%20for%20different%20anatomical%20regions%20during%20deformable%20image%20registration.%20Historically%2C%20optimization-based%20registration%20models%20have%20harnessed%20spatially%20varying%20regularization%20to%20address%20anatomical%20subtleties.%20However%2C%20most%20modern%20deep%20learning-based%20models%20tend%20to%20gravitate%20towards%20spatially%20invariant%20regularization%2C%20wherein%20a%20homogenous%20regularization%20strength%20is%20applied%20across%20the%20entire%20image%2C%20potentially%20disregarding%20localized%20variations.%20In%20this%20paper%2C%20we%20propose%20a%20hierarchical%20probabilistic%20model%20that%20integrates%20a%20prior%20distribution%20on%20the%20deformation%20regularization%20strength%2C%20enabling%20the%20end-to-end%20learning%20of%20a%20spatially%20varying%20deformation%20regularizer%20directly%20from%20the%20data.%20The%20proposed%20method%20is%20straightforward%20to%20implement%20and%20easily%20integrates%20with%20various%20registration%20network%20architectures.%20Additionally%2C%20automatic%20tuning%20of%20hyperparameters%20is%20achieved%20through%20Bayesian%20optimization%2C%20allowing%20efficient%20identification%20of%20optimal%20hyperparameters%20for%20any%20given%20registration%20task.%20Comprehensive%20evaluations%20on%20publicly%20available%20datasets%20demonstrate%20that%20the%20proposed%20method%20significantly%20improves%20registration%20performance%20and%20enhances%20the%20interpretability%20of%20deep%20learning-based%20registration%2C%20all%20while%20maintaining%20smooth%20deformations.&entry.1838667208=http%3A//arxiv.org/abs/2412.17982v2&entry.124074799=Read"},
{"title": "End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI", "author": "Yihong Chen and Jiancheng Yang and Deniz Sayin Mercadier and Hieu Le and Juerg Schwitter and Pascal Fua", "abstract": "Reconstructing cardiac motion from CMR sequences is critical for diagnosis, prognosis, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their applicability during intervention when only sparse observations are available. We present TetHeart, the first end-to-end framework for unified 4D heart mesh recovery from both offline full-stack and intra-procedural sparse-slice observations. Our method leverages deformable tetrahedra to capture shape and motion in a coherent space shared across cardiac structures. Before a procedure, it initializes detailed, patient-specific heart meshes from high-quality full stacks, which can then be updated using whatever slices can be obtained in real-time, down to a single one during the procedure. TetHeart incorporates several key innovations: (i) an attentive slice-adaptive 2D-3D feature assembly mechanism that integrates information from arbitrary numbers of slices at any position; (ii) a distillation strategy to ensure accurate reconstruction under extreme sparsity; and (iii) a weakly supervised motion learning scheme requiring annotations only at keyframes, such as the end-diastolic and end-systolic phases. Trained and validated on three large public datasets and evaluated zero-shot on additional private interventional and public datasets without retraining, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings.", "link": "http://arxiv.org/abs/2509.12090v2", "date": "2025-11-20", "relevancy": 2.6987, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5455}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5426}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%204D%20Heart%20Mesh%20Recovery%20Across%20Full-Stack%20and%20Sparse%20Cardiac%20MRI&body=Title%3A%20End-to-End%204D%20Heart%20Mesh%20Recovery%20Across%20Full-Stack%20and%20Sparse%20Cardiac%20MRI%0AAuthor%3A%20Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Juerg%20Schwitter%20and%20Pascal%20Fua%0AAbstract%3A%20Reconstructing%20cardiac%20motion%20from%20CMR%20sequences%20is%20critical%20for%20diagnosis%2C%20prognosis%2C%20and%20intervention.%20Existing%20methods%20rely%20on%20complete%20CMR%20stacks%20to%20infer%20full%20heart%20motion%2C%20limiting%20their%20applicability%20during%20intervention%20when%20only%20sparse%20observations%20are%20available.%20We%20present%20TetHeart%2C%20the%20first%20end-to-end%20framework%20for%20unified%204D%20heart%20mesh%20recovery%20from%20both%20offline%20full-stack%20and%20intra-procedural%20sparse-slice%20observations.%20Our%20method%20leverages%20deformable%20tetrahedra%20to%20capture%20shape%20and%20motion%20in%20a%20coherent%20space%20shared%20across%20cardiac%20structures.%20Before%20a%20procedure%2C%20it%20initializes%20detailed%2C%20patient-specific%20heart%20meshes%20from%20high-quality%20full%20stacks%2C%20which%20can%20then%20be%20updated%20using%20whatever%20slices%20can%20be%20obtained%20in%20real-time%2C%20down%20to%20a%20single%20one%20during%20the%20procedure.%20TetHeart%20incorporates%20several%20key%20innovations%3A%20%28i%29%20an%20attentive%20slice-adaptive%202D-3D%20feature%20assembly%20mechanism%20that%20integrates%20information%20from%20arbitrary%20numbers%20of%20slices%20at%20any%20position%3B%20%28ii%29%20a%20distillation%20strategy%20to%20ensure%20accurate%20reconstruction%20under%20extreme%20sparsity%3B%20and%20%28iii%29%20a%20weakly%20supervised%20motion%20learning%20scheme%20requiring%20annotations%20only%20at%20keyframes%2C%20such%20as%20the%20end-diastolic%20and%20end-systolic%20phases.%20Trained%20and%20validated%20on%20three%20large%20public%20datasets%20and%20evaluated%20zero-shot%20on%20additional%20private%20interventional%20and%20public%20datasets%20without%20retraining%2C%20TetHeart%20achieves%20state-of-the-art%20accuracy%20and%20strong%20generalization%20in%20both%20pre-%20and%20intra-procedural%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2509.12090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%25204D%2520Heart%2520Mesh%2520Recovery%2520Across%2520Full-Stack%2520and%2520Sparse%2520Cardiac%2520MRI%26entry.906535625%3DYihong%2520Chen%2520and%2520Jiancheng%2520Yang%2520and%2520Deniz%2520Sayin%2520Mercadier%2520and%2520Hieu%2520Le%2520and%2520Juerg%2520Schwitter%2520and%2520Pascal%2520Fua%26entry.1292438233%3DReconstructing%2520cardiac%2520motion%2520from%2520CMR%2520sequences%2520is%2520critical%2520for%2520diagnosis%252C%2520prognosis%252C%2520and%2520intervention.%2520Existing%2520methods%2520rely%2520on%2520complete%2520CMR%2520stacks%2520to%2520infer%2520full%2520heart%2520motion%252C%2520limiting%2520their%2520applicability%2520during%2520intervention%2520when%2520only%2520sparse%2520observations%2520are%2520available.%2520We%2520present%2520TetHeart%252C%2520the%2520first%2520end-to-end%2520framework%2520for%2520unified%25204D%2520heart%2520mesh%2520recovery%2520from%2520both%2520offline%2520full-stack%2520and%2520intra-procedural%2520sparse-slice%2520observations.%2520Our%2520method%2520leverages%2520deformable%2520tetrahedra%2520to%2520capture%2520shape%2520and%2520motion%2520in%2520a%2520coherent%2520space%2520shared%2520across%2520cardiac%2520structures.%2520Before%2520a%2520procedure%252C%2520it%2520initializes%2520detailed%252C%2520patient-specific%2520heart%2520meshes%2520from%2520high-quality%2520full%2520stacks%252C%2520which%2520can%2520then%2520be%2520updated%2520using%2520whatever%2520slices%2520can%2520be%2520obtained%2520in%2520real-time%252C%2520down%2520to%2520a%2520single%2520one%2520during%2520the%2520procedure.%2520TetHeart%2520incorporates%2520several%2520key%2520innovations%253A%2520%2528i%2529%2520an%2520attentive%2520slice-adaptive%25202D-3D%2520feature%2520assembly%2520mechanism%2520that%2520integrates%2520information%2520from%2520arbitrary%2520numbers%2520of%2520slices%2520at%2520any%2520position%253B%2520%2528ii%2529%2520a%2520distillation%2520strategy%2520to%2520ensure%2520accurate%2520reconstruction%2520under%2520extreme%2520sparsity%253B%2520and%2520%2528iii%2529%2520a%2520weakly%2520supervised%2520motion%2520learning%2520scheme%2520requiring%2520annotations%2520only%2520at%2520keyframes%252C%2520such%2520as%2520the%2520end-diastolic%2520and%2520end-systolic%2520phases.%2520Trained%2520and%2520validated%2520on%2520three%2520large%2520public%2520datasets%2520and%2520evaluated%2520zero-shot%2520on%2520additional%2520private%2520interventional%2520and%2520public%2520datasets%2520without%2520retraining%252C%2520TetHeart%2520achieves%2520state-of-the-art%2520accuracy%2520and%2520strong%2520generalization%2520in%2520both%2520pre-%2520and%2520intra-procedural%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%204D%20Heart%20Mesh%20Recovery%20Across%20Full-Stack%20and%20Sparse%20Cardiac%20MRI&entry.906535625=Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Juerg%20Schwitter%20and%20Pascal%20Fua&entry.1292438233=Reconstructing%20cardiac%20motion%20from%20CMR%20sequences%20is%20critical%20for%20diagnosis%2C%20prognosis%2C%20and%20intervention.%20Existing%20methods%20rely%20on%20complete%20CMR%20stacks%20to%20infer%20full%20heart%20motion%2C%20limiting%20their%20applicability%20during%20intervention%20when%20only%20sparse%20observations%20are%20available.%20We%20present%20TetHeart%2C%20the%20first%20end-to-end%20framework%20for%20unified%204D%20heart%20mesh%20recovery%20from%20both%20offline%20full-stack%20and%20intra-procedural%20sparse-slice%20observations.%20Our%20method%20leverages%20deformable%20tetrahedra%20to%20capture%20shape%20and%20motion%20in%20a%20coherent%20space%20shared%20across%20cardiac%20structures.%20Before%20a%20procedure%2C%20it%20initializes%20detailed%2C%20patient-specific%20heart%20meshes%20from%20high-quality%20full%20stacks%2C%20which%20can%20then%20be%20updated%20using%20whatever%20slices%20can%20be%20obtained%20in%20real-time%2C%20down%20to%20a%20single%20one%20during%20the%20procedure.%20TetHeart%20incorporates%20several%20key%20innovations%3A%20%28i%29%20an%20attentive%20slice-adaptive%202D-3D%20feature%20assembly%20mechanism%20that%20integrates%20information%20from%20arbitrary%20numbers%20of%20slices%20at%20any%20position%3B%20%28ii%29%20a%20distillation%20strategy%20to%20ensure%20accurate%20reconstruction%20under%20extreme%20sparsity%3B%20and%20%28iii%29%20a%20weakly%20supervised%20motion%20learning%20scheme%20requiring%20annotations%20only%20at%20keyframes%2C%20such%20as%20the%20end-diastolic%20and%20end-systolic%20phases.%20Trained%20and%20validated%20on%20three%20large%20public%20datasets%20and%20evaluated%20zero-shot%20on%20additional%20private%20interventional%20and%20public%20datasets%20without%20retraining%2C%20TetHeart%20achieves%20state-of-the-art%20accuracy%20and%20strong%20generalization%20in%20both%20pre-%20and%20intra-procedural%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2509.12090v2&entry.124074799=Read"},
{"title": "Learning to Think Fast and Slow for Visual Language Models", "author": "Chenyu Lin and Cheng Chi and Jinlin Wu and Sharon Li and Kaiyang Zhou", "abstract": "When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.", "link": "http://arxiv.org/abs/2511.16670v1", "date": "2025-11-20", "relevancy": 2.6815, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Think%20Fast%20and%20Slow%20for%20Visual%20Language%20Models&body=Title%3A%20Learning%20to%20Think%20Fast%20and%20Slow%20for%20Visual%20Language%20Models%0AAuthor%3A%20Chenyu%20Lin%20and%20Cheng%20Chi%20and%20Jinlin%20Wu%20and%20Sharon%20Li%20and%20Kaiyang%20Zhou%0AAbstract%3A%20When%20confronted%20with%20complex%20problems%2C%20we%20tend%20to%20think%20slowly%3B%20conversely%2C%20for%20simple%20questions%2C%20we%20think%20quickly.%20Such%20a%20two-system%20thinking%20mechanism%20allows%20us%20to%20efficiently%20allocate%20cognitive%20resources%2C%20enabling%20quick%20decision-making%20for%20straightforward%20issues%20while%20reserving%20deeper%20analytical%20thinking%20for%20more%20intricate%20challenges.%20However%2C%20existing%20reasoning-oriented%20visual%20language%20models%20%28VLMs%29%2C%20whether%20trained%20with%20explicit%20chain-of-thought%20annotations%20or%20rule-based%20RL%20rewards%2C%20mainly%20pursue%20lengthy%2C%20detailed%20reasoning%20chains%2C%20which%20often%20lead%20to%20excessive%20computational%20costs.%20In%20this%20work%2C%20we%20propose%20a%20simple%20RL%20approach%2C%20which%20enables%20VLMs%20to%20automatically%20switch%20between%20fast%20and%20slow%20thinking%20modes%20depending%20on%20task%20difficulty.%20The%20approach%20consists%20of%20two%20stages%3A%20in%20the%20first%20stage%2C%20we%20label%20data%20as%20either%20requiring%20fast%20thinking%20or%20slow%20thinking%20based%20on%20the%20model%20output%20length%2C%20which%20is%20inspired%20by%20the%20observation%20that%20pre-trained%20VLMs%20typically%20produce%20answers%20of%20varying%20lengths%20for%20different%20types%20of%20questions%3B%20in%20the%20second%20stage%2C%20we%20train%20the%20model%20using%20GRPO%20along%20with%20the%20thinking%20mode%20labels%20to%20develop%20dual-mode%20thinking.%20Despite%20its%20simplicity%2C%20our%20model%2C%20named%20DualMindVLM%2C%20significantly%20outperforms%20the%20base%20model%20and%20achieves%20performance%20on%20par%20with%20state-of-the-art%20visual%20reasoning%20models%2C%20while%20maintaining%20exceptionally%20high%20token%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Think%2520Fast%2520and%2520Slow%2520for%2520Visual%2520Language%2520Models%26entry.906535625%3DChenyu%2520Lin%2520and%2520Cheng%2520Chi%2520and%2520Jinlin%2520Wu%2520and%2520Sharon%2520Li%2520and%2520Kaiyang%2520Zhou%26entry.1292438233%3DWhen%2520confronted%2520with%2520complex%2520problems%252C%2520we%2520tend%2520to%2520think%2520slowly%253B%2520conversely%252C%2520for%2520simple%2520questions%252C%2520we%2520think%2520quickly.%2520Such%2520a%2520two-system%2520thinking%2520mechanism%2520allows%2520us%2520to%2520efficiently%2520allocate%2520cognitive%2520resources%252C%2520enabling%2520quick%2520decision-making%2520for%2520straightforward%2520issues%2520while%2520reserving%2520deeper%2520analytical%2520thinking%2520for%2520more%2520intricate%2520challenges.%2520However%252C%2520existing%2520reasoning-oriented%2520visual%2520language%2520models%2520%2528VLMs%2529%252C%2520whether%2520trained%2520with%2520explicit%2520chain-of-thought%2520annotations%2520or%2520rule-based%2520RL%2520rewards%252C%2520mainly%2520pursue%2520lengthy%252C%2520detailed%2520reasoning%2520chains%252C%2520which%2520often%2520lead%2520to%2520excessive%2520computational%2520costs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520RL%2520approach%252C%2520which%2520enables%2520VLMs%2520to%2520automatically%2520switch%2520between%2520fast%2520and%2520slow%2520thinking%2520modes%2520depending%2520on%2520task%2520difficulty.%2520The%2520approach%2520consists%2520of%2520two%2520stages%253A%2520in%2520the%2520first%2520stage%252C%2520we%2520label%2520data%2520as%2520either%2520requiring%2520fast%2520thinking%2520or%2520slow%2520thinking%2520based%2520on%2520the%2520model%2520output%2520length%252C%2520which%2520is%2520inspired%2520by%2520the%2520observation%2520that%2520pre-trained%2520VLMs%2520typically%2520produce%2520answers%2520of%2520varying%2520lengths%2520for%2520different%2520types%2520of%2520questions%253B%2520in%2520the%2520second%2520stage%252C%2520we%2520train%2520the%2520model%2520using%2520GRPO%2520along%2520with%2520the%2520thinking%2520mode%2520labels%2520to%2520develop%2520dual-mode%2520thinking.%2520Despite%2520its%2520simplicity%252C%2520our%2520model%252C%2520named%2520DualMindVLM%252C%2520significantly%2520outperforms%2520the%2520base%2520model%2520and%2520achieves%2520performance%2520on%2520par%2520with%2520state-of-the-art%2520visual%2520reasoning%2520models%252C%2520while%2520maintaining%2520exceptionally%2520high%2520token%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Think%20Fast%20and%20Slow%20for%20Visual%20Language%20Models&entry.906535625=Chenyu%20Lin%20and%20Cheng%20Chi%20and%20Jinlin%20Wu%20and%20Sharon%20Li%20and%20Kaiyang%20Zhou&entry.1292438233=When%20confronted%20with%20complex%20problems%2C%20we%20tend%20to%20think%20slowly%3B%20conversely%2C%20for%20simple%20questions%2C%20we%20think%20quickly.%20Such%20a%20two-system%20thinking%20mechanism%20allows%20us%20to%20efficiently%20allocate%20cognitive%20resources%2C%20enabling%20quick%20decision-making%20for%20straightforward%20issues%20while%20reserving%20deeper%20analytical%20thinking%20for%20more%20intricate%20challenges.%20However%2C%20existing%20reasoning-oriented%20visual%20language%20models%20%28VLMs%29%2C%20whether%20trained%20with%20explicit%20chain-of-thought%20annotations%20or%20rule-based%20RL%20rewards%2C%20mainly%20pursue%20lengthy%2C%20detailed%20reasoning%20chains%2C%20which%20often%20lead%20to%20excessive%20computational%20costs.%20In%20this%20work%2C%20we%20propose%20a%20simple%20RL%20approach%2C%20which%20enables%20VLMs%20to%20automatically%20switch%20between%20fast%20and%20slow%20thinking%20modes%20depending%20on%20task%20difficulty.%20The%20approach%20consists%20of%20two%20stages%3A%20in%20the%20first%20stage%2C%20we%20label%20data%20as%20either%20requiring%20fast%20thinking%20or%20slow%20thinking%20based%20on%20the%20model%20output%20length%2C%20which%20is%20inspired%20by%20the%20observation%20that%20pre-trained%20VLMs%20typically%20produce%20answers%20of%20varying%20lengths%20for%20different%20types%20of%20questions%3B%20in%20the%20second%20stage%2C%20we%20train%20the%20model%20using%20GRPO%20along%20with%20the%20thinking%20mode%20labels%20to%20develop%20dual-mode%20thinking.%20Despite%20its%20simplicity%2C%20our%20model%2C%20named%20DualMindVLM%2C%20significantly%20outperforms%20the%20base%20model%20and%20achieves%20performance%20on%20par%20with%20state-of-the-art%20visual%20reasoning%20models%2C%20while%20maintaining%20exceptionally%20high%20token%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2511.16670v1&entry.124074799=Read"},
{"title": "Contrastive vision-language learning with paraphrasing and negation", "author": "Kwun Ho Ngan and Saman Sadeghi Afgeh and Joe Townsend and Artur d'Avila Garcez", "abstract": "Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.", "link": "http://arxiv.org/abs/2511.16527v1", "date": "2025-11-20", "relevancy": 2.6508, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20vision-language%20learning%20with%20paraphrasing%20and%20negation&body=Title%3A%20Contrastive%20vision-language%20learning%20with%20paraphrasing%20and%20negation%0AAuthor%3A%20Kwun%20Ho%20Ngan%20and%20Saman%20Sadeghi%20Afgeh%20and%20Joe%20Townsend%20and%20Artur%20d%27Avila%20Garcez%0AAbstract%3A%20Contrastive%20vision-language%20models%20continue%20to%20be%20the%20dominant%20approach%20for%20image%20and%20text%20retrieval.%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20trains%20two%20neural%20networks%20in%20contrastive%20manner%20to%20align%20their%20image%20and%20text%20embeddings%20in%20a%20shared%20latent%20space.%20Recent%20results%20evaluating%20CLIP%20on%20negated%20or%20paraphrased%20text%20have%20shown%20mixed%20performance%20because%20negation%20changes%20meaning%20radically%20with%20minimal%20lexical%20changes%2C%20while%20paraphrasing%20can%20create%20very%20different%20textual%20expressions%20with%20the%20same%20intended%20meaning.%20This%20poses%20a%20significant%20challenge%20for%20improving%20the%20evaluation%20results%20and%20alignment%20of%20vision-language%20models.%20To%20address%20this%20challenge%2C%20this%20paper%20evaluates%20the%20combination%20of%20paraphrasing%20and%20negation%2C%20proposes%20a%20new%20CLIP%20contrastive%20loss%20function%20accounting%20for%20both%20paraphrasing%20and%20negation%2C%20and%20applies%20LLM-generated%20training%20triples%20consisting%20of%20original%2C%20paraphrased%20and%20negated%20textual%20captions%20to%20CLIP-like%20training%20models.%20The%20approach%2C%20called%20SemCLIP%2C%20is%20shown%20to%20move%20paraphrased%20captions%20towards%20the%20original%20image%20embeddings%20while%20pushing%20negated%20captions%20further%20away%20in%20embedding%20space.%20Empirically%2C%20SemCLIP%20is%20shown%20to%20be%20capable%20of%20preserving%20CLIP%27s%20performance%20while%20increasing%20considerably%20the%20distances%20to%20negated%20captions.%20On%20the%20CC-Neg%20benchmark%20using%20an%20original%20over%20negation%20image-retrieval%20accuracy%20metric%2C%20SemCLIP%20improves%20accuracy%20from%2068.1%25%20to%2078.1%25.%20Although%20results%20are%20mixed%20when%20compared%20with%20CLIP%20on%20the%20Sugarcrepe%2B%2B%20benchmark%2C%20SemCLIP%27s%20performance%20is%20generally%20better%20than%20the%20models%20trained%20with%20negated%20captions.%20This%20robustness%20to%20negation%20extends%20to%20downstream%20zero-shot%20classification%20tasks%20where%20SemCLIP%20pre-trained%20on%20Sugarcrepe%2B%2B%20performs%20better%20than%20CLIP%20on%20all%20tested%20downstream%20tasks.%20These%20results%20indicate%20that%20SemCLIP%20can%20achieve%20significant%20robustness%20to%20semantic%20transformations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520vision-language%2520learning%2520with%2520paraphrasing%2520and%2520negation%26entry.906535625%3DKwun%2520Ho%2520Ngan%2520and%2520Saman%2520Sadeghi%2520Afgeh%2520and%2520Joe%2520Townsend%2520and%2520Artur%2520d%2527Avila%2520Garcez%26entry.1292438233%3DContrastive%2520vision-language%2520models%2520continue%2520to%2520be%2520the%2520dominant%2520approach%2520for%2520image%2520and%2520text%2520retrieval.%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520trains%2520two%2520neural%2520networks%2520in%2520contrastive%2520manner%2520to%2520align%2520their%2520image%2520and%2520text%2520embeddings%2520in%2520a%2520shared%2520latent%2520space.%2520Recent%2520results%2520evaluating%2520CLIP%2520on%2520negated%2520or%2520paraphrased%2520text%2520have%2520shown%2520mixed%2520performance%2520because%2520negation%2520changes%2520meaning%2520radically%2520with%2520minimal%2520lexical%2520changes%252C%2520while%2520paraphrasing%2520can%2520create%2520very%2520different%2520textual%2520expressions%2520with%2520the%2520same%2520intended%2520meaning.%2520This%2520poses%2520a%2520significant%2520challenge%2520for%2520improving%2520the%2520evaluation%2520results%2520and%2520alignment%2520of%2520vision-language%2520models.%2520To%2520address%2520this%2520challenge%252C%2520this%2520paper%2520evaluates%2520the%2520combination%2520of%2520paraphrasing%2520and%2520negation%252C%2520proposes%2520a%2520new%2520CLIP%2520contrastive%2520loss%2520function%2520accounting%2520for%2520both%2520paraphrasing%2520and%2520negation%252C%2520and%2520applies%2520LLM-generated%2520training%2520triples%2520consisting%2520of%2520original%252C%2520paraphrased%2520and%2520negated%2520textual%2520captions%2520to%2520CLIP-like%2520training%2520models.%2520The%2520approach%252C%2520called%2520SemCLIP%252C%2520is%2520shown%2520to%2520move%2520paraphrased%2520captions%2520towards%2520the%2520original%2520image%2520embeddings%2520while%2520pushing%2520negated%2520captions%2520further%2520away%2520in%2520embedding%2520space.%2520Empirically%252C%2520SemCLIP%2520is%2520shown%2520to%2520be%2520capable%2520of%2520preserving%2520CLIP%2527s%2520performance%2520while%2520increasing%2520considerably%2520the%2520distances%2520to%2520negated%2520captions.%2520On%2520the%2520CC-Neg%2520benchmark%2520using%2520an%2520original%2520over%2520negation%2520image-retrieval%2520accuracy%2520metric%252C%2520SemCLIP%2520improves%2520accuracy%2520from%252068.1%2525%2520to%252078.1%2525.%2520Although%2520results%2520are%2520mixed%2520when%2520compared%2520with%2520CLIP%2520on%2520the%2520Sugarcrepe%252B%252B%2520benchmark%252C%2520SemCLIP%2527s%2520performance%2520is%2520generally%2520better%2520than%2520the%2520models%2520trained%2520with%2520negated%2520captions.%2520This%2520robustness%2520to%2520negation%2520extends%2520to%2520downstream%2520zero-shot%2520classification%2520tasks%2520where%2520SemCLIP%2520pre-trained%2520on%2520Sugarcrepe%252B%252B%2520performs%2520better%2520than%2520CLIP%2520on%2520all%2520tested%2520downstream%2520tasks.%2520These%2520results%2520indicate%2520that%2520SemCLIP%2520can%2520achieve%2520significant%2520robustness%2520to%2520semantic%2520transformations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20vision-language%20learning%20with%20paraphrasing%20and%20negation&entry.906535625=Kwun%20Ho%20Ngan%20and%20Saman%20Sadeghi%20Afgeh%20and%20Joe%20Townsend%20and%20Artur%20d%27Avila%20Garcez&entry.1292438233=Contrastive%20vision-language%20models%20continue%20to%20be%20the%20dominant%20approach%20for%20image%20and%20text%20retrieval.%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20trains%20two%20neural%20networks%20in%20contrastive%20manner%20to%20align%20their%20image%20and%20text%20embeddings%20in%20a%20shared%20latent%20space.%20Recent%20results%20evaluating%20CLIP%20on%20negated%20or%20paraphrased%20text%20have%20shown%20mixed%20performance%20because%20negation%20changes%20meaning%20radically%20with%20minimal%20lexical%20changes%2C%20while%20paraphrasing%20can%20create%20very%20different%20textual%20expressions%20with%20the%20same%20intended%20meaning.%20This%20poses%20a%20significant%20challenge%20for%20improving%20the%20evaluation%20results%20and%20alignment%20of%20vision-language%20models.%20To%20address%20this%20challenge%2C%20this%20paper%20evaluates%20the%20combination%20of%20paraphrasing%20and%20negation%2C%20proposes%20a%20new%20CLIP%20contrastive%20loss%20function%20accounting%20for%20both%20paraphrasing%20and%20negation%2C%20and%20applies%20LLM-generated%20training%20triples%20consisting%20of%20original%2C%20paraphrased%20and%20negated%20textual%20captions%20to%20CLIP-like%20training%20models.%20The%20approach%2C%20called%20SemCLIP%2C%20is%20shown%20to%20move%20paraphrased%20captions%20towards%20the%20original%20image%20embeddings%20while%20pushing%20negated%20captions%20further%20away%20in%20embedding%20space.%20Empirically%2C%20SemCLIP%20is%20shown%20to%20be%20capable%20of%20preserving%20CLIP%27s%20performance%20while%20increasing%20considerably%20the%20distances%20to%20negated%20captions.%20On%20the%20CC-Neg%20benchmark%20using%20an%20original%20over%20negation%20image-retrieval%20accuracy%20metric%2C%20SemCLIP%20improves%20accuracy%20from%2068.1%25%20to%2078.1%25.%20Although%20results%20are%20mixed%20when%20compared%20with%20CLIP%20on%20the%20Sugarcrepe%2B%2B%20benchmark%2C%20SemCLIP%27s%20performance%20is%20generally%20better%20than%20the%20models%20trained%20with%20negated%20captions.%20This%20robustness%20to%20negation%20extends%20to%20downstream%20zero-shot%20classification%20tasks%20where%20SemCLIP%20pre-trained%20on%20Sugarcrepe%2B%2B%20performs%20better%20than%20CLIP%20on%20all%20tested%20downstream%20tasks.%20These%20results%20indicate%20that%20SemCLIP%20can%20achieve%20significant%20robustness%20to%20semantic%20transformations.&entry.1838667208=http%3A//arxiv.org/abs/2511.16527v1&entry.124074799=Read"},
{"title": "Sparse Autoencoders are Topic Models", "author": "Leander Girrbach and Zeynep Akata", "abstract": "Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.", "link": "http://arxiv.org/abs/2511.16309v1", "date": "2025-11-20", "relevancy": 2.6485, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Autoencoders%20are%20Topic%20Models&body=Title%3A%20Sparse%20Autoencoders%20are%20Topic%20Models%0AAuthor%3A%20Leander%20Girrbach%20and%20Zeynep%20Akata%0AAbstract%3A%20Sparse%20autoencoders%20%28SAEs%29%20are%20used%20to%20analyze%20embeddings%2C%20but%20their%20role%20and%20practical%20value%20are%20debated.%20We%20propose%20a%20new%20perspective%20on%20SAEs%20by%20demonstrating%20that%20they%20can%20be%20naturally%20understood%20as%20topic%20models.%20We%20extend%20Latent%20Dirichlet%20Allocation%20to%20embedding%20spaces%20and%20derive%20the%20SAE%20objective%20as%20a%20maximum%20a%20posteriori%20estimator%20under%20this%20model.%20This%20view%20implies%20SAE%20features%20are%20thematic%20components%20rather%20than%20steerable%20directions.%20Based%20on%20this%2C%20we%20introduce%20SAE-TM%2C%20a%20topic%20modeling%20framework%20that%3A%20%281%29%20trains%20an%20SAE%20to%20learn%20reusable%20topic%20atoms%2C%20%282%29%20interprets%20them%20as%20word%20distributions%20on%20downstream%20data%2C%20and%20%283%29%20merges%20them%20into%20any%20number%20of%20topics%20without%20retraining.%20SAE-TM%20yields%20more%20coherent%20topics%20than%20strong%20baselines%20on%20text%20and%20image%20datasets%20while%20maintaining%20diversity.%20Finally%2C%20we%20analyze%20thematic%20structure%20in%20image%20datasets%20and%20trace%20topic%20changes%20over%20time%20in%20Japanese%20woodblock%20prints.%20Our%20work%20positions%20SAEs%20as%20effective%20tools%20for%20large-scale%20thematic%20analysis%20across%20modalities.%20Code%20and%20data%20will%20be%20released%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Autoencoders%2520are%2520Topic%2520Models%26entry.906535625%3DLeander%2520Girrbach%2520and%2520Zeynep%2520Akata%26entry.1292438233%3DSparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520used%2520to%2520analyze%2520embeddings%252C%2520but%2520their%2520role%2520and%2520practical%2520value%2520are%2520debated.%2520We%2520propose%2520a%2520new%2520perspective%2520on%2520SAEs%2520by%2520demonstrating%2520that%2520they%2520can%2520be%2520naturally%2520understood%2520as%2520topic%2520models.%2520We%2520extend%2520Latent%2520Dirichlet%2520Allocation%2520to%2520embedding%2520spaces%2520and%2520derive%2520the%2520SAE%2520objective%2520as%2520a%2520maximum%2520a%2520posteriori%2520estimator%2520under%2520this%2520model.%2520This%2520view%2520implies%2520SAE%2520features%2520are%2520thematic%2520components%2520rather%2520than%2520steerable%2520directions.%2520Based%2520on%2520this%252C%2520we%2520introduce%2520SAE-TM%252C%2520a%2520topic%2520modeling%2520framework%2520that%253A%2520%25281%2529%2520trains%2520an%2520SAE%2520to%2520learn%2520reusable%2520topic%2520atoms%252C%2520%25282%2529%2520interprets%2520them%2520as%2520word%2520distributions%2520on%2520downstream%2520data%252C%2520and%2520%25283%2529%2520merges%2520them%2520into%2520any%2520number%2520of%2520topics%2520without%2520retraining.%2520SAE-TM%2520yields%2520more%2520coherent%2520topics%2520than%2520strong%2520baselines%2520on%2520text%2520and%2520image%2520datasets%2520while%2520maintaining%2520diversity.%2520Finally%252C%2520we%2520analyze%2520thematic%2520structure%2520in%2520image%2520datasets%2520and%2520trace%2520topic%2520changes%2520over%2520time%2520in%2520Japanese%2520woodblock%2520prints.%2520Our%2520work%2520positions%2520SAEs%2520as%2520effective%2520tools%2520for%2520large-scale%2520thematic%2520analysis%2520across%2520modalities.%2520Code%2520and%2520data%2520will%2520be%2520released%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Autoencoders%20are%20Topic%20Models&entry.906535625=Leander%20Girrbach%20and%20Zeynep%20Akata&entry.1292438233=Sparse%20autoencoders%20%28SAEs%29%20are%20used%20to%20analyze%20embeddings%2C%20but%20their%20role%20and%20practical%20value%20are%20debated.%20We%20propose%20a%20new%20perspective%20on%20SAEs%20by%20demonstrating%20that%20they%20can%20be%20naturally%20understood%20as%20topic%20models.%20We%20extend%20Latent%20Dirichlet%20Allocation%20to%20embedding%20spaces%20and%20derive%20the%20SAE%20objective%20as%20a%20maximum%20a%20posteriori%20estimator%20under%20this%20model.%20This%20view%20implies%20SAE%20features%20are%20thematic%20components%20rather%20than%20steerable%20directions.%20Based%20on%20this%2C%20we%20introduce%20SAE-TM%2C%20a%20topic%20modeling%20framework%20that%3A%20%281%29%20trains%20an%20SAE%20to%20learn%20reusable%20topic%20atoms%2C%20%282%29%20interprets%20them%20as%20word%20distributions%20on%20downstream%20data%2C%20and%20%283%29%20merges%20them%20into%20any%20number%20of%20topics%20without%20retraining.%20SAE-TM%20yields%20more%20coherent%20topics%20than%20strong%20baselines%20on%20text%20and%20image%20datasets%20while%20maintaining%20diversity.%20Finally%2C%20we%20analyze%20thematic%20structure%20in%20image%20datasets%20and%20trace%20topic%20changes%20over%20time%20in%20Japanese%20woodblock%20prints.%20Our%20work%20positions%20SAEs%20as%20effective%20tools%20for%20large-scale%20thematic%20analysis%20across%20modalities.%20Code%20and%20data%20will%20be%20released%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2511.16309v1&entry.124074799=Read"},
{"title": "Cognitive Foundations for Reasoning and Their Manifestation in LLMs", "author": "Priyanka Kargupta and Shuyue Stella Li and Haocheng Wang and Jinu Lee and Shan Chen and Orevaoghene Ahia and Dean Light and Thomas L. Griffiths and Max Kleiman-Weiner and Jiawei Han and Asli Celikyilmaz and Yulia Tsvetkov", "abstract": "Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. We propose a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which we make publicly available. Our analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, we establish a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale.", "link": "http://arxiv.org/abs/2511.16660v1", "date": "2025-11-20", "relevancy": 2.6474, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Foundations%20for%20Reasoning%20and%20Their%20Manifestation%20in%20LLMs&body=Title%3A%20Cognitive%20Foundations%20for%20Reasoning%20and%20Their%20Manifestation%20in%20LLMs%0AAuthor%3A%20Priyanka%20Kargupta%20and%20Shuyue%20Stella%20Li%20and%20Haocheng%20Wang%20and%20Jinu%20Lee%20and%20Shan%20Chen%20and%20Orevaoghene%20Ahia%20and%20Dean%20Light%20and%20Thomas%20L.%20Griffiths%20and%20Max%20Kleiman-Weiner%20and%20Jiawei%20Han%20and%20Asli%20Celikyilmaz%20and%20Yulia%20Tsvetkov%0AAbstract%3A%20Large%20language%20models%20solve%20complex%20problems%20yet%20fail%20on%20simpler%20variants%2C%20suggesting%20they%20achieve%20correct%20outputs%20through%20mechanisms%20fundamentally%20different%20from%20human%20reasoning.%20We%20synthesize%20cognitive%20science%20research%20into%20a%20taxonomy%20of%2028%20cognitive%20elements%20spanning%20computational%20constraints%2C%20meta-cognitive%20controls%2C%20knowledge%20representations%2C%20and%20transformation%20operations%2C%20then%20analyze%20their%20behavioral%20manifestations%20in%20reasoning%20traces.%20We%20propose%20a%20fine-grained%20cognitive%20evaluation%20framework%20and%20conduct%20the%20first%20large-scale%20analysis%20of%20170K%20traces%20from%2017%20models%20across%20text%2C%20vision%2C%20and%20audio%20modalities%2C%20alongside%2054%20human%20think-aloud%20traces%2C%20which%20we%20make%20publicly%20available.%20Our%20analysis%20reveals%20systematic%20structural%20differences%3A%20humans%20employ%20hierarchical%20nesting%20and%20meta-cognitive%20monitoring%20while%20models%20rely%20on%20shallow%20forward%20chaining%2C%20with%20divergence%20most%20pronounced%20on%20ill-structured%20problems.%20Meta-analysis%20of%201%2C598%20LLM%20reasoning%20papers%20reveals%20the%20research%20community%20concentrates%20on%20easily%20quantifiable%20behaviors%20%28sequential%20organization%3A%2055%25%2C%20decomposition%3A%2060%25%29%20while%20neglecting%20meta-cognitive%20controls%20%28self-awareness%3A%2016%25%2C%20evaluation%3A%208%25%29%20that%20correlate%20with%20success.%20Models%20possess%20behavioral%20repertoires%20associated%20with%20success%20but%20fail%20to%20deploy%20them%20spontaneously.%20Leveraging%20these%20patterns%2C%20we%20develop%20test-time%20reasoning%20guidance%20that%20automatically%20scaffold%20successful%20structures%2C%20improving%20performance%20by%20up%20to%2060%25%20on%20complex%20problems.%20By%20bridging%20cognitive%20science%20and%20LLM%20research%2C%20we%20establish%20a%20foundation%20for%20developing%20models%20that%20reason%20through%20principled%20cognitive%20mechanisms%20rather%20than%20brittle%20spurious%20reasoning%20shortcuts%20or%20memorization%2C%20opening%20new%20directions%20for%20both%20improving%20model%20capabilities%20and%20testing%20theories%20of%20human%20cognition%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Foundations%2520for%2520Reasoning%2520and%2520Their%2520Manifestation%2520in%2520LLMs%26entry.906535625%3DPriyanka%2520Kargupta%2520and%2520Shuyue%2520Stella%2520Li%2520and%2520Haocheng%2520Wang%2520and%2520Jinu%2520Lee%2520and%2520Shan%2520Chen%2520and%2520Orevaoghene%2520Ahia%2520and%2520Dean%2520Light%2520and%2520Thomas%2520L.%2520Griffiths%2520and%2520Max%2520Kleiman-Weiner%2520and%2520Jiawei%2520Han%2520and%2520Asli%2520Celikyilmaz%2520and%2520Yulia%2520Tsvetkov%26entry.1292438233%3DLarge%2520language%2520models%2520solve%2520complex%2520problems%2520yet%2520fail%2520on%2520simpler%2520variants%252C%2520suggesting%2520they%2520achieve%2520correct%2520outputs%2520through%2520mechanisms%2520fundamentally%2520different%2520from%2520human%2520reasoning.%2520We%2520synthesize%2520cognitive%2520science%2520research%2520into%2520a%2520taxonomy%2520of%252028%2520cognitive%2520elements%2520spanning%2520computational%2520constraints%252C%2520meta-cognitive%2520controls%252C%2520knowledge%2520representations%252C%2520and%2520transformation%2520operations%252C%2520then%2520analyze%2520their%2520behavioral%2520manifestations%2520in%2520reasoning%2520traces.%2520We%2520propose%2520a%2520fine-grained%2520cognitive%2520evaluation%2520framework%2520and%2520conduct%2520the%2520first%2520large-scale%2520analysis%2520of%2520170K%2520traces%2520from%252017%2520models%2520across%2520text%252C%2520vision%252C%2520and%2520audio%2520modalities%252C%2520alongside%252054%2520human%2520think-aloud%2520traces%252C%2520which%2520we%2520make%2520publicly%2520available.%2520Our%2520analysis%2520reveals%2520systematic%2520structural%2520differences%253A%2520humans%2520employ%2520hierarchical%2520nesting%2520and%2520meta-cognitive%2520monitoring%2520while%2520models%2520rely%2520on%2520shallow%2520forward%2520chaining%252C%2520with%2520divergence%2520most%2520pronounced%2520on%2520ill-structured%2520problems.%2520Meta-analysis%2520of%25201%252C598%2520LLM%2520reasoning%2520papers%2520reveals%2520the%2520research%2520community%2520concentrates%2520on%2520easily%2520quantifiable%2520behaviors%2520%2528sequential%2520organization%253A%252055%2525%252C%2520decomposition%253A%252060%2525%2529%2520while%2520neglecting%2520meta-cognitive%2520controls%2520%2528self-awareness%253A%252016%2525%252C%2520evaluation%253A%25208%2525%2529%2520that%2520correlate%2520with%2520success.%2520Models%2520possess%2520behavioral%2520repertoires%2520associated%2520with%2520success%2520but%2520fail%2520to%2520deploy%2520them%2520spontaneously.%2520Leveraging%2520these%2520patterns%252C%2520we%2520develop%2520test-time%2520reasoning%2520guidance%2520that%2520automatically%2520scaffold%2520successful%2520structures%252C%2520improving%2520performance%2520by%2520up%2520to%252060%2525%2520on%2520complex%2520problems.%2520By%2520bridging%2520cognitive%2520science%2520and%2520LLM%2520research%252C%2520we%2520establish%2520a%2520foundation%2520for%2520developing%2520models%2520that%2520reason%2520through%2520principled%2520cognitive%2520mechanisms%2520rather%2520than%2520brittle%2520spurious%2520reasoning%2520shortcuts%2520or%2520memorization%252C%2520opening%2520new%2520directions%2520for%2520both%2520improving%2520model%2520capabilities%2520and%2520testing%2520theories%2520of%2520human%2520cognition%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Foundations%20for%20Reasoning%20and%20Their%20Manifestation%20in%20LLMs&entry.906535625=Priyanka%20Kargupta%20and%20Shuyue%20Stella%20Li%20and%20Haocheng%20Wang%20and%20Jinu%20Lee%20and%20Shan%20Chen%20and%20Orevaoghene%20Ahia%20and%20Dean%20Light%20and%20Thomas%20L.%20Griffiths%20and%20Max%20Kleiman-Weiner%20and%20Jiawei%20Han%20and%20Asli%20Celikyilmaz%20and%20Yulia%20Tsvetkov&entry.1292438233=Large%20language%20models%20solve%20complex%20problems%20yet%20fail%20on%20simpler%20variants%2C%20suggesting%20they%20achieve%20correct%20outputs%20through%20mechanisms%20fundamentally%20different%20from%20human%20reasoning.%20We%20synthesize%20cognitive%20science%20research%20into%20a%20taxonomy%20of%2028%20cognitive%20elements%20spanning%20computational%20constraints%2C%20meta-cognitive%20controls%2C%20knowledge%20representations%2C%20and%20transformation%20operations%2C%20then%20analyze%20their%20behavioral%20manifestations%20in%20reasoning%20traces.%20We%20propose%20a%20fine-grained%20cognitive%20evaluation%20framework%20and%20conduct%20the%20first%20large-scale%20analysis%20of%20170K%20traces%20from%2017%20models%20across%20text%2C%20vision%2C%20and%20audio%20modalities%2C%20alongside%2054%20human%20think-aloud%20traces%2C%20which%20we%20make%20publicly%20available.%20Our%20analysis%20reveals%20systematic%20structural%20differences%3A%20humans%20employ%20hierarchical%20nesting%20and%20meta-cognitive%20monitoring%20while%20models%20rely%20on%20shallow%20forward%20chaining%2C%20with%20divergence%20most%20pronounced%20on%20ill-structured%20problems.%20Meta-analysis%20of%201%2C598%20LLM%20reasoning%20papers%20reveals%20the%20research%20community%20concentrates%20on%20easily%20quantifiable%20behaviors%20%28sequential%20organization%3A%2055%25%2C%20decomposition%3A%2060%25%29%20while%20neglecting%20meta-cognitive%20controls%20%28self-awareness%3A%2016%25%2C%20evaluation%3A%208%25%29%20that%20correlate%20with%20success.%20Models%20possess%20behavioral%20repertoires%20associated%20with%20success%20but%20fail%20to%20deploy%20them%20spontaneously.%20Leveraging%20these%20patterns%2C%20we%20develop%20test-time%20reasoning%20guidance%20that%20automatically%20scaffold%20successful%20structures%2C%20improving%20performance%20by%20up%20to%2060%25%20on%20complex%20problems.%20By%20bridging%20cognitive%20science%20and%20LLM%20research%2C%20we%20establish%20a%20foundation%20for%20developing%20models%20that%20reason%20through%20principled%20cognitive%20mechanisms%20rather%20than%20brittle%20spurious%20reasoning%20shortcuts%20or%20memorization%2C%20opening%20new%20directions%20for%20both%20improving%20model%20capabilities%20and%20testing%20theories%20of%20human%20cognition%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2511.16660v1&entry.124074799=Read"},
{"title": "PrIntMesh: Precise Intersection Surfaces for 3D Organ Mesh Reconstruction", "author": "Deniz Sayin Mercadier and Hieu Le and Yihong Chen and Jiancheng Yang and Udaranga Wickramasinghe and Pascal Fua", "abstract": "Human organs are composed of interconnected substructures whose geometry and spatial relationships constrain one another. Yet, most deep-learning approaches treat these parts independently, producing anatomically implausible reconstructions. We introduce PrIntMesh, a template-based, topology-preserving framework that reconstructs organs as unified systems. Starting from a connected template, PrIntMesh jointly deforms all substructures to match patient-specific anatomy, while explicitly preserving internal boundaries and enforcing smooth, artifact-free surfaces. We demonstrate its effectiveness on the heart, hippocampus, and lungs, achieving high geometric accuracy, correct topology, and robust performance even with limited or noisy training data. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency, and provides a data-efficient solution suitable for clinical use.", "link": "http://arxiv.org/abs/2511.16186v1", "date": "2025-11-20", "relevancy": 2.625, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5425}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5304}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrIntMesh%3A%20Precise%20Intersection%20Surfaces%20for%203D%20Organ%20Mesh%20Reconstruction&body=Title%3A%20PrIntMesh%3A%20Precise%20Intersection%20Surfaces%20for%203D%20Organ%20Mesh%20Reconstruction%0AAuthor%3A%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Udaranga%20Wickramasinghe%20and%20Pascal%20Fua%0AAbstract%3A%20Human%20organs%20are%20composed%20of%20interconnected%20substructures%20whose%20geometry%20and%20spatial%20relationships%20constrain%20one%20another.%20Yet%2C%20most%20deep-learning%20approaches%20treat%20these%20parts%20independently%2C%20producing%20anatomically%20implausible%20reconstructions.%20We%20introduce%20PrIntMesh%2C%20a%20template-based%2C%20topology-preserving%20framework%20that%20reconstructs%20organs%20as%20unified%20systems.%20Starting%20from%20a%20connected%20template%2C%20PrIntMesh%20jointly%20deforms%20all%20substructures%20to%20match%20patient-specific%20anatomy%2C%20while%20explicitly%20preserving%20internal%20boundaries%20and%20enforcing%20smooth%2C%20artifact-free%20surfaces.%20We%20demonstrate%20its%20effectiveness%20on%20the%20heart%2C%20hippocampus%2C%20and%20lungs%2C%20achieving%20high%20geometric%20accuracy%2C%20correct%20topology%2C%20and%20robust%20performance%20even%20with%20limited%20or%20noisy%20training%20data.%20Compared%20to%20voxel-%20and%20surface-based%20methods%2C%20PrIntMesh%20better%20reconstructs%20shared%20interfaces%2C%20maintains%20structural%20consistency%2C%20and%20provides%20a%20data-efficient%20solution%20suitable%20for%20clinical%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrIntMesh%253A%2520Precise%2520Intersection%2520Surfaces%2520for%25203D%2520Organ%2520Mesh%2520Reconstruction%26entry.906535625%3DDeniz%2520Sayin%2520Mercadier%2520and%2520Hieu%2520Le%2520and%2520Yihong%2520Chen%2520and%2520Jiancheng%2520Yang%2520and%2520Udaranga%2520Wickramasinghe%2520and%2520Pascal%2520Fua%26entry.1292438233%3DHuman%2520organs%2520are%2520composed%2520of%2520interconnected%2520substructures%2520whose%2520geometry%2520and%2520spatial%2520relationships%2520constrain%2520one%2520another.%2520Yet%252C%2520most%2520deep-learning%2520approaches%2520treat%2520these%2520parts%2520independently%252C%2520producing%2520anatomically%2520implausible%2520reconstructions.%2520We%2520introduce%2520PrIntMesh%252C%2520a%2520template-based%252C%2520topology-preserving%2520framework%2520that%2520reconstructs%2520organs%2520as%2520unified%2520systems.%2520Starting%2520from%2520a%2520connected%2520template%252C%2520PrIntMesh%2520jointly%2520deforms%2520all%2520substructures%2520to%2520match%2520patient-specific%2520anatomy%252C%2520while%2520explicitly%2520preserving%2520internal%2520boundaries%2520and%2520enforcing%2520smooth%252C%2520artifact-free%2520surfaces.%2520We%2520demonstrate%2520its%2520effectiveness%2520on%2520the%2520heart%252C%2520hippocampus%252C%2520and%2520lungs%252C%2520achieving%2520high%2520geometric%2520accuracy%252C%2520correct%2520topology%252C%2520and%2520robust%2520performance%2520even%2520with%2520limited%2520or%2520noisy%2520training%2520data.%2520Compared%2520to%2520voxel-%2520and%2520surface-based%2520methods%252C%2520PrIntMesh%2520better%2520reconstructs%2520shared%2520interfaces%252C%2520maintains%2520structural%2520consistency%252C%2520and%2520provides%2520a%2520data-efficient%2520solution%2520suitable%2520for%2520clinical%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrIntMesh%3A%20Precise%20Intersection%20Surfaces%20for%203D%20Organ%20Mesh%20Reconstruction&entry.906535625=Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Udaranga%20Wickramasinghe%20and%20Pascal%20Fua&entry.1292438233=Human%20organs%20are%20composed%20of%20interconnected%20substructures%20whose%20geometry%20and%20spatial%20relationships%20constrain%20one%20another.%20Yet%2C%20most%20deep-learning%20approaches%20treat%20these%20parts%20independently%2C%20producing%20anatomically%20implausible%20reconstructions.%20We%20introduce%20PrIntMesh%2C%20a%20template-based%2C%20topology-preserving%20framework%20that%20reconstructs%20organs%20as%20unified%20systems.%20Starting%20from%20a%20connected%20template%2C%20PrIntMesh%20jointly%20deforms%20all%20substructures%20to%20match%20patient-specific%20anatomy%2C%20while%20explicitly%20preserving%20internal%20boundaries%20and%20enforcing%20smooth%2C%20artifact-free%20surfaces.%20We%20demonstrate%20its%20effectiveness%20on%20the%20heart%2C%20hippocampus%2C%20and%20lungs%2C%20achieving%20high%20geometric%20accuracy%2C%20correct%20topology%2C%20and%20robust%20performance%20even%20with%20limited%20or%20noisy%20training%20data.%20Compared%20to%20voxel-%20and%20surface-based%20methods%2C%20PrIntMesh%20better%20reconstructs%20shared%20interfaces%2C%20maintains%20structural%20consistency%2C%20and%20provides%20a%20data-efficient%20solution%20suitable%20for%20clinical%20use.&entry.1838667208=http%3A//arxiv.org/abs/2511.16186v1&entry.124074799=Read"},
{"title": "Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models", "author": "Shuang Liang and Zhihao Xu and Jialing Tao and Hui Xue and Xiting Wang", "abstract": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs) remain vulnerable to jailbreak attacks, posing serious safety risks. To address this, existing detection methods either learn attack-specific parameters, which hinders generalization to unseen attacks, or rely on heuristically sound principles, which limit accuracy and efficiency. To overcome these limitations, we propose Learning to Detect (LoD), a general framework that accurately detects unknown jailbreak attacks by shifting the focus from attack-specific learning to task-specific learning. This framework includes a Multi-modal Safety Concept Activation Vector module for safety-oriented representation learning and a Safety Pattern Auto-Encoder module for unsupervised attack classification. Extensive experiments show that our method achieves consistently higher detection AUROC on diverse unknown attacks while improving efficiency. The code is available at https://anonymous.4open.science/r/Learning-to-Detect-51CB.", "link": "http://arxiv.org/abs/2508.09201v3", "date": "2025-11-20", "relevancy": 2.6244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Detect%20Unknown%20Jailbreak%20Attacks%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Learning%20to%20Detect%20Unknown%20Jailbreak%20Attacks%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Shuang%20Liang%20and%20Zhihao%20Xu%20and%20Jialing%20Tao%20and%20Hui%20Xue%20and%20Xiting%20Wang%0AAbstract%3A%20Despite%20extensive%20alignment%20efforts%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20remain%20vulnerable%20to%20jailbreak%20attacks%2C%20posing%20serious%20safety%20risks.%20To%20address%20this%2C%20existing%20detection%20methods%20either%20learn%20attack-specific%20parameters%2C%20which%20hinders%20generalization%20to%20unseen%20attacks%2C%20or%20rely%20on%20heuristically%20sound%20principles%2C%20which%20limit%20accuracy%20and%20efficiency.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Learning%20to%20Detect%20%28LoD%29%2C%20a%20general%20framework%20that%20accurately%20detects%20unknown%20jailbreak%20attacks%20by%20shifting%20the%20focus%20from%20attack-specific%20learning%20to%20task-specific%20learning.%20This%20framework%20includes%20a%20Multi-modal%20Safety%20Concept%20Activation%20Vector%20module%20for%20safety-oriented%20representation%20learning%20and%20a%20Safety%20Pattern%20Auto-Encoder%20module%20for%20unsupervised%20attack%20classification.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%20consistently%20higher%20detection%20AUROC%20on%20diverse%20unknown%20attacks%20while%20improving%20efficiency.%20The%20code%20is%20available%20at%20https%3A//anonymous.4open.science/r/Learning-to-Detect-51CB.%0ALink%3A%20http%3A//arxiv.org/abs/2508.09201v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Detect%2520Unknown%2520Jailbreak%2520Attacks%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DShuang%2520Liang%2520and%2520Zhihao%2520Xu%2520and%2520Jialing%2520Tao%2520and%2520Hui%2520Xue%2520and%2520Xiting%2520Wang%26entry.1292438233%3DDespite%2520extensive%2520alignment%2520efforts%252C%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520remain%2520vulnerable%2520to%2520jailbreak%2520attacks%252C%2520posing%2520serious%2520safety%2520risks.%2520To%2520address%2520this%252C%2520existing%2520detection%2520methods%2520either%2520learn%2520attack-specific%2520parameters%252C%2520which%2520hinders%2520generalization%2520to%2520unseen%2520attacks%252C%2520or%2520rely%2520on%2520heuristically%2520sound%2520principles%252C%2520which%2520limit%2520accuracy%2520and%2520efficiency.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Learning%2520to%2520Detect%2520%2528LoD%2529%252C%2520a%2520general%2520framework%2520that%2520accurately%2520detects%2520unknown%2520jailbreak%2520attacks%2520by%2520shifting%2520the%2520focus%2520from%2520attack-specific%2520learning%2520to%2520task-specific%2520learning.%2520This%2520framework%2520includes%2520a%2520Multi-modal%2520Safety%2520Concept%2520Activation%2520Vector%2520module%2520for%2520safety-oriented%2520representation%2520learning%2520and%2520a%2520Safety%2520Pattern%2520Auto-Encoder%2520module%2520for%2520unsupervised%2520attack%2520classification.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520achieves%2520consistently%2520higher%2520detection%2520AUROC%2520on%2520diverse%2520unknown%2520attacks%2520while%2520improving%2520efficiency.%2520The%2520code%2520is%2520available%2520at%2520https%253A//anonymous.4open.science/r/Learning-to-Detect-51CB.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09201v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Detect%20Unknown%20Jailbreak%20Attacks%20in%20Large%20Vision-Language%20Models&entry.906535625=Shuang%20Liang%20and%20Zhihao%20Xu%20and%20Jialing%20Tao%20and%20Hui%20Xue%20and%20Xiting%20Wang&entry.1292438233=Despite%20extensive%20alignment%20efforts%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20remain%20vulnerable%20to%20jailbreak%20attacks%2C%20posing%20serious%20safety%20risks.%20To%20address%20this%2C%20existing%20detection%20methods%20either%20learn%20attack-specific%20parameters%2C%20which%20hinders%20generalization%20to%20unseen%20attacks%2C%20or%20rely%20on%20heuristically%20sound%20principles%2C%20which%20limit%20accuracy%20and%20efficiency.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Learning%20to%20Detect%20%28LoD%29%2C%20a%20general%20framework%20that%20accurately%20detects%20unknown%20jailbreak%20attacks%20by%20shifting%20the%20focus%20from%20attack-specific%20learning%20to%20task-specific%20learning.%20This%20framework%20includes%20a%20Multi-modal%20Safety%20Concept%20Activation%20Vector%20module%20for%20safety-oriented%20representation%20learning%20and%20a%20Safety%20Pattern%20Auto-Encoder%20module%20for%20unsupervised%20attack%20classification.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%20consistently%20higher%20detection%20AUROC%20on%20diverse%20unknown%20attacks%20while%20improving%20efficiency.%20The%20code%20is%20available%20at%20https%3A//anonymous.4open.science/r/Learning-to-Detect-51CB.&entry.1838667208=http%3A//arxiv.org/abs/2508.09201v3&entry.124074799=Read"},
{"title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking", "author": "Haofeng Liu and Ziyue Wang and Sudhanshu Mishra and Mingqi Gao and Guanyi Qin and Chang Han Low and Alex Y. W. Kong and Yueming Jin", "abstract": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \\textbf{SAM2} for \\textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\\mathcal{J}$\\&$\\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\\mathcal{J}$\\&$\\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.", "link": "http://arxiv.org/abs/2511.16618v1", "date": "2025-11-20", "relevancy": 2.6196, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5385}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM2S%3A%20Segment%20Anything%20in%20Surgical%20Videos%20via%20Semantic%20Long-term%20Tracking&body=Title%3A%20SAM2S%3A%20Segment%20Anything%20in%20Surgical%20Videos%20via%20Semantic%20Long-term%20Tracking%0AAuthor%3A%20Haofeng%20Liu%20and%20Ziyue%20Wang%20and%20Sudhanshu%20Mishra%20and%20Mingqi%20Gao%20and%20Guanyi%20Qin%20and%20Chang%20Han%20Low%20and%20Alex%20Y.%20W.%20Kong%20and%20Yueming%20Jin%0AAbstract%3A%20Surgical%20video%20segmentation%20is%20crucial%20for%20computer-assisted%20surgery%2C%20enabling%20precise%20localization%20and%20tracking%20of%20instruments%20and%20tissues.%20Interactive%20Video%20Object%20Segmentation%20%28iVOS%29%20models%20such%20as%20Segment%20Anything%20Model%202%20%28SAM2%29%20provide%20prompt-based%20flexibility%20beyond%20methods%20with%20predefined%20categories%2C%20but%20face%20challenges%20in%20surgical%20scenarios%20due%20to%20the%20domain%20gap%20and%20limited%20long-term%20tracking.%20To%20address%20these%20limitations%2C%20we%20construct%20SA-SV%2C%20the%20largest%20surgical%20iVOS%20benchmark%20with%20instance-level%20spatio-temporal%20annotations%20%28masklets%29%20spanning%20eight%20procedure%20types%20%2861k%20frames%2C%201.6k%20masklets%29%2C%20enabling%20comprehensive%20development%20and%20evaluation%20for%20long-term%20tracking%20and%20zero-shot%20generalization.%20Building%20on%20SA-SV%2C%20we%20propose%20SAM2S%2C%20a%20foundation%20model%20enhancing%20%5Ctextbf%7BSAM2%7D%20for%20%5Ctextbf%7BS%7Durgical%20iVOS%20through%3A%20%281%29%20DiveMem%2C%20a%20trainable%20diverse%20memory%20mechanism%20for%20robust%20long-term%20tracking%3B%20%282%29%20temporal%20semantic%20learning%20for%20instrument%20understanding%3B%20and%20%283%29%20ambiguity-resilient%20learning%20to%20mitigate%20annotation%20inconsistencies%20across%20multi-source%20datasets.%20Extensive%20experiments%20demonstrate%20that%20fine-tuning%20on%20SA-SV%20enables%20substantial%20performance%20gains%2C%20with%20SAM2%20improving%20by%2012.99%20average%20%24%5Cmathcal%7BJ%7D%24%5C%26%24%5Cmathcal%7BF%7D%24%20over%20vanilla%20SAM2.%20SAM2S%20further%20advances%20performance%20to%2080.42%20average%20%24%5Cmathcal%7BJ%7D%24%5C%26%24%5Cmathcal%7BF%7D%24%2C%20surpassing%20vanilla%20and%20fine-tuned%20SAM2%20by%2017.10%20and%204.11%20points%20respectively%2C%20while%20maintaining%2068%20FPS%20real-time%20inference%20and%20strong%20zero-shot%20generalization.%20Code%20and%20dataset%20will%20be%20released%20at%20https%3A//jinlab-imvr.github.io/SAM2S.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM2S%253A%2520Segment%2520Anything%2520in%2520Surgical%2520Videos%2520via%2520Semantic%2520Long-term%2520Tracking%26entry.906535625%3DHaofeng%2520Liu%2520and%2520Ziyue%2520Wang%2520and%2520Sudhanshu%2520Mishra%2520and%2520Mingqi%2520Gao%2520and%2520Guanyi%2520Qin%2520and%2520Chang%2520Han%2520Low%2520and%2520Alex%2520Y.%2520W.%2520Kong%2520and%2520Yueming%2520Jin%26entry.1292438233%3DSurgical%2520video%2520segmentation%2520is%2520crucial%2520for%2520computer-assisted%2520surgery%252C%2520enabling%2520precise%2520localization%2520and%2520tracking%2520of%2520instruments%2520and%2520tissues.%2520Interactive%2520Video%2520Object%2520Segmentation%2520%2528iVOS%2529%2520models%2520such%2520as%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520provide%2520prompt-based%2520flexibility%2520beyond%2520methods%2520with%2520predefined%2520categories%252C%2520but%2520face%2520challenges%2520in%2520surgical%2520scenarios%2520due%2520to%2520the%2520domain%2520gap%2520and%2520limited%2520long-term%2520tracking.%2520To%2520address%2520these%2520limitations%252C%2520we%2520construct%2520SA-SV%252C%2520the%2520largest%2520surgical%2520iVOS%2520benchmark%2520with%2520instance-level%2520spatio-temporal%2520annotations%2520%2528masklets%2529%2520spanning%2520eight%2520procedure%2520types%2520%252861k%2520frames%252C%25201.6k%2520masklets%2529%252C%2520enabling%2520comprehensive%2520development%2520and%2520evaluation%2520for%2520long-term%2520tracking%2520and%2520zero-shot%2520generalization.%2520Building%2520on%2520SA-SV%252C%2520we%2520propose%2520SAM2S%252C%2520a%2520foundation%2520model%2520enhancing%2520%255Ctextbf%257BSAM2%257D%2520for%2520%255Ctextbf%257BS%257Durgical%2520iVOS%2520through%253A%2520%25281%2529%2520DiveMem%252C%2520a%2520trainable%2520diverse%2520memory%2520mechanism%2520for%2520robust%2520long-term%2520tracking%253B%2520%25282%2529%2520temporal%2520semantic%2520learning%2520for%2520instrument%2520understanding%253B%2520and%2520%25283%2529%2520ambiguity-resilient%2520learning%2520to%2520mitigate%2520annotation%2520inconsistencies%2520across%2520multi-source%2520datasets.%2520Extensive%2520experiments%2520demonstrate%2520that%2520fine-tuning%2520on%2520SA-SV%2520enables%2520substantial%2520performance%2520gains%252C%2520with%2520SAM2%2520improving%2520by%252012.99%2520average%2520%2524%255Cmathcal%257BJ%257D%2524%255C%2526%2524%255Cmathcal%257BF%257D%2524%2520over%2520vanilla%2520SAM2.%2520SAM2S%2520further%2520advances%2520performance%2520to%252080.42%2520average%2520%2524%255Cmathcal%257BJ%257D%2524%255C%2526%2524%255Cmathcal%257BF%257D%2524%252C%2520surpassing%2520vanilla%2520and%2520fine-tuned%2520SAM2%2520by%252017.10%2520and%25204.11%2520points%2520respectively%252C%2520while%2520maintaining%252068%2520FPS%2520real-time%2520inference%2520and%2520strong%2520zero-shot%2520generalization.%2520Code%2520and%2520dataset%2520will%2520be%2520released%2520at%2520https%253A//jinlab-imvr.github.io/SAM2S.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM2S%3A%20Segment%20Anything%20in%20Surgical%20Videos%20via%20Semantic%20Long-term%20Tracking&entry.906535625=Haofeng%20Liu%20and%20Ziyue%20Wang%20and%20Sudhanshu%20Mishra%20and%20Mingqi%20Gao%20and%20Guanyi%20Qin%20and%20Chang%20Han%20Low%20and%20Alex%20Y.%20W.%20Kong%20and%20Yueming%20Jin&entry.1292438233=Surgical%20video%20segmentation%20is%20crucial%20for%20computer-assisted%20surgery%2C%20enabling%20precise%20localization%20and%20tracking%20of%20instruments%20and%20tissues.%20Interactive%20Video%20Object%20Segmentation%20%28iVOS%29%20models%20such%20as%20Segment%20Anything%20Model%202%20%28SAM2%29%20provide%20prompt-based%20flexibility%20beyond%20methods%20with%20predefined%20categories%2C%20but%20face%20challenges%20in%20surgical%20scenarios%20due%20to%20the%20domain%20gap%20and%20limited%20long-term%20tracking.%20To%20address%20these%20limitations%2C%20we%20construct%20SA-SV%2C%20the%20largest%20surgical%20iVOS%20benchmark%20with%20instance-level%20spatio-temporal%20annotations%20%28masklets%29%20spanning%20eight%20procedure%20types%20%2861k%20frames%2C%201.6k%20masklets%29%2C%20enabling%20comprehensive%20development%20and%20evaluation%20for%20long-term%20tracking%20and%20zero-shot%20generalization.%20Building%20on%20SA-SV%2C%20we%20propose%20SAM2S%2C%20a%20foundation%20model%20enhancing%20%5Ctextbf%7BSAM2%7D%20for%20%5Ctextbf%7BS%7Durgical%20iVOS%20through%3A%20%281%29%20DiveMem%2C%20a%20trainable%20diverse%20memory%20mechanism%20for%20robust%20long-term%20tracking%3B%20%282%29%20temporal%20semantic%20learning%20for%20instrument%20understanding%3B%20and%20%283%29%20ambiguity-resilient%20learning%20to%20mitigate%20annotation%20inconsistencies%20across%20multi-source%20datasets.%20Extensive%20experiments%20demonstrate%20that%20fine-tuning%20on%20SA-SV%20enables%20substantial%20performance%20gains%2C%20with%20SAM2%20improving%20by%2012.99%20average%20%24%5Cmathcal%7BJ%7D%24%5C%26%24%5Cmathcal%7BF%7D%24%20over%20vanilla%20SAM2.%20SAM2S%20further%20advances%20performance%20to%2080.42%20average%20%24%5Cmathcal%7BJ%7D%24%5C%26%24%5Cmathcal%7BF%7D%24%2C%20surpassing%20vanilla%20and%20fine-tuned%20SAM2%20by%2017.10%20and%204.11%20points%20respectively%2C%20while%20maintaining%2068%20FPS%20real-time%20inference%20and%20strong%20zero-shot%20generalization.%20Code%20and%20dataset%20will%20be%20released%20at%20https%3A//jinlab-imvr.github.io/SAM2S.&entry.1838667208=http%3A//arxiv.org/abs/2511.16618v1&entry.124074799=Read"},
{"title": "Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI", "author": "Mahdi Alehdaghi and Rajarshi Bhattacharya and Pourya Shamsolmoali and Rafael M. O. Cruz and Maguelonne Heritier and Eric Granger", "abstract": "As AI systems grow more capable, it becomes increasingly important that their decisions remain understandable and aligned with human expectations. A key challenge is the limited interpretability of deep models. Post-hoc methods like GradCAM offer heatmaps but provide limited conceptual insight, while prototype-based approaches offer example-based explanations but often rely on rigid region selection and lack semantic consistency.\n  To address these limitations, we propose PCMNet, a part-prototypical concept mining network that learns human-comprehensible prototypes from meaningful image regions without additional supervision. By clustering these prototypes into concept groups and extracting concept activation vectors, PCMNet provides structured, concept-level explanations and enhances robustness to occlusion and challenging conditions, which are both critical for building reliable and aligned AI systems.\n  Experiments across multiple image classification benchmarks show that PCMNet outperforms state-of-the-art methods in interpretability, stability, and robustness. This work contributes to AI alignment by enhancing transparency, controllability, and trustworthiness in AI systems. Our code is available at: https://github.com/alehdaghi/PCMNet.", "link": "http://arxiv.org/abs/2504.12197v3", "date": "2025-11-20", "relevancy": 2.6086, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5268}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Patches%3A%20Mining%20Interpretable%20Part-Prototypes%20for%20Explainable%20AI&body=Title%3A%20Beyond%20Patches%3A%20Mining%20Interpretable%20Part-Prototypes%20for%20Explainable%20AI%0AAuthor%3A%20Mahdi%20Alehdaghi%20and%20Rajarshi%20Bhattacharya%20and%20Pourya%20Shamsolmoali%20and%20Rafael%20M.%20O.%20Cruz%20and%20Maguelonne%20Heritier%20and%20Eric%20Granger%0AAbstract%3A%20As%20AI%20systems%20grow%20more%20capable%2C%20it%20becomes%20increasingly%20important%20that%20their%20decisions%20remain%20understandable%20and%20aligned%20with%20human%20expectations.%20A%20key%20challenge%20is%20the%20limited%20interpretability%20of%20deep%20models.%20Post-hoc%20methods%20like%20GradCAM%20offer%20heatmaps%20but%20provide%20limited%20conceptual%20insight%2C%20while%20prototype-based%20approaches%20offer%20example-based%20explanations%20but%20often%20rely%20on%20rigid%20region%20selection%20and%20lack%20semantic%20consistency.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20PCMNet%2C%20a%20part-prototypical%20concept%20mining%20network%20that%20learns%20human-comprehensible%20prototypes%20from%20meaningful%20image%20regions%20without%20additional%20supervision.%20By%20clustering%20these%20prototypes%20into%20concept%20groups%20and%20extracting%20concept%20activation%20vectors%2C%20PCMNet%20provides%20structured%2C%20concept-level%20explanations%20and%20enhances%20robustness%20to%20occlusion%20and%20challenging%20conditions%2C%20which%20are%20both%20critical%20for%20building%20reliable%20and%20aligned%20AI%20systems.%0A%20%20Experiments%20across%20multiple%20image%20classification%20benchmarks%20show%20that%20PCMNet%20outperforms%20state-of-the-art%20methods%20in%20interpretability%2C%20stability%2C%20and%20robustness.%20This%20work%20contributes%20to%20AI%20alignment%20by%20enhancing%20transparency%2C%20controllability%2C%20and%20trustworthiness%20in%20AI%20systems.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/alehdaghi/PCMNet.%0ALink%3A%20http%3A//arxiv.org/abs/2504.12197v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Patches%253A%2520Mining%2520Interpretable%2520Part-Prototypes%2520for%2520Explainable%2520AI%26entry.906535625%3DMahdi%2520Alehdaghi%2520and%2520Rajarshi%2520Bhattacharya%2520and%2520Pourya%2520Shamsolmoali%2520and%2520Rafael%2520M.%2520O.%2520Cruz%2520and%2520Maguelonne%2520Heritier%2520and%2520Eric%2520Granger%26entry.1292438233%3DAs%2520AI%2520systems%2520grow%2520more%2520capable%252C%2520it%2520becomes%2520increasingly%2520important%2520that%2520their%2520decisions%2520remain%2520understandable%2520and%2520aligned%2520with%2520human%2520expectations.%2520A%2520key%2520challenge%2520is%2520the%2520limited%2520interpretability%2520of%2520deep%2520models.%2520Post-hoc%2520methods%2520like%2520GradCAM%2520offer%2520heatmaps%2520but%2520provide%2520limited%2520conceptual%2520insight%252C%2520while%2520prototype-based%2520approaches%2520offer%2520example-based%2520explanations%2520but%2520often%2520rely%2520on%2520rigid%2520region%2520selection%2520and%2520lack%2520semantic%2520consistency.%250A%2520%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520PCMNet%252C%2520a%2520part-prototypical%2520concept%2520mining%2520network%2520that%2520learns%2520human-comprehensible%2520prototypes%2520from%2520meaningful%2520image%2520regions%2520without%2520additional%2520supervision.%2520By%2520clustering%2520these%2520prototypes%2520into%2520concept%2520groups%2520and%2520extracting%2520concept%2520activation%2520vectors%252C%2520PCMNet%2520provides%2520structured%252C%2520concept-level%2520explanations%2520and%2520enhances%2520robustness%2520to%2520occlusion%2520and%2520challenging%2520conditions%252C%2520which%2520are%2520both%2520critical%2520for%2520building%2520reliable%2520and%2520aligned%2520AI%2520systems.%250A%2520%2520Experiments%2520across%2520multiple%2520image%2520classification%2520benchmarks%2520show%2520that%2520PCMNet%2520outperforms%2520state-of-the-art%2520methods%2520in%2520interpretability%252C%2520stability%252C%2520and%2520robustness.%2520This%2520work%2520contributes%2520to%2520AI%2520alignment%2520by%2520enhancing%2520transparency%252C%2520controllability%252C%2520and%2520trustworthiness%2520in%2520AI%2520systems.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/alehdaghi/PCMNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12197v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Patches%3A%20Mining%20Interpretable%20Part-Prototypes%20for%20Explainable%20AI&entry.906535625=Mahdi%20Alehdaghi%20and%20Rajarshi%20Bhattacharya%20and%20Pourya%20Shamsolmoali%20and%20Rafael%20M.%20O.%20Cruz%20and%20Maguelonne%20Heritier%20and%20Eric%20Granger&entry.1292438233=As%20AI%20systems%20grow%20more%20capable%2C%20it%20becomes%20increasingly%20important%20that%20their%20decisions%20remain%20understandable%20and%20aligned%20with%20human%20expectations.%20A%20key%20challenge%20is%20the%20limited%20interpretability%20of%20deep%20models.%20Post-hoc%20methods%20like%20GradCAM%20offer%20heatmaps%20but%20provide%20limited%20conceptual%20insight%2C%20while%20prototype-based%20approaches%20offer%20example-based%20explanations%20but%20often%20rely%20on%20rigid%20region%20selection%20and%20lack%20semantic%20consistency.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20PCMNet%2C%20a%20part-prototypical%20concept%20mining%20network%20that%20learns%20human-comprehensible%20prototypes%20from%20meaningful%20image%20regions%20without%20additional%20supervision.%20By%20clustering%20these%20prototypes%20into%20concept%20groups%20and%20extracting%20concept%20activation%20vectors%2C%20PCMNet%20provides%20structured%2C%20concept-level%20explanations%20and%20enhances%20robustness%20to%20occlusion%20and%20challenging%20conditions%2C%20which%20are%20both%20critical%20for%20building%20reliable%20and%20aligned%20AI%20systems.%0A%20%20Experiments%20across%20multiple%20image%20classification%20benchmarks%20show%20that%20PCMNet%20outperforms%20state-of-the-art%20methods%20in%20interpretability%2C%20stability%2C%20and%20robustness.%20This%20work%20contributes%20to%20AI%20alignment%20by%20enhancing%20transparency%2C%20controllability%2C%20and%20trustworthiness%20in%20AI%20systems.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/alehdaghi/PCMNet.&entry.1838667208=http%3A//arxiv.org/abs/2504.12197v3&entry.124074799=Read"},
{"title": "Distance-Preserving Representations for Genomic Spatial Reconstruction", "author": "Wenbin Zhou and Jin-Hong Du", "abstract": "The spatial context of single-cell gene expression data is crucial for many downstream analyses, yet often remains inaccessible due to practical and technical limitations, restricting the utility of such datasets. In this paper, we propose a generic representation learning and transfer learning framework dp-VAE, capable of reconstructing the spatial coordinates associated with the provided gene expression data. Central to our approach is a distance-preserving regularizer integrated into the loss function during training, ensuring the model effectively captures and utilizes spatial context signals from reference datasets. During the inference stage, the produced latent representation of the model can be used to reconstruct or impute the spatial context of the provided gene expression by solving a constrained optimization problem. We also explore the theoretical connections between distance-preserving loss, distortion, and the bi-Lipschitz condition within generative models. Finally, we demonstrate the effectiveness of dp-VAE in different tasks involving training robustness, out-of-sample evaluation, and transfer learning inference applications by testing it over 27 publicly available datasets. This underscores its applicability to a wide range of genomics studies that were previously hindered by the absence of spatial data.", "link": "http://arxiv.org/abs/2408.00911v3", "date": "2025-11-20", "relevancy": 2.5769, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5256}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5119}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distance-Preserving%20Representations%20for%20Genomic%20Spatial%20Reconstruction&body=Title%3A%20Distance-Preserving%20Representations%20for%20Genomic%20Spatial%20Reconstruction%0AAuthor%3A%20Wenbin%20Zhou%20and%20Jin-Hong%20Du%0AAbstract%3A%20The%20spatial%20context%20of%20single-cell%20gene%20expression%20data%20is%20crucial%20for%20many%20downstream%20analyses%2C%20yet%20often%20remains%20inaccessible%20due%20to%20practical%20and%20technical%20limitations%2C%20restricting%20the%20utility%20of%20such%20datasets.%20In%20this%20paper%2C%20we%20propose%20a%20generic%20representation%20learning%20and%20transfer%20learning%20framework%20dp-VAE%2C%20capable%20of%20reconstructing%20the%20spatial%20coordinates%20associated%20with%20the%20provided%20gene%20expression%20data.%20Central%20to%20our%20approach%20is%20a%20distance-preserving%20regularizer%20integrated%20into%20the%20loss%20function%20during%20training%2C%20ensuring%20the%20model%20effectively%20captures%20and%20utilizes%20spatial%20context%20signals%20from%20reference%20datasets.%20During%20the%20inference%20stage%2C%20the%20produced%20latent%20representation%20of%20the%20model%20can%20be%20used%20to%20reconstruct%20or%20impute%20the%20spatial%20context%20of%20the%20provided%20gene%20expression%20by%20solving%20a%20constrained%20optimization%20problem.%20We%20also%20explore%20the%20theoretical%20connections%20between%20distance-preserving%20loss%2C%20distortion%2C%20and%20the%20bi-Lipschitz%20condition%20within%20generative%20models.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20dp-VAE%20in%20different%20tasks%20involving%20training%20robustness%2C%20out-of-sample%20evaluation%2C%20and%20transfer%20learning%20inference%20applications%20by%20testing%20it%20over%2027%20publicly%20available%20datasets.%20This%20underscores%20its%20applicability%20to%20a%20wide%20range%20of%20genomics%20studies%20that%20were%20previously%20hindered%20by%20the%20absence%20of%20spatial%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2408.00911v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistance-Preserving%2520Representations%2520for%2520Genomic%2520Spatial%2520Reconstruction%26entry.906535625%3DWenbin%2520Zhou%2520and%2520Jin-Hong%2520Du%26entry.1292438233%3DThe%2520spatial%2520context%2520of%2520single-cell%2520gene%2520expression%2520data%2520is%2520crucial%2520for%2520many%2520downstream%2520analyses%252C%2520yet%2520often%2520remains%2520inaccessible%2520due%2520to%2520practical%2520and%2520technical%2520limitations%252C%2520restricting%2520the%2520utility%2520of%2520such%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520generic%2520representation%2520learning%2520and%2520transfer%2520learning%2520framework%2520dp-VAE%252C%2520capable%2520of%2520reconstructing%2520the%2520spatial%2520coordinates%2520associated%2520with%2520the%2520provided%2520gene%2520expression%2520data.%2520Central%2520to%2520our%2520approach%2520is%2520a%2520distance-preserving%2520regularizer%2520integrated%2520into%2520the%2520loss%2520function%2520during%2520training%252C%2520ensuring%2520the%2520model%2520effectively%2520captures%2520and%2520utilizes%2520spatial%2520context%2520signals%2520from%2520reference%2520datasets.%2520During%2520the%2520inference%2520stage%252C%2520the%2520produced%2520latent%2520representation%2520of%2520the%2520model%2520can%2520be%2520used%2520to%2520reconstruct%2520or%2520impute%2520the%2520spatial%2520context%2520of%2520the%2520provided%2520gene%2520expression%2520by%2520solving%2520a%2520constrained%2520optimization%2520problem.%2520We%2520also%2520explore%2520the%2520theoretical%2520connections%2520between%2520distance-preserving%2520loss%252C%2520distortion%252C%2520and%2520the%2520bi-Lipschitz%2520condition%2520within%2520generative%2520models.%2520Finally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520dp-VAE%2520in%2520different%2520tasks%2520involving%2520training%2520robustness%252C%2520out-of-sample%2520evaluation%252C%2520and%2520transfer%2520learning%2520inference%2520applications%2520by%2520testing%2520it%2520over%252027%2520publicly%2520available%2520datasets.%2520This%2520underscores%2520its%2520applicability%2520to%2520a%2520wide%2520range%2520of%2520genomics%2520studies%2520that%2520were%2520previously%2520hindered%2520by%2520the%2520absence%2520of%2520spatial%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00911v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distance-Preserving%20Representations%20for%20Genomic%20Spatial%20Reconstruction&entry.906535625=Wenbin%20Zhou%20and%20Jin-Hong%20Du&entry.1292438233=The%20spatial%20context%20of%20single-cell%20gene%20expression%20data%20is%20crucial%20for%20many%20downstream%20analyses%2C%20yet%20often%20remains%20inaccessible%20due%20to%20practical%20and%20technical%20limitations%2C%20restricting%20the%20utility%20of%20such%20datasets.%20In%20this%20paper%2C%20we%20propose%20a%20generic%20representation%20learning%20and%20transfer%20learning%20framework%20dp-VAE%2C%20capable%20of%20reconstructing%20the%20spatial%20coordinates%20associated%20with%20the%20provided%20gene%20expression%20data.%20Central%20to%20our%20approach%20is%20a%20distance-preserving%20regularizer%20integrated%20into%20the%20loss%20function%20during%20training%2C%20ensuring%20the%20model%20effectively%20captures%20and%20utilizes%20spatial%20context%20signals%20from%20reference%20datasets.%20During%20the%20inference%20stage%2C%20the%20produced%20latent%20representation%20of%20the%20model%20can%20be%20used%20to%20reconstruct%20or%20impute%20the%20spatial%20context%20of%20the%20provided%20gene%20expression%20by%20solving%20a%20constrained%20optimization%20problem.%20We%20also%20explore%20the%20theoretical%20connections%20between%20distance-preserving%20loss%2C%20distortion%2C%20and%20the%20bi-Lipschitz%20condition%20within%20generative%20models.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20dp-VAE%20in%20different%20tasks%20involving%20training%20robustness%2C%20out-of-sample%20evaluation%2C%20and%20transfer%20learning%20inference%20applications%20by%20testing%20it%20over%2027%20publicly%20available%20datasets.%20This%20underscores%20its%20applicability%20to%20a%20wide%20range%20of%20genomics%20studies%20that%20were%20previously%20hindered%20by%20the%20absence%20of%20spatial%20data.&entry.1838667208=http%3A//arxiv.org/abs/2408.00911v3&entry.124074799=Read"},
{"title": "MHR: Momentum Human Rig", "author": "Aaron Ferguson and Ahmed A. A. Osman and Berta Bescos and Carsten Stoll and Chris Twigg and Christoph Lassner and David Otte and Eric Vignola and Fabian Prada and Federica Bogo and Igor Santesteban and Javier Romero and Jenna Zarate and Jeongseok Lee and Jinhyung Park and Jinlong Yang and John Doublestein and Kishore Venkateshan and Kris Kitani and Ladislav Kavan and Marco Dal Farra and Matthew Hu and Matthew Cioffi and Michael Fabris and Michael Ranieri and Mohammad Modarres and Petr Kadlecek and Rawal Khirodkar and Rinat Abdrashitov and Romain Pr\u00e9vost and Roman Rajbhandari and Ronald Mallet and Russel Pearsall and Sandy Kao and Sanjeev Kumar and Scott Parrish and Shoou-I Yu and Shunsuke Saito and Takaaki Shiratori and Te-Li Wang and Tony Tung and Yichen Xu and Yuan Dong and Yuhua Chen and Yuanlu Xu and Yuting Ye and Zhongshi Jiang", "abstract": "We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.", "link": "http://arxiv.org/abs/2511.15586v2", "date": "2025-11-20", "relevancy": 2.5515, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5694}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4982}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MHR%3A%20Momentum%20Human%20Rig&body=Title%3A%20MHR%3A%20Momentum%20Human%20Rig%0AAuthor%3A%20Aaron%20Ferguson%20and%20Ahmed%20A.%20A.%20Osman%20and%20Berta%20Bescos%20and%20Carsten%20Stoll%20and%20Chris%20Twigg%20and%20Christoph%20Lassner%20and%20David%20Otte%20and%20Eric%20Vignola%20and%20Fabian%20Prada%20and%20Federica%20Bogo%20and%20Igor%20Santesteban%20and%20Javier%20Romero%20and%20Jenna%20Zarate%20and%20Jeongseok%20Lee%20and%20Jinhyung%20Park%20and%20Jinlong%20Yang%20and%20John%20Doublestein%20and%20Kishore%20Venkateshan%20and%20Kris%20Kitani%20and%20Ladislav%20Kavan%20and%20Marco%20Dal%20Farra%20and%20Matthew%20Hu%20and%20Matthew%20Cioffi%20and%20Michael%20Fabris%20and%20Michael%20Ranieri%20and%20Mohammad%20Modarres%20and%20Petr%20Kadlecek%20and%20Rawal%20Khirodkar%20and%20Rinat%20Abdrashitov%20and%20Romain%20Pr%C3%A9vost%20and%20Roman%20Rajbhandari%20and%20Ronald%20Mallet%20and%20Russel%20Pearsall%20and%20Sandy%20Kao%20and%20Sanjeev%20Kumar%20and%20Scott%20Parrish%20and%20Shoou-I%20Yu%20and%20Shunsuke%20Saito%20and%20Takaaki%20Shiratori%20and%20Te-Li%20Wang%20and%20Tony%20Tung%20and%20Yichen%20Xu%20and%20Yuan%20Dong%20and%20Yuhua%20Chen%20and%20Yuanlu%20Xu%20and%20Yuting%20Ye%20and%20Zhongshi%20Jiang%0AAbstract%3A%20We%20present%20MHR%2C%20a%20parametric%20human%20body%20model%20that%20combines%20the%20decoupled%20skeleton/shape%20paradigm%20of%20ATLAS%20with%20a%20flexible%2C%20modern%20rig%20and%20pose%20corrective%20system%20inspired%20by%20the%20Momentum%20library.%20Our%20model%20enables%20expressive%2C%20anatomically%20plausible%20human%20animation%2C%20supporting%20non-linear%20pose%20correctives%2C%20and%20is%20designed%20for%20robust%20integration%20in%20AR/VR%20and%20graphics%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMHR%253A%2520Momentum%2520Human%2520Rig%26entry.906535625%3DAaron%2520Ferguson%2520and%2520Ahmed%2520A.%2520A.%2520Osman%2520and%2520Berta%2520Bescos%2520and%2520Carsten%2520Stoll%2520and%2520Chris%2520Twigg%2520and%2520Christoph%2520Lassner%2520and%2520David%2520Otte%2520and%2520Eric%2520Vignola%2520and%2520Fabian%2520Prada%2520and%2520Federica%2520Bogo%2520and%2520Igor%2520Santesteban%2520and%2520Javier%2520Romero%2520and%2520Jenna%2520Zarate%2520and%2520Jeongseok%2520Lee%2520and%2520Jinhyung%2520Park%2520and%2520Jinlong%2520Yang%2520and%2520John%2520Doublestein%2520and%2520Kishore%2520Venkateshan%2520and%2520Kris%2520Kitani%2520and%2520Ladislav%2520Kavan%2520and%2520Marco%2520Dal%2520Farra%2520and%2520Matthew%2520Hu%2520and%2520Matthew%2520Cioffi%2520and%2520Michael%2520Fabris%2520and%2520Michael%2520Ranieri%2520and%2520Mohammad%2520Modarres%2520and%2520Petr%2520Kadlecek%2520and%2520Rawal%2520Khirodkar%2520and%2520Rinat%2520Abdrashitov%2520and%2520Romain%2520Pr%25C3%25A9vost%2520and%2520Roman%2520Rajbhandari%2520and%2520Ronald%2520Mallet%2520and%2520Russel%2520Pearsall%2520and%2520Sandy%2520Kao%2520and%2520Sanjeev%2520Kumar%2520and%2520Scott%2520Parrish%2520and%2520Shoou-I%2520Yu%2520and%2520Shunsuke%2520Saito%2520and%2520Takaaki%2520Shiratori%2520and%2520Te-Li%2520Wang%2520and%2520Tony%2520Tung%2520and%2520Yichen%2520Xu%2520and%2520Yuan%2520Dong%2520and%2520Yuhua%2520Chen%2520and%2520Yuanlu%2520Xu%2520and%2520Yuting%2520Ye%2520and%2520Zhongshi%2520Jiang%26entry.1292438233%3DWe%2520present%2520MHR%252C%2520a%2520parametric%2520human%2520body%2520model%2520that%2520combines%2520the%2520decoupled%2520skeleton/shape%2520paradigm%2520of%2520ATLAS%2520with%2520a%2520flexible%252C%2520modern%2520rig%2520and%2520pose%2520corrective%2520system%2520inspired%2520by%2520the%2520Momentum%2520library.%2520Our%2520model%2520enables%2520expressive%252C%2520anatomically%2520plausible%2520human%2520animation%252C%2520supporting%2520non-linear%2520pose%2520correctives%252C%2520and%2520is%2520designed%2520for%2520robust%2520integration%2520in%2520AR/VR%2520and%2520graphics%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MHR%3A%20Momentum%20Human%20Rig&entry.906535625=Aaron%20Ferguson%20and%20Ahmed%20A.%20A.%20Osman%20and%20Berta%20Bescos%20and%20Carsten%20Stoll%20and%20Chris%20Twigg%20and%20Christoph%20Lassner%20and%20David%20Otte%20and%20Eric%20Vignola%20and%20Fabian%20Prada%20and%20Federica%20Bogo%20and%20Igor%20Santesteban%20and%20Javier%20Romero%20and%20Jenna%20Zarate%20and%20Jeongseok%20Lee%20and%20Jinhyung%20Park%20and%20Jinlong%20Yang%20and%20John%20Doublestein%20and%20Kishore%20Venkateshan%20and%20Kris%20Kitani%20and%20Ladislav%20Kavan%20and%20Marco%20Dal%20Farra%20and%20Matthew%20Hu%20and%20Matthew%20Cioffi%20and%20Michael%20Fabris%20and%20Michael%20Ranieri%20and%20Mohammad%20Modarres%20and%20Petr%20Kadlecek%20and%20Rawal%20Khirodkar%20and%20Rinat%20Abdrashitov%20and%20Romain%20Pr%C3%A9vost%20and%20Roman%20Rajbhandari%20and%20Ronald%20Mallet%20and%20Russel%20Pearsall%20and%20Sandy%20Kao%20and%20Sanjeev%20Kumar%20and%20Scott%20Parrish%20and%20Shoou-I%20Yu%20and%20Shunsuke%20Saito%20and%20Takaaki%20Shiratori%20and%20Te-Li%20Wang%20and%20Tony%20Tung%20and%20Yichen%20Xu%20and%20Yuan%20Dong%20and%20Yuhua%20Chen%20and%20Yuanlu%20Xu%20and%20Yuting%20Ye%20and%20Zhongshi%20Jiang&entry.1292438233=We%20present%20MHR%2C%20a%20parametric%20human%20body%20model%20that%20combines%20the%20decoupled%20skeleton/shape%20paradigm%20of%20ATLAS%20with%20a%20flexible%2C%20modern%20rig%20and%20pose%20corrective%20system%20inspired%20by%20the%20Momentum%20library.%20Our%20model%20enables%20expressive%2C%20anatomically%20plausible%20human%20animation%2C%20supporting%20non-linear%20pose%20correctives%2C%20and%20is%20designed%20for%20robust%20integration%20in%20AR/VR%20and%20graphics%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2511.15586v2&entry.124074799=Read"},
{"title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference", "author": "Ziyan Liu and Yeqiu Chen and Hongyi Cai and Tao Lin and Shuo Yang and Zheng Liu and Bo Zhao", "abstract": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.", "link": "http://arxiv.org/abs/2511.16449v1", "date": "2025-11-20", "relevancy": 2.5502, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLA-Pruner%3A%20Temporal-Aware%20Dual-Level%20Visual%20Token%20Pruning%20for%20Efficient%20Vision-Language-Action%20Inference&body=Title%3A%20VLA-Pruner%3A%20Temporal-Aware%20Dual-Level%20Visual%20Token%20Pruning%20for%20Efficient%20Vision-Language-Action%20Inference%0AAuthor%3A%20Ziyan%20Liu%20and%20Yeqiu%20Chen%20and%20Hongyi%20Cai%20and%20Tao%20Lin%20and%20Shuo%20Yang%20and%20Zheng%20Liu%20and%20Bo%20Zhao%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20great%20promise%20for%20embodied%20AI%2C%20yet%20the%20heavy%20computational%20cost%20of%20processing%20continuous%20visual%20streams%20severely%20limits%20their%20real-time%20deployment.%20Token%20pruning%20%28keeping%20salient%20visual%20tokens%20and%20dropping%20redundant%20ones%29%20has%20emerged%20as%20an%20effective%20approach%20for%20accelerating%20Vision-Language%20Models%20%28VLMs%29%2C%20offering%20a%20solution%20for%20efficient%20VLA.%20However%2C%20these%20VLM-specific%20token%20pruning%20methods%20select%20tokens%20based%20solely%20on%20semantic%20salience%20metrics%20%28e.g.%2C%20prefill%20attention%29%2C%20while%20overlooking%20the%20VLA%27s%20intrinsic%20dual-system%20nature%20of%20high-level%20semantic%20understanding%20and%20low-level%20action%20execution.%20Consequently%2C%20these%20methods%20bias%20token%20retention%20toward%20semantic%20cues%2C%20discard%20critical%20information%20for%20action%20generation%2C%20and%20significantly%20degrade%20VLA%20performance.%20To%20bridge%20this%20gap%2C%20we%20propose%20VLA-Pruner%2C%20a%20versatile%20plug-and-play%20VLA-specific%20token%20prune%20method%20that%20aligns%20with%20the%20dual-system%20nature%20of%20VLA%20models%20and%20exploits%20the%20temporal%20continuity%20in%20robot%20manipulation.%20Specifically%2C%20VLA-Pruner%20adopts%20a%20dual-level%20importance%20criterion%20for%20visual%20token%20retention%3A%20vision-language%20prefill%20attention%20for%20semantic-level%20relevance%20and%20action%20decode%20attention%2C%20estimated%20via%20temporal%20smoothing%2C%20for%20action-level%20importance.%20Based%20on%20this%20criterion%2C%20VLA-Pruner%20proposes%20a%20novel%20dual-level%20token%20selection%20strategy%20that%20adaptively%20preserves%20a%20compact%2C%20informative%20set%20of%20visual%20tokens%20for%20both%20semantic%20understanding%20and%20action%20execution%20under%20given%20compute%20budget.%20Experiments%20show%20that%20VLA-Pruner%20achieves%20state-of-the-art%20performance%20across%20multiple%20VLA%20architectures%20and%20diverse%20robotic%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLA-Pruner%253A%2520Temporal-Aware%2520Dual-Level%2520Visual%2520Token%2520Pruning%2520for%2520Efficient%2520Vision-Language-Action%2520Inference%26entry.906535625%3DZiyan%2520Liu%2520and%2520Yeqiu%2520Chen%2520and%2520Hongyi%2520Cai%2520and%2520Tao%2520Lin%2520and%2520Shuo%2520Yang%2520and%2520Zheng%2520Liu%2520and%2520Bo%2520Zhao%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520great%2520promise%2520for%2520embodied%2520AI%252C%2520yet%2520the%2520heavy%2520computational%2520cost%2520of%2520processing%2520continuous%2520visual%2520streams%2520severely%2520limits%2520their%2520real-time%2520deployment.%2520Token%2520pruning%2520%2528keeping%2520salient%2520visual%2520tokens%2520and%2520dropping%2520redundant%2520ones%2529%2520has%2520emerged%2520as%2520an%2520effective%2520approach%2520for%2520accelerating%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520offering%2520a%2520solution%2520for%2520efficient%2520VLA.%2520However%252C%2520these%2520VLM-specific%2520token%2520pruning%2520methods%2520select%2520tokens%2520based%2520solely%2520on%2520semantic%2520salience%2520metrics%2520%2528e.g.%252C%2520prefill%2520attention%2529%252C%2520while%2520overlooking%2520the%2520VLA%2527s%2520intrinsic%2520dual-system%2520nature%2520of%2520high-level%2520semantic%2520understanding%2520and%2520low-level%2520action%2520execution.%2520Consequently%252C%2520these%2520methods%2520bias%2520token%2520retention%2520toward%2520semantic%2520cues%252C%2520discard%2520critical%2520information%2520for%2520action%2520generation%252C%2520and%2520significantly%2520degrade%2520VLA%2520performance.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520VLA-Pruner%252C%2520a%2520versatile%2520plug-and-play%2520VLA-specific%2520token%2520prune%2520method%2520that%2520aligns%2520with%2520the%2520dual-system%2520nature%2520of%2520VLA%2520models%2520and%2520exploits%2520the%2520temporal%2520continuity%2520in%2520robot%2520manipulation.%2520Specifically%252C%2520VLA-Pruner%2520adopts%2520a%2520dual-level%2520importance%2520criterion%2520for%2520visual%2520token%2520retention%253A%2520vision-language%2520prefill%2520attention%2520for%2520semantic-level%2520relevance%2520and%2520action%2520decode%2520attention%252C%2520estimated%2520via%2520temporal%2520smoothing%252C%2520for%2520action-level%2520importance.%2520Based%2520on%2520this%2520criterion%252C%2520VLA-Pruner%2520proposes%2520a%2520novel%2520dual-level%2520token%2520selection%2520strategy%2520that%2520adaptively%2520preserves%2520a%2520compact%252C%2520informative%2520set%2520of%2520visual%2520tokens%2520for%2520both%2520semantic%2520understanding%2520and%2520action%2520execution%2520under%2520given%2520compute%2520budget.%2520Experiments%2520show%2520that%2520VLA-Pruner%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520VLA%2520architectures%2520and%2520diverse%2520robotic%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLA-Pruner%3A%20Temporal-Aware%20Dual-Level%20Visual%20Token%20Pruning%20for%20Efficient%20Vision-Language-Action%20Inference&entry.906535625=Ziyan%20Liu%20and%20Yeqiu%20Chen%20and%20Hongyi%20Cai%20and%20Tao%20Lin%20and%20Shuo%20Yang%20and%20Zheng%20Liu%20and%20Bo%20Zhao&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20great%20promise%20for%20embodied%20AI%2C%20yet%20the%20heavy%20computational%20cost%20of%20processing%20continuous%20visual%20streams%20severely%20limits%20their%20real-time%20deployment.%20Token%20pruning%20%28keeping%20salient%20visual%20tokens%20and%20dropping%20redundant%20ones%29%20has%20emerged%20as%20an%20effective%20approach%20for%20accelerating%20Vision-Language%20Models%20%28VLMs%29%2C%20offering%20a%20solution%20for%20efficient%20VLA.%20However%2C%20these%20VLM-specific%20token%20pruning%20methods%20select%20tokens%20based%20solely%20on%20semantic%20salience%20metrics%20%28e.g.%2C%20prefill%20attention%29%2C%20while%20overlooking%20the%20VLA%27s%20intrinsic%20dual-system%20nature%20of%20high-level%20semantic%20understanding%20and%20low-level%20action%20execution.%20Consequently%2C%20these%20methods%20bias%20token%20retention%20toward%20semantic%20cues%2C%20discard%20critical%20information%20for%20action%20generation%2C%20and%20significantly%20degrade%20VLA%20performance.%20To%20bridge%20this%20gap%2C%20we%20propose%20VLA-Pruner%2C%20a%20versatile%20plug-and-play%20VLA-specific%20token%20prune%20method%20that%20aligns%20with%20the%20dual-system%20nature%20of%20VLA%20models%20and%20exploits%20the%20temporal%20continuity%20in%20robot%20manipulation.%20Specifically%2C%20VLA-Pruner%20adopts%20a%20dual-level%20importance%20criterion%20for%20visual%20token%20retention%3A%20vision-language%20prefill%20attention%20for%20semantic-level%20relevance%20and%20action%20decode%20attention%2C%20estimated%20via%20temporal%20smoothing%2C%20for%20action-level%20importance.%20Based%20on%20this%20criterion%2C%20VLA-Pruner%20proposes%20a%20novel%20dual-level%20token%20selection%20strategy%20that%20adaptively%20preserves%20a%20compact%2C%20informative%20set%20of%20visual%20tokens%20for%20both%20semantic%20understanding%20and%20action%20execution%20under%20given%20compute%20budget.%20Experiments%20show%20that%20VLA-Pruner%20achieves%20state-of-the-art%20performance%20across%20multiple%20VLA%20architectures%20and%20diverse%20robotic%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.16449v1&entry.124074799=Read"},
{"title": "Causal Synthetic Data Generation in Recruitment", "author": "Andrea Iommi and Antonio Mastropietro and Riccardo Guidotti and Anna Monreale and Salvatore Ruggieri", "abstract": "The importance of Synthetic Data Generation (SDG) has increased significantly in domains where data quality is poor or access is limited due to privacy and regulatory constraints. One such domain is recruitment, where publicly available datasets are scarce due to the sensitive nature of information typically found in curricula vitae, such as gender, disability status, or age. %\nThis lack of accessible, representative data presents a significant obstacle to the development of fair and transparent machine learning models, particularly ranking algorithms that require large volumes of data to effectively learn how to recommend candidates. In the absence of such data, these models are prone to poor generalisation and may fail to perform reliably in real-world scenarios. %\nRecent advances in Causal Generative Models (CGMs) offer a promising solution. CGMs enable the generation of synthetic datasets that preserve the underlying causal relationships within the data, providing greater control over fairness and interpretability in the data generation process. %\nIn this study, we present a specialised SDG method involving two CGMs: one modelling job offers and the other modelling curricula. Each model is structured according to a causal graph informed by domain expertise. We use these models to generate synthetic datasets and evaluate the fairness of candidate rankings under controlled scenarios that introduce specific biases.", "link": "http://arxiv.org/abs/2511.16204v1", "date": "2025-11-20", "relevancy": 2.5489, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5126}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5089}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Synthetic%20Data%20Generation%20in%20Recruitment&body=Title%3A%20Causal%20Synthetic%20Data%20Generation%20in%20Recruitment%0AAuthor%3A%20Andrea%20Iommi%20and%20Antonio%20Mastropietro%20and%20Riccardo%20Guidotti%20and%20Anna%20Monreale%20and%20Salvatore%20Ruggieri%0AAbstract%3A%20The%20importance%20of%20Synthetic%20Data%20Generation%20%28SDG%29%20has%20increased%20significantly%20in%20domains%20where%20data%20quality%20is%20poor%20or%20access%20is%20limited%20due%20to%20privacy%20and%20regulatory%20constraints.%20One%20such%20domain%20is%20recruitment%2C%20where%20publicly%20available%20datasets%20are%20scarce%20due%20to%20the%20sensitive%20nature%20of%20information%20typically%20found%20in%20curricula%20vitae%2C%20such%20as%20gender%2C%20disability%20status%2C%20or%20age.%20%25%0AThis%20lack%20of%20accessible%2C%20representative%20data%20presents%20a%20significant%20obstacle%20to%20the%20development%20of%20fair%20and%20transparent%20machine%20learning%20models%2C%20particularly%20ranking%20algorithms%20that%20require%20large%20volumes%20of%20data%20to%20effectively%20learn%20how%20to%20recommend%20candidates.%20In%20the%20absence%20of%20such%20data%2C%20these%20models%20are%20prone%20to%20poor%20generalisation%20and%20may%20fail%20to%20perform%20reliably%20in%20real-world%20scenarios.%20%25%0ARecent%20advances%20in%20Causal%20Generative%20Models%20%28CGMs%29%20offer%20a%20promising%20solution.%20CGMs%20enable%20the%20generation%20of%20synthetic%20datasets%20that%20preserve%20the%20underlying%20causal%20relationships%20within%20the%20data%2C%20providing%20greater%20control%20over%20fairness%20and%20interpretability%20in%20the%20data%20generation%20process.%20%25%0AIn%20this%20study%2C%20we%20present%20a%20specialised%20SDG%20method%20involving%20two%20CGMs%3A%20one%20modelling%20job%20offers%20and%20the%20other%20modelling%20curricula.%20Each%20model%20is%20structured%20according%20to%20a%20causal%20graph%20informed%20by%20domain%20expertise.%20We%20use%20these%20models%20to%20generate%20synthetic%20datasets%20and%20evaluate%20the%20fairness%20of%20candidate%20rankings%20under%20controlled%20scenarios%20that%20introduce%20specific%20biases.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Synthetic%2520Data%2520Generation%2520in%2520Recruitment%26entry.906535625%3DAndrea%2520Iommi%2520and%2520Antonio%2520Mastropietro%2520and%2520Riccardo%2520Guidotti%2520and%2520Anna%2520Monreale%2520and%2520Salvatore%2520Ruggieri%26entry.1292438233%3DThe%2520importance%2520of%2520Synthetic%2520Data%2520Generation%2520%2528SDG%2529%2520has%2520increased%2520significantly%2520in%2520domains%2520where%2520data%2520quality%2520is%2520poor%2520or%2520access%2520is%2520limited%2520due%2520to%2520privacy%2520and%2520regulatory%2520constraints.%2520One%2520such%2520domain%2520is%2520recruitment%252C%2520where%2520publicly%2520available%2520datasets%2520are%2520scarce%2520due%2520to%2520the%2520sensitive%2520nature%2520of%2520information%2520typically%2520found%2520in%2520curricula%2520vitae%252C%2520such%2520as%2520gender%252C%2520disability%2520status%252C%2520or%2520age.%2520%2525%250AThis%2520lack%2520of%2520accessible%252C%2520representative%2520data%2520presents%2520a%2520significant%2520obstacle%2520to%2520the%2520development%2520of%2520fair%2520and%2520transparent%2520machine%2520learning%2520models%252C%2520particularly%2520ranking%2520algorithms%2520that%2520require%2520large%2520volumes%2520of%2520data%2520to%2520effectively%2520learn%2520how%2520to%2520recommend%2520candidates.%2520In%2520the%2520absence%2520of%2520such%2520data%252C%2520these%2520models%2520are%2520prone%2520to%2520poor%2520generalisation%2520and%2520may%2520fail%2520to%2520perform%2520reliably%2520in%2520real-world%2520scenarios.%2520%2525%250ARecent%2520advances%2520in%2520Causal%2520Generative%2520Models%2520%2528CGMs%2529%2520offer%2520a%2520promising%2520solution.%2520CGMs%2520enable%2520the%2520generation%2520of%2520synthetic%2520datasets%2520that%2520preserve%2520the%2520underlying%2520causal%2520relationships%2520within%2520the%2520data%252C%2520providing%2520greater%2520control%2520over%2520fairness%2520and%2520interpretability%2520in%2520the%2520data%2520generation%2520process.%2520%2525%250AIn%2520this%2520study%252C%2520we%2520present%2520a%2520specialised%2520SDG%2520method%2520involving%2520two%2520CGMs%253A%2520one%2520modelling%2520job%2520offers%2520and%2520the%2520other%2520modelling%2520curricula.%2520Each%2520model%2520is%2520structured%2520according%2520to%2520a%2520causal%2520graph%2520informed%2520by%2520domain%2520expertise.%2520We%2520use%2520these%2520models%2520to%2520generate%2520synthetic%2520datasets%2520and%2520evaluate%2520the%2520fairness%2520of%2520candidate%2520rankings%2520under%2520controlled%2520scenarios%2520that%2520introduce%2520specific%2520biases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Synthetic%20Data%20Generation%20in%20Recruitment&entry.906535625=Andrea%20Iommi%20and%20Antonio%20Mastropietro%20and%20Riccardo%20Guidotti%20and%20Anna%20Monreale%20and%20Salvatore%20Ruggieri&entry.1292438233=The%20importance%20of%20Synthetic%20Data%20Generation%20%28SDG%29%20has%20increased%20significantly%20in%20domains%20where%20data%20quality%20is%20poor%20or%20access%20is%20limited%20due%20to%20privacy%20and%20regulatory%20constraints.%20One%20such%20domain%20is%20recruitment%2C%20where%20publicly%20available%20datasets%20are%20scarce%20due%20to%20the%20sensitive%20nature%20of%20information%20typically%20found%20in%20curricula%20vitae%2C%20such%20as%20gender%2C%20disability%20status%2C%20or%20age.%20%25%0AThis%20lack%20of%20accessible%2C%20representative%20data%20presents%20a%20significant%20obstacle%20to%20the%20development%20of%20fair%20and%20transparent%20machine%20learning%20models%2C%20particularly%20ranking%20algorithms%20that%20require%20large%20volumes%20of%20data%20to%20effectively%20learn%20how%20to%20recommend%20candidates.%20In%20the%20absence%20of%20such%20data%2C%20these%20models%20are%20prone%20to%20poor%20generalisation%20and%20may%20fail%20to%20perform%20reliably%20in%20real-world%20scenarios.%20%25%0ARecent%20advances%20in%20Causal%20Generative%20Models%20%28CGMs%29%20offer%20a%20promising%20solution.%20CGMs%20enable%20the%20generation%20of%20synthetic%20datasets%20that%20preserve%20the%20underlying%20causal%20relationships%20within%20the%20data%2C%20providing%20greater%20control%20over%20fairness%20and%20interpretability%20in%20the%20data%20generation%20process.%20%25%0AIn%20this%20study%2C%20we%20present%20a%20specialised%20SDG%20method%20involving%20two%20CGMs%3A%20one%20modelling%20job%20offers%20and%20the%20other%20modelling%20curricula.%20Each%20model%20is%20structured%20according%20to%20a%20causal%20graph%20informed%20by%20domain%20expertise.%20We%20use%20these%20models%20to%20generate%20synthetic%20datasets%20and%20evaluate%20the%20fairness%20of%20candidate%20rankings%20under%20controlled%20scenarios%20that%20introduce%20specific%20biases.&entry.1838667208=http%3A//arxiv.org/abs/2511.16204v1&entry.124074799=Read"},
{"title": "Evolution Strategies at the Hyperscale", "author": "Bidipta Sarkar and Mattie Fellows and Juan Agustin Duque and Alistair Letcher and Antonio Le\u00f3n Villares and Anya Sims and Dylan Cope and Jarek Liesen and Lukas Seier and Theo Wolf and Uljad Berdica and Alexander David Goldie and Aaron Courville and Karin Sevegnani and Shimon Whiteson and Jakob Nicolaus Foerster", "abstract": "We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling potential through parallelisation. Na{\u00ef}ve ES becomes prohibitively expensive at scale due to the computational and memory costs associated with generating matrix perturbations $E\\in\\mathbb{R}^{m\\times n}$ and the batched matrix multiplications needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks by generating random matrices $A\\in \\mathbb{R}^{m\\times r},\\ B\\in \\mathbb{R}^{n\\times r}$ with $r\\ll \\min(m,n)$ to form a low-rank matrix perturbation $A B^\\top$ that are used in place of the full-rank perturbation $E$. As the overall update is an average across a population of $N$ workers, this still results in a high-rank update but with significant memory and computation savings, reducing the auxiliary storage from $mn$ to $r(m+n)$ per layer and the cost of a forward pass from $\\mathcal{O}(mn)$ to $\\mathcal{O}(r(m+n))$ when compared to full-rank ES. A theoretical analysis reveals our low-rank update converges to the full-rank update at a fast $\\mathcal{O}\\left(\\frac{1}{r}\\right)$ rate. Our experiments show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL settings, despite being faster, (2) it is competitive with GRPO as a technique for improving LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent language models that operate purely in integer datatypes.", "link": "http://arxiv.org/abs/2511.16652v1", "date": "2025-11-20", "relevancy": 2.547, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5129}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5108}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolution%20Strategies%20at%20the%20Hyperscale&body=Title%3A%20Evolution%20Strategies%20at%20the%20Hyperscale%0AAuthor%3A%20Bidipta%20Sarkar%20and%20Mattie%20Fellows%20and%20Juan%20Agustin%20Duque%20and%20Alistair%20Letcher%20and%20Antonio%20Le%C3%B3n%20Villares%20and%20Anya%20Sims%20and%20Dylan%20Cope%20and%20Jarek%20Liesen%20and%20Lukas%20Seier%20and%20Theo%20Wolf%20and%20Uljad%20Berdica%20and%20Alexander%20David%20Goldie%20and%20Aaron%20Courville%20and%20Karin%20Sevegnani%20and%20Shimon%20Whiteson%20and%20Jakob%20Nicolaus%20Foerster%0AAbstract%3A%20We%20introduce%20Evolution%20Guided%20General%20Optimization%20via%20Low-rank%20Learning%20%28EGGROLL%29%2C%20an%20evolution%20strategies%20%28ES%29%20algorithm%20designed%20to%20scale%20backprop-free%20optimization%20to%20large%20population%20sizes%20for%20modern%20large%20neural%20network%20architectures%20with%20billions%20of%20parameters.%20ES%20is%20a%20set%20of%20powerful%20blackbox%20optimisation%20methods%20that%20can%20handle%20non-differentiable%20or%20noisy%20objectives%20with%20excellent%20scaling%20potential%20through%20parallelisation.%20Na%7B%C3%AF%7Dve%20ES%20becomes%20prohibitively%20expensive%20at%20scale%20due%20to%20the%20computational%20and%20memory%20costs%20associated%20with%20generating%20matrix%20perturbations%20%24E%5Cin%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes%20n%7D%24%20and%20the%20batched%20matrix%20multiplications%20needed%20to%20compute%20per-member%20forward%20passes.%20EGGROLL%20overcomes%20these%20bottlenecks%20by%20generating%20random%20matrices%20%24A%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes%20r%7D%2C%5C%20B%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20r%7D%24%20with%20%24r%5Cll%20%5Cmin%28m%2Cn%29%24%20to%20form%20a%20low-rank%20matrix%20perturbation%20%24A%20B%5E%5Ctop%24%20that%20are%20used%20in%20place%20of%20the%20full-rank%20perturbation%20%24E%24.%20As%20the%20overall%20update%20is%20an%20average%20across%20a%20population%20of%20%24N%24%20workers%2C%20this%20still%20results%20in%20a%20high-rank%20update%20but%20with%20significant%20memory%20and%20computation%20savings%2C%20reducing%20the%20auxiliary%20storage%20from%20%24mn%24%20to%20%24r%28m%2Bn%29%24%20per%20layer%20and%20the%20cost%20of%20a%20forward%20pass%20from%20%24%5Cmathcal%7BO%7D%28mn%29%24%20to%20%24%5Cmathcal%7BO%7D%28r%28m%2Bn%29%29%24%20when%20compared%20to%20full-rank%20ES.%20A%20theoretical%20analysis%20reveals%20our%20low-rank%20update%20converges%20to%20the%20full-rank%20update%20at%20a%20fast%20%24%5Cmathcal%7BO%7D%5Cleft%28%5Cfrac%7B1%7D%7Br%7D%5Cright%29%24%20rate.%20Our%20experiments%20show%20that%20%281%29%20EGGROLL%20does%20not%20compromise%20the%20performance%20of%20ES%20in%20tabula-rasa%20RL%20settings%2C%20despite%20being%20faster%2C%20%282%29%20it%20is%20competitive%20with%20GRPO%20as%20a%20technique%20for%20improving%20LLM%20reasoning%2C%20and%20%283%29%20EGGROLL%20enables%20stable%20pre-training%20of%20nonlinear%20recurrent%20language%20models%20that%20operate%20purely%20in%20integer%20datatypes.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolution%2520Strategies%2520at%2520the%2520Hyperscale%26entry.906535625%3DBidipta%2520Sarkar%2520and%2520Mattie%2520Fellows%2520and%2520Juan%2520Agustin%2520Duque%2520and%2520Alistair%2520Letcher%2520and%2520Antonio%2520Le%25C3%25B3n%2520Villares%2520and%2520Anya%2520Sims%2520and%2520Dylan%2520Cope%2520and%2520Jarek%2520Liesen%2520and%2520Lukas%2520Seier%2520and%2520Theo%2520Wolf%2520and%2520Uljad%2520Berdica%2520and%2520Alexander%2520David%2520Goldie%2520and%2520Aaron%2520Courville%2520and%2520Karin%2520Sevegnani%2520and%2520Shimon%2520Whiteson%2520and%2520Jakob%2520Nicolaus%2520Foerster%26entry.1292438233%3DWe%2520introduce%2520Evolution%2520Guided%2520General%2520Optimization%2520via%2520Low-rank%2520Learning%2520%2528EGGROLL%2529%252C%2520an%2520evolution%2520strategies%2520%2528ES%2529%2520algorithm%2520designed%2520to%2520scale%2520backprop-free%2520optimization%2520to%2520large%2520population%2520sizes%2520for%2520modern%2520large%2520neural%2520network%2520architectures%2520with%2520billions%2520of%2520parameters.%2520ES%2520is%2520a%2520set%2520of%2520powerful%2520blackbox%2520optimisation%2520methods%2520that%2520can%2520handle%2520non-differentiable%2520or%2520noisy%2520objectives%2520with%2520excellent%2520scaling%2520potential%2520through%2520parallelisation.%2520Na%257B%25C3%25AF%257Dve%2520ES%2520becomes%2520prohibitively%2520expensive%2520at%2520scale%2520due%2520to%2520the%2520computational%2520and%2520memory%2520costs%2520associated%2520with%2520generating%2520matrix%2520perturbations%2520%2524E%255Cin%255Cmathbb%257BR%257D%255E%257Bm%255Ctimes%2520n%257D%2524%2520and%2520the%2520batched%2520matrix%2520multiplications%2520needed%2520to%2520compute%2520per-member%2520forward%2520passes.%2520EGGROLL%2520overcomes%2520these%2520bottlenecks%2520by%2520generating%2520random%2520matrices%2520%2524A%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bm%255Ctimes%2520r%257D%252C%255C%2520B%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bn%255Ctimes%2520r%257D%2524%2520with%2520%2524r%255Cll%2520%255Cmin%2528m%252Cn%2529%2524%2520to%2520form%2520a%2520low-rank%2520matrix%2520perturbation%2520%2524A%2520B%255E%255Ctop%2524%2520that%2520are%2520used%2520in%2520place%2520of%2520the%2520full-rank%2520perturbation%2520%2524E%2524.%2520As%2520the%2520overall%2520update%2520is%2520an%2520average%2520across%2520a%2520population%2520of%2520%2524N%2524%2520workers%252C%2520this%2520still%2520results%2520in%2520a%2520high-rank%2520update%2520but%2520with%2520significant%2520memory%2520and%2520computation%2520savings%252C%2520reducing%2520the%2520auxiliary%2520storage%2520from%2520%2524mn%2524%2520to%2520%2524r%2528m%252Bn%2529%2524%2520per%2520layer%2520and%2520the%2520cost%2520of%2520a%2520forward%2520pass%2520from%2520%2524%255Cmathcal%257BO%257D%2528mn%2529%2524%2520to%2520%2524%255Cmathcal%257BO%257D%2528r%2528m%252Bn%2529%2529%2524%2520when%2520compared%2520to%2520full-rank%2520ES.%2520A%2520theoretical%2520analysis%2520reveals%2520our%2520low-rank%2520update%2520converges%2520to%2520the%2520full-rank%2520update%2520at%2520a%2520fast%2520%2524%255Cmathcal%257BO%257D%255Cleft%2528%255Cfrac%257B1%257D%257Br%257D%255Cright%2529%2524%2520rate.%2520Our%2520experiments%2520show%2520that%2520%25281%2529%2520EGGROLL%2520does%2520not%2520compromise%2520the%2520performance%2520of%2520ES%2520in%2520tabula-rasa%2520RL%2520settings%252C%2520despite%2520being%2520faster%252C%2520%25282%2529%2520it%2520is%2520competitive%2520with%2520GRPO%2520as%2520a%2520technique%2520for%2520improving%2520LLM%2520reasoning%252C%2520and%2520%25283%2529%2520EGGROLL%2520enables%2520stable%2520pre-training%2520of%2520nonlinear%2520recurrent%2520language%2520models%2520that%2520operate%2520purely%2520in%2520integer%2520datatypes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolution%20Strategies%20at%20the%20Hyperscale&entry.906535625=Bidipta%20Sarkar%20and%20Mattie%20Fellows%20and%20Juan%20Agustin%20Duque%20and%20Alistair%20Letcher%20and%20Antonio%20Le%C3%B3n%20Villares%20and%20Anya%20Sims%20and%20Dylan%20Cope%20and%20Jarek%20Liesen%20and%20Lukas%20Seier%20and%20Theo%20Wolf%20and%20Uljad%20Berdica%20and%20Alexander%20David%20Goldie%20and%20Aaron%20Courville%20and%20Karin%20Sevegnani%20and%20Shimon%20Whiteson%20and%20Jakob%20Nicolaus%20Foerster&entry.1292438233=We%20introduce%20Evolution%20Guided%20General%20Optimization%20via%20Low-rank%20Learning%20%28EGGROLL%29%2C%20an%20evolution%20strategies%20%28ES%29%20algorithm%20designed%20to%20scale%20backprop-free%20optimization%20to%20large%20population%20sizes%20for%20modern%20large%20neural%20network%20architectures%20with%20billions%20of%20parameters.%20ES%20is%20a%20set%20of%20powerful%20blackbox%20optimisation%20methods%20that%20can%20handle%20non-differentiable%20or%20noisy%20objectives%20with%20excellent%20scaling%20potential%20through%20parallelisation.%20Na%7B%C3%AF%7Dve%20ES%20becomes%20prohibitively%20expensive%20at%20scale%20due%20to%20the%20computational%20and%20memory%20costs%20associated%20with%20generating%20matrix%20perturbations%20%24E%5Cin%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes%20n%7D%24%20and%20the%20batched%20matrix%20multiplications%20needed%20to%20compute%20per-member%20forward%20passes.%20EGGROLL%20overcomes%20these%20bottlenecks%20by%20generating%20random%20matrices%20%24A%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes%20r%7D%2C%5C%20B%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20r%7D%24%20with%20%24r%5Cll%20%5Cmin%28m%2Cn%29%24%20to%20form%20a%20low-rank%20matrix%20perturbation%20%24A%20B%5E%5Ctop%24%20that%20are%20used%20in%20place%20of%20the%20full-rank%20perturbation%20%24E%24.%20As%20the%20overall%20update%20is%20an%20average%20across%20a%20population%20of%20%24N%24%20workers%2C%20this%20still%20results%20in%20a%20high-rank%20update%20but%20with%20significant%20memory%20and%20computation%20savings%2C%20reducing%20the%20auxiliary%20storage%20from%20%24mn%24%20to%20%24r%28m%2Bn%29%24%20per%20layer%20and%20the%20cost%20of%20a%20forward%20pass%20from%20%24%5Cmathcal%7BO%7D%28mn%29%24%20to%20%24%5Cmathcal%7BO%7D%28r%28m%2Bn%29%29%24%20when%20compared%20to%20full-rank%20ES.%20A%20theoretical%20analysis%20reveals%20our%20low-rank%20update%20converges%20to%20the%20full-rank%20update%20at%20a%20fast%20%24%5Cmathcal%7BO%7D%5Cleft%28%5Cfrac%7B1%7D%7Br%7D%5Cright%29%24%20rate.%20Our%20experiments%20show%20that%20%281%29%20EGGROLL%20does%20not%20compromise%20the%20performance%20of%20ES%20in%20tabula-rasa%20RL%20settings%2C%20despite%20being%20faster%2C%20%282%29%20it%20is%20competitive%20with%20GRPO%20as%20a%20technique%20for%20improving%20LLM%20reasoning%2C%20and%20%283%29%20EGGROLL%20enables%20stable%20pre-training%20of%20nonlinear%20recurrent%20language%20models%20that%20operate%20purely%20in%20integer%20datatypes.&entry.1838667208=http%3A//arxiv.org/abs/2511.16652v1&entry.124074799=Read"},
{"title": "Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them", "author": "Guanyu Chen and Peiyang Wang and Yizhou Jiang and Yuqian Liu and Chujie Zhao and Ying Fang and Tianren Zhang and Feng Chen", "abstract": "Large language models (LLMs) have been able to perform various forms of reasoning tasks in a wide range of scenarios, but are they truly engaging in task abstraction and rule-based reasoning beyond mere memorization? To answer this question, we propose a novel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether LLMs perform abstract reasoning by altering their original understanding of fundamental rules. In particular, by constructing datasets with math expressions or logical formulas that contradict correct principles, we fine-tune the model to learn those contradictory rules and assess its generalization ability on unseen test domains. Through a series of experiments, we find that current LLMs are capable of applying contradictory rules to solve practical math word problems and natural language reasoning tasks, implying the presence of an internal mechanism in LLMs that abstracts before reasoning.", "link": "http://arxiv.org/abs/2503.16401v2", "date": "2025-11-20", "relevancy": 2.5417, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Hidden%20Reasoning%20Process%20of%20Large%20Language%20Models%20by%20Misleading%20Them&body=Title%3A%20Exploring%20the%20Hidden%20Reasoning%20Process%20of%20Large%20Language%20Models%20by%20Misleading%20Them%0AAuthor%3A%20Guanyu%20Chen%20and%20Peiyang%20Wang%20and%20Yizhou%20Jiang%20and%20Yuqian%20Liu%20and%20Chujie%20Zhao%20and%20Ying%20Fang%20and%20Tianren%20Zhang%20and%20Feng%20Chen%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20been%20able%20to%20perform%20various%20forms%20of%20reasoning%20tasks%20in%20a%20wide%20range%20of%20scenarios%2C%20but%20are%20they%20truly%20engaging%20in%20task%20abstraction%20and%20rule-based%20reasoning%20beyond%20mere%20memorization%3F%20To%20answer%20this%20question%2C%20we%20propose%20a%20novel%20experimental%20approach%2C%20Misleading%20Fine-Tuning%20%28MisFT%29%2C%20to%20examine%20whether%20LLMs%20perform%20abstract%20reasoning%20by%20altering%20their%20original%20understanding%20of%20fundamental%20rules.%20In%20particular%2C%20by%20constructing%20datasets%20with%20math%20expressions%20or%20logical%20formulas%20that%20contradict%20correct%20principles%2C%20we%20fine-tune%20the%20model%20to%20learn%20those%20contradictory%20rules%20and%20assess%20its%20generalization%20ability%20on%20unseen%20test%20domains.%20Through%20a%20series%20of%20experiments%2C%20we%20find%20that%20current%20LLMs%20are%20capable%20of%20applying%20contradictory%20rules%20to%20solve%20practical%20math%20word%20problems%20and%20natural%20language%20reasoning%20tasks%2C%20implying%20the%20presence%20of%20an%20internal%20mechanism%20in%20LLMs%20that%20abstracts%20before%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2503.16401v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Hidden%2520Reasoning%2520Process%2520of%2520Large%2520Language%2520Models%2520by%2520Misleading%2520Them%26entry.906535625%3DGuanyu%2520Chen%2520and%2520Peiyang%2520Wang%2520and%2520Yizhou%2520Jiang%2520and%2520Yuqian%2520Liu%2520and%2520Chujie%2520Zhao%2520and%2520Ying%2520Fang%2520and%2520Tianren%2520Zhang%2520and%2520Feng%2520Chen%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520able%2520to%2520perform%2520various%2520forms%2520of%2520reasoning%2520tasks%2520in%2520a%2520wide%2520range%2520of%2520scenarios%252C%2520but%2520are%2520they%2520truly%2520engaging%2520in%2520task%2520abstraction%2520and%2520rule-based%2520reasoning%2520beyond%2520mere%2520memorization%253F%2520To%2520answer%2520this%2520question%252C%2520we%2520propose%2520a%2520novel%2520experimental%2520approach%252C%2520Misleading%2520Fine-Tuning%2520%2528MisFT%2529%252C%2520to%2520examine%2520whether%2520LLMs%2520perform%2520abstract%2520reasoning%2520by%2520altering%2520their%2520original%2520understanding%2520of%2520fundamental%2520rules.%2520In%2520particular%252C%2520by%2520constructing%2520datasets%2520with%2520math%2520expressions%2520or%2520logical%2520formulas%2520that%2520contradict%2520correct%2520principles%252C%2520we%2520fine-tune%2520the%2520model%2520to%2520learn%2520those%2520contradictory%2520rules%2520and%2520assess%2520its%2520generalization%2520ability%2520on%2520unseen%2520test%2520domains.%2520Through%2520a%2520series%2520of%2520experiments%252C%2520we%2520find%2520that%2520current%2520LLMs%2520are%2520capable%2520of%2520applying%2520contradictory%2520rules%2520to%2520solve%2520practical%2520math%2520word%2520problems%2520and%2520natural%2520language%2520reasoning%2520tasks%252C%2520implying%2520the%2520presence%2520of%2520an%2520internal%2520mechanism%2520in%2520LLMs%2520that%2520abstracts%2520before%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16401v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Hidden%20Reasoning%20Process%20of%20Large%20Language%20Models%20by%20Misleading%20Them&entry.906535625=Guanyu%20Chen%20and%20Peiyang%20Wang%20and%20Yizhou%20Jiang%20and%20Yuqian%20Liu%20and%20Chujie%20Zhao%20and%20Ying%20Fang%20and%20Tianren%20Zhang%20and%20Feng%20Chen&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20been%20able%20to%20perform%20various%20forms%20of%20reasoning%20tasks%20in%20a%20wide%20range%20of%20scenarios%2C%20but%20are%20they%20truly%20engaging%20in%20task%20abstraction%20and%20rule-based%20reasoning%20beyond%20mere%20memorization%3F%20To%20answer%20this%20question%2C%20we%20propose%20a%20novel%20experimental%20approach%2C%20Misleading%20Fine-Tuning%20%28MisFT%29%2C%20to%20examine%20whether%20LLMs%20perform%20abstract%20reasoning%20by%20altering%20their%20original%20understanding%20of%20fundamental%20rules.%20In%20particular%2C%20by%20constructing%20datasets%20with%20math%20expressions%20or%20logical%20formulas%20that%20contradict%20correct%20principles%2C%20we%20fine-tune%20the%20model%20to%20learn%20those%20contradictory%20rules%20and%20assess%20its%20generalization%20ability%20on%20unseen%20test%20domains.%20Through%20a%20series%20of%20experiments%2C%20we%20find%20that%20current%20LLMs%20are%20capable%20of%20applying%20contradictory%20rules%20to%20solve%20practical%20math%20word%20problems%20and%20natural%20language%20reasoning%20tasks%2C%20implying%20the%20presence%20of%20an%20internal%20mechanism%20in%20LLMs%20that%20abstracts%20before%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2503.16401v2&entry.124074799=Read"},
{"title": "STAMP: Spatial-Temporal Adapter with Multi-Head Pooling", "author": "Brad Shook and Abby Turner and Jieshi Chen and Micha\u0142 Wili\u0144ski and Mononito Goswami and Jonathan Elmer and Artur Dubrawski", "abstract": "Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.", "link": "http://arxiv.org/abs/2511.10848v2", "date": "2025-11-20", "relevancy": 2.5307, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.539}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4918}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAMP%3A%20Spatial-Temporal%20Adapter%20with%20Multi-Head%20Pooling&body=Title%3A%20STAMP%3A%20Spatial-Temporal%20Adapter%20with%20Multi-Head%20Pooling%0AAuthor%3A%20Brad%20Shook%20and%20Abby%20Turner%20and%20Jieshi%20Chen%20and%20Micha%C5%82%20Wili%C5%84ski%20and%20Mononito%20Goswami%20and%20Jonathan%20Elmer%20and%20Artur%20Dubrawski%0AAbstract%3A%20Time%20series%20foundation%20models%20%28TSFMs%29%20pretrained%20on%20data%20from%20multiple%20domains%20have%20shown%20strong%20performance%20on%20diverse%20modeling%20tasks.%20Various%20efforts%20have%20been%20made%20to%20develop%20foundation%20models%20specific%20to%20electroencephalography%20%28EEG%29%20data%2C%20which%20records%20brain%20electrical%20activity%20as%20time%20series.%20However%2C%20no%20comparative%20analysis%20of%20EEG-specific%20foundation%20models%20%28EEGFMs%29%20versus%20general%20TSFMs%20has%20been%20performed%20on%20EEG-specific%20tasks.%20We%20introduce%20a%20novel%20Spatial-Temporal%20Adapter%20with%20Multi-Head%20Pooling%20%28STAMP%29%2C%20which%20leverages%20univariate%20embeddings%20produced%20by%20a%20general%20TSFM%2C%20implicitly%20models%20spatial-temporal%20characteristics%20of%20EEG%20data%2C%20and%20achieves%20performance%20comparable%20to%20state-of-the-art%20EEGFMs.%20A%20comprehensive%20analysis%20is%20performed%20on%208%20benchmark%20datasets%20of%20clinical%20tasks%20using%20EEG%20for%20classification%2C%20along%20with%20ablation%20studies.%20Our%20proposed%20adapter%20is%20lightweight%20in%20trainable%20parameters%20and%20flexible%20in%20the%20inputs%20it%20can%20accommodate%2C%20supporting%20easy%20modeling%20of%20EEG%20data%20using%20TSFMs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10848v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAMP%253A%2520Spatial-Temporal%2520Adapter%2520with%2520Multi-Head%2520Pooling%26entry.906535625%3DBrad%2520Shook%2520and%2520Abby%2520Turner%2520and%2520Jieshi%2520Chen%2520and%2520Micha%25C5%2582%2520Wili%25C5%2584ski%2520and%2520Mononito%2520Goswami%2520and%2520Jonathan%2520Elmer%2520and%2520Artur%2520Dubrawski%26entry.1292438233%3DTime%2520series%2520foundation%2520models%2520%2528TSFMs%2529%2520pretrained%2520on%2520data%2520from%2520multiple%2520domains%2520have%2520shown%2520strong%2520performance%2520on%2520diverse%2520modeling%2520tasks.%2520Various%2520efforts%2520have%2520been%2520made%2520to%2520develop%2520foundation%2520models%2520specific%2520to%2520electroencephalography%2520%2528EEG%2529%2520data%252C%2520which%2520records%2520brain%2520electrical%2520activity%2520as%2520time%2520series.%2520However%252C%2520no%2520comparative%2520analysis%2520of%2520EEG-specific%2520foundation%2520models%2520%2528EEGFMs%2529%2520versus%2520general%2520TSFMs%2520has%2520been%2520performed%2520on%2520EEG-specific%2520tasks.%2520We%2520introduce%2520a%2520novel%2520Spatial-Temporal%2520Adapter%2520with%2520Multi-Head%2520Pooling%2520%2528STAMP%2529%252C%2520which%2520leverages%2520univariate%2520embeddings%2520produced%2520by%2520a%2520general%2520TSFM%252C%2520implicitly%2520models%2520spatial-temporal%2520characteristics%2520of%2520EEG%2520data%252C%2520and%2520achieves%2520performance%2520comparable%2520to%2520state-of-the-art%2520EEGFMs.%2520A%2520comprehensive%2520analysis%2520is%2520performed%2520on%25208%2520benchmark%2520datasets%2520of%2520clinical%2520tasks%2520using%2520EEG%2520for%2520classification%252C%2520along%2520with%2520ablation%2520studies.%2520Our%2520proposed%2520adapter%2520is%2520lightweight%2520in%2520trainable%2520parameters%2520and%2520flexible%2520in%2520the%2520inputs%2520it%2520can%2520accommodate%252C%2520supporting%2520easy%2520modeling%2520of%2520EEG%2520data%2520using%2520TSFMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10848v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAMP%3A%20Spatial-Temporal%20Adapter%20with%20Multi-Head%20Pooling&entry.906535625=Brad%20Shook%20and%20Abby%20Turner%20and%20Jieshi%20Chen%20and%20Micha%C5%82%20Wili%C5%84ski%20and%20Mononito%20Goswami%20and%20Jonathan%20Elmer%20and%20Artur%20Dubrawski&entry.1292438233=Time%20series%20foundation%20models%20%28TSFMs%29%20pretrained%20on%20data%20from%20multiple%20domains%20have%20shown%20strong%20performance%20on%20diverse%20modeling%20tasks.%20Various%20efforts%20have%20been%20made%20to%20develop%20foundation%20models%20specific%20to%20electroencephalography%20%28EEG%29%20data%2C%20which%20records%20brain%20electrical%20activity%20as%20time%20series.%20However%2C%20no%20comparative%20analysis%20of%20EEG-specific%20foundation%20models%20%28EEGFMs%29%20versus%20general%20TSFMs%20has%20been%20performed%20on%20EEG-specific%20tasks.%20We%20introduce%20a%20novel%20Spatial-Temporal%20Adapter%20with%20Multi-Head%20Pooling%20%28STAMP%29%2C%20which%20leverages%20univariate%20embeddings%20produced%20by%20a%20general%20TSFM%2C%20implicitly%20models%20spatial-temporal%20characteristics%20of%20EEG%20data%2C%20and%20achieves%20performance%20comparable%20to%20state-of-the-art%20EEGFMs.%20A%20comprehensive%20analysis%20is%20performed%20on%208%20benchmark%20datasets%20of%20clinical%20tasks%20using%20EEG%20for%20classification%2C%20along%20with%20ablation%20studies.%20Our%20proposed%20adapter%20is%20lightweight%20in%20trainable%20parameters%20and%20flexible%20in%20the%20inputs%20it%20can%20accommodate%2C%20supporting%20easy%20modeling%20of%20EEG%20data%20using%20TSFMs.&entry.1838667208=http%3A//arxiv.org/abs/2511.10848v2&entry.124074799=Read"},
{"title": "CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering", "author": "Joni Vanherck and Steven Moonen and Brent Zoomers and Kobe Werner and Jeroen Put and Lode Jorissen and Nick Michiels", "abstract": "Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.", "link": "http://arxiv.org/abs/2511.16349v1", "date": "2025-11-20", "relevancy": 2.5187, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6385}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6283}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRISTAL%3A%20Real-time%20Camera%20Registration%20in%20Static%20LiDAR%20Scans%20using%20Neural%20Rendering&body=Title%3A%20CRISTAL%3A%20Real-time%20Camera%20Registration%20in%20Static%20LiDAR%20Scans%20using%20Neural%20Rendering%0AAuthor%3A%20Joni%20Vanherck%20and%20Steven%20Moonen%20and%20Brent%20Zoomers%20and%20Kobe%20Werner%20and%20Jeroen%20Put%20and%20Lode%20Jorissen%20and%20Nick%20Michiels%0AAbstract%3A%20Accurate%20camera%20localization%20is%20crucial%20for%20robotics%20and%20Extended%20Reality%20%28XR%29%2C%20enabling%20reliable%20navigation%20and%20alignment%20of%20virtual%20and%20real%20content.%20Existing%20visual%20methods%20often%20suffer%20from%20drift%2C%20scale%20ambiguity%2C%20and%20depend%20on%20fiducials%20or%20loop%20closure.%20This%20work%20introduces%20a%20real-time%20method%20for%20localizing%20a%20camera%20within%20a%20pre-captured%2C%20highly%20accurate%20colored%20LiDAR%20point%20cloud.%20By%20rendering%20synthetic%20views%20from%20this%20cloud%2C%202D-3D%20correspondences%20are%20established%20between%20live%20frames%20and%20the%20point%20cloud.%20A%20neural%20rendering%20technique%20narrows%20the%20domain%20gap%20between%20synthetic%20and%20real%20images%2C%20reducing%20occlusion%20and%20background%20artifacts%20to%20improve%20feature%20matching.%20The%20result%20is%20drift-free%20camera%20tracking%20with%20correct%20metric%20scale%20in%20the%20global%20LiDAR%20coordinate%20system.%20Two%20real-time%20variants%20are%20presented%3A%20Online%20Render%20and%20Match%2C%20and%20Prebuild%20and%20Localize.%20We%20demonstrate%20improved%20results%20on%20the%20ScanNet%2B%2B%20dataset%20and%20outperform%20existing%20SLAM%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRISTAL%253A%2520Real-time%2520Camera%2520Registration%2520in%2520Static%2520LiDAR%2520Scans%2520using%2520Neural%2520Rendering%26entry.906535625%3DJoni%2520Vanherck%2520and%2520Steven%2520Moonen%2520and%2520Brent%2520Zoomers%2520and%2520Kobe%2520Werner%2520and%2520Jeroen%2520Put%2520and%2520Lode%2520Jorissen%2520and%2520Nick%2520Michiels%26entry.1292438233%3DAccurate%2520camera%2520localization%2520is%2520crucial%2520for%2520robotics%2520and%2520Extended%2520Reality%2520%2528XR%2529%252C%2520enabling%2520reliable%2520navigation%2520and%2520alignment%2520of%2520virtual%2520and%2520real%2520content.%2520Existing%2520visual%2520methods%2520often%2520suffer%2520from%2520drift%252C%2520scale%2520ambiguity%252C%2520and%2520depend%2520on%2520fiducials%2520or%2520loop%2520closure.%2520This%2520work%2520introduces%2520a%2520real-time%2520method%2520for%2520localizing%2520a%2520camera%2520within%2520a%2520pre-captured%252C%2520highly%2520accurate%2520colored%2520LiDAR%2520point%2520cloud.%2520By%2520rendering%2520synthetic%2520views%2520from%2520this%2520cloud%252C%25202D-3D%2520correspondences%2520are%2520established%2520between%2520live%2520frames%2520and%2520the%2520point%2520cloud.%2520A%2520neural%2520rendering%2520technique%2520narrows%2520the%2520domain%2520gap%2520between%2520synthetic%2520and%2520real%2520images%252C%2520reducing%2520occlusion%2520and%2520background%2520artifacts%2520to%2520improve%2520feature%2520matching.%2520The%2520result%2520is%2520drift-free%2520camera%2520tracking%2520with%2520correct%2520metric%2520scale%2520in%2520the%2520global%2520LiDAR%2520coordinate%2520system.%2520Two%2520real-time%2520variants%2520are%2520presented%253A%2520Online%2520Render%2520and%2520Match%252C%2520and%2520Prebuild%2520and%2520Localize.%2520We%2520demonstrate%2520improved%2520results%2520on%2520the%2520ScanNet%252B%252B%2520dataset%2520and%2520outperform%2520existing%2520SLAM%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRISTAL%3A%20Real-time%20Camera%20Registration%20in%20Static%20LiDAR%20Scans%20using%20Neural%20Rendering&entry.906535625=Joni%20Vanherck%20and%20Steven%20Moonen%20and%20Brent%20Zoomers%20and%20Kobe%20Werner%20and%20Jeroen%20Put%20and%20Lode%20Jorissen%20and%20Nick%20Michiels&entry.1292438233=Accurate%20camera%20localization%20is%20crucial%20for%20robotics%20and%20Extended%20Reality%20%28XR%29%2C%20enabling%20reliable%20navigation%20and%20alignment%20of%20virtual%20and%20real%20content.%20Existing%20visual%20methods%20often%20suffer%20from%20drift%2C%20scale%20ambiguity%2C%20and%20depend%20on%20fiducials%20or%20loop%20closure.%20This%20work%20introduces%20a%20real-time%20method%20for%20localizing%20a%20camera%20within%20a%20pre-captured%2C%20highly%20accurate%20colored%20LiDAR%20point%20cloud.%20By%20rendering%20synthetic%20views%20from%20this%20cloud%2C%202D-3D%20correspondences%20are%20established%20between%20live%20frames%20and%20the%20point%20cloud.%20A%20neural%20rendering%20technique%20narrows%20the%20domain%20gap%20between%20synthetic%20and%20real%20images%2C%20reducing%20occlusion%20and%20background%20artifacts%20to%20improve%20feature%20matching.%20The%20result%20is%20drift-free%20camera%20tracking%20with%20correct%20metric%20scale%20in%20the%20global%20LiDAR%20coordinate%20system.%20Two%20real-time%20variants%20are%20presented%3A%20Online%20Render%20and%20Match%2C%20and%20Prebuild%20and%20Localize.%20We%20demonstrate%20improved%20results%20on%20the%20ScanNet%2B%2B%20dataset%20and%20outperform%20existing%20SLAM%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2511.16349v1&entry.124074799=Read"},
{"title": "Unsupervised Image Classification with Adaptive Nearest Neighbor Selection and Cluster Ensembles", "author": "Melih Baydar and Emre Akbas", "abstract": "Unsupervised image classification, or image clustering, aims to group unlabeled images into semantically meaningful categories. Early methods integrated representation learning and clustering within an iterative framework. However, the rise of foundational models have recently shifted focus solely to clustering, bypassing the representation learning step. In this work, we build upon a recent multi-head clustering approach by introducing adaptive nearest neighbor selection and cluster ensembling strategies to improve clustering performance. Our method, \"Image Clustering through Cluster Ensembles\" (ICCE), begins with a clustering stage, where we train multiple clustering heads on a frozen backbone, producing diverse image clusterings. We then employ a cluster ensembling technique to consolidate these potentially conflicting results into a unified consensus clustering. Finally, we train an image classifier using the consensus clustering result as pseudo-labels. ICCE achieves state-of-the-art performance on ten image classification benchmarks, achieving 99.3% accuracy on CIFAR10, 89% on CIFAR100, and 70.4% on ImageNet datasets, narrowing the performance gap with supervised methods. To the best of our knowledge, ICCE is the first fully unsupervised image classification method to exceed 70% accuracy on ImageNet.", "link": "http://arxiv.org/abs/2511.16213v1", "date": "2025-11-20", "relevancy": 2.5121, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5128}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4989}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Image%20Classification%20with%20Adaptive%20Nearest%20Neighbor%20Selection%20and%20Cluster%20Ensembles&body=Title%3A%20Unsupervised%20Image%20Classification%20with%20Adaptive%20Nearest%20Neighbor%20Selection%20and%20Cluster%20Ensembles%0AAuthor%3A%20Melih%20Baydar%20and%20Emre%20Akbas%0AAbstract%3A%20Unsupervised%20image%20classification%2C%20or%20image%20clustering%2C%20aims%20to%20group%20unlabeled%20images%20into%20semantically%20meaningful%20categories.%20Early%20methods%20integrated%20representation%20learning%20and%20clustering%20within%20an%20iterative%20framework.%20However%2C%20the%20rise%20of%20foundational%20models%20have%20recently%20shifted%20focus%20solely%20to%20clustering%2C%20bypassing%20the%20representation%20learning%20step.%20In%20this%20work%2C%20we%20build%20upon%20a%20recent%20multi-head%20clustering%20approach%20by%20introducing%20adaptive%20nearest%20neighbor%20selection%20and%20cluster%20ensembling%20strategies%20to%20improve%20clustering%20performance.%20Our%20method%2C%20%22Image%20Clustering%20through%20Cluster%20Ensembles%22%20%28ICCE%29%2C%20begins%20with%20a%20clustering%20stage%2C%20where%20we%20train%20multiple%20clustering%20heads%20on%20a%20frozen%20backbone%2C%20producing%20diverse%20image%20clusterings.%20We%20then%20employ%20a%20cluster%20ensembling%20technique%20to%20consolidate%20these%20potentially%20conflicting%20results%20into%20a%20unified%20consensus%20clustering.%20Finally%2C%20we%20train%20an%20image%20classifier%20using%20the%20consensus%20clustering%20result%20as%20pseudo-labels.%20ICCE%20achieves%20state-of-the-art%20performance%20on%20ten%20image%20classification%20benchmarks%2C%20achieving%2099.3%25%20accuracy%20on%20CIFAR10%2C%2089%25%20on%20CIFAR100%2C%20and%2070.4%25%20on%20ImageNet%20datasets%2C%20narrowing%20the%20performance%20gap%20with%20supervised%20methods.%20To%20the%20best%20of%20our%20knowledge%2C%20ICCE%20is%20the%20first%20fully%20unsupervised%20image%20classification%20method%20to%20exceed%2070%25%20accuracy%20on%20ImageNet.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Image%2520Classification%2520with%2520Adaptive%2520Nearest%2520Neighbor%2520Selection%2520and%2520Cluster%2520Ensembles%26entry.906535625%3DMelih%2520Baydar%2520and%2520Emre%2520Akbas%26entry.1292438233%3DUnsupervised%2520image%2520classification%252C%2520or%2520image%2520clustering%252C%2520aims%2520to%2520group%2520unlabeled%2520images%2520into%2520semantically%2520meaningful%2520categories.%2520Early%2520methods%2520integrated%2520representation%2520learning%2520and%2520clustering%2520within%2520an%2520iterative%2520framework.%2520However%252C%2520the%2520rise%2520of%2520foundational%2520models%2520have%2520recently%2520shifted%2520focus%2520solely%2520to%2520clustering%252C%2520bypassing%2520the%2520representation%2520learning%2520step.%2520In%2520this%2520work%252C%2520we%2520build%2520upon%2520a%2520recent%2520multi-head%2520clustering%2520approach%2520by%2520introducing%2520adaptive%2520nearest%2520neighbor%2520selection%2520and%2520cluster%2520ensembling%2520strategies%2520to%2520improve%2520clustering%2520performance.%2520Our%2520method%252C%2520%2522Image%2520Clustering%2520through%2520Cluster%2520Ensembles%2522%2520%2528ICCE%2529%252C%2520begins%2520with%2520a%2520clustering%2520stage%252C%2520where%2520we%2520train%2520multiple%2520clustering%2520heads%2520on%2520a%2520frozen%2520backbone%252C%2520producing%2520diverse%2520image%2520clusterings.%2520We%2520then%2520employ%2520a%2520cluster%2520ensembling%2520technique%2520to%2520consolidate%2520these%2520potentially%2520conflicting%2520results%2520into%2520a%2520unified%2520consensus%2520clustering.%2520Finally%252C%2520we%2520train%2520an%2520image%2520classifier%2520using%2520the%2520consensus%2520clustering%2520result%2520as%2520pseudo-labels.%2520ICCE%2520achieves%2520state-of-the-art%2520performance%2520on%2520ten%2520image%2520classification%2520benchmarks%252C%2520achieving%252099.3%2525%2520accuracy%2520on%2520CIFAR10%252C%252089%2525%2520on%2520CIFAR100%252C%2520and%252070.4%2525%2520on%2520ImageNet%2520datasets%252C%2520narrowing%2520the%2520performance%2520gap%2520with%2520supervised%2520methods.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520ICCE%2520is%2520the%2520first%2520fully%2520unsupervised%2520image%2520classification%2520method%2520to%2520exceed%252070%2525%2520accuracy%2520on%2520ImageNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Image%20Classification%20with%20Adaptive%20Nearest%20Neighbor%20Selection%20and%20Cluster%20Ensembles&entry.906535625=Melih%20Baydar%20and%20Emre%20Akbas&entry.1292438233=Unsupervised%20image%20classification%2C%20or%20image%20clustering%2C%20aims%20to%20group%20unlabeled%20images%20into%20semantically%20meaningful%20categories.%20Early%20methods%20integrated%20representation%20learning%20and%20clustering%20within%20an%20iterative%20framework.%20However%2C%20the%20rise%20of%20foundational%20models%20have%20recently%20shifted%20focus%20solely%20to%20clustering%2C%20bypassing%20the%20representation%20learning%20step.%20In%20this%20work%2C%20we%20build%20upon%20a%20recent%20multi-head%20clustering%20approach%20by%20introducing%20adaptive%20nearest%20neighbor%20selection%20and%20cluster%20ensembling%20strategies%20to%20improve%20clustering%20performance.%20Our%20method%2C%20%22Image%20Clustering%20through%20Cluster%20Ensembles%22%20%28ICCE%29%2C%20begins%20with%20a%20clustering%20stage%2C%20where%20we%20train%20multiple%20clustering%20heads%20on%20a%20frozen%20backbone%2C%20producing%20diverse%20image%20clusterings.%20We%20then%20employ%20a%20cluster%20ensembling%20technique%20to%20consolidate%20these%20potentially%20conflicting%20results%20into%20a%20unified%20consensus%20clustering.%20Finally%2C%20we%20train%20an%20image%20classifier%20using%20the%20consensus%20clustering%20result%20as%20pseudo-labels.%20ICCE%20achieves%20state-of-the-art%20performance%20on%20ten%20image%20classification%20benchmarks%2C%20achieving%2099.3%25%20accuracy%20on%20CIFAR10%2C%2089%25%20on%20CIFAR100%2C%20and%2070.4%25%20on%20ImageNet%20datasets%2C%20narrowing%20the%20performance%20gap%20with%20supervised%20methods.%20To%20the%20best%20of%20our%20knowledge%2C%20ICCE%20is%20the%20first%20fully%20unsupervised%20image%20classification%20method%20to%20exceed%2070%25%20accuracy%20on%20ImageNet.&entry.1838667208=http%3A//arxiv.org/abs/2511.16213v1&entry.124074799=Read"},
{"title": "MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control", "author": "Mengting Wei and Tuomas Varanka and Xingxun Jiang and Huai-Qian Khor and Guoying Zhao", "abstract": "We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace.", "link": "http://arxiv.org/abs/2501.02260v3", "date": "2025-11-20", "relevancy": 2.509, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6607}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6166}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicFace%3A%20High-Fidelity%20Facial%20Expression%20Editing%20with%20Action-Unit%20Control&body=Title%3A%20MagicFace%3A%20High-Fidelity%20Facial%20Expression%20Editing%20with%20Action-Unit%20Control%0AAuthor%3A%20Mengting%20Wei%20and%20Tuomas%20Varanka%20and%20Xingxun%20Jiang%20and%20Huai-Qian%20Khor%20and%20Guoying%20Zhao%0AAbstract%3A%20We%20address%20the%20problem%20of%20facial%20expression%20editing%20by%20controling%20the%20relative%20variation%20of%20facial%20action-unit%20%28AU%29%20from%20the%20same%20person.%20This%20enables%20us%20to%20edit%20this%20specific%20person%27s%20expression%20in%20a%20fine-grained%2C%20continuous%20and%20interpretable%20manner%2C%20while%20preserving%20their%20identity%2C%20pose%2C%20background%20and%20detailed%20facial%20attributes.%20Key%20to%20our%20model%2C%20which%20we%20dub%20MagicFace%2C%20is%20a%20diffusion%20model%20conditioned%20on%20AU%20variations%20and%20an%20ID%20encoder%20to%20preserve%20facial%20details%20of%20high%20consistency.%20Specifically%2C%20to%20preserve%20the%20facial%20details%20with%20the%20input%20identity%2C%20we%20leverage%20the%20power%20of%20pretrained%20Stable-Diffusion%20models%20and%20design%20an%20ID%20encoder%20to%20merge%20appearance%20features%20through%20self-attention.%20To%20keep%20background%20and%20pose%20consistency%2C%20we%20introduce%20an%20efficient%20Attribute%20Controller%20by%20explicitly%20informing%20the%20model%20of%20current%20background%20and%20pose%20of%20the%20target.%20By%20injecting%20AU%20variations%20into%20a%20denoising%20UNet%2C%20our%20model%20can%20animate%20arbitrary%20identities%20with%20various%20AU%20combinations%2C%20yielding%20superior%20results%20in%20high-fidelity%20expression%20editing%20compared%20to%20other%20facial%20expression%20editing%20works.%20Code%20is%20publicly%20available%20at%20https%3A//github.com/weimengting/MagicFace.%0ALink%3A%20http%3A//arxiv.org/abs/2501.02260v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicFace%253A%2520High-Fidelity%2520Facial%2520Expression%2520Editing%2520with%2520Action-Unit%2520Control%26entry.906535625%3DMengting%2520Wei%2520and%2520Tuomas%2520Varanka%2520and%2520Xingxun%2520Jiang%2520and%2520Huai-Qian%2520Khor%2520and%2520Guoying%2520Zhao%26entry.1292438233%3DWe%2520address%2520the%2520problem%2520of%2520facial%2520expression%2520editing%2520by%2520controling%2520the%2520relative%2520variation%2520of%2520facial%2520action-unit%2520%2528AU%2529%2520from%2520the%2520same%2520person.%2520This%2520enables%2520us%2520to%2520edit%2520this%2520specific%2520person%2527s%2520expression%2520in%2520a%2520fine-grained%252C%2520continuous%2520and%2520interpretable%2520manner%252C%2520while%2520preserving%2520their%2520identity%252C%2520pose%252C%2520background%2520and%2520detailed%2520facial%2520attributes.%2520Key%2520to%2520our%2520model%252C%2520which%2520we%2520dub%2520MagicFace%252C%2520is%2520a%2520diffusion%2520model%2520conditioned%2520on%2520AU%2520variations%2520and%2520an%2520ID%2520encoder%2520to%2520preserve%2520facial%2520details%2520of%2520high%2520consistency.%2520Specifically%252C%2520to%2520preserve%2520the%2520facial%2520details%2520with%2520the%2520input%2520identity%252C%2520we%2520leverage%2520the%2520power%2520of%2520pretrained%2520Stable-Diffusion%2520models%2520and%2520design%2520an%2520ID%2520encoder%2520to%2520merge%2520appearance%2520features%2520through%2520self-attention.%2520To%2520keep%2520background%2520and%2520pose%2520consistency%252C%2520we%2520introduce%2520an%2520efficient%2520Attribute%2520Controller%2520by%2520explicitly%2520informing%2520the%2520model%2520of%2520current%2520background%2520and%2520pose%2520of%2520the%2520target.%2520By%2520injecting%2520AU%2520variations%2520into%2520a%2520denoising%2520UNet%252C%2520our%2520model%2520can%2520animate%2520arbitrary%2520identities%2520with%2520various%2520AU%2520combinations%252C%2520yielding%2520superior%2520results%2520in%2520high-fidelity%2520expression%2520editing%2520compared%2520to%2520other%2520facial%2520expression%2520editing%2520works.%2520Code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/weimengting/MagicFace.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02260v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicFace%3A%20High-Fidelity%20Facial%20Expression%20Editing%20with%20Action-Unit%20Control&entry.906535625=Mengting%20Wei%20and%20Tuomas%20Varanka%20and%20Xingxun%20Jiang%20and%20Huai-Qian%20Khor%20and%20Guoying%20Zhao&entry.1292438233=We%20address%20the%20problem%20of%20facial%20expression%20editing%20by%20controling%20the%20relative%20variation%20of%20facial%20action-unit%20%28AU%29%20from%20the%20same%20person.%20This%20enables%20us%20to%20edit%20this%20specific%20person%27s%20expression%20in%20a%20fine-grained%2C%20continuous%20and%20interpretable%20manner%2C%20while%20preserving%20their%20identity%2C%20pose%2C%20background%20and%20detailed%20facial%20attributes.%20Key%20to%20our%20model%2C%20which%20we%20dub%20MagicFace%2C%20is%20a%20diffusion%20model%20conditioned%20on%20AU%20variations%20and%20an%20ID%20encoder%20to%20preserve%20facial%20details%20of%20high%20consistency.%20Specifically%2C%20to%20preserve%20the%20facial%20details%20with%20the%20input%20identity%2C%20we%20leverage%20the%20power%20of%20pretrained%20Stable-Diffusion%20models%20and%20design%20an%20ID%20encoder%20to%20merge%20appearance%20features%20through%20self-attention.%20To%20keep%20background%20and%20pose%20consistency%2C%20we%20introduce%20an%20efficient%20Attribute%20Controller%20by%20explicitly%20informing%20the%20model%20of%20current%20background%20and%20pose%20of%20the%20target.%20By%20injecting%20AU%20variations%20into%20a%20denoising%20UNet%2C%20our%20model%20can%20animate%20arbitrary%20identities%20with%20various%20AU%20combinations%2C%20yielding%20superior%20results%20in%20high-fidelity%20expression%20editing%20compared%20to%20other%20facial%20expression%20editing%20works.%20Code%20is%20publicly%20available%20at%20https%3A//github.com/weimengting/MagicFace.&entry.1838667208=http%3A//arxiv.org/abs/2501.02260v3&entry.124074799=Read"},
{"title": "BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization", "author": "Rahul Kumar and Vipul Baghel and Sudhanshu Singh and Bikash Kumar Badatya and Shivam Yadav and Babji Srinivasan and Ravi Hegde", "abstract": "Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.", "link": "http://arxiv.org/abs/2511.16524v1", "date": "2025-11-20", "relevancy": 2.4689, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5037}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4968}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoxingVI%3A%20A%20Multi-Modal%20Benchmark%20for%20Boxing%20Action%20Recognition%20and%20Localization&body=Title%3A%20BoxingVI%3A%20A%20Multi-Modal%20Benchmark%20for%20Boxing%20Action%20Recognition%20and%20Localization%0AAuthor%3A%20Rahul%20Kumar%20and%20Vipul%20Baghel%20and%20Sudhanshu%20Singh%20and%20Bikash%20Kumar%20Badatya%20and%20Shivam%20Yadav%20and%20Babji%20Srinivasan%20and%20Ravi%20Hegde%0AAbstract%3A%20Accurate%20analysis%20of%20combat%20sports%20using%20computer%20vision%20has%20gained%20traction%20in%20recent%20years%2C%20yet%20the%20development%20of%20robust%20datasets%20remains%20a%20major%20bottleneck%20due%20to%20the%20dynamic%2C%20unstructured%20nature%20of%20actions%20and%20variations%20in%20recording%20environments.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%2C%20well-annotated%20video%20dataset%20tailored%20for%20punch%20detection%20and%20classification%20in%20boxing.%20The%20dataset%20comprises%206%2C915%20high-quality%20punch%20clips%20categorized%20into%20six%20distinct%20punch%20types%2C%20extracted%20from%2020%20publicly%20available%20YouTube%20sparring%20sessions%20and%20involving%2018%20different%20athletes.%20Each%20clip%20is%20manually%20segmented%20and%20labeled%20to%20ensure%20precise%20temporal%20boundaries%20and%20class%20consistency%2C%20capturing%20a%20wide%20range%20of%20motion%20styles%2C%20camera%20angles%2C%20and%20athlete%20physiques.%20This%20dataset%20is%20specifically%20curated%20to%20support%20research%20in%20real-time%20vision-based%20action%20recognition%2C%20especially%20in%20low-resource%20and%20unconstrained%20environments.%20By%20providing%20a%20rich%20benchmark%20with%20diverse%20punch%20examples%2C%20this%20contribution%20aims%20to%20accelerate%20progress%20in%20movement%20analysis%2C%20automated%20coaching%2C%20and%20performance%20assessment%20within%20boxing%20and%20related%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoxingVI%253A%2520A%2520Multi-Modal%2520Benchmark%2520for%2520Boxing%2520Action%2520Recognition%2520and%2520Localization%26entry.906535625%3DRahul%2520Kumar%2520and%2520Vipul%2520Baghel%2520and%2520Sudhanshu%2520Singh%2520and%2520Bikash%2520Kumar%2520Badatya%2520and%2520Shivam%2520Yadav%2520and%2520Babji%2520Srinivasan%2520and%2520Ravi%2520Hegde%26entry.1292438233%3DAccurate%2520analysis%2520of%2520combat%2520sports%2520using%2520computer%2520vision%2520has%2520gained%2520traction%2520in%2520recent%2520years%252C%2520yet%2520the%2520development%2520of%2520robust%2520datasets%2520remains%2520a%2520major%2520bottleneck%2520due%2520to%2520the%2520dynamic%252C%2520unstructured%2520nature%2520of%2520actions%2520and%2520variations%2520in%2520recording%2520environments.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520comprehensive%252C%2520well-annotated%2520video%2520dataset%2520tailored%2520for%2520punch%2520detection%2520and%2520classification%2520in%2520boxing.%2520The%2520dataset%2520comprises%25206%252C915%2520high-quality%2520punch%2520clips%2520categorized%2520into%2520six%2520distinct%2520punch%2520types%252C%2520extracted%2520from%252020%2520publicly%2520available%2520YouTube%2520sparring%2520sessions%2520and%2520involving%252018%2520different%2520athletes.%2520Each%2520clip%2520is%2520manually%2520segmented%2520and%2520labeled%2520to%2520ensure%2520precise%2520temporal%2520boundaries%2520and%2520class%2520consistency%252C%2520capturing%2520a%2520wide%2520range%2520of%2520motion%2520styles%252C%2520camera%2520angles%252C%2520and%2520athlete%2520physiques.%2520This%2520dataset%2520is%2520specifically%2520curated%2520to%2520support%2520research%2520in%2520real-time%2520vision-based%2520action%2520recognition%252C%2520especially%2520in%2520low-resource%2520and%2520unconstrained%2520environments.%2520By%2520providing%2520a%2520rich%2520benchmark%2520with%2520diverse%2520punch%2520examples%252C%2520this%2520contribution%2520aims%2520to%2520accelerate%2520progress%2520in%2520movement%2520analysis%252C%2520automated%2520coaching%252C%2520and%2520performance%2520assessment%2520within%2520boxing%2520and%2520related%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoxingVI%3A%20A%20Multi-Modal%20Benchmark%20for%20Boxing%20Action%20Recognition%20and%20Localization&entry.906535625=Rahul%20Kumar%20and%20Vipul%20Baghel%20and%20Sudhanshu%20Singh%20and%20Bikash%20Kumar%20Badatya%20and%20Shivam%20Yadav%20and%20Babji%20Srinivasan%20and%20Ravi%20Hegde&entry.1292438233=Accurate%20analysis%20of%20combat%20sports%20using%20computer%20vision%20has%20gained%20traction%20in%20recent%20years%2C%20yet%20the%20development%20of%20robust%20datasets%20remains%20a%20major%20bottleneck%20due%20to%20the%20dynamic%2C%20unstructured%20nature%20of%20actions%20and%20variations%20in%20recording%20environments.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%2C%20well-annotated%20video%20dataset%20tailored%20for%20punch%20detection%20and%20classification%20in%20boxing.%20The%20dataset%20comprises%206%2C915%20high-quality%20punch%20clips%20categorized%20into%20six%20distinct%20punch%20types%2C%20extracted%20from%2020%20publicly%20available%20YouTube%20sparring%20sessions%20and%20involving%2018%20different%20athletes.%20Each%20clip%20is%20manually%20segmented%20and%20labeled%20to%20ensure%20precise%20temporal%20boundaries%20and%20class%20consistency%2C%20capturing%20a%20wide%20range%20of%20motion%20styles%2C%20camera%20angles%2C%20and%20athlete%20physiques.%20This%20dataset%20is%20specifically%20curated%20to%20support%20research%20in%20real-time%20vision-based%20action%20recognition%2C%20especially%20in%20low-resource%20and%20unconstrained%20environments.%20By%20providing%20a%20rich%20benchmark%20with%20diverse%20punch%20examples%2C%20this%20contribution%20aims%20to%20accelerate%20progress%20in%20movement%20analysis%2C%20automated%20coaching%2C%20and%20performance%20assessment%20within%20boxing%20and%20related%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2511.16524v1&entry.124074799=Read"},
{"title": "A low-rank non-convex norm method for multiview graph clustering", "author": "Alaeddine Zahir and Khalide Jbilou and Ahmed Ratnani", "abstract": "This study introduces a novel technique for multi-view clustering known as the \"Consensus Graph-Based Multi-View Clustering Method Using Low-Rank Non-Convex Norm\" (CGMVC-NC). Multi-view clustering is a challenging task in machine learning as it requires the integration of information from multiple data sources or views to cluster data points accurately. The suggested approach makes use of the structural characteristics of multi-view data tensors, introducing a non-convex tensor norm to identify correlations between these views. In contrast to conventional methods, this approach demonstrates superior clustering accuracy across several benchmark datasets. Despite the non-convex nature of the tensor norm used, the proposed method remains amenable to efficient optimization using existing algorithms. The approach provides a valuable tool for multi-view data analysis and has the potential to enhance our understanding of complex systems in various fields. Further research can explore the application of this method to other types of data and extend it to other machine-learning tasks.", "link": "http://arxiv.org/abs/2312.11157v2", "date": "2025-11-20", "relevancy": 2.4675, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.506}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4873}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20low-rank%20non-convex%20norm%20method%20for%20multiview%20graph%20clustering&body=Title%3A%20A%20low-rank%20non-convex%20norm%20method%20for%20multiview%20graph%20clustering%0AAuthor%3A%20Alaeddine%20Zahir%20and%20Khalide%20Jbilou%20and%20Ahmed%20Ratnani%0AAbstract%3A%20This%20study%20introduces%20a%20novel%20technique%20for%20multi-view%20clustering%20known%20as%20the%20%22Consensus%20Graph-Based%20Multi-View%20Clustering%20Method%20Using%20Low-Rank%20Non-Convex%20Norm%22%20%28CGMVC-NC%29.%20Multi-view%20clustering%20is%20a%20challenging%20task%20in%20machine%20learning%20as%20it%20requires%20the%20integration%20of%20information%20from%20multiple%20data%20sources%20or%20views%20to%20cluster%20data%20points%20accurately.%20The%20suggested%20approach%20makes%20use%20of%20the%20structural%20characteristics%20of%20multi-view%20data%20tensors%2C%20introducing%20a%20non-convex%20tensor%20norm%20to%20identify%20correlations%20between%20these%20views.%20In%20contrast%20to%20conventional%20methods%2C%20this%20approach%20demonstrates%20superior%20clustering%20accuracy%20across%20several%20benchmark%20datasets.%20Despite%20the%20non-convex%20nature%20of%20the%20tensor%20norm%20used%2C%20the%20proposed%20method%20remains%20amenable%20to%20efficient%20optimization%20using%20existing%20algorithms.%20The%20approach%20provides%20a%20valuable%20tool%20for%20multi-view%20data%20analysis%20and%20has%20the%20potential%20to%20enhance%20our%20understanding%20of%20complex%20systems%20in%20various%20fields.%20Further%20research%20can%20explore%20the%20application%20of%20this%20method%20to%20other%20types%20of%20data%20and%20extend%20it%20to%20other%20machine-learning%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2312.11157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520low-rank%2520non-convex%2520norm%2520method%2520for%2520multiview%2520graph%2520clustering%26entry.906535625%3DAlaeddine%2520Zahir%2520and%2520Khalide%2520Jbilou%2520and%2520Ahmed%2520Ratnani%26entry.1292438233%3DThis%2520study%2520introduces%2520a%2520novel%2520technique%2520for%2520multi-view%2520clustering%2520known%2520as%2520the%2520%2522Consensus%2520Graph-Based%2520Multi-View%2520Clustering%2520Method%2520Using%2520Low-Rank%2520Non-Convex%2520Norm%2522%2520%2528CGMVC-NC%2529.%2520Multi-view%2520clustering%2520is%2520a%2520challenging%2520task%2520in%2520machine%2520learning%2520as%2520it%2520requires%2520the%2520integration%2520of%2520information%2520from%2520multiple%2520data%2520sources%2520or%2520views%2520to%2520cluster%2520data%2520points%2520accurately.%2520The%2520suggested%2520approach%2520makes%2520use%2520of%2520the%2520structural%2520characteristics%2520of%2520multi-view%2520data%2520tensors%252C%2520introducing%2520a%2520non-convex%2520tensor%2520norm%2520to%2520identify%2520correlations%2520between%2520these%2520views.%2520In%2520contrast%2520to%2520conventional%2520methods%252C%2520this%2520approach%2520demonstrates%2520superior%2520clustering%2520accuracy%2520across%2520several%2520benchmark%2520datasets.%2520Despite%2520the%2520non-convex%2520nature%2520of%2520the%2520tensor%2520norm%2520used%252C%2520the%2520proposed%2520method%2520remains%2520amenable%2520to%2520efficient%2520optimization%2520using%2520existing%2520algorithms.%2520The%2520approach%2520provides%2520a%2520valuable%2520tool%2520for%2520multi-view%2520data%2520analysis%2520and%2520has%2520the%2520potential%2520to%2520enhance%2520our%2520understanding%2520of%2520complex%2520systems%2520in%2520various%2520fields.%2520Further%2520research%2520can%2520explore%2520the%2520application%2520of%2520this%2520method%2520to%2520other%2520types%2520of%2520data%2520and%2520extend%2520it%2520to%2520other%2520machine-learning%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20low-rank%20non-convex%20norm%20method%20for%20multiview%20graph%20clustering&entry.906535625=Alaeddine%20Zahir%20and%20Khalide%20Jbilou%20and%20Ahmed%20Ratnani&entry.1292438233=This%20study%20introduces%20a%20novel%20technique%20for%20multi-view%20clustering%20known%20as%20the%20%22Consensus%20Graph-Based%20Multi-View%20Clustering%20Method%20Using%20Low-Rank%20Non-Convex%20Norm%22%20%28CGMVC-NC%29.%20Multi-view%20clustering%20is%20a%20challenging%20task%20in%20machine%20learning%20as%20it%20requires%20the%20integration%20of%20information%20from%20multiple%20data%20sources%20or%20views%20to%20cluster%20data%20points%20accurately.%20The%20suggested%20approach%20makes%20use%20of%20the%20structural%20characteristics%20of%20multi-view%20data%20tensors%2C%20introducing%20a%20non-convex%20tensor%20norm%20to%20identify%20correlations%20between%20these%20views.%20In%20contrast%20to%20conventional%20methods%2C%20this%20approach%20demonstrates%20superior%20clustering%20accuracy%20across%20several%20benchmark%20datasets.%20Despite%20the%20non-convex%20nature%20of%20the%20tensor%20norm%20used%2C%20the%20proposed%20method%20remains%20amenable%20to%20efficient%20optimization%20using%20existing%20algorithms.%20The%20approach%20provides%20a%20valuable%20tool%20for%20multi-view%20data%20analysis%20and%20has%20the%20potential%20to%20enhance%20our%20understanding%20of%20complex%20systems%20in%20various%20fields.%20Further%20research%20can%20explore%20the%20application%20of%20this%20method%20to%20other%20types%20of%20data%20and%20extend%20it%20to%20other%20machine-learning%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2312.11157v2&entry.124074799=Read"},
{"title": "DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks", "author": "Vincenzo Pomponi and Paolo Franceschi and Stefano Baraldo and Loris Roveda and Oliver Avram and Luca Maria Gambardella and Anna Valente", "abstract": "Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.", "link": "http://arxiv.org/abs/2511.16223v1", "date": "2025-11-20", "relevancy": 2.4642, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.623}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6166}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaMimicGen%3A%20A%20Data%20Generation%20Framework%20for%20Robot%20Learning%20of%20Dynamic%20Tasks&body=Title%3A%20DynaMimicGen%3A%20A%20Data%20Generation%20Framework%20for%20Robot%20Learning%20of%20Dynamic%20Tasks%0AAuthor%3A%20Vincenzo%20Pomponi%20and%20Paolo%20Franceschi%20and%20Stefano%20Baraldo%20and%20Loris%20Roveda%20and%20Oliver%20Avram%20and%20Luca%20Maria%20Gambardella%20and%20Anna%20Valente%0AAbstract%3A%20Learning%20robust%20manipulation%20policies%20typically%20requires%20large%20and%20diverse%20datasets%2C%20the%20collection%20of%20which%20is%20time-consuming%2C%20labor-intensive%2C%20and%20often%20impractical%20for%20dynamic%20environments.%20In%20this%20work%2C%20we%20introduce%20DynaMimicGen%20%28D-MG%29%2C%20a%20scalable%20dataset%20generation%20framework%20that%20enables%20policy%20training%20from%20minimal%20human%20supervision%20while%20uniquely%20supporting%20dynamic%20task%20settings.%20Given%20only%20a%20few%20human%20demonstrations%2C%20D-MG%20first%20segments%20the%20demonstrations%20into%20meaningful%20sub-tasks%2C%20then%20leverages%20Dynamic%20Movement%20Primitives%20%28DMPs%29%20to%20adapt%20and%20generalize%20the%20demonstrated%20behaviors%20to%20novel%20and%20dynamically%20changing%20environments.%20Improving%20prior%20methods%20that%20rely%20on%20static%20assumptions%20or%20simplistic%20trajectory%20interpolation%2C%20D-MG%20produces%20smooth%2C%20realistic%2C%20and%20task-consistent%20Cartesian%20trajectories%20that%20adapt%20in%20real%20time%20to%20changes%20in%20object%20poses%2C%20robot%20states%2C%20or%20scene%20geometry%20during%20task%20execution.%20Our%20method%20supports%20different%20scenarios%20-%20including%20scene%20layouts%2C%20object%20instances%2C%20and%20robot%20configurations%20-%20making%20it%20suitable%20for%20both%20static%20and%20highly%20dynamic%20manipulation%20tasks.%20We%20show%20that%20robot%20agents%20trained%20via%20imitation%20learning%20on%20D-MG-generated%20data%20achieve%20strong%20performance%20across%20long-horizon%20and%20contact-rich%20benchmarks%2C%20including%20tasks%20like%20cube%20stacking%20and%20placing%20mugs%20in%20drawers%2C%20even%20under%20unpredictable%20environment%20changes.%20By%20eliminating%20the%20need%20for%20extensive%20human%20demonstrations%20and%20enabling%20generalization%20in%20dynamic%20settings%2C%20D-MG%20offers%20a%20powerful%20and%20efficient%20alternative%20to%20manual%20data%20collection%2C%20paving%20the%20way%20toward%20scalable%2C%20autonomous%20robot%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaMimicGen%253A%2520A%2520Data%2520Generation%2520Framework%2520for%2520Robot%2520Learning%2520of%2520Dynamic%2520Tasks%26entry.906535625%3DVincenzo%2520Pomponi%2520and%2520Paolo%2520Franceschi%2520and%2520Stefano%2520Baraldo%2520and%2520Loris%2520Roveda%2520and%2520Oliver%2520Avram%2520and%2520Luca%2520Maria%2520Gambardella%2520and%2520Anna%2520Valente%26entry.1292438233%3DLearning%2520robust%2520manipulation%2520policies%2520typically%2520requires%2520large%2520and%2520diverse%2520datasets%252C%2520the%2520collection%2520of%2520which%2520is%2520time-consuming%252C%2520labor-intensive%252C%2520and%2520often%2520impractical%2520for%2520dynamic%2520environments.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DynaMimicGen%2520%2528D-MG%2529%252C%2520a%2520scalable%2520dataset%2520generation%2520framework%2520that%2520enables%2520policy%2520training%2520from%2520minimal%2520human%2520supervision%2520while%2520uniquely%2520supporting%2520dynamic%2520task%2520settings.%2520Given%2520only%2520a%2520few%2520human%2520demonstrations%252C%2520D-MG%2520first%2520segments%2520the%2520demonstrations%2520into%2520meaningful%2520sub-tasks%252C%2520then%2520leverages%2520Dynamic%2520Movement%2520Primitives%2520%2528DMPs%2529%2520to%2520adapt%2520and%2520generalize%2520the%2520demonstrated%2520behaviors%2520to%2520novel%2520and%2520dynamically%2520changing%2520environments.%2520Improving%2520prior%2520methods%2520that%2520rely%2520on%2520static%2520assumptions%2520or%2520simplistic%2520trajectory%2520interpolation%252C%2520D-MG%2520produces%2520smooth%252C%2520realistic%252C%2520and%2520task-consistent%2520Cartesian%2520trajectories%2520that%2520adapt%2520in%2520real%2520time%2520to%2520changes%2520in%2520object%2520poses%252C%2520robot%2520states%252C%2520or%2520scene%2520geometry%2520during%2520task%2520execution.%2520Our%2520method%2520supports%2520different%2520scenarios%2520-%2520including%2520scene%2520layouts%252C%2520object%2520instances%252C%2520and%2520robot%2520configurations%2520-%2520making%2520it%2520suitable%2520for%2520both%2520static%2520and%2520highly%2520dynamic%2520manipulation%2520tasks.%2520We%2520show%2520that%2520robot%2520agents%2520trained%2520via%2520imitation%2520learning%2520on%2520D-MG-generated%2520data%2520achieve%2520strong%2520performance%2520across%2520long-horizon%2520and%2520contact-rich%2520benchmarks%252C%2520including%2520tasks%2520like%2520cube%2520stacking%2520and%2520placing%2520mugs%2520in%2520drawers%252C%2520even%2520under%2520unpredictable%2520environment%2520changes.%2520By%2520eliminating%2520the%2520need%2520for%2520extensive%2520human%2520demonstrations%2520and%2520enabling%2520generalization%2520in%2520dynamic%2520settings%252C%2520D-MG%2520offers%2520a%2520powerful%2520and%2520efficient%2520alternative%2520to%2520manual%2520data%2520collection%252C%2520paving%2520the%2520way%2520toward%2520scalable%252C%2520autonomous%2520robot%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaMimicGen%3A%20A%20Data%20Generation%20Framework%20for%20Robot%20Learning%20of%20Dynamic%20Tasks&entry.906535625=Vincenzo%20Pomponi%20and%20Paolo%20Franceschi%20and%20Stefano%20Baraldo%20and%20Loris%20Roveda%20and%20Oliver%20Avram%20and%20Luca%20Maria%20Gambardella%20and%20Anna%20Valente&entry.1292438233=Learning%20robust%20manipulation%20policies%20typically%20requires%20large%20and%20diverse%20datasets%2C%20the%20collection%20of%20which%20is%20time-consuming%2C%20labor-intensive%2C%20and%20often%20impractical%20for%20dynamic%20environments.%20In%20this%20work%2C%20we%20introduce%20DynaMimicGen%20%28D-MG%29%2C%20a%20scalable%20dataset%20generation%20framework%20that%20enables%20policy%20training%20from%20minimal%20human%20supervision%20while%20uniquely%20supporting%20dynamic%20task%20settings.%20Given%20only%20a%20few%20human%20demonstrations%2C%20D-MG%20first%20segments%20the%20demonstrations%20into%20meaningful%20sub-tasks%2C%20then%20leverages%20Dynamic%20Movement%20Primitives%20%28DMPs%29%20to%20adapt%20and%20generalize%20the%20demonstrated%20behaviors%20to%20novel%20and%20dynamically%20changing%20environments.%20Improving%20prior%20methods%20that%20rely%20on%20static%20assumptions%20or%20simplistic%20trajectory%20interpolation%2C%20D-MG%20produces%20smooth%2C%20realistic%2C%20and%20task-consistent%20Cartesian%20trajectories%20that%20adapt%20in%20real%20time%20to%20changes%20in%20object%20poses%2C%20robot%20states%2C%20or%20scene%20geometry%20during%20task%20execution.%20Our%20method%20supports%20different%20scenarios%20-%20including%20scene%20layouts%2C%20object%20instances%2C%20and%20robot%20configurations%20-%20making%20it%20suitable%20for%20both%20static%20and%20highly%20dynamic%20manipulation%20tasks.%20We%20show%20that%20robot%20agents%20trained%20via%20imitation%20learning%20on%20D-MG-generated%20data%20achieve%20strong%20performance%20across%20long-horizon%20and%20contact-rich%20benchmarks%2C%20including%20tasks%20like%20cube%20stacking%20and%20placing%20mugs%20in%20drawers%2C%20even%20under%20unpredictable%20environment%20changes.%20By%20eliminating%20the%20need%20for%20extensive%20human%20demonstrations%20and%20enabling%20generalization%20in%20dynamic%20settings%2C%20D-MG%20offers%20a%20powerful%20and%20efficient%20alternative%20to%20manual%20data%20collection%2C%20paving%20the%20way%20toward%20scalable%2C%20autonomous%20robot%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2511.16223v1&entry.124074799=Read"},
{"title": "DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration", "author": "Meng-Cheng Shih and Tsai-Ling Huang and Yu-Heng Shih and Hong-Han Shuai and Hsuan-Tung Liu and Yi-Ren Yeh and Ching-Chun Huang", "abstract": "Offline signature verification (OSV) is a frequently utilized technology in forensics. This paper proposes a new model, DetailSemNet, for OSV. Unlike previous methods that rely on holistic features for pair comparisons, our approach underscores the significance of fine-grained differences for robust OSV. We propose to match local structures between two signature images, significantly boosting verification accuracy. Furthermore, we observe that without specific architectural modifications, transformer-based backbones might naturally obscure local details, adversely impacting OSV performance. To address this, we introduce a Detail Semantics Integrator, leveraging feature disentanglement and re-entanglement. This integrator is specifically designed to enhance intricate details while simultaneously expanding discriminative semantics, thereby augmenting the efficacy of local structural matching. We evaluate our method against leading benchmarks in offline signature verification. Our model consistently outperforms recent methods, achieving state-of-the-art results with clear margins. The emphasis on local structure matching not only improves performance but also enhances the model's interpretability, supporting our findings. Additionally, our model demonstrates remarkable generalization capabilities in cross-dataset testing scenarios. The combination of generalizability and interpretability significantly bolsters the potential of DetailSemNet for real-world applications.", "link": "http://arxiv.org/abs/2511.16364v1", "date": "2025-11-20", "relevancy": 2.4597, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DetailSemNet%3A%20Elevating%20Signature%20Verification%20through%20Detail-Semantic%20Integration&body=Title%3A%20DetailSemNet%3A%20Elevating%20Signature%20Verification%20through%20Detail-Semantic%20Integration%0AAuthor%3A%20Meng-Cheng%20Shih%20and%20Tsai-Ling%20Huang%20and%20Yu-Heng%20Shih%20and%20Hong-Han%20Shuai%20and%20Hsuan-Tung%20Liu%20and%20Yi-Ren%20Yeh%20and%20Ching-Chun%20Huang%0AAbstract%3A%20Offline%20signature%20verification%20%28OSV%29%20is%20a%20frequently%20utilized%20technology%20in%20forensics.%20This%20paper%20proposes%20a%20new%20model%2C%20DetailSemNet%2C%20for%20OSV.%20Unlike%20previous%20methods%20that%20rely%20on%20holistic%20features%20for%20pair%20comparisons%2C%20our%20approach%20underscores%20the%20significance%20of%20fine-grained%20differences%20for%20robust%20OSV.%20We%20propose%20to%20match%20local%20structures%20between%20two%20signature%20images%2C%20significantly%20boosting%20verification%20accuracy.%20Furthermore%2C%20we%20observe%20that%20without%20specific%20architectural%20modifications%2C%20transformer-based%20backbones%20might%20naturally%20obscure%20local%20details%2C%20adversely%20impacting%20OSV%20performance.%20To%20address%20this%2C%20we%20introduce%20a%20Detail%20Semantics%20Integrator%2C%20leveraging%20feature%20disentanglement%20and%20re-entanglement.%20This%20integrator%20is%20specifically%20designed%20to%20enhance%20intricate%20details%20while%20simultaneously%20expanding%20discriminative%20semantics%2C%20thereby%20augmenting%20the%20efficacy%20of%20local%20structural%20matching.%20We%20evaluate%20our%20method%20against%20leading%20benchmarks%20in%20offline%20signature%20verification.%20Our%20model%20consistently%20outperforms%20recent%20methods%2C%20achieving%20state-of-the-art%20results%20with%20clear%20margins.%20The%20emphasis%20on%20local%20structure%20matching%20not%20only%20improves%20performance%20but%20also%20enhances%20the%20model%27s%20interpretability%2C%20supporting%20our%20findings.%20Additionally%2C%20our%20model%20demonstrates%20remarkable%20generalization%20capabilities%20in%20cross-dataset%20testing%20scenarios.%20The%20combination%20of%20generalizability%20and%20interpretability%20significantly%20bolsters%20the%20potential%20of%20DetailSemNet%20for%20real-world%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetailSemNet%253A%2520Elevating%2520Signature%2520Verification%2520through%2520Detail-Semantic%2520Integration%26entry.906535625%3DMeng-Cheng%2520Shih%2520and%2520Tsai-Ling%2520Huang%2520and%2520Yu-Heng%2520Shih%2520and%2520Hong-Han%2520Shuai%2520and%2520Hsuan-Tung%2520Liu%2520and%2520Yi-Ren%2520Yeh%2520and%2520Ching-Chun%2520Huang%26entry.1292438233%3DOffline%2520signature%2520verification%2520%2528OSV%2529%2520is%2520a%2520frequently%2520utilized%2520technology%2520in%2520forensics.%2520This%2520paper%2520proposes%2520a%2520new%2520model%252C%2520DetailSemNet%252C%2520for%2520OSV.%2520Unlike%2520previous%2520methods%2520that%2520rely%2520on%2520holistic%2520features%2520for%2520pair%2520comparisons%252C%2520our%2520approach%2520underscores%2520the%2520significance%2520of%2520fine-grained%2520differences%2520for%2520robust%2520OSV.%2520We%2520propose%2520to%2520match%2520local%2520structures%2520between%2520two%2520signature%2520images%252C%2520significantly%2520boosting%2520verification%2520accuracy.%2520Furthermore%252C%2520we%2520observe%2520that%2520without%2520specific%2520architectural%2520modifications%252C%2520transformer-based%2520backbones%2520might%2520naturally%2520obscure%2520local%2520details%252C%2520adversely%2520impacting%2520OSV%2520performance.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520Detail%2520Semantics%2520Integrator%252C%2520leveraging%2520feature%2520disentanglement%2520and%2520re-entanglement.%2520This%2520integrator%2520is%2520specifically%2520designed%2520to%2520enhance%2520intricate%2520details%2520while%2520simultaneously%2520expanding%2520discriminative%2520semantics%252C%2520thereby%2520augmenting%2520the%2520efficacy%2520of%2520local%2520structural%2520matching.%2520We%2520evaluate%2520our%2520method%2520against%2520leading%2520benchmarks%2520in%2520offline%2520signature%2520verification.%2520Our%2520model%2520consistently%2520outperforms%2520recent%2520methods%252C%2520achieving%2520state-of-the-art%2520results%2520with%2520clear%2520margins.%2520The%2520emphasis%2520on%2520local%2520structure%2520matching%2520not%2520only%2520improves%2520performance%2520but%2520also%2520enhances%2520the%2520model%2527s%2520interpretability%252C%2520supporting%2520our%2520findings.%2520Additionally%252C%2520our%2520model%2520demonstrates%2520remarkable%2520generalization%2520capabilities%2520in%2520cross-dataset%2520testing%2520scenarios.%2520The%2520combination%2520of%2520generalizability%2520and%2520interpretability%2520significantly%2520bolsters%2520the%2520potential%2520of%2520DetailSemNet%2520for%2520real-world%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DetailSemNet%3A%20Elevating%20Signature%20Verification%20through%20Detail-Semantic%20Integration&entry.906535625=Meng-Cheng%20Shih%20and%20Tsai-Ling%20Huang%20and%20Yu-Heng%20Shih%20and%20Hong-Han%20Shuai%20and%20Hsuan-Tung%20Liu%20and%20Yi-Ren%20Yeh%20and%20Ching-Chun%20Huang&entry.1292438233=Offline%20signature%20verification%20%28OSV%29%20is%20a%20frequently%20utilized%20technology%20in%20forensics.%20This%20paper%20proposes%20a%20new%20model%2C%20DetailSemNet%2C%20for%20OSV.%20Unlike%20previous%20methods%20that%20rely%20on%20holistic%20features%20for%20pair%20comparisons%2C%20our%20approach%20underscores%20the%20significance%20of%20fine-grained%20differences%20for%20robust%20OSV.%20We%20propose%20to%20match%20local%20structures%20between%20two%20signature%20images%2C%20significantly%20boosting%20verification%20accuracy.%20Furthermore%2C%20we%20observe%20that%20without%20specific%20architectural%20modifications%2C%20transformer-based%20backbones%20might%20naturally%20obscure%20local%20details%2C%20adversely%20impacting%20OSV%20performance.%20To%20address%20this%2C%20we%20introduce%20a%20Detail%20Semantics%20Integrator%2C%20leveraging%20feature%20disentanglement%20and%20re-entanglement.%20This%20integrator%20is%20specifically%20designed%20to%20enhance%20intricate%20details%20while%20simultaneously%20expanding%20discriminative%20semantics%2C%20thereby%20augmenting%20the%20efficacy%20of%20local%20structural%20matching.%20We%20evaluate%20our%20method%20against%20leading%20benchmarks%20in%20offline%20signature%20verification.%20Our%20model%20consistently%20outperforms%20recent%20methods%2C%20achieving%20state-of-the-art%20results%20with%20clear%20margins.%20The%20emphasis%20on%20local%20structure%20matching%20not%20only%20improves%20performance%20but%20also%20enhances%20the%20model%27s%20interpretability%2C%20supporting%20our%20findings.%20Additionally%2C%20our%20model%20demonstrates%20remarkable%20generalization%20capabilities%20in%20cross-dataset%20testing%20scenarios.%20The%20combination%20of%20generalizability%20and%20interpretability%20significantly%20bolsters%20the%20potential%20of%20DetailSemNet%20for%20real-world%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.16364v1&entry.124074799=Read"},
{"title": "Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks", "author": "Emre Anakok and Pierre Barbillon and Colin Fontaine and Elisa Thebault", "abstract": "Citizen science monitoring programs can generate large amounts of valuable data, but are often affected by sampling bias. We focus on a citizen science initiative that records plant-pollinator interactions, with the goal of learning embeddings that summarize the observed interactions while accounting for such bias. In our approach, plant and pollinator species are embedded based on their probability of interaction. These embeddings are derived using an adaptation of variational graph autoencoders for bipartite graphs. To mitigate the influence of sampling bias, we incorporate the Hilbert-Schmidt Independence Criterion (HSIC) to ensure independence from continuous variables related to the sampling process. This allows us to integrate a fairness perspective, commonly explored in the social sciences, into the analysis of ecological data. We validate our method through a simulation study replicating key aspects of the sampling process and demonstrate its applicability and effectiveness using the Spipoll dataset.", "link": "http://arxiv.org/abs/2403.02011v3", "date": "2025-11-20", "relevancy": 2.4584, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5127}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4945}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bipartite%20Graph%20Variational%20Auto-Encoder%20with%20Fair%20Latent%20Representation%20to%20Account%20for%20Sampling%20Bias%20in%20Ecological%20Networks&body=Title%3A%20Bipartite%20Graph%20Variational%20Auto-Encoder%20with%20Fair%20Latent%20Representation%20to%20Account%20for%20Sampling%20Bias%20in%20Ecological%20Networks%0AAuthor%3A%20Emre%20Anakok%20and%20Pierre%20Barbillon%20and%20Colin%20Fontaine%20and%20Elisa%20Thebault%0AAbstract%3A%20Citizen%20science%20monitoring%20programs%20can%20generate%20large%20amounts%20of%20valuable%20data%2C%20but%20are%20often%20affected%20by%20sampling%20bias.%20We%20focus%20on%20a%20citizen%20science%20initiative%20that%20records%20plant-pollinator%20interactions%2C%20with%20the%20goal%20of%20learning%20embeddings%20that%20summarize%20the%20observed%20interactions%20while%20accounting%20for%20such%20bias.%20In%20our%20approach%2C%20plant%20and%20pollinator%20species%20are%20embedded%20based%20on%20their%20probability%20of%20interaction.%20These%20embeddings%20are%20derived%20using%20an%20adaptation%20of%20variational%20graph%20autoencoders%20for%20bipartite%20graphs.%20To%20mitigate%20the%20influence%20of%20sampling%20bias%2C%20we%20incorporate%20the%20Hilbert-Schmidt%20Independence%20Criterion%20%28HSIC%29%20to%20ensure%20independence%20from%20continuous%20variables%20related%20to%20the%20sampling%20process.%20This%20allows%20us%20to%20integrate%20a%20fairness%20perspective%2C%20commonly%20explored%20in%20the%20social%20sciences%2C%20into%20the%20analysis%20of%20ecological%20data.%20We%20validate%20our%20method%20through%20a%20simulation%20study%20replicating%20key%20aspects%20of%20the%20sampling%20process%20and%20demonstrate%20its%20applicability%20and%20effectiveness%20using%20the%20Spipoll%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2403.02011v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBipartite%2520Graph%2520Variational%2520Auto-Encoder%2520with%2520Fair%2520Latent%2520Representation%2520to%2520Account%2520for%2520Sampling%2520Bias%2520in%2520Ecological%2520Networks%26entry.906535625%3DEmre%2520Anakok%2520and%2520Pierre%2520Barbillon%2520and%2520Colin%2520Fontaine%2520and%2520Elisa%2520Thebault%26entry.1292438233%3DCitizen%2520science%2520monitoring%2520programs%2520can%2520generate%2520large%2520amounts%2520of%2520valuable%2520data%252C%2520but%2520are%2520often%2520affected%2520by%2520sampling%2520bias.%2520We%2520focus%2520on%2520a%2520citizen%2520science%2520initiative%2520that%2520records%2520plant-pollinator%2520interactions%252C%2520with%2520the%2520goal%2520of%2520learning%2520embeddings%2520that%2520summarize%2520the%2520observed%2520interactions%2520while%2520accounting%2520for%2520such%2520bias.%2520In%2520our%2520approach%252C%2520plant%2520and%2520pollinator%2520species%2520are%2520embedded%2520based%2520on%2520their%2520probability%2520of%2520interaction.%2520These%2520embeddings%2520are%2520derived%2520using%2520an%2520adaptation%2520of%2520variational%2520graph%2520autoencoders%2520for%2520bipartite%2520graphs.%2520To%2520mitigate%2520the%2520influence%2520of%2520sampling%2520bias%252C%2520we%2520incorporate%2520the%2520Hilbert-Schmidt%2520Independence%2520Criterion%2520%2528HSIC%2529%2520to%2520ensure%2520independence%2520from%2520continuous%2520variables%2520related%2520to%2520the%2520sampling%2520process.%2520This%2520allows%2520us%2520to%2520integrate%2520a%2520fairness%2520perspective%252C%2520commonly%2520explored%2520in%2520the%2520social%2520sciences%252C%2520into%2520the%2520analysis%2520of%2520ecological%2520data.%2520We%2520validate%2520our%2520method%2520through%2520a%2520simulation%2520study%2520replicating%2520key%2520aspects%2520of%2520the%2520sampling%2520process%2520and%2520demonstrate%2520its%2520applicability%2520and%2520effectiveness%2520using%2520the%2520Spipoll%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02011v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bipartite%20Graph%20Variational%20Auto-Encoder%20with%20Fair%20Latent%20Representation%20to%20Account%20for%20Sampling%20Bias%20in%20Ecological%20Networks&entry.906535625=Emre%20Anakok%20and%20Pierre%20Barbillon%20and%20Colin%20Fontaine%20and%20Elisa%20Thebault&entry.1292438233=Citizen%20science%20monitoring%20programs%20can%20generate%20large%20amounts%20of%20valuable%20data%2C%20but%20are%20often%20affected%20by%20sampling%20bias.%20We%20focus%20on%20a%20citizen%20science%20initiative%20that%20records%20plant-pollinator%20interactions%2C%20with%20the%20goal%20of%20learning%20embeddings%20that%20summarize%20the%20observed%20interactions%20while%20accounting%20for%20such%20bias.%20In%20our%20approach%2C%20plant%20and%20pollinator%20species%20are%20embedded%20based%20on%20their%20probability%20of%20interaction.%20These%20embeddings%20are%20derived%20using%20an%20adaptation%20of%20variational%20graph%20autoencoders%20for%20bipartite%20graphs.%20To%20mitigate%20the%20influence%20of%20sampling%20bias%2C%20we%20incorporate%20the%20Hilbert-Schmidt%20Independence%20Criterion%20%28HSIC%29%20to%20ensure%20independence%20from%20continuous%20variables%20related%20to%20the%20sampling%20process.%20This%20allows%20us%20to%20integrate%20a%20fairness%20perspective%2C%20commonly%20explored%20in%20the%20social%20sciences%2C%20into%20the%20analysis%20of%20ecological%20data.%20We%20validate%20our%20method%20through%20a%20simulation%20study%20replicating%20key%20aspects%20of%20the%20sampling%20process%20and%20demonstrate%20its%20applicability%20and%20effectiveness%20using%20the%20Spipoll%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2403.02011v3&entry.124074799=Read"},
{"title": "Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation", "author": "Zongcai Tan and Lan Wei and Dandan Zhang", "abstract": "Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.", "link": "http://arxiv.org/abs/2511.16494v1", "date": "2025-11-20", "relevancy": 2.4563, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6291}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6068}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Machine%20Learning%20for%20Efficient%20Sim-to-Real%20Data%20Augmentation%20in%20Micro-Object%20Pose%20Estimation&body=Title%3A%20Physics-Informed%20Machine%20Learning%20for%20Efficient%20Sim-to-Real%20Data%20Augmentation%20in%20Micro-Object%20Pose%20Estimation%0AAuthor%3A%20Zongcai%20Tan%20and%20Lan%20Wei%20and%20Dandan%20Zhang%0AAbstract%3A%20Precise%20pose%20estimation%20of%20optical%20microrobots%20is%20essential%20for%20enabling%20high-precision%20object%20tracking%20and%20autonomous%20biological%20studies.%20However%2C%20current%20methods%20rely%20heavily%20on%20large%2C%20high-quality%20microscope%20image%20datasets%2C%20which%20are%20difficult%20and%20costly%20to%20acquire%20due%20to%20the%20complexity%20of%20microrobot%20fabrication%20and%20the%20labour-intensive%20labelling.%20Digital%20twin%20systems%20offer%20a%20promising%20path%20for%20sim-to-real%20data%20augmentation%2C%20yet%20existing%20techniques%20struggle%20to%20replicate%20complex%20optical%20microscopy%20phenomena%2C%20such%20as%20diffraction%20artifacts%20and%20depth-dependent%20imaging.This%20work%20proposes%20a%20novel%20physics-informed%20deep%20generative%20learning%20framework%20that%2C%20for%20the%20first%20time%2C%20integrates%20wave%20optics-based%20physical%20rendering%20and%20depth%20alignment%20into%20a%20generative%20adversarial%20network%20%28GAN%29%2C%20to%20synthesise%20high-fidelity%20microscope%20images%20for%20microrobot%20pose%20estimation%20efficiently.%20Our%20method%20improves%20the%20structural%20similarity%20index%20%28SSIM%29%20by%2035.6%25%20compared%20to%20purely%20AI-driven%20methods%2C%20while%20maintaining%20real-time%20rendering%20speeds%20%280.022%20s/frame%29.The%20pose%20estimator%20%28CNN%20backbone%29%20trained%20on%20our%20synthetic%20data%20achieves%2093.9%25/91.9%25%20%28pitch/roll%29%20accuracy%2C%20just%205.0%25/5.4%25%20%28pitch/roll%29%20below%20that%20of%20an%20estimator%20trained%20exclusively%20on%20real%20data.%20Furthermore%2C%20our%20framework%20generalises%20to%20unseen%20poses%2C%20enabling%20data%20augmentation%20and%20robust%20pose%20estimation%20for%20novel%20microrobot%20configurations%20without%20additional%20training%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520Machine%2520Learning%2520for%2520Efficient%2520Sim-to-Real%2520Data%2520Augmentation%2520in%2520Micro-Object%2520Pose%2520Estimation%26entry.906535625%3DZongcai%2520Tan%2520and%2520Lan%2520Wei%2520and%2520Dandan%2520Zhang%26entry.1292438233%3DPrecise%2520pose%2520estimation%2520of%2520optical%2520microrobots%2520is%2520essential%2520for%2520enabling%2520high-precision%2520object%2520tracking%2520and%2520autonomous%2520biological%2520studies.%2520However%252C%2520current%2520methods%2520rely%2520heavily%2520on%2520large%252C%2520high-quality%2520microscope%2520image%2520datasets%252C%2520which%2520are%2520difficult%2520and%2520costly%2520to%2520acquire%2520due%2520to%2520the%2520complexity%2520of%2520microrobot%2520fabrication%2520and%2520the%2520labour-intensive%2520labelling.%2520Digital%2520twin%2520systems%2520offer%2520a%2520promising%2520path%2520for%2520sim-to-real%2520data%2520augmentation%252C%2520yet%2520existing%2520techniques%2520struggle%2520to%2520replicate%2520complex%2520optical%2520microscopy%2520phenomena%252C%2520such%2520as%2520diffraction%2520artifacts%2520and%2520depth-dependent%2520imaging.This%2520work%2520proposes%2520a%2520novel%2520physics-informed%2520deep%2520generative%2520learning%2520framework%2520that%252C%2520for%2520the%2520first%2520time%252C%2520integrates%2520wave%2520optics-based%2520physical%2520rendering%2520and%2520depth%2520alignment%2520into%2520a%2520generative%2520adversarial%2520network%2520%2528GAN%2529%252C%2520to%2520synthesise%2520high-fidelity%2520microscope%2520images%2520for%2520microrobot%2520pose%2520estimation%2520efficiently.%2520Our%2520method%2520improves%2520the%2520structural%2520similarity%2520index%2520%2528SSIM%2529%2520by%252035.6%2525%2520compared%2520to%2520purely%2520AI-driven%2520methods%252C%2520while%2520maintaining%2520real-time%2520rendering%2520speeds%2520%25280.022%2520s/frame%2529.The%2520pose%2520estimator%2520%2528CNN%2520backbone%2529%2520trained%2520on%2520our%2520synthetic%2520data%2520achieves%252093.9%2525/91.9%2525%2520%2528pitch/roll%2529%2520accuracy%252C%2520just%25205.0%2525/5.4%2525%2520%2528pitch/roll%2529%2520below%2520that%2520of%2520an%2520estimator%2520trained%2520exclusively%2520on%2520real%2520data.%2520Furthermore%252C%2520our%2520framework%2520generalises%2520to%2520unseen%2520poses%252C%2520enabling%2520data%2520augmentation%2520and%2520robust%2520pose%2520estimation%2520for%2520novel%2520microrobot%2520configurations%2520without%2520additional%2520training%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Machine%20Learning%20for%20Efficient%20Sim-to-Real%20Data%20Augmentation%20in%20Micro-Object%20Pose%20Estimation&entry.906535625=Zongcai%20Tan%20and%20Lan%20Wei%20and%20Dandan%20Zhang&entry.1292438233=Precise%20pose%20estimation%20of%20optical%20microrobots%20is%20essential%20for%20enabling%20high-precision%20object%20tracking%20and%20autonomous%20biological%20studies.%20However%2C%20current%20methods%20rely%20heavily%20on%20large%2C%20high-quality%20microscope%20image%20datasets%2C%20which%20are%20difficult%20and%20costly%20to%20acquire%20due%20to%20the%20complexity%20of%20microrobot%20fabrication%20and%20the%20labour-intensive%20labelling.%20Digital%20twin%20systems%20offer%20a%20promising%20path%20for%20sim-to-real%20data%20augmentation%2C%20yet%20existing%20techniques%20struggle%20to%20replicate%20complex%20optical%20microscopy%20phenomena%2C%20such%20as%20diffraction%20artifacts%20and%20depth-dependent%20imaging.This%20work%20proposes%20a%20novel%20physics-informed%20deep%20generative%20learning%20framework%20that%2C%20for%20the%20first%20time%2C%20integrates%20wave%20optics-based%20physical%20rendering%20and%20depth%20alignment%20into%20a%20generative%20adversarial%20network%20%28GAN%29%2C%20to%20synthesise%20high-fidelity%20microscope%20images%20for%20microrobot%20pose%20estimation%20efficiently.%20Our%20method%20improves%20the%20structural%20similarity%20index%20%28SSIM%29%20by%2035.6%25%20compared%20to%20purely%20AI-driven%20methods%2C%20while%20maintaining%20real-time%20rendering%20speeds%20%280.022%20s/frame%29.The%20pose%20estimator%20%28CNN%20backbone%29%20trained%20on%20our%20synthetic%20data%20achieves%2093.9%25/91.9%25%20%28pitch/roll%29%20accuracy%2C%20just%205.0%25/5.4%25%20%28pitch/roll%29%20below%20that%20of%20an%20estimator%20trained%20exclusively%20on%20real%20data.%20Furthermore%2C%20our%20framework%20generalises%20to%20unseen%20poses%2C%20enabling%20data%20augmentation%20and%20robust%20pose%20estimation%20for%20novel%20microrobot%20configurations%20without%20additional%20training%20data.&entry.1838667208=http%3A//arxiv.org/abs/2511.16494v1&entry.124074799=Read"},
{"title": "TC-Light: Temporally Coherent Generative Rendering for Realistic World Transfer", "author": "Yang Liu and Chuanchen Luo and Zimo Tang and Yingyan Li and Yuran Yang and Yuanyong Ning and Lue Fan and Junran Peng and Zhaoxiang Zhang", "abstract": "Illumination and texture editing are critical dimensions for world-to-world transfer, which is valuable for applications including sim2real and real2real visual data scaling up for embodied AI. Existing techniques generatively re-render the input video to realize the transfer, such as video relighting models and conditioned world generation models. Nevertheless, these models are predominantly limited to the domain of training data (e.g., portrait) or fall into the bottleneck of temporal consistency and computation efficiency, especially when the input video involves complex dynamics and long durations. In this paper, we propose TC-Light, a novel generative renderer to overcome these problems. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible re-rendering results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/.", "link": "http://arxiv.org/abs/2506.18904v3", "date": "2025-11-20", "relevancy": 2.4444, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6183}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6063}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TC-Light%3A%20Temporally%20Coherent%20Generative%20Rendering%20for%20Realistic%20World%20Transfer&body=Title%3A%20TC-Light%3A%20Temporally%20Coherent%20Generative%20Rendering%20for%20Realistic%20World%20Transfer%0AAuthor%3A%20Yang%20Liu%20and%20Chuanchen%20Luo%20and%20Zimo%20Tang%20and%20Yingyan%20Li%20and%20Yuran%20Yang%20and%20Yuanyong%20Ning%20and%20Lue%20Fan%20and%20Junran%20Peng%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20Illumination%20and%20texture%20editing%20are%20critical%20dimensions%20for%20world-to-world%20transfer%2C%20which%20is%20valuable%20for%20applications%20including%20sim2real%20and%20real2real%20visual%20data%20scaling%20up%20for%20embodied%20AI.%20Existing%20techniques%20generatively%20re-render%20the%20input%20video%20to%20realize%20the%20transfer%2C%20such%20as%20video%20relighting%20models%20and%20conditioned%20world%20generation%20models.%20Nevertheless%2C%20these%20models%20are%20predominantly%20limited%20to%20the%20domain%20of%20training%20data%20%28e.g.%2C%20portrait%29%20or%20fall%20into%20the%20bottleneck%20of%20temporal%20consistency%20and%20computation%20efficiency%2C%20especially%20when%20the%20input%20video%20involves%20complex%20dynamics%20and%20long%20durations.%20In%20this%20paper%2C%20we%20propose%20TC-Light%2C%20a%20novel%20generative%20renderer%20to%20overcome%20these%20problems.%20Starting%20from%20the%20video%20preliminarily%20relighted%20by%20an%20inflated%20video%20relighting%20model%2C%20it%20optimizes%20appearance%20embedding%20in%20the%20first%20stage%20to%20align%20global%20illumination.%20Then%20it%20optimizes%20the%20proposed%20canonical%20video%20representation%2C%20i.e.%2C%20Unique%20Video%20Tensor%20%28UVT%29%2C%20to%20align%20fine-grained%20texture%20and%20lighting%20in%20the%20second%20stage.%20To%20comprehensively%20evaluate%20performance%2C%20we%20also%20establish%20a%20long%20and%20highly%20dynamic%20video%20benchmark.%20Extensive%20experiments%20show%20that%20our%20method%20enables%20physically%20plausible%20re-rendering%20results%20with%20superior%20temporal%20coherence%20and%20low%20computation%20cost.%20The%20code%20and%20video%20demos%20are%20available%20at%20https%3A//dekuliutesla.github.io/tclight/.%0ALink%3A%20http%3A//arxiv.org/abs/2506.18904v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTC-Light%253A%2520Temporally%2520Coherent%2520Generative%2520Rendering%2520for%2520Realistic%2520World%2520Transfer%26entry.906535625%3DYang%2520Liu%2520and%2520Chuanchen%2520Luo%2520and%2520Zimo%2520Tang%2520and%2520Yingyan%2520Li%2520and%2520Yuran%2520Yang%2520and%2520Yuanyong%2520Ning%2520and%2520Lue%2520Fan%2520and%2520Junran%2520Peng%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3DIllumination%2520and%2520texture%2520editing%2520are%2520critical%2520dimensions%2520for%2520world-to-world%2520transfer%252C%2520which%2520is%2520valuable%2520for%2520applications%2520including%2520sim2real%2520and%2520real2real%2520visual%2520data%2520scaling%2520up%2520for%2520embodied%2520AI.%2520Existing%2520techniques%2520generatively%2520re-render%2520the%2520input%2520video%2520to%2520realize%2520the%2520transfer%252C%2520such%2520as%2520video%2520relighting%2520models%2520and%2520conditioned%2520world%2520generation%2520models.%2520Nevertheless%252C%2520these%2520models%2520are%2520predominantly%2520limited%2520to%2520the%2520domain%2520of%2520training%2520data%2520%2528e.g.%252C%2520portrait%2529%2520or%2520fall%2520into%2520the%2520bottleneck%2520of%2520temporal%2520consistency%2520and%2520computation%2520efficiency%252C%2520especially%2520when%2520the%2520input%2520video%2520involves%2520complex%2520dynamics%2520and%2520long%2520durations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TC-Light%252C%2520a%2520novel%2520generative%2520renderer%2520to%2520overcome%2520these%2520problems.%2520Starting%2520from%2520the%2520video%2520preliminarily%2520relighted%2520by%2520an%2520inflated%2520video%2520relighting%2520model%252C%2520it%2520optimizes%2520appearance%2520embedding%2520in%2520the%2520first%2520stage%2520to%2520align%2520global%2520illumination.%2520Then%2520it%2520optimizes%2520the%2520proposed%2520canonical%2520video%2520representation%252C%2520i.e.%252C%2520Unique%2520Video%2520Tensor%2520%2528UVT%2529%252C%2520to%2520align%2520fine-grained%2520texture%2520and%2520lighting%2520in%2520the%2520second%2520stage.%2520To%2520comprehensively%2520evaluate%2520performance%252C%2520we%2520also%2520establish%2520a%2520long%2520and%2520highly%2520dynamic%2520video%2520benchmark.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520enables%2520physically%2520plausible%2520re-rendering%2520results%2520with%2520superior%2520temporal%2520coherence%2520and%2520low%2520computation%2520cost.%2520The%2520code%2520and%2520video%2520demos%2520are%2520available%2520at%2520https%253A//dekuliutesla.github.io/tclight/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18904v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TC-Light%3A%20Temporally%20Coherent%20Generative%20Rendering%20for%20Realistic%20World%20Transfer&entry.906535625=Yang%20Liu%20and%20Chuanchen%20Luo%20and%20Zimo%20Tang%20and%20Yingyan%20Li%20and%20Yuran%20Yang%20and%20Yuanyong%20Ning%20and%20Lue%20Fan%20and%20Junran%20Peng%20and%20Zhaoxiang%20Zhang&entry.1292438233=Illumination%20and%20texture%20editing%20are%20critical%20dimensions%20for%20world-to-world%20transfer%2C%20which%20is%20valuable%20for%20applications%20including%20sim2real%20and%20real2real%20visual%20data%20scaling%20up%20for%20embodied%20AI.%20Existing%20techniques%20generatively%20re-render%20the%20input%20video%20to%20realize%20the%20transfer%2C%20such%20as%20video%20relighting%20models%20and%20conditioned%20world%20generation%20models.%20Nevertheless%2C%20these%20models%20are%20predominantly%20limited%20to%20the%20domain%20of%20training%20data%20%28e.g.%2C%20portrait%29%20or%20fall%20into%20the%20bottleneck%20of%20temporal%20consistency%20and%20computation%20efficiency%2C%20especially%20when%20the%20input%20video%20involves%20complex%20dynamics%20and%20long%20durations.%20In%20this%20paper%2C%20we%20propose%20TC-Light%2C%20a%20novel%20generative%20renderer%20to%20overcome%20these%20problems.%20Starting%20from%20the%20video%20preliminarily%20relighted%20by%20an%20inflated%20video%20relighting%20model%2C%20it%20optimizes%20appearance%20embedding%20in%20the%20first%20stage%20to%20align%20global%20illumination.%20Then%20it%20optimizes%20the%20proposed%20canonical%20video%20representation%2C%20i.e.%2C%20Unique%20Video%20Tensor%20%28UVT%29%2C%20to%20align%20fine-grained%20texture%20and%20lighting%20in%20the%20second%20stage.%20To%20comprehensively%20evaluate%20performance%2C%20we%20also%20establish%20a%20long%20and%20highly%20dynamic%20video%20benchmark.%20Extensive%20experiments%20show%20that%20our%20method%20enables%20physically%20plausible%20re-rendering%20results%20with%20superior%20temporal%20coherence%20and%20low%20computation%20cost.%20The%20code%20and%20video%20demos%20are%20available%20at%20https%3A//dekuliutesla.github.io/tclight/.&entry.1838667208=http%3A//arxiv.org/abs/2506.18904v3&entry.124074799=Read"},
{"title": "DiffuSyn Bench: Evaluating Vision-Language Models on Real-World Complexities with Diffusion-Generated Synthetic Benchmarks", "author": "Haokun Zhou and Yipeng Hong", "abstract": "This study assesses the ability of Large Vision-Language Models (LVLMs) to differentiate between AI-generated and human-generated images. It introduces a new automated benchmark construction method for this evaluation. The experiment compared common LVLMs with human participants using a mixed dataset of AI and human-created images. Results showed that LVLMs could distinguish between the image types to some extent but exhibited a rightward bias, and perform significantly worse compared to humans. To build on these findings, we developed an automated benchmark construction process using AI. This process involved topic retrieval, narrative script generation, error embedding, and image generation, creating a diverse set of text-image pairs with intentional errors. We validated our method through constructing two caparable benchmarks. This study highlights the strengths and weaknesses of LVLMs in real-world understanding and advances benchmark construction techniques, providing a scalable and automatic approach for AI model evaluation.", "link": "http://arxiv.org/abs/2406.04470v3", "date": "2025-11-20", "relevancy": 2.4421, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6196}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffuSyn%20Bench%3A%20Evaluating%20Vision-Language%20Models%20on%20Real-World%20Complexities%20with%20Diffusion-Generated%20Synthetic%20Benchmarks&body=Title%3A%20DiffuSyn%20Bench%3A%20Evaluating%20Vision-Language%20Models%20on%20Real-World%20Complexities%20with%20Diffusion-Generated%20Synthetic%20Benchmarks%0AAuthor%3A%20Haokun%20Zhou%20and%20Yipeng%20Hong%0AAbstract%3A%20This%20study%20assesses%20the%20ability%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20differentiate%20between%20AI-generated%20and%20human-generated%20images.%20It%20introduces%20a%20new%20automated%20benchmark%20construction%20method%20for%20this%20evaluation.%20The%20experiment%20compared%20common%20LVLMs%20with%20human%20participants%20using%20a%20mixed%20dataset%20of%20AI%20and%20human-created%20images.%20Results%20showed%20that%20LVLMs%20could%20distinguish%20between%20the%20image%20types%20to%20some%20extent%20but%20exhibited%20a%20rightward%20bias%2C%20and%20perform%20significantly%20worse%20compared%20to%20humans.%20To%20build%20on%20these%20findings%2C%20we%20developed%20an%20automated%20benchmark%20construction%20process%20using%20AI.%20This%20process%20involved%20topic%20retrieval%2C%20narrative%20script%20generation%2C%20error%20embedding%2C%20and%20image%20generation%2C%20creating%20a%20diverse%20set%20of%20text-image%20pairs%20with%20intentional%20errors.%20We%20validated%20our%20method%20through%20constructing%20two%20caparable%20benchmarks.%20This%20study%20highlights%20the%20strengths%20and%20weaknesses%20of%20LVLMs%20in%20real-world%20understanding%20and%20advances%20benchmark%20construction%20techniques%2C%20providing%20a%20scalable%20and%20automatic%20approach%20for%20AI%20model%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2406.04470v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffuSyn%2520Bench%253A%2520Evaluating%2520Vision-Language%2520Models%2520on%2520Real-World%2520Complexities%2520with%2520Diffusion-Generated%2520Synthetic%2520Benchmarks%26entry.906535625%3DHaokun%2520Zhou%2520and%2520Yipeng%2520Hong%26entry.1292438233%3DThis%2520study%2520assesses%2520the%2520ability%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520differentiate%2520between%2520AI-generated%2520and%2520human-generated%2520images.%2520It%2520introduces%2520a%2520new%2520automated%2520benchmark%2520construction%2520method%2520for%2520this%2520evaluation.%2520The%2520experiment%2520compared%2520common%2520LVLMs%2520with%2520human%2520participants%2520using%2520a%2520mixed%2520dataset%2520of%2520AI%2520and%2520human-created%2520images.%2520Results%2520showed%2520that%2520LVLMs%2520could%2520distinguish%2520between%2520the%2520image%2520types%2520to%2520some%2520extent%2520but%2520exhibited%2520a%2520rightward%2520bias%252C%2520and%2520perform%2520significantly%2520worse%2520compared%2520to%2520humans.%2520To%2520build%2520on%2520these%2520findings%252C%2520we%2520developed%2520an%2520automated%2520benchmark%2520construction%2520process%2520using%2520AI.%2520This%2520process%2520involved%2520topic%2520retrieval%252C%2520narrative%2520script%2520generation%252C%2520error%2520embedding%252C%2520and%2520image%2520generation%252C%2520creating%2520a%2520diverse%2520set%2520of%2520text-image%2520pairs%2520with%2520intentional%2520errors.%2520We%2520validated%2520our%2520method%2520through%2520constructing%2520two%2520caparable%2520benchmarks.%2520This%2520study%2520highlights%2520the%2520strengths%2520and%2520weaknesses%2520of%2520LVLMs%2520in%2520real-world%2520understanding%2520and%2520advances%2520benchmark%2520construction%2520techniques%252C%2520providing%2520a%2520scalable%2520and%2520automatic%2520approach%2520for%2520AI%2520model%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04470v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffuSyn%20Bench%3A%20Evaluating%20Vision-Language%20Models%20on%20Real-World%20Complexities%20with%20Diffusion-Generated%20Synthetic%20Benchmarks&entry.906535625=Haokun%20Zhou%20and%20Yipeng%20Hong&entry.1292438233=This%20study%20assesses%20the%20ability%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20differentiate%20between%20AI-generated%20and%20human-generated%20images.%20It%20introduces%20a%20new%20automated%20benchmark%20construction%20method%20for%20this%20evaluation.%20The%20experiment%20compared%20common%20LVLMs%20with%20human%20participants%20using%20a%20mixed%20dataset%20of%20AI%20and%20human-created%20images.%20Results%20showed%20that%20LVLMs%20could%20distinguish%20between%20the%20image%20types%20to%20some%20extent%20but%20exhibited%20a%20rightward%20bias%2C%20and%20perform%20significantly%20worse%20compared%20to%20humans.%20To%20build%20on%20these%20findings%2C%20we%20developed%20an%20automated%20benchmark%20construction%20process%20using%20AI.%20This%20process%20involved%20topic%20retrieval%2C%20narrative%20script%20generation%2C%20error%20embedding%2C%20and%20image%20generation%2C%20creating%20a%20diverse%20set%20of%20text-image%20pairs%20with%20intentional%20errors.%20We%20validated%20our%20method%20through%20constructing%20two%20caparable%20benchmarks.%20This%20study%20highlights%20the%20strengths%20and%20weaknesses%20of%20LVLMs%20in%20real-world%20understanding%20and%20advances%20benchmark%20construction%20techniques%2C%20providing%20a%20scalable%20and%20automatic%20approach%20for%20AI%20model%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2406.04470v3&entry.124074799=Read"},
{"title": "Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors", "author": "Alan Yufei Dong and Jihao Andreas Lin and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "Scalable Gaussian process (GP) inference is essential for sequential decision-making tasks, yet improving GP scalability remains a challenging problem with many open avenues of research. This paper focuses on iterative GPs, where iterative linear solvers, such as conjugate gradients, stochastic gradient descent or alternative projections, are used to approximate the GP posterior. We propose a new method which improves solver convergence of a large linear system by leveraging the known solution to a smaller system contained within. This is significant for tasks with incremental data additions, and we show that our technique achieves speed-ups when solving to tolerance, as well as improved Bayesian optimisation performance under a fixed compute budget.", "link": "http://arxiv.org/abs/2511.16340v1", "date": "2025-11-20", "relevancy": 2.4279, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5091}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4961}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Iterative%20Gaussian%20Processes%20via%20Warm%20Starting%20Sequential%20Posteriors&body=Title%3A%20Improving%20Iterative%20Gaussian%20Processes%20via%20Warm%20Starting%20Sequential%20Posteriors%0AAuthor%3A%20Alan%20Yufei%20Dong%20and%20Jihao%20Andreas%20Lin%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20Scalable%20Gaussian%20process%20%28GP%29%20inference%20is%20essential%20for%20sequential%20decision-making%20tasks%2C%20yet%20improving%20GP%20scalability%20remains%20a%20challenging%20problem%20with%20many%20open%20avenues%20of%20research.%20This%20paper%20focuses%20on%20iterative%20GPs%2C%20where%20iterative%20linear%20solvers%2C%20such%20as%20conjugate%20gradients%2C%20stochastic%20gradient%20descent%20or%20alternative%20projections%2C%20are%20used%20to%20approximate%20the%20GP%20posterior.%20We%20propose%20a%20new%20method%20which%20improves%20solver%20convergence%20of%20a%20large%20linear%20system%20by%20leveraging%20the%20known%20solution%20to%20a%20smaller%20system%20contained%20within.%20This%20is%20significant%20for%20tasks%20with%20incremental%20data%20additions%2C%20and%20we%20show%20that%20our%20technique%20achieves%20speed-ups%20when%20solving%20to%20tolerance%2C%20as%20well%20as%20improved%20Bayesian%20optimisation%20performance%20under%20a%20fixed%20compute%20budget.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Iterative%2520Gaussian%2520Processes%2520via%2520Warm%2520Starting%2520Sequential%2520Posteriors%26entry.906535625%3DAlan%2520Yufei%2520Dong%2520and%2520Jihao%2520Andreas%2520Lin%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3DScalable%2520Gaussian%2520process%2520%2528GP%2529%2520inference%2520is%2520essential%2520for%2520sequential%2520decision-making%2520tasks%252C%2520yet%2520improving%2520GP%2520scalability%2520remains%2520a%2520challenging%2520problem%2520with%2520many%2520open%2520avenues%2520of%2520research.%2520This%2520paper%2520focuses%2520on%2520iterative%2520GPs%252C%2520where%2520iterative%2520linear%2520solvers%252C%2520such%2520as%2520conjugate%2520gradients%252C%2520stochastic%2520gradient%2520descent%2520or%2520alternative%2520projections%252C%2520are%2520used%2520to%2520approximate%2520the%2520GP%2520posterior.%2520We%2520propose%2520a%2520new%2520method%2520which%2520improves%2520solver%2520convergence%2520of%2520a%2520large%2520linear%2520system%2520by%2520leveraging%2520the%2520known%2520solution%2520to%2520a%2520smaller%2520system%2520contained%2520within.%2520This%2520is%2520significant%2520for%2520tasks%2520with%2520incremental%2520data%2520additions%252C%2520and%2520we%2520show%2520that%2520our%2520technique%2520achieves%2520speed-ups%2520when%2520solving%2520to%2520tolerance%252C%2520as%2520well%2520as%2520improved%2520Bayesian%2520optimisation%2520performance%2520under%2520a%2520fixed%2520compute%2520budget.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Iterative%20Gaussian%20Processes%20via%20Warm%20Starting%20Sequential%20Posteriors&entry.906535625=Alan%20Yufei%20Dong%20and%20Jihao%20Andreas%20Lin%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=Scalable%20Gaussian%20process%20%28GP%29%20inference%20is%20essential%20for%20sequential%20decision-making%20tasks%2C%20yet%20improving%20GP%20scalability%20remains%20a%20challenging%20problem%20with%20many%20open%20avenues%20of%20research.%20This%20paper%20focuses%20on%20iterative%20GPs%2C%20where%20iterative%20linear%20solvers%2C%20such%20as%20conjugate%20gradients%2C%20stochastic%20gradient%20descent%20or%20alternative%20projections%2C%20are%20used%20to%20approximate%20the%20GP%20posterior.%20We%20propose%20a%20new%20method%20which%20improves%20solver%20convergence%20of%20a%20large%20linear%20system%20by%20leveraging%20the%20known%20solution%20to%20a%20smaller%20system%20contained%20within.%20This%20is%20significant%20for%20tasks%20with%20incremental%20data%20additions%2C%20and%20we%20show%20that%20our%20technique%20achieves%20speed-ups%20when%20solving%20to%20tolerance%2C%20as%20well%20as%20improved%20Bayesian%20optimisation%20performance%20under%20a%20fixed%20compute%20budget.&entry.1838667208=http%3A//arxiv.org/abs/2511.16340v1&entry.124074799=Read"},
{"title": "YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras", "author": "Fan Yang and Sosuke Yamao and Ikuo Kusajima and Atsunori Moteki and Shoichi Masui and Shan Jiang", "abstract": "Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications.", "link": "http://arxiv.org/abs/2511.16521v1", "date": "2025-11-20", "relevancy": 2.4044, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6171}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOWO%3A%20You%20Only%20Walk%20Once%20to%20Jointly%20Map%20An%20Indoor%20Scene%20and%20Register%20Ceiling-mounted%20Cameras&body=Title%3A%20YOWO%3A%20You%20Only%20Walk%20Once%20to%20Jointly%20Map%20An%20Indoor%20Scene%20and%20Register%20Ceiling-mounted%20Cameras%0AAuthor%3A%20Fan%20Yang%20and%20Sosuke%20Yamao%20and%20Ikuo%20Kusajima%20and%20Atsunori%20Moteki%20and%20Shoichi%20Masui%20and%20Shan%20Jiang%0AAbstract%3A%20Using%20ceiling-mounted%20cameras%20%28CMCs%29%20for%20indoor%20visual%20capturing%20opens%20up%20a%20wide%20range%20of%20applications.%20However%2C%20registering%20CMCs%20to%20the%20target%20scene%20layout%20presents%20a%20challenging%20task.%20While%20manual%20registration%20with%20specialized%20tools%20is%20inefficient%20and%20costly%2C%20automatic%20registration%20with%20visual%20localization%20may%20yield%20poor%20results%20when%20visual%20ambiguity%20exists.%20To%20alleviate%20these%20issues%2C%20we%20propose%20a%20novel%20solution%20for%20jointly%20mapping%20an%20indoor%20scene%20and%20registering%20CMCs%20to%20the%20scene%20layout.%20Our%20approach%20involves%20equipping%20a%20mobile%20agent%20with%20a%20head-mounted%20RGB-D%20camera%20to%20traverse%20the%20entire%20scene%20once%20and%20synchronize%20CMCs%20to%20capture%20this%20mobile%20agent.%20The%20egocentric%20videos%20generate%20world-coordinate%20agent%20trajectories%20and%20the%20scene%20layout%2C%20while%20the%20videos%20of%20CMCs%20provide%20pseudo-scale%20agent%20trajectories%20and%20CMC%20relative%20poses.%20By%20correlating%20all%20the%20trajectories%20with%20their%20corresponding%20timestamps%2C%20the%20CMC%20relative%20poses%20can%20be%20aligned%20to%20the%20world-coordinate%20scene%20layout.%20Based%20on%20this%20initialization%2C%20a%20factor%20graph%20is%20customized%20to%20enable%20the%20joint%20optimization%20of%20ego-camera%20poses%2C%20scene%20layout%2C%20and%20CMC%20poses.%20We%20also%20develop%20a%20new%20dataset%2C%20setting%20the%20first%20benchmark%20for%20collaborative%20scene%20mapping%20and%20CMC%20registration%20%28https%3A//sites.google.com/view/yowo/home%29.%20Experimental%20results%20indicate%20that%20our%20method%20not%20only%20effectively%20accomplishes%20two%20tasks%20within%20a%20unified%20framework%2C%20but%20also%20jointly%20enhances%20their%20performance.%20We%20thus%20provide%20a%20reliable%20tool%20to%20facilitate%20downstream%20position-aware%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOWO%253A%2520You%2520Only%2520Walk%2520Once%2520to%2520Jointly%2520Map%2520An%2520Indoor%2520Scene%2520and%2520Register%2520Ceiling-mounted%2520Cameras%26entry.906535625%3DFan%2520Yang%2520and%2520Sosuke%2520Yamao%2520and%2520Ikuo%2520Kusajima%2520and%2520Atsunori%2520Moteki%2520and%2520Shoichi%2520Masui%2520and%2520Shan%2520Jiang%26entry.1292438233%3DUsing%2520ceiling-mounted%2520cameras%2520%2528CMCs%2529%2520for%2520indoor%2520visual%2520capturing%2520opens%2520up%2520a%2520wide%2520range%2520of%2520applications.%2520However%252C%2520registering%2520CMCs%2520to%2520the%2520target%2520scene%2520layout%2520presents%2520a%2520challenging%2520task.%2520While%2520manual%2520registration%2520with%2520specialized%2520tools%2520is%2520inefficient%2520and%2520costly%252C%2520automatic%2520registration%2520with%2520visual%2520localization%2520may%2520yield%2520poor%2520results%2520when%2520visual%2520ambiguity%2520exists.%2520To%2520alleviate%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520solution%2520for%2520jointly%2520mapping%2520an%2520indoor%2520scene%2520and%2520registering%2520CMCs%2520to%2520the%2520scene%2520layout.%2520Our%2520approach%2520involves%2520equipping%2520a%2520mobile%2520agent%2520with%2520a%2520head-mounted%2520RGB-D%2520camera%2520to%2520traverse%2520the%2520entire%2520scene%2520once%2520and%2520synchronize%2520CMCs%2520to%2520capture%2520this%2520mobile%2520agent.%2520The%2520egocentric%2520videos%2520generate%2520world-coordinate%2520agent%2520trajectories%2520and%2520the%2520scene%2520layout%252C%2520while%2520the%2520videos%2520of%2520CMCs%2520provide%2520pseudo-scale%2520agent%2520trajectories%2520and%2520CMC%2520relative%2520poses.%2520By%2520correlating%2520all%2520the%2520trajectories%2520with%2520their%2520corresponding%2520timestamps%252C%2520the%2520CMC%2520relative%2520poses%2520can%2520be%2520aligned%2520to%2520the%2520world-coordinate%2520scene%2520layout.%2520Based%2520on%2520this%2520initialization%252C%2520a%2520factor%2520graph%2520is%2520customized%2520to%2520enable%2520the%2520joint%2520optimization%2520of%2520ego-camera%2520poses%252C%2520scene%2520layout%252C%2520and%2520CMC%2520poses.%2520We%2520also%2520develop%2520a%2520new%2520dataset%252C%2520setting%2520the%2520first%2520benchmark%2520for%2520collaborative%2520scene%2520mapping%2520and%2520CMC%2520registration%2520%2528https%253A//sites.google.com/view/yowo/home%2529.%2520Experimental%2520results%2520indicate%2520that%2520our%2520method%2520not%2520only%2520effectively%2520accomplishes%2520two%2520tasks%2520within%2520a%2520unified%2520framework%252C%2520but%2520also%2520jointly%2520enhances%2520their%2520performance.%2520We%2520thus%2520provide%2520a%2520reliable%2520tool%2520to%2520facilitate%2520downstream%2520position-aware%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOWO%3A%20You%20Only%20Walk%20Once%20to%20Jointly%20Map%20An%20Indoor%20Scene%20and%20Register%20Ceiling-mounted%20Cameras&entry.906535625=Fan%20Yang%20and%20Sosuke%20Yamao%20and%20Ikuo%20Kusajima%20and%20Atsunori%20Moteki%20and%20Shoichi%20Masui%20and%20Shan%20Jiang&entry.1292438233=Using%20ceiling-mounted%20cameras%20%28CMCs%29%20for%20indoor%20visual%20capturing%20opens%20up%20a%20wide%20range%20of%20applications.%20However%2C%20registering%20CMCs%20to%20the%20target%20scene%20layout%20presents%20a%20challenging%20task.%20While%20manual%20registration%20with%20specialized%20tools%20is%20inefficient%20and%20costly%2C%20automatic%20registration%20with%20visual%20localization%20may%20yield%20poor%20results%20when%20visual%20ambiguity%20exists.%20To%20alleviate%20these%20issues%2C%20we%20propose%20a%20novel%20solution%20for%20jointly%20mapping%20an%20indoor%20scene%20and%20registering%20CMCs%20to%20the%20scene%20layout.%20Our%20approach%20involves%20equipping%20a%20mobile%20agent%20with%20a%20head-mounted%20RGB-D%20camera%20to%20traverse%20the%20entire%20scene%20once%20and%20synchronize%20CMCs%20to%20capture%20this%20mobile%20agent.%20The%20egocentric%20videos%20generate%20world-coordinate%20agent%20trajectories%20and%20the%20scene%20layout%2C%20while%20the%20videos%20of%20CMCs%20provide%20pseudo-scale%20agent%20trajectories%20and%20CMC%20relative%20poses.%20By%20correlating%20all%20the%20trajectories%20with%20their%20corresponding%20timestamps%2C%20the%20CMC%20relative%20poses%20can%20be%20aligned%20to%20the%20world-coordinate%20scene%20layout.%20Based%20on%20this%20initialization%2C%20a%20factor%20graph%20is%20customized%20to%20enable%20the%20joint%20optimization%20of%20ego-camera%20poses%2C%20scene%20layout%2C%20and%20CMC%20poses.%20We%20also%20develop%20a%20new%20dataset%2C%20setting%20the%20first%20benchmark%20for%20collaborative%20scene%20mapping%20and%20CMC%20registration%20%28https%3A//sites.google.com/view/yowo/home%29.%20Experimental%20results%20indicate%20that%20our%20method%20not%20only%20effectively%20accomplishes%20two%20tasks%20within%20a%20unified%20framework%2C%20but%20also%20jointly%20enhances%20their%20performance.%20We%20thus%20provide%20a%20reliable%20tool%20to%20facilitate%20downstream%20position-aware%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.16521v1&entry.124074799=Read"},
{"title": "TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming", "author": "Zeyuan Yin and Xiaoming Liu", "abstract": "Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\\textbf{TRIM}$ ($\\textbf{T}$rajectory $\\textbf{R}$eduction and $\\textbf{I}$nstance $\\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\\href{https://github.com/zeyuanyin/TRIM}{link}$.", "link": "http://arxiv.org/abs/2511.16642v1", "date": "2025-11-20", "relevancy": 2.3971, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6267}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5856}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRIM%3A%20Scalable%203D%20Gaussian%20Diffusion%20Inference%20with%20Temporal%20and%20Spatial%20Trimming&body=Title%3A%20TRIM%3A%20Scalable%203D%20Gaussian%20Diffusion%20Inference%20with%20Temporal%20and%20Spatial%20Trimming%0AAuthor%3A%20Zeyuan%20Yin%20and%20Xiaoming%20Liu%0AAbstract%3A%20Recent%20advances%20in%203D%20Gaussian%20diffusion%20models%20suffer%20from%20time-intensive%20denoising%20and%20post-denoising%20processing%20due%20to%20the%20massive%20number%20of%20Gaussian%20primitives%2C%20resulting%20in%20slow%20generation%20and%20limited%20scalability%20along%20sampling%20trajectories.%20To%20improve%20the%20efficiency%20of%203D%20diffusion%20models%2C%20we%20propose%20%24%5Ctextbf%7BTRIM%7D%24%20%28%24%5Ctextbf%7BT%7D%24rajectory%20%24%5Ctextbf%7BR%7D%24eduction%20and%20%24%5Ctextbf%7BI%7D%24nstance%20%24%5Ctextbf%7BM%7D%24ask%20denoising%29%2C%20a%20post-training%20approach%20that%20incorporates%20both%20temporal%20and%20spatial%20trimming%20strategies%2C%20to%20accelerate%20inference%20without%20compromising%20output%20quality%20while%20supporting%20the%20inference-time%20scaling%20for%20Gaussian%20diffusion%20models.%20Instead%20of%20scaling%20denoising%20trajectories%20in%20a%20costly%20end-to-end%20manner%2C%20we%20develop%20a%20lightweight%20selector%20model%20to%20evaluate%20latent%20Gaussian%20primitives%20derived%20from%20multiple%20sampled%20noises%2C%20enabling%20early%20trajectory%20reduction%20by%20selecting%20candidates%20with%20high-quality%20potential.%20Furthermore%2C%20we%20introduce%20instance%20mask%20denoising%20to%20prune%20learnable%20Gaussian%20primitives%20by%20filtering%20out%20redundant%20background%20regions%2C%20reducing%20inference%20computation%20at%20each%20denoising%20step.%20Extensive%20experiments%20and%20analysis%20demonstrate%20that%20TRIM%20significantly%20improves%20both%20the%20efficiency%20and%20quality%20of%203D%20generation.%20Source%20code%20is%20available%20at%20%24%5Chref%7Bhttps%3A//github.com/zeyuanyin/TRIM%7D%7Blink%7D%24.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRIM%253A%2520Scalable%25203D%2520Gaussian%2520Diffusion%2520Inference%2520with%2520Temporal%2520and%2520Spatial%2520Trimming%26entry.906535625%3DZeyuan%2520Yin%2520and%2520Xiaoming%2520Liu%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520Gaussian%2520diffusion%2520models%2520suffer%2520from%2520time-intensive%2520denoising%2520and%2520post-denoising%2520processing%2520due%2520to%2520the%2520massive%2520number%2520of%2520Gaussian%2520primitives%252C%2520resulting%2520in%2520slow%2520generation%2520and%2520limited%2520scalability%2520along%2520sampling%2520trajectories.%2520To%2520improve%2520the%2520efficiency%2520of%25203D%2520diffusion%2520models%252C%2520we%2520propose%2520%2524%255Ctextbf%257BTRIM%257D%2524%2520%2528%2524%255Ctextbf%257BT%257D%2524rajectory%2520%2524%255Ctextbf%257BR%257D%2524eduction%2520and%2520%2524%255Ctextbf%257BI%257D%2524nstance%2520%2524%255Ctextbf%257BM%257D%2524ask%2520denoising%2529%252C%2520a%2520post-training%2520approach%2520that%2520incorporates%2520both%2520temporal%2520and%2520spatial%2520trimming%2520strategies%252C%2520to%2520accelerate%2520inference%2520without%2520compromising%2520output%2520quality%2520while%2520supporting%2520the%2520inference-time%2520scaling%2520for%2520Gaussian%2520diffusion%2520models.%2520Instead%2520of%2520scaling%2520denoising%2520trajectories%2520in%2520a%2520costly%2520end-to-end%2520manner%252C%2520we%2520develop%2520a%2520lightweight%2520selector%2520model%2520to%2520evaluate%2520latent%2520Gaussian%2520primitives%2520derived%2520from%2520multiple%2520sampled%2520noises%252C%2520enabling%2520early%2520trajectory%2520reduction%2520by%2520selecting%2520candidates%2520with%2520high-quality%2520potential.%2520Furthermore%252C%2520we%2520introduce%2520instance%2520mask%2520denoising%2520to%2520prune%2520learnable%2520Gaussian%2520primitives%2520by%2520filtering%2520out%2520redundant%2520background%2520regions%252C%2520reducing%2520inference%2520computation%2520at%2520each%2520denoising%2520step.%2520Extensive%2520experiments%2520and%2520analysis%2520demonstrate%2520that%2520TRIM%2520significantly%2520improves%2520both%2520the%2520efficiency%2520and%2520quality%2520of%25203D%2520generation.%2520Source%2520code%2520is%2520available%2520at%2520%2524%255Chref%257Bhttps%253A//github.com/zeyuanyin/TRIM%257D%257Blink%257D%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRIM%3A%20Scalable%203D%20Gaussian%20Diffusion%20Inference%20with%20Temporal%20and%20Spatial%20Trimming&entry.906535625=Zeyuan%20Yin%20and%20Xiaoming%20Liu&entry.1292438233=Recent%20advances%20in%203D%20Gaussian%20diffusion%20models%20suffer%20from%20time-intensive%20denoising%20and%20post-denoising%20processing%20due%20to%20the%20massive%20number%20of%20Gaussian%20primitives%2C%20resulting%20in%20slow%20generation%20and%20limited%20scalability%20along%20sampling%20trajectories.%20To%20improve%20the%20efficiency%20of%203D%20diffusion%20models%2C%20we%20propose%20%24%5Ctextbf%7BTRIM%7D%24%20%28%24%5Ctextbf%7BT%7D%24rajectory%20%24%5Ctextbf%7BR%7D%24eduction%20and%20%24%5Ctextbf%7BI%7D%24nstance%20%24%5Ctextbf%7BM%7D%24ask%20denoising%29%2C%20a%20post-training%20approach%20that%20incorporates%20both%20temporal%20and%20spatial%20trimming%20strategies%2C%20to%20accelerate%20inference%20without%20compromising%20output%20quality%20while%20supporting%20the%20inference-time%20scaling%20for%20Gaussian%20diffusion%20models.%20Instead%20of%20scaling%20denoising%20trajectories%20in%20a%20costly%20end-to-end%20manner%2C%20we%20develop%20a%20lightweight%20selector%20model%20to%20evaluate%20latent%20Gaussian%20primitives%20derived%20from%20multiple%20sampled%20noises%2C%20enabling%20early%20trajectory%20reduction%20by%20selecting%20candidates%20with%20high-quality%20potential.%20Furthermore%2C%20we%20introduce%20instance%20mask%20denoising%20to%20prune%20learnable%20Gaussian%20primitives%20by%20filtering%20out%20redundant%20background%20regions%2C%20reducing%20inference%20computation%20at%20each%20denoising%20step.%20Extensive%20experiments%20and%20analysis%20demonstrate%20that%20TRIM%20significantly%20improves%20both%20the%20efficiency%20and%20quality%20of%203D%20generation.%20Source%20code%20is%20available%20at%20%24%5Chref%7Bhttps%3A//github.com/zeyuanyin/TRIM%7D%7Blink%7D%24.&entry.1838667208=http%3A//arxiv.org/abs/2511.16642v1&entry.124074799=Read"},
{"title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO", "author": "Junhao Cheng and Liang Hou and Xin Tao and Jing Liao", "abstract": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.", "link": "http://arxiv.org/abs/2511.16669v1", "date": "2025-11-20", "relevancy": 2.3874, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6332}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5735}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-as-Answer%3A%20Predict%20and%20Generate%20Next%20Video%20Event%20with%20Joint-GRPO&body=Title%3A%20Video-as-Answer%3A%20Predict%20and%20Generate%20Next%20Video%20Event%20with%20Joint-GRPO%0AAuthor%3A%20Junhao%20Cheng%20and%20Liang%20Hou%20and%20Xin%20Tao%20and%20Jing%20Liao%0AAbstract%3A%20While%20language%20models%20have%20become%20impactful%20in%20many%20real-world%20applications%2C%20video%20generation%20remains%20largely%20confined%20to%20entertainment.%20Motivated%20by%20video%27s%20inherent%20capacity%20to%20demonstrate%20physical-world%20information%20that%20is%20difficult%20to%20convey%20through%20language%20alone%20%28e.g.%2C%20imagine%20teaching%20someone%20to%20tie%20a%20tie%20using%20only%20text%29%2C%20we%20identify%20an%20underutilized%20opportunity%20to%20extend%20video%20as%20a%20new%20answer%20modality%20for%20Next-Event%20Prediction%20%28NEP%29%2C%20formalized%20as%20Video-Next-Event%20Prediction%20%28VNEP%29.%20While%20the%20established%20NEP%20task%20takes%20a%20video%20with%20a%20procedural%20or%20predictive%20question%20as%20input%20to%20predict%20the%20next%20event%20in%20text%2C%20VNEP%20requires%20dynamic%20video%20responses.%20This%20shift%20from%20telling%20to%20showing%20unlocks%20more%20intuitive%20and%20customized%20answers%20for%20procedural%20learning%20and%20creative%20exploration.%20However%2C%20this%20task%20remains%20challenging%20for%20existing%20models%2C%20as%20it%20demands%20an%20understanding%20of%20multimodal%20input%2C%20instruction-conditioned%20reasoning%2C%20and%20the%20generation%20of%20video%20with%20visual%20and%20semantic%20consistency.%20To%20address%20this%2C%20we%20introduce%20VANS%2C%20a%20model%20that%20leverages%20reinforcement%20learning%20to%20align%20a%20Vision-Language%20Model%20%28VLM%29%20with%20a%20Video%20Diffusion%20Model%20%28VDM%29%20for%20VNEP.%20The%20core%20of%20VANS%20is%20our%20proposed%20Joint-GRPO%20that%20orchestrates%20the%20VLM%20and%20VDM%20to%20function%20as%20a%20unit.%20Driven%20by%20a%20shared%20reward%20on%20their%20respective%20output%2C%20it%20optimizes%20the%20VLM%20to%20produce%20captions%20that%20are%20both%20accurate%20and%20friendly%20to%20visualize%2C%20while%20guiding%20the%20VDM%20to%20generate%20videos%20that%20are%20faithful%20to%20these%20captions%20and%20the%20input%20visual%20context.%20To%20enable%20this%20learning%2C%20we%20craft%20VANS-Data-100K%2C%20a%20dedicated%20dataset%20for%20the%20VNEP%20task.%20Experiments%20on%20procedural%20and%20predictive%20benchmarks%20demonstrate%20that%20VANS%20achieves%20state-of-the-art%20performance%20in%20both%20video%20event%20prediction%20and%20visualization.%20Codes%20are%20released%20in%20https%3A//github.com/KlingTeam/VANS.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-as-Answer%253A%2520Predict%2520and%2520Generate%2520Next%2520Video%2520Event%2520with%2520Joint-GRPO%26entry.906535625%3DJunhao%2520Cheng%2520and%2520Liang%2520Hou%2520and%2520Xin%2520Tao%2520and%2520Jing%2520Liao%26entry.1292438233%3DWhile%2520language%2520models%2520have%2520become%2520impactful%2520in%2520many%2520real-world%2520applications%252C%2520video%2520generation%2520remains%2520largely%2520confined%2520to%2520entertainment.%2520Motivated%2520by%2520video%2527s%2520inherent%2520capacity%2520to%2520demonstrate%2520physical-world%2520information%2520that%2520is%2520difficult%2520to%2520convey%2520through%2520language%2520alone%2520%2528e.g.%252C%2520imagine%2520teaching%2520someone%2520to%2520tie%2520a%2520tie%2520using%2520only%2520text%2529%252C%2520we%2520identify%2520an%2520underutilized%2520opportunity%2520to%2520extend%2520video%2520as%2520a%2520new%2520answer%2520modality%2520for%2520Next-Event%2520Prediction%2520%2528NEP%2529%252C%2520formalized%2520as%2520Video-Next-Event%2520Prediction%2520%2528VNEP%2529.%2520While%2520the%2520established%2520NEP%2520task%2520takes%2520a%2520video%2520with%2520a%2520procedural%2520or%2520predictive%2520question%2520as%2520input%2520to%2520predict%2520the%2520next%2520event%2520in%2520text%252C%2520VNEP%2520requires%2520dynamic%2520video%2520responses.%2520This%2520shift%2520from%2520telling%2520to%2520showing%2520unlocks%2520more%2520intuitive%2520and%2520customized%2520answers%2520for%2520procedural%2520learning%2520and%2520creative%2520exploration.%2520However%252C%2520this%2520task%2520remains%2520challenging%2520for%2520existing%2520models%252C%2520as%2520it%2520demands%2520an%2520understanding%2520of%2520multimodal%2520input%252C%2520instruction-conditioned%2520reasoning%252C%2520and%2520the%2520generation%2520of%2520video%2520with%2520visual%2520and%2520semantic%2520consistency.%2520To%2520address%2520this%252C%2520we%2520introduce%2520VANS%252C%2520a%2520model%2520that%2520leverages%2520reinforcement%2520learning%2520to%2520align%2520a%2520Vision-Language%2520Model%2520%2528VLM%2529%2520with%2520a%2520Video%2520Diffusion%2520Model%2520%2528VDM%2529%2520for%2520VNEP.%2520The%2520core%2520of%2520VANS%2520is%2520our%2520proposed%2520Joint-GRPO%2520that%2520orchestrates%2520the%2520VLM%2520and%2520VDM%2520to%2520function%2520as%2520a%2520unit.%2520Driven%2520by%2520a%2520shared%2520reward%2520on%2520their%2520respective%2520output%252C%2520it%2520optimizes%2520the%2520VLM%2520to%2520produce%2520captions%2520that%2520are%2520both%2520accurate%2520and%2520friendly%2520to%2520visualize%252C%2520while%2520guiding%2520the%2520VDM%2520to%2520generate%2520videos%2520that%2520are%2520faithful%2520to%2520these%2520captions%2520and%2520the%2520input%2520visual%2520context.%2520To%2520enable%2520this%2520learning%252C%2520we%2520craft%2520VANS-Data-100K%252C%2520a%2520dedicated%2520dataset%2520for%2520the%2520VNEP%2520task.%2520Experiments%2520on%2520procedural%2520and%2520predictive%2520benchmarks%2520demonstrate%2520that%2520VANS%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520video%2520event%2520prediction%2520and%2520visualization.%2520Codes%2520are%2520released%2520in%2520https%253A//github.com/KlingTeam/VANS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-as-Answer%3A%20Predict%20and%20Generate%20Next%20Video%20Event%20with%20Joint-GRPO&entry.906535625=Junhao%20Cheng%20and%20Liang%20Hou%20and%20Xin%20Tao%20and%20Jing%20Liao&entry.1292438233=While%20language%20models%20have%20become%20impactful%20in%20many%20real-world%20applications%2C%20video%20generation%20remains%20largely%20confined%20to%20entertainment.%20Motivated%20by%20video%27s%20inherent%20capacity%20to%20demonstrate%20physical-world%20information%20that%20is%20difficult%20to%20convey%20through%20language%20alone%20%28e.g.%2C%20imagine%20teaching%20someone%20to%20tie%20a%20tie%20using%20only%20text%29%2C%20we%20identify%20an%20underutilized%20opportunity%20to%20extend%20video%20as%20a%20new%20answer%20modality%20for%20Next-Event%20Prediction%20%28NEP%29%2C%20formalized%20as%20Video-Next-Event%20Prediction%20%28VNEP%29.%20While%20the%20established%20NEP%20task%20takes%20a%20video%20with%20a%20procedural%20or%20predictive%20question%20as%20input%20to%20predict%20the%20next%20event%20in%20text%2C%20VNEP%20requires%20dynamic%20video%20responses.%20This%20shift%20from%20telling%20to%20showing%20unlocks%20more%20intuitive%20and%20customized%20answers%20for%20procedural%20learning%20and%20creative%20exploration.%20However%2C%20this%20task%20remains%20challenging%20for%20existing%20models%2C%20as%20it%20demands%20an%20understanding%20of%20multimodal%20input%2C%20instruction-conditioned%20reasoning%2C%20and%20the%20generation%20of%20video%20with%20visual%20and%20semantic%20consistency.%20To%20address%20this%2C%20we%20introduce%20VANS%2C%20a%20model%20that%20leverages%20reinforcement%20learning%20to%20align%20a%20Vision-Language%20Model%20%28VLM%29%20with%20a%20Video%20Diffusion%20Model%20%28VDM%29%20for%20VNEP.%20The%20core%20of%20VANS%20is%20our%20proposed%20Joint-GRPO%20that%20orchestrates%20the%20VLM%20and%20VDM%20to%20function%20as%20a%20unit.%20Driven%20by%20a%20shared%20reward%20on%20their%20respective%20output%2C%20it%20optimizes%20the%20VLM%20to%20produce%20captions%20that%20are%20both%20accurate%20and%20friendly%20to%20visualize%2C%20while%20guiding%20the%20VDM%20to%20generate%20videos%20that%20are%20faithful%20to%20these%20captions%20and%20the%20input%20visual%20context.%20To%20enable%20this%20learning%2C%20we%20craft%20VANS-Data-100K%2C%20a%20dedicated%20dataset%20for%20the%20VNEP%20task.%20Experiments%20on%20procedural%20and%20predictive%20benchmarks%20demonstrate%20that%20VANS%20achieves%20state-of-the-art%20performance%20in%20both%20video%20event%20prediction%20and%20visualization.%20Codes%20are%20released%20in%20https%3A//github.com/KlingTeam/VANS.&entry.1838667208=http%3A//arxiv.org/abs/2511.16669v1&entry.124074799=Read"},
{"title": "NaTex: Seamless Texture Generation as Latent Color Diffusion", "author": "Zeqiang Lai and Yunfei Zhao and Zibo Zhao and Xin Yang and Xin Huang and Jingwei Huang and Xiangyu Yue and Chunchao Guo", "abstract": "We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.", "link": "http://arxiv.org/abs/2511.16317v1", "date": "2025-11-20", "relevancy": 2.3842, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6264}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5858}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NaTex%3A%20Seamless%20Texture%20Generation%20as%20Latent%20Color%20Diffusion&body=Title%3A%20NaTex%3A%20Seamless%20Texture%20Generation%20as%20Latent%20Color%20Diffusion%0AAuthor%3A%20Zeqiang%20Lai%20and%20Yunfei%20Zhao%20and%20Zibo%20Zhao%20and%20Xin%20Yang%20and%20Xin%20Huang%20and%20Jingwei%20Huang%20and%20Xiangyu%20Yue%20and%20Chunchao%20Guo%0AAbstract%3A%20We%20present%20NaTex%2C%20a%20native%20texture%20generation%20framework%20that%20predicts%20texture%20color%20directly%20in%203D%20space.%20In%20contrast%20to%20previous%20approaches%20that%20rely%20on%20baking%202D%20multi-view%20images%20synthesized%20by%20geometry-conditioned%20Multi-View%20Diffusion%20models%20%28MVDs%29%2C%20NaTex%20avoids%20several%20inherent%20limitations%20of%20the%20MVD%20pipeline.%20These%20include%20difficulties%20in%20handling%20occluded%20regions%20that%20require%20inpainting%2C%20achieving%20precise%20mesh-texture%20alignment%20along%20boundaries%2C%20and%20maintaining%20cross-view%20consistency%20and%20coherence%20in%20both%20content%20and%20color%20intensity.%20NaTex%20features%20a%20novel%20paradigm%20that%20addresses%20the%20aforementioned%20issues%20by%20viewing%20texture%20as%20a%20dense%20color%20point%20cloud.%20Driven%20by%20this%20idea%2C%20we%20propose%20latent%20color%20diffusion%2C%20which%20comprises%20a%20geometry-awared%20color%20point%20cloud%20VAE%20and%20a%20multi-control%20diffusion%20transformer%20%28DiT%29%2C%20entirely%20trained%20from%20scratch%20using%203D%20data%2C%20for%20texture%20reconstruction%20and%20generation.%20To%20enable%20precise%20alignment%2C%20we%20introduce%20native%20geometry%20control%20that%20conditions%20the%20DiT%20on%20direct%203D%20spatial%20information%20via%20positional%20embeddings%20and%20geometry%20latents.%20We%20co-design%20the%20VAE-DiT%20architecture%2C%20where%20the%20geometry%20latents%20are%20extracted%20via%20a%20dedicated%20geometry%20branch%20tightly%20coupled%20with%20the%20color%20VAE%2C%20providing%20fine-grained%20surface%20guidance%20that%20maintains%20strong%20correspondence%20with%20the%20texture.%20With%20these%20designs%2C%20NaTex%20demonstrates%20strong%20performance%2C%20significantly%20outperforming%20previous%20methods%20in%20texture%20coherence%20and%20alignment.%20Moreover%2C%20NaTex%20also%20exhibits%20strong%20generalization%20capabilities%2C%20either%20training-free%20or%20with%20simple%20tuning%2C%20for%20various%20downstream%20applications%2C%20e.g.%2C%20material%20generation%2C%20texture%20refinement%2C%20and%20part%20segmentation%20and%20texturing.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNaTex%253A%2520Seamless%2520Texture%2520Generation%2520as%2520Latent%2520Color%2520Diffusion%26entry.906535625%3DZeqiang%2520Lai%2520and%2520Yunfei%2520Zhao%2520and%2520Zibo%2520Zhao%2520and%2520Xin%2520Yang%2520and%2520Xin%2520Huang%2520and%2520Jingwei%2520Huang%2520and%2520Xiangyu%2520Yue%2520and%2520Chunchao%2520Guo%26entry.1292438233%3DWe%2520present%2520NaTex%252C%2520a%2520native%2520texture%2520generation%2520framework%2520that%2520predicts%2520texture%2520color%2520directly%2520in%25203D%2520space.%2520In%2520contrast%2520to%2520previous%2520approaches%2520that%2520rely%2520on%2520baking%25202D%2520multi-view%2520images%2520synthesized%2520by%2520geometry-conditioned%2520Multi-View%2520Diffusion%2520models%2520%2528MVDs%2529%252C%2520NaTex%2520avoids%2520several%2520inherent%2520limitations%2520of%2520the%2520MVD%2520pipeline.%2520These%2520include%2520difficulties%2520in%2520handling%2520occluded%2520regions%2520that%2520require%2520inpainting%252C%2520achieving%2520precise%2520mesh-texture%2520alignment%2520along%2520boundaries%252C%2520and%2520maintaining%2520cross-view%2520consistency%2520and%2520coherence%2520in%2520both%2520content%2520and%2520color%2520intensity.%2520NaTex%2520features%2520a%2520novel%2520paradigm%2520that%2520addresses%2520the%2520aforementioned%2520issues%2520by%2520viewing%2520texture%2520as%2520a%2520dense%2520color%2520point%2520cloud.%2520Driven%2520by%2520this%2520idea%252C%2520we%2520propose%2520latent%2520color%2520diffusion%252C%2520which%2520comprises%2520a%2520geometry-awared%2520color%2520point%2520cloud%2520VAE%2520and%2520a%2520multi-control%2520diffusion%2520transformer%2520%2528DiT%2529%252C%2520entirely%2520trained%2520from%2520scratch%2520using%25203D%2520data%252C%2520for%2520texture%2520reconstruction%2520and%2520generation.%2520To%2520enable%2520precise%2520alignment%252C%2520we%2520introduce%2520native%2520geometry%2520control%2520that%2520conditions%2520the%2520DiT%2520on%2520direct%25203D%2520spatial%2520information%2520via%2520positional%2520embeddings%2520and%2520geometry%2520latents.%2520We%2520co-design%2520the%2520VAE-DiT%2520architecture%252C%2520where%2520the%2520geometry%2520latents%2520are%2520extracted%2520via%2520a%2520dedicated%2520geometry%2520branch%2520tightly%2520coupled%2520with%2520the%2520color%2520VAE%252C%2520providing%2520fine-grained%2520surface%2520guidance%2520that%2520maintains%2520strong%2520correspondence%2520with%2520the%2520texture.%2520With%2520these%2520designs%252C%2520NaTex%2520demonstrates%2520strong%2520performance%252C%2520significantly%2520outperforming%2520previous%2520methods%2520in%2520texture%2520coherence%2520and%2520alignment.%2520Moreover%252C%2520NaTex%2520also%2520exhibits%2520strong%2520generalization%2520capabilities%252C%2520either%2520training-free%2520or%2520with%2520simple%2520tuning%252C%2520for%2520various%2520downstream%2520applications%252C%2520e.g.%252C%2520material%2520generation%252C%2520texture%2520refinement%252C%2520and%2520part%2520segmentation%2520and%2520texturing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NaTex%3A%20Seamless%20Texture%20Generation%20as%20Latent%20Color%20Diffusion&entry.906535625=Zeqiang%20Lai%20and%20Yunfei%20Zhao%20and%20Zibo%20Zhao%20and%20Xin%20Yang%20and%20Xin%20Huang%20and%20Jingwei%20Huang%20and%20Xiangyu%20Yue%20and%20Chunchao%20Guo&entry.1292438233=We%20present%20NaTex%2C%20a%20native%20texture%20generation%20framework%20that%20predicts%20texture%20color%20directly%20in%203D%20space.%20In%20contrast%20to%20previous%20approaches%20that%20rely%20on%20baking%202D%20multi-view%20images%20synthesized%20by%20geometry-conditioned%20Multi-View%20Diffusion%20models%20%28MVDs%29%2C%20NaTex%20avoids%20several%20inherent%20limitations%20of%20the%20MVD%20pipeline.%20These%20include%20difficulties%20in%20handling%20occluded%20regions%20that%20require%20inpainting%2C%20achieving%20precise%20mesh-texture%20alignment%20along%20boundaries%2C%20and%20maintaining%20cross-view%20consistency%20and%20coherence%20in%20both%20content%20and%20color%20intensity.%20NaTex%20features%20a%20novel%20paradigm%20that%20addresses%20the%20aforementioned%20issues%20by%20viewing%20texture%20as%20a%20dense%20color%20point%20cloud.%20Driven%20by%20this%20idea%2C%20we%20propose%20latent%20color%20diffusion%2C%20which%20comprises%20a%20geometry-awared%20color%20point%20cloud%20VAE%20and%20a%20multi-control%20diffusion%20transformer%20%28DiT%29%2C%20entirely%20trained%20from%20scratch%20using%203D%20data%2C%20for%20texture%20reconstruction%20and%20generation.%20To%20enable%20precise%20alignment%2C%20we%20introduce%20native%20geometry%20control%20that%20conditions%20the%20DiT%20on%20direct%203D%20spatial%20information%20via%20positional%20embeddings%20and%20geometry%20latents.%20We%20co-design%20the%20VAE-DiT%20architecture%2C%20where%20the%20geometry%20latents%20are%20extracted%20via%20a%20dedicated%20geometry%20branch%20tightly%20coupled%20with%20the%20color%20VAE%2C%20providing%20fine-grained%20surface%20guidance%20that%20maintains%20strong%20correspondence%20with%20the%20texture.%20With%20these%20designs%2C%20NaTex%20demonstrates%20strong%20performance%2C%20significantly%20outperforming%20previous%20methods%20in%20texture%20coherence%20and%20alignment.%20Moreover%2C%20NaTex%20also%20exhibits%20strong%20generalization%20capabilities%2C%20either%20training-free%20or%20with%20simple%20tuning%2C%20for%20various%20downstream%20applications%2C%20e.g.%2C%20material%20generation%2C%20texture%20refinement%2C%20and%20part%20segmentation%20and%20texturing.&entry.1838667208=http%3A//arxiv.org/abs/2511.16317v1&entry.124074799=Read"},
{"title": "Human Motion Unlearning", "author": "Edoardo De Matteis and Matteo Migliarini and Alessio Sampieri and Indro Spinelli and Fabio Galasso", "abstract": "We introduce the task of human motion unlearning to prevent the synthesis of toxic animations while preserving the general text-to-motion generative performance. Unlearning toxic motions is challenging as those can be generated from explicit text prompts and from implicit toxic combinations of safe motions (e.g., \"kicking\" is \"loading and swinging a leg\"). We propose the first motion unlearning benchmark by filtering toxic motions from the large and recent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines, by adapting state-of-the-art image unlearning techniques to process spatio-temporal signals. Finally, we propose a novel motion unlearning model based on Latent Code Replacement, which we dub LCR. LCR is training-free and suitable to the discrete latent spaces of state-of-the-art text-to-motion diffusion models. LCR is simple and consistently outperforms baselines qualitatively and quantitatively. Project page: https://www.pinlab.org/hmu.", "link": "http://arxiv.org/abs/2503.18674v2", "date": "2025-11-20", "relevancy": 2.3782, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6483}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6136}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Motion%20Unlearning&body=Title%3A%20Human%20Motion%20Unlearning%0AAuthor%3A%20Edoardo%20De%20Matteis%20and%20Matteo%20Migliarini%20and%20Alessio%20Sampieri%20and%20Indro%20Spinelli%20and%20Fabio%20Galasso%0AAbstract%3A%20We%20introduce%20the%20task%20of%20human%20motion%20unlearning%20to%20prevent%20the%20synthesis%20of%20toxic%20animations%20while%20preserving%20the%20general%20text-to-motion%20generative%20performance.%20Unlearning%20toxic%20motions%20is%20challenging%20as%20those%20can%20be%20generated%20from%20explicit%20text%20prompts%20and%20from%20implicit%20toxic%20combinations%20of%20safe%20motions%20%28e.g.%2C%20%22kicking%22%20is%20%22loading%20and%20swinging%20a%20leg%22%29.%20We%20propose%20the%20first%20motion%20unlearning%20benchmark%20by%20filtering%20toxic%20motions%20from%20the%20large%20and%20recent%20text-to-motion%20datasets%20of%20HumanML3D%20and%20Motion-X.%20We%20propose%20baselines%2C%20by%20adapting%20state-of-the-art%20image%20unlearning%20techniques%20to%20process%20spatio-temporal%20signals.%20Finally%2C%20we%20propose%20a%20novel%20motion%20unlearning%20model%20based%20on%20Latent%20Code%20Replacement%2C%20which%20we%20dub%20LCR.%20LCR%20is%20training-free%20and%20suitable%20to%20the%20discrete%20latent%20spaces%20of%20state-of-the-art%20text-to-motion%20diffusion%20models.%20LCR%20is%20simple%20and%20consistently%20outperforms%20baselines%20qualitatively%20and%20quantitatively.%20Project%20page%3A%20https%3A//www.pinlab.org/hmu.%0ALink%3A%20http%3A//arxiv.org/abs/2503.18674v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Motion%2520Unlearning%26entry.906535625%3DEdoardo%2520De%2520Matteis%2520and%2520Matteo%2520Migliarini%2520and%2520Alessio%2520Sampieri%2520and%2520Indro%2520Spinelli%2520and%2520Fabio%2520Galasso%26entry.1292438233%3DWe%2520introduce%2520the%2520task%2520of%2520human%2520motion%2520unlearning%2520to%2520prevent%2520the%2520synthesis%2520of%2520toxic%2520animations%2520while%2520preserving%2520the%2520general%2520text-to-motion%2520generative%2520performance.%2520Unlearning%2520toxic%2520motions%2520is%2520challenging%2520as%2520those%2520can%2520be%2520generated%2520from%2520explicit%2520text%2520prompts%2520and%2520from%2520implicit%2520toxic%2520combinations%2520of%2520safe%2520motions%2520%2528e.g.%252C%2520%2522kicking%2522%2520is%2520%2522loading%2520and%2520swinging%2520a%2520leg%2522%2529.%2520We%2520propose%2520the%2520first%2520motion%2520unlearning%2520benchmark%2520by%2520filtering%2520toxic%2520motions%2520from%2520the%2520large%2520and%2520recent%2520text-to-motion%2520datasets%2520of%2520HumanML3D%2520and%2520Motion-X.%2520We%2520propose%2520baselines%252C%2520by%2520adapting%2520state-of-the-art%2520image%2520unlearning%2520techniques%2520to%2520process%2520spatio-temporal%2520signals.%2520Finally%252C%2520we%2520propose%2520a%2520novel%2520motion%2520unlearning%2520model%2520based%2520on%2520Latent%2520Code%2520Replacement%252C%2520which%2520we%2520dub%2520LCR.%2520LCR%2520is%2520training-free%2520and%2520suitable%2520to%2520the%2520discrete%2520latent%2520spaces%2520of%2520state-of-the-art%2520text-to-motion%2520diffusion%2520models.%2520LCR%2520is%2520simple%2520and%2520consistently%2520outperforms%2520baselines%2520qualitatively%2520and%2520quantitatively.%2520Project%2520page%253A%2520https%253A//www.pinlab.org/hmu.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18674v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Motion%20Unlearning&entry.906535625=Edoardo%20De%20Matteis%20and%20Matteo%20Migliarini%20and%20Alessio%20Sampieri%20and%20Indro%20Spinelli%20and%20Fabio%20Galasso&entry.1292438233=We%20introduce%20the%20task%20of%20human%20motion%20unlearning%20to%20prevent%20the%20synthesis%20of%20toxic%20animations%20while%20preserving%20the%20general%20text-to-motion%20generative%20performance.%20Unlearning%20toxic%20motions%20is%20challenging%20as%20those%20can%20be%20generated%20from%20explicit%20text%20prompts%20and%20from%20implicit%20toxic%20combinations%20of%20safe%20motions%20%28e.g.%2C%20%22kicking%22%20is%20%22loading%20and%20swinging%20a%20leg%22%29.%20We%20propose%20the%20first%20motion%20unlearning%20benchmark%20by%20filtering%20toxic%20motions%20from%20the%20large%20and%20recent%20text-to-motion%20datasets%20of%20HumanML3D%20and%20Motion-X.%20We%20propose%20baselines%2C%20by%20adapting%20state-of-the-art%20image%20unlearning%20techniques%20to%20process%20spatio-temporal%20signals.%20Finally%2C%20we%20propose%20a%20novel%20motion%20unlearning%20model%20based%20on%20Latent%20Code%20Replacement%2C%20which%20we%20dub%20LCR.%20LCR%20is%20training-free%20and%20suitable%20to%20the%20discrete%20latent%20spaces%20of%20state-of-the-art%20text-to-motion%20diffusion%20models.%20LCR%20is%20simple%20and%20consistently%20outperforms%20baselines%20qualitatively%20and%20quantitatively.%20Project%20page%3A%20https%3A//www.pinlab.org/hmu.&entry.1838667208=http%3A//arxiv.org/abs/2503.18674v2&entry.124074799=Read"},
{"title": "Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness", "author": "Thomas Pethick and Wanyun Xie and Mete Erdogan and Kimon Antonakopoulos and Tony Silveti-Falls and Volkan Cevher", "abstract": "This work introduces a hybrid non-Euclidean optimization method which generalizes gradient norm clipping by combining steepest descent and conditional gradient approaches. The method achieves the best of both worlds by establishing a descent property under a generalized notion of ($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner by identifying a connection to the Frank-Wolfe short step. In the stochastic case, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a momentum based gradient estimator. We discuss how to instantiate the algorithms for deep learning, which we dub Clipped Scion, and demonstrate their properties on image classification and language modeling. The code is available at https://github.com/LIONS-EPFL/ClippedScion.", "link": "http://arxiv.org/abs/2506.01913v2", "date": "2025-11-20", "relevancy": 2.3747, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4809}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4736}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Gradient%20Norm%20Clipping%20%26%20Non-Euclidean%20%24%28L_0%2CL_1%29%24-Smoothness&body=Title%3A%20Generalized%20Gradient%20Norm%20Clipping%20%26%20Non-Euclidean%20%24%28L_0%2CL_1%29%24-Smoothness%0AAuthor%3A%20Thomas%20Pethick%20and%20Wanyun%20Xie%20and%20Mete%20Erdogan%20and%20Kimon%20Antonakopoulos%20and%20Tony%20Silveti-Falls%20and%20Volkan%20Cevher%0AAbstract%3A%20This%20work%20introduces%20a%20hybrid%20non-Euclidean%20optimization%20method%20which%20generalizes%20gradient%20norm%20clipping%20by%20combining%20steepest%20descent%20and%20conditional%20gradient%20approaches.%20The%20method%20achieves%20the%20best%20of%20both%20worlds%20by%20establishing%20a%20descent%20property%20under%20a%20generalized%20notion%20of%20%28%24L_0%24%2C%24L_1%24%29-smoothness.%20Weight%20decay%20is%20incorporated%20in%20a%20principled%20manner%20by%20identifying%20a%20connection%20to%20the%20Frank-Wolfe%20short%20step.%20In%20the%20stochastic%20case%2C%20we%20show%20an%20order%20optimal%20%24O%28n%5E%7B-1/4%7D%29%24%20convergence%20rate%20by%20leveraging%20a%20momentum%20based%20gradient%20estimator.%20We%20discuss%20how%20to%20instantiate%20the%20algorithms%20for%20deep%20learning%2C%20which%20we%20dub%20Clipped%20Scion%2C%20and%20demonstrate%20their%20properties%20on%20image%20classification%20and%20language%20modeling.%20The%20code%20is%20available%20at%20https%3A//github.com/LIONS-EPFL/ClippedScion.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01913v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Gradient%2520Norm%2520Clipping%2520%2526%2520Non-Euclidean%2520%2524%2528L_0%252CL_1%2529%2524-Smoothness%26entry.906535625%3DThomas%2520Pethick%2520and%2520Wanyun%2520Xie%2520and%2520Mete%2520Erdogan%2520and%2520Kimon%2520Antonakopoulos%2520and%2520Tony%2520Silveti-Falls%2520and%2520Volkan%2520Cevher%26entry.1292438233%3DThis%2520work%2520introduces%2520a%2520hybrid%2520non-Euclidean%2520optimization%2520method%2520which%2520generalizes%2520gradient%2520norm%2520clipping%2520by%2520combining%2520steepest%2520descent%2520and%2520conditional%2520gradient%2520approaches.%2520The%2520method%2520achieves%2520the%2520best%2520of%2520both%2520worlds%2520by%2520establishing%2520a%2520descent%2520property%2520under%2520a%2520generalized%2520notion%2520of%2520%2528%2524L_0%2524%252C%2524L_1%2524%2529-smoothness.%2520Weight%2520decay%2520is%2520incorporated%2520in%2520a%2520principled%2520manner%2520by%2520identifying%2520a%2520connection%2520to%2520the%2520Frank-Wolfe%2520short%2520step.%2520In%2520the%2520stochastic%2520case%252C%2520we%2520show%2520an%2520order%2520optimal%2520%2524O%2528n%255E%257B-1/4%257D%2529%2524%2520convergence%2520rate%2520by%2520leveraging%2520a%2520momentum%2520based%2520gradient%2520estimator.%2520We%2520discuss%2520how%2520to%2520instantiate%2520the%2520algorithms%2520for%2520deep%2520learning%252C%2520which%2520we%2520dub%2520Clipped%2520Scion%252C%2520and%2520demonstrate%2520their%2520properties%2520on%2520image%2520classification%2520and%2520language%2520modeling.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/LIONS-EPFL/ClippedScion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01913v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Gradient%20Norm%20Clipping%20%26%20Non-Euclidean%20%24%28L_0%2CL_1%29%24-Smoothness&entry.906535625=Thomas%20Pethick%20and%20Wanyun%20Xie%20and%20Mete%20Erdogan%20and%20Kimon%20Antonakopoulos%20and%20Tony%20Silveti-Falls%20and%20Volkan%20Cevher&entry.1292438233=This%20work%20introduces%20a%20hybrid%20non-Euclidean%20optimization%20method%20which%20generalizes%20gradient%20norm%20clipping%20by%20combining%20steepest%20descent%20and%20conditional%20gradient%20approaches.%20The%20method%20achieves%20the%20best%20of%20both%20worlds%20by%20establishing%20a%20descent%20property%20under%20a%20generalized%20notion%20of%20%28%24L_0%24%2C%24L_1%24%29-smoothness.%20Weight%20decay%20is%20incorporated%20in%20a%20principled%20manner%20by%20identifying%20a%20connection%20to%20the%20Frank-Wolfe%20short%20step.%20In%20the%20stochastic%20case%2C%20we%20show%20an%20order%20optimal%20%24O%28n%5E%7B-1/4%7D%29%24%20convergence%20rate%20by%20leveraging%20a%20momentum%20based%20gradient%20estimator.%20We%20discuss%20how%20to%20instantiate%20the%20algorithms%20for%20deep%20learning%2C%20which%20we%20dub%20Clipped%20Scion%2C%20and%20demonstrate%20their%20properties%20on%20image%20classification%20and%20language%20modeling.%20The%20code%20is%20available%20at%20https%3A//github.com/LIONS-EPFL/ClippedScion.&entry.1838667208=http%3A//arxiv.org/abs/2506.01913v2&entry.124074799=Read"},
{"title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering", "author": "Jianhan Qi and Yuheng Jia and Hui Liu and Junhui Hou", "abstract": "Hyperspectral image (HSI) clustering groups pixels into clusters without labeled data, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.", "link": "http://arxiv.org/abs/2506.09920v4", "date": "2025-11-20", "relevancy": 2.3718, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4793}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4722}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural-Spectral%20Graph%20Convolution%20with%20Evidential%20Edge%20Learning%20for%20Hyperspectral%20Image%20Clustering&body=Title%3A%20Structural-Spectral%20Graph%20Convolution%20with%20Evidential%20Edge%20Learning%20for%20Hyperspectral%20Image%20Clustering%0AAuthor%3A%20Jianhan%20Qi%20and%20Yuheng%20Jia%20and%20Hui%20Liu%20and%20Junhui%20Hou%0AAbstract%3A%20Hyperspectral%20image%20%28HSI%29%20clustering%20groups%20pixels%20into%20clusters%20without%20labeled%20data%2C%20which%20is%20an%20important%20yet%20challenging%20task.%20For%20large-scale%20HSIs%2C%20most%20methods%20rely%20on%20superpixel%20segmentation%20and%20perform%20superpixel-level%20clustering%20based%20on%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%20existing%20GNNs%20cannot%20fully%20exploit%20the%20spectral%20information%20of%20the%20input%20HSI%2C%20and%20the%20inaccurate%20superpixel%20topological%20graph%20may%20lead%20to%20the%20confusion%20of%20different%20class%20semantics%20during%20information%20aggregation.%20To%20address%20these%20challenges%2C%20we%20first%20propose%20a%20structural-spectral%20graph%20convolutional%20operator%20%28SSGCO%29%20tailored%20for%20graph-structured%20HSI%20superpixels%20to%20improve%20their%20representation%20quality%20through%20the%20co-extraction%20of%20spatial%20and%20spectral%20features.%20Second%2C%20we%20propose%20an%20evidence-guided%20adaptive%20edge%20learning%20%28EGAEL%29%20module%20that%20adaptively%20predicts%20and%20refines%20edge%20weights%20in%20the%20superpixel%20topological%20graph.%20We%20integrate%20the%20proposed%20method%20into%20a%20contrastive%20learning%20framework%20to%20achieve%20clustering%2C%20where%20representation%20learning%20and%20clustering%20are%20simultaneously%20conducted.%20Experiments%20demonstrate%20that%20the%20proposed%20method%20improves%20clustering%20accuracy%20by%202.61%25%2C%206.06%25%2C%204.96%25%20and%203.15%25%20over%20the%20best%20compared%20methods%20on%20four%20HSI%20datasets.%20Our%20code%20is%20available%20at%20https%3A//github.com/jhqi/SSGCO-EGAEL.%0ALink%3A%20http%3A//arxiv.org/abs/2506.09920v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural-Spectral%2520Graph%2520Convolution%2520with%2520Evidential%2520Edge%2520Learning%2520for%2520Hyperspectral%2520Image%2520Clustering%26entry.906535625%3DJianhan%2520Qi%2520and%2520Yuheng%2520Jia%2520and%2520Hui%2520Liu%2520and%2520Junhui%2520Hou%26entry.1292438233%3DHyperspectral%2520image%2520%2528HSI%2529%2520clustering%2520groups%2520pixels%2520into%2520clusters%2520without%2520labeled%2520data%252C%2520which%2520is%2520an%2520important%2520yet%2520challenging%2520task.%2520For%2520large-scale%2520HSIs%252C%2520most%2520methods%2520rely%2520on%2520superpixel%2520segmentation%2520and%2520perform%2520superpixel-level%2520clustering%2520based%2520on%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520However%252C%2520existing%2520GNNs%2520cannot%2520fully%2520exploit%2520the%2520spectral%2520information%2520of%2520the%2520input%2520HSI%252C%2520and%2520the%2520inaccurate%2520superpixel%2520topological%2520graph%2520may%2520lead%2520to%2520the%2520confusion%2520of%2520different%2520class%2520semantics%2520during%2520information%2520aggregation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520first%2520propose%2520a%2520structural-spectral%2520graph%2520convolutional%2520operator%2520%2528SSGCO%2529%2520tailored%2520for%2520graph-structured%2520HSI%2520superpixels%2520to%2520improve%2520their%2520representation%2520quality%2520through%2520the%2520co-extraction%2520of%2520spatial%2520and%2520spectral%2520features.%2520Second%252C%2520we%2520propose%2520an%2520evidence-guided%2520adaptive%2520edge%2520learning%2520%2528EGAEL%2529%2520module%2520that%2520adaptively%2520predicts%2520and%2520refines%2520edge%2520weights%2520in%2520the%2520superpixel%2520topological%2520graph.%2520We%2520integrate%2520the%2520proposed%2520method%2520into%2520a%2520contrastive%2520learning%2520framework%2520to%2520achieve%2520clustering%252C%2520where%2520representation%2520learning%2520and%2520clustering%2520are%2520simultaneously%2520conducted.%2520Experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520improves%2520clustering%2520accuracy%2520by%25202.61%2525%252C%25206.06%2525%252C%25204.96%2525%2520and%25203.15%2525%2520over%2520the%2520best%2520compared%2520methods%2520on%2520four%2520HSI%2520datasets.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/jhqi/SSGCO-EGAEL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09920v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural-Spectral%20Graph%20Convolution%20with%20Evidential%20Edge%20Learning%20for%20Hyperspectral%20Image%20Clustering&entry.906535625=Jianhan%20Qi%20and%20Yuheng%20Jia%20and%20Hui%20Liu%20and%20Junhui%20Hou&entry.1292438233=Hyperspectral%20image%20%28HSI%29%20clustering%20groups%20pixels%20into%20clusters%20without%20labeled%20data%2C%20which%20is%20an%20important%20yet%20challenging%20task.%20For%20large-scale%20HSIs%2C%20most%20methods%20rely%20on%20superpixel%20segmentation%20and%20perform%20superpixel-level%20clustering%20based%20on%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%20existing%20GNNs%20cannot%20fully%20exploit%20the%20spectral%20information%20of%20the%20input%20HSI%2C%20and%20the%20inaccurate%20superpixel%20topological%20graph%20may%20lead%20to%20the%20confusion%20of%20different%20class%20semantics%20during%20information%20aggregation.%20To%20address%20these%20challenges%2C%20we%20first%20propose%20a%20structural-spectral%20graph%20convolutional%20operator%20%28SSGCO%29%20tailored%20for%20graph-structured%20HSI%20superpixels%20to%20improve%20their%20representation%20quality%20through%20the%20co-extraction%20of%20spatial%20and%20spectral%20features.%20Second%2C%20we%20propose%20an%20evidence-guided%20adaptive%20edge%20learning%20%28EGAEL%29%20module%20that%20adaptively%20predicts%20and%20refines%20edge%20weights%20in%20the%20superpixel%20topological%20graph.%20We%20integrate%20the%20proposed%20method%20into%20a%20contrastive%20learning%20framework%20to%20achieve%20clustering%2C%20where%20representation%20learning%20and%20clustering%20are%20simultaneously%20conducted.%20Experiments%20demonstrate%20that%20the%20proposed%20method%20improves%20clustering%20accuracy%20by%202.61%25%2C%206.06%25%2C%204.96%25%20and%203.15%25%20over%20the%20best%20compared%20methods%20on%20four%20HSI%20datasets.%20Our%20code%20is%20available%20at%20https%3A//github.com/jhqi/SSGCO-EGAEL.&entry.1838667208=http%3A//arxiv.org/abs/2506.09920v4&entry.124074799=Read"},
{"title": "System Filter-Based Common Components Modeling for Cross-Subject EEG Decoding", "author": "Xiaoyuan Li and Xinru Xue and Bohan Zhang and Ye Sun and Shoushuo Xi and Gang Liu", "abstract": "Brain-computer interface (BCI) technology enables direct communication between the brain and external devices through electroencephalography (EEG) signals. However, existing decoding models often mix common and personalized components, leading to interference from individual variability that limits cross-subject decoding performance. To address this issue, this paper proposes a system filter that extends the concept of signal filtering to the system level. The method expands a system into its spectral representation, selectively removes unnecessary components, and reconstructs the system from the retained target components, thereby achieving explicit system-level decomposition and filtering. We further integrate the system filter into a Cross-Subject Decoding framework based on the System Filter (CSD-SF) and evaluate it on the four-class motor imagery (MI) task of the BCIC IV 2a dataset. Personalized models are transformed into relation spectrums, and statistical testing across subjects is used to remove personalized components. The remaining stable relations, representing common components across subjects, are then used to construct a common model for cross-subject decoding. Experimental results show an average improvement of 3.28% in decoding accuracy over baseline methods, demonstrating that the proposed system filter effectively isolates stable common components and enhances model robustness and generalizability in cross-subject EEG decoding.", "link": "http://arxiv.org/abs/2507.05268v2", "date": "2025-11-20", "relevancy": 2.3657, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4766}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20System%20Filter-Based%20Common%20Components%20Modeling%20for%20Cross-Subject%20EEG%20Decoding&body=Title%3A%20System%20Filter-Based%20Common%20Components%20Modeling%20for%20Cross-Subject%20EEG%20Decoding%0AAuthor%3A%20Xiaoyuan%20Li%20and%20Xinru%20Xue%20and%20Bohan%20Zhang%20and%20Ye%20Sun%20and%20Shoushuo%20Xi%20and%20Gang%20Liu%0AAbstract%3A%20Brain-computer%20interface%20%28BCI%29%20technology%20enables%20direct%20communication%20between%20the%20brain%20and%20external%20devices%20through%20electroencephalography%20%28EEG%29%20signals.%20However%2C%20existing%20decoding%20models%20often%20mix%20common%20and%20personalized%20components%2C%20leading%20to%20interference%20from%20individual%20variability%20that%20limits%20cross-subject%20decoding%20performance.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20system%20filter%20that%20extends%20the%20concept%20of%20signal%20filtering%20to%20the%20system%20level.%20The%20method%20expands%20a%20system%20into%20its%20spectral%20representation%2C%20selectively%20removes%20unnecessary%20components%2C%20and%20reconstructs%20the%20system%20from%20the%20retained%20target%20components%2C%20thereby%20achieving%20explicit%20system-level%20decomposition%20and%20filtering.%20We%20further%20integrate%20the%20system%20filter%20into%20a%20Cross-Subject%20Decoding%20framework%20based%20on%20the%20System%20Filter%20%28CSD-SF%29%20and%20evaluate%20it%20on%20the%20four-class%20motor%20imagery%20%28MI%29%20task%20of%20the%20BCIC%20IV%202a%20dataset.%20Personalized%20models%20are%20transformed%20into%20relation%20spectrums%2C%20and%20statistical%20testing%20across%20subjects%20is%20used%20to%20remove%20personalized%20components.%20The%20remaining%20stable%20relations%2C%20representing%20common%20components%20across%20subjects%2C%20are%20then%20used%20to%20construct%20a%20common%20model%20for%20cross-subject%20decoding.%20Experimental%20results%20show%20an%20average%20improvement%20of%203.28%25%20in%20decoding%20accuracy%20over%20baseline%20methods%2C%20demonstrating%20that%20the%20proposed%20system%20filter%20effectively%20isolates%20stable%20common%20components%20and%20enhances%20model%20robustness%20and%20generalizability%20in%20cross-subject%20EEG%20decoding.%0ALink%3A%20http%3A//arxiv.org/abs/2507.05268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystem%2520Filter-Based%2520Common%2520Components%2520Modeling%2520for%2520Cross-Subject%2520EEG%2520Decoding%26entry.906535625%3DXiaoyuan%2520Li%2520and%2520Xinru%2520Xue%2520and%2520Bohan%2520Zhang%2520and%2520Ye%2520Sun%2520and%2520Shoushuo%2520Xi%2520and%2520Gang%2520Liu%26entry.1292438233%3DBrain-computer%2520interface%2520%2528BCI%2529%2520technology%2520enables%2520direct%2520communication%2520between%2520the%2520brain%2520and%2520external%2520devices%2520through%2520electroencephalography%2520%2528EEG%2529%2520signals.%2520However%252C%2520existing%2520decoding%2520models%2520often%2520mix%2520common%2520and%2520personalized%2520components%252C%2520leading%2520to%2520interference%2520from%2520individual%2520variability%2520that%2520limits%2520cross-subject%2520decoding%2520performance.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520system%2520filter%2520that%2520extends%2520the%2520concept%2520of%2520signal%2520filtering%2520to%2520the%2520system%2520level.%2520The%2520method%2520expands%2520a%2520system%2520into%2520its%2520spectral%2520representation%252C%2520selectively%2520removes%2520unnecessary%2520components%252C%2520and%2520reconstructs%2520the%2520system%2520from%2520the%2520retained%2520target%2520components%252C%2520thereby%2520achieving%2520explicit%2520system-level%2520decomposition%2520and%2520filtering.%2520We%2520further%2520integrate%2520the%2520system%2520filter%2520into%2520a%2520Cross-Subject%2520Decoding%2520framework%2520based%2520on%2520the%2520System%2520Filter%2520%2528CSD-SF%2529%2520and%2520evaluate%2520it%2520on%2520the%2520four-class%2520motor%2520imagery%2520%2528MI%2529%2520task%2520of%2520the%2520BCIC%2520IV%25202a%2520dataset.%2520Personalized%2520models%2520are%2520transformed%2520into%2520relation%2520spectrums%252C%2520and%2520statistical%2520testing%2520across%2520subjects%2520is%2520used%2520to%2520remove%2520personalized%2520components.%2520The%2520remaining%2520stable%2520relations%252C%2520representing%2520common%2520components%2520across%2520subjects%252C%2520are%2520then%2520used%2520to%2520construct%2520a%2520common%2520model%2520for%2520cross-subject%2520decoding.%2520Experimental%2520results%2520show%2520an%2520average%2520improvement%2520of%25203.28%2525%2520in%2520decoding%2520accuracy%2520over%2520baseline%2520methods%252C%2520demonstrating%2520that%2520the%2520proposed%2520system%2520filter%2520effectively%2520isolates%2520stable%2520common%2520components%2520and%2520enhances%2520model%2520robustness%2520and%2520generalizability%2520in%2520cross-subject%2520EEG%2520decoding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=System%20Filter-Based%20Common%20Components%20Modeling%20for%20Cross-Subject%20EEG%20Decoding&entry.906535625=Xiaoyuan%20Li%20and%20Xinru%20Xue%20and%20Bohan%20Zhang%20and%20Ye%20Sun%20and%20Shoushuo%20Xi%20and%20Gang%20Liu&entry.1292438233=Brain-computer%20interface%20%28BCI%29%20technology%20enables%20direct%20communication%20between%20the%20brain%20and%20external%20devices%20through%20electroencephalography%20%28EEG%29%20signals.%20However%2C%20existing%20decoding%20models%20often%20mix%20common%20and%20personalized%20components%2C%20leading%20to%20interference%20from%20individual%20variability%20that%20limits%20cross-subject%20decoding%20performance.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20system%20filter%20that%20extends%20the%20concept%20of%20signal%20filtering%20to%20the%20system%20level.%20The%20method%20expands%20a%20system%20into%20its%20spectral%20representation%2C%20selectively%20removes%20unnecessary%20components%2C%20and%20reconstructs%20the%20system%20from%20the%20retained%20target%20components%2C%20thereby%20achieving%20explicit%20system-level%20decomposition%20and%20filtering.%20We%20further%20integrate%20the%20system%20filter%20into%20a%20Cross-Subject%20Decoding%20framework%20based%20on%20the%20System%20Filter%20%28CSD-SF%29%20and%20evaluate%20it%20on%20the%20four-class%20motor%20imagery%20%28MI%29%20task%20of%20the%20BCIC%20IV%202a%20dataset.%20Personalized%20models%20are%20transformed%20into%20relation%20spectrums%2C%20and%20statistical%20testing%20across%20subjects%20is%20used%20to%20remove%20personalized%20components.%20The%20remaining%20stable%20relations%2C%20representing%20common%20components%20across%20subjects%2C%20are%20then%20used%20to%20construct%20a%20common%20model%20for%20cross-subject%20decoding.%20Experimental%20results%20show%20an%20average%20improvement%20of%203.28%25%20in%20decoding%20accuracy%20over%20baseline%20methods%2C%20demonstrating%20that%20the%20proposed%20system%20filter%20effectively%20isolates%20stable%20common%20components%20and%20enhances%20model%20robustness%20and%20generalizability%20in%20cross-subject%20EEG%20decoding.&entry.1838667208=http%3A//arxiv.org/abs/2507.05268v2&entry.124074799=Read"},
{"title": "CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement", "author": "Pan Yang and Cheng Deng and Jing Yang and Han Zhao and Yun Liu and Yuling Chen and Xiaoli Ruan and Yanping Chen", "abstract": "Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.", "link": "http://arxiv.org/abs/2511.16378v1", "date": "2025-11-20", "relevancy": 2.3644, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6385}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5653}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMS%3A%20Towards%20Compositional%20Zero-Shot%20Learning%20via%20Gated%20Cross-Attention%20and%20Multi-Space%20Disentanglement&body=Title%3A%20CAMS%3A%20Towards%20Compositional%20Zero-Shot%20Learning%20via%20Gated%20Cross-Attention%20and%20Multi-Space%20Disentanglement%0AAuthor%3A%20Pan%20Yang%20and%20Cheng%20Deng%20and%20Jing%20Yang%20and%20Han%20Zhao%20and%20Yun%20Liu%20and%20Yuling%20Chen%20and%20Xiaoli%20Ruan%20and%20Yanping%20Chen%0AAbstract%3A%20Compositional%20zero-shot%20learning%20%28CZSL%29%20aims%20to%20learn%20the%20concepts%20of%20attributes%20and%20objects%20in%20seen%20compositions%20and%20to%20recognize%20their%20unseen%20compositions.%20Most%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29-based%20CZSL%20methods%20focus%20on%20disentangling%20attributes%20and%20objects%20by%20leveraging%20the%20global%20semantic%20representation%20obtained%20from%20the%20image%20encoder.%20However%2C%20this%20representation%20has%20limited%20representational%20capacity%20and%20do%20not%20allow%20for%20complete%20disentanglement%20of%20the%20two.%20To%20this%20end%2C%20we%20propose%20CAMS%2C%20which%20aims%20to%20extract%20semantic%20features%20from%20visual%20features%20and%20perform%20semantic%20disentanglement%20in%20multidimensional%20spaces%2C%20thereby%20improving%20generalization%20over%20unseen%20attribute-object%20compositions.%20Specifically%2C%20CAMS%20designs%20a%20Gated%20Cross-Attention%20that%20captures%20fine-grained%20semantic%20features%20from%20the%20high-level%20image%20encoding%20blocks%20of%20CLIP%20through%20a%20set%20of%20latent%20units%2C%20while%20adaptively%20suppressing%20background%20and%20other%20irrelevant%20information.%20Subsequently%2C%20it%20conducts%20Multi-Space%20Disentanglement%20to%20achieve%20disentanglement%20of%20attribute%20and%20object%20semantics.%20Experiments%20on%20three%20popular%20benchmarks%20%28MIT-States%2C%20UT-Zappos%2C%20and%20C-GQA%29%20demonstrate%20that%20CAMS%20achieves%20state-of-the-art%20performance%20in%20both%20closed-world%20and%20open-world%20settings.%20The%20code%20is%20available%20at%20https%3A//github.com/ybyangjing/CAMS.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMS%253A%2520Towards%2520Compositional%2520Zero-Shot%2520Learning%2520via%2520Gated%2520Cross-Attention%2520and%2520Multi-Space%2520Disentanglement%26entry.906535625%3DPan%2520Yang%2520and%2520Cheng%2520Deng%2520and%2520Jing%2520Yang%2520and%2520Han%2520Zhao%2520and%2520Yun%2520Liu%2520and%2520Yuling%2520Chen%2520and%2520Xiaoli%2520Ruan%2520and%2520Yanping%2520Chen%26entry.1292438233%3DCompositional%2520zero-shot%2520learning%2520%2528CZSL%2529%2520aims%2520to%2520learn%2520the%2520concepts%2520of%2520attributes%2520and%2520objects%2520in%2520seen%2520compositions%2520and%2520to%2520recognize%2520their%2520unseen%2520compositions.%2520Most%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529-based%2520CZSL%2520methods%2520focus%2520on%2520disentangling%2520attributes%2520and%2520objects%2520by%2520leveraging%2520the%2520global%2520semantic%2520representation%2520obtained%2520from%2520the%2520image%2520encoder.%2520However%252C%2520this%2520representation%2520has%2520limited%2520representational%2520capacity%2520and%2520do%2520not%2520allow%2520for%2520complete%2520disentanglement%2520of%2520the%2520two.%2520To%2520this%2520end%252C%2520we%2520propose%2520CAMS%252C%2520which%2520aims%2520to%2520extract%2520semantic%2520features%2520from%2520visual%2520features%2520and%2520perform%2520semantic%2520disentanglement%2520in%2520multidimensional%2520spaces%252C%2520thereby%2520improving%2520generalization%2520over%2520unseen%2520attribute-object%2520compositions.%2520Specifically%252C%2520CAMS%2520designs%2520a%2520Gated%2520Cross-Attention%2520that%2520captures%2520fine-grained%2520semantic%2520features%2520from%2520the%2520high-level%2520image%2520encoding%2520blocks%2520of%2520CLIP%2520through%2520a%2520set%2520of%2520latent%2520units%252C%2520while%2520adaptively%2520suppressing%2520background%2520and%2520other%2520irrelevant%2520information.%2520Subsequently%252C%2520it%2520conducts%2520Multi-Space%2520Disentanglement%2520to%2520achieve%2520disentanglement%2520of%2520attribute%2520and%2520object%2520semantics.%2520Experiments%2520on%2520three%2520popular%2520benchmarks%2520%2528MIT-States%252C%2520UT-Zappos%252C%2520and%2520C-GQA%2529%2520demonstrate%2520that%2520CAMS%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520closed-world%2520and%2520open-world%2520settings.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/ybyangjing/CAMS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMS%3A%20Towards%20Compositional%20Zero-Shot%20Learning%20via%20Gated%20Cross-Attention%20and%20Multi-Space%20Disentanglement&entry.906535625=Pan%20Yang%20and%20Cheng%20Deng%20and%20Jing%20Yang%20and%20Han%20Zhao%20and%20Yun%20Liu%20and%20Yuling%20Chen%20and%20Xiaoli%20Ruan%20and%20Yanping%20Chen&entry.1292438233=Compositional%20zero-shot%20learning%20%28CZSL%29%20aims%20to%20learn%20the%20concepts%20of%20attributes%20and%20objects%20in%20seen%20compositions%20and%20to%20recognize%20their%20unseen%20compositions.%20Most%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29-based%20CZSL%20methods%20focus%20on%20disentangling%20attributes%20and%20objects%20by%20leveraging%20the%20global%20semantic%20representation%20obtained%20from%20the%20image%20encoder.%20However%2C%20this%20representation%20has%20limited%20representational%20capacity%20and%20do%20not%20allow%20for%20complete%20disentanglement%20of%20the%20two.%20To%20this%20end%2C%20we%20propose%20CAMS%2C%20which%20aims%20to%20extract%20semantic%20features%20from%20visual%20features%20and%20perform%20semantic%20disentanglement%20in%20multidimensional%20spaces%2C%20thereby%20improving%20generalization%20over%20unseen%20attribute-object%20compositions.%20Specifically%2C%20CAMS%20designs%20a%20Gated%20Cross-Attention%20that%20captures%20fine-grained%20semantic%20features%20from%20the%20high-level%20image%20encoding%20blocks%20of%20CLIP%20through%20a%20set%20of%20latent%20units%2C%20while%20adaptively%20suppressing%20background%20and%20other%20irrelevant%20information.%20Subsequently%2C%20it%20conducts%20Multi-Space%20Disentanglement%20to%20achieve%20disentanglement%20of%20attribute%20and%20object%20semantics.%20Experiments%20on%20three%20popular%20benchmarks%20%28MIT-States%2C%20UT-Zappos%2C%20and%20C-GQA%29%20demonstrate%20that%20CAMS%20achieves%20state-of-the-art%20performance%20in%20both%20closed-world%20and%20open-world%20settings.%20The%20code%20is%20available%20at%20https%3A//github.com/ybyangjing/CAMS.&entry.1838667208=http%3A//arxiv.org/abs/2511.16378v1&entry.124074799=Read"},
{"title": "Spectral Identifiability for Interpretable Probe Geometry", "author": "William Hao-Cheng Huang", "abstract": "Linear probes are widely used to interpret and evaluate neural representations, yet their reliability remains unclear, as probes may appear accurate in some regimes but collapse unpredictably in others. We uncover a spectral mechanism behind this phenomenon and formalize it as the Spectral Identifiability Principle (SIP), a verifiable Fisher-inspired condition for probe stability. When the eigengap separating task-relevant directions is larger than the Fisher estimation error, the estimated subspace concentrates and accuracy remains consistent, whereas closing this gap induces instability in a phase-transition manner. Our analysis connects eigengap geometry, sample size, and misclassification risk through finite-sample reasoning, providing an interpretable diagnostic rather than a loose generalization bound. Controlled synthetic studies, where Fisher quantities are computed exactly, confirm these predictions and show how spectral inspection can anticipate unreliable probes before they distort downstream evaluation.", "link": "http://arxiv.org/abs/2511.16288v1", "date": "2025-11-20", "relevancy": 2.3594, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Identifiability%20for%20Interpretable%20Probe%20Geometry&body=Title%3A%20Spectral%20Identifiability%20for%20Interpretable%20Probe%20Geometry%0AAuthor%3A%20William%20Hao-Cheng%20Huang%0AAbstract%3A%20Linear%20probes%20are%20widely%20used%20to%20interpret%20and%20evaluate%20neural%20representations%2C%20yet%20their%20reliability%20remains%20unclear%2C%20as%20probes%20may%20appear%20accurate%20in%20some%20regimes%20but%20collapse%20unpredictably%20in%20others.%20We%20uncover%20a%20spectral%20mechanism%20behind%20this%20phenomenon%20and%20formalize%20it%20as%20the%20Spectral%20Identifiability%20Principle%20%28SIP%29%2C%20a%20verifiable%20Fisher-inspired%20condition%20for%20probe%20stability.%20When%20the%20eigengap%20separating%20task-relevant%20directions%20is%20larger%20than%20the%20Fisher%20estimation%20error%2C%20the%20estimated%20subspace%20concentrates%20and%20accuracy%20remains%20consistent%2C%20whereas%20closing%20this%20gap%20induces%20instability%20in%20a%20phase-transition%20manner.%20Our%20analysis%20connects%20eigengap%20geometry%2C%20sample%20size%2C%20and%20misclassification%20risk%20through%20finite-sample%20reasoning%2C%20providing%20an%20interpretable%20diagnostic%20rather%20than%20a%20loose%20generalization%20bound.%20Controlled%20synthetic%20studies%2C%20where%20Fisher%20quantities%20are%20computed%20exactly%2C%20confirm%20these%20predictions%20and%20show%20how%20spectral%20inspection%20can%20anticipate%20unreliable%20probes%20before%20they%20distort%20downstream%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Identifiability%2520for%2520Interpretable%2520Probe%2520Geometry%26entry.906535625%3DWilliam%2520Hao-Cheng%2520Huang%26entry.1292438233%3DLinear%2520probes%2520are%2520widely%2520used%2520to%2520interpret%2520and%2520evaluate%2520neural%2520representations%252C%2520yet%2520their%2520reliability%2520remains%2520unclear%252C%2520as%2520probes%2520may%2520appear%2520accurate%2520in%2520some%2520regimes%2520but%2520collapse%2520unpredictably%2520in%2520others.%2520We%2520uncover%2520a%2520spectral%2520mechanism%2520behind%2520this%2520phenomenon%2520and%2520formalize%2520it%2520as%2520the%2520Spectral%2520Identifiability%2520Principle%2520%2528SIP%2529%252C%2520a%2520verifiable%2520Fisher-inspired%2520condition%2520for%2520probe%2520stability.%2520When%2520the%2520eigengap%2520separating%2520task-relevant%2520directions%2520is%2520larger%2520than%2520the%2520Fisher%2520estimation%2520error%252C%2520the%2520estimated%2520subspace%2520concentrates%2520and%2520accuracy%2520remains%2520consistent%252C%2520whereas%2520closing%2520this%2520gap%2520induces%2520instability%2520in%2520a%2520phase-transition%2520manner.%2520Our%2520analysis%2520connects%2520eigengap%2520geometry%252C%2520sample%2520size%252C%2520and%2520misclassification%2520risk%2520through%2520finite-sample%2520reasoning%252C%2520providing%2520an%2520interpretable%2520diagnostic%2520rather%2520than%2520a%2520loose%2520generalization%2520bound.%2520Controlled%2520synthetic%2520studies%252C%2520where%2520Fisher%2520quantities%2520are%2520computed%2520exactly%252C%2520confirm%2520these%2520predictions%2520and%2520show%2520how%2520spectral%2520inspection%2520can%2520anticipate%2520unreliable%2520probes%2520before%2520they%2520distort%2520downstream%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Identifiability%20for%20Interpretable%20Probe%20Geometry&entry.906535625=William%20Hao-Cheng%20Huang&entry.1292438233=Linear%20probes%20are%20widely%20used%20to%20interpret%20and%20evaluate%20neural%20representations%2C%20yet%20their%20reliability%20remains%20unclear%2C%20as%20probes%20may%20appear%20accurate%20in%20some%20regimes%20but%20collapse%20unpredictably%20in%20others.%20We%20uncover%20a%20spectral%20mechanism%20behind%20this%20phenomenon%20and%20formalize%20it%20as%20the%20Spectral%20Identifiability%20Principle%20%28SIP%29%2C%20a%20verifiable%20Fisher-inspired%20condition%20for%20probe%20stability.%20When%20the%20eigengap%20separating%20task-relevant%20directions%20is%20larger%20than%20the%20Fisher%20estimation%20error%2C%20the%20estimated%20subspace%20concentrates%20and%20accuracy%20remains%20consistent%2C%20whereas%20closing%20this%20gap%20induces%20instability%20in%20a%20phase-transition%20manner.%20Our%20analysis%20connects%20eigengap%20geometry%2C%20sample%20size%2C%20and%20misclassification%20risk%20through%20finite-sample%20reasoning%2C%20providing%20an%20interpretable%20diagnostic%20rather%20than%20a%20loose%20generalization%20bound.%20Controlled%20synthetic%20studies%2C%20where%20Fisher%20quantities%20are%20computed%20exactly%2C%20confirm%20these%20predictions%20and%20show%20how%20spectral%20inspection%20can%20anticipate%20unreliable%20probes%20before%20they%20distort%20downstream%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2511.16288v1&entry.124074799=Read"},
{"title": "The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation", "author": "Jiaheng Zhang and Daqiang Zhang", "abstract": "The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.\n  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.\n  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.", "link": "http://arxiv.org/abs/2511.16543v1", "date": "2025-11-20", "relevancy": 2.3585, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4845}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Oracle%20and%20The%20Prism%3A%20A%20Decoupled%20and%20Efficient%20Framework%20for%20Generative%20Recommendation%20Explanation&body=Title%3A%20The%20Oracle%20and%20The%20Prism%3A%20A%20Decoupled%20and%20Efficient%20Framework%20for%20Generative%20Recommendation%20Explanation%0AAuthor%3A%20Jiaheng%20Zhang%20and%20Daqiang%20Zhang%0AAbstract%3A%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20explainable%20recommendation%20systems%20often%20leads%20to%20a%20performance-efficiency%20trade-off%20in%20end-to-end%20architectures%2C%20where%20joint%20optimization%20of%20ranking%20and%20explanation%20can%20result%20in%20suboptimal%20compromises.%20To%20resolve%20this%2C%20we%20propose%20Prism%2C%20a%20novel%20decoupled%20framework%20that%20rigorously%20separates%20the%20recommendation%20process%20into%20a%20dedicated%20ranking%20stage%20and%20an%20explanation%20generation%20stage.%0A%20%20Inspired%20by%20knowledge%20distillation%2C%20Prism%20leverages%20a%20powerful%20teacher%20LLM%20%28e.g.%2C%20FLAN-T5-XXL%29%20as%20an%20Oracle%20to%20produce%20high-fidelity%20explanatory%20knowledge.%20A%20compact%2C%20fine-tuned%20student%20model%20%28e.g.%2C%20BART-Base%29%2C%20the%20Prism%2C%20then%20specializes%20in%20synthesizing%20this%20knowledge%20into%20personalized%20explanations.%20This%20decomposition%20ensures%20that%20each%20component%20is%20optimized%20for%20its%20specific%20objective%2C%20eliminating%20inherent%20conflicts%20in%20coupled%20models.%0A%20%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20our%20140M-parameter%20Prism%20model%20significantly%20outperforms%20its%2011B-parameter%20teacher%20in%20human%20evaluations%20of%20faithfulness%20and%20personalization%2C%20while%20achieving%20a%2024%20times%20speedup%20and%20a%2010%20times%20reduction%20in%20memory%20consumption%20during%20inference.%20These%20results%20validate%20that%20decoupling%2C%20coupled%20with%20targeted%20distillation%2C%20provides%20an%20efficient%20and%20effective%20pathway%20to%20high-quality%20explainable%20recommendation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Oracle%2520and%2520The%2520Prism%253A%2520A%2520Decoupled%2520and%2520Efficient%2520Framework%2520for%2520Generative%2520Recommendation%2520Explanation%26entry.906535625%3DJiaheng%2520Zhang%2520and%2520Daqiang%2520Zhang%26entry.1292438233%3DThe%2520integration%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520into%2520explainable%2520recommendation%2520systems%2520often%2520leads%2520to%2520a%2520performance-efficiency%2520trade-off%2520in%2520end-to-end%2520architectures%252C%2520where%2520joint%2520optimization%2520of%2520ranking%2520and%2520explanation%2520can%2520result%2520in%2520suboptimal%2520compromises.%2520To%2520resolve%2520this%252C%2520we%2520propose%2520Prism%252C%2520a%2520novel%2520decoupled%2520framework%2520that%2520rigorously%2520separates%2520the%2520recommendation%2520process%2520into%2520a%2520dedicated%2520ranking%2520stage%2520and%2520an%2520explanation%2520generation%2520stage.%250A%2520%2520Inspired%2520by%2520knowledge%2520distillation%252C%2520Prism%2520leverages%2520a%2520powerful%2520teacher%2520LLM%2520%2528e.g.%252C%2520FLAN-T5-XXL%2529%2520as%2520an%2520Oracle%2520to%2520produce%2520high-fidelity%2520explanatory%2520knowledge.%2520A%2520compact%252C%2520fine-tuned%2520student%2520model%2520%2528e.g.%252C%2520BART-Base%2529%252C%2520the%2520Prism%252C%2520then%2520specializes%2520in%2520synthesizing%2520this%2520knowledge%2520into%2520personalized%2520explanations.%2520This%2520decomposition%2520ensures%2520that%2520each%2520component%2520is%2520optimized%2520for%2520its%2520specific%2520objective%252C%2520eliminating%2520inherent%2520conflicts%2520in%2520coupled%2520models.%250A%2520%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520140M-parameter%2520Prism%2520model%2520significantly%2520outperforms%2520its%252011B-parameter%2520teacher%2520in%2520human%2520evaluations%2520of%2520faithfulness%2520and%2520personalization%252C%2520while%2520achieving%2520a%252024%2520times%2520speedup%2520and%2520a%252010%2520times%2520reduction%2520in%2520memory%2520consumption%2520during%2520inference.%2520These%2520results%2520validate%2520that%2520decoupling%252C%2520coupled%2520with%2520targeted%2520distillation%252C%2520provides%2520an%2520efficient%2520and%2520effective%2520pathway%2520to%2520high-quality%2520explainable%2520recommendation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Oracle%20and%20The%20Prism%3A%20A%20Decoupled%20and%20Efficient%20Framework%20for%20Generative%20Recommendation%20Explanation&entry.906535625=Jiaheng%20Zhang%20and%20Daqiang%20Zhang&entry.1292438233=The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20explainable%20recommendation%20systems%20often%20leads%20to%20a%20performance-efficiency%20trade-off%20in%20end-to-end%20architectures%2C%20where%20joint%20optimization%20of%20ranking%20and%20explanation%20can%20result%20in%20suboptimal%20compromises.%20To%20resolve%20this%2C%20we%20propose%20Prism%2C%20a%20novel%20decoupled%20framework%20that%20rigorously%20separates%20the%20recommendation%20process%20into%20a%20dedicated%20ranking%20stage%20and%20an%20explanation%20generation%20stage.%0A%20%20Inspired%20by%20knowledge%20distillation%2C%20Prism%20leverages%20a%20powerful%20teacher%20LLM%20%28e.g.%2C%20FLAN-T5-XXL%29%20as%20an%20Oracle%20to%20produce%20high-fidelity%20explanatory%20knowledge.%20A%20compact%2C%20fine-tuned%20student%20model%20%28e.g.%2C%20BART-Base%29%2C%20the%20Prism%2C%20then%20specializes%20in%20synthesizing%20this%20knowledge%20into%20personalized%20explanations.%20This%20decomposition%20ensures%20that%20each%20component%20is%20optimized%20for%20its%20specific%20objective%2C%20eliminating%20inherent%20conflicts%20in%20coupled%20models.%0A%20%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20our%20140M-parameter%20Prism%20model%20significantly%20outperforms%20its%2011B-parameter%20teacher%20in%20human%20evaluations%20of%20faithfulness%20and%20personalization%2C%20while%20achieving%20a%2024%20times%20speedup%20and%20a%2010%20times%20reduction%20in%20memory%20consumption%20during%20inference.%20These%20results%20validate%20that%20decoupling%2C%20coupled%20with%20targeted%20distillation%2C%20provides%20an%20efficient%20and%20effective%20pathway%20to%20high-quality%20explainable%20recommendation.&entry.1838667208=http%3A//arxiv.org/abs/2511.16543v1&entry.124074799=Read"},
{"title": "When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?", "author": "Romy M\u00fcller", "abstract": "Concept-based explainable artificial intelligence (C-XAI) can let people see which representations an AI model has learned. This is particularly important when high-level semantic information (e.g., actions and relations) is used to make decisions about abstract categories (e.g., danger). In such tasks, AI models need to generalise beyond situation-specific details, and this ability can be reflected in C-XAI outputs that randomise over irrelevant features. However, it is unclear whether people appreciate such generalisation and can distinguish it from other, less desirable forms of imprecision in C-XAI outputs. Therefore, the present study investigated how the generality and relevance of C-XAI outputs affect people's evaluation of AI. In an experimental railway safety evaluation scenario, participants rated the performance of a simulated AI that classified traffic scenes involving people as dangerous or not. These classification decisions were explained via concepts in the form of similar image snippets. The latter differed in their match with the classified image, either regarding a highly relevant feature (i.e., people's relation to tracks) or a less relevant feature (i.e., people's action). Contrary to the hypotheses, concepts that generalised over less relevant features were rated lower than concepts that matched the classified image precisely. Moreover, their ratings were no better than those for systematic misrepresentations of the less relevant feature. Conversely, participants were highly sensitive to imprecisions in relevant features. These findings cast doubts on the assumption that people can easily infer from C-XAI outputs whether AI models have gained a deeper understanding of complex situations.", "link": "http://arxiv.org/abs/2506.17936v2", "date": "2025-11-20", "relevancy": 2.3566, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4839}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20concept-based%20XAI%20is%20imprecise%3A%20Do%20people%20distinguish%20between%20generalisations%20and%20misrepresentations%3F&body=Title%3A%20When%20concept-based%20XAI%20is%20imprecise%3A%20Do%20people%20distinguish%20between%20generalisations%20and%20misrepresentations%3F%0AAuthor%3A%20Romy%20M%C3%BCller%0AAbstract%3A%20Concept-based%20explainable%20artificial%20intelligence%20%28C-XAI%29%20can%20let%20people%20see%20which%20representations%20an%20AI%20model%20has%20learned.%20This%20is%20particularly%20important%20when%20high-level%20semantic%20information%20%28e.g.%2C%20actions%20and%20relations%29%20is%20used%20to%20make%20decisions%20about%20abstract%20categories%20%28e.g.%2C%20danger%29.%20In%20such%20tasks%2C%20AI%20models%20need%20to%20generalise%20beyond%20situation-specific%20details%2C%20and%20this%20ability%20can%20be%20reflected%20in%20C-XAI%20outputs%20that%20randomise%20over%20irrelevant%20features.%20However%2C%20it%20is%20unclear%20whether%20people%20appreciate%20such%20generalisation%20and%20can%20distinguish%20it%20from%20other%2C%20less%20desirable%20forms%20of%20imprecision%20in%20C-XAI%20outputs.%20Therefore%2C%20the%20present%20study%20investigated%20how%20the%20generality%20and%20relevance%20of%20C-XAI%20outputs%20affect%20people%27s%20evaluation%20of%20AI.%20In%20an%20experimental%20railway%20safety%20evaluation%20scenario%2C%20participants%20rated%20the%20performance%20of%20a%20simulated%20AI%20that%20classified%20traffic%20scenes%20involving%20people%20as%20dangerous%20or%20not.%20These%20classification%20decisions%20were%20explained%20via%20concepts%20in%20the%20form%20of%20similar%20image%20snippets.%20The%20latter%20differed%20in%20their%20match%20with%20the%20classified%20image%2C%20either%20regarding%20a%20highly%20relevant%20feature%20%28i.e.%2C%20people%27s%20relation%20to%20tracks%29%20or%20a%20less%20relevant%20feature%20%28i.e.%2C%20people%27s%20action%29.%20Contrary%20to%20the%20hypotheses%2C%20concepts%20that%20generalised%20over%20less%20relevant%20features%20were%20rated%20lower%20than%20concepts%20that%20matched%20the%20classified%20image%20precisely.%20Moreover%2C%20their%20ratings%20were%20no%20better%20than%20those%20for%20systematic%20misrepresentations%20of%20the%20less%20relevant%20feature.%20Conversely%2C%20participants%20were%20highly%20sensitive%20to%20imprecisions%20in%20relevant%20features.%20These%20findings%20cast%20doubts%20on%20the%20assumption%20that%20people%20can%20easily%20infer%20from%20C-XAI%20outputs%20whether%20AI%20models%20have%20gained%20a%20deeper%20understanding%20of%20complex%20situations.%0ALink%3A%20http%3A//arxiv.org/abs/2506.17936v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520concept-based%2520XAI%2520is%2520imprecise%253A%2520Do%2520people%2520distinguish%2520between%2520generalisations%2520and%2520misrepresentations%253F%26entry.906535625%3DRomy%2520M%25C3%25BCller%26entry.1292438233%3DConcept-based%2520explainable%2520artificial%2520intelligence%2520%2528C-XAI%2529%2520can%2520let%2520people%2520see%2520which%2520representations%2520an%2520AI%2520model%2520has%2520learned.%2520This%2520is%2520particularly%2520important%2520when%2520high-level%2520semantic%2520information%2520%2528e.g.%252C%2520actions%2520and%2520relations%2529%2520is%2520used%2520to%2520make%2520decisions%2520about%2520abstract%2520categories%2520%2528e.g.%252C%2520danger%2529.%2520In%2520such%2520tasks%252C%2520AI%2520models%2520need%2520to%2520generalise%2520beyond%2520situation-specific%2520details%252C%2520and%2520this%2520ability%2520can%2520be%2520reflected%2520in%2520C-XAI%2520outputs%2520that%2520randomise%2520over%2520irrelevant%2520features.%2520However%252C%2520it%2520is%2520unclear%2520whether%2520people%2520appreciate%2520such%2520generalisation%2520and%2520can%2520distinguish%2520it%2520from%2520other%252C%2520less%2520desirable%2520forms%2520of%2520imprecision%2520in%2520C-XAI%2520outputs.%2520Therefore%252C%2520the%2520present%2520study%2520investigated%2520how%2520the%2520generality%2520and%2520relevance%2520of%2520C-XAI%2520outputs%2520affect%2520people%2527s%2520evaluation%2520of%2520AI.%2520In%2520an%2520experimental%2520railway%2520safety%2520evaluation%2520scenario%252C%2520participants%2520rated%2520the%2520performance%2520of%2520a%2520simulated%2520AI%2520that%2520classified%2520traffic%2520scenes%2520involving%2520people%2520as%2520dangerous%2520or%2520not.%2520These%2520classification%2520decisions%2520were%2520explained%2520via%2520concepts%2520in%2520the%2520form%2520of%2520similar%2520image%2520snippets.%2520The%2520latter%2520differed%2520in%2520their%2520match%2520with%2520the%2520classified%2520image%252C%2520either%2520regarding%2520a%2520highly%2520relevant%2520feature%2520%2528i.e.%252C%2520people%2527s%2520relation%2520to%2520tracks%2529%2520or%2520a%2520less%2520relevant%2520feature%2520%2528i.e.%252C%2520people%2527s%2520action%2529.%2520Contrary%2520to%2520the%2520hypotheses%252C%2520concepts%2520that%2520generalised%2520over%2520less%2520relevant%2520features%2520were%2520rated%2520lower%2520than%2520concepts%2520that%2520matched%2520the%2520classified%2520image%2520precisely.%2520Moreover%252C%2520their%2520ratings%2520were%2520no%2520better%2520than%2520those%2520for%2520systematic%2520misrepresentations%2520of%2520the%2520less%2520relevant%2520feature.%2520Conversely%252C%2520participants%2520were%2520highly%2520sensitive%2520to%2520imprecisions%2520in%2520relevant%2520features.%2520These%2520findings%2520cast%2520doubts%2520on%2520the%2520assumption%2520that%2520people%2520can%2520easily%2520infer%2520from%2520C-XAI%2520outputs%2520whether%2520AI%2520models%2520have%2520gained%2520a%2520deeper%2520understanding%2520of%2520complex%2520situations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.17936v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20concept-based%20XAI%20is%20imprecise%3A%20Do%20people%20distinguish%20between%20generalisations%20and%20misrepresentations%3F&entry.906535625=Romy%20M%C3%BCller&entry.1292438233=Concept-based%20explainable%20artificial%20intelligence%20%28C-XAI%29%20can%20let%20people%20see%20which%20representations%20an%20AI%20model%20has%20learned.%20This%20is%20particularly%20important%20when%20high-level%20semantic%20information%20%28e.g.%2C%20actions%20and%20relations%29%20is%20used%20to%20make%20decisions%20about%20abstract%20categories%20%28e.g.%2C%20danger%29.%20In%20such%20tasks%2C%20AI%20models%20need%20to%20generalise%20beyond%20situation-specific%20details%2C%20and%20this%20ability%20can%20be%20reflected%20in%20C-XAI%20outputs%20that%20randomise%20over%20irrelevant%20features.%20However%2C%20it%20is%20unclear%20whether%20people%20appreciate%20such%20generalisation%20and%20can%20distinguish%20it%20from%20other%2C%20less%20desirable%20forms%20of%20imprecision%20in%20C-XAI%20outputs.%20Therefore%2C%20the%20present%20study%20investigated%20how%20the%20generality%20and%20relevance%20of%20C-XAI%20outputs%20affect%20people%27s%20evaluation%20of%20AI.%20In%20an%20experimental%20railway%20safety%20evaluation%20scenario%2C%20participants%20rated%20the%20performance%20of%20a%20simulated%20AI%20that%20classified%20traffic%20scenes%20involving%20people%20as%20dangerous%20or%20not.%20These%20classification%20decisions%20were%20explained%20via%20concepts%20in%20the%20form%20of%20similar%20image%20snippets.%20The%20latter%20differed%20in%20their%20match%20with%20the%20classified%20image%2C%20either%20regarding%20a%20highly%20relevant%20feature%20%28i.e.%2C%20people%27s%20relation%20to%20tracks%29%20or%20a%20less%20relevant%20feature%20%28i.e.%2C%20people%27s%20action%29.%20Contrary%20to%20the%20hypotheses%2C%20concepts%20that%20generalised%20over%20less%20relevant%20features%20were%20rated%20lower%20than%20concepts%20that%20matched%20the%20classified%20image%20precisely.%20Moreover%2C%20their%20ratings%20were%20no%20better%20than%20those%20for%20systematic%20misrepresentations%20of%20the%20less%20relevant%20feature.%20Conversely%2C%20participants%20were%20highly%20sensitive%20to%20imprecisions%20in%20relevant%20features.%20These%20findings%20cast%20doubts%20on%20the%20assumption%20that%20people%20can%20easily%20infer%20from%20C-XAI%20outputs%20whether%20AI%20models%20have%20gained%20a%20deeper%20understanding%20of%20complex%20situations.&entry.1838667208=http%3A//arxiv.org/abs/2506.17936v2&entry.124074799=Read"},
{"title": "Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks", "author": "\u00c9lo\u00efse Benito-Rodriguez and Einar Urdshals and Jasmina Nasufi and Nicky Pochinkov", "abstract": "Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.", "link": "http://arxiv.org/abs/2511.16540v1", "date": "2025-11-20", "relevancy": 2.3468, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Tokens%20in%20Language%20Models%3A%20Interpreting%20Activations%20through%20Text%20Genre%20Chunks&body=Title%3A%20Beyond%20Tokens%20in%20Language%20Models%3A%20Interpreting%20Activations%20through%20Text%20Genre%20Chunks%0AAuthor%3A%20%C3%89lo%C3%AFse%20Benito-Rodriguez%20and%20Einar%20Urdshals%20and%20Jasmina%20Nasufi%20and%20Nicky%20Pochinkov%0AAbstract%3A%20Understanding%20Large%20Language%20Models%20%28LLMs%29%20is%20key%20to%20ensure%20their%20safe%20and%20beneficial%20deployment.%20This%20task%20is%20complicated%20by%20the%20difficulty%20of%20interpretability%20of%20LLM%20structures%2C%20and%20the%20inability%20to%20have%20all%20their%20outputs%20human-evaluated.%20In%20this%20paper%2C%20we%20present%20the%20first%20step%20towards%20a%20predictive%20framework%2C%20where%20the%20genre%20of%20a%20text%20used%20to%20prompt%20an%20LLM%2C%20is%20predicted%20based%20on%20its%20activations.%20Using%20Mistral-7B%20and%20two%20datasets%2C%20we%20show%20that%20genre%20can%20be%20extracted%20with%20F1-scores%20of%20up%20to%2098%25%20and%2071%25%20using%20scikit-learn%20classifiers.%20Across%20both%20datasets%2C%20results%20consistently%20outperform%20the%20control%20task%2C%20providing%20a%20proof%20of%20concept%20that%20text%20genres%20can%20be%20inferred%20from%20LLMs%20with%20shallow%20learning%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Tokens%2520in%2520Language%2520Models%253A%2520Interpreting%2520Activations%2520through%2520Text%2520Genre%2520Chunks%26entry.906535625%3D%25C3%2589lo%25C3%25AFse%2520Benito-Rodriguez%2520and%2520Einar%2520Urdshals%2520and%2520Jasmina%2520Nasufi%2520and%2520Nicky%2520Pochinkov%26entry.1292438233%3DUnderstanding%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520key%2520to%2520ensure%2520their%2520safe%2520and%2520beneficial%2520deployment.%2520This%2520task%2520is%2520complicated%2520by%2520the%2520difficulty%2520of%2520interpretability%2520of%2520LLM%2520structures%252C%2520and%2520the%2520inability%2520to%2520have%2520all%2520their%2520outputs%2520human-evaluated.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520step%2520towards%2520a%2520predictive%2520framework%252C%2520where%2520the%2520genre%2520of%2520a%2520text%2520used%2520to%2520prompt%2520an%2520LLM%252C%2520is%2520predicted%2520based%2520on%2520its%2520activations.%2520Using%2520Mistral-7B%2520and%2520two%2520datasets%252C%2520we%2520show%2520that%2520genre%2520can%2520be%2520extracted%2520with%2520F1-scores%2520of%2520up%2520to%252098%2525%2520and%252071%2525%2520using%2520scikit-learn%2520classifiers.%2520Across%2520both%2520datasets%252C%2520results%2520consistently%2520outperform%2520the%2520control%2520task%252C%2520providing%2520a%2520proof%2520of%2520concept%2520that%2520text%2520genres%2520can%2520be%2520inferred%2520from%2520LLMs%2520with%2520shallow%2520learning%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Tokens%20in%20Language%20Models%3A%20Interpreting%20Activations%20through%20Text%20Genre%20Chunks&entry.906535625=%C3%89lo%C3%AFse%20Benito-Rodriguez%20and%20Einar%20Urdshals%20and%20Jasmina%20Nasufi%20and%20Nicky%20Pochinkov&entry.1292438233=Understanding%20Large%20Language%20Models%20%28LLMs%29%20is%20key%20to%20ensure%20their%20safe%20and%20beneficial%20deployment.%20This%20task%20is%20complicated%20by%20the%20difficulty%20of%20interpretability%20of%20LLM%20structures%2C%20and%20the%20inability%20to%20have%20all%20their%20outputs%20human-evaluated.%20In%20this%20paper%2C%20we%20present%20the%20first%20step%20towards%20a%20predictive%20framework%2C%20where%20the%20genre%20of%20a%20text%20used%20to%20prompt%20an%20LLM%2C%20is%20predicted%20based%20on%20its%20activations.%20Using%20Mistral-7B%20and%20two%20datasets%2C%20we%20show%20that%20genre%20can%20be%20extracted%20with%20F1-scores%20of%20up%20to%2098%25%20and%2071%25%20using%20scikit-learn%20classifiers.%20Across%20both%20datasets%2C%20results%20consistently%20outperform%20the%20control%20task%2C%20providing%20a%20proof%20of%20concept%20that%20text%20genres%20can%20be%20inferred%20from%20LLMs%20with%20shallow%20learning%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.16540v1&entry.124074799=Read"},
{"title": "NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening", "author": "Misaal Khan and Mayank Vatsa and Kuldeep Singh and Richa Singh", "abstract": "Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.", "link": "http://arxiv.org/abs/2511.16566v1", "date": "2025-11-20", "relevancy": 2.3461, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4864}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4641}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NutriScreener%3A%20Retrieval-Augmented%20Multi-Pose%20Graph%20Attention%20Network%20for%20Malnourishment%20Screening&body=Title%3A%20NutriScreener%3A%20Retrieval-Augmented%20Multi-Pose%20Graph%20Attention%20Network%20for%20Malnourishment%20Screening%0AAuthor%3A%20Misaal%20Khan%20and%20Mayank%20Vatsa%20and%20Kuldeep%20Singh%20and%20Richa%20Singh%0AAbstract%3A%20Child%20malnutrition%20remains%20a%20global%20crisis%2C%20yet%20existing%20screening%20methods%20are%20laborious%20and%20poorly%20scalable%2C%20hindering%20early%20intervention.%20In%20this%20work%2C%20we%20present%20NutriScreener%2C%20a%20retrieval-augmented%2C%20multi-pose%20graph%20attention%20network%20that%20combines%20CLIP-based%20visual%20embeddings%2C%20class-boosted%20knowledge%20retrieval%2C%20and%20context%20awareness%20to%20enable%20robust%20malnutrition%20detection%20and%20anthropometric%20prediction%20from%20children%27s%20images%2C%20simultaneously%20addressing%20generalizability%20and%20class%20imbalance.%20In%20a%20clinical%20study%2C%20doctors%20rated%20it%204.3/5%20for%20accuracy%20and%204.6/5%20for%20efficiency%2C%20confirming%20its%20deployment%20readiness%20in%20low-resource%20settings.%20Trained%20and%20tested%20on%202%2C141%20children%20from%20AnthroVision%20and%20additionally%20evaluated%20on%20diverse%20cross-continent%20populations%2C%20including%20ARAN%20and%20an%20in-house%20collected%20CampusPose%20dataset%2C%20it%20achieves%200.79%20recall%2C%200.82%20AUC%2C%20and%20significantly%20lower%20anthropometric%20RMSEs%2C%20demonstrating%20reliable%20measurement%20in%20unconstrained%20pediatric%20settings.%20Cross-dataset%20results%20show%20up%20to%2025%25%20recall%20gain%20and%20up%20to%203.5%20cm%20RMSE%20reduction%20using%20demographically%20matched%20knowledge%20bases.%20NutriScreener%20offers%20a%20scalable%20and%20accurate%20solution%20for%20early%20malnutrition%20detection%20in%20low-resource%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNutriScreener%253A%2520Retrieval-Augmented%2520Multi-Pose%2520Graph%2520Attention%2520Network%2520for%2520Malnourishment%2520Screening%26entry.906535625%3DMisaal%2520Khan%2520and%2520Mayank%2520Vatsa%2520and%2520Kuldeep%2520Singh%2520and%2520Richa%2520Singh%26entry.1292438233%3DChild%2520malnutrition%2520remains%2520a%2520global%2520crisis%252C%2520yet%2520existing%2520screening%2520methods%2520are%2520laborious%2520and%2520poorly%2520scalable%252C%2520hindering%2520early%2520intervention.%2520In%2520this%2520work%252C%2520we%2520present%2520NutriScreener%252C%2520a%2520retrieval-augmented%252C%2520multi-pose%2520graph%2520attention%2520network%2520that%2520combines%2520CLIP-based%2520visual%2520embeddings%252C%2520class-boosted%2520knowledge%2520retrieval%252C%2520and%2520context%2520awareness%2520to%2520enable%2520robust%2520malnutrition%2520detection%2520and%2520anthropometric%2520prediction%2520from%2520children%2527s%2520images%252C%2520simultaneously%2520addressing%2520generalizability%2520and%2520class%2520imbalance.%2520In%2520a%2520clinical%2520study%252C%2520doctors%2520rated%2520it%25204.3/5%2520for%2520accuracy%2520and%25204.6/5%2520for%2520efficiency%252C%2520confirming%2520its%2520deployment%2520readiness%2520in%2520low-resource%2520settings.%2520Trained%2520and%2520tested%2520on%25202%252C141%2520children%2520from%2520AnthroVision%2520and%2520additionally%2520evaluated%2520on%2520diverse%2520cross-continent%2520populations%252C%2520including%2520ARAN%2520and%2520an%2520in-house%2520collected%2520CampusPose%2520dataset%252C%2520it%2520achieves%25200.79%2520recall%252C%25200.82%2520AUC%252C%2520and%2520significantly%2520lower%2520anthropometric%2520RMSEs%252C%2520demonstrating%2520reliable%2520measurement%2520in%2520unconstrained%2520pediatric%2520settings.%2520Cross-dataset%2520results%2520show%2520up%2520to%252025%2525%2520recall%2520gain%2520and%2520up%2520to%25203.5%2520cm%2520RMSE%2520reduction%2520using%2520demographically%2520matched%2520knowledge%2520bases.%2520NutriScreener%2520offers%2520a%2520scalable%2520and%2520accurate%2520solution%2520for%2520early%2520malnutrition%2520detection%2520in%2520low-resource%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NutriScreener%3A%20Retrieval-Augmented%20Multi-Pose%20Graph%20Attention%20Network%20for%20Malnourishment%20Screening&entry.906535625=Misaal%20Khan%20and%20Mayank%20Vatsa%20and%20Kuldeep%20Singh%20and%20Richa%20Singh&entry.1292438233=Child%20malnutrition%20remains%20a%20global%20crisis%2C%20yet%20existing%20screening%20methods%20are%20laborious%20and%20poorly%20scalable%2C%20hindering%20early%20intervention.%20In%20this%20work%2C%20we%20present%20NutriScreener%2C%20a%20retrieval-augmented%2C%20multi-pose%20graph%20attention%20network%20that%20combines%20CLIP-based%20visual%20embeddings%2C%20class-boosted%20knowledge%20retrieval%2C%20and%20context%20awareness%20to%20enable%20robust%20malnutrition%20detection%20and%20anthropometric%20prediction%20from%20children%27s%20images%2C%20simultaneously%20addressing%20generalizability%20and%20class%20imbalance.%20In%20a%20clinical%20study%2C%20doctors%20rated%20it%204.3/5%20for%20accuracy%20and%204.6/5%20for%20efficiency%2C%20confirming%20its%20deployment%20readiness%20in%20low-resource%20settings.%20Trained%20and%20tested%20on%202%2C141%20children%20from%20AnthroVision%20and%20additionally%20evaluated%20on%20diverse%20cross-continent%20populations%2C%20including%20ARAN%20and%20an%20in-house%20collected%20CampusPose%20dataset%2C%20it%20achieves%200.79%20recall%2C%200.82%20AUC%2C%20and%20significantly%20lower%20anthropometric%20RMSEs%2C%20demonstrating%20reliable%20measurement%20in%20unconstrained%20pediatric%20settings.%20Cross-dataset%20results%20show%20up%20to%2025%25%20recall%20gain%20and%20up%20to%203.5%20cm%20RMSE%20reduction%20using%20demographically%20matched%20knowledge%20bases.%20NutriScreener%20offers%20a%20scalable%20and%20accurate%20solution%20for%20early%20malnutrition%20detection%20in%20low-resource%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.16566v1&entry.124074799=Read"},
{"title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling", "author": "Minseok Seo and Mark Hamilton and Changick Kim", "abstract": "We present \\textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\\approx0.419 \\text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling.", "link": "http://arxiv.org/abs/2511.16301v1", "date": "2025-11-20", "relevancy": 2.3369, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6069}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5752}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Upsample%20Anything%3A%20A%20Simple%20and%20Hard%20to%20Beat%20Baseline%20for%20Feature%20Upsampling&body=Title%3A%20Upsample%20Anything%3A%20A%20Simple%20and%20Hard%20to%20Beat%20Baseline%20for%20Feature%20Upsampling%0AAuthor%3A%20Minseok%20Seo%20and%20Mark%20Hamilton%20and%20Changick%20Kim%0AAbstract%3A%20We%20present%20%5Ctextbf%7BUpsample%20Anything%7D%2C%20a%20lightweight%20test-time%20optimization%20%28TTO%29%20framework%20that%20restores%20low-resolution%20features%20to%20high-resolution%2C%20pixel-wise%20outputs%20without%20any%20training.%20Although%20Vision%20Foundation%20Models%20demonstrate%20strong%20generalization%20across%20diverse%20downstream%20tasks%2C%20their%20representations%20are%20typically%20downsampled%20by%2014x/16x%20%28e.g.%2C%20ViT%29%2C%20which%20limits%20their%20direct%20use%20in%20pixel-level%20applications.%20Existing%20feature%20upsampling%20approaches%20depend%20on%20dataset-specific%20retraining%20or%20heavy%20implicit%20optimization%2C%20restricting%20scalability%20and%20generalization.%20Upsample%20Anything%20addresses%20these%20issues%20through%20a%20simple%20per-image%20optimization%20that%20learns%20an%20anisotropic%20Gaussian%20kernel%20combining%20spatial%20and%20range%20cues%2C%20effectively%20bridging%20Gaussian%20Splatting%20and%20Joint%20Bilateral%20Upsampling.%20The%20learned%20kernel%20acts%20as%20a%20universal%2C%20edge-aware%20operator%20that%20transfers%20seamlessly%20across%20architectures%20and%20modalities%2C%20enabling%20precise%20high-resolution%20reconstruction%20of%20features%2C%20depth%2C%20or%20probability%20maps.%20It%20runs%20in%20only%20%24%5Capprox0.419%20%5Ctext%7Bs%7D%24%20per%20224x224%20image%20and%20achieves%20state-of-the-art%20performance%20on%20semantic%20segmentation%2C%20depth%20estimation%2C%20and%20both%20depth%20and%20probability%20map%20upsampling.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpsample%2520Anything%253A%2520A%2520Simple%2520and%2520Hard%2520to%2520Beat%2520Baseline%2520for%2520Feature%2520Upsampling%26entry.906535625%3DMinseok%2520Seo%2520and%2520Mark%2520Hamilton%2520and%2520Changick%2520Kim%26entry.1292438233%3DWe%2520present%2520%255Ctextbf%257BUpsample%2520Anything%257D%252C%2520a%2520lightweight%2520test-time%2520optimization%2520%2528TTO%2529%2520framework%2520that%2520restores%2520low-resolution%2520features%2520to%2520high-resolution%252C%2520pixel-wise%2520outputs%2520without%2520any%2520training.%2520Although%2520Vision%2520Foundation%2520Models%2520demonstrate%2520strong%2520generalization%2520across%2520diverse%2520downstream%2520tasks%252C%2520their%2520representations%2520are%2520typically%2520downsampled%2520by%252014x/16x%2520%2528e.g.%252C%2520ViT%2529%252C%2520which%2520limits%2520their%2520direct%2520use%2520in%2520pixel-level%2520applications.%2520Existing%2520feature%2520upsampling%2520approaches%2520depend%2520on%2520dataset-specific%2520retraining%2520or%2520heavy%2520implicit%2520optimization%252C%2520restricting%2520scalability%2520and%2520generalization.%2520Upsample%2520Anything%2520addresses%2520these%2520issues%2520through%2520a%2520simple%2520per-image%2520optimization%2520that%2520learns%2520an%2520anisotropic%2520Gaussian%2520kernel%2520combining%2520spatial%2520and%2520range%2520cues%252C%2520effectively%2520bridging%2520Gaussian%2520Splatting%2520and%2520Joint%2520Bilateral%2520Upsampling.%2520The%2520learned%2520kernel%2520acts%2520as%2520a%2520universal%252C%2520edge-aware%2520operator%2520that%2520transfers%2520seamlessly%2520across%2520architectures%2520and%2520modalities%252C%2520enabling%2520precise%2520high-resolution%2520reconstruction%2520of%2520features%252C%2520depth%252C%2520or%2520probability%2520maps.%2520It%2520runs%2520in%2520only%2520%2524%255Capprox0.419%2520%255Ctext%257Bs%257D%2524%2520per%2520224x224%2520image%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%2520semantic%2520segmentation%252C%2520depth%2520estimation%252C%2520and%2520both%2520depth%2520and%2520probability%2520map%2520upsampling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Upsample%20Anything%3A%20A%20Simple%20and%20Hard%20to%20Beat%20Baseline%20for%20Feature%20Upsampling&entry.906535625=Minseok%20Seo%20and%20Mark%20Hamilton%20and%20Changick%20Kim&entry.1292438233=We%20present%20%5Ctextbf%7BUpsample%20Anything%7D%2C%20a%20lightweight%20test-time%20optimization%20%28TTO%29%20framework%20that%20restores%20low-resolution%20features%20to%20high-resolution%2C%20pixel-wise%20outputs%20without%20any%20training.%20Although%20Vision%20Foundation%20Models%20demonstrate%20strong%20generalization%20across%20diverse%20downstream%20tasks%2C%20their%20representations%20are%20typically%20downsampled%20by%2014x/16x%20%28e.g.%2C%20ViT%29%2C%20which%20limits%20their%20direct%20use%20in%20pixel-level%20applications.%20Existing%20feature%20upsampling%20approaches%20depend%20on%20dataset-specific%20retraining%20or%20heavy%20implicit%20optimization%2C%20restricting%20scalability%20and%20generalization.%20Upsample%20Anything%20addresses%20these%20issues%20through%20a%20simple%20per-image%20optimization%20that%20learns%20an%20anisotropic%20Gaussian%20kernel%20combining%20spatial%20and%20range%20cues%2C%20effectively%20bridging%20Gaussian%20Splatting%20and%20Joint%20Bilateral%20Upsampling.%20The%20learned%20kernel%20acts%20as%20a%20universal%2C%20edge-aware%20operator%20that%20transfers%20seamlessly%20across%20architectures%20and%20modalities%2C%20enabling%20precise%20high-resolution%20reconstruction%20of%20features%2C%20depth%2C%20or%20probability%20maps.%20It%20runs%20in%20only%20%24%5Capprox0.419%20%5Ctext%7Bs%7D%24%20per%20224x224%20image%20and%20achieves%20state-of-the-art%20performance%20on%20semantic%20segmentation%2C%20depth%20estimation%2C%20and%20both%20depth%20and%20probability%20map%20upsampling.&entry.1838667208=http%3A//arxiv.org/abs/2511.16301v1&entry.124074799=Read"},
{"title": "Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin", "author": "Ming-Lun Lee and Fu-Shiang Yang and Cheng-Kuan Lin and Yan-Ann Chen and Chih-Yu Lin and Yu-Chee Tseng", "abstract": "Federated learning (FL) enables clients to collaboratively train a shared model in a distributed manner, setting it apart from traditional deep learning paradigms. However, most existing FL research assumes consistent client participation, overlooking the practical scenario of dynamic participation (DPFL), where clients may intermittently join or leave during training. Moreover, no existing benchmarking framework systematically supports the study of DPFL-specific challenges. In this work, we present the first open-source framework explicitly designed for benchmarking FL models under dynamic client participation. Our framework provides configurable data distributions, participation patterns, and evaluation metrics tailored to DPFL scenarios. Using this platform, we benchmark four major categories of widely adopted FL models and uncover substantial performance degradation under dynamic participation. To address these challenges, we further propose Knowledge-Pool Federated Learning (KPFL), a generic plugin that maintains a shared knowledge pool across both active and idle clients. KPFL leverages dual-age and data-bias weighting, combined with generative knowledge distillation, to mitigate instability and prevent knowledge loss. Extensive experiments demonstrate the significant impact of dynamic participation on FL performance and the effectiveness of KPFL in improving model robustness and generalization.", "link": "http://arxiv.org/abs/2511.16523v1", "date": "2025-11-20", "relevancy": 2.3364, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4872}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4576}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Participation%20in%20Federated%20Learning%3A%20Benchmarks%20and%20a%20Knowledge%20Pool%20Plugin&body=Title%3A%20Dynamic%20Participation%20in%20Federated%20Learning%3A%20Benchmarks%20and%20a%20Knowledge%20Pool%20Plugin%0AAuthor%3A%20Ming-Lun%20Lee%20and%20Fu-Shiang%20Yang%20and%20Cheng-Kuan%20Lin%20and%20Yan-Ann%20Chen%20and%20Chih-Yu%20Lin%20and%20Yu-Chee%20Tseng%0AAbstract%3A%20Federated%20learning%20%28FL%29%20enables%20clients%20to%20collaboratively%20train%20a%20shared%20model%20in%20a%20distributed%20manner%2C%20setting%20it%20apart%20from%20traditional%20deep%20learning%20paradigms.%20However%2C%20most%20existing%20FL%20research%20assumes%20consistent%20client%20participation%2C%20overlooking%20the%20practical%20scenario%20of%20dynamic%20participation%20%28DPFL%29%2C%20where%20clients%20may%20intermittently%20join%20or%20leave%20during%20training.%20Moreover%2C%20no%20existing%20benchmarking%20framework%20systematically%20supports%20the%20study%20of%20DPFL-specific%20challenges.%20In%20this%20work%2C%20we%20present%20the%20first%20open-source%20framework%20explicitly%20designed%20for%20benchmarking%20FL%20models%20under%20dynamic%20client%20participation.%20Our%20framework%20provides%20configurable%20data%20distributions%2C%20participation%20patterns%2C%20and%20evaluation%20metrics%20tailored%20to%20DPFL%20scenarios.%20Using%20this%20platform%2C%20we%20benchmark%20four%20major%20categories%20of%20widely%20adopted%20FL%20models%20and%20uncover%20substantial%20performance%20degradation%20under%20dynamic%20participation.%20To%20address%20these%20challenges%2C%20we%20further%20propose%20Knowledge-Pool%20Federated%20Learning%20%28KPFL%29%2C%20a%20generic%20plugin%20that%20maintains%20a%20shared%20knowledge%20pool%20across%20both%20active%20and%20idle%20clients.%20KPFL%20leverages%20dual-age%20and%20data-bias%20weighting%2C%20combined%20with%20generative%20knowledge%20distillation%2C%20to%20mitigate%20instability%20and%20prevent%20knowledge%20loss.%20Extensive%20experiments%20demonstrate%20the%20significant%20impact%20of%20dynamic%20participation%20on%20FL%20performance%20and%20the%20effectiveness%20of%20KPFL%20in%20improving%20model%20robustness%20and%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Participation%2520in%2520Federated%2520Learning%253A%2520Benchmarks%2520and%2520a%2520Knowledge%2520Pool%2520Plugin%26entry.906535625%3DMing-Lun%2520Lee%2520and%2520Fu-Shiang%2520Yang%2520and%2520Cheng-Kuan%2520Lin%2520and%2520Yan-Ann%2520Chen%2520and%2520Chih-Yu%2520Lin%2520and%2520Yu-Chee%2520Tseng%26entry.1292438233%3DFederated%2520learning%2520%2528FL%2529%2520enables%2520clients%2520to%2520collaboratively%2520train%2520a%2520shared%2520model%2520in%2520a%2520distributed%2520manner%252C%2520setting%2520it%2520apart%2520from%2520traditional%2520deep%2520learning%2520paradigms.%2520However%252C%2520most%2520existing%2520FL%2520research%2520assumes%2520consistent%2520client%2520participation%252C%2520overlooking%2520the%2520practical%2520scenario%2520of%2520dynamic%2520participation%2520%2528DPFL%2529%252C%2520where%2520clients%2520may%2520intermittently%2520join%2520or%2520leave%2520during%2520training.%2520Moreover%252C%2520no%2520existing%2520benchmarking%2520framework%2520systematically%2520supports%2520the%2520study%2520of%2520DPFL-specific%2520challenges.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520open-source%2520framework%2520explicitly%2520designed%2520for%2520benchmarking%2520FL%2520models%2520under%2520dynamic%2520client%2520participation.%2520Our%2520framework%2520provides%2520configurable%2520data%2520distributions%252C%2520participation%2520patterns%252C%2520and%2520evaluation%2520metrics%2520tailored%2520to%2520DPFL%2520scenarios.%2520Using%2520this%2520platform%252C%2520we%2520benchmark%2520four%2520major%2520categories%2520of%2520widely%2520adopted%2520FL%2520models%2520and%2520uncover%2520substantial%2520performance%2520degradation%2520under%2520dynamic%2520participation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520further%2520propose%2520Knowledge-Pool%2520Federated%2520Learning%2520%2528KPFL%2529%252C%2520a%2520generic%2520plugin%2520that%2520maintains%2520a%2520shared%2520knowledge%2520pool%2520across%2520both%2520active%2520and%2520idle%2520clients.%2520KPFL%2520leverages%2520dual-age%2520and%2520data-bias%2520weighting%252C%2520combined%2520with%2520generative%2520knowledge%2520distillation%252C%2520to%2520mitigate%2520instability%2520and%2520prevent%2520knowledge%2520loss.%2520Extensive%2520experiments%2520demonstrate%2520the%2520significant%2520impact%2520of%2520dynamic%2520participation%2520on%2520FL%2520performance%2520and%2520the%2520effectiveness%2520of%2520KPFL%2520in%2520improving%2520model%2520robustness%2520and%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Participation%20in%20Federated%20Learning%3A%20Benchmarks%20and%20a%20Knowledge%20Pool%20Plugin&entry.906535625=Ming-Lun%20Lee%20and%20Fu-Shiang%20Yang%20and%20Cheng-Kuan%20Lin%20and%20Yan-Ann%20Chen%20and%20Chih-Yu%20Lin%20and%20Yu-Chee%20Tseng&entry.1292438233=Federated%20learning%20%28FL%29%20enables%20clients%20to%20collaboratively%20train%20a%20shared%20model%20in%20a%20distributed%20manner%2C%20setting%20it%20apart%20from%20traditional%20deep%20learning%20paradigms.%20However%2C%20most%20existing%20FL%20research%20assumes%20consistent%20client%20participation%2C%20overlooking%20the%20practical%20scenario%20of%20dynamic%20participation%20%28DPFL%29%2C%20where%20clients%20may%20intermittently%20join%20or%20leave%20during%20training.%20Moreover%2C%20no%20existing%20benchmarking%20framework%20systematically%20supports%20the%20study%20of%20DPFL-specific%20challenges.%20In%20this%20work%2C%20we%20present%20the%20first%20open-source%20framework%20explicitly%20designed%20for%20benchmarking%20FL%20models%20under%20dynamic%20client%20participation.%20Our%20framework%20provides%20configurable%20data%20distributions%2C%20participation%20patterns%2C%20and%20evaluation%20metrics%20tailored%20to%20DPFL%20scenarios.%20Using%20this%20platform%2C%20we%20benchmark%20four%20major%20categories%20of%20widely%20adopted%20FL%20models%20and%20uncover%20substantial%20performance%20degradation%20under%20dynamic%20participation.%20To%20address%20these%20challenges%2C%20we%20further%20propose%20Knowledge-Pool%20Federated%20Learning%20%28KPFL%29%2C%20a%20generic%20plugin%20that%20maintains%20a%20shared%20knowledge%20pool%20across%20both%20active%20and%20idle%20clients.%20KPFL%20leverages%20dual-age%20and%20data-bias%20weighting%2C%20combined%20with%20generative%20knowledge%20distillation%2C%20to%20mitigate%20instability%20and%20prevent%20knowledge%20loss.%20Extensive%20experiments%20demonstrate%20the%20significant%20impact%20of%20dynamic%20participation%20on%20FL%20performance%20and%20the%20effectiveness%20of%20KPFL%20in%20improving%20model%20robustness%20and%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2511.16523v1&entry.124074799=Read"},
{"title": "CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation", "author": "Samer Abualhanud and Christian Grannemann and Max Mehltretter", "abstract": "Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360\u00b0 field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.", "link": "http://arxiv.org/abs/2511.16428v1", "date": "2025-11-20", "relevancy": 2.3339, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6061}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5949}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CylinderDepth%3A%20Cylindrical%20Spatial%20Attention%20for%20Multi-View%20Consistent%20Self-Supervised%20Surround%20Depth%20Estimation&body=Title%3A%20CylinderDepth%3A%20Cylindrical%20Spatial%20Attention%20for%20Multi-View%20Consistent%20Self-Supervised%20Surround%20Depth%20Estimation%0AAuthor%3A%20Samer%20Abualhanud%20and%20Christian%20Grannemann%20and%20Max%20Mehltretter%0AAbstract%3A%20Self-supervised%20surround-view%20depth%20estimation%20enables%20dense%2C%20low-cost%203D%20perception%20with%20a%20360%C2%B0%20field%20of%20view%20from%20multiple%20minimally%20overlapping%20images.%20Yet%2C%20most%20existing%20methods%20suffer%20from%20depth%20estimates%20that%20are%20inconsistent%20between%20overlapping%20images.%20Addressing%20this%20limitation%2C%20we%20propose%20a%20novel%20geometry-guided%20method%20for%20calibrated%2C%20time-synchronized%20multi-camera%20rigs%20that%20predicts%20dense%2C%20metric%2C%20and%20cross-view-consistent%20depth.%20Given%20the%20intrinsic%20and%20relative%20orientation%20parameters%2C%20a%20first%20depth%20map%20is%20predicted%20per%20image%20and%20the%20so-derived%203D%20points%20from%20all%20images%20are%20projected%20onto%20a%20shared%20unit%20cylinder%2C%20establishing%20neighborhood%20relations%20across%20different%20images.%20This%20produces%20a%202D%20position%20map%20for%20every%20image%2C%20where%20each%20pixel%20is%20assigned%20its%20projected%20position%20on%20the%20cylinder.%20Based%20on%20these%20position%20maps%2C%20we%20apply%20an%20explicit%2C%20non-learned%20spatial%20attention%20that%20aggregates%20features%20among%20pixels%20across%20images%20according%20to%20their%20distances%20on%20the%20cylinder%2C%20to%20predict%20a%20final%20depth%20map%20per%20image.%20Evaluated%20on%20the%20DDAD%20and%20nuScenes%20datasets%2C%20our%20approach%20improves%20the%20consistency%20of%20depth%20estimates%20across%20images%20and%20the%20overall%20depth%20compared%20to%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCylinderDepth%253A%2520Cylindrical%2520Spatial%2520Attention%2520for%2520Multi-View%2520Consistent%2520Self-Supervised%2520Surround%2520Depth%2520Estimation%26entry.906535625%3DSamer%2520Abualhanud%2520and%2520Christian%2520Grannemann%2520and%2520Max%2520Mehltretter%26entry.1292438233%3DSelf-supervised%2520surround-view%2520depth%2520estimation%2520enables%2520dense%252C%2520low-cost%25203D%2520perception%2520with%2520a%2520360%25C2%25B0%2520field%2520of%2520view%2520from%2520multiple%2520minimally%2520overlapping%2520images.%2520Yet%252C%2520most%2520existing%2520methods%2520suffer%2520from%2520depth%2520estimates%2520that%2520are%2520inconsistent%2520between%2520overlapping%2520images.%2520Addressing%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520geometry-guided%2520method%2520for%2520calibrated%252C%2520time-synchronized%2520multi-camera%2520rigs%2520that%2520predicts%2520dense%252C%2520metric%252C%2520and%2520cross-view-consistent%2520depth.%2520Given%2520the%2520intrinsic%2520and%2520relative%2520orientation%2520parameters%252C%2520a%2520first%2520depth%2520map%2520is%2520predicted%2520per%2520image%2520and%2520the%2520so-derived%25203D%2520points%2520from%2520all%2520images%2520are%2520projected%2520onto%2520a%2520shared%2520unit%2520cylinder%252C%2520establishing%2520neighborhood%2520relations%2520across%2520different%2520images.%2520This%2520produces%2520a%25202D%2520position%2520map%2520for%2520every%2520image%252C%2520where%2520each%2520pixel%2520is%2520assigned%2520its%2520projected%2520position%2520on%2520the%2520cylinder.%2520Based%2520on%2520these%2520position%2520maps%252C%2520we%2520apply%2520an%2520explicit%252C%2520non-learned%2520spatial%2520attention%2520that%2520aggregates%2520features%2520among%2520pixels%2520across%2520images%2520according%2520to%2520their%2520distances%2520on%2520the%2520cylinder%252C%2520to%2520predict%2520a%2520final%2520depth%2520map%2520per%2520image.%2520Evaluated%2520on%2520the%2520DDAD%2520and%2520nuScenes%2520datasets%252C%2520our%2520approach%2520improves%2520the%2520consistency%2520of%2520depth%2520estimates%2520across%2520images%2520and%2520the%2520overall%2520depth%2520compared%2520to%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CylinderDepth%3A%20Cylindrical%20Spatial%20Attention%20for%20Multi-View%20Consistent%20Self-Supervised%20Surround%20Depth%20Estimation&entry.906535625=Samer%20Abualhanud%20and%20Christian%20Grannemann%20and%20Max%20Mehltretter&entry.1292438233=Self-supervised%20surround-view%20depth%20estimation%20enables%20dense%2C%20low-cost%203D%20perception%20with%20a%20360%C2%B0%20field%20of%20view%20from%20multiple%20minimally%20overlapping%20images.%20Yet%2C%20most%20existing%20methods%20suffer%20from%20depth%20estimates%20that%20are%20inconsistent%20between%20overlapping%20images.%20Addressing%20this%20limitation%2C%20we%20propose%20a%20novel%20geometry-guided%20method%20for%20calibrated%2C%20time-synchronized%20multi-camera%20rigs%20that%20predicts%20dense%2C%20metric%2C%20and%20cross-view-consistent%20depth.%20Given%20the%20intrinsic%20and%20relative%20orientation%20parameters%2C%20a%20first%20depth%20map%20is%20predicted%20per%20image%20and%20the%20so-derived%203D%20points%20from%20all%20images%20are%20projected%20onto%20a%20shared%20unit%20cylinder%2C%20establishing%20neighborhood%20relations%20across%20different%20images.%20This%20produces%20a%202D%20position%20map%20for%20every%20image%2C%20where%20each%20pixel%20is%20assigned%20its%20projected%20position%20on%20the%20cylinder.%20Based%20on%20these%20position%20maps%2C%20we%20apply%20an%20explicit%2C%20non-learned%20spatial%20attention%20that%20aggregates%20features%20among%20pixels%20across%20images%20according%20to%20their%20distances%20on%20the%20cylinder%2C%20to%20predict%20a%20final%20depth%20map%20per%20image.%20Evaluated%20on%20the%20DDAD%20and%20nuScenes%20datasets%2C%20our%20approach%20improves%20the%20consistency%20of%20depth%20estimates%20across%20images%20and%20the%20overall%20depth%20compared%20to%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.16428v1&entry.124074799=Read"},
{"title": "Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs", "author": "Sinan Mutlu and Georgios F. Angelis and Savas Ozkan and Paul Wisbey and Anastasios Drosou and Mete Ozay", "abstract": "Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.", "link": "http://arxiv.org/abs/2511.16264v1", "date": "2025-11-20", "relevancy": 2.328, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6426}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5507}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mem-MLP%3A%20Real-Time%203D%20Human%20Motion%20Generation%20from%20Sparse%20Inputs&body=Title%3A%20Mem-MLP%3A%20Real-Time%203D%20Human%20Motion%20Generation%20from%20Sparse%20Inputs%0AAuthor%3A%20Sinan%20Mutlu%20and%20Georgios%20F.%20Angelis%20and%20Savas%20Ozkan%20and%20Paul%20Wisbey%20and%20Anastasios%20Drosou%20and%20Mete%20Ozay%0AAbstract%3A%20Realistic%20and%20smooth%20full-body%20tracking%20is%20crucial%20for%20immersive%20AR/VR%20applications.%20Existing%20systems%20primarily%20track%20head%20and%20hands%20via%20Head%20Mounted%20Devices%20%28HMDs%29%20and%20controllers%2C%20making%20the%203D%20full-body%20reconstruction%20in-complete.%20One%20potential%20approach%20is%20to%20generate%20the%20full-body%20motions%20from%20sparse%20inputs%20collected%20from%20limited%20sensors%20using%20a%20Neural%20Network%20%28NN%29%20model.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20based%20on%20a%20multi-layer%20perceptron%20%28MLP%29%20backbone%20that%20is%20enhanced%20with%20residual%20connections%20and%20a%20novel%20NN-component%20called%20Memory-Block.%20In%20particular%2C%20Memory-Block%20represents%20missing%20sensor%20data%20with%20trainable%20code-vectors%2C%20which%20are%20combined%20with%20the%20sparse%20signals%20from%20previous%20time%20instances%20to%20improve%20the%20temporal%20consistency.%20Furthermore%2C%20we%20formulate%20our%20solution%20as%20a%20multi-task%20learning%20problem%2C%20allowing%20our%20MLP-backbone%20to%20learn%20robust%20representations%20that%20boost%20accuracy.%20Our%20experiments%20show%20that%20our%20method%20outperforms%20state-of-the-art%20baselines%20by%20substantially%20reducing%20prediction%20errors.%20Moreover%2C%20it%20achieves%2072%20FPS%20on%20mobile%20HMDs%20that%20ultimately%20improves%20the%20accuracy-running%20time%20tradeoff.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMem-MLP%253A%2520Real-Time%25203D%2520Human%2520Motion%2520Generation%2520from%2520Sparse%2520Inputs%26entry.906535625%3DSinan%2520Mutlu%2520and%2520Georgios%2520F.%2520Angelis%2520and%2520Savas%2520Ozkan%2520and%2520Paul%2520Wisbey%2520and%2520Anastasios%2520Drosou%2520and%2520Mete%2520Ozay%26entry.1292438233%3DRealistic%2520and%2520smooth%2520full-body%2520tracking%2520is%2520crucial%2520for%2520immersive%2520AR/VR%2520applications.%2520Existing%2520systems%2520primarily%2520track%2520head%2520and%2520hands%2520via%2520Head%2520Mounted%2520Devices%2520%2528HMDs%2529%2520and%2520controllers%252C%2520making%2520the%25203D%2520full-body%2520reconstruction%2520in-complete.%2520One%2520potential%2520approach%2520is%2520to%2520generate%2520the%2520full-body%2520motions%2520from%2520sparse%2520inputs%2520collected%2520from%2520limited%2520sensors%2520using%2520a%2520Neural%2520Network%2520%2528NN%2529%2520model.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520based%2520on%2520a%2520multi-layer%2520perceptron%2520%2528MLP%2529%2520backbone%2520that%2520is%2520enhanced%2520with%2520residual%2520connections%2520and%2520a%2520novel%2520NN-component%2520called%2520Memory-Block.%2520In%2520particular%252C%2520Memory-Block%2520represents%2520missing%2520sensor%2520data%2520with%2520trainable%2520code-vectors%252C%2520which%2520are%2520combined%2520with%2520the%2520sparse%2520signals%2520from%2520previous%2520time%2520instances%2520to%2520improve%2520the%2520temporal%2520consistency.%2520Furthermore%252C%2520we%2520formulate%2520our%2520solution%2520as%2520a%2520multi-task%2520learning%2520problem%252C%2520allowing%2520our%2520MLP-backbone%2520to%2520learn%2520robust%2520representations%2520that%2520boost%2520accuracy.%2520Our%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520baselines%2520by%2520substantially%2520reducing%2520prediction%2520errors.%2520Moreover%252C%2520it%2520achieves%252072%2520FPS%2520on%2520mobile%2520HMDs%2520that%2520ultimately%2520improves%2520the%2520accuracy-running%2520time%2520tradeoff.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mem-MLP%3A%20Real-Time%203D%20Human%20Motion%20Generation%20from%20Sparse%20Inputs&entry.906535625=Sinan%20Mutlu%20and%20Georgios%20F.%20Angelis%20and%20Savas%20Ozkan%20and%20Paul%20Wisbey%20and%20Anastasios%20Drosou%20and%20Mete%20Ozay&entry.1292438233=Realistic%20and%20smooth%20full-body%20tracking%20is%20crucial%20for%20immersive%20AR/VR%20applications.%20Existing%20systems%20primarily%20track%20head%20and%20hands%20via%20Head%20Mounted%20Devices%20%28HMDs%29%20and%20controllers%2C%20making%20the%203D%20full-body%20reconstruction%20in-complete.%20One%20potential%20approach%20is%20to%20generate%20the%20full-body%20motions%20from%20sparse%20inputs%20collected%20from%20limited%20sensors%20using%20a%20Neural%20Network%20%28NN%29%20model.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20based%20on%20a%20multi-layer%20perceptron%20%28MLP%29%20backbone%20that%20is%20enhanced%20with%20residual%20connections%20and%20a%20novel%20NN-component%20called%20Memory-Block.%20In%20particular%2C%20Memory-Block%20represents%20missing%20sensor%20data%20with%20trainable%20code-vectors%2C%20which%20are%20combined%20with%20the%20sparse%20signals%20from%20previous%20time%20instances%20to%20improve%20the%20temporal%20consistency.%20Furthermore%2C%20we%20formulate%20our%20solution%20as%20a%20multi-task%20learning%20problem%2C%20allowing%20our%20MLP-backbone%20to%20learn%20robust%20representations%20that%20boost%20accuracy.%20Our%20experiments%20show%20that%20our%20method%20outperforms%20state-of-the-art%20baselines%20by%20substantially%20reducing%20prediction%20errors.%20Moreover%2C%20it%20achieves%2072%20FPS%20on%20mobile%20HMDs%20that%20ultimately%20improves%20the%20accuracy-running%20time%20tradeoff.&entry.1838667208=http%3A//arxiv.org/abs/2511.16264v1&entry.124074799=Read"},
{"title": "End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss", "author": "Hai Lan and Zongyan Li and Jianmin Hu and Jialing Yang and Houde Dai", "abstract": "Marker-based optical motion capture (MoCap), while long regarded as the gold standard for accuracy, faces practical challenges, such as time-consuming preparation and marker identification ambiguity, due to its reliance on dense marker configurations, which fundamentally limit its scalability. To address this, we introduce a novel fundamental unit for MoCap, the Rigid Body Marker (RBM), which provides unambiguous 6-DoF data and drastically simplifies setup. Leveraging this new data modality, we develop a deep-learning-based regression model that directly estimates SMPL parameters under a geodesic loss. This end-to-end approach matches the performance of optimization-based methods while requiring over an order of magnitude less computation. Trained on synthesized data from the AMASS dataset, our end-to-end model achieves state-of-the-art accuracy in body pose estimation. Real-world data captured using a Vicon optical tracking system further demonstrates the practical viability of our approach. Overall, the results show that combining sparse 6-DoF RBM with a manifold-aware geodesic loss yields a practical and high-fidelity solution for real-time MoCap in graphics, virtual reality, and biomechanics.", "link": "http://arxiv.org/abs/2511.16418v1", "date": "2025-11-20", "relevancy": 2.3272, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5995}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5785}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Motion%20Capture%20from%20Rigid%20Body%20Markers%20with%20Geodesic%20Loss&body=Title%3A%20End-to-End%20Motion%20Capture%20from%20Rigid%20Body%20Markers%20with%20Geodesic%20Loss%0AAuthor%3A%20Hai%20Lan%20and%20Zongyan%20Li%20and%20Jianmin%20Hu%20and%20Jialing%20Yang%20and%20Houde%20Dai%0AAbstract%3A%20Marker-based%20optical%20motion%20capture%20%28MoCap%29%2C%20while%20long%20regarded%20as%20the%20gold%20standard%20for%20accuracy%2C%20faces%20practical%20challenges%2C%20such%20as%20time-consuming%20preparation%20and%20marker%20identification%20ambiguity%2C%20due%20to%20its%20reliance%20on%20dense%20marker%20configurations%2C%20which%20fundamentally%20limit%20its%20scalability.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20fundamental%20unit%20for%20MoCap%2C%20the%20Rigid%20Body%20Marker%20%28RBM%29%2C%20which%20provides%20unambiguous%206-DoF%20data%20and%20drastically%20simplifies%20setup.%20Leveraging%20this%20new%20data%20modality%2C%20we%20develop%20a%20deep-learning-based%20regression%20model%20that%20directly%20estimates%20SMPL%20parameters%20under%20a%20geodesic%20loss.%20This%20end-to-end%20approach%20matches%20the%20performance%20of%20optimization-based%20methods%20while%20requiring%20over%20an%20order%20of%20magnitude%20less%20computation.%20Trained%20on%20synthesized%20data%20from%20the%20AMASS%20dataset%2C%20our%20end-to-end%20model%20achieves%20state-of-the-art%20accuracy%20in%20body%20pose%20estimation.%20Real-world%20data%20captured%20using%20a%20Vicon%20optical%20tracking%20system%20further%20demonstrates%20the%20practical%20viability%20of%20our%20approach.%20Overall%2C%20the%20results%20show%20that%20combining%20sparse%206-DoF%20RBM%20with%20a%20manifold-aware%20geodesic%20loss%20yields%20a%20practical%20and%20high-fidelity%20solution%20for%20real-time%20MoCap%20in%20graphics%2C%20virtual%20reality%2C%20and%20biomechanics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Motion%2520Capture%2520from%2520Rigid%2520Body%2520Markers%2520with%2520Geodesic%2520Loss%26entry.906535625%3DHai%2520Lan%2520and%2520Zongyan%2520Li%2520and%2520Jianmin%2520Hu%2520and%2520Jialing%2520Yang%2520and%2520Houde%2520Dai%26entry.1292438233%3DMarker-based%2520optical%2520motion%2520capture%2520%2528MoCap%2529%252C%2520while%2520long%2520regarded%2520as%2520the%2520gold%2520standard%2520for%2520accuracy%252C%2520faces%2520practical%2520challenges%252C%2520such%2520as%2520time-consuming%2520preparation%2520and%2520marker%2520identification%2520ambiguity%252C%2520due%2520to%2520its%2520reliance%2520on%2520dense%2520marker%2520configurations%252C%2520which%2520fundamentally%2520limit%2520its%2520scalability.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520fundamental%2520unit%2520for%2520MoCap%252C%2520the%2520Rigid%2520Body%2520Marker%2520%2528RBM%2529%252C%2520which%2520provides%2520unambiguous%25206-DoF%2520data%2520and%2520drastically%2520simplifies%2520setup.%2520Leveraging%2520this%2520new%2520data%2520modality%252C%2520we%2520develop%2520a%2520deep-learning-based%2520regression%2520model%2520that%2520directly%2520estimates%2520SMPL%2520parameters%2520under%2520a%2520geodesic%2520loss.%2520This%2520end-to-end%2520approach%2520matches%2520the%2520performance%2520of%2520optimization-based%2520methods%2520while%2520requiring%2520over%2520an%2520order%2520of%2520magnitude%2520less%2520computation.%2520Trained%2520on%2520synthesized%2520data%2520from%2520the%2520AMASS%2520dataset%252C%2520our%2520end-to-end%2520model%2520achieves%2520state-of-the-art%2520accuracy%2520in%2520body%2520pose%2520estimation.%2520Real-world%2520data%2520captured%2520using%2520a%2520Vicon%2520optical%2520tracking%2520system%2520further%2520demonstrates%2520the%2520practical%2520viability%2520of%2520our%2520approach.%2520Overall%252C%2520the%2520results%2520show%2520that%2520combining%2520sparse%25206-DoF%2520RBM%2520with%2520a%2520manifold-aware%2520geodesic%2520loss%2520yields%2520a%2520practical%2520and%2520high-fidelity%2520solution%2520for%2520real-time%2520MoCap%2520in%2520graphics%252C%2520virtual%2520reality%252C%2520and%2520biomechanics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Motion%20Capture%20from%20Rigid%20Body%20Markers%20with%20Geodesic%20Loss&entry.906535625=Hai%20Lan%20and%20Zongyan%20Li%20and%20Jianmin%20Hu%20and%20Jialing%20Yang%20and%20Houde%20Dai&entry.1292438233=Marker-based%20optical%20motion%20capture%20%28MoCap%29%2C%20while%20long%20regarded%20as%20the%20gold%20standard%20for%20accuracy%2C%20faces%20practical%20challenges%2C%20such%20as%20time-consuming%20preparation%20and%20marker%20identification%20ambiguity%2C%20due%20to%20its%20reliance%20on%20dense%20marker%20configurations%2C%20which%20fundamentally%20limit%20its%20scalability.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20fundamental%20unit%20for%20MoCap%2C%20the%20Rigid%20Body%20Marker%20%28RBM%29%2C%20which%20provides%20unambiguous%206-DoF%20data%20and%20drastically%20simplifies%20setup.%20Leveraging%20this%20new%20data%20modality%2C%20we%20develop%20a%20deep-learning-based%20regression%20model%20that%20directly%20estimates%20SMPL%20parameters%20under%20a%20geodesic%20loss.%20This%20end-to-end%20approach%20matches%20the%20performance%20of%20optimization-based%20methods%20while%20requiring%20over%20an%20order%20of%20magnitude%20less%20computation.%20Trained%20on%20synthesized%20data%20from%20the%20AMASS%20dataset%2C%20our%20end-to-end%20model%20achieves%20state-of-the-art%20accuracy%20in%20body%20pose%20estimation.%20Real-world%20data%20captured%20using%20a%20Vicon%20optical%20tracking%20system%20further%20demonstrates%20the%20practical%20viability%20of%20our%20approach.%20Overall%2C%20the%20results%20show%20that%20combining%20sparse%206-DoF%20RBM%20with%20a%20manifold-aware%20geodesic%20loss%20yields%20a%20practical%20and%20high-fidelity%20solution%20for%20real-time%20MoCap%20in%20graphics%2C%20virtual%20reality%2C%20and%20biomechanics.&entry.1838667208=http%3A//arxiv.org/abs/2511.16418v1&entry.124074799=Read"},
{"title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence", "author": "Kun Ouyang and Yuanxin Liu and Linli Yao and Yishuo Cai and Hao Zhou and Jie Zhou and Fandong Meng and Xu Sun", "abstract": "Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding, yet still struggle with inaccurate evidence localization. To address these limitations, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies context and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we 1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that include frame identification, evidence reasoning, and action decision, and 2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to progressively incentivize multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long video understanding tasks, validating its strong scalability and robustness.", "link": "http://arxiv.org/abs/2510.20470v2", "date": "2025-11-20", "relevancy": 2.3164, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5868}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conan%3A%20Progressive%20Learning%20to%20Reason%20Like%20a%20Detective%20over%20Multi-Scale%20Visual%20Evidence&body=Title%3A%20Conan%3A%20Progressive%20Learning%20to%20Reason%20Like%20a%20Detective%20over%20Multi-Scale%20Visual%20Evidence%0AAuthor%3A%20Kun%20Ouyang%20and%20Yuanxin%20Liu%20and%20Linli%20Yao%20and%20Yishuo%20Cai%20and%20Hao%20Zhou%20and%20Jie%20Zhou%20and%20Fandong%20Meng%20and%20Xu%20Sun%0AAbstract%3A%20Video%20reasoning%2C%20which%20requires%20multi-step%20deduction%20across%20frames%2C%20remains%20a%20major%20challenge%20for%20multimodal%20large%20language%20models%20%28MLLMs%29.%20While%20reinforcement%20learning%20%28RL%29-based%20methods%20enhance%20reasoning%20capabilities%2C%20they%20often%20rely%20on%20text-only%20chains%20that%20yield%20ungrounded%20or%20hallucinated%20conclusions.%20Conversely%2C%20frame-retrieval%20approaches%20introduce%20visual%20grounding%2C%20yet%20still%20struggle%20with%20inaccurate%20evidence%20localization.%20To%20address%20these%20limitations%2C%20we%20present%20Conan%2C%20a%20framework%20for%20evidence-grounded%20multi-step%20video%20reasoning.%20Conan%20identifies%20context%20and%20evidence%20frames%2C%20reasons%20over%20cross-frame%20clues%2C%20and%20adaptively%20decides%20when%20to%20conclude%20or%20explore%20further.%20To%20achieve%20this%2C%20we%201%29%20construct%20Conan-91K%2C%20a%20large-scale%20dataset%20of%20automatically%20generated%20reasoning%20traces%20that%20include%20frame%20identification%2C%20evidence%20reasoning%2C%20and%20action%20decision%2C%20and%202%29%20design%20a%20multi-stage%20progressive%20cold-start%20strategy%20combined%20with%20an%20Identification-Reasoning-Action%20%28AIR%29%20RLVR%20training%20framework%20to%20progressively%20incentivize%20multi-step%20visual%20reasoning.%20Extensive%20experiments%20on%20six%20multi-step%20reasoning%20benchmarks%20demonstrate%20that%20Conan%20surpasses%20the%20baseline%20Qwen2.5-VL-7B-Instruct%20by%20an%20average%20of%20over%2010%25%20in%20accuracy%2C%20achieving%20state-of-the-art%20performance.%20Furthermore%2C%20Conan%20generalizes%20effectively%20to%20long%20video%20understanding%20tasks%2C%20validating%20its%20strong%20scalability%20and%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2510.20470v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConan%253A%2520Progressive%2520Learning%2520to%2520Reason%2520Like%2520a%2520Detective%2520over%2520Multi-Scale%2520Visual%2520Evidence%26entry.906535625%3DKun%2520Ouyang%2520and%2520Yuanxin%2520Liu%2520and%2520Linli%2520Yao%2520and%2520Yishuo%2520Cai%2520and%2520Hao%2520Zhou%2520and%2520Jie%2520Zhou%2520and%2520Fandong%2520Meng%2520and%2520Xu%2520Sun%26entry.1292438233%3DVideo%2520reasoning%252C%2520which%2520requires%2520multi-step%2520deduction%2520across%2520frames%252C%2520remains%2520a%2520major%2520challenge%2520for%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520While%2520reinforcement%2520learning%2520%2528RL%2529-based%2520methods%2520enhance%2520reasoning%2520capabilities%252C%2520they%2520often%2520rely%2520on%2520text-only%2520chains%2520that%2520yield%2520ungrounded%2520or%2520hallucinated%2520conclusions.%2520Conversely%252C%2520frame-retrieval%2520approaches%2520introduce%2520visual%2520grounding%252C%2520yet%2520still%2520struggle%2520with%2520inaccurate%2520evidence%2520localization.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520Conan%252C%2520a%2520framework%2520for%2520evidence-grounded%2520multi-step%2520video%2520reasoning.%2520Conan%2520identifies%2520context%2520and%2520evidence%2520frames%252C%2520reasons%2520over%2520cross-frame%2520clues%252C%2520and%2520adaptively%2520decides%2520when%2520to%2520conclude%2520or%2520explore%2520further.%2520To%2520achieve%2520this%252C%2520we%25201%2529%2520construct%2520Conan-91K%252C%2520a%2520large-scale%2520dataset%2520of%2520automatically%2520generated%2520reasoning%2520traces%2520that%2520include%2520frame%2520identification%252C%2520evidence%2520reasoning%252C%2520and%2520action%2520decision%252C%2520and%25202%2529%2520design%2520a%2520multi-stage%2520progressive%2520cold-start%2520strategy%2520combined%2520with%2520an%2520Identification-Reasoning-Action%2520%2528AIR%2529%2520RLVR%2520training%2520framework%2520to%2520progressively%2520incentivize%2520multi-step%2520visual%2520reasoning.%2520Extensive%2520experiments%2520on%2520six%2520multi-step%2520reasoning%2520benchmarks%2520demonstrate%2520that%2520Conan%2520surpasses%2520the%2520baseline%2520Qwen2.5-VL-7B-Instruct%2520by%2520an%2520average%2520of%2520over%252010%2525%2520in%2520accuracy%252C%2520achieving%2520state-of-the-art%2520performance.%2520Furthermore%252C%2520Conan%2520generalizes%2520effectively%2520to%2520long%2520video%2520understanding%2520tasks%252C%2520validating%2520its%2520strong%2520scalability%2520and%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.20470v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conan%3A%20Progressive%20Learning%20to%20Reason%20Like%20a%20Detective%20over%20Multi-Scale%20Visual%20Evidence&entry.906535625=Kun%20Ouyang%20and%20Yuanxin%20Liu%20and%20Linli%20Yao%20and%20Yishuo%20Cai%20and%20Hao%20Zhou%20and%20Jie%20Zhou%20and%20Fandong%20Meng%20and%20Xu%20Sun&entry.1292438233=Video%20reasoning%2C%20which%20requires%20multi-step%20deduction%20across%20frames%2C%20remains%20a%20major%20challenge%20for%20multimodal%20large%20language%20models%20%28MLLMs%29.%20While%20reinforcement%20learning%20%28RL%29-based%20methods%20enhance%20reasoning%20capabilities%2C%20they%20often%20rely%20on%20text-only%20chains%20that%20yield%20ungrounded%20or%20hallucinated%20conclusions.%20Conversely%2C%20frame-retrieval%20approaches%20introduce%20visual%20grounding%2C%20yet%20still%20struggle%20with%20inaccurate%20evidence%20localization.%20To%20address%20these%20limitations%2C%20we%20present%20Conan%2C%20a%20framework%20for%20evidence-grounded%20multi-step%20video%20reasoning.%20Conan%20identifies%20context%20and%20evidence%20frames%2C%20reasons%20over%20cross-frame%20clues%2C%20and%20adaptively%20decides%20when%20to%20conclude%20or%20explore%20further.%20To%20achieve%20this%2C%20we%201%29%20construct%20Conan-91K%2C%20a%20large-scale%20dataset%20of%20automatically%20generated%20reasoning%20traces%20that%20include%20frame%20identification%2C%20evidence%20reasoning%2C%20and%20action%20decision%2C%20and%202%29%20design%20a%20multi-stage%20progressive%20cold-start%20strategy%20combined%20with%20an%20Identification-Reasoning-Action%20%28AIR%29%20RLVR%20training%20framework%20to%20progressively%20incentivize%20multi-step%20visual%20reasoning.%20Extensive%20experiments%20on%20six%20multi-step%20reasoning%20benchmarks%20demonstrate%20that%20Conan%20surpasses%20the%20baseline%20Qwen2.5-VL-7B-Instruct%20by%20an%20average%20of%20over%2010%25%20in%20accuracy%2C%20achieving%20state-of-the-art%20performance.%20Furthermore%2C%20Conan%20generalizes%20effectively%20to%20long%20video%20understanding%20tasks%2C%20validating%20its%20strong%20scalability%20and%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2510.20470v2&entry.124074799=Read"},
{"title": "Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies", "author": "Jonathan Kamp and Lisa Beinborn and Antske Fokkens", "abstract": "Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.", "link": "http://arxiv.org/abs/2511.16353v1", "date": "2025-11-20", "relevancy": 2.3115, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4645}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Sufficient%20Rationales%3A%20Analysing%20the%20Relationship%20Between%20Explanation%20Faithfulness%20and%20Token-level%20Regularisation%20Strategies&body=Title%3A%20Learning%20from%20Sufficient%20Rationales%3A%20Analysing%20the%20Relationship%20Between%20Explanation%20Faithfulness%20and%20Token-level%20Regularisation%20Strategies%0AAuthor%3A%20Jonathan%20Kamp%20and%20Lisa%20Beinborn%20and%20Antske%20Fokkens%0AAbstract%3A%20Human%20explanations%20of%20natural%20language%2C%20rationales%2C%20form%20a%20tool%20to%20assess%20whether%20models%20learn%20a%20label%20for%20the%20right%20reasons%20or%20rely%20on%20dataset-specific%20shortcuts.%20Sufficiency%20is%20a%20common%20metric%20for%20estimating%20the%20informativeness%20of%20rationales%2C%20but%20it%20provides%20limited%20insight%20into%20the%20effects%20of%20rationale%20information%20on%20model%20performance.%20We%20address%20this%20limitation%20by%20relating%20sufficiency%20to%20two%20modelling%20paradigms%3A%20the%20ability%20of%20models%20to%20identify%20which%20tokens%20are%20part%20of%20the%20rationale%20%28through%20token%20classification%29%20and%20the%20ability%20of%20improving%20model%20performance%20by%20incorporating%20rationales%20in%20the%20input%20%28through%20attention%20regularisation%29.%20We%20find%20that%20highly%20informative%20rationales%20are%20not%20likely%20to%20help%20classify%20the%20instance%20correctly.%20Sufficiency%20conversely%20captures%20the%20classification%20impact%20of%20the%20non-rationalised%20context%2C%20which%20interferes%20with%20rationale%20information%20in%20the%20same%20input.%20We%20also%20find%20that%20incorporating%20rationale%20information%20in%20model%20inputs%20can%20boost%20cross-domain%20classification%2C%20but%20results%20are%20inconsistent%20per%20task%20and%20model%20type.%20Finally%2C%20sufficiency%20and%20token%20classification%20appear%20to%20be%20unrelated.%20These%20results%20exemplify%20the%20complexity%20of%20rationales%2C%20showing%20that%20metrics%20capable%20of%20systematically%20capturing%20this%20type%20of%20information%20merit%20further%20investigation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Sufficient%2520Rationales%253A%2520Analysing%2520the%2520Relationship%2520Between%2520Explanation%2520Faithfulness%2520and%2520Token-level%2520Regularisation%2520Strategies%26entry.906535625%3DJonathan%2520Kamp%2520and%2520Lisa%2520Beinborn%2520and%2520Antske%2520Fokkens%26entry.1292438233%3DHuman%2520explanations%2520of%2520natural%2520language%252C%2520rationales%252C%2520form%2520a%2520tool%2520to%2520assess%2520whether%2520models%2520learn%2520a%2520label%2520for%2520the%2520right%2520reasons%2520or%2520rely%2520on%2520dataset-specific%2520shortcuts.%2520Sufficiency%2520is%2520a%2520common%2520metric%2520for%2520estimating%2520the%2520informativeness%2520of%2520rationales%252C%2520but%2520it%2520provides%2520limited%2520insight%2520into%2520the%2520effects%2520of%2520rationale%2520information%2520on%2520model%2520performance.%2520We%2520address%2520this%2520limitation%2520by%2520relating%2520sufficiency%2520to%2520two%2520modelling%2520paradigms%253A%2520the%2520ability%2520of%2520models%2520to%2520identify%2520which%2520tokens%2520are%2520part%2520of%2520the%2520rationale%2520%2528through%2520token%2520classification%2529%2520and%2520the%2520ability%2520of%2520improving%2520model%2520performance%2520by%2520incorporating%2520rationales%2520in%2520the%2520input%2520%2528through%2520attention%2520regularisation%2529.%2520We%2520find%2520that%2520highly%2520informative%2520rationales%2520are%2520not%2520likely%2520to%2520help%2520classify%2520the%2520instance%2520correctly.%2520Sufficiency%2520conversely%2520captures%2520the%2520classification%2520impact%2520of%2520the%2520non-rationalised%2520context%252C%2520which%2520interferes%2520with%2520rationale%2520information%2520in%2520the%2520same%2520input.%2520We%2520also%2520find%2520that%2520incorporating%2520rationale%2520information%2520in%2520model%2520inputs%2520can%2520boost%2520cross-domain%2520classification%252C%2520but%2520results%2520are%2520inconsistent%2520per%2520task%2520and%2520model%2520type.%2520Finally%252C%2520sufficiency%2520and%2520token%2520classification%2520appear%2520to%2520be%2520unrelated.%2520These%2520results%2520exemplify%2520the%2520complexity%2520of%2520rationales%252C%2520showing%2520that%2520metrics%2520capable%2520of%2520systematically%2520capturing%2520this%2520type%2520of%2520information%2520merit%2520further%2520investigation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Sufficient%20Rationales%3A%20Analysing%20the%20Relationship%20Between%20Explanation%20Faithfulness%20and%20Token-level%20Regularisation%20Strategies&entry.906535625=Jonathan%20Kamp%20and%20Lisa%20Beinborn%20and%20Antske%20Fokkens&entry.1292438233=Human%20explanations%20of%20natural%20language%2C%20rationales%2C%20form%20a%20tool%20to%20assess%20whether%20models%20learn%20a%20label%20for%20the%20right%20reasons%20or%20rely%20on%20dataset-specific%20shortcuts.%20Sufficiency%20is%20a%20common%20metric%20for%20estimating%20the%20informativeness%20of%20rationales%2C%20but%20it%20provides%20limited%20insight%20into%20the%20effects%20of%20rationale%20information%20on%20model%20performance.%20We%20address%20this%20limitation%20by%20relating%20sufficiency%20to%20two%20modelling%20paradigms%3A%20the%20ability%20of%20models%20to%20identify%20which%20tokens%20are%20part%20of%20the%20rationale%20%28through%20token%20classification%29%20and%20the%20ability%20of%20improving%20model%20performance%20by%20incorporating%20rationales%20in%20the%20input%20%28through%20attention%20regularisation%29.%20We%20find%20that%20highly%20informative%20rationales%20are%20not%20likely%20to%20help%20classify%20the%20instance%20correctly.%20Sufficiency%20conversely%20captures%20the%20classification%20impact%20of%20the%20non-rationalised%20context%2C%20which%20interferes%20with%20rationale%20information%20in%20the%20same%20input.%20We%20also%20find%20that%20incorporating%20rationale%20information%20in%20model%20inputs%20can%20boost%20cross-domain%20classification%2C%20but%20results%20are%20inconsistent%20per%20task%20and%20model%20type.%20Finally%2C%20sufficiency%20and%20token%20classification%20appear%20to%20be%20unrelated.%20These%20results%20exemplify%20the%20complexity%20of%20rationales%2C%20showing%20that%20metrics%20capable%20of%20systematically%20capturing%20this%20type%20of%20information%20merit%20further%20investigation.&entry.1838667208=http%3A//arxiv.org/abs/2511.16353v1&entry.124074799=Read"},
{"title": "CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation", "author": "Hariprasath Govindarajan and Maciej K. Wozniak and Marvin Klingner and Camille Maurice and B Ravi Kiran and Senthil Yogamani", "abstract": "Vision foundation models (VFMs) such as DINO have led to a paradigm shift in 2D camera-based perception towards extracting generalized features to support many downstream tasks. Recent works introduce self-supervised cross-modal knowledge distillation (KD) as a way to transfer these powerful generalization capabilities into 3D LiDAR-based models. However, they either rely on highly complex distillation losses, pseudo-semantic maps, or limit KD to features useful for semantic segmentation only. In this work, we propose CleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework introducing a set of simple yet effective design choices: Unlike contrastive approaches relying on complex loss design choices, our method employs a direct feature similarity loss in combination with a multi layer perceptron (MLP) projection head to allow the 3D network to learn complex semantic dependencies throughout the projection. Crucially, our approach does not depend on pseudo-semantic maps, allowing for direct knowledge transfer from a VFM without explicit semantic supervision. Additionally, we introduce the auxiliary self-supervised spatial task of occupancy prediction to enhance the semantic knowledge, obtained from a VFM through KD, with 3D spatial reasoning capabilities. Experiments on standard autonomous driving benchmarks for 2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-art performance in both semantic segmentation and 3D object detection (3DOD) by up to 10% mIoU, especially when fine tuning on really low data amounts, showing the effectiveness of our simple yet powerful KD strategy", "link": "http://arxiv.org/abs/2503.09878v3", "date": "2025-11-20", "relevancy": 2.2954, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5921}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5625}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CleverDistiller%3A%20Simple%20and%20Spatially%20Consistent%20Cross-modal%20Distillation&body=Title%3A%20CleverDistiller%3A%20Simple%20and%20Spatially%20Consistent%20Cross-modal%20Distillation%0AAuthor%3A%20Hariprasath%20Govindarajan%20and%20Maciej%20K.%20Wozniak%20and%20Marvin%20Klingner%20and%20Camille%20Maurice%20and%20B%20Ravi%20Kiran%20and%20Senthil%20Yogamani%0AAbstract%3A%20Vision%20foundation%20models%20%28VFMs%29%20such%20as%20DINO%20have%20led%20to%20a%20paradigm%20shift%20in%202D%20camera-based%20perception%20towards%20extracting%20generalized%20features%20to%20support%20many%20downstream%20tasks.%20Recent%20works%20introduce%20self-supervised%20cross-modal%20knowledge%20distillation%20%28KD%29%20as%20a%20way%20to%20transfer%20these%20powerful%20generalization%20capabilities%20into%203D%20LiDAR-based%20models.%20However%2C%20they%20either%20rely%20on%20highly%20complex%20distillation%20losses%2C%20pseudo-semantic%20maps%2C%20or%20limit%20KD%20to%20features%20useful%20for%20semantic%20segmentation%20only.%20In%20this%20work%2C%20we%20propose%20CleverDistiller%2C%20a%20self-supervised%2C%20cross-modal%202D-to-3D%20KD%20framework%20introducing%20a%20set%20of%20simple%20yet%20effective%20design%20choices%3A%20Unlike%20contrastive%20approaches%20relying%20on%20complex%20loss%20design%20choices%2C%20our%20method%20employs%20a%20direct%20feature%20similarity%20loss%20in%20combination%20with%20a%20multi%20layer%20perceptron%20%28MLP%29%20projection%20head%20to%20allow%20the%203D%20network%20to%20learn%20complex%20semantic%20dependencies%20throughout%20the%20projection.%20Crucially%2C%20our%20approach%20does%20not%20depend%20on%20pseudo-semantic%20maps%2C%20allowing%20for%20direct%20knowledge%20transfer%20from%20a%20VFM%20without%20explicit%20semantic%20supervision.%20Additionally%2C%20we%20introduce%20the%20auxiliary%20self-supervised%20spatial%20task%20of%20occupancy%20prediction%20to%20enhance%20the%20semantic%20knowledge%2C%20obtained%20from%20a%20VFM%20through%20KD%2C%20with%203D%20spatial%20reasoning%20capabilities.%20Experiments%20on%20standard%20autonomous%20driving%20benchmarks%20for%202D-to-3D%20KD%20demonstrate%20that%20CleverDistiller%20achieves%20state-of-the-art%20performance%20in%20both%20semantic%20segmentation%20and%203D%20object%20detection%20%283DOD%29%20by%20up%20to%2010%25%20mIoU%2C%20especially%20when%20fine%20tuning%20on%20really%20low%20data%20amounts%2C%20showing%20the%20effectiveness%20of%20our%20simple%20yet%20powerful%20KD%20strategy%0ALink%3A%20http%3A//arxiv.org/abs/2503.09878v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCleverDistiller%253A%2520Simple%2520and%2520Spatially%2520Consistent%2520Cross-modal%2520Distillation%26entry.906535625%3DHariprasath%2520Govindarajan%2520and%2520Maciej%2520K.%2520Wozniak%2520and%2520Marvin%2520Klingner%2520and%2520Camille%2520Maurice%2520and%2520B%2520Ravi%2520Kiran%2520and%2520Senthil%2520Yogamani%26entry.1292438233%3DVision%2520foundation%2520models%2520%2528VFMs%2529%2520such%2520as%2520DINO%2520have%2520led%2520to%2520a%2520paradigm%2520shift%2520in%25202D%2520camera-based%2520perception%2520towards%2520extracting%2520generalized%2520features%2520to%2520support%2520many%2520downstream%2520tasks.%2520Recent%2520works%2520introduce%2520self-supervised%2520cross-modal%2520knowledge%2520distillation%2520%2528KD%2529%2520as%2520a%2520way%2520to%2520transfer%2520these%2520powerful%2520generalization%2520capabilities%2520into%25203D%2520LiDAR-based%2520models.%2520However%252C%2520they%2520either%2520rely%2520on%2520highly%2520complex%2520distillation%2520losses%252C%2520pseudo-semantic%2520maps%252C%2520or%2520limit%2520KD%2520to%2520features%2520useful%2520for%2520semantic%2520segmentation%2520only.%2520In%2520this%2520work%252C%2520we%2520propose%2520CleverDistiller%252C%2520a%2520self-supervised%252C%2520cross-modal%25202D-to-3D%2520KD%2520framework%2520introducing%2520a%2520set%2520of%2520simple%2520yet%2520effective%2520design%2520choices%253A%2520Unlike%2520contrastive%2520approaches%2520relying%2520on%2520complex%2520loss%2520design%2520choices%252C%2520our%2520method%2520employs%2520a%2520direct%2520feature%2520similarity%2520loss%2520in%2520combination%2520with%2520a%2520multi%2520layer%2520perceptron%2520%2528MLP%2529%2520projection%2520head%2520to%2520allow%2520the%25203D%2520network%2520to%2520learn%2520complex%2520semantic%2520dependencies%2520throughout%2520the%2520projection.%2520Crucially%252C%2520our%2520approach%2520does%2520not%2520depend%2520on%2520pseudo-semantic%2520maps%252C%2520allowing%2520for%2520direct%2520knowledge%2520transfer%2520from%2520a%2520VFM%2520without%2520explicit%2520semantic%2520supervision.%2520Additionally%252C%2520we%2520introduce%2520the%2520auxiliary%2520self-supervised%2520spatial%2520task%2520of%2520occupancy%2520prediction%2520to%2520enhance%2520the%2520semantic%2520knowledge%252C%2520obtained%2520from%2520a%2520VFM%2520through%2520KD%252C%2520with%25203D%2520spatial%2520reasoning%2520capabilities.%2520Experiments%2520on%2520standard%2520autonomous%2520driving%2520benchmarks%2520for%25202D-to-3D%2520KD%2520demonstrate%2520that%2520CleverDistiller%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520semantic%2520segmentation%2520and%25203D%2520object%2520detection%2520%25283DOD%2529%2520by%2520up%2520to%252010%2525%2520mIoU%252C%2520especially%2520when%2520fine%2520tuning%2520on%2520really%2520low%2520data%2520amounts%252C%2520showing%2520the%2520effectiveness%2520of%2520our%2520simple%2520yet%2520powerful%2520KD%2520strategy%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09878v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CleverDistiller%3A%20Simple%20and%20Spatially%20Consistent%20Cross-modal%20Distillation&entry.906535625=Hariprasath%20Govindarajan%20and%20Maciej%20K.%20Wozniak%20and%20Marvin%20Klingner%20and%20Camille%20Maurice%20and%20B%20Ravi%20Kiran%20and%20Senthil%20Yogamani&entry.1292438233=Vision%20foundation%20models%20%28VFMs%29%20such%20as%20DINO%20have%20led%20to%20a%20paradigm%20shift%20in%202D%20camera-based%20perception%20towards%20extracting%20generalized%20features%20to%20support%20many%20downstream%20tasks.%20Recent%20works%20introduce%20self-supervised%20cross-modal%20knowledge%20distillation%20%28KD%29%20as%20a%20way%20to%20transfer%20these%20powerful%20generalization%20capabilities%20into%203D%20LiDAR-based%20models.%20However%2C%20they%20either%20rely%20on%20highly%20complex%20distillation%20losses%2C%20pseudo-semantic%20maps%2C%20or%20limit%20KD%20to%20features%20useful%20for%20semantic%20segmentation%20only.%20In%20this%20work%2C%20we%20propose%20CleverDistiller%2C%20a%20self-supervised%2C%20cross-modal%202D-to-3D%20KD%20framework%20introducing%20a%20set%20of%20simple%20yet%20effective%20design%20choices%3A%20Unlike%20contrastive%20approaches%20relying%20on%20complex%20loss%20design%20choices%2C%20our%20method%20employs%20a%20direct%20feature%20similarity%20loss%20in%20combination%20with%20a%20multi%20layer%20perceptron%20%28MLP%29%20projection%20head%20to%20allow%20the%203D%20network%20to%20learn%20complex%20semantic%20dependencies%20throughout%20the%20projection.%20Crucially%2C%20our%20approach%20does%20not%20depend%20on%20pseudo-semantic%20maps%2C%20allowing%20for%20direct%20knowledge%20transfer%20from%20a%20VFM%20without%20explicit%20semantic%20supervision.%20Additionally%2C%20we%20introduce%20the%20auxiliary%20self-supervised%20spatial%20task%20of%20occupancy%20prediction%20to%20enhance%20the%20semantic%20knowledge%2C%20obtained%20from%20a%20VFM%20through%20KD%2C%20with%203D%20spatial%20reasoning%20capabilities.%20Experiments%20on%20standard%20autonomous%20driving%20benchmarks%20for%202D-to-3D%20KD%20demonstrate%20that%20CleverDistiller%20achieves%20state-of-the-art%20performance%20in%20both%20semantic%20segmentation%20and%203D%20object%20detection%20%283DOD%29%20by%20up%20to%2010%25%20mIoU%2C%20especially%20when%20fine%20tuning%20on%20really%20low%20data%20amounts%2C%20showing%20the%20effectiveness%20of%20our%20simple%20yet%20powerful%20KD%20strategy&entry.1838667208=http%3A//arxiv.org/abs/2503.09878v3&entry.124074799=Read"},
{"title": "LLMInit: A Free Lunch from Large Language Models for Selective Initialization of Recommendation", "author": "Weizhi Zhang and Liangwei Yang and Wooseong Yang and Henry Peng Zou and Yuqing Liu and Ke Xu and Sourav Medya and Philip S. Yu", "abstract": "Collaborative filtering (CF) is widely adopted in industrial recommender systems (RecSys) for modeling user-item interactions across numerous applications, but often struggles with cold-start and data-sparse scenarios. Recent advancements in pre-trained large language models (LLMs) with rich semantic knowledge, offer promising solutions to these challenges. However, deploying LLMs at scale is hindered by their significant computational demands and latency. In this paper, we propose a novel and scalable LLM-RecSys framework, LLMInit, designed to integrate pretrained LLM embeddings into CF models through selective initialization strategies. Specifically, we identify the embedding collapse issue observed when CF models scale and match the large embedding sizes in LLMs and avoid the problem by introducing efficient sampling methods, including, random, uniform, and variance-based selections. Comprehensive experiments conducted on multiple real-world datasets demonstrate that LLMInit significantly improves recommendation performance while maintaining low computational costs, offering a practical and scalable solution for industrial applications. To facilitate industry adoption and promote future research, we provide open-source access to our implementation at https://github.com/DavidZWZ/LLMInit.", "link": "http://arxiv.org/abs/2503.01814v2", "date": "2025-11-20", "relevancy": 2.2938, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMInit%3A%20A%20Free%20Lunch%20from%20Large%20Language%20Models%20for%20Selective%20Initialization%20of%20Recommendation&body=Title%3A%20LLMInit%3A%20A%20Free%20Lunch%20from%20Large%20Language%20Models%20for%20Selective%20Initialization%20of%20Recommendation%0AAuthor%3A%20Weizhi%20Zhang%20and%20Liangwei%20Yang%20and%20Wooseong%20Yang%20and%20Henry%20Peng%20Zou%20and%20Yuqing%20Liu%20and%20Ke%20Xu%20and%20Sourav%20Medya%20and%20Philip%20S.%20Yu%0AAbstract%3A%20Collaborative%20filtering%20%28CF%29%20is%20widely%20adopted%20in%20industrial%20recommender%20systems%20%28RecSys%29%20for%20modeling%20user-item%20interactions%20across%20numerous%20applications%2C%20but%20often%20struggles%20with%20cold-start%20and%20data-sparse%20scenarios.%20Recent%20advancements%20in%20pre-trained%20large%20language%20models%20%28LLMs%29%20with%20rich%20semantic%20knowledge%2C%20offer%20promising%20solutions%20to%20these%20challenges.%20However%2C%20deploying%20LLMs%20at%20scale%20is%20hindered%20by%20their%20significant%20computational%20demands%20and%20latency.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20scalable%20LLM-RecSys%20framework%2C%20LLMInit%2C%20designed%20to%20integrate%20pretrained%20LLM%20embeddings%20into%20CF%20models%20through%20selective%20initialization%20strategies.%20Specifically%2C%20we%20identify%20the%20embedding%20collapse%20issue%20observed%20when%20CF%20models%20scale%20and%20match%20the%20large%20embedding%20sizes%20in%20LLMs%20and%20avoid%20the%20problem%20by%20introducing%20efficient%20sampling%20methods%2C%20including%2C%20random%2C%20uniform%2C%20and%20variance-based%20selections.%20Comprehensive%20experiments%20conducted%20on%20multiple%20real-world%20datasets%20demonstrate%20that%20LLMInit%20significantly%20improves%20recommendation%20performance%20while%20maintaining%20low%20computational%20costs%2C%20offering%20a%20practical%20and%20scalable%20solution%20for%20industrial%20applications.%20To%20facilitate%20industry%20adoption%20and%20promote%20future%20research%2C%20we%20provide%20open-source%20access%20to%20our%20implementation%20at%20https%3A//github.com/DavidZWZ/LLMInit.%0ALink%3A%20http%3A//arxiv.org/abs/2503.01814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMInit%253A%2520A%2520Free%2520Lunch%2520from%2520Large%2520Language%2520Models%2520for%2520Selective%2520Initialization%2520of%2520Recommendation%26entry.906535625%3DWeizhi%2520Zhang%2520and%2520Liangwei%2520Yang%2520and%2520Wooseong%2520Yang%2520and%2520Henry%2520Peng%2520Zou%2520and%2520Yuqing%2520Liu%2520and%2520Ke%2520Xu%2520and%2520Sourav%2520Medya%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3DCollaborative%2520filtering%2520%2528CF%2529%2520is%2520widely%2520adopted%2520in%2520industrial%2520recommender%2520systems%2520%2528RecSys%2529%2520for%2520modeling%2520user-item%2520interactions%2520across%2520numerous%2520applications%252C%2520but%2520often%2520struggles%2520with%2520cold-start%2520and%2520data-sparse%2520scenarios.%2520Recent%2520advancements%2520in%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520rich%2520semantic%2520knowledge%252C%2520offer%2520promising%2520solutions%2520to%2520these%2520challenges.%2520However%252C%2520deploying%2520LLMs%2520at%2520scale%2520is%2520hindered%2520by%2520their%2520significant%2520computational%2520demands%2520and%2520latency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520and%2520scalable%2520LLM-RecSys%2520framework%252C%2520LLMInit%252C%2520designed%2520to%2520integrate%2520pretrained%2520LLM%2520embeddings%2520into%2520CF%2520models%2520through%2520selective%2520initialization%2520strategies.%2520Specifically%252C%2520we%2520identify%2520the%2520embedding%2520collapse%2520issue%2520observed%2520when%2520CF%2520models%2520scale%2520and%2520match%2520the%2520large%2520embedding%2520sizes%2520in%2520LLMs%2520and%2520avoid%2520the%2520problem%2520by%2520introducing%2520efficient%2520sampling%2520methods%252C%2520including%252C%2520random%252C%2520uniform%252C%2520and%2520variance-based%2520selections.%2520Comprehensive%2520experiments%2520conducted%2520on%2520multiple%2520real-world%2520datasets%2520demonstrate%2520that%2520LLMInit%2520significantly%2520improves%2520recommendation%2520performance%2520while%2520maintaining%2520low%2520computational%2520costs%252C%2520offering%2520a%2520practical%2520and%2520scalable%2520solution%2520for%2520industrial%2520applications.%2520To%2520facilitate%2520industry%2520adoption%2520and%2520promote%2520future%2520research%252C%2520we%2520provide%2520open-source%2520access%2520to%2520our%2520implementation%2520at%2520https%253A//github.com/DavidZWZ/LLMInit.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMInit%3A%20A%20Free%20Lunch%20from%20Large%20Language%20Models%20for%20Selective%20Initialization%20of%20Recommendation&entry.906535625=Weizhi%20Zhang%20and%20Liangwei%20Yang%20and%20Wooseong%20Yang%20and%20Henry%20Peng%20Zou%20and%20Yuqing%20Liu%20and%20Ke%20Xu%20and%20Sourav%20Medya%20and%20Philip%20S.%20Yu&entry.1292438233=Collaborative%20filtering%20%28CF%29%20is%20widely%20adopted%20in%20industrial%20recommender%20systems%20%28RecSys%29%20for%20modeling%20user-item%20interactions%20across%20numerous%20applications%2C%20but%20often%20struggles%20with%20cold-start%20and%20data-sparse%20scenarios.%20Recent%20advancements%20in%20pre-trained%20large%20language%20models%20%28LLMs%29%20with%20rich%20semantic%20knowledge%2C%20offer%20promising%20solutions%20to%20these%20challenges.%20However%2C%20deploying%20LLMs%20at%20scale%20is%20hindered%20by%20their%20significant%20computational%20demands%20and%20latency.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20scalable%20LLM-RecSys%20framework%2C%20LLMInit%2C%20designed%20to%20integrate%20pretrained%20LLM%20embeddings%20into%20CF%20models%20through%20selective%20initialization%20strategies.%20Specifically%2C%20we%20identify%20the%20embedding%20collapse%20issue%20observed%20when%20CF%20models%20scale%20and%20match%20the%20large%20embedding%20sizes%20in%20LLMs%20and%20avoid%20the%20problem%20by%20introducing%20efficient%20sampling%20methods%2C%20including%2C%20random%2C%20uniform%2C%20and%20variance-based%20selections.%20Comprehensive%20experiments%20conducted%20on%20multiple%20real-world%20datasets%20demonstrate%20that%20LLMInit%20significantly%20improves%20recommendation%20performance%20while%20maintaining%20low%20computational%20costs%2C%20offering%20a%20practical%20and%20scalable%20solution%20for%20industrial%20applications.%20To%20facilitate%20industry%20adoption%20and%20promote%20future%20research%2C%20we%20provide%20open-source%20access%20to%20our%20implementation%20at%20https%3A//github.com/DavidZWZ/LLMInit.&entry.1838667208=http%3A//arxiv.org/abs/2503.01814v2&entry.124074799=Read"},
{"title": "Optimizing Quantum Key Distribution Network Performance using Graph Neural Networks", "author": "Akshit Pramod Anchan and Ameiy Acharya and Leki Chom Thungon", "abstract": "This paper proposes an optimization of Quantum Key Distribution (QKD) Networks using Graph Neural Networks (GNN) framework. Today, the development of quantum computers threatens the security systems of classical cryptography. Moreover, as QKD networks are designed for protecting secret communication, they suffer from multiple operational difficulties: adaptive to dynamic conditions, optimization for multiple parameters and effective resource utilization. In order to overcome these obstacles, we propose a GNN-based framework which can model QKD networks as dynamic graphs and extracts exploitable characteristics from these networks' structure. The graph contains not only topological information but also specific characteristics associated with quantum communication (the number of edges between nodes, etc). Experimental results demonstrate that the GNN-optimized QKD network achieves a substantial increase in total key rate (from 27.1 Kbits/s to 470 Kbits/s), a reduced average QBER (from 6.6% to 6.0%), and maintains path integrity with a slight reduction in average transmission distance (from 7.13 km to 6.42 km). Furthermore, we analyze network performance across varying scales (10 to 250 nodes), showing improved link prediction accuracy and enhanced key generation rate in medium-sized networks. This work introduces a novel operation mode for QKD networks, shifting the paradigm of network optimization through adaptive and scalable quantum communication systems that enhance security and performance.", "link": "http://arxiv.org/abs/2511.16468v1", "date": "2025-11-20", "relevancy": 2.2897, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5044}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4417}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Quantum%20Key%20Distribution%20Network%20Performance%20using%20Graph%20Neural%20Networks&body=Title%3A%20Optimizing%20Quantum%20Key%20Distribution%20Network%20Performance%20using%20Graph%20Neural%20Networks%0AAuthor%3A%20Akshit%20Pramod%20Anchan%20and%20Ameiy%20Acharya%20and%20Leki%20Chom%20Thungon%0AAbstract%3A%20This%20paper%20proposes%20an%20optimization%20of%20Quantum%20Key%20Distribution%20%28QKD%29%20Networks%20using%20Graph%20Neural%20Networks%20%28GNN%29%20framework.%20Today%2C%20the%20development%20of%20quantum%20computers%20threatens%20the%20security%20systems%20of%20classical%20cryptography.%20Moreover%2C%20as%20QKD%20networks%20are%20designed%20for%20protecting%20secret%20communication%2C%20they%20suffer%20from%20multiple%20operational%20difficulties%3A%20adaptive%20to%20dynamic%20conditions%2C%20optimization%20for%20multiple%20parameters%20and%20effective%20resource%20utilization.%20In%20order%20to%20overcome%20these%20obstacles%2C%20we%20propose%20a%20GNN-based%20framework%20which%20can%20model%20QKD%20networks%20as%20dynamic%20graphs%20and%20extracts%20exploitable%20characteristics%20from%20these%20networks%27%20structure.%20The%20graph%20contains%20not%20only%20topological%20information%20but%20also%20specific%20characteristics%20associated%20with%20quantum%20communication%20%28the%20number%20of%20edges%20between%20nodes%2C%20etc%29.%20Experimental%20results%20demonstrate%20that%20the%20GNN-optimized%20QKD%20network%20achieves%20a%20substantial%20increase%20in%20total%20key%20rate%20%28from%2027.1%20Kbits/s%20to%20470%20Kbits/s%29%2C%20a%20reduced%20average%20QBER%20%28from%206.6%25%20to%206.0%25%29%2C%20and%20maintains%20path%20integrity%20with%20a%20slight%20reduction%20in%20average%20transmission%20distance%20%28from%207.13%20km%20to%206.42%20km%29.%20Furthermore%2C%20we%20analyze%20network%20performance%20across%20varying%20scales%20%2810%20to%20250%20nodes%29%2C%20showing%20improved%20link%20prediction%20accuracy%20and%20enhanced%20key%20generation%20rate%20in%20medium-sized%20networks.%20This%20work%20introduces%20a%20novel%20operation%20mode%20for%20QKD%20networks%2C%20shifting%20the%20paradigm%20of%20network%20optimization%20through%20adaptive%20and%20scalable%20quantum%20communication%20systems%20that%20enhance%20security%20and%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Quantum%2520Key%2520Distribution%2520Network%2520Performance%2520using%2520Graph%2520Neural%2520Networks%26entry.906535625%3DAkshit%2520Pramod%2520Anchan%2520and%2520Ameiy%2520Acharya%2520and%2520Leki%2520Chom%2520Thungon%26entry.1292438233%3DThis%2520paper%2520proposes%2520an%2520optimization%2520of%2520Quantum%2520Key%2520Distribution%2520%2528QKD%2529%2520Networks%2520using%2520Graph%2520Neural%2520Networks%2520%2528GNN%2529%2520framework.%2520Today%252C%2520the%2520development%2520of%2520quantum%2520computers%2520threatens%2520the%2520security%2520systems%2520of%2520classical%2520cryptography.%2520Moreover%252C%2520as%2520QKD%2520networks%2520are%2520designed%2520for%2520protecting%2520secret%2520communication%252C%2520they%2520suffer%2520from%2520multiple%2520operational%2520difficulties%253A%2520adaptive%2520to%2520dynamic%2520conditions%252C%2520optimization%2520for%2520multiple%2520parameters%2520and%2520effective%2520resource%2520utilization.%2520In%2520order%2520to%2520overcome%2520these%2520obstacles%252C%2520we%2520propose%2520a%2520GNN-based%2520framework%2520which%2520can%2520model%2520QKD%2520networks%2520as%2520dynamic%2520graphs%2520and%2520extracts%2520exploitable%2520characteristics%2520from%2520these%2520networks%2527%2520structure.%2520The%2520graph%2520contains%2520not%2520only%2520topological%2520information%2520but%2520also%2520specific%2520characteristics%2520associated%2520with%2520quantum%2520communication%2520%2528the%2520number%2520of%2520edges%2520between%2520nodes%252C%2520etc%2529.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520GNN-optimized%2520QKD%2520network%2520achieves%2520a%2520substantial%2520increase%2520in%2520total%2520key%2520rate%2520%2528from%252027.1%2520Kbits/s%2520to%2520470%2520Kbits/s%2529%252C%2520a%2520reduced%2520average%2520QBER%2520%2528from%25206.6%2525%2520to%25206.0%2525%2529%252C%2520and%2520maintains%2520path%2520integrity%2520with%2520a%2520slight%2520reduction%2520in%2520average%2520transmission%2520distance%2520%2528from%25207.13%2520km%2520to%25206.42%2520km%2529.%2520Furthermore%252C%2520we%2520analyze%2520network%2520performance%2520across%2520varying%2520scales%2520%252810%2520to%2520250%2520nodes%2529%252C%2520showing%2520improved%2520link%2520prediction%2520accuracy%2520and%2520enhanced%2520key%2520generation%2520rate%2520in%2520medium-sized%2520networks.%2520This%2520work%2520introduces%2520a%2520novel%2520operation%2520mode%2520for%2520QKD%2520networks%252C%2520shifting%2520the%2520paradigm%2520of%2520network%2520optimization%2520through%2520adaptive%2520and%2520scalable%2520quantum%2520communication%2520systems%2520that%2520enhance%2520security%2520and%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Quantum%20Key%20Distribution%20Network%20Performance%20using%20Graph%20Neural%20Networks&entry.906535625=Akshit%20Pramod%20Anchan%20and%20Ameiy%20Acharya%20and%20Leki%20Chom%20Thungon&entry.1292438233=This%20paper%20proposes%20an%20optimization%20of%20Quantum%20Key%20Distribution%20%28QKD%29%20Networks%20using%20Graph%20Neural%20Networks%20%28GNN%29%20framework.%20Today%2C%20the%20development%20of%20quantum%20computers%20threatens%20the%20security%20systems%20of%20classical%20cryptography.%20Moreover%2C%20as%20QKD%20networks%20are%20designed%20for%20protecting%20secret%20communication%2C%20they%20suffer%20from%20multiple%20operational%20difficulties%3A%20adaptive%20to%20dynamic%20conditions%2C%20optimization%20for%20multiple%20parameters%20and%20effective%20resource%20utilization.%20In%20order%20to%20overcome%20these%20obstacles%2C%20we%20propose%20a%20GNN-based%20framework%20which%20can%20model%20QKD%20networks%20as%20dynamic%20graphs%20and%20extracts%20exploitable%20characteristics%20from%20these%20networks%27%20structure.%20The%20graph%20contains%20not%20only%20topological%20information%20but%20also%20specific%20characteristics%20associated%20with%20quantum%20communication%20%28the%20number%20of%20edges%20between%20nodes%2C%20etc%29.%20Experimental%20results%20demonstrate%20that%20the%20GNN-optimized%20QKD%20network%20achieves%20a%20substantial%20increase%20in%20total%20key%20rate%20%28from%2027.1%20Kbits/s%20to%20470%20Kbits/s%29%2C%20a%20reduced%20average%20QBER%20%28from%206.6%25%20to%206.0%25%29%2C%20and%20maintains%20path%20integrity%20with%20a%20slight%20reduction%20in%20average%20transmission%20distance%20%28from%207.13%20km%20to%206.42%20km%29.%20Furthermore%2C%20we%20analyze%20network%20performance%20across%20varying%20scales%20%2810%20to%20250%20nodes%29%2C%20showing%20improved%20link%20prediction%20accuracy%20and%20enhanced%20key%20generation%20rate%20in%20medium-sized%20networks.%20This%20work%20introduces%20a%20novel%20operation%20mode%20for%20QKD%20networks%2C%20shifting%20the%20paradigm%20of%20network%20optimization%20through%20adaptive%20and%20scalable%20quantum%20communication%20systems%20that%20enhance%20security%20and%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.16468v1&entry.124074799=Read"},
{"title": "Generative AI, Managerial Expectations, and Economic Activity", "author": "Manish Jha and Jialin Qian and Michael Weber and Baozhong Yang", "abstract": "We use generative AI to extract managerial expectations about their economic outlook from 120,000+ corporate conference call transcripts. The resulting AI Economy Score predicts GDP growth, production, and employment up to 10 quarters ahead, beyond existing measures like survey forecasts. Moreover, industry and firm-level measures provide valuable information about sector-specific and individual firm activities. A composite measure that integrates managerial expectations about firm, industry, and macroeconomic conditions further significantly improves the forecasting power and predictive horizon of national and sectoral growth. Our findings show managerial expectations offer unique insights into economic activity, with implications for both macroeconomic and microeconomic decision-making.", "link": "http://arxiv.org/abs/2410.03897v4", "date": "2025-11-20", "relevancy": 2.2888, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5011}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4415}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%2C%20Managerial%20Expectations%2C%20and%20Economic%20Activity&body=Title%3A%20Generative%20AI%2C%20Managerial%20Expectations%2C%20and%20Economic%20Activity%0AAuthor%3A%20Manish%20Jha%20and%20Jialin%20Qian%20and%20Michael%20Weber%20and%20Baozhong%20Yang%0AAbstract%3A%20We%20use%20generative%20AI%20to%20extract%20managerial%20expectations%20about%20their%20economic%20outlook%20from%20120%2C000%2B%20corporate%20conference%20call%20transcripts.%20The%20resulting%20AI%20Economy%20Score%20predicts%20GDP%20growth%2C%20production%2C%20and%20employment%20up%20to%2010%20quarters%20ahead%2C%20beyond%20existing%20measures%20like%20survey%20forecasts.%20Moreover%2C%20industry%20and%20firm-level%20measures%20provide%20valuable%20information%20about%20sector-specific%20and%20individual%20firm%20activities.%20A%20composite%20measure%20that%20integrates%20managerial%20expectations%20about%20firm%2C%20industry%2C%20and%20macroeconomic%20conditions%20further%20significantly%20improves%20the%20forecasting%20power%20and%20predictive%20horizon%20of%20national%20and%20sectoral%20growth.%20Our%20findings%20show%20managerial%20expectations%20offer%20unique%20insights%20into%20economic%20activity%2C%20with%20implications%20for%20both%20macroeconomic%20and%20microeconomic%20decision-making.%0ALink%3A%20http%3A//arxiv.org/abs/2410.03897v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%252C%2520Managerial%2520Expectations%252C%2520and%2520Economic%2520Activity%26entry.906535625%3DManish%2520Jha%2520and%2520Jialin%2520Qian%2520and%2520Michael%2520Weber%2520and%2520Baozhong%2520Yang%26entry.1292438233%3DWe%2520use%2520generative%2520AI%2520to%2520extract%2520managerial%2520expectations%2520about%2520their%2520economic%2520outlook%2520from%2520120%252C000%252B%2520corporate%2520conference%2520call%2520transcripts.%2520The%2520resulting%2520AI%2520Economy%2520Score%2520predicts%2520GDP%2520growth%252C%2520production%252C%2520and%2520employment%2520up%2520to%252010%2520quarters%2520ahead%252C%2520beyond%2520existing%2520measures%2520like%2520survey%2520forecasts.%2520Moreover%252C%2520industry%2520and%2520firm-level%2520measures%2520provide%2520valuable%2520information%2520about%2520sector-specific%2520and%2520individual%2520firm%2520activities.%2520A%2520composite%2520measure%2520that%2520integrates%2520managerial%2520expectations%2520about%2520firm%252C%2520industry%252C%2520and%2520macroeconomic%2520conditions%2520further%2520significantly%2520improves%2520the%2520forecasting%2520power%2520and%2520predictive%2520horizon%2520of%2520national%2520and%2520sectoral%2520growth.%2520Our%2520findings%2520show%2520managerial%2520expectations%2520offer%2520unique%2520insights%2520into%2520economic%2520activity%252C%2520with%2520implications%2520for%2520both%2520macroeconomic%2520and%2520microeconomic%2520decision-making.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03897v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%2C%20Managerial%20Expectations%2C%20and%20Economic%20Activity&entry.906535625=Manish%20Jha%20and%20Jialin%20Qian%20and%20Michael%20Weber%20and%20Baozhong%20Yang&entry.1292438233=We%20use%20generative%20AI%20to%20extract%20managerial%20expectations%20about%20their%20economic%20outlook%20from%20120%2C000%2B%20corporate%20conference%20call%20transcripts.%20The%20resulting%20AI%20Economy%20Score%20predicts%20GDP%20growth%2C%20production%2C%20and%20employment%20up%20to%2010%20quarters%20ahead%2C%20beyond%20existing%20measures%20like%20survey%20forecasts.%20Moreover%2C%20industry%20and%20firm-level%20measures%20provide%20valuable%20information%20about%20sector-specific%20and%20individual%20firm%20activities.%20A%20composite%20measure%20that%20integrates%20managerial%20expectations%20about%20firm%2C%20industry%2C%20and%20macroeconomic%20conditions%20further%20significantly%20improves%20the%20forecasting%20power%20and%20predictive%20horizon%20of%20national%20and%20sectoral%20growth.%20Our%20findings%20show%20managerial%20expectations%20offer%20unique%20insights%20into%20economic%20activity%2C%20with%20implications%20for%20both%20macroeconomic%20and%20microeconomic%20decision-making.&entry.1838667208=http%3A//arxiv.org/abs/2410.03897v4&entry.124074799=Read"},
{"title": "A Decade of You Only Look Once (YOLO) for Object Detection: A Review", "author": "Leo Thomas Ramos and Angel D. Sappa", "abstract": "This review marks the tenth anniversary of You Only Look Once (YOLO), one of the most influential frameworks in real-time object detection. Over the past decade, YOLO has evolved from a streamlined detector into a diverse family of architectures characterized by efficient design, modular scalability, and cross-domain adaptability. The paper presents a technical overview of the main versions (from YOLOv1 to YOLOv13), highlights key architectural trends, and surveys the principal application areas in which YOLO has been adopted. It also addresses evaluation practices, ethical considerations, and potential future directions for the framework's continued development. The analysis aims to provide a comprehensive and critical perspective on YOLO's trajectory and ongoing transformation.", "link": "http://arxiv.org/abs/2504.18586v3", "date": "2025-11-20", "relevancy": 2.2844, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Decade%20of%20You%20Only%20Look%20Once%20%28YOLO%29%20for%20Object%20Detection%3A%20A%20Review&body=Title%3A%20A%20Decade%20of%20You%20Only%20Look%20Once%20%28YOLO%29%20for%20Object%20Detection%3A%20A%20Review%0AAuthor%3A%20Leo%20Thomas%20Ramos%20and%20Angel%20D.%20Sappa%0AAbstract%3A%20This%20review%20marks%20the%20tenth%20anniversary%20of%20You%20Only%20Look%20Once%20%28YOLO%29%2C%20one%20of%20the%20most%20influential%20frameworks%20in%20real-time%20object%20detection.%20Over%20the%20past%20decade%2C%20YOLO%20has%20evolved%20from%20a%20streamlined%20detector%20into%20a%20diverse%20family%20of%20architectures%20characterized%20by%20efficient%20design%2C%20modular%20scalability%2C%20and%20cross-domain%20adaptability.%20The%20paper%20presents%20a%20technical%20overview%20of%20the%20main%20versions%20%28from%20YOLOv1%20to%20YOLOv13%29%2C%20highlights%20key%20architectural%20trends%2C%20and%20surveys%20the%20principal%20application%20areas%20in%20which%20YOLO%20has%20been%20adopted.%20It%20also%20addresses%20evaluation%20practices%2C%20ethical%20considerations%2C%20and%20potential%20future%20directions%20for%20the%20framework%27s%20continued%20development.%20The%20analysis%20aims%20to%20provide%20a%20comprehensive%20and%20critical%20perspective%20on%20YOLO%27s%20trajectory%20and%20ongoing%20transformation.%0ALink%3A%20http%3A//arxiv.org/abs/2504.18586v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Decade%2520of%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520for%2520Object%2520Detection%253A%2520A%2520Review%26entry.906535625%3DLeo%2520Thomas%2520Ramos%2520and%2520Angel%2520D.%2520Sappa%26entry.1292438233%3DThis%2520review%2520marks%2520the%2520tenth%2520anniversary%2520of%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%252C%2520one%2520of%2520the%2520most%2520influential%2520frameworks%2520in%2520real-time%2520object%2520detection.%2520Over%2520the%2520past%2520decade%252C%2520YOLO%2520has%2520evolved%2520from%2520a%2520streamlined%2520detector%2520into%2520a%2520diverse%2520family%2520of%2520architectures%2520characterized%2520by%2520efficient%2520design%252C%2520modular%2520scalability%252C%2520and%2520cross-domain%2520adaptability.%2520The%2520paper%2520presents%2520a%2520technical%2520overview%2520of%2520the%2520main%2520versions%2520%2528from%2520YOLOv1%2520to%2520YOLOv13%2529%252C%2520highlights%2520key%2520architectural%2520trends%252C%2520and%2520surveys%2520the%2520principal%2520application%2520areas%2520in%2520which%2520YOLO%2520has%2520been%2520adopted.%2520It%2520also%2520addresses%2520evaluation%2520practices%252C%2520ethical%2520considerations%252C%2520and%2520potential%2520future%2520directions%2520for%2520the%2520framework%2527s%2520continued%2520development.%2520The%2520analysis%2520aims%2520to%2520provide%2520a%2520comprehensive%2520and%2520critical%2520perspective%2520on%2520YOLO%2527s%2520trajectory%2520and%2520ongoing%2520transformation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18586v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Decade%20of%20You%20Only%20Look%20Once%20%28YOLO%29%20for%20Object%20Detection%3A%20A%20Review&entry.906535625=Leo%20Thomas%20Ramos%20and%20Angel%20D.%20Sappa&entry.1292438233=This%20review%20marks%20the%20tenth%20anniversary%20of%20You%20Only%20Look%20Once%20%28YOLO%29%2C%20one%20of%20the%20most%20influential%20frameworks%20in%20real-time%20object%20detection.%20Over%20the%20past%20decade%2C%20YOLO%20has%20evolved%20from%20a%20streamlined%20detector%20into%20a%20diverse%20family%20of%20architectures%20characterized%20by%20efficient%20design%2C%20modular%20scalability%2C%20and%20cross-domain%20adaptability.%20The%20paper%20presents%20a%20technical%20overview%20of%20the%20main%20versions%20%28from%20YOLOv1%20to%20YOLOv13%29%2C%20highlights%20key%20architectural%20trends%2C%20and%20surveys%20the%20principal%20application%20areas%20in%20which%20YOLO%20has%20been%20adopted.%20It%20also%20addresses%20evaluation%20practices%2C%20ethical%20considerations%2C%20and%20potential%20future%20directions%20for%20the%20framework%27s%20continued%20development.%20The%20analysis%20aims%20to%20provide%20a%20comprehensive%20and%20critical%20perspective%20on%20YOLO%27s%20trajectory%20and%20ongoing%20transformation.&entry.1838667208=http%3A//arxiv.org/abs/2504.18586v3&entry.124074799=Read"},
{"title": "LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging", "author": "Seungeon Lee and Soumi Das and Manish Gupta and Krishna P. Gummadi", "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models. However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.", "link": "http://arxiv.org/abs/2511.07129v2", "date": "2025-11-20", "relevancy": 2.2832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4613}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.461}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA%20on%20the%20Go%3A%20Instance-level%20Dynamic%20LoRA%20Selection%20and%20Merging&body=Title%3A%20LoRA%20on%20the%20Go%3A%20Instance-level%20Dynamic%20LoRA%20Selection%20and%20Merging%0AAuthor%3A%20Seungeon%20Lee%20and%20Soumi%20Das%20and%20Manish%20Gupta%20and%20Krishna%20P.%20Gummadi%0AAbstract%3A%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20emerged%20as%20a%20parameter-efficient%20approach%20for%20fine-tuning%20large%20language%20models.%20However%2C%20conventional%20LoRA%20adapters%20are%20typically%20trained%20for%20a%20single%20task%2C%20limiting%20their%20applicability%20in%20real-world%20settings%20where%20inputs%20may%20span%20diverse%20and%20unpredictable%20domains.%20At%20inference%20time%2C%20existing%20approaches%20combine%20multiple%20LoRAs%20for%20improving%20performance%20on%20diverse%20tasks%2C%20while%20usually%20requiring%20labeled%20data%20or%20additional%20task-specific%20training%2C%20which%20is%20expensive%20at%20scale.%20In%20this%20work%2C%20we%20introduce%20LoRA%20on%20the%20Go%20%28LoGo%29%2C%20a%20training-free%20framework%20that%20dynamically%20selects%20and%20merges%20adapters%20at%20the%20instance%20level%20without%20any%20additional%20requirements.%20LoGo%20leverages%20signals%20extracted%20from%20a%20single%20forward%20pass%20through%20LoRA%20adapters%2C%20to%20identify%20the%20most%20relevant%20adapters%20and%20determine%20their%20contributions%20on-the-fly.%20Across%205%20NLP%20benchmarks%2C%2027%20datasets%2C%20and%203%20model%20families%2C%20LoGo%20outperforms%20training-based%20baselines%20on%20some%20tasks%20upto%20a%20margin%20of%203.6%25%20while%20remaining%20competitive%20on%20other%20tasks%20and%20maintaining%20inference%20throughput%2C%20highlighting%20its%20effectiveness%20and%20practicality.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA%2520on%2520the%2520Go%253A%2520Instance-level%2520Dynamic%2520LoRA%2520Selection%2520and%2520Merging%26entry.906535625%3DSeungeon%2520Lee%2520and%2520Soumi%2520Das%2520and%2520Manish%2520Gupta%2520and%2520Krishna%2520P.%2520Gummadi%26entry.1292438233%3DLow-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520emerged%2520as%2520a%2520parameter-efficient%2520approach%2520for%2520fine-tuning%2520large%2520language%2520models.%2520However%252C%2520conventional%2520LoRA%2520adapters%2520are%2520typically%2520trained%2520for%2520a%2520single%2520task%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%2520settings%2520where%2520inputs%2520may%2520span%2520diverse%2520and%2520unpredictable%2520domains.%2520At%2520inference%2520time%252C%2520existing%2520approaches%2520combine%2520multiple%2520LoRAs%2520for%2520improving%2520performance%2520on%2520diverse%2520tasks%252C%2520while%2520usually%2520requiring%2520labeled%2520data%2520or%2520additional%2520task-specific%2520training%252C%2520which%2520is%2520expensive%2520at%2520scale.%2520In%2520this%2520work%252C%2520we%2520introduce%2520LoRA%2520on%2520the%2520Go%2520%2528LoGo%2529%252C%2520a%2520training-free%2520framework%2520that%2520dynamically%2520selects%2520and%2520merges%2520adapters%2520at%2520the%2520instance%2520level%2520without%2520any%2520additional%2520requirements.%2520LoGo%2520leverages%2520signals%2520extracted%2520from%2520a%2520single%2520forward%2520pass%2520through%2520LoRA%2520adapters%252C%2520to%2520identify%2520the%2520most%2520relevant%2520adapters%2520and%2520determine%2520their%2520contributions%2520on-the-fly.%2520Across%25205%2520NLP%2520benchmarks%252C%252027%2520datasets%252C%2520and%25203%2520model%2520families%252C%2520LoGo%2520outperforms%2520training-based%2520baselines%2520on%2520some%2520tasks%2520upto%2520a%2520margin%2520of%25203.6%2525%2520while%2520remaining%2520competitive%2520on%2520other%2520tasks%2520and%2520maintaining%2520inference%2520throughput%252C%2520highlighting%2520its%2520effectiveness%2520and%2520practicality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA%20on%20the%20Go%3A%20Instance-level%20Dynamic%20LoRA%20Selection%20and%20Merging&entry.906535625=Seungeon%20Lee%20and%20Soumi%20Das%20and%20Manish%20Gupta%20and%20Krishna%20P.%20Gummadi&entry.1292438233=Low-Rank%20Adaptation%20%28LoRA%29%20has%20emerged%20as%20a%20parameter-efficient%20approach%20for%20fine-tuning%20large%20language%20models.%20However%2C%20conventional%20LoRA%20adapters%20are%20typically%20trained%20for%20a%20single%20task%2C%20limiting%20their%20applicability%20in%20real-world%20settings%20where%20inputs%20may%20span%20diverse%20and%20unpredictable%20domains.%20At%20inference%20time%2C%20existing%20approaches%20combine%20multiple%20LoRAs%20for%20improving%20performance%20on%20diverse%20tasks%2C%20while%20usually%20requiring%20labeled%20data%20or%20additional%20task-specific%20training%2C%20which%20is%20expensive%20at%20scale.%20In%20this%20work%2C%20we%20introduce%20LoRA%20on%20the%20Go%20%28LoGo%29%2C%20a%20training-free%20framework%20that%20dynamically%20selects%20and%20merges%20adapters%20at%20the%20instance%20level%20without%20any%20additional%20requirements.%20LoGo%20leverages%20signals%20extracted%20from%20a%20single%20forward%20pass%20through%20LoRA%20adapters%2C%20to%20identify%20the%20most%20relevant%20adapters%20and%20determine%20their%20contributions%20on-the-fly.%20Across%205%20NLP%20benchmarks%2C%2027%20datasets%2C%20and%203%20model%20families%2C%20LoGo%20outperforms%20training-based%20baselines%20on%20some%20tasks%20upto%20a%20margin%20of%203.6%25%20while%20remaining%20competitive%20on%20other%20tasks%20and%20maintaining%20inference%20throughput%2C%20highlighting%20its%20effectiveness%20and%20practicality.&entry.1838667208=http%3A//arxiv.org/abs/2511.07129v2&entry.124074799=Read"},
{"title": "Complex variational autoencoders admit K\u00e4hler structure", "author": "Andrew Gracyk", "abstract": "It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level K\u00e4hler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric K\u00e4hler potential relation is exactly achieved under relative entropy. We propose a K\u00e4hler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying K\u00e4hler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.", "link": "http://arxiv.org/abs/2511.15172v2", "date": "2025-11-20", "relevancy": 2.2645, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complex%20variational%20autoencoders%20admit%20K%C3%A4hler%20structure&body=Title%3A%20Complex%20variational%20autoencoders%20admit%20K%C3%A4hler%20structure%0AAuthor%3A%20Andrew%20Gracyk%0AAbstract%3A%20It%20has%20been%20discovered%20that%20latent-Euclidean%20variational%20autoencoders%20%28VAEs%29%20admit%2C%20in%20various%20capacities%2C%20Riemannian%20structure.%20We%20adapt%20these%20arguments%20but%20for%20complex%20VAEs%20with%20a%20complex%20latent%20stage.%20We%20show%20that%20complex%20VAEs%20reveal%20to%20some%20level%20K%C3%A4hler%20geometric%20structure.%20Our%20methods%20will%20be%20tailored%20for%20decoder%20geometry.%20We%20derive%20the%20Fisher%20information%20metric%20in%20the%20complex%20case%20under%20a%20latent%20complex%20Gaussian%20regularization%20with%20trivial%20relation%20matrix.%20It%20is%20well%20known%20from%20statistical%20information%20theory%20that%20the%20Fisher%20information%20coincides%20with%20the%20Hessian%20of%20the%20Kullback-Leibler%20%28KL%29%20divergence.%20Thus%2C%20the%20metric%20K%C3%A4hler%20potential%20relation%20is%20exactly%20achieved%20under%20relative%20entropy.%20We%20propose%20a%20K%C3%A4hler%20potential%20derivative%20of%20complex%20Gaussian%20mixtures%20that%20has%20rough%20equivalence%20to%20the%20Fisher%20information%20metric%20while%20still%20being%20faithful%20to%20the%20underlying%20K%C3%A4hler%20geometry.%20Computation%20of%20the%20metric%20via%20this%20potential%20is%20efficient%2C%20and%20through%20our%20potential%2C%20valid%20as%20a%20plurisubharmonic%20%28PSH%29%20function%2C%20large%20scale%20computational%20burden%20of%20automatic%20differentiation%20is%20displaced%20to%20small%20scale.%20We%20show%20that%20we%20can%20regularize%20the%20latent%20space%20with%20decoder%20geometry%2C%20and%20that%20we%20can%20sample%20in%20accordance%20with%20a%20weighted%20complex%20volume%20element.%20We%20demonstrate%20these%20strategies%2C%20at%20the%20exchange%20of%20sample%20variation%2C%20yield%20consistently%20smoother%20representations%20and%20fewer%20semantic%20outliers.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15172v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplex%2520variational%2520autoencoders%2520admit%2520K%25C3%25A4hler%2520structure%26entry.906535625%3DAndrew%2520Gracyk%26entry.1292438233%3DIt%2520has%2520been%2520discovered%2520that%2520latent-Euclidean%2520variational%2520autoencoders%2520%2528VAEs%2529%2520admit%252C%2520in%2520various%2520capacities%252C%2520Riemannian%2520structure.%2520We%2520adapt%2520these%2520arguments%2520but%2520for%2520complex%2520VAEs%2520with%2520a%2520complex%2520latent%2520stage.%2520We%2520show%2520that%2520complex%2520VAEs%2520reveal%2520to%2520some%2520level%2520K%25C3%25A4hler%2520geometric%2520structure.%2520Our%2520methods%2520will%2520be%2520tailored%2520for%2520decoder%2520geometry.%2520We%2520derive%2520the%2520Fisher%2520information%2520metric%2520in%2520the%2520complex%2520case%2520under%2520a%2520latent%2520complex%2520Gaussian%2520regularization%2520with%2520trivial%2520relation%2520matrix.%2520It%2520is%2520well%2520known%2520from%2520statistical%2520information%2520theory%2520that%2520the%2520Fisher%2520information%2520coincides%2520with%2520the%2520Hessian%2520of%2520the%2520Kullback-Leibler%2520%2528KL%2529%2520divergence.%2520Thus%252C%2520the%2520metric%2520K%25C3%25A4hler%2520potential%2520relation%2520is%2520exactly%2520achieved%2520under%2520relative%2520entropy.%2520We%2520propose%2520a%2520K%25C3%25A4hler%2520potential%2520derivative%2520of%2520complex%2520Gaussian%2520mixtures%2520that%2520has%2520rough%2520equivalence%2520to%2520the%2520Fisher%2520information%2520metric%2520while%2520still%2520being%2520faithful%2520to%2520the%2520underlying%2520K%25C3%25A4hler%2520geometry.%2520Computation%2520of%2520the%2520metric%2520via%2520this%2520potential%2520is%2520efficient%252C%2520and%2520through%2520our%2520potential%252C%2520valid%2520as%2520a%2520plurisubharmonic%2520%2528PSH%2529%2520function%252C%2520large%2520scale%2520computational%2520burden%2520of%2520automatic%2520differentiation%2520is%2520displaced%2520to%2520small%2520scale.%2520We%2520show%2520that%2520we%2520can%2520regularize%2520the%2520latent%2520space%2520with%2520decoder%2520geometry%252C%2520and%2520that%2520we%2520can%2520sample%2520in%2520accordance%2520with%2520a%2520weighted%2520complex%2520volume%2520element.%2520We%2520demonstrate%2520these%2520strategies%252C%2520at%2520the%2520exchange%2520of%2520sample%2520variation%252C%2520yield%2520consistently%2520smoother%2520representations%2520and%2520fewer%2520semantic%2520outliers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15172v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complex%20variational%20autoencoders%20admit%20K%C3%A4hler%20structure&entry.906535625=Andrew%20Gracyk&entry.1292438233=It%20has%20been%20discovered%20that%20latent-Euclidean%20variational%20autoencoders%20%28VAEs%29%20admit%2C%20in%20various%20capacities%2C%20Riemannian%20structure.%20We%20adapt%20these%20arguments%20but%20for%20complex%20VAEs%20with%20a%20complex%20latent%20stage.%20We%20show%20that%20complex%20VAEs%20reveal%20to%20some%20level%20K%C3%A4hler%20geometric%20structure.%20Our%20methods%20will%20be%20tailored%20for%20decoder%20geometry.%20We%20derive%20the%20Fisher%20information%20metric%20in%20the%20complex%20case%20under%20a%20latent%20complex%20Gaussian%20regularization%20with%20trivial%20relation%20matrix.%20It%20is%20well%20known%20from%20statistical%20information%20theory%20that%20the%20Fisher%20information%20coincides%20with%20the%20Hessian%20of%20the%20Kullback-Leibler%20%28KL%29%20divergence.%20Thus%2C%20the%20metric%20K%C3%A4hler%20potential%20relation%20is%20exactly%20achieved%20under%20relative%20entropy.%20We%20propose%20a%20K%C3%A4hler%20potential%20derivative%20of%20complex%20Gaussian%20mixtures%20that%20has%20rough%20equivalence%20to%20the%20Fisher%20information%20metric%20while%20still%20being%20faithful%20to%20the%20underlying%20K%C3%A4hler%20geometry.%20Computation%20of%20the%20metric%20via%20this%20potential%20is%20efficient%2C%20and%20through%20our%20potential%2C%20valid%20as%20a%20plurisubharmonic%20%28PSH%29%20function%2C%20large%20scale%20computational%20burden%20of%20automatic%20differentiation%20is%20displaced%20to%20small%20scale.%20We%20show%20that%20we%20can%20regularize%20the%20latent%20space%20with%20decoder%20geometry%2C%20and%20that%20we%20can%20sample%20in%20accordance%20with%20a%20weighted%20complex%20volume%20element.%20We%20demonstrate%20these%20strategies%2C%20at%20the%20exchange%20of%20sample%20variation%2C%20yield%20consistently%20smoother%20representations%20and%20fewer%20semantic%20outliers.&entry.1838667208=http%3A//arxiv.org/abs/2511.15172v2&entry.124074799=Read"},
{"title": "Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration", "author": "Fan Yang and Shigeyuki Odashima and Shoichi Masui and Ikuo Kusajima and Sosuke Yamao and Shan Jiang", "abstract": "We present a robust multi-camera gymnast tracking, which has been applied at international gymnastics championships for gymnastics judging. Despite considerable progress in multi-camera tracking algorithms, tracking gymnasts presents unique challenges: (i) due to space restrictions, only a limited number of cameras can be installed in the gymnastics stadium; and (ii) due to variations in lighting, background, uniforms, and occlusions, multi-camera gymnast detection may fail in certain views and only provide valid detections from two opposing views. These factors complicate the accurate determination of a gymnast's 3D trajectory using conventional multi-camera triangulation. To alleviate this issue, we incorporate gymnastics domain knowledge into our tracking solution. Given that a gymnast's 3D center typically lies within a predefined vertical plane during \\revised{much of their} performance, we can apply a ray-plane intersection to generate coplanar 3D trajectory candidates for opposing-view detections. More specifically, we propose a novel cascaded data association (DA) paradigm that employs triangulation to generate 3D trajectory candidates when cross-view detections are sufficient, and resort to the ray-plane intersection when they are insufficient. Consequently, coplanar candidates are used to compensate for uncertain trajectories, thereby minimizing tracking failures. The robustness of our method is validated through extensive experimentation, demonstrating its superiority over existing methods in challenging scenarios. Furthermore, our gymnastics judging system, equipped with this tracking method, has been successfully applied to recent Gymnastics World Championships, earning significant recognition from the International Gymnastics Federation.", "link": "http://arxiv.org/abs/2511.16532v1", "date": "2025-11-20", "relevancy": 2.2591, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5675}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5653}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Multi-Camera%20Gymnast%20Tracking%20Through%20Domain%20Knowledge%20Integration&body=Title%3A%20Enhancing%20Multi-Camera%20Gymnast%20Tracking%20Through%20Domain%20Knowledge%20Integration%0AAuthor%3A%20Fan%20Yang%20and%20Shigeyuki%20Odashima%20and%20Shoichi%20Masui%20and%20Ikuo%20Kusajima%20and%20Sosuke%20Yamao%20and%20Shan%20Jiang%0AAbstract%3A%20We%20present%20a%20robust%20multi-camera%20gymnast%20tracking%2C%20which%20has%20been%20applied%20at%20international%20gymnastics%20championships%20for%20gymnastics%20judging.%20Despite%20considerable%20progress%20in%20multi-camera%20tracking%20algorithms%2C%20tracking%20gymnasts%20presents%20unique%20challenges%3A%20%28i%29%20due%20to%20space%20restrictions%2C%20only%20a%20limited%20number%20of%20cameras%20can%20be%20installed%20in%20the%20gymnastics%20stadium%3B%20and%20%28ii%29%20due%20to%20variations%20in%20lighting%2C%20background%2C%20uniforms%2C%20and%20occlusions%2C%20multi-camera%20gymnast%20detection%20may%20fail%20in%20certain%20views%20and%20only%20provide%20valid%20detections%20from%20two%20opposing%20views.%20These%20factors%20complicate%20the%20accurate%20determination%20of%20a%20gymnast%27s%203D%20trajectory%20using%20conventional%20multi-camera%20triangulation.%20To%20alleviate%20this%20issue%2C%20we%20incorporate%20gymnastics%20domain%20knowledge%20into%20our%20tracking%20solution.%20Given%20that%20a%20gymnast%27s%203D%20center%20typically%20lies%20within%20a%20predefined%20vertical%20plane%20during%20%5Crevised%7Bmuch%20of%20their%7D%20performance%2C%20we%20can%20apply%20a%20ray-plane%20intersection%20to%20generate%20coplanar%203D%20trajectory%20candidates%20for%20opposing-view%20detections.%20More%20specifically%2C%20we%20propose%20a%20novel%20cascaded%20data%20association%20%28DA%29%20paradigm%20that%20employs%20triangulation%20to%20generate%203D%20trajectory%20candidates%20when%20cross-view%20detections%20are%20sufficient%2C%20and%20resort%20to%20the%20ray-plane%20intersection%20when%20they%20are%20insufficient.%20Consequently%2C%20coplanar%20candidates%20are%20used%20to%20compensate%20for%20uncertain%20trajectories%2C%20thereby%20minimizing%20tracking%20failures.%20The%20robustness%20of%20our%20method%20is%20validated%20through%20extensive%20experimentation%2C%20demonstrating%20its%20superiority%20over%20existing%20methods%20in%20challenging%20scenarios.%20Furthermore%2C%20our%20gymnastics%20judging%20system%2C%20equipped%20with%20this%20tracking%20method%2C%20has%20been%20successfully%20applied%20to%20recent%20Gymnastics%20World%20Championships%2C%20earning%20significant%20recognition%20from%20the%20International%20Gymnastics%20Federation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Multi-Camera%2520Gymnast%2520Tracking%2520Through%2520Domain%2520Knowledge%2520Integration%26entry.906535625%3DFan%2520Yang%2520and%2520Shigeyuki%2520Odashima%2520and%2520Shoichi%2520Masui%2520and%2520Ikuo%2520Kusajima%2520and%2520Sosuke%2520Yamao%2520and%2520Shan%2520Jiang%26entry.1292438233%3DWe%2520present%2520a%2520robust%2520multi-camera%2520gymnast%2520tracking%252C%2520which%2520has%2520been%2520applied%2520at%2520international%2520gymnastics%2520championships%2520for%2520gymnastics%2520judging.%2520Despite%2520considerable%2520progress%2520in%2520multi-camera%2520tracking%2520algorithms%252C%2520tracking%2520gymnasts%2520presents%2520unique%2520challenges%253A%2520%2528i%2529%2520due%2520to%2520space%2520restrictions%252C%2520only%2520a%2520limited%2520number%2520of%2520cameras%2520can%2520be%2520installed%2520in%2520the%2520gymnastics%2520stadium%253B%2520and%2520%2528ii%2529%2520due%2520to%2520variations%2520in%2520lighting%252C%2520background%252C%2520uniforms%252C%2520and%2520occlusions%252C%2520multi-camera%2520gymnast%2520detection%2520may%2520fail%2520in%2520certain%2520views%2520and%2520only%2520provide%2520valid%2520detections%2520from%2520two%2520opposing%2520views.%2520These%2520factors%2520complicate%2520the%2520accurate%2520determination%2520of%2520a%2520gymnast%2527s%25203D%2520trajectory%2520using%2520conventional%2520multi-camera%2520triangulation.%2520To%2520alleviate%2520this%2520issue%252C%2520we%2520incorporate%2520gymnastics%2520domain%2520knowledge%2520into%2520our%2520tracking%2520solution.%2520Given%2520that%2520a%2520gymnast%2527s%25203D%2520center%2520typically%2520lies%2520within%2520a%2520predefined%2520vertical%2520plane%2520during%2520%255Crevised%257Bmuch%2520of%2520their%257D%2520performance%252C%2520we%2520can%2520apply%2520a%2520ray-plane%2520intersection%2520to%2520generate%2520coplanar%25203D%2520trajectory%2520candidates%2520for%2520opposing-view%2520detections.%2520More%2520specifically%252C%2520we%2520propose%2520a%2520novel%2520cascaded%2520data%2520association%2520%2528DA%2529%2520paradigm%2520that%2520employs%2520triangulation%2520to%2520generate%25203D%2520trajectory%2520candidates%2520when%2520cross-view%2520detections%2520are%2520sufficient%252C%2520and%2520resort%2520to%2520the%2520ray-plane%2520intersection%2520when%2520they%2520are%2520insufficient.%2520Consequently%252C%2520coplanar%2520candidates%2520are%2520used%2520to%2520compensate%2520for%2520uncertain%2520trajectories%252C%2520thereby%2520minimizing%2520tracking%2520failures.%2520The%2520robustness%2520of%2520our%2520method%2520is%2520validated%2520through%2520extensive%2520experimentation%252C%2520demonstrating%2520its%2520superiority%2520over%2520existing%2520methods%2520in%2520challenging%2520scenarios.%2520Furthermore%252C%2520our%2520gymnastics%2520judging%2520system%252C%2520equipped%2520with%2520this%2520tracking%2520method%252C%2520has%2520been%2520successfully%2520applied%2520to%2520recent%2520Gymnastics%2520World%2520Championships%252C%2520earning%2520significant%2520recognition%2520from%2520the%2520International%2520Gymnastics%2520Federation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Multi-Camera%20Gymnast%20Tracking%20Through%20Domain%20Knowledge%20Integration&entry.906535625=Fan%20Yang%20and%20Shigeyuki%20Odashima%20and%20Shoichi%20Masui%20and%20Ikuo%20Kusajima%20and%20Sosuke%20Yamao%20and%20Shan%20Jiang&entry.1292438233=We%20present%20a%20robust%20multi-camera%20gymnast%20tracking%2C%20which%20has%20been%20applied%20at%20international%20gymnastics%20championships%20for%20gymnastics%20judging.%20Despite%20considerable%20progress%20in%20multi-camera%20tracking%20algorithms%2C%20tracking%20gymnasts%20presents%20unique%20challenges%3A%20%28i%29%20due%20to%20space%20restrictions%2C%20only%20a%20limited%20number%20of%20cameras%20can%20be%20installed%20in%20the%20gymnastics%20stadium%3B%20and%20%28ii%29%20due%20to%20variations%20in%20lighting%2C%20background%2C%20uniforms%2C%20and%20occlusions%2C%20multi-camera%20gymnast%20detection%20may%20fail%20in%20certain%20views%20and%20only%20provide%20valid%20detections%20from%20two%20opposing%20views.%20These%20factors%20complicate%20the%20accurate%20determination%20of%20a%20gymnast%27s%203D%20trajectory%20using%20conventional%20multi-camera%20triangulation.%20To%20alleviate%20this%20issue%2C%20we%20incorporate%20gymnastics%20domain%20knowledge%20into%20our%20tracking%20solution.%20Given%20that%20a%20gymnast%27s%203D%20center%20typically%20lies%20within%20a%20predefined%20vertical%20plane%20during%20%5Crevised%7Bmuch%20of%20their%7D%20performance%2C%20we%20can%20apply%20a%20ray-plane%20intersection%20to%20generate%20coplanar%203D%20trajectory%20candidates%20for%20opposing-view%20detections.%20More%20specifically%2C%20we%20propose%20a%20novel%20cascaded%20data%20association%20%28DA%29%20paradigm%20that%20employs%20triangulation%20to%20generate%203D%20trajectory%20candidates%20when%20cross-view%20detections%20are%20sufficient%2C%20and%20resort%20to%20the%20ray-plane%20intersection%20when%20they%20are%20insufficient.%20Consequently%2C%20coplanar%20candidates%20are%20used%20to%20compensate%20for%20uncertain%20trajectories%2C%20thereby%20minimizing%20tracking%20failures.%20The%20robustness%20of%20our%20method%20is%20validated%20through%20extensive%20experimentation%2C%20demonstrating%20its%20superiority%20over%20existing%20methods%20in%20challenging%20scenarios.%20Furthermore%2C%20our%20gymnastics%20judging%20system%2C%20equipped%20with%20this%20tracking%20method%2C%20has%20been%20successfully%20applied%20to%20recent%20Gymnastics%20World%20Championships%2C%20earning%20significant%20recognition%20from%20the%20International%20Gymnastics%20Federation.&entry.1838667208=http%3A//arxiv.org/abs/2511.16532v1&entry.124074799=Read"},
{"title": "Boosting Predictive Performance on Tabular Data through Data Augmentation with Latent-Space Flow-Based Diffusion", "author": "Md. Tawfique Ihsan and Md. Rakibul Hasan Rafi and Ahmed Shoyeb Raihan and Imtiaz Ahmed and Abdullahil Azeem", "abstract": "Severe class imbalance is common in real-world tabular learning, where rare but important minority classes are essential for reliable prediction. Existing generative oversampling methods such as GANs, VAEs, and diffusion models can improve minority-class performance, but they often struggle with tabular heterogeneity, training stability, and privacy concerns. We propose a family of latent-space, tree-driven diffusion methods for minority oversampling that use conditional flow matching with gradient-boosted trees as the vector-field learner. The models operate in compact latent spaces to preserve tabular structure and reduce computation. We introduce three variants: PCAForest, which uses linear PCA embedding; EmbedForest, which uses a learned nonlinear embedding; and AttentionForest, which uses an attention-augmented embedding. Each method couples a GBT-based flow with a decoder back to the original feature space. Across 11 datasets from healthcare, finance, and manufacturing, AttentionForest achieves the best average minority recall while maintaining competitive precision, calibration, and distributional similarity. PCAForest and EmbedForest reach similar utility with much faster generation, offering favorable accuracy-efficiency trade-offs. Privacy evaluated with nearest-neighbor distance ratio and distance-to-closest-record is comparable to or better than the ForestDiffusion baseline. Ablation studies show that smaller embeddings tend to improve minority recall, while aggressive learning rates harm stability. Overall, latent-space, tree-driven diffusion provides an efficient and privacy-aware approach to high-fidelity tabular data augmentation under severe class imbalance.", "link": "http://arxiv.org/abs/2511.16571v1", "date": "2025-11-20", "relevancy": 2.2441, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.565}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5624}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Predictive%20Performance%20on%20Tabular%20Data%20through%20Data%20Augmentation%20with%20Latent-Space%20Flow-Based%20Diffusion&body=Title%3A%20Boosting%20Predictive%20Performance%20on%20Tabular%20Data%20through%20Data%20Augmentation%20with%20Latent-Space%20Flow-Based%20Diffusion%0AAuthor%3A%20Md.%20Tawfique%20Ihsan%20and%20Md.%20Rakibul%20Hasan%20Rafi%20and%20Ahmed%20Shoyeb%20Raihan%20and%20Imtiaz%20Ahmed%20and%20Abdullahil%20Azeem%0AAbstract%3A%20Severe%20class%20imbalance%20is%20common%20in%20real-world%20tabular%20learning%2C%20where%20rare%20but%20important%20minority%20classes%20are%20essential%20for%20reliable%20prediction.%20Existing%20generative%20oversampling%20methods%20such%20as%20GANs%2C%20VAEs%2C%20and%20diffusion%20models%20can%20improve%20minority-class%20performance%2C%20but%20they%20often%20struggle%20with%20tabular%20heterogeneity%2C%20training%20stability%2C%20and%20privacy%20concerns.%20We%20propose%20a%20family%20of%20latent-space%2C%20tree-driven%20diffusion%20methods%20for%20minority%20oversampling%20that%20use%20conditional%20flow%20matching%20with%20gradient-boosted%20trees%20as%20the%20vector-field%20learner.%20The%20models%20operate%20in%20compact%20latent%20spaces%20to%20preserve%20tabular%20structure%20and%20reduce%20computation.%20We%20introduce%20three%20variants%3A%20PCAForest%2C%20which%20uses%20linear%20PCA%20embedding%3B%20EmbedForest%2C%20which%20uses%20a%20learned%20nonlinear%20embedding%3B%20and%20AttentionForest%2C%20which%20uses%20an%20attention-augmented%20embedding.%20Each%20method%20couples%20a%20GBT-based%20flow%20with%20a%20decoder%20back%20to%20the%20original%20feature%20space.%20Across%2011%20datasets%20from%20healthcare%2C%20finance%2C%20and%20manufacturing%2C%20AttentionForest%20achieves%20the%20best%20average%20minority%20recall%20while%20maintaining%20competitive%20precision%2C%20calibration%2C%20and%20distributional%20similarity.%20PCAForest%20and%20EmbedForest%20reach%20similar%20utility%20with%20much%20faster%20generation%2C%20offering%20favorable%20accuracy-efficiency%20trade-offs.%20Privacy%20evaluated%20with%20nearest-neighbor%20distance%20ratio%20and%20distance-to-closest-record%20is%20comparable%20to%20or%20better%20than%20the%20ForestDiffusion%20baseline.%20Ablation%20studies%20show%20that%20smaller%20embeddings%20tend%20to%20improve%20minority%20recall%2C%20while%20aggressive%20learning%20rates%20harm%20stability.%20Overall%2C%20latent-space%2C%20tree-driven%20diffusion%20provides%20an%20efficient%20and%20privacy-aware%20approach%20to%20high-fidelity%20tabular%20data%20augmentation%20under%20severe%20class%20imbalance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Predictive%2520Performance%2520on%2520Tabular%2520Data%2520through%2520Data%2520Augmentation%2520with%2520Latent-Space%2520Flow-Based%2520Diffusion%26entry.906535625%3DMd.%2520Tawfique%2520Ihsan%2520and%2520Md.%2520Rakibul%2520Hasan%2520Rafi%2520and%2520Ahmed%2520Shoyeb%2520Raihan%2520and%2520Imtiaz%2520Ahmed%2520and%2520Abdullahil%2520Azeem%26entry.1292438233%3DSevere%2520class%2520imbalance%2520is%2520common%2520in%2520real-world%2520tabular%2520learning%252C%2520where%2520rare%2520but%2520important%2520minority%2520classes%2520are%2520essential%2520for%2520reliable%2520prediction.%2520Existing%2520generative%2520oversampling%2520methods%2520such%2520as%2520GANs%252C%2520VAEs%252C%2520and%2520diffusion%2520models%2520can%2520improve%2520minority-class%2520performance%252C%2520but%2520they%2520often%2520struggle%2520with%2520tabular%2520heterogeneity%252C%2520training%2520stability%252C%2520and%2520privacy%2520concerns.%2520We%2520propose%2520a%2520family%2520of%2520latent-space%252C%2520tree-driven%2520diffusion%2520methods%2520for%2520minority%2520oversampling%2520that%2520use%2520conditional%2520flow%2520matching%2520with%2520gradient-boosted%2520trees%2520as%2520the%2520vector-field%2520learner.%2520The%2520models%2520operate%2520in%2520compact%2520latent%2520spaces%2520to%2520preserve%2520tabular%2520structure%2520and%2520reduce%2520computation.%2520We%2520introduce%2520three%2520variants%253A%2520PCAForest%252C%2520which%2520uses%2520linear%2520PCA%2520embedding%253B%2520EmbedForest%252C%2520which%2520uses%2520a%2520learned%2520nonlinear%2520embedding%253B%2520and%2520AttentionForest%252C%2520which%2520uses%2520an%2520attention-augmented%2520embedding.%2520Each%2520method%2520couples%2520a%2520GBT-based%2520flow%2520with%2520a%2520decoder%2520back%2520to%2520the%2520original%2520feature%2520space.%2520Across%252011%2520datasets%2520from%2520healthcare%252C%2520finance%252C%2520and%2520manufacturing%252C%2520AttentionForest%2520achieves%2520the%2520best%2520average%2520minority%2520recall%2520while%2520maintaining%2520competitive%2520precision%252C%2520calibration%252C%2520and%2520distributional%2520similarity.%2520PCAForest%2520and%2520EmbedForest%2520reach%2520similar%2520utility%2520with%2520much%2520faster%2520generation%252C%2520offering%2520favorable%2520accuracy-efficiency%2520trade-offs.%2520Privacy%2520evaluated%2520with%2520nearest-neighbor%2520distance%2520ratio%2520and%2520distance-to-closest-record%2520is%2520comparable%2520to%2520or%2520better%2520than%2520the%2520ForestDiffusion%2520baseline.%2520Ablation%2520studies%2520show%2520that%2520smaller%2520embeddings%2520tend%2520to%2520improve%2520minority%2520recall%252C%2520while%2520aggressive%2520learning%2520rates%2520harm%2520stability.%2520Overall%252C%2520latent-space%252C%2520tree-driven%2520diffusion%2520provides%2520an%2520efficient%2520and%2520privacy-aware%2520approach%2520to%2520high-fidelity%2520tabular%2520data%2520augmentation%2520under%2520severe%2520class%2520imbalance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Predictive%20Performance%20on%20Tabular%20Data%20through%20Data%20Augmentation%20with%20Latent-Space%20Flow-Based%20Diffusion&entry.906535625=Md.%20Tawfique%20Ihsan%20and%20Md.%20Rakibul%20Hasan%20Rafi%20and%20Ahmed%20Shoyeb%20Raihan%20and%20Imtiaz%20Ahmed%20and%20Abdullahil%20Azeem&entry.1292438233=Severe%20class%20imbalance%20is%20common%20in%20real-world%20tabular%20learning%2C%20where%20rare%20but%20important%20minority%20classes%20are%20essential%20for%20reliable%20prediction.%20Existing%20generative%20oversampling%20methods%20such%20as%20GANs%2C%20VAEs%2C%20and%20diffusion%20models%20can%20improve%20minority-class%20performance%2C%20but%20they%20often%20struggle%20with%20tabular%20heterogeneity%2C%20training%20stability%2C%20and%20privacy%20concerns.%20We%20propose%20a%20family%20of%20latent-space%2C%20tree-driven%20diffusion%20methods%20for%20minority%20oversampling%20that%20use%20conditional%20flow%20matching%20with%20gradient-boosted%20trees%20as%20the%20vector-field%20learner.%20The%20models%20operate%20in%20compact%20latent%20spaces%20to%20preserve%20tabular%20structure%20and%20reduce%20computation.%20We%20introduce%20three%20variants%3A%20PCAForest%2C%20which%20uses%20linear%20PCA%20embedding%3B%20EmbedForest%2C%20which%20uses%20a%20learned%20nonlinear%20embedding%3B%20and%20AttentionForest%2C%20which%20uses%20an%20attention-augmented%20embedding.%20Each%20method%20couples%20a%20GBT-based%20flow%20with%20a%20decoder%20back%20to%20the%20original%20feature%20space.%20Across%2011%20datasets%20from%20healthcare%2C%20finance%2C%20and%20manufacturing%2C%20AttentionForest%20achieves%20the%20best%20average%20minority%20recall%20while%20maintaining%20competitive%20precision%2C%20calibration%2C%20and%20distributional%20similarity.%20PCAForest%20and%20EmbedForest%20reach%20similar%20utility%20with%20much%20faster%20generation%2C%20offering%20favorable%20accuracy-efficiency%20trade-offs.%20Privacy%20evaluated%20with%20nearest-neighbor%20distance%20ratio%20and%20distance-to-closest-record%20is%20comparable%20to%20or%20better%20than%20the%20ForestDiffusion%20baseline.%20Ablation%20studies%20show%20that%20smaller%20embeddings%20tend%20to%20improve%20minority%20recall%2C%20while%20aggressive%20learning%20rates%20harm%20stability.%20Overall%2C%20latent-space%2C%20tree-driven%20diffusion%20provides%20an%20efficient%20and%20privacy-aware%20approach%20to%20high-fidelity%20tabular%20data%20augmentation%20under%20severe%20class%20imbalance.&entry.1838667208=http%3A//arxiv.org/abs/2511.16571v1&entry.124074799=Read"},
{"title": "PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks", "author": "Kewei Chen and Yayu Long and Mingsheng Shang", "abstract": "Multi-robot systems in complex physical collaborations face a \"shared brain dilemma\": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace \"raw data communication\" with \"semantic communication\" by performing \"semantic distillation\" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the \"shared brain dilemma\" in resource-constrained multi-robot systems.", "link": "http://arxiv.org/abs/2511.16200v1", "date": "2025-11-20", "relevancy": 2.2406, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6083}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.556}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIPHEN%3A%20Physical%20Interaction%20Prediction%20with%20Hamiltonian%20Energy%20Networks&body=Title%3A%20PIPHEN%3A%20Physical%20Interaction%20Prediction%20with%20Hamiltonian%20Energy%20Networks%0AAuthor%3A%20Kewei%20Chen%20and%20Yayu%20Long%20and%20Mingsheng%20Shang%0AAbstract%3A%20Multi-robot%20systems%20in%20complex%20physical%20collaborations%20face%20a%20%22shared%20brain%20dilemma%22%3A%20transmitting%20high-dimensional%20multimedia%20data%20%28e.g.%2C%20video%20streams%20at%20~30MB/s%29%20creates%20severe%20bandwidth%20bottlenecks%20and%20decision-making%20latency.%20To%20address%20this%2C%20we%20propose%20PIPHEN%2C%20an%20innovative%20distributed%20physical%20cognition-control%20framework.%20Its%20core%20idea%20is%20to%20replace%20%22raw%20data%20communication%22%20with%20%22semantic%20communication%22%20by%20performing%20%22semantic%20distillation%22%20at%20the%20robot%20edge%2C%20reconstructing%20high-dimensional%20perceptual%20data%20into%20compact%2C%20structured%20physical%20representations.%20This%20idea%20is%20primarily%20realized%20through%20two%20key%20components%3A%20%281%29%20a%20novel%20Physical%20Interaction%20Prediction%20Network%20%28PIPN%29%2C%20derived%20from%20large%20model%20knowledge%20distillation%2C%20to%20generate%20this%20representation%3B%20and%20%282%29%20a%20Hamiltonian%20Energy%20Network%20%28HEN%29%20controller%2C%20based%20on%20energy%20conservation%2C%20to%20precisely%20translate%20this%20representation%20into%20coordinated%20actions.%20Experiments%20show%20that%2C%20compared%20to%20baseline%20methods%2C%20PIPHEN%20can%20compress%20the%20information%20representation%20to%20less%20than%205%25%20of%20the%20original%20data%20volume%20and%20reduce%20collaborative%20decision-making%20latency%20from%20315ms%20to%2076ms%2C%20while%20significantly%20improving%20task%20success%20rates.%20This%20work%20provides%20a%20fundamentally%20efficient%20paradigm%20for%20resolving%20the%20%22shared%20brain%20dilemma%22%20in%20resource-constrained%20multi-robot%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIPHEN%253A%2520Physical%2520Interaction%2520Prediction%2520with%2520Hamiltonian%2520Energy%2520Networks%26entry.906535625%3DKewei%2520Chen%2520and%2520Yayu%2520Long%2520and%2520Mingsheng%2520Shang%26entry.1292438233%3DMulti-robot%2520systems%2520in%2520complex%2520physical%2520collaborations%2520face%2520a%2520%2522shared%2520brain%2520dilemma%2522%253A%2520transmitting%2520high-dimensional%2520multimedia%2520data%2520%2528e.g.%252C%2520video%2520streams%2520at%2520~30MB/s%2529%2520creates%2520severe%2520bandwidth%2520bottlenecks%2520and%2520decision-making%2520latency.%2520To%2520address%2520this%252C%2520we%2520propose%2520PIPHEN%252C%2520an%2520innovative%2520distributed%2520physical%2520cognition-control%2520framework.%2520Its%2520core%2520idea%2520is%2520to%2520replace%2520%2522raw%2520data%2520communication%2522%2520with%2520%2522semantic%2520communication%2522%2520by%2520performing%2520%2522semantic%2520distillation%2522%2520at%2520the%2520robot%2520edge%252C%2520reconstructing%2520high-dimensional%2520perceptual%2520data%2520into%2520compact%252C%2520structured%2520physical%2520representations.%2520This%2520idea%2520is%2520primarily%2520realized%2520through%2520two%2520key%2520components%253A%2520%25281%2529%2520a%2520novel%2520Physical%2520Interaction%2520Prediction%2520Network%2520%2528PIPN%2529%252C%2520derived%2520from%2520large%2520model%2520knowledge%2520distillation%252C%2520to%2520generate%2520this%2520representation%253B%2520and%2520%25282%2529%2520a%2520Hamiltonian%2520Energy%2520Network%2520%2528HEN%2529%2520controller%252C%2520based%2520on%2520energy%2520conservation%252C%2520to%2520precisely%2520translate%2520this%2520representation%2520into%2520coordinated%2520actions.%2520Experiments%2520show%2520that%252C%2520compared%2520to%2520baseline%2520methods%252C%2520PIPHEN%2520can%2520compress%2520the%2520information%2520representation%2520to%2520less%2520than%25205%2525%2520of%2520the%2520original%2520data%2520volume%2520and%2520reduce%2520collaborative%2520decision-making%2520latency%2520from%2520315ms%2520to%252076ms%252C%2520while%2520significantly%2520improving%2520task%2520success%2520rates.%2520This%2520work%2520provides%2520a%2520fundamentally%2520efficient%2520paradigm%2520for%2520resolving%2520the%2520%2522shared%2520brain%2520dilemma%2522%2520in%2520resource-constrained%2520multi-robot%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIPHEN%3A%20Physical%20Interaction%20Prediction%20with%20Hamiltonian%20Energy%20Networks&entry.906535625=Kewei%20Chen%20and%20Yayu%20Long%20and%20Mingsheng%20Shang&entry.1292438233=Multi-robot%20systems%20in%20complex%20physical%20collaborations%20face%20a%20%22shared%20brain%20dilemma%22%3A%20transmitting%20high-dimensional%20multimedia%20data%20%28e.g.%2C%20video%20streams%20at%20~30MB/s%29%20creates%20severe%20bandwidth%20bottlenecks%20and%20decision-making%20latency.%20To%20address%20this%2C%20we%20propose%20PIPHEN%2C%20an%20innovative%20distributed%20physical%20cognition-control%20framework.%20Its%20core%20idea%20is%20to%20replace%20%22raw%20data%20communication%22%20with%20%22semantic%20communication%22%20by%20performing%20%22semantic%20distillation%22%20at%20the%20robot%20edge%2C%20reconstructing%20high-dimensional%20perceptual%20data%20into%20compact%2C%20structured%20physical%20representations.%20This%20idea%20is%20primarily%20realized%20through%20two%20key%20components%3A%20%281%29%20a%20novel%20Physical%20Interaction%20Prediction%20Network%20%28PIPN%29%2C%20derived%20from%20large%20model%20knowledge%20distillation%2C%20to%20generate%20this%20representation%3B%20and%20%282%29%20a%20Hamiltonian%20Energy%20Network%20%28HEN%29%20controller%2C%20based%20on%20energy%20conservation%2C%20to%20precisely%20translate%20this%20representation%20into%20coordinated%20actions.%20Experiments%20show%20that%2C%20compared%20to%20baseline%20methods%2C%20PIPHEN%20can%20compress%20the%20information%20representation%20to%20less%20than%205%25%20of%20the%20original%20data%20volume%20and%20reduce%20collaborative%20decision-making%20latency%20from%20315ms%20to%2076ms%2C%20while%20significantly%20improving%20task%20success%20rates.%20This%20work%20provides%20a%20fundamentally%20efficient%20paradigm%20for%20resolving%20the%20%22shared%20brain%20dilemma%22%20in%20resource-constrained%20multi-robot%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.16200v1&entry.124074799=Read"},
{"title": "Decoding Deception: Understanding Automatic Speech Recognition Vulnerabilities in Evasion and Poisoning Attacks", "author": "Aravindhan G and Yuvaraj Govindarajulu and Parin Shah", "abstract": "Recent studies have demonstrated the vulnerability of Automatic Speech Recognition systems to adversarial examples, which can deceive these systems into misinterpreting input speech commands. While previous research has primarily focused on white-box attacks with constrained optimizations, and transferability based black-box attacks against commercial Automatic Speech Recognition devices, this paper explores cost efficient white-box attack and non transferability black-box adversarial attacks on Automatic Speech Recognition systems, drawing insights from approaches such as Fast Gradient Sign Method and Zeroth-Order Optimization. Further, the novelty of the paper includes how poisoning attack can degrade the performances of state-of-the-art models leading to misinterpretation of audio signals. Through experimentation and analysis, we illustrate how hybrid models can generate subtle yet impactful adversarial examples with very little perturbation having Signal Noise Ratio of 35dB that can be generated within a minute. These vulnerabilities of state-of-the-art open source model have practical security implications, and emphasize the need for adversarial security.", "link": "http://arxiv.org/abs/2509.22060v2", "date": "2025-11-20", "relevancy": 2.2336, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4693}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Deception%3A%20Understanding%20Automatic%20Speech%20Recognition%20Vulnerabilities%20in%20Evasion%20and%20Poisoning%20Attacks&body=Title%3A%20Decoding%20Deception%3A%20Understanding%20Automatic%20Speech%20Recognition%20Vulnerabilities%20in%20Evasion%20and%20Poisoning%20Attacks%0AAuthor%3A%20Aravindhan%20G%20and%20Yuvaraj%20Govindarajulu%20and%20Parin%20Shah%0AAbstract%3A%20Recent%20studies%20have%20demonstrated%20the%20vulnerability%20of%20Automatic%20Speech%20Recognition%20systems%20to%20adversarial%20examples%2C%20which%20can%20deceive%20these%20systems%20into%20misinterpreting%20input%20speech%20commands.%20While%20previous%20research%20has%20primarily%20focused%20on%20white-box%20attacks%20with%20constrained%20optimizations%2C%20and%20transferability%20based%20black-box%20attacks%20against%20commercial%20Automatic%20Speech%20Recognition%20devices%2C%20this%20paper%20explores%20cost%20efficient%20white-box%20attack%20and%20non%20transferability%20black-box%20adversarial%20attacks%20on%20Automatic%20Speech%20Recognition%20systems%2C%20drawing%20insights%20from%20approaches%20such%20as%20Fast%20Gradient%20Sign%20Method%20and%20Zeroth-Order%20Optimization.%20Further%2C%20the%20novelty%20of%20the%20paper%20includes%20how%20poisoning%20attack%20can%20degrade%20the%20performances%20of%20state-of-the-art%20models%20leading%20to%20misinterpretation%20of%20audio%20signals.%20Through%20experimentation%20and%20analysis%2C%20we%20illustrate%20how%20hybrid%20models%20can%20generate%20subtle%20yet%20impactful%20adversarial%20examples%20with%20very%20little%20perturbation%20having%20Signal%20Noise%20Ratio%20of%2035dB%20that%20can%20be%20generated%20within%20a%20minute.%20These%20vulnerabilities%20of%20state-of-the-art%20open%20source%20model%20have%20practical%20security%20implications%2C%20and%20emphasize%20the%20need%20for%20adversarial%20security.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Deception%253A%2520Understanding%2520Automatic%2520Speech%2520Recognition%2520Vulnerabilities%2520in%2520Evasion%2520and%2520Poisoning%2520Attacks%26entry.906535625%3DAravindhan%2520G%2520and%2520Yuvaraj%2520Govindarajulu%2520and%2520Parin%2520Shah%26entry.1292438233%3DRecent%2520studies%2520have%2520demonstrated%2520the%2520vulnerability%2520of%2520Automatic%2520Speech%2520Recognition%2520systems%2520to%2520adversarial%2520examples%252C%2520which%2520can%2520deceive%2520these%2520systems%2520into%2520misinterpreting%2520input%2520speech%2520commands.%2520While%2520previous%2520research%2520has%2520primarily%2520focused%2520on%2520white-box%2520attacks%2520with%2520constrained%2520optimizations%252C%2520and%2520transferability%2520based%2520black-box%2520attacks%2520against%2520commercial%2520Automatic%2520Speech%2520Recognition%2520devices%252C%2520this%2520paper%2520explores%2520cost%2520efficient%2520white-box%2520attack%2520and%2520non%2520transferability%2520black-box%2520adversarial%2520attacks%2520on%2520Automatic%2520Speech%2520Recognition%2520systems%252C%2520drawing%2520insights%2520from%2520approaches%2520such%2520as%2520Fast%2520Gradient%2520Sign%2520Method%2520and%2520Zeroth-Order%2520Optimization.%2520Further%252C%2520the%2520novelty%2520of%2520the%2520paper%2520includes%2520how%2520poisoning%2520attack%2520can%2520degrade%2520the%2520performances%2520of%2520state-of-the-art%2520models%2520leading%2520to%2520misinterpretation%2520of%2520audio%2520signals.%2520Through%2520experimentation%2520and%2520analysis%252C%2520we%2520illustrate%2520how%2520hybrid%2520models%2520can%2520generate%2520subtle%2520yet%2520impactful%2520adversarial%2520examples%2520with%2520very%2520little%2520perturbation%2520having%2520Signal%2520Noise%2520Ratio%2520of%252035dB%2520that%2520can%2520be%2520generated%2520within%2520a%2520minute.%2520These%2520vulnerabilities%2520of%2520state-of-the-art%2520open%2520source%2520model%2520have%2520practical%2520security%2520implications%252C%2520and%2520emphasize%2520the%2520need%2520for%2520adversarial%2520security.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Deception%3A%20Understanding%20Automatic%20Speech%20Recognition%20Vulnerabilities%20in%20Evasion%20and%20Poisoning%20Attacks&entry.906535625=Aravindhan%20G%20and%20Yuvaraj%20Govindarajulu%20and%20Parin%20Shah&entry.1292438233=Recent%20studies%20have%20demonstrated%20the%20vulnerability%20of%20Automatic%20Speech%20Recognition%20systems%20to%20adversarial%20examples%2C%20which%20can%20deceive%20these%20systems%20into%20misinterpreting%20input%20speech%20commands.%20While%20previous%20research%20has%20primarily%20focused%20on%20white-box%20attacks%20with%20constrained%20optimizations%2C%20and%20transferability%20based%20black-box%20attacks%20against%20commercial%20Automatic%20Speech%20Recognition%20devices%2C%20this%20paper%20explores%20cost%20efficient%20white-box%20attack%20and%20non%20transferability%20black-box%20adversarial%20attacks%20on%20Automatic%20Speech%20Recognition%20systems%2C%20drawing%20insights%20from%20approaches%20such%20as%20Fast%20Gradient%20Sign%20Method%20and%20Zeroth-Order%20Optimization.%20Further%2C%20the%20novelty%20of%20the%20paper%20includes%20how%20poisoning%20attack%20can%20degrade%20the%20performances%20of%20state-of-the-art%20models%20leading%20to%20misinterpretation%20of%20audio%20signals.%20Through%20experimentation%20and%20analysis%2C%20we%20illustrate%20how%20hybrid%20models%20can%20generate%20subtle%20yet%20impactful%20adversarial%20examples%20with%20very%20little%20perturbation%20having%20Signal%20Noise%20Ratio%20of%2035dB%20that%20can%20be%20generated%20within%20a%20minute.%20These%20vulnerabilities%20of%20state-of-the-art%20open%20source%20model%20have%20practical%20security%20implications%2C%20and%20emphasize%20the%20need%20for%20adversarial%20security.&entry.1838667208=http%3A//arxiv.org/abs/2509.22060v2&entry.124074799=Read"},
{"title": "Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach", "author": "Chi-Han Chen and Chieh-Ming Chen and Wen-Huang Cheng and Ching-Chun Huang", "abstract": "The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection", "link": "http://arxiv.org/abs/2511.16343v1", "date": "2025-11-20", "relevancy": 2.2233, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5646}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5571}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aerial%20View%20River%20Landform%20Video%20segmentation%3A%20A%20Weakly%20Supervised%20Context-aware%20Temporal%20Consistency%20Distillation%20Approach&body=Title%3A%20Aerial%20View%20River%20Landform%20Video%20segmentation%3A%20A%20Weakly%20Supervised%20Context-aware%20Temporal%20Consistency%20Distillation%20Approach%0AAuthor%3A%20Chi-Han%20Chen%20and%20Chieh-Ming%20Chen%20and%20Wen-Huang%20Cheng%20and%20Ching-Chun%20Huang%0AAbstract%3A%20The%20study%20of%20terrain%20and%20landform%20classification%20through%20UAV%20remote%20sensing%20diverges%20significantly%20from%20ground%20vehicle%20patrol%20tasks.%20Besides%20grappling%20with%20the%20complexity%20of%20data%20annotation%20and%20ensuring%20temporal%20consistency%2C%20it%20also%20confronts%20the%20scarcity%20of%20relevant%20data%20and%20the%20limitations%20imposed%20by%20the%20effective%20range%20of%20many%20technologies.%20This%20research%20substantiates%20that%2C%20in%20aerial%20positioning%20tasks%2C%20both%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20and%20temporal%20consistency%20%28TC%29%20metrics%20are%20of%20paramount%20importance.%20It%20is%20demonstrated%20that%20fully%20labeled%20data%20is%20not%20the%20optimal%20choice%2C%20as%20selecting%20only%20key%20data%20lacks%20the%20enhancement%20in%20TC%2C%20leading%20to%20failures.%20Hence%2C%20a%20teacher-student%20architecture%2C%20coupled%20with%20key%20frame%20selection%20and%20key%20frame%20updating%20algorithms%2C%20is%20proposed.%20This%20framework%20successfully%20performs%20weakly%20supervised%20learning%20and%20TC%20knowledge%20distillation%2C%20overcoming%20the%20deficiencies%20of%20traditional%20TC%20training%20in%20aerial%20tasks.%20The%20experimental%20results%20reveal%20that%20our%20method%20utilizing%20merely%2030%5C%25%20of%20labeled%20data%2C%20concurrently%20elevates%20mIoU%20and%20temporal%20consistency%20ensuring%20stable%20localization%20of%20terrain%20objects.%20Result%20demo%20%3A%20https%3A//gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection%0ALink%3A%20http%3A//arxiv.org/abs/2511.16343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAerial%2520View%2520River%2520Landform%2520Video%2520segmentation%253A%2520A%2520Weakly%2520Supervised%2520Context-aware%2520Temporal%2520Consistency%2520Distillation%2520Approach%26entry.906535625%3DChi-Han%2520Chen%2520and%2520Chieh-Ming%2520Chen%2520and%2520Wen-Huang%2520Cheng%2520and%2520Ching-Chun%2520Huang%26entry.1292438233%3DThe%2520study%2520of%2520terrain%2520and%2520landform%2520classification%2520through%2520UAV%2520remote%2520sensing%2520diverges%2520significantly%2520from%2520ground%2520vehicle%2520patrol%2520tasks.%2520Besides%2520grappling%2520with%2520the%2520complexity%2520of%2520data%2520annotation%2520and%2520ensuring%2520temporal%2520consistency%252C%2520it%2520also%2520confronts%2520the%2520scarcity%2520of%2520relevant%2520data%2520and%2520the%2520limitations%2520imposed%2520by%2520the%2520effective%2520range%2520of%2520many%2520technologies.%2520This%2520research%2520substantiates%2520that%252C%2520in%2520aerial%2520positioning%2520tasks%252C%2520both%2520the%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520and%2520temporal%2520consistency%2520%2528TC%2529%2520metrics%2520are%2520of%2520paramount%2520importance.%2520It%2520is%2520demonstrated%2520that%2520fully%2520labeled%2520data%2520is%2520not%2520the%2520optimal%2520choice%252C%2520as%2520selecting%2520only%2520key%2520data%2520lacks%2520the%2520enhancement%2520in%2520TC%252C%2520leading%2520to%2520failures.%2520Hence%252C%2520a%2520teacher-student%2520architecture%252C%2520coupled%2520with%2520key%2520frame%2520selection%2520and%2520key%2520frame%2520updating%2520algorithms%252C%2520is%2520proposed.%2520This%2520framework%2520successfully%2520performs%2520weakly%2520supervised%2520learning%2520and%2520TC%2520knowledge%2520distillation%252C%2520overcoming%2520the%2520deficiencies%2520of%2520traditional%2520TC%2520training%2520in%2520aerial%2520tasks.%2520The%2520experimental%2520results%2520reveal%2520that%2520our%2520method%2520utilizing%2520merely%252030%255C%2525%2520of%2520labeled%2520data%252C%2520concurrently%2520elevates%2520mIoU%2520and%2520temporal%2520consistency%2520ensuring%2520stable%2520localization%2520of%2520terrain%2520objects.%2520Result%2520demo%2520%253A%2520https%253A//gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aerial%20View%20River%20Landform%20Video%20segmentation%3A%20A%20Weakly%20Supervised%20Context-aware%20Temporal%20Consistency%20Distillation%20Approach&entry.906535625=Chi-Han%20Chen%20and%20Chieh-Ming%20Chen%20and%20Wen-Huang%20Cheng%20and%20Ching-Chun%20Huang&entry.1292438233=The%20study%20of%20terrain%20and%20landform%20classification%20through%20UAV%20remote%20sensing%20diverges%20significantly%20from%20ground%20vehicle%20patrol%20tasks.%20Besides%20grappling%20with%20the%20complexity%20of%20data%20annotation%20and%20ensuring%20temporal%20consistency%2C%20it%20also%20confronts%20the%20scarcity%20of%20relevant%20data%20and%20the%20limitations%20imposed%20by%20the%20effective%20range%20of%20many%20technologies.%20This%20research%20substantiates%20that%2C%20in%20aerial%20positioning%20tasks%2C%20both%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20and%20temporal%20consistency%20%28TC%29%20metrics%20are%20of%20paramount%20importance.%20It%20is%20demonstrated%20that%20fully%20labeled%20data%20is%20not%20the%20optimal%20choice%2C%20as%20selecting%20only%20key%20data%20lacks%20the%20enhancement%20in%20TC%2C%20leading%20to%20failures.%20Hence%2C%20a%20teacher-student%20architecture%2C%20coupled%20with%20key%20frame%20selection%20and%20key%20frame%20updating%20algorithms%2C%20is%20proposed.%20This%20framework%20successfully%20performs%20weakly%20supervised%20learning%20and%20TC%20knowledge%20distillation%2C%20overcoming%20the%20deficiencies%20of%20traditional%20TC%20training%20in%20aerial%20tasks.%20The%20experimental%20results%20reveal%20that%20our%20method%20utilizing%20merely%2030%5C%25%20of%20labeled%20data%2C%20concurrently%20elevates%20mIoU%20and%20temporal%20consistency%20ensuring%20stable%20localization%20of%20terrain%20objects.%20Result%20demo%20%3A%20https%3A//gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection&entry.1838667208=http%3A//arxiv.org/abs/2511.16343v1&entry.124074799=Read"},
{"title": "Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution", "author": "Jaime \u00c1lvarez Urue\u00f1a and David Camacho and Javier Huertas Tato", "abstract": "The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.\n  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.\n  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\\% and 4.27\\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.", "link": "http://arxiv.org/abs/2511.16541v1", "date": "2025-11-20", "relevancy": 2.2203, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5624}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5546}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Contrastive%20Learning%20for%20Few-Shot%20AI-Generated%20Image%20Detection%20and%20Attribution&body=Title%3A%20Supervised%20Contrastive%20Learning%20for%20Few-Shot%20AI-Generated%20Image%20Detection%20and%20Attribution%0AAuthor%3A%20Jaime%20%C3%81lvarez%20Urue%C3%B1a%20and%20David%20Camacho%20and%20Javier%20Huertas%20Tato%0AAbstract%3A%20The%20rapid%20advancement%20of%20generative%20artificial%20intelligence%20has%20enabled%20the%20creation%20of%20synthetic%20images%20that%20are%20increasingly%20indistinguishable%20from%20authentic%20content%2C%20posing%20significant%20challenges%20for%20digital%20media%20integrity.%20This%20problem%20is%20compounded%20by%20the%20accelerated%20release%20cycle%20of%20novel%20generative%20models%2C%20which%20renders%20traditional%20detection%20approaches%20%28reliant%20on%20periodic%20retraining%29%20computationally%20infeasible%20and%20operationally%20impractical.%0A%20%20This%20work%20proposes%20a%20novel%20two-stage%20detection%20framework%20designed%20to%20address%20the%20generalization%20challenge%20inherent%20in%20synthetic%20image%20detection.%20The%20first%20stage%20employs%20a%20vision%20deep%20learning%20model%20trained%20via%20supervised%20contrastive%20learning%20to%20extract%20discriminative%20embeddings%20from%20input%20imagery.%20Critically%2C%20this%20model%20was%20trained%20on%20a%20strategically%20partitioned%20subset%20of%20available%20generators%2C%20with%20specific%20architectures%20withheld%20from%20training%20to%20rigorously%20ablate%20cross-generator%20generalization%20capabilities.%20The%20second%20stage%20utilizes%20a%20k-nearest%20neighbors%20%28k-NN%29%20classifier%20operating%20on%20the%20learned%20embedding%20space%2C%20trained%20in%20a%20few-shot%20learning%20paradigm%20incorporating%20limited%20samples%20from%20previously%20unseen%20test%20generators.%0A%20%20With%20merely%20150%20images%20per%20class%20in%20the%20few-shot%20learning%20regime%2C%20which%20are%20easily%20obtainable%20from%20current%20generation%20models%2C%20the%20proposed%20framework%20achieves%20an%20average%20detection%20accuracy%20of%2091.3%5C%25%2C%20representing%20a%205.2%20percentage%20point%20improvement%20over%20existing%20approaches%20.%20For%20the%20source%20attribution%20task%2C%20the%20proposed%20approach%20obtains%20improvements%20of%20of%2014.70%5C%25%20and%204.27%5C%25%20in%20AUC%20and%20OSCR%20respectively%20on%20an%20open%20set%20classification%20context%2C%20marking%20a%20significant%20advancement%20toward%20robust%2C%20scalable%20forensic%20attribution%20systems%20capable%20of%20adapting%20to%20the%20evolving%20generative%20AI%20landscape%20without%20requiring%20exhaustive%20retraining%20protocols.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Contrastive%2520Learning%2520for%2520Few-Shot%2520AI-Generated%2520Image%2520Detection%2520and%2520Attribution%26entry.906535625%3DJaime%2520%25C3%2581lvarez%2520Urue%25C3%25B1a%2520and%2520David%2520Camacho%2520and%2520Javier%2520Huertas%2520Tato%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520generative%2520artificial%2520intelligence%2520has%2520enabled%2520the%2520creation%2520of%2520synthetic%2520images%2520that%2520are%2520increasingly%2520indistinguishable%2520from%2520authentic%2520content%252C%2520posing%2520significant%2520challenges%2520for%2520digital%2520media%2520integrity.%2520This%2520problem%2520is%2520compounded%2520by%2520the%2520accelerated%2520release%2520cycle%2520of%2520novel%2520generative%2520models%252C%2520which%2520renders%2520traditional%2520detection%2520approaches%2520%2528reliant%2520on%2520periodic%2520retraining%2529%2520computationally%2520infeasible%2520and%2520operationally%2520impractical.%250A%2520%2520This%2520work%2520proposes%2520a%2520novel%2520two-stage%2520detection%2520framework%2520designed%2520to%2520address%2520the%2520generalization%2520challenge%2520inherent%2520in%2520synthetic%2520image%2520detection.%2520The%2520first%2520stage%2520employs%2520a%2520vision%2520deep%2520learning%2520model%2520trained%2520via%2520supervised%2520contrastive%2520learning%2520to%2520extract%2520discriminative%2520embeddings%2520from%2520input%2520imagery.%2520Critically%252C%2520this%2520model%2520was%2520trained%2520on%2520a%2520strategically%2520partitioned%2520subset%2520of%2520available%2520generators%252C%2520with%2520specific%2520architectures%2520withheld%2520from%2520training%2520to%2520rigorously%2520ablate%2520cross-generator%2520generalization%2520capabilities.%2520The%2520second%2520stage%2520utilizes%2520a%2520k-nearest%2520neighbors%2520%2528k-NN%2529%2520classifier%2520operating%2520on%2520the%2520learned%2520embedding%2520space%252C%2520trained%2520in%2520a%2520few-shot%2520learning%2520paradigm%2520incorporating%2520limited%2520samples%2520from%2520previously%2520unseen%2520test%2520generators.%250A%2520%2520With%2520merely%2520150%2520images%2520per%2520class%2520in%2520the%2520few-shot%2520learning%2520regime%252C%2520which%2520are%2520easily%2520obtainable%2520from%2520current%2520generation%2520models%252C%2520the%2520proposed%2520framework%2520achieves%2520an%2520average%2520detection%2520accuracy%2520of%252091.3%255C%2525%252C%2520representing%2520a%25205.2%2520percentage%2520point%2520improvement%2520over%2520existing%2520approaches%2520.%2520For%2520the%2520source%2520attribution%2520task%252C%2520the%2520proposed%2520approach%2520obtains%2520improvements%2520of%2520of%252014.70%255C%2525%2520and%25204.27%255C%2525%2520in%2520AUC%2520and%2520OSCR%2520respectively%2520on%2520an%2520open%2520set%2520classification%2520context%252C%2520marking%2520a%2520significant%2520advancement%2520toward%2520robust%252C%2520scalable%2520forensic%2520attribution%2520systems%2520capable%2520of%2520adapting%2520to%2520the%2520evolving%2520generative%2520AI%2520landscape%2520without%2520requiring%2520exhaustive%2520retraining%2520protocols.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Contrastive%20Learning%20for%20Few-Shot%20AI-Generated%20Image%20Detection%20and%20Attribution&entry.906535625=Jaime%20%C3%81lvarez%20Urue%C3%B1a%20and%20David%20Camacho%20and%20Javier%20Huertas%20Tato&entry.1292438233=The%20rapid%20advancement%20of%20generative%20artificial%20intelligence%20has%20enabled%20the%20creation%20of%20synthetic%20images%20that%20are%20increasingly%20indistinguishable%20from%20authentic%20content%2C%20posing%20significant%20challenges%20for%20digital%20media%20integrity.%20This%20problem%20is%20compounded%20by%20the%20accelerated%20release%20cycle%20of%20novel%20generative%20models%2C%20which%20renders%20traditional%20detection%20approaches%20%28reliant%20on%20periodic%20retraining%29%20computationally%20infeasible%20and%20operationally%20impractical.%0A%20%20This%20work%20proposes%20a%20novel%20two-stage%20detection%20framework%20designed%20to%20address%20the%20generalization%20challenge%20inherent%20in%20synthetic%20image%20detection.%20The%20first%20stage%20employs%20a%20vision%20deep%20learning%20model%20trained%20via%20supervised%20contrastive%20learning%20to%20extract%20discriminative%20embeddings%20from%20input%20imagery.%20Critically%2C%20this%20model%20was%20trained%20on%20a%20strategically%20partitioned%20subset%20of%20available%20generators%2C%20with%20specific%20architectures%20withheld%20from%20training%20to%20rigorously%20ablate%20cross-generator%20generalization%20capabilities.%20The%20second%20stage%20utilizes%20a%20k-nearest%20neighbors%20%28k-NN%29%20classifier%20operating%20on%20the%20learned%20embedding%20space%2C%20trained%20in%20a%20few-shot%20learning%20paradigm%20incorporating%20limited%20samples%20from%20previously%20unseen%20test%20generators.%0A%20%20With%20merely%20150%20images%20per%20class%20in%20the%20few-shot%20learning%20regime%2C%20which%20are%20easily%20obtainable%20from%20current%20generation%20models%2C%20the%20proposed%20framework%20achieves%20an%20average%20detection%20accuracy%20of%2091.3%5C%25%2C%20representing%20a%205.2%20percentage%20point%20improvement%20over%20existing%20approaches%20.%20For%20the%20source%20attribution%20task%2C%20the%20proposed%20approach%20obtains%20improvements%20of%20of%2014.70%5C%25%20and%204.27%5C%25%20in%20AUC%20and%20OSCR%20respectively%20on%20an%20open%20set%20classification%20context%2C%20marking%20a%20significant%20advancement%20toward%20robust%2C%20scalable%20forensic%20attribution%20systems%20capable%20of%20adapting%20to%20the%20evolving%20generative%20AI%20landscape%20without%20requiring%20exhaustive%20retraining%20protocols.&entry.1838667208=http%3A//arxiv.org/abs/2511.16541v1&entry.124074799=Read"},
{"title": "FastSurfer-CC: A robust, accurate, and comprehensive framework for corpus callosum morphometry", "author": "Clemens Pollak and Kersten Diers and Santiago Estrada and David K\u00fcgler and Martin Reuter", "abstract": "The corpus callosum, the largest commissural structure in the human brain, is a central focus in research on aging and neurological diseases. It is also a critical target for interventions such as deep brain stimulation and serves as an important biomarker in clinical trials, including those investigating remyelination therapies. Despite extensive research on corpus callosum segmentation, few publicly available tools provide a comprehensive and automated analysis pipeline. To address this gap, we present FastSurfer-CC, an efficient and fully automated framework for corpus callosum morphometry. FastSurfer-CC automatically identifies mid-sagittal slices, segments the corpus callosum and fornix, localizes the anterior and posterior commissures to standardize head positioning, generates thickness profiles and subdivisions, and extracts eight shape metrics for statistical analysis. We demonstrate that FastSurfer-CC outperforms existing specialized tools across the individual tasks. Moreover, our method reveals statistically significant differences between Huntington's disease patients and healthy controls that are not detected by the current state-of-the-art.", "link": "http://arxiv.org/abs/2511.16471v1", "date": "2025-11-20", "relevancy": 2.2128, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastSurfer-CC%3A%20A%20robust%2C%20accurate%2C%20and%20comprehensive%20framework%20for%20corpus%20callosum%20morphometry&body=Title%3A%20FastSurfer-CC%3A%20A%20robust%2C%20accurate%2C%20and%20comprehensive%20framework%20for%20corpus%20callosum%20morphometry%0AAuthor%3A%20Clemens%20Pollak%20and%20Kersten%20Diers%20and%20Santiago%20Estrada%20and%20David%20K%C3%BCgler%20and%20Martin%20Reuter%0AAbstract%3A%20The%20corpus%20callosum%2C%20the%20largest%20commissural%20structure%20in%20the%20human%20brain%2C%20is%20a%20central%20focus%20in%20research%20on%20aging%20and%20neurological%20diseases.%20It%20is%20also%20a%20critical%20target%20for%20interventions%20such%20as%20deep%20brain%20stimulation%20and%20serves%20as%20an%20important%20biomarker%20in%20clinical%20trials%2C%20including%20those%20investigating%20remyelination%20therapies.%20Despite%20extensive%20research%20on%20corpus%20callosum%20segmentation%2C%20few%20publicly%20available%20tools%20provide%20a%20comprehensive%20and%20automated%20analysis%20pipeline.%20To%20address%20this%20gap%2C%20we%20present%20FastSurfer-CC%2C%20an%20efficient%20and%20fully%20automated%20framework%20for%20corpus%20callosum%20morphometry.%20FastSurfer-CC%20automatically%20identifies%20mid-sagittal%20slices%2C%20segments%20the%20corpus%20callosum%20and%20fornix%2C%20localizes%20the%20anterior%20and%20posterior%20commissures%20to%20standardize%20head%20positioning%2C%20generates%20thickness%20profiles%20and%20subdivisions%2C%20and%20extracts%20eight%20shape%20metrics%20for%20statistical%20analysis.%20We%20demonstrate%20that%20FastSurfer-CC%20outperforms%20existing%20specialized%20tools%20across%20the%20individual%20tasks.%20Moreover%2C%20our%20method%20reveals%20statistically%20significant%20differences%20between%20Huntington%27s%20disease%20patients%20and%20healthy%20controls%20that%20are%20not%20detected%20by%20the%20current%20state-of-the-art.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastSurfer-CC%253A%2520A%2520robust%252C%2520accurate%252C%2520and%2520comprehensive%2520framework%2520for%2520corpus%2520callosum%2520morphometry%26entry.906535625%3DClemens%2520Pollak%2520and%2520Kersten%2520Diers%2520and%2520Santiago%2520Estrada%2520and%2520David%2520K%25C3%25BCgler%2520and%2520Martin%2520Reuter%26entry.1292438233%3DThe%2520corpus%2520callosum%252C%2520the%2520largest%2520commissural%2520structure%2520in%2520the%2520human%2520brain%252C%2520is%2520a%2520central%2520focus%2520in%2520research%2520on%2520aging%2520and%2520neurological%2520diseases.%2520It%2520is%2520also%2520a%2520critical%2520target%2520for%2520interventions%2520such%2520as%2520deep%2520brain%2520stimulation%2520and%2520serves%2520as%2520an%2520important%2520biomarker%2520in%2520clinical%2520trials%252C%2520including%2520those%2520investigating%2520remyelination%2520therapies.%2520Despite%2520extensive%2520research%2520on%2520corpus%2520callosum%2520segmentation%252C%2520few%2520publicly%2520available%2520tools%2520provide%2520a%2520comprehensive%2520and%2520automated%2520analysis%2520pipeline.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520FastSurfer-CC%252C%2520an%2520efficient%2520and%2520fully%2520automated%2520framework%2520for%2520corpus%2520callosum%2520morphometry.%2520FastSurfer-CC%2520automatically%2520identifies%2520mid-sagittal%2520slices%252C%2520segments%2520the%2520corpus%2520callosum%2520and%2520fornix%252C%2520localizes%2520the%2520anterior%2520and%2520posterior%2520commissures%2520to%2520standardize%2520head%2520positioning%252C%2520generates%2520thickness%2520profiles%2520and%2520subdivisions%252C%2520and%2520extracts%2520eight%2520shape%2520metrics%2520for%2520statistical%2520analysis.%2520We%2520demonstrate%2520that%2520FastSurfer-CC%2520outperforms%2520existing%2520specialized%2520tools%2520across%2520the%2520individual%2520tasks.%2520Moreover%252C%2520our%2520method%2520reveals%2520statistically%2520significant%2520differences%2520between%2520Huntington%2527s%2520disease%2520patients%2520and%2520healthy%2520controls%2520that%2520are%2520not%2520detected%2520by%2520the%2520current%2520state-of-the-art.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastSurfer-CC%3A%20A%20robust%2C%20accurate%2C%20and%20comprehensive%20framework%20for%20corpus%20callosum%20morphometry&entry.906535625=Clemens%20Pollak%20and%20Kersten%20Diers%20and%20Santiago%20Estrada%20and%20David%20K%C3%BCgler%20and%20Martin%20Reuter&entry.1292438233=The%20corpus%20callosum%2C%20the%20largest%20commissural%20structure%20in%20the%20human%20brain%2C%20is%20a%20central%20focus%20in%20research%20on%20aging%20and%20neurological%20diseases.%20It%20is%20also%20a%20critical%20target%20for%20interventions%20such%20as%20deep%20brain%20stimulation%20and%20serves%20as%20an%20important%20biomarker%20in%20clinical%20trials%2C%20including%20those%20investigating%20remyelination%20therapies.%20Despite%20extensive%20research%20on%20corpus%20callosum%20segmentation%2C%20few%20publicly%20available%20tools%20provide%20a%20comprehensive%20and%20automated%20analysis%20pipeline.%20To%20address%20this%20gap%2C%20we%20present%20FastSurfer-CC%2C%20an%20efficient%20and%20fully%20automated%20framework%20for%20corpus%20callosum%20morphometry.%20FastSurfer-CC%20automatically%20identifies%20mid-sagittal%20slices%2C%20segments%20the%20corpus%20callosum%20and%20fornix%2C%20localizes%20the%20anterior%20and%20posterior%20commissures%20to%20standardize%20head%20positioning%2C%20generates%20thickness%20profiles%20and%20subdivisions%2C%20and%20extracts%20eight%20shape%20metrics%20for%20statistical%20analysis.%20We%20demonstrate%20that%20FastSurfer-CC%20outperforms%20existing%20specialized%20tools%20across%20the%20individual%20tasks.%20Moreover%2C%20our%20method%20reveals%20statistically%20significant%20differences%20between%20Huntington%27s%20disease%20patients%20and%20healthy%20controls%20that%20are%20not%20detected%20by%20the%20current%20state-of-the-art.&entry.1838667208=http%3A//arxiv.org/abs/2511.16471v1&entry.124074799=Read"},
{"title": "Solving Spatial Supersensing Without Spatial Supersensing", "author": "Vishaal Udandarao and Shyamgopal Karthik and Surabhi S. Nath and Andreas Hochlehnert and Matthias Bethge and Ameya Prabhu", "abstract": "Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity", "link": "http://arxiv.org/abs/2511.16655v1", "date": "2025-11-20", "relevancy": 2.2091, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Spatial%20Supersensing%20Without%20Spatial%20Supersensing&body=Title%3A%20Solving%20Spatial%20Supersensing%20Without%20Spatial%20Supersensing%0AAuthor%3A%20Vishaal%20Udandarao%20and%20Shyamgopal%20Karthik%20and%20Surabhi%20S.%20Nath%20and%20Andreas%20Hochlehnert%20and%20Matthias%20Bethge%20and%20Ameya%20Prabhu%0AAbstract%3A%20Cambrian-S%20aims%20to%20take%20the%20first%20steps%20towards%20improving%20video%20world%20models%20with%20spatial%20supersensing%20by%20introducing%20%28i%29%20two%20benchmarks%2C%20VSI-Super-Recall%20%28VSR%29%20and%20VSI-Super-Counting%20%28VSC%29%2C%20and%20%28ii%29%20bespoke%20predictive%20sensing%20inference%20strategies%20tailored%20to%20each%20benchmark.%20In%20this%20work%2C%20we%20conduct%20a%20critical%20analysis%20of%20Cambrian-S%20across%20both%20these%20fronts.%20First%2C%20we%20introduce%20a%20simple%20baseline%2C%20NoSense%2C%20which%20discards%20almost%20all%20temporal%20structure%20and%20uses%20only%20a%20bag-of-words%20SigLIP%20model%2C%20yet%20near-perfectly%20solves%20VSR%2C%20achieving%2095%25%20accuracy%20even%20on%204-hour%20videos.%20This%20shows%20benchmarks%20like%20VSR%20can%20be%20nearly%20solved%20without%20spatial%20cognition%2C%20world%20modeling%20or%20spatial%20supersensing.%20Second%2C%20we%20hypothesize%20that%20the%20tailored%20inference%20methods%20proposed%20by%20Cambrian-S%20likely%20exploit%20shortcut%20heuristics%20in%20the%20benchmark.%20We%20illustrate%20this%20with%20a%20simple%20sanity%20check%20on%20the%20VSC%20benchmark%2C%20called%20VSC-Repeat%3A%20We%20concatenate%20each%20video%20with%20itself%201-5%20times%2C%20which%20does%20not%20change%20the%20number%20of%20unique%20objects.%20However%2C%20this%20simple%20perturbation%20entirely%20collapses%20the%20mean%20relative%20accuracy%20of%20Cambrian-S%20from%2042%25%20to%200%25.%20A%20system%20that%20performs%20spatial%20supersensing%20and%20integrates%20information%20across%20experiences%20should%20recognize%20views%20of%20the%20same%20scene%20and%20keep%20object-count%20predictions%20unchanged%3B%20instead%2C%20Cambrian-S%20inference%20algorithm%20relies%20largely%20on%20a%20shortcut%20in%20the%20VSC%20benchmark%20that%20rooms%20are%20never%20revisited.%20Taken%20together%2C%20our%20findings%20suggest%20that%20%28i%29%20current%20VSI-Super%20benchmarks%20do%20not%20yet%20reliably%20measure%20spatial%20supersensing%2C%20and%20%28ii%29%20predictive-sensing%20inference%20recipes%20used%20by%20Cambrian-S%20improve%20performance%20by%20inadvertently%20exploiting%20shortcuts%20rather%20than%20from%20robust%20spatial%20supersensing.%20We%20include%20the%20response%20from%20the%20Cambrian-S%20authors%20%28in%20Appendix%20A%29%20to%20provide%20a%20balanced%20perspective%20alongside%20our%20claims.%20We%20release%20our%20code%20at%3A%20https%3A//github.com/bethgelab/supersanity%0ALink%3A%20http%3A//arxiv.org/abs/2511.16655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Spatial%2520Supersensing%2520Without%2520Spatial%2520Supersensing%26entry.906535625%3DVishaal%2520Udandarao%2520and%2520Shyamgopal%2520Karthik%2520and%2520Surabhi%2520S.%2520Nath%2520and%2520Andreas%2520Hochlehnert%2520and%2520Matthias%2520Bethge%2520and%2520Ameya%2520Prabhu%26entry.1292438233%3DCambrian-S%2520aims%2520to%2520take%2520the%2520first%2520steps%2520towards%2520improving%2520video%2520world%2520models%2520with%2520spatial%2520supersensing%2520by%2520introducing%2520%2528i%2529%2520two%2520benchmarks%252C%2520VSI-Super-Recall%2520%2528VSR%2529%2520and%2520VSI-Super-Counting%2520%2528VSC%2529%252C%2520and%2520%2528ii%2529%2520bespoke%2520predictive%2520sensing%2520inference%2520strategies%2520tailored%2520to%2520each%2520benchmark.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520critical%2520analysis%2520of%2520Cambrian-S%2520across%2520both%2520these%2520fronts.%2520First%252C%2520we%2520introduce%2520a%2520simple%2520baseline%252C%2520NoSense%252C%2520which%2520discards%2520almost%2520all%2520temporal%2520structure%2520and%2520uses%2520only%2520a%2520bag-of-words%2520SigLIP%2520model%252C%2520yet%2520near-perfectly%2520solves%2520VSR%252C%2520achieving%252095%2525%2520accuracy%2520even%2520on%25204-hour%2520videos.%2520This%2520shows%2520benchmarks%2520like%2520VSR%2520can%2520be%2520nearly%2520solved%2520without%2520spatial%2520cognition%252C%2520world%2520modeling%2520or%2520spatial%2520supersensing.%2520Second%252C%2520we%2520hypothesize%2520that%2520the%2520tailored%2520inference%2520methods%2520proposed%2520by%2520Cambrian-S%2520likely%2520exploit%2520shortcut%2520heuristics%2520in%2520the%2520benchmark.%2520We%2520illustrate%2520this%2520with%2520a%2520simple%2520sanity%2520check%2520on%2520the%2520VSC%2520benchmark%252C%2520called%2520VSC-Repeat%253A%2520We%2520concatenate%2520each%2520video%2520with%2520itself%25201-5%2520times%252C%2520which%2520does%2520not%2520change%2520the%2520number%2520of%2520unique%2520objects.%2520However%252C%2520this%2520simple%2520perturbation%2520entirely%2520collapses%2520the%2520mean%2520relative%2520accuracy%2520of%2520Cambrian-S%2520from%252042%2525%2520to%25200%2525.%2520A%2520system%2520that%2520performs%2520spatial%2520supersensing%2520and%2520integrates%2520information%2520across%2520experiences%2520should%2520recognize%2520views%2520of%2520the%2520same%2520scene%2520and%2520keep%2520object-count%2520predictions%2520unchanged%253B%2520instead%252C%2520Cambrian-S%2520inference%2520algorithm%2520relies%2520largely%2520on%2520a%2520shortcut%2520in%2520the%2520VSC%2520benchmark%2520that%2520rooms%2520are%2520never%2520revisited.%2520Taken%2520together%252C%2520our%2520findings%2520suggest%2520that%2520%2528i%2529%2520current%2520VSI-Super%2520benchmarks%2520do%2520not%2520yet%2520reliably%2520measure%2520spatial%2520supersensing%252C%2520and%2520%2528ii%2529%2520predictive-sensing%2520inference%2520recipes%2520used%2520by%2520Cambrian-S%2520improve%2520performance%2520by%2520inadvertently%2520exploiting%2520shortcuts%2520rather%2520than%2520from%2520robust%2520spatial%2520supersensing.%2520We%2520include%2520the%2520response%2520from%2520the%2520Cambrian-S%2520authors%2520%2528in%2520Appendix%2520A%2529%2520to%2520provide%2520a%2520balanced%2520perspective%2520alongside%2520our%2520claims.%2520We%2520release%2520our%2520code%2520at%253A%2520https%253A//github.com/bethgelab/supersanity%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Spatial%20Supersensing%20Without%20Spatial%20Supersensing&entry.906535625=Vishaal%20Udandarao%20and%20Shyamgopal%20Karthik%20and%20Surabhi%20S.%20Nath%20and%20Andreas%20Hochlehnert%20and%20Matthias%20Bethge%20and%20Ameya%20Prabhu&entry.1292438233=Cambrian-S%20aims%20to%20take%20the%20first%20steps%20towards%20improving%20video%20world%20models%20with%20spatial%20supersensing%20by%20introducing%20%28i%29%20two%20benchmarks%2C%20VSI-Super-Recall%20%28VSR%29%20and%20VSI-Super-Counting%20%28VSC%29%2C%20and%20%28ii%29%20bespoke%20predictive%20sensing%20inference%20strategies%20tailored%20to%20each%20benchmark.%20In%20this%20work%2C%20we%20conduct%20a%20critical%20analysis%20of%20Cambrian-S%20across%20both%20these%20fronts.%20First%2C%20we%20introduce%20a%20simple%20baseline%2C%20NoSense%2C%20which%20discards%20almost%20all%20temporal%20structure%20and%20uses%20only%20a%20bag-of-words%20SigLIP%20model%2C%20yet%20near-perfectly%20solves%20VSR%2C%20achieving%2095%25%20accuracy%20even%20on%204-hour%20videos.%20This%20shows%20benchmarks%20like%20VSR%20can%20be%20nearly%20solved%20without%20spatial%20cognition%2C%20world%20modeling%20or%20spatial%20supersensing.%20Second%2C%20we%20hypothesize%20that%20the%20tailored%20inference%20methods%20proposed%20by%20Cambrian-S%20likely%20exploit%20shortcut%20heuristics%20in%20the%20benchmark.%20We%20illustrate%20this%20with%20a%20simple%20sanity%20check%20on%20the%20VSC%20benchmark%2C%20called%20VSC-Repeat%3A%20We%20concatenate%20each%20video%20with%20itself%201-5%20times%2C%20which%20does%20not%20change%20the%20number%20of%20unique%20objects.%20However%2C%20this%20simple%20perturbation%20entirely%20collapses%20the%20mean%20relative%20accuracy%20of%20Cambrian-S%20from%2042%25%20to%200%25.%20A%20system%20that%20performs%20spatial%20supersensing%20and%20integrates%20information%20across%20experiences%20should%20recognize%20views%20of%20the%20same%20scene%20and%20keep%20object-count%20predictions%20unchanged%3B%20instead%2C%20Cambrian-S%20inference%20algorithm%20relies%20largely%20on%20a%20shortcut%20in%20the%20VSC%20benchmark%20that%20rooms%20are%20never%20revisited.%20Taken%20together%2C%20our%20findings%20suggest%20that%20%28i%29%20current%20VSI-Super%20benchmarks%20do%20not%20yet%20reliably%20measure%20spatial%20supersensing%2C%20and%20%28ii%29%20predictive-sensing%20inference%20recipes%20used%20by%20Cambrian-S%20improve%20performance%20by%20inadvertently%20exploiting%20shortcuts%20rather%20than%20from%20robust%20spatial%20supersensing.%20We%20include%20the%20response%20from%20the%20Cambrian-S%20authors%20%28in%20Appendix%20A%29%20to%20provide%20a%20balanced%20perspective%20alongside%20our%20claims.%20We%20release%20our%20code%20at%3A%20https%3A//github.com/bethgelab/supersanity&entry.1838667208=http%3A//arxiv.org/abs/2511.16655v1&entry.124074799=Read"},
{"title": "CD-DPE: Dual-Prompt Expert Network based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution", "author": "Xianming Gu and Lihui Wang and Ying Cao and Zeyu Deng and Yingfeng Ou and Guodong Hu and Yi Chen", "abstract": "Multi-contrast magnetic resonance imaging (MRI) super-resolution intends to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging structural information present in HR reference images acquired with different contrasts. This technique enhances anatomical detail and soft tissue differentiation, which is vital for early diagnosis and clinical decision-making. However, inherent contrasts disparities between modalities pose fundamental challenges in effectively utilizing reference image textures to guide target image reconstruction, often resulting in suboptimal feature integration. To address this issue, we propose a dual-prompt expert network based on a convolutional dictionary feature decoupling (CD-DPE) strategy for multi-contrast MRI super-resolution. Specifically, we introduce an iterative convolutional dictionary feature decoupling module (CD-FDM) to separate features into cross-contrast and intra-contrast components, thereby reducing redundancy and interference. To fully integrate these features, a novel dual-prompt feature fusion expert module (DP-FFEM) is proposed. This module uses a frequency prompt to guide the selection of relevant reference features for incorporation into the target image, while an adaptive routing prompt determines the optimal method for fusing reference and target features to enhance reconstruction quality. Extensive experiments on public multi-contrast MRI datasets demonstrate that CD-DPE outperforms state-of-the-art methods in reconstructing fine details. Additionally, experiments on unseen datasets demonstrated that CD-DPE exhibits strong generalization capabilities.", "link": "http://arxiv.org/abs/2511.14014v2", "date": "2025-11-20", "relevancy": 2.207, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5727}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5559}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CD-DPE%3A%20Dual-Prompt%20Expert%20Network%20based%20on%20Convolutional%20Dictionary%20Feature%20Decoupling%20for%20Multi-Contrast%20MRI%20Super-Resolution&body=Title%3A%20CD-DPE%3A%20Dual-Prompt%20Expert%20Network%20based%20on%20Convolutional%20Dictionary%20Feature%20Decoupling%20for%20Multi-Contrast%20MRI%20Super-Resolution%0AAuthor%3A%20Xianming%20Gu%20and%20Lihui%20Wang%20and%20Ying%20Cao%20and%20Zeyu%20Deng%20and%20Yingfeng%20Ou%20and%20Guodong%20Hu%20and%20Yi%20Chen%0AAbstract%3A%20Multi-contrast%20magnetic%20resonance%20imaging%20%28MRI%29%20super-resolution%20intends%20to%20reconstruct%20high-resolution%20%28HR%29%20images%20from%20low-resolution%20%28LR%29%20scans%20by%20leveraging%20structural%20information%20present%20in%20HR%20reference%20images%20acquired%20with%20different%20contrasts.%20This%20technique%20enhances%20anatomical%20detail%20and%20soft%20tissue%20differentiation%2C%20which%20is%20vital%20for%20early%20diagnosis%20and%20clinical%20decision-making.%20However%2C%20inherent%20contrasts%20disparities%20between%20modalities%20pose%20fundamental%20challenges%20in%20effectively%20utilizing%20reference%20image%20textures%20to%20guide%20target%20image%20reconstruction%2C%20often%20resulting%20in%20suboptimal%20feature%20integration.%20To%20address%20this%20issue%2C%20we%20propose%20a%20dual-prompt%20expert%20network%20based%20on%20a%20convolutional%20dictionary%20feature%20decoupling%20%28CD-DPE%29%20strategy%20for%20multi-contrast%20MRI%20super-resolution.%20Specifically%2C%20we%20introduce%20an%20iterative%20convolutional%20dictionary%20feature%20decoupling%20module%20%28CD-FDM%29%20to%20separate%20features%20into%20cross-contrast%20and%20intra-contrast%20components%2C%20thereby%20reducing%20redundancy%20and%20interference.%20To%20fully%20integrate%20these%20features%2C%20a%20novel%20dual-prompt%20feature%20fusion%20expert%20module%20%28DP-FFEM%29%20is%20proposed.%20This%20module%20uses%20a%20frequency%20prompt%20to%20guide%20the%20selection%20of%20relevant%20reference%20features%20for%20incorporation%20into%20the%20target%20image%2C%20while%20an%20adaptive%20routing%20prompt%20determines%20the%20optimal%20method%20for%20fusing%20reference%20and%20target%20features%20to%20enhance%20reconstruction%20quality.%20Extensive%20experiments%20on%20public%20multi-contrast%20MRI%20datasets%20demonstrate%20that%20CD-DPE%20outperforms%20state-of-the-art%20methods%20in%20reconstructing%20fine%20details.%20Additionally%2C%20experiments%20on%20unseen%20datasets%20demonstrated%20that%20CD-DPE%20exhibits%20strong%20generalization%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14014v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCD-DPE%253A%2520Dual-Prompt%2520Expert%2520Network%2520based%2520on%2520Convolutional%2520Dictionary%2520Feature%2520Decoupling%2520for%2520Multi-Contrast%2520MRI%2520Super-Resolution%26entry.906535625%3DXianming%2520Gu%2520and%2520Lihui%2520Wang%2520and%2520Ying%2520Cao%2520and%2520Zeyu%2520Deng%2520and%2520Yingfeng%2520Ou%2520and%2520Guodong%2520Hu%2520and%2520Yi%2520Chen%26entry.1292438233%3DMulti-contrast%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520super-resolution%2520intends%2520to%2520reconstruct%2520high-resolution%2520%2528HR%2529%2520images%2520from%2520low-resolution%2520%2528LR%2529%2520scans%2520by%2520leveraging%2520structural%2520information%2520present%2520in%2520HR%2520reference%2520images%2520acquired%2520with%2520different%2520contrasts.%2520This%2520technique%2520enhances%2520anatomical%2520detail%2520and%2520soft%2520tissue%2520differentiation%252C%2520which%2520is%2520vital%2520for%2520early%2520diagnosis%2520and%2520clinical%2520decision-making.%2520However%252C%2520inherent%2520contrasts%2520disparities%2520between%2520modalities%2520pose%2520fundamental%2520challenges%2520in%2520effectively%2520utilizing%2520reference%2520image%2520textures%2520to%2520guide%2520target%2520image%2520reconstruction%252C%2520often%2520resulting%2520in%2520suboptimal%2520feature%2520integration.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520dual-prompt%2520expert%2520network%2520based%2520on%2520a%2520convolutional%2520dictionary%2520feature%2520decoupling%2520%2528CD-DPE%2529%2520strategy%2520for%2520multi-contrast%2520MRI%2520super-resolution.%2520Specifically%252C%2520we%2520introduce%2520an%2520iterative%2520convolutional%2520dictionary%2520feature%2520decoupling%2520module%2520%2528CD-FDM%2529%2520to%2520separate%2520features%2520into%2520cross-contrast%2520and%2520intra-contrast%2520components%252C%2520thereby%2520reducing%2520redundancy%2520and%2520interference.%2520To%2520fully%2520integrate%2520these%2520features%252C%2520a%2520novel%2520dual-prompt%2520feature%2520fusion%2520expert%2520module%2520%2528DP-FFEM%2529%2520is%2520proposed.%2520This%2520module%2520uses%2520a%2520frequency%2520prompt%2520to%2520guide%2520the%2520selection%2520of%2520relevant%2520reference%2520features%2520for%2520incorporation%2520into%2520the%2520target%2520image%252C%2520while%2520an%2520adaptive%2520routing%2520prompt%2520determines%2520the%2520optimal%2520method%2520for%2520fusing%2520reference%2520and%2520target%2520features%2520to%2520enhance%2520reconstruction%2520quality.%2520Extensive%2520experiments%2520on%2520public%2520multi-contrast%2520MRI%2520datasets%2520demonstrate%2520that%2520CD-DPE%2520outperforms%2520state-of-the-art%2520methods%2520in%2520reconstructing%2520fine%2520details.%2520Additionally%252C%2520experiments%2520on%2520unseen%2520datasets%2520demonstrated%2520that%2520CD-DPE%2520exhibits%2520strong%2520generalization%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14014v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CD-DPE%3A%20Dual-Prompt%20Expert%20Network%20based%20on%20Convolutional%20Dictionary%20Feature%20Decoupling%20for%20Multi-Contrast%20MRI%20Super-Resolution&entry.906535625=Xianming%20Gu%20and%20Lihui%20Wang%20and%20Ying%20Cao%20and%20Zeyu%20Deng%20and%20Yingfeng%20Ou%20and%20Guodong%20Hu%20and%20Yi%20Chen&entry.1292438233=Multi-contrast%20magnetic%20resonance%20imaging%20%28MRI%29%20super-resolution%20intends%20to%20reconstruct%20high-resolution%20%28HR%29%20images%20from%20low-resolution%20%28LR%29%20scans%20by%20leveraging%20structural%20information%20present%20in%20HR%20reference%20images%20acquired%20with%20different%20contrasts.%20This%20technique%20enhances%20anatomical%20detail%20and%20soft%20tissue%20differentiation%2C%20which%20is%20vital%20for%20early%20diagnosis%20and%20clinical%20decision-making.%20However%2C%20inherent%20contrasts%20disparities%20between%20modalities%20pose%20fundamental%20challenges%20in%20effectively%20utilizing%20reference%20image%20textures%20to%20guide%20target%20image%20reconstruction%2C%20often%20resulting%20in%20suboptimal%20feature%20integration.%20To%20address%20this%20issue%2C%20we%20propose%20a%20dual-prompt%20expert%20network%20based%20on%20a%20convolutional%20dictionary%20feature%20decoupling%20%28CD-DPE%29%20strategy%20for%20multi-contrast%20MRI%20super-resolution.%20Specifically%2C%20we%20introduce%20an%20iterative%20convolutional%20dictionary%20feature%20decoupling%20module%20%28CD-FDM%29%20to%20separate%20features%20into%20cross-contrast%20and%20intra-contrast%20components%2C%20thereby%20reducing%20redundancy%20and%20interference.%20To%20fully%20integrate%20these%20features%2C%20a%20novel%20dual-prompt%20feature%20fusion%20expert%20module%20%28DP-FFEM%29%20is%20proposed.%20This%20module%20uses%20a%20frequency%20prompt%20to%20guide%20the%20selection%20of%20relevant%20reference%20features%20for%20incorporation%20into%20the%20target%20image%2C%20while%20an%20adaptive%20routing%20prompt%20determines%20the%20optimal%20method%20for%20fusing%20reference%20and%20target%20features%20to%20enhance%20reconstruction%20quality.%20Extensive%20experiments%20on%20public%20multi-contrast%20MRI%20datasets%20demonstrate%20that%20CD-DPE%20outperforms%20state-of-the-art%20methods%20in%20reconstructing%20fine%20details.%20Additionally%2C%20experiments%20on%20unseen%20datasets%20demonstrated%20that%20CD-DPE%20exhibits%20strong%20generalization%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2511.14014v2&entry.124074799=Read"},
{"title": "Arctic-Extract Technical Report", "author": "Mateusz Chili\u0144ski and Julita O\u0142tusek and Wojciech Ja\u015bkowski", "abstract": "Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.", "link": "http://arxiv.org/abs/2511.16470v1", "date": "2025-11-20", "relevancy": 1.8018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4522}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arctic-Extract%20Technical%20Report&body=Title%3A%20Arctic-Extract%20Technical%20Report%0AAuthor%3A%20Mateusz%20Chili%C5%84ski%20and%20Julita%20O%C5%82tusek%20and%20Wojciech%20Ja%C5%9Bkowski%0AAbstract%3A%20Arctic-Extract%20is%20a%20state-of-the-art%20model%20designed%20for%20extracting%20structural%20data%20%28question%20answering%2C%20entities%20and%20tables%29%20from%20scanned%20or%20digital-born%20business%20documents.%20Despite%20its%20SoTA%20capabilities%2C%20the%20model%20is%20deployable%20on%20resource-constrained%20hardware%2C%20weighting%20only%206.6%20GiB%2C%20making%20it%20suitable%20for%20deployment%20on%20devices%20with%20limited%20resources%2C%20such%20as%20A10%20GPUs%20with%2024%20GB%20of%20memory.%20Arctic-Extract%20can%20process%20up%20to%20125%20A4%20pages%20on%20those%20GPUs%2C%20making%20suitable%20for%20long%20document%20processing.%20This%20paper%20highlights%20Arctic-Extract%27s%20training%20protocols%20and%20evaluation%20results%2C%20demonstrating%20its%20strong%20performance%20in%20document%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArctic-Extract%2520Technical%2520Report%26entry.906535625%3DMateusz%2520Chili%25C5%2584ski%2520and%2520Julita%2520O%25C5%2582tusek%2520and%2520Wojciech%2520Ja%25C5%259Bkowski%26entry.1292438233%3DArctic-Extract%2520is%2520a%2520state-of-the-art%2520model%2520designed%2520for%2520extracting%2520structural%2520data%2520%2528question%2520answering%252C%2520entities%2520and%2520tables%2529%2520from%2520scanned%2520or%2520digital-born%2520business%2520documents.%2520Despite%2520its%2520SoTA%2520capabilities%252C%2520the%2520model%2520is%2520deployable%2520on%2520resource-constrained%2520hardware%252C%2520weighting%2520only%25206.6%2520GiB%252C%2520making%2520it%2520suitable%2520for%2520deployment%2520on%2520devices%2520with%2520limited%2520resources%252C%2520such%2520as%2520A10%2520GPUs%2520with%252024%2520GB%2520of%2520memory.%2520Arctic-Extract%2520can%2520process%2520up%2520to%2520125%2520A4%2520pages%2520on%2520those%2520GPUs%252C%2520making%2520suitable%2520for%2520long%2520document%2520processing.%2520This%2520paper%2520highlights%2520Arctic-Extract%2527s%2520training%2520protocols%2520and%2520evaluation%2520results%252C%2520demonstrating%2520its%2520strong%2520performance%2520in%2520document%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arctic-Extract%20Technical%20Report&entry.906535625=Mateusz%20Chili%C5%84ski%20and%20Julita%20O%C5%82tusek%20and%20Wojciech%20Ja%C5%9Bkowski&entry.1292438233=Arctic-Extract%20is%20a%20state-of-the-art%20model%20designed%20for%20extracting%20structural%20data%20%28question%20answering%2C%20entities%20and%20tables%29%20from%20scanned%20or%20digital-born%20business%20documents.%20Despite%20its%20SoTA%20capabilities%2C%20the%20model%20is%20deployable%20on%20resource-constrained%20hardware%2C%20weighting%20only%206.6%20GiB%2C%20making%20it%20suitable%20for%20deployment%20on%20devices%20with%20limited%20resources%2C%20such%20as%20A10%20GPUs%20with%2024%20GB%20of%20memory.%20Arctic-Extract%20can%20process%20up%20to%20125%20A4%20pages%20on%20those%20GPUs%2C%20making%20suitable%20for%20long%20document%20processing.%20This%20paper%20highlights%20Arctic-Extract%27s%20training%20protocols%20and%20evaluation%20results%2C%20demonstrating%20its%20strong%20performance%20in%20document%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2511.16470v1&entry.124074799=Read"},
{"title": "FairLRF: Achieving Fairness through Sparse Low Rank Factorization", "author": "Yuanbo Guo and Jun Xia and Yiyu Shi", "abstract": "As deep learning (DL) techniques become integral to various applications, ensuring model fairness while maintaining high performance has become increasingly critical, particularly in sensitive fields such as medical diagnosis. Although a variety of bias-mitigation methods have been proposed, many rely on computationally expensive debiasing strategies or suffer substantial drops in model accuracy, which limits their practicality in real-world, resource-constrained settings. To address this issue, we propose a fairness-oriented low rank factorization (LRF) framework that leverages singular value decomposition (SVD) to improve DL model fairness. Unlike traditional SVD, which is mainly used for model compression by decomposing and reducing weight matrices, our work shows that SVD can also serve as an effective tool for fairness enhancement. Specifically, we observed that elements in the unitary matrices obtained from SVD contribute unequally to model bias across groups defined by sensitive attributes. Motivated by this observation, we propose a method, named FairLRF, that selectively removes bias-inducing elements from unitary matrices to reduce group disparities, thus enhancing model fairness. Extensive experiments show that our method outperforms conventional LRF methods as well as state-of-the-art fairness-enhancing techniques. Additionally, an ablation study examines how major hyper-parameters may influence the performance of processed models. To the best of our knowledge, this is the first work utilizing SVD not primarily for compression but for fairness enhancement.", "link": "http://arxiv.org/abs/2511.16549v1", "date": "2025-11-20", "relevancy": 1.4187, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4944}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4701}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairLRF%3A%20Achieving%20Fairness%20through%20Sparse%20Low%20Rank%20Factorization&body=Title%3A%20FairLRF%3A%20Achieving%20Fairness%20through%20Sparse%20Low%20Rank%20Factorization%0AAuthor%3A%20Yuanbo%20Guo%20and%20Jun%20Xia%20and%20Yiyu%20Shi%0AAbstract%3A%20As%20deep%20learning%20%28DL%29%20techniques%20become%20integral%20to%20various%20applications%2C%20ensuring%20model%20fairness%20while%20maintaining%20high%20performance%20has%20become%20increasingly%20critical%2C%20particularly%20in%20sensitive%20fields%20such%20as%20medical%20diagnosis.%20Although%20a%20variety%20of%20bias-mitigation%20methods%20have%20been%20proposed%2C%20many%20rely%20on%20computationally%20expensive%20debiasing%20strategies%20or%20suffer%20substantial%20drops%20in%20model%20accuracy%2C%20which%20limits%20their%20practicality%20in%20real-world%2C%20resource-constrained%20settings.%20To%20address%20this%20issue%2C%20we%20propose%20a%20fairness-oriented%20low%20rank%20factorization%20%28LRF%29%20framework%20that%20leverages%20singular%20value%20decomposition%20%28SVD%29%20to%20improve%20DL%20model%20fairness.%20Unlike%20traditional%20SVD%2C%20which%20is%20mainly%20used%20for%20model%20compression%20by%20decomposing%20and%20reducing%20weight%20matrices%2C%20our%20work%20shows%20that%20SVD%20can%20also%20serve%20as%20an%20effective%20tool%20for%20fairness%20enhancement.%20Specifically%2C%20we%20observed%20that%20elements%20in%20the%20unitary%20matrices%20obtained%20from%20SVD%20contribute%20unequally%20to%20model%20bias%20across%20groups%20defined%20by%20sensitive%20attributes.%20Motivated%20by%20this%20observation%2C%20we%20propose%20a%20method%2C%20named%20FairLRF%2C%20that%20selectively%20removes%20bias-inducing%20elements%20from%20unitary%20matrices%20to%20reduce%20group%20disparities%2C%20thus%20enhancing%20model%20fairness.%20Extensive%20experiments%20show%20that%20our%20method%20outperforms%20conventional%20LRF%20methods%20as%20well%20as%20state-of-the-art%20fairness-enhancing%20techniques.%20Additionally%2C%20an%20ablation%20study%20examines%20how%20major%20hyper-parameters%20may%20influence%20the%20performance%20of%20processed%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20utilizing%20SVD%20not%20primarily%20for%20compression%20but%20for%20fairness%20enhancement.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairLRF%253A%2520Achieving%2520Fairness%2520through%2520Sparse%2520Low%2520Rank%2520Factorization%26entry.906535625%3DYuanbo%2520Guo%2520and%2520Jun%2520Xia%2520and%2520Yiyu%2520Shi%26entry.1292438233%3DAs%2520deep%2520learning%2520%2528DL%2529%2520techniques%2520become%2520integral%2520to%2520various%2520applications%252C%2520ensuring%2520model%2520fairness%2520while%2520maintaining%2520high%2520performance%2520has%2520become%2520increasingly%2520critical%252C%2520particularly%2520in%2520sensitive%2520fields%2520such%2520as%2520medical%2520diagnosis.%2520Although%2520a%2520variety%2520of%2520bias-mitigation%2520methods%2520have%2520been%2520proposed%252C%2520many%2520rely%2520on%2520computationally%2520expensive%2520debiasing%2520strategies%2520or%2520suffer%2520substantial%2520drops%2520in%2520model%2520accuracy%252C%2520which%2520limits%2520their%2520practicality%2520in%2520real-world%252C%2520resource-constrained%2520settings.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520fairness-oriented%2520low%2520rank%2520factorization%2520%2528LRF%2529%2520framework%2520that%2520leverages%2520singular%2520value%2520decomposition%2520%2528SVD%2529%2520to%2520improve%2520DL%2520model%2520fairness.%2520Unlike%2520traditional%2520SVD%252C%2520which%2520is%2520mainly%2520used%2520for%2520model%2520compression%2520by%2520decomposing%2520and%2520reducing%2520weight%2520matrices%252C%2520our%2520work%2520shows%2520that%2520SVD%2520can%2520also%2520serve%2520as%2520an%2520effective%2520tool%2520for%2520fairness%2520enhancement.%2520Specifically%252C%2520we%2520observed%2520that%2520elements%2520in%2520the%2520unitary%2520matrices%2520obtained%2520from%2520SVD%2520contribute%2520unequally%2520to%2520model%2520bias%2520across%2520groups%2520defined%2520by%2520sensitive%2520attributes.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520a%2520method%252C%2520named%2520FairLRF%252C%2520that%2520selectively%2520removes%2520bias-inducing%2520elements%2520from%2520unitary%2520matrices%2520to%2520reduce%2520group%2520disparities%252C%2520thus%2520enhancing%2520model%2520fairness.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520conventional%2520LRF%2520methods%2520as%2520well%2520as%2520state-of-the-art%2520fairness-enhancing%2520techniques.%2520Additionally%252C%2520an%2520ablation%2520study%2520examines%2520how%2520major%2520hyper-parameters%2520may%2520influence%2520the%2520performance%2520of%2520processed%2520models.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520utilizing%2520SVD%2520not%2520primarily%2520for%2520compression%2520but%2520for%2520fairness%2520enhancement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairLRF%3A%20Achieving%20Fairness%20through%20Sparse%20Low%20Rank%20Factorization&entry.906535625=Yuanbo%20Guo%20and%20Jun%20Xia%20and%20Yiyu%20Shi&entry.1292438233=As%20deep%20learning%20%28DL%29%20techniques%20become%20integral%20to%20various%20applications%2C%20ensuring%20model%20fairness%20while%20maintaining%20high%20performance%20has%20become%20increasingly%20critical%2C%20particularly%20in%20sensitive%20fields%20such%20as%20medical%20diagnosis.%20Although%20a%20variety%20of%20bias-mitigation%20methods%20have%20been%20proposed%2C%20many%20rely%20on%20computationally%20expensive%20debiasing%20strategies%20or%20suffer%20substantial%20drops%20in%20model%20accuracy%2C%20which%20limits%20their%20practicality%20in%20real-world%2C%20resource-constrained%20settings.%20To%20address%20this%20issue%2C%20we%20propose%20a%20fairness-oriented%20low%20rank%20factorization%20%28LRF%29%20framework%20that%20leverages%20singular%20value%20decomposition%20%28SVD%29%20to%20improve%20DL%20model%20fairness.%20Unlike%20traditional%20SVD%2C%20which%20is%20mainly%20used%20for%20model%20compression%20by%20decomposing%20and%20reducing%20weight%20matrices%2C%20our%20work%20shows%20that%20SVD%20can%20also%20serve%20as%20an%20effective%20tool%20for%20fairness%20enhancement.%20Specifically%2C%20we%20observed%20that%20elements%20in%20the%20unitary%20matrices%20obtained%20from%20SVD%20contribute%20unequally%20to%20model%20bias%20across%20groups%20defined%20by%20sensitive%20attributes.%20Motivated%20by%20this%20observation%2C%20we%20propose%20a%20method%2C%20named%20FairLRF%2C%20that%20selectively%20removes%20bias-inducing%20elements%20from%20unitary%20matrices%20to%20reduce%20group%20disparities%2C%20thus%20enhancing%20model%20fairness.%20Extensive%20experiments%20show%20that%20our%20method%20outperforms%20conventional%20LRF%20methods%20as%20well%20as%20state-of-the-art%20fairness-enhancing%20techniques.%20Additionally%2C%20an%20ablation%20study%20examines%20how%20major%20hyper-parameters%20may%20influence%20the%20performance%20of%20processed%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20utilizing%20SVD%20not%20primarily%20for%20compression%20but%20for%20fairness%20enhancement.&entry.1838667208=http%3A//arxiv.org/abs/2511.16549v1&entry.124074799=Read"},
{"title": "gfnx: Fast and Scalable Library for Generative Flow Networks in JAX", "author": "Daniil Tiapkin and Artem Agarkov and Nikita Morozov and Ian Maksimov and Askar Tsyganov and Timofei Gritsaev and Sergey Samsonov", "abstract": "In this paper, we present gfnx, a fast and scalable package for training and evaluating Generative Flow Networks (GFlowNets) written in JAX. gfnx provides an extensive set of environments and metrics for benchmarking, accompanied with single-file implementations of core objectives for training GFlowNets. We include synthetic hypergrids, multiple sequence generation environments with various editing regimes and particular reward designs for molecular generation, phylogenetic tree construction, Bayesian structure learning, and sampling from the Ising model energy. Across different tasks, gfnx achieves significant wall-clock speedups compared to Pytorch-based benchmarks (such as torchgfn library) and author implementations. For example, gfnx achieves up to 55 times speedup on CPU-based sequence generation environments, and up to 80 times speedup with the GPU-based Bayesian network structure learning setup. Our package provides a diverse set of benchmarks and aims to standardize empirical evaluation and accelerate research and applications of GFlowNets. The library is available on GitHub (https://github.com/d-tiapkin/gfnx) and on pypi (https://pypi.org/project/gfnx/). Documentation is available on https://gfnx.readthedocs.io.", "link": "http://arxiv.org/abs/2511.16592v1", "date": "2025-11-20", "relevancy": 1.9971, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5303}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4976}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20gfnx%3A%20Fast%20and%20Scalable%20Library%20for%20Generative%20Flow%20Networks%20in%20JAX&body=Title%3A%20gfnx%3A%20Fast%20and%20Scalable%20Library%20for%20Generative%20Flow%20Networks%20in%20JAX%0AAuthor%3A%20Daniil%20Tiapkin%20and%20Artem%20Agarkov%20and%20Nikita%20Morozov%20and%20Ian%20Maksimov%20and%20Askar%20Tsyganov%20and%20Timofei%20Gritsaev%20and%20Sergey%20Samsonov%0AAbstract%3A%20In%20this%20paper%2C%20we%20present%20gfnx%2C%20a%20fast%20and%20scalable%20package%20for%20training%20and%20evaluating%20Generative%20Flow%20Networks%20%28GFlowNets%29%20written%20in%20JAX.%20gfnx%20provides%20an%20extensive%20set%20of%20environments%20and%20metrics%20for%20benchmarking%2C%20accompanied%20with%20single-file%20implementations%20of%20core%20objectives%20for%20training%20GFlowNets.%20We%20include%20synthetic%20hypergrids%2C%20multiple%20sequence%20generation%20environments%20with%20various%20editing%20regimes%20and%20particular%20reward%20designs%20for%20molecular%20generation%2C%20phylogenetic%20tree%20construction%2C%20Bayesian%20structure%20learning%2C%20and%20sampling%20from%20the%20Ising%20model%20energy.%20Across%20different%20tasks%2C%20gfnx%20achieves%20significant%20wall-clock%20speedups%20compared%20to%20Pytorch-based%20benchmarks%20%28such%20as%20torchgfn%20library%29%20and%20author%20implementations.%20For%20example%2C%20gfnx%20achieves%20up%20to%2055%20times%20speedup%20on%20CPU-based%20sequence%20generation%20environments%2C%20and%20up%20to%2080%20times%20speedup%20with%20the%20GPU-based%20Bayesian%20network%20structure%20learning%20setup.%20Our%20package%20provides%20a%20diverse%20set%20of%20benchmarks%20and%20aims%20to%20standardize%20empirical%20evaluation%20and%20accelerate%20research%20and%20applications%20of%20GFlowNets.%20The%20library%20is%20available%20on%20GitHub%20%28https%3A//github.com/d-tiapkin/gfnx%29%20and%20on%20pypi%20%28https%3A//pypi.org/project/gfnx/%29.%20Documentation%20is%20available%20on%20https%3A//gfnx.readthedocs.io.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dgfnx%253A%2520Fast%2520and%2520Scalable%2520Library%2520for%2520Generative%2520Flow%2520Networks%2520in%2520JAX%26entry.906535625%3DDaniil%2520Tiapkin%2520and%2520Artem%2520Agarkov%2520and%2520Nikita%2520Morozov%2520and%2520Ian%2520Maksimov%2520and%2520Askar%2520Tsyganov%2520and%2520Timofei%2520Gritsaev%2520and%2520Sergey%2520Samsonov%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520present%2520gfnx%252C%2520a%2520fast%2520and%2520scalable%2520package%2520for%2520training%2520and%2520evaluating%2520Generative%2520Flow%2520Networks%2520%2528GFlowNets%2529%2520written%2520in%2520JAX.%2520gfnx%2520provides%2520an%2520extensive%2520set%2520of%2520environments%2520and%2520metrics%2520for%2520benchmarking%252C%2520accompanied%2520with%2520single-file%2520implementations%2520of%2520core%2520objectives%2520for%2520training%2520GFlowNets.%2520We%2520include%2520synthetic%2520hypergrids%252C%2520multiple%2520sequence%2520generation%2520environments%2520with%2520various%2520editing%2520regimes%2520and%2520particular%2520reward%2520designs%2520for%2520molecular%2520generation%252C%2520phylogenetic%2520tree%2520construction%252C%2520Bayesian%2520structure%2520learning%252C%2520and%2520sampling%2520from%2520the%2520Ising%2520model%2520energy.%2520Across%2520different%2520tasks%252C%2520gfnx%2520achieves%2520significant%2520wall-clock%2520speedups%2520compared%2520to%2520Pytorch-based%2520benchmarks%2520%2528such%2520as%2520torchgfn%2520library%2529%2520and%2520author%2520implementations.%2520For%2520example%252C%2520gfnx%2520achieves%2520up%2520to%252055%2520times%2520speedup%2520on%2520CPU-based%2520sequence%2520generation%2520environments%252C%2520and%2520up%2520to%252080%2520times%2520speedup%2520with%2520the%2520GPU-based%2520Bayesian%2520network%2520structure%2520learning%2520setup.%2520Our%2520package%2520provides%2520a%2520diverse%2520set%2520of%2520benchmarks%2520and%2520aims%2520to%2520standardize%2520empirical%2520evaluation%2520and%2520accelerate%2520research%2520and%2520applications%2520of%2520GFlowNets.%2520The%2520library%2520is%2520available%2520on%2520GitHub%2520%2528https%253A//github.com/d-tiapkin/gfnx%2529%2520and%2520on%2520pypi%2520%2528https%253A//pypi.org/project/gfnx/%2529.%2520Documentation%2520is%2520available%2520on%2520https%253A//gfnx.readthedocs.io.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=gfnx%3A%20Fast%20and%20Scalable%20Library%20for%20Generative%20Flow%20Networks%20in%20JAX&entry.906535625=Daniil%20Tiapkin%20and%20Artem%20Agarkov%20and%20Nikita%20Morozov%20and%20Ian%20Maksimov%20and%20Askar%20Tsyganov%20and%20Timofei%20Gritsaev%20and%20Sergey%20Samsonov&entry.1292438233=In%20this%20paper%2C%20we%20present%20gfnx%2C%20a%20fast%20and%20scalable%20package%20for%20training%20and%20evaluating%20Generative%20Flow%20Networks%20%28GFlowNets%29%20written%20in%20JAX.%20gfnx%20provides%20an%20extensive%20set%20of%20environments%20and%20metrics%20for%20benchmarking%2C%20accompanied%20with%20single-file%20implementations%20of%20core%20objectives%20for%20training%20GFlowNets.%20We%20include%20synthetic%20hypergrids%2C%20multiple%20sequence%20generation%20environments%20with%20various%20editing%20regimes%20and%20particular%20reward%20designs%20for%20molecular%20generation%2C%20phylogenetic%20tree%20construction%2C%20Bayesian%20structure%20learning%2C%20and%20sampling%20from%20the%20Ising%20model%20energy.%20Across%20different%20tasks%2C%20gfnx%20achieves%20significant%20wall-clock%20speedups%20compared%20to%20Pytorch-based%20benchmarks%20%28such%20as%20torchgfn%20library%29%20and%20author%20implementations.%20For%20example%2C%20gfnx%20achieves%20up%20to%2055%20times%20speedup%20on%20CPU-based%20sequence%20generation%20environments%2C%20and%20up%20to%2080%20times%20speedup%20with%20the%20GPU-based%20Bayesian%20network%20structure%20learning%20setup.%20Our%20package%20provides%20a%20diverse%20set%20of%20benchmarks%20and%20aims%20to%20standardize%20empirical%20evaluation%20and%20accelerate%20research%20and%20applications%20of%20GFlowNets.%20The%20library%20is%20available%20on%20GitHub%20%28https%3A//github.com/d-tiapkin/gfnx%29%20and%20on%20pypi%20%28https%3A//pypi.org/project/gfnx/%29.%20Documentation%20is%20available%20on%20https%3A//gfnx.readthedocs.io.&entry.1838667208=http%3A//arxiv.org/abs/2511.16592v1&entry.124074799=Read"},
{"title": "Real-Time Inference for Distributed Multimodal Systems under Communication Delay Uncertainty", "author": "Victor Croisfelt and Jo\u00e3o Henrique Inacio de Souza and Shashi Raj Pandey and Beatriz Soret and Petar Popovski", "abstract": "Connected cyber-physical systems perform inference based on real-time inputs from multiple data streams. Uncertain communication delays across data streams challenge the temporal flow of the inference process. State-of-the-art (SotA) non-blocking inference methods rely on a reference-modality paradigm, requiring one modality input to be fully received before processing, while depending on costly offline profiling. We propose a novel, neuro-inspired non-blocking inference paradigm that primarily employs adaptive temporal windows of integration (TWIs) to dynamically adjust to stochastic delay patterns across heterogeneous streams while relaxing the reference-modality requirement. Our communication-delay-aware framework achieves robust real-time inference with finer-grained control over the accuracy-latency tradeoff. Experiments on the audio-visual event localization (AVEL) task demonstrate superior adaptability to network dynamics compared to SotA approaches.", "link": "http://arxiv.org/abs/2511.16225v1", "date": "2025-11-20", "relevancy": 1.541, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5564}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5368}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Inference%20for%20Distributed%20Multimodal%20Systems%20under%20Communication%20Delay%20Uncertainty&body=Title%3A%20Real-Time%20Inference%20for%20Distributed%20Multimodal%20Systems%20under%20Communication%20Delay%20Uncertainty%0AAuthor%3A%20Victor%20Croisfelt%20and%20Jo%C3%A3o%20Henrique%20Inacio%20de%20Souza%20and%20Shashi%20Raj%20Pandey%20and%20Beatriz%20Soret%20and%20Petar%20Popovski%0AAbstract%3A%20Connected%20cyber-physical%20systems%20perform%20inference%20based%20on%20real-time%20inputs%20from%20multiple%20data%20streams.%20Uncertain%20communication%20delays%20across%20data%20streams%20challenge%20the%20temporal%20flow%20of%20the%20inference%20process.%20State-of-the-art%20%28SotA%29%20non-blocking%20inference%20methods%20rely%20on%20a%20reference-modality%20paradigm%2C%20requiring%20one%20modality%20input%20to%20be%20fully%20received%20before%20processing%2C%20while%20depending%20on%20costly%20offline%20profiling.%20We%20propose%20a%20novel%2C%20neuro-inspired%20non-blocking%20inference%20paradigm%20that%20primarily%20employs%20adaptive%20temporal%20windows%20of%20integration%20%28TWIs%29%20to%20dynamically%20adjust%20to%20stochastic%20delay%20patterns%20across%20heterogeneous%20streams%20while%20relaxing%20the%20reference-modality%20requirement.%20Our%20communication-delay-aware%20framework%20achieves%20robust%20real-time%20inference%20with%20finer-grained%20control%20over%20the%20accuracy-latency%20tradeoff.%20Experiments%20on%20the%20audio-visual%20event%20localization%20%28AVEL%29%20task%20demonstrate%20superior%20adaptability%20to%20network%20dynamics%20compared%20to%20SotA%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Inference%2520for%2520Distributed%2520Multimodal%2520Systems%2520under%2520Communication%2520Delay%2520Uncertainty%26entry.906535625%3DVictor%2520Croisfelt%2520and%2520Jo%25C3%25A3o%2520Henrique%2520Inacio%2520de%2520Souza%2520and%2520Shashi%2520Raj%2520Pandey%2520and%2520Beatriz%2520Soret%2520and%2520Petar%2520Popovski%26entry.1292438233%3DConnected%2520cyber-physical%2520systems%2520perform%2520inference%2520based%2520on%2520real-time%2520inputs%2520from%2520multiple%2520data%2520streams.%2520Uncertain%2520communication%2520delays%2520across%2520data%2520streams%2520challenge%2520the%2520temporal%2520flow%2520of%2520the%2520inference%2520process.%2520State-of-the-art%2520%2528SotA%2529%2520non-blocking%2520inference%2520methods%2520rely%2520on%2520a%2520reference-modality%2520paradigm%252C%2520requiring%2520one%2520modality%2520input%2520to%2520be%2520fully%2520received%2520before%2520processing%252C%2520while%2520depending%2520on%2520costly%2520offline%2520profiling.%2520We%2520propose%2520a%2520novel%252C%2520neuro-inspired%2520non-blocking%2520inference%2520paradigm%2520that%2520primarily%2520employs%2520adaptive%2520temporal%2520windows%2520of%2520integration%2520%2528TWIs%2529%2520to%2520dynamically%2520adjust%2520to%2520stochastic%2520delay%2520patterns%2520across%2520heterogeneous%2520streams%2520while%2520relaxing%2520the%2520reference-modality%2520requirement.%2520Our%2520communication-delay-aware%2520framework%2520achieves%2520robust%2520real-time%2520inference%2520with%2520finer-grained%2520control%2520over%2520the%2520accuracy-latency%2520tradeoff.%2520Experiments%2520on%2520the%2520audio-visual%2520event%2520localization%2520%2528AVEL%2529%2520task%2520demonstrate%2520superior%2520adaptability%2520to%2520network%2520dynamics%2520compared%2520to%2520SotA%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Inference%20for%20Distributed%20Multimodal%20Systems%20under%20Communication%20Delay%20Uncertainty&entry.906535625=Victor%20Croisfelt%20and%20Jo%C3%A3o%20Henrique%20Inacio%20de%20Souza%20and%20Shashi%20Raj%20Pandey%20and%20Beatriz%20Soret%20and%20Petar%20Popovski&entry.1292438233=Connected%20cyber-physical%20systems%20perform%20inference%20based%20on%20real-time%20inputs%20from%20multiple%20data%20streams.%20Uncertain%20communication%20delays%20across%20data%20streams%20challenge%20the%20temporal%20flow%20of%20the%20inference%20process.%20State-of-the-art%20%28SotA%29%20non-blocking%20inference%20methods%20rely%20on%20a%20reference-modality%20paradigm%2C%20requiring%20one%20modality%20input%20to%20be%20fully%20received%20before%20processing%2C%20while%20depending%20on%20costly%20offline%20profiling.%20We%20propose%20a%20novel%2C%20neuro-inspired%20non-blocking%20inference%20paradigm%20that%20primarily%20employs%20adaptive%20temporal%20windows%20of%20integration%20%28TWIs%29%20to%20dynamically%20adjust%20to%20stochastic%20delay%20patterns%20across%20heterogeneous%20streams%20while%20relaxing%20the%20reference-modality%20requirement.%20Our%20communication-delay-aware%20framework%20achieves%20robust%20real-time%20inference%20with%20finer-grained%20control%20over%20the%20accuracy-latency%20tradeoff.%20Experiments%20on%20the%20audio-visual%20event%20localization%20%28AVEL%29%20task%20demonstrate%20superior%20adaptability%20to%20network%20dynamics%20compared%20to%20SotA%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2511.16225v1&entry.124074799=Read"},
{"title": "LLM4EO: Large Language Model for Evolutionary Optimization in Flexible Job Shop Scheduling", "author": "Rongjie Liao and Junhao Qiu and Xin Chen and Xiaoping Li", "abstract": "Customized static operator design has enabled widespread application of Evolutionary Algorithms (EAs), but their search performance is transient during iterations and prone to degradation. Dynamic operators aim to address this but typically rely on predefined designs and localized parameter control during the search process, lacking adaptive optimization throughout evolution. To overcome these limitations, this work leverages Large Language Models (LLMs) to perceive evolutionary dynamics and enable operator-level meta-evolution. The proposed framework, LLMs for Evolutionary Optimization (LLM4EO), comprises three components: knowledge-transfer-based operator design, evolution perception and analysis, and adaptive operator evolution. Firstly, initialization of operators is performed by transferring the strengths of classical operators via LLMs. Then, search preferences and potential limitations of operators are analyzed by integrating fitness performance and evolutionary features, accompanied by corresponding suggestions for improvement. Upon stagnation of population evolution, gene selection priorities of operators are dynamically optimized via improvement prompting strategies. This approach achieves co-evolution of populations and operators in the search, introducing a novel paradigm for enhancing the efficiency and adaptability of EAs. Finally, a series of validations on multiple benchmark datasets of the flexible job shop scheduling problem demonstrate that LLM4EO accelerates population evolution and outperforms both mainstream evolutionary programming and traditional EAs.", "link": "http://arxiv.org/abs/2511.16485v1", "date": "2025-11-20", "relevancy": 1.3883, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.487}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4583}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM4EO%3A%20Large%20Language%20Model%20for%20Evolutionary%20Optimization%20in%20Flexible%20Job%20Shop%20Scheduling&body=Title%3A%20LLM4EO%3A%20Large%20Language%20Model%20for%20Evolutionary%20Optimization%20in%20Flexible%20Job%20Shop%20Scheduling%0AAuthor%3A%20Rongjie%20Liao%20and%20Junhao%20Qiu%20and%20Xin%20Chen%20and%20Xiaoping%20Li%0AAbstract%3A%20Customized%20static%20operator%20design%20has%20enabled%20widespread%20application%20of%20Evolutionary%20Algorithms%20%28EAs%29%2C%20but%20their%20search%20performance%20is%20transient%20during%20iterations%20and%20prone%20to%20degradation.%20Dynamic%20operators%20aim%20to%20address%20this%20but%20typically%20rely%20on%20predefined%20designs%20and%20localized%20parameter%20control%20during%20the%20search%20process%2C%20lacking%20adaptive%20optimization%20throughout%20evolution.%20To%20overcome%20these%20limitations%2C%20this%20work%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20perceive%20evolutionary%20dynamics%20and%20enable%20operator-level%20meta-evolution.%20The%20proposed%20framework%2C%20LLMs%20for%20Evolutionary%20Optimization%20%28LLM4EO%29%2C%20comprises%20three%20components%3A%20knowledge-transfer-based%20operator%20design%2C%20evolution%20perception%20and%20analysis%2C%20and%20adaptive%20operator%20evolution.%20Firstly%2C%20initialization%20of%20operators%20is%20performed%20by%20transferring%20the%20strengths%20of%20classical%20operators%20via%20LLMs.%20Then%2C%20search%20preferences%20and%20potential%20limitations%20of%20operators%20are%20analyzed%20by%20integrating%20fitness%20performance%20and%20evolutionary%20features%2C%20accompanied%20by%20corresponding%20suggestions%20for%20improvement.%20Upon%20stagnation%20of%20population%20evolution%2C%20gene%20selection%20priorities%20of%20operators%20are%20dynamically%20optimized%20via%20improvement%20prompting%20strategies.%20This%20approach%20achieves%20co-evolution%20of%20populations%20and%20operators%20in%20the%20search%2C%20introducing%20a%20novel%20paradigm%20for%20enhancing%20the%20efficiency%20and%20adaptability%20of%20EAs.%20Finally%2C%20a%20series%20of%20validations%20on%20multiple%20benchmark%20datasets%20of%20the%20flexible%20job%20shop%20scheduling%20problem%20demonstrate%20that%20LLM4EO%20accelerates%20population%20evolution%20and%20outperforms%20both%20mainstream%20evolutionary%20programming%20and%20traditional%20EAs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM4EO%253A%2520Large%2520Language%2520Model%2520for%2520Evolutionary%2520Optimization%2520in%2520Flexible%2520Job%2520Shop%2520Scheduling%26entry.906535625%3DRongjie%2520Liao%2520and%2520Junhao%2520Qiu%2520and%2520Xin%2520Chen%2520and%2520Xiaoping%2520Li%26entry.1292438233%3DCustomized%2520static%2520operator%2520design%2520has%2520enabled%2520widespread%2520application%2520of%2520Evolutionary%2520Algorithms%2520%2528EAs%2529%252C%2520but%2520their%2520search%2520performance%2520is%2520transient%2520during%2520iterations%2520and%2520prone%2520to%2520degradation.%2520Dynamic%2520operators%2520aim%2520to%2520address%2520this%2520but%2520typically%2520rely%2520on%2520predefined%2520designs%2520and%2520localized%2520parameter%2520control%2520during%2520the%2520search%2520process%252C%2520lacking%2520adaptive%2520optimization%2520throughout%2520evolution.%2520To%2520overcome%2520these%2520limitations%252C%2520this%2520work%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520perceive%2520evolutionary%2520dynamics%2520and%2520enable%2520operator-level%2520meta-evolution.%2520The%2520proposed%2520framework%252C%2520LLMs%2520for%2520Evolutionary%2520Optimization%2520%2528LLM4EO%2529%252C%2520comprises%2520three%2520components%253A%2520knowledge-transfer-based%2520operator%2520design%252C%2520evolution%2520perception%2520and%2520analysis%252C%2520and%2520adaptive%2520operator%2520evolution.%2520Firstly%252C%2520initialization%2520of%2520operators%2520is%2520performed%2520by%2520transferring%2520the%2520strengths%2520of%2520classical%2520operators%2520via%2520LLMs.%2520Then%252C%2520search%2520preferences%2520and%2520potential%2520limitations%2520of%2520operators%2520are%2520analyzed%2520by%2520integrating%2520fitness%2520performance%2520and%2520evolutionary%2520features%252C%2520accompanied%2520by%2520corresponding%2520suggestions%2520for%2520improvement.%2520Upon%2520stagnation%2520of%2520population%2520evolution%252C%2520gene%2520selection%2520priorities%2520of%2520operators%2520are%2520dynamically%2520optimized%2520via%2520improvement%2520prompting%2520strategies.%2520This%2520approach%2520achieves%2520co-evolution%2520of%2520populations%2520and%2520operators%2520in%2520the%2520search%252C%2520introducing%2520a%2520novel%2520paradigm%2520for%2520enhancing%2520the%2520efficiency%2520and%2520adaptability%2520of%2520EAs.%2520Finally%252C%2520a%2520series%2520of%2520validations%2520on%2520multiple%2520benchmark%2520datasets%2520of%2520the%2520flexible%2520job%2520shop%2520scheduling%2520problem%2520demonstrate%2520that%2520LLM4EO%2520accelerates%2520population%2520evolution%2520and%2520outperforms%2520both%2520mainstream%2520evolutionary%2520programming%2520and%2520traditional%2520EAs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM4EO%3A%20Large%20Language%20Model%20for%20Evolutionary%20Optimization%20in%20Flexible%20Job%20Shop%20Scheduling&entry.906535625=Rongjie%20Liao%20and%20Junhao%20Qiu%20and%20Xin%20Chen%20and%20Xiaoping%20Li&entry.1292438233=Customized%20static%20operator%20design%20has%20enabled%20widespread%20application%20of%20Evolutionary%20Algorithms%20%28EAs%29%2C%20but%20their%20search%20performance%20is%20transient%20during%20iterations%20and%20prone%20to%20degradation.%20Dynamic%20operators%20aim%20to%20address%20this%20but%20typically%20rely%20on%20predefined%20designs%20and%20localized%20parameter%20control%20during%20the%20search%20process%2C%20lacking%20adaptive%20optimization%20throughout%20evolution.%20To%20overcome%20these%20limitations%2C%20this%20work%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20perceive%20evolutionary%20dynamics%20and%20enable%20operator-level%20meta-evolution.%20The%20proposed%20framework%2C%20LLMs%20for%20Evolutionary%20Optimization%20%28LLM4EO%29%2C%20comprises%20three%20components%3A%20knowledge-transfer-based%20operator%20design%2C%20evolution%20perception%20and%20analysis%2C%20and%20adaptive%20operator%20evolution.%20Firstly%2C%20initialization%20of%20operators%20is%20performed%20by%20transferring%20the%20strengths%20of%20classical%20operators%20via%20LLMs.%20Then%2C%20search%20preferences%20and%20potential%20limitations%20of%20operators%20are%20analyzed%20by%20integrating%20fitness%20performance%20and%20evolutionary%20features%2C%20accompanied%20by%20corresponding%20suggestions%20for%20improvement.%20Upon%20stagnation%20of%20population%20evolution%2C%20gene%20selection%20priorities%20of%20operators%20are%20dynamically%20optimized%20via%20improvement%20prompting%20strategies.%20This%20approach%20achieves%20co-evolution%20of%20populations%20and%20operators%20in%20the%20search%2C%20introducing%20a%20novel%20paradigm%20for%20enhancing%20the%20efficiency%20and%20adaptability%20of%20EAs.%20Finally%2C%20a%20series%20of%20validations%20on%20multiple%20benchmark%20datasets%20of%20the%20flexible%20job%20shop%20scheduling%20problem%20demonstrate%20that%20LLM4EO%20accelerates%20population%20evolution%20and%20outperforms%20both%20mainstream%20evolutionary%20programming%20and%20traditional%20EAs.&entry.1838667208=http%3A//arxiv.org/abs/2511.16485v1&entry.124074799=Read"},
{"title": "Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs", "author": "Alina Fastowski and Bardh Prenkaj and Yuxiao Li and Gjergji Kasneci", "abstract": "LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to \"victim\" LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.", "link": "http://arxiv.org/abs/2511.05919v2", "date": "2025-11-20", "relevancy": 1.7363, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4712}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4437}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Injecting%20Falsehoods%3A%20Adversarial%20Man-in-the-Middle%20Attacks%20Undermining%20Factual%20Recall%20in%20LLMs&body=Title%3A%20Injecting%20Falsehoods%3A%20Adversarial%20Man-in-the-Middle%20Attacks%20Undermining%20Factual%20Recall%20in%20LLMs%0AAuthor%3A%20Alina%20Fastowski%20and%20Bardh%20Prenkaj%20and%20Yuxiao%20Li%20and%20Gjergji%20Kasneci%0AAbstract%3A%20LLMs%20are%20now%20an%20integral%20part%20of%20information%20retrieval.%20As%20such%2C%20their%20role%20as%20question%20answering%20chatbots%20raises%20significant%20concerns%20due%20to%20their%20shown%20vulnerability%20to%20adversarial%20man-in-the-middle%20%28MitM%29%20attacks.%20Here%2C%20we%20propose%20the%20first%20principled%20attack%20evaluation%20on%20LLM%20factual%20memory%20under%20prompt%20injection%20via%20Xmera%2C%20our%20novel%2C%20theory-grounded%20MitM%20framework.%20By%20perturbing%20the%20input%20given%20to%20%22victim%22%20LLMs%20in%20three%20closed-book%20and%20fact-based%20QA%20settings%2C%20we%20undermine%20the%20correctness%20of%20the%20responses%20and%20assess%20the%20uncertainty%20of%20their%20generation%20process.%20Surprisingly%2C%20trivial%20instruction-based%20attacks%20report%20the%20highest%20success%20rate%20%28up%20to%20~85.3%25%29%20while%20simultaneously%20having%20a%20high%20uncertainty%20for%20incorrectly%20answered%20questions.%20To%20provide%20a%20simple%20defense%20mechanism%20against%20Xmera%2C%20we%20train%20Random%20Forest%20classifiers%20on%20the%20response%20uncertainty%20levels%20to%20distinguish%20between%20attacked%20and%20unattacked%20queries%20%28average%20AUC%20of%20up%20to%20~96%25%29.%20We%20believe%20that%20signaling%20users%20to%20be%20cautious%20about%20the%20answers%20they%20receive%20from%20black-box%20and%20potentially%20corrupt%20LLMs%20is%20a%20first%20checkpoint%20toward%20user%20cyberspace%20safety.%0ALink%3A%20http%3A//arxiv.org/abs/2511.05919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInjecting%2520Falsehoods%253A%2520Adversarial%2520Man-in-the-Middle%2520Attacks%2520Undermining%2520Factual%2520Recall%2520in%2520LLMs%26entry.906535625%3DAlina%2520Fastowski%2520and%2520Bardh%2520Prenkaj%2520and%2520Yuxiao%2520Li%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3DLLMs%2520are%2520now%2520an%2520integral%2520part%2520of%2520information%2520retrieval.%2520As%2520such%252C%2520their%2520role%2520as%2520question%2520answering%2520chatbots%2520raises%2520significant%2520concerns%2520due%2520to%2520their%2520shown%2520vulnerability%2520to%2520adversarial%2520man-in-the-middle%2520%2528MitM%2529%2520attacks.%2520Here%252C%2520we%2520propose%2520the%2520first%2520principled%2520attack%2520evaluation%2520on%2520LLM%2520factual%2520memory%2520under%2520prompt%2520injection%2520via%2520Xmera%252C%2520our%2520novel%252C%2520theory-grounded%2520MitM%2520framework.%2520By%2520perturbing%2520the%2520input%2520given%2520to%2520%2522victim%2522%2520LLMs%2520in%2520three%2520closed-book%2520and%2520fact-based%2520QA%2520settings%252C%2520we%2520undermine%2520the%2520correctness%2520of%2520the%2520responses%2520and%2520assess%2520the%2520uncertainty%2520of%2520their%2520generation%2520process.%2520Surprisingly%252C%2520trivial%2520instruction-based%2520attacks%2520report%2520the%2520highest%2520success%2520rate%2520%2528up%2520to%2520~85.3%2525%2529%2520while%2520simultaneously%2520having%2520a%2520high%2520uncertainty%2520for%2520incorrectly%2520answered%2520questions.%2520To%2520provide%2520a%2520simple%2520defense%2520mechanism%2520against%2520Xmera%252C%2520we%2520train%2520Random%2520Forest%2520classifiers%2520on%2520the%2520response%2520uncertainty%2520levels%2520to%2520distinguish%2520between%2520attacked%2520and%2520unattacked%2520queries%2520%2528average%2520AUC%2520of%2520up%2520to%2520~96%2525%2529.%2520We%2520believe%2520that%2520signaling%2520users%2520to%2520be%2520cautious%2520about%2520the%2520answers%2520they%2520receive%2520from%2520black-box%2520and%2520potentially%2520corrupt%2520LLMs%2520is%2520a%2520first%2520checkpoint%2520toward%2520user%2520cyberspace%2520safety.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Injecting%20Falsehoods%3A%20Adversarial%20Man-in-the-Middle%20Attacks%20Undermining%20Factual%20Recall%20in%20LLMs&entry.906535625=Alina%20Fastowski%20and%20Bardh%20Prenkaj%20and%20Yuxiao%20Li%20and%20Gjergji%20Kasneci&entry.1292438233=LLMs%20are%20now%20an%20integral%20part%20of%20information%20retrieval.%20As%20such%2C%20their%20role%20as%20question%20answering%20chatbots%20raises%20significant%20concerns%20due%20to%20their%20shown%20vulnerability%20to%20adversarial%20man-in-the-middle%20%28MitM%29%20attacks.%20Here%2C%20we%20propose%20the%20first%20principled%20attack%20evaluation%20on%20LLM%20factual%20memory%20under%20prompt%20injection%20via%20Xmera%2C%20our%20novel%2C%20theory-grounded%20MitM%20framework.%20By%20perturbing%20the%20input%20given%20to%20%22victim%22%20LLMs%20in%20three%20closed-book%20and%20fact-based%20QA%20settings%2C%20we%20undermine%20the%20correctness%20of%20the%20responses%20and%20assess%20the%20uncertainty%20of%20their%20generation%20process.%20Surprisingly%2C%20trivial%20instruction-based%20attacks%20report%20the%20highest%20success%20rate%20%28up%20to%20~85.3%25%29%20while%20simultaneously%20having%20a%20high%20uncertainty%20for%20incorrectly%20answered%20questions.%20To%20provide%20a%20simple%20defense%20mechanism%20against%20Xmera%2C%20we%20train%20Random%20Forest%20classifiers%20on%20the%20response%20uncertainty%20levels%20to%20distinguish%20between%20attacked%20and%20unattacked%20queries%20%28average%20AUC%20of%20up%20to%20~96%25%29.%20We%20believe%20that%20signaling%20users%20to%20be%20cautious%20about%20the%20answers%20they%20receive%20from%20black-box%20and%20potentially%20corrupt%20LLMs%20is%20a%20first%20checkpoint%20toward%20user%20cyberspace%20safety.&entry.1838667208=http%3A//arxiv.org/abs/2511.05919v2&entry.124074799=Read"},
{"title": "From generative AI to the brain: five takeaways", "author": "Claudius Gros", "abstract": "The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research.", "link": "http://arxiv.org/abs/2511.16432v1", "date": "2025-11-20", "relevancy": 1.8872, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5058}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4483}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20generative%20AI%20to%20the%20brain%3A%20five%20takeaways&body=Title%3A%20From%20generative%20AI%20to%20the%20brain%3A%20five%20takeaways%0AAuthor%3A%20Claudius%20Gros%0AAbstract%3A%20The%20big%20strides%20seen%20in%20generative%20AI%20are%20not%20based%20on%20somewhat%20obscure%20algorithms%2C%20but%20due%20to%20clearly%20defined%20generative%20principles.%20The%20resulting%20concrete%20implementations%20have%20proven%20themselves%20in%20large%20numbers%20of%20applications.%20We%20suggest%20that%20it%20is%20imperative%20to%20thoroughly%20investigate%20which%20of%20these%20generative%20principles%20may%20be%20operative%20also%20in%20the%20brain%2C%20and%20hence%20relevant%20for%20cognitive%20neuroscience.%20In%20addition%2C%20ML%20research%20led%20to%20a%20range%20of%20interesting%20characterizations%20of%20neural%20information%20processing%20systems.%20We%20discuss%20five%20examples%2C%20the%20shortcomings%20of%20world%20modelling%2C%20the%20generation%20of%20thought%20processes%2C%20attention%2C%20neural%20scaling%20laws%2C%20and%20quantization%2C%20that%20illustrate%20how%20much%20neuroscience%20could%20potentially%20learn%20from%20ML%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520generative%2520AI%2520to%2520the%2520brain%253A%2520five%2520takeaways%26entry.906535625%3DClaudius%2520Gros%26entry.1292438233%3DThe%2520big%2520strides%2520seen%2520in%2520generative%2520AI%2520are%2520not%2520based%2520on%2520somewhat%2520obscure%2520algorithms%252C%2520but%2520due%2520to%2520clearly%2520defined%2520generative%2520principles.%2520The%2520resulting%2520concrete%2520implementations%2520have%2520proven%2520themselves%2520in%2520large%2520numbers%2520of%2520applications.%2520We%2520suggest%2520that%2520it%2520is%2520imperative%2520to%2520thoroughly%2520investigate%2520which%2520of%2520these%2520generative%2520principles%2520may%2520be%2520operative%2520also%2520in%2520the%2520brain%252C%2520and%2520hence%2520relevant%2520for%2520cognitive%2520neuroscience.%2520In%2520addition%252C%2520ML%2520research%2520led%2520to%2520a%2520range%2520of%2520interesting%2520characterizations%2520of%2520neural%2520information%2520processing%2520systems.%2520We%2520discuss%2520five%2520examples%252C%2520the%2520shortcomings%2520of%2520world%2520modelling%252C%2520the%2520generation%2520of%2520thought%2520processes%252C%2520attention%252C%2520neural%2520scaling%2520laws%252C%2520and%2520quantization%252C%2520that%2520illustrate%2520how%2520much%2520neuroscience%2520could%2520potentially%2520learn%2520from%2520ML%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20generative%20AI%20to%20the%20brain%3A%20five%20takeaways&entry.906535625=Claudius%20Gros&entry.1292438233=The%20big%20strides%20seen%20in%20generative%20AI%20are%20not%20based%20on%20somewhat%20obscure%20algorithms%2C%20but%20due%20to%20clearly%20defined%20generative%20principles.%20The%20resulting%20concrete%20implementations%20have%20proven%20themselves%20in%20large%20numbers%20of%20applications.%20We%20suggest%20that%20it%20is%20imperative%20to%20thoroughly%20investigate%20which%20of%20these%20generative%20principles%20may%20be%20operative%20also%20in%20the%20brain%2C%20and%20hence%20relevant%20for%20cognitive%20neuroscience.%20In%20addition%2C%20ML%20research%20led%20to%20a%20range%20of%20interesting%20characterizations%20of%20neural%20information%20processing%20systems.%20We%20discuss%20five%20examples%2C%20the%20shortcomings%20of%20world%20modelling%2C%20the%20generation%20of%20thought%20processes%2C%20attention%2C%20neural%20scaling%20laws%2C%20and%20quantization%2C%20that%20illustrate%20how%20much%20neuroscience%20could%20potentially%20learn%20from%20ML%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.16432v1&entry.124074799=Read"},
{"title": "TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models", "author": "Li Zhang and Zhongxuan Han and XiaoHua Feng and Jiaming Zhang and Yuyuan Li and Linbo Jiang and Jianan Lin and Chaochao Chen", "abstract": "Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.", "link": "http://arxiv.org/abs/2511.16423v1", "date": "2025-11-20", "relevancy": 2.0974, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5551}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5514}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TOFA%3A%20Training-Free%20One-Shot%20Federated%20Adaptation%20for%20Vision-Language%20Models&body=Title%3A%20TOFA%3A%20Training-Free%20One-Shot%20Federated%20Adaptation%20for%20Vision-Language%20Models%0AAuthor%3A%20Li%20Zhang%20and%20Zhongxuan%20Han%20and%20XiaoHua%20Feng%20and%20Jiaming%20Zhang%20and%20Yuyuan%20Li%20and%20Linbo%20Jiang%20and%20Jianan%20Lin%20and%20Chaochao%20Chen%0AAbstract%3A%20Efficient%20and%20lightweight%20adaptation%20of%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20to%20downstream%20tasks%20through%20collaborative%20interactions%20between%20local%20clients%20and%20a%20central%20server%20is%20a%20rapidly%20emerging%20research%20topic%20in%20federated%20learning.%20Existing%20adaptation%20algorithms%20are%20typically%20trained%20iteratively%2C%20which%20incur%20significant%20communication%20costs%20and%20increase%20the%20susceptibility%20to%20potential%20attacks.%20Motivated%20by%20the%20one-shot%20federated%20training%20techniques%20that%20reduce%20client-server%20exchanges%20to%20a%20single%20round%2C%20developing%20a%20lightweight%20one-shot%20federated%20VLM%20adaptation%20method%20to%20alleviate%20these%20issues%20is%20particularly%20attractive.%20However%2C%20current%20one-shot%20approaches%20face%20certain%20challenges%20in%20adapting%20VLMs%20within%20federated%20settings%3A%20%281%29%20insufficient%20exploitation%20of%20the%20rich%20multimodal%20information%20inherent%20in%20VLMs%3B%20%282%29%20lack%20of%20specialized%20adaptation%20strategies%20to%20systematically%20handle%20the%20severe%20data%20heterogeneity%3B%20and%20%283%29%20requiring%20additional%20training%20resource%20of%20clients%20or%20server.%20To%20bridge%20these%20gaps%2C%20we%20propose%20a%20novel%20Training-free%20One-shot%20Federated%20Adaptation%20framework%20for%20VLMs%2C%20named%20TOFA.%20To%20fully%20leverage%20the%20generalizable%20multimodal%20features%20in%20pre-trained%20VLMs%2C%20TOFA%20employs%20both%20visual%20and%20textual%20pipelines%20to%20extract%20task-relevant%20representations.%20In%20the%20visual%20pipeline%2C%20a%20hierarchical%20Bayesian%20model%20learns%20personalized%2C%20class-specific%20prototype%20distributions.%20For%20the%20textual%20pipeline%2C%20TOFA%20evaluates%20and%20globally%20aligns%20the%20generated%20local%20text%20prompts%20for%20robustness.%20An%20adaptive%20weight%20calibration%20mechanism%20is%20also%20introduced%20to%20combine%20predictions%20from%20both%20modalities%2C%20balancing%20personalization%20and%20robustness%20to%20handle%20data%20heterogeneity.%20Our%20method%20is%20training-free%2C%20not%20relying%20on%20additional%20training%20resources%20on%20either%20the%20client%20or%20server%20side.%20Extensive%20experiments%20across%209%20datasets%20in%20various%20federated%20settings%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20TOFA%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTOFA%253A%2520Training-Free%2520One-Shot%2520Federated%2520Adaptation%2520for%2520Vision-Language%2520Models%26entry.906535625%3DLi%2520Zhang%2520and%2520Zhongxuan%2520Han%2520and%2520XiaoHua%2520Feng%2520and%2520Jiaming%2520Zhang%2520and%2520Yuyuan%2520Li%2520and%2520Linbo%2520Jiang%2520and%2520Jianan%2520Lin%2520and%2520Chaochao%2520Chen%26entry.1292438233%3DEfficient%2520and%2520lightweight%2520adaptation%2520of%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520downstream%2520tasks%2520through%2520collaborative%2520interactions%2520between%2520local%2520clients%2520and%2520a%2520central%2520server%2520is%2520a%2520rapidly%2520emerging%2520research%2520topic%2520in%2520federated%2520learning.%2520Existing%2520adaptation%2520algorithms%2520are%2520typically%2520trained%2520iteratively%252C%2520which%2520incur%2520significant%2520communication%2520costs%2520and%2520increase%2520the%2520susceptibility%2520to%2520potential%2520attacks.%2520Motivated%2520by%2520the%2520one-shot%2520federated%2520training%2520techniques%2520that%2520reduce%2520client-server%2520exchanges%2520to%2520a%2520single%2520round%252C%2520developing%2520a%2520lightweight%2520one-shot%2520federated%2520VLM%2520adaptation%2520method%2520to%2520alleviate%2520these%2520issues%2520is%2520particularly%2520attractive.%2520However%252C%2520current%2520one-shot%2520approaches%2520face%2520certain%2520challenges%2520in%2520adapting%2520VLMs%2520within%2520federated%2520settings%253A%2520%25281%2529%2520insufficient%2520exploitation%2520of%2520the%2520rich%2520multimodal%2520information%2520inherent%2520in%2520VLMs%253B%2520%25282%2529%2520lack%2520of%2520specialized%2520adaptation%2520strategies%2520to%2520systematically%2520handle%2520the%2520severe%2520data%2520heterogeneity%253B%2520and%2520%25283%2529%2520requiring%2520additional%2520training%2520resource%2520of%2520clients%2520or%2520server.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520propose%2520a%2520novel%2520Training-free%2520One-shot%2520Federated%2520Adaptation%2520framework%2520for%2520VLMs%252C%2520named%2520TOFA.%2520To%2520fully%2520leverage%2520the%2520generalizable%2520multimodal%2520features%2520in%2520pre-trained%2520VLMs%252C%2520TOFA%2520employs%2520both%2520visual%2520and%2520textual%2520pipelines%2520to%2520extract%2520task-relevant%2520representations.%2520In%2520the%2520visual%2520pipeline%252C%2520a%2520hierarchical%2520Bayesian%2520model%2520learns%2520personalized%252C%2520class-specific%2520prototype%2520distributions.%2520For%2520the%2520textual%2520pipeline%252C%2520TOFA%2520evaluates%2520and%2520globally%2520aligns%2520the%2520generated%2520local%2520text%2520prompts%2520for%2520robustness.%2520An%2520adaptive%2520weight%2520calibration%2520mechanism%2520is%2520also%2520introduced%2520to%2520combine%2520predictions%2520from%2520both%2520modalities%252C%2520balancing%2520personalization%2520and%2520robustness%2520to%2520handle%2520data%2520heterogeneity.%2520Our%2520method%2520is%2520training-free%252C%2520not%2520relying%2520on%2520additional%2520training%2520resources%2520on%2520either%2520the%2520client%2520or%2520server%2520side.%2520Extensive%2520experiments%2520across%25209%2520datasets%2520in%2520various%2520federated%2520settings%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520TOFA%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TOFA%3A%20Training-Free%20One-Shot%20Federated%20Adaptation%20for%20Vision-Language%20Models&entry.906535625=Li%20Zhang%20and%20Zhongxuan%20Han%20and%20XiaoHua%20Feng%20and%20Jiaming%20Zhang%20and%20Yuyuan%20Li%20and%20Linbo%20Jiang%20and%20Jianan%20Lin%20and%20Chaochao%20Chen&entry.1292438233=Efficient%20and%20lightweight%20adaptation%20of%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20to%20downstream%20tasks%20through%20collaborative%20interactions%20between%20local%20clients%20and%20a%20central%20server%20is%20a%20rapidly%20emerging%20research%20topic%20in%20federated%20learning.%20Existing%20adaptation%20algorithms%20are%20typically%20trained%20iteratively%2C%20which%20incur%20significant%20communication%20costs%20and%20increase%20the%20susceptibility%20to%20potential%20attacks.%20Motivated%20by%20the%20one-shot%20federated%20training%20techniques%20that%20reduce%20client-server%20exchanges%20to%20a%20single%20round%2C%20developing%20a%20lightweight%20one-shot%20federated%20VLM%20adaptation%20method%20to%20alleviate%20these%20issues%20is%20particularly%20attractive.%20However%2C%20current%20one-shot%20approaches%20face%20certain%20challenges%20in%20adapting%20VLMs%20within%20federated%20settings%3A%20%281%29%20insufficient%20exploitation%20of%20the%20rich%20multimodal%20information%20inherent%20in%20VLMs%3B%20%282%29%20lack%20of%20specialized%20adaptation%20strategies%20to%20systematically%20handle%20the%20severe%20data%20heterogeneity%3B%20and%20%283%29%20requiring%20additional%20training%20resource%20of%20clients%20or%20server.%20To%20bridge%20these%20gaps%2C%20we%20propose%20a%20novel%20Training-free%20One-shot%20Federated%20Adaptation%20framework%20for%20VLMs%2C%20named%20TOFA.%20To%20fully%20leverage%20the%20generalizable%20multimodal%20features%20in%20pre-trained%20VLMs%2C%20TOFA%20employs%20both%20visual%20and%20textual%20pipelines%20to%20extract%20task-relevant%20representations.%20In%20the%20visual%20pipeline%2C%20a%20hierarchical%20Bayesian%20model%20learns%20personalized%2C%20class-specific%20prototype%20distributions.%20For%20the%20textual%20pipeline%2C%20TOFA%20evaluates%20and%20globally%20aligns%20the%20generated%20local%20text%20prompts%20for%20robustness.%20An%20adaptive%20weight%20calibration%20mechanism%20is%20also%20introduced%20to%20combine%20predictions%20from%20both%20modalities%2C%20balancing%20personalization%20and%20robustness%20to%20handle%20data%20heterogeneity.%20Our%20method%20is%20training-free%2C%20not%20relying%20on%20additional%20training%20resources%20on%20either%20the%20client%20or%20server%20side.%20Extensive%20experiments%20across%209%20datasets%20in%20various%20federated%20settings%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20TOFA%20method.&entry.1838667208=http%3A//arxiv.org/abs/2511.16423v1&entry.124074799=Read"},
{"title": "Fast-DataShapley: Neural Modeling for Training Data Valuation", "author": "Haifeng Sun and Yu Xiong and Runze Wu and Xinyu Cai and Changjie Fan and Lan Zhang and Xiang-Yang Li", "abstract": "The value and copyright of training data are crucial in the artificial intelligence industry. Service platforms should protect data providers' legitimate rights and fairly reward them for their contributions. Shapley value, a potent tool for evaluating contributions, outperforms other methods in theory, but its computational overhead escalates exponentially with the number of data providers. Recent works based on Shapley values attempt to mitigate computation complexity by approximation algorithms. However, they need to retrain for each test sample, leading to intolerable costs. We propose Fast-DataShapley, a one-pass training method that leverages the weighted least squares characterization of the Shapley value to train a reusable explainer model with real-time reasoning speed. Given new test samples, no retraining is required to calculate the Shapley values of the training data. Additionally, we propose three methods with theoretical guarantees to reduce training overhead from two aspects: the approximate calculation of the utility function and the group calculation of the training data. We analyze time complexity to show the efficiency of our methods. The experimental evaluations on various image datasets demonstrate superior performance and efficiency compared to baselines. Specifically, the performance is improved to more than 2 times, and the explainer's training speed can be increased by two orders of magnitude.", "link": "http://arxiv.org/abs/2506.05281v3", "date": "2025-11-20", "relevancy": 1.4862, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5087}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5062}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast-DataShapley%3A%20Neural%20Modeling%20for%20Training%20Data%20Valuation&body=Title%3A%20Fast-DataShapley%3A%20Neural%20Modeling%20for%20Training%20Data%20Valuation%0AAuthor%3A%20Haifeng%20Sun%20and%20Yu%20Xiong%20and%20Runze%20Wu%20and%20Xinyu%20Cai%20and%20Changjie%20Fan%20and%20Lan%20Zhang%20and%20Xiang-Yang%20Li%0AAbstract%3A%20The%20value%20and%20copyright%20of%20training%20data%20are%20crucial%20in%20the%20artificial%20intelligence%20industry.%20Service%20platforms%20should%20protect%20data%20providers%27%20legitimate%20rights%20and%20fairly%20reward%20them%20for%20their%20contributions.%20Shapley%20value%2C%20a%20potent%20tool%20for%20evaluating%20contributions%2C%20outperforms%20other%20methods%20in%20theory%2C%20but%20its%20computational%20overhead%20escalates%20exponentially%20with%20the%20number%20of%20data%20providers.%20Recent%20works%20based%20on%20Shapley%20values%20attempt%20to%20mitigate%20computation%20complexity%20by%20approximation%20algorithms.%20However%2C%20they%20need%20to%20retrain%20for%20each%20test%20sample%2C%20leading%20to%20intolerable%20costs.%20We%20propose%20Fast-DataShapley%2C%20a%20one-pass%20training%20method%20that%20leverages%20the%20weighted%20least%20squares%20characterization%20of%20the%20Shapley%20value%20to%20train%20a%20reusable%20explainer%20model%20with%20real-time%20reasoning%20speed.%20Given%20new%20test%20samples%2C%20no%20retraining%20is%20required%20to%20calculate%20the%20Shapley%20values%20of%20the%20training%20data.%20Additionally%2C%20we%20propose%20three%20methods%20with%20theoretical%20guarantees%20to%20reduce%20training%20overhead%20from%20two%20aspects%3A%20the%20approximate%20calculation%20of%20the%20utility%20function%20and%20the%20group%20calculation%20of%20the%20training%20data.%20We%20analyze%20time%20complexity%20to%20show%20the%20efficiency%20of%20our%20methods.%20The%20experimental%20evaluations%20on%20various%20image%20datasets%20demonstrate%20superior%20performance%20and%20efficiency%20compared%20to%20baselines.%20Specifically%2C%20the%20performance%20is%20improved%20to%20more%20than%202%20times%2C%20and%20the%20explainer%27s%20training%20speed%20can%20be%20increased%20by%20two%20orders%20of%20magnitude.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05281v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast-DataShapley%253A%2520Neural%2520Modeling%2520for%2520Training%2520Data%2520Valuation%26entry.906535625%3DHaifeng%2520Sun%2520and%2520Yu%2520Xiong%2520and%2520Runze%2520Wu%2520and%2520Xinyu%2520Cai%2520and%2520Changjie%2520Fan%2520and%2520Lan%2520Zhang%2520and%2520Xiang-Yang%2520Li%26entry.1292438233%3DThe%2520value%2520and%2520copyright%2520of%2520training%2520data%2520are%2520crucial%2520in%2520the%2520artificial%2520intelligence%2520industry.%2520Service%2520platforms%2520should%2520protect%2520data%2520providers%2527%2520legitimate%2520rights%2520and%2520fairly%2520reward%2520them%2520for%2520their%2520contributions.%2520Shapley%2520value%252C%2520a%2520potent%2520tool%2520for%2520evaluating%2520contributions%252C%2520outperforms%2520other%2520methods%2520in%2520theory%252C%2520but%2520its%2520computational%2520overhead%2520escalates%2520exponentially%2520with%2520the%2520number%2520of%2520data%2520providers.%2520Recent%2520works%2520based%2520on%2520Shapley%2520values%2520attempt%2520to%2520mitigate%2520computation%2520complexity%2520by%2520approximation%2520algorithms.%2520However%252C%2520they%2520need%2520to%2520retrain%2520for%2520each%2520test%2520sample%252C%2520leading%2520to%2520intolerable%2520costs.%2520We%2520propose%2520Fast-DataShapley%252C%2520a%2520one-pass%2520training%2520method%2520that%2520leverages%2520the%2520weighted%2520least%2520squares%2520characterization%2520of%2520the%2520Shapley%2520value%2520to%2520train%2520a%2520reusable%2520explainer%2520model%2520with%2520real-time%2520reasoning%2520speed.%2520Given%2520new%2520test%2520samples%252C%2520no%2520retraining%2520is%2520required%2520to%2520calculate%2520the%2520Shapley%2520values%2520of%2520the%2520training%2520data.%2520Additionally%252C%2520we%2520propose%2520three%2520methods%2520with%2520theoretical%2520guarantees%2520to%2520reduce%2520training%2520overhead%2520from%2520two%2520aspects%253A%2520the%2520approximate%2520calculation%2520of%2520the%2520utility%2520function%2520and%2520the%2520group%2520calculation%2520of%2520the%2520training%2520data.%2520We%2520analyze%2520time%2520complexity%2520to%2520show%2520the%2520efficiency%2520of%2520our%2520methods.%2520The%2520experimental%2520evaluations%2520on%2520various%2520image%2520datasets%2520demonstrate%2520superior%2520performance%2520and%2520efficiency%2520compared%2520to%2520baselines.%2520Specifically%252C%2520the%2520performance%2520is%2520improved%2520to%2520more%2520than%25202%2520times%252C%2520and%2520the%2520explainer%2527s%2520training%2520speed%2520can%2520be%2520increased%2520by%2520two%2520orders%2520of%2520magnitude.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05281v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast-DataShapley%3A%20Neural%20Modeling%20for%20Training%20Data%20Valuation&entry.906535625=Haifeng%20Sun%20and%20Yu%20Xiong%20and%20Runze%20Wu%20and%20Xinyu%20Cai%20and%20Changjie%20Fan%20and%20Lan%20Zhang%20and%20Xiang-Yang%20Li&entry.1292438233=The%20value%20and%20copyright%20of%20training%20data%20are%20crucial%20in%20the%20artificial%20intelligence%20industry.%20Service%20platforms%20should%20protect%20data%20providers%27%20legitimate%20rights%20and%20fairly%20reward%20them%20for%20their%20contributions.%20Shapley%20value%2C%20a%20potent%20tool%20for%20evaluating%20contributions%2C%20outperforms%20other%20methods%20in%20theory%2C%20but%20its%20computational%20overhead%20escalates%20exponentially%20with%20the%20number%20of%20data%20providers.%20Recent%20works%20based%20on%20Shapley%20values%20attempt%20to%20mitigate%20computation%20complexity%20by%20approximation%20algorithms.%20However%2C%20they%20need%20to%20retrain%20for%20each%20test%20sample%2C%20leading%20to%20intolerable%20costs.%20We%20propose%20Fast-DataShapley%2C%20a%20one-pass%20training%20method%20that%20leverages%20the%20weighted%20least%20squares%20characterization%20of%20the%20Shapley%20value%20to%20train%20a%20reusable%20explainer%20model%20with%20real-time%20reasoning%20speed.%20Given%20new%20test%20samples%2C%20no%20retraining%20is%20required%20to%20calculate%20the%20Shapley%20values%20of%20the%20training%20data.%20Additionally%2C%20we%20propose%20three%20methods%20with%20theoretical%20guarantees%20to%20reduce%20training%20overhead%20from%20two%20aspects%3A%20the%20approximate%20calculation%20of%20the%20utility%20function%20and%20the%20group%20calculation%20of%20the%20training%20data.%20We%20analyze%20time%20complexity%20to%20show%20the%20efficiency%20of%20our%20methods.%20The%20experimental%20evaluations%20on%20various%20image%20datasets%20demonstrate%20superior%20performance%20and%20efficiency%20compared%20to%20baselines.%20Specifically%2C%20the%20performance%20is%20improved%20to%20more%20than%202%20times%2C%20and%20the%20explainer%27s%20training%20speed%20can%20be%20increased%20by%20two%20orders%20of%20magnitude.&entry.1838667208=http%3A//arxiv.org/abs/2506.05281v3&entry.124074799=Read"},
{"title": "Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense", "author": "Sayak Mukherjee and Samrat Chatterjee and Emilie Purvine and Ted Fujimoto and Tegan Emerson", "abstract": "Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.", "link": "http://arxiv.org/abs/2511.16483v1", "date": "2025-11-20", "relevancy": 1.4458, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4933}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4817}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model-Based%20Reward%20Design%20for%20Deep%20Reinforcement%20Learning-Driven%20Autonomous%20Cyber%20Defense&body=Title%3A%20Large%20Language%20Model-Based%20Reward%20Design%20for%20Deep%20Reinforcement%20Learning-Driven%20Autonomous%20Cyber%20Defense%0AAuthor%3A%20Sayak%20Mukherjee%20and%20Samrat%20Chatterjee%20and%20Emilie%20Purvine%20and%20Ted%20Fujimoto%20and%20Tegan%20Emerson%0AAbstract%3A%20Designing%20rewards%20for%20autonomous%20cyber%20attack%20and%20defense%20learning%20agents%20in%20a%20complex%2C%20dynamic%20environment%20is%20a%20challenging%20task%20for%20subject%20matter%20experts.%20We%20propose%20a%20large%20language%20model%20%28LLM%29-based%20reward%20design%20approach%20to%20generate%20autonomous%20cyber%20defense%20policies%20in%20a%20deep%20reinforcement%20learning%20%28DRL%29-driven%20experimental%20simulation%20environment.%20Multiple%20attack%20and%20defense%20agent%20personas%20were%20crafted%2C%20reflecting%20heterogeneity%20in%20agent%20actions%2C%20to%20generate%20LLM-guided%20reward%20designs%20where%20the%20LLM%20was%20first%20provided%20with%20contextual%20cyber%20simulation%20environment%20information.%20These%20reward%20structures%20were%20then%20utilized%20within%20a%20DRL-driven%20attack-defense%20simulation%20environment%20to%20learn%20an%20ensemble%20of%20cyber%20defense%20policies.%20Our%20results%20suggest%20that%20LLM-guided%20reward%20designs%20can%20lead%20to%20effective%20defense%20strategies%20against%20diverse%20adversarial%20behaviors.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model-Based%2520Reward%2520Design%2520for%2520Deep%2520Reinforcement%2520Learning-Driven%2520Autonomous%2520Cyber%2520Defense%26entry.906535625%3DSayak%2520Mukherjee%2520and%2520Samrat%2520Chatterjee%2520and%2520Emilie%2520Purvine%2520and%2520Ted%2520Fujimoto%2520and%2520Tegan%2520Emerson%26entry.1292438233%3DDesigning%2520rewards%2520for%2520autonomous%2520cyber%2520attack%2520and%2520defense%2520learning%2520agents%2520in%2520a%2520complex%252C%2520dynamic%2520environment%2520is%2520a%2520challenging%2520task%2520for%2520subject%2520matter%2520experts.%2520We%2520propose%2520a%2520large%2520language%2520model%2520%2528LLM%2529-based%2520reward%2520design%2520approach%2520to%2520generate%2520autonomous%2520cyber%2520defense%2520policies%2520in%2520a%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529-driven%2520experimental%2520simulation%2520environment.%2520Multiple%2520attack%2520and%2520defense%2520agent%2520personas%2520were%2520crafted%252C%2520reflecting%2520heterogeneity%2520in%2520agent%2520actions%252C%2520to%2520generate%2520LLM-guided%2520reward%2520designs%2520where%2520the%2520LLM%2520was%2520first%2520provided%2520with%2520contextual%2520cyber%2520simulation%2520environment%2520information.%2520These%2520reward%2520structures%2520were%2520then%2520utilized%2520within%2520a%2520DRL-driven%2520attack-defense%2520simulation%2520environment%2520to%2520learn%2520an%2520ensemble%2520of%2520cyber%2520defense%2520policies.%2520Our%2520results%2520suggest%2520that%2520LLM-guided%2520reward%2520designs%2520can%2520lead%2520to%2520effective%2520defense%2520strategies%2520against%2520diverse%2520adversarial%2520behaviors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model-Based%20Reward%20Design%20for%20Deep%20Reinforcement%20Learning-Driven%20Autonomous%20Cyber%20Defense&entry.906535625=Sayak%20Mukherjee%20and%20Samrat%20Chatterjee%20and%20Emilie%20Purvine%20and%20Ted%20Fujimoto%20and%20Tegan%20Emerson&entry.1292438233=Designing%20rewards%20for%20autonomous%20cyber%20attack%20and%20defense%20learning%20agents%20in%20a%20complex%2C%20dynamic%20environment%20is%20a%20challenging%20task%20for%20subject%20matter%20experts.%20We%20propose%20a%20large%20language%20model%20%28LLM%29-based%20reward%20design%20approach%20to%20generate%20autonomous%20cyber%20defense%20policies%20in%20a%20deep%20reinforcement%20learning%20%28DRL%29-driven%20experimental%20simulation%20environment.%20Multiple%20attack%20and%20defense%20agent%20personas%20were%20crafted%2C%20reflecting%20heterogeneity%20in%20agent%20actions%2C%20to%20generate%20LLM-guided%20reward%20designs%20where%20the%20LLM%20was%20first%20provided%20with%20contextual%20cyber%20simulation%20environment%20information.%20These%20reward%20structures%20were%20then%20utilized%20within%20a%20DRL-driven%20attack-defense%20simulation%20environment%20to%20learn%20an%20ensemble%20of%20cyber%20defense%20policies.%20Our%20results%20suggest%20that%20LLM-guided%20reward%20designs%20can%20lead%20to%20effective%20defense%20strategies%20against%20diverse%20adversarial%20behaviors.&entry.1838667208=http%3A//arxiv.org/abs/2511.16483v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


