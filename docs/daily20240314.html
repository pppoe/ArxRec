<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving\n  Representation Learning", "author": "Jialv Zou and Bencheng Liao and Qian Zhang and Wenyu Liu and Xinggang Wang", "abstract": "  Learning robust and scalable visual representations from massive multi-view\nvideo data remains a challenge in computer vision and autonomous driving.\nExisting pre-training methods either rely on expensive supervised learning with\n3D annotations, limiting the scalability, or focus on single-frame or monocular\ninputs, neglecting the temporal information. We propose MIM4D, a novel\npre-training paradigm based on dual masked image modeling (MIM). MIM4D\nleverages both spatial and temporal relations by training on masked multi-view\nvideo inputs. It constructs pseudo-3D features using continuous scene flow and\nprojects them onto 2D plane for supervision. To address the lack of dense 3D\nsupervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable\nrendering to learn geometric representations. We demonstrate that MIM4D\nachieves state-of-the-art performance on the nuScenes dataset for visual\nrepresentation learning in autonomous driving. It significantly improves\nexisting methods on multiple downstream tasks, including BEV segmentation (8.7%\nIoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP). Our\nwork offers a new choice for learning representation at scale in autonomous\ndriving. Code and models are released at https://github.com/hustvl/MIM4D\n", "link": "http://arxiv.org/abs/2403.08760v1", "date": "2024-03-13", "relevancy": 2.9608, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6349}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5726}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.569}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MIM4D%3A%20Masked%20Modeling%20with%20Multi-View%20Video%20for%20Autonomous%20Driving%0A%20%20Representation%20Learning&body=Title%3A%20MIM4D%3A%20Masked%20Modeling%20with%20Multi-View%20Video%20for%20Autonomous%20Driving%0A%20%20Representation%20Learning%0AAuthor%3A%20Jialv%20Zou%20and%20Bencheng%20Liao%20and%20Qian%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Learning%20robust%20and%20scalable%20visual%20representations%20from%20massive%20multi-view%0Avideo%20data%20remains%20a%20challenge%20in%20computer%20vision%20and%20autonomous%20driving.%0AExisting%20pre-training%20methods%20either%20rely%20on%20expensive%20supervised%20learning%20with%0A3D%20annotations%2C%20limiting%20the%20scalability%2C%20or%20focus%20on%20single-frame%20or%20monocular%0Ainputs%2C%20neglecting%20the%20temporal%20information.%20We%20propose%20MIM4D%2C%20a%20novel%0Apre-training%20paradigm%20based%20on%20dual%20masked%20image%20modeling%20%28MIM%29.%20MIM4D%0Aleverages%20both%20spatial%20and%20temporal%20relations%20by%20training%20on%20masked%20multi-view%0Avideo%20inputs.%20It%20constructs%20pseudo-3D%20features%20using%20continuous%20scene%20flow%20and%0Aprojects%20them%20onto%202D%20plane%20for%20supervision.%20To%20address%20the%20lack%20of%20dense%203D%0Asupervision%2C%20MIM4D%20reconstruct%20pixels%20by%20employing%203D%20volumetric%20differentiable%0Arendering%20to%20learn%20geometric%20representations.%20We%20demonstrate%20that%20MIM4D%0Aachieves%20state-of-the-art%20performance%20on%20the%20nuScenes%20dataset%20for%20visual%0Arepresentation%20learning%20in%20autonomous%20driving.%20It%20significantly%20improves%0Aexisting%20methods%20on%20multiple%20downstream%20tasks%2C%20including%20BEV%20segmentation%20%288.7%25%0AIoU%29%2C%203D%20object%20detection%20%283.5%25%20mAP%29%2C%20and%20HD%20map%20construction%20%281.4%25%20mAP%29.%20Our%0Awork%20offers%20a%20new%20choice%20for%20learning%20representation%20at%20scale%20in%20autonomous%0Adriving.%20Code%20and%20models%20are%20released%20at%20https%3A//github.com/hustvl/MIM4D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08760v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIM4D%3A%20Masked%20Modeling%20with%20Multi-View%20Video%20for%20Autonomous%20Driving%0A%20%20Representation%20Learning&entry.906535625=Jialv%20Zou%20and%20Bencheng%20Liao%20and%20Qian%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20Learning%20robust%20and%20scalable%20visual%20representations%20from%20massive%20multi-view%0Avideo%20data%20remains%20a%20challenge%20in%20computer%20vision%20and%20autonomous%20driving.%0AExisting%20pre-training%20methods%20either%20rely%20on%20expensive%20supervised%20learning%20with%0A3D%20annotations%2C%20limiting%20the%20scalability%2C%20or%20focus%20on%20single-frame%20or%20monocular%0Ainputs%2C%20neglecting%20the%20temporal%20information.%20We%20propose%20MIM4D%2C%20a%20novel%0Apre-training%20paradigm%20based%20on%20dual%20masked%20image%20modeling%20%28MIM%29.%20MIM4D%0Aleverages%20both%20spatial%20and%20temporal%20relations%20by%20training%20on%20masked%20multi-view%0Avideo%20inputs.%20It%20constructs%20pseudo-3D%20features%20using%20continuous%20scene%20flow%20and%0Aprojects%20them%20onto%202D%20plane%20for%20supervision.%20To%20address%20the%20lack%20of%20dense%203D%0Asupervision%2C%20MIM4D%20reconstruct%20pixels%20by%20employing%203D%20volumetric%20differentiable%0Arendering%20to%20learn%20geometric%20representations.%20We%20demonstrate%20that%20MIM4D%0Aachieves%20state-of-the-art%20performance%20on%20the%20nuScenes%20dataset%20for%20visual%0Arepresentation%20learning%20in%20autonomous%20driving.%20It%20significantly%20improves%0Aexisting%20methods%20on%20multiple%20downstream%20tasks%2C%20including%20BEV%20segmentation%20%288.7%25%0AIoU%29%2C%203D%20object%20detection%20%283.5%25%20mAP%29%2C%20and%20HD%20map%20construction%20%281.4%25%20mAP%29.%20Our%0Awork%20offers%20a%20new%20choice%20for%20learning%20representation%20at%20scale%20in%20autonomous%0Adriving.%20Code%20and%20models%20are%20released%20at%20https%3A//github.com/hustvl/MIM4D%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08760v1&entry.124074799=Read"},
{"title": "HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map\n  Construction", "author": "Yi Zhou and Hui Zhang and Jiaqian Yu and Yifan Yang and Sangil Jung and Seung-In Park and ByungIn Yoo", "abstract": "  Vectorized High-Definition (HD) map construction requires predictions of the\ncategory and point coordinates of map elements (e.g. road boundary, lane\ndivider, pedestrian crossing, etc.). State-of-the-art methods are mainly based\non point-level representation learning for regressing accurate point\ncoordinates. However, this pipeline has limitations in obtaining element-level\ninformation and handling element-level failures, e.g. erroneous element shape\nor entanglement between elements. To tackle the above issues, we propose a\nsimple yet effective HybrId framework named HIMap to sufficiently learn and\ninteract both point-level and element-level information. Concretely, we\nintroduce a hybrid representation called HIQuery to represent all map elements,\nand propose a point-element interactor to interactively extract and encode the\nhybrid information of elements, e.g. point position and element shape, into the\nHIQuery. Additionally, we present a point-element consistency constraint to\nenhance the consistency between the point-level and element-level information.\nFinally, the output point-element integrated HIQuery can be directly converted\ninto map elements' class, point coordinates, and mask. We conduct extensive\nexperiments and consistently outperform previous methods on both nuScenes and\nArgoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes\ndataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.\n", "link": "http://arxiv.org/abs/2403.08639v1", "date": "2024-03-13", "relevancy": 2.6664, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5346}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5122}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20HIMap%3A%20HybrId%20Representation%20Learning%20for%20End-to-end%20Vectorized%20HD%20Map%0A%20%20Construction&body=Title%3A%20HIMap%3A%20HybrId%20Representation%20Learning%20for%20End-to-end%20Vectorized%20HD%20Map%0A%20%20Construction%0AAuthor%3A%20Yi%20Zhou%20and%20Hui%20Zhang%20and%20Jiaqian%20Yu%20and%20Yifan%20Yang%20and%20Sangil%20Jung%20and%20Seung-In%20Park%20and%20ByungIn%20Yoo%0AAbstract%3A%20%20%20Vectorized%20High-Definition%20%28HD%29%20map%20construction%20requires%20predictions%20of%20the%0Acategory%20and%20point%20coordinates%20of%20map%20elements%20%28e.g.%20road%20boundary%2C%20lane%0Adivider%2C%20pedestrian%20crossing%2C%20etc.%29.%20State-of-the-art%20methods%20are%20mainly%20based%0Aon%20point-level%20representation%20learning%20for%20regressing%20accurate%20point%0Acoordinates.%20However%2C%20this%20pipeline%20has%20limitations%20in%20obtaining%20element-level%0Ainformation%20and%20handling%20element-level%20failures%2C%20e.g.%20erroneous%20element%20shape%0Aor%20entanglement%20between%20elements.%20To%20tackle%20the%20above%20issues%2C%20we%20propose%20a%0Asimple%20yet%20effective%20HybrId%20framework%20named%20HIMap%20to%20sufficiently%20learn%20and%0Ainteract%20both%20point-level%20and%20element-level%20information.%20Concretely%2C%20we%0Aintroduce%20a%20hybrid%20representation%20called%20HIQuery%20to%20represent%20all%20map%20elements%2C%0Aand%20propose%20a%20point-element%20interactor%20to%20interactively%20extract%20and%20encode%20the%0Ahybrid%20information%20of%20elements%2C%20e.g.%20point%20position%20and%20element%20shape%2C%20into%20the%0AHIQuery.%20Additionally%2C%20we%20present%20a%20point-element%20consistency%20constraint%20to%0Aenhance%20the%20consistency%20between%20the%20point-level%20and%20element-level%20information.%0AFinally%2C%20the%20output%20point-element%20integrated%20HIQuery%20can%20be%20directly%20converted%0Ainto%20map%20elements%27%20class%2C%20point%20coordinates%2C%20and%20mask.%20We%20conduct%20extensive%0Aexperiments%20and%20consistently%20outperform%20previous%20methods%20on%20both%20nuScenes%20and%0AArgoverse2%20datasets.%20Notably%2C%20our%20method%20achieves%20%2477.8%24%20mAP%20on%20the%20nuScenes%0Adataset%2C%20remarkably%20superior%20to%20previous%20SOTAs%20by%20%248.3%24%20mAP%20at%20least.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08639v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HIMap%3A%20HybrId%20Representation%20Learning%20for%20End-to-end%20Vectorized%20HD%20Map%0A%20%20Construction&entry.906535625=Yi%20Zhou%20and%20Hui%20Zhang%20and%20Jiaqian%20Yu%20and%20Yifan%20Yang%20and%20Sangil%20Jung%20and%20Seung-In%20Park%20and%20ByungIn%20Yoo&entry.1292438233=%20%20Vectorized%20High-Definition%20%28HD%29%20map%20construction%20requires%20predictions%20of%20the%0Acategory%20and%20point%20coordinates%20of%20map%20elements%20%28e.g.%20road%20boundary%2C%20lane%0Adivider%2C%20pedestrian%20crossing%2C%20etc.%29.%20State-of-the-art%20methods%20are%20mainly%20based%0Aon%20point-level%20representation%20learning%20for%20regressing%20accurate%20point%0Acoordinates.%20However%2C%20this%20pipeline%20has%20limitations%20in%20obtaining%20element-level%0Ainformation%20and%20handling%20element-level%20failures%2C%20e.g.%20erroneous%20element%20shape%0Aor%20entanglement%20between%20elements.%20To%20tackle%20the%20above%20issues%2C%20we%20propose%20a%0Asimple%20yet%20effective%20HybrId%20framework%20named%20HIMap%20to%20sufficiently%20learn%20and%0Ainteract%20both%20point-level%20and%20element-level%20information.%20Concretely%2C%20we%0Aintroduce%20a%20hybrid%20representation%20called%20HIQuery%20to%20represent%20all%20map%20elements%2C%0Aand%20propose%20a%20point-element%20interactor%20to%20interactively%20extract%20and%20encode%20the%0Ahybrid%20information%20of%20elements%2C%20e.g.%20point%20position%20and%20element%20shape%2C%20into%20the%0AHIQuery.%20Additionally%2C%20we%20present%20a%20point-element%20consistency%20constraint%20to%0Aenhance%20the%20consistency%20between%20the%20point-level%20and%20element-level%20information.%0AFinally%2C%20the%20output%20point-element%20integrated%20HIQuery%20can%20be%20directly%20converted%0Ainto%20map%20elements%27%20class%2C%20point%20coordinates%2C%20and%20mask.%20We%20conduct%20extensive%0Aexperiments%20and%20consistently%20outperform%20previous%20methods%20on%20both%20nuScenes%20and%0AArgoverse2%20datasets.%20Notably%2C%20our%20method%20achieves%20%2477.8%24%20mAP%20on%20the%20nuScenes%0Adataset%2C%20remarkably%20superior%20to%20previous%20SOTAs%20by%20%248.3%24%20mAP%20at%20least.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08639v1&entry.124074799=Read"},
{"title": "GenTKG: Generative Forecasting on Temporal Knowledge Graph", "author": "Ruotong Liao and Xu Jia and Yunpu Ma and Yangzhe Li and Volker Tresp", "abstract": "  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.\n", "link": "http://arxiv.org/abs/2310.07793v4", "date": "2024-03-13", "relevancy": 2.6385, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5523}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5323}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4985}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20GenTKG%3A%20Generative%20Forecasting%20on%20Temporal%20Knowledge%20Graph&body=Title%3A%20GenTKG%3A%20Generative%20Forecasting%20on%20Temporal%20Knowledge%20Graph%0AAuthor%3A%20Ruotong%20Liao%20and%20Xu%20Jia%20and%20Yunpu%20Ma%20and%20Yangzhe%20Li%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20The%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20ignited%20interest%0Ain%20the%20temporal%20knowledge%20graph%20%28tKG%29%20domain%2C%20where%20conventional%0Aembedding-based%20and%20rule-based%20methods%20dominate.%20The%20question%20remains%20open%20of%0Awhether%20pre-trained%20LLMs%20can%20understand%20structured%20temporal%20relational%20data%20and%0Areplace%20them%20as%20the%20foundation%20model%20for%20temporal%20relational%20forecasting.%0ATherefore%2C%20we%20bring%20temporal%20knowledge%20forecasting%20into%20the%20generative%20setting.%0AHowever%2C%20challenges%20occur%20in%20the%20huge%20chasms%20between%20complex%20temporal%20graph%0Adata%20structure%20and%20sequential%20natural%20expressions%20LLMs%20can%20handle%2C%20and%20between%0Athe%20enormous%20data%20sizes%20of%20tKGs%20and%20heavy%20computation%20costs%20of%20finetuning%20LLMs.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20retrieval-augmented%20generation%0Aframework%20named%20GenTKG%20combining%20a%20temporal%20logical%20rule-based%20retrieval%0Astrategy%20and%20few-shot%20parameter-efficient%20instruction%20tuning%20to%20solve%20the%20above%0Achallenges%2C%20respectively.%20Extensive%20experiments%20have%20shown%20that%20GenTKG%0Aoutperforms%20conventional%20methods%20of%20temporal%20relational%20forecasting%20with%20low%0Acomputation%20resources%20using%20extremely%20limited%20training%20data%20as%20few%20as%2016%0Asamples.%20GenTKG%20also%20highlights%20remarkable%20cross-domain%20generalizability%20with%0Aoutperforming%20performance%20on%20unseen%20datasets%20without%20re-training%2C%20and%20in-domain%0Ageneralizability%20regardless%20of%20time%20split%20in%20the%20same%20dataset.%20Our%20work%20reveals%0Athe%20huge%20potential%20of%20LLMs%20in%20the%20tKG%20domain%20and%20opens%20a%20new%20frontier%20for%0Agenerative%20forecasting%20on%20tKGs.%20Code%20and%20data%20are%20released%20here%3A%0Ahttps%3A//github.com/mayhugotong/GenTKG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07793v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenTKG%3A%20Generative%20Forecasting%20on%20Temporal%20Knowledge%20Graph&entry.906535625=Ruotong%20Liao%20and%20Xu%20Jia%20and%20Yunpu%20Ma%20and%20Yangzhe%20Li%20and%20Volker%20Tresp&entry.1292438233=%20%20The%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20ignited%20interest%0Ain%20the%20temporal%20knowledge%20graph%20%28tKG%29%20domain%2C%20where%20conventional%0Aembedding-based%20and%20rule-based%20methods%20dominate.%20The%20question%20remains%20open%20of%0Awhether%20pre-trained%20LLMs%20can%20understand%20structured%20temporal%20relational%20data%20and%0Areplace%20them%20as%20the%20foundation%20model%20for%20temporal%20relational%20forecasting.%0ATherefore%2C%20we%20bring%20temporal%20knowledge%20forecasting%20into%20the%20generative%20setting.%0AHowever%2C%20challenges%20occur%20in%20the%20huge%20chasms%20between%20complex%20temporal%20graph%0Adata%20structure%20and%20sequential%20natural%20expressions%20LLMs%20can%20handle%2C%20and%20between%0Athe%20enormous%20data%20sizes%20of%20tKGs%20and%20heavy%20computation%20costs%20of%20finetuning%20LLMs.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20retrieval-augmented%20generation%0Aframework%20named%20GenTKG%20combining%20a%20temporal%20logical%20rule-based%20retrieval%0Astrategy%20and%20few-shot%20parameter-efficient%20instruction%20tuning%20to%20solve%20the%20above%0Achallenges%2C%20respectively.%20Extensive%20experiments%20have%20shown%20that%20GenTKG%0Aoutperforms%20conventional%20methods%20of%20temporal%20relational%20forecasting%20with%20low%0Acomputation%20resources%20using%20extremely%20limited%20training%20data%20as%20few%20as%2016%0Asamples.%20GenTKG%20also%20highlights%20remarkable%20cross-domain%20generalizability%20with%0Aoutperforming%20performance%20on%20unseen%20datasets%20without%20re-training%2C%20and%20in-domain%0Ageneralizability%20regardless%20of%20time%20split%20in%20the%20same%20dataset.%20Our%20work%20reveals%0Athe%20huge%20potential%20of%20LLMs%20in%20the%20tKG%20domain%20and%20opens%20a%20new%20frontier%20for%0Agenerative%20forecasting%20on%20tKGs.%20Code%20and%20data%20are%20released%20here%3A%0Ahttps%3A//github.com/mayhugotong/GenTKG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07793v4&entry.124074799=Read"},
{"title": "Continual Adversarial Defense", "author": "Qian Wang and Yaoyao Liu and Hefei Ling and Yingwei Li and Qihao Liu and Ping Li and Jiazhong Chen and Alan Yuille and Ning Yu", "abstract": "  In response to the rapidly evolving nature of adversarial attacks against\nvisual classifiers on a monthly basis, numerous defenses have been proposed to\ngeneralize against as many known attacks as possible. However, designing a\ndefense method that generalizes to all types of attacks is not realistic\nbecause the environment in which defense systems operate is dynamic and\ncomprises various unique attacks that emerge as time goes on. The defense\nsystem must gather online few-shot defense feedback to promptly enhance itself,\nleveraging efficient memory utilization. Therefore, we propose the first\ncontinual adversarial defense (CAD) framework that adapts to any attacks in a\ndynamic scenario, where various attacks emerge stage by stage. In practice, CAD\nis modeled under four principles: (1) continual adaptation to new attacks\nwithout catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient\nadaptation, and (4) high accuracy on both clean and adversarial images. We\nexplore and integrate cutting-edge continual learning, few-shot learning, and\nensemble learning techniques to qualify the principles. Experiments conducted\non CIFAR-10 and ImageNet-100 validate the effectiveness of our approach against\nmultiple stages of modern adversarial attacks and demonstrate significant\nimprovements over numerous baseline methods. In particular, CAD is capable of\nquickly adapting with minimal feedback and a low cost of defense failure, while\nmaintaining good performance against previous attacks. Our research sheds light\non a brand-new paradigm for continual defense adaptation against dynamic and\nevolving attacks.\n", "link": "http://arxiv.org/abs/2312.09481v2", "date": "2024-03-13", "relevancy": 2.6335, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5311}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5292}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5197}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Continual%20Adversarial%20Defense&body=Title%3A%20Continual%20Adversarial%20Defense%0AAuthor%3A%20Qian%20Wang%20and%20Yaoyao%20Liu%20and%20Hefei%20Ling%20and%20Yingwei%20Li%20and%20Qihao%20Liu%20and%20Ping%20Li%20and%20Jiazhong%20Chen%20and%20Alan%20Yuille%20and%20Ning%20Yu%0AAbstract%3A%20%20%20In%20response%20to%20the%20rapidly%20evolving%20nature%20of%20adversarial%20attacks%20against%0Avisual%20classifiers%20on%20a%20monthly%20basis%2C%20numerous%20defenses%20have%20been%20proposed%20to%0Ageneralize%20against%20as%20many%20known%20attacks%20as%20possible.%20However%2C%20designing%20a%0Adefense%20method%20that%20generalizes%20to%20all%20types%20of%20attacks%20is%20not%20realistic%0Abecause%20the%20environment%20in%20which%20defense%20systems%20operate%20is%20dynamic%20and%0Acomprises%20various%20unique%20attacks%20that%20emerge%20as%20time%20goes%20on.%20The%20defense%0Asystem%20must%20gather%20online%20few-shot%20defense%20feedback%20to%20promptly%20enhance%20itself%2C%0Aleveraging%20efficient%20memory%20utilization.%20Therefore%2C%20we%20propose%20the%20first%0Acontinual%20adversarial%20defense%20%28CAD%29%20framework%20that%20adapts%20to%20any%20attacks%20in%20a%0Adynamic%20scenario%2C%20where%20various%20attacks%20emerge%20stage%20by%20stage.%20In%20practice%2C%20CAD%0Ais%20modeled%20under%20four%20principles%3A%20%281%29%20continual%20adaptation%20to%20new%20attacks%0Awithout%20catastrophic%20forgetting%2C%20%282%29%20few-shot%20adaptation%2C%20%283%29%20memory-efficient%0Aadaptation%2C%20and%20%284%29%20high%20accuracy%20on%20both%20clean%20and%20adversarial%20images.%20We%0Aexplore%20and%20integrate%20cutting-edge%20continual%20learning%2C%20few-shot%20learning%2C%20and%0Aensemble%20learning%20techniques%20to%20qualify%20the%20principles.%20Experiments%20conducted%0Aon%20CIFAR-10%20and%20ImageNet-100%20validate%20the%20effectiveness%20of%20our%20approach%20against%0Amultiple%20stages%20of%20modern%20adversarial%20attacks%20and%20demonstrate%20significant%0Aimprovements%20over%20numerous%20baseline%20methods.%20In%20particular%2C%20CAD%20is%20capable%20of%0Aquickly%20adapting%20with%20minimal%20feedback%20and%20a%20low%20cost%20of%20defense%20failure%2C%20while%0Amaintaining%20good%20performance%20against%20previous%20attacks.%20Our%20research%20sheds%20light%0Aon%20a%20brand-new%20paradigm%20for%20continual%20defense%20adaptation%20against%20dynamic%20and%0Aevolving%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09481v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Adversarial%20Defense&entry.906535625=Qian%20Wang%20and%20Yaoyao%20Liu%20and%20Hefei%20Ling%20and%20Yingwei%20Li%20and%20Qihao%20Liu%20and%20Ping%20Li%20and%20Jiazhong%20Chen%20and%20Alan%20Yuille%20and%20Ning%20Yu&entry.1292438233=%20%20In%20response%20to%20the%20rapidly%20evolving%20nature%20of%20adversarial%20attacks%20against%0Avisual%20classifiers%20on%20a%20monthly%20basis%2C%20numerous%20defenses%20have%20been%20proposed%20to%0Ageneralize%20against%20as%20many%20known%20attacks%20as%20possible.%20However%2C%20designing%20a%0Adefense%20method%20that%20generalizes%20to%20all%20types%20of%20attacks%20is%20not%20realistic%0Abecause%20the%20environment%20in%20which%20defense%20systems%20operate%20is%20dynamic%20and%0Acomprises%20various%20unique%20attacks%20that%20emerge%20as%20time%20goes%20on.%20The%20defense%0Asystem%20must%20gather%20online%20few-shot%20defense%20feedback%20to%20promptly%20enhance%20itself%2C%0Aleveraging%20efficient%20memory%20utilization.%20Therefore%2C%20we%20propose%20the%20first%0Acontinual%20adversarial%20defense%20%28CAD%29%20framework%20that%20adapts%20to%20any%20attacks%20in%20a%0Adynamic%20scenario%2C%20where%20various%20attacks%20emerge%20stage%20by%20stage.%20In%20practice%2C%20CAD%0Ais%20modeled%20under%20four%20principles%3A%20%281%29%20continual%20adaptation%20to%20new%20attacks%0Awithout%20catastrophic%20forgetting%2C%20%282%29%20few-shot%20adaptation%2C%20%283%29%20memory-efficient%0Aadaptation%2C%20and%20%284%29%20high%20accuracy%20on%20both%20clean%20and%20adversarial%20images.%20We%0Aexplore%20and%20integrate%20cutting-edge%20continual%20learning%2C%20few-shot%20learning%2C%20and%0Aensemble%20learning%20techniques%20to%20qualify%20the%20principles.%20Experiments%20conducted%0Aon%20CIFAR-10%20and%20ImageNet-100%20validate%20the%20effectiveness%20of%20our%20approach%20against%0Amultiple%20stages%20of%20modern%20adversarial%20attacks%20and%20demonstrate%20significant%0Aimprovements%20over%20numerous%20baseline%20methods.%20In%20particular%2C%20CAD%20is%20capable%20of%0Aquickly%20adapting%20with%20minimal%20feedback%20and%20a%20low%20cost%20of%20defense%20failure%2C%20while%0Amaintaining%20good%20performance%20against%20previous%20attacks.%20Our%20research%20sheds%20light%0Aon%20a%20brand-new%20paradigm%20for%20continual%20defense%20adaptation%20against%20dynamic%20and%0Aevolving%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09481v2&entry.124074799=Read"},
{"title": "Cross-Domain Few-Shot Segmentation via Iterative Support-Query\n  Correspondence Mining", "author": "Jiahao Nie and Yun Xing and Gongjie Zhang and Pei Yan and Aoran Xiao and Yap-Peng Tan and Alex C. Kot and Shijian Lu", "abstract": "  Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting\nnovel categories from a distinct domain using only limited exemplars. In this\npaper, we undertake a comprehensive study of CD-FSS and uncover two crucial\ninsights: (i) the necessity of a fine-tuning stage to effectively transfer the\nlearned meta-knowledge across domains, and (ii) the overfitting risk during the\nna\\\"ive fine-tuning due to the scarcity of novel category examples. With these\ninsights, we propose a novel cross-domain fine-tuning strategy that addresses\nthe challenging CD-FSS tasks. We first design Bi-directional Few-shot\nPrediction (BFP), which establishes support-query correspondence in a\nbi-directional manner, crafting augmented supervision to reduce the overfitting\nrisk. Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which\nis a recursive framework to capture the support-query correspondence\niteratively, targeting maximal exploitation of supervisory signals from the\nsparse novel category samples. Extensive empirical evaluations show that our\nmethod significantly outperforms the state-of-the-arts (+7.8\\%), which verifies\nthat IFA tackles the cross-domain challenges and mitigates the overfitting\nsimultaneously. The code is available at: https://github.com/niejiahao1998/IFA.\n", "link": "http://arxiv.org/abs/2401.08407v2", "date": "2024-03-13", "relevancy": 2.611, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5326}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5192}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5148}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Few-Shot%20Segmentation%20via%20Iterative%20Support-Query%0A%20%20Correspondence%20Mining&body=Title%3A%20Cross-Domain%20Few-Shot%20Segmentation%20via%20Iterative%20Support-Query%0A%20%20Correspondence%20Mining%0AAuthor%3A%20Jiahao%20Nie%20and%20Yun%20Xing%20and%20Gongjie%20Zhang%20and%20Pei%20Yan%20and%20Aoran%20Xiao%20and%20Yap-Peng%20Tan%20and%20Alex%20C.%20Kot%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20Cross-Domain%20Few-Shot%20Segmentation%20%28CD-FSS%29%20poses%20the%20challenge%20of%20segmenting%0Anovel%20categories%20from%20a%20distinct%20domain%20using%20only%20limited%20exemplars.%20In%20this%0Apaper%2C%20we%20undertake%20a%20comprehensive%20study%20of%20CD-FSS%20and%20uncover%20two%20crucial%0Ainsights%3A%20%28i%29%20the%20necessity%20of%20a%20fine-tuning%20stage%20to%20effectively%20transfer%20the%0Alearned%20meta-knowledge%20across%20domains%2C%20and%20%28ii%29%20the%20overfitting%20risk%20during%20the%0Ana%5C%22ive%20fine-tuning%20due%20to%20the%20scarcity%20of%20novel%20category%20examples.%20With%20these%0Ainsights%2C%20we%20propose%20a%20novel%20cross-domain%20fine-tuning%20strategy%20that%20addresses%0Athe%20challenging%20CD-FSS%20tasks.%20We%20first%20design%20Bi-directional%20Few-shot%0APrediction%20%28BFP%29%2C%20which%20establishes%20support-query%20correspondence%20in%20a%0Abi-directional%20manner%2C%20crafting%20augmented%20supervision%20to%20reduce%20the%20overfitting%0Arisk.%20Then%20we%20further%20extend%20BFP%20into%20Iterative%20Few-shot%20Adaptor%20%28IFA%29%2C%20which%0Ais%20a%20recursive%20framework%20to%20capture%20the%20support-query%20correspondence%0Aiteratively%2C%20targeting%20maximal%20exploitation%20of%20supervisory%20signals%20from%20the%0Asparse%20novel%20category%20samples.%20Extensive%20empirical%20evaluations%20show%20that%20our%0Amethod%20significantly%20outperforms%20the%20state-of-the-arts%20%28%2B7.8%5C%25%29%2C%20which%20verifies%0Athat%20IFA%20tackles%20the%20cross-domain%20challenges%20and%20mitigates%20the%20overfitting%0Asimultaneously.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/niejiahao1998/IFA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08407v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Few-Shot%20Segmentation%20via%20Iterative%20Support-Query%0A%20%20Correspondence%20Mining&entry.906535625=Jiahao%20Nie%20and%20Yun%20Xing%20and%20Gongjie%20Zhang%20and%20Pei%20Yan%20and%20Aoran%20Xiao%20and%20Yap-Peng%20Tan%20and%20Alex%20C.%20Kot%20and%20Shijian%20Lu&entry.1292438233=%20%20Cross-Domain%20Few-Shot%20Segmentation%20%28CD-FSS%29%20poses%20the%20challenge%20of%20segmenting%0Anovel%20categories%20from%20a%20distinct%20domain%20using%20only%20limited%20exemplars.%20In%20this%0Apaper%2C%20we%20undertake%20a%20comprehensive%20study%20of%20CD-FSS%20and%20uncover%20two%20crucial%0Ainsights%3A%20%28i%29%20the%20necessity%20of%20a%20fine-tuning%20stage%20to%20effectively%20transfer%20the%0Alearned%20meta-knowledge%20across%20domains%2C%20and%20%28ii%29%20the%20overfitting%20risk%20during%20the%0Ana%5C%22ive%20fine-tuning%20due%20to%20the%20scarcity%20of%20novel%20category%20examples.%20With%20these%0Ainsights%2C%20we%20propose%20a%20novel%20cross-domain%20fine-tuning%20strategy%20that%20addresses%0Athe%20challenging%20CD-FSS%20tasks.%20We%20first%20design%20Bi-directional%20Few-shot%0APrediction%20%28BFP%29%2C%20which%20establishes%20support-query%20correspondence%20in%20a%0Abi-directional%20manner%2C%20crafting%20augmented%20supervision%20to%20reduce%20the%20overfitting%0Arisk.%20Then%20we%20further%20extend%20BFP%20into%20Iterative%20Few-shot%20Adaptor%20%28IFA%29%2C%20which%0Ais%20a%20recursive%20framework%20to%20capture%20the%20support-query%20correspondence%0Aiteratively%2C%20targeting%20maximal%20exploitation%20of%20supervisory%20signals%20from%20the%0Asparse%20novel%20category%20samples.%20Extensive%20empirical%20evaluations%20show%20that%20our%0Amethod%20significantly%20outperforms%20the%20state-of-the-arts%20%28%2B7.8%5C%25%29%2C%20which%20verifies%0Athat%20IFA%20tackles%20the%20cross-domain%20challenges%20and%20mitigates%20the%20overfitting%0Asimultaneously.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/niejiahao1998/IFA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08407v2&entry.124074799=Read"},
{"title": "Self-Supervised Learning for Covariance Estimation", "author": "Tzvi Diskin and Ami Wiesel", "abstract": "  We consider the use of deep learning for covariance estimation. We propose to\nglobally learn a neural network that will then be applied locally at inference\ntime. Leveraging recent advancements in self-supervised foundational models, we\ntrain the network without any labeling by simply masking different samples and\nlearning to predict their covariance given their surrounding neighbors. The\narchitecture is based on the popular attention mechanism. Its main advantage\nover classical methods is the automatic exploitation of global characteristics\nwithout any distributional assumptions or regularization. It can be pre-trained\nas a foundation model and then be repurposed for various downstream tasks,\ne.g., adaptive target detection in radar or hyperspectral imagery.\n", "link": "http://arxiv.org/abs/2403.08662v1", "date": "2024-03-13", "relevancy": 2.5931, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.535}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5213}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4995}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20for%20Covariance%20Estimation&body=Title%3A%20Self-Supervised%20Learning%20for%20Covariance%20Estimation%0AAuthor%3A%20Tzvi%20Diskin%20and%20Ami%20Wiesel%0AAbstract%3A%20%20%20We%20consider%20the%20use%20of%20deep%20learning%20for%20covariance%20estimation.%20We%20propose%20to%0Aglobally%20learn%20a%20neural%20network%20that%20will%20then%20be%20applied%20locally%20at%20inference%0Atime.%20Leveraging%20recent%20advancements%20in%20self-supervised%20foundational%20models%2C%20we%0Atrain%20the%20network%20without%20any%20labeling%20by%20simply%20masking%20different%20samples%20and%0Alearning%20to%20predict%20their%20covariance%20given%20their%20surrounding%20neighbors.%20The%0Aarchitecture%20is%20based%20on%20the%20popular%20attention%20mechanism.%20Its%20main%20advantage%0Aover%20classical%20methods%20is%20the%20automatic%20exploitation%20of%20global%20characteristics%0Awithout%20any%20distributional%20assumptions%20or%20regularization.%20It%20can%20be%20pre-trained%0Aas%20a%20foundation%20model%20and%20then%20be%20repurposed%20for%20various%20downstream%20tasks%2C%0Ae.g.%2C%20adaptive%20target%20detection%20in%20radar%20or%20hyperspectral%20imagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08662v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20for%20Covariance%20Estimation&entry.906535625=Tzvi%20Diskin%20and%20Ami%20Wiesel&entry.1292438233=%20%20We%20consider%20the%20use%20of%20deep%20learning%20for%20covariance%20estimation.%20We%20propose%20to%0Aglobally%20learn%20a%20neural%20network%20that%20will%20then%20be%20applied%20locally%20at%20inference%0Atime.%20Leveraging%20recent%20advancements%20in%20self-supervised%20foundational%20models%2C%20we%0Atrain%20the%20network%20without%20any%20labeling%20by%20simply%20masking%20different%20samples%20and%0Alearning%20to%20predict%20their%20covariance%20given%20their%20surrounding%20neighbors.%20The%0Aarchitecture%20is%20based%20on%20the%20popular%20attention%20mechanism.%20Its%20main%20advantage%0Aover%20classical%20methods%20is%20the%20automatic%20exploitation%20of%20global%20characteristics%0Awithout%20any%20distributional%20assumptions%20or%20regularization.%20It%20can%20be%20pre-trained%0Aas%20a%20foundation%20model%20and%20then%20be%20repurposed%20for%20various%20downstream%20tasks%2C%0Ae.g.%2C%20adaptive%20target%20detection%20in%20radar%20or%20hyperspectral%20imagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08662v1&entry.124074799=Read"},
{"title": "Refractive COLMAP: Refractive Structure-from-Motion Revisited", "author": "Mengkun She and Felix Seegr\u00e4ber and David Nakath and Kevin K\u00f6ser", "abstract": "  In this paper, we present a complete refractive Structure-from-Motion (RSfM)\nframework for underwater 3D reconstruction using refractive camera setups (for\nboth, flat- and dome-port underwater housings). Despite notable achievements in\nrefractive multi-view geometry over the past decade, a robust, complete and\npublicly available solution for such tasks is not available at present, and\noften practical applications have to resort to approximating refraction effects\nby the intrinsic (distortion) parameters of a pinhole camera model. To fill\nthis gap, we have integrated refraction considerations throughout the entire\nSfM process within the state-of-the-art, open-source SfM framework COLMAP.\nNumerical simulations and reconstruction results on synthetically generated but\nphoto-realistic images with ground truth validate that enabling refraction does\nnot compromise accuracy or robustness as compared to in-air reconstructions.\nFinally, we demonstrate the capability of our approach for large-scale\nrefractive scenarios using a dataset consisting of nearly 6000 images. The\nimplementation is released as open-source at:\nhttps://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.\n", "link": "http://arxiv.org/abs/2403.08640v1", "date": "2024-03-13", "relevancy": 2.5799, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.548}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5129}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4871}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited&body=Title%3A%20Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited%0AAuthor%3A%20Mengkun%20She%20and%20Felix%20Seegr%C3%A4ber%20and%20David%20Nakath%20and%20Kevin%20K%C3%B6ser%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20complete%20refractive%20Structure-from-Motion%20%28RSfM%29%0Aframework%20for%20underwater%203D%20reconstruction%20using%20refractive%20camera%20setups%20%28for%0Aboth%2C%20flat-%20and%20dome-port%20underwater%20housings%29.%20Despite%20notable%20achievements%20in%0Arefractive%20multi-view%20geometry%20over%20the%20past%20decade%2C%20a%20robust%2C%20complete%20and%0Apublicly%20available%20solution%20for%20such%20tasks%20is%20not%20available%20at%20present%2C%20and%0Aoften%20practical%20applications%20have%20to%20resort%20to%20approximating%20refraction%20effects%0Aby%20the%20intrinsic%20%28distortion%29%20parameters%20of%20a%20pinhole%20camera%20model.%20To%20fill%0Athis%20gap%2C%20we%20have%20integrated%20refraction%20considerations%20throughout%20the%20entire%0ASfM%20process%20within%20the%20state-of-the-art%2C%20open-source%20SfM%20framework%20COLMAP.%0ANumerical%20simulations%20and%20reconstruction%20results%20on%20synthetically%20generated%20but%0Aphoto-realistic%20images%20with%20ground%20truth%20validate%20that%20enabling%20refraction%20does%0Anot%20compromise%20accuracy%20or%20robustness%20as%20compared%20to%20in-air%20reconstructions.%0AFinally%2C%20we%20demonstrate%20the%20capability%20of%20our%20approach%20for%20large-scale%0Arefractive%20scenarios%20using%20a%20dataset%20consisting%20of%20nearly%206000%20images.%20The%0Aimplementation%20is%20released%20as%20open-source%20at%3A%0Ahttps%3A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08640v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited&entry.906535625=Mengkun%20She%20and%20Felix%20Seegr%C3%A4ber%20and%20David%20Nakath%20and%20Kevin%20K%C3%B6ser&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20complete%20refractive%20Structure-from-Motion%20%28RSfM%29%0Aframework%20for%20underwater%203D%20reconstruction%20using%20refractive%20camera%20setups%20%28for%0Aboth%2C%20flat-%20and%20dome-port%20underwater%20housings%29.%20Despite%20notable%20achievements%20in%0Arefractive%20multi-view%20geometry%20over%20the%20past%20decade%2C%20a%20robust%2C%20complete%20and%0Apublicly%20available%20solution%20for%20such%20tasks%20is%20not%20available%20at%20present%2C%20and%0Aoften%20practical%20applications%20have%20to%20resort%20to%20approximating%20refraction%20effects%0Aby%20the%20intrinsic%20%28distortion%29%20parameters%20of%20a%20pinhole%20camera%20model.%20To%20fill%0Athis%20gap%2C%20we%20have%20integrated%20refraction%20considerations%20throughout%20the%20entire%0ASfM%20process%20within%20the%20state-of-the-art%2C%20open-source%20SfM%20framework%20COLMAP.%0ANumerical%20simulations%20and%20reconstruction%20results%20on%20synthetically%20generated%20but%0Aphoto-realistic%20images%20with%20ground%20truth%20validate%20that%20enabling%20refraction%20does%0Anot%20compromise%20accuracy%20or%20robustness%20as%20compared%20to%20in-air%20reconstructions.%0AFinally%2C%20we%20demonstrate%20the%20capability%20of%20our%20approach%20for%20large-scale%0Arefractive%20scenarios%20using%20a%20dataset%20consisting%20of%20nearly%206000%20images.%20The%0Aimplementation%20is%20released%20as%20open-source%20at%3A%0Ahttps%3A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08640v1&entry.124074799=Read"},
{"title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis", "author": "Enric Corona and Andrei Zanfir and Eduard Gabriel Bazavan and Nikos Kolotouros and Thiemo Alldieck and Cristian Sminchisescu", "abstract": "  We propose VLOGGER, a method for audio-driven human video generation from a\nsingle input image of a person, which builds on the success of recent\ngenerative diffusion models. Our method consists of 1) a stochastic\nhuman-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture\nthat augments text-to-image models with both spatial and temporal controls.\nThis supports the generation of high quality video of variable length, easily\ncontrollable through high-level representations of human faces and bodies. In\ncontrast to previous work, our method does not require training for each\nperson, does not rely on face detection and cropping, generates the complete\nimage (not just the face or the lips), and considers a broad spectrum of\nscenarios (e.g. visible torso or diverse subject identities) that are critical\nto correctly synthesize humans who communicate. We also curate MENTOR, a new\nand diverse dataset with 3d pose and expression annotations, one order of\nmagnitude larger than previous ones (800,000 identities) and with dynamic\ngestures, on which we train and ablate our main technical contributions.\n  VLOGGER outperforms state-of-the-art methods in three public benchmarks,\nconsidering image quality, identity preservation and temporal consistency while\nalso generating upper-body gestures. We analyze the performance of VLOGGER with\nrespect to multiple diversity metrics, showing that our architectural choices\nand the use of MENTOR benefit training a fair and unbiased model at scale.\nFinally we show applications in video editing and personalization.\n", "link": "http://arxiv.org/abs/2403.08764v1", "date": "2024-03-13", "relevancy": 2.5634, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6962}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6712}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5733}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20VLOGGER%3A%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20Synthesis&body=Title%3A%20VLOGGER%3A%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20Synthesis%0AAuthor%3A%20Enric%20Corona%20and%20Andrei%20Zanfir%20and%20Eduard%20Gabriel%20Bazavan%20and%20Nikos%20Kolotouros%20and%20Thiemo%20Alldieck%20and%20Cristian%20Sminchisescu%0AAbstract%3A%20%20%20We%20propose%20VLOGGER%2C%20a%20method%20for%20audio-driven%20human%20video%20generation%20from%20a%0Asingle%20input%20image%20of%20a%20person%2C%20which%20builds%20on%20the%20success%20of%20recent%0Agenerative%20diffusion%20models.%20Our%20method%20consists%20of%201%29%20a%20stochastic%0Ahuman-to-3d-motion%20diffusion%20model%2C%20and%202%29%20a%20novel%20diffusion-based%20architecture%0Athat%20augments%20text-to-image%20models%20with%20both%20spatial%20and%20temporal%20controls.%0AThis%20supports%20the%20generation%20of%20high%20quality%20video%20of%20variable%20length%2C%20easily%0Acontrollable%20through%20high-level%20representations%20of%20human%20faces%20and%20bodies.%20In%0Acontrast%20to%20previous%20work%2C%20our%20method%20does%20not%20require%20training%20for%20each%0Aperson%2C%20does%20not%20rely%20on%20face%20detection%20and%20cropping%2C%20generates%20the%20complete%0Aimage%20%28not%20just%20the%20face%20or%20the%20lips%29%2C%20and%20considers%20a%20broad%20spectrum%20of%0Ascenarios%20%28e.g.%20visible%20torso%20or%20diverse%20subject%20identities%29%20that%20are%20critical%0Ato%20correctly%20synthesize%20humans%20who%20communicate.%20We%20also%20curate%20MENTOR%2C%20a%20new%0Aand%20diverse%20dataset%20with%203d%20pose%20and%20expression%20annotations%2C%20one%20order%20of%0Amagnitude%20larger%20than%20previous%20ones%20%28800%2C000%20identities%29%20and%20with%20dynamic%0Agestures%2C%20on%20which%20we%20train%20and%20ablate%20our%20main%20technical%20contributions.%0A%20%20VLOGGER%20outperforms%20state-of-the-art%20methods%20in%20three%20public%20benchmarks%2C%0Aconsidering%20image%20quality%2C%20identity%20preservation%20and%20temporal%20consistency%20while%0Aalso%20generating%20upper-body%20gestures.%20We%20analyze%20the%20performance%20of%20VLOGGER%20with%0Arespect%20to%20multiple%20diversity%20metrics%2C%20showing%20that%20our%20architectural%20choices%0Aand%20the%20use%20of%20MENTOR%20benefit%20training%20a%20fair%20and%20unbiased%20model%20at%20scale.%0AFinally%20we%20show%20applications%20in%20video%20editing%20and%20personalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08764v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLOGGER%3A%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20Synthesis&entry.906535625=Enric%20Corona%20and%20Andrei%20Zanfir%20and%20Eduard%20Gabriel%20Bazavan%20and%20Nikos%20Kolotouros%20and%20Thiemo%20Alldieck%20and%20Cristian%20Sminchisescu&entry.1292438233=%20%20We%20propose%20VLOGGER%2C%20a%20method%20for%20audio-driven%20human%20video%20generation%20from%20a%0Asingle%20input%20image%20of%20a%20person%2C%20which%20builds%20on%20the%20success%20of%20recent%0Agenerative%20diffusion%20models.%20Our%20method%20consists%20of%201%29%20a%20stochastic%0Ahuman-to-3d-motion%20diffusion%20model%2C%20and%202%29%20a%20novel%20diffusion-based%20architecture%0Athat%20augments%20text-to-image%20models%20with%20both%20spatial%20and%20temporal%20controls.%0AThis%20supports%20the%20generation%20of%20high%20quality%20video%20of%20variable%20length%2C%20easily%0Acontrollable%20through%20high-level%20representations%20of%20human%20faces%20and%20bodies.%20In%0Acontrast%20to%20previous%20work%2C%20our%20method%20does%20not%20require%20training%20for%20each%0Aperson%2C%20does%20not%20rely%20on%20face%20detection%20and%20cropping%2C%20generates%20the%20complete%0Aimage%20%28not%20just%20the%20face%20or%20the%20lips%29%2C%20and%20considers%20a%20broad%20spectrum%20of%0Ascenarios%20%28e.g.%20visible%20torso%20or%20diverse%20subject%20identities%29%20that%20are%20critical%0Ato%20correctly%20synthesize%20humans%20who%20communicate.%20We%20also%20curate%20MENTOR%2C%20a%20new%0Aand%20diverse%20dataset%20with%203d%20pose%20and%20expression%20annotations%2C%20one%20order%20of%0Amagnitude%20larger%20than%20previous%20ones%20%28800%2C000%20identities%29%20and%20with%20dynamic%0Agestures%2C%20on%20which%20we%20train%20and%20ablate%20our%20main%20technical%20contributions.%0A%20%20VLOGGER%20outperforms%20state-of-the-art%20methods%20in%20three%20public%20benchmarks%2C%0Aconsidering%20image%20quality%2C%20identity%20preservation%20and%20temporal%20consistency%20while%0Aalso%20generating%20upper-body%20gestures.%20We%20analyze%20the%20performance%20of%20VLOGGER%20with%0Arespect%20to%20multiple%20diversity%20metrics%2C%20showing%20that%20our%20architectural%20choices%0Aand%20the%20use%20of%20MENTOR%20benefit%20training%20a%20fair%20and%20unbiased%20model%20at%20scale.%0AFinally%20we%20show%20applications%20in%20video%20editing%20and%20personalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08764v1&entry.124074799=Read"},
{"title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models\n  for Generic Graph Mining", "author": "Yanchao Tan and Hang Lv and Xinyi Huang and Jiawei Zhang and Shiping Wang and Carl Yang", "abstract": "  Graphs with abundant attributes are essential in modeling interconnected\nentities and improving predictions in various real-world applications.\nTraditional Graph Neural Networks (GNNs), which are commonly used for modeling\nattributed graphs, need to be re-trained every time when applied to different\ngraph tasks and datasets. Although the emergence of Large Language Models\n(LLMs) has introduced a new paradigm in natural language processing, the\ngenerative potential of LLMs in graph mining remains largely under-explored. To\nthis end, we propose a novel framework MuseGraph, which seamlessly integrates\nthe strengths of GNNs and LLMs and facilitates a more effective and generic\napproach for graph mining across different tasks and datasets. Specifically, we\nfirst introduce a compact graph description via the proposed adaptive input\ngeneration to encapsulate key information from the graph under the constraints\nof language token limitations. Then, we propose a diverse instruction\ngeneration mechanism, which distills the reasoning capabilities from LLMs\n(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction\npackages for different graph tasks. Finally, we propose a graph-aware\ninstruction tuning with a dynamic instruction package allocation strategy\nacross tasks and datasets, ensuring the effectiveness and generalization of the\ntraining process. Our experimental results demonstrate significant improvements\nin different graph tasks, showcasing the potential of our MuseGraph in\nenhancing the accuracy of graph-oriented downstream tasks while keeping the\ngeneration powers of LLMs.\n", "link": "http://arxiv.org/abs/2403.04780v2", "date": "2024-03-13", "relevancy": 2.5607, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5244}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5071}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5048}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MuseGraph%3A%20Graph-oriented%20Instruction%20Tuning%20of%20Large%20Language%20Models%0A%20%20for%20Generic%20Graph%20Mining&body=Title%3A%20MuseGraph%3A%20Graph-oriented%20Instruction%20Tuning%20of%20Large%20Language%20Models%0A%20%20for%20Generic%20Graph%20Mining%0AAuthor%3A%20Yanchao%20Tan%20and%20Hang%20Lv%20and%20Xinyi%20Huang%20and%20Jiawei%20Zhang%20and%20Shiping%20Wang%20and%20Carl%20Yang%0AAbstract%3A%20%20%20Graphs%20with%20abundant%20attributes%20are%20essential%20in%20modeling%20interconnected%0Aentities%20and%20improving%20predictions%20in%20various%20real-world%20applications.%0ATraditional%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20are%20commonly%20used%20for%20modeling%0Aattributed%20graphs%2C%20need%20to%20be%20re-trained%20every%20time%20when%20applied%20to%20different%0Agraph%20tasks%20and%20datasets.%20Although%20the%20emergence%20of%20Large%20Language%20Models%0A%28LLMs%29%20has%20introduced%20a%20new%20paradigm%20in%20natural%20language%20processing%2C%20the%0Agenerative%20potential%20of%20LLMs%20in%20graph%20mining%20remains%20largely%20under-explored.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20framework%20MuseGraph%2C%20which%20seamlessly%20integrates%0Athe%20strengths%20of%20GNNs%20and%20LLMs%20and%20facilitates%20a%20more%20effective%20and%20generic%0Aapproach%20for%20graph%20mining%20across%20different%20tasks%20and%20datasets.%20Specifically%2C%20we%0Afirst%20introduce%20a%20compact%20graph%20description%20via%20the%20proposed%20adaptive%20input%0Ageneration%20to%20encapsulate%20key%20information%20from%20the%20graph%20under%20the%20constraints%0Aof%20language%20token%20limitations.%20Then%2C%20we%20propose%20a%20diverse%20instruction%0Ageneration%20mechanism%2C%20which%20distills%20the%20reasoning%20capabilities%20from%20LLMs%0A%28e.g.%2C%20GPT-4%29%20to%20create%20task-specific%20Chain-of-Thought-based%20instruction%0Apackages%20for%20different%20graph%20tasks.%20Finally%2C%20we%20propose%20a%20graph-aware%0Ainstruction%20tuning%20with%20a%20dynamic%20instruction%20package%20allocation%20strategy%0Aacross%20tasks%20and%20datasets%2C%20ensuring%20the%20effectiveness%20and%20generalization%20of%20the%0Atraining%20process.%20Our%20experimental%20results%20demonstrate%20significant%20improvements%0Ain%20different%20graph%20tasks%2C%20showcasing%20the%20potential%20of%20our%20MuseGraph%20in%0Aenhancing%20the%20accuracy%20of%20graph-oriented%20downstream%20tasks%20while%20keeping%20the%0Ageneration%20powers%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04780v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuseGraph%3A%20Graph-oriented%20Instruction%20Tuning%20of%20Large%20Language%20Models%0A%20%20for%20Generic%20Graph%20Mining&entry.906535625=Yanchao%20Tan%20and%20Hang%20Lv%20and%20Xinyi%20Huang%20and%20Jiawei%20Zhang%20and%20Shiping%20Wang%20and%20Carl%20Yang&entry.1292438233=%20%20Graphs%20with%20abundant%20attributes%20are%20essential%20in%20modeling%20interconnected%0Aentities%20and%20improving%20predictions%20in%20various%20real-world%20applications.%0ATraditional%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20are%20commonly%20used%20for%20modeling%0Aattributed%20graphs%2C%20need%20to%20be%20re-trained%20every%20time%20when%20applied%20to%20different%0Agraph%20tasks%20and%20datasets.%20Although%20the%20emergence%20of%20Large%20Language%20Models%0A%28LLMs%29%20has%20introduced%20a%20new%20paradigm%20in%20natural%20language%20processing%2C%20the%0Agenerative%20potential%20of%20LLMs%20in%20graph%20mining%20remains%20largely%20under-explored.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20framework%20MuseGraph%2C%20which%20seamlessly%20integrates%0Athe%20strengths%20of%20GNNs%20and%20LLMs%20and%20facilitates%20a%20more%20effective%20and%20generic%0Aapproach%20for%20graph%20mining%20across%20different%20tasks%20and%20datasets.%20Specifically%2C%20we%0Afirst%20introduce%20a%20compact%20graph%20description%20via%20the%20proposed%20adaptive%20input%0Ageneration%20to%20encapsulate%20key%20information%20from%20the%20graph%20under%20the%20constraints%0Aof%20language%20token%20limitations.%20Then%2C%20we%20propose%20a%20diverse%20instruction%0Ageneration%20mechanism%2C%20which%20distills%20the%20reasoning%20capabilities%20from%20LLMs%0A%28e.g.%2C%20GPT-4%29%20to%20create%20task-specific%20Chain-of-Thought-based%20instruction%0Apackages%20for%20different%20graph%20tasks.%20Finally%2C%20we%20propose%20a%20graph-aware%0Ainstruction%20tuning%20with%20a%20dynamic%20instruction%20package%20allocation%20strategy%0Aacross%20tasks%20and%20datasets%2C%20ensuring%20the%20effectiveness%20and%20generalization%20of%20the%0Atraining%20process.%20Our%20experimental%20results%20demonstrate%20significant%20improvements%0Ain%20different%20graph%20tasks%2C%20showcasing%20the%20potential%20of%20our%20MuseGraph%20in%0Aenhancing%20the%20accuracy%20of%20graph-oriented%20downstream%20tasks%20while%20keeping%20the%0Ageneration%20powers%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04780v2&entry.124074799=Read"},
{"title": "FastMAC: Stochastic Spectral Sampling of Correspondence Graph", "author": "Yifei Zhang and Hao Zhao and Hongyang Li and Siheng Chen", "abstract": "  3D correspondence, i.e., a pair of 3D points, is a fundamental concept in\ncomputer vision. A set of 3D correspondences, when equipped with compatibility\nedges, forms a correspondence graph. This graph is a critical component in\nseveral state-of-the-art 3D point cloud registration approaches, e.g., the one\nbased on maximal cliques (MAC). However, its properties have not been well\nunderstood. So we present the first study that introduces graph signal\nprocessing into the domain of correspondence graph. We exploit the generalized\ndegree signal on correspondence graph and pursue sampling strategies that\npreserve high-frequency components of this signal. To address time-consuming\nsingular value decomposition in deterministic sampling, we resort to a\nstochastic approximate sampling strategy. As such, the core of our method is\nthe stochastic spectral sampling of correspondence graph. As an application, we\nbuild a complete 3D registration algorithm termed as FastMAC, that reaches\nreal-time speed while leading to little to none performance drop. Through\nextensive experiments, we validate that FastMAC works for both indoor and\noutdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while\nmaintaining high registration success rate on KITTI. Codes are publicly\navailable at https://github.com/Forrest-110/FastMAC.\n", "link": "http://arxiv.org/abs/2403.08770v1", "date": "2024-03-13", "relevancy": 2.548, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4966}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4839}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20FastMAC%3A%20Stochastic%20Spectral%20Sampling%20of%20Correspondence%20Graph&body=Title%3A%20FastMAC%3A%20Stochastic%20Spectral%20Sampling%20of%20Correspondence%20Graph%0AAuthor%3A%20Yifei%20Zhang%20and%20Hao%20Zhao%20and%20Hongyang%20Li%20and%20Siheng%20Chen%0AAbstract%3A%20%20%203D%20correspondence%2C%20i.e.%2C%20a%20pair%20of%203D%20points%2C%20is%20a%20fundamental%20concept%20in%0Acomputer%20vision.%20A%20set%20of%203D%20correspondences%2C%20when%20equipped%20with%20compatibility%0Aedges%2C%20forms%20a%20correspondence%20graph.%20This%20graph%20is%20a%20critical%20component%20in%0Aseveral%20state-of-the-art%203D%20point%20cloud%20registration%20approaches%2C%20e.g.%2C%20the%20one%0Abased%20on%20maximal%20cliques%20%28MAC%29.%20However%2C%20its%20properties%20have%20not%20been%20well%0Aunderstood.%20So%20we%20present%20the%20first%20study%20that%20introduces%20graph%20signal%0Aprocessing%20into%20the%20domain%20of%20correspondence%20graph.%20We%20exploit%20the%20generalized%0Adegree%20signal%20on%20correspondence%20graph%20and%20pursue%20sampling%20strategies%20that%0Apreserve%20high-frequency%20components%20of%20this%20signal.%20To%20address%20time-consuming%0Asingular%20value%20decomposition%20in%20deterministic%20sampling%2C%20we%20resort%20to%20a%0Astochastic%20approximate%20sampling%20strategy.%20As%20such%2C%20the%20core%20of%20our%20method%20is%0Athe%20stochastic%20spectral%20sampling%20of%20correspondence%20graph.%20As%20an%20application%2C%20we%0Abuild%20a%20complete%203D%20registration%20algorithm%20termed%20as%20FastMAC%2C%20that%20reaches%0Areal-time%20speed%20while%20leading%20to%20little%20to%20none%20performance%20drop.%20Through%0Aextensive%20experiments%2C%20we%20validate%20that%20FastMAC%20works%20for%20both%20indoor%20and%0Aoutdoor%20benchmarks.%20For%20example%2C%20FastMAC%20can%20accelerate%20MAC%20by%2080%20times%20while%0Amaintaining%20high%20registration%20success%20rate%20on%20KITTI.%20Codes%20are%20publicly%0Aavailable%20at%20https%3A//github.com/Forrest-110/FastMAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08770v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastMAC%3A%20Stochastic%20Spectral%20Sampling%20of%20Correspondence%20Graph&entry.906535625=Yifei%20Zhang%20and%20Hao%20Zhao%20and%20Hongyang%20Li%20and%20Siheng%20Chen&entry.1292438233=%20%203D%20correspondence%2C%20i.e.%2C%20a%20pair%20of%203D%20points%2C%20is%20a%20fundamental%20concept%20in%0Acomputer%20vision.%20A%20set%20of%203D%20correspondences%2C%20when%20equipped%20with%20compatibility%0Aedges%2C%20forms%20a%20correspondence%20graph.%20This%20graph%20is%20a%20critical%20component%20in%0Aseveral%20state-of-the-art%203D%20point%20cloud%20registration%20approaches%2C%20e.g.%2C%20the%20one%0Abased%20on%20maximal%20cliques%20%28MAC%29.%20However%2C%20its%20properties%20have%20not%20been%20well%0Aunderstood.%20So%20we%20present%20the%20first%20study%20that%20introduces%20graph%20signal%0Aprocessing%20into%20the%20domain%20of%20correspondence%20graph.%20We%20exploit%20the%20generalized%0Adegree%20signal%20on%20correspondence%20graph%20and%20pursue%20sampling%20strategies%20that%0Apreserve%20high-frequency%20components%20of%20this%20signal.%20To%20address%20time-consuming%0Asingular%20value%20decomposition%20in%20deterministic%20sampling%2C%20we%20resort%20to%20a%0Astochastic%20approximate%20sampling%20strategy.%20As%20such%2C%20the%20core%20of%20our%20method%20is%0Athe%20stochastic%20spectral%20sampling%20of%20correspondence%20graph.%20As%20an%20application%2C%20we%0Abuild%20a%20complete%203D%20registration%20algorithm%20termed%20as%20FastMAC%2C%20that%20reaches%0Areal-time%20speed%20while%20leading%20to%20little%20to%20none%20performance%20drop.%20Through%0Aextensive%20experiments%2C%20we%20validate%20that%20FastMAC%20works%20for%20both%20indoor%20and%0Aoutdoor%20benchmarks.%20For%20example%2C%20FastMAC%20can%20accelerate%20MAC%20by%2080%20times%20while%0Amaintaining%20high%20registration%20success%20rate%20on%20KITTI.%20Codes%20are%20publicly%0Aavailable%20at%20https%3A//github.com/Forrest-110/FastMAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08770v1&entry.124074799=Read"},
{"title": "Adaptive Sharpness-Aware Pruning for Robust Sparse Networks", "author": "Anna Bair and Hongxu Yin and Maying Shen and Pavlo Molchanov and Jose Alvarez", "abstract": "  Robustness and compactness are two essential attributes of deep learning\nmodels that are deployed in the real world. The goals of robustness and\ncompactness may seem to be at odds, since robustness requires generalization\nacross domains, while the process of compression exploits specificity in one\ndomain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies\nthese goals through the lens of network sharpness. The AdaSAP method produces\nsparse networks that are robust to input variations which are unseen at\ntraining time. We achieve this by strategically incorporating weight\nperturbations in order to optimize the loss landscape. This allows the model to\nbe both primed for pruning and regularized for improved robustness. AdaSAP\nimproves the robust accuracy of pruned models on image classification by up to\n+6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a\ncorrupted Pascal VOC dataset, over a wide range of compression ratios, pruning\ncriteria, and network architectures, outperforming recent pruning art by large\nmargins.\n", "link": "http://arxiv.org/abs/2306.14306v2", "date": "2024-03-13", "relevancy": 2.5355, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5327}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5112}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4774}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Sharpness-Aware%20Pruning%20for%20Robust%20Sparse%20Networks&body=Title%3A%20Adaptive%20Sharpness-Aware%20Pruning%20for%20Robust%20Sparse%20Networks%0AAuthor%3A%20Anna%20Bair%20and%20Hongxu%20Yin%20and%20Maying%20Shen%20and%20Pavlo%20Molchanov%20and%20Jose%20Alvarez%0AAbstract%3A%20%20%20Robustness%20and%20compactness%20are%20two%20essential%20attributes%20of%20deep%20learning%0Amodels%20that%20are%20deployed%20in%20the%20real%20world.%20The%20goals%20of%20robustness%20and%0Acompactness%20may%20seem%20to%20be%20at%20odds%2C%20since%20robustness%20requires%20generalization%0Aacross%20domains%2C%20while%20the%20process%20of%20compression%20exploits%20specificity%20in%20one%0Adomain.%20We%20introduce%20Adaptive%20Sharpness-Aware%20Pruning%20%28AdaSAP%29%2C%20which%20unifies%0Athese%20goals%20through%20the%20lens%20of%20network%20sharpness.%20The%20AdaSAP%20method%20produces%0Asparse%20networks%20that%20are%20robust%20to%20input%20variations%20which%20are%20unseen%20at%0Atraining%20time.%20We%20achieve%20this%20by%20strategically%20incorporating%20weight%0Aperturbations%20in%20order%20to%20optimize%20the%20loss%20landscape.%20This%20allows%20the%20model%20to%0Abe%20both%20primed%20for%20pruning%20and%20regularized%20for%20improved%20robustness.%20AdaSAP%0Aimproves%20the%20robust%20accuracy%20of%20pruned%20models%20on%20image%20classification%20by%20up%20to%0A%2B6%25%20on%20ImageNet%20C%20and%20%2B4%25%20on%20ImageNet%20V2%2C%20and%20on%20object%20detection%20by%20%2B4%25%20on%20a%0Acorrupted%20Pascal%20VOC%20dataset%2C%20over%20a%20wide%20range%20of%20compression%20ratios%2C%20pruning%0Acriteria%2C%20and%20network%20architectures%2C%20outperforming%20recent%20pruning%20art%20by%20large%0Amargins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.14306v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Sharpness-Aware%20Pruning%20for%20Robust%20Sparse%20Networks&entry.906535625=Anna%20Bair%20and%20Hongxu%20Yin%20and%20Maying%20Shen%20and%20Pavlo%20Molchanov%20and%20Jose%20Alvarez&entry.1292438233=%20%20Robustness%20and%20compactness%20are%20two%20essential%20attributes%20of%20deep%20learning%0Amodels%20that%20are%20deployed%20in%20the%20real%20world.%20The%20goals%20of%20robustness%20and%0Acompactness%20may%20seem%20to%20be%20at%20odds%2C%20since%20robustness%20requires%20generalization%0Aacross%20domains%2C%20while%20the%20process%20of%20compression%20exploits%20specificity%20in%20one%0Adomain.%20We%20introduce%20Adaptive%20Sharpness-Aware%20Pruning%20%28AdaSAP%29%2C%20which%20unifies%0Athese%20goals%20through%20the%20lens%20of%20network%20sharpness.%20The%20AdaSAP%20method%20produces%0Asparse%20networks%20that%20are%20robust%20to%20input%20variations%20which%20are%20unseen%20at%0Atraining%20time.%20We%20achieve%20this%20by%20strategically%20incorporating%20weight%0Aperturbations%20in%20order%20to%20optimize%20the%20loss%20landscape.%20This%20allows%20the%20model%20to%0Abe%20both%20primed%20for%20pruning%20and%20regularized%20for%20improved%20robustness.%20AdaSAP%0Aimproves%20the%20robust%20accuracy%20of%20pruned%20models%20on%20image%20classification%20by%20up%20to%0A%2B6%25%20on%20ImageNet%20C%20and%20%2B4%25%20on%20ImageNet%20V2%2C%20and%20on%20object%20detection%20by%20%2B4%25%20on%20a%0Acorrupted%20Pascal%20VOC%20dataset%2C%20over%20a%20wide%20range%20of%20compression%20ratios%2C%20pruning%0Acriteria%2C%20and%20network%20architectures%2C%20outperforming%20recent%20pruning%20art%20by%20large%0Amargins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.14306v2&entry.124074799=Read"},
{"title": "Exploiting Structural Consistency of Chest Anatomy for Unsupervised\n  Anomaly Detection in Radiography Images", "author": "Tiange Xiang and Yixiao Zhang and Yongyi Lu and Alan Yuille and Chaoyi Zhang and Weidong Cai and Zongwei Zhou", "abstract": "  Radiography imaging protocols focus on particular body regions, therefore\nproducing images of great similarity and yielding recurrent anatomical\nstructures across patients. Exploiting this structured information could\npotentially ease the detection of anomalies from radiography images. To this\nend, we propose a Simple Space-Aware Memory Matrix for In-painting and\nDetecting anomalies from radiography images (abbreviated as SimSID). We\nformulate anomaly detection as an image reconstruction task, consisting of a\nspace-aware memory matrix and an in-painting block in the feature space. During\nthe training, SimSID can taxonomize the ingrained anatomical structures into\nrecurrent visual patterns, and in the inference, it can identify anomalies\n(unseen/modified visual patterns) from the test image. Our SimSID surpasses the\nstate of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9%\nAUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively.\nCode: https://github.com/MrGiovanni/SimSID\n", "link": "http://arxiv.org/abs/2403.08689v1", "date": "2024-03-13", "relevancy": 2.5214, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5108}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5102}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4918}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Structural%20Consistency%20of%20Chest%20Anatomy%20for%20Unsupervised%0A%20%20Anomaly%20Detection%20in%20Radiography%20Images&body=Title%3A%20Exploiting%20Structural%20Consistency%20of%20Chest%20Anatomy%20for%20Unsupervised%0A%20%20Anomaly%20Detection%20in%20Radiography%20Images%0AAuthor%3A%20Tiange%20Xiang%20and%20Yixiao%20Zhang%20and%20Yongyi%20Lu%20and%20Alan%20Yuille%20and%20Chaoyi%20Zhang%20and%20Weidong%20Cai%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20Radiography%20imaging%20protocols%20focus%20on%20particular%20body%20regions%2C%20therefore%0Aproducing%20images%20of%20great%20similarity%20and%20yielding%20recurrent%20anatomical%0Astructures%20across%20patients.%20Exploiting%20this%20structured%20information%20could%0Apotentially%20ease%20the%20detection%20of%20anomalies%20from%20radiography%20images.%20To%20this%0Aend%2C%20we%20propose%20a%20Simple%20Space-Aware%20Memory%20Matrix%20for%20In-painting%20and%0ADetecting%20anomalies%20from%20radiography%20images%20%28abbreviated%20as%20SimSID%29.%20We%0Aformulate%20anomaly%20detection%20as%20an%20image%20reconstruction%20task%2C%20consisting%20of%20a%0Aspace-aware%20memory%20matrix%20and%20an%20in-painting%20block%20in%20the%20feature%20space.%20During%0Athe%20training%2C%20SimSID%20can%20taxonomize%20the%20ingrained%20anatomical%20structures%20into%0Arecurrent%20visual%20patterns%2C%20and%20in%20the%20inference%2C%20it%20can%20identify%20anomalies%0A%28unseen/modified%20visual%20patterns%29%20from%20the%20test%20image.%20Our%20SimSID%20surpasses%20the%0Astate%20of%20the%20arts%20in%20unsupervised%20anomaly%20detection%20by%20%2B8.0%25%2C%20%2B5.0%25%2C%20and%20%2B9.9%25%0AAUC%20scores%20on%20ZhangLab%2C%20COVIDx%2C%20and%20CheXpert%20benchmark%20datasets%2C%20respectively.%0ACode%3A%20https%3A//github.com/MrGiovanni/SimSID%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08689v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Structural%20Consistency%20of%20Chest%20Anatomy%20for%20Unsupervised%0A%20%20Anomaly%20Detection%20in%20Radiography%20Images&entry.906535625=Tiange%20Xiang%20and%20Yixiao%20Zhang%20and%20Yongyi%20Lu%20and%20Alan%20Yuille%20and%20Chaoyi%20Zhang%20and%20Weidong%20Cai%20and%20Zongwei%20Zhou&entry.1292438233=%20%20Radiography%20imaging%20protocols%20focus%20on%20particular%20body%20regions%2C%20therefore%0Aproducing%20images%20of%20great%20similarity%20and%20yielding%20recurrent%20anatomical%0Astructures%20across%20patients.%20Exploiting%20this%20structured%20information%20could%0Apotentially%20ease%20the%20detection%20of%20anomalies%20from%20radiography%20images.%20To%20this%0Aend%2C%20we%20propose%20a%20Simple%20Space-Aware%20Memory%20Matrix%20for%20In-painting%20and%0ADetecting%20anomalies%20from%20radiography%20images%20%28abbreviated%20as%20SimSID%29.%20We%0Aformulate%20anomaly%20detection%20as%20an%20image%20reconstruction%20task%2C%20consisting%20of%20a%0Aspace-aware%20memory%20matrix%20and%20an%20in-painting%20block%20in%20the%20feature%20space.%20During%0Athe%20training%2C%20SimSID%20can%20taxonomize%20the%20ingrained%20anatomical%20structures%20into%0Arecurrent%20visual%20patterns%2C%20and%20in%20the%20inference%2C%20it%20can%20identify%20anomalies%0A%28unseen/modified%20visual%20patterns%29%20from%20the%20test%20image.%20Our%20SimSID%20surpasses%20the%0Astate%20of%20the%20arts%20in%20unsupervised%20anomaly%20detection%20by%20%2B8.0%25%2C%20%2B5.0%25%2C%20and%20%2B9.9%25%0AAUC%20scores%20on%20ZhangLab%2C%20COVIDx%2C%20and%20CheXpert%20benchmark%20datasets%2C%20respectively.%0ACode%3A%20https%3A//github.com/MrGiovanni/SimSID%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08689v1&entry.124074799=Read"},
{"title": "3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface", "author": "Linyi Jin and Nilesh Kulkarni and David Fouhey", "abstract": "  This paper introduces 3DFIRES, a novel system for scene-level 3D\nreconstruction from posed images. Designed to work with as few as one view,\n3DFIRES reconstructs the complete geometry of unseen scenes, including hidden\nsurfaces. With multiple view inputs, our method produces full reconstruction\nwithin all camera frustums. A key feature of our approach is the fusion of\nmulti-view information at the feature level, enabling the production of\ncoherent and comprehensive 3D reconstruction. We train our system on\nnon-watertight scans from large-scale real scene dataset. We show it matches\nthe efficacy of single-view reconstruction methods with only one input and\nsurpasses existing techniques in both quantitative and qualitative measures for\nsparse-view 3D reconstruction.\n", "link": "http://arxiv.org/abs/2403.08768v1", "date": "2024-03-13", "relevancy": 2.4921, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5009}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5003}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4941}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%203DFIRES%3A%20Few%20Image%203D%20REconstruction%20for%20Scenes%20with%20Hidden%20Surface&body=Title%3A%203DFIRES%3A%20Few%20Image%203D%20REconstruction%20for%20Scenes%20with%20Hidden%20Surface%0AAuthor%3A%20Linyi%20Jin%20and%20Nilesh%20Kulkarni%20and%20David%20Fouhey%0AAbstract%3A%20%20%20This%20paper%20introduces%203DFIRES%2C%20a%20novel%20system%20for%20scene-level%203D%0Areconstruction%20from%20posed%20images.%20Designed%20to%20work%20with%20as%20few%20as%20one%20view%2C%0A3DFIRES%20reconstructs%20the%20complete%20geometry%20of%20unseen%20scenes%2C%20including%20hidden%0Asurfaces.%20With%20multiple%20view%20inputs%2C%20our%20method%20produces%20full%20reconstruction%0Awithin%20all%20camera%20frustums.%20A%20key%20feature%20of%20our%20approach%20is%20the%20fusion%20of%0Amulti-view%20information%20at%20the%20feature%20level%2C%20enabling%20the%20production%20of%0Acoherent%20and%20comprehensive%203D%20reconstruction.%20We%20train%20our%20system%20on%0Anon-watertight%20scans%20from%20large-scale%20real%20scene%20dataset.%20We%20show%20it%20matches%0Athe%20efficacy%20of%20single-view%20reconstruction%20methods%20with%20only%20one%20input%20and%0Asurpasses%20existing%20techniques%20in%20both%20quantitative%20and%20qualitative%20measures%20for%0Asparse-view%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08768v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DFIRES%3A%20Few%20Image%203D%20REconstruction%20for%20Scenes%20with%20Hidden%20Surface&entry.906535625=Linyi%20Jin%20and%20Nilesh%20Kulkarni%20and%20David%20Fouhey&entry.1292438233=%20%20This%20paper%20introduces%203DFIRES%2C%20a%20novel%20system%20for%20scene-level%203D%0Areconstruction%20from%20posed%20images.%20Designed%20to%20work%20with%20as%20few%20as%20one%20view%2C%0A3DFIRES%20reconstructs%20the%20complete%20geometry%20of%20unseen%20scenes%2C%20including%20hidden%0Asurfaces.%20With%20multiple%20view%20inputs%2C%20our%20method%20produces%20full%20reconstruction%0Awithin%20all%20camera%20frustums.%20A%20key%20feature%20of%20our%20approach%20is%20the%20fusion%20of%0Amulti-view%20information%20at%20the%20feature%20level%2C%20enabling%20the%20production%20of%0Acoherent%20and%20comprehensive%203D%20reconstruction.%20We%20train%20our%20system%20on%0Anon-watertight%20scans%20from%20large-scale%20real%20scene%20dataset.%20We%20show%20it%20matches%0Athe%20efficacy%20of%20single-view%20reconstruction%20methods%20with%20only%20one%20input%20and%0Asurpasses%20existing%20techniques%20in%20both%20quantitative%20and%20qualitative%20measures%20for%0Asparse-view%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08768v1&entry.124074799=Read"},
{"title": "Resisting Backdoor Attacks in Federated Learning via Bidirectional\n  Elections and Individual Perspective", "author": "Zhen Qin and Feiyi Chen and Chen Zhi and Xueqiang Yan and Shuiguang Deng", "abstract": "  Existing approaches defend against backdoor attacks in federated learning\n(FL) mainly through a) mitigating the impact of infected models, or b)\nexcluding infected models. The former negatively impacts model accuracy, while\nthe latter usually relies on globally clear boundaries between benign and\ninfected model updates. However, model updates are easy to be mixed and\nscattered throughout in reality due to the diverse distributions of local data.\nThis work focuses on excluding infected models in FL. Unlike previous\nperspectives from a global view, we propose Snowball, a novel anti-backdoor FL\nframework through bidirectional elections from an individual perspective\ninspired by one principle deduced by us and two principles in FL and deep\nlearning. It is characterized by a) bottom-up election, where each candidate\nmodel update votes to several peer ones such that a few model updates are\nelected as selectees for aggregation; and b) top-down election, where selectees\nprogressively enlarge themselves through picking up from the candidates. We\ncompare Snowball with state-of-the-art defenses to backdoor attacks in FL on\nfive real-world datasets, demonstrating its superior resistance to backdoor\nattacks and slight impact on the accuracy of the global model.\n", "link": "http://arxiv.org/abs/2309.16456v2", "date": "2024-03-13", "relevancy": 2.4639, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5024}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4883}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4877}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Resisting%20Backdoor%20Attacks%20in%20Federated%20Learning%20via%20Bidirectional%0A%20%20Elections%20and%20Individual%20Perspective&body=Title%3A%20Resisting%20Backdoor%20Attacks%20in%20Federated%20Learning%20via%20Bidirectional%0A%20%20Elections%20and%20Individual%20Perspective%0AAuthor%3A%20Zhen%20Qin%20and%20Feiyi%20Chen%20and%20Chen%20Zhi%20and%20Xueqiang%20Yan%20and%20Shuiguang%20Deng%0AAbstract%3A%20%20%20Existing%20approaches%20defend%20against%20backdoor%20attacks%20in%20federated%20learning%0A%28FL%29%20mainly%20through%20a%29%20mitigating%20the%20impact%20of%20infected%20models%2C%20or%20b%29%0Aexcluding%20infected%20models.%20The%20former%20negatively%20impacts%20model%20accuracy%2C%20while%0Athe%20latter%20usually%20relies%20on%20globally%20clear%20boundaries%20between%20benign%20and%0Ainfected%20model%20updates.%20However%2C%20model%20updates%20are%20easy%20to%20be%20mixed%20and%0Ascattered%20throughout%20in%20reality%20due%20to%20the%20diverse%20distributions%20of%20local%20data.%0AThis%20work%20focuses%20on%20excluding%20infected%20models%20in%20FL.%20Unlike%20previous%0Aperspectives%20from%20a%20global%20view%2C%20we%20propose%20Snowball%2C%20a%20novel%20anti-backdoor%20FL%0Aframework%20through%20bidirectional%20elections%20from%20an%20individual%20perspective%0Ainspired%20by%20one%20principle%20deduced%20by%20us%20and%20two%20principles%20in%20FL%20and%20deep%0Alearning.%20It%20is%20characterized%20by%20a%29%20bottom-up%20election%2C%20where%20each%20candidate%0Amodel%20update%20votes%20to%20several%20peer%20ones%20such%20that%20a%20few%20model%20updates%20are%0Aelected%20as%20selectees%20for%20aggregation%3B%20and%20b%29%20top-down%20election%2C%20where%20selectees%0Aprogressively%20enlarge%20themselves%20through%20picking%20up%20from%20the%20candidates.%20We%0Acompare%20Snowball%20with%20state-of-the-art%20defenses%20to%20backdoor%20attacks%20in%20FL%20on%0Afive%20real-world%20datasets%2C%20demonstrating%20its%20superior%20resistance%20to%20backdoor%0Aattacks%20and%20slight%20impact%20on%20the%20accuracy%20of%20the%20global%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16456v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resisting%20Backdoor%20Attacks%20in%20Federated%20Learning%20via%20Bidirectional%0A%20%20Elections%20and%20Individual%20Perspective&entry.906535625=Zhen%20Qin%20and%20Feiyi%20Chen%20and%20Chen%20Zhi%20and%20Xueqiang%20Yan%20and%20Shuiguang%20Deng&entry.1292438233=%20%20Existing%20approaches%20defend%20against%20backdoor%20attacks%20in%20federated%20learning%0A%28FL%29%20mainly%20through%20a%29%20mitigating%20the%20impact%20of%20infected%20models%2C%20or%20b%29%0Aexcluding%20infected%20models.%20The%20former%20negatively%20impacts%20model%20accuracy%2C%20while%0Athe%20latter%20usually%20relies%20on%20globally%20clear%20boundaries%20between%20benign%20and%0Ainfected%20model%20updates.%20However%2C%20model%20updates%20are%20easy%20to%20be%20mixed%20and%0Ascattered%20throughout%20in%20reality%20due%20to%20the%20diverse%20distributions%20of%20local%20data.%0AThis%20work%20focuses%20on%20excluding%20infected%20models%20in%20FL.%20Unlike%20previous%0Aperspectives%20from%20a%20global%20view%2C%20we%20propose%20Snowball%2C%20a%20novel%20anti-backdoor%20FL%0Aframework%20through%20bidirectional%20elections%20from%20an%20individual%20perspective%0Ainspired%20by%20one%20principle%20deduced%20by%20us%20and%20two%20principles%20in%20FL%20and%20deep%0Alearning.%20It%20is%20characterized%20by%20a%29%20bottom-up%20election%2C%20where%20each%20candidate%0Amodel%20update%20votes%20to%20several%20peer%20ones%20such%20that%20a%20few%20model%20updates%20are%0Aelected%20as%20selectees%20for%20aggregation%3B%20and%20b%29%20top-down%20election%2C%20where%20selectees%0Aprogressively%20enlarge%20themselves%20through%20picking%20up%20from%20the%20candidates.%20We%0Acompare%20Snowball%20with%20state-of-the-art%20defenses%20to%20backdoor%20attacks%20in%20FL%20on%0Afive%20real-world%20datasets%2C%20demonstrating%20its%20superior%20resistance%20to%20backdoor%0Aattacks%20and%20slight%20impact%20on%20the%20accuracy%20of%20the%20global%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16456v2&entry.124074799=Read"},
{"title": "In-Context Learning Learns Label Relationships but Is Not Conventional\n  Learning", "author": "Jannik Kossen and Yarin Gal and Tom Rainforth", "abstract": "  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n", "link": "http://arxiv.org/abs/2307.12375v4", "date": "2024-03-13", "relevancy": 2.4578, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5076}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4714}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20In-Context%20Learning%20Learns%20Label%20Relationships%20but%20Is%20Not%20Conventional%0A%20%20Learning&body=Title%3A%20In-Context%20Learning%20Learns%20Label%20Relationships%20but%20Is%20Not%20Conventional%0A%20%20Learning%0AAuthor%3A%20Jannik%20Kossen%20and%20Yarin%20Gal%20and%20Tom%20Rainforth%0AAbstract%3A%20%20%20The%20predictions%20of%20Large%20Language%20Models%20%28LLMs%29%20on%20downstream%20tasks%20often%0Aimprove%20significantly%20when%20including%20examples%20of%20the%20input--label%20relationship%0Ain%20the%20context.%20However%2C%20there%20is%20currently%20no%20consensus%20about%20how%20this%0Ain-context%20learning%20%28ICL%29%20ability%20of%20LLMs%20works.%20For%20example%2C%20while%20Xie%20et%20al.%0A%282021%29%20liken%20ICL%20to%20a%20general-purpose%20learning%20algorithm%2C%20Min%20et%20al.%20%282022%29%0Aargue%20ICL%20does%20not%20even%20learn%20label%20relationships%20from%20in-context%20examples.%20In%0Athis%20paper%2C%20we%20provide%20novel%20insights%20into%20how%20ICL%20leverages%20label%20information%2C%0Arevealing%20both%20capabilities%20and%20limitations.%20To%20ensure%20we%20obtain%20a%0Acomprehensive%20picture%20of%20ICL%20behavior%2C%20we%20study%20probabilistic%20aspects%20of%20ICL%0Apredictions%20and%20thoroughly%20examine%20the%20dynamics%20of%20ICL%20as%20more%20examples%20are%0Aprovided.%20Our%20experiments%20show%20that%20ICL%20predictions%20almost%20always%20depend%20on%0Ain-context%20labels%20and%20that%20ICL%20can%20learn%20truly%20novel%20tasks%20in-context.%20However%2C%0Awe%20also%20find%20that%20ICL%20struggles%20to%20fully%20overcome%20prediction%20preferences%0Aacquired%20from%20pre-training%20data%20and%2C%20further%2C%20that%20ICL%20does%20not%20consider%20all%0Ain-context%20information%20equally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.12375v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Learning%20Learns%20Label%20Relationships%20but%20Is%20Not%20Conventional%0A%20%20Learning&entry.906535625=Jannik%20Kossen%20and%20Yarin%20Gal%20and%20Tom%20Rainforth&entry.1292438233=%20%20The%20predictions%20of%20Large%20Language%20Models%20%28LLMs%29%20on%20downstream%20tasks%20often%0Aimprove%20significantly%20when%20including%20examples%20of%20the%20input--label%20relationship%0Ain%20the%20context.%20However%2C%20there%20is%20currently%20no%20consensus%20about%20how%20this%0Ain-context%20learning%20%28ICL%29%20ability%20of%20LLMs%20works.%20For%20example%2C%20while%20Xie%20et%20al.%0A%282021%29%20liken%20ICL%20to%20a%20general-purpose%20learning%20algorithm%2C%20Min%20et%20al.%20%282022%29%0Aargue%20ICL%20does%20not%20even%20learn%20label%20relationships%20from%20in-context%20examples.%20In%0Athis%20paper%2C%20we%20provide%20novel%20insights%20into%20how%20ICL%20leverages%20label%20information%2C%0Arevealing%20both%20capabilities%20and%20limitations.%20To%20ensure%20we%20obtain%20a%0Acomprehensive%20picture%20of%20ICL%20behavior%2C%20we%20study%20probabilistic%20aspects%20of%20ICL%0Apredictions%20and%20thoroughly%20examine%20the%20dynamics%20of%20ICL%20as%20more%20examples%20are%0Aprovided.%20Our%20experiments%20show%20that%20ICL%20predictions%20almost%20always%20depend%20on%0Ain-context%20labels%20and%20that%20ICL%20can%20learn%20truly%20novel%20tasks%20in-context.%20However%2C%0Awe%20also%20find%20that%20ICL%20struggles%20to%20fully%20overcome%20prediction%20preferences%0Aacquired%20from%20pre-training%20data%20and%2C%20further%2C%20that%20ICL%20does%20not%20consider%20all%0Ain-context%20information%20equally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12375v4&entry.124074799=Read"},
{"title": "A New Quantum CNN Model for Image Classification", "author": "X. Q. Zhao and T. L. Chen", "abstract": "  Quantum density matrix represents all the information of the entire quantum\nsystem, and novel models of meaning employing density matrices naturally model\nlinguistic phenomena such as hyponymy and linguistic ambiguity, among others in\nquantum question answering tasks. Naturally, we argue that the quantum density\nmatrix can enhance the image feature information and the relationship between\nthe features for the classical image classification. Specifically, we (i)\ncombine density matrices and CNN to design a new mechanism; (ii) apply the new\nmechanism to some representative classical image classification tasks. A series\nof experiments show that the application of quantum density matrix in image\nclassification has the generalization and high efficiency on different\ndatasets. The application of quantum density matrix both in classical question\nanswering tasks and classical image classification tasks show more effective\nperformance.\n", "link": "http://arxiv.org/abs/2203.11155v5", "date": "2024-03-13", "relevancy": 2.4504, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4968}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4915}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.482}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20New%20Quantum%20CNN%20Model%20for%20Image%20Classification&body=Title%3A%20A%20New%20Quantum%20CNN%20Model%20for%20Image%20Classification%0AAuthor%3A%20X.%20Q.%20Zhao%20and%20T.%20L.%20Chen%0AAbstract%3A%20%20%20Quantum%20density%20matrix%20represents%20all%20the%20information%20of%20the%20entire%20quantum%0Asystem%2C%20and%20novel%20models%20of%20meaning%20employing%20density%20matrices%20naturally%20model%0Alinguistic%20phenomena%20such%20as%20hyponymy%20and%20linguistic%20ambiguity%2C%20among%20others%20in%0Aquantum%20question%20answering%20tasks.%20Naturally%2C%20we%20argue%20that%20the%20quantum%20density%0Amatrix%20can%20enhance%20the%20image%20feature%20information%20and%20the%20relationship%20between%0Athe%20features%20for%20the%20classical%20image%20classification.%20Specifically%2C%20we%20%28i%29%0Acombine%20density%20matrices%20and%20CNN%20to%20design%20a%20new%20mechanism%3B%20%28ii%29%20apply%20the%20new%0Amechanism%20to%20some%20representative%20classical%20image%20classification%20tasks.%20A%20series%0Aof%20experiments%20show%20that%20the%20application%20of%20quantum%20density%20matrix%20in%20image%0Aclassification%20has%20the%20generalization%20and%20high%20efficiency%20on%20different%0Adatasets.%20The%20application%20of%20quantum%20density%20matrix%20both%20in%20classical%20question%0Aanswering%20tasks%20and%20classical%20image%20classification%20tasks%20show%20more%20effective%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.11155v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Quantum%20CNN%20Model%20for%20Image%20Classification&entry.906535625=X.%20Q.%20Zhao%20and%20T.%20L.%20Chen&entry.1292438233=%20%20Quantum%20density%20matrix%20represents%20all%20the%20information%20of%20the%20entire%20quantum%0Asystem%2C%20and%20novel%20models%20of%20meaning%20employing%20density%20matrices%20naturally%20model%0Alinguistic%20phenomena%20such%20as%20hyponymy%20and%20linguistic%20ambiguity%2C%20among%20others%20in%0Aquantum%20question%20answering%20tasks.%20Naturally%2C%20we%20argue%20that%20the%20quantum%20density%0Amatrix%20can%20enhance%20the%20image%20feature%20information%20and%20the%20relationship%20between%0Athe%20features%20for%20the%20classical%20image%20classification.%20Specifically%2C%20we%20%28i%29%0Acombine%20density%20matrices%20and%20CNN%20to%20design%20a%20new%20mechanism%3B%20%28ii%29%20apply%20the%20new%0Amechanism%20to%20some%20representative%20classical%20image%20classification%20tasks.%20A%20series%0Aof%20experiments%20show%20that%20the%20application%20of%20quantum%20density%20matrix%20in%20image%0Aclassification%20has%20the%20generalization%20and%20high%20efficiency%20on%20different%0Adatasets.%20The%20application%20of%20quantum%20density%20matrix%20both%20in%20classical%20question%0Aanswering%20tasks%20and%20classical%20image%20classification%20tasks%20show%20more%20effective%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.11155v5&entry.124074799=Read"},
{"title": "A Decade's Battle on Dataset Bias: Are We There Yet?", "author": "Zhuang Liu and Kaiming He", "abstract": "  We revisit the \"dataset classification\" experiment suggested by Torralba and\nEfros a decade ago, in the new era with large-scale, diverse, and hopefully\nless biased datasets as well as more capable neural network architectures.\nSurprisingly, we observe that modern neural networks can achieve excellent\naccuracy in classifying which dataset an image is from: e.g., we report 84.7%\naccuracy on held-out validation data for the three-way classification problem\nconsisting of the YFCC, CC, and DataComp datasets. Our further experiments show\nthat such a dataset classifier could learn semantic features that are\ngeneralizable and transferable, which cannot be simply explained by\nmemorization. We hope our discovery will inspire the community to rethink the\nissue involving dataset bias and model capabilities.\n", "link": "http://arxiv.org/abs/2403.08632v1", "date": "2024-03-13", "relevancy": 2.4401, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.508}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4827}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4733}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Decade%27s%20Battle%20on%20Dataset%20Bias%3A%20Are%20We%20There%20Yet%3F&body=Title%3A%20A%20Decade%27s%20Battle%20on%20Dataset%20Bias%3A%20Are%20We%20There%20Yet%3F%0AAuthor%3A%20Zhuang%20Liu%20and%20Kaiming%20He%0AAbstract%3A%20%20%20We%20revisit%20the%20%22dataset%20classification%22%20experiment%20suggested%20by%20Torralba%20and%0AEfros%20a%20decade%20ago%2C%20in%20the%20new%20era%20with%20large-scale%2C%20diverse%2C%20and%20hopefully%0Aless%20biased%20datasets%20as%20well%20as%20more%20capable%20neural%20network%20architectures.%0ASurprisingly%2C%20we%20observe%20that%20modern%20neural%20networks%20can%20achieve%20excellent%0Aaccuracy%20in%20classifying%20which%20dataset%20an%20image%20is%20from%3A%20e.g.%2C%20we%20report%2084.7%25%0Aaccuracy%20on%20held-out%20validation%20data%20for%20the%20three-way%20classification%20problem%0Aconsisting%20of%20the%20YFCC%2C%20CC%2C%20and%20DataComp%20datasets.%20Our%20further%20experiments%20show%0Athat%20such%20a%20dataset%20classifier%20could%20learn%20semantic%20features%20that%20are%0Ageneralizable%20and%20transferable%2C%20which%20cannot%20be%20simply%20explained%20by%0Amemorization.%20We%20hope%20our%20discovery%20will%20inspire%20the%20community%20to%20rethink%20the%0Aissue%20involving%20dataset%20bias%20and%20model%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08632v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Decade%27s%20Battle%20on%20Dataset%20Bias%3A%20Are%20We%20There%20Yet%3F&entry.906535625=Zhuang%20Liu%20and%20Kaiming%20He&entry.1292438233=%20%20We%20revisit%20the%20%22dataset%20classification%22%20experiment%20suggested%20by%20Torralba%20and%0AEfros%20a%20decade%20ago%2C%20in%20the%20new%20era%20with%20large-scale%2C%20diverse%2C%20and%20hopefully%0Aless%20biased%20datasets%20as%20well%20as%20more%20capable%20neural%20network%20architectures.%0ASurprisingly%2C%20we%20observe%20that%20modern%20neural%20networks%20can%20achieve%20excellent%0Aaccuracy%20in%20classifying%20which%20dataset%20an%20image%20is%20from%3A%20e.g.%2C%20we%20report%2084.7%25%0Aaccuracy%20on%20held-out%20validation%20data%20for%20the%20three-way%20classification%20problem%0Aconsisting%20of%20the%20YFCC%2C%20CC%2C%20and%20DataComp%20datasets.%20Our%20further%20experiments%20show%0Athat%20such%20a%20dataset%20classifier%20could%20learn%20semantic%20features%20that%20are%0Ageneralizable%20and%20transferable%2C%20which%20cannot%20be%20simply%20explained%20by%0Amemorization.%20We%20hope%20our%20discovery%20will%20inspire%20the%20community%20to%20rethink%20the%0Aissue%20involving%20dataset%20bias%20and%20model%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08632v1&entry.124074799=Read"},
{"title": "Fast Dual-Regularized Autoencoder for Sparse Biological Data", "author": "Aleksandar Poleksic", "abstract": "  Relationship inference from sparse data is an important task with\napplications ranging from product recommendation to drug discovery. A recently\nproposed linear model for sparse matrix completion has demonstrated surprising\nadvantage in speed and accuracy over more sophisticated recommender systems\nalgorithms. Here we extend the linear model to develop a shallow autoencoder\nfor the dual neighborhood-regularized matrix completion problem. We demonstrate\nthe speed and accuracy advantage of our approach over the existing\nstate-of-the-art in predicting drug-target interactions and drug-disease\nassociations.\n", "link": "http://arxiv.org/abs/2401.16664v2", "date": "2024-03-13", "relevancy": 2.4319, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4954}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4868}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4769}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Fast%20Dual-Regularized%20Autoencoder%20for%20Sparse%20Biological%20Data&body=Title%3A%20Fast%20Dual-Regularized%20Autoencoder%20for%20Sparse%20Biological%20Data%0AAuthor%3A%20Aleksandar%20Poleksic%0AAbstract%3A%20%20%20Relationship%20inference%20from%20sparse%20data%20is%20an%20important%20task%20with%0Aapplications%20ranging%20from%20product%20recommendation%20to%20drug%20discovery.%20A%20recently%0Aproposed%20linear%20model%20for%20sparse%20matrix%20completion%20has%20demonstrated%20surprising%0Aadvantage%20in%20speed%20and%20accuracy%20over%20more%20sophisticated%20recommender%20systems%0Aalgorithms.%20Here%20we%20extend%20the%20linear%20model%20to%20develop%20a%20shallow%20autoencoder%0Afor%20the%20dual%20neighborhood-regularized%20matrix%20completion%20problem.%20We%20demonstrate%0Athe%20speed%20and%20accuracy%20advantage%20of%20our%20approach%20over%20the%20existing%0Astate-of-the-art%20in%20predicting%20drug-target%20interactions%20and%20drug-disease%0Aassociations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16664v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Dual-Regularized%20Autoencoder%20for%20Sparse%20Biological%20Data&entry.906535625=Aleksandar%20Poleksic&entry.1292438233=%20%20Relationship%20inference%20from%20sparse%20data%20is%20an%20important%20task%20with%0Aapplications%20ranging%20from%20product%20recommendation%20to%20drug%20discovery.%20A%20recently%0Aproposed%20linear%20model%20for%20sparse%20matrix%20completion%20has%20demonstrated%20surprising%0Aadvantage%20in%20speed%20and%20accuracy%20over%20more%20sophisticated%20recommender%20systems%0Aalgorithms.%20Here%20we%20extend%20the%20linear%20model%20to%20develop%20a%20shallow%20autoencoder%0Afor%20the%20dual%20neighborhood-regularized%20matrix%20completion%20problem.%20We%20demonstrate%0Athe%20speed%20and%20accuracy%20advantage%20of%20our%20approach%20over%20the%20existing%0Astate-of-the-art%20in%20predicting%20drug-target%20interactions%20and%20drug-disease%0Aassociations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16664v2&entry.124074799=Read"},
{"title": "Data-Efficient Sleep Staging with Synthetic Time Series Pretraining", "author": "Niklas Grieger and Siamak Mehrkanoon and Stephan Bialonski", "abstract": "  Analyzing electroencephalographic (EEG) time series can be challenging,\nespecially with deep neural networks, due to the large variability among human\nsubjects and often small datasets. To address these challenges, various\nstrategies, such as self-supervised learning, have been suggested, but they\ntypically rely on extensive empirical datasets. Inspired by recent advances in\ncomputer vision, we propose a pretraining task termed \"frequency pretraining\"\nto pretrain a neural network for sleep staging by predicting the frequency\ncontent of randomly generated synthetic time series. Our experiments\ndemonstrate that our method surpasses fully supervised learning in scenarios\nwith limited data and few subjects, and matches its performance in regimes with\nmany subjects. Furthermore, our results underline the relevance of frequency\ninformation for sleep stage scoring, while also demonstrating that deep neural\nnetworks utilize information beyond frequencies to enhance sleep staging\nperformance, which is consistent with previous research. We anticipate that our\napproach will be advantageous across a broad spectrum of applications where EEG\ndata is limited or derived from a small number of subjects, including the\ndomain of brain-computer interfaces.\n", "link": "http://arxiv.org/abs/2403.08592v1", "date": "2024-03-13", "relevancy": 2.4168, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5304}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4683}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4514}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Data-Efficient%20Sleep%20Staging%20with%20Synthetic%20Time%20Series%20Pretraining&body=Title%3A%20Data-Efficient%20Sleep%20Staging%20with%20Synthetic%20Time%20Series%20Pretraining%0AAuthor%3A%20Niklas%20Grieger%20and%20Siamak%20Mehrkanoon%20and%20Stephan%20Bialonski%0AAbstract%3A%20%20%20Analyzing%20electroencephalographic%20%28EEG%29%20time%20series%20can%20be%20challenging%2C%0Aespecially%20with%20deep%20neural%20networks%2C%20due%20to%20the%20large%20variability%20among%20human%0Asubjects%20and%20often%20small%20datasets.%20To%20address%20these%20challenges%2C%20various%0Astrategies%2C%20such%20as%20self-supervised%20learning%2C%20have%20been%20suggested%2C%20but%20they%0Atypically%20rely%20on%20extensive%20empirical%20datasets.%20Inspired%20by%20recent%20advances%20in%0Acomputer%20vision%2C%20we%20propose%20a%20pretraining%20task%20termed%20%22frequency%20pretraining%22%0Ato%20pretrain%20a%20neural%20network%20for%20sleep%20staging%20by%20predicting%20the%20frequency%0Acontent%20of%20randomly%20generated%20synthetic%20time%20series.%20Our%20experiments%0Ademonstrate%20that%20our%20method%20surpasses%20fully%20supervised%20learning%20in%20scenarios%0Awith%20limited%20data%20and%20few%20subjects%2C%20and%20matches%20its%20performance%20in%20regimes%20with%0Amany%20subjects.%20Furthermore%2C%20our%20results%20underline%20the%20relevance%20of%20frequency%0Ainformation%20for%20sleep%20stage%20scoring%2C%20while%20also%20demonstrating%20that%20deep%20neural%0Anetworks%20utilize%20information%20beyond%20frequencies%20to%20enhance%20sleep%20staging%0Aperformance%2C%20which%20is%20consistent%20with%20previous%20research.%20We%20anticipate%20that%20our%0Aapproach%20will%20be%20advantageous%20across%20a%20broad%20spectrum%20of%20applications%20where%20EEG%0Adata%20is%20limited%20or%20derived%20from%20a%20small%20number%20of%20subjects%2C%20including%20the%0Adomain%20of%20brain-computer%20interfaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08592v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Efficient%20Sleep%20Staging%20with%20Synthetic%20Time%20Series%20Pretraining&entry.906535625=Niklas%20Grieger%20and%20Siamak%20Mehrkanoon%20and%20Stephan%20Bialonski&entry.1292438233=%20%20Analyzing%20electroencephalographic%20%28EEG%29%20time%20series%20can%20be%20challenging%2C%0Aespecially%20with%20deep%20neural%20networks%2C%20due%20to%20the%20large%20variability%20among%20human%0Asubjects%20and%20often%20small%20datasets.%20To%20address%20these%20challenges%2C%20various%0Astrategies%2C%20such%20as%20self-supervised%20learning%2C%20have%20been%20suggested%2C%20but%20they%0Atypically%20rely%20on%20extensive%20empirical%20datasets.%20Inspired%20by%20recent%20advances%20in%0Acomputer%20vision%2C%20we%20propose%20a%20pretraining%20task%20termed%20%22frequency%20pretraining%22%0Ato%20pretrain%20a%20neural%20network%20for%20sleep%20staging%20by%20predicting%20the%20frequency%0Acontent%20of%20randomly%20generated%20synthetic%20time%20series.%20Our%20experiments%0Ademonstrate%20that%20our%20method%20surpasses%20fully%20supervised%20learning%20in%20scenarios%0Awith%20limited%20data%20and%20few%20subjects%2C%20and%20matches%20its%20performance%20in%20regimes%20with%0Amany%20subjects.%20Furthermore%2C%20our%20results%20underline%20the%20relevance%20of%20frequency%0Ainformation%20for%20sleep%20stage%20scoring%2C%20while%20also%20demonstrating%20that%20deep%20neural%0Anetworks%20utilize%20information%20beyond%20frequencies%20to%20enhance%20sleep%20staging%0Aperformance%2C%20which%20is%20consistent%20with%20previous%20research.%20We%20anticipate%20that%20our%0Aapproach%20will%20be%20advantageous%20across%20a%20broad%20spectrum%20of%20applications%20where%20EEG%0Adata%20is%20limited%20or%20derived%20from%20a%20small%20number%20of%20subjects%2C%20including%20the%0Adomain%20of%20brain-computer%20interfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08592v1&entry.124074799=Read"},
{"title": "DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction", "author": "Yanlong Li and Chamara Madarasingha and Kanchana Thilakarathna", "abstract": "  Point cloud streaming is increasingly getting popular, evolving into the norm\nfor interactive service delivery and the future Metaverse. However, the\nsubstantial volume of data associated with point clouds presents numerous\nchallenges, particularly in terms of high bandwidth consumption and large\nstorage capacity. Despite various solutions proposed thus far, with a focus on\npoint cloud compression, upsampling, and completion, these\nreconstruction-related methods continue to fall short in delivering high\nfidelity point cloud output. As a solution, in DiffPMAE, we propose an\neffective point cloud reconstruction architecture. Inspired by self-supervised\nlearning concepts, we combine Masked Auto-Encoding and Diffusion Model\nmechanism to remotely reconstruct point cloud data. By the nature of this\nreconstruction process, DiffPMAE can be extended to many related downstream\ntasks including point cloud compression, upsampling and completion. Leveraging\nShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the\nperformance of DiffPMAE exceeding many state-of-the-art methods in-terms of\nauto-encoding and downstream tasks considered.\n", "link": "http://arxiv.org/abs/2312.03298v2", "date": "2024-03-13", "relevancy": 2.4155, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6793}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5472}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DiffPMAE%3A%20Diffusion%20Masked%20Autoencoders%20for%20Point%20Cloud%20Reconstruction&body=Title%3A%20DiffPMAE%3A%20Diffusion%20Masked%20Autoencoders%20for%20Point%20Cloud%20Reconstruction%0AAuthor%3A%20Yanlong%20Li%20and%20Chamara%20Madarasingha%20and%20Kanchana%20Thilakarathna%0AAbstract%3A%20%20%20Point%20cloud%20streaming%20is%20increasingly%20getting%20popular%2C%20evolving%20into%20the%20norm%0Afor%20interactive%20service%20delivery%20and%20the%20future%20Metaverse.%20However%2C%20the%0Asubstantial%20volume%20of%20data%20associated%20with%20point%20clouds%20presents%20numerous%0Achallenges%2C%20particularly%20in%20terms%20of%20high%20bandwidth%20consumption%20and%20large%0Astorage%20capacity.%20Despite%20various%20solutions%20proposed%20thus%20far%2C%20with%20a%20focus%20on%0Apoint%20cloud%20compression%2C%20upsampling%2C%20and%20completion%2C%20these%0Areconstruction-related%20methods%20continue%20to%20fall%20short%20in%20delivering%20high%0Afidelity%20point%20cloud%20output.%20As%20a%20solution%2C%20in%20DiffPMAE%2C%20we%20propose%20an%0Aeffective%20point%20cloud%20reconstruction%20architecture.%20Inspired%20by%20self-supervised%0Alearning%20concepts%2C%20we%20combine%20Masked%20Auto-Encoding%20and%20Diffusion%20Model%0Amechanism%20to%20remotely%20reconstruct%20point%20cloud%20data.%20By%20the%20nature%20of%20this%0Areconstruction%20process%2C%20DiffPMAE%20can%20be%20extended%20to%20many%20related%20downstream%0Atasks%20including%20point%20cloud%20compression%2C%20upsampling%20and%20completion.%20Leveraging%0AShapeNet-55%20and%20ModelNet%20datasets%20with%20over%2060000%20objects%2C%20we%20validate%20the%0Aperformance%20of%20DiffPMAE%20exceeding%20many%20state-of-the-art%20methods%20in-terms%20of%0Aauto-encoding%20and%20downstream%20tasks%20considered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03298v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffPMAE%3A%20Diffusion%20Masked%20Autoencoders%20for%20Point%20Cloud%20Reconstruction&entry.906535625=Yanlong%20Li%20and%20Chamara%20Madarasingha%20and%20Kanchana%20Thilakarathna&entry.1292438233=%20%20Point%20cloud%20streaming%20is%20increasingly%20getting%20popular%2C%20evolving%20into%20the%20norm%0Afor%20interactive%20service%20delivery%20and%20the%20future%20Metaverse.%20However%2C%20the%0Asubstantial%20volume%20of%20data%20associated%20with%20point%20clouds%20presents%20numerous%0Achallenges%2C%20particularly%20in%20terms%20of%20high%20bandwidth%20consumption%20and%20large%0Astorage%20capacity.%20Despite%20various%20solutions%20proposed%20thus%20far%2C%20with%20a%20focus%20on%0Apoint%20cloud%20compression%2C%20upsampling%2C%20and%20completion%2C%20these%0Areconstruction-related%20methods%20continue%20to%20fall%20short%20in%20delivering%20high%0Afidelity%20point%20cloud%20output.%20As%20a%20solution%2C%20in%20DiffPMAE%2C%20we%20propose%20an%0Aeffective%20point%20cloud%20reconstruction%20architecture.%20Inspired%20by%20self-supervised%0Alearning%20concepts%2C%20we%20combine%20Masked%20Auto-Encoding%20and%20Diffusion%20Model%0Amechanism%20to%20remotely%20reconstruct%20point%20cloud%20data.%20By%20the%20nature%20of%20this%0Areconstruction%20process%2C%20DiffPMAE%20can%20be%20extended%20to%20many%20related%20downstream%0Atasks%20including%20point%20cloud%20compression%2C%20upsampling%20and%20completion.%20Leveraging%0AShapeNet-55%20and%20ModelNet%20datasets%20with%20over%2060000%20objects%2C%20we%20validate%20the%0Aperformance%20of%20DiffPMAE%20exceeding%20many%20state-of-the-art%20methods%20in-terms%20of%0Aauto-encoding%20and%20downstream%20tasks%20considered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03298v2&entry.124074799=Read"},
{"title": "Occluded Cloth-Changing Person Re-Identification", "author": "Zhihao Chen and Yiyuan Ge", "abstract": "  Cloth-changing person re-identification aims to retrieve and identify\nspe-cific pedestrians by using cloth-irrelevant features in person\ncloth-changing scenarios. However, pedestrian images captured by surveillance\nprobes usually contain occlusions in real-world scenarios. The perfor-mance of\nexisting cloth-changing re-identification methods is significantly degraded due\nto the reduction of discriminative cloth-irrelevant features caused by\nocclusion. We define cloth-changing person re-identification in occlusion\nscenarios as occluded cloth-changing person re-identification (Occ-CC-ReID),\nand to the best of our knowledge, we are the first to pro-pose occluded\ncloth-changing person re-identification as a new task. We constructed two\noccluded cloth-changing person re-identification datasets for different\nocclusion scenarios: Occluded-PRCC and Occluded-LTCC. The datasets can be\nobtained from the following link:\nhttps://github.com/1024AILab/Occluded-Cloth-Changing-Person- Re-Identification.\n", "link": "http://arxiv.org/abs/2403.08557v1", "date": "2024-03-13", "relevancy": 2.3889, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4911}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4845}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4577}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Occluded%20Cloth-Changing%20Person%20Re-Identification&body=Title%3A%20Occluded%20Cloth-Changing%20Person%20Re-Identification%0AAuthor%3A%20Zhihao%20Chen%20and%20Yiyuan%20Ge%0AAbstract%3A%20%20%20Cloth-changing%20person%20re-identification%20aims%20to%20retrieve%20and%20identify%0Aspe-cific%20pedestrians%20by%20using%20cloth-irrelevant%20features%20in%20person%0Acloth-changing%20scenarios.%20However%2C%20pedestrian%20images%20captured%20by%20surveillance%0Aprobes%20usually%20contain%20occlusions%20in%20real-world%20scenarios.%20The%20perfor-mance%20of%0Aexisting%20cloth-changing%20re-identification%20methods%20is%20significantly%20degraded%20due%0Ato%20the%20reduction%20of%20discriminative%20cloth-irrelevant%20features%20caused%20by%0Aocclusion.%20We%20define%20cloth-changing%20person%20re-identification%20in%20occlusion%0Ascenarios%20as%20occluded%20cloth-changing%20person%20re-identification%20%28Occ-CC-ReID%29%2C%0Aand%20to%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20pro-pose%20occluded%0Acloth-changing%20person%20re-identification%20as%20a%20new%20task.%20We%20constructed%20two%0Aoccluded%20cloth-changing%20person%20re-identification%20datasets%20for%20different%0Aocclusion%20scenarios%3A%20Occluded-PRCC%20and%20Occluded-LTCC.%20The%20datasets%20can%20be%0Aobtained%20from%20the%20following%20link%3A%0Ahttps%3A//github.com/1024AILab/Occluded-Cloth-Changing-Person-%20Re-Identification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08557v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occluded%20Cloth-Changing%20Person%20Re-Identification&entry.906535625=Zhihao%20Chen%20and%20Yiyuan%20Ge&entry.1292438233=%20%20Cloth-changing%20person%20re-identification%20aims%20to%20retrieve%20and%20identify%0Aspe-cific%20pedestrians%20by%20using%20cloth-irrelevant%20features%20in%20person%0Acloth-changing%20scenarios.%20However%2C%20pedestrian%20images%20captured%20by%20surveillance%0Aprobes%20usually%20contain%20occlusions%20in%20real-world%20scenarios.%20The%20perfor-mance%20of%0Aexisting%20cloth-changing%20re-identification%20methods%20is%20significantly%20degraded%20due%0Ato%20the%20reduction%20of%20discriminative%20cloth-irrelevant%20features%20caused%20by%0Aocclusion.%20We%20define%20cloth-changing%20person%20re-identification%20in%20occlusion%0Ascenarios%20as%20occluded%20cloth-changing%20person%20re-identification%20%28Occ-CC-ReID%29%2C%0Aand%20to%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20pro-pose%20occluded%0Acloth-changing%20person%20re-identification%20as%20a%20new%20task.%20We%20constructed%20two%0Aoccluded%20cloth-changing%20person%20re-identification%20datasets%20for%20different%0Aocclusion%20scenarios%3A%20Occluded-PRCC%20and%20Occluded-LTCC.%20The%20datasets%20can%20be%0Aobtained%20from%20the%20following%20link%3A%0Ahttps%3A//github.com/1024AILab/Occluded-Cloth-Changing-Person-%20Re-Identification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08557v1&entry.124074799=Read"},
{"title": "Token Alignment via Character Matching for Subword Completion", "author": "Ben Athiwaratkun and Shiqi Wang and Mingyue Shang and Yuchen Tian and Zijian Wang and Sujan Kumar Gonugondla and Sanjay Krishna Gouda and Rob Kwiatowski and Ramesh Nallapati and Bing Xiang", "abstract": "  Generative models, widely utilized in various applications, can often\nstruggle with prompts corresponding to partial tokens. This struggle stems from\ntokenization, where partial tokens fall out of distribution during inference,\nleading to incorrect or nonsensical outputs. This paper examines a technique to\nalleviate the tokenization artifact on text completion in generative models,\nmaintaining performance even in regular non-subword cases. The method, termed\ntoken alignment, involves backtracking to the last complete tokens and ensuring\nthe model's generation aligns with the prompt. This approach showcases marked\nimprovement across many partial token scenarios, including nuanced cases like\nspace-prefix and partial indentation, with only a minor time increase. The\ntechnique and analysis detailed in this paper contribute to the continuous\nadvancement of generative models in handling partial inputs, bearing relevance\nfor applications like code completion and text autocompletion.\n", "link": "http://arxiv.org/abs/2403.08688v1", "date": "2024-03-13", "relevancy": 2.3862, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5011}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4751}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4556}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Token%20Alignment%20via%20Character%20Matching%20for%20Subword%20Completion&body=Title%3A%20Token%20Alignment%20via%20Character%20Matching%20for%20Subword%20Completion%0AAuthor%3A%20Ben%20Athiwaratkun%20and%20Shiqi%20Wang%20and%20Mingyue%20Shang%20and%20Yuchen%20Tian%20and%20Zijian%20Wang%20and%20Sujan%20Kumar%20Gonugondla%20and%20Sanjay%20Krishna%20Gouda%20and%20Rob%20Kwiatowski%20and%20Ramesh%20Nallapati%20and%20Bing%20Xiang%0AAbstract%3A%20%20%20Generative%20models%2C%20widely%20utilized%20in%20various%20applications%2C%20can%20often%0Astruggle%20with%20prompts%20corresponding%20to%20partial%20tokens.%20This%20struggle%20stems%20from%0Atokenization%2C%20where%20partial%20tokens%20fall%20out%20of%20distribution%20during%20inference%2C%0Aleading%20to%20incorrect%20or%20nonsensical%20outputs.%20This%20paper%20examines%20a%20technique%20to%0Aalleviate%20the%20tokenization%20artifact%20on%20text%20completion%20in%20generative%20models%2C%0Amaintaining%20performance%20even%20in%20regular%20non-subword%20cases.%20The%20method%2C%20termed%0Atoken%20alignment%2C%20involves%20backtracking%20to%20the%20last%20complete%20tokens%20and%20ensuring%0Athe%20model%27s%20generation%20aligns%20with%20the%20prompt.%20This%20approach%20showcases%20marked%0Aimprovement%20across%20many%20partial%20token%20scenarios%2C%20including%20nuanced%20cases%20like%0Aspace-prefix%20and%20partial%20indentation%2C%20with%20only%20a%20minor%20time%20increase.%20The%0Atechnique%20and%20analysis%20detailed%20in%20this%20paper%20contribute%20to%20the%20continuous%0Aadvancement%20of%20generative%20models%20in%20handling%20partial%20inputs%2C%20bearing%20relevance%0Afor%20applications%20like%20code%20completion%20and%20text%20autocompletion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08688v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Alignment%20via%20Character%20Matching%20for%20Subword%20Completion&entry.906535625=Ben%20Athiwaratkun%20and%20Shiqi%20Wang%20and%20Mingyue%20Shang%20and%20Yuchen%20Tian%20and%20Zijian%20Wang%20and%20Sujan%20Kumar%20Gonugondla%20and%20Sanjay%20Krishna%20Gouda%20and%20Rob%20Kwiatowski%20and%20Ramesh%20Nallapati%20and%20Bing%20Xiang&entry.1292438233=%20%20Generative%20models%2C%20widely%20utilized%20in%20various%20applications%2C%20can%20often%0Astruggle%20with%20prompts%20corresponding%20to%20partial%20tokens.%20This%20struggle%20stems%20from%0Atokenization%2C%20where%20partial%20tokens%20fall%20out%20of%20distribution%20during%20inference%2C%0Aleading%20to%20incorrect%20or%20nonsensical%20outputs.%20This%20paper%20examines%20a%20technique%20to%0Aalleviate%20the%20tokenization%20artifact%20on%20text%20completion%20in%20generative%20models%2C%0Amaintaining%20performance%20even%20in%20regular%20non-subword%20cases.%20The%20method%2C%20termed%0Atoken%20alignment%2C%20involves%20backtracking%20to%20the%20last%20complete%20tokens%20and%20ensuring%0Athe%20model%27s%20generation%20aligns%20with%20the%20prompt.%20This%20approach%20showcases%20marked%0Aimprovement%20across%20many%20partial%20token%20scenarios%2C%20including%20nuanced%20cases%20like%0Aspace-prefix%20and%20partial%20indentation%2C%20with%20only%20a%20minor%20time%20increase.%20The%0Atechnique%20and%20analysis%20detailed%20in%20this%20paper%20contribute%20to%20the%20continuous%0Aadvancement%20of%20generative%20models%20in%20handling%20partial%20inputs%2C%20bearing%20relevance%0Afor%20applications%20like%20code%20completion%20and%20text%20autocompletion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08688v1&entry.124074799=Read"},
{"title": "Real-time 3D semantic occupancy prediction for autonomous vehicles using\n  memory-efficient sparse convolution", "author": "Samuel Sze and Lars Kunze", "abstract": "  In autonomous vehicles, understanding the surrounding 3D environment of the\nego vehicle in real-time is essential. A compact way to represent scenes while\nencoding geometric distances and semantic object information is via 3D semantic\noccupancy maps. State of the art 3D mapping methods leverage transformers with\ncross-attention mechanisms to elevate 2D vision-centric camera features into\nthe 3D domain. However, these methods encounter significant challenges in\nreal-time applications due to their high computational demands during\ninference. This limitation is particularly problematic in autonomous vehicles,\nwhere GPU resources must be shared with other tasks such as localization and\nplanning. In this paper, we introduce an approach that extracts features from\nfront-view 2D camera images and LiDAR scans, then employs a sparse convolution\nnetwork (Minkowski Engine), for 3D semantic occupancy prediction. Given that\noutdoor scenes in autonomous driving scenarios are inherently sparse, the\nutilization of sparse convolution is particularly apt. By jointly solving the\nproblems of 3D scene completion of sparse scenes and 3D semantic segmentation,\nwe provide a more efficient learning framework suitable for real-time\napplications in autonomous vehicles. We also demonstrate competitive accuracy\non the nuScenes dataset.\n", "link": "http://arxiv.org/abs/2403.08748v1", "date": "2024-03-13", "relevancy": 2.3778, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.613}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6076}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5738}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Real-time%203D%20semantic%20occupancy%20prediction%20for%20autonomous%20vehicles%20using%0A%20%20memory-efficient%20sparse%20convolution&body=Title%3A%20Real-time%203D%20semantic%20occupancy%20prediction%20for%20autonomous%20vehicles%20using%0A%20%20memory-efficient%20sparse%20convolution%0AAuthor%3A%20Samuel%20Sze%20and%20Lars%20Kunze%0AAbstract%3A%20%20%20In%20autonomous%20vehicles%2C%20understanding%20the%20surrounding%203D%20environment%20of%20the%0Aego%20vehicle%20in%20real-time%20is%20essential.%20A%20compact%20way%20to%20represent%20scenes%20while%0Aencoding%20geometric%20distances%20and%20semantic%20object%20information%20is%20via%203D%20semantic%0Aoccupancy%20maps.%20State%20of%20the%20art%203D%20mapping%20methods%20leverage%20transformers%20with%0Across-attention%20mechanisms%20to%20elevate%202D%20vision-centric%20camera%20features%20into%0Athe%203D%20domain.%20However%2C%20these%20methods%20encounter%20significant%20challenges%20in%0Areal-time%20applications%20due%20to%20their%20high%20computational%20demands%20during%0Ainference.%20This%20limitation%20is%20particularly%20problematic%20in%20autonomous%20vehicles%2C%0Awhere%20GPU%20resources%20must%20be%20shared%20with%20other%20tasks%20such%20as%20localization%20and%0Aplanning.%20In%20this%20paper%2C%20we%20introduce%20an%20approach%20that%20extracts%20features%20from%0Afront-view%202D%20camera%20images%20and%20LiDAR%20scans%2C%20then%20employs%20a%20sparse%20convolution%0Anetwork%20%28Minkowski%20Engine%29%2C%20for%203D%20semantic%20occupancy%20prediction.%20Given%20that%0Aoutdoor%20scenes%20in%20autonomous%20driving%20scenarios%20are%20inherently%20sparse%2C%20the%0Autilization%20of%20sparse%20convolution%20is%20particularly%20apt.%20By%20jointly%20solving%20the%0Aproblems%20of%203D%20scene%20completion%20of%20sparse%20scenes%20and%203D%20semantic%20segmentation%2C%0Awe%20provide%20a%20more%20efficient%20learning%20framework%20suitable%20for%20real-time%0Aapplications%20in%20autonomous%20vehicles.%20We%20also%20demonstrate%20competitive%20accuracy%0Aon%20the%20nuScenes%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08748v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%203D%20semantic%20occupancy%20prediction%20for%20autonomous%20vehicles%20using%0A%20%20memory-efficient%20sparse%20convolution&entry.906535625=Samuel%20Sze%20and%20Lars%20Kunze&entry.1292438233=%20%20In%20autonomous%20vehicles%2C%20understanding%20the%20surrounding%203D%20environment%20of%20the%0Aego%20vehicle%20in%20real-time%20is%20essential.%20A%20compact%20way%20to%20represent%20scenes%20while%0Aencoding%20geometric%20distances%20and%20semantic%20object%20information%20is%20via%203D%20semantic%0Aoccupancy%20maps.%20State%20of%20the%20art%203D%20mapping%20methods%20leverage%20transformers%20with%0Across-attention%20mechanisms%20to%20elevate%202D%20vision-centric%20camera%20features%20into%0Athe%203D%20domain.%20However%2C%20these%20methods%20encounter%20significant%20challenges%20in%0Areal-time%20applications%20due%20to%20their%20high%20computational%20demands%20during%0Ainference.%20This%20limitation%20is%20particularly%20problematic%20in%20autonomous%20vehicles%2C%0Awhere%20GPU%20resources%20must%20be%20shared%20with%20other%20tasks%20such%20as%20localization%20and%0Aplanning.%20In%20this%20paper%2C%20we%20introduce%20an%20approach%20that%20extracts%20features%20from%0Afront-view%202D%20camera%20images%20and%20LiDAR%20scans%2C%20then%20employs%20a%20sparse%20convolution%0Anetwork%20%28Minkowski%20Engine%29%2C%20for%203D%20semantic%20occupancy%20prediction.%20Given%20that%0Aoutdoor%20scenes%20in%20autonomous%20driving%20scenarios%20are%20inherently%20sparse%2C%20the%0Autilization%20of%20sparse%20convolution%20is%20particularly%20apt.%20By%20jointly%20solving%20the%0Aproblems%20of%203D%20scene%20completion%20of%20sparse%20scenes%20and%203D%20semantic%20segmentation%2C%0Awe%20provide%20a%20more%20efficient%20learning%20framework%20suitable%20for%20real-time%0Aapplications%20in%20autonomous%20vehicles.%20We%20also%20demonstrate%20competitive%20accuracy%0Aon%20the%20nuScenes%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08748v1&entry.124074799=Read"},
{"title": "MonoOcc: Digging into Monocular Semantic Occupancy Prediction", "author": "Yupeng Zheng and Xiang Li and Pengfei Li and Yuhang Zheng and Bu Jin and Chengliang Zhong and Xiaoxiao Long and Hao Zhao and Qichao Zhang", "abstract": "  Monocular Semantic Occupancy Prediction aims to infer the complete 3D\ngeometry and semantic information of scenes from only 2D images. It has\ngarnered significant attention, particularly due to its potential to enhance\nthe 3D perception of autonomous vehicles. However, existing methods rely on a\ncomplex cascaded framework with relatively limited information to restore 3D\nscenes, including a dependency on supervision solely on the whole network's\noutput, single-frame input, and the utilization of a small backbone. These\nchallenges, in turn, hinder the optimization of the framework and yield\ninferior prediction results, particularly concerning smaller and long-tailed\nobjects. To address these issues, we propose MonoOcc. In particular, we (i)\nimprove the monocular occupancy prediction framework by proposing an auxiliary\nsemantic loss as supervision to the shallow layers of the framework and an\nimage-conditioned cross-attention module to refine voxel features with visual\nclues, and (ii) employ a distillation module that transfers temporal\ninformation and richer knowledge from a larger image backbone to the monocular\nsemantic occupancy prediction framework with low cost of hardware. With these\nadvantages, our method yields state-of-the-art performance on the camera-based\nSemanticKITTI Scene Completion benchmark. Codes and models can be accessed at\nhttps://github.com/ucaszyp/MonoOcc\n", "link": "http://arxiv.org/abs/2403.08766v1", "date": "2024-03-13", "relevancy": 2.3728, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6378}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5949}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5736}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MonoOcc%3A%20Digging%20into%20Monocular%20Semantic%20Occupancy%20Prediction&body=Title%3A%20MonoOcc%3A%20Digging%20into%20Monocular%20Semantic%20Occupancy%20Prediction%0AAuthor%3A%20Yupeng%20Zheng%20and%20Xiang%20Li%20and%20Pengfei%20Li%20and%20Yuhang%20Zheng%20and%20Bu%20Jin%20and%20Chengliang%20Zhong%20and%20Xiaoxiao%20Long%20and%20Hao%20Zhao%20and%20Qichao%20Zhang%0AAbstract%3A%20%20%20Monocular%20Semantic%20Occupancy%20Prediction%20aims%20to%20infer%20the%20complete%203D%0Ageometry%20and%20semantic%20information%20of%20scenes%20from%20only%202D%20images.%20It%20has%0Agarnered%20significant%20attention%2C%20particularly%20due%20to%20its%20potential%20to%20enhance%0Athe%203D%20perception%20of%20autonomous%20vehicles.%20However%2C%20existing%20methods%20rely%20on%20a%0Acomplex%20cascaded%20framework%20with%20relatively%20limited%20information%20to%20restore%203D%0Ascenes%2C%20including%20a%20dependency%20on%20supervision%20solely%20on%20the%20whole%20network%27s%0Aoutput%2C%20single-frame%20input%2C%20and%20the%20utilization%20of%20a%20small%20backbone.%20These%0Achallenges%2C%20in%20turn%2C%20hinder%20the%20optimization%20of%20the%20framework%20and%20yield%0Ainferior%20prediction%20results%2C%20particularly%20concerning%20smaller%20and%20long-tailed%0Aobjects.%20To%20address%20these%20issues%2C%20we%20propose%20MonoOcc.%20In%20particular%2C%20we%20%28i%29%0Aimprove%20the%20monocular%20occupancy%20prediction%20framework%20by%20proposing%20an%20auxiliary%0Asemantic%20loss%20as%20supervision%20to%20the%20shallow%20layers%20of%20the%20framework%20and%20an%0Aimage-conditioned%20cross-attention%20module%20to%20refine%20voxel%20features%20with%20visual%0Aclues%2C%20and%20%28ii%29%20employ%20a%20distillation%20module%20that%20transfers%20temporal%0Ainformation%20and%20richer%20knowledge%20from%20a%20larger%20image%20backbone%20to%20the%20monocular%0Asemantic%20occupancy%20prediction%20framework%20with%20low%20cost%20of%20hardware.%20With%20these%0Aadvantages%2C%20our%20method%20yields%20state-of-the-art%20performance%20on%20the%20camera-based%0ASemanticKITTI%20Scene%20Completion%20benchmark.%20Codes%20and%20models%20can%20be%20accessed%20at%0Ahttps%3A//github.com/ucaszyp/MonoOcc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08766v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoOcc%3A%20Digging%20into%20Monocular%20Semantic%20Occupancy%20Prediction&entry.906535625=Yupeng%20Zheng%20and%20Xiang%20Li%20and%20Pengfei%20Li%20and%20Yuhang%20Zheng%20and%20Bu%20Jin%20and%20Chengliang%20Zhong%20and%20Xiaoxiao%20Long%20and%20Hao%20Zhao%20and%20Qichao%20Zhang&entry.1292438233=%20%20Monocular%20Semantic%20Occupancy%20Prediction%20aims%20to%20infer%20the%20complete%203D%0Ageometry%20and%20semantic%20information%20of%20scenes%20from%20only%202D%20images.%20It%20has%0Agarnered%20significant%20attention%2C%20particularly%20due%20to%20its%20potential%20to%20enhance%0Athe%203D%20perception%20of%20autonomous%20vehicles.%20However%2C%20existing%20methods%20rely%20on%20a%0Acomplex%20cascaded%20framework%20with%20relatively%20limited%20information%20to%20restore%203D%0Ascenes%2C%20including%20a%20dependency%20on%20supervision%20solely%20on%20the%20whole%20network%27s%0Aoutput%2C%20single-frame%20input%2C%20and%20the%20utilization%20of%20a%20small%20backbone.%20These%0Achallenges%2C%20in%20turn%2C%20hinder%20the%20optimization%20of%20the%20framework%20and%20yield%0Ainferior%20prediction%20results%2C%20particularly%20concerning%20smaller%20and%20long-tailed%0Aobjects.%20To%20address%20these%20issues%2C%20we%20propose%20MonoOcc.%20In%20particular%2C%20we%20%28i%29%0Aimprove%20the%20monocular%20occupancy%20prediction%20framework%20by%20proposing%20an%20auxiliary%0Asemantic%20loss%20as%20supervision%20to%20the%20shallow%20layers%20of%20the%20framework%20and%20an%0Aimage-conditioned%20cross-attention%20module%20to%20refine%20voxel%20features%20with%20visual%0Aclues%2C%20and%20%28ii%29%20employ%20a%20distillation%20module%20that%20transfers%20temporal%0Ainformation%20and%20richer%20knowledge%20from%20a%20larger%20image%20backbone%20to%20the%20monocular%0Asemantic%20occupancy%20prediction%20framework%20with%20low%20cost%20of%20hardware.%20With%20these%0Aadvantages%2C%20our%20method%20yields%20state-of-the-art%20performance%20on%20the%20camera-based%0ASemanticKITTI%20Scene%20Completion%20benchmark.%20Codes%20and%20models%20can%20be%20accessed%20at%0Ahttps%3A//github.com/ucaszyp/MonoOcc%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08766v1&entry.124074799=Read"},
{"title": "A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems", "author": "Alexandre Duval and Simon V. Mathis and Chaitanya K. Joshi and Victor Schmidt and Santiago Miret and Fragkiskos D. Malliaros and Taco Cohen and Pietro Li\u00f2 and Yoshua Bengio and Michael Bronstein", "abstract": "  Recent advances in computational modelling of atomic systems, spanning\nmolecules, proteins, and materials, represent them as geometric graphs with\natoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric\nattributes transform according to the inherent physical symmetries of 3D atomic\nsystems, including rotations and translations in Euclidean space, as well as\nnode permutations. In recent years, Geometric Graph Neural Networks have\nemerged as the preferred machine learning architecture powering applications\nranging from protein structure prediction to molecular simulations and material\ngeneration. Their specificity lies in the inductive biases they leverage - such\nas physical symmetries and chemical properties - to learn informative\nrepresentations of these geometric graphs.\n  In this opinionated paper, we provide a comprehensive and self-contained\noverview of the field of Geometric GNNs for 3D atomic systems. We cover\nfundamental background material and introduce a pedagogical taxonomy of\nGeometric GNN architectures: (1) invariant networks, (2) equivariant networks\nin Cartesian basis, (3) equivariant networks in spherical basis, and (4)\nunconstrained networks. Additionally, we outline key datasets and application\nareas and suggest future research directions. The objective of this work is to\npresent a structured perspective on the field, making it accessible to\nnewcomers and aiding practitioners in gaining an intuition for its mathematical\nabstractions.\n", "link": "http://arxiv.org/abs/2312.07511v2", "date": "2024-03-13", "relevancy": 2.3579, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5084}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4659}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4404}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Hitchhiker%27s%20Guide%20to%20Geometric%20GNNs%20for%203D%20Atomic%20Systems&body=Title%3A%20A%20Hitchhiker%27s%20Guide%20to%20Geometric%20GNNs%20for%203D%20Atomic%20Systems%0AAuthor%3A%20Alexandre%20Duval%20and%20Simon%20V.%20Mathis%20and%20Chaitanya%20K.%20Joshi%20and%20Victor%20Schmidt%20and%20Santiago%20Miret%20and%20Fragkiskos%20D.%20Malliaros%20and%20Taco%20Cohen%20and%20Pietro%20Li%C3%B2%20and%20Yoshua%20Bengio%20and%20Michael%20Bronstein%0AAbstract%3A%20%20%20Recent%20advances%20in%20computational%20modelling%20of%20atomic%20systems%2C%20spanning%0Amolecules%2C%20proteins%2C%20and%20materials%2C%20represent%20them%20as%20geometric%20graphs%20with%0Aatoms%20embedded%20as%20nodes%20in%203D%20Euclidean%20space.%20In%20these%20graphs%2C%20the%20geometric%0Aattributes%20transform%20according%20to%20the%20inherent%20physical%20symmetries%20of%203D%20atomic%0Asystems%2C%20including%20rotations%20and%20translations%20in%20Euclidean%20space%2C%20as%20well%20as%0Anode%20permutations.%20In%20recent%20years%2C%20Geometric%20Graph%20Neural%20Networks%20have%0Aemerged%20as%20the%20preferred%20machine%20learning%20architecture%20powering%20applications%0Aranging%20from%20protein%20structure%20prediction%20to%20molecular%20simulations%20and%20material%0Ageneration.%20Their%20specificity%20lies%20in%20the%20inductive%20biases%20they%20leverage%20-%20such%0Aas%20physical%20symmetries%20and%20chemical%20properties%20-%20to%20learn%20informative%0Arepresentations%20of%20these%20geometric%20graphs.%0A%20%20In%20this%20opinionated%20paper%2C%20we%20provide%20a%20comprehensive%20and%20self-contained%0Aoverview%20of%20the%20field%20of%20Geometric%20GNNs%20for%203D%20atomic%20systems.%20We%20cover%0Afundamental%20background%20material%20and%20introduce%20a%20pedagogical%20taxonomy%20of%0AGeometric%20GNN%20architectures%3A%20%281%29%20invariant%20networks%2C%20%282%29%20equivariant%20networks%0Ain%20Cartesian%20basis%2C%20%283%29%20equivariant%20networks%20in%20spherical%20basis%2C%20and%20%284%29%0Aunconstrained%20networks.%20Additionally%2C%20we%20outline%20key%20datasets%20and%20application%0Aareas%20and%20suggest%20future%20research%20directions.%20The%20objective%20of%20this%20work%20is%20to%0Apresent%20a%20structured%20perspective%20on%20the%20field%2C%20making%20it%20accessible%20to%0Anewcomers%20and%20aiding%20practitioners%20in%20gaining%20an%20intuition%20for%20its%20mathematical%0Aabstractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07511v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hitchhiker%27s%20Guide%20to%20Geometric%20GNNs%20for%203D%20Atomic%20Systems&entry.906535625=Alexandre%20Duval%20and%20Simon%20V.%20Mathis%20and%20Chaitanya%20K.%20Joshi%20and%20Victor%20Schmidt%20and%20Santiago%20Miret%20and%20Fragkiskos%20D.%20Malliaros%20and%20Taco%20Cohen%20and%20Pietro%20Li%C3%B2%20and%20Yoshua%20Bengio%20and%20Michael%20Bronstein&entry.1292438233=%20%20Recent%20advances%20in%20computational%20modelling%20of%20atomic%20systems%2C%20spanning%0Amolecules%2C%20proteins%2C%20and%20materials%2C%20represent%20them%20as%20geometric%20graphs%20with%0Aatoms%20embedded%20as%20nodes%20in%203D%20Euclidean%20space.%20In%20these%20graphs%2C%20the%20geometric%0Aattributes%20transform%20according%20to%20the%20inherent%20physical%20symmetries%20of%203D%20atomic%0Asystems%2C%20including%20rotations%20and%20translations%20in%20Euclidean%20space%2C%20as%20well%20as%0Anode%20permutations.%20In%20recent%20years%2C%20Geometric%20Graph%20Neural%20Networks%20have%0Aemerged%20as%20the%20preferred%20machine%20learning%20architecture%20powering%20applications%0Aranging%20from%20protein%20structure%20prediction%20to%20molecular%20simulations%20and%20material%0Ageneration.%20Their%20specificity%20lies%20in%20the%20inductive%20biases%20they%20leverage%20-%20such%0Aas%20physical%20symmetries%20and%20chemical%20properties%20-%20to%20learn%20informative%0Arepresentations%20of%20these%20geometric%20graphs.%0A%20%20In%20this%20opinionated%20paper%2C%20we%20provide%20a%20comprehensive%20and%20self-contained%0Aoverview%20of%20the%20field%20of%20Geometric%20GNNs%20for%203D%20atomic%20systems.%20We%20cover%0Afundamental%20background%20material%20and%20introduce%20a%20pedagogical%20taxonomy%20of%0AGeometric%20GNN%20architectures%3A%20%281%29%20invariant%20networks%2C%20%282%29%20equivariant%20networks%0Ain%20Cartesian%20basis%2C%20%283%29%20equivariant%20networks%20in%20spherical%20basis%2C%20and%20%284%29%0Aunconstrained%20networks.%20Additionally%2C%20we%20outline%20key%20datasets%20and%20application%0Aareas%20and%20suggest%20future%20research%20directions.%20The%20objective%20of%20this%20work%20is%20to%0Apresent%20a%20structured%20perspective%20on%20the%20field%2C%20making%20it%20accessible%20to%0Anewcomers%20and%20aiding%20practitioners%20in%20gaining%20an%20intuition%20for%20its%20mathematical%0Aabstractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07511v2&entry.124074799=Read"},
{"title": "Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation", "author": "Dongliang Cao and Marvin Eisenberger and Nafie El Amrani and Daniel Cremers and Florian Bernard", "abstract": "  Although 3D shape matching and interpolation are highly interrelated, they\nare often studied separately and applied sequentially to relate different 3D\nshapes, thus resulting in sub-optimal performance. In this work we present a\nunified framework to predict both point-wise correspondences and shape\ninterpolation between 3D shapes. To this end, we combine the deep functional\nmap framework with classical surface deformation models to map shapes in both\nspectral and spatial domains. On the one hand, by incorporating spatial maps,\nour method obtains more accurate and smooth point-wise correspondences compared\nto previous functional map methods for shape matching. On the other hand, by\nintroducing spectral maps, our method gets rid of commonly used but\ncomputationally expensive geodesic distance constraints that are only valid for\nnear-isometric shape deformations. Furthermore, we propose a novel test-time\nadaptation scheme to capture both pose-dominant and shape-dominant\ndeformations. Using different challenging datasets, we demonstrate that our\nmethod outperforms previous state-of-the-art methods for both shape matching\nand interpolation, even compared to supervised approaches.\n", "link": "http://arxiv.org/abs/2402.18920v4", "date": "2024-03-13", "relevancy": 2.3454, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5943}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Spectral%20Meets%20Spatial%3A%20Harmonising%203D%20Shape%20Matching%20and%20Interpolation&body=Title%3A%20Spectral%20Meets%20Spatial%3A%20Harmonising%203D%20Shape%20Matching%20and%20Interpolation%0AAuthor%3A%20Dongliang%20Cao%20and%20Marvin%20Eisenberger%20and%20Nafie%20El%20Amrani%20and%20Daniel%20Cremers%20and%20Florian%20Bernard%0AAbstract%3A%20%20%20Although%203D%20shape%20matching%20and%20interpolation%20are%20highly%20interrelated%2C%20they%0Aare%20often%20studied%20separately%20and%20applied%20sequentially%20to%20relate%20different%203D%0Ashapes%2C%20thus%20resulting%20in%20sub-optimal%20performance.%20In%20this%20work%20we%20present%20a%0Aunified%20framework%20to%20predict%20both%20point-wise%20correspondences%20and%20shape%0Ainterpolation%20between%203D%20shapes.%20To%20this%20end%2C%20we%20combine%20the%20deep%20functional%0Amap%20framework%20with%20classical%20surface%20deformation%20models%20to%20map%20shapes%20in%20both%0Aspectral%20and%20spatial%20domains.%20On%20the%20one%20hand%2C%20by%20incorporating%20spatial%20maps%2C%0Aour%20method%20obtains%20more%20accurate%20and%20smooth%20point-wise%20correspondences%20compared%0Ato%20previous%20functional%20map%20methods%20for%20shape%20matching.%20On%20the%20other%20hand%2C%20by%0Aintroducing%20spectral%20maps%2C%20our%20method%20gets%20rid%20of%20commonly%20used%20but%0Acomputationally%20expensive%20geodesic%20distance%20constraints%20that%20are%20only%20valid%20for%0Anear-isometric%20shape%20deformations.%20Furthermore%2C%20we%20propose%20a%20novel%20test-time%0Aadaptation%20scheme%20to%20capture%20both%20pose-dominant%20and%20shape-dominant%0Adeformations.%20Using%20different%20challenging%20datasets%2C%20we%20demonstrate%20that%20our%0Amethod%20outperforms%20previous%20state-of-the-art%20methods%20for%20both%20shape%20matching%0Aand%20interpolation%2C%20even%20compared%20to%20supervised%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18920v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Meets%20Spatial%3A%20Harmonising%203D%20Shape%20Matching%20and%20Interpolation&entry.906535625=Dongliang%20Cao%20and%20Marvin%20Eisenberger%20and%20Nafie%20El%20Amrani%20and%20Daniel%20Cremers%20and%20Florian%20Bernard&entry.1292438233=%20%20Although%203D%20shape%20matching%20and%20interpolation%20are%20highly%20interrelated%2C%20they%0Aare%20often%20studied%20separately%20and%20applied%20sequentially%20to%20relate%20different%203D%0Ashapes%2C%20thus%20resulting%20in%20sub-optimal%20performance.%20In%20this%20work%20we%20present%20a%0Aunified%20framework%20to%20predict%20both%20point-wise%20correspondences%20and%20shape%0Ainterpolation%20between%203D%20shapes.%20To%20this%20end%2C%20we%20combine%20the%20deep%20functional%0Amap%20framework%20with%20classical%20surface%20deformation%20models%20to%20map%20shapes%20in%20both%0Aspectral%20and%20spatial%20domains.%20On%20the%20one%20hand%2C%20by%20incorporating%20spatial%20maps%2C%0Aour%20method%20obtains%20more%20accurate%20and%20smooth%20point-wise%20correspondences%20compared%0Ato%20previous%20functional%20map%20methods%20for%20shape%20matching.%20On%20the%20other%20hand%2C%20by%0Aintroducing%20spectral%20maps%2C%20our%20method%20gets%20rid%20of%20commonly%20used%20but%0Acomputationally%20expensive%20geodesic%20distance%20constraints%20that%20are%20only%20valid%20for%0Anear-isometric%20shape%20deformations.%20Furthermore%2C%20we%20propose%20a%20novel%20test-time%0Aadaptation%20scheme%20to%20capture%20both%20pose-dominant%20and%20shape-dominant%0Adeformations.%20Using%20different%20challenging%20datasets%2C%20we%20demonstrate%20that%20our%0Amethod%20outperforms%20previous%20state-of-the-art%20methods%20for%20both%20shape%20matching%0Aand%20interpolation%2C%20even%20compared%20to%20supervised%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18920v4&entry.124074799=Read"},
{"title": "DAM: Dynamic Adapter Merging for Continual Video QA Learning", "author": "Feng Cheng and Ziyang Wang and Yi-Lin Sung and Yan-Bo Lin and Mohit Bansal and Gedas Bertasius", "abstract": "  We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM\n", "link": "http://arxiv.org/abs/2403.08755v1", "date": "2024-03-13", "relevancy": 2.3215, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6241}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5538}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5473}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DAM%3A%20Dynamic%20Adapter%20Merging%20for%20Continual%20Video%20QA%20Learning&body=Title%3A%20DAM%3A%20Dynamic%20Adapter%20Merging%20for%20Continual%20Video%20QA%20Learning%0AAuthor%3A%20Feng%20Cheng%20and%20Ziyang%20Wang%20and%20Yi-Lin%20Sung%20and%20Yan-Bo%20Lin%20and%20Mohit%20Bansal%20and%20Gedas%20Bertasius%0AAbstract%3A%20%20%20We%20present%20a%20parameter-efficient%20method%20for%20continual%20video%0Aquestion-answering%20%28VidQA%29%20learning.%20Our%20method%2C%20named%20DAM%2C%20uses%20the%20proposed%0ADynamic%20Adapter%20Merging%20to%20%28i%29%20mitigate%20catastrophic%20forgetting%2C%20%28ii%29%20enable%0Aefficient%20adaptation%20to%20continually%20arriving%20datasets%2C%20%28iii%29%20handle%20inputs%20from%0Aunknown%20datasets%20during%20inference%2C%20and%20%28iv%29%20enable%20knowledge%20sharing%20across%0Asimilar%20dataset%20domains.%20Given%20a%20set%20of%20continually%20streaming%20VidQA%20datasets%2C%0Awe%20sequentially%20train%20dataset-specific%20adapters%20for%20each%20dataset%20while%20freezing%0Athe%20parameters%20of%20a%20large%20pretrained%20video-language%20backbone.%20During%20inference%2C%0Agiven%20a%20video-question%20sample%20from%20an%20unknown%20domain%2C%20our%20method%20first%20uses%20the%0Aproposed%20non-parametric%20router%20function%20to%20compute%20a%20probability%20for%20each%0Aadapter%2C%20reflecting%20how%20relevant%20that%20adapter%20is%20to%20the%20current%20video-question%0Ainput%20instance.%20Subsequently%2C%20the%20proposed%20dynamic%20adapter%20merging%20scheme%0Aaggregates%20all%20the%20adapter%20weights%20into%20a%20new%20adapter%20instance%20tailored%20for%0Athat%20particular%20test%20sample%20to%20compute%20the%20final%20VidQA%20prediction%2C%20mitigating%0Athe%20impact%20of%20inaccurate%20router%20predictions%20and%20facilitating%20knowledge%20sharing%0Aacross%20domains.%20Our%20DAM%20model%20outperforms%20prior%20state-of-the-art%20continual%0Alearning%20approaches%20by%209.1%25%20while%20exhibiting%201.9%25%20less%20forgetting%20on%206%20VidQA%0Adatasets%20spanning%20various%20domains.%20We%20further%20extend%20DAM%20to%20continual%20image%0Aclassification%20and%20image%20QA%20and%20outperform%20prior%20methods%20by%20a%20large%20margin.%20The%0Acode%20is%20publicly%20available%20at%3A%20https%3A//github.com/klauscc/DAM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08755v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAM%3A%20Dynamic%20Adapter%20Merging%20for%20Continual%20Video%20QA%20Learning&entry.906535625=Feng%20Cheng%20and%20Ziyang%20Wang%20and%20Yi-Lin%20Sung%20and%20Yan-Bo%20Lin%20and%20Mohit%20Bansal%20and%20Gedas%20Bertasius&entry.1292438233=%20%20We%20present%20a%20parameter-efficient%20method%20for%20continual%20video%0Aquestion-answering%20%28VidQA%29%20learning.%20Our%20method%2C%20named%20DAM%2C%20uses%20the%20proposed%0ADynamic%20Adapter%20Merging%20to%20%28i%29%20mitigate%20catastrophic%20forgetting%2C%20%28ii%29%20enable%0Aefficient%20adaptation%20to%20continually%20arriving%20datasets%2C%20%28iii%29%20handle%20inputs%20from%0Aunknown%20datasets%20during%20inference%2C%20and%20%28iv%29%20enable%20knowledge%20sharing%20across%0Asimilar%20dataset%20domains.%20Given%20a%20set%20of%20continually%20streaming%20VidQA%20datasets%2C%0Awe%20sequentially%20train%20dataset-specific%20adapters%20for%20each%20dataset%20while%20freezing%0Athe%20parameters%20of%20a%20large%20pretrained%20video-language%20backbone.%20During%20inference%2C%0Agiven%20a%20video-question%20sample%20from%20an%20unknown%20domain%2C%20our%20method%20first%20uses%20the%0Aproposed%20non-parametric%20router%20function%20to%20compute%20a%20probability%20for%20each%0Aadapter%2C%20reflecting%20how%20relevant%20that%20adapter%20is%20to%20the%20current%20video-question%0Ainput%20instance.%20Subsequently%2C%20the%20proposed%20dynamic%20adapter%20merging%20scheme%0Aaggregates%20all%20the%20adapter%20weights%20into%20a%20new%20adapter%20instance%20tailored%20for%0Athat%20particular%20test%20sample%20to%20compute%20the%20final%20VidQA%20prediction%2C%20mitigating%0Athe%20impact%20of%20inaccurate%20router%20predictions%20and%20facilitating%20knowledge%20sharing%0Aacross%20domains.%20Our%20DAM%20model%20outperforms%20prior%20state-of-the-art%20continual%0Alearning%20approaches%20by%209.1%25%20while%20exhibiting%201.9%25%20less%20forgetting%20on%206%20VidQA%0Adatasets%20spanning%20various%20domains.%20We%20further%20extend%20DAM%20to%20continual%20image%0Aclassification%20and%20image%20QA%20and%20outperform%20prior%20methods%20by%20a%20large%20margin.%20The%0Acode%20is%20publicly%20available%20at%3A%20https%3A//github.com/klauscc/DAM%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08755v1&entry.124074799=Read"},
{"title": "SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple\n  Cameras and Scenes by One Model", "author": "Yihao Liu and Feng Xue and Anlong Ming", "abstract": "  The generalization of monocular metric depth estimation (MMDE) has been a\nlongstanding challenge. Recent methods made progress by combining relative and\nmetric depth or aligning input image focal length. However, they are still\nbeset by challenges in camera, scene, and data levels: (1) Sensitivity to\ndifferent cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on\nmassive training data. This paper proposes SM4Depth, a seamless MMDE method, to\naddress all the issues above within a single network. First, we reveal that a\nconsistent field of view (FOV) is the key to resolve ``metric ambiguity''\nacross cameras, which guides us to propose a more straightforward preprocessing\nunit. Second, to achieve consistently high accuracy across scenes, we\nexplicitly model the metric scale determination as discretizing the depth\ninterval into bins and propose variation-based unnormalized depth bins. This\nmethod bridges the depth gap of diverse scenes by reducing the ambiguity of the\nconventional metric bin. Third, to reduce the reliance on massive training\ndata, we propose a ``divide and conquer\" solution. Instead of estimating\ndirectly from the vast solution space, the correct metric bins are estimated\nfrom multiple solution sub-spaces for complexity reduction. Finally, with just\n150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves\nstate-of-the-art performance on most previously unseen datasets, especially\nsurpassing ZoeDepth and Metric3D on mRI$_\\theta$. The code can be found at\nhttps://github.com/1hao-Liu/SM4Depth.\n", "link": "http://arxiv.org/abs/2403.08556v1", "date": "2024-03-13", "relevancy": 2.3131, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5975}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5798}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5691}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20SM4Depth%3A%20Seamless%20Monocular%20Metric%20Depth%20Estimation%20across%20Multiple%0A%20%20Cameras%20and%20Scenes%20by%20One%20Model&body=Title%3A%20SM4Depth%3A%20Seamless%20Monocular%20Metric%20Depth%20Estimation%20across%20Multiple%0A%20%20Cameras%20and%20Scenes%20by%20One%20Model%0AAuthor%3A%20Yihao%20Liu%20and%20Feng%20Xue%20and%20Anlong%20Ming%0AAbstract%3A%20%20%20The%20generalization%20of%20monocular%20metric%20depth%20estimation%20%28MMDE%29%20has%20been%20a%0Alongstanding%20challenge.%20Recent%20methods%20made%20progress%20by%20combining%20relative%20and%0Ametric%20depth%20or%20aligning%20input%20image%20focal%20length.%20However%2C%20they%20are%20still%0Abeset%20by%20challenges%20in%20camera%2C%20scene%2C%20and%20data%20levels%3A%20%281%29%20Sensitivity%20to%0Adifferent%20cameras%3B%20%282%29%20Inconsistent%20accuracy%20across%20scenes%3B%20%283%29%20Reliance%20on%0Amassive%20training%20data.%20This%20paper%20proposes%20SM4Depth%2C%20a%20seamless%20MMDE%20method%2C%20to%0Aaddress%20all%20the%20issues%20above%20within%20a%20single%20network.%20First%2C%20we%20reveal%20that%20a%0Aconsistent%20field%20of%20view%20%28FOV%29%20is%20the%20key%20to%20resolve%20%60%60metric%20ambiguity%27%27%0Aacross%20cameras%2C%20which%20guides%20us%20to%20propose%20a%20more%20straightforward%20preprocessing%0Aunit.%20Second%2C%20to%20achieve%20consistently%20high%20accuracy%20across%20scenes%2C%20we%0Aexplicitly%20model%20the%20metric%20scale%20determination%20as%20discretizing%20the%20depth%0Ainterval%20into%20bins%20and%20propose%20variation-based%20unnormalized%20depth%20bins.%20This%0Amethod%20bridges%20the%20depth%20gap%20of%20diverse%20scenes%20by%20reducing%20the%20ambiguity%20of%20the%0Aconventional%20metric%20bin.%20Third%2C%20to%20reduce%20the%20reliance%20on%20massive%20training%0Adata%2C%20we%20propose%20a%20%60%60divide%20and%20conquer%22%20solution.%20Instead%20of%20estimating%0Adirectly%20from%20the%20vast%20solution%20space%2C%20the%20correct%20metric%20bins%20are%20estimated%0Afrom%20multiple%20solution%20sub-spaces%20for%20complexity%20reduction.%20Finally%2C%20with%20just%0A150K%20RGB-D%20pairs%20and%20a%20consumer-grade%20GPU%20for%20training%2C%20SM4Depth%20achieves%0Astate-of-the-art%20performance%20on%20most%20previously%20unseen%20datasets%2C%20especially%0Asurpassing%20ZoeDepth%20and%20Metric3D%20on%20mRI%24_%5Ctheta%24.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/1hao-Liu/SM4Depth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08556v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SM4Depth%3A%20Seamless%20Monocular%20Metric%20Depth%20Estimation%20across%20Multiple%0A%20%20Cameras%20and%20Scenes%20by%20One%20Model&entry.906535625=Yihao%20Liu%20and%20Feng%20Xue%20and%20Anlong%20Ming&entry.1292438233=%20%20The%20generalization%20of%20monocular%20metric%20depth%20estimation%20%28MMDE%29%20has%20been%20a%0Alongstanding%20challenge.%20Recent%20methods%20made%20progress%20by%20combining%20relative%20and%0Ametric%20depth%20or%20aligning%20input%20image%20focal%20length.%20However%2C%20they%20are%20still%0Abeset%20by%20challenges%20in%20camera%2C%20scene%2C%20and%20data%20levels%3A%20%281%29%20Sensitivity%20to%0Adifferent%20cameras%3B%20%282%29%20Inconsistent%20accuracy%20across%20scenes%3B%20%283%29%20Reliance%20on%0Amassive%20training%20data.%20This%20paper%20proposes%20SM4Depth%2C%20a%20seamless%20MMDE%20method%2C%20to%0Aaddress%20all%20the%20issues%20above%20within%20a%20single%20network.%20First%2C%20we%20reveal%20that%20a%0Aconsistent%20field%20of%20view%20%28FOV%29%20is%20the%20key%20to%20resolve%20%60%60metric%20ambiguity%27%27%0Aacross%20cameras%2C%20which%20guides%20us%20to%20propose%20a%20more%20straightforward%20preprocessing%0Aunit.%20Second%2C%20to%20achieve%20consistently%20high%20accuracy%20across%20scenes%2C%20we%0Aexplicitly%20model%20the%20metric%20scale%20determination%20as%20discretizing%20the%20depth%0Ainterval%20into%20bins%20and%20propose%20variation-based%20unnormalized%20depth%20bins.%20This%0Amethod%20bridges%20the%20depth%20gap%20of%20diverse%20scenes%20by%20reducing%20the%20ambiguity%20of%20the%0Aconventional%20metric%20bin.%20Third%2C%20to%20reduce%20the%20reliance%20on%20massive%20training%0Adata%2C%20we%20propose%20a%20%60%60divide%20and%20conquer%22%20solution.%20Instead%20of%20estimating%0Adirectly%20from%20the%20vast%20solution%20space%2C%20the%20correct%20metric%20bins%20are%20estimated%0Afrom%20multiple%20solution%20sub-spaces%20for%20complexity%20reduction.%20Finally%2C%20with%20just%0A150K%20RGB-D%20pairs%20and%20a%20consumer-grade%20GPU%20for%20training%2C%20SM4Depth%20achieves%0Astate-of-the-art%20performance%20on%20most%20previously%20unseen%20datasets%2C%20especially%0Asurpassing%20ZoeDepth%20and%20Metric3D%20on%20mRI%24_%5Ctheta%24.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/1hao-Liu/SM4Depth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08556v1&entry.124074799=Read"},
{"title": "Local Binary and Multiclass SVMs Trained on a Quantum Annealer", "author": "Enrico Zardini and Amer Delilbasic and Enrico Blanzieri and Gabriele Cavallaro and Davide Pastorello", "abstract": "  Support vector machines (SVMs) are widely used machine learning models (e.g.,\nin remote sensing), with formulations for both classification and regression\ntasks. In the last years, with the advent of working quantum annealers, hybrid\nSVM models characterised by quantum training and classical execution have been\nintroduced. These models have demonstrated comparable performance to their\nclassical counterparts. However, they are limited in the training set size due\nto the restricted connectivity of the current quantum annealers. Hence, to take\nadvantage of large datasets (like those related to Earth observation), a\nstrategy is required. In the classical domain, local SVMs, namely, SVMs trained\non the data samples selected by a k-nearest neighbors model, have already\nproven successful. Here, the local application of quantum-trained SVM models is\nproposed and empirically assessed. In particular, this approach allows\novercoming the constraints on the training set size of the quantum-trained\nmodels while enhancing their performance. In practice, the FaLK-SVM method,\ndesigned for efficient local SVMs, has been combined with quantum-trained SVM\nmodels for binary and multiclass classification. In addition, for comparison,\nFaLK-SVM has been interfaced for the first time with a classical single-step\nmulticlass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave's\nquantum annealers and real-world datasets taken from the remote sensing domain\nhave been employed. The results have shown the effectiveness and scalability of\nthe proposed approach, but also its practical applicability in a real-world\nlarge-scale scenario.\n", "link": "http://arxiv.org/abs/2403.08584v1", "date": "2024-03-13", "relevancy": 2.3107, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4645}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4612}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4607}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Local%20Binary%20and%20Multiclass%20SVMs%20Trained%20on%20a%20Quantum%20Annealer&body=Title%3A%20Local%20Binary%20and%20Multiclass%20SVMs%20Trained%20on%20a%20Quantum%20Annealer%0AAuthor%3A%20Enrico%20Zardini%20and%20Amer%20Delilbasic%20and%20Enrico%20Blanzieri%20and%20Gabriele%20Cavallaro%20and%20Davide%20Pastorello%0AAbstract%3A%20%20%20Support%20vector%20machines%20%28SVMs%29%20are%20widely%20used%20machine%20learning%20models%20%28e.g.%2C%0Ain%20remote%20sensing%29%2C%20with%20formulations%20for%20both%20classification%20and%20regression%0Atasks.%20In%20the%20last%20years%2C%20with%20the%20advent%20of%20working%20quantum%20annealers%2C%20hybrid%0ASVM%20models%20characterised%20by%20quantum%20training%20and%20classical%20execution%20have%20been%0Aintroduced.%20These%20models%20have%20demonstrated%20comparable%20performance%20to%20their%0Aclassical%20counterparts.%20However%2C%20they%20are%20limited%20in%20the%20training%20set%20size%20due%0Ato%20the%20restricted%20connectivity%20of%20the%20current%20quantum%20annealers.%20Hence%2C%20to%20take%0Aadvantage%20of%20large%20datasets%20%28like%20those%20related%20to%20Earth%20observation%29%2C%20a%0Astrategy%20is%20required.%20In%20the%20classical%20domain%2C%20local%20SVMs%2C%20namely%2C%20SVMs%20trained%0Aon%20the%20data%20samples%20selected%20by%20a%20k-nearest%20neighbors%20model%2C%20have%20already%0Aproven%20successful.%20Here%2C%20the%20local%20application%20of%20quantum-trained%20SVM%20models%20is%0Aproposed%20and%20empirically%20assessed.%20In%20particular%2C%20this%20approach%20allows%0Aovercoming%20the%20constraints%20on%20the%20training%20set%20size%20of%20the%20quantum-trained%0Amodels%20while%20enhancing%20their%20performance.%20In%20practice%2C%20the%20FaLK-SVM%20method%2C%0Adesigned%20for%20efficient%20local%20SVMs%2C%20has%20been%20combined%20with%20quantum-trained%20SVM%0Amodels%20for%20binary%20and%20multiclass%20classification.%20In%20addition%2C%20for%20comparison%2C%0AFaLK-SVM%20has%20been%20interfaced%20for%20the%20first%20time%20with%20a%20classical%20single-step%0Amulticlass%20SVM%20model%20%28CS%20SVM%29.%20Concerning%20the%20empirical%20evaluation%2C%20D-Wave%27s%0Aquantum%20annealers%20and%20real-world%20datasets%20taken%20from%20the%20remote%20sensing%20domain%0Ahave%20been%20employed.%20The%20results%20have%20shown%20the%20effectiveness%20and%20scalability%20of%0Athe%20proposed%20approach%2C%20but%20also%20its%20practical%20applicability%20in%20a%20real-world%0Alarge-scale%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08584v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Binary%20and%20Multiclass%20SVMs%20Trained%20on%20a%20Quantum%20Annealer&entry.906535625=Enrico%20Zardini%20and%20Amer%20Delilbasic%20and%20Enrico%20Blanzieri%20and%20Gabriele%20Cavallaro%20and%20Davide%20Pastorello&entry.1292438233=%20%20Support%20vector%20machines%20%28SVMs%29%20are%20widely%20used%20machine%20learning%20models%20%28e.g.%2C%0Ain%20remote%20sensing%29%2C%20with%20formulations%20for%20both%20classification%20and%20regression%0Atasks.%20In%20the%20last%20years%2C%20with%20the%20advent%20of%20working%20quantum%20annealers%2C%20hybrid%0ASVM%20models%20characterised%20by%20quantum%20training%20and%20classical%20execution%20have%20been%0Aintroduced.%20These%20models%20have%20demonstrated%20comparable%20performance%20to%20their%0Aclassical%20counterparts.%20However%2C%20they%20are%20limited%20in%20the%20training%20set%20size%20due%0Ato%20the%20restricted%20connectivity%20of%20the%20current%20quantum%20annealers.%20Hence%2C%20to%20take%0Aadvantage%20of%20large%20datasets%20%28like%20those%20related%20to%20Earth%20observation%29%2C%20a%0Astrategy%20is%20required.%20In%20the%20classical%20domain%2C%20local%20SVMs%2C%20namely%2C%20SVMs%20trained%0Aon%20the%20data%20samples%20selected%20by%20a%20k-nearest%20neighbors%20model%2C%20have%20already%0Aproven%20successful.%20Here%2C%20the%20local%20application%20of%20quantum-trained%20SVM%20models%20is%0Aproposed%20and%20empirically%20assessed.%20In%20particular%2C%20this%20approach%20allows%0Aovercoming%20the%20constraints%20on%20the%20training%20set%20size%20of%20the%20quantum-trained%0Amodels%20while%20enhancing%20their%20performance.%20In%20practice%2C%20the%20FaLK-SVM%20method%2C%0Adesigned%20for%20efficient%20local%20SVMs%2C%20has%20been%20combined%20with%20quantum-trained%20SVM%0Amodels%20for%20binary%20and%20multiclass%20classification.%20In%20addition%2C%20for%20comparison%2C%0AFaLK-SVM%20has%20been%20interfaced%20for%20the%20first%20time%20with%20a%20classical%20single-step%0Amulticlass%20SVM%20model%20%28CS%20SVM%29.%20Concerning%20the%20empirical%20evaluation%2C%20D-Wave%27s%0Aquantum%20annealers%20and%20real-world%20datasets%20taken%20from%20the%20remote%20sensing%20domain%0Ahave%20been%20employed.%20The%20results%20have%20shown%20the%20effectiveness%20and%20scalability%20of%0Athe%20proposed%20approach%2C%20but%20also%20its%20practical%20applicability%20in%20a%20real-world%0Alarge-scale%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08584v1&entry.124074799=Read"},
{"title": "Consistent Prompting for Rehearsal-Free Continual Learning", "author": "Zhanxin Gao and Jun Cen and Xiaobin Chang", "abstract": "  Continual learning empowers models to adapt autonomously to the ever-changing\nenvironment or data streams without forgetting old knowledge. Prompt-based\napproaches are built on frozen pre-trained models to learn the task-specific\nprompts and classifiers efficiently. Existing prompt-based methods are\ninconsistent between training and testing, limiting their effectiveness. Two\ntypes of inconsistency are revealed. Test predictions are made from all\nclassifiers while training only focuses on the current task classifier without\nholistic alignment, leading to Classifier inconsistency. Prompt inconsistency\nindicates that the prompt selected during testing may not correspond to the one\nassociated with this task during training. In this paper, we propose a novel\nprompt-based method, Consistent Prompting (CPrompt), for more aligned training\nand testing. Specifically, all existing classifiers are exposed to prompt\ntraining, resulting in classifier consistency learning. In addition, prompt\nconsistency learning is proposed to enhance prediction robustness and boost\nprompt selection accuracy. Our Consistent Prompting surpasses its prompt-based\ncounterparts and achieves state-of-the-art performance on multiple continual\nlearning benchmarks. Detailed analysis shows that improvements come from more\nconsistent training and testing.\n", "link": "http://arxiv.org/abs/2403.08568v1", "date": "2024-03-13", "relevancy": 2.271, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4596}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4585}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4445}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Consistent%20Prompting%20for%20Rehearsal-Free%20Continual%20Learning&body=Title%3A%20Consistent%20Prompting%20for%20Rehearsal-Free%20Continual%20Learning%0AAuthor%3A%20Zhanxin%20Gao%20and%20Jun%20Cen%20and%20Xiaobin%20Chang%0AAbstract%3A%20%20%20Continual%20learning%20empowers%20models%20to%20adapt%20autonomously%20to%20the%20ever-changing%0Aenvironment%20or%20data%20streams%20without%20forgetting%20old%20knowledge.%20Prompt-based%0Aapproaches%20are%20built%20on%20frozen%20pre-trained%20models%20to%20learn%20the%20task-specific%0Aprompts%20and%20classifiers%20efficiently.%20Existing%20prompt-based%20methods%20are%0Ainconsistent%20between%20training%20and%20testing%2C%20limiting%20their%20effectiveness.%20Two%0Atypes%20of%20inconsistency%20are%20revealed.%20Test%20predictions%20are%20made%20from%20all%0Aclassifiers%20while%20training%20only%20focuses%20on%20the%20current%20task%20classifier%20without%0Aholistic%20alignment%2C%20leading%20to%20Classifier%20inconsistency.%20Prompt%20inconsistency%0Aindicates%20that%20the%20prompt%20selected%20during%20testing%20may%20not%20correspond%20to%20the%20one%0Aassociated%20with%20this%20task%20during%20training.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aprompt-based%20method%2C%20Consistent%20Prompting%20%28CPrompt%29%2C%20for%20more%20aligned%20training%0Aand%20testing.%20Specifically%2C%20all%20existing%20classifiers%20are%20exposed%20to%20prompt%0Atraining%2C%20resulting%20in%20classifier%20consistency%20learning.%20In%20addition%2C%20prompt%0Aconsistency%20learning%20is%20proposed%20to%20enhance%20prediction%20robustness%20and%20boost%0Aprompt%20selection%20accuracy.%20Our%20Consistent%20Prompting%20surpasses%20its%20prompt-based%0Acounterparts%20and%20achieves%20state-of-the-art%20performance%20on%20multiple%20continual%0Alearning%20benchmarks.%20Detailed%20analysis%20shows%20that%20improvements%20come%20from%20more%0Aconsistent%20training%20and%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08568v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Prompting%20for%20Rehearsal-Free%20Continual%20Learning&entry.906535625=Zhanxin%20Gao%20and%20Jun%20Cen%20and%20Xiaobin%20Chang&entry.1292438233=%20%20Continual%20learning%20empowers%20models%20to%20adapt%20autonomously%20to%20the%20ever-changing%0Aenvironment%20or%20data%20streams%20without%20forgetting%20old%20knowledge.%20Prompt-based%0Aapproaches%20are%20built%20on%20frozen%20pre-trained%20models%20to%20learn%20the%20task-specific%0Aprompts%20and%20classifiers%20efficiently.%20Existing%20prompt-based%20methods%20are%0Ainconsistent%20between%20training%20and%20testing%2C%20limiting%20their%20effectiveness.%20Two%0Atypes%20of%20inconsistency%20are%20revealed.%20Test%20predictions%20are%20made%20from%20all%0Aclassifiers%20while%20training%20only%20focuses%20on%20the%20current%20task%20classifier%20without%0Aholistic%20alignment%2C%20leading%20to%20Classifier%20inconsistency.%20Prompt%20inconsistency%0Aindicates%20that%20the%20prompt%20selected%20during%20testing%20may%20not%20correspond%20to%20the%20one%0Aassociated%20with%20this%20task%20during%20training.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aprompt-based%20method%2C%20Consistent%20Prompting%20%28CPrompt%29%2C%20for%20more%20aligned%20training%0Aand%20testing.%20Specifically%2C%20all%20existing%20classifiers%20are%20exposed%20to%20prompt%0Atraining%2C%20resulting%20in%20classifier%20consistency%20learning.%20In%20addition%2C%20prompt%0Aconsistency%20learning%20is%20proposed%20to%20enhance%20prediction%20robustness%20and%20boost%0Aprompt%20selection%20accuracy.%20Our%20Consistent%20Prompting%20surpasses%20its%20prompt-based%0Acounterparts%20and%20achieves%20state-of-the-art%20performance%20on%20multiple%20continual%0Alearning%20benchmarks.%20Detailed%20analysis%20shows%20that%20improvements%20come%20from%20more%0Aconsistent%20training%20and%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08568v1&entry.124074799=Read"},
{"title": "Improving Implicit Regularization of SGD with Preconditioning for Least\n  Square Problems", "author": "Junwei Su and Difan Zou and Chuan Wu", "abstract": "  Stochastic gradient descent (SGD) exhibits strong algorithmic regularization\neffects in practice and plays an important role in the generalization of modern\nmachine learning. However, prior research has revealed instances where the\ngeneralization performance of SGD is worse than ridge regression due to uneven\noptimization along different dimensions. Preconditioning offers a natural\nsolution to this issue by rebalancing optimization across different directions.\nYet, the extent to which preconditioning can enhance the generalization\nperformance of SGD and whether it can bridge the existing gap with ridge\nregression remains uncertain. In this paper, we study the generalization\nperformance of SGD with preconditioning for the least squared problem. We make\na comprehensive comparison between preconditioned SGD and (standard \\&\npreconditioned) ridge regression. Our study makes several key contributions\ntoward understanding and improving SGD with preconditioning. First, we\nestablish excess risk bounds (generalization performance) for preconditioned\nSGD and ridge regression under an arbitrary preconditions matrix. Second,\nleveraging the excessive risk characterization of preconditioned SGD and ridge\nregression, we show that (through construction) there exists a simple\npreconditioned matrix that can outperform (standard \\& preconditioned) ridge\nregression. Finally, we show that our proposed preconditioning matrix is\nstraightforward enough to allow robust estimation from finite samples while\nmaintaining a theoretical advantage over ridge regression. Our empirical\nresults align with our theoretical findings, collectively showcasing the\nenhanced regularization effect of preconditioned SGD.\n", "link": "http://arxiv.org/abs/2403.08585v1", "date": "2024-03-13", "relevancy": 2.1976, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4567}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4359}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4261}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Improving%20Implicit%20Regularization%20of%20SGD%20with%20Preconditioning%20for%20Least%0A%20%20Square%20Problems&body=Title%3A%20Improving%20Implicit%20Regularization%20of%20SGD%20with%20Preconditioning%20for%20Least%0A%20%20Square%20Problems%0AAuthor%3A%20Junwei%20Su%20and%20Difan%20Zou%20and%20Chuan%20Wu%0AAbstract%3A%20%20%20Stochastic%20gradient%20descent%20%28SGD%29%20exhibits%20strong%20algorithmic%20regularization%0Aeffects%20in%20practice%20and%20plays%20an%20important%20role%20in%20the%20generalization%20of%20modern%0Amachine%20learning.%20However%2C%20prior%20research%20has%20revealed%20instances%20where%20the%0Ageneralization%20performance%20of%20SGD%20is%20worse%20than%20ridge%20regression%20due%20to%20uneven%0Aoptimization%20along%20different%20dimensions.%20Preconditioning%20offers%20a%20natural%0Asolution%20to%20this%20issue%20by%20rebalancing%20optimization%20across%20different%20directions.%0AYet%2C%20the%20extent%20to%20which%20preconditioning%20can%20enhance%20the%20generalization%0Aperformance%20of%20SGD%20and%20whether%20it%20can%20bridge%20the%20existing%20gap%20with%20ridge%0Aregression%20remains%20uncertain.%20In%20this%20paper%2C%20we%20study%20the%20generalization%0Aperformance%20of%20SGD%20with%20preconditioning%20for%20the%20least%20squared%20problem.%20We%20make%0Aa%20comprehensive%20comparison%20between%20preconditioned%20SGD%20and%20%28standard%20%5C%26%0Apreconditioned%29%20ridge%20regression.%20Our%20study%20makes%20several%20key%20contributions%0Atoward%20understanding%20and%20improving%20SGD%20with%20preconditioning.%20First%2C%20we%0Aestablish%20excess%20risk%20bounds%20%28generalization%20performance%29%20for%20preconditioned%0ASGD%20and%20ridge%20regression%20under%20an%20arbitrary%20preconditions%20matrix.%20Second%2C%0Aleveraging%20the%20excessive%20risk%20characterization%20of%20preconditioned%20SGD%20and%20ridge%0Aregression%2C%20we%20show%20that%20%28through%20construction%29%20there%20exists%20a%20simple%0Apreconditioned%20matrix%20that%20can%20outperform%20%28standard%20%5C%26%20preconditioned%29%20ridge%0Aregression.%20Finally%2C%20we%20show%20that%20our%20proposed%20preconditioning%20matrix%20is%0Astraightforward%20enough%20to%20allow%20robust%20estimation%20from%20finite%20samples%20while%0Amaintaining%20a%20theoretical%20advantage%20over%20ridge%20regression.%20Our%20empirical%0Aresults%20align%20with%20our%20theoretical%20findings%2C%20collectively%20showcasing%20the%0Aenhanced%20regularization%20effect%20of%20preconditioned%20SGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08585v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Implicit%20Regularization%20of%20SGD%20with%20Preconditioning%20for%20Least%0A%20%20Square%20Problems&entry.906535625=Junwei%20Su%20and%20Difan%20Zou%20and%20Chuan%20Wu&entry.1292438233=%20%20Stochastic%20gradient%20descent%20%28SGD%29%20exhibits%20strong%20algorithmic%20regularization%0Aeffects%20in%20practice%20and%20plays%20an%20important%20role%20in%20the%20generalization%20of%20modern%0Amachine%20learning.%20However%2C%20prior%20research%20has%20revealed%20instances%20where%20the%0Ageneralization%20performance%20of%20SGD%20is%20worse%20than%20ridge%20regression%20due%20to%20uneven%0Aoptimization%20along%20different%20dimensions.%20Preconditioning%20offers%20a%20natural%0Asolution%20to%20this%20issue%20by%20rebalancing%20optimization%20across%20different%20directions.%0AYet%2C%20the%20extent%20to%20which%20preconditioning%20can%20enhance%20the%20generalization%0Aperformance%20of%20SGD%20and%20whether%20it%20can%20bridge%20the%20existing%20gap%20with%20ridge%0Aregression%20remains%20uncertain.%20In%20this%20paper%2C%20we%20study%20the%20generalization%0Aperformance%20of%20SGD%20with%20preconditioning%20for%20the%20least%20squared%20problem.%20We%20make%0Aa%20comprehensive%20comparison%20between%20preconditioned%20SGD%20and%20%28standard%20%5C%26%0Apreconditioned%29%20ridge%20regression.%20Our%20study%20makes%20several%20key%20contributions%0Atoward%20understanding%20and%20improving%20SGD%20with%20preconditioning.%20First%2C%20we%0Aestablish%20excess%20risk%20bounds%20%28generalization%20performance%29%20for%20preconditioned%0ASGD%20and%20ridge%20regression%20under%20an%20arbitrary%20preconditions%20matrix.%20Second%2C%0Aleveraging%20the%20excessive%20risk%20characterization%20of%20preconditioned%20SGD%20and%20ridge%0Aregression%2C%20we%20show%20that%20%28through%20construction%29%20there%20exists%20a%20simple%0Apreconditioned%20matrix%20that%20can%20outperform%20%28standard%20%5C%26%20preconditioned%29%20ridge%0Aregression.%20Finally%2C%20we%20show%20that%20our%20proposed%20preconditioning%20matrix%20is%0Astraightforward%20enough%20to%20allow%20robust%20estimation%20from%20finite%20samples%20while%0Amaintaining%20a%20theoretical%20advantage%20over%20ridge%20regression.%20Our%20empirical%0Aresults%20align%20with%20our%20theoretical%20findings%2C%20collectively%20showcasing%20the%0Aenhanced%20regularization%20effect%20of%20preconditioned%20SGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08585v1&entry.124074799=Read"},
{"title": "OneVOS: Unifying Video Object Segmentation with All-in-One Transformer\n  Framework", "author": "Wanyun Li and Pinxue Guo and Xinyu Zhou and Lingyi Hong and Yangji He and Xiangyu Zheng and Wei Zhang and Wenqiang Zhang", "abstract": "  Contemporary Video Object Segmentation (VOS) approaches typically consist\nstages of feature extraction, matching, memory management, and multiple objects\naggregation. Recent advanced models either employ a discrete modeling for these\ncomponents in a sequential manner, or optimize a combined pipeline through\nsubstructure aggregation. However, these existing explicit staged approaches\nprevent the VOS framework from being optimized as a unified whole, leading to\nthe limited capacity and suboptimal performance in tackling complex videos. In\nthis paper, we propose OneVOS, a novel framework that unifies the core\ncomponents of VOS with All-in-One Transformer. Specifically, to unify all\naforementioned modules into a vision transformer, we model all the features of\nframes, masks and memory for multiple objects as transformer tokens, and\nintegrally accomplish feature extraction, matching and memory management of\nmultiple objects through the flexible attention mechanism. Furthermore, a\nUnidirectional Hybrid Attention is proposed through a double decoupling of the\noriginal attention operation, to rectify semantic errors and ambiguities of\nstored tokens in OneVOS framework. Finally, to alleviate the storage burden and\nexpedite inference, we propose the Dynamic Token Selector, which unveils the\nworking mechanism of OneVOS and naturally leads to a more efficient version of\nOneVOS. Extensive experiments demonstrate the superiority of OneVOS, achieving\nstate-of-the-art performance across 7 datasets, particularly excelling in\ncomplex LVOS and MOSE datasets with 70.1% and 66.4% $J \\& F$ scores, surpassing\nprevious state-of-the-art methods by 4.2% and 7.0%, respectively. And our code\nwill be available for reproducibility and further research.\n", "link": "http://arxiv.org/abs/2403.08682v1", "date": "2024-03-13", "relevancy": 2.1727, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5522}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.548}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20OneVOS%3A%20Unifying%20Video%20Object%20Segmentation%20with%20All-in-One%20Transformer%0A%20%20Framework&body=Title%3A%20OneVOS%3A%20Unifying%20Video%20Object%20Segmentation%20with%20All-in-One%20Transformer%0A%20%20Framework%0AAuthor%3A%20Wanyun%20Li%20and%20Pinxue%20Guo%20and%20Xinyu%20Zhou%20and%20Lingyi%20Hong%20and%20Yangji%20He%20and%20Xiangyu%20Zheng%20and%20Wei%20Zhang%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20Contemporary%20Video%20Object%20Segmentation%20%28VOS%29%20approaches%20typically%20consist%0Astages%20of%20feature%20extraction%2C%20matching%2C%20memory%20management%2C%20and%20multiple%20objects%0Aaggregation.%20Recent%20advanced%20models%20either%20employ%20a%20discrete%20modeling%20for%20these%0Acomponents%20in%20a%20sequential%20manner%2C%20or%20optimize%20a%20combined%20pipeline%20through%0Asubstructure%20aggregation.%20However%2C%20these%20existing%20explicit%20staged%20approaches%0Aprevent%20the%20VOS%20framework%20from%20being%20optimized%20as%20a%20unified%20whole%2C%20leading%20to%0Athe%20limited%20capacity%20and%20suboptimal%20performance%20in%20tackling%20complex%20videos.%20In%0Athis%20paper%2C%20we%20propose%20OneVOS%2C%20a%20novel%20framework%20that%20unifies%20the%20core%0Acomponents%20of%20VOS%20with%20All-in-One%20Transformer.%20Specifically%2C%20to%20unify%20all%0Aaforementioned%20modules%20into%20a%20vision%20transformer%2C%20we%20model%20all%20the%20features%20of%0Aframes%2C%20masks%20and%20memory%20for%20multiple%20objects%20as%20transformer%20tokens%2C%20and%0Aintegrally%20accomplish%20feature%20extraction%2C%20matching%20and%20memory%20management%20of%0Amultiple%20objects%20through%20the%20flexible%20attention%20mechanism.%20Furthermore%2C%20a%0AUnidirectional%20Hybrid%20Attention%20is%20proposed%20through%20a%20double%20decoupling%20of%20the%0Aoriginal%20attention%20operation%2C%20to%20rectify%20semantic%20errors%20and%20ambiguities%20of%0Astored%20tokens%20in%20OneVOS%20framework.%20Finally%2C%20to%20alleviate%20the%20storage%20burden%20and%0Aexpedite%20inference%2C%20we%20propose%20the%20Dynamic%20Token%20Selector%2C%20which%20unveils%20the%0Aworking%20mechanism%20of%20OneVOS%20and%20naturally%20leads%20to%20a%20more%20efficient%20version%20of%0AOneVOS.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20OneVOS%2C%20achieving%0Astate-of-the-art%20performance%20across%207%20datasets%2C%20particularly%20excelling%20in%0Acomplex%20LVOS%20and%20MOSE%20datasets%20with%2070.1%25%20and%2066.4%25%20%24J%20%5C%26%20F%24%20scores%2C%20surpassing%0Aprevious%20state-of-the-art%20methods%20by%204.2%25%20and%207.0%25%2C%20respectively.%20And%20our%20code%0Awill%20be%20available%20for%20reproducibility%20and%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08682v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneVOS%3A%20Unifying%20Video%20Object%20Segmentation%20with%20All-in-One%20Transformer%0A%20%20Framework&entry.906535625=Wanyun%20Li%20and%20Pinxue%20Guo%20and%20Xinyu%20Zhou%20and%20Lingyi%20Hong%20and%20Yangji%20He%20and%20Xiangyu%20Zheng%20and%20Wei%20Zhang%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20Contemporary%20Video%20Object%20Segmentation%20%28VOS%29%20approaches%20typically%20consist%0Astages%20of%20feature%20extraction%2C%20matching%2C%20memory%20management%2C%20and%20multiple%20objects%0Aaggregation.%20Recent%20advanced%20models%20either%20employ%20a%20discrete%20modeling%20for%20these%0Acomponents%20in%20a%20sequential%20manner%2C%20or%20optimize%20a%20combined%20pipeline%20through%0Asubstructure%20aggregation.%20However%2C%20these%20existing%20explicit%20staged%20approaches%0Aprevent%20the%20VOS%20framework%20from%20being%20optimized%20as%20a%20unified%20whole%2C%20leading%20to%0Athe%20limited%20capacity%20and%20suboptimal%20performance%20in%20tackling%20complex%20videos.%20In%0Athis%20paper%2C%20we%20propose%20OneVOS%2C%20a%20novel%20framework%20that%20unifies%20the%20core%0Acomponents%20of%20VOS%20with%20All-in-One%20Transformer.%20Specifically%2C%20to%20unify%20all%0Aaforementioned%20modules%20into%20a%20vision%20transformer%2C%20we%20model%20all%20the%20features%20of%0Aframes%2C%20masks%20and%20memory%20for%20multiple%20objects%20as%20transformer%20tokens%2C%20and%0Aintegrally%20accomplish%20feature%20extraction%2C%20matching%20and%20memory%20management%20of%0Amultiple%20objects%20through%20the%20flexible%20attention%20mechanism.%20Furthermore%2C%20a%0AUnidirectional%20Hybrid%20Attention%20is%20proposed%20through%20a%20double%20decoupling%20of%20the%0Aoriginal%20attention%20operation%2C%20to%20rectify%20semantic%20errors%20and%20ambiguities%20of%0Astored%20tokens%20in%20OneVOS%20framework.%20Finally%2C%20to%20alleviate%20the%20storage%20burden%20and%0Aexpedite%20inference%2C%20we%20propose%20the%20Dynamic%20Token%20Selector%2C%20which%20unveils%20the%0Aworking%20mechanism%20of%20OneVOS%20and%20naturally%20leads%20to%20a%20more%20efficient%20version%20of%0AOneVOS.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20OneVOS%2C%20achieving%0Astate-of-the-art%20performance%20across%207%20datasets%2C%20particularly%20excelling%20in%0Acomplex%20LVOS%20and%20MOSE%20datasets%20with%2070.1%25%20and%2066.4%25%20%24J%20%5C%26%20F%24%20scores%2C%20surpassing%0Aprevious%20state-of-the-art%20methods%20by%204.2%25%20and%207.0%25%2C%20respectively.%20And%20our%20code%0Awill%20be%20available%20for%20reproducibility%20and%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08682v1&entry.124074799=Read"},
{"title": "Data Augmentation in Human-Centric Vision", "author": "Wentao Jiang and Yige Zhang and Shaozhong Zheng and Si Liu and Shuicheng Yan", "abstract": "  This survey presents a comprehensive analysis of data augmentation techniques\nin human-centric vision tasks, a first of its kind in the field. It delves into\na wide range of research areas including person ReID, human parsing, human pose\nestimation, and pedestrian detection, addressing the significant challenges\nposed by overfitting and limited training data in these domains. Our work\ncategorizes data augmentation methods into two main types: data generation and\ndata perturbation. Data generation covers techniques like graphic engine-based\ngeneration, generative model-based generation, and data recombination, while\ndata perturbation is divided into image-level and human-level perturbations.\nEach method is tailored to the unique requirements of human-centric tasks, with\nsome applicable across multiple areas. Our contributions include an extensive\nliterature review, providing deep insights into the influence of these\naugmentation techniques in human-centric vision and highlighting the nuances of\neach method. We also discuss open issues and future directions, such as the\nintegration of advanced generative models like Latent Diffusion Models, for\ncreating more realistic and diverse training data. This survey not only\nencapsulates the current state of data augmentation in human-centric vision but\nalso charts a course for future research, aiming to develop more robust,\naccurate, and efficient human-centric vision systems.\n", "link": "http://arxiv.org/abs/2403.08650v1", "date": "2024-03-13", "relevancy": 2.1682, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.559}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.533}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5222}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Data%20Augmentation%20in%20Human-Centric%20Vision&body=Title%3A%20Data%20Augmentation%20in%20Human-Centric%20Vision%0AAuthor%3A%20Wentao%20Jiang%20and%20Yige%20Zhang%20and%20Shaozhong%20Zheng%20and%20Si%20Liu%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20This%20survey%20presents%20a%20comprehensive%20analysis%20of%20data%20augmentation%20techniques%0Ain%20human-centric%20vision%20tasks%2C%20a%20first%20of%20its%20kind%20in%20the%20field.%20It%20delves%20into%0Aa%20wide%20range%20of%20research%20areas%20including%20person%20ReID%2C%20human%20parsing%2C%20human%20pose%0Aestimation%2C%20and%20pedestrian%20detection%2C%20addressing%20the%20significant%20challenges%0Aposed%20by%20overfitting%20and%20limited%20training%20data%20in%20these%20domains.%20Our%20work%0Acategorizes%20data%20augmentation%20methods%20into%20two%20main%20types%3A%20data%20generation%20and%0Adata%20perturbation.%20Data%20generation%20covers%20techniques%20like%20graphic%20engine-based%0Ageneration%2C%20generative%20model-based%20generation%2C%20and%20data%20recombination%2C%20while%0Adata%20perturbation%20is%20divided%20into%20image-level%20and%20human-level%20perturbations.%0AEach%20method%20is%20tailored%20to%20the%20unique%20requirements%20of%20human-centric%20tasks%2C%20with%0Asome%20applicable%20across%20multiple%20areas.%20Our%20contributions%20include%20an%20extensive%0Aliterature%20review%2C%20providing%20deep%20insights%20into%20the%20influence%20of%20these%0Aaugmentation%20techniques%20in%20human-centric%20vision%20and%20highlighting%20the%20nuances%20of%0Aeach%20method.%20We%20also%20discuss%20open%20issues%20and%20future%20directions%2C%20such%20as%20the%0Aintegration%20of%20advanced%20generative%20models%20like%20Latent%20Diffusion%20Models%2C%20for%0Acreating%20more%20realistic%20and%20diverse%20training%20data.%20This%20survey%20not%20only%0Aencapsulates%20the%20current%20state%20of%20data%20augmentation%20in%20human-centric%20vision%20but%0Aalso%20charts%20a%20course%20for%20future%20research%2C%20aiming%20to%20develop%20more%20robust%2C%0Aaccurate%2C%20and%20efficient%20human-centric%20vision%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08650v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Augmentation%20in%20Human-Centric%20Vision&entry.906535625=Wentao%20Jiang%20and%20Yige%20Zhang%20and%20Shaozhong%20Zheng%20and%20Si%20Liu%20and%20Shuicheng%20Yan&entry.1292438233=%20%20This%20survey%20presents%20a%20comprehensive%20analysis%20of%20data%20augmentation%20techniques%0Ain%20human-centric%20vision%20tasks%2C%20a%20first%20of%20its%20kind%20in%20the%20field.%20It%20delves%20into%0Aa%20wide%20range%20of%20research%20areas%20including%20person%20ReID%2C%20human%20parsing%2C%20human%20pose%0Aestimation%2C%20and%20pedestrian%20detection%2C%20addressing%20the%20significant%20challenges%0Aposed%20by%20overfitting%20and%20limited%20training%20data%20in%20these%20domains.%20Our%20work%0Acategorizes%20data%20augmentation%20methods%20into%20two%20main%20types%3A%20data%20generation%20and%0Adata%20perturbation.%20Data%20generation%20covers%20techniques%20like%20graphic%20engine-based%0Ageneration%2C%20generative%20model-based%20generation%2C%20and%20data%20recombination%2C%20while%0Adata%20perturbation%20is%20divided%20into%20image-level%20and%20human-level%20perturbations.%0AEach%20method%20is%20tailored%20to%20the%20unique%20requirements%20of%20human-centric%20tasks%2C%20with%0Asome%20applicable%20across%20multiple%20areas.%20Our%20contributions%20include%20an%20extensive%0Aliterature%20review%2C%20providing%20deep%20insights%20into%20the%20influence%20of%20these%0Aaugmentation%20techniques%20in%20human-centric%20vision%20and%20highlighting%20the%20nuances%20of%0Aeach%20method.%20We%20also%20discuss%20open%20issues%20and%20future%20directions%2C%20such%20as%20the%0Aintegration%20of%20advanced%20generative%20models%20like%20Latent%20Diffusion%20Models%2C%20for%0Acreating%20more%20realistic%20and%20diverse%20training%20data.%20This%20survey%20not%20only%0Aencapsulates%20the%20current%20state%20of%20data%20augmentation%20in%20human-centric%20vision%20but%0Aalso%20charts%20a%20course%20for%20future%20research%2C%20aiming%20to%20develop%20more%20robust%2C%0Aaccurate%2C%20and%20efficient%20human-centric%20vision%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08650v1&entry.124074799=Read"},
{"title": "On the Convergence of Locally Adaptive and Scalable Diffusion-Based\n  Sampling Methods for Deep Bayesian Neural Network Posteriors", "author": "Tim Rensmeyer and Oliver Niggemann", "abstract": "  Achieving robust uncertainty quantification for deep neural networks\nrepresents an important requirement in many real-world applications of deep\nlearning such as medical imaging where it is necessary to assess the\nreliability of a neural network's prediction. Bayesian neural networks are a\npromising approach for modeling uncertainties in deep neural networks.\nUnfortunately, generating samples from the posterior distribution of neural\nnetworks is a major challenge. One significant advance in that direction would\nbe the incorporation of adaptive step sizes, similar to modern neural network\noptimizers, into Monte Carlo Markov chain sampling algorithms without\nsignificantly increasing computational demand. Over the past years, several\npapers have introduced sampling algorithms with claims that they achieve this\nproperty. However, do they indeed converge to the correct distribution? In this\npaper, we demonstrate that these methods can have a substantial bias in the\ndistribution they sample, even in the limit of vanishing step sizes and at full\nbatch size.\n", "link": "http://arxiv.org/abs/2403.08609v1", "date": "2024-03-13", "relevancy": 2.1535, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5293}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5244}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20On%20the%20Convergence%20of%20Locally%20Adaptive%20and%20Scalable%20Diffusion-Based%0A%20%20Sampling%20Methods%20for%20Deep%20Bayesian%20Neural%20Network%20Posteriors&body=Title%3A%20On%20the%20Convergence%20of%20Locally%20Adaptive%20and%20Scalable%20Diffusion-Based%0A%20%20Sampling%20Methods%20for%20Deep%20Bayesian%20Neural%20Network%20Posteriors%0AAuthor%3A%20Tim%20Rensmeyer%20and%20Oliver%20Niggemann%0AAbstract%3A%20%20%20Achieving%20robust%20uncertainty%20quantification%20for%20deep%20neural%20networks%0Arepresents%20an%20important%20requirement%20in%20many%20real-world%20applications%20of%20deep%0Alearning%20such%20as%20medical%20imaging%20where%20it%20is%20necessary%20to%20assess%20the%0Areliability%20of%20a%20neural%20network%27s%20prediction.%20Bayesian%20neural%20networks%20are%20a%0Apromising%20approach%20for%20modeling%20uncertainties%20in%20deep%20neural%20networks.%0AUnfortunately%2C%20generating%20samples%20from%20the%20posterior%20distribution%20of%20neural%0Anetworks%20is%20a%20major%20challenge.%20One%20significant%20advance%20in%20that%20direction%20would%0Abe%20the%20incorporation%20of%20adaptive%20step%20sizes%2C%20similar%20to%20modern%20neural%20network%0Aoptimizers%2C%20into%20Monte%20Carlo%20Markov%20chain%20sampling%20algorithms%20without%0Asignificantly%20increasing%20computational%20demand.%20Over%20the%20past%20years%2C%20several%0Apapers%20have%20introduced%20sampling%20algorithms%20with%20claims%20that%20they%20achieve%20this%0Aproperty.%20However%2C%20do%20they%20indeed%20converge%20to%20the%20correct%20distribution%3F%20In%20this%0Apaper%2C%20we%20demonstrate%20that%20these%20methods%20can%20have%20a%20substantial%20bias%20in%20the%0Adistribution%20they%20sample%2C%20even%20in%20the%20limit%20of%20vanishing%20step%20sizes%20and%20at%20full%0Abatch%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08609v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Convergence%20of%20Locally%20Adaptive%20and%20Scalable%20Diffusion-Based%0A%20%20Sampling%20Methods%20for%20Deep%20Bayesian%20Neural%20Network%20Posteriors&entry.906535625=Tim%20Rensmeyer%20and%20Oliver%20Niggemann&entry.1292438233=%20%20Achieving%20robust%20uncertainty%20quantification%20for%20deep%20neural%20networks%0Arepresents%20an%20important%20requirement%20in%20many%20real-world%20applications%20of%20deep%0Alearning%20such%20as%20medical%20imaging%20where%20it%20is%20necessary%20to%20assess%20the%0Areliability%20of%20a%20neural%20network%27s%20prediction.%20Bayesian%20neural%20networks%20are%20a%0Apromising%20approach%20for%20modeling%20uncertainties%20in%20deep%20neural%20networks.%0AUnfortunately%2C%20generating%20samples%20from%20the%20posterior%20distribution%20of%20neural%0Anetworks%20is%20a%20major%20challenge.%20One%20significant%20advance%20in%20that%20direction%20would%0Abe%20the%20incorporation%20of%20adaptive%20step%20sizes%2C%20similar%20to%20modern%20neural%20network%0Aoptimizers%2C%20into%20Monte%20Carlo%20Markov%20chain%20sampling%20algorithms%20without%0Asignificantly%20increasing%20computational%20demand.%20Over%20the%20past%20years%2C%20several%0Apapers%20have%20introduced%20sampling%20algorithms%20with%20claims%20that%20they%20achieve%20this%0Aproperty.%20However%2C%20do%20they%20indeed%20converge%20to%20the%20correct%20distribution%3F%20In%20this%0Apaper%2C%20we%20demonstrate%20that%20these%20methods%20can%20have%20a%20substantial%20bias%20in%20the%0Adistribution%20they%20sample%2C%20even%20in%20the%20limit%20of%20vanishing%20step%20sizes%20and%20at%20full%0Abatch%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08609v1&entry.124074799=Read"},
{"title": "ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning\n  in Instructional Videos", "author": "Lei Shi and Paul B\u00fcrkner and Andreas Bulling", "abstract": "  We present ActionDiffusion -- a novel diffusion model for procedure planning\nin instructional videos that is the first to take temporal inter-dependencies\nbetween actions into account in a diffusion model for procedure planning. This\napproach is in stark contrast to existing methods that fail to exploit the rich\ninformation content available in the particular order in which actions are\nperformed. Our method unifies the learning of temporal dependencies between\nactions and denoising of the action plan in the diffusion process by projecting\nthe action information into the noise space. This is achieved 1) by adding\naction embeddings in the noise masks in the noise-adding phase and 2) by\nintroducing an attention mechanism in the noise prediction network to learn the\ncorrelations between different action steps. We report extensive experiments on\nthree instructional video benchmark datasets (CrossTask, Coin, and NIV) and\nshow that our method outperforms previous state-of-the-art methods on all\nmetrics on CrossTask and NIV and all metrics except accuracy on Coin dataset.\nWe show that by adding action embeddings into the noise mask the diffusion\nmodel can better learn action temporal dependencies and increase the\nperformances on procedure planning.\n", "link": "http://arxiv.org/abs/2403.08591v1", "date": "2024-03-13", "relevancy": 2.1346, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.611}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5214}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.515}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20ActionDiffusion%3A%20An%20Action-aware%20Diffusion%20Model%20for%20Procedure%20Planning%0A%20%20in%20Instructional%20Videos&body=Title%3A%20ActionDiffusion%3A%20An%20Action-aware%20Diffusion%20Model%20for%20Procedure%20Planning%0A%20%20in%20Instructional%20Videos%0AAuthor%3A%20Lei%20Shi%20and%20Paul%20B%C3%BCrkner%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20We%20present%20ActionDiffusion%20--%20a%20novel%20diffusion%20model%20for%20procedure%20planning%0Ain%20instructional%20videos%20that%20is%20the%20first%20to%20take%20temporal%20inter-dependencies%0Abetween%20actions%20into%20account%20in%20a%20diffusion%20model%20for%20procedure%20planning.%20This%0Aapproach%20is%20in%20stark%20contrast%20to%20existing%20methods%20that%20fail%20to%20exploit%20the%20rich%0Ainformation%20content%20available%20in%20the%20particular%20order%20in%20which%20actions%20are%0Aperformed.%20Our%20method%20unifies%20the%20learning%20of%20temporal%20dependencies%20between%0Aactions%20and%20denoising%20of%20the%20action%20plan%20in%20the%20diffusion%20process%20by%20projecting%0Athe%20action%20information%20into%20the%20noise%20space.%20This%20is%20achieved%201%29%20by%20adding%0Aaction%20embeddings%20in%20the%20noise%20masks%20in%20the%20noise-adding%20phase%20and%202%29%20by%0Aintroducing%20an%20attention%20mechanism%20in%20the%20noise%20prediction%20network%20to%20learn%20the%0Acorrelations%20between%20different%20action%20steps.%20We%20report%20extensive%20experiments%20on%0Athree%20instructional%20video%20benchmark%20datasets%20%28CrossTask%2C%20Coin%2C%20and%20NIV%29%20and%0Ashow%20that%20our%20method%20outperforms%20previous%20state-of-the-art%20methods%20on%20all%0Ametrics%20on%20CrossTask%20and%20NIV%20and%20all%20metrics%20except%20accuracy%20on%20Coin%20dataset.%0AWe%20show%20that%20by%20adding%20action%20embeddings%20into%20the%20noise%20mask%20the%20diffusion%0Amodel%20can%20better%20learn%20action%20temporal%20dependencies%20and%20increase%20the%0Aperformances%20on%20procedure%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08591v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActionDiffusion%3A%20An%20Action-aware%20Diffusion%20Model%20for%20Procedure%20Planning%0A%20%20in%20Instructional%20Videos&entry.906535625=Lei%20Shi%20and%20Paul%20B%C3%BCrkner%20and%20Andreas%20Bulling&entry.1292438233=%20%20We%20present%20ActionDiffusion%20--%20a%20novel%20diffusion%20model%20for%20procedure%20planning%0Ain%20instructional%20videos%20that%20is%20the%20first%20to%20take%20temporal%20inter-dependencies%0Abetween%20actions%20into%20account%20in%20a%20diffusion%20model%20for%20procedure%20planning.%20This%0Aapproach%20is%20in%20stark%20contrast%20to%20existing%20methods%20that%20fail%20to%20exploit%20the%20rich%0Ainformation%20content%20available%20in%20the%20particular%20order%20in%20which%20actions%20are%0Aperformed.%20Our%20method%20unifies%20the%20learning%20of%20temporal%20dependencies%20between%0Aactions%20and%20denoising%20of%20the%20action%20plan%20in%20the%20diffusion%20process%20by%20projecting%0Athe%20action%20information%20into%20the%20noise%20space.%20This%20is%20achieved%201%29%20by%20adding%0Aaction%20embeddings%20in%20the%20noise%20masks%20in%20the%20noise-adding%20phase%20and%202%29%20by%0Aintroducing%20an%20attention%20mechanism%20in%20the%20noise%20prediction%20network%20to%20learn%20the%0Acorrelations%20between%20different%20action%20steps.%20We%20report%20extensive%20experiments%20on%0Athree%20instructional%20video%20benchmark%20datasets%20%28CrossTask%2C%20Coin%2C%20and%20NIV%29%20and%0Ashow%20that%20our%20method%20outperforms%20previous%20state-of-the-art%20methods%20on%20all%0Ametrics%20on%20CrossTask%20and%20NIV%20and%20all%20metrics%20except%20accuracy%20on%20Coin%20dataset.%0AWe%20show%20that%20by%20adding%20action%20embeddings%20into%20the%20noise%20mask%20the%20diffusion%0Amodel%20can%20better%20learn%20action%20temporal%20dependencies%20and%20increase%20the%0Aperformances%20on%20procedure%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08591v1&entry.124074799=Read"},
{"title": "A Novel Implicit Neural Representation for Volume Data", "author": "Armin Sheibanifard and Hongchuan Yu", "abstract": "  The storage of medical images is one of the challenges in the medical imaging\nfield. There are variable works that use implicit neural representation (INR)\nto compress volumetric medical images. However, there is room to improve the\ncompression rate for volumetric medical images. Most of the INR techniques need\na huge amount of GPU memory and a long training time for high-quality medical\nvolume rendering. In this paper, we present a novel implicit neural\nrepresentation to compress volume data using our proposed architecture, that\nis, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet\nhigh-resolution scheme. Our architecture can effectively reduce training time,\nand gain a high compression rate while retaining the final rendering quality.\nMoreover, it can save GPU memory in comparison with the existing works. The\nexperiments show that the quality of reconstructed images and training speed\nusing our architecture is higher than current works which use the SIREN only.\nBesides, the GPU memory cost is evidently decreased\n", "link": "http://arxiv.org/abs/2403.08566v1", "date": "2024-03-13", "relevancy": 2.1295, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5567}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5286}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5096}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Implicit%20Neural%20Representation%20for%20Volume%20Data&body=Title%3A%20A%20Novel%20Implicit%20Neural%20Representation%20for%20Volume%20Data%0AAuthor%3A%20Armin%20Sheibanifard%20and%20Hongchuan%20Yu%0AAbstract%3A%20%20%20The%20storage%20of%20medical%20images%20is%20one%20of%20the%20challenges%20in%20the%20medical%20imaging%0Afield.%20There%20are%20variable%20works%20that%20use%20implicit%20neural%20representation%20%28INR%29%0Ato%20compress%20volumetric%20medical%20images.%20However%2C%20there%20is%20room%20to%20improve%20the%0Acompression%20rate%20for%20volumetric%20medical%20images.%20Most%20of%20the%20INR%20techniques%20need%0Aa%20huge%20amount%20of%20GPU%20memory%20and%20a%20long%20training%20time%20for%20high-quality%20medical%0Avolume%20rendering.%20In%20this%20paper%2C%20we%20present%20a%20novel%20implicit%20neural%0Arepresentation%20to%20compress%20volume%20data%20using%20our%20proposed%20architecture%2C%20that%0Ais%2C%20the%20Lanczos%20downsampling%20scheme%2C%20SIREN%20deep%20network%2C%20and%20SRDenseNet%0Ahigh-resolution%20scheme.%20Our%20architecture%20can%20effectively%20reduce%20training%20time%2C%0Aand%20gain%20a%20high%20compression%20rate%20while%20retaining%20the%20final%20rendering%20quality.%0AMoreover%2C%20it%20can%20save%20GPU%20memory%20in%20comparison%20with%20the%20existing%20works.%20The%0Aexperiments%20show%20that%20the%20quality%20of%20reconstructed%20images%20and%20training%20speed%0Ausing%20our%20architecture%20is%20higher%20than%20current%20works%20which%20use%20the%20SIREN%20only.%0ABesides%2C%20the%20GPU%20memory%20cost%20is%20evidently%20decreased%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08566v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Implicit%20Neural%20Representation%20for%20Volume%20Data&entry.906535625=Armin%20Sheibanifard%20and%20Hongchuan%20Yu&entry.1292438233=%20%20The%20storage%20of%20medical%20images%20is%20one%20of%20the%20challenges%20in%20the%20medical%20imaging%0Afield.%20There%20are%20variable%20works%20that%20use%20implicit%20neural%20representation%20%28INR%29%0Ato%20compress%20volumetric%20medical%20images.%20However%2C%20there%20is%20room%20to%20improve%20the%0Acompression%20rate%20for%20volumetric%20medical%20images.%20Most%20of%20the%20INR%20techniques%20need%0Aa%20huge%20amount%20of%20GPU%20memory%20and%20a%20long%20training%20time%20for%20high-quality%20medical%0Avolume%20rendering.%20In%20this%20paper%2C%20we%20present%20a%20novel%20implicit%20neural%0Arepresentation%20to%20compress%20volume%20data%20using%20our%20proposed%20architecture%2C%20that%0Ais%2C%20the%20Lanczos%20downsampling%20scheme%2C%20SIREN%20deep%20network%2C%20and%20SRDenseNet%0Ahigh-resolution%20scheme.%20Our%20architecture%20can%20effectively%20reduce%20training%20time%2C%0Aand%20gain%20a%20high%20compression%20rate%20while%20retaining%20the%20final%20rendering%20quality.%0AMoreover%2C%20it%20can%20save%20GPU%20memory%20in%20comparison%20with%20the%20existing%20works.%20The%0Aexperiments%20show%20that%20the%20quality%20of%20reconstructed%20images%20and%20training%20speed%0Ausing%20our%20architecture%20is%20higher%20than%20current%20works%20which%20use%20the%20SIREN%20only.%0ABesides%2C%20the%20GPU%20memory%20cost%20is%20evidently%20decreased%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08566v1&entry.124074799=Read"},
{"title": "Strengthening Multimodal Large Language Model with Bootstrapped\n  Preference Optimization", "author": "Renjie Pi and Tianyang Han and Wei Xiong and Jipeng Zhang and Runtao Liu and Rui Pan and Tong Zhang", "abstract": "  Multimodal Large Language Models (MLLMs) excel in generating responses based\non visual inputs. However, they often suffer from a bias towards generating\nresponses similar to their pretraining corpus, overshadowing the importance of\nvisual information. We treat this bias as a \"preference\" for pretraining\nstatistics, which hinders the model's grounding in visual input. To mitigate\nthis issue, we propose Bootstrapped Preference Optimization (BPO), which\nconducts preference learning with datasets containing negative responses\nbootstrapped from the model itself. Specifically, we propose the following two\nstrategies: 1) using distorted image inputs to the MLLM for eliciting responses\nthat contain signified pretraining bias; 2) leveraging text-based LLM to\nexplicitly inject erroneous but common elements into the original response.\nThose undesirable responses are paired with original annotated responses from\nthe datasets to construct the preference dataset, which is subsequently\nutilized to perform preference learning. Our approach effectively suppresses\npretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive\nexperimentation demonstrates significant performance improvements across\nmultiple benchmarks, advancing the state-of-the-art in multimodal\nconversational systems.\n", "link": "http://arxiv.org/abs/2403.08730v1", "date": "2024-03-13", "relevancy": 2.1211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5448}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.53}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5159}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Strengthening%20Multimodal%20Large%20Language%20Model%20with%20Bootstrapped%0A%20%20Preference%20Optimization&body=Title%3A%20Strengthening%20Multimodal%20Large%20Language%20Model%20with%20Bootstrapped%0A%20%20Preference%20Optimization%0AAuthor%3A%20Renjie%20Pi%20and%20Tianyang%20Han%20and%20Wei%20Xiong%20and%20Jipeng%20Zhang%20and%20Runtao%20Liu%20and%20Rui%20Pan%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20in%20generating%20responses%20based%0Aon%20visual%20inputs.%20However%2C%20they%20often%20suffer%20from%20a%20bias%20towards%20generating%0Aresponses%20similar%20to%20their%20pretraining%20corpus%2C%20overshadowing%20the%20importance%20of%0Avisual%20information.%20We%20treat%20this%20bias%20as%20a%20%22preference%22%20for%20pretraining%0Astatistics%2C%20which%20hinders%20the%20model%27s%20grounding%20in%20visual%20input.%20To%20mitigate%0Athis%20issue%2C%20we%20propose%20Bootstrapped%20Preference%20Optimization%20%28BPO%29%2C%20which%0Aconducts%20preference%20learning%20with%20datasets%20containing%20negative%20responses%0Abootstrapped%20from%20the%20model%20itself.%20Specifically%2C%20we%20propose%20the%20following%20two%0Astrategies%3A%201%29%20using%20distorted%20image%20inputs%20to%20the%20MLLM%20for%20eliciting%20responses%0Athat%20contain%20signified%20pretraining%20bias%3B%202%29%20leveraging%20text-based%20LLM%20to%0Aexplicitly%20inject%20erroneous%20but%20common%20elements%20into%20the%20original%20response.%0AThose%20undesirable%20responses%20are%20paired%20with%20original%20annotated%20responses%20from%0Athe%20datasets%20to%20construct%20the%20preference%20dataset%2C%20which%20is%20subsequently%0Autilized%20to%20perform%20preference%20learning.%20Our%20approach%20effectively%20suppresses%0Apretrained%20LLM%20bias%2C%20enabling%20enhanced%20grounding%20in%20visual%20inputs.%20Extensive%0Aexperimentation%20demonstrates%20significant%20performance%20improvements%20across%0Amultiple%20benchmarks%2C%20advancing%20the%20state-of-the-art%20in%20multimodal%0Aconversational%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08730v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strengthening%20Multimodal%20Large%20Language%20Model%20with%20Bootstrapped%0A%20%20Preference%20Optimization&entry.906535625=Renjie%20Pi%20and%20Tianyang%20Han%20and%20Wei%20Xiong%20and%20Jipeng%20Zhang%20and%20Runtao%20Liu%20and%20Rui%20Pan%20and%20Tong%20Zhang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20in%20generating%20responses%20based%0Aon%20visual%20inputs.%20However%2C%20they%20often%20suffer%20from%20a%20bias%20towards%20generating%0Aresponses%20similar%20to%20their%20pretraining%20corpus%2C%20overshadowing%20the%20importance%20of%0Avisual%20information.%20We%20treat%20this%20bias%20as%20a%20%22preference%22%20for%20pretraining%0Astatistics%2C%20which%20hinders%20the%20model%27s%20grounding%20in%20visual%20input.%20To%20mitigate%0Athis%20issue%2C%20we%20propose%20Bootstrapped%20Preference%20Optimization%20%28BPO%29%2C%20which%0Aconducts%20preference%20learning%20with%20datasets%20containing%20negative%20responses%0Abootstrapped%20from%20the%20model%20itself.%20Specifically%2C%20we%20propose%20the%20following%20two%0Astrategies%3A%201%29%20using%20distorted%20image%20inputs%20to%20the%20MLLM%20for%20eliciting%20responses%0Athat%20contain%20signified%20pretraining%20bias%3B%202%29%20leveraging%20text-based%20LLM%20to%0Aexplicitly%20inject%20erroneous%20but%20common%20elements%20into%20the%20original%20response.%0AThose%20undesirable%20responses%20are%20paired%20with%20original%20annotated%20responses%20from%0Athe%20datasets%20to%20construct%20the%20preference%20dataset%2C%20which%20is%20subsequently%0Autilized%20to%20perform%20preference%20learning.%20Our%20approach%20effectively%20suppresses%0Apretrained%20LLM%20bias%2C%20enabling%20enhanced%20grounding%20in%20visual%20inputs.%20Extensive%0Aexperimentation%20demonstrates%20significant%20performance%20improvements%20across%0Amultiple%20benchmarks%2C%20advancing%20the%20state-of-the-art%20in%20multimodal%0Aconversational%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08730v1&entry.124074799=Read"},
{"title": "Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of\n  Gaussians Mechanisms", "author": "Arun Ganesh", "abstract": "  We give a procedure for computing group-level $(\\epsilon, \\delta)$-DP\nguarantees for DP-SGD, when using Poisson sampling or fixed batch size\nsampling. Up to discretization errors in the implementation, the DP guarantees\ncomputed by this procedure are tight (assuming we release every intermediate\niterate).\n", "link": "http://arxiv.org/abs/2401.10294v2", "date": "2024-03-13", "relevancy": 2.1084, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4384}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4162}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4104}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Tight%20Group-Level%20DP%20Guarantees%20for%20DP-SGD%20with%20Sampling%20via%20Mixture%20of%0A%20%20Gaussians%20Mechanisms&body=Title%3A%20Tight%20Group-Level%20DP%20Guarantees%20for%20DP-SGD%20with%20Sampling%20via%20Mixture%20of%0A%20%20Gaussians%20Mechanisms%0AAuthor%3A%20Arun%20Ganesh%0AAbstract%3A%20%20%20We%20give%20a%20procedure%20for%20computing%20group-level%20%24%28%5Cepsilon%2C%20%5Cdelta%29%24-DP%0Aguarantees%20for%20DP-SGD%2C%20when%20using%20Poisson%20sampling%20or%20fixed%20batch%20size%0Asampling.%20Up%20to%20discretization%20errors%20in%20the%20implementation%2C%20the%20DP%20guarantees%0Acomputed%20by%20this%20procedure%20are%20tight%20%28assuming%20we%20release%20every%20intermediate%0Aiterate%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10294v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tight%20Group-Level%20DP%20Guarantees%20for%20DP-SGD%20with%20Sampling%20via%20Mixture%20of%0A%20%20Gaussians%20Mechanisms&entry.906535625=Arun%20Ganesh&entry.1292438233=%20%20We%20give%20a%20procedure%20for%20computing%20group-level%20%24%28%5Cepsilon%2C%20%5Cdelta%29%24-DP%0Aguarantees%20for%20DP-SGD%2C%20when%20using%20Poisson%20sampling%20or%20fixed%20batch%20size%0Asampling.%20Up%20to%20discretization%20errors%20in%20the%20implementation%2C%20the%20DP%20guarantees%0Acomputed%20by%20this%20procedure%20are%20tight%20%28assuming%20we%20release%20every%20intermediate%0Aiterate%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10294v2&entry.124074799=Read"},
{"title": "Ambient Diffusion Posterior Sampling: Solving Inverse Problems with\n  Diffusion Models trained on Corrupted Data", "author": "Asad Aali and Giannis Daras and Brett Levac and Sidharth Kumar and Alexandros G. Dimakis and Jonathan I. Tamir", "abstract": "  We provide a framework for solving inverse problems with diffusion models\nlearned from linearly corrupted data. Our method, Ambient Diffusion Posterior\nSampling (A-DPS), leverages a generative model pre-trained on one type of\ncorruption (e.g. image inpainting) to perform posterior sampling conditioned on\nmeasurements from a potentially different forward process (e.g. image\nblurring). We test the efficacy of our approach on standard natural image\ndatasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes\noutperform models trained on clean data for several image restoration tasks in\nboth speed and performance. We further extend the Ambient Diffusion framework\nto train MRI models with access only to Fourier subsampled multi-coil MRI\nmeasurements at various acceleration factors (R=2, 4, 6, 8). We again observe\nthat models trained on highly subsampled data are better priors for solving\ninverse problems in the high acceleration regime than models trained on fully\nsampled data. We open-source our code and the trained Ambient Diffusion MRI\nmodels: https://github.com/utcsilab/ambient-diffusion-mri .\n", "link": "http://arxiv.org/abs/2403.08728v1", "date": "2024-03-13", "relevancy": 2.1024, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5827}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5184}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5099}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Ambient%20Diffusion%20Posterior%20Sampling%3A%20Solving%20Inverse%20Problems%20with%0A%20%20Diffusion%20Models%20trained%20on%20Corrupted%20Data&body=Title%3A%20Ambient%20Diffusion%20Posterior%20Sampling%3A%20Solving%20Inverse%20Problems%20with%0A%20%20Diffusion%20Models%20trained%20on%20Corrupted%20Data%0AAuthor%3A%20Asad%20Aali%20and%20Giannis%20Daras%20and%20Brett%20Levac%20and%20Sidharth%20Kumar%20and%20Alexandros%20G.%20Dimakis%20and%20Jonathan%20I.%20Tamir%0AAbstract%3A%20%20%20We%20provide%20a%20framework%20for%20solving%20inverse%20problems%20with%20diffusion%20models%0Alearned%20from%20linearly%20corrupted%20data.%20Our%20method%2C%20Ambient%20Diffusion%20Posterior%0ASampling%20%28A-DPS%29%2C%20leverages%20a%20generative%20model%20pre-trained%20on%20one%20type%20of%0Acorruption%20%28e.g.%20image%20inpainting%29%20to%20perform%20posterior%20sampling%20conditioned%20on%0Ameasurements%20from%20a%20potentially%20different%20forward%20process%20%28e.g.%20image%0Ablurring%29.%20We%20test%20the%20efficacy%20of%20our%20approach%20on%20standard%20natural%20image%0Adatasets%20%28CelebA%2C%20FFHQ%2C%20and%20AFHQ%29%20and%20we%20show%20that%20A-DPS%20can%20sometimes%0Aoutperform%20models%20trained%20on%20clean%20data%20for%20several%20image%20restoration%20tasks%20in%0Aboth%20speed%20and%20performance.%20We%20further%20extend%20the%20Ambient%20Diffusion%20framework%0Ato%20train%20MRI%20models%20with%20access%20only%20to%20Fourier%20subsampled%20multi-coil%20MRI%0Ameasurements%20at%20various%20acceleration%20factors%20%28R%3D2%2C%204%2C%206%2C%208%29.%20We%20again%20observe%0Athat%20models%20trained%20on%20highly%20subsampled%20data%20are%20better%20priors%20for%20solving%0Ainverse%20problems%20in%20the%20high%20acceleration%20regime%20than%20models%20trained%20on%20fully%0Asampled%20data.%20We%20open-source%20our%20code%20and%20the%20trained%20Ambient%20Diffusion%20MRI%0Amodels%3A%20https%3A//github.com/utcsilab/ambient-diffusion-mri%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08728v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ambient%20Diffusion%20Posterior%20Sampling%3A%20Solving%20Inverse%20Problems%20with%0A%20%20Diffusion%20Models%20trained%20on%20Corrupted%20Data&entry.906535625=Asad%20Aali%20and%20Giannis%20Daras%20and%20Brett%20Levac%20and%20Sidharth%20Kumar%20and%20Alexandros%20G.%20Dimakis%20and%20Jonathan%20I.%20Tamir&entry.1292438233=%20%20We%20provide%20a%20framework%20for%20solving%20inverse%20problems%20with%20diffusion%20models%0Alearned%20from%20linearly%20corrupted%20data.%20Our%20method%2C%20Ambient%20Diffusion%20Posterior%0ASampling%20%28A-DPS%29%2C%20leverages%20a%20generative%20model%20pre-trained%20on%20one%20type%20of%0Acorruption%20%28e.g.%20image%20inpainting%29%20to%20perform%20posterior%20sampling%20conditioned%20on%0Ameasurements%20from%20a%20potentially%20different%20forward%20process%20%28e.g.%20image%0Ablurring%29.%20We%20test%20the%20efficacy%20of%20our%20approach%20on%20standard%20natural%20image%0Adatasets%20%28CelebA%2C%20FFHQ%2C%20and%20AFHQ%29%20and%20we%20show%20that%20A-DPS%20can%20sometimes%0Aoutperform%20models%20trained%20on%20clean%20data%20for%20several%20image%20restoration%20tasks%20in%0Aboth%20speed%20and%20performance.%20We%20further%20extend%20the%20Ambient%20Diffusion%20framework%0Ato%20train%20MRI%20models%20with%20access%20only%20to%20Fourier%20subsampled%20multi-coil%20MRI%0Ameasurements%20at%20various%20acceleration%20factors%20%28R%3D2%2C%204%2C%206%2C%208%29.%20We%20again%20observe%0Athat%20models%20trained%20on%20highly%20subsampled%20data%20are%20better%20priors%20for%20solving%0Ainverse%20problems%20in%20the%20high%20acceleration%20regime%20than%20models%20trained%20on%20fully%0Asampled%20data.%20We%20open-source%20our%20code%20and%20the%20trained%20Ambient%20Diffusion%20MRI%0Amodels%3A%20https%3A//github.com/utcsilab/ambient-diffusion-mri%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08728v1&entry.124074799=Read"},
{"title": "Leveraging Compressed Frame Sizes For Ultra-Fast Video Classification", "author": "Yuxing Han and Yunan Ding and Chen Ye Gan and Jiangtao Wen", "abstract": "  Classifying videos into distinct categories, such as Sport and Music Video,\nis crucial for multimedia understanding and retrieval, especially when an\nimmense volume of video content is being constantly generated. Traditional\nmethods require video decompression to extract pixel-level features like color,\ntexture, and motion, thereby increasing computational and storage demands.\nMoreover, these methods often suffer from performance degradation in\nlow-quality videos. We present a novel approach that examines only the\npost-compression bitstream of a video to perform classification, eliminating\nthe need for bitstream decoding. To validate our approach, we built a\ncomprehensive data set comprising over 29,000 YouTube video clips, totaling\n6,000 hours and spanning 11 distinct categories. Our evaluations indicate\nprecision, accuracy, and recall rates consistently above 80%, many exceeding\n90%, and some reaching 99%. The algorithm operates approximately 15,000 times\nfaster than real-time for 30fps videos, outperforming traditional Dynamic Time\nWarping (DTW) algorithm by seven orders of magnitude.\n", "link": "http://arxiv.org/abs/2403.08580v1", "date": "2024-03-13", "relevancy": 2.0943, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5373}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5276}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.514}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Compressed%20Frame%20Sizes%20For%20Ultra-Fast%20Video%20Classification&body=Title%3A%20Leveraging%20Compressed%20Frame%20Sizes%20For%20Ultra-Fast%20Video%20Classification%0AAuthor%3A%20Yuxing%20Han%20and%20Yunan%20Ding%20and%20Chen%20Ye%20Gan%20and%20Jiangtao%20Wen%0AAbstract%3A%20%20%20Classifying%20videos%20into%20distinct%20categories%2C%20such%20as%20Sport%20and%20Music%20Video%2C%0Ais%20crucial%20for%20multimedia%20understanding%20and%20retrieval%2C%20especially%20when%20an%0Aimmense%20volume%20of%20video%20content%20is%20being%20constantly%20generated.%20Traditional%0Amethods%20require%20video%20decompression%20to%20extract%20pixel-level%20features%20like%20color%2C%0Atexture%2C%20and%20motion%2C%20thereby%20increasing%20computational%20and%20storage%20demands.%0AMoreover%2C%20these%20methods%20often%20suffer%20from%20performance%20degradation%20in%0Alow-quality%20videos.%20We%20present%20a%20novel%20approach%20that%20examines%20only%20the%0Apost-compression%20bitstream%20of%20a%20video%20to%20perform%20classification%2C%20eliminating%0Athe%20need%20for%20bitstream%20decoding.%20To%20validate%20our%20approach%2C%20we%20built%20a%0Acomprehensive%20data%20set%20comprising%20over%2029%2C000%20YouTube%20video%20clips%2C%20totaling%0A6%2C000%20hours%20and%20spanning%2011%20distinct%20categories.%20Our%20evaluations%20indicate%0Aprecision%2C%20accuracy%2C%20and%20recall%20rates%20consistently%20above%2080%25%2C%20many%20exceeding%0A90%25%2C%20and%20some%20reaching%2099%25.%20The%20algorithm%20operates%20approximately%2015%2C000%20times%0Afaster%20than%20real-time%20for%2030fps%20videos%2C%20outperforming%20traditional%20Dynamic%20Time%0AWarping%20%28DTW%29%20algorithm%20by%20seven%20orders%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08580v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Compressed%20Frame%20Sizes%20For%20Ultra-Fast%20Video%20Classification&entry.906535625=Yuxing%20Han%20and%20Yunan%20Ding%20and%20Chen%20Ye%20Gan%20and%20Jiangtao%20Wen&entry.1292438233=%20%20Classifying%20videos%20into%20distinct%20categories%2C%20such%20as%20Sport%20and%20Music%20Video%2C%0Ais%20crucial%20for%20multimedia%20understanding%20and%20retrieval%2C%20especially%20when%20an%0Aimmense%20volume%20of%20video%20content%20is%20being%20constantly%20generated.%20Traditional%0Amethods%20require%20video%20decompression%20to%20extract%20pixel-level%20features%20like%20color%2C%0Atexture%2C%20and%20motion%2C%20thereby%20increasing%20computational%20and%20storage%20demands.%0AMoreover%2C%20these%20methods%20often%20suffer%20from%20performance%20degradation%20in%0Alow-quality%20videos.%20We%20present%20a%20novel%20approach%20that%20examines%20only%20the%0Apost-compression%20bitstream%20of%20a%20video%20to%20perform%20classification%2C%20eliminating%0Athe%20need%20for%20bitstream%20decoding.%20To%20validate%20our%20approach%2C%20we%20built%20a%0Acomprehensive%20data%20set%20comprising%20over%2029%2C000%20YouTube%20video%20clips%2C%20totaling%0A6%2C000%20hours%20and%20spanning%2011%20distinct%20categories.%20Our%20evaluations%20indicate%0Aprecision%2C%20accuracy%2C%20and%20recall%20rates%20consistently%20above%2080%25%2C%20many%20exceeding%0A90%25%2C%20and%20some%20reaching%2099%25.%20The%20algorithm%20operates%20approximately%2015%2C000%20times%0Afaster%20than%20real-time%20for%2030fps%20videos%2C%20outperforming%20traditional%20Dynamic%20Time%0AWarping%20%28DTW%29%20algorithm%20by%20seven%20orders%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08580v1&entry.124074799=Read"},
{"title": "Medical Multimodal-Multitask Foundation Model for Superior Chest CT\n  Performance", "author": "Chuang Niu and Qing Lyu and Christopher D. Carothers and Parisa Kaviani and Josh Tan and Pingkun Yan and Mannudeep K. Kalra and Christopher T. Whitlow and Ge Wang", "abstract": "  Patient management requires multitasking interaction with multimodal data.\nWhile today's AI, particularly large foundation models, promises unprecedented\nopportunities, progress remains relatively slow in developing medical\nmultimodal multitask foundation models. There are two main challenges along\nthis direction: the data challenge -- the high bar to curate medical multimodal\nmultitask datasets including 3D medical tomographic images in alignment with\nother clinical datasets, and the model challenge -- the unavailability of a\nscalable and adaptable foundation model architecture to synergize multimodal\ndatasets for diverse clinical tasks. Here we propose the first-of-its-kind\nmedical multimodal-multitask foundation model (M3FM) with an emphasis on lung\ncancer screening. To train our M3FM, we first curated a comprehensive\nmultimodal multitask dataset consisting of 163,725 3D chest CT exams, 48\nclinical data types, and 17 medical tasks on lung, heart, and other chest\ndiseases. Then, we created and applied a multimodal question-answering\nframework as a unified training strategy to effectively integrate multimodal\ninformation and naturally perform multiple tasks with free-text prompting.\nExtensive experimental results demonstrate that M3FM consistently outperforms\nthe previous state-of-the-art models. M3FM can identify informative multimodal\ndata elements that are relevant to specific clinical tasks, being instrumental\nin building AI models and gaining insights into correlations among multimodal\ndata and diseases. M3FM can be adapted to boost the performance of new tasks\nwith a small out-of-distribution dataset. M3FM has enabled superior volumetric\nCT imaging performance for lung cancer screening, cardiac disease prediction,\nand other CT-related tasks. M3FM can be extended to incorporate more data types\nand improve other medical tasks, towards AI-empowered precise and efficient\nmedicine.\n", "link": "http://arxiv.org/abs/2304.02649v2", "date": "2024-03-13", "relevancy": 2.0935, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5682}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5345}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4741}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Medical%20Multimodal-Multitask%20Foundation%20Model%20for%20Superior%20Chest%20CT%0A%20%20Performance&body=Title%3A%20Medical%20Multimodal-Multitask%20Foundation%20Model%20for%20Superior%20Chest%20CT%0A%20%20Performance%0AAuthor%3A%20Chuang%20Niu%20and%20Qing%20Lyu%20and%20Christopher%20D.%20Carothers%20and%20Parisa%20Kaviani%20and%20Josh%20Tan%20and%20Pingkun%20Yan%20and%20Mannudeep%20K.%20Kalra%20and%20Christopher%20T.%20Whitlow%20and%20Ge%20Wang%0AAbstract%3A%20%20%20Patient%20management%20requires%20multitasking%20interaction%20with%20multimodal%20data.%0AWhile%20today%27s%20AI%2C%20particularly%20large%20foundation%20models%2C%20promises%20unprecedented%0Aopportunities%2C%20progress%20remains%20relatively%20slow%20in%20developing%20medical%0Amultimodal%20multitask%20foundation%20models.%20There%20are%20two%20main%20challenges%20along%0Athis%20direction%3A%20the%20data%20challenge%20--%20the%20high%20bar%20to%20curate%20medical%20multimodal%0Amultitask%20datasets%20including%203D%20medical%20tomographic%20images%20in%20alignment%20with%0Aother%20clinical%20datasets%2C%20and%20the%20model%20challenge%20--%20the%20unavailability%20of%20a%0Ascalable%20and%20adaptable%20foundation%20model%20architecture%20to%20synergize%20multimodal%0Adatasets%20for%20diverse%20clinical%20tasks.%20Here%20we%20propose%20the%20first-of-its-kind%0Amedical%20multimodal-multitask%20foundation%20model%20%28M3FM%29%20with%20an%20emphasis%20on%20lung%0Acancer%20screening.%20To%20train%20our%20M3FM%2C%20we%20first%20curated%20a%20comprehensive%0Amultimodal%20multitask%20dataset%20consisting%20of%20163%2C725%203D%20chest%20CT%20exams%2C%2048%0Aclinical%20data%20types%2C%20and%2017%20medical%20tasks%20on%20lung%2C%20heart%2C%20and%20other%20chest%0Adiseases.%20Then%2C%20we%20created%20and%20applied%20a%20multimodal%20question-answering%0Aframework%20as%20a%20unified%20training%20strategy%20to%20effectively%20integrate%20multimodal%0Ainformation%20and%20naturally%20perform%20multiple%20tasks%20with%20free-text%20prompting.%0AExtensive%20experimental%20results%20demonstrate%20that%20M3FM%20consistently%20outperforms%0Athe%20previous%20state-of-the-art%20models.%20M3FM%20can%20identify%20informative%20multimodal%0Adata%20elements%20that%20are%20relevant%20to%20specific%20clinical%20tasks%2C%20being%20instrumental%0Ain%20building%20AI%20models%20and%20gaining%20insights%20into%20correlations%20among%20multimodal%0Adata%20and%20diseases.%20M3FM%20can%20be%20adapted%20to%20boost%20the%20performance%20of%20new%20tasks%0Awith%20a%20small%20out-of-distribution%20dataset.%20M3FM%20has%20enabled%20superior%20volumetric%0ACT%20imaging%20performance%20for%20lung%20cancer%20screening%2C%20cardiac%20disease%20prediction%2C%0Aand%20other%20CT-related%20tasks.%20M3FM%20can%20be%20extended%20to%20incorporate%20more%20data%20types%0Aand%20improve%20other%20medical%20tasks%2C%20towards%20AI-empowered%20precise%20and%20efficient%0Amedicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02649v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medical%20Multimodal-Multitask%20Foundation%20Model%20for%20Superior%20Chest%20CT%0A%20%20Performance&entry.906535625=Chuang%20Niu%20and%20Qing%20Lyu%20and%20Christopher%20D.%20Carothers%20and%20Parisa%20Kaviani%20and%20Josh%20Tan%20and%20Pingkun%20Yan%20and%20Mannudeep%20K.%20Kalra%20and%20Christopher%20T.%20Whitlow%20and%20Ge%20Wang&entry.1292438233=%20%20Patient%20management%20requires%20multitasking%20interaction%20with%20multimodal%20data.%0AWhile%20today%27s%20AI%2C%20particularly%20large%20foundation%20models%2C%20promises%20unprecedented%0Aopportunities%2C%20progress%20remains%20relatively%20slow%20in%20developing%20medical%0Amultimodal%20multitask%20foundation%20models.%20There%20are%20two%20main%20challenges%20along%0Athis%20direction%3A%20the%20data%20challenge%20--%20the%20high%20bar%20to%20curate%20medical%20multimodal%0Amultitask%20datasets%20including%203D%20medical%20tomographic%20images%20in%20alignment%20with%0Aother%20clinical%20datasets%2C%20and%20the%20model%20challenge%20--%20the%20unavailability%20of%20a%0Ascalable%20and%20adaptable%20foundation%20model%20architecture%20to%20synergize%20multimodal%0Adatasets%20for%20diverse%20clinical%20tasks.%20Here%20we%20propose%20the%20first-of-its-kind%0Amedical%20multimodal-multitask%20foundation%20model%20%28M3FM%29%20with%20an%20emphasis%20on%20lung%0Acancer%20screening.%20To%20train%20our%20M3FM%2C%20we%20first%20curated%20a%20comprehensive%0Amultimodal%20multitask%20dataset%20consisting%20of%20163%2C725%203D%20chest%20CT%20exams%2C%2048%0Aclinical%20data%20types%2C%20and%2017%20medical%20tasks%20on%20lung%2C%20heart%2C%20and%20other%20chest%0Adiseases.%20Then%2C%20we%20created%20and%20applied%20a%20multimodal%20question-answering%0Aframework%20as%20a%20unified%20training%20strategy%20to%20effectively%20integrate%20multimodal%0Ainformation%20and%20naturally%20perform%20multiple%20tasks%20with%20free-text%20prompting.%0AExtensive%20experimental%20results%20demonstrate%20that%20M3FM%20consistently%20outperforms%0Athe%20previous%20state-of-the-art%20models.%20M3FM%20can%20identify%20informative%20multimodal%0Adata%20elements%20that%20are%20relevant%20to%20specific%20clinical%20tasks%2C%20being%20instrumental%0Ain%20building%20AI%20models%20and%20gaining%20insights%20into%20correlations%20among%20multimodal%0Adata%20and%20diseases.%20M3FM%20can%20be%20adapted%20to%20boost%20the%20performance%20of%20new%20tasks%0Awith%20a%20small%20out-of-distribution%20dataset.%20M3FM%20has%20enabled%20superior%20volumetric%0ACT%20imaging%20performance%20for%20lung%20cancer%20screening%2C%20cardiac%20disease%20prediction%2C%0Aand%20other%20CT-related%20tasks.%20M3FM%20can%20be%20extended%20to%20incorporate%20more%20data%20types%0Aand%20improve%20other%20medical%20tasks%2C%20towards%20AI-empowered%20precise%20and%20efficient%0Amedicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02649v2&entry.124074799=Read"},
{"title": "Referential communication in heterogeneous communities of pre-trained\n  visual deep networks", "author": "Mat\u00e9o Mahaut and Francesca Franzon and Roberto Dess\u00ec and Marco Baroni", "abstract": "  As large pre-trained image-processing neural networks are being embedded in\nautonomous agents such as self-driving cars or robots, the question arises of\nhow such systems can communicate with each other about the surrounding world,\ndespite their different architectures and training regimes. As a first step in\nthis direction, we systematically explore the task of \\textit{referential\ncommunication} in a community of heterogeneous state-of-the-art pre-trained\nvisual networks, showing that they can develop, in a self-supervised way, a\nshared protocol to refer to a target object among a set of candidates. This\nshared protocol can also be used, to some extent, to communicate about\npreviously unseen object categories of different granularity. Moreover, a\nvisual network that was not initially part of an existing community can learn\nthe community's protocol with remarkable ease. Finally, we study, both\nqualitatively and quantitatively, the properties of the emergent protocol,\nproviding some evidence that it is capturing high-level semantic features of\nobjects.\n", "link": "http://arxiv.org/abs/2302.08913v4", "date": "2024-03-13", "relevancy": 2.0686, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5277}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.523}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5043}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Referential%20communication%20in%20heterogeneous%20communities%20of%20pre-trained%0A%20%20visual%20deep%20networks&body=Title%3A%20Referential%20communication%20in%20heterogeneous%20communities%20of%20pre-trained%0A%20%20visual%20deep%20networks%0AAuthor%3A%20Mat%C3%A9o%20Mahaut%20and%20Francesca%20Franzon%20and%20Roberto%20Dess%C3%AC%20and%20Marco%20Baroni%0AAbstract%3A%20%20%20As%20large%20pre-trained%20image-processing%20neural%20networks%20are%20being%20embedded%20in%0Aautonomous%20agents%20such%20as%20self-driving%20cars%20or%20robots%2C%20the%20question%20arises%20of%0Ahow%20such%20systems%20can%20communicate%20with%20each%20other%20about%20the%20surrounding%20world%2C%0Adespite%20their%20different%20architectures%20and%20training%20regimes.%20As%20a%20first%20step%20in%0Athis%20direction%2C%20we%20systematically%20explore%20the%20task%20of%20%5Ctextit%7Breferential%0Acommunication%7D%20in%20a%20community%20of%20heterogeneous%20state-of-the-art%20pre-trained%0Avisual%20networks%2C%20showing%20that%20they%20can%20develop%2C%20in%20a%20self-supervised%20way%2C%20a%0Ashared%20protocol%20to%20refer%20to%20a%20target%20object%20among%20a%20set%20of%20candidates.%20This%0Ashared%20protocol%20can%20also%20be%20used%2C%20to%20some%20extent%2C%20to%20communicate%20about%0Apreviously%20unseen%20object%20categories%20of%20different%20granularity.%20Moreover%2C%20a%0Avisual%20network%20that%20was%20not%20initially%20part%20of%20an%20existing%20community%20can%20learn%0Athe%20community%27s%20protocol%20with%20remarkable%20ease.%20Finally%2C%20we%20study%2C%20both%0Aqualitatively%20and%20quantitatively%2C%20the%20properties%20of%20the%20emergent%20protocol%2C%0Aproviding%20some%20evidence%20that%20it%20is%20capturing%20high-level%20semantic%20features%20of%0Aobjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.08913v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Referential%20communication%20in%20heterogeneous%20communities%20of%20pre-trained%0A%20%20visual%20deep%20networks&entry.906535625=Mat%C3%A9o%20Mahaut%20and%20Francesca%20Franzon%20and%20Roberto%20Dess%C3%AC%20and%20Marco%20Baroni&entry.1292438233=%20%20As%20large%20pre-trained%20image-processing%20neural%20networks%20are%20being%20embedded%20in%0Aautonomous%20agents%20such%20as%20self-driving%20cars%20or%20robots%2C%20the%20question%20arises%20of%0Ahow%20such%20systems%20can%20communicate%20with%20each%20other%20about%20the%20surrounding%20world%2C%0Adespite%20their%20different%20architectures%20and%20training%20regimes.%20As%20a%20first%20step%20in%0Athis%20direction%2C%20we%20systematically%20explore%20the%20task%20of%20%5Ctextit%7Breferential%0Acommunication%7D%20in%20a%20community%20of%20heterogeneous%20state-of-the-art%20pre-trained%0Avisual%20networks%2C%20showing%20that%20they%20can%20develop%2C%20in%20a%20self-supervised%20way%2C%20a%0Ashared%20protocol%20to%20refer%20to%20a%20target%20object%20among%20a%20set%20of%20candidates.%20This%0Ashared%20protocol%20can%20also%20be%20used%2C%20to%20some%20extent%2C%20to%20communicate%20about%0Apreviously%20unseen%20object%20categories%20of%20different%20granularity.%20Moreover%2C%20a%0Avisual%20network%20that%20was%20not%20initially%20part%20of%20an%20existing%20community%20can%20learn%0Athe%20community%27s%20protocol%20with%20remarkable%20ease.%20Finally%2C%20we%20study%2C%20both%0Aqualitatively%20and%20quantitatively%2C%20the%20properties%20of%20the%20emergent%20protocol%2C%0Aproviding%20some%20evidence%20that%20it%20is%20capturing%20high-level%20semantic%20features%20of%0Aobjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.08913v4&entry.124074799=Read"},
{"title": "AGI: Artificial General Intelligence for Education", "author": "Ehsan Latif and Gengchen Mai and Matthew Nyaaba and Xuansheng Wu and Ninghao Liu and Guoyu Lu and Sheng Li and Tianming Liu and Xiaoming Zhai", "abstract": "  Artificial general intelligence (AGI) has gained global recognition as a\nfuture technology due to the emergence of breakthrough large language models\nand chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventional\nAI models, typically designed for a limited range of tasks, demand significant\namounts of domain-specific data for training and may not always consider\nintricate interpersonal dynamics in education. AGI, driven by the recent large\npre-trained models, represents a significant leap in the capability of machines\nto perform tasks that require human-level intelligence, such as reasoning,\nproblem-solving, decision-making, and even understanding human emotions and\nsocial interactions. This position paper reviews AGI's key concepts,\ncapabilities, scope, and potential within future education, including achieving\nfuture educational goals, designing pedagogy and curriculum, and performing\nassessments. It highlights that AGI can significantly improve intelligent\ntutoring systems, educational assessment, and evaluation procedures. AGI\nsystems can adapt to individual student needs, offering tailored learning\nexperiences. They can also provide comprehensive feedback on student\nperformance and dynamically adjust teaching methods based on student progress.\nThe paper emphasizes that AGI's capabilities extend to understanding human\nemotions and social interactions, which are critical in educational settings.\nThe paper discusses that ethical issues in education with AGI include data\nbias, fairness, and privacy and emphasizes the need for codes of conduct to\nensure responsible AGI use in academic settings like homework, teaching, and\nrecruitment. We also conclude that the development of AGI necessitates\ninterdisciplinary collaborations between educators and AI engineers to advance\nresearch and application efforts.\n", "link": "http://arxiv.org/abs/2304.12479v5", "date": "2024-03-13", "relevancy": 2.0532, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5605}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4802}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4782}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20AGI%3A%20Artificial%20General%20Intelligence%20for%20Education&body=Title%3A%20AGI%3A%20Artificial%20General%20Intelligence%20for%20Education%0AAuthor%3A%20Ehsan%20Latif%20and%20Gengchen%20Mai%20and%20Matthew%20Nyaaba%20and%20Xuansheng%20Wu%20and%20Ninghao%20Liu%20and%20Guoyu%20Lu%20and%20Sheng%20Li%20and%20Tianming%20Liu%20and%20Xiaoming%20Zhai%0AAbstract%3A%20%20%20Artificial%20general%20intelligence%20%28AGI%29%20has%20gained%20global%20recognition%20as%20a%0Afuture%20technology%20due%20to%20the%20emergence%20of%20breakthrough%20large%20language%20models%0Aand%20chatbots%20such%20as%20GPT-4%20and%20ChatGPT%2C%20respectively.%20Compared%20to%20conventional%0AAI%20models%2C%20typically%20designed%20for%20a%20limited%20range%20of%20tasks%2C%20demand%20significant%0Aamounts%20of%20domain-specific%20data%20for%20training%20and%20may%20not%20always%20consider%0Aintricate%20interpersonal%20dynamics%20in%20education.%20AGI%2C%20driven%20by%20the%20recent%20large%0Apre-trained%20models%2C%20represents%20a%20significant%20leap%20in%20the%20capability%20of%20machines%0Ato%20perform%20tasks%20that%20require%20human-level%20intelligence%2C%20such%20as%20reasoning%2C%0Aproblem-solving%2C%20decision-making%2C%20and%20even%20understanding%20human%20emotions%20and%0Asocial%20interactions.%20This%20position%20paper%20reviews%20AGI%27s%20key%20concepts%2C%0Acapabilities%2C%20scope%2C%20and%20potential%20within%20future%20education%2C%20including%20achieving%0Afuture%20educational%20goals%2C%20designing%20pedagogy%20and%20curriculum%2C%20and%20performing%0Aassessments.%20It%20highlights%20that%20AGI%20can%20significantly%20improve%20intelligent%0Atutoring%20systems%2C%20educational%20assessment%2C%20and%20evaluation%20procedures.%20AGI%0Asystems%20can%20adapt%20to%20individual%20student%20needs%2C%20offering%20tailored%20learning%0Aexperiences.%20They%20can%20also%20provide%20comprehensive%20feedback%20on%20student%0Aperformance%20and%20dynamically%20adjust%20teaching%20methods%20based%20on%20student%20progress.%0AThe%20paper%20emphasizes%20that%20AGI%27s%20capabilities%20extend%20to%20understanding%20human%0Aemotions%20and%20social%20interactions%2C%20which%20are%20critical%20in%20educational%20settings.%0AThe%20paper%20discusses%20that%20ethical%20issues%20in%20education%20with%20AGI%20include%20data%0Abias%2C%20fairness%2C%20and%20privacy%20and%20emphasizes%20the%20need%20for%20codes%20of%20conduct%20to%0Aensure%20responsible%20AGI%20use%20in%20academic%20settings%20like%20homework%2C%20teaching%2C%20and%0Arecruitment.%20We%20also%20conclude%20that%20the%20development%20of%20AGI%20necessitates%0Ainterdisciplinary%20collaborations%20between%20educators%20and%20AI%20engineers%20to%20advance%0Aresearch%20and%20application%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.12479v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGI%3A%20Artificial%20General%20Intelligence%20for%20Education&entry.906535625=Ehsan%20Latif%20and%20Gengchen%20Mai%20and%20Matthew%20Nyaaba%20and%20Xuansheng%20Wu%20and%20Ninghao%20Liu%20and%20Guoyu%20Lu%20and%20Sheng%20Li%20and%20Tianming%20Liu%20and%20Xiaoming%20Zhai&entry.1292438233=%20%20Artificial%20general%20intelligence%20%28AGI%29%20has%20gained%20global%20recognition%20as%20a%0Afuture%20technology%20due%20to%20the%20emergence%20of%20breakthrough%20large%20language%20models%0Aand%20chatbots%20such%20as%20GPT-4%20and%20ChatGPT%2C%20respectively.%20Compared%20to%20conventional%0AAI%20models%2C%20typically%20designed%20for%20a%20limited%20range%20of%20tasks%2C%20demand%20significant%0Aamounts%20of%20domain-specific%20data%20for%20training%20and%20may%20not%20always%20consider%0Aintricate%20interpersonal%20dynamics%20in%20education.%20AGI%2C%20driven%20by%20the%20recent%20large%0Apre-trained%20models%2C%20represents%20a%20significant%20leap%20in%20the%20capability%20of%20machines%0Ato%20perform%20tasks%20that%20require%20human-level%20intelligence%2C%20such%20as%20reasoning%2C%0Aproblem-solving%2C%20decision-making%2C%20and%20even%20understanding%20human%20emotions%20and%0Asocial%20interactions.%20This%20position%20paper%20reviews%20AGI%27s%20key%20concepts%2C%0Acapabilities%2C%20scope%2C%20and%20potential%20within%20future%20education%2C%20including%20achieving%0Afuture%20educational%20goals%2C%20designing%20pedagogy%20and%20curriculum%2C%20and%20performing%0Aassessments.%20It%20highlights%20that%20AGI%20can%20significantly%20improve%20intelligent%0Atutoring%20systems%2C%20educational%20assessment%2C%20and%20evaluation%20procedures.%20AGI%0Asystems%20can%20adapt%20to%20individual%20student%20needs%2C%20offering%20tailored%20learning%0Aexperiences.%20They%20can%20also%20provide%20comprehensive%20feedback%20on%20student%0Aperformance%20and%20dynamically%20adjust%20teaching%20methods%20based%20on%20student%20progress.%0AThe%20paper%20emphasizes%20that%20AGI%27s%20capabilities%20extend%20to%20understanding%20human%0Aemotions%20and%20social%20interactions%2C%20which%20are%20critical%20in%20educational%20settings.%0AThe%20paper%20discusses%20that%20ethical%20issues%20in%20education%20with%20AGI%20include%20data%0Abias%2C%20fairness%2C%20and%20privacy%20and%20emphasizes%20the%20need%20for%20codes%20of%20conduct%20to%0Aensure%20responsible%20AGI%20use%20in%20academic%20settings%20like%20homework%2C%20teaching%2C%20and%0Arecruitment.%20We%20also%20conclude%20that%20the%20development%20of%20AGI%20necessitates%0Ainterdisciplinary%20collaborations%20between%20educators%20and%20AI%20engineers%20to%20advance%0Aresearch%20and%20application%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.12479v5&entry.124074799=Read"},
{"title": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models", "author": "Adam Ibrahim and Benjamin Th\u00e9rien and Kshitij Gupta and Mats L. Richter and Quentin Anthony and Timoth\u00e9e Lesort and Eugene Belilovsky and Irina Rish", "abstract": "  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n", "link": "http://arxiv.org/abs/2403.08763v1", "date": "2024-03-13", "relevancy": 2.026, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5463}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4892}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models&body=Title%3A%20Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Adam%20Ibrahim%20and%20Benjamin%20Th%C3%A9rien%20and%20Kshitij%20Gupta%20and%20Mats%20L.%20Richter%20and%20Quentin%20Anthony%20and%20Timoth%C3%A9e%20Lesort%20and%20Eugene%20Belilovsky%20and%20Irina%20Rish%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20routinely%20pre-trained%20on%20billions%20of%20tokens%2C%0Aonly%20to%20start%20the%20process%20over%20again%20once%20new%20data%20becomes%20available.%20A%20much%0Amore%20efficient%20solution%20is%20to%20continually%20pre-train%20these%20models%2C%20saving%0Asignificant%20compute%20compared%20to%20re-training.%20However%2C%20the%20distribution%20shift%0Ainduced%20by%20new%20data%20typically%20results%20in%20degraded%20performance%20on%20previous%20data%0Aor%20poor%20adaptation%20to%20the%20new%20data.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20and%0Ascalable%20combination%20of%20learning%20rate%20%28LR%29%20re-warming%2C%20LR%20re-decaying%2C%20and%0Areplay%20of%20previous%20data%20is%20sufficient%20to%20match%20the%20performance%20of%20fully%0Are-training%20from%20scratch%20on%20all%20available%20data%2C%20as%20measured%20by%20final%20loss%20and%0Alanguage%20model%20%28LM%29%20evaluation%20benchmarks.%20Specifically%2C%20we%20show%20this%20for%20a%0Aweak%20but%20realistic%20distribution%20shift%20between%20two%20commonly%20used%20LLM%0Apre-training%20datasets%20%28English%24%5Crightarrow%24English%29%20and%20a%20stronger%20distribution%0Ashift%20%28English%24%5Crightarrow%24German%29%20at%20the%20%24405%24M%20parameter%20model%20scale%20with%0Alarge%20dataset%20sizes%20%28hundreds%20of%20billions%20of%20tokens%29.%20Selecting%20the%20weak%20but%0Arealistic%20shift%20for%20larger-scale%20experiments%2C%20we%20also%20find%20that%20our%20continual%0Alearning%20strategies%20match%20the%20re-training%20baseline%20for%20a%2010B%20parameter%20LLM.%20Our%0Aresults%20demonstrate%20that%20LLMs%20can%20be%20successfully%20updated%20via%20simple%20and%0Ascalable%20continual%20learning%20strategies%2C%20matching%20the%20re-training%20baseline%20using%0Aonly%20a%20fraction%20of%20the%20compute.%20Finally%2C%20inspired%20by%20previous%20work%2C%20we%20propose%0Aalternatives%20to%20the%20cosine%20learning%20rate%20schedule%20that%20help%20circumvent%0Aforgetting%20induced%20by%20LR%20re-warming%20and%20that%20are%20not%20bound%20to%20a%20fixed%20token%0Abudget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08763v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models&entry.906535625=Adam%20Ibrahim%20and%20Benjamin%20Th%C3%A9rien%20and%20Kshitij%20Gupta%20and%20Mats%20L.%20Richter%20and%20Quentin%20Anthony%20and%20Timoth%C3%A9e%20Lesort%20and%20Eugene%20Belilovsky%20and%20Irina%20Rish&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20routinely%20pre-trained%20on%20billions%20of%20tokens%2C%0Aonly%20to%20start%20the%20process%20over%20again%20once%20new%20data%20becomes%20available.%20A%20much%0Amore%20efficient%20solution%20is%20to%20continually%20pre-train%20these%20models%2C%20saving%0Asignificant%20compute%20compared%20to%20re-training.%20However%2C%20the%20distribution%20shift%0Ainduced%20by%20new%20data%20typically%20results%20in%20degraded%20performance%20on%20previous%20data%0Aor%20poor%20adaptation%20to%20the%20new%20data.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20and%0Ascalable%20combination%20of%20learning%20rate%20%28LR%29%20re-warming%2C%20LR%20re-decaying%2C%20and%0Areplay%20of%20previous%20data%20is%20sufficient%20to%20match%20the%20performance%20of%20fully%0Are-training%20from%20scratch%20on%20all%20available%20data%2C%20as%20measured%20by%20final%20loss%20and%0Alanguage%20model%20%28LM%29%20evaluation%20benchmarks.%20Specifically%2C%20we%20show%20this%20for%20a%0Aweak%20but%20realistic%20distribution%20shift%20between%20two%20commonly%20used%20LLM%0Apre-training%20datasets%20%28English%24%5Crightarrow%24English%29%20and%20a%20stronger%20distribution%0Ashift%20%28English%24%5Crightarrow%24German%29%20at%20the%20%24405%24M%20parameter%20model%20scale%20with%0Alarge%20dataset%20sizes%20%28hundreds%20of%20billions%20of%20tokens%29.%20Selecting%20the%20weak%20but%0Arealistic%20shift%20for%20larger-scale%20experiments%2C%20we%20also%20find%20that%20our%20continual%0Alearning%20strategies%20match%20the%20re-training%20baseline%20for%20a%2010B%20parameter%20LLM.%20Our%0Aresults%20demonstrate%20that%20LLMs%20can%20be%20successfully%20updated%20via%20simple%20and%0Ascalable%20continual%20learning%20strategies%2C%20matching%20the%20re-training%20baseline%20using%0Aonly%20a%20fraction%20of%20the%20compute.%20Finally%2C%20inspired%20by%20previous%20work%2C%20we%20propose%0Aalternatives%20to%20the%20cosine%20learning%20rate%20schedule%20that%20help%20circumvent%0Aforgetting%20induced%20by%20LR%20re-warming%20and%20that%20are%20not%20bound%20to%20a%20fixed%20token%0Abudget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08763v1&entry.124074799=Read"},
{"title": "Demystifying Embedding Spaces using Large Language Models", "author": "Guy Tennenholtz and Yinlam Chow and Chih-Wei Hsu and Jihwan Jeong and Lior Shani and Azamat Tulepbergenov and Deepak Ramachandran and Martin Mladenov and Craig Boutilier", "abstract": "  Embeddings have become a pivotal means to represent complex, multi-faceted\ninformation about entities, concepts, and relationships in a condensed and\nuseful format. Nevertheless, they often preclude direct interpretation. While\ndownstream tasks make use of these compressed representations, meaningful\ninterpretation usually requires visualization using dimensionality reduction or\nspecialized machine learning interpretability methods. This paper addresses the\nchallenge of making such embeddings more interpretable and broadly useful, by\nemploying Large Language Models (LLMs) to directly interact with embeddings --\ntransforming abstract vectors into understandable narratives. By injecting\nembeddings into LLMs, we enable querying and exploration of complex embedding\ndata. We demonstrate our approach on a variety of diverse tasks, including:\nenhancing concept activation vectors (CAVs), communicating novel embedded\nentities, and decoding user preferences in recommender systems. Our work\ncouples the immense information potential of embeddings with the interpretative\npower of LLMs.\n", "link": "http://arxiv.org/abs/2310.04475v2", "date": "2024-03-13", "relevancy": 2.0186, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5216}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.497}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4907}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Demystifying%20Embedding%20Spaces%20using%20Large%20Language%20Models&body=Title%3A%20Demystifying%20Embedding%20Spaces%20using%20Large%20Language%20Models%0AAuthor%3A%20Guy%20Tennenholtz%20and%20Yinlam%20Chow%20and%20Chih-Wei%20Hsu%20and%20Jihwan%20Jeong%20and%20Lior%20Shani%20and%20Azamat%20Tulepbergenov%20and%20Deepak%20Ramachandran%20and%20Martin%20Mladenov%20and%20Craig%20Boutilier%0AAbstract%3A%20%20%20Embeddings%20have%20become%20a%20pivotal%20means%20to%20represent%20complex%2C%20multi-faceted%0Ainformation%20about%20entities%2C%20concepts%2C%20and%20relationships%20in%20a%20condensed%20and%0Auseful%20format.%20Nevertheless%2C%20they%20often%20preclude%20direct%20interpretation.%20While%0Adownstream%20tasks%20make%20use%20of%20these%20compressed%20representations%2C%20meaningful%0Ainterpretation%20usually%20requires%20visualization%20using%20dimensionality%20reduction%20or%0Aspecialized%20machine%20learning%20interpretability%20methods.%20This%20paper%20addresses%20the%0Achallenge%20of%20making%20such%20embeddings%20more%20interpretable%20and%20broadly%20useful%2C%20by%0Aemploying%20Large%20Language%20Models%20%28LLMs%29%20to%20directly%20interact%20with%20embeddings%20--%0Atransforming%20abstract%20vectors%20into%20understandable%20narratives.%20By%20injecting%0Aembeddings%20into%20LLMs%2C%20we%20enable%20querying%20and%20exploration%20of%20complex%20embedding%0Adata.%20We%20demonstrate%20our%20approach%20on%20a%20variety%20of%20diverse%20tasks%2C%20including%3A%0Aenhancing%20concept%20activation%20vectors%20%28CAVs%29%2C%20communicating%20novel%20embedded%0Aentities%2C%20and%20decoding%20user%20preferences%20in%20recommender%20systems.%20Our%20work%0Acouples%20the%20immense%20information%20potential%20of%20embeddings%20with%20the%20interpretative%0Apower%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04475v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20Embedding%20Spaces%20using%20Large%20Language%20Models&entry.906535625=Guy%20Tennenholtz%20and%20Yinlam%20Chow%20and%20Chih-Wei%20Hsu%20and%20Jihwan%20Jeong%20and%20Lior%20Shani%20and%20Azamat%20Tulepbergenov%20and%20Deepak%20Ramachandran%20and%20Martin%20Mladenov%20and%20Craig%20Boutilier&entry.1292438233=%20%20Embeddings%20have%20become%20a%20pivotal%20means%20to%20represent%20complex%2C%20multi-faceted%0Ainformation%20about%20entities%2C%20concepts%2C%20and%20relationships%20in%20a%20condensed%20and%0Auseful%20format.%20Nevertheless%2C%20they%20often%20preclude%20direct%20interpretation.%20While%0Adownstream%20tasks%20make%20use%20of%20these%20compressed%20representations%2C%20meaningful%0Ainterpretation%20usually%20requires%20visualization%20using%20dimensionality%20reduction%20or%0Aspecialized%20machine%20learning%20interpretability%20methods.%20This%20paper%20addresses%20the%0Achallenge%20of%20making%20such%20embeddings%20more%20interpretable%20and%20broadly%20useful%2C%20by%0Aemploying%20Large%20Language%20Models%20%28LLMs%29%20to%20directly%20interact%20with%20embeddings%20--%0Atransforming%20abstract%20vectors%20into%20understandable%20narratives.%20By%20injecting%0Aembeddings%20into%20LLMs%2C%20we%20enable%20querying%20and%20exploration%20of%20complex%20embedding%0Adata.%20We%20demonstrate%20our%20approach%20on%20a%20variety%20of%20diverse%20tasks%2C%20including%3A%0Aenhancing%20concept%20activation%20vectors%20%28CAVs%29%2C%20communicating%20novel%20embedded%0Aentities%2C%20and%20decoding%20user%20preferences%20in%20recommender%20systems.%20Our%20work%0Acouples%20the%20immense%20information%20potential%20of%20embeddings%20with%20the%20interpretative%0Apower%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04475v2&entry.124074799=Read"},
{"title": "Can Direct Latent Model Learning Solve Linear Quadratic Gaussian\n  Control?", "author": "Yi Tian and Kaiqing Zhang and Russ Tedrake and Suvrit Sra", "abstract": "  We study the task of learning state representations from potentially\nhigh-dimensional observations, with the goal of controlling an unknown\npartially observable system. We pursue a direct latent model learning approach,\nwhere a dynamic model in some latent state space is learned by predicting\nquantities directly related to planning (e.g., costs) without reconstructing\nthe observations. In particular, we focus on an intuitive cost-driven state\nrepresentation learning method for solving Linear Quadratic Gaussian (LQG)\ncontrol, one of the most fundamental partially observable control problems. As\nour main results, we establish finite-sample guarantees of finding a\nnear-optimal state representation function and a near-optimal controller using\nthe directly learned latent model. To the best of our knowledge, despite\nvarious empirical successes, prior to this work it was unclear if such a\ncost-driven latent model learner enjoys finite-sample guarantees. Our work\nunderscores the value of predicting multi-step costs, an idea that is key to\nour theory, and notably also an idea that is known to be empirically valuable\nfor learning state representations.\n", "link": "http://arxiv.org/abs/2212.14511v2", "date": "2024-03-13", "relevancy": 2.0129, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5263}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5116}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4769}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Can%20Direct%20Latent%20Model%20Learning%20Solve%20Linear%20Quadratic%20Gaussian%0A%20%20Control%3F&body=Title%3A%20Can%20Direct%20Latent%20Model%20Learning%20Solve%20Linear%20Quadratic%20Gaussian%0A%20%20Control%3F%0AAuthor%3A%20Yi%20Tian%20and%20Kaiqing%20Zhang%20and%20Russ%20Tedrake%20and%20Suvrit%20Sra%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20learning%20state%20representations%20from%20potentially%0Ahigh-dimensional%20observations%2C%20with%20the%20goal%20of%20controlling%20an%20unknown%0Apartially%20observable%20system.%20We%20pursue%20a%20direct%20latent%20model%20learning%20approach%2C%0Awhere%20a%20dynamic%20model%20in%20some%20latent%20state%20space%20is%20learned%20by%20predicting%0Aquantities%20directly%20related%20to%20planning%20%28e.g.%2C%20costs%29%20without%20reconstructing%0Athe%20observations.%20In%20particular%2C%20we%20focus%20on%20an%20intuitive%20cost-driven%20state%0Arepresentation%20learning%20method%20for%20solving%20Linear%20Quadratic%20Gaussian%20%28LQG%29%0Acontrol%2C%20one%20of%20the%20most%20fundamental%20partially%20observable%20control%20problems.%20As%0Aour%20main%20results%2C%20we%20establish%20finite-sample%20guarantees%20of%20finding%20a%0Anear-optimal%20state%20representation%20function%20and%20a%20near-optimal%20controller%20using%0Athe%20directly%20learned%20latent%20model.%20To%20the%20best%20of%20our%20knowledge%2C%20despite%0Avarious%20empirical%20successes%2C%20prior%20to%20this%20work%20it%20was%20unclear%20if%20such%20a%0Acost-driven%20latent%20model%20learner%20enjoys%20finite-sample%20guarantees.%20Our%20work%0Aunderscores%20the%20value%20of%20predicting%20multi-step%20costs%2C%20an%20idea%20that%20is%20key%20to%0Aour%20theory%2C%20and%20notably%20also%20an%20idea%20that%20is%20known%20to%20be%20empirically%20valuable%0Afor%20learning%20state%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.14511v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Direct%20Latent%20Model%20Learning%20Solve%20Linear%20Quadratic%20Gaussian%0A%20%20Control%3F&entry.906535625=Yi%20Tian%20and%20Kaiqing%20Zhang%20and%20Russ%20Tedrake%20and%20Suvrit%20Sra&entry.1292438233=%20%20We%20study%20the%20task%20of%20learning%20state%20representations%20from%20potentially%0Ahigh-dimensional%20observations%2C%20with%20the%20goal%20of%20controlling%20an%20unknown%0Apartially%20observable%20system.%20We%20pursue%20a%20direct%20latent%20model%20learning%20approach%2C%0Awhere%20a%20dynamic%20model%20in%20some%20latent%20state%20space%20is%20learned%20by%20predicting%0Aquantities%20directly%20related%20to%20planning%20%28e.g.%2C%20costs%29%20without%20reconstructing%0Athe%20observations.%20In%20particular%2C%20we%20focus%20on%20an%20intuitive%20cost-driven%20state%0Arepresentation%20learning%20method%20for%20solving%20Linear%20Quadratic%20Gaussian%20%28LQG%29%0Acontrol%2C%20one%20of%20the%20most%20fundamental%20partially%20observable%20control%20problems.%20As%0Aour%20main%20results%2C%20we%20establish%20finite-sample%20guarantees%20of%20finding%20a%0Anear-optimal%20state%20representation%20function%20and%20a%20near-optimal%20controller%20using%0Athe%20directly%20learned%20latent%20model.%20To%20the%20best%20of%20our%20knowledge%2C%20despite%0Avarious%20empirical%20successes%2C%20prior%20to%20this%20work%20it%20was%20unclear%20if%20such%20a%0Acost-driven%20latent%20model%20learner%20enjoys%20finite-sample%20guarantees.%20Our%20work%0Aunderscores%20the%20value%20of%20predicting%20multi-step%20costs%2C%20an%20idea%20that%20is%20key%20to%0Aour%20theory%2C%20and%20notably%20also%20an%20idea%20that%20is%20known%20to%20be%20empirically%20valuable%0Afor%20learning%20state%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.14511v2&entry.124074799=Read"},
{"title": "Deep Learning for In-Orbit Cloud Segmentation and Classification in\n  Hyperspectral Satellite Data", "author": "Daniel Kovac and Jan Mucha and Jon Alvarez Justo and Jiri Mekyska and Zoltan Galaz and Krystof Novotny and Radoslav Pitonak and Jan Knezik and Jonas Herec and Tor Arne Johansen", "abstract": "  This article explores the latest Convolutional Neural Networks (CNNs) for\ncloud detection aboard hyperspectral satellites. The performance of the latest\n1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and\n2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed.\nEvaluation criteria include precision and computational efficiency for in-orbit\ndeployment. Experiments utilize NASA's EO-1 Hyperion data, with varying\nspectral channel numbers after Principal Component Analysis. Results indicate\nthat 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs,\nwhile maintaining compactness with larger spectral channel sets, albeit with\nincreased inference times. However, the performance of 1D CNN degrades with\nsignificant channel reduction. In this context, the 2D-Justo-UNet-Simple offers\nthe best balance for in-orbit deployment, considering precision, memory, and\ntime costs. While nnU-net is suitable for on-ground processing, deployment of\nlightweight 1D-Justo-LiuNet is recommended for high-precision applications.\nAlternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced\ncosts between timing and precision in orbit.\n", "link": "http://arxiv.org/abs/2403.08695v1", "date": "2024-03-13", "relevancy": 2.0031, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5357}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4658}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20In-Orbit%20Cloud%20Segmentation%20and%20Classification%20in%0A%20%20Hyperspectral%20Satellite%20Data&body=Title%3A%20Deep%20Learning%20for%20In-Orbit%20Cloud%20Segmentation%20and%20Classification%20in%0A%20%20Hyperspectral%20Satellite%20Data%0AAuthor%3A%20Daniel%20Kovac%20and%20Jan%20Mucha%20and%20Jon%20Alvarez%20Justo%20and%20Jiri%20Mekyska%20and%20Zoltan%20Galaz%20and%20Krystof%20Novotny%20and%20Radoslav%20Pitonak%20and%20Jan%20Knezik%20and%20Jonas%20Herec%20and%20Tor%20Arne%20Johansen%0AAbstract%3A%20%20%20This%20article%20explores%20the%20latest%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%0Acloud%20detection%20aboard%20hyperspectral%20satellites.%20The%20performance%20of%20the%20latest%0A1D%20CNN%20%281D-Justo-LiuNet%29%20and%20two%20recent%202D%20CNNs%20%28nnU-net%20and%0A2D-Justo-UNet-Simple%29%20for%20cloud%20segmentation%20and%20classification%20is%20assessed.%0AEvaluation%20criteria%20include%20precision%20and%20computational%20efficiency%20for%20in-orbit%0Adeployment.%20Experiments%20utilize%20NASA%27s%20EO-1%20Hyperion%20data%2C%20with%20varying%0Aspectral%20channel%20numbers%20after%20Principal%20Component%20Analysis.%20Results%20indicate%0Athat%201D-Justo-LiuNet%20achieves%20the%20highest%20accuracy%2C%20outperforming%202D%20CNNs%2C%0Awhile%20maintaining%20compactness%20with%20larger%20spectral%20channel%20sets%2C%20albeit%20with%0Aincreased%20inference%20times.%20However%2C%20the%20performance%20of%201D%20CNN%20degrades%20with%0Asignificant%20channel%20reduction.%20In%20this%20context%2C%20the%202D-Justo-UNet-Simple%20offers%0Athe%20best%20balance%20for%20in-orbit%20deployment%2C%20considering%20precision%2C%20memory%2C%20and%0Atime%20costs.%20While%20nnU-net%20is%20suitable%20for%20on-ground%20processing%2C%20deployment%20of%0Alightweight%201D-Justo-LiuNet%20is%20recommended%20for%20high-precision%20applications.%0AAlternatively%2C%20lightweight%202D-Justo-UNet-Simple%20is%20recommended%20for%20balanced%0Acosts%20between%20timing%20and%20precision%20in%20orbit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08695v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20In-Orbit%20Cloud%20Segmentation%20and%20Classification%20in%0A%20%20Hyperspectral%20Satellite%20Data&entry.906535625=Daniel%20Kovac%20and%20Jan%20Mucha%20and%20Jon%20Alvarez%20Justo%20and%20Jiri%20Mekyska%20and%20Zoltan%20Galaz%20and%20Krystof%20Novotny%20and%20Radoslav%20Pitonak%20and%20Jan%20Knezik%20and%20Jonas%20Herec%20and%20Tor%20Arne%20Johansen&entry.1292438233=%20%20This%20article%20explores%20the%20latest%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%0Acloud%20detection%20aboard%20hyperspectral%20satellites.%20The%20performance%20of%20the%20latest%0A1D%20CNN%20%281D-Justo-LiuNet%29%20and%20two%20recent%202D%20CNNs%20%28nnU-net%20and%0A2D-Justo-UNet-Simple%29%20for%20cloud%20segmentation%20and%20classification%20is%20assessed.%0AEvaluation%20criteria%20include%20precision%20and%20computational%20efficiency%20for%20in-orbit%0Adeployment.%20Experiments%20utilize%20NASA%27s%20EO-1%20Hyperion%20data%2C%20with%20varying%0Aspectral%20channel%20numbers%20after%20Principal%20Component%20Analysis.%20Results%20indicate%0Athat%201D-Justo-LiuNet%20achieves%20the%20highest%20accuracy%2C%20outperforming%202D%20CNNs%2C%0Awhile%20maintaining%20compactness%20with%20larger%20spectral%20channel%20sets%2C%20albeit%20with%0Aincreased%20inference%20times.%20However%2C%20the%20performance%20of%201D%20CNN%20degrades%20with%0Asignificant%20channel%20reduction.%20In%20this%20context%2C%20the%202D-Justo-UNet-Simple%20offers%0Athe%20best%20balance%20for%20in-orbit%20deployment%2C%20considering%20precision%2C%20memory%2C%20and%0Atime%20costs.%20While%20nnU-net%20is%20suitable%20for%20on-ground%20processing%2C%20deployment%20of%0Alightweight%201D-Justo-LiuNet%20is%20recommended%20for%20high-precision%20applications.%0AAlternatively%2C%20lightweight%202D-Justo-UNet-Simple%20is%20recommended%20for%20balanced%0Acosts%20between%20timing%20and%20precision%20in%20orbit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08695v1&entry.124074799=Read"},
{"title": "Clinically Feasible Diffusion Reconstruction for Highly-Accelerated\n  Cardiac Cine MRI", "author": "Shihan Qiu and Shaoyan Pan and Yikang Liu and Lin Zhao and Jian Xu and Qi Liu and Terrence Chen and Eric Z. Chen and Xiao Chen and Shanhui Sun", "abstract": "  The currently limited quality of accelerated cardiac cine reconstruction may\npotentially be improved by the emerging diffusion models, but the clinically\nunacceptable long processing time poses a challenge. We aim to develop a\nclinically feasible diffusion-model-based reconstruction pipeline to improve\nthe image quality of cine MRI. A multi-in multi-out diffusion enhancement model\ntogether with fast inference strategies were developed to be used in\nconjunction with a reconstruction model. The diffusion reconstruction reduced\nspatial and temporal blurring in prospectively undersampled clinical data, as\nvalidated by experts inspection. The 1.5s per video processing time enabled the\napproach to be applied in clinical scenarios.\n", "link": "http://arxiv.org/abs/2403.08749v1", "date": "2024-03-13", "relevancy": 1.9916, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5312}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4994}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4831}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Clinically%20Feasible%20Diffusion%20Reconstruction%20for%20Highly-Accelerated%0A%20%20Cardiac%20Cine%20MRI&body=Title%3A%20Clinically%20Feasible%20Diffusion%20Reconstruction%20for%20Highly-Accelerated%0A%20%20Cardiac%20Cine%20MRI%0AAuthor%3A%20Shihan%20Qiu%20and%20Shaoyan%20Pan%20and%20Yikang%20Liu%20and%20Lin%20Zhao%20and%20Jian%20Xu%20and%20Qi%20Liu%20and%20Terrence%20Chen%20and%20Eric%20Z.%20Chen%20and%20Xiao%20Chen%20and%20Shanhui%20Sun%0AAbstract%3A%20%20%20The%20currently%20limited%20quality%20of%20accelerated%20cardiac%20cine%20reconstruction%20may%0Apotentially%20be%20improved%20by%20the%20emerging%20diffusion%20models%2C%20but%20the%20clinically%0Aunacceptable%20long%20processing%20time%20poses%20a%20challenge.%20We%20aim%20to%20develop%20a%0Aclinically%20feasible%20diffusion-model-based%20reconstruction%20pipeline%20to%20improve%0Athe%20image%20quality%20of%20cine%20MRI.%20A%20multi-in%20multi-out%20diffusion%20enhancement%20model%0Atogether%20with%20fast%20inference%20strategies%20were%20developed%20to%20be%20used%20in%0Aconjunction%20with%20a%20reconstruction%20model.%20The%20diffusion%20reconstruction%20reduced%0Aspatial%20and%20temporal%20blurring%20in%20prospectively%20undersampled%20clinical%20data%2C%20as%0Avalidated%20by%20experts%20inspection.%20The%201.5s%20per%20video%20processing%20time%20enabled%20the%0Aapproach%20to%20be%20applied%20in%20clinical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08749v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clinically%20Feasible%20Diffusion%20Reconstruction%20for%20Highly-Accelerated%0A%20%20Cardiac%20Cine%20MRI&entry.906535625=Shihan%20Qiu%20and%20Shaoyan%20Pan%20and%20Yikang%20Liu%20and%20Lin%20Zhao%20and%20Jian%20Xu%20and%20Qi%20Liu%20and%20Terrence%20Chen%20and%20Eric%20Z.%20Chen%20and%20Xiao%20Chen%20and%20Shanhui%20Sun&entry.1292438233=%20%20The%20currently%20limited%20quality%20of%20accelerated%20cardiac%20cine%20reconstruction%20may%0Apotentially%20be%20improved%20by%20the%20emerging%20diffusion%20models%2C%20but%20the%20clinically%0Aunacceptable%20long%20processing%20time%20poses%20a%20challenge.%20We%20aim%20to%20develop%20a%0Aclinically%20feasible%20diffusion-model-based%20reconstruction%20pipeline%20to%20improve%0Athe%20image%20quality%20of%20cine%20MRI.%20A%20multi-in%20multi-out%20diffusion%20enhancement%20model%0Atogether%20with%20fast%20inference%20strategies%20were%20developed%20to%20be%20used%20in%0Aconjunction%20with%20a%20reconstruction%20model.%20The%20diffusion%20reconstruction%20reduced%0Aspatial%20and%20temporal%20blurring%20in%20prospectively%20undersampled%20clinical%20data%2C%20as%0Avalidated%20by%20experts%20inspection.%20The%201.5s%20per%20video%20processing%20time%20enabled%20the%0Aapproach%20to%20be%20applied%20in%20clinical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08749v1&entry.124074799=Read"},
{"title": "A Causal Inspired Early-Branching Structure for Domain Generalization", "author": "Liang Chen and Yong Zhang and Yibing Song and Zhen Zhang and Lingqiao Liu", "abstract": "  Learning domain-invariant semantic representations is crucial for achieving\ndomain generalization (DG), where a model is required to perform well on unseen\ntarget domains. One critical challenge is that standard training often results\nin entangled semantic and domain-specific features. Previous works suggest\nformulating the problem from a causal perspective and solving the entanglement\nproblem by enforcing marginal independence between the causal (\\ie semantic)\nand non-causal (\\ie domain-specific) features. Despite its simplicity, the\nbasic marginal independent-based idea alone may be insufficient to identify the\ncausal feature. By d-separation, we observe that the causal feature can be\nfurther characterized by being independent of the domain conditioned on the\nobject, and we propose the following two strategies as complements for the\nbasic framework.\n  First, the observation implicitly implies that for the same object, the\ncausal feature should not be associated with the non-causal feature, revealing\nthat the common practice of obtaining the two features with a shared base\nfeature extractor and two lightweight prediction heads might be inappropriate.\nTo meet the constraint, we propose a simple early-branching structure, where\nthe causal and non-causal feature obtaining branches share the first few blocks\nwhile diverging thereafter, for better structure design; Second, the\nobservation implies that the causal feature remains invariant across different\ndomains for the same object. To this end, we suggest that augmentation should\nbe incorporated into the framework to better characterize the causal feature,\nand we further suggest an effective random domain sampling scheme to fulfill\nthe task. Theoretical and experimental results show that the two strategies are\nbeneficial for the basic marginal independent-based framework. Code is\navailable at \\url{https://github.com/liangchen527/CausEB}.\n", "link": "http://arxiv.org/abs/2403.08649v1", "date": "2024-03-13", "relevancy": 1.9897, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5019}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4994}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4937}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Causal%20Inspired%20Early-Branching%20Structure%20for%20Domain%20Generalization&body=Title%3A%20A%20Causal%20Inspired%20Early-Branching%20Structure%20for%20Domain%20Generalization%0AAuthor%3A%20Liang%20Chen%20and%20Yong%20Zhang%20and%20Yibing%20Song%20and%20Zhen%20Zhang%20and%20Lingqiao%20Liu%0AAbstract%3A%20%20%20Learning%20domain-invariant%20semantic%20representations%20is%20crucial%20for%20achieving%0Adomain%20generalization%20%28DG%29%2C%20where%20a%20model%20is%20required%20to%20perform%20well%20on%20unseen%0Atarget%20domains.%20One%20critical%20challenge%20is%20that%20standard%20training%20often%20results%0Ain%20entangled%20semantic%20and%20domain-specific%20features.%20Previous%20works%20suggest%0Aformulating%20the%20problem%20from%20a%20causal%20perspective%20and%20solving%20the%20entanglement%0Aproblem%20by%20enforcing%20marginal%20independence%20between%20the%20causal%20%28%5Cie%20semantic%29%0Aand%20non-causal%20%28%5Cie%20domain-specific%29%20features.%20Despite%20its%20simplicity%2C%20the%0Abasic%20marginal%20independent-based%20idea%20alone%20may%20be%20insufficient%20to%20identify%20the%0Acausal%20feature.%20By%20d-separation%2C%20we%20observe%20that%20the%20causal%20feature%20can%20be%0Afurther%20characterized%20by%20being%20independent%20of%20the%20domain%20conditioned%20on%20the%0Aobject%2C%20and%20we%20propose%20the%20following%20two%20strategies%20as%20complements%20for%20the%0Abasic%20framework.%0A%20%20First%2C%20the%20observation%20implicitly%20implies%20that%20for%20the%20same%20object%2C%20the%0Acausal%20feature%20should%20not%20be%20associated%20with%20the%20non-causal%20feature%2C%20revealing%0Athat%20the%20common%20practice%20of%20obtaining%20the%20two%20features%20with%20a%20shared%20base%0Afeature%20extractor%20and%20two%20lightweight%20prediction%20heads%20might%20be%20inappropriate.%0ATo%20meet%20the%20constraint%2C%20we%20propose%20a%20simple%20early-branching%20structure%2C%20where%0Athe%20causal%20and%20non-causal%20feature%20obtaining%20branches%20share%20the%20first%20few%20blocks%0Awhile%20diverging%20thereafter%2C%20for%20better%20structure%20design%3B%20Second%2C%20the%0Aobservation%20implies%20that%20the%20causal%20feature%20remains%20invariant%20across%20different%0Adomains%20for%20the%20same%20object.%20To%20this%20end%2C%20we%20suggest%20that%20augmentation%20should%0Abe%20incorporated%20into%20the%20framework%20to%20better%20characterize%20the%20causal%20feature%2C%0Aand%20we%20further%20suggest%20an%20effective%20random%20domain%20sampling%20scheme%20to%20fulfill%0Athe%20task.%20Theoretical%20and%20experimental%20results%20show%20that%20the%20two%20strategies%20are%0Abeneficial%20for%20the%20basic%20marginal%20independent-based%20framework.%20Code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/liangchen527/CausEB%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08649v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Causal%20Inspired%20Early-Branching%20Structure%20for%20Domain%20Generalization&entry.906535625=Liang%20Chen%20and%20Yong%20Zhang%20and%20Yibing%20Song%20and%20Zhen%20Zhang%20and%20Lingqiao%20Liu&entry.1292438233=%20%20Learning%20domain-invariant%20semantic%20representations%20is%20crucial%20for%20achieving%0Adomain%20generalization%20%28DG%29%2C%20where%20a%20model%20is%20required%20to%20perform%20well%20on%20unseen%0Atarget%20domains.%20One%20critical%20challenge%20is%20that%20standard%20training%20often%20results%0Ain%20entangled%20semantic%20and%20domain-specific%20features.%20Previous%20works%20suggest%0Aformulating%20the%20problem%20from%20a%20causal%20perspective%20and%20solving%20the%20entanglement%0Aproblem%20by%20enforcing%20marginal%20independence%20between%20the%20causal%20%28%5Cie%20semantic%29%0Aand%20non-causal%20%28%5Cie%20domain-specific%29%20features.%20Despite%20its%20simplicity%2C%20the%0Abasic%20marginal%20independent-based%20idea%20alone%20may%20be%20insufficient%20to%20identify%20the%0Acausal%20feature.%20By%20d-separation%2C%20we%20observe%20that%20the%20causal%20feature%20can%20be%0Afurther%20characterized%20by%20being%20independent%20of%20the%20domain%20conditioned%20on%20the%0Aobject%2C%20and%20we%20propose%20the%20following%20two%20strategies%20as%20complements%20for%20the%0Abasic%20framework.%0A%20%20First%2C%20the%20observation%20implicitly%20implies%20that%20for%20the%20same%20object%2C%20the%0Acausal%20feature%20should%20not%20be%20associated%20with%20the%20non-causal%20feature%2C%20revealing%0Athat%20the%20common%20practice%20of%20obtaining%20the%20two%20features%20with%20a%20shared%20base%0Afeature%20extractor%20and%20two%20lightweight%20prediction%20heads%20might%20be%20inappropriate.%0ATo%20meet%20the%20constraint%2C%20we%20propose%20a%20simple%20early-branching%20structure%2C%20where%0Athe%20causal%20and%20non-causal%20feature%20obtaining%20branches%20share%20the%20first%20few%20blocks%0Awhile%20diverging%20thereafter%2C%20for%20better%20structure%20design%3B%20Second%2C%20the%0Aobservation%20implies%20that%20the%20causal%20feature%20remains%20invariant%20across%20different%0Adomains%20for%20the%20same%20object.%20To%20this%20end%2C%20we%20suggest%20that%20augmentation%20should%0Abe%20incorporated%20into%20the%20framework%20to%20better%20characterize%20the%20causal%20feature%2C%0Aand%20we%20further%20suggest%20an%20effective%20random%20domain%20sampling%20scheme%20to%20fulfill%0Athe%20task.%20Theoretical%20and%20experimental%20results%20show%20that%20the%20two%20strategies%20are%0Abeneficial%20for%20the%20basic%20marginal%20independent-based%20framework.%20Code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/liangchen527/CausEB%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08649v1&entry.124074799=Read"},
{"title": "Non-discrimination Criteria for Generative Language Models", "author": "Sara Sterlie and Nina Weng and Aasa Feragen", "abstract": "  Within recent years, generative AI, such as large language models, has\nundergone rapid development. As these models become increasingly available to\nthe public, concerns arise about perpetuating and amplifying harmful biases in\napplications. Gender stereotypes can be harmful and limiting for the\nindividuals they target, whether they consist of misrepresentation or\ndiscrimination. Recognizing gender bias as a pervasive societal construct, this\npaper studies how to uncover and quantify the presence of gender biases in\ngenerative language models. In particular, we derive generative AI analogues of\nthree well-known non-discrimination criteria from classification, namely\nindependence, separation and sufficiency. To demonstrate these criteria in\naction, we design prompts for each of the criteria with a focus on occupational\ngender stereotype, specifically utilizing the medical test to introduce the\nground truth in the generative AI context. Our results address the presence of\noccupational gender bias within such conversational language models.\n", "link": "http://arxiv.org/abs/2403.08564v1", "date": "2024-03-13", "relevancy": 1.949, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5265}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4639}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4476}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Non-discrimination%20Criteria%20for%20Generative%20Language%20Models&body=Title%3A%20Non-discrimination%20Criteria%20for%20Generative%20Language%20Models%0AAuthor%3A%20Sara%20Sterlie%20and%20Nina%20Weng%20and%20Aasa%20Feragen%0AAbstract%3A%20%20%20Within%20recent%20years%2C%20generative%20AI%2C%20such%20as%20large%20language%20models%2C%20has%0Aundergone%20rapid%20development.%20As%20these%20models%20become%20increasingly%20available%20to%0Athe%20public%2C%20concerns%20arise%20about%20perpetuating%20and%20amplifying%20harmful%20biases%20in%0Aapplications.%20Gender%20stereotypes%20can%20be%20harmful%20and%20limiting%20for%20the%0Aindividuals%20they%20target%2C%20whether%20they%20consist%20of%20misrepresentation%20or%0Adiscrimination.%20Recognizing%20gender%20bias%20as%20a%20pervasive%20societal%20construct%2C%20this%0Apaper%20studies%20how%20to%20uncover%20and%20quantify%20the%20presence%20of%20gender%20biases%20in%0Agenerative%20language%20models.%20In%20particular%2C%20we%20derive%20generative%20AI%20analogues%20of%0Athree%20well-known%20non-discrimination%20criteria%20from%20classification%2C%20namely%0Aindependence%2C%20separation%20and%20sufficiency.%20To%20demonstrate%20these%20criteria%20in%0Aaction%2C%20we%20design%20prompts%20for%20each%20of%20the%20criteria%20with%20a%20focus%20on%20occupational%0Agender%20stereotype%2C%20specifically%20utilizing%20the%20medical%20test%20to%20introduce%20the%0Aground%20truth%20in%20the%20generative%20AI%20context.%20Our%20results%20address%20the%20presence%20of%0Aoccupational%20gender%20bias%20within%20such%20conversational%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08564v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-discrimination%20Criteria%20for%20Generative%20Language%20Models&entry.906535625=Sara%20Sterlie%20and%20Nina%20Weng%20and%20Aasa%20Feragen&entry.1292438233=%20%20Within%20recent%20years%2C%20generative%20AI%2C%20such%20as%20large%20language%20models%2C%20has%0Aundergone%20rapid%20development.%20As%20these%20models%20become%20increasingly%20available%20to%0Athe%20public%2C%20concerns%20arise%20about%20perpetuating%20and%20amplifying%20harmful%20biases%20in%0Aapplications.%20Gender%20stereotypes%20can%20be%20harmful%20and%20limiting%20for%20the%0Aindividuals%20they%20target%2C%20whether%20they%20consist%20of%20misrepresentation%20or%0Adiscrimination.%20Recognizing%20gender%20bias%20as%20a%20pervasive%20societal%20construct%2C%20this%0Apaper%20studies%20how%20to%20uncover%20and%20quantify%20the%20presence%20of%20gender%20biases%20in%0Agenerative%20language%20models.%20In%20particular%2C%20we%20derive%20generative%20AI%20analogues%20of%0Athree%20well-known%20non-discrimination%20criteria%20from%20classification%2C%20namely%0Aindependence%2C%20separation%20and%20sufficiency.%20To%20demonstrate%20these%20criteria%20in%0Aaction%2C%20we%20design%20prompts%20for%20each%20of%20the%20criteria%20with%20a%20focus%20on%20occupational%0Agender%20stereotype%2C%20specifically%20utilizing%20the%20medical%20test%20to%20introduce%20the%0Aground%20truth%20in%20the%20generative%20AI%20context.%20Our%20results%20address%20the%20presence%20of%0Aoccupational%20gender%20bias%20within%20such%20conversational%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08564v1&entry.124074799=Read"},
{"title": "Linear attention is (maybe) all you need (to understand transformer\n  optimization)", "author": "Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra", "abstract": "  Transformer training is notoriously difficult, requiring a careful design of\noptimizers and use of various heuristics. We make progress towards\nunderstanding the subtleties of training Transformers by carefully studying a\nsimple yet canonical linearized shallow Transformer model. Specifically, we\ntrain linear Transformers to solve regression tasks, inspired by J.~von Oswald\net al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we\nobserve that our proposed linearized models can reproduce several prominent\naspects of Transformer training dynamics. Consequently, the results obtained in\nthis paper suggest that a simple linearized Transformer model could actually be\na valuable, realistic abstraction for understanding Transformer optimization.\n", "link": "http://arxiv.org/abs/2310.01082v2", "date": "2024-03-13", "relevancy": 1.9308, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4721}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4664}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Linear%20attention%20is%20%28maybe%29%20all%20you%20need%20%28to%20understand%20transformer%0A%20%20optimization%29&body=Title%3A%20Linear%20attention%20is%20%28maybe%29%20all%20you%20need%20%28to%20understand%20transformer%0A%20%20optimization%29%0AAuthor%3A%20Kwangjun%20Ahn%20and%20Xiang%20Cheng%20and%20Minhak%20Song%20and%20Chulhee%20Yun%20and%20Ali%20Jadbabaie%20and%20Suvrit%20Sra%0AAbstract%3A%20%20%20Transformer%20training%20is%20notoriously%20difficult%2C%20requiring%20a%20careful%20design%20of%0Aoptimizers%20and%20use%20of%20various%20heuristics.%20We%20make%20progress%20towards%0Aunderstanding%20the%20subtleties%20of%20training%20Transformers%20by%20carefully%20studying%20a%0Asimple%20yet%20canonical%20linearized%20shallow%20Transformer%20model.%20Specifically%2C%20we%0Atrain%20linear%20Transformers%20to%20solve%20regression%20tasks%2C%20inspired%20by%20J.~von%20Oswald%0Aet%20al.~%28ICML%202023%29%2C%20and%20K.~Ahn%20et%20al.~%28NeurIPS%202023%29.%20Most%20importantly%2C%20we%0Aobserve%20that%20our%20proposed%20linearized%20models%20can%20reproduce%20several%20prominent%0Aaspects%20of%20Transformer%20training%20dynamics.%20Consequently%2C%20the%20results%20obtained%20in%0Athis%20paper%20suggest%20that%20a%20simple%20linearized%20Transformer%20model%20could%20actually%20be%0Aa%20valuable%2C%20realistic%20abstraction%20for%20understanding%20Transformer%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01082v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20attention%20is%20%28maybe%29%20all%20you%20need%20%28to%20understand%20transformer%0A%20%20optimization%29&entry.906535625=Kwangjun%20Ahn%20and%20Xiang%20Cheng%20and%20Minhak%20Song%20and%20Chulhee%20Yun%20and%20Ali%20Jadbabaie%20and%20Suvrit%20Sra&entry.1292438233=%20%20Transformer%20training%20is%20notoriously%20difficult%2C%20requiring%20a%20careful%20design%20of%0Aoptimizers%20and%20use%20of%20various%20heuristics.%20We%20make%20progress%20towards%0Aunderstanding%20the%20subtleties%20of%20training%20Transformers%20by%20carefully%20studying%20a%0Asimple%20yet%20canonical%20linearized%20shallow%20Transformer%20model.%20Specifically%2C%20we%0Atrain%20linear%20Transformers%20to%20solve%20regression%20tasks%2C%20inspired%20by%20J.~von%20Oswald%0Aet%20al.~%28ICML%202023%29%2C%20and%20K.~Ahn%20et%20al.~%28NeurIPS%202023%29.%20Most%20importantly%2C%20we%0Aobserve%20that%20our%20proposed%20linearized%20models%20can%20reproduce%20several%20prominent%0Aaspects%20of%20Transformer%20training%20dynamics.%20Consequently%2C%20the%20results%20obtained%20in%0Athis%20paper%20suggest%20that%20a%20simple%20linearized%20Transformer%20model%20could%20actually%20be%0Aa%20valuable%2C%20realistic%20abstraction%20for%20understanding%20Transformer%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01082v2&entry.124074799=Read"},
{"title": "iCONTRA: Toward Thematic Collection Design Via Interactive Concept\n  Transfer", "author": "Dinh-Khoi Vo and Duy-Nam Ly and Khanh-Duy Le and Tam V. Nguyen and Minh-Triet Tran and Trung-Nghia Le", "abstract": "  Creating thematic collections in industries demands innovative designs and\ncohesive concepts. Designers may face challenges in maintaining thematic\nconsistency when drawing inspiration from existing objects, landscapes, or\nartifacts. While AI-powered graphic design tools offer help, they often fail to\ngenerate cohesive sets based on specific thematic concepts. In response, we\nintroduce iCONTRA, an interactive CONcept TRAnsfer system. With a user-friendly\ninterface, iCONTRA enables both experienced designers and novices to\neffortlessly explore creative design concepts and efficiently generate thematic\ncollections. We also propose a zero-shot image editing algorithm, eliminating\nthe need for fine-tuning models, which gradually integrates information from\ninitial objects, ensuring consistency in the generation process without\ninfluencing the background. A pilot study suggests iCONTRA's potential to\nreduce designers' efforts. Experimental results demonstrate its effectiveness\nin producing consistent and high-quality object concept transfers. iCONTRA\nstands as a promising tool for innovation and creative exploration in thematic\ncollection design. The source code will be available at:\nhttps://github.com/vdkhoi20/iCONTRA.\n", "link": "http://arxiv.org/abs/2403.08746v1", "date": "2024-03-13", "relevancy": 1.9178, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4974}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4687}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4658}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20iCONTRA%3A%20Toward%20Thematic%20Collection%20Design%20Via%20Interactive%20Concept%0A%20%20Transfer&body=Title%3A%20iCONTRA%3A%20Toward%20Thematic%20Collection%20Design%20Via%20Interactive%20Concept%0A%20%20Transfer%0AAuthor%3A%20Dinh-Khoi%20Vo%20and%20Duy-Nam%20Ly%20and%20Khanh-Duy%20Le%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Trung-Nghia%20Le%0AAbstract%3A%20%20%20Creating%20thematic%20collections%20in%20industries%20demands%20innovative%20designs%20and%0Acohesive%20concepts.%20Designers%20may%20face%20challenges%20in%20maintaining%20thematic%0Aconsistency%20when%20drawing%20inspiration%20from%20existing%20objects%2C%20landscapes%2C%20or%0Aartifacts.%20While%20AI-powered%20graphic%20design%20tools%20offer%20help%2C%20they%20often%20fail%20to%0Agenerate%20cohesive%20sets%20based%20on%20specific%20thematic%20concepts.%20In%20response%2C%20we%0Aintroduce%20iCONTRA%2C%20an%20interactive%20CONcept%20TRAnsfer%20system.%20With%20a%20user-friendly%0Ainterface%2C%20iCONTRA%20enables%20both%20experienced%20designers%20and%20novices%20to%0Aeffortlessly%20explore%20creative%20design%20concepts%20and%20efficiently%20generate%20thematic%0Acollections.%20We%20also%20propose%20a%20zero-shot%20image%20editing%20algorithm%2C%20eliminating%0Athe%20need%20for%20fine-tuning%20models%2C%20which%20gradually%20integrates%20information%20from%0Ainitial%20objects%2C%20ensuring%20consistency%20in%20the%20generation%20process%20without%0Ainfluencing%20the%20background.%20A%20pilot%20study%20suggests%20iCONTRA%27s%20potential%20to%0Areduce%20designers%27%20efforts.%20Experimental%20results%20demonstrate%20its%20effectiveness%0Ain%20producing%20consistent%20and%20high-quality%20object%20concept%20transfers.%20iCONTRA%0Astands%20as%20a%20promising%20tool%20for%20innovation%20and%20creative%20exploration%20in%20thematic%0Acollection%20design.%20The%20source%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/vdkhoi20/iCONTRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08746v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iCONTRA%3A%20Toward%20Thematic%20Collection%20Design%20Via%20Interactive%20Concept%0A%20%20Transfer&entry.906535625=Dinh-Khoi%20Vo%20and%20Duy-Nam%20Ly%20and%20Khanh-Duy%20Le%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Trung-Nghia%20Le&entry.1292438233=%20%20Creating%20thematic%20collections%20in%20industries%20demands%20innovative%20designs%20and%0Acohesive%20concepts.%20Designers%20may%20face%20challenges%20in%20maintaining%20thematic%0Aconsistency%20when%20drawing%20inspiration%20from%20existing%20objects%2C%20landscapes%2C%20or%0Aartifacts.%20While%20AI-powered%20graphic%20design%20tools%20offer%20help%2C%20they%20often%20fail%20to%0Agenerate%20cohesive%20sets%20based%20on%20specific%20thematic%20concepts.%20In%20response%2C%20we%0Aintroduce%20iCONTRA%2C%20an%20interactive%20CONcept%20TRAnsfer%20system.%20With%20a%20user-friendly%0Ainterface%2C%20iCONTRA%20enables%20both%20experienced%20designers%20and%20novices%20to%0Aeffortlessly%20explore%20creative%20design%20concepts%20and%20efficiently%20generate%20thematic%0Acollections.%20We%20also%20propose%20a%20zero-shot%20image%20editing%20algorithm%2C%20eliminating%0Athe%20need%20for%20fine-tuning%20models%2C%20which%20gradually%20integrates%20information%20from%0Ainitial%20objects%2C%20ensuring%20consistency%20in%20the%20generation%20process%20without%0Ainfluencing%20the%20background.%20A%20pilot%20study%20suggests%20iCONTRA%27s%20potential%20to%0Areduce%20designers%27%20efforts.%20Experimental%20results%20demonstrate%20its%20effectiveness%0Ain%20producing%20consistent%20and%20high-quality%20object%20concept%20transfers.%20iCONTRA%0Astands%20as%20a%20promising%20tool%20for%20innovation%20and%20creative%20exploration%20in%20thematic%0Acollection%20design.%20The%20source%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/vdkhoi20/iCONTRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08746v1&entry.124074799=Read"},
{"title": "Link Prediction for Social Networks using Representation Learning and\n  Heuristic-based Features", "author": "Samarth Khanna and Sree Bhattacharyya and Sudipto Ghosh and Kushagra Agarwal and Asit Kumar Das", "abstract": "  The exponential growth in scale and relevance of social networks enable them\nto provide expansive insights. Predicting missing links in social networks\nefficiently can help in various modern-day business applications ranging from\ngenerating recommendations to influence analysis. Several categories of\nsolutions exist for the same. Here, we explore various feature extraction\ntechniques to generate representations of nodes and edges in a social network\nthat allow us to predict missing links. We compare the results of using ten\nfeature extraction techniques categorized across Structural embeddings,\nNeighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics,\nfollowed by modeling with ensemble classifiers and custom Neural Networks.\nFurther, we propose combining heuristic-based features and learned\nrepresentations that demonstrate improved performance for the link prediction\ntask on social network datasets. Using this method to generate accurate\nrecommendations for many applications is a matter of further study that appears\nvery promising. The code for all the experiments has been made public.\n", "link": "http://arxiv.org/abs/2403.08613v1", "date": "2024-03-13", "relevancy": 1.8947, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4882}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4878}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4538}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Link%20Prediction%20for%20Social%20Networks%20using%20Representation%20Learning%20and%0A%20%20Heuristic-based%20Features&body=Title%3A%20Link%20Prediction%20for%20Social%20Networks%20using%20Representation%20Learning%20and%0A%20%20Heuristic-based%20Features%0AAuthor%3A%20Samarth%20Khanna%20and%20Sree%20Bhattacharyya%20and%20Sudipto%20Ghosh%20and%20Kushagra%20Agarwal%20and%20Asit%20Kumar%20Das%0AAbstract%3A%20%20%20The%20exponential%20growth%20in%20scale%20and%20relevance%20of%20social%20networks%20enable%20them%0Ato%20provide%20expansive%20insights.%20Predicting%20missing%20links%20in%20social%20networks%0Aefficiently%20can%20help%20in%20various%20modern-day%20business%20applications%20ranging%20from%0Agenerating%20recommendations%20to%20influence%20analysis.%20Several%20categories%20of%0Asolutions%20exist%20for%20the%20same.%20Here%2C%20we%20explore%20various%20feature%20extraction%0Atechniques%20to%20generate%20representations%20of%20nodes%20and%20edges%20in%20a%20social%20network%0Athat%20allow%20us%20to%20predict%20missing%20links.%20We%20compare%20the%20results%20of%20using%20ten%0Afeature%20extraction%20techniques%20categorized%20across%20Structural%20embeddings%2C%0ANeighborhood-based%20embeddings%2C%20Graph%20Neural%20Networks%2C%20and%20Graph%20Heuristics%2C%0Afollowed%20by%20modeling%20with%20ensemble%20classifiers%20and%20custom%20Neural%20Networks.%0AFurther%2C%20we%20propose%20combining%20heuristic-based%20features%20and%20learned%0Arepresentations%20that%20demonstrate%20improved%20performance%20for%20the%20link%20prediction%0Atask%20on%20social%20network%20datasets.%20Using%20this%20method%20to%20generate%20accurate%0Arecommendations%20for%20many%20applications%20is%20a%20matter%20of%20further%20study%20that%20appears%0Avery%20promising.%20The%20code%20for%20all%20the%20experiments%20has%20been%20made%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08613v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Link%20Prediction%20for%20Social%20Networks%20using%20Representation%20Learning%20and%0A%20%20Heuristic-based%20Features&entry.906535625=Samarth%20Khanna%20and%20Sree%20Bhattacharyya%20and%20Sudipto%20Ghosh%20and%20Kushagra%20Agarwal%20and%20Asit%20Kumar%20Das&entry.1292438233=%20%20The%20exponential%20growth%20in%20scale%20and%20relevance%20of%20social%20networks%20enable%20them%0Ato%20provide%20expansive%20insights.%20Predicting%20missing%20links%20in%20social%20networks%0Aefficiently%20can%20help%20in%20various%20modern-day%20business%20applications%20ranging%20from%0Agenerating%20recommendations%20to%20influence%20analysis.%20Several%20categories%20of%0Asolutions%20exist%20for%20the%20same.%20Here%2C%20we%20explore%20various%20feature%20extraction%0Atechniques%20to%20generate%20representations%20of%20nodes%20and%20edges%20in%20a%20social%20network%0Athat%20allow%20us%20to%20predict%20missing%20links.%20We%20compare%20the%20results%20of%20using%20ten%0Afeature%20extraction%20techniques%20categorized%20across%20Structural%20embeddings%2C%0ANeighborhood-based%20embeddings%2C%20Graph%20Neural%20Networks%2C%20and%20Graph%20Heuristics%2C%0Afollowed%20by%20modeling%20with%20ensemble%20classifiers%20and%20custom%20Neural%20Networks.%0AFurther%2C%20we%20propose%20combining%20heuristic-based%20features%20and%20learned%0Arepresentations%20that%20demonstrate%20improved%20performance%20for%20the%20link%20prediction%0Atask%20on%20social%20network%20datasets.%20Using%20this%20method%20to%20generate%20accurate%0Arecommendations%20for%20many%20applications%20is%20a%20matter%20of%20further%20study%20that%20appears%0Avery%20promising.%20The%20code%20for%20all%20the%20experiments%20has%20been%20made%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08613v1&entry.124074799=Read"},
{"title": "Machine Learning Optimized Orthogonal Basis Piecewise Polynomial\n  Approximation", "author": "Hannes Waclawek and Stefan Huber", "abstract": "  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,\nlike trajectory planning, to approximate position profiles given in the form of\na set of points. While the approximation target along with domain-specific\nrequirements, like Ck -continuity, can be formulated as a system of equations\nand a result can be computed directly, such closed-form solutions posses\nlimited flexibility with respect to polynomial degrees, polynomial bases or\nadding further domain-specific requirements. Sufficiently complex optimization\ngoals soon call for the use of numerical methods, like gradient descent. Since\ngradient descent lies at the heart of training Artificial Neural Networks\n(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set\nof gradient-based optimizers potentially suitable for a wide range of\noptimization problems beyond the training task for ANNs. Our approach is to\nutilize the versatility of PP models and combine it with the potential of\nmodern ML optimizers for the use in function approximation in 1D trajectory\nplanning in the context of electronic cam design. We utilize available\noptimizers of the ML framework TensorFlow directly, outside of the scope of\nANNs, to optimize model parameters of our PP model. In this paper, we show how\nan orthogonal polynomial basis contributes to improving approximation and\ncontinuity optimization performance. Utilizing Chebyshev polynomials of the\nfirst kind, we develop a novel regularization approach enabling clearly\nimproved convergence behavior. We show that, using this regularization\napproach, Chebyshev basis performs better than power basis for all relevant\noptimizers in the combined approximation and continuity optimization setting\nand demonstrate usability of the presented approach within the electronic cam\ndomain.\n", "link": "http://arxiv.org/abs/2403.08579v1", "date": "2024-03-13", "relevancy": 1.8941, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.508}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4996}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4286}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Optimized%20Orthogonal%20Basis%20Piecewise%20Polynomial%0A%20%20Approximation&body=Title%3A%20Machine%20Learning%20Optimized%20Orthogonal%20Basis%20Piecewise%20Polynomial%0A%20%20Approximation%0AAuthor%3A%20Hannes%20Waclawek%20and%20Stefan%20Huber%0AAbstract%3A%20%20%20Piecewise%20Polynomials%20%28PPs%29%20are%20utilized%20in%20several%20engineering%20disciplines%2C%0Alike%20trajectory%20planning%2C%20to%20approximate%20position%20profiles%20given%20in%20the%20form%20of%0Aa%20set%20of%20points.%20While%20the%20approximation%20target%20along%20with%20domain-specific%0Arequirements%2C%20like%20Ck%20-continuity%2C%20can%20be%20formulated%20as%20a%20system%20of%20equations%0Aand%20a%20result%20can%20be%20computed%20directly%2C%20such%20closed-form%20solutions%20posses%0Alimited%20flexibility%20with%20respect%20to%20polynomial%20degrees%2C%20polynomial%20bases%20or%0Aadding%20further%20domain-specific%20requirements.%20Sufficiently%20complex%20optimization%0Agoals%20soon%20call%20for%20the%20use%20of%20numerical%20methods%2C%20like%20gradient%20descent.%20Since%0Agradient%20descent%20lies%20at%20the%20heart%20of%20training%20Artificial%20Neural%20Networks%0A%28ANNs%29%2C%20modern%20Machine%20Learning%20%28ML%29%20frameworks%20like%20TensorFlow%20come%20with%20a%20set%0Aof%20gradient-based%20optimizers%20potentially%20suitable%20for%20a%20wide%20range%20of%0Aoptimization%20problems%20beyond%20the%20training%20task%20for%20ANNs.%20Our%20approach%20is%20to%0Autilize%20the%20versatility%20of%20PP%20models%20and%20combine%20it%20with%20the%20potential%20of%0Amodern%20ML%20optimizers%20for%20the%20use%20in%20function%20approximation%20in%201D%20trajectory%0Aplanning%20in%20the%20context%20of%20electronic%20cam%20design.%20We%20utilize%20available%0Aoptimizers%20of%20the%20ML%20framework%20TensorFlow%20directly%2C%20outside%20of%20the%20scope%20of%0AANNs%2C%20to%20optimize%20model%20parameters%20of%20our%20PP%20model.%20In%20this%20paper%2C%20we%20show%20how%0Aan%20orthogonal%20polynomial%20basis%20contributes%20to%20improving%20approximation%20and%0Acontinuity%20optimization%20performance.%20Utilizing%20Chebyshev%20polynomials%20of%20the%0Afirst%20kind%2C%20we%20develop%20a%20novel%20regularization%20approach%20enabling%20clearly%0Aimproved%20convergence%20behavior.%20We%20show%20that%2C%20using%20this%20regularization%0Aapproach%2C%20Chebyshev%20basis%20performs%20better%20than%20power%20basis%20for%20all%20relevant%0Aoptimizers%20in%20the%20combined%20approximation%20and%20continuity%20optimization%20setting%0Aand%20demonstrate%20usability%20of%20the%20presented%20approach%20within%20the%20electronic%20cam%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08579v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Optimized%20Orthogonal%20Basis%20Piecewise%20Polynomial%0A%20%20Approximation&entry.906535625=Hannes%20Waclawek%20and%20Stefan%20Huber&entry.1292438233=%20%20Piecewise%20Polynomials%20%28PPs%29%20are%20utilized%20in%20several%20engineering%20disciplines%2C%0Alike%20trajectory%20planning%2C%20to%20approximate%20position%20profiles%20given%20in%20the%20form%20of%0Aa%20set%20of%20points.%20While%20the%20approximation%20target%20along%20with%20domain-specific%0Arequirements%2C%20like%20Ck%20-continuity%2C%20can%20be%20formulated%20as%20a%20system%20of%20equations%0Aand%20a%20result%20can%20be%20computed%20directly%2C%20such%20closed-form%20solutions%20posses%0Alimited%20flexibility%20with%20respect%20to%20polynomial%20degrees%2C%20polynomial%20bases%20or%0Aadding%20further%20domain-specific%20requirements.%20Sufficiently%20complex%20optimization%0Agoals%20soon%20call%20for%20the%20use%20of%20numerical%20methods%2C%20like%20gradient%20descent.%20Since%0Agradient%20descent%20lies%20at%20the%20heart%20of%20training%20Artificial%20Neural%20Networks%0A%28ANNs%29%2C%20modern%20Machine%20Learning%20%28ML%29%20frameworks%20like%20TensorFlow%20come%20with%20a%20set%0Aof%20gradient-based%20optimizers%20potentially%20suitable%20for%20a%20wide%20range%20of%0Aoptimization%20problems%20beyond%20the%20training%20task%20for%20ANNs.%20Our%20approach%20is%20to%0Autilize%20the%20versatility%20of%20PP%20models%20and%20combine%20it%20with%20the%20potential%20of%0Amodern%20ML%20optimizers%20for%20the%20use%20in%20function%20approximation%20in%201D%20trajectory%0Aplanning%20in%20the%20context%20of%20electronic%20cam%20design.%20We%20utilize%20available%0Aoptimizers%20of%20the%20ML%20framework%20TensorFlow%20directly%2C%20outside%20of%20the%20scope%20of%0AANNs%2C%20to%20optimize%20model%20parameters%20of%20our%20PP%20model.%20In%20this%20paper%2C%20we%20show%20how%0Aan%20orthogonal%20polynomial%20basis%20contributes%20to%20improving%20approximation%20and%0Acontinuity%20optimization%20performance.%20Utilizing%20Chebyshev%20polynomials%20of%20the%0Afirst%20kind%2C%20we%20develop%20a%20novel%20regularization%20approach%20enabling%20clearly%0Aimproved%20convergence%20behavior.%20We%20show%20that%2C%20using%20this%20regularization%0Aapproach%2C%20Chebyshev%20basis%20performs%20better%20than%20power%20basis%20for%20all%20relevant%0Aoptimizers%20in%20the%20combined%20approximation%20and%20continuity%20optimization%20setting%0Aand%20demonstrate%20usability%20of%20the%20presented%20approach%20within%20the%20electronic%20cam%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08579v1&entry.124074799=Read"},
{"title": "When can we Approximate Wide Contrastive Models with Neural Tangent\n  Kernels and Principal Component Analysis?", "author": "Gautham Govind Anil and Pascal Esser and Debarghya Ghoshdastidar", "abstract": "  Contrastive learning is a paradigm for learning representations from\nunlabelled data that has been highly successful for image and text data.\nSeveral recent works have examined contrastive losses to claim that contrastive\nmodels effectively learn spectral embeddings, while few works show relations\nbetween (wide) contrastive models and kernel principal component analysis\n(PCA). However, it is not known if trained contrastive models indeed correspond\nto kernel methods or PCA. In this work, we analyze the training dynamics of\ntwo-layer contrastive models, with non-linear activation, and answer when these\nmodels are close to PCA or kernel methods. It is well known in the supervised\nsetting that neural networks are equivalent to neural tangent kernel (NTK)\nmachines, and that the NTK of infinitely wide networks remains constant during\ntraining. We provide the first convergence results of NTK for contrastive\nlosses, and present a nuanced picture: NTK of wide networks remains almost\nconstant for cosine similarity based contrastive losses, but not for losses\nbased on dot product similarity. We further study the training dynamics of\ncontrastive models with orthogonality constraints on output layer, which is\nimplicitly assumed in works relating contrastive learning to spectral\nembedding. Our deviation bounds suggest that representations learned by\ncontrastive models are close to the principal components of a certain matrix\ncomputed from random features. We empirically show that our theoretical results\npossibly hold beyond two-layer networks.\n", "link": "http://arxiv.org/abs/2403.08673v1", "date": "2024-03-13", "relevancy": 1.8834, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4747}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4706}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4671}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20When%20can%20we%20Approximate%20Wide%20Contrastive%20Models%20with%20Neural%20Tangent%0A%20%20Kernels%20and%20Principal%20Component%20Analysis%3F&body=Title%3A%20When%20can%20we%20Approximate%20Wide%20Contrastive%20Models%20with%20Neural%20Tangent%0A%20%20Kernels%20and%20Principal%20Component%20Analysis%3F%0AAuthor%3A%20Gautham%20Govind%20Anil%20and%20Pascal%20Esser%20and%20Debarghya%20Ghoshdastidar%0AAbstract%3A%20%20%20Contrastive%20learning%20is%20a%20paradigm%20for%20learning%20representations%20from%0Aunlabelled%20data%20that%20has%20been%20highly%20successful%20for%20image%20and%20text%20data.%0ASeveral%20recent%20works%20have%20examined%20contrastive%20losses%20to%20claim%20that%20contrastive%0Amodels%20effectively%20learn%20spectral%20embeddings%2C%20while%20few%20works%20show%20relations%0Abetween%20%28wide%29%20contrastive%20models%20and%20kernel%20principal%20component%20analysis%0A%28PCA%29.%20However%2C%20it%20is%20not%20known%20if%20trained%20contrastive%20models%20indeed%20correspond%0Ato%20kernel%20methods%20or%20PCA.%20In%20this%20work%2C%20we%20analyze%20the%20training%20dynamics%20of%0Atwo-layer%20contrastive%20models%2C%20with%20non-linear%20activation%2C%20and%20answer%20when%20these%0Amodels%20are%20close%20to%20PCA%20or%20kernel%20methods.%20It%20is%20well%20known%20in%20the%20supervised%0Asetting%20that%20neural%20networks%20are%20equivalent%20to%20neural%20tangent%20kernel%20%28NTK%29%0Amachines%2C%20and%20that%20the%20NTK%20of%20infinitely%20wide%20networks%20remains%20constant%20during%0Atraining.%20We%20provide%20the%20first%20convergence%20results%20of%20NTK%20for%20contrastive%0Alosses%2C%20and%20present%20a%20nuanced%20picture%3A%20NTK%20of%20wide%20networks%20remains%20almost%0Aconstant%20for%20cosine%20similarity%20based%20contrastive%20losses%2C%20but%20not%20for%20losses%0Abased%20on%20dot%20product%20similarity.%20We%20further%20study%20the%20training%20dynamics%20of%0Acontrastive%20models%20with%20orthogonality%20constraints%20on%20output%20layer%2C%20which%20is%0Aimplicitly%20assumed%20in%20works%20relating%20contrastive%20learning%20to%20spectral%0Aembedding.%20Our%20deviation%20bounds%20suggest%20that%20representations%20learned%20by%0Acontrastive%20models%20are%20close%20to%20the%20principal%20components%20of%20a%20certain%20matrix%0Acomputed%20from%20random%20features.%20We%20empirically%20show%20that%20our%20theoretical%20results%0Apossibly%20hold%20beyond%20two-layer%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08673v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20can%20we%20Approximate%20Wide%20Contrastive%20Models%20with%20Neural%20Tangent%0A%20%20Kernels%20and%20Principal%20Component%20Analysis%3F&entry.906535625=Gautham%20Govind%20Anil%20and%20Pascal%20Esser%20and%20Debarghya%20Ghoshdastidar&entry.1292438233=%20%20Contrastive%20learning%20is%20a%20paradigm%20for%20learning%20representations%20from%0Aunlabelled%20data%20that%20has%20been%20highly%20successful%20for%20image%20and%20text%20data.%0ASeveral%20recent%20works%20have%20examined%20contrastive%20losses%20to%20claim%20that%20contrastive%0Amodels%20effectively%20learn%20spectral%20embeddings%2C%20while%20few%20works%20show%20relations%0Abetween%20%28wide%29%20contrastive%20models%20and%20kernel%20principal%20component%20analysis%0A%28PCA%29.%20However%2C%20it%20is%20not%20known%20if%20trained%20contrastive%20models%20indeed%20correspond%0Ato%20kernel%20methods%20or%20PCA.%20In%20this%20work%2C%20we%20analyze%20the%20training%20dynamics%20of%0Atwo-layer%20contrastive%20models%2C%20with%20non-linear%20activation%2C%20and%20answer%20when%20these%0Amodels%20are%20close%20to%20PCA%20or%20kernel%20methods.%20It%20is%20well%20known%20in%20the%20supervised%0Asetting%20that%20neural%20networks%20are%20equivalent%20to%20neural%20tangent%20kernel%20%28NTK%29%0Amachines%2C%20and%20that%20the%20NTK%20of%20infinitely%20wide%20networks%20remains%20constant%20during%0Atraining.%20We%20provide%20the%20first%20convergence%20results%20of%20NTK%20for%20contrastive%0Alosses%2C%20and%20present%20a%20nuanced%20picture%3A%20NTK%20of%20wide%20networks%20remains%20almost%0Aconstant%20for%20cosine%20similarity%20based%20contrastive%20losses%2C%20but%20not%20for%20losses%0Abased%20on%20dot%20product%20similarity.%20We%20further%20study%20the%20training%20dynamics%20of%0Acontrastive%20models%20with%20orthogonality%20constraints%20on%20output%20layer%2C%20which%20is%0Aimplicitly%20assumed%20in%20works%20relating%20contrastive%20learning%20to%20spectral%0Aembedding.%20Our%20deviation%20bounds%20suggest%20that%20representations%20learned%20by%0Acontrastive%20models%20are%20close%20to%20the%20principal%20components%20of%20a%20certain%20matrix%0Acomputed%20from%20random%20features.%20We%20empirically%20show%20that%20our%20theoretical%20results%0Apossibly%20hold%20beyond%20two-layer%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08673v1&entry.124074799=Read"},
{"title": "Implicit Regularization of Gradient Flow on One-Layer Softmax Attention", "author": "Heejune Sheen and Siyu Chen and Tianhao Wang and Harrison H. Zhou", "abstract": "  We study gradient flow on the exponential loss for a classification problem\nwith a one-layer softmax attention model, where the key and query weight\nmatrices are trained separately. Under a separability assumption on the data,\nwe show that when gradient flow achieves the minimal loss value, it further\nimplicitly minimizes the nuclear norm of the product of the key and query\nweight matrices. Such implicit regularization can be described by a Support\nVector Machine (SVM) problem with respect to the attention weights. This\nfinding contrasts with prior results showing that the gradient descent induces\nan implicit regularization on the Frobenius norm on the product weight matrix\nwhen the key and query matrices are combined into a single weight matrix for\ntraining. For diagonal key and query matrices, our analysis builds upon the\nreparameterization technique and exploits approximate KKT conditions of the SVM\nassociated with the classification data. Moreover, the results are extended to\ngeneral weights configurations given proper alignment of the weight matrices'\nsingular spaces with the data features at initialization.\n", "link": "http://arxiv.org/abs/2403.08699v1", "date": "2024-03-13", "relevancy": 1.8754, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5108}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4473}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Implicit%20Regularization%20of%20Gradient%20Flow%20on%20One-Layer%20Softmax%20Attention&body=Title%3A%20Implicit%20Regularization%20of%20Gradient%20Flow%20on%20One-Layer%20Softmax%20Attention%0AAuthor%3A%20Heejune%20Sheen%20and%20Siyu%20Chen%20and%20Tianhao%20Wang%20and%20Harrison%20H.%20Zhou%0AAbstract%3A%20%20%20We%20study%20gradient%20flow%20on%20the%20exponential%20loss%20for%20a%20classification%20problem%0Awith%20a%20one-layer%20softmax%20attention%20model%2C%20where%20the%20key%20and%20query%20weight%0Amatrices%20are%20trained%20separately.%20Under%20a%20separability%20assumption%20on%20the%20data%2C%0Awe%20show%20that%20when%20gradient%20flow%20achieves%20the%20minimal%20loss%20value%2C%20it%20further%0Aimplicitly%20minimizes%20the%20nuclear%20norm%20of%20the%20product%20of%20the%20key%20and%20query%0Aweight%20matrices.%20Such%20implicit%20regularization%20can%20be%20described%20by%20a%20Support%0AVector%20Machine%20%28SVM%29%20problem%20with%20respect%20to%20the%20attention%20weights.%20This%0Afinding%20contrasts%20with%20prior%20results%20showing%20that%20the%20gradient%20descent%20induces%0Aan%20implicit%20regularization%20on%20the%20Frobenius%20norm%20on%20the%20product%20weight%20matrix%0Awhen%20the%20key%20and%20query%20matrices%20are%20combined%20into%20a%20single%20weight%20matrix%20for%0Atraining.%20For%20diagonal%20key%20and%20query%20matrices%2C%20our%20analysis%20builds%20upon%20the%0Areparameterization%20technique%20and%20exploits%20approximate%20KKT%20conditions%20of%20the%20SVM%0Aassociated%20with%20the%20classification%20data.%20Moreover%2C%20the%20results%20are%20extended%20to%0Ageneral%20weights%20configurations%20given%20proper%20alignment%20of%20the%20weight%20matrices%27%0Asingular%20spaces%20with%20the%20data%20features%20at%20initialization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08699v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Regularization%20of%20Gradient%20Flow%20on%20One-Layer%20Softmax%20Attention&entry.906535625=Heejune%20Sheen%20and%20Siyu%20Chen%20and%20Tianhao%20Wang%20and%20Harrison%20H.%20Zhou&entry.1292438233=%20%20We%20study%20gradient%20flow%20on%20the%20exponential%20loss%20for%20a%20classification%20problem%0Awith%20a%20one-layer%20softmax%20attention%20model%2C%20where%20the%20key%20and%20query%20weight%0Amatrices%20are%20trained%20separately.%20Under%20a%20separability%20assumption%20on%20the%20data%2C%0Awe%20show%20that%20when%20gradient%20flow%20achieves%20the%20minimal%20loss%20value%2C%20it%20further%0Aimplicitly%20minimizes%20the%20nuclear%20norm%20of%20the%20product%20of%20the%20key%20and%20query%0Aweight%20matrices.%20Such%20implicit%20regularization%20can%20be%20described%20by%20a%20Support%0AVector%20Machine%20%28SVM%29%20problem%20with%20respect%20to%20the%20attention%20weights.%20This%0Afinding%20contrasts%20with%20prior%20results%20showing%20that%20the%20gradient%20descent%20induces%0Aan%20implicit%20regularization%20on%20the%20Frobenius%20norm%20on%20the%20product%20weight%20matrix%0Awhen%20the%20key%20and%20query%20matrices%20are%20combined%20into%20a%20single%20weight%20matrix%20for%0Atraining.%20For%20diagonal%20key%20and%20query%20matrices%2C%20our%20analysis%20builds%20upon%20the%0Areparameterization%20technique%20and%20exploits%20approximate%20KKT%20conditions%20of%20the%20SVM%0Aassociated%20with%20the%20classification%20data.%20Moreover%2C%20the%20results%20are%20extended%20to%0Ageneral%20weights%20configurations%20given%20proper%20alignment%20of%20the%20weight%20matrices%27%0Asingular%20spaces%20with%20the%20data%20features%20at%20initialization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08699v1&entry.124074799=Read"},
{"title": "CoLiDE: Concomitant Linear DAG Estimation", "author": "Seyed Saman Saboksayr and Gonzalo Mateos and Mariano Tepper", "abstract": "  We deal with the combinatorial problem of learning directed acyclic graph\n(DAG) structure from observational data adhering to a linear structural\nequation model (SEM). Leveraging advances in differentiable, nonconvex\ncharacterizations of acyclicity, recent efforts have advocated a continuous\nconstrained optimization paradigm to efficiently explore the space of DAGs.\nMost existing methods employ lasso-type score functions to guide this search,\nwhich (i) require expensive penalty parameter retuning when the\n$\\textit{unknown}$ SEM noise variances change across problem instances; and\n(ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we\npropose a new convex score function for sparsity-aware learning of linear DAGs,\nwhich incorporates concomitant estimation of scale and thus effectively\ndecouples the sparsity parameter from the exogenous noise levels.\nRegularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE\n($\\textbf{Co}$ncomitant $\\textbf{Li}$near $\\textbf{D}$AG\n$\\textbf{E}$stimation), a regression-based criterion amenable to efficient\ngradient computation and closed-form estimation of noise variances in\nheteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods\nwithout incurring added complexity, especially when the DAGs are larger and the\nnoise level profile is heterogeneous. We also find CoLiDE exhibits enhanced\nstability manifested via reduced standard deviations in several domain-specific\nmetrics, underscoring the robustness of our novel linear DAG estimator.\n", "link": "http://arxiv.org/abs/2310.02895v2", "date": "2024-03-13", "relevancy": 1.8657, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4785}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4708}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4572}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20CoLiDE%3A%20Concomitant%20Linear%20DAG%20Estimation&body=Title%3A%20CoLiDE%3A%20Concomitant%20Linear%20DAG%20Estimation%0AAuthor%3A%20Seyed%20Saman%20Saboksayr%20and%20Gonzalo%20Mateos%20and%20Mariano%20Tepper%0AAbstract%3A%20%20%20We%20deal%20with%20the%20combinatorial%20problem%20of%20learning%20directed%20acyclic%20graph%0A%28DAG%29%20structure%20from%20observational%20data%20adhering%20to%20a%20linear%20structural%0Aequation%20model%20%28SEM%29.%20Leveraging%20advances%20in%20differentiable%2C%20nonconvex%0Acharacterizations%20of%20acyclicity%2C%20recent%20efforts%20have%20advocated%20a%20continuous%0Aconstrained%20optimization%20paradigm%20to%20efficiently%20explore%20the%20space%20of%20DAGs.%0AMost%20existing%20methods%20employ%20lasso-type%20score%20functions%20to%20guide%20this%20search%2C%0Awhich%20%28i%29%20require%20expensive%20penalty%20parameter%20retuning%20when%20the%0A%24%5Ctextit%7Bunknown%7D%24%20SEM%20noise%20variances%20change%20across%20problem%20instances%3B%20and%0A%28ii%29%20implicitly%20rely%20on%20limiting%20homoscedasticity%20assumptions.%20In%20this%20work%2C%20we%0Apropose%20a%20new%20convex%20score%20function%20for%20sparsity-aware%20learning%20of%20linear%20DAGs%2C%0Awhich%20incorporates%20concomitant%20estimation%20of%20scale%20and%20thus%20effectively%0Adecouples%20the%20sparsity%20parameter%20from%20the%20exogenous%20noise%20levels.%0ARegularization%20via%20a%20smooth%2C%20nonconvex%20acyclicity%20penalty%20term%20yields%20CoLiDE%0A%28%24%5Ctextbf%7BCo%7D%24ncomitant%20%24%5Ctextbf%7BLi%7D%24near%20%24%5Ctextbf%7BD%7D%24AG%0A%24%5Ctextbf%7BE%7D%24stimation%29%2C%20a%20regression-based%20criterion%20amenable%20to%20efficient%0Agradient%20computation%20and%20closed-form%20estimation%20of%20noise%20variances%20in%0Aheteroscedastic%20scenarios.%20Our%20algorithm%20outperforms%20state-of-the-art%20methods%0Awithout%20incurring%20added%20complexity%2C%20especially%20when%20the%20DAGs%20are%20larger%20and%20the%0Anoise%20level%20profile%20is%20heterogeneous.%20We%20also%20find%20CoLiDE%20exhibits%20enhanced%0Astability%20manifested%20via%20reduced%20standard%20deviations%20in%20several%20domain-specific%0Ametrics%2C%20underscoring%20the%20robustness%20of%20our%20novel%20linear%20DAG%20estimator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02895v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoLiDE%3A%20Concomitant%20Linear%20DAG%20Estimation&entry.906535625=Seyed%20Saman%20Saboksayr%20and%20Gonzalo%20Mateos%20and%20Mariano%20Tepper&entry.1292438233=%20%20We%20deal%20with%20the%20combinatorial%20problem%20of%20learning%20directed%20acyclic%20graph%0A%28DAG%29%20structure%20from%20observational%20data%20adhering%20to%20a%20linear%20structural%0Aequation%20model%20%28SEM%29.%20Leveraging%20advances%20in%20differentiable%2C%20nonconvex%0Acharacterizations%20of%20acyclicity%2C%20recent%20efforts%20have%20advocated%20a%20continuous%0Aconstrained%20optimization%20paradigm%20to%20efficiently%20explore%20the%20space%20of%20DAGs.%0AMost%20existing%20methods%20employ%20lasso-type%20score%20functions%20to%20guide%20this%20search%2C%0Awhich%20%28i%29%20require%20expensive%20penalty%20parameter%20retuning%20when%20the%0A%24%5Ctextit%7Bunknown%7D%24%20SEM%20noise%20variances%20change%20across%20problem%20instances%3B%20and%0A%28ii%29%20implicitly%20rely%20on%20limiting%20homoscedasticity%20assumptions.%20In%20this%20work%2C%20we%0Apropose%20a%20new%20convex%20score%20function%20for%20sparsity-aware%20learning%20of%20linear%20DAGs%2C%0Awhich%20incorporates%20concomitant%20estimation%20of%20scale%20and%20thus%20effectively%0Adecouples%20the%20sparsity%20parameter%20from%20the%20exogenous%20noise%20levels.%0ARegularization%20via%20a%20smooth%2C%20nonconvex%20acyclicity%20penalty%20term%20yields%20CoLiDE%0A%28%24%5Ctextbf%7BCo%7D%24ncomitant%20%24%5Ctextbf%7BLi%7D%24near%20%24%5Ctextbf%7BD%7D%24AG%0A%24%5Ctextbf%7BE%7D%24stimation%29%2C%20a%20regression-based%20criterion%20amenable%20to%20efficient%0Agradient%20computation%20and%20closed-form%20estimation%20of%20noise%20variances%20in%0Aheteroscedastic%20scenarios.%20Our%20algorithm%20outperforms%20state-of-the-art%20methods%0Awithout%20incurring%20added%20complexity%2C%20especially%20when%20the%20DAGs%20are%20larger%20and%20the%0Anoise%20level%20profile%20is%20heterogeneous.%20We%20also%20find%20CoLiDE%20exhibits%20enhanced%0Astability%20manifested%20via%20reduced%20standard%20deviations%20in%20several%20domain-specific%0Ametrics%2C%20underscoring%20the%20robustness%20of%20our%20novel%20linear%20DAG%20estimator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02895v2&entry.124074799=Read"},
{"title": "Human Alignment of Large Language Models through Online Preference\n  Optimisation", "author": "Daniele Calandriello and Daniel Guo and Remi Munos and Mark Rowland and Yunhao Tang and Bernardo Avila Pires and Pierre Harvey Richemond and Charline Le Lan and Michal Valko and Tianqi Liu and Rishabh Joshi and Zeyu Zheng and Bilal Piot", "abstract": "  Ensuring alignment of language models' outputs with human preferences is\ncritical to guarantee a useful, safe, and pleasant user experience. Thus, human\nalignment has been extensively studied recently and several methods such as\nReinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation\n(DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper,\nour contribution is two-fold. First, we show the equivalence between two recent\nalignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror\nDescent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD,\nthat leverages the regularised sampling approach proposed by Nash-MD.\n  This equivalence may seem surprising at first sight, since IPO is an offline\nmethod whereas Nash-MD is an online method using a preference model. However,\nthis equivalence can be proven when we consider the online version of IPO, that\nis when both generations are sampled by the online policy and annotated by a\ntrained preference model. Optimising the IPO loss with such a stream of data\nbecomes then equivalent to finding the Nash equilibrium of the preference model\nthrough self-play. Building on this equivalence, we introduce the IPO-MD\nalgorithm that generates data with a mixture policy (between the online and\nreference policy) similarly as the general Nash-MD algorithm. We compare\nonline-IPO and IPO-MD to different online versions of existing losses on\npreference data such as DPO and SLiC on a summarisation task.\n", "link": "http://arxiv.org/abs/2403.08635v1", "date": "2024-03-13", "relevancy": 1.8521, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4827}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4591}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Human%20Alignment%20of%20Large%20Language%20Models%20through%20Online%20Preference%0A%20%20Optimisation&body=Title%3A%20Human%20Alignment%20of%20Large%20Language%20Models%20through%20Online%20Preference%0A%20%20Optimisation%0AAuthor%3A%20Daniele%20Calandriello%20and%20Daniel%20Guo%20and%20Remi%20Munos%20and%20Mark%20Rowland%20and%20Yunhao%20Tang%20and%20Bernardo%20Avila%20Pires%20and%20Pierre%20Harvey%20Richemond%20and%20Charline%20Le%20Lan%20and%20Michal%20Valko%20and%20Tianqi%20Liu%20and%20Rishabh%20Joshi%20and%20Zeyu%20Zheng%20and%20Bilal%20Piot%0AAbstract%3A%20%20%20Ensuring%20alignment%20of%20language%20models%27%20outputs%20with%20human%20preferences%20is%0Acritical%20to%20guarantee%20a%20useful%2C%20safe%2C%20and%20pleasant%20user%20experience.%20Thus%2C%20human%0Aalignment%20has%20been%20extensively%20studied%20recently%20and%20several%20methods%20such%20as%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20Direct%20Policy%20Optimisation%0A%28DPO%29%20and%20Sequence%20Likelihood%20Calibration%20%28SLiC%29%20have%20emerged.%20In%20this%20paper%2C%0Aour%20contribution%20is%20two-fold.%20First%2C%20we%20show%20the%20equivalence%20between%20two%20recent%0Aalignment%20methods%2C%20namely%20Identity%20Policy%20Optimisation%20%28IPO%29%20and%20Nash%20Mirror%0ADescent%20%28Nash-MD%29.%20Second%2C%20we%20introduce%20a%20generalisation%20of%20IPO%2C%20named%20IPO-MD%2C%0Athat%20leverages%20the%20regularised%20sampling%20approach%20proposed%20by%20Nash-MD.%0A%20%20This%20equivalence%20may%20seem%20surprising%20at%20first%20sight%2C%20since%20IPO%20is%20an%20offline%0Amethod%20whereas%20Nash-MD%20is%20an%20online%20method%20using%20a%20preference%20model.%20However%2C%0Athis%20equivalence%20can%20be%20proven%20when%20we%20consider%20the%20online%20version%20of%20IPO%2C%20that%0Ais%20when%20both%20generations%20are%20sampled%20by%20the%20online%20policy%20and%20annotated%20by%20a%0Atrained%20preference%20model.%20Optimising%20the%20IPO%20loss%20with%20such%20a%20stream%20of%20data%0Abecomes%20then%20equivalent%20to%20finding%20the%20Nash%20equilibrium%20of%20the%20preference%20model%0Athrough%20self-play.%20Building%20on%20this%20equivalence%2C%20we%20introduce%20the%20IPO-MD%0Aalgorithm%20that%20generates%20data%20with%20a%20mixture%20policy%20%28between%20the%20online%20and%0Areference%20policy%29%20similarly%20as%20the%20general%20Nash-MD%20algorithm.%20We%20compare%0Aonline-IPO%20and%20IPO-MD%20to%20different%20online%20versions%20of%20existing%20losses%20on%0Apreference%20data%20such%20as%20DPO%20and%20SLiC%20on%20a%20summarisation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08635v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Alignment%20of%20Large%20Language%20Models%20through%20Online%20Preference%0A%20%20Optimisation&entry.906535625=Daniele%20Calandriello%20and%20Daniel%20Guo%20and%20Remi%20Munos%20and%20Mark%20Rowland%20and%20Yunhao%20Tang%20and%20Bernardo%20Avila%20Pires%20and%20Pierre%20Harvey%20Richemond%20and%20Charline%20Le%20Lan%20and%20Michal%20Valko%20and%20Tianqi%20Liu%20and%20Rishabh%20Joshi%20and%20Zeyu%20Zheng%20and%20Bilal%20Piot&entry.1292438233=%20%20Ensuring%20alignment%20of%20language%20models%27%20outputs%20with%20human%20preferences%20is%0Acritical%20to%20guarantee%20a%20useful%2C%20safe%2C%20and%20pleasant%20user%20experience.%20Thus%2C%20human%0Aalignment%20has%20been%20extensively%20studied%20recently%20and%20several%20methods%20such%20as%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20Direct%20Policy%20Optimisation%0A%28DPO%29%20and%20Sequence%20Likelihood%20Calibration%20%28SLiC%29%20have%20emerged.%20In%20this%20paper%2C%0Aour%20contribution%20is%20two-fold.%20First%2C%20we%20show%20the%20equivalence%20between%20two%20recent%0Aalignment%20methods%2C%20namely%20Identity%20Policy%20Optimisation%20%28IPO%29%20and%20Nash%20Mirror%0ADescent%20%28Nash-MD%29.%20Second%2C%20we%20introduce%20a%20generalisation%20of%20IPO%2C%20named%20IPO-MD%2C%0Athat%20leverages%20the%20regularised%20sampling%20approach%20proposed%20by%20Nash-MD.%0A%20%20This%20equivalence%20may%20seem%20surprising%20at%20first%20sight%2C%20since%20IPO%20is%20an%20offline%0Amethod%20whereas%20Nash-MD%20is%20an%20online%20method%20using%20a%20preference%20model.%20However%2C%0Athis%20equivalence%20can%20be%20proven%20when%20we%20consider%20the%20online%20version%20of%20IPO%2C%20that%0Ais%20when%20both%20generations%20are%20sampled%20by%20the%20online%20policy%20and%20annotated%20by%20a%0Atrained%20preference%20model.%20Optimising%20the%20IPO%20loss%20with%20such%20a%20stream%20of%20data%0Abecomes%20then%20equivalent%20to%20finding%20the%20Nash%20equilibrium%20of%20the%20preference%20model%0Athrough%20self-play.%20Building%20on%20this%20equivalence%2C%20we%20introduce%20the%20IPO-MD%0Aalgorithm%20that%20generates%20data%20with%20a%20mixture%20policy%20%28between%20the%20online%20and%0Areference%20policy%29%20similarly%20as%20the%20general%20Nash-MD%20algorithm.%20We%20compare%0Aonline-IPO%20and%20IPO-MD%20to%20different%20online%20versions%20of%20existing%20losses%20on%0Apreference%20data%20such%20as%20DPO%20and%20SLiC%20on%20a%20summarisation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08635v1&entry.124074799=Read"},
{"title": "Regret Analysis of Policy Optimization over Submanifolds for Linearly\n  Constrained Online LQG", "author": "Ting-Jui Chang and Shahin Shahrampour", "abstract": "  Recent advancement in online optimization and control has provided novel\ntools to study online linear quadratic regulator (LQR) problems, where cost\nmatrices are varying adversarially over time. However, the controller\nparameterization of existing works may not satisfy practical conditions like\nsparsity due to physical connections. In this work, we study online linear\nquadratic Gaussian problems with a given linear constraint imposed on the\ncontroller. Inspired by the recent work of [1] which proposed, for a linearly\nconstrained policy optimization of an offline LQR, a second order method\nequipped with a Riemannian metric that emerges naturally in the context of\noptimal control problems, we propose online optimistic Newton on manifold\n(OONM) which provides an online controller based on the prediction on the first\nand second order information of the function sequence. To quantify the proposed\nalgorithm, we leverage the notion of regret defined as the sub-optimality of\nits cumulative cost to that of a (locally) minimizing controller sequence and\nprovide the regret bound in terms of the path-length of the minimizer sequence.\nSimulation results are also provided to verify the property of OONM.\n", "link": "http://arxiv.org/abs/2403.08553v1", "date": "2024-03-13", "relevancy": 1.8317, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4673}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4624}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4468}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Regret%20Analysis%20of%20Policy%20Optimization%20over%20Submanifolds%20for%20Linearly%0A%20%20Constrained%20Online%20LQG&body=Title%3A%20Regret%20Analysis%20of%20Policy%20Optimization%20over%20Submanifolds%20for%20Linearly%0A%20%20Constrained%20Online%20LQG%0AAuthor%3A%20Ting-Jui%20Chang%20and%20Shahin%20Shahrampour%0AAbstract%3A%20%20%20Recent%20advancement%20in%20online%20optimization%20and%20control%20has%20provided%20novel%0Atools%20to%20study%20online%20linear%20quadratic%20regulator%20%28LQR%29%20problems%2C%20where%20cost%0Amatrices%20are%20varying%20adversarially%20over%20time.%20However%2C%20the%20controller%0Aparameterization%20of%20existing%20works%20may%20not%20satisfy%20practical%20conditions%20like%0Asparsity%20due%20to%20physical%20connections.%20In%20this%20work%2C%20we%20study%20online%20linear%0Aquadratic%20Gaussian%20problems%20with%20a%20given%20linear%20constraint%20imposed%20on%20the%0Acontroller.%20Inspired%20by%20the%20recent%20work%20of%20%5B1%5D%20which%20proposed%2C%20for%20a%20linearly%0Aconstrained%20policy%20optimization%20of%20an%20offline%20LQR%2C%20a%20second%20order%20method%0Aequipped%20with%20a%20Riemannian%20metric%20that%20emerges%20naturally%20in%20the%20context%20of%0Aoptimal%20control%20problems%2C%20we%20propose%20online%20optimistic%20Newton%20on%20manifold%0A%28OONM%29%20which%20provides%20an%20online%20controller%20based%20on%20the%20prediction%20on%20the%20first%0Aand%20second%20order%20information%20of%20the%20function%20sequence.%20To%20quantify%20the%20proposed%0Aalgorithm%2C%20we%20leverage%20the%20notion%20of%20regret%20defined%20as%20the%20sub-optimality%20of%0Aits%20cumulative%20cost%20to%20that%20of%20a%20%28locally%29%20minimizing%20controller%20sequence%20and%0Aprovide%20the%20regret%20bound%20in%20terms%20of%20the%20path-length%20of%20the%20minimizer%20sequence.%0ASimulation%20results%20are%20also%20provided%20to%20verify%20the%20property%20of%20OONM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08553v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regret%20Analysis%20of%20Policy%20Optimization%20over%20Submanifolds%20for%20Linearly%0A%20%20Constrained%20Online%20LQG&entry.906535625=Ting-Jui%20Chang%20and%20Shahin%20Shahrampour&entry.1292438233=%20%20Recent%20advancement%20in%20online%20optimization%20and%20control%20has%20provided%20novel%0Atools%20to%20study%20online%20linear%20quadratic%20regulator%20%28LQR%29%20problems%2C%20where%20cost%0Amatrices%20are%20varying%20adversarially%20over%20time.%20However%2C%20the%20controller%0Aparameterization%20of%20existing%20works%20may%20not%20satisfy%20practical%20conditions%20like%0Asparsity%20due%20to%20physical%20connections.%20In%20this%20work%2C%20we%20study%20online%20linear%0Aquadratic%20Gaussian%20problems%20with%20a%20given%20linear%20constraint%20imposed%20on%20the%0Acontroller.%20Inspired%20by%20the%20recent%20work%20of%20%5B1%5D%20which%20proposed%2C%20for%20a%20linearly%0Aconstrained%20policy%20optimization%20of%20an%20offline%20LQR%2C%20a%20second%20order%20method%0Aequipped%20with%20a%20Riemannian%20metric%20that%20emerges%20naturally%20in%20the%20context%20of%0Aoptimal%20control%20problems%2C%20we%20propose%20online%20optimistic%20Newton%20on%20manifold%0A%28OONM%29%20which%20provides%20an%20online%20controller%20based%20on%20the%20prediction%20on%20the%20first%0Aand%20second%20order%20information%20of%20the%20function%20sequence.%20To%20quantify%20the%20proposed%0Aalgorithm%2C%20we%20leverage%20the%20notion%20of%20regret%20defined%20as%20the%20sub-optimality%20of%0Aits%20cumulative%20cost%20to%20that%20of%20a%20%28locally%29%20minimizing%20controller%20sequence%20and%0Aprovide%20the%20regret%20bound%20in%20terms%20of%20the%20path-length%20of%20the%20minimizer%20sequence.%0ASimulation%20results%20are%20also%20provided%20to%20verify%20the%20property%20of%20OONM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08553v1&entry.124074799=Read"},
{"title": "Historical Astronomical Diagrams Decomposition in Geometric Primitives", "author": "Syrine Kalleli and Scott Trigg and S\u00e9gol\u00e8ne Albouy and Mathieu Husson and Mathieu Aubry", "abstract": "  Automatically extracting the geometric content from the hundreds of thousands\nof diagrams drawn in historical manuscripts would enable historians to study\nthe diffusion of astronomical knowledge on a global scale. However,\nstate-of-the-art vectorization methods, often designed to tackle modern data,\nare not adapted to the complexity and diversity of historical astronomical\ndiagrams. Our contribution is thus twofold. First, we introduce a unique\ndataset of 303 astronomical diagrams from diverse traditions, ranging from the\nXIIth to the XVIIIth century, annotated with more than 3000 line segments,\ncircles and arcs. Second, we develop a model that builds on DINO-DETR to enable\nthe prediction of multiple geometric primitives. We show that it can be trained\nsolely on synthetic data and accurately predict primitives on our challenging\ndataset. Our approach widely improves over the LETR baseline, which is\nrestricted to lines, by introducing a meaningful parametrization for multiple\nprimitives, jointly training for detection and parameter refinement, using\ndeformable attention and training on rich synthetic data. Our dataset and code\nare available on our webpage.\n", "link": "http://arxiv.org/abs/2403.08721v1", "date": "2024-03-13", "relevancy": 1.8176, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.467}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4412}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Historical%20Astronomical%20Diagrams%20Decomposition%20in%20Geometric%20Primitives&body=Title%3A%20Historical%20Astronomical%20Diagrams%20Decomposition%20in%20Geometric%20Primitives%0AAuthor%3A%20Syrine%20Kalleli%20and%20Scott%20Trigg%20and%20S%C3%A9gol%C3%A8ne%20Albouy%20and%20Mathieu%20Husson%20and%20Mathieu%20Aubry%0AAbstract%3A%20%20%20Automatically%20extracting%20the%20geometric%20content%20from%20the%20hundreds%20of%20thousands%0Aof%20diagrams%20drawn%20in%20historical%20manuscripts%20would%20enable%20historians%20to%20study%0Athe%20diffusion%20of%20astronomical%20knowledge%20on%20a%20global%20scale.%20However%2C%0Astate-of-the-art%20vectorization%20methods%2C%20often%20designed%20to%20tackle%20modern%20data%2C%0Aare%20not%20adapted%20to%20the%20complexity%20and%20diversity%20of%20historical%20astronomical%0Adiagrams.%20Our%20contribution%20is%20thus%20twofold.%20First%2C%20we%20introduce%20a%20unique%0Adataset%20of%20303%20astronomical%20diagrams%20from%20diverse%20traditions%2C%20ranging%20from%20the%0AXIIth%20to%20the%20XVIIIth%20century%2C%20annotated%20with%20more%20than%203000%20line%20segments%2C%0Acircles%20and%20arcs.%20Second%2C%20we%20develop%20a%20model%20that%20builds%20on%20DINO-DETR%20to%20enable%0Athe%20prediction%20of%20multiple%20geometric%20primitives.%20We%20show%20that%20it%20can%20be%20trained%0Asolely%20on%20synthetic%20data%20and%20accurately%20predict%20primitives%20on%20our%20challenging%0Adataset.%20Our%20approach%20widely%20improves%20over%20the%20LETR%20baseline%2C%20which%20is%0Arestricted%20to%20lines%2C%20by%20introducing%20a%20meaningful%20parametrization%20for%20multiple%0Aprimitives%2C%20jointly%20training%20for%20detection%20and%20parameter%20refinement%2C%20using%0Adeformable%20attention%20and%20training%20on%20rich%20synthetic%20data.%20Our%20dataset%20and%20code%0Aare%20available%20on%20our%20webpage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08721v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Historical%20Astronomical%20Diagrams%20Decomposition%20in%20Geometric%20Primitives&entry.906535625=Syrine%20Kalleli%20and%20Scott%20Trigg%20and%20S%C3%A9gol%C3%A8ne%20Albouy%20and%20Mathieu%20Husson%20and%20Mathieu%20Aubry&entry.1292438233=%20%20Automatically%20extracting%20the%20geometric%20content%20from%20the%20hundreds%20of%20thousands%0Aof%20diagrams%20drawn%20in%20historical%20manuscripts%20would%20enable%20historians%20to%20study%0Athe%20diffusion%20of%20astronomical%20knowledge%20on%20a%20global%20scale.%20However%2C%0Astate-of-the-art%20vectorization%20methods%2C%20often%20designed%20to%20tackle%20modern%20data%2C%0Aare%20not%20adapted%20to%20the%20complexity%20and%20diversity%20of%20historical%20astronomical%0Adiagrams.%20Our%20contribution%20is%20thus%20twofold.%20First%2C%20we%20introduce%20a%20unique%0Adataset%20of%20303%20astronomical%20diagrams%20from%20diverse%20traditions%2C%20ranging%20from%20the%0AXIIth%20to%20the%20XVIIIth%20century%2C%20annotated%20with%20more%20than%203000%20line%20segments%2C%0Acircles%20and%20arcs.%20Second%2C%20we%20develop%20a%20model%20that%20builds%20on%20DINO-DETR%20to%20enable%0Athe%20prediction%20of%20multiple%20geometric%20primitives.%20We%20show%20that%20it%20can%20be%20trained%0Asolely%20on%20synthetic%20data%20and%20accurately%20predict%20primitives%20on%20our%20challenging%0Adataset.%20Our%20approach%20widely%20improves%20over%20the%20LETR%20baseline%2C%20which%20is%0Arestricted%20to%20lines%2C%20by%20introducing%20a%20meaningful%20parametrization%20for%20multiple%0Aprimitives%2C%20jointly%20training%20for%20detection%20and%20parameter%20refinement%2C%20using%0Adeformable%20attention%20and%20training%20on%20rich%20synthetic%20data.%20Our%20dataset%20and%20code%0Aare%20available%20on%20our%20webpage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08721v1&entry.124074799=Read"},
{"title": "Neural reproducing kernel Banach spaces and representer theorems for\n  deep networks", "author": "Francesca Bartolucci and Ernesto De Vito and Lorenzo Rosasco and Stefano Vigogna", "abstract": "  Studying the function spaces defined by neural networks helps to understand\nthe corresponding learning models and their inductive bias. While in some\nlimits neural networks correspond to function spaces that are reproducing\nkernel Hilbert spaces, these regimes do not capture the properties of the\nnetworks used in practice. In contrast, in this paper we show that deep neural\nnetworks define suitable reproducing kernel Banach spaces.\n  These spaces are equipped with norms that enforce a form of sparsity,\nenabling them to adapt to potential latent structures within the input data and\ntheir representations. In particular, leveraging the theory of reproducing\nkernel Banach spaces, combined with variational results, we derive representer\ntheorems that justify the finite architectures commonly employed in\napplications. Our study extends analogous results for shallow networks and can\nbe seen as a step towards considering more practically plausible neural\narchitectures.\n", "link": "http://arxiv.org/abs/2403.08750v1", "date": "2024-03-13", "relevancy": 1.7721, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4516}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4421}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4406}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Neural%20reproducing%20kernel%20Banach%20spaces%20and%20representer%20theorems%20for%0A%20%20deep%20networks&body=Title%3A%20Neural%20reproducing%20kernel%20Banach%20spaces%20and%20representer%20theorems%20for%0A%20%20deep%20networks%0AAuthor%3A%20Francesca%20Bartolucci%20and%20Ernesto%20De%20Vito%20and%20Lorenzo%20Rosasco%20and%20Stefano%20Vigogna%0AAbstract%3A%20%20%20Studying%20the%20function%20spaces%20defined%20by%20neural%20networks%20helps%20to%20understand%0Athe%20corresponding%20learning%20models%20and%20their%20inductive%20bias.%20While%20in%20some%0Alimits%20neural%20networks%20correspond%20to%20function%20spaces%20that%20are%20reproducing%0Akernel%20Hilbert%20spaces%2C%20these%20regimes%20do%20not%20capture%20the%20properties%20of%20the%0Anetworks%20used%20in%20practice.%20In%20contrast%2C%20in%20this%20paper%20we%20show%20that%20deep%20neural%0Anetworks%20define%20suitable%20reproducing%20kernel%20Banach%20spaces.%0A%20%20These%20spaces%20are%20equipped%20with%20norms%20that%20enforce%20a%20form%20of%20sparsity%2C%0Aenabling%20them%20to%20adapt%20to%20potential%20latent%20structures%20within%20the%20input%20data%20and%0Atheir%20representations.%20In%20particular%2C%20leveraging%20the%20theory%20of%20reproducing%0Akernel%20Banach%20spaces%2C%20combined%20with%20variational%20results%2C%20we%20derive%20representer%0Atheorems%20that%20justify%20the%20finite%20architectures%20commonly%20employed%20in%0Aapplications.%20Our%20study%20extends%20analogous%20results%20for%20shallow%20networks%20and%20can%0Abe%20seen%20as%20a%20step%20towards%20considering%20more%20practically%20plausible%20neural%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08750v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20reproducing%20kernel%20Banach%20spaces%20and%20representer%20theorems%20for%0A%20%20deep%20networks&entry.906535625=Francesca%20Bartolucci%20and%20Ernesto%20De%20Vito%20and%20Lorenzo%20Rosasco%20and%20Stefano%20Vigogna&entry.1292438233=%20%20Studying%20the%20function%20spaces%20defined%20by%20neural%20networks%20helps%20to%20understand%0Athe%20corresponding%20learning%20models%20and%20their%20inductive%20bias.%20While%20in%20some%0Alimits%20neural%20networks%20correspond%20to%20function%20spaces%20that%20are%20reproducing%0Akernel%20Hilbert%20spaces%2C%20these%20regimes%20do%20not%20capture%20the%20properties%20of%20the%0Anetworks%20used%20in%20practice.%20In%20contrast%2C%20in%20this%20paper%20we%20show%20that%20deep%20neural%0Anetworks%20define%20suitable%20reproducing%20kernel%20Banach%20spaces.%0A%20%20These%20spaces%20are%20equipped%20with%20norms%20that%20enforce%20a%20form%20of%20sparsity%2C%0Aenabling%20them%20to%20adapt%20to%20potential%20latent%20structures%20within%20the%20input%20data%20and%0Atheir%20representations.%20In%20particular%2C%20leveraging%20the%20theory%20of%20reproducing%0Akernel%20Banach%20spaces%2C%20combined%20with%20variational%20results%2C%20we%20derive%20representer%0Atheorems%20that%20justify%20the%20finite%20architectures%20commonly%20employed%20in%0Aapplications.%20Our%20study%20extends%20analogous%20results%20for%20shallow%20networks%20and%20can%0Abe%20seen%20as%20a%20step%20towards%20considering%20more%20practically%20plausible%20neural%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08750v1&entry.124074799=Read"},
{"title": "Language-Grounded Dynamic Scene Graphs for Interactive Object Search\n  with Mobile Manipulation", "author": "Daniel Honerkamp and Martin Buchner and Fabien Despinoy and Tim Welschehold and Abhinav Valada", "abstract": "  To fully leverage the capabilities of mobile manipulation robots, it is\nimperative that they are able to autonomously execute long-horizon tasks in\nlarge unexplored environments. While large language models (LLMs) have shown\nemergent reasoning skills on arbitrary tasks, existing work primarily\nconcentrates on explored environments, typically focusing on either navigation\nor manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel\napproach that grounds language models within structured representations derived\nfrom open-vocabulary scene graphs, dynamically updated as the environment is\nexplored. We tightly interleave these representations with an object-centric\naction space. The resulting approach is zero-shot, open-vocabulary, and readily\nextendable to a spectrum of mobile manipulation and household robotic tasks. We\ndemonstrate the effectiveness of MoMa-LLM in a novel semantic interactive\nsearch task in large realistic indoor environments. In extensive experiments in\nboth simulation and the real world, we show substantially improved search\nefficiency compared to conventional baselines and state-of-the-art approaches,\nas well as its applicability to more abstract tasks. We make the code publicly\navailable at http://moma-llm.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2403.08605v1", "date": "2024-03-13", "relevancy": 1.7587, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.7369}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5746}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5306}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Language-Grounded%20Dynamic%20Scene%20Graphs%20for%20Interactive%20Object%20Search%0A%20%20with%20Mobile%20Manipulation&body=Title%3A%20Language-Grounded%20Dynamic%20Scene%20Graphs%20for%20Interactive%20Object%20Search%0A%20%20with%20Mobile%20Manipulation%0AAuthor%3A%20Daniel%20Honerkamp%20and%20Martin%20Buchner%20and%20Fabien%20Despinoy%20and%20Tim%20Welschehold%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20To%20fully%20leverage%20the%20capabilities%20of%20mobile%20manipulation%20robots%2C%20it%20is%0Aimperative%20that%20they%20are%20able%20to%20autonomously%20execute%20long-horizon%20tasks%20in%0Alarge%20unexplored%20environments.%20While%20large%20language%20models%20%28LLMs%29%20have%20shown%0Aemergent%20reasoning%20skills%20on%20arbitrary%20tasks%2C%20existing%20work%20primarily%0Aconcentrates%20on%20explored%20environments%2C%20typically%20focusing%20on%20either%20navigation%0Aor%20manipulation%20tasks%20in%20isolation.%20In%20this%20work%2C%20we%20propose%20MoMa-LLM%2C%20a%20novel%0Aapproach%20that%20grounds%20language%20models%20within%20structured%20representations%20derived%0Afrom%20open-vocabulary%20scene%20graphs%2C%20dynamically%20updated%20as%20the%20environment%20is%0Aexplored.%20We%20tightly%20interleave%20these%20representations%20with%20an%20object-centric%0Aaction%20space.%20The%20resulting%20approach%20is%20zero-shot%2C%20open-vocabulary%2C%20and%20readily%0Aextendable%20to%20a%20spectrum%20of%20mobile%20manipulation%20and%20household%20robotic%20tasks.%20We%0Ademonstrate%20the%20effectiveness%20of%20MoMa-LLM%20in%20a%20novel%20semantic%20interactive%0Asearch%20task%20in%20large%20realistic%20indoor%20environments.%20In%20extensive%20experiments%20in%0Aboth%20simulation%20and%20the%20real%20world%2C%20we%20show%20substantially%20improved%20search%0Aefficiency%20compared%20to%20conventional%20baselines%20and%20state-of-the-art%20approaches%2C%0Aas%20well%20as%20its%20applicability%20to%20more%20abstract%20tasks.%20We%20make%20the%20code%20publicly%0Aavailable%20at%20http%3A//moma-llm.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08605v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Grounded%20Dynamic%20Scene%20Graphs%20for%20Interactive%20Object%20Search%0A%20%20with%20Mobile%20Manipulation&entry.906535625=Daniel%20Honerkamp%20and%20Martin%20Buchner%20and%20Fabien%20Despinoy%20and%20Tim%20Welschehold%20and%20Abhinav%20Valada&entry.1292438233=%20%20To%20fully%20leverage%20the%20capabilities%20of%20mobile%20manipulation%20robots%2C%20it%20is%0Aimperative%20that%20they%20are%20able%20to%20autonomously%20execute%20long-horizon%20tasks%20in%0Alarge%20unexplored%20environments.%20While%20large%20language%20models%20%28LLMs%29%20have%20shown%0Aemergent%20reasoning%20skills%20on%20arbitrary%20tasks%2C%20existing%20work%20primarily%0Aconcentrates%20on%20explored%20environments%2C%20typically%20focusing%20on%20either%20navigation%0Aor%20manipulation%20tasks%20in%20isolation.%20In%20this%20work%2C%20we%20propose%20MoMa-LLM%2C%20a%20novel%0Aapproach%20that%20grounds%20language%20models%20within%20structured%20representations%20derived%0Afrom%20open-vocabulary%20scene%20graphs%2C%20dynamically%20updated%20as%20the%20environment%20is%0Aexplored.%20We%20tightly%20interleave%20these%20representations%20with%20an%20object-centric%0Aaction%20space.%20The%20resulting%20approach%20is%20zero-shot%2C%20open-vocabulary%2C%20and%20readily%0Aextendable%20to%20a%20spectrum%20of%20mobile%20manipulation%20and%20household%20robotic%20tasks.%20We%0Ademonstrate%20the%20effectiveness%20of%20MoMa-LLM%20in%20a%20novel%20semantic%20interactive%0Asearch%20task%20in%20large%20realistic%20indoor%20environments.%20In%20extensive%20experiments%20in%0Aboth%20simulation%20and%20the%20real%20world%2C%20we%20show%20substantially%20improved%20search%0Aefficiency%20compared%20to%20conventional%20baselines%20and%20state-of-the-art%20approaches%2C%0Aas%20well%20as%20its%20applicability%20to%20more%20abstract%20tasks.%20We%20make%20the%20code%20publicly%0Aavailable%20at%20http%3A//moma-llm.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08605v1&entry.124074799=Read"},
{"title": "Leveraging Non-Decimated Wavelet Packet Features and Transformer Models\n  for Time Series Forecasting", "author": "Guy P Nason and James L. Wei", "abstract": "  This article combines wavelet analysis techniques with machine learning\nmethods for univariate time series forecasting, focusing on three main\ncontributions. Firstly, we consider the use of Daubechies wavelets with\ndifferent numbers of vanishing moments as input features to both non-temporal\nand temporal forecasting methods, by selecting these numbers during the\ncross-validation phase. Secondly, we compare the use of both the non-decimated\nwavelet transform and the non-decimated wavelet packet transform for computing\nthese features, the latter providing a much larger set of potentially useful\ncoefficient vectors. The wavelet coefficients are computed using a shifted\nversion of the typical pyramidal algorithm to ensure no leakage of future\ninformation into these inputs. Thirdly, we evaluate the use of these wavelet\nfeatures on a significantly wider set of forecasting methods than previous\nstudies, including both temporal and non-temporal models, and both statistical\nand deep learning-based methods. The latter include state-of-the-art\ntransformer-based neural network architectures. Our experiments suggest\nsignificant benefit in replacing higher-order lagged features with wavelet\nfeatures across all examined non-temporal methods for one-step-forward\nforecasting, and modest benefit when used as inputs for temporal deep\nlearning-based models for long-horizon forecasting.\n", "link": "http://arxiv.org/abs/2403.08630v1", "date": "2024-03-13", "relevancy": 1.7579, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4794}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.436}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4269}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Non-Decimated%20Wavelet%20Packet%20Features%20and%20Transformer%20Models%0A%20%20for%20Time%20Series%20Forecasting&body=Title%3A%20Leveraging%20Non-Decimated%20Wavelet%20Packet%20Features%20and%20Transformer%20Models%0A%20%20for%20Time%20Series%20Forecasting%0AAuthor%3A%20Guy%20P%20Nason%20and%20James%20L.%20Wei%0AAbstract%3A%20%20%20This%20article%20combines%20wavelet%20analysis%20techniques%20with%20machine%20learning%0Amethods%20for%20univariate%20time%20series%20forecasting%2C%20focusing%20on%20three%20main%0Acontributions.%20Firstly%2C%20we%20consider%20the%20use%20of%20Daubechies%20wavelets%20with%0Adifferent%20numbers%20of%20vanishing%20moments%20as%20input%20features%20to%20both%20non-temporal%0Aand%20temporal%20forecasting%20methods%2C%20by%20selecting%20these%20numbers%20during%20the%0Across-validation%20phase.%20Secondly%2C%20we%20compare%20the%20use%20of%20both%20the%20non-decimated%0Awavelet%20transform%20and%20the%20non-decimated%20wavelet%20packet%20transform%20for%20computing%0Athese%20features%2C%20the%20latter%20providing%20a%20much%20larger%20set%20of%20potentially%20useful%0Acoefficient%20vectors.%20The%20wavelet%20coefficients%20are%20computed%20using%20a%20shifted%0Aversion%20of%20the%20typical%20pyramidal%20algorithm%20to%20ensure%20no%20leakage%20of%20future%0Ainformation%20into%20these%20inputs.%20Thirdly%2C%20we%20evaluate%20the%20use%20of%20these%20wavelet%0Afeatures%20on%20a%20significantly%20wider%20set%20of%20forecasting%20methods%20than%20previous%0Astudies%2C%20including%20both%20temporal%20and%20non-temporal%20models%2C%20and%20both%20statistical%0Aand%20deep%20learning-based%20methods.%20The%20latter%20include%20state-of-the-art%0Atransformer-based%20neural%20network%20architectures.%20Our%20experiments%20suggest%0Asignificant%20benefit%20in%20replacing%20higher-order%20lagged%20features%20with%20wavelet%0Afeatures%20across%20all%20examined%20non-temporal%20methods%20for%20one-step-forward%0Aforecasting%2C%20and%20modest%20benefit%20when%20used%20as%20inputs%20for%20temporal%20deep%0Alearning-based%20models%20for%20long-horizon%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08630v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Non-Decimated%20Wavelet%20Packet%20Features%20and%20Transformer%20Models%0A%20%20for%20Time%20Series%20Forecasting&entry.906535625=Guy%20P%20Nason%20and%20James%20L.%20Wei&entry.1292438233=%20%20This%20article%20combines%20wavelet%20analysis%20techniques%20with%20machine%20learning%0Amethods%20for%20univariate%20time%20series%20forecasting%2C%20focusing%20on%20three%20main%0Acontributions.%20Firstly%2C%20we%20consider%20the%20use%20of%20Daubechies%20wavelets%20with%0Adifferent%20numbers%20of%20vanishing%20moments%20as%20input%20features%20to%20both%20non-temporal%0Aand%20temporal%20forecasting%20methods%2C%20by%20selecting%20these%20numbers%20during%20the%0Across-validation%20phase.%20Secondly%2C%20we%20compare%20the%20use%20of%20both%20the%20non-decimated%0Awavelet%20transform%20and%20the%20non-decimated%20wavelet%20packet%20transform%20for%20computing%0Athese%20features%2C%20the%20latter%20providing%20a%20much%20larger%20set%20of%20potentially%20useful%0Acoefficient%20vectors.%20The%20wavelet%20coefficients%20are%20computed%20using%20a%20shifted%0Aversion%20of%20the%20typical%20pyramidal%20algorithm%20to%20ensure%20no%20leakage%20of%20future%0Ainformation%20into%20these%20inputs.%20Thirdly%2C%20we%20evaluate%20the%20use%20of%20these%20wavelet%0Afeatures%20on%20a%20significantly%20wider%20set%20of%20forecasting%20methods%20than%20previous%0Astudies%2C%20including%20both%20temporal%20and%20non-temporal%20models%2C%20and%20both%20statistical%0Aand%20deep%20learning-based%20methods.%20The%20latter%20include%20state-of-the-art%0Atransformer-based%20neural%20network%20architectures.%20Our%20experiments%20suggest%0Asignificant%20benefit%20in%20replacing%20higher-order%20lagged%20features%20with%20wavelet%0Afeatures%20across%20all%20examined%20non-temporal%20methods%20for%20one-step-forward%0Aforecasting%2C%20and%20modest%20benefit%20when%20used%20as%20inputs%20for%20temporal%20deep%0Alearning-based%20models%20for%20long-horizon%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08630v1&entry.124074799=Read"},
{"title": "Efficient Combinatorial Optimization via Heat Diffusion", "author": "Hengyuan Ma and Wenlian Lu and Jianfeng Feng", "abstract": "  Combinatorial optimization problems are widespread but inherently challenging\ndue to their discrete nature.The primary limitation of existing methods is that\nthey can only access a small fraction of the solution space at each iteration,\nresulting in limited efficiency for searching the global optimal. To overcome\nthis challenge, diverging from conventional efforts of expanding the solver's\nsearch scope, we focus on enabling information to actively propagate to the\nsolver through heat diffusion. By transforming the target function while\npreserving its optima, heat diffusion facilitates information flow from distant\nregions to the solver, providing more efficient navigation. Utilizing heat\ndiffusion, we propose a framework for solving general combinatorial\noptimization problems. The proposed methodology demonstrates superior\nperformance across a range of the most challenging and widely encountered\ncombinatorial optimizations. Echoing recent advancements in harnessing\nthermodynamics for generative artificial intelligence, our study further\nreveals its significant potential in advancing combinatorial optimization.\n", "link": "http://arxiv.org/abs/2403.08757v1", "date": "2024-03-13", "relevancy": 1.751, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4539}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4384}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4307}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Efficient%20Combinatorial%20Optimization%20via%20Heat%20Diffusion&body=Title%3A%20Efficient%20Combinatorial%20Optimization%20via%20Heat%20Diffusion%0AAuthor%3A%20Hengyuan%20Ma%20and%20Wenlian%20Lu%20and%20Jianfeng%20Feng%0AAbstract%3A%20%20%20Combinatorial%20optimization%20problems%20are%20widespread%20but%20inherently%20challenging%0Adue%20to%20their%20discrete%20nature.The%20primary%20limitation%20of%20existing%20methods%20is%20that%0Athey%20can%20only%20access%20a%20small%20fraction%20of%20the%20solution%20space%20at%20each%20iteration%2C%0Aresulting%20in%20limited%20efficiency%20for%20searching%20the%20global%20optimal.%20To%20overcome%0Athis%20challenge%2C%20diverging%20from%20conventional%20efforts%20of%20expanding%20the%20solver%27s%0Asearch%20scope%2C%20we%20focus%20on%20enabling%20information%20to%20actively%20propagate%20to%20the%0Asolver%20through%20heat%20diffusion.%20By%20transforming%20the%20target%20function%20while%0Apreserving%20its%20optima%2C%20heat%20diffusion%20facilitates%20information%20flow%20from%20distant%0Aregions%20to%20the%20solver%2C%20providing%20more%20efficient%20navigation.%20Utilizing%20heat%0Adiffusion%2C%20we%20propose%20a%20framework%20for%20solving%20general%20combinatorial%0Aoptimization%20problems.%20The%20proposed%20methodology%20demonstrates%20superior%0Aperformance%20across%20a%20range%20of%20the%20most%20challenging%20and%20widely%20encountered%0Acombinatorial%20optimizations.%20Echoing%20recent%20advancements%20in%20harnessing%0Athermodynamics%20for%20generative%20artificial%20intelligence%2C%20our%20study%20further%0Areveals%20its%20significant%20potential%20in%20advancing%20combinatorial%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08757v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Combinatorial%20Optimization%20via%20Heat%20Diffusion&entry.906535625=Hengyuan%20Ma%20and%20Wenlian%20Lu%20and%20Jianfeng%20Feng&entry.1292438233=%20%20Combinatorial%20optimization%20problems%20are%20widespread%20but%20inherently%20challenging%0Adue%20to%20their%20discrete%20nature.The%20primary%20limitation%20of%20existing%20methods%20is%20that%0Athey%20can%20only%20access%20a%20small%20fraction%20of%20the%20solution%20space%20at%20each%20iteration%2C%0Aresulting%20in%20limited%20efficiency%20for%20searching%20the%20global%20optimal.%20To%20overcome%0Athis%20challenge%2C%20diverging%20from%20conventional%20efforts%20of%20expanding%20the%20solver%27s%0Asearch%20scope%2C%20we%20focus%20on%20enabling%20information%20to%20actively%20propagate%20to%20the%0Asolver%20through%20heat%20diffusion.%20By%20transforming%20the%20target%20function%20while%0Apreserving%20its%20optima%2C%20heat%20diffusion%20facilitates%20information%20flow%20from%20distant%0Aregions%20to%20the%20solver%2C%20providing%20more%20efficient%20navigation.%20Utilizing%20heat%0Adiffusion%2C%20we%20propose%20a%20framework%20for%20solving%20general%20combinatorial%0Aoptimization%20problems.%20The%20proposed%20methodology%20demonstrates%20superior%0Aperformance%20across%20a%20range%20of%20the%20most%20challenging%20and%20widely%20encountered%0Acombinatorial%20optimizations.%20Echoing%20recent%20advancements%20in%20harnessing%0Athermodynamics%20for%20generative%20artificial%20intelligence%2C%20our%20study%20further%0Areveals%20its%20significant%20potential%20in%20advancing%20combinatorial%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08757v1&entry.124074799=Read"},
{"title": "Randomized Kaczmarz in Adversarial Distributed Setting", "author": "Longxiu Huang and Xia Li and Deanna Needell", "abstract": "  Developing large-scale distributed methods that are robust to the presence of\nadversarial or corrupted workers is an important part of making such methods\npractical for real-world problems. In this paper, we propose an iterative\napproach that is adversary-tolerant for convex optimization problems. By\nleveraging simple statistics, our method ensures convergence and is capable of\nadapting to adversarial distributions. Additionally, the efficiency of the\nproposed methods for solving convex problems is shown in simulations with the\npresence of adversaries. Through simulations, we demonstrate the efficiency of\nour approach in the presence of adversaries and its ability to identify\nadversarial workers with high accuracy and tolerate varying levels of adversary\nrates.\n", "link": "http://arxiv.org/abs/2302.14615v2", "date": "2024-03-13", "relevancy": 1.7443, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4324}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4106}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Randomized%20Kaczmarz%20in%20Adversarial%20Distributed%20Setting&body=Title%3A%20Randomized%20Kaczmarz%20in%20Adversarial%20Distributed%20Setting%0AAuthor%3A%20Longxiu%20Huang%20and%20Xia%20Li%20and%20Deanna%20Needell%0AAbstract%3A%20%20%20Developing%20large-scale%20distributed%20methods%20that%20are%20robust%20to%20the%20presence%20of%0Aadversarial%20or%20corrupted%20workers%20is%20an%20important%20part%20of%20making%20such%20methods%0Apractical%20for%20real-world%20problems.%20In%20this%20paper%2C%20we%20propose%20an%20iterative%0Aapproach%20that%20is%20adversary-tolerant%20for%20convex%20optimization%20problems.%20By%0Aleveraging%20simple%20statistics%2C%20our%20method%20ensures%20convergence%20and%20is%20capable%20of%0Aadapting%20to%20adversarial%20distributions.%20Additionally%2C%20the%20efficiency%20of%20the%0Aproposed%20methods%20for%20solving%20convex%20problems%20is%20shown%20in%20simulations%20with%20the%0Apresence%20of%20adversaries.%20Through%20simulations%2C%20we%20demonstrate%20the%20efficiency%20of%0Aour%20approach%20in%20the%20presence%20of%20adversaries%20and%20its%20ability%20to%20identify%0Aadversarial%20workers%20with%20high%20accuracy%20and%20tolerate%20varying%20levels%20of%20adversary%0Arates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.14615v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Randomized%20Kaczmarz%20in%20Adversarial%20Distributed%20Setting&entry.906535625=Longxiu%20Huang%20and%20Xia%20Li%20and%20Deanna%20Needell&entry.1292438233=%20%20Developing%20large-scale%20distributed%20methods%20that%20are%20robust%20to%20the%20presence%20of%0Aadversarial%20or%20corrupted%20workers%20is%20an%20important%20part%20of%20making%20such%20methods%0Apractical%20for%20real-world%20problems.%20In%20this%20paper%2C%20we%20propose%20an%20iterative%0Aapproach%20that%20is%20adversary-tolerant%20for%20convex%20optimization%20problems.%20By%0Aleveraging%20simple%20statistics%2C%20our%20method%20ensures%20convergence%20and%20is%20capable%20of%0Aadapting%20to%20adversarial%20distributions.%20Additionally%2C%20the%20efficiency%20of%20the%0Aproposed%20methods%20for%20solving%20convex%20problems%20is%20shown%20in%20simulations%20with%20the%0Apresence%20of%20adversaries.%20Through%20simulations%2C%20we%20demonstrate%20the%20efficiency%20of%0Aour%20approach%20in%20the%20presence%20of%20adversaries%20and%20its%20ability%20to%20identify%0Aadversarial%20workers%20with%20high%20accuracy%20and%20tolerate%20varying%20levels%20of%20adversary%0Arates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.14615v2&entry.124074799=Read"},
{"title": "Constructing Variables Using Classifiers as an Aid to Regression: An\n  Empirical Assessment", "author": "Colin Troisemaine and Vincent Lemaire", "abstract": "  This paper proposes a method for the automatic creation of variables (in the\ncase of regression) that complement the information contained in the initial\ninput vector. The method works as a pre-processing step in which the continuous\nvalues of the variable to be regressed are discretized into a set of intervals\nwhich are then used to define value thresholds. Then classifiers are trained to\npredict whether the value to be regressed is less than or equal to each of\nthese thresholds. The different outputs of the classifiers are then\nconcatenated in the form of an additional vector of variables that enriches the\ninitial vector of the regression problem. The implemented system can thus be\nconsidered as a generic pre-processing tool. We tested the proposed enrichment\nmethod with 5 types of regressors and evaluated it in 33 regression datasets.\nOur experimental results confirm the interest of the approach.\n", "link": "http://arxiv.org/abs/2403.06829v2", "date": "2024-03-13", "relevancy": 1.6832, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4271}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4256}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4135}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Constructing%20Variables%20Using%20Classifiers%20as%20an%20Aid%20to%20Regression%3A%20An%0A%20%20Empirical%20Assessment&body=Title%3A%20Constructing%20Variables%20Using%20Classifiers%20as%20an%20Aid%20to%20Regression%3A%20An%0A%20%20Empirical%20Assessment%0AAuthor%3A%20Colin%20Troisemaine%20and%20Vincent%20Lemaire%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20method%20for%20the%20automatic%20creation%20of%20variables%20%28in%20the%0Acase%20of%20regression%29%20that%20complement%20the%20information%20contained%20in%20the%20initial%0Ainput%20vector.%20The%20method%20works%20as%20a%20pre-processing%20step%20in%20which%20the%20continuous%0Avalues%20of%20the%20variable%20to%20be%20regressed%20are%20discretized%20into%20a%20set%20of%20intervals%0Awhich%20are%20then%20used%20to%20define%20value%20thresholds.%20Then%20classifiers%20are%20trained%20to%0Apredict%20whether%20the%20value%20to%20be%20regressed%20is%20less%20than%20or%20equal%20to%20each%20of%0Athese%20thresholds.%20The%20different%20outputs%20of%20the%20classifiers%20are%20then%0Aconcatenated%20in%20the%20form%20of%20an%20additional%20vector%20of%20variables%20that%20enriches%20the%0Ainitial%20vector%20of%20the%20regression%20problem.%20The%20implemented%20system%20can%20thus%20be%0Aconsidered%20as%20a%20generic%20pre-processing%20tool.%20We%20tested%20the%20proposed%20enrichment%0Amethod%20with%205%20types%20of%20regressors%20and%20evaluated%20it%20in%2033%20regression%20datasets.%0AOur%20experimental%20results%20confirm%20the%20interest%20of%20the%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06829v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constructing%20Variables%20Using%20Classifiers%20as%20an%20Aid%20to%20Regression%3A%20An%0A%20%20Empirical%20Assessment&entry.906535625=Colin%20Troisemaine%20and%20Vincent%20Lemaire&entry.1292438233=%20%20This%20paper%20proposes%20a%20method%20for%20the%20automatic%20creation%20of%20variables%20%28in%20the%0Acase%20of%20regression%29%20that%20complement%20the%20information%20contained%20in%20the%20initial%0Ainput%20vector.%20The%20method%20works%20as%20a%20pre-processing%20step%20in%20which%20the%20continuous%0Avalues%20of%20the%20variable%20to%20be%20regressed%20are%20discretized%20into%20a%20set%20of%20intervals%0Awhich%20are%20then%20used%20to%20define%20value%20thresholds.%20Then%20classifiers%20are%20trained%20to%0Apredict%20whether%20the%20value%20to%20be%20regressed%20is%20less%20than%20or%20equal%20to%20each%20of%0Athese%20thresholds.%20The%20different%20outputs%20of%20the%20classifiers%20are%20then%0Aconcatenated%20in%20the%20form%20of%20an%20additional%20vector%20of%20variables%20that%20enriches%20the%0Ainitial%20vector%20of%20the%20regression%20problem.%20The%20implemented%20system%20can%20thus%20be%0Aconsidered%20as%20a%20generic%20pre-processing%20tool.%20We%20tested%20the%20proposed%20enrichment%0Amethod%20with%205%20types%20of%20regressors%20and%20evaluated%20it%20in%2033%20regression%20datasets.%0AOur%20experimental%20results%20confirm%20the%20interest%20of%20the%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06829v2&entry.124074799=Read"},
{"title": "Adaptive morphing of wing and tail for stable, resilient, and\n  energy-efficient flight of avian-informed drones", "author": "Simon L. Jeger and Valentin W\u00fcest and Charbel Toumieh and Dario Floreano", "abstract": "  Avian-informed drones feature morphing wing and tail surfaces, enhancing\nagility and adaptability in flight. Despite their large potential, realising\ntheir full capabilities remains challenging due to the lack of generalized\ncontrol strategies accommodating their large degrees of freedom and\ncross-coupling effects between their control surfaces. Here we propose a new\nbody-rate controller for avian-informed drones that uses all available\nactuators to control the motion of the drone. The method exhibits robustness\nagainst physical perturbations, turbulent airflow, and even loss of certain\nactuators mid-flight. Furthermore, wing and tail morphing is leveraged to\nenhance energy efficiency at 8m/s, 10m/s and 12m/s using in-flight Bayesian\noptimization. The resulting morphing configurations yield significant gains\nacross all three speeds of up to 11.5% compared to non-morphing configurations\nand display a strong resemblance to avian flight at different speeds. This\nresearch lays the groundwork for the development of autonomous avian-informed\ndrones that operate under diverse wind conditions, emphasizing the role of\nmorphing in improving energy efficiency.\n", "link": "http://arxiv.org/abs/2403.08598v1", "date": "2024-03-13", "relevancy": 1.6654, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4448}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4205}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4008}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Adaptive%20morphing%20of%20wing%20and%20tail%20for%20stable%2C%20resilient%2C%20and%0A%20%20energy-efficient%20flight%20of%20avian-informed%20drones&body=Title%3A%20Adaptive%20morphing%20of%20wing%20and%20tail%20for%20stable%2C%20resilient%2C%20and%0A%20%20energy-efficient%20flight%20of%20avian-informed%20drones%0AAuthor%3A%20Simon%20L.%20Jeger%20and%20Valentin%20W%C3%BCest%20and%20Charbel%20Toumieh%20and%20Dario%20Floreano%0AAbstract%3A%20%20%20Avian-informed%20drones%20feature%20morphing%20wing%20and%20tail%20surfaces%2C%20enhancing%0Aagility%20and%20adaptability%20in%20flight.%20Despite%20their%20large%20potential%2C%20realising%0Atheir%20full%20capabilities%20remains%20challenging%20due%20to%20the%20lack%20of%20generalized%0Acontrol%20strategies%20accommodating%20their%20large%20degrees%20of%20freedom%20and%0Across-coupling%20effects%20between%20their%20control%20surfaces.%20Here%20we%20propose%20a%20new%0Abody-rate%20controller%20for%20avian-informed%20drones%20that%20uses%20all%20available%0Aactuators%20to%20control%20the%20motion%20of%20the%20drone.%20The%20method%20exhibits%20robustness%0Aagainst%20physical%20perturbations%2C%20turbulent%20airflow%2C%20and%20even%20loss%20of%20certain%0Aactuators%20mid-flight.%20Furthermore%2C%20wing%20and%20tail%20morphing%20is%20leveraged%20to%0Aenhance%20energy%20efficiency%20at%208m/s%2C%2010m/s%20and%2012m/s%20using%20in-flight%20Bayesian%0Aoptimization.%20The%20resulting%20morphing%20configurations%20yield%20significant%20gains%0Aacross%20all%20three%20speeds%20of%20up%20to%2011.5%25%20compared%20to%20non-morphing%20configurations%0Aand%20display%20a%20strong%20resemblance%20to%20avian%20flight%20at%20different%20speeds.%20This%0Aresearch%20lays%20the%20groundwork%20for%20the%20development%20of%20autonomous%20avian-informed%0Adrones%20that%20operate%20under%20diverse%20wind%20conditions%2C%20emphasizing%20the%20role%20of%0Amorphing%20in%20improving%20energy%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08598v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20morphing%20of%20wing%20and%20tail%20for%20stable%2C%20resilient%2C%20and%0A%20%20energy-efficient%20flight%20of%20avian-informed%20drones&entry.906535625=Simon%20L.%20Jeger%20and%20Valentin%20W%C3%BCest%20and%20Charbel%20Toumieh%20and%20Dario%20Floreano&entry.1292438233=%20%20Avian-informed%20drones%20feature%20morphing%20wing%20and%20tail%20surfaces%2C%20enhancing%0Aagility%20and%20adaptability%20in%20flight.%20Despite%20their%20large%20potential%2C%20realising%0Atheir%20full%20capabilities%20remains%20challenging%20due%20to%20the%20lack%20of%20generalized%0Acontrol%20strategies%20accommodating%20their%20large%20degrees%20of%20freedom%20and%0Across-coupling%20effects%20between%20their%20control%20surfaces.%20Here%20we%20propose%20a%20new%0Abody-rate%20controller%20for%20avian-informed%20drones%20that%20uses%20all%20available%0Aactuators%20to%20control%20the%20motion%20of%20the%20drone.%20The%20method%20exhibits%20robustness%0Aagainst%20physical%20perturbations%2C%20turbulent%20airflow%2C%20and%20even%20loss%20of%20certain%0Aactuators%20mid-flight.%20Furthermore%2C%20wing%20and%20tail%20morphing%20is%20leveraged%20to%0Aenhance%20energy%20efficiency%20at%208m/s%2C%2010m/s%20and%2012m/s%20using%20in-flight%20Bayesian%0Aoptimization.%20The%20resulting%20morphing%20configurations%20yield%20significant%20gains%0Aacross%20all%20three%20speeds%20of%20up%20to%2011.5%25%20compared%20to%20non-morphing%20configurations%0Aand%20display%20a%20strong%20resemblance%20to%20avian%20flight%20at%20different%20speeds.%20This%0Aresearch%20lays%20the%20groundwork%20for%20the%20development%20of%20autonomous%20avian-informed%0Adrones%20that%20operate%20under%20diverse%20wind%20conditions%2C%20emphasizing%20the%20role%20of%0Amorphing%20in%20improving%20energy%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08598v1&entry.124074799=Read"},
{"title": "Structural perspective on constraint-based learning of Markov networks", "author": "Tuukka Korhonen and Fedor V. Fomin and Pekka Parviainen", "abstract": "  Markov networks are probabilistic graphical models that employ undirected\ngraphs to depict conditional independence relationships among variables. Our\nfocus lies in constraint-based structure learning, which entails learning the\nundirected graph from data through the execution of conditional independence\ntests. We establish theoretical limits concerning two critical aspects of\nconstraint-based learning of Markov networks: the number of tests and the sizes\nof the conditioning sets. These bounds uncover an exciting interplay between\nthe structural properties of the graph and the amount of tests required to\nlearn a Markov network. The starting point of our work is that the graph\nparameter maximum pairwise connectivity, $\\kappa$, that is, the maximum number\nof vertex-disjoint paths connecting a pair of vertices in the graph, is\nresponsible for the sizes of independence tests required to learn the graph. On\none hand, we show that at least one test with the size of the conditioning set\nat least $\\kappa$ is always necessary. On the other hand, we prove that any\ngraph can be learned by performing tests of size at most $\\kappa$. This\ncompletely resolves the question of the minimum size of conditioning sets\nrequired to learn the graph. When it comes to the number of tests, our upper\nbound on the sizes of conditioning sets implies that every $n$-vertex graph can\nbe learned by at most $n^{\\kappa}$ tests with conditioning sets of sizes at\nmost $\\kappa$. We show that for any upper bound $q$ on the sizes of the\nconditioning sets, there exist graphs with $O(n q)$ vertices that require at\nleast $n^{\\Omega(\\kappa)}$ tests to learn. This lower bound holds even when the\ntreewidth and the maximum degree of the graph are at most $\\kappa+2$. On the\npositive side, we prove that every graph of bounded treewidth can be learned by\na polynomial number of tests with conditioning sets of sizes at most $2\\kappa$.\n", "link": "http://arxiv.org/abs/2403.08562v1", "date": "2024-03-13", "relevancy": 1.6549, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4342}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4222}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3971}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Structural%20perspective%20on%20constraint-based%20learning%20of%20Markov%20networks&body=Title%3A%20Structural%20perspective%20on%20constraint-based%20learning%20of%20Markov%20networks%0AAuthor%3A%20Tuukka%20Korhonen%20and%20Fedor%20V.%20Fomin%20and%20Pekka%20Parviainen%0AAbstract%3A%20%20%20Markov%20networks%20are%20probabilistic%20graphical%20models%20that%20employ%20undirected%0Agraphs%20to%20depict%20conditional%20independence%20relationships%20among%20variables.%20Our%0Afocus%20lies%20in%20constraint-based%20structure%20learning%2C%20which%20entails%20learning%20the%0Aundirected%20graph%20from%20data%20through%20the%20execution%20of%20conditional%20independence%0Atests.%20We%20establish%20theoretical%20limits%20concerning%20two%20critical%20aspects%20of%0Aconstraint-based%20learning%20of%20Markov%20networks%3A%20the%20number%20of%20tests%20and%20the%20sizes%0Aof%20the%20conditioning%20sets.%20These%20bounds%20uncover%20an%20exciting%20interplay%20between%0Athe%20structural%20properties%20of%20the%20graph%20and%20the%20amount%20of%20tests%20required%20to%0Alearn%20a%20Markov%20network.%20The%20starting%20point%20of%20our%20work%20is%20that%20the%20graph%0Aparameter%20maximum%20pairwise%20connectivity%2C%20%24%5Ckappa%24%2C%20that%20is%2C%20the%20maximum%20number%0Aof%20vertex-disjoint%20paths%20connecting%20a%20pair%20of%20vertices%20in%20the%20graph%2C%20is%0Aresponsible%20for%20the%20sizes%20of%20independence%20tests%20required%20to%20learn%20the%20graph.%20On%0Aone%20hand%2C%20we%20show%20that%20at%20least%20one%20test%20with%20the%20size%20of%20the%20conditioning%20set%0Aat%20least%20%24%5Ckappa%24%20is%20always%20necessary.%20On%20the%20other%20hand%2C%20we%20prove%20that%20any%0Agraph%20can%20be%20learned%20by%20performing%20tests%20of%20size%20at%20most%20%24%5Ckappa%24.%20This%0Acompletely%20resolves%20the%20question%20of%20the%20minimum%20size%20of%20conditioning%20sets%0Arequired%20to%20learn%20the%20graph.%20When%20it%20comes%20to%20the%20number%20of%20tests%2C%20our%20upper%0Abound%20on%20the%20sizes%20of%20conditioning%20sets%20implies%20that%20every%20%24n%24-vertex%20graph%20can%0Abe%20learned%20by%20at%20most%20%24n%5E%7B%5Ckappa%7D%24%20tests%20with%20conditioning%20sets%20of%20sizes%20at%0Amost%20%24%5Ckappa%24.%20We%20show%20that%20for%20any%20upper%20bound%20%24q%24%20on%20the%20sizes%20of%20the%0Aconditioning%20sets%2C%20there%20exist%20graphs%20with%20%24O%28n%20q%29%24%20vertices%20that%20require%20at%0Aleast%20%24n%5E%7B%5COmega%28%5Ckappa%29%7D%24%20tests%20to%20learn.%20This%20lower%20bound%20holds%20even%20when%20the%0Atreewidth%20and%20the%20maximum%20degree%20of%20the%20graph%20are%20at%20most%20%24%5Ckappa%2B2%24.%20On%20the%0Apositive%20side%2C%20we%20prove%20that%20every%20graph%20of%20bounded%20treewidth%20can%20be%20learned%20by%0Aa%20polynomial%20number%20of%20tests%20with%20conditioning%20sets%20of%20sizes%20at%20most%20%242%5Ckappa%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08562v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20perspective%20on%20constraint-based%20learning%20of%20Markov%20networks&entry.906535625=Tuukka%20Korhonen%20and%20Fedor%20V.%20Fomin%20and%20Pekka%20Parviainen&entry.1292438233=%20%20Markov%20networks%20are%20probabilistic%20graphical%20models%20that%20employ%20undirected%0Agraphs%20to%20depict%20conditional%20independence%20relationships%20among%20variables.%20Our%0Afocus%20lies%20in%20constraint-based%20structure%20learning%2C%20which%20entails%20learning%20the%0Aundirected%20graph%20from%20data%20through%20the%20execution%20of%20conditional%20independence%0Atests.%20We%20establish%20theoretical%20limits%20concerning%20two%20critical%20aspects%20of%0Aconstraint-based%20learning%20of%20Markov%20networks%3A%20the%20number%20of%20tests%20and%20the%20sizes%0Aof%20the%20conditioning%20sets.%20These%20bounds%20uncover%20an%20exciting%20interplay%20between%0Athe%20structural%20properties%20of%20the%20graph%20and%20the%20amount%20of%20tests%20required%20to%0Alearn%20a%20Markov%20network.%20The%20starting%20point%20of%20our%20work%20is%20that%20the%20graph%0Aparameter%20maximum%20pairwise%20connectivity%2C%20%24%5Ckappa%24%2C%20that%20is%2C%20the%20maximum%20number%0Aof%20vertex-disjoint%20paths%20connecting%20a%20pair%20of%20vertices%20in%20the%20graph%2C%20is%0Aresponsible%20for%20the%20sizes%20of%20independence%20tests%20required%20to%20learn%20the%20graph.%20On%0Aone%20hand%2C%20we%20show%20that%20at%20least%20one%20test%20with%20the%20size%20of%20the%20conditioning%20set%0Aat%20least%20%24%5Ckappa%24%20is%20always%20necessary.%20On%20the%20other%20hand%2C%20we%20prove%20that%20any%0Agraph%20can%20be%20learned%20by%20performing%20tests%20of%20size%20at%20most%20%24%5Ckappa%24.%20This%0Acompletely%20resolves%20the%20question%20of%20the%20minimum%20size%20of%20conditioning%20sets%0Arequired%20to%20learn%20the%20graph.%20When%20it%20comes%20to%20the%20number%20of%20tests%2C%20our%20upper%0Abound%20on%20the%20sizes%20of%20conditioning%20sets%20implies%20that%20every%20%24n%24-vertex%20graph%20can%0Abe%20learned%20by%20at%20most%20%24n%5E%7B%5Ckappa%7D%24%20tests%20with%20conditioning%20sets%20of%20sizes%20at%0Amost%20%24%5Ckappa%24.%20We%20show%20that%20for%20any%20upper%20bound%20%24q%24%20on%20the%20sizes%20of%20the%0Aconditioning%20sets%2C%20there%20exist%20graphs%20with%20%24O%28n%20q%29%24%20vertices%20that%20require%20at%0Aleast%20%24n%5E%7B%5COmega%28%5Ckappa%29%7D%24%20tests%20to%20learn.%20This%20lower%20bound%20holds%20even%20when%20the%0Atreewidth%20and%20the%20maximum%20degree%20of%20the%20graph%20are%20at%20most%20%24%5Ckappa%2B2%24.%20On%20the%0Apositive%20side%2C%20we%20prove%20that%20every%20graph%20of%20bounded%20treewidth%20can%20be%20learned%20by%0Aa%20polynomial%20number%20of%20tests%20with%20conditioning%20sets%20of%20sizes%20at%20most%20%242%5Ckappa%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08562v1&entry.124074799=Read"},
{"title": "DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for\n  Contact-rich Robotic Manipulation", "author": "Zilin Si and Gu Zhang and Qingwei Ben and Branden Romero and Zhou Xian and Chao Liu and Chuang Gan", "abstract": "  We introduce DIFFTACTILE, a physics-based differentiable tactile simulation\nsystem designed to enhance robotic manipulation with dense and physically\naccurate tactile feedback. In contrast to prior tactile simulators which\nprimarily focus on manipulating rigid bodies and often rely on simplified\napproximations to model stress and deformations of materials in contact,\nDIFFTACTILE emphasizes physics-based contact modeling with high fidelity,\nsupporting simulations of diverse contact modes and interactions with objects\npossessing a wide range of material properties. Our system incorporates several\nkey components, including a Finite Element Method (FEM)-based soft body model\nfor simulating the sensing elastomer, a multi-material simulator for modeling\ndiverse object types (such as elastic, elastoplastic, cables) under\nmanipulation, a penalty-based contact model for handling contact dynamics. The\ndifferentiable nature of our system facilitates gradient-based optimization for\nboth 1) refining physical properties in simulation using real-world data, hence\nnarrowing the sim-to-real gap and 2) efficient learning of tactile-assisted\ngrasping and contact-rich manipulation skills. Additionally, we introduce a\nmethod to infer the optical response of our tactile sensor to contact using an\nefficient pixel-based neural module. We anticipate that DIFFTACTILE will serve\nas a useful platform for studying contact-rich manipulations, leveraging the\nbenefits of dense tactile feedback and differentiable physics. Code and\nsupplementary materials are available at the project website\nhttps://difftactile.github.io/.\n", "link": "http://arxiv.org/abs/2403.08716v1", "date": "2024-03-13", "relevancy": 1.6309, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5542}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5515}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5093}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DIFFTACTILE%3A%20A%20Physics-based%20Differentiable%20Tactile%20Simulator%20for%0A%20%20Contact-rich%20Robotic%20Manipulation&body=Title%3A%20DIFFTACTILE%3A%20A%20Physics-based%20Differentiable%20Tactile%20Simulator%20for%0A%20%20Contact-rich%20Robotic%20Manipulation%0AAuthor%3A%20Zilin%20Si%20and%20Gu%20Zhang%20and%20Qingwei%20Ben%20and%20Branden%20Romero%20and%20Zhou%20Xian%20and%20Chao%20Liu%20and%20Chuang%20Gan%0AAbstract%3A%20%20%20We%20introduce%20DIFFTACTILE%2C%20a%20physics-based%20differentiable%20tactile%20simulation%0Asystem%20designed%20to%20enhance%20robotic%20manipulation%20with%20dense%20and%20physically%0Aaccurate%20tactile%20feedback.%20In%20contrast%20to%20prior%20tactile%20simulators%20which%0Aprimarily%20focus%20on%20manipulating%20rigid%20bodies%20and%20often%20rely%20on%20simplified%0Aapproximations%20to%20model%20stress%20and%20deformations%20of%20materials%20in%20contact%2C%0ADIFFTACTILE%20emphasizes%20physics-based%20contact%20modeling%20with%20high%20fidelity%2C%0Asupporting%20simulations%20of%20diverse%20contact%20modes%20and%20interactions%20with%20objects%0Apossessing%20a%20wide%20range%20of%20material%20properties.%20Our%20system%20incorporates%20several%0Akey%20components%2C%20including%20a%20Finite%20Element%20Method%20%28FEM%29-based%20soft%20body%20model%0Afor%20simulating%20the%20sensing%20elastomer%2C%20a%20multi-material%20simulator%20for%20modeling%0Adiverse%20object%20types%20%28such%20as%20elastic%2C%20elastoplastic%2C%20cables%29%20under%0Amanipulation%2C%20a%20penalty-based%20contact%20model%20for%20handling%20contact%20dynamics.%20The%0Adifferentiable%20nature%20of%20our%20system%20facilitates%20gradient-based%20optimization%20for%0Aboth%201%29%20refining%20physical%20properties%20in%20simulation%20using%20real-world%20data%2C%20hence%0Anarrowing%20the%20sim-to-real%20gap%20and%202%29%20efficient%20learning%20of%20tactile-assisted%0Agrasping%20and%20contact-rich%20manipulation%20skills.%20Additionally%2C%20we%20introduce%20a%0Amethod%20to%20infer%20the%20optical%20response%20of%20our%20tactile%20sensor%20to%20contact%20using%20an%0Aefficient%20pixel-based%20neural%20module.%20We%20anticipate%20that%20DIFFTACTILE%20will%20serve%0Aas%20a%20useful%20platform%20for%20studying%20contact-rich%20manipulations%2C%20leveraging%20the%0Abenefits%20of%20dense%20tactile%20feedback%20and%20differentiable%20physics.%20Code%20and%0Asupplementary%20materials%20are%20available%20at%20the%20project%20website%0Ahttps%3A//difftactile.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08716v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIFFTACTILE%3A%20A%20Physics-based%20Differentiable%20Tactile%20Simulator%20for%0A%20%20Contact-rich%20Robotic%20Manipulation&entry.906535625=Zilin%20Si%20and%20Gu%20Zhang%20and%20Qingwei%20Ben%20and%20Branden%20Romero%20and%20Zhou%20Xian%20and%20Chao%20Liu%20and%20Chuang%20Gan&entry.1292438233=%20%20We%20introduce%20DIFFTACTILE%2C%20a%20physics-based%20differentiable%20tactile%20simulation%0Asystem%20designed%20to%20enhance%20robotic%20manipulation%20with%20dense%20and%20physically%0Aaccurate%20tactile%20feedback.%20In%20contrast%20to%20prior%20tactile%20simulators%20which%0Aprimarily%20focus%20on%20manipulating%20rigid%20bodies%20and%20often%20rely%20on%20simplified%0Aapproximations%20to%20model%20stress%20and%20deformations%20of%20materials%20in%20contact%2C%0ADIFFTACTILE%20emphasizes%20physics-based%20contact%20modeling%20with%20high%20fidelity%2C%0Asupporting%20simulations%20of%20diverse%20contact%20modes%20and%20interactions%20with%20objects%0Apossessing%20a%20wide%20range%20of%20material%20properties.%20Our%20system%20incorporates%20several%0Akey%20components%2C%20including%20a%20Finite%20Element%20Method%20%28FEM%29-based%20soft%20body%20model%0Afor%20simulating%20the%20sensing%20elastomer%2C%20a%20multi-material%20simulator%20for%20modeling%0Adiverse%20object%20types%20%28such%20as%20elastic%2C%20elastoplastic%2C%20cables%29%20under%0Amanipulation%2C%20a%20penalty-based%20contact%20model%20for%20handling%20contact%20dynamics.%20The%0Adifferentiable%20nature%20of%20our%20system%20facilitates%20gradient-based%20optimization%20for%0Aboth%201%29%20refining%20physical%20properties%20in%20simulation%20using%20real-world%20data%2C%20hence%0Anarrowing%20the%20sim-to-real%20gap%20and%202%29%20efficient%20learning%20of%20tactile-assisted%0Agrasping%20and%20contact-rich%20manipulation%20skills.%20Additionally%2C%20we%20introduce%20a%0Amethod%20to%20infer%20the%20optical%20response%20of%20our%20tactile%20sensor%20to%20contact%20using%20an%0Aefficient%20pixel-based%20neural%20module.%20We%20anticipate%20that%20DIFFTACTILE%20will%20serve%0Aas%20a%20useful%20platform%20for%20studying%20contact-rich%20manipulations%2C%20leveraging%20the%0Abenefits%20of%20dense%20tactile%20feedback%20and%20differentiable%20physics.%20Code%20and%0Asupplementary%20materials%20are%20available%20at%20the%20project%20website%0Ahttps%3A//difftactile.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08716v1&entry.124074799=Read"},
{"title": "Learning How to Strategically Disclose Information", "author": "Raj Kiriti Velicheti and Melih Bastopcu and S. Rasoul Etesami and Tamer Ba\u015far", "abstract": "  Strategic information disclosure, in its simplest form, considers a game\nbetween an information provider (sender) who has access to some private\ninformation that an information receiver is interested in. While the receiver\ntakes an action that affects the utilities of both players, the sender can\ndesign information (or modify beliefs) of the receiver through signal\ncommitment, hence posing a Stackelberg game. However, obtaining a Stackelberg\nequilibrium for this game traditionally requires the sender to have access to\nthe receiver's objective. In this work, we consider an online version of\ninformation design where a sender interacts with a receiver of an unknown type\nwho is adversarially chosen at each round. Restricting attention to Gaussian\nprior and quadratic costs for the sender and the receiver, we show that\n$\\mathcal{O}(\\sqrt{T})$ regret is achievable with full information feedback,\nwhere $T$ is the total number of interactions between the sender and the\nreceiver. Further, we propose a novel parametrization that allows the sender to\nachieve $\\mathcal{O}(\\sqrt{T})$ regret for a general convex utility function.\nWe then consider the Bayesian Persuasion problem with an additional cost term\nin the objective function, which penalizes signaling policies that are more\ninformative and obtain $\\mathcal{O}(\\log(T))$ regret. Finally, we establish a\nsublinear regret bound for the partial information feedback setting and provide\nsimulations to support our theoretical results.\n", "link": "http://arxiv.org/abs/2403.08741v1", "date": "2024-03-13", "relevancy": 1.6159, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4238}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3939}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3797}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Learning%20How%20to%20Strategically%20Disclose%20Information&body=Title%3A%20Learning%20How%20to%20Strategically%20Disclose%20Information%0AAuthor%3A%20Raj%20Kiriti%20Velicheti%20and%20Melih%20Bastopcu%20and%20S.%20Rasoul%20Etesami%20and%20Tamer%20Ba%C5%9Far%0AAbstract%3A%20%20%20Strategic%20information%20disclosure%2C%20in%20its%20simplest%20form%2C%20considers%20a%20game%0Abetween%20an%20information%20provider%20%28sender%29%20who%20has%20access%20to%20some%20private%0Ainformation%20that%20an%20information%20receiver%20is%20interested%20in.%20While%20the%20receiver%0Atakes%20an%20action%20that%20affects%20the%20utilities%20of%20both%20players%2C%20the%20sender%20can%0Adesign%20information%20%28or%20modify%20beliefs%29%20of%20the%20receiver%20through%20signal%0Acommitment%2C%20hence%20posing%20a%20Stackelberg%20game.%20However%2C%20obtaining%20a%20Stackelberg%0Aequilibrium%20for%20this%20game%20traditionally%20requires%20the%20sender%20to%20have%20access%20to%0Athe%20receiver%27s%20objective.%20In%20this%20work%2C%20we%20consider%20an%20online%20version%20of%0Ainformation%20design%20where%20a%20sender%20interacts%20with%20a%20receiver%20of%20an%20unknown%20type%0Awho%20is%20adversarially%20chosen%20at%20each%20round.%20Restricting%20attention%20to%20Gaussian%0Aprior%20and%20quadratic%20costs%20for%20the%20sender%20and%20the%20receiver%2C%20we%20show%20that%0A%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%7D%29%24%20regret%20is%20achievable%20with%20full%20information%20feedback%2C%0Awhere%20%24T%24%20is%20the%20total%20number%20of%20interactions%20between%20the%20sender%20and%20the%0Areceiver.%20Further%2C%20we%20propose%20a%20novel%20parametrization%20that%20allows%20the%20sender%20to%0Aachieve%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%7D%29%24%20regret%20for%20a%20general%20convex%20utility%20function.%0AWe%20then%20consider%20the%20Bayesian%20Persuasion%20problem%20with%20an%20additional%20cost%20term%0Ain%20the%20objective%20function%2C%20which%20penalizes%20signaling%20policies%20that%20are%20more%0Ainformative%20and%20obtain%20%24%5Cmathcal%7BO%7D%28%5Clog%28T%29%29%24%20regret.%20Finally%2C%20we%20establish%20a%0Asublinear%20regret%20bound%20for%20the%20partial%20information%20feedback%20setting%20and%20provide%0Asimulations%20to%20support%20our%20theoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08741v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20How%20to%20Strategically%20Disclose%20Information&entry.906535625=Raj%20Kiriti%20Velicheti%20and%20Melih%20Bastopcu%20and%20S.%20Rasoul%20Etesami%20and%20Tamer%20Ba%C5%9Far&entry.1292438233=%20%20Strategic%20information%20disclosure%2C%20in%20its%20simplest%20form%2C%20considers%20a%20game%0Abetween%20an%20information%20provider%20%28sender%29%20who%20has%20access%20to%20some%20private%0Ainformation%20that%20an%20information%20receiver%20is%20interested%20in.%20While%20the%20receiver%0Atakes%20an%20action%20that%20affects%20the%20utilities%20of%20both%20players%2C%20the%20sender%20can%0Adesign%20information%20%28or%20modify%20beliefs%29%20of%20the%20receiver%20through%20signal%0Acommitment%2C%20hence%20posing%20a%20Stackelberg%20game.%20However%2C%20obtaining%20a%20Stackelberg%0Aequilibrium%20for%20this%20game%20traditionally%20requires%20the%20sender%20to%20have%20access%20to%0Athe%20receiver%27s%20objective.%20In%20this%20work%2C%20we%20consider%20an%20online%20version%20of%0Ainformation%20design%20where%20a%20sender%20interacts%20with%20a%20receiver%20of%20an%20unknown%20type%0Awho%20is%20adversarially%20chosen%20at%20each%20round.%20Restricting%20attention%20to%20Gaussian%0Aprior%20and%20quadratic%20costs%20for%20the%20sender%20and%20the%20receiver%2C%20we%20show%20that%0A%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%7D%29%24%20regret%20is%20achievable%20with%20full%20information%20feedback%2C%0Awhere%20%24T%24%20is%20the%20total%20number%20of%20interactions%20between%20the%20sender%20and%20the%0Areceiver.%20Further%2C%20we%20propose%20a%20novel%20parametrization%20that%20allows%20the%20sender%20to%0Aachieve%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%7D%29%24%20regret%20for%20a%20general%20convex%20utility%20function.%0AWe%20then%20consider%20the%20Bayesian%20Persuasion%20problem%20with%20an%20additional%20cost%20term%0Ain%20the%20objective%20function%2C%20which%20penalizes%20signaling%20policies%20that%20are%20more%0Ainformative%20and%20obtain%20%24%5Cmathcal%7BO%7D%28%5Clog%28T%29%29%24%20regret.%20Finally%2C%20we%20establish%20a%0Asublinear%20regret%20bound%20for%20the%20partial%20information%20feedback%20setting%20and%20provide%0Asimulations%20to%20support%20our%20theoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08741v1&entry.124074799=Read"},
{"title": "Verifix: Post-Training Correction to Improve Label Noise Robustness with\n  Verified Samples", "author": "Sangamesh Kodge and Deepak Ravikumar and Gobinda Saha and Kaushik Roy", "abstract": "  Label corruption, where training samples have incorrect labels, can\nsignificantly degrade the performance of machine learning models. This\ncorruption often arises from non-expert labeling or adversarial attacks.\nAcquiring large, perfectly labeled datasets is costly, and retraining large\nmodels from scratch when a clean dataset becomes available is computationally\nexpensive. To address this challenge, we propose Post-Training Correction, a\nnew paradigm that adjusts model parameters after initial training to mitigate\nlabel noise, eliminating the need for retraining. We introduce Verifix, a novel\nSingular Value Decomposition (SVD) based algorithm that leverages a small,\nverified dataset to correct the model weights using a single update. Verifix\nuses SVD to estimate a Clean Activation Space and then projects the model's\nweights onto this space to suppress activations corresponding to corrupted\ndata. We demonstrate Verifix's effectiveness on both synthetic and real-world\nlabel noise. Experiments on the CIFAR dataset with 25% synthetic corruption\nshow 7.36% generalization improvements on average. Additionally, we observe\ngeneralization improvements of up to 2.63% on naturally corrupted datasets like\nWebVision1.0 and Clothing1M.\n", "link": "http://arxiv.org/abs/2403.08618v1", "date": "2024-03-13", "relevancy": 1.6152, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5491}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5393}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5108}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Verifix%3A%20Post-Training%20Correction%20to%20Improve%20Label%20Noise%20Robustness%20with%0A%20%20Verified%20Samples&body=Title%3A%20Verifix%3A%20Post-Training%20Correction%20to%20Improve%20Label%20Noise%20Robustness%20with%0A%20%20Verified%20Samples%0AAuthor%3A%20Sangamesh%20Kodge%20and%20Deepak%20Ravikumar%20and%20Gobinda%20Saha%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Label%20corruption%2C%20where%20training%20samples%20have%20incorrect%20labels%2C%20can%0Asignificantly%20degrade%20the%20performance%20of%20machine%20learning%20models.%20This%0Acorruption%20often%20arises%20from%20non-expert%20labeling%20or%20adversarial%20attacks.%0AAcquiring%20large%2C%20perfectly%20labeled%20datasets%20is%20costly%2C%20and%20retraining%20large%0Amodels%20from%20scratch%20when%20a%20clean%20dataset%20becomes%20available%20is%20computationally%0Aexpensive.%20To%20address%20this%20challenge%2C%20we%20propose%20Post-Training%20Correction%2C%20a%0Anew%20paradigm%20that%20adjusts%20model%20parameters%20after%20initial%20training%20to%20mitigate%0Alabel%20noise%2C%20eliminating%20the%20need%20for%20retraining.%20We%20introduce%20Verifix%2C%20a%20novel%0ASingular%20Value%20Decomposition%20%28SVD%29%20based%20algorithm%20that%20leverages%20a%20small%2C%0Averified%20dataset%20to%20correct%20the%20model%20weights%20using%20a%20single%20update.%20Verifix%0Auses%20SVD%20to%20estimate%20a%20Clean%20Activation%20Space%20and%20then%20projects%20the%20model%27s%0Aweights%20onto%20this%20space%20to%20suppress%20activations%20corresponding%20to%20corrupted%0Adata.%20We%20demonstrate%20Verifix%27s%20effectiveness%20on%20both%20synthetic%20and%20real-world%0Alabel%20noise.%20Experiments%20on%20the%20CIFAR%20dataset%20with%2025%25%20synthetic%20corruption%0Ashow%207.36%25%20generalization%20improvements%20on%20average.%20Additionally%2C%20we%20observe%0Ageneralization%20improvements%20of%20up%20to%202.63%25%20on%20naturally%20corrupted%20datasets%20like%0AWebVision1.0%20and%20Clothing1M.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08618v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verifix%3A%20Post-Training%20Correction%20to%20Improve%20Label%20Noise%20Robustness%20with%0A%20%20Verified%20Samples&entry.906535625=Sangamesh%20Kodge%20and%20Deepak%20Ravikumar%20and%20Gobinda%20Saha%20and%20Kaushik%20Roy&entry.1292438233=%20%20Label%20corruption%2C%20where%20training%20samples%20have%20incorrect%20labels%2C%20can%0Asignificantly%20degrade%20the%20performance%20of%20machine%20learning%20models.%20This%0Acorruption%20often%20arises%20from%20non-expert%20labeling%20or%20adversarial%20attacks.%0AAcquiring%20large%2C%20perfectly%20labeled%20datasets%20is%20costly%2C%20and%20retraining%20large%0Amodels%20from%20scratch%20when%20a%20clean%20dataset%20becomes%20available%20is%20computationally%0Aexpensive.%20To%20address%20this%20challenge%2C%20we%20propose%20Post-Training%20Correction%2C%20a%0Anew%20paradigm%20that%20adjusts%20model%20parameters%20after%20initial%20training%20to%20mitigate%0Alabel%20noise%2C%20eliminating%20the%20need%20for%20retraining.%20We%20introduce%20Verifix%2C%20a%20novel%0ASingular%20Value%20Decomposition%20%28SVD%29%20based%20algorithm%20that%20leverages%20a%20small%2C%0Averified%20dataset%20to%20correct%20the%20model%20weights%20using%20a%20single%20update.%20Verifix%0Auses%20SVD%20to%20estimate%20a%20Clean%20Activation%20Space%20and%20then%20projects%20the%20model%27s%0Aweights%20onto%20this%20space%20to%20suppress%20activations%20corresponding%20to%20corrupted%0Adata.%20We%20demonstrate%20Verifix%27s%20effectiveness%20on%20both%20synthetic%20and%20real-world%0Alabel%20noise.%20Experiments%20on%20the%20CIFAR%20dataset%20with%2025%25%20synthetic%20corruption%0Ashow%207.36%25%20generalization%20improvements%20on%20average.%20Additionally%2C%20we%20observe%0Ageneralization%20improvements%20of%20up%20to%202.63%25%20on%20naturally%20corrupted%20datasets%20like%0AWebVision1.0%20and%20Clothing1M.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08618v1&entry.124074799=Read"},
{"title": "PRAGO: Differentiable Multi-View Pose Optimization From Objectness\n  Detections", "author": "Matteo Taiana and Matteo Toso and Stuart James and Alessio Del Bue", "abstract": "  Robustly estimating camera poses from a set of images is a fundamental task\nwhich remains challenging for differentiable methods, especially in the case of\nsmall and sparse camera pose graphs. To overcome this challenge, we propose\nPose-refined Rotation Averaging Graph Optimization (PRAGO). From a set of\nobjectness detections on unordered images, our method reconstructs the\nrotational pose, and in turn, the absolute pose, in a differentiable manner\nbenefiting from the optimization of a sequence of geometrical tasks. We show\nhow our objectness pose-refinement module in PRAGO is able to refine the\ninherent ambiguities in pairwise relative pose estimation without removing\nedges and avoiding making early decisions on the viability of graph edges.\nPRAGO then refines the absolute rotations through iterative graph construction,\nreweighting the graph edges to compute the final rotational pose, which can be\nconverted into absolute poses using translation averaging. We show that PRAGO\nis able to outperform non-differentiable solvers on small and sparse scenes\nextracted from 7-Scenes achieving a relative improvement of 21% for rotations\nwhile achieving similar translation estimates.\n", "link": "http://arxiv.org/abs/2403.08586v1", "date": "2024-03-13", "relevancy": 1.6002, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5249}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5179}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20PRAGO%3A%20Differentiable%20Multi-View%20Pose%20Optimization%20From%20Objectness%0A%20%20Detections&body=Title%3A%20PRAGO%3A%20Differentiable%20Multi-View%20Pose%20Optimization%20From%20Objectness%0A%20%20Detections%0AAuthor%3A%20Matteo%20Taiana%20and%20Matteo%20Toso%20and%20Stuart%20James%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20Robustly%20estimating%20camera%20poses%20from%20a%20set%20of%20images%20is%20a%20fundamental%20task%0Awhich%20remains%20challenging%20for%20differentiable%20methods%2C%20especially%20in%20the%20case%20of%0Asmall%20and%20sparse%20camera%20pose%20graphs.%20To%20overcome%20this%20challenge%2C%20we%20propose%0APose-refined%20Rotation%20Averaging%20Graph%20Optimization%20%28PRAGO%29.%20From%20a%20set%20of%0Aobjectness%20detections%20on%20unordered%20images%2C%20our%20method%20reconstructs%20the%0Arotational%20pose%2C%20and%20in%20turn%2C%20the%20absolute%20pose%2C%20in%20a%20differentiable%20manner%0Abenefiting%20from%20the%20optimization%20of%20a%20sequence%20of%20geometrical%20tasks.%20We%20show%0Ahow%20our%20objectness%20pose-refinement%20module%20in%20PRAGO%20is%20able%20to%20refine%20the%0Ainherent%20ambiguities%20in%20pairwise%20relative%20pose%20estimation%20without%20removing%0Aedges%20and%20avoiding%20making%20early%20decisions%20on%20the%20viability%20of%20graph%20edges.%0APRAGO%20then%20refines%20the%20absolute%20rotations%20through%20iterative%20graph%20construction%2C%0Areweighting%20the%20graph%20edges%20to%20compute%20the%20final%20rotational%20pose%2C%20which%20can%20be%0Aconverted%20into%20absolute%20poses%20using%20translation%20averaging.%20We%20show%20that%20PRAGO%0Ais%20able%20to%20outperform%20non-differentiable%20solvers%20on%20small%20and%20sparse%20scenes%0Aextracted%20from%207-Scenes%20achieving%20a%20relative%20improvement%20of%2021%25%20for%20rotations%0Awhile%20achieving%20similar%20translation%20estimates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08586v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRAGO%3A%20Differentiable%20Multi-View%20Pose%20Optimization%20From%20Objectness%0A%20%20Detections&entry.906535625=Matteo%20Taiana%20and%20Matteo%20Toso%20and%20Stuart%20James%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20Robustly%20estimating%20camera%20poses%20from%20a%20set%20of%20images%20is%20a%20fundamental%20task%0Awhich%20remains%20challenging%20for%20differentiable%20methods%2C%20especially%20in%20the%20case%20of%0Asmall%20and%20sparse%20camera%20pose%20graphs.%20To%20overcome%20this%20challenge%2C%20we%20propose%0APose-refined%20Rotation%20Averaging%20Graph%20Optimization%20%28PRAGO%29.%20From%20a%20set%20of%0Aobjectness%20detections%20on%20unordered%20images%2C%20our%20method%20reconstructs%20the%0Arotational%20pose%2C%20and%20in%20turn%2C%20the%20absolute%20pose%2C%20in%20a%20differentiable%20manner%0Abenefiting%20from%20the%20optimization%20of%20a%20sequence%20of%20geometrical%20tasks.%20We%20show%0Ahow%20our%20objectness%20pose-refinement%20module%20in%20PRAGO%20is%20able%20to%20refine%20the%0Ainherent%20ambiguities%20in%20pairwise%20relative%20pose%20estimation%20without%20removing%0Aedges%20and%20avoiding%20making%20early%20decisions%20on%20the%20viability%20of%20graph%20edges.%0APRAGO%20then%20refines%20the%20absolute%20rotations%20through%20iterative%20graph%20construction%2C%0Areweighting%20the%20graph%20edges%20to%20compute%20the%20final%20rotational%20pose%2C%20which%20can%20be%0Aconverted%20into%20absolute%20poses%20using%20translation%20averaging.%20We%20show%20that%20PRAGO%0Ais%20able%20to%20outperform%20non-differentiable%20solvers%20on%20small%20and%20sparse%20scenes%0Aextracted%20from%207-Scenes%20achieving%20a%20relative%20improvement%20of%2021%25%20for%20rotations%0Awhile%20achieving%20similar%20translation%20estimates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08586v1&entry.124074799=Read"},
{"title": "Scaling Up Dynamic Human-Scene Interaction Modeling", "author": "Nan Jiang and Zhiyuan Zhang and Hongjie Li and Xiaoxuan Ma and Zan Wang and Yixin Chen and Tengyu Liu and Yixin Zhu and Siyuan Huang", "abstract": "  Confronting the challenges of data scarcity and advanced motion synthesis in\nhuman-scene interaction modeling, we introduce the TRUMANS dataset alongside a\nnovel HSI motion synthesis method. TRUMANS stands as the most comprehensive\nmotion-captured HSI dataset currently available, encompassing over 15 hours of\nhuman interactions across 100 indoor scenes. It intricately captures whole-body\nhuman motions and part-level object dynamics, focusing on the realism of\ncontact. This dataset is further scaled up by transforming physical\nenvironments into exact virtual models and applying extensive augmentations to\nappearance and motion for both humans and objects while maintaining interaction\nfidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model\nthat efficiently generates HSI sequences of any length, taking into account\nboth scene context and intended actions. In experiments, our approach shows\nremarkable zero-shot generalizability on a range of 3D scene datasets (e.g.,\nPROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic\noriginal motion-captured sequences, as confirmed by quantitative experiments\nand human studies.\n", "link": "http://arxiv.org/abs/2403.08629v1", "date": "2024-03-13", "relevancy": 1.5988, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5596}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5186}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Scaling%20Up%20Dynamic%20Human-Scene%20Interaction%20Modeling&body=Title%3A%20Scaling%20Up%20Dynamic%20Human-Scene%20Interaction%20Modeling%0AAuthor%3A%20Nan%20Jiang%20and%20Zhiyuan%20Zhang%20and%20Hongjie%20Li%20and%20Xiaoxuan%20Ma%20and%20Zan%20Wang%20and%20Yixin%20Chen%20and%20Tengyu%20Liu%20and%20Yixin%20Zhu%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Confronting%20the%20challenges%20of%20data%20scarcity%20and%20advanced%20motion%20synthesis%20in%0Ahuman-scene%20interaction%20modeling%2C%20we%20introduce%20the%20TRUMANS%20dataset%20alongside%20a%0Anovel%20HSI%20motion%20synthesis%20method.%20TRUMANS%20stands%20as%20the%20most%20comprehensive%0Amotion-captured%20HSI%20dataset%20currently%20available%2C%20encompassing%20over%2015%20hours%20of%0Ahuman%20interactions%20across%20100%20indoor%20scenes.%20It%20intricately%20captures%20whole-body%0Ahuman%20motions%20and%20part-level%20object%20dynamics%2C%20focusing%20on%20the%20realism%20of%0Acontact.%20This%20dataset%20is%20further%20scaled%20up%20by%20transforming%20physical%0Aenvironments%20into%20exact%20virtual%20models%20and%20applying%20extensive%20augmentations%20to%0Aappearance%20and%20motion%20for%20both%20humans%20and%20objects%20while%20maintaining%20interaction%0Afidelity.%20Utilizing%20TRUMANS%2C%20we%20devise%20a%20diffusion-based%20autoregressive%20model%0Athat%20efficiently%20generates%20HSI%20sequences%20of%20any%20length%2C%20taking%20into%20account%0Aboth%20scene%20context%20and%20intended%20actions.%20In%20experiments%2C%20our%20approach%20shows%0Aremarkable%20zero-shot%20generalizability%20on%20a%20range%20of%203D%20scene%20datasets%20%28e.g.%2C%0APROX%2C%20Replica%2C%20ScanNet%2C%20ScanNet%2B%2B%29%2C%20producing%20motions%20that%20closely%20mimic%0Aoriginal%20motion-captured%20sequences%2C%20as%20confirmed%20by%20quantitative%20experiments%0Aand%20human%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08629v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Up%20Dynamic%20Human-Scene%20Interaction%20Modeling&entry.906535625=Nan%20Jiang%20and%20Zhiyuan%20Zhang%20and%20Hongjie%20Li%20and%20Xiaoxuan%20Ma%20and%20Zan%20Wang%20and%20Yixin%20Chen%20and%20Tengyu%20Liu%20and%20Yixin%20Zhu%20and%20Siyuan%20Huang&entry.1292438233=%20%20Confronting%20the%20challenges%20of%20data%20scarcity%20and%20advanced%20motion%20synthesis%20in%0Ahuman-scene%20interaction%20modeling%2C%20we%20introduce%20the%20TRUMANS%20dataset%20alongside%20a%0Anovel%20HSI%20motion%20synthesis%20method.%20TRUMANS%20stands%20as%20the%20most%20comprehensive%0Amotion-captured%20HSI%20dataset%20currently%20available%2C%20encompassing%20over%2015%20hours%20of%0Ahuman%20interactions%20across%20100%20indoor%20scenes.%20It%20intricately%20captures%20whole-body%0Ahuman%20motions%20and%20part-level%20object%20dynamics%2C%20focusing%20on%20the%20realism%20of%0Acontact.%20This%20dataset%20is%20further%20scaled%20up%20by%20transforming%20physical%0Aenvironments%20into%20exact%20virtual%20models%20and%20applying%20extensive%20augmentations%20to%0Aappearance%20and%20motion%20for%20both%20humans%20and%20objects%20while%20maintaining%20interaction%0Afidelity.%20Utilizing%20TRUMANS%2C%20we%20devise%20a%20diffusion-based%20autoregressive%20model%0Athat%20efficiently%20generates%20HSI%20sequences%20of%20any%20length%2C%20taking%20into%20account%0Aboth%20scene%20context%20and%20intended%20actions.%20In%20experiments%2C%20our%20approach%20shows%0Aremarkable%20zero-shot%20generalizability%20on%20a%20range%20of%203D%20scene%20datasets%20%28e.g.%2C%0APROX%2C%20Replica%2C%20ScanNet%2C%20ScanNet%2B%2B%29%2C%20producing%20motions%20that%20closely%20mimic%0Aoriginal%20motion-captured%20sequences%2C%20as%20confirmed%20by%20quantitative%20experiments%0Aand%20human%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08629v1&entry.124074799=Read"},
{"title": "Spatiotemporal Diffusion Model with Paired Sampling for Accelerated\n  Cardiac Cine MRI", "author": "Shihan Qiu and Shaoyan Pan and Yikang Liu and Lin Zhao and Jian Xu and Qi Liu and Terrence Chen and Eric Z. Chen and Xiao Chen and Shanhui Sun", "abstract": "  Current deep learning reconstruction for accelerated cardiac cine MRI suffers\nfrom spatial and temporal blurring. We aim to improve image sharpness and\nmotion delineation for cine MRI under high undersampling rates. A\nspatiotemporal diffusion enhancement model conditional on an existing deep\nlearning reconstruction along with a novel paired sampling strategy was\ndeveloped. The diffusion model provided sharper tissue boundaries and clearer\nmotion than the original reconstruction in experts evaluation on clinical data.\nThe innovative paired sampling strategy substantially reduced artificial noises\nin the generative results.\n", "link": "http://arxiv.org/abs/2403.08758v1", "date": "2024-03-13", "relevancy": 1.5898, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5419}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5298}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5252}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Spatiotemporal%20Diffusion%20Model%20with%20Paired%20Sampling%20for%20Accelerated%0A%20%20Cardiac%20Cine%20MRI&body=Title%3A%20Spatiotemporal%20Diffusion%20Model%20with%20Paired%20Sampling%20for%20Accelerated%0A%20%20Cardiac%20Cine%20MRI%0AAuthor%3A%20Shihan%20Qiu%20and%20Shaoyan%20Pan%20and%20Yikang%20Liu%20and%20Lin%20Zhao%20and%20Jian%20Xu%20and%20Qi%20Liu%20and%20Terrence%20Chen%20and%20Eric%20Z.%20Chen%20and%20Xiao%20Chen%20and%20Shanhui%20Sun%0AAbstract%3A%20%20%20Current%20deep%20learning%20reconstruction%20for%20accelerated%20cardiac%20cine%20MRI%20suffers%0Afrom%20spatial%20and%20temporal%20blurring.%20We%20aim%20to%20improve%20image%20sharpness%20and%0Amotion%20delineation%20for%20cine%20MRI%20under%20high%20undersampling%20rates.%20A%0Aspatiotemporal%20diffusion%20enhancement%20model%20conditional%20on%20an%20existing%20deep%0Alearning%20reconstruction%20along%20with%20a%20novel%20paired%20sampling%20strategy%20was%0Adeveloped.%20The%20diffusion%20model%20provided%20sharper%20tissue%20boundaries%20and%20clearer%0Amotion%20than%20the%20original%20reconstruction%20in%20experts%20evaluation%20on%20clinical%20data.%0AThe%20innovative%20paired%20sampling%20strategy%20substantially%20reduced%20artificial%20noises%0Ain%20the%20generative%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08758v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatiotemporal%20Diffusion%20Model%20with%20Paired%20Sampling%20for%20Accelerated%0A%20%20Cardiac%20Cine%20MRI&entry.906535625=Shihan%20Qiu%20and%20Shaoyan%20Pan%20and%20Yikang%20Liu%20and%20Lin%20Zhao%20and%20Jian%20Xu%20and%20Qi%20Liu%20and%20Terrence%20Chen%20and%20Eric%20Z.%20Chen%20and%20Xiao%20Chen%20and%20Shanhui%20Sun&entry.1292438233=%20%20Current%20deep%20learning%20reconstruction%20for%20accelerated%20cardiac%20cine%20MRI%20suffers%0Afrom%20spatial%20and%20temporal%20blurring.%20We%20aim%20to%20improve%20image%20sharpness%20and%0Amotion%20delineation%20for%20cine%20MRI%20under%20high%20undersampling%20rates.%20A%0Aspatiotemporal%20diffusion%20enhancement%20model%20conditional%20on%20an%20existing%20deep%0Alearning%20reconstruction%20along%20with%20a%20novel%20paired%20sampling%20strategy%20was%0Adeveloped.%20The%20diffusion%20model%20provided%20sharper%20tissue%20boundaries%20and%20clearer%0Amotion%20than%20the%20original%20reconstruction%20in%20experts%20evaluation%20on%20clinical%20data.%0AThe%20innovative%20paired%20sampling%20strategy%20substantially%20reduced%20artificial%20noises%0Ain%20the%20generative%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08758v1&entry.124074799=Read"},
{"title": "SkillDiffuser: Interpretable Hierarchical Planning via Skill\n  Abstractions in Diffusion-Based Task Execution", "author": "Zhixuan Liang and Yao Mu and Hengbo Ma and Masayoshi Tomizuka and Mingyu Ding and Ping Luo", "abstract": "  Diffusion models have demonstrated strong potential for robotic trajectory\nplanning. However, generating coherent trajectories from high-level\ninstructions remains challenging, especially for long-range composition tasks\nrequiring multiple sequential skills. We propose SkillDiffuser, an end-to-end\nhierarchical planning framework integrating interpretable skill learning with\nconditional diffusion planning to address this problem. At the higher level,\nthe skill abstraction module learns discrete, human-understandable skill\nrepresentations from visual observations and language instructions. These\nlearned skill embeddings are then used to condition the diffusion model to\ngenerate customized latent trajectories aligned with the skills. This allows\ngenerating diverse state trajectories that adhere to the learnable skills. By\nintegrating skill learning with conditional trajectory generation,\nSkillDiffuser produces coherent behavior following abstract instructions across\ndiverse tasks. Experiments on multi-task robotic manipulation benchmarks like\nMeta-World and LOReL demonstrate state-of-the-art performance and\nhuman-interpretable skill representations from SkillDiffuser. More\nvisualization results and information could be found on our website.\n", "link": "http://arxiv.org/abs/2312.11598v2", "date": "2024-03-13", "relevancy": 1.5781, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.536}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5286}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5095}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20SkillDiffuser%3A%20Interpretable%20Hierarchical%20Planning%20via%20Skill%0A%20%20Abstractions%20in%20Diffusion-Based%20Task%20Execution&body=Title%3A%20SkillDiffuser%3A%20Interpretable%20Hierarchical%20Planning%20via%20Skill%0A%20%20Abstractions%20in%20Diffusion-Based%20Task%20Execution%0AAuthor%3A%20Zhixuan%20Liang%20and%20Yao%20Mu%20and%20Hengbo%20Ma%20and%20Masayoshi%20Tomizuka%20and%20Mingyu%20Ding%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20strong%20potential%20for%20robotic%20trajectory%0Aplanning.%20However%2C%20generating%20coherent%20trajectories%20from%20high-level%0Ainstructions%20remains%20challenging%2C%20especially%20for%20long-range%20composition%20tasks%0Arequiring%20multiple%20sequential%20skills.%20We%20propose%20SkillDiffuser%2C%20an%20end-to-end%0Ahierarchical%20planning%20framework%20integrating%20interpretable%20skill%20learning%20with%0Aconditional%20diffusion%20planning%20to%20address%20this%20problem.%20At%20the%20higher%20level%2C%0Athe%20skill%20abstraction%20module%20learns%20discrete%2C%20human-understandable%20skill%0Arepresentations%20from%20visual%20observations%20and%20language%20instructions.%20These%0Alearned%20skill%20embeddings%20are%20then%20used%20to%20condition%20the%20diffusion%20model%20to%0Agenerate%20customized%20latent%20trajectories%20aligned%20with%20the%20skills.%20This%20allows%0Agenerating%20diverse%20state%20trajectories%20that%20adhere%20to%20the%20learnable%20skills.%20By%0Aintegrating%20skill%20learning%20with%20conditional%20trajectory%20generation%2C%0ASkillDiffuser%20produces%20coherent%20behavior%20following%20abstract%20instructions%20across%0Adiverse%20tasks.%20Experiments%20on%20multi-task%20robotic%20manipulation%20benchmarks%20like%0AMeta-World%20and%20LOReL%20demonstrate%20state-of-the-art%20performance%20and%0Ahuman-interpretable%20skill%20representations%20from%20SkillDiffuser.%20More%0Avisualization%20results%20and%20information%20could%20be%20found%20on%20our%20website.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11598v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkillDiffuser%3A%20Interpretable%20Hierarchical%20Planning%20via%20Skill%0A%20%20Abstractions%20in%20Diffusion-Based%20Task%20Execution&entry.906535625=Zhixuan%20Liang%20and%20Yao%20Mu%20and%20Hengbo%20Ma%20and%20Masayoshi%20Tomizuka%20and%20Mingyu%20Ding%20and%20Ping%20Luo&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20strong%20potential%20for%20robotic%20trajectory%0Aplanning.%20However%2C%20generating%20coherent%20trajectories%20from%20high-level%0Ainstructions%20remains%20challenging%2C%20especially%20for%20long-range%20composition%20tasks%0Arequiring%20multiple%20sequential%20skills.%20We%20propose%20SkillDiffuser%2C%20an%20end-to-end%0Ahierarchical%20planning%20framework%20integrating%20interpretable%20skill%20learning%20with%0Aconditional%20diffusion%20planning%20to%20address%20this%20problem.%20At%20the%20higher%20level%2C%0Athe%20skill%20abstraction%20module%20learns%20discrete%2C%20human-understandable%20skill%0Arepresentations%20from%20visual%20observations%20and%20language%20instructions.%20These%0Alearned%20skill%20embeddings%20are%20then%20used%20to%20condition%20the%20diffusion%20model%20to%0Agenerate%20customized%20latent%20trajectories%20aligned%20with%20the%20skills.%20This%20allows%0Agenerating%20diverse%20state%20trajectories%20that%20adhere%20to%20the%20learnable%20skills.%20By%0Aintegrating%20skill%20learning%20with%20conditional%20trajectory%20generation%2C%0ASkillDiffuser%20produces%20coherent%20behavior%20following%20abstract%20instructions%20across%0Adiverse%20tasks.%20Experiments%20on%20multi-task%20robotic%20manipulation%20benchmarks%20like%0AMeta-World%20and%20LOReL%20demonstrate%20state-of-the-art%20performance%20and%0Ahuman-interpretable%20skill%20representations%20from%20SkillDiffuser.%20More%0Avisualization%20results%20and%20information%20could%20be%20found%20on%20our%20website.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11598v2&entry.124074799=Read"},
{"title": "Synthesizing Robust Walking Gaits via Discrete-Time Barrier Functions\n  with Application to Multi-Contact Exoskeleton Locomotion", "author": "Maegan Tucker and Kejun Li and Aaron D. Ames", "abstract": "  Successfully achieving bipedal locomotion remains challenging due to\nreal-world factors such as model uncertainty, random disturbances, and\nimperfect state estimation. In this work, we propose a novel metric for\nlocomotive robustness -- the estimated size of the hybrid forward invariant set\nassociated with the step-to-step dynamics. Here, the forward invariant set can\nbe loosely interpreted as the region of attraction for the discrete-time\ndynamics. We illustrate the use of this metric towards synthesizing nominal\nwalking gaits using a simulation-in-the-loop learning approach. Further, we\nleverage discrete-time barrier functions and a sampling-based approach to\napproximate sets that are maximally forward invariant. Lastly, we\nexperimentally demonstrate that this approach results in successful locomotion\nfor both flat-foot walking and multi-contact walking on the Atalante lower-body\nexoskeleton.\n", "link": "http://arxiv.org/abs/2310.06169v2", "date": "2024-03-13", "relevancy": 1.5752, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.59}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5092}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4997}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Synthesizing%20Robust%20Walking%20Gaits%20via%20Discrete-Time%20Barrier%20Functions%0A%20%20with%20Application%20to%20Multi-Contact%20Exoskeleton%20Locomotion&body=Title%3A%20Synthesizing%20Robust%20Walking%20Gaits%20via%20Discrete-Time%20Barrier%20Functions%0A%20%20with%20Application%20to%20Multi-Contact%20Exoskeleton%20Locomotion%0AAuthor%3A%20Maegan%20Tucker%20and%20Kejun%20Li%20and%20Aaron%20D.%20Ames%0AAbstract%3A%20%20%20Successfully%20achieving%20bipedal%20locomotion%20remains%20challenging%20due%20to%0Areal-world%20factors%20such%20as%20model%20uncertainty%2C%20random%20disturbances%2C%20and%0Aimperfect%20state%20estimation.%20In%20this%20work%2C%20we%20propose%20a%20novel%20metric%20for%0Alocomotive%20robustness%20--%20the%20estimated%20size%20of%20the%20hybrid%20forward%20invariant%20set%0Aassociated%20with%20the%20step-to-step%20dynamics.%20Here%2C%20the%20forward%20invariant%20set%20can%0Abe%20loosely%20interpreted%20as%20the%20region%20of%20attraction%20for%20the%20discrete-time%0Adynamics.%20We%20illustrate%20the%20use%20of%20this%20metric%20towards%20synthesizing%20nominal%0Awalking%20gaits%20using%20a%20simulation-in-the-loop%20learning%20approach.%20Further%2C%20we%0Aleverage%20discrete-time%20barrier%20functions%20and%20a%20sampling-based%20approach%20to%0Aapproximate%20sets%20that%20are%20maximally%20forward%20invariant.%20Lastly%2C%20we%0Aexperimentally%20demonstrate%20that%20this%20approach%20results%20in%20successful%20locomotion%0Afor%20both%20flat-foot%20walking%20and%20multi-contact%20walking%20on%20the%20Atalante%20lower-body%0Aexoskeleton.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06169v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%20Robust%20Walking%20Gaits%20via%20Discrete-Time%20Barrier%20Functions%0A%20%20with%20Application%20to%20Multi-Contact%20Exoskeleton%20Locomotion&entry.906535625=Maegan%20Tucker%20and%20Kejun%20Li%20and%20Aaron%20D.%20Ames&entry.1292438233=%20%20Successfully%20achieving%20bipedal%20locomotion%20remains%20challenging%20due%20to%0Areal-world%20factors%20such%20as%20model%20uncertainty%2C%20random%20disturbances%2C%20and%0Aimperfect%20state%20estimation.%20In%20this%20work%2C%20we%20propose%20a%20novel%20metric%20for%0Alocomotive%20robustness%20--%20the%20estimated%20size%20of%20the%20hybrid%20forward%20invariant%20set%0Aassociated%20with%20the%20step-to-step%20dynamics.%20Here%2C%20the%20forward%20invariant%20set%20can%0Abe%20loosely%20interpreted%20as%20the%20region%20of%20attraction%20for%20the%20discrete-time%0Adynamics.%20We%20illustrate%20the%20use%20of%20this%20metric%20towards%20synthesizing%20nominal%0Awalking%20gaits%20using%20a%20simulation-in-the-loop%20learning%20approach.%20Further%2C%20we%0Aleverage%20discrete-time%20barrier%20functions%20and%20a%20sampling-based%20approach%20to%0Aapproximate%20sets%20that%20are%20maximally%20forward%20invariant.%20Lastly%2C%20we%0Aexperimentally%20demonstrate%20that%20this%20approach%20results%20in%20successful%20locomotion%0Afor%20both%20flat-foot%20walking%20and%20multi-contact%20walking%20on%20the%20Atalante%20lower-body%0Aexoskeleton.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06169v2&entry.124074799=Read"},
{"title": "GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting\n  Editing", "author": "Jing Wu and Jia-Wang Bian and Xinghui Li and Guangrun Wang and Ian Reid and Philip Torr and Victor Adrian Prisacariu", "abstract": "  We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed\nby the 3D Gaussian Splatting (3DGS).\n  Our method first renders a collection of images by using the 3DGS and edits\nthem by using a pre-trained 2D diffusion model (ControlNet) based on the input\nprompt, which is then used to optimise the 3D model.\n  Our key contribution is multi-view consistent editing, which enables editing\nall images together instead of iteratively editing one image while updating the\n3D model as in previous works.\n  It leads to faster editing as well as higher visual quality.\n  This is achieved by the two terms:\n  (a) depth-conditioned editing that enforces geometric consistency across\nmulti-view images by leveraging naturally consistent depth maps.\n  (b) attention-based latent code alignment that unifies the appearance of\nedited images by conditioning their editing to several reference views through\nself and cross-view attention between images' latent representations.\n  Experiments demonstrate that our method achieves faster editing and better\nvisual results than previous state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.08733v1", "date": "2024-03-13", "relevancy": 1.5622, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5255}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5162}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5134}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20GaussCtrl%3A%20Multi-View%20Consistent%20Text-Driven%203D%20Gaussian%20Splatting%0A%20%20Editing&body=Title%3A%20GaussCtrl%3A%20Multi-View%20Consistent%20Text-Driven%203D%20Gaussian%20Splatting%0A%20%20Editing%0AAuthor%3A%20Jing%20Wu%20and%20Jia-Wang%20Bian%20and%20Xinghui%20Li%20and%20Guangrun%20Wang%20and%20Ian%20Reid%20and%20Philip%20Torr%20and%20Victor%20Adrian%20Prisacariu%0AAbstract%3A%20%20%20We%20propose%20GaussCtrl%2C%20a%20text-driven%20method%20to%20edit%20a%203D%20scene%20reconstructed%0Aby%20the%203D%20Gaussian%20Splatting%20%283DGS%29.%0A%20%20Our%20method%20first%20renders%20a%20collection%20of%20images%20by%20using%20the%203DGS%20and%20edits%0Athem%20by%20using%20a%20pre-trained%202D%20diffusion%20model%20%28ControlNet%29%20based%20on%20the%20input%0Aprompt%2C%20which%20is%20then%20used%20to%20optimise%20the%203D%20model.%0A%20%20Our%20key%20contribution%20is%20multi-view%20consistent%20editing%2C%20which%20enables%20editing%0Aall%20images%20together%20instead%20of%20iteratively%20editing%20one%20image%20while%20updating%20the%0A3D%20model%20as%20in%20previous%20works.%0A%20%20It%20leads%20to%20faster%20editing%20as%20well%20as%20higher%20visual%20quality.%0A%20%20This%20is%20achieved%20by%20the%20two%20terms%3A%0A%20%20%28a%29%20depth-conditioned%20editing%20that%20enforces%20geometric%20consistency%20across%0Amulti-view%20images%20by%20leveraging%20naturally%20consistent%20depth%20maps.%0A%20%20%28b%29%20attention-based%20latent%20code%20alignment%20that%20unifies%20the%20appearance%20of%0Aedited%20images%20by%20conditioning%20their%20editing%20to%20several%20reference%20views%20through%0Aself%20and%20cross-view%20attention%20between%20images%27%20latent%20representations.%0A%20%20Experiments%20demonstrate%20that%20our%20method%20achieves%20faster%20editing%20and%20better%0Avisual%20results%20than%20previous%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08733v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussCtrl%3A%20Multi-View%20Consistent%20Text-Driven%203D%20Gaussian%20Splatting%0A%20%20Editing&entry.906535625=Jing%20Wu%20and%20Jia-Wang%20Bian%20and%20Xinghui%20Li%20and%20Guangrun%20Wang%20and%20Ian%20Reid%20and%20Philip%20Torr%20and%20Victor%20Adrian%20Prisacariu&entry.1292438233=%20%20We%20propose%20GaussCtrl%2C%20a%20text-driven%20method%20to%20edit%20a%203D%20scene%20reconstructed%0Aby%20the%203D%20Gaussian%20Splatting%20%283DGS%29.%0A%20%20Our%20method%20first%20renders%20a%20collection%20of%20images%20by%20using%20the%203DGS%20and%20edits%0Athem%20by%20using%20a%20pre-trained%202D%20diffusion%20model%20%28ControlNet%29%20based%20on%20the%20input%0Aprompt%2C%20which%20is%20then%20used%20to%20optimise%20the%203D%20model.%0A%20%20Our%20key%20contribution%20is%20multi-view%20consistent%20editing%2C%20which%20enables%20editing%0Aall%20images%20together%20instead%20of%20iteratively%20editing%20one%20image%20while%20updating%20the%0A3D%20model%20as%20in%20previous%20works.%0A%20%20It%20leads%20to%20faster%20editing%20as%20well%20as%20higher%20visual%20quality.%0A%20%20This%20is%20achieved%20by%20the%20two%20terms%3A%0A%20%20%28a%29%20depth-conditioned%20editing%20that%20enforces%20geometric%20consistency%20across%0Amulti-view%20images%20by%20leveraging%20naturally%20consistent%20depth%20maps.%0A%20%20%28b%29%20attention-based%20latent%20code%20alignment%20that%20unifies%20the%20appearance%20of%0Aedited%20images%20by%20conditioning%20their%20editing%20to%20several%20reference%20views%20through%0Aself%20and%20cross-view%20attention%20between%20images%27%20latent%20representations.%0A%20%20Experiments%20demonstrate%20that%20our%20method%20achieves%20faster%20editing%20and%20better%0Avisual%20results%20than%20previous%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08733v1&entry.124074799=Read"},
{"title": "Can physical information aid the generalization ability of Neural\n  Networks for hydraulic modeling?", "author": "Gianmarco Guglielmo and Andrea Montessori and Jean-Michel Tucny and Michele La Rocca and Pietro Prestininzi", "abstract": "  Application of Neural Networks to river hydraulics is fledgling, despite the\nfield suffering from data scarcity, a challenge for machine learning\ntechniques. Consequently, many purely data-driven Neural Networks proved to\nlack predictive capabilities. In this work, we propose to mitigate such problem\nby introducing physical information into the training phase. The idea is\nborrowed from Physics-Informed Neural Networks which have been recently\nproposed in other contexts. Physics-Informed Neural Networks embed physical\ninformation in the form of the residual of the Partial Differential Equations\n(PDEs) governing the phenomenon and, as such, are conceived as neural solvers,\ni.e. an alternative to traditional numerical solvers. Such approach is seldom\nsuitable for environmental hydraulics, where epistemic uncertainties are large,\nand computing residuals of PDEs exhibits difficulties similar to those faced by\nclassical numerical methods. Instead, we envisaged the employment of Neural\nNetworks as neural operators, featuring physical constraints formulated without\nresorting to PDEs. The proposed novel methodology shares similarities with data\naugmentation and regularization. We show that incorporating such soft physical\ninformation can improve predictive capabilities.\n", "link": "http://arxiv.org/abs/2403.08589v1", "date": "2024-03-13", "relevancy": 1.561, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5701}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5151}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5025}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Can%20physical%20information%20aid%20the%20generalization%20ability%20of%20Neural%0A%20%20Networks%20for%20hydraulic%20modeling%3F&body=Title%3A%20Can%20physical%20information%20aid%20the%20generalization%20ability%20of%20Neural%0A%20%20Networks%20for%20hydraulic%20modeling%3F%0AAuthor%3A%20Gianmarco%20Guglielmo%20and%20Andrea%20Montessori%20and%20Jean-Michel%20Tucny%20and%20Michele%20La%20Rocca%20and%20Pietro%20Prestininzi%0AAbstract%3A%20%20%20Application%20of%20Neural%20Networks%20to%20river%20hydraulics%20is%20fledgling%2C%20despite%20the%0Afield%20suffering%20from%20data%20scarcity%2C%20a%20challenge%20for%20machine%20learning%0Atechniques.%20Consequently%2C%20many%20purely%20data-driven%20Neural%20Networks%20proved%20to%0Alack%20predictive%20capabilities.%20In%20this%20work%2C%20we%20propose%20to%20mitigate%20such%20problem%0Aby%20introducing%20physical%20information%20into%20the%20training%20phase.%20The%20idea%20is%0Aborrowed%20from%20Physics-Informed%20Neural%20Networks%20which%20have%20been%20recently%0Aproposed%20in%20other%20contexts.%20Physics-Informed%20Neural%20Networks%20embed%20physical%0Ainformation%20in%20the%20form%20of%20the%20residual%20of%20the%20Partial%20Differential%20Equations%0A%28PDEs%29%20governing%20the%20phenomenon%20and%2C%20as%20such%2C%20are%20conceived%20as%20neural%20solvers%2C%0Ai.e.%20an%20alternative%20to%20traditional%20numerical%20solvers.%20Such%20approach%20is%20seldom%0Asuitable%20for%20environmental%20hydraulics%2C%20where%20epistemic%20uncertainties%20are%20large%2C%0Aand%20computing%20residuals%20of%20PDEs%20exhibits%20difficulties%20similar%20to%20those%20faced%20by%0Aclassical%20numerical%20methods.%20Instead%2C%20we%20envisaged%20the%20employment%20of%20Neural%0ANetworks%20as%20neural%20operators%2C%20featuring%20physical%20constraints%20formulated%20without%0Aresorting%20to%20PDEs.%20The%20proposed%20novel%20methodology%20shares%20similarities%20with%20data%0Aaugmentation%20and%20regularization.%20We%20show%20that%20incorporating%20such%20soft%20physical%0Ainformation%20can%20improve%20predictive%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08589v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20physical%20information%20aid%20the%20generalization%20ability%20of%20Neural%0A%20%20Networks%20for%20hydraulic%20modeling%3F&entry.906535625=Gianmarco%20Guglielmo%20and%20Andrea%20Montessori%20and%20Jean-Michel%20Tucny%20and%20Michele%20La%20Rocca%20and%20Pietro%20Prestininzi&entry.1292438233=%20%20Application%20of%20Neural%20Networks%20to%20river%20hydraulics%20is%20fledgling%2C%20despite%20the%0Afield%20suffering%20from%20data%20scarcity%2C%20a%20challenge%20for%20machine%20learning%0Atechniques.%20Consequently%2C%20many%20purely%20data-driven%20Neural%20Networks%20proved%20to%0Alack%20predictive%20capabilities.%20In%20this%20work%2C%20we%20propose%20to%20mitigate%20such%20problem%0Aby%20introducing%20physical%20information%20into%20the%20training%20phase.%20The%20idea%20is%0Aborrowed%20from%20Physics-Informed%20Neural%20Networks%20which%20have%20been%20recently%0Aproposed%20in%20other%20contexts.%20Physics-Informed%20Neural%20Networks%20embed%20physical%0Ainformation%20in%20the%20form%20of%20the%20residual%20of%20the%20Partial%20Differential%20Equations%0A%28PDEs%29%20governing%20the%20phenomenon%20and%2C%20as%20such%2C%20are%20conceived%20as%20neural%20solvers%2C%0Ai.e.%20an%20alternative%20to%20traditional%20numerical%20solvers.%20Such%20approach%20is%20seldom%0Asuitable%20for%20environmental%20hydraulics%2C%20where%20epistemic%20uncertainties%20are%20large%2C%0Aand%20computing%20residuals%20of%20PDEs%20exhibits%20difficulties%20similar%20to%20those%20faced%20by%0Aclassical%20numerical%20methods.%20Instead%2C%20we%20envisaged%20the%20employment%20of%20Neural%0ANetworks%20as%20neural%20operators%2C%20featuring%20physical%20constraints%20formulated%20without%0Aresorting%20to%20PDEs.%20The%20proposed%20novel%20methodology%20shares%20similarities%20with%20data%0Aaugmentation%20and%20regularization.%20We%20show%20that%20incorporating%20such%20soft%20physical%0Ainformation%20can%20improve%20predictive%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08589v1&entry.124074799=Read"},
{"title": "Diffusion-based Iterative Counterfactual Explanations for Fetal\n  Ultrasound Image Quality Assessment", "author": "Paraskevas Pegios and Manxi Lin and Nina Weng and Morten Bo S\u00f8ndergaard Svendsen and Zahra Bashir and Siavash Bigdeli and Anders Nymark Christensen and Martin Tolsgaard and Aasa Feragen", "abstract": "  Obstetric ultrasound image quality is crucial for accurate diagnosis and\nmonitoring of fetal health. However, producing high-quality standard planes is\ndifficult, influenced by the sonographer's expertise and factors like the\nmaternal BMI or the fetus dynamics. In this work, we propose using\ndiffusion-based counterfactual explainable AI to generate realistic\nhigh-quality standard planes from low-quality non-standard ones. Through\nquantitative and qualitative evaluation, we demonstrate the effectiveness of\nour method in producing plausible counterfactuals of increased quality. This\nshows future promise both for enhancing training of clinicians by providing\nvisual feedback, as well as for improving image quality and, consequently,\ndownstream diagnosis and monitoring.\n", "link": "http://arxiv.org/abs/2403.08700v1", "date": "2024-03-13", "relevancy": 1.5419, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5205}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5189}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4928}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Iterative%20Counterfactual%20Explanations%20for%20Fetal%0A%20%20Ultrasound%20Image%20Quality%20Assessment&body=Title%3A%20Diffusion-based%20Iterative%20Counterfactual%20Explanations%20for%20Fetal%0A%20%20Ultrasound%20Image%20Quality%20Assessment%0AAuthor%3A%20Paraskevas%20Pegios%20and%20Manxi%20Lin%20and%20Nina%20Weng%20and%20Morten%20Bo%20S%C3%B8ndergaard%20Svendsen%20and%20Zahra%20Bashir%20and%20Siavash%20Bigdeli%20and%20Anders%20Nymark%20Christensen%20and%20Martin%20Tolsgaard%20and%20Aasa%20Feragen%0AAbstract%3A%20%20%20Obstetric%20ultrasound%20image%20quality%20is%20crucial%20for%20accurate%20diagnosis%20and%0Amonitoring%20of%20fetal%20health.%20However%2C%20producing%20high-quality%20standard%20planes%20is%0Adifficult%2C%20influenced%20by%20the%20sonographer%27s%20expertise%20and%20factors%20like%20the%0Amaternal%20BMI%20or%20the%20fetus%20dynamics.%20In%20this%20work%2C%20we%20propose%20using%0Adiffusion-based%20counterfactual%20explainable%20AI%20to%20generate%20realistic%0Ahigh-quality%20standard%20planes%20from%20low-quality%20non-standard%20ones.%20Through%0Aquantitative%20and%20qualitative%20evaluation%2C%20we%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20in%20producing%20plausible%20counterfactuals%20of%20increased%20quality.%20This%0Ashows%20future%20promise%20both%20for%20enhancing%20training%20of%20clinicians%20by%20providing%0Avisual%20feedback%2C%20as%20well%20as%20for%20improving%20image%20quality%20and%2C%20consequently%2C%0Adownstream%20diagnosis%20and%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08700v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Iterative%20Counterfactual%20Explanations%20for%20Fetal%0A%20%20Ultrasound%20Image%20Quality%20Assessment&entry.906535625=Paraskevas%20Pegios%20and%20Manxi%20Lin%20and%20Nina%20Weng%20and%20Morten%20Bo%20S%C3%B8ndergaard%20Svendsen%20and%20Zahra%20Bashir%20and%20Siavash%20Bigdeli%20and%20Anders%20Nymark%20Christensen%20and%20Martin%20Tolsgaard%20and%20Aasa%20Feragen&entry.1292438233=%20%20Obstetric%20ultrasound%20image%20quality%20is%20crucial%20for%20accurate%20diagnosis%20and%0Amonitoring%20of%20fetal%20health.%20However%2C%20producing%20high-quality%20standard%20planes%20is%0Adifficult%2C%20influenced%20by%20the%20sonographer%27s%20expertise%20and%20factors%20like%20the%0Amaternal%20BMI%20or%20the%20fetus%20dynamics.%20In%20this%20work%2C%20we%20propose%20using%0Adiffusion-based%20counterfactual%20explainable%20AI%20to%20generate%20realistic%0Ahigh-quality%20standard%20planes%20from%20low-quality%20non-standard%20ones.%20Through%0Aquantitative%20and%20qualitative%20evaluation%2C%20we%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20in%20producing%20plausible%20counterfactuals%20of%20increased%20quality.%20This%0Ashows%20future%20promise%20both%20for%20enhancing%20training%20of%20clinicians%20by%20providing%0Avisual%20feedback%2C%20as%20well%20as%20for%20improving%20image%20quality%20and%2C%20consequently%2C%0Adownstream%20diagnosis%20and%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08700v1&entry.124074799=Read"},
{"title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and\n  Diffusion Models", "author": "Yongchan Kwon and Eric Wu and Kevin Wu and James Zou", "abstract": "  Quantifying the impact of training data points is crucial for understanding\nthe outputs of machine learning models and for improving the transparency of\nthe AI pipeline. The influence function is a principled and popular data\nattribution method, but its computational cost often makes it challenging to\nuse. This issue becomes more pronounced in the setting of large language models\nand text-to-image models. In this work, we propose DataInf, an efficient\ninfluence approximation method that is practical for large-scale generative AI\nmodels. Leveraging an easy-to-compute closed-form expression, DataInf\noutperforms existing influence computation algorithms in terms of computational\nand memory efficiency. Our theoretical analysis shows that DataInf is\nparticularly well-suited for parameter-efficient fine-tuning techniques such as\nLoRA. Through systematic empirical evaluations, we show that DataInf accurately\napproximates influence scores and is orders of magnitude faster than existing\nmethods. In applications to RoBERTa-large, Llama-2-13B-chat, and\nstable-diffusion-v1.5 models, DataInf effectively identifies the most\ninfluential fine-tuning examples better than other approximate influence\nscores. Moreover, it can help to identify which data points are mislabeled.\n", "link": "http://arxiv.org/abs/2310.00902v3", "date": "2024-03-13", "relevancy": 1.5409, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5542}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5046}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4957}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DataInf%3A%20Efficiently%20Estimating%20Data%20Influence%20in%20LoRA-tuned%20LLMs%20and%0A%20%20Diffusion%20Models&body=Title%3A%20DataInf%3A%20Efficiently%20Estimating%20Data%20Influence%20in%20LoRA-tuned%20LLMs%20and%0A%20%20Diffusion%20Models%0AAuthor%3A%20Yongchan%20Kwon%20and%20Eric%20Wu%20and%20Kevin%20Wu%20and%20James%20Zou%0AAbstract%3A%20%20%20Quantifying%20the%20impact%20of%20training%20data%20points%20is%20crucial%20for%20understanding%0Athe%20outputs%20of%20machine%20learning%20models%20and%20for%20improving%20the%20transparency%20of%0Athe%20AI%20pipeline.%20The%20influence%20function%20is%20a%20principled%20and%20popular%20data%0Aattribution%20method%2C%20but%20its%20computational%20cost%20often%20makes%20it%20challenging%20to%0Ause.%20This%20issue%20becomes%20more%20pronounced%20in%20the%20setting%20of%20large%20language%20models%0Aand%20text-to-image%20models.%20In%20this%20work%2C%20we%20propose%20DataInf%2C%20an%20efficient%0Ainfluence%20approximation%20method%20that%20is%20practical%20for%20large-scale%20generative%20AI%0Amodels.%20Leveraging%20an%20easy-to-compute%20closed-form%20expression%2C%20DataInf%0Aoutperforms%20existing%20influence%20computation%20algorithms%20in%20terms%20of%20computational%0Aand%20memory%20efficiency.%20Our%20theoretical%20analysis%20shows%20that%20DataInf%20is%0Aparticularly%20well-suited%20for%20parameter-efficient%20fine-tuning%20techniques%20such%20as%0ALoRA.%20Through%20systematic%20empirical%20evaluations%2C%20we%20show%20that%20DataInf%20accurately%0Aapproximates%20influence%20scores%20and%20is%20orders%20of%20magnitude%20faster%20than%20existing%0Amethods.%20In%20applications%20to%20RoBERTa-large%2C%20Llama-2-13B-chat%2C%20and%0Astable-diffusion-v1.5%20models%2C%20DataInf%20effectively%20identifies%20the%20most%0Ainfluential%20fine-tuning%20examples%20better%20than%20other%20approximate%20influence%0Ascores.%20Moreover%2C%20it%20can%20help%20to%20identify%20which%20data%20points%20are%20mislabeled.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00902v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataInf%3A%20Efficiently%20Estimating%20Data%20Influence%20in%20LoRA-tuned%20LLMs%20and%0A%20%20Diffusion%20Models&entry.906535625=Yongchan%20Kwon%20and%20Eric%20Wu%20and%20Kevin%20Wu%20and%20James%20Zou&entry.1292438233=%20%20Quantifying%20the%20impact%20of%20training%20data%20points%20is%20crucial%20for%20understanding%0Athe%20outputs%20of%20machine%20learning%20models%20and%20for%20improving%20the%20transparency%20of%0Athe%20AI%20pipeline.%20The%20influence%20function%20is%20a%20principled%20and%20popular%20data%0Aattribution%20method%2C%20but%20its%20computational%20cost%20often%20makes%20it%20challenging%20to%0Ause.%20This%20issue%20becomes%20more%20pronounced%20in%20the%20setting%20of%20large%20language%20models%0Aand%20text-to-image%20models.%20In%20this%20work%2C%20we%20propose%20DataInf%2C%20an%20efficient%0Ainfluence%20approximation%20method%20that%20is%20practical%20for%20large-scale%20generative%20AI%0Amodels.%20Leveraging%20an%20easy-to-compute%20closed-form%20expression%2C%20DataInf%0Aoutperforms%20existing%20influence%20computation%20algorithms%20in%20terms%20of%20computational%0Aand%20memory%20efficiency.%20Our%20theoretical%20analysis%20shows%20that%20DataInf%20is%0Aparticularly%20well-suited%20for%20parameter-efficient%20fine-tuning%20techniques%20such%20as%0ALoRA.%20Through%20systematic%20empirical%20evaluations%2C%20we%20show%20that%20DataInf%20accurately%0Aapproximates%20influence%20scores%20and%20is%20orders%20of%20magnitude%20faster%20than%20existing%0Amethods.%20In%20applications%20to%20RoBERTa-large%2C%20Llama-2-13B-chat%2C%20and%0Astable-diffusion-v1.5%20models%2C%20DataInf%20effectively%20identifies%20the%20most%0Ainfluential%20fine-tuning%20examples%20better%20than%20other%20approximate%20influence%0Ascores.%20Moreover%2C%20it%20can%20help%20to%20identify%20which%20data%20points%20are%20mislabeled.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00902v3&entry.124074799=Read"},
{"title": "Imitate the Good and Avoid the Bad: An Incremental Approach to Safe\n  Reinforcement Learning", "author": "Huy Hoang and Tien Mai and Pradeep Varakantham", "abstract": "  A popular framework for enforcing safe actions in Reinforcement Learning (RL)\nis Constrained RL, where trajectory based constraints on expected cost (or\nother cost measures) are employed to enforce safety and more importantly these\nconstraints are enforced while maximizing expected reward. Most recent\napproaches for solving Constrained RL convert the trajectory based cost\nconstraint into a surrogate problem that can be solved using minor\nmodifications to RL methods. A key drawback with such approaches is an over or\nunderestimation of the cost constraint at each state. Therefore, we provide an\napproach that does not modify the trajectory based cost constraint and instead\nimitates ``good'' trajectories and avoids ``bad'' trajectories generated from\nincrementally improving policies. We employ an oracle that utilizes a reward\nthreshold (which is varied with learning) and the overall cost constraint to\nlabel trajectories as ``good'' or ``bad''. A key advantage of our approach is\nthat we are able to work from any starting policy or set of trajectories and\nimprove on it. In an exhaustive set of experiments, we demonstrate that our\napproach is able to outperform top benchmark approaches for solving Constrained\nRL problems, with respect to expected cost, CVaR cost, or even unknown cost\nconstraints.\n", "link": "http://arxiv.org/abs/2312.10385v3", "date": "2024-03-13", "relevancy": 1.5301, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4816}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.466}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Imitate%20the%20Good%20and%20Avoid%20the%20Bad%3A%20An%20Incremental%20Approach%20to%20Safe%0A%20%20Reinforcement%20Learning&body=Title%3A%20Imitate%20the%20Good%20and%20Avoid%20the%20Bad%3A%20An%20Incremental%20Approach%20to%20Safe%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Huy%20Hoang%20and%20Tien%20Mai%20and%20Pradeep%20Varakantham%0AAbstract%3A%20%20%20A%20popular%20framework%20for%20enforcing%20safe%20actions%20in%20Reinforcement%20Learning%20%28RL%29%0Ais%20Constrained%20RL%2C%20where%20trajectory%20based%20constraints%20on%20expected%20cost%20%28or%0Aother%20cost%20measures%29%20are%20employed%20to%20enforce%20safety%20and%20more%20importantly%20these%0Aconstraints%20are%20enforced%20while%20maximizing%20expected%20reward.%20Most%20recent%0Aapproaches%20for%20solving%20Constrained%20RL%20convert%20the%20trajectory%20based%20cost%0Aconstraint%20into%20a%20surrogate%20problem%20that%20can%20be%20solved%20using%20minor%0Amodifications%20to%20RL%20methods.%20A%20key%20drawback%20with%20such%20approaches%20is%20an%20over%20or%0Aunderestimation%20of%20the%20cost%20constraint%20at%20each%20state.%20Therefore%2C%20we%20provide%20an%0Aapproach%20that%20does%20not%20modify%20the%20trajectory%20based%20cost%20constraint%20and%20instead%0Aimitates%20%60%60good%27%27%20trajectories%20and%20avoids%20%60%60bad%27%27%20trajectories%20generated%20from%0Aincrementally%20improving%20policies.%20We%20employ%20an%20oracle%20that%20utilizes%20a%20reward%0Athreshold%20%28which%20is%20varied%20with%20learning%29%20and%20the%20overall%20cost%20constraint%20to%0Alabel%20trajectories%20as%20%60%60good%27%27%20or%20%60%60bad%27%27.%20A%20key%20advantage%20of%20our%20approach%20is%0Athat%20we%20are%20able%20to%20work%20from%20any%20starting%20policy%20or%20set%20of%20trajectories%20and%0Aimprove%20on%20it.%20In%20an%20exhaustive%20set%20of%20experiments%2C%20we%20demonstrate%20that%20our%0Aapproach%20is%20able%20to%20outperform%20top%20benchmark%20approaches%20for%20solving%20Constrained%0ARL%20problems%2C%20with%20respect%20to%20expected%20cost%2C%20CVaR%20cost%2C%20or%20even%20unknown%20cost%0Aconstraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10385v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitate%20the%20Good%20and%20Avoid%20the%20Bad%3A%20An%20Incremental%20Approach%20to%20Safe%0A%20%20Reinforcement%20Learning&entry.906535625=Huy%20Hoang%20and%20Tien%20Mai%20and%20Pradeep%20Varakantham&entry.1292438233=%20%20A%20popular%20framework%20for%20enforcing%20safe%20actions%20in%20Reinforcement%20Learning%20%28RL%29%0Ais%20Constrained%20RL%2C%20where%20trajectory%20based%20constraints%20on%20expected%20cost%20%28or%0Aother%20cost%20measures%29%20are%20employed%20to%20enforce%20safety%20and%20more%20importantly%20these%0Aconstraints%20are%20enforced%20while%20maximizing%20expected%20reward.%20Most%20recent%0Aapproaches%20for%20solving%20Constrained%20RL%20convert%20the%20trajectory%20based%20cost%0Aconstraint%20into%20a%20surrogate%20problem%20that%20can%20be%20solved%20using%20minor%0Amodifications%20to%20RL%20methods.%20A%20key%20drawback%20with%20such%20approaches%20is%20an%20over%20or%0Aunderestimation%20of%20the%20cost%20constraint%20at%20each%20state.%20Therefore%2C%20we%20provide%20an%0Aapproach%20that%20does%20not%20modify%20the%20trajectory%20based%20cost%20constraint%20and%20instead%0Aimitates%20%60%60good%27%27%20trajectories%20and%20avoids%20%60%60bad%27%27%20trajectories%20generated%20from%0Aincrementally%20improving%20policies.%20We%20employ%20an%20oracle%20that%20utilizes%20a%20reward%0Athreshold%20%28which%20is%20varied%20with%20learning%29%20and%20the%20overall%20cost%20constraint%20to%0Alabel%20trajectories%20as%20%60%60good%27%27%20or%20%60%60bad%27%27.%20A%20key%20advantage%20of%20our%20approach%20is%0Athat%20we%20are%20able%20to%20work%20from%20any%20starting%20policy%20or%20set%20of%20trajectories%20and%0Aimprove%20on%20it.%20In%20an%20exhaustive%20set%20of%20experiments%2C%20we%20demonstrate%20that%20our%0Aapproach%20is%20able%20to%20outperform%20top%20benchmark%20approaches%20for%20solving%20Constrained%0ARL%20problems%2C%20with%20respect%20to%20expected%20cost%2C%20CVaR%20cost%2C%20or%20even%20unknown%20cost%0Aconstraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10385v3&entry.124074799=Read"},
{"title": "HAIFIT: Human-Centered AI for Fashion Image Translation", "author": "Jianan Jiang and Xinglin Li and Weiren Yu and Di Wu", "abstract": "  In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications.\n", "link": "http://arxiv.org/abs/2403.08651v1", "date": "2024-03-13", "relevancy": 1.529, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5225}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5087}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4991}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20HAIFIT%3A%20Human-Centered%20AI%20for%20Fashion%20Image%20Translation&body=Title%3A%20HAIFIT%3A%20Human-Centered%20AI%20for%20Fashion%20Image%20Translation%0AAuthor%3A%20Jianan%20Jiang%20and%20Xinglin%20Li%20and%20Weiren%20Yu%20and%20Di%20Wu%0AAbstract%3A%20%20%20In%20the%20realm%20of%20fashion%20design%2C%20sketches%20serve%20as%20the%20canvas%20for%20expressing%0Aan%20artist%27s%20distinctive%20drawing%20style%20and%20creative%20vision%2C%20capturing%20intricate%0Adetails%20like%20stroke%20variations%20and%20texture%20nuances.%20The%20advent%20of%0Asketch-to-image%20cross-modal%20translation%20technology%20has%20notably%20aided%20designers.%0AHowever%2C%20existing%20methods%20often%20compromise%20these%20sketch%20details%20during%20image%0Ageneration%2C%20resulting%20in%20images%20that%20deviate%20from%20the%20designer%27s%20intended%0Aconcept.%20This%20limitation%20hampers%20the%20ability%20to%20offer%20designers%20a%20precise%0Apreview%20of%20the%20final%20output.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20HAIFIT%2C%20a%0Anovel%20approach%20that%20transforms%20sketches%20into%20high-fidelity%2C%20lifelike%20clothing%0Aimages%20by%20integrating%20multi-scale%20features%20and%20capturing%20extensive%20feature%20map%0Adependencies%20from%20diverse%20perspectives.%20Through%20extensive%20qualitative%20and%0Aquantitative%20evaluations%20conducted%20on%20our%20self-collected%20dataset%2C%20our%20method%0Ademonstrates%20superior%20performance%20compared%20to%20existing%20methods%20in%20generating%0Aphotorealistic%20clothing%20images.%20Our%20method%20excels%20in%20preserving%20the%20distinctive%0Astyle%20and%20intricate%20details%20essential%20for%20fashion%20design%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08651v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAIFIT%3A%20Human-Centered%20AI%20for%20Fashion%20Image%20Translation&entry.906535625=Jianan%20Jiang%20and%20Xinglin%20Li%20and%20Weiren%20Yu%20and%20Di%20Wu&entry.1292438233=%20%20In%20the%20realm%20of%20fashion%20design%2C%20sketches%20serve%20as%20the%20canvas%20for%20expressing%0Aan%20artist%27s%20distinctive%20drawing%20style%20and%20creative%20vision%2C%20capturing%20intricate%0Adetails%20like%20stroke%20variations%20and%20texture%20nuances.%20The%20advent%20of%0Asketch-to-image%20cross-modal%20translation%20technology%20has%20notably%20aided%20designers.%0AHowever%2C%20existing%20methods%20often%20compromise%20these%20sketch%20details%20during%20image%0Ageneration%2C%20resulting%20in%20images%20that%20deviate%20from%20the%20designer%27s%20intended%0Aconcept.%20This%20limitation%20hampers%20the%20ability%20to%20offer%20designers%20a%20precise%0Apreview%20of%20the%20final%20output.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20HAIFIT%2C%20a%0Anovel%20approach%20that%20transforms%20sketches%20into%20high-fidelity%2C%20lifelike%20clothing%0Aimages%20by%20integrating%20multi-scale%20features%20and%20capturing%20extensive%20feature%20map%0Adependencies%20from%20diverse%20perspectives.%20Through%20extensive%20qualitative%20and%0Aquantitative%20evaluations%20conducted%20on%20our%20self-collected%20dataset%2C%20our%20method%0Ademonstrates%20superior%20performance%20compared%20to%20existing%20methods%20in%20generating%0Aphotorealistic%20clothing%20images.%20Our%20method%20excels%20in%20preserving%20the%20distinctive%0Astyle%20and%20intricate%20details%20essential%20for%20fashion%20design%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08651v1&entry.124074799=Read"},
{"title": "Extracting Explanations, Justification, and Uncertainty from Black-Box\n  Deep Neural Networks", "author": "Paul Ardis and Arjuna Flenner", "abstract": "  Deep Neural Networks (DNNs) do not inherently compute or exhibit\nempirically-justified task confidence. In mission critical applications, it is\nimportant to both understand associated DNN reasoning and its supporting\nevidence. In this paper, we propose a novel Bayesian approach to extract\nexplanations, justifications, and uncertainty estimates from DNNs. Our approach\nis efficient both in terms of memory and computation, and can be applied to any\nblack box DNN without any retraining, including applications to anomaly\ndetection and out-of-distribution detection tasks. We validate our approach on\nthe CIFAR-10 dataset, and show that it can significantly improve the\ninterpretability and reliability of DNNs.\n", "link": "http://arxiv.org/abs/2403.08652v1", "date": "2024-03-13", "relevancy": 1.5236, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5195}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5163}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Extracting%20Explanations%2C%20Justification%2C%20and%20Uncertainty%20from%20Black-Box%0A%20%20Deep%20Neural%20Networks&body=Title%3A%20Extracting%20Explanations%2C%20Justification%2C%20and%20Uncertainty%20from%20Black-Box%0A%20%20Deep%20Neural%20Networks%0AAuthor%3A%20Paul%20Ardis%20and%20Arjuna%20Flenner%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20do%20not%20inherently%20compute%20or%20exhibit%0Aempirically-justified%20task%20confidence.%20In%20mission%20critical%20applications%2C%20it%20is%0Aimportant%20to%20both%20understand%20associated%20DNN%20reasoning%20and%20its%20supporting%0Aevidence.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Bayesian%20approach%20to%20extract%0Aexplanations%2C%20justifications%2C%20and%20uncertainty%20estimates%20from%20DNNs.%20Our%20approach%0Ais%20efficient%20both%20in%20terms%20of%20memory%20and%20computation%2C%20and%20can%20be%20applied%20to%20any%0Ablack%20box%20DNN%20without%20any%20retraining%2C%20including%20applications%20to%20anomaly%0Adetection%20and%20out-of-distribution%20detection%20tasks.%20We%20validate%20our%20approach%20on%0Athe%20CIFAR-10%20dataset%2C%20and%20show%20that%20it%20can%20significantly%20improve%20the%0Ainterpretability%20and%20reliability%20of%20DNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08652v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20Explanations%2C%20Justification%2C%20and%20Uncertainty%20from%20Black-Box%0A%20%20Deep%20Neural%20Networks&entry.906535625=Paul%20Ardis%20and%20Arjuna%20Flenner&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20do%20not%20inherently%20compute%20or%20exhibit%0Aempirically-justified%20task%20confidence.%20In%20mission%20critical%20applications%2C%20it%20is%0Aimportant%20to%20both%20understand%20associated%20DNN%20reasoning%20and%20its%20supporting%0Aevidence.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Bayesian%20approach%20to%20extract%0Aexplanations%2C%20justifications%2C%20and%20uncertainty%20estimates%20from%20DNNs.%20Our%20approach%0Ais%20efficient%20both%20in%20terms%20of%20memory%20and%20computation%2C%20and%20can%20be%20applied%20to%20any%0Ablack%20box%20DNN%20without%20any%20retraining%2C%20including%20applications%20to%20anomaly%0Adetection%20and%20out-of-distribution%20detection%20tasks.%20We%20validate%20our%20approach%20on%0Athe%20CIFAR-10%20dataset%2C%20and%20show%20that%20it%20can%20significantly%20improve%20the%0Ainterpretability%20and%20reliability%20of%20DNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08652v1&entry.124074799=Read"},
{"title": "Kernel-Based Testing for Single-Cell Differential Analysis", "author": "Anthony Ozier-Lafontaine and Camille Fourneaux and Ghislain Durif and C\u00e9line Vallot and Olivier Gandrillon and Sandrine Giraud and Bertrand Michel and Franck Picard", "abstract": "  Single-cell technologies offer insights into molecular feature distributions,\nbut comparing them poses challenges. We propose a kernel-testing framework for\nnon-linear cell-wise distribution comparison, analyzing gene expression and\nepigenomic modifications. Our method allows feature-wise and global\ntranscriptome/epigenome comparisons, revealing cell population heterogeneities.\nUsing a classifier based on embedding variability, we identify transitions in\ncell states, overcoming limitations of traditional single-cell analysis.\nApplied to single-cell ChIP-Seq data, our approach identifies untreated breast\ncancer cells with an epigenomic profile resembling persister cells. This\ndemonstrates the effectiveness of kernel testing in uncovering subtle\npopulation variations that might be missed by other methods.\n", "link": "http://arxiv.org/abs/2307.08509v2", "date": "2024-03-13", "relevancy": 1.5206, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3824}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3821}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3696}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Kernel-Based%20Testing%20for%20Single-Cell%20Differential%20Analysis&body=Title%3A%20Kernel-Based%20Testing%20for%20Single-Cell%20Differential%20Analysis%0AAuthor%3A%20Anthony%20Ozier-Lafontaine%20and%20Camille%20Fourneaux%20and%20Ghislain%20Durif%20and%20C%C3%A9line%20Vallot%20and%20Olivier%20Gandrillon%20and%20Sandrine%20Giraud%20and%20Bertrand%20Michel%20and%20Franck%20Picard%0AAbstract%3A%20%20%20Single-cell%20technologies%20offer%20insights%20into%20molecular%20feature%20distributions%2C%0Abut%20comparing%20them%20poses%20challenges.%20We%20propose%20a%20kernel-testing%20framework%20for%0Anon-linear%20cell-wise%20distribution%20comparison%2C%20analyzing%20gene%20expression%20and%0Aepigenomic%20modifications.%20Our%20method%20allows%20feature-wise%20and%20global%0Atranscriptome/epigenome%20comparisons%2C%20revealing%20cell%20population%20heterogeneities.%0AUsing%20a%20classifier%20based%20on%20embedding%20variability%2C%20we%20identify%20transitions%20in%0Acell%20states%2C%20overcoming%20limitations%20of%20traditional%20single-cell%20analysis.%0AApplied%20to%20single-cell%20ChIP-Seq%20data%2C%20our%20approach%20identifies%20untreated%20breast%0Acancer%20cells%20with%20an%20epigenomic%20profile%20resembling%20persister%20cells.%20This%0Ademonstrates%20the%20effectiveness%20of%20kernel%20testing%20in%20uncovering%20subtle%0Apopulation%20variations%20that%20might%20be%20missed%20by%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08509v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel-Based%20Testing%20for%20Single-Cell%20Differential%20Analysis&entry.906535625=Anthony%20Ozier-Lafontaine%20and%20Camille%20Fourneaux%20and%20Ghislain%20Durif%20and%20C%C3%A9line%20Vallot%20and%20Olivier%20Gandrillon%20and%20Sandrine%20Giraud%20and%20Bertrand%20Michel%20and%20Franck%20Picard&entry.1292438233=%20%20Single-cell%20technologies%20offer%20insights%20into%20molecular%20feature%20distributions%2C%0Abut%20comparing%20them%20poses%20challenges.%20We%20propose%20a%20kernel-testing%20framework%20for%0Anon-linear%20cell-wise%20distribution%20comparison%2C%20analyzing%20gene%20expression%20and%0Aepigenomic%20modifications.%20Our%20method%20allows%20feature-wise%20and%20global%0Atranscriptome/epigenome%20comparisons%2C%20revealing%20cell%20population%20heterogeneities.%0AUsing%20a%20classifier%20based%20on%20embedding%20variability%2C%20we%20identify%20transitions%20in%0Acell%20states%2C%20overcoming%20limitations%20of%20traditional%20single-cell%20analysis.%0AApplied%20to%20single-cell%20ChIP-Seq%20data%2C%20our%20approach%20identifies%20untreated%20breast%0Acancer%20cells%20with%20an%20epigenomic%20profile%20resembling%20persister%20cells.%20This%0Ademonstrates%20the%20effectiveness%20of%20kernel%20testing%20in%20uncovering%20subtle%0Apopulation%20variations%20that%20might%20be%20missed%20by%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08509v2&entry.124074799=Read"},
{"title": "Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over\n  Structured Environments", "author": "Sitao Cheng and Ziyuan Zhuang and Yong Xu and Fangkai Yang and Chaoyun Zhang and Xiaoting Qin and Xiang Huang and Ling Chen and Qingwei Lin and Dongmei Zhang and Saravan Rajmohan and Qi Zhang", "abstract": "  Large Language Models (LLMs) have shown potential in reasoning over\nstructured environments, e.g., knowledge graph and table. Such tasks typically\nrequire multi-hop reasoning, i.e., match natural language utterance with\ninstances in the environment. Previous methods leverage LLMs to incrementally\nbuild a reasoning path, where the LLMs either invoke tools or pick up schemas\nby step-by-step interacting with the environment. We propose\nReasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently\nand faithfully reason over structured environments. In Readi, LLMs initially\ngenerate a reasoning path given a query, and edit the path only when necessary.\nWe instantiate the path on structured environments and provide feedback to edit\nthe path if anything goes wrong. Experimental results on three KGQA datasets\nand two TableQA datasets show the effectiveness of Readi, significantly\nsurpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9%\non WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and\n74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).\nOur code will be available upon publication.\n", "link": "http://arxiv.org/abs/2403.08593v1", "date": "2024-03-13", "relevancy": 1.5004, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5199}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4912}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Call%20Me%20When%20Necessary%3A%20LLMs%20can%20Efficiently%20and%20Faithfully%20Reason%20over%0A%20%20Structured%20Environments&body=Title%3A%20Call%20Me%20When%20Necessary%3A%20LLMs%20can%20Efficiently%20and%20Faithfully%20Reason%20over%0A%20%20Structured%20Environments%0AAuthor%3A%20Sitao%20Cheng%20and%20Ziyuan%20Zhuang%20and%20Yong%20Xu%20and%20Fangkai%20Yang%20and%20Chaoyun%20Zhang%20and%20Xiaoting%20Qin%20and%20Xiang%20Huang%20and%20Ling%20Chen%20and%20Qingwei%20Lin%20and%20Dongmei%20Zhang%20and%20Saravan%20Rajmohan%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20potential%20in%20reasoning%20over%0Astructured%20environments%2C%20e.g.%2C%20knowledge%20graph%20and%20table.%20Such%20tasks%20typically%0Arequire%20multi-hop%20reasoning%2C%20i.e.%2C%20match%20natural%20language%20utterance%20with%0Ainstances%20in%20the%20environment.%20Previous%20methods%20leverage%20LLMs%20to%20incrementally%0Abuild%20a%20reasoning%20path%2C%20where%20the%20LLMs%20either%20invoke%20tools%20or%20pick%20up%20schemas%0Aby%20step-by-step%20interacting%20with%20the%20environment.%20We%20propose%0AReasoning-Path-Editing%20%28Readi%29%2C%20a%20novel%20framework%20where%20LLMs%20can%20efficiently%0Aand%20faithfully%20reason%20over%20structured%20environments.%20In%20Readi%2C%20LLMs%20initially%0Agenerate%20a%20reasoning%20path%20given%20a%20query%2C%20and%20edit%20the%20path%20only%20when%20necessary.%0AWe%20instantiate%20the%20path%20on%20structured%20environments%20and%20provide%20feedback%20to%20edit%0Athe%20path%20if%20anything%20goes%20wrong.%20Experimental%20results%20on%20three%20KGQA%20datasets%0Aand%20two%20TableQA%20datasets%20show%20the%20effectiveness%20of%20Readi%2C%20significantly%0Asurpassing%20all%20LLM-based%20methods%20%28by%209.1%25%20on%20WebQSP%2C%2012.4%25%20on%20MQA-3H%20and%2010.9%25%0Aon%20WTQ%29%2C%20comparable%20with%20state-of-the-art%20fine-tuned%20methods%20%2867%25%20on%20CWQ%20and%0A74.7%25%20on%20WebQSP%29%20and%20substantially%20boosting%20the%20vanilla%20LLMs%20%28by%2014.9%25%20on%20CWQ%29.%0AOur%20code%20will%20be%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08593v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Call%20Me%20When%20Necessary%3A%20LLMs%20can%20Efficiently%20and%20Faithfully%20Reason%20over%0A%20%20Structured%20Environments&entry.906535625=Sitao%20Cheng%20and%20Ziyuan%20Zhuang%20and%20Yong%20Xu%20and%20Fangkai%20Yang%20and%20Chaoyun%20Zhang%20and%20Xiaoting%20Qin%20and%20Xiang%20Huang%20and%20Ling%20Chen%20and%20Qingwei%20Lin%20and%20Dongmei%20Zhang%20and%20Saravan%20Rajmohan%20and%20Qi%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20potential%20in%20reasoning%20over%0Astructured%20environments%2C%20e.g.%2C%20knowledge%20graph%20and%20table.%20Such%20tasks%20typically%0Arequire%20multi-hop%20reasoning%2C%20i.e.%2C%20match%20natural%20language%20utterance%20with%0Ainstances%20in%20the%20environment.%20Previous%20methods%20leverage%20LLMs%20to%20incrementally%0Abuild%20a%20reasoning%20path%2C%20where%20the%20LLMs%20either%20invoke%20tools%20or%20pick%20up%20schemas%0Aby%20step-by-step%20interacting%20with%20the%20environment.%20We%20propose%0AReasoning-Path-Editing%20%28Readi%29%2C%20a%20novel%20framework%20where%20LLMs%20can%20efficiently%0Aand%20faithfully%20reason%20over%20structured%20environments.%20In%20Readi%2C%20LLMs%20initially%0Agenerate%20a%20reasoning%20path%20given%20a%20query%2C%20and%20edit%20the%20path%20only%20when%20necessary.%0AWe%20instantiate%20the%20path%20on%20structured%20environments%20and%20provide%20feedback%20to%20edit%0Athe%20path%20if%20anything%20goes%20wrong.%20Experimental%20results%20on%20three%20KGQA%20datasets%0Aand%20two%20TableQA%20datasets%20show%20the%20effectiveness%20of%20Readi%2C%20significantly%0Asurpassing%20all%20LLM-based%20methods%20%28by%209.1%25%20on%20WebQSP%2C%2012.4%25%20on%20MQA-3H%20and%2010.9%25%0Aon%20WTQ%29%2C%20comparable%20with%20state-of-the-art%20fine-tuned%20methods%20%2867%25%20on%20CWQ%20and%0A74.7%25%20on%20WebQSP%29%20and%20substantially%20boosting%20the%20vanilla%20LLMs%20%28by%2014.9%25%20on%20CWQ%29.%0AOur%20code%20will%20be%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08593v1&entry.124074799=Read"},
{"title": "Class Incremental Learning via Likelihood Ratio Based Task Prediction", "author": "Haowei Lin and Yijia Shao and Weinan Qian and Ningxin Pan and Yiduo Guo and Bing Liu", "abstract": "  Class incremental learning (CIL) is a challenging setting of continual\nlearning, which learns a series of tasks sequentially. Each task consists of a\nset of unique classes. The key feature of CIL is that no task identifier (or\ntask-id) is provided at test time. Predicting the task-id for each test sample\nis a challenging problem. An emerging theory-guided approach (called TIL+OOD)\nis to train a task-specific model for each task in a shared network for all\ntasks based on a task-incremental learning (TIL) method to deal with\ncatastrophic forgetting. The model for each task is an out-of-distribution\n(OOD) detector rather than a conventional classifier. The OOD detector can\nperform both within-task (in-distribution (IND)) class prediction and OOD\ndetection. The OOD detection capability is the key to task-id prediction during\ninference. However, this paper argues that using a traditional OOD detector for\ntask-id prediction is sub-optimal because additional information (e.g., the\nreplay data and the learned tasks) available in CIL can be exploited to design\na better and principled method for task-id prediction. We call the new method\nTPL (Task-id Prediction based on Likelihood Ratio). TPL markedly outperforms\nstrong CIL baselines and has negligible catastrophic forgetting. The code of\nTPL is publicly available at https://github.com/linhaowei1/TPL.\n", "link": "http://arxiv.org/abs/2309.15048v4", "date": "2024-03-13", "relevancy": 1.4867, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5112}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4781}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4739}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Class%20Incremental%20Learning%20via%20Likelihood%20Ratio%20Based%20Task%20Prediction&body=Title%3A%20Class%20Incremental%20Learning%20via%20Likelihood%20Ratio%20Based%20Task%20Prediction%0AAuthor%3A%20Haowei%20Lin%20and%20Yijia%20Shao%20and%20Weinan%20Qian%20and%20Ningxin%20Pan%20and%20Yiduo%20Guo%20and%20Bing%20Liu%0AAbstract%3A%20%20%20Class%20incremental%20learning%20%28CIL%29%20is%20a%20challenging%20setting%20of%20continual%0Alearning%2C%20which%20learns%20a%20series%20of%20tasks%20sequentially.%20Each%20task%20consists%20of%20a%0Aset%20of%20unique%20classes.%20The%20key%20feature%20of%20CIL%20is%20that%20no%20task%20identifier%20%28or%0Atask-id%29%20is%20provided%20at%20test%20time.%20Predicting%20the%20task-id%20for%20each%20test%20sample%0Ais%20a%20challenging%20problem.%20An%20emerging%20theory-guided%20approach%20%28called%20TIL%2BOOD%29%0Ais%20to%20train%20a%20task-specific%20model%20for%20each%20task%20in%20a%20shared%20network%20for%20all%0Atasks%20based%20on%20a%20task-incremental%20learning%20%28TIL%29%20method%20to%20deal%20with%0Acatastrophic%20forgetting.%20The%20model%20for%20each%20task%20is%20an%20out-of-distribution%0A%28OOD%29%20detector%20rather%20than%20a%20conventional%20classifier.%20The%20OOD%20detector%20can%0Aperform%20both%20within-task%20%28in-distribution%20%28IND%29%29%20class%20prediction%20and%20OOD%0Adetection.%20The%20OOD%20detection%20capability%20is%20the%20key%20to%20task-id%20prediction%20during%0Ainference.%20However%2C%20this%20paper%20argues%20that%20using%20a%20traditional%20OOD%20detector%20for%0Atask-id%20prediction%20is%20sub-optimal%20because%20additional%20information%20%28e.g.%2C%20the%0Areplay%20data%20and%20the%20learned%20tasks%29%20available%20in%20CIL%20can%20be%20exploited%20to%20design%0Aa%20better%20and%20principled%20method%20for%20task-id%20prediction.%20We%20call%20the%20new%20method%0ATPL%20%28Task-id%20Prediction%20based%20on%20Likelihood%20Ratio%29.%20TPL%20markedly%20outperforms%0Astrong%20CIL%20baselines%20and%20has%20negligible%20catastrophic%20forgetting.%20The%20code%20of%0ATPL%20is%20publicly%20available%20at%20https%3A//github.com/linhaowei1/TPL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.15048v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class%20Incremental%20Learning%20via%20Likelihood%20Ratio%20Based%20Task%20Prediction&entry.906535625=Haowei%20Lin%20and%20Yijia%20Shao%20and%20Weinan%20Qian%20and%20Ningxin%20Pan%20and%20Yiduo%20Guo%20and%20Bing%20Liu&entry.1292438233=%20%20Class%20incremental%20learning%20%28CIL%29%20is%20a%20challenging%20setting%20of%20continual%0Alearning%2C%20which%20learns%20a%20series%20of%20tasks%20sequentially.%20Each%20task%20consists%20of%20a%0Aset%20of%20unique%20classes.%20The%20key%20feature%20of%20CIL%20is%20that%20no%20task%20identifier%20%28or%0Atask-id%29%20is%20provided%20at%20test%20time.%20Predicting%20the%20task-id%20for%20each%20test%20sample%0Ais%20a%20challenging%20problem.%20An%20emerging%20theory-guided%20approach%20%28called%20TIL%2BOOD%29%0Ais%20to%20train%20a%20task-specific%20model%20for%20each%20task%20in%20a%20shared%20network%20for%20all%0Atasks%20based%20on%20a%20task-incremental%20learning%20%28TIL%29%20method%20to%20deal%20with%0Acatastrophic%20forgetting.%20The%20model%20for%20each%20task%20is%20an%20out-of-distribution%0A%28OOD%29%20detector%20rather%20than%20a%20conventional%20classifier.%20The%20OOD%20detector%20can%0Aperform%20both%20within-task%20%28in-distribution%20%28IND%29%29%20class%20prediction%20and%20OOD%0Adetection.%20The%20OOD%20detection%20capability%20is%20the%20key%20to%20task-id%20prediction%20during%0Ainference.%20However%2C%20this%20paper%20argues%20that%20using%20a%20traditional%20OOD%20detector%20for%0Atask-id%20prediction%20is%20sub-optimal%20because%20additional%20information%20%28e.g.%2C%20the%0Areplay%20data%20and%20the%20learned%20tasks%29%20available%20in%20CIL%20can%20be%20exploited%20to%20design%0Aa%20better%20and%20principled%20method%20for%20task-id%20prediction.%20We%20call%20the%20new%20method%0ATPL%20%28Task-id%20Prediction%20based%20on%20Likelihood%20Ratio%29.%20TPL%20markedly%20outperforms%0Astrong%20CIL%20baselines%20and%20has%20negligible%20catastrophic%20forgetting.%20The%20code%20of%0ATPL%20is%20publicly%20available%20at%20https%3A//github.com/linhaowei1/TPL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15048v4&entry.124074799=Read"},
{"title": "Multifidelity linear regression for scientific machine learning from\n  scarce data", "author": "Elizabeth Qian and Anirban Chaudhuri and Dayoung Kang and Vignesh Sella", "abstract": "  Machine learning (ML) methods, which fit to data the parameters of a given\nparameterized model class, have garnered significant interest as potential\nmethods for learning surrogate models for complex engineering systems for which\ntraditional simulation is expensive. However, in many scientific and\nengineering settings, generating high-fidelity data on which to train ML models\nis expensive, and the available budget for generating training data is limited.\nML models trained on the resulting scarce high-fidelity data have high variance\nand are sensitive to vagaries of the training data set. We propose a new\nmultifidelity training approach for scientific machine learning that exploits\nthe scientific context where data of varying fidelities and costs are\navailable; for example high-fidelity data may be generated by an expensive\nfully resolved physics simulation whereas lower-fidelity data may arise from a\ncheaper model based on simplifying assumptions. We use the multifidelity data\nto define new multifidelity Monte Carlo estimators for the unknown parameters\nof linear regression models, and provide theoretical analyses that guarantee\nthe approach's accuracy and improved robustness to small training budgets.\nNumerical results verify the theoretical analysis and demonstrate that\nmultifidelity learned models trained on scarce high-fidelity data and\nadditional low-fidelity data achieve order-of-magnitude lower model variance\nthan standard models trained on only high-fidelity data of comparable cost.\nThis illustrates that in the scarce data regime, our multifidelity training\nstrategy yields models with lower expected error than standard training\napproaches.\n", "link": "http://arxiv.org/abs/2403.08627v1", "date": "2024-03-13", "relevancy": 1.4793, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4907}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4758}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Multifidelity%20linear%20regression%20for%20scientific%20machine%20learning%20from%0A%20%20scarce%20data&body=Title%3A%20Multifidelity%20linear%20regression%20for%20scientific%20machine%20learning%20from%0A%20%20scarce%20data%0AAuthor%3A%20Elizabeth%20Qian%20and%20Anirban%20Chaudhuri%20and%20Dayoung%20Kang%20and%20Vignesh%20Sella%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20methods%2C%20which%20fit%20to%20data%20the%20parameters%20of%20a%20given%0Aparameterized%20model%20class%2C%20have%20garnered%20significant%20interest%20as%20potential%0Amethods%20for%20learning%20surrogate%20models%20for%20complex%20engineering%20systems%20for%20which%0Atraditional%20simulation%20is%20expensive.%20However%2C%20in%20many%20scientific%20and%0Aengineering%20settings%2C%20generating%20high-fidelity%20data%20on%20which%20to%20train%20ML%20models%0Ais%20expensive%2C%20and%20the%20available%20budget%20for%20generating%20training%20data%20is%20limited.%0AML%20models%20trained%20on%20the%20resulting%20scarce%20high-fidelity%20data%20have%20high%20variance%0Aand%20are%20sensitive%20to%20vagaries%20of%20the%20training%20data%20set.%20We%20propose%20a%20new%0Amultifidelity%20training%20approach%20for%20scientific%20machine%20learning%20that%20exploits%0Athe%20scientific%20context%20where%20data%20of%20varying%20fidelities%20and%20costs%20are%0Aavailable%3B%20for%20example%20high-fidelity%20data%20may%20be%20generated%20by%20an%20expensive%0Afully%20resolved%20physics%20simulation%20whereas%20lower-fidelity%20data%20may%20arise%20from%20a%0Acheaper%20model%20based%20on%20simplifying%20assumptions.%20We%20use%20the%20multifidelity%20data%0Ato%20define%20new%20multifidelity%20Monte%20Carlo%20estimators%20for%20the%20unknown%20parameters%0Aof%20linear%20regression%20models%2C%20and%20provide%20theoretical%20analyses%20that%20guarantee%0Athe%20approach%27s%20accuracy%20and%20improved%20robustness%20to%20small%20training%20budgets.%0ANumerical%20results%20verify%20the%20theoretical%20analysis%20and%20demonstrate%20that%0Amultifidelity%20learned%20models%20trained%20on%20scarce%20high-fidelity%20data%20and%0Aadditional%20low-fidelity%20data%20achieve%20order-of-magnitude%20lower%20model%20variance%0Athan%20standard%20models%20trained%20on%20only%20high-fidelity%20data%20of%20comparable%20cost.%0AThis%20illustrates%20that%20in%20the%20scarce%20data%20regime%2C%20our%20multifidelity%20training%0Astrategy%20yields%20models%20with%20lower%20expected%20error%20than%20standard%20training%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08627v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multifidelity%20linear%20regression%20for%20scientific%20machine%20learning%20from%0A%20%20scarce%20data&entry.906535625=Elizabeth%20Qian%20and%20Anirban%20Chaudhuri%20and%20Dayoung%20Kang%20and%20Vignesh%20Sella&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20methods%2C%20which%20fit%20to%20data%20the%20parameters%20of%20a%20given%0Aparameterized%20model%20class%2C%20have%20garnered%20significant%20interest%20as%20potential%0Amethods%20for%20learning%20surrogate%20models%20for%20complex%20engineering%20systems%20for%20which%0Atraditional%20simulation%20is%20expensive.%20However%2C%20in%20many%20scientific%20and%0Aengineering%20settings%2C%20generating%20high-fidelity%20data%20on%20which%20to%20train%20ML%20models%0Ais%20expensive%2C%20and%20the%20available%20budget%20for%20generating%20training%20data%20is%20limited.%0AML%20models%20trained%20on%20the%20resulting%20scarce%20high-fidelity%20data%20have%20high%20variance%0Aand%20are%20sensitive%20to%20vagaries%20of%20the%20training%20data%20set.%20We%20propose%20a%20new%0Amultifidelity%20training%20approach%20for%20scientific%20machine%20learning%20that%20exploits%0Athe%20scientific%20context%20where%20data%20of%20varying%20fidelities%20and%20costs%20are%0Aavailable%3B%20for%20example%20high-fidelity%20data%20may%20be%20generated%20by%20an%20expensive%0Afully%20resolved%20physics%20simulation%20whereas%20lower-fidelity%20data%20may%20arise%20from%20a%0Acheaper%20model%20based%20on%20simplifying%20assumptions.%20We%20use%20the%20multifidelity%20data%0Ato%20define%20new%20multifidelity%20Monte%20Carlo%20estimators%20for%20the%20unknown%20parameters%0Aof%20linear%20regression%20models%2C%20and%20provide%20theoretical%20analyses%20that%20guarantee%0Athe%20approach%27s%20accuracy%20and%20improved%20robustness%20to%20small%20training%20budgets.%0ANumerical%20results%20verify%20the%20theoretical%20analysis%20and%20demonstrate%20that%0Amultifidelity%20learned%20models%20trained%20on%20scarce%20high-fidelity%20data%20and%0Aadditional%20low-fidelity%20data%20achieve%20order-of-magnitude%20lower%20model%20variance%0Athan%20standard%20models%20trained%20on%20only%20high-fidelity%20data%20of%20comparable%20cost.%0AThis%20illustrates%20that%20in%20the%20scarce%20data%20regime%2C%20our%20multifidelity%20training%0Astrategy%20yields%20models%20with%20lower%20expected%20error%20than%20standard%20training%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08627v1&entry.124074799=Read"},
{"title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution\n  in Large Language Models", "author": "Carlo Nicolini and Jacopo Staiano and Bruno Lepri and Raffaele Marino", "abstract": "  A substantial gap persists in understanding the reasons behind the\nexceptional performance of the Transformer architecture in NLP. A particularly\nunexplored area involves the mechanistic description of how the distribution of\nparameters evolves over time during training. In this work we suggest that\nlooking at the time evolution of the statistic distribution of model\nparameters, and specifically at bifurcation effects, can help understanding the\nmodel quality, potentially reducing training costs and evaluation efforts and\nempirically showing the reasons behind the effectiveness of weights\nsparsification.\n", "link": "http://arxiv.org/abs/2403.08739v1", "date": "2024-03-13", "relevancy": 1.4653, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5288}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4807}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4674}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20The%20Garden%20of%20Forking%20Paths%3A%20Observing%20Dynamic%20Parameters%20Distribution%0A%20%20in%20Large%20Language%20Models&body=Title%3A%20The%20Garden%20of%20Forking%20Paths%3A%20Observing%20Dynamic%20Parameters%20Distribution%0A%20%20in%20Large%20Language%20Models%0AAuthor%3A%20Carlo%20Nicolini%20and%20Jacopo%20Staiano%20and%20Bruno%20Lepri%20and%20Raffaele%20Marino%0AAbstract%3A%20%20%20A%20substantial%20gap%20persists%20in%20understanding%20the%20reasons%20behind%20the%0Aexceptional%20performance%20of%20the%20Transformer%20architecture%20in%20NLP.%20A%20particularly%0Aunexplored%20area%20involves%20the%20mechanistic%20description%20of%20how%20the%20distribution%20of%0Aparameters%20evolves%20over%20time%20during%20training.%20In%20this%20work%20we%20suggest%20that%0Alooking%20at%20the%20time%20evolution%20of%20the%20statistic%20distribution%20of%20model%0Aparameters%2C%20and%20specifically%20at%20bifurcation%20effects%2C%20can%20help%20understanding%20the%0Amodel%20quality%2C%20potentially%20reducing%20training%20costs%20and%20evaluation%20efforts%20and%0Aempirically%20showing%20the%20reasons%20behind%20the%20effectiveness%20of%20weights%0Asparsification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08739v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Garden%20of%20Forking%20Paths%3A%20Observing%20Dynamic%20Parameters%20Distribution%0A%20%20in%20Large%20Language%20Models&entry.906535625=Carlo%20Nicolini%20and%20Jacopo%20Staiano%20and%20Bruno%20Lepri%20and%20Raffaele%20Marino&entry.1292438233=%20%20A%20substantial%20gap%20persists%20in%20understanding%20the%20reasons%20behind%20the%0Aexceptional%20performance%20of%20the%20Transformer%20architecture%20in%20NLP.%20A%20particularly%0Aunexplored%20area%20involves%20the%20mechanistic%20description%20of%20how%20the%20distribution%20of%0Aparameters%20evolves%20over%20time%20during%20training.%20In%20this%20work%20we%20suggest%20that%0Alooking%20at%20the%20time%20evolution%20of%20the%20statistic%20distribution%20of%20model%0Aparameters%2C%20and%20specifically%20at%20bifurcation%20effects%2C%20can%20help%20understanding%20the%0Amodel%20quality%2C%20potentially%20reducing%20training%20costs%20and%20evaluation%20efforts%20and%0Aempirically%20showing%20the%20reasons%20behind%20the%20effectiveness%20of%20weights%0Asparsification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08739v1&entry.124074799=Read"},
{"title": "Federated Knowledge Graph Unlearning via Diffusion Model", "author": "Bingchen Liu and Yuanyuan Fang", "abstract": "  Federated learning (FL) promotes the development and application of\nartificial intelligence technologies by enabling model sharing and\ncollaboration while safeguarding data privacy. Knowledge graph (KG) embedding\nrepresentation provides a foundation for knowledge reasoning and applications\nby mapping entities and relations into vector space. Federated KG embedding\nenables the utilization of knowledge from diverse client sources while\nsafeguarding the privacy of local data. However, due to demands such as privacy\nprotection and the need to adapt to dynamic data changes, investigations into\nmachine unlearning (MU) have been sparked. However, it is challenging to\nmaintain the performance of KG embedding models while forgetting the influence\nof specific forgotten data on the model. In this paper, we propose FedDM, a\nnovel framework tailored for machine unlearning in federated knowledge graphs.\nLeveraging diffusion models, we generate noisy data to sensibly mitigate the\ninfluence of specific knowledge on FL models while preserving the overall\nperformance concerning the remaining data. We conduct experimental evaluations\non benchmark datasets to assess the efficacy of the proposed model. Extensive\nexperiments demonstrate that FedDM yields promising results in knowledge\nforgetting.\n", "link": "http://arxiv.org/abs/2403.08554v1", "date": "2024-03-13", "relevancy": 1.4638, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5007}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4906}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4683}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Federated%20Knowledge%20Graph%20Unlearning%20via%20Diffusion%20Model&body=Title%3A%20Federated%20Knowledge%20Graph%20Unlearning%20via%20Diffusion%20Model%0AAuthor%3A%20Bingchen%20Liu%20and%20Yuanyuan%20Fang%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20promotes%20the%20development%20and%20application%20of%0Aartificial%20intelligence%20technologies%20by%20enabling%20model%20sharing%20and%0Acollaboration%20while%20safeguarding%20data%20privacy.%20Knowledge%20graph%20%28KG%29%20embedding%0Arepresentation%20provides%20a%20foundation%20for%20knowledge%20reasoning%20and%20applications%0Aby%20mapping%20entities%20and%20relations%20into%20vector%20space.%20Federated%20KG%20embedding%0Aenables%20the%20utilization%20of%20knowledge%20from%20diverse%20client%20sources%20while%0Asafeguarding%20the%20privacy%20of%20local%20data.%20However%2C%20due%20to%20demands%20such%20as%20privacy%0Aprotection%20and%20the%20need%20to%20adapt%20to%20dynamic%20data%20changes%2C%20investigations%20into%0Amachine%20unlearning%20%28MU%29%20have%20been%20sparked.%20However%2C%20it%20is%20challenging%20to%0Amaintain%20the%20performance%20of%20KG%20embedding%20models%20while%20forgetting%20the%20influence%0Aof%20specific%20forgotten%20data%20on%20the%20model.%20In%20this%20paper%2C%20we%20propose%20FedDM%2C%20a%0Anovel%20framework%20tailored%20for%20machine%20unlearning%20in%20federated%20knowledge%20graphs.%0ALeveraging%20diffusion%20models%2C%20we%20generate%20noisy%20data%20to%20sensibly%20mitigate%20the%0Ainfluence%20of%20specific%20knowledge%20on%20FL%20models%20while%20preserving%20the%20overall%0Aperformance%20concerning%20the%20remaining%20data.%20We%20conduct%20experimental%20evaluations%0Aon%20benchmark%20datasets%20to%20assess%20the%20efficacy%20of%20the%20proposed%20model.%20Extensive%0Aexperiments%20demonstrate%20that%20FedDM%20yields%20promising%20results%20in%20knowledge%0Aforgetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08554v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Knowledge%20Graph%20Unlearning%20via%20Diffusion%20Model&entry.906535625=Bingchen%20Liu%20and%20Yuanyuan%20Fang&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20promotes%20the%20development%20and%20application%20of%0Aartificial%20intelligence%20technologies%20by%20enabling%20model%20sharing%20and%0Acollaboration%20while%20safeguarding%20data%20privacy.%20Knowledge%20graph%20%28KG%29%20embedding%0Arepresentation%20provides%20a%20foundation%20for%20knowledge%20reasoning%20and%20applications%0Aby%20mapping%20entities%20and%20relations%20into%20vector%20space.%20Federated%20KG%20embedding%0Aenables%20the%20utilization%20of%20knowledge%20from%20diverse%20client%20sources%20while%0Asafeguarding%20the%20privacy%20of%20local%20data.%20However%2C%20due%20to%20demands%20such%20as%20privacy%0Aprotection%20and%20the%20need%20to%20adapt%20to%20dynamic%20data%20changes%2C%20investigations%20into%0Amachine%20unlearning%20%28MU%29%20have%20been%20sparked.%20However%2C%20it%20is%20challenging%20to%0Amaintain%20the%20performance%20of%20KG%20embedding%20models%20while%20forgetting%20the%20influence%0Aof%20specific%20forgotten%20data%20on%20the%20model.%20In%20this%20paper%2C%20we%20propose%20FedDM%2C%20a%0Anovel%20framework%20tailored%20for%20machine%20unlearning%20in%20federated%20knowledge%20graphs.%0ALeveraging%20diffusion%20models%2C%20we%20generate%20noisy%20data%20to%20sensibly%20mitigate%20the%0Ainfluence%20of%20specific%20knowledge%20on%20FL%20models%20while%20preserving%20the%20overall%0Aperformance%20concerning%20the%20remaining%20data.%20We%20conduct%20experimental%20evaluations%0Aon%20benchmark%20datasets%20to%20assess%20the%20efficacy%20of%20the%20proposed%20model.%20Extensive%0Aexperiments%20demonstrate%20that%20FedDM%20yields%20promising%20results%20in%20knowledge%0Aforgetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08554v1&entry.124074799=Read"},
{"title": "mForms : Multimodal Form-Filling with Question Answering", "author": "Larry Heck and Simon Heck and Anirudh Sundar", "abstract": "  This paper presents a new approach to form-filling by reformulating the task\nas multimodal natural language Question Answering (QA). The reformulation is\nachieved by first translating the elements on the GUI form (text fields,\nbuttons, icons, etc.) to natural language questions, where these questions\ncapture the element's multimodal semantics. After a match is determined between\nthe form element (Question) and the user utterance (Answer), the form element\nis filled through a pre-trained extractive QA system. By leveraging pre-trained\nQA models and not requiring form-specific training, this approach to\nform-filling is zero-shot. The paper also presents an approach to further\nrefine the form-filling by using multi-task training to incorporate a\npotentially large number of successive tasks. Finally, the paper introduces a\nmultimodal natural language form-filling dataset Multimodal Forms (mForms), as\nwell as a multimodal extension of the popular ATIS dataset to support future\nresearch and experimentation. Results show the new approach not only maintains\nrobust accuracy for sparse training conditions but achieves state-of-the-art F1\nof 0.97 on ATIS with approximately 1/10th of the training data.\n", "link": "http://arxiv.org/abs/2011.12340v3", "date": "2024-03-13", "relevancy": 1.4367, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4936}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4613}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4599}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20mForms%20%3A%20Multimodal%20Form-Filling%20with%20Question%20Answering&body=Title%3A%20mForms%20%3A%20Multimodal%20Form-Filling%20with%20Question%20Answering%0AAuthor%3A%20Larry%20Heck%20and%20Simon%20Heck%20and%20Anirudh%20Sundar%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20approach%20to%20form-filling%20by%20reformulating%20the%20task%0Aas%20multimodal%20natural%20language%20Question%20Answering%20%28QA%29.%20The%20reformulation%20is%0Aachieved%20by%20first%20translating%20the%20elements%20on%20the%20GUI%20form%20%28text%20fields%2C%0Abuttons%2C%20icons%2C%20etc.%29%20to%20natural%20language%20questions%2C%20where%20these%20questions%0Acapture%20the%20element%27s%20multimodal%20semantics.%20After%20a%20match%20is%20determined%20between%0Athe%20form%20element%20%28Question%29%20and%20the%20user%20utterance%20%28Answer%29%2C%20the%20form%20element%0Ais%20filled%20through%20a%20pre-trained%20extractive%20QA%20system.%20By%20leveraging%20pre-trained%0AQA%20models%20and%20not%20requiring%20form-specific%20training%2C%20this%20approach%20to%0Aform-filling%20is%20zero-shot.%20The%20paper%20also%20presents%20an%20approach%20to%20further%0Arefine%20the%20form-filling%20by%20using%20multi-task%20training%20to%20incorporate%20a%0Apotentially%20large%20number%20of%20successive%20tasks.%20Finally%2C%20the%20paper%20introduces%20a%0Amultimodal%20natural%20language%20form-filling%20dataset%20Multimodal%20Forms%20%28mForms%29%2C%20as%0Awell%20as%20a%20multimodal%20extension%20of%20the%20popular%20ATIS%20dataset%20to%20support%20future%0Aresearch%20and%20experimentation.%20Results%20show%20the%20new%20approach%20not%20only%20maintains%0Arobust%20accuracy%20for%20sparse%20training%20conditions%20but%20achieves%20state-of-the-art%20F1%0Aof%200.97%20on%20ATIS%20with%20approximately%201/10th%20of%20the%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2011.12340v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mForms%20%3A%20Multimodal%20Form-Filling%20with%20Question%20Answering&entry.906535625=Larry%20Heck%20and%20Simon%20Heck%20and%20Anirudh%20Sundar&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20approach%20to%20form-filling%20by%20reformulating%20the%20task%0Aas%20multimodal%20natural%20language%20Question%20Answering%20%28QA%29.%20The%20reformulation%20is%0Aachieved%20by%20first%20translating%20the%20elements%20on%20the%20GUI%20form%20%28text%20fields%2C%0Abuttons%2C%20icons%2C%20etc.%29%20to%20natural%20language%20questions%2C%20where%20these%20questions%0Acapture%20the%20element%27s%20multimodal%20semantics.%20After%20a%20match%20is%20determined%20between%0Athe%20form%20element%20%28Question%29%20and%20the%20user%20utterance%20%28Answer%29%2C%20the%20form%20element%0Ais%20filled%20through%20a%20pre-trained%20extractive%20QA%20system.%20By%20leveraging%20pre-trained%0AQA%20models%20and%20not%20requiring%20form-specific%20training%2C%20this%20approach%20to%0Aform-filling%20is%20zero-shot.%20The%20paper%20also%20presents%20an%20approach%20to%20further%0Arefine%20the%20form-filling%20by%20using%20multi-task%20training%20to%20incorporate%20a%0Apotentially%20large%20number%20of%20successive%20tasks.%20Finally%2C%20the%20paper%20introduces%20a%0Amultimodal%20natural%20language%20form-filling%20dataset%20Multimodal%20Forms%20%28mForms%29%2C%20as%0Awell%20as%20a%20multimodal%20extension%20of%20the%20popular%20ATIS%20dataset%20to%20support%20future%0Aresearch%20and%20experimentation.%20Results%20show%20the%20new%20approach%20not%20only%20maintains%0Arobust%20accuracy%20for%20sparse%20training%20conditions%20but%20achieves%20state-of-the-art%20F1%0Aof%200.97%20on%20ATIS%20with%20approximately%201/10th%20of%20the%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2011.12340v3&entry.124074799=Read"},
{"title": "A Physics-driven GraphSAGE Method for Physical Process Simulations\n  Described by Partial Differential Equations", "author": "Hang Hu and Sidi Wu and Guoxiong Cai and Na Liu", "abstract": "  Physics-informed neural networks (PINNs) have successfully addressed various\ncomputational physics problems based on partial differential equations (PDEs).\nHowever, while tackling issues related to irregularities like singularities and\noscillations, trained solutions usually suffer low accuracy. In addition, most\ncurrent works only offer the trained solution for predetermined input\nparameters. If any change occurs in input parameters, transfer learning or\nretraining is required, and traditional numerical techniques also need an\nindependent simulation. In this work, a physics-driven GraphSAGE approach\n(PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal\nbasis functions is presented to solve computational problems governed by\nirregular PDEs and to develop parametric PDE surrogate models. This approach\nemploys graph representations of physical domains, thereby reducing the demands\nfor evaluated points due to local refinement. A distance-related edge feature\nand a feature mapping strategy are devised to help training and convergence for\nsingularity and oscillation situations, respectively. The merits of the\nproposed method are demonstrated through a couple of cases. Moreover, the\nrobust PDE surrogate model for heat conduction problems parameterized by the\nGaussian random field source is successfully established, which not only\nprovides the solution accurately but is several times faster than the finite\nelement method in our experiments.\n", "link": "http://arxiv.org/abs/2403.08569v1", "date": "2024-03-13", "relevancy": 1.3354, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4945}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.458}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4202}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Physics-driven%20GraphSAGE%20Method%20for%20Physical%20Process%20Simulations%0A%20%20Described%20by%20Partial%20Differential%20Equations&body=Title%3A%20A%20Physics-driven%20GraphSAGE%20Method%20for%20Physical%20Process%20Simulations%0A%20%20Described%20by%20Partial%20Differential%20Equations%0AAuthor%3A%20Hang%20Hu%20and%20Sidi%20Wu%20and%20Guoxiong%20Cai%20and%20Na%20Liu%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20successfully%20addressed%20various%0Acomputational%20physics%20problems%20based%20on%20partial%20differential%20equations%20%28PDEs%29.%0AHowever%2C%20while%20tackling%20issues%20related%20to%20irregularities%20like%20singularities%20and%0Aoscillations%2C%20trained%20solutions%20usually%20suffer%20low%20accuracy.%20In%20addition%2C%20most%0Acurrent%20works%20only%20offer%20the%20trained%20solution%20for%20predetermined%20input%0Aparameters.%20If%20any%20change%20occurs%20in%20input%20parameters%2C%20transfer%20learning%20or%0Aretraining%20is%20required%2C%20and%20traditional%20numerical%20techniques%20also%20need%20an%0Aindependent%20simulation.%20In%20this%20work%2C%20a%20physics-driven%20GraphSAGE%20approach%0A%28PD-GraphSAGE%29%20based%20on%20the%20Galerkin%20method%20and%20piecewise%20polynomial%20nodal%0Abasis%20functions%20is%20presented%20to%20solve%20computational%20problems%20governed%20by%0Airregular%20PDEs%20and%20to%20develop%20parametric%20PDE%20surrogate%20models.%20This%20approach%0Aemploys%20graph%20representations%20of%20physical%20domains%2C%20thereby%20reducing%20the%20demands%0Afor%20evaluated%20points%20due%20to%20local%20refinement.%20A%20distance-related%20edge%20feature%0Aand%20a%20feature%20mapping%20strategy%20are%20devised%20to%20help%20training%20and%20convergence%20for%0Asingularity%20and%20oscillation%20situations%2C%20respectively.%20The%20merits%20of%20the%0Aproposed%20method%20are%20demonstrated%20through%20a%20couple%20of%20cases.%20Moreover%2C%20the%0Arobust%20PDE%20surrogate%20model%20for%20heat%20conduction%20problems%20parameterized%20by%20the%0AGaussian%20random%20field%20source%20is%20successfully%20established%2C%20which%20not%20only%0Aprovides%20the%20solution%20accurately%20but%20is%20several%20times%20faster%20than%20the%20finite%0Aelement%20method%20in%20our%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08569v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Physics-driven%20GraphSAGE%20Method%20for%20Physical%20Process%20Simulations%0A%20%20Described%20by%20Partial%20Differential%20Equations&entry.906535625=Hang%20Hu%20and%20Sidi%20Wu%20and%20Guoxiong%20Cai%20and%20Na%20Liu&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20successfully%20addressed%20various%0Acomputational%20physics%20problems%20based%20on%20partial%20differential%20equations%20%28PDEs%29.%0AHowever%2C%20while%20tackling%20issues%20related%20to%20irregularities%20like%20singularities%20and%0Aoscillations%2C%20trained%20solutions%20usually%20suffer%20low%20accuracy.%20In%20addition%2C%20most%0Acurrent%20works%20only%20offer%20the%20trained%20solution%20for%20predetermined%20input%0Aparameters.%20If%20any%20change%20occurs%20in%20input%20parameters%2C%20transfer%20learning%20or%0Aretraining%20is%20required%2C%20and%20traditional%20numerical%20techniques%20also%20need%20an%0Aindependent%20simulation.%20In%20this%20work%2C%20a%20physics-driven%20GraphSAGE%20approach%0A%28PD-GraphSAGE%29%20based%20on%20the%20Galerkin%20method%20and%20piecewise%20polynomial%20nodal%0Abasis%20functions%20is%20presented%20to%20solve%20computational%20problems%20governed%20by%0Airregular%20PDEs%20and%20to%20develop%20parametric%20PDE%20surrogate%20models.%20This%20approach%0Aemploys%20graph%20representations%20of%20physical%20domains%2C%20thereby%20reducing%20the%20demands%0Afor%20evaluated%20points%20due%20to%20local%20refinement.%20A%20distance-related%20edge%20feature%0Aand%20a%20feature%20mapping%20strategy%20are%20devised%20to%20help%20training%20and%20convergence%20for%0Asingularity%20and%20oscillation%20situations%2C%20respectively.%20The%20merits%20of%20the%0Aproposed%20method%20are%20demonstrated%20through%20a%20couple%20of%20cases.%20Moreover%2C%20the%0Arobust%20PDE%20surrogate%20model%20for%20heat%20conduction%20problems%20parameterized%20by%20the%0AGaussian%20random%20field%20source%20is%20successfully%20established%2C%20which%20not%20only%0Aprovides%20the%20solution%20accurately%20but%20is%20several%20times%20faster%20than%20the%20finite%0Aelement%20method%20in%20our%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08569v1&entry.124074799=Read"},
{"title": "Dr. Jekyll and Mr. Hyde: Two Faces of LLMs", "author": "Matteo Gioele Collu and Tom Janssen-Groesbeek and Stefanos Koffas and Mauro Conti and Stjepan Picek", "abstract": "  Only a year ago, we witnessed a rise in the use of Large Language Models\n(LLMs), especially when combined with applications like chatbot assistants.\nSafety mechanisms and specialized training procedures are implemented to\nprevent improper responses from these assistants. In this work, we bypass these\nmeasures for ChatGPT and Bard (and, to some extent, Bing chat) by making them\nimpersonate complex personas with opposite characteristics as those of the\ntruthful assistants they are supposed to be. We start by creating elaborate\nbiographies of these personas, which we then use in a new session with the same\nchatbots. Our conversation followed a role-play style to get the response the\nassistant was not allowed to provide. By making use of personas, we show that\nthe response that is prohibited is actually provided, making it possible to\nobtain unauthorized, illegal, or harmful information. This work shows that by\nusing adversarial personas, one can overcome safety mechanisms set out by\nChatGPT and Bard. We also introduce several ways of activating such adversarial\npersonas, altogether showing that both chatbots are vulnerable to this kind of\nattack. With the same principle, we introduce two defenses that push the model\nto interpret trustworthy personalities and make it more robust against such\nattacks.\n", "link": "http://arxiv.org/abs/2312.03853v2", "date": "2024-03-13", "relevancy": 1.3748, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4671}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4527}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4417}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Dr.%20Jekyll%20and%20Mr.%20Hyde%3A%20Two%20Faces%20of%20LLMs&body=Title%3A%20Dr.%20Jekyll%20and%20Mr.%20Hyde%3A%20Two%20Faces%20of%20LLMs%0AAuthor%3A%20Matteo%20Gioele%20Collu%20and%20Tom%20Janssen-Groesbeek%20and%20Stefanos%20Koffas%20and%20Mauro%20Conti%20and%20Stjepan%20Picek%0AAbstract%3A%20%20%20Only%20a%20year%20ago%2C%20we%20witnessed%20a%20rise%20in%20the%20use%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20especially%20when%20combined%20with%20applications%20like%20chatbot%20assistants.%0ASafety%20mechanisms%20and%20specialized%20training%20procedures%20are%20implemented%20to%0Aprevent%20improper%20responses%20from%20these%20assistants.%20In%20this%20work%2C%20we%20bypass%20these%0Ameasures%20for%20ChatGPT%20and%20Bard%20%28and%2C%20to%20some%20extent%2C%20Bing%20chat%29%20by%20making%20them%0Aimpersonate%20complex%20personas%20with%20opposite%20characteristics%20as%20those%20of%20the%0Atruthful%20assistants%20they%20are%20supposed%20to%20be.%20We%20start%20by%20creating%20elaborate%0Abiographies%20of%20these%20personas%2C%20which%20we%20then%20use%20in%20a%20new%20session%20with%20the%20same%0Achatbots.%20Our%20conversation%20followed%20a%20role-play%20style%20to%20get%20the%20response%20the%0Aassistant%20was%20not%20allowed%20to%20provide.%20By%20making%20use%20of%20personas%2C%20we%20show%20that%0Athe%20response%20that%20is%20prohibited%20is%20actually%20provided%2C%20making%20it%20possible%20to%0Aobtain%20unauthorized%2C%20illegal%2C%20or%20harmful%20information.%20This%20work%20shows%20that%20by%0Ausing%20adversarial%20personas%2C%20one%20can%20overcome%20safety%20mechanisms%20set%20out%20by%0AChatGPT%20and%20Bard.%20We%20also%20introduce%20several%20ways%20of%20activating%20such%20adversarial%0Apersonas%2C%20altogether%20showing%20that%20both%20chatbots%20are%20vulnerable%20to%20this%20kind%20of%0Aattack.%20With%20the%20same%20principle%2C%20we%20introduce%20two%20defenses%20that%20push%20the%20model%0Ato%20interpret%20trustworthy%20personalities%20and%20make%20it%20more%20robust%20against%20such%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03853v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dr.%20Jekyll%20and%20Mr.%20Hyde%3A%20Two%20Faces%20of%20LLMs&entry.906535625=Matteo%20Gioele%20Collu%20and%20Tom%20Janssen-Groesbeek%20and%20Stefanos%20Koffas%20and%20Mauro%20Conti%20and%20Stjepan%20Picek&entry.1292438233=%20%20Only%20a%20year%20ago%2C%20we%20witnessed%20a%20rise%20in%20the%20use%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20especially%20when%20combined%20with%20applications%20like%20chatbot%20assistants.%0ASafety%20mechanisms%20and%20specialized%20training%20procedures%20are%20implemented%20to%0Aprevent%20improper%20responses%20from%20these%20assistants.%20In%20this%20work%2C%20we%20bypass%20these%0Ameasures%20for%20ChatGPT%20and%20Bard%20%28and%2C%20to%20some%20extent%2C%20Bing%20chat%29%20by%20making%20them%0Aimpersonate%20complex%20personas%20with%20opposite%20characteristics%20as%20those%20of%20the%0Atruthful%20assistants%20they%20are%20supposed%20to%20be.%20We%20start%20by%20creating%20elaborate%0Abiographies%20of%20these%20personas%2C%20which%20we%20then%20use%20in%20a%20new%20session%20with%20the%20same%0Achatbots.%20Our%20conversation%20followed%20a%20role-play%20style%20to%20get%20the%20response%20the%0Aassistant%20was%20not%20allowed%20to%20provide.%20By%20making%20use%20of%20personas%2C%20we%20show%20that%0Athe%20response%20that%20is%20prohibited%20is%20actually%20provided%2C%20making%20it%20possible%20to%0Aobtain%20unauthorized%2C%20illegal%2C%20or%20harmful%20information.%20This%20work%20shows%20that%20by%0Ausing%20adversarial%20personas%2C%20one%20can%20overcome%20safety%20mechanisms%20set%20out%20by%0AChatGPT%20and%20Bard.%20We%20also%20introduce%20several%20ways%20of%20activating%20such%20adversarial%0Apersonas%2C%20altogether%20showing%20that%20both%20chatbots%20are%20vulnerable%20to%20this%20kind%20of%0Aattack.%20With%20the%20same%20principle%2C%20we%20introduce%20two%20defenses%20that%20push%20the%20model%0Ato%20interpret%20trustworthy%20personalities%20and%20make%20it%20more%20robust%20against%20such%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03853v2&entry.124074799=Read"},
{"title": "Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative\n  Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches", "author": "Yun Xin Teoh and Alice Othmani and Siew Li Goh and Juliana Usman and Khin Wee Lai", "abstract": "  Knee osteoarthritis is a degenerative joint disease that induces chronic pain\nand disability. Bone morphological analysis is a promising tool to understand\nthe mechanical aspect of this disorder. This study proposes a 2D bone\nmorphological analysis using manually segmented bones to explore morphological\nfeatures related to distinct pain conditions. Furthermore, six semantic\nsegmentation algorithms are assessed for extracting femur and tibia bones from\nX-ray images. Our analysis reveals that the morphology of the femur undergoes\nsignificant changes in instances where pain worsens. Conversely, improvements\nin pain may not manifest pronounced alterations in bone shape. The\nfew-shot-learning-based algorithm, UniverSeg, demonstrated superior\nsegmentation results with Dice scores of 99.69% for femur and 99.60% for tibia.\nRegarding pain condition classification, the zero-shot-learning-based\nalgorithm, CP-SAM, achieved the highest accuracy at 66% among all models.\nUniverSeg is recommended for automatic knee bone segmentation, while SAM models\nshow potential with prompt encoder modifications for optimized outcomes. These\nfindings highlight the effectiveness of few-shot learning for semantic\nsegmentation and the potential of zero-shot learning in enhancing\nclassification models for knee osteoarthritis diagnosis.\n", "link": "http://arxiv.org/abs/2403.08761v1", "date": "2024-03-13", "relevancy": 1.4281, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4828}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.464}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Segmentation%20of%20Knee%20Bones%20for%20Osteoarthritis%20Assessment%3A%20A%20Comparative%0A%20%20Analysis%20of%20Supervised%2C%20Few-Shot%2C%20and%20Zero-Shot%20Learning%20Approaches&body=Title%3A%20Segmentation%20of%20Knee%20Bones%20for%20Osteoarthritis%20Assessment%3A%20A%20Comparative%0A%20%20Analysis%20of%20Supervised%2C%20Few-Shot%2C%20and%20Zero-Shot%20Learning%20Approaches%0AAuthor%3A%20Yun%20Xin%20Teoh%20and%20Alice%20Othmani%20and%20Siew%20Li%20Goh%20and%20Juliana%20Usman%20and%20Khin%20Wee%20Lai%0AAbstract%3A%20%20%20Knee%20osteoarthritis%20is%20a%20degenerative%20joint%20disease%20that%20induces%20chronic%20pain%0Aand%20disability.%20Bone%20morphological%20analysis%20is%20a%20promising%20tool%20to%20understand%0Athe%20mechanical%20aspect%20of%20this%20disorder.%20This%20study%20proposes%20a%202D%20bone%0Amorphological%20analysis%20using%20manually%20segmented%20bones%20to%20explore%20morphological%0Afeatures%20related%20to%20distinct%20pain%20conditions.%20Furthermore%2C%20six%20semantic%0Asegmentation%20algorithms%20are%20assessed%20for%20extracting%20femur%20and%20tibia%20bones%20from%0AX-ray%20images.%20Our%20analysis%20reveals%20that%20the%20morphology%20of%20the%20femur%20undergoes%0Asignificant%20changes%20in%20instances%20where%20pain%20worsens.%20Conversely%2C%20improvements%0Ain%20pain%20may%20not%20manifest%20pronounced%20alterations%20in%20bone%20shape.%20The%0Afew-shot-learning-based%20algorithm%2C%20UniverSeg%2C%20demonstrated%20superior%0Asegmentation%20results%20with%20Dice%20scores%20of%2099.69%25%20for%20femur%20and%2099.60%25%20for%20tibia.%0ARegarding%20pain%20condition%20classification%2C%20the%20zero-shot-learning-based%0Aalgorithm%2C%20CP-SAM%2C%20achieved%20the%20highest%20accuracy%20at%2066%25%20among%20all%20models.%0AUniverSeg%20is%20recommended%20for%20automatic%20knee%20bone%20segmentation%2C%20while%20SAM%20models%0Ashow%20potential%20with%20prompt%20encoder%20modifications%20for%20optimized%20outcomes.%20These%0Afindings%20highlight%20the%20effectiveness%20of%20few-shot%20learning%20for%20semantic%0Asegmentation%20and%20the%20potential%20of%20zero-shot%20learning%20in%20enhancing%0Aclassification%20models%20for%20knee%20osteoarthritis%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08761v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation%20of%20Knee%20Bones%20for%20Osteoarthritis%20Assessment%3A%20A%20Comparative%0A%20%20Analysis%20of%20Supervised%2C%20Few-Shot%2C%20and%20Zero-Shot%20Learning%20Approaches&entry.906535625=Yun%20Xin%20Teoh%20and%20Alice%20Othmani%20and%20Siew%20Li%20Goh%20and%20Juliana%20Usman%20and%20Khin%20Wee%20Lai&entry.1292438233=%20%20Knee%20osteoarthritis%20is%20a%20degenerative%20joint%20disease%20that%20induces%20chronic%20pain%0Aand%20disability.%20Bone%20morphological%20analysis%20is%20a%20promising%20tool%20to%20understand%0Athe%20mechanical%20aspect%20of%20this%20disorder.%20This%20study%20proposes%20a%202D%20bone%0Amorphological%20analysis%20using%20manually%20segmented%20bones%20to%20explore%20morphological%0Afeatures%20related%20to%20distinct%20pain%20conditions.%20Furthermore%2C%20six%20semantic%0Asegmentation%20algorithms%20are%20assessed%20for%20extracting%20femur%20and%20tibia%20bones%20from%0AX-ray%20images.%20Our%20analysis%20reveals%20that%20the%20morphology%20of%20the%20femur%20undergoes%0Asignificant%20changes%20in%20instances%20where%20pain%20worsens.%20Conversely%2C%20improvements%0Ain%20pain%20may%20not%20manifest%20pronounced%20alterations%20in%20bone%20shape.%20The%0Afew-shot-learning-based%20algorithm%2C%20UniverSeg%2C%20demonstrated%20superior%0Asegmentation%20results%20with%20Dice%20scores%20of%2099.69%25%20for%20femur%20and%2099.60%25%20for%20tibia.%0ARegarding%20pain%20condition%20classification%2C%20the%20zero-shot-learning-based%0Aalgorithm%2C%20CP-SAM%2C%20achieved%20the%20highest%20accuracy%20at%2066%25%20among%20all%20models.%0AUniverSeg%20is%20recommended%20for%20automatic%20knee%20bone%20segmentation%2C%20while%20SAM%20models%0Ashow%20potential%20with%20prompt%20encoder%20modifications%20for%20optimized%20outcomes.%20These%0Afindings%20highlight%20the%20effectiveness%20of%20few-shot%20learning%20for%20semantic%0Asegmentation%20and%20the%20potential%20of%20zero-shot%20learning%20in%20enhancing%0Aclassification%20models%20for%20knee%20osteoarthritis%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08761v1&entry.124074799=Read"},
{"title": "Digital Twin-assisted Reinforcement Learning for Resource-aware\n  Microservice Offloading in Edge Computing", "author": "Xiangchun Chen and Jiannong Cao and Zhixuan Liang and Yuvraj Sahni and Mingjin Zhang", "abstract": "  Collaborative edge computing (CEC) has emerged as a promising paradigm,\nenabling edge nodes to collaborate and execute microservices from end devices.\nMicroservice offloading, a fundamentally important problem, decides when and\nwhere microservices are executed upon the arrival of services. However, the\ndynamic nature of the real-world CEC environment often leads to inefficient\nmicroservice offloading strategies, resulting in underutilized resources and\nnetwork congestion. To address this challenge, we formulate an online joint\nmicroservice offloading and bandwidth allocation problem, JMOBA, to minimize\nthe average completion time of services. In this paper, we introduce a novel\nmicroservice offloading algorithm, DTDRLMO, which leverages deep reinforcement\nlearning (DRL) and digital twin technology. Specifically, we employ digital\ntwin techniques to predict and adapt to changing edge node loads and network\nconditions of CEC in real-time. Furthermore, this approach enables the\ngeneration of an efficient offloading plan, selecting the most suitable edge\nnode for each microservice. Simulation results on real-world and synthetic\ndatasets demonstrate that DTDRLMO outperforms heuristic and learning-based\nmethods in average service completion time.\n", "link": "http://arxiv.org/abs/2403.08687v1", "date": "2024-03-13", "relevancy": 1.4236, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4769}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.473}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4701}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Digital%20Twin-assisted%20Reinforcement%20Learning%20for%20Resource-aware%0A%20%20Microservice%20Offloading%20in%20Edge%20Computing&body=Title%3A%20Digital%20Twin-assisted%20Reinforcement%20Learning%20for%20Resource-aware%0A%20%20Microservice%20Offloading%20in%20Edge%20Computing%0AAuthor%3A%20Xiangchun%20Chen%20and%20Jiannong%20Cao%20and%20Zhixuan%20Liang%20and%20Yuvraj%20Sahni%20and%20Mingjin%20Zhang%0AAbstract%3A%20%20%20Collaborative%20edge%20computing%20%28CEC%29%20has%20emerged%20as%20a%20promising%20paradigm%2C%0Aenabling%20edge%20nodes%20to%20collaborate%20and%20execute%20microservices%20from%20end%20devices.%0AMicroservice%20offloading%2C%20a%20fundamentally%20important%20problem%2C%20decides%20when%20and%0Awhere%20microservices%20are%20executed%20upon%20the%20arrival%20of%20services.%20However%2C%20the%0Adynamic%20nature%20of%20the%20real-world%20CEC%20environment%20often%20leads%20to%20inefficient%0Amicroservice%20offloading%20strategies%2C%20resulting%20in%20underutilized%20resources%20and%0Anetwork%20congestion.%20To%20address%20this%20challenge%2C%20we%20formulate%20an%20online%20joint%0Amicroservice%20offloading%20and%20bandwidth%20allocation%20problem%2C%20JMOBA%2C%20to%20minimize%0Athe%20average%20completion%20time%20of%20services.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Amicroservice%20offloading%20algorithm%2C%20DTDRLMO%2C%20which%20leverages%20deep%20reinforcement%0Alearning%20%28DRL%29%20and%20digital%20twin%20technology.%20Specifically%2C%20we%20employ%20digital%0Atwin%20techniques%20to%20predict%20and%20adapt%20to%20changing%20edge%20node%20loads%20and%20network%0Aconditions%20of%20CEC%20in%20real-time.%20Furthermore%2C%20this%20approach%20enables%20the%0Ageneration%20of%20an%20efficient%20offloading%20plan%2C%20selecting%20the%20most%20suitable%20edge%0Anode%20for%20each%20microservice.%20Simulation%20results%20on%20real-world%20and%20synthetic%0Adatasets%20demonstrate%20that%20DTDRLMO%20outperforms%20heuristic%20and%20learning-based%0Amethods%20in%20average%20service%20completion%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08687v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Twin-assisted%20Reinforcement%20Learning%20for%20Resource-aware%0A%20%20Microservice%20Offloading%20in%20Edge%20Computing&entry.906535625=Xiangchun%20Chen%20and%20Jiannong%20Cao%20and%20Zhixuan%20Liang%20and%20Yuvraj%20Sahni%20and%20Mingjin%20Zhang&entry.1292438233=%20%20Collaborative%20edge%20computing%20%28CEC%29%20has%20emerged%20as%20a%20promising%20paradigm%2C%0Aenabling%20edge%20nodes%20to%20collaborate%20and%20execute%20microservices%20from%20end%20devices.%0AMicroservice%20offloading%2C%20a%20fundamentally%20important%20problem%2C%20decides%20when%20and%0Awhere%20microservices%20are%20executed%20upon%20the%20arrival%20of%20services.%20However%2C%20the%0Adynamic%20nature%20of%20the%20real-world%20CEC%20environment%20often%20leads%20to%20inefficient%0Amicroservice%20offloading%20strategies%2C%20resulting%20in%20underutilized%20resources%20and%0Anetwork%20congestion.%20To%20address%20this%20challenge%2C%20we%20formulate%20an%20online%20joint%0Amicroservice%20offloading%20and%20bandwidth%20allocation%20problem%2C%20JMOBA%2C%20to%20minimize%0Athe%20average%20completion%20time%20of%20services.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Amicroservice%20offloading%20algorithm%2C%20DTDRLMO%2C%20which%20leverages%20deep%20reinforcement%0Alearning%20%28DRL%29%20and%20digital%20twin%20technology.%20Specifically%2C%20we%20employ%20digital%0Atwin%20techniques%20to%20predict%20and%20adapt%20to%20changing%20edge%20node%20loads%20and%20network%0Aconditions%20of%20CEC%20in%20real-time.%20Furthermore%2C%20this%20approach%20enables%20the%0Ageneration%20of%20an%20efficient%20offloading%20plan%2C%20selecting%20the%20most%20suitable%20edge%0Anode%20for%20each%20microservice.%20Simulation%20results%20on%20real-world%20and%20synthetic%0Adatasets%20demonstrate%20that%20DTDRLMO%20outperforms%20heuristic%20and%20learning-based%0Amethods%20in%20average%20service%20completion%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08687v1&entry.124074799=Read"},
{"title": "Disparate Effect Of Missing Mediators On Transportability of Causal\n  Effects", "author": "Vishwali Mhasawade and Rumi Chunara", "abstract": "  Transported mediation effects provide an avenue to understand how upstream\ninterventions (such as improved neighborhood conditions like green spaces)\nwould work differently when applied to different populations as a result of\nfactors that mediate the effects. However, when mediators are missing in the\npopulation where the effect is to be transported, these estimates could be\nbiased. We study this issue of missing mediators, motivated by challenges in\npublic health, wherein mediators can be missing, not at random. We propose a\nsensitivity analysis framework that quantifies the impact of missing mediator\ndata on transported mediation effects. This framework enables us to identify\nthe settings under which the conditional transported mediation effect is\nrendered insignificant for the subgroup with missing mediator data.\nSpecifically, we provide the bounds on the transported mediation effect as a\nfunction of missingness. We then apply the framework to longitudinal data from\nthe Moving to Opportunity Study, a large-scale housing voucher experiment, to\nquantify the effect of missing mediators on transport effect estimates of\nvoucher receipt, an upstream intervention on living location, in childhood on\nsubsequent risk of mental health or substance use disorder mediated through\nparental health across sites. Our findings provide a tangible understanding of\nhow much missing data can be withstood for unbiased effect estimates.\n", "link": "http://arxiv.org/abs/2403.08638v1", "date": "2024-03-13", "relevancy": 1.1176, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3995}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3725}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3618}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Disparate%20Effect%20Of%20Missing%20Mediators%20On%20Transportability%20of%20Causal%0A%20%20Effects&body=Title%3A%20Disparate%20Effect%20Of%20Missing%20Mediators%20On%20Transportability%20of%20Causal%0A%20%20Effects%0AAuthor%3A%20Vishwali%20Mhasawade%20and%20Rumi%20Chunara%0AAbstract%3A%20%20%20Transported%20mediation%20effects%20provide%20an%20avenue%20to%20understand%20how%20upstream%0Ainterventions%20%28such%20as%20improved%20neighborhood%20conditions%20like%20green%20spaces%29%0Awould%20work%20differently%20when%20applied%20to%20different%20populations%20as%20a%20result%20of%0Afactors%20that%20mediate%20the%20effects.%20However%2C%20when%20mediators%20are%20missing%20in%20the%0Apopulation%20where%20the%20effect%20is%20to%20be%20transported%2C%20these%20estimates%20could%20be%0Abiased.%20We%20study%20this%20issue%20of%20missing%20mediators%2C%20motivated%20by%20challenges%20in%0Apublic%20health%2C%20wherein%20mediators%20can%20be%20missing%2C%20not%20at%20random.%20We%20propose%20a%0Asensitivity%20analysis%20framework%20that%20quantifies%20the%20impact%20of%20missing%20mediator%0Adata%20on%20transported%20mediation%20effects.%20This%20framework%20enables%20us%20to%20identify%0Athe%20settings%20under%20which%20the%20conditional%20transported%20mediation%20effect%20is%0Arendered%20insignificant%20for%20the%20subgroup%20with%20missing%20mediator%20data.%0ASpecifically%2C%20we%20provide%20the%20bounds%20on%20the%20transported%20mediation%20effect%20as%20a%0Afunction%20of%20missingness.%20We%20then%20apply%20the%20framework%20to%20longitudinal%20data%20from%0Athe%20Moving%20to%20Opportunity%20Study%2C%20a%20large-scale%20housing%20voucher%20experiment%2C%20to%0Aquantify%20the%20effect%20of%20missing%20mediators%20on%20transport%20effect%20estimates%20of%0Avoucher%20receipt%2C%20an%20upstream%20intervention%20on%20living%20location%2C%20in%20childhood%20on%0Asubsequent%20risk%20of%20mental%20health%20or%20substance%20use%20disorder%20mediated%20through%0Aparental%20health%20across%20sites.%20Our%20findings%20provide%20a%20tangible%20understanding%20of%0Ahow%20much%20missing%20data%20can%20be%20withstood%20for%20unbiased%20effect%20estimates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08638v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disparate%20Effect%20Of%20Missing%20Mediators%20On%20Transportability%20of%20Causal%0A%20%20Effects&entry.906535625=Vishwali%20Mhasawade%20and%20Rumi%20Chunara&entry.1292438233=%20%20Transported%20mediation%20effects%20provide%20an%20avenue%20to%20understand%20how%20upstream%0Ainterventions%20%28such%20as%20improved%20neighborhood%20conditions%20like%20green%20spaces%29%0Awould%20work%20differently%20when%20applied%20to%20different%20populations%20as%20a%20result%20of%0Afactors%20that%20mediate%20the%20effects.%20However%2C%20when%20mediators%20are%20missing%20in%20the%0Apopulation%20where%20the%20effect%20is%20to%20be%20transported%2C%20these%20estimates%20could%20be%0Abiased.%20We%20study%20this%20issue%20of%20missing%20mediators%2C%20motivated%20by%20challenges%20in%0Apublic%20health%2C%20wherein%20mediators%20can%20be%20missing%2C%20not%20at%20random.%20We%20propose%20a%0Asensitivity%20analysis%20framework%20that%20quantifies%20the%20impact%20of%20missing%20mediator%0Adata%20on%20transported%20mediation%20effects.%20This%20framework%20enables%20us%20to%20identify%0Athe%20settings%20under%20which%20the%20conditional%20transported%20mediation%20effect%20is%0Arendered%20insignificant%20for%20the%20subgroup%20with%20missing%20mediator%20data.%0ASpecifically%2C%20we%20provide%20the%20bounds%20on%20the%20transported%20mediation%20effect%20as%20a%0Afunction%20of%20missingness.%20We%20then%20apply%20the%20framework%20to%20longitudinal%20data%20from%0Athe%20Moving%20to%20Opportunity%20Study%2C%20a%20large-scale%20housing%20voucher%20experiment%2C%20to%0Aquantify%20the%20effect%20of%20missing%20mediators%20on%20transport%20effect%20estimates%20of%0Avoucher%20receipt%2C%20an%20upstream%20intervention%20on%20living%20location%2C%20in%20childhood%20on%0Asubsequent%20risk%20of%20mental%20health%20or%20substance%20use%20disorder%20mediated%20through%0Aparental%20health%20across%20sites.%20Our%20findings%20provide%20a%20tangible%20understanding%20of%0Ahow%20much%20missing%20data%20can%20be%20withstood%20for%20unbiased%20effect%20estimates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08638v1&entry.124074799=Read"},
{"title": "MedInsight: A Multi-Source Context Augmentation Framework for Generating\n  Patient-Centric Medical Responses using Large Language Models", "author": "Subash Neupane and Shaswata Mitra and Sudip Mittal and Noorbakhsh Amiri Golilarz and Shahram Rahimi and Amin Amirlatifi", "abstract": "  Large Language Models (LLMs) have shown impressive capabilities in generating\nhuman-like responses. However, their lack of domain-specific knowledge limits\ntheir applicability in healthcare settings, where contextual and comprehensive\nresponses are vital. To address this challenge and enable the generation of\npatient-centric responses that are contextually relevant and comprehensive, we\npropose MedInsight:a novel retrieval augmented framework that augments LLM\ninputs (prompts) with relevant background information from multiple sources.\nMedInsight extracts pertinent details from the patient's medical record or\nconsultation transcript. It then integrates information from authoritative\nmedical textbooks and curated web resources based on the patient's health\nhistory and condition. By constructing an augmented context combining the\npatient's record with relevant medical knowledge, MedInsight generates\nenriched, patient-specific responses tailored for healthcare applications such\nas diagnosis, treatment recommendations, or patient education. Experiments on\nthe MTSamples dataset validate MedInsight's effectiveness in generating\ncontextually appropriate medical responses. Quantitative evaluation using the\nRagas metric and TruLens for answer similarity and answer correctness\ndemonstrates the model's efficacy. Furthermore, human evaluation studies\ninvolving Subject Matter Expert (SMEs) confirm MedInsight's utility, with\nmoderate inter-rater agreement on the relevance and correctness of the\ngenerated responses.\n", "link": "http://arxiv.org/abs/2403.08607v1", "date": "2024-03-13", "relevancy": 1.4112, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5406}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4519}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4465}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MedInsight%3A%20A%20Multi-Source%20Context%20Augmentation%20Framework%20for%20Generating%0A%20%20Patient-Centric%20Medical%20Responses%20using%20Large%20Language%20Models&body=Title%3A%20MedInsight%3A%20A%20Multi-Source%20Context%20Augmentation%20Framework%20for%20Generating%0A%20%20Patient-Centric%20Medical%20Responses%20using%20Large%20Language%20Models%0AAuthor%3A%20Subash%20Neupane%20and%20Shaswata%20Mitra%20and%20Sudip%20Mittal%20and%20Noorbakhsh%20Amiri%20Golilarz%20and%20Shahram%20Rahimi%20and%20Amin%20Amirlatifi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20in%20generating%0Ahuman-like%20responses.%20However%2C%20their%20lack%20of%20domain-specific%20knowledge%20limits%0Atheir%20applicability%20in%20healthcare%20settings%2C%20where%20contextual%20and%20comprehensive%0Aresponses%20are%20vital.%20To%20address%20this%20challenge%20and%20enable%20the%20generation%20of%0Apatient-centric%20responses%20that%20are%20contextually%20relevant%20and%20comprehensive%2C%20we%0Apropose%20MedInsight%3Aa%20novel%20retrieval%20augmented%20framework%20that%20augments%20LLM%0Ainputs%20%28prompts%29%20with%20relevant%20background%20information%20from%20multiple%20sources.%0AMedInsight%20extracts%20pertinent%20details%20from%20the%20patient%27s%20medical%20record%20or%0Aconsultation%20transcript.%20It%20then%20integrates%20information%20from%20authoritative%0Amedical%20textbooks%20and%20curated%20web%20resources%20based%20on%20the%20patient%27s%20health%0Ahistory%20and%20condition.%20By%20constructing%20an%20augmented%20context%20combining%20the%0Apatient%27s%20record%20with%20relevant%20medical%20knowledge%2C%20MedInsight%20generates%0Aenriched%2C%20patient-specific%20responses%20tailored%20for%20healthcare%20applications%20such%0Aas%20diagnosis%2C%20treatment%20recommendations%2C%20or%20patient%20education.%20Experiments%20on%0Athe%20MTSamples%20dataset%20validate%20MedInsight%27s%20effectiveness%20in%20generating%0Acontextually%20appropriate%20medical%20responses.%20Quantitative%20evaluation%20using%20the%0ARagas%20metric%20and%20TruLens%20for%20answer%20similarity%20and%20answer%20correctness%0Ademonstrates%20the%20model%27s%20efficacy.%20Furthermore%2C%20human%20evaluation%20studies%0Ainvolving%20Subject%20Matter%20Expert%20%28SMEs%29%20confirm%20MedInsight%27s%20utility%2C%20with%0Amoderate%20inter-rater%20agreement%20on%20the%20relevance%20and%20correctness%20of%20the%0Agenerated%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08607v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedInsight%3A%20A%20Multi-Source%20Context%20Augmentation%20Framework%20for%20Generating%0A%20%20Patient-Centric%20Medical%20Responses%20using%20Large%20Language%20Models&entry.906535625=Subash%20Neupane%20and%20Shaswata%20Mitra%20and%20Sudip%20Mittal%20and%20Noorbakhsh%20Amiri%20Golilarz%20and%20Shahram%20Rahimi%20and%20Amin%20Amirlatifi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20in%20generating%0Ahuman-like%20responses.%20However%2C%20their%20lack%20of%20domain-specific%20knowledge%20limits%0Atheir%20applicability%20in%20healthcare%20settings%2C%20where%20contextual%20and%20comprehensive%0Aresponses%20are%20vital.%20To%20address%20this%20challenge%20and%20enable%20the%20generation%20of%0Apatient-centric%20responses%20that%20are%20contextually%20relevant%20and%20comprehensive%2C%20we%0Apropose%20MedInsight%3Aa%20novel%20retrieval%20augmented%20framework%20that%20augments%20LLM%0Ainputs%20%28prompts%29%20with%20relevant%20background%20information%20from%20multiple%20sources.%0AMedInsight%20extracts%20pertinent%20details%20from%20the%20patient%27s%20medical%20record%20or%0Aconsultation%20transcript.%20It%20then%20integrates%20information%20from%20authoritative%0Amedical%20textbooks%20and%20curated%20web%20resources%20based%20on%20the%20patient%27s%20health%0Ahistory%20and%20condition.%20By%20constructing%20an%20augmented%20context%20combining%20the%0Apatient%27s%20record%20with%20relevant%20medical%20knowledge%2C%20MedInsight%20generates%0Aenriched%2C%20patient-specific%20responses%20tailored%20for%20healthcare%20applications%20such%0Aas%20diagnosis%2C%20treatment%20recommendations%2C%20or%20patient%20education.%20Experiments%20on%0Athe%20MTSamples%20dataset%20validate%20MedInsight%27s%20effectiveness%20in%20generating%0Acontextually%20appropriate%20medical%20responses.%20Quantitative%20evaluation%20using%20the%0ARagas%20metric%20and%20TruLens%20for%20answer%20similarity%20and%20answer%20correctness%0Ademonstrates%20the%20model%27s%20efficacy.%20Furthermore%2C%20human%20evaluation%20studies%0Ainvolving%20Subject%20Matter%20Expert%20%28SMEs%29%20confirm%20MedInsight%27s%20utility%2C%20with%0Amoderate%20inter-rater%20agreement%20on%20the%20relevance%20and%20correctness%20of%20the%0Agenerated%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08607v1&entry.124074799=Read"},
{"title": "Single file motion of robot swarms", "author": "Laciel Alonso-Llanes and Angel Garcimart\u00edn and Iker Zuriguel", "abstract": "  We present experimental results on the single file motion of a group of\nrobots interacting with each other through position sensors. We successfully\nreplicate the fundamental diagram typical of these systems, with a transition\nfrom free flow to congested traffic as the density of the system increases. In\nthe latter scenario we also observe the characteristic stop-and-go waves. The\nunique advantages of this novel system, such as experimental stability and\nrepeatability, allow for extended experimental runs, facilitating a\ncomprehensive statistical analysis of the global dynamics. Above a certain\ndensity, we observe a divergence of the average jam duration and the average\nnumber of robots involved in it. This discovery enables us to precisely\nidentify another transition: from congested intermittent flow (for intermediate\ndensities) to a totally congested scenario for high densities. Beyond this\nfinding, the present work demonstrates the suitability of robot swarms to model\ncomplex behaviors in many particle systems.\n", "link": "http://arxiv.org/abs/2403.08683v1", "date": "2024-03-13", "relevancy": 1.3562, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4698}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4693}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4381}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Single%20file%20motion%20of%20robot%20swarms&body=Title%3A%20Single%20file%20motion%20of%20robot%20swarms%0AAuthor%3A%20Laciel%20Alonso-Llanes%20and%20Angel%20Garcimart%C3%ADn%20and%20Iker%20Zuriguel%0AAbstract%3A%20%20%20We%20present%20experimental%20results%20on%20the%20single%20file%20motion%20of%20a%20group%20of%0Arobots%20interacting%20with%20each%20other%20through%20position%20sensors.%20We%20successfully%0Areplicate%20the%20fundamental%20diagram%20typical%20of%20these%20systems%2C%20with%20a%20transition%0Afrom%20free%20flow%20to%20congested%20traffic%20as%20the%20density%20of%20the%20system%20increases.%20In%0Athe%20latter%20scenario%20we%20also%20observe%20the%20characteristic%20stop-and-go%20waves.%20The%0Aunique%20advantages%20of%20this%20novel%20system%2C%20such%20as%20experimental%20stability%20and%0Arepeatability%2C%20allow%20for%20extended%20experimental%20runs%2C%20facilitating%20a%0Acomprehensive%20statistical%20analysis%20of%20the%20global%20dynamics.%20Above%20a%20certain%0Adensity%2C%20we%20observe%20a%20divergence%20of%20the%20average%20jam%20duration%20and%20the%20average%0Anumber%20of%20robots%20involved%20in%20it.%20This%20discovery%20enables%20us%20to%20precisely%0Aidentify%20another%20transition%3A%20from%20congested%20intermittent%20flow%20%28for%20intermediate%0Adensities%29%20to%20a%20totally%20congested%20scenario%20for%20high%20densities.%20Beyond%20this%0Afinding%2C%20the%20present%20work%20demonstrates%20the%20suitability%20of%20robot%20swarms%20to%20model%0Acomplex%20behaviors%20in%20many%20particle%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08683v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20file%20motion%20of%20robot%20swarms&entry.906535625=Laciel%20Alonso-Llanes%20and%20Angel%20Garcimart%C3%ADn%20and%20Iker%20Zuriguel&entry.1292438233=%20%20We%20present%20experimental%20results%20on%20the%20single%20file%20motion%20of%20a%20group%20of%0Arobots%20interacting%20with%20each%20other%20through%20position%20sensors.%20We%20successfully%0Areplicate%20the%20fundamental%20diagram%20typical%20of%20these%20systems%2C%20with%20a%20transition%0Afrom%20free%20flow%20to%20congested%20traffic%20as%20the%20density%20of%20the%20system%20increases.%20In%0Athe%20latter%20scenario%20we%20also%20observe%20the%20characteristic%20stop-and-go%20waves.%20The%0Aunique%20advantages%20of%20this%20novel%20system%2C%20such%20as%20experimental%20stability%20and%0Arepeatability%2C%20allow%20for%20extended%20experimental%20runs%2C%20facilitating%20a%0Acomprehensive%20statistical%20analysis%20of%20the%20global%20dynamics.%20Above%20a%20certain%0Adensity%2C%20we%20observe%20a%20divergence%20of%20the%20average%20jam%20duration%20and%20the%20average%0Anumber%20of%20robots%20involved%20in%20it.%20This%20discovery%20enables%20us%20to%20precisely%0Aidentify%20another%20transition%3A%20from%20congested%20intermittent%20flow%20%28for%20intermediate%0Adensities%29%20to%20a%20totally%20congested%20scenario%20for%20high%20densities.%20Beyond%20this%0Afinding%2C%20the%20present%20work%20demonstrates%20the%20suitability%20of%20robot%20swarms%20to%20model%0Acomplex%20behaviors%20in%20many%20particle%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08683v1&entry.124074799=Read"},
{"title": "Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing\n  Framework", "author": "Jingling Li and Zeyu Tang and Xiaoyu Liu and Peter Spirtes and Kun Zhang and Liu Leqi and Yang Liu", "abstract": "  Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.\n", "link": "http://arxiv.org/abs/2403.08743v1", "date": "2024-03-13", "relevancy": 1.4167, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4988}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4684}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4553}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Steering%20LLMs%20Towards%20Unbiased%20Responses%3A%20A%20Causality-Guided%20Debiasing%0A%20%20Framework&body=Title%3A%20Steering%20LLMs%20Towards%20Unbiased%20Responses%3A%20A%20Causality-Guided%20Debiasing%0A%20%20Framework%0AAuthor%3A%20Jingling%20Li%20and%20Zeyu%20Tang%20and%20Xiaoyu%20Liu%20and%20Peter%20Spirtes%20and%20Kun%20Zhang%20and%20Liu%20Leqi%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20easily%20generate%20biased%20and%20discriminative%0Aresponses.%20As%20LLMs%20tap%20into%20consequential%20decision-making%20%28e.g.%2C%20hiring%20and%0Ahealthcare%29%2C%20it%20is%20of%20crucial%20importance%20to%20develop%20strategies%20to%20mitigate%0Athese%20biases.%20This%20paper%20focuses%20on%20social%20bias%2C%20tackling%20the%20association%0Abetween%20demographic%20information%20and%20LLM%20outputs.%20We%20propose%20a%20causality-guided%0Adebiasing%20framework%20that%20utilizes%20causal%20understandings%20of%20%281%29%20the%0Adata-generating%20process%20of%20the%20training%20corpus%20fed%20to%20LLMs%2C%20and%20%282%29%20the%0Ainternal%20reasoning%20process%20of%20LLM%20inference%2C%20to%20guide%20the%20design%20of%20prompts%20for%0Adebiasing%20LLM%20outputs%20through%20selection%20mechanisms.%20Our%20framework%20unifies%0Aexisting%20de-biasing%20prompting%20approaches%20such%20as%20inhibitive%20instructions%20and%0Ain-context%20contrastive%20examples%2C%20and%20sheds%20light%20on%20new%20ways%20of%20debiasing%20by%0Aencouraging%20bias-free%20reasoning.%20Our%20strong%20empirical%20performance%20on%20real-world%0Adatasets%20demonstrates%20that%20our%20framework%20provides%20principled%20guidelines%20on%0Adebiasing%20LLM%20outputs%20even%20with%20only%20the%20black-box%20access.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08743v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20LLMs%20Towards%20Unbiased%20Responses%3A%20A%20Causality-Guided%20Debiasing%0A%20%20Framework&entry.906535625=Jingling%20Li%20and%20Zeyu%20Tang%20and%20Xiaoyu%20Liu%20and%20Peter%20Spirtes%20and%20Kun%20Zhang%20and%20Liu%20Leqi%20and%20Yang%20Liu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20easily%20generate%20biased%20and%20discriminative%0Aresponses.%20As%20LLMs%20tap%20into%20consequential%20decision-making%20%28e.g.%2C%20hiring%20and%0Ahealthcare%29%2C%20it%20is%20of%20crucial%20importance%20to%20develop%20strategies%20to%20mitigate%0Athese%20biases.%20This%20paper%20focuses%20on%20social%20bias%2C%20tackling%20the%20association%0Abetween%20demographic%20information%20and%20LLM%20outputs.%20We%20propose%20a%20causality-guided%0Adebiasing%20framework%20that%20utilizes%20causal%20understandings%20of%20%281%29%20the%0Adata-generating%20process%20of%20the%20training%20corpus%20fed%20to%20LLMs%2C%20and%20%282%29%20the%0Ainternal%20reasoning%20process%20of%20LLM%20inference%2C%20to%20guide%20the%20design%20of%20prompts%20for%0Adebiasing%20LLM%20outputs%20through%20selection%20mechanisms.%20Our%20framework%20unifies%0Aexisting%20de-biasing%20prompting%20approaches%20such%20as%20inhibitive%20instructions%20and%0Ain-context%20contrastive%20examples%2C%20and%20sheds%20light%20on%20new%20ways%20of%20debiasing%20by%0Aencouraging%20bias-free%20reasoning.%20Our%20strong%20empirical%20performance%20on%20real-world%0Adatasets%20demonstrates%20that%20our%20framework%20provides%20principled%20guidelines%20on%0Adebiasing%20LLM%20outputs%20even%20with%20only%20the%20black-box%20access.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08743v1&entry.124074799=Read"},
{"title": "Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records", "author": "Erlend Frayling and Jake Lever and Graham McDonald", "abstract": "  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n", "link": "http://arxiv.org/abs/2403.08664v1", "date": "2024-03-13", "relevancy": 1.3689, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4647}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4471}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4444}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20and%20Few-shot%20Generation%20Strategies%20for%20Artificial%20Clinical%0A%20%20Records&body=Title%3A%20Zero-shot%20and%20Few-shot%20Generation%20Strategies%20for%20Artificial%20Clinical%0A%20%20Records%0AAuthor%3A%20Erlend%20Frayling%20and%20Jake%20Lever%20and%20Graham%20McDonald%0AAbstract%3A%20%20%20The%20challenge%20of%20accessing%20historical%20patient%20data%20for%20clinical%20research%2C%0Awhile%20adhering%20to%20privacy%20regulations%2C%20is%20a%20significant%20obstacle%20in%20medical%0Ascience.%20An%20innovative%20approach%20to%20circumvent%20this%20issue%20involves%20utilising%0Asynthetic%20medical%20records%20that%20mirror%20real%20patient%20data%20without%20compromising%0Aindividual%20privacy.%20The%20creation%20of%20these%20synthetic%20datasets%2C%20particularly%0Awithout%20using%20actual%20patient%20data%20to%20train%20Large%20Language%20Models%20%28LLMs%29%2C%0Apresents%20a%20novel%20solution%20as%20gaining%20access%20to%20sensitive%20patient%20information%20to%0Atrain%20models%20is%20also%20a%20challenge.%20This%20study%20assesses%20the%20capability%20of%20the%0ALlama%202%20LLM%20to%20create%20synthetic%20medical%20records%20that%20accurately%20reflect%20real%0Apatient%20information%2C%20employing%20zero-shot%20and%20few-shot%20prompting%20strategies%20for%0Acomparison%20against%20fine-tuned%20methodologies%20that%20do%20require%20sensitive%20patient%0Adata%20during%20training.%20We%20focus%20on%20generating%20synthetic%20narratives%20for%20the%0AHistory%20of%20Present%20Illness%20section%2C%20utilising%20data%20from%20the%20MIMIC-IV%20dataset%0Afor%20comparison.%20In%20this%20work%20introduce%20a%20novel%20prompting%20technique%20that%0Aleverages%20a%20chain-of-thought%20approach%2C%20enhancing%20the%20model%27s%20ability%20to%0Agenerate%20more%20accurate%20and%20contextually%20relevant%20medical%20narratives%20without%0Aprior%20fine-tuning.%20Our%20findings%20suggest%20that%20this%20chain-of-thought%20prompted%0Aapproach%20allows%20the%20zero-shot%20model%20to%20achieve%20results%20on%20par%20with%20those%20of%0Afine-tuned%20models%2C%20based%20on%20Rouge%20metrics%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08664v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20and%20Few-shot%20Generation%20Strategies%20for%20Artificial%20Clinical%0A%20%20Records&entry.906535625=Erlend%20Frayling%20and%20Jake%20Lever%20and%20Graham%20McDonald&entry.1292438233=%20%20The%20challenge%20of%20accessing%20historical%20patient%20data%20for%20clinical%20research%2C%0Awhile%20adhering%20to%20privacy%20regulations%2C%20is%20a%20significant%20obstacle%20in%20medical%0Ascience.%20An%20innovative%20approach%20to%20circumvent%20this%20issue%20involves%20utilising%0Asynthetic%20medical%20records%20that%20mirror%20real%20patient%20data%20without%20compromising%0Aindividual%20privacy.%20The%20creation%20of%20these%20synthetic%20datasets%2C%20particularly%0Awithout%20using%20actual%20patient%20data%20to%20train%20Large%20Language%20Models%20%28LLMs%29%2C%0Apresents%20a%20novel%20solution%20as%20gaining%20access%20to%20sensitive%20patient%20information%20to%0Atrain%20models%20is%20also%20a%20challenge.%20This%20study%20assesses%20the%20capability%20of%20the%0ALlama%202%20LLM%20to%20create%20synthetic%20medical%20records%20that%20accurately%20reflect%20real%0Apatient%20information%2C%20employing%20zero-shot%20and%20few-shot%20prompting%20strategies%20for%0Acomparison%20against%20fine-tuned%20methodologies%20that%20do%20require%20sensitive%20patient%0Adata%20during%20training.%20We%20focus%20on%20generating%20synthetic%20narratives%20for%20the%0AHistory%20of%20Present%20Illness%20section%2C%20utilising%20data%20from%20the%20MIMIC-IV%20dataset%0Afor%20comparison.%20In%20this%20work%20introduce%20a%20novel%20prompting%20technique%20that%0Aleverages%20a%20chain-of-thought%20approach%2C%20enhancing%20the%20model%27s%20ability%20to%0Agenerate%20more%20accurate%20and%20contextually%20relevant%20medical%20narratives%20without%0Aprior%20fine-tuning.%20Our%20findings%20suggest%20that%20this%20chain-of-thought%20prompted%0Aapproach%20allows%20the%20zero-shot%20model%20to%20achieve%20results%20on%20par%20with%20those%20of%0Afine-tuned%20models%2C%20based%20on%20Rouge%20metrics%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08664v1&entry.124074799=Read"},
{"title": "Caformer: Rethinking Time Series Analysis from Causal Perspective", "author": "Kexuan Zhang and Xiaobei Zou and Yang Tang", "abstract": "  Time series analysis is a vital task with broad applications in various\ndomains. However, effectively capturing cross-dimension and cross-time\ndependencies in non-stationary time series poses significant challenges,\nparticularly in the context of environmental factors. The spurious correlation\ninduced by the environment confounds the causal relationships between\ncross-dimension and cross-time dependencies. In this paper, we introduce a\nnovel framework called Caformer (\\underline{\\textbf{Ca}}usal\nTrans\\underline{\\textbf{former}}) for time series analysis from a causal\nperspective. Specifically, our framework comprises three components: Dynamic\nLearner, Environment Learner, and Dependency Learner. The Dynamic Learner\nunveils dynamic interactions among dimensions, the Environment Learner\nmitigates spurious correlations caused by environment with a back-door\nadjustment, and the Dependency Learner aims to infer robust interactions across\nboth time and dimensions. Our Caformer demonstrates consistent state-of-the-art\nperformance across five mainstream time series analysis tasks, including long-\nand short-term forecasting, imputation, classification, and anomaly detection,\nwith proper interpretability.\n", "link": "http://arxiv.org/abs/2403.08572v1", "date": "2024-03-13", "relevancy": 1.2872, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4331}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4282}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4198}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Caformer%3A%20Rethinking%20Time%20Series%20Analysis%20from%20Causal%20Perspective&body=Title%3A%20Caformer%3A%20Rethinking%20Time%20Series%20Analysis%20from%20Causal%20Perspective%0AAuthor%3A%20Kexuan%20Zhang%20and%20Xiaobei%20Zou%20and%20Yang%20Tang%0AAbstract%3A%20%20%20Time%20series%20analysis%20is%20a%20vital%20task%20with%20broad%20applications%20in%20various%0Adomains.%20However%2C%20effectively%20capturing%20cross-dimension%20and%20cross-time%0Adependencies%20in%20non-stationary%20time%20series%20poses%20significant%20challenges%2C%0Aparticularly%20in%20the%20context%20of%20environmental%20factors.%20The%20spurious%20correlation%0Ainduced%20by%20the%20environment%20confounds%20the%20causal%20relationships%20between%0Across-dimension%20and%20cross-time%20dependencies.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20framework%20called%20Caformer%20%28%5Cunderline%7B%5Ctextbf%7BCa%7D%7Dusal%0ATrans%5Cunderline%7B%5Ctextbf%7Bformer%7D%7D%29%20for%20time%20series%20analysis%20from%20a%20causal%0Aperspective.%20Specifically%2C%20our%20framework%20comprises%20three%20components%3A%20Dynamic%0ALearner%2C%20Environment%20Learner%2C%20and%20Dependency%20Learner.%20The%20Dynamic%20Learner%0Aunveils%20dynamic%20interactions%20among%20dimensions%2C%20the%20Environment%20Learner%0Amitigates%20spurious%20correlations%20caused%20by%20environment%20with%20a%20back-door%0Aadjustment%2C%20and%20the%20Dependency%20Learner%20aims%20to%20infer%20robust%20interactions%20across%0Aboth%20time%20and%20dimensions.%20Our%20Caformer%20demonstrates%20consistent%20state-of-the-art%0Aperformance%20across%20five%20mainstream%20time%20series%20analysis%20tasks%2C%20including%20long-%0Aand%20short-term%20forecasting%2C%20imputation%2C%20classification%2C%20and%20anomaly%20detection%2C%0Awith%20proper%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08572v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Caformer%3A%20Rethinking%20Time%20Series%20Analysis%20from%20Causal%20Perspective&entry.906535625=Kexuan%20Zhang%20and%20Xiaobei%20Zou%20and%20Yang%20Tang&entry.1292438233=%20%20Time%20series%20analysis%20is%20a%20vital%20task%20with%20broad%20applications%20in%20various%0Adomains.%20However%2C%20effectively%20capturing%20cross-dimension%20and%20cross-time%0Adependencies%20in%20non-stationary%20time%20series%20poses%20significant%20challenges%2C%0Aparticularly%20in%20the%20context%20of%20environmental%20factors.%20The%20spurious%20correlation%0Ainduced%20by%20the%20environment%20confounds%20the%20causal%20relationships%20between%0Across-dimension%20and%20cross-time%20dependencies.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20framework%20called%20Caformer%20%28%5Cunderline%7B%5Ctextbf%7BCa%7D%7Dusal%0ATrans%5Cunderline%7B%5Ctextbf%7Bformer%7D%7D%29%20for%20time%20series%20analysis%20from%20a%20causal%0Aperspective.%20Specifically%2C%20our%20framework%20comprises%20three%20components%3A%20Dynamic%0ALearner%2C%20Environment%20Learner%2C%20and%20Dependency%20Learner.%20The%20Dynamic%20Learner%0Aunveils%20dynamic%20interactions%20among%20dimensions%2C%20the%20Environment%20Learner%0Amitigates%20spurious%20correlations%20caused%20by%20environment%20with%20a%20back-door%0Aadjustment%2C%20and%20the%20Dependency%20Learner%20aims%20to%20infer%20robust%20interactions%20across%0Aboth%20time%20and%20dimensions.%20Our%20Caformer%20demonstrates%20consistent%20state-of-the-art%0Aperformance%20across%20five%20mainstream%20time%20series%20analysis%20tasks%2C%20including%20long-%0Aand%20short-term%20forecasting%2C%20imputation%2C%20classification%2C%20and%20anomaly%20detection%2C%0Awith%20proper%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08572v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


