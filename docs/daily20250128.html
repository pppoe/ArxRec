<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250127.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "3DGS$^2$: Near Second-order Converging 3D Gaussian Splatting", "author": "Lei Lan and Tianjia Shao and Zixuan Lu and Yu Zhang and Chenfanfu Jiang and Yin Yang", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a mainstream solution for novel\nview synthesis and 3D reconstruction. By explicitly encoding a 3D scene using a\ncollection of Gaussian kernels, 3DGS achieves high-quality rendering with\nsuperior efficiency. As a learning-based approach, 3DGS training has been dealt\nwith the standard stochastic gradient descent (SGD) method, which offers at\nmost linear convergence. Consequently, training often requires tens of minutes,\neven with GPU acceleration. This paper introduces a (near) second-order\nconvergent training algorithm for 3DGS, leveraging its unique properties. Our\napproach is inspired by two key observations. First, the attributes of a\nGaussian kernel contribute independently to the image-space loss, which\nendorses isolated and local optimization algorithms. We exploit this by\nsplitting the optimization at the level of individual kernel attributes,\nanalytically constructing small-size Newton systems for each parameter group,\nand efficiently solving these systems on GPU threads. This achieves Newton-like\nconvergence per training image without relying on the global Hessian. Second,\nkernels exhibit sparse and structured coupling across input images. This\nproperty allows us to effectively utilize spatial information to mitigate\novershoot during stochastic training. Our method converges an order faster than\nstandard GPU-based 3DGS training, requiring over $10\\times$ fewer iterations\nwhile maintaining or surpassing the quality of the compared with the SGD-based\n3DGS reconstructions.\n", "link": "http://arxiv.org/abs/2501.13975v2", "date": "2025-01-27", "relevancy": 3.4542, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.717}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6941}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGS%24%5E2%24%3A%20Near%20Second-order%20Converging%203D%20Gaussian%20Splatting&body=Title%3A%203DGS%24%5E2%24%3A%20Near%20Second-order%20Converging%203D%20Gaussian%20Splatting%0AAuthor%3A%20Lei%20Lan%20and%20Tianjia%20Shao%20and%20Zixuan%20Lu%20and%20Yu%20Zhang%20and%20Chenfanfu%20Jiang%20and%20Yin%20Yang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20mainstream%20solution%20for%20novel%0Aview%20synthesis%20and%203D%20reconstruction.%20By%20explicitly%20encoding%20a%203D%20scene%20using%20a%0Acollection%20of%20Gaussian%20kernels%2C%203DGS%20achieves%20high-quality%20rendering%20with%0Asuperior%20efficiency.%20As%20a%20learning-based%20approach%2C%203DGS%20training%20has%20been%20dealt%0Awith%20the%20standard%20stochastic%20gradient%20descent%20%28SGD%29%20method%2C%20which%20offers%20at%0Amost%20linear%20convergence.%20Consequently%2C%20training%20often%20requires%20tens%20of%20minutes%2C%0Aeven%20with%20GPU%20acceleration.%20This%20paper%20introduces%20a%20%28near%29%20second-order%0Aconvergent%20training%20algorithm%20for%203DGS%2C%20leveraging%20its%20unique%20properties.%20Our%0Aapproach%20is%20inspired%20by%20two%20key%20observations.%20First%2C%20the%20attributes%20of%20a%0AGaussian%20kernel%20contribute%20independently%20to%20the%20image-space%20loss%2C%20which%0Aendorses%20isolated%20and%20local%20optimization%20algorithms.%20We%20exploit%20this%20by%0Asplitting%20the%20optimization%20at%20the%20level%20of%20individual%20kernel%20attributes%2C%0Aanalytically%20constructing%20small-size%20Newton%20systems%20for%20each%20parameter%20group%2C%0Aand%20efficiently%20solving%20these%20systems%20on%20GPU%20threads.%20This%20achieves%20Newton-like%0Aconvergence%20per%20training%20image%20without%20relying%20on%20the%20global%20Hessian.%20Second%2C%0Akernels%20exhibit%20sparse%20and%20structured%20coupling%20across%20input%20images.%20This%0Aproperty%20allows%20us%20to%20effectively%20utilize%20spatial%20information%20to%20mitigate%0Aovershoot%20during%20stochastic%20training.%20Our%20method%20converges%20an%20order%20faster%20than%0Astandard%20GPU-based%203DGS%20training%2C%20requiring%20over%20%2410%5Ctimes%24%20fewer%20iterations%0Awhile%20maintaining%20or%20surpassing%20the%20quality%20of%20the%20compared%20with%20the%20SGD-based%0A3DGS%20reconstructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13975v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGS%2524%255E2%2524%253A%2520Near%2520Second-order%2520Converging%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DLei%2520Lan%2520and%2520Tianjia%2520Shao%2520and%2520Zixuan%2520Lu%2520and%2520Yu%2520Zhang%2520and%2520Chenfanfu%2520Jiang%2520and%2520Yin%2520Yang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520mainstream%2520solution%2520for%2520novel%250Aview%2520synthesis%2520and%25203D%2520reconstruction.%2520By%2520explicitly%2520encoding%2520a%25203D%2520scene%2520using%2520a%250Acollection%2520of%2520Gaussian%2520kernels%252C%25203DGS%2520achieves%2520high-quality%2520rendering%2520with%250Asuperior%2520efficiency.%2520As%2520a%2520learning-based%2520approach%252C%25203DGS%2520training%2520has%2520been%2520dealt%250Awith%2520the%2520standard%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520method%252C%2520which%2520offers%2520at%250Amost%2520linear%2520convergence.%2520Consequently%252C%2520training%2520often%2520requires%2520tens%2520of%2520minutes%252C%250Aeven%2520with%2520GPU%2520acceleration.%2520This%2520paper%2520introduces%2520a%2520%2528near%2529%2520second-order%250Aconvergent%2520training%2520algorithm%2520for%25203DGS%252C%2520leveraging%2520its%2520unique%2520properties.%2520Our%250Aapproach%2520is%2520inspired%2520by%2520two%2520key%2520observations.%2520First%252C%2520the%2520attributes%2520of%2520a%250AGaussian%2520kernel%2520contribute%2520independently%2520to%2520the%2520image-space%2520loss%252C%2520which%250Aendorses%2520isolated%2520and%2520local%2520optimization%2520algorithms.%2520We%2520exploit%2520this%2520by%250Asplitting%2520the%2520optimization%2520at%2520the%2520level%2520of%2520individual%2520kernel%2520attributes%252C%250Aanalytically%2520constructing%2520small-size%2520Newton%2520systems%2520for%2520each%2520parameter%2520group%252C%250Aand%2520efficiently%2520solving%2520these%2520systems%2520on%2520GPU%2520threads.%2520This%2520achieves%2520Newton-like%250Aconvergence%2520per%2520training%2520image%2520without%2520relying%2520on%2520the%2520global%2520Hessian.%2520Second%252C%250Akernels%2520exhibit%2520sparse%2520and%2520structured%2520coupling%2520across%2520input%2520images.%2520This%250Aproperty%2520allows%2520us%2520to%2520effectively%2520utilize%2520spatial%2520information%2520to%2520mitigate%250Aovershoot%2520during%2520stochastic%2520training.%2520Our%2520method%2520converges%2520an%2520order%2520faster%2520than%250Astandard%2520GPU-based%25203DGS%2520training%252C%2520requiring%2520over%2520%252410%255Ctimes%2524%2520fewer%2520iterations%250Awhile%2520maintaining%2520or%2520surpassing%2520the%2520quality%2520of%2520the%2520compared%2520with%2520the%2520SGD-based%250A3DGS%2520reconstructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13975v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGS%24%5E2%24%3A%20Near%20Second-order%20Converging%203D%20Gaussian%20Splatting&entry.906535625=Lei%20Lan%20and%20Tianjia%20Shao%20and%20Zixuan%20Lu%20and%20Yu%20Zhang%20and%20Chenfanfu%20Jiang%20and%20Yin%20Yang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20mainstream%20solution%20for%20novel%0Aview%20synthesis%20and%203D%20reconstruction.%20By%20explicitly%20encoding%20a%203D%20scene%20using%20a%0Acollection%20of%20Gaussian%20kernels%2C%203DGS%20achieves%20high-quality%20rendering%20with%0Asuperior%20efficiency.%20As%20a%20learning-based%20approach%2C%203DGS%20training%20has%20been%20dealt%0Awith%20the%20standard%20stochastic%20gradient%20descent%20%28SGD%29%20method%2C%20which%20offers%20at%0Amost%20linear%20convergence.%20Consequently%2C%20training%20often%20requires%20tens%20of%20minutes%2C%0Aeven%20with%20GPU%20acceleration.%20This%20paper%20introduces%20a%20%28near%29%20second-order%0Aconvergent%20training%20algorithm%20for%203DGS%2C%20leveraging%20its%20unique%20properties.%20Our%0Aapproach%20is%20inspired%20by%20two%20key%20observations.%20First%2C%20the%20attributes%20of%20a%0AGaussian%20kernel%20contribute%20independently%20to%20the%20image-space%20loss%2C%20which%0Aendorses%20isolated%20and%20local%20optimization%20algorithms.%20We%20exploit%20this%20by%0Asplitting%20the%20optimization%20at%20the%20level%20of%20individual%20kernel%20attributes%2C%0Aanalytically%20constructing%20small-size%20Newton%20systems%20for%20each%20parameter%20group%2C%0Aand%20efficiently%20solving%20these%20systems%20on%20GPU%20threads.%20This%20achieves%20Newton-like%0Aconvergence%20per%20training%20image%20without%20relying%20on%20the%20global%20Hessian.%20Second%2C%0Akernels%20exhibit%20sparse%20and%20structured%20coupling%20across%20input%20images.%20This%0Aproperty%20allows%20us%20to%20effectively%20utilize%20spatial%20information%20to%20mitigate%0Aovershoot%20during%20stochastic%20training.%20Our%20method%20converges%20an%20order%20faster%20than%0Astandard%20GPU-based%203DGS%20training%2C%20requiring%20over%20%2410%5Ctimes%24%20fewer%20iterations%0Awhile%20maintaining%20or%20surpassing%20the%20quality%20of%20the%20compared%20with%20the%20SGD-based%0A3DGS%20reconstructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13975v2&entry.124074799=Read"},
{"title": "PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for\n  View-Adaptive Rendering", "author": "Junxi Jin and Xiulai Li and Haiping Huang and Lianjun Liu and Yujie Sun and Boyi Liu", "abstract": "  Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in\nreal-time, high-quality 3D scene rendering. However, it faces several\nchallenges, including Gaussian redundancy, limited ability to capture\nview-dependent effects, and difficulties in handling complex lighting and\nspecular reflections. Additionally, methods that use spherical harmonics for\ncolor representation often struggle to effectively capture specular highlights\nand anisotropic components, especially when modeling view-dependent colors\nunder complex lighting conditions, leading to insufficient contrast and\nunnatural color saturation. To address these limitations, we introduce PEP-GS,\na perceptually-enhanced framework that dynamically predicts Gaussian\nattributes, including opacity, color, and covariance. We replace traditional\nspherical harmonics with a Hierarchical Granular-Structural Attention\nmechanism, which enables more accurate modeling of complex view-dependent color\neffects and specular highlights. By employing a stable and interpretable\nframework for opacity and covariance estimation, PEP-GS avoids the removal of\nessential Gaussians prematurely, ensuring a more accurate scene representation.\nFurthermore, perceptual optimization is applied to the final rendered images,\nenhancing perceptual consistency across different views and ensuring\nhigh-quality renderings with improved texture fidelity and fine-scale detail\npreservation. Experimental results demonstrate that PEP-GS outperforms\nstate-of-the-art methods, particularly in challenging scenarios involving\nview-dependent effects, specular reflections, and fine-scale details.\n", "link": "http://arxiv.org/abs/2411.05731v2", "date": "2025-01-27", "relevancy": 3.2909, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6768}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.651}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEP-GS%3A%20Perceptually-Enhanced%20Precise%20Structured%203D%20Gaussians%20for%0A%20%20View-Adaptive%20Rendering&body=Title%3A%20PEP-GS%3A%20Perceptually-Enhanced%20Precise%20Structured%203D%20Gaussians%20for%0A%20%20View-Adaptive%20Rendering%0AAuthor%3A%20Junxi%20Jin%20and%20Xiulai%20Li%20and%20Haiping%20Huang%20and%20Lianjun%20Liu%20and%20Yujie%20Sun%20and%20Boyi%20Liu%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%20achieved%20significant%20success%20in%0Areal-time%2C%20high-quality%203D%20scene%20rendering.%20However%2C%20it%20faces%20several%0Achallenges%2C%20including%20Gaussian%20redundancy%2C%20limited%20ability%20to%20capture%0Aview-dependent%20effects%2C%20and%20difficulties%20in%20handling%20complex%20lighting%20and%0Aspecular%20reflections.%20Additionally%2C%20methods%20that%20use%20spherical%20harmonics%20for%0Acolor%20representation%20often%20struggle%20to%20effectively%20capture%20specular%20highlights%0Aand%20anisotropic%20components%2C%20especially%20when%20modeling%20view-dependent%20colors%0Aunder%20complex%20lighting%20conditions%2C%20leading%20to%20insufficient%20contrast%20and%0Aunnatural%20color%20saturation.%20To%20address%20these%20limitations%2C%20we%20introduce%20PEP-GS%2C%0Aa%20perceptually-enhanced%20framework%20that%20dynamically%20predicts%20Gaussian%0Aattributes%2C%20including%20opacity%2C%20color%2C%20and%20covariance.%20We%20replace%20traditional%0Aspherical%20harmonics%20with%20a%20Hierarchical%20Granular-Structural%20Attention%0Amechanism%2C%20which%20enables%20more%20accurate%20modeling%20of%20complex%20view-dependent%20color%0Aeffects%20and%20specular%20highlights.%20By%20employing%20a%20stable%20and%20interpretable%0Aframework%20for%20opacity%20and%20covariance%20estimation%2C%20PEP-GS%20avoids%20the%20removal%20of%0Aessential%20Gaussians%20prematurely%2C%20ensuring%20a%20more%20accurate%20scene%20representation.%0AFurthermore%2C%20perceptual%20optimization%20is%20applied%20to%20the%20final%20rendered%20images%2C%0Aenhancing%20perceptual%20consistency%20across%20different%20views%20and%20ensuring%0Ahigh-quality%20renderings%20with%20improved%20texture%20fidelity%20and%20fine-scale%20detail%0Apreservation.%20Experimental%20results%20demonstrate%20that%20PEP-GS%20outperforms%0Astate-of-the-art%20methods%2C%20particularly%20in%20challenging%20scenarios%20involving%0Aview-dependent%20effects%2C%20specular%20reflections%2C%20and%20fine-scale%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05731v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEP-GS%253A%2520Perceptually-Enhanced%2520Precise%2520Structured%25203D%2520Gaussians%2520for%250A%2520%2520View-Adaptive%2520Rendering%26entry.906535625%3DJunxi%2520Jin%2520and%2520Xiulai%2520Li%2520and%2520Haiping%2520Huang%2520and%2520Lianjun%2520Liu%2520and%2520Yujie%2520Sun%2520and%2520Boyi%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520has%2520achieved%2520significant%2520success%2520in%250Areal-time%252C%2520high-quality%25203D%2520scene%2520rendering.%2520However%252C%2520it%2520faces%2520several%250Achallenges%252C%2520including%2520Gaussian%2520redundancy%252C%2520limited%2520ability%2520to%2520capture%250Aview-dependent%2520effects%252C%2520and%2520difficulties%2520in%2520handling%2520complex%2520lighting%2520and%250Aspecular%2520reflections.%2520Additionally%252C%2520methods%2520that%2520use%2520spherical%2520harmonics%2520for%250Acolor%2520representation%2520often%2520struggle%2520to%2520effectively%2520capture%2520specular%2520highlights%250Aand%2520anisotropic%2520components%252C%2520especially%2520when%2520modeling%2520view-dependent%2520colors%250Aunder%2520complex%2520lighting%2520conditions%252C%2520leading%2520to%2520insufficient%2520contrast%2520and%250Aunnatural%2520color%2520saturation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520PEP-GS%252C%250Aa%2520perceptually-enhanced%2520framework%2520that%2520dynamically%2520predicts%2520Gaussian%250Aattributes%252C%2520including%2520opacity%252C%2520color%252C%2520and%2520covariance.%2520We%2520replace%2520traditional%250Aspherical%2520harmonics%2520with%2520a%2520Hierarchical%2520Granular-Structural%2520Attention%250Amechanism%252C%2520which%2520enables%2520more%2520accurate%2520modeling%2520of%2520complex%2520view-dependent%2520color%250Aeffects%2520and%2520specular%2520highlights.%2520By%2520employing%2520a%2520stable%2520and%2520interpretable%250Aframework%2520for%2520opacity%2520and%2520covariance%2520estimation%252C%2520PEP-GS%2520avoids%2520the%2520removal%2520of%250Aessential%2520Gaussians%2520prematurely%252C%2520ensuring%2520a%2520more%2520accurate%2520scene%2520representation.%250AFurthermore%252C%2520perceptual%2520optimization%2520is%2520applied%2520to%2520the%2520final%2520rendered%2520images%252C%250Aenhancing%2520perceptual%2520consistency%2520across%2520different%2520views%2520and%2520ensuring%250Ahigh-quality%2520renderings%2520with%2520improved%2520texture%2520fidelity%2520and%2520fine-scale%2520detail%250Apreservation.%2520Experimental%2520results%2520demonstrate%2520that%2520PEP-GS%2520outperforms%250Astate-of-the-art%2520methods%252C%2520particularly%2520in%2520challenging%2520scenarios%2520involving%250Aview-dependent%2520effects%252C%2520specular%2520reflections%252C%2520and%2520fine-scale%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05731v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEP-GS%3A%20Perceptually-Enhanced%20Precise%20Structured%203D%20Gaussians%20for%0A%20%20View-Adaptive%20Rendering&entry.906535625=Junxi%20Jin%20and%20Xiulai%20Li%20and%20Haiping%20Huang%20and%20Lianjun%20Liu%20and%20Yujie%20Sun%20and%20Boyi%20Liu&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%20achieved%20significant%20success%20in%0Areal-time%2C%20high-quality%203D%20scene%20rendering.%20However%2C%20it%20faces%20several%0Achallenges%2C%20including%20Gaussian%20redundancy%2C%20limited%20ability%20to%20capture%0Aview-dependent%20effects%2C%20and%20difficulties%20in%20handling%20complex%20lighting%20and%0Aspecular%20reflections.%20Additionally%2C%20methods%20that%20use%20spherical%20harmonics%20for%0Acolor%20representation%20often%20struggle%20to%20effectively%20capture%20specular%20highlights%0Aand%20anisotropic%20components%2C%20especially%20when%20modeling%20view-dependent%20colors%0Aunder%20complex%20lighting%20conditions%2C%20leading%20to%20insufficient%20contrast%20and%0Aunnatural%20color%20saturation.%20To%20address%20these%20limitations%2C%20we%20introduce%20PEP-GS%2C%0Aa%20perceptually-enhanced%20framework%20that%20dynamically%20predicts%20Gaussian%0Aattributes%2C%20including%20opacity%2C%20color%2C%20and%20covariance.%20We%20replace%20traditional%0Aspherical%20harmonics%20with%20a%20Hierarchical%20Granular-Structural%20Attention%0Amechanism%2C%20which%20enables%20more%20accurate%20modeling%20of%20complex%20view-dependent%20color%0Aeffects%20and%20specular%20highlights.%20By%20employing%20a%20stable%20and%20interpretable%0Aframework%20for%20opacity%20and%20covariance%20estimation%2C%20PEP-GS%20avoids%20the%20removal%20of%0Aessential%20Gaussians%20prematurely%2C%20ensuring%20a%20more%20accurate%20scene%20representation.%0AFurthermore%2C%20perceptual%20optimization%20is%20applied%20to%20the%20final%20rendered%20images%2C%0Aenhancing%20perceptual%20consistency%20across%20different%20views%20and%20ensuring%0Ahigh-quality%20renderings%20with%20improved%20texture%20fidelity%20and%20fine-scale%20detail%0Apreservation.%20Experimental%20results%20demonstrate%20that%20PEP-GS%20outperforms%0Astate-of-the-art%20methods%2C%20particularly%20in%20challenging%20scenarios%20involving%0Aview-dependent%20effects%2C%20specular%20reflections%2C%20and%20fine-scale%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05731v2&entry.124074799=Read"},
{"title": "BAG: Body-Aligned 3D Wearable Asset Generation", "author": "Zhongjin Luo and Yang Li and Mingrui Zhang and Senbo Wang and Han Yan and Xibin Song and Taizhang Shang and Wei Mao and Hongdong Li and Xiaoguang Han and Pan Ji", "abstract": "  While recent advancements have shown remarkable progress in general 3D shape\ngeneration models, the challenge of leveraging these approaches to\nautomatically generate wearable 3D assets remains unexplored. To this end, we\npresent BAG, a Body-aligned Asset Generation method to output 3D wearable asset\nthat can be automatically dressed on given 3D human bodies. This is achived by\ncontrolling the 3D generation process using human body shape and pose\ninformation. Specifically, we first build a general single-image to consistent\nmultiview image diffusion model, and train it on the large Objaverse dataset to\nachieve diversity and generalizability. Then we train a Controlnet to guide the\nmultiview generator to produce body-aligned multiview images. The control\nsignal utilizes the multiview 2D projections of the target human body, where\npixel values represent the XYZ coordinates of the body surface in a canonical\nspace. The body-conditioned multiview diffusion generates body-aligned\nmultiview images, which are then fed into a native 3D diffusion model to\nproduce the 3D shape of the asset. Finally, by recovering the similarity\ntransformation using multiview silhouette supervision and addressing asset-body\npenetration with physics simulators, the 3D asset can be accurately fitted onto\nthe target human body. Experimental results demonstrate significant advantages\nover existing methods in terms of image prompt-following capability, shape\ndiversity, and shape quality. Our project page is available at\nhttps://bag-3d.github.io/.\n", "link": "http://arxiv.org/abs/2501.16177v1", "date": "2025-01-27", "relevancy": 3.2801, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6753}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6464}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BAG%3A%20Body-Aligned%203D%20Wearable%20Asset%20Generation&body=Title%3A%20BAG%3A%20Body-Aligned%203D%20Wearable%20Asset%20Generation%0AAuthor%3A%20Zhongjin%20Luo%20and%20Yang%20Li%20and%20Mingrui%20Zhang%20and%20Senbo%20Wang%20and%20Han%20Yan%20and%20Xibin%20Song%20and%20Taizhang%20Shang%20and%20Wei%20Mao%20and%20Hongdong%20Li%20and%20Xiaoguang%20Han%20and%20Pan%20Ji%0AAbstract%3A%20%20%20While%20recent%20advancements%20have%20shown%20remarkable%20progress%20in%20general%203D%20shape%0Ageneration%20models%2C%20the%20challenge%20of%20leveraging%20these%20approaches%20to%0Aautomatically%20generate%20wearable%203D%20assets%20remains%20unexplored.%20To%20this%20end%2C%20we%0Apresent%20BAG%2C%20a%20Body-aligned%20Asset%20Generation%20method%20to%20output%203D%20wearable%20asset%0Athat%20can%20be%20automatically%20dressed%20on%20given%203D%20human%20bodies.%20This%20is%20achived%20by%0Acontrolling%20the%203D%20generation%20process%20using%20human%20body%20shape%20and%20pose%0Ainformation.%20Specifically%2C%20we%20first%20build%20a%20general%20single-image%20to%20consistent%0Amultiview%20image%20diffusion%20model%2C%20and%20train%20it%20on%20the%20large%20Objaverse%20dataset%20to%0Aachieve%20diversity%20and%20generalizability.%20Then%20we%20train%20a%20Controlnet%20to%20guide%20the%0Amultiview%20generator%20to%20produce%20body-aligned%20multiview%20images.%20The%20control%0Asignal%20utilizes%20the%20multiview%202D%20projections%20of%20the%20target%20human%20body%2C%20where%0Apixel%20values%20represent%20the%20XYZ%20coordinates%20of%20the%20body%20surface%20in%20a%20canonical%0Aspace.%20The%20body-conditioned%20multiview%20diffusion%20generates%20body-aligned%0Amultiview%20images%2C%20which%20are%20then%20fed%20into%20a%20native%203D%20diffusion%20model%20to%0Aproduce%20the%203D%20shape%20of%20the%20asset.%20Finally%2C%20by%20recovering%20the%20similarity%0Atransformation%20using%20multiview%20silhouette%20supervision%20and%20addressing%20asset-body%0Apenetration%20with%20physics%20simulators%2C%20the%203D%20asset%20can%20be%20accurately%20fitted%20onto%0Athe%20target%20human%20body.%20Experimental%20results%20demonstrate%20significant%20advantages%0Aover%20existing%20methods%20in%20terms%20of%20image%20prompt-following%20capability%2C%20shape%0Adiversity%2C%20and%20shape%20quality.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//bag-3d.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBAG%253A%2520Body-Aligned%25203D%2520Wearable%2520Asset%2520Generation%26entry.906535625%3DZhongjin%2520Luo%2520and%2520Yang%2520Li%2520and%2520Mingrui%2520Zhang%2520and%2520Senbo%2520Wang%2520and%2520Han%2520Yan%2520and%2520Xibin%2520Song%2520and%2520Taizhang%2520Shang%2520and%2520Wei%2520Mao%2520and%2520Hongdong%2520Li%2520and%2520Xiaoguang%2520Han%2520and%2520Pan%2520Ji%26entry.1292438233%3D%2520%2520While%2520recent%2520advancements%2520have%2520shown%2520remarkable%2520progress%2520in%2520general%25203D%2520shape%250Ageneration%2520models%252C%2520the%2520challenge%2520of%2520leveraging%2520these%2520approaches%2520to%250Aautomatically%2520generate%2520wearable%25203D%2520assets%2520remains%2520unexplored.%2520To%2520this%2520end%252C%2520we%250Apresent%2520BAG%252C%2520a%2520Body-aligned%2520Asset%2520Generation%2520method%2520to%2520output%25203D%2520wearable%2520asset%250Athat%2520can%2520be%2520automatically%2520dressed%2520on%2520given%25203D%2520human%2520bodies.%2520This%2520is%2520achived%2520by%250Acontrolling%2520the%25203D%2520generation%2520process%2520using%2520human%2520body%2520shape%2520and%2520pose%250Ainformation.%2520Specifically%252C%2520we%2520first%2520build%2520a%2520general%2520single-image%2520to%2520consistent%250Amultiview%2520image%2520diffusion%2520model%252C%2520and%2520train%2520it%2520on%2520the%2520large%2520Objaverse%2520dataset%2520to%250Aachieve%2520diversity%2520and%2520generalizability.%2520Then%2520we%2520train%2520a%2520Controlnet%2520to%2520guide%2520the%250Amultiview%2520generator%2520to%2520produce%2520body-aligned%2520multiview%2520images.%2520The%2520control%250Asignal%2520utilizes%2520the%2520multiview%25202D%2520projections%2520of%2520the%2520target%2520human%2520body%252C%2520where%250Apixel%2520values%2520represent%2520the%2520XYZ%2520coordinates%2520of%2520the%2520body%2520surface%2520in%2520a%2520canonical%250Aspace.%2520The%2520body-conditioned%2520multiview%2520diffusion%2520generates%2520body-aligned%250Amultiview%2520images%252C%2520which%2520are%2520then%2520fed%2520into%2520a%2520native%25203D%2520diffusion%2520model%2520to%250Aproduce%2520the%25203D%2520shape%2520of%2520the%2520asset.%2520Finally%252C%2520by%2520recovering%2520the%2520similarity%250Atransformation%2520using%2520multiview%2520silhouette%2520supervision%2520and%2520addressing%2520asset-body%250Apenetration%2520with%2520physics%2520simulators%252C%2520the%25203D%2520asset%2520can%2520be%2520accurately%2520fitted%2520onto%250Athe%2520target%2520human%2520body.%2520Experimental%2520results%2520demonstrate%2520significant%2520advantages%250Aover%2520existing%2520methods%2520in%2520terms%2520of%2520image%2520prompt-following%2520capability%252C%2520shape%250Adiversity%252C%2520and%2520shape%2520quality.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//bag-3d.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAG%3A%20Body-Aligned%203D%20Wearable%20Asset%20Generation&entry.906535625=Zhongjin%20Luo%20and%20Yang%20Li%20and%20Mingrui%20Zhang%20and%20Senbo%20Wang%20and%20Han%20Yan%20and%20Xibin%20Song%20and%20Taizhang%20Shang%20and%20Wei%20Mao%20and%20Hongdong%20Li%20and%20Xiaoguang%20Han%20and%20Pan%20Ji&entry.1292438233=%20%20While%20recent%20advancements%20have%20shown%20remarkable%20progress%20in%20general%203D%20shape%0Ageneration%20models%2C%20the%20challenge%20of%20leveraging%20these%20approaches%20to%0Aautomatically%20generate%20wearable%203D%20assets%20remains%20unexplored.%20To%20this%20end%2C%20we%0Apresent%20BAG%2C%20a%20Body-aligned%20Asset%20Generation%20method%20to%20output%203D%20wearable%20asset%0Athat%20can%20be%20automatically%20dressed%20on%20given%203D%20human%20bodies.%20This%20is%20achived%20by%0Acontrolling%20the%203D%20generation%20process%20using%20human%20body%20shape%20and%20pose%0Ainformation.%20Specifically%2C%20we%20first%20build%20a%20general%20single-image%20to%20consistent%0Amultiview%20image%20diffusion%20model%2C%20and%20train%20it%20on%20the%20large%20Objaverse%20dataset%20to%0Aachieve%20diversity%20and%20generalizability.%20Then%20we%20train%20a%20Controlnet%20to%20guide%20the%0Amultiview%20generator%20to%20produce%20body-aligned%20multiview%20images.%20The%20control%0Asignal%20utilizes%20the%20multiview%202D%20projections%20of%20the%20target%20human%20body%2C%20where%0Apixel%20values%20represent%20the%20XYZ%20coordinates%20of%20the%20body%20surface%20in%20a%20canonical%0Aspace.%20The%20body-conditioned%20multiview%20diffusion%20generates%20body-aligned%0Amultiview%20images%2C%20which%20are%20then%20fed%20into%20a%20native%203D%20diffusion%20model%20to%0Aproduce%20the%203D%20shape%20of%20the%20asset.%20Finally%2C%20by%20recovering%20the%20similarity%0Atransformation%20using%20multiview%20silhouette%20supervision%20and%20addressing%20asset-body%0Apenetration%20with%20physics%20simulators%2C%20the%203D%20asset%20can%20be%20accurately%20fitted%20onto%0Athe%20target%20human%20body.%20Experimental%20results%20demonstrate%20significant%20advantages%0Aover%20existing%20methods%20in%20terms%20of%20image%20prompt-following%20capability%2C%20shape%0Adiversity%2C%20and%20shape%20quality.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//bag-3d.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16177v1&entry.124074799=Read"},
{"title": "3D Reconstruction of non-visible surfaces of objects from a Single Depth\n  View -- Comparative Study", "author": "Rafa\u0142 Staszak and Piotr Micha\u0142ek and Jakub Chudzi\u0144ski and Marek Kopicki and Dominik Belter", "abstract": "  Scene and object reconstruction is an important problem in robotics, in\nparticular in planning collision-free trajectories or in object manipulation.\nThis paper compares two strategies for the reconstruction of nonvisible parts\nof the object surface from a single RGB-D camera view. The first method, named\nDeepSDF predicts the Signed Distance Transform to the object surface for a\ngiven point in 3D space. The second method, named MirrorNet reconstructs the\noccluded objects' parts by generating images from the other side of the\nobserved object. Experiments performed with objects from the ShapeNet dataset,\nshow that the view-dependent MirrorNet is faster and has smaller reconstruction\nerrors in most categories.\n", "link": "http://arxiv.org/abs/2501.16101v1", "date": "2025-01-27", "relevancy": 3.0391, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6152}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6152}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Reconstruction%20of%20non-visible%20surfaces%20of%20objects%20from%20a%20Single%20Depth%0A%20%20View%20--%20Comparative%20Study&body=Title%3A%203D%20Reconstruction%20of%20non-visible%20surfaces%20of%20objects%20from%20a%20Single%20Depth%0A%20%20View%20--%20Comparative%20Study%0AAuthor%3A%20Rafa%C5%82%20Staszak%20and%20Piotr%20Micha%C5%82ek%20and%20Jakub%20Chudzi%C5%84ski%20and%20Marek%20Kopicki%20and%20Dominik%20Belter%0AAbstract%3A%20%20%20Scene%20and%20object%20reconstruction%20is%20an%20important%20problem%20in%20robotics%2C%20in%0Aparticular%20in%20planning%20collision-free%20trajectories%20or%20in%20object%20manipulation.%0AThis%20paper%20compares%20two%20strategies%20for%20the%20reconstruction%20of%20nonvisible%20parts%0Aof%20the%20object%20surface%20from%20a%20single%20RGB-D%20camera%20view.%20The%20first%20method%2C%20named%0ADeepSDF%20predicts%20the%20Signed%20Distance%20Transform%20to%20the%20object%20surface%20for%20a%0Agiven%20point%20in%203D%20space.%20The%20second%20method%2C%20named%20MirrorNet%20reconstructs%20the%0Aoccluded%20objects%27%20parts%20by%20generating%20images%20from%20the%20other%20side%20of%20the%0Aobserved%20object.%20Experiments%20performed%20with%20objects%20from%20the%20ShapeNet%20dataset%2C%0Ashow%20that%20the%20view-dependent%20MirrorNet%20is%20faster%20and%20has%20smaller%20reconstruction%0Aerrors%20in%20most%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Reconstruction%2520of%2520non-visible%2520surfaces%2520of%2520objects%2520from%2520a%2520Single%2520Depth%250A%2520%2520View%2520--%2520Comparative%2520Study%26entry.906535625%3DRafa%25C5%2582%2520Staszak%2520and%2520Piotr%2520Micha%25C5%2582ek%2520and%2520Jakub%2520Chudzi%25C5%2584ski%2520and%2520Marek%2520Kopicki%2520and%2520Dominik%2520Belter%26entry.1292438233%3D%2520%2520Scene%2520and%2520object%2520reconstruction%2520is%2520an%2520important%2520problem%2520in%2520robotics%252C%2520in%250Aparticular%2520in%2520planning%2520collision-free%2520trajectories%2520or%2520in%2520object%2520manipulation.%250AThis%2520paper%2520compares%2520two%2520strategies%2520for%2520the%2520reconstruction%2520of%2520nonvisible%2520parts%250Aof%2520the%2520object%2520surface%2520from%2520a%2520single%2520RGB-D%2520camera%2520view.%2520The%2520first%2520method%252C%2520named%250ADeepSDF%2520predicts%2520the%2520Signed%2520Distance%2520Transform%2520to%2520the%2520object%2520surface%2520for%2520a%250Agiven%2520point%2520in%25203D%2520space.%2520The%2520second%2520method%252C%2520named%2520MirrorNet%2520reconstructs%2520the%250Aoccluded%2520objects%2527%2520parts%2520by%2520generating%2520images%2520from%2520the%2520other%2520side%2520of%2520the%250Aobserved%2520object.%2520Experiments%2520performed%2520with%2520objects%2520from%2520the%2520ShapeNet%2520dataset%252C%250Ashow%2520that%2520the%2520view-dependent%2520MirrorNet%2520is%2520faster%2520and%2520has%2520smaller%2520reconstruction%250Aerrors%2520in%2520most%2520categories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Reconstruction%20of%20non-visible%20surfaces%20of%20objects%20from%20a%20Single%20Depth%0A%20%20View%20--%20Comparative%20Study&entry.906535625=Rafa%C5%82%20Staszak%20and%20Piotr%20Micha%C5%82ek%20and%20Jakub%20Chudzi%C5%84ski%20and%20Marek%20Kopicki%20and%20Dominik%20Belter&entry.1292438233=%20%20Scene%20and%20object%20reconstruction%20is%20an%20important%20problem%20in%20robotics%2C%20in%0Aparticular%20in%20planning%20collision-free%20trajectories%20or%20in%20object%20manipulation.%0AThis%20paper%20compares%20two%20strategies%20for%20the%20reconstruction%20of%20nonvisible%20parts%0Aof%20the%20object%20surface%20from%20a%20single%20RGB-D%20camera%20view.%20The%20first%20method%2C%20named%0ADeepSDF%20predicts%20the%20Signed%20Distance%20Transform%20to%20the%20object%20surface%20for%20a%0Agiven%20point%20in%203D%20space.%20The%20second%20method%2C%20named%20MirrorNet%20reconstructs%20the%0Aoccluded%20objects%27%20parts%20by%20generating%20images%20from%20the%20other%20side%20of%20the%0Aobserved%20object.%20Experiments%20performed%20with%20objects%20from%20the%20ShapeNet%20dataset%2C%0Ashow%20that%20the%20view-dependent%20MirrorNet%20is%20faster%20and%20has%20smaller%20reconstruction%0Aerrors%20in%20most%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16101v1&entry.124074799=Read"},
{"title": "Toward Efficient Generalization in 3D Human Pose Estimation via a\n  Canonical Domain Approach", "author": "Hoosang Lee and Jeha Ryu", "abstract": "  Recent advancements in deep learning methods have significantly improved the\nperformance of 3D Human Pose Estimation (HPE). However, performance degradation\ncaused by domain gaps between source and target domains remains a major\nchallenge to generalization, necessitating extensive data augmentation and/or\nfine-tuning for each specific target domain. To address this issue more\nefficiently, we propose a novel canonical domain approach that maps both the\nsource and target domains into a unified canonical domain, alleviating the need\nfor additional fine-tuning in the target domain. To construct the canonical\ndomain, we introduce a canonicalization process to generate a novel canonical\n2D-3D pose mapping that ensures 2D-3D pose consistency and simplifies 2D-3D\npose patterns, enabling more efficient training of lifting networks. The\ncanonicalization of both domains is achieved through the following steps: (1)\nin the source domain, the lifting network is trained within the canonical\ndomain; (2) in the target domain, input 2D poses are canonicalized prior to\ninference by leveraging the properties of perspective projection and known\ncamera intrinsics. Consequently, the trained network can be directly applied to\nthe target domain without requiring additional fine-tuning. Experiments\nconducted with various lifting networks and publicly available datasets (e.g.,\nHuman3.6M, Fit3D, MPI-INF-3DHP) demonstrate that the proposed method\nsubstantially improves generalization capability across datasets while using\nthe same data volume.\n", "link": "http://arxiv.org/abs/2501.16146v1", "date": "2025-01-27", "relevancy": 3.0258, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6263}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5969}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Efficient%20Generalization%20in%203D%20Human%20Pose%20Estimation%20via%20a%0A%20%20Canonical%20Domain%20Approach&body=Title%3A%20Toward%20Efficient%20Generalization%20in%203D%20Human%20Pose%20Estimation%20via%20a%0A%20%20Canonical%20Domain%20Approach%0AAuthor%3A%20Hoosang%20Lee%20and%20Jeha%20Ryu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20methods%20have%20significantly%20improved%20the%0Aperformance%20of%203D%20Human%20Pose%20Estimation%20%28HPE%29.%20However%2C%20performance%20degradation%0Acaused%20by%20domain%20gaps%20between%20source%20and%20target%20domains%20remains%20a%20major%0Achallenge%20to%20generalization%2C%20necessitating%20extensive%20data%20augmentation%20and/or%0Afine-tuning%20for%20each%20specific%20target%20domain.%20To%20address%20this%20issue%20more%0Aefficiently%2C%20we%20propose%20a%20novel%20canonical%20domain%20approach%20that%20maps%20both%20the%0Asource%20and%20target%20domains%20into%20a%20unified%20canonical%20domain%2C%20alleviating%20the%20need%0Afor%20additional%20fine-tuning%20in%20the%20target%20domain.%20To%20construct%20the%20canonical%0Adomain%2C%20we%20introduce%20a%20canonicalization%20process%20to%20generate%20a%20novel%20canonical%0A2D-3D%20pose%20mapping%20that%20ensures%202D-3D%20pose%20consistency%20and%20simplifies%202D-3D%0Apose%20patterns%2C%20enabling%20more%20efficient%20training%20of%20lifting%20networks.%20The%0Acanonicalization%20of%20both%20domains%20is%20achieved%20through%20the%20following%20steps%3A%20%281%29%0Ain%20the%20source%20domain%2C%20the%20lifting%20network%20is%20trained%20within%20the%20canonical%0Adomain%3B%20%282%29%20in%20the%20target%20domain%2C%20input%202D%20poses%20are%20canonicalized%20prior%20to%0Ainference%20by%20leveraging%20the%20properties%20of%20perspective%20projection%20and%20known%0Acamera%20intrinsics.%20Consequently%2C%20the%20trained%20network%20can%20be%20directly%20applied%20to%0Athe%20target%20domain%20without%20requiring%20additional%20fine-tuning.%20Experiments%0Aconducted%20with%20various%20lifting%20networks%20and%20publicly%20available%20datasets%20%28e.g.%2C%0AHuman3.6M%2C%20Fit3D%2C%20MPI-INF-3DHP%29%20demonstrate%20that%20the%20proposed%20method%0Asubstantially%20improves%20generalization%20capability%20across%20datasets%20while%20using%0Athe%20same%20data%20volume.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Efficient%2520Generalization%2520in%25203D%2520Human%2520Pose%2520Estimation%2520via%2520a%250A%2520%2520Canonical%2520Domain%2520Approach%26entry.906535625%3DHoosang%2520Lee%2520and%2520Jeha%2520Ryu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520learning%2520methods%2520have%2520significantly%2520improved%2520the%250Aperformance%2520of%25203D%2520Human%2520Pose%2520Estimation%2520%2528HPE%2529.%2520However%252C%2520performance%2520degradation%250Acaused%2520by%2520domain%2520gaps%2520between%2520source%2520and%2520target%2520domains%2520remains%2520a%2520major%250Achallenge%2520to%2520generalization%252C%2520necessitating%2520extensive%2520data%2520augmentation%2520and/or%250Afine-tuning%2520for%2520each%2520specific%2520target%2520domain.%2520To%2520address%2520this%2520issue%2520more%250Aefficiently%252C%2520we%2520propose%2520a%2520novel%2520canonical%2520domain%2520approach%2520that%2520maps%2520both%2520the%250Asource%2520and%2520target%2520domains%2520into%2520a%2520unified%2520canonical%2520domain%252C%2520alleviating%2520the%2520need%250Afor%2520additional%2520fine-tuning%2520in%2520the%2520target%2520domain.%2520To%2520construct%2520the%2520canonical%250Adomain%252C%2520we%2520introduce%2520a%2520canonicalization%2520process%2520to%2520generate%2520a%2520novel%2520canonical%250A2D-3D%2520pose%2520mapping%2520that%2520ensures%25202D-3D%2520pose%2520consistency%2520and%2520simplifies%25202D-3D%250Apose%2520patterns%252C%2520enabling%2520more%2520efficient%2520training%2520of%2520lifting%2520networks.%2520The%250Acanonicalization%2520of%2520both%2520domains%2520is%2520achieved%2520through%2520the%2520following%2520steps%253A%2520%25281%2529%250Ain%2520the%2520source%2520domain%252C%2520the%2520lifting%2520network%2520is%2520trained%2520within%2520the%2520canonical%250Adomain%253B%2520%25282%2529%2520in%2520the%2520target%2520domain%252C%2520input%25202D%2520poses%2520are%2520canonicalized%2520prior%2520to%250Ainference%2520by%2520leveraging%2520the%2520properties%2520of%2520perspective%2520projection%2520and%2520known%250Acamera%2520intrinsics.%2520Consequently%252C%2520the%2520trained%2520network%2520can%2520be%2520directly%2520applied%2520to%250Athe%2520target%2520domain%2520without%2520requiring%2520additional%2520fine-tuning.%2520Experiments%250Aconducted%2520with%2520various%2520lifting%2520networks%2520and%2520publicly%2520available%2520datasets%2520%2528e.g.%252C%250AHuman3.6M%252C%2520Fit3D%252C%2520MPI-INF-3DHP%2529%2520demonstrate%2520that%2520the%2520proposed%2520method%250Asubstantially%2520improves%2520generalization%2520capability%2520across%2520datasets%2520while%2520using%250Athe%2520same%2520data%2520volume.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Efficient%20Generalization%20in%203D%20Human%20Pose%20Estimation%20via%20a%0A%20%20Canonical%20Domain%20Approach&entry.906535625=Hoosang%20Lee%20and%20Jeha%20Ryu&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20methods%20have%20significantly%20improved%20the%0Aperformance%20of%203D%20Human%20Pose%20Estimation%20%28HPE%29.%20However%2C%20performance%20degradation%0Acaused%20by%20domain%20gaps%20between%20source%20and%20target%20domains%20remains%20a%20major%0Achallenge%20to%20generalization%2C%20necessitating%20extensive%20data%20augmentation%20and/or%0Afine-tuning%20for%20each%20specific%20target%20domain.%20To%20address%20this%20issue%20more%0Aefficiently%2C%20we%20propose%20a%20novel%20canonical%20domain%20approach%20that%20maps%20both%20the%0Asource%20and%20target%20domains%20into%20a%20unified%20canonical%20domain%2C%20alleviating%20the%20need%0Afor%20additional%20fine-tuning%20in%20the%20target%20domain.%20To%20construct%20the%20canonical%0Adomain%2C%20we%20introduce%20a%20canonicalization%20process%20to%20generate%20a%20novel%20canonical%0A2D-3D%20pose%20mapping%20that%20ensures%202D-3D%20pose%20consistency%20and%20simplifies%202D-3D%0Apose%20patterns%2C%20enabling%20more%20efficient%20training%20of%20lifting%20networks.%20The%0Acanonicalization%20of%20both%20domains%20is%20achieved%20through%20the%20following%20steps%3A%20%281%29%0Ain%20the%20source%20domain%2C%20the%20lifting%20network%20is%20trained%20within%20the%20canonical%0Adomain%3B%20%282%29%20in%20the%20target%20domain%2C%20input%202D%20poses%20are%20canonicalized%20prior%20to%0Ainference%20by%20leveraging%20the%20properties%20of%20perspective%20projection%20and%20known%0Acamera%20intrinsics.%20Consequently%2C%20the%20trained%20network%20can%20be%20directly%20applied%20to%0Athe%20target%20domain%20without%20requiring%20additional%20fine-tuning.%20Experiments%0Aconducted%20with%20various%20lifting%20networks%20and%20publicly%20available%20datasets%20%28e.g.%2C%0AHuman3.6M%2C%20Fit3D%2C%20MPI-INF-3DHP%29%20demonstrate%20that%20the%20proposed%20method%0Asubstantially%20improves%20generalization%20capability%20across%20datasets%20while%20using%0Athe%20same%20data%20volume.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16146v1&entry.124074799=Read"},
{"title": "CILP-FGDI: Exploiting Vision-Language Model for Generalizable Person\n  Re-Identification", "author": "Huazhong Zhao and Lei Qi and Xin Geng", "abstract": "  The Visual Language Model, known for its robust cross-modal capabilities, has\nbeen extensively applied in various computer vision tasks. In this paper, we\nexplore the use of CLIP (Contrastive Language-Image Pretraining), a\nvision-language model pretrained on large-scale image-text pairs to align\nvisual and textual features, for acquiring fine-grained and domain-invariant\nrepresentations in generalizable person re-identification. The adaptation of\nCLIP to the task presents two primary challenges: learning more fine-grained\nfeatures to enhance discriminative ability, and learning more domain-invariant\nfeatures to improve the model's generalization capabilities. To mitigate the\nfirst challenge thereby enhance the ability to learn fine-grained features, a\nthree-stage strategy is proposed to boost the accuracy of text descriptions.\nInitially, the image encoder is trained to effectively adapt to person\nre-identification tasks. In the second stage, the features extracted by the\nimage encoder are used to generate textual descriptions (i.e., prompts) for\neach image. Finally, the text encoder with the learned prompts is employed to\nguide the training of the final image encoder. To enhance the model's\ngeneralization capabilities to unseen domains, a bidirectional guiding method\nis introduced to learn domain-invariant image features. Specifically,\ndomain-invariant and domain-relevant prompts are generated, and both positive\n(pulling together image features and domain-invariant prompts) and negative\n(pushing apart image features and domain-relevant prompts) views are used to\ntrain the image encoder. Collectively, these strategies contribute to the\ndevelopment of an innovative CLIP-based framework for learning fine-grained\ngeneralized features in person re-identification.\n", "link": "http://arxiv.org/abs/2501.16065v1", "date": "2025-01-27", "relevancy": 2.9827, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6053}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CILP-FGDI%3A%20Exploiting%20Vision-Language%20Model%20for%20Generalizable%20Person%0A%20%20Re-Identification&body=Title%3A%20CILP-FGDI%3A%20Exploiting%20Vision-Language%20Model%20for%20Generalizable%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Huazhong%20Zhao%20and%20Lei%20Qi%20and%20Xin%20Geng%0AAbstract%3A%20%20%20The%20Visual%20Language%20Model%2C%20known%20for%20its%20robust%20cross-modal%20capabilities%2C%20has%0Abeen%20extensively%20applied%20in%20various%20computer%20vision%20tasks.%20In%20this%20paper%2C%20we%0Aexplore%20the%20use%20of%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%2C%20a%0Avision-language%20model%20pretrained%20on%20large-scale%20image-text%20pairs%20to%20align%0Avisual%20and%20textual%20features%2C%20for%20acquiring%20fine-grained%20and%20domain-invariant%0Arepresentations%20in%20generalizable%20person%20re-identification.%20The%20adaptation%20of%0ACLIP%20to%20the%20task%20presents%20two%20primary%20challenges%3A%20learning%20more%20fine-grained%0Afeatures%20to%20enhance%20discriminative%20ability%2C%20and%20learning%20more%20domain-invariant%0Afeatures%20to%20improve%20the%20model%27s%20generalization%20capabilities.%20To%20mitigate%20the%0Afirst%20challenge%20thereby%20enhance%20the%20ability%20to%20learn%20fine-grained%20features%2C%20a%0Athree-stage%20strategy%20is%20proposed%20to%20boost%20the%20accuracy%20of%20text%20descriptions.%0AInitially%2C%20the%20image%20encoder%20is%20trained%20to%20effectively%20adapt%20to%20person%0Are-identification%20tasks.%20In%20the%20second%20stage%2C%20the%20features%20extracted%20by%20the%0Aimage%20encoder%20are%20used%20to%20generate%20textual%20descriptions%20%28i.e.%2C%20prompts%29%20for%0Aeach%20image.%20Finally%2C%20the%20text%20encoder%20with%20the%20learned%20prompts%20is%20employed%20to%0Aguide%20the%20training%20of%20the%20final%20image%20encoder.%20To%20enhance%20the%20model%27s%0Ageneralization%20capabilities%20to%20unseen%20domains%2C%20a%20bidirectional%20guiding%20method%0Ais%20introduced%20to%20learn%20domain-invariant%20image%20features.%20Specifically%2C%0Adomain-invariant%20and%20domain-relevant%20prompts%20are%20generated%2C%20and%20both%20positive%0A%28pulling%20together%20image%20features%20and%20domain-invariant%20prompts%29%20and%20negative%0A%28pushing%20apart%20image%20features%20and%20domain-relevant%20prompts%29%20views%20are%20used%20to%0Atrain%20the%20image%20encoder.%20Collectively%2C%20these%20strategies%20contribute%20to%20the%0Adevelopment%20of%20an%20innovative%20CLIP-based%20framework%20for%20learning%20fine-grained%0Ageneralized%20features%20in%20person%20re-identification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCILP-FGDI%253A%2520Exploiting%2520Vision-Language%2520Model%2520for%2520Generalizable%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DHuazhong%2520Zhao%2520and%2520Lei%2520Qi%2520and%2520Xin%2520Geng%26entry.1292438233%3D%2520%2520The%2520Visual%2520Language%2520Model%252C%2520known%2520for%2520its%2520robust%2520cross-modal%2520capabilities%252C%2520has%250Abeen%2520extensively%2520applied%2520in%2520various%2520computer%2520vision%2520tasks.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520the%2520use%2520of%2520CLIP%2520%2528Contrastive%2520Language-Image%2520Pretraining%2529%252C%2520a%250Avision-language%2520model%2520pretrained%2520on%2520large-scale%2520image-text%2520pairs%2520to%2520align%250Avisual%2520and%2520textual%2520features%252C%2520for%2520acquiring%2520fine-grained%2520and%2520domain-invariant%250Arepresentations%2520in%2520generalizable%2520person%2520re-identification.%2520The%2520adaptation%2520of%250ACLIP%2520to%2520the%2520task%2520presents%2520two%2520primary%2520challenges%253A%2520learning%2520more%2520fine-grained%250Afeatures%2520to%2520enhance%2520discriminative%2520ability%252C%2520and%2520learning%2520more%2520domain-invariant%250Afeatures%2520to%2520improve%2520the%2520model%2527s%2520generalization%2520capabilities.%2520To%2520mitigate%2520the%250Afirst%2520challenge%2520thereby%2520enhance%2520the%2520ability%2520to%2520learn%2520fine-grained%2520features%252C%2520a%250Athree-stage%2520strategy%2520is%2520proposed%2520to%2520boost%2520the%2520accuracy%2520of%2520text%2520descriptions.%250AInitially%252C%2520the%2520image%2520encoder%2520is%2520trained%2520to%2520effectively%2520adapt%2520to%2520person%250Are-identification%2520tasks.%2520In%2520the%2520second%2520stage%252C%2520the%2520features%2520extracted%2520by%2520the%250Aimage%2520encoder%2520are%2520used%2520to%2520generate%2520textual%2520descriptions%2520%2528i.e.%252C%2520prompts%2529%2520for%250Aeach%2520image.%2520Finally%252C%2520the%2520text%2520encoder%2520with%2520the%2520learned%2520prompts%2520is%2520employed%2520to%250Aguide%2520the%2520training%2520of%2520the%2520final%2520image%2520encoder.%2520To%2520enhance%2520the%2520model%2527s%250Ageneralization%2520capabilities%2520to%2520unseen%2520domains%252C%2520a%2520bidirectional%2520guiding%2520method%250Ais%2520introduced%2520to%2520learn%2520domain-invariant%2520image%2520features.%2520Specifically%252C%250Adomain-invariant%2520and%2520domain-relevant%2520prompts%2520are%2520generated%252C%2520and%2520both%2520positive%250A%2528pulling%2520together%2520image%2520features%2520and%2520domain-invariant%2520prompts%2529%2520and%2520negative%250A%2528pushing%2520apart%2520image%2520features%2520and%2520domain-relevant%2520prompts%2529%2520views%2520are%2520used%2520to%250Atrain%2520the%2520image%2520encoder.%2520Collectively%252C%2520these%2520strategies%2520contribute%2520to%2520the%250Adevelopment%2520of%2520an%2520innovative%2520CLIP-based%2520framework%2520for%2520learning%2520fine-grained%250Ageneralized%2520features%2520in%2520person%2520re-identification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CILP-FGDI%3A%20Exploiting%20Vision-Language%20Model%20for%20Generalizable%20Person%0A%20%20Re-Identification&entry.906535625=Huazhong%20Zhao%20and%20Lei%20Qi%20and%20Xin%20Geng&entry.1292438233=%20%20The%20Visual%20Language%20Model%2C%20known%20for%20its%20robust%20cross-modal%20capabilities%2C%20has%0Abeen%20extensively%20applied%20in%20various%20computer%20vision%20tasks.%20In%20this%20paper%2C%20we%0Aexplore%20the%20use%20of%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%2C%20a%0Avision-language%20model%20pretrained%20on%20large-scale%20image-text%20pairs%20to%20align%0Avisual%20and%20textual%20features%2C%20for%20acquiring%20fine-grained%20and%20domain-invariant%0Arepresentations%20in%20generalizable%20person%20re-identification.%20The%20adaptation%20of%0ACLIP%20to%20the%20task%20presents%20two%20primary%20challenges%3A%20learning%20more%20fine-grained%0Afeatures%20to%20enhance%20discriminative%20ability%2C%20and%20learning%20more%20domain-invariant%0Afeatures%20to%20improve%20the%20model%27s%20generalization%20capabilities.%20To%20mitigate%20the%0Afirst%20challenge%20thereby%20enhance%20the%20ability%20to%20learn%20fine-grained%20features%2C%20a%0Athree-stage%20strategy%20is%20proposed%20to%20boost%20the%20accuracy%20of%20text%20descriptions.%0AInitially%2C%20the%20image%20encoder%20is%20trained%20to%20effectively%20adapt%20to%20person%0Are-identification%20tasks.%20In%20the%20second%20stage%2C%20the%20features%20extracted%20by%20the%0Aimage%20encoder%20are%20used%20to%20generate%20textual%20descriptions%20%28i.e.%2C%20prompts%29%20for%0Aeach%20image.%20Finally%2C%20the%20text%20encoder%20with%20the%20learned%20prompts%20is%20employed%20to%0Aguide%20the%20training%20of%20the%20final%20image%20encoder.%20To%20enhance%20the%20model%27s%0Ageneralization%20capabilities%20to%20unseen%20domains%2C%20a%20bidirectional%20guiding%20method%0Ais%20introduced%20to%20learn%20domain-invariant%20image%20features.%20Specifically%2C%0Adomain-invariant%20and%20domain-relevant%20prompts%20are%20generated%2C%20and%20both%20positive%0A%28pulling%20together%20image%20features%20and%20domain-invariant%20prompts%29%20and%20negative%0A%28pushing%20apart%20image%20features%20and%20domain-relevant%20prompts%29%20views%20are%20used%20to%0Atrain%20the%20image%20encoder.%20Collectively%2C%20these%20strategies%20contribute%20to%20the%0Adevelopment%20of%20an%20innovative%20CLIP-based%20framework%20for%20learning%20fine-grained%0Ageneralized%20features%20in%20person%20re-identification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16065v1&entry.124074799=Read"},
{"title": "Brain-Adapter: Enhancing Neurological Disorder Analysis with\n  Adapter-Tuning Multimodal Large Language Models", "author": "Jing Zhang and Xiaowei Yu and Yanjun Lyu and Lu Zhang and Tong Chen and Chao Cao and Yan Zhuang and Minheng Chen and Tianming Liu and Dajiang Zhu", "abstract": "  Understanding brain disorders is crucial for accurate clinical diagnosis and\ntreatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a\npromising approach to interpreting medical images with the support of text\ndescriptions. However, previous research has primarily focused on 2D medical\nimages, leaving richer spatial information of 3D images under-explored, and\nsingle-modality-based methods are limited by overlooking the critical clinical\ninformation contained in other modalities. To address this issue, this paper\nproposes Brain-Adapter, a novel approach that incorporates an extra bottleneck\nlayer to learn new knowledge and instill it into the original pre-trained\nknowledge. The major idea is to incorporate a lightweight bottleneck layer to\ntrain fewer parameters while capturing essential information and utilize a\nContrastive Language-Image Pre-training (CLIP) strategy to align multimodal\ndata within a unified representation space. Extensive experiments demonstrated\nthe effectiveness of our approach in integrating multimodal data to\nsignificantly improve the diagnosis accuracy without high computational costs,\nhighlighting the potential to enhance real-world diagnostic workflows.\n", "link": "http://arxiv.org/abs/2501.16282v1", "date": "2025-01-27", "relevancy": 2.9712, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-Adapter%3A%20Enhancing%20Neurological%20Disorder%20Analysis%20with%0A%20%20Adapter-Tuning%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Brain-Adapter%3A%20Enhancing%20Neurological%20Disorder%20Analysis%20with%0A%20%20Adapter-Tuning%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Jing%20Zhang%20and%20Xiaowei%20Yu%20and%20Yanjun%20Lyu%20and%20Lu%20Zhang%20and%20Tong%20Chen%20and%20Chao%20Cao%20and%20Yan%20Zhuang%20and%20Minheng%20Chen%20and%20Tianming%20Liu%20and%20Dajiang%20Zhu%0AAbstract%3A%20%20%20Understanding%20brain%20disorders%20is%20crucial%20for%20accurate%20clinical%20diagnosis%20and%0Atreatment.%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20a%0Apromising%20approach%20to%20interpreting%20medical%20images%20with%20the%20support%20of%20text%0Adescriptions.%20However%2C%20previous%20research%20has%20primarily%20focused%20on%202D%20medical%0Aimages%2C%20leaving%20richer%20spatial%20information%20of%203D%20images%20under-explored%2C%20and%0Asingle-modality-based%20methods%20are%20limited%20by%20overlooking%20the%20critical%20clinical%0Ainformation%20contained%20in%20other%20modalities.%20To%20address%20this%20issue%2C%20this%20paper%0Aproposes%20Brain-Adapter%2C%20a%20novel%20approach%20that%20incorporates%20an%20extra%20bottleneck%0Alayer%20to%20learn%20new%20knowledge%20and%20instill%20it%20into%20the%20original%20pre-trained%0Aknowledge.%20The%20major%20idea%20is%20to%20incorporate%20a%20lightweight%20bottleneck%20layer%20to%0Atrain%20fewer%20parameters%20while%20capturing%20essential%20information%20and%20utilize%20a%0AContrastive%20Language-Image%20Pre-training%20%28CLIP%29%20strategy%20to%20align%20multimodal%0Adata%20within%20a%20unified%20representation%20space.%20Extensive%20experiments%20demonstrated%0Athe%20effectiveness%20of%20our%20approach%20in%20integrating%20multimodal%20data%20to%0Asignificantly%20improve%20the%20diagnosis%20accuracy%20without%20high%20computational%20costs%2C%0Ahighlighting%20the%20potential%20to%20enhance%20real-world%20diagnostic%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-Adapter%253A%2520Enhancing%2520Neurological%2520Disorder%2520Analysis%2520with%250A%2520%2520Adapter-Tuning%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DJing%2520Zhang%2520and%2520Xiaowei%2520Yu%2520and%2520Yanjun%2520Lyu%2520and%2520Lu%2520Zhang%2520and%2520Tong%2520Chen%2520and%2520Chao%2520Cao%2520and%2520Yan%2520Zhuang%2520and%2520Minheng%2520Chen%2520and%2520Tianming%2520Liu%2520and%2520Dajiang%2520Zhu%26entry.1292438233%3D%2520%2520Understanding%2520brain%2520disorders%2520is%2520crucial%2520for%2520accurate%2520clinical%2520diagnosis%2520and%250Atreatment.%2520Recent%2520advances%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520offer%2520a%250Apromising%2520approach%2520to%2520interpreting%2520medical%2520images%2520with%2520the%2520support%2520of%2520text%250Adescriptions.%2520However%252C%2520previous%2520research%2520has%2520primarily%2520focused%2520on%25202D%2520medical%250Aimages%252C%2520leaving%2520richer%2520spatial%2520information%2520of%25203D%2520images%2520under-explored%252C%2520and%250Asingle-modality-based%2520methods%2520are%2520limited%2520by%2520overlooking%2520the%2520critical%2520clinical%250Ainformation%2520contained%2520in%2520other%2520modalities.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%250Aproposes%2520Brain-Adapter%252C%2520a%2520novel%2520approach%2520that%2520incorporates%2520an%2520extra%2520bottleneck%250Alayer%2520to%2520learn%2520new%2520knowledge%2520and%2520instill%2520it%2520into%2520the%2520original%2520pre-trained%250Aknowledge.%2520The%2520major%2520idea%2520is%2520to%2520incorporate%2520a%2520lightweight%2520bottleneck%2520layer%2520to%250Atrain%2520fewer%2520parameters%2520while%2520capturing%2520essential%2520information%2520and%2520utilize%2520a%250AContrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520strategy%2520to%2520align%2520multimodal%250Adata%2520within%2520a%2520unified%2520representation%2520space.%2520Extensive%2520experiments%2520demonstrated%250Athe%2520effectiveness%2520of%2520our%2520approach%2520in%2520integrating%2520multimodal%2520data%2520to%250Asignificantly%2520improve%2520the%2520diagnosis%2520accuracy%2520without%2520high%2520computational%2520costs%252C%250Ahighlighting%2520the%2520potential%2520to%2520enhance%2520real-world%2520diagnostic%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-Adapter%3A%20Enhancing%20Neurological%20Disorder%20Analysis%20with%0A%20%20Adapter-Tuning%20Multimodal%20Large%20Language%20Models&entry.906535625=Jing%20Zhang%20and%20Xiaowei%20Yu%20and%20Yanjun%20Lyu%20and%20Lu%20Zhang%20and%20Tong%20Chen%20and%20Chao%20Cao%20and%20Yan%20Zhuang%20and%20Minheng%20Chen%20and%20Tianming%20Liu%20and%20Dajiang%20Zhu&entry.1292438233=%20%20Understanding%20brain%20disorders%20is%20crucial%20for%20accurate%20clinical%20diagnosis%20and%0Atreatment.%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20a%0Apromising%20approach%20to%20interpreting%20medical%20images%20with%20the%20support%20of%20text%0Adescriptions.%20However%2C%20previous%20research%20has%20primarily%20focused%20on%202D%20medical%0Aimages%2C%20leaving%20richer%20spatial%20information%20of%203D%20images%20under-explored%2C%20and%0Asingle-modality-based%20methods%20are%20limited%20by%20overlooking%20the%20critical%20clinical%0Ainformation%20contained%20in%20other%20modalities.%20To%20address%20this%20issue%2C%20this%20paper%0Aproposes%20Brain-Adapter%2C%20a%20novel%20approach%20that%20incorporates%20an%20extra%20bottleneck%0Alayer%20to%20learn%20new%20knowledge%20and%20instill%20it%20into%20the%20original%20pre-trained%0Aknowledge.%20The%20major%20idea%20is%20to%20incorporate%20a%20lightweight%20bottleneck%20layer%20to%0Atrain%20fewer%20parameters%20while%20capturing%20essential%20information%20and%20utilize%20a%0AContrastive%20Language-Image%20Pre-training%20%28CLIP%29%20strategy%20to%20align%20multimodal%0Adata%20within%20a%20unified%20representation%20space.%20Extensive%20experiments%20demonstrated%0Athe%20effectiveness%20of%20our%20approach%20in%20integrating%20multimodal%20data%20to%0Asignificantly%20improve%20the%20diagnosis%20accuracy%20without%20high%20computational%20costs%2C%0Ahighlighting%20the%20potential%20to%20enhance%20real-world%20diagnostic%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16282v1&entry.124074799=Read"},
{"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining", "author": "Wenqi Zhang and Hang Zhang and Xin Li and Jiashuo Sun and Yongliang Shen and Weiming Lu and Deli Zhao and Yueting Zhuang and Lidong Bing", "abstract": "  Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook.\n", "link": "http://arxiv.org/abs/2501.00958v3", "date": "2025-01-27", "relevancy": 2.9439, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202.5%20Years%20in%20Class%3A%20A%20Multimodal%20Textbook%20for%20Vision-Language%0A%20%20Pretraining&body=Title%3A%202.5%20Years%20in%20Class%3A%20A%20Multimodal%20Textbook%20for%20Vision-Language%0A%20%20Pretraining%0AAuthor%3A%20Wenqi%20Zhang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Jiashuo%20Sun%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Deli%20Zhao%20and%20Yueting%20Zhuang%20and%20Lidong%20Bing%0AAbstract%3A%20%20%20Compared%20to%20image-text%20pair%20data%2C%20interleaved%20corpora%20enable%20Vision-Language%0AModels%20%28VLMs%29%20to%20understand%20the%20world%20more%20naturally%20like%20humans.%20However%2C%20such%0Aexisting%20datasets%20are%20crawled%20from%20webpage%2C%20facing%20challenges%20like%20low%0Aknowledge%20density%2C%20loose%20image-text%20relations%2C%20and%20poor%20logical%20coherence%0Abetween%20images.%20On%20the%20other%20hand%2C%20the%20internet%20hosts%20vast%20instructional%20videos%0A%28e.g.%2C%20online%20geometry%20courses%29%20that%20are%20widely%20used%20by%20humans%20to%20learn%0Afoundational%20subjects%2C%20yet%20these%20valuable%20resources%20remain%20underexplored%20in%20VLM%0Atraining.%20In%20this%20paper%2C%20we%20introduce%20a%20high-quality%20%5Ctextbf%7Bmultimodal%0Atextbook%7D%20corpus%20with%20richer%20foundational%20knowledge%20for%20VLM%20pretraining.%20It%0Acollects%20over%202.5%20years%20of%20instructional%20videos%2C%20totaling%2022%2C000%20class%20hours.%0AWe%20first%20use%20an%20LLM-proposed%20taxonomy%20to%20systematically%20gather%20instructional%0Avideos.%20Then%20we%20progressively%20extract%20and%20refine%20visual%20%28keyframes%29%2C%20audio%0A%28ASR%29%2C%20and%20textual%20knowledge%20%28OCR%29%20from%20the%20videos%2C%20and%20organize%20as%20an%0Aimage-text%20interleaved%20corpus%20based%20on%20temporal%20order.%20Compared%20to%20its%0Acounterparts%2C%20our%20video-centric%20textbook%20offers%20more%20coherent%20context%2C%20richer%0Aknowledge%2C%20and%20better%20image-text%20alignment.%20Experiments%20demonstrate%20its%20superb%0Apretraining%20performance%2C%20particularly%20in%20knowledge-%20and%20reasoning-intensive%0Atasks%20like%20ScienceQA%20and%20MathVista.%20Moreover%2C%20VLMs%20pre-trained%20on%20our%20textbook%0Aexhibit%20outstanding%20interleaved%20context%20awareness%2C%20leveraging%20visual%20and%0Atextual%20cues%20in%20their%20few-shot%20context%20for%20task%20solving.%20Our%20code%20are%20available%0Aat%20https%3A//github.com/DAMO-NLP-SG/multimodal_textbook.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00958v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2.5%2520Years%2520in%2520Class%253A%2520A%2520Multimodal%2520Textbook%2520for%2520Vision-Language%250A%2520%2520Pretraining%26entry.906535625%3DWenqi%2520Zhang%2520and%2520Hang%2520Zhang%2520and%2520Xin%2520Li%2520and%2520Jiashuo%2520Sun%2520and%2520Yongliang%2520Shen%2520and%2520Weiming%2520Lu%2520and%2520Deli%2520Zhao%2520and%2520Yueting%2520Zhuang%2520and%2520Lidong%2520Bing%26entry.1292438233%3D%2520%2520Compared%2520to%2520image-text%2520pair%2520data%252C%2520interleaved%2520corpora%2520enable%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520to%2520understand%2520the%2520world%2520more%2520naturally%2520like%2520humans.%2520However%252C%2520such%250Aexisting%2520datasets%2520are%2520crawled%2520from%2520webpage%252C%2520facing%2520challenges%2520like%2520low%250Aknowledge%2520density%252C%2520loose%2520image-text%2520relations%252C%2520and%2520poor%2520logical%2520coherence%250Abetween%2520images.%2520On%2520the%2520other%2520hand%252C%2520the%2520internet%2520hosts%2520vast%2520instructional%2520videos%250A%2528e.g.%252C%2520online%2520geometry%2520courses%2529%2520that%2520are%2520widely%2520used%2520by%2520humans%2520to%2520learn%250Afoundational%2520subjects%252C%2520yet%2520these%2520valuable%2520resources%2520remain%2520underexplored%2520in%2520VLM%250Atraining.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520high-quality%2520%255Ctextbf%257Bmultimodal%250Atextbook%257D%2520corpus%2520with%2520richer%2520foundational%2520knowledge%2520for%2520VLM%2520pretraining.%2520It%250Acollects%2520over%25202.5%2520years%2520of%2520instructional%2520videos%252C%2520totaling%252022%252C000%2520class%2520hours.%250AWe%2520first%2520use%2520an%2520LLM-proposed%2520taxonomy%2520to%2520systematically%2520gather%2520instructional%250Avideos.%2520Then%2520we%2520progressively%2520extract%2520and%2520refine%2520visual%2520%2528keyframes%2529%252C%2520audio%250A%2528ASR%2529%252C%2520and%2520textual%2520knowledge%2520%2528OCR%2529%2520from%2520the%2520videos%252C%2520and%2520organize%2520as%2520an%250Aimage-text%2520interleaved%2520corpus%2520based%2520on%2520temporal%2520order.%2520Compared%2520to%2520its%250Acounterparts%252C%2520our%2520video-centric%2520textbook%2520offers%2520more%2520coherent%2520context%252C%2520richer%250Aknowledge%252C%2520and%2520better%2520image-text%2520alignment.%2520Experiments%2520demonstrate%2520its%2520superb%250Apretraining%2520performance%252C%2520particularly%2520in%2520knowledge-%2520and%2520reasoning-intensive%250Atasks%2520like%2520ScienceQA%2520and%2520MathVista.%2520Moreover%252C%2520VLMs%2520pre-trained%2520on%2520our%2520textbook%250Aexhibit%2520outstanding%2520interleaved%2520context%2520awareness%252C%2520leveraging%2520visual%2520and%250Atextual%2520cues%2520in%2520their%2520few-shot%2520context%2520for%2520task%2520solving.%2520Our%2520code%2520are%2520available%250Aat%2520https%253A//github.com/DAMO-NLP-SG/multimodal_textbook.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00958v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2.5%20Years%20in%20Class%3A%20A%20Multimodal%20Textbook%20for%20Vision-Language%0A%20%20Pretraining&entry.906535625=Wenqi%20Zhang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Jiashuo%20Sun%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Deli%20Zhao%20and%20Yueting%20Zhuang%20and%20Lidong%20Bing&entry.1292438233=%20%20Compared%20to%20image-text%20pair%20data%2C%20interleaved%20corpora%20enable%20Vision-Language%0AModels%20%28VLMs%29%20to%20understand%20the%20world%20more%20naturally%20like%20humans.%20However%2C%20such%0Aexisting%20datasets%20are%20crawled%20from%20webpage%2C%20facing%20challenges%20like%20low%0Aknowledge%20density%2C%20loose%20image-text%20relations%2C%20and%20poor%20logical%20coherence%0Abetween%20images.%20On%20the%20other%20hand%2C%20the%20internet%20hosts%20vast%20instructional%20videos%0A%28e.g.%2C%20online%20geometry%20courses%29%20that%20are%20widely%20used%20by%20humans%20to%20learn%0Afoundational%20subjects%2C%20yet%20these%20valuable%20resources%20remain%20underexplored%20in%20VLM%0Atraining.%20In%20this%20paper%2C%20we%20introduce%20a%20high-quality%20%5Ctextbf%7Bmultimodal%0Atextbook%7D%20corpus%20with%20richer%20foundational%20knowledge%20for%20VLM%20pretraining.%20It%0Acollects%20over%202.5%20years%20of%20instructional%20videos%2C%20totaling%2022%2C000%20class%20hours.%0AWe%20first%20use%20an%20LLM-proposed%20taxonomy%20to%20systematically%20gather%20instructional%0Avideos.%20Then%20we%20progressively%20extract%20and%20refine%20visual%20%28keyframes%29%2C%20audio%0A%28ASR%29%2C%20and%20textual%20knowledge%20%28OCR%29%20from%20the%20videos%2C%20and%20organize%20as%20an%0Aimage-text%20interleaved%20corpus%20based%20on%20temporal%20order.%20Compared%20to%20its%0Acounterparts%2C%20our%20video-centric%20textbook%20offers%20more%20coherent%20context%2C%20richer%0Aknowledge%2C%20and%20better%20image-text%20alignment.%20Experiments%20demonstrate%20its%20superb%0Apretraining%20performance%2C%20particularly%20in%20knowledge-%20and%20reasoning-intensive%0Atasks%20like%20ScienceQA%20and%20MathVista.%20Moreover%2C%20VLMs%20pre-trained%20on%20our%20textbook%0Aexhibit%20outstanding%20interleaved%20context%20awareness%2C%20leveraging%20visual%20and%0Atextual%20cues%20in%20their%20few-shot%20context%20for%20task%20solving.%20Our%20code%20are%20available%0Aat%20https%3A//github.com/DAMO-NLP-SG/multimodal_textbook.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00958v3&entry.124074799=Read"},
{"title": "Large Models in Dialogue for Active Perception and Anomaly Detection", "author": "Tzoulio Chamiti and Nikolaos Passalis and Anastasios Tefas", "abstract": "  Autonomous aerial monitoring is an important task aimed at gathering\ninformation from areas that may not be easily accessible by humans. At the same\ntime, this task often requires recognizing anomalies from a significant\ndistance or not previously encountered in the past. In this paper, we propose a\nnovel framework that leverages the advanced capabilities provided by Large\nLanguage Models (LLMs) to actively collect information and perform anomaly\ndetection in novel scenes. To this end, we propose an LLM based model dialogue\napproach, in which two deep learning models engage in a dialogue to actively\ncontrol a drone to increase perception and anomaly detection accuracy. We\nconduct our experiments in a high fidelity simulation environment where an LLM\nis provided with a predetermined set of natural language movement commands\nmapped into executable code functions. Additionally, we deploy a multimodal\nVisual Question Answering (VQA) model charged with the task of visual question\nanswering and captioning. By engaging the two models in conversation, the LLM\nasks exploratory questions while simultaneously flying a drone into different\nparts of the scene, providing a novel way to implement active perception. By\nleveraging LLMs reasoning ability, we output an improved detailed description\nof the scene going beyond existing static perception approaches. In addition to\ninformation gathering, our approach is utilized for anomaly detection and our\nresults demonstrate the proposed methods effectiveness in informing and\nalerting about potential hazards.\n", "link": "http://arxiv.org/abs/2501.16300v1", "date": "2025-01-27", "relevancy": 2.8772, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Models%20in%20Dialogue%20for%20Active%20Perception%20and%20Anomaly%20Detection&body=Title%3A%20Large%20Models%20in%20Dialogue%20for%20Active%20Perception%20and%20Anomaly%20Detection%0AAuthor%3A%20Tzoulio%20Chamiti%20and%20Nikolaos%20Passalis%20and%20Anastasios%20Tefas%0AAbstract%3A%20%20%20Autonomous%20aerial%20monitoring%20is%20an%20important%20task%20aimed%20at%20gathering%0Ainformation%20from%20areas%20that%20may%20not%20be%20easily%20accessible%20by%20humans.%20At%20the%20same%0Atime%2C%20this%20task%20often%20requires%20recognizing%20anomalies%20from%20a%20significant%0Adistance%20or%20not%20previously%20encountered%20in%20the%20past.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20framework%20that%20leverages%20the%20advanced%20capabilities%20provided%20by%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20actively%20collect%20information%20and%20perform%20anomaly%0Adetection%20in%20novel%20scenes.%20To%20this%20end%2C%20we%20propose%20an%20LLM%20based%20model%20dialogue%0Aapproach%2C%20in%20which%20two%20deep%20learning%20models%20engage%20in%20a%20dialogue%20to%20actively%0Acontrol%20a%20drone%20to%20increase%20perception%20and%20anomaly%20detection%20accuracy.%20We%0Aconduct%20our%20experiments%20in%20a%20high%20fidelity%20simulation%20environment%20where%20an%20LLM%0Ais%20provided%20with%20a%20predetermined%20set%20of%20natural%20language%20movement%20commands%0Amapped%20into%20executable%20code%20functions.%20Additionally%2C%20we%20deploy%20a%20multimodal%0AVisual%20Question%20Answering%20%28VQA%29%20model%20charged%20with%20the%20task%20of%20visual%20question%0Aanswering%20and%20captioning.%20By%20engaging%20the%20two%20models%20in%20conversation%2C%20the%20LLM%0Aasks%20exploratory%20questions%20while%20simultaneously%20flying%20a%20drone%20into%20different%0Aparts%20of%20the%20scene%2C%20providing%20a%20novel%20way%20to%20implement%20active%20perception.%20By%0Aleveraging%20LLMs%20reasoning%20ability%2C%20we%20output%20an%20improved%20detailed%20description%0Aof%20the%20scene%20going%20beyond%20existing%20static%20perception%20approaches.%20In%20addition%20to%0Ainformation%20gathering%2C%20our%20approach%20is%20utilized%20for%20anomaly%20detection%20and%20our%0Aresults%20demonstrate%20the%20proposed%20methods%20effectiveness%20in%20informing%20and%0Aalerting%20about%20potential%20hazards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Models%2520in%2520Dialogue%2520for%2520Active%2520Perception%2520and%2520Anomaly%2520Detection%26entry.906535625%3DTzoulio%2520Chamiti%2520and%2520Nikolaos%2520Passalis%2520and%2520Anastasios%2520Tefas%26entry.1292438233%3D%2520%2520Autonomous%2520aerial%2520monitoring%2520is%2520an%2520important%2520task%2520aimed%2520at%2520gathering%250Ainformation%2520from%2520areas%2520that%2520may%2520not%2520be%2520easily%2520accessible%2520by%2520humans.%2520At%2520the%2520same%250Atime%252C%2520this%2520task%2520often%2520requires%2520recognizing%2520anomalies%2520from%2520a%2520significant%250Adistance%2520or%2520not%2520previously%2520encountered%2520in%2520the%2520past.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520that%2520leverages%2520the%2520advanced%2520capabilities%2520provided%2520by%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520actively%2520collect%2520information%2520and%2520perform%2520anomaly%250Adetection%2520in%2520novel%2520scenes.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520LLM%2520based%2520model%2520dialogue%250Aapproach%252C%2520in%2520which%2520two%2520deep%2520learning%2520models%2520engage%2520in%2520a%2520dialogue%2520to%2520actively%250Acontrol%2520a%2520drone%2520to%2520increase%2520perception%2520and%2520anomaly%2520detection%2520accuracy.%2520We%250Aconduct%2520our%2520experiments%2520in%2520a%2520high%2520fidelity%2520simulation%2520environment%2520where%2520an%2520LLM%250Ais%2520provided%2520with%2520a%2520predetermined%2520set%2520of%2520natural%2520language%2520movement%2520commands%250Amapped%2520into%2520executable%2520code%2520functions.%2520Additionally%252C%2520we%2520deploy%2520a%2520multimodal%250AVisual%2520Question%2520Answering%2520%2528VQA%2529%2520model%2520charged%2520with%2520the%2520task%2520of%2520visual%2520question%250Aanswering%2520and%2520captioning.%2520By%2520engaging%2520the%2520two%2520models%2520in%2520conversation%252C%2520the%2520LLM%250Aasks%2520exploratory%2520questions%2520while%2520simultaneously%2520flying%2520a%2520drone%2520into%2520different%250Aparts%2520of%2520the%2520scene%252C%2520providing%2520a%2520novel%2520way%2520to%2520implement%2520active%2520perception.%2520By%250Aleveraging%2520LLMs%2520reasoning%2520ability%252C%2520we%2520output%2520an%2520improved%2520detailed%2520description%250Aof%2520the%2520scene%2520going%2520beyond%2520existing%2520static%2520perception%2520approaches.%2520In%2520addition%2520to%250Ainformation%2520gathering%252C%2520our%2520approach%2520is%2520utilized%2520for%2520anomaly%2520detection%2520and%2520our%250Aresults%2520demonstrate%2520the%2520proposed%2520methods%2520effectiveness%2520in%2520informing%2520and%250Aalerting%2520about%2520potential%2520hazards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Models%20in%20Dialogue%20for%20Active%20Perception%20and%20Anomaly%20Detection&entry.906535625=Tzoulio%20Chamiti%20and%20Nikolaos%20Passalis%20and%20Anastasios%20Tefas&entry.1292438233=%20%20Autonomous%20aerial%20monitoring%20is%20an%20important%20task%20aimed%20at%20gathering%0Ainformation%20from%20areas%20that%20may%20not%20be%20easily%20accessible%20by%20humans.%20At%20the%20same%0Atime%2C%20this%20task%20often%20requires%20recognizing%20anomalies%20from%20a%20significant%0Adistance%20or%20not%20previously%20encountered%20in%20the%20past.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20framework%20that%20leverages%20the%20advanced%20capabilities%20provided%20by%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20actively%20collect%20information%20and%20perform%20anomaly%0Adetection%20in%20novel%20scenes.%20To%20this%20end%2C%20we%20propose%20an%20LLM%20based%20model%20dialogue%0Aapproach%2C%20in%20which%20two%20deep%20learning%20models%20engage%20in%20a%20dialogue%20to%20actively%0Acontrol%20a%20drone%20to%20increase%20perception%20and%20anomaly%20detection%20accuracy.%20We%0Aconduct%20our%20experiments%20in%20a%20high%20fidelity%20simulation%20environment%20where%20an%20LLM%0Ais%20provided%20with%20a%20predetermined%20set%20of%20natural%20language%20movement%20commands%0Amapped%20into%20executable%20code%20functions.%20Additionally%2C%20we%20deploy%20a%20multimodal%0AVisual%20Question%20Answering%20%28VQA%29%20model%20charged%20with%20the%20task%20of%20visual%20question%0Aanswering%20and%20captioning.%20By%20engaging%20the%20two%20models%20in%20conversation%2C%20the%20LLM%0Aasks%20exploratory%20questions%20while%20simultaneously%20flying%20a%20drone%20into%20different%0Aparts%20of%20the%20scene%2C%20providing%20a%20novel%20way%20to%20implement%20active%20perception.%20By%0Aleveraging%20LLMs%20reasoning%20ability%2C%20we%20output%20an%20improved%20detailed%20description%0Aof%20the%20scene%20going%20beyond%20existing%20static%20perception%20approaches.%20In%20addition%20to%0Ainformation%20gathering%2C%20our%20approach%20is%20utilized%20for%20anomaly%20detection%20and%20our%0Aresults%20demonstrate%20the%20proposed%20methods%20effectiveness%20in%20informing%20and%0Aalerting%20about%20potential%20hazards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16300v1&entry.124074799=Read"},
{"title": "Visual-Lidar Map Alignment for Infrastructure Inspections", "author": "Jake McLaughlin and Nicholas Charron and Sriram Narasimhan", "abstract": "  Routine and repetitive infrastructure inspections present safety, efficiency,\nand consistency challenges as they are performed manually, often in challenging\nor hazardous environments. They can also introduce subjectivity and errors into\nthe process, resulting in undesirable outcomes. Simultaneous localization and\nmapping (SLAM) presents an opportunity to generate high-quality 3D maps that\ncan be used to extract accurate and objective inspection data. Yet, many SLAM\nalgorithms are limited in their ability to align 3D maps from repeated\ninspections in GPS-denied settings automatically. This limitation hinders\npractical long-term asset health assessments by requiring tedious manual\nalignment for data association across scans from previous inspections. This\npaper introduces a versatile map alignment algorithm leveraging both visual and\nlidar data for improved place recognition robustness and presents an\ninfrastructure-focused dataset tailored for consecutive inspections. By\ndetaching map alignment from SLAM, our approach enhances infrastructure\ninspection pipelines, supports monitoring asset degradation over time, and\ninvigorates SLAM research by permitting exploration beyond existing\nmulti-session SLAM algorithms.\n", "link": "http://arxiv.org/abs/2501.14486v2", "date": "2025-01-27", "relevancy": 2.8509, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6099}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5527}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-Lidar%20Map%20Alignment%20for%20Infrastructure%20Inspections&body=Title%3A%20Visual-Lidar%20Map%20Alignment%20for%20Infrastructure%20Inspections%0AAuthor%3A%20Jake%20McLaughlin%20and%20Nicholas%20Charron%20and%20Sriram%20Narasimhan%0AAbstract%3A%20%20%20Routine%20and%20repetitive%20infrastructure%20inspections%20present%20safety%2C%20efficiency%2C%0Aand%20consistency%20challenges%20as%20they%20are%20performed%20manually%2C%20often%20in%20challenging%0Aor%20hazardous%20environments.%20They%20can%20also%20introduce%20subjectivity%20and%20errors%20into%0Athe%20process%2C%20resulting%20in%20undesirable%20outcomes.%20Simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20presents%20an%20opportunity%20to%20generate%20high-quality%203D%20maps%20that%0Acan%20be%20used%20to%20extract%20accurate%20and%20objective%20inspection%20data.%20Yet%2C%20many%20SLAM%0Aalgorithms%20are%20limited%20in%20their%20ability%20to%20align%203D%20maps%20from%20repeated%0Ainspections%20in%20GPS-denied%20settings%20automatically.%20This%20limitation%20hinders%0Apractical%20long-term%20asset%20health%20assessments%20by%20requiring%20tedious%20manual%0Aalignment%20for%20data%20association%20across%20scans%20from%20previous%20inspections.%20This%0Apaper%20introduces%20a%20versatile%20map%20alignment%20algorithm%20leveraging%20both%20visual%20and%0Alidar%20data%20for%20improved%20place%20recognition%20robustness%20and%20presents%20an%0Ainfrastructure-focused%20dataset%20tailored%20for%20consecutive%20inspections.%20By%0Adetaching%20map%20alignment%20from%20SLAM%2C%20our%20approach%20enhances%20infrastructure%0Ainspection%20pipelines%2C%20supports%20monitoring%20asset%20degradation%20over%20time%2C%20and%0Ainvigorates%20SLAM%20research%20by%20permitting%20exploration%20beyond%20existing%0Amulti-session%20SLAM%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14486v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-Lidar%2520Map%2520Alignment%2520for%2520Infrastructure%2520Inspections%26entry.906535625%3DJake%2520McLaughlin%2520and%2520Nicholas%2520Charron%2520and%2520Sriram%2520Narasimhan%26entry.1292438233%3D%2520%2520Routine%2520and%2520repetitive%2520infrastructure%2520inspections%2520present%2520safety%252C%2520efficiency%252C%250Aand%2520consistency%2520challenges%2520as%2520they%2520are%2520performed%2520manually%252C%2520often%2520in%2520challenging%250Aor%2520hazardous%2520environments.%2520They%2520can%2520also%2520introduce%2520subjectivity%2520and%2520errors%2520into%250Athe%2520process%252C%2520resulting%2520in%2520undesirable%2520outcomes.%2520Simultaneous%2520localization%2520and%250Amapping%2520%2528SLAM%2529%2520presents%2520an%2520opportunity%2520to%2520generate%2520high-quality%25203D%2520maps%2520that%250Acan%2520be%2520used%2520to%2520extract%2520accurate%2520and%2520objective%2520inspection%2520data.%2520Yet%252C%2520many%2520SLAM%250Aalgorithms%2520are%2520limited%2520in%2520their%2520ability%2520to%2520align%25203D%2520maps%2520from%2520repeated%250Ainspections%2520in%2520GPS-denied%2520settings%2520automatically.%2520This%2520limitation%2520hinders%250Apractical%2520long-term%2520asset%2520health%2520assessments%2520by%2520requiring%2520tedious%2520manual%250Aalignment%2520for%2520data%2520association%2520across%2520scans%2520from%2520previous%2520inspections.%2520This%250Apaper%2520introduces%2520a%2520versatile%2520map%2520alignment%2520algorithm%2520leveraging%2520both%2520visual%2520and%250Alidar%2520data%2520for%2520improved%2520place%2520recognition%2520robustness%2520and%2520presents%2520an%250Ainfrastructure-focused%2520dataset%2520tailored%2520for%2520consecutive%2520inspections.%2520By%250Adetaching%2520map%2520alignment%2520from%2520SLAM%252C%2520our%2520approach%2520enhances%2520infrastructure%250Ainspection%2520pipelines%252C%2520supports%2520monitoring%2520asset%2520degradation%2520over%2520time%252C%2520and%250Ainvigorates%2520SLAM%2520research%2520by%2520permitting%2520exploration%2520beyond%2520existing%250Amulti-session%2520SLAM%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14486v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-Lidar%20Map%20Alignment%20for%20Infrastructure%20Inspections&entry.906535625=Jake%20McLaughlin%20and%20Nicholas%20Charron%20and%20Sriram%20Narasimhan&entry.1292438233=%20%20Routine%20and%20repetitive%20infrastructure%20inspections%20present%20safety%2C%20efficiency%2C%0Aand%20consistency%20challenges%20as%20they%20are%20performed%20manually%2C%20often%20in%20challenging%0Aor%20hazardous%20environments.%20They%20can%20also%20introduce%20subjectivity%20and%20errors%20into%0Athe%20process%2C%20resulting%20in%20undesirable%20outcomes.%20Simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20presents%20an%20opportunity%20to%20generate%20high-quality%203D%20maps%20that%0Acan%20be%20used%20to%20extract%20accurate%20and%20objective%20inspection%20data.%20Yet%2C%20many%20SLAM%0Aalgorithms%20are%20limited%20in%20their%20ability%20to%20align%203D%20maps%20from%20repeated%0Ainspections%20in%20GPS-denied%20settings%20automatically.%20This%20limitation%20hinders%0Apractical%20long-term%20asset%20health%20assessments%20by%20requiring%20tedious%20manual%0Aalignment%20for%20data%20association%20across%20scans%20from%20previous%20inspections.%20This%0Apaper%20introduces%20a%20versatile%20map%20alignment%20algorithm%20leveraging%20both%20visual%20and%0Alidar%20data%20for%20improved%20place%20recognition%20robustness%20and%20presents%20an%0Ainfrastructure-focused%20dataset%20tailored%20for%20consecutive%20inspections.%20By%0Adetaching%20map%20alignment%20from%20SLAM%2C%20our%20approach%20enhances%20infrastructure%0Ainspection%20pipelines%2C%20supports%20monitoring%20asset%20degradation%20over%20time%2C%20and%0Ainvigorates%20SLAM%20research%20by%20permitting%20exploration%20beyond%20existing%0Amulti-session%20SLAM%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14486v2&entry.124074799=Read"},
{"title": "FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution\n  Multimodal Large Language Models via Visual Registers", "author": "Renshan Zhang and Rui Shao and Gongwei Chen and Kaiwen Zhou and Weili Guan and Liqiang Nie", "abstract": "  The incorporation of high-resolution visual input equips multimodal large\nlanguage models (MLLMs) with enhanced visual perception capabilities for\nreal-world tasks. However, most existing high-resolution MLLMs rely on a\ncropping-based approach to process images, which leads to fragmented visual\nencoding and a sharp increase in redundant tokens. To tackle these issues, we\npropose the FALCON model. FALCON introduces a novel visual register technique\nto simultaneously: 1) Eliminate redundant tokens at the stage of visual\nencoding. To directly address the visual redundancy present in the output of\nvision encoder, we propose a Register-based Representation Compacting\n(ReCompact) mechanism. This mechanism introduces a set of learnable visual\nregisters designed to adaptively aggregate essential information while\ndiscarding redundancy. It enables the encoder to produce a more compact visual\nrepresentation with a minimal number of output tokens, thus eliminating the\nneed for an additional compression module. 2) Ensure continuity in visual\nencoding. To address the potential encoding errors caused by fragmented visual\ninputs, we develop a Register Interactive Attention (ReAtten) module. This\nmodule facilitates effective and efficient information exchange across\nsub-images by enabling interactions between visual registers. It ensures the\ncontinuity of visual semantics throughout the encoding. We conduct\ncomprehensive experiments with FALCON on high-resolution benchmarks across a\nwide range of scenarios. FALCON demonstrates superior performance with a\nremarkable 9-fold and 16-fold reduction in visual tokens.\n", "link": "http://arxiv.org/abs/2501.16297v1", "date": "2025-01-27", "relevancy": 2.847, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FALCON%3A%20Resolving%20Visual%20Redundancy%20and%20Fragmentation%20in%20High-resolution%0A%20%20Multimodal%20Large%20Language%20Models%20via%20Visual%20Registers&body=Title%3A%20FALCON%3A%20Resolving%20Visual%20Redundancy%20and%20Fragmentation%20in%20High-resolution%0A%20%20Multimodal%20Large%20Language%20Models%20via%20Visual%20Registers%0AAuthor%3A%20Renshan%20Zhang%20and%20Rui%20Shao%20and%20Gongwei%20Chen%20and%20Kaiwen%20Zhou%20and%20Weili%20Guan%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20The%20incorporation%20of%20high-resolution%20visual%20input%20equips%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20with%20enhanced%20visual%20perception%20capabilities%20for%0Areal-world%20tasks.%20However%2C%20most%20existing%20high-resolution%20MLLMs%20rely%20on%20a%0Acropping-based%20approach%20to%20process%20images%2C%20which%20leads%20to%20fragmented%20visual%0Aencoding%20and%20a%20sharp%20increase%20in%20redundant%20tokens.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20the%20FALCON%20model.%20FALCON%20introduces%20a%20novel%20visual%20register%20technique%0Ato%20simultaneously%3A%201%29%20Eliminate%20redundant%20tokens%20at%20the%20stage%20of%20visual%0Aencoding.%20To%20directly%20address%20the%20visual%20redundancy%20present%20in%20the%20output%20of%0Avision%20encoder%2C%20we%20propose%20a%20Register-based%20Representation%20Compacting%0A%28ReCompact%29%20mechanism.%20This%20mechanism%20introduces%20a%20set%20of%20learnable%20visual%0Aregisters%20designed%20to%20adaptively%20aggregate%20essential%20information%20while%0Adiscarding%20redundancy.%20It%20enables%20the%20encoder%20to%20produce%20a%20more%20compact%20visual%0Arepresentation%20with%20a%20minimal%20number%20of%20output%20tokens%2C%20thus%20eliminating%20the%0Aneed%20for%20an%20additional%20compression%20module.%202%29%20Ensure%20continuity%20in%20visual%0Aencoding.%20To%20address%20the%20potential%20encoding%20errors%20caused%20by%20fragmented%20visual%0Ainputs%2C%20we%20develop%20a%20Register%20Interactive%20Attention%20%28ReAtten%29%20module.%20This%0Amodule%20facilitates%20effective%20and%20efficient%20information%20exchange%20across%0Asub-images%20by%20enabling%20interactions%20between%20visual%20registers.%20It%20ensures%20the%0Acontinuity%20of%20visual%20semantics%20throughout%20the%20encoding.%20We%20conduct%0Acomprehensive%20experiments%20with%20FALCON%20on%20high-resolution%20benchmarks%20across%20a%0Awide%20range%20of%20scenarios.%20FALCON%20demonstrates%20superior%20performance%20with%20a%0Aremarkable%209-fold%20and%2016-fold%20reduction%20in%20visual%20tokens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFALCON%253A%2520Resolving%2520Visual%2520Redundancy%2520and%2520Fragmentation%2520in%2520High-resolution%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%2520via%2520Visual%2520Registers%26entry.906535625%3DRenshan%2520Zhang%2520and%2520Rui%2520Shao%2520and%2520Gongwei%2520Chen%2520and%2520Kaiwen%2520Zhou%2520and%2520Weili%2520Guan%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520The%2520incorporation%2520of%2520high-resolution%2520visual%2520input%2520equips%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%2520with%2520enhanced%2520visual%2520perception%2520capabilities%2520for%250Areal-world%2520tasks.%2520However%252C%2520most%2520existing%2520high-resolution%2520MLLMs%2520rely%2520on%2520a%250Acropping-based%2520approach%2520to%2520process%2520images%252C%2520which%2520leads%2520to%2520fragmented%2520visual%250Aencoding%2520and%2520a%2520sharp%2520increase%2520in%2520redundant%2520tokens.%2520To%2520tackle%2520these%2520issues%252C%2520we%250Apropose%2520the%2520FALCON%2520model.%2520FALCON%2520introduces%2520a%2520novel%2520visual%2520register%2520technique%250Ato%2520simultaneously%253A%25201%2529%2520Eliminate%2520redundant%2520tokens%2520at%2520the%2520stage%2520of%2520visual%250Aencoding.%2520To%2520directly%2520address%2520the%2520visual%2520redundancy%2520present%2520in%2520the%2520output%2520of%250Avision%2520encoder%252C%2520we%2520propose%2520a%2520Register-based%2520Representation%2520Compacting%250A%2528ReCompact%2529%2520mechanism.%2520This%2520mechanism%2520introduces%2520a%2520set%2520of%2520learnable%2520visual%250Aregisters%2520designed%2520to%2520adaptively%2520aggregate%2520essential%2520information%2520while%250Adiscarding%2520redundancy.%2520It%2520enables%2520the%2520encoder%2520to%2520produce%2520a%2520more%2520compact%2520visual%250Arepresentation%2520with%2520a%2520minimal%2520number%2520of%2520output%2520tokens%252C%2520thus%2520eliminating%2520the%250Aneed%2520for%2520an%2520additional%2520compression%2520module.%25202%2529%2520Ensure%2520continuity%2520in%2520visual%250Aencoding.%2520To%2520address%2520the%2520potential%2520encoding%2520errors%2520caused%2520by%2520fragmented%2520visual%250Ainputs%252C%2520we%2520develop%2520a%2520Register%2520Interactive%2520Attention%2520%2528ReAtten%2529%2520module.%2520This%250Amodule%2520facilitates%2520effective%2520and%2520efficient%2520information%2520exchange%2520across%250Asub-images%2520by%2520enabling%2520interactions%2520between%2520visual%2520registers.%2520It%2520ensures%2520the%250Acontinuity%2520of%2520visual%2520semantics%2520throughout%2520the%2520encoding.%2520We%2520conduct%250Acomprehensive%2520experiments%2520with%2520FALCON%2520on%2520high-resolution%2520benchmarks%2520across%2520a%250Awide%2520range%2520of%2520scenarios.%2520FALCON%2520demonstrates%2520superior%2520performance%2520with%2520a%250Aremarkable%25209-fold%2520and%252016-fold%2520reduction%2520in%2520visual%2520tokens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FALCON%3A%20Resolving%20Visual%20Redundancy%20and%20Fragmentation%20in%20High-resolution%0A%20%20Multimodal%20Large%20Language%20Models%20via%20Visual%20Registers&entry.906535625=Renshan%20Zhang%20and%20Rui%20Shao%20and%20Gongwei%20Chen%20and%20Kaiwen%20Zhou%20and%20Weili%20Guan%20and%20Liqiang%20Nie&entry.1292438233=%20%20The%20incorporation%20of%20high-resolution%20visual%20input%20equips%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20with%20enhanced%20visual%20perception%20capabilities%20for%0Areal-world%20tasks.%20However%2C%20most%20existing%20high-resolution%20MLLMs%20rely%20on%20a%0Acropping-based%20approach%20to%20process%20images%2C%20which%20leads%20to%20fragmented%20visual%0Aencoding%20and%20a%20sharp%20increase%20in%20redundant%20tokens.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20the%20FALCON%20model.%20FALCON%20introduces%20a%20novel%20visual%20register%20technique%0Ato%20simultaneously%3A%201%29%20Eliminate%20redundant%20tokens%20at%20the%20stage%20of%20visual%0Aencoding.%20To%20directly%20address%20the%20visual%20redundancy%20present%20in%20the%20output%20of%0Avision%20encoder%2C%20we%20propose%20a%20Register-based%20Representation%20Compacting%0A%28ReCompact%29%20mechanism.%20This%20mechanism%20introduces%20a%20set%20of%20learnable%20visual%0Aregisters%20designed%20to%20adaptively%20aggregate%20essential%20information%20while%0Adiscarding%20redundancy.%20It%20enables%20the%20encoder%20to%20produce%20a%20more%20compact%20visual%0Arepresentation%20with%20a%20minimal%20number%20of%20output%20tokens%2C%20thus%20eliminating%20the%0Aneed%20for%20an%20additional%20compression%20module.%202%29%20Ensure%20continuity%20in%20visual%0Aencoding.%20To%20address%20the%20potential%20encoding%20errors%20caused%20by%20fragmented%20visual%0Ainputs%2C%20we%20develop%20a%20Register%20Interactive%20Attention%20%28ReAtten%29%20module.%20This%0Amodule%20facilitates%20effective%20and%20efficient%20information%20exchange%20across%0Asub-images%20by%20enabling%20interactions%20between%20visual%20registers.%20It%20ensures%20the%0Acontinuity%20of%20visual%20semantics%20throughout%20the%20encoding.%20We%20conduct%0Acomprehensive%20experiments%20with%20FALCON%20on%20high-resolution%20benchmarks%20across%20a%0Awide%20range%20of%20scenarios.%20FALCON%20demonstrates%20superior%20performance%20with%20a%0Aremarkable%209-fold%20and%2016-fold%20reduction%20in%20visual%20tokens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16297v1&entry.124074799=Read"},
{"title": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning", "author": "Pu Yang and Bin Dong", "abstract": "  Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports.\n", "link": "http://arxiv.org/abs/2501.01834v3", "date": "2025-01-27", "relevancy": 2.7558, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoColl%3A%20Agent-Based%20Specific%20and%20General%20Model%20Collaboration%20for%20Image%0A%20%20Captioning&body=Title%3A%20MoColl%3A%20Agent-Based%20Specific%20and%20General%20Model%20Collaboration%20for%20Image%0A%20%20Captioning%0AAuthor%3A%20Pu%20Yang%20and%20Bin%20Dong%0AAbstract%3A%20%20%20Image%20captioning%20is%20a%20critical%20task%20at%20the%20intersection%20of%20computer%20vision%0Aand%20natural%20language%20processing%2C%20with%20wide-ranging%20applications%20across%20various%0Adomains.%20For%20complex%20tasks%20such%20as%20diagnostic%20report%20generation%2C%20deep%20learning%0Amodels%20require%20not%20only%20domain-specific%20image-caption%20datasets%20but%20also%20the%0Aincorporation%20of%20relevant%20general%20knowledge%20to%20provide%20contextual%20accuracy.%0AExisting%20approaches%20exhibit%20inherent%20limitations%3A%20specialized%20models%20excel%20in%0Acapturing%20domain-specific%20details%20but%20lack%20generalization%2C%20while%0Avision-language%20models%20%28VLMs%29%20built%20on%20large%20language%20models%20%28LLMs%29%20leverage%0Ageneral%20knowledge%20but%20struggle%20with%20domain-specific%20adaptation.%20To%20address%0Athese%20limitations%2C%20this%20paper%20proposes%20a%20novel%20agent-enhanced%20model%0Acollaboration%20framework%2C%20which%20we%20call%20MoColl%2C%20designed%20to%20effectively%0Aintegrate%20domain-specific%20and%20general%20knowledge.%20Specifically%2C%20our%20approach%20is%0Ato%20decompose%20complex%20image%20captioning%20tasks%20into%20a%20series%20of%20interconnected%0Aquestion-answer%20subtasks.%20A%20trainable%20visual%20question%20answering%20%28VQA%29%20model%20is%0Aemployed%20as%20a%20specialized%20tool%20to%20focus%20on%20domain-specific%20visual%20analysis%2C%0Aanswering%20task-specific%20questions%20based%20on%20image%20content.%20Concurrently%2C%20an%0ALLM-based%20agent%20with%20general%20knowledge%20formulates%20these%20questions%20and%0Asynthesizes%20the%20resulting%20question-answer%20pairs%20into%20coherent%20captions.%20Beyond%0Aits%20role%20in%20leveraging%20the%20VQA%20model%2C%20the%20agent%20further%20guides%20its%20training%20to%0Aenhance%20its%20domain-specific%20capabilities.%20Experimental%20results%20on%20radiology%0Areport%20generation%20validate%20the%20effectiveness%20of%20the%20proposed%20framework%2C%0Ademonstrating%20significant%20improvements%20in%20the%20quality%20of%20generated%20reports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01834v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoColl%253A%2520Agent-Based%2520Specific%2520and%2520General%2520Model%2520Collaboration%2520for%2520Image%250A%2520%2520Captioning%26entry.906535625%3DPu%2520Yang%2520and%2520Bin%2520Dong%26entry.1292438233%3D%2520%2520Image%2520captioning%2520is%2520a%2520critical%2520task%2520at%2520the%2520intersection%2520of%2520computer%2520vision%250Aand%2520natural%2520language%2520processing%252C%2520with%2520wide-ranging%2520applications%2520across%2520various%250Adomains.%2520For%2520complex%2520tasks%2520such%2520as%2520diagnostic%2520report%2520generation%252C%2520deep%2520learning%250Amodels%2520require%2520not%2520only%2520domain-specific%2520image-caption%2520datasets%2520but%2520also%2520the%250Aincorporation%2520of%2520relevant%2520general%2520knowledge%2520to%2520provide%2520contextual%2520accuracy.%250AExisting%2520approaches%2520exhibit%2520inherent%2520limitations%253A%2520specialized%2520models%2520excel%2520in%250Acapturing%2520domain-specific%2520details%2520but%2520lack%2520generalization%252C%2520while%250Avision-language%2520models%2520%2528VLMs%2529%2520built%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520leverage%250Ageneral%2520knowledge%2520but%2520struggle%2520with%2520domain-specific%2520adaptation.%2520To%2520address%250Athese%2520limitations%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520agent-enhanced%2520model%250Acollaboration%2520framework%252C%2520which%2520we%2520call%2520MoColl%252C%2520designed%2520to%2520effectively%250Aintegrate%2520domain-specific%2520and%2520general%2520knowledge.%2520Specifically%252C%2520our%2520approach%2520is%250Ato%2520decompose%2520complex%2520image%2520captioning%2520tasks%2520into%2520a%2520series%2520of%2520interconnected%250Aquestion-answer%2520subtasks.%2520A%2520trainable%2520visual%2520question%2520answering%2520%2528VQA%2529%2520model%2520is%250Aemployed%2520as%2520a%2520specialized%2520tool%2520to%2520focus%2520on%2520domain-specific%2520visual%2520analysis%252C%250Aanswering%2520task-specific%2520questions%2520based%2520on%2520image%2520content.%2520Concurrently%252C%2520an%250ALLM-based%2520agent%2520with%2520general%2520knowledge%2520formulates%2520these%2520questions%2520and%250Asynthesizes%2520the%2520resulting%2520question-answer%2520pairs%2520into%2520coherent%2520captions.%2520Beyond%250Aits%2520role%2520in%2520leveraging%2520the%2520VQA%2520model%252C%2520the%2520agent%2520further%2520guides%2520its%2520training%2520to%250Aenhance%2520its%2520domain-specific%2520capabilities.%2520Experimental%2520results%2520on%2520radiology%250Areport%2520generation%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520framework%252C%250Ademonstrating%2520significant%2520improvements%2520in%2520the%2520quality%2520of%2520generated%2520reports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01834v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoColl%3A%20Agent-Based%20Specific%20and%20General%20Model%20Collaboration%20for%20Image%0A%20%20Captioning&entry.906535625=Pu%20Yang%20and%20Bin%20Dong&entry.1292438233=%20%20Image%20captioning%20is%20a%20critical%20task%20at%20the%20intersection%20of%20computer%20vision%0Aand%20natural%20language%20processing%2C%20with%20wide-ranging%20applications%20across%20various%0Adomains.%20For%20complex%20tasks%20such%20as%20diagnostic%20report%20generation%2C%20deep%20learning%0Amodels%20require%20not%20only%20domain-specific%20image-caption%20datasets%20but%20also%20the%0Aincorporation%20of%20relevant%20general%20knowledge%20to%20provide%20contextual%20accuracy.%0AExisting%20approaches%20exhibit%20inherent%20limitations%3A%20specialized%20models%20excel%20in%0Acapturing%20domain-specific%20details%20but%20lack%20generalization%2C%20while%0Avision-language%20models%20%28VLMs%29%20built%20on%20large%20language%20models%20%28LLMs%29%20leverage%0Ageneral%20knowledge%20but%20struggle%20with%20domain-specific%20adaptation.%20To%20address%0Athese%20limitations%2C%20this%20paper%20proposes%20a%20novel%20agent-enhanced%20model%0Acollaboration%20framework%2C%20which%20we%20call%20MoColl%2C%20designed%20to%20effectively%0Aintegrate%20domain-specific%20and%20general%20knowledge.%20Specifically%2C%20our%20approach%20is%0Ato%20decompose%20complex%20image%20captioning%20tasks%20into%20a%20series%20of%20interconnected%0Aquestion-answer%20subtasks.%20A%20trainable%20visual%20question%20answering%20%28VQA%29%20model%20is%0Aemployed%20as%20a%20specialized%20tool%20to%20focus%20on%20domain-specific%20visual%20analysis%2C%0Aanswering%20task-specific%20questions%20based%20on%20image%20content.%20Concurrently%2C%20an%0ALLM-based%20agent%20with%20general%20knowledge%20formulates%20these%20questions%20and%0Asynthesizes%20the%20resulting%20question-answer%20pairs%20into%20coherent%20captions.%20Beyond%0Aits%20role%20in%20leveraging%20the%20VQA%20model%2C%20the%20agent%20further%20guides%20its%20training%20to%0Aenhance%20its%20domain-specific%20capabilities.%20Experimental%20results%20on%20radiology%0Areport%20generation%20validate%20the%20effectiveness%20of%20the%20proposed%20framework%2C%0Ademonstrating%20significant%20improvements%20in%20the%20quality%20of%20generated%20reports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01834v3&entry.124074799=Read"},
{"title": "VCRScore: Image captioning metric based on V\\&L Transformers, CLIP, and\n  precision-recall", "author": "Guillermo Ruiz and Tania Ram\u00edrez and Daniela Moctezuma", "abstract": "  Image captioning has become an essential Vision & Language research task. It\nis about predicting the most accurate caption given a specific image or video.\nThe research community has achieved impressive results by continuously\nproposing new models and approaches to improve the overall model's performance.\nNevertheless, despite increasing proposals, the performance metrics used to\nmeasure their advances have remained practically untouched through the years. A\nprobe of that, nowadays metrics like BLEU, METEOR, CIDEr, and ROUGE are still\nvery used, aside from more sophisticated metrics such as BertScore and\nClipScore.\n  Hence, it is essential to adjust how are measure the advances, limitations,\nand scopes of the new image captioning proposals, as well as to adapt new\nmetrics to these new advanced image captioning approaches.\n  This work proposes a new evaluation metric for the image captioning problem.\nTo do that, first, it was generated a human-labeled dataset to assess to which\ndegree the captions correlate with the image's content. Taking these human\nscores as ground truth, we propose a new metric, and compare it with several\nwell-known metrics, from classical to newer ones. Outperformed results were\nalso found, and interesting insights were presented and discussed.\n", "link": "http://arxiv.org/abs/2501.09155v2", "date": "2025-01-27", "relevancy": 2.7114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCRScore%3A%20Image%20captioning%20metric%20based%20on%20V%5C%26L%20Transformers%2C%20CLIP%2C%20and%0A%20%20precision-recall&body=Title%3A%20VCRScore%3A%20Image%20captioning%20metric%20based%20on%20V%5C%26L%20Transformers%2C%20CLIP%2C%20and%0A%20%20precision-recall%0AAuthor%3A%20Guillermo%20Ruiz%20and%20Tania%20Ram%C3%ADrez%20and%20Daniela%20Moctezuma%0AAbstract%3A%20%20%20Image%20captioning%20has%20become%20an%20essential%20Vision%20%26%20Language%20research%20task.%20It%0Ais%20about%20predicting%20the%20most%20accurate%20caption%20given%20a%20specific%20image%20or%20video.%0AThe%20research%20community%20has%20achieved%20impressive%20results%20by%20continuously%0Aproposing%20new%20models%20and%20approaches%20to%20improve%20the%20overall%20model%27s%20performance.%0ANevertheless%2C%20despite%20increasing%20proposals%2C%20the%20performance%20metrics%20used%20to%0Ameasure%20their%20advances%20have%20remained%20practically%20untouched%20through%20the%20years.%20A%0Aprobe%20of%20that%2C%20nowadays%20metrics%20like%20BLEU%2C%20METEOR%2C%20CIDEr%2C%20and%20ROUGE%20are%20still%0Avery%20used%2C%20aside%20from%20more%20sophisticated%20metrics%20such%20as%20BertScore%20and%0AClipScore.%0A%20%20Hence%2C%20it%20is%20essential%20to%20adjust%20how%20are%20measure%20the%20advances%2C%20limitations%2C%0Aand%20scopes%20of%20the%20new%20image%20captioning%20proposals%2C%20as%20well%20as%20to%20adapt%20new%0Ametrics%20to%20these%20new%20advanced%20image%20captioning%20approaches.%0A%20%20This%20work%20proposes%20a%20new%20evaluation%20metric%20for%20the%20image%20captioning%20problem.%0ATo%20do%20that%2C%20first%2C%20it%20was%20generated%20a%20human-labeled%20dataset%20to%20assess%20to%20which%0Adegree%20the%20captions%20correlate%20with%20the%20image%27s%20content.%20Taking%20these%20human%0Ascores%20as%20ground%20truth%2C%20we%20propose%20a%20new%20metric%2C%20and%20compare%20it%20with%20several%0Awell-known%20metrics%2C%20from%20classical%20to%20newer%20ones.%20Outperformed%20results%20were%0Aalso%20found%2C%20and%20interesting%20insights%20were%20presented%20and%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCRScore%253A%2520Image%2520captioning%2520metric%2520based%2520on%2520V%255C%2526L%2520Transformers%252C%2520CLIP%252C%2520and%250A%2520%2520precision-recall%26entry.906535625%3DGuillermo%2520Ruiz%2520and%2520Tania%2520Ram%25C3%25ADrez%2520and%2520Daniela%2520Moctezuma%26entry.1292438233%3D%2520%2520Image%2520captioning%2520has%2520become%2520an%2520essential%2520Vision%2520%2526%2520Language%2520research%2520task.%2520It%250Ais%2520about%2520predicting%2520the%2520most%2520accurate%2520caption%2520given%2520a%2520specific%2520image%2520or%2520video.%250AThe%2520research%2520community%2520has%2520achieved%2520impressive%2520results%2520by%2520continuously%250Aproposing%2520new%2520models%2520and%2520approaches%2520to%2520improve%2520the%2520overall%2520model%2527s%2520performance.%250ANevertheless%252C%2520despite%2520increasing%2520proposals%252C%2520the%2520performance%2520metrics%2520used%2520to%250Ameasure%2520their%2520advances%2520have%2520remained%2520practically%2520untouched%2520through%2520the%2520years.%2520A%250Aprobe%2520of%2520that%252C%2520nowadays%2520metrics%2520like%2520BLEU%252C%2520METEOR%252C%2520CIDEr%252C%2520and%2520ROUGE%2520are%2520still%250Avery%2520used%252C%2520aside%2520from%2520more%2520sophisticated%2520metrics%2520such%2520as%2520BertScore%2520and%250AClipScore.%250A%2520%2520Hence%252C%2520it%2520is%2520essential%2520to%2520adjust%2520how%2520are%2520measure%2520the%2520advances%252C%2520limitations%252C%250Aand%2520scopes%2520of%2520the%2520new%2520image%2520captioning%2520proposals%252C%2520as%2520well%2520as%2520to%2520adapt%2520new%250Ametrics%2520to%2520these%2520new%2520advanced%2520image%2520captioning%2520approaches.%250A%2520%2520This%2520work%2520proposes%2520a%2520new%2520evaluation%2520metric%2520for%2520the%2520image%2520captioning%2520problem.%250ATo%2520do%2520that%252C%2520first%252C%2520it%2520was%2520generated%2520a%2520human-labeled%2520dataset%2520to%2520assess%2520to%2520which%250Adegree%2520the%2520captions%2520correlate%2520with%2520the%2520image%2527s%2520content.%2520Taking%2520these%2520human%250Ascores%2520as%2520ground%2520truth%252C%2520we%2520propose%2520a%2520new%2520metric%252C%2520and%2520compare%2520it%2520with%2520several%250Awell-known%2520metrics%252C%2520from%2520classical%2520to%2520newer%2520ones.%2520Outperformed%2520results%2520were%250Aalso%2520found%252C%2520and%2520interesting%2520insights%2520were%2520presented%2520and%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCRScore%3A%20Image%20captioning%20metric%20based%20on%20V%5C%26L%20Transformers%2C%20CLIP%2C%20and%0A%20%20precision-recall&entry.906535625=Guillermo%20Ruiz%20and%20Tania%20Ram%C3%ADrez%20and%20Daniela%20Moctezuma&entry.1292438233=%20%20Image%20captioning%20has%20become%20an%20essential%20Vision%20%26%20Language%20research%20task.%20It%0Ais%20about%20predicting%20the%20most%20accurate%20caption%20given%20a%20specific%20image%20or%20video.%0AThe%20research%20community%20has%20achieved%20impressive%20results%20by%20continuously%0Aproposing%20new%20models%20and%20approaches%20to%20improve%20the%20overall%20model%27s%20performance.%0ANevertheless%2C%20despite%20increasing%20proposals%2C%20the%20performance%20metrics%20used%20to%0Ameasure%20their%20advances%20have%20remained%20practically%20untouched%20through%20the%20years.%20A%0Aprobe%20of%20that%2C%20nowadays%20metrics%20like%20BLEU%2C%20METEOR%2C%20CIDEr%2C%20and%20ROUGE%20are%20still%0Avery%20used%2C%20aside%20from%20more%20sophisticated%20metrics%20such%20as%20BertScore%20and%0AClipScore.%0A%20%20Hence%2C%20it%20is%20essential%20to%20adjust%20how%20are%20measure%20the%20advances%2C%20limitations%2C%0Aand%20scopes%20of%20the%20new%20image%20captioning%20proposals%2C%20as%20well%20as%20to%20adapt%20new%0Ametrics%20to%20these%20new%20advanced%20image%20captioning%20approaches.%0A%20%20This%20work%20proposes%20a%20new%20evaluation%20metric%20for%20the%20image%20captioning%20problem.%0ATo%20do%20that%2C%20first%2C%20it%20was%20generated%20a%20human-labeled%20dataset%20to%20assess%20to%20which%0Adegree%20the%20captions%20correlate%20with%20the%20image%27s%20content.%20Taking%20these%20human%0Ascores%20as%20ground%20truth%2C%20we%20propose%20a%20new%20metric%2C%20and%20compare%20it%20with%20several%0Awell-known%20metrics%2C%20from%20classical%20to%20newer%20ones.%20Outperformed%20results%20were%0Aalso%20found%2C%20and%20interesting%20insights%20were%20presented%20and%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09155v2&entry.124074799=Read"},
{"title": "Dimensions underlying the representational alignment of deep neural\n  networks with humans", "author": "Florian P. Mahner and Lukas Muttenthaler and Umut G\u00fc\u00e7l\u00fc and Martin N. Hebart", "abstract": "  Determining the similarities and differences between humans and artificial\nintelligence (AI) is an important goal both in computational cognitive\nneuroscience and machine learning, promising a deeper understanding of human\ncognition and safer, more reliable AI systems. Much previous work comparing\nrepresentations in humans and AI has relied on global, scalar measures to\nquantify their alignment. However, without explicit hypotheses, these measures\nonly inform us about the degree of alignment, not the factors that determine\nit. To address this challenge, we propose a generic framework to compare human\nand AI representations, based on identifying latent representational dimensions\nunderlying the same behavior in both domains. Applying this framework to humans\nand a deep neural network (DNN) model of natural images revealed a\nlow-dimensional DNN embedding of both visual and semantic dimensions. In\ncontrast to humans, DNNs exhibited a clear dominance of visual over semantic\nproperties, indicating divergent strategies for representing images. While\nin-silico experiments showed seemingly consistent interpretability of DNN\ndimensions, a direct comparison between human and DNN representations revealed\nsubstantial differences in how they process images. By making representations\ndirectly comparable, our results reveal important challenges for\nrepresentational alignment and offer a means for improving their comparability.\n", "link": "http://arxiv.org/abs/2406.19087v2", "date": "2025-01-27", "relevancy": 2.7036, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimensions%20underlying%20the%20representational%20alignment%20of%20deep%20neural%0A%20%20networks%20with%20humans&body=Title%3A%20Dimensions%20underlying%20the%20representational%20alignment%20of%20deep%20neural%0A%20%20networks%20with%20humans%0AAuthor%3A%20Florian%20P.%20Mahner%20and%20Lukas%20Muttenthaler%20and%20Umut%20G%C3%BC%C3%A7l%C3%BC%20and%20Martin%20N.%20Hebart%0AAbstract%3A%20%20%20Determining%20the%20similarities%20and%20differences%20between%20humans%20and%20artificial%0Aintelligence%20%28AI%29%20is%20an%20important%20goal%20both%20in%20computational%20cognitive%0Aneuroscience%20and%20machine%20learning%2C%20promising%20a%20deeper%20understanding%20of%20human%0Acognition%20and%20safer%2C%20more%20reliable%20AI%20systems.%20Much%20previous%20work%20comparing%0Arepresentations%20in%20humans%20and%20AI%20has%20relied%20on%20global%2C%20scalar%20measures%20to%0Aquantify%20their%20alignment.%20However%2C%20without%20explicit%20hypotheses%2C%20these%20measures%0Aonly%20inform%20us%20about%20the%20degree%20of%20alignment%2C%20not%20the%20factors%20that%20determine%0Ait.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20generic%20framework%20to%20compare%20human%0Aand%20AI%20representations%2C%20based%20on%20identifying%20latent%20representational%20dimensions%0Aunderlying%20the%20same%20behavior%20in%20both%20domains.%20Applying%20this%20framework%20to%20humans%0Aand%20a%20deep%20neural%20network%20%28DNN%29%20model%20of%20natural%20images%20revealed%20a%0Alow-dimensional%20DNN%20embedding%20of%20both%20visual%20and%20semantic%20dimensions.%20In%0Acontrast%20to%20humans%2C%20DNNs%20exhibited%20a%20clear%20dominance%20of%20visual%20over%20semantic%0Aproperties%2C%20indicating%20divergent%20strategies%20for%20representing%20images.%20While%0Ain-silico%20experiments%20showed%20seemingly%20consistent%20interpretability%20of%20DNN%0Adimensions%2C%20a%20direct%20comparison%20between%20human%20and%20DNN%20representations%20revealed%0Asubstantial%20differences%20in%20how%20they%20process%20images.%20By%20making%20representations%0Adirectly%20comparable%2C%20our%20results%20reveal%20important%20challenges%20for%0Arepresentational%20alignment%20and%20offer%20a%20means%20for%20improving%20their%20comparability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19087v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimensions%2520underlying%2520the%2520representational%2520alignment%2520of%2520deep%2520neural%250A%2520%2520networks%2520with%2520humans%26entry.906535625%3DFlorian%2520P.%2520Mahner%2520and%2520Lukas%2520Muttenthaler%2520and%2520Umut%2520G%25C3%25BC%25C3%25A7l%25C3%25BC%2520and%2520Martin%2520N.%2520Hebart%26entry.1292438233%3D%2520%2520Determining%2520the%2520similarities%2520and%2520differences%2520between%2520humans%2520and%2520artificial%250Aintelligence%2520%2528AI%2529%2520is%2520an%2520important%2520goal%2520both%2520in%2520computational%2520cognitive%250Aneuroscience%2520and%2520machine%2520learning%252C%2520promising%2520a%2520deeper%2520understanding%2520of%2520human%250Acognition%2520and%2520safer%252C%2520more%2520reliable%2520AI%2520systems.%2520Much%2520previous%2520work%2520comparing%250Arepresentations%2520in%2520humans%2520and%2520AI%2520has%2520relied%2520on%2520global%252C%2520scalar%2520measures%2520to%250Aquantify%2520their%2520alignment.%2520However%252C%2520without%2520explicit%2520hypotheses%252C%2520these%2520measures%250Aonly%2520inform%2520us%2520about%2520the%2520degree%2520of%2520alignment%252C%2520not%2520the%2520factors%2520that%2520determine%250Ait.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520generic%2520framework%2520to%2520compare%2520human%250Aand%2520AI%2520representations%252C%2520based%2520on%2520identifying%2520latent%2520representational%2520dimensions%250Aunderlying%2520the%2520same%2520behavior%2520in%2520both%2520domains.%2520Applying%2520this%2520framework%2520to%2520humans%250Aand%2520a%2520deep%2520neural%2520network%2520%2528DNN%2529%2520model%2520of%2520natural%2520images%2520revealed%2520a%250Alow-dimensional%2520DNN%2520embedding%2520of%2520both%2520visual%2520and%2520semantic%2520dimensions.%2520In%250Acontrast%2520to%2520humans%252C%2520DNNs%2520exhibited%2520a%2520clear%2520dominance%2520of%2520visual%2520over%2520semantic%250Aproperties%252C%2520indicating%2520divergent%2520strategies%2520for%2520representing%2520images.%2520While%250Ain-silico%2520experiments%2520showed%2520seemingly%2520consistent%2520interpretability%2520of%2520DNN%250Adimensions%252C%2520a%2520direct%2520comparison%2520between%2520human%2520and%2520DNN%2520representations%2520revealed%250Asubstantial%2520differences%2520in%2520how%2520they%2520process%2520images.%2520By%2520making%2520representations%250Adirectly%2520comparable%252C%2520our%2520results%2520reveal%2520important%2520challenges%2520for%250Arepresentational%2520alignment%2520and%2520offer%2520a%2520means%2520for%2520improving%2520their%2520comparability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19087v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimensions%20underlying%20the%20representational%20alignment%20of%20deep%20neural%0A%20%20networks%20with%20humans&entry.906535625=Florian%20P.%20Mahner%20and%20Lukas%20Muttenthaler%20and%20Umut%20G%C3%BC%C3%A7l%C3%BC%20and%20Martin%20N.%20Hebart&entry.1292438233=%20%20Determining%20the%20similarities%20and%20differences%20between%20humans%20and%20artificial%0Aintelligence%20%28AI%29%20is%20an%20important%20goal%20both%20in%20computational%20cognitive%0Aneuroscience%20and%20machine%20learning%2C%20promising%20a%20deeper%20understanding%20of%20human%0Acognition%20and%20safer%2C%20more%20reliable%20AI%20systems.%20Much%20previous%20work%20comparing%0Arepresentations%20in%20humans%20and%20AI%20has%20relied%20on%20global%2C%20scalar%20measures%20to%0Aquantify%20their%20alignment.%20However%2C%20without%20explicit%20hypotheses%2C%20these%20measures%0Aonly%20inform%20us%20about%20the%20degree%20of%20alignment%2C%20not%20the%20factors%20that%20determine%0Ait.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20generic%20framework%20to%20compare%20human%0Aand%20AI%20representations%2C%20based%20on%20identifying%20latent%20representational%20dimensions%0Aunderlying%20the%20same%20behavior%20in%20both%20domains.%20Applying%20this%20framework%20to%20humans%0Aand%20a%20deep%20neural%20network%20%28DNN%29%20model%20of%20natural%20images%20revealed%20a%0Alow-dimensional%20DNN%20embedding%20of%20both%20visual%20and%20semantic%20dimensions.%20In%0Acontrast%20to%20humans%2C%20DNNs%20exhibited%20a%20clear%20dominance%20of%20visual%20over%20semantic%0Aproperties%2C%20indicating%20divergent%20strategies%20for%20representing%20images.%20While%0Ain-silico%20experiments%20showed%20seemingly%20consistent%20interpretability%20of%20DNN%0Adimensions%2C%20a%20direct%20comparison%20between%20human%20and%20DNN%20representations%20revealed%0Asubstantial%20differences%20in%20how%20they%20process%20images.%20By%20making%20representations%0Adirectly%20comparable%2C%20our%20results%20reveal%20important%20challenges%20for%0Arepresentational%20alignment%20and%20offer%20a%20means%20for%20improving%20their%20comparability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19087v2&entry.124074799=Read"},
{"title": "Enhancing Visual Inspection Capability of Multi-Modal Large Language\n  Models on Medical Time Series with Supportive Conformalized and Interpretable\n  Small Specialized Models", "author": "Huayu Li and Xiwen Chen and Ci Zhang and Stuart F. Quan and William D. S. Killgore and Shu-Fen Wung and Chen X. Chen and Geng Yuan and Jin Lu and Ao Li", "abstract": "  Large language models (LLMs) exhibit remarkable capabilities in visual\ninspection of medical time-series data, achieving proficiency comparable to\nhuman clinicians. However, their broad scope limits domain-specific precision,\nand proprietary weights hinder fine-tuning for specialized datasets. In\ncontrast, small specialized models (SSMs) excel in targeted tasks but lack the\ncontextual reasoning required for complex clinical decision-making. To address\nthese challenges, we propose ConMIL (Conformalized Multiple Instance Learning),\na decision-support SSM that integrates seamlessly with LLMs. By using Multiple\nInstance Learning (MIL) to identify clinically significant signal segments and\nconformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'\ninterpretative capabilities for medical time-series analysis. Experimental\nresults demonstrate that ConMIL significantly improves the performance of\nstate-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,\n\\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for\nconfident samples in arrhythmia detection and sleep staging, compared to\nstandalone LLM accuracy of 46.13% and 13.16%. These findings highlight the\npotential of ConMIL to bridge task-specific precision and broader contextual\nreasoning, enabling more reliable and interpretable AI-driven clinical decision\nsupport.\n", "link": "http://arxiv.org/abs/2501.16215v1", "date": "2025-01-27", "relevancy": 2.7023, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5356}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Visual%20Inspection%20Capability%20of%20Multi-Modal%20Large%20Language%0A%20%20Models%20on%20Medical%20Time%20Series%20with%20Supportive%20Conformalized%20and%20Interpretable%0A%20%20Small%20Specialized%20Models&body=Title%3A%20Enhancing%20Visual%20Inspection%20Capability%20of%20Multi-Modal%20Large%20Language%0A%20%20Models%20on%20Medical%20Time%20Series%20with%20Supportive%20Conformalized%20and%20Interpretable%0A%20%20Small%20Specialized%20Models%0AAuthor%3A%20Huayu%20Li%20and%20Xiwen%20Chen%20and%20Ci%20Zhang%20and%20Stuart%20F.%20Quan%20and%20William%20D.%20S.%20Killgore%20and%20Shu-Fen%20Wung%20and%20Chen%20X.%20Chen%20and%20Geng%20Yuan%20and%20Jin%20Lu%20and%20Ao%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%20in%20visual%0Ainspection%20of%20medical%20time-series%20data%2C%20achieving%20proficiency%20comparable%20to%0Ahuman%20clinicians.%20However%2C%20their%20broad%20scope%20limits%20domain-specific%20precision%2C%0Aand%20proprietary%20weights%20hinder%20fine-tuning%20for%20specialized%20datasets.%20In%0Acontrast%2C%20small%20specialized%20models%20%28SSMs%29%20excel%20in%20targeted%20tasks%20but%20lack%20the%0Acontextual%20reasoning%20required%20for%20complex%20clinical%20decision-making.%20To%20address%0Athese%20challenges%2C%20we%20propose%20ConMIL%20%28Conformalized%20Multiple%20Instance%20Learning%29%2C%0Aa%20decision-support%20SSM%20that%20integrates%20seamlessly%20with%20LLMs.%20By%20using%20Multiple%0AInstance%20Learning%20%28MIL%29%20to%20identify%20clinically%20significant%20signal%20segments%20and%0Aconformal%20prediction%20for%20calibrated%20set-valued%20outputs%2C%20ConMIL%20enhances%20LLMs%27%0Ainterpretative%20capabilities%20for%20medical%20time-series%20analysis.%20Experimental%0Aresults%20demonstrate%20that%20ConMIL%20significantly%20improves%20the%20performance%20of%0Astate-of-the-art%20LLMs%2C%20such%20as%20ChatGPT4.0%20and%20Qwen2-VL-7B.%20Specifically%2C%0A%5CConMIL%7B%7D-supported%20Qwen2-VL-7B%20achieves%2094.92%25%20and%2096.82%25%20precision%20for%0Aconfident%20samples%20in%20arrhythmia%20detection%20and%20sleep%20staging%2C%20compared%20to%0Astandalone%20LLM%20accuracy%20of%2046.13%25%20and%2013.16%25.%20These%20findings%20highlight%20the%0Apotential%20of%20ConMIL%20to%20bridge%20task-specific%20precision%20and%20broader%20contextual%0Areasoning%2C%20enabling%20more%20reliable%20and%20interpretable%20AI-driven%20clinical%20decision%0Asupport.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Visual%2520Inspection%2520Capability%2520of%2520Multi-Modal%2520Large%2520Language%250A%2520%2520Models%2520on%2520Medical%2520Time%2520Series%2520with%2520Supportive%2520Conformalized%2520and%2520Interpretable%250A%2520%2520Small%2520Specialized%2520Models%26entry.906535625%3DHuayu%2520Li%2520and%2520Xiwen%2520Chen%2520and%2520Ci%2520Zhang%2520and%2520Stuart%2520F.%2520Quan%2520and%2520William%2520D.%2520S.%2520Killgore%2520and%2520Shu-Fen%2520Wung%2520and%2520Chen%2520X.%2520Chen%2520and%2520Geng%2520Yuan%2520and%2520Jin%2520Lu%2520and%2520Ao%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520capabilities%2520in%2520visual%250Ainspection%2520of%2520medical%2520time-series%2520data%252C%2520achieving%2520proficiency%2520comparable%2520to%250Ahuman%2520clinicians.%2520However%252C%2520their%2520broad%2520scope%2520limits%2520domain-specific%2520precision%252C%250Aand%2520proprietary%2520weights%2520hinder%2520fine-tuning%2520for%2520specialized%2520datasets.%2520In%250Acontrast%252C%2520small%2520specialized%2520models%2520%2528SSMs%2529%2520excel%2520in%2520targeted%2520tasks%2520but%2520lack%2520the%250Acontextual%2520reasoning%2520required%2520for%2520complex%2520clinical%2520decision-making.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520ConMIL%2520%2528Conformalized%2520Multiple%2520Instance%2520Learning%2529%252C%250Aa%2520decision-support%2520SSM%2520that%2520integrates%2520seamlessly%2520with%2520LLMs.%2520By%2520using%2520Multiple%250AInstance%2520Learning%2520%2528MIL%2529%2520to%2520identify%2520clinically%2520significant%2520signal%2520segments%2520and%250Aconformal%2520prediction%2520for%2520calibrated%2520set-valued%2520outputs%252C%2520ConMIL%2520enhances%2520LLMs%2527%250Ainterpretative%2520capabilities%2520for%2520medical%2520time-series%2520analysis.%2520Experimental%250Aresults%2520demonstrate%2520that%2520ConMIL%2520significantly%2520improves%2520the%2520performance%2520of%250Astate-of-the-art%2520LLMs%252C%2520such%2520as%2520ChatGPT4.0%2520and%2520Qwen2-VL-7B.%2520Specifically%252C%250A%255CConMIL%257B%257D-supported%2520Qwen2-VL-7B%2520achieves%252094.92%2525%2520and%252096.82%2525%2520precision%2520for%250Aconfident%2520samples%2520in%2520arrhythmia%2520detection%2520and%2520sleep%2520staging%252C%2520compared%2520to%250Astandalone%2520LLM%2520accuracy%2520of%252046.13%2525%2520and%252013.16%2525.%2520These%2520findings%2520highlight%2520the%250Apotential%2520of%2520ConMIL%2520to%2520bridge%2520task-specific%2520precision%2520and%2520broader%2520contextual%250Areasoning%252C%2520enabling%2520more%2520reliable%2520and%2520interpretable%2520AI-driven%2520clinical%2520decision%250Asupport.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Visual%20Inspection%20Capability%20of%20Multi-Modal%20Large%20Language%0A%20%20Models%20on%20Medical%20Time%20Series%20with%20Supportive%20Conformalized%20and%20Interpretable%0A%20%20Small%20Specialized%20Models&entry.906535625=Huayu%20Li%20and%20Xiwen%20Chen%20and%20Ci%20Zhang%20and%20Stuart%20F.%20Quan%20and%20William%20D.%20S.%20Killgore%20and%20Shu-Fen%20Wung%20and%20Chen%20X.%20Chen%20and%20Geng%20Yuan%20and%20Jin%20Lu%20and%20Ao%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%20in%20visual%0Ainspection%20of%20medical%20time-series%20data%2C%20achieving%20proficiency%20comparable%20to%0Ahuman%20clinicians.%20However%2C%20their%20broad%20scope%20limits%20domain-specific%20precision%2C%0Aand%20proprietary%20weights%20hinder%20fine-tuning%20for%20specialized%20datasets.%20In%0Acontrast%2C%20small%20specialized%20models%20%28SSMs%29%20excel%20in%20targeted%20tasks%20but%20lack%20the%0Acontextual%20reasoning%20required%20for%20complex%20clinical%20decision-making.%20To%20address%0Athese%20challenges%2C%20we%20propose%20ConMIL%20%28Conformalized%20Multiple%20Instance%20Learning%29%2C%0Aa%20decision-support%20SSM%20that%20integrates%20seamlessly%20with%20LLMs.%20By%20using%20Multiple%0AInstance%20Learning%20%28MIL%29%20to%20identify%20clinically%20significant%20signal%20segments%20and%0Aconformal%20prediction%20for%20calibrated%20set-valued%20outputs%2C%20ConMIL%20enhances%20LLMs%27%0Ainterpretative%20capabilities%20for%20medical%20time-series%20analysis.%20Experimental%0Aresults%20demonstrate%20that%20ConMIL%20significantly%20improves%20the%20performance%20of%0Astate-of-the-art%20LLMs%2C%20such%20as%20ChatGPT4.0%20and%20Qwen2-VL-7B.%20Specifically%2C%0A%5CConMIL%7B%7D-supported%20Qwen2-VL-7B%20achieves%2094.92%25%20and%2096.82%25%20precision%20for%0Aconfident%20samples%20in%20arrhythmia%20detection%20and%20sleep%20staging%2C%20compared%20to%0Astandalone%20LLM%20accuracy%20of%2046.13%25%20and%2013.16%25.%20These%20findings%20highlight%20the%0Apotential%20of%20ConMIL%20to%20bridge%20task-specific%20precision%20and%20broader%20contextual%0Areasoning%2C%20enabling%20more%20reliable%20and%20interpretable%20AI-driven%20clinical%20decision%0Asupport.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16215v1&entry.124074799=Read"},
{"title": "LinPrim: Linear Primitives for Differentiable Volumetric Rendering", "author": "Nicolas von L\u00fctzow and Matthias Nie\u00dfner", "abstract": "  Volumetric rendering has become central to modern novel view synthesis\nmethods, which use differentiable rendering to optimize 3D scene\nrepresentations directly from observed views. While many recent works build on\nNeRF or 3D Gaussians, we explore an alternative volumetric scene\nrepresentation. More specifically, we introduce two new scene representations\nbased on linear primitives-octahedra and tetrahedra-both of which define\nhomogeneous volumes bounded by triangular faces. This formulation aligns\nnaturally with standard mesh-based tools, minimizing overhead for downstream\napplications. To optimize these primitives, we present a differentiable\nrasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based\noptimization while maintaining realtime rendering capabilities. Through\nexperiments on real-world datasets, we demonstrate comparable performance to\nstate-of-the-art volumetric methods while requiring fewer primitives to achieve\nsimilar reconstruction fidelity. Our findings provide insights into the\ngeometry of volumetric rendering and suggest that adopting explicit polyhedra\ncan expand the design space of scene representations.\n", "link": "http://arxiv.org/abs/2501.16312v1", "date": "2025-01-27", "relevancy": 2.6818, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5544}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5392}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LinPrim%3A%20Linear%20Primitives%20for%20Differentiable%20Volumetric%20Rendering&body=Title%3A%20LinPrim%3A%20Linear%20Primitives%20for%20Differentiable%20Volumetric%20Rendering%0AAuthor%3A%20Nicolas%20von%20L%C3%BCtzow%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20Volumetric%20rendering%20has%20become%20central%20to%20modern%20novel%20view%20synthesis%0Amethods%2C%20which%20use%20differentiable%20rendering%20to%20optimize%203D%20scene%0Arepresentations%20directly%20from%20observed%20views.%20While%20many%20recent%20works%20build%20on%0ANeRF%20or%203D%20Gaussians%2C%20we%20explore%20an%20alternative%20volumetric%20scene%0Arepresentation.%20More%20specifically%2C%20we%20introduce%20two%20new%20scene%20representations%0Abased%20on%20linear%20primitives-octahedra%20and%20tetrahedra-both%20of%20which%20define%0Ahomogeneous%20volumes%20bounded%20by%20triangular%20faces.%20This%20formulation%20aligns%0Anaturally%20with%20standard%20mesh-based%20tools%2C%20minimizing%20overhead%20for%20downstream%0Aapplications.%20To%20optimize%20these%20primitives%2C%20we%20present%20a%20differentiable%0Arasterizer%20that%20runs%20efficiently%20on%20GPUs%2C%20allowing%20end-to-end%20gradient-based%0Aoptimization%20while%20maintaining%20realtime%20rendering%20capabilities.%20Through%0Aexperiments%20on%20real-world%20datasets%2C%20we%20demonstrate%20comparable%20performance%20to%0Astate-of-the-art%20volumetric%20methods%20while%20requiring%20fewer%20primitives%20to%20achieve%0Asimilar%20reconstruction%20fidelity.%20Our%20findings%20provide%20insights%20into%20the%0Ageometry%20of%20volumetric%20rendering%20and%20suggest%20that%20adopting%20explicit%20polyhedra%0Acan%20expand%20the%20design%20space%20of%20scene%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinPrim%253A%2520Linear%2520Primitives%2520for%2520Differentiable%2520Volumetric%2520Rendering%26entry.906535625%3DNicolas%2520von%2520L%25C3%25BCtzow%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520Volumetric%2520rendering%2520has%2520become%2520central%2520to%2520modern%2520novel%2520view%2520synthesis%250Amethods%252C%2520which%2520use%2520differentiable%2520rendering%2520to%2520optimize%25203D%2520scene%250Arepresentations%2520directly%2520from%2520observed%2520views.%2520While%2520many%2520recent%2520works%2520build%2520on%250ANeRF%2520or%25203D%2520Gaussians%252C%2520we%2520explore%2520an%2520alternative%2520volumetric%2520scene%250Arepresentation.%2520More%2520specifically%252C%2520we%2520introduce%2520two%2520new%2520scene%2520representations%250Abased%2520on%2520linear%2520primitives-octahedra%2520and%2520tetrahedra-both%2520of%2520which%2520define%250Ahomogeneous%2520volumes%2520bounded%2520by%2520triangular%2520faces.%2520This%2520formulation%2520aligns%250Anaturally%2520with%2520standard%2520mesh-based%2520tools%252C%2520minimizing%2520overhead%2520for%2520downstream%250Aapplications.%2520To%2520optimize%2520these%2520primitives%252C%2520we%2520present%2520a%2520differentiable%250Arasterizer%2520that%2520runs%2520efficiently%2520on%2520GPUs%252C%2520allowing%2520end-to-end%2520gradient-based%250Aoptimization%2520while%2520maintaining%2520realtime%2520rendering%2520capabilities.%2520Through%250Aexperiments%2520on%2520real-world%2520datasets%252C%2520we%2520demonstrate%2520comparable%2520performance%2520to%250Astate-of-the-art%2520volumetric%2520methods%2520while%2520requiring%2520fewer%2520primitives%2520to%2520achieve%250Asimilar%2520reconstruction%2520fidelity.%2520Our%2520findings%2520provide%2520insights%2520into%2520the%250Ageometry%2520of%2520volumetric%2520rendering%2520and%2520suggest%2520that%2520adopting%2520explicit%2520polyhedra%250Acan%2520expand%2520the%2520design%2520space%2520of%2520scene%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LinPrim%3A%20Linear%20Primitives%20for%20Differentiable%20Volumetric%20Rendering&entry.906535625=Nicolas%20von%20L%C3%BCtzow%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20Volumetric%20rendering%20has%20become%20central%20to%20modern%20novel%20view%20synthesis%0Amethods%2C%20which%20use%20differentiable%20rendering%20to%20optimize%203D%20scene%0Arepresentations%20directly%20from%20observed%20views.%20While%20many%20recent%20works%20build%20on%0ANeRF%20or%203D%20Gaussians%2C%20we%20explore%20an%20alternative%20volumetric%20scene%0Arepresentation.%20More%20specifically%2C%20we%20introduce%20two%20new%20scene%20representations%0Abased%20on%20linear%20primitives-octahedra%20and%20tetrahedra-both%20of%20which%20define%0Ahomogeneous%20volumes%20bounded%20by%20triangular%20faces.%20This%20formulation%20aligns%0Anaturally%20with%20standard%20mesh-based%20tools%2C%20minimizing%20overhead%20for%20downstream%0Aapplications.%20To%20optimize%20these%20primitives%2C%20we%20present%20a%20differentiable%0Arasterizer%20that%20runs%20efficiently%20on%20GPUs%2C%20allowing%20end-to-end%20gradient-based%0Aoptimization%20while%20maintaining%20realtime%20rendering%20capabilities.%20Through%0Aexperiments%20on%20real-world%20datasets%2C%20we%20demonstrate%20comparable%20performance%20to%0Astate-of-the-art%20volumetric%20methods%20while%20requiring%20fewer%20primitives%20to%20achieve%0Asimilar%20reconstruction%20fidelity.%20Our%20findings%20provide%20insights%20into%20the%0Ageometry%20of%20volumetric%20rendering%20and%20suggest%20that%20adopting%20explicit%20polyhedra%0Acan%20expand%20the%20design%20space%20of%20scene%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16312v1&entry.124074799=Read"},
{"title": "MetaDecorator: Generating Immersive Virtual Tours through Multimodality", "author": "Shuang Xie and Yang Liu and Jeannie S. A. Lee and Haiwei Dong", "abstract": "  MetaDecorator, is a framework that empowers users to personalize virtual\nspaces. By leveraging text-driven prompts and image synthesis techniques,\nMetaDecorator adorns static panoramas captured by 360{\\deg} imaging devices,\ntransforming them into uniquely styled and visually appealing environments.\nThis significantly enhances the realism and engagement of virtual tours\ncompared to traditional offerings. Beyond the core framework, we also discuss\nthe integration of Large Language Models (LLMs) and haptics in the VR\napplication to provide a more immersive experience.\n", "link": "http://arxiv.org/abs/2501.16164v1", "date": "2025-01-27", "relevancy": 2.6559, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5382}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5382}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaDecorator%3A%20Generating%20Immersive%20Virtual%20Tours%20through%20Multimodality&body=Title%3A%20MetaDecorator%3A%20Generating%20Immersive%20Virtual%20Tours%20through%20Multimodality%0AAuthor%3A%20Shuang%20Xie%20and%20Yang%20Liu%20and%20Jeannie%20S.%20A.%20Lee%20and%20Haiwei%20Dong%0AAbstract%3A%20%20%20MetaDecorator%2C%20is%20a%20framework%20that%20empowers%20users%20to%20personalize%20virtual%0Aspaces.%20By%20leveraging%20text-driven%20prompts%20and%20image%20synthesis%20techniques%2C%0AMetaDecorator%20adorns%20static%20panoramas%20captured%20by%20360%7B%5Cdeg%7D%20imaging%20devices%2C%0Atransforming%20them%20into%20uniquely%20styled%20and%20visually%20appealing%20environments.%0AThis%20significantly%20enhances%20the%20realism%20and%20engagement%20of%20virtual%20tours%0Acompared%20to%20traditional%20offerings.%20Beyond%20the%20core%20framework%2C%20we%20also%20discuss%0Athe%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20haptics%20in%20the%20VR%0Aapplication%20to%20provide%20a%20more%20immersive%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaDecorator%253A%2520Generating%2520Immersive%2520Virtual%2520Tours%2520through%2520Multimodality%26entry.906535625%3DShuang%2520Xie%2520and%2520Yang%2520Liu%2520and%2520Jeannie%2520S.%2520A.%2520Lee%2520and%2520Haiwei%2520Dong%26entry.1292438233%3D%2520%2520MetaDecorator%252C%2520is%2520a%2520framework%2520that%2520empowers%2520users%2520to%2520personalize%2520virtual%250Aspaces.%2520By%2520leveraging%2520text-driven%2520prompts%2520and%2520image%2520synthesis%2520techniques%252C%250AMetaDecorator%2520adorns%2520static%2520panoramas%2520captured%2520by%2520360%257B%255Cdeg%257D%2520imaging%2520devices%252C%250Atransforming%2520them%2520into%2520uniquely%2520styled%2520and%2520visually%2520appealing%2520environments.%250AThis%2520significantly%2520enhances%2520the%2520realism%2520and%2520engagement%2520of%2520virtual%2520tours%250Acompared%2520to%2520traditional%2520offerings.%2520Beyond%2520the%2520core%2520framework%252C%2520we%2520also%2520discuss%250Athe%2520integration%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520haptics%2520in%2520the%2520VR%250Aapplication%2520to%2520provide%2520a%2520more%2520immersive%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaDecorator%3A%20Generating%20Immersive%20Virtual%20Tours%20through%20Multimodality&entry.906535625=Shuang%20Xie%20and%20Yang%20Liu%20and%20Jeannie%20S.%20A.%20Lee%20and%20Haiwei%20Dong&entry.1292438233=%20%20MetaDecorator%2C%20is%20a%20framework%20that%20empowers%20users%20to%20personalize%20virtual%0Aspaces.%20By%20leveraging%20text-driven%20prompts%20and%20image%20synthesis%20techniques%2C%0AMetaDecorator%20adorns%20static%20panoramas%20captured%20by%20360%7B%5Cdeg%7D%20imaging%20devices%2C%0Atransforming%20them%20into%20uniquely%20styled%20and%20visually%20appealing%20environments.%0AThis%20significantly%20enhances%20the%20realism%20and%20engagement%20of%20virtual%20tours%0Acompared%20to%20traditional%20offerings.%20Beyond%20the%20core%20framework%2C%20we%20also%20discuss%0Athe%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20haptics%20in%20the%20VR%0Aapplication%20to%20provide%20a%20more%20immersive%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16164v1&entry.124074799=Read"},
{"title": "CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention", "author": "Alexandru Dimofte and Glenn Anta Bucagu and Thorir Mar Ingolfsson and Xiaying Wang and Andrea Cossettini and Luca Benini and Yawei Li", "abstract": "  Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.\n", "link": "http://arxiv.org/abs/2501.10885v2", "date": "2025-01-27", "relevancy": 2.6172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CEReBrO%3A%20Compact%20Encoder%20for%20Representations%20of%20Brain%20Oscillations%20Using%0A%20%20Efficient%20Alternating%20Attention&body=Title%3A%20CEReBrO%3A%20Compact%20Encoder%20for%20Representations%20of%20Brain%20Oscillations%20Using%0A%20%20Efficient%20Alternating%20Attention%0AAuthor%3A%20Alexandru%20Dimofte%20and%20Glenn%20Anta%20Bucagu%20and%20Thorir%20Mar%20Ingolfsson%20and%20Xiaying%20Wang%20and%20Andrea%20Cossettini%20and%20Luca%20Benini%20and%20Yawei%20Li%0AAbstract%3A%20%20%20Electroencephalograph%20%28EEG%29%20is%20a%20crucial%20tool%20for%20studying%20brain%20activity.%0ARecently%2C%20self-supervised%20learning%20methods%20leveraging%20large%20unlabeled%20datasets%0Ahave%20emerged%20as%20a%20potential%20solution%20to%20the%20scarcity%20of%20widely%20available%0Aannotated%20EEG%20data.%20However%2C%20current%20methods%20suffer%20from%20at%20least%20one%20of%20the%0Afollowing%20limitations%3A%20i%29%20sub-optimal%20EEG%20signal%20modeling%2C%20ii%29%20model%20sizes%20in%0Athe%20hundreds%20of%20millions%20of%20trainable%20parameters%2C%20and%20iii%29%20reliance%20on%20private%0Adatasets%20and/or%20inconsistent%20public%20benchmarks%2C%20hindering%20reproducibility.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20Compact%20Encoder%20for%20Representations%20of%0ABrain%20Oscillations%20using%20alternating%20attention%20%28CEReBrO%29%2C%20a%20new%20small%20EEG%0Afoundation%20model.%20Our%20tokenization%20scheme%20represents%20EEG%20signals%20at%20a%0Aper-channel%20patch%20granularity.%20We%20propose%20an%20alternating%20attention%20mechanism%0Athat%20jointly%20models%20intra-channel%20temporal%20dynamics%20and%20inter-channel%20spatial%0Acorrelations%2C%20achieving%202x%20speed%20improvement%20with%206x%20less%20memory%20required%0Acompared%20to%20standard%20self-attention.%20We%20present%20several%20model%20sizes%20ranging%0Afrom%203.6%20million%20to%2085%20million%20parameters.%20Pre-trained%20on%20over%2020%2C000%20hours%20of%0Apublicly%20available%20scalp%20EEG%20recordings%20with%20diverse%20channel%20configurations%2C%0Aour%20models%20set%20new%20benchmarks%20in%20emotion%20detection%20and%20seizure%20detection%20tasks%2C%0Awith%20competitive%20performance%20in%20anomaly%20classification%20and%20gait%20prediction.%0AThis%20validates%20our%20models%27%20effectiveness%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10885v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCEReBrO%253A%2520Compact%2520Encoder%2520for%2520Representations%2520of%2520Brain%2520Oscillations%2520Using%250A%2520%2520Efficient%2520Alternating%2520Attention%26entry.906535625%3DAlexandru%2520Dimofte%2520and%2520Glenn%2520Anta%2520Bucagu%2520and%2520Thorir%2520Mar%2520Ingolfsson%2520and%2520Xiaying%2520Wang%2520and%2520Andrea%2520Cossettini%2520and%2520Luca%2520Benini%2520and%2520Yawei%2520Li%26entry.1292438233%3D%2520%2520Electroencephalograph%2520%2528EEG%2529%2520is%2520a%2520crucial%2520tool%2520for%2520studying%2520brain%2520activity.%250ARecently%252C%2520self-supervised%2520learning%2520methods%2520leveraging%2520large%2520unlabeled%2520datasets%250Ahave%2520emerged%2520as%2520a%2520potential%2520solution%2520to%2520the%2520scarcity%2520of%2520widely%2520available%250Aannotated%2520EEG%2520data.%2520However%252C%2520current%2520methods%2520suffer%2520from%2520at%2520least%2520one%2520of%2520the%250Afollowing%2520limitations%253A%2520i%2529%2520sub-optimal%2520EEG%2520signal%2520modeling%252C%2520ii%2529%2520model%2520sizes%2520in%250Athe%2520hundreds%2520of%2520millions%2520of%2520trainable%2520parameters%252C%2520and%2520iii%2529%2520reliance%2520on%2520private%250Adatasets%2520and/or%2520inconsistent%2520public%2520benchmarks%252C%2520hindering%2520reproducibility.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520Compact%2520Encoder%2520for%2520Representations%2520of%250ABrain%2520Oscillations%2520using%2520alternating%2520attention%2520%2528CEReBrO%2529%252C%2520a%2520new%2520small%2520EEG%250Afoundation%2520model.%2520Our%2520tokenization%2520scheme%2520represents%2520EEG%2520signals%2520at%2520a%250Aper-channel%2520patch%2520granularity.%2520We%2520propose%2520an%2520alternating%2520attention%2520mechanism%250Athat%2520jointly%2520models%2520intra-channel%2520temporal%2520dynamics%2520and%2520inter-channel%2520spatial%250Acorrelations%252C%2520achieving%25202x%2520speed%2520improvement%2520with%25206x%2520less%2520memory%2520required%250Acompared%2520to%2520standard%2520self-attention.%2520We%2520present%2520several%2520model%2520sizes%2520ranging%250Afrom%25203.6%2520million%2520to%252085%2520million%2520parameters.%2520Pre-trained%2520on%2520over%252020%252C000%2520hours%2520of%250Apublicly%2520available%2520scalp%2520EEG%2520recordings%2520with%2520diverse%2520channel%2520configurations%252C%250Aour%2520models%2520set%2520new%2520benchmarks%2520in%2520emotion%2520detection%2520and%2520seizure%2520detection%2520tasks%252C%250Awith%2520competitive%2520performance%2520in%2520anomaly%2520classification%2520and%2520gait%2520prediction.%250AThis%2520validates%2520our%2520models%2527%2520effectiveness%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10885v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CEReBrO%3A%20Compact%20Encoder%20for%20Representations%20of%20Brain%20Oscillations%20Using%0A%20%20Efficient%20Alternating%20Attention&entry.906535625=Alexandru%20Dimofte%20and%20Glenn%20Anta%20Bucagu%20and%20Thorir%20Mar%20Ingolfsson%20and%20Xiaying%20Wang%20and%20Andrea%20Cossettini%20and%20Luca%20Benini%20and%20Yawei%20Li&entry.1292438233=%20%20Electroencephalograph%20%28EEG%29%20is%20a%20crucial%20tool%20for%20studying%20brain%20activity.%0ARecently%2C%20self-supervised%20learning%20methods%20leveraging%20large%20unlabeled%20datasets%0Ahave%20emerged%20as%20a%20potential%20solution%20to%20the%20scarcity%20of%20widely%20available%0Aannotated%20EEG%20data.%20However%2C%20current%20methods%20suffer%20from%20at%20least%20one%20of%20the%0Afollowing%20limitations%3A%20i%29%20sub-optimal%20EEG%20signal%20modeling%2C%20ii%29%20model%20sizes%20in%0Athe%20hundreds%20of%20millions%20of%20trainable%20parameters%2C%20and%20iii%29%20reliance%20on%20private%0Adatasets%20and/or%20inconsistent%20public%20benchmarks%2C%20hindering%20reproducibility.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20Compact%20Encoder%20for%20Representations%20of%0ABrain%20Oscillations%20using%20alternating%20attention%20%28CEReBrO%29%2C%20a%20new%20small%20EEG%0Afoundation%20model.%20Our%20tokenization%20scheme%20represents%20EEG%20signals%20at%20a%0Aper-channel%20patch%20granularity.%20We%20propose%20an%20alternating%20attention%20mechanism%0Athat%20jointly%20models%20intra-channel%20temporal%20dynamics%20and%20inter-channel%20spatial%0Acorrelations%2C%20achieving%202x%20speed%20improvement%20with%206x%20less%20memory%20required%0Acompared%20to%20standard%20self-attention.%20We%20present%20several%20model%20sizes%20ranging%0Afrom%203.6%20million%20to%2085%20million%20parameters.%20Pre-trained%20on%20over%2020%2C000%20hours%20of%0Apublicly%20available%20scalp%20EEG%20recordings%20with%20diverse%20channel%20configurations%2C%0Aour%20models%20set%20new%20benchmarks%20in%20emotion%20detection%20and%20seizure%20detection%20tasks%2C%0Awith%20competitive%20performance%20in%20anomaly%20classification%20and%20gait%20prediction.%0AThis%20validates%20our%20models%27%20effectiveness%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10885v2&entry.124074799=Read"},
{"title": "Enhancing Brain Age Estimation with a Multimodal 3D CNN Approach\n  Combining Structural MRI and AI-Synthesized Cerebral Blood Volume Data", "author": "Jordan Jomsky and Zongyu Li and Yiren Zhang and Tal Nuriel and Jia Guo", "abstract": "  The increasing global aging population necessitates improved methods to\nassess brain aging and its related neurodegenerative changes. Brain Age Gap\nEstimation (BrainAGE) offers a neuroimaging biomarker for understanding these\nchanges by predicting brain age from MRI scans. Current approaches primarily\nuse T1-weighted magnetic resonance imaging (T1w MRI) data, capturing only\nstructural brain information. To address this limitation, AI-generated Cerebral\nBlood Volume (AICBV) data, synthesized from non-contrast MRI scans, offers\nfunctional insights by revealing subtle blood-tissue contrasts otherwise\nundetectable in standard imaging. We integrated AICBV with T1w MRI to predict\nbrain age, combining both structural and functional metrics. We developed a\ndeep learning model using a VGG-based architecture for both modalities and\ncombined their predictions using linear regression. Our model achieved a mean\nabsolute error (MAE) of 3.95 years and an $R^2$ of 0.943 on the test set ($n =\n288$), outperforming existing models trained on similar data. We have further\ncreated gradient-based class activation maps (Grad-CAM) to visualize the\nregions of the brain that most influenced the model's predictions, providing\ninterpretable insights into the structural and functional contributors to brain\naging.\n", "link": "http://arxiv.org/abs/2412.01865v3", "date": "2025-01-27", "relevancy": 2.5592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5229}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5063}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Brain%20Age%20Estimation%20with%20a%20Multimodal%203D%20CNN%20Approach%0A%20%20Combining%20Structural%20MRI%20and%20AI-Synthesized%20Cerebral%20Blood%20Volume%20Data&body=Title%3A%20Enhancing%20Brain%20Age%20Estimation%20with%20a%20Multimodal%203D%20CNN%20Approach%0A%20%20Combining%20Structural%20MRI%20and%20AI-Synthesized%20Cerebral%20Blood%20Volume%20Data%0AAuthor%3A%20Jordan%20Jomsky%20and%20Zongyu%20Li%20and%20Yiren%20Zhang%20and%20Tal%20Nuriel%20and%20Jia%20Guo%0AAbstract%3A%20%20%20The%20increasing%20global%20aging%20population%20necessitates%20improved%20methods%20to%0Aassess%20brain%20aging%20and%20its%20related%20neurodegenerative%20changes.%20Brain%20Age%20Gap%0AEstimation%20%28BrainAGE%29%20offers%20a%20neuroimaging%20biomarker%20for%20understanding%20these%0Achanges%20by%20predicting%20brain%20age%20from%20MRI%20scans.%20Current%20approaches%20primarily%0Ause%20T1-weighted%20magnetic%20resonance%20imaging%20%28T1w%20MRI%29%20data%2C%20capturing%20only%0Astructural%20brain%20information.%20To%20address%20this%20limitation%2C%20AI-generated%20Cerebral%0ABlood%20Volume%20%28AICBV%29%20data%2C%20synthesized%20from%20non-contrast%20MRI%20scans%2C%20offers%0Afunctional%20insights%20by%20revealing%20subtle%20blood-tissue%20contrasts%20otherwise%0Aundetectable%20in%20standard%20imaging.%20We%20integrated%20AICBV%20with%20T1w%20MRI%20to%20predict%0Abrain%20age%2C%20combining%20both%20structural%20and%20functional%20metrics.%20We%20developed%20a%0Adeep%20learning%20model%20using%20a%20VGG-based%20architecture%20for%20both%20modalities%20and%0Acombined%20their%20predictions%20using%20linear%20regression.%20Our%20model%20achieved%20a%20mean%0Aabsolute%20error%20%28MAE%29%20of%203.95%20years%20and%20an%20%24R%5E2%24%20of%200.943%20on%20the%20test%20set%20%28%24n%20%3D%0A288%24%29%2C%20outperforming%20existing%20models%20trained%20on%20similar%20data.%20We%20have%20further%0Acreated%20gradient-based%20class%20activation%20maps%20%28Grad-CAM%29%20to%20visualize%20the%0Aregions%20of%20the%20brain%20that%20most%20influenced%20the%20model%27s%20predictions%2C%20providing%0Ainterpretable%20insights%20into%20the%20structural%20and%20functional%20contributors%20to%20brain%0Aaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01865v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Brain%2520Age%2520Estimation%2520with%2520a%2520Multimodal%25203D%2520CNN%2520Approach%250A%2520%2520Combining%2520Structural%2520MRI%2520and%2520AI-Synthesized%2520Cerebral%2520Blood%2520Volume%2520Data%26entry.906535625%3DJordan%2520Jomsky%2520and%2520Zongyu%2520Li%2520and%2520Yiren%2520Zhang%2520and%2520Tal%2520Nuriel%2520and%2520Jia%2520Guo%26entry.1292438233%3D%2520%2520The%2520increasing%2520global%2520aging%2520population%2520necessitates%2520improved%2520methods%2520to%250Aassess%2520brain%2520aging%2520and%2520its%2520related%2520neurodegenerative%2520changes.%2520Brain%2520Age%2520Gap%250AEstimation%2520%2528BrainAGE%2529%2520offers%2520a%2520neuroimaging%2520biomarker%2520for%2520understanding%2520these%250Achanges%2520by%2520predicting%2520brain%2520age%2520from%2520MRI%2520scans.%2520Current%2520approaches%2520primarily%250Ause%2520T1-weighted%2520magnetic%2520resonance%2520imaging%2520%2528T1w%2520MRI%2529%2520data%252C%2520capturing%2520only%250Astructural%2520brain%2520information.%2520To%2520address%2520this%2520limitation%252C%2520AI-generated%2520Cerebral%250ABlood%2520Volume%2520%2528AICBV%2529%2520data%252C%2520synthesized%2520from%2520non-contrast%2520MRI%2520scans%252C%2520offers%250Afunctional%2520insights%2520by%2520revealing%2520subtle%2520blood-tissue%2520contrasts%2520otherwise%250Aundetectable%2520in%2520standard%2520imaging.%2520We%2520integrated%2520AICBV%2520with%2520T1w%2520MRI%2520to%2520predict%250Abrain%2520age%252C%2520combining%2520both%2520structural%2520and%2520functional%2520metrics.%2520We%2520developed%2520a%250Adeep%2520learning%2520model%2520using%2520a%2520VGG-based%2520architecture%2520for%2520both%2520modalities%2520and%250Acombined%2520their%2520predictions%2520using%2520linear%2520regression.%2520Our%2520model%2520achieved%2520a%2520mean%250Aabsolute%2520error%2520%2528MAE%2529%2520of%25203.95%2520years%2520and%2520an%2520%2524R%255E2%2524%2520of%25200.943%2520on%2520the%2520test%2520set%2520%2528%2524n%2520%253D%250A288%2524%2529%252C%2520outperforming%2520existing%2520models%2520trained%2520on%2520similar%2520data.%2520We%2520have%2520further%250Acreated%2520gradient-based%2520class%2520activation%2520maps%2520%2528Grad-CAM%2529%2520to%2520visualize%2520the%250Aregions%2520of%2520the%2520brain%2520that%2520most%2520influenced%2520the%2520model%2527s%2520predictions%252C%2520providing%250Ainterpretable%2520insights%2520into%2520the%2520structural%2520and%2520functional%2520contributors%2520to%2520brain%250Aaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01865v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Brain%20Age%20Estimation%20with%20a%20Multimodal%203D%20CNN%20Approach%0A%20%20Combining%20Structural%20MRI%20and%20AI-Synthesized%20Cerebral%20Blood%20Volume%20Data&entry.906535625=Jordan%20Jomsky%20and%20Zongyu%20Li%20and%20Yiren%20Zhang%20and%20Tal%20Nuriel%20and%20Jia%20Guo&entry.1292438233=%20%20The%20increasing%20global%20aging%20population%20necessitates%20improved%20methods%20to%0Aassess%20brain%20aging%20and%20its%20related%20neurodegenerative%20changes.%20Brain%20Age%20Gap%0AEstimation%20%28BrainAGE%29%20offers%20a%20neuroimaging%20biomarker%20for%20understanding%20these%0Achanges%20by%20predicting%20brain%20age%20from%20MRI%20scans.%20Current%20approaches%20primarily%0Ause%20T1-weighted%20magnetic%20resonance%20imaging%20%28T1w%20MRI%29%20data%2C%20capturing%20only%0Astructural%20brain%20information.%20To%20address%20this%20limitation%2C%20AI-generated%20Cerebral%0ABlood%20Volume%20%28AICBV%29%20data%2C%20synthesized%20from%20non-contrast%20MRI%20scans%2C%20offers%0Afunctional%20insights%20by%20revealing%20subtle%20blood-tissue%20contrasts%20otherwise%0Aundetectable%20in%20standard%20imaging.%20We%20integrated%20AICBV%20with%20T1w%20MRI%20to%20predict%0Abrain%20age%2C%20combining%20both%20structural%20and%20functional%20metrics.%20We%20developed%20a%0Adeep%20learning%20model%20using%20a%20VGG-based%20architecture%20for%20both%20modalities%20and%0Acombined%20their%20predictions%20using%20linear%20regression.%20Our%20model%20achieved%20a%20mean%0Aabsolute%20error%20%28MAE%29%20of%203.95%20years%20and%20an%20%24R%5E2%24%20of%200.943%20on%20the%20test%20set%20%28%24n%20%3D%0A288%24%29%2C%20outperforming%20existing%20models%20trained%20on%20similar%20data.%20We%20have%20further%0Acreated%20gradient-based%20class%20activation%20maps%20%28Grad-CAM%29%20to%20visualize%20the%0Aregions%20of%20the%20brain%20that%20most%20influenced%20the%20model%27s%20predictions%2C%20providing%0Ainterpretable%20insights%20into%20the%20structural%20and%20functional%20contributors%20to%20brain%0Aaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01865v3&entry.124074799=Read"},
{"title": "Controllable Forgetting Mechanism for Few-Shot Class-Incremental\n  Learning", "author": "Kirill Paramonov and Mete Ozay and Eunju Yang and Jijoong Moon and Umberto Michieli", "abstract": "  Class-incremental learning in the context of limited personal labeled samples\n(few-shot) is critical for numerous real-world applications, such as smart home\ndevices. A key challenge in these scenarios is balancing the trade-off between\nadapting to new, personalized classes and maintaining the performance of the\nmodel on the original, base classes. Fine-tuning the model on novel classes\noften leads to the phenomenon of catastrophic forgetting, where the accuracy of\nbase classes declines unpredictably and significantly. In this paper, we\npropose a simple yet effective mechanism to address this challenge by\ncontrolling the trade-off between novel and base class accuracy. We\nspecifically target the ultra-low-shot scenario, where only a single example is\navailable per novel class. Our approach introduces a Novel Class Detection\n(NCD) rule, which adjusts the degree of forgetting a priori while\nsimultaneously enhancing performance on novel classes. We demonstrate the\nversatility of our solution by applying it to state-of-the-art Few-Shot\nClass-Incremental Learning (FSCIL) methods, showing consistent improvements\nacross different settings. To better quantify the trade-off between novel and\nbase class performance, we introduce new metrics: NCR@2FOR and NCR@5FOR. Our\napproach achieves up to a 30% improvement in novel class accuracy on the\nCIFAR100 dataset (1-shot, 1 novel class) while maintaining a controlled base\nclass forgetting rate of 2%.\n", "link": "http://arxiv.org/abs/2501.15998v1", "date": "2025-01-27", "relevancy": 2.5555, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5308}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5013}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Forgetting%20Mechanism%20for%20Few-Shot%20Class-Incremental%0A%20%20Learning&body=Title%3A%20Controllable%20Forgetting%20Mechanism%20for%20Few-Shot%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Kirill%20Paramonov%20and%20Mete%20Ozay%20and%20Eunju%20Yang%20and%20Jijoong%20Moon%20and%20Umberto%20Michieli%0AAbstract%3A%20%20%20Class-incremental%20learning%20in%20the%20context%20of%20limited%20personal%20labeled%20samples%0A%28few-shot%29%20is%20critical%20for%20numerous%20real-world%20applications%2C%20such%20as%20smart%20home%0Adevices.%20A%20key%20challenge%20in%20these%20scenarios%20is%20balancing%20the%20trade-off%20between%0Aadapting%20to%20new%2C%20personalized%20classes%20and%20maintaining%20the%20performance%20of%20the%0Amodel%20on%20the%20original%2C%20base%20classes.%20Fine-tuning%20the%20model%20on%20novel%20classes%0Aoften%20leads%20to%20the%20phenomenon%20of%20catastrophic%20forgetting%2C%20where%20the%20accuracy%20of%0Abase%20classes%20declines%20unpredictably%20and%20significantly.%20In%20this%20paper%2C%20we%0Apropose%20a%20simple%20yet%20effective%20mechanism%20to%20address%20this%20challenge%20by%0Acontrolling%20the%20trade-off%20between%20novel%20and%20base%20class%20accuracy.%20We%0Aspecifically%20target%20the%20ultra-low-shot%20scenario%2C%20where%20only%20a%20single%20example%20is%0Aavailable%20per%20novel%20class.%20Our%20approach%20introduces%20a%20Novel%20Class%20Detection%0A%28NCD%29%20rule%2C%20which%20adjusts%20the%20degree%20of%20forgetting%20a%20priori%20while%0Asimultaneously%20enhancing%20performance%20on%20novel%20classes.%20We%20demonstrate%20the%0Aversatility%20of%20our%20solution%20by%20applying%20it%20to%20state-of-the-art%20Few-Shot%0AClass-Incremental%20Learning%20%28FSCIL%29%20methods%2C%20showing%20consistent%20improvements%0Aacross%20different%20settings.%20To%20better%20quantify%20the%20trade-off%20between%20novel%20and%0Abase%20class%20performance%2C%20we%20introduce%20new%20metrics%3A%20NCR%402FOR%20and%20NCR%405FOR.%20Our%0Aapproach%20achieves%20up%20to%20a%2030%25%20improvement%20in%20novel%20class%20accuracy%20on%20the%0ACIFAR100%20dataset%20%281-shot%2C%201%20novel%20class%29%20while%20maintaining%20a%20controlled%20base%0Aclass%20forgetting%20rate%20of%202%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Forgetting%2520Mechanism%2520for%2520Few-Shot%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DKirill%2520Paramonov%2520and%2520Mete%2520Ozay%2520and%2520Eunju%2520Yang%2520and%2520Jijoong%2520Moon%2520and%2520Umberto%2520Michieli%26entry.1292438233%3D%2520%2520Class-incremental%2520learning%2520in%2520the%2520context%2520of%2520limited%2520personal%2520labeled%2520samples%250A%2528few-shot%2529%2520is%2520critical%2520for%2520numerous%2520real-world%2520applications%252C%2520such%2520as%2520smart%2520home%250Adevices.%2520A%2520key%2520challenge%2520in%2520these%2520scenarios%2520is%2520balancing%2520the%2520trade-off%2520between%250Aadapting%2520to%2520new%252C%2520personalized%2520classes%2520and%2520maintaining%2520the%2520performance%2520of%2520the%250Amodel%2520on%2520the%2520original%252C%2520base%2520classes.%2520Fine-tuning%2520the%2520model%2520on%2520novel%2520classes%250Aoften%2520leads%2520to%2520the%2520phenomenon%2520of%2520catastrophic%2520forgetting%252C%2520where%2520the%2520accuracy%2520of%250Abase%2520classes%2520declines%2520unpredictably%2520and%2520significantly.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520simple%2520yet%2520effective%2520mechanism%2520to%2520address%2520this%2520challenge%2520by%250Acontrolling%2520the%2520trade-off%2520between%2520novel%2520and%2520base%2520class%2520accuracy.%2520We%250Aspecifically%2520target%2520the%2520ultra-low-shot%2520scenario%252C%2520where%2520only%2520a%2520single%2520example%2520is%250Aavailable%2520per%2520novel%2520class.%2520Our%2520approach%2520introduces%2520a%2520Novel%2520Class%2520Detection%250A%2528NCD%2529%2520rule%252C%2520which%2520adjusts%2520the%2520degree%2520of%2520forgetting%2520a%2520priori%2520while%250Asimultaneously%2520enhancing%2520performance%2520on%2520novel%2520classes.%2520We%2520demonstrate%2520the%250Aversatility%2520of%2520our%2520solution%2520by%2520applying%2520it%2520to%2520state-of-the-art%2520Few-Shot%250AClass-Incremental%2520Learning%2520%2528FSCIL%2529%2520methods%252C%2520showing%2520consistent%2520improvements%250Aacross%2520different%2520settings.%2520To%2520better%2520quantify%2520the%2520trade-off%2520between%2520novel%2520and%250Abase%2520class%2520performance%252C%2520we%2520introduce%2520new%2520metrics%253A%2520NCR%25402FOR%2520and%2520NCR%25405FOR.%2520Our%250Aapproach%2520achieves%2520up%2520to%2520a%252030%2525%2520improvement%2520in%2520novel%2520class%2520accuracy%2520on%2520the%250ACIFAR100%2520dataset%2520%25281-shot%252C%25201%2520novel%2520class%2529%2520while%2520maintaining%2520a%2520controlled%2520base%250Aclass%2520forgetting%2520rate%2520of%25202%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Forgetting%20Mechanism%20for%20Few-Shot%20Class-Incremental%0A%20%20Learning&entry.906535625=Kirill%20Paramonov%20and%20Mete%20Ozay%20and%20Eunju%20Yang%20and%20Jijoong%20Moon%20and%20Umberto%20Michieli&entry.1292438233=%20%20Class-incremental%20learning%20in%20the%20context%20of%20limited%20personal%20labeled%20samples%0A%28few-shot%29%20is%20critical%20for%20numerous%20real-world%20applications%2C%20such%20as%20smart%20home%0Adevices.%20A%20key%20challenge%20in%20these%20scenarios%20is%20balancing%20the%20trade-off%20between%0Aadapting%20to%20new%2C%20personalized%20classes%20and%20maintaining%20the%20performance%20of%20the%0Amodel%20on%20the%20original%2C%20base%20classes.%20Fine-tuning%20the%20model%20on%20novel%20classes%0Aoften%20leads%20to%20the%20phenomenon%20of%20catastrophic%20forgetting%2C%20where%20the%20accuracy%20of%0Abase%20classes%20declines%20unpredictably%20and%20significantly.%20In%20this%20paper%2C%20we%0Apropose%20a%20simple%20yet%20effective%20mechanism%20to%20address%20this%20challenge%20by%0Acontrolling%20the%20trade-off%20between%20novel%20and%20base%20class%20accuracy.%20We%0Aspecifically%20target%20the%20ultra-low-shot%20scenario%2C%20where%20only%20a%20single%20example%20is%0Aavailable%20per%20novel%20class.%20Our%20approach%20introduces%20a%20Novel%20Class%20Detection%0A%28NCD%29%20rule%2C%20which%20adjusts%20the%20degree%20of%20forgetting%20a%20priori%20while%0Asimultaneously%20enhancing%20performance%20on%20novel%20classes.%20We%20demonstrate%20the%0Aversatility%20of%20our%20solution%20by%20applying%20it%20to%20state-of-the-art%20Few-Shot%0AClass-Incremental%20Learning%20%28FSCIL%29%20methods%2C%20showing%20consistent%20improvements%0Aacross%20different%20settings.%20To%20better%20quantify%20the%20trade-off%20between%20novel%20and%0Abase%20class%20performance%2C%20we%20introduce%20new%20metrics%3A%20NCR%402FOR%20and%20NCR%405FOR.%20Our%0Aapproach%20achieves%20up%20to%20a%2030%25%20improvement%20in%20novel%20class%20accuracy%20on%20the%0ACIFAR100%20dataset%20%281-shot%2C%201%20novel%20class%29%20while%20maintaining%20a%20controlled%20base%0Aclass%20forgetting%20rate%20of%202%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15998v1&entry.124074799=Read"},
{"title": "AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive\n  Chain-of-Thought", "author": "Xin Huang and Tarun Kumar Vangani and Zhengyuan Liu and Bowei Zou and Ai Ti Aw", "abstract": "  Large language models (LLMs) have shown impressive multilingual capabilities\nthrough pretraining on diverse corpora. While these models show strong\nreasoning abilities, their performance varies significantly across languages\ndue to uneven training data distribution. Existing approaches using machine\ntranslation, and extensive multilingual pretraining and cross-lingual tuning\nface scalability challenges and often fail to capture nuanced reasoning\nprocesses across languages. In this paper, we introduce AdaCoT (Adaptive\nChain-of-Thought), a framework that enhances multilingual reasoning by\ndynamically routing thought processes through intermediary \"thinking languages\"\nbefore generating target-language responses. AdaCoT leverages a\nlanguage-agnostic core and incorporates an adaptive, reward-based mechanism for\nselecting optimal reasoning pathways without requiring additional pretraining.\nOur comprehensive evaluation across multiple benchmarks demonstrates\nsubstantial improvements in both factual reasoning quality and cross-lingual\nconsistency, with particularly strong performance gains in low-resource\nlanguage settings. The results suggest that adaptive reasoning paths can\neffectively bridge the performance gap between high and low-resource languages\nwhile maintaining cultural and linguistic nuances.\n", "link": "http://arxiv.org/abs/2501.16154v1", "date": "2025-01-27", "relevancy": 2.5473, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaCoT%3A%20Rethinking%20Cross-Lingual%20Factual%20Reasoning%20through%20Adaptive%0A%20%20Chain-of-Thought&body=Title%3A%20AdaCoT%3A%20Rethinking%20Cross-Lingual%20Factual%20Reasoning%20through%20Adaptive%0A%20%20Chain-of-Thought%0AAuthor%3A%20Xin%20Huang%20and%20Tarun%20Kumar%20Vangani%20and%20Zhengyuan%20Liu%20and%20Bowei%20Zou%20and%20Ai%20Ti%20Aw%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20multilingual%20capabilities%0Athrough%20pretraining%20on%20diverse%20corpora.%20While%20these%20models%20show%20strong%0Areasoning%20abilities%2C%20their%20performance%20varies%20significantly%20across%20languages%0Adue%20to%20uneven%20training%20data%20distribution.%20Existing%20approaches%20using%20machine%0Atranslation%2C%20and%20extensive%20multilingual%20pretraining%20and%20cross-lingual%20tuning%0Aface%20scalability%20challenges%20and%20often%20fail%20to%20capture%20nuanced%20reasoning%0Aprocesses%20across%20languages.%20In%20this%20paper%2C%20we%20introduce%20AdaCoT%20%28Adaptive%0AChain-of-Thought%29%2C%20a%20framework%20that%20enhances%20multilingual%20reasoning%20by%0Adynamically%20routing%20thought%20processes%20through%20intermediary%20%22thinking%20languages%22%0Abefore%20generating%20target-language%20responses.%20AdaCoT%20leverages%20a%0Alanguage-agnostic%20core%20and%20incorporates%20an%20adaptive%2C%20reward-based%20mechanism%20for%0Aselecting%20optimal%20reasoning%20pathways%20without%20requiring%20additional%20pretraining.%0AOur%20comprehensive%20evaluation%20across%20multiple%20benchmarks%20demonstrates%0Asubstantial%20improvements%20in%20both%20factual%20reasoning%20quality%20and%20cross-lingual%0Aconsistency%2C%20with%20particularly%20strong%20performance%20gains%20in%20low-resource%0Alanguage%20settings.%20The%20results%20suggest%20that%20adaptive%20reasoning%20paths%20can%0Aeffectively%20bridge%20the%20performance%20gap%20between%20high%20and%20low-resource%20languages%0Awhile%20maintaining%20cultural%20and%20linguistic%20nuances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaCoT%253A%2520Rethinking%2520Cross-Lingual%2520Factual%2520Reasoning%2520through%2520Adaptive%250A%2520%2520Chain-of-Thought%26entry.906535625%3DXin%2520Huang%2520and%2520Tarun%2520Kumar%2520Vangani%2520and%2520Zhengyuan%2520Liu%2520and%2520Bowei%2520Zou%2520and%2520Ai%2520Ti%2520Aw%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520multilingual%2520capabilities%250Athrough%2520pretraining%2520on%2520diverse%2520corpora.%2520While%2520these%2520models%2520show%2520strong%250Areasoning%2520abilities%252C%2520their%2520performance%2520varies%2520significantly%2520across%2520languages%250Adue%2520to%2520uneven%2520training%2520data%2520distribution.%2520Existing%2520approaches%2520using%2520machine%250Atranslation%252C%2520and%2520extensive%2520multilingual%2520pretraining%2520and%2520cross-lingual%2520tuning%250Aface%2520scalability%2520challenges%2520and%2520often%2520fail%2520to%2520capture%2520nuanced%2520reasoning%250Aprocesses%2520across%2520languages.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520AdaCoT%2520%2528Adaptive%250AChain-of-Thought%2529%252C%2520a%2520framework%2520that%2520enhances%2520multilingual%2520reasoning%2520by%250Adynamically%2520routing%2520thought%2520processes%2520through%2520intermediary%2520%2522thinking%2520languages%2522%250Abefore%2520generating%2520target-language%2520responses.%2520AdaCoT%2520leverages%2520a%250Alanguage-agnostic%2520core%2520and%2520incorporates%2520an%2520adaptive%252C%2520reward-based%2520mechanism%2520for%250Aselecting%2520optimal%2520reasoning%2520pathways%2520without%2520requiring%2520additional%2520pretraining.%250AOur%2520comprehensive%2520evaluation%2520across%2520multiple%2520benchmarks%2520demonstrates%250Asubstantial%2520improvements%2520in%2520both%2520factual%2520reasoning%2520quality%2520and%2520cross-lingual%250Aconsistency%252C%2520with%2520particularly%2520strong%2520performance%2520gains%2520in%2520low-resource%250Alanguage%2520settings.%2520The%2520results%2520suggest%2520that%2520adaptive%2520reasoning%2520paths%2520can%250Aeffectively%2520bridge%2520the%2520performance%2520gap%2520between%2520high%2520and%2520low-resource%2520languages%250Awhile%2520maintaining%2520cultural%2520and%2520linguistic%2520nuances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaCoT%3A%20Rethinking%20Cross-Lingual%20Factual%20Reasoning%20through%20Adaptive%0A%20%20Chain-of-Thought&entry.906535625=Xin%20Huang%20and%20Tarun%20Kumar%20Vangani%20and%20Zhengyuan%20Liu%20and%20Bowei%20Zou%20and%20Ai%20Ti%20Aw&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20multilingual%20capabilities%0Athrough%20pretraining%20on%20diverse%20corpora.%20While%20these%20models%20show%20strong%0Areasoning%20abilities%2C%20their%20performance%20varies%20significantly%20across%20languages%0Adue%20to%20uneven%20training%20data%20distribution.%20Existing%20approaches%20using%20machine%0Atranslation%2C%20and%20extensive%20multilingual%20pretraining%20and%20cross-lingual%20tuning%0Aface%20scalability%20challenges%20and%20often%20fail%20to%20capture%20nuanced%20reasoning%0Aprocesses%20across%20languages.%20In%20this%20paper%2C%20we%20introduce%20AdaCoT%20%28Adaptive%0AChain-of-Thought%29%2C%20a%20framework%20that%20enhances%20multilingual%20reasoning%20by%0Adynamically%20routing%20thought%20processes%20through%20intermediary%20%22thinking%20languages%22%0Abefore%20generating%20target-language%20responses.%20AdaCoT%20leverages%20a%0Alanguage-agnostic%20core%20and%20incorporates%20an%20adaptive%2C%20reward-based%20mechanism%20for%0Aselecting%20optimal%20reasoning%20pathways%20without%20requiring%20additional%20pretraining.%0AOur%20comprehensive%20evaluation%20across%20multiple%20benchmarks%20demonstrates%0Asubstantial%20improvements%20in%20both%20factual%20reasoning%20quality%20and%20cross-lingual%0Aconsistency%2C%20with%20particularly%20strong%20performance%20gains%20in%20low-resource%0Alanguage%20settings.%20The%20results%20suggest%20that%20adaptive%20reasoning%20paths%20can%0Aeffectively%20bridge%20the%20performance%20gap%20between%20high%20and%20low-resource%20languages%0Awhile%20maintaining%20cultural%20and%20linguistic%20nuances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16154v1&entry.124074799=Read"},
{"title": "What Does an Audio Deepfake Detector Focus on? A Study in the Time\n  Domain", "author": "Petr Grinberg and Ankur Kumar and Surya Koppisetti and Gaurav Bharaj", "abstract": "  Adding explanations to audio deepfake detection (ADD) models will boost their\nreal-world application by providing insight on the decision making process. In\nthis paper, we propose a relevancy-based explainable AI (XAI) method to analyze\nthe predictions of transformer-based ADD models. We compare against standard\nGrad-CAM and SHAP-based methods, using quantitative faithfulness metrics as\nwell as a partial spoof test, to comprehensively analyze the relative\nimportance of different temporal regions in an audio. We consider large\ndatasets, unlike previous works where only limited utterances are studied, and\nfind that the XAI methods differ in their explanations. The proposed\nrelevancy-based XAI method performs the best overall on a variety of metrics.\nFurther investigation on the relative importance of speech/non-speech, phonetic\ncontent, and voice onsets/offsets suggest that the XAI results obtained from\nanalyzing limited utterances don't necessarily hold when evaluated on large\ndatasets.\n", "link": "http://arxiv.org/abs/2501.13887v2", "date": "2025-01-27", "relevancy": 2.5454, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Does%20an%20Audio%20Deepfake%20Detector%20Focus%20on%3F%20A%20Study%20in%20the%20Time%0A%20%20Domain&body=Title%3A%20What%20Does%20an%20Audio%20Deepfake%20Detector%20Focus%20on%3F%20A%20Study%20in%20the%20Time%0A%20%20Domain%0AAuthor%3A%20Petr%20Grinberg%20and%20Ankur%20Kumar%20and%20Surya%20Koppisetti%20and%20Gaurav%20Bharaj%0AAbstract%3A%20%20%20Adding%20explanations%20to%20audio%20deepfake%20detection%20%28ADD%29%20models%20will%20boost%20their%0Areal-world%20application%20by%20providing%20insight%20on%20the%20decision%20making%20process.%20In%0Athis%20paper%2C%20we%20propose%20a%20relevancy-based%20explainable%20AI%20%28XAI%29%20method%20to%20analyze%0Athe%20predictions%20of%20transformer-based%20ADD%20models.%20We%20compare%20against%20standard%0AGrad-CAM%20and%20SHAP-based%20methods%2C%20using%20quantitative%20faithfulness%20metrics%20as%0Awell%20as%20a%20partial%20spoof%20test%2C%20to%20comprehensively%20analyze%20the%20relative%0Aimportance%20of%20different%20temporal%20regions%20in%20an%20audio.%20We%20consider%20large%0Adatasets%2C%20unlike%20previous%20works%20where%20only%20limited%20utterances%20are%20studied%2C%20and%0Afind%20that%20the%20XAI%20methods%20differ%20in%20their%20explanations.%20The%20proposed%0Arelevancy-based%20XAI%20method%20performs%20the%20best%20overall%20on%20a%20variety%20of%20metrics.%0AFurther%20investigation%20on%20the%20relative%20importance%20of%20speech/non-speech%2C%20phonetic%0Acontent%2C%20and%20voice%20onsets/offsets%20suggest%20that%20the%20XAI%20results%20obtained%20from%0Aanalyzing%20limited%20utterances%20don%27t%20necessarily%20hold%20when%20evaluated%20on%20large%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13887v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Does%2520an%2520Audio%2520Deepfake%2520Detector%2520Focus%2520on%253F%2520A%2520Study%2520in%2520the%2520Time%250A%2520%2520Domain%26entry.906535625%3DPetr%2520Grinberg%2520and%2520Ankur%2520Kumar%2520and%2520Surya%2520Koppisetti%2520and%2520Gaurav%2520Bharaj%26entry.1292438233%3D%2520%2520Adding%2520explanations%2520to%2520audio%2520deepfake%2520detection%2520%2528ADD%2529%2520models%2520will%2520boost%2520their%250Areal-world%2520application%2520by%2520providing%2520insight%2520on%2520the%2520decision%2520making%2520process.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520relevancy-based%2520explainable%2520AI%2520%2528XAI%2529%2520method%2520to%2520analyze%250Athe%2520predictions%2520of%2520transformer-based%2520ADD%2520models.%2520We%2520compare%2520against%2520standard%250AGrad-CAM%2520and%2520SHAP-based%2520methods%252C%2520using%2520quantitative%2520faithfulness%2520metrics%2520as%250Awell%2520as%2520a%2520partial%2520spoof%2520test%252C%2520to%2520comprehensively%2520analyze%2520the%2520relative%250Aimportance%2520of%2520different%2520temporal%2520regions%2520in%2520an%2520audio.%2520We%2520consider%2520large%250Adatasets%252C%2520unlike%2520previous%2520works%2520where%2520only%2520limited%2520utterances%2520are%2520studied%252C%2520and%250Afind%2520that%2520the%2520XAI%2520methods%2520differ%2520in%2520their%2520explanations.%2520The%2520proposed%250Arelevancy-based%2520XAI%2520method%2520performs%2520the%2520best%2520overall%2520on%2520a%2520variety%2520of%2520metrics.%250AFurther%2520investigation%2520on%2520the%2520relative%2520importance%2520of%2520speech/non-speech%252C%2520phonetic%250Acontent%252C%2520and%2520voice%2520onsets/offsets%2520suggest%2520that%2520the%2520XAI%2520results%2520obtained%2520from%250Aanalyzing%2520limited%2520utterances%2520don%2527t%2520necessarily%2520hold%2520when%2520evaluated%2520on%2520large%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13887v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Does%20an%20Audio%20Deepfake%20Detector%20Focus%20on%3F%20A%20Study%20in%20the%20Time%0A%20%20Domain&entry.906535625=Petr%20Grinberg%20and%20Ankur%20Kumar%20and%20Surya%20Koppisetti%20and%20Gaurav%20Bharaj&entry.1292438233=%20%20Adding%20explanations%20to%20audio%20deepfake%20detection%20%28ADD%29%20models%20will%20boost%20their%0Areal-world%20application%20by%20providing%20insight%20on%20the%20decision%20making%20process.%20In%0Athis%20paper%2C%20we%20propose%20a%20relevancy-based%20explainable%20AI%20%28XAI%29%20method%20to%20analyze%0Athe%20predictions%20of%20transformer-based%20ADD%20models.%20We%20compare%20against%20standard%0AGrad-CAM%20and%20SHAP-based%20methods%2C%20using%20quantitative%20faithfulness%20metrics%20as%0Awell%20as%20a%20partial%20spoof%20test%2C%20to%20comprehensively%20analyze%20the%20relative%0Aimportance%20of%20different%20temporal%20regions%20in%20an%20audio.%20We%20consider%20large%0Adatasets%2C%20unlike%20previous%20works%20where%20only%20limited%20utterances%20are%20studied%2C%20and%0Afind%20that%20the%20XAI%20methods%20differ%20in%20their%20explanations.%20The%20proposed%0Arelevancy-based%20XAI%20method%20performs%20the%20best%20overall%20on%20a%20variety%20of%20metrics.%0AFurther%20investigation%20on%20the%20relative%20importance%20of%20speech/non-speech%2C%20phonetic%0Acontent%2C%20and%20voice%20onsets/offsets%20suggest%20that%20the%20XAI%20results%20obtained%20from%0Aanalyzing%20limited%20utterances%20don%27t%20necessarily%20hold%20when%20evaluated%20on%20large%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13887v2&entry.124074799=Read"},
{"title": "Large Language Model-based Augmentation for Imbalanced Node\n  Classification on Text-Attributed Graphs", "author": "Leyao Wang and Yu Wang and Bo Ni and Yuying Zhao and Tyler Derr", "abstract": "  Node classification on graphs often suffers from class imbalance, leading to\nbiased predictions and significant risks in real-world applications. While\ndata-centric solutions have been explored, they largely overlook\nText-Attributed Graphs (TAGs) and the potential of using rich textual semantics\nto improve the classification of minority nodes. Given this gap, we propose\nLarge Language Model-based Augmentation on Text-Attributed Graphs (LA-TAG), a\nnovel framework that leverages Large Language Models (LLMs) to handle\nimbalanced node classification. Specifically, we develop prompting strategies\ninspired by interpolation to synthesize textual node attributes. Additionally,\nto effectively integrate synthetic nodes into the graph structure, we introduce\na textual link predictor that connects the generated nodes to the original\ngraph, preserving structural and contextual information. Experiments across\nvarious datasets and evaluation metrics demonstrate that LA-TAG outperforms\nexisting textual augmentation and graph imbalance learning methods, emphasizing\nthe efficacy of our approach in addressing class imbalance in TAGs.\n", "link": "http://arxiv.org/abs/2410.16882v2", "date": "2025-01-27", "relevancy": 2.5416, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5247}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.513}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model-based%20Augmentation%20for%20Imbalanced%20Node%0A%20%20Classification%20on%20Text-Attributed%20Graphs&body=Title%3A%20Large%20Language%20Model-based%20Augmentation%20for%20Imbalanced%20Node%0A%20%20Classification%20on%20Text-Attributed%20Graphs%0AAuthor%3A%20Leyao%20Wang%20and%20Yu%20Wang%20and%20Bo%20Ni%20and%20Yuying%20Zhao%20and%20Tyler%20Derr%0AAbstract%3A%20%20%20Node%20classification%20on%20graphs%20often%20suffers%20from%20class%20imbalance%2C%20leading%20to%0Abiased%20predictions%20and%20significant%20risks%20in%20real-world%20applications.%20While%0Adata-centric%20solutions%20have%20been%20explored%2C%20they%20largely%20overlook%0AText-Attributed%20Graphs%20%28TAGs%29%20and%20the%20potential%20of%20using%20rich%20textual%20semantics%0Ato%20improve%20the%20classification%20of%20minority%20nodes.%20Given%20this%20gap%2C%20we%20propose%0ALarge%20Language%20Model-based%20Augmentation%20on%20Text-Attributed%20Graphs%20%28LA-TAG%29%2C%20a%0Anovel%20framework%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20handle%0Aimbalanced%20node%20classification.%20Specifically%2C%20we%20develop%20prompting%20strategies%0Ainspired%20by%20interpolation%20to%20synthesize%20textual%20node%20attributes.%20Additionally%2C%0Ato%20effectively%20integrate%20synthetic%20nodes%20into%20the%20graph%20structure%2C%20we%20introduce%0Aa%20textual%20link%20predictor%20that%20connects%20the%20generated%20nodes%20to%20the%20original%0Agraph%2C%20preserving%20structural%20and%20contextual%20information.%20Experiments%20across%0Avarious%20datasets%20and%20evaluation%20metrics%20demonstrate%20that%20LA-TAG%20outperforms%0Aexisting%20textual%20augmentation%20and%20graph%20imbalance%20learning%20methods%2C%20emphasizing%0Athe%20efficacy%20of%20our%20approach%20in%20addressing%20class%20imbalance%20in%20TAGs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16882v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model-based%2520Augmentation%2520for%2520Imbalanced%2520Node%250A%2520%2520Classification%2520on%2520Text-Attributed%2520Graphs%26entry.906535625%3DLeyao%2520Wang%2520and%2520Yu%2520Wang%2520and%2520Bo%2520Ni%2520and%2520Yuying%2520Zhao%2520and%2520Tyler%2520Derr%26entry.1292438233%3D%2520%2520Node%2520classification%2520on%2520graphs%2520often%2520suffers%2520from%2520class%2520imbalance%252C%2520leading%2520to%250Abiased%2520predictions%2520and%2520significant%2520risks%2520in%2520real-world%2520applications.%2520While%250Adata-centric%2520solutions%2520have%2520been%2520explored%252C%2520they%2520largely%2520overlook%250AText-Attributed%2520Graphs%2520%2528TAGs%2529%2520and%2520the%2520potential%2520of%2520using%2520rich%2520textual%2520semantics%250Ato%2520improve%2520the%2520classification%2520of%2520minority%2520nodes.%2520Given%2520this%2520gap%252C%2520we%2520propose%250ALarge%2520Language%2520Model-based%2520Augmentation%2520on%2520Text-Attributed%2520Graphs%2520%2528LA-TAG%2529%252C%2520a%250Anovel%2520framework%2520that%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520handle%250Aimbalanced%2520node%2520classification.%2520Specifically%252C%2520we%2520develop%2520prompting%2520strategies%250Ainspired%2520by%2520interpolation%2520to%2520synthesize%2520textual%2520node%2520attributes.%2520Additionally%252C%250Ato%2520effectively%2520integrate%2520synthetic%2520nodes%2520into%2520the%2520graph%2520structure%252C%2520we%2520introduce%250Aa%2520textual%2520link%2520predictor%2520that%2520connects%2520the%2520generated%2520nodes%2520to%2520the%2520original%250Agraph%252C%2520preserving%2520structural%2520and%2520contextual%2520information.%2520Experiments%2520across%250Avarious%2520datasets%2520and%2520evaluation%2520metrics%2520demonstrate%2520that%2520LA-TAG%2520outperforms%250Aexisting%2520textual%2520augmentation%2520and%2520graph%2520imbalance%2520learning%2520methods%252C%2520emphasizing%250Athe%2520efficacy%2520of%2520our%2520approach%2520in%2520addressing%2520class%2520imbalance%2520in%2520TAGs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16882v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model-based%20Augmentation%20for%20Imbalanced%20Node%0A%20%20Classification%20on%20Text-Attributed%20Graphs&entry.906535625=Leyao%20Wang%20and%20Yu%20Wang%20and%20Bo%20Ni%20and%20Yuying%20Zhao%20and%20Tyler%20Derr&entry.1292438233=%20%20Node%20classification%20on%20graphs%20often%20suffers%20from%20class%20imbalance%2C%20leading%20to%0Abiased%20predictions%20and%20significant%20risks%20in%20real-world%20applications.%20While%0Adata-centric%20solutions%20have%20been%20explored%2C%20they%20largely%20overlook%0AText-Attributed%20Graphs%20%28TAGs%29%20and%20the%20potential%20of%20using%20rich%20textual%20semantics%0Ato%20improve%20the%20classification%20of%20minority%20nodes.%20Given%20this%20gap%2C%20we%20propose%0ALarge%20Language%20Model-based%20Augmentation%20on%20Text-Attributed%20Graphs%20%28LA-TAG%29%2C%20a%0Anovel%20framework%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20handle%0Aimbalanced%20node%20classification.%20Specifically%2C%20we%20develop%20prompting%20strategies%0Ainspired%20by%20interpolation%20to%20synthesize%20textual%20node%20attributes.%20Additionally%2C%0Ato%20effectively%20integrate%20synthetic%20nodes%20into%20the%20graph%20structure%2C%20we%20introduce%0Aa%20textual%20link%20predictor%20that%20connects%20the%20generated%20nodes%20to%20the%20original%0Agraph%2C%20preserving%20structural%20and%20contextual%20information.%20Experiments%20across%0Avarious%20datasets%20and%20evaluation%20metrics%20demonstrate%20that%20LA-TAG%20outperforms%0Aexisting%20textual%20augmentation%20and%20graph%20imbalance%20learning%20methods%2C%20emphasizing%0Athe%20efficacy%20of%20our%20approach%20in%20addressing%20class%20imbalance%20in%20TAGs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16882v2&entry.124074799=Read"},
{"title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization", "author": "Mengzhao Chen and Yi Liu and Jiahao Wang and Yi Bin and Wenqi Shao and Ping Luo", "abstract": "  Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.\n", "link": "http://arxiv.org/abs/2410.05265v2", "date": "2025-01-27", "relevancy": 2.5279, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrefixQuant%3A%20Eliminating%20Outliers%20by%20Prefixed%20Tokens%20for%20Large%20Language%0A%20%20Models%20Quantization&body=Title%3A%20PrefixQuant%3A%20Eliminating%20Outliers%20by%20Prefixed%20Tokens%20for%20Large%20Language%0A%20%20Models%20Quantization%0AAuthor%3A%20Mengzhao%20Chen%20and%20Yi%20Liu%20and%20Jiahao%20Wang%20and%20Yi%20Bin%20and%20Wenqi%20Shao%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Existing%20weight-activation%20quantization%20methods%20for%20Large%20Language%20Models%0A%28LLMs%29%20primarily%20address%20channel-wise%20outliers%20but%20often%20neglect%20token-wise%0Aoutliers%2C%20which%20limits%20the%20accuracy%20of%20quantized%20models.%20In%20this%20work%2C%20we%0Apropose%20PrefixQuant%2C%20a%20novel%20quantization%20method%20that%20achieves%20state-of-the-art%0Aperformance%20across%20various%20precision%20levels%20%28W4A4KV4%20and%20W4A8KV4%29%20and%0Agranularities%20%28dynamic%20and%20static%20quantization%29%20by%20effectively%20isolating%0Atoken-wise%20outliers.%20First%2C%20PrefixQuant%20eliminates%20token-wise%20outliers%20by%0Aprefixing%20outlier%20tokens%20in%20the%20KV%20cache%2C%20a%20process%20that%20is%20training-free%20and%0Ahighly%20efficient%20%28e.g.%2C%201%20minutes%20for%20Llama-3-70B%29.%20Second%2C%20PrefixQuant%0Aintroduces%20new%20trainable%20parameters%20for%20block-wise%20training%20to%20compensate%20for%0Aquantization%20error.%20Our%20experiments%20show%20that%20PrefixQuant%20significantly%0Aoutperforms%20existing%20dynamic%20quantization%20methods%2C%20even%20under%20coarser%20static%0Aquantization%20settings.%20For%20instance%2C%20PrefixQuant%20achieves%20an%20average%20accuracy%0Aimprovement%20of%20%2B3.08%20and%20%2B2.85%20points%20over%20SpinQuant%20%28dynamic%20quantization%29%20on%0Afive%20zero-shot%20reasoning%20tasks%20under%20dynamic%20and%20static%20quantization%20settings%2C%0Arespectively%2C%20on%20W4A4KV4%20Llama-3-8B.%20Additionally%2C%20we%20demonstrate%20up%20to%202.74x%0Aprefilling%20speedup%20and%202.16x%20decoding%20speedup%20for%20LLMs%20using%20W4A4%20PrefixQuant.%0AOur%20code%20is%20available%20at%20https%3A//github.com/ChenMnZ/PrefixQuant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrefixQuant%253A%2520Eliminating%2520Outliers%2520by%2520Prefixed%2520Tokens%2520for%2520Large%2520Language%250A%2520%2520Models%2520Quantization%26entry.906535625%3DMengzhao%2520Chen%2520and%2520Yi%2520Liu%2520and%2520Jiahao%2520Wang%2520and%2520Yi%2520Bin%2520and%2520Wenqi%2520Shao%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Existing%2520weight-activation%2520quantization%2520methods%2520for%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520primarily%2520address%2520channel-wise%2520outliers%2520but%2520often%2520neglect%2520token-wise%250Aoutliers%252C%2520which%2520limits%2520the%2520accuracy%2520of%2520quantized%2520models.%2520In%2520this%2520work%252C%2520we%250Apropose%2520PrefixQuant%252C%2520a%2520novel%2520quantization%2520method%2520that%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520various%2520precision%2520levels%2520%2528W4A4KV4%2520and%2520W4A8KV4%2529%2520and%250Agranularities%2520%2528dynamic%2520and%2520static%2520quantization%2529%2520by%2520effectively%2520isolating%250Atoken-wise%2520outliers.%2520First%252C%2520PrefixQuant%2520eliminates%2520token-wise%2520outliers%2520by%250Aprefixing%2520outlier%2520tokens%2520in%2520the%2520KV%2520cache%252C%2520a%2520process%2520that%2520is%2520training-free%2520and%250Ahighly%2520efficient%2520%2528e.g.%252C%25201%2520minutes%2520for%2520Llama-3-70B%2529.%2520Second%252C%2520PrefixQuant%250Aintroduces%2520new%2520trainable%2520parameters%2520for%2520block-wise%2520training%2520to%2520compensate%2520for%250Aquantization%2520error.%2520Our%2520experiments%2520show%2520that%2520PrefixQuant%2520significantly%250Aoutperforms%2520existing%2520dynamic%2520quantization%2520methods%252C%2520even%2520under%2520coarser%2520static%250Aquantization%2520settings.%2520For%2520instance%252C%2520PrefixQuant%2520achieves%2520an%2520average%2520accuracy%250Aimprovement%2520of%2520%252B3.08%2520and%2520%252B2.85%2520points%2520over%2520SpinQuant%2520%2528dynamic%2520quantization%2529%2520on%250Afive%2520zero-shot%2520reasoning%2520tasks%2520under%2520dynamic%2520and%2520static%2520quantization%2520settings%252C%250Arespectively%252C%2520on%2520W4A4KV4%2520Llama-3-8B.%2520Additionally%252C%2520we%2520demonstrate%2520up%2520to%25202.74x%250Aprefilling%2520speedup%2520and%25202.16x%2520decoding%2520speedup%2520for%2520LLMs%2520using%2520W4A4%2520PrefixQuant.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/ChenMnZ/PrefixQuant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrefixQuant%3A%20Eliminating%20Outliers%20by%20Prefixed%20Tokens%20for%20Large%20Language%0A%20%20Models%20Quantization&entry.906535625=Mengzhao%20Chen%20and%20Yi%20Liu%20and%20Jiahao%20Wang%20and%20Yi%20Bin%20and%20Wenqi%20Shao%20and%20Ping%20Luo&entry.1292438233=%20%20Existing%20weight-activation%20quantization%20methods%20for%20Large%20Language%20Models%0A%28LLMs%29%20primarily%20address%20channel-wise%20outliers%20but%20often%20neglect%20token-wise%0Aoutliers%2C%20which%20limits%20the%20accuracy%20of%20quantized%20models.%20In%20this%20work%2C%20we%0Apropose%20PrefixQuant%2C%20a%20novel%20quantization%20method%20that%20achieves%20state-of-the-art%0Aperformance%20across%20various%20precision%20levels%20%28W4A4KV4%20and%20W4A8KV4%29%20and%0Agranularities%20%28dynamic%20and%20static%20quantization%29%20by%20effectively%20isolating%0Atoken-wise%20outliers.%20First%2C%20PrefixQuant%20eliminates%20token-wise%20outliers%20by%0Aprefixing%20outlier%20tokens%20in%20the%20KV%20cache%2C%20a%20process%20that%20is%20training-free%20and%0Ahighly%20efficient%20%28e.g.%2C%201%20minutes%20for%20Llama-3-70B%29.%20Second%2C%20PrefixQuant%0Aintroduces%20new%20trainable%20parameters%20for%20block-wise%20training%20to%20compensate%20for%0Aquantization%20error.%20Our%20experiments%20show%20that%20PrefixQuant%20significantly%0Aoutperforms%20existing%20dynamic%20quantization%20methods%2C%20even%20under%20coarser%20static%0Aquantization%20settings.%20For%20instance%2C%20PrefixQuant%20achieves%20an%20average%20accuracy%0Aimprovement%20of%20%2B3.08%20and%20%2B2.85%20points%20over%20SpinQuant%20%28dynamic%20quantization%29%20on%0Afive%20zero-shot%20reasoning%20tasks%20under%20dynamic%20and%20static%20quantization%20settings%2C%0Arespectively%2C%20on%20W4A4KV4%20Llama-3-8B.%20Additionally%2C%20we%20demonstrate%20up%20to%202.74x%0Aprefilling%20speedup%20and%202.16x%20decoding%20speedup%20for%20LLMs%20using%20W4A4%20PrefixQuant.%0AOur%20code%20is%20available%20at%20https%3A//github.com/ChenMnZ/PrefixQuant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05265v2&entry.124074799=Read"},
{"title": "Two-Timescale Gradient Descent Ascent Algorithms for Nonconvex Minimax\n  Optimization", "author": "Tianyi Lin and Chi Jin and Michael. I. Jordan", "abstract": "  We provide a unified analysis of two-timescale gradient descent ascent\n(TTGDA) for solving structured nonconvex minimax optimization problems in the\nform of $\\min_\\textbf{x} \\max_{\\textbf{y} \\in Y} f(\\textbf{x}, \\textbf{y})$,\nwhere the objective function $f(\\textbf{x}, \\textbf{y})$ is nonconvex in\n$\\textbf{x}$ and concave in $\\textbf{y}$, and the constraint set $Y \\subseteq\n\\mathbb{R}^n$ is convex and bounded. In the convex-concave setting, the\nsingle-timescale gradient descent ascent (GDA) algorithm is widely used in\napplications and has been shown to have strong convergence guarantees. In more\ngeneral settings, however, it can fail to converge. Our contribution is to\ndesign TTGDA algorithms that are effective beyond the convex-concave setting,\nefficiently finding a stationary point of the function $\\Phi(\\cdot) :=\n\\max_{\\textbf{y} \\in Y} f(\\cdot, \\textbf{y})$. We also establish theoretical\nbounds on the complexity of solving both smooth and nonsmooth nonconvex-concave\nminimax optimization problems. To the best of our knowledge, this is the first\nsystematic analysis of TTGDA for nonconvex minimax optimization, shedding light\non its superior performance in training generative adversarial networks (GANs)\nand in other real-world application problems.\n", "link": "http://arxiv.org/abs/2408.11974v3", "date": "2025-01-27", "relevancy": 2.5177, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5149}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5022}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-Timescale%20Gradient%20Descent%20Ascent%20Algorithms%20for%20Nonconvex%20Minimax%0A%20%20Optimization&body=Title%3A%20Two-Timescale%20Gradient%20Descent%20Ascent%20Algorithms%20for%20Nonconvex%20Minimax%0A%20%20Optimization%0AAuthor%3A%20Tianyi%20Lin%20and%20Chi%20Jin%20and%20Michael.%20I.%20Jordan%0AAbstract%3A%20%20%20We%20provide%20a%20unified%20analysis%20of%20two-timescale%20gradient%20descent%20ascent%0A%28TTGDA%29%20for%20solving%20structured%20nonconvex%20minimax%20optimization%20problems%20in%20the%0Aform%20of%20%24%5Cmin_%5Ctextbf%7Bx%7D%20%5Cmax_%7B%5Ctextbf%7By%7D%20%5Cin%20Y%7D%20f%28%5Ctextbf%7Bx%7D%2C%20%5Ctextbf%7By%7D%29%24%2C%0Awhere%20the%20objective%20function%20%24f%28%5Ctextbf%7Bx%7D%2C%20%5Ctextbf%7By%7D%29%24%20is%20nonconvex%20in%0A%24%5Ctextbf%7Bx%7D%24%20and%20concave%20in%20%24%5Ctextbf%7By%7D%24%2C%20and%20the%20constraint%20set%20%24Y%20%5Csubseteq%0A%5Cmathbb%7BR%7D%5En%24%20is%20convex%20and%20bounded.%20In%20the%20convex-concave%20setting%2C%20the%0Asingle-timescale%20gradient%20descent%20ascent%20%28GDA%29%20algorithm%20is%20widely%20used%20in%0Aapplications%20and%20has%20been%20shown%20to%20have%20strong%20convergence%20guarantees.%20In%20more%0Ageneral%20settings%2C%20however%2C%20it%20can%20fail%20to%20converge.%20Our%20contribution%20is%20to%0Adesign%20TTGDA%20algorithms%20that%20are%20effective%20beyond%20the%20convex-concave%20setting%2C%0Aefficiently%20finding%20a%20stationary%20point%20of%20the%20function%20%24%5CPhi%28%5Ccdot%29%20%3A%3D%0A%5Cmax_%7B%5Ctextbf%7By%7D%20%5Cin%20Y%7D%20f%28%5Ccdot%2C%20%5Ctextbf%7By%7D%29%24.%20We%20also%20establish%20theoretical%0Abounds%20on%20the%20complexity%20of%20solving%20both%20smooth%20and%20nonsmooth%20nonconvex-concave%0Aminimax%20optimization%20problems.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Asystematic%20analysis%20of%20TTGDA%20for%20nonconvex%20minimax%20optimization%2C%20shedding%20light%0Aon%20its%20superior%20performance%20in%20training%20generative%20adversarial%20networks%20%28GANs%29%0Aand%20in%20other%20real-world%20application%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11974v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-Timescale%2520Gradient%2520Descent%2520Ascent%2520Algorithms%2520for%2520Nonconvex%2520Minimax%250A%2520%2520Optimization%26entry.906535625%3DTianyi%2520Lin%2520and%2520Chi%2520Jin%2520and%2520Michael.%2520I.%2520Jordan%26entry.1292438233%3D%2520%2520We%2520provide%2520a%2520unified%2520analysis%2520of%2520two-timescale%2520gradient%2520descent%2520ascent%250A%2528TTGDA%2529%2520for%2520solving%2520structured%2520nonconvex%2520minimax%2520optimization%2520problems%2520in%2520the%250Aform%2520of%2520%2524%255Cmin_%255Ctextbf%257Bx%257D%2520%255Cmax_%257B%255Ctextbf%257By%257D%2520%255Cin%2520Y%257D%2520f%2528%255Ctextbf%257Bx%257D%252C%2520%255Ctextbf%257By%257D%2529%2524%252C%250Awhere%2520the%2520objective%2520function%2520%2524f%2528%255Ctextbf%257Bx%257D%252C%2520%255Ctextbf%257By%257D%2529%2524%2520is%2520nonconvex%2520in%250A%2524%255Ctextbf%257Bx%257D%2524%2520and%2520concave%2520in%2520%2524%255Ctextbf%257By%257D%2524%252C%2520and%2520the%2520constraint%2520set%2520%2524Y%2520%255Csubseteq%250A%255Cmathbb%257BR%257D%255En%2524%2520is%2520convex%2520and%2520bounded.%2520In%2520the%2520convex-concave%2520setting%252C%2520the%250Asingle-timescale%2520gradient%2520descent%2520ascent%2520%2528GDA%2529%2520algorithm%2520is%2520widely%2520used%2520in%250Aapplications%2520and%2520has%2520been%2520shown%2520to%2520have%2520strong%2520convergence%2520guarantees.%2520In%2520more%250Ageneral%2520settings%252C%2520however%252C%2520it%2520can%2520fail%2520to%2520converge.%2520Our%2520contribution%2520is%2520to%250Adesign%2520TTGDA%2520algorithms%2520that%2520are%2520effective%2520beyond%2520the%2520convex-concave%2520setting%252C%250Aefficiently%2520finding%2520a%2520stationary%2520point%2520of%2520the%2520function%2520%2524%255CPhi%2528%255Ccdot%2529%2520%253A%253D%250A%255Cmax_%257B%255Ctextbf%257By%257D%2520%255Cin%2520Y%257D%2520f%2528%255Ccdot%252C%2520%255Ctextbf%257By%257D%2529%2524.%2520We%2520also%2520establish%2520theoretical%250Abounds%2520on%2520the%2520complexity%2520of%2520solving%2520both%2520smooth%2520and%2520nonsmooth%2520nonconvex-concave%250Aminimax%2520optimization%2520problems.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Asystematic%2520analysis%2520of%2520TTGDA%2520for%2520nonconvex%2520minimax%2520optimization%252C%2520shedding%2520light%250Aon%2520its%2520superior%2520performance%2520in%2520training%2520generative%2520adversarial%2520networks%2520%2528GANs%2529%250Aand%2520in%2520other%2520real-world%2520application%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11974v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-Timescale%20Gradient%20Descent%20Ascent%20Algorithms%20for%20Nonconvex%20Minimax%0A%20%20Optimization&entry.906535625=Tianyi%20Lin%20and%20Chi%20Jin%20and%20Michael.%20I.%20Jordan&entry.1292438233=%20%20We%20provide%20a%20unified%20analysis%20of%20two-timescale%20gradient%20descent%20ascent%0A%28TTGDA%29%20for%20solving%20structured%20nonconvex%20minimax%20optimization%20problems%20in%20the%0Aform%20of%20%24%5Cmin_%5Ctextbf%7Bx%7D%20%5Cmax_%7B%5Ctextbf%7By%7D%20%5Cin%20Y%7D%20f%28%5Ctextbf%7Bx%7D%2C%20%5Ctextbf%7By%7D%29%24%2C%0Awhere%20the%20objective%20function%20%24f%28%5Ctextbf%7Bx%7D%2C%20%5Ctextbf%7By%7D%29%24%20is%20nonconvex%20in%0A%24%5Ctextbf%7Bx%7D%24%20and%20concave%20in%20%24%5Ctextbf%7By%7D%24%2C%20and%20the%20constraint%20set%20%24Y%20%5Csubseteq%0A%5Cmathbb%7BR%7D%5En%24%20is%20convex%20and%20bounded.%20In%20the%20convex-concave%20setting%2C%20the%0Asingle-timescale%20gradient%20descent%20ascent%20%28GDA%29%20algorithm%20is%20widely%20used%20in%0Aapplications%20and%20has%20been%20shown%20to%20have%20strong%20convergence%20guarantees.%20In%20more%0Ageneral%20settings%2C%20however%2C%20it%20can%20fail%20to%20converge.%20Our%20contribution%20is%20to%0Adesign%20TTGDA%20algorithms%20that%20are%20effective%20beyond%20the%20convex-concave%20setting%2C%0Aefficiently%20finding%20a%20stationary%20point%20of%20the%20function%20%24%5CPhi%28%5Ccdot%29%20%3A%3D%0A%5Cmax_%7B%5Ctextbf%7By%7D%20%5Cin%20Y%7D%20f%28%5Ccdot%2C%20%5Ctextbf%7By%7D%29%24.%20We%20also%20establish%20theoretical%0Abounds%20on%20the%20complexity%20of%20solving%20both%20smooth%20and%20nonsmooth%20nonconvex-concave%0Aminimax%20optimization%20problems.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Asystematic%20analysis%20of%20TTGDA%20for%20nonconvex%20minimax%20optimization%2C%20shedding%20light%0Aon%20its%20superior%20performance%20in%20training%20generative%20adversarial%20networks%20%28GANs%29%0Aand%20in%20other%20real-world%20application%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11974v3&entry.124074799=Read"},
{"title": "Accelerating lensed quasar discovery and modeling with physics-informed\n  variational autoencoders", "author": "Irham T. Andika and Stefan Schuldt and Sherry H. Suyu and Satadru Bag and Raoul Ca\u00f1ameras and Alejandra Melo and Claudio Grillo and James H. H. Chan", "abstract": "  Strongly lensed quasars provide valuable insights into the rate of cosmic\nexpansion, the distribution of dark matter in foreground deflectors, and the\ncharacteristics of quasar hosts. However, detecting them in astronomical images\nis difficult due to the prevalence of non-lensing objects. To address this\nchallenge, we developed a generative deep learning model called VariLens, built\nupon a physics-informed variational autoencoder. This model seamlessly\nintegrates three essential modules: image reconstruction, object\nclassification, and lens modeling, offering a fast and comprehensive approach\nto strong lens analysis. VariLens is capable of rapidly determining both (1)\nthe probability that an object is a lens system and (2) key parameters of a\nsingular isothermal ellipsoid (SIE) mass model -- including the Einstein radius\n($\\theta_\\mathrm{E}$), lens center, and ellipticity -- in just milliseconds\nusing a single CPU. A direct comparison of VariLens estimates with traditional\nlens modeling for 20 known lensed quasars within the Subaru Hyper Suprime-Cam\n(HSC) footprint shows good agreement, with both results consistent within\n$2\\sigma$ for systems with $\\theta_\\mathrm{E}<3$ arcsecs. To identify new\nlensed quasar candidates, we begin with an initial sample of approximately 80\nmillion sources, combining HSC data with multiwavelength information from\nvarious surveys. After applying a photometric preselection aimed at locating\n$z>1.5$ sources, the number of candidates was reduced to 710,966. Subsequently,\nVariLens highlights 13,831 sources, each showing a high likelihood of being a\nlens. A visual assessment of these objects results in 42 promising candidates\nthat await spectroscopic confirmation. These results underscore the potential\nof automated deep learning pipelines to efficiently detect and model strong\nlenses in large datasets.\n", "link": "http://arxiv.org/abs/2412.12709v3", "date": "2025-01-27", "relevancy": 2.5124, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20lensed%20quasar%20discovery%20and%20modeling%20with%20physics-informed%0A%20%20variational%20autoencoders&body=Title%3A%20Accelerating%20lensed%20quasar%20discovery%20and%20modeling%20with%20physics-informed%0A%20%20variational%20autoencoders%0AAuthor%3A%20Irham%20T.%20Andika%20and%20Stefan%20Schuldt%20and%20Sherry%20H.%20Suyu%20and%20Satadru%20Bag%20and%20Raoul%20Ca%C3%B1ameras%20and%20Alejandra%20Melo%20and%20Claudio%20Grillo%20and%20James%20H.%20H.%20Chan%0AAbstract%3A%20%20%20Strongly%20lensed%20quasars%20provide%20valuable%20insights%20into%20the%20rate%20of%20cosmic%0Aexpansion%2C%20the%20distribution%20of%20dark%20matter%20in%20foreground%20deflectors%2C%20and%20the%0Acharacteristics%20of%20quasar%20hosts.%20However%2C%20detecting%20them%20in%20astronomical%20images%0Ais%20difficult%20due%20to%20the%20prevalence%20of%20non-lensing%20objects.%20To%20address%20this%0Achallenge%2C%20we%20developed%20a%20generative%20deep%20learning%20model%20called%20VariLens%2C%20built%0Aupon%20a%20physics-informed%20variational%20autoencoder.%20This%20model%20seamlessly%0Aintegrates%20three%20essential%20modules%3A%20image%20reconstruction%2C%20object%0Aclassification%2C%20and%20lens%20modeling%2C%20offering%20a%20fast%20and%20comprehensive%20approach%0Ato%20strong%20lens%20analysis.%20VariLens%20is%20capable%20of%20rapidly%20determining%20both%20%281%29%0Athe%20probability%20that%20an%20object%20is%20a%20lens%20system%20and%20%282%29%20key%20parameters%20of%20a%0Asingular%20isothermal%20ellipsoid%20%28SIE%29%20mass%20model%20--%20including%20the%20Einstein%20radius%0A%28%24%5Ctheta_%5Cmathrm%7BE%7D%24%29%2C%20lens%20center%2C%20and%20ellipticity%20--%20in%20just%20milliseconds%0Ausing%20a%20single%20CPU.%20A%20direct%20comparison%20of%20VariLens%20estimates%20with%20traditional%0Alens%20modeling%20for%2020%20known%20lensed%20quasars%20within%20the%20Subaru%20Hyper%20Suprime-Cam%0A%28HSC%29%20footprint%20shows%20good%20agreement%2C%20with%20both%20results%20consistent%20within%0A%242%5Csigma%24%20for%20systems%20with%20%24%5Ctheta_%5Cmathrm%7BE%7D%3C3%24%20arcsecs.%20To%20identify%20new%0Alensed%20quasar%20candidates%2C%20we%20begin%20with%20an%20initial%20sample%20of%20approximately%2080%0Amillion%20sources%2C%20combining%20HSC%20data%20with%20multiwavelength%20information%20from%0Avarious%20surveys.%20After%20applying%20a%20photometric%20preselection%20aimed%20at%20locating%0A%24z%3E1.5%24%20sources%2C%20the%20number%20of%20candidates%20was%20reduced%20to%20710%2C966.%20Subsequently%2C%0AVariLens%20highlights%2013%2C831%20sources%2C%20each%20showing%20a%20high%20likelihood%20of%20being%20a%0Alens.%20A%20visual%20assessment%20of%20these%20objects%20results%20in%2042%20promising%20candidates%0Athat%20await%20spectroscopic%20confirmation.%20These%20results%20underscore%20the%20potential%0Aof%20automated%20deep%20learning%20pipelines%20to%20efficiently%20detect%20and%20model%20strong%0Alenses%20in%20large%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12709v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520lensed%2520quasar%2520discovery%2520and%2520modeling%2520with%2520physics-informed%250A%2520%2520variational%2520autoencoders%26entry.906535625%3DIrham%2520T.%2520Andika%2520and%2520Stefan%2520Schuldt%2520and%2520Sherry%2520H.%2520Suyu%2520and%2520Satadru%2520Bag%2520and%2520Raoul%2520Ca%25C3%25B1ameras%2520and%2520Alejandra%2520Melo%2520and%2520Claudio%2520Grillo%2520and%2520James%2520H.%2520H.%2520Chan%26entry.1292438233%3D%2520%2520Strongly%2520lensed%2520quasars%2520provide%2520valuable%2520insights%2520into%2520the%2520rate%2520of%2520cosmic%250Aexpansion%252C%2520the%2520distribution%2520of%2520dark%2520matter%2520in%2520foreground%2520deflectors%252C%2520and%2520the%250Acharacteristics%2520of%2520quasar%2520hosts.%2520However%252C%2520detecting%2520them%2520in%2520astronomical%2520images%250Ais%2520difficult%2520due%2520to%2520the%2520prevalence%2520of%2520non-lensing%2520objects.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520developed%2520a%2520generative%2520deep%2520learning%2520model%2520called%2520VariLens%252C%2520built%250Aupon%2520a%2520physics-informed%2520variational%2520autoencoder.%2520This%2520model%2520seamlessly%250Aintegrates%2520three%2520essential%2520modules%253A%2520image%2520reconstruction%252C%2520object%250Aclassification%252C%2520and%2520lens%2520modeling%252C%2520offering%2520a%2520fast%2520and%2520comprehensive%2520approach%250Ato%2520strong%2520lens%2520analysis.%2520VariLens%2520is%2520capable%2520of%2520rapidly%2520determining%2520both%2520%25281%2529%250Athe%2520probability%2520that%2520an%2520object%2520is%2520a%2520lens%2520system%2520and%2520%25282%2529%2520key%2520parameters%2520of%2520a%250Asingular%2520isothermal%2520ellipsoid%2520%2528SIE%2529%2520mass%2520model%2520--%2520including%2520the%2520Einstein%2520radius%250A%2528%2524%255Ctheta_%255Cmathrm%257BE%257D%2524%2529%252C%2520lens%2520center%252C%2520and%2520ellipticity%2520--%2520in%2520just%2520milliseconds%250Ausing%2520a%2520single%2520CPU.%2520A%2520direct%2520comparison%2520of%2520VariLens%2520estimates%2520with%2520traditional%250Alens%2520modeling%2520for%252020%2520known%2520lensed%2520quasars%2520within%2520the%2520Subaru%2520Hyper%2520Suprime-Cam%250A%2528HSC%2529%2520footprint%2520shows%2520good%2520agreement%252C%2520with%2520both%2520results%2520consistent%2520within%250A%25242%255Csigma%2524%2520for%2520systems%2520with%2520%2524%255Ctheta_%255Cmathrm%257BE%257D%253C3%2524%2520arcsecs.%2520To%2520identify%2520new%250Alensed%2520quasar%2520candidates%252C%2520we%2520begin%2520with%2520an%2520initial%2520sample%2520of%2520approximately%252080%250Amillion%2520sources%252C%2520combining%2520HSC%2520data%2520with%2520multiwavelength%2520information%2520from%250Avarious%2520surveys.%2520After%2520applying%2520a%2520photometric%2520preselection%2520aimed%2520at%2520locating%250A%2524z%253E1.5%2524%2520sources%252C%2520the%2520number%2520of%2520candidates%2520was%2520reduced%2520to%2520710%252C966.%2520Subsequently%252C%250AVariLens%2520highlights%252013%252C831%2520sources%252C%2520each%2520showing%2520a%2520high%2520likelihood%2520of%2520being%2520a%250Alens.%2520A%2520visual%2520assessment%2520of%2520these%2520objects%2520results%2520in%252042%2520promising%2520candidates%250Athat%2520await%2520spectroscopic%2520confirmation.%2520These%2520results%2520underscore%2520the%2520potential%250Aof%2520automated%2520deep%2520learning%2520pipelines%2520to%2520efficiently%2520detect%2520and%2520model%2520strong%250Alenses%2520in%2520large%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12709v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20lensed%20quasar%20discovery%20and%20modeling%20with%20physics-informed%0A%20%20variational%20autoencoders&entry.906535625=Irham%20T.%20Andika%20and%20Stefan%20Schuldt%20and%20Sherry%20H.%20Suyu%20and%20Satadru%20Bag%20and%20Raoul%20Ca%C3%B1ameras%20and%20Alejandra%20Melo%20and%20Claudio%20Grillo%20and%20James%20H.%20H.%20Chan&entry.1292438233=%20%20Strongly%20lensed%20quasars%20provide%20valuable%20insights%20into%20the%20rate%20of%20cosmic%0Aexpansion%2C%20the%20distribution%20of%20dark%20matter%20in%20foreground%20deflectors%2C%20and%20the%0Acharacteristics%20of%20quasar%20hosts.%20However%2C%20detecting%20them%20in%20astronomical%20images%0Ais%20difficult%20due%20to%20the%20prevalence%20of%20non-lensing%20objects.%20To%20address%20this%0Achallenge%2C%20we%20developed%20a%20generative%20deep%20learning%20model%20called%20VariLens%2C%20built%0Aupon%20a%20physics-informed%20variational%20autoencoder.%20This%20model%20seamlessly%0Aintegrates%20three%20essential%20modules%3A%20image%20reconstruction%2C%20object%0Aclassification%2C%20and%20lens%20modeling%2C%20offering%20a%20fast%20and%20comprehensive%20approach%0Ato%20strong%20lens%20analysis.%20VariLens%20is%20capable%20of%20rapidly%20determining%20both%20%281%29%0Athe%20probability%20that%20an%20object%20is%20a%20lens%20system%20and%20%282%29%20key%20parameters%20of%20a%0Asingular%20isothermal%20ellipsoid%20%28SIE%29%20mass%20model%20--%20including%20the%20Einstein%20radius%0A%28%24%5Ctheta_%5Cmathrm%7BE%7D%24%29%2C%20lens%20center%2C%20and%20ellipticity%20--%20in%20just%20milliseconds%0Ausing%20a%20single%20CPU.%20A%20direct%20comparison%20of%20VariLens%20estimates%20with%20traditional%0Alens%20modeling%20for%2020%20known%20lensed%20quasars%20within%20the%20Subaru%20Hyper%20Suprime-Cam%0A%28HSC%29%20footprint%20shows%20good%20agreement%2C%20with%20both%20results%20consistent%20within%0A%242%5Csigma%24%20for%20systems%20with%20%24%5Ctheta_%5Cmathrm%7BE%7D%3C3%24%20arcsecs.%20To%20identify%20new%0Alensed%20quasar%20candidates%2C%20we%20begin%20with%20an%20initial%20sample%20of%20approximately%2080%0Amillion%20sources%2C%20combining%20HSC%20data%20with%20multiwavelength%20information%20from%0Avarious%20surveys.%20After%20applying%20a%20photometric%20preselection%20aimed%20at%20locating%0A%24z%3E1.5%24%20sources%2C%20the%20number%20of%20candidates%20was%20reduced%20to%20710%2C966.%20Subsequently%2C%0AVariLens%20highlights%2013%2C831%20sources%2C%20each%20showing%20a%20high%20likelihood%20of%20being%20a%0Alens.%20A%20visual%20assessment%20of%20these%20objects%20results%20in%2042%20promising%20candidates%0Athat%20await%20spectroscopic%20confirmation.%20These%20results%20underscore%20the%20potential%0Aof%20automated%20deep%20learning%20pipelines%20to%20efficiently%20detect%20and%20model%20strong%0Alenses%20in%20large%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12709v3&entry.124074799=Read"},
{"title": "Training Dynamics of In-Context Learning in Linear Attention", "author": "Yedi Zhang and Aaditya K. Singh and Peter E. Latham and Andrew Saxe", "abstract": "  While attention-based models have demonstrated the remarkable ability of\nin-context learning, the theoretical understanding of how these models acquired\nthis ability through gradient descent training is still preliminary. Towards\nanswering this question, we study the gradient descent dynamics of multi-head\nlinear self-attention trained for in-context linear regression. We examine two\nparametrizations of linear self-attention: one with the key and query weights\nmerged as a single matrix (common in theoretical studies), and one with\nseparate key and query matrices (closer to practical settings). For the merged\nparametrization, we show the training dynamics has two fixed points and the\nloss trajectory exhibits a single, abrupt drop. We derive an analytical\ntime-course solution for a certain class of datasets and initialization. For\nthe separate parametrization, we show the training dynamics has exponentially\nmany fixed points and the loss exhibits saddle-to-saddle dynamics, which we\nreduce to scalar ordinary differential equations. During training, the model\nimplements principal component regression in context with the number of\nprincipal components increasing over training time. Overall, we characterize\nhow in-context learning abilities evolve during gradient descent training of\nlinear attention, revealing dynamics of abrupt acquisition versus progressive\nimprovements in models with different parametrizations.\n", "link": "http://arxiv.org/abs/2501.16265v1", "date": "2025-01-27", "relevancy": 2.4905, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5089}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5021}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Dynamics%20of%20In-Context%20Learning%20in%20Linear%20Attention&body=Title%3A%20Training%20Dynamics%20of%20In-Context%20Learning%20in%20Linear%20Attention%0AAuthor%3A%20Yedi%20Zhang%20and%20Aaditya%20K.%20Singh%20and%20Peter%20E.%20Latham%20and%20Andrew%20Saxe%0AAbstract%3A%20%20%20While%20attention-based%20models%20have%20demonstrated%20the%20remarkable%20ability%20of%0Ain-context%20learning%2C%20the%20theoretical%20understanding%20of%20how%20these%20models%20acquired%0Athis%20ability%20through%20gradient%20descent%20training%20is%20still%20preliminary.%20Towards%0Aanswering%20this%20question%2C%20we%20study%20the%20gradient%20descent%20dynamics%20of%20multi-head%0Alinear%20self-attention%20trained%20for%20in-context%20linear%20regression.%20We%20examine%20two%0Aparametrizations%20of%20linear%20self-attention%3A%20one%20with%20the%20key%20and%20query%20weights%0Amerged%20as%20a%20single%20matrix%20%28common%20in%20theoretical%20studies%29%2C%20and%20one%20with%0Aseparate%20key%20and%20query%20matrices%20%28closer%20to%20practical%20settings%29.%20For%20the%20merged%0Aparametrization%2C%20we%20show%20the%20training%20dynamics%20has%20two%20fixed%20points%20and%20the%0Aloss%20trajectory%20exhibits%20a%20single%2C%20abrupt%20drop.%20We%20derive%20an%20analytical%0Atime-course%20solution%20for%20a%20certain%20class%20of%20datasets%20and%20initialization.%20For%0Athe%20separate%20parametrization%2C%20we%20show%20the%20training%20dynamics%20has%20exponentially%0Amany%20fixed%20points%20and%20the%20loss%20exhibits%20saddle-to-saddle%20dynamics%2C%20which%20we%0Areduce%20to%20scalar%20ordinary%20differential%20equations.%20During%20training%2C%20the%20model%0Aimplements%20principal%20component%20regression%20in%20context%20with%20the%20number%20of%0Aprincipal%20components%20increasing%20over%20training%20time.%20Overall%2C%20we%20characterize%0Ahow%20in-context%20learning%20abilities%20evolve%20during%20gradient%20descent%20training%20of%0Alinear%20attention%2C%20revealing%20dynamics%20of%20abrupt%20acquisition%20versus%20progressive%0Aimprovements%20in%20models%20with%20different%20parametrizations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Dynamics%2520of%2520In-Context%2520Learning%2520in%2520Linear%2520Attention%26entry.906535625%3DYedi%2520Zhang%2520and%2520Aaditya%2520K.%2520Singh%2520and%2520Peter%2520E.%2520Latham%2520and%2520Andrew%2520Saxe%26entry.1292438233%3D%2520%2520While%2520attention-based%2520models%2520have%2520demonstrated%2520the%2520remarkable%2520ability%2520of%250Ain-context%2520learning%252C%2520the%2520theoretical%2520understanding%2520of%2520how%2520these%2520models%2520acquired%250Athis%2520ability%2520through%2520gradient%2520descent%2520training%2520is%2520still%2520preliminary.%2520Towards%250Aanswering%2520this%2520question%252C%2520we%2520study%2520the%2520gradient%2520descent%2520dynamics%2520of%2520multi-head%250Alinear%2520self-attention%2520trained%2520for%2520in-context%2520linear%2520regression.%2520We%2520examine%2520two%250Aparametrizations%2520of%2520linear%2520self-attention%253A%2520one%2520with%2520the%2520key%2520and%2520query%2520weights%250Amerged%2520as%2520a%2520single%2520matrix%2520%2528common%2520in%2520theoretical%2520studies%2529%252C%2520and%2520one%2520with%250Aseparate%2520key%2520and%2520query%2520matrices%2520%2528closer%2520to%2520practical%2520settings%2529.%2520For%2520the%2520merged%250Aparametrization%252C%2520we%2520show%2520the%2520training%2520dynamics%2520has%2520two%2520fixed%2520points%2520and%2520the%250Aloss%2520trajectory%2520exhibits%2520a%2520single%252C%2520abrupt%2520drop.%2520We%2520derive%2520an%2520analytical%250Atime-course%2520solution%2520for%2520a%2520certain%2520class%2520of%2520datasets%2520and%2520initialization.%2520For%250Athe%2520separate%2520parametrization%252C%2520we%2520show%2520the%2520training%2520dynamics%2520has%2520exponentially%250Amany%2520fixed%2520points%2520and%2520the%2520loss%2520exhibits%2520saddle-to-saddle%2520dynamics%252C%2520which%2520we%250Areduce%2520to%2520scalar%2520ordinary%2520differential%2520equations.%2520During%2520training%252C%2520the%2520model%250Aimplements%2520principal%2520component%2520regression%2520in%2520context%2520with%2520the%2520number%2520of%250Aprincipal%2520components%2520increasing%2520over%2520training%2520time.%2520Overall%252C%2520we%2520characterize%250Ahow%2520in-context%2520learning%2520abilities%2520evolve%2520during%2520gradient%2520descent%2520training%2520of%250Alinear%2520attention%252C%2520revealing%2520dynamics%2520of%2520abrupt%2520acquisition%2520versus%2520progressive%250Aimprovements%2520in%2520models%2520with%2520different%2520parametrizations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Dynamics%20of%20In-Context%20Learning%20in%20Linear%20Attention&entry.906535625=Yedi%20Zhang%20and%20Aaditya%20K.%20Singh%20and%20Peter%20E.%20Latham%20and%20Andrew%20Saxe&entry.1292438233=%20%20While%20attention-based%20models%20have%20demonstrated%20the%20remarkable%20ability%20of%0Ain-context%20learning%2C%20the%20theoretical%20understanding%20of%20how%20these%20models%20acquired%0Athis%20ability%20through%20gradient%20descent%20training%20is%20still%20preliminary.%20Towards%0Aanswering%20this%20question%2C%20we%20study%20the%20gradient%20descent%20dynamics%20of%20multi-head%0Alinear%20self-attention%20trained%20for%20in-context%20linear%20regression.%20We%20examine%20two%0Aparametrizations%20of%20linear%20self-attention%3A%20one%20with%20the%20key%20and%20query%20weights%0Amerged%20as%20a%20single%20matrix%20%28common%20in%20theoretical%20studies%29%2C%20and%20one%20with%0Aseparate%20key%20and%20query%20matrices%20%28closer%20to%20practical%20settings%29.%20For%20the%20merged%0Aparametrization%2C%20we%20show%20the%20training%20dynamics%20has%20two%20fixed%20points%20and%20the%0Aloss%20trajectory%20exhibits%20a%20single%2C%20abrupt%20drop.%20We%20derive%20an%20analytical%0Atime-course%20solution%20for%20a%20certain%20class%20of%20datasets%20and%20initialization.%20For%0Athe%20separate%20parametrization%2C%20we%20show%20the%20training%20dynamics%20has%20exponentially%0Amany%20fixed%20points%20and%20the%20loss%20exhibits%20saddle-to-saddle%20dynamics%2C%20which%20we%0Areduce%20to%20scalar%20ordinary%20differential%20equations.%20During%20training%2C%20the%20model%0Aimplements%20principal%20component%20regression%20in%20context%20with%20the%20number%20of%0Aprincipal%20components%20increasing%20over%20training%20time.%20Overall%2C%20we%20characterize%0Ahow%20in-context%20learning%20abilities%20evolve%20during%20gradient%20descent%20training%20of%0Alinear%20attention%2C%20revealing%20dynamics%20of%20abrupt%20acquisition%20versus%20progressive%0Aimprovements%20in%20models%20with%20different%20parametrizations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16265v1&entry.124074799=Read"},
{"title": "RelightVid: Temporal-Consistent Diffusion Model for Video Relighting", "author": "Ye Fang and Zeyi Sun and Shangzhan Zhang and Tong Wu and Yinghao Xu and Pan Zhang and Jiaqi Wang and Gordon Wetzstein and Dahua Lin", "abstract": "  Diffusion models have demonstrated remarkable success in image generation and\nediting, with recent advancements enabling albedo-preserving image relighting.\nHowever, applying these models to video relighting remains challenging due to\nthe lack of paired video relighting datasets and the high demands for output\nfidelity and temporal consistency, further complicated by the inherent\nrandomness of diffusion models. To address these challenges, we introduce\nRelightVid, a flexible framework for video relighting that can accept\nbackground video, text prompts, or environment maps as relighting conditions.\nTrained on in-the-wild videos with carefully designed illumination\naugmentations and rendered videos under extreme dynamic lighting, RelightVid\nachieves arbitrary video relighting with high temporal consistency without\nintrinsic decomposition while preserving the illumination priors of its image\nbackbone.\n", "link": "http://arxiv.org/abs/2501.16330v1", "date": "2025-01-27", "relevancy": 2.4834, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6322}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6226}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelightVid%3A%20Temporal-Consistent%20Diffusion%20Model%20for%20Video%20Relighting&body=Title%3A%20RelightVid%3A%20Temporal-Consistent%20Diffusion%20Model%20for%20Video%20Relighting%0AAuthor%3A%20Ye%20Fang%20and%20Zeyi%20Sun%20and%20Shangzhan%20Zhang%20and%20Tong%20Wu%20and%20Yinghao%20Xu%20and%20Pan%20Zhang%20and%20Jiaqi%20Wang%20and%20Gordon%20Wetzstein%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20remarkable%20success%20in%20image%20generation%20and%0Aediting%2C%20with%20recent%20advancements%20enabling%20albedo-preserving%20image%20relighting.%0AHowever%2C%20applying%20these%20models%20to%20video%20relighting%20remains%20challenging%20due%20to%0Athe%20lack%20of%20paired%20video%20relighting%20datasets%20and%20the%20high%20demands%20for%20output%0Afidelity%20and%20temporal%20consistency%2C%20further%20complicated%20by%20the%20inherent%0Arandomness%20of%20diffusion%20models.%20To%20address%20these%20challenges%2C%20we%20introduce%0ARelightVid%2C%20a%20flexible%20framework%20for%20video%20relighting%20that%20can%20accept%0Abackground%20video%2C%20text%20prompts%2C%20or%20environment%20maps%20as%20relighting%20conditions.%0ATrained%20on%20in-the-wild%20videos%20with%20carefully%20designed%20illumination%0Aaugmentations%20and%20rendered%20videos%20under%20extreme%20dynamic%20lighting%2C%20RelightVid%0Aachieves%20arbitrary%20video%20relighting%20with%20high%20temporal%20consistency%20without%0Aintrinsic%20decomposition%20while%20preserving%20the%20illumination%20priors%20of%20its%20image%0Abackbone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelightVid%253A%2520Temporal-Consistent%2520Diffusion%2520Model%2520for%2520Video%2520Relighting%26entry.906535625%3DYe%2520Fang%2520and%2520Zeyi%2520Sun%2520and%2520Shangzhan%2520Zhang%2520and%2520Tong%2520Wu%2520and%2520Yinghao%2520Xu%2520and%2520Pan%2520Zhang%2520and%2520Jiaqi%2520Wang%2520and%2520Gordon%2520Wetzstein%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520image%2520generation%2520and%250Aediting%252C%2520with%2520recent%2520advancements%2520enabling%2520albedo-preserving%2520image%2520relighting.%250AHowever%252C%2520applying%2520these%2520models%2520to%2520video%2520relighting%2520remains%2520challenging%2520due%2520to%250Athe%2520lack%2520of%2520paired%2520video%2520relighting%2520datasets%2520and%2520the%2520high%2520demands%2520for%2520output%250Afidelity%2520and%2520temporal%2520consistency%252C%2520further%2520complicated%2520by%2520the%2520inherent%250Arandomness%2520of%2520diffusion%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250ARelightVid%252C%2520a%2520flexible%2520framework%2520for%2520video%2520relighting%2520that%2520can%2520accept%250Abackground%2520video%252C%2520text%2520prompts%252C%2520or%2520environment%2520maps%2520as%2520relighting%2520conditions.%250ATrained%2520on%2520in-the-wild%2520videos%2520with%2520carefully%2520designed%2520illumination%250Aaugmentations%2520and%2520rendered%2520videos%2520under%2520extreme%2520dynamic%2520lighting%252C%2520RelightVid%250Aachieves%2520arbitrary%2520video%2520relighting%2520with%2520high%2520temporal%2520consistency%2520without%250Aintrinsic%2520decomposition%2520while%2520preserving%2520the%2520illumination%2520priors%2520of%2520its%2520image%250Abackbone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelightVid%3A%20Temporal-Consistent%20Diffusion%20Model%20for%20Video%20Relighting&entry.906535625=Ye%20Fang%20and%20Zeyi%20Sun%20and%20Shangzhan%20Zhang%20and%20Tong%20Wu%20and%20Yinghao%20Xu%20and%20Pan%20Zhang%20and%20Jiaqi%20Wang%20and%20Gordon%20Wetzstein%20and%20Dahua%20Lin&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20remarkable%20success%20in%20image%20generation%20and%0Aediting%2C%20with%20recent%20advancements%20enabling%20albedo-preserving%20image%20relighting.%0AHowever%2C%20applying%20these%20models%20to%20video%20relighting%20remains%20challenging%20due%20to%0Athe%20lack%20of%20paired%20video%20relighting%20datasets%20and%20the%20high%20demands%20for%20output%0Afidelity%20and%20temporal%20consistency%2C%20further%20complicated%20by%20the%20inherent%0Arandomness%20of%20diffusion%20models.%20To%20address%20these%20challenges%2C%20we%20introduce%0ARelightVid%2C%20a%20flexible%20framework%20for%20video%20relighting%20that%20can%20accept%0Abackground%20video%2C%20text%20prompts%2C%20or%20environment%20maps%20as%20relighting%20conditions.%0ATrained%20on%20in-the-wild%20videos%20with%20carefully%20designed%20illumination%0Aaugmentations%20and%20rendered%20videos%20under%20extreme%20dynamic%20lighting%2C%20RelightVid%0Aachieves%20arbitrary%20video%20relighting%20with%20high%20temporal%20consistency%20without%0Aintrinsic%20decomposition%20while%20preserving%20the%20illumination%20priors%20of%20its%20image%0Abackbone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16330v1&entry.124074799=Read"},
{"title": "Path Analysis for Effective Fault Localization in Deep Neural Networks", "author": "Soroush Hashemifar and Saeed Parsa and Akram Kalaee", "abstract": "  Deep learning has revolutionized numerous fields, yet the reliability of Deep\nNeural Networks (DNNs) remains a concern due to their complexity and data\ndependency. Traditional software fault localization methods, such as\nSpectrum-based Fault Localization (SBFL), have been adapted for DNNs but often\nfall short in effectiveness. These methods typically overlook the propagation\nof faults through neural pathways, resulting in less precise fault detection.\nResearch indicates that examining neural pathways, rather than individual\nneurons, is crucial because issues in one neuron can affect its entire pathway.\nBy investigating these interconnected pathways, we can better identify and\naddress problems arising from the collective activity of neurons. To address\nthis limitation, we introduce the NP-SBFL method, which leverages Layer-wise\nRelevance Propagation (LRP) to identify essential faulty neural pathways. Our\nmethod explores multiple fault sources to accurately pinpoint faulty neurons by\nanalyzing their interconnections. Additionally, our multi-stage gradient ascent\n(MGA) technique, an extension of gradient ascent (GA), enables sequential\nneuron activation to enhance fault detection. We evaluated NP-SBFL-MGA on the\nwell-established MNIST and CIFAR-10 datasets, comparing it to other methods\nlike DeepFault and NP-SBFL-GA, as well as three neuron measures: Tarantula,\nOchiai, and Barinel. Our evaluation utilized all training and test samples\n(60,000 for MNIST and 50,000 for CIFAR-10) and revealed that NP-SBFL-MGA\nsignificantly outperformed the baselines in identifying suspicious pathways and\ngenerating adversarial inputs. Notably, Tarantula with NP-SBFL-MGA achieved a\nremarkable 96.75% fault detection rate compared to DeepFault's 89.90%.\nNP-SBFL-MGA highlights a strong correlation between critical path coverage and\nthe number of failed tests in DNN fault localization.\n", "link": "http://arxiv.org/abs/2310.18987v4", "date": "2025-01-27", "relevancy": 2.4812, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5104}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4895}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Path%20Analysis%20for%20Effective%20Fault%20Localization%20in%20Deep%20Neural%20Networks&body=Title%3A%20Path%20Analysis%20for%20Effective%20Fault%20Localization%20in%20Deep%20Neural%20Networks%0AAuthor%3A%20Soroush%20Hashemifar%20and%20Saeed%20Parsa%20and%20Akram%20Kalaee%0AAbstract%3A%20%20%20Deep%20learning%20has%20revolutionized%20numerous%20fields%2C%20yet%20the%20reliability%20of%20Deep%0ANeural%20Networks%20%28DNNs%29%20remains%20a%20concern%20due%20to%20their%20complexity%20and%20data%0Adependency.%20Traditional%20software%20fault%20localization%20methods%2C%20such%20as%0ASpectrum-based%20Fault%20Localization%20%28SBFL%29%2C%20have%20been%20adapted%20for%20DNNs%20but%20often%0Afall%20short%20in%20effectiveness.%20These%20methods%20typically%20overlook%20the%20propagation%0Aof%20faults%20through%20neural%20pathways%2C%20resulting%20in%20less%20precise%20fault%20detection.%0AResearch%20indicates%20that%20examining%20neural%20pathways%2C%20rather%20than%20individual%0Aneurons%2C%20is%20crucial%20because%20issues%20in%20one%20neuron%20can%20affect%20its%20entire%20pathway.%0ABy%20investigating%20these%20interconnected%20pathways%2C%20we%20can%20better%20identify%20and%0Aaddress%20problems%20arising%20from%20the%20collective%20activity%20of%20neurons.%20To%20address%0Athis%20limitation%2C%20we%20introduce%20the%20NP-SBFL%20method%2C%20which%20leverages%20Layer-wise%0ARelevance%20Propagation%20%28LRP%29%20to%20identify%20essential%20faulty%20neural%20pathways.%20Our%0Amethod%20explores%20multiple%20fault%20sources%20to%20accurately%20pinpoint%20faulty%20neurons%20by%0Aanalyzing%20their%20interconnections.%20Additionally%2C%20our%20multi-stage%20gradient%20ascent%0A%28MGA%29%20technique%2C%20an%20extension%20of%20gradient%20ascent%20%28GA%29%2C%20enables%20sequential%0Aneuron%20activation%20to%20enhance%20fault%20detection.%20We%20evaluated%20NP-SBFL-MGA%20on%20the%0Awell-established%20MNIST%20and%20CIFAR-10%20datasets%2C%20comparing%20it%20to%20other%20methods%0Alike%20DeepFault%20and%20NP-SBFL-GA%2C%20as%20well%20as%20three%20neuron%20measures%3A%20Tarantula%2C%0AOchiai%2C%20and%20Barinel.%20Our%20evaluation%20utilized%20all%20training%20and%20test%20samples%0A%2860%2C000%20for%20MNIST%20and%2050%2C000%20for%20CIFAR-10%29%20and%20revealed%20that%20NP-SBFL-MGA%0Asignificantly%20outperformed%20the%20baselines%20in%20identifying%20suspicious%20pathways%20and%0Agenerating%20adversarial%20inputs.%20Notably%2C%20Tarantula%20with%20NP-SBFL-MGA%20achieved%20a%0Aremarkable%2096.75%25%20fault%20detection%20rate%20compared%20to%20DeepFault%27s%2089.90%25.%0ANP-SBFL-MGA%20highlights%20a%20strong%20correlation%20between%20critical%20path%20coverage%20and%0Athe%20number%20of%20failed%20tests%20in%20DNN%20fault%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18987v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPath%2520Analysis%2520for%2520Effective%2520Fault%2520Localization%2520in%2520Deep%2520Neural%2520Networks%26entry.906535625%3DSoroush%2520Hashemifar%2520and%2520Saeed%2520Parsa%2520and%2520Akram%2520Kalaee%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520revolutionized%2520numerous%2520fields%252C%2520yet%2520the%2520reliability%2520of%2520Deep%250ANeural%2520Networks%2520%2528DNNs%2529%2520remains%2520a%2520concern%2520due%2520to%2520their%2520complexity%2520and%2520data%250Adependency.%2520Traditional%2520software%2520fault%2520localization%2520methods%252C%2520such%2520as%250ASpectrum-based%2520Fault%2520Localization%2520%2528SBFL%2529%252C%2520have%2520been%2520adapted%2520for%2520DNNs%2520but%2520often%250Afall%2520short%2520in%2520effectiveness.%2520These%2520methods%2520typically%2520overlook%2520the%2520propagation%250Aof%2520faults%2520through%2520neural%2520pathways%252C%2520resulting%2520in%2520less%2520precise%2520fault%2520detection.%250AResearch%2520indicates%2520that%2520examining%2520neural%2520pathways%252C%2520rather%2520than%2520individual%250Aneurons%252C%2520is%2520crucial%2520because%2520issues%2520in%2520one%2520neuron%2520can%2520affect%2520its%2520entire%2520pathway.%250ABy%2520investigating%2520these%2520interconnected%2520pathways%252C%2520we%2520can%2520better%2520identify%2520and%250Aaddress%2520problems%2520arising%2520from%2520the%2520collective%2520activity%2520of%2520neurons.%2520To%2520address%250Athis%2520limitation%252C%2520we%2520introduce%2520the%2520NP-SBFL%2520method%252C%2520which%2520leverages%2520Layer-wise%250ARelevance%2520Propagation%2520%2528LRP%2529%2520to%2520identify%2520essential%2520faulty%2520neural%2520pathways.%2520Our%250Amethod%2520explores%2520multiple%2520fault%2520sources%2520to%2520accurately%2520pinpoint%2520faulty%2520neurons%2520by%250Aanalyzing%2520their%2520interconnections.%2520Additionally%252C%2520our%2520multi-stage%2520gradient%2520ascent%250A%2528MGA%2529%2520technique%252C%2520an%2520extension%2520of%2520gradient%2520ascent%2520%2528GA%2529%252C%2520enables%2520sequential%250Aneuron%2520activation%2520to%2520enhance%2520fault%2520detection.%2520We%2520evaluated%2520NP-SBFL-MGA%2520on%2520the%250Awell-established%2520MNIST%2520and%2520CIFAR-10%2520datasets%252C%2520comparing%2520it%2520to%2520other%2520methods%250Alike%2520DeepFault%2520and%2520NP-SBFL-GA%252C%2520as%2520well%2520as%2520three%2520neuron%2520measures%253A%2520Tarantula%252C%250AOchiai%252C%2520and%2520Barinel.%2520Our%2520evaluation%2520utilized%2520all%2520training%2520and%2520test%2520samples%250A%252860%252C000%2520for%2520MNIST%2520and%252050%252C000%2520for%2520CIFAR-10%2529%2520and%2520revealed%2520that%2520NP-SBFL-MGA%250Asignificantly%2520outperformed%2520the%2520baselines%2520in%2520identifying%2520suspicious%2520pathways%2520and%250Agenerating%2520adversarial%2520inputs.%2520Notably%252C%2520Tarantula%2520with%2520NP-SBFL-MGA%2520achieved%2520a%250Aremarkable%252096.75%2525%2520fault%2520detection%2520rate%2520compared%2520to%2520DeepFault%2527s%252089.90%2525.%250ANP-SBFL-MGA%2520highlights%2520a%2520strong%2520correlation%2520between%2520critical%2520path%2520coverage%2520and%250Athe%2520number%2520of%2520failed%2520tests%2520in%2520DNN%2520fault%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18987v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Path%20Analysis%20for%20Effective%20Fault%20Localization%20in%20Deep%20Neural%20Networks&entry.906535625=Soroush%20Hashemifar%20and%20Saeed%20Parsa%20and%20Akram%20Kalaee&entry.1292438233=%20%20Deep%20learning%20has%20revolutionized%20numerous%20fields%2C%20yet%20the%20reliability%20of%20Deep%0ANeural%20Networks%20%28DNNs%29%20remains%20a%20concern%20due%20to%20their%20complexity%20and%20data%0Adependency.%20Traditional%20software%20fault%20localization%20methods%2C%20such%20as%0ASpectrum-based%20Fault%20Localization%20%28SBFL%29%2C%20have%20been%20adapted%20for%20DNNs%20but%20often%0Afall%20short%20in%20effectiveness.%20These%20methods%20typically%20overlook%20the%20propagation%0Aof%20faults%20through%20neural%20pathways%2C%20resulting%20in%20less%20precise%20fault%20detection.%0AResearch%20indicates%20that%20examining%20neural%20pathways%2C%20rather%20than%20individual%0Aneurons%2C%20is%20crucial%20because%20issues%20in%20one%20neuron%20can%20affect%20its%20entire%20pathway.%0ABy%20investigating%20these%20interconnected%20pathways%2C%20we%20can%20better%20identify%20and%0Aaddress%20problems%20arising%20from%20the%20collective%20activity%20of%20neurons.%20To%20address%0Athis%20limitation%2C%20we%20introduce%20the%20NP-SBFL%20method%2C%20which%20leverages%20Layer-wise%0ARelevance%20Propagation%20%28LRP%29%20to%20identify%20essential%20faulty%20neural%20pathways.%20Our%0Amethod%20explores%20multiple%20fault%20sources%20to%20accurately%20pinpoint%20faulty%20neurons%20by%0Aanalyzing%20their%20interconnections.%20Additionally%2C%20our%20multi-stage%20gradient%20ascent%0A%28MGA%29%20technique%2C%20an%20extension%20of%20gradient%20ascent%20%28GA%29%2C%20enables%20sequential%0Aneuron%20activation%20to%20enhance%20fault%20detection.%20We%20evaluated%20NP-SBFL-MGA%20on%20the%0Awell-established%20MNIST%20and%20CIFAR-10%20datasets%2C%20comparing%20it%20to%20other%20methods%0Alike%20DeepFault%20and%20NP-SBFL-GA%2C%20as%20well%20as%20three%20neuron%20measures%3A%20Tarantula%2C%0AOchiai%2C%20and%20Barinel.%20Our%20evaluation%20utilized%20all%20training%20and%20test%20samples%0A%2860%2C000%20for%20MNIST%20and%2050%2C000%20for%20CIFAR-10%29%20and%20revealed%20that%20NP-SBFL-MGA%0Asignificantly%20outperformed%20the%20baselines%20in%20identifying%20suspicious%20pathways%20and%0Agenerating%20adversarial%20inputs.%20Notably%2C%20Tarantula%20with%20NP-SBFL-MGA%20achieved%20a%0Aremarkable%2096.75%25%20fault%20detection%20rate%20compared%20to%20DeepFault%27s%2089.90%25.%0ANP-SBFL-MGA%20highlights%20a%20strong%20correlation%20between%20critical%20path%20coverage%20and%0Athe%20number%20of%20failed%20tests%20in%20DNN%20fault%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18987v4&entry.124074799=Read"},
{"title": "Hierarchical Mixture of Experts: Generalizable Learning for High-Level\n  Synthesis", "author": "Weikai Li and Ding Wang and Zijian Ding and Atefeh Sohrabizadeh and Zongyue Qin and Jason Cong and Yizhou Sun", "abstract": "  High-level synthesis (HLS) is a widely used tool in designing Field\nProgrammable Gate Array (FPGA). HLS enables FPGA design with software\nprogramming languages by compiling the source code into an FPGA circuit. The\nsource code includes a program (called ``kernel'') and several pragmas that\ninstruct hardware synthesis, such as parallelization, pipeline, etc. While it\nis relatively easy for software developers to design the program, it heavily\nrelies on hardware knowledge to design the pragmas, posing a big challenge for\nsoftware developers. Recently, different machine learning algorithms, such as\nGNNs, have been proposed to automate the pragma design via performance\nprediction. However, when applying the trained model on new kernels, the\nsignificant domain shift often leads to unsatisfactory performance. We propose\na more domain-generalizable model structure: a two-level hierarchical Mixture\nof Experts (MoE), that can be flexibly adapted to any GNN model. Different\nexpert networks can learn to deal with different regions in the representation\nspace, and they can utilize similar patterns between the old kernels and new\nkernels. In the low-level MoE, we apply MoE on three natural granularities of a\nprogram: node, basic block, and graph. The high-level MoE learns to aggregate\nthe three granularities for the final decision. To stably train the\nhierarchical MoE, we further propose a two-stage training method. Extensive\nexperiments verify the effectiveness of the hierarchical MoE.\n", "link": "http://arxiv.org/abs/2410.19225v3", "date": "2025-01-27", "relevancy": 2.4783, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5035}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4917}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Mixture%20of%20Experts%3A%20Generalizable%20Learning%20for%20High-Level%0A%20%20Synthesis&body=Title%3A%20Hierarchical%20Mixture%20of%20Experts%3A%20Generalizable%20Learning%20for%20High-Level%0A%20%20Synthesis%0AAuthor%3A%20Weikai%20Li%20and%20Ding%20Wang%20and%20Zijian%20Ding%20and%20Atefeh%20Sohrabizadeh%20and%20Zongyue%20Qin%20and%20Jason%20Cong%20and%20Yizhou%20Sun%0AAbstract%3A%20%20%20High-level%20synthesis%20%28HLS%29%20is%20a%20widely%20used%20tool%20in%20designing%20Field%0AProgrammable%20Gate%20Array%20%28FPGA%29.%20HLS%20enables%20FPGA%20design%20with%20software%0Aprogramming%20languages%20by%20compiling%20the%20source%20code%20into%20an%20FPGA%20circuit.%20The%0Asource%20code%20includes%20a%20program%20%28called%20%60%60kernel%27%27%29%20and%20several%20pragmas%20that%0Ainstruct%20hardware%20synthesis%2C%20such%20as%20parallelization%2C%20pipeline%2C%20etc.%20While%20it%0Ais%20relatively%20easy%20for%20software%20developers%20to%20design%20the%20program%2C%20it%20heavily%0Arelies%20on%20hardware%20knowledge%20to%20design%20the%20pragmas%2C%20posing%20a%20big%20challenge%20for%0Asoftware%20developers.%20Recently%2C%20different%20machine%20learning%20algorithms%2C%20such%20as%0AGNNs%2C%20have%20been%20proposed%20to%20automate%20the%20pragma%20design%20via%20performance%0Aprediction.%20However%2C%20when%20applying%20the%20trained%20model%20on%20new%20kernels%2C%20the%0Asignificant%20domain%20shift%20often%20leads%20to%20unsatisfactory%20performance.%20We%20propose%0Aa%20more%20domain-generalizable%20model%20structure%3A%20a%20two-level%20hierarchical%20Mixture%0Aof%20Experts%20%28MoE%29%2C%20that%20can%20be%20flexibly%20adapted%20to%20any%20GNN%20model.%20Different%0Aexpert%20networks%20can%20learn%20to%20deal%20with%20different%20regions%20in%20the%20representation%0Aspace%2C%20and%20they%20can%20utilize%20similar%20patterns%20between%20the%20old%20kernels%20and%20new%0Akernels.%20In%20the%20low-level%20MoE%2C%20we%20apply%20MoE%20on%20three%20natural%20granularities%20of%20a%0Aprogram%3A%20node%2C%20basic%20block%2C%20and%20graph.%20The%20high-level%20MoE%20learns%20to%20aggregate%0Athe%20three%20granularities%20for%20the%20final%20decision.%20To%20stably%20train%20the%0Ahierarchical%20MoE%2C%20we%20further%20propose%20a%20two-stage%20training%20method.%20Extensive%0Aexperiments%20verify%20the%20effectiveness%20of%20the%20hierarchical%20MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19225v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Mixture%2520of%2520Experts%253A%2520Generalizable%2520Learning%2520for%2520High-Level%250A%2520%2520Synthesis%26entry.906535625%3DWeikai%2520Li%2520and%2520Ding%2520Wang%2520and%2520Zijian%2520Ding%2520and%2520Atefeh%2520Sohrabizadeh%2520and%2520Zongyue%2520Qin%2520and%2520Jason%2520Cong%2520and%2520Yizhou%2520Sun%26entry.1292438233%3D%2520%2520High-level%2520synthesis%2520%2528HLS%2529%2520is%2520a%2520widely%2520used%2520tool%2520in%2520designing%2520Field%250AProgrammable%2520Gate%2520Array%2520%2528FPGA%2529.%2520HLS%2520enables%2520FPGA%2520design%2520with%2520software%250Aprogramming%2520languages%2520by%2520compiling%2520the%2520source%2520code%2520into%2520an%2520FPGA%2520circuit.%2520The%250Asource%2520code%2520includes%2520a%2520program%2520%2528called%2520%2560%2560kernel%2527%2527%2529%2520and%2520several%2520pragmas%2520that%250Ainstruct%2520hardware%2520synthesis%252C%2520such%2520as%2520parallelization%252C%2520pipeline%252C%2520etc.%2520While%2520it%250Ais%2520relatively%2520easy%2520for%2520software%2520developers%2520to%2520design%2520the%2520program%252C%2520it%2520heavily%250Arelies%2520on%2520hardware%2520knowledge%2520to%2520design%2520the%2520pragmas%252C%2520posing%2520a%2520big%2520challenge%2520for%250Asoftware%2520developers.%2520Recently%252C%2520different%2520machine%2520learning%2520algorithms%252C%2520such%2520as%250AGNNs%252C%2520have%2520been%2520proposed%2520to%2520automate%2520the%2520pragma%2520design%2520via%2520performance%250Aprediction.%2520However%252C%2520when%2520applying%2520the%2520trained%2520model%2520on%2520new%2520kernels%252C%2520the%250Asignificant%2520domain%2520shift%2520often%2520leads%2520to%2520unsatisfactory%2520performance.%2520We%2520propose%250Aa%2520more%2520domain-generalizable%2520model%2520structure%253A%2520a%2520two-level%2520hierarchical%2520Mixture%250Aof%2520Experts%2520%2528MoE%2529%252C%2520that%2520can%2520be%2520flexibly%2520adapted%2520to%2520any%2520GNN%2520model.%2520Different%250Aexpert%2520networks%2520can%2520learn%2520to%2520deal%2520with%2520different%2520regions%2520in%2520the%2520representation%250Aspace%252C%2520and%2520they%2520can%2520utilize%2520similar%2520patterns%2520between%2520the%2520old%2520kernels%2520and%2520new%250Akernels.%2520In%2520the%2520low-level%2520MoE%252C%2520we%2520apply%2520MoE%2520on%2520three%2520natural%2520granularities%2520of%2520a%250Aprogram%253A%2520node%252C%2520basic%2520block%252C%2520and%2520graph.%2520The%2520high-level%2520MoE%2520learns%2520to%2520aggregate%250Athe%2520three%2520granularities%2520for%2520the%2520final%2520decision.%2520To%2520stably%2520train%2520the%250Ahierarchical%2520MoE%252C%2520we%2520further%2520propose%2520a%2520two-stage%2520training%2520method.%2520Extensive%250Aexperiments%2520verify%2520the%2520effectiveness%2520of%2520the%2520hierarchical%2520MoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19225v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Mixture%20of%20Experts%3A%20Generalizable%20Learning%20for%20High-Level%0A%20%20Synthesis&entry.906535625=Weikai%20Li%20and%20Ding%20Wang%20and%20Zijian%20Ding%20and%20Atefeh%20Sohrabizadeh%20and%20Zongyue%20Qin%20and%20Jason%20Cong%20and%20Yizhou%20Sun&entry.1292438233=%20%20High-level%20synthesis%20%28HLS%29%20is%20a%20widely%20used%20tool%20in%20designing%20Field%0AProgrammable%20Gate%20Array%20%28FPGA%29.%20HLS%20enables%20FPGA%20design%20with%20software%0Aprogramming%20languages%20by%20compiling%20the%20source%20code%20into%20an%20FPGA%20circuit.%20The%0Asource%20code%20includes%20a%20program%20%28called%20%60%60kernel%27%27%29%20and%20several%20pragmas%20that%0Ainstruct%20hardware%20synthesis%2C%20such%20as%20parallelization%2C%20pipeline%2C%20etc.%20While%20it%0Ais%20relatively%20easy%20for%20software%20developers%20to%20design%20the%20program%2C%20it%20heavily%0Arelies%20on%20hardware%20knowledge%20to%20design%20the%20pragmas%2C%20posing%20a%20big%20challenge%20for%0Asoftware%20developers.%20Recently%2C%20different%20machine%20learning%20algorithms%2C%20such%20as%0AGNNs%2C%20have%20been%20proposed%20to%20automate%20the%20pragma%20design%20via%20performance%0Aprediction.%20However%2C%20when%20applying%20the%20trained%20model%20on%20new%20kernels%2C%20the%0Asignificant%20domain%20shift%20often%20leads%20to%20unsatisfactory%20performance.%20We%20propose%0Aa%20more%20domain-generalizable%20model%20structure%3A%20a%20two-level%20hierarchical%20Mixture%0Aof%20Experts%20%28MoE%29%2C%20that%20can%20be%20flexibly%20adapted%20to%20any%20GNN%20model.%20Different%0Aexpert%20networks%20can%20learn%20to%20deal%20with%20different%20regions%20in%20the%20representation%0Aspace%2C%20and%20they%20can%20utilize%20similar%20patterns%20between%20the%20old%20kernels%20and%20new%0Akernels.%20In%20the%20low-level%20MoE%2C%20we%20apply%20MoE%20on%20three%20natural%20granularities%20of%20a%0Aprogram%3A%20node%2C%20basic%20block%2C%20and%20graph.%20The%20high-level%20MoE%20learns%20to%20aggregate%0Athe%20three%20granularities%20for%20the%20final%20decision.%20To%20stably%20train%20the%0Ahierarchical%20MoE%2C%20we%20further%20propose%20a%20two-stage%20training%20method.%20Extensive%0Aexperiments%20verify%20the%20effectiveness%20of%20the%20hierarchical%20MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19225v3&entry.124074799=Read"},
{"title": "SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP", "author": "Li Pang and Jing Yao and Kaiyu Li and Xiangyong Cao", "abstract": "  Hyperspectral image (HSI) classification aims at categorizing each pixel in\nan HSI into a specific land cover class, which is crucial for applications like\nremote sensing, environmental monitoring, and agriculture. Although deep\nlearning-based HSI classification methods have achieved significant\nadvancements, existing methods still rely on manually labeled data for\ntraining, which is both time-consuming and labor-intensive.To address this\nlimitation, we introduce a novel zero-shot hyperspectral image classification\nframework based on CLIP (SPECIAL), aiming to eliminate the need for manual\nannotations. The SPECIAL framework consists of two main stages: (1) CLIP-based\npseudo-label generation, and (2) noisy label learning. In the first stage, HSI\nis spectrally interpolated to produce RGB bands. These bands are subsequently\nclassified using CLIP, resulting in noisy pseudo-labels that are accompanied by\nconfidence scores.To improve the quality of these labels, we propose a scaling\nstrategy that fuses predictions from multiple spatial scales. In the second\nstage, spectral information and a label refinement technique are incorporated\nto mitigate label noise and further enhance classification accuracy.\nExperimental results on three benchmark datasets demonstrate that our SPECIAL\noutperforms existing methods in zero-shot HSI classification, showing its\npotential for more practical applications. The code is available at\nhttps://github.com/LiPang/SPECIAL.\n", "link": "http://arxiv.org/abs/2501.16222v1", "date": "2025-01-27", "relevancy": 2.4553, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4911}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPECIAL%3A%20Zero-shot%20Hyperspectral%20Image%20Classification%20With%20CLIP&body=Title%3A%20SPECIAL%3A%20Zero-shot%20Hyperspectral%20Image%20Classification%20With%20CLIP%0AAuthor%3A%20Li%20Pang%20and%20Jing%20Yao%20and%20Kaiyu%20Li%20and%20Xiangyong%20Cao%0AAbstract%3A%20%20%20Hyperspectral%20image%20%28HSI%29%20classification%20aims%20at%20categorizing%20each%20pixel%20in%0Aan%20HSI%20into%20a%20specific%20land%20cover%20class%2C%20which%20is%20crucial%20for%20applications%20like%0Aremote%20sensing%2C%20environmental%20monitoring%2C%20and%20agriculture.%20Although%20deep%0Alearning-based%20HSI%20classification%20methods%20have%20achieved%20significant%0Aadvancements%2C%20existing%20methods%20still%20rely%20on%20manually%20labeled%20data%20for%0Atraining%2C%20which%20is%20both%20time-consuming%20and%20labor-intensive.To%20address%20this%0Alimitation%2C%20we%20introduce%20a%20novel%20zero-shot%20hyperspectral%20image%20classification%0Aframework%20based%20on%20CLIP%20%28SPECIAL%29%2C%20aiming%20to%20eliminate%20the%20need%20for%20manual%0Aannotations.%20The%20SPECIAL%20framework%20consists%20of%20two%20main%20stages%3A%20%281%29%20CLIP-based%0Apseudo-label%20generation%2C%20and%20%282%29%20noisy%20label%20learning.%20In%20the%20first%20stage%2C%20HSI%0Ais%20spectrally%20interpolated%20to%20produce%20RGB%20bands.%20These%20bands%20are%20subsequently%0Aclassified%20using%20CLIP%2C%20resulting%20in%20noisy%20pseudo-labels%20that%20are%20accompanied%20by%0Aconfidence%20scores.To%20improve%20the%20quality%20of%20these%20labels%2C%20we%20propose%20a%20scaling%0Astrategy%20that%20fuses%20predictions%20from%20multiple%20spatial%20scales.%20In%20the%20second%0Astage%2C%20spectral%20information%20and%20a%20label%20refinement%20technique%20are%20incorporated%0Ato%20mitigate%20label%20noise%20and%20further%20enhance%20classification%20accuracy.%0AExperimental%20results%20on%20three%20benchmark%20datasets%20demonstrate%20that%20our%20SPECIAL%0Aoutperforms%20existing%20methods%20in%20zero-shot%20HSI%20classification%2C%20showing%20its%0Apotential%20for%20more%20practical%20applications.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiPang/SPECIAL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPECIAL%253A%2520Zero-shot%2520Hyperspectral%2520Image%2520Classification%2520With%2520CLIP%26entry.906535625%3DLi%2520Pang%2520and%2520Jing%2520Yao%2520and%2520Kaiyu%2520Li%2520and%2520Xiangyong%2520Cao%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520%2528HSI%2529%2520classification%2520aims%2520at%2520categorizing%2520each%2520pixel%2520in%250Aan%2520HSI%2520into%2520a%2520specific%2520land%2520cover%2520class%252C%2520which%2520is%2520crucial%2520for%2520applications%2520like%250Aremote%2520sensing%252C%2520environmental%2520monitoring%252C%2520and%2520agriculture.%2520Although%2520deep%250Alearning-based%2520HSI%2520classification%2520methods%2520have%2520achieved%2520significant%250Aadvancements%252C%2520existing%2520methods%2520still%2520rely%2520on%2520manually%2520labeled%2520data%2520for%250Atraining%252C%2520which%2520is%2520both%2520time-consuming%2520and%2520labor-intensive.To%2520address%2520this%250Alimitation%252C%2520we%2520introduce%2520a%2520novel%2520zero-shot%2520hyperspectral%2520image%2520classification%250Aframework%2520based%2520on%2520CLIP%2520%2528SPECIAL%2529%252C%2520aiming%2520to%2520eliminate%2520the%2520need%2520for%2520manual%250Aannotations.%2520The%2520SPECIAL%2520framework%2520consists%2520of%2520two%2520main%2520stages%253A%2520%25281%2529%2520CLIP-based%250Apseudo-label%2520generation%252C%2520and%2520%25282%2529%2520noisy%2520label%2520learning.%2520In%2520the%2520first%2520stage%252C%2520HSI%250Ais%2520spectrally%2520interpolated%2520to%2520produce%2520RGB%2520bands.%2520These%2520bands%2520are%2520subsequently%250Aclassified%2520using%2520CLIP%252C%2520resulting%2520in%2520noisy%2520pseudo-labels%2520that%2520are%2520accompanied%2520by%250Aconfidence%2520scores.To%2520improve%2520the%2520quality%2520of%2520these%2520labels%252C%2520we%2520propose%2520a%2520scaling%250Astrategy%2520that%2520fuses%2520predictions%2520from%2520multiple%2520spatial%2520scales.%2520In%2520the%2520second%250Astage%252C%2520spectral%2520information%2520and%2520a%2520label%2520refinement%2520technique%2520are%2520incorporated%250Ato%2520mitigate%2520label%2520noise%2520and%2520further%2520enhance%2520classification%2520accuracy.%250AExperimental%2520results%2520on%2520three%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520SPECIAL%250Aoutperforms%2520existing%2520methods%2520in%2520zero-shot%2520HSI%2520classification%252C%2520showing%2520its%250Apotential%2520for%2520more%2520practical%2520applications.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LiPang/SPECIAL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPECIAL%3A%20Zero-shot%20Hyperspectral%20Image%20Classification%20With%20CLIP&entry.906535625=Li%20Pang%20and%20Jing%20Yao%20and%20Kaiyu%20Li%20and%20Xiangyong%20Cao&entry.1292438233=%20%20Hyperspectral%20image%20%28HSI%29%20classification%20aims%20at%20categorizing%20each%20pixel%20in%0Aan%20HSI%20into%20a%20specific%20land%20cover%20class%2C%20which%20is%20crucial%20for%20applications%20like%0Aremote%20sensing%2C%20environmental%20monitoring%2C%20and%20agriculture.%20Although%20deep%0Alearning-based%20HSI%20classification%20methods%20have%20achieved%20significant%0Aadvancements%2C%20existing%20methods%20still%20rely%20on%20manually%20labeled%20data%20for%0Atraining%2C%20which%20is%20both%20time-consuming%20and%20labor-intensive.To%20address%20this%0Alimitation%2C%20we%20introduce%20a%20novel%20zero-shot%20hyperspectral%20image%20classification%0Aframework%20based%20on%20CLIP%20%28SPECIAL%29%2C%20aiming%20to%20eliminate%20the%20need%20for%20manual%0Aannotations.%20The%20SPECIAL%20framework%20consists%20of%20two%20main%20stages%3A%20%281%29%20CLIP-based%0Apseudo-label%20generation%2C%20and%20%282%29%20noisy%20label%20learning.%20In%20the%20first%20stage%2C%20HSI%0Ais%20spectrally%20interpolated%20to%20produce%20RGB%20bands.%20These%20bands%20are%20subsequently%0Aclassified%20using%20CLIP%2C%20resulting%20in%20noisy%20pseudo-labels%20that%20are%20accompanied%20by%0Aconfidence%20scores.To%20improve%20the%20quality%20of%20these%20labels%2C%20we%20propose%20a%20scaling%0Astrategy%20that%20fuses%20predictions%20from%20multiple%20spatial%20scales.%20In%20the%20second%0Astage%2C%20spectral%20information%20and%20a%20label%20refinement%20technique%20are%20incorporated%0Ato%20mitigate%20label%20noise%20and%20further%20enhance%20classification%20accuracy.%0AExperimental%20results%20on%20three%20benchmark%20datasets%20demonstrate%20that%20our%20SPECIAL%0Aoutperforms%20existing%20methods%20in%20zero-shot%20HSI%20classification%2C%20showing%20its%0Apotential%20for%20more%20practical%20applications.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiPang/SPECIAL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16222v1&entry.124074799=Read"},
{"title": "Gaussian entropic optimal transport: Schr\u00f6dinger bridges and the\n  Sinkhorn algorithm", "author": "O. Deniz Akyildiz and Pierre Del Moral and Joaqu\u00edn Miguez", "abstract": "  Entropic optimal transport problems are regularized versions of optimal\ntransport problems. These models play an increasingly important role in machine\nlearning and generative modelling. For finite spaces, these problems are\ncommonly solved using Sinkhorn algorithm (a.k.a. iterative proportional fitting\nprocedure). However, in more general settings the Sinkhorn iterations are based\non nonlinear conditional/conjugate transformations and exact finite-dimensional\nsolutions cannot be computed. This article presents a finite-dimensional\nrecursive formulation of the iterative proportional fitting procedure for\ngeneral Gaussian multivariate models. As expected, this recursive formulation\nis closely related to the celebrated Kalman filter and related Riccati matrix\ndifference equations, and it yields algorithms that can be implemented in\npractical settings without further approximations. We extend this filtering\nmethodology to develop a refined and self-contained convergence analysis of\nGaussian Sinkhorn algorithms, including closed form expressions of entropic\ntransport maps and Schr\\\"odinger bridges.\n", "link": "http://arxiv.org/abs/2412.18432v2", "date": "2025-01-27", "relevancy": 2.4431, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5094}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.485}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20entropic%20optimal%20transport%3A%20Schr%C3%B6dinger%20bridges%20and%20the%0A%20%20Sinkhorn%20algorithm&body=Title%3A%20Gaussian%20entropic%20optimal%20transport%3A%20Schr%C3%B6dinger%20bridges%20and%20the%0A%20%20Sinkhorn%20algorithm%0AAuthor%3A%20O.%20Deniz%20Akyildiz%20and%20Pierre%20Del%20Moral%20and%20Joaqu%C3%ADn%20Miguez%0AAbstract%3A%20%20%20Entropic%20optimal%20transport%20problems%20are%20regularized%20versions%20of%20optimal%0Atransport%20problems.%20These%20models%20play%20an%20increasingly%20important%20role%20in%20machine%0Alearning%20and%20generative%20modelling.%20For%20finite%20spaces%2C%20these%20problems%20are%0Acommonly%20solved%20using%20Sinkhorn%20algorithm%20%28a.k.a.%20iterative%20proportional%20fitting%0Aprocedure%29.%20However%2C%20in%20more%20general%20settings%20the%20Sinkhorn%20iterations%20are%20based%0Aon%20nonlinear%20conditional/conjugate%20transformations%20and%20exact%20finite-dimensional%0Asolutions%20cannot%20be%20computed.%20This%20article%20presents%20a%20finite-dimensional%0Arecursive%20formulation%20of%20the%20iterative%20proportional%20fitting%20procedure%20for%0Ageneral%20Gaussian%20multivariate%20models.%20As%20expected%2C%20this%20recursive%20formulation%0Ais%20closely%20related%20to%20the%20celebrated%20Kalman%20filter%20and%20related%20Riccati%20matrix%0Adifference%20equations%2C%20and%20it%20yields%20algorithms%20that%20can%20be%20implemented%20in%0Apractical%20settings%20without%20further%20approximations.%20We%20extend%20this%20filtering%0Amethodology%20to%20develop%20a%20refined%20and%20self-contained%20convergence%20analysis%20of%0AGaussian%20Sinkhorn%20algorithms%2C%20including%20closed%20form%20expressions%20of%20entropic%0Atransport%20maps%20and%20Schr%5C%22odinger%20bridges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520entropic%2520optimal%2520transport%253A%2520Schr%25C3%25B6dinger%2520bridges%2520and%2520the%250A%2520%2520Sinkhorn%2520algorithm%26entry.906535625%3DO.%2520Deniz%2520Akyildiz%2520and%2520Pierre%2520Del%2520Moral%2520and%2520Joaqu%25C3%25ADn%2520Miguez%26entry.1292438233%3D%2520%2520Entropic%2520optimal%2520transport%2520problems%2520are%2520regularized%2520versions%2520of%2520optimal%250Atransport%2520problems.%2520These%2520models%2520play%2520an%2520increasingly%2520important%2520role%2520in%2520machine%250Alearning%2520and%2520generative%2520modelling.%2520For%2520finite%2520spaces%252C%2520these%2520problems%2520are%250Acommonly%2520solved%2520using%2520Sinkhorn%2520algorithm%2520%2528a.k.a.%2520iterative%2520proportional%2520fitting%250Aprocedure%2529.%2520However%252C%2520in%2520more%2520general%2520settings%2520the%2520Sinkhorn%2520iterations%2520are%2520based%250Aon%2520nonlinear%2520conditional/conjugate%2520transformations%2520and%2520exact%2520finite-dimensional%250Asolutions%2520cannot%2520be%2520computed.%2520This%2520article%2520presents%2520a%2520finite-dimensional%250Arecursive%2520formulation%2520of%2520the%2520iterative%2520proportional%2520fitting%2520procedure%2520for%250Ageneral%2520Gaussian%2520multivariate%2520models.%2520As%2520expected%252C%2520this%2520recursive%2520formulation%250Ais%2520closely%2520related%2520to%2520the%2520celebrated%2520Kalman%2520filter%2520and%2520related%2520Riccati%2520matrix%250Adifference%2520equations%252C%2520and%2520it%2520yields%2520algorithms%2520that%2520can%2520be%2520implemented%2520in%250Apractical%2520settings%2520without%2520further%2520approximations.%2520We%2520extend%2520this%2520filtering%250Amethodology%2520to%2520develop%2520a%2520refined%2520and%2520self-contained%2520convergence%2520analysis%2520of%250AGaussian%2520Sinkhorn%2520algorithms%252C%2520including%2520closed%2520form%2520expressions%2520of%2520entropic%250Atransport%2520maps%2520and%2520Schr%255C%2522odinger%2520bridges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20entropic%20optimal%20transport%3A%20Schr%C3%B6dinger%20bridges%20and%20the%0A%20%20Sinkhorn%20algorithm&entry.906535625=O.%20Deniz%20Akyildiz%20and%20Pierre%20Del%20Moral%20and%20Joaqu%C3%ADn%20Miguez&entry.1292438233=%20%20Entropic%20optimal%20transport%20problems%20are%20regularized%20versions%20of%20optimal%0Atransport%20problems.%20These%20models%20play%20an%20increasingly%20important%20role%20in%20machine%0Alearning%20and%20generative%20modelling.%20For%20finite%20spaces%2C%20these%20problems%20are%0Acommonly%20solved%20using%20Sinkhorn%20algorithm%20%28a.k.a.%20iterative%20proportional%20fitting%0Aprocedure%29.%20However%2C%20in%20more%20general%20settings%20the%20Sinkhorn%20iterations%20are%20based%0Aon%20nonlinear%20conditional/conjugate%20transformations%20and%20exact%20finite-dimensional%0Asolutions%20cannot%20be%20computed.%20This%20article%20presents%20a%20finite-dimensional%0Arecursive%20formulation%20of%20the%20iterative%20proportional%20fitting%20procedure%20for%0Ageneral%20Gaussian%20multivariate%20models.%20As%20expected%2C%20this%20recursive%20formulation%0Ais%20closely%20related%20to%20the%20celebrated%20Kalman%20filter%20and%20related%20Riccati%20matrix%0Adifference%20equations%2C%20and%20it%20yields%20algorithms%20that%20can%20be%20implemented%20in%0Apractical%20settings%20without%20further%20approximations.%20We%20extend%20this%20filtering%0Amethodology%20to%20develop%20a%20refined%20and%20self-contained%20convergence%20analysis%20of%0AGaussian%20Sinkhorn%20algorithms%2C%20including%20closed%20form%20expressions%20of%20entropic%0Atransport%20maps%20and%20Schr%5C%22odinger%20bridges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18432v2&entry.124074799=Read"},
{"title": "Automatic Calibration of a Multi-Camera System with Limited Overlapping\n  Fields of View for 3D Surgical Scene Reconstruction", "author": "Tim Fl\u00fcckiger and Jonas Hein and Valery Fischer and Philipp F\u00fcrnstahl and Lilian Calvet", "abstract": "  Purpose: The purpose of this study is to develop an automated and accurate\nexternal camera calibration method for multi-camera systems used in 3D surgical\nscene reconstruction (3D-SSR), eliminating the need for operator intervention\nor specialized expertise. The method specifically addresses the problem of\nlimited overlapping fields of view caused by significant variations in optical\nzoom levels and camera locations.\n  Methods: We contribute a novel, fast, and fully automatic calibration method\nbased on the projection of multi-scale markers (MSMs) using a ceiling-mounted\nprojector. MSMs consist of 2D patterns projected at varying scales, ensuring\naccurate extraction of well distributed point correspondences across\nsignificantly different viewpoints and zoom levels. Validation is performed\nusing both synthetic and real data captured in a mock-up OR, with comparisons\nto traditional manual marker-based methods as well as markerless calibration\nmethods.\n  Results: The method achieves accuracy comparable to manual,\noperator-dependent calibration methods while exhibiting higher robustness under\nconditions of significant differences in zoom levels. Additionally, we show\nthat state-of-the-art Structure-from-Motion (SfM) pipelines are ineffective in\n3D-SSR settings, even when additional texture is projected onto the OR floor.\n  Conclusion: The use of a ceiling-mounted entry-level projector proves to be\nan effective alternative to operator-dependent, traditional marker-based\nmethods, paving the way for fully automated 3D-SSR.\n", "link": "http://arxiv.org/abs/2501.16221v1", "date": "2025-01-27", "relevancy": 2.4228, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6227}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6132}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Calibration%20of%20a%20Multi-Camera%20System%20with%20Limited%20Overlapping%0A%20%20Fields%20of%20View%20for%203D%20Surgical%20Scene%20Reconstruction&body=Title%3A%20Automatic%20Calibration%20of%20a%20Multi-Camera%20System%20with%20Limited%20Overlapping%0A%20%20Fields%20of%20View%20for%203D%20Surgical%20Scene%20Reconstruction%0AAuthor%3A%20Tim%20Fl%C3%BCckiger%20and%20Jonas%20Hein%20and%20Valery%20Fischer%20and%20Philipp%20F%C3%BCrnstahl%20and%20Lilian%20Calvet%0AAbstract%3A%20%20%20Purpose%3A%20The%20purpose%20of%20this%20study%20is%20to%20develop%20an%20automated%20and%20accurate%0Aexternal%20camera%20calibration%20method%20for%20multi-camera%20systems%20used%20in%203D%20surgical%0Ascene%20reconstruction%20%283D-SSR%29%2C%20eliminating%20the%20need%20for%20operator%20intervention%0Aor%20specialized%20expertise.%20The%20method%20specifically%20addresses%20the%20problem%20of%0Alimited%20overlapping%20fields%20of%20view%20caused%20by%20significant%20variations%20in%20optical%0Azoom%20levels%20and%20camera%20locations.%0A%20%20Methods%3A%20We%20contribute%20a%20novel%2C%20fast%2C%20and%20fully%20automatic%20calibration%20method%0Abased%20on%20the%20projection%20of%20multi-scale%20markers%20%28MSMs%29%20using%20a%20ceiling-mounted%0Aprojector.%20MSMs%20consist%20of%202D%20patterns%20projected%20at%20varying%20scales%2C%20ensuring%0Aaccurate%20extraction%20of%20well%20distributed%20point%20correspondences%20across%0Asignificantly%20different%20viewpoints%20and%20zoom%20levels.%20Validation%20is%20performed%0Ausing%20both%20synthetic%20and%20real%20data%20captured%20in%20a%20mock-up%20OR%2C%20with%20comparisons%0Ato%20traditional%20manual%20marker-based%20methods%20as%20well%20as%20markerless%20calibration%0Amethods.%0A%20%20Results%3A%20The%20method%20achieves%20accuracy%20comparable%20to%20manual%2C%0Aoperator-dependent%20calibration%20methods%20while%20exhibiting%20higher%20robustness%20under%0Aconditions%20of%20significant%20differences%20in%20zoom%20levels.%20Additionally%2C%20we%20show%0Athat%20state-of-the-art%20Structure-from-Motion%20%28SfM%29%20pipelines%20are%20ineffective%20in%0A3D-SSR%20settings%2C%20even%20when%20additional%20texture%20is%20projected%20onto%20the%20OR%20floor.%0A%20%20Conclusion%3A%20The%20use%20of%20a%20ceiling-mounted%20entry-level%20projector%20proves%20to%20be%0Aan%20effective%20alternative%20to%20operator-dependent%2C%20traditional%20marker-based%0Amethods%2C%20paving%20the%20way%20for%20fully%20automated%203D-SSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Calibration%2520of%2520a%2520Multi-Camera%2520System%2520with%2520Limited%2520Overlapping%250A%2520%2520Fields%2520of%2520View%2520for%25203D%2520Surgical%2520Scene%2520Reconstruction%26entry.906535625%3DTim%2520Fl%25C3%25BCckiger%2520and%2520Jonas%2520Hein%2520and%2520Valery%2520Fischer%2520and%2520Philipp%2520F%25C3%25BCrnstahl%2520and%2520Lilian%2520Calvet%26entry.1292438233%3D%2520%2520Purpose%253A%2520The%2520purpose%2520of%2520this%2520study%2520is%2520to%2520develop%2520an%2520automated%2520and%2520accurate%250Aexternal%2520camera%2520calibration%2520method%2520for%2520multi-camera%2520systems%2520used%2520in%25203D%2520surgical%250Ascene%2520reconstruction%2520%25283D-SSR%2529%252C%2520eliminating%2520the%2520need%2520for%2520operator%2520intervention%250Aor%2520specialized%2520expertise.%2520The%2520method%2520specifically%2520addresses%2520the%2520problem%2520of%250Alimited%2520overlapping%2520fields%2520of%2520view%2520caused%2520by%2520significant%2520variations%2520in%2520optical%250Azoom%2520levels%2520and%2520camera%2520locations.%250A%2520%2520Methods%253A%2520We%2520contribute%2520a%2520novel%252C%2520fast%252C%2520and%2520fully%2520automatic%2520calibration%2520method%250Abased%2520on%2520the%2520projection%2520of%2520multi-scale%2520markers%2520%2528MSMs%2529%2520using%2520a%2520ceiling-mounted%250Aprojector.%2520MSMs%2520consist%2520of%25202D%2520patterns%2520projected%2520at%2520varying%2520scales%252C%2520ensuring%250Aaccurate%2520extraction%2520of%2520well%2520distributed%2520point%2520correspondences%2520across%250Asignificantly%2520different%2520viewpoints%2520and%2520zoom%2520levels.%2520Validation%2520is%2520performed%250Ausing%2520both%2520synthetic%2520and%2520real%2520data%2520captured%2520in%2520a%2520mock-up%2520OR%252C%2520with%2520comparisons%250Ato%2520traditional%2520manual%2520marker-based%2520methods%2520as%2520well%2520as%2520markerless%2520calibration%250Amethods.%250A%2520%2520Results%253A%2520The%2520method%2520achieves%2520accuracy%2520comparable%2520to%2520manual%252C%250Aoperator-dependent%2520calibration%2520methods%2520while%2520exhibiting%2520higher%2520robustness%2520under%250Aconditions%2520of%2520significant%2520differences%2520in%2520zoom%2520levels.%2520Additionally%252C%2520we%2520show%250Athat%2520state-of-the-art%2520Structure-from-Motion%2520%2528SfM%2529%2520pipelines%2520are%2520ineffective%2520in%250A3D-SSR%2520settings%252C%2520even%2520when%2520additional%2520texture%2520is%2520projected%2520onto%2520the%2520OR%2520floor.%250A%2520%2520Conclusion%253A%2520The%2520use%2520of%2520a%2520ceiling-mounted%2520entry-level%2520projector%2520proves%2520to%2520be%250Aan%2520effective%2520alternative%2520to%2520operator-dependent%252C%2520traditional%2520marker-based%250Amethods%252C%2520paving%2520the%2520way%2520for%2520fully%2520automated%25203D-SSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Calibration%20of%20a%20Multi-Camera%20System%20with%20Limited%20Overlapping%0A%20%20Fields%20of%20View%20for%203D%20Surgical%20Scene%20Reconstruction&entry.906535625=Tim%20Fl%C3%BCckiger%20and%20Jonas%20Hein%20and%20Valery%20Fischer%20and%20Philipp%20F%C3%BCrnstahl%20and%20Lilian%20Calvet&entry.1292438233=%20%20Purpose%3A%20The%20purpose%20of%20this%20study%20is%20to%20develop%20an%20automated%20and%20accurate%0Aexternal%20camera%20calibration%20method%20for%20multi-camera%20systems%20used%20in%203D%20surgical%0Ascene%20reconstruction%20%283D-SSR%29%2C%20eliminating%20the%20need%20for%20operator%20intervention%0Aor%20specialized%20expertise.%20The%20method%20specifically%20addresses%20the%20problem%20of%0Alimited%20overlapping%20fields%20of%20view%20caused%20by%20significant%20variations%20in%20optical%0Azoom%20levels%20and%20camera%20locations.%0A%20%20Methods%3A%20We%20contribute%20a%20novel%2C%20fast%2C%20and%20fully%20automatic%20calibration%20method%0Abased%20on%20the%20projection%20of%20multi-scale%20markers%20%28MSMs%29%20using%20a%20ceiling-mounted%0Aprojector.%20MSMs%20consist%20of%202D%20patterns%20projected%20at%20varying%20scales%2C%20ensuring%0Aaccurate%20extraction%20of%20well%20distributed%20point%20correspondences%20across%0Asignificantly%20different%20viewpoints%20and%20zoom%20levels.%20Validation%20is%20performed%0Ausing%20both%20synthetic%20and%20real%20data%20captured%20in%20a%20mock-up%20OR%2C%20with%20comparisons%0Ato%20traditional%20manual%20marker-based%20methods%20as%20well%20as%20markerless%20calibration%0Amethods.%0A%20%20Results%3A%20The%20method%20achieves%20accuracy%20comparable%20to%20manual%2C%0Aoperator-dependent%20calibration%20methods%20while%20exhibiting%20higher%20robustness%20under%0Aconditions%20of%20significant%20differences%20in%20zoom%20levels.%20Additionally%2C%20we%20show%0Athat%20state-of-the-art%20Structure-from-Motion%20%28SfM%29%20pipelines%20are%20ineffective%20in%0A3D-SSR%20settings%2C%20even%20when%20additional%20texture%20is%20projected%20onto%20the%20OR%20floor.%0A%20%20Conclusion%3A%20The%20use%20of%20a%20ceiling-mounted%20entry-level%20projector%20proves%20to%20be%0Aan%20effective%20alternative%20to%20operator-dependent%2C%20traditional%20marker-based%0Amethods%2C%20paving%20the%20way%20for%20fully%20automated%203D-SSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16221v1&entry.124074799=Read"},
{"title": "Evaluating The Performance of Using Large Language Models to Automate\n  Summarization of CT Simulation Orders in Radiation Oncology", "author": "Meiyun Cao and Shaw Hu and Jason Sharp and Edward Clouser and Jason Holmes and Linda L. Lam and Xiaoning Ding and Diego Santos Toesca and Wendy S. Lindholm and Samir H. Patel and Sujay A. Vora and Peilong Wang and Wei Liu", "abstract": "  Purpose: This study aims to use a large language model (LLM) to automate the\ngeneration of summaries from the CT simulation orders and evaluate its\nperformance.\n  Materials and Methods: A total of 607 CT simulation orders for patients were\ncollected from the Aria database at our institution. A locally hosted Llama 3.1\n405B model, accessed via the Application Programming Interface (API) service,\nwas used to extract keywords from the CT simulation orders and generate\nsummaries. The downloaded CT simulation orders were categorized into seven\ngroups based on treatment modalities and disease sites. For each group, a\ncustomized instruction prompt was developed collaboratively with therapists to\nguide the Llama 3.1 405B model in generating summaries. The ground truth for\nthe corresponding summaries was manually derived by carefully reviewing each CT\nsimulation order and subsequently verified by therapists. The accuracy of the\nLLM-generated summaries was evaluated by therapists using the verified ground\ntruth as a reference.\n  Results: About 98% of the LLM-generated summaries aligned with the manually\ngenerated ground truth in terms of accuracy. Our evaluations showed an improved\nconsistency in format and enhanced readability of the LLM-generated summaries\ncompared to the corresponding therapists-generated summaries. This automated\napproach demonstrated a consistent performance across all groups, regardless of\nmodality or disease site.\n  Conclusions: This study demonstrated the high precision and consistency of\nthe Llama 3.1 405B model in extracting keywords and summarizing CT simulation\norders, suggesting that LLMs have great potential to help with this task,\nreduce the workload of therapists and improve workflow efficiency.\n", "link": "http://arxiv.org/abs/2501.16309v1", "date": "2025-01-27", "relevancy": 2.3758, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4786}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4786}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20The%20Performance%20of%20Using%20Large%20Language%20Models%20to%20Automate%0A%20%20Summarization%20of%20CT%20Simulation%20Orders%20in%20Radiation%20Oncology&body=Title%3A%20Evaluating%20The%20Performance%20of%20Using%20Large%20Language%20Models%20to%20Automate%0A%20%20Summarization%20of%20CT%20Simulation%20Orders%20in%20Radiation%20Oncology%0AAuthor%3A%20Meiyun%20Cao%20and%20Shaw%20Hu%20and%20Jason%20Sharp%20and%20Edward%20Clouser%20and%20Jason%20Holmes%20and%20Linda%20L.%20Lam%20and%20Xiaoning%20Ding%20and%20Diego%20Santos%20Toesca%20and%20Wendy%20S.%20Lindholm%20and%20Samir%20H.%20Patel%20and%20Sujay%20A.%20Vora%20and%20Peilong%20Wang%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Purpose%3A%20This%20study%20aims%20to%20use%20a%20large%20language%20model%20%28LLM%29%20to%20automate%20the%0Ageneration%20of%20summaries%20from%20the%20CT%20simulation%20orders%20and%20evaluate%20its%0Aperformance.%0A%20%20Materials%20and%20Methods%3A%20A%20total%20of%20607%20CT%20simulation%20orders%20for%20patients%20were%0Acollected%20from%20the%20Aria%20database%20at%20our%20institution.%20A%20locally%20hosted%20Llama%203.1%0A405B%20model%2C%20accessed%20via%20the%20Application%20Programming%20Interface%20%28API%29%20service%2C%0Awas%20used%20to%20extract%20keywords%20from%20the%20CT%20simulation%20orders%20and%20generate%0Asummaries.%20The%20downloaded%20CT%20simulation%20orders%20were%20categorized%20into%20seven%0Agroups%20based%20on%20treatment%20modalities%20and%20disease%20sites.%20For%20each%20group%2C%20a%0Acustomized%20instruction%20prompt%20was%20developed%20collaboratively%20with%20therapists%20to%0Aguide%20the%20Llama%203.1%20405B%20model%20in%20generating%20summaries.%20The%20ground%20truth%20for%0Athe%20corresponding%20summaries%20was%20manually%20derived%20by%20carefully%20reviewing%20each%20CT%0Asimulation%20order%20and%20subsequently%20verified%20by%20therapists.%20The%20accuracy%20of%20the%0ALLM-generated%20summaries%20was%20evaluated%20by%20therapists%20using%20the%20verified%20ground%0Atruth%20as%20a%20reference.%0A%20%20Results%3A%20About%2098%25%20of%20the%20LLM-generated%20summaries%20aligned%20with%20the%20manually%0Agenerated%20ground%20truth%20in%20terms%20of%20accuracy.%20Our%20evaluations%20showed%20an%20improved%0Aconsistency%20in%20format%20and%20enhanced%20readability%20of%20the%20LLM-generated%20summaries%0Acompared%20to%20the%20corresponding%20therapists-generated%20summaries.%20This%20automated%0Aapproach%20demonstrated%20a%20consistent%20performance%20across%20all%20groups%2C%20regardless%20of%0Amodality%20or%20disease%20site.%0A%20%20Conclusions%3A%20This%20study%20demonstrated%20the%20high%20precision%20and%20consistency%20of%0Athe%20Llama%203.1%20405B%20model%20in%20extracting%20keywords%20and%20summarizing%20CT%20simulation%0Aorders%2C%20suggesting%20that%20LLMs%20have%20great%20potential%20to%20help%20with%20this%20task%2C%0Areduce%20the%20workload%20of%20therapists%20and%20improve%20workflow%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520The%2520Performance%2520of%2520Using%2520Large%2520Language%2520Models%2520to%2520Automate%250A%2520%2520Summarization%2520of%2520CT%2520Simulation%2520Orders%2520in%2520Radiation%2520Oncology%26entry.906535625%3DMeiyun%2520Cao%2520and%2520Shaw%2520Hu%2520and%2520Jason%2520Sharp%2520and%2520Edward%2520Clouser%2520and%2520Jason%2520Holmes%2520and%2520Linda%2520L.%2520Lam%2520and%2520Xiaoning%2520Ding%2520and%2520Diego%2520Santos%2520Toesca%2520and%2520Wendy%2520S.%2520Lindholm%2520and%2520Samir%2520H.%2520Patel%2520and%2520Sujay%2520A.%2520Vora%2520and%2520Peilong%2520Wang%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520Purpose%253A%2520This%2520study%2520aims%2520to%2520use%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520automate%2520the%250Ageneration%2520of%2520summaries%2520from%2520the%2520CT%2520simulation%2520orders%2520and%2520evaluate%2520its%250Aperformance.%250A%2520%2520Materials%2520and%2520Methods%253A%2520A%2520total%2520of%2520607%2520CT%2520simulation%2520orders%2520for%2520patients%2520were%250Acollected%2520from%2520the%2520Aria%2520database%2520at%2520our%2520institution.%2520A%2520locally%2520hosted%2520Llama%25203.1%250A405B%2520model%252C%2520accessed%2520via%2520the%2520Application%2520Programming%2520Interface%2520%2528API%2529%2520service%252C%250Awas%2520used%2520to%2520extract%2520keywords%2520from%2520the%2520CT%2520simulation%2520orders%2520and%2520generate%250Asummaries.%2520The%2520downloaded%2520CT%2520simulation%2520orders%2520were%2520categorized%2520into%2520seven%250Agroups%2520based%2520on%2520treatment%2520modalities%2520and%2520disease%2520sites.%2520For%2520each%2520group%252C%2520a%250Acustomized%2520instruction%2520prompt%2520was%2520developed%2520collaboratively%2520with%2520therapists%2520to%250Aguide%2520the%2520Llama%25203.1%2520405B%2520model%2520in%2520generating%2520summaries.%2520The%2520ground%2520truth%2520for%250Athe%2520corresponding%2520summaries%2520was%2520manually%2520derived%2520by%2520carefully%2520reviewing%2520each%2520CT%250Asimulation%2520order%2520and%2520subsequently%2520verified%2520by%2520therapists.%2520The%2520accuracy%2520of%2520the%250ALLM-generated%2520summaries%2520was%2520evaluated%2520by%2520therapists%2520using%2520the%2520verified%2520ground%250Atruth%2520as%2520a%2520reference.%250A%2520%2520Results%253A%2520About%252098%2525%2520of%2520the%2520LLM-generated%2520summaries%2520aligned%2520with%2520the%2520manually%250Agenerated%2520ground%2520truth%2520in%2520terms%2520of%2520accuracy.%2520Our%2520evaluations%2520showed%2520an%2520improved%250Aconsistency%2520in%2520format%2520and%2520enhanced%2520readability%2520of%2520the%2520LLM-generated%2520summaries%250Acompared%2520to%2520the%2520corresponding%2520therapists-generated%2520summaries.%2520This%2520automated%250Aapproach%2520demonstrated%2520a%2520consistent%2520performance%2520across%2520all%2520groups%252C%2520regardless%2520of%250Amodality%2520or%2520disease%2520site.%250A%2520%2520Conclusions%253A%2520This%2520study%2520demonstrated%2520the%2520high%2520precision%2520and%2520consistency%2520of%250Athe%2520Llama%25203.1%2520405B%2520model%2520in%2520extracting%2520keywords%2520and%2520summarizing%2520CT%2520simulation%250Aorders%252C%2520suggesting%2520that%2520LLMs%2520have%2520great%2520potential%2520to%2520help%2520with%2520this%2520task%252C%250Areduce%2520the%2520workload%2520of%2520therapists%2520and%2520improve%2520workflow%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20The%20Performance%20of%20Using%20Large%20Language%20Models%20to%20Automate%0A%20%20Summarization%20of%20CT%20Simulation%20Orders%20in%20Radiation%20Oncology&entry.906535625=Meiyun%20Cao%20and%20Shaw%20Hu%20and%20Jason%20Sharp%20and%20Edward%20Clouser%20and%20Jason%20Holmes%20and%20Linda%20L.%20Lam%20and%20Xiaoning%20Ding%20and%20Diego%20Santos%20Toesca%20and%20Wendy%20S.%20Lindholm%20and%20Samir%20H.%20Patel%20and%20Sujay%20A.%20Vora%20and%20Peilong%20Wang%20and%20Wei%20Liu&entry.1292438233=%20%20Purpose%3A%20This%20study%20aims%20to%20use%20a%20large%20language%20model%20%28LLM%29%20to%20automate%20the%0Ageneration%20of%20summaries%20from%20the%20CT%20simulation%20orders%20and%20evaluate%20its%0Aperformance.%0A%20%20Materials%20and%20Methods%3A%20A%20total%20of%20607%20CT%20simulation%20orders%20for%20patients%20were%0Acollected%20from%20the%20Aria%20database%20at%20our%20institution.%20A%20locally%20hosted%20Llama%203.1%0A405B%20model%2C%20accessed%20via%20the%20Application%20Programming%20Interface%20%28API%29%20service%2C%0Awas%20used%20to%20extract%20keywords%20from%20the%20CT%20simulation%20orders%20and%20generate%0Asummaries.%20The%20downloaded%20CT%20simulation%20orders%20were%20categorized%20into%20seven%0Agroups%20based%20on%20treatment%20modalities%20and%20disease%20sites.%20For%20each%20group%2C%20a%0Acustomized%20instruction%20prompt%20was%20developed%20collaboratively%20with%20therapists%20to%0Aguide%20the%20Llama%203.1%20405B%20model%20in%20generating%20summaries.%20The%20ground%20truth%20for%0Athe%20corresponding%20summaries%20was%20manually%20derived%20by%20carefully%20reviewing%20each%20CT%0Asimulation%20order%20and%20subsequently%20verified%20by%20therapists.%20The%20accuracy%20of%20the%0ALLM-generated%20summaries%20was%20evaluated%20by%20therapists%20using%20the%20verified%20ground%0Atruth%20as%20a%20reference.%0A%20%20Results%3A%20About%2098%25%20of%20the%20LLM-generated%20summaries%20aligned%20with%20the%20manually%0Agenerated%20ground%20truth%20in%20terms%20of%20accuracy.%20Our%20evaluations%20showed%20an%20improved%0Aconsistency%20in%20format%20and%20enhanced%20readability%20of%20the%20LLM-generated%20summaries%0Acompared%20to%20the%20corresponding%20therapists-generated%20summaries.%20This%20automated%0Aapproach%20demonstrated%20a%20consistent%20performance%20across%20all%20groups%2C%20regardless%20of%0Amodality%20or%20disease%20site.%0A%20%20Conclusions%3A%20This%20study%20demonstrated%20the%20high%20precision%20and%20consistency%20of%0Athe%20Llama%203.1%20405B%20model%20in%20extracting%20keywords%20and%20summarizing%20CT%20simulation%0Aorders%2C%20suggesting%20that%20LLMs%20have%20great%20potential%20to%20help%20with%20this%20task%2C%0Areduce%20the%20workload%20of%20therapists%20and%20improve%20workflow%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16309v1&entry.124074799=Read"},
{"title": "Contrastive Representation Learning Helps Cross-institutional Knowledge\n  Transfer: A Study in Pediatric Ventilation Management", "author": "Yuxuan Liu and Jinpei Han and Padmanabhan Ramnarayan and A. Aldo Faisal", "abstract": "  Clinical machine learning deployment across institutions faces significant\nchallenges when patient populations and clinical practices differ\nsubstantially. We present a systematic framework for cross-institutional\nknowledge transfer in clinical time series, demonstrated through pediatric\nventilation management between a general pediatric intensive care unit (PICU)\nand a cardiac-focused unit. Using contrastive predictive coding (CPC) for\nrepresentation learning, we investigate how different data regimes and\nfine-tuning strategies affect knowledge transfer across institutional\nboundaries. Our results show that while direct model transfer performs poorly,\nCPC with appropriate fine-tuning enables effective knowledge sharing between\ninstitutions, with benefits particularly evident in limited data scenarios.\nAnalysis of transfer patterns reveals an important asymmetry: temporal\nprogression patterns transfer more readily than point-of-care decisions,\nsuggesting practical pathways for cross-institutional deployment. Through a\nsystematic evaluation of fine-tuning approaches and transfer patterns, our work\nprovides insights for developing more generalizable clinical decision support\nsystems while enabling smaller specialized units to leverage knowledge from\nlarger centers.\n", "link": "http://arxiv.org/abs/2501.13587v2", "date": "2025-01-27", "relevancy": 2.3671, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4699}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Representation%20Learning%20Helps%20Cross-institutional%20Knowledge%0A%20%20Transfer%3A%20A%20Study%20in%20Pediatric%20Ventilation%20Management&body=Title%3A%20Contrastive%20Representation%20Learning%20Helps%20Cross-institutional%20Knowledge%0A%20%20Transfer%3A%20A%20Study%20in%20Pediatric%20Ventilation%20Management%0AAuthor%3A%20Yuxuan%20Liu%20and%20Jinpei%20Han%20and%20Padmanabhan%20Ramnarayan%20and%20A.%20Aldo%20Faisal%0AAbstract%3A%20%20%20Clinical%20machine%20learning%20deployment%20across%20institutions%20faces%20significant%0Achallenges%20when%20patient%20populations%20and%20clinical%20practices%20differ%0Asubstantially.%20We%20present%20a%20systematic%20framework%20for%20cross-institutional%0Aknowledge%20transfer%20in%20clinical%20time%20series%2C%20demonstrated%20through%20pediatric%0Aventilation%20management%20between%20a%20general%20pediatric%20intensive%20care%20unit%20%28PICU%29%0Aand%20a%20cardiac-focused%20unit.%20Using%20contrastive%20predictive%20coding%20%28CPC%29%20for%0Arepresentation%20learning%2C%20we%20investigate%20how%20different%20data%20regimes%20and%0Afine-tuning%20strategies%20affect%20knowledge%20transfer%20across%20institutional%0Aboundaries.%20Our%20results%20show%20that%20while%20direct%20model%20transfer%20performs%20poorly%2C%0ACPC%20with%20appropriate%20fine-tuning%20enables%20effective%20knowledge%20sharing%20between%0Ainstitutions%2C%20with%20benefits%20particularly%20evident%20in%20limited%20data%20scenarios.%0AAnalysis%20of%20transfer%20patterns%20reveals%20an%20important%20asymmetry%3A%20temporal%0Aprogression%20patterns%20transfer%20more%20readily%20than%20point-of-care%20decisions%2C%0Asuggesting%20practical%20pathways%20for%20cross-institutional%20deployment.%20Through%20a%0Asystematic%20evaluation%20of%20fine-tuning%20approaches%20and%20transfer%20patterns%2C%20our%20work%0Aprovides%20insights%20for%20developing%20more%20generalizable%20clinical%20decision%20support%0Asystems%20while%20enabling%20smaller%20specialized%20units%20to%20leverage%20knowledge%20from%0Alarger%20centers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Representation%2520Learning%2520Helps%2520Cross-institutional%2520Knowledge%250A%2520%2520Transfer%253A%2520A%2520Study%2520in%2520Pediatric%2520Ventilation%2520Management%26entry.906535625%3DYuxuan%2520Liu%2520and%2520Jinpei%2520Han%2520and%2520Padmanabhan%2520Ramnarayan%2520and%2520A.%2520Aldo%2520Faisal%26entry.1292438233%3D%2520%2520Clinical%2520machine%2520learning%2520deployment%2520across%2520institutions%2520faces%2520significant%250Achallenges%2520when%2520patient%2520populations%2520and%2520clinical%2520practices%2520differ%250Asubstantially.%2520We%2520present%2520a%2520systematic%2520framework%2520for%2520cross-institutional%250Aknowledge%2520transfer%2520in%2520clinical%2520time%2520series%252C%2520demonstrated%2520through%2520pediatric%250Aventilation%2520management%2520between%2520a%2520general%2520pediatric%2520intensive%2520care%2520unit%2520%2528PICU%2529%250Aand%2520a%2520cardiac-focused%2520unit.%2520Using%2520contrastive%2520predictive%2520coding%2520%2528CPC%2529%2520for%250Arepresentation%2520learning%252C%2520we%2520investigate%2520how%2520different%2520data%2520regimes%2520and%250Afine-tuning%2520strategies%2520affect%2520knowledge%2520transfer%2520across%2520institutional%250Aboundaries.%2520Our%2520results%2520show%2520that%2520while%2520direct%2520model%2520transfer%2520performs%2520poorly%252C%250ACPC%2520with%2520appropriate%2520fine-tuning%2520enables%2520effective%2520knowledge%2520sharing%2520between%250Ainstitutions%252C%2520with%2520benefits%2520particularly%2520evident%2520in%2520limited%2520data%2520scenarios.%250AAnalysis%2520of%2520transfer%2520patterns%2520reveals%2520an%2520important%2520asymmetry%253A%2520temporal%250Aprogression%2520patterns%2520transfer%2520more%2520readily%2520than%2520point-of-care%2520decisions%252C%250Asuggesting%2520practical%2520pathways%2520for%2520cross-institutional%2520deployment.%2520Through%2520a%250Asystematic%2520evaluation%2520of%2520fine-tuning%2520approaches%2520and%2520transfer%2520patterns%252C%2520our%2520work%250Aprovides%2520insights%2520for%2520developing%2520more%2520generalizable%2520clinical%2520decision%2520support%250Asystems%2520while%2520enabling%2520smaller%2520specialized%2520units%2520to%2520leverage%2520knowledge%2520from%250Alarger%2520centers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Representation%20Learning%20Helps%20Cross-institutional%20Knowledge%0A%20%20Transfer%3A%20A%20Study%20in%20Pediatric%20Ventilation%20Management&entry.906535625=Yuxuan%20Liu%20and%20Jinpei%20Han%20and%20Padmanabhan%20Ramnarayan%20and%20A.%20Aldo%20Faisal&entry.1292438233=%20%20Clinical%20machine%20learning%20deployment%20across%20institutions%20faces%20significant%0Achallenges%20when%20patient%20populations%20and%20clinical%20practices%20differ%0Asubstantially.%20We%20present%20a%20systematic%20framework%20for%20cross-institutional%0Aknowledge%20transfer%20in%20clinical%20time%20series%2C%20demonstrated%20through%20pediatric%0Aventilation%20management%20between%20a%20general%20pediatric%20intensive%20care%20unit%20%28PICU%29%0Aand%20a%20cardiac-focused%20unit.%20Using%20contrastive%20predictive%20coding%20%28CPC%29%20for%0Arepresentation%20learning%2C%20we%20investigate%20how%20different%20data%20regimes%20and%0Afine-tuning%20strategies%20affect%20knowledge%20transfer%20across%20institutional%0Aboundaries.%20Our%20results%20show%20that%20while%20direct%20model%20transfer%20performs%20poorly%2C%0ACPC%20with%20appropriate%20fine-tuning%20enables%20effective%20knowledge%20sharing%20between%0Ainstitutions%2C%20with%20benefits%20particularly%20evident%20in%20limited%20data%20scenarios.%0AAnalysis%20of%20transfer%20patterns%20reveals%20an%20important%20asymmetry%3A%20temporal%0Aprogression%20patterns%20transfer%20more%20readily%20than%20point-of-care%20decisions%2C%0Asuggesting%20practical%20pathways%20for%20cross-institutional%20deployment.%20Through%20a%0Asystematic%20evaluation%20of%20fine-tuning%20approaches%20and%20transfer%20patterns%2C%20our%20work%0Aprovides%20insights%20for%20developing%20more%20generalizable%20clinical%20decision%20support%0Asystems%20while%20enabling%20smaller%20specialized%20units%20to%20leverage%20knowledge%20from%0Alarger%20centers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13587v2&entry.124074799=Read"},
{"title": "Benchmarking Vision Foundation Models for Input Monitoring in Autonomous\n  Driving", "author": "Mert Keser and Halil Ibrahim Orhan and Niki Amini-Naieni and Gesina Schwalbe and Alois Knoll and Matthias Rottmann", "abstract": "  Deep neural networks (DNNs) remain challenged by distribution shifts in\ncomplex open-world domains like automated driving (AD): Absolute robustness\nagainst yet unknown novel objects (semantic shift) or styles like lighting\nconditions (covariate shift) cannot be guaranteed. Hence, reliable\noperation-time monitors for identification of out-of-training-data-distribution\n(OOD) scenarios are imperative. Current approaches for OOD classification are\nuntested for complex domains like AD, are limited in the kinds of shifts they\ndetect, or even require supervision with OOD samples. To prepare for\nunanticipated shifts, we instead establish a framework around a principled,\nunsupervised, and model-agnostic method that unifies detection of all kinds of\nshifts: Find a full model of the training data's feature distribution, to then\nuse its density at new points as in-distribution (ID) score. To implement this,\nwe propose to combine the newly available Vision Foundation Models (VFM) as\nfeature extractors with one of four alternative density modeling techniques. In\nan extensive benchmark of 4 VFMs against 20 baselines, we show the superior\nperformance of VFM feature encodings compared to shift-specific OOD monitors.\nAdditionally, we find that sophisticated architectures outperform larger latent\nspace dimensionality; and our method identifies samples with higher risk of\nerrors on downstream tasks, despite being model-agnostic. This suggests that\nVFMs are promising to realize model-agnostic, unsupervised, reliable safety\nmonitors in complex vision tasks.\n", "link": "http://arxiv.org/abs/2501.08083v2", "date": "2025-01-27", "relevancy": 2.3333, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Vision%20Foundation%20Models%20for%20Input%20Monitoring%20in%20Autonomous%0A%20%20Driving&body=Title%3A%20Benchmarking%20Vision%20Foundation%20Models%20for%20Input%20Monitoring%20in%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Mert%20Keser%20and%20Halil%20Ibrahim%20Orhan%20and%20Niki%20Amini-Naieni%20and%20Gesina%20Schwalbe%20and%20Alois%20Knoll%20and%20Matthias%20Rottmann%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20remain%20challenged%20by%20distribution%20shifts%20in%0Acomplex%20open-world%20domains%20like%20automated%20driving%20%28AD%29%3A%20Absolute%20robustness%0Aagainst%20yet%20unknown%20novel%20objects%20%28semantic%20shift%29%20or%20styles%20like%20lighting%0Aconditions%20%28covariate%20shift%29%20cannot%20be%20guaranteed.%20Hence%2C%20reliable%0Aoperation-time%20monitors%20for%20identification%20of%20out-of-training-data-distribution%0A%28OOD%29%20scenarios%20are%20imperative.%20Current%20approaches%20for%20OOD%20classification%20are%0Auntested%20for%20complex%20domains%20like%20AD%2C%20are%20limited%20in%20the%20kinds%20of%20shifts%20they%0Adetect%2C%20or%20even%20require%20supervision%20with%20OOD%20samples.%20To%20prepare%20for%0Aunanticipated%20shifts%2C%20we%20instead%20establish%20a%20framework%20around%20a%20principled%2C%0Aunsupervised%2C%20and%20model-agnostic%20method%20that%20unifies%20detection%20of%20all%20kinds%20of%0Ashifts%3A%20Find%20a%20full%20model%20of%20the%20training%20data%27s%20feature%20distribution%2C%20to%20then%0Ause%20its%20density%20at%20new%20points%20as%20in-distribution%20%28ID%29%20score.%20To%20implement%20this%2C%0Awe%20propose%20to%20combine%20the%20newly%20available%20Vision%20Foundation%20Models%20%28VFM%29%20as%0Afeature%20extractors%20with%20one%20of%20four%20alternative%20density%20modeling%20techniques.%20In%0Aan%20extensive%20benchmark%20of%204%20VFMs%20against%2020%20baselines%2C%20we%20show%20the%20superior%0Aperformance%20of%20VFM%20feature%20encodings%20compared%20to%20shift-specific%20OOD%20monitors.%0AAdditionally%2C%20we%20find%20that%20sophisticated%20architectures%20outperform%20larger%20latent%0Aspace%20dimensionality%3B%20and%20our%20method%20identifies%20samples%20with%20higher%20risk%20of%0Aerrors%20on%20downstream%20tasks%2C%20despite%20being%20model-agnostic.%20This%20suggests%20that%0AVFMs%20are%20promising%20to%20realize%20model-agnostic%2C%20unsupervised%2C%20reliable%20safety%0Amonitors%20in%20complex%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Vision%2520Foundation%2520Models%2520for%2520Input%2520Monitoring%2520in%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DMert%2520Keser%2520and%2520Halil%2520Ibrahim%2520Orhan%2520and%2520Niki%2520Amini-Naieni%2520and%2520Gesina%2520Schwalbe%2520and%2520Alois%2520Knoll%2520and%2520Matthias%2520Rottmann%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520remain%2520challenged%2520by%2520distribution%2520shifts%2520in%250Acomplex%2520open-world%2520domains%2520like%2520automated%2520driving%2520%2528AD%2529%253A%2520Absolute%2520robustness%250Aagainst%2520yet%2520unknown%2520novel%2520objects%2520%2528semantic%2520shift%2529%2520or%2520styles%2520like%2520lighting%250Aconditions%2520%2528covariate%2520shift%2529%2520cannot%2520be%2520guaranteed.%2520Hence%252C%2520reliable%250Aoperation-time%2520monitors%2520for%2520identification%2520of%2520out-of-training-data-distribution%250A%2528OOD%2529%2520scenarios%2520are%2520imperative.%2520Current%2520approaches%2520for%2520OOD%2520classification%2520are%250Auntested%2520for%2520complex%2520domains%2520like%2520AD%252C%2520are%2520limited%2520in%2520the%2520kinds%2520of%2520shifts%2520they%250Adetect%252C%2520or%2520even%2520require%2520supervision%2520with%2520OOD%2520samples.%2520To%2520prepare%2520for%250Aunanticipated%2520shifts%252C%2520we%2520instead%2520establish%2520a%2520framework%2520around%2520a%2520principled%252C%250Aunsupervised%252C%2520and%2520model-agnostic%2520method%2520that%2520unifies%2520detection%2520of%2520all%2520kinds%2520of%250Ashifts%253A%2520Find%2520a%2520full%2520model%2520of%2520the%2520training%2520data%2527s%2520feature%2520distribution%252C%2520to%2520then%250Ause%2520its%2520density%2520at%2520new%2520points%2520as%2520in-distribution%2520%2528ID%2529%2520score.%2520To%2520implement%2520this%252C%250Awe%2520propose%2520to%2520combine%2520the%2520newly%2520available%2520Vision%2520Foundation%2520Models%2520%2528VFM%2529%2520as%250Afeature%2520extractors%2520with%2520one%2520of%2520four%2520alternative%2520density%2520modeling%2520techniques.%2520In%250Aan%2520extensive%2520benchmark%2520of%25204%2520VFMs%2520against%252020%2520baselines%252C%2520we%2520show%2520the%2520superior%250Aperformance%2520of%2520VFM%2520feature%2520encodings%2520compared%2520to%2520shift-specific%2520OOD%2520monitors.%250AAdditionally%252C%2520we%2520find%2520that%2520sophisticated%2520architectures%2520outperform%2520larger%2520latent%250Aspace%2520dimensionality%253B%2520and%2520our%2520method%2520identifies%2520samples%2520with%2520higher%2520risk%2520of%250Aerrors%2520on%2520downstream%2520tasks%252C%2520despite%2520being%2520model-agnostic.%2520This%2520suggests%2520that%250AVFMs%2520are%2520promising%2520to%2520realize%2520model-agnostic%252C%2520unsupervised%252C%2520reliable%2520safety%250Amonitors%2520in%2520complex%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Vision%20Foundation%20Models%20for%20Input%20Monitoring%20in%20Autonomous%0A%20%20Driving&entry.906535625=Mert%20Keser%20and%20Halil%20Ibrahim%20Orhan%20and%20Niki%20Amini-Naieni%20and%20Gesina%20Schwalbe%20and%20Alois%20Knoll%20and%20Matthias%20Rottmann&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20remain%20challenged%20by%20distribution%20shifts%20in%0Acomplex%20open-world%20domains%20like%20automated%20driving%20%28AD%29%3A%20Absolute%20robustness%0Aagainst%20yet%20unknown%20novel%20objects%20%28semantic%20shift%29%20or%20styles%20like%20lighting%0Aconditions%20%28covariate%20shift%29%20cannot%20be%20guaranteed.%20Hence%2C%20reliable%0Aoperation-time%20monitors%20for%20identification%20of%20out-of-training-data-distribution%0A%28OOD%29%20scenarios%20are%20imperative.%20Current%20approaches%20for%20OOD%20classification%20are%0Auntested%20for%20complex%20domains%20like%20AD%2C%20are%20limited%20in%20the%20kinds%20of%20shifts%20they%0Adetect%2C%20or%20even%20require%20supervision%20with%20OOD%20samples.%20To%20prepare%20for%0Aunanticipated%20shifts%2C%20we%20instead%20establish%20a%20framework%20around%20a%20principled%2C%0Aunsupervised%2C%20and%20model-agnostic%20method%20that%20unifies%20detection%20of%20all%20kinds%20of%0Ashifts%3A%20Find%20a%20full%20model%20of%20the%20training%20data%27s%20feature%20distribution%2C%20to%20then%0Ause%20its%20density%20at%20new%20points%20as%20in-distribution%20%28ID%29%20score.%20To%20implement%20this%2C%0Awe%20propose%20to%20combine%20the%20newly%20available%20Vision%20Foundation%20Models%20%28VFM%29%20as%0Afeature%20extractors%20with%20one%20of%20four%20alternative%20density%20modeling%20techniques.%20In%0Aan%20extensive%20benchmark%20of%204%20VFMs%20against%2020%20baselines%2C%20we%20show%20the%20superior%0Aperformance%20of%20VFM%20feature%20encodings%20compared%20to%20shift-specific%20OOD%20monitors.%0AAdditionally%2C%20we%20find%20that%20sophisticated%20architectures%20outperform%20larger%20latent%0Aspace%20dimensionality%3B%20and%20our%20method%20identifies%20samples%20with%20higher%20risk%20of%0Aerrors%20on%20downstream%20tasks%2C%20despite%20being%20model-agnostic.%20This%20suggests%20that%0AVFMs%20are%20promising%20to%20realize%20model-agnostic%2C%20unsupervised%2C%20reliable%20safety%0Amonitors%20in%20complex%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08083v2&entry.124074799=Read"},
{"title": "ReFill: Reinforcement Learning for Fill-In Minimization", "author": "Elfarouk Harb and Ho Shan Lam", "abstract": "  Efficiently solving sparse linear systems $Ax=b$, where $A$ is a large,\nsparse, symmetric positive semi-definite matrix, is a core challenge in\nscientific computing, machine learning, and optimization. A major bottleneck in\nGaussian elimination for these systems is fill-in, the creation of non-zero\nentries that increase memory and computational cost. Minimizing fill-in is\nNP-hard, and existing heuristics like Minimum Degree and Nested Dissection\noffer limited adaptability across diverse problem instances.\n  We introduce \\textit{ReFill}, a reinforcement learning framework enhanced by\nGraph Neural Networks (GNNs) to learn adaptive ordering strategies for fill-in\nminimization. ReFill trains a GNN-based heuristic to predict efficient\nelimination orders, outperforming traditional heuristics by dynamically\nadapting to the structure of input matrices. Experiments demonstrate that\nReFill outperforms strong heuristics in reducing fill-in, highlighting the\nuntapped potential of learning-based methods for this well-studied classical\nproblem.\n", "link": "http://arxiv.org/abs/2501.16130v1", "date": "2025-01-27", "relevancy": 2.3262, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4891}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4596}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReFill%3A%20Reinforcement%20Learning%20for%20Fill-In%20Minimization&body=Title%3A%20ReFill%3A%20Reinforcement%20Learning%20for%20Fill-In%20Minimization%0AAuthor%3A%20Elfarouk%20Harb%20and%20Ho%20Shan%20Lam%0AAbstract%3A%20%20%20Efficiently%20solving%20sparse%20linear%20systems%20%24Ax%3Db%24%2C%20where%20%24A%24%20is%20a%20large%2C%0Asparse%2C%20symmetric%20positive%20semi-definite%20matrix%2C%20is%20a%20core%20challenge%20in%0Ascientific%20computing%2C%20machine%20learning%2C%20and%20optimization.%20A%20major%20bottleneck%20in%0AGaussian%20elimination%20for%20these%20systems%20is%20fill-in%2C%20the%20creation%20of%20non-zero%0Aentries%20that%20increase%20memory%20and%20computational%20cost.%20Minimizing%20fill-in%20is%0ANP-hard%2C%20and%20existing%20heuristics%20like%20Minimum%20Degree%20and%20Nested%20Dissection%0Aoffer%20limited%20adaptability%20across%20diverse%20problem%20instances.%0A%20%20We%20introduce%20%5Ctextit%7BReFill%7D%2C%20a%20reinforcement%20learning%20framework%20enhanced%20by%0AGraph%20Neural%20Networks%20%28GNNs%29%20to%20learn%20adaptive%20ordering%20strategies%20for%20fill-in%0Aminimization.%20ReFill%20trains%20a%20GNN-based%20heuristic%20to%20predict%20efficient%0Aelimination%20orders%2C%20outperforming%20traditional%20heuristics%20by%20dynamically%0Aadapting%20to%20the%20structure%20of%20input%20matrices.%20Experiments%20demonstrate%20that%0AReFill%20outperforms%20strong%20heuristics%20in%20reducing%20fill-in%2C%20highlighting%20the%0Auntapped%20potential%20of%20learning-based%20methods%20for%20this%20well-studied%20classical%0Aproblem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReFill%253A%2520Reinforcement%2520Learning%2520for%2520Fill-In%2520Minimization%26entry.906535625%3DElfarouk%2520Harb%2520and%2520Ho%2520Shan%2520Lam%26entry.1292438233%3D%2520%2520Efficiently%2520solving%2520sparse%2520linear%2520systems%2520%2524Ax%253Db%2524%252C%2520where%2520%2524A%2524%2520is%2520a%2520large%252C%250Asparse%252C%2520symmetric%2520positive%2520semi-definite%2520matrix%252C%2520is%2520a%2520core%2520challenge%2520in%250Ascientific%2520computing%252C%2520machine%2520learning%252C%2520and%2520optimization.%2520A%2520major%2520bottleneck%2520in%250AGaussian%2520elimination%2520for%2520these%2520systems%2520is%2520fill-in%252C%2520the%2520creation%2520of%2520non-zero%250Aentries%2520that%2520increase%2520memory%2520and%2520computational%2520cost.%2520Minimizing%2520fill-in%2520is%250ANP-hard%252C%2520and%2520existing%2520heuristics%2520like%2520Minimum%2520Degree%2520and%2520Nested%2520Dissection%250Aoffer%2520limited%2520adaptability%2520across%2520diverse%2520problem%2520instances.%250A%2520%2520We%2520introduce%2520%255Ctextit%257BReFill%257D%252C%2520a%2520reinforcement%2520learning%2520framework%2520enhanced%2520by%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520learn%2520adaptive%2520ordering%2520strategies%2520for%2520fill-in%250Aminimization.%2520ReFill%2520trains%2520a%2520GNN-based%2520heuristic%2520to%2520predict%2520efficient%250Aelimination%2520orders%252C%2520outperforming%2520traditional%2520heuristics%2520by%2520dynamically%250Aadapting%2520to%2520the%2520structure%2520of%2520input%2520matrices.%2520Experiments%2520demonstrate%2520that%250AReFill%2520outperforms%2520strong%2520heuristics%2520in%2520reducing%2520fill-in%252C%2520highlighting%2520the%250Auntapped%2520potential%2520of%2520learning-based%2520methods%2520for%2520this%2520well-studied%2520classical%250Aproblem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReFill%3A%20Reinforcement%20Learning%20for%20Fill-In%20Minimization&entry.906535625=Elfarouk%20Harb%20and%20Ho%20Shan%20Lam&entry.1292438233=%20%20Efficiently%20solving%20sparse%20linear%20systems%20%24Ax%3Db%24%2C%20where%20%24A%24%20is%20a%20large%2C%0Asparse%2C%20symmetric%20positive%20semi-definite%20matrix%2C%20is%20a%20core%20challenge%20in%0Ascientific%20computing%2C%20machine%20learning%2C%20and%20optimization.%20A%20major%20bottleneck%20in%0AGaussian%20elimination%20for%20these%20systems%20is%20fill-in%2C%20the%20creation%20of%20non-zero%0Aentries%20that%20increase%20memory%20and%20computational%20cost.%20Minimizing%20fill-in%20is%0ANP-hard%2C%20and%20existing%20heuristics%20like%20Minimum%20Degree%20and%20Nested%20Dissection%0Aoffer%20limited%20adaptability%20across%20diverse%20problem%20instances.%0A%20%20We%20introduce%20%5Ctextit%7BReFill%7D%2C%20a%20reinforcement%20learning%20framework%20enhanced%20by%0AGraph%20Neural%20Networks%20%28GNNs%29%20to%20learn%20adaptive%20ordering%20strategies%20for%20fill-in%0Aminimization.%20ReFill%20trains%20a%20GNN-based%20heuristic%20to%20predict%20efficient%0Aelimination%20orders%2C%20outperforming%20traditional%20heuristics%20by%20dynamically%0Aadapting%20to%20the%20structure%20of%20input%20matrices.%20Experiments%20demonstrate%20that%0AReFill%20outperforms%20strong%20heuristics%20in%20reducing%20fill-in%2C%20highlighting%20the%0Auntapped%20potential%20of%20learning-based%20methods%20for%20this%20well-studied%20classical%0Aproblem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16130v1&entry.124074799=Read"},
{"title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs", "author": "Mohamed Elfeki and Rui Liu and Chad Voegele", "abstract": "  The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount.\n", "link": "http://arxiv.org/abs/2501.16273v1", "date": "2025-01-27", "relevancy": 2.3191, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Return%20of%20the%20Encoder%3A%20Maximizing%20Parameter%20Efficiency%20for%20SLMs&body=Title%3A%20Return%20of%20the%20Encoder%3A%20Maximizing%20Parameter%20Efficiency%20for%20SLMs%0AAuthor%3A%20Mohamed%20Elfeki%20and%20Rui%20Liu%20and%20Chad%20Voegele%0AAbstract%3A%20%20%20The%20dominance%20of%20large%20decoder-only%20language%20models%20has%20overshadowed%0Aencoder-decoder%20architectures%2C%20despite%20their%20fundamental%20efficiency%20advantages%0Ain%20sequence%20processing.%20For%20small%20language%20models%20%28SLMs%29%20-%20those%20with%201%20billion%0Aparameters%20or%20fewer%20-%20our%20systematic%20analysis%20across%20GPU%2C%20CPU%2C%20and%20NPU%0Aplatforms%20reveals%20that%20encoder-decoder%20architectures%20achieve%2047%25%20lower%0Afirst-token%20latency%20and%204.7x%20higher%20throughput%20compared%20to%20decoder-only%20models%0Aon%20edge%20devices.%20These%20gains%20may%20be%20attributed%20to%20encoder-decoder%27s%20one-time%0Ainput%20processing%20and%20efficient%20separation%20of%20understanding%20and%20generation%0Aphases.%0A%20%20We%20introduce%20a%20novel%20knowledge%20distillation%20framework%20that%20enables%0Aencoder-decoder%20models%20to%20leverage%20capabilities%20from%20large%20scalable%0Adecoder-only%20teachers%20while%20preserving%20their%20architectural%20advantages%2C%0Aachieving%20up%20to%206%20average%20performance%20points%20improvement%20across%20diverse%20tasks%2C%0Awith%20significant%20gains%20in%20asymmetric%20sequence%20tasks%20where%20input%20and%20output%0Adistributions%20can%20benefit%20from%20different%20processing%20approaches.%0A%20%20When%20combined%20with%20modern%20advances%20like%20Rotary%20Positional%20Embeddings%20%28RoPE%29%0Aand%20Vision%20encoders%2C%20our%20systematic%20investigation%20demonstrates%20that%0Aencoder-decoder%20architectures%20provide%20a%20more%20practical%20path%20toward%20deploying%0Acapable%20language%20models%20in%20resource-constrained%20environments.%20Our%20findings%0Achallenge%20the%20prevailing%20trend%20toward%20decoder-only%20scaling%2C%20showing%20that%0Aarchitectural%20choices%20become%20increasingly%20crucial%20as%20parameter%20budgets%0Adecrease%2C%20particularly%20for%20on-device%20and%20edge%20deployments%20where%20computational%0Aefficiency%20is%20paramount.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReturn%2520of%2520the%2520Encoder%253A%2520Maximizing%2520Parameter%2520Efficiency%2520for%2520SLMs%26entry.906535625%3DMohamed%2520Elfeki%2520and%2520Rui%2520Liu%2520and%2520Chad%2520Voegele%26entry.1292438233%3D%2520%2520The%2520dominance%2520of%2520large%2520decoder-only%2520language%2520models%2520has%2520overshadowed%250Aencoder-decoder%2520architectures%252C%2520despite%2520their%2520fundamental%2520efficiency%2520advantages%250Ain%2520sequence%2520processing.%2520For%2520small%2520language%2520models%2520%2528SLMs%2529%2520-%2520those%2520with%25201%2520billion%250Aparameters%2520or%2520fewer%2520-%2520our%2520systematic%2520analysis%2520across%2520GPU%252C%2520CPU%252C%2520and%2520NPU%250Aplatforms%2520reveals%2520that%2520encoder-decoder%2520architectures%2520achieve%252047%2525%2520lower%250Afirst-token%2520latency%2520and%25204.7x%2520higher%2520throughput%2520compared%2520to%2520decoder-only%2520models%250Aon%2520edge%2520devices.%2520These%2520gains%2520may%2520be%2520attributed%2520to%2520encoder-decoder%2527s%2520one-time%250Ainput%2520processing%2520and%2520efficient%2520separation%2520of%2520understanding%2520and%2520generation%250Aphases.%250A%2520%2520We%2520introduce%2520a%2520novel%2520knowledge%2520distillation%2520framework%2520that%2520enables%250Aencoder-decoder%2520models%2520to%2520leverage%2520capabilities%2520from%2520large%2520scalable%250Adecoder-only%2520teachers%2520while%2520preserving%2520their%2520architectural%2520advantages%252C%250Aachieving%2520up%2520to%25206%2520average%2520performance%2520points%2520improvement%2520across%2520diverse%2520tasks%252C%250Awith%2520significant%2520gains%2520in%2520asymmetric%2520sequence%2520tasks%2520where%2520input%2520and%2520output%250Adistributions%2520can%2520benefit%2520from%2520different%2520processing%2520approaches.%250A%2520%2520When%2520combined%2520with%2520modern%2520advances%2520like%2520Rotary%2520Positional%2520Embeddings%2520%2528RoPE%2529%250Aand%2520Vision%2520encoders%252C%2520our%2520systematic%2520investigation%2520demonstrates%2520that%250Aencoder-decoder%2520architectures%2520provide%2520a%2520more%2520practical%2520path%2520toward%2520deploying%250Acapable%2520language%2520models%2520in%2520resource-constrained%2520environments.%2520Our%2520findings%250Achallenge%2520the%2520prevailing%2520trend%2520toward%2520decoder-only%2520scaling%252C%2520showing%2520that%250Aarchitectural%2520choices%2520become%2520increasingly%2520crucial%2520as%2520parameter%2520budgets%250Adecrease%252C%2520particularly%2520for%2520on-device%2520and%2520edge%2520deployments%2520where%2520computational%250Aefficiency%2520is%2520paramount.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Return%20of%20the%20Encoder%3A%20Maximizing%20Parameter%20Efficiency%20for%20SLMs&entry.906535625=Mohamed%20Elfeki%20and%20Rui%20Liu%20and%20Chad%20Voegele&entry.1292438233=%20%20The%20dominance%20of%20large%20decoder-only%20language%20models%20has%20overshadowed%0Aencoder-decoder%20architectures%2C%20despite%20their%20fundamental%20efficiency%20advantages%0Ain%20sequence%20processing.%20For%20small%20language%20models%20%28SLMs%29%20-%20those%20with%201%20billion%0Aparameters%20or%20fewer%20-%20our%20systematic%20analysis%20across%20GPU%2C%20CPU%2C%20and%20NPU%0Aplatforms%20reveals%20that%20encoder-decoder%20architectures%20achieve%2047%25%20lower%0Afirst-token%20latency%20and%204.7x%20higher%20throughput%20compared%20to%20decoder-only%20models%0Aon%20edge%20devices.%20These%20gains%20may%20be%20attributed%20to%20encoder-decoder%27s%20one-time%0Ainput%20processing%20and%20efficient%20separation%20of%20understanding%20and%20generation%0Aphases.%0A%20%20We%20introduce%20a%20novel%20knowledge%20distillation%20framework%20that%20enables%0Aencoder-decoder%20models%20to%20leverage%20capabilities%20from%20large%20scalable%0Adecoder-only%20teachers%20while%20preserving%20their%20architectural%20advantages%2C%0Aachieving%20up%20to%206%20average%20performance%20points%20improvement%20across%20diverse%20tasks%2C%0Awith%20significant%20gains%20in%20asymmetric%20sequence%20tasks%20where%20input%20and%20output%0Adistributions%20can%20benefit%20from%20different%20processing%20approaches.%0A%20%20When%20combined%20with%20modern%20advances%20like%20Rotary%20Positional%20Embeddings%20%28RoPE%29%0Aand%20Vision%20encoders%2C%20our%20systematic%20investigation%20demonstrates%20that%0Aencoder-decoder%20architectures%20provide%20a%20more%20practical%20path%20toward%20deploying%0Acapable%20language%20models%20in%20resource-constrained%20environments.%20Our%20findings%0Achallenge%20the%20prevailing%20trend%20toward%20decoder-only%20scaling%2C%20showing%20that%0Aarchitectural%20choices%20become%20increasingly%20crucial%20as%20parameter%20budgets%0Adecrease%2C%20particularly%20for%20on-device%20and%20edge%20deployments%20where%20computational%0Aefficiency%20is%20paramount.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16273v1&entry.124074799=Read"},
{"title": "MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models", "author": "Michael Birsak and John Femiani and Biao Zhang and Peter Wonka", "abstract": "  Assigning realistic materials to 3D models remains a significant challenge in\ncomputer graphics. We propose MatCLIP, a novel method that extracts shape- and\nlighting-insensitive descriptors of Physically Based Rendering (PBR) materials\nto assign plausible textures to 3D objects based on images, such as the output\nof Latent Diffusion Models (LDMs) or photographs. Matching PBR materials to\nstatic images is challenging because the PBR representation captures the\ndynamic appearance of materials under varying viewing angles, shapes, and\nlighting conditions. By extending an Alpha-CLIP-based model on material\nrenderings across diverse shapes and lighting, and encoding multiple viewing\nconditions for PBR materials, our approach generates descriptors that bridge\nthe domains of PBR representations with photographs or renderings, including\nLDM outputs. This enables consistent material assignments without requiring\nexplicit knowledge of material relationships between different parts of an\nobject. MatCLIP achieves a top-1 classification accuracy of 76.6%,\noutperforming state-of-the-art methods such as PhotoShape and MatAtlas by over\n15 percentage points on publicly available datasets. Our method can be used to\nconstruct material assignments for 3D shape datasets such as ShapeNet,\n3DCoMPaT++, and Objaverse. All code and data will be released.\n", "link": "http://arxiv.org/abs/2501.15981v1", "date": "2025-01-27", "relevancy": 2.3093, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6081}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.566}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatCLIP%3A%20Light-%20and%20Shape-Insensitive%20Assignment%20of%20PBR%20Material%20Models&body=Title%3A%20MatCLIP%3A%20Light-%20and%20Shape-Insensitive%20Assignment%20of%20PBR%20Material%20Models%0AAuthor%3A%20Michael%20Birsak%20and%20John%20Femiani%20and%20Biao%20Zhang%20and%20Peter%20Wonka%0AAbstract%3A%20%20%20Assigning%20realistic%20materials%20to%203D%20models%20remains%20a%20significant%20challenge%20in%0Acomputer%20graphics.%20We%20propose%20MatCLIP%2C%20a%20novel%20method%20that%20extracts%20shape-%20and%0Alighting-insensitive%20descriptors%20of%20Physically%20Based%20Rendering%20%28PBR%29%20materials%0Ato%20assign%20plausible%20textures%20to%203D%20objects%20based%20on%20images%2C%20such%20as%20the%20output%0Aof%20Latent%20Diffusion%20Models%20%28LDMs%29%20or%20photographs.%20Matching%20PBR%20materials%20to%0Astatic%20images%20is%20challenging%20because%20the%20PBR%20representation%20captures%20the%0Adynamic%20appearance%20of%20materials%20under%20varying%20viewing%20angles%2C%20shapes%2C%20and%0Alighting%20conditions.%20By%20extending%20an%20Alpha-CLIP-based%20model%20on%20material%0Arenderings%20across%20diverse%20shapes%20and%20lighting%2C%20and%20encoding%20multiple%20viewing%0Aconditions%20for%20PBR%20materials%2C%20our%20approach%20generates%20descriptors%20that%20bridge%0Athe%20domains%20of%20PBR%20representations%20with%20photographs%20or%20renderings%2C%20including%0ALDM%20outputs.%20This%20enables%20consistent%20material%20assignments%20without%20requiring%0Aexplicit%20knowledge%20of%20material%20relationships%20between%20different%20parts%20of%20an%0Aobject.%20MatCLIP%20achieves%20a%20top-1%20classification%20accuracy%20of%2076.6%25%2C%0Aoutperforming%20state-of-the-art%20methods%20such%20as%20PhotoShape%20and%20MatAtlas%20by%20over%0A15%20percentage%20points%20on%20publicly%20available%20datasets.%20Our%20method%20can%20be%20used%20to%0Aconstruct%20material%20assignments%20for%203D%20shape%20datasets%20such%20as%20ShapeNet%2C%0A3DCoMPaT%2B%2B%2C%20and%20Objaverse.%20All%20code%20and%20data%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatCLIP%253A%2520Light-%2520and%2520Shape-Insensitive%2520Assignment%2520of%2520PBR%2520Material%2520Models%26entry.906535625%3DMichael%2520Birsak%2520and%2520John%2520Femiani%2520and%2520Biao%2520Zhang%2520and%2520Peter%2520Wonka%26entry.1292438233%3D%2520%2520Assigning%2520realistic%2520materials%2520to%25203D%2520models%2520remains%2520a%2520significant%2520challenge%2520in%250Acomputer%2520graphics.%2520We%2520propose%2520MatCLIP%252C%2520a%2520novel%2520method%2520that%2520extracts%2520shape-%2520and%250Alighting-insensitive%2520descriptors%2520of%2520Physically%2520Based%2520Rendering%2520%2528PBR%2529%2520materials%250Ato%2520assign%2520plausible%2520textures%2520to%25203D%2520objects%2520based%2520on%2520images%252C%2520such%2520as%2520the%2520output%250Aof%2520Latent%2520Diffusion%2520Models%2520%2528LDMs%2529%2520or%2520photographs.%2520Matching%2520PBR%2520materials%2520to%250Astatic%2520images%2520is%2520challenging%2520because%2520the%2520PBR%2520representation%2520captures%2520the%250Adynamic%2520appearance%2520of%2520materials%2520under%2520varying%2520viewing%2520angles%252C%2520shapes%252C%2520and%250Alighting%2520conditions.%2520By%2520extending%2520an%2520Alpha-CLIP-based%2520model%2520on%2520material%250Arenderings%2520across%2520diverse%2520shapes%2520and%2520lighting%252C%2520and%2520encoding%2520multiple%2520viewing%250Aconditions%2520for%2520PBR%2520materials%252C%2520our%2520approach%2520generates%2520descriptors%2520that%2520bridge%250Athe%2520domains%2520of%2520PBR%2520representations%2520with%2520photographs%2520or%2520renderings%252C%2520including%250ALDM%2520outputs.%2520This%2520enables%2520consistent%2520material%2520assignments%2520without%2520requiring%250Aexplicit%2520knowledge%2520of%2520material%2520relationships%2520between%2520different%2520parts%2520of%2520an%250Aobject.%2520MatCLIP%2520achieves%2520a%2520top-1%2520classification%2520accuracy%2520of%252076.6%2525%252C%250Aoutperforming%2520state-of-the-art%2520methods%2520such%2520as%2520PhotoShape%2520and%2520MatAtlas%2520by%2520over%250A15%2520percentage%2520points%2520on%2520publicly%2520available%2520datasets.%2520Our%2520method%2520can%2520be%2520used%2520to%250Aconstruct%2520material%2520assignments%2520for%25203D%2520shape%2520datasets%2520such%2520as%2520ShapeNet%252C%250A3DCoMPaT%252B%252B%252C%2520and%2520Objaverse.%2520All%2520code%2520and%2520data%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatCLIP%3A%20Light-%20and%20Shape-Insensitive%20Assignment%20of%20PBR%20Material%20Models&entry.906535625=Michael%20Birsak%20and%20John%20Femiani%20and%20Biao%20Zhang%20and%20Peter%20Wonka&entry.1292438233=%20%20Assigning%20realistic%20materials%20to%203D%20models%20remains%20a%20significant%20challenge%20in%0Acomputer%20graphics.%20We%20propose%20MatCLIP%2C%20a%20novel%20method%20that%20extracts%20shape-%20and%0Alighting-insensitive%20descriptors%20of%20Physically%20Based%20Rendering%20%28PBR%29%20materials%0Ato%20assign%20plausible%20textures%20to%203D%20objects%20based%20on%20images%2C%20such%20as%20the%20output%0Aof%20Latent%20Diffusion%20Models%20%28LDMs%29%20or%20photographs.%20Matching%20PBR%20materials%20to%0Astatic%20images%20is%20challenging%20because%20the%20PBR%20representation%20captures%20the%0Adynamic%20appearance%20of%20materials%20under%20varying%20viewing%20angles%2C%20shapes%2C%20and%0Alighting%20conditions.%20By%20extending%20an%20Alpha-CLIP-based%20model%20on%20material%0Arenderings%20across%20diverse%20shapes%20and%20lighting%2C%20and%20encoding%20multiple%20viewing%0Aconditions%20for%20PBR%20materials%2C%20our%20approach%20generates%20descriptors%20that%20bridge%0Athe%20domains%20of%20PBR%20representations%20with%20photographs%20or%20renderings%2C%20including%0ALDM%20outputs.%20This%20enables%20consistent%20material%20assignments%20without%20requiring%0Aexplicit%20knowledge%20of%20material%20relationships%20between%20different%20parts%20of%20an%0Aobject.%20MatCLIP%20achieves%20a%20top-1%20classification%20accuracy%20of%2076.6%25%2C%0Aoutperforming%20state-of-the-art%20methods%20such%20as%20PhotoShape%20and%20MatAtlas%20by%20over%0A15%20percentage%20points%20on%20publicly%20available%20datasets.%20Our%20method%20can%20be%20used%20to%0Aconstruct%20material%20assignments%20for%203D%20shape%20datasets%20such%20as%20ShapeNet%2C%0A3DCoMPaT%2B%2B%2C%20and%20Objaverse.%20All%20code%20and%20data%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15981v1&entry.124074799=Read"},
{"title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment\n  Benchmarking", "author": "Benjamin Feuer and Micah Goldblum and Teresa Datta and Sanjana Nambiar and Raz Besaleli and Samuel Dooley and Max Cembalest and John P. Dickerson", "abstract": "  The release of ChatGPT in November 2022 sparked an explosion of interest in\npost-training and an avalanche of new preference optimization (PO) methods.\nThese methods claim superior alignment by virtue of better correspondence with\nhuman pairwise preferences, often measured by LLM-judges. In this work, we\nattempt to answer the following question -- do LLM-judge preferences translate\nto progress on other, more concrete metrics for alignment, and if not, why not?\nWe define a concrete metric for alignment, and introduce SOS-Bench (Substance\nOutweighs Style Benchmark), which is to the best of our knowledge the largest\nstandardized, reproducible LLM meta-benchmark to date. We find that (1)\nLLM-judge preferences do not correlate with concrete measures of safety, world\nknowledge, and instruction following; (2) LLM-judges have powerful implicit\nbiases, prioritizing style over factuality and safety; and (3) the supervised\nfine-tuning (SFT) stage of post-training, and not the PO stage, has the\ngreatest impact on alignment, with data scaling and prompt diversity as the\ndriving factors. Our codebase and complete results can be found at\nhttps://github.com/penfever/sos-bench.\n", "link": "http://arxiv.org/abs/2409.15268v3", "date": "2025-01-27", "relevancy": 2.3047, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4643}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Style%20Outweighs%20Substance%3A%20Failure%20Modes%20of%20LLM%20Judges%20in%20Alignment%0A%20%20Benchmarking&body=Title%3A%20Style%20Outweighs%20Substance%3A%20Failure%20Modes%20of%20LLM%20Judges%20in%20Alignment%0A%20%20Benchmarking%0AAuthor%3A%20Benjamin%20Feuer%20and%20Micah%20Goldblum%20and%20Teresa%20Datta%20and%20Sanjana%20Nambiar%20and%20Raz%20Besaleli%20and%20Samuel%20Dooley%20and%20Max%20Cembalest%20and%20John%20P.%20Dickerson%0AAbstract%3A%20%20%20The%20release%20of%20ChatGPT%20in%20November%202022%20sparked%20an%20explosion%20of%20interest%20in%0Apost-training%20and%20an%20avalanche%20of%20new%20preference%20optimization%20%28PO%29%20methods.%0AThese%20methods%20claim%20superior%20alignment%20by%20virtue%20of%20better%20correspondence%20with%0Ahuman%20pairwise%20preferences%2C%20often%20measured%20by%20LLM-judges.%20In%20this%20work%2C%20we%0Aattempt%20to%20answer%20the%20following%20question%20--%20do%20LLM-judge%20preferences%20translate%0Ato%20progress%20on%20other%2C%20more%20concrete%20metrics%20for%20alignment%2C%20and%20if%20not%2C%20why%20not%3F%0AWe%20define%20a%20concrete%20metric%20for%20alignment%2C%20and%20introduce%20SOS-Bench%20%28Substance%0AOutweighs%20Style%20Benchmark%29%2C%20which%20is%20to%20the%20best%20of%20our%20knowledge%20the%20largest%0Astandardized%2C%20reproducible%20LLM%20meta-benchmark%20to%20date.%20We%20find%20that%20%281%29%0ALLM-judge%20preferences%20do%20not%20correlate%20with%20concrete%20measures%20of%20safety%2C%20world%0Aknowledge%2C%20and%20instruction%20following%3B%20%282%29%20LLM-judges%20have%20powerful%20implicit%0Abiases%2C%20prioritizing%20style%20over%20factuality%20and%20safety%3B%20and%20%283%29%20the%20supervised%0Afine-tuning%20%28SFT%29%20stage%20of%20post-training%2C%20and%20not%20the%20PO%20stage%2C%20has%20the%0Agreatest%20impact%20on%20alignment%2C%20with%20data%20scaling%20and%20prompt%20diversity%20as%20the%0Adriving%20factors.%20Our%20codebase%20and%20complete%20results%20can%20be%20found%20at%0Ahttps%3A//github.com/penfever/sos-bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15268v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyle%2520Outweighs%2520Substance%253A%2520Failure%2520Modes%2520of%2520LLM%2520Judges%2520in%2520Alignment%250A%2520%2520Benchmarking%26entry.906535625%3DBenjamin%2520Feuer%2520and%2520Micah%2520Goldblum%2520and%2520Teresa%2520Datta%2520and%2520Sanjana%2520Nambiar%2520and%2520Raz%2520Besaleli%2520and%2520Samuel%2520Dooley%2520and%2520Max%2520Cembalest%2520and%2520John%2520P.%2520Dickerson%26entry.1292438233%3D%2520%2520The%2520release%2520of%2520ChatGPT%2520in%2520November%25202022%2520sparked%2520an%2520explosion%2520of%2520interest%2520in%250Apost-training%2520and%2520an%2520avalanche%2520of%2520new%2520preference%2520optimization%2520%2528PO%2529%2520methods.%250AThese%2520methods%2520claim%2520superior%2520alignment%2520by%2520virtue%2520of%2520better%2520correspondence%2520with%250Ahuman%2520pairwise%2520preferences%252C%2520often%2520measured%2520by%2520LLM-judges.%2520In%2520this%2520work%252C%2520we%250Aattempt%2520to%2520answer%2520the%2520following%2520question%2520--%2520do%2520LLM-judge%2520preferences%2520translate%250Ato%2520progress%2520on%2520other%252C%2520more%2520concrete%2520metrics%2520for%2520alignment%252C%2520and%2520if%2520not%252C%2520why%2520not%253F%250AWe%2520define%2520a%2520concrete%2520metric%2520for%2520alignment%252C%2520and%2520introduce%2520SOS-Bench%2520%2528Substance%250AOutweighs%2520Style%2520Benchmark%2529%252C%2520which%2520is%2520to%2520the%2520best%2520of%2520our%2520knowledge%2520the%2520largest%250Astandardized%252C%2520reproducible%2520LLM%2520meta-benchmark%2520to%2520date.%2520We%2520find%2520that%2520%25281%2529%250ALLM-judge%2520preferences%2520do%2520not%2520correlate%2520with%2520concrete%2520measures%2520of%2520safety%252C%2520world%250Aknowledge%252C%2520and%2520instruction%2520following%253B%2520%25282%2529%2520LLM-judges%2520have%2520powerful%2520implicit%250Abiases%252C%2520prioritizing%2520style%2520over%2520factuality%2520and%2520safety%253B%2520and%2520%25283%2529%2520the%2520supervised%250Afine-tuning%2520%2528SFT%2529%2520stage%2520of%2520post-training%252C%2520and%2520not%2520the%2520PO%2520stage%252C%2520has%2520the%250Agreatest%2520impact%2520on%2520alignment%252C%2520with%2520data%2520scaling%2520and%2520prompt%2520diversity%2520as%2520the%250Adriving%2520factors.%2520Our%2520codebase%2520and%2520complete%2520results%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/penfever/sos-bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15268v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Style%20Outweighs%20Substance%3A%20Failure%20Modes%20of%20LLM%20Judges%20in%20Alignment%0A%20%20Benchmarking&entry.906535625=Benjamin%20Feuer%20and%20Micah%20Goldblum%20and%20Teresa%20Datta%20and%20Sanjana%20Nambiar%20and%20Raz%20Besaleli%20and%20Samuel%20Dooley%20and%20Max%20Cembalest%20and%20John%20P.%20Dickerson&entry.1292438233=%20%20The%20release%20of%20ChatGPT%20in%20November%202022%20sparked%20an%20explosion%20of%20interest%20in%0Apost-training%20and%20an%20avalanche%20of%20new%20preference%20optimization%20%28PO%29%20methods.%0AThese%20methods%20claim%20superior%20alignment%20by%20virtue%20of%20better%20correspondence%20with%0Ahuman%20pairwise%20preferences%2C%20often%20measured%20by%20LLM-judges.%20In%20this%20work%2C%20we%0Aattempt%20to%20answer%20the%20following%20question%20--%20do%20LLM-judge%20preferences%20translate%0Ato%20progress%20on%20other%2C%20more%20concrete%20metrics%20for%20alignment%2C%20and%20if%20not%2C%20why%20not%3F%0AWe%20define%20a%20concrete%20metric%20for%20alignment%2C%20and%20introduce%20SOS-Bench%20%28Substance%0AOutweighs%20Style%20Benchmark%29%2C%20which%20is%20to%20the%20best%20of%20our%20knowledge%20the%20largest%0Astandardized%2C%20reproducible%20LLM%20meta-benchmark%20to%20date.%20We%20find%20that%20%281%29%0ALLM-judge%20preferences%20do%20not%20correlate%20with%20concrete%20measures%20of%20safety%2C%20world%0Aknowledge%2C%20and%20instruction%20following%3B%20%282%29%20LLM-judges%20have%20powerful%20implicit%0Abiases%2C%20prioritizing%20style%20over%20factuality%20and%20safety%3B%20and%20%283%29%20the%20supervised%0Afine-tuning%20%28SFT%29%20stage%20of%20post-training%2C%20and%20not%20the%20PO%20stage%2C%20has%20the%0Agreatest%20impact%20on%20alignment%2C%20with%20data%20scaling%20and%20prompt%20diversity%20as%20the%0Adriving%20factors.%20Our%20codebase%20and%20complete%20results%20can%20be%20found%20at%0Ahttps%3A//github.com/penfever/sos-bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15268v3&entry.124074799=Read"},
{"title": "Beyond the Alphabet: Deep Signal Embedding for Enhanced DNA Clustering", "author": "Hadas Abraham and Barak Gahtan and Adir Kobovich and Orian Leitersdorf and Alex M. Bronstein and Eitan Yaakobi", "abstract": "  The emerging field of DNA storage employs strands of DNA bases (A/T/C/G) as a\nstorage medium for digital information to enable massive density and\ndurability. The DNA storage pipeline includes: (1) encoding the raw data into\nsequences of DNA bases; (2) synthesizing the sequences as DNA \\textit{strands}\nthat are stored over time as an unordered set; (3) sequencing the DNA strands\nto generate DNA \\textit{reads}; and (4) deducing the original data. The DNA\nsynthesis and sequencing stages each generate several independent error-prone\nduplicates of each strand which are then utilized in the final stage to\nreconstruct the best estimate for the original strand. Specifically, the reads\nare first \\textit{clustered} into groups likely originating from the same\nstrand (based on their similarity to each other), and then each group\napproximates the strand that led to the reads of that group. This work improves\nthe DNA clustering stage by embedding it as part of the DNA sequencing.\nTraditional DNA storage solutions begin after the DNA sequencing process\ngenerates discrete DNA reads (A/T/C/G), yet we identify that there is untapped\npotential in using the raw signals generated by the Nanopore DNA sequencing\nmachine before they are discretized into bases, a process known as\n\\textit{basecalling}, which is done using a deep neural network. We propose a\ndeep neural network that clusters these signals directly, demonstrating\nsuperior accuracy, and reduced computation times compared to current approaches\nthat cluster after basecalling.\n", "link": "http://arxiv.org/abs/2410.06188v2", "date": "2025-01-27", "relevancy": 2.3038, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4631}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Alphabet%3A%20Deep%20Signal%20Embedding%20for%20Enhanced%20DNA%20Clustering&body=Title%3A%20Beyond%20the%20Alphabet%3A%20Deep%20Signal%20Embedding%20for%20Enhanced%20DNA%20Clustering%0AAuthor%3A%20Hadas%20Abraham%20and%20Barak%20Gahtan%20and%20Adir%20Kobovich%20and%20Orian%20Leitersdorf%20and%20Alex%20M.%20Bronstein%20and%20Eitan%20Yaakobi%0AAbstract%3A%20%20%20The%20emerging%20field%20of%20DNA%20storage%20employs%20strands%20of%20DNA%20bases%20%28A/T/C/G%29%20as%20a%0Astorage%20medium%20for%20digital%20information%20to%20enable%20massive%20density%20and%0Adurability.%20The%20DNA%20storage%20pipeline%20includes%3A%20%281%29%20encoding%20the%20raw%20data%20into%0Asequences%20of%20DNA%20bases%3B%20%282%29%20synthesizing%20the%20sequences%20as%20DNA%20%5Ctextit%7Bstrands%7D%0Athat%20are%20stored%20over%20time%20as%20an%20unordered%20set%3B%20%283%29%20sequencing%20the%20DNA%20strands%0Ato%20generate%20DNA%20%5Ctextit%7Breads%7D%3B%20and%20%284%29%20deducing%20the%20original%20data.%20The%20DNA%0Asynthesis%20and%20sequencing%20stages%20each%20generate%20several%20independent%20error-prone%0Aduplicates%20of%20each%20strand%20which%20are%20then%20utilized%20in%20the%20final%20stage%20to%0Areconstruct%20the%20best%20estimate%20for%20the%20original%20strand.%20Specifically%2C%20the%20reads%0Aare%20first%20%5Ctextit%7Bclustered%7D%20into%20groups%20likely%20originating%20from%20the%20same%0Astrand%20%28based%20on%20their%20similarity%20to%20each%20other%29%2C%20and%20then%20each%20group%0Aapproximates%20the%20strand%20that%20led%20to%20the%20reads%20of%20that%20group.%20This%20work%20improves%0Athe%20DNA%20clustering%20stage%20by%20embedding%20it%20as%20part%20of%20the%20DNA%20sequencing.%0ATraditional%20DNA%20storage%20solutions%20begin%20after%20the%20DNA%20sequencing%20process%0Agenerates%20discrete%20DNA%20reads%20%28A/T/C/G%29%2C%20yet%20we%20identify%20that%20there%20is%20untapped%0Apotential%20in%20using%20the%20raw%20signals%20generated%20by%20the%20Nanopore%20DNA%20sequencing%0Amachine%20before%20they%20are%20discretized%20into%20bases%2C%20a%20process%20known%20as%0A%5Ctextit%7Bbasecalling%7D%2C%20which%20is%20done%20using%20a%20deep%20neural%20network.%20We%20propose%20a%0Adeep%20neural%20network%20that%20clusters%20these%20signals%20directly%2C%20demonstrating%0Asuperior%20accuracy%2C%20and%20reduced%20computation%20times%20compared%20to%20current%20approaches%0Athat%20cluster%20after%20basecalling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Alphabet%253A%2520Deep%2520Signal%2520Embedding%2520for%2520Enhanced%2520DNA%2520Clustering%26entry.906535625%3DHadas%2520Abraham%2520and%2520Barak%2520Gahtan%2520and%2520Adir%2520Kobovich%2520and%2520Orian%2520Leitersdorf%2520and%2520Alex%2520M.%2520Bronstein%2520and%2520Eitan%2520Yaakobi%26entry.1292438233%3D%2520%2520The%2520emerging%2520field%2520of%2520DNA%2520storage%2520employs%2520strands%2520of%2520DNA%2520bases%2520%2528A/T/C/G%2529%2520as%2520a%250Astorage%2520medium%2520for%2520digital%2520information%2520to%2520enable%2520massive%2520density%2520and%250Adurability.%2520The%2520DNA%2520storage%2520pipeline%2520includes%253A%2520%25281%2529%2520encoding%2520the%2520raw%2520data%2520into%250Asequences%2520of%2520DNA%2520bases%253B%2520%25282%2529%2520synthesizing%2520the%2520sequences%2520as%2520DNA%2520%255Ctextit%257Bstrands%257D%250Athat%2520are%2520stored%2520over%2520time%2520as%2520an%2520unordered%2520set%253B%2520%25283%2529%2520sequencing%2520the%2520DNA%2520strands%250Ato%2520generate%2520DNA%2520%255Ctextit%257Breads%257D%253B%2520and%2520%25284%2529%2520deducing%2520the%2520original%2520data.%2520The%2520DNA%250Asynthesis%2520and%2520sequencing%2520stages%2520each%2520generate%2520several%2520independent%2520error-prone%250Aduplicates%2520of%2520each%2520strand%2520which%2520are%2520then%2520utilized%2520in%2520the%2520final%2520stage%2520to%250Areconstruct%2520the%2520best%2520estimate%2520for%2520the%2520original%2520strand.%2520Specifically%252C%2520the%2520reads%250Aare%2520first%2520%255Ctextit%257Bclustered%257D%2520into%2520groups%2520likely%2520originating%2520from%2520the%2520same%250Astrand%2520%2528based%2520on%2520their%2520similarity%2520to%2520each%2520other%2529%252C%2520and%2520then%2520each%2520group%250Aapproximates%2520the%2520strand%2520that%2520led%2520to%2520the%2520reads%2520of%2520that%2520group.%2520This%2520work%2520improves%250Athe%2520DNA%2520clustering%2520stage%2520by%2520embedding%2520it%2520as%2520part%2520of%2520the%2520DNA%2520sequencing.%250ATraditional%2520DNA%2520storage%2520solutions%2520begin%2520after%2520the%2520DNA%2520sequencing%2520process%250Agenerates%2520discrete%2520DNA%2520reads%2520%2528A/T/C/G%2529%252C%2520yet%2520we%2520identify%2520that%2520there%2520is%2520untapped%250Apotential%2520in%2520using%2520the%2520raw%2520signals%2520generated%2520by%2520the%2520Nanopore%2520DNA%2520sequencing%250Amachine%2520before%2520they%2520are%2520discretized%2520into%2520bases%252C%2520a%2520process%2520known%2520as%250A%255Ctextit%257Bbasecalling%257D%252C%2520which%2520is%2520done%2520using%2520a%2520deep%2520neural%2520network.%2520We%2520propose%2520a%250Adeep%2520neural%2520network%2520that%2520clusters%2520these%2520signals%2520directly%252C%2520demonstrating%250Asuperior%2520accuracy%252C%2520and%2520reduced%2520computation%2520times%2520compared%2520to%2520current%2520approaches%250Athat%2520cluster%2520after%2520basecalling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Alphabet%3A%20Deep%20Signal%20Embedding%20for%20Enhanced%20DNA%20Clustering&entry.906535625=Hadas%20Abraham%20and%20Barak%20Gahtan%20and%20Adir%20Kobovich%20and%20Orian%20Leitersdorf%20and%20Alex%20M.%20Bronstein%20and%20Eitan%20Yaakobi&entry.1292438233=%20%20The%20emerging%20field%20of%20DNA%20storage%20employs%20strands%20of%20DNA%20bases%20%28A/T/C/G%29%20as%20a%0Astorage%20medium%20for%20digital%20information%20to%20enable%20massive%20density%20and%0Adurability.%20The%20DNA%20storage%20pipeline%20includes%3A%20%281%29%20encoding%20the%20raw%20data%20into%0Asequences%20of%20DNA%20bases%3B%20%282%29%20synthesizing%20the%20sequences%20as%20DNA%20%5Ctextit%7Bstrands%7D%0Athat%20are%20stored%20over%20time%20as%20an%20unordered%20set%3B%20%283%29%20sequencing%20the%20DNA%20strands%0Ato%20generate%20DNA%20%5Ctextit%7Breads%7D%3B%20and%20%284%29%20deducing%20the%20original%20data.%20The%20DNA%0Asynthesis%20and%20sequencing%20stages%20each%20generate%20several%20independent%20error-prone%0Aduplicates%20of%20each%20strand%20which%20are%20then%20utilized%20in%20the%20final%20stage%20to%0Areconstruct%20the%20best%20estimate%20for%20the%20original%20strand.%20Specifically%2C%20the%20reads%0Aare%20first%20%5Ctextit%7Bclustered%7D%20into%20groups%20likely%20originating%20from%20the%20same%0Astrand%20%28based%20on%20their%20similarity%20to%20each%20other%29%2C%20and%20then%20each%20group%0Aapproximates%20the%20strand%20that%20led%20to%20the%20reads%20of%20that%20group.%20This%20work%20improves%0Athe%20DNA%20clustering%20stage%20by%20embedding%20it%20as%20part%20of%20the%20DNA%20sequencing.%0ATraditional%20DNA%20storage%20solutions%20begin%20after%20the%20DNA%20sequencing%20process%0Agenerates%20discrete%20DNA%20reads%20%28A/T/C/G%29%2C%20yet%20we%20identify%20that%20there%20is%20untapped%0Apotential%20in%20using%20the%20raw%20signals%20generated%20by%20the%20Nanopore%20DNA%20sequencing%0Amachine%20before%20they%20are%20discretized%20into%20bases%2C%20a%20process%20known%20as%0A%5Ctextit%7Bbasecalling%7D%2C%20which%20is%20done%20using%20a%20deep%20neural%20network.%20We%20propose%20a%0Adeep%20neural%20network%20that%20clusters%20these%20signals%20directly%2C%20demonstrating%0Asuperior%20accuracy%2C%20and%20reduced%20computation%20times%20compared%20to%20current%20approaches%0Athat%20cluster%20after%20basecalling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06188v2&entry.124074799=Read"},
{"title": "Towards Physically Interpretable World Models: Meaningful Weakly\n  Supervised Representations for Visual Trajectory Prediction", "author": "Zhenjiang Mao and Ivan Ruchkin", "abstract": "  Deep learning models are increasingly employed for perception, prediction,\nand control in complex systems. Embedding physical knowledge into these models\nis crucial for achieving realistic and consistent outputs, a challenge often\naddressed by physics-informed machine learning. However, integrating physical\nknowledge with representation learning becomes difficult when dealing with\nhigh-dimensional observation data, such as images, particularly under\nconditions of incomplete or imprecise state information. To address this, we\npropose Physically Interpretable World Models, a novel architecture that aligns\nlearned latent representations with real-world physical quantities. Our method\ncombines a variational autoencoder with a dynamical model that incorporates\nunknown system parameters, enabling the discovery of physically meaningful\nrepresentations. By employing weak supervision with interval-based constraints,\nour approach eliminates the reliance on ground-truth physical annotations.\nExperimental results demonstrate that our method improves the quality of\nlearned representations while achieving accurate predictions of future states,\nadvancing the field of representation learning in dynamic systems.\n", "link": "http://arxiv.org/abs/2412.12870v2", "date": "2025-01-27", "relevancy": 2.3027, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6174}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5776}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Physically%20Interpretable%20World%20Models%3A%20Meaningful%20Weakly%0A%20%20Supervised%20Representations%20for%20Visual%20Trajectory%20Prediction&body=Title%3A%20Towards%20Physically%20Interpretable%20World%20Models%3A%20Meaningful%20Weakly%0A%20%20Supervised%20Representations%20for%20Visual%20Trajectory%20Prediction%0AAuthor%3A%20Zhenjiang%20Mao%20and%20Ivan%20Ruchkin%0AAbstract%3A%20%20%20Deep%20learning%20models%20are%20increasingly%20employed%20for%20perception%2C%20prediction%2C%0Aand%20control%20in%20complex%20systems.%20Embedding%20physical%20knowledge%20into%20these%20models%0Ais%20crucial%20for%20achieving%20realistic%20and%20consistent%20outputs%2C%20a%20challenge%20often%0Aaddressed%20by%20physics-informed%20machine%20learning.%20However%2C%20integrating%20physical%0Aknowledge%20with%20representation%20learning%20becomes%20difficult%20when%20dealing%20with%0Ahigh-dimensional%20observation%20data%2C%20such%20as%20images%2C%20particularly%20under%0Aconditions%20of%20incomplete%20or%20imprecise%20state%20information.%20To%20address%20this%2C%20we%0Apropose%20Physically%20Interpretable%20World%20Models%2C%20a%20novel%20architecture%20that%20aligns%0Alearned%20latent%20representations%20with%20real-world%20physical%20quantities.%20Our%20method%0Acombines%20a%20variational%20autoencoder%20with%20a%20dynamical%20model%20that%20incorporates%0Aunknown%20system%20parameters%2C%20enabling%20the%20discovery%20of%20physically%20meaningful%0Arepresentations.%20By%20employing%20weak%20supervision%20with%20interval-based%20constraints%2C%0Aour%20approach%20eliminates%20the%20reliance%20on%20ground-truth%20physical%20annotations.%0AExperimental%20results%20demonstrate%20that%20our%20method%20improves%20the%20quality%20of%0Alearned%20representations%20while%20achieving%20accurate%20predictions%20of%20future%20states%2C%0Aadvancing%20the%20field%20of%20representation%20learning%20in%20dynamic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Physically%2520Interpretable%2520World%2520Models%253A%2520Meaningful%2520Weakly%250A%2520%2520Supervised%2520Representations%2520for%2520Visual%2520Trajectory%2520Prediction%26entry.906535625%3DZhenjiang%2520Mao%2520and%2520Ivan%2520Ruchkin%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520are%2520increasingly%2520employed%2520for%2520perception%252C%2520prediction%252C%250Aand%2520control%2520in%2520complex%2520systems.%2520Embedding%2520physical%2520knowledge%2520into%2520these%2520models%250Ais%2520crucial%2520for%2520achieving%2520realistic%2520and%2520consistent%2520outputs%252C%2520a%2520challenge%2520often%250Aaddressed%2520by%2520physics-informed%2520machine%2520learning.%2520However%252C%2520integrating%2520physical%250Aknowledge%2520with%2520representation%2520learning%2520becomes%2520difficult%2520when%2520dealing%2520with%250Ahigh-dimensional%2520observation%2520data%252C%2520such%2520as%2520images%252C%2520particularly%2520under%250Aconditions%2520of%2520incomplete%2520or%2520imprecise%2520state%2520information.%2520To%2520address%2520this%252C%2520we%250Apropose%2520Physically%2520Interpretable%2520World%2520Models%252C%2520a%2520novel%2520architecture%2520that%2520aligns%250Alearned%2520latent%2520representations%2520with%2520real-world%2520physical%2520quantities.%2520Our%2520method%250Acombines%2520a%2520variational%2520autoencoder%2520with%2520a%2520dynamical%2520model%2520that%2520incorporates%250Aunknown%2520system%2520parameters%252C%2520enabling%2520the%2520discovery%2520of%2520physically%2520meaningful%250Arepresentations.%2520By%2520employing%2520weak%2520supervision%2520with%2520interval-based%2520constraints%252C%250Aour%2520approach%2520eliminates%2520the%2520reliance%2520on%2520ground-truth%2520physical%2520annotations.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520improves%2520the%2520quality%2520of%250Alearned%2520representations%2520while%2520achieving%2520accurate%2520predictions%2520of%2520future%2520states%252C%250Aadvancing%2520the%2520field%2520of%2520representation%2520learning%2520in%2520dynamic%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Physically%20Interpretable%20World%20Models%3A%20Meaningful%20Weakly%0A%20%20Supervised%20Representations%20for%20Visual%20Trajectory%20Prediction&entry.906535625=Zhenjiang%20Mao%20and%20Ivan%20Ruchkin&entry.1292438233=%20%20Deep%20learning%20models%20are%20increasingly%20employed%20for%20perception%2C%20prediction%2C%0Aand%20control%20in%20complex%20systems.%20Embedding%20physical%20knowledge%20into%20these%20models%0Ais%20crucial%20for%20achieving%20realistic%20and%20consistent%20outputs%2C%20a%20challenge%20often%0Aaddressed%20by%20physics-informed%20machine%20learning.%20However%2C%20integrating%20physical%0Aknowledge%20with%20representation%20learning%20becomes%20difficult%20when%20dealing%20with%0Ahigh-dimensional%20observation%20data%2C%20such%20as%20images%2C%20particularly%20under%0Aconditions%20of%20incomplete%20or%20imprecise%20state%20information.%20To%20address%20this%2C%20we%0Apropose%20Physically%20Interpretable%20World%20Models%2C%20a%20novel%20architecture%20that%20aligns%0Alearned%20latent%20representations%20with%20real-world%20physical%20quantities.%20Our%20method%0Acombines%20a%20variational%20autoencoder%20with%20a%20dynamical%20model%20that%20incorporates%0Aunknown%20system%20parameters%2C%20enabling%20the%20discovery%20of%20physically%20meaningful%0Arepresentations.%20By%20employing%20weak%20supervision%20with%20interval-based%20constraints%2C%0Aour%20approach%20eliminates%20the%20reliance%20on%20ground-truth%20physical%20annotations.%0AExperimental%20results%20demonstrate%20that%20our%20method%20improves%20the%20quality%20of%0Alearned%20representations%20while%20achieving%20accurate%20predictions%20of%20future%20states%2C%0Aadvancing%20the%20field%20of%20representation%20learning%20in%20dynamic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12870v2&entry.124074799=Read"},
{"title": "CLISC: Bridging clip and sam by enhanced cam for unsupervised brain\n  tumor segmentation", "author": "Xiaochuan Ma and Jia Fu and Wenjun Liao and Shichuan Zhang and Guotai Wang", "abstract": "  Brain tumor segmentation is important for diagnosis of the tumor, and current\ndeep-learning methods rely on a large set of annotated images for training,\nwith high annotation costs. Unsupervised segmentation is promising to avoid\nhuman annotations while the performance is often limited. In this study, we\npresent a novel unsupervised segmentation approach that leverages the\ncapabilities of foundation models, and it consists of three main steps: (1) A\nvision-language model (i.e., CLIP) is employed to obtain image-level\npseudo-labels for training a classification network. Class Activation Mapping\n(CAM) is then employed to extract Regions of Interest (ROIs), where an adaptive\nmasking-based data augmentation is used to enhance ROI identification.(2) The\nROIs are used to generate bounding box and point prompts for the Segment\nAnything Model (SAM) to obtain segmentation pseudo-labels. (3) A 3D\nsegmentation network is trained with the SAM-derived pseudo-labels, where\nlow-quality pseudo-labels are filtered out in a self-learning process based on\nthe similarity between the SAM's output and the network's prediction.\nEvaluation on the BraTS2020 dataset demonstrates that our approach obtained an\naverage Dice Similarity Score (DSC) of 85.60%, outperforming five\nstate-of-the-art unsupervised segmentation methods by more than 10 percentage\npoints. Besides, our approach outperforms directly using SAM for zero-shot\ninference, and its performance is close to fully supervised learning.\n", "link": "http://arxiv.org/abs/2501.16246v1", "date": "2025-01-27", "relevancy": 2.2881, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6026}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLISC%3A%20Bridging%20clip%20and%20sam%20by%20enhanced%20cam%20for%20unsupervised%20brain%0A%20%20tumor%20segmentation&body=Title%3A%20CLISC%3A%20Bridging%20clip%20and%20sam%20by%20enhanced%20cam%20for%20unsupervised%20brain%0A%20%20tumor%20segmentation%0AAuthor%3A%20Xiaochuan%20Ma%20and%20Jia%20Fu%20and%20Wenjun%20Liao%20and%20Shichuan%20Zhang%20and%20Guotai%20Wang%0AAbstract%3A%20%20%20Brain%20tumor%20segmentation%20is%20important%20for%20diagnosis%20of%20the%20tumor%2C%20and%20current%0Adeep-learning%20methods%20rely%20on%20a%20large%20set%20of%20annotated%20images%20for%20training%2C%0Awith%20high%20annotation%20costs.%20Unsupervised%20segmentation%20is%20promising%20to%20avoid%0Ahuman%20annotations%20while%20the%20performance%20is%20often%20limited.%20In%20this%20study%2C%20we%0Apresent%20a%20novel%20unsupervised%20segmentation%20approach%20that%20leverages%20the%0Acapabilities%20of%20foundation%20models%2C%20and%20it%20consists%20of%20three%20main%20steps%3A%20%281%29%20A%0Avision-language%20model%20%28i.e.%2C%20CLIP%29%20is%20employed%20to%20obtain%20image-level%0Apseudo-labels%20for%20training%20a%20classification%20network.%20Class%20Activation%20Mapping%0A%28CAM%29%20is%20then%20employed%20to%20extract%20Regions%20of%20Interest%20%28ROIs%29%2C%20where%20an%20adaptive%0Amasking-based%20data%20augmentation%20is%20used%20to%20enhance%20ROI%20identification.%282%29%20The%0AROIs%20are%20used%20to%20generate%20bounding%20box%20and%20point%20prompts%20for%20the%20Segment%0AAnything%20Model%20%28SAM%29%20to%20obtain%20segmentation%20pseudo-labels.%20%283%29%20A%203D%0Asegmentation%20network%20is%20trained%20with%20the%20SAM-derived%20pseudo-labels%2C%20where%0Alow-quality%20pseudo-labels%20are%20filtered%20out%20in%20a%20self-learning%20process%20based%20on%0Athe%20similarity%20between%20the%20SAM%27s%20output%20and%20the%20network%27s%20prediction.%0AEvaluation%20on%20the%20BraTS2020%20dataset%20demonstrates%20that%20our%20approach%20obtained%20an%0Aaverage%20Dice%20Similarity%20Score%20%28DSC%29%20of%2085.60%25%2C%20outperforming%20five%0Astate-of-the-art%20unsupervised%20segmentation%20methods%20by%20more%20than%2010%20percentage%0Apoints.%20Besides%2C%20our%20approach%20outperforms%20directly%20using%20SAM%20for%20zero-shot%0Ainference%2C%20and%20its%20performance%20is%20close%20to%20fully%20supervised%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLISC%253A%2520Bridging%2520clip%2520and%2520sam%2520by%2520enhanced%2520cam%2520for%2520unsupervised%2520brain%250A%2520%2520tumor%2520segmentation%26entry.906535625%3DXiaochuan%2520Ma%2520and%2520Jia%2520Fu%2520and%2520Wenjun%2520Liao%2520and%2520Shichuan%2520Zhang%2520and%2520Guotai%2520Wang%26entry.1292438233%3D%2520%2520Brain%2520tumor%2520segmentation%2520is%2520important%2520for%2520diagnosis%2520of%2520the%2520tumor%252C%2520and%2520current%250Adeep-learning%2520methods%2520rely%2520on%2520a%2520large%2520set%2520of%2520annotated%2520images%2520for%2520training%252C%250Awith%2520high%2520annotation%2520costs.%2520Unsupervised%2520segmentation%2520is%2520promising%2520to%2520avoid%250Ahuman%2520annotations%2520while%2520the%2520performance%2520is%2520often%2520limited.%2520In%2520this%2520study%252C%2520we%250Apresent%2520a%2520novel%2520unsupervised%2520segmentation%2520approach%2520that%2520leverages%2520the%250Acapabilities%2520of%2520foundation%2520models%252C%2520and%2520it%2520consists%2520of%2520three%2520main%2520steps%253A%2520%25281%2529%2520A%250Avision-language%2520model%2520%2528i.e.%252C%2520CLIP%2529%2520is%2520employed%2520to%2520obtain%2520image-level%250Apseudo-labels%2520for%2520training%2520a%2520classification%2520network.%2520Class%2520Activation%2520Mapping%250A%2528CAM%2529%2520is%2520then%2520employed%2520to%2520extract%2520Regions%2520of%2520Interest%2520%2528ROIs%2529%252C%2520where%2520an%2520adaptive%250Amasking-based%2520data%2520augmentation%2520is%2520used%2520to%2520enhance%2520ROI%2520identification.%25282%2529%2520The%250AROIs%2520are%2520used%2520to%2520generate%2520bounding%2520box%2520and%2520point%2520prompts%2520for%2520the%2520Segment%250AAnything%2520Model%2520%2528SAM%2529%2520to%2520obtain%2520segmentation%2520pseudo-labels.%2520%25283%2529%2520A%25203D%250Asegmentation%2520network%2520is%2520trained%2520with%2520the%2520SAM-derived%2520pseudo-labels%252C%2520where%250Alow-quality%2520pseudo-labels%2520are%2520filtered%2520out%2520in%2520a%2520self-learning%2520process%2520based%2520on%250Athe%2520similarity%2520between%2520the%2520SAM%2527s%2520output%2520and%2520the%2520network%2527s%2520prediction.%250AEvaluation%2520on%2520the%2520BraTS2020%2520dataset%2520demonstrates%2520that%2520our%2520approach%2520obtained%2520an%250Aaverage%2520Dice%2520Similarity%2520Score%2520%2528DSC%2529%2520of%252085.60%2525%252C%2520outperforming%2520five%250Astate-of-the-art%2520unsupervised%2520segmentation%2520methods%2520by%2520more%2520than%252010%2520percentage%250Apoints.%2520Besides%252C%2520our%2520approach%2520outperforms%2520directly%2520using%2520SAM%2520for%2520zero-shot%250Ainference%252C%2520and%2520its%2520performance%2520is%2520close%2520to%2520fully%2520supervised%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLISC%3A%20Bridging%20clip%20and%20sam%20by%20enhanced%20cam%20for%20unsupervised%20brain%0A%20%20tumor%20segmentation&entry.906535625=Xiaochuan%20Ma%20and%20Jia%20Fu%20and%20Wenjun%20Liao%20and%20Shichuan%20Zhang%20and%20Guotai%20Wang&entry.1292438233=%20%20Brain%20tumor%20segmentation%20is%20important%20for%20diagnosis%20of%20the%20tumor%2C%20and%20current%0Adeep-learning%20methods%20rely%20on%20a%20large%20set%20of%20annotated%20images%20for%20training%2C%0Awith%20high%20annotation%20costs.%20Unsupervised%20segmentation%20is%20promising%20to%20avoid%0Ahuman%20annotations%20while%20the%20performance%20is%20often%20limited.%20In%20this%20study%2C%20we%0Apresent%20a%20novel%20unsupervised%20segmentation%20approach%20that%20leverages%20the%0Acapabilities%20of%20foundation%20models%2C%20and%20it%20consists%20of%20three%20main%20steps%3A%20%281%29%20A%0Avision-language%20model%20%28i.e.%2C%20CLIP%29%20is%20employed%20to%20obtain%20image-level%0Apseudo-labels%20for%20training%20a%20classification%20network.%20Class%20Activation%20Mapping%0A%28CAM%29%20is%20then%20employed%20to%20extract%20Regions%20of%20Interest%20%28ROIs%29%2C%20where%20an%20adaptive%0Amasking-based%20data%20augmentation%20is%20used%20to%20enhance%20ROI%20identification.%282%29%20The%0AROIs%20are%20used%20to%20generate%20bounding%20box%20and%20point%20prompts%20for%20the%20Segment%0AAnything%20Model%20%28SAM%29%20to%20obtain%20segmentation%20pseudo-labels.%20%283%29%20A%203D%0Asegmentation%20network%20is%20trained%20with%20the%20SAM-derived%20pseudo-labels%2C%20where%0Alow-quality%20pseudo-labels%20are%20filtered%20out%20in%20a%20self-learning%20process%20based%20on%0Athe%20similarity%20between%20the%20SAM%27s%20output%20and%20the%20network%27s%20prediction.%0AEvaluation%20on%20the%20BraTS2020%20dataset%20demonstrates%20that%20our%20approach%20obtained%20an%0Aaverage%20Dice%20Similarity%20Score%20%28DSC%29%20of%2085.60%25%2C%20outperforming%20five%0Astate-of-the-art%20unsupervised%20segmentation%20methods%20by%20more%20than%2010%20percentage%0Apoints.%20Besides%2C%20our%20approach%20outperforms%20directly%20using%20SAM%20for%20zero-shot%0Ainference%2C%20and%20its%20performance%20is%20close%20to%20fully%20supervised%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16246v1&entry.124074799=Read"},
{"title": "Multi-view Structural Convolution Network for Domain-Invariant Point\n  Cloud Recognition of Autonomous Vehicles", "author": "Younggun Kim and Beomsik Cho and Seonghoon Ryoo and Soomok Lee", "abstract": "  Point cloud representation has recently become a research hotspot in the\nfield of computer vision and has been utilized for autonomous vehicles.\nHowever, adapting deep learning networks for point cloud data recognition is\nchallenging due to the variability in datasets and sensor technologies. This\nvariability underscores the necessity for adaptive techniques to maintain\naccuracy under different conditions. In this paper, we present the Multi-View\nStructural Convolution Network (MSCN) designed for domain-invariant point cloud\nrecognition. MSCN comprises Structural Convolution Layers (SCL) that extract\nlocal context geometric features from point clouds and Structural Aggregation\nLayers (SAL) that extract and aggregate both local and overall context features\nfrom point clouds. Additionally, our MSCN enhances feature representation\nrobustness by training with unseen domain point clouds derived from source\ndomain point clouds. This method acquires domain-invariant features and\nexhibits robust, consistent performance across various point cloud datasets,\nensuring compatibility with diverse sensor configurations without the need for\nparameter adjustments. This highlights MSCN's potential to significantly\nimprove the reliability and domain invariant features in different\nenvironments. Our code is available at https://github.com/MLMLab/MSCN.\n", "link": "http://arxiv.org/abs/2501.16289v1", "date": "2025-01-27", "relevancy": 2.2873, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.585}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5683}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%20of%20Autonomous%20Vehicles&body=Title%3A%20Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%20of%20Autonomous%20Vehicles%0AAuthor%3A%20Younggun%20Kim%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee%0AAbstract%3A%20%20%20Point%20cloud%20representation%20has%20recently%20become%20a%20research%20hotspot%20in%20the%0Afield%20of%20computer%20vision%20and%20has%20been%20utilized%20for%20autonomous%20vehicles.%0AHowever%2C%20adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20is%0Achallenging%20due%20to%20the%20variability%20in%20datasets%20and%20sensor%20technologies.%20This%0Avariability%20underscores%20the%20necessity%20for%20adaptive%20techniques%20to%20maintain%0Aaccuracy%20under%20different%20conditions.%20In%20this%20paper%2C%20we%20present%20the%20Multi-View%0AStructural%20Convolution%20Network%20%28MSCN%29%20designed%20for%20domain-invariant%20point%20cloud%0Arecognition.%20MSCN%20comprises%20Structural%20Convolution%20Layers%20%28SCL%29%20that%20extract%0Alocal%20context%20geometric%20features%20from%20point%20clouds%20and%20Structural%20Aggregation%0ALayers%20%28SAL%29%20that%20extract%20and%20aggregate%20both%20local%20and%20overall%20context%20features%0Afrom%20point%20clouds.%20Additionally%2C%20our%20MSCN%20enhances%20feature%20representation%0Arobustness%20by%20training%20with%20unseen%20domain%20point%20clouds%20derived%20from%20source%0Adomain%20point%20clouds.%20This%20method%20acquires%20domain-invariant%20features%20and%0Aexhibits%20robust%2C%20consistent%20performance%20across%20various%20point%20cloud%20datasets%2C%0Aensuring%20compatibility%20with%20diverse%20sensor%20configurations%20without%20the%20need%20for%0Aparameter%20adjustments.%20This%20highlights%20MSCN%27s%20potential%20to%20significantly%0Aimprove%20the%20reliability%20and%20domain%20invariant%20features%20in%20different%0Aenvironments.%20Our%20code%20is%20available%20at%20https%3A//github.com/MLMLab/MSCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Structural%2520Convolution%2520Network%2520for%2520Domain-Invariant%2520Point%250A%2520%2520Cloud%2520Recognition%2520of%2520Autonomous%2520Vehicles%26entry.906535625%3DYounggun%2520Kim%2520and%2520Beomsik%2520Cho%2520and%2520Seonghoon%2520Ryoo%2520and%2520Soomok%2520Lee%26entry.1292438233%3D%2520%2520Point%2520cloud%2520representation%2520has%2520recently%2520become%2520a%2520research%2520hotspot%2520in%2520the%250Afield%2520of%2520computer%2520vision%2520and%2520has%2520been%2520utilized%2520for%2520autonomous%2520vehicles.%250AHowever%252C%2520adapting%2520deep%2520learning%2520networks%2520for%2520point%2520cloud%2520data%2520recognition%2520is%250Achallenging%2520due%2520to%2520the%2520variability%2520in%2520datasets%2520and%2520sensor%2520technologies.%2520This%250Avariability%2520underscores%2520the%2520necessity%2520for%2520adaptive%2520techniques%2520to%2520maintain%250Aaccuracy%2520under%2520different%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520Multi-View%250AStructural%2520Convolution%2520Network%2520%2528MSCN%2529%2520designed%2520for%2520domain-invariant%2520point%2520cloud%250Arecognition.%2520MSCN%2520comprises%2520Structural%2520Convolution%2520Layers%2520%2528SCL%2529%2520that%2520extract%250Alocal%2520context%2520geometric%2520features%2520from%2520point%2520clouds%2520and%2520Structural%2520Aggregation%250ALayers%2520%2528SAL%2529%2520that%2520extract%2520and%2520aggregate%2520both%2520local%2520and%2520overall%2520context%2520features%250Afrom%2520point%2520clouds.%2520Additionally%252C%2520our%2520MSCN%2520enhances%2520feature%2520representation%250Arobustness%2520by%2520training%2520with%2520unseen%2520domain%2520point%2520clouds%2520derived%2520from%2520source%250Adomain%2520point%2520clouds.%2520This%2520method%2520acquires%2520domain-invariant%2520features%2520and%250Aexhibits%2520robust%252C%2520consistent%2520performance%2520across%2520various%2520point%2520cloud%2520datasets%252C%250Aensuring%2520compatibility%2520with%2520diverse%2520sensor%2520configurations%2520without%2520the%2520need%2520for%250Aparameter%2520adjustments.%2520This%2520highlights%2520MSCN%2527s%2520potential%2520to%2520significantly%250Aimprove%2520the%2520reliability%2520and%2520domain%2520invariant%2520features%2520in%2520different%250Aenvironments.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/MLMLab/MSCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%20of%20Autonomous%20Vehicles&entry.906535625=Younggun%20Kim%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee&entry.1292438233=%20%20Point%20cloud%20representation%20has%20recently%20become%20a%20research%20hotspot%20in%20the%0Afield%20of%20computer%20vision%20and%20has%20been%20utilized%20for%20autonomous%20vehicles.%0AHowever%2C%20adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20is%0Achallenging%20due%20to%20the%20variability%20in%20datasets%20and%20sensor%20technologies.%20This%0Avariability%20underscores%20the%20necessity%20for%20adaptive%20techniques%20to%20maintain%0Aaccuracy%20under%20different%20conditions.%20In%20this%20paper%2C%20we%20present%20the%20Multi-View%0AStructural%20Convolution%20Network%20%28MSCN%29%20designed%20for%20domain-invariant%20point%20cloud%0Arecognition.%20MSCN%20comprises%20Structural%20Convolution%20Layers%20%28SCL%29%20that%20extract%0Alocal%20context%20geometric%20features%20from%20point%20clouds%20and%20Structural%20Aggregation%0ALayers%20%28SAL%29%20that%20extract%20and%20aggregate%20both%20local%20and%20overall%20context%20features%0Afrom%20point%20clouds.%20Additionally%2C%20our%20MSCN%20enhances%20feature%20representation%0Arobustness%20by%20training%20with%20unseen%20domain%20point%20clouds%20derived%20from%20source%0Adomain%20point%20clouds.%20This%20method%20acquires%20domain-invariant%20features%20and%0Aexhibits%20robust%2C%20consistent%20performance%20across%20various%20point%20cloud%20datasets%2C%0Aensuring%20compatibility%20with%20diverse%20sensor%20configurations%20without%20the%20need%20for%0Aparameter%20adjustments.%20This%20highlights%20MSCN%27s%20potential%20to%20significantly%0Aimprove%20the%20reliability%20and%20domain%20invariant%20features%20in%20different%0Aenvironments.%20Our%20code%20is%20available%20at%20https%3A//github.com/MLMLab/MSCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16289v1&entry.124074799=Read"},
{"title": "Understanding Long Videos via LLM-Powered Entity Relation Graphs", "author": "Meng Chu and Yicong Li and Tat-Seng Chua", "abstract": "  The analysis of extended video content poses unique challenges in artificial\nintelligence, particularly when dealing with the complexity of tracking and\nunderstanding visual elements across time. Current methodologies that process\nvideo frames sequentially struggle to maintain coherent tracking of objects,\nespecially when these objects temporarily vanish and later reappear in the\nfootage. A critical limitation of these approaches is their inability to\neffectively identify crucial moments in the video, largely due to their limited\ngrasp of temporal relationships. To overcome these obstacles, we present\nGraphVideoAgent, a cutting-edge system that leverages the power of graph-based\nobject tracking in conjunction with large language model capabilities. At its\ncore, our framework employs a dynamic graph structure that maps and monitors\nthe evolving relationships between visual entities throughout the video\nsequence. This innovative approach enables more nuanced understanding of how\nobjects interact and transform over time, facilitating improved frame selection\nthrough comprehensive contextual awareness. Our approach demonstrates\nremarkable effectiveness when tested against industry benchmarks. In\nevaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2\nimprovement over existing methods while requiring analysis of only 8.2 frames\non average. Similarly, testing on the NExT-QA benchmark yielded a 2.0\nperformance increase with an average frame requirement of 8.1. These results\nunderscore the efficiency of our graph-guided methodology in enhancing both\naccuracy and computational performance in long-form video understanding tasks.\n", "link": "http://arxiv.org/abs/2501.15953v1", "date": "2025-01-27", "relevancy": 2.2863, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Long%20Videos%20via%20LLM-Powered%20Entity%20Relation%20Graphs&body=Title%3A%20Understanding%20Long%20Videos%20via%20LLM-Powered%20Entity%20Relation%20Graphs%0AAuthor%3A%20Meng%20Chu%20and%20Yicong%20Li%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20The%20analysis%20of%20extended%20video%20content%20poses%20unique%20challenges%20in%20artificial%0Aintelligence%2C%20particularly%20when%20dealing%20with%20the%20complexity%20of%20tracking%20and%0Aunderstanding%20visual%20elements%20across%20time.%20Current%20methodologies%20that%20process%0Avideo%20frames%20sequentially%20struggle%20to%20maintain%20coherent%20tracking%20of%20objects%2C%0Aespecially%20when%20these%20objects%20temporarily%20vanish%20and%20later%20reappear%20in%20the%0Afootage.%20A%20critical%20limitation%20of%20these%20approaches%20is%20their%20inability%20to%0Aeffectively%20identify%20crucial%20moments%20in%20the%20video%2C%20largely%20due%20to%20their%20limited%0Agrasp%20of%20temporal%20relationships.%20To%20overcome%20these%20obstacles%2C%20we%20present%0AGraphVideoAgent%2C%20a%20cutting-edge%20system%20that%20leverages%20the%20power%20of%20graph-based%0Aobject%20tracking%20in%20conjunction%20with%20large%20language%20model%20capabilities.%20At%20its%0Acore%2C%20our%20framework%20employs%20a%20dynamic%20graph%20structure%20that%20maps%20and%20monitors%0Athe%20evolving%20relationships%20between%20visual%20entities%20throughout%20the%20video%0Asequence.%20This%20innovative%20approach%20enables%20more%20nuanced%20understanding%20of%20how%0Aobjects%20interact%20and%20transform%20over%20time%2C%20facilitating%20improved%20frame%20selection%0Athrough%20comprehensive%20contextual%20awareness.%20Our%20approach%20demonstrates%0Aremarkable%20effectiveness%20when%20tested%20against%20industry%20benchmarks.%20In%0Aevaluations%20on%20the%20EgoSchema%20dataset%2C%20GraphVideoAgent%20achieved%20a%202.2%0Aimprovement%20over%20existing%20methods%20while%20requiring%20analysis%20of%20only%208.2%20frames%0Aon%20average.%20Similarly%2C%20testing%20on%20the%20NExT-QA%20benchmark%20yielded%20a%202.0%0Aperformance%20increase%20with%20an%20average%20frame%20requirement%20of%208.1.%20These%20results%0Aunderscore%20the%20efficiency%20of%20our%20graph-guided%20methodology%20in%20enhancing%20both%0Aaccuracy%20and%20computational%20performance%20in%20long-form%20video%20understanding%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Long%2520Videos%2520via%2520LLM-Powered%2520Entity%2520Relation%2520Graphs%26entry.906535625%3DMeng%2520Chu%2520and%2520Yicong%2520Li%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520The%2520analysis%2520of%2520extended%2520video%2520content%2520poses%2520unique%2520challenges%2520in%2520artificial%250Aintelligence%252C%2520particularly%2520when%2520dealing%2520with%2520the%2520complexity%2520of%2520tracking%2520and%250Aunderstanding%2520visual%2520elements%2520across%2520time.%2520Current%2520methodologies%2520that%2520process%250Avideo%2520frames%2520sequentially%2520struggle%2520to%2520maintain%2520coherent%2520tracking%2520of%2520objects%252C%250Aespecially%2520when%2520these%2520objects%2520temporarily%2520vanish%2520and%2520later%2520reappear%2520in%2520the%250Afootage.%2520A%2520critical%2520limitation%2520of%2520these%2520approaches%2520is%2520their%2520inability%2520to%250Aeffectively%2520identify%2520crucial%2520moments%2520in%2520the%2520video%252C%2520largely%2520due%2520to%2520their%2520limited%250Agrasp%2520of%2520temporal%2520relationships.%2520To%2520overcome%2520these%2520obstacles%252C%2520we%2520present%250AGraphVideoAgent%252C%2520a%2520cutting-edge%2520system%2520that%2520leverages%2520the%2520power%2520of%2520graph-based%250Aobject%2520tracking%2520in%2520conjunction%2520with%2520large%2520language%2520model%2520capabilities.%2520At%2520its%250Acore%252C%2520our%2520framework%2520employs%2520a%2520dynamic%2520graph%2520structure%2520that%2520maps%2520and%2520monitors%250Athe%2520evolving%2520relationships%2520between%2520visual%2520entities%2520throughout%2520the%2520video%250Asequence.%2520This%2520innovative%2520approach%2520enables%2520more%2520nuanced%2520understanding%2520of%2520how%250Aobjects%2520interact%2520and%2520transform%2520over%2520time%252C%2520facilitating%2520improved%2520frame%2520selection%250Athrough%2520comprehensive%2520contextual%2520awareness.%2520Our%2520approach%2520demonstrates%250Aremarkable%2520effectiveness%2520when%2520tested%2520against%2520industry%2520benchmarks.%2520In%250Aevaluations%2520on%2520the%2520EgoSchema%2520dataset%252C%2520GraphVideoAgent%2520achieved%2520a%25202.2%250Aimprovement%2520over%2520existing%2520methods%2520while%2520requiring%2520analysis%2520of%2520only%25208.2%2520frames%250Aon%2520average.%2520Similarly%252C%2520testing%2520on%2520the%2520NExT-QA%2520benchmark%2520yielded%2520a%25202.0%250Aperformance%2520increase%2520with%2520an%2520average%2520frame%2520requirement%2520of%25208.1.%2520These%2520results%250Aunderscore%2520the%2520efficiency%2520of%2520our%2520graph-guided%2520methodology%2520in%2520enhancing%2520both%250Aaccuracy%2520and%2520computational%2520performance%2520in%2520long-form%2520video%2520understanding%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Long%20Videos%20via%20LLM-Powered%20Entity%20Relation%20Graphs&entry.906535625=Meng%20Chu%20and%20Yicong%20Li%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20The%20analysis%20of%20extended%20video%20content%20poses%20unique%20challenges%20in%20artificial%0Aintelligence%2C%20particularly%20when%20dealing%20with%20the%20complexity%20of%20tracking%20and%0Aunderstanding%20visual%20elements%20across%20time.%20Current%20methodologies%20that%20process%0Avideo%20frames%20sequentially%20struggle%20to%20maintain%20coherent%20tracking%20of%20objects%2C%0Aespecially%20when%20these%20objects%20temporarily%20vanish%20and%20later%20reappear%20in%20the%0Afootage.%20A%20critical%20limitation%20of%20these%20approaches%20is%20their%20inability%20to%0Aeffectively%20identify%20crucial%20moments%20in%20the%20video%2C%20largely%20due%20to%20their%20limited%0Agrasp%20of%20temporal%20relationships.%20To%20overcome%20these%20obstacles%2C%20we%20present%0AGraphVideoAgent%2C%20a%20cutting-edge%20system%20that%20leverages%20the%20power%20of%20graph-based%0Aobject%20tracking%20in%20conjunction%20with%20large%20language%20model%20capabilities.%20At%20its%0Acore%2C%20our%20framework%20employs%20a%20dynamic%20graph%20structure%20that%20maps%20and%20monitors%0Athe%20evolving%20relationships%20between%20visual%20entities%20throughout%20the%20video%0Asequence.%20This%20innovative%20approach%20enables%20more%20nuanced%20understanding%20of%20how%0Aobjects%20interact%20and%20transform%20over%20time%2C%20facilitating%20improved%20frame%20selection%0Athrough%20comprehensive%20contextual%20awareness.%20Our%20approach%20demonstrates%0Aremarkable%20effectiveness%20when%20tested%20against%20industry%20benchmarks.%20In%0Aevaluations%20on%20the%20EgoSchema%20dataset%2C%20GraphVideoAgent%20achieved%20a%202.2%0Aimprovement%20over%20existing%20methods%20while%20requiring%20analysis%20of%20only%208.2%20frames%0Aon%20average.%20Similarly%2C%20testing%20on%20the%20NExT-QA%20benchmark%20yielded%20a%202.0%0Aperformance%20increase%20with%20an%20average%20frame%20requirement%20of%208.1.%20These%20results%0Aunderscore%20the%20efficiency%20of%20our%20graph-guided%20methodology%20in%20enhancing%20both%0Aaccuracy%20and%20computational%20performance%20in%20long-form%20video%20understanding%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15953v1&entry.124074799=Read"},
{"title": "Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning", "author": "Shiping Ge and Qiang Chen and Zhiwei Jiang and Yafeng Yin and Liu Qin and Ziyao Chen and Qing Gu", "abstract": "  Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods.\n", "link": "http://arxiv.org/abs/2412.12791v2", "date": "2025-01-27", "relevancy": 2.2796, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5767}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5737}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Location-Caption%20Alignment%20via%20Complementary%20Masking%20for%0A%20%20Weakly-Supervised%20Dense%20Video%20Captioning&body=Title%3A%20Implicit%20Location-Caption%20Alignment%20via%20Complementary%20Masking%20for%0A%20%20Weakly-Supervised%20Dense%20Video%20Captioning%0AAuthor%3A%20Shiping%20Ge%20and%20Qiang%20Chen%20and%20Zhiwei%20Jiang%20and%20Yafeng%20Yin%20and%20Liu%20Qin%20and%20Ziyao%20Chen%20and%20Qing%20Gu%0AAbstract%3A%20%20%20Weakly-Supervised%20Dense%20Video%20Captioning%20%28WSDVC%29%20aims%20to%20localize%20and%0Adescribe%20all%20events%20of%20interest%20in%20a%20video%20without%20requiring%20annotations%20of%0Aevent%20boundaries.%20This%20setting%20poses%20a%20great%20challenge%20in%20accurately%20locating%0Athe%20temporal%20location%20of%20event%2C%20as%20the%20relevant%20supervision%20is%20unavailable.%0AExisting%20methods%20rely%20on%20explicit%20alignment%20constraints%20between%20event%20locations%0Aand%20captions%2C%20which%20involve%20complex%20event%20proposal%20procedures%20during%20both%0Atraining%20and%20inference.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20novel%20implicit%0Alocation-caption%20alignment%20paradigm%20by%20complementary%20masking%2C%20which%20simplifies%0Athe%20complex%20event%20proposal%20and%20localization%20process%20while%20maintaining%0Aeffectiveness.%20Specifically%2C%20our%20model%20comprises%20two%20components%3A%20a%20dual-mode%0Avideo%20captioning%20module%20and%20a%20mask%20generation%20module.%20The%20dual-mode%20video%0Acaptioning%20module%20captures%20global%20event%20information%20and%20generates%20descriptive%0Acaptions%2C%20while%20the%20mask%20generation%20module%20generates%20differentiable%20positive%0Aand%20negative%20masks%20for%20localizing%20the%20events.%20These%20masks%20enable%20the%20implicit%0Aalignment%20of%20event%20locations%20and%20captions%20by%20ensuring%20that%20captions%20generated%0Afrom%20positively%20and%20negatively%20masked%20videos%20are%20complementary%2C%20thereby%20forming%0Aa%20complete%20video%20description.%20In%20this%20way%2C%20even%20under%20weak%20supervision%2C%20the%0Aevent%20location%20and%20event%20caption%20can%20be%20aligned%20implicitly.%20Extensive%0Aexperiments%20on%20the%20public%20datasets%20demonstrate%20that%20our%20method%20outperforms%0Aexisting%20weakly-supervised%20methods%20and%20achieves%20competitive%20results%20compared%20to%0Afully-supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Location-Caption%2520Alignment%2520via%2520Complementary%2520Masking%2520for%250A%2520%2520Weakly-Supervised%2520Dense%2520Video%2520Captioning%26entry.906535625%3DShiping%2520Ge%2520and%2520Qiang%2520Chen%2520and%2520Zhiwei%2520Jiang%2520and%2520Yafeng%2520Yin%2520and%2520Liu%2520Qin%2520and%2520Ziyao%2520Chen%2520and%2520Qing%2520Gu%26entry.1292438233%3D%2520%2520Weakly-Supervised%2520Dense%2520Video%2520Captioning%2520%2528WSDVC%2529%2520aims%2520to%2520localize%2520and%250Adescribe%2520all%2520events%2520of%2520interest%2520in%2520a%2520video%2520without%2520requiring%2520annotations%2520of%250Aevent%2520boundaries.%2520This%2520setting%2520poses%2520a%2520great%2520challenge%2520in%2520accurately%2520locating%250Athe%2520temporal%2520location%2520of%2520event%252C%2520as%2520the%2520relevant%2520supervision%2520is%2520unavailable.%250AExisting%2520methods%2520rely%2520on%2520explicit%2520alignment%2520constraints%2520between%2520event%2520locations%250Aand%2520captions%252C%2520which%2520involve%2520complex%2520event%2520proposal%2520procedures%2520during%2520both%250Atraining%2520and%2520inference.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520implicit%250Alocation-caption%2520alignment%2520paradigm%2520by%2520complementary%2520masking%252C%2520which%2520simplifies%250Athe%2520complex%2520event%2520proposal%2520and%2520localization%2520process%2520while%2520maintaining%250Aeffectiveness.%2520Specifically%252C%2520our%2520model%2520comprises%2520two%2520components%253A%2520a%2520dual-mode%250Avideo%2520captioning%2520module%2520and%2520a%2520mask%2520generation%2520module.%2520The%2520dual-mode%2520video%250Acaptioning%2520module%2520captures%2520global%2520event%2520information%2520and%2520generates%2520descriptive%250Acaptions%252C%2520while%2520the%2520mask%2520generation%2520module%2520generates%2520differentiable%2520positive%250Aand%2520negative%2520masks%2520for%2520localizing%2520the%2520events.%2520These%2520masks%2520enable%2520the%2520implicit%250Aalignment%2520of%2520event%2520locations%2520and%2520captions%2520by%2520ensuring%2520that%2520captions%2520generated%250Afrom%2520positively%2520and%2520negatively%2520masked%2520videos%2520are%2520complementary%252C%2520thereby%2520forming%250Aa%2520complete%2520video%2520description.%2520In%2520this%2520way%252C%2520even%2520under%2520weak%2520supervision%252C%2520the%250Aevent%2520location%2520and%2520event%2520caption%2520can%2520be%2520aligned%2520implicitly.%2520Extensive%250Aexperiments%2520on%2520the%2520public%2520datasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Aexisting%2520weakly-supervised%2520methods%2520and%2520achieves%2520competitive%2520results%2520compared%2520to%250Afully-supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Location-Caption%20Alignment%20via%20Complementary%20Masking%20for%0A%20%20Weakly-Supervised%20Dense%20Video%20Captioning&entry.906535625=Shiping%20Ge%20and%20Qiang%20Chen%20and%20Zhiwei%20Jiang%20and%20Yafeng%20Yin%20and%20Liu%20Qin%20and%20Ziyao%20Chen%20and%20Qing%20Gu&entry.1292438233=%20%20Weakly-Supervised%20Dense%20Video%20Captioning%20%28WSDVC%29%20aims%20to%20localize%20and%0Adescribe%20all%20events%20of%20interest%20in%20a%20video%20without%20requiring%20annotations%20of%0Aevent%20boundaries.%20This%20setting%20poses%20a%20great%20challenge%20in%20accurately%20locating%0Athe%20temporal%20location%20of%20event%2C%20as%20the%20relevant%20supervision%20is%20unavailable.%0AExisting%20methods%20rely%20on%20explicit%20alignment%20constraints%20between%20event%20locations%0Aand%20captions%2C%20which%20involve%20complex%20event%20proposal%20procedures%20during%20both%0Atraining%20and%20inference.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20novel%20implicit%0Alocation-caption%20alignment%20paradigm%20by%20complementary%20masking%2C%20which%20simplifies%0Athe%20complex%20event%20proposal%20and%20localization%20process%20while%20maintaining%0Aeffectiveness.%20Specifically%2C%20our%20model%20comprises%20two%20components%3A%20a%20dual-mode%0Avideo%20captioning%20module%20and%20a%20mask%20generation%20module.%20The%20dual-mode%20video%0Acaptioning%20module%20captures%20global%20event%20information%20and%20generates%20descriptive%0Acaptions%2C%20while%20the%20mask%20generation%20module%20generates%20differentiable%20positive%0Aand%20negative%20masks%20for%20localizing%20the%20events.%20These%20masks%20enable%20the%20implicit%0Aalignment%20of%20event%20locations%20and%20captions%20by%20ensuring%20that%20captions%20generated%0Afrom%20positively%20and%20negatively%20masked%20videos%20are%20complementary%2C%20thereby%20forming%0Aa%20complete%20video%20description.%20In%20this%20way%2C%20even%20under%20weak%20supervision%2C%20the%0Aevent%20location%20and%20event%20caption%20can%20be%20aligned%20implicitly.%20Extensive%0Aexperiments%20on%20the%20public%20datasets%20demonstrate%20that%20our%20method%20outperforms%0Aexisting%20weakly-supervised%20methods%20and%20achieves%20competitive%20results%20compared%20to%0Afully-supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12791v2&entry.124074799=Read"},
{"title": "ARFlow: Autogressive Flow with Hybrid Linear Attention", "author": "Mude Hui and Rui-Jie Zhu and Songlin Yang and Yu Zhang and Zirui Wang and Yuyin Zhou and Jason Eshraghian and Cihang Xie", "abstract": "  Flow models are effective at progressively generating realistic images, but\nthey generally struggle to capture long-range dependencies during the\ngeneration process as they compress all the information from previous time\nsteps into a single corrupted image. To address this limitation, we propose\nintegrating autoregressive modeling -- known for its excellence in modeling\ncomplex, high-dimensional joint probability distributions -- into flow models.\nDuring training, at each step, we construct causally-ordered sequences by\nsampling multiple images from the same semantic category and applying different\nlevels of noise, where images with higher noise levels serve as causal\npredecessors to those with lower noise levels. This design enables the model to\nlearn broader category-level variations while maintaining proper causal\nrelationships in the flow process. During generation, the model\nautoregressively conditions the previously generated images from earlier\ndenoising steps, forming a contextual and coherent generation trajectory.\nAdditionally, we design a customized hybrid linear attention mechanism tailored\nto our modeling approach to enhance computational efficiency. Our approach,\ntermed ARFlow, under 400k training steps, achieves 14.08 FID scores on ImageNet\nat 128 * 128 without classifier-free guidance, reaching 4.34 FID with\nclassifier-free guidance 1.5, significantly outperforming the previous\nflow-based model SiT's 9.17 FID. Extensive ablation studies demonstrate the\neffectiveness of our modeling strategy and chunk-wise attention design.\n", "link": "http://arxiv.org/abs/2501.16085v1", "date": "2025-01-27", "relevancy": 2.2431, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6726}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5385}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARFlow%3A%20Autogressive%20Flow%20with%20Hybrid%20Linear%20Attention&body=Title%3A%20ARFlow%3A%20Autogressive%20Flow%20with%20Hybrid%20Linear%20Attention%0AAuthor%3A%20Mude%20Hui%20and%20Rui-Jie%20Zhu%20and%20Songlin%20Yang%20and%20Yu%20Zhang%20and%20Zirui%20Wang%20and%20Yuyin%20Zhou%20and%20Jason%20Eshraghian%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20Flow%20models%20are%20effective%20at%20progressively%20generating%20realistic%20images%2C%20but%0Athey%20generally%20struggle%20to%20capture%20long-range%20dependencies%20during%20the%0Ageneration%20process%20as%20they%20compress%20all%20the%20information%20from%20previous%20time%0Asteps%20into%20a%20single%20corrupted%20image.%20To%20address%20this%20limitation%2C%20we%20propose%0Aintegrating%20autoregressive%20modeling%20--%20known%20for%20its%20excellence%20in%20modeling%0Acomplex%2C%20high-dimensional%20joint%20probability%20distributions%20--%20into%20flow%20models.%0ADuring%20training%2C%20at%20each%20step%2C%20we%20construct%20causally-ordered%20sequences%20by%0Asampling%20multiple%20images%20from%20the%20same%20semantic%20category%20and%20applying%20different%0Alevels%20of%20noise%2C%20where%20images%20with%20higher%20noise%20levels%20serve%20as%20causal%0Apredecessors%20to%20those%20with%20lower%20noise%20levels.%20This%20design%20enables%20the%20model%20to%0Alearn%20broader%20category-level%20variations%20while%20maintaining%20proper%20causal%0Arelationships%20in%20the%20flow%20process.%20During%20generation%2C%20the%20model%0Aautoregressively%20conditions%20the%20previously%20generated%20images%20from%20earlier%0Adenoising%20steps%2C%20forming%20a%20contextual%20and%20coherent%20generation%20trajectory.%0AAdditionally%2C%20we%20design%20a%20customized%20hybrid%20linear%20attention%20mechanism%20tailored%0Ato%20our%20modeling%20approach%20to%20enhance%20computational%20efficiency.%20Our%20approach%2C%0Atermed%20ARFlow%2C%20under%20400k%20training%20steps%2C%20achieves%2014.08%20FID%20scores%20on%20ImageNet%0Aat%20128%20%2A%20128%20without%20classifier-free%20guidance%2C%20reaching%204.34%20FID%20with%0Aclassifier-free%20guidance%201.5%2C%20significantly%20outperforming%20the%20previous%0Aflow-based%20model%20SiT%27s%209.17%20FID.%20Extensive%20ablation%20studies%20demonstrate%20the%0Aeffectiveness%20of%20our%20modeling%20strategy%20and%20chunk-wise%20attention%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARFlow%253A%2520Autogressive%2520Flow%2520with%2520Hybrid%2520Linear%2520Attention%26entry.906535625%3DMude%2520Hui%2520and%2520Rui-Jie%2520Zhu%2520and%2520Songlin%2520Yang%2520and%2520Yu%2520Zhang%2520and%2520Zirui%2520Wang%2520and%2520Yuyin%2520Zhou%2520and%2520Jason%2520Eshraghian%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520Flow%2520models%2520are%2520effective%2520at%2520progressively%2520generating%2520realistic%2520images%252C%2520but%250Athey%2520generally%2520struggle%2520to%2520capture%2520long-range%2520dependencies%2520during%2520the%250Ageneration%2520process%2520as%2520they%2520compress%2520all%2520the%2520information%2520from%2520previous%2520time%250Asteps%2520into%2520a%2520single%2520corrupted%2520image.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250Aintegrating%2520autoregressive%2520modeling%2520--%2520known%2520for%2520its%2520excellence%2520in%2520modeling%250Acomplex%252C%2520high-dimensional%2520joint%2520probability%2520distributions%2520--%2520into%2520flow%2520models.%250ADuring%2520training%252C%2520at%2520each%2520step%252C%2520we%2520construct%2520causally-ordered%2520sequences%2520by%250Asampling%2520multiple%2520images%2520from%2520the%2520same%2520semantic%2520category%2520and%2520applying%2520different%250Alevels%2520of%2520noise%252C%2520where%2520images%2520with%2520higher%2520noise%2520levels%2520serve%2520as%2520causal%250Apredecessors%2520to%2520those%2520with%2520lower%2520noise%2520levels.%2520This%2520design%2520enables%2520the%2520model%2520to%250Alearn%2520broader%2520category-level%2520variations%2520while%2520maintaining%2520proper%2520causal%250Arelationships%2520in%2520the%2520flow%2520process.%2520During%2520generation%252C%2520the%2520model%250Aautoregressively%2520conditions%2520the%2520previously%2520generated%2520images%2520from%2520earlier%250Adenoising%2520steps%252C%2520forming%2520a%2520contextual%2520and%2520coherent%2520generation%2520trajectory.%250AAdditionally%252C%2520we%2520design%2520a%2520customized%2520hybrid%2520linear%2520attention%2520mechanism%2520tailored%250Ato%2520our%2520modeling%2520approach%2520to%2520enhance%2520computational%2520efficiency.%2520Our%2520approach%252C%250Atermed%2520ARFlow%252C%2520under%2520400k%2520training%2520steps%252C%2520achieves%252014.08%2520FID%2520scores%2520on%2520ImageNet%250Aat%2520128%2520%252A%2520128%2520without%2520classifier-free%2520guidance%252C%2520reaching%25204.34%2520FID%2520with%250Aclassifier-free%2520guidance%25201.5%252C%2520significantly%2520outperforming%2520the%2520previous%250Aflow-based%2520model%2520SiT%2527s%25209.17%2520FID.%2520Extensive%2520ablation%2520studies%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520modeling%2520strategy%2520and%2520chunk-wise%2520attention%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARFlow%3A%20Autogressive%20Flow%20with%20Hybrid%20Linear%20Attention&entry.906535625=Mude%20Hui%20and%20Rui-Jie%20Zhu%20and%20Songlin%20Yang%20and%20Yu%20Zhang%20and%20Zirui%20Wang%20and%20Yuyin%20Zhou%20and%20Jason%20Eshraghian%20and%20Cihang%20Xie&entry.1292438233=%20%20Flow%20models%20are%20effective%20at%20progressively%20generating%20realistic%20images%2C%20but%0Athey%20generally%20struggle%20to%20capture%20long-range%20dependencies%20during%20the%0Ageneration%20process%20as%20they%20compress%20all%20the%20information%20from%20previous%20time%0Asteps%20into%20a%20single%20corrupted%20image.%20To%20address%20this%20limitation%2C%20we%20propose%0Aintegrating%20autoregressive%20modeling%20--%20known%20for%20its%20excellence%20in%20modeling%0Acomplex%2C%20high-dimensional%20joint%20probability%20distributions%20--%20into%20flow%20models.%0ADuring%20training%2C%20at%20each%20step%2C%20we%20construct%20causally-ordered%20sequences%20by%0Asampling%20multiple%20images%20from%20the%20same%20semantic%20category%20and%20applying%20different%0Alevels%20of%20noise%2C%20where%20images%20with%20higher%20noise%20levels%20serve%20as%20causal%0Apredecessors%20to%20those%20with%20lower%20noise%20levels.%20This%20design%20enables%20the%20model%20to%0Alearn%20broader%20category-level%20variations%20while%20maintaining%20proper%20causal%0Arelationships%20in%20the%20flow%20process.%20During%20generation%2C%20the%20model%0Aautoregressively%20conditions%20the%20previously%20generated%20images%20from%20earlier%0Adenoising%20steps%2C%20forming%20a%20contextual%20and%20coherent%20generation%20trajectory.%0AAdditionally%2C%20we%20design%20a%20customized%20hybrid%20linear%20attention%20mechanism%20tailored%0Ato%20our%20modeling%20approach%20to%20enhance%20computational%20efficiency.%20Our%20approach%2C%0Atermed%20ARFlow%2C%20under%20400k%20training%20steps%2C%20achieves%2014.08%20FID%20scores%20on%20ImageNet%0Aat%20128%20%2A%20128%20without%20classifier-free%20guidance%2C%20reaching%204.34%20FID%20with%0Aclassifier-free%20guidance%201.5%2C%20significantly%20outperforming%20the%20previous%0Aflow-based%20model%20SiT%27s%209.17%20FID.%20Extensive%20ablation%20studies%20demonstrate%20the%0Aeffectiveness%20of%20our%20modeling%20strategy%20and%20chunk-wise%20attention%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16085v1&entry.124074799=Read"},
{"title": "Text-driven Adaptation of Foundation Models for Few-shot Surgical\n  Workflow Analysis", "author": "Tingxuan Chen and Kun Yuan and Vinkle Srivastav and Nassir Navab and Nicolas Padoy", "abstract": "  Purpose: Surgical workflow analysis is crucial for improving surgical\nefficiency and safety. However, previous studies rely heavily on large-scale\nannotated datasets, posing challenges in cost, scalability, and reliance on\nexpert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven\nAdaptation), designed to handle various surgical workflow analysis tasks with\nminimal paired image-label data.\n  Methods: Our approach has two key components. First, Few-shot selection-based\nmodality alignment selects a small subset of images and aligns their embeddings\nwith text embeddings from the downstream task, bridging the modality gap.\nSecond, Text-driven adaptation leverages only text data to train a decoder,\neliminating the need for paired image-text data. This decoder is then applied\nto aligned image embeddings, enabling image-related tasks without explicit\nimage-text pairs.\n  Results: We evaluate our approach to generative tasks (image captioning) and\ndiscriminative tasks (triplet recognition and phase recognition). Results show\nthat Surg-FTDA outperforms baselines and generalizes well across downstream\ntasks.\n  Conclusion: We propose a text-driven adaptation approach that mitigates the\nmodality gap and handles multiple downstream tasks in surgical workflow\nanalysis, with minimal reliance on large annotated datasets. The code and\ndataset will be released in https://github.com/CAMMA-public/Surg-FTDA\n", "link": "http://arxiv.org/abs/2501.09555v2", "date": "2025-01-27", "relevancy": 2.2397, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5712}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5541}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-driven%20Adaptation%20of%20Foundation%20Models%20for%20Few-shot%20Surgical%0A%20%20Workflow%20Analysis&body=Title%3A%20Text-driven%20Adaptation%20of%20Foundation%20Models%20for%20Few-shot%20Surgical%0A%20%20Workflow%20Analysis%0AAuthor%3A%20Tingxuan%20Chen%20and%20Kun%20Yuan%20and%20Vinkle%20Srivastav%20and%20Nassir%20Navab%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Purpose%3A%20Surgical%20workflow%20analysis%20is%20crucial%20for%20improving%20surgical%0Aefficiency%20and%20safety.%20However%2C%20previous%20studies%20rely%20heavily%20on%20large-scale%0Aannotated%20datasets%2C%20posing%20challenges%20in%20cost%2C%20scalability%2C%20and%20reliance%20on%0Aexpert%20annotations.%20To%20address%20this%2C%20we%20propose%20Surg-FTDA%20%28Few-shot%20Text-driven%0AAdaptation%29%2C%20designed%20to%20handle%20various%20surgical%20workflow%20analysis%20tasks%20with%0Aminimal%20paired%20image-label%20data.%0A%20%20Methods%3A%20Our%20approach%20has%20two%20key%20components.%20First%2C%20Few-shot%20selection-based%0Amodality%20alignment%20selects%20a%20small%20subset%20of%20images%20and%20aligns%20their%20embeddings%0Awith%20text%20embeddings%20from%20the%20downstream%20task%2C%20bridging%20the%20modality%20gap.%0ASecond%2C%20Text-driven%20adaptation%20leverages%20only%20text%20data%20to%20train%20a%20decoder%2C%0Aeliminating%20the%20need%20for%20paired%20image-text%20data.%20This%20decoder%20is%20then%20applied%0Ato%20aligned%20image%20embeddings%2C%20enabling%20image-related%20tasks%20without%20explicit%0Aimage-text%20pairs.%0A%20%20Results%3A%20We%20evaluate%20our%20approach%20to%20generative%20tasks%20%28image%20captioning%29%20and%0Adiscriminative%20tasks%20%28triplet%20recognition%20and%20phase%20recognition%29.%20Results%20show%0Athat%20Surg-FTDA%20outperforms%20baselines%20and%20generalizes%20well%20across%20downstream%0Atasks.%0A%20%20Conclusion%3A%20We%20propose%20a%20text-driven%20adaptation%20approach%20that%20mitigates%20the%0Amodality%20gap%20and%20handles%20multiple%20downstream%20tasks%20in%20surgical%20workflow%0Aanalysis%2C%20with%20minimal%20reliance%20on%20large%20annotated%20datasets.%20The%20code%20and%0Adataset%20will%20be%20released%20in%20https%3A//github.com/CAMMA-public/Surg-FTDA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-driven%2520Adaptation%2520of%2520Foundation%2520Models%2520for%2520Few-shot%2520Surgical%250A%2520%2520Workflow%2520Analysis%26entry.906535625%3DTingxuan%2520Chen%2520and%2520Kun%2520Yuan%2520and%2520Vinkle%2520Srivastav%2520and%2520Nassir%2520Navab%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Purpose%253A%2520Surgical%2520workflow%2520analysis%2520is%2520crucial%2520for%2520improving%2520surgical%250Aefficiency%2520and%2520safety.%2520However%252C%2520previous%2520studies%2520rely%2520heavily%2520on%2520large-scale%250Aannotated%2520datasets%252C%2520posing%2520challenges%2520in%2520cost%252C%2520scalability%252C%2520and%2520reliance%2520on%250Aexpert%2520annotations.%2520To%2520address%2520this%252C%2520we%2520propose%2520Surg-FTDA%2520%2528Few-shot%2520Text-driven%250AAdaptation%2529%252C%2520designed%2520to%2520handle%2520various%2520surgical%2520workflow%2520analysis%2520tasks%2520with%250Aminimal%2520paired%2520image-label%2520data.%250A%2520%2520Methods%253A%2520Our%2520approach%2520has%2520two%2520key%2520components.%2520First%252C%2520Few-shot%2520selection-based%250Amodality%2520alignment%2520selects%2520a%2520small%2520subset%2520of%2520images%2520and%2520aligns%2520their%2520embeddings%250Awith%2520text%2520embeddings%2520from%2520the%2520downstream%2520task%252C%2520bridging%2520the%2520modality%2520gap.%250ASecond%252C%2520Text-driven%2520adaptation%2520leverages%2520only%2520text%2520data%2520to%2520train%2520a%2520decoder%252C%250Aeliminating%2520the%2520need%2520for%2520paired%2520image-text%2520data.%2520This%2520decoder%2520is%2520then%2520applied%250Ato%2520aligned%2520image%2520embeddings%252C%2520enabling%2520image-related%2520tasks%2520without%2520explicit%250Aimage-text%2520pairs.%250A%2520%2520Results%253A%2520We%2520evaluate%2520our%2520approach%2520to%2520generative%2520tasks%2520%2528image%2520captioning%2529%2520and%250Adiscriminative%2520tasks%2520%2528triplet%2520recognition%2520and%2520phase%2520recognition%2529.%2520Results%2520show%250Athat%2520Surg-FTDA%2520outperforms%2520baselines%2520and%2520generalizes%2520well%2520across%2520downstream%250Atasks.%250A%2520%2520Conclusion%253A%2520We%2520propose%2520a%2520text-driven%2520adaptation%2520approach%2520that%2520mitigates%2520the%250Amodality%2520gap%2520and%2520handles%2520multiple%2520downstream%2520tasks%2520in%2520surgical%2520workflow%250Aanalysis%252C%2520with%2520minimal%2520reliance%2520on%2520large%2520annotated%2520datasets.%2520The%2520code%2520and%250Adataset%2520will%2520be%2520released%2520in%2520https%253A//github.com/CAMMA-public/Surg-FTDA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-driven%20Adaptation%20of%20Foundation%20Models%20for%20Few-shot%20Surgical%0A%20%20Workflow%20Analysis&entry.906535625=Tingxuan%20Chen%20and%20Kun%20Yuan%20and%20Vinkle%20Srivastav%20and%20Nassir%20Navab%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Purpose%3A%20Surgical%20workflow%20analysis%20is%20crucial%20for%20improving%20surgical%0Aefficiency%20and%20safety.%20However%2C%20previous%20studies%20rely%20heavily%20on%20large-scale%0Aannotated%20datasets%2C%20posing%20challenges%20in%20cost%2C%20scalability%2C%20and%20reliance%20on%0Aexpert%20annotations.%20To%20address%20this%2C%20we%20propose%20Surg-FTDA%20%28Few-shot%20Text-driven%0AAdaptation%29%2C%20designed%20to%20handle%20various%20surgical%20workflow%20analysis%20tasks%20with%0Aminimal%20paired%20image-label%20data.%0A%20%20Methods%3A%20Our%20approach%20has%20two%20key%20components.%20First%2C%20Few-shot%20selection-based%0Amodality%20alignment%20selects%20a%20small%20subset%20of%20images%20and%20aligns%20their%20embeddings%0Awith%20text%20embeddings%20from%20the%20downstream%20task%2C%20bridging%20the%20modality%20gap.%0ASecond%2C%20Text-driven%20adaptation%20leverages%20only%20text%20data%20to%20train%20a%20decoder%2C%0Aeliminating%20the%20need%20for%20paired%20image-text%20data.%20This%20decoder%20is%20then%20applied%0Ato%20aligned%20image%20embeddings%2C%20enabling%20image-related%20tasks%20without%20explicit%0Aimage-text%20pairs.%0A%20%20Results%3A%20We%20evaluate%20our%20approach%20to%20generative%20tasks%20%28image%20captioning%29%20and%0Adiscriminative%20tasks%20%28triplet%20recognition%20and%20phase%20recognition%29.%20Results%20show%0Athat%20Surg-FTDA%20outperforms%20baselines%20and%20generalizes%20well%20across%20downstream%0Atasks.%0A%20%20Conclusion%3A%20We%20propose%20a%20text-driven%20adaptation%20approach%20that%20mitigates%20the%0Amodality%20gap%20and%20handles%20multiple%20downstream%20tasks%20in%20surgical%20workflow%0Aanalysis%2C%20with%20minimal%20reliance%20on%20large%20annotated%20datasets.%20The%20code%20and%0Adataset%20will%20be%20released%20in%20https%3A//github.com/CAMMA-public/Surg-FTDA%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09555v2&entry.124074799=Read"},
{"title": "Efficient Distillation of Deep Spiking Neural Networks for Full-Range\n  Timestep Deployment", "author": "Chengting Yu and Xiaochen Zhao and Lei Liu and Shu Yang and Gaoang Wang and Erping Li and Aili Wang", "abstract": "  Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative\nto traditional Artificial Neural Networks (ANNs), prized for their potential\nenergy efficiency on neuromorphic hardware. Despite this, SNNs often suffer\nfrom accuracy degradation compared to ANNs and face deployment challenges due\nto fixed inference timesteps, which require retraining for adjustments,\nlimiting operational flexibility. To address these issues, our work considers\nthe spatio-temporal property inherent in SNNs, and proposes a novel\ndistillation framework for deep SNNs that optimizes performance across\nfull-range timesteps without specific retraining, enhancing both efficacy and\ndeployment adaptability. We provide both theoretical analysis and empirical\nvalidations to illustrate that training guarantees the convergence of all\nimplicit models across full-range timesteps. Experimental results on CIFAR-10,\nCIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance\namong distillation-based SNNs training methods.\n", "link": "http://arxiv.org/abs/2501.15925v1", "date": "2025-01-27", "relevancy": 2.2314, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5803}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5608}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Distillation%20of%20Deep%20Spiking%20Neural%20Networks%20for%20Full-Range%0A%20%20Timestep%20Deployment&body=Title%3A%20Efficient%20Distillation%20of%20Deep%20Spiking%20Neural%20Networks%20for%20Full-Range%0A%20%20Timestep%20Deployment%0AAuthor%3A%20Chengting%20Yu%20and%20Xiaochen%20Zhao%20and%20Lei%20Liu%20and%20Shu%20Yang%20and%20Gaoang%20Wang%20and%20Erping%20Li%20and%20Aili%20Wang%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20emerging%20as%20a%20brain-inspired%20alternative%0Ato%20traditional%20Artificial%20Neural%20Networks%20%28ANNs%29%2C%20prized%20for%20their%20potential%0Aenergy%20efficiency%20on%20neuromorphic%20hardware.%20Despite%20this%2C%20SNNs%20often%20suffer%0Afrom%20accuracy%20degradation%20compared%20to%20ANNs%20and%20face%20deployment%20challenges%20due%0Ato%20fixed%20inference%20timesteps%2C%20which%20require%20retraining%20for%20adjustments%2C%0Alimiting%20operational%20flexibility.%20To%20address%20these%20issues%2C%20our%20work%20considers%0Athe%20spatio-temporal%20property%20inherent%20in%20SNNs%2C%20and%20proposes%20a%20novel%0Adistillation%20framework%20for%20deep%20SNNs%20that%20optimizes%20performance%20across%0Afull-range%20timesteps%20without%20specific%20retraining%2C%20enhancing%20both%20efficacy%20and%0Adeployment%20adaptability.%20We%20provide%20both%20theoretical%20analysis%20and%20empirical%0Avalidations%20to%20illustrate%20that%20training%20guarantees%20the%20convergence%20of%20all%0Aimplicit%20models%20across%20full-range%20timesteps.%20Experimental%20results%20on%20CIFAR-10%2C%0ACIFAR-100%2C%20CIFAR10-DVS%2C%20and%20ImageNet%20demonstrate%20state-of-the-art%20performance%0Aamong%20distillation-based%20SNNs%20training%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Distillation%2520of%2520Deep%2520Spiking%2520Neural%2520Networks%2520for%2520Full-Range%250A%2520%2520Timestep%2520Deployment%26entry.906535625%3DChengting%2520Yu%2520and%2520Xiaochen%2520Zhao%2520and%2520Lei%2520Liu%2520and%2520Shu%2520Yang%2520and%2520Gaoang%2520Wang%2520and%2520Erping%2520Li%2520and%2520Aili%2520Wang%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520emerging%2520as%2520a%2520brain-inspired%2520alternative%250Ato%2520traditional%2520Artificial%2520Neural%2520Networks%2520%2528ANNs%2529%252C%2520prized%2520for%2520their%2520potential%250Aenergy%2520efficiency%2520on%2520neuromorphic%2520hardware.%2520Despite%2520this%252C%2520SNNs%2520often%2520suffer%250Afrom%2520accuracy%2520degradation%2520compared%2520to%2520ANNs%2520and%2520face%2520deployment%2520challenges%2520due%250Ato%2520fixed%2520inference%2520timesteps%252C%2520which%2520require%2520retraining%2520for%2520adjustments%252C%250Alimiting%2520operational%2520flexibility.%2520To%2520address%2520these%2520issues%252C%2520our%2520work%2520considers%250Athe%2520spatio-temporal%2520property%2520inherent%2520in%2520SNNs%252C%2520and%2520proposes%2520a%2520novel%250Adistillation%2520framework%2520for%2520deep%2520SNNs%2520that%2520optimizes%2520performance%2520across%250Afull-range%2520timesteps%2520without%2520specific%2520retraining%252C%2520enhancing%2520both%2520efficacy%2520and%250Adeployment%2520adaptability.%2520We%2520provide%2520both%2520theoretical%2520analysis%2520and%2520empirical%250Avalidations%2520to%2520illustrate%2520that%2520training%2520guarantees%2520the%2520convergence%2520of%2520all%250Aimplicit%2520models%2520across%2520full-range%2520timesteps.%2520Experimental%2520results%2520on%2520CIFAR-10%252C%250ACIFAR-100%252C%2520CIFAR10-DVS%252C%2520and%2520ImageNet%2520demonstrate%2520state-of-the-art%2520performance%250Aamong%2520distillation-based%2520SNNs%2520training%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Distillation%20of%20Deep%20Spiking%20Neural%20Networks%20for%20Full-Range%0A%20%20Timestep%20Deployment&entry.906535625=Chengting%20Yu%20and%20Xiaochen%20Zhao%20and%20Lei%20Liu%20and%20Shu%20Yang%20and%20Gaoang%20Wang%20and%20Erping%20Li%20and%20Aili%20Wang&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20emerging%20as%20a%20brain-inspired%20alternative%0Ato%20traditional%20Artificial%20Neural%20Networks%20%28ANNs%29%2C%20prized%20for%20their%20potential%0Aenergy%20efficiency%20on%20neuromorphic%20hardware.%20Despite%20this%2C%20SNNs%20often%20suffer%0Afrom%20accuracy%20degradation%20compared%20to%20ANNs%20and%20face%20deployment%20challenges%20due%0Ato%20fixed%20inference%20timesteps%2C%20which%20require%20retraining%20for%20adjustments%2C%0Alimiting%20operational%20flexibility.%20To%20address%20these%20issues%2C%20our%20work%20considers%0Athe%20spatio-temporal%20property%20inherent%20in%20SNNs%2C%20and%20proposes%20a%20novel%0Adistillation%20framework%20for%20deep%20SNNs%20that%20optimizes%20performance%20across%0Afull-range%20timesteps%20without%20specific%20retraining%2C%20enhancing%20both%20efficacy%20and%0Adeployment%20adaptability.%20We%20provide%20both%20theoretical%20analysis%20and%20empirical%0Avalidations%20to%20illustrate%20that%20training%20guarantees%20the%20convergence%20of%20all%0Aimplicit%20models%20across%20full-range%20timesteps.%20Experimental%20results%20on%20CIFAR-10%2C%0ACIFAR-100%2C%20CIFAR10-DVS%2C%20and%20ImageNet%20demonstrate%20state-of-the-art%20performance%0Aamong%20distillation-based%20SNNs%20training%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15925v1&entry.124074799=Read"},
{"title": "Freestyle Sketch-in-the-Loop Image Segmentation", "author": "Subhadeep Koley and Viswanatha Reddy Gajjala and Aneeshan Sain and Pinaki Nath Chowdhury and Tao Xiang and Ayan Kumar Bhunia and Yi-Zhe Song", "abstract": "  In this paper, we expand the domain of sketch research into the field of\nimage segmentation, aiming to establish freehand sketches as a query modality\nfor subjective image segmentation. Our innovative approach introduces a\n\"sketch-in-the-loop\" image segmentation framework, enabling the segmentation of\nvisual concepts partially, completely, or in groupings - a truly \"freestyle\"\napproach - without the need for a purpose-made dataset (i.e., mask-free). This\nframework capitalises on the synergy between sketch-based image retrieval\n(SBIR) models and large-scale pre-trained models (CLIP or DINOv2). The former\nprovides an effective training signal, while fine-tuned versions of the latter\nexecute the subjective segmentation. Additionally, our purpose-made\naugmentation strategy enhances the versatility of our sketch-guided mask\ngeneration, allowing segmentation at multiple granularity levels. Extensive\nevaluations across diverse benchmark datasets underscore the superior\nperformance of our method in comparison to existing approaches across various\nevaluation scenarios.\n", "link": "http://arxiv.org/abs/2501.16022v1", "date": "2025-01-27", "relevancy": 2.1935, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5678}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Freestyle%20Sketch-in-the-Loop%20Image%20Segmentation&body=Title%3A%20Freestyle%20Sketch-in-the-Loop%20Image%20Segmentation%0AAuthor%3A%20Subhadeep%20Koley%20and%20Viswanatha%20Reddy%20Gajjala%20and%20Aneeshan%20Sain%20and%20Pinaki%20Nath%20Chowdhury%20and%20Tao%20Xiang%20and%20Ayan%20Kumar%20Bhunia%20and%20Yi-Zhe%20Song%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20expand%20the%20domain%20of%20sketch%20research%20into%20the%20field%20of%0Aimage%20segmentation%2C%20aiming%20to%20establish%20freehand%20sketches%20as%20a%20query%20modality%0Afor%20subjective%20image%20segmentation.%20Our%20innovative%20approach%20introduces%20a%0A%22sketch-in-the-loop%22%20image%20segmentation%20framework%2C%20enabling%20the%20segmentation%20of%0Avisual%20concepts%20partially%2C%20completely%2C%20or%20in%20groupings%20-%20a%20truly%20%22freestyle%22%0Aapproach%20-%20without%20the%20need%20for%20a%20purpose-made%20dataset%20%28i.e.%2C%20mask-free%29.%20This%0Aframework%20capitalises%20on%20the%20synergy%20between%20sketch-based%20image%20retrieval%0A%28SBIR%29%20models%20and%20large-scale%20pre-trained%20models%20%28CLIP%20or%20DINOv2%29.%20The%20former%0Aprovides%20an%20effective%20training%20signal%2C%20while%20fine-tuned%20versions%20of%20the%20latter%0Aexecute%20the%20subjective%20segmentation.%20Additionally%2C%20our%20purpose-made%0Aaugmentation%20strategy%20enhances%20the%20versatility%20of%20our%20sketch-guided%20mask%0Ageneration%2C%20allowing%20segmentation%20at%20multiple%20granularity%20levels.%20Extensive%0Aevaluations%20across%20diverse%20benchmark%20datasets%20underscore%20the%20superior%0Aperformance%20of%20our%20method%20in%20comparison%20to%20existing%20approaches%20across%20various%0Aevaluation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreestyle%2520Sketch-in-the-Loop%2520Image%2520Segmentation%26entry.906535625%3DSubhadeep%2520Koley%2520and%2520Viswanatha%2520Reddy%2520Gajjala%2520and%2520Aneeshan%2520Sain%2520and%2520Pinaki%2520Nath%2520Chowdhury%2520and%2520Tao%2520Xiang%2520and%2520Ayan%2520Kumar%2520Bhunia%2520and%2520Yi-Zhe%2520Song%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520expand%2520the%2520domain%2520of%2520sketch%2520research%2520into%2520the%2520field%2520of%250Aimage%2520segmentation%252C%2520aiming%2520to%2520establish%2520freehand%2520sketches%2520as%2520a%2520query%2520modality%250Afor%2520subjective%2520image%2520segmentation.%2520Our%2520innovative%2520approach%2520introduces%2520a%250A%2522sketch-in-the-loop%2522%2520image%2520segmentation%2520framework%252C%2520enabling%2520the%2520segmentation%2520of%250Avisual%2520concepts%2520partially%252C%2520completely%252C%2520or%2520in%2520groupings%2520-%2520a%2520truly%2520%2522freestyle%2522%250Aapproach%2520-%2520without%2520the%2520need%2520for%2520a%2520purpose-made%2520dataset%2520%2528i.e.%252C%2520mask-free%2529.%2520This%250Aframework%2520capitalises%2520on%2520the%2520synergy%2520between%2520sketch-based%2520image%2520retrieval%250A%2528SBIR%2529%2520models%2520and%2520large-scale%2520pre-trained%2520models%2520%2528CLIP%2520or%2520DINOv2%2529.%2520The%2520former%250Aprovides%2520an%2520effective%2520training%2520signal%252C%2520while%2520fine-tuned%2520versions%2520of%2520the%2520latter%250Aexecute%2520the%2520subjective%2520segmentation.%2520Additionally%252C%2520our%2520purpose-made%250Aaugmentation%2520strategy%2520enhances%2520the%2520versatility%2520of%2520our%2520sketch-guided%2520mask%250Ageneration%252C%2520allowing%2520segmentation%2520at%2520multiple%2520granularity%2520levels.%2520Extensive%250Aevaluations%2520across%2520diverse%2520benchmark%2520datasets%2520underscore%2520the%2520superior%250Aperformance%2520of%2520our%2520method%2520in%2520comparison%2520to%2520existing%2520approaches%2520across%2520various%250Aevaluation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Freestyle%20Sketch-in-the-Loop%20Image%20Segmentation&entry.906535625=Subhadeep%20Koley%20and%20Viswanatha%20Reddy%20Gajjala%20and%20Aneeshan%20Sain%20and%20Pinaki%20Nath%20Chowdhury%20and%20Tao%20Xiang%20and%20Ayan%20Kumar%20Bhunia%20and%20Yi-Zhe%20Song&entry.1292438233=%20%20In%20this%20paper%2C%20we%20expand%20the%20domain%20of%20sketch%20research%20into%20the%20field%20of%0Aimage%20segmentation%2C%20aiming%20to%20establish%20freehand%20sketches%20as%20a%20query%20modality%0Afor%20subjective%20image%20segmentation.%20Our%20innovative%20approach%20introduces%20a%0A%22sketch-in-the-loop%22%20image%20segmentation%20framework%2C%20enabling%20the%20segmentation%20of%0Avisual%20concepts%20partially%2C%20completely%2C%20or%20in%20groupings%20-%20a%20truly%20%22freestyle%22%0Aapproach%20-%20without%20the%20need%20for%20a%20purpose-made%20dataset%20%28i.e.%2C%20mask-free%29.%20This%0Aframework%20capitalises%20on%20the%20synergy%20between%20sketch-based%20image%20retrieval%0A%28SBIR%29%20models%20and%20large-scale%20pre-trained%20models%20%28CLIP%20or%20DINOv2%29.%20The%20former%0Aprovides%20an%20effective%20training%20signal%2C%20while%20fine-tuned%20versions%20of%20the%20latter%0Aexecute%20the%20subjective%20segmentation.%20Additionally%2C%20our%20purpose-made%0Aaugmentation%20strategy%20enhances%20the%20versatility%20of%20our%20sketch-guided%20mask%0Ageneration%2C%20allowing%20segmentation%20at%20multiple%20granularity%20levels.%20Extensive%0Aevaluations%20across%20diverse%20benchmark%20datasets%20underscore%20the%20superior%0Aperformance%20of%20our%20method%20in%20comparison%20to%20existing%20approaches%20across%20various%0Aevaluation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16022v1&entry.124074799=Read"},
{"title": "Graph Neural Network Based Hybrid Beamforming Design in Wideband\n  Terahertz MIMO-OFDM Systems", "author": "Beier Li and Mai Vu", "abstract": "  6G wireless technology is projected to adopt higher and wider frequency\nbands, enabled by highly directional beamforming. However, the vast bandwidths\navailable also make the impact of beam squint in massive multiple input and\nmultiple output (MIMO) systems non-negligible. Traditional approaches such as\nadding a true-time-delay line (TTD) on each antenna are costly due to the\nmassive antenna arrays required. This paper puts forth a signal processing\nalternative, specifically adapted to the multicarrier structure of OFDM\nsystems, through an innovative application of Graph Neural Networks (GNNs) to\noptimize hybrid beamforming. By integrating two types of graph nodes to\nrepresent the analog and the digital beamforming matrices efficiently, our\napproach not only reduces the computational and memory burdens but also\nachieves high spectral efficiency performance, approaching that of all digital\nbeamforming. The GNN runtime and memory requirement are at a fraction of the\nprocessing time and resource consumption of traditional signal processing\nmethods, hence enabling real-time adaptation of hybrid beamforming.\nFurthermore, the proposed GNN exhibits strong resiliency to beam squinting,\nachieving almost constant spectral efficiency even as the system bandwidth\nincreases at higher carrier frequencies.\n", "link": "http://arxiv.org/abs/2501.16306v1", "date": "2025-01-27", "relevancy": 2.19, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4511}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4465}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Network%20Based%20Hybrid%20Beamforming%20Design%20in%20Wideband%0A%20%20Terahertz%20MIMO-OFDM%20Systems&body=Title%3A%20Graph%20Neural%20Network%20Based%20Hybrid%20Beamforming%20Design%20in%20Wideband%0A%20%20Terahertz%20MIMO-OFDM%20Systems%0AAuthor%3A%20Beier%20Li%20and%20Mai%20Vu%0AAbstract%3A%20%20%206G%20wireless%20technology%20is%20projected%20to%20adopt%20higher%20and%20wider%20frequency%0Abands%2C%20enabled%20by%20highly%20directional%20beamforming.%20However%2C%20the%20vast%20bandwidths%0Aavailable%20also%20make%20the%20impact%20of%20beam%20squint%20in%20massive%20multiple%20input%20and%0Amultiple%20output%20%28MIMO%29%20systems%20non-negligible.%20Traditional%20approaches%20such%20as%0Aadding%20a%20true-time-delay%20line%20%28TTD%29%20on%20each%20antenna%20are%20costly%20due%20to%20the%0Amassive%20antenna%20arrays%20required.%20This%20paper%20puts%20forth%20a%20signal%20processing%0Aalternative%2C%20specifically%20adapted%20to%20the%20multicarrier%20structure%20of%20OFDM%0Asystems%2C%20through%20an%20innovative%20application%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20to%0Aoptimize%20hybrid%20beamforming.%20By%20integrating%20two%20types%20of%20graph%20nodes%20to%0Arepresent%20the%20analog%20and%20the%20digital%20beamforming%20matrices%20efficiently%2C%20our%0Aapproach%20not%20only%20reduces%20the%20computational%20and%20memory%20burdens%20but%20also%0Aachieves%20high%20spectral%20efficiency%20performance%2C%20approaching%20that%20of%20all%20digital%0Abeamforming.%20The%20GNN%20runtime%20and%20memory%20requirement%20are%20at%20a%20fraction%20of%20the%0Aprocessing%20time%20and%20resource%20consumption%20of%20traditional%20signal%20processing%0Amethods%2C%20hence%20enabling%20real-time%20adaptation%20of%20hybrid%20beamforming.%0AFurthermore%2C%20the%20proposed%20GNN%20exhibits%20strong%20resiliency%20to%20beam%20squinting%2C%0Aachieving%20almost%20constant%20spectral%20efficiency%20even%20as%20the%20system%20bandwidth%0Aincreases%20at%20higher%20carrier%20frequencies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Network%2520Based%2520Hybrid%2520Beamforming%2520Design%2520in%2520Wideband%250A%2520%2520Terahertz%2520MIMO-OFDM%2520Systems%26entry.906535625%3DBeier%2520Li%2520and%2520Mai%2520Vu%26entry.1292438233%3D%2520%25206G%2520wireless%2520technology%2520is%2520projected%2520to%2520adopt%2520higher%2520and%2520wider%2520frequency%250Abands%252C%2520enabled%2520by%2520highly%2520directional%2520beamforming.%2520However%252C%2520the%2520vast%2520bandwidths%250Aavailable%2520also%2520make%2520the%2520impact%2520of%2520beam%2520squint%2520in%2520massive%2520multiple%2520input%2520and%250Amultiple%2520output%2520%2528MIMO%2529%2520systems%2520non-negligible.%2520Traditional%2520approaches%2520such%2520as%250Aadding%2520a%2520true-time-delay%2520line%2520%2528TTD%2529%2520on%2520each%2520antenna%2520are%2520costly%2520due%2520to%2520the%250Amassive%2520antenna%2520arrays%2520required.%2520This%2520paper%2520puts%2520forth%2520a%2520signal%2520processing%250Aalternative%252C%2520specifically%2520adapted%2520to%2520the%2520multicarrier%2520structure%2520of%2520OFDM%250Asystems%252C%2520through%2520an%2520innovative%2520application%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%250Aoptimize%2520hybrid%2520beamforming.%2520By%2520integrating%2520two%2520types%2520of%2520graph%2520nodes%2520to%250Arepresent%2520the%2520analog%2520and%2520the%2520digital%2520beamforming%2520matrices%2520efficiently%252C%2520our%250Aapproach%2520not%2520only%2520reduces%2520the%2520computational%2520and%2520memory%2520burdens%2520but%2520also%250Aachieves%2520high%2520spectral%2520efficiency%2520performance%252C%2520approaching%2520that%2520of%2520all%2520digital%250Abeamforming.%2520The%2520GNN%2520runtime%2520and%2520memory%2520requirement%2520are%2520at%2520a%2520fraction%2520of%2520the%250Aprocessing%2520time%2520and%2520resource%2520consumption%2520of%2520traditional%2520signal%2520processing%250Amethods%252C%2520hence%2520enabling%2520real-time%2520adaptation%2520of%2520hybrid%2520beamforming.%250AFurthermore%252C%2520the%2520proposed%2520GNN%2520exhibits%2520strong%2520resiliency%2520to%2520beam%2520squinting%252C%250Aachieving%2520almost%2520constant%2520spectral%2520efficiency%2520even%2520as%2520the%2520system%2520bandwidth%250Aincreases%2520at%2520higher%2520carrier%2520frequencies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Network%20Based%20Hybrid%20Beamforming%20Design%20in%20Wideband%0A%20%20Terahertz%20MIMO-OFDM%20Systems&entry.906535625=Beier%20Li%20and%20Mai%20Vu&entry.1292438233=%20%206G%20wireless%20technology%20is%20projected%20to%20adopt%20higher%20and%20wider%20frequency%0Abands%2C%20enabled%20by%20highly%20directional%20beamforming.%20However%2C%20the%20vast%20bandwidths%0Aavailable%20also%20make%20the%20impact%20of%20beam%20squint%20in%20massive%20multiple%20input%20and%0Amultiple%20output%20%28MIMO%29%20systems%20non-negligible.%20Traditional%20approaches%20such%20as%0Aadding%20a%20true-time-delay%20line%20%28TTD%29%20on%20each%20antenna%20are%20costly%20due%20to%20the%0Amassive%20antenna%20arrays%20required.%20This%20paper%20puts%20forth%20a%20signal%20processing%0Aalternative%2C%20specifically%20adapted%20to%20the%20multicarrier%20structure%20of%20OFDM%0Asystems%2C%20through%20an%20innovative%20application%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20to%0Aoptimize%20hybrid%20beamforming.%20By%20integrating%20two%20types%20of%20graph%20nodes%20to%0Arepresent%20the%20analog%20and%20the%20digital%20beamforming%20matrices%20efficiently%2C%20our%0Aapproach%20not%20only%20reduces%20the%20computational%20and%20memory%20burdens%20but%20also%0Aachieves%20high%20spectral%20efficiency%20performance%2C%20approaching%20that%20of%20all%20digital%0Abeamforming.%20The%20GNN%20runtime%20and%20memory%20requirement%20are%20at%20a%20fraction%20of%20the%0Aprocessing%20time%20and%20resource%20consumption%20of%20traditional%20signal%20processing%0Amethods%2C%20hence%20enabling%20real-time%20adaptation%20of%20hybrid%20beamforming.%0AFurthermore%2C%20the%20proposed%20GNN%20exhibits%20strong%20resiliency%20to%20beam%20squinting%2C%0Aachieving%20almost%20constant%20spectral%20efficiency%20even%20as%20the%20system%20bandwidth%0Aincreases%20at%20higher%20carrier%20frequencies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16306v1&entry.124074799=Read"},
{"title": "PDC-ViT : Source Camera Identification using Pixel Difference\n  Convolution and Vision Transformer", "author": "Omar Elharrouss and Younes Akbari and Noor Almaadeed and Somaya Al-Maadeed and Fouad Khelifi and Ahmed Bouridane", "abstract": "  Source camera identification has emerged as a vital solution to unlock\nincidents involving critical cases like terrorism, violence, and other criminal\nactivities. The ability to trace the origin of an image/video can aid law\nenforcement agencies in gathering evidence and constructing the timeline of\nevents. Moreover, identifying the owner of a certain device narrows down the\narea of search in a criminal investigation where smartphone devices are\ninvolved. This paper proposes a new pixel-based method for source camera\nidentification, integrating Pixel Difference Convolution (PDC) with a Vision\nTransformer network (ViT), and named PDC-ViT. While the PDC acts as the\nbackbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC\n(RPDC). These techniques enhance the capability to capture subtle variations in\npixel information, which are crucial for distinguishing between different\nsource cameras. The second part of the methodology focuses on classification,\nwhich is based on a Vision Transformer network. Unlike traditional methods that\nutilize image patches directly for training the classification network, the\nproposed approach uniquely inputs PDC features into the Vision Transformer\nnetwork. To demonstrate the effectiveness of the PDC-ViT approach, it has been\nassessed on five different datasets, which include various image contents and\nvideo scenes. The method has also been compared with state-of-the-art source\ncamera identification methods. Experimental results demonstrate the\neffectiveness and superiority of the proposed system in terms of accuracy and\nrobustness when compared to its competitors. For example, our proposed PDC-ViT\nhas achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision\ndataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively.\n", "link": "http://arxiv.org/abs/2501.16227v1", "date": "2025-01-27", "relevancy": 2.1892, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5734}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PDC-ViT%20%3A%20Source%20Camera%20Identification%20using%20Pixel%20Difference%0A%20%20Convolution%20and%20Vision%20Transformer&body=Title%3A%20PDC-ViT%20%3A%20Source%20Camera%20Identification%20using%20Pixel%20Difference%0A%20%20Convolution%20and%20Vision%20Transformer%0AAuthor%3A%20Omar%20Elharrouss%20and%20Younes%20Akbari%20and%20Noor%20Almaadeed%20and%20Somaya%20Al-Maadeed%20and%20Fouad%20Khelifi%20and%20Ahmed%20Bouridane%0AAbstract%3A%20%20%20Source%20camera%20identification%20has%20emerged%20as%20a%20vital%20solution%20to%20unlock%0Aincidents%20involving%20critical%20cases%20like%20terrorism%2C%20violence%2C%20and%20other%20criminal%0Aactivities.%20The%20ability%20to%20trace%20the%20origin%20of%20an%20image/video%20can%20aid%20law%0Aenforcement%20agencies%20in%20gathering%20evidence%20and%20constructing%20the%20timeline%20of%0Aevents.%20Moreover%2C%20identifying%20the%20owner%20of%20a%20certain%20device%20narrows%20down%20the%0Aarea%20of%20search%20in%20a%20criminal%20investigation%20where%20smartphone%20devices%20are%0Ainvolved.%20This%20paper%20proposes%20a%20new%20pixel-based%20method%20for%20source%20camera%0Aidentification%2C%20integrating%20Pixel%20Difference%20Convolution%20%28PDC%29%20with%20a%20Vision%0ATransformer%20network%20%28ViT%29%2C%20and%20named%20PDC-ViT.%20While%20the%20PDC%20acts%20as%20the%0Abackbone%20for%20feature%20extraction%20by%20exploiting%20Angular%20PDC%20%28APDC%29%20and%20Radial%20PDC%0A%28RPDC%29.%20These%20techniques%20enhance%20the%20capability%20to%20capture%20subtle%20variations%20in%0Apixel%20information%2C%20which%20are%20crucial%20for%20distinguishing%20between%20different%0Asource%20cameras.%20The%20second%20part%20of%20the%20methodology%20focuses%20on%20classification%2C%0Awhich%20is%20based%20on%20a%20Vision%20Transformer%20network.%20Unlike%20traditional%20methods%20that%0Autilize%20image%20patches%20directly%20for%20training%20the%20classification%20network%2C%20the%0Aproposed%20approach%20uniquely%20inputs%20PDC%20features%20into%20the%20Vision%20Transformer%0Anetwork.%20To%20demonstrate%20the%20effectiveness%20of%20the%20PDC-ViT%20approach%2C%20it%20has%20been%0Aassessed%20on%20five%20different%20datasets%2C%20which%20include%20various%20image%20contents%20and%0Avideo%20scenes.%20The%20method%20has%20also%20been%20compared%20with%20state-of-the-art%20source%0Acamera%20identification%20methods.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20and%20superiority%20of%20the%20proposed%20system%20in%20terms%20of%20accuracy%20and%0Arobustness%20when%20compared%20to%20its%20competitors.%20For%20example%2C%20our%20proposed%20PDC-ViT%0Ahas%20achieved%20an%20accuracy%20of%2094.30%25%2C%2084%25%2C%2094.22%25%20and%2092.29%25%20using%20the%20Vision%0Adataset%2C%20Daxing%20dataset%2C%20Socrates%20dataset%20and%20QUFVD%20dataset%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPDC-ViT%2520%253A%2520Source%2520Camera%2520Identification%2520using%2520Pixel%2520Difference%250A%2520%2520Convolution%2520and%2520Vision%2520Transformer%26entry.906535625%3DOmar%2520Elharrouss%2520and%2520Younes%2520Akbari%2520and%2520Noor%2520Almaadeed%2520and%2520Somaya%2520Al-Maadeed%2520and%2520Fouad%2520Khelifi%2520and%2520Ahmed%2520Bouridane%26entry.1292438233%3D%2520%2520Source%2520camera%2520identification%2520has%2520emerged%2520as%2520a%2520vital%2520solution%2520to%2520unlock%250Aincidents%2520involving%2520critical%2520cases%2520like%2520terrorism%252C%2520violence%252C%2520and%2520other%2520criminal%250Aactivities.%2520The%2520ability%2520to%2520trace%2520the%2520origin%2520of%2520an%2520image/video%2520can%2520aid%2520law%250Aenforcement%2520agencies%2520in%2520gathering%2520evidence%2520and%2520constructing%2520the%2520timeline%2520of%250Aevents.%2520Moreover%252C%2520identifying%2520the%2520owner%2520of%2520a%2520certain%2520device%2520narrows%2520down%2520the%250Aarea%2520of%2520search%2520in%2520a%2520criminal%2520investigation%2520where%2520smartphone%2520devices%2520are%250Ainvolved.%2520This%2520paper%2520proposes%2520a%2520new%2520pixel-based%2520method%2520for%2520source%2520camera%250Aidentification%252C%2520integrating%2520Pixel%2520Difference%2520Convolution%2520%2528PDC%2529%2520with%2520a%2520Vision%250ATransformer%2520network%2520%2528ViT%2529%252C%2520and%2520named%2520PDC-ViT.%2520While%2520the%2520PDC%2520acts%2520as%2520the%250Abackbone%2520for%2520feature%2520extraction%2520by%2520exploiting%2520Angular%2520PDC%2520%2528APDC%2529%2520and%2520Radial%2520PDC%250A%2528RPDC%2529.%2520These%2520techniques%2520enhance%2520the%2520capability%2520to%2520capture%2520subtle%2520variations%2520in%250Apixel%2520information%252C%2520which%2520are%2520crucial%2520for%2520distinguishing%2520between%2520different%250Asource%2520cameras.%2520The%2520second%2520part%2520of%2520the%2520methodology%2520focuses%2520on%2520classification%252C%250Awhich%2520is%2520based%2520on%2520a%2520Vision%2520Transformer%2520network.%2520Unlike%2520traditional%2520methods%2520that%250Autilize%2520image%2520patches%2520directly%2520for%2520training%2520the%2520classification%2520network%252C%2520the%250Aproposed%2520approach%2520uniquely%2520inputs%2520PDC%2520features%2520into%2520the%2520Vision%2520Transformer%250Anetwork.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520PDC-ViT%2520approach%252C%2520it%2520has%2520been%250Aassessed%2520on%2520five%2520different%2520datasets%252C%2520which%2520include%2520various%2520image%2520contents%2520and%250Avideo%2520scenes.%2520The%2520method%2520has%2520also%2520been%2520compared%2520with%2520state-of-the-art%2520source%250Acamera%2520identification%2520methods.%2520Experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520and%2520superiority%2520of%2520the%2520proposed%2520system%2520in%2520terms%2520of%2520accuracy%2520and%250Arobustness%2520when%2520compared%2520to%2520its%2520competitors.%2520For%2520example%252C%2520our%2520proposed%2520PDC-ViT%250Ahas%2520achieved%2520an%2520accuracy%2520of%252094.30%2525%252C%252084%2525%252C%252094.22%2525%2520and%252092.29%2525%2520using%2520the%2520Vision%250Adataset%252C%2520Daxing%2520dataset%252C%2520Socrates%2520dataset%2520and%2520QUFVD%2520dataset%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PDC-ViT%20%3A%20Source%20Camera%20Identification%20using%20Pixel%20Difference%0A%20%20Convolution%20and%20Vision%20Transformer&entry.906535625=Omar%20Elharrouss%20and%20Younes%20Akbari%20and%20Noor%20Almaadeed%20and%20Somaya%20Al-Maadeed%20and%20Fouad%20Khelifi%20and%20Ahmed%20Bouridane&entry.1292438233=%20%20Source%20camera%20identification%20has%20emerged%20as%20a%20vital%20solution%20to%20unlock%0Aincidents%20involving%20critical%20cases%20like%20terrorism%2C%20violence%2C%20and%20other%20criminal%0Aactivities.%20The%20ability%20to%20trace%20the%20origin%20of%20an%20image/video%20can%20aid%20law%0Aenforcement%20agencies%20in%20gathering%20evidence%20and%20constructing%20the%20timeline%20of%0Aevents.%20Moreover%2C%20identifying%20the%20owner%20of%20a%20certain%20device%20narrows%20down%20the%0Aarea%20of%20search%20in%20a%20criminal%20investigation%20where%20smartphone%20devices%20are%0Ainvolved.%20This%20paper%20proposes%20a%20new%20pixel-based%20method%20for%20source%20camera%0Aidentification%2C%20integrating%20Pixel%20Difference%20Convolution%20%28PDC%29%20with%20a%20Vision%0ATransformer%20network%20%28ViT%29%2C%20and%20named%20PDC-ViT.%20While%20the%20PDC%20acts%20as%20the%0Abackbone%20for%20feature%20extraction%20by%20exploiting%20Angular%20PDC%20%28APDC%29%20and%20Radial%20PDC%0A%28RPDC%29.%20These%20techniques%20enhance%20the%20capability%20to%20capture%20subtle%20variations%20in%0Apixel%20information%2C%20which%20are%20crucial%20for%20distinguishing%20between%20different%0Asource%20cameras.%20The%20second%20part%20of%20the%20methodology%20focuses%20on%20classification%2C%0Awhich%20is%20based%20on%20a%20Vision%20Transformer%20network.%20Unlike%20traditional%20methods%20that%0Autilize%20image%20patches%20directly%20for%20training%20the%20classification%20network%2C%20the%0Aproposed%20approach%20uniquely%20inputs%20PDC%20features%20into%20the%20Vision%20Transformer%0Anetwork.%20To%20demonstrate%20the%20effectiveness%20of%20the%20PDC-ViT%20approach%2C%20it%20has%20been%0Aassessed%20on%20five%20different%20datasets%2C%20which%20include%20various%20image%20contents%20and%0Avideo%20scenes.%20The%20method%20has%20also%20been%20compared%20with%20state-of-the-art%20source%0Acamera%20identification%20methods.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20and%20superiority%20of%20the%20proposed%20system%20in%20terms%20of%20accuracy%20and%0Arobustness%20when%20compared%20to%20its%20competitors.%20For%20example%2C%20our%20proposed%20PDC-ViT%0Ahas%20achieved%20an%20accuracy%20of%2094.30%25%2C%2084%25%2C%2094.22%25%20and%2092.29%25%20using%20the%20Vision%0Adataset%2C%20Daxing%20dataset%2C%20Socrates%20dataset%20and%20QUFVD%20dataset%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16227v1&entry.124074799=Read"},
{"title": "AI Agents for Computer Use: A Review of Instruction-based Computer\n  Control, GUI Automation, and Operator Assistants", "author": "Pascal J. Sager and Benjamin Meyer and Peng Yan and Rebekka von Wartburg-Kottler and Layan Etaiwi and Aref Enayati and Gabriel Nobel and Ahmed Abdulkadir and Benjamin F. Grewe and Thilo Stadelmann", "abstract": "  Instruction-based computer control agents (CCAs) execute complex action\nsequences on personal computers or mobile devices to fulfill tasks using the\nsame graphical user interfaces as a human user would, provided instructions in\nnatural language. This review offers a comprehensive overview of the emerging\nfield of instruction-based computer control, examining available agents --\ntheir taxonomy, development, and respective resources -- and emphasizing the\nshift from manually designed, specialized agents to leveraging foundation\nmodels such as large language models (LLMs) and vision-language models (VLMs).\nWe formalize the problem and establish a taxonomy of the field to analyze\nagents from three perspectives: (a) the environment perspective, analyzing\ncomputer environments; (b) the interaction perspective, describing observations\nspaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard\nactions, executable code); and (c) the agent perspective, focusing on the core\nprinciple of how an agent acts and learns to act. Our framework encompasses\nboth specialized and foundation agents, facilitating their comparative analysis\nand revealing how prior solutions in specialized agents, such as an environment\nlearning step, can guide the development of more capable foundation agents.\nAdditionally, we review current CCA datasets and CCA evaluation methods and\noutline the challenges to deploying such agents in a productive setting. In\ntotal, we review and classify 86 CCAs and 33 related datasets. By highlighting\ntrends, limitations, and future research directions, this work presents a\ncomprehensive foundation to obtain a broad understanding of the field and push\nits future development.\n", "link": "http://arxiv.org/abs/2501.16150v1", "date": "2025-01-27", "relevancy": 2.1885, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5674}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5642}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Agents%20for%20Computer%20Use%3A%20A%20Review%20of%20Instruction-based%20Computer%0A%20%20Control%2C%20GUI%20Automation%2C%20and%20Operator%20Assistants&body=Title%3A%20AI%20Agents%20for%20Computer%20Use%3A%20A%20Review%20of%20Instruction-based%20Computer%0A%20%20Control%2C%20GUI%20Automation%2C%20and%20Operator%20Assistants%0AAuthor%3A%20Pascal%20J.%20Sager%20and%20Benjamin%20Meyer%20and%20Peng%20Yan%20and%20Rebekka%20von%20Wartburg-Kottler%20and%20Layan%20Etaiwi%20and%20Aref%20Enayati%20and%20Gabriel%20Nobel%20and%20Ahmed%20Abdulkadir%20and%20Benjamin%20F.%20Grewe%20and%20Thilo%20Stadelmann%0AAbstract%3A%20%20%20Instruction-based%20computer%20control%20agents%20%28CCAs%29%20execute%20complex%20action%0Asequences%20on%20personal%20computers%20or%20mobile%20devices%20to%20fulfill%20tasks%20using%20the%0Asame%20graphical%20user%20interfaces%20as%20a%20human%20user%20would%2C%20provided%20instructions%20in%0Anatural%20language.%20This%20review%20offers%20a%20comprehensive%20overview%20of%20the%20emerging%0Afield%20of%20instruction-based%20computer%20control%2C%20examining%20available%20agents%20--%0Atheir%20taxonomy%2C%20development%2C%20and%20respective%20resources%20--%20and%20emphasizing%20the%0Ashift%20from%20manually%20designed%2C%20specialized%20agents%20to%20leveraging%20foundation%0Amodels%20such%20as%20large%20language%20models%20%28LLMs%29%20and%20vision-language%20models%20%28VLMs%29.%0AWe%20formalize%20the%20problem%20and%20establish%20a%20taxonomy%20of%20the%20field%20to%20analyze%0Aagents%20from%20three%20perspectives%3A%20%28a%29%20the%20environment%20perspective%2C%20analyzing%0Acomputer%20environments%3B%20%28b%29%20the%20interaction%20perspective%2C%20describing%20observations%0Aspaces%20%28e.g.%2C%20screenshots%2C%20HTML%29%20and%20action%20spaces%20%28e.g.%2C%20mouse%20and%20keyboard%0Aactions%2C%20executable%20code%29%3B%20and%20%28c%29%20the%20agent%20perspective%2C%20focusing%20on%20the%20core%0Aprinciple%20of%20how%20an%20agent%20acts%20and%20learns%20to%20act.%20Our%20framework%20encompasses%0Aboth%20specialized%20and%20foundation%20agents%2C%20facilitating%20their%20comparative%20analysis%0Aand%20revealing%20how%20prior%20solutions%20in%20specialized%20agents%2C%20such%20as%20an%20environment%0Alearning%20step%2C%20can%20guide%20the%20development%20of%20more%20capable%20foundation%20agents.%0AAdditionally%2C%20we%20review%20current%20CCA%20datasets%20and%20CCA%20evaluation%20methods%20and%0Aoutline%20the%20challenges%20to%20deploying%20such%20agents%20in%20a%20productive%20setting.%20In%0Atotal%2C%20we%20review%20and%20classify%2086%20CCAs%20and%2033%20related%20datasets.%20By%20highlighting%0Atrends%2C%20limitations%2C%20and%20future%20research%20directions%2C%20this%20work%20presents%20a%0Acomprehensive%20foundation%20to%20obtain%20a%20broad%20understanding%20of%20the%20field%20and%20push%0Aits%20future%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Agents%2520for%2520Computer%2520Use%253A%2520A%2520Review%2520of%2520Instruction-based%2520Computer%250A%2520%2520Control%252C%2520GUI%2520Automation%252C%2520and%2520Operator%2520Assistants%26entry.906535625%3DPascal%2520J.%2520Sager%2520and%2520Benjamin%2520Meyer%2520and%2520Peng%2520Yan%2520and%2520Rebekka%2520von%2520Wartburg-Kottler%2520and%2520Layan%2520Etaiwi%2520and%2520Aref%2520Enayati%2520and%2520Gabriel%2520Nobel%2520and%2520Ahmed%2520Abdulkadir%2520and%2520Benjamin%2520F.%2520Grewe%2520and%2520Thilo%2520Stadelmann%26entry.1292438233%3D%2520%2520Instruction-based%2520computer%2520control%2520agents%2520%2528CCAs%2529%2520execute%2520complex%2520action%250Asequences%2520on%2520personal%2520computers%2520or%2520mobile%2520devices%2520to%2520fulfill%2520tasks%2520using%2520the%250Asame%2520graphical%2520user%2520interfaces%2520as%2520a%2520human%2520user%2520would%252C%2520provided%2520instructions%2520in%250Anatural%2520language.%2520This%2520review%2520offers%2520a%2520comprehensive%2520overview%2520of%2520the%2520emerging%250Afield%2520of%2520instruction-based%2520computer%2520control%252C%2520examining%2520available%2520agents%2520--%250Atheir%2520taxonomy%252C%2520development%252C%2520and%2520respective%2520resources%2520--%2520and%2520emphasizing%2520the%250Ashift%2520from%2520manually%2520designed%252C%2520specialized%2520agents%2520to%2520leveraging%2520foundation%250Amodels%2520such%2520as%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520vision-language%2520models%2520%2528VLMs%2529.%250AWe%2520formalize%2520the%2520problem%2520and%2520establish%2520a%2520taxonomy%2520of%2520the%2520field%2520to%2520analyze%250Aagents%2520from%2520three%2520perspectives%253A%2520%2528a%2529%2520the%2520environment%2520perspective%252C%2520analyzing%250Acomputer%2520environments%253B%2520%2528b%2529%2520the%2520interaction%2520perspective%252C%2520describing%2520observations%250Aspaces%2520%2528e.g.%252C%2520screenshots%252C%2520HTML%2529%2520and%2520action%2520spaces%2520%2528e.g.%252C%2520mouse%2520and%2520keyboard%250Aactions%252C%2520executable%2520code%2529%253B%2520and%2520%2528c%2529%2520the%2520agent%2520perspective%252C%2520focusing%2520on%2520the%2520core%250Aprinciple%2520of%2520how%2520an%2520agent%2520acts%2520and%2520learns%2520to%2520act.%2520Our%2520framework%2520encompasses%250Aboth%2520specialized%2520and%2520foundation%2520agents%252C%2520facilitating%2520their%2520comparative%2520analysis%250Aand%2520revealing%2520how%2520prior%2520solutions%2520in%2520specialized%2520agents%252C%2520such%2520as%2520an%2520environment%250Alearning%2520step%252C%2520can%2520guide%2520the%2520development%2520of%2520more%2520capable%2520foundation%2520agents.%250AAdditionally%252C%2520we%2520review%2520current%2520CCA%2520datasets%2520and%2520CCA%2520evaluation%2520methods%2520and%250Aoutline%2520the%2520challenges%2520to%2520deploying%2520such%2520agents%2520in%2520a%2520productive%2520setting.%2520In%250Atotal%252C%2520we%2520review%2520and%2520classify%252086%2520CCAs%2520and%252033%2520related%2520datasets.%2520By%2520highlighting%250Atrends%252C%2520limitations%252C%2520and%2520future%2520research%2520directions%252C%2520this%2520work%2520presents%2520a%250Acomprehensive%2520foundation%2520to%2520obtain%2520a%2520broad%2520understanding%2520of%2520the%2520field%2520and%2520push%250Aits%2520future%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Agents%20for%20Computer%20Use%3A%20A%20Review%20of%20Instruction-based%20Computer%0A%20%20Control%2C%20GUI%20Automation%2C%20and%20Operator%20Assistants&entry.906535625=Pascal%20J.%20Sager%20and%20Benjamin%20Meyer%20and%20Peng%20Yan%20and%20Rebekka%20von%20Wartburg-Kottler%20and%20Layan%20Etaiwi%20and%20Aref%20Enayati%20and%20Gabriel%20Nobel%20and%20Ahmed%20Abdulkadir%20and%20Benjamin%20F.%20Grewe%20and%20Thilo%20Stadelmann&entry.1292438233=%20%20Instruction-based%20computer%20control%20agents%20%28CCAs%29%20execute%20complex%20action%0Asequences%20on%20personal%20computers%20or%20mobile%20devices%20to%20fulfill%20tasks%20using%20the%0Asame%20graphical%20user%20interfaces%20as%20a%20human%20user%20would%2C%20provided%20instructions%20in%0Anatural%20language.%20This%20review%20offers%20a%20comprehensive%20overview%20of%20the%20emerging%0Afield%20of%20instruction-based%20computer%20control%2C%20examining%20available%20agents%20--%0Atheir%20taxonomy%2C%20development%2C%20and%20respective%20resources%20--%20and%20emphasizing%20the%0Ashift%20from%20manually%20designed%2C%20specialized%20agents%20to%20leveraging%20foundation%0Amodels%20such%20as%20large%20language%20models%20%28LLMs%29%20and%20vision-language%20models%20%28VLMs%29.%0AWe%20formalize%20the%20problem%20and%20establish%20a%20taxonomy%20of%20the%20field%20to%20analyze%0Aagents%20from%20three%20perspectives%3A%20%28a%29%20the%20environment%20perspective%2C%20analyzing%0Acomputer%20environments%3B%20%28b%29%20the%20interaction%20perspective%2C%20describing%20observations%0Aspaces%20%28e.g.%2C%20screenshots%2C%20HTML%29%20and%20action%20spaces%20%28e.g.%2C%20mouse%20and%20keyboard%0Aactions%2C%20executable%20code%29%3B%20and%20%28c%29%20the%20agent%20perspective%2C%20focusing%20on%20the%20core%0Aprinciple%20of%20how%20an%20agent%20acts%20and%20learns%20to%20act.%20Our%20framework%20encompasses%0Aboth%20specialized%20and%20foundation%20agents%2C%20facilitating%20their%20comparative%20analysis%0Aand%20revealing%20how%20prior%20solutions%20in%20specialized%20agents%2C%20such%20as%20an%20environment%0Alearning%20step%2C%20can%20guide%20the%20development%20of%20more%20capable%20foundation%20agents.%0AAdditionally%2C%20we%20review%20current%20CCA%20datasets%20and%20CCA%20evaluation%20methods%20and%0Aoutline%20the%20challenges%20to%20deploying%20such%20agents%20in%20a%20productive%20setting.%20In%0Atotal%2C%20we%20review%20and%20classify%2086%20CCAs%20and%2033%20related%20datasets.%20By%20highlighting%0Atrends%2C%20limitations%2C%20and%20future%20research%20directions%2C%20this%20work%20presents%20a%0Acomprehensive%20foundation%20to%20obtain%20a%20broad%20understanding%20of%20the%20field%20and%20push%0Aits%20future%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16150v1&entry.124074799=Read"},
{"title": "GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous\n  Exploration", "author": "Yue Fan and Handong Zhao and Ruiyi Zhang and Yu Shen and Xin Eric Wang and Gang Wu", "abstract": "  Graphical User Interface (GUI) action grounding is a critical step in GUI\nautomation that maps language instructions to actionable elements on GUI\nscreens. Most recent works of GUI action grounding leverage large GUI datasets\nto fine-tune MLLMs. However, the fine-tuning data always covers limited GUI\nenvironments, and we find the performance of the resulting model deteriorates\nin novel environments. We argue that the GUI grounding models should be further\naligned to the novel environments to reveal their full potential, when the\ninference is known to involve novel environments, i.e., environments not used\nduring the previous fine-tuning. To realize this, we first propose GUI-Bee, an\nMLLM-based autonomous agent, to collect high-quality, environment-specific data\nthrough exploration and then continuously fine-tune GUI grounding models with\nthe collected data. Our agent leverages a novel Q-value-Incentive In-Context\nReinforcement Learning (Q-ICRL) method to optimize exploration efficiency and\ndata quality. Additionally, we introduce NovelScreenSpot, a benchmark for\ntesting how well the data can help align GUI action grounding models to novel\nenvironments and demonstrate the effectiveness of data collected by GUI-Bee in\nthe experiments. Furthermore, we conduct an ablation study to validate the\nQ-ICRL method in enhancing the efficiency of GUI-Bee. Project page:\nhttps://gui-bee.github.io\n", "link": "http://arxiv.org/abs/2501.13896v2", "date": "2025-01-27", "relevancy": 2.181, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.593}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5627}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI-Bee%3A%20Align%20GUI%20Action%20Grounding%20to%20Novel%20Environments%20via%20Autonomous%0A%20%20Exploration&body=Title%3A%20GUI-Bee%3A%20Align%20GUI%20Action%20Grounding%20to%20Novel%20Environments%20via%20Autonomous%0A%20%20Exploration%0AAuthor%3A%20Yue%20Fan%20and%20Handong%20Zhao%20and%20Ruiyi%20Zhang%20and%20Yu%20Shen%20and%20Xin%20Eric%20Wang%20and%20Gang%20Wu%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28GUI%29%20action%20grounding%20is%20a%20critical%20step%20in%20GUI%0Aautomation%20that%20maps%20language%20instructions%20to%20actionable%20elements%20on%20GUI%0Ascreens.%20Most%20recent%20works%20of%20GUI%20action%20grounding%20leverage%20large%20GUI%20datasets%0Ato%20fine-tune%20MLLMs.%20However%2C%20the%20fine-tuning%20data%20always%20covers%20limited%20GUI%0Aenvironments%2C%20and%20we%20find%20the%20performance%20of%20the%20resulting%20model%20deteriorates%0Ain%20novel%20environments.%20We%20argue%20that%20the%20GUI%20grounding%20models%20should%20be%20further%0Aaligned%20to%20the%20novel%20environments%20to%20reveal%20their%20full%20potential%2C%20when%20the%0Ainference%20is%20known%20to%20involve%20novel%20environments%2C%20i.e.%2C%20environments%20not%20used%0Aduring%20the%20previous%20fine-tuning.%20To%20realize%20this%2C%20we%20first%20propose%20GUI-Bee%2C%20an%0AMLLM-based%20autonomous%20agent%2C%20to%20collect%20high-quality%2C%20environment-specific%20data%0Athrough%20exploration%20and%20then%20continuously%20fine-tune%20GUI%20grounding%20models%20with%0Athe%20collected%20data.%20Our%20agent%20leverages%20a%20novel%20Q-value-Incentive%20In-Context%0AReinforcement%20Learning%20%28Q-ICRL%29%20method%20to%20optimize%20exploration%20efficiency%20and%0Adata%20quality.%20Additionally%2C%20we%20introduce%20NovelScreenSpot%2C%20a%20benchmark%20for%0Atesting%20how%20well%20the%20data%20can%20help%20align%20GUI%20action%20grounding%20models%20to%20novel%0Aenvironments%20and%20demonstrate%20the%20effectiveness%20of%20data%20collected%20by%20GUI-Bee%20in%0Athe%20experiments.%20Furthermore%2C%20we%20conduct%20an%20ablation%20study%20to%20validate%20the%0AQ-ICRL%20method%20in%20enhancing%20the%20efficiency%20of%20GUI-Bee.%20Project%20page%3A%0Ahttps%3A//gui-bee.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI-Bee%253A%2520Align%2520GUI%2520Action%2520Grounding%2520to%2520Novel%2520Environments%2520via%2520Autonomous%250A%2520%2520Exploration%26entry.906535625%3DYue%2520Fan%2520and%2520Handong%2520Zhao%2520and%2520Ruiyi%2520Zhang%2520and%2520Yu%2520Shen%2520and%2520Xin%2520Eric%2520Wang%2520and%2520Gang%2520Wu%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520action%2520grounding%2520is%2520a%2520critical%2520step%2520in%2520GUI%250Aautomation%2520that%2520maps%2520language%2520instructions%2520to%2520actionable%2520elements%2520on%2520GUI%250Ascreens.%2520Most%2520recent%2520works%2520of%2520GUI%2520action%2520grounding%2520leverage%2520large%2520GUI%2520datasets%250Ato%2520fine-tune%2520MLLMs.%2520However%252C%2520the%2520fine-tuning%2520data%2520always%2520covers%2520limited%2520GUI%250Aenvironments%252C%2520and%2520we%2520find%2520the%2520performance%2520of%2520the%2520resulting%2520model%2520deteriorates%250Ain%2520novel%2520environments.%2520We%2520argue%2520that%2520the%2520GUI%2520grounding%2520models%2520should%2520be%2520further%250Aaligned%2520to%2520the%2520novel%2520environments%2520to%2520reveal%2520their%2520full%2520potential%252C%2520when%2520the%250Ainference%2520is%2520known%2520to%2520involve%2520novel%2520environments%252C%2520i.e.%252C%2520environments%2520not%2520used%250Aduring%2520the%2520previous%2520fine-tuning.%2520To%2520realize%2520this%252C%2520we%2520first%2520propose%2520GUI-Bee%252C%2520an%250AMLLM-based%2520autonomous%2520agent%252C%2520to%2520collect%2520high-quality%252C%2520environment-specific%2520data%250Athrough%2520exploration%2520and%2520then%2520continuously%2520fine-tune%2520GUI%2520grounding%2520models%2520with%250Athe%2520collected%2520data.%2520Our%2520agent%2520leverages%2520a%2520novel%2520Q-value-Incentive%2520In-Context%250AReinforcement%2520Learning%2520%2528Q-ICRL%2529%2520method%2520to%2520optimize%2520exploration%2520efficiency%2520and%250Adata%2520quality.%2520Additionally%252C%2520we%2520introduce%2520NovelScreenSpot%252C%2520a%2520benchmark%2520for%250Atesting%2520how%2520well%2520the%2520data%2520can%2520help%2520align%2520GUI%2520action%2520grounding%2520models%2520to%2520novel%250Aenvironments%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520data%2520collected%2520by%2520GUI-Bee%2520in%250Athe%2520experiments.%2520Furthermore%252C%2520we%2520conduct%2520an%2520ablation%2520study%2520to%2520validate%2520the%250AQ-ICRL%2520method%2520in%2520enhancing%2520the%2520efficiency%2520of%2520GUI-Bee.%2520Project%2520page%253A%250Ahttps%253A//gui-bee.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI-Bee%3A%20Align%20GUI%20Action%20Grounding%20to%20Novel%20Environments%20via%20Autonomous%0A%20%20Exploration&entry.906535625=Yue%20Fan%20and%20Handong%20Zhao%20and%20Ruiyi%20Zhang%20and%20Yu%20Shen%20and%20Xin%20Eric%20Wang%20and%20Gang%20Wu&entry.1292438233=%20%20Graphical%20User%20Interface%20%28GUI%29%20action%20grounding%20is%20a%20critical%20step%20in%20GUI%0Aautomation%20that%20maps%20language%20instructions%20to%20actionable%20elements%20on%20GUI%0Ascreens.%20Most%20recent%20works%20of%20GUI%20action%20grounding%20leverage%20large%20GUI%20datasets%0Ato%20fine-tune%20MLLMs.%20However%2C%20the%20fine-tuning%20data%20always%20covers%20limited%20GUI%0Aenvironments%2C%20and%20we%20find%20the%20performance%20of%20the%20resulting%20model%20deteriorates%0Ain%20novel%20environments.%20We%20argue%20that%20the%20GUI%20grounding%20models%20should%20be%20further%0Aaligned%20to%20the%20novel%20environments%20to%20reveal%20their%20full%20potential%2C%20when%20the%0Ainference%20is%20known%20to%20involve%20novel%20environments%2C%20i.e.%2C%20environments%20not%20used%0Aduring%20the%20previous%20fine-tuning.%20To%20realize%20this%2C%20we%20first%20propose%20GUI-Bee%2C%20an%0AMLLM-based%20autonomous%20agent%2C%20to%20collect%20high-quality%2C%20environment-specific%20data%0Athrough%20exploration%20and%20then%20continuously%20fine-tune%20GUI%20grounding%20models%20with%0Athe%20collected%20data.%20Our%20agent%20leverages%20a%20novel%20Q-value-Incentive%20In-Context%0AReinforcement%20Learning%20%28Q-ICRL%29%20method%20to%20optimize%20exploration%20efficiency%20and%0Adata%20quality.%20Additionally%2C%20we%20introduce%20NovelScreenSpot%2C%20a%20benchmark%20for%0Atesting%20how%20well%20the%20data%20can%20help%20align%20GUI%20action%20grounding%20models%20to%20novel%0Aenvironments%20and%20demonstrate%20the%20effectiveness%20of%20data%20collected%20by%20GUI-Bee%20in%0Athe%20experiments.%20Furthermore%2C%20we%20conduct%20an%20ablation%20study%20to%20validate%20the%0AQ-ICRL%20method%20in%20enhancing%20the%20efficiency%20of%20GUI-Bee.%20Project%20page%3A%0Ahttps%3A//gui-bee.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13896v2&entry.124074799=Read"},
{"title": "Domain Adaptation-Enhanced Searchlight: Enabling classification of brain\n  states from visual perception to mental imagery", "author": "Alexander Olza and David Soto and Roberto Santana", "abstract": "  In cognitive neuroscience and brain-computer interface research, accurately\npredicting imagined stimuli is crucial. This study investigates the\neffectiveness of Domain Adaptation (DA) in enhancing imagery prediction using\nprimarily visual data from fMRI scans of 18 subjects. Initially, we train a\nbaseline model on visual stimuli to predict imagined stimuli, utilizing data\nfrom 14 brain regions. We then develop several models to improve imagery\nprediction, comparing different DA methods. Our results demonstrate that DA\nsignificantly enhances imagery prediction in binary classification on our\ndataset, as well as in multiclass classification on a publicly available\ndataset. We then conduct a DA-enhanced searchlight analysis, followed by\npermutation-based statistical tests to identify brain regions where imagery\ndecoding is consistently above chance across subjects. Our DA-enhanced\nsearchlight predicts imagery contents in a highly distributed set of brain\nregions, including the visual cortex and the frontoparietal cortex, thereby\noutperforming standard cross-domain classification methods. The complete code\nand data for this paper have been made openly available for the use of the\nscientific community.\n", "link": "http://arxiv.org/abs/2408.01163v3", "date": "2025-01-27", "relevancy": 2.1794, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Adaptation-Enhanced%20Searchlight%3A%20Enabling%20classification%20of%20brain%0A%20%20states%20from%20visual%20perception%20to%20mental%20imagery&body=Title%3A%20Domain%20Adaptation-Enhanced%20Searchlight%3A%20Enabling%20classification%20of%20brain%0A%20%20states%20from%20visual%20perception%20to%20mental%20imagery%0AAuthor%3A%20Alexander%20Olza%20and%20David%20Soto%20and%20Roberto%20Santana%0AAbstract%3A%20%20%20In%20cognitive%20neuroscience%20and%20brain-computer%20interface%20research%2C%20accurately%0Apredicting%20imagined%20stimuli%20is%20crucial.%20This%20study%20investigates%20the%0Aeffectiveness%20of%20Domain%20Adaptation%20%28DA%29%20in%20enhancing%20imagery%20prediction%20using%0Aprimarily%20visual%20data%20from%20fMRI%20scans%20of%2018%20subjects.%20Initially%2C%20we%20train%20a%0Abaseline%20model%20on%20visual%20stimuli%20to%20predict%20imagined%20stimuli%2C%20utilizing%20data%0Afrom%2014%20brain%20regions.%20We%20then%20develop%20several%20models%20to%20improve%20imagery%0Aprediction%2C%20comparing%20different%20DA%20methods.%20Our%20results%20demonstrate%20that%20DA%0Asignificantly%20enhances%20imagery%20prediction%20in%20binary%20classification%20on%20our%0Adataset%2C%20as%20well%20as%20in%20multiclass%20classification%20on%20a%20publicly%20available%0Adataset.%20We%20then%20conduct%20a%20DA-enhanced%20searchlight%20analysis%2C%20followed%20by%0Apermutation-based%20statistical%20tests%20to%20identify%20brain%20regions%20where%20imagery%0Adecoding%20is%20consistently%20above%20chance%20across%20subjects.%20Our%20DA-enhanced%0Asearchlight%20predicts%20imagery%20contents%20in%20a%20highly%20distributed%20set%20of%20brain%0Aregions%2C%20including%20the%20visual%20cortex%20and%20the%20frontoparietal%20cortex%2C%20thereby%0Aoutperforming%20standard%20cross-domain%20classification%20methods.%20The%20complete%20code%0Aand%20data%20for%20this%20paper%20have%20been%20made%20openly%20available%20for%20the%20use%20of%20the%0Ascientific%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01163v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Adaptation-Enhanced%2520Searchlight%253A%2520Enabling%2520classification%2520of%2520brain%250A%2520%2520states%2520from%2520visual%2520perception%2520to%2520mental%2520imagery%26entry.906535625%3DAlexander%2520Olza%2520and%2520David%2520Soto%2520and%2520Roberto%2520Santana%26entry.1292438233%3D%2520%2520In%2520cognitive%2520neuroscience%2520and%2520brain-computer%2520interface%2520research%252C%2520accurately%250Apredicting%2520imagined%2520stimuli%2520is%2520crucial.%2520This%2520study%2520investigates%2520the%250Aeffectiveness%2520of%2520Domain%2520Adaptation%2520%2528DA%2529%2520in%2520enhancing%2520imagery%2520prediction%2520using%250Aprimarily%2520visual%2520data%2520from%2520fMRI%2520scans%2520of%252018%2520subjects.%2520Initially%252C%2520we%2520train%2520a%250Abaseline%2520model%2520on%2520visual%2520stimuli%2520to%2520predict%2520imagined%2520stimuli%252C%2520utilizing%2520data%250Afrom%252014%2520brain%2520regions.%2520We%2520then%2520develop%2520several%2520models%2520to%2520improve%2520imagery%250Aprediction%252C%2520comparing%2520different%2520DA%2520methods.%2520Our%2520results%2520demonstrate%2520that%2520DA%250Asignificantly%2520enhances%2520imagery%2520prediction%2520in%2520binary%2520classification%2520on%2520our%250Adataset%252C%2520as%2520well%2520as%2520in%2520multiclass%2520classification%2520on%2520a%2520publicly%2520available%250Adataset.%2520We%2520then%2520conduct%2520a%2520DA-enhanced%2520searchlight%2520analysis%252C%2520followed%2520by%250Apermutation-based%2520statistical%2520tests%2520to%2520identify%2520brain%2520regions%2520where%2520imagery%250Adecoding%2520is%2520consistently%2520above%2520chance%2520across%2520subjects.%2520Our%2520DA-enhanced%250Asearchlight%2520predicts%2520imagery%2520contents%2520in%2520a%2520highly%2520distributed%2520set%2520of%2520brain%250Aregions%252C%2520including%2520the%2520visual%2520cortex%2520and%2520the%2520frontoparietal%2520cortex%252C%2520thereby%250Aoutperforming%2520standard%2520cross-domain%2520classification%2520methods.%2520The%2520complete%2520code%250Aand%2520data%2520for%2520this%2520paper%2520have%2520been%2520made%2520openly%2520available%2520for%2520the%2520use%2520of%2520the%250Ascientific%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01163v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Adaptation-Enhanced%20Searchlight%3A%20Enabling%20classification%20of%20brain%0A%20%20states%20from%20visual%20perception%20to%20mental%20imagery&entry.906535625=Alexander%20Olza%20and%20David%20Soto%20and%20Roberto%20Santana&entry.1292438233=%20%20In%20cognitive%20neuroscience%20and%20brain-computer%20interface%20research%2C%20accurately%0Apredicting%20imagined%20stimuli%20is%20crucial.%20This%20study%20investigates%20the%0Aeffectiveness%20of%20Domain%20Adaptation%20%28DA%29%20in%20enhancing%20imagery%20prediction%20using%0Aprimarily%20visual%20data%20from%20fMRI%20scans%20of%2018%20subjects.%20Initially%2C%20we%20train%20a%0Abaseline%20model%20on%20visual%20stimuli%20to%20predict%20imagined%20stimuli%2C%20utilizing%20data%0Afrom%2014%20brain%20regions.%20We%20then%20develop%20several%20models%20to%20improve%20imagery%0Aprediction%2C%20comparing%20different%20DA%20methods.%20Our%20results%20demonstrate%20that%20DA%0Asignificantly%20enhances%20imagery%20prediction%20in%20binary%20classification%20on%20our%0Adataset%2C%20as%20well%20as%20in%20multiclass%20classification%20on%20a%20publicly%20available%0Adataset.%20We%20then%20conduct%20a%20DA-enhanced%20searchlight%20analysis%2C%20followed%20by%0Apermutation-based%20statistical%20tests%20to%20identify%20brain%20regions%20where%20imagery%0Adecoding%20is%20consistently%20above%20chance%20across%20subjects.%20Our%20DA-enhanced%0Asearchlight%20predicts%20imagery%20contents%20in%20a%20highly%20distributed%20set%20of%20brain%0Aregions%2C%20including%20the%20visual%20cortex%20and%20the%20frontoparietal%20cortex%2C%20thereby%0Aoutperforming%20standard%20cross-domain%20classification%20methods.%20The%20complete%20code%0Aand%20data%20for%20this%20paper%20have%20been%20made%20openly%20available%20for%20the%20use%20of%20the%0Ascientific%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01163v3&entry.124074799=Read"},
{"title": "Upside Down Reinforcement Learning with Policy Generators", "author": "Jacopo Di Ventura and Dylan R. Ashley and Francesco Faccio and Vincent Herrmann and J\u00fcrgen Schmidhuber", "abstract": "  Upside Down Reinforcement Learning (UDRL) is a promising framework for\nsolving reinforcement learning problems which focuses on learning\ncommand-conditioned policies. In this work, we extend UDRL to the task of\nlearning a command-conditioned generator of deep neural network policies. We\naccomplish this using Hypernetworks - a variant of Fast Weight Programmers,\nwhich learn to decode input commands representing a desired expected return\ninto command-specific weight matrices. Our method, dubbed Upside Down\nReinforcement Learning with Policy Generators (UDRLPG), streamlines comparable\ntechniques by removing the need for an evaluator or critic to update the\nweights of the generator. To counteract the increased variance in last returns\ncaused by not having an evaluator, we decouple the sampling probability of the\nbuffer from the absolute number of policies in it, which, together with a\nsimple weighting strategy, improves the empirical convergence of the algorithm.\nCompared with existing algorithms, UDRLPG achieves competitive performance and\nhigh returns, sometimes outperforming more complex architectures. Our\nexperiments show that a trained generator can generalize to create policies\nthat achieve unseen returns zero-shot. The proposed method appears to be\neffective in mitigating some of the challenges associated with learning highly\nmultimodal functions. Altogether, we believe that UDRLPG represents a promising\nstep forward in achieving greater empirical sample efficiency in RL. A full\nimplementation of UDRLPG is publicly available at\nhttps://github.com/JacopoD/udrlpg_\n", "link": "http://arxiv.org/abs/2501.16288v1", "date": "2025-01-27", "relevancy": 2.1728, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5721}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5385}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Upside%20Down%20Reinforcement%20Learning%20with%20Policy%20Generators&body=Title%3A%20Upside%20Down%20Reinforcement%20Learning%20with%20Policy%20Generators%0AAuthor%3A%20Jacopo%20Di%20Ventura%20and%20Dylan%20R.%20Ashley%20and%20Francesco%20Faccio%20and%20Vincent%20Herrmann%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Upside%20Down%20Reinforcement%20Learning%20%28UDRL%29%20is%20a%20promising%20framework%20for%0Asolving%20reinforcement%20learning%20problems%20which%20focuses%20on%20learning%0Acommand-conditioned%20policies.%20In%20this%20work%2C%20we%20extend%20UDRL%20to%20the%20task%20of%0Alearning%20a%20command-conditioned%20generator%20of%20deep%20neural%20network%20policies.%20We%0Aaccomplish%20this%20using%20Hypernetworks%20-%20a%20variant%20of%20Fast%20Weight%20Programmers%2C%0Awhich%20learn%20to%20decode%20input%20commands%20representing%20a%20desired%20expected%20return%0Ainto%20command-specific%20weight%20matrices.%20Our%20method%2C%20dubbed%20Upside%20Down%0AReinforcement%20Learning%20with%20Policy%20Generators%20%28UDRLPG%29%2C%20streamlines%20comparable%0Atechniques%20by%20removing%20the%20need%20for%20an%20evaluator%20or%20critic%20to%20update%20the%0Aweights%20of%20the%20generator.%20To%20counteract%20the%20increased%20variance%20in%20last%20returns%0Acaused%20by%20not%20having%20an%20evaluator%2C%20we%20decouple%20the%20sampling%20probability%20of%20the%0Abuffer%20from%20the%20absolute%20number%20of%20policies%20in%20it%2C%20which%2C%20together%20with%20a%0Asimple%20weighting%20strategy%2C%20improves%20the%20empirical%20convergence%20of%20the%20algorithm.%0ACompared%20with%20existing%20algorithms%2C%20UDRLPG%20achieves%20competitive%20performance%20and%0Ahigh%20returns%2C%20sometimes%20outperforming%20more%20complex%20architectures.%20Our%0Aexperiments%20show%20that%20a%20trained%20generator%20can%20generalize%20to%20create%20policies%0Athat%20achieve%20unseen%20returns%20zero-shot.%20The%20proposed%20method%20appears%20to%20be%0Aeffective%20in%20mitigating%20some%20of%20the%20challenges%20associated%20with%20learning%20highly%0Amultimodal%20functions.%20Altogether%2C%20we%20believe%20that%20UDRLPG%20represents%20a%20promising%0Astep%20forward%20in%20achieving%20greater%20empirical%20sample%20efficiency%20in%20RL.%20A%20full%0Aimplementation%20of%20UDRLPG%20is%20publicly%20available%20at%0Ahttps%3A//github.com/JacopoD/udrlpg_%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpside%2520Down%2520Reinforcement%2520Learning%2520with%2520Policy%2520Generators%26entry.906535625%3DJacopo%2520Di%2520Ventura%2520and%2520Dylan%2520R.%2520Ashley%2520and%2520Francesco%2520Faccio%2520and%2520Vincent%2520Herrmann%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Upside%2520Down%2520Reinforcement%2520Learning%2520%2528UDRL%2529%2520is%2520a%2520promising%2520framework%2520for%250Asolving%2520reinforcement%2520learning%2520problems%2520which%2520focuses%2520on%2520learning%250Acommand-conditioned%2520policies.%2520In%2520this%2520work%252C%2520we%2520extend%2520UDRL%2520to%2520the%2520task%2520of%250Alearning%2520a%2520command-conditioned%2520generator%2520of%2520deep%2520neural%2520network%2520policies.%2520We%250Aaccomplish%2520this%2520using%2520Hypernetworks%2520-%2520a%2520variant%2520of%2520Fast%2520Weight%2520Programmers%252C%250Awhich%2520learn%2520to%2520decode%2520input%2520commands%2520representing%2520a%2520desired%2520expected%2520return%250Ainto%2520command-specific%2520weight%2520matrices.%2520Our%2520method%252C%2520dubbed%2520Upside%2520Down%250AReinforcement%2520Learning%2520with%2520Policy%2520Generators%2520%2528UDRLPG%2529%252C%2520streamlines%2520comparable%250Atechniques%2520by%2520removing%2520the%2520need%2520for%2520an%2520evaluator%2520or%2520critic%2520to%2520update%2520the%250Aweights%2520of%2520the%2520generator.%2520To%2520counteract%2520the%2520increased%2520variance%2520in%2520last%2520returns%250Acaused%2520by%2520not%2520having%2520an%2520evaluator%252C%2520we%2520decouple%2520the%2520sampling%2520probability%2520of%2520the%250Abuffer%2520from%2520the%2520absolute%2520number%2520of%2520policies%2520in%2520it%252C%2520which%252C%2520together%2520with%2520a%250Asimple%2520weighting%2520strategy%252C%2520improves%2520the%2520empirical%2520convergence%2520of%2520the%2520algorithm.%250ACompared%2520with%2520existing%2520algorithms%252C%2520UDRLPG%2520achieves%2520competitive%2520performance%2520and%250Ahigh%2520returns%252C%2520sometimes%2520outperforming%2520more%2520complex%2520architectures.%2520Our%250Aexperiments%2520show%2520that%2520a%2520trained%2520generator%2520can%2520generalize%2520to%2520create%2520policies%250Athat%2520achieve%2520unseen%2520returns%2520zero-shot.%2520The%2520proposed%2520method%2520appears%2520to%2520be%250Aeffective%2520in%2520mitigating%2520some%2520of%2520the%2520challenges%2520associated%2520with%2520learning%2520highly%250Amultimodal%2520functions.%2520Altogether%252C%2520we%2520believe%2520that%2520UDRLPG%2520represents%2520a%2520promising%250Astep%2520forward%2520in%2520achieving%2520greater%2520empirical%2520sample%2520efficiency%2520in%2520RL.%2520A%2520full%250Aimplementation%2520of%2520UDRLPG%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/JacopoD/udrlpg_%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Upside%20Down%20Reinforcement%20Learning%20with%20Policy%20Generators&entry.906535625=Jacopo%20Di%20Ventura%20and%20Dylan%20R.%20Ashley%20and%20Francesco%20Faccio%20and%20Vincent%20Herrmann%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Upside%20Down%20Reinforcement%20Learning%20%28UDRL%29%20is%20a%20promising%20framework%20for%0Asolving%20reinforcement%20learning%20problems%20which%20focuses%20on%20learning%0Acommand-conditioned%20policies.%20In%20this%20work%2C%20we%20extend%20UDRL%20to%20the%20task%20of%0Alearning%20a%20command-conditioned%20generator%20of%20deep%20neural%20network%20policies.%20We%0Aaccomplish%20this%20using%20Hypernetworks%20-%20a%20variant%20of%20Fast%20Weight%20Programmers%2C%0Awhich%20learn%20to%20decode%20input%20commands%20representing%20a%20desired%20expected%20return%0Ainto%20command-specific%20weight%20matrices.%20Our%20method%2C%20dubbed%20Upside%20Down%0AReinforcement%20Learning%20with%20Policy%20Generators%20%28UDRLPG%29%2C%20streamlines%20comparable%0Atechniques%20by%20removing%20the%20need%20for%20an%20evaluator%20or%20critic%20to%20update%20the%0Aweights%20of%20the%20generator.%20To%20counteract%20the%20increased%20variance%20in%20last%20returns%0Acaused%20by%20not%20having%20an%20evaluator%2C%20we%20decouple%20the%20sampling%20probability%20of%20the%0Abuffer%20from%20the%20absolute%20number%20of%20policies%20in%20it%2C%20which%2C%20together%20with%20a%0Asimple%20weighting%20strategy%2C%20improves%20the%20empirical%20convergence%20of%20the%20algorithm.%0ACompared%20with%20existing%20algorithms%2C%20UDRLPG%20achieves%20competitive%20performance%20and%0Ahigh%20returns%2C%20sometimes%20outperforming%20more%20complex%20architectures.%20Our%0Aexperiments%20show%20that%20a%20trained%20generator%20can%20generalize%20to%20create%20policies%0Athat%20achieve%20unseen%20returns%20zero-shot.%20The%20proposed%20method%20appears%20to%20be%0Aeffective%20in%20mitigating%20some%20of%20the%20challenges%20associated%20with%20learning%20highly%0Amultimodal%20functions.%20Altogether%2C%20we%20believe%20that%20UDRLPG%20represents%20a%20promising%0Astep%20forward%20in%20achieving%20greater%20empirical%20sample%20efficiency%20in%20RL.%20A%20full%0Aimplementation%20of%20UDRLPG%20is%20publicly%20available%20at%0Ahttps%3A//github.com/JacopoD/udrlpg_%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16288v1&entry.124074799=Read"},
{"title": "Automated Detection of Sport Highlights from Audio and Video Sources", "author": "Francesco Della Santa and Morgana Lalli", "abstract": "  This study presents a novel Deep Learning-based and lightweight approach for\nthe automated detection of sports highlights (HLs) from audio and video\nsources. HL detection is a key task in sports video analysis, traditionally\nrequiring significant human effort. Our solution leverages Deep Learning (DL)\nmodels trained on relatively small datasets of audio Mel-spectrograms and\ngrayscale video frames, achieving promising accuracy rates of 89% and 83% for\naudio and video detection, respectively. The use of small datasets, combined\nwith simple architectures, demonstrates the practicality of our method for fast\nand cost-effective deployment. Furthermore, an ensemble model combining both\nmodalities shows improved robustness against false positives and false\nnegatives. The proposed methodology offers a scalable solution for automated HL\ndetection across various types of sports video content, reducing the need for\nmanual intervention. Future work will focus on enhancing model architectures\nand extending this approach to broader scene-detection tasks in media analysis.\n", "link": "http://arxiv.org/abs/2501.16100v1", "date": "2025-01-27", "relevancy": 2.1712, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5668}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Detection%20of%20Sport%20Highlights%20from%20Audio%20and%20Video%20Sources&body=Title%3A%20Automated%20Detection%20of%20Sport%20Highlights%20from%20Audio%20and%20Video%20Sources%0AAuthor%3A%20Francesco%20Della%20Santa%20and%20Morgana%20Lalli%0AAbstract%3A%20%20%20This%20study%20presents%20a%20novel%20Deep%20Learning-based%20and%20lightweight%20approach%20for%0Athe%20automated%20detection%20of%20sports%20highlights%20%28HLs%29%20from%20audio%20and%20video%0Asources.%20HL%20detection%20is%20a%20key%20task%20in%20sports%20video%20analysis%2C%20traditionally%0Arequiring%20significant%20human%20effort.%20Our%20solution%20leverages%20Deep%20Learning%20%28DL%29%0Amodels%20trained%20on%20relatively%20small%20datasets%20of%20audio%20Mel-spectrograms%20and%0Agrayscale%20video%20frames%2C%20achieving%20promising%20accuracy%20rates%20of%2089%25%20and%2083%25%20for%0Aaudio%20and%20video%20detection%2C%20respectively.%20The%20use%20of%20small%20datasets%2C%20combined%0Awith%20simple%20architectures%2C%20demonstrates%20the%20practicality%20of%20our%20method%20for%20fast%0Aand%20cost-effective%20deployment.%20Furthermore%2C%20an%20ensemble%20model%20combining%20both%0Amodalities%20shows%20improved%20robustness%20against%20false%20positives%20and%20false%0Anegatives.%20The%20proposed%20methodology%20offers%20a%20scalable%20solution%20for%20automated%20HL%0Adetection%20across%20various%20types%20of%20sports%20video%20content%2C%20reducing%20the%20need%20for%0Amanual%20intervention.%20Future%20work%20will%20focus%20on%20enhancing%20model%20architectures%0Aand%20extending%20this%20approach%20to%20broader%20scene-detection%20tasks%20in%20media%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Detection%2520of%2520Sport%2520Highlights%2520from%2520Audio%2520and%2520Video%2520Sources%26entry.906535625%3DFrancesco%2520Della%2520Santa%2520and%2520Morgana%2520Lalli%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520novel%2520Deep%2520Learning-based%2520and%2520lightweight%2520approach%2520for%250Athe%2520automated%2520detection%2520of%2520sports%2520highlights%2520%2528HLs%2529%2520from%2520audio%2520and%2520video%250Asources.%2520HL%2520detection%2520is%2520a%2520key%2520task%2520in%2520sports%2520video%2520analysis%252C%2520traditionally%250Arequiring%2520significant%2520human%2520effort.%2520Our%2520solution%2520leverages%2520Deep%2520Learning%2520%2528DL%2529%250Amodels%2520trained%2520on%2520relatively%2520small%2520datasets%2520of%2520audio%2520Mel-spectrograms%2520and%250Agrayscale%2520video%2520frames%252C%2520achieving%2520promising%2520accuracy%2520rates%2520of%252089%2525%2520and%252083%2525%2520for%250Aaudio%2520and%2520video%2520detection%252C%2520respectively.%2520The%2520use%2520of%2520small%2520datasets%252C%2520combined%250Awith%2520simple%2520architectures%252C%2520demonstrates%2520the%2520practicality%2520of%2520our%2520method%2520for%2520fast%250Aand%2520cost-effective%2520deployment.%2520Furthermore%252C%2520an%2520ensemble%2520model%2520combining%2520both%250Amodalities%2520shows%2520improved%2520robustness%2520against%2520false%2520positives%2520and%2520false%250Anegatives.%2520The%2520proposed%2520methodology%2520offers%2520a%2520scalable%2520solution%2520for%2520automated%2520HL%250Adetection%2520across%2520various%2520types%2520of%2520sports%2520video%2520content%252C%2520reducing%2520the%2520need%2520for%250Amanual%2520intervention.%2520Future%2520work%2520will%2520focus%2520on%2520enhancing%2520model%2520architectures%250Aand%2520extending%2520this%2520approach%2520to%2520broader%2520scene-detection%2520tasks%2520in%2520media%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Detection%20of%20Sport%20Highlights%20from%20Audio%20and%20Video%20Sources&entry.906535625=Francesco%20Della%20Santa%20and%20Morgana%20Lalli&entry.1292438233=%20%20This%20study%20presents%20a%20novel%20Deep%20Learning-based%20and%20lightweight%20approach%20for%0Athe%20automated%20detection%20of%20sports%20highlights%20%28HLs%29%20from%20audio%20and%20video%0Asources.%20HL%20detection%20is%20a%20key%20task%20in%20sports%20video%20analysis%2C%20traditionally%0Arequiring%20significant%20human%20effort.%20Our%20solution%20leverages%20Deep%20Learning%20%28DL%29%0Amodels%20trained%20on%20relatively%20small%20datasets%20of%20audio%20Mel-spectrograms%20and%0Agrayscale%20video%20frames%2C%20achieving%20promising%20accuracy%20rates%20of%2089%25%20and%2083%25%20for%0Aaudio%20and%20video%20detection%2C%20respectively.%20The%20use%20of%20small%20datasets%2C%20combined%0Awith%20simple%20architectures%2C%20demonstrates%20the%20practicality%20of%20our%20method%20for%20fast%0Aand%20cost-effective%20deployment.%20Furthermore%2C%20an%20ensemble%20model%20combining%20both%0Amodalities%20shows%20improved%20robustness%20against%20false%20positives%20and%20false%0Anegatives.%20The%20proposed%20methodology%20offers%20a%20scalable%20solution%20for%20automated%20HL%0Adetection%20across%20various%20types%20of%20sports%20video%20content%2C%20reducing%20the%20need%20for%0Amanual%20intervention.%20Future%20work%20will%20focus%20on%20enhancing%20model%20architectures%0Aand%20extending%20this%20approach%20to%20broader%20scene-detection%20tasks%20in%20media%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16100v1&entry.124074799=Read"},
{"title": "The Linear Attention Resurrection in Vision Transformer", "author": "Chuanyang Zheng", "abstract": "  Vision Transformers (ViTs) have recently taken computer vision by storm.\nHowever, the softmax attention underlying ViTs comes with a quadratic\ncomplexity in time and memory, hindering the application of ViTs to\nhigh-resolution images. We revisit the attention design and propose a linear\nattention method to address the limitation, which doesn't sacrifice ViT's core\nadvantage of capturing global representation like existing methods (e.g. local\nwindow attention of Swin). We further investigate the key difference between\nlinear attention and softmax attention. Our empirical results suggest that\nlinear attention lacks a fundamental property of concentrating the distribution\nof the attention matrix. Inspired by this observation, we introduce a local\nconcentration module to enhance linear attention. By incorporating enhanced\nlinear global attention and local window attention, we propose a new ViT\narchitecture, dubbed L$^2$ViT. Notably, L$^2$ViT can effectively capture both\nglobal interactions and local representations while enjoying linear\ncomputational complexity. Extensive experiments demonstrate the strong\nperformance of L$^2$ViT. On image classification, L$^2$ViT achieves 84.4% Top-1\naccuracy on ImageNet-1K without any extra training data or label. By further\npre-training on ImageNet-22k, it attains 87.0% when fine-tuned with resolution\n384$^2$. For downstream tasks, L$^2$ViT delivers favorable performance as a\nbackbone on object detection as well as semantic segmentation.\n", "link": "http://arxiv.org/abs/2501.16182v1", "date": "2025-01-27", "relevancy": 2.1523, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5442}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5431}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Linear%20Attention%20Resurrection%20in%20Vision%20Transformer&body=Title%3A%20The%20Linear%20Attention%20Resurrection%20in%20Vision%20Transformer%0AAuthor%3A%20Chuanyang%20Zheng%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20recently%20taken%20computer%20vision%20by%20storm.%0AHowever%2C%20the%20softmax%20attention%20underlying%20ViTs%20comes%20with%20a%20quadratic%0Acomplexity%20in%20time%20and%20memory%2C%20hindering%20the%20application%20of%20ViTs%20to%0Ahigh-resolution%20images.%20We%20revisit%20the%20attention%20design%20and%20propose%20a%20linear%0Aattention%20method%20to%20address%20the%20limitation%2C%20which%20doesn%27t%20sacrifice%20ViT%27s%20core%0Aadvantage%20of%20capturing%20global%20representation%20like%20existing%20methods%20%28e.g.%20local%0Awindow%20attention%20of%20Swin%29.%20We%20further%20investigate%20the%20key%20difference%20between%0Alinear%20attention%20and%20softmax%20attention.%20Our%20empirical%20results%20suggest%20that%0Alinear%20attention%20lacks%20a%20fundamental%20property%20of%20concentrating%20the%20distribution%0Aof%20the%20attention%20matrix.%20Inspired%20by%20this%20observation%2C%20we%20introduce%20a%20local%0Aconcentration%20module%20to%20enhance%20linear%20attention.%20By%20incorporating%20enhanced%0Alinear%20global%20attention%20and%20local%20window%20attention%2C%20we%20propose%20a%20new%20ViT%0Aarchitecture%2C%20dubbed%20L%24%5E2%24ViT.%20Notably%2C%20L%24%5E2%24ViT%20can%20effectively%20capture%20both%0Aglobal%20interactions%20and%20local%20representations%20while%20enjoying%20linear%0Acomputational%20complexity.%20Extensive%20experiments%20demonstrate%20the%20strong%0Aperformance%20of%20L%24%5E2%24ViT.%20On%20image%20classification%2C%20L%24%5E2%24ViT%20achieves%2084.4%25%20Top-1%0Aaccuracy%20on%20ImageNet-1K%20without%20any%20extra%20training%20data%20or%20label.%20By%20further%0Apre-training%20on%20ImageNet-22k%2C%20it%20attains%2087.0%25%20when%20fine-tuned%20with%20resolution%0A384%24%5E2%24.%20For%20downstream%20tasks%2C%20L%24%5E2%24ViT%20delivers%20favorable%20performance%20as%20a%0Abackbone%20on%20object%20detection%20as%20well%20as%20semantic%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Linear%2520Attention%2520Resurrection%2520in%2520Vision%2520Transformer%26entry.906535625%3DChuanyang%2520Zheng%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520recently%2520taken%2520computer%2520vision%2520by%2520storm.%250AHowever%252C%2520the%2520softmax%2520attention%2520underlying%2520ViTs%2520comes%2520with%2520a%2520quadratic%250Acomplexity%2520in%2520time%2520and%2520memory%252C%2520hindering%2520the%2520application%2520of%2520ViTs%2520to%250Ahigh-resolution%2520images.%2520We%2520revisit%2520the%2520attention%2520design%2520and%2520propose%2520a%2520linear%250Aattention%2520method%2520to%2520address%2520the%2520limitation%252C%2520which%2520doesn%2527t%2520sacrifice%2520ViT%2527s%2520core%250Aadvantage%2520of%2520capturing%2520global%2520representation%2520like%2520existing%2520methods%2520%2528e.g.%2520local%250Awindow%2520attention%2520of%2520Swin%2529.%2520We%2520further%2520investigate%2520the%2520key%2520difference%2520between%250Alinear%2520attention%2520and%2520softmax%2520attention.%2520Our%2520empirical%2520results%2520suggest%2520that%250Alinear%2520attention%2520lacks%2520a%2520fundamental%2520property%2520of%2520concentrating%2520the%2520distribution%250Aof%2520the%2520attention%2520matrix.%2520Inspired%2520by%2520this%2520observation%252C%2520we%2520introduce%2520a%2520local%250Aconcentration%2520module%2520to%2520enhance%2520linear%2520attention.%2520By%2520incorporating%2520enhanced%250Alinear%2520global%2520attention%2520and%2520local%2520window%2520attention%252C%2520we%2520propose%2520a%2520new%2520ViT%250Aarchitecture%252C%2520dubbed%2520L%2524%255E2%2524ViT.%2520Notably%252C%2520L%2524%255E2%2524ViT%2520can%2520effectively%2520capture%2520both%250Aglobal%2520interactions%2520and%2520local%2520representations%2520while%2520enjoying%2520linear%250Acomputational%2520complexity.%2520Extensive%2520experiments%2520demonstrate%2520the%2520strong%250Aperformance%2520of%2520L%2524%255E2%2524ViT.%2520On%2520image%2520classification%252C%2520L%2524%255E2%2524ViT%2520achieves%252084.4%2525%2520Top-1%250Aaccuracy%2520on%2520ImageNet-1K%2520without%2520any%2520extra%2520training%2520data%2520or%2520label.%2520By%2520further%250Apre-training%2520on%2520ImageNet-22k%252C%2520it%2520attains%252087.0%2525%2520when%2520fine-tuned%2520with%2520resolution%250A384%2524%255E2%2524.%2520For%2520downstream%2520tasks%252C%2520L%2524%255E2%2524ViT%2520delivers%2520favorable%2520performance%2520as%2520a%250Abackbone%2520on%2520object%2520detection%2520as%2520well%2520as%2520semantic%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Linear%20Attention%20Resurrection%20in%20Vision%20Transformer&entry.906535625=Chuanyang%20Zheng&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20recently%20taken%20computer%20vision%20by%20storm.%0AHowever%2C%20the%20softmax%20attention%20underlying%20ViTs%20comes%20with%20a%20quadratic%0Acomplexity%20in%20time%20and%20memory%2C%20hindering%20the%20application%20of%20ViTs%20to%0Ahigh-resolution%20images.%20We%20revisit%20the%20attention%20design%20and%20propose%20a%20linear%0Aattention%20method%20to%20address%20the%20limitation%2C%20which%20doesn%27t%20sacrifice%20ViT%27s%20core%0Aadvantage%20of%20capturing%20global%20representation%20like%20existing%20methods%20%28e.g.%20local%0Awindow%20attention%20of%20Swin%29.%20We%20further%20investigate%20the%20key%20difference%20between%0Alinear%20attention%20and%20softmax%20attention.%20Our%20empirical%20results%20suggest%20that%0Alinear%20attention%20lacks%20a%20fundamental%20property%20of%20concentrating%20the%20distribution%0Aof%20the%20attention%20matrix.%20Inspired%20by%20this%20observation%2C%20we%20introduce%20a%20local%0Aconcentration%20module%20to%20enhance%20linear%20attention.%20By%20incorporating%20enhanced%0Alinear%20global%20attention%20and%20local%20window%20attention%2C%20we%20propose%20a%20new%20ViT%0Aarchitecture%2C%20dubbed%20L%24%5E2%24ViT.%20Notably%2C%20L%24%5E2%24ViT%20can%20effectively%20capture%20both%0Aglobal%20interactions%20and%20local%20representations%20while%20enjoying%20linear%0Acomputational%20complexity.%20Extensive%20experiments%20demonstrate%20the%20strong%0Aperformance%20of%20L%24%5E2%24ViT.%20On%20image%20classification%2C%20L%24%5E2%24ViT%20achieves%2084.4%25%20Top-1%0Aaccuracy%20on%20ImageNet-1K%20without%20any%20extra%20training%20data%20or%20label.%20By%20further%0Apre-training%20on%20ImageNet-22k%2C%20it%20attains%2087.0%25%20when%20fine-tuned%20with%20resolution%0A384%24%5E2%24.%20For%20downstream%20tasks%2C%20L%24%5E2%24ViT%20delivers%20favorable%20performance%20as%20a%0Abackbone%20on%20object%20detection%20as%20well%20as%20semantic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16182v1&entry.124074799=Read"},
{"title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in\n  Large Language Models", "author": "Bo Gao and Michael W. Spratling", "abstract": "  Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by decomposing the Softmax operation into a non-linear\ntransformation and the $l_1$-norm. We identify the latter as essential for\nmaintaining model performance. By replacing the non-linear transformation with\nthe Softplus activation function and introducing a dynamic scale factor for\ndifferent token lengths based on invariance entropy, we create a novel\nattention mechanism with performance better than conventional Softmax attention\nacross various inference lengths. To further improve the length extrapolation\nability of the proposed attention mechanism, we introduce a fine-tuning-free\nre-weighting mechanism that amplifies significant attention weights while\ndiminishing weaker ones, enabling the model to concentrate more effectively on\nrelevant tokens without requiring retraining. When combined with our proposed\nattention mechanism, this approach demonstrates significant promise in managing\nlonger sequences, maintaining nearly constant validation loss even at\n16$\\times$ the training token length while ensuring numerical stability. Our\ncode is available at: https://github.com/iminfine/freeatten.\n", "link": "http://arxiv.org/abs/2501.13428v2", "date": "2025-01-27", "relevancy": 2.1508, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5632}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5195}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Softplus%20Attention%20with%20Re-weighting%20Boosts%20Length%20Extrapolation%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Softplus%20Attention%20with%20Re-weighting%20Boosts%20Length%20Extrapolation%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Bo%20Gao%20and%20Michael%20W.%20Spratling%0AAbstract%3A%20%20%20Large%20language%20models%20have%20achieved%20remarkable%20success%20in%20recent%20years%2C%0Aprimarily%20due%20to%20the%20implementation%20of%20self-attention%20mechanisms.%20However%2C%0Atraditional%20Softmax%20attention%20suffers%20from%20numerical%20instability%20and%20reduced%0Aperformance%20as%20the%20length%20of%20inference%20tokens%20increases.%20This%20paper%20addresses%0Athese%20issues%20by%20decomposing%20the%20Softmax%20operation%20into%20a%20non-linear%0Atransformation%20and%20the%20%24l_1%24-norm.%20We%20identify%20the%20latter%20as%20essential%20for%0Amaintaining%20model%20performance.%20By%20replacing%20the%20non-linear%20transformation%20with%0Athe%20Softplus%20activation%20function%20and%20introducing%20a%20dynamic%20scale%20factor%20for%0Adifferent%20token%20lengths%20based%20on%20invariance%20entropy%2C%20we%20create%20a%20novel%0Aattention%20mechanism%20with%20performance%20better%20than%20conventional%20Softmax%20attention%0Aacross%20various%20inference%20lengths.%20To%20further%20improve%20the%20length%20extrapolation%0Aability%20of%20the%20proposed%20attention%20mechanism%2C%20we%20introduce%20a%20fine-tuning-free%0Are-weighting%20mechanism%20that%20amplifies%20significant%20attention%20weights%20while%0Adiminishing%20weaker%20ones%2C%20enabling%20the%20model%20to%20concentrate%20more%20effectively%20on%0Arelevant%20tokens%20without%20requiring%20retraining.%20When%20combined%20with%20our%20proposed%0Aattention%20mechanism%2C%20this%20approach%20demonstrates%20significant%20promise%20in%20managing%0Alonger%20sequences%2C%20maintaining%20nearly%20constant%20validation%20loss%20even%20at%0A16%24%5Ctimes%24%20the%20training%20token%20length%20while%20ensuring%20numerical%20stability.%20Our%0Acode%20is%20available%20at%3A%20https%3A//github.com/iminfine/freeatten.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13428v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoftplus%2520Attention%2520with%2520Re-weighting%2520Boosts%2520Length%2520Extrapolation%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DBo%2520Gao%2520and%2520Michael%2520W.%2520Spratling%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520achieved%2520remarkable%2520success%2520in%2520recent%2520years%252C%250Aprimarily%2520due%2520to%2520the%2520implementation%2520of%2520self-attention%2520mechanisms.%2520However%252C%250Atraditional%2520Softmax%2520attention%2520suffers%2520from%2520numerical%2520instability%2520and%2520reduced%250Aperformance%2520as%2520the%2520length%2520of%2520inference%2520tokens%2520increases.%2520This%2520paper%2520addresses%250Athese%2520issues%2520by%2520decomposing%2520the%2520Softmax%2520operation%2520into%2520a%2520non-linear%250Atransformation%2520and%2520the%2520%2524l_1%2524-norm.%2520We%2520identify%2520the%2520latter%2520as%2520essential%2520for%250Amaintaining%2520model%2520performance.%2520By%2520replacing%2520the%2520non-linear%2520transformation%2520with%250Athe%2520Softplus%2520activation%2520function%2520and%2520introducing%2520a%2520dynamic%2520scale%2520factor%2520for%250Adifferent%2520token%2520lengths%2520based%2520on%2520invariance%2520entropy%252C%2520we%2520create%2520a%2520novel%250Aattention%2520mechanism%2520with%2520performance%2520better%2520than%2520conventional%2520Softmax%2520attention%250Aacross%2520various%2520inference%2520lengths.%2520To%2520further%2520improve%2520the%2520length%2520extrapolation%250Aability%2520of%2520the%2520proposed%2520attention%2520mechanism%252C%2520we%2520introduce%2520a%2520fine-tuning-free%250Are-weighting%2520mechanism%2520that%2520amplifies%2520significant%2520attention%2520weights%2520while%250Adiminishing%2520weaker%2520ones%252C%2520enabling%2520the%2520model%2520to%2520concentrate%2520more%2520effectively%2520on%250Arelevant%2520tokens%2520without%2520requiring%2520retraining.%2520When%2520combined%2520with%2520our%2520proposed%250Aattention%2520mechanism%252C%2520this%2520approach%2520demonstrates%2520significant%2520promise%2520in%2520managing%250Alonger%2520sequences%252C%2520maintaining%2520nearly%2520constant%2520validation%2520loss%2520even%2520at%250A16%2524%255Ctimes%2524%2520the%2520training%2520token%2520length%2520while%2520ensuring%2520numerical%2520stability.%2520Our%250Acode%2520is%2520available%2520at%253A%2520https%253A//github.com/iminfine/freeatten.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13428v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Softplus%20Attention%20with%20Re-weighting%20Boosts%20Length%20Extrapolation%20in%0A%20%20Large%20Language%20Models&entry.906535625=Bo%20Gao%20and%20Michael%20W.%20Spratling&entry.1292438233=%20%20Large%20language%20models%20have%20achieved%20remarkable%20success%20in%20recent%20years%2C%0Aprimarily%20due%20to%20the%20implementation%20of%20self-attention%20mechanisms.%20However%2C%0Atraditional%20Softmax%20attention%20suffers%20from%20numerical%20instability%20and%20reduced%0Aperformance%20as%20the%20length%20of%20inference%20tokens%20increases.%20This%20paper%20addresses%0Athese%20issues%20by%20decomposing%20the%20Softmax%20operation%20into%20a%20non-linear%0Atransformation%20and%20the%20%24l_1%24-norm.%20We%20identify%20the%20latter%20as%20essential%20for%0Amaintaining%20model%20performance.%20By%20replacing%20the%20non-linear%20transformation%20with%0Athe%20Softplus%20activation%20function%20and%20introducing%20a%20dynamic%20scale%20factor%20for%0Adifferent%20token%20lengths%20based%20on%20invariance%20entropy%2C%20we%20create%20a%20novel%0Aattention%20mechanism%20with%20performance%20better%20than%20conventional%20Softmax%20attention%0Aacross%20various%20inference%20lengths.%20To%20further%20improve%20the%20length%20extrapolation%0Aability%20of%20the%20proposed%20attention%20mechanism%2C%20we%20introduce%20a%20fine-tuning-free%0Are-weighting%20mechanism%20that%20amplifies%20significant%20attention%20weights%20while%0Adiminishing%20weaker%20ones%2C%20enabling%20the%20model%20to%20concentrate%20more%20effectively%20on%0Arelevant%20tokens%20without%20requiring%20retraining.%20When%20combined%20with%20our%20proposed%0Aattention%20mechanism%2C%20this%20approach%20demonstrates%20significant%20promise%20in%20managing%0Alonger%20sequences%2C%20maintaining%20nearly%20constant%20validation%20loss%20even%20at%0A16%24%5Ctimes%24%20the%20training%20token%20length%20while%20ensuring%20numerical%20stability.%20Our%0Acode%20is%20available%20at%3A%20https%3A//github.com/iminfine/freeatten.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13428v2&entry.124074799=Read"},
{"title": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation", "author": "Karishma Thakrar", "abstract": "  Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation.\n", "link": "http://arxiv.org/abs/2412.18644v2", "date": "2025-01-27", "relevancy": 2.1483, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5777}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.517}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaGRAG%20%7C%20Exploring%20the%20Topology%20of%20Information%20for%20Advancing%20Language%0A%20%20Understanding%20and%20Generation%20in%20Graph%20Retrieval-Augmented%20Generation&body=Title%3A%20DynaGRAG%20%7C%20Exploring%20the%20Topology%20of%20Information%20for%20Advancing%20Language%0A%20%20Understanding%20and%20Generation%20in%20Graph%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Karishma%20Thakrar%0AAbstract%3A%20%20%20Graph%20Retrieval-Augmented%20Generation%20%28GRAG%20or%20Graph%20RAG%29%20architectures%20aim%20to%0Aenhance%20language%20understanding%20and%20generation%20by%20leveraging%20external%20knowledge.%0AHowever%2C%20effectively%20capturing%20and%20integrating%20the%20rich%20semantic%20information%0Apresent%20in%20textual%20and%20structured%20data%20remains%20a%20challenge.%20To%20address%20this%2C%20a%0Anovel%20GRAG%20framework%2C%20Dynamic%20Graph%20Retrieval-Agumented%20Generation%20%28DynaGRAG%29%2C%0Ais%20proposed%20to%20focus%20on%20enhancing%20subgraph%20representation%20and%20diversity%20within%0Athe%20knowledge%20graph.%20By%20improving%20graph%20density%2C%20capturing%20entity%20and%20relation%0Ainformation%20more%20effectively%2C%20and%20dynamically%20prioritizing%20relevant%20and%20diverse%0Asubgraphs%20and%20information%20within%20them%2C%20the%20proposed%20approach%20enables%20a%20more%0Acomprehensive%20understanding%20of%20the%20underlying%20semantic%20structure.%20This%20is%0Aachieved%20through%20a%20combination%20of%20de-duplication%20processes%2C%20two-step%20mean%0Apooling%20of%20embeddings%2C%20query-aware%20retrieval%20considering%20unique%20nodes%2C%20and%20a%0ADynamic%20Similarity-Aware%20BFS%20%28DSA-BFS%29%20traversal%20algorithm.%20Integrating%20Graph%0AConvolutional%20Networks%20%28GCNs%29%20and%20Large%20Language%20Models%20%28LLMs%29%20through%20hard%0Aprompting%20further%20enhances%20the%20learning%20of%20rich%20node%20and%20edge%20representations%0Awhile%20preserving%20the%20hierarchical%20subgraph%20structure.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20DynaGRAG%2C%20showcasing%20the%20significance%20of%0Aenhanced%20subgraph%20representation%20and%20diversity%20for%20improved%20language%0Aunderstanding%20and%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18644v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaGRAG%2520%257C%2520Exploring%2520the%2520Topology%2520of%2520Information%2520for%2520Advancing%2520Language%250A%2520%2520Understanding%2520and%2520Generation%2520in%2520Graph%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DKarishma%2520Thakrar%26entry.1292438233%3D%2520%2520Graph%2520Retrieval-Augmented%2520Generation%2520%2528GRAG%2520or%2520Graph%2520RAG%2529%2520architectures%2520aim%2520to%250Aenhance%2520language%2520understanding%2520and%2520generation%2520by%2520leveraging%2520external%2520knowledge.%250AHowever%252C%2520effectively%2520capturing%2520and%2520integrating%2520the%2520rich%2520semantic%2520information%250Apresent%2520in%2520textual%2520and%2520structured%2520data%2520remains%2520a%2520challenge.%2520To%2520address%2520this%252C%2520a%250Anovel%2520GRAG%2520framework%252C%2520Dynamic%2520Graph%2520Retrieval-Agumented%2520Generation%2520%2528DynaGRAG%2529%252C%250Ais%2520proposed%2520to%2520focus%2520on%2520enhancing%2520subgraph%2520representation%2520and%2520diversity%2520within%250Athe%2520knowledge%2520graph.%2520By%2520improving%2520graph%2520density%252C%2520capturing%2520entity%2520and%2520relation%250Ainformation%2520more%2520effectively%252C%2520and%2520dynamically%2520prioritizing%2520relevant%2520and%2520diverse%250Asubgraphs%2520and%2520information%2520within%2520them%252C%2520the%2520proposed%2520approach%2520enables%2520a%2520more%250Acomprehensive%2520understanding%2520of%2520the%2520underlying%2520semantic%2520structure.%2520This%2520is%250Aachieved%2520through%2520a%2520combination%2520of%2520de-duplication%2520processes%252C%2520two-step%2520mean%250Apooling%2520of%2520embeddings%252C%2520query-aware%2520retrieval%2520considering%2520unique%2520nodes%252C%2520and%2520a%250ADynamic%2520Similarity-Aware%2520BFS%2520%2528DSA-BFS%2529%2520traversal%2520algorithm.%2520Integrating%2520Graph%250AConvolutional%2520Networks%2520%2528GCNs%2529%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520through%2520hard%250Aprompting%2520further%2520enhances%2520the%2520learning%2520of%2520rich%2520node%2520and%2520edge%2520representations%250Awhile%2520preserving%2520the%2520hierarchical%2520subgraph%2520structure.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520DynaGRAG%252C%2520showcasing%2520the%2520significance%2520of%250Aenhanced%2520subgraph%2520representation%2520and%2520diversity%2520for%2520improved%2520language%250Aunderstanding%2520and%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18644v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaGRAG%20%7C%20Exploring%20the%20Topology%20of%20Information%20for%20Advancing%20Language%0A%20%20Understanding%20and%20Generation%20in%20Graph%20Retrieval-Augmented%20Generation&entry.906535625=Karishma%20Thakrar&entry.1292438233=%20%20Graph%20Retrieval-Augmented%20Generation%20%28GRAG%20or%20Graph%20RAG%29%20architectures%20aim%20to%0Aenhance%20language%20understanding%20and%20generation%20by%20leveraging%20external%20knowledge.%0AHowever%2C%20effectively%20capturing%20and%20integrating%20the%20rich%20semantic%20information%0Apresent%20in%20textual%20and%20structured%20data%20remains%20a%20challenge.%20To%20address%20this%2C%20a%0Anovel%20GRAG%20framework%2C%20Dynamic%20Graph%20Retrieval-Agumented%20Generation%20%28DynaGRAG%29%2C%0Ais%20proposed%20to%20focus%20on%20enhancing%20subgraph%20representation%20and%20diversity%20within%0Athe%20knowledge%20graph.%20By%20improving%20graph%20density%2C%20capturing%20entity%20and%20relation%0Ainformation%20more%20effectively%2C%20and%20dynamically%20prioritizing%20relevant%20and%20diverse%0Asubgraphs%20and%20information%20within%20them%2C%20the%20proposed%20approach%20enables%20a%20more%0Acomprehensive%20understanding%20of%20the%20underlying%20semantic%20structure.%20This%20is%0Aachieved%20through%20a%20combination%20of%20de-duplication%20processes%2C%20two-step%20mean%0Apooling%20of%20embeddings%2C%20query-aware%20retrieval%20considering%20unique%20nodes%2C%20and%20a%0ADynamic%20Similarity-Aware%20BFS%20%28DSA-BFS%29%20traversal%20algorithm.%20Integrating%20Graph%0AConvolutional%20Networks%20%28GCNs%29%20and%20Large%20Language%20Models%20%28LLMs%29%20through%20hard%0Aprompting%20further%20enhances%20the%20learning%20of%20rich%20node%20and%20edge%20representations%0Awhile%20preserving%20the%20hierarchical%20subgraph%20structure.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20DynaGRAG%2C%20showcasing%20the%20significance%20of%0Aenhanced%20subgraph%20representation%20and%20diversity%20for%20improved%20language%0Aunderstanding%20and%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18644v2&entry.124074799=Read"},
{"title": "ScaDyG:A New Paradigm for Large-scale Dynamic Graph Learning", "author": "Xiang Wu and Xunkai Li and Rong-Hua Li and Kangfei Zhao and Guoren Wang", "abstract": "  Dynamic graphs (DGs), which capture time-evolving relationships between graph\nentities, have widespread real-world applications. To efficiently encode DGs\nfor downstream tasks, most dynamic graph neural networks follow the traditional\nmessage-passing mechanism and extend it with time-based techniques. Despite\ntheir effectiveness, the growth of historical interactions introduces\nsignificant scalability issues, particularly in industry scenarios. To address\nthis limitation, we propose ScaDyG, with the core idea of designing a\ntime-aware scalable learning paradigm as follows: 1) Time-aware Topology\nReformulation: ScaDyG first segments historical interactions into time steps\n(intra and inter) based on dynamic modeling, enabling weight-free and\ntime-aware graph propagation within pre-processing. 2) Dynamic Temporal\nEncoding: To further achieve fine-grained graph propagation within time steps,\nScaDyG integrates temporal encoding through a combination of exponential\nfunctions in a scalable manner. 3) Hypernetwork-driven Message Aggregation:\nAfter obtaining the propagated features (i.e., messages), ScaDyG utilizes\nhypernetwork to analyze historical dependencies, implementing node-wise\nrepresentation by an adaptive temporal fusion. Extensive experiments on 12\ndatasets demonstrate that ScaDyG performs comparably well or even outperforms\nother SOTA methods in both node and link-level downstream tasks, with fewer\nlearnable parameters and higher efficiency.\n", "link": "http://arxiv.org/abs/2501.16002v1", "date": "2025-01-27", "relevancy": 2.1291, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5406}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5353}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScaDyG%3AA%20New%20Paradigm%20for%20Large-scale%20Dynamic%20Graph%20Learning&body=Title%3A%20ScaDyG%3AA%20New%20Paradigm%20for%20Large-scale%20Dynamic%20Graph%20Learning%0AAuthor%3A%20Xiang%20Wu%20and%20Xunkai%20Li%20and%20Rong-Hua%20Li%20and%20Kangfei%20Zhao%20and%20Guoren%20Wang%0AAbstract%3A%20%20%20Dynamic%20graphs%20%28DGs%29%2C%20which%20capture%20time-evolving%20relationships%20between%20graph%0Aentities%2C%20have%20widespread%20real-world%20applications.%20To%20efficiently%20encode%20DGs%0Afor%20downstream%20tasks%2C%20most%20dynamic%20graph%20neural%20networks%20follow%20the%20traditional%0Amessage-passing%20mechanism%20and%20extend%20it%20with%20time-based%20techniques.%20Despite%0Atheir%20effectiveness%2C%20the%20growth%20of%20historical%20interactions%20introduces%0Asignificant%20scalability%20issues%2C%20particularly%20in%20industry%20scenarios.%20To%20address%0Athis%20limitation%2C%20we%20propose%20ScaDyG%2C%20with%20the%20core%20idea%20of%20designing%20a%0Atime-aware%20scalable%20learning%20paradigm%20as%20follows%3A%201%29%20Time-aware%20Topology%0AReformulation%3A%20ScaDyG%20first%20segments%20historical%20interactions%20into%20time%20steps%0A%28intra%20and%20inter%29%20based%20on%20dynamic%20modeling%2C%20enabling%20weight-free%20and%0Atime-aware%20graph%20propagation%20within%20pre-processing.%202%29%20Dynamic%20Temporal%0AEncoding%3A%20To%20further%20achieve%20fine-grained%20graph%20propagation%20within%20time%20steps%2C%0AScaDyG%20integrates%20temporal%20encoding%20through%20a%20combination%20of%20exponential%0Afunctions%20in%20a%20scalable%20manner.%203%29%20Hypernetwork-driven%20Message%20Aggregation%3A%0AAfter%20obtaining%20the%20propagated%20features%20%28i.e.%2C%20messages%29%2C%20ScaDyG%20utilizes%0Ahypernetwork%20to%20analyze%20historical%20dependencies%2C%20implementing%20node-wise%0Arepresentation%20by%20an%20adaptive%20temporal%20fusion.%20Extensive%20experiments%20on%2012%0Adatasets%20demonstrate%20that%20ScaDyG%20performs%20comparably%20well%20or%20even%20outperforms%0Aother%20SOTA%20methods%20in%20both%20node%20and%20link-level%20downstream%20tasks%2C%20with%20fewer%0Alearnable%20parameters%20and%20higher%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaDyG%253AA%2520New%2520Paradigm%2520for%2520Large-scale%2520Dynamic%2520Graph%2520Learning%26entry.906535625%3DXiang%2520Wu%2520and%2520Xunkai%2520Li%2520and%2520Rong-Hua%2520Li%2520and%2520Kangfei%2520Zhao%2520and%2520Guoren%2520Wang%26entry.1292438233%3D%2520%2520Dynamic%2520graphs%2520%2528DGs%2529%252C%2520which%2520capture%2520time-evolving%2520relationships%2520between%2520graph%250Aentities%252C%2520have%2520widespread%2520real-world%2520applications.%2520To%2520efficiently%2520encode%2520DGs%250Afor%2520downstream%2520tasks%252C%2520most%2520dynamic%2520graph%2520neural%2520networks%2520follow%2520the%2520traditional%250Amessage-passing%2520mechanism%2520and%2520extend%2520it%2520with%2520time-based%2520techniques.%2520Despite%250Atheir%2520effectiveness%252C%2520the%2520growth%2520of%2520historical%2520interactions%2520introduces%250Asignificant%2520scalability%2520issues%252C%2520particularly%2520in%2520industry%2520scenarios.%2520To%2520address%250Athis%2520limitation%252C%2520we%2520propose%2520ScaDyG%252C%2520with%2520the%2520core%2520idea%2520of%2520designing%2520a%250Atime-aware%2520scalable%2520learning%2520paradigm%2520as%2520follows%253A%25201%2529%2520Time-aware%2520Topology%250AReformulation%253A%2520ScaDyG%2520first%2520segments%2520historical%2520interactions%2520into%2520time%2520steps%250A%2528intra%2520and%2520inter%2529%2520based%2520on%2520dynamic%2520modeling%252C%2520enabling%2520weight-free%2520and%250Atime-aware%2520graph%2520propagation%2520within%2520pre-processing.%25202%2529%2520Dynamic%2520Temporal%250AEncoding%253A%2520To%2520further%2520achieve%2520fine-grained%2520graph%2520propagation%2520within%2520time%2520steps%252C%250AScaDyG%2520integrates%2520temporal%2520encoding%2520through%2520a%2520combination%2520of%2520exponential%250Afunctions%2520in%2520a%2520scalable%2520manner.%25203%2529%2520Hypernetwork-driven%2520Message%2520Aggregation%253A%250AAfter%2520obtaining%2520the%2520propagated%2520features%2520%2528i.e.%252C%2520messages%2529%252C%2520ScaDyG%2520utilizes%250Ahypernetwork%2520to%2520analyze%2520historical%2520dependencies%252C%2520implementing%2520node-wise%250Arepresentation%2520by%2520an%2520adaptive%2520temporal%2520fusion.%2520Extensive%2520experiments%2520on%252012%250Adatasets%2520demonstrate%2520that%2520ScaDyG%2520performs%2520comparably%2520well%2520or%2520even%2520outperforms%250Aother%2520SOTA%2520methods%2520in%2520both%2520node%2520and%2520link-level%2520downstream%2520tasks%252C%2520with%2520fewer%250Alearnable%2520parameters%2520and%2520higher%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScaDyG%3AA%20New%20Paradigm%20for%20Large-scale%20Dynamic%20Graph%20Learning&entry.906535625=Xiang%20Wu%20and%20Xunkai%20Li%20and%20Rong-Hua%20Li%20and%20Kangfei%20Zhao%20and%20Guoren%20Wang&entry.1292438233=%20%20Dynamic%20graphs%20%28DGs%29%2C%20which%20capture%20time-evolving%20relationships%20between%20graph%0Aentities%2C%20have%20widespread%20real-world%20applications.%20To%20efficiently%20encode%20DGs%0Afor%20downstream%20tasks%2C%20most%20dynamic%20graph%20neural%20networks%20follow%20the%20traditional%0Amessage-passing%20mechanism%20and%20extend%20it%20with%20time-based%20techniques.%20Despite%0Atheir%20effectiveness%2C%20the%20growth%20of%20historical%20interactions%20introduces%0Asignificant%20scalability%20issues%2C%20particularly%20in%20industry%20scenarios.%20To%20address%0Athis%20limitation%2C%20we%20propose%20ScaDyG%2C%20with%20the%20core%20idea%20of%20designing%20a%0Atime-aware%20scalable%20learning%20paradigm%20as%20follows%3A%201%29%20Time-aware%20Topology%0AReformulation%3A%20ScaDyG%20first%20segments%20historical%20interactions%20into%20time%20steps%0A%28intra%20and%20inter%29%20based%20on%20dynamic%20modeling%2C%20enabling%20weight-free%20and%0Atime-aware%20graph%20propagation%20within%20pre-processing.%202%29%20Dynamic%20Temporal%0AEncoding%3A%20To%20further%20achieve%20fine-grained%20graph%20propagation%20within%20time%20steps%2C%0AScaDyG%20integrates%20temporal%20encoding%20through%20a%20combination%20of%20exponential%0Afunctions%20in%20a%20scalable%20manner.%203%29%20Hypernetwork-driven%20Message%20Aggregation%3A%0AAfter%20obtaining%20the%20propagated%20features%20%28i.e.%2C%20messages%29%2C%20ScaDyG%20utilizes%0Ahypernetwork%20to%20analyze%20historical%20dependencies%2C%20implementing%20node-wise%0Arepresentation%20by%20an%20adaptive%20temporal%20fusion.%20Extensive%20experiments%20on%2012%0Adatasets%20demonstrate%20that%20ScaDyG%20performs%20comparably%20well%20or%20even%20outperforms%0Aother%20SOTA%20methods%20in%20both%20node%20and%20link-level%20downstream%20tasks%2C%20with%20fewer%0Alearnable%20parameters%20and%20higher%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16002v1&entry.124074799=Read"},
{"title": "Generative AI for Lyapunov Optimization Theory in UAV-based Low-Altitude\n  Economy Networking", "author": "Zhang Liu and Dusit Niyato and Jiacheng Wang and Geng Sun and Lianfen Huang and Zhibin Gao and Xianbin Wang", "abstract": "  Lyapunov optimization theory has recently emerged as a powerful mathematical\nframework for solving complex stochastic optimization problems by transforming\nlong-term objectives into a sequence of real-time short-term decisions while\nensuring system stability. This theory is particularly valuable in unmanned\naerial vehicle (UAV)-based low-altitude economy (LAE) networking scenarios,\nwhere it could effectively address inherent challenges of dynamic network\nconditions, multiple optimization objectives, and stability requirements.\nRecently, generative artificial intelligence (GenAI) has garnered significant\nattention for its unprecedented capability to generate diverse digital content.\nExtending beyond content generation, in this paper, we propose a framework\nintegrating generative diffusion models with reinforcement learning to address\nLyapunov optimization problems in UAV-based LAE networking. We begin by\nintroducing the fundamentals of Lyapunov optimization theory and analyzing the\nlimitations of both conventional methods and traditional AI-enabled approaches.\nWe then examine various GenAI models and comprehensively analyze their\npotential contributions to Lyapunov optimization. Subsequently, we develop a\nLyapunov-guided generative diffusion model-based reinforcement learning\nframework and validate its effectiveness through a UAV-based LAE networking\ncase study. Finally, we outline several directions for future research.\n", "link": "http://arxiv.org/abs/2501.15928v1", "date": "2025-01-27", "relevancy": 2.1291, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5438}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5273}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20for%20Lyapunov%20Optimization%20Theory%20in%20UAV-based%20Low-Altitude%0A%20%20Economy%20Networking&body=Title%3A%20Generative%20AI%20for%20Lyapunov%20Optimization%20Theory%20in%20UAV-based%20Low-Altitude%0A%20%20Economy%20Networking%0AAuthor%3A%20Zhang%20Liu%20and%20Dusit%20Niyato%20and%20Jiacheng%20Wang%20and%20Geng%20Sun%20and%20Lianfen%20Huang%20and%20Zhibin%20Gao%20and%20Xianbin%20Wang%0AAbstract%3A%20%20%20Lyapunov%20optimization%20theory%20has%20recently%20emerged%20as%20a%20powerful%20mathematical%0Aframework%20for%20solving%20complex%20stochastic%20optimization%20problems%20by%20transforming%0Along-term%20objectives%20into%20a%20sequence%20of%20real-time%20short-term%20decisions%20while%0Aensuring%20system%20stability.%20This%20theory%20is%20particularly%20valuable%20in%20unmanned%0Aaerial%20vehicle%20%28UAV%29-based%20low-altitude%20economy%20%28LAE%29%20networking%20scenarios%2C%0Awhere%20it%20could%20effectively%20address%20inherent%20challenges%20of%20dynamic%20network%0Aconditions%2C%20multiple%20optimization%20objectives%2C%20and%20stability%20requirements.%0ARecently%2C%20generative%20artificial%20intelligence%20%28GenAI%29%20has%20garnered%20significant%0Aattention%20for%20its%20unprecedented%20capability%20to%20generate%20diverse%20digital%20content.%0AExtending%20beyond%20content%20generation%2C%20in%20this%20paper%2C%20we%20propose%20a%20framework%0Aintegrating%20generative%20diffusion%20models%20with%20reinforcement%20learning%20to%20address%0ALyapunov%20optimization%20problems%20in%20UAV-based%20LAE%20networking.%20We%20begin%20by%0Aintroducing%20the%20fundamentals%20of%20Lyapunov%20optimization%20theory%20and%20analyzing%20the%0Alimitations%20of%20both%20conventional%20methods%20and%20traditional%20AI-enabled%20approaches.%0AWe%20then%20examine%20various%20GenAI%20models%20and%20comprehensively%20analyze%20their%0Apotential%20contributions%20to%20Lyapunov%20optimization.%20Subsequently%2C%20we%20develop%20a%0ALyapunov-guided%20generative%20diffusion%20model-based%20reinforcement%20learning%0Aframework%20and%20validate%20its%20effectiveness%20through%20a%20UAV-based%20LAE%20networking%0Acase%20study.%20Finally%2C%20we%20outline%20several%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520for%2520Lyapunov%2520Optimization%2520Theory%2520in%2520UAV-based%2520Low-Altitude%250A%2520%2520Economy%2520Networking%26entry.906535625%3DZhang%2520Liu%2520and%2520Dusit%2520Niyato%2520and%2520Jiacheng%2520Wang%2520and%2520Geng%2520Sun%2520and%2520Lianfen%2520Huang%2520and%2520Zhibin%2520Gao%2520and%2520Xianbin%2520Wang%26entry.1292438233%3D%2520%2520Lyapunov%2520optimization%2520theory%2520has%2520recently%2520emerged%2520as%2520a%2520powerful%2520mathematical%250Aframework%2520for%2520solving%2520complex%2520stochastic%2520optimization%2520problems%2520by%2520transforming%250Along-term%2520objectives%2520into%2520a%2520sequence%2520of%2520real-time%2520short-term%2520decisions%2520while%250Aensuring%2520system%2520stability.%2520This%2520theory%2520is%2520particularly%2520valuable%2520in%2520unmanned%250Aaerial%2520vehicle%2520%2528UAV%2529-based%2520low-altitude%2520economy%2520%2528LAE%2529%2520networking%2520scenarios%252C%250Awhere%2520it%2520could%2520effectively%2520address%2520inherent%2520challenges%2520of%2520dynamic%2520network%250Aconditions%252C%2520multiple%2520optimization%2520objectives%252C%2520and%2520stability%2520requirements.%250ARecently%252C%2520generative%2520artificial%2520intelligence%2520%2528GenAI%2529%2520has%2520garnered%2520significant%250Aattention%2520for%2520its%2520unprecedented%2520capability%2520to%2520generate%2520diverse%2520digital%2520content.%250AExtending%2520beyond%2520content%2520generation%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520framework%250Aintegrating%2520generative%2520diffusion%2520models%2520with%2520reinforcement%2520learning%2520to%2520address%250ALyapunov%2520optimization%2520problems%2520in%2520UAV-based%2520LAE%2520networking.%2520We%2520begin%2520by%250Aintroducing%2520the%2520fundamentals%2520of%2520Lyapunov%2520optimization%2520theory%2520and%2520analyzing%2520the%250Alimitations%2520of%2520both%2520conventional%2520methods%2520and%2520traditional%2520AI-enabled%2520approaches.%250AWe%2520then%2520examine%2520various%2520GenAI%2520models%2520and%2520comprehensively%2520analyze%2520their%250Apotential%2520contributions%2520to%2520Lyapunov%2520optimization.%2520Subsequently%252C%2520we%2520develop%2520a%250ALyapunov-guided%2520generative%2520diffusion%2520model-based%2520reinforcement%2520learning%250Aframework%2520and%2520validate%2520its%2520effectiveness%2520through%2520a%2520UAV-based%2520LAE%2520networking%250Acase%2520study.%2520Finally%252C%2520we%2520outline%2520several%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20for%20Lyapunov%20Optimization%20Theory%20in%20UAV-based%20Low-Altitude%0A%20%20Economy%20Networking&entry.906535625=Zhang%20Liu%20and%20Dusit%20Niyato%20and%20Jiacheng%20Wang%20and%20Geng%20Sun%20and%20Lianfen%20Huang%20and%20Zhibin%20Gao%20and%20Xianbin%20Wang&entry.1292438233=%20%20Lyapunov%20optimization%20theory%20has%20recently%20emerged%20as%20a%20powerful%20mathematical%0Aframework%20for%20solving%20complex%20stochastic%20optimization%20problems%20by%20transforming%0Along-term%20objectives%20into%20a%20sequence%20of%20real-time%20short-term%20decisions%20while%0Aensuring%20system%20stability.%20This%20theory%20is%20particularly%20valuable%20in%20unmanned%0Aaerial%20vehicle%20%28UAV%29-based%20low-altitude%20economy%20%28LAE%29%20networking%20scenarios%2C%0Awhere%20it%20could%20effectively%20address%20inherent%20challenges%20of%20dynamic%20network%0Aconditions%2C%20multiple%20optimization%20objectives%2C%20and%20stability%20requirements.%0ARecently%2C%20generative%20artificial%20intelligence%20%28GenAI%29%20has%20garnered%20significant%0Aattention%20for%20its%20unprecedented%20capability%20to%20generate%20diverse%20digital%20content.%0AExtending%20beyond%20content%20generation%2C%20in%20this%20paper%2C%20we%20propose%20a%20framework%0Aintegrating%20generative%20diffusion%20models%20with%20reinforcement%20learning%20to%20address%0ALyapunov%20optimization%20problems%20in%20UAV-based%20LAE%20networking.%20We%20begin%20by%0Aintroducing%20the%20fundamentals%20of%20Lyapunov%20optimization%20theory%20and%20analyzing%20the%0Alimitations%20of%20both%20conventional%20methods%20and%20traditional%20AI-enabled%20approaches.%0AWe%20then%20examine%20various%20GenAI%20models%20and%20comprehensively%20analyze%20their%0Apotential%20contributions%20to%20Lyapunov%20optimization.%20Subsequently%2C%20we%20develop%20a%0ALyapunov-guided%20generative%20diffusion%20model-based%20reinforcement%20learning%0Aframework%20and%20validate%20its%20effectiveness%20through%20a%20UAV-based%20LAE%20networking%0Acase%20study.%20Finally%2C%20we%20outline%20several%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15928v1&entry.124074799=Read"},
{"title": "Skeleton-Guided-Translation: A Benchmarking Framework for Code\n  Repository Translation with Fine-Grained Quality Evaluation", "author": "Xing Zhang and Jiaheng Wen and Fangkai Yang and Pu Zhao and Yu Kang and Junhao Wang and Maoquan Wang and Yufan Huang and Elsie Nallipogu and Qingwei Lin and Yingnong Dang and Saravan Rajmohan and Dongmei Zhang and Qi Zhang", "abstract": "  The advancement of large language models has intensified the need to\nmodernize enterprise applications and migrate legacy systems to secure,\nversatile languages. However, existing code translation benchmarks primarily\nfocus on individual functions, overlooking the complexities involved in\ntranslating entire repositories, such as maintaining inter-module coherence and\nmanaging dependencies. While some recent repository-level translation\nbenchmarks attempt to address these challenges, they still face limitations,\nincluding poor maintainability and overly coarse evaluation granularity, which\nmake them less developer-friendly. We introduce Skeleton-Guided-Translation, a\nframework for repository-level Java to C# code translation with fine-grained\nquality evaluation. It uses a two-step process: first translating the\nrepository's structural \"skeletons\", then translating the full repository\nguided by these skeletons. Building on this, we present TRANSREPO-BENCH, a\nbenchmark of high quality open-source Java repositories and their corresponding\nC# skeletons, including matching unit tests and build configurations. Our unit\ntests are fixed and can be applied across multiple or incremental translations\nwithout manual adjustments, enhancing automation and scalability in\nevaluations. Additionally, we develop fine-grained evaluation metrics that\nassess translation quality at the individual test case level, addressing\ntraditional binary metrics' inability to distinguish when build failures cause\nall tests to fail. Evaluations using TRANSREPO-BENCH highlight key challenges\nand advance more accurate repository level code translation.\n", "link": "http://arxiv.org/abs/2501.16050v1", "date": "2025-01-27", "relevancy": 2.1287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.426}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skeleton-Guided-Translation%3A%20A%20Benchmarking%20Framework%20for%20Code%0A%20%20Repository%20Translation%20with%20Fine-Grained%20Quality%20Evaluation&body=Title%3A%20Skeleton-Guided-Translation%3A%20A%20Benchmarking%20Framework%20for%20Code%0A%20%20Repository%20Translation%20with%20Fine-Grained%20Quality%20Evaluation%0AAuthor%3A%20Xing%20Zhang%20and%20Jiaheng%20Wen%20and%20Fangkai%20Yang%20and%20Pu%20Zhao%20and%20Yu%20Kang%20and%20Junhao%20Wang%20and%20Maoquan%20Wang%20and%20Yufan%20Huang%20and%20Elsie%20Nallipogu%20and%20Qingwei%20Lin%20and%20Yingnong%20Dang%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20The%20advancement%20of%20large%20language%20models%20has%20intensified%20the%20need%20to%0Amodernize%20enterprise%20applications%20and%20migrate%20legacy%20systems%20to%20secure%2C%0Aversatile%20languages.%20However%2C%20existing%20code%20translation%20benchmarks%20primarily%0Afocus%20on%20individual%20functions%2C%20overlooking%20the%20complexities%20involved%20in%0Atranslating%20entire%20repositories%2C%20such%20as%20maintaining%20inter-module%20coherence%20and%0Amanaging%20dependencies.%20While%20some%20recent%20repository-level%20translation%0Abenchmarks%20attempt%20to%20address%20these%20challenges%2C%20they%20still%20face%20limitations%2C%0Aincluding%20poor%20maintainability%20and%20overly%20coarse%20evaluation%20granularity%2C%20which%0Amake%20them%20less%20developer-friendly.%20We%20introduce%20Skeleton-Guided-Translation%2C%20a%0Aframework%20for%20repository-level%20Java%20to%20C%23%20code%20translation%20with%20fine-grained%0Aquality%20evaluation.%20It%20uses%20a%20two-step%20process%3A%20first%20translating%20the%0Arepository%27s%20structural%20%22skeletons%22%2C%20then%20translating%20the%20full%20repository%0Aguided%20by%20these%20skeletons.%20Building%20on%20this%2C%20we%20present%20TRANSREPO-BENCH%2C%20a%0Abenchmark%20of%20high%20quality%20open-source%20Java%20repositories%20and%20their%20corresponding%0AC%23%20skeletons%2C%20including%20matching%20unit%20tests%20and%20build%20configurations.%20Our%20unit%0Atests%20are%20fixed%20and%20can%20be%20applied%20across%20multiple%20or%20incremental%20translations%0Awithout%20manual%20adjustments%2C%20enhancing%20automation%20and%20scalability%20in%0Aevaluations.%20Additionally%2C%20we%20develop%20fine-grained%20evaluation%20metrics%20that%0Aassess%20translation%20quality%20at%20the%20individual%20test%20case%20level%2C%20addressing%0Atraditional%20binary%20metrics%27%20inability%20to%20distinguish%20when%20build%20failures%20cause%0Aall%20tests%20to%20fail.%20Evaluations%20using%20TRANSREPO-BENCH%20highlight%20key%20challenges%0Aand%20advance%20more%20accurate%20repository%20level%20code%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkeleton-Guided-Translation%253A%2520A%2520Benchmarking%2520Framework%2520for%2520Code%250A%2520%2520Repository%2520Translation%2520with%2520Fine-Grained%2520Quality%2520Evaluation%26entry.906535625%3DXing%2520Zhang%2520and%2520Jiaheng%2520Wen%2520and%2520Fangkai%2520Yang%2520and%2520Pu%2520Zhao%2520and%2520Yu%2520Kang%2520and%2520Junhao%2520Wang%2520and%2520Maoquan%2520Wang%2520and%2520Yufan%2520Huang%2520and%2520Elsie%2520Nallipogu%2520and%2520Qingwei%2520Lin%2520and%2520Yingnong%2520Dang%2520and%2520Saravan%2520Rajmohan%2520and%2520Dongmei%2520Zhang%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520large%2520language%2520models%2520has%2520intensified%2520the%2520need%2520to%250Amodernize%2520enterprise%2520applications%2520and%2520migrate%2520legacy%2520systems%2520to%2520secure%252C%250Aversatile%2520languages.%2520However%252C%2520existing%2520code%2520translation%2520benchmarks%2520primarily%250Afocus%2520on%2520individual%2520functions%252C%2520overlooking%2520the%2520complexities%2520involved%2520in%250Atranslating%2520entire%2520repositories%252C%2520such%2520as%2520maintaining%2520inter-module%2520coherence%2520and%250Amanaging%2520dependencies.%2520While%2520some%2520recent%2520repository-level%2520translation%250Abenchmarks%2520attempt%2520to%2520address%2520these%2520challenges%252C%2520they%2520still%2520face%2520limitations%252C%250Aincluding%2520poor%2520maintainability%2520and%2520overly%2520coarse%2520evaluation%2520granularity%252C%2520which%250Amake%2520them%2520less%2520developer-friendly.%2520We%2520introduce%2520Skeleton-Guided-Translation%252C%2520a%250Aframework%2520for%2520repository-level%2520Java%2520to%2520C%2523%2520code%2520translation%2520with%2520fine-grained%250Aquality%2520evaluation.%2520It%2520uses%2520a%2520two-step%2520process%253A%2520first%2520translating%2520the%250Arepository%2527s%2520structural%2520%2522skeletons%2522%252C%2520then%2520translating%2520the%2520full%2520repository%250Aguided%2520by%2520these%2520skeletons.%2520Building%2520on%2520this%252C%2520we%2520present%2520TRANSREPO-BENCH%252C%2520a%250Abenchmark%2520of%2520high%2520quality%2520open-source%2520Java%2520repositories%2520and%2520their%2520corresponding%250AC%2523%2520skeletons%252C%2520including%2520matching%2520unit%2520tests%2520and%2520build%2520configurations.%2520Our%2520unit%250Atests%2520are%2520fixed%2520and%2520can%2520be%2520applied%2520across%2520multiple%2520or%2520incremental%2520translations%250Awithout%2520manual%2520adjustments%252C%2520enhancing%2520automation%2520and%2520scalability%2520in%250Aevaluations.%2520Additionally%252C%2520we%2520develop%2520fine-grained%2520evaluation%2520metrics%2520that%250Aassess%2520translation%2520quality%2520at%2520the%2520individual%2520test%2520case%2520level%252C%2520addressing%250Atraditional%2520binary%2520metrics%2527%2520inability%2520to%2520distinguish%2520when%2520build%2520failures%2520cause%250Aall%2520tests%2520to%2520fail.%2520Evaluations%2520using%2520TRANSREPO-BENCH%2520highlight%2520key%2520challenges%250Aand%2520advance%2520more%2520accurate%2520repository%2520level%2520code%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skeleton-Guided-Translation%3A%20A%20Benchmarking%20Framework%20for%20Code%0A%20%20Repository%20Translation%20with%20Fine-Grained%20Quality%20Evaluation&entry.906535625=Xing%20Zhang%20and%20Jiaheng%20Wen%20and%20Fangkai%20Yang%20and%20Pu%20Zhao%20and%20Yu%20Kang%20and%20Junhao%20Wang%20and%20Maoquan%20Wang%20and%20Yufan%20Huang%20and%20Elsie%20Nallipogu%20and%20Qingwei%20Lin%20and%20Yingnong%20Dang%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang&entry.1292438233=%20%20The%20advancement%20of%20large%20language%20models%20has%20intensified%20the%20need%20to%0Amodernize%20enterprise%20applications%20and%20migrate%20legacy%20systems%20to%20secure%2C%0Aversatile%20languages.%20However%2C%20existing%20code%20translation%20benchmarks%20primarily%0Afocus%20on%20individual%20functions%2C%20overlooking%20the%20complexities%20involved%20in%0Atranslating%20entire%20repositories%2C%20such%20as%20maintaining%20inter-module%20coherence%20and%0Amanaging%20dependencies.%20While%20some%20recent%20repository-level%20translation%0Abenchmarks%20attempt%20to%20address%20these%20challenges%2C%20they%20still%20face%20limitations%2C%0Aincluding%20poor%20maintainability%20and%20overly%20coarse%20evaluation%20granularity%2C%20which%0Amake%20them%20less%20developer-friendly.%20We%20introduce%20Skeleton-Guided-Translation%2C%20a%0Aframework%20for%20repository-level%20Java%20to%20C%23%20code%20translation%20with%20fine-grained%0Aquality%20evaluation.%20It%20uses%20a%20two-step%20process%3A%20first%20translating%20the%0Arepository%27s%20structural%20%22skeletons%22%2C%20then%20translating%20the%20full%20repository%0Aguided%20by%20these%20skeletons.%20Building%20on%20this%2C%20we%20present%20TRANSREPO-BENCH%2C%20a%0Abenchmark%20of%20high%20quality%20open-source%20Java%20repositories%20and%20their%20corresponding%0AC%23%20skeletons%2C%20including%20matching%20unit%20tests%20and%20build%20configurations.%20Our%20unit%0Atests%20are%20fixed%20and%20can%20be%20applied%20across%20multiple%20or%20incremental%20translations%0Awithout%20manual%20adjustments%2C%20enhancing%20automation%20and%20scalability%20in%0Aevaluations.%20Additionally%2C%20we%20develop%20fine-grained%20evaluation%20metrics%20that%0Aassess%20translation%20quality%20at%20the%20individual%20test%20case%20level%2C%20addressing%0Atraditional%20binary%20metrics%27%20inability%20to%20distinguish%20when%20build%20failures%20cause%0Aall%20tests%20to%20fail.%20Evaluations%20using%20TRANSREPO-BENCH%20highlight%20key%20challenges%0Aand%20advance%20more%20accurate%20repository%20level%20code%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16050v1&entry.124074799=Read"},
{"title": "Separate This, and All of these Things Around It: Music Source\n  Separation via Hyperellipsoidal Queries", "author": "Karn N. Watcharasupat and Alexander Lerch", "abstract": "  Music source separation is an audio-to-audio retrieval task of extracting one\nor more constituent components, or composites thereof, from a musical audio\nmixture. Each of these constituent components is often referred to as a \"stem\"\nin literature. Historically, music source separation has been dominated by a\nstem-based paradigm, leading to most state-of-the-art systems being either a\ncollection of single-stem extraction models, or a tightly coupled system with a\nfixed, difficult-to-modify, set of supported stems. Combined with the limited\ndata availability, advances in music source separation have thus been mostly\nlimited to the \"VDBO\" set of stems: \\textit{vocals}, \\textit{drum},\n\\textit{bass}, and the catch-all \\textit{others}. Recent work in music source\nseparation has begun to challenge the fixed-stem paradigm, moving towards\nmodels able to extract any musical sound as long as this target type of sound\ncould be specified to the model as an additional query input. We generalize\nthis idea to a \\textit{query-by-region} source separation system, specifying\nthe target based on the query regardless of how many sound sources or which\nsound classes are contained within it. To do so, we propose the use of\nhyperellipsoidal regions as queries to allow for an intuitive yet easily\nparametrizable approach to specifying both the target (location) as well as its\nspread. Evaluation of the proposed system on the MoisesDB dataset demonstrated\nstate-of-the-art performance of the proposed system both in terms of\nsignal-to-noise ratios and retrieval metrics.\n", "link": "http://arxiv.org/abs/2501.16171v1", "date": "2025-01-27", "relevancy": 2.1263, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4346}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Separate%20This%2C%20and%20All%20of%20these%20Things%20Around%20It%3A%20Music%20Source%0A%20%20Separation%20via%20Hyperellipsoidal%20Queries&body=Title%3A%20Separate%20This%2C%20and%20All%20of%20these%20Things%20Around%20It%3A%20Music%20Source%0A%20%20Separation%20via%20Hyperellipsoidal%20Queries%0AAuthor%3A%20Karn%20N.%20Watcharasupat%20and%20Alexander%20Lerch%0AAbstract%3A%20%20%20Music%20source%20separation%20is%20an%20audio-to-audio%20retrieval%20task%20of%20extracting%20one%0Aor%20more%20constituent%20components%2C%20or%20composites%20thereof%2C%20from%20a%20musical%20audio%0Amixture.%20Each%20of%20these%20constituent%20components%20is%20often%20referred%20to%20as%20a%20%22stem%22%0Ain%20literature.%20Historically%2C%20music%20source%20separation%20has%20been%20dominated%20by%20a%0Astem-based%20paradigm%2C%20leading%20to%20most%20state-of-the-art%20systems%20being%20either%20a%0Acollection%20of%20single-stem%20extraction%20models%2C%20or%20a%20tightly%20coupled%20system%20with%20a%0Afixed%2C%20difficult-to-modify%2C%20set%20of%20supported%20stems.%20Combined%20with%20the%20limited%0Adata%20availability%2C%20advances%20in%20music%20source%20separation%20have%20thus%20been%20mostly%0Alimited%20to%20the%20%22VDBO%22%20set%20of%20stems%3A%20%5Ctextit%7Bvocals%7D%2C%20%5Ctextit%7Bdrum%7D%2C%0A%5Ctextit%7Bbass%7D%2C%20and%20the%20catch-all%20%5Ctextit%7Bothers%7D.%20Recent%20work%20in%20music%20source%0Aseparation%20has%20begun%20to%20challenge%20the%20fixed-stem%20paradigm%2C%20moving%20towards%0Amodels%20able%20to%20extract%20any%20musical%20sound%20as%20long%20as%20this%20target%20type%20of%20sound%0Acould%20be%20specified%20to%20the%20model%20as%20an%20additional%20query%20input.%20We%20generalize%0Athis%20idea%20to%20a%20%5Ctextit%7Bquery-by-region%7D%20source%20separation%20system%2C%20specifying%0Athe%20target%20based%20on%20the%20query%20regardless%20of%20how%20many%20sound%20sources%20or%20which%0Asound%20classes%20are%20contained%20within%20it.%20To%20do%20so%2C%20we%20propose%20the%20use%20of%0Ahyperellipsoidal%20regions%20as%20queries%20to%20allow%20for%20an%20intuitive%20yet%20easily%0Aparametrizable%20approach%20to%20specifying%20both%20the%20target%20%28location%29%20as%20well%20as%20its%0Aspread.%20Evaluation%20of%20the%20proposed%20system%20on%20the%20MoisesDB%20dataset%20demonstrated%0Astate-of-the-art%20performance%20of%20the%20proposed%20system%20both%20in%20terms%20of%0Asignal-to-noise%20ratios%20and%20retrieval%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeparate%2520This%252C%2520and%2520All%2520of%2520these%2520Things%2520Around%2520It%253A%2520Music%2520Source%250A%2520%2520Separation%2520via%2520Hyperellipsoidal%2520Queries%26entry.906535625%3DKarn%2520N.%2520Watcharasupat%2520and%2520Alexander%2520Lerch%26entry.1292438233%3D%2520%2520Music%2520source%2520separation%2520is%2520an%2520audio-to-audio%2520retrieval%2520task%2520of%2520extracting%2520one%250Aor%2520more%2520constituent%2520components%252C%2520or%2520composites%2520thereof%252C%2520from%2520a%2520musical%2520audio%250Amixture.%2520Each%2520of%2520these%2520constituent%2520components%2520is%2520often%2520referred%2520to%2520as%2520a%2520%2522stem%2522%250Ain%2520literature.%2520Historically%252C%2520music%2520source%2520separation%2520has%2520been%2520dominated%2520by%2520a%250Astem-based%2520paradigm%252C%2520leading%2520to%2520most%2520state-of-the-art%2520systems%2520being%2520either%2520a%250Acollection%2520of%2520single-stem%2520extraction%2520models%252C%2520or%2520a%2520tightly%2520coupled%2520system%2520with%2520a%250Afixed%252C%2520difficult-to-modify%252C%2520set%2520of%2520supported%2520stems.%2520Combined%2520with%2520the%2520limited%250Adata%2520availability%252C%2520advances%2520in%2520music%2520source%2520separation%2520have%2520thus%2520been%2520mostly%250Alimited%2520to%2520the%2520%2522VDBO%2522%2520set%2520of%2520stems%253A%2520%255Ctextit%257Bvocals%257D%252C%2520%255Ctextit%257Bdrum%257D%252C%250A%255Ctextit%257Bbass%257D%252C%2520and%2520the%2520catch-all%2520%255Ctextit%257Bothers%257D.%2520Recent%2520work%2520in%2520music%2520source%250Aseparation%2520has%2520begun%2520to%2520challenge%2520the%2520fixed-stem%2520paradigm%252C%2520moving%2520towards%250Amodels%2520able%2520to%2520extract%2520any%2520musical%2520sound%2520as%2520long%2520as%2520this%2520target%2520type%2520of%2520sound%250Acould%2520be%2520specified%2520to%2520the%2520model%2520as%2520an%2520additional%2520query%2520input.%2520We%2520generalize%250Athis%2520idea%2520to%2520a%2520%255Ctextit%257Bquery-by-region%257D%2520source%2520separation%2520system%252C%2520specifying%250Athe%2520target%2520based%2520on%2520the%2520query%2520regardless%2520of%2520how%2520many%2520sound%2520sources%2520or%2520which%250Asound%2520classes%2520are%2520contained%2520within%2520it.%2520To%2520do%2520so%252C%2520we%2520propose%2520the%2520use%2520of%250Ahyperellipsoidal%2520regions%2520as%2520queries%2520to%2520allow%2520for%2520an%2520intuitive%2520yet%2520easily%250Aparametrizable%2520approach%2520to%2520specifying%2520both%2520the%2520target%2520%2528location%2529%2520as%2520well%2520as%2520its%250Aspread.%2520Evaluation%2520of%2520the%2520proposed%2520system%2520on%2520the%2520MoisesDB%2520dataset%2520demonstrated%250Astate-of-the-art%2520performance%2520of%2520the%2520proposed%2520system%2520both%2520in%2520terms%2520of%250Asignal-to-noise%2520ratios%2520and%2520retrieval%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separate%20This%2C%20and%20All%20of%20these%20Things%20Around%20It%3A%20Music%20Source%0A%20%20Separation%20via%20Hyperellipsoidal%20Queries&entry.906535625=Karn%20N.%20Watcharasupat%20and%20Alexander%20Lerch&entry.1292438233=%20%20Music%20source%20separation%20is%20an%20audio-to-audio%20retrieval%20task%20of%20extracting%20one%0Aor%20more%20constituent%20components%2C%20or%20composites%20thereof%2C%20from%20a%20musical%20audio%0Amixture.%20Each%20of%20these%20constituent%20components%20is%20often%20referred%20to%20as%20a%20%22stem%22%0Ain%20literature.%20Historically%2C%20music%20source%20separation%20has%20been%20dominated%20by%20a%0Astem-based%20paradigm%2C%20leading%20to%20most%20state-of-the-art%20systems%20being%20either%20a%0Acollection%20of%20single-stem%20extraction%20models%2C%20or%20a%20tightly%20coupled%20system%20with%20a%0Afixed%2C%20difficult-to-modify%2C%20set%20of%20supported%20stems.%20Combined%20with%20the%20limited%0Adata%20availability%2C%20advances%20in%20music%20source%20separation%20have%20thus%20been%20mostly%0Alimited%20to%20the%20%22VDBO%22%20set%20of%20stems%3A%20%5Ctextit%7Bvocals%7D%2C%20%5Ctextit%7Bdrum%7D%2C%0A%5Ctextit%7Bbass%7D%2C%20and%20the%20catch-all%20%5Ctextit%7Bothers%7D.%20Recent%20work%20in%20music%20source%0Aseparation%20has%20begun%20to%20challenge%20the%20fixed-stem%20paradigm%2C%20moving%20towards%0Amodels%20able%20to%20extract%20any%20musical%20sound%20as%20long%20as%20this%20target%20type%20of%20sound%0Acould%20be%20specified%20to%20the%20model%20as%20an%20additional%20query%20input.%20We%20generalize%0Athis%20idea%20to%20a%20%5Ctextit%7Bquery-by-region%7D%20source%20separation%20system%2C%20specifying%0Athe%20target%20based%20on%20the%20query%20regardless%20of%20how%20many%20sound%20sources%20or%20which%0Asound%20classes%20are%20contained%20within%20it.%20To%20do%20so%2C%20we%20propose%20the%20use%20of%0Ahyperellipsoidal%20regions%20as%20queries%20to%20allow%20for%20an%20intuitive%20yet%20easily%0Aparametrizable%20approach%20to%20specifying%20both%20the%20target%20%28location%29%20as%20well%20as%20its%0Aspread.%20Evaluation%20of%20the%20proposed%20system%20on%20the%20MoisesDB%20dataset%20demonstrated%0Astate-of-the-art%20performance%20of%20the%20proposed%20system%20both%20in%20terms%20of%0Asignal-to-noise%20ratios%20and%20retrieval%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16171v1&entry.124074799=Read"},
{"title": "Enhancing Noise-Robust Losses for Large-Scale Noisy Data Learning", "author": "Max Staats and Matthias Thamm and Bernd Rosenow", "abstract": "  Large annotated datasets inevitably contain noisy labels, which poses a major\nchallenge for training deep neural networks as they easily memorize the labels.\nNoise-robust loss functions have emerged as a notable strategy to counteract\nthis issue, but it remains challenging to create a robust loss function which\nis not susceptible to underfitting. Through a quantitative approach, this paper\nexplores the limited overlap between the network output at initialization and\nregions of non-vanishing gradients of bounded loss functions in the initial\nlearning phase. Using these insights, we address underfitting of several noise\nrobust losses with a novel method denoted as logit bias, which adds a real\nnumber $\\epsilon$ to the logit at the position of the correct class. The logit\nbias enables these losses to achieve state-of-the-art results, even on datasets\nlike WebVision, consisting of over a million images from 1000 classes. In\naddition, we demonstrate that our method can be used to determine optimal\nparameters for several loss functions -- without having to train networks.\nRemarkably, our method determines the hyperparameters based on the number of\nclasses, resulting in loss functions which require zero dataset or\nnoise-dependent parameters.\n", "link": "http://arxiv.org/abs/2306.05497v3", "date": "2025-01-27", "relevancy": 2.1076, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5452}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5171}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Noise-Robust%20Losses%20for%20Large-Scale%20Noisy%20Data%20Learning&body=Title%3A%20Enhancing%20Noise-Robust%20Losses%20for%20Large-Scale%20Noisy%20Data%20Learning%0AAuthor%3A%20Max%20Staats%20and%20Matthias%20Thamm%20and%20Bernd%20Rosenow%0AAbstract%3A%20%20%20Large%20annotated%20datasets%20inevitably%20contain%20noisy%20labels%2C%20which%20poses%20a%20major%0Achallenge%20for%20training%20deep%20neural%20networks%20as%20they%20easily%20memorize%20the%20labels.%0ANoise-robust%20loss%20functions%20have%20emerged%20as%20a%20notable%20strategy%20to%20counteract%0Athis%20issue%2C%20but%20it%20remains%20challenging%20to%20create%20a%20robust%20loss%20function%20which%0Ais%20not%20susceptible%20to%20underfitting.%20Through%20a%20quantitative%20approach%2C%20this%20paper%0Aexplores%20the%20limited%20overlap%20between%20the%20network%20output%20at%20initialization%20and%0Aregions%20of%20non-vanishing%20gradients%20of%20bounded%20loss%20functions%20in%20the%20initial%0Alearning%20phase.%20Using%20these%20insights%2C%20we%20address%20underfitting%20of%20several%20noise%0Arobust%20losses%20with%20a%20novel%20method%20denoted%20as%20logit%20bias%2C%20which%20adds%20a%20real%0Anumber%20%24%5Cepsilon%24%20to%20the%20logit%20at%20the%20position%20of%20the%20correct%20class.%20The%20logit%0Abias%20enables%20these%20losses%20to%20achieve%20state-of-the-art%20results%2C%20even%20on%20datasets%0Alike%20WebVision%2C%20consisting%20of%20over%20a%20million%20images%20from%201000%20classes.%20In%0Aaddition%2C%20we%20demonstrate%20that%20our%20method%20can%20be%20used%20to%20determine%20optimal%0Aparameters%20for%20several%20loss%20functions%20--%20without%20having%20to%20train%20networks.%0ARemarkably%2C%20our%20method%20determines%20the%20hyperparameters%20based%20on%20the%20number%20of%0Aclasses%2C%20resulting%20in%20loss%20functions%20which%20require%20zero%20dataset%20or%0Anoise-dependent%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.05497v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Noise-Robust%2520Losses%2520for%2520Large-Scale%2520Noisy%2520Data%2520Learning%26entry.906535625%3DMax%2520Staats%2520and%2520Matthias%2520Thamm%2520and%2520Bernd%2520Rosenow%26entry.1292438233%3D%2520%2520Large%2520annotated%2520datasets%2520inevitably%2520contain%2520noisy%2520labels%252C%2520which%2520poses%2520a%2520major%250Achallenge%2520for%2520training%2520deep%2520neural%2520networks%2520as%2520they%2520easily%2520memorize%2520the%2520labels.%250ANoise-robust%2520loss%2520functions%2520have%2520emerged%2520as%2520a%2520notable%2520strategy%2520to%2520counteract%250Athis%2520issue%252C%2520but%2520it%2520remains%2520challenging%2520to%2520create%2520a%2520robust%2520loss%2520function%2520which%250Ais%2520not%2520susceptible%2520to%2520underfitting.%2520Through%2520a%2520quantitative%2520approach%252C%2520this%2520paper%250Aexplores%2520the%2520limited%2520overlap%2520between%2520the%2520network%2520output%2520at%2520initialization%2520and%250Aregions%2520of%2520non-vanishing%2520gradients%2520of%2520bounded%2520loss%2520functions%2520in%2520the%2520initial%250Alearning%2520phase.%2520Using%2520these%2520insights%252C%2520we%2520address%2520underfitting%2520of%2520several%2520noise%250Arobust%2520losses%2520with%2520a%2520novel%2520method%2520denoted%2520as%2520logit%2520bias%252C%2520which%2520adds%2520a%2520real%250Anumber%2520%2524%255Cepsilon%2524%2520to%2520the%2520logit%2520at%2520the%2520position%2520of%2520the%2520correct%2520class.%2520The%2520logit%250Abias%2520enables%2520these%2520losses%2520to%2520achieve%2520state-of-the-art%2520results%252C%2520even%2520on%2520datasets%250Alike%2520WebVision%252C%2520consisting%2520of%2520over%2520a%2520million%2520images%2520from%25201000%2520classes.%2520In%250Aaddition%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520can%2520be%2520used%2520to%2520determine%2520optimal%250Aparameters%2520for%2520several%2520loss%2520functions%2520--%2520without%2520having%2520to%2520train%2520networks.%250ARemarkably%252C%2520our%2520method%2520determines%2520the%2520hyperparameters%2520based%2520on%2520the%2520number%2520of%250Aclasses%252C%2520resulting%2520in%2520loss%2520functions%2520which%2520require%2520zero%2520dataset%2520or%250Anoise-dependent%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.05497v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Noise-Robust%20Losses%20for%20Large-Scale%20Noisy%20Data%20Learning&entry.906535625=Max%20Staats%20and%20Matthias%20Thamm%20and%20Bernd%20Rosenow&entry.1292438233=%20%20Large%20annotated%20datasets%20inevitably%20contain%20noisy%20labels%2C%20which%20poses%20a%20major%0Achallenge%20for%20training%20deep%20neural%20networks%20as%20they%20easily%20memorize%20the%20labels.%0ANoise-robust%20loss%20functions%20have%20emerged%20as%20a%20notable%20strategy%20to%20counteract%0Athis%20issue%2C%20but%20it%20remains%20challenging%20to%20create%20a%20robust%20loss%20function%20which%0Ais%20not%20susceptible%20to%20underfitting.%20Through%20a%20quantitative%20approach%2C%20this%20paper%0Aexplores%20the%20limited%20overlap%20between%20the%20network%20output%20at%20initialization%20and%0Aregions%20of%20non-vanishing%20gradients%20of%20bounded%20loss%20functions%20in%20the%20initial%0Alearning%20phase.%20Using%20these%20insights%2C%20we%20address%20underfitting%20of%20several%20noise%0Arobust%20losses%20with%20a%20novel%20method%20denoted%20as%20logit%20bias%2C%20which%20adds%20a%20real%0Anumber%20%24%5Cepsilon%24%20to%20the%20logit%20at%20the%20position%20of%20the%20correct%20class.%20The%20logit%0Abias%20enables%20these%20losses%20to%20achieve%20state-of-the-art%20results%2C%20even%20on%20datasets%0Alike%20WebVision%2C%20consisting%20of%20over%20a%20million%20images%20from%201000%20classes.%20In%0Aaddition%2C%20we%20demonstrate%20that%20our%20method%20can%20be%20used%20to%20determine%20optimal%0Aparameters%20for%20several%20loss%20functions%20--%20without%20having%20to%20train%20networks.%0ARemarkably%2C%20our%20method%20determines%20the%20hyperparameters%20based%20on%20the%20number%20of%0Aclasses%2C%20resulting%20in%20loss%20functions%20which%20require%20zero%20dataset%20or%0Anoise-dependent%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05497v3&entry.124074799=Read"},
{"title": "Adaptive Iterative Compression for High-Resolution Files: an Approach\n  Focused on Preserving Visual Quality in Cinematic Workflows", "author": "Leonardo Melo and Filipe Litaiff", "abstract": "  This study presents an iterative adaptive compression model for\nhigh-resolution DPX-derived TIFF files used in cinematographic workflows and\ndigital preservation. The model employs SSIM and PSNR metrics to dynamically\nadjust compression parameters across three configurations (C0, C1, C2),\nachieving storage reductions up to 83.4 % while maintaining high visual\nfidelity (SSIM > 0.95). Validation across three diverse productions - black and\nwhite classic, soft-palette drama, and complex action film - demonstrated the\nmethod's effectiveness in preserving critical visual elements while\nsignificantly reducing storage requirements. Professional evaluators reported\n90% acceptance rate for the optimal C1 configuration, with artifacts remaining\nbelow perceptual threshold in critical areas. Comparative analysis with\nJPEG2000 and H.265 showed superior quality preservation at equivalent\ncompression rates, particularly for high bit-depth content. While requiring\nadditional computational overhead, the method's storage benefits and quality\ncontrol capabilities make it suitable for professional workflows, with\npotential applications in medical imaging and cloud storage optimization.\n", "link": "http://arxiv.org/abs/2501.16319v1", "date": "2025-01-27", "relevancy": 2.1025, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5405}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5301}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Iterative%20Compression%20for%20High-Resolution%20Files%3A%20an%20Approach%0A%20%20Focused%20on%20Preserving%20Visual%20Quality%20in%20Cinematic%20Workflows&body=Title%3A%20Adaptive%20Iterative%20Compression%20for%20High-Resolution%20Files%3A%20an%20Approach%0A%20%20Focused%20on%20Preserving%20Visual%20Quality%20in%20Cinematic%20Workflows%0AAuthor%3A%20Leonardo%20Melo%20and%20Filipe%20Litaiff%0AAbstract%3A%20%20%20This%20study%20presents%20an%20iterative%20adaptive%20compression%20model%20for%0Ahigh-resolution%20DPX-derived%20TIFF%20files%20used%20in%20cinematographic%20workflows%20and%0Adigital%20preservation.%20The%20model%20employs%20SSIM%20and%20PSNR%20metrics%20to%20dynamically%0Aadjust%20compression%20parameters%20across%20three%20configurations%20%28C0%2C%20C1%2C%20C2%29%2C%0Aachieving%20storage%20reductions%20up%20to%2083.4%20%25%20while%20maintaining%20high%20visual%0Afidelity%20%28SSIM%20%3E%200.95%29.%20Validation%20across%20three%20diverse%20productions%20-%20black%20and%0Awhite%20classic%2C%20soft-palette%20drama%2C%20and%20complex%20action%20film%20-%20demonstrated%20the%0Amethod%27s%20effectiveness%20in%20preserving%20critical%20visual%20elements%20while%0Asignificantly%20reducing%20storage%20requirements.%20Professional%20evaluators%20reported%0A90%25%20acceptance%20rate%20for%20the%20optimal%20C1%20configuration%2C%20with%20artifacts%20remaining%0Abelow%20perceptual%20threshold%20in%20critical%20areas.%20Comparative%20analysis%20with%0AJPEG2000%20and%20H.265%20showed%20superior%20quality%20preservation%20at%20equivalent%0Acompression%20rates%2C%20particularly%20for%20high%20bit-depth%20content.%20While%20requiring%0Aadditional%20computational%20overhead%2C%20the%20method%27s%20storage%20benefits%20and%20quality%0Acontrol%20capabilities%20make%20it%20suitable%20for%20professional%20workflows%2C%20with%0Apotential%20applications%20in%20medical%20imaging%20and%20cloud%20storage%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Iterative%2520Compression%2520for%2520High-Resolution%2520Files%253A%2520an%2520Approach%250A%2520%2520Focused%2520on%2520Preserving%2520Visual%2520Quality%2520in%2520Cinematic%2520Workflows%26entry.906535625%3DLeonardo%2520Melo%2520and%2520Filipe%2520Litaiff%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520an%2520iterative%2520adaptive%2520compression%2520model%2520for%250Ahigh-resolution%2520DPX-derived%2520TIFF%2520files%2520used%2520in%2520cinematographic%2520workflows%2520and%250Adigital%2520preservation.%2520The%2520model%2520employs%2520SSIM%2520and%2520PSNR%2520metrics%2520to%2520dynamically%250Aadjust%2520compression%2520parameters%2520across%2520three%2520configurations%2520%2528C0%252C%2520C1%252C%2520C2%2529%252C%250Aachieving%2520storage%2520reductions%2520up%2520to%252083.4%2520%2525%2520while%2520maintaining%2520high%2520visual%250Afidelity%2520%2528SSIM%2520%253E%25200.95%2529.%2520Validation%2520across%2520three%2520diverse%2520productions%2520-%2520black%2520and%250Awhite%2520classic%252C%2520soft-palette%2520drama%252C%2520and%2520complex%2520action%2520film%2520-%2520demonstrated%2520the%250Amethod%2527s%2520effectiveness%2520in%2520preserving%2520critical%2520visual%2520elements%2520while%250Asignificantly%2520reducing%2520storage%2520requirements.%2520Professional%2520evaluators%2520reported%250A90%2525%2520acceptance%2520rate%2520for%2520the%2520optimal%2520C1%2520configuration%252C%2520with%2520artifacts%2520remaining%250Abelow%2520perceptual%2520threshold%2520in%2520critical%2520areas.%2520Comparative%2520analysis%2520with%250AJPEG2000%2520and%2520H.265%2520showed%2520superior%2520quality%2520preservation%2520at%2520equivalent%250Acompression%2520rates%252C%2520particularly%2520for%2520high%2520bit-depth%2520content.%2520While%2520requiring%250Aadditional%2520computational%2520overhead%252C%2520the%2520method%2527s%2520storage%2520benefits%2520and%2520quality%250Acontrol%2520capabilities%2520make%2520it%2520suitable%2520for%2520professional%2520workflows%252C%2520with%250Apotential%2520applications%2520in%2520medical%2520imaging%2520and%2520cloud%2520storage%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Iterative%20Compression%20for%20High-Resolution%20Files%3A%20an%20Approach%0A%20%20Focused%20on%20Preserving%20Visual%20Quality%20in%20Cinematic%20Workflows&entry.906535625=Leonardo%20Melo%20and%20Filipe%20Litaiff&entry.1292438233=%20%20This%20study%20presents%20an%20iterative%20adaptive%20compression%20model%20for%0Ahigh-resolution%20DPX-derived%20TIFF%20files%20used%20in%20cinematographic%20workflows%20and%0Adigital%20preservation.%20The%20model%20employs%20SSIM%20and%20PSNR%20metrics%20to%20dynamically%0Aadjust%20compression%20parameters%20across%20three%20configurations%20%28C0%2C%20C1%2C%20C2%29%2C%0Aachieving%20storage%20reductions%20up%20to%2083.4%20%25%20while%20maintaining%20high%20visual%0Afidelity%20%28SSIM%20%3E%200.95%29.%20Validation%20across%20three%20diverse%20productions%20-%20black%20and%0Awhite%20classic%2C%20soft-palette%20drama%2C%20and%20complex%20action%20film%20-%20demonstrated%20the%0Amethod%27s%20effectiveness%20in%20preserving%20critical%20visual%20elements%20while%0Asignificantly%20reducing%20storage%20requirements.%20Professional%20evaluators%20reported%0A90%25%20acceptance%20rate%20for%20the%20optimal%20C1%20configuration%2C%20with%20artifacts%20remaining%0Abelow%20perceptual%20threshold%20in%20critical%20areas.%20Comparative%20analysis%20with%0AJPEG2000%20and%20H.265%20showed%20superior%20quality%20preservation%20at%20equivalent%0Acompression%20rates%2C%20particularly%20for%20high%20bit-depth%20content.%20While%20requiring%0Aadditional%20computational%20overhead%2C%20the%20method%27s%20storage%20benefits%20and%20quality%0Acontrol%20capabilities%20make%20it%20suitable%20for%20professional%20workflows%2C%20with%0Apotential%20applications%20in%20medical%20imaging%20and%20cloud%20storage%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16319v1&entry.124074799=Read"},
{"title": "Merino: Entropy-driven Design for Generative Language Models on IoT\n  Devices", "author": "Youpeng Zhao and Ming Lin and Huadong Tang and Qiang Wu and Jun Wang", "abstract": "  Generative Large Language Models (LLMs) stand as a revolutionary advancement\nin the modern era of artificial intelligence (AI). However, scaling down LLMs\nfor resource-constrained hardware, such as Internet-of-Things (IoT) devices\nrequires non-trivial efforts and domain knowledge. In this paper, we propose a\nnovel information-entropy framework for designing mobile-friendly generative\nlanguage models. The whole design procedure involves solving a mathematical\nprogramming (MP) problem, which can be done on the CPU within minutes, making\nit nearly zero-cost. We evaluate our designed models, termed MeRino, across\nfourteen NLP downstream tasks, showing their competitive performance against\nthe state-of-the-art autoregressive transformer models under the mobile\nsetting. Notably, MeRino achieves similar or better performance on both\nlanguage modeling and zero-shot learning tasks, compared to the 350M parameter\nOPT while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model\nsize.\n", "link": "http://arxiv.org/abs/2403.07921v3", "date": "2025-01-27", "relevancy": 2.0886, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5431}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5302}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Merino%3A%20Entropy-driven%20Design%20for%20Generative%20Language%20Models%20on%20IoT%0A%20%20Devices&body=Title%3A%20Merino%3A%20Entropy-driven%20Design%20for%20Generative%20Language%20Models%20on%20IoT%0A%20%20Devices%0AAuthor%3A%20Youpeng%20Zhao%20and%20Ming%20Lin%20and%20Huadong%20Tang%20and%20Qiang%20Wu%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Generative%20Large%20Language%20Models%20%28LLMs%29%20stand%20as%20a%20revolutionary%20advancement%0Ain%20the%20modern%20era%20of%20artificial%20intelligence%20%28AI%29.%20However%2C%20scaling%20down%20LLMs%0Afor%20resource-constrained%20hardware%2C%20such%20as%20Internet-of-Things%20%28IoT%29%20devices%0Arequires%20non-trivial%20efforts%20and%20domain%20knowledge.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20information-entropy%20framework%20for%20designing%20mobile-friendly%20generative%0Alanguage%20models.%20The%20whole%20design%20procedure%20involves%20solving%20a%20mathematical%0Aprogramming%20%28MP%29%20problem%2C%20which%20can%20be%20done%20on%20the%20CPU%20within%20minutes%2C%20making%0Ait%20nearly%20zero-cost.%20We%20evaluate%20our%20designed%20models%2C%20termed%20MeRino%2C%20across%0Afourteen%20NLP%20downstream%20tasks%2C%20showing%20their%20competitive%20performance%20against%0Athe%20state-of-the-art%20autoregressive%20transformer%20models%20under%20the%20mobile%0Asetting.%20Notably%2C%20MeRino%20achieves%20similar%20or%20better%20performance%20on%20both%0Alanguage%20modeling%20and%20zero-shot%20learning%20tasks%2C%20compared%20to%20the%20350M%20parameter%0AOPT%20while%20being%204.9x%20faster%20on%20NVIDIA%20Jetson%20Nano%20with%205.5x%20reduction%20in%20model%0Asize.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07921v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMerino%253A%2520Entropy-driven%2520Design%2520for%2520Generative%2520Language%2520Models%2520on%2520IoT%250A%2520%2520Devices%26entry.906535625%3DYoupeng%2520Zhao%2520and%2520Ming%2520Lin%2520and%2520Huadong%2520Tang%2520and%2520Qiang%2520Wu%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Generative%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520stand%2520as%2520a%2520revolutionary%2520advancement%250Ain%2520the%2520modern%2520era%2520of%2520artificial%2520intelligence%2520%2528AI%2529.%2520However%252C%2520scaling%2520down%2520LLMs%250Afor%2520resource-constrained%2520hardware%252C%2520such%2520as%2520Internet-of-Things%2520%2528IoT%2529%2520devices%250Arequires%2520non-trivial%2520efforts%2520and%2520domain%2520knowledge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520information-entropy%2520framework%2520for%2520designing%2520mobile-friendly%2520generative%250Alanguage%2520models.%2520The%2520whole%2520design%2520procedure%2520involves%2520solving%2520a%2520mathematical%250Aprogramming%2520%2528MP%2529%2520problem%252C%2520which%2520can%2520be%2520done%2520on%2520the%2520CPU%2520within%2520minutes%252C%2520making%250Ait%2520nearly%2520zero-cost.%2520We%2520evaluate%2520our%2520designed%2520models%252C%2520termed%2520MeRino%252C%2520across%250Afourteen%2520NLP%2520downstream%2520tasks%252C%2520showing%2520their%2520competitive%2520performance%2520against%250Athe%2520state-of-the-art%2520autoregressive%2520transformer%2520models%2520under%2520the%2520mobile%250Asetting.%2520Notably%252C%2520MeRino%2520achieves%2520similar%2520or%2520better%2520performance%2520on%2520both%250Alanguage%2520modeling%2520and%2520zero-shot%2520learning%2520tasks%252C%2520compared%2520to%2520the%2520350M%2520parameter%250AOPT%2520while%2520being%25204.9x%2520faster%2520on%2520NVIDIA%2520Jetson%2520Nano%2520with%25205.5x%2520reduction%2520in%2520model%250Asize.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07921v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Merino%3A%20Entropy-driven%20Design%20for%20Generative%20Language%20Models%20on%20IoT%0A%20%20Devices&entry.906535625=Youpeng%20Zhao%20and%20Ming%20Lin%20and%20Huadong%20Tang%20and%20Qiang%20Wu%20and%20Jun%20Wang&entry.1292438233=%20%20Generative%20Large%20Language%20Models%20%28LLMs%29%20stand%20as%20a%20revolutionary%20advancement%0Ain%20the%20modern%20era%20of%20artificial%20intelligence%20%28AI%29.%20However%2C%20scaling%20down%20LLMs%0Afor%20resource-constrained%20hardware%2C%20such%20as%20Internet-of-Things%20%28IoT%29%20devices%0Arequires%20non-trivial%20efforts%20and%20domain%20knowledge.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20information-entropy%20framework%20for%20designing%20mobile-friendly%20generative%0Alanguage%20models.%20The%20whole%20design%20procedure%20involves%20solving%20a%20mathematical%0Aprogramming%20%28MP%29%20problem%2C%20which%20can%20be%20done%20on%20the%20CPU%20within%20minutes%2C%20making%0Ait%20nearly%20zero-cost.%20We%20evaluate%20our%20designed%20models%2C%20termed%20MeRino%2C%20across%0Afourteen%20NLP%20downstream%20tasks%2C%20showing%20their%20competitive%20performance%20against%0Athe%20state-of-the-art%20autoregressive%20transformer%20models%20under%20the%20mobile%0Asetting.%20Notably%2C%20MeRino%20achieves%20similar%20or%20better%20performance%20on%20both%0Alanguage%20modeling%20and%20zero-shot%20learning%20tasks%2C%20compared%20to%20the%20350M%20parameter%0AOPT%20while%20being%204.9x%20faster%20on%20NVIDIA%20Jetson%20Nano%20with%205.5x%20reduction%20in%20model%0Asize.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07921v3&entry.124074799=Read"},
{"title": "Random-Set Neural Networks (RS-NN)", "author": "Shireen Kudukkil Manchingal and Muhammad Mubashar and Kaizheng Wang and Keivan Shariatmadar and Fabio Cuzzolin", "abstract": "  Machine learning is increasingly deployed in safety-critical domains where\nerroneous predictions may lead to potentially catastrophic consequences,\nhighlighting the need for learning systems to be aware of how confident they\nare in their own predictions: in other words, 'to know when they do not know'.\nIn this paper, we propose a novel Random-Set Neural Network (RS-NN) approach to\nclassification which predicts belief functions (rather than classical\nprobability vectors) over the class list using the mathematics of random sets,\ni.e., distributions over the collection of sets of classes. RS-NN encodes the\n'epistemic' uncertainty induced by training sets that are insufficiently\nrepresentative or limited in size via the size of the convex set of probability\nvectors associated with a predicted belief function. Our approach outperforms\nstate-of-the-art Bayesian and Ensemble methods in terms of accuracy,\nuncertainty estimation and out-of-distribution (OoD) detection on multiple\nbenchmarks (CIFAR-10 vs SVHN/Intel-Image, MNIST vs FMNIST/KMNIST, ImageNet vs\nImageNet-O). RS-NN also scales up effectively to large-scale architectures\n(e.g. WideResNet-28-10, VGG16, Inception V3, EfficientNetB2 and ViT-Base-16),\nexhibits remarkable robustness to adversarial attacks and can provide\nstatistical guarantees in a conformal learning setting.\n", "link": "http://arxiv.org/abs/2307.05772v3", "date": "2025-01-27", "relevancy": 2.0757, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5267}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5219}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random-Set%20Neural%20Networks%20%28RS-NN%29&body=Title%3A%20Random-Set%20Neural%20Networks%20%28RS-NN%29%0AAuthor%3A%20Shireen%20Kudukkil%20Manchingal%20and%20Muhammad%20Mubashar%20and%20Kaizheng%20Wang%20and%20Keivan%20Shariatmadar%20and%20Fabio%20Cuzzolin%0AAbstract%3A%20%20%20Machine%20learning%20is%20increasingly%20deployed%20in%20safety-critical%20domains%20where%0Aerroneous%20predictions%20may%20lead%20to%20potentially%20catastrophic%20consequences%2C%0Ahighlighting%20the%20need%20for%20learning%20systems%20to%20be%20aware%20of%20how%20confident%20they%0Aare%20in%20their%20own%20predictions%3A%20in%20other%20words%2C%20%27to%20know%20when%20they%20do%20not%20know%27.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20Random-Set%20Neural%20Network%20%28RS-NN%29%20approach%20to%0Aclassification%20which%20predicts%20belief%20functions%20%28rather%20than%20classical%0Aprobability%20vectors%29%20over%20the%20class%20list%20using%20the%20mathematics%20of%20random%20sets%2C%0Ai.e.%2C%20distributions%20over%20the%20collection%20of%20sets%20of%20classes.%20RS-NN%20encodes%20the%0A%27epistemic%27%20uncertainty%20induced%20by%20training%20sets%20that%20are%20insufficiently%0Arepresentative%20or%20limited%20in%20size%20via%20the%20size%20of%20the%20convex%20set%20of%20probability%0Avectors%20associated%20with%20a%20predicted%20belief%20function.%20Our%20approach%20outperforms%0Astate-of-the-art%20Bayesian%20and%20Ensemble%20methods%20in%20terms%20of%20accuracy%2C%0Auncertainty%20estimation%20and%20out-of-distribution%20%28OoD%29%20detection%20on%20multiple%0Abenchmarks%20%28CIFAR-10%20vs%20SVHN/Intel-Image%2C%20MNIST%20vs%20FMNIST/KMNIST%2C%20ImageNet%20vs%0AImageNet-O%29.%20RS-NN%20also%20scales%20up%20effectively%20to%20large-scale%20architectures%0A%28e.g.%20WideResNet-28-10%2C%20VGG16%2C%20Inception%20V3%2C%20EfficientNetB2%20and%20ViT-Base-16%29%2C%0Aexhibits%20remarkable%20robustness%20to%20adversarial%20attacks%20and%20can%20provide%0Astatistical%20guarantees%20in%20a%20conformal%20learning%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.05772v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom-Set%2520Neural%2520Networks%2520%2528RS-NN%2529%26entry.906535625%3DShireen%2520Kudukkil%2520Manchingal%2520and%2520Muhammad%2520Mubashar%2520and%2520Kaizheng%2520Wang%2520and%2520Keivan%2520Shariatmadar%2520and%2520Fabio%2520Cuzzolin%26entry.1292438233%3D%2520%2520Machine%2520learning%2520is%2520increasingly%2520deployed%2520in%2520safety-critical%2520domains%2520where%250Aerroneous%2520predictions%2520may%2520lead%2520to%2520potentially%2520catastrophic%2520consequences%252C%250Ahighlighting%2520the%2520need%2520for%2520learning%2520systems%2520to%2520be%2520aware%2520of%2520how%2520confident%2520they%250Aare%2520in%2520their%2520own%2520predictions%253A%2520in%2520other%2520words%252C%2520%2527to%2520know%2520when%2520they%2520do%2520not%2520know%2527.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Random-Set%2520Neural%2520Network%2520%2528RS-NN%2529%2520approach%2520to%250Aclassification%2520which%2520predicts%2520belief%2520functions%2520%2528rather%2520than%2520classical%250Aprobability%2520vectors%2529%2520over%2520the%2520class%2520list%2520using%2520the%2520mathematics%2520of%2520random%2520sets%252C%250Ai.e.%252C%2520distributions%2520over%2520the%2520collection%2520of%2520sets%2520of%2520classes.%2520RS-NN%2520encodes%2520the%250A%2527epistemic%2527%2520uncertainty%2520induced%2520by%2520training%2520sets%2520that%2520are%2520insufficiently%250Arepresentative%2520or%2520limited%2520in%2520size%2520via%2520the%2520size%2520of%2520the%2520convex%2520set%2520of%2520probability%250Avectors%2520associated%2520with%2520a%2520predicted%2520belief%2520function.%2520Our%2520approach%2520outperforms%250Astate-of-the-art%2520Bayesian%2520and%2520Ensemble%2520methods%2520in%2520terms%2520of%2520accuracy%252C%250Auncertainty%2520estimation%2520and%2520out-of-distribution%2520%2528OoD%2529%2520detection%2520on%2520multiple%250Abenchmarks%2520%2528CIFAR-10%2520vs%2520SVHN/Intel-Image%252C%2520MNIST%2520vs%2520FMNIST/KMNIST%252C%2520ImageNet%2520vs%250AImageNet-O%2529.%2520RS-NN%2520also%2520scales%2520up%2520effectively%2520to%2520large-scale%2520architectures%250A%2528e.g.%2520WideResNet-28-10%252C%2520VGG16%252C%2520Inception%2520V3%252C%2520EfficientNetB2%2520and%2520ViT-Base-16%2529%252C%250Aexhibits%2520remarkable%2520robustness%2520to%2520adversarial%2520attacks%2520and%2520can%2520provide%250Astatistical%2520guarantees%2520in%2520a%2520conformal%2520learning%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.05772v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random-Set%20Neural%20Networks%20%28RS-NN%29&entry.906535625=Shireen%20Kudukkil%20Manchingal%20and%20Muhammad%20Mubashar%20and%20Kaizheng%20Wang%20and%20Keivan%20Shariatmadar%20and%20Fabio%20Cuzzolin&entry.1292438233=%20%20Machine%20learning%20is%20increasingly%20deployed%20in%20safety-critical%20domains%20where%0Aerroneous%20predictions%20may%20lead%20to%20potentially%20catastrophic%20consequences%2C%0Ahighlighting%20the%20need%20for%20learning%20systems%20to%20be%20aware%20of%20how%20confident%20they%0Aare%20in%20their%20own%20predictions%3A%20in%20other%20words%2C%20%27to%20know%20when%20they%20do%20not%20know%27.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20Random-Set%20Neural%20Network%20%28RS-NN%29%20approach%20to%0Aclassification%20which%20predicts%20belief%20functions%20%28rather%20than%20classical%0Aprobability%20vectors%29%20over%20the%20class%20list%20using%20the%20mathematics%20of%20random%20sets%2C%0Ai.e.%2C%20distributions%20over%20the%20collection%20of%20sets%20of%20classes.%20RS-NN%20encodes%20the%0A%27epistemic%27%20uncertainty%20induced%20by%20training%20sets%20that%20are%20insufficiently%0Arepresentative%20or%20limited%20in%20size%20via%20the%20size%20of%20the%20convex%20set%20of%20probability%0Avectors%20associated%20with%20a%20predicted%20belief%20function.%20Our%20approach%20outperforms%0Astate-of-the-art%20Bayesian%20and%20Ensemble%20methods%20in%20terms%20of%20accuracy%2C%0Auncertainty%20estimation%20and%20out-of-distribution%20%28OoD%29%20detection%20on%20multiple%0Abenchmarks%20%28CIFAR-10%20vs%20SVHN/Intel-Image%2C%20MNIST%20vs%20FMNIST/KMNIST%2C%20ImageNet%20vs%0AImageNet-O%29.%20RS-NN%20also%20scales%20up%20effectively%20to%20large-scale%20architectures%0A%28e.g.%20WideResNet-28-10%2C%20VGG16%2C%20Inception%20V3%2C%20EfficientNetB2%20and%20ViT-Base-16%29%2C%0Aexhibits%20remarkable%20robustness%20to%20adversarial%20attacks%20and%20can%20provide%0Astatistical%20guarantees%20in%20a%20conformal%20learning%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.05772v3&entry.124074799=Read"},
{"title": "Selective Generation for Controllable Language Models", "author": "Minjae Lee and Kyungmin Kim and Taesoo Kim and Sangdon Park", "abstract": "  Trustworthiness of generative language models (GLMs) is crucial in their\ndeployment to critical decision making systems. Hence, certified risk control\nmethods such as selective prediction and conformal prediction have been applied\nto mitigating the hallucination problem in various supervised downstream tasks.\nHowever, the lack of appropriate correctness metric hinders applying such\nprincipled methods to language generation tasks. In this paper, we circumvent\nthis problem by leveraging the concept of textual entailment to evaluate the\ncorrectness of the generated sequence, and propose two selective generation\nalgorithms which control the false discovery rate with respect to the textual\nentailment relation (FDR-E) with a theoretical guarantee:\n$\\texttt{SGen}^{\\texttt{Sup}}$ and $\\texttt{SGen}^{\\texttt{Semi}}$.\n$\\texttt{SGen}^{\\texttt{Sup}}$, a direct modification of the selective\nprediction, is a supervised learning algorithm which exploits\nentailment-labeled data, annotated by humans. Since human annotation is costly,\nwe further propose a semi-supervised version, $\\texttt{SGen}^{\\texttt{Semi}}$,\nwhich fully utilizes the unlabeled data by pseudo-labeling, leveraging an\nentailment set function learned via conformal prediction. Furthermore,\n$\\texttt{SGen}^{\\texttt{Semi}}$ enables to use more general class of selection\nfunctions, neuro-selection functions, and provides users with an optimal\nselection function class given multiple candidates. Finally, we demonstrate the\nefficacy of the $\\texttt{SGen}$ family in achieving a desired FDR-E level with\ncomparable selection efficiency to those from baselines on both open and closed\nsource GLMs. Code and datasets are provided at\nhttps://github.com/ml-postech/selective-generation.\n", "link": "http://arxiv.org/abs/2307.09254v4", "date": "2025-01-27", "relevancy": 2.074, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5334}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5114}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Generation%20for%20Controllable%20Language%20Models&body=Title%3A%20Selective%20Generation%20for%20Controllable%20Language%20Models%0AAuthor%3A%20Minjae%20Lee%20and%20Kyungmin%20Kim%20and%20Taesoo%20Kim%20and%20Sangdon%20Park%0AAbstract%3A%20%20%20Trustworthiness%20of%20generative%20language%20models%20%28GLMs%29%20is%20crucial%20in%20their%0Adeployment%20to%20critical%20decision%20making%20systems.%20Hence%2C%20certified%20risk%20control%0Amethods%20such%20as%20selective%20prediction%20and%20conformal%20prediction%20have%20been%20applied%0Ato%20mitigating%20the%20hallucination%20problem%20in%20various%20supervised%20downstream%20tasks.%0AHowever%2C%20the%20lack%20of%20appropriate%20correctness%20metric%20hinders%20applying%20such%0Aprincipled%20methods%20to%20language%20generation%20tasks.%20In%20this%20paper%2C%20we%20circumvent%0Athis%20problem%20by%20leveraging%20the%20concept%20of%20textual%20entailment%20to%20evaluate%20the%0Acorrectness%20of%20the%20generated%20sequence%2C%20and%20propose%20two%20selective%20generation%0Aalgorithms%20which%20control%20the%20false%20discovery%20rate%20with%20respect%20to%20the%20textual%0Aentailment%20relation%20%28FDR-E%29%20with%20a%20theoretical%20guarantee%3A%0A%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSup%7D%7D%24%20and%20%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSemi%7D%7D%24.%0A%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSup%7D%7D%24%2C%20a%20direct%20modification%20of%20the%20selective%0Aprediction%2C%20is%20a%20supervised%20learning%20algorithm%20which%20exploits%0Aentailment-labeled%20data%2C%20annotated%20by%20humans.%20Since%20human%20annotation%20is%20costly%2C%0Awe%20further%20propose%20a%20semi-supervised%20version%2C%20%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSemi%7D%7D%24%2C%0Awhich%20fully%20utilizes%20the%20unlabeled%20data%20by%20pseudo-labeling%2C%20leveraging%20an%0Aentailment%20set%20function%20learned%20via%20conformal%20prediction.%20Furthermore%2C%0A%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSemi%7D%7D%24%20enables%20to%20use%20more%20general%20class%20of%20selection%0Afunctions%2C%20neuro-selection%20functions%2C%20and%20provides%20users%20with%20an%20optimal%0Aselection%20function%20class%20given%20multiple%20candidates.%20Finally%2C%20we%20demonstrate%20the%0Aefficacy%20of%20the%20%24%5Ctexttt%7BSGen%7D%24%20family%20in%20achieving%20a%20desired%20FDR-E%20level%20with%0Acomparable%20selection%20efficiency%20to%20those%20from%20baselines%20on%20both%20open%20and%20closed%0Asource%20GLMs.%20Code%20and%20datasets%20are%20provided%20at%0Ahttps%3A//github.com/ml-postech/selective-generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09254v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Generation%2520for%2520Controllable%2520Language%2520Models%26entry.906535625%3DMinjae%2520Lee%2520and%2520Kyungmin%2520Kim%2520and%2520Taesoo%2520Kim%2520and%2520Sangdon%2520Park%26entry.1292438233%3D%2520%2520Trustworthiness%2520of%2520generative%2520language%2520models%2520%2528GLMs%2529%2520is%2520crucial%2520in%2520their%250Adeployment%2520to%2520critical%2520decision%2520making%2520systems.%2520Hence%252C%2520certified%2520risk%2520control%250Amethods%2520such%2520as%2520selective%2520prediction%2520and%2520conformal%2520prediction%2520have%2520been%2520applied%250Ato%2520mitigating%2520the%2520hallucination%2520problem%2520in%2520various%2520supervised%2520downstream%2520tasks.%250AHowever%252C%2520the%2520lack%2520of%2520appropriate%2520correctness%2520metric%2520hinders%2520applying%2520such%250Aprincipled%2520methods%2520to%2520language%2520generation%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520circumvent%250Athis%2520problem%2520by%2520leveraging%2520the%2520concept%2520of%2520textual%2520entailment%2520to%2520evaluate%2520the%250Acorrectness%2520of%2520the%2520generated%2520sequence%252C%2520and%2520propose%2520two%2520selective%2520generation%250Aalgorithms%2520which%2520control%2520the%2520false%2520discovery%2520rate%2520with%2520respect%2520to%2520the%2520textual%250Aentailment%2520relation%2520%2528FDR-E%2529%2520with%2520a%2520theoretical%2520guarantee%253A%250A%2524%255Ctexttt%257BSGen%257D%255E%257B%255Ctexttt%257BSup%257D%257D%2524%2520and%2520%2524%255Ctexttt%257BSGen%257D%255E%257B%255Ctexttt%257BSemi%257D%257D%2524.%250A%2524%255Ctexttt%257BSGen%257D%255E%257B%255Ctexttt%257BSup%257D%257D%2524%252C%2520a%2520direct%2520modification%2520of%2520the%2520selective%250Aprediction%252C%2520is%2520a%2520supervised%2520learning%2520algorithm%2520which%2520exploits%250Aentailment-labeled%2520data%252C%2520annotated%2520by%2520humans.%2520Since%2520human%2520annotation%2520is%2520costly%252C%250Awe%2520further%2520propose%2520a%2520semi-supervised%2520version%252C%2520%2524%255Ctexttt%257BSGen%257D%255E%257B%255Ctexttt%257BSemi%257D%257D%2524%252C%250Awhich%2520fully%2520utilizes%2520the%2520unlabeled%2520data%2520by%2520pseudo-labeling%252C%2520leveraging%2520an%250Aentailment%2520set%2520function%2520learned%2520via%2520conformal%2520prediction.%2520Furthermore%252C%250A%2524%255Ctexttt%257BSGen%257D%255E%257B%255Ctexttt%257BSemi%257D%257D%2524%2520enables%2520to%2520use%2520more%2520general%2520class%2520of%2520selection%250Afunctions%252C%2520neuro-selection%2520functions%252C%2520and%2520provides%2520users%2520with%2520an%2520optimal%250Aselection%2520function%2520class%2520given%2520multiple%2520candidates.%2520Finally%252C%2520we%2520demonstrate%2520the%250Aefficacy%2520of%2520the%2520%2524%255Ctexttt%257BSGen%257D%2524%2520family%2520in%2520achieving%2520a%2520desired%2520FDR-E%2520level%2520with%250Acomparable%2520selection%2520efficiency%2520to%2520those%2520from%2520baselines%2520on%2520both%2520open%2520and%2520closed%250Asource%2520GLMs.%2520Code%2520and%2520datasets%2520are%2520provided%2520at%250Ahttps%253A//github.com/ml-postech/selective-generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09254v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Generation%20for%20Controllable%20Language%20Models&entry.906535625=Minjae%20Lee%20and%20Kyungmin%20Kim%20and%20Taesoo%20Kim%20and%20Sangdon%20Park&entry.1292438233=%20%20Trustworthiness%20of%20generative%20language%20models%20%28GLMs%29%20is%20crucial%20in%20their%0Adeployment%20to%20critical%20decision%20making%20systems.%20Hence%2C%20certified%20risk%20control%0Amethods%20such%20as%20selective%20prediction%20and%20conformal%20prediction%20have%20been%20applied%0Ato%20mitigating%20the%20hallucination%20problem%20in%20various%20supervised%20downstream%20tasks.%0AHowever%2C%20the%20lack%20of%20appropriate%20correctness%20metric%20hinders%20applying%20such%0Aprincipled%20methods%20to%20language%20generation%20tasks.%20In%20this%20paper%2C%20we%20circumvent%0Athis%20problem%20by%20leveraging%20the%20concept%20of%20textual%20entailment%20to%20evaluate%20the%0Acorrectness%20of%20the%20generated%20sequence%2C%20and%20propose%20two%20selective%20generation%0Aalgorithms%20which%20control%20the%20false%20discovery%20rate%20with%20respect%20to%20the%20textual%0Aentailment%20relation%20%28FDR-E%29%20with%20a%20theoretical%20guarantee%3A%0A%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSup%7D%7D%24%20and%20%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSemi%7D%7D%24.%0A%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSup%7D%7D%24%2C%20a%20direct%20modification%20of%20the%20selective%0Aprediction%2C%20is%20a%20supervised%20learning%20algorithm%20which%20exploits%0Aentailment-labeled%20data%2C%20annotated%20by%20humans.%20Since%20human%20annotation%20is%20costly%2C%0Awe%20further%20propose%20a%20semi-supervised%20version%2C%20%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSemi%7D%7D%24%2C%0Awhich%20fully%20utilizes%20the%20unlabeled%20data%20by%20pseudo-labeling%2C%20leveraging%20an%0Aentailment%20set%20function%20learned%20via%20conformal%20prediction.%20Furthermore%2C%0A%24%5Ctexttt%7BSGen%7D%5E%7B%5Ctexttt%7BSemi%7D%7D%24%20enables%20to%20use%20more%20general%20class%20of%20selection%0Afunctions%2C%20neuro-selection%20functions%2C%20and%20provides%20users%20with%20an%20optimal%0Aselection%20function%20class%20given%20multiple%20candidates.%20Finally%2C%20we%20demonstrate%20the%0Aefficacy%20of%20the%20%24%5Ctexttt%7BSGen%7D%24%20family%20in%20achieving%20a%20desired%20FDR-E%20level%20with%0Acomparable%20selection%20efficiency%20to%20those%20from%20baselines%20on%20both%20open%20and%20closed%0Asource%20GLMs.%20Code%20and%20datasets%20are%20provided%20at%0Ahttps%3A//github.com/ml-postech/selective-generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09254v4&entry.124074799=Read"},
{"title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language\n  Models", "author": "Yibin Wang and Haizhou Shi and Ligong Han and Dimitris Metaxas and Hao Wang", "abstract": "  Large Language Models (LLMs) often suffer from overconfidence during\ninference, particularly when adapted to downstream domain-specific tasks with\nlimited data. Previous work addresses this issue by employing approximate\nBayesian estimation after the LLMs are trained, enabling them to quantify\nuncertainty. However, such post-training approaches' performance is severely\nlimited by the parameters learned during training. In this paper, we go beyond\npost-training Bayesianization and propose Bayesian Low-Rank Adaptation by\nBackpropagation (BLoB), an algorithm that continuously and jointly adjusts both\nthe mean and covariance of LLM parameters throughout the whole fine-tuning\nprocess. Our empirical results verify the effectiveness of BLoB in terms of\ngeneralization and uncertainty estimation, when evaluated on both\nin-distribution and out-of-distribution data.\n", "link": "http://arxiv.org/abs/2406.11675v5", "date": "2025-01-27", "relevancy": 2.0544, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5307}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.511}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLoB%3A%20Bayesian%20Low-Rank%20Adaptation%20by%20Backpropagation%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20BLoB%3A%20Bayesian%20Low-Rank%20Adaptation%20by%20Backpropagation%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yibin%20Wang%20and%20Haizhou%20Shi%20and%20Ligong%20Han%20and%20Dimitris%20Metaxas%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20suffer%20from%20overconfidence%20during%0Ainference%2C%20particularly%20when%20adapted%20to%20downstream%20domain-specific%20tasks%20with%0Alimited%20data.%20Previous%20work%20addresses%20this%20issue%20by%20employing%20approximate%0ABayesian%20estimation%20after%20the%20LLMs%20are%20trained%2C%20enabling%20them%20to%20quantify%0Auncertainty.%20However%2C%20such%20post-training%20approaches%27%20performance%20is%20severely%0Alimited%20by%20the%20parameters%20learned%20during%20training.%20In%20this%20paper%2C%20we%20go%20beyond%0Apost-training%20Bayesianization%20and%20propose%20Bayesian%20Low-Rank%20Adaptation%20by%0ABackpropagation%20%28BLoB%29%2C%20an%20algorithm%20that%20continuously%20and%20jointly%20adjusts%20both%0Athe%20mean%20and%20covariance%20of%20LLM%20parameters%20throughout%20the%20whole%20fine-tuning%0Aprocess.%20Our%20empirical%20results%20verify%20the%20effectiveness%20of%20BLoB%20in%20terms%20of%0Ageneralization%20and%20uncertainty%20estimation%2C%20when%20evaluated%20on%20both%0Ain-distribution%20and%20out-of-distribution%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11675v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLoB%253A%2520Bayesian%2520Low-Rank%2520Adaptation%2520by%2520Backpropagation%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYibin%2520Wang%2520and%2520Haizhou%2520Shi%2520and%2520Ligong%2520Han%2520and%2520Dimitris%2520Metaxas%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520suffer%2520from%2520overconfidence%2520during%250Ainference%252C%2520particularly%2520when%2520adapted%2520to%2520downstream%2520domain-specific%2520tasks%2520with%250Alimited%2520data.%2520Previous%2520work%2520addresses%2520this%2520issue%2520by%2520employing%2520approximate%250ABayesian%2520estimation%2520after%2520the%2520LLMs%2520are%2520trained%252C%2520enabling%2520them%2520to%2520quantify%250Auncertainty.%2520However%252C%2520such%2520post-training%2520approaches%2527%2520performance%2520is%2520severely%250Alimited%2520by%2520the%2520parameters%2520learned%2520during%2520training.%2520In%2520this%2520paper%252C%2520we%2520go%2520beyond%250Apost-training%2520Bayesianization%2520and%2520propose%2520Bayesian%2520Low-Rank%2520Adaptation%2520by%250ABackpropagation%2520%2528BLoB%2529%252C%2520an%2520algorithm%2520that%2520continuously%2520and%2520jointly%2520adjusts%2520both%250Athe%2520mean%2520and%2520covariance%2520of%2520LLM%2520parameters%2520throughout%2520the%2520whole%2520fine-tuning%250Aprocess.%2520Our%2520empirical%2520results%2520verify%2520the%2520effectiveness%2520of%2520BLoB%2520in%2520terms%2520of%250Ageneralization%2520and%2520uncertainty%2520estimation%252C%2520when%2520evaluated%2520on%2520both%250Ain-distribution%2520and%2520out-of-distribution%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11675v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLoB%3A%20Bayesian%20Low-Rank%20Adaptation%20by%20Backpropagation%20for%20Large%20Language%0A%20%20Models&entry.906535625=Yibin%20Wang%20and%20Haizhou%20Shi%20and%20Ligong%20Han%20and%20Dimitris%20Metaxas%20and%20Hao%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20suffer%20from%20overconfidence%20during%0Ainference%2C%20particularly%20when%20adapted%20to%20downstream%20domain-specific%20tasks%20with%0Alimited%20data.%20Previous%20work%20addresses%20this%20issue%20by%20employing%20approximate%0ABayesian%20estimation%20after%20the%20LLMs%20are%20trained%2C%20enabling%20them%20to%20quantify%0Auncertainty.%20However%2C%20such%20post-training%20approaches%27%20performance%20is%20severely%0Alimited%20by%20the%20parameters%20learned%20during%20training.%20In%20this%20paper%2C%20we%20go%20beyond%0Apost-training%20Bayesianization%20and%20propose%20Bayesian%20Low-Rank%20Adaptation%20by%0ABackpropagation%20%28BLoB%29%2C%20an%20algorithm%20that%20continuously%20and%20jointly%20adjusts%20both%0Athe%20mean%20and%20covariance%20of%20LLM%20parameters%20throughout%20the%20whole%20fine-tuning%0Aprocess.%20Our%20empirical%20results%20verify%20the%20effectiveness%20of%20BLoB%20in%20terms%20of%0Ageneralization%20and%20uncertainty%20estimation%2C%20when%20evaluated%20on%20both%0Ain-distribution%20and%20out-of-distribution%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11675v5&entry.124074799=Read"},
{"title": "Learning Exactly Linearizable Deep Dynamics Models", "author": "Ryuta Moriyasu and Masayuki Kusunoki and Kenji Kashima", "abstract": "  Research on control using models based on machine-learning methods has now\nshifted to the practical engineering stage. Achieving high performance and\ntheoretically guaranteeing the safety of the system is critical for such\napplications. In this paper, we propose a learning method for exactly\nlinearizable dynamical models that can easily apply various control theories to\nensure stability, reliability, etc., and to provide a high degree of freedom of\nexpression. As an example, we present a design that combines simple linear\ncontrol and control barrier functions. The proposed model is employed for the\nreal-time control of an automotive engine, and the results demonstrate good\npredictive performance and stable control under constraints.\n", "link": "http://arxiv.org/abs/2311.18261v2", "date": "2025-01-27", "relevancy": 2.049, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5291}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5013}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Exactly%20Linearizable%20Deep%20Dynamics%20Models&body=Title%3A%20Learning%20Exactly%20Linearizable%20Deep%20Dynamics%20Models%0AAuthor%3A%20Ryuta%20Moriyasu%20and%20Masayuki%20Kusunoki%20and%20Kenji%20Kashima%0AAbstract%3A%20%20%20Research%20on%20control%20using%20models%20based%20on%20machine-learning%20methods%20has%20now%0Ashifted%20to%20the%20practical%20engineering%20stage.%20Achieving%20high%20performance%20and%0Atheoretically%20guaranteeing%20the%20safety%20of%20the%20system%20is%20critical%20for%20such%0Aapplications.%20In%20this%20paper%2C%20we%20propose%20a%20learning%20method%20for%20exactly%0Alinearizable%20dynamical%20models%20that%20can%20easily%20apply%20various%20control%20theories%20to%0Aensure%20stability%2C%20reliability%2C%20etc.%2C%20and%20to%20provide%20a%20high%20degree%20of%20freedom%20of%0Aexpression.%20As%20an%20example%2C%20we%20present%20a%20design%20that%20combines%20simple%20linear%0Acontrol%20and%20control%20barrier%20functions.%20The%20proposed%20model%20is%20employed%20for%20the%0Areal-time%20control%20of%20an%20automotive%20engine%2C%20and%20the%20results%20demonstrate%20good%0Apredictive%20performance%20and%20stable%20control%20under%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18261v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Exactly%2520Linearizable%2520Deep%2520Dynamics%2520Models%26entry.906535625%3DRyuta%2520Moriyasu%2520and%2520Masayuki%2520Kusunoki%2520and%2520Kenji%2520Kashima%26entry.1292438233%3D%2520%2520Research%2520on%2520control%2520using%2520models%2520based%2520on%2520machine-learning%2520methods%2520has%2520now%250Ashifted%2520to%2520the%2520practical%2520engineering%2520stage.%2520Achieving%2520high%2520performance%2520and%250Atheoretically%2520guaranteeing%2520the%2520safety%2520of%2520the%2520system%2520is%2520critical%2520for%2520such%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520learning%2520method%2520for%2520exactly%250Alinearizable%2520dynamical%2520models%2520that%2520can%2520easily%2520apply%2520various%2520control%2520theories%2520to%250Aensure%2520stability%252C%2520reliability%252C%2520etc.%252C%2520and%2520to%2520provide%2520a%2520high%2520degree%2520of%2520freedom%2520of%250Aexpression.%2520As%2520an%2520example%252C%2520we%2520present%2520a%2520design%2520that%2520combines%2520simple%2520linear%250Acontrol%2520and%2520control%2520barrier%2520functions.%2520The%2520proposed%2520model%2520is%2520employed%2520for%2520the%250Areal-time%2520control%2520of%2520an%2520automotive%2520engine%252C%2520and%2520the%2520results%2520demonstrate%2520good%250Apredictive%2520performance%2520and%2520stable%2520control%2520under%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18261v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Exactly%20Linearizable%20Deep%20Dynamics%20Models&entry.906535625=Ryuta%20Moriyasu%20and%20Masayuki%20Kusunoki%20and%20Kenji%20Kashima&entry.1292438233=%20%20Research%20on%20control%20using%20models%20based%20on%20machine-learning%20methods%20has%20now%0Ashifted%20to%20the%20practical%20engineering%20stage.%20Achieving%20high%20performance%20and%0Atheoretically%20guaranteeing%20the%20safety%20of%20the%20system%20is%20critical%20for%20such%0Aapplications.%20In%20this%20paper%2C%20we%20propose%20a%20learning%20method%20for%20exactly%0Alinearizable%20dynamical%20models%20that%20can%20easily%20apply%20various%20control%20theories%20to%0Aensure%20stability%2C%20reliability%2C%20etc.%2C%20and%20to%20provide%20a%20high%20degree%20of%20freedom%20of%0Aexpression.%20As%20an%20example%2C%20we%20present%20a%20design%20that%20combines%20simple%20linear%0Acontrol%20and%20control%20barrier%20functions.%20The%20proposed%20model%20is%20employed%20for%20the%0Areal-time%20control%20of%20an%20automotive%20engine%2C%20and%20the%20results%20demonstrate%20good%0Apredictive%20performance%20and%20stable%20control%20under%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18261v2&entry.124074799=Read"},
{"title": "From Molecules to Mixtures: Learning Representations of Olfactory\n  Mixture Similarity using Inductive Biases", "author": "Gary Tom and Cher Tian Ser and Ella M. Rajaonson and Stanley Lo and Hyun Suk Park and Brian K. Lee and Benjamin Sanchez-Lengeling", "abstract": "  Olfaction -- how molecules are perceived as odors to humans -- remains poorly\nunderstood. Recently, the principal odor map (POM) was introduced to digitize\nthe olfactory properties of single compounds. However, smells in real life are\nnot pure single molecules, but complex mixtures of molecules, whose\nrepresentations remain relatively under-explored. In this work, we introduce\nPOMMix, an extension of the POM to represent mixtures. Our representation\nbuilds upon the symmetries of the problem space in a hierarchical manner: (1)\ngraph neural networks for building molecular embeddings, (2) attention\nmechanisms for aggregating molecular representations into mixture\nrepresentations, and (3) cosine prediction heads to encode olfactory perceptual\ndistance in the mixture embedding space. POMMix achieves state-of-the-art\npredictive performance across multiple datasets. We also evaluate the\ngeneralizability of the representation on multiple splits when applied to\nunseen molecules and mixture sizes. Our work advances the effort to digitize\nolfaction, and highlights the synergy of domain expertise and deep learning in\ncrafting expressive representations in low-data regimes.\n", "link": "http://arxiv.org/abs/2501.16271v1", "date": "2025-01-27", "relevancy": 2.0179, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5456}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4974}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Molecules%20to%20Mixtures%3A%20Learning%20Representations%20of%20Olfactory%0A%20%20Mixture%20Similarity%20using%20Inductive%20Biases&body=Title%3A%20From%20Molecules%20to%20Mixtures%3A%20Learning%20Representations%20of%20Olfactory%0A%20%20Mixture%20Similarity%20using%20Inductive%20Biases%0AAuthor%3A%20Gary%20Tom%20and%20Cher%20Tian%20Ser%20and%20Ella%20M.%20Rajaonson%20and%20Stanley%20Lo%20and%20Hyun%20Suk%20Park%20and%20Brian%20K.%20Lee%20and%20Benjamin%20Sanchez-Lengeling%0AAbstract%3A%20%20%20Olfaction%20--%20how%20molecules%20are%20perceived%20as%20odors%20to%20humans%20--%20remains%20poorly%0Aunderstood.%20Recently%2C%20the%20principal%20odor%20map%20%28POM%29%20was%20introduced%20to%20digitize%0Athe%20olfactory%20properties%20of%20single%20compounds.%20However%2C%20smells%20in%20real%20life%20are%0Anot%20pure%20single%20molecules%2C%20but%20complex%20mixtures%20of%20molecules%2C%20whose%0Arepresentations%20remain%20relatively%20under-explored.%20In%20this%20work%2C%20we%20introduce%0APOMMix%2C%20an%20extension%20of%20the%20POM%20to%20represent%20mixtures.%20Our%20representation%0Abuilds%20upon%20the%20symmetries%20of%20the%20problem%20space%20in%20a%20hierarchical%20manner%3A%20%281%29%0Agraph%20neural%20networks%20for%20building%20molecular%20embeddings%2C%20%282%29%20attention%0Amechanisms%20for%20aggregating%20molecular%20representations%20into%20mixture%0Arepresentations%2C%20and%20%283%29%20cosine%20prediction%20heads%20to%20encode%20olfactory%20perceptual%0Adistance%20in%20the%20mixture%20embedding%20space.%20POMMix%20achieves%20state-of-the-art%0Apredictive%20performance%20across%20multiple%20datasets.%20We%20also%20evaluate%20the%0Ageneralizability%20of%20the%20representation%20on%20multiple%20splits%20when%20applied%20to%0Aunseen%20molecules%20and%20mixture%20sizes.%20Our%20work%20advances%20the%20effort%20to%20digitize%0Aolfaction%2C%20and%20highlights%20the%20synergy%20of%20domain%20expertise%20and%20deep%20learning%20in%0Acrafting%20expressive%20representations%20in%20low-data%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Molecules%2520to%2520Mixtures%253A%2520Learning%2520Representations%2520of%2520Olfactory%250A%2520%2520Mixture%2520Similarity%2520using%2520Inductive%2520Biases%26entry.906535625%3DGary%2520Tom%2520and%2520Cher%2520Tian%2520Ser%2520and%2520Ella%2520M.%2520Rajaonson%2520and%2520Stanley%2520Lo%2520and%2520Hyun%2520Suk%2520Park%2520and%2520Brian%2520K.%2520Lee%2520and%2520Benjamin%2520Sanchez-Lengeling%26entry.1292438233%3D%2520%2520Olfaction%2520--%2520how%2520molecules%2520are%2520perceived%2520as%2520odors%2520to%2520humans%2520--%2520remains%2520poorly%250Aunderstood.%2520Recently%252C%2520the%2520principal%2520odor%2520map%2520%2528POM%2529%2520was%2520introduced%2520to%2520digitize%250Athe%2520olfactory%2520properties%2520of%2520single%2520compounds.%2520However%252C%2520smells%2520in%2520real%2520life%2520are%250Anot%2520pure%2520single%2520molecules%252C%2520but%2520complex%2520mixtures%2520of%2520molecules%252C%2520whose%250Arepresentations%2520remain%2520relatively%2520under-explored.%2520In%2520this%2520work%252C%2520we%2520introduce%250APOMMix%252C%2520an%2520extension%2520of%2520the%2520POM%2520to%2520represent%2520mixtures.%2520Our%2520representation%250Abuilds%2520upon%2520the%2520symmetries%2520of%2520the%2520problem%2520space%2520in%2520a%2520hierarchical%2520manner%253A%2520%25281%2529%250Agraph%2520neural%2520networks%2520for%2520building%2520molecular%2520embeddings%252C%2520%25282%2529%2520attention%250Amechanisms%2520for%2520aggregating%2520molecular%2520representations%2520into%2520mixture%250Arepresentations%252C%2520and%2520%25283%2529%2520cosine%2520prediction%2520heads%2520to%2520encode%2520olfactory%2520perceptual%250Adistance%2520in%2520the%2520mixture%2520embedding%2520space.%2520POMMix%2520achieves%2520state-of-the-art%250Apredictive%2520performance%2520across%2520multiple%2520datasets.%2520We%2520also%2520evaluate%2520the%250Ageneralizability%2520of%2520the%2520representation%2520on%2520multiple%2520splits%2520when%2520applied%2520to%250Aunseen%2520molecules%2520and%2520mixture%2520sizes.%2520Our%2520work%2520advances%2520the%2520effort%2520to%2520digitize%250Aolfaction%252C%2520and%2520highlights%2520the%2520synergy%2520of%2520domain%2520expertise%2520and%2520deep%2520learning%2520in%250Acrafting%2520expressive%2520representations%2520in%2520low-data%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Molecules%20to%20Mixtures%3A%20Learning%20Representations%20of%20Olfactory%0A%20%20Mixture%20Similarity%20using%20Inductive%20Biases&entry.906535625=Gary%20Tom%20and%20Cher%20Tian%20Ser%20and%20Ella%20M.%20Rajaonson%20and%20Stanley%20Lo%20and%20Hyun%20Suk%20Park%20and%20Brian%20K.%20Lee%20and%20Benjamin%20Sanchez-Lengeling&entry.1292438233=%20%20Olfaction%20--%20how%20molecules%20are%20perceived%20as%20odors%20to%20humans%20--%20remains%20poorly%0Aunderstood.%20Recently%2C%20the%20principal%20odor%20map%20%28POM%29%20was%20introduced%20to%20digitize%0Athe%20olfactory%20properties%20of%20single%20compounds.%20However%2C%20smells%20in%20real%20life%20are%0Anot%20pure%20single%20molecules%2C%20but%20complex%20mixtures%20of%20molecules%2C%20whose%0Arepresentations%20remain%20relatively%20under-explored.%20In%20this%20work%2C%20we%20introduce%0APOMMix%2C%20an%20extension%20of%20the%20POM%20to%20represent%20mixtures.%20Our%20representation%0Abuilds%20upon%20the%20symmetries%20of%20the%20problem%20space%20in%20a%20hierarchical%20manner%3A%20%281%29%0Agraph%20neural%20networks%20for%20building%20molecular%20embeddings%2C%20%282%29%20attention%0Amechanisms%20for%20aggregating%20molecular%20representations%20into%20mixture%0Arepresentations%2C%20and%20%283%29%20cosine%20prediction%20heads%20to%20encode%20olfactory%20perceptual%0Adistance%20in%20the%20mixture%20embedding%20space.%20POMMix%20achieves%20state-of-the-art%0Apredictive%20performance%20across%20multiple%20datasets.%20We%20also%20evaluate%20the%0Ageneralizability%20of%20the%20representation%20on%20multiple%20splits%20when%20applied%20to%0Aunseen%20molecules%20and%20mixture%20sizes.%20Our%20work%20advances%20the%20effort%20to%20digitize%0Aolfaction%2C%20and%20highlights%20the%20synergy%20of%20domain%20expertise%20and%20deep%20learning%20in%0Acrafting%20expressive%20representations%20in%20low-data%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16271v1&entry.124074799=Read"},
{"title": "BiMix: A Bivariate Data Mixing Law for Language Model Pretraining", "author": "Ce Ge and Zhijian Ma and Daoyuan Chen and Yaliang Li and Bolin Ding", "abstract": "  Large language models have demonstrated remarkable capabilities across\nvarious tasks, primarily attributed to the utilization of diversely sourced\ndata. However, the impact of pretraining data composition on model performance\nremains poorly understood. This paper introduces $\\textbf{BiMix}$, a novel\nbivariate data mixing law that models the joint scaling behavior of domain\nproportions and data volume in LLM pretraining. $\\textbf{BiMix}$ provides a\nsystematic framework for understanding and optimizing data mixtures across\ndiverse domains. Through extensive experiments on two large-scale datasets, we\ndemonstrate $\\textbf{BiMix}$'s high accuracy in loss extrapolation (mean\nrelative error < 0.2%) and its generalization to unseen mixtures (R${}^{2}$ >\n0.97). Optimization of domain proportions yields superior model performance\ncompared to existing methods. Furthermore, we establish entropy-based measures\nas efficient proxies for data mixing, offering a computationally lightweight\nstrategy. Our work contributes both theoretical insights into data mixing\ndynamics and practical tools for enhancing LLM training efficiency, paving the\nway for more effective scaling strategies in language model development.\n", "link": "http://arxiv.org/abs/2405.14908v4", "date": "2025-01-27", "relevancy": 2.0134, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5172}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4984}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiMix%3A%20A%20Bivariate%20Data%20Mixing%20Law%20for%20Language%20Model%20Pretraining&body=Title%3A%20BiMix%3A%20A%20Bivariate%20Data%20Mixing%20Law%20for%20Language%20Model%20Pretraining%0AAuthor%3A%20Ce%20Ge%20and%20Zhijian%20Ma%20and%20Daoyuan%20Chen%20and%20Yaliang%20Li%20and%20Bolin%20Ding%0AAbstract%3A%20%20%20Large%20language%20models%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20tasks%2C%20primarily%20attributed%20to%20the%20utilization%20of%20diversely%20sourced%0Adata.%20However%2C%20the%20impact%20of%20pretraining%20data%20composition%20on%20model%20performance%0Aremains%20poorly%20understood.%20This%20paper%20introduces%20%24%5Ctextbf%7BBiMix%7D%24%2C%20a%20novel%0Abivariate%20data%20mixing%20law%20that%20models%20the%20joint%20scaling%20behavior%20of%20domain%0Aproportions%20and%20data%20volume%20in%20LLM%20pretraining.%20%24%5Ctextbf%7BBiMix%7D%24%20provides%20a%0Asystematic%20framework%20for%20understanding%20and%20optimizing%20data%20mixtures%20across%0Adiverse%20domains.%20Through%20extensive%20experiments%20on%20two%20large-scale%20datasets%2C%20we%0Ademonstrate%20%24%5Ctextbf%7BBiMix%7D%24%27s%20high%20accuracy%20in%20loss%20extrapolation%20%28mean%0Arelative%20error%20%3C%200.2%25%29%20and%20its%20generalization%20to%20unseen%20mixtures%20%28R%24%7B%7D%5E%7B2%7D%24%20%3E%0A0.97%29.%20Optimization%20of%20domain%20proportions%20yields%20superior%20model%20performance%0Acompared%20to%20existing%20methods.%20Furthermore%2C%20we%20establish%20entropy-based%20measures%0Aas%20efficient%20proxies%20for%20data%20mixing%2C%20offering%20a%20computationally%20lightweight%0Astrategy.%20Our%20work%20contributes%20both%20theoretical%20insights%20into%20data%20mixing%0Adynamics%20and%20practical%20tools%20for%20enhancing%20LLM%20training%20efficiency%2C%20paving%20the%0Away%20for%20more%20effective%20scaling%20strategies%20in%20language%20model%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14908v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiMix%253A%2520A%2520Bivariate%2520Data%2520Mixing%2520Law%2520for%2520Language%2520Model%2520Pretraining%26entry.906535625%3DCe%2520Ge%2520and%2520Zhijian%2520Ma%2520and%2520Daoyuan%2520Chen%2520and%2520Yaliang%2520Li%2520and%2520Bolin%2520Ding%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Avarious%2520tasks%252C%2520primarily%2520attributed%2520to%2520the%2520utilization%2520of%2520diversely%2520sourced%250Adata.%2520However%252C%2520the%2520impact%2520of%2520pretraining%2520data%2520composition%2520on%2520model%2520performance%250Aremains%2520poorly%2520understood.%2520This%2520paper%2520introduces%2520%2524%255Ctextbf%257BBiMix%257D%2524%252C%2520a%2520novel%250Abivariate%2520data%2520mixing%2520law%2520that%2520models%2520the%2520joint%2520scaling%2520behavior%2520of%2520domain%250Aproportions%2520and%2520data%2520volume%2520in%2520LLM%2520pretraining.%2520%2524%255Ctextbf%257BBiMix%257D%2524%2520provides%2520a%250Asystematic%2520framework%2520for%2520understanding%2520and%2520optimizing%2520data%2520mixtures%2520across%250Adiverse%2520domains.%2520Through%2520extensive%2520experiments%2520on%2520two%2520large-scale%2520datasets%252C%2520we%250Ademonstrate%2520%2524%255Ctextbf%257BBiMix%257D%2524%2527s%2520high%2520accuracy%2520in%2520loss%2520extrapolation%2520%2528mean%250Arelative%2520error%2520%253C%25200.2%2525%2529%2520and%2520its%2520generalization%2520to%2520unseen%2520mixtures%2520%2528R%2524%257B%257D%255E%257B2%257D%2524%2520%253E%250A0.97%2529.%2520Optimization%2520of%2520domain%2520proportions%2520yields%2520superior%2520model%2520performance%250Acompared%2520to%2520existing%2520methods.%2520Furthermore%252C%2520we%2520establish%2520entropy-based%2520measures%250Aas%2520efficient%2520proxies%2520for%2520data%2520mixing%252C%2520offering%2520a%2520computationally%2520lightweight%250Astrategy.%2520Our%2520work%2520contributes%2520both%2520theoretical%2520insights%2520into%2520data%2520mixing%250Adynamics%2520and%2520practical%2520tools%2520for%2520enhancing%2520LLM%2520training%2520efficiency%252C%2520paving%2520the%250Away%2520for%2520more%2520effective%2520scaling%2520strategies%2520in%2520language%2520model%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14908v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiMix%3A%20A%20Bivariate%20Data%20Mixing%20Law%20for%20Language%20Model%20Pretraining&entry.906535625=Ce%20Ge%20and%20Zhijian%20Ma%20and%20Daoyuan%20Chen%20and%20Yaliang%20Li%20and%20Bolin%20Ding&entry.1292438233=%20%20Large%20language%20models%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20tasks%2C%20primarily%20attributed%20to%20the%20utilization%20of%20diversely%20sourced%0Adata.%20However%2C%20the%20impact%20of%20pretraining%20data%20composition%20on%20model%20performance%0Aremains%20poorly%20understood.%20This%20paper%20introduces%20%24%5Ctextbf%7BBiMix%7D%24%2C%20a%20novel%0Abivariate%20data%20mixing%20law%20that%20models%20the%20joint%20scaling%20behavior%20of%20domain%0Aproportions%20and%20data%20volume%20in%20LLM%20pretraining.%20%24%5Ctextbf%7BBiMix%7D%24%20provides%20a%0Asystematic%20framework%20for%20understanding%20and%20optimizing%20data%20mixtures%20across%0Adiverse%20domains.%20Through%20extensive%20experiments%20on%20two%20large-scale%20datasets%2C%20we%0Ademonstrate%20%24%5Ctextbf%7BBiMix%7D%24%27s%20high%20accuracy%20in%20loss%20extrapolation%20%28mean%0Arelative%20error%20%3C%200.2%25%29%20and%20its%20generalization%20to%20unseen%20mixtures%20%28R%24%7B%7D%5E%7B2%7D%24%20%3E%0A0.97%29.%20Optimization%20of%20domain%20proportions%20yields%20superior%20model%20performance%0Acompared%20to%20existing%20methods.%20Furthermore%2C%20we%20establish%20entropy-based%20measures%0Aas%20efficient%20proxies%20for%20data%20mixing%2C%20offering%20a%20computationally%20lightweight%0Astrategy.%20Our%20work%20contributes%20both%20theoretical%20insights%20into%20data%20mixing%0Adynamics%20and%20practical%20tools%20for%20enhancing%20LLM%20training%20efficiency%2C%20paving%20the%0Away%20for%20more%20effective%20scaling%20strategies%20in%20language%20model%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14908v4&entry.124074799=Read"},
{"title": "Generating Spatial Synthetic Populations Using Wasserstein Generative\n  Adversarial Network: A Case Study with EU-SILC Data for Helsinki and\n  Thessaloniki", "author": "Vanja Falck", "abstract": "  Using agent-based social simulations can enhance our understanding of urban\nplanning, public health, and economic forecasting. Realistic synthetic\npopulations with numerous attributes strengthen these simulations. The\nWasserstein Generative Adversarial Network, trained on census data like\nEU-SILC, can create robust synthetic populations. These methods, aided by\nexternal statistics or EU-SILC weights, generate spatial synthetic populations\nfor agent-based models. The increased access to high-quality micro-data has\nsparked interest in synthetic populations, which preserve demographic profiles\nand analytical strength while ensuring privacy and preventing discrimination.\nThis study uses national data from Finland and Greece for Helsinki and\nThessaloniki to explore balanced spatial synthetic population generation.\nResults show challenges related to balancing data with or without aggregated\nstatistics for the target population and the general under-representation of\nfringe profiles by deep generative methods. The latter can lead to\ndiscrimination in agent-based simulations.\n", "link": "http://arxiv.org/abs/2501.16080v1", "date": "2025-01-27", "relevancy": 2.003, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5334}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.508}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Spatial%20Synthetic%20Populations%20Using%20Wasserstein%20Generative%0A%20%20Adversarial%20Network%3A%20A%20Case%20Study%20with%20EU-SILC%20Data%20for%20Helsinki%20and%0A%20%20Thessaloniki&body=Title%3A%20Generating%20Spatial%20Synthetic%20Populations%20Using%20Wasserstein%20Generative%0A%20%20Adversarial%20Network%3A%20A%20Case%20Study%20with%20EU-SILC%20Data%20for%20Helsinki%20and%0A%20%20Thessaloniki%0AAuthor%3A%20Vanja%20Falck%0AAbstract%3A%20%20%20Using%20agent-based%20social%20simulations%20can%20enhance%20our%20understanding%20of%20urban%0Aplanning%2C%20public%20health%2C%20and%20economic%20forecasting.%20Realistic%20synthetic%0Apopulations%20with%20numerous%20attributes%20strengthen%20these%20simulations.%20The%0AWasserstein%20Generative%20Adversarial%20Network%2C%20trained%20on%20census%20data%20like%0AEU-SILC%2C%20can%20create%20robust%20synthetic%20populations.%20These%20methods%2C%20aided%20by%0Aexternal%20statistics%20or%20EU-SILC%20weights%2C%20generate%20spatial%20synthetic%20populations%0Afor%20agent-based%20models.%20The%20increased%20access%20to%20high-quality%20micro-data%20has%0Asparked%20interest%20in%20synthetic%20populations%2C%20which%20preserve%20demographic%20profiles%0Aand%20analytical%20strength%20while%20ensuring%20privacy%20and%20preventing%20discrimination.%0AThis%20study%20uses%20national%20data%20from%20Finland%20and%20Greece%20for%20Helsinki%20and%0AThessaloniki%20to%20explore%20balanced%20spatial%20synthetic%20population%20generation.%0AResults%20show%20challenges%20related%20to%20balancing%20data%20with%20or%20without%20aggregated%0Astatistics%20for%20the%20target%20population%20and%20the%20general%20under-representation%20of%0Afringe%20profiles%20by%20deep%20generative%20methods.%20The%20latter%20can%20lead%20to%0Adiscrimination%20in%20agent-based%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Spatial%2520Synthetic%2520Populations%2520Using%2520Wasserstein%2520Generative%250A%2520%2520Adversarial%2520Network%253A%2520A%2520Case%2520Study%2520with%2520EU-SILC%2520Data%2520for%2520Helsinki%2520and%250A%2520%2520Thessaloniki%26entry.906535625%3DVanja%2520Falck%26entry.1292438233%3D%2520%2520Using%2520agent-based%2520social%2520simulations%2520can%2520enhance%2520our%2520understanding%2520of%2520urban%250Aplanning%252C%2520public%2520health%252C%2520and%2520economic%2520forecasting.%2520Realistic%2520synthetic%250Apopulations%2520with%2520numerous%2520attributes%2520strengthen%2520these%2520simulations.%2520The%250AWasserstein%2520Generative%2520Adversarial%2520Network%252C%2520trained%2520on%2520census%2520data%2520like%250AEU-SILC%252C%2520can%2520create%2520robust%2520synthetic%2520populations.%2520These%2520methods%252C%2520aided%2520by%250Aexternal%2520statistics%2520or%2520EU-SILC%2520weights%252C%2520generate%2520spatial%2520synthetic%2520populations%250Afor%2520agent-based%2520models.%2520The%2520increased%2520access%2520to%2520high-quality%2520micro-data%2520has%250Asparked%2520interest%2520in%2520synthetic%2520populations%252C%2520which%2520preserve%2520demographic%2520profiles%250Aand%2520analytical%2520strength%2520while%2520ensuring%2520privacy%2520and%2520preventing%2520discrimination.%250AThis%2520study%2520uses%2520national%2520data%2520from%2520Finland%2520and%2520Greece%2520for%2520Helsinki%2520and%250AThessaloniki%2520to%2520explore%2520balanced%2520spatial%2520synthetic%2520population%2520generation.%250AResults%2520show%2520challenges%2520related%2520to%2520balancing%2520data%2520with%2520or%2520without%2520aggregated%250Astatistics%2520for%2520the%2520target%2520population%2520and%2520the%2520general%2520under-representation%2520of%250Afringe%2520profiles%2520by%2520deep%2520generative%2520methods.%2520The%2520latter%2520can%2520lead%2520to%250Adiscrimination%2520in%2520agent-based%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Spatial%20Synthetic%20Populations%20Using%20Wasserstein%20Generative%0A%20%20Adversarial%20Network%3A%20A%20Case%20Study%20with%20EU-SILC%20Data%20for%20Helsinki%20and%0A%20%20Thessaloniki&entry.906535625=Vanja%20Falck&entry.1292438233=%20%20Using%20agent-based%20social%20simulations%20can%20enhance%20our%20understanding%20of%20urban%0Aplanning%2C%20public%20health%2C%20and%20economic%20forecasting.%20Realistic%20synthetic%0Apopulations%20with%20numerous%20attributes%20strengthen%20these%20simulations.%20The%0AWasserstein%20Generative%20Adversarial%20Network%2C%20trained%20on%20census%20data%20like%0AEU-SILC%2C%20can%20create%20robust%20synthetic%20populations.%20These%20methods%2C%20aided%20by%0Aexternal%20statistics%20or%20EU-SILC%20weights%2C%20generate%20spatial%20synthetic%20populations%0Afor%20agent-based%20models.%20The%20increased%20access%20to%20high-quality%20micro-data%20has%0Asparked%20interest%20in%20synthetic%20populations%2C%20which%20preserve%20demographic%20profiles%0Aand%20analytical%20strength%20while%20ensuring%20privacy%20and%20preventing%20discrimination.%0AThis%20study%20uses%20national%20data%20from%20Finland%20and%20Greece%20for%20Helsinki%20and%0AThessaloniki%20to%20explore%20balanced%20spatial%20synthetic%20population%20generation.%0AResults%20show%20challenges%20related%20to%20balancing%20data%20with%20or%20without%20aggregated%0Astatistics%20for%20the%20target%20population%20and%20the%20general%20under-representation%20of%0Afringe%20profiles%20by%20deep%20generative%20methods.%20The%20latter%20can%20lead%20to%0Adiscrimination%20in%20agent-based%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16080v1&entry.124074799=Read"},
{"title": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs", "author": "Jialun Cao and Yaojie Lu and Meiziniu Li and Haoyang Ma and Haokun Li and Mengda He and Cheng Wen and Le Sun and Hongyu Zhang and Shengchao Qin and Shing-Chi Cheung and Cong Tian", "abstract": "  The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO, showing significant progress. However, these studies\nintertwined multiple skills simultaneously, i.e., problem-solving, reasoning,\nand writing formal specifications, making it hard to precisely identify the\nLLMs' strengths and weaknesses in each task. This paper focuses on formal\nverification, an immediate application scenario of formal reasoning, and\ndecomposes it into six sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five mainstream formal specification\nlanguages (Coq, Lean4, Dafny, ACSL, and TLA+) in six\nformal-verification-related tasks by distilling GPT-4o. They are split into a\n14k+ fine-tuning dataset FM-alpaca and a 4k benchmark FM-Bench. We found that\nLLMs are good at writing proof segments when given either the code, or the\ndetailed description of proof steps. Also, the fine-tuning brought about a\nnearly threefold improvement at most. Interestingly, we observed that\nfine-tuning with formal data also enhances mathematics, reasoning, and coding\nabilities. We hope our findings inspire further research. Fine-tuned models are\nreleased to facilitate subsequent studies\n", "link": "http://arxiv.org/abs/2501.16207v1", "date": "2025-01-27", "relevancy": 2.0005, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Informal%20to%20Formal%20--%20Incorporating%20and%20Evaluating%20LLMs%20on%20Natural%0A%20%20Language%20Requirements%20to%20Verifiable%20Formal%20Proofs&body=Title%3A%20From%20Informal%20to%20Formal%20--%20Incorporating%20and%20Evaluating%20LLMs%20on%20Natural%0A%20%20Language%20Requirements%20to%20Verifiable%20Formal%20Proofs%0AAuthor%3A%20Jialun%20Cao%20and%20Yaojie%20Lu%20and%20Meiziniu%20Li%20and%20Haoyang%20Ma%20and%20Haokun%20Li%20and%20Mengda%20He%20and%20Cheng%20Wen%20and%20Le%20Sun%20and%20Hongyu%20Zhang%20and%20Shengchao%20Qin%20and%20Shing-Chi%20Cheung%20and%20Cong%20Tian%0AAbstract%3A%20%20%20The%20research%20in%20AI-based%20formal%20mathematical%20reasoning%20has%20shown%20an%0Aunstoppable%20growth%20trend.%20These%20studies%20have%20excelled%20in%20mathematical%0Acompetitions%20like%20IMO%2C%20showing%20significant%20progress.%20However%2C%20these%20studies%0Aintertwined%20multiple%20skills%20simultaneously%2C%20i.e.%2C%20problem-solving%2C%20reasoning%2C%0Aand%20writing%20formal%20specifications%2C%20making%20it%20hard%20to%20precisely%20identify%20the%0ALLMs%27%20strengths%20and%20weaknesses%20in%20each%20task.%20This%20paper%20focuses%20on%20formal%0Averification%2C%20an%20immediate%20application%20scenario%20of%20formal%20reasoning%2C%20and%0Adecomposes%20it%20into%20six%20sub-tasks.%20We%20constructed%2018k%20high-quality%0Ainstruction-response%20pairs%20across%20five%20mainstream%20formal%20specification%0Alanguages%20%28Coq%2C%20Lean4%2C%20Dafny%2C%20ACSL%2C%20and%20TLA%2B%29%20in%20six%0Aformal-verification-related%20tasks%20by%20distilling%20GPT-4o.%20They%20are%20split%20into%20a%0A14k%2B%20fine-tuning%20dataset%20FM-alpaca%20and%20a%204k%20benchmark%20FM-Bench.%20We%20found%20that%0ALLMs%20are%20good%20at%20writing%20proof%20segments%20when%20given%20either%20the%20code%2C%20or%20the%0Adetailed%20description%20of%20proof%20steps.%20Also%2C%20the%20fine-tuning%20brought%20about%20a%0Anearly%20threefold%20improvement%20at%20most.%20Interestingly%2C%20we%20observed%20that%0Afine-tuning%20with%20formal%20data%20also%20enhances%20mathematics%2C%20reasoning%2C%20and%20coding%0Aabilities.%20We%20hope%20our%20findings%20inspire%20further%20research.%20Fine-tuned%20models%20are%0Areleased%20to%20facilitate%20subsequent%20studies%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Informal%2520to%2520Formal%2520--%2520Incorporating%2520and%2520Evaluating%2520LLMs%2520on%2520Natural%250A%2520%2520Language%2520Requirements%2520to%2520Verifiable%2520Formal%2520Proofs%26entry.906535625%3DJialun%2520Cao%2520and%2520Yaojie%2520Lu%2520and%2520Meiziniu%2520Li%2520and%2520Haoyang%2520Ma%2520and%2520Haokun%2520Li%2520and%2520Mengda%2520He%2520and%2520Cheng%2520Wen%2520and%2520Le%2520Sun%2520and%2520Hongyu%2520Zhang%2520and%2520Shengchao%2520Qin%2520and%2520Shing-Chi%2520Cheung%2520and%2520Cong%2520Tian%26entry.1292438233%3D%2520%2520The%2520research%2520in%2520AI-based%2520formal%2520mathematical%2520reasoning%2520has%2520shown%2520an%250Aunstoppable%2520growth%2520trend.%2520These%2520studies%2520have%2520excelled%2520in%2520mathematical%250Acompetitions%2520like%2520IMO%252C%2520showing%2520significant%2520progress.%2520However%252C%2520these%2520studies%250Aintertwined%2520multiple%2520skills%2520simultaneously%252C%2520i.e.%252C%2520problem-solving%252C%2520reasoning%252C%250Aand%2520writing%2520formal%2520specifications%252C%2520making%2520it%2520hard%2520to%2520precisely%2520identify%2520the%250ALLMs%2527%2520strengths%2520and%2520weaknesses%2520in%2520each%2520task.%2520This%2520paper%2520focuses%2520on%2520formal%250Averification%252C%2520an%2520immediate%2520application%2520scenario%2520of%2520formal%2520reasoning%252C%2520and%250Adecomposes%2520it%2520into%2520six%2520sub-tasks.%2520We%2520constructed%252018k%2520high-quality%250Ainstruction-response%2520pairs%2520across%2520five%2520mainstream%2520formal%2520specification%250Alanguages%2520%2528Coq%252C%2520Lean4%252C%2520Dafny%252C%2520ACSL%252C%2520and%2520TLA%252B%2529%2520in%2520six%250Aformal-verification-related%2520tasks%2520by%2520distilling%2520GPT-4o.%2520They%2520are%2520split%2520into%2520a%250A14k%252B%2520fine-tuning%2520dataset%2520FM-alpaca%2520and%2520a%25204k%2520benchmark%2520FM-Bench.%2520We%2520found%2520that%250ALLMs%2520are%2520good%2520at%2520writing%2520proof%2520segments%2520when%2520given%2520either%2520the%2520code%252C%2520or%2520the%250Adetailed%2520description%2520of%2520proof%2520steps.%2520Also%252C%2520the%2520fine-tuning%2520brought%2520about%2520a%250Anearly%2520threefold%2520improvement%2520at%2520most.%2520Interestingly%252C%2520we%2520observed%2520that%250Afine-tuning%2520with%2520formal%2520data%2520also%2520enhances%2520mathematics%252C%2520reasoning%252C%2520and%2520coding%250Aabilities.%2520We%2520hope%2520our%2520findings%2520inspire%2520further%2520research.%2520Fine-tuned%2520models%2520are%250Areleased%2520to%2520facilitate%2520subsequent%2520studies%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Informal%20to%20Formal%20--%20Incorporating%20and%20Evaluating%20LLMs%20on%20Natural%0A%20%20Language%20Requirements%20to%20Verifiable%20Formal%20Proofs&entry.906535625=Jialun%20Cao%20and%20Yaojie%20Lu%20and%20Meiziniu%20Li%20and%20Haoyang%20Ma%20and%20Haokun%20Li%20and%20Mengda%20He%20and%20Cheng%20Wen%20and%20Le%20Sun%20and%20Hongyu%20Zhang%20and%20Shengchao%20Qin%20and%20Shing-Chi%20Cheung%20and%20Cong%20Tian&entry.1292438233=%20%20The%20research%20in%20AI-based%20formal%20mathematical%20reasoning%20has%20shown%20an%0Aunstoppable%20growth%20trend.%20These%20studies%20have%20excelled%20in%20mathematical%0Acompetitions%20like%20IMO%2C%20showing%20significant%20progress.%20However%2C%20these%20studies%0Aintertwined%20multiple%20skills%20simultaneously%2C%20i.e.%2C%20problem-solving%2C%20reasoning%2C%0Aand%20writing%20formal%20specifications%2C%20making%20it%20hard%20to%20precisely%20identify%20the%0ALLMs%27%20strengths%20and%20weaknesses%20in%20each%20task.%20This%20paper%20focuses%20on%20formal%0Averification%2C%20an%20immediate%20application%20scenario%20of%20formal%20reasoning%2C%20and%0Adecomposes%20it%20into%20six%20sub-tasks.%20We%20constructed%2018k%20high-quality%0Ainstruction-response%20pairs%20across%20five%20mainstream%20formal%20specification%0Alanguages%20%28Coq%2C%20Lean4%2C%20Dafny%2C%20ACSL%2C%20and%20TLA%2B%29%20in%20six%0Aformal-verification-related%20tasks%20by%20distilling%20GPT-4o.%20They%20are%20split%20into%20a%0A14k%2B%20fine-tuning%20dataset%20FM-alpaca%20and%20a%204k%20benchmark%20FM-Bench.%20We%20found%20that%0ALLMs%20are%20good%20at%20writing%20proof%20segments%20when%20given%20either%20the%20code%2C%20or%20the%0Adetailed%20description%20of%20proof%20steps.%20Also%2C%20the%20fine-tuning%20brought%20about%20a%0Anearly%20threefold%20improvement%20at%20most.%20Interestingly%2C%20we%20observed%20that%0Afine-tuning%20with%20formal%20data%20also%20enhances%20mathematics%2C%20reasoning%2C%20and%20coding%0Aabilities.%20We%20hope%20our%20findings%20inspire%20further%20research.%20Fine-tuned%20models%20are%0Areleased%20to%20facilitate%20subsequent%20studies%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16207v1&entry.124074799=Read"},
{"title": "Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric\n  Properties of Generated Sea Route Graphs", "author": "Tobias Rohe and Florian Burger and Michael K\u00f6lle and Sebastian W\u00f6lckert and Maximilian Zorn and Claudia Linnhoff-Popien", "abstract": "  The demand for artificially generated data for the development, training and\ntesting of new algorithms is omnipresent. Quantum computing (QC), does offer\nthe hope that its inherent probabilistic functionality can be utilised in this\nfield of generative artificial intelligence. In this study, we use\nquantum-classical hybrid generative adversarial networks (QuGANs) to\nartificially generate graphs of shipping routes. We create a training dataset\nbased on real shipping data and investigate to what extent QuGANs are able to\nlearn and reproduce inherent distributions and geometric features of this data.\nWe compare hybrid QuGANs with classical Generative Adversarial Networks (GANs),\nwith a special focus on their parameter efficiency. Our results indicate that\nQuGANs are indeed able to quickly learn and represent underlying geometric\nproperties and distributions, although they seem to have difficulties in\nintroducing variance into the sampled data. Compared to classical GANs of\ngreater size, measured in the number of parameters used, some QuGANs show\nsimilar result quality. Our reference to concrete use cases, such as the\ngeneration of shipping data, provides an illustrative example and demonstrate\nthe potential and diversity in which QC can be used.\n", "link": "http://arxiv.org/abs/2501.08678v2", "date": "2025-01-27", "relevancy": 2.0003, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5036}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5004}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Parameter-Efficiency%20of%20Hybrid%20QuGANs%20Based%20on%20Geometric%0A%20%20Properties%20of%20Generated%20Sea%20Route%20Graphs&body=Title%3A%20Investigating%20Parameter-Efficiency%20of%20Hybrid%20QuGANs%20Based%20on%20Geometric%0A%20%20Properties%20of%20Generated%20Sea%20Route%20Graphs%0AAuthor%3A%20Tobias%20Rohe%20and%20Florian%20Burger%20and%20Michael%20K%C3%B6lle%20and%20Sebastian%20W%C3%B6lckert%20and%20Maximilian%20Zorn%20and%20Claudia%20Linnhoff-Popien%0AAbstract%3A%20%20%20The%20demand%20for%20artificially%20generated%20data%20for%20the%20development%2C%20training%20and%0Atesting%20of%20new%20algorithms%20is%20omnipresent.%20Quantum%20computing%20%28QC%29%2C%20does%20offer%0Athe%20hope%20that%20its%20inherent%20probabilistic%20functionality%20can%20be%20utilised%20in%20this%0Afield%20of%20generative%20artificial%20intelligence.%20In%20this%20study%2C%20we%20use%0Aquantum-classical%20hybrid%20generative%20adversarial%20networks%20%28QuGANs%29%20to%0Aartificially%20generate%20graphs%20of%20shipping%20routes.%20We%20create%20a%20training%20dataset%0Abased%20on%20real%20shipping%20data%20and%20investigate%20to%20what%20extent%20QuGANs%20are%20able%20to%0Alearn%20and%20reproduce%20inherent%20distributions%20and%20geometric%20features%20of%20this%20data.%0AWe%20compare%20hybrid%20QuGANs%20with%20classical%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%0Awith%20a%20special%20focus%20on%20their%20parameter%20efficiency.%20Our%20results%20indicate%20that%0AQuGANs%20are%20indeed%20able%20to%20quickly%20learn%20and%20represent%20underlying%20geometric%0Aproperties%20and%20distributions%2C%20although%20they%20seem%20to%20have%20difficulties%20in%0Aintroducing%20variance%20into%20the%20sampled%20data.%20Compared%20to%20classical%20GANs%20of%0Agreater%20size%2C%20measured%20in%20the%20number%20of%20parameters%20used%2C%20some%20QuGANs%20show%0Asimilar%20result%20quality.%20Our%20reference%20to%20concrete%20use%20cases%2C%20such%20as%20the%0Ageneration%20of%20shipping%20data%2C%20provides%20an%20illustrative%20example%20and%20demonstrate%0Athe%20potential%20and%20diversity%20in%20which%20QC%20can%20be%20used.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Parameter-Efficiency%2520of%2520Hybrid%2520QuGANs%2520Based%2520on%2520Geometric%250A%2520%2520Properties%2520of%2520Generated%2520Sea%2520Route%2520Graphs%26entry.906535625%3DTobias%2520Rohe%2520and%2520Florian%2520Burger%2520and%2520Michael%2520K%25C3%25B6lle%2520and%2520Sebastian%2520W%25C3%25B6lckert%2520and%2520Maximilian%2520Zorn%2520and%2520Claudia%2520Linnhoff-Popien%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520artificially%2520generated%2520data%2520for%2520the%2520development%252C%2520training%2520and%250Atesting%2520of%2520new%2520algorithms%2520is%2520omnipresent.%2520Quantum%2520computing%2520%2528QC%2529%252C%2520does%2520offer%250Athe%2520hope%2520that%2520its%2520inherent%2520probabilistic%2520functionality%2520can%2520be%2520utilised%2520in%2520this%250Afield%2520of%2520generative%2520artificial%2520intelligence.%2520In%2520this%2520study%252C%2520we%2520use%250Aquantum-classical%2520hybrid%2520generative%2520adversarial%2520networks%2520%2528QuGANs%2529%2520to%250Aartificially%2520generate%2520graphs%2520of%2520shipping%2520routes.%2520We%2520create%2520a%2520training%2520dataset%250Abased%2520on%2520real%2520shipping%2520data%2520and%2520investigate%2520to%2520what%2520extent%2520QuGANs%2520are%2520able%2520to%250Alearn%2520and%2520reproduce%2520inherent%2520distributions%2520and%2520geometric%2520features%2520of%2520this%2520data.%250AWe%2520compare%2520hybrid%2520QuGANs%2520with%2520classical%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%252C%250Awith%2520a%2520special%2520focus%2520on%2520their%2520parameter%2520efficiency.%2520Our%2520results%2520indicate%2520that%250AQuGANs%2520are%2520indeed%2520able%2520to%2520quickly%2520learn%2520and%2520represent%2520underlying%2520geometric%250Aproperties%2520and%2520distributions%252C%2520although%2520they%2520seem%2520to%2520have%2520difficulties%2520in%250Aintroducing%2520variance%2520into%2520the%2520sampled%2520data.%2520Compared%2520to%2520classical%2520GANs%2520of%250Agreater%2520size%252C%2520measured%2520in%2520the%2520number%2520of%2520parameters%2520used%252C%2520some%2520QuGANs%2520show%250Asimilar%2520result%2520quality.%2520Our%2520reference%2520to%2520concrete%2520use%2520cases%252C%2520such%2520as%2520the%250Ageneration%2520of%2520shipping%2520data%252C%2520provides%2520an%2520illustrative%2520example%2520and%2520demonstrate%250Athe%2520potential%2520and%2520diversity%2520in%2520which%2520QC%2520can%2520be%2520used.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Parameter-Efficiency%20of%20Hybrid%20QuGANs%20Based%20on%20Geometric%0A%20%20Properties%20of%20Generated%20Sea%20Route%20Graphs&entry.906535625=Tobias%20Rohe%20and%20Florian%20Burger%20and%20Michael%20K%C3%B6lle%20and%20Sebastian%20W%C3%B6lckert%20and%20Maximilian%20Zorn%20and%20Claudia%20Linnhoff-Popien&entry.1292438233=%20%20The%20demand%20for%20artificially%20generated%20data%20for%20the%20development%2C%20training%20and%0Atesting%20of%20new%20algorithms%20is%20omnipresent.%20Quantum%20computing%20%28QC%29%2C%20does%20offer%0Athe%20hope%20that%20its%20inherent%20probabilistic%20functionality%20can%20be%20utilised%20in%20this%0Afield%20of%20generative%20artificial%20intelligence.%20In%20this%20study%2C%20we%20use%0Aquantum-classical%20hybrid%20generative%20adversarial%20networks%20%28QuGANs%29%20to%0Aartificially%20generate%20graphs%20of%20shipping%20routes.%20We%20create%20a%20training%20dataset%0Abased%20on%20real%20shipping%20data%20and%20investigate%20to%20what%20extent%20QuGANs%20are%20able%20to%0Alearn%20and%20reproduce%20inherent%20distributions%20and%20geometric%20features%20of%20this%20data.%0AWe%20compare%20hybrid%20QuGANs%20with%20classical%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%0Awith%20a%20special%20focus%20on%20their%20parameter%20efficiency.%20Our%20results%20indicate%20that%0AQuGANs%20are%20indeed%20able%20to%20quickly%20learn%20and%20represent%20underlying%20geometric%0Aproperties%20and%20distributions%2C%20although%20they%20seem%20to%20have%20difficulties%20in%0Aintroducing%20variance%20into%20the%20sampled%20data.%20Compared%20to%20classical%20GANs%20of%0Agreater%20size%2C%20measured%20in%20the%20number%20of%20parameters%20used%2C%20some%20QuGANs%20show%0Asimilar%20result%20quality.%20Our%20reference%20to%20concrete%20use%20cases%2C%20such%20as%20the%0Ageneration%20of%20shipping%20data%2C%20provides%20an%20illustrative%20example%20and%20demonstrate%0Athe%20potential%20and%20diversity%20in%20which%20QC%20can%20be%20used.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08678v2&entry.124074799=Read"},
{"title": "MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis", "author": "Mai A. Shaaban and Adnan Khan and Mohammad Yaqub", "abstract": "  Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces MedPromptX, the first clinical decision support\nsystem that integrates multimodal large language models (MLLMs), few-shot\nprompting (FP) and visual grounding (VG) to combine imagery with EHR data for\nchest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing\nEHR information, providing a comprehensive understanding of patients' medical\nhistory. Additionally, FP reduces the necessity for extensive training of MLLMs\nwhile effectively tackling the issue of hallucination. Nevertheless, the\nprocess of determining the optimal number of few-shot examples and selecting\nhigh-quality candidates can be burdensome, yet it profoundly influences model\nperformance. Hence, we propose a new technique that dynamically refines\nfew-shot data for real-time adjustment to new patient scenarios. Moreover, VG\nnarrows the search area in X-ray images, thereby enhancing the identification\nof abnormalities. We also release MedPromptX-VQA, a new in-context visual\nquestion answering dataset encompassing interleaved images and EHR data derived\nfrom MIMIC-IV and MIMIC-CXR-JPG databases. Results demonstrate the SOTA\nperformance of MedPromptX, achieving an 11% improvement in F1-score compared to\nthe baselines. Code and data are publicly available on\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX.\n", "link": "http://arxiv.org/abs/2403.15585v4", "date": "2025-01-27", "relevancy": 1.9954, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedPromptX%3A%20Grounded%20Multimodal%20Prompting%20for%20Chest%20X-ray%20Diagnosis&body=Title%3A%20MedPromptX%3A%20Grounded%20Multimodal%20Prompting%20for%20Chest%20X-ray%20Diagnosis%0AAuthor%3A%20Mai%20A.%20Shaaban%20and%20Adnan%20Khan%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Chest%20X-ray%20images%20are%20commonly%20used%20for%20predicting%20acute%20and%20chronic%0Acardiopulmonary%20conditions%2C%20but%20efforts%20to%20integrate%20them%20with%20structured%0Aclinical%20data%20face%20challenges%20due%20to%20incomplete%20electronic%20health%20records%0A%28EHR%29.%20This%20paper%20introduces%20MedPromptX%2C%20the%20first%20clinical%20decision%20support%0Asystem%20that%20integrates%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20few-shot%0Aprompting%20%28FP%29%20and%20visual%20grounding%20%28VG%29%20to%20combine%20imagery%20with%20EHR%20data%20for%0Achest%20X-ray%20diagnosis.%20A%20pre-trained%20MLLM%20is%20utilized%20to%20complement%20the%20missing%0AEHR%20information%2C%20providing%20a%20comprehensive%20understanding%20of%20patients%27%20medical%0Ahistory.%20Additionally%2C%20FP%20reduces%20the%20necessity%20for%20extensive%20training%20of%20MLLMs%0Awhile%20effectively%20tackling%20the%20issue%20of%20hallucination.%20Nevertheless%2C%20the%0Aprocess%20of%20determining%20the%20optimal%20number%20of%20few-shot%20examples%20and%20selecting%0Ahigh-quality%20candidates%20can%20be%20burdensome%2C%20yet%20it%20profoundly%20influences%20model%0Aperformance.%20Hence%2C%20we%20propose%20a%20new%20technique%20that%20dynamically%20refines%0Afew-shot%20data%20for%20real-time%20adjustment%20to%20new%20patient%20scenarios.%20Moreover%2C%20VG%0Anarrows%20the%20search%20area%20in%20X-ray%20images%2C%20thereby%20enhancing%20the%20identification%0Aof%20abnormalities.%20We%20also%20release%20MedPromptX-VQA%2C%20a%20new%20in-context%20visual%0Aquestion%20answering%20dataset%20encompassing%20interleaved%20images%20and%20EHR%20data%20derived%0Afrom%20MIMIC-IV%20and%20MIMIC-CXR-JPG%20databases.%20Results%20demonstrate%20the%20SOTA%0Aperformance%20of%20MedPromptX%2C%20achieving%20an%2011%25%20improvement%20in%20F1-score%20compared%20to%0Athe%20baselines.%20Code%20and%20data%20are%20publicly%20available%20on%0Ahttps%3A//github.com/BioMedIA-MBZUAI/MedPromptX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15585v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedPromptX%253A%2520Grounded%2520Multimodal%2520Prompting%2520for%2520Chest%2520X-ray%2520Diagnosis%26entry.906535625%3DMai%2520A.%2520Shaaban%2520and%2520Adnan%2520Khan%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Chest%2520X-ray%2520images%2520are%2520commonly%2520used%2520for%2520predicting%2520acute%2520and%2520chronic%250Acardiopulmonary%2520conditions%252C%2520but%2520efforts%2520to%2520integrate%2520them%2520with%2520structured%250Aclinical%2520data%2520face%2520challenges%2520due%2520to%2520incomplete%2520electronic%2520health%2520records%250A%2528EHR%2529.%2520This%2520paper%2520introduces%2520MedPromptX%252C%2520the%2520first%2520clinical%2520decision%2520support%250Asystem%2520that%2520integrates%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520few-shot%250Aprompting%2520%2528FP%2529%2520and%2520visual%2520grounding%2520%2528VG%2529%2520to%2520combine%2520imagery%2520with%2520EHR%2520data%2520for%250Achest%2520X-ray%2520diagnosis.%2520A%2520pre-trained%2520MLLM%2520is%2520utilized%2520to%2520complement%2520the%2520missing%250AEHR%2520information%252C%2520providing%2520a%2520comprehensive%2520understanding%2520of%2520patients%2527%2520medical%250Ahistory.%2520Additionally%252C%2520FP%2520reduces%2520the%2520necessity%2520for%2520extensive%2520training%2520of%2520MLLMs%250Awhile%2520effectively%2520tackling%2520the%2520issue%2520of%2520hallucination.%2520Nevertheless%252C%2520the%250Aprocess%2520of%2520determining%2520the%2520optimal%2520number%2520of%2520few-shot%2520examples%2520and%2520selecting%250Ahigh-quality%2520candidates%2520can%2520be%2520burdensome%252C%2520yet%2520it%2520profoundly%2520influences%2520model%250Aperformance.%2520Hence%252C%2520we%2520propose%2520a%2520new%2520technique%2520that%2520dynamically%2520refines%250Afew-shot%2520data%2520for%2520real-time%2520adjustment%2520to%2520new%2520patient%2520scenarios.%2520Moreover%252C%2520VG%250Anarrows%2520the%2520search%2520area%2520in%2520X-ray%2520images%252C%2520thereby%2520enhancing%2520the%2520identification%250Aof%2520abnormalities.%2520We%2520also%2520release%2520MedPromptX-VQA%252C%2520a%2520new%2520in-context%2520visual%250Aquestion%2520answering%2520dataset%2520encompassing%2520interleaved%2520images%2520and%2520EHR%2520data%2520derived%250Afrom%2520MIMIC-IV%2520and%2520MIMIC-CXR-JPG%2520databases.%2520Results%2520demonstrate%2520the%2520SOTA%250Aperformance%2520of%2520MedPromptX%252C%2520achieving%2520an%252011%2525%2520improvement%2520in%2520F1-score%2520compared%2520to%250Athe%2520baselines.%2520Code%2520and%2520data%2520are%2520publicly%2520available%2520on%250Ahttps%253A//github.com/BioMedIA-MBZUAI/MedPromptX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15585v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedPromptX%3A%20Grounded%20Multimodal%20Prompting%20for%20Chest%20X-ray%20Diagnosis&entry.906535625=Mai%20A.%20Shaaban%20and%20Adnan%20Khan%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Chest%20X-ray%20images%20are%20commonly%20used%20for%20predicting%20acute%20and%20chronic%0Acardiopulmonary%20conditions%2C%20but%20efforts%20to%20integrate%20them%20with%20structured%0Aclinical%20data%20face%20challenges%20due%20to%20incomplete%20electronic%20health%20records%0A%28EHR%29.%20This%20paper%20introduces%20MedPromptX%2C%20the%20first%20clinical%20decision%20support%0Asystem%20that%20integrates%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20few-shot%0Aprompting%20%28FP%29%20and%20visual%20grounding%20%28VG%29%20to%20combine%20imagery%20with%20EHR%20data%20for%0Achest%20X-ray%20diagnosis.%20A%20pre-trained%20MLLM%20is%20utilized%20to%20complement%20the%20missing%0AEHR%20information%2C%20providing%20a%20comprehensive%20understanding%20of%20patients%27%20medical%0Ahistory.%20Additionally%2C%20FP%20reduces%20the%20necessity%20for%20extensive%20training%20of%20MLLMs%0Awhile%20effectively%20tackling%20the%20issue%20of%20hallucination.%20Nevertheless%2C%20the%0Aprocess%20of%20determining%20the%20optimal%20number%20of%20few-shot%20examples%20and%20selecting%0Ahigh-quality%20candidates%20can%20be%20burdensome%2C%20yet%20it%20profoundly%20influences%20model%0Aperformance.%20Hence%2C%20we%20propose%20a%20new%20technique%20that%20dynamically%20refines%0Afew-shot%20data%20for%20real-time%20adjustment%20to%20new%20patient%20scenarios.%20Moreover%2C%20VG%0Anarrows%20the%20search%20area%20in%20X-ray%20images%2C%20thereby%20enhancing%20the%20identification%0Aof%20abnormalities.%20We%20also%20release%20MedPromptX-VQA%2C%20a%20new%20in-context%20visual%0Aquestion%20answering%20dataset%20encompassing%20interleaved%20images%20and%20EHR%20data%20derived%0Afrom%20MIMIC-IV%20and%20MIMIC-CXR-JPG%20databases.%20Results%20demonstrate%20the%20SOTA%0Aperformance%20of%20MedPromptX%2C%20achieving%20an%2011%25%20improvement%20in%20F1-score%20compared%20to%0Athe%20baselines.%20Code%20and%20data%20are%20publicly%20available%20on%0Ahttps%3A//github.com/BioMedIA-MBZUAI/MedPromptX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15585v4&entry.124074799=Read"},
{"title": "Segmentation Dataset for Reinforced Concrete Construction", "author": "Patrick Schmidt and Lazaros Nalpantidis", "abstract": "  This paper provides a dataset of 14,805 RGB images with segmentation labels\nfor autonomous robotic inspection of reinforced concrete defects. Baselines for\nthe YOLOv8L-seg, DeepLabV3, and U-Net segmentation models are established.\nLabelling inconsistencies are addressed statistically, and their influence on\nmodel performance is analyzed. An error identification tool is employed to\nexamine the error modes of the models. The paper demonstrates that YOLOv8L-seg\nperforms best, achieving a validation mIOU score of up to 0.59. Label\ninconsistencies were found to have a negligible effect on model performance,\nwhile the inclusion of more data improved the performance. False negatives were\nidentified as the primary failure mode. The results highlight the importance of\ndata availability for the performance of deep learning-based models. The lack\nof publicly available data is identified as a significant contributor to false\nnegatives. To address this, the paper advocates for an increased open-source\napproach within the construction community.\n", "link": "http://arxiv.org/abs/2407.09372v2", "date": "2025-01-27", "relevancy": 1.986, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5066}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4953}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation%20Dataset%20for%20Reinforced%20Concrete%20Construction&body=Title%3A%20Segmentation%20Dataset%20for%20Reinforced%20Concrete%20Construction%0AAuthor%3A%20Patrick%20Schmidt%20and%20Lazaros%20Nalpantidis%0AAbstract%3A%20%20%20This%20paper%20provides%20a%20dataset%20of%2014%2C805%20RGB%20images%20with%20segmentation%20labels%0Afor%20autonomous%20robotic%20inspection%20of%20reinforced%20concrete%20defects.%20Baselines%20for%0Athe%20YOLOv8L-seg%2C%20DeepLabV3%2C%20and%20U-Net%20segmentation%20models%20are%20established.%0ALabelling%20inconsistencies%20are%20addressed%20statistically%2C%20and%20their%20influence%20on%0Amodel%20performance%20is%20analyzed.%20An%20error%20identification%20tool%20is%20employed%20to%0Aexamine%20the%20error%20modes%20of%20the%20models.%20The%20paper%20demonstrates%20that%20YOLOv8L-seg%0Aperforms%20best%2C%20achieving%20a%20validation%20mIOU%20score%20of%20up%20to%200.59.%20Label%0Ainconsistencies%20were%20found%20to%20have%20a%20negligible%20effect%20on%20model%20performance%2C%0Awhile%20the%20inclusion%20of%20more%20data%20improved%20the%20performance.%20False%20negatives%20were%0Aidentified%20as%20the%20primary%20failure%20mode.%20The%20results%20highlight%20the%20importance%20of%0Adata%20availability%20for%20the%20performance%20of%20deep%20learning-based%20models.%20The%20lack%0Aof%20publicly%20available%20data%20is%20identified%20as%20a%20significant%20contributor%20to%20false%0Anegatives.%20To%20address%20this%2C%20the%20paper%20advocates%20for%20an%20increased%20open-source%0Aapproach%20within%20the%20construction%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation%2520Dataset%2520for%2520Reinforced%2520Concrete%2520Construction%26entry.906535625%3DPatrick%2520Schmidt%2520and%2520Lazaros%2520Nalpantidis%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520a%2520dataset%2520of%252014%252C805%2520RGB%2520images%2520with%2520segmentation%2520labels%250Afor%2520autonomous%2520robotic%2520inspection%2520of%2520reinforced%2520concrete%2520defects.%2520Baselines%2520for%250Athe%2520YOLOv8L-seg%252C%2520DeepLabV3%252C%2520and%2520U-Net%2520segmentation%2520models%2520are%2520established.%250ALabelling%2520inconsistencies%2520are%2520addressed%2520statistically%252C%2520and%2520their%2520influence%2520on%250Amodel%2520performance%2520is%2520analyzed.%2520An%2520error%2520identification%2520tool%2520is%2520employed%2520to%250Aexamine%2520the%2520error%2520modes%2520of%2520the%2520models.%2520The%2520paper%2520demonstrates%2520that%2520YOLOv8L-seg%250Aperforms%2520best%252C%2520achieving%2520a%2520validation%2520mIOU%2520score%2520of%2520up%2520to%25200.59.%2520Label%250Ainconsistencies%2520were%2520found%2520to%2520have%2520a%2520negligible%2520effect%2520on%2520model%2520performance%252C%250Awhile%2520the%2520inclusion%2520of%2520more%2520data%2520improved%2520the%2520performance.%2520False%2520negatives%2520were%250Aidentified%2520as%2520the%2520primary%2520failure%2520mode.%2520The%2520results%2520highlight%2520the%2520importance%2520of%250Adata%2520availability%2520for%2520the%2520performance%2520of%2520deep%2520learning-based%2520models.%2520The%2520lack%250Aof%2520publicly%2520available%2520data%2520is%2520identified%2520as%2520a%2520significant%2520contributor%2520to%2520false%250Anegatives.%2520To%2520address%2520this%252C%2520the%2520paper%2520advocates%2520for%2520an%2520increased%2520open-source%250Aapproach%2520within%2520the%2520construction%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation%20Dataset%20for%20Reinforced%20Concrete%20Construction&entry.906535625=Patrick%20Schmidt%20and%20Lazaros%20Nalpantidis&entry.1292438233=%20%20This%20paper%20provides%20a%20dataset%20of%2014%2C805%20RGB%20images%20with%20segmentation%20labels%0Afor%20autonomous%20robotic%20inspection%20of%20reinforced%20concrete%20defects.%20Baselines%20for%0Athe%20YOLOv8L-seg%2C%20DeepLabV3%2C%20and%20U-Net%20segmentation%20models%20are%20established.%0ALabelling%20inconsistencies%20are%20addressed%20statistically%2C%20and%20their%20influence%20on%0Amodel%20performance%20is%20analyzed.%20An%20error%20identification%20tool%20is%20employed%20to%0Aexamine%20the%20error%20modes%20of%20the%20models.%20The%20paper%20demonstrates%20that%20YOLOv8L-seg%0Aperforms%20best%2C%20achieving%20a%20validation%20mIOU%20score%20of%20up%20to%200.59.%20Label%0Ainconsistencies%20were%20found%20to%20have%20a%20negligible%20effect%20on%20model%20performance%2C%0Awhile%20the%20inclusion%20of%20more%20data%20improved%20the%20performance.%20False%20negatives%20were%0Aidentified%20as%20the%20primary%20failure%20mode.%20The%20results%20highlight%20the%20importance%20of%0Adata%20availability%20for%20the%20performance%20of%20deep%20learning-based%20models.%20The%20lack%0Aof%20publicly%20available%20data%20is%20identified%20as%20a%20significant%20contributor%20to%20false%0Anegatives.%20To%20address%20this%2C%20the%20paper%20advocates%20for%20an%20increased%20open-source%0Aapproach%20within%20the%20construction%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09372v2&entry.124074799=Read"},
{"title": "Language-Based Bayesian Optimization Research Assistant (BORA)", "author": "Abdoulatif Ciss\u00e9 and Xenophon Evangelopoulos and Vladimir V. Gusev and Andrew I. Cooper", "abstract": "  Many important scientific problems involve multivariate optimization coupled\nwith slow and laborious experimental measurements. These complex,\nhigh-dimensional searches can be defined by non-convex optimization landscapes\nthat resemble needle-in-a-haystack surfaces, leading to entrapment in local\nminima. Contextualizing optimizers with human domain knowledge is a powerful\napproach to guide searches to localized fruitful regions. However, this\napproach is susceptible to human confirmation bias and it is also challenging\nfor domain experts to keep track of the rapidly expanding scientific\nliterature. Here, we propose the use of Large Language Models (LLMs) for\ncontextualizing Bayesian optimization (BO) via a hybrid optimization framework\nthat intelligently and economically blends stochastic inference with domain\nknowledge-based insights from the LLM, which is used to suggest new,\nbetter-performing areas of the search space for exploration. Our method fosters\nuser engagement by offering real-time commentary on the optimization progress,\nexplaining the reasoning behind the search strategies. We validate the\neffectiveness of our approach on synthetic benchmarks with up to 15 independent\nvariables and demonstrate the ability of LLMs to reason in four real-world\nexperimental tasks where context-aware suggestions boost optimization\nperformance substantially.\n", "link": "http://arxiv.org/abs/2501.16224v1", "date": "2025-01-27", "relevancy": 1.9822, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Based%20Bayesian%20Optimization%20Research%20Assistant%20%28BORA%29&body=Title%3A%20Language-Based%20Bayesian%20Optimization%20Research%20Assistant%20%28BORA%29%0AAuthor%3A%20Abdoulatif%20Ciss%C3%A9%20and%20Xenophon%20Evangelopoulos%20and%20Vladimir%20V.%20Gusev%20and%20Andrew%20I.%20Cooper%0AAbstract%3A%20%20%20Many%20important%20scientific%20problems%20involve%20multivariate%20optimization%20coupled%0Awith%20slow%20and%20laborious%20experimental%20measurements.%20These%20complex%2C%0Ahigh-dimensional%20searches%20can%20be%20defined%20by%20non-convex%20optimization%20landscapes%0Athat%20resemble%20needle-in-a-haystack%20surfaces%2C%20leading%20to%20entrapment%20in%20local%0Aminima.%20Contextualizing%20optimizers%20with%20human%20domain%20knowledge%20is%20a%20powerful%0Aapproach%20to%20guide%20searches%20to%20localized%20fruitful%20regions.%20However%2C%20this%0Aapproach%20is%20susceptible%20to%20human%20confirmation%20bias%20and%20it%20is%20also%20challenging%0Afor%20domain%20experts%20to%20keep%20track%20of%20the%20rapidly%20expanding%20scientific%0Aliterature.%20Here%2C%20we%20propose%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Acontextualizing%20Bayesian%20optimization%20%28BO%29%20via%20a%20hybrid%20optimization%20framework%0Athat%20intelligently%20and%20economically%20blends%20stochastic%20inference%20with%20domain%0Aknowledge-based%20insights%20from%20the%20LLM%2C%20which%20is%20used%20to%20suggest%20new%2C%0Abetter-performing%20areas%20of%20the%20search%20space%20for%20exploration.%20Our%20method%20fosters%0Auser%20engagement%20by%20offering%20real-time%20commentary%20on%20the%20optimization%20progress%2C%0Aexplaining%20the%20reasoning%20behind%20the%20search%20strategies.%20We%20validate%20the%0Aeffectiveness%20of%20our%20approach%20on%20synthetic%20benchmarks%20with%20up%20to%2015%20independent%0Avariables%20and%20demonstrate%20the%20ability%20of%20LLMs%20to%20reason%20in%20four%20real-world%0Aexperimental%20tasks%20where%20context-aware%20suggestions%20boost%20optimization%0Aperformance%20substantially.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Based%2520Bayesian%2520Optimization%2520Research%2520Assistant%2520%2528BORA%2529%26entry.906535625%3DAbdoulatif%2520Ciss%25C3%25A9%2520and%2520Xenophon%2520Evangelopoulos%2520and%2520Vladimir%2520V.%2520Gusev%2520and%2520Andrew%2520I.%2520Cooper%26entry.1292438233%3D%2520%2520Many%2520important%2520scientific%2520problems%2520involve%2520multivariate%2520optimization%2520coupled%250Awith%2520slow%2520and%2520laborious%2520experimental%2520measurements.%2520These%2520complex%252C%250Ahigh-dimensional%2520searches%2520can%2520be%2520defined%2520by%2520non-convex%2520optimization%2520landscapes%250Athat%2520resemble%2520needle-in-a-haystack%2520surfaces%252C%2520leading%2520to%2520entrapment%2520in%2520local%250Aminima.%2520Contextualizing%2520optimizers%2520with%2520human%2520domain%2520knowledge%2520is%2520a%2520powerful%250Aapproach%2520to%2520guide%2520searches%2520to%2520localized%2520fruitful%2520regions.%2520However%252C%2520this%250Aapproach%2520is%2520susceptible%2520to%2520human%2520confirmation%2520bias%2520and%2520it%2520is%2520also%2520challenging%250Afor%2520domain%2520experts%2520to%2520keep%2520track%2520of%2520the%2520rapidly%2520expanding%2520scientific%250Aliterature.%2520Here%252C%2520we%2520propose%2520the%2520use%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%250Acontextualizing%2520Bayesian%2520optimization%2520%2528BO%2529%2520via%2520a%2520hybrid%2520optimization%2520framework%250Athat%2520intelligently%2520and%2520economically%2520blends%2520stochastic%2520inference%2520with%2520domain%250Aknowledge-based%2520insights%2520from%2520the%2520LLM%252C%2520which%2520is%2520used%2520to%2520suggest%2520new%252C%250Abetter-performing%2520areas%2520of%2520the%2520search%2520space%2520for%2520exploration.%2520Our%2520method%2520fosters%250Auser%2520engagement%2520by%2520offering%2520real-time%2520commentary%2520on%2520the%2520optimization%2520progress%252C%250Aexplaining%2520the%2520reasoning%2520behind%2520the%2520search%2520strategies.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520on%2520synthetic%2520benchmarks%2520with%2520up%2520to%252015%2520independent%250Avariables%2520and%2520demonstrate%2520the%2520ability%2520of%2520LLMs%2520to%2520reason%2520in%2520four%2520real-world%250Aexperimental%2520tasks%2520where%2520context-aware%2520suggestions%2520boost%2520optimization%250Aperformance%2520substantially.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Based%20Bayesian%20Optimization%20Research%20Assistant%20%28BORA%29&entry.906535625=Abdoulatif%20Ciss%C3%A9%20and%20Xenophon%20Evangelopoulos%20and%20Vladimir%20V.%20Gusev%20and%20Andrew%20I.%20Cooper&entry.1292438233=%20%20Many%20important%20scientific%20problems%20involve%20multivariate%20optimization%20coupled%0Awith%20slow%20and%20laborious%20experimental%20measurements.%20These%20complex%2C%0Ahigh-dimensional%20searches%20can%20be%20defined%20by%20non-convex%20optimization%20landscapes%0Athat%20resemble%20needle-in-a-haystack%20surfaces%2C%20leading%20to%20entrapment%20in%20local%0Aminima.%20Contextualizing%20optimizers%20with%20human%20domain%20knowledge%20is%20a%20powerful%0Aapproach%20to%20guide%20searches%20to%20localized%20fruitful%20regions.%20However%2C%20this%0Aapproach%20is%20susceptible%20to%20human%20confirmation%20bias%20and%20it%20is%20also%20challenging%0Afor%20domain%20experts%20to%20keep%20track%20of%20the%20rapidly%20expanding%20scientific%0Aliterature.%20Here%2C%20we%20propose%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Acontextualizing%20Bayesian%20optimization%20%28BO%29%20via%20a%20hybrid%20optimization%20framework%0Athat%20intelligently%20and%20economically%20blends%20stochastic%20inference%20with%20domain%0Aknowledge-based%20insights%20from%20the%20LLM%2C%20which%20is%20used%20to%20suggest%20new%2C%0Abetter-performing%20areas%20of%20the%20search%20space%20for%20exploration.%20Our%20method%20fosters%0Auser%20engagement%20by%20offering%20real-time%20commentary%20on%20the%20optimization%20progress%2C%0Aexplaining%20the%20reasoning%20behind%20the%20search%20strategies.%20We%20validate%20the%0Aeffectiveness%20of%20our%20approach%20on%20synthetic%20benchmarks%20with%20up%20to%2015%20independent%0Avariables%20and%20demonstrate%20the%20ability%20of%20LLMs%20to%20reason%20in%20four%20real-world%0Aexperimental%20tasks%20where%20context-aware%20suggestions%20boost%20optimization%0Aperformance%20substantially.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16224v1&entry.124074799=Read"},
{"title": "Challenging Assumptions in Learning Generic Text Style Embeddings", "author": "Phil Ostheimer and Marius Kloft and Sophie Fellenz", "abstract": "  Recent advancements in language representation learning primarily emphasize\nlanguage modeling for deriving meaningful representations, often neglecting\nstyle-specific considerations. This study addresses this gap by creating\ngeneric, sentence-level style embeddings crucial for style-centric tasks. Our\napproach is grounded on the premise that low-level text style changes can\ncompose any high-level style. We hypothesize that applying this concept to\nrepresentation learning enables the development of versatile text style\nembeddings. By fine-tuning a general-purpose text encoder using contrastive\nlearning and standard cross-entropy loss, we aim to capture these low-level\nstyle shifts, anticipating that they offer insights applicable to high-level\ntext styles. The outcomes prompt us to reconsider the underlying assumptions as\nthe results do not always show that the learned style representations capture\nhigh-level text styles.\n", "link": "http://arxiv.org/abs/2501.16073v1", "date": "2025-01-27", "relevancy": 1.9814, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5077}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4922}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenging%20Assumptions%20in%20Learning%20Generic%20Text%20Style%20Embeddings&body=Title%3A%20Challenging%20Assumptions%20in%20Learning%20Generic%20Text%20Style%20Embeddings%0AAuthor%3A%20Phil%20Ostheimer%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz%0AAbstract%3A%20%20%20Recent%20advancements%20in%20language%20representation%20learning%20primarily%20emphasize%0Alanguage%20modeling%20for%20deriving%20meaningful%20representations%2C%20often%20neglecting%0Astyle-specific%20considerations.%20This%20study%20addresses%20this%20gap%20by%20creating%0Ageneric%2C%20sentence-level%20style%20embeddings%20crucial%20for%20style-centric%20tasks.%20Our%0Aapproach%20is%20grounded%20on%20the%20premise%20that%20low-level%20text%20style%20changes%20can%0Acompose%20any%20high-level%20style.%20We%20hypothesize%20that%20applying%20this%20concept%20to%0Arepresentation%20learning%20enables%20the%20development%20of%20versatile%20text%20style%0Aembeddings.%20By%20fine-tuning%20a%20general-purpose%20text%20encoder%20using%20contrastive%0Alearning%20and%20standard%20cross-entropy%20loss%2C%20we%20aim%20to%20capture%20these%20low-level%0Astyle%20shifts%2C%20anticipating%20that%20they%20offer%20insights%20applicable%20to%20high-level%0Atext%20styles.%20The%20outcomes%20prompt%20us%20to%20reconsider%20the%20underlying%20assumptions%20as%0Athe%20results%20do%20not%20always%20show%20that%20the%20learned%20style%20representations%20capture%0Ahigh-level%20text%20styles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenging%2520Assumptions%2520in%2520Learning%2520Generic%2520Text%2520Style%2520Embeddings%26entry.906535625%3DPhil%2520Ostheimer%2520and%2520Marius%2520Kloft%2520and%2520Sophie%2520Fellenz%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520language%2520representation%2520learning%2520primarily%2520emphasize%250Alanguage%2520modeling%2520for%2520deriving%2520meaningful%2520representations%252C%2520often%2520neglecting%250Astyle-specific%2520considerations.%2520This%2520study%2520addresses%2520this%2520gap%2520by%2520creating%250Ageneric%252C%2520sentence-level%2520style%2520embeddings%2520crucial%2520for%2520style-centric%2520tasks.%2520Our%250Aapproach%2520is%2520grounded%2520on%2520the%2520premise%2520that%2520low-level%2520text%2520style%2520changes%2520can%250Acompose%2520any%2520high-level%2520style.%2520We%2520hypothesize%2520that%2520applying%2520this%2520concept%2520to%250Arepresentation%2520learning%2520enables%2520the%2520development%2520of%2520versatile%2520text%2520style%250Aembeddings.%2520By%2520fine-tuning%2520a%2520general-purpose%2520text%2520encoder%2520using%2520contrastive%250Alearning%2520and%2520standard%2520cross-entropy%2520loss%252C%2520we%2520aim%2520to%2520capture%2520these%2520low-level%250Astyle%2520shifts%252C%2520anticipating%2520that%2520they%2520offer%2520insights%2520applicable%2520to%2520high-level%250Atext%2520styles.%2520The%2520outcomes%2520prompt%2520us%2520to%2520reconsider%2520the%2520underlying%2520assumptions%2520as%250Athe%2520results%2520do%2520not%2520always%2520show%2520that%2520the%2520learned%2520style%2520representations%2520capture%250Ahigh-level%2520text%2520styles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenging%20Assumptions%20in%20Learning%20Generic%20Text%20Style%20Embeddings&entry.906535625=Phil%20Ostheimer%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz&entry.1292438233=%20%20Recent%20advancements%20in%20language%20representation%20learning%20primarily%20emphasize%0Alanguage%20modeling%20for%20deriving%20meaningful%20representations%2C%20often%20neglecting%0Astyle-specific%20considerations.%20This%20study%20addresses%20this%20gap%20by%20creating%0Ageneric%2C%20sentence-level%20style%20embeddings%20crucial%20for%20style-centric%20tasks.%20Our%0Aapproach%20is%20grounded%20on%20the%20premise%20that%20low-level%20text%20style%20changes%20can%0Acompose%20any%20high-level%20style.%20We%20hypothesize%20that%20applying%20this%20concept%20to%0Arepresentation%20learning%20enables%20the%20development%20of%20versatile%20text%20style%0Aembeddings.%20By%20fine-tuning%20a%20general-purpose%20text%20encoder%20using%20contrastive%0Alearning%20and%20standard%20cross-entropy%20loss%2C%20we%20aim%20to%20capture%20these%20low-level%0Astyle%20shifts%2C%20anticipating%20that%20they%20offer%20insights%20applicable%20to%20high-level%0Atext%20styles.%20The%20outcomes%20prompt%20us%20to%20reconsider%20the%20underlying%20assumptions%20as%0Athe%20results%20do%20not%20always%20show%20that%20the%20learned%20style%20representations%20capture%0Ahigh-level%20text%20styles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16073v1&entry.124074799=Read"},
{"title": "Implicit Bias in Matrix Factorization and its Explicit Realization in a\n  New Architecture", "author": "Yikun Hou and Suvrit Sra and Alp Yurtsever", "abstract": "  Gradient descent for matrix factorization is known to exhibit an implicit\nbias toward approximately low-rank solutions. While existing theories often\nassume the boundedness of iterates, empirically the bias persists even with\nunbounded sequences. We thus hypothesize that implicit bias is driven by\ndivergent dynamics markedly different from the convergent dynamics for data\nfitting. Using this perspective, we introduce a new factorization model:\n$X\\approx UDV^\\top$, where $U$ and $V$ are constrained within norm balls, while\n$D$ is a diagonal factor allowing the model to span the entire search space.\nOur experiments reveal that this model exhibits a strong implicit bias\nregardless of initialization and step size, yielding truly (rather than\napproximately) low-rank solutions. Furthermore, drawing parallels between\nmatrix factorization and neural networks, we propose a novel neural network\nmodel featuring constrained layers and diagonal components. This model achieves\nstrong performance across various regression and classification tasks while\nfinding low-rank solutions, resulting in efficient and lightweight networks.\n", "link": "http://arxiv.org/abs/2501.16322v1", "date": "2025-01-27", "relevancy": 1.9741, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.517}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5035}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Bias%20in%20Matrix%20Factorization%20and%20its%20Explicit%20Realization%20in%20a%0A%20%20New%20Architecture&body=Title%3A%20Implicit%20Bias%20in%20Matrix%20Factorization%20and%20its%20Explicit%20Realization%20in%20a%0A%20%20New%20Architecture%0AAuthor%3A%20Yikun%20Hou%20and%20Suvrit%20Sra%20and%20Alp%20Yurtsever%0AAbstract%3A%20%20%20Gradient%20descent%20for%20matrix%20factorization%20is%20known%20to%20exhibit%20an%20implicit%0Abias%20toward%20approximately%20low-rank%20solutions.%20While%20existing%20theories%20often%0Aassume%20the%20boundedness%20of%20iterates%2C%20empirically%20the%20bias%20persists%20even%20with%0Aunbounded%20sequences.%20We%20thus%20hypothesize%20that%20implicit%20bias%20is%20driven%20by%0Adivergent%20dynamics%20markedly%20different%20from%20the%20convergent%20dynamics%20for%20data%0Afitting.%20Using%20this%20perspective%2C%20we%20introduce%20a%20new%20factorization%20model%3A%0A%24X%5Capprox%20UDV%5E%5Ctop%24%2C%20where%20%24U%24%20and%20%24V%24%20are%20constrained%20within%20norm%20balls%2C%20while%0A%24D%24%20is%20a%20diagonal%20factor%20allowing%20the%20model%20to%20span%20the%20entire%20search%20space.%0AOur%20experiments%20reveal%20that%20this%20model%20exhibits%20a%20strong%20implicit%20bias%0Aregardless%20of%20initialization%20and%20step%20size%2C%20yielding%20truly%20%28rather%20than%0Aapproximately%29%20low-rank%20solutions.%20Furthermore%2C%20drawing%20parallels%20between%0Amatrix%20factorization%20and%20neural%20networks%2C%20we%20propose%20a%20novel%20neural%20network%0Amodel%20featuring%20constrained%20layers%20and%20diagonal%20components.%20This%20model%20achieves%0Astrong%20performance%20across%20various%20regression%20and%20classification%20tasks%20while%0Afinding%20low-rank%20solutions%2C%20resulting%20in%20efficient%20and%20lightweight%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Bias%2520in%2520Matrix%2520Factorization%2520and%2520its%2520Explicit%2520Realization%2520in%2520a%250A%2520%2520New%2520Architecture%26entry.906535625%3DYikun%2520Hou%2520and%2520Suvrit%2520Sra%2520and%2520Alp%2520Yurtsever%26entry.1292438233%3D%2520%2520Gradient%2520descent%2520for%2520matrix%2520factorization%2520is%2520known%2520to%2520exhibit%2520an%2520implicit%250Abias%2520toward%2520approximately%2520low-rank%2520solutions.%2520While%2520existing%2520theories%2520often%250Aassume%2520the%2520boundedness%2520of%2520iterates%252C%2520empirically%2520the%2520bias%2520persists%2520even%2520with%250Aunbounded%2520sequences.%2520We%2520thus%2520hypothesize%2520that%2520implicit%2520bias%2520is%2520driven%2520by%250Adivergent%2520dynamics%2520markedly%2520different%2520from%2520the%2520convergent%2520dynamics%2520for%2520data%250Afitting.%2520Using%2520this%2520perspective%252C%2520we%2520introduce%2520a%2520new%2520factorization%2520model%253A%250A%2524X%255Capprox%2520UDV%255E%255Ctop%2524%252C%2520where%2520%2524U%2524%2520and%2520%2524V%2524%2520are%2520constrained%2520within%2520norm%2520balls%252C%2520while%250A%2524D%2524%2520is%2520a%2520diagonal%2520factor%2520allowing%2520the%2520model%2520to%2520span%2520the%2520entire%2520search%2520space.%250AOur%2520experiments%2520reveal%2520that%2520this%2520model%2520exhibits%2520a%2520strong%2520implicit%2520bias%250Aregardless%2520of%2520initialization%2520and%2520step%2520size%252C%2520yielding%2520truly%2520%2528rather%2520than%250Aapproximately%2529%2520low-rank%2520solutions.%2520Furthermore%252C%2520drawing%2520parallels%2520between%250Amatrix%2520factorization%2520and%2520neural%2520networks%252C%2520we%2520propose%2520a%2520novel%2520neural%2520network%250Amodel%2520featuring%2520constrained%2520layers%2520and%2520diagonal%2520components.%2520This%2520model%2520achieves%250Astrong%2520performance%2520across%2520various%2520regression%2520and%2520classification%2520tasks%2520while%250Afinding%2520low-rank%2520solutions%252C%2520resulting%2520in%2520efficient%2520and%2520lightweight%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Bias%20in%20Matrix%20Factorization%20and%20its%20Explicit%20Realization%20in%20a%0A%20%20New%20Architecture&entry.906535625=Yikun%20Hou%20and%20Suvrit%20Sra%20and%20Alp%20Yurtsever&entry.1292438233=%20%20Gradient%20descent%20for%20matrix%20factorization%20is%20known%20to%20exhibit%20an%20implicit%0Abias%20toward%20approximately%20low-rank%20solutions.%20While%20existing%20theories%20often%0Aassume%20the%20boundedness%20of%20iterates%2C%20empirically%20the%20bias%20persists%20even%20with%0Aunbounded%20sequences.%20We%20thus%20hypothesize%20that%20implicit%20bias%20is%20driven%20by%0Adivergent%20dynamics%20markedly%20different%20from%20the%20convergent%20dynamics%20for%20data%0Afitting.%20Using%20this%20perspective%2C%20we%20introduce%20a%20new%20factorization%20model%3A%0A%24X%5Capprox%20UDV%5E%5Ctop%24%2C%20where%20%24U%24%20and%20%24V%24%20are%20constrained%20within%20norm%20balls%2C%20while%0A%24D%24%20is%20a%20diagonal%20factor%20allowing%20the%20model%20to%20span%20the%20entire%20search%20space.%0AOur%20experiments%20reveal%20that%20this%20model%20exhibits%20a%20strong%20implicit%20bias%0Aregardless%20of%20initialization%20and%20step%20size%2C%20yielding%20truly%20%28rather%20than%0Aapproximately%29%20low-rank%20solutions.%20Furthermore%2C%20drawing%20parallels%20between%0Amatrix%20factorization%20and%20neural%20networks%2C%20we%20propose%20a%20novel%20neural%20network%0Amodel%20featuring%20constrained%20layers%20and%20diagonal%20components.%20This%20model%20achieves%0Astrong%20performance%20across%20various%20regression%20and%20classification%20tasks%20while%0Afinding%20low-rank%20solutions%2C%20resulting%20in%20efficient%20and%20lightweight%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16322v1&entry.124074799=Read"},
{"title": "Robust and highly scalable estimation of directional couplings from\n  time-shifted signals", "author": "Louis Rouillard and Luca Ambrogioni and Demian Wassermann", "abstract": "  The estimation of directed couplings between the nodes of a network from\nindirect measurements is a central methodological challenge in scientific\nfields such as neuroscience, systems biology and economics. Unfortunately, the\nproblem is generally ill-posed due to the possible presence of unknown delays\nin the measurements. In this paper, we offer a solution of this problem by\nusing a variational Bayes framework, where the uncertainty over the delays is\nmarginalized in order to obtain conservative coupling estimates. To overcome\nthe well-known overconfidence of classical variational methods, we use a\nhybrid-VI scheme where the (possibly flat or multimodal) posterior over the\nmeasurement parameters is estimated using a forward KL loss while the (nearly\nconvex) conditional posterior over the couplings is estimated using the highly\nscalable gradient-based VI. In our ground-truth experiments, we show that the\nnetwork provides reliable and conservative estimates of the couplings, greatly\noutperforming similar methods such as regression DCM.\n", "link": "http://arxiv.org/abs/2406.02545v2", "date": "2025-01-27", "relevancy": 1.9718, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5469}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4901}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20highly%20scalable%20estimation%20of%20directional%20couplings%20from%0A%20%20time-shifted%20signals&body=Title%3A%20Robust%20and%20highly%20scalable%20estimation%20of%20directional%20couplings%20from%0A%20%20time-shifted%20signals%0AAuthor%3A%20Louis%20Rouillard%20and%20Luca%20Ambrogioni%20and%20Demian%20Wassermann%0AAbstract%3A%20%20%20The%20estimation%20of%20directed%20couplings%20between%20the%20nodes%20of%20a%20network%20from%0Aindirect%20measurements%20is%20a%20central%20methodological%20challenge%20in%20scientific%0Afields%20such%20as%20neuroscience%2C%20systems%20biology%20and%20economics.%20Unfortunately%2C%20the%0Aproblem%20is%20generally%20ill-posed%20due%20to%20the%20possible%20presence%20of%20unknown%20delays%0Ain%20the%20measurements.%20In%20this%20paper%2C%20we%20offer%20a%20solution%20of%20this%20problem%20by%0Ausing%20a%20variational%20Bayes%20framework%2C%20where%20the%20uncertainty%20over%20the%20delays%20is%0Amarginalized%20in%20order%20to%20obtain%20conservative%20coupling%20estimates.%20To%20overcome%0Athe%20well-known%20overconfidence%20of%20classical%20variational%20methods%2C%20we%20use%20a%0Ahybrid-VI%20scheme%20where%20the%20%28possibly%20flat%20or%20multimodal%29%20posterior%20over%20the%0Ameasurement%20parameters%20is%20estimated%20using%20a%20forward%20KL%20loss%20while%20the%20%28nearly%0Aconvex%29%20conditional%20posterior%20over%20the%20couplings%20is%20estimated%20using%20the%20highly%0Ascalable%20gradient-based%20VI.%20In%20our%20ground-truth%20experiments%2C%20we%20show%20that%20the%0Anetwork%20provides%20reliable%20and%20conservative%20estimates%20of%20the%20couplings%2C%20greatly%0Aoutperforming%20similar%20methods%20such%20as%20regression%20DCM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520highly%2520scalable%2520estimation%2520of%2520directional%2520couplings%2520from%250A%2520%2520time-shifted%2520signals%26entry.906535625%3DLouis%2520Rouillard%2520and%2520Luca%2520Ambrogioni%2520and%2520Demian%2520Wassermann%26entry.1292438233%3D%2520%2520The%2520estimation%2520of%2520directed%2520couplings%2520between%2520the%2520nodes%2520of%2520a%2520network%2520from%250Aindirect%2520measurements%2520is%2520a%2520central%2520methodological%2520challenge%2520in%2520scientific%250Afields%2520such%2520as%2520neuroscience%252C%2520systems%2520biology%2520and%2520economics.%2520Unfortunately%252C%2520the%250Aproblem%2520is%2520generally%2520ill-posed%2520due%2520to%2520the%2520possible%2520presence%2520of%2520unknown%2520delays%250Ain%2520the%2520measurements.%2520In%2520this%2520paper%252C%2520we%2520offer%2520a%2520solution%2520of%2520this%2520problem%2520by%250Ausing%2520a%2520variational%2520Bayes%2520framework%252C%2520where%2520the%2520uncertainty%2520over%2520the%2520delays%2520is%250Amarginalized%2520in%2520order%2520to%2520obtain%2520conservative%2520coupling%2520estimates.%2520To%2520overcome%250Athe%2520well-known%2520overconfidence%2520of%2520classical%2520variational%2520methods%252C%2520we%2520use%2520a%250Ahybrid-VI%2520scheme%2520where%2520the%2520%2528possibly%2520flat%2520or%2520multimodal%2529%2520posterior%2520over%2520the%250Ameasurement%2520parameters%2520is%2520estimated%2520using%2520a%2520forward%2520KL%2520loss%2520while%2520the%2520%2528nearly%250Aconvex%2529%2520conditional%2520posterior%2520over%2520the%2520couplings%2520is%2520estimated%2520using%2520the%2520highly%250Ascalable%2520gradient-based%2520VI.%2520In%2520our%2520ground-truth%2520experiments%252C%2520we%2520show%2520that%2520the%250Anetwork%2520provides%2520reliable%2520and%2520conservative%2520estimates%2520of%2520the%2520couplings%252C%2520greatly%250Aoutperforming%2520similar%2520methods%2520such%2520as%2520regression%2520DCM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20highly%20scalable%20estimation%20of%20directional%20couplings%20from%0A%20%20time-shifted%20signals&entry.906535625=Louis%20Rouillard%20and%20Luca%20Ambrogioni%20and%20Demian%20Wassermann&entry.1292438233=%20%20The%20estimation%20of%20directed%20couplings%20between%20the%20nodes%20of%20a%20network%20from%0Aindirect%20measurements%20is%20a%20central%20methodological%20challenge%20in%20scientific%0Afields%20such%20as%20neuroscience%2C%20systems%20biology%20and%20economics.%20Unfortunately%2C%20the%0Aproblem%20is%20generally%20ill-posed%20due%20to%20the%20possible%20presence%20of%20unknown%20delays%0Ain%20the%20measurements.%20In%20this%20paper%2C%20we%20offer%20a%20solution%20of%20this%20problem%20by%0Ausing%20a%20variational%20Bayes%20framework%2C%20where%20the%20uncertainty%20over%20the%20delays%20is%0Amarginalized%20in%20order%20to%20obtain%20conservative%20coupling%20estimates.%20To%20overcome%0Athe%20well-known%20overconfidence%20of%20classical%20variational%20methods%2C%20we%20use%20a%0Ahybrid-VI%20scheme%20where%20the%20%28possibly%20flat%20or%20multimodal%29%20posterior%20over%20the%0Ameasurement%20parameters%20is%20estimated%20using%20a%20forward%20KL%20loss%20while%20the%20%28nearly%0Aconvex%29%20conditional%20posterior%20over%20the%20couplings%20is%20estimated%20using%20the%20highly%0Ascalable%20gradient-based%20VI.%20In%20our%20ground-truth%20experiments%2C%20we%20show%20that%20the%0Anetwork%20provides%20reliable%20and%20conservative%20estimates%20of%20the%20couplings%2C%20greatly%0Aoutperforming%20similar%20methods%20such%20as%20regression%20DCM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02545v2&entry.124074799=Read"},
{"title": "Contextual Feedback Loops: Amplifying Deep Reasoning with Iterative\n  Top-Down Feedback", "author": "Jacob Fein-Ashley and Rajgopal Kannan and Viktor Prasanna", "abstract": "  Conventional deep networks rely on one-way backpropagation that overlooks\nreconciling high-level predictions with lower-level representations. We propose\n\\emph{Contextual Feedback Loops} (CFLs), a lightweight mechanism that\nre-injects top-down context into earlier layers for iterative refinement.\nConcretely, CFLs map the network's prediction to a compact \\emph{context\nvector}, which is fused back into each layer via gating adapters. Unrolled over\nmultiple feedback steps, CFLs unify feed-forward and feedback-driven inference,\nletting top-level outputs continually refine lower-level features. Despite\nminimal overhead, CFLs yield consistent gains on tasks including CIFAR-10,\nImageNet-1k, SpeechCommands, and GLUE SST-2. Moreover, by a Banach Fixed Point\nargument under mild Lipschitz conditions, these updates converge stably.\nOverall, CFLs show that even modest top-down feedback can substantially improve\ndeep models, aligning with cognitive theories of iterative perception.\n", "link": "http://arxiv.org/abs/2412.17737v5", "date": "2025-01-27", "relevancy": 1.9657, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5009}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Feedback%20Loops%3A%20Amplifying%20Deep%20Reasoning%20with%20Iterative%0A%20%20Top-Down%20Feedback&body=Title%3A%20Contextual%20Feedback%20Loops%3A%20Amplifying%20Deep%20Reasoning%20with%20Iterative%0A%20%20Top-Down%20Feedback%0AAuthor%3A%20Jacob%20Fein-Ashley%20and%20Rajgopal%20Kannan%20and%20Viktor%20Prasanna%0AAbstract%3A%20%20%20Conventional%20deep%20networks%20rely%20on%20one-way%20backpropagation%20that%20overlooks%0Areconciling%20high-level%20predictions%20with%20lower-level%20representations.%20We%20propose%0A%5Cemph%7BContextual%20Feedback%20Loops%7D%20%28CFLs%29%2C%20a%20lightweight%20mechanism%20that%0Are-injects%20top-down%20context%20into%20earlier%20layers%20for%20iterative%20refinement.%0AConcretely%2C%20CFLs%20map%20the%20network%27s%20prediction%20to%20a%20compact%20%5Cemph%7Bcontext%0Avector%7D%2C%20which%20is%20fused%20back%20into%20each%20layer%20via%20gating%20adapters.%20Unrolled%20over%0Amultiple%20feedback%20steps%2C%20CFLs%20unify%20feed-forward%20and%20feedback-driven%20inference%2C%0Aletting%20top-level%20outputs%20continually%20refine%20lower-level%20features.%20Despite%0Aminimal%20overhead%2C%20CFLs%20yield%20consistent%20gains%20on%20tasks%20including%20CIFAR-10%2C%0AImageNet-1k%2C%20SpeechCommands%2C%20and%20GLUE%20SST-2.%20Moreover%2C%20by%20a%20Banach%20Fixed%20Point%0Aargument%20under%20mild%20Lipschitz%20conditions%2C%20these%20updates%20converge%20stably.%0AOverall%2C%20CFLs%20show%20that%20even%20modest%20top-down%20feedback%20can%20substantially%20improve%0Adeep%20models%2C%20aligning%20with%20cognitive%20theories%20of%20iterative%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17737v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Feedback%2520Loops%253A%2520Amplifying%2520Deep%2520Reasoning%2520with%2520Iterative%250A%2520%2520Top-Down%2520Feedback%26entry.906535625%3DJacob%2520Fein-Ashley%2520and%2520Rajgopal%2520Kannan%2520and%2520Viktor%2520Prasanna%26entry.1292438233%3D%2520%2520Conventional%2520deep%2520networks%2520rely%2520on%2520one-way%2520backpropagation%2520that%2520overlooks%250Areconciling%2520high-level%2520predictions%2520with%2520lower-level%2520representations.%2520We%2520propose%250A%255Cemph%257BContextual%2520Feedback%2520Loops%257D%2520%2528CFLs%2529%252C%2520a%2520lightweight%2520mechanism%2520that%250Are-injects%2520top-down%2520context%2520into%2520earlier%2520layers%2520for%2520iterative%2520refinement.%250AConcretely%252C%2520CFLs%2520map%2520the%2520network%2527s%2520prediction%2520to%2520a%2520compact%2520%255Cemph%257Bcontext%250Avector%257D%252C%2520which%2520is%2520fused%2520back%2520into%2520each%2520layer%2520via%2520gating%2520adapters.%2520Unrolled%2520over%250Amultiple%2520feedback%2520steps%252C%2520CFLs%2520unify%2520feed-forward%2520and%2520feedback-driven%2520inference%252C%250Aletting%2520top-level%2520outputs%2520continually%2520refine%2520lower-level%2520features.%2520Despite%250Aminimal%2520overhead%252C%2520CFLs%2520yield%2520consistent%2520gains%2520on%2520tasks%2520including%2520CIFAR-10%252C%250AImageNet-1k%252C%2520SpeechCommands%252C%2520and%2520GLUE%2520SST-2.%2520Moreover%252C%2520by%2520a%2520Banach%2520Fixed%2520Point%250Aargument%2520under%2520mild%2520Lipschitz%2520conditions%252C%2520these%2520updates%2520converge%2520stably.%250AOverall%252C%2520CFLs%2520show%2520that%2520even%2520modest%2520top-down%2520feedback%2520can%2520substantially%2520improve%250Adeep%2520models%252C%2520aligning%2520with%2520cognitive%2520theories%2520of%2520iterative%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17737v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Feedback%20Loops%3A%20Amplifying%20Deep%20Reasoning%20with%20Iterative%0A%20%20Top-Down%20Feedback&entry.906535625=Jacob%20Fein-Ashley%20and%20Rajgopal%20Kannan%20and%20Viktor%20Prasanna&entry.1292438233=%20%20Conventional%20deep%20networks%20rely%20on%20one-way%20backpropagation%20that%20overlooks%0Areconciling%20high-level%20predictions%20with%20lower-level%20representations.%20We%20propose%0A%5Cemph%7BContextual%20Feedback%20Loops%7D%20%28CFLs%29%2C%20a%20lightweight%20mechanism%20that%0Are-injects%20top-down%20context%20into%20earlier%20layers%20for%20iterative%20refinement.%0AConcretely%2C%20CFLs%20map%20the%20network%27s%20prediction%20to%20a%20compact%20%5Cemph%7Bcontext%0Avector%7D%2C%20which%20is%20fused%20back%20into%20each%20layer%20via%20gating%20adapters.%20Unrolled%20over%0Amultiple%20feedback%20steps%2C%20CFLs%20unify%20feed-forward%20and%20feedback-driven%20inference%2C%0Aletting%20top-level%20outputs%20continually%20refine%20lower-level%20features.%20Despite%0Aminimal%20overhead%2C%20CFLs%20yield%20consistent%20gains%20on%20tasks%20including%20CIFAR-10%2C%0AImageNet-1k%2C%20SpeechCommands%2C%20and%20GLUE%20SST-2.%20Moreover%2C%20by%20a%20Banach%20Fixed%20Point%0Aargument%20under%20mild%20Lipschitz%20conditions%2C%20these%20updates%20converge%20stably.%0AOverall%2C%20CFLs%20show%20that%20even%20modest%20top-down%20feedback%20can%20substantially%20improve%0Adeep%20models%2C%20aligning%20with%20cognitive%20theories%20of%20iterative%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17737v5&entry.124074799=Read"},
{"title": "Enhancing the Convergence of Federated Learning Aggregation Strategies\n  with Limited Data", "author": "Judith S\u00e1inz-Pardo D\u00edaz and \u00c1lvaro L\u00f3pez Garc\u00eda", "abstract": "  The development of deep learning techniques is a leading field applied to\ncases in which medical data is used, particularly in cases of image diagnosis.\nThis type of data has privacy and legal restrictions that in many cases prevent\nit from being processed from central servers. However, in this area\ncollaboration between different research centers, in order to create models as\nrobust as possible, trained with the largest quantity and diversity of data\navailable, is a critical point to be taken into account. In this sense, the\napplication of privacy aware distributed architectures, such as federated\nlearning arises. When applying this type of architecture, the server aggregates\nthe different local models trained with the data of each data owner to build a\nglobal model. This point is critical and therefore it is fundamental to analyze\ndifferent ways of aggregation according to the use case, taking into account\nthe distribution of the clients, the characteristics of the model, etc. In this\npaper we propose a novel aggregation strategy and we apply it to a use case of\ncerebral magnetic resonance image classification. In this use case the\naggregation function proposed manages to improve the convergence obtained over\nthe rounds of the federated learning process in relation to different\naggregation strategies classically implemented and applied.\n", "link": "http://arxiv.org/abs/2501.15949v1", "date": "2025-01-27", "relevancy": 1.9642, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5007}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.491}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20the%20Convergence%20of%20Federated%20Learning%20Aggregation%20Strategies%0A%20%20with%20Limited%20Data&body=Title%3A%20Enhancing%20the%20Convergence%20of%20Federated%20Learning%20Aggregation%20Strategies%0A%20%20with%20Limited%20Data%0AAuthor%3A%20Judith%20S%C3%A1inz-Pardo%20D%C3%ADaz%20and%20%C3%81lvaro%20L%C3%B3pez%20Garc%C3%ADa%0AAbstract%3A%20%20%20The%20development%20of%20deep%20learning%20techniques%20is%20a%20leading%20field%20applied%20to%0Acases%20in%20which%20medical%20data%20is%20used%2C%20particularly%20in%20cases%20of%20image%20diagnosis.%0AThis%20type%20of%20data%20has%20privacy%20and%20legal%20restrictions%20that%20in%20many%20cases%20prevent%0Ait%20from%20being%20processed%20from%20central%20servers.%20However%2C%20in%20this%20area%0Acollaboration%20between%20different%20research%20centers%2C%20in%20order%20to%20create%20models%20as%0Arobust%20as%20possible%2C%20trained%20with%20the%20largest%20quantity%20and%20diversity%20of%20data%0Aavailable%2C%20is%20a%20critical%20point%20to%20be%20taken%20into%20account.%20In%20this%20sense%2C%20the%0Aapplication%20of%20privacy%20aware%20distributed%20architectures%2C%20such%20as%20federated%0Alearning%20arises.%20When%20applying%20this%20type%20of%20architecture%2C%20the%20server%20aggregates%0Athe%20different%20local%20models%20trained%20with%20the%20data%20of%20each%20data%20owner%20to%20build%20a%0Aglobal%20model.%20This%20point%20is%20critical%20and%20therefore%20it%20is%20fundamental%20to%20analyze%0Adifferent%20ways%20of%20aggregation%20according%20to%20the%20use%20case%2C%20taking%20into%20account%0Athe%20distribution%20of%20the%20clients%2C%20the%20characteristics%20of%20the%20model%2C%20etc.%20In%20this%0Apaper%20we%20propose%20a%20novel%20aggregation%20strategy%20and%20we%20apply%20it%20to%20a%20use%20case%20of%0Acerebral%20magnetic%20resonance%20image%20classification.%20In%20this%20use%20case%20the%0Aaggregation%20function%20proposed%20manages%20to%20improve%20the%20convergence%20obtained%20over%0Athe%20rounds%20of%20the%20federated%20learning%20process%20in%20relation%20to%20different%0Aaggregation%20strategies%20classically%20implemented%20and%20applied.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520the%2520Convergence%2520of%2520Federated%2520Learning%2520Aggregation%2520Strategies%250A%2520%2520with%2520Limited%2520Data%26entry.906535625%3DJudith%2520S%25C3%25A1inz-Pardo%2520D%25C3%25ADaz%2520and%2520%25C3%2581lvaro%2520L%25C3%25B3pez%2520Garc%25C3%25ADa%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520deep%2520learning%2520techniques%2520is%2520a%2520leading%2520field%2520applied%2520to%250Acases%2520in%2520which%2520medical%2520data%2520is%2520used%252C%2520particularly%2520in%2520cases%2520of%2520image%2520diagnosis.%250AThis%2520type%2520of%2520data%2520has%2520privacy%2520and%2520legal%2520restrictions%2520that%2520in%2520many%2520cases%2520prevent%250Ait%2520from%2520being%2520processed%2520from%2520central%2520servers.%2520However%252C%2520in%2520this%2520area%250Acollaboration%2520between%2520different%2520research%2520centers%252C%2520in%2520order%2520to%2520create%2520models%2520as%250Arobust%2520as%2520possible%252C%2520trained%2520with%2520the%2520largest%2520quantity%2520and%2520diversity%2520of%2520data%250Aavailable%252C%2520is%2520a%2520critical%2520point%2520to%2520be%2520taken%2520into%2520account.%2520In%2520this%2520sense%252C%2520the%250Aapplication%2520of%2520privacy%2520aware%2520distributed%2520architectures%252C%2520such%2520as%2520federated%250Alearning%2520arises.%2520When%2520applying%2520this%2520type%2520of%2520architecture%252C%2520the%2520server%2520aggregates%250Athe%2520different%2520local%2520models%2520trained%2520with%2520the%2520data%2520of%2520each%2520data%2520owner%2520to%2520build%2520a%250Aglobal%2520model.%2520This%2520point%2520is%2520critical%2520and%2520therefore%2520it%2520is%2520fundamental%2520to%2520analyze%250Adifferent%2520ways%2520of%2520aggregation%2520according%2520to%2520the%2520use%2520case%252C%2520taking%2520into%2520account%250Athe%2520distribution%2520of%2520the%2520clients%252C%2520the%2520characteristics%2520of%2520the%2520model%252C%2520etc.%2520In%2520this%250Apaper%2520we%2520propose%2520a%2520novel%2520aggregation%2520strategy%2520and%2520we%2520apply%2520it%2520to%2520a%2520use%2520case%2520of%250Acerebral%2520magnetic%2520resonance%2520image%2520classification.%2520In%2520this%2520use%2520case%2520the%250Aaggregation%2520function%2520proposed%2520manages%2520to%2520improve%2520the%2520convergence%2520obtained%2520over%250Athe%2520rounds%2520of%2520the%2520federated%2520learning%2520process%2520in%2520relation%2520to%2520different%250Aaggregation%2520strategies%2520classically%2520implemented%2520and%2520applied.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20the%20Convergence%20of%20Federated%20Learning%20Aggregation%20Strategies%0A%20%20with%20Limited%20Data&entry.906535625=Judith%20S%C3%A1inz-Pardo%20D%C3%ADaz%20and%20%C3%81lvaro%20L%C3%B3pez%20Garc%C3%ADa&entry.1292438233=%20%20The%20development%20of%20deep%20learning%20techniques%20is%20a%20leading%20field%20applied%20to%0Acases%20in%20which%20medical%20data%20is%20used%2C%20particularly%20in%20cases%20of%20image%20diagnosis.%0AThis%20type%20of%20data%20has%20privacy%20and%20legal%20restrictions%20that%20in%20many%20cases%20prevent%0Ait%20from%20being%20processed%20from%20central%20servers.%20However%2C%20in%20this%20area%0Acollaboration%20between%20different%20research%20centers%2C%20in%20order%20to%20create%20models%20as%0Arobust%20as%20possible%2C%20trained%20with%20the%20largest%20quantity%20and%20diversity%20of%20data%0Aavailable%2C%20is%20a%20critical%20point%20to%20be%20taken%20into%20account.%20In%20this%20sense%2C%20the%0Aapplication%20of%20privacy%20aware%20distributed%20architectures%2C%20such%20as%20federated%0Alearning%20arises.%20When%20applying%20this%20type%20of%20architecture%2C%20the%20server%20aggregates%0Athe%20different%20local%20models%20trained%20with%20the%20data%20of%20each%20data%20owner%20to%20build%20a%0Aglobal%20model.%20This%20point%20is%20critical%20and%20therefore%20it%20is%20fundamental%20to%20analyze%0Adifferent%20ways%20of%20aggregation%20according%20to%20the%20use%20case%2C%20taking%20into%20account%0Athe%20distribution%20of%20the%20clients%2C%20the%20characteristics%20of%20the%20model%2C%20etc.%20In%20this%0Apaper%20we%20propose%20a%20novel%20aggregation%20strategy%20and%20we%20apply%20it%20to%20a%20use%20case%20of%0Acerebral%20magnetic%20resonance%20image%20classification.%20In%20this%20use%20case%20the%0Aaggregation%20function%20proposed%20manages%20to%20improve%20the%20convergence%20obtained%20over%0Athe%20rounds%20of%20the%20federated%20learning%20process%20in%20relation%20to%20different%0Aaggregation%20strategies%20classically%20implemented%20and%20applied.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15949v1&entry.124074799=Read"},
{"title": "Beyond the Neural Fog: Interpretable Learning for AC Optimal Power Flow", "author": "Salvador Pineda and Juan P\u00e9rez-Ruiz and Juan Miguel Morales", "abstract": "  The AC optimal power flow (AC-OPF) problem is essential for power system\noperations, but its non-convex nature makes it challenging to solve. A widely\nused simplification is the linearized DC optimal power flow (DC-OPF) problem,\nwhich can be solved to global optimality, but whose optimal solution is always\ninfeasible in the original AC-OPF problem. Recently, neural networks (NN) have\nbeen introduced for solving the AC-OPF problem at significantly faster\ncomputation times. However, these methods necessitate extensive datasets, are\ndifficult to train, and are often viewed as black boxes, leading to resistance\nfrom operators who prefer more transparent and interpretable solutions. In this\npaper, we introduce a novel learning-based approach that merges simplicity and\ninterpretability, providing a bridge between traditional approximation methods\nand black-box learning techniques. Our approach not only provides transparency\nfor operators but also achieves competitive accuracy. Numerical results across\nvarious power networks demonstrate that our method provides accuracy comparable\nto, and often surpassing, that of neural networks, particularly when training\ndatasets are limited.\n", "link": "http://arxiv.org/abs/2408.05228v2", "date": "2025-01-27", "relevancy": 1.9542, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.511}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4969}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Neural%20Fog%3A%20Interpretable%20Learning%20for%20AC%20Optimal%20Power%20Flow&body=Title%3A%20Beyond%20the%20Neural%20Fog%3A%20Interpretable%20Learning%20for%20AC%20Optimal%20Power%20Flow%0AAuthor%3A%20Salvador%20Pineda%20and%20Juan%20P%C3%A9rez-Ruiz%20and%20Juan%20Miguel%20Morales%0AAbstract%3A%20%20%20The%20AC%20optimal%20power%20flow%20%28AC-OPF%29%20problem%20is%20essential%20for%20power%20system%0Aoperations%2C%20but%20its%20non-convex%20nature%20makes%20it%20challenging%20to%20solve.%20A%20widely%0Aused%20simplification%20is%20the%20linearized%20DC%20optimal%20power%20flow%20%28DC-OPF%29%20problem%2C%0Awhich%20can%20be%20solved%20to%20global%20optimality%2C%20but%20whose%20optimal%20solution%20is%20always%0Ainfeasible%20in%20the%20original%20AC-OPF%20problem.%20Recently%2C%20neural%20networks%20%28NN%29%20have%0Abeen%20introduced%20for%20solving%20the%20AC-OPF%20problem%20at%20significantly%20faster%0Acomputation%20times.%20However%2C%20these%20methods%20necessitate%20extensive%20datasets%2C%20are%0Adifficult%20to%20train%2C%20and%20are%20often%20viewed%20as%20black%20boxes%2C%20leading%20to%20resistance%0Afrom%20operators%20who%20prefer%20more%20transparent%20and%20interpretable%20solutions.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20learning-based%20approach%20that%20merges%20simplicity%20and%0Ainterpretability%2C%20providing%20a%20bridge%20between%20traditional%20approximation%20methods%0Aand%20black-box%20learning%20techniques.%20Our%20approach%20not%20only%20provides%20transparency%0Afor%20operators%20but%20also%20achieves%20competitive%20accuracy.%20Numerical%20results%20across%0Avarious%20power%20networks%20demonstrate%20that%20our%20method%20provides%20accuracy%20comparable%0Ato%2C%20and%20often%20surpassing%2C%20that%20of%20neural%20networks%2C%20particularly%20when%20training%0Adatasets%20are%20limited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Neural%2520Fog%253A%2520Interpretable%2520Learning%2520for%2520AC%2520Optimal%2520Power%2520Flow%26entry.906535625%3DSalvador%2520Pineda%2520and%2520Juan%2520P%25C3%25A9rez-Ruiz%2520and%2520Juan%2520Miguel%2520Morales%26entry.1292438233%3D%2520%2520The%2520AC%2520optimal%2520power%2520flow%2520%2528AC-OPF%2529%2520problem%2520is%2520essential%2520for%2520power%2520system%250Aoperations%252C%2520but%2520its%2520non-convex%2520nature%2520makes%2520it%2520challenging%2520to%2520solve.%2520A%2520widely%250Aused%2520simplification%2520is%2520the%2520linearized%2520DC%2520optimal%2520power%2520flow%2520%2528DC-OPF%2529%2520problem%252C%250Awhich%2520can%2520be%2520solved%2520to%2520global%2520optimality%252C%2520but%2520whose%2520optimal%2520solution%2520is%2520always%250Ainfeasible%2520in%2520the%2520original%2520AC-OPF%2520problem.%2520Recently%252C%2520neural%2520networks%2520%2528NN%2529%2520have%250Abeen%2520introduced%2520for%2520solving%2520the%2520AC-OPF%2520problem%2520at%2520significantly%2520faster%250Acomputation%2520times.%2520However%252C%2520these%2520methods%2520necessitate%2520extensive%2520datasets%252C%2520are%250Adifficult%2520to%2520train%252C%2520and%2520are%2520often%2520viewed%2520as%2520black%2520boxes%252C%2520leading%2520to%2520resistance%250Afrom%2520operators%2520who%2520prefer%2520more%2520transparent%2520and%2520interpretable%2520solutions.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520learning-based%2520approach%2520that%2520merges%2520simplicity%2520and%250Ainterpretability%252C%2520providing%2520a%2520bridge%2520between%2520traditional%2520approximation%2520methods%250Aand%2520black-box%2520learning%2520techniques.%2520Our%2520approach%2520not%2520only%2520provides%2520transparency%250Afor%2520operators%2520but%2520also%2520achieves%2520competitive%2520accuracy.%2520Numerical%2520results%2520across%250Avarious%2520power%2520networks%2520demonstrate%2520that%2520our%2520method%2520provides%2520accuracy%2520comparable%250Ato%252C%2520and%2520often%2520surpassing%252C%2520that%2520of%2520neural%2520networks%252C%2520particularly%2520when%2520training%250Adatasets%2520are%2520limited.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Neural%20Fog%3A%20Interpretable%20Learning%20for%20AC%20Optimal%20Power%20Flow&entry.906535625=Salvador%20Pineda%20and%20Juan%20P%C3%A9rez-Ruiz%20and%20Juan%20Miguel%20Morales&entry.1292438233=%20%20The%20AC%20optimal%20power%20flow%20%28AC-OPF%29%20problem%20is%20essential%20for%20power%20system%0Aoperations%2C%20but%20its%20non-convex%20nature%20makes%20it%20challenging%20to%20solve.%20A%20widely%0Aused%20simplification%20is%20the%20linearized%20DC%20optimal%20power%20flow%20%28DC-OPF%29%20problem%2C%0Awhich%20can%20be%20solved%20to%20global%20optimality%2C%20but%20whose%20optimal%20solution%20is%20always%0Ainfeasible%20in%20the%20original%20AC-OPF%20problem.%20Recently%2C%20neural%20networks%20%28NN%29%20have%0Abeen%20introduced%20for%20solving%20the%20AC-OPF%20problem%20at%20significantly%20faster%0Acomputation%20times.%20However%2C%20these%20methods%20necessitate%20extensive%20datasets%2C%20are%0Adifficult%20to%20train%2C%20and%20are%20often%20viewed%20as%20black%20boxes%2C%20leading%20to%20resistance%0Afrom%20operators%20who%20prefer%20more%20transparent%20and%20interpretable%20solutions.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20learning-based%20approach%20that%20merges%20simplicity%20and%0Ainterpretability%2C%20providing%20a%20bridge%20between%20traditional%20approximation%20methods%0Aand%20black-box%20learning%20techniques.%20Our%20approach%20not%20only%20provides%20transparency%0Afor%20operators%20but%20also%20achieves%20competitive%20accuracy.%20Numerical%20results%20across%0Avarious%20power%20networks%20demonstrate%20that%20our%20method%20provides%20accuracy%20comparable%0Ato%2C%20and%20often%20surpassing%2C%20that%20of%20neural%20networks%2C%20particularly%20when%20training%0Adatasets%20are%20limited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05228v2&entry.124074799=Read"},
{"title": "Towards General-Purpose Model-Free Reinforcement Learning", "author": "Scott Fujimoto and Pierluca D'Oro and Amy Zhang and Yuandong Tian and Michael Rabbat", "abstract": "  Reinforcement learning (RL) promises a framework for near-universal\nproblem-solving. In practice however, RL algorithms are often tailored to\nspecific benchmarks, relying on carefully tuned hyperparameters and algorithmic\nchoices. Recently, powerful model-based RL methods have shown impressive\ngeneral results across benchmarks but come at the cost of increased complexity\nand slow run times, limiting their broader applicability. In this paper, we\nattempt to find a unifying model-free deep RL algorithm that can address a\ndiverse class of domains and problem settings. To achieve this, we leverage\nmodel-based representations that approximately linearize the value function,\ntaking advantage of the denser task objectives used by model-based RL while\navoiding the costs associated with planning or simulated trajectories. We\nevaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a\nsingle set of hyperparameters and show a competitive performance against\ndomain-specific and general baselines, providing a concrete step towards\nbuilding general-purpose model-free deep RL algorithms.\n", "link": "http://arxiv.org/abs/2501.16142v1", "date": "2025-01-27", "relevancy": 1.95, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4953}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20General-Purpose%20Model-Free%20Reinforcement%20Learning&body=Title%3A%20Towards%20General-Purpose%20Model-Free%20Reinforcement%20Learning%0AAuthor%3A%20Scott%20Fujimoto%20and%20Pierluca%20D%27Oro%20and%20Amy%20Zhang%20and%20Yuandong%20Tian%20and%20Michael%20Rabbat%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20promises%20a%20framework%20for%20near-universal%0Aproblem-solving.%20In%20practice%20however%2C%20RL%20algorithms%20are%20often%20tailored%20to%0Aspecific%20benchmarks%2C%20relying%20on%20carefully%20tuned%20hyperparameters%20and%20algorithmic%0Achoices.%20Recently%2C%20powerful%20model-based%20RL%20methods%20have%20shown%20impressive%0Ageneral%20results%20across%20benchmarks%20but%20come%20at%20the%20cost%20of%20increased%20complexity%0Aand%20slow%20run%20times%2C%20limiting%20their%20broader%20applicability.%20In%20this%20paper%2C%20we%0Aattempt%20to%20find%20a%20unifying%20model-free%20deep%20RL%20algorithm%20that%20can%20address%20a%0Adiverse%20class%20of%20domains%20and%20problem%20settings.%20To%20achieve%20this%2C%20we%20leverage%0Amodel-based%20representations%20that%20approximately%20linearize%20the%20value%20function%2C%0Ataking%20advantage%20of%20the%20denser%20task%20objectives%20used%20by%20model-based%20RL%20while%0Aavoiding%20the%20costs%20associated%20with%20planning%20or%20simulated%20trajectories.%20We%0Aevaluate%20our%20algorithm%2C%20MR.Q%2C%20on%20a%20variety%20of%20common%20RL%20benchmarks%20with%20a%0Asingle%20set%20of%20hyperparameters%20and%20show%20a%20competitive%20performance%20against%0Adomain-specific%20and%20general%20baselines%2C%20providing%20a%20concrete%20step%20towards%0Abuilding%20general-purpose%20model-free%20deep%20RL%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520General-Purpose%2520Model-Free%2520Reinforcement%2520Learning%26entry.906535625%3DScott%2520Fujimoto%2520and%2520Pierluca%2520D%2527Oro%2520and%2520Amy%2520Zhang%2520and%2520Yuandong%2520Tian%2520and%2520Michael%2520Rabbat%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520promises%2520a%2520framework%2520for%2520near-universal%250Aproblem-solving.%2520In%2520practice%2520however%252C%2520RL%2520algorithms%2520are%2520often%2520tailored%2520to%250Aspecific%2520benchmarks%252C%2520relying%2520on%2520carefully%2520tuned%2520hyperparameters%2520and%2520algorithmic%250Achoices.%2520Recently%252C%2520powerful%2520model-based%2520RL%2520methods%2520have%2520shown%2520impressive%250Ageneral%2520results%2520across%2520benchmarks%2520but%2520come%2520at%2520the%2520cost%2520of%2520increased%2520complexity%250Aand%2520slow%2520run%2520times%252C%2520limiting%2520their%2520broader%2520applicability.%2520In%2520this%2520paper%252C%2520we%250Aattempt%2520to%2520find%2520a%2520unifying%2520model-free%2520deep%2520RL%2520algorithm%2520that%2520can%2520address%2520a%250Adiverse%2520class%2520of%2520domains%2520and%2520problem%2520settings.%2520To%2520achieve%2520this%252C%2520we%2520leverage%250Amodel-based%2520representations%2520that%2520approximately%2520linearize%2520the%2520value%2520function%252C%250Ataking%2520advantage%2520of%2520the%2520denser%2520task%2520objectives%2520used%2520by%2520model-based%2520RL%2520while%250Aavoiding%2520the%2520costs%2520associated%2520with%2520planning%2520or%2520simulated%2520trajectories.%2520We%250Aevaluate%2520our%2520algorithm%252C%2520MR.Q%252C%2520on%2520a%2520variety%2520of%2520common%2520RL%2520benchmarks%2520with%2520a%250Asingle%2520set%2520of%2520hyperparameters%2520and%2520show%2520a%2520competitive%2520performance%2520against%250Adomain-specific%2520and%2520general%2520baselines%252C%2520providing%2520a%2520concrete%2520step%2520towards%250Abuilding%2520general-purpose%2520model-free%2520deep%2520RL%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20General-Purpose%20Model-Free%20Reinforcement%20Learning&entry.906535625=Scott%20Fujimoto%20and%20Pierluca%20D%27Oro%20and%20Amy%20Zhang%20and%20Yuandong%20Tian%20and%20Michael%20Rabbat&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20promises%20a%20framework%20for%20near-universal%0Aproblem-solving.%20In%20practice%20however%2C%20RL%20algorithms%20are%20often%20tailored%20to%0Aspecific%20benchmarks%2C%20relying%20on%20carefully%20tuned%20hyperparameters%20and%20algorithmic%0Achoices.%20Recently%2C%20powerful%20model-based%20RL%20methods%20have%20shown%20impressive%0Ageneral%20results%20across%20benchmarks%20but%20come%20at%20the%20cost%20of%20increased%20complexity%0Aand%20slow%20run%20times%2C%20limiting%20their%20broader%20applicability.%20In%20this%20paper%2C%20we%0Aattempt%20to%20find%20a%20unifying%20model-free%20deep%20RL%20algorithm%20that%20can%20address%20a%0Adiverse%20class%20of%20domains%20and%20problem%20settings.%20To%20achieve%20this%2C%20we%20leverage%0Amodel-based%20representations%20that%20approximately%20linearize%20the%20value%20function%2C%0Ataking%20advantage%20of%20the%20denser%20task%20objectives%20used%20by%20model-based%20RL%20while%0Aavoiding%20the%20costs%20associated%20with%20planning%20or%20simulated%20trajectories.%20We%0Aevaluate%20our%20algorithm%2C%20MR.Q%2C%20on%20a%20variety%20of%20common%20RL%20benchmarks%20with%20a%0Asingle%20set%20of%20hyperparameters%20and%20show%20a%20competitive%20performance%20against%0Adomain-specific%20and%20general%20baselines%2C%20providing%20a%20concrete%20step%20towards%0Abuilding%20general-purpose%20model-free%20deep%20RL%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16142v1&entry.124074799=Read"},
{"title": "Application of Structured State Space Models to High energy physics with\n  locality-sensitive hashing", "author": "Cheng Jiang and Sitian Qian", "abstract": "  Modern high-energy physics (HEP) experiments are increasingly challenged by\nthe vast size and complexity of their datasets, particularly regarding\nlarge-scale point cloud processing and long sequences. In this study, to\naddress these challenges, we explore the application of structured state space\nmodels (SSMs), proposing one of the first trials to integrate local-sensitive\nhashing into either a hybrid or pure Mamba Model. Our results demonstrate that\npure SSMs could serve as powerful backbones for HEP problems involving tasks\nfor long sequence data with local inductive bias. By integrating\nlocality-sensitive hashing into Mamba blocks, we achieve significant\nimprovements over traditional backbones in key HEP tasks, surpassing them in\ninference speed and physics metrics while reducing computational overhead. In\nkey tests, our approach demonstrated promising results, presenting a viable\nalternative to traditional transformer backbones by significantly reducing\nFLOPS while maintaining robust performance.\n", "link": "http://arxiv.org/abs/2501.16237v1", "date": "2025-01-27", "relevancy": 1.9498, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.492}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4898}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20Structured%20State%20Space%20Models%20to%20High%20energy%20physics%20with%0A%20%20locality-sensitive%20hashing&body=Title%3A%20Application%20of%20Structured%20State%20Space%20Models%20to%20High%20energy%20physics%20with%0A%20%20locality-sensitive%20hashing%0AAuthor%3A%20Cheng%20Jiang%20and%20Sitian%20Qian%0AAbstract%3A%20%20%20Modern%20high-energy%20physics%20%28HEP%29%20experiments%20are%20increasingly%20challenged%20by%0Athe%20vast%20size%20and%20complexity%20of%20their%20datasets%2C%20particularly%20regarding%0Alarge-scale%20point%20cloud%20processing%20and%20long%20sequences.%20In%20this%20study%2C%20to%0Aaddress%20these%20challenges%2C%20we%20explore%20the%20application%20of%20structured%20state%20space%0Amodels%20%28SSMs%29%2C%20proposing%20one%20of%20the%20first%20trials%20to%20integrate%20local-sensitive%0Ahashing%20into%20either%20a%20hybrid%20or%20pure%20Mamba%20Model.%20Our%20results%20demonstrate%20that%0Apure%20SSMs%20could%20serve%20as%20powerful%20backbones%20for%20HEP%20problems%20involving%20tasks%0Afor%20long%20sequence%20data%20with%20local%20inductive%20bias.%20By%20integrating%0Alocality-sensitive%20hashing%20into%20Mamba%20blocks%2C%20we%20achieve%20significant%0Aimprovements%20over%20traditional%20backbones%20in%20key%20HEP%20tasks%2C%20surpassing%20them%20in%0Ainference%20speed%20and%20physics%20metrics%20while%20reducing%20computational%20overhead.%20In%0Akey%20tests%2C%20our%20approach%20demonstrated%20promising%20results%2C%20presenting%20a%20viable%0Aalternative%20to%20traditional%20transformer%20backbones%20by%20significantly%20reducing%0AFLOPS%20while%20maintaining%20robust%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520Structured%2520State%2520Space%2520Models%2520to%2520High%2520energy%2520physics%2520with%250A%2520%2520locality-sensitive%2520hashing%26entry.906535625%3DCheng%2520Jiang%2520and%2520Sitian%2520Qian%26entry.1292438233%3D%2520%2520Modern%2520high-energy%2520physics%2520%2528HEP%2529%2520experiments%2520are%2520increasingly%2520challenged%2520by%250Athe%2520vast%2520size%2520and%2520complexity%2520of%2520their%2520datasets%252C%2520particularly%2520regarding%250Alarge-scale%2520point%2520cloud%2520processing%2520and%2520long%2520sequences.%2520In%2520this%2520study%252C%2520to%250Aaddress%2520these%2520challenges%252C%2520we%2520explore%2520the%2520application%2520of%2520structured%2520state%2520space%250Amodels%2520%2528SSMs%2529%252C%2520proposing%2520one%2520of%2520the%2520first%2520trials%2520to%2520integrate%2520local-sensitive%250Ahashing%2520into%2520either%2520a%2520hybrid%2520or%2520pure%2520Mamba%2520Model.%2520Our%2520results%2520demonstrate%2520that%250Apure%2520SSMs%2520could%2520serve%2520as%2520powerful%2520backbones%2520for%2520HEP%2520problems%2520involving%2520tasks%250Afor%2520long%2520sequence%2520data%2520with%2520local%2520inductive%2520bias.%2520By%2520integrating%250Alocality-sensitive%2520hashing%2520into%2520Mamba%2520blocks%252C%2520we%2520achieve%2520significant%250Aimprovements%2520over%2520traditional%2520backbones%2520in%2520key%2520HEP%2520tasks%252C%2520surpassing%2520them%2520in%250Ainference%2520speed%2520and%2520physics%2520metrics%2520while%2520reducing%2520computational%2520overhead.%2520In%250Akey%2520tests%252C%2520our%2520approach%2520demonstrated%2520promising%2520results%252C%2520presenting%2520a%2520viable%250Aalternative%2520to%2520traditional%2520transformer%2520backbones%2520by%2520significantly%2520reducing%250AFLOPS%2520while%2520maintaining%2520robust%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20Structured%20State%20Space%20Models%20to%20High%20energy%20physics%20with%0A%20%20locality-sensitive%20hashing&entry.906535625=Cheng%20Jiang%20and%20Sitian%20Qian&entry.1292438233=%20%20Modern%20high-energy%20physics%20%28HEP%29%20experiments%20are%20increasingly%20challenged%20by%0Athe%20vast%20size%20and%20complexity%20of%20their%20datasets%2C%20particularly%20regarding%0Alarge-scale%20point%20cloud%20processing%20and%20long%20sequences.%20In%20this%20study%2C%20to%0Aaddress%20these%20challenges%2C%20we%20explore%20the%20application%20of%20structured%20state%20space%0Amodels%20%28SSMs%29%2C%20proposing%20one%20of%20the%20first%20trials%20to%20integrate%20local-sensitive%0Ahashing%20into%20either%20a%20hybrid%20or%20pure%20Mamba%20Model.%20Our%20results%20demonstrate%20that%0Apure%20SSMs%20could%20serve%20as%20powerful%20backbones%20for%20HEP%20problems%20involving%20tasks%0Afor%20long%20sequence%20data%20with%20local%20inductive%20bias.%20By%20integrating%0Alocality-sensitive%20hashing%20into%20Mamba%20blocks%2C%20we%20achieve%20significant%0Aimprovements%20over%20traditional%20backbones%20in%20key%20HEP%20tasks%2C%20surpassing%20them%20in%0Ainference%20speed%20and%20physics%20metrics%20while%20reducing%20computational%20overhead.%20In%0Akey%20tests%2C%20our%20approach%20demonstrated%20promising%20results%2C%20presenting%20a%20viable%0Aalternative%20to%20traditional%20transformer%20backbones%20by%20significantly%20reducing%0AFLOPS%20while%20maintaining%20robust%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16237v1&entry.124074799=Read"},
{"title": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy", "author": "Tunazzina Islam and Dan Goldwasser", "abstract": "  The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events.\n", "link": "http://arxiv.org/abs/2404.10259v4", "date": "2025-01-27", "relevancy": 1.9485, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4872}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4872}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Latent%20Arguments%20in%20Social%20Media%20Messaging%20by%20Employing%0A%20%20LLMs-in-the-Loop%20Strategy&body=Title%3A%20Uncovering%20Latent%20Arguments%20in%20Social%20Media%20Messaging%20by%20Employing%0A%20%20LLMs-in-the-Loop%20Strategy%0AAuthor%3A%20Tunazzina%20Islam%20and%20Dan%20Goldwasser%0AAbstract%3A%20%20%20The%20widespread%20use%20of%20social%20media%20has%20led%20to%20a%20surge%20in%20popularity%20for%0Aautomated%20methods%20of%20analyzing%20public%20opinion.%20Supervised%20methods%20are%20adept%20at%0Atext%20categorization%2C%20yet%20the%20dynamic%20nature%20of%20social%20media%20discussions%20poses%20a%0Acontinual%20challenge%20for%20these%20techniques%20due%20to%20the%20constant%20shifting%20of%20the%0Afocus.%20On%20the%20other%20hand%2C%20traditional%20unsupervised%20methods%20for%20extracting%0Athemes%20from%20public%20discourse%2C%20such%20as%20topic%20modeling%2C%20often%20reveal%20overarching%0Apatterns%20that%20might%20not%20capture%20specific%20nuances.%20Consequently%2C%20a%20significant%0Aportion%20of%20research%20into%20social%20media%20discourse%20still%20depends%20on%0Alabor-intensive%20manual%20coding%20techniques%20and%20a%20human-in-the-loop%20approach%2C%0Awhich%20are%20both%20time-consuming%20and%20costly.%20In%20this%20work%2C%20we%20study%20the%20problem%20of%0Adiscovering%20arguments%20associated%20with%20a%20specific%20theme.%20We%20propose%20a%20generic%0ALLMs-in-the-Loop%20strategy%20that%20leverages%20the%20advanced%20capabilities%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20extract%20latent%20arguments%20from%20social%20media%20messaging.%0ATo%20demonstrate%20our%20approach%2C%20we%20apply%20our%20framework%20to%20contentious%20topics.%20We%0Ause%20two%20publicly%20available%20datasets%3A%20%281%29%20the%20climate%20campaigns%20dataset%20of%2014k%0AFacebook%20ads%20with%2025%20themes%20and%20%282%29%20the%20COVID-19%20vaccine%20campaigns%20dataset%20of%0A9k%20Facebook%20ads%20with%2014%20themes.%20Additionally%2C%20we%20design%20a%20downstream%20task%20as%0Astance%20prediction%20by%20leveraging%20talking%20points%20in%20climate%20debates.%20Furthermore%2C%0Awe%20analyze%20demographic%20targeting%20and%20the%20adaptation%20of%20messaging%20based%20on%0Areal-world%20events.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10259v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Latent%2520Arguments%2520in%2520Social%2520Media%2520Messaging%2520by%2520Employing%250A%2520%2520LLMs-in-the-Loop%2520Strategy%26entry.906535625%3DTunazzina%2520Islam%2520and%2520Dan%2520Goldwasser%26entry.1292438233%3D%2520%2520The%2520widespread%2520use%2520of%2520social%2520media%2520has%2520led%2520to%2520a%2520surge%2520in%2520popularity%2520for%250Aautomated%2520methods%2520of%2520analyzing%2520public%2520opinion.%2520Supervised%2520methods%2520are%2520adept%2520at%250Atext%2520categorization%252C%2520yet%2520the%2520dynamic%2520nature%2520of%2520social%2520media%2520discussions%2520poses%2520a%250Acontinual%2520challenge%2520for%2520these%2520techniques%2520due%2520to%2520the%2520constant%2520shifting%2520of%2520the%250Afocus.%2520On%2520the%2520other%2520hand%252C%2520traditional%2520unsupervised%2520methods%2520for%2520extracting%250Athemes%2520from%2520public%2520discourse%252C%2520such%2520as%2520topic%2520modeling%252C%2520often%2520reveal%2520overarching%250Apatterns%2520that%2520might%2520not%2520capture%2520specific%2520nuances.%2520Consequently%252C%2520a%2520significant%250Aportion%2520of%2520research%2520into%2520social%2520media%2520discourse%2520still%2520depends%2520on%250Alabor-intensive%2520manual%2520coding%2520techniques%2520and%2520a%2520human-in-the-loop%2520approach%252C%250Awhich%2520are%2520both%2520time-consuming%2520and%2520costly.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520problem%2520of%250Adiscovering%2520arguments%2520associated%2520with%2520a%2520specific%2520theme.%2520We%2520propose%2520a%2520generic%250ALLMs-in-the-Loop%2520strategy%2520that%2520leverages%2520the%2520advanced%2520capabilities%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520extract%2520latent%2520arguments%2520from%2520social%2520media%2520messaging.%250ATo%2520demonstrate%2520our%2520approach%252C%2520we%2520apply%2520our%2520framework%2520to%2520contentious%2520topics.%2520We%250Ause%2520two%2520publicly%2520available%2520datasets%253A%2520%25281%2529%2520the%2520climate%2520campaigns%2520dataset%2520of%252014k%250AFacebook%2520ads%2520with%252025%2520themes%2520and%2520%25282%2529%2520the%2520COVID-19%2520vaccine%2520campaigns%2520dataset%2520of%250A9k%2520Facebook%2520ads%2520with%252014%2520themes.%2520Additionally%252C%2520we%2520design%2520a%2520downstream%2520task%2520as%250Astance%2520prediction%2520by%2520leveraging%2520talking%2520points%2520in%2520climate%2520debates.%2520Furthermore%252C%250Awe%2520analyze%2520demographic%2520targeting%2520and%2520the%2520adaptation%2520of%2520messaging%2520based%2520on%250Areal-world%2520events.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10259v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Latent%20Arguments%20in%20Social%20Media%20Messaging%20by%20Employing%0A%20%20LLMs-in-the-Loop%20Strategy&entry.906535625=Tunazzina%20Islam%20and%20Dan%20Goldwasser&entry.1292438233=%20%20The%20widespread%20use%20of%20social%20media%20has%20led%20to%20a%20surge%20in%20popularity%20for%0Aautomated%20methods%20of%20analyzing%20public%20opinion.%20Supervised%20methods%20are%20adept%20at%0Atext%20categorization%2C%20yet%20the%20dynamic%20nature%20of%20social%20media%20discussions%20poses%20a%0Acontinual%20challenge%20for%20these%20techniques%20due%20to%20the%20constant%20shifting%20of%20the%0Afocus.%20On%20the%20other%20hand%2C%20traditional%20unsupervised%20methods%20for%20extracting%0Athemes%20from%20public%20discourse%2C%20such%20as%20topic%20modeling%2C%20often%20reveal%20overarching%0Apatterns%20that%20might%20not%20capture%20specific%20nuances.%20Consequently%2C%20a%20significant%0Aportion%20of%20research%20into%20social%20media%20discourse%20still%20depends%20on%0Alabor-intensive%20manual%20coding%20techniques%20and%20a%20human-in-the-loop%20approach%2C%0Awhich%20are%20both%20time-consuming%20and%20costly.%20In%20this%20work%2C%20we%20study%20the%20problem%20of%0Adiscovering%20arguments%20associated%20with%20a%20specific%20theme.%20We%20propose%20a%20generic%0ALLMs-in-the-Loop%20strategy%20that%20leverages%20the%20advanced%20capabilities%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20extract%20latent%20arguments%20from%20social%20media%20messaging.%0ATo%20demonstrate%20our%20approach%2C%20we%20apply%20our%20framework%20to%20contentious%20topics.%20We%0Ause%20two%20publicly%20available%20datasets%3A%20%281%29%20the%20climate%20campaigns%20dataset%20of%2014k%0AFacebook%20ads%20with%2025%20themes%20and%20%282%29%20the%20COVID-19%20vaccine%20campaigns%20dataset%20of%0A9k%20Facebook%20ads%20with%2014%20themes.%20Additionally%2C%20we%20design%20a%20downstream%20task%20as%0Astance%20prediction%20by%20leveraging%20talking%20points%20in%20climate%20debates.%20Furthermore%2C%0Awe%20analyze%20demographic%20targeting%20and%20the%20adaptation%20of%20messaging%20based%20on%0Areal-world%20events.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10259v4&entry.124074799=Read"},
{"title": "Empirical Studies of Parameter Efficient Methods for Large Language\n  Models of Code and Knowledge Transfer to R", "author": "Amirreza Esmaeili and Iman Saberi and Fatemeh H. Fard", "abstract": "  Parameter Efficient Fine-Tuning (PEFT) methods are proposed as an alternative\nfine-tuning approach for Large Language Models (LLM) to minimize high training\ncosts. While prior research demonstrates the effectiveness of PEFT methods in\nknowledge transfer using smaller language models, their application to larger\nLLMs, particularly in low-resource and unseen programming languages such as R,\nremains under-explored. In this work, we evaluate PEFT methods, LoRA,\nCompacter, and IA^3 on LLMs for code summarization and generation, with a\nparticular emphasis on knowledge transfer to R as an unseen under-explored\ntarget language. Our experiments reveal that LoRA consistently outperforms\nCompacter and IA^3 in all settings, while Compacter offers significant resource\nefficiency with minimal performance trade-offs. Additionally, we find that the\nnumber of trainable parameters has a greater influence on the functional\naccuracy of the generated code than PEFT architecture. Our study can direct\nfuture research in developing code intelligent tasks for unseen languages\nincluding R, as well as the choice of PEFT methods for knowledge transfer,\nespecially when balancing the computational cost and performance.\n", "link": "http://arxiv.org/abs/2405.01553v2", "date": "2025-01-27", "relevancy": 1.9467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4926}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20Studies%20of%20Parameter%20Efficient%20Methods%20for%20Large%20Language%0A%20%20Models%20of%20Code%20and%20Knowledge%20Transfer%20to%20R&body=Title%3A%20Empirical%20Studies%20of%20Parameter%20Efficient%20Methods%20for%20Large%20Language%0A%20%20Models%20of%20Code%20and%20Knowledge%20Transfer%20to%20R%0AAuthor%3A%20Amirreza%20Esmaeili%20and%20Iman%20Saberi%20and%20Fatemeh%20H.%20Fard%0AAbstract%3A%20%20%20Parameter%20Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20are%20proposed%20as%20an%20alternative%0Afine-tuning%20approach%20for%20Large%20Language%20Models%20%28LLM%29%20to%20minimize%20high%20training%0Acosts.%20While%20prior%20research%20demonstrates%20the%20effectiveness%20of%20PEFT%20methods%20in%0Aknowledge%20transfer%20using%20smaller%20language%20models%2C%20their%20application%20to%20larger%0ALLMs%2C%20particularly%20in%20low-resource%20and%20unseen%20programming%20languages%20such%20as%20R%2C%0Aremains%20under-explored.%20In%20this%20work%2C%20we%20evaluate%20PEFT%20methods%2C%20LoRA%2C%0ACompacter%2C%20and%20IA%5E3%20on%20LLMs%20for%20code%20summarization%20and%20generation%2C%20with%20a%0Aparticular%20emphasis%20on%20knowledge%20transfer%20to%20R%20as%20an%20unseen%20under-explored%0Atarget%20language.%20Our%20experiments%20reveal%20that%20LoRA%20consistently%20outperforms%0ACompacter%20and%20IA%5E3%20in%20all%20settings%2C%20while%20Compacter%20offers%20significant%20resource%0Aefficiency%20with%20minimal%20performance%20trade-offs.%20Additionally%2C%20we%20find%20that%20the%0Anumber%20of%20trainable%20parameters%20has%20a%20greater%20influence%20on%20the%20functional%0Aaccuracy%20of%20the%20generated%20code%20than%20PEFT%20architecture.%20Our%20study%20can%20direct%0Afuture%20research%20in%20developing%20code%20intelligent%20tasks%20for%20unseen%20languages%0Aincluding%20R%2C%20as%20well%20as%20the%20choice%20of%20PEFT%20methods%20for%20knowledge%20transfer%2C%0Aespecially%20when%20balancing%20the%20computational%20cost%20and%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520Studies%2520of%2520Parameter%2520Efficient%2520Methods%2520for%2520Large%2520Language%250A%2520%2520Models%2520of%2520Code%2520and%2520Knowledge%2520Transfer%2520to%2520R%26entry.906535625%3DAmirreza%2520Esmaeili%2520and%2520Iman%2520Saberi%2520and%2520Fatemeh%2520H.%2520Fard%26entry.1292438233%3D%2520%2520Parameter%2520Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520are%2520proposed%2520as%2520an%2520alternative%250Afine-tuning%2520approach%2520for%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520to%2520minimize%2520high%2520training%250Acosts.%2520While%2520prior%2520research%2520demonstrates%2520the%2520effectiveness%2520of%2520PEFT%2520methods%2520in%250Aknowledge%2520transfer%2520using%2520smaller%2520language%2520models%252C%2520their%2520application%2520to%2520larger%250ALLMs%252C%2520particularly%2520in%2520low-resource%2520and%2520unseen%2520programming%2520languages%2520such%2520as%2520R%252C%250Aremains%2520under-explored.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520PEFT%2520methods%252C%2520LoRA%252C%250ACompacter%252C%2520and%2520IA%255E3%2520on%2520LLMs%2520for%2520code%2520summarization%2520and%2520generation%252C%2520with%2520a%250Aparticular%2520emphasis%2520on%2520knowledge%2520transfer%2520to%2520R%2520as%2520an%2520unseen%2520under-explored%250Atarget%2520language.%2520Our%2520experiments%2520reveal%2520that%2520LoRA%2520consistently%2520outperforms%250ACompacter%2520and%2520IA%255E3%2520in%2520all%2520settings%252C%2520while%2520Compacter%2520offers%2520significant%2520resource%250Aefficiency%2520with%2520minimal%2520performance%2520trade-offs.%2520Additionally%252C%2520we%2520find%2520that%2520the%250Anumber%2520of%2520trainable%2520parameters%2520has%2520a%2520greater%2520influence%2520on%2520the%2520functional%250Aaccuracy%2520of%2520the%2520generated%2520code%2520than%2520PEFT%2520architecture.%2520Our%2520study%2520can%2520direct%250Afuture%2520research%2520in%2520developing%2520code%2520intelligent%2520tasks%2520for%2520unseen%2520languages%250Aincluding%2520R%252C%2520as%2520well%2520as%2520the%2520choice%2520of%2520PEFT%2520methods%2520for%2520knowledge%2520transfer%252C%250Aespecially%2520when%2520balancing%2520the%2520computational%2520cost%2520and%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20Studies%20of%20Parameter%20Efficient%20Methods%20for%20Large%20Language%0A%20%20Models%20of%20Code%20and%20Knowledge%20Transfer%20to%20R&entry.906535625=Amirreza%20Esmaeili%20and%20Iman%20Saberi%20and%20Fatemeh%20H.%20Fard&entry.1292438233=%20%20Parameter%20Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20are%20proposed%20as%20an%20alternative%0Afine-tuning%20approach%20for%20Large%20Language%20Models%20%28LLM%29%20to%20minimize%20high%20training%0Acosts.%20While%20prior%20research%20demonstrates%20the%20effectiveness%20of%20PEFT%20methods%20in%0Aknowledge%20transfer%20using%20smaller%20language%20models%2C%20their%20application%20to%20larger%0ALLMs%2C%20particularly%20in%20low-resource%20and%20unseen%20programming%20languages%20such%20as%20R%2C%0Aremains%20under-explored.%20In%20this%20work%2C%20we%20evaluate%20PEFT%20methods%2C%20LoRA%2C%0ACompacter%2C%20and%20IA%5E3%20on%20LLMs%20for%20code%20summarization%20and%20generation%2C%20with%20a%0Aparticular%20emphasis%20on%20knowledge%20transfer%20to%20R%20as%20an%20unseen%20under-explored%0Atarget%20language.%20Our%20experiments%20reveal%20that%20LoRA%20consistently%20outperforms%0ACompacter%20and%20IA%5E3%20in%20all%20settings%2C%20while%20Compacter%20offers%20significant%20resource%0Aefficiency%20with%20minimal%20performance%20trade-offs.%20Additionally%2C%20we%20find%20that%20the%0Anumber%20of%20trainable%20parameters%20has%20a%20greater%20influence%20on%20the%20functional%0Aaccuracy%20of%20the%20generated%20code%20than%20PEFT%20architecture.%20Our%20study%20can%20direct%0Afuture%20research%20in%20developing%20code%20intelligent%20tasks%20for%20unseen%20languages%0Aincluding%20R%2C%20as%20well%20as%20the%20choice%20of%20PEFT%20methods%20for%20knowledge%20transfer%2C%0Aespecially%20when%20balancing%20the%20computational%20cost%20and%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01553v2&entry.124074799=Read"},
{"title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant", "author": "Armand Nicolicioiu and Eugenia Iofinova and Andrej Jovanovic and Eldar Kurtic and Mahdi Nikdan and Andrei Panferov and Ilia Markov and Nir Shavit and Dan Alistarh", "abstract": "  The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We also perform an ablation study showing\nthat less than 100 emails are generally sufficient to produce a credible Panza\nmodel. We are releasing the full Panza code as well as a new \"David\"\npersonalized email dataset licensed for research use, both available on\nhttps://github.com/IST-DASLab/PanzaMail.\n", "link": "http://arxiv.org/abs/2407.10994v3", "date": "2025-01-27", "relevancy": 1.9436, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4981}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4809}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panza%3A%20Design%20and%20Analysis%20of%20a%20Fully-Local%20Personalized%20Text%20Writing%0A%20%20Assistant&body=Title%3A%20Panza%3A%20Design%20and%20Analysis%20of%20a%20Fully-Local%20Personalized%20Text%20Writing%0A%20%20Assistant%0AAuthor%3A%20Armand%20Nicolicioiu%20and%20Eugenia%20Iofinova%20and%20Andrej%20Jovanovic%20and%20Eldar%20Kurtic%20and%20Mahdi%20Nikdan%20and%20Andrei%20Panferov%20and%20Ilia%20Markov%20and%20Nir%20Shavit%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20The%20availability%20of%20powerful%20open-source%20large%20language%20models%20%28LLMs%29%20opens%0Aexciting%20use%20cases%2C%20such%20as%20automated%20personal%20assistants%20that%20adapt%20to%20the%0Auser%27s%20unique%20data%20and%20demands.%20Two%20key%20requirements%20for%20such%20assistants%20are%0Apersonalization%20-%20in%20the%20sense%20that%20the%20assistant%20should%20reflect%20the%20user%27s%20own%0Awriting%20style%20-%20and%20privacy%20-%20users%20may%20prefer%20to%20always%20store%20their%20personal%0Adata%20locally%2C%20on%20their%20own%20computing%20device.%20In%20this%20application%20paper%2C%20we%0Apresent%20a%20new%20design%20and%20evaluation%20for%20such%20an%20automated%20assistant%2C%20for%20the%0Aspecific%20use%20case%20of%20email%20generation%2C%20which%20we%20call%20Panza.%20Specifically%2C%20Panza%0Acan%20be%20trained%20and%20deployed%20locally%20on%20commodity%20hardware%2C%20and%20is%20personalized%0Ato%20the%20user%27s%20writing%20style.%20Panza%27s%20personalization%20features%20are%20based%20on%20a%0Acombination%20of%20fine-tuning%20using%20a%20variant%20of%20the%20Reverse%20Instructions%0Atechnique%20together%20with%20Retrieval-Augmented%20Generation%20%28RAG%29.%20We%20demonstrate%0Athat%20this%20combination%20allows%20us%20to%20fine-tune%20an%20LLM%20to%20better%20reflect%20a%20user%27s%0Awriting%20style%20using%20limited%20data%2C%20while%20executing%20on%20extremely%20limited%0Aresources%2C%20e.g.%20on%20a%20free%20Google%20Colab%20instance.%20Our%20key%20methodological%0Acontribution%20is%20what%20we%20believe%20to%20be%20the%20first%20detailed%20study%20of%20evaluation%0Ametrics%20for%20this%20personalized%20writing%20task%2C%20and%20of%20how%20different%20choices%20of%0Asystem%20components%20-%20e.g.%20the%20use%20of%20RAG%20and%20of%20different%20fine-tuning%20approaches%0A-%20impact%20the%20system%27s%20performance.%20We%20also%20perform%20an%20ablation%20study%20showing%0Athat%20less%20than%20100%20emails%20are%20generally%20sufficient%20to%20produce%20a%20credible%20Panza%0Amodel.%20We%20are%20releasing%20the%20full%20Panza%20code%20as%20well%20as%20a%20new%20%22David%22%0Apersonalized%20email%20dataset%20licensed%20for%20research%20use%2C%20both%20available%20on%0Ahttps%3A//github.com/IST-DASLab/PanzaMail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10994v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanza%253A%2520Design%2520and%2520Analysis%2520of%2520a%2520Fully-Local%2520Personalized%2520Text%2520Writing%250A%2520%2520Assistant%26entry.906535625%3DArmand%2520Nicolicioiu%2520and%2520Eugenia%2520Iofinova%2520and%2520Andrej%2520Jovanovic%2520and%2520Eldar%2520Kurtic%2520and%2520Mahdi%2520Nikdan%2520and%2520Andrei%2520Panferov%2520and%2520Ilia%2520Markov%2520and%2520Nir%2520Shavit%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520The%2520availability%2520of%2520powerful%2520open-source%2520large%2520language%2520models%2520%2528LLMs%2529%2520opens%250Aexciting%2520use%2520cases%252C%2520such%2520as%2520automated%2520personal%2520assistants%2520that%2520adapt%2520to%2520the%250Auser%2527s%2520unique%2520data%2520and%2520demands.%2520Two%2520key%2520requirements%2520for%2520such%2520assistants%2520are%250Apersonalization%2520-%2520in%2520the%2520sense%2520that%2520the%2520assistant%2520should%2520reflect%2520the%2520user%2527s%2520own%250Awriting%2520style%2520-%2520and%2520privacy%2520-%2520users%2520may%2520prefer%2520to%2520always%2520store%2520their%2520personal%250Adata%2520locally%252C%2520on%2520their%2520own%2520computing%2520device.%2520In%2520this%2520application%2520paper%252C%2520we%250Apresent%2520a%2520new%2520design%2520and%2520evaluation%2520for%2520such%2520an%2520automated%2520assistant%252C%2520for%2520the%250Aspecific%2520use%2520case%2520of%2520email%2520generation%252C%2520which%2520we%2520call%2520Panza.%2520Specifically%252C%2520Panza%250Acan%2520be%2520trained%2520and%2520deployed%2520locally%2520on%2520commodity%2520hardware%252C%2520and%2520is%2520personalized%250Ato%2520the%2520user%2527s%2520writing%2520style.%2520Panza%2527s%2520personalization%2520features%2520are%2520based%2520on%2520a%250Acombination%2520of%2520fine-tuning%2520using%2520a%2520variant%2520of%2520the%2520Reverse%2520Instructions%250Atechnique%2520together%2520with%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529.%2520We%2520demonstrate%250Athat%2520this%2520combination%2520allows%2520us%2520to%2520fine-tune%2520an%2520LLM%2520to%2520better%2520reflect%2520a%2520user%2527s%250Awriting%2520style%2520using%2520limited%2520data%252C%2520while%2520executing%2520on%2520extremely%2520limited%250Aresources%252C%2520e.g.%2520on%2520a%2520free%2520Google%2520Colab%2520instance.%2520Our%2520key%2520methodological%250Acontribution%2520is%2520what%2520we%2520believe%2520to%2520be%2520the%2520first%2520detailed%2520study%2520of%2520evaluation%250Ametrics%2520for%2520this%2520personalized%2520writing%2520task%252C%2520and%2520of%2520how%2520different%2520choices%2520of%250Asystem%2520components%2520-%2520e.g.%2520the%2520use%2520of%2520RAG%2520and%2520of%2520different%2520fine-tuning%2520approaches%250A-%2520impact%2520the%2520system%2527s%2520performance.%2520We%2520also%2520perform%2520an%2520ablation%2520study%2520showing%250Athat%2520less%2520than%2520100%2520emails%2520are%2520generally%2520sufficient%2520to%2520produce%2520a%2520credible%2520Panza%250Amodel.%2520We%2520are%2520releasing%2520the%2520full%2520Panza%2520code%2520as%2520well%2520as%2520a%2520new%2520%2522David%2522%250Apersonalized%2520email%2520dataset%2520licensed%2520for%2520research%2520use%252C%2520both%2520available%2520on%250Ahttps%253A//github.com/IST-DASLab/PanzaMail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10994v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panza%3A%20Design%20and%20Analysis%20of%20a%20Fully-Local%20Personalized%20Text%20Writing%0A%20%20Assistant&entry.906535625=Armand%20Nicolicioiu%20and%20Eugenia%20Iofinova%20and%20Andrej%20Jovanovic%20and%20Eldar%20Kurtic%20and%20Mahdi%20Nikdan%20and%20Andrei%20Panferov%20and%20Ilia%20Markov%20and%20Nir%20Shavit%20and%20Dan%20Alistarh&entry.1292438233=%20%20The%20availability%20of%20powerful%20open-source%20large%20language%20models%20%28LLMs%29%20opens%0Aexciting%20use%20cases%2C%20such%20as%20automated%20personal%20assistants%20that%20adapt%20to%20the%0Auser%27s%20unique%20data%20and%20demands.%20Two%20key%20requirements%20for%20such%20assistants%20are%0Apersonalization%20-%20in%20the%20sense%20that%20the%20assistant%20should%20reflect%20the%20user%27s%20own%0Awriting%20style%20-%20and%20privacy%20-%20users%20may%20prefer%20to%20always%20store%20their%20personal%0Adata%20locally%2C%20on%20their%20own%20computing%20device.%20In%20this%20application%20paper%2C%20we%0Apresent%20a%20new%20design%20and%20evaluation%20for%20such%20an%20automated%20assistant%2C%20for%20the%0Aspecific%20use%20case%20of%20email%20generation%2C%20which%20we%20call%20Panza.%20Specifically%2C%20Panza%0Acan%20be%20trained%20and%20deployed%20locally%20on%20commodity%20hardware%2C%20and%20is%20personalized%0Ato%20the%20user%27s%20writing%20style.%20Panza%27s%20personalization%20features%20are%20based%20on%20a%0Acombination%20of%20fine-tuning%20using%20a%20variant%20of%20the%20Reverse%20Instructions%0Atechnique%20together%20with%20Retrieval-Augmented%20Generation%20%28RAG%29.%20We%20demonstrate%0Athat%20this%20combination%20allows%20us%20to%20fine-tune%20an%20LLM%20to%20better%20reflect%20a%20user%27s%0Awriting%20style%20using%20limited%20data%2C%20while%20executing%20on%20extremely%20limited%0Aresources%2C%20e.g.%20on%20a%20free%20Google%20Colab%20instance.%20Our%20key%20methodological%0Acontribution%20is%20what%20we%20believe%20to%20be%20the%20first%20detailed%20study%20of%20evaluation%0Ametrics%20for%20this%20personalized%20writing%20task%2C%20and%20of%20how%20different%20choices%20of%0Asystem%20components%20-%20e.g.%20the%20use%20of%20RAG%20and%20of%20different%20fine-tuning%20approaches%0A-%20impact%20the%20system%27s%20performance.%20We%20also%20perform%20an%20ablation%20study%20showing%0Athat%20less%20than%20100%20emails%20are%20generally%20sufficient%20to%20produce%20a%20credible%20Panza%0Amodel.%20We%20are%20releasing%20the%20full%20Panza%20code%20as%20well%20as%20a%20new%20%22David%22%0Apersonalized%20email%20dataset%20licensed%20for%20research%20use%2C%20both%20available%20on%0Ahttps%3A//github.com/IST-DASLab/PanzaMail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10994v3&entry.124074799=Read"},
{"title": "Distilling foundation models for robust and efficient models in digital\n  pathology", "author": "Alexandre Filiot and Nicolas Dop and Oussama Tchita and Auriane Riou and Thomas Peeters and Daria Valter and Marin Scalbert and Charlie Saillard and Genevi\u00e8ve Robin and Antoine Olivier", "abstract": "  In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance.\n", "link": "http://arxiv.org/abs/2501.16239v1", "date": "2025-01-27", "relevancy": 1.5908, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5597}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.524}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20foundation%20models%20for%20robust%20and%20efficient%20models%20in%20digital%0A%20%20pathology&body=Title%3A%20Distilling%20foundation%20models%20for%20robust%20and%20efficient%20models%20in%20digital%0A%20%20pathology%0AAuthor%3A%20Alexandre%20Filiot%20and%20Nicolas%20Dop%20and%20Oussama%20Tchita%20and%20Auriane%20Riou%20and%20Thomas%20Peeters%20and%20Daria%20Valter%20and%20Marin%20Scalbert%20and%20Charlie%20Saillard%20and%20Genevi%C3%A8ve%20Robin%20and%20Antoine%20Olivier%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20advent%20of%20foundation%20models%20%28FM%29%20for%20digital%20pathology%0Ahas%20relied%20heavily%20on%20scaling%20the%20pre-training%20datasets%20and%20the%20model%20size%2C%0Ayielding%20large%20and%20powerful%20models.%20While%20it%20resulted%20in%20improving%20the%0Aperformance%20on%20diverse%20downstream%20tasks%2C%20it%20also%20introduced%20increased%0Acomputational%20cost%20and%20inference%20time.%20In%20this%20work%2C%20we%20explore%20the%0Adistillation%20of%20a%20large%20foundation%20model%20into%20a%20smaller%20one%2C%20reducing%20the%0Anumber%20of%20parameters%20by%20several%20orders%20of%20magnitude.%20Leveraging%20distillation%0Atechniques%2C%20our%20distilled%20model%2C%20H0-mini%2C%20achieves%20nearly%20comparable%0Aperformance%20to%20large%20FMs%20at%20a%20significantly%20reduced%20inference%20cost.%20It%20is%0Aevaluated%20on%20several%20public%20benchmarks%2C%20achieving%203rd%20place%20on%20the%20HEST%0Abenchmark%20and%205th%20place%20on%20the%20EVA%20benchmark.%20Additionally%2C%20a%20robustness%0Aanalysis%20conducted%20on%20the%20PLISM%20dataset%20demonstrates%20that%20our%20distilled%20model%0Areaches%20excellent%20robustness%20to%20variations%20in%20staining%20and%20scanning%20conditions%2C%0Asignificantly%20outperforming%20other%20state-of-the%20art%20models.%20This%20opens%20new%0Aperspectives%20to%20design%20lightweight%20and%20robust%20models%20for%20digital%20pathology%2C%0Awithout%20compromising%20on%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520foundation%2520models%2520for%2520robust%2520and%2520efficient%2520models%2520in%2520digital%250A%2520%2520pathology%26entry.906535625%3DAlexandre%2520Filiot%2520and%2520Nicolas%2520Dop%2520and%2520Oussama%2520Tchita%2520and%2520Auriane%2520Riou%2520and%2520Thomas%2520Peeters%2520and%2520Daria%2520Valter%2520and%2520Marin%2520Scalbert%2520and%2520Charlie%2520Saillard%2520and%2520Genevi%25C3%25A8ve%2520Robin%2520and%2520Antoine%2520Olivier%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520advent%2520of%2520foundation%2520models%2520%2528FM%2529%2520for%2520digital%2520pathology%250Ahas%2520relied%2520heavily%2520on%2520scaling%2520the%2520pre-training%2520datasets%2520and%2520the%2520model%2520size%252C%250Ayielding%2520large%2520and%2520powerful%2520models.%2520While%2520it%2520resulted%2520in%2520improving%2520the%250Aperformance%2520on%2520diverse%2520downstream%2520tasks%252C%2520it%2520also%2520introduced%2520increased%250Acomputational%2520cost%2520and%2520inference%2520time.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Adistillation%2520of%2520a%2520large%2520foundation%2520model%2520into%2520a%2520smaller%2520one%252C%2520reducing%2520the%250Anumber%2520of%2520parameters%2520by%2520several%2520orders%2520of%2520magnitude.%2520Leveraging%2520distillation%250Atechniques%252C%2520our%2520distilled%2520model%252C%2520H0-mini%252C%2520achieves%2520nearly%2520comparable%250Aperformance%2520to%2520large%2520FMs%2520at%2520a%2520significantly%2520reduced%2520inference%2520cost.%2520It%2520is%250Aevaluated%2520on%2520several%2520public%2520benchmarks%252C%2520achieving%25203rd%2520place%2520on%2520the%2520HEST%250Abenchmark%2520and%25205th%2520place%2520on%2520the%2520EVA%2520benchmark.%2520Additionally%252C%2520a%2520robustness%250Aanalysis%2520conducted%2520on%2520the%2520PLISM%2520dataset%2520demonstrates%2520that%2520our%2520distilled%2520model%250Areaches%2520excellent%2520robustness%2520to%2520variations%2520in%2520staining%2520and%2520scanning%2520conditions%252C%250Asignificantly%2520outperforming%2520other%2520state-of-the%2520art%2520models.%2520This%2520opens%2520new%250Aperspectives%2520to%2520design%2520lightweight%2520and%2520robust%2520models%2520for%2520digital%2520pathology%252C%250Awithout%2520compromising%2520on%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20foundation%20models%20for%20robust%20and%20efficient%20models%20in%20digital%0A%20%20pathology&entry.906535625=Alexandre%20Filiot%20and%20Nicolas%20Dop%20and%20Oussama%20Tchita%20and%20Auriane%20Riou%20and%20Thomas%20Peeters%20and%20Daria%20Valter%20and%20Marin%20Scalbert%20and%20Charlie%20Saillard%20and%20Genevi%C3%A8ve%20Robin%20and%20Antoine%20Olivier&entry.1292438233=%20%20In%20recent%20years%2C%20the%20advent%20of%20foundation%20models%20%28FM%29%20for%20digital%20pathology%0Ahas%20relied%20heavily%20on%20scaling%20the%20pre-training%20datasets%20and%20the%20model%20size%2C%0Ayielding%20large%20and%20powerful%20models.%20While%20it%20resulted%20in%20improving%20the%0Aperformance%20on%20diverse%20downstream%20tasks%2C%20it%20also%20introduced%20increased%0Acomputational%20cost%20and%20inference%20time.%20In%20this%20work%2C%20we%20explore%20the%0Adistillation%20of%20a%20large%20foundation%20model%20into%20a%20smaller%20one%2C%20reducing%20the%0Anumber%20of%20parameters%20by%20several%20orders%20of%20magnitude.%20Leveraging%20distillation%0Atechniques%2C%20our%20distilled%20model%2C%20H0-mini%2C%20achieves%20nearly%20comparable%0Aperformance%20to%20large%20FMs%20at%20a%20significantly%20reduced%20inference%20cost.%20It%20is%0Aevaluated%20on%20several%20public%20benchmarks%2C%20achieving%203rd%20place%20on%20the%20HEST%0Abenchmark%20and%205th%20place%20on%20the%20EVA%20benchmark.%20Additionally%2C%20a%20robustness%0Aanalysis%20conducted%20on%20the%20PLISM%20dataset%20demonstrates%20that%20our%20distilled%20model%0Areaches%20excellent%20robustness%20to%20variations%20in%20staining%20and%20scanning%20conditions%2C%0Asignificantly%20outperforming%20other%20state-of-the%20art%20models.%20This%20opens%20new%0Aperspectives%20to%20design%20lightweight%20and%20robust%20models%20for%20digital%20pathology%2C%0Awithout%20compromising%20on%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16239v1&entry.124074799=Read"},
{"title": "sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for\n  Automatic Sleep Staging", "author": "Jingyuan Chen and Yuan Yao and Mie Anderson and Natalie Hauglund and Celia Kjaerby and Verena Untiet and Maiken Nedergaard and Jiebo Luo", "abstract": "  Automatic sleep staging based on electroencephalography (EEG) and\nelectromyography (EMG) signals is an important aspect of sleep-related\nresearch. Current sleep staging methods suffer from two major drawbacks. First,\nthere are limited information interactions between modalities in the existing\nmethods. Second, current methods do not develop unified models that can handle\ndifferent sources of input. To address these issues, we propose a novel sleep\nstage scoring model sDREAMER, which emphasizes cross-modality interaction and\nper-channel performance. Specifically, we develop a mixture-of-modality-expert\n(MoME) model with three pathways for EEG, EMG, and mixed signals with partially\nshared weights. We further propose a self-distillation training scheme for\nfurther information interaction across modalities. Our model is trained with\nmulti-channel inputs and can make classifications on either single-channel or\nmulti-channel inputs. Experiments demonstrate that our model outperforms the\nexisting transformer-based sleep scoring methods for multi-channel inference.\nFor single-channel inference, our model also outperforms the transformer-based\nmodels trained with single-channel signals.\n", "link": "http://arxiv.org/abs/2501.16329v1", "date": "2025-01-27", "relevancy": 1.523, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5233}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4949}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20sDREAMER%3A%20Self-distilled%20Mixture-of-Modality-Experts%20Transformer%20for%0A%20%20Automatic%20Sleep%20Staging&body=Title%3A%20sDREAMER%3A%20Self-distilled%20Mixture-of-Modality-Experts%20Transformer%20for%0A%20%20Automatic%20Sleep%20Staging%0AAuthor%3A%20Jingyuan%20Chen%20and%20Yuan%20Yao%20and%20Mie%20Anderson%20and%20Natalie%20Hauglund%20and%20Celia%20Kjaerby%20and%20Verena%20Untiet%20and%20Maiken%20Nedergaard%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%20Automatic%20sleep%20staging%20based%20on%20electroencephalography%20%28EEG%29%20and%0Aelectromyography%20%28EMG%29%20signals%20is%20an%20important%20aspect%20of%20sleep-related%0Aresearch.%20Current%20sleep%20staging%20methods%20suffer%20from%20two%20major%20drawbacks.%20First%2C%0Athere%20are%20limited%20information%20interactions%20between%20modalities%20in%20the%20existing%0Amethods.%20Second%2C%20current%20methods%20do%20not%20develop%20unified%20models%20that%20can%20handle%0Adifferent%20sources%20of%20input.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20sleep%0Astage%20scoring%20model%20sDREAMER%2C%20which%20emphasizes%20cross-modality%20interaction%20and%0Aper-channel%20performance.%20Specifically%2C%20we%20develop%20a%20mixture-of-modality-expert%0A%28MoME%29%20model%20with%20three%20pathways%20for%20EEG%2C%20EMG%2C%20and%20mixed%20signals%20with%20partially%0Ashared%20weights.%20We%20further%20propose%20a%20self-distillation%20training%20scheme%20for%0Afurther%20information%20interaction%20across%20modalities.%20Our%20model%20is%20trained%20with%0Amulti-channel%20inputs%20and%20can%20make%20classifications%20on%20either%20single-channel%20or%0Amulti-channel%20inputs.%20Experiments%20demonstrate%20that%20our%20model%20outperforms%20the%0Aexisting%20transformer-based%20sleep%20scoring%20methods%20for%20multi-channel%20inference.%0AFor%20single-channel%20inference%2C%20our%20model%20also%20outperforms%20the%20transformer-based%0Amodels%20trained%20with%20single-channel%20signals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DsDREAMER%253A%2520Self-distilled%2520Mixture-of-Modality-Experts%2520Transformer%2520for%250A%2520%2520Automatic%2520Sleep%2520Staging%26entry.906535625%3DJingyuan%2520Chen%2520and%2520Yuan%2520Yao%2520and%2520Mie%2520Anderson%2520and%2520Natalie%2520Hauglund%2520and%2520Celia%2520Kjaerby%2520and%2520Verena%2520Untiet%2520and%2520Maiken%2520Nedergaard%2520and%2520Jiebo%2520Luo%26entry.1292438233%3D%2520%2520Automatic%2520sleep%2520staging%2520based%2520on%2520electroencephalography%2520%2528EEG%2529%2520and%250Aelectromyography%2520%2528EMG%2529%2520signals%2520is%2520an%2520important%2520aspect%2520of%2520sleep-related%250Aresearch.%2520Current%2520sleep%2520staging%2520methods%2520suffer%2520from%2520two%2520major%2520drawbacks.%2520First%252C%250Athere%2520are%2520limited%2520information%2520interactions%2520between%2520modalities%2520in%2520the%2520existing%250Amethods.%2520Second%252C%2520current%2520methods%2520do%2520not%2520develop%2520unified%2520models%2520that%2520can%2520handle%250Adifferent%2520sources%2520of%2520input.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520sleep%250Astage%2520scoring%2520model%2520sDREAMER%252C%2520which%2520emphasizes%2520cross-modality%2520interaction%2520and%250Aper-channel%2520performance.%2520Specifically%252C%2520we%2520develop%2520a%2520mixture-of-modality-expert%250A%2528MoME%2529%2520model%2520with%2520three%2520pathways%2520for%2520EEG%252C%2520EMG%252C%2520and%2520mixed%2520signals%2520with%2520partially%250Ashared%2520weights.%2520We%2520further%2520propose%2520a%2520self-distillation%2520training%2520scheme%2520for%250Afurther%2520information%2520interaction%2520across%2520modalities.%2520Our%2520model%2520is%2520trained%2520with%250Amulti-channel%2520inputs%2520and%2520can%2520make%2520classifications%2520on%2520either%2520single-channel%2520or%250Amulti-channel%2520inputs.%2520Experiments%2520demonstrate%2520that%2520our%2520model%2520outperforms%2520the%250Aexisting%2520transformer-based%2520sleep%2520scoring%2520methods%2520for%2520multi-channel%2520inference.%250AFor%2520single-channel%2520inference%252C%2520our%2520model%2520also%2520outperforms%2520the%2520transformer-based%250Amodels%2520trained%2520with%2520single-channel%2520signals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=sDREAMER%3A%20Self-distilled%20Mixture-of-Modality-Experts%20Transformer%20for%0A%20%20Automatic%20Sleep%20Staging&entry.906535625=Jingyuan%20Chen%20and%20Yuan%20Yao%20and%20Mie%20Anderson%20and%20Natalie%20Hauglund%20and%20Celia%20Kjaerby%20and%20Verena%20Untiet%20and%20Maiken%20Nedergaard%20and%20Jiebo%20Luo&entry.1292438233=%20%20Automatic%20sleep%20staging%20based%20on%20electroencephalography%20%28EEG%29%20and%0Aelectromyography%20%28EMG%29%20signals%20is%20an%20important%20aspect%20of%20sleep-related%0Aresearch.%20Current%20sleep%20staging%20methods%20suffer%20from%20two%20major%20drawbacks.%20First%2C%0Athere%20are%20limited%20information%20interactions%20between%20modalities%20in%20the%20existing%0Amethods.%20Second%2C%20current%20methods%20do%20not%20develop%20unified%20models%20that%20can%20handle%0Adifferent%20sources%20of%20input.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20sleep%0Astage%20scoring%20model%20sDREAMER%2C%20which%20emphasizes%20cross-modality%20interaction%20and%0Aper-channel%20performance.%20Specifically%2C%20we%20develop%20a%20mixture-of-modality-expert%0A%28MoME%29%20model%20with%20three%20pathways%20for%20EEG%2C%20EMG%2C%20and%20mixed%20signals%20with%20partially%0Ashared%20weights.%20We%20further%20propose%20a%20self-distillation%20training%20scheme%20for%0Afurther%20information%20interaction%20across%20modalities.%20Our%20model%20is%20trained%20with%0Amulti-channel%20inputs%20and%20can%20make%20classifications%20on%20either%20single-channel%20or%0Amulti-channel%20inputs.%20Experiments%20demonstrate%20that%20our%20model%20outperforms%20the%0Aexisting%20transformer-based%20sleep%20scoring%20methods%20for%20multi-channel%20inference.%0AFor%20single-channel%20inference%2C%20our%20model%20also%20outperforms%20the%20transformer-based%0Amodels%20trained%20with%20single-channel%20signals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16329v1&entry.124074799=Read"},
{"title": "Improving Tropical Cyclone Forecasting With Video Diffusion Models", "author": "Zhibo Ren and Pritthijit Nath and Pancham Shukla", "abstract": "  Tropical cyclone (TC) forecasting is crucial for disaster preparedness and\nmitigation. While recent deep learning approaches have shown promise, existing\nmethods often treat TC evolution as a series of independent frame-to-frame\npredictions, limiting their ability to capture long-term dynamics. We present a\nnovel application of video diffusion models for TC forecasting that explicitly\nmodels temporal dependencies through additional temporal layers. Our approach\nenables the model to generate multiple frames simultaneously, better capturing\ncyclone evolution patterns. We introduce a two-stage training strategy that\nsignificantly improves individual-frame quality and performance in low-data\nregimes. Experimental results show our method outperforms the previous approach\nof Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably,\nwe extend the reliable forecasting horizon from 36 to 50 hours. Through\ncomprehensive evaluation using both traditional metrics and Fr\\'echet Video\nDistance (FVD), we demonstrate that our approach produces more temporally\ncoherent forecasts while maintaining competitive single-frame quality. Code\naccessible at https://github.com/Ren-creater/forecast-video-diffmodels.\n", "link": "http://arxiv.org/abs/2501.16003v1", "date": "2025-01-27", "relevancy": 1.6849, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5774}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5584}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Tropical%20Cyclone%20Forecasting%20With%20Video%20Diffusion%20Models&body=Title%3A%20Improving%20Tropical%20Cyclone%20Forecasting%20With%20Video%20Diffusion%20Models%0AAuthor%3A%20Zhibo%20Ren%20and%20Pritthijit%20Nath%20and%20Pancham%20Shukla%0AAbstract%3A%20%20%20Tropical%20cyclone%20%28TC%29%20forecasting%20is%20crucial%20for%20disaster%20preparedness%20and%0Amitigation.%20While%20recent%20deep%20learning%20approaches%20have%20shown%20promise%2C%20existing%0Amethods%20often%20treat%20TC%20evolution%20as%20a%20series%20of%20independent%20frame-to-frame%0Apredictions%2C%20limiting%20their%20ability%20to%20capture%20long-term%20dynamics.%20We%20present%20a%0Anovel%20application%20of%20video%20diffusion%20models%20for%20TC%20forecasting%20that%20explicitly%0Amodels%20temporal%20dependencies%20through%20additional%20temporal%20layers.%20Our%20approach%0Aenables%20the%20model%20to%20generate%20multiple%20frames%20simultaneously%2C%20better%20capturing%0Acyclone%20evolution%20patterns.%20We%20introduce%20a%20two-stage%20training%20strategy%20that%0Asignificantly%20improves%20individual-frame%20quality%20and%20performance%20in%20low-data%0Aregimes.%20Experimental%20results%20show%20our%20method%20outperforms%20the%20previous%20approach%0Aof%20Nath%20et%20al.%20by%2019.3%25%20in%20MAE%2C%2016.2%25%20in%20PSNR%2C%20and%2036.1%25%20in%20SSIM.%20Most%20notably%2C%0Awe%20extend%20the%20reliable%20forecasting%20horizon%20from%2036%20to%2050%20hours.%20Through%0Acomprehensive%20evaluation%20using%20both%20traditional%20metrics%20and%20Fr%5C%27echet%20Video%0ADistance%20%28FVD%29%2C%20we%20demonstrate%20that%20our%20approach%20produces%20more%20temporally%0Acoherent%20forecasts%20while%20maintaining%20competitive%20single-frame%20quality.%20Code%0Aaccessible%20at%20https%3A//github.com/Ren-creater/forecast-video-diffmodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Tropical%2520Cyclone%2520Forecasting%2520With%2520Video%2520Diffusion%2520Models%26entry.906535625%3DZhibo%2520Ren%2520and%2520Pritthijit%2520Nath%2520and%2520Pancham%2520Shukla%26entry.1292438233%3D%2520%2520Tropical%2520cyclone%2520%2528TC%2529%2520forecasting%2520is%2520crucial%2520for%2520disaster%2520preparedness%2520and%250Amitigation.%2520While%2520recent%2520deep%2520learning%2520approaches%2520have%2520shown%2520promise%252C%2520existing%250Amethods%2520often%2520treat%2520TC%2520evolution%2520as%2520a%2520series%2520of%2520independent%2520frame-to-frame%250Apredictions%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520long-term%2520dynamics.%2520We%2520present%2520a%250Anovel%2520application%2520of%2520video%2520diffusion%2520models%2520for%2520TC%2520forecasting%2520that%2520explicitly%250Amodels%2520temporal%2520dependencies%2520through%2520additional%2520temporal%2520layers.%2520Our%2520approach%250Aenables%2520the%2520model%2520to%2520generate%2520multiple%2520frames%2520simultaneously%252C%2520better%2520capturing%250Acyclone%2520evolution%2520patterns.%2520We%2520introduce%2520a%2520two-stage%2520training%2520strategy%2520that%250Asignificantly%2520improves%2520individual-frame%2520quality%2520and%2520performance%2520in%2520low-data%250Aregimes.%2520Experimental%2520results%2520show%2520our%2520method%2520outperforms%2520the%2520previous%2520approach%250Aof%2520Nath%2520et%2520al.%2520by%252019.3%2525%2520in%2520MAE%252C%252016.2%2525%2520in%2520PSNR%252C%2520and%252036.1%2525%2520in%2520SSIM.%2520Most%2520notably%252C%250Awe%2520extend%2520the%2520reliable%2520forecasting%2520horizon%2520from%252036%2520to%252050%2520hours.%2520Through%250Acomprehensive%2520evaluation%2520using%2520both%2520traditional%2520metrics%2520and%2520Fr%255C%2527echet%2520Video%250ADistance%2520%2528FVD%2529%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520produces%2520more%2520temporally%250Acoherent%2520forecasts%2520while%2520maintaining%2520competitive%2520single-frame%2520quality.%2520Code%250Aaccessible%2520at%2520https%253A//github.com/Ren-creater/forecast-video-diffmodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Tropical%20Cyclone%20Forecasting%20With%20Video%20Diffusion%20Models&entry.906535625=Zhibo%20Ren%20and%20Pritthijit%20Nath%20and%20Pancham%20Shukla&entry.1292438233=%20%20Tropical%20cyclone%20%28TC%29%20forecasting%20is%20crucial%20for%20disaster%20preparedness%20and%0Amitigation.%20While%20recent%20deep%20learning%20approaches%20have%20shown%20promise%2C%20existing%0Amethods%20often%20treat%20TC%20evolution%20as%20a%20series%20of%20independent%20frame-to-frame%0Apredictions%2C%20limiting%20their%20ability%20to%20capture%20long-term%20dynamics.%20We%20present%20a%0Anovel%20application%20of%20video%20diffusion%20models%20for%20TC%20forecasting%20that%20explicitly%0Amodels%20temporal%20dependencies%20through%20additional%20temporal%20layers.%20Our%20approach%0Aenables%20the%20model%20to%20generate%20multiple%20frames%20simultaneously%2C%20better%20capturing%0Acyclone%20evolution%20patterns.%20We%20introduce%20a%20two-stage%20training%20strategy%20that%0Asignificantly%20improves%20individual-frame%20quality%20and%20performance%20in%20low-data%0Aregimes.%20Experimental%20results%20show%20our%20method%20outperforms%20the%20previous%20approach%0Aof%20Nath%20et%20al.%20by%2019.3%25%20in%20MAE%2C%2016.2%25%20in%20PSNR%2C%20and%2036.1%25%20in%20SSIM.%20Most%20notably%2C%0Awe%20extend%20the%20reliable%20forecasting%20horizon%20from%2036%20to%2050%20hours.%20Through%0Acomprehensive%20evaluation%20using%20both%20traditional%20metrics%20and%20Fr%5C%27echet%20Video%0ADistance%20%28FVD%29%2C%20we%20demonstrate%20that%20our%20approach%20produces%20more%20temporally%0Acoherent%20forecasts%20while%20maintaining%20competitive%20single-frame%20quality.%20Code%0Aaccessible%20at%20https%3A//github.com/Ren-creater/forecast-video-diffmodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16003v1&entry.124074799=Read"},
{"title": "Integrating Probabilistic Trees and Causal Networks for Clinical and\n  Epidemiological Data", "author": "Sheresh Zahoor and Pietro Li\u00f2 and Ga\u00ebl Dias and Mohammed Hasanuzzaman", "abstract": "  Healthcare decision-making requires not only accurate predictions but also\ninsights into how factors influence patient outcomes. While traditional Machine\nLearning (ML) models excel at predicting outcomes, such as identifying high\nrisk patients, they are limited in addressing what-if questions about\ninterventions. This study introduces the Probabilistic Causal Fusion (PCF)\nframework, which integrates Causal Bayesian Networks (CBNs) and Probability\nTrees (PTrees) to extend beyond predictions. PCF leverages causal relationships\nfrom CBNs to structure PTrees, enabling both the quantification of factor\nimpacts and simulation of hypothetical interventions. PCF was validated on\nthree real-world healthcare datasets i.e. MIMIC-IV, Framingham Heart Study, and\nDiabetes, chosen for their clinically diverse variables. It demonstrated\npredictive performance comparable to traditional ML models while providing\nadditional causal reasoning capabilities. To enhance interpretability, PCF\nincorporates sensitivity analysis and SHapley Additive exPlanations (SHAP).\nSensitivity analysis quantifies the influence of causal parameters on outcomes\nsuch as Length of Stay (LOS), Coronary Heart Disease (CHD), and Diabetes, while\nSHAP highlights the importance of individual features in predictive modeling.\nBy combining causal reasoning with predictive modeling, PCF bridges the gap\nbetween clinical intuition and data-driven insights. Its ability to uncover\nrelationships between modifiable factors and simulate hypothetical scenarios\nprovides clinicians with a clearer understanding of causal pathways. This\napproach supports more informed, evidence-based decision-making, offering a\nrobust framework for addressing complex questions in diverse healthcare\nsettings.\n", "link": "http://arxiv.org/abs/2501.15973v1", "date": "2025-01-27", "relevancy": 1.3619, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4437}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Probabilistic%20Trees%20and%20Causal%20Networks%20for%20Clinical%20and%0A%20%20Epidemiological%20Data&body=Title%3A%20Integrating%20Probabilistic%20Trees%20and%20Causal%20Networks%20for%20Clinical%20and%0A%20%20Epidemiological%20Data%0AAuthor%3A%20Sheresh%20Zahoor%20and%20Pietro%20Li%C3%B2%20and%20Ga%C3%ABl%20Dias%20and%20Mohammed%20Hasanuzzaman%0AAbstract%3A%20%20%20Healthcare%20decision-making%20requires%20not%20only%20accurate%20predictions%20but%20also%0Ainsights%20into%20how%20factors%20influence%20patient%20outcomes.%20While%20traditional%20Machine%0ALearning%20%28ML%29%20models%20excel%20at%20predicting%20outcomes%2C%20such%20as%20identifying%20high%0Arisk%20patients%2C%20they%20are%20limited%20in%20addressing%20what-if%20questions%20about%0Ainterventions.%20This%20study%20introduces%20the%20Probabilistic%20Causal%20Fusion%20%28PCF%29%0Aframework%2C%20which%20integrates%20Causal%20Bayesian%20Networks%20%28CBNs%29%20and%20Probability%0ATrees%20%28PTrees%29%20to%20extend%20beyond%20predictions.%20PCF%20leverages%20causal%20relationships%0Afrom%20CBNs%20to%20structure%20PTrees%2C%20enabling%20both%20the%20quantification%20of%20factor%0Aimpacts%20and%20simulation%20of%20hypothetical%20interventions.%20PCF%20was%20validated%20on%0Athree%20real-world%20healthcare%20datasets%20i.e.%20MIMIC-IV%2C%20Framingham%20Heart%20Study%2C%20and%0ADiabetes%2C%20chosen%20for%20their%20clinically%20diverse%20variables.%20It%20demonstrated%0Apredictive%20performance%20comparable%20to%20traditional%20ML%20models%20while%20providing%0Aadditional%20causal%20reasoning%20capabilities.%20To%20enhance%20interpretability%2C%20PCF%0Aincorporates%20sensitivity%20analysis%20and%20SHapley%20Additive%20exPlanations%20%28SHAP%29.%0ASensitivity%20analysis%20quantifies%20the%20influence%20of%20causal%20parameters%20on%20outcomes%0Asuch%20as%20Length%20of%20Stay%20%28LOS%29%2C%20Coronary%20Heart%20Disease%20%28CHD%29%2C%20and%20Diabetes%2C%20while%0ASHAP%20highlights%20the%20importance%20of%20individual%20features%20in%20predictive%20modeling.%0ABy%20combining%20causal%20reasoning%20with%20predictive%20modeling%2C%20PCF%20bridges%20the%20gap%0Abetween%20clinical%20intuition%20and%20data-driven%20insights.%20Its%20ability%20to%20uncover%0Arelationships%20between%20modifiable%20factors%20and%20simulate%20hypothetical%20scenarios%0Aprovides%20clinicians%20with%20a%20clearer%20understanding%20of%20causal%20pathways.%20This%0Aapproach%20supports%20more%20informed%2C%20evidence-based%20decision-making%2C%20offering%20a%0Arobust%20framework%20for%20addressing%20complex%20questions%20in%20diverse%20healthcare%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Probabilistic%2520Trees%2520and%2520Causal%2520Networks%2520for%2520Clinical%2520and%250A%2520%2520Epidemiological%2520Data%26entry.906535625%3DSheresh%2520Zahoor%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Ga%25C3%25ABl%2520Dias%2520and%2520Mohammed%2520Hasanuzzaman%26entry.1292438233%3D%2520%2520Healthcare%2520decision-making%2520requires%2520not%2520only%2520accurate%2520predictions%2520but%2520also%250Ainsights%2520into%2520how%2520factors%2520influence%2520patient%2520outcomes.%2520While%2520traditional%2520Machine%250ALearning%2520%2528ML%2529%2520models%2520excel%2520at%2520predicting%2520outcomes%252C%2520such%2520as%2520identifying%2520high%250Arisk%2520patients%252C%2520they%2520are%2520limited%2520in%2520addressing%2520what-if%2520questions%2520about%250Ainterventions.%2520This%2520study%2520introduces%2520the%2520Probabilistic%2520Causal%2520Fusion%2520%2528PCF%2529%250Aframework%252C%2520which%2520integrates%2520Causal%2520Bayesian%2520Networks%2520%2528CBNs%2529%2520and%2520Probability%250ATrees%2520%2528PTrees%2529%2520to%2520extend%2520beyond%2520predictions.%2520PCF%2520leverages%2520causal%2520relationships%250Afrom%2520CBNs%2520to%2520structure%2520PTrees%252C%2520enabling%2520both%2520the%2520quantification%2520of%2520factor%250Aimpacts%2520and%2520simulation%2520of%2520hypothetical%2520interventions.%2520PCF%2520was%2520validated%2520on%250Athree%2520real-world%2520healthcare%2520datasets%2520i.e.%2520MIMIC-IV%252C%2520Framingham%2520Heart%2520Study%252C%2520and%250ADiabetes%252C%2520chosen%2520for%2520their%2520clinically%2520diverse%2520variables.%2520It%2520demonstrated%250Apredictive%2520performance%2520comparable%2520to%2520traditional%2520ML%2520models%2520while%2520providing%250Aadditional%2520causal%2520reasoning%2520capabilities.%2520To%2520enhance%2520interpretability%252C%2520PCF%250Aincorporates%2520sensitivity%2520analysis%2520and%2520SHapley%2520Additive%2520exPlanations%2520%2528SHAP%2529.%250ASensitivity%2520analysis%2520quantifies%2520the%2520influence%2520of%2520causal%2520parameters%2520on%2520outcomes%250Asuch%2520as%2520Length%2520of%2520Stay%2520%2528LOS%2529%252C%2520Coronary%2520Heart%2520Disease%2520%2528CHD%2529%252C%2520and%2520Diabetes%252C%2520while%250ASHAP%2520highlights%2520the%2520importance%2520of%2520individual%2520features%2520in%2520predictive%2520modeling.%250ABy%2520combining%2520causal%2520reasoning%2520with%2520predictive%2520modeling%252C%2520PCF%2520bridges%2520the%2520gap%250Abetween%2520clinical%2520intuition%2520and%2520data-driven%2520insights.%2520Its%2520ability%2520to%2520uncover%250Arelationships%2520between%2520modifiable%2520factors%2520and%2520simulate%2520hypothetical%2520scenarios%250Aprovides%2520clinicians%2520with%2520a%2520clearer%2520understanding%2520of%2520causal%2520pathways.%2520This%250Aapproach%2520supports%2520more%2520informed%252C%2520evidence-based%2520decision-making%252C%2520offering%2520a%250Arobust%2520framework%2520for%2520addressing%2520complex%2520questions%2520in%2520diverse%2520healthcare%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Probabilistic%20Trees%20and%20Causal%20Networks%20for%20Clinical%20and%0A%20%20Epidemiological%20Data&entry.906535625=Sheresh%20Zahoor%20and%20Pietro%20Li%C3%B2%20and%20Ga%C3%ABl%20Dias%20and%20Mohammed%20Hasanuzzaman&entry.1292438233=%20%20Healthcare%20decision-making%20requires%20not%20only%20accurate%20predictions%20but%20also%0Ainsights%20into%20how%20factors%20influence%20patient%20outcomes.%20While%20traditional%20Machine%0ALearning%20%28ML%29%20models%20excel%20at%20predicting%20outcomes%2C%20such%20as%20identifying%20high%0Arisk%20patients%2C%20they%20are%20limited%20in%20addressing%20what-if%20questions%20about%0Ainterventions.%20This%20study%20introduces%20the%20Probabilistic%20Causal%20Fusion%20%28PCF%29%0Aframework%2C%20which%20integrates%20Causal%20Bayesian%20Networks%20%28CBNs%29%20and%20Probability%0ATrees%20%28PTrees%29%20to%20extend%20beyond%20predictions.%20PCF%20leverages%20causal%20relationships%0Afrom%20CBNs%20to%20structure%20PTrees%2C%20enabling%20both%20the%20quantification%20of%20factor%0Aimpacts%20and%20simulation%20of%20hypothetical%20interventions.%20PCF%20was%20validated%20on%0Athree%20real-world%20healthcare%20datasets%20i.e.%20MIMIC-IV%2C%20Framingham%20Heart%20Study%2C%20and%0ADiabetes%2C%20chosen%20for%20their%20clinically%20diverse%20variables.%20It%20demonstrated%0Apredictive%20performance%20comparable%20to%20traditional%20ML%20models%20while%20providing%0Aadditional%20causal%20reasoning%20capabilities.%20To%20enhance%20interpretability%2C%20PCF%0Aincorporates%20sensitivity%20analysis%20and%20SHapley%20Additive%20exPlanations%20%28SHAP%29.%0ASensitivity%20analysis%20quantifies%20the%20influence%20of%20causal%20parameters%20on%20outcomes%0Asuch%20as%20Length%20of%20Stay%20%28LOS%29%2C%20Coronary%20Heart%20Disease%20%28CHD%29%2C%20and%20Diabetes%2C%20while%0ASHAP%20highlights%20the%20importance%20of%20individual%20features%20in%20predictive%20modeling.%0ABy%20combining%20causal%20reasoning%20with%20predictive%20modeling%2C%20PCF%20bridges%20the%20gap%0Abetween%20clinical%20intuition%20and%20data-driven%20insights.%20Its%20ability%20to%20uncover%0Arelationships%20between%20modifiable%20factors%20and%20simulate%20hypothetical%20scenarios%0Aprovides%20clinicians%20with%20a%20clearer%20understanding%20of%20causal%20pathways.%20This%0Aapproach%20supports%20more%20informed%2C%20evidence-based%20decision-making%2C%20offering%20a%0Arobust%20framework%20for%20addressing%20complex%20questions%20in%20diverse%20healthcare%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15973v1&entry.124074799=Read"},
{"title": "Multi-View Attention Syntactic Enhanced Graph Convolutional Network for\n  Aspect-based Sentiment Analysis", "author": "Xiang Huang and Hao Peng and Shuo Sun and Zhifeng Hao and Hui Lin and Shuhai Wang", "abstract": "  Aspect-based Sentiment Analysis (ABSA) is the task aimed at predicting the\nsentiment polarity of aspect words within sentences. Recently, incorporating\ngraph neural networks (GNNs) to capture additional syntactic structure\ninformation in the dependency tree derived from syntactic dependency parsing\nhas been proven to be an effective paradigm for boosting ABSA. Despite GNNs\nenhancing model capability by fusing more types of information, most works only\nutilize a single topology view of the dependency tree or simply conflate\ndifferent perspectives of information without distinction, which limits the\nmodel performance. To address these challenges, in this paper, we propose a new\nmulti-view attention syntactic enhanced graph convolutional network (MASGCN)\nthat weighs different syntactic information of views using attention\nmechanisms. Specifically, we first construct distance mask matrices from the\ndependency tree to obtain multiple subgraph views for GNNs. To aggregate\nfeatures from different views, we propose a multi-view attention mechanism to\ncalculate the attention weights of views. Furthermore, to incorporate more\nsyntactic information, we fuse the dependency type information matrix into the\nadjacency matrices and present a structural entropy loss to learn the\ndependency type adjacency matrix. Comprehensive experiments on four benchmark\ndatasets demonstrate that our model outperforms state-of-the-art methods. The\ncodes and datasets are available at https://github.com/SELGroup/MASGCN.\n", "link": "http://arxiv.org/abs/2501.15968v1", "date": "2025-01-27", "relevancy": 1.9286, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4999}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4838}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Attention%20Syntactic%20Enhanced%20Graph%20Convolutional%20Network%20for%0A%20%20Aspect-based%20Sentiment%20Analysis&body=Title%3A%20Multi-View%20Attention%20Syntactic%20Enhanced%20Graph%20Convolutional%20Network%20for%0A%20%20Aspect-based%20Sentiment%20Analysis%0AAuthor%3A%20Xiang%20Huang%20and%20Hao%20Peng%20and%20Shuo%20Sun%20and%20Zhifeng%20Hao%20and%20Hui%20Lin%20and%20Shuhai%20Wang%0AAbstract%3A%20%20%20Aspect-based%20Sentiment%20Analysis%20%28ABSA%29%20is%20the%20task%20aimed%20at%20predicting%20the%0Asentiment%20polarity%20of%20aspect%20words%20within%20sentences.%20Recently%2C%20incorporating%0Agraph%20neural%20networks%20%28GNNs%29%20to%20capture%20additional%20syntactic%20structure%0Ainformation%20in%20the%20dependency%20tree%20derived%20from%20syntactic%20dependency%20parsing%0Ahas%20been%20proven%20to%20be%20an%20effective%20paradigm%20for%20boosting%20ABSA.%20Despite%20GNNs%0Aenhancing%20model%20capability%20by%20fusing%20more%20types%20of%20information%2C%20most%20works%20only%0Autilize%20a%20single%20topology%20view%20of%20the%20dependency%20tree%20or%20simply%20conflate%0Adifferent%20perspectives%20of%20information%20without%20distinction%2C%20which%20limits%20the%0Amodel%20performance.%20To%20address%20these%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20a%20new%0Amulti-view%20attention%20syntactic%20enhanced%20graph%20convolutional%20network%20%28MASGCN%29%0Athat%20weighs%20different%20syntactic%20information%20of%20views%20using%20attention%0Amechanisms.%20Specifically%2C%20we%20first%20construct%20distance%20mask%20matrices%20from%20the%0Adependency%20tree%20to%20obtain%20multiple%20subgraph%20views%20for%20GNNs.%20To%20aggregate%0Afeatures%20from%20different%20views%2C%20we%20propose%20a%20multi-view%20attention%20mechanism%20to%0Acalculate%20the%20attention%20weights%20of%20views.%20Furthermore%2C%20to%20incorporate%20more%0Asyntactic%20information%2C%20we%20fuse%20the%20dependency%20type%20information%20matrix%20into%20the%0Aadjacency%20matrices%20and%20present%20a%20structural%20entropy%20loss%20to%20learn%20the%0Adependency%20type%20adjacency%20matrix.%20Comprehensive%20experiments%20on%20four%20benchmark%0Adatasets%20demonstrate%20that%20our%20model%20outperforms%20state-of-the-art%20methods.%20The%0Acodes%20and%20datasets%20are%20available%20at%20https%3A//github.com/SELGroup/MASGCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Attention%2520Syntactic%2520Enhanced%2520Graph%2520Convolutional%2520Network%2520for%250A%2520%2520Aspect-based%2520Sentiment%2520Analysis%26entry.906535625%3DXiang%2520Huang%2520and%2520Hao%2520Peng%2520and%2520Shuo%2520Sun%2520and%2520Zhifeng%2520Hao%2520and%2520Hui%2520Lin%2520and%2520Shuhai%2520Wang%26entry.1292438233%3D%2520%2520Aspect-based%2520Sentiment%2520Analysis%2520%2528ABSA%2529%2520is%2520the%2520task%2520aimed%2520at%2520predicting%2520the%250Asentiment%2520polarity%2520of%2520aspect%2520words%2520within%2520sentences.%2520Recently%252C%2520incorporating%250Agraph%2520neural%2520networks%2520%2528GNNs%2529%2520to%2520capture%2520additional%2520syntactic%2520structure%250Ainformation%2520in%2520the%2520dependency%2520tree%2520derived%2520from%2520syntactic%2520dependency%2520parsing%250Ahas%2520been%2520proven%2520to%2520be%2520an%2520effective%2520paradigm%2520for%2520boosting%2520ABSA.%2520Despite%2520GNNs%250Aenhancing%2520model%2520capability%2520by%2520fusing%2520more%2520types%2520of%2520information%252C%2520most%2520works%2520only%250Autilize%2520a%2520single%2520topology%2520view%2520of%2520the%2520dependency%2520tree%2520or%2520simply%2520conflate%250Adifferent%2520perspectives%2520of%2520information%2520without%2520distinction%252C%2520which%2520limits%2520the%250Amodel%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%250Amulti-view%2520attention%2520syntactic%2520enhanced%2520graph%2520convolutional%2520network%2520%2528MASGCN%2529%250Athat%2520weighs%2520different%2520syntactic%2520information%2520of%2520views%2520using%2520attention%250Amechanisms.%2520Specifically%252C%2520we%2520first%2520construct%2520distance%2520mask%2520matrices%2520from%2520the%250Adependency%2520tree%2520to%2520obtain%2520multiple%2520subgraph%2520views%2520for%2520GNNs.%2520To%2520aggregate%250Afeatures%2520from%2520different%2520views%252C%2520we%2520propose%2520a%2520multi-view%2520attention%2520mechanism%2520to%250Acalculate%2520the%2520attention%2520weights%2520of%2520views.%2520Furthermore%252C%2520to%2520incorporate%2520more%250Asyntactic%2520information%252C%2520we%2520fuse%2520the%2520dependency%2520type%2520information%2520matrix%2520into%2520the%250Aadjacency%2520matrices%2520and%2520present%2520a%2520structural%2520entropy%2520loss%2520to%2520learn%2520the%250Adependency%2520type%2520adjacency%2520matrix.%2520Comprehensive%2520experiments%2520on%2520four%2520benchmark%250Adatasets%2520demonstrate%2520that%2520our%2520model%2520outperforms%2520state-of-the-art%2520methods.%2520The%250Acodes%2520and%2520datasets%2520are%2520available%2520at%2520https%253A//github.com/SELGroup/MASGCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Attention%20Syntactic%20Enhanced%20Graph%20Convolutional%20Network%20for%0A%20%20Aspect-based%20Sentiment%20Analysis&entry.906535625=Xiang%20Huang%20and%20Hao%20Peng%20and%20Shuo%20Sun%20and%20Zhifeng%20Hao%20and%20Hui%20Lin%20and%20Shuhai%20Wang&entry.1292438233=%20%20Aspect-based%20Sentiment%20Analysis%20%28ABSA%29%20is%20the%20task%20aimed%20at%20predicting%20the%0Asentiment%20polarity%20of%20aspect%20words%20within%20sentences.%20Recently%2C%20incorporating%0Agraph%20neural%20networks%20%28GNNs%29%20to%20capture%20additional%20syntactic%20structure%0Ainformation%20in%20the%20dependency%20tree%20derived%20from%20syntactic%20dependency%20parsing%0Ahas%20been%20proven%20to%20be%20an%20effective%20paradigm%20for%20boosting%20ABSA.%20Despite%20GNNs%0Aenhancing%20model%20capability%20by%20fusing%20more%20types%20of%20information%2C%20most%20works%20only%0Autilize%20a%20single%20topology%20view%20of%20the%20dependency%20tree%20or%20simply%20conflate%0Adifferent%20perspectives%20of%20information%20without%20distinction%2C%20which%20limits%20the%0Amodel%20performance.%20To%20address%20these%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20a%20new%0Amulti-view%20attention%20syntactic%20enhanced%20graph%20convolutional%20network%20%28MASGCN%29%0Athat%20weighs%20different%20syntactic%20information%20of%20views%20using%20attention%0Amechanisms.%20Specifically%2C%20we%20first%20construct%20distance%20mask%20matrices%20from%20the%0Adependency%20tree%20to%20obtain%20multiple%20subgraph%20views%20for%20GNNs.%20To%20aggregate%0Afeatures%20from%20different%20views%2C%20we%20propose%20a%20multi-view%20attention%20mechanism%20to%0Acalculate%20the%20attention%20weights%20of%20views.%20Furthermore%2C%20to%20incorporate%20more%0Asyntactic%20information%2C%20we%20fuse%20the%20dependency%20type%20information%20matrix%20into%20the%0Aadjacency%20matrices%20and%20present%20a%20structural%20entropy%20loss%20to%20learn%20the%0Adependency%20type%20adjacency%20matrix.%20Comprehensive%20experiments%20on%20four%20benchmark%0Adatasets%20demonstrate%20that%20our%20model%20outperforms%20state-of-the-art%20methods.%20The%0Acodes%20and%20datasets%20are%20available%20at%20https%3A//github.com/SELGroup/MASGCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15968v1&entry.124074799=Read"},
{"title": "TimeHF: Billion-Scale Time Series Models Guided by Human Feedback", "author": "Yongzhi Qi and Hao Hu and Dazhou Lei and Jianshen Zhang and Zhengxin Shi and Yulin Huang and Zhengyu Chen and Xiaoming Lin and Zuo-Jun Max Shen", "abstract": "  Time series neural networks perform exceptionally well in real-world\napplications but encounter challenges such as limited scalability, poor\ngeneralization, and suboptimal zero-shot performance. Inspired by large\nlanguage models, there is interest in developing large time series models (LTM)\nto address these issues. However, current methods struggle with training\ncomplexity, adapting human feedback, and achieving high predictive accuracy. We\nintroduce TimeHF, a novel pipeline for creating LTMs with 6 billion parameters,\nincorporating human feedback. We use patch convolutional embedding to capture\nlong time series information and design a human feedback mechanism called\ntime-series policy optimization. Deployed in JD.com's supply chain, TimeHF\nhandles automated replenishment for over 20,000 products, improving prediction\naccuracy by 33.21% over existing methods. This work advances LTM technology and\nshows significant industrial benefits.\n", "link": "http://arxiv.org/abs/2501.15942v1", "date": "2025-01-27", "relevancy": 1.4959, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5168}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4941}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeHF%3A%20Billion-Scale%20Time%20Series%20Models%20Guided%20by%20Human%20Feedback&body=Title%3A%20TimeHF%3A%20Billion-Scale%20Time%20Series%20Models%20Guided%20by%20Human%20Feedback%0AAuthor%3A%20Yongzhi%20Qi%20and%20Hao%20Hu%20and%20Dazhou%20Lei%20and%20Jianshen%20Zhang%20and%20Zhengxin%20Shi%20and%20Yulin%20Huang%20and%20Zhengyu%20Chen%20and%20Xiaoming%20Lin%20and%20Zuo-Jun%20Max%20Shen%0AAbstract%3A%20%20%20Time%20series%20neural%20networks%20perform%20exceptionally%20well%20in%20real-world%0Aapplications%20but%20encounter%20challenges%20such%20as%20limited%20scalability%2C%20poor%0Ageneralization%2C%20and%20suboptimal%20zero-shot%20performance.%20Inspired%20by%20large%0Alanguage%20models%2C%20there%20is%20interest%20in%20developing%20large%20time%20series%20models%20%28LTM%29%0Ato%20address%20these%20issues.%20However%2C%20current%20methods%20struggle%20with%20training%0Acomplexity%2C%20adapting%20human%20feedback%2C%20and%20achieving%20high%20predictive%20accuracy.%20We%0Aintroduce%20TimeHF%2C%20a%20novel%20pipeline%20for%20creating%20LTMs%20with%206%20billion%20parameters%2C%0Aincorporating%20human%20feedback.%20We%20use%20patch%20convolutional%20embedding%20to%20capture%0Along%20time%20series%20information%20and%20design%20a%20human%20feedback%20mechanism%20called%0Atime-series%20policy%20optimization.%20Deployed%20in%20JD.com%27s%20supply%20chain%2C%20TimeHF%0Ahandles%20automated%20replenishment%20for%20over%2020%2C000%20products%2C%20improving%20prediction%0Aaccuracy%20by%2033.21%25%20over%20existing%20methods.%20This%20work%20advances%20LTM%20technology%20and%0Ashows%20significant%20industrial%20benefits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeHF%253A%2520Billion-Scale%2520Time%2520Series%2520Models%2520Guided%2520by%2520Human%2520Feedback%26entry.906535625%3DYongzhi%2520Qi%2520and%2520Hao%2520Hu%2520and%2520Dazhou%2520Lei%2520and%2520Jianshen%2520Zhang%2520and%2520Zhengxin%2520Shi%2520and%2520Yulin%2520Huang%2520and%2520Zhengyu%2520Chen%2520and%2520Xiaoming%2520Lin%2520and%2520Zuo-Jun%2520Max%2520Shen%26entry.1292438233%3D%2520%2520Time%2520series%2520neural%2520networks%2520perform%2520exceptionally%2520well%2520in%2520real-world%250Aapplications%2520but%2520encounter%2520challenges%2520such%2520as%2520limited%2520scalability%252C%2520poor%250Ageneralization%252C%2520and%2520suboptimal%2520zero-shot%2520performance.%2520Inspired%2520by%2520large%250Alanguage%2520models%252C%2520there%2520is%2520interest%2520in%2520developing%2520large%2520time%2520series%2520models%2520%2528LTM%2529%250Ato%2520address%2520these%2520issues.%2520However%252C%2520current%2520methods%2520struggle%2520with%2520training%250Acomplexity%252C%2520adapting%2520human%2520feedback%252C%2520and%2520achieving%2520high%2520predictive%2520accuracy.%2520We%250Aintroduce%2520TimeHF%252C%2520a%2520novel%2520pipeline%2520for%2520creating%2520LTMs%2520with%25206%2520billion%2520parameters%252C%250Aincorporating%2520human%2520feedback.%2520We%2520use%2520patch%2520convolutional%2520embedding%2520to%2520capture%250Along%2520time%2520series%2520information%2520and%2520design%2520a%2520human%2520feedback%2520mechanism%2520called%250Atime-series%2520policy%2520optimization.%2520Deployed%2520in%2520JD.com%2527s%2520supply%2520chain%252C%2520TimeHF%250Ahandles%2520automated%2520replenishment%2520for%2520over%252020%252C000%2520products%252C%2520improving%2520prediction%250Aaccuracy%2520by%252033.21%2525%2520over%2520existing%2520methods.%2520This%2520work%2520advances%2520LTM%2520technology%2520and%250Ashows%2520significant%2520industrial%2520benefits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeHF%3A%20Billion-Scale%20Time%20Series%20Models%20Guided%20by%20Human%20Feedback&entry.906535625=Yongzhi%20Qi%20and%20Hao%20Hu%20and%20Dazhou%20Lei%20and%20Jianshen%20Zhang%20and%20Zhengxin%20Shi%20and%20Yulin%20Huang%20and%20Zhengyu%20Chen%20and%20Xiaoming%20Lin%20and%20Zuo-Jun%20Max%20Shen&entry.1292438233=%20%20Time%20series%20neural%20networks%20perform%20exceptionally%20well%20in%20real-world%0Aapplications%20but%20encounter%20challenges%20such%20as%20limited%20scalability%2C%20poor%0Ageneralization%2C%20and%20suboptimal%20zero-shot%20performance.%20Inspired%20by%20large%0Alanguage%20models%2C%20there%20is%20interest%20in%20developing%20large%20time%20series%20models%20%28LTM%29%0Ato%20address%20these%20issues.%20However%2C%20current%20methods%20struggle%20with%20training%0Acomplexity%2C%20adapting%20human%20feedback%2C%20and%20achieving%20high%20predictive%20accuracy.%20We%0Aintroduce%20TimeHF%2C%20a%20novel%20pipeline%20for%20creating%20LTMs%20with%206%20billion%20parameters%2C%0Aincorporating%20human%20feedback.%20We%20use%20patch%20convolutional%20embedding%20to%20capture%0Along%20time%20series%20information%20and%20design%20a%20human%20feedback%20mechanism%20called%0Atime-series%20policy%20optimization.%20Deployed%20in%20JD.com%27s%20supply%20chain%2C%20TimeHF%0Ahandles%20automated%20replenishment%20for%20over%2020%2C000%20products%2C%20improving%20prediction%0Aaccuracy%20by%2033.21%25%20over%20existing%20methods.%20This%20work%20advances%20LTM%20technology%20and%0Ashows%20significant%20industrial%20benefits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15942v1&entry.124074799=Read"},
{"title": "An Explainable Disease Surveillance System for Early Prediction of\n  Multiple Chronic Diseases", "author": "Shaheer Ahmad Khan and Muhammad Usamah Shahid and Ahmad Abdullah and Ibrahim Hashmat and Muddassar Farooq", "abstract": "  This study addresses a critical gap in the healthcare system by developing a\nclinically meaningful, practical, and explainable disease surveillance system\nfor multiple chronic diseases, utilizing routine EHR data from multiple U.S.\npractices integrated with CureMD's EMR/EHR system. Unlike traditional\nsystems--using AI models that rely on features from patients' labs--our\napproach focuses on routinely available data, such as medical history, vitals,\ndiagnoses, and medications, to preemptively assess the risks of chronic\ndiseases in the next year. We trained three distinct models for each chronic\ndisease: prediction models that forecast the risk of a disease 3, 6, and 12\nmonths before a potential diagnosis. We developed Random Forest models, which\nwere internally validated using F1 scores and AUROC as performance metrics and\nfurther evaluated by a panel of expert physicians for clinical relevance based\non inferences grounded in medical knowledge. Additionally, we discuss our\nimplementation of integrating these models into a practical EMR system. Beyond\nusing Shapley attributes and surrogate models for explainability, we also\nintroduce a new rule-engineering framework to enhance the intrinsic\nexplainability of Random Forests.\n", "link": "http://arxiv.org/abs/2501.15969v1", "date": "2025-01-27", "relevancy": 1.3141, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.45}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4366}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Explainable%20Disease%20Surveillance%20System%20for%20Early%20Prediction%20of%0A%20%20Multiple%20Chronic%20Diseases&body=Title%3A%20An%20Explainable%20Disease%20Surveillance%20System%20for%20Early%20Prediction%20of%0A%20%20Multiple%20Chronic%20Diseases%0AAuthor%3A%20Shaheer%20Ahmad%20Khan%20and%20Muhammad%20Usamah%20Shahid%20and%20Ahmad%20Abdullah%20and%20Ibrahim%20Hashmat%20and%20Muddassar%20Farooq%0AAbstract%3A%20%20%20This%20study%20addresses%20a%20critical%20gap%20in%20the%20healthcare%20system%20by%20developing%20a%0Aclinically%20meaningful%2C%20practical%2C%20and%20explainable%20disease%20surveillance%20system%0Afor%20multiple%20chronic%20diseases%2C%20utilizing%20routine%20EHR%20data%20from%20multiple%20U.S.%0Apractices%20integrated%20with%20CureMD%27s%20EMR/EHR%20system.%20Unlike%20traditional%0Asystems--using%20AI%20models%20that%20rely%20on%20features%20from%20patients%27%20labs--our%0Aapproach%20focuses%20on%20routinely%20available%20data%2C%20such%20as%20medical%20history%2C%20vitals%2C%0Adiagnoses%2C%20and%20medications%2C%20to%20preemptively%20assess%20the%20risks%20of%20chronic%0Adiseases%20in%20the%20next%20year.%20We%20trained%20three%20distinct%20models%20for%20each%20chronic%0Adisease%3A%20prediction%20models%20that%20forecast%20the%20risk%20of%20a%20disease%203%2C%206%2C%20and%2012%0Amonths%20before%20a%20potential%20diagnosis.%20We%20developed%20Random%20Forest%20models%2C%20which%0Awere%20internally%20validated%20using%20F1%20scores%20and%20AUROC%20as%20performance%20metrics%20and%0Afurther%20evaluated%20by%20a%20panel%20of%20expert%20physicians%20for%20clinical%20relevance%20based%0Aon%20inferences%20grounded%20in%20medical%20knowledge.%20Additionally%2C%20we%20discuss%20our%0Aimplementation%20of%20integrating%20these%20models%20into%20a%20practical%20EMR%20system.%20Beyond%0Ausing%20Shapley%20attributes%20and%20surrogate%20models%20for%20explainability%2C%20we%20also%0Aintroduce%20a%20new%20rule-engineering%20framework%20to%20enhance%20the%20intrinsic%0Aexplainability%20of%20Random%20Forests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Explainable%2520Disease%2520Surveillance%2520System%2520for%2520Early%2520Prediction%2520of%250A%2520%2520Multiple%2520Chronic%2520Diseases%26entry.906535625%3DShaheer%2520Ahmad%2520Khan%2520and%2520Muhammad%2520Usamah%2520Shahid%2520and%2520Ahmad%2520Abdullah%2520and%2520Ibrahim%2520Hashmat%2520and%2520Muddassar%2520Farooq%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520a%2520critical%2520gap%2520in%2520the%2520healthcare%2520system%2520by%2520developing%2520a%250Aclinically%2520meaningful%252C%2520practical%252C%2520and%2520explainable%2520disease%2520surveillance%2520system%250Afor%2520multiple%2520chronic%2520diseases%252C%2520utilizing%2520routine%2520EHR%2520data%2520from%2520multiple%2520U.S.%250Apractices%2520integrated%2520with%2520CureMD%2527s%2520EMR/EHR%2520system.%2520Unlike%2520traditional%250Asystems--using%2520AI%2520models%2520that%2520rely%2520on%2520features%2520from%2520patients%2527%2520labs--our%250Aapproach%2520focuses%2520on%2520routinely%2520available%2520data%252C%2520such%2520as%2520medical%2520history%252C%2520vitals%252C%250Adiagnoses%252C%2520and%2520medications%252C%2520to%2520preemptively%2520assess%2520the%2520risks%2520of%2520chronic%250Adiseases%2520in%2520the%2520next%2520year.%2520We%2520trained%2520three%2520distinct%2520models%2520for%2520each%2520chronic%250Adisease%253A%2520prediction%2520models%2520that%2520forecast%2520the%2520risk%2520of%2520a%2520disease%25203%252C%25206%252C%2520and%252012%250Amonths%2520before%2520a%2520potential%2520diagnosis.%2520We%2520developed%2520Random%2520Forest%2520models%252C%2520which%250Awere%2520internally%2520validated%2520using%2520F1%2520scores%2520and%2520AUROC%2520as%2520performance%2520metrics%2520and%250Afurther%2520evaluated%2520by%2520a%2520panel%2520of%2520expert%2520physicians%2520for%2520clinical%2520relevance%2520based%250Aon%2520inferences%2520grounded%2520in%2520medical%2520knowledge.%2520Additionally%252C%2520we%2520discuss%2520our%250Aimplementation%2520of%2520integrating%2520these%2520models%2520into%2520a%2520practical%2520EMR%2520system.%2520Beyond%250Ausing%2520Shapley%2520attributes%2520and%2520surrogate%2520models%2520for%2520explainability%252C%2520we%2520also%250Aintroduce%2520a%2520new%2520rule-engineering%2520framework%2520to%2520enhance%2520the%2520intrinsic%250Aexplainability%2520of%2520Random%2520Forests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Explainable%20Disease%20Surveillance%20System%20for%20Early%20Prediction%20of%0A%20%20Multiple%20Chronic%20Diseases&entry.906535625=Shaheer%20Ahmad%20Khan%20and%20Muhammad%20Usamah%20Shahid%20and%20Ahmad%20Abdullah%20and%20Ibrahim%20Hashmat%20and%20Muddassar%20Farooq&entry.1292438233=%20%20This%20study%20addresses%20a%20critical%20gap%20in%20the%20healthcare%20system%20by%20developing%20a%0Aclinically%20meaningful%2C%20practical%2C%20and%20explainable%20disease%20surveillance%20system%0Afor%20multiple%20chronic%20diseases%2C%20utilizing%20routine%20EHR%20data%20from%20multiple%20U.S.%0Apractices%20integrated%20with%20CureMD%27s%20EMR/EHR%20system.%20Unlike%20traditional%0Asystems--using%20AI%20models%20that%20rely%20on%20features%20from%20patients%27%20labs--our%0Aapproach%20focuses%20on%20routinely%20available%20data%2C%20such%20as%20medical%20history%2C%20vitals%2C%0Adiagnoses%2C%20and%20medications%2C%20to%20preemptively%20assess%20the%20risks%20of%20chronic%0Adiseases%20in%20the%20next%20year.%20We%20trained%20three%20distinct%20models%20for%20each%20chronic%0Adisease%3A%20prediction%20models%20that%20forecast%20the%20risk%20of%20a%20disease%203%2C%206%2C%20and%2012%0Amonths%20before%20a%20potential%20diagnosis.%20We%20developed%20Random%20Forest%20models%2C%20which%0Awere%20internally%20validated%20using%20F1%20scores%20and%20AUROC%20as%20performance%20metrics%20and%0Afurther%20evaluated%20by%20a%20panel%20of%20expert%20physicians%20for%20clinical%20relevance%20based%0Aon%20inferences%20grounded%20in%20medical%20knowledge.%20Additionally%2C%20we%20discuss%20our%0Aimplementation%20of%20integrating%20these%20models%20into%20a%20practical%20EMR%20system.%20Beyond%0Ausing%20Shapley%20attributes%20and%20surrogate%20models%20for%20explainability%2C%20we%20also%0Aintroduce%20a%20new%20rule-engineering%20framework%20to%20enhance%20the%20intrinsic%0Aexplainability%20of%20Random%20Forests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15969v1&entry.124074799=Read"},
{"title": "AlgoRxplorers | Precision in Mutation -- Enhancing Drug Design with\n  Advanced Protein Stability Prediction Tools", "author": "Karishma Thakrar and Jiangqin Ma and Max Diamond and Akash Patel", "abstract": "  Predicting the impact of single-point amino acid mutations on protein\nstability is essential for understanding disease mechanisms and advancing drug\ndevelopment. Protein stability, quantified by changes in Gibbs free energy\n($\\Delta\\Delta G$), is influenced by these mutations. However, the scarcity of\ndata and the complexity of model interpretation pose challenges in accurately\npredicting stability changes. This study proposes the application of deep\nneural networks, leveraging transfer learning and fusing complementary\ninformation from different models, to create a feature-rich representation of\nthe protein stability landscape. We developed four models, with our third\nmodel, ThermoMPNN+, demonstrating the best performance in predicting\n$\\Delta\\Delta G$ values. This approach, which integrates diverse feature sets\nand embeddings through latent transfusion techniques, aims to refine\n$\\Delta\\Delta G$ predictions and contribute to a deeper understanding of\nprotein dynamics, potentially leading to advancements in disease research and\ndrug discovery.\n", "link": "http://arxiv.org/abs/2501.07014v2", "date": "2025-01-27", "relevancy": 1.9401, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4886}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.486}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlgoRxplorers%20%7C%20Precision%20in%20Mutation%20--%20Enhancing%20Drug%20Design%20with%0A%20%20Advanced%20Protein%20Stability%20Prediction%20Tools&body=Title%3A%20AlgoRxplorers%20%7C%20Precision%20in%20Mutation%20--%20Enhancing%20Drug%20Design%20with%0A%20%20Advanced%20Protein%20Stability%20Prediction%20Tools%0AAuthor%3A%20Karishma%20Thakrar%20and%20Jiangqin%20Ma%20and%20Max%20Diamond%20and%20Akash%20Patel%0AAbstract%3A%20%20%20Predicting%20the%20impact%20of%20single-point%20amino%20acid%20mutations%20on%20protein%0Astability%20is%20essential%20for%20understanding%20disease%20mechanisms%20and%20advancing%20drug%0Adevelopment.%20Protein%20stability%2C%20quantified%20by%20changes%20in%20Gibbs%20free%20energy%0A%28%24%5CDelta%5CDelta%20G%24%29%2C%20is%20influenced%20by%20these%20mutations.%20However%2C%20the%20scarcity%20of%0Adata%20and%20the%20complexity%20of%20model%20interpretation%20pose%20challenges%20in%20accurately%0Apredicting%20stability%20changes.%20This%20study%20proposes%20the%20application%20of%20deep%0Aneural%20networks%2C%20leveraging%20transfer%20learning%20and%20fusing%20complementary%0Ainformation%20from%20different%20models%2C%20to%20create%20a%20feature-rich%20representation%20of%0Athe%20protein%20stability%20landscape.%20We%20developed%20four%20models%2C%20with%20our%20third%0Amodel%2C%20ThermoMPNN%2B%2C%20demonstrating%20the%20best%20performance%20in%20predicting%0A%24%5CDelta%5CDelta%20G%24%20values.%20This%20approach%2C%20which%20integrates%20diverse%20feature%20sets%0Aand%20embeddings%20through%20latent%20transfusion%20techniques%2C%20aims%20to%20refine%0A%24%5CDelta%5CDelta%20G%24%20predictions%20and%20contribute%20to%20a%20deeper%20understanding%20of%0Aprotein%20dynamics%2C%20potentially%20leading%20to%20advancements%20in%20disease%20research%20and%0Adrug%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07014v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgoRxplorers%2520%257C%2520Precision%2520in%2520Mutation%2520--%2520Enhancing%2520Drug%2520Design%2520with%250A%2520%2520Advanced%2520Protein%2520Stability%2520Prediction%2520Tools%26entry.906535625%3DKarishma%2520Thakrar%2520and%2520Jiangqin%2520Ma%2520and%2520Max%2520Diamond%2520and%2520Akash%2520Patel%26entry.1292438233%3D%2520%2520Predicting%2520the%2520impact%2520of%2520single-point%2520amino%2520acid%2520mutations%2520on%2520protein%250Astability%2520is%2520essential%2520for%2520understanding%2520disease%2520mechanisms%2520and%2520advancing%2520drug%250Adevelopment.%2520Protein%2520stability%252C%2520quantified%2520by%2520changes%2520in%2520Gibbs%2520free%2520energy%250A%2528%2524%255CDelta%255CDelta%2520G%2524%2529%252C%2520is%2520influenced%2520by%2520these%2520mutations.%2520However%252C%2520the%2520scarcity%2520of%250Adata%2520and%2520the%2520complexity%2520of%2520model%2520interpretation%2520pose%2520challenges%2520in%2520accurately%250Apredicting%2520stability%2520changes.%2520This%2520study%2520proposes%2520the%2520application%2520of%2520deep%250Aneural%2520networks%252C%2520leveraging%2520transfer%2520learning%2520and%2520fusing%2520complementary%250Ainformation%2520from%2520different%2520models%252C%2520to%2520create%2520a%2520feature-rich%2520representation%2520of%250Athe%2520protein%2520stability%2520landscape.%2520We%2520developed%2520four%2520models%252C%2520with%2520our%2520third%250Amodel%252C%2520ThermoMPNN%252B%252C%2520demonstrating%2520the%2520best%2520performance%2520in%2520predicting%250A%2524%255CDelta%255CDelta%2520G%2524%2520values.%2520This%2520approach%252C%2520which%2520integrates%2520diverse%2520feature%2520sets%250Aand%2520embeddings%2520through%2520latent%2520transfusion%2520techniques%252C%2520aims%2520to%2520refine%250A%2524%255CDelta%255CDelta%2520G%2524%2520predictions%2520and%2520contribute%2520to%2520a%2520deeper%2520understanding%2520of%250Aprotein%2520dynamics%252C%2520potentially%2520leading%2520to%2520advancements%2520in%2520disease%2520research%2520and%250Adrug%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07014v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlgoRxplorers%20%7C%20Precision%20in%20Mutation%20--%20Enhancing%20Drug%20Design%20with%0A%20%20Advanced%20Protein%20Stability%20Prediction%20Tools&entry.906535625=Karishma%20Thakrar%20and%20Jiangqin%20Ma%20and%20Max%20Diamond%20and%20Akash%20Patel&entry.1292438233=%20%20Predicting%20the%20impact%20of%20single-point%20amino%20acid%20mutations%20on%20protein%0Astability%20is%20essential%20for%20understanding%20disease%20mechanisms%20and%20advancing%20drug%0Adevelopment.%20Protein%20stability%2C%20quantified%20by%20changes%20in%20Gibbs%20free%20energy%0A%28%24%5CDelta%5CDelta%20G%24%29%2C%20is%20influenced%20by%20these%20mutations.%20However%2C%20the%20scarcity%20of%0Adata%20and%20the%20complexity%20of%20model%20interpretation%20pose%20challenges%20in%20accurately%0Apredicting%20stability%20changes.%20This%20study%20proposes%20the%20application%20of%20deep%0Aneural%20networks%2C%20leveraging%20transfer%20learning%20and%20fusing%20complementary%0Ainformation%20from%20different%20models%2C%20to%20create%20a%20feature-rich%20representation%20of%0Athe%20protein%20stability%20landscape.%20We%20developed%20four%20models%2C%20with%20our%20third%0Amodel%2C%20ThermoMPNN%2B%2C%20demonstrating%20the%20best%20performance%20in%20predicting%0A%24%5CDelta%5CDelta%20G%24%20values.%20This%20approach%2C%20which%20integrates%20diverse%20feature%20sets%0Aand%20embeddings%20through%20latent%20transfusion%20techniques%2C%20aims%20to%20refine%0A%24%5CDelta%5CDelta%20G%24%20predictions%20and%20contribute%20to%20a%20deeper%20understanding%20of%0Aprotein%20dynamics%2C%20potentially%20leading%20to%20advancements%20in%20disease%20research%20and%0Adrug%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07014v2&entry.124074799=Read"},
{"title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits", "author": "Pavamana K J and Chandramani Kishore Singh", "abstract": "  We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.\n", "link": "http://arxiv.org/abs/2405.00080v4", "date": "2025-01-27", "relevancy": 1.7145, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4561}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4307}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recommenadation%20aided%20Caching%20using%20Combinatorial%20Multi-armed%20Bandits&body=Title%3A%20Recommenadation%20aided%20Caching%20using%20Combinatorial%20Multi-armed%20Bandits%0AAuthor%3A%20Pavamana%20K%20J%20and%20Chandramani%20Kishore%20Singh%0AAbstract%3A%20%20%20We%20study%20content%20caching%20with%20recommendations%20in%20a%20wireless%20network%20where%20the%0Ausers%20are%20connected%20through%20a%20base%20station%20equipped%20with%20a%20finite-capacity%0Acache.%20We%20assume%20a%20fixed%20set%20of%20contents%20with%20unknown%20user%20preferences%20and%0Acontent%20popularities.%20The%20base%20station%20can%20cache%20a%20subset%20of%20the%20contents%20and%0Acan%20also%20recommend%20subsets%20of%20the%20contents%20to%20different%20users%20in%20order%20to%0Aencourage%20them%20to%20request%20the%20recommended%20contents.%20Recommendations%2C%20depending%0Aon%20their%20acceptability%2C%20can%20thus%20be%20used%20to%20increase%20cache%20hits.%20We%20first%0Aassume%20that%20the%20users%27%20recommendation%20acceptabilities%20are%20known%20and%20formulate%0Athe%20cache%20hit%20optimization%20problem%20as%20a%20combinatorial%20multi-armed%20bandit%0A%28CMAB%29.%20We%20propose%20a%20UCB-based%20algorithm%20to%20decide%20which%20contents%20to%20cache%20and%0Arecommend%20and%20provide%20an%20upper%20bound%20on%20the%20regret%20of%20this%20algorithm.%0ASubsequently%2C%20we%20consider%20a%20more%20general%20scenario%20where%20the%20users%27%0Arecommendation%20acceptabilities%20are%20also%20unknown%20and%20propose%20another%20UCB-based%0Aalgorithm%20that%20learns%20these%20as%20well.%20We%20numerically%20demonstrate%20the%20performance%0Aof%20our%20algorithms%20and%20compare%20these%20to%20state-of-the-art%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00080v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecommenadation%2520aided%2520Caching%2520using%2520Combinatorial%2520Multi-armed%2520Bandits%26entry.906535625%3DPavamana%2520K%2520J%2520and%2520Chandramani%2520Kishore%2520Singh%26entry.1292438233%3D%2520%2520We%2520study%2520content%2520caching%2520with%2520recommendations%2520in%2520a%2520wireless%2520network%2520where%2520the%250Ausers%2520are%2520connected%2520through%2520a%2520base%2520station%2520equipped%2520with%2520a%2520finite-capacity%250Acache.%2520We%2520assume%2520a%2520fixed%2520set%2520of%2520contents%2520with%2520unknown%2520user%2520preferences%2520and%250Acontent%2520popularities.%2520The%2520base%2520station%2520can%2520cache%2520a%2520subset%2520of%2520the%2520contents%2520and%250Acan%2520also%2520recommend%2520subsets%2520of%2520the%2520contents%2520to%2520different%2520users%2520in%2520order%2520to%250Aencourage%2520them%2520to%2520request%2520the%2520recommended%2520contents.%2520Recommendations%252C%2520depending%250Aon%2520their%2520acceptability%252C%2520can%2520thus%2520be%2520used%2520to%2520increase%2520cache%2520hits.%2520We%2520first%250Aassume%2520that%2520the%2520users%2527%2520recommendation%2520acceptabilities%2520are%2520known%2520and%2520formulate%250Athe%2520cache%2520hit%2520optimization%2520problem%2520as%2520a%2520combinatorial%2520multi-armed%2520bandit%250A%2528CMAB%2529.%2520We%2520propose%2520a%2520UCB-based%2520algorithm%2520to%2520decide%2520which%2520contents%2520to%2520cache%2520and%250Arecommend%2520and%2520provide%2520an%2520upper%2520bound%2520on%2520the%2520regret%2520of%2520this%2520algorithm.%250ASubsequently%252C%2520we%2520consider%2520a%2520more%2520general%2520scenario%2520where%2520the%2520users%2527%250Arecommendation%2520acceptabilities%2520are%2520also%2520unknown%2520and%2520propose%2520another%2520UCB-based%250Aalgorithm%2520that%2520learns%2520these%2520as%2520well.%2520We%2520numerically%2520demonstrate%2520the%2520performance%250Aof%2520our%2520algorithms%2520and%2520compare%2520these%2520to%2520state-of-the-art%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00080v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recommenadation%20aided%20Caching%20using%20Combinatorial%20Multi-armed%20Bandits&entry.906535625=Pavamana%20K%20J%20and%20Chandramani%20Kishore%20Singh&entry.1292438233=%20%20We%20study%20content%20caching%20with%20recommendations%20in%20a%20wireless%20network%20where%20the%0Ausers%20are%20connected%20through%20a%20base%20station%20equipped%20with%20a%20finite-capacity%0Acache.%20We%20assume%20a%20fixed%20set%20of%20contents%20with%20unknown%20user%20preferences%20and%0Acontent%20popularities.%20The%20base%20station%20can%20cache%20a%20subset%20of%20the%20contents%20and%0Acan%20also%20recommend%20subsets%20of%20the%20contents%20to%20different%20users%20in%20order%20to%0Aencourage%20them%20to%20request%20the%20recommended%20contents.%20Recommendations%2C%20depending%0Aon%20their%20acceptability%2C%20can%20thus%20be%20used%20to%20increase%20cache%20hits.%20We%20first%0Aassume%20that%20the%20users%27%20recommendation%20acceptabilities%20are%20known%20and%20formulate%0Athe%20cache%20hit%20optimization%20problem%20as%20a%20combinatorial%20multi-armed%20bandit%0A%28CMAB%29.%20We%20propose%20a%20UCB-based%20algorithm%20to%20decide%20which%20contents%20to%20cache%20and%0Arecommend%20and%20provide%20an%20upper%20bound%20on%20the%20regret%20of%20this%20algorithm.%0ASubsequently%2C%20we%20consider%20a%20more%20general%20scenario%20where%20the%20users%27%0Arecommendation%20acceptabilities%20are%20also%20unknown%20and%20propose%20another%20UCB-based%0Aalgorithm%20that%20learns%20these%20as%20well.%20We%20numerically%20demonstrate%20the%20performance%0Aof%20our%20algorithms%20and%20compare%20these%20to%20state-of-the-art%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00080v4&entry.124074799=Read"},
{"title": "Solving Turbulent Rayleigh-B\u00e9nard Convection using Fourier Neural\n  Operators", "author": "Michiel Straat and Thorben Markmann and Barbara Hammer", "abstract": "  We train Fourier Neural Operator (FNO) surrogate models for Rayleigh-B\\'enard\nConvection (RBC), a model for convection processes that occur in nature and\nindustrial settings. We compare the prediction accuracy and model properties of\nFNO surrogates to two popular surrogates used in fluid dynamics: the Dynamic\nMode Decomposition and the Linearly-Recurrent Autoencoder Network. We regard\nDirect Numerical Simulations (DNS) of the RBC equations as the ground truth on\nwhich the models are trained and evaluated in different settings. The FNO\nperforms favorably when compared to the DMD and LRAN and its predictions are\nfast and highly accurate for this task. Additionally, we show its zero-shot\nsuper-resolution ability for the convection dynamics. The FNO model has a high\npotential to be used in downstream tasks such as flow control in RBC.\n", "link": "http://arxiv.org/abs/2501.16209v1", "date": "2025-01-27", "relevancy": 1.4327, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5291}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4692}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Turbulent%20Rayleigh-B%C3%A9nard%20Convection%20using%20Fourier%20Neural%0A%20%20Operators&body=Title%3A%20Solving%20Turbulent%20Rayleigh-B%C3%A9nard%20Convection%20using%20Fourier%20Neural%0A%20%20Operators%0AAuthor%3A%20Michiel%20Straat%20and%20Thorben%20Markmann%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20We%20train%20Fourier%20Neural%20Operator%20%28FNO%29%20surrogate%20models%20for%20Rayleigh-B%5C%27enard%0AConvection%20%28RBC%29%2C%20a%20model%20for%20convection%20processes%20that%20occur%20in%20nature%20and%0Aindustrial%20settings.%20We%20compare%20the%20prediction%20accuracy%20and%20model%20properties%20of%0AFNO%20surrogates%20to%20two%20popular%20surrogates%20used%20in%20fluid%20dynamics%3A%20the%20Dynamic%0AMode%20Decomposition%20and%20the%20Linearly-Recurrent%20Autoencoder%20Network.%20We%20regard%0ADirect%20Numerical%20Simulations%20%28DNS%29%20of%20the%20RBC%20equations%20as%20the%20ground%20truth%20on%0Awhich%20the%20models%20are%20trained%20and%20evaluated%20in%20different%20settings.%20The%20FNO%0Aperforms%20favorably%20when%20compared%20to%20the%20DMD%20and%20LRAN%20and%20its%20predictions%20are%0Afast%20and%20highly%20accurate%20for%20this%20task.%20Additionally%2C%20we%20show%20its%20zero-shot%0Asuper-resolution%20ability%20for%20the%20convection%20dynamics.%20The%20FNO%20model%20has%20a%20high%0Apotential%20to%20be%20used%20in%20downstream%20tasks%20such%20as%20flow%20control%20in%20RBC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Turbulent%2520Rayleigh-B%25C3%25A9nard%2520Convection%2520using%2520Fourier%2520Neural%250A%2520%2520Operators%26entry.906535625%3DMichiel%2520Straat%2520and%2520Thorben%2520Markmann%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520We%2520train%2520Fourier%2520Neural%2520Operator%2520%2528FNO%2529%2520surrogate%2520models%2520for%2520Rayleigh-B%255C%2527enard%250AConvection%2520%2528RBC%2529%252C%2520a%2520model%2520for%2520convection%2520processes%2520that%2520occur%2520in%2520nature%2520and%250Aindustrial%2520settings.%2520We%2520compare%2520the%2520prediction%2520accuracy%2520and%2520model%2520properties%2520of%250AFNO%2520surrogates%2520to%2520two%2520popular%2520surrogates%2520used%2520in%2520fluid%2520dynamics%253A%2520the%2520Dynamic%250AMode%2520Decomposition%2520and%2520the%2520Linearly-Recurrent%2520Autoencoder%2520Network.%2520We%2520regard%250ADirect%2520Numerical%2520Simulations%2520%2528DNS%2529%2520of%2520the%2520RBC%2520equations%2520as%2520the%2520ground%2520truth%2520on%250Awhich%2520the%2520models%2520are%2520trained%2520and%2520evaluated%2520in%2520different%2520settings.%2520The%2520FNO%250Aperforms%2520favorably%2520when%2520compared%2520to%2520the%2520DMD%2520and%2520LRAN%2520and%2520its%2520predictions%2520are%250Afast%2520and%2520highly%2520accurate%2520for%2520this%2520task.%2520Additionally%252C%2520we%2520show%2520its%2520zero-shot%250Asuper-resolution%2520ability%2520for%2520the%2520convection%2520dynamics.%2520The%2520FNO%2520model%2520has%2520a%2520high%250Apotential%2520to%2520be%2520used%2520in%2520downstream%2520tasks%2520such%2520as%2520flow%2520control%2520in%2520RBC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Turbulent%20Rayleigh-B%C3%A9nard%20Convection%20using%20Fourier%20Neural%0A%20%20Operators&entry.906535625=Michiel%20Straat%20and%20Thorben%20Markmann%20and%20Barbara%20Hammer&entry.1292438233=%20%20We%20train%20Fourier%20Neural%20Operator%20%28FNO%29%20surrogate%20models%20for%20Rayleigh-B%5C%27enard%0AConvection%20%28RBC%29%2C%20a%20model%20for%20convection%20processes%20that%20occur%20in%20nature%20and%0Aindustrial%20settings.%20We%20compare%20the%20prediction%20accuracy%20and%20model%20properties%20of%0AFNO%20surrogates%20to%20two%20popular%20surrogates%20used%20in%20fluid%20dynamics%3A%20the%20Dynamic%0AMode%20Decomposition%20and%20the%20Linearly-Recurrent%20Autoencoder%20Network.%20We%20regard%0ADirect%20Numerical%20Simulations%20%28DNS%29%20of%20the%20RBC%20equations%20as%20the%20ground%20truth%20on%0Awhich%20the%20models%20are%20trained%20and%20evaluated%20in%20different%20settings.%20The%20FNO%0Aperforms%20favorably%20when%20compared%20to%20the%20DMD%20and%20LRAN%20and%20its%20predictions%20are%0Afast%20and%20highly%20accurate%20for%20this%20task.%20Additionally%2C%20we%20show%20its%20zero-shot%0Asuper-resolution%20ability%20for%20the%20convection%20dynamics.%20The%20FNO%20model%20has%20a%20high%0Apotential%20to%20be%20used%20in%20downstream%20tasks%20such%20as%20flow%20control%20in%20RBC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16209v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


