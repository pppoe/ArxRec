<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240829.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion\n  Model", "author": "Fangfu Liu and Wenqiang Sun and Hanyang Wang and Yikai Wang and Haowen Sun and Junliang Ye and Jun Zhang and Yueqi Duan", "abstract": "  Advancements in 3D scene reconstruction have transformed 2D images from the\nreal world into 3D models, producing realistic 3D results from hundreds of\ninput photos. Despite great success in dense-view reconstruction scenarios,\nrendering a detailed scene from insufficient captured views is still an\nill-posed optimization problem, often resulting in artifacts and distortions in\nunseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction\nparadigm that reframes the ambiguous reconstruction challenge as a temporal\ngeneration task. The key insight is to unleash the strong generative prior of\nlarge pre-trained video diffusion models for sparse-view reconstruction.\nHowever, 3D view consistency struggles to be accurately preserved in directly\ngenerated video frames from pre-trained models. To address this, given limited\ninput views, the proposed ReconX first constructs a global point cloud and\nencodes it into a contextual space as the 3D structure condition. Guided by the\ncondition, the video diffusion model then synthesizes video frames that are\nboth detail-preserved and exhibit a high degree of 3D consistency, ensuring the\ncoherence of the scene from various perspectives. Finally, we recover the 3D\nscene from the generated video through a confidence-aware 3D Gaussian Splatting\noptimization scheme. Extensive experiments on various real-world datasets show\nthe superiority of our ReconX over state-of-the-art methods in terms of quality\nand generalizability.\n", "link": "http://arxiv.org/abs/2408.16767v1", "date": "2024-08-29", "relevancy": 3.4706, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7216}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReconX%3A%20Reconstruct%20Any%20Scene%20from%20Sparse%20Views%20with%20Video%20Diffusion%0A%20%20Model&body=Title%3A%20ReconX%3A%20Reconstruct%20Any%20Scene%20from%20Sparse%20Views%20with%20Video%20Diffusion%0A%20%20Model%0AAuthor%3A%20Fangfu%20Liu%20and%20Wenqiang%20Sun%20and%20Hanyang%20Wang%20and%20Yikai%20Wang%20and%20Haowen%20Sun%20and%20Junliang%20Ye%20and%20Jun%20Zhang%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20Advancements%20in%203D%20scene%20reconstruction%20have%20transformed%202D%20images%20from%20the%0Areal%20world%20into%203D%20models%2C%20producing%20realistic%203D%20results%20from%20hundreds%20of%0Ainput%20photos.%20Despite%20great%20success%20in%20dense-view%20reconstruction%20scenarios%2C%0Arendering%20a%20detailed%20scene%20from%20insufficient%20captured%20views%20is%20still%20an%0Aill-posed%20optimization%20problem%2C%20often%20resulting%20in%20artifacts%20and%20distortions%20in%0Aunseen%20areas.%20In%20this%20paper%2C%20we%20propose%20ReconX%2C%20a%20novel%203D%20scene%20reconstruction%0Aparadigm%20that%20reframes%20the%20ambiguous%20reconstruction%20challenge%20as%20a%20temporal%0Ageneration%20task.%20The%20key%20insight%20is%20to%20unleash%20the%20strong%20generative%20prior%20of%0Alarge%20pre-trained%20video%20diffusion%20models%20for%20sparse-view%20reconstruction.%0AHowever%2C%203D%20view%20consistency%20struggles%20to%20be%20accurately%20preserved%20in%20directly%0Agenerated%20video%20frames%20from%20pre-trained%20models.%20To%20address%20this%2C%20given%20limited%0Ainput%20views%2C%20the%20proposed%20ReconX%20first%20constructs%20a%20global%20point%20cloud%20and%0Aencodes%20it%20into%20a%20contextual%20space%20as%20the%203D%20structure%20condition.%20Guided%20by%20the%0Acondition%2C%20the%20video%20diffusion%20model%20then%20synthesizes%20video%20frames%20that%20are%0Aboth%20detail-preserved%20and%20exhibit%20a%20high%20degree%20of%203D%20consistency%2C%20ensuring%20the%0Acoherence%20of%20the%20scene%20from%20various%20perspectives.%20Finally%2C%20we%20recover%20the%203D%0Ascene%20from%20the%20generated%20video%20through%20a%20confidence-aware%203D%20Gaussian%20Splatting%0Aoptimization%20scheme.%20Extensive%20experiments%20on%20various%20real-world%20datasets%20show%0Athe%20superiority%20of%20our%20ReconX%20over%20state-of-the-art%20methods%20in%20terms%20of%20quality%0Aand%20generalizability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconX%253A%2520Reconstruct%2520Any%2520Scene%2520from%2520Sparse%2520Views%2520with%2520Video%2520Diffusion%250A%2520%2520Model%26entry.906535625%3DFangfu%2520Liu%2520and%2520Wenqiang%2520Sun%2520and%2520Hanyang%2520Wang%2520and%2520Yikai%2520Wang%2520and%2520Haowen%2520Sun%2520and%2520Junliang%2520Ye%2520and%2520Jun%2520Zhang%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520Advancements%2520in%25203D%2520scene%2520reconstruction%2520have%2520transformed%25202D%2520images%2520from%2520the%250Areal%2520world%2520into%25203D%2520models%252C%2520producing%2520realistic%25203D%2520results%2520from%2520hundreds%2520of%250Ainput%2520photos.%2520Despite%2520great%2520success%2520in%2520dense-view%2520reconstruction%2520scenarios%252C%250Arendering%2520a%2520detailed%2520scene%2520from%2520insufficient%2520captured%2520views%2520is%2520still%2520an%250Aill-posed%2520optimization%2520problem%252C%2520often%2520resulting%2520in%2520artifacts%2520and%2520distortions%2520in%250Aunseen%2520areas.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ReconX%252C%2520a%2520novel%25203D%2520scene%2520reconstruction%250Aparadigm%2520that%2520reframes%2520the%2520ambiguous%2520reconstruction%2520challenge%2520as%2520a%2520temporal%250Ageneration%2520task.%2520The%2520key%2520insight%2520is%2520to%2520unleash%2520the%2520strong%2520generative%2520prior%2520of%250Alarge%2520pre-trained%2520video%2520diffusion%2520models%2520for%2520sparse-view%2520reconstruction.%250AHowever%252C%25203D%2520view%2520consistency%2520struggles%2520to%2520be%2520accurately%2520preserved%2520in%2520directly%250Agenerated%2520video%2520frames%2520from%2520pre-trained%2520models.%2520To%2520address%2520this%252C%2520given%2520limited%250Ainput%2520views%252C%2520the%2520proposed%2520ReconX%2520first%2520constructs%2520a%2520global%2520point%2520cloud%2520and%250Aencodes%2520it%2520into%2520a%2520contextual%2520space%2520as%2520the%25203D%2520structure%2520condition.%2520Guided%2520by%2520the%250Acondition%252C%2520the%2520video%2520diffusion%2520model%2520then%2520synthesizes%2520video%2520frames%2520that%2520are%250Aboth%2520detail-preserved%2520and%2520exhibit%2520a%2520high%2520degree%2520of%25203D%2520consistency%252C%2520ensuring%2520the%250Acoherence%2520of%2520the%2520scene%2520from%2520various%2520perspectives.%2520Finally%252C%2520we%2520recover%2520the%25203D%250Ascene%2520from%2520the%2520generated%2520video%2520through%2520a%2520confidence-aware%25203D%2520Gaussian%2520Splatting%250Aoptimization%2520scheme.%2520Extensive%2520experiments%2520on%2520various%2520real-world%2520datasets%2520show%250Athe%2520superiority%2520of%2520our%2520ReconX%2520over%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520quality%250Aand%2520generalizability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReconX%3A%20Reconstruct%20Any%20Scene%20from%20Sparse%20Views%20with%20Video%20Diffusion%0A%20%20Model&entry.906535625=Fangfu%20Liu%20and%20Wenqiang%20Sun%20and%20Hanyang%20Wang%20and%20Yikai%20Wang%20and%20Haowen%20Sun%20and%20Junliang%20Ye%20and%20Jun%20Zhang%20and%20Yueqi%20Duan&entry.1292438233=%20%20Advancements%20in%203D%20scene%20reconstruction%20have%20transformed%202D%20images%20from%20the%0Areal%20world%20into%203D%20models%2C%20producing%20realistic%203D%20results%20from%20hundreds%20of%0Ainput%20photos.%20Despite%20great%20success%20in%20dense-view%20reconstruction%20scenarios%2C%0Arendering%20a%20detailed%20scene%20from%20insufficient%20captured%20views%20is%20still%20an%0Aill-posed%20optimization%20problem%2C%20often%20resulting%20in%20artifacts%20and%20distortions%20in%0Aunseen%20areas.%20In%20this%20paper%2C%20we%20propose%20ReconX%2C%20a%20novel%203D%20scene%20reconstruction%0Aparadigm%20that%20reframes%20the%20ambiguous%20reconstruction%20challenge%20as%20a%20temporal%0Ageneration%20task.%20The%20key%20insight%20is%20to%20unleash%20the%20strong%20generative%20prior%20of%0Alarge%20pre-trained%20video%20diffusion%20models%20for%20sparse-view%20reconstruction.%0AHowever%2C%203D%20view%20consistency%20struggles%20to%20be%20accurately%20preserved%20in%20directly%0Agenerated%20video%20frames%20from%20pre-trained%20models.%20To%20address%20this%2C%20given%20limited%0Ainput%20views%2C%20the%20proposed%20ReconX%20first%20constructs%20a%20global%20point%20cloud%20and%0Aencodes%20it%20into%20a%20contextual%20space%20as%20the%203D%20structure%20condition.%20Guided%20by%20the%0Acondition%2C%20the%20video%20diffusion%20model%20then%20synthesizes%20video%20frames%20that%20are%0Aboth%20detail-preserved%20and%20exhibit%20a%20high%20degree%20of%203D%20consistency%2C%20ensuring%20the%0Acoherence%20of%20the%20scene%20from%20various%20perspectives.%20Finally%2C%20we%20recover%20the%203D%0Ascene%20from%20the%20generated%20video%20through%20a%20confidence-aware%203D%20Gaussian%20Splatting%0Aoptimization%20scheme.%20Extensive%20experiments%20on%20various%20real-world%20datasets%20show%0Athe%20superiority%20of%20our%20ReconX%20over%20state-of-the-art%20methods%20in%20terms%20of%20quality%0Aand%20generalizability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16767v1&entry.124074799=Read"},
{"title": "UV-free Texture Generation with Denoising and Geodesic Heat Diffusions", "author": "Simone Foti and Stefanos Zafeiriou and Tolga Birdal", "abstract": "  Seams, distortions, wasted UV space, vertex-duplication, and varying\nresolution over the surface are the most prominent issues of the standard\nUV-based texturing of meshes. These issues are particularly acute when\nautomatic UV-unwrapping techniques are used. For this reason, instead of\ngenerating textures in automatically generated UV-planes like most\nstate-of-the-art methods, we propose to represent textures as coloured\npoint-clouds whose colours are generated by a denoising diffusion probabilistic\nmodel constrained to operate on the surface of 3D objects. Our sampling and\nresolution agnostic generative model heavily relies on heat diffusion over the\nsurface of the meshes for spatial communication between points. To enable\nprocessing of arbitrarily sampled point-cloud textures and ensure long-distance\ntexture consistency we introduce a fast re-sampling of the mesh spectral\nproperties used during the heat diffusion and introduce a novel\nheat-diffusion-based self-attention mechanism. Our code and pre-trained models\nare available at github.com/simofoti/UV3-TeD.\n", "link": "http://arxiv.org/abs/2408.16762v1", "date": "2024-08-29", "relevancy": 3.0205, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6166}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6166}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UV-free%20Texture%20Generation%20with%20Denoising%20and%20Geodesic%20Heat%20Diffusions&body=Title%3A%20UV-free%20Texture%20Generation%20with%20Denoising%20and%20Geodesic%20Heat%20Diffusions%0AAuthor%3A%20Simone%20Foti%20and%20Stefanos%20Zafeiriou%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Seams%2C%20distortions%2C%20wasted%20UV%20space%2C%20vertex-duplication%2C%20and%20varying%0Aresolution%20over%20the%20surface%20are%20the%20most%20prominent%20issues%20of%20the%20standard%0AUV-based%20texturing%20of%20meshes.%20These%20issues%20are%20particularly%20acute%20when%0Aautomatic%20UV-unwrapping%20techniques%20are%20used.%20For%20this%20reason%2C%20instead%20of%0Agenerating%20textures%20in%20automatically%20generated%20UV-planes%20like%20most%0Astate-of-the-art%20methods%2C%20we%20propose%20to%20represent%20textures%20as%20coloured%0Apoint-clouds%20whose%20colours%20are%20generated%20by%20a%20denoising%20diffusion%20probabilistic%0Amodel%20constrained%20to%20operate%20on%20the%20surface%20of%203D%20objects.%20Our%20sampling%20and%0Aresolution%20agnostic%20generative%20model%20heavily%20relies%20on%20heat%20diffusion%20over%20the%0Asurface%20of%20the%20meshes%20for%20spatial%20communication%20between%20points.%20To%20enable%0Aprocessing%20of%20arbitrarily%20sampled%20point-cloud%20textures%20and%20ensure%20long-distance%0Atexture%20consistency%20we%20introduce%20a%20fast%20re-sampling%20of%20the%20mesh%20spectral%0Aproperties%20used%20during%20the%20heat%20diffusion%20and%20introduce%20a%20novel%0Aheat-diffusion-based%20self-attention%20mechanism.%20Our%20code%20and%20pre-trained%20models%0Aare%20available%20at%20github.com/simofoti/UV3-TeD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUV-free%2520Texture%2520Generation%2520with%2520Denoising%2520and%2520Geodesic%2520Heat%2520Diffusions%26entry.906535625%3DSimone%2520Foti%2520and%2520Stefanos%2520Zafeiriou%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Seams%252C%2520distortions%252C%2520wasted%2520UV%2520space%252C%2520vertex-duplication%252C%2520and%2520varying%250Aresolution%2520over%2520the%2520surface%2520are%2520the%2520most%2520prominent%2520issues%2520of%2520the%2520standard%250AUV-based%2520texturing%2520of%2520meshes.%2520These%2520issues%2520are%2520particularly%2520acute%2520when%250Aautomatic%2520UV-unwrapping%2520techniques%2520are%2520used.%2520For%2520this%2520reason%252C%2520instead%2520of%250Agenerating%2520textures%2520in%2520automatically%2520generated%2520UV-planes%2520like%2520most%250Astate-of-the-art%2520methods%252C%2520we%2520propose%2520to%2520represent%2520textures%2520as%2520coloured%250Apoint-clouds%2520whose%2520colours%2520are%2520generated%2520by%2520a%2520denoising%2520diffusion%2520probabilistic%250Amodel%2520constrained%2520to%2520operate%2520on%2520the%2520surface%2520of%25203D%2520objects.%2520Our%2520sampling%2520and%250Aresolution%2520agnostic%2520generative%2520model%2520heavily%2520relies%2520on%2520heat%2520diffusion%2520over%2520the%250Asurface%2520of%2520the%2520meshes%2520for%2520spatial%2520communication%2520between%2520points.%2520To%2520enable%250Aprocessing%2520of%2520arbitrarily%2520sampled%2520point-cloud%2520textures%2520and%2520ensure%2520long-distance%250Atexture%2520consistency%2520we%2520introduce%2520a%2520fast%2520re-sampling%2520of%2520the%2520mesh%2520spectral%250Aproperties%2520used%2520during%2520the%2520heat%2520diffusion%2520and%2520introduce%2520a%2520novel%250Aheat-diffusion-based%2520self-attention%2520mechanism.%2520Our%2520code%2520and%2520pre-trained%2520models%250Aare%2520available%2520at%2520github.com/simofoti/UV3-TeD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UV-free%20Texture%20Generation%20with%20Denoising%20and%20Geodesic%20Heat%20Diffusions&entry.906535625=Simone%20Foti%20and%20Stefanos%20Zafeiriou%20and%20Tolga%20Birdal&entry.1292438233=%20%20Seams%2C%20distortions%2C%20wasted%20UV%20space%2C%20vertex-duplication%2C%20and%20varying%0Aresolution%20over%20the%20surface%20are%20the%20most%20prominent%20issues%20of%20the%20standard%0AUV-based%20texturing%20of%20meshes.%20These%20issues%20are%20particularly%20acute%20when%0Aautomatic%20UV-unwrapping%20techniques%20are%20used.%20For%20this%20reason%2C%20instead%20of%0Agenerating%20textures%20in%20automatically%20generated%20UV-planes%20like%20most%0Astate-of-the-art%20methods%2C%20we%20propose%20to%20represent%20textures%20as%20coloured%0Apoint-clouds%20whose%20colours%20are%20generated%20by%20a%20denoising%20diffusion%20probabilistic%0Amodel%20constrained%20to%20operate%20on%20the%20surface%20of%203D%20objects.%20Our%20sampling%20and%0Aresolution%20agnostic%20generative%20model%20heavily%20relies%20on%20heat%20diffusion%20over%20the%0Asurface%20of%20the%20meshes%20for%20spatial%20communication%20between%20points.%20To%20enable%0Aprocessing%20of%20arbitrarily%20sampled%20point-cloud%20textures%20and%20ensure%20long-distance%0Atexture%20consistency%20we%20introduce%20a%20fast%20re-sampling%20of%20the%20mesh%20spectral%0Aproperties%20used%20during%20the%20heat%20diffusion%20and%20introduce%20a%20novel%0Aheat-diffusion-based%20self-attention%20mechanism.%20Our%20code%20and%20pre-trained%20models%0Aare%20available%20at%20github.com/simofoti/UV3-TeD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16762v1&entry.124074799=Read"},
{"title": "Generic Objects as Pose Probes for Few-Shot View Synthesis", "author": "Zhirui Gao and Renjiao Yi and Chenyang Zhu and Ke Zhuang and Wei Chen and Kai Xu", "abstract": "  Radiance fields including NeRFs and 3D Gaussians demonstrate great potential\nin high-fidelity rendering and scene reconstruction, while they require a\nsubstantial number of posed images as inputs. COLMAP is frequently employed for\npreprocessing to estimate poses, while it necessitates a large number of\nfeature matches to operate effectively, and it struggles with scenes\ncharacterized by sparse features, large baselines between images, or a limited\nnumber of input images. We aim to tackle few-view NeRF reconstruction using\nonly 3 to 6 unposed scene images. Traditional methods often use calibration\nboards but they are not common in images. We propose a novel idea of utilizing\neveryday objects, commonly found in both images and real life, as \"pose\nprobes\". The probe object is automatically segmented by SAM, whose shape is\ninitialized from a cube. We apply a dual-branch volume rendering optimization\n(object NeRF and scene NeRF) to constrain the pose optimization and jointly\nrefine the geometry. Specifically, object poses of two views are first\nestimated by PnP matching in an SDF representation, which serves as initial\nposes. PnP matching, requiring only a few features, is suitable for\nfeature-sparse scenes. Additional views are incrementally incorporated to\nrefine poses from preceding views. In experiments, PoseProbe achieves\nstate-of-the-art performance in both pose estimation and novel view synthesis\nacross multiple datasets. We demonstrate its effectiveness, particularly in\nfew-view and large-baseline scenes where COLMAP struggles. In ablations, using\ndifferent objects in a scene yields comparable performance.\n", "link": "http://arxiv.org/abs/2408.16690v1", "date": "2024-08-29", "relevancy": 2.9922, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6075}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5939}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generic%20Objects%20as%20Pose%20Probes%20for%20Few-Shot%20View%20Synthesis&body=Title%3A%20Generic%20Objects%20as%20Pose%20Probes%20for%20Few-Shot%20View%20Synthesis%0AAuthor%3A%20Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Chenyang%20Zhu%20and%20Ke%20Zhuang%20and%20Wei%20Chen%20and%20Kai%20Xu%0AAbstract%3A%20%20%20Radiance%20fields%20including%20NeRFs%20and%203D%20Gaussians%20demonstrate%20great%20potential%0Ain%20high-fidelity%20rendering%20and%20scene%20reconstruction%2C%20while%20they%20require%20a%0Asubstantial%20number%20of%20posed%20images%20as%20inputs.%20COLMAP%20is%20frequently%20employed%20for%0Apreprocessing%20to%20estimate%20poses%2C%20while%20it%20necessitates%20a%20large%20number%20of%0Afeature%20matches%20to%20operate%20effectively%2C%20and%20it%20struggles%20with%20scenes%0Acharacterized%20by%20sparse%20features%2C%20large%20baselines%20between%20images%2C%20or%20a%20limited%0Anumber%20of%20input%20images.%20We%20aim%20to%20tackle%20few-view%20NeRF%20reconstruction%20using%0Aonly%203%20to%206%20unposed%20scene%20images.%20Traditional%20methods%20often%20use%20calibration%0Aboards%20but%20they%20are%20not%20common%20in%20images.%20We%20propose%20a%20novel%20idea%20of%20utilizing%0Aeveryday%20objects%2C%20commonly%20found%20in%20both%20images%20and%20real%20life%2C%20as%20%22pose%0Aprobes%22.%20The%20probe%20object%20is%20automatically%20segmented%20by%20SAM%2C%20whose%20shape%20is%0Ainitialized%20from%20a%20cube.%20We%20apply%20a%20dual-branch%20volume%20rendering%20optimization%0A%28object%20NeRF%20and%20scene%20NeRF%29%20to%20constrain%20the%20pose%20optimization%20and%20jointly%0Arefine%20the%20geometry.%20Specifically%2C%20object%20poses%20of%20two%20views%20are%20first%0Aestimated%20by%20PnP%20matching%20in%20an%20SDF%20representation%2C%20which%20serves%20as%20initial%0Aposes.%20PnP%20matching%2C%20requiring%20only%20a%20few%20features%2C%20is%20suitable%20for%0Afeature-sparse%20scenes.%20Additional%20views%20are%20incrementally%20incorporated%20to%0Arefine%20poses%20from%20preceding%20views.%20In%20experiments%2C%20PoseProbe%20achieves%0Astate-of-the-art%20performance%20in%20both%20pose%20estimation%20and%20novel%20view%20synthesis%0Aacross%20multiple%20datasets.%20We%20demonstrate%20its%20effectiveness%2C%20particularly%20in%0Afew-view%20and%20large-baseline%20scenes%20where%20COLMAP%20struggles.%20In%20ablations%2C%20using%0Adifferent%20objects%20in%20a%20scene%20yields%20comparable%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneric%2520Objects%2520as%2520Pose%2520Probes%2520for%2520Few-Shot%2520View%2520Synthesis%26entry.906535625%3DZhirui%2520Gao%2520and%2520Renjiao%2520Yi%2520and%2520Chenyang%2520Zhu%2520and%2520Ke%2520Zhuang%2520and%2520Wei%2520Chen%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520Radiance%2520fields%2520including%2520NeRFs%2520and%25203D%2520Gaussians%2520demonstrate%2520great%2520potential%250Ain%2520high-fidelity%2520rendering%2520and%2520scene%2520reconstruction%252C%2520while%2520they%2520require%2520a%250Asubstantial%2520number%2520of%2520posed%2520images%2520as%2520inputs.%2520COLMAP%2520is%2520frequently%2520employed%2520for%250Apreprocessing%2520to%2520estimate%2520poses%252C%2520while%2520it%2520necessitates%2520a%2520large%2520number%2520of%250Afeature%2520matches%2520to%2520operate%2520effectively%252C%2520and%2520it%2520struggles%2520with%2520scenes%250Acharacterized%2520by%2520sparse%2520features%252C%2520large%2520baselines%2520between%2520images%252C%2520or%2520a%2520limited%250Anumber%2520of%2520input%2520images.%2520We%2520aim%2520to%2520tackle%2520few-view%2520NeRF%2520reconstruction%2520using%250Aonly%25203%2520to%25206%2520unposed%2520scene%2520images.%2520Traditional%2520methods%2520often%2520use%2520calibration%250Aboards%2520but%2520they%2520are%2520not%2520common%2520in%2520images.%2520We%2520propose%2520a%2520novel%2520idea%2520of%2520utilizing%250Aeveryday%2520objects%252C%2520commonly%2520found%2520in%2520both%2520images%2520and%2520real%2520life%252C%2520as%2520%2522pose%250Aprobes%2522.%2520The%2520probe%2520object%2520is%2520automatically%2520segmented%2520by%2520SAM%252C%2520whose%2520shape%2520is%250Ainitialized%2520from%2520a%2520cube.%2520We%2520apply%2520a%2520dual-branch%2520volume%2520rendering%2520optimization%250A%2528object%2520NeRF%2520and%2520scene%2520NeRF%2529%2520to%2520constrain%2520the%2520pose%2520optimization%2520and%2520jointly%250Arefine%2520the%2520geometry.%2520Specifically%252C%2520object%2520poses%2520of%2520two%2520views%2520are%2520first%250Aestimated%2520by%2520PnP%2520matching%2520in%2520an%2520SDF%2520representation%252C%2520which%2520serves%2520as%2520initial%250Aposes.%2520PnP%2520matching%252C%2520requiring%2520only%2520a%2520few%2520features%252C%2520is%2520suitable%2520for%250Afeature-sparse%2520scenes.%2520Additional%2520views%2520are%2520incrementally%2520incorporated%2520to%250Arefine%2520poses%2520from%2520preceding%2520views.%2520In%2520experiments%252C%2520PoseProbe%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520pose%2520estimation%2520and%2520novel%2520view%2520synthesis%250Aacross%2520multiple%2520datasets.%2520We%2520demonstrate%2520its%2520effectiveness%252C%2520particularly%2520in%250Afew-view%2520and%2520large-baseline%2520scenes%2520where%2520COLMAP%2520struggles.%2520In%2520ablations%252C%2520using%250Adifferent%2520objects%2520in%2520a%2520scene%2520yields%2520comparable%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generic%20Objects%20as%20Pose%20Probes%20for%20Few-Shot%20View%20Synthesis&entry.906535625=Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Chenyang%20Zhu%20and%20Ke%20Zhuang%20and%20Wei%20Chen%20and%20Kai%20Xu&entry.1292438233=%20%20Radiance%20fields%20including%20NeRFs%20and%203D%20Gaussians%20demonstrate%20great%20potential%0Ain%20high-fidelity%20rendering%20and%20scene%20reconstruction%2C%20while%20they%20require%20a%0Asubstantial%20number%20of%20posed%20images%20as%20inputs.%20COLMAP%20is%20frequently%20employed%20for%0Apreprocessing%20to%20estimate%20poses%2C%20while%20it%20necessitates%20a%20large%20number%20of%0Afeature%20matches%20to%20operate%20effectively%2C%20and%20it%20struggles%20with%20scenes%0Acharacterized%20by%20sparse%20features%2C%20large%20baselines%20between%20images%2C%20or%20a%20limited%0Anumber%20of%20input%20images.%20We%20aim%20to%20tackle%20few-view%20NeRF%20reconstruction%20using%0Aonly%203%20to%206%20unposed%20scene%20images.%20Traditional%20methods%20often%20use%20calibration%0Aboards%20but%20they%20are%20not%20common%20in%20images.%20We%20propose%20a%20novel%20idea%20of%20utilizing%0Aeveryday%20objects%2C%20commonly%20found%20in%20both%20images%20and%20real%20life%2C%20as%20%22pose%0Aprobes%22.%20The%20probe%20object%20is%20automatically%20segmented%20by%20SAM%2C%20whose%20shape%20is%0Ainitialized%20from%20a%20cube.%20We%20apply%20a%20dual-branch%20volume%20rendering%20optimization%0A%28object%20NeRF%20and%20scene%20NeRF%29%20to%20constrain%20the%20pose%20optimization%20and%20jointly%0Arefine%20the%20geometry.%20Specifically%2C%20object%20poses%20of%20two%20views%20are%20first%0Aestimated%20by%20PnP%20matching%20in%20an%20SDF%20representation%2C%20which%20serves%20as%20initial%0Aposes.%20PnP%20matching%2C%20requiring%20only%20a%20few%20features%2C%20is%20suitable%20for%0Afeature-sparse%20scenes.%20Additional%20views%20are%20incrementally%20incorporated%20to%0Arefine%20poses%20from%20preceding%20views.%20In%20experiments%2C%20PoseProbe%20achieves%0Astate-of-the-art%20performance%20in%20both%20pose%20estimation%20and%20novel%20view%20synthesis%0Aacross%20multiple%20datasets.%20We%20demonstrate%20its%20effectiveness%2C%20particularly%20in%0Afew-view%20and%20large-baseline%20scenes%20where%20COLMAP%20struggles.%20In%20ablations%2C%20using%0Adifferent%20objects%20in%20a%20scene%20yields%20comparable%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16690v1&entry.124074799=Read"},
{"title": "SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners", "author": "Ziyu Guo and Renrui Zhang and Xiangyang Zhu and Chengzhuo Tong and Peng Gao and Chunyuan Li and Pheng-Ann Heng", "abstract": "  We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .\n", "link": "http://arxiv.org/abs/2408.16768v1", "date": "2024-08-29", "relevancy": 2.9886, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6027}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6027}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM2Point%3A%20Segment%20Any%203D%20as%20Videos%20in%20Zero-shot%20and%20Promptable%20Manners&body=Title%3A%20SAM2Point%3A%20Segment%20Any%203D%20as%20Videos%20in%20Zero-shot%20and%20Promptable%20Manners%0AAuthor%3A%20Ziyu%20Guo%20and%20Renrui%20Zhang%20and%20Xiangyang%20Zhu%20and%20Chengzhuo%20Tong%20and%20Peng%20Gao%20and%20Chunyuan%20Li%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20%20%20We%20introduce%20SAM2Point%2C%20a%20preliminary%20exploration%20adapting%20Segment%20Anything%0AModel%202%20%28SAM%202%29%20for%20zero-shot%20and%20promptable%203D%20segmentation.%20SAM2Point%0Ainterprets%20any%203D%20data%20as%20a%20series%20of%20multi-directional%20videos%2C%20and%20leverages%0ASAM%202%20for%203D-space%20segmentation%2C%20without%20further%20training%20or%202D-3D%20projection.%0AOur%20framework%20supports%20various%20prompt%20types%2C%20including%203D%20points%2C%20boxes%2C%20and%0Amasks%2C%20and%20can%20generalize%20across%20diverse%20scenarios%2C%20such%20as%203D%20objects%2C%20indoor%0Ascenes%2C%20outdoor%20environments%2C%20and%20raw%20sparse%20LiDAR.%20Demonstrations%20on%20multiple%0A3D%20datasets%2C%20e.g.%2C%20Objaverse%2C%20S3DIS%2C%20ScanNet%2C%20Semantic3D%2C%20and%20KITTI%2C%20highlight%0Athe%20robust%20generalization%20capabilities%20of%20SAM2Point.%20To%20our%20best%20knowledge%2C%20we%0Apresent%20the%20most%20faithful%20implementation%20of%20SAM%20in%203D%2C%20which%20may%20serve%20as%20a%0Astarting%20point%20for%20future%20research%20in%20promptable%203D%20segmentation.%20Online%20Demo%3A%0Ahttps%3A//huggingface.co/spaces/ZiyuG/SAM2Point%20.%20Code%3A%0Ahttps%3A//github.com/ZiyuGuo99/SAM2Point%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM2Point%253A%2520Segment%2520Any%25203D%2520as%2520Videos%2520in%2520Zero-shot%2520and%2520Promptable%2520Manners%26entry.906535625%3DZiyu%2520Guo%2520and%2520Renrui%2520Zhang%2520and%2520Xiangyang%2520Zhu%2520and%2520Chengzhuo%2520Tong%2520and%2520Peng%2520Gao%2520and%2520Chunyuan%2520Li%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3D%2520%2520We%2520introduce%2520SAM2Point%252C%2520a%2520preliminary%2520exploration%2520adapting%2520Segment%2520Anything%250AModel%25202%2520%2528SAM%25202%2529%2520for%2520zero-shot%2520and%2520promptable%25203D%2520segmentation.%2520SAM2Point%250Ainterprets%2520any%25203D%2520data%2520as%2520a%2520series%2520of%2520multi-directional%2520videos%252C%2520and%2520leverages%250ASAM%25202%2520for%25203D-space%2520segmentation%252C%2520without%2520further%2520training%2520or%25202D-3D%2520projection.%250AOur%2520framework%2520supports%2520various%2520prompt%2520types%252C%2520including%25203D%2520points%252C%2520boxes%252C%2520and%250Amasks%252C%2520and%2520can%2520generalize%2520across%2520diverse%2520scenarios%252C%2520such%2520as%25203D%2520objects%252C%2520indoor%250Ascenes%252C%2520outdoor%2520environments%252C%2520and%2520raw%2520sparse%2520LiDAR.%2520Demonstrations%2520on%2520multiple%250A3D%2520datasets%252C%2520e.g.%252C%2520Objaverse%252C%2520S3DIS%252C%2520ScanNet%252C%2520Semantic3D%252C%2520and%2520KITTI%252C%2520highlight%250Athe%2520robust%2520generalization%2520capabilities%2520of%2520SAM2Point.%2520To%2520our%2520best%2520knowledge%252C%2520we%250Apresent%2520the%2520most%2520faithful%2520implementation%2520of%2520SAM%2520in%25203D%252C%2520which%2520may%2520serve%2520as%2520a%250Astarting%2520point%2520for%2520future%2520research%2520in%2520promptable%25203D%2520segmentation.%2520Online%2520Demo%253A%250Ahttps%253A//huggingface.co/spaces/ZiyuG/SAM2Point%2520.%2520Code%253A%250Ahttps%253A//github.com/ZiyuGuo99/SAM2Point%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM2Point%3A%20Segment%20Any%203D%20as%20Videos%20in%20Zero-shot%20and%20Promptable%20Manners&entry.906535625=Ziyu%20Guo%20and%20Renrui%20Zhang%20and%20Xiangyang%20Zhu%20and%20Chengzhuo%20Tong%20and%20Peng%20Gao%20and%20Chunyuan%20Li%20and%20Pheng-Ann%20Heng&entry.1292438233=%20%20We%20introduce%20SAM2Point%2C%20a%20preliminary%20exploration%20adapting%20Segment%20Anything%0AModel%202%20%28SAM%202%29%20for%20zero-shot%20and%20promptable%203D%20segmentation.%20SAM2Point%0Ainterprets%20any%203D%20data%20as%20a%20series%20of%20multi-directional%20videos%2C%20and%20leverages%0ASAM%202%20for%203D-space%20segmentation%2C%20without%20further%20training%20or%202D-3D%20projection.%0AOur%20framework%20supports%20various%20prompt%20types%2C%20including%203D%20points%2C%20boxes%2C%20and%0Amasks%2C%20and%20can%20generalize%20across%20diverse%20scenarios%2C%20such%20as%203D%20objects%2C%20indoor%0Ascenes%2C%20outdoor%20environments%2C%20and%20raw%20sparse%20LiDAR.%20Demonstrations%20on%20multiple%0A3D%20datasets%2C%20e.g.%2C%20Objaverse%2C%20S3DIS%2C%20ScanNet%2C%20Semantic3D%2C%20and%20KITTI%2C%20highlight%0Athe%20robust%20generalization%20capabilities%20of%20SAM2Point.%20To%20our%20best%20knowledge%2C%20we%0Apresent%20the%20most%20faithful%20implementation%20of%20SAM%20in%203D%2C%20which%20may%20serve%20as%20a%0Astarting%20point%20for%20future%20research%20in%20promptable%203D%20segmentation.%20Online%20Demo%3A%0Ahttps%3A//huggingface.co/spaces/ZiyuG/SAM2Point%20.%20Code%3A%0Ahttps%3A//github.com/ZiyuGuo99/SAM2Point%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16768v1&entry.124074799=Read"},
{"title": "OmniRe: Omni Urban Scene Reconstruction", "author": "Ziyu Chen and Jiawei Yang and Jiahui Huang and Riccardo de Lutio and Janick Martinez Esturo and Boris Ivanovic and Or Litany and Zan Gojcic and Sanja Fidler and Marco Pavone and Li Song and Yue Wang", "abstract": "  We introduce OmniRe, a holistic approach for efficiently reconstructing\nhigh-fidelity dynamic urban scenes from on-device logs. Recent methods for\nmodeling driving sequences using neural radiance fields or Gaussian Splatting\nhave demonstrated the potential of reconstructing challenging dynamic scenes,\nbut often overlook pedestrians and other non-vehicle dynamic actors, hindering\na complete pipeline for dynamic urban scene reconstruction. To that end, we\npropose a comprehensive 3DGS framework for driving scenes, named OmniRe, that\nallows for accurate, full-length reconstruction of diverse dynamic objects in a\ndriving log. OmniRe builds dynamic neural scene graphs based on Gaussian\nrepresentations and constructs multiple local canonical spaces that model\nvarious dynamic actors, including vehicles, pedestrians, and cyclists, among\nmany others. This capability is unmatched by existing methods. OmniRe allows us\nto holistically reconstruct different objects present in the scene,\nsubsequently enabling the simulation of reconstructed scenarios with all actors\nparticipating in real-time (~60Hz). Extensive evaluations on the Waymo dataset\nshow that our approach outperforms prior state-of-the-art methods\nquantitatively and qualitatively by a large margin. We believe our work fills a\ncritical gap in driving reconstruction.\n", "link": "http://arxiv.org/abs/2408.16760v1", "date": "2024-08-29", "relevancy": 2.9572, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6182}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5956}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniRe%3A%20Omni%20Urban%20Scene%20Reconstruction&body=Title%3A%20OmniRe%3A%20Omni%20Urban%20Scene%20Reconstruction%0AAuthor%3A%20Ziyu%20Chen%20and%20Jiawei%20Yang%20and%20Jiahui%20Huang%20and%20Riccardo%20de%20Lutio%20and%20Janick%20Martinez%20Esturo%20and%20Boris%20Ivanovic%20and%20Or%20Litany%20and%20Zan%20Gojcic%20and%20Sanja%20Fidler%20and%20Marco%20Pavone%20and%20Li%20Song%20and%20Yue%20Wang%0AAbstract%3A%20%20%20We%20introduce%20OmniRe%2C%20a%20holistic%20approach%20for%20efficiently%20reconstructing%0Ahigh-fidelity%20dynamic%20urban%20scenes%20from%20on-device%20logs.%20Recent%20methods%20for%0Amodeling%20driving%20sequences%20using%20neural%20radiance%20fields%20or%20Gaussian%20Splatting%0Ahave%20demonstrated%20the%20potential%20of%20reconstructing%20challenging%20dynamic%20scenes%2C%0Abut%20often%20overlook%20pedestrians%20and%20other%20non-vehicle%20dynamic%20actors%2C%20hindering%0Aa%20complete%20pipeline%20for%20dynamic%20urban%20scene%20reconstruction.%20To%20that%20end%2C%20we%0Apropose%20a%20comprehensive%203DGS%20framework%20for%20driving%20scenes%2C%20named%20OmniRe%2C%20that%0Aallows%20for%20accurate%2C%20full-length%20reconstruction%20of%20diverse%20dynamic%20objects%20in%20a%0Adriving%20log.%20OmniRe%20builds%20dynamic%20neural%20scene%20graphs%20based%20on%20Gaussian%0Arepresentations%20and%20constructs%20multiple%20local%20canonical%20spaces%20that%20model%0Avarious%20dynamic%20actors%2C%20including%20vehicles%2C%20pedestrians%2C%20and%20cyclists%2C%20among%0Amany%20others.%20This%20capability%20is%20unmatched%20by%20existing%20methods.%20OmniRe%20allows%20us%0Ato%20holistically%20reconstruct%20different%20objects%20present%20in%20the%20scene%2C%0Asubsequently%20enabling%20the%20simulation%20of%20reconstructed%20scenarios%20with%20all%20actors%0Aparticipating%20in%20real-time%20%28~60Hz%29.%20Extensive%20evaluations%20on%20the%20Waymo%20dataset%0Ashow%20that%20our%20approach%20outperforms%20prior%20state-of-the-art%20methods%0Aquantitatively%20and%20qualitatively%20by%20a%20large%20margin.%20We%20believe%20our%20work%20fills%20a%0Acritical%20gap%20in%20driving%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniRe%253A%2520Omni%2520Urban%2520Scene%2520Reconstruction%26entry.906535625%3DZiyu%2520Chen%2520and%2520Jiawei%2520Yang%2520and%2520Jiahui%2520Huang%2520and%2520Riccardo%2520de%2520Lutio%2520and%2520Janick%2520Martinez%2520Esturo%2520and%2520Boris%2520Ivanovic%2520and%2520Or%2520Litany%2520and%2520Zan%2520Gojcic%2520and%2520Sanja%2520Fidler%2520and%2520Marco%2520Pavone%2520and%2520Li%2520Song%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520OmniRe%252C%2520a%2520holistic%2520approach%2520for%2520efficiently%2520reconstructing%250Ahigh-fidelity%2520dynamic%2520urban%2520scenes%2520from%2520on-device%2520logs.%2520Recent%2520methods%2520for%250Amodeling%2520driving%2520sequences%2520using%2520neural%2520radiance%2520fields%2520or%2520Gaussian%2520Splatting%250Ahave%2520demonstrated%2520the%2520potential%2520of%2520reconstructing%2520challenging%2520dynamic%2520scenes%252C%250Abut%2520often%2520overlook%2520pedestrians%2520and%2520other%2520non-vehicle%2520dynamic%2520actors%252C%2520hindering%250Aa%2520complete%2520pipeline%2520for%2520dynamic%2520urban%2520scene%2520reconstruction.%2520To%2520that%2520end%252C%2520we%250Apropose%2520a%2520comprehensive%25203DGS%2520framework%2520for%2520driving%2520scenes%252C%2520named%2520OmniRe%252C%2520that%250Aallows%2520for%2520accurate%252C%2520full-length%2520reconstruction%2520of%2520diverse%2520dynamic%2520objects%2520in%2520a%250Adriving%2520log.%2520OmniRe%2520builds%2520dynamic%2520neural%2520scene%2520graphs%2520based%2520on%2520Gaussian%250Arepresentations%2520and%2520constructs%2520multiple%2520local%2520canonical%2520spaces%2520that%2520model%250Avarious%2520dynamic%2520actors%252C%2520including%2520vehicles%252C%2520pedestrians%252C%2520and%2520cyclists%252C%2520among%250Amany%2520others.%2520This%2520capability%2520is%2520unmatched%2520by%2520existing%2520methods.%2520OmniRe%2520allows%2520us%250Ato%2520holistically%2520reconstruct%2520different%2520objects%2520present%2520in%2520the%2520scene%252C%250Asubsequently%2520enabling%2520the%2520simulation%2520of%2520reconstructed%2520scenarios%2520with%2520all%2520actors%250Aparticipating%2520in%2520real-time%2520%2528~60Hz%2529.%2520Extensive%2520evaluations%2520on%2520the%2520Waymo%2520dataset%250Ashow%2520that%2520our%2520approach%2520outperforms%2520prior%2520state-of-the-art%2520methods%250Aquantitatively%2520and%2520qualitatively%2520by%2520a%2520large%2520margin.%2520We%2520believe%2520our%2520work%2520fills%2520a%250Acritical%2520gap%2520in%2520driving%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniRe%3A%20Omni%20Urban%20Scene%20Reconstruction&entry.906535625=Ziyu%20Chen%20and%20Jiawei%20Yang%20and%20Jiahui%20Huang%20and%20Riccardo%20de%20Lutio%20and%20Janick%20Martinez%20Esturo%20and%20Boris%20Ivanovic%20and%20Or%20Litany%20and%20Zan%20Gojcic%20and%20Sanja%20Fidler%20and%20Marco%20Pavone%20and%20Li%20Song%20and%20Yue%20Wang&entry.1292438233=%20%20We%20introduce%20OmniRe%2C%20a%20holistic%20approach%20for%20efficiently%20reconstructing%0Ahigh-fidelity%20dynamic%20urban%20scenes%20from%20on-device%20logs.%20Recent%20methods%20for%0Amodeling%20driving%20sequences%20using%20neural%20radiance%20fields%20or%20Gaussian%20Splatting%0Ahave%20demonstrated%20the%20potential%20of%20reconstructing%20challenging%20dynamic%20scenes%2C%0Abut%20often%20overlook%20pedestrians%20and%20other%20non-vehicle%20dynamic%20actors%2C%20hindering%0Aa%20complete%20pipeline%20for%20dynamic%20urban%20scene%20reconstruction.%20To%20that%20end%2C%20we%0Apropose%20a%20comprehensive%203DGS%20framework%20for%20driving%20scenes%2C%20named%20OmniRe%2C%20that%0Aallows%20for%20accurate%2C%20full-length%20reconstruction%20of%20diverse%20dynamic%20objects%20in%20a%0Adriving%20log.%20OmniRe%20builds%20dynamic%20neural%20scene%20graphs%20based%20on%20Gaussian%0Arepresentations%20and%20constructs%20multiple%20local%20canonical%20spaces%20that%20model%0Avarious%20dynamic%20actors%2C%20including%20vehicles%2C%20pedestrians%2C%20and%20cyclists%2C%20among%0Amany%20others.%20This%20capability%20is%20unmatched%20by%20existing%20methods.%20OmniRe%20allows%20us%0Ato%20holistically%20reconstruct%20different%20objects%20present%20in%20the%20scene%2C%0Asubsequently%20enabling%20the%20simulation%20of%20reconstructed%20scenarios%20with%20all%20actors%0Aparticipating%20in%20real-time%20%28~60Hz%29.%20Extensive%20evaluations%20on%20the%20Waymo%20dataset%0Ashow%20that%20our%20approach%20outperforms%20prior%20state-of-the-art%20methods%0Aquantitatively%20and%20qualitatively%20by%20a%20large%20margin.%20We%20believe%20our%20work%20fills%20a%0Acritical%20gap%20in%20driving%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16760v1&entry.124074799=Read"},
{"title": "Fast Text-to-3D-Aware Face Generation and Manipulation via Direct\n  Cross-modal Mapping and Geometric Regularization", "author": "Jinlu Zhang and Yiyi Zhou and Qiancheng Zheng and Xiaoxiong Du and Gen Luo and Jun Peng and Xiaoshuai Sun and Rongrong Ji", "abstract": "  Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging\nresearch hot spot in machine learning, which still suffers from low efficiency\nand poor quality. In this paper, we propose an End-to-End Efficient and\nEffective network for fast and accurate T3D face generation and manipulation,\ntermed $E^3$-FaceNet. Different from existing complex generation paradigms,\n$E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware\nvisual space. We introduce a novel Style Code Enhancer to enhance cross-modal\nsemantic alignment, alongside an innovative Geometric Regularization objective\nto maintain consistency across multi-view generations. Extensive experiments on\nthree benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve\npicture-like 3D face generation and manipulation, but also improve inference\nspeed by orders of magnitudes. For instance, compared with Latent3D,\n$E^3$-FaceNet speeds up the five-view generations by almost 470 times, while\nstill exceeding in generation quality. Our code is released at\nhttps://github.com/Aria-Zhangjl/E3-FaceNet.\n", "link": "http://arxiv.org/abs/2403.06702v3", "date": "2024-08-29", "relevancy": 2.9471, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5975}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5854}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Text-to-3D-Aware%20Face%20Generation%20and%20Manipulation%20via%20Direct%0A%20%20Cross-modal%20Mapping%20and%20Geometric%20Regularization&body=Title%3A%20Fast%20Text-to-3D-Aware%20Face%20Generation%20and%20Manipulation%20via%20Direct%0A%20%20Cross-modal%20Mapping%20and%20Geometric%20Regularization%0AAuthor%3A%20Jinlu%20Zhang%20and%20Yiyi%20Zhou%20and%20Qiancheng%20Zheng%20and%20Xiaoxiong%20Du%20and%20Gen%20Luo%20and%20Jun%20Peng%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Text-to-3D-aware%20face%20%28T3D%20Face%29%20generation%20and%20manipulation%20is%20an%20emerging%0Aresearch%20hot%20spot%20in%20machine%20learning%2C%20which%20still%20suffers%20from%20low%20efficiency%0Aand%20poor%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20End-to-End%20Efficient%20and%0AEffective%20network%20for%20fast%20and%20accurate%20T3D%20face%20generation%20and%20manipulation%2C%0Atermed%20%24E%5E3%24-FaceNet.%20Different%20from%20existing%20complex%20generation%20paradigms%2C%0A%24E%5E3%24-FaceNet%20resorts%20to%20a%20direct%20mapping%20from%20text%20instructions%20to%203D-aware%0Avisual%20space.%20We%20introduce%20a%20novel%20Style%20Code%20Enhancer%20to%20enhance%20cross-modal%0Asemantic%20alignment%2C%20alongside%20an%20innovative%20Geometric%20Regularization%20objective%0Ato%20maintain%20consistency%20across%20multi-view%20generations.%20Extensive%20experiments%20on%0Athree%20benchmark%20datasets%20demonstrate%20that%20%24E%5E3%24-FaceNet%20can%20not%20only%20achieve%0Apicture-like%203D%20face%20generation%20and%20manipulation%2C%20but%20also%20improve%20inference%0Aspeed%20by%20orders%20of%20magnitudes.%20For%20instance%2C%20compared%20with%20Latent3D%2C%0A%24E%5E3%24-FaceNet%20speeds%20up%20the%20five-view%20generations%20by%20almost%20470%20times%2C%20while%0Astill%20exceeding%20in%20generation%20quality.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/Aria-Zhangjl/E3-FaceNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06702v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Text-to-3D-Aware%2520Face%2520Generation%2520and%2520Manipulation%2520via%2520Direct%250A%2520%2520Cross-modal%2520Mapping%2520and%2520Geometric%2520Regularization%26entry.906535625%3DJinlu%2520Zhang%2520and%2520Yiyi%2520Zhou%2520and%2520Qiancheng%2520Zheng%2520and%2520Xiaoxiong%2520Du%2520and%2520Gen%2520Luo%2520and%2520Jun%2520Peng%2520and%2520Xiaoshuai%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Text-to-3D-aware%2520face%2520%2528T3D%2520Face%2529%2520generation%2520and%2520manipulation%2520is%2520an%2520emerging%250Aresearch%2520hot%2520spot%2520in%2520machine%2520learning%252C%2520which%2520still%2520suffers%2520from%2520low%2520efficiency%250Aand%2520poor%2520quality.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520End-to-End%2520Efficient%2520and%250AEffective%2520network%2520for%2520fast%2520and%2520accurate%2520T3D%2520face%2520generation%2520and%2520manipulation%252C%250Atermed%2520%2524E%255E3%2524-FaceNet.%2520Different%2520from%2520existing%2520complex%2520generation%2520paradigms%252C%250A%2524E%255E3%2524-FaceNet%2520resorts%2520to%2520a%2520direct%2520mapping%2520from%2520text%2520instructions%2520to%25203D-aware%250Avisual%2520space.%2520We%2520introduce%2520a%2520novel%2520Style%2520Code%2520Enhancer%2520to%2520enhance%2520cross-modal%250Asemantic%2520alignment%252C%2520alongside%2520an%2520innovative%2520Geometric%2520Regularization%2520objective%250Ato%2520maintain%2520consistency%2520across%2520multi-view%2520generations.%2520Extensive%2520experiments%2520on%250Athree%2520benchmark%2520datasets%2520demonstrate%2520that%2520%2524E%255E3%2524-FaceNet%2520can%2520not%2520only%2520achieve%250Apicture-like%25203D%2520face%2520generation%2520and%2520manipulation%252C%2520but%2520also%2520improve%2520inference%250Aspeed%2520by%2520orders%2520of%2520magnitudes.%2520For%2520instance%252C%2520compared%2520with%2520Latent3D%252C%250A%2524E%255E3%2524-FaceNet%2520speeds%2520up%2520the%2520five-view%2520generations%2520by%2520almost%2520470%2520times%252C%2520while%250Astill%2520exceeding%2520in%2520generation%2520quality.%2520Our%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/Aria-Zhangjl/E3-FaceNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06702v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Text-to-3D-Aware%20Face%20Generation%20and%20Manipulation%20via%20Direct%0A%20%20Cross-modal%20Mapping%20and%20Geometric%20Regularization&entry.906535625=Jinlu%20Zhang%20and%20Yiyi%20Zhou%20and%20Qiancheng%20Zheng%20and%20Xiaoxiong%20Du%20and%20Gen%20Luo%20and%20Jun%20Peng%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20Text-to-3D-aware%20face%20%28T3D%20Face%29%20generation%20and%20manipulation%20is%20an%20emerging%0Aresearch%20hot%20spot%20in%20machine%20learning%2C%20which%20still%20suffers%20from%20low%20efficiency%0Aand%20poor%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20End-to-End%20Efficient%20and%0AEffective%20network%20for%20fast%20and%20accurate%20T3D%20face%20generation%20and%20manipulation%2C%0Atermed%20%24E%5E3%24-FaceNet.%20Different%20from%20existing%20complex%20generation%20paradigms%2C%0A%24E%5E3%24-FaceNet%20resorts%20to%20a%20direct%20mapping%20from%20text%20instructions%20to%203D-aware%0Avisual%20space.%20We%20introduce%20a%20novel%20Style%20Code%20Enhancer%20to%20enhance%20cross-modal%0Asemantic%20alignment%2C%20alongside%20an%20innovative%20Geometric%20Regularization%20objective%0Ato%20maintain%20consistency%20across%20multi-view%20generations.%20Extensive%20experiments%20on%0Athree%20benchmark%20datasets%20demonstrate%20that%20%24E%5E3%24-FaceNet%20can%20not%20only%20achieve%0Apicture-like%203D%20face%20generation%20and%20manipulation%2C%20but%20also%20improve%20inference%0Aspeed%20by%20orders%20of%20magnitudes.%20For%20instance%2C%20compared%20with%20Latent3D%2C%0A%24E%5E3%24-FaceNet%20speeds%20up%20the%20five-view%20generations%20by%20almost%20470%20times%2C%20while%0Astill%20exceeding%20in%20generation%20quality.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/Aria-Zhangjl/E3-FaceNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06702v3&entry.124074799=Read"},
{"title": "Multi-source Domain Adaptation for Panoramic Semantic Segmentation", "author": "Jing Jiang and Sicheng Zhao and Jiankun Zhu and Wenbo Tang and Zhaopan Xu and Jidong Yang and Pengfei Xu and Hongxun Yao", "abstract": "  Panoramic semantic segmentation has received widespread attention recently\ndue to its comprehensive 360\\degree field of view. However, labeling such\nimages demands greater resources compared to pinhole images. As a result, many\nunsupervised domain adaptation methods for panoramic semantic segmentation have\nemerged, utilizing real pinhole images or low-cost synthetic panoramic images.\nBut, the segmentation model lacks understanding of the panoramic structure when\nonly utilizing real pinhole images, and it lacks perception of real-world\nscenes when only adopting synthetic panoramic images. Therefore, in this paper,\nwe propose a new task of multi-source domain adaptation for panoramic semantic\nsegmentation, aiming to utilize both real pinhole and synthetic panoramic\nimages in the source domains, enabling the segmentation model to perform well\non unlabeled real panoramic images in the target domain. Further, we propose\nDeformation Transform Aligner for Panoramic Semantic Segmentation (DTA4PASS),\nwhich converts all pinhole images in the source domains into panoramic-like\nimages, and then aligns the converted source domains with the target domain.\nSpecifically, DTA4PASS consists of two main components: Unpaired Semantic\nMorphing (USM) and Distortion Gating Alignment (DGA). Firstly, in USM, the\nSemantic Dual-view Discriminator (SDD) assists in training the diffeomorphic\ndeformation network, enabling the effective transformation of pinhole images\nwithout paired panoramic views. Secondly, DGA assigns pinhole-like and\npanoramic-like features to each image by gating, and aligns these two features\nthrough uncertainty estimation. DTA4PASS outperforms the previous\nstate-of-the-art methods by 1.92% and 2.19% on the outdoor and indoor\nmulti-source domain adaptation scenarios, respectively. The source code will be\nreleased.\n", "link": "http://arxiv.org/abs/2408.16469v1", "date": "2024-08-29", "relevancy": 2.9386, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6263}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5708}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-source%20Domain%20Adaptation%20for%20Panoramic%20Semantic%20Segmentation&body=Title%3A%20Multi-source%20Domain%20Adaptation%20for%20Panoramic%20Semantic%20Segmentation%0AAuthor%3A%20Jing%20Jiang%20and%20Sicheng%20Zhao%20and%20Jiankun%20Zhu%20and%20Wenbo%20Tang%20and%20Zhaopan%20Xu%20and%20Jidong%20Yang%20and%20Pengfei%20Xu%20and%20Hongxun%20Yao%0AAbstract%3A%20%20%20Panoramic%20semantic%20segmentation%20has%20received%20widespread%20attention%20recently%0Adue%20to%20its%20comprehensive%20360%5Cdegree%20field%20of%20view.%20However%2C%20labeling%20such%0Aimages%20demands%20greater%20resources%20compared%20to%20pinhole%20images.%20As%20a%20result%2C%20many%0Aunsupervised%20domain%20adaptation%20methods%20for%20panoramic%20semantic%20segmentation%20have%0Aemerged%2C%20utilizing%20real%20pinhole%20images%20or%20low-cost%20synthetic%20panoramic%20images.%0ABut%2C%20the%20segmentation%20model%20lacks%20understanding%20of%20the%20panoramic%20structure%20when%0Aonly%20utilizing%20real%20pinhole%20images%2C%20and%20it%20lacks%20perception%20of%20real-world%0Ascenes%20when%20only%20adopting%20synthetic%20panoramic%20images.%20Therefore%2C%20in%20this%20paper%2C%0Awe%20propose%20a%20new%20task%20of%20multi-source%20domain%20adaptation%20for%20panoramic%20semantic%0Asegmentation%2C%20aiming%20to%20utilize%20both%20real%20pinhole%20and%20synthetic%20panoramic%0Aimages%20in%20the%20source%20domains%2C%20enabling%20the%20segmentation%20model%20to%20perform%20well%0Aon%20unlabeled%20real%20panoramic%20images%20in%20the%20target%20domain.%20Further%2C%20we%20propose%0ADeformation%20Transform%20Aligner%20for%20Panoramic%20Semantic%20Segmentation%20%28DTA4PASS%29%2C%0Awhich%20converts%20all%20pinhole%20images%20in%20the%20source%20domains%20into%20panoramic-like%0Aimages%2C%20and%20then%20aligns%20the%20converted%20source%20domains%20with%20the%20target%20domain.%0ASpecifically%2C%20DTA4PASS%20consists%20of%20two%20main%20components%3A%20Unpaired%20Semantic%0AMorphing%20%28USM%29%20and%20Distortion%20Gating%20Alignment%20%28DGA%29.%20Firstly%2C%20in%20USM%2C%20the%0ASemantic%20Dual-view%20Discriminator%20%28SDD%29%20assists%20in%20training%20the%20diffeomorphic%0Adeformation%20network%2C%20enabling%20the%20effective%20transformation%20of%20pinhole%20images%0Awithout%20paired%20panoramic%20views.%20Secondly%2C%20DGA%20assigns%20pinhole-like%20and%0Apanoramic-like%20features%20to%20each%20image%20by%20gating%2C%20and%20aligns%20these%20two%20features%0Athrough%20uncertainty%20estimation.%20DTA4PASS%20outperforms%20the%20previous%0Astate-of-the-art%20methods%20by%201.92%25%20and%202.19%25%20on%20the%20outdoor%20and%20indoor%0Amulti-source%20domain%20adaptation%20scenarios%2C%20respectively.%20The%20source%20code%20will%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-source%2520Domain%2520Adaptation%2520for%2520Panoramic%2520Semantic%2520Segmentation%26entry.906535625%3DJing%2520Jiang%2520and%2520Sicheng%2520Zhao%2520and%2520Jiankun%2520Zhu%2520and%2520Wenbo%2520Tang%2520and%2520Zhaopan%2520Xu%2520and%2520Jidong%2520Yang%2520and%2520Pengfei%2520Xu%2520and%2520Hongxun%2520Yao%26entry.1292438233%3D%2520%2520Panoramic%2520semantic%2520segmentation%2520has%2520received%2520widespread%2520attention%2520recently%250Adue%2520to%2520its%2520comprehensive%2520360%255Cdegree%2520field%2520of%2520view.%2520However%252C%2520labeling%2520such%250Aimages%2520demands%2520greater%2520resources%2520compared%2520to%2520pinhole%2520images.%2520As%2520a%2520result%252C%2520many%250Aunsupervised%2520domain%2520adaptation%2520methods%2520for%2520panoramic%2520semantic%2520segmentation%2520have%250Aemerged%252C%2520utilizing%2520real%2520pinhole%2520images%2520or%2520low-cost%2520synthetic%2520panoramic%2520images.%250ABut%252C%2520the%2520segmentation%2520model%2520lacks%2520understanding%2520of%2520the%2520panoramic%2520structure%2520when%250Aonly%2520utilizing%2520real%2520pinhole%2520images%252C%2520and%2520it%2520lacks%2520perception%2520of%2520real-world%250Ascenes%2520when%2520only%2520adopting%2520synthetic%2520panoramic%2520images.%2520Therefore%252C%2520in%2520this%2520paper%252C%250Awe%2520propose%2520a%2520new%2520task%2520of%2520multi-source%2520domain%2520adaptation%2520for%2520panoramic%2520semantic%250Asegmentation%252C%2520aiming%2520to%2520utilize%2520both%2520real%2520pinhole%2520and%2520synthetic%2520panoramic%250Aimages%2520in%2520the%2520source%2520domains%252C%2520enabling%2520the%2520segmentation%2520model%2520to%2520perform%2520well%250Aon%2520unlabeled%2520real%2520panoramic%2520images%2520in%2520the%2520target%2520domain.%2520Further%252C%2520we%2520propose%250ADeformation%2520Transform%2520Aligner%2520for%2520Panoramic%2520Semantic%2520Segmentation%2520%2528DTA4PASS%2529%252C%250Awhich%2520converts%2520all%2520pinhole%2520images%2520in%2520the%2520source%2520domains%2520into%2520panoramic-like%250Aimages%252C%2520and%2520then%2520aligns%2520the%2520converted%2520source%2520domains%2520with%2520the%2520target%2520domain.%250ASpecifically%252C%2520DTA4PASS%2520consists%2520of%2520two%2520main%2520components%253A%2520Unpaired%2520Semantic%250AMorphing%2520%2528USM%2529%2520and%2520Distortion%2520Gating%2520Alignment%2520%2528DGA%2529.%2520Firstly%252C%2520in%2520USM%252C%2520the%250ASemantic%2520Dual-view%2520Discriminator%2520%2528SDD%2529%2520assists%2520in%2520training%2520the%2520diffeomorphic%250Adeformation%2520network%252C%2520enabling%2520the%2520effective%2520transformation%2520of%2520pinhole%2520images%250Awithout%2520paired%2520panoramic%2520views.%2520Secondly%252C%2520DGA%2520assigns%2520pinhole-like%2520and%250Apanoramic-like%2520features%2520to%2520each%2520image%2520by%2520gating%252C%2520and%2520aligns%2520these%2520two%2520features%250Athrough%2520uncertainty%2520estimation.%2520DTA4PASS%2520outperforms%2520the%2520previous%250Astate-of-the-art%2520methods%2520by%25201.92%2525%2520and%25202.19%2525%2520on%2520the%2520outdoor%2520and%2520indoor%250Amulti-source%2520domain%2520adaptation%2520scenarios%252C%2520respectively.%2520The%2520source%2520code%2520will%2520be%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-source%20Domain%20Adaptation%20for%20Panoramic%20Semantic%20Segmentation&entry.906535625=Jing%20Jiang%20and%20Sicheng%20Zhao%20and%20Jiankun%20Zhu%20and%20Wenbo%20Tang%20and%20Zhaopan%20Xu%20and%20Jidong%20Yang%20and%20Pengfei%20Xu%20and%20Hongxun%20Yao&entry.1292438233=%20%20Panoramic%20semantic%20segmentation%20has%20received%20widespread%20attention%20recently%0Adue%20to%20its%20comprehensive%20360%5Cdegree%20field%20of%20view.%20However%2C%20labeling%20such%0Aimages%20demands%20greater%20resources%20compared%20to%20pinhole%20images.%20As%20a%20result%2C%20many%0Aunsupervised%20domain%20adaptation%20methods%20for%20panoramic%20semantic%20segmentation%20have%0Aemerged%2C%20utilizing%20real%20pinhole%20images%20or%20low-cost%20synthetic%20panoramic%20images.%0ABut%2C%20the%20segmentation%20model%20lacks%20understanding%20of%20the%20panoramic%20structure%20when%0Aonly%20utilizing%20real%20pinhole%20images%2C%20and%20it%20lacks%20perception%20of%20real-world%0Ascenes%20when%20only%20adopting%20synthetic%20panoramic%20images.%20Therefore%2C%20in%20this%20paper%2C%0Awe%20propose%20a%20new%20task%20of%20multi-source%20domain%20adaptation%20for%20panoramic%20semantic%0Asegmentation%2C%20aiming%20to%20utilize%20both%20real%20pinhole%20and%20synthetic%20panoramic%0Aimages%20in%20the%20source%20domains%2C%20enabling%20the%20segmentation%20model%20to%20perform%20well%0Aon%20unlabeled%20real%20panoramic%20images%20in%20the%20target%20domain.%20Further%2C%20we%20propose%0ADeformation%20Transform%20Aligner%20for%20Panoramic%20Semantic%20Segmentation%20%28DTA4PASS%29%2C%0Awhich%20converts%20all%20pinhole%20images%20in%20the%20source%20domains%20into%20panoramic-like%0Aimages%2C%20and%20then%20aligns%20the%20converted%20source%20domains%20with%20the%20target%20domain.%0ASpecifically%2C%20DTA4PASS%20consists%20of%20two%20main%20components%3A%20Unpaired%20Semantic%0AMorphing%20%28USM%29%20and%20Distortion%20Gating%20Alignment%20%28DGA%29.%20Firstly%2C%20in%20USM%2C%20the%0ASemantic%20Dual-view%20Discriminator%20%28SDD%29%20assists%20in%20training%20the%20diffeomorphic%0Adeformation%20network%2C%20enabling%20the%20effective%20transformation%20of%20pinhole%20images%0Awithout%20paired%20panoramic%20views.%20Secondly%2C%20DGA%20assigns%20pinhole-like%20and%0Apanoramic-like%20features%20to%20each%20image%20by%20gating%2C%20and%20aligns%20these%20two%20features%0Athrough%20uncertainty%20estimation.%20DTA4PASS%20outperforms%20the%20previous%0Astate-of-the-art%20methods%20by%201.92%25%20and%202.19%25%20on%20the%20outdoor%20and%20indoor%0Amulti-source%20domain%20adaptation%20scenarios%2C%20respectively.%20The%20source%20code%20will%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16469v1&entry.124074799=Read"},
{"title": "GRPose: Learning Graph Relations for Human Image Generation with Pose\n  Priors", "author": "Xiangchen Yin and Donglin Di and Lei Fan and Hao Li and Chen Wei and Xiaofei Gou and Yang Song and Xiao Sun and Xun Yang", "abstract": "  Recent methods using diffusion models have made significant progress in human\nimage generation with various additional controls such as pose priors. However,\nexisting approaches still struggle to generate high-quality images with\nconsistent pose alignment, resulting in unsatisfactory outputs. In this paper,\nwe propose a framework delving into the graph relations of pose priors to\nprovide control information for human image generation. The main idea is to\nestablish a graph topological structure between the pose priors and latent\nrepresentation of diffusion models to capture the intrinsic associations\nbetween different pose parts. A Progressive Graph Integrator (PGI) is designed\nto learn the spatial relationships of the pose priors with the graph structure,\nadopting a hierarchical strategy within an Adapter to gradually propagate\ninformation across different pose parts. A pose perception loss is further\nintroduced based on a pretrained pose estimation network to minimize the pose\ndifferences. Extensive qualitative and quantitative experiments conducted on\nthe Human-Art and LAION-Human datasets demonstrate that our model achieves\nsuperior performance, with a 9.98% increase in pose average precision compared\nto the latest benchmark model. The code is released on *******.\n", "link": "http://arxiv.org/abs/2408.16540v1", "date": "2024-08-29", "relevancy": 2.922, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5977}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5788}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRPose%3A%20Learning%20Graph%20Relations%20for%20Human%20Image%20Generation%20with%20Pose%0A%20%20Priors&body=Title%3A%20GRPose%3A%20Learning%20Graph%20Relations%20for%20Human%20Image%20Generation%20with%20Pose%0A%20%20Priors%0AAuthor%3A%20Xiangchen%20Yin%20and%20Donglin%20Di%20and%20Lei%20Fan%20and%20Hao%20Li%20and%20Chen%20Wei%20and%20Xiaofei%20Gou%20and%20Yang%20Song%20and%20Xiao%20Sun%20and%20Xun%20Yang%0AAbstract%3A%20%20%20Recent%20methods%20using%20diffusion%20models%20have%20made%20significant%20progress%20in%20human%0Aimage%20generation%20with%20various%20additional%20controls%20such%20as%20pose%20priors.%20However%2C%0Aexisting%20approaches%20still%20struggle%20to%20generate%20high-quality%20images%20with%0Aconsistent%20pose%20alignment%2C%20resulting%20in%20unsatisfactory%20outputs.%20In%20this%20paper%2C%0Awe%20propose%20a%20framework%20delving%20into%20the%20graph%20relations%20of%20pose%20priors%20to%0Aprovide%20control%20information%20for%20human%20image%20generation.%20The%20main%20idea%20is%20to%0Aestablish%20a%20graph%20topological%20structure%20between%20the%20pose%20priors%20and%20latent%0Arepresentation%20of%20diffusion%20models%20to%20capture%20the%20intrinsic%20associations%0Abetween%20different%20pose%20parts.%20A%20Progressive%20Graph%20Integrator%20%28PGI%29%20is%20designed%0Ato%20learn%20the%20spatial%20relationships%20of%20the%20pose%20priors%20with%20the%20graph%20structure%2C%0Aadopting%20a%20hierarchical%20strategy%20within%20an%20Adapter%20to%20gradually%20propagate%0Ainformation%20across%20different%20pose%20parts.%20A%20pose%20perception%20loss%20is%20further%0Aintroduced%20based%20on%20a%20pretrained%20pose%20estimation%20network%20to%20minimize%20the%20pose%0Adifferences.%20Extensive%20qualitative%20and%20quantitative%20experiments%20conducted%20on%0Athe%20Human-Art%20and%20LAION-Human%20datasets%20demonstrate%20that%20our%20model%20achieves%0Asuperior%20performance%2C%20with%20a%209.98%25%20increase%20in%20pose%20average%20precision%20compared%0Ato%20the%20latest%20benchmark%20model.%20The%20code%20is%20released%20on%20%2A%2A%2A%2A%2A%2A%2A.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRPose%253A%2520Learning%2520Graph%2520Relations%2520for%2520Human%2520Image%2520Generation%2520with%2520Pose%250A%2520%2520Priors%26entry.906535625%3DXiangchen%2520Yin%2520and%2520Donglin%2520Di%2520and%2520Lei%2520Fan%2520and%2520Hao%2520Li%2520and%2520Chen%2520Wei%2520and%2520Xiaofei%2520Gou%2520and%2520Yang%2520Song%2520and%2520Xiao%2520Sun%2520and%2520Xun%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520methods%2520using%2520diffusion%2520models%2520have%2520made%2520significant%2520progress%2520in%2520human%250Aimage%2520generation%2520with%2520various%2520additional%2520controls%2520such%2520as%2520pose%2520priors.%2520However%252C%250Aexisting%2520approaches%2520still%2520struggle%2520to%2520generate%2520high-quality%2520images%2520with%250Aconsistent%2520pose%2520alignment%252C%2520resulting%2520in%2520unsatisfactory%2520outputs.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520framework%2520delving%2520into%2520the%2520graph%2520relations%2520of%2520pose%2520priors%2520to%250Aprovide%2520control%2520information%2520for%2520human%2520image%2520generation.%2520The%2520main%2520idea%2520is%2520to%250Aestablish%2520a%2520graph%2520topological%2520structure%2520between%2520the%2520pose%2520priors%2520and%2520latent%250Arepresentation%2520of%2520diffusion%2520models%2520to%2520capture%2520the%2520intrinsic%2520associations%250Abetween%2520different%2520pose%2520parts.%2520A%2520Progressive%2520Graph%2520Integrator%2520%2528PGI%2529%2520is%2520designed%250Ato%2520learn%2520the%2520spatial%2520relationships%2520of%2520the%2520pose%2520priors%2520with%2520the%2520graph%2520structure%252C%250Aadopting%2520a%2520hierarchical%2520strategy%2520within%2520an%2520Adapter%2520to%2520gradually%2520propagate%250Ainformation%2520across%2520different%2520pose%2520parts.%2520A%2520pose%2520perception%2520loss%2520is%2520further%250Aintroduced%2520based%2520on%2520a%2520pretrained%2520pose%2520estimation%2520network%2520to%2520minimize%2520the%2520pose%250Adifferences.%2520Extensive%2520qualitative%2520and%2520quantitative%2520experiments%2520conducted%2520on%250Athe%2520Human-Art%2520and%2520LAION-Human%2520datasets%2520demonstrate%2520that%2520our%2520model%2520achieves%250Asuperior%2520performance%252C%2520with%2520a%25209.98%2525%2520increase%2520in%2520pose%2520average%2520precision%2520compared%250Ato%2520the%2520latest%2520benchmark%2520model.%2520The%2520code%2520is%2520released%2520on%2520%252A%252A%252A%252A%252A%252A%252A.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRPose%3A%20Learning%20Graph%20Relations%20for%20Human%20Image%20Generation%20with%20Pose%0A%20%20Priors&entry.906535625=Xiangchen%20Yin%20and%20Donglin%20Di%20and%20Lei%20Fan%20and%20Hao%20Li%20and%20Chen%20Wei%20and%20Xiaofei%20Gou%20and%20Yang%20Song%20and%20Xiao%20Sun%20and%20Xun%20Yang&entry.1292438233=%20%20Recent%20methods%20using%20diffusion%20models%20have%20made%20significant%20progress%20in%20human%0Aimage%20generation%20with%20various%20additional%20controls%20such%20as%20pose%20priors.%20However%2C%0Aexisting%20approaches%20still%20struggle%20to%20generate%20high-quality%20images%20with%0Aconsistent%20pose%20alignment%2C%20resulting%20in%20unsatisfactory%20outputs.%20In%20this%20paper%2C%0Awe%20propose%20a%20framework%20delving%20into%20the%20graph%20relations%20of%20pose%20priors%20to%0Aprovide%20control%20information%20for%20human%20image%20generation.%20The%20main%20idea%20is%20to%0Aestablish%20a%20graph%20topological%20structure%20between%20the%20pose%20priors%20and%20latent%0Arepresentation%20of%20diffusion%20models%20to%20capture%20the%20intrinsic%20associations%0Abetween%20different%20pose%20parts.%20A%20Progressive%20Graph%20Integrator%20%28PGI%29%20is%20designed%0Ato%20learn%20the%20spatial%20relationships%20of%20the%20pose%20priors%20with%20the%20graph%20structure%2C%0Aadopting%20a%20hierarchical%20strategy%20within%20an%20Adapter%20to%20gradually%20propagate%0Ainformation%20across%20different%20pose%20parts.%20A%20pose%20perception%20loss%20is%20further%0Aintroduced%20based%20on%20a%20pretrained%20pose%20estimation%20network%20to%20minimize%20the%20pose%0Adifferences.%20Extensive%20qualitative%20and%20quantitative%20experiments%20conducted%20on%0Athe%20Human-Art%20and%20LAION-Human%20datasets%20demonstrate%20that%20our%20model%20achieves%0Asuperior%20performance%2C%20with%20a%209.98%25%20increase%20in%20pose%20average%20precision%20compared%0Ato%20the%20latest%20benchmark%20model.%20The%20code%20is%20released%20on%20%2A%2A%2A%2A%2A%2A%2A.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16540v1&entry.124074799=Read"},
{"title": "Mismatched: Evaluating the Limits of Image Matching Approaches and\n  Benchmarks", "author": "Sierra Bonilla and Chiara Di Vece and Rema Daher and Xinwei Ju and Danail Stoyanov and Francisco Vasconcelos and Sophia Bano", "abstract": "  Three-dimensional (3D) reconstruction from two-dimensional images is an\nactive research field in computer vision, with applications ranging from\nnavigation and object tracking to segmentation and three-dimensional modeling.\nTraditionally, parametric techniques have been employed for this task. However,\nrecent advancements have seen a shift towards learning-based methods. Given the\nrapid pace of research and the frequent introduction of new image matching\nmethods, it is essential to evaluate them. In this paper, we present a\ncomprehensive evaluation of various image matching methods using a\nstructure-from-motion pipeline. We assess the performance of these methods on\nboth in-domain and out-of-domain datasets, identifying key limitations in both\nthe methods and benchmarks. We also investigate the impact of edge detection as\na pre-processing step. Our analysis reveals that image matching for 3D\nreconstruction remains an open challenge, necessitating careful selection and\ntuning of models for specific scenarios, while also highlighting mismatches in\nhow metrics currently represent method performance.\n", "link": "http://arxiv.org/abs/2408.16445v1", "date": "2024-08-29", "relevancy": 2.9144, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6245}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5635}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mismatched%3A%20Evaluating%20the%20Limits%20of%20Image%20Matching%20Approaches%20and%0A%20%20Benchmarks&body=Title%3A%20Mismatched%3A%20Evaluating%20the%20Limits%20of%20Image%20Matching%20Approaches%20and%0A%20%20Benchmarks%0AAuthor%3A%20Sierra%20Bonilla%20and%20Chiara%20Di%20Vece%20and%20Rema%20Daher%20and%20Xinwei%20Ju%20and%20Danail%20Stoyanov%20and%20Francisco%20Vasconcelos%20and%20Sophia%20Bano%0AAbstract%3A%20%20%20Three-dimensional%20%283D%29%20reconstruction%20from%20two-dimensional%20images%20is%20an%0Aactive%20research%20field%20in%20computer%20vision%2C%20with%20applications%20ranging%20from%0Anavigation%20and%20object%20tracking%20to%20segmentation%20and%20three-dimensional%20modeling.%0ATraditionally%2C%20parametric%20techniques%20have%20been%20employed%20for%20this%20task.%20However%2C%0Arecent%20advancements%20have%20seen%20a%20shift%20towards%20learning-based%20methods.%20Given%20the%0Arapid%20pace%20of%20research%20and%20the%20frequent%20introduction%20of%20new%20image%20matching%0Amethods%2C%20it%20is%20essential%20to%20evaluate%20them.%20In%20this%20paper%2C%20we%20present%20a%0Acomprehensive%20evaluation%20of%20various%20image%20matching%20methods%20using%20a%0Astructure-from-motion%20pipeline.%20We%20assess%20the%20performance%20of%20these%20methods%20on%0Aboth%20in-domain%20and%20out-of-domain%20datasets%2C%20identifying%20key%20limitations%20in%20both%0Athe%20methods%20and%20benchmarks.%20We%20also%20investigate%20the%20impact%20of%20edge%20detection%20as%0Aa%20pre-processing%20step.%20Our%20analysis%20reveals%20that%20image%20matching%20for%203D%0Areconstruction%20remains%20an%20open%20challenge%2C%20necessitating%20careful%20selection%20and%0Atuning%20of%20models%20for%20specific%20scenarios%2C%20while%20also%20highlighting%20mismatches%20in%0Ahow%20metrics%20currently%20represent%20method%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMismatched%253A%2520Evaluating%2520the%2520Limits%2520of%2520Image%2520Matching%2520Approaches%2520and%250A%2520%2520Benchmarks%26entry.906535625%3DSierra%2520Bonilla%2520and%2520Chiara%2520Di%2520Vece%2520and%2520Rema%2520Daher%2520and%2520Xinwei%2520Ju%2520and%2520Danail%2520Stoyanov%2520and%2520Francisco%2520Vasconcelos%2520and%2520Sophia%2520Bano%26entry.1292438233%3D%2520%2520Three-dimensional%2520%25283D%2529%2520reconstruction%2520from%2520two-dimensional%2520images%2520is%2520an%250Aactive%2520research%2520field%2520in%2520computer%2520vision%252C%2520with%2520applications%2520ranging%2520from%250Anavigation%2520and%2520object%2520tracking%2520to%2520segmentation%2520and%2520three-dimensional%2520modeling.%250ATraditionally%252C%2520parametric%2520techniques%2520have%2520been%2520employed%2520for%2520this%2520task.%2520However%252C%250Arecent%2520advancements%2520have%2520seen%2520a%2520shift%2520towards%2520learning-based%2520methods.%2520Given%2520the%250Arapid%2520pace%2520of%2520research%2520and%2520the%2520frequent%2520introduction%2520of%2520new%2520image%2520matching%250Amethods%252C%2520it%2520is%2520essential%2520to%2520evaluate%2520them.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Acomprehensive%2520evaluation%2520of%2520various%2520image%2520matching%2520methods%2520using%2520a%250Astructure-from-motion%2520pipeline.%2520We%2520assess%2520the%2520performance%2520of%2520these%2520methods%2520on%250Aboth%2520in-domain%2520and%2520out-of-domain%2520datasets%252C%2520identifying%2520key%2520limitations%2520in%2520both%250Athe%2520methods%2520and%2520benchmarks.%2520We%2520also%2520investigate%2520the%2520impact%2520of%2520edge%2520detection%2520as%250Aa%2520pre-processing%2520step.%2520Our%2520analysis%2520reveals%2520that%2520image%2520matching%2520for%25203D%250Areconstruction%2520remains%2520an%2520open%2520challenge%252C%2520necessitating%2520careful%2520selection%2520and%250Atuning%2520of%2520models%2520for%2520specific%2520scenarios%252C%2520while%2520also%2520highlighting%2520mismatches%2520in%250Ahow%2520metrics%2520currently%2520represent%2520method%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mismatched%3A%20Evaluating%20the%20Limits%20of%20Image%20Matching%20Approaches%20and%0A%20%20Benchmarks&entry.906535625=Sierra%20Bonilla%20and%20Chiara%20Di%20Vece%20and%20Rema%20Daher%20and%20Xinwei%20Ju%20and%20Danail%20Stoyanov%20and%20Francisco%20Vasconcelos%20and%20Sophia%20Bano&entry.1292438233=%20%20Three-dimensional%20%283D%29%20reconstruction%20from%20two-dimensional%20images%20is%20an%0Aactive%20research%20field%20in%20computer%20vision%2C%20with%20applications%20ranging%20from%0Anavigation%20and%20object%20tracking%20to%20segmentation%20and%20three-dimensional%20modeling.%0ATraditionally%2C%20parametric%20techniques%20have%20been%20employed%20for%20this%20task.%20However%2C%0Arecent%20advancements%20have%20seen%20a%20shift%20towards%20learning-based%20methods.%20Given%20the%0Arapid%20pace%20of%20research%20and%20the%20frequent%20introduction%20of%20new%20image%20matching%0Amethods%2C%20it%20is%20essential%20to%20evaluate%20them.%20In%20this%20paper%2C%20we%20present%20a%0Acomprehensive%20evaluation%20of%20various%20image%20matching%20methods%20using%20a%0Astructure-from-motion%20pipeline.%20We%20assess%20the%20performance%20of%20these%20methods%20on%0Aboth%20in-domain%20and%20out-of-domain%20datasets%2C%20identifying%20key%20limitations%20in%20both%0Athe%20methods%20and%20benchmarks.%20We%20also%20investigate%20the%20impact%20of%20edge%20detection%20as%0Aa%20pre-processing%20step.%20Our%20analysis%20reveals%20that%20image%20matching%20for%203D%0Areconstruction%20remains%20an%20open%20challenge%2C%20necessitating%20careful%20selection%20and%0Atuning%20of%20models%20for%20specific%20scenarios%2C%20while%20also%20highlighting%20mismatches%20in%0Ahow%20metrics%20currently%20represent%20method%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16445v1&entry.124074799=Read"},
{"title": "DriveGenVLM: Real-world Video Generation for Vision Language Model based\n  Autonomous Driving", "author": "Yongjie Fu and Anmol Jain and Xuan Di and Xu Chen and Zhaobin Mo", "abstract": "  The advancement of autonomous driving technologies necessitates increasingly\nsophisticated methods for understanding and predicting real-world scenarios.\nVision language models (VLMs) are emerging as revolutionary tools with\nsignificant potential to influence autonomous driving. In this paper, we\npropose the DriveGenVLM framework to generate driving videos and use VLMs to\nunderstand them. To achieve this, we employ a video generation framework\ngrounded in denoising diffusion probabilistic models (DDPM) aimed at predicting\nreal-world video sequences. We then explore the adequacy of our generated\nvideos for use in VLMs by employing a pre-trained model known as Efficient\nIn-context Learning on Egocentric Videos (EILEV). The diffusion model is\ntrained with the Waymo open dataset and evaluated using the Fr\\'echet Video\nDistance (FVD) score to ensure the quality and realism of the generated videos.\nCorresponding narrations are provided by EILEV for these generated videos,\nwhich may be beneficial in the autonomous driving domain. These narrations can\nenhance traffic scene understanding, aid in navigation, and improve planning\ncapabilities. The integration of video generation with VLMs in the DriveGenVLM\nframework represents a significant step forward in leveraging advanced AI\nmodels to address complex challenges in autonomous driving.\n", "link": "http://arxiv.org/abs/2408.16647v1", "date": "2024-08-29", "relevancy": 2.8644, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5885}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5819}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveGenVLM%3A%20Real-world%20Video%20Generation%20for%20Vision%20Language%20Model%20based%0A%20%20Autonomous%20Driving&body=Title%3A%20DriveGenVLM%3A%20Real-world%20Video%20Generation%20for%20Vision%20Language%20Model%20based%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Yongjie%20Fu%20and%20Anmol%20Jain%20and%20Xuan%20Di%20and%20Xu%20Chen%20and%20Zhaobin%20Mo%0AAbstract%3A%20%20%20The%20advancement%20of%20autonomous%20driving%20technologies%20necessitates%20increasingly%0Asophisticated%20methods%20for%20understanding%20and%20predicting%20real-world%20scenarios.%0AVision%20language%20models%20%28VLMs%29%20are%20emerging%20as%20revolutionary%20tools%20with%0Asignificant%20potential%20to%20influence%20autonomous%20driving.%20In%20this%20paper%2C%20we%0Apropose%20the%20DriveGenVLM%20framework%20to%20generate%20driving%20videos%20and%20use%20VLMs%20to%0Aunderstand%20them.%20To%20achieve%20this%2C%20we%20employ%20a%20video%20generation%20framework%0Agrounded%20in%20denoising%20diffusion%20probabilistic%20models%20%28DDPM%29%20aimed%20at%20predicting%0Areal-world%20video%20sequences.%20We%20then%20explore%20the%20adequacy%20of%20our%20generated%0Avideos%20for%20use%20in%20VLMs%20by%20employing%20a%20pre-trained%20model%20known%20as%20Efficient%0AIn-context%20Learning%20on%20Egocentric%20Videos%20%28EILEV%29.%20The%20diffusion%20model%20is%0Atrained%20with%20the%20Waymo%20open%20dataset%20and%20evaluated%20using%20the%20Fr%5C%27echet%20Video%0ADistance%20%28FVD%29%20score%20to%20ensure%20the%20quality%20and%20realism%20of%20the%20generated%20videos.%0ACorresponding%20narrations%20are%20provided%20by%20EILEV%20for%20these%20generated%20videos%2C%0Awhich%20may%20be%20beneficial%20in%20the%20autonomous%20driving%20domain.%20These%20narrations%20can%0Aenhance%20traffic%20scene%20understanding%2C%20aid%20in%20navigation%2C%20and%20improve%20planning%0Acapabilities.%20The%20integration%20of%20video%20generation%20with%20VLMs%20in%20the%20DriveGenVLM%0Aframework%20represents%20a%20significant%20step%20forward%20in%20leveraging%20advanced%20AI%0Amodels%20to%20address%20complex%20challenges%20in%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveGenVLM%253A%2520Real-world%2520Video%2520Generation%2520for%2520Vision%2520Language%2520Model%2520based%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DYongjie%2520Fu%2520and%2520Anmol%2520Jain%2520and%2520Xuan%2520Di%2520and%2520Xu%2520Chen%2520and%2520Zhaobin%2520Mo%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520autonomous%2520driving%2520technologies%2520necessitates%2520increasingly%250Asophisticated%2520methods%2520for%2520understanding%2520and%2520predicting%2520real-world%2520scenarios.%250AVision%2520language%2520models%2520%2528VLMs%2529%2520are%2520emerging%2520as%2520revolutionary%2520tools%2520with%250Asignificant%2520potential%2520to%2520influence%2520autonomous%2520driving.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520the%2520DriveGenVLM%2520framework%2520to%2520generate%2520driving%2520videos%2520and%2520use%2520VLMs%2520to%250Aunderstand%2520them.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520a%2520video%2520generation%2520framework%250Agrounded%2520in%2520denoising%2520diffusion%2520probabilistic%2520models%2520%2528DDPM%2529%2520aimed%2520at%2520predicting%250Areal-world%2520video%2520sequences.%2520We%2520then%2520explore%2520the%2520adequacy%2520of%2520our%2520generated%250Avideos%2520for%2520use%2520in%2520VLMs%2520by%2520employing%2520a%2520pre-trained%2520model%2520known%2520as%2520Efficient%250AIn-context%2520Learning%2520on%2520Egocentric%2520Videos%2520%2528EILEV%2529.%2520The%2520diffusion%2520model%2520is%250Atrained%2520with%2520the%2520Waymo%2520open%2520dataset%2520and%2520evaluated%2520using%2520the%2520Fr%255C%2527echet%2520Video%250ADistance%2520%2528FVD%2529%2520score%2520to%2520ensure%2520the%2520quality%2520and%2520realism%2520of%2520the%2520generated%2520videos.%250ACorresponding%2520narrations%2520are%2520provided%2520by%2520EILEV%2520for%2520these%2520generated%2520videos%252C%250Awhich%2520may%2520be%2520beneficial%2520in%2520the%2520autonomous%2520driving%2520domain.%2520These%2520narrations%2520can%250Aenhance%2520traffic%2520scene%2520understanding%252C%2520aid%2520in%2520navigation%252C%2520and%2520improve%2520planning%250Acapabilities.%2520The%2520integration%2520of%2520video%2520generation%2520with%2520VLMs%2520in%2520the%2520DriveGenVLM%250Aframework%2520represents%2520a%2520significant%2520step%2520forward%2520in%2520leveraging%2520advanced%2520AI%250Amodels%2520to%2520address%2520complex%2520challenges%2520in%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveGenVLM%3A%20Real-world%20Video%20Generation%20for%20Vision%20Language%20Model%20based%0A%20%20Autonomous%20Driving&entry.906535625=Yongjie%20Fu%20and%20Anmol%20Jain%20and%20Xuan%20Di%20and%20Xu%20Chen%20and%20Zhaobin%20Mo&entry.1292438233=%20%20The%20advancement%20of%20autonomous%20driving%20technologies%20necessitates%20increasingly%0Asophisticated%20methods%20for%20understanding%20and%20predicting%20real-world%20scenarios.%0AVision%20language%20models%20%28VLMs%29%20are%20emerging%20as%20revolutionary%20tools%20with%0Asignificant%20potential%20to%20influence%20autonomous%20driving.%20In%20this%20paper%2C%20we%0Apropose%20the%20DriveGenVLM%20framework%20to%20generate%20driving%20videos%20and%20use%20VLMs%20to%0Aunderstand%20them.%20To%20achieve%20this%2C%20we%20employ%20a%20video%20generation%20framework%0Agrounded%20in%20denoising%20diffusion%20probabilistic%20models%20%28DDPM%29%20aimed%20at%20predicting%0Areal-world%20video%20sequences.%20We%20then%20explore%20the%20adequacy%20of%20our%20generated%0Avideos%20for%20use%20in%20VLMs%20by%20employing%20a%20pre-trained%20model%20known%20as%20Efficient%0AIn-context%20Learning%20on%20Egocentric%20Videos%20%28EILEV%29.%20The%20diffusion%20model%20is%0Atrained%20with%20the%20Waymo%20open%20dataset%20and%20evaluated%20using%20the%20Fr%5C%27echet%20Video%0ADistance%20%28FVD%29%20score%20to%20ensure%20the%20quality%20and%20realism%20of%20the%20generated%20videos.%0ACorresponding%20narrations%20are%20provided%20by%20EILEV%20for%20these%20generated%20videos%2C%0Awhich%20may%20be%20beneficial%20in%20the%20autonomous%20driving%20domain.%20These%20narrations%20can%0Aenhance%20traffic%20scene%20understanding%2C%20aid%20in%20navigation%2C%20and%20improve%20planning%0Acapabilities.%20The%20integration%20of%20video%20generation%20with%20VLMs%20in%20the%20DriveGenVLM%0Aframework%20represents%20a%20significant%20step%20forward%20in%20leveraging%20advanced%20AI%0Amodels%20to%20address%20complex%20challenges%20in%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16647v1&entry.124074799=Read"},
{"title": "Prediction-Feedback DETR for Temporal Action Detection", "author": "Jihwan Kim and Miso Lee and Cheol-Ho Cho and Jihyun Lee and Jae-Pil Heo", "abstract": "  Temporal Action Detection (TAD) is fundamental yet challenging for real-world\nvideo applications. Leveraging the unique benefits of transformers, various\nDETR-based approaches have been adopted in TAD. However, it has recently been\nidentified that the attention collapse in self-attention causes the performance\ndegradation of DETR for TAD. Building upon previous research, this paper newly\naddresses the attention collapse problem in cross-attention within DETR-based\nTAD methods. Moreover, our findings reveal that cross-attention exhibits\npatterns distinct from predictions, indicating a short-cut phenomenon. To\nresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),\nwhich utilizes predictions to restore the collapse and align the cross- and\nself-attention with predictions. Specifically, we devise novel\nprediction-feedback objectives using guidance from the relations of the\npredictions. As a result, Pred-DETR significantly alleviates the collapse and\nachieves state-of-the-art performance among DETR-based methods on various\nchallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and\nFineAction.\n", "link": "http://arxiv.org/abs/2408.16729v1", "date": "2024-08-29", "relevancy": 2.8419, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction-Feedback%20DETR%20for%20Temporal%20Action%20Detection&body=Title%3A%20Prediction-Feedback%20DETR%20for%20Temporal%20Action%20Detection%0AAuthor%3A%20Jihwan%20Kim%20and%20Miso%20Lee%20and%20Cheol-Ho%20Cho%20and%20Jihyun%20Lee%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20Temporal%20Action%20Detection%20%28TAD%29%20is%20fundamental%20yet%20challenging%20for%20real-world%0Avideo%20applications.%20Leveraging%20the%20unique%20benefits%20of%20transformers%2C%20various%0ADETR-based%20approaches%20have%20been%20adopted%20in%20TAD.%20However%2C%20it%20has%20recently%20been%0Aidentified%20that%20the%20attention%20collapse%20in%20self-attention%20causes%20the%20performance%0Adegradation%20of%20DETR%20for%20TAD.%20Building%20upon%20previous%20research%2C%20this%20paper%20newly%0Aaddresses%20the%20attention%20collapse%20problem%20in%20cross-attention%20within%20DETR-based%0ATAD%20methods.%20Moreover%2C%20our%20findings%20reveal%20that%20cross-attention%20exhibits%0Apatterns%20distinct%20from%20predictions%2C%20indicating%20a%20short-cut%20phenomenon.%20To%0Aresolve%20this%2C%20we%20propose%20a%20new%20framework%2C%20Prediction-Feedback%20DETR%20%28Pred-DETR%29%2C%0Awhich%20utilizes%20predictions%20to%20restore%20the%20collapse%20and%20align%20the%20cross-%20and%0Aself-attention%20with%20predictions.%20Specifically%2C%20we%20devise%20novel%0Aprediction-feedback%20objectives%20using%20guidance%20from%20the%20relations%20of%20the%0Apredictions.%20As%20a%20result%2C%20Pred-DETR%20significantly%20alleviates%20the%20collapse%20and%0Aachieves%20state-of-the-art%20performance%20among%20DETR-based%20methods%20on%20various%0Achallenging%20benchmarks%20including%20THUMOS14%2C%20ActivityNet-v1.3%2C%20HACS%2C%20and%0AFineAction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction-Feedback%2520DETR%2520for%2520Temporal%2520Action%2520Detection%26entry.906535625%3DJihwan%2520Kim%2520and%2520Miso%2520Lee%2520and%2520Cheol-Ho%2520Cho%2520and%2520Jihyun%2520Lee%2520and%2520Jae-Pil%2520Heo%26entry.1292438233%3D%2520%2520Temporal%2520Action%2520Detection%2520%2528TAD%2529%2520is%2520fundamental%2520yet%2520challenging%2520for%2520real-world%250Avideo%2520applications.%2520Leveraging%2520the%2520unique%2520benefits%2520of%2520transformers%252C%2520various%250ADETR-based%2520approaches%2520have%2520been%2520adopted%2520in%2520TAD.%2520However%252C%2520it%2520has%2520recently%2520been%250Aidentified%2520that%2520the%2520attention%2520collapse%2520in%2520self-attention%2520causes%2520the%2520performance%250Adegradation%2520of%2520DETR%2520for%2520TAD.%2520Building%2520upon%2520previous%2520research%252C%2520this%2520paper%2520newly%250Aaddresses%2520the%2520attention%2520collapse%2520problem%2520in%2520cross-attention%2520within%2520DETR-based%250ATAD%2520methods.%2520Moreover%252C%2520our%2520findings%2520reveal%2520that%2520cross-attention%2520exhibits%250Apatterns%2520distinct%2520from%2520predictions%252C%2520indicating%2520a%2520short-cut%2520phenomenon.%2520To%250Aresolve%2520this%252C%2520we%2520propose%2520a%2520new%2520framework%252C%2520Prediction-Feedback%2520DETR%2520%2528Pred-DETR%2529%252C%250Awhich%2520utilizes%2520predictions%2520to%2520restore%2520the%2520collapse%2520and%2520align%2520the%2520cross-%2520and%250Aself-attention%2520with%2520predictions.%2520Specifically%252C%2520we%2520devise%2520novel%250Aprediction-feedback%2520objectives%2520using%2520guidance%2520from%2520the%2520relations%2520of%2520the%250Apredictions.%2520As%2520a%2520result%252C%2520Pred-DETR%2520significantly%2520alleviates%2520the%2520collapse%2520and%250Aachieves%2520state-of-the-art%2520performance%2520among%2520DETR-based%2520methods%2520on%2520various%250Achallenging%2520benchmarks%2520including%2520THUMOS14%252C%2520ActivityNet-v1.3%252C%2520HACS%252C%2520and%250AFineAction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction-Feedback%20DETR%20for%20Temporal%20Action%20Detection&entry.906535625=Jihwan%20Kim%20and%20Miso%20Lee%20and%20Cheol-Ho%20Cho%20and%20Jihyun%20Lee%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20Temporal%20Action%20Detection%20%28TAD%29%20is%20fundamental%20yet%20challenging%20for%20real-world%0Avideo%20applications.%20Leveraging%20the%20unique%20benefits%20of%20transformers%2C%20various%0ADETR-based%20approaches%20have%20been%20adopted%20in%20TAD.%20However%2C%20it%20has%20recently%20been%0Aidentified%20that%20the%20attention%20collapse%20in%20self-attention%20causes%20the%20performance%0Adegradation%20of%20DETR%20for%20TAD.%20Building%20upon%20previous%20research%2C%20this%20paper%20newly%0Aaddresses%20the%20attention%20collapse%20problem%20in%20cross-attention%20within%20DETR-based%0ATAD%20methods.%20Moreover%2C%20our%20findings%20reveal%20that%20cross-attention%20exhibits%0Apatterns%20distinct%20from%20predictions%2C%20indicating%20a%20short-cut%20phenomenon.%20To%0Aresolve%20this%2C%20we%20propose%20a%20new%20framework%2C%20Prediction-Feedback%20DETR%20%28Pred-DETR%29%2C%0Awhich%20utilizes%20predictions%20to%20restore%20the%20collapse%20and%20align%20the%20cross-%20and%0Aself-attention%20with%20predictions.%20Specifically%2C%20we%20devise%20novel%0Aprediction-feedback%20objectives%20using%20guidance%20from%20the%20relations%20of%20the%0Apredictions.%20As%20a%20result%2C%20Pred-DETR%20significantly%20alleviates%20the%20collapse%20and%0Aachieves%20state-of-the-art%20performance%20among%20DETR-based%20methods%20on%20various%0Achallenging%20benchmarks%20including%20THUMOS14%2C%20ActivityNet-v1.3%2C%20HACS%2C%20and%0AFineAction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16729v1&entry.124074799=Read"},
{"title": "Spurfies: Sparse Surface Reconstruction using Local Geometry Priors", "author": "Kevin Raj and Christopher Wewer and Raza Yunus and Eddy Ilg and Jan Eric Lenssen", "abstract": "  We introduce Spurfies, a novel method for sparse-view surface reconstruction\nthat disentangles appearance and geometry information to utilize local geometry\npriors trained on synthetic data. Recent research heavily focuses on 3D\nreconstruction using dense multi-view setups, typically requiring hundreds of\nimages. However, these methods often struggle with few-view scenarios. Existing\nsparse-view reconstruction techniques often rely on multi-view stereo networks\nthat need to learn joint priors for geometry and appearance from a large amount\nof data. In contrast, we introduce a neural point representation that\ndisentangles geometry and appearance to train a local geometry prior using a\nsubset of the synthetic ShapeNet dataset only. During inference, we utilize\nthis surface prior as additional constraint for surface and appearance\nreconstruction from sparse input views via differentiable volume rendering,\nrestricting the space of possible solutions. We validate the effectiveness of\nour method on the DTU dataset and demonstrate that it outperforms previous\nstate of the art by 35% in surface quality while achieving competitive novel\nview synthesis quality. Moreover, in contrast to previous works, our method can\nbe applied to larger, unbounded scenes, such as Mip-NeRF 360.\n", "link": "http://arxiv.org/abs/2408.16544v1", "date": "2024-08-29", "relevancy": 2.8393, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5963}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.56}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spurfies%3A%20Sparse%20Surface%20Reconstruction%20using%20Local%20Geometry%20Priors&body=Title%3A%20Spurfies%3A%20Sparse%20Surface%20Reconstruction%20using%20Local%20Geometry%20Priors%0AAuthor%3A%20Kevin%20Raj%20and%20Christopher%20Wewer%20and%20Raza%20Yunus%20and%20Eddy%20Ilg%20and%20Jan%20Eric%20Lenssen%0AAbstract%3A%20%20%20We%20introduce%20Spurfies%2C%20a%20novel%20method%20for%20sparse-view%20surface%20reconstruction%0Athat%20disentangles%20appearance%20and%20geometry%20information%20to%20utilize%20local%20geometry%0Apriors%20trained%20on%20synthetic%20data.%20Recent%20research%20heavily%20focuses%20on%203D%0Areconstruction%20using%20dense%20multi-view%20setups%2C%20typically%20requiring%20hundreds%20of%0Aimages.%20However%2C%20these%20methods%20often%20struggle%20with%20few-view%20scenarios.%20Existing%0Asparse-view%20reconstruction%20techniques%20often%20rely%20on%20multi-view%20stereo%20networks%0Athat%20need%20to%20learn%20joint%20priors%20for%20geometry%20and%20appearance%20from%20a%20large%20amount%0Aof%20data.%20In%20contrast%2C%20we%20introduce%20a%20neural%20point%20representation%20that%0Adisentangles%20geometry%20and%20appearance%20to%20train%20a%20local%20geometry%20prior%20using%20a%0Asubset%20of%20the%20synthetic%20ShapeNet%20dataset%20only.%20During%20inference%2C%20we%20utilize%0Athis%20surface%20prior%20as%20additional%20constraint%20for%20surface%20and%20appearance%0Areconstruction%20from%20sparse%20input%20views%20via%20differentiable%20volume%20rendering%2C%0Arestricting%20the%20space%20of%20possible%20solutions.%20We%20validate%20the%20effectiveness%20of%0Aour%20method%20on%20the%20DTU%20dataset%20and%20demonstrate%20that%20it%20outperforms%20previous%0Astate%20of%20the%20art%20by%2035%25%20in%20surface%20quality%20while%20achieving%20competitive%20novel%0Aview%20synthesis%20quality.%20Moreover%2C%20in%20contrast%20to%20previous%20works%2C%20our%20method%20can%0Abe%20applied%20to%20larger%2C%20unbounded%20scenes%2C%20such%20as%20Mip-NeRF%20360.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpurfies%253A%2520Sparse%2520Surface%2520Reconstruction%2520using%2520Local%2520Geometry%2520Priors%26entry.906535625%3DKevin%2520Raj%2520and%2520Christopher%2520Wewer%2520and%2520Raza%2520Yunus%2520and%2520Eddy%2520Ilg%2520and%2520Jan%2520Eric%2520Lenssen%26entry.1292438233%3D%2520%2520We%2520introduce%2520Spurfies%252C%2520a%2520novel%2520method%2520for%2520sparse-view%2520surface%2520reconstruction%250Athat%2520disentangles%2520appearance%2520and%2520geometry%2520information%2520to%2520utilize%2520local%2520geometry%250Apriors%2520trained%2520on%2520synthetic%2520data.%2520Recent%2520research%2520heavily%2520focuses%2520on%25203D%250Areconstruction%2520using%2520dense%2520multi-view%2520setups%252C%2520typically%2520requiring%2520hundreds%2520of%250Aimages.%2520However%252C%2520these%2520methods%2520often%2520struggle%2520with%2520few-view%2520scenarios.%2520Existing%250Asparse-view%2520reconstruction%2520techniques%2520often%2520rely%2520on%2520multi-view%2520stereo%2520networks%250Athat%2520need%2520to%2520learn%2520joint%2520priors%2520for%2520geometry%2520and%2520appearance%2520from%2520a%2520large%2520amount%250Aof%2520data.%2520In%2520contrast%252C%2520we%2520introduce%2520a%2520neural%2520point%2520representation%2520that%250Adisentangles%2520geometry%2520and%2520appearance%2520to%2520train%2520a%2520local%2520geometry%2520prior%2520using%2520a%250Asubset%2520of%2520the%2520synthetic%2520ShapeNet%2520dataset%2520only.%2520During%2520inference%252C%2520we%2520utilize%250Athis%2520surface%2520prior%2520as%2520additional%2520constraint%2520for%2520surface%2520and%2520appearance%250Areconstruction%2520from%2520sparse%2520input%2520views%2520via%2520differentiable%2520volume%2520rendering%252C%250Arestricting%2520the%2520space%2520of%2520possible%2520solutions.%2520We%2520validate%2520the%2520effectiveness%2520of%250Aour%2520method%2520on%2520the%2520DTU%2520dataset%2520and%2520demonstrate%2520that%2520it%2520outperforms%2520previous%250Astate%2520of%2520the%2520art%2520by%252035%2525%2520in%2520surface%2520quality%2520while%2520achieving%2520competitive%2520novel%250Aview%2520synthesis%2520quality.%2520Moreover%252C%2520in%2520contrast%2520to%2520previous%2520works%252C%2520our%2520method%2520can%250Abe%2520applied%2520to%2520larger%252C%2520unbounded%2520scenes%252C%2520such%2520as%2520Mip-NeRF%2520360.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spurfies%3A%20Sparse%20Surface%20Reconstruction%20using%20Local%20Geometry%20Priors&entry.906535625=Kevin%20Raj%20and%20Christopher%20Wewer%20and%20Raza%20Yunus%20and%20Eddy%20Ilg%20and%20Jan%20Eric%20Lenssen&entry.1292438233=%20%20We%20introduce%20Spurfies%2C%20a%20novel%20method%20for%20sparse-view%20surface%20reconstruction%0Athat%20disentangles%20appearance%20and%20geometry%20information%20to%20utilize%20local%20geometry%0Apriors%20trained%20on%20synthetic%20data.%20Recent%20research%20heavily%20focuses%20on%203D%0Areconstruction%20using%20dense%20multi-view%20setups%2C%20typically%20requiring%20hundreds%20of%0Aimages.%20However%2C%20these%20methods%20often%20struggle%20with%20few-view%20scenarios.%20Existing%0Asparse-view%20reconstruction%20techniques%20often%20rely%20on%20multi-view%20stereo%20networks%0Athat%20need%20to%20learn%20joint%20priors%20for%20geometry%20and%20appearance%20from%20a%20large%20amount%0Aof%20data.%20In%20contrast%2C%20we%20introduce%20a%20neural%20point%20representation%20that%0Adisentangles%20geometry%20and%20appearance%20to%20train%20a%20local%20geometry%20prior%20using%20a%0Asubset%20of%20the%20synthetic%20ShapeNet%20dataset%20only.%20During%20inference%2C%20we%20utilize%0Athis%20surface%20prior%20as%20additional%20constraint%20for%20surface%20and%20appearance%0Areconstruction%20from%20sparse%20input%20views%20via%20differentiable%20volume%20rendering%2C%0Arestricting%20the%20space%20of%20possible%20solutions.%20We%20validate%20the%20effectiveness%20of%0Aour%20method%20on%20the%20DTU%20dataset%20and%20demonstrate%20that%20it%20outperforms%20previous%0Astate%20of%20the%20art%20by%2035%25%20in%20surface%20quality%20while%20achieving%20competitive%20novel%0Aview%20synthesis%20quality.%20Moreover%2C%20in%20contrast%20to%20previous%20works%2C%20our%20method%20can%0Abe%20applied%20to%20larger%2C%20unbounded%20scenes%2C%20such%20as%20Mip-NeRF%20360.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16544v1&entry.124074799=Read"},
{"title": "Learning to Detect and Segment for Open Vocabulary Object Detection", "author": "Tao Wang and Nan Li", "abstract": "  Open vocabulary object detection has been greatly advanced by the recent\ndevelopment of vision-language pretrained model, which helps recognize novel\nobjects with only semantic categories. The prior works mainly focus on\nknowledge transferring to the object proposal classification and employ\nclass-agnostic box and mask prediction. In this work, we propose CondHead, a\nprincipled dynamic network design to better generalize the box regression and\nmask segmentation for open vocabulary setting. The core idea is to\nconditionally parameterize the network heads on semantic embedding and thus the\nmodel is guided with class-specific knowledge to better detect novel\ncategories. Specifically, CondHead is composed of two streams of network heads,\nthe dynamically aggregated head and the dynamically generated head. The former\nis instantiated with a set of static heads that are conditionally aggregated,\nthese heads are optimized as experts and are expected to learn sophisticated\nprediction. The latter is instantiated with dynamically generated parameters\nand encodes general class-specific information. With such a conditional design,\nthe detection model is bridged by the semantic embedding to offer strongly\ngeneralizable class-wise box and mask prediction. Our method brings significant\nimprovement to the state-of-the-art open vocabulary object detection methods\nwith very minor overhead, e.g., it surpasses a RegionClip model by 3.0\ndetection AP on novel categories, with only 1.1% more computation.\n", "link": "http://arxiv.org/abs/2212.12130v6", "date": "2024-08-29", "relevancy": 2.8087, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Detect%20and%20Segment%20for%20Open%20Vocabulary%20Object%20Detection&body=Title%3A%20Learning%20to%20Detect%20and%20Segment%20for%20Open%20Vocabulary%20Object%20Detection%0AAuthor%3A%20Tao%20Wang%20and%20Nan%20Li%0AAbstract%3A%20%20%20Open%20vocabulary%20object%20detection%20has%20been%20greatly%20advanced%20by%20the%20recent%0Adevelopment%20of%20vision-language%20pretrained%20model%2C%20which%20helps%20recognize%20novel%0Aobjects%20with%20only%20semantic%20categories.%20The%20prior%20works%20mainly%20focus%20on%0Aknowledge%20transferring%20to%20the%20object%20proposal%20classification%20and%20employ%0Aclass-agnostic%20box%20and%20mask%20prediction.%20In%20this%20work%2C%20we%20propose%20CondHead%2C%20a%0Aprincipled%20dynamic%20network%20design%20to%20better%20generalize%20the%20box%20regression%20and%0Amask%20segmentation%20for%20open%20vocabulary%20setting.%20The%20core%20idea%20is%20to%0Aconditionally%20parameterize%20the%20network%20heads%20on%20semantic%20embedding%20and%20thus%20the%0Amodel%20is%20guided%20with%20class-specific%20knowledge%20to%20better%20detect%20novel%0Acategories.%20Specifically%2C%20CondHead%20is%20composed%20of%20two%20streams%20of%20network%20heads%2C%0Athe%20dynamically%20aggregated%20head%20and%20the%20dynamically%20generated%20head.%20The%20former%0Ais%20instantiated%20with%20a%20set%20of%20static%20heads%20that%20are%20conditionally%20aggregated%2C%0Athese%20heads%20are%20optimized%20as%20experts%20and%20are%20expected%20to%20learn%20sophisticated%0Aprediction.%20The%20latter%20is%20instantiated%20with%20dynamically%20generated%20parameters%0Aand%20encodes%20general%20class-specific%20information.%20With%20such%20a%20conditional%20design%2C%0Athe%20detection%20model%20is%20bridged%20by%20the%20semantic%20embedding%20to%20offer%20strongly%0Ageneralizable%20class-wise%20box%20and%20mask%20prediction.%20Our%20method%20brings%20significant%0Aimprovement%20to%20the%20state-of-the-art%20open%20vocabulary%20object%20detection%20methods%0Awith%20very%20minor%20overhead%2C%20e.g.%2C%20it%20surpasses%20a%20RegionClip%20model%20by%203.0%0Adetection%20AP%20on%20novel%20categories%2C%20with%20only%201.1%25%20more%20computation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.12130v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Detect%2520and%2520Segment%2520for%2520Open%2520Vocabulary%2520Object%2520Detection%26entry.906535625%3DTao%2520Wang%2520and%2520Nan%2520Li%26entry.1292438233%3D%2520%2520Open%2520vocabulary%2520object%2520detection%2520has%2520been%2520greatly%2520advanced%2520by%2520the%2520recent%250Adevelopment%2520of%2520vision-language%2520pretrained%2520model%252C%2520which%2520helps%2520recognize%2520novel%250Aobjects%2520with%2520only%2520semantic%2520categories.%2520The%2520prior%2520works%2520mainly%2520focus%2520on%250Aknowledge%2520transferring%2520to%2520the%2520object%2520proposal%2520classification%2520and%2520employ%250Aclass-agnostic%2520box%2520and%2520mask%2520prediction.%2520In%2520this%2520work%252C%2520we%2520propose%2520CondHead%252C%2520a%250Aprincipled%2520dynamic%2520network%2520design%2520to%2520better%2520generalize%2520the%2520box%2520regression%2520and%250Amask%2520segmentation%2520for%2520open%2520vocabulary%2520setting.%2520The%2520core%2520idea%2520is%2520to%250Aconditionally%2520parameterize%2520the%2520network%2520heads%2520on%2520semantic%2520embedding%2520and%2520thus%2520the%250Amodel%2520is%2520guided%2520with%2520class-specific%2520knowledge%2520to%2520better%2520detect%2520novel%250Acategories.%2520Specifically%252C%2520CondHead%2520is%2520composed%2520of%2520two%2520streams%2520of%2520network%2520heads%252C%250Athe%2520dynamically%2520aggregated%2520head%2520and%2520the%2520dynamically%2520generated%2520head.%2520The%2520former%250Ais%2520instantiated%2520with%2520a%2520set%2520of%2520static%2520heads%2520that%2520are%2520conditionally%2520aggregated%252C%250Athese%2520heads%2520are%2520optimized%2520as%2520experts%2520and%2520are%2520expected%2520to%2520learn%2520sophisticated%250Aprediction.%2520The%2520latter%2520is%2520instantiated%2520with%2520dynamically%2520generated%2520parameters%250Aand%2520encodes%2520general%2520class-specific%2520information.%2520With%2520such%2520a%2520conditional%2520design%252C%250Athe%2520detection%2520model%2520is%2520bridged%2520by%2520the%2520semantic%2520embedding%2520to%2520offer%2520strongly%250Ageneralizable%2520class-wise%2520box%2520and%2520mask%2520prediction.%2520Our%2520method%2520brings%2520significant%250Aimprovement%2520to%2520the%2520state-of-the-art%2520open%2520vocabulary%2520object%2520detection%2520methods%250Awith%2520very%2520minor%2520overhead%252C%2520e.g.%252C%2520it%2520surpasses%2520a%2520RegionClip%2520model%2520by%25203.0%250Adetection%2520AP%2520on%2520novel%2520categories%252C%2520with%2520only%25201.1%2525%2520more%2520computation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.12130v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Detect%20and%20Segment%20for%20Open%20Vocabulary%20Object%20Detection&entry.906535625=Tao%20Wang%20and%20Nan%20Li&entry.1292438233=%20%20Open%20vocabulary%20object%20detection%20has%20been%20greatly%20advanced%20by%20the%20recent%0Adevelopment%20of%20vision-language%20pretrained%20model%2C%20which%20helps%20recognize%20novel%0Aobjects%20with%20only%20semantic%20categories.%20The%20prior%20works%20mainly%20focus%20on%0Aknowledge%20transferring%20to%20the%20object%20proposal%20classification%20and%20employ%0Aclass-agnostic%20box%20and%20mask%20prediction.%20In%20this%20work%2C%20we%20propose%20CondHead%2C%20a%0Aprincipled%20dynamic%20network%20design%20to%20better%20generalize%20the%20box%20regression%20and%0Amask%20segmentation%20for%20open%20vocabulary%20setting.%20The%20core%20idea%20is%20to%0Aconditionally%20parameterize%20the%20network%20heads%20on%20semantic%20embedding%20and%20thus%20the%0Amodel%20is%20guided%20with%20class-specific%20knowledge%20to%20better%20detect%20novel%0Acategories.%20Specifically%2C%20CondHead%20is%20composed%20of%20two%20streams%20of%20network%20heads%2C%0Athe%20dynamically%20aggregated%20head%20and%20the%20dynamically%20generated%20head.%20The%20former%0Ais%20instantiated%20with%20a%20set%20of%20static%20heads%20that%20are%20conditionally%20aggregated%2C%0Athese%20heads%20are%20optimized%20as%20experts%20and%20are%20expected%20to%20learn%20sophisticated%0Aprediction.%20The%20latter%20is%20instantiated%20with%20dynamically%20generated%20parameters%0Aand%20encodes%20general%20class-specific%20information.%20With%20such%20a%20conditional%20design%2C%0Athe%20detection%20model%20is%20bridged%20by%20the%20semantic%20embedding%20to%20offer%20strongly%0Ageneralizable%20class-wise%20box%20and%20mask%20prediction.%20Our%20method%20brings%20significant%0Aimprovement%20to%20the%20state-of-the-art%20open%20vocabulary%20object%20detection%20methods%0Awith%20very%20minor%20overhead%2C%20e.g.%2C%20it%20surpasses%20a%20RegionClip%20model%20by%203.0%0Adetection%20AP%20on%20novel%20categories%2C%20with%20only%201.1%25%20more%20computation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.12130v6&entry.124074799=Read"},
{"title": "Subspace Representation Learning for Sparse Linear Arrays to Localize\n  More Sources than Sensors: A Deep Learning Methodology", "author": "Kuan-Lin Chen and Bhaskar D. Rao", "abstract": "  Localizing more sources than sensors with a sparse linear array (SLA) has\nlong relied on minimizing a distance between two covariance matrices and recent\nalgorithms often utilize semidefinite programming (SDP). Although deep neural\nnetwork (DNN)-based methods offer new alternatives, they still depend on\ncovariance matrix fitting. In this paper, we develop a novel methodology that\nestimates the co-array subspaces from a sample covariance for SLAs. Our\nmethodology trains a DNN to learn signal and noise subspace representations\nthat are invariant to the selection of bases. To learn such representations, we\npropose loss functions that gauge the separation between the desired and the\nestimated subspace. In particular, we propose losses that measure the length of\nthe shortest path between subspaces viewed on a union of Grassmannians, and\nprove that it is possible for a DNN to approximate signal subspaces. The\ncomputation of learning subspaces of different dimensions is accelerated by a\nnew batch sampling strategy called consistent rank sampling. The methodology is\nrobust to array imperfections due to its geometry-agnostic and data-driven\nnature. In addition, we propose a fully end-to-end gridless approach that\ndirectly learns angles to study the possibility of bypassing subspace methods.\nNumerical results show that learning such subspace representations is more\nbeneficial than learning covariances or angles. It outperforms conventional\nSDP-based methods such as the sparse and parametric approach (SPA) and existing\nDNN-based covariance reconstruction methods for a wide range of signal-to-noise\nratios (SNRs), snapshots, and source numbers for both perfect and imperfect\narrays.\n", "link": "http://arxiv.org/abs/2408.16605v1", "date": "2024-08-29", "relevancy": 2.7731, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5655}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5501}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subspace%20Representation%20Learning%20for%20Sparse%20Linear%20Arrays%20to%20Localize%0A%20%20More%20Sources%20than%20Sensors%3A%20A%20Deep%20Learning%20Methodology&body=Title%3A%20Subspace%20Representation%20Learning%20for%20Sparse%20Linear%20Arrays%20to%20Localize%0A%20%20More%20Sources%20than%20Sensors%3A%20A%20Deep%20Learning%20Methodology%0AAuthor%3A%20Kuan-Lin%20Chen%20and%20Bhaskar%20D.%20Rao%0AAbstract%3A%20%20%20Localizing%20more%20sources%20than%20sensors%20with%20a%20sparse%20linear%20array%20%28SLA%29%20has%0Along%20relied%20on%20minimizing%20a%20distance%20between%20two%20covariance%20matrices%20and%20recent%0Aalgorithms%20often%20utilize%20semidefinite%20programming%20%28SDP%29.%20Although%20deep%20neural%0Anetwork%20%28DNN%29-based%20methods%20offer%20new%20alternatives%2C%20they%20still%20depend%20on%0Acovariance%20matrix%20fitting.%20In%20this%20paper%2C%20we%20develop%20a%20novel%20methodology%20that%0Aestimates%20the%20co-array%20subspaces%20from%20a%20sample%20covariance%20for%20SLAs.%20Our%0Amethodology%20trains%20a%20DNN%20to%20learn%20signal%20and%20noise%20subspace%20representations%0Athat%20are%20invariant%20to%20the%20selection%20of%20bases.%20To%20learn%20such%20representations%2C%20we%0Apropose%20loss%20functions%20that%20gauge%20the%20separation%20between%20the%20desired%20and%20the%0Aestimated%20subspace.%20In%20particular%2C%20we%20propose%20losses%20that%20measure%20the%20length%20of%0Athe%20shortest%20path%20between%20subspaces%20viewed%20on%20a%20union%20of%20Grassmannians%2C%20and%0Aprove%20that%20it%20is%20possible%20for%20a%20DNN%20to%20approximate%20signal%20subspaces.%20The%0Acomputation%20of%20learning%20subspaces%20of%20different%20dimensions%20is%20accelerated%20by%20a%0Anew%20batch%20sampling%20strategy%20called%20consistent%20rank%20sampling.%20The%20methodology%20is%0Arobust%20to%20array%20imperfections%20due%20to%20its%20geometry-agnostic%20and%20data-driven%0Anature.%20In%20addition%2C%20we%20propose%20a%20fully%20end-to-end%20gridless%20approach%20that%0Adirectly%20learns%20angles%20to%20study%20the%20possibility%20of%20bypassing%20subspace%20methods.%0ANumerical%20results%20show%20that%20learning%20such%20subspace%20representations%20is%20more%0Abeneficial%20than%20learning%20covariances%20or%20angles.%20It%20outperforms%20conventional%0ASDP-based%20methods%20such%20as%20the%20sparse%20and%20parametric%20approach%20%28SPA%29%20and%20existing%0ADNN-based%20covariance%20reconstruction%20methods%20for%20a%20wide%20range%20of%20signal-to-noise%0Aratios%20%28SNRs%29%2C%20snapshots%2C%20and%20source%20numbers%20for%20both%20perfect%20and%20imperfect%0Aarrays.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubspace%2520Representation%2520Learning%2520for%2520Sparse%2520Linear%2520Arrays%2520to%2520Localize%250A%2520%2520More%2520Sources%2520than%2520Sensors%253A%2520A%2520Deep%2520Learning%2520Methodology%26entry.906535625%3DKuan-Lin%2520Chen%2520and%2520Bhaskar%2520D.%2520Rao%26entry.1292438233%3D%2520%2520Localizing%2520more%2520sources%2520than%2520sensors%2520with%2520a%2520sparse%2520linear%2520array%2520%2528SLA%2529%2520has%250Along%2520relied%2520on%2520minimizing%2520a%2520distance%2520between%2520two%2520covariance%2520matrices%2520and%2520recent%250Aalgorithms%2520often%2520utilize%2520semidefinite%2520programming%2520%2528SDP%2529.%2520Although%2520deep%2520neural%250Anetwork%2520%2528DNN%2529-based%2520methods%2520offer%2520new%2520alternatives%252C%2520they%2520still%2520depend%2520on%250Acovariance%2520matrix%2520fitting.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520novel%2520methodology%2520that%250Aestimates%2520the%2520co-array%2520subspaces%2520from%2520a%2520sample%2520covariance%2520for%2520SLAs.%2520Our%250Amethodology%2520trains%2520a%2520DNN%2520to%2520learn%2520signal%2520and%2520noise%2520subspace%2520representations%250Athat%2520are%2520invariant%2520to%2520the%2520selection%2520of%2520bases.%2520To%2520learn%2520such%2520representations%252C%2520we%250Apropose%2520loss%2520functions%2520that%2520gauge%2520the%2520separation%2520between%2520the%2520desired%2520and%2520the%250Aestimated%2520subspace.%2520In%2520particular%252C%2520we%2520propose%2520losses%2520that%2520measure%2520the%2520length%2520of%250Athe%2520shortest%2520path%2520between%2520subspaces%2520viewed%2520on%2520a%2520union%2520of%2520Grassmannians%252C%2520and%250Aprove%2520that%2520it%2520is%2520possible%2520for%2520a%2520DNN%2520to%2520approximate%2520signal%2520subspaces.%2520The%250Acomputation%2520of%2520learning%2520subspaces%2520of%2520different%2520dimensions%2520is%2520accelerated%2520by%2520a%250Anew%2520batch%2520sampling%2520strategy%2520called%2520consistent%2520rank%2520sampling.%2520The%2520methodology%2520is%250Arobust%2520to%2520array%2520imperfections%2520due%2520to%2520its%2520geometry-agnostic%2520and%2520data-driven%250Anature.%2520In%2520addition%252C%2520we%2520propose%2520a%2520fully%2520end-to-end%2520gridless%2520approach%2520that%250Adirectly%2520learns%2520angles%2520to%2520study%2520the%2520possibility%2520of%2520bypassing%2520subspace%2520methods.%250ANumerical%2520results%2520show%2520that%2520learning%2520such%2520subspace%2520representations%2520is%2520more%250Abeneficial%2520than%2520learning%2520covariances%2520or%2520angles.%2520It%2520outperforms%2520conventional%250ASDP-based%2520methods%2520such%2520as%2520the%2520sparse%2520and%2520parametric%2520approach%2520%2528SPA%2529%2520and%2520existing%250ADNN-based%2520covariance%2520reconstruction%2520methods%2520for%2520a%2520wide%2520range%2520of%2520signal-to-noise%250Aratios%2520%2528SNRs%2529%252C%2520snapshots%252C%2520and%2520source%2520numbers%2520for%2520both%2520perfect%2520and%2520imperfect%250Aarrays.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subspace%20Representation%20Learning%20for%20Sparse%20Linear%20Arrays%20to%20Localize%0A%20%20More%20Sources%20than%20Sensors%3A%20A%20Deep%20Learning%20Methodology&entry.906535625=Kuan-Lin%20Chen%20and%20Bhaskar%20D.%20Rao&entry.1292438233=%20%20Localizing%20more%20sources%20than%20sensors%20with%20a%20sparse%20linear%20array%20%28SLA%29%20has%0Along%20relied%20on%20minimizing%20a%20distance%20between%20two%20covariance%20matrices%20and%20recent%0Aalgorithms%20often%20utilize%20semidefinite%20programming%20%28SDP%29.%20Although%20deep%20neural%0Anetwork%20%28DNN%29-based%20methods%20offer%20new%20alternatives%2C%20they%20still%20depend%20on%0Acovariance%20matrix%20fitting.%20In%20this%20paper%2C%20we%20develop%20a%20novel%20methodology%20that%0Aestimates%20the%20co-array%20subspaces%20from%20a%20sample%20covariance%20for%20SLAs.%20Our%0Amethodology%20trains%20a%20DNN%20to%20learn%20signal%20and%20noise%20subspace%20representations%0Athat%20are%20invariant%20to%20the%20selection%20of%20bases.%20To%20learn%20such%20representations%2C%20we%0Apropose%20loss%20functions%20that%20gauge%20the%20separation%20between%20the%20desired%20and%20the%0Aestimated%20subspace.%20In%20particular%2C%20we%20propose%20losses%20that%20measure%20the%20length%20of%0Athe%20shortest%20path%20between%20subspaces%20viewed%20on%20a%20union%20of%20Grassmannians%2C%20and%0Aprove%20that%20it%20is%20possible%20for%20a%20DNN%20to%20approximate%20signal%20subspaces.%20The%0Acomputation%20of%20learning%20subspaces%20of%20different%20dimensions%20is%20accelerated%20by%20a%0Anew%20batch%20sampling%20strategy%20called%20consistent%20rank%20sampling.%20The%20methodology%20is%0Arobust%20to%20array%20imperfections%20due%20to%20its%20geometry-agnostic%20and%20data-driven%0Anature.%20In%20addition%2C%20we%20propose%20a%20fully%20end-to-end%20gridless%20approach%20that%0Adirectly%20learns%20angles%20to%20study%20the%20possibility%20of%20bypassing%20subspace%20methods.%0ANumerical%20results%20show%20that%20learning%20such%20subspace%20representations%20is%20more%0Abeneficial%20than%20learning%20covariances%20or%20angles.%20It%20outperforms%20conventional%0ASDP-based%20methods%20such%20as%20the%20sparse%20and%20parametric%20approach%20%28SPA%29%20and%20existing%0ADNN-based%20covariance%20reconstruction%20methods%20for%20a%20wide%20range%20of%20signal-to-noise%0Aratios%20%28SNRs%29%2C%20snapshots%2C%20and%20source%20numbers%20for%20both%20perfect%20and%20imperfect%0Aarrays.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16605v1&entry.124074799=Read"},
{"title": "OP-Align: Object-level and Part-level Alignment for Self-supervised\n  Category-level Articulated Object Pose Estimation", "author": "Yuchen Che and Ryo Furukawa and Asako Kanezaki", "abstract": "  Category-level articulated object pose estimation focuses on the pose\nestimation of unknown articulated objects within known categories. Despite its\nsignificance, this task remains challenging due to the varying shapes and poses\nof objects, expensive dataset annotation costs, and complex real-world\nenvironments. In this paper, we propose a novel self-supervised approach that\nleverages a single-frame point cloud to solve this task. Our model consistently\ngenerates reconstruction with a canonical pose and joint state for the entire\ninput object, and it estimates object-level poses that reduce overall pose\nvariance and part-level poses that align each part of the input with its\ncorresponding part of the reconstruction. Experimental results demonstrate that\nour approach significantly outperforms previous self-supervised methods and is\ncomparable to the state-of-the-art supervised methods. To assess the\nperformance of our model in real-world scenarios, we also introduce a new\nreal-world articulated object benchmark dataset.\n", "link": "http://arxiv.org/abs/2408.16547v1", "date": "2024-08-29", "relevancy": 2.761, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5577}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5504}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OP-Align%3A%20Object-level%20and%20Part-level%20Alignment%20for%20Self-supervised%0A%20%20Category-level%20Articulated%20Object%20Pose%20Estimation&body=Title%3A%20OP-Align%3A%20Object-level%20and%20Part-level%20Alignment%20for%20Self-supervised%0A%20%20Category-level%20Articulated%20Object%20Pose%20Estimation%0AAuthor%3A%20Yuchen%20Che%20and%20Ryo%20Furukawa%20and%20Asako%20Kanezaki%0AAbstract%3A%20%20%20Category-level%20articulated%20object%20pose%20estimation%20focuses%20on%20the%20pose%0Aestimation%20of%20unknown%20articulated%20objects%20within%20known%20categories.%20Despite%20its%0Asignificance%2C%20this%20task%20remains%20challenging%20due%20to%20the%20varying%20shapes%20and%20poses%0Aof%20objects%2C%20expensive%20dataset%20annotation%20costs%2C%20and%20complex%20real-world%0Aenvironments.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20self-supervised%20approach%20that%0Aleverages%20a%20single-frame%20point%20cloud%20to%20solve%20this%20task.%20Our%20model%20consistently%0Agenerates%20reconstruction%20with%20a%20canonical%20pose%20and%20joint%20state%20for%20the%20entire%0Ainput%20object%2C%20and%20it%20estimates%20object-level%20poses%20that%20reduce%20overall%20pose%0Avariance%20and%20part-level%20poses%20that%20align%20each%20part%20of%20the%20input%20with%20its%0Acorresponding%20part%20of%20the%20reconstruction.%20Experimental%20results%20demonstrate%20that%0Aour%20approach%20significantly%20outperforms%20previous%20self-supervised%20methods%20and%20is%0Acomparable%20to%20the%20state-of-the-art%20supervised%20methods.%20To%20assess%20the%0Aperformance%20of%20our%20model%20in%20real-world%20scenarios%2C%20we%20also%20introduce%20a%20new%0Areal-world%20articulated%20object%20benchmark%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOP-Align%253A%2520Object-level%2520and%2520Part-level%2520Alignment%2520for%2520Self-supervised%250A%2520%2520Category-level%2520Articulated%2520Object%2520Pose%2520Estimation%26entry.906535625%3DYuchen%2520Che%2520and%2520Ryo%2520Furukawa%2520and%2520Asako%2520Kanezaki%26entry.1292438233%3D%2520%2520Category-level%2520articulated%2520object%2520pose%2520estimation%2520focuses%2520on%2520the%2520pose%250Aestimation%2520of%2520unknown%2520articulated%2520objects%2520within%2520known%2520categories.%2520Despite%2520its%250Asignificance%252C%2520this%2520task%2520remains%2520challenging%2520due%2520to%2520the%2520varying%2520shapes%2520and%2520poses%250Aof%2520objects%252C%2520expensive%2520dataset%2520annotation%2520costs%252C%2520and%2520complex%2520real-world%250Aenvironments.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520self-supervised%2520approach%2520that%250Aleverages%2520a%2520single-frame%2520point%2520cloud%2520to%2520solve%2520this%2520task.%2520Our%2520model%2520consistently%250Agenerates%2520reconstruction%2520with%2520a%2520canonical%2520pose%2520and%2520joint%2520state%2520for%2520the%2520entire%250Ainput%2520object%252C%2520and%2520it%2520estimates%2520object-level%2520poses%2520that%2520reduce%2520overall%2520pose%250Avariance%2520and%2520part-level%2520poses%2520that%2520align%2520each%2520part%2520of%2520the%2520input%2520with%2520its%250Acorresponding%2520part%2520of%2520the%2520reconstruction.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520approach%2520significantly%2520outperforms%2520previous%2520self-supervised%2520methods%2520and%2520is%250Acomparable%2520to%2520the%2520state-of-the-art%2520supervised%2520methods.%2520To%2520assess%2520the%250Aperformance%2520of%2520our%2520model%2520in%2520real-world%2520scenarios%252C%2520we%2520also%2520introduce%2520a%2520new%250Areal-world%2520articulated%2520object%2520benchmark%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OP-Align%3A%20Object-level%20and%20Part-level%20Alignment%20for%20Self-supervised%0A%20%20Category-level%20Articulated%20Object%20Pose%20Estimation&entry.906535625=Yuchen%20Che%20and%20Ryo%20Furukawa%20and%20Asako%20Kanezaki&entry.1292438233=%20%20Category-level%20articulated%20object%20pose%20estimation%20focuses%20on%20the%20pose%0Aestimation%20of%20unknown%20articulated%20objects%20within%20known%20categories.%20Despite%20its%0Asignificance%2C%20this%20task%20remains%20challenging%20due%20to%20the%20varying%20shapes%20and%20poses%0Aof%20objects%2C%20expensive%20dataset%20annotation%20costs%2C%20and%20complex%20real-world%0Aenvironments.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20self-supervised%20approach%20that%0Aleverages%20a%20single-frame%20point%20cloud%20to%20solve%20this%20task.%20Our%20model%20consistently%0Agenerates%20reconstruction%20with%20a%20canonical%20pose%20and%20joint%20state%20for%20the%20entire%0Ainput%20object%2C%20and%20it%20estimates%20object-level%20poses%20that%20reduce%20overall%20pose%0Avariance%20and%20part-level%20poses%20that%20align%20each%20part%20of%20the%20input%20with%20its%0Acorresponding%20part%20of%20the%20reconstruction.%20Experimental%20results%20demonstrate%20that%0Aour%20approach%20significantly%20outperforms%20previous%20self-supervised%20methods%20and%20is%0Acomparable%20to%20the%20state-of-the-art%20supervised%20methods.%20To%20assess%20the%0Aperformance%20of%20our%20model%20in%20real-world%20scenarios%2C%20we%20also%20introduce%20a%20new%0Areal-world%20articulated%20object%20benchmark%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16547v1&entry.124074799=Read"},
{"title": "PartFormer: Awakening Latent Diverse Representation from Vision\n  Transformer for Object Re-Identification", "author": "Lei Tan and Pingyang Dai and Jie Chen and Liujuan Cao and Yongjian Wu and Rongrong Ji", "abstract": "  Extracting robust feature representation is critical for object\nre-identification to accurately identify objects across non-overlapping\ncameras. Although having a strong representation ability, the Vision\nTransformer (ViT) tends to overfit on most distinct regions of training data,\nlimiting its generalizability and attention to holistic object features.\nMeanwhile, due to the structural difference between CNN and ViT, fine-grained\nstrategies that effectively address this issue in CNN do not continue to be\nsuccessful in ViT. To address this issue, by observing the latent diverse\nrepresentation hidden behind the multi-head attention, we present PartFormer,\nan innovative adaptation of ViT designed to overcome the granularity\nlimitations in object Re-ID tasks. The PartFormer integrates a Head\nDisentangling Block (HDB) that awakens the diverse representation of multi-head\nself-attention without the typical loss of feature richness induced by\nconcatenation and FFN layers post-attention. To avoid the homogenization of\nattention heads and promote robust part-based feature learning, two head\ndiversity constraints are imposed: attention diversity constraint and\ncorrelation diversity constraint. These constraints enable the model to exploit\ndiverse and discriminative feature representations from different attention\nheads. Comprehensive experiments on various object Re-ID benchmarks demonstrate\nthe superiority of the PartFormer. Specifically, our framework significantly\noutperforms state-of-the-art by 2.4\\% mAP scores on the most challenging MSMT17\ndataset.\n", "link": "http://arxiv.org/abs/2408.16684v1", "date": "2024-08-29", "relevancy": 2.7321, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5484}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartFormer%3A%20Awakening%20Latent%20Diverse%20Representation%20from%20Vision%0A%20%20Transformer%20for%20Object%20Re-Identification&body=Title%3A%20PartFormer%3A%20Awakening%20Latent%20Diverse%20Representation%20from%20Vision%0A%20%20Transformer%20for%20Object%20Re-Identification%0AAuthor%3A%20Lei%20Tan%20and%20Pingyang%20Dai%20and%20Jie%20Chen%20and%20Liujuan%20Cao%20and%20Yongjian%20Wu%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Extracting%20robust%20feature%20representation%20is%20critical%20for%20object%0Are-identification%20to%20accurately%20identify%20objects%20across%20non-overlapping%0Acameras.%20Although%20having%20a%20strong%20representation%20ability%2C%20the%20Vision%0ATransformer%20%28ViT%29%20tends%20to%20overfit%20on%20most%20distinct%20regions%20of%20training%20data%2C%0Alimiting%20its%20generalizability%20and%20attention%20to%20holistic%20object%20features.%0AMeanwhile%2C%20due%20to%20the%20structural%20difference%20between%20CNN%20and%20ViT%2C%20fine-grained%0Astrategies%20that%20effectively%20address%20this%20issue%20in%20CNN%20do%20not%20continue%20to%20be%0Asuccessful%20in%20ViT.%20To%20address%20this%20issue%2C%20by%20observing%20the%20latent%20diverse%0Arepresentation%20hidden%20behind%20the%20multi-head%20attention%2C%20we%20present%20PartFormer%2C%0Aan%20innovative%20adaptation%20of%20ViT%20designed%20to%20overcome%20the%20granularity%0Alimitations%20in%20object%20Re-ID%20tasks.%20The%20PartFormer%20integrates%20a%20Head%0ADisentangling%20Block%20%28HDB%29%20that%20awakens%20the%20diverse%20representation%20of%20multi-head%0Aself-attention%20without%20the%20typical%20loss%20of%20feature%20richness%20induced%20by%0Aconcatenation%20and%20FFN%20layers%20post-attention.%20To%20avoid%20the%20homogenization%20of%0Aattention%20heads%20and%20promote%20robust%20part-based%20feature%20learning%2C%20two%20head%0Adiversity%20constraints%20are%20imposed%3A%20attention%20diversity%20constraint%20and%0Acorrelation%20diversity%20constraint.%20These%20constraints%20enable%20the%20model%20to%20exploit%0Adiverse%20and%20discriminative%20feature%20representations%20from%20different%20attention%0Aheads.%20Comprehensive%20experiments%20on%20various%20object%20Re-ID%20benchmarks%20demonstrate%0Athe%20superiority%20of%20the%20PartFormer.%20Specifically%2C%20our%20framework%20significantly%0Aoutperforms%20state-of-the-art%20by%202.4%5C%25%20mAP%20scores%20on%20the%20most%20challenging%20MSMT17%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartFormer%253A%2520Awakening%2520Latent%2520Diverse%2520Representation%2520from%2520Vision%250A%2520%2520Transformer%2520for%2520Object%2520Re-Identification%26entry.906535625%3DLei%2520Tan%2520and%2520Pingyang%2520Dai%2520and%2520Jie%2520Chen%2520and%2520Liujuan%2520Cao%2520and%2520Yongjian%2520Wu%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Extracting%2520robust%2520feature%2520representation%2520is%2520critical%2520for%2520object%250Are-identification%2520to%2520accurately%2520identify%2520objects%2520across%2520non-overlapping%250Acameras.%2520Although%2520having%2520a%2520strong%2520representation%2520ability%252C%2520the%2520Vision%250ATransformer%2520%2528ViT%2529%2520tends%2520to%2520overfit%2520on%2520most%2520distinct%2520regions%2520of%2520training%2520data%252C%250Alimiting%2520its%2520generalizability%2520and%2520attention%2520to%2520holistic%2520object%2520features.%250AMeanwhile%252C%2520due%2520to%2520the%2520structural%2520difference%2520between%2520CNN%2520and%2520ViT%252C%2520fine-grained%250Astrategies%2520that%2520effectively%2520address%2520this%2520issue%2520in%2520CNN%2520do%2520not%2520continue%2520to%2520be%250Asuccessful%2520in%2520ViT.%2520To%2520address%2520this%2520issue%252C%2520by%2520observing%2520the%2520latent%2520diverse%250Arepresentation%2520hidden%2520behind%2520the%2520multi-head%2520attention%252C%2520we%2520present%2520PartFormer%252C%250Aan%2520innovative%2520adaptation%2520of%2520ViT%2520designed%2520to%2520overcome%2520the%2520granularity%250Alimitations%2520in%2520object%2520Re-ID%2520tasks.%2520The%2520PartFormer%2520integrates%2520a%2520Head%250ADisentangling%2520Block%2520%2528HDB%2529%2520that%2520awakens%2520the%2520diverse%2520representation%2520of%2520multi-head%250Aself-attention%2520without%2520the%2520typical%2520loss%2520of%2520feature%2520richness%2520induced%2520by%250Aconcatenation%2520and%2520FFN%2520layers%2520post-attention.%2520To%2520avoid%2520the%2520homogenization%2520of%250Aattention%2520heads%2520and%2520promote%2520robust%2520part-based%2520feature%2520learning%252C%2520two%2520head%250Adiversity%2520constraints%2520are%2520imposed%253A%2520attention%2520diversity%2520constraint%2520and%250Acorrelation%2520diversity%2520constraint.%2520These%2520constraints%2520enable%2520the%2520model%2520to%2520exploit%250Adiverse%2520and%2520discriminative%2520feature%2520representations%2520from%2520different%2520attention%250Aheads.%2520Comprehensive%2520experiments%2520on%2520various%2520object%2520Re-ID%2520benchmarks%2520demonstrate%250Athe%2520superiority%2520of%2520the%2520PartFormer.%2520Specifically%252C%2520our%2520framework%2520significantly%250Aoutperforms%2520state-of-the-art%2520by%25202.4%255C%2525%2520mAP%2520scores%2520on%2520the%2520most%2520challenging%2520MSMT17%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartFormer%3A%20Awakening%20Latent%20Diverse%20Representation%20from%20Vision%0A%20%20Transformer%20for%20Object%20Re-Identification&entry.906535625=Lei%20Tan%20and%20Pingyang%20Dai%20and%20Jie%20Chen%20and%20Liujuan%20Cao%20and%20Yongjian%20Wu%20and%20Rongrong%20Ji&entry.1292438233=%20%20Extracting%20robust%20feature%20representation%20is%20critical%20for%20object%0Are-identification%20to%20accurately%20identify%20objects%20across%20non-overlapping%0Acameras.%20Although%20having%20a%20strong%20representation%20ability%2C%20the%20Vision%0ATransformer%20%28ViT%29%20tends%20to%20overfit%20on%20most%20distinct%20regions%20of%20training%20data%2C%0Alimiting%20its%20generalizability%20and%20attention%20to%20holistic%20object%20features.%0AMeanwhile%2C%20due%20to%20the%20structural%20difference%20between%20CNN%20and%20ViT%2C%20fine-grained%0Astrategies%20that%20effectively%20address%20this%20issue%20in%20CNN%20do%20not%20continue%20to%20be%0Asuccessful%20in%20ViT.%20To%20address%20this%20issue%2C%20by%20observing%20the%20latent%20diverse%0Arepresentation%20hidden%20behind%20the%20multi-head%20attention%2C%20we%20present%20PartFormer%2C%0Aan%20innovative%20adaptation%20of%20ViT%20designed%20to%20overcome%20the%20granularity%0Alimitations%20in%20object%20Re-ID%20tasks.%20The%20PartFormer%20integrates%20a%20Head%0ADisentangling%20Block%20%28HDB%29%20that%20awakens%20the%20diverse%20representation%20of%20multi-head%0Aself-attention%20without%20the%20typical%20loss%20of%20feature%20richness%20induced%20by%0Aconcatenation%20and%20FFN%20layers%20post-attention.%20To%20avoid%20the%20homogenization%20of%0Aattention%20heads%20and%20promote%20robust%20part-based%20feature%20learning%2C%20two%20head%0Adiversity%20constraints%20are%20imposed%3A%20attention%20diversity%20constraint%20and%0Acorrelation%20diversity%20constraint.%20These%20constraints%20enable%20the%20model%20to%20exploit%0Adiverse%20and%20discriminative%20feature%20representations%20from%20different%20attention%0Aheads.%20Comprehensive%20experiments%20on%20various%20object%20Re-ID%20benchmarks%20demonstrate%0Athe%20superiority%20of%20the%20PartFormer.%20Specifically%2C%20our%20framework%20significantly%0Aoutperforms%20state-of-the-art%20by%202.4%5C%25%20mAP%20scores%20on%20the%20most%20challenging%20MSMT17%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16684v1&entry.124074799=Read"},
{"title": "HYGENE: A Diffusion-based Hypergraph Generation Method", "author": "Dorian Gailhard and Enzo Tartaglione and Lirida Naviner De Barros and Jhony H. Giraldo", "abstract": "  Hypergraphs are powerful mathematical structures that can model complex,\nhigh-order relationships in various domains, including social networks,\nbioinformatics, and recommender systems. However, generating realistic and\ndiverse hypergraphs remains challenging due to their inherent complexity and\nlack of effective generative models. In this paper, we introduce a\ndiffusion-based Hypergraph Generation (HYGENE) method that addresses these\nchallenges through a progressive local expansion approach. HYGENE works on the\nbipartite representation of hypergraphs, starting with a single pair of\nconnected nodes and iteratively expanding it to form the target hypergraph. At\neach step, nodes and hyperedges are added in a localized manner using a\ndenoising diffusion process, which allows for the construction of the global\nstructure before refining local details. Our experiments demonstrated the\neffectiveness of HYGENE, proving its ability to closely mimic a variety of\nproperties in hypergraphs. To the best of our knowledge, this is the first\nattempt to employ deep learning models for hypergraph generation, and our work\naims to lay the groundwork for future research in this area.\n", "link": "http://arxiv.org/abs/2408.16457v1", "date": "2024-08-29", "relevancy": 2.7208, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5688}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5512}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HYGENE%3A%20A%20Diffusion-based%20Hypergraph%20Generation%20Method&body=Title%3A%20HYGENE%3A%20A%20Diffusion-based%20Hypergraph%20Generation%20Method%0AAuthor%3A%20Dorian%20Gailhard%20and%20Enzo%20Tartaglione%20and%20Lirida%20Naviner%20De%20Barros%20and%20Jhony%20H.%20Giraldo%0AAbstract%3A%20%20%20Hypergraphs%20are%20powerful%20mathematical%20structures%20that%20can%20model%20complex%2C%0Ahigh-order%20relationships%20in%20various%20domains%2C%20including%20social%20networks%2C%0Abioinformatics%2C%20and%20recommender%20systems.%20However%2C%20generating%20realistic%20and%0Adiverse%20hypergraphs%20remains%20challenging%20due%20to%20their%20inherent%20complexity%20and%0Alack%20of%20effective%20generative%20models.%20In%20this%20paper%2C%20we%20introduce%20a%0Adiffusion-based%20Hypergraph%20Generation%20%28HYGENE%29%20method%20that%20addresses%20these%0Achallenges%20through%20a%20progressive%20local%20expansion%20approach.%20HYGENE%20works%20on%20the%0Abipartite%20representation%20of%20hypergraphs%2C%20starting%20with%20a%20single%20pair%20of%0Aconnected%20nodes%20and%20iteratively%20expanding%20it%20to%20form%20the%20target%20hypergraph.%20At%0Aeach%20step%2C%20nodes%20and%20hyperedges%20are%20added%20in%20a%20localized%20manner%20using%20a%0Adenoising%20diffusion%20process%2C%20which%20allows%20for%20the%20construction%20of%20the%20global%0Astructure%20before%20refining%20local%20details.%20Our%20experiments%20demonstrated%20the%0Aeffectiveness%20of%20HYGENE%2C%20proving%20its%20ability%20to%20closely%20mimic%20a%20variety%20of%0Aproperties%20in%20hypergraphs.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Aattempt%20to%20employ%20deep%20learning%20models%20for%20hypergraph%20generation%2C%20and%20our%20work%0Aaims%20to%20lay%20the%20groundwork%20for%20future%20research%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHYGENE%253A%2520A%2520Diffusion-based%2520Hypergraph%2520Generation%2520Method%26entry.906535625%3DDorian%2520Gailhard%2520and%2520Enzo%2520Tartaglione%2520and%2520Lirida%2520Naviner%2520De%2520Barros%2520and%2520Jhony%2520H.%2520Giraldo%26entry.1292438233%3D%2520%2520Hypergraphs%2520are%2520powerful%2520mathematical%2520structures%2520that%2520can%2520model%2520complex%252C%250Ahigh-order%2520relationships%2520in%2520various%2520domains%252C%2520including%2520social%2520networks%252C%250Abioinformatics%252C%2520and%2520recommender%2520systems.%2520However%252C%2520generating%2520realistic%2520and%250Adiverse%2520hypergraphs%2520remains%2520challenging%2520due%2520to%2520their%2520inherent%2520complexity%2520and%250Alack%2520of%2520effective%2520generative%2520models.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Adiffusion-based%2520Hypergraph%2520Generation%2520%2528HYGENE%2529%2520method%2520that%2520addresses%2520these%250Achallenges%2520through%2520a%2520progressive%2520local%2520expansion%2520approach.%2520HYGENE%2520works%2520on%2520the%250Abipartite%2520representation%2520of%2520hypergraphs%252C%2520starting%2520with%2520a%2520single%2520pair%2520of%250Aconnected%2520nodes%2520and%2520iteratively%2520expanding%2520it%2520to%2520form%2520the%2520target%2520hypergraph.%2520At%250Aeach%2520step%252C%2520nodes%2520and%2520hyperedges%2520are%2520added%2520in%2520a%2520localized%2520manner%2520using%2520a%250Adenoising%2520diffusion%2520process%252C%2520which%2520allows%2520for%2520the%2520construction%2520of%2520the%2520global%250Astructure%2520before%2520refining%2520local%2520details.%2520Our%2520experiments%2520demonstrated%2520the%250Aeffectiveness%2520of%2520HYGENE%252C%2520proving%2520its%2520ability%2520to%2520closely%2520mimic%2520a%2520variety%2520of%250Aproperties%2520in%2520hypergraphs.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Aattempt%2520to%2520employ%2520deep%2520learning%2520models%2520for%2520hypergraph%2520generation%252C%2520and%2520our%2520work%250Aaims%2520to%2520lay%2520the%2520groundwork%2520for%2520future%2520research%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HYGENE%3A%20A%20Diffusion-based%20Hypergraph%20Generation%20Method&entry.906535625=Dorian%20Gailhard%20and%20Enzo%20Tartaglione%20and%20Lirida%20Naviner%20De%20Barros%20and%20Jhony%20H.%20Giraldo&entry.1292438233=%20%20Hypergraphs%20are%20powerful%20mathematical%20structures%20that%20can%20model%20complex%2C%0Ahigh-order%20relationships%20in%20various%20domains%2C%20including%20social%20networks%2C%0Abioinformatics%2C%20and%20recommender%20systems.%20However%2C%20generating%20realistic%20and%0Adiverse%20hypergraphs%20remains%20challenging%20due%20to%20their%20inherent%20complexity%20and%0Alack%20of%20effective%20generative%20models.%20In%20this%20paper%2C%20we%20introduce%20a%0Adiffusion-based%20Hypergraph%20Generation%20%28HYGENE%29%20method%20that%20addresses%20these%0Achallenges%20through%20a%20progressive%20local%20expansion%20approach.%20HYGENE%20works%20on%20the%0Abipartite%20representation%20of%20hypergraphs%2C%20starting%20with%20a%20single%20pair%20of%0Aconnected%20nodes%20and%20iteratively%20expanding%20it%20to%20form%20the%20target%20hypergraph.%20At%0Aeach%20step%2C%20nodes%20and%20hyperedges%20are%20added%20in%20a%20localized%20manner%20using%20a%0Adenoising%20diffusion%20process%2C%20which%20allows%20for%20the%20construction%20of%20the%20global%0Astructure%20before%20refining%20local%20details.%20Our%20experiments%20demonstrated%20the%0Aeffectiveness%20of%20HYGENE%2C%20proving%20its%20ability%20to%20closely%20mimic%20a%20variety%20of%0Aproperties%20in%20hypergraphs.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Aattempt%20to%20employ%20deep%20learning%20models%20for%20hypergraph%20generation%2C%20and%20our%20work%0Aaims%20to%20lay%20the%20groundwork%20for%20future%20research%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16457v1&entry.124074799=Read"},
{"title": "Mumpy: Multilateral Temporal-view Pyramid Transformer for Video\n  Inpainting Detection", "author": "Ying Zhang and Yuezun Li and Bo Peng and Jiaran Zhou and Huiyu Zhou and Junyu Dong", "abstract": "  The task of video inpainting detection is to expose the pixel-level inpainted\nregions within a video sequence. Existing methods usually focus on leveraging\nspatial and temporal inconsistencies. However, these methods typically employ\nfixed operations to combine spatial and temporal clues, limiting their\napplicability in different scenarios. In this paper, we introduce a novel\nMultilateral Temporal-view Pyramid Transformer ({\\em MumPy}) that collaborates\nspatial-temporal clues flexibly. Our method utilizes a newly designed\nmultilateral temporal-view encoder to extract various collaborations of\nspatial-temporal clues and introduces a deformable window-based temporal-view\ninteraction module to enhance the diversity of these collaborations.\nSubsequently, we develop a multi-pyramid decoder to aggregate the various types\nof features and generate detection maps. By adjusting the contribution strength\nof spatial and temporal clues, our method can effectively identify inpainted\nregions. We validate our method on existing datasets and also introduce a new\nchallenging and large-scale Video Inpainting dataset based on the YouTube-VOS\ndataset, which employs several more recent inpainting methods. The results\ndemonstrate the superiority of our method in both in-domain and cross-domain\nevaluation scenarios.\n", "link": "http://arxiv.org/abs/2404.11054v3", "date": "2024-08-29", "relevancy": 2.7122, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5444}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mumpy%3A%20Multilateral%20Temporal-view%20Pyramid%20Transformer%20for%20Video%0A%20%20Inpainting%20Detection&body=Title%3A%20Mumpy%3A%20Multilateral%20Temporal-view%20Pyramid%20Transformer%20for%20Video%0A%20%20Inpainting%20Detection%0AAuthor%3A%20Ying%20Zhang%20and%20Yuezun%20Li%20and%20Bo%20Peng%20and%20Jiaran%20Zhou%20and%20Huiyu%20Zhou%20and%20Junyu%20Dong%0AAbstract%3A%20%20%20The%20task%20of%20video%20inpainting%20detection%20is%20to%20expose%20the%20pixel-level%20inpainted%0Aregions%20within%20a%20video%20sequence.%20Existing%20methods%20usually%20focus%20on%20leveraging%0Aspatial%20and%20temporal%20inconsistencies.%20However%2C%20these%20methods%20typically%20employ%0Afixed%20operations%20to%20combine%20spatial%20and%20temporal%20clues%2C%20limiting%20their%0Aapplicability%20in%20different%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0AMultilateral%20Temporal-view%20Pyramid%20Transformer%20%28%7B%5Cem%20MumPy%7D%29%20that%20collaborates%0Aspatial-temporal%20clues%20flexibly.%20Our%20method%20utilizes%20a%20newly%20designed%0Amultilateral%20temporal-view%20encoder%20to%20extract%20various%20collaborations%20of%0Aspatial-temporal%20clues%20and%20introduces%20a%20deformable%20window-based%20temporal-view%0Ainteraction%20module%20to%20enhance%20the%20diversity%20of%20these%20collaborations.%0ASubsequently%2C%20we%20develop%20a%20multi-pyramid%20decoder%20to%20aggregate%20the%20various%20types%0Aof%20features%20and%20generate%20detection%20maps.%20By%20adjusting%20the%20contribution%20strength%0Aof%20spatial%20and%20temporal%20clues%2C%20our%20method%20can%20effectively%20identify%20inpainted%0Aregions.%20We%20validate%20our%20method%20on%20existing%20datasets%20and%20also%20introduce%20a%20new%0Achallenging%20and%20large-scale%20Video%20Inpainting%20dataset%20based%20on%20the%20YouTube-VOS%0Adataset%2C%20which%20employs%20several%20more%20recent%20inpainting%20methods.%20The%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20in%20both%20in-domain%20and%20cross-domain%0Aevaluation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11054v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMumpy%253A%2520Multilateral%2520Temporal-view%2520Pyramid%2520Transformer%2520for%2520Video%250A%2520%2520Inpainting%2520Detection%26entry.906535625%3DYing%2520Zhang%2520and%2520Yuezun%2520Li%2520and%2520Bo%2520Peng%2520and%2520Jiaran%2520Zhou%2520and%2520Huiyu%2520Zhou%2520and%2520Junyu%2520Dong%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520video%2520inpainting%2520detection%2520is%2520to%2520expose%2520the%2520pixel-level%2520inpainted%250Aregions%2520within%2520a%2520video%2520sequence.%2520Existing%2520methods%2520usually%2520focus%2520on%2520leveraging%250Aspatial%2520and%2520temporal%2520inconsistencies.%2520However%252C%2520these%2520methods%2520typically%2520employ%250Afixed%2520operations%2520to%2520combine%2520spatial%2520and%2520temporal%2520clues%252C%2520limiting%2520their%250Aapplicability%2520in%2520different%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250AMultilateral%2520Temporal-view%2520Pyramid%2520Transformer%2520%2528%257B%255Cem%2520MumPy%257D%2529%2520that%2520collaborates%250Aspatial-temporal%2520clues%2520flexibly.%2520Our%2520method%2520utilizes%2520a%2520newly%2520designed%250Amultilateral%2520temporal-view%2520encoder%2520to%2520extract%2520various%2520collaborations%2520of%250Aspatial-temporal%2520clues%2520and%2520introduces%2520a%2520deformable%2520window-based%2520temporal-view%250Ainteraction%2520module%2520to%2520enhance%2520the%2520diversity%2520of%2520these%2520collaborations.%250ASubsequently%252C%2520we%2520develop%2520a%2520multi-pyramid%2520decoder%2520to%2520aggregate%2520the%2520various%2520types%250Aof%2520features%2520and%2520generate%2520detection%2520maps.%2520By%2520adjusting%2520the%2520contribution%2520strength%250Aof%2520spatial%2520and%2520temporal%2520clues%252C%2520our%2520method%2520can%2520effectively%2520identify%2520inpainted%250Aregions.%2520We%2520validate%2520our%2520method%2520on%2520existing%2520datasets%2520and%2520also%2520introduce%2520a%2520new%250Achallenging%2520and%2520large-scale%2520Video%2520Inpainting%2520dataset%2520based%2520on%2520the%2520YouTube-VOS%250Adataset%252C%2520which%2520employs%2520several%2520more%2520recent%2520inpainting%2520methods.%2520The%2520results%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method%2520in%2520both%2520in-domain%2520and%2520cross-domain%250Aevaluation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11054v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mumpy%3A%20Multilateral%20Temporal-view%20Pyramid%20Transformer%20for%20Video%0A%20%20Inpainting%20Detection&entry.906535625=Ying%20Zhang%20and%20Yuezun%20Li%20and%20Bo%20Peng%20and%20Jiaran%20Zhou%20and%20Huiyu%20Zhou%20and%20Junyu%20Dong&entry.1292438233=%20%20The%20task%20of%20video%20inpainting%20detection%20is%20to%20expose%20the%20pixel-level%20inpainted%0Aregions%20within%20a%20video%20sequence.%20Existing%20methods%20usually%20focus%20on%20leveraging%0Aspatial%20and%20temporal%20inconsistencies.%20However%2C%20these%20methods%20typically%20employ%0Afixed%20operations%20to%20combine%20spatial%20and%20temporal%20clues%2C%20limiting%20their%0Aapplicability%20in%20different%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0AMultilateral%20Temporal-view%20Pyramid%20Transformer%20%28%7B%5Cem%20MumPy%7D%29%20that%20collaborates%0Aspatial-temporal%20clues%20flexibly.%20Our%20method%20utilizes%20a%20newly%20designed%0Amultilateral%20temporal-view%20encoder%20to%20extract%20various%20collaborations%20of%0Aspatial-temporal%20clues%20and%20introduces%20a%20deformable%20window-based%20temporal-view%0Ainteraction%20module%20to%20enhance%20the%20diversity%20of%20these%20collaborations.%0ASubsequently%2C%20we%20develop%20a%20multi-pyramid%20decoder%20to%20aggregate%20the%20various%20types%0Aof%20features%20and%20generate%20detection%20maps.%20By%20adjusting%20the%20contribution%20strength%0Aof%20spatial%20and%20temporal%20clues%2C%20our%20method%20can%20effectively%20identify%20inpainted%0Aregions.%20We%20validate%20our%20method%20on%20existing%20datasets%20and%20also%20introduce%20a%20new%0Achallenging%20and%20large-scale%20Video%20Inpainting%20dataset%20based%20on%20the%20YouTube-VOS%0Adataset%2C%20which%20employs%20several%20more%20recent%20inpainting%20methods.%20The%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20in%20both%20in-domain%20and%20cross-domain%0Aevaluation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11054v3&entry.124074799=Read"},
{"title": "High-Dimensional Sparse Data Low-rank Representation via Accelerated\n  Asynchronous Parallel Stochastic Gradient Descent", "author": "Qicong Hu and Hao Wu", "abstract": "  Data characterized by high dimensionality and sparsity are commonly used to\ndescribe real-world node interactions. Low-rank representation (LR) can map\nhigh-dimensional sparse (HDS) data to low-dimensional feature spaces and infer\nnode interactions via modeling data latent associations. Unfortunately,\nexisting optimization algorithms for LR models are computationally inefficient\nand slowly convergent on large-scale datasets. To address this issue, this\npaper proposes an Accelerated Asynchronous Parallel Stochastic Gradient Descent\nA2PSGD for High-Dimensional Sparse Data Low-rank Representation with three\nfold-ideas: a) establishing a lock-free scheduler to simultaneously respond to\nscheduling requests from multiple threads; b) introducing a greedy\nalgorithm-based load balancing strategy for balancing the computational load\namong threads; c) incorporating Nesterov's accelerated gradient into the\nlearning scheme to accelerate model convergence. Empirical studies show that\nA2PSGD outperforms existing optimization algorithms for HDS data LR in both\naccuracy and training time.\n", "link": "http://arxiv.org/abs/2408.16592v1", "date": "2024-08-29", "relevancy": 2.7044, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.553}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5377}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Dimensional%20Sparse%20Data%20Low-rank%20Representation%20via%20Accelerated%0A%20%20Asynchronous%20Parallel%20Stochastic%20Gradient%20Descent&body=Title%3A%20High-Dimensional%20Sparse%20Data%20Low-rank%20Representation%20via%20Accelerated%0A%20%20Asynchronous%20Parallel%20Stochastic%20Gradient%20Descent%0AAuthor%3A%20Qicong%20Hu%20and%20Hao%20Wu%0AAbstract%3A%20%20%20Data%20characterized%20by%20high%20dimensionality%20and%20sparsity%20are%20commonly%20used%20to%0Adescribe%20real-world%20node%20interactions.%20Low-rank%20representation%20%28LR%29%20can%20map%0Ahigh-dimensional%20sparse%20%28HDS%29%20data%20to%20low-dimensional%20feature%20spaces%20and%20infer%0Anode%20interactions%20via%20modeling%20data%20latent%20associations.%20Unfortunately%2C%0Aexisting%20optimization%20algorithms%20for%20LR%20models%20are%20computationally%20inefficient%0Aand%20slowly%20convergent%20on%20large-scale%20datasets.%20To%20address%20this%20issue%2C%20this%0Apaper%20proposes%20an%20Accelerated%20Asynchronous%20Parallel%20Stochastic%20Gradient%20Descent%0AA2PSGD%20for%20High-Dimensional%20Sparse%20Data%20Low-rank%20Representation%20with%20three%0Afold-ideas%3A%20a%29%20establishing%20a%20lock-free%20scheduler%20to%20simultaneously%20respond%20to%0Ascheduling%20requests%20from%20multiple%20threads%3B%20b%29%20introducing%20a%20greedy%0Aalgorithm-based%20load%20balancing%20strategy%20for%20balancing%20the%20computational%20load%0Aamong%20threads%3B%20c%29%20incorporating%20Nesterov%27s%20accelerated%20gradient%20into%20the%0Alearning%20scheme%20to%20accelerate%20model%20convergence.%20Empirical%20studies%20show%20that%0AA2PSGD%20outperforms%20existing%20optimization%20algorithms%20for%20HDS%20data%20LR%20in%20both%0Aaccuracy%20and%20training%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Dimensional%2520Sparse%2520Data%2520Low-rank%2520Representation%2520via%2520Accelerated%250A%2520%2520Asynchronous%2520Parallel%2520Stochastic%2520Gradient%2520Descent%26entry.906535625%3DQicong%2520Hu%2520and%2520Hao%2520Wu%26entry.1292438233%3D%2520%2520Data%2520characterized%2520by%2520high%2520dimensionality%2520and%2520sparsity%2520are%2520commonly%2520used%2520to%250Adescribe%2520real-world%2520node%2520interactions.%2520Low-rank%2520representation%2520%2528LR%2529%2520can%2520map%250Ahigh-dimensional%2520sparse%2520%2528HDS%2529%2520data%2520to%2520low-dimensional%2520feature%2520spaces%2520and%2520infer%250Anode%2520interactions%2520via%2520modeling%2520data%2520latent%2520associations.%2520Unfortunately%252C%250Aexisting%2520optimization%2520algorithms%2520for%2520LR%2520models%2520are%2520computationally%2520inefficient%250Aand%2520slowly%2520convergent%2520on%2520large-scale%2520datasets.%2520To%2520address%2520this%2520issue%252C%2520this%250Apaper%2520proposes%2520an%2520Accelerated%2520Asynchronous%2520Parallel%2520Stochastic%2520Gradient%2520Descent%250AA2PSGD%2520for%2520High-Dimensional%2520Sparse%2520Data%2520Low-rank%2520Representation%2520with%2520three%250Afold-ideas%253A%2520a%2529%2520establishing%2520a%2520lock-free%2520scheduler%2520to%2520simultaneously%2520respond%2520to%250Ascheduling%2520requests%2520from%2520multiple%2520threads%253B%2520b%2529%2520introducing%2520a%2520greedy%250Aalgorithm-based%2520load%2520balancing%2520strategy%2520for%2520balancing%2520the%2520computational%2520load%250Aamong%2520threads%253B%2520c%2529%2520incorporating%2520Nesterov%2527s%2520accelerated%2520gradient%2520into%2520the%250Alearning%2520scheme%2520to%2520accelerate%2520model%2520convergence.%2520Empirical%2520studies%2520show%2520that%250AA2PSGD%2520outperforms%2520existing%2520optimization%2520algorithms%2520for%2520HDS%2520data%2520LR%2520in%2520both%250Aaccuracy%2520and%2520training%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Dimensional%20Sparse%20Data%20Low-rank%20Representation%20via%20Accelerated%0A%20%20Asynchronous%20Parallel%20Stochastic%20Gradient%20Descent&entry.906535625=Qicong%20Hu%20and%20Hao%20Wu&entry.1292438233=%20%20Data%20characterized%20by%20high%20dimensionality%20and%20sparsity%20are%20commonly%20used%20to%0Adescribe%20real-world%20node%20interactions.%20Low-rank%20representation%20%28LR%29%20can%20map%0Ahigh-dimensional%20sparse%20%28HDS%29%20data%20to%20low-dimensional%20feature%20spaces%20and%20infer%0Anode%20interactions%20via%20modeling%20data%20latent%20associations.%20Unfortunately%2C%0Aexisting%20optimization%20algorithms%20for%20LR%20models%20are%20computationally%20inefficient%0Aand%20slowly%20convergent%20on%20large-scale%20datasets.%20To%20address%20this%20issue%2C%20this%0Apaper%20proposes%20an%20Accelerated%20Asynchronous%20Parallel%20Stochastic%20Gradient%20Descent%0AA2PSGD%20for%20High-Dimensional%20Sparse%20Data%20Low-rank%20Representation%20with%20three%0Afold-ideas%3A%20a%29%20establishing%20a%20lock-free%20scheduler%20to%20simultaneously%20respond%20to%0Ascheduling%20requests%20from%20multiple%20threads%3B%20b%29%20introducing%20a%20greedy%0Aalgorithm-based%20load%20balancing%20strategy%20for%20balancing%20the%20computational%20load%0Aamong%20threads%3B%20c%29%20incorporating%20Nesterov%27s%20accelerated%20gradient%20into%20the%0Alearning%20scheme%20to%20accelerate%20model%20convergence.%20Empirical%20studies%20show%20that%0AA2PSGD%20outperforms%20existing%20optimization%20algorithms%20for%20HDS%20data%20LR%20in%20both%0Aaccuracy%20and%20training%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16592v1&entry.124074799=Read"},
{"title": "Locally Grouped and Scale-Guided Attention for Dense Pest Counting", "author": "Chang-Hwan Son", "abstract": "  This study introduces a new dense pest counting problem to predict densely\ndistributed pests captured by digital traps. Unlike traditional detection-based\ncounting models for sparsely distributed objects, trap-based pest counting must\ndeal with dense pest distributions that pose challenges such as severe\nocclusion, wide pose variation, and similar appearances in colors and textures.\nTo address these problems, it is essential to incorporate the local attention\nmechanism, which identifies locally important and unimportant areas to learn\nlocally grouped features, thereby enhancing discriminative performance.\nAccordingly, this study presents a novel design that integrates locally grouped\nand scale-guided attention into a multiscale CenterNet framework. To group\nlocal features with similar attributes, a straightforward method is introduced\nusing the heatmap predicted by the first hourglass containing pest centroid\ninformation, which eliminates the need for complex clustering models. To\nenhance attentiveness, the pixel attention module transforms the heatmap into a\nlearnable map. Subsequently, scale-guided attention is deployed to make the\nobject and background features more discriminative, achieving multiscale\nfeature fusion. Through experiments, the proposed model is verified to enhance\nobject features based on local grouping and discriminative feature attention\nlearning. Additionally, the proposed model is highly effective in overcoming\nocclusion and pose variation problems, making it more suitable for dense pest\ncounting. In particular, the proposed model outperforms state-of-the-art models\nby a large margin, with a remarkable contribution to dense pest counting.\n", "link": "http://arxiv.org/abs/2408.16503v1", "date": "2024-08-29", "relevancy": 2.6892, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5569}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5289}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locally%20Grouped%20and%20Scale-Guided%20Attention%20for%20Dense%20Pest%20Counting&body=Title%3A%20Locally%20Grouped%20and%20Scale-Guided%20Attention%20for%20Dense%20Pest%20Counting%0AAuthor%3A%20Chang-Hwan%20Son%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20new%20dense%20pest%20counting%20problem%20to%20predict%20densely%0Adistributed%20pests%20captured%20by%20digital%20traps.%20Unlike%20traditional%20detection-based%0Acounting%20models%20for%20sparsely%20distributed%20objects%2C%20trap-based%20pest%20counting%20must%0Adeal%20with%20dense%20pest%20distributions%20that%20pose%20challenges%20such%20as%20severe%0Aocclusion%2C%20wide%20pose%20variation%2C%20and%20similar%20appearances%20in%20colors%20and%20textures.%0ATo%20address%20these%20problems%2C%20it%20is%20essential%20to%20incorporate%20the%20local%20attention%0Amechanism%2C%20which%20identifies%20locally%20important%20and%20unimportant%20areas%20to%20learn%0Alocally%20grouped%20features%2C%20thereby%20enhancing%20discriminative%20performance.%0AAccordingly%2C%20this%20study%20presents%20a%20novel%20design%20that%20integrates%20locally%20grouped%0Aand%20scale-guided%20attention%20into%20a%20multiscale%20CenterNet%20framework.%20To%20group%0Alocal%20features%20with%20similar%20attributes%2C%20a%20straightforward%20method%20is%20introduced%0Ausing%20the%20heatmap%20predicted%20by%20the%20first%20hourglass%20containing%20pest%20centroid%0Ainformation%2C%20which%20eliminates%20the%20need%20for%20complex%20clustering%20models.%20To%0Aenhance%20attentiveness%2C%20the%20pixel%20attention%20module%20transforms%20the%20heatmap%20into%20a%0Alearnable%20map.%20Subsequently%2C%20scale-guided%20attention%20is%20deployed%20to%20make%20the%0Aobject%20and%20background%20features%20more%20discriminative%2C%20achieving%20multiscale%0Afeature%20fusion.%20Through%20experiments%2C%20the%20proposed%20model%20is%20verified%20to%20enhance%0Aobject%20features%20based%20on%20local%20grouping%20and%20discriminative%20feature%20attention%0Alearning.%20Additionally%2C%20the%20proposed%20model%20is%20highly%20effective%20in%20overcoming%0Aocclusion%20and%20pose%20variation%20problems%2C%20making%20it%20more%20suitable%20for%20dense%20pest%0Acounting.%20In%20particular%2C%20the%20proposed%20model%20outperforms%20state-of-the-art%20models%0Aby%20a%20large%20margin%2C%20with%20a%20remarkable%20contribution%20to%20dense%20pest%20counting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocally%2520Grouped%2520and%2520Scale-Guided%2520Attention%2520for%2520Dense%2520Pest%2520Counting%26entry.906535625%3DChang-Hwan%2520Son%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520new%2520dense%2520pest%2520counting%2520problem%2520to%2520predict%2520densely%250Adistributed%2520pests%2520captured%2520by%2520digital%2520traps.%2520Unlike%2520traditional%2520detection-based%250Acounting%2520models%2520for%2520sparsely%2520distributed%2520objects%252C%2520trap-based%2520pest%2520counting%2520must%250Adeal%2520with%2520dense%2520pest%2520distributions%2520that%2520pose%2520challenges%2520such%2520as%2520severe%250Aocclusion%252C%2520wide%2520pose%2520variation%252C%2520and%2520similar%2520appearances%2520in%2520colors%2520and%2520textures.%250ATo%2520address%2520these%2520problems%252C%2520it%2520is%2520essential%2520to%2520incorporate%2520the%2520local%2520attention%250Amechanism%252C%2520which%2520identifies%2520locally%2520important%2520and%2520unimportant%2520areas%2520to%2520learn%250Alocally%2520grouped%2520features%252C%2520thereby%2520enhancing%2520discriminative%2520performance.%250AAccordingly%252C%2520this%2520study%2520presents%2520a%2520novel%2520design%2520that%2520integrates%2520locally%2520grouped%250Aand%2520scale-guided%2520attention%2520into%2520a%2520multiscale%2520CenterNet%2520framework.%2520To%2520group%250Alocal%2520features%2520with%2520similar%2520attributes%252C%2520a%2520straightforward%2520method%2520is%2520introduced%250Ausing%2520the%2520heatmap%2520predicted%2520by%2520the%2520first%2520hourglass%2520containing%2520pest%2520centroid%250Ainformation%252C%2520which%2520eliminates%2520the%2520need%2520for%2520complex%2520clustering%2520models.%2520To%250Aenhance%2520attentiveness%252C%2520the%2520pixel%2520attention%2520module%2520transforms%2520the%2520heatmap%2520into%2520a%250Alearnable%2520map.%2520Subsequently%252C%2520scale-guided%2520attention%2520is%2520deployed%2520to%2520make%2520the%250Aobject%2520and%2520background%2520features%2520more%2520discriminative%252C%2520achieving%2520multiscale%250Afeature%2520fusion.%2520Through%2520experiments%252C%2520the%2520proposed%2520model%2520is%2520verified%2520to%2520enhance%250Aobject%2520features%2520based%2520on%2520local%2520grouping%2520and%2520discriminative%2520feature%2520attention%250Alearning.%2520Additionally%252C%2520the%2520proposed%2520model%2520is%2520highly%2520effective%2520in%2520overcoming%250Aocclusion%2520and%2520pose%2520variation%2520problems%252C%2520making%2520it%2520more%2520suitable%2520for%2520dense%2520pest%250Acounting.%2520In%2520particular%252C%2520the%2520proposed%2520model%2520outperforms%2520state-of-the-art%2520models%250Aby%2520a%2520large%2520margin%252C%2520with%2520a%2520remarkable%2520contribution%2520to%2520dense%2520pest%2520counting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locally%20Grouped%20and%20Scale-Guided%20Attention%20for%20Dense%20Pest%20Counting&entry.906535625=Chang-Hwan%20Son&entry.1292438233=%20%20This%20study%20introduces%20a%20new%20dense%20pest%20counting%20problem%20to%20predict%20densely%0Adistributed%20pests%20captured%20by%20digital%20traps.%20Unlike%20traditional%20detection-based%0Acounting%20models%20for%20sparsely%20distributed%20objects%2C%20trap-based%20pest%20counting%20must%0Adeal%20with%20dense%20pest%20distributions%20that%20pose%20challenges%20such%20as%20severe%0Aocclusion%2C%20wide%20pose%20variation%2C%20and%20similar%20appearances%20in%20colors%20and%20textures.%0ATo%20address%20these%20problems%2C%20it%20is%20essential%20to%20incorporate%20the%20local%20attention%0Amechanism%2C%20which%20identifies%20locally%20important%20and%20unimportant%20areas%20to%20learn%0Alocally%20grouped%20features%2C%20thereby%20enhancing%20discriminative%20performance.%0AAccordingly%2C%20this%20study%20presents%20a%20novel%20design%20that%20integrates%20locally%20grouped%0Aand%20scale-guided%20attention%20into%20a%20multiscale%20CenterNet%20framework.%20To%20group%0Alocal%20features%20with%20similar%20attributes%2C%20a%20straightforward%20method%20is%20introduced%0Ausing%20the%20heatmap%20predicted%20by%20the%20first%20hourglass%20containing%20pest%20centroid%0Ainformation%2C%20which%20eliminates%20the%20need%20for%20complex%20clustering%20models.%20To%0Aenhance%20attentiveness%2C%20the%20pixel%20attention%20module%20transforms%20the%20heatmap%20into%20a%0Alearnable%20map.%20Subsequently%2C%20scale-guided%20attention%20is%20deployed%20to%20make%20the%0Aobject%20and%20background%20features%20more%20discriminative%2C%20achieving%20multiscale%0Afeature%20fusion.%20Through%20experiments%2C%20the%20proposed%20model%20is%20verified%20to%20enhance%0Aobject%20features%20based%20on%20local%20grouping%20and%20discriminative%20feature%20attention%0Alearning.%20Additionally%2C%20the%20proposed%20model%20is%20highly%20effective%20in%20overcoming%0Aocclusion%20and%20pose%20variation%20problems%2C%20making%20it%20more%20suitable%20for%20dense%20pest%0Acounting.%20In%20particular%2C%20the%20proposed%20model%20outperforms%20state-of-the-art%20models%0Aby%20a%20large%20margin%2C%20with%20a%20remarkable%20contribution%20to%20dense%20pest%20counting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16503v1&entry.124074799=Read"},
{"title": "Enhancing Sound Source Localization via False Negative Elimination", "author": "Zengjie Song and Jiangshe Zhang and Yuxi Wang and Junsong Fan and Zhaoxiang Zhang", "abstract": "  Sound source localization aims to localize objects emitting the sound in\nvisual scenes. Recent works obtaining impressive results typically rely on\ncontrastive learning. However, the common practice of randomly sampling\nnegatives in prior arts can lead to the false negative issue, where the sounds\nsemantically similar to visual instance are sampled as negatives and\nincorrectly pushed away from the visual anchor/query. As a result, this\nmisalignment of audio and visual features could yield inferior performance. To\naddress this issue, we propose a novel audio-visual learning framework which is\ninstantiated with two individual learning schemes: self-supervised predictive\nlearning (SSPL) and semantic-aware contrastive learning (SACL). SSPL explores\nimage-audio positive pairs alone to discover semantically coherent similarities\nbetween audio and visual features, while a predictive coding module for feature\nalignment is introduced to facilitate the positive-only learning. In this\nregard SSPL acts as a negative-free method to eliminate false negatives. By\ncontrast, SACL is designed to compact visual features and remove false\nnegatives, providing reliable visual anchor and audio negatives for contrast.\nDifferent from SSPL, SACL releases the potential of audio-visual contrastive\nlearning, offering an effective alternative to achieve the same goal.\nComprehensive experiments demonstrate the superiority of our approach over the\nstate-of-the-arts. Furthermore, we highlight the versatility of the learned\nrepresentation by extending the approach to audio-visual event classification\nand object detection tasks. Code and models are available at:\nhttps://github.com/zjsong/SACL.\n", "link": "http://arxiv.org/abs/2408.16448v1", "date": "2024-08-29", "relevancy": 2.6867, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5514}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5331}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Sound%20Source%20Localization%20via%20False%20Negative%20Elimination&body=Title%3A%20Enhancing%20Sound%20Source%20Localization%20via%20False%20Negative%20Elimination%0AAuthor%3A%20Zengjie%20Song%20and%20Jiangshe%20Zhang%20and%20Yuxi%20Wang%20and%20Junsong%20Fan%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Sound%20source%20localization%20aims%20to%20localize%20objects%20emitting%20the%20sound%20in%0Avisual%20scenes.%20Recent%20works%20obtaining%20impressive%20results%20typically%20rely%20on%0Acontrastive%20learning.%20However%2C%20the%20common%20practice%20of%20randomly%20sampling%0Anegatives%20in%20prior%20arts%20can%20lead%20to%20the%20false%20negative%20issue%2C%20where%20the%20sounds%0Asemantically%20similar%20to%20visual%20instance%20are%20sampled%20as%20negatives%20and%0Aincorrectly%20pushed%20away%20from%20the%20visual%20anchor/query.%20As%20a%20result%2C%20this%0Amisalignment%20of%20audio%20and%20visual%20features%20could%20yield%20inferior%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20audio-visual%20learning%20framework%20which%20is%0Ainstantiated%20with%20two%20individual%20learning%20schemes%3A%20self-supervised%20predictive%0Alearning%20%28SSPL%29%20and%20semantic-aware%20contrastive%20learning%20%28SACL%29.%20SSPL%20explores%0Aimage-audio%20positive%20pairs%20alone%20to%20discover%20semantically%20coherent%20similarities%0Abetween%20audio%20and%20visual%20features%2C%20while%20a%20predictive%20coding%20module%20for%20feature%0Aalignment%20is%20introduced%20to%20facilitate%20the%20positive-only%20learning.%20In%20this%0Aregard%20SSPL%20acts%20as%20a%20negative-free%20method%20to%20eliminate%20false%20negatives.%20By%0Acontrast%2C%20SACL%20is%20designed%20to%20compact%20visual%20features%20and%20remove%20false%0Anegatives%2C%20providing%20reliable%20visual%20anchor%20and%20audio%20negatives%20for%20contrast.%0ADifferent%20from%20SSPL%2C%20SACL%20releases%20the%20potential%20of%20audio-visual%20contrastive%0Alearning%2C%20offering%20an%20effective%20alternative%20to%20achieve%20the%20same%20goal.%0AComprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20the%0Astate-of-the-arts.%20Furthermore%2C%20we%20highlight%20the%20versatility%20of%20the%20learned%0Arepresentation%20by%20extending%20the%20approach%20to%20audio-visual%20event%20classification%0Aand%20object%20detection%20tasks.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/zjsong/SACL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Sound%2520Source%2520Localization%2520via%2520False%2520Negative%2520Elimination%26entry.906535625%3DZengjie%2520Song%2520and%2520Jiangshe%2520Zhang%2520and%2520Yuxi%2520Wang%2520and%2520Junsong%2520Fan%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Sound%2520source%2520localization%2520aims%2520to%2520localize%2520objects%2520emitting%2520the%2520sound%2520in%250Avisual%2520scenes.%2520Recent%2520works%2520obtaining%2520impressive%2520results%2520typically%2520rely%2520on%250Acontrastive%2520learning.%2520However%252C%2520the%2520common%2520practice%2520of%2520randomly%2520sampling%250Anegatives%2520in%2520prior%2520arts%2520can%2520lead%2520to%2520the%2520false%2520negative%2520issue%252C%2520where%2520the%2520sounds%250Asemantically%2520similar%2520to%2520visual%2520instance%2520are%2520sampled%2520as%2520negatives%2520and%250Aincorrectly%2520pushed%2520away%2520from%2520the%2520visual%2520anchor/query.%2520As%2520a%2520result%252C%2520this%250Amisalignment%2520of%2520audio%2520and%2520visual%2520features%2520could%2520yield%2520inferior%2520performance.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520audio-visual%2520learning%2520framework%2520which%2520is%250Ainstantiated%2520with%2520two%2520individual%2520learning%2520schemes%253A%2520self-supervised%2520predictive%250Alearning%2520%2528SSPL%2529%2520and%2520semantic-aware%2520contrastive%2520learning%2520%2528SACL%2529.%2520SSPL%2520explores%250Aimage-audio%2520positive%2520pairs%2520alone%2520to%2520discover%2520semantically%2520coherent%2520similarities%250Abetween%2520audio%2520and%2520visual%2520features%252C%2520while%2520a%2520predictive%2520coding%2520module%2520for%2520feature%250Aalignment%2520is%2520introduced%2520to%2520facilitate%2520the%2520positive-only%2520learning.%2520In%2520this%250Aregard%2520SSPL%2520acts%2520as%2520a%2520negative-free%2520method%2520to%2520eliminate%2520false%2520negatives.%2520By%250Acontrast%252C%2520SACL%2520is%2520designed%2520to%2520compact%2520visual%2520features%2520and%2520remove%2520false%250Anegatives%252C%2520providing%2520reliable%2520visual%2520anchor%2520and%2520audio%2520negatives%2520for%2520contrast.%250ADifferent%2520from%2520SSPL%252C%2520SACL%2520releases%2520the%2520potential%2520of%2520audio-visual%2520contrastive%250Alearning%252C%2520offering%2520an%2520effective%2520alternative%2520to%2520achieve%2520the%2520same%2520goal.%250AComprehensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520the%250Astate-of-the-arts.%2520Furthermore%252C%2520we%2520highlight%2520the%2520versatility%2520of%2520the%2520learned%250Arepresentation%2520by%2520extending%2520the%2520approach%2520to%2520audio-visual%2520event%2520classification%250Aand%2520object%2520detection%2520tasks.%2520Code%2520and%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/zjsong/SACL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Sound%20Source%20Localization%20via%20False%20Negative%20Elimination&entry.906535625=Zengjie%20Song%20and%20Jiangshe%20Zhang%20and%20Yuxi%20Wang%20and%20Junsong%20Fan%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Sound%20source%20localization%20aims%20to%20localize%20objects%20emitting%20the%20sound%20in%0Avisual%20scenes.%20Recent%20works%20obtaining%20impressive%20results%20typically%20rely%20on%0Acontrastive%20learning.%20However%2C%20the%20common%20practice%20of%20randomly%20sampling%0Anegatives%20in%20prior%20arts%20can%20lead%20to%20the%20false%20negative%20issue%2C%20where%20the%20sounds%0Asemantically%20similar%20to%20visual%20instance%20are%20sampled%20as%20negatives%20and%0Aincorrectly%20pushed%20away%20from%20the%20visual%20anchor/query.%20As%20a%20result%2C%20this%0Amisalignment%20of%20audio%20and%20visual%20features%20could%20yield%20inferior%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20audio-visual%20learning%20framework%20which%20is%0Ainstantiated%20with%20two%20individual%20learning%20schemes%3A%20self-supervised%20predictive%0Alearning%20%28SSPL%29%20and%20semantic-aware%20contrastive%20learning%20%28SACL%29.%20SSPL%20explores%0Aimage-audio%20positive%20pairs%20alone%20to%20discover%20semantically%20coherent%20similarities%0Abetween%20audio%20and%20visual%20features%2C%20while%20a%20predictive%20coding%20module%20for%20feature%0Aalignment%20is%20introduced%20to%20facilitate%20the%20positive-only%20learning.%20In%20this%0Aregard%20SSPL%20acts%20as%20a%20negative-free%20method%20to%20eliminate%20false%20negatives.%20By%0Acontrast%2C%20SACL%20is%20designed%20to%20compact%20visual%20features%20and%20remove%20false%0Anegatives%2C%20providing%20reliable%20visual%20anchor%20and%20audio%20negatives%20for%20contrast.%0ADifferent%20from%20SSPL%2C%20SACL%20releases%20the%20potential%20of%20audio-visual%20contrastive%0Alearning%2C%20offering%20an%20effective%20alternative%20to%20achieve%20the%20same%20goal.%0AComprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20the%0Astate-of-the-arts.%20Furthermore%2C%20we%20highlight%20the%20versatility%20of%20the%20learned%0Arepresentation%20by%20extending%20the%20approach%20to%20audio-visual%20event%20classification%0Aand%20object%20detection%20tasks.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/zjsong/SACL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16448v1&entry.124074799=Read"},
{"title": "Blending Low and High-Level Semantics of Time Series for Better Masked\n  Time Series Generation", "author": "Johan Vik Mathisen and Erlend Lokna and Daesoo Lee and Erlend Aune", "abstract": "  State-of-the-art approaches in time series generation (TSG), such as\nTimeVQVAE, utilize vector quantization-based tokenization to effectively model\ncomplex distributions of time series. These approaches first learn to transform\ntime series into a sequence of discrete latent vectors, and then a prior model\nis learned to model the sequence. The discrete latent vectors, however, only\ncapture low-level semantics (\\textit{e.g.,} shapes). We hypothesize that\nhigher-fidelity time series can be generated by training a prior model on more\ninformative discrete latent vectors that contain both low and high-level\nsemantics (\\textit{e.g.,} characteristic dynamics). In this paper, we introduce\na novel framework, termed NC-VQVAE, to integrate self-supervised learning into\nthose TSG methods to derive a discrete latent space where low and high-level\nsemantics are captured. Our experimental results demonstrate that NC-VQVAE\nresults in a considerable improvement in the quality of synthetic samples.\n", "link": "http://arxiv.org/abs/2408.16613v1", "date": "2024-08-29", "relevancy": 2.6696, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5545}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5295}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blending%20Low%20and%20High-Level%20Semantics%20of%20Time%20Series%20for%20Better%20Masked%0A%20%20Time%20Series%20Generation&body=Title%3A%20Blending%20Low%20and%20High-Level%20Semantics%20of%20Time%20Series%20for%20Better%20Masked%0A%20%20Time%20Series%20Generation%0AAuthor%3A%20Johan%20Vik%20Mathisen%20and%20Erlend%20Lokna%20and%20Daesoo%20Lee%20and%20Erlend%20Aune%0AAbstract%3A%20%20%20State-of-the-art%20approaches%20in%20time%20series%20generation%20%28TSG%29%2C%20such%20as%0ATimeVQVAE%2C%20utilize%20vector%20quantization-based%20tokenization%20to%20effectively%20model%0Acomplex%20distributions%20of%20time%20series.%20These%20approaches%20first%20learn%20to%20transform%0Atime%20series%20into%20a%20sequence%20of%20discrete%20latent%20vectors%2C%20and%20then%20a%20prior%20model%0Ais%20learned%20to%20model%20the%20sequence.%20The%20discrete%20latent%20vectors%2C%20however%2C%20only%0Acapture%20low-level%20semantics%20%28%5Ctextit%7Be.g.%2C%7D%20shapes%29.%20We%20hypothesize%20that%0Ahigher-fidelity%20time%20series%20can%20be%20generated%20by%20training%20a%20prior%20model%20on%20more%0Ainformative%20discrete%20latent%20vectors%20that%20contain%20both%20low%20and%20high-level%0Asemantics%20%28%5Ctextit%7Be.g.%2C%7D%20characteristic%20dynamics%29.%20In%20this%20paper%2C%20we%20introduce%0Aa%20novel%20framework%2C%20termed%20NC-VQVAE%2C%20to%20integrate%20self-supervised%20learning%20into%0Athose%20TSG%20methods%20to%20derive%20a%20discrete%20latent%20space%20where%20low%20and%20high-level%0Asemantics%20are%20captured.%20Our%20experimental%20results%20demonstrate%20that%20NC-VQVAE%0Aresults%20in%20a%20considerable%20improvement%20in%20the%20quality%20of%20synthetic%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlending%2520Low%2520and%2520High-Level%2520Semantics%2520of%2520Time%2520Series%2520for%2520Better%2520Masked%250A%2520%2520Time%2520Series%2520Generation%26entry.906535625%3DJohan%2520Vik%2520Mathisen%2520and%2520Erlend%2520Lokna%2520and%2520Daesoo%2520Lee%2520and%2520Erlend%2520Aune%26entry.1292438233%3D%2520%2520State-of-the-art%2520approaches%2520in%2520time%2520series%2520generation%2520%2528TSG%2529%252C%2520such%2520as%250ATimeVQVAE%252C%2520utilize%2520vector%2520quantization-based%2520tokenization%2520to%2520effectively%2520model%250Acomplex%2520distributions%2520of%2520time%2520series.%2520These%2520approaches%2520first%2520learn%2520to%2520transform%250Atime%2520series%2520into%2520a%2520sequence%2520of%2520discrete%2520latent%2520vectors%252C%2520and%2520then%2520a%2520prior%2520model%250Ais%2520learned%2520to%2520model%2520the%2520sequence.%2520The%2520discrete%2520latent%2520vectors%252C%2520however%252C%2520only%250Acapture%2520low-level%2520semantics%2520%2528%255Ctextit%257Be.g.%252C%257D%2520shapes%2529.%2520We%2520hypothesize%2520that%250Ahigher-fidelity%2520time%2520series%2520can%2520be%2520generated%2520by%2520training%2520a%2520prior%2520model%2520on%2520more%250Ainformative%2520discrete%2520latent%2520vectors%2520that%2520contain%2520both%2520low%2520and%2520high-level%250Asemantics%2520%2528%255Ctextit%257Be.g.%252C%257D%2520characteristic%2520dynamics%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Aa%2520novel%2520framework%252C%2520termed%2520NC-VQVAE%252C%2520to%2520integrate%2520self-supervised%2520learning%2520into%250Athose%2520TSG%2520methods%2520to%2520derive%2520a%2520discrete%2520latent%2520space%2520where%2520low%2520and%2520high-level%250Asemantics%2520are%2520captured.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520NC-VQVAE%250Aresults%2520in%2520a%2520considerable%2520improvement%2520in%2520the%2520quality%2520of%2520synthetic%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blending%20Low%20and%20High-Level%20Semantics%20of%20Time%20Series%20for%20Better%20Masked%0A%20%20Time%20Series%20Generation&entry.906535625=Johan%20Vik%20Mathisen%20and%20Erlend%20Lokna%20and%20Daesoo%20Lee%20and%20Erlend%20Aune&entry.1292438233=%20%20State-of-the-art%20approaches%20in%20time%20series%20generation%20%28TSG%29%2C%20such%20as%0ATimeVQVAE%2C%20utilize%20vector%20quantization-based%20tokenization%20to%20effectively%20model%0Acomplex%20distributions%20of%20time%20series.%20These%20approaches%20first%20learn%20to%20transform%0Atime%20series%20into%20a%20sequence%20of%20discrete%20latent%20vectors%2C%20and%20then%20a%20prior%20model%0Ais%20learned%20to%20model%20the%20sequence.%20The%20discrete%20latent%20vectors%2C%20however%2C%20only%0Acapture%20low-level%20semantics%20%28%5Ctextit%7Be.g.%2C%7D%20shapes%29.%20We%20hypothesize%20that%0Ahigher-fidelity%20time%20series%20can%20be%20generated%20by%20training%20a%20prior%20model%20on%20more%0Ainformative%20discrete%20latent%20vectors%20that%20contain%20both%20low%20and%20high-level%0Asemantics%20%28%5Ctextit%7Be.g.%2C%7D%20characteristic%20dynamics%29.%20In%20this%20paper%2C%20we%20introduce%0Aa%20novel%20framework%2C%20termed%20NC-VQVAE%2C%20to%20integrate%20self-supervised%20learning%20into%0Athose%20TSG%20methods%20to%20derive%20a%20discrete%20latent%20space%20where%20low%20and%20high-level%0Asemantics%20are%20captured.%20Our%20experimental%20results%20demonstrate%20that%20NC-VQVAE%0Aresults%20in%20a%20considerable%20improvement%20in%20the%20quality%20of%20synthetic%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16613v1&entry.124074799=Read"},
{"title": "Space3D-Bench: Spatial 3D Question Answering Benchmark", "author": "Emilia Szymanska and Mihai Dusmanu and Jan-Willem Buurlage and Mahdi Rad and Marc Pollefeys", "abstract": "  Answering questions about the spatial properties of the environment poses\nchallenges for existing language and vision foundation models due to a lack of\nunderstanding of the 3D world notably in terms of relationships between\nobjects. To push the field forward, multiple 3D Q&A datasets were proposed\nwhich, overall, provide a variety of questions, but they individually focus on\nparticular aspects of 3D reasoning or are limited in terms of data modalities.\nTo address this, we present Space3D-Bench - a collection of 1000 general\nspatial questions and answers related to scenes of the Replica dataset which\noffers a variety of data modalities: point clouds, posed RGB-D images,\nnavigation meshes and 3D object detections. To ensure that the questions cover\na wide range of 3D objectives, we propose an indoor spatial questions taxonomy\ninspired by geographic information systems and use it to balance the dataset\naccordingly. Moreover, we provide an assessment system that grades natural\nlanguage responses based on predefined ground-truth answers by leveraging a\nVision Language Model's comprehension of both text and images to compare the\nresponses with ground-truth textual information or relevant visual data.\nFinally, we introduce a baseline called RAG3D-Chat integrating the world\nunderstanding of foundation models with rich context retrieval, achieving an\naccuracy of 67% on the proposed dataset.\n", "link": "http://arxiv.org/abs/2408.16662v1", "date": "2024-08-29", "relevancy": 2.6517, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5342}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5342}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Space3D-Bench%3A%20Spatial%203D%20Question%20Answering%20Benchmark&body=Title%3A%20Space3D-Bench%3A%20Spatial%203D%20Question%20Answering%20Benchmark%0AAuthor%3A%20Emilia%20Szymanska%20and%20Mihai%20Dusmanu%20and%20Jan-Willem%20Buurlage%20and%20Mahdi%20Rad%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20Answering%20questions%20about%20the%20spatial%20properties%20of%20the%20environment%20poses%0Achallenges%20for%20existing%20language%20and%20vision%20foundation%20models%20due%20to%20a%20lack%20of%0Aunderstanding%20of%20the%203D%20world%20notably%20in%20terms%20of%20relationships%20between%0Aobjects.%20To%20push%20the%20field%20forward%2C%20multiple%203D%20Q%26A%20datasets%20were%20proposed%0Awhich%2C%20overall%2C%20provide%20a%20variety%20of%20questions%2C%20but%20they%20individually%20focus%20on%0Aparticular%20aspects%20of%203D%20reasoning%20or%20are%20limited%20in%20terms%20of%20data%20modalities.%0ATo%20address%20this%2C%20we%20present%20Space3D-Bench%20-%20a%20collection%20of%201000%20general%0Aspatial%20questions%20and%20answers%20related%20to%20scenes%20of%20the%20Replica%20dataset%20which%0Aoffers%20a%20variety%20of%20data%20modalities%3A%20point%20clouds%2C%20posed%20RGB-D%20images%2C%0Anavigation%20meshes%20and%203D%20object%20detections.%20To%20ensure%20that%20the%20questions%20cover%0Aa%20wide%20range%20of%203D%20objectives%2C%20we%20propose%20an%20indoor%20spatial%20questions%20taxonomy%0Ainspired%20by%20geographic%20information%20systems%20and%20use%20it%20to%20balance%20the%20dataset%0Aaccordingly.%20Moreover%2C%20we%20provide%20an%20assessment%20system%20that%20grades%20natural%0Alanguage%20responses%20based%20on%20predefined%20ground-truth%20answers%20by%20leveraging%20a%0AVision%20Language%20Model%27s%20comprehension%20of%20both%20text%20and%20images%20to%20compare%20the%0Aresponses%20with%20ground-truth%20textual%20information%20or%20relevant%20visual%20data.%0AFinally%2C%20we%20introduce%20a%20baseline%20called%20RAG3D-Chat%20integrating%20the%20world%0Aunderstanding%20of%20foundation%20models%20with%20rich%20context%20retrieval%2C%20achieving%20an%0Aaccuracy%20of%2067%25%20on%20the%20proposed%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpace3D-Bench%253A%2520Spatial%25203D%2520Question%2520Answering%2520Benchmark%26entry.906535625%3DEmilia%2520Szymanska%2520and%2520Mihai%2520Dusmanu%2520and%2520Jan-Willem%2520Buurlage%2520and%2520Mahdi%2520Rad%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520Answering%2520questions%2520about%2520the%2520spatial%2520properties%2520of%2520the%2520environment%2520poses%250Achallenges%2520for%2520existing%2520language%2520and%2520vision%2520foundation%2520models%2520due%2520to%2520a%2520lack%2520of%250Aunderstanding%2520of%2520the%25203D%2520world%2520notably%2520in%2520terms%2520of%2520relationships%2520between%250Aobjects.%2520To%2520push%2520the%2520field%2520forward%252C%2520multiple%25203D%2520Q%2526A%2520datasets%2520were%2520proposed%250Awhich%252C%2520overall%252C%2520provide%2520a%2520variety%2520of%2520questions%252C%2520but%2520they%2520individually%2520focus%2520on%250Aparticular%2520aspects%2520of%25203D%2520reasoning%2520or%2520are%2520limited%2520in%2520terms%2520of%2520data%2520modalities.%250ATo%2520address%2520this%252C%2520we%2520present%2520Space3D-Bench%2520-%2520a%2520collection%2520of%25201000%2520general%250Aspatial%2520questions%2520and%2520answers%2520related%2520to%2520scenes%2520of%2520the%2520Replica%2520dataset%2520which%250Aoffers%2520a%2520variety%2520of%2520data%2520modalities%253A%2520point%2520clouds%252C%2520posed%2520RGB-D%2520images%252C%250Anavigation%2520meshes%2520and%25203D%2520object%2520detections.%2520To%2520ensure%2520that%2520the%2520questions%2520cover%250Aa%2520wide%2520range%2520of%25203D%2520objectives%252C%2520we%2520propose%2520an%2520indoor%2520spatial%2520questions%2520taxonomy%250Ainspired%2520by%2520geographic%2520information%2520systems%2520and%2520use%2520it%2520to%2520balance%2520the%2520dataset%250Aaccordingly.%2520Moreover%252C%2520we%2520provide%2520an%2520assessment%2520system%2520that%2520grades%2520natural%250Alanguage%2520responses%2520based%2520on%2520predefined%2520ground-truth%2520answers%2520by%2520leveraging%2520a%250AVision%2520Language%2520Model%2527s%2520comprehension%2520of%2520both%2520text%2520and%2520images%2520to%2520compare%2520the%250Aresponses%2520with%2520ground-truth%2520textual%2520information%2520or%2520relevant%2520visual%2520data.%250AFinally%252C%2520we%2520introduce%2520a%2520baseline%2520called%2520RAG3D-Chat%2520integrating%2520the%2520world%250Aunderstanding%2520of%2520foundation%2520models%2520with%2520rich%2520context%2520retrieval%252C%2520achieving%2520an%250Aaccuracy%2520of%252067%2525%2520on%2520the%2520proposed%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Space3D-Bench%3A%20Spatial%203D%20Question%20Answering%20Benchmark&entry.906535625=Emilia%20Szymanska%20and%20Mihai%20Dusmanu%20and%20Jan-Willem%20Buurlage%20and%20Mahdi%20Rad%20and%20Marc%20Pollefeys&entry.1292438233=%20%20Answering%20questions%20about%20the%20spatial%20properties%20of%20the%20environment%20poses%0Achallenges%20for%20existing%20language%20and%20vision%20foundation%20models%20due%20to%20a%20lack%20of%0Aunderstanding%20of%20the%203D%20world%20notably%20in%20terms%20of%20relationships%20between%0Aobjects.%20To%20push%20the%20field%20forward%2C%20multiple%203D%20Q%26A%20datasets%20were%20proposed%0Awhich%2C%20overall%2C%20provide%20a%20variety%20of%20questions%2C%20but%20they%20individually%20focus%20on%0Aparticular%20aspects%20of%203D%20reasoning%20or%20are%20limited%20in%20terms%20of%20data%20modalities.%0ATo%20address%20this%2C%20we%20present%20Space3D-Bench%20-%20a%20collection%20of%201000%20general%0Aspatial%20questions%20and%20answers%20related%20to%20scenes%20of%20the%20Replica%20dataset%20which%0Aoffers%20a%20variety%20of%20data%20modalities%3A%20point%20clouds%2C%20posed%20RGB-D%20images%2C%0Anavigation%20meshes%20and%203D%20object%20detections.%20To%20ensure%20that%20the%20questions%20cover%0Aa%20wide%20range%20of%203D%20objectives%2C%20we%20propose%20an%20indoor%20spatial%20questions%20taxonomy%0Ainspired%20by%20geographic%20information%20systems%20and%20use%20it%20to%20balance%20the%20dataset%0Aaccordingly.%20Moreover%2C%20we%20provide%20an%20assessment%20system%20that%20grades%20natural%0Alanguage%20responses%20based%20on%20predefined%20ground-truth%20answers%20by%20leveraging%20a%0AVision%20Language%20Model%27s%20comprehension%20of%20both%20text%20and%20images%20to%20compare%20the%0Aresponses%20with%20ground-truth%20textual%20information%20or%20relevant%20visual%20data.%0AFinally%2C%20we%20introduce%20a%20baseline%20called%20RAG3D-Chat%20integrating%20the%20world%0Aunderstanding%20of%20foundation%20models%20with%20rich%20context%20retrieval%2C%20achieving%20an%0Aaccuracy%20of%2067%25%20on%20the%20proposed%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16662v1&entry.124074799=Read"},
{"title": "A comparison between humans and AI at recognizing objects in unusual\n  poses", "author": "Netta Ollikka and Amro Abbas and Andrea Perin and Markku Kilpel\u00e4inen and St\u00e9phane Deny", "abstract": "  Deep learning is closing the gap with human vision on several object\nrecognition benchmarks. Here we investigate this gap for challenging images\nwhere objects are seen in unusual poses. We find that humans excel at\nrecognizing objects in such poses. In contrast, state-of-the-art deep networks\nfor vision (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) and state-of-the-art\nlarge vision-language models (Claude 3.5, Gemini 1.5, GPT-4) are systematically\nbrittle on unusual poses, with the exception of Gemini showing excellent\nrobustness in that condition. As we limit image exposure time, human\nperformance degrades to the level of deep networks, suggesting that additional\nmental processes (requiring additional time) are necessary to identify objects\nin unusual poses. An analysis of error patterns of humans vs. networks reveals\nthat even time-limited humans are dissimilar to feed-forward deep networks. In\nconclusion, our comparison reveals that humans and deep networks rely on\ndifferent mechanisms for recognizing objects in unusual poses. Understanding\nthe nature of the mental processes taking place during extra viewing time may\nbe key to reproduce the robustness of human vision in silico.\n", "link": "http://arxiv.org/abs/2402.03973v2", "date": "2024-08-29", "relevancy": 2.6502, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5563}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5211}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20comparison%20between%20humans%20and%20AI%20at%20recognizing%20objects%20in%20unusual%0A%20%20poses&body=Title%3A%20A%20comparison%20between%20humans%20and%20AI%20at%20recognizing%20objects%20in%20unusual%0A%20%20poses%0AAuthor%3A%20Netta%20Ollikka%20and%20Amro%20Abbas%20and%20Andrea%20Perin%20and%20Markku%20Kilpel%C3%A4inen%20and%20St%C3%A9phane%20Deny%0AAbstract%3A%20%20%20Deep%20learning%20is%20closing%20the%20gap%20with%20human%20vision%20on%20several%20object%0Arecognition%20benchmarks.%20Here%20we%20investigate%20this%20gap%20for%20challenging%20images%0Awhere%20objects%20are%20seen%20in%20unusual%20poses.%20We%20find%20that%20humans%20excel%20at%0Arecognizing%20objects%20in%20such%20poses.%20In%20contrast%2C%20state-of-the-art%20deep%20networks%0Afor%20vision%20%28EfficientNet%2C%20SWAG%2C%20ViT%2C%20SWIN%2C%20BEiT%2C%20ConvNext%29%20and%20state-of-the-art%0Alarge%20vision-language%20models%20%28Claude%203.5%2C%20Gemini%201.5%2C%20GPT-4%29%20are%20systematically%0Abrittle%20on%20unusual%20poses%2C%20with%20the%20exception%20of%20Gemini%20showing%20excellent%0Arobustness%20in%20that%20condition.%20As%20we%20limit%20image%20exposure%20time%2C%20human%0Aperformance%20degrades%20to%20the%20level%20of%20deep%20networks%2C%20suggesting%20that%20additional%0Amental%20processes%20%28requiring%20additional%20time%29%20are%20necessary%20to%20identify%20objects%0Ain%20unusual%20poses.%20An%20analysis%20of%20error%20patterns%20of%20humans%20vs.%20networks%20reveals%0Athat%20even%20time-limited%20humans%20are%20dissimilar%20to%20feed-forward%20deep%20networks.%20In%0Aconclusion%2C%20our%20comparison%20reveals%20that%20humans%20and%20deep%20networks%20rely%20on%0Adifferent%20mechanisms%20for%20recognizing%20objects%20in%20unusual%20poses.%20Understanding%0Athe%20nature%20of%20the%20mental%20processes%20taking%20place%20during%20extra%20viewing%20time%20may%0Abe%20key%20to%20reproduce%20the%20robustness%20of%20human%20vision%20in%20silico.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03973v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520comparison%2520between%2520humans%2520and%2520AI%2520at%2520recognizing%2520objects%2520in%2520unusual%250A%2520%2520poses%26entry.906535625%3DNetta%2520Ollikka%2520and%2520Amro%2520Abbas%2520and%2520Andrea%2520Perin%2520and%2520Markku%2520Kilpel%25C3%25A4inen%2520and%2520St%25C3%25A9phane%2520Deny%26entry.1292438233%3D%2520%2520Deep%2520learning%2520is%2520closing%2520the%2520gap%2520with%2520human%2520vision%2520on%2520several%2520object%250Arecognition%2520benchmarks.%2520Here%2520we%2520investigate%2520this%2520gap%2520for%2520challenging%2520images%250Awhere%2520objects%2520are%2520seen%2520in%2520unusual%2520poses.%2520We%2520find%2520that%2520humans%2520excel%2520at%250Arecognizing%2520objects%2520in%2520such%2520poses.%2520In%2520contrast%252C%2520state-of-the-art%2520deep%2520networks%250Afor%2520vision%2520%2528EfficientNet%252C%2520SWAG%252C%2520ViT%252C%2520SWIN%252C%2520BEiT%252C%2520ConvNext%2529%2520and%2520state-of-the-art%250Alarge%2520vision-language%2520models%2520%2528Claude%25203.5%252C%2520Gemini%25201.5%252C%2520GPT-4%2529%2520are%2520systematically%250Abrittle%2520on%2520unusual%2520poses%252C%2520with%2520the%2520exception%2520of%2520Gemini%2520showing%2520excellent%250Arobustness%2520in%2520that%2520condition.%2520As%2520we%2520limit%2520image%2520exposure%2520time%252C%2520human%250Aperformance%2520degrades%2520to%2520the%2520level%2520of%2520deep%2520networks%252C%2520suggesting%2520that%2520additional%250Amental%2520processes%2520%2528requiring%2520additional%2520time%2529%2520are%2520necessary%2520to%2520identify%2520objects%250Ain%2520unusual%2520poses.%2520An%2520analysis%2520of%2520error%2520patterns%2520of%2520humans%2520vs.%2520networks%2520reveals%250Athat%2520even%2520time-limited%2520humans%2520are%2520dissimilar%2520to%2520feed-forward%2520deep%2520networks.%2520In%250Aconclusion%252C%2520our%2520comparison%2520reveals%2520that%2520humans%2520and%2520deep%2520networks%2520rely%2520on%250Adifferent%2520mechanisms%2520for%2520recognizing%2520objects%2520in%2520unusual%2520poses.%2520Understanding%250Athe%2520nature%2520of%2520the%2520mental%2520processes%2520taking%2520place%2520during%2520extra%2520viewing%2520time%2520may%250Abe%2520key%2520to%2520reproduce%2520the%2520robustness%2520of%2520human%2520vision%2520in%2520silico.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03973v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comparison%20between%20humans%20and%20AI%20at%20recognizing%20objects%20in%20unusual%0A%20%20poses&entry.906535625=Netta%20Ollikka%20and%20Amro%20Abbas%20and%20Andrea%20Perin%20and%20Markku%20Kilpel%C3%A4inen%20and%20St%C3%A9phane%20Deny&entry.1292438233=%20%20Deep%20learning%20is%20closing%20the%20gap%20with%20human%20vision%20on%20several%20object%0Arecognition%20benchmarks.%20Here%20we%20investigate%20this%20gap%20for%20challenging%20images%0Awhere%20objects%20are%20seen%20in%20unusual%20poses.%20We%20find%20that%20humans%20excel%20at%0Arecognizing%20objects%20in%20such%20poses.%20In%20contrast%2C%20state-of-the-art%20deep%20networks%0Afor%20vision%20%28EfficientNet%2C%20SWAG%2C%20ViT%2C%20SWIN%2C%20BEiT%2C%20ConvNext%29%20and%20state-of-the-art%0Alarge%20vision-language%20models%20%28Claude%203.5%2C%20Gemini%201.5%2C%20GPT-4%29%20are%20systematically%0Abrittle%20on%20unusual%20poses%2C%20with%20the%20exception%20of%20Gemini%20showing%20excellent%0Arobustness%20in%20that%20condition.%20As%20we%20limit%20image%20exposure%20time%2C%20human%0Aperformance%20degrades%20to%20the%20level%20of%20deep%20networks%2C%20suggesting%20that%20additional%0Amental%20processes%20%28requiring%20additional%20time%29%20are%20necessary%20to%20identify%20objects%0Ain%20unusual%20poses.%20An%20analysis%20of%20error%20patterns%20of%20humans%20vs.%20networks%20reveals%0Athat%20even%20time-limited%20humans%20are%20dissimilar%20to%20feed-forward%20deep%20networks.%20In%0Aconclusion%2C%20our%20comparison%20reveals%20that%20humans%20and%20deep%20networks%20rely%20on%0Adifferent%20mechanisms%20for%20recognizing%20objects%20in%20unusual%20poses.%20Understanding%0Athe%20nature%20of%20the%20mental%20processes%20taking%20place%20during%20extra%20viewing%20time%20may%0Abe%20key%20to%20reproduce%20the%20robustness%20of%20human%20vision%20in%20silico.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03973v2&entry.124074799=Read"},
{"title": "3D Pose-Based Temporal Action Segmentation for Figure Skating: A\n  Fine-Grained and Jump Procedure-Aware Annotation Approach", "author": "Ryota Tanaka and Tomohiro Suzuki and Keisuke Fujii", "abstract": "  Understanding human actions from videos is essential in many domains,\nincluding sports. In figure skating, technical judgments are performed by\nwatching skaters' 3D movements, and its part of the judging procedure can be\nregarded as a Temporal Action Segmentation (TAS) task. TAS tasks in figure\nskating that automatically assign temporal semantics to video are actively\nresearched. However, there is a lack of datasets and effective methods for TAS\ntasks requiring 3D pose data. In this study, we first created the FS-Jump3D\ndataset of complex and dynamic figure skating jumps using optical markerless\nmotion capture. We also propose a new fine-grained figure skating jump TAS\ndataset annotation method with which TAS models can learn jump procedures. In\nthe experimental results, we validated the usefulness of 3D pose features as\ninput and the fine-grained dataset for the TAS model in figure skating.\nFS-Jump3D Dataset is available at https://github.com/ryota-skating/FS-Jump3D.\n", "link": "http://arxiv.org/abs/2408.16638v1", "date": "2024-08-29", "relevancy": 2.6222, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5404}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5217}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Pose-Based%20Temporal%20Action%20Segmentation%20for%20Figure%20Skating%3A%20A%0A%20%20Fine-Grained%20and%20Jump%20Procedure-Aware%20Annotation%20Approach&body=Title%3A%203D%20Pose-Based%20Temporal%20Action%20Segmentation%20for%20Figure%20Skating%3A%20A%0A%20%20Fine-Grained%20and%20Jump%20Procedure-Aware%20Annotation%20Approach%0AAuthor%3A%20Ryota%20Tanaka%20and%20Tomohiro%20Suzuki%20and%20Keisuke%20Fujii%0AAbstract%3A%20%20%20Understanding%20human%20actions%20from%20videos%20is%20essential%20in%20many%20domains%2C%0Aincluding%20sports.%20In%20figure%20skating%2C%20technical%20judgments%20are%20performed%20by%0Awatching%20skaters%27%203D%20movements%2C%20and%20its%20part%20of%20the%20judging%20procedure%20can%20be%0Aregarded%20as%20a%20Temporal%20Action%20Segmentation%20%28TAS%29%20task.%20TAS%20tasks%20in%20figure%0Askating%20that%20automatically%20assign%20temporal%20semantics%20to%20video%20are%20actively%0Aresearched.%20However%2C%20there%20is%20a%20lack%20of%20datasets%20and%20effective%20methods%20for%20TAS%0Atasks%20requiring%203D%20pose%20data.%20In%20this%20study%2C%20we%20first%20created%20the%20FS-Jump3D%0Adataset%20of%20complex%20and%20dynamic%20figure%20skating%20jumps%20using%20optical%20markerless%0Amotion%20capture.%20We%20also%20propose%20a%20new%20fine-grained%20figure%20skating%20jump%20TAS%0Adataset%20annotation%20method%20with%20which%20TAS%20models%20can%20learn%20jump%20procedures.%20In%0Athe%20experimental%20results%2C%20we%20validated%20the%20usefulness%20of%203D%20pose%20features%20as%0Ainput%20and%20the%20fine-grained%20dataset%20for%20the%20TAS%20model%20in%20figure%20skating.%0AFS-Jump3D%20Dataset%20is%20available%20at%20https%3A//github.com/ryota-skating/FS-Jump3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Pose-Based%2520Temporal%2520Action%2520Segmentation%2520for%2520Figure%2520Skating%253A%2520A%250A%2520%2520Fine-Grained%2520and%2520Jump%2520Procedure-Aware%2520Annotation%2520Approach%26entry.906535625%3DRyota%2520Tanaka%2520and%2520Tomohiro%2520Suzuki%2520and%2520Keisuke%2520Fujii%26entry.1292438233%3D%2520%2520Understanding%2520human%2520actions%2520from%2520videos%2520is%2520essential%2520in%2520many%2520domains%252C%250Aincluding%2520sports.%2520In%2520figure%2520skating%252C%2520technical%2520judgments%2520are%2520performed%2520by%250Awatching%2520skaters%2527%25203D%2520movements%252C%2520and%2520its%2520part%2520of%2520the%2520judging%2520procedure%2520can%2520be%250Aregarded%2520as%2520a%2520Temporal%2520Action%2520Segmentation%2520%2528TAS%2529%2520task.%2520TAS%2520tasks%2520in%2520figure%250Askating%2520that%2520automatically%2520assign%2520temporal%2520semantics%2520to%2520video%2520are%2520actively%250Aresearched.%2520However%252C%2520there%2520is%2520a%2520lack%2520of%2520datasets%2520and%2520effective%2520methods%2520for%2520TAS%250Atasks%2520requiring%25203D%2520pose%2520data.%2520In%2520this%2520study%252C%2520we%2520first%2520created%2520the%2520FS-Jump3D%250Adataset%2520of%2520complex%2520and%2520dynamic%2520figure%2520skating%2520jumps%2520using%2520optical%2520markerless%250Amotion%2520capture.%2520We%2520also%2520propose%2520a%2520new%2520fine-grained%2520figure%2520skating%2520jump%2520TAS%250Adataset%2520annotation%2520method%2520with%2520which%2520TAS%2520models%2520can%2520learn%2520jump%2520procedures.%2520In%250Athe%2520experimental%2520results%252C%2520we%2520validated%2520the%2520usefulness%2520of%25203D%2520pose%2520features%2520as%250Ainput%2520and%2520the%2520fine-grained%2520dataset%2520for%2520the%2520TAS%2520model%2520in%2520figure%2520skating.%250AFS-Jump3D%2520Dataset%2520is%2520available%2520at%2520https%253A//github.com/ryota-skating/FS-Jump3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Pose-Based%20Temporal%20Action%20Segmentation%20for%20Figure%20Skating%3A%20A%0A%20%20Fine-Grained%20and%20Jump%20Procedure-Aware%20Annotation%20Approach&entry.906535625=Ryota%20Tanaka%20and%20Tomohiro%20Suzuki%20and%20Keisuke%20Fujii&entry.1292438233=%20%20Understanding%20human%20actions%20from%20videos%20is%20essential%20in%20many%20domains%2C%0Aincluding%20sports.%20In%20figure%20skating%2C%20technical%20judgments%20are%20performed%20by%0Awatching%20skaters%27%203D%20movements%2C%20and%20its%20part%20of%20the%20judging%20procedure%20can%20be%0Aregarded%20as%20a%20Temporal%20Action%20Segmentation%20%28TAS%29%20task.%20TAS%20tasks%20in%20figure%0Askating%20that%20automatically%20assign%20temporal%20semantics%20to%20video%20are%20actively%0Aresearched.%20However%2C%20there%20is%20a%20lack%20of%20datasets%20and%20effective%20methods%20for%20TAS%0Atasks%20requiring%203D%20pose%20data.%20In%20this%20study%2C%20we%20first%20created%20the%20FS-Jump3D%0Adataset%20of%20complex%20and%20dynamic%20figure%20skating%20jumps%20using%20optical%20markerless%0Amotion%20capture.%20We%20also%20propose%20a%20new%20fine-grained%20figure%20skating%20jump%20TAS%0Adataset%20annotation%20method%20with%20which%20TAS%20models%20can%20learn%20jump%20procedures.%20In%0Athe%20experimental%20results%2C%20we%20validated%20the%20usefulness%20of%203D%20pose%20features%20as%0Ainput%20and%20the%20fine-grained%20dataset%20for%20the%20TAS%20model%20in%20figure%20skating.%0AFS-Jump3D%20Dataset%20is%20available%20at%20https%3A//github.com/ryota-skating/FS-Jump3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16638v1&entry.124074799=Read"},
{"title": "H-SGANet: Hybrid Sparse Graph Attention Network for Deformable Medical\n  Image Registration", "author": "Yufeng Zhou and Wenming Cao", "abstract": "  The integration of Convolutional Neural Network (ConvNet) and Transformer has\nemerged as a strong candidate for image registration, leveraging the strengths\nof both models and a large parameter space. However, this hybrid model,\ntreating brain MRI volumes as grid or sequence structures, faces challenges in\naccurately representing anatomical connectivity, diverse brain regions, and\nvital connections contributing to the brain's internal architecture. Concerns\nalso arise regarding the computational expense and GPU memory usage associated\nwith this model. To tackle these issues, a lightweight hybrid sparse graph\nattention network (H-SGANet) has been developed. This network incorporates a\ncentral mechanism, Sparse Graph Attention (SGA), based on a Vision Graph Neural\nNetwork (ViG) with predetermined anatomical connections. The SGA module expands\nthe model's receptive field and seamlessly integrates into the network. To\nfurther amplify the advantages of the hybrid network, the Separable\nSelf-Attention (SSA) is employed as an enhanced token mixer, integrated with\ndepth-wise convolution to constitute SSAFormer. This strategic integration is\ndesigned to more effectively extract long-range dependencies. As a hybrid\nConvNet-ViG-Transformer model, H-SGANet offers threefold benefits for\nvolumetric medical image registration. It optimizes fixed and moving images\nconcurrently through a hybrid feature fusion layer and an end-to-end learning\nframework. Compared to VoxelMorph, a model with a similar parameter count,\nH-SGANet demonstrates significant performance enhancements of 3.5% and 1.5% in\nDice score on the OASIS dataset and LPBA40 dataset, respectively.\n", "link": "http://arxiv.org/abs/2408.16719v1", "date": "2024-08-29", "relevancy": 2.6022, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5245}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5223}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H-SGANet%3A%20Hybrid%20Sparse%20Graph%20Attention%20Network%20for%20Deformable%20Medical%0A%20%20Image%20Registration&body=Title%3A%20H-SGANet%3A%20Hybrid%20Sparse%20Graph%20Attention%20Network%20for%20Deformable%20Medical%0A%20%20Image%20Registration%0AAuthor%3A%20Yufeng%20Zhou%20and%20Wenming%20Cao%0AAbstract%3A%20%20%20The%20integration%20of%20Convolutional%20Neural%20Network%20%28ConvNet%29%20and%20Transformer%20has%0Aemerged%20as%20a%20strong%20candidate%20for%20image%20registration%2C%20leveraging%20the%20strengths%0Aof%20both%20models%20and%20a%20large%20parameter%20space.%20However%2C%20this%20hybrid%20model%2C%0Atreating%20brain%20MRI%20volumes%20as%20grid%20or%20sequence%20structures%2C%20faces%20challenges%20in%0Aaccurately%20representing%20anatomical%20connectivity%2C%20diverse%20brain%20regions%2C%20and%0Avital%20connections%20contributing%20to%20the%20brain%27s%20internal%20architecture.%20Concerns%0Aalso%20arise%20regarding%20the%20computational%20expense%20and%20GPU%20memory%20usage%20associated%0Awith%20this%20model.%20To%20tackle%20these%20issues%2C%20a%20lightweight%20hybrid%20sparse%20graph%0Aattention%20network%20%28H-SGANet%29%20has%20been%20developed.%20This%20network%20incorporates%20a%0Acentral%20mechanism%2C%20Sparse%20Graph%20Attention%20%28SGA%29%2C%20based%20on%20a%20Vision%20Graph%20Neural%0ANetwork%20%28ViG%29%20with%20predetermined%20anatomical%20connections.%20The%20SGA%20module%20expands%0Athe%20model%27s%20receptive%20field%20and%20seamlessly%20integrates%20into%20the%20network.%20To%0Afurther%20amplify%20the%20advantages%20of%20the%20hybrid%20network%2C%20the%20Separable%0ASelf-Attention%20%28SSA%29%20is%20employed%20as%20an%20enhanced%20token%20mixer%2C%20integrated%20with%0Adepth-wise%20convolution%20to%20constitute%20SSAFormer.%20This%20strategic%20integration%20is%0Adesigned%20to%20more%20effectively%20extract%20long-range%20dependencies.%20As%20a%20hybrid%0AConvNet-ViG-Transformer%20model%2C%20H-SGANet%20offers%20threefold%20benefits%20for%0Avolumetric%20medical%20image%20registration.%20It%20optimizes%20fixed%20and%20moving%20images%0Aconcurrently%20through%20a%20hybrid%20feature%20fusion%20layer%20and%20an%20end-to-end%20learning%0Aframework.%20Compared%20to%20VoxelMorph%2C%20a%20model%20with%20a%20similar%20parameter%20count%2C%0AH-SGANet%20demonstrates%20significant%20performance%20enhancements%20of%203.5%25%20and%201.5%25%20in%0ADice%20score%20on%20the%20OASIS%20dataset%20and%20LPBA40%20dataset%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH-SGANet%253A%2520Hybrid%2520Sparse%2520Graph%2520Attention%2520Network%2520for%2520Deformable%2520Medical%250A%2520%2520Image%2520Registration%26entry.906535625%3DYufeng%2520Zhou%2520and%2520Wenming%2520Cao%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Convolutional%2520Neural%2520Network%2520%2528ConvNet%2529%2520and%2520Transformer%2520has%250Aemerged%2520as%2520a%2520strong%2520candidate%2520for%2520image%2520registration%252C%2520leveraging%2520the%2520strengths%250Aof%2520both%2520models%2520and%2520a%2520large%2520parameter%2520space.%2520However%252C%2520this%2520hybrid%2520model%252C%250Atreating%2520brain%2520MRI%2520volumes%2520as%2520grid%2520or%2520sequence%2520structures%252C%2520faces%2520challenges%2520in%250Aaccurately%2520representing%2520anatomical%2520connectivity%252C%2520diverse%2520brain%2520regions%252C%2520and%250Avital%2520connections%2520contributing%2520to%2520the%2520brain%2527s%2520internal%2520architecture.%2520Concerns%250Aalso%2520arise%2520regarding%2520the%2520computational%2520expense%2520and%2520GPU%2520memory%2520usage%2520associated%250Awith%2520this%2520model.%2520To%2520tackle%2520these%2520issues%252C%2520a%2520lightweight%2520hybrid%2520sparse%2520graph%250Aattention%2520network%2520%2528H-SGANet%2529%2520has%2520been%2520developed.%2520This%2520network%2520incorporates%2520a%250Acentral%2520mechanism%252C%2520Sparse%2520Graph%2520Attention%2520%2528SGA%2529%252C%2520based%2520on%2520a%2520Vision%2520Graph%2520Neural%250ANetwork%2520%2528ViG%2529%2520with%2520predetermined%2520anatomical%2520connections.%2520The%2520SGA%2520module%2520expands%250Athe%2520model%2527s%2520receptive%2520field%2520and%2520seamlessly%2520integrates%2520into%2520the%2520network.%2520To%250Afurther%2520amplify%2520the%2520advantages%2520of%2520the%2520hybrid%2520network%252C%2520the%2520Separable%250ASelf-Attention%2520%2528SSA%2529%2520is%2520employed%2520as%2520an%2520enhanced%2520token%2520mixer%252C%2520integrated%2520with%250Adepth-wise%2520convolution%2520to%2520constitute%2520SSAFormer.%2520This%2520strategic%2520integration%2520is%250Adesigned%2520to%2520more%2520effectively%2520extract%2520long-range%2520dependencies.%2520As%2520a%2520hybrid%250AConvNet-ViG-Transformer%2520model%252C%2520H-SGANet%2520offers%2520threefold%2520benefits%2520for%250Avolumetric%2520medical%2520image%2520registration.%2520It%2520optimizes%2520fixed%2520and%2520moving%2520images%250Aconcurrently%2520through%2520a%2520hybrid%2520feature%2520fusion%2520layer%2520and%2520an%2520end-to-end%2520learning%250Aframework.%2520Compared%2520to%2520VoxelMorph%252C%2520a%2520model%2520with%2520a%2520similar%2520parameter%2520count%252C%250AH-SGANet%2520demonstrates%2520significant%2520performance%2520enhancements%2520of%25203.5%2525%2520and%25201.5%2525%2520in%250ADice%2520score%2520on%2520the%2520OASIS%2520dataset%2520and%2520LPBA40%2520dataset%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H-SGANet%3A%20Hybrid%20Sparse%20Graph%20Attention%20Network%20for%20Deformable%20Medical%0A%20%20Image%20Registration&entry.906535625=Yufeng%20Zhou%20and%20Wenming%20Cao&entry.1292438233=%20%20The%20integration%20of%20Convolutional%20Neural%20Network%20%28ConvNet%29%20and%20Transformer%20has%0Aemerged%20as%20a%20strong%20candidate%20for%20image%20registration%2C%20leveraging%20the%20strengths%0Aof%20both%20models%20and%20a%20large%20parameter%20space.%20However%2C%20this%20hybrid%20model%2C%0Atreating%20brain%20MRI%20volumes%20as%20grid%20or%20sequence%20structures%2C%20faces%20challenges%20in%0Aaccurately%20representing%20anatomical%20connectivity%2C%20diverse%20brain%20regions%2C%20and%0Avital%20connections%20contributing%20to%20the%20brain%27s%20internal%20architecture.%20Concerns%0Aalso%20arise%20regarding%20the%20computational%20expense%20and%20GPU%20memory%20usage%20associated%0Awith%20this%20model.%20To%20tackle%20these%20issues%2C%20a%20lightweight%20hybrid%20sparse%20graph%0Aattention%20network%20%28H-SGANet%29%20has%20been%20developed.%20This%20network%20incorporates%20a%0Acentral%20mechanism%2C%20Sparse%20Graph%20Attention%20%28SGA%29%2C%20based%20on%20a%20Vision%20Graph%20Neural%0ANetwork%20%28ViG%29%20with%20predetermined%20anatomical%20connections.%20The%20SGA%20module%20expands%0Athe%20model%27s%20receptive%20field%20and%20seamlessly%20integrates%20into%20the%20network.%20To%0Afurther%20amplify%20the%20advantages%20of%20the%20hybrid%20network%2C%20the%20Separable%0ASelf-Attention%20%28SSA%29%20is%20employed%20as%20an%20enhanced%20token%20mixer%2C%20integrated%20with%0Adepth-wise%20convolution%20to%20constitute%20SSAFormer.%20This%20strategic%20integration%20is%0Adesigned%20to%20more%20effectively%20extract%20long-range%20dependencies.%20As%20a%20hybrid%0AConvNet-ViG-Transformer%20model%2C%20H-SGANet%20offers%20threefold%20benefits%20for%0Avolumetric%20medical%20image%20registration.%20It%20optimizes%20fixed%20and%20moving%20images%0Aconcurrently%20through%20a%20hybrid%20feature%20fusion%20layer%20and%20an%20end-to-end%20learning%0Aframework.%20Compared%20to%20VoxelMorph%2C%20a%20model%20with%20a%20similar%20parameter%20count%2C%0AH-SGANet%20demonstrates%20significant%20performance%20enhancements%20of%203.5%25%20and%201.5%25%20in%0ADice%20score%20on%20the%20OASIS%20dataset%20and%20LPBA40%20dataset%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16719v1&entry.124074799=Read"},
{"title": "VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation", "author": "Bocheng Zou and Mu Cai and Jianrui Zhang and Yong Jae Lee", "abstract": "  In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io.\n", "link": "http://arxiv.org/abs/2407.10972v2", "date": "2024-08-29", "relevancy": 2.5738, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5226}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5115}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VGBench%3A%20Evaluating%20Large%20Language%20Models%20on%20Vector%20Graphics%0A%20%20Understanding%20and%20Generation&body=Title%3A%20VGBench%3A%20Evaluating%20Large%20Language%20Models%20on%20Vector%20Graphics%0A%20%20Understanding%20and%20Generation%0AAuthor%3A%20Bocheng%20Zou%20and%20Mu%20Cai%20and%20Jianrui%20Zhang%20and%20Yong%20Jae%20Lee%0AAbstract%3A%20%20%20In%20the%20realm%20of%20vision%20models%2C%20the%20primary%20mode%20of%20representation%20is%20using%0Apixels%20to%20rasterize%20the%20visual%20world.%20Yet%20this%20is%20not%20always%20the%20best%20or%20unique%0Away%20to%20represent%20visual%20content%2C%20especially%20for%20designers%20and%20artists%20who%0Adepict%20the%20world%20using%20geometry%20primitives%20such%20as%20polygons.%20Vector%20graphics%0A%28VG%29%2C%20on%20the%20other%20hand%2C%20offer%20a%20textual%20representation%20of%20visual%20content%2C%0Awhich%20can%20be%20more%20concise%20and%20powerful%20for%20content%20like%20cartoons%2C%20sketches%20and%0Ascientific%20figures.%20Recent%20studies%20have%20shown%20promising%20results%20on%20processing%0Avector%20graphics%20with%20capable%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20such%20works%0Afocus%20solely%20on%20qualitative%20results%2C%20understanding%2C%20or%20a%20specific%20type%20of%0Avector%20graphics.%20We%20propose%20VGBench%2C%20a%20comprehensive%20benchmark%20for%20LLMs%20on%0Ahandling%20vector%20graphics%20through%20diverse%20aspects%2C%20including%20%28a%29%20both%20visual%0Aunderstanding%20and%20generation%2C%20%28b%29%20evaluation%20of%20various%20vector%20graphics%0Aformats%2C%20%28c%29%20diverse%20question%20types%2C%20%28d%29%20wide%20range%20of%20prompting%20techniques%2C%0A%28e%29%20under%20multiple%20LLMs%20and%20%28f%29%20comparison%20with%20VLMs%20on%20rasterized%0Arepresentations.%20Evaluating%20on%20our%20collected%204279%20understanding%20and%205845%0Ageneration%20samples%2C%20we%20find%20that%20LLMs%20show%20strong%20capability%20on%20both%20aspects%0Awhile%20exhibiting%20less%20desirable%20performance%20on%20low-level%20formats%20%28SVG%29.%20Both%0Adata%20and%20evaluation%20pipeline%20will%20be%20open-sourced%20at%20https%3A//vgbench.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10972v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVGBench%253A%2520Evaluating%2520Large%2520Language%2520Models%2520on%2520Vector%2520Graphics%250A%2520%2520Understanding%2520and%2520Generation%26entry.906535625%3DBocheng%2520Zou%2520and%2520Mu%2520Cai%2520and%2520Jianrui%2520Zhang%2520and%2520Yong%2520Jae%2520Lee%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520vision%2520models%252C%2520the%2520primary%2520mode%2520of%2520representation%2520is%2520using%250Apixels%2520to%2520rasterize%2520the%2520visual%2520world.%2520Yet%2520this%2520is%2520not%2520always%2520the%2520best%2520or%2520unique%250Away%2520to%2520represent%2520visual%2520content%252C%2520especially%2520for%2520designers%2520and%2520artists%2520who%250Adepict%2520the%2520world%2520using%2520geometry%2520primitives%2520such%2520as%2520polygons.%2520Vector%2520graphics%250A%2528VG%2529%252C%2520on%2520the%2520other%2520hand%252C%2520offer%2520a%2520textual%2520representation%2520of%2520visual%2520content%252C%250Awhich%2520can%2520be%2520more%2520concise%2520and%2520powerful%2520for%2520content%2520like%2520cartoons%252C%2520sketches%2520and%250Ascientific%2520figures.%2520Recent%2520studies%2520have%2520shown%2520promising%2520results%2520on%2520processing%250Avector%2520graphics%2520with%2520capable%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520However%252C%2520such%2520works%250Afocus%2520solely%2520on%2520qualitative%2520results%252C%2520understanding%252C%2520or%2520a%2520specific%2520type%2520of%250Avector%2520graphics.%2520We%2520propose%2520VGBench%252C%2520a%2520comprehensive%2520benchmark%2520for%2520LLMs%2520on%250Ahandling%2520vector%2520graphics%2520through%2520diverse%2520aspects%252C%2520including%2520%2528a%2529%2520both%2520visual%250Aunderstanding%2520and%2520generation%252C%2520%2528b%2529%2520evaluation%2520of%2520various%2520vector%2520graphics%250Aformats%252C%2520%2528c%2529%2520diverse%2520question%2520types%252C%2520%2528d%2529%2520wide%2520range%2520of%2520prompting%2520techniques%252C%250A%2528e%2529%2520under%2520multiple%2520LLMs%2520and%2520%2528f%2529%2520comparison%2520with%2520VLMs%2520on%2520rasterized%250Arepresentations.%2520Evaluating%2520on%2520our%2520collected%25204279%2520understanding%2520and%25205845%250Ageneration%2520samples%252C%2520we%2520find%2520that%2520LLMs%2520show%2520strong%2520capability%2520on%2520both%2520aspects%250Awhile%2520exhibiting%2520less%2520desirable%2520performance%2520on%2520low-level%2520formats%2520%2528SVG%2529.%2520Both%250Adata%2520and%2520evaluation%2520pipeline%2520will%2520be%2520open-sourced%2520at%2520https%253A//vgbench.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10972v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGBench%3A%20Evaluating%20Large%20Language%20Models%20on%20Vector%20Graphics%0A%20%20Understanding%20and%20Generation&entry.906535625=Bocheng%20Zou%20and%20Mu%20Cai%20and%20Jianrui%20Zhang%20and%20Yong%20Jae%20Lee&entry.1292438233=%20%20In%20the%20realm%20of%20vision%20models%2C%20the%20primary%20mode%20of%20representation%20is%20using%0Apixels%20to%20rasterize%20the%20visual%20world.%20Yet%20this%20is%20not%20always%20the%20best%20or%20unique%0Away%20to%20represent%20visual%20content%2C%20especially%20for%20designers%20and%20artists%20who%0Adepict%20the%20world%20using%20geometry%20primitives%20such%20as%20polygons.%20Vector%20graphics%0A%28VG%29%2C%20on%20the%20other%20hand%2C%20offer%20a%20textual%20representation%20of%20visual%20content%2C%0Awhich%20can%20be%20more%20concise%20and%20powerful%20for%20content%20like%20cartoons%2C%20sketches%20and%0Ascientific%20figures.%20Recent%20studies%20have%20shown%20promising%20results%20on%20processing%0Avector%20graphics%20with%20capable%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20such%20works%0Afocus%20solely%20on%20qualitative%20results%2C%20understanding%2C%20or%20a%20specific%20type%20of%0Avector%20graphics.%20We%20propose%20VGBench%2C%20a%20comprehensive%20benchmark%20for%20LLMs%20on%0Ahandling%20vector%20graphics%20through%20diverse%20aspects%2C%20including%20%28a%29%20both%20visual%0Aunderstanding%20and%20generation%2C%20%28b%29%20evaluation%20of%20various%20vector%20graphics%0Aformats%2C%20%28c%29%20diverse%20question%20types%2C%20%28d%29%20wide%20range%20of%20prompting%20techniques%2C%0A%28e%29%20under%20multiple%20LLMs%20and%20%28f%29%20comparison%20with%20VLMs%20on%20rasterized%0Arepresentations.%20Evaluating%20on%20our%20collected%204279%20understanding%20and%205845%0Ageneration%20samples%2C%20we%20find%20that%20LLMs%20show%20strong%20capability%20on%20both%20aspects%0Awhile%20exhibiting%20less%20desirable%20performance%20on%20low-level%20formats%20%28SVG%29.%20Both%0Adata%20and%20evaluation%20pipeline%20will%20be%20open-sourced%20at%20https%3A//vgbench.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10972v2&entry.124074799=Read"},
{"title": "One-Shot Learning Meets Depth Diffusion in Multi-Object Videos", "author": "Anisha Jain", "abstract": "  Creating editable videos that depict complex interactions between multiple\nobjects in various artistic styles has long been a challenging task in\nfilmmaking. Progress is often hampered by the scarcity of data sets that\ncontain paired text descriptions and corresponding videos that showcase these\ninteractions. This paper introduces a novel depth-conditioning approach that\nsignificantly advances this field by enabling the generation of coherent and\ndiverse videos from just a single text-video pair using a pre-trained\ndepth-aware Text-to-Image (T2I) model. Our method fine-tunes the pre-trained\nmodel to capture continuous motion by employing custom-designed spatial and\ntemporal attention mechanisms. During inference, we use the DDIM inversion to\nprovide structural guidance for video generation. This innovative technique\nallows for continuously controllable depth in videos, facilitating the\ngeneration of multiobject interactions while maintaining the concept generation\nand compositional strengths of the original T2I model across various artistic\nstyles, such as photorealism, animation, and impressionism.\n", "link": "http://arxiv.org/abs/2408.16704v1", "date": "2024-08-29", "relevancy": 2.5733, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7198}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Shot%20Learning%20Meets%20Depth%20Diffusion%20in%20Multi-Object%20Videos&body=Title%3A%20One-Shot%20Learning%20Meets%20Depth%20Diffusion%20in%20Multi-Object%20Videos%0AAuthor%3A%20Anisha%20Jain%0AAbstract%3A%20%20%20Creating%20editable%20videos%20that%20depict%20complex%20interactions%20between%20multiple%0Aobjects%20in%20various%20artistic%20styles%20has%20long%20been%20a%20challenging%20task%20in%0Afilmmaking.%20Progress%20is%20often%20hampered%20by%20the%20scarcity%20of%20data%20sets%20that%0Acontain%20paired%20text%20descriptions%20and%20corresponding%20videos%20that%20showcase%20these%0Ainteractions.%20This%20paper%20introduces%20a%20novel%20depth-conditioning%20approach%20that%0Asignificantly%20advances%20this%20field%20by%20enabling%20the%20generation%20of%20coherent%20and%0Adiverse%20videos%20from%20just%20a%20single%20text-video%20pair%20using%20a%20pre-trained%0Adepth-aware%20Text-to-Image%20%28T2I%29%20model.%20Our%20method%20fine-tunes%20the%20pre-trained%0Amodel%20to%20capture%20continuous%20motion%20by%20employing%20custom-designed%20spatial%20and%0Atemporal%20attention%20mechanisms.%20During%20inference%2C%20we%20use%20the%20DDIM%20inversion%20to%0Aprovide%20structural%20guidance%20for%20video%20generation.%20This%20innovative%20technique%0Aallows%20for%20continuously%20controllable%20depth%20in%20videos%2C%20facilitating%20the%0Ageneration%20of%20multiobject%20interactions%20while%20maintaining%20the%20concept%20generation%0Aand%20compositional%20strengths%20of%20the%20original%20T2I%20model%20across%20various%20artistic%0Astyles%2C%20such%20as%20photorealism%2C%20animation%2C%20and%20impressionism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Shot%2520Learning%2520Meets%2520Depth%2520Diffusion%2520in%2520Multi-Object%2520Videos%26entry.906535625%3DAnisha%2520Jain%26entry.1292438233%3D%2520%2520Creating%2520editable%2520videos%2520that%2520depict%2520complex%2520interactions%2520between%2520multiple%250Aobjects%2520in%2520various%2520artistic%2520styles%2520has%2520long%2520been%2520a%2520challenging%2520task%2520in%250Afilmmaking.%2520Progress%2520is%2520often%2520hampered%2520by%2520the%2520scarcity%2520of%2520data%2520sets%2520that%250Acontain%2520paired%2520text%2520descriptions%2520and%2520corresponding%2520videos%2520that%2520showcase%2520these%250Ainteractions.%2520This%2520paper%2520introduces%2520a%2520novel%2520depth-conditioning%2520approach%2520that%250Asignificantly%2520advances%2520this%2520field%2520by%2520enabling%2520the%2520generation%2520of%2520coherent%2520and%250Adiverse%2520videos%2520from%2520just%2520a%2520single%2520text-video%2520pair%2520using%2520a%2520pre-trained%250Adepth-aware%2520Text-to-Image%2520%2528T2I%2529%2520model.%2520Our%2520method%2520fine-tunes%2520the%2520pre-trained%250Amodel%2520to%2520capture%2520continuous%2520motion%2520by%2520employing%2520custom-designed%2520spatial%2520and%250Atemporal%2520attention%2520mechanisms.%2520During%2520inference%252C%2520we%2520use%2520the%2520DDIM%2520inversion%2520to%250Aprovide%2520structural%2520guidance%2520for%2520video%2520generation.%2520This%2520innovative%2520technique%250Aallows%2520for%2520continuously%2520controllable%2520depth%2520in%2520videos%252C%2520facilitating%2520the%250Ageneration%2520of%2520multiobject%2520interactions%2520while%2520maintaining%2520the%2520concept%2520generation%250Aand%2520compositional%2520strengths%2520of%2520the%2520original%2520T2I%2520model%2520across%2520various%2520artistic%250Astyles%252C%2520such%2520as%2520photorealism%252C%2520animation%252C%2520and%2520impressionism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Shot%20Learning%20Meets%20Depth%20Diffusion%20in%20Multi-Object%20Videos&entry.906535625=Anisha%20Jain&entry.1292438233=%20%20Creating%20editable%20videos%20that%20depict%20complex%20interactions%20between%20multiple%0Aobjects%20in%20various%20artistic%20styles%20has%20long%20been%20a%20challenging%20task%20in%0Afilmmaking.%20Progress%20is%20often%20hampered%20by%20the%20scarcity%20of%20data%20sets%20that%0Acontain%20paired%20text%20descriptions%20and%20corresponding%20videos%20that%20showcase%20these%0Ainteractions.%20This%20paper%20introduces%20a%20novel%20depth-conditioning%20approach%20that%0Asignificantly%20advances%20this%20field%20by%20enabling%20the%20generation%20of%20coherent%20and%0Adiverse%20videos%20from%20just%20a%20single%20text-video%20pair%20using%20a%20pre-trained%0Adepth-aware%20Text-to-Image%20%28T2I%29%20model.%20Our%20method%20fine-tunes%20the%20pre-trained%0Amodel%20to%20capture%20continuous%20motion%20by%20employing%20custom-designed%20spatial%20and%0Atemporal%20attention%20mechanisms.%20During%20inference%2C%20we%20use%20the%20DDIM%20inversion%20to%0Aprovide%20structural%20guidance%20for%20video%20generation.%20This%20innovative%20technique%0Aallows%20for%20continuously%20controllable%20depth%20in%20videos%2C%20facilitating%20the%0Ageneration%20of%20multiobject%20interactions%20while%20maintaining%20the%20concept%20generation%0Aand%20compositional%20strengths%20of%20the%20original%20T2I%20model%20across%20various%20artistic%0Astyles%2C%20such%20as%20photorealism%2C%20animation%2C%20and%20impressionism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16704v1&entry.124074799=Read"},
{"title": "Alignment is All You Need: A Training-free Augmentation Strategy for\n  Pose-guided Video Generation", "author": "Xiaoyu Jin and Zunnan Xu and Mingwen Ou and Wenming Yang", "abstract": "  Character animation is a transformative field in computer graphics and\nvision, enabling dynamic and realistic video animations from static images.\nDespite advancements, maintaining appearance consistency in animations remains\na challenge. Our approach addresses this by introducing a training-free\nframework that ensures the generated video sequence preserves the reference\nimage's subtleties, such as physique and proportions, through a dual alignment\nstrategy. We decouple skeletal and motion priors from pose information,\nenabling precise control over animation generation. Our method also improves\npixel-level alignment for conditional control from the reference character,\nenhancing the temporal consistency and visual cohesion of animations. Our\nmethod significantly enhances the quality of video generation without the need\nfor large datasets or expensive computational resources.\n", "link": "http://arxiv.org/abs/2408.16506v1", "date": "2024-08-29", "relevancy": 2.5431, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.7225}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5812}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment%20is%20All%20You%20Need%3A%20A%20Training-free%20Augmentation%20Strategy%20for%0A%20%20Pose-guided%20Video%20Generation&body=Title%3A%20Alignment%20is%20All%20You%20Need%3A%20A%20Training-free%20Augmentation%20Strategy%20for%0A%20%20Pose-guided%20Video%20Generation%0AAuthor%3A%20Xiaoyu%20Jin%20and%20Zunnan%20Xu%20and%20Mingwen%20Ou%20and%20Wenming%20Yang%0AAbstract%3A%20%20%20Character%20animation%20is%20a%20transformative%20field%20in%20computer%20graphics%20and%0Avision%2C%20enabling%20dynamic%20and%20realistic%20video%20animations%20from%20static%20images.%0ADespite%20advancements%2C%20maintaining%20appearance%20consistency%20in%20animations%20remains%0Aa%20challenge.%20Our%20approach%20addresses%20this%20by%20introducing%20a%20training-free%0Aframework%20that%20ensures%20the%20generated%20video%20sequence%20preserves%20the%20reference%0Aimage%27s%20subtleties%2C%20such%20as%20physique%20and%20proportions%2C%20through%20a%20dual%20alignment%0Astrategy.%20We%20decouple%20skeletal%20and%20motion%20priors%20from%20pose%20information%2C%0Aenabling%20precise%20control%20over%20animation%20generation.%20Our%20method%20also%20improves%0Apixel-level%20alignment%20for%20conditional%20control%20from%20the%20reference%20character%2C%0Aenhancing%20the%20temporal%20consistency%20and%20visual%20cohesion%20of%20animations.%20Our%0Amethod%20significantly%20enhances%20the%20quality%20of%20video%20generation%20without%20the%20need%0Afor%20large%20datasets%20or%20expensive%20computational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment%2520is%2520All%2520You%2520Need%253A%2520A%2520Training-free%2520Augmentation%2520Strategy%2520for%250A%2520%2520Pose-guided%2520Video%2520Generation%26entry.906535625%3DXiaoyu%2520Jin%2520and%2520Zunnan%2520Xu%2520and%2520Mingwen%2520Ou%2520and%2520Wenming%2520Yang%26entry.1292438233%3D%2520%2520Character%2520animation%2520is%2520a%2520transformative%2520field%2520in%2520computer%2520graphics%2520and%250Avision%252C%2520enabling%2520dynamic%2520and%2520realistic%2520video%2520animations%2520from%2520static%2520images.%250ADespite%2520advancements%252C%2520maintaining%2520appearance%2520consistency%2520in%2520animations%2520remains%250Aa%2520challenge.%2520Our%2520approach%2520addresses%2520this%2520by%2520introducing%2520a%2520training-free%250Aframework%2520that%2520ensures%2520the%2520generated%2520video%2520sequence%2520preserves%2520the%2520reference%250Aimage%2527s%2520subtleties%252C%2520such%2520as%2520physique%2520and%2520proportions%252C%2520through%2520a%2520dual%2520alignment%250Astrategy.%2520We%2520decouple%2520skeletal%2520and%2520motion%2520priors%2520from%2520pose%2520information%252C%250Aenabling%2520precise%2520control%2520over%2520animation%2520generation.%2520Our%2520method%2520also%2520improves%250Apixel-level%2520alignment%2520for%2520conditional%2520control%2520from%2520the%2520reference%2520character%252C%250Aenhancing%2520the%2520temporal%2520consistency%2520and%2520visual%2520cohesion%2520of%2520animations.%2520Our%250Amethod%2520significantly%2520enhances%2520the%2520quality%2520of%2520video%2520generation%2520without%2520the%2520need%250Afor%2520large%2520datasets%2520or%2520expensive%2520computational%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment%20is%20All%20You%20Need%3A%20A%20Training-free%20Augmentation%20Strategy%20for%0A%20%20Pose-guided%20Video%20Generation&entry.906535625=Xiaoyu%20Jin%20and%20Zunnan%20Xu%20and%20Mingwen%20Ou%20and%20Wenming%20Yang&entry.1292438233=%20%20Character%20animation%20is%20a%20transformative%20field%20in%20computer%20graphics%20and%0Avision%2C%20enabling%20dynamic%20and%20realistic%20video%20animations%20from%20static%20images.%0ADespite%20advancements%2C%20maintaining%20appearance%20consistency%20in%20animations%20remains%0Aa%20challenge.%20Our%20approach%20addresses%20this%20by%20introducing%20a%20training-free%0Aframework%20that%20ensures%20the%20generated%20video%20sequence%20preserves%20the%20reference%0Aimage%27s%20subtleties%2C%20such%20as%20physique%20and%20proportions%2C%20through%20a%20dual%20alignment%0Astrategy.%20We%20decouple%20skeletal%20and%20motion%20priors%20from%20pose%20information%2C%0Aenabling%20precise%20control%20over%20animation%20generation.%20Our%20method%20also%20improves%0Apixel-level%20alignment%20for%20conditional%20control%20from%20the%20reference%20character%2C%0Aenhancing%20the%20temporal%20consistency%20and%20visual%20cohesion%20of%20animations.%20Our%0Amethod%20significantly%20enhances%20the%20quality%20of%20video%20generation%20without%20the%20need%0Afor%20large%20datasets%20or%20expensive%20computational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16506v1&entry.124074799=Read"},
{"title": "MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair\n  Face Recognition", "author": "Eduarda Caldeira and Jaime S. Cardoso and Ana F. Sequeira and Pedro C. Neto", "abstract": "  As in school, one teacher to cover all subjects is insufficient to distill\nequally robust information to a student. Hence, each subject is taught by a\nhighly specialised teacher. Following a similar philosophy, we propose a\nmultiple specialized teacher framework to distill knowledge to a student\nnetwork. In our approach, directed at face recognition use cases, we train four\nteachers on one specific ethnicity, leading to four highly specialized and\nbiased teachers. Our strategy learns a project of these four teachers into a\ncommon space and distill that information to a student network. Our results\nhighlighted increased performance and reduced bias for all our experiments. In\naddition, we further show that having biased/specialized teachers is crucial by\nshowing that our approach achieves better results than when knowledge is\ndistilled from four teachers trained on balanced datasets. Our approach\nrepresents a step forward to the understanding of the importance of\nethnicity-specific features.\n", "link": "http://arxiv.org/abs/2408.16563v1", "date": "2024-08-29", "relevancy": 2.4799, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4998}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.498}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MST-KD%3A%20Multiple%20Specialized%20Teachers%20Knowledge%20Distillation%20for%20Fair%0A%20%20Face%20Recognition&body=Title%3A%20MST-KD%3A%20Multiple%20Specialized%20Teachers%20Knowledge%20Distillation%20for%20Fair%0A%20%20Face%20Recognition%0AAuthor%3A%20Eduarda%20Caldeira%20and%20Jaime%20S.%20Cardoso%20and%20Ana%20F.%20Sequeira%20and%20Pedro%20C.%20Neto%0AAbstract%3A%20%20%20As%20in%20school%2C%20one%20teacher%20to%20cover%20all%20subjects%20is%20insufficient%20to%20distill%0Aequally%20robust%20information%20to%20a%20student.%20Hence%2C%20each%20subject%20is%20taught%20by%20a%0Ahighly%20specialised%20teacher.%20Following%20a%20similar%20philosophy%2C%20we%20propose%20a%0Amultiple%20specialized%20teacher%20framework%20to%20distill%20knowledge%20to%20a%20student%0Anetwork.%20In%20our%20approach%2C%20directed%20at%20face%20recognition%20use%20cases%2C%20we%20train%20four%0Ateachers%20on%20one%20specific%20ethnicity%2C%20leading%20to%20four%20highly%20specialized%20and%0Abiased%20teachers.%20Our%20strategy%20learns%20a%20project%20of%20these%20four%20teachers%20into%20a%0Acommon%20space%20and%20distill%20that%20information%20to%20a%20student%20network.%20Our%20results%0Ahighlighted%20increased%20performance%20and%20reduced%20bias%20for%20all%20our%20experiments.%20In%0Aaddition%2C%20we%20further%20show%20that%20having%20biased/specialized%20teachers%20is%20crucial%20by%0Ashowing%20that%20our%20approach%20achieves%20better%20results%20than%20when%20knowledge%20is%0Adistilled%20from%20four%20teachers%20trained%20on%20balanced%20datasets.%20Our%20approach%0Arepresents%20a%20step%20forward%20to%20the%20understanding%20of%20the%20importance%20of%0Aethnicity-specific%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMST-KD%253A%2520Multiple%2520Specialized%2520Teachers%2520Knowledge%2520Distillation%2520for%2520Fair%250A%2520%2520Face%2520Recognition%26entry.906535625%3DEduarda%2520Caldeira%2520and%2520Jaime%2520S.%2520Cardoso%2520and%2520Ana%2520F.%2520Sequeira%2520and%2520Pedro%2520C.%2520Neto%26entry.1292438233%3D%2520%2520As%2520in%2520school%252C%2520one%2520teacher%2520to%2520cover%2520all%2520subjects%2520is%2520insufficient%2520to%2520distill%250Aequally%2520robust%2520information%2520to%2520a%2520student.%2520Hence%252C%2520each%2520subject%2520is%2520taught%2520by%2520a%250Ahighly%2520specialised%2520teacher.%2520Following%2520a%2520similar%2520philosophy%252C%2520we%2520propose%2520a%250Amultiple%2520specialized%2520teacher%2520framework%2520to%2520distill%2520knowledge%2520to%2520a%2520student%250Anetwork.%2520In%2520our%2520approach%252C%2520directed%2520at%2520face%2520recognition%2520use%2520cases%252C%2520we%2520train%2520four%250Ateachers%2520on%2520one%2520specific%2520ethnicity%252C%2520leading%2520to%2520four%2520highly%2520specialized%2520and%250Abiased%2520teachers.%2520Our%2520strategy%2520learns%2520a%2520project%2520of%2520these%2520four%2520teachers%2520into%2520a%250Acommon%2520space%2520and%2520distill%2520that%2520information%2520to%2520a%2520student%2520network.%2520Our%2520results%250Ahighlighted%2520increased%2520performance%2520and%2520reduced%2520bias%2520for%2520all%2520our%2520experiments.%2520In%250Aaddition%252C%2520we%2520further%2520show%2520that%2520having%2520biased/specialized%2520teachers%2520is%2520crucial%2520by%250Ashowing%2520that%2520our%2520approach%2520achieves%2520better%2520results%2520than%2520when%2520knowledge%2520is%250Adistilled%2520from%2520four%2520teachers%2520trained%2520on%2520balanced%2520datasets.%2520Our%2520approach%250Arepresents%2520a%2520step%2520forward%2520to%2520the%2520understanding%2520of%2520the%2520importance%2520of%250Aethnicity-specific%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MST-KD%3A%20Multiple%20Specialized%20Teachers%20Knowledge%20Distillation%20for%20Fair%0A%20%20Face%20Recognition&entry.906535625=Eduarda%20Caldeira%20and%20Jaime%20S.%20Cardoso%20and%20Ana%20F.%20Sequeira%20and%20Pedro%20C.%20Neto&entry.1292438233=%20%20As%20in%20school%2C%20one%20teacher%20to%20cover%20all%20subjects%20is%20insufficient%20to%20distill%0Aequally%20robust%20information%20to%20a%20student.%20Hence%2C%20each%20subject%20is%20taught%20by%20a%0Ahighly%20specialised%20teacher.%20Following%20a%20similar%20philosophy%2C%20we%20propose%20a%0Amultiple%20specialized%20teacher%20framework%20to%20distill%20knowledge%20to%20a%20student%0Anetwork.%20In%20our%20approach%2C%20directed%20at%20face%20recognition%20use%20cases%2C%20we%20train%20four%0Ateachers%20on%20one%20specific%20ethnicity%2C%20leading%20to%20four%20highly%20specialized%20and%0Abiased%20teachers.%20Our%20strategy%20learns%20a%20project%20of%20these%20four%20teachers%20into%20a%0Acommon%20space%20and%20distill%20that%20information%20to%20a%20student%20network.%20Our%20results%0Ahighlighted%20increased%20performance%20and%20reduced%20bias%20for%20all%20our%20experiments.%20In%0Aaddition%2C%20we%20further%20show%20that%20having%20biased/specialized%20teachers%20is%20crucial%20by%0Ashowing%20that%20our%20approach%20achieves%20better%20results%20than%20when%20knowledge%20is%0Adistilled%20from%20four%20teachers%20trained%20on%20balanced%20datasets.%20Our%20approach%0Arepresents%20a%20step%20forward%20to%20the%20understanding%20of%20the%20importance%20of%0Aethnicity-specific%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16563v1&entry.124074799=Read"},
{"title": "TempoKGAT: A Novel Graph Attention Network Approach for Temporal Graph\n  Analysis", "author": "Lena Sasal and Daniel Busby and Abdenour Hadid", "abstract": "  Graph neural networks (GNN) have shown significant capabilities in handling\nstructured data, yet their application to dynamic, temporal data remains\nlimited. This paper presents a new type of graph attention network, called\nTempoKGAT, which combines time-decaying weight and a selective neighbor\naggregation mechanism on the spatial domain, which helps uncover latent\npatterns in the graph data. In this approach, a top-k neighbor selection based\non the edge weights is introduced to represent the evolving features of the\ngraph data. We evaluated the performance of our TempoKGAT on multiple datasets\nfrom the traffic, energy, and health sectors involving spatio-temporal data. We\ncompared the performance of our approach to several state-of-the-art methods\nfound in the literature on several open-source datasets. Our method shows\nsuperior accuracy on all datasets. These results indicate that TempoKGAT builds\non existing methodologies to optimize prediction accuracy and provide new\ninsights into model interpretation in temporal contexts.\n", "link": "http://arxiv.org/abs/2408.16391v1", "date": "2024-08-29", "relevancy": 2.4681, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5014}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4927}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempoKGAT%3A%20A%20Novel%20Graph%20Attention%20Network%20Approach%20for%20Temporal%20Graph%0A%20%20Analysis&body=Title%3A%20TempoKGAT%3A%20A%20Novel%20Graph%20Attention%20Network%20Approach%20for%20Temporal%20Graph%0A%20%20Analysis%0AAuthor%3A%20Lena%20Sasal%20and%20Daniel%20Busby%20and%20Abdenour%20Hadid%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNN%29%20have%20shown%20significant%20capabilities%20in%20handling%0Astructured%20data%2C%20yet%20their%20application%20to%20dynamic%2C%20temporal%20data%20remains%0Alimited.%20This%20paper%20presents%20a%20new%20type%20of%20graph%20attention%20network%2C%20called%0ATempoKGAT%2C%20which%20combines%20time-decaying%20weight%20and%20a%20selective%20neighbor%0Aaggregation%20mechanism%20on%20the%20spatial%20domain%2C%20which%20helps%20uncover%20latent%0Apatterns%20in%20the%20graph%20data.%20In%20this%20approach%2C%20a%20top-k%20neighbor%20selection%20based%0Aon%20the%20edge%20weights%20is%20introduced%20to%20represent%20the%20evolving%20features%20of%20the%0Agraph%20data.%20We%20evaluated%20the%20performance%20of%20our%20TempoKGAT%20on%20multiple%20datasets%0Afrom%20the%20traffic%2C%20energy%2C%20and%20health%20sectors%20involving%20spatio-temporal%20data.%20We%0Acompared%20the%20performance%20of%20our%20approach%20to%20several%20state-of-the-art%20methods%0Afound%20in%20the%20literature%20on%20several%20open-source%20datasets.%20Our%20method%20shows%0Asuperior%20accuracy%20on%20all%20datasets.%20These%20results%20indicate%20that%20TempoKGAT%20builds%0Aon%20existing%20methodologies%20to%20optimize%20prediction%20accuracy%20and%20provide%20new%0Ainsights%20into%20model%20interpretation%20in%20temporal%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempoKGAT%253A%2520A%2520Novel%2520Graph%2520Attention%2520Network%2520Approach%2520for%2520Temporal%2520Graph%250A%2520%2520Analysis%26entry.906535625%3DLena%2520Sasal%2520and%2520Daniel%2520Busby%2520and%2520Abdenour%2520Hadid%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNN%2529%2520have%2520shown%2520significant%2520capabilities%2520in%2520handling%250Astructured%2520data%252C%2520yet%2520their%2520application%2520to%2520dynamic%252C%2520temporal%2520data%2520remains%250Alimited.%2520This%2520paper%2520presents%2520a%2520new%2520type%2520of%2520graph%2520attention%2520network%252C%2520called%250ATempoKGAT%252C%2520which%2520combines%2520time-decaying%2520weight%2520and%2520a%2520selective%2520neighbor%250Aaggregation%2520mechanism%2520on%2520the%2520spatial%2520domain%252C%2520which%2520helps%2520uncover%2520latent%250Apatterns%2520in%2520the%2520graph%2520data.%2520In%2520this%2520approach%252C%2520a%2520top-k%2520neighbor%2520selection%2520based%250Aon%2520the%2520edge%2520weights%2520is%2520introduced%2520to%2520represent%2520the%2520evolving%2520features%2520of%2520the%250Agraph%2520data.%2520We%2520evaluated%2520the%2520performance%2520of%2520our%2520TempoKGAT%2520on%2520multiple%2520datasets%250Afrom%2520the%2520traffic%252C%2520energy%252C%2520and%2520health%2520sectors%2520involving%2520spatio-temporal%2520data.%2520We%250Acompared%2520the%2520performance%2520of%2520our%2520approach%2520to%2520several%2520state-of-the-art%2520methods%250Afound%2520in%2520the%2520literature%2520on%2520several%2520open-source%2520datasets.%2520Our%2520method%2520shows%250Asuperior%2520accuracy%2520on%2520all%2520datasets.%2520These%2520results%2520indicate%2520that%2520TempoKGAT%2520builds%250Aon%2520existing%2520methodologies%2520to%2520optimize%2520prediction%2520accuracy%2520and%2520provide%2520new%250Ainsights%2520into%2520model%2520interpretation%2520in%2520temporal%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempoKGAT%3A%20A%20Novel%20Graph%20Attention%20Network%20Approach%20for%20Temporal%20Graph%0A%20%20Analysis&entry.906535625=Lena%20Sasal%20and%20Daniel%20Busby%20and%20Abdenour%20Hadid&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNN%29%20have%20shown%20significant%20capabilities%20in%20handling%0Astructured%20data%2C%20yet%20their%20application%20to%20dynamic%2C%20temporal%20data%20remains%0Alimited.%20This%20paper%20presents%20a%20new%20type%20of%20graph%20attention%20network%2C%20called%0ATempoKGAT%2C%20which%20combines%20time-decaying%20weight%20and%20a%20selective%20neighbor%0Aaggregation%20mechanism%20on%20the%20spatial%20domain%2C%20which%20helps%20uncover%20latent%0Apatterns%20in%20the%20graph%20data.%20In%20this%20approach%2C%20a%20top-k%20neighbor%20selection%20based%0Aon%20the%20edge%20weights%20is%20introduced%20to%20represent%20the%20evolving%20features%20of%20the%0Agraph%20data.%20We%20evaluated%20the%20performance%20of%20our%20TempoKGAT%20on%20multiple%20datasets%0Afrom%20the%20traffic%2C%20energy%2C%20and%20health%20sectors%20involving%20spatio-temporal%20data.%20We%0Acompared%20the%20performance%20of%20our%20approach%20to%20several%20state-of-the-art%20methods%0Afound%20in%20the%20literature%20on%20several%20open-source%20datasets.%20Our%20method%20shows%0Asuperior%20accuracy%20on%20all%20datasets.%20These%20results%20indicate%20that%20TempoKGAT%20builds%0Aon%20existing%20methodologies%20to%20optimize%20prediction%20accuracy%20and%20provide%20new%0Ainsights%20into%20model%20interpretation%20in%20temporal%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16391v1&entry.124074799=Read"},
{"title": "Generalist Segmentation Algorithm for Photoreceptors Analysis in\n  Adaptive Optics Imaging", "author": "Mikhail Kulyabin and Aline Sindel and Hilde Pedersen and Stuart Gilson and Rigmor Baraas and Andreas Maier", "abstract": "  Analyzing the cone photoreceptor pattern in images obtained from the living\nhuman retina using quantitative methods can be crucial for the early detection\nand management of various eye conditions. Confocal adaptive optics scanning\nlight ophthalmoscope (AOSLO) imaging enables visualization of the cones from\nreflections of waveguiding cone photoreceptors. While there have been\nsignificant improvements in automated algorithms for segmenting cones in\nconfocal AOSLO images, the process of labelling data remains labor-intensive\nand manual. This paper introduces a method based on deep learning (DL) for\ndetecting and segmenting cones in AOSLO images. The models were trained on a\nsemi-automatically labelled dataset of 20 AOSLO batches of images of 18\nparticipants for 0$^{\\circ}$, 1$^{\\circ}$, and 2$^{\\circ}$ from the foveal\ncenter. F1 scores were 0.968, 0.958, and 0.954 for 0$^{\\circ}$, 1$^{\\circ}$,\nand 2$^{\\circ}$, respectively, which is better than previously reported DL\napproaches. Our method minimizes the need for labelled data by only\nnecessitating a fraction of labelled cones, which is especially beneficial in\nthe field of ophthalmology, where labelled data can often be limited.\n", "link": "http://arxiv.org/abs/2408.14810v2", "date": "2024-08-29", "relevancy": 2.4597, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4991}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.49}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalist%20Segmentation%20Algorithm%20for%20Photoreceptors%20Analysis%20in%0A%20%20Adaptive%20Optics%20Imaging&body=Title%3A%20Generalist%20Segmentation%20Algorithm%20for%20Photoreceptors%20Analysis%20in%0A%20%20Adaptive%20Optics%20Imaging%0AAuthor%3A%20Mikhail%20Kulyabin%20and%20Aline%20Sindel%20and%20Hilde%20Pedersen%20and%20Stuart%20Gilson%20and%20Rigmor%20Baraas%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Analyzing%20the%20cone%20photoreceptor%20pattern%20in%20images%20obtained%20from%20the%20living%0Ahuman%20retina%20using%20quantitative%20methods%20can%20be%20crucial%20for%20the%20early%20detection%0Aand%20management%20of%20various%20eye%20conditions.%20Confocal%20adaptive%20optics%20scanning%0Alight%20ophthalmoscope%20%28AOSLO%29%20imaging%20enables%20visualization%20of%20the%20cones%20from%0Areflections%20of%20waveguiding%20cone%20photoreceptors.%20While%20there%20have%20been%0Asignificant%20improvements%20in%20automated%20algorithms%20for%20segmenting%20cones%20in%0Aconfocal%20AOSLO%20images%2C%20the%20process%20of%20labelling%20data%20remains%20labor-intensive%0Aand%20manual.%20This%20paper%20introduces%20a%20method%20based%20on%20deep%20learning%20%28DL%29%20for%0Adetecting%20and%20segmenting%20cones%20in%20AOSLO%20images.%20The%20models%20were%20trained%20on%20a%0Asemi-automatically%20labelled%20dataset%20of%2020%20AOSLO%20batches%20of%20images%20of%2018%0Aparticipants%20for%200%24%5E%7B%5Ccirc%7D%24%2C%201%24%5E%7B%5Ccirc%7D%24%2C%20and%202%24%5E%7B%5Ccirc%7D%24%20from%20the%20foveal%0Acenter.%20F1%20scores%20were%200.968%2C%200.958%2C%20and%200.954%20for%200%24%5E%7B%5Ccirc%7D%24%2C%201%24%5E%7B%5Ccirc%7D%24%2C%0Aand%202%24%5E%7B%5Ccirc%7D%24%2C%20respectively%2C%20which%20is%20better%20than%20previously%20reported%20DL%0Aapproaches.%20Our%20method%20minimizes%20the%20need%20for%20labelled%20data%20by%20only%0Anecessitating%20a%20fraction%20of%20labelled%20cones%2C%20which%20is%20especially%20beneficial%20in%0Athe%20field%20of%20ophthalmology%2C%20where%20labelled%20data%20can%20often%20be%20limited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralist%2520Segmentation%2520Algorithm%2520for%2520Photoreceptors%2520Analysis%2520in%250A%2520%2520Adaptive%2520Optics%2520Imaging%26entry.906535625%3DMikhail%2520Kulyabin%2520and%2520Aline%2520Sindel%2520and%2520Hilde%2520Pedersen%2520and%2520Stuart%2520Gilson%2520and%2520Rigmor%2520Baraas%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Analyzing%2520the%2520cone%2520photoreceptor%2520pattern%2520in%2520images%2520obtained%2520from%2520the%2520living%250Ahuman%2520retina%2520using%2520quantitative%2520methods%2520can%2520be%2520crucial%2520for%2520the%2520early%2520detection%250Aand%2520management%2520of%2520various%2520eye%2520conditions.%2520Confocal%2520adaptive%2520optics%2520scanning%250Alight%2520ophthalmoscope%2520%2528AOSLO%2529%2520imaging%2520enables%2520visualization%2520of%2520the%2520cones%2520from%250Areflections%2520of%2520waveguiding%2520cone%2520photoreceptors.%2520While%2520there%2520have%2520been%250Asignificant%2520improvements%2520in%2520automated%2520algorithms%2520for%2520segmenting%2520cones%2520in%250Aconfocal%2520AOSLO%2520images%252C%2520the%2520process%2520of%2520labelling%2520data%2520remains%2520labor-intensive%250Aand%2520manual.%2520This%2520paper%2520introduces%2520a%2520method%2520based%2520on%2520deep%2520learning%2520%2528DL%2529%2520for%250Adetecting%2520and%2520segmenting%2520cones%2520in%2520AOSLO%2520images.%2520The%2520models%2520were%2520trained%2520on%2520a%250Asemi-automatically%2520labelled%2520dataset%2520of%252020%2520AOSLO%2520batches%2520of%2520images%2520of%252018%250Aparticipants%2520for%25200%2524%255E%257B%255Ccirc%257D%2524%252C%25201%2524%255E%257B%255Ccirc%257D%2524%252C%2520and%25202%2524%255E%257B%255Ccirc%257D%2524%2520from%2520the%2520foveal%250Acenter.%2520F1%2520scores%2520were%25200.968%252C%25200.958%252C%2520and%25200.954%2520for%25200%2524%255E%257B%255Ccirc%257D%2524%252C%25201%2524%255E%257B%255Ccirc%257D%2524%252C%250Aand%25202%2524%255E%257B%255Ccirc%257D%2524%252C%2520respectively%252C%2520which%2520is%2520better%2520than%2520previously%2520reported%2520DL%250Aapproaches.%2520Our%2520method%2520minimizes%2520the%2520need%2520for%2520labelled%2520data%2520by%2520only%250Anecessitating%2520a%2520fraction%2520of%2520labelled%2520cones%252C%2520which%2520is%2520especially%2520beneficial%2520in%250Athe%2520field%2520of%2520ophthalmology%252C%2520where%2520labelled%2520data%2520can%2520often%2520be%2520limited.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalist%20Segmentation%20Algorithm%20for%20Photoreceptors%20Analysis%20in%0A%20%20Adaptive%20Optics%20Imaging&entry.906535625=Mikhail%20Kulyabin%20and%20Aline%20Sindel%20and%20Hilde%20Pedersen%20and%20Stuart%20Gilson%20and%20Rigmor%20Baraas%20and%20Andreas%20Maier&entry.1292438233=%20%20Analyzing%20the%20cone%20photoreceptor%20pattern%20in%20images%20obtained%20from%20the%20living%0Ahuman%20retina%20using%20quantitative%20methods%20can%20be%20crucial%20for%20the%20early%20detection%0Aand%20management%20of%20various%20eye%20conditions.%20Confocal%20adaptive%20optics%20scanning%0Alight%20ophthalmoscope%20%28AOSLO%29%20imaging%20enables%20visualization%20of%20the%20cones%20from%0Areflections%20of%20waveguiding%20cone%20photoreceptors.%20While%20there%20have%20been%0Asignificant%20improvements%20in%20automated%20algorithms%20for%20segmenting%20cones%20in%0Aconfocal%20AOSLO%20images%2C%20the%20process%20of%20labelling%20data%20remains%20labor-intensive%0Aand%20manual.%20This%20paper%20introduces%20a%20method%20based%20on%20deep%20learning%20%28DL%29%20for%0Adetecting%20and%20segmenting%20cones%20in%20AOSLO%20images.%20The%20models%20were%20trained%20on%20a%0Asemi-automatically%20labelled%20dataset%20of%2020%20AOSLO%20batches%20of%20images%20of%2018%0Aparticipants%20for%200%24%5E%7B%5Ccirc%7D%24%2C%201%24%5E%7B%5Ccirc%7D%24%2C%20and%202%24%5E%7B%5Ccirc%7D%24%20from%20the%20foveal%0Acenter.%20F1%20scores%20were%200.968%2C%200.958%2C%20and%200.954%20for%200%24%5E%7B%5Ccirc%7D%24%2C%201%24%5E%7B%5Ccirc%7D%24%2C%0Aand%202%24%5E%7B%5Ccirc%7D%24%2C%20respectively%2C%20which%20is%20better%20than%20previously%20reported%20DL%0Aapproaches.%20Our%20method%20minimizes%20the%20need%20for%20labelled%20data%20by%20only%0Anecessitating%20a%20fraction%20of%20labelled%20cones%2C%20which%20is%20especially%20beneficial%20in%0Athe%20field%20of%20ophthalmology%2C%20where%20labelled%20data%20can%20often%20be%20limited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14810v2&entry.124074799=Read"},
{"title": "A GREAT Architecture for Edge-Based Graph Problems Like TSP", "author": "Attila Lischka and Jiaming Wu and Morteza Haghir Chehreghani and Bal\u00e1zs Kulcs\u00e1r", "abstract": "  In the last years, many neural network-based approaches have been proposed to\ntackle combinatorial optimization problems such as routing problems. Many of\nthese approaches are based on graph neural networks (GNNs) or related\ntransformers, operating on the Euclidean coordinates representing the routing\nproblems. However, GNNs are inherently not well suited to operate on dense\ngraphs, such as in routing problems. Furthermore, models operating on Euclidean\ncoordinates cannot be applied to non-Euclidean versions of routing problems\nthat are often found in real-world settings. To overcome these limitations, we\npropose a novel GNN-related edge-based neural model called Graph Edge Attention\nNetwork (GREAT). We evaluate the performance of GREAT in the\nedge-classification task to predict optimal edges in the Traveling Salesman\nProblem (TSP). We can use such a trained GREAT model to produce sparse TSP\ngraph instances, keeping only the edges GREAT finds promising. Compared to\nother, non-learning-based methods to sparsify TSP graphs, GREAT can produce\nvery sparse graphs while keeping most of the optimal edges. Furthermore, we\nbuild a reinforcement learning-based GREAT framework which we apply to\nEuclidean and non-Euclidean asymmetric TSP. This framework achieves\nstate-of-the-art results.\n", "link": "http://arxiv.org/abs/2408.16717v1", "date": "2024-08-29", "relevancy": 2.459, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4971}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4894}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20GREAT%20Architecture%20for%20Edge-Based%20Graph%20Problems%20Like%20TSP&body=Title%3A%20A%20GREAT%20Architecture%20for%20Edge-Based%20Graph%20Problems%20Like%20TSP%0AAuthor%3A%20Attila%20Lischka%20and%20Jiaming%20Wu%20and%20Morteza%20Haghir%20Chehreghani%20and%20Bal%C3%A1zs%20Kulcs%C3%A1r%0AAbstract%3A%20%20%20In%20the%20last%20years%2C%20many%20neural%20network-based%20approaches%20have%20been%20proposed%20to%0Atackle%20combinatorial%20optimization%20problems%20such%20as%20routing%20problems.%20Many%20of%0Athese%20approaches%20are%20based%20on%20graph%20neural%20networks%20%28GNNs%29%20or%20related%0Atransformers%2C%20operating%20on%20the%20Euclidean%20coordinates%20representing%20the%20routing%0Aproblems.%20However%2C%20GNNs%20are%20inherently%20not%20well%20suited%20to%20operate%20on%20dense%0Agraphs%2C%20such%20as%20in%20routing%20problems.%20Furthermore%2C%20models%20operating%20on%20Euclidean%0Acoordinates%20cannot%20be%20applied%20to%20non-Euclidean%20versions%20of%20routing%20problems%0Athat%20are%20often%20found%20in%20real-world%20settings.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20novel%20GNN-related%20edge-based%20neural%20model%20called%20Graph%20Edge%20Attention%0ANetwork%20%28GREAT%29.%20We%20evaluate%20the%20performance%20of%20GREAT%20in%20the%0Aedge-classification%20task%20to%20predict%20optimal%20edges%20in%20the%20Traveling%20Salesman%0AProblem%20%28TSP%29.%20We%20can%20use%20such%20a%20trained%20GREAT%20model%20to%20produce%20sparse%20TSP%0Agraph%20instances%2C%20keeping%20only%20the%20edges%20GREAT%20finds%20promising.%20Compared%20to%0Aother%2C%20non-learning-based%20methods%20to%20sparsify%20TSP%20graphs%2C%20GREAT%20can%20produce%0Avery%20sparse%20graphs%20while%20keeping%20most%20of%20the%20optimal%20edges.%20Furthermore%2C%20we%0Abuild%20a%20reinforcement%20learning-based%20GREAT%20framework%20which%20we%20apply%20to%0AEuclidean%20and%20non-Euclidean%20asymmetric%20TSP.%20This%20framework%20achieves%0Astate-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520GREAT%2520Architecture%2520for%2520Edge-Based%2520Graph%2520Problems%2520Like%2520TSP%26entry.906535625%3DAttila%2520Lischka%2520and%2520Jiaming%2520Wu%2520and%2520Morteza%2520Haghir%2520Chehreghani%2520and%2520Bal%25C3%25A1zs%2520Kulcs%25C3%25A1r%26entry.1292438233%3D%2520%2520In%2520the%2520last%2520years%252C%2520many%2520neural%2520network-based%2520approaches%2520have%2520been%2520proposed%2520to%250Atackle%2520combinatorial%2520optimization%2520problems%2520such%2520as%2520routing%2520problems.%2520Many%2520of%250Athese%2520approaches%2520are%2520based%2520on%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520or%2520related%250Atransformers%252C%2520operating%2520on%2520the%2520Euclidean%2520coordinates%2520representing%2520the%2520routing%250Aproblems.%2520However%252C%2520GNNs%2520are%2520inherently%2520not%2520well%2520suited%2520to%2520operate%2520on%2520dense%250Agraphs%252C%2520such%2520as%2520in%2520routing%2520problems.%2520Furthermore%252C%2520models%2520operating%2520on%2520Euclidean%250Acoordinates%2520cannot%2520be%2520applied%2520to%2520non-Euclidean%2520versions%2520of%2520routing%2520problems%250Athat%2520are%2520often%2520found%2520in%2520real-world%2520settings.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520novel%2520GNN-related%2520edge-based%2520neural%2520model%2520called%2520Graph%2520Edge%2520Attention%250ANetwork%2520%2528GREAT%2529.%2520We%2520evaluate%2520the%2520performance%2520of%2520GREAT%2520in%2520the%250Aedge-classification%2520task%2520to%2520predict%2520optimal%2520edges%2520in%2520the%2520Traveling%2520Salesman%250AProblem%2520%2528TSP%2529.%2520We%2520can%2520use%2520such%2520a%2520trained%2520GREAT%2520model%2520to%2520produce%2520sparse%2520TSP%250Agraph%2520instances%252C%2520keeping%2520only%2520the%2520edges%2520GREAT%2520finds%2520promising.%2520Compared%2520to%250Aother%252C%2520non-learning-based%2520methods%2520to%2520sparsify%2520TSP%2520graphs%252C%2520GREAT%2520can%2520produce%250Avery%2520sparse%2520graphs%2520while%2520keeping%2520most%2520of%2520the%2520optimal%2520edges.%2520Furthermore%252C%2520we%250Abuild%2520a%2520reinforcement%2520learning-based%2520GREAT%2520framework%2520which%2520we%2520apply%2520to%250AEuclidean%2520and%2520non-Euclidean%2520asymmetric%2520TSP.%2520This%2520framework%2520achieves%250Astate-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20GREAT%20Architecture%20for%20Edge-Based%20Graph%20Problems%20Like%20TSP&entry.906535625=Attila%20Lischka%20and%20Jiaming%20Wu%20and%20Morteza%20Haghir%20Chehreghani%20and%20Bal%C3%A1zs%20Kulcs%C3%A1r&entry.1292438233=%20%20In%20the%20last%20years%2C%20many%20neural%20network-based%20approaches%20have%20been%20proposed%20to%0Atackle%20combinatorial%20optimization%20problems%20such%20as%20routing%20problems.%20Many%20of%0Athese%20approaches%20are%20based%20on%20graph%20neural%20networks%20%28GNNs%29%20or%20related%0Atransformers%2C%20operating%20on%20the%20Euclidean%20coordinates%20representing%20the%20routing%0Aproblems.%20However%2C%20GNNs%20are%20inherently%20not%20well%20suited%20to%20operate%20on%20dense%0Agraphs%2C%20such%20as%20in%20routing%20problems.%20Furthermore%2C%20models%20operating%20on%20Euclidean%0Acoordinates%20cannot%20be%20applied%20to%20non-Euclidean%20versions%20of%20routing%20problems%0Athat%20are%20often%20found%20in%20real-world%20settings.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20novel%20GNN-related%20edge-based%20neural%20model%20called%20Graph%20Edge%20Attention%0ANetwork%20%28GREAT%29.%20We%20evaluate%20the%20performance%20of%20GREAT%20in%20the%0Aedge-classification%20task%20to%20predict%20optimal%20edges%20in%20the%20Traveling%20Salesman%0AProblem%20%28TSP%29.%20We%20can%20use%20such%20a%20trained%20GREAT%20model%20to%20produce%20sparse%20TSP%0Agraph%20instances%2C%20keeping%20only%20the%20edges%20GREAT%20finds%20promising.%20Compared%20to%0Aother%2C%20non-learning-based%20methods%20to%20sparsify%20TSP%20graphs%2C%20GREAT%20can%20produce%0Avery%20sparse%20graphs%20while%20keeping%20most%20of%20the%20optimal%20edges.%20Furthermore%2C%20we%0Abuild%20a%20reinforcement%20learning-based%20GREAT%20framework%20which%20we%20apply%20to%0AEuclidean%20and%20non-Euclidean%20asymmetric%20TSP.%20This%20framework%20achieves%0Astate-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16717v1&entry.124074799=Read"},
{"title": "Creating a Segmented Pointcloud of Grapevines by Combining Multiple\n  Viewpoints Through Visual Odometry", "author": "Michael Adlerstein and Angelo Bratta and Jo\u00e3o Carlos Virgolino Soares and Giovanni Dessy and Miguel Fernandes and Matteo Gatti and Claudio Semini", "abstract": "  Grapevine winter pruning is a labor-intensive and repetitive process that\nsignificantly influences the quality and quantity of the grape harvest and\nproduced wine of the following season. It requires a careful and expert\ndetection of the point to be cut. Because of its complexity, repetitive nature\nand time constraint, the task requires skilled labor that needs to be trained.\nThis extended abstract presents the computer vision pipeline employed in\nproject Vinum, using detectron2 as a segmentation network and keypoint visual\nodometry to merge different observation into a single pointcloud used to make\ninformed pruning decisions.\n", "link": "http://arxiv.org/abs/2408.16472v1", "date": "2024-08-29", "relevancy": 2.4516, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4905}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creating%20a%20Segmented%20Pointcloud%20of%20Grapevines%20by%20Combining%20Multiple%0A%20%20Viewpoints%20Through%20Visual%20Odometry&body=Title%3A%20Creating%20a%20Segmented%20Pointcloud%20of%20Grapevines%20by%20Combining%20Multiple%0A%20%20Viewpoints%20Through%20Visual%20Odometry%0AAuthor%3A%20Michael%20Adlerstein%20and%20Angelo%20Bratta%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Giovanni%20Dessy%20and%20Miguel%20Fernandes%20and%20Matteo%20Gatti%20and%20Claudio%20Semini%0AAbstract%3A%20%20%20Grapevine%20winter%20pruning%20is%20a%20labor-intensive%20and%20repetitive%20process%20that%0Asignificantly%20influences%20the%20quality%20and%20quantity%20of%20the%20grape%20harvest%20and%0Aproduced%20wine%20of%20the%20following%20season.%20It%20requires%20a%20careful%20and%20expert%0Adetection%20of%20the%20point%20to%20be%20cut.%20Because%20of%20its%20complexity%2C%20repetitive%20nature%0Aand%20time%20constraint%2C%20the%20task%20requires%20skilled%20labor%20that%20needs%20to%20be%20trained.%0AThis%20extended%20abstract%20presents%20the%20computer%20vision%20pipeline%20employed%20in%0Aproject%20Vinum%2C%20using%20detectron2%20as%20a%20segmentation%20network%20and%20keypoint%20visual%0Aodometry%20to%20merge%20different%20observation%20into%20a%20single%20pointcloud%20used%20to%20make%0Ainformed%20pruning%20decisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreating%2520a%2520Segmented%2520Pointcloud%2520of%2520Grapevines%2520by%2520Combining%2520Multiple%250A%2520%2520Viewpoints%2520Through%2520Visual%2520Odometry%26entry.906535625%3DMichael%2520Adlerstein%2520and%2520Angelo%2520Bratta%2520and%2520Jo%25C3%25A3o%2520Carlos%2520Virgolino%2520Soares%2520and%2520Giovanni%2520Dessy%2520and%2520Miguel%2520Fernandes%2520and%2520Matteo%2520Gatti%2520and%2520Claudio%2520Semini%26entry.1292438233%3D%2520%2520Grapevine%2520winter%2520pruning%2520is%2520a%2520labor-intensive%2520and%2520repetitive%2520process%2520that%250Asignificantly%2520influences%2520the%2520quality%2520and%2520quantity%2520of%2520the%2520grape%2520harvest%2520and%250Aproduced%2520wine%2520of%2520the%2520following%2520season.%2520It%2520requires%2520a%2520careful%2520and%2520expert%250Adetection%2520of%2520the%2520point%2520to%2520be%2520cut.%2520Because%2520of%2520its%2520complexity%252C%2520repetitive%2520nature%250Aand%2520time%2520constraint%252C%2520the%2520task%2520requires%2520skilled%2520labor%2520that%2520needs%2520to%2520be%2520trained.%250AThis%2520extended%2520abstract%2520presents%2520the%2520computer%2520vision%2520pipeline%2520employed%2520in%250Aproject%2520Vinum%252C%2520using%2520detectron2%2520as%2520a%2520segmentation%2520network%2520and%2520keypoint%2520visual%250Aodometry%2520to%2520merge%2520different%2520observation%2520into%2520a%2520single%2520pointcloud%2520used%2520to%2520make%250Ainformed%2520pruning%2520decisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creating%20a%20Segmented%20Pointcloud%20of%20Grapevines%20by%20Combining%20Multiple%0A%20%20Viewpoints%20Through%20Visual%20Odometry&entry.906535625=Michael%20Adlerstein%20and%20Angelo%20Bratta%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Giovanni%20Dessy%20and%20Miguel%20Fernandes%20and%20Matteo%20Gatti%20and%20Claudio%20Semini&entry.1292438233=%20%20Grapevine%20winter%20pruning%20is%20a%20labor-intensive%20and%20repetitive%20process%20that%0Asignificantly%20influences%20the%20quality%20and%20quantity%20of%20the%20grape%20harvest%20and%0Aproduced%20wine%20of%20the%20following%20season.%20It%20requires%20a%20careful%20and%20expert%0Adetection%20of%20the%20point%20to%20be%20cut.%20Because%20of%20its%20complexity%2C%20repetitive%20nature%0Aand%20time%20constraint%2C%20the%20task%20requires%20skilled%20labor%20that%20needs%20to%20be%20trained.%0AThis%20extended%20abstract%20presents%20the%20computer%20vision%20pipeline%20employed%20in%0Aproject%20Vinum%2C%20using%20detectron2%20as%20a%20segmentation%20network%20and%20keypoint%20visual%0Aodometry%20to%20merge%20different%20observation%20into%20a%20single%20pointcloud%20used%20to%20make%0Ainformed%20pruning%20decisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16472v1&entry.124074799=Read"},
{"title": "Identifying Terrain Physical Parameters from Vision -- Towards\n  Physical-Parameter-Aware Locomotion and Navigation", "author": "Jiaqi Chen and Jonas Frey and Ruyi Zhou and Takahiro Miki and Georg Martius and Marco Hutter", "abstract": "  Identifying the physical properties of the surrounding environment is\nessential for robotic locomotion and navigation to deal with non-geometric\nhazards, such as slippery and deformable terrains. It would be of great benefit\nfor robots to anticipate these extreme physical properties before contact;\nhowever, estimating environmental physical parameters from vision is still an\nopen challenge. Animals can achieve this by using their prior experience and\nknowledge of what they have seen and how it felt. In this work, we propose a\ncross-modal self-supervised learning framework for vision-based environmental\nphysical parameter estimation, which paves the way for future\nphysical-property-aware locomotion and navigation. We bridge the gap between\nexisting policies trained in simulation and identification of physical terrain\nparameters from vision. We propose to train a physical decoder in simulation to\npredict friction and stiffness from multi-modal input. The trained network\nallows the labeling of real-world images with physical parameters in a\nself-supervised manner to further train a visual network during deployment,\nwhich can densely predict the friction and stiffness from image data. We\nvalidate our physical decoder in simulation and the real world using a\nquadruped ANYmal robot, outperforming an existing baseline method. We show that\nour visual network can predict the physical properties in indoor and outdoor\nexperiments while allowing fast adaptation to new environments.\n", "link": "http://arxiv.org/abs/2408.16567v1", "date": "2024-08-29", "relevancy": 2.4411, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.747}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Terrain%20Physical%20Parameters%20from%20Vision%20--%20Towards%0A%20%20Physical-Parameter-Aware%20Locomotion%20and%20Navigation&body=Title%3A%20Identifying%20Terrain%20Physical%20Parameters%20from%20Vision%20--%20Towards%0A%20%20Physical-Parameter-Aware%20Locomotion%20and%20Navigation%0AAuthor%3A%20Jiaqi%20Chen%20and%20Jonas%20Frey%20and%20Ruyi%20Zhou%20and%20Takahiro%20Miki%20and%20Georg%20Martius%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Identifying%20the%20physical%20properties%20of%20the%20surrounding%20environment%20is%0Aessential%20for%20robotic%20locomotion%20and%20navigation%20to%20deal%20with%20non-geometric%0Ahazards%2C%20such%20as%20slippery%20and%20deformable%20terrains.%20It%20would%20be%20of%20great%20benefit%0Afor%20robots%20to%20anticipate%20these%20extreme%20physical%20properties%20before%20contact%3B%0Ahowever%2C%20estimating%20environmental%20physical%20parameters%20from%20vision%20is%20still%20an%0Aopen%20challenge.%20Animals%20can%20achieve%20this%20by%20using%20their%20prior%20experience%20and%0Aknowledge%20of%20what%20they%20have%20seen%20and%20how%20it%20felt.%20In%20this%20work%2C%20we%20propose%20a%0Across-modal%20self-supervised%20learning%20framework%20for%20vision-based%20environmental%0Aphysical%20parameter%20estimation%2C%20which%20paves%20the%20way%20for%20future%0Aphysical-property-aware%20locomotion%20and%20navigation.%20We%20bridge%20the%20gap%20between%0Aexisting%20policies%20trained%20in%20simulation%20and%20identification%20of%20physical%20terrain%0Aparameters%20from%20vision.%20We%20propose%20to%20train%20a%20physical%20decoder%20in%20simulation%20to%0Apredict%20friction%20and%20stiffness%20from%20multi-modal%20input.%20The%20trained%20network%0Aallows%20the%20labeling%20of%20real-world%20images%20with%20physical%20parameters%20in%20a%0Aself-supervised%20manner%20to%20further%20train%20a%20visual%20network%20during%20deployment%2C%0Awhich%20can%20densely%20predict%20the%20friction%20and%20stiffness%20from%20image%20data.%20We%0Avalidate%20our%20physical%20decoder%20in%20simulation%20and%20the%20real%20world%20using%20a%0Aquadruped%20ANYmal%20robot%2C%20outperforming%20an%20existing%20baseline%20method.%20We%20show%20that%0Aour%20visual%20network%20can%20predict%20the%20physical%20properties%20in%20indoor%20and%20outdoor%0Aexperiments%20while%20allowing%20fast%20adaptation%20to%20new%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Terrain%2520Physical%2520Parameters%2520from%2520Vision%2520--%2520Towards%250A%2520%2520Physical-Parameter-Aware%2520Locomotion%2520and%2520Navigation%26entry.906535625%3DJiaqi%2520Chen%2520and%2520Jonas%2520Frey%2520and%2520Ruyi%2520Zhou%2520and%2520Takahiro%2520Miki%2520and%2520Georg%2520Martius%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Identifying%2520the%2520physical%2520properties%2520of%2520the%2520surrounding%2520environment%2520is%250Aessential%2520for%2520robotic%2520locomotion%2520and%2520navigation%2520to%2520deal%2520with%2520non-geometric%250Ahazards%252C%2520such%2520as%2520slippery%2520and%2520deformable%2520terrains.%2520It%2520would%2520be%2520of%2520great%2520benefit%250Afor%2520robots%2520to%2520anticipate%2520these%2520extreme%2520physical%2520properties%2520before%2520contact%253B%250Ahowever%252C%2520estimating%2520environmental%2520physical%2520parameters%2520from%2520vision%2520is%2520still%2520an%250Aopen%2520challenge.%2520Animals%2520can%2520achieve%2520this%2520by%2520using%2520their%2520prior%2520experience%2520and%250Aknowledge%2520of%2520what%2520they%2520have%2520seen%2520and%2520how%2520it%2520felt.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Across-modal%2520self-supervised%2520learning%2520framework%2520for%2520vision-based%2520environmental%250Aphysical%2520parameter%2520estimation%252C%2520which%2520paves%2520the%2520way%2520for%2520future%250Aphysical-property-aware%2520locomotion%2520and%2520navigation.%2520We%2520bridge%2520the%2520gap%2520between%250Aexisting%2520policies%2520trained%2520in%2520simulation%2520and%2520identification%2520of%2520physical%2520terrain%250Aparameters%2520from%2520vision.%2520We%2520propose%2520to%2520train%2520a%2520physical%2520decoder%2520in%2520simulation%2520to%250Apredict%2520friction%2520and%2520stiffness%2520from%2520multi-modal%2520input.%2520The%2520trained%2520network%250Aallows%2520the%2520labeling%2520of%2520real-world%2520images%2520with%2520physical%2520parameters%2520in%2520a%250Aself-supervised%2520manner%2520to%2520further%2520train%2520a%2520visual%2520network%2520during%2520deployment%252C%250Awhich%2520can%2520densely%2520predict%2520the%2520friction%2520and%2520stiffness%2520from%2520image%2520data.%2520We%250Avalidate%2520our%2520physical%2520decoder%2520in%2520simulation%2520and%2520the%2520real%2520world%2520using%2520a%250Aquadruped%2520ANYmal%2520robot%252C%2520outperforming%2520an%2520existing%2520baseline%2520method.%2520We%2520show%2520that%250Aour%2520visual%2520network%2520can%2520predict%2520the%2520physical%2520properties%2520in%2520indoor%2520and%2520outdoor%250Aexperiments%2520while%2520allowing%2520fast%2520adaptation%2520to%2520new%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Terrain%20Physical%20Parameters%20from%20Vision%20--%20Towards%0A%20%20Physical-Parameter-Aware%20Locomotion%20and%20Navigation&entry.906535625=Jiaqi%20Chen%20and%20Jonas%20Frey%20and%20Ruyi%20Zhou%20and%20Takahiro%20Miki%20and%20Georg%20Martius%20and%20Marco%20Hutter&entry.1292438233=%20%20Identifying%20the%20physical%20properties%20of%20the%20surrounding%20environment%20is%0Aessential%20for%20robotic%20locomotion%20and%20navigation%20to%20deal%20with%20non-geometric%0Ahazards%2C%20such%20as%20slippery%20and%20deformable%20terrains.%20It%20would%20be%20of%20great%20benefit%0Afor%20robots%20to%20anticipate%20these%20extreme%20physical%20properties%20before%20contact%3B%0Ahowever%2C%20estimating%20environmental%20physical%20parameters%20from%20vision%20is%20still%20an%0Aopen%20challenge.%20Animals%20can%20achieve%20this%20by%20using%20their%20prior%20experience%20and%0Aknowledge%20of%20what%20they%20have%20seen%20and%20how%20it%20felt.%20In%20this%20work%2C%20we%20propose%20a%0Across-modal%20self-supervised%20learning%20framework%20for%20vision-based%20environmental%0Aphysical%20parameter%20estimation%2C%20which%20paves%20the%20way%20for%20future%0Aphysical-property-aware%20locomotion%20and%20navigation.%20We%20bridge%20the%20gap%20between%0Aexisting%20policies%20trained%20in%20simulation%20and%20identification%20of%20physical%20terrain%0Aparameters%20from%20vision.%20We%20propose%20to%20train%20a%20physical%20decoder%20in%20simulation%20to%0Apredict%20friction%20and%20stiffness%20from%20multi-modal%20input.%20The%20trained%20network%0Aallows%20the%20labeling%20of%20real-world%20images%20with%20physical%20parameters%20in%20a%0Aself-supervised%20manner%20to%20further%20train%20a%20visual%20network%20during%20deployment%2C%0Awhich%20can%20densely%20predict%20the%20friction%20and%20stiffness%20from%20image%20data.%20We%0Avalidate%20our%20physical%20decoder%20in%20simulation%20and%20the%20real%20world%20using%20a%0Aquadruped%20ANYmal%20robot%2C%20outperforming%20an%20existing%20baseline%20method.%20We%20show%20that%0Aour%20visual%20network%20can%20predict%20the%20physical%20properties%20in%20indoor%20and%20outdoor%0Aexperiments%20while%20allowing%20fast%20adaptation%20to%20new%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16567v1&entry.124074799=Read"},
{"title": "CW-CNN & CW-AN: Convolutional Networks and Attention Networks for\n  CW-Complexes", "author": "Rahul Khorana", "abstract": "  We present a novel framework for learning on CW-complex structured data\npoints. Recent advances have discussed CW-complexes as ideal learning\nrepresentations for problems in cheminformatics. However, there is a lack of\navailable machine learning methods suitable for learning on CW-complexes. In\nthis paper we develop notions of convolution and attention that are well\ndefined for CW-complexes. These notions enable us to create the first neural\nnetwork that can receive a CW-complex as input. We illustrate and interpret\nthis framework in the context of supervised prediction.\n", "link": "http://arxiv.org/abs/2408.16686v1", "date": "2024-08-29", "relevancy": 2.4098, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4955}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4814}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CW-CNN%20%26%20CW-AN%3A%20Convolutional%20Networks%20and%20Attention%20Networks%20for%0A%20%20CW-Complexes&body=Title%3A%20CW-CNN%20%26%20CW-AN%3A%20Convolutional%20Networks%20and%20Attention%20Networks%20for%0A%20%20CW-Complexes%0AAuthor%3A%20Rahul%20Khorana%0AAbstract%3A%20%20%20We%20present%20a%20novel%20framework%20for%20learning%20on%20CW-complex%20structured%20data%0Apoints.%20Recent%20advances%20have%20discussed%20CW-complexes%20as%20ideal%20learning%0Arepresentations%20for%20problems%20in%20cheminformatics.%20However%2C%20there%20is%20a%20lack%20of%0Aavailable%20machine%20learning%20methods%20suitable%20for%20learning%20on%20CW-complexes.%20In%0Athis%20paper%20we%20develop%20notions%20of%20convolution%20and%20attention%20that%20are%20well%0Adefined%20for%20CW-complexes.%20These%20notions%20enable%20us%20to%20create%20the%20first%20neural%0Anetwork%20that%20can%20receive%20a%20CW-complex%20as%20input.%20We%20illustrate%20and%20interpret%0Athis%20framework%20in%20the%20context%20of%20supervised%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCW-CNN%2520%2526%2520CW-AN%253A%2520Convolutional%2520Networks%2520and%2520Attention%2520Networks%2520for%250A%2520%2520CW-Complexes%26entry.906535625%3DRahul%2520Khorana%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520framework%2520for%2520learning%2520on%2520CW-complex%2520structured%2520data%250Apoints.%2520Recent%2520advances%2520have%2520discussed%2520CW-complexes%2520as%2520ideal%2520learning%250Arepresentations%2520for%2520problems%2520in%2520cheminformatics.%2520However%252C%2520there%2520is%2520a%2520lack%2520of%250Aavailable%2520machine%2520learning%2520methods%2520suitable%2520for%2520learning%2520on%2520CW-complexes.%2520In%250Athis%2520paper%2520we%2520develop%2520notions%2520of%2520convolution%2520and%2520attention%2520that%2520are%2520well%250Adefined%2520for%2520CW-complexes.%2520These%2520notions%2520enable%2520us%2520to%2520create%2520the%2520first%2520neural%250Anetwork%2520that%2520can%2520receive%2520a%2520CW-complex%2520as%2520input.%2520We%2520illustrate%2520and%2520interpret%250Athis%2520framework%2520in%2520the%2520context%2520of%2520supervised%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CW-CNN%20%26%20CW-AN%3A%20Convolutional%20Networks%20and%20Attention%20Networks%20for%0A%20%20CW-Complexes&entry.906535625=Rahul%20Khorana&entry.1292438233=%20%20We%20present%20a%20novel%20framework%20for%20learning%20on%20CW-complex%20structured%20data%0Apoints.%20Recent%20advances%20have%20discussed%20CW-complexes%20as%20ideal%20learning%0Arepresentations%20for%20problems%20in%20cheminformatics.%20However%2C%20there%20is%20a%20lack%20of%0Aavailable%20machine%20learning%20methods%20suitable%20for%20learning%20on%20CW-complexes.%20In%0Athis%20paper%20we%20develop%20notions%20of%20convolution%20and%20attention%20that%20are%20well%0Adefined%20for%20CW-complexes.%20These%20notions%20enable%20us%20to%20create%20the%20first%20neural%0Anetwork%20that%20can%20receive%20a%20CW-complex%20as%20input.%20We%20illustrate%20and%20interpret%0Athis%20framework%20in%20the%20context%20of%20supervised%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16686v1&entry.124074799=Read"},
{"title": "Manipulate-Anything: Automating Real-World Robots using Vision-Language\n  Models", "author": "Jiafei Duan and Wentao Yuan and Wilbert Pumacay and Yi Ru Wang and Kiana Ehsani and Dieter Fox and Ranjay Krishna", "abstract": "  Large-scale endeavors like and widespread community efforts such as\nOpen-X-Embodiment have contributed to growing the scale of robot demonstration\ndata. However, there is still an opportunity to improve the quality, quantity,\nand diversity of robot demonstration data. Although vision-language models have\nbeen shown to automatically generate demonstration data, their utility has been\nlimited to environments with privileged state information, they require\nhand-designed skills, and are limited to interactions with few object\ninstances. We propose Manipulate-Anything, a scalable automated generation\nmethod for real-world robotic manipulation. Unlike prior work, our method can\noperate in real-world environments without any privileged state information,\nhand-designed skills, and can manipulate any static object. We evaluate our\nmethod using two setups. First, Manipulate-Anything successfully generates\ntrajectories for all 7 real-world and 14 simulation tasks, significantly\noutperforming existing methods like VoxPoser. Second, Manipulate-Anything's\ndemonstrations can train more robust behavior cloning policies than training\nwith human demonstrations, or from data generated by VoxPoser, Scaling-up, and\nCode-As-Policies. We believe Manipulate-Anything can be a scalable method for\nboth generating data for robotics and solving novel tasks in a zero-shot\nsetting. Project page: https://robot-ma.github.io/.\n", "link": "http://arxiv.org/abs/2406.18915v3", "date": "2024-08-29", "relevancy": 2.4067, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6733}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5918}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manipulate-Anything%3A%20Automating%20Real-World%20Robots%20using%20Vision-Language%0A%20%20Models&body=Title%3A%20Manipulate-Anything%3A%20Automating%20Real-World%20Robots%20using%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Jiafei%20Duan%20and%20Wentao%20Yuan%20and%20Wilbert%20Pumacay%20and%20Yi%20Ru%20Wang%20and%20Kiana%20Ehsani%20and%20Dieter%20Fox%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20Large-scale%20endeavors%20like%20and%20widespread%20community%20efforts%20such%20as%0AOpen-X-Embodiment%20have%20contributed%20to%20growing%20the%20scale%20of%20robot%20demonstration%0Adata.%20However%2C%20there%20is%20still%20an%20opportunity%20to%20improve%20the%20quality%2C%20quantity%2C%0Aand%20diversity%20of%20robot%20demonstration%20data.%20Although%20vision-language%20models%20have%0Abeen%20shown%20to%20automatically%20generate%20demonstration%20data%2C%20their%20utility%20has%20been%0Alimited%20to%20environments%20with%20privileged%20state%20information%2C%20they%20require%0Ahand-designed%20skills%2C%20and%20are%20limited%20to%20interactions%20with%20few%20object%0Ainstances.%20We%20propose%20Manipulate-Anything%2C%20a%20scalable%20automated%20generation%0Amethod%20for%20real-world%20robotic%20manipulation.%20Unlike%20prior%20work%2C%20our%20method%20can%0Aoperate%20in%20real-world%20environments%20without%20any%20privileged%20state%20information%2C%0Ahand-designed%20skills%2C%20and%20can%20manipulate%20any%20static%20object.%20We%20evaluate%20our%0Amethod%20using%20two%20setups.%20First%2C%20Manipulate-Anything%20successfully%20generates%0Atrajectories%20for%20all%207%20real-world%20and%2014%20simulation%20tasks%2C%20significantly%0Aoutperforming%20existing%20methods%20like%20VoxPoser.%20Second%2C%20Manipulate-Anything%27s%0Ademonstrations%20can%20train%20more%20robust%20behavior%20cloning%20policies%20than%20training%0Awith%20human%20demonstrations%2C%20or%20from%20data%20generated%20by%20VoxPoser%2C%20Scaling-up%2C%20and%0ACode-As-Policies.%20We%20believe%20Manipulate-Anything%20can%20be%20a%20scalable%20method%20for%0Aboth%20generating%20data%20for%20robotics%20and%20solving%20novel%20tasks%20in%20a%20zero-shot%0Asetting.%20Project%20page%3A%20https%3A//robot-ma.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18915v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManipulate-Anything%253A%2520Automating%2520Real-World%2520Robots%2520using%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DJiafei%2520Duan%2520and%2520Wentao%2520Yuan%2520and%2520Wilbert%2520Pumacay%2520and%2520Yi%2520Ru%2520Wang%2520and%2520Kiana%2520Ehsani%2520and%2520Dieter%2520Fox%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3D%2520%2520Large-scale%2520endeavors%2520like%2520and%2520widespread%2520community%2520efforts%2520such%2520as%250AOpen-X-Embodiment%2520have%2520contributed%2520to%2520growing%2520the%2520scale%2520of%2520robot%2520demonstration%250Adata.%2520However%252C%2520there%2520is%2520still%2520an%2520opportunity%2520to%2520improve%2520the%2520quality%252C%2520quantity%252C%250Aand%2520diversity%2520of%2520robot%2520demonstration%2520data.%2520Although%2520vision-language%2520models%2520have%250Abeen%2520shown%2520to%2520automatically%2520generate%2520demonstration%2520data%252C%2520their%2520utility%2520has%2520been%250Alimited%2520to%2520environments%2520with%2520privileged%2520state%2520information%252C%2520they%2520require%250Ahand-designed%2520skills%252C%2520and%2520are%2520limited%2520to%2520interactions%2520with%2520few%2520object%250Ainstances.%2520We%2520propose%2520Manipulate-Anything%252C%2520a%2520scalable%2520automated%2520generation%250Amethod%2520for%2520real-world%2520robotic%2520manipulation.%2520Unlike%2520prior%2520work%252C%2520our%2520method%2520can%250Aoperate%2520in%2520real-world%2520environments%2520without%2520any%2520privileged%2520state%2520information%252C%250Ahand-designed%2520skills%252C%2520and%2520can%2520manipulate%2520any%2520static%2520object.%2520We%2520evaluate%2520our%250Amethod%2520using%2520two%2520setups.%2520First%252C%2520Manipulate-Anything%2520successfully%2520generates%250Atrajectories%2520for%2520all%25207%2520real-world%2520and%252014%2520simulation%2520tasks%252C%2520significantly%250Aoutperforming%2520existing%2520methods%2520like%2520VoxPoser.%2520Second%252C%2520Manipulate-Anything%2527s%250Ademonstrations%2520can%2520train%2520more%2520robust%2520behavior%2520cloning%2520policies%2520than%2520training%250Awith%2520human%2520demonstrations%252C%2520or%2520from%2520data%2520generated%2520by%2520VoxPoser%252C%2520Scaling-up%252C%2520and%250ACode-As-Policies.%2520We%2520believe%2520Manipulate-Anything%2520can%2520be%2520a%2520scalable%2520method%2520for%250Aboth%2520generating%2520data%2520for%2520robotics%2520and%2520solving%2520novel%2520tasks%2520in%2520a%2520zero-shot%250Asetting.%2520Project%2520page%253A%2520https%253A//robot-ma.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18915v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manipulate-Anything%3A%20Automating%20Real-World%20Robots%20using%20Vision-Language%0A%20%20Models&entry.906535625=Jiafei%20Duan%20and%20Wentao%20Yuan%20and%20Wilbert%20Pumacay%20and%20Yi%20Ru%20Wang%20and%20Kiana%20Ehsani%20and%20Dieter%20Fox%20and%20Ranjay%20Krishna&entry.1292438233=%20%20Large-scale%20endeavors%20like%20and%20widespread%20community%20efforts%20such%20as%0AOpen-X-Embodiment%20have%20contributed%20to%20growing%20the%20scale%20of%20robot%20demonstration%0Adata.%20However%2C%20there%20is%20still%20an%20opportunity%20to%20improve%20the%20quality%2C%20quantity%2C%0Aand%20diversity%20of%20robot%20demonstration%20data.%20Although%20vision-language%20models%20have%0Abeen%20shown%20to%20automatically%20generate%20demonstration%20data%2C%20their%20utility%20has%20been%0Alimited%20to%20environments%20with%20privileged%20state%20information%2C%20they%20require%0Ahand-designed%20skills%2C%20and%20are%20limited%20to%20interactions%20with%20few%20object%0Ainstances.%20We%20propose%20Manipulate-Anything%2C%20a%20scalable%20automated%20generation%0Amethod%20for%20real-world%20robotic%20manipulation.%20Unlike%20prior%20work%2C%20our%20method%20can%0Aoperate%20in%20real-world%20environments%20without%20any%20privileged%20state%20information%2C%0Ahand-designed%20skills%2C%20and%20can%20manipulate%20any%20static%20object.%20We%20evaluate%20our%0Amethod%20using%20two%20setups.%20First%2C%20Manipulate-Anything%20successfully%20generates%0Atrajectories%20for%20all%207%20real-world%20and%2014%20simulation%20tasks%2C%20significantly%0Aoutperforming%20existing%20methods%20like%20VoxPoser.%20Second%2C%20Manipulate-Anything%27s%0Ademonstrations%20can%20train%20more%20robust%20behavior%20cloning%20policies%20than%20training%0Awith%20human%20demonstrations%2C%20or%20from%20data%20generated%20by%20VoxPoser%2C%20Scaling-up%2C%20and%0ACode-As-Policies.%20We%20believe%20Manipulate-Anything%20can%20be%20a%20scalable%20method%20for%0Aboth%20generating%20data%20for%20robotics%20and%20solving%20novel%20tasks%20in%20a%20zero-shot%0Asetting.%20Project%20page%3A%20https%3A//robot-ma.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18915v3&entry.124074799=Read"},
{"title": "3D Whole-body Grasp Synthesis with Directional Controllability", "author": "Georgios Paschalidis and Romana Wilschut and Dimitrije Anti\u0107 and Omid Taheri and Dimitrios Tzionas", "abstract": "  Synthesizing 3D whole-bodies that realistically grasp objects is useful for\nanimation, mixed reality, and robotics. This is challenging, because the hands\nand body need to look natural w.r.t. each other, the grasped object, as well as\nthe local scene (i.e., a receptacle supporting the object). Only recent work\ntackles this, with a divide-and-conquer approach; it first generates a\n\"guiding\" right-hand grasp, and then searches for bodies that match this.\nHowever, the guiding-hand synthesis lacks controllability and receptacle\nawareness, so it likely has an implausible direction (i.e., a body can't match\nthis without penetrating the receptacle) and needs corrections through major\npost-processing. Moreover, the body search needs exhaustive sampling and is\nexpensive. These are strong limitations. We tackle these with a novel method\ncalled CWGrasp. Our key idea is that performing geometry-based reasoning \"early\non,\" instead of \"too late,\" provides rich \"control\" signals for inference. To\nthis end, CWGrasp first samples a plausible reaching-direction vector (used\nlater for both the arm and hand) from a probabilistic model built via\nraycasting from the object and collision checking. Then, it generates a\nreaching body with a desired arm direction, as well as a \"guiding\" grasping\nhand with a desired palm direction that complies with the arm's one.\nEventually, CWGrasp refines the body to match the \"guiding\" hand, while\nplausibly contacting the scene. Notably, generating already-compatible \"parts\"\ngreatly simplifies the \"whole.\" Moreover, CWGrasp uniquely tackles both right-\nand left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets.\nCWGrasp outperforms baselines, at lower runtime and budget, while all\ncomponents help performance. Code and models will be released.\n", "link": "http://arxiv.org/abs/2408.16770v1", "date": "2024-08-29", "relevancy": 2.3954, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6677}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5719}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Whole-body%20Grasp%20Synthesis%20with%20Directional%20Controllability&body=Title%3A%203D%20Whole-body%20Grasp%20Synthesis%20with%20Directional%20Controllability%0AAuthor%3A%20Georgios%20Paschalidis%20and%20Romana%20Wilschut%20and%20Dimitrije%20Anti%C4%87%20and%20Omid%20Taheri%20and%20Dimitrios%20Tzionas%0AAbstract%3A%20%20%20Synthesizing%203D%20whole-bodies%20that%20realistically%20grasp%20objects%20is%20useful%20for%0Aanimation%2C%20mixed%20reality%2C%20and%20robotics.%20This%20is%20challenging%2C%20because%20the%20hands%0Aand%20body%20need%20to%20look%20natural%20w.r.t.%20each%20other%2C%20the%20grasped%20object%2C%20as%20well%20as%0Athe%20local%20scene%20%28i.e.%2C%20a%20receptacle%20supporting%20the%20object%29.%20Only%20recent%20work%0Atackles%20this%2C%20with%20a%20divide-and-conquer%20approach%3B%20it%20first%20generates%20a%0A%22guiding%22%20right-hand%20grasp%2C%20and%20then%20searches%20for%20bodies%20that%20match%20this.%0AHowever%2C%20the%20guiding-hand%20synthesis%20lacks%20controllability%20and%20receptacle%0Aawareness%2C%20so%20it%20likely%20has%20an%20implausible%20direction%20%28i.e.%2C%20a%20body%20can%27t%20match%0Athis%20without%20penetrating%20the%20receptacle%29%20and%20needs%20corrections%20through%20major%0Apost-processing.%20Moreover%2C%20the%20body%20search%20needs%20exhaustive%20sampling%20and%20is%0Aexpensive.%20These%20are%20strong%20limitations.%20We%20tackle%20these%20with%20a%20novel%20method%0Acalled%20CWGrasp.%20Our%20key%20idea%20is%20that%20performing%20geometry-based%20reasoning%20%22early%0Aon%2C%22%20instead%20of%20%22too%20late%2C%22%20provides%20rich%20%22control%22%20signals%20for%20inference.%20To%0Athis%20end%2C%20CWGrasp%20first%20samples%20a%20plausible%20reaching-direction%20vector%20%28used%0Alater%20for%20both%20the%20arm%20and%20hand%29%20from%20a%20probabilistic%20model%20built%20via%0Araycasting%20from%20the%20object%20and%20collision%20checking.%20Then%2C%20it%20generates%20a%0Areaching%20body%20with%20a%20desired%20arm%20direction%2C%20as%20well%20as%20a%20%22guiding%22%20grasping%0Ahand%20with%20a%20desired%20palm%20direction%20that%20complies%20with%20the%20arm%27s%20one.%0AEventually%2C%20CWGrasp%20refines%20the%20body%20to%20match%20the%20%22guiding%22%20hand%2C%20while%0Aplausibly%20contacting%20the%20scene.%20Notably%2C%20generating%20already-compatible%20%22parts%22%0Agreatly%20simplifies%20the%20%22whole.%22%20Moreover%2C%20CWGrasp%20uniquely%20tackles%20both%20right-%0Aand%20left-hand%20grasps.%20We%20evaluate%20on%20the%20GRAB%20and%20ReplicaGrasp%20datasets.%0ACWGrasp%20outperforms%20baselines%2C%20at%20lower%20runtime%20and%20budget%2C%20while%20all%0Acomponents%20help%20performance.%20Code%20and%20models%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Whole-body%2520Grasp%2520Synthesis%2520with%2520Directional%2520Controllability%26entry.906535625%3DGeorgios%2520Paschalidis%2520and%2520Romana%2520Wilschut%2520and%2520Dimitrije%2520Anti%25C4%2587%2520and%2520Omid%2520Taheri%2520and%2520Dimitrios%2520Tzionas%26entry.1292438233%3D%2520%2520Synthesizing%25203D%2520whole-bodies%2520that%2520realistically%2520grasp%2520objects%2520is%2520useful%2520for%250Aanimation%252C%2520mixed%2520reality%252C%2520and%2520robotics.%2520This%2520is%2520challenging%252C%2520because%2520the%2520hands%250Aand%2520body%2520need%2520to%2520look%2520natural%2520w.r.t.%2520each%2520other%252C%2520the%2520grasped%2520object%252C%2520as%2520well%2520as%250Athe%2520local%2520scene%2520%2528i.e.%252C%2520a%2520receptacle%2520supporting%2520the%2520object%2529.%2520Only%2520recent%2520work%250Atackles%2520this%252C%2520with%2520a%2520divide-and-conquer%2520approach%253B%2520it%2520first%2520generates%2520a%250A%2522guiding%2522%2520right-hand%2520grasp%252C%2520and%2520then%2520searches%2520for%2520bodies%2520that%2520match%2520this.%250AHowever%252C%2520the%2520guiding-hand%2520synthesis%2520lacks%2520controllability%2520and%2520receptacle%250Aawareness%252C%2520so%2520it%2520likely%2520has%2520an%2520implausible%2520direction%2520%2528i.e.%252C%2520a%2520body%2520can%2527t%2520match%250Athis%2520without%2520penetrating%2520the%2520receptacle%2529%2520and%2520needs%2520corrections%2520through%2520major%250Apost-processing.%2520Moreover%252C%2520the%2520body%2520search%2520needs%2520exhaustive%2520sampling%2520and%2520is%250Aexpensive.%2520These%2520are%2520strong%2520limitations.%2520We%2520tackle%2520these%2520with%2520a%2520novel%2520method%250Acalled%2520CWGrasp.%2520Our%2520key%2520idea%2520is%2520that%2520performing%2520geometry-based%2520reasoning%2520%2522early%250Aon%252C%2522%2520instead%2520of%2520%2522too%2520late%252C%2522%2520provides%2520rich%2520%2522control%2522%2520signals%2520for%2520inference.%2520To%250Athis%2520end%252C%2520CWGrasp%2520first%2520samples%2520a%2520plausible%2520reaching-direction%2520vector%2520%2528used%250Alater%2520for%2520both%2520the%2520arm%2520and%2520hand%2529%2520from%2520a%2520probabilistic%2520model%2520built%2520via%250Araycasting%2520from%2520the%2520object%2520and%2520collision%2520checking.%2520Then%252C%2520it%2520generates%2520a%250Areaching%2520body%2520with%2520a%2520desired%2520arm%2520direction%252C%2520as%2520well%2520as%2520a%2520%2522guiding%2522%2520grasping%250Ahand%2520with%2520a%2520desired%2520palm%2520direction%2520that%2520complies%2520with%2520the%2520arm%2527s%2520one.%250AEventually%252C%2520CWGrasp%2520refines%2520the%2520body%2520to%2520match%2520the%2520%2522guiding%2522%2520hand%252C%2520while%250Aplausibly%2520contacting%2520the%2520scene.%2520Notably%252C%2520generating%2520already-compatible%2520%2522parts%2522%250Agreatly%2520simplifies%2520the%2520%2522whole.%2522%2520Moreover%252C%2520CWGrasp%2520uniquely%2520tackles%2520both%2520right-%250Aand%2520left-hand%2520grasps.%2520We%2520evaluate%2520on%2520the%2520GRAB%2520and%2520ReplicaGrasp%2520datasets.%250ACWGrasp%2520outperforms%2520baselines%252C%2520at%2520lower%2520runtime%2520and%2520budget%252C%2520while%2520all%250Acomponents%2520help%2520performance.%2520Code%2520and%2520models%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Whole-body%20Grasp%20Synthesis%20with%20Directional%20Controllability&entry.906535625=Georgios%20Paschalidis%20and%20Romana%20Wilschut%20and%20Dimitrije%20Anti%C4%87%20and%20Omid%20Taheri%20and%20Dimitrios%20Tzionas&entry.1292438233=%20%20Synthesizing%203D%20whole-bodies%20that%20realistically%20grasp%20objects%20is%20useful%20for%0Aanimation%2C%20mixed%20reality%2C%20and%20robotics.%20This%20is%20challenging%2C%20because%20the%20hands%0Aand%20body%20need%20to%20look%20natural%20w.r.t.%20each%20other%2C%20the%20grasped%20object%2C%20as%20well%20as%0Athe%20local%20scene%20%28i.e.%2C%20a%20receptacle%20supporting%20the%20object%29.%20Only%20recent%20work%0Atackles%20this%2C%20with%20a%20divide-and-conquer%20approach%3B%20it%20first%20generates%20a%0A%22guiding%22%20right-hand%20grasp%2C%20and%20then%20searches%20for%20bodies%20that%20match%20this.%0AHowever%2C%20the%20guiding-hand%20synthesis%20lacks%20controllability%20and%20receptacle%0Aawareness%2C%20so%20it%20likely%20has%20an%20implausible%20direction%20%28i.e.%2C%20a%20body%20can%27t%20match%0Athis%20without%20penetrating%20the%20receptacle%29%20and%20needs%20corrections%20through%20major%0Apost-processing.%20Moreover%2C%20the%20body%20search%20needs%20exhaustive%20sampling%20and%20is%0Aexpensive.%20These%20are%20strong%20limitations.%20We%20tackle%20these%20with%20a%20novel%20method%0Acalled%20CWGrasp.%20Our%20key%20idea%20is%20that%20performing%20geometry-based%20reasoning%20%22early%0Aon%2C%22%20instead%20of%20%22too%20late%2C%22%20provides%20rich%20%22control%22%20signals%20for%20inference.%20To%0Athis%20end%2C%20CWGrasp%20first%20samples%20a%20plausible%20reaching-direction%20vector%20%28used%0Alater%20for%20both%20the%20arm%20and%20hand%29%20from%20a%20probabilistic%20model%20built%20via%0Araycasting%20from%20the%20object%20and%20collision%20checking.%20Then%2C%20it%20generates%20a%0Areaching%20body%20with%20a%20desired%20arm%20direction%2C%20as%20well%20as%20a%20%22guiding%22%20grasping%0Ahand%20with%20a%20desired%20palm%20direction%20that%20complies%20with%20the%20arm%27s%20one.%0AEventually%2C%20CWGrasp%20refines%20the%20body%20to%20match%20the%20%22guiding%22%20hand%2C%20while%0Aplausibly%20contacting%20the%20scene.%20Notably%2C%20generating%20already-compatible%20%22parts%22%0Agreatly%20simplifies%20the%20%22whole.%22%20Moreover%2C%20CWGrasp%20uniquely%20tackles%20both%20right-%0Aand%20left-hand%20grasps.%20We%20evaluate%20on%20the%20GRAB%20and%20ReplicaGrasp%20datasets.%0ACWGrasp%20outperforms%20baselines%2C%20at%20lower%20runtime%20and%20budget%2C%20while%20all%0Acomponents%20help%20performance.%20Code%20and%20models%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16770v1&entry.124074799=Read"},
{"title": "SFR-GNN: Simple and Fast Robust GNNs against Structural Attacks", "author": "Xing Ai and Guanyu Zhu and Yulin Zhu and Yu Zheng and Gaolei Li and Jianhua Li and Kai Zhou", "abstract": "  Graph Neural Networks (GNNs) have demonstrated commendable performance for\ngraph-structured data. Yet, GNNs are often vulnerable to adversarial structural\nattacks as embedding generation relies on graph topology. Existing efforts are\ndedicated to purifying the maliciously modified structure or applying adaptive\naggregation, thereby enhancing the robustness against adversarial structural\nattacks. It is inevitable for a defender to consume heavy computational costs\ndue to lacking prior knowledge about modified structures. To this end, we\npropose an efficient defense method, called Simple and Fast Robust Graph Neural\nNetwork (SFR-GNN), supported by mutual information theory. The SFR-GNN first\npre-trains a GNN model using node attributes and then fine-tunes it over the\nmodified graph in the manner of contrastive learning, which is free of\npurifying modified structures and adaptive aggregation, thus achieving great\nefficiency gains. Consequently, SFR-GNN exhibits a 24%--162% speedup compared\nto advanced robust models, demonstrating superior robustness for node\nclassification tasks.\n", "link": "http://arxiv.org/abs/2408.16537v1", "date": "2024-08-29", "relevancy": 2.3932, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.492}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4817}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SFR-GNN%3A%20Simple%20and%20Fast%20Robust%20GNNs%20against%20Structural%20Attacks&body=Title%3A%20SFR-GNN%3A%20Simple%20and%20Fast%20Robust%20GNNs%20against%20Structural%20Attacks%0AAuthor%3A%20Xing%20Ai%20and%20Guanyu%20Zhu%20and%20Yulin%20Zhu%20and%20Yu%20Zheng%20and%20Gaolei%20Li%20and%20Jianhua%20Li%20and%20Kai%20Zhou%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20commendable%20performance%20for%0Agraph-structured%20data.%20Yet%2C%20GNNs%20are%20often%20vulnerable%20to%20adversarial%20structural%0Aattacks%20as%20embedding%20generation%20relies%20on%20graph%20topology.%20Existing%20efforts%20are%0Adedicated%20to%20purifying%20the%20maliciously%20modified%20structure%20or%20applying%20adaptive%0Aaggregation%2C%20thereby%20enhancing%20the%20robustness%20against%20adversarial%20structural%0Aattacks.%20It%20is%20inevitable%20for%20a%20defender%20to%20consume%20heavy%20computational%20costs%0Adue%20to%20lacking%20prior%20knowledge%20about%20modified%20structures.%20To%20this%20end%2C%20we%0Apropose%20an%20efficient%20defense%20method%2C%20called%20Simple%20and%20Fast%20Robust%20Graph%20Neural%0ANetwork%20%28SFR-GNN%29%2C%20supported%20by%20mutual%20information%20theory.%20The%20SFR-GNN%20first%0Apre-trains%20a%20GNN%20model%20using%20node%20attributes%20and%20then%20fine-tunes%20it%20over%20the%0Amodified%20graph%20in%20the%20manner%20of%20contrastive%20learning%2C%20which%20is%20free%20of%0Apurifying%20modified%20structures%20and%20adaptive%20aggregation%2C%20thus%20achieving%20great%0Aefficiency%20gains.%20Consequently%2C%20SFR-GNN%20exhibits%20a%2024%25--162%25%20speedup%20compared%0Ato%20advanced%20robust%20models%2C%20demonstrating%20superior%20robustness%20for%20node%0Aclassification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSFR-GNN%253A%2520Simple%2520and%2520Fast%2520Robust%2520GNNs%2520against%2520Structural%2520Attacks%26entry.906535625%3DXing%2520Ai%2520and%2520Guanyu%2520Zhu%2520and%2520Yulin%2520Zhu%2520and%2520Yu%2520Zheng%2520and%2520Gaolei%2520Li%2520and%2520Jianhua%2520Li%2520and%2520Kai%2520Zhou%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520commendable%2520performance%2520for%250Agraph-structured%2520data.%2520Yet%252C%2520GNNs%2520are%2520often%2520vulnerable%2520to%2520adversarial%2520structural%250Aattacks%2520as%2520embedding%2520generation%2520relies%2520on%2520graph%2520topology.%2520Existing%2520efforts%2520are%250Adedicated%2520to%2520purifying%2520the%2520maliciously%2520modified%2520structure%2520or%2520applying%2520adaptive%250Aaggregation%252C%2520thereby%2520enhancing%2520the%2520robustness%2520against%2520adversarial%2520structural%250Aattacks.%2520It%2520is%2520inevitable%2520for%2520a%2520defender%2520to%2520consume%2520heavy%2520computational%2520costs%250Adue%2520to%2520lacking%2520prior%2520knowledge%2520about%2520modified%2520structures.%2520To%2520this%2520end%252C%2520we%250Apropose%2520an%2520efficient%2520defense%2520method%252C%2520called%2520Simple%2520and%2520Fast%2520Robust%2520Graph%2520Neural%250ANetwork%2520%2528SFR-GNN%2529%252C%2520supported%2520by%2520mutual%2520information%2520theory.%2520The%2520SFR-GNN%2520first%250Apre-trains%2520a%2520GNN%2520model%2520using%2520node%2520attributes%2520and%2520then%2520fine-tunes%2520it%2520over%2520the%250Amodified%2520graph%2520in%2520the%2520manner%2520of%2520contrastive%2520learning%252C%2520which%2520is%2520free%2520of%250Apurifying%2520modified%2520structures%2520and%2520adaptive%2520aggregation%252C%2520thus%2520achieving%2520great%250Aefficiency%2520gains.%2520Consequently%252C%2520SFR-GNN%2520exhibits%2520a%252024%2525--162%2525%2520speedup%2520compared%250Ato%2520advanced%2520robust%2520models%252C%2520demonstrating%2520superior%2520robustness%2520for%2520node%250Aclassification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFR-GNN%3A%20Simple%20and%20Fast%20Robust%20GNNs%20against%20Structural%20Attacks&entry.906535625=Xing%20Ai%20and%20Guanyu%20Zhu%20and%20Yulin%20Zhu%20and%20Yu%20Zheng%20and%20Gaolei%20Li%20and%20Jianhua%20Li%20and%20Kai%20Zhou&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20commendable%20performance%20for%0Agraph-structured%20data.%20Yet%2C%20GNNs%20are%20often%20vulnerable%20to%20adversarial%20structural%0Aattacks%20as%20embedding%20generation%20relies%20on%20graph%20topology.%20Existing%20efforts%20are%0Adedicated%20to%20purifying%20the%20maliciously%20modified%20structure%20or%20applying%20adaptive%0Aaggregation%2C%20thereby%20enhancing%20the%20robustness%20against%20adversarial%20structural%0Aattacks.%20It%20is%20inevitable%20for%20a%20defender%20to%20consume%20heavy%20computational%20costs%0Adue%20to%20lacking%20prior%20knowledge%20about%20modified%20structures.%20To%20this%20end%2C%20we%0Apropose%20an%20efficient%20defense%20method%2C%20called%20Simple%20and%20Fast%20Robust%20Graph%20Neural%0ANetwork%20%28SFR-GNN%29%2C%20supported%20by%20mutual%20information%20theory.%20The%20SFR-GNN%20first%0Apre-trains%20a%20GNN%20model%20using%20node%20attributes%20and%20then%20fine-tunes%20it%20over%20the%0Amodified%20graph%20in%20the%20manner%20of%20contrastive%20learning%2C%20which%20is%20free%20of%0Apurifying%20modified%20structures%20and%20adaptive%20aggregation%2C%20thus%20achieving%20great%0Aefficiency%20gains.%20Consequently%2C%20SFR-GNN%20exhibits%20a%2024%25--162%25%20speedup%20compared%0Ato%20advanced%20robust%20models%2C%20demonstrating%20superior%20robustness%20for%20node%0Aclassification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16537v1&entry.124074799=Read"},
{"title": "Adaptive Log-Euclidean Metrics for SPD Matrix Learning", "author": "Ziheng Chen and Yue Song and Tianyang Xu and Zhiwu Huang and Xiao-Jun Wu and Nicu Sebe", "abstract": "  Symmetric Positive Definite (SPD) matrices have received wide attention in\nmachine learning due to their intrinsic capacity to encode underlying\nstructural correlation in data. Many successful Riemannian metrics have been\nproposed to reflect the non-Euclidean geometry of SPD manifolds. However, most\nexisting metric tensors are fixed, which might lead to sub-optimal performance\nfor SPD matrix learning, especially for deep SPD neural networks. To remedy\nthis limitation, we leverage the commonly encountered pullback techniques and\npropose Adaptive Log-Euclidean Metrics (ALEMs), which extend the widely used\nLog-Euclidean Metric (LEM). Compared with the previous Riemannian metrics, our\nmetrics contain learnable parameters, which can better adapt to the complex\ndynamics of Riemannian neural networks with minor extra computations. We also\npresent a complete theoretical analysis to support our ALEMs, including\nalgebraic and Riemannian properties. The experimental and theoretical results\ndemonstrate the merit of the proposed metrics in improving the performance of\nSPD neural networks. The efficacy of our metrics is further showcased on a set\nof recently developed Riemannian building blocks, including Riemannian batch\nnormalization, Riemannian Residual blocks, and Riemannian classifiers.\n", "link": "http://arxiv.org/abs/2303.15477v5", "date": "2024-08-29", "relevancy": 2.379, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4835}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4721}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Log-Euclidean%20Metrics%20for%20SPD%20Matrix%20Learning&body=Title%3A%20Adaptive%20Log-Euclidean%20Metrics%20for%20SPD%20Matrix%20Learning%0AAuthor%3A%20Ziheng%20Chen%20and%20Yue%20Song%20and%20Tianyang%20Xu%20and%20Zhiwu%20Huang%20and%20Xiao-Jun%20Wu%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Symmetric%20Positive%20Definite%20%28SPD%29%20matrices%20have%20received%20wide%20attention%20in%0Amachine%20learning%20due%20to%20their%20intrinsic%20capacity%20to%20encode%20underlying%0Astructural%20correlation%20in%20data.%20Many%20successful%20Riemannian%20metrics%20have%20been%0Aproposed%20to%20reflect%20the%20non-Euclidean%20geometry%20of%20SPD%20manifolds.%20However%2C%20most%0Aexisting%20metric%20tensors%20are%20fixed%2C%20which%20might%20lead%20to%20sub-optimal%20performance%0Afor%20SPD%20matrix%20learning%2C%20especially%20for%20deep%20SPD%20neural%20networks.%20To%20remedy%0Athis%20limitation%2C%20we%20leverage%20the%20commonly%20encountered%20pullback%20techniques%20and%0Apropose%20Adaptive%20Log-Euclidean%20Metrics%20%28ALEMs%29%2C%20which%20extend%20the%20widely%20used%0ALog-Euclidean%20Metric%20%28LEM%29.%20Compared%20with%20the%20previous%20Riemannian%20metrics%2C%20our%0Ametrics%20contain%20learnable%20parameters%2C%20which%20can%20better%20adapt%20to%20the%20complex%0Adynamics%20of%20Riemannian%20neural%20networks%20with%20minor%20extra%20computations.%20We%20also%0Apresent%20a%20complete%20theoretical%20analysis%20to%20support%20our%20ALEMs%2C%20including%0Aalgebraic%20and%20Riemannian%20properties.%20The%20experimental%20and%20theoretical%20results%0Ademonstrate%20the%20merit%20of%20the%20proposed%20metrics%20in%20improving%20the%20performance%20of%0ASPD%20neural%20networks.%20The%20efficacy%20of%20our%20metrics%20is%20further%20showcased%20on%20a%20set%0Aof%20recently%20developed%20Riemannian%20building%20blocks%2C%20including%20Riemannian%20batch%0Anormalization%2C%20Riemannian%20Residual%20blocks%2C%20and%20Riemannian%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.15477v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Log-Euclidean%2520Metrics%2520for%2520SPD%2520Matrix%2520Learning%26entry.906535625%3DZiheng%2520Chen%2520and%2520Yue%2520Song%2520and%2520Tianyang%2520Xu%2520and%2520Zhiwu%2520Huang%2520and%2520Xiao-Jun%2520Wu%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Symmetric%2520Positive%2520Definite%2520%2528SPD%2529%2520matrices%2520have%2520received%2520wide%2520attention%2520in%250Amachine%2520learning%2520due%2520to%2520their%2520intrinsic%2520capacity%2520to%2520encode%2520underlying%250Astructural%2520correlation%2520in%2520data.%2520Many%2520successful%2520Riemannian%2520metrics%2520have%2520been%250Aproposed%2520to%2520reflect%2520the%2520non-Euclidean%2520geometry%2520of%2520SPD%2520manifolds.%2520However%252C%2520most%250Aexisting%2520metric%2520tensors%2520are%2520fixed%252C%2520which%2520might%2520lead%2520to%2520sub-optimal%2520performance%250Afor%2520SPD%2520matrix%2520learning%252C%2520especially%2520for%2520deep%2520SPD%2520neural%2520networks.%2520To%2520remedy%250Athis%2520limitation%252C%2520we%2520leverage%2520the%2520commonly%2520encountered%2520pullback%2520techniques%2520and%250Apropose%2520Adaptive%2520Log-Euclidean%2520Metrics%2520%2528ALEMs%2529%252C%2520which%2520extend%2520the%2520widely%2520used%250ALog-Euclidean%2520Metric%2520%2528LEM%2529.%2520Compared%2520with%2520the%2520previous%2520Riemannian%2520metrics%252C%2520our%250Ametrics%2520contain%2520learnable%2520parameters%252C%2520which%2520can%2520better%2520adapt%2520to%2520the%2520complex%250Adynamics%2520of%2520Riemannian%2520neural%2520networks%2520with%2520minor%2520extra%2520computations.%2520We%2520also%250Apresent%2520a%2520complete%2520theoretical%2520analysis%2520to%2520support%2520our%2520ALEMs%252C%2520including%250Aalgebraic%2520and%2520Riemannian%2520properties.%2520The%2520experimental%2520and%2520theoretical%2520results%250Ademonstrate%2520the%2520merit%2520of%2520the%2520proposed%2520metrics%2520in%2520improving%2520the%2520performance%2520of%250ASPD%2520neural%2520networks.%2520The%2520efficacy%2520of%2520our%2520metrics%2520is%2520further%2520showcased%2520on%2520a%2520set%250Aof%2520recently%2520developed%2520Riemannian%2520building%2520blocks%252C%2520including%2520Riemannian%2520batch%250Anormalization%252C%2520Riemannian%2520Residual%2520blocks%252C%2520and%2520Riemannian%2520classifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.15477v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Log-Euclidean%20Metrics%20for%20SPD%20Matrix%20Learning&entry.906535625=Ziheng%20Chen%20and%20Yue%20Song%20and%20Tianyang%20Xu%20and%20Zhiwu%20Huang%20and%20Xiao-Jun%20Wu%20and%20Nicu%20Sebe&entry.1292438233=%20%20Symmetric%20Positive%20Definite%20%28SPD%29%20matrices%20have%20received%20wide%20attention%20in%0Amachine%20learning%20due%20to%20their%20intrinsic%20capacity%20to%20encode%20underlying%0Astructural%20correlation%20in%20data.%20Many%20successful%20Riemannian%20metrics%20have%20been%0Aproposed%20to%20reflect%20the%20non-Euclidean%20geometry%20of%20SPD%20manifolds.%20However%2C%20most%0Aexisting%20metric%20tensors%20are%20fixed%2C%20which%20might%20lead%20to%20sub-optimal%20performance%0Afor%20SPD%20matrix%20learning%2C%20especially%20for%20deep%20SPD%20neural%20networks.%20To%20remedy%0Athis%20limitation%2C%20we%20leverage%20the%20commonly%20encountered%20pullback%20techniques%20and%0Apropose%20Adaptive%20Log-Euclidean%20Metrics%20%28ALEMs%29%2C%20which%20extend%20the%20widely%20used%0ALog-Euclidean%20Metric%20%28LEM%29.%20Compared%20with%20the%20previous%20Riemannian%20metrics%2C%20our%0Ametrics%20contain%20learnable%20parameters%2C%20which%20can%20better%20adapt%20to%20the%20complex%0Adynamics%20of%20Riemannian%20neural%20networks%20with%20minor%20extra%20computations.%20We%20also%0Apresent%20a%20complete%20theoretical%20analysis%20to%20support%20our%20ALEMs%2C%20including%0Aalgebraic%20and%20Riemannian%20properties.%20The%20experimental%20and%20theoretical%20results%0Ademonstrate%20the%20merit%20of%20the%20proposed%20metrics%20in%20improving%20the%20performance%20of%0ASPD%20neural%20networks.%20The%20efficacy%20of%20our%20metrics%20is%20further%20showcased%20on%20a%20set%0Aof%20recently%20developed%20Riemannian%20building%20blocks%2C%20including%20Riemannian%20batch%0Anormalization%2C%20Riemannian%20Residual%20blocks%2C%20and%20Riemannian%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15477v5&entry.124074799=Read"},
{"title": "Next Level Message-Passing with Hierarchical Support Graphs", "author": "Carlos Vonessen and Florian Gr\u00f6tschla and Roger Wattenhofer", "abstract": "  Message-Passing Neural Networks (MPNNs) are extensively employed in graph\nlearning tasks but suffer from limitations such as the restricted scope of\ninformation exchange, by being confined to neighboring nodes during each round\nof message passing. Various strategies have been proposed to address these\nlimitations, including incorporating virtual nodes to facilitate global\ninformation exchange. In this study, we introduce the Hierarchical Support\nGraph (HSG), an extension of the virtual node concept created through recursive\ncoarsening of the original graph. This approach provides a flexible framework\nfor enhancing information flow in graphs, independent of the specific MPNN\nlayers utilized. We present a theoretical analysis of HSGs, investigate their\nempirical performance, and demonstrate that HSGs can surpass other methods\naugmented with virtual nodes, achieving state-of-the-art results across\nmultiple datasets.\n", "link": "http://arxiv.org/abs/2406.15852v2", "date": "2024-08-29", "relevancy": 2.353, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4838}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4766}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next%20Level%20Message-Passing%20with%20Hierarchical%20Support%20Graphs&body=Title%3A%20Next%20Level%20Message-Passing%20with%20Hierarchical%20Support%20Graphs%0AAuthor%3A%20Carlos%20Vonessen%20and%20Florian%20Gr%C3%B6tschla%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20Message-Passing%20Neural%20Networks%20%28MPNNs%29%20are%20extensively%20employed%20in%20graph%0Alearning%20tasks%20but%20suffer%20from%20limitations%20such%20as%20the%20restricted%20scope%20of%0Ainformation%20exchange%2C%20by%20being%20confined%20to%20neighboring%20nodes%20during%20each%20round%0Aof%20message%20passing.%20Various%20strategies%20have%20been%20proposed%20to%20address%20these%0Alimitations%2C%20including%20incorporating%20virtual%20nodes%20to%20facilitate%20global%0Ainformation%20exchange.%20In%20this%20study%2C%20we%20introduce%20the%20Hierarchical%20Support%0AGraph%20%28HSG%29%2C%20an%20extension%20of%20the%20virtual%20node%20concept%20created%20through%20recursive%0Acoarsening%20of%20the%20original%20graph.%20This%20approach%20provides%20a%20flexible%20framework%0Afor%20enhancing%20information%20flow%20in%20graphs%2C%20independent%20of%20the%20specific%20MPNN%0Alayers%20utilized.%20We%20present%20a%20theoretical%20analysis%20of%20HSGs%2C%20investigate%20their%0Aempirical%20performance%2C%20and%20demonstrate%20that%20HSGs%20can%20surpass%20other%20methods%0Aaugmented%20with%20virtual%20nodes%2C%20achieving%20state-of-the-art%20results%20across%0Amultiple%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15852v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext%2520Level%2520Message-Passing%2520with%2520Hierarchical%2520Support%2520Graphs%26entry.906535625%3DCarlos%2520Vonessen%2520and%2520Florian%2520Gr%25C3%25B6tschla%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520Message-Passing%2520Neural%2520Networks%2520%2528MPNNs%2529%2520are%2520extensively%2520employed%2520in%2520graph%250Alearning%2520tasks%2520but%2520suffer%2520from%2520limitations%2520such%2520as%2520the%2520restricted%2520scope%2520of%250Ainformation%2520exchange%252C%2520by%2520being%2520confined%2520to%2520neighboring%2520nodes%2520during%2520each%2520round%250Aof%2520message%2520passing.%2520Various%2520strategies%2520have%2520been%2520proposed%2520to%2520address%2520these%250Alimitations%252C%2520including%2520incorporating%2520virtual%2520nodes%2520to%2520facilitate%2520global%250Ainformation%2520exchange.%2520In%2520this%2520study%252C%2520we%2520introduce%2520the%2520Hierarchical%2520Support%250AGraph%2520%2528HSG%2529%252C%2520an%2520extension%2520of%2520the%2520virtual%2520node%2520concept%2520created%2520through%2520recursive%250Acoarsening%2520of%2520the%2520original%2520graph.%2520This%2520approach%2520provides%2520a%2520flexible%2520framework%250Afor%2520enhancing%2520information%2520flow%2520in%2520graphs%252C%2520independent%2520of%2520the%2520specific%2520MPNN%250Alayers%2520utilized.%2520We%2520present%2520a%2520theoretical%2520analysis%2520of%2520HSGs%252C%2520investigate%2520their%250Aempirical%2520performance%252C%2520and%2520demonstrate%2520that%2520HSGs%2520can%2520surpass%2520other%2520methods%250Aaugmented%2520with%2520virtual%2520nodes%252C%2520achieving%2520state-of-the-art%2520results%2520across%250Amultiple%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15852v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next%20Level%20Message-Passing%20with%20Hierarchical%20Support%20Graphs&entry.906535625=Carlos%20Vonessen%20and%20Florian%20Gr%C3%B6tschla%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20Message-Passing%20Neural%20Networks%20%28MPNNs%29%20are%20extensively%20employed%20in%20graph%0Alearning%20tasks%20but%20suffer%20from%20limitations%20such%20as%20the%20restricted%20scope%20of%0Ainformation%20exchange%2C%20by%20being%20confined%20to%20neighboring%20nodes%20during%20each%20round%0Aof%20message%20passing.%20Various%20strategies%20have%20been%20proposed%20to%20address%20these%0Alimitations%2C%20including%20incorporating%20virtual%20nodes%20to%20facilitate%20global%0Ainformation%20exchange.%20In%20this%20study%2C%20we%20introduce%20the%20Hierarchical%20Support%0AGraph%20%28HSG%29%2C%20an%20extension%20of%20the%20virtual%20node%20concept%20created%20through%20recursive%0Acoarsening%20of%20the%20original%20graph.%20This%20approach%20provides%20a%20flexible%20framework%0Afor%20enhancing%20information%20flow%20in%20graphs%2C%20independent%20of%20the%20specific%20MPNN%0Alayers%20utilized.%20We%20present%20a%20theoretical%20analysis%20of%20HSGs%2C%20investigate%20their%0Aempirical%20performance%2C%20and%20demonstrate%20that%20HSGs%20can%20surpass%20other%20methods%0Aaugmented%20with%20virtual%20nodes%2C%20achieving%20state-of-the-art%20results%20across%0Amultiple%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15852v2&entry.124074799=Read"},
{"title": "Eigen-Cluster VIS: Improving Weakly-supervised Video Instance\n  Segmentation by Leveraging Spatio-temporal Consistency", "author": "Farnoosh Arefi and Amir M. Mansourian and Shohreh Kasaei", "abstract": "  The performance of Video Instance Segmentation (VIS) methods has improved\nsignificantly with the advent of transformer networks. However, these networks\noften face challenges in training due to the high annotation cost. To address\nthis, unsupervised and weakly-supervised methods have been developed to reduce\nthe dependency on annotations. This work introduces a novel weakly-supervised\nmethod called Eigen-cluster VIS that, without requiring any mask annotations,\nachieves competitive accuracy compared to other VIS approaches. This method is\nbased on two key innovations: a Temporal Eigenvalue Loss (TEL) and a clip-level\nQuality Cluster Coefficient (QCC). The TEL ensures temporal coherence by\nleveraging the eigenvalues of the Laplacian matrix derived from graph adjacency\nmatrices. By minimizing the mean absolute error (MAE) between the eigenvalues\nof adjacent frames, this loss function promotes smooth transitions and stable\nsegmentation boundaries over time, reducing temporal discontinuities and\nimproving overall segmentation quality. The QCC employs the K-means method to\nensure the quality of spatio-temporal clusters without relying on ground truth\nmasks. Using the Davies-Bouldin score, the QCC provides an unsupervised measure\nof feature discrimination, allowing the model to self-evaluate and adapt to\nvarying object distributions, enhancing robustness during the testing phase.\nThese enhancements are computationally efficient and straightforward, offering\nsignificant performance gains without additional annotated data. The proposed\nEigen-Cluster VIS method is evaluated on the YouTube-VIS 2019/2021 and OVIS\ndatasets, demonstrating that it effectively narrows the performance gap between\nthe fully-supervised and weakly-supervised VIS approaches. The code is\navailable on: https://github.com/farnooshar/EigenClusterVIS\n", "link": "http://arxiv.org/abs/2408.16661v1", "date": "2024-08-29", "relevancy": 2.3183, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5858}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5753}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eigen-Cluster%20VIS%3A%20Improving%20Weakly-supervised%20Video%20Instance%0A%20%20Segmentation%20by%20Leveraging%20Spatio-temporal%20Consistency&body=Title%3A%20Eigen-Cluster%20VIS%3A%20Improving%20Weakly-supervised%20Video%20Instance%0A%20%20Segmentation%20by%20Leveraging%20Spatio-temporal%20Consistency%0AAuthor%3A%20Farnoosh%20Arefi%20and%20Amir%20M.%20Mansourian%20and%20Shohreh%20Kasaei%0AAbstract%3A%20%20%20The%20performance%20of%20Video%20Instance%20Segmentation%20%28VIS%29%20methods%20has%20improved%0Asignificantly%20with%20the%20advent%20of%20transformer%20networks.%20However%2C%20these%20networks%0Aoften%20face%20challenges%20in%20training%20due%20to%20the%20high%20annotation%20cost.%20To%20address%0Athis%2C%20unsupervised%20and%20weakly-supervised%20methods%20have%20been%20developed%20to%20reduce%0Athe%20dependency%20on%20annotations.%20This%20work%20introduces%20a%20novel%20weakly-supervised%0Amethod%20called%20Eigen-cluster%20VIS%20that%2C%20without%20requiring%20any%20mask%20annotations%2C%0Aachieves%20competitive%20accuracy%20compared%20to%20other%20VIS%20approaches.%20This%20method%20is%0Abased%20on%20two%20key%20innovations%3A%20a%20Temporal%20Eigenvalue%20Loss%20%28TEL%29%20and%20a%20clip-level%0AQuality%20Cluster%20Coefficient%20%28QCC%29.%20The%20TEL%20ensures%20temporal%20coherence%20by%0Aleveraging%20the%20eigenvalues%20of%20the%20Laplacian%20matrix%20derived%20from%20graph%20adjacency%0Amatrices.%20By%20minimizing%20the%20mean%20absolute%20error%20%28MAE%29%20between%20the%20eigenvalues%0Aof%20adjacent%20frames%2C%20this%20loss%20function%20promotes%20smooth%20transitions%20and%20stable%0Asegmentation%20boundaries%20over%20time%2C%20reducing%20temporal%20discontinuities%20and%0Aimproving%20overall%20segmentation%20quality.%20The%20QCC%20employs%20the%20K-means%20method%20to%0Aensure%20the%20quality%20of%20spatio-temporal%20clusters%20without%20relying%20on%20ground%20truth%0Amasks.%20Using%20the%20Davies-Bouldin%20score%2C%20the%20QCC%20provides%20an%20unsupervised%20measure%0Aof%20feature%20discrimination%2C%20allowing%20the%20model%20to%20self-evaluate%20and%20adapt%20to%0Avarying%20object%20distributions%2C%20enhancing%20robustness%20during%20the%20testing%20phase.%0AThese%20enhancements%20are%20computationally%20efficient%20and%20straightforward%2C%20offering%0Asignificant%20performance%20gains%20without%20additional%20annotated%20data.%20The%20proposed%0AEigen-Cluster%20VIS%20method%20is%20evaluated%20on%20the%20YouTube-VIS%202019/2021%20and%20OVIS%0Adatasets%2C%20demonstrating%20that%20it%20effectively%20narrows%20the%20performance%20gap%20between%0Athe%20fully-supervised%20and%20weakly-supervised%20VIS%20approaches.%20The%20code%20is%0Aavailable%20on%3A%20https%3A//github.com/farnooshar/EigenClusterVIS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEigen-Cluster%2520VIS%253A%2520Improving%2520Weakly-supervised%2520Video%2520Instance%250A%2520%2520Segmentation%2520by%2520Leveraging%2520Spatio-temporal%2520Consistency%26entry.906535625%3DFarnoosh%2520Arefi%2520and%2520Amir%2520M.%2520Mansourian%2520and%2520Shohreh%2520Kasaei%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520Video%2520Instance%2520Segmentation%2520%2528VIS%2529%2520methods%2520has%2520improved%250Asignificantly%2520with%2520the%2520advent%2520of%2520transformer%2520networks.%2520However%252C%2520these%2520networks%250Aoften%2520face%2520challenges%2520in%2520training%2520due%2520to%2520the%2520high%2520annotation%2520cost.%2520To%2520address%250Athis%252C%2520unsupervised%2520and%2520weakly-supervised%2520methods%2520have%2520been%2520developed%2520to%2520reduce%250Athe%2520dependency%2520on%2520annotations.%2520This%2520work%2520introduces%2520a%2520novel%2520weakly-supervised%250Amethod%2520called%2520Eigen-cluster%2520VIS%2520that%252C%2520without%2520requiring%2520any%2520mask%2520annotations%252C%250Aachieves%2520competitive%2520accuracy%2520compared%2520to%2520other%2520VIS%2520approaches.%2520This%2520method%2520is%250Abased%2520on%2520two%2520key%2520innovations%253A%2520a%2520Temporal%2520Eigenvalue%2520Loss%2520%2528TEL%2529%2520and%2520a%2520clip-level%250AQuality%2520Cluster%2520Coefficient%2520%2528QCC%2529.%2520The%2520TEL%2520ensures%2520temporal%2520coherence%2520by%250Aleveraging%2520the%2520eigenvalues%2520of%2520the%2520Laplacian%2520matrix%2520derived%2520from%2520graph%2520adjacency%250Amatrices.%2520By%2520minimizing%2520the%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520between%2520the%2520eigenvalues%250Aof%2520adjacent%2520frames%252C%2520this%2520loss%2520function%2520promotes%2520smooth%2520transitions%2520and%2520stable%250Asegmentation%2520boundaries%2520over%2520time%252C%2520reducing%2520temporal%2520discontinuities%2520and%250Aimproving%2520overall%2520segmentation%2520quality.%2520The%2520QCC%2520employs%2520the%2520K-means%2520method%2520to%250Aensure%2520the%2520quality%2520of%2520spatio-temporal%2520clusters%2520without%2520relying%2520on%2520ground%2520truth%250Amasks.%2520Using%2520the%2520Davies-Bouldin%2520score%252C%2520the%2520QCC%2520provides%2520an%2520unsupervised%2520measure%250Aof%2520feature%2520discrimination%252C%2520allowing%2520the%2520model%2520to%2520self-evaluate%2520and%2520adapt%2520to%250Avarying%2520object%2520distributions%252C%2520enhancing%2520robustness%2520during%2520the%2520testing%2520phase.%250AThese%2520enhancements%2520are%2520computationally%2520efficient%2520and%2520straightforward%252C%2520offering%250Asignificant%2520performance%2520gains%2520without%2520additional%2520annotated%2520data.%2520The%2520proposed%250AEigen-Cluster%2520VIS%2520method%2520is%2520evaluated%2520on%2520the%2520YouTube-VIS%25202019/2021%2520and%2520OVIS%250Adatasets%252C%2520demonstrating%2520that%2520it%2520effectively%2520narrows%2520the%2520performance%2520gap%2520between%250Athe%2520fully-supervised%2520and%2520weakly-supervised%2520VIS%2520approaches.%2520The%2520code%2520is%250Aavailable%2520on%253A%2520https%253A//github.com/farnooshar/EigenClusterVIS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eigen-Cluster%20VIS%3A%20Improving%20Weakly-supervised%20Video%20Instance%0A%20%20Segmentation%20by%20Leveraging%20Spatio-temporal%20Consistency&entry.906535625=Farnoosh%20Arefi%20and%20Amir%20M.%20Mansourian%20and%20Shohreh%20Kasaei&entry.1292438233=%20%20The%20performance%20of%20Video%20Instance%20Segmentation%20%28VIS%29%20methods%20has%20improved%0Asignificantly%20with%20the%20advent%20of%20transformer%20networks.%20However%2C%20these%20networks%0Aoften%20face%20challenges%20in%20training%20due%20to%20the%20high%20annotation%20cost.%20To%20address%0Athis%2C%20unsupervised%20and%20weakly-supervised%20methods%20have%20been%20developed%20to%20reduce%0Athe%20dependency%20on%20annotations.%20This%20work%20introduces%20a%20novel%20weakly-supervised%0Amethod%20called%20Eigen-cluster%20VIS%20that%2C%20without%20requiring%20any%20mask%20annotations%2C%0Aachieves%20competitive%20accuracy%20compared%20to%20other%20VIS%20approaches.%20This%20method%20is%0Abased%20on%20two%20key%20innovations%3A%20a%20Temporal%20Eigenvalue%20Loss%20%28TEL%29%20and%20a%20clip-level%0AQuality%20Cluster%20Coefficient%20%28QCC%29.%20The%20TEL%20ensures%20temporal%20coherence%20by%0Aleveraging%20the%20eigenvalues%20of%20the%20Laplacian%20matrix%20derived%20from%20graph%20adjacency%0Amatrices.%20By%20minimizing%20the%20mean%20absolute%20error%20%28MAE%29%20between%20the%20eigenvalues%0Aof%20adjacent%20frames%2C%20this%20loss%20function%20promotes%20smooth%20transitions%20and%20stable%0Asegmentation%20boundaries%20over%20time%2C%20reducing%20temporal%20discontinuities%20and%0Aimproving%20overall%20segmentation%20quality.%20The%20QCC%20employs%20the%20K-means%20method%20to%0Aensure%20the%20quality%20of%20spatio-temporal%20clusters%20without%20relying%20on%20ground%20truth%0Amasks.%20Using%20the%20Davies-Bouldin%20score%2C%20the%20QCC%20provides%20an%20unsupervised%20measure%0Aof%20feature%20discrimination%2C%20allowing%20the%20model%20to%20self-evaluate%20and%20adapt%20to%0Avarying%20object%20distributions%2C%20enhancing%20robustness%20during%20the%20testing%20phase.%0AThese%20enhancements%20are%20computationally%20efficient%20and%20straightforward%2C%20offering%0Asignificant%20performance%20gains%20without%20additional%20annotated%20data.%20The%20proposed%0AEigen-Cluster%20VIS%20method%20is%20evaluated%20on%20the%20YouTube-VIS%202019/2021%20and%20OVIS%0Adatasets%2C%20demonstrating%20that%20it%20effectively%20narrows%20the%20performance%20gap%20between%0Athe%20fully-supervised%20and%20weakly-supervised%20VIS%20approaches.%20The%20code%20is%0Aavailable%20on%3A%20https%3A//github.com/farnooshar/EigenClusterVIS%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16661v1&entry.124074799=Read"},
{"title": "In-Hand Following of Deformable Linear Objects Using Dexterous Fingers\n  with Tactile Sensing", "author": "Mingrui Yu and Boyuan Liang and Xiang Zhang and Xinghao Zhu and Lingfeng Sun and Changhao Wang and Shiji Song and Xiang Li and Masayoshi Tomizuka", "abstract": "  Most research on deformable linear object (DLO) manipulation assumes rigid\ngrasping. However, beyond rigid grasping and re-grasping, in-hand following is\nalso an essential skill that humans use to dexterously manipulate DLOs, which\nrequires continuously changing the grasp point by in-hand sliding while holding\nthe DLO to prevent it from falling. Achieving such a skill is very challenging\nfor robots without using specially designed but not versatile end-effectors.\nPrevious works have attempted using generic parallel grippers, but their\nrobustness is unsatisfactory owing to the conflict between following and\nholding, which is hard to balance with a one-degree-of-freedom gripper. In this\nwork, inspired by how humans use fingers to follow DLOs, we explore the usage\nof a generic dexterous hand with tactile sensing to imitate human skills and\nachieve robust in-hand DLO following. To enable the hardware system to function\nin the real world, we develop a framework that includes Cartesian-space\narm-hand control, tactile-based in-hand 3-D DLO pose estimation, and\ntask-specific motion design. Experimental results demonstrate the significant\nsuperiority of our method over using parallel grippers, as well as its great\nrobustness, generalizability, and efficiency.\n", "link": "http://arxiv.org/abs/2403.12676v2", "date": "2024-08-29", "relevancy": 2.2957, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6209}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5451}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Hand%20Following%20of%20Deformable%20Linear%20Objects%20Using%20Dexterous%20Fingers%0A%20%20with%20Tactile%20Sensing&body=Title%3A%20In-Hand%20Following%20of%20Deformable%20Linear%20Objects%20Using%20Dexterous%20Fingers%0A%20%20with%20Tactile%20Sensing%0AAuthor%3A%20Mingrui%20Yu%20and%20Boyuan%20Liang%20and%20Xiang%20Zhang%20and%20Xinghao%20Zhu%20and%20Lingfeng%20Sun%20and%20Changhao%20Wang%20and%20Shiji%20Song%20and%20Xiang%20Li%20and%20Masayoshi%20Tomizuka%0AAbstract%3A%20%20%20Most%20research%20on%20deformable%20linear%20object%20%28DLO%29%20manipulation%20assumes%20rigid%0Agrasping.%20However%2C%20beyond%20rigid%20grasping%20and%20re-grasping%2C%20in-hand%20following%20is%0Aalso%20an%20essential%20skill%20that%20humans%20use%20to%20dexterously%20manipulate%20DLOs%2C%20which%0Arequires%20continuously%20changing%20the%20grasp%20point%20by%20in-hand%20sliding%20while%20holding%0Athe%20DLO%20to%20prevent%20it%20from%20falling.%20Achieving%20such%20a%20skill%20is%20very%20challenging%0Afor%20robots%20without%20using%20specially%20designed%20but%20not%20versatile%20end-effectors.%0APrevious%20works%20have%20attempted%20using%20generic%20parallel%20grippers%2C%20but%20their%0Arobustness%20is%20unsatisfactory%20owing%20to%20the%20conflict%20between%20following%20and%0Aholding%2C%20which%20is%20hard%20to%20balance%20with%20a%20one-degree-of-freedom%20gripper.%20In%20this%0Awork%2C%20inspired%20by%20how%20humans%20use%20fingers%20to%20follow%20DLOs%2C%20we%20explore%20the%20usage%0Aof%20a%20generic%20dexterous%20hand%20with%20tactile%20sensing%20to%20imitate%20human%20skills%20and%0Aachieve%20robust%20in-hand%20DLO%20following.%20To%20enable%20the%20hardware%20system%20to%20function%0Ain%20the%20real%20world%2C%20we%20develop%20a%20framework%20that%20includes%20Cartesian-space%0Aarm-hand%20control%2C%20tactile-based%20in-hand%203-D%20DLO%20pose%20estimation%2C%20and%0Atask-specific%20motion%20design.%20Experimental%20results%20demonstrate%20the%20significant%0Asuperiority%20of%20our%20method%20over%20using%20parallel%20grippers%2C%20as%20well%20as%20its%20great%0Arobustness%2C%20generalizability%2C%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12676v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Hand%2520Following%2520of%2520Deformable%2520Linear%2520Objects%2520Using%2520Dexterous%2520Fingers%250A%2520%2520with%2520Tactile%2520Sensing%26entry.906535625%3DMingrui%2520Yu%2520and%2520Boyuan%2520Liang%2520and%2520Xiang%2520Zhang%2520and%2520Xinghao%2520Zhu%2520and%2520Lingfeng%2520Sun%2520and%2520Changhao%2520Wang%2520and%2520Shiji%2520Song%2520and%2520Xiang%2520Li%2520and%2520Masayoshi%2520Tomizuka%26entry.1292438233%3D%2520%2520Most%2520research%2520on%2520deformable%2520linear%2520object%2520%2528DLO%2529%2520manipulation%2520assumes%2520rigid%250Agrasping.%2520However%252C%2520beyond%2520rigid%2520grasping%2520and%2520re-grasping%252C%2520in-hand%2520following%2520is%250Aalso%2520an%2520essential%2520skill%2520that%2520humans%2520use%2520to%2520dexterously%2520manipulate%2520DLOs%252C%2520which%250Arequires%2520continuously%2520changing%2520the%2520grasp%2520point%2520by%2520in-hand%2520sliding%2520while%2520holding%250Athe%2520DLO%2520to%2520prevent%2520it%2520from%2520falling.%2520Achieving%2520such%2520a%2520skill%2520is%2520very%2520challenging%250Afor%2520robots%2520without%2520using%2520specially%2520designed%2520but%2520not%2520versatile%2520end-effectors.%250APrevious%2520works%2520have%2520attempted%2520using%2520generic%2520parallel%2520grippers%252C%2520but%2520their%250Arobustness%2520is%2520unsatisfactory%2520owing%2520to%2520the%2520conflict%2520between%2520following%2520and%250Aholding%252C%2520which%2520is%2520hard%2520to%2520balance%2520with%2520a%2520one-degree-of-freedom%2520gripper.%2520In%2520this%250Awork%252C%2520inspired%2520by%2520how%2520humans%2520use%2520fingers%2520to%2520follow%2520DLOs%252C%2520we%2520explore%2520the%2520usage%250Aof%2520a%2520generic%2520dexterous%2520hand%2520with%2520tactile%2520sensing%2520to%2520imitate%2520human%2520skills%2520and%250Aachieve%2520robust%2520in-hand%2520DLO%2520following.%2520To%2520enable%2520the%2520hardware%2520system%2520to%2520function%250Ain%2520the%2520real%2520world%252C%2520we%2520develop%2520a%2520framework%2520that%2520includes%2520Cartesian-space%250Aarm-hand%2520control%252C%2520tactile-based%2520in-hand%25203-D%2520DLO%2520pose%2520estimation%252C%2520and%250Atask-specific%2520motion%2520design.%2520Experimental%2520results%2520demonstrate%2520the%2520significant%250Asuperiority%2520of%2520our%2520method%2520over%2520using%2520parallel%2520grippers%252C%2520as%2520well%2520as%2520its%2520great%250Arobustness%252C%2520generalizability%252C%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12676v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Hand%20Following%20of%20Deformable%20Linear%20Objects%20Using%20Dexterous%20Fingers%0A%20%20with%20Tactile%20Sensing&entry.906535625=Mingrui%20Yu%20and%20Boyuan%20Liang%20and%20Xiang%20Zhang%20and%20Xinghao%20Zhu%20and%20Lingfeng%20Sun%20and%20Changhao%20Wang%20and%20Shiji%20Song%20and%20Xiang%20Li%20and%20Masayoshi%20Tomizuka&entry.1292438233=%20%20Most%20research%20on%20deformable%20linear%20object%20%28DLO%29%20manipulation%20assumes%20rigid%0Agrasping.%20However%2C%20beyond%20rigid%20grasping%20and%20re-grasping%2C%20in-hand%20following%20is%0Aalso%20an%20essential%20skill%20that%20humans%20use%20to%20dexterously%20manipulate%20DLOs%2C%20which%0Arequires%20continuously%20changing%20the%20grasp%20point%20by%20in-hand%20sliding%20while%20holding%0Athe%20DLO%20to%20prevent%20it%20from%20falling.%20Achieving%20such%20a%20skill%20is%20very%20challenging%0Afor%20robots%20without%20using%20specially%20designed%20but%20not%20versatile%20end-effectors.%0APrevious%20works%20have%20attempted%20using%20generic%20parallel%20grippers%2C%20but%20their%0Arobustness%20is%20unsatisfactory%20owing%20to%20the%20conflict%20between%20following%20and%0Aholding%2C%20which%20is%20hard%20to%20balance%20with%20a%20one-degree-of-freedom%20gripper.%20In%20this%0Awork%2C%20inspired%20by%20how%20humans%20use%20fingers%20to%20follow%20DLOs%2C%20we%20explore%20the%20usage%0Aof%20a%20generic%20dexterous%20hand%20with%20tactile%20sensing%20to%20imitate%20human%20skills%20and%0Aachieve%20robust%20in-hand%20DLO%20following.%20To%20enable%20the%20hardware%20system%20to%20function%0Ain%20the%20real%20world%2C%20we%20develop%20a%20framework%20that%20includes%20Cartesian-space%0Aarm-hand%20control%2C%20tactile-based%20in-hand%203-D%20DLO%20pose%20estimation%2C%20and%0Atask-specific%20motion%20design.%20Experimental%20results%20demonstrate%20the%20significant%0Asuperiority%20of%20our%20method%20over%20using%20parallel%20grippers%2C%20as%20well%20as%20its%20great%0Arobustness%2C%20generalizability%2C%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12676v2&entry.124074799=Read"},
{"title": "Easy, Interpretable, Effective: openSMILE for voice deepfake detection", "author": "Octavian Pascu and Dan Oneata and Horia Cucu and Nicolas M. M\u00fcller", "abstract": "  In this paper, we demonstrate that attacks in the latest ASVspoof5 dataset --\na de facto standard in the field of voice authenticity and deepfake detection\n-- can be identified with surprising accuracy using a small subset of very\nsimplistic features. These are derived from the openSMILE library, and are\nscalar-valued, easy to compute, and human interpretable. For example, attack\nA10`s unvoiced segments have a mean length of 0.09 +- 0.02, while bona fide\ninstances have a mean length of 0.18 +- 0.07. Using this feature alone, a\nthreshold classifier achieves an Equal Error Rate (EER) of 10.3% for attack\nA10. Similarly, across all attacks, we achieve up to 0.8% EER, with an overall\nEER of 15.7 +- 6.0%. We explore the generalization capabilities of these\nfeatures and find that some of them transfer effectively between attacks,\nprimarily when the attacks originate from similar Text-to-Speech (TTS)\narchitectures. This finding may indicate that voice anti-spoofing is, in part,\na problem of identifying and remembering signatures or fingerprints of\nindividual TTS systems. This allows to better understand anti-spoofing models\nand their challenges in real-world application.\n", "link": "http://arxiv.org/abs/2408.15775v2", "date": "2024-08-29", "relevancy": 2.2503, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4579}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4505}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Easy%2C%20Interpretable%2C%20Effective%3A%20openSMILE%20for%20voice%20deepfake%20detection&body=Title%3A%20Easy%2C%20Interpretable%2C%20Effective%3A%20openSMILE%20for%20voice%20deepfake%20detection%0AAuthor%3A%20Octavian%20Pascu%20and%20Dan%20Oneata%20and%20Horia%20Cucu%20and%20Nicolas%20M.%20M%C3%BCller%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20demonstrate%20that%20attacks%20in%20the%20latest%20ASVspoof5%20dataset%20--%0Aa%20de%20facto%20standard%20in%20the%20field%20of%20voice%20authenticity%20and%20deepfake%20detection%0A--%20can%20be%20identified%20with%20surprising%20accuracy%20using%20a%20small%20subset%20of%20very%0Asimplistic%20features.%20These%20are%20derived%20from%20the%20openSMILE%20library%2C%20and%20are%0Ascalar-valued%2C%20easy%20to%20compute%2C%20and%20human%20interpretable.%20For%20example%2C%20attack%0AA10%60s%20unvoiced%20segments%20have%20a%20mean%20length%20of%200.09%20%2B-%200.02%2C%20while%20bona%20fide%0Ainstances%20have%20a%20mean%20length%20of%200.18%20%2B-%200.07.%20Using%20this%20feature%20alone%2C%20a%0Athreshold%20classifier%20achieves%20an%20Equal%20Error%20Rate%20%28EER%29%20of%2010.3%25%20for%20attack%0AA10.%20Similarly%2C%20across%20all%20attacks%2C%20we%20achieve%20up%20to%200.8%25%20EER%2C%20with%20an%20overall%0AEER%20of%2015.7%20%2B-%206.0%25.%20We%20explore%20the%20generalization%20capabilities%20of%20these%0Afeatures%20and%20find%20that%20some%20of%20them%20transfer%20effectively%20between%20attacks%2C%0Aprimarily%20when%20the%20attacks%20originate%20from%20similar%20Text-to-Speech%20%28TTS%29%0Aarchitectures.%20This%20finding%20may%20indicate%20that%20voice%20anti-spoofing%20is%2C%20in%20part%2C%0Aa%20problem%20of%20identifying%20and%20remembering%20signatures%20or%20fingerprints%20of%0Aindividual%20TTS%20systems.%20This%20allows%20to%20better%20understand%20anti-spoofing%20models%0Aand%20their%20challenges%20in%20real-world%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasy%252C%2520Interpretable%252C%2520Effective%253A%2520openSMILE%2520for%2520voice%2520deepfake%2520detection%26entry.906535625%3DOctavian%2520Pascu%2520and%2520Dan%2520Oneata%2520and%2520Horia%2520Cucu%2520and%2520Nicolas%2520M.%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520attacks%2520in%2520the%2520latest%2520ASVspoof5%2520dataset%2520--%250Aa%2520de%2520facto%2520standard%2520in%2520the%2520field%2520of%2520voice%2520authenticity%2520and%2520deepfake%2520detection%250A--%2520can%2520be%2520identified%2520with%2520surprising%2520accuracy%2520using%2520a%2520small%2520subset%2520of%2520very%250Asimplistic%2520features.%2520These%2520are%2520derived%2520from%2520the%2520openSMILE%2520library%252C%2520and%2520are%250Ascalar-valued%252C%2520easy%2520to%2520compute%252C%2520and%2520human%2520interpretable.%2520For%2520example%252C%2520attack%250AA10%2560s%2520unvoiced%2520segments%2520have%2520a%2520mean%2520length%2520of%25200.09%2520%252B-%25200.02%252C%2520while%2520bona%2520fide%250Ainstances%2520have%2520a%2520mean%2520length%2520of%25200.18%2520%252B-%25200.07.%2520Using%2520this%2520feature%2520alone%252C%2520a%250Athreshold%2520classifier%2520achieves%2520an%2520Equal%2520Error%2520Rate%2520%2528EER%2529%2520of%252010.3%2525%2520for%2520attack%250AA10.%2520Similarly%252C%2520across%2520all%2520attacks%252C%2520we%2520achieve%2520up%2520to%25200.8%2525%2520EER%252C%2520with%2520an%2520overall%250AEER%2520of%252015.7%2520%252B-%25206.0%2525.%2520We%2520explore%2520the%2520generalization%2520capabilities%2520of%2520these%250Afeatures%2520and%2520find%2520that%2520some%2520of%2520them%2520transfer%2520effectively%2520between%2520attacks%252C%250Aprimarily%2520when%2520the%2520attacks%2520originate%2520from%2520similar%2520Text-to-Speech%2520%2528TTS%2529%250Aarchitectures.%2520This%2520finding%2520may%2520indicate%2520that%2520voice%2520anti-spoofing%2520is%252C%2520in%2520part%252C%250Aa%2520problem%2520of%2520identifying%2520and%2520remembering%2520signatures%2520or%2520fingerprints%2520of%250Aindividual%2520TTS%2520systems.%2520This%2520allows%2520to%2520better%2520understand%2520anti-spoofing%2520models%250Aand%2520their%2520challenges%2520in%2520real-world%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Easy%2C%20Interpretable%2C%20Effective%3A%20openSMILE%20for%20voice%20deepfake%20detection&entry.906535625=Octavian%20Pascu%20and%20Dan%20Oneata%20and%20Horia%20Cucu%20and%20Nicolas%20M.%20M%C3%BCller&entry.1292438233=%20%20In%20this%20paper%2C%20we%20demonstrate%20that%20attacks%20in%20the%20latest%20ASVspoof5%20dataset%20--%0Aa%20de%20facto%20standard%20in%20the%20field%20of%20voice%20authenticity%20and%20deepfake%20detection%0A--%20can%20be%20identified%20with%20surprising%20accuracy%20using%20a%20small%20subset%20of%20very%0Asimplistic%20features.%20These%20are%20derived%20from%20the%20openSMILE%20library%2C%20and%20are%0Ascalar-valued%2C%20easy%20to%20compute%2C%20and%20human%20interpretable.%20For%20example%2C%20attack%0AA10%60s%20unvoiced%20segments%20have%20a%20mean%20length%20of%200.09%20%2B-%200.02%2C%20while%20bona%20fide%0Ainstances%20have%20a%20mean%20length%20of%200.18%20%2B-%200.07.%20Using%20this%20feature%20alone%2C%20a%0Athreshold%20classifier%20achieves%20an%20Equal%20Error%20Rate%20%28EER%29%20of%2010.3%25%20for%20attack%0AA10.%20Similarly%2C%20across%20all%20attacks%2C%20we%20achieve%20up%20to%200.8%25%20EER%2C%20with%20an%20overall%0AEER%20of%2015.7%20%2B-%206.0%25.%20We%20explore%20the%20generalization%20capabilities%20of%20these%0Afeatures%20and%20find%20that%20some%20of%20them%20transfer%20effectively%20between%20attacks%2C%0Aprimarily%20when%20the%20attacks%20originate%20from%20similar%20Text-to-Speech%20%28TTS%29%0Aarchitectures.%20This%20finding%20may%20indicate%20that%20voice%20anti-spoofing%20is%2C%20in%20part%2C%0Aa%20problem%20of%20identifying%20and%20remembering%20signatures%20or%20fingerprints%20of%0Aindividual%20TTS%20systems.%20This%20allows%20to%20better%20understand%20anti-spoofing%20models%0Aand%20their%20challenges%20in%20real-world%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15775v2&entry.124074799=Read"},
{"title": "Towards Modality-agnostic Label-efficient Segmentation with\n  Entropy-Regularized Distribution Alignment", "author": "Liyao Tang and Zhe Chen and Shanshan Zhao and Chaoyue Wang and Dacheng Tao", "abstract": "  Label-efficient segmentation aims to perform effective segmentation on input\ndata using only sparse and limited ground-truth labels for training. This topic\nis widely studied in 3D point cloud segmentation due to the difficulty of\nannotating point clouds densely, while it is also essential for cost-effective\nsegmentation on 2D images. Until recently, pseudo-labels have been widely\nemployed to facilitate training with limited ground-truth labels, and promising\nprogress has been witnessed in both the 2D and 3D segmentation. However,\nexisting pseudo-labeling approaches could suffer heavily from the noises and\nvariations in unlabelled data, which would result in significant discrepancies\nbetween generated pseudo-labels and current model predictions during training.\nWe analyze that this can further confuse and affect the model learning process,\nwhich shows to be a shared problem in label-efficient learning across both 2D\nand 3D modalities. To address this issue, we propose a novel learning strategy\nto regularize the pseudo-labels generated for training, thus effectively\nnarrowing the gaps between pseudo-labels and model predictions. More\nspecifically, our method introduces an Entropy Regularization loss and a\nDistribution Alignment loss for label-efficient learning, resulting in an ERDA\nlearning strategy. Interestingly, by using KL distance to formulate the\ndistribution alignment loss, ERDA reduces to a deceptively simple\ncross-entropy-based loss which optimizes both the pseudo-label generation\nmodule and the segmentation model simultaneously. In addition, we innovate in\nthe pseudo-label generation to make our ERDA consistently effective across both\n2D and 3D data modalities for segmentation. Enjoying simplicity and more\nmodality-agnostic pseudo-label generation, our method has shown outstanding\nperformance in fully utilizing all unlabeled data points for training across\n...\n", "link": "http://arxiv.org/abs/2408.16520v1", "date": "2024-08-29", "relevancy": 2.2452, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6233}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5561}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Modality-agnostic%20Label-efficient%20Segmentation%20with%0A%20%20Entropy-Regularized%20Distribution%20Alignment&body=Title%3A%20Towards%20Modality-agnostic%20Label-efficient%20Segmentation%20with%0A%20%20Entropy-Regularized%20Distribution%20Alignment%0AAuthor%3A%20Liyao%20Tang%20and%20Zhe%20Chen%20and%20Shanshan%20Zhao%20and%20Chaoyue%20Wang%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Label-efficient%20segmentation%20aims%20to%20perform%20effective%20segmentation%20on%20input%0Adata%20using%20only%20sparse%20and%20limited%20ground-truth%20labels%20for%20training.%20This%20topic%0Ais%20widely%20studied%20in%203D%20point%20cloud%20segmentation%20due%20to%20the%20difficulty%20of%0Aannotating%20point%20clouds%20densely%2C%20while%20it%20is%20also%20essential%20for%20cost-effective%0Asegmentation%20on%202D%20images.%20Until%20recently%2C%20pseudo-labels%20have%20been%20widely%0Aemployed%20to%20facilitate%20training%20with%20limited%20ground-truth%20labels%2C%20and%20promising%0Aprogress%20has%20been%20witnessed%20in%20both%20the%202D%20and%203D%20segmentation.%20However%2C%0Aexisting%20pseudo-labeling%20approaches%20could%20suffer%20heavily%20from%20the%20noises%20and%0Avariations%20in%20unlabelled%20data%2C%20which%20would%20result%20in%20significant%20discrepancies%0Abetween%20generated%20pseudo-labels%20and%20current%20model%20predictions%20during%20training.%0AWe%20analyze%20that%20this%20can%20further%20confuse%20and%20affect%20the%20model%20learning%20process%2C%0Awhich%20shows%20to%20be%20a%20shared%20problem%20in%20label-efficient%20learning%20across%20both%202D%0Aand%203D%20modalities.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20learning%20strategy%0Ato%20regularize%20the%20pseudo-labels%20generated%20for%20training%2C%20thus%20effectively%0Anarrowing%20the%20gaps%20between%20pseudo-labels%20and%20model%20predictions.%20More%0Aspecifically%2C%20our%20method%20introduces%20an%20Entropy%20Regularization%20loss%20and%20a%0ADistribution%20Alignment%20loss%20for%20label-efficient%20learning%2C%20resulting%20in%20an%20ERDA%0Alearning%20strategy.%20Interestingly%2C%20by%20using%20KL%20distance%20to%20formulate%20the%0Adistribution%20alignment%20loss%2C%20ERDA%20reduces%20to%20a%20deceptively%20simple%0Across-entropy-based%20loss%20which%20optimizes%20both%20the%20pseudo-label%20generation%0Amodule%20and%20the%20segmentation%20model%20simultaneously.%20In%20addition%2C%20we%20innovate%20in%0Athe%20pseudo-label%20generation%20to%20make%20our%20ERDA%20consistently%20effective%20across%20both%0A2D%20and%203D%20data%20modalities%20for%20segmentation.%20Enjoying%20simplicity%20and%20more%0Amodality-agnostic%20pseudo-label%20generation%2C%20our%20method%20has%20shown%20outstanding%0Aperformance%20in%20fully%20utilizing%20all%20unlabeled%20data%20points%20for%20training%20across%0A...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Modality-agnostic%2520Label-efficient%2520Segmentation%2520with%250A%2520%2520Entropy-Regularized%2520Distribution%2520Alignment%26entry.906535625%3DLiyao%2520Tang%2520and%2520Zhe%2520Chen%2520and%2520Shanshan%2520Zhao%2520and%2520Chaoyue%2520Wang%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Label-efficient%2520segmentation%2520aims%2520to%2520perform%2520effective%2520segmentation%2520on%2520input%250Adata%2520using%2520only%2520sparse%2520and%2520limited%2520ground-truth%2520labels%2520for%2520training.%2520This%2520topic%250Ais%2520widely%2520studied%2520in%25203D%2520point%2520cloud%2520segmentation%2520due%2520to%2520the%2520difficulty%2520of%250Aannotating%2520point%2520clouds%2520densely%252C%2520while%2520it%2520is%2520also%2520essential%2520for%2520cost-effective%250Asegmentation%2520on%25202D%2520images.%2520Until%2520recently%252C%2520pseudo-labels%2520have%2520been%2520widely%250Aemployed%2520to%2520facilitate%2520training%2520with%2520limited%2520ground-truth%2520labels%252C%2520and%2520promising%250Aprogress%2520has%2520been%2520witnessed%2520in%2520both%2520the%25202D%2520and%25203D%2520segmentation.%2520However%252C%250Aexisting%2520pseudo-labeling%2520approaches%2520could%2520suffer%2520heavily%2520from%2520the%2520noises%2520and%250Avariations%2520in%2520unlabelled%2520data%252C%2520which%2520would%2520result%2520in%2520significant%2520discrepancies%250Abetween%2520generated%2520pseudo-labels%2520and%2520current%2520model%2520predictions%2520during%2520training.%250AWe%2520analyze%2520that%2520this%2520can%2520further%2520confuse%2520and%2520affect%2520the%2520model%2520learning%2520process%252C%250Awhich%2520shows%2520to%2520be%2520a%2520shared%2520problem%2520in%2520label-efficient%2520learning%2520across%2520both%25202D%250Aand%25203D%2520modalities.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520learning%2520strategy%250Ato%2520regularize%2520the%2520pseudo-labels%2520generated%2520for%2520training%252C%2520thus%2520effectively%250Anarrowing%2520the%2520gaps%2520between%2520pseudo-labels%2520and%2520model%2520predictions.%2520More%250Aspecifically%252C%2520our%2520method%2520introduces%2520an%2520Entropy%2520Regularization%2520loss%2520and%2520a%250ADistribution%2520Alignment%2520loss%2520for%2520label-efficient%2520learning%252C%2520resulting%2520in%2520an%2520ERDA%250Alearning%2520strategy.%2520Interestingly%252C%2520by%2520using%2520KL%2520distance%2520to%2520formulate%2520the%250Adistribution%2520alignment%2520loss%252C%2520ERDA%2520reduces%2520to%2520a%2520deceptively%2520simple%250Across-entropy-based%2520loss%2520which%2520optimizes%2520both%2520the%2520pseudo-label%2520generation%250Amodule%2520and%2520the%2520segmentation%2520model%2520simultaneously.%2520In%2520addition%252C%2520we%2520innovate%2520in%250Athe%2520pseudo-label%2520generation%2520to%2520make%2520our%2520ERDA%2520consistently%2520effective%2520across%2520both%250A2D%2520and%25203D%2520data%2520modalities%2520for%2520segmentation.%2520Enjoying%2520simplicity%2520and%2520more%250Amodality-agnostic%2520pseudo-label%2520generation%252C%2520our%2520method%2520has%2520shown%2520outstanding%250Aperformance%2520in%2520fully%2520utilizing%2520all%2520unlabeled%2520data%2520points%2520for%2520training%2520across%250A...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Modality-agnostic%20Label-efficient%20Segmentation%20with%0A%20%20Entropy-Regularized%20Distribution%20Alignment&entry.906535625=Liyao%20Tang%20and%20Zhe%20Chen%20and%20Shanshan%20Zhao%20and%20Chaoyue%20Wang%20and%20Dacheng%20Tao&entry.1292438233=%20%20Label-efficient%20segmentation%20aims%20to%20perform%20effective%20segmentation%20on%20input%0Adata%20using%20only%20sparse%20and%20limited%20ground-truth%20labels%20for%20training.%20This%20topic%0Ais%20widely%20studied%20in%203D%20point%20cloud%20segmentation%20due%20to%20the%20difficulty%20of%0Aannotating%20point%20clouds%20densely%2C%20while%20it%20is%20also%20essential%20for%20cost-effective%0Asegmentation%20on%202D%20images.%20Until%20recently%2C%20pseudo-labels%20have%20been%20widely%0Aemployed%20to%20facilitate%20training%20with%20limited%20ground-truth%20labels%2C%20and%20promising%0Aprogress%20has%20been%20witnessed%20in%20both%20the%202D%20and%203D%20segmentation.%20However%2C%0Aexisting%20pseudo-labeling%20approaches%20could%20suffer%20heavily%20from%20the%20noises%20and%0Avariations%20in%20unlabelled%20data%2C%20which%20would%20result%20in%20significant%20discrepancies%0Abetween%20generated%20pseudo-labels%20and%20current%20model%20predictions%20during%20training.%0AWe%20analyze%20that%20this%20can%20further%20confuse%20and%20affect%20the%20model%20learning%20process%2C%0Awhich%20shows%20to%20be%20a%20shared%20problem%20in%20label-efficient%20learning%20across%20both%202D%0Aand%203D%20modalities.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20learning%20strategy%0Ato%20regularize%20the%20pseudo-labels%20generated%20for%20training%2C%20thus%20effectively%0Anarrowing%20the%20gaps%20between%20pseudo-labels%20and%20model%20predictions.%20More%0Aspecifically%2C%20our%20method%20introduces%20an%20Entropy%20Regularization%20loss%20and%20a%0ADistribution%20Alignment%20loss%20for%20label-efficient%20learning%2C%20resulting%20in%20an%20ERDA%0Alearning%20strategy.%20Interestingly%2C%20by%20using%20KL%20distance%20to%20formulate%20the%0Adistribution%20alignment%20loss%2C%20ERDA%20reduces%20to%20a%20deceptively%20simple%0Across-entropy-based%20loss%20which%20optimizes%20both%20the%20pseudo-label%20generation%0Amodule%20and%20the%20segmentation%20model%20simultaneously.%20In%20addition%2C%20we%20innovate%20in%0Athe%20pseudo-label%20generation%20to%20make%20our%20ERDA%20consistently%20effective%20across%20both%0A2D%20and%203D%20data%20modalities%20for%20segmentation.%20Enjoying%20simplicity%20and%20more%0Amodality-agnostic%20pseudo-label%20generation%2C%20our%20method%20has%20shown%20outstanding%0Aperformance%20in%20fully%20utilizing%20all%20unlabeled%20data%20points%20for%20training%20across%0A...%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16520v1&entry.124074799=Read"},
{"title": "UAV-Based Human Body Detector Selection and Fusion for Geolocated\n  Saliency Map Generation", "author": "Piotr Rudol and Patrick Doherty and Mariusz Wzorek and Chattrakul Sombattheera", "abstract": "  The problem of reliably detecting and geolocating objects of different\nclasses in soft real-time is essential in many application areas, such as\nSearch and Rescue performed using Unmanned Aerial Vehicles (UAVs). This\nresearch addresses the complementary problems of system contextual vision-based\ndetector selection, allocation, and execution, in addition to the fusion of\ndetection results from teams of UAVs for the purpose of accurately and reliably\ngeolocating objects of interest in a timely manner. In an offline step, an\napplication-independent evaluation of vision-based detectors from a system\nperspective is first performed. Based on this evaluation, the most appropriate\nalgorithms for online object detection for each platform are selected\nautomatically before a mission, taking into account a number of practical\nsystem considerations, such as the available communication links, video\ncompression used, and the available computational resources. The detection\nresults are fused using a method for building maps of salient locations which\ntakes advantage of a novel sensor model for vision-based detections for both\npositive and negative observations. A number of simulated and real flight\nexperiments are also presented, validating the proposed method.\n", "link": "http://arxiv.org/abs/2408.16501v1", "date": "2024-08-29", "relevancy": 2.2298, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6184}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAV-Based%20Human%20Body%20Detector%20Selection%20and%20Fusion%20for%20Geolocated%0A%20%20Saliency%20Map%20Generation&body=Title%3A%20UAV-Based%20Human%20Body%20Detector%20Selection%20and%20Fusion%20for%20Geolocated%0A%20%20Saliency%20Map%20Generation%0AAuthor%3A%20Piotr%20Rudol%20and%20Patrick%20Doherty%20and%20Mariusz%20Wzorek%20and%20Chattrakul%20Sombattheera%0AAbstract%3A%20%20%20The%20problem%20of%20reliably%20detecting%20and%20geolocating%20objects%20of%20different%0Aclasses%20in%20soft%20real-time%20is%20essential%20in%20many%20application%20areas%2C%20such%20as%0ASearch%20and%20Rescue%20performed%20using%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29.%20This%0Aresearch%20addresses%20the%20complementary%20problems%20of%20system%20contextual%20vision-based%0Adetector%20selection%2C%20allocation%2C%20and%20execution%2C%20in%20addition%20to%20the%20fusion%20of%0Adetection%20results%20from%20teams%20of%20UAVs%20for%20the%20purpose%20of%20accurately%20and%20reliably%0Ageolocating%20objects%20of%20interest%20in%20a%20timely%20manner.%20In%20an%20offline%20step%2C%20an%0Aapplication-independent%20evaluation%20of%20vision-based%20detectors%20from%20a%20system%0Aperspective%20is%20first%20performed.%20Based%20on%20this%20evaluation%2C%20the%20most%20appropriate%0Aalgorithms%20for%20online%20object%20detection%20for%20each%20platform%20are%20selected%0Aautomatically%20before%20a%20mission%2C%20taking%20into%20account%20a%20number%20of%20practical%0Asystem%20considerations%2C%20such%20as%20the%20available%20communication%20links%2C%20video%0Acompression%20used%2C%20and%20the%20available%20computational%20resources.%20The%20detection%0Aresults%20are%20fused%20using%20a%20method%20for%20building%20maps%20of%20salient%20locations%20which%0Atakes%20advantage%20of%20a%20novel%20sensor%20model%20for%20vision-based%20detections%20for%20both%0Apositive%20and%20negative%20observations.%20A%20number%20of%20simulated%20and%20real%20flight%0Aexperiments%20are%20also%20presented%2C%20validating%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAV-Based%2520Human%2520Body%2520Detector%2520Selection%2520and%2520Fusion%2520for%2520Geolocated%250A%2520%2520Saliency%2520Map%2520Generation%26entry.906535625%3DPiotr%2520Rudol%2520and%2520Patrick%2520Doherty%2520and%2520Mariusz%2520Wzorek%2520and%2520Chattrakul%2520Sombattheera%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520reliably%2520detecting%2520and%2520geolocating%2520objects%2520of%2520different%250Aclasses%2520in%2520soft%2520real-time%2520is%2520essential%2520in%2520many%2520application%2520areas%252C%2520such%2520as%250ASearch%2520and%2520Rescue%2520performed%2520using%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529.%2520This%250Aresearch%2520addresses%2520the%2520complementary%2520problems%2520of%2520system%2520contextual%2520vision-based%250Adetector%2520selection%252C%2520allocation%252C%2520and%2520execution%252C%2520in%2520addition%2520to%2520the%2520fusion%2520of%250Adetection%2520results%2520from%2520teams%2520of%2520UAVs%2520for%2520the%2520purpose%2520of%2520accurately%2520and%2520reliably%250Ageolocating%2520objects%2520of%2520interest%2520in%2520a%2520timely%2520manner.%2520In%2520an%2520offline%2520step%252C%2520an%250Aapplication-independent%2520evaluation%2520of%2520vision-based%2520detectors%2520from%2520a%2520system%250Aperspective%2520is%2520first%2520performed.%2520Based%2520on%2520this%2520evaluation%252C%2520the%2520most%2520appropriate%250Aalgorithms%2520for%2520online%2520object%2520detection%2520for%2520each%2520platform%2520are%2520selected%250Aautomatically%2520before%2520a%2520mission%252C%2520taking%2520into%2520account%2520a%2520number%2520of%2520practical%250Asystem%2520considerations%252C%2520such%2520as%2520the%2520available%2520communication%2520links%252C%2520video%250Acompression%2520used%252C%2520and%2520the%2520available%2520computational%2520resources.%2520The%2520detection%250Aresults%2520are%2520fused%2520using%2520a%2520method%2520for%2520building%2520maps%2520of%2520salient%2520locations%2520which%250Atakes%2520advantage%2520of%2520a%2520novel%2520sensor%2520model%2520for%2520vision-based%2520detections%2520for%2520both%250Apositive%2520and%2520negative%2520observations.%2520A%2520number%2520of%2520simulated%2520and%2520real%2520flight%250Aexperiments%2520are%2520also%2520presented%252C%2520validating%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAV-Based%20Human%20Body%20Detector%20Selection%20and%20Fusion%20for%20Geolocated%0A%20%20Saliency%20Map%20Generation&entry.906535625=Piotr%20Rudol%20and%20Patrick%20Doherty%20and%20Mariusz%20Wzorek%20and%20Chattrakul%20Sombattheera&entry.1292438233=%20%20The%20problem%20of%20reliably%20detecting%20and%20geolocating%20objects%20of%20different%0Aclasses%20in%20soft%20real-time%20is%20essential%20in%20many%20application%20areas%2C%20such%20as%0ASearch%20and%20Rescue%20performed%20using%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29.%20This%0Aresearch%20addresses%20the%20complementary%20problems%20of%20system%20contextual%20vision-based%0Adetector%20selection%2C%20allocation%2C%20and%20execution%2C%20in%20addition%20to%20the%20fusion%20of%0Adetection%20results%20from%20teams%20of%20UAVs%20for%20the%20purpose%20of%20accurately%20and%20reliably%0Ageolocating%20objects%20of%20interest%20in%20a%20timely%20manner.%20In%20an%20offline%20step%2C%20an%0Aapplication-independent%20evaluation%20of%20vision-based%20detectors%20from%20a%20system%0Aperspective%20is%20first%20performed.%20Based%20on%20this%20evaluation%2C%20the%20most%20appropriate%0Aalgorithms%20for%20online%20object%20detection%20for%20each%20platform%20are%20selected%0Aautomatically%20before%20a%20mission%2C%20taking%20into%20account%20a%20number%20of%20practical%0Asystem%20considerations%2C%20such%20as%20the%20available%20communication%20links%2C%20video%0Acompression%20used%2C%20and%20the%20available%20computational%20resources.%20The%20detection%0Aresults%20are%20fused%20using%20a%20method%20for%20building%20maps%20of%20salient%20locations%20which%0Atakes%20advantage%20of%20a%20novel%20sensor%20model%20for%20vision-based%20detections%20for%20both%0Apositive%20and%20negative%20observations.%20A%20number%20of%20simulated%20and%20real%20flight%0Aexperiments%20are%20also%20presented%2C%20validating%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16501v1&entry.124074799=Read"},
{"title": "CogVLM2: Visual Language Models for Image and Video Understanding", "author": "Wenyi Hong and Weihan Wang and Ming Ding and Wenmeng Yu and Qingsong Lv and Yan Wang and Yean Cheng and Shiyu Huang and Junhui Ji and Zhao Xue and Lei Zhao and Zhuoyi Yang and Xiaotao Gu and Xiaohan Zhang and Guanyu Feng and Da Yin and Zihan Wang and Ji Qi and Xixuan Song and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Yuxiao Dong and Jie Tang", "abstract": "  Beginning with VisualGLM and CogVLM, we are continuously exploring VLMs in\npursuit of enhanced vision-language fusion, efficient higher-resolution\narchitecture, and broader modalities and applications. Here we propose the\nCogVLM2 family, a new generation of visual language models for image and video\nunderstanding including CogVLM2, CogVLM2-Video and GLM-4V. As an image\nunderstanding model, CogVLM2 inherits the visual expert architecture with\nimproved training recipes in both pre-training and post-training stages,\nsupporting input resolution up to $1344 \\times 1344$ pixels. As a video\nunderstanding model, CogVLM2-Video integrates multi-frame input with timestamps\nand proposes automated temporal grounding data construction. Notably, CogVLM2\nfamily has achieved state-of-the-art results on benchmarks like MMBench,\nMM-Vet, TextVQA, MVBench and VCGBench. All models are open-sourced in\nhttps://github.com/THUDM/CogVLM2 and https://github.com/THUDM/GLM-4,\ncontributing to the advancement of the field.\n", "link": "http://arxiv.org/abs/2408.16500v1", "date": "2024-08-29", "relevancy": 2.2275, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.575}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5501}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogVLM2%3A%20Visual%20Language%20Models%20for%20Image%20and%20Video%20Understanding&body=Title%3A%20CogVLM2%3A%20Visual%20Language%20Models%20for%20Image%20and%20Video%20Understanding%0AAuthor%3A%20Wenyi%20Hong%20and%20Weihan%20Wang%20and%20Ming%20Ding%20and%20Wenmeng%20Yu%20and%20Qingsong%20Lv%20and%20Yan%20Wang%20and%20Yean%20Cheng%20and%20Shiyu%20Huang%20and%20Junhui%20Ji%20and%20Zhao%20Xue%20and%20Lei%20Zhao%20and%20Zhuoyi%20Yang%20and%20Xiaotao%20Gu%20and%20Xiaohan%20Zhang%20and%20Guanyu%20Feng%20and%20Da%20Yin%20and%20Zihan%20Wang%20and%20Ji%20Qi%20and%20Xixuan%20Song%20and%20Peng%20Zhang%20and%20Debing%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%0AAbstract%3A%20%20%20Beginning%20with%20VisualGLM%20and%20CogVLM%2C%20we%20are%20continuously%20exploring%20VLMs%20in%0Apursuit%20of%20enhanced%20vision-language%20fusion%2C%20efficient%20higher-resolution%0Aarchitecture%2C%20and%20broader%20modalities%20and%20applications.%20Here%20we%20propose%20the%0ACogVLM2%20family%2C%20a%20new%20generation%20of%20visual%20language%20models%20for%20image%20and%20video%0Aunderstanding%20including%20CogVLM2%2C%20CogVLM2-Video%20and%20GLM-4V.%20As%20an%20image%0Aunderstanding%20model%2C%20CogVLM2%20inherits%20the%20visual%20expert%20architecture%20with%0Aimproved%20training%20recipes%20in%20both%20pre-training%20and%20post-training%20stages%2C%0Asupporting%20input%20resolution%20up%20to%20%241344%20%5Ctimes%201344%24%20pixels.%20As%20a%20video%0Aunderstanding%20model%2C%20CogVLM2-Video%20integrates%20multi-frame%20input%20with%20timestamps%0Aand%20proposes%20automated%20temporal%20grounding%20data%20construction.%20Notably%2C%20CogVLM2%0Afamily%20has%20achieved%20state-of-the-art%20results%20on%20benchmarks%20like%20MMBench%2C%0AMM-Vet%2C%20TextVQA%2C%20MVBench%20and%20VCGBench.%20All%20models%20are%20open-sourced%20in%0Ahttps%3A//github.com/THUDM/CogVLM2%20and%20https%3A//github.com/THUDM/GLM-4%2C%0Acontributing%20to%20the%20advancement%20of%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogVLM2%253A%2520Visual%2520Language%2520Models%2520for%2520Image%2520and%2520Video%2520Understanding%26entry.906535625%3DWenyi%2520Hong%2520and%2520Weihan%2520Wang%2520and%2520Ming%2520Ding%2520and%2520Wenmeng%2520Yu%2520and%2520Qingsong%2520Lv%2520and%2520Yan%2520Wang%2520and%2520Yean%2520Cheng%2520and%2520Shiyu%2520Huang%2520and%2520Junhui%2520Ji%2520and%2520Zhao%2520Xue%2520and%2520Lei%2520Zhao%2520and%2520Zhuoyi%2520Yang%2520and%2520Xiaotao%2520Gu%2520and%2520Xiaohan%2520Zhang%2520and%2520Guanyu%2520Feng%2520and%2520Da%2520Yin%2520and%2520Zihan%2520Wang%2520and%2520Ji%2520Qi%2520and%2520Xixuan%2520Song%2520and%2520Peng%2520Zhang%2520and%2520Debing%2520Liu%2520and%2520Bin%2520Xu%2520and%2520Juanzi%2520Li%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520Beginning%2520with%2520VisualGLM%2520and%2520CogVLM%252C%2520we%2520are%2520continuously%2520exploring%2520VLMs%2520in%250Apursuit%2520of%2520enhanced%2520vision-language%2520fusion%252C%2520efficient%2520higher-resolution%250Aarchitecture%252C%2520and%2520broader%2520modalities%2520and%2520applications.%2520Here%2520we%2520propose%2520the%250ACogVLM2%2520family%252C%2520a%2520new%2520generation%2520of%2520visual%2520language%2520models%2520for%2520image%2520and%2520video%250Aunderstanding%2520including%2520CogVLM2%252C%2520CogVLM2-Video%2520and%2520GLM-4V.%2520As%2520an%2520image%250Aunderstanding%2520model%252C%2520CogVLM2%2520inherits%2520the%2520visual%2520expert%2520architecture%2520with%250Aimproved%2520training%2520recipes%2520in%2520both%2520pre-training%2520and%2520post-training%2520stages%252C%250Asupporting%2520input%2520resolution%2520up%2520to%2520%25241344%2520%255Ctimes%25201344%2524%2520pixels.%2520As%2520a%2520video%250Aunderstanding%2520model%252C%2520CogVLM2-Video%2520integrates%2520multi-frame%2520input%2520with%2520timestamps%250Aand%2520proposes%2520automated%2520temporal%2520grounding%2520data%2520construction.%2520Notably%252C%2520CogVLM2%250Afamily%2520has%2520achieved%2520state-of-the-art%2520results%2520on%2520benchmarks%2520like%2520MMBench%252C%250AMM-Vet%252C%2520TextVQA%252C%2520MVBench%2520and%2520VCGBench.%2520All%2520models%2520are%2520open-sourced%2520in%250Ahttps%253A//github.com/THUDM/CogVLM2%2520and%2520https%253A//github.com/THUDM/GLM-4%252C%250Acontributing%2520to%2520the%2520advancement%2520of%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogVLM2%3A%20Visual%20Language%20Models%20for%20Image%20and%20Video%20Understanding&entry.906535625=Wenyi%20Hong%20and%20Weihan%20Wang%20and%20Ming%20Ding%20and%20Wenmeng%20Yu%20and%20Qingsong%20Lv%20and%20Yan%20Wang%20and%20Yean%20Cheng%20and%20Shiyu%20Huang%20and%20Junhui%20Ji%20and%20Zhao%20Xue%20and%20Lei%20Zhao%20and%20Zhuoyi%20Yang%20and%20Xiaotao%20Gu%20and%20Xiaohan%20Zhang%20and%20Guanyu%20Feng%20and%20Da%20Yin%20and%20Zihan%20Wang%20and%20Ji%20Qi%20and%20Xixuan%20Song%20and%20Peng%20Zhang%20and%20Debing%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Yuxiao%20Dong%20and%20Jie%20Tang&entry.1292438233=%20%20Beginning%20with%20VisualGLM%20and%20CogVLM%2C%20we%20are%20continuously%20exploring%20VLMs%20in%0Apursuit%20of%20enhanced%20vision-language%20fusion%2C%20efficient%20higher-resolution%0Aarchitecture%2C%20and%20broader%20modalities%20and%20applications.%20Here%20we%20propose%20the%0ACogVLM2%20family%2C%20a%20new%20generation%20of%20visual%20language%20models%20for%20image%20and%20video%0Aunderstanding%20including%20CogVLM2%2C%20CogVLM2-Video%20and%20GLM-4V.%20As%20an%20image%0Aunderstanding%20model%2C%20CogVLM2%20inherits%20the%20visual%20expert%20architecture%20with%0Aimproved%20training%20recipes%20in%20both%20pre-training%20and%20post-training%20stages%2C%0Asupporting%20input%20resolution%20up%20to%20%241344%20%5Ctimes%201344%24%20pixels.%20As%20a%20video%0Aunderstanding%20model%2C%20CogVLM2-Video%20integrates%20multi-frame%20input%20with%20timestamps%0Aand%20proposes%20automated%20temporal%20grounding%20data%20construction.%20Notably%2C%20CogVLM2%0Afamily%20has%20achieved%20state-of-the-art%20results%20on%20benchmarks%20like%20MMBench%2C%0AMM-Vet%2C%20TextVQA%2C%20MVBench%20and%20VCGBench.%20All%20models%20are%20open-sourced%20in%0Ahttps%3A//github.com/THUDM/CogVLM2%20and%20https%3A//github.com/THUDM/GLM-4%2C%0Acontributing%20to%20the%20advancement%20of%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16500v1&entry.124074799=Read"},
{"title": "Text-Enhanced Zero-Shot Action Recognition: A training-free approach", "author": "Massimo Bosetti and Shibingfeng Zhang and Bendetta Liberatori and Giacomo Zara and Elisa Ricci and Paolo Rota", "abstract": "  Vision-language models (VLMs) have demonstrated remarkable performance across\nvarious visual tasks, leveraging joint learning of visual and textual\nrepresentations. While these models excel in zero-shot image tasks, their\napplication to zero-shot video action recognition (ZSVAR) remains challenging\ndue to the dynamic and temporal nature of actions. Existing methods for ZS-VAR\ntypically require extensive training on specific datasets, which can be\nresource-intensive and may introduce domain biases. In this work, we propose\nText-Enhanced Action Recognition (TEAR), a simple approach to ZS-VAR that is\ntraining-free and does not require the availability of training data or\nextensive computational resources. Drawing inspiration from recent findings in\nvision and language literature, we utilize action descriptors for decomposition\nand contextual information to enhance zero-shot action recognition. Through\nexperiments on UCF101, HMDB51, and Kinetics-600 datasets, we showcase the\neffectiveness and applicability of our proposed approach in addressing the\nchallenges of ZS-VAR.\n", "link": "http://arxiv.org/abs/2408.16412v1", "date": "2024-08-29", "relevancy": 2.2239, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5613}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Enhanced%20Zero-Shot%20Action%20Recognition%3A%20A%20training-free%20approach&body=Title%3A%20Text-Enhanced%20Zero-Shot%20Action%20Recognition%3A%20A%20training-free%20approach%0AAuthor%3A%20Massimo%20Bosetti%20and%20Shibingfeng%20Zhang%20and%20Bendetta%20Liberatori%20and%20Giacomo%20Zara%20and%20Elisa%20Ricci%20and%20Paolo%20Rota%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Avarious%20visual%20tasks%2C%20leveraging%20joint%20learning%20of%20visual%20and%20textual%0Arepresentations.%20While%20these%20models%20excel%20in%20zero-shot%20image%20tasks%2C%20their%0Aapplication%20to%20zero-shot%20video%20action%20recognition%20%28ZSVAR%29%20remains%20challenging%0Adue%20to%20the%20dynamic%20and%20temporal%20nature%20of%20actions.%20Existing%20methods%20for%20ZS-VAR%0Atypically%20require%20extensive%20training%20on%20specific%20datasets%2C%20which%20can%20be%0Aresource-intensive%20and%20may%20introduce%20domain%20biases.%20In%20this%20work%2C%20we%20propose%0AText-Enhanced%20Action%20Recognition%20%28TEAR%29%2C%20a%20simple%20approach%20to%20ZS-VAR%20that%20is%0Atraining-free%20and%20does%20not%20require%20the%20availability%20of%20training%20data%20or%0Aextensive%20computational%20resources.%20Drawing%20inspiration%20from%20recent%20findings%20in%0Avision%20and%20language%20literature%2C%20we%20utilize%20action%20descriptors%20for%20decomposition%0Aand%20contextual%20information%20to%20enhance%20zero-shot%20action%20recognition.%20Through%0Aexperiments%20on%20UCF101%2C%20HMDB51%2C%20and%20Kinetics-600%20datasets%2C%20we%20showcase%20the%0Aeffectiveness%20and%20applicability%20of%20our%20proposed%20approach%20in%20addressing%20the%0Achallenges%20of%20ZS-VAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Enhanced%2520Zero-Shot%2520Action%2520Recognition%253A%2520A%2520training-free%2520approach%26entry.906535625%3DMassimo%2520Bosetti%2520and%2520Shibingfeng%2520Zhang%2520and%2520Bendetta%2520Liberatori%2520and%2520Giacomo%2520Zara%2520and%2520Elisa%2520Ricci%2520and%2520Paolo%2520Rota%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%250Avarious%2520visual%2520tasks%252C%2520leveraging%2520joint%2520learning%2520of%2520visual%2520and%2520textual%250Arepresentations.%2520While%2520these%2520models%2520excel%2520in%2520zero-shot%2520image%2520tasks%252C%2520their%250Aapplication%2520to%2520zero-shot%2520video%2520action%2520recognition%2520%2528ZSVAR%2529%2520remains%2520challenging%250Adue%2520to%2520the%2520dynamic%2520and%2520temporal%2520nature%2520of%2520actions.%2520Existing%2520methods%2520for%2520ZS-VAR%250Atypically%2520require%2520extensive%2520training%2520on%2520specific%2520datasets%252C%2520which%2520can%2520be%250Aresource-intensive%2520and%2520may%2520introduce%2520domain%2520biases.%2520In%2520this%2520work%252C%2520we%2520propose%250AText-Enhanced%2520Action%2520Recognition%2520%2528TEAR%2529%252C%2520a%2520simple%2520approach%2520to%2520ZS-VAR%2520that%2520is%250Atraining-free%2520and%2520does%2520not%2520require%2520the%2520availability%2520of%2520training%2520data%2520or%250Aextensive%2520computational%2520resources.%2520Drawing%2520inspiration%2520from%2520recent%2520findings%2520in%250Avision%2520and%2520language%2520literature%252C%2520we%2520utilize%2520action%2520descriptors%2520for%2520decomposition%250Aand%2520contextual%2520information%2520to%2520enhance%2520zero-shot%2520action%2520recognition.%2520Through%250Aexperiments%2520on%2520UCF101%252C%2520HMDB51%252C%2520and%2520Kinetics-600%2520datasets%252C%2520we%2520showcase%2520the%250Aeffectiveness%2520and%2520applicability%2520of%2520our%2520proposed%2520approach%2520in%2520addressing%2520the%250Achallenges%2520of%2520ZS-VAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Enhanced%20Zero-Shot%20Action%20Recognition%3A%20A%20training-free%20approach&entry.906535625=Massimo%20Bosetti%20and%20Shibingfeng%20Zhang%20and%20Bendetta%20Liberatori%20and%20Giacomo%20Zara%20and%20Elisa%20Ricci%20and%20Paolo%20Rota&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Avarious%20visual%20tasks%2C%20leveraging%20joint%20learning%20of%20visual%20and%20textual%0Arepresentations.%20While%20these%20models%20excel%20in%20zero-shot%20image%20tasks%2C%20their%0Aapplication%20to%20zero-shot%20video%20action%20recognition%20%28ZSVAR%29%20remains%20challenging%0Adue%20to%20the%20dynamic%20and%20temporal%20nature%20of%20actions.%20Existing%20methods%20for%20ZS-VAR%0Atypically%20require%20extensive%20training%20on%20specific%20datasets%2C%20which%20can%20be%0Aresource-intensive%20and%20may%20introduce%20domain%20biases.%20In%20this%20work%2C%20we%20propose%0AText-Enhanced%20Action%20Recognition%20%28TEAR%29%2C%20a%20simple%20approach%20to%20ZS-VAR%20that%20is%0Atraining-free%20and%20does%20not%20require%20the%20availability%20of%20training%20data%20or%0Aextensive%20computational%20resources.%20Drawing%20inspiration%20from%20recent%20findings%20in%0Avision%20and%20language%20literature%2C%20we%20utilize%20action%20descriptors%20for%20decomposition%0Aand%20contextual%20information%20to%20enhance%20zero-shot%20action%20recognition.%20Through%0Aexperiments%20on%20UCF101%2C%20HMDB51%2C%20and%20Kinetics-600%20datasets%2C%20we%20showcase%20the%0Aeffectiveness%20and%20applicability%20of%20our%20proposed%20approach%20in%20addressing%20the%0Achallenges%20of%20ZS-VAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16412v1&entry.124074799=Read"},
{"title": "Optimal Parallelization of Boosting", "author": "Arthur da Cunha and Mikael M\u00f8ller H\u00f8gsgaard and Kasper Green Larsen", "abstract": "  Recent works on the parallel complexity of Boosting have established strong\nlower bounds on the tradeoff between the number of training rounds $p$ and the\ntotal parallel work per round $t$. These works have also presented highly\nnon-trivial parallel algorithms that shed light on different regions of this\ntradeoff. Despite these advancements, a significant gap persists between the\ntheoretical lower bounds and the performance of these algorithms across much of\nthe tradeoff space. In this work, we essentially close this gap by providing\nboth improved lower bounds on the parallel complexity of weak-to-strong\nlearners, and a parallel Boosting algorithm whose performance matches these\nbounds across the entire $p$ vs.~$t$ compromise spectrum, up to logarithmic\nfactors. Ultimately, this work settles the true parallel complexity of Boosting\nalgorithms that are nearly sample-optimal.\n", "link": "http://arxiv.org/abs/2408.16653v1", "date": "2024-08-29", "relevancy": 2.2153, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4788}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4278}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Parallelization%20of%20Boosting&body=Title%3A%20Optimal%20Parallelization%20of%20Boosting%0AAuthor%3A%20Arthur%20da%20Cunha%20and%20Mikael%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Kasper%20Green%20Larsen%0AAbstract%3A%20%20%20Recent%20works%20on%20the%20parallel%20complexity%20of%20Boosting%20have%20established%20strong%0Alower%20bounds%20on%20the%20tradeoff%20between%20the%20number%20of%20training%20rounds%20%24p%24%20and%20the%0Atotal%20parallel%20work%20per%20round%20%24t%24.%20These%20works%20have%20also%20presented%20highly%0Anon-trivial%20parallel%20algorithms%20that%20shed%20light%20on%20different%20regions%20of%20this%0Atradeoff.%20Despite%20these%20advancements%2C%20a%20significant%20gap%20persists%20between%20the%0Atheoretical%20lower%20bounds%20and%20the%20performance%20of%20these%20algorithms%20across%20much%20of%0Athe%20tradeoff%20space.%20In%20this%20work%2C%20we%20essentially%20close%20this%20gap%20by%20providing%0Aboth%20improved%20lower%20bounds%20on%20the%20parallel%20complexity%20of%20weak-to-strong%0Alearners%2C%20and%20a%20parallel%20Boosting%20algorithm%20whose%20performance%20matches%20these%0Abounds%20across%20the%20entire%20%24p%24%20vs.~%24t%24%20compromise%20spectrum%2C%20up%20to%20logarithmic%0Afactors.%20Ultimately%2C%20this%20work%20settles%20the%20true%20parallel%20complexity%20of%20Boosting%0Aalgorithms%20that%20are%20nearly%20sample-optimal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Parallelization%2520of%2520Boosting%26entry.906535625%3DArthur%2520da%2520Cunha%2520and%2520Mikael%2520M%25C3%25B8ller%2520H%25C3%25B8gsgaard%2520and%2520Kasper%2520Green%2520Larsen%26entry.1292438233%3D%2520%2520Recent%2520works%2520on%2520the%2520parallel%2520complexity%2520of%2520Boosting%2520have%2520established%2520strong%250Alower%2520bounds%2520on%2520the%2520tradeoff%2520between%2520the%2520number%2520of%2520training%2520rounds%2520%2524p%2524%2520and%2520the%250Atotal%2520parallel%2520work%2520per%2520round%2520%2524t%2524.%2520These%2520works%2520have%2520also%2520presented%2520highly%250Anon-trivial%2520parallel%2520algorithms%2520that%2520shed%2520light%2520on%2520different%2520regions%2520of%2520this%250Atradeoff.%2520Despite%2520these%2520advancements%252C%2520a%2520significant%2520gap%2520persists%2520between%2520the%250Atheoretical%2520lower%2520bounds%2520and%2520the%2520performance%2520of%2520these%2520algorithms%2520across%2520much%2520of%250Athe%2520tradeoff%2520space.%2520In%2520this%2520work%252C%2520we%2520essentially%2520close%2520this%2520gap%2520by%2520providing%250Aboth%2520improved%2520lower%2520bounds%2520on%2520the%2520parallel%2520complexity%2520of%2520weak-to-strong%250Alearners%252C%2520and%2520a%2520parallel%2520Boosting%2520algorithm%2520whose%2520performance%2520matches%2520these%250Abounds%2520across%2520the%2520entire%2520%2524p%2524%2520vs.~%2524t%2524%2520compromise%2520spectrum%252C%2520up%2520to%2520logarithmic%250Afactors.%2520Ultimately%252C%2520this%2520work%2520settles%2520the%2520true%2520parallel%2520complexity%2520of%2520Boosting%250Aalgorithms%2520that%2520are%2520nearly%2520sample-optimal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Parallelization%20of%20Boosting&entry.906535625=Arthur%20da%20Cunha%20and%20Mikael%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Kasper%20Green%20Larsen&entry.1292438233=%20%20Recent%20works%20on%20the%20parallel%20complexity%20of%20Boosting%20have%20established%20strong%0Alower%20bounds%20on%20the%20tradeoff%20between%20the%20number%20of%20training%20rounds%20%24p%24%20and%20the%0Atotal%20parallel%20work%20per%20round%20%24t%24.%20These%20works%20have%20also%20presented%20highly%0Anon-trivial%20parallel%20algorithms%20that%20shed%20light%20on%20different%20regions%20of%20this%0Atradeoff.%20Despite%20these%20advancements%2C%20a%20significant%20gap%20persists%20between%20the%0Atheoretical%20lower%20bounds%20and%20the%20performance%20of%20these%20algorithms%20across%20much%20of%0Athe%20tradeoff%20space.%20In%20this%20work%2C%20we%20essentially%20close%20this%20gap%20by%20providing%0Aboth%20improved%20lower%20bounds%20on%20the%20parallel%20complexity%20of%20weak-to-strong%0Alearners%2C%20and%20a%20parallel%20Boosting%20algorithm%20whose%20performance%20matches%20these%0Abounds%20across%20the%20entire%20%24p%24%20vs.~%24t%24%20compromise%20spectrum%2C%20up%20to%20logarithmic%0Afactors.%20Ultimately%2C%20this%20work%20settles%20the%20true%20parallel%20complexity%20of%20Boosting%0Aalgorithms%20that%20are%20nearly%20sample-optimal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16653v1&entry.124074799=Read"},
{"title": "A Simple and Generalist Approach for Panoptic Segmentation", "author": "Nedyalko Prisadnikov and Wouter Van Gansbeke and Danda Pani Paudel and Luc Van Gool", "abstract": "  Generalist vision models aim for one and the same architecture for a variety\nof vision tasks. While such shared architecture may seem attractive, generalist\nmodels tend to be outperformed by their bespoken counterparts, especially in\nthe case of panoptic segmentation. We address this problem by introducing two\nkey contributions, without compromising the desirable properties of generalist\nmodels. These contributions are: (i) a positional-embedding (PE) based loss for\nimproved centroid regressions; (ii) Edge Distance Sampling (EDS) for the better\nseparation of instance boundaries. The PE-based loss facilitates a better\nper-pixel regression of the associated instance's centroid, whereas EDS\ncontributes by carefully handling the void regions (caused by missing labels)\nand smaller instances. These two simple yet effective modifications\nsignificantly improve established baselines, while achieving state-of-the-art\nresults among all generalist solutions. More specifically, our method achieves\na panoptic quality(PQ) of 52.5 on the COCO dataset, which is an improvement of\n10 points over the best model with similar approach (Painter), and is superior\nby 2 to the best performing diffusion-based method Pix2Seq-$\\mathcal{D}$.\nFurthermore, we provide insights into and an in-depth analysis of our\ncontributions through exhaustive experiments. Our source code and model weights\nwill be made publicly available.\n", "link": "http://arxiv.org/abs/2408.16504v1", "date": "2024-08-29", "relevancy": 2.2072, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5712}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.539}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20and%20Generalist%20Approach%20for%20Panoptic%20Segmentation&body=Title%3A%20A%20Simple%20and%20Generalist%20Approach%20for%20Panoptic%20Segmentation%0AAuthor%3A%20Nedyalko%20Prisadnikov%20and%20Wouter%20Van%20Gansbeke%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Generalist%20vision%20models%20aim%20for%20one%20and%20the%20same%20architecture%20for%20a%20variety%0Aof%20vision%20tasks.%20While%20such%20shared%20architecture%20may%20seem%20attractive%2C%20generalist%0Amodels%20tend%20to%20be%20outperformed%20by%20their%20bespoken%20counterparts%2C%20especially%20in%0Athe%20case%20of%20panoptic%20segmentation.%20We%20address%20this%20problem%20by%20introducing%20two%0Akey%20contributions%2C%20without%20compromising%20the%20desirable%20properties%20of%20generalist%0Amodels.%20These%20contributions%20are%3A%20%28i%29%20a%20positional-embedding%20%28PE%29%20based%20loss%20for%0Aimproved%20centroid%20regressions%3B%20%28ii%29%20Edge%20Distance%20Sampling%20%28EDS%29%20for%20the%20better%0Aseparation%20of%20instance%20boundaries.%20The%20PE-based%20loss%20facilitates%20a%20better%0Aper-pixel%20regression%20of%20the%20associated%20instance%27s%20centroid%2C%20whereas%20EDS%0Acontributes%20by%20carefully%20handling%20the%20void%20regions%20%28caused%20by%20missing%20labels%29%0Aand%20smaller%20instances.%20These%20two%20simple%20yet%20effective%20modifications%0Asignificantly%20improve%20established%20baselines%2C%20while%20achieving%20state-of-the-art%0Aresults%20among%20all%20generalist%20solutions.%20More%20specifically%2C%20our%20method%20achieves%0Aa%20panoptic%20quality%28PQ%29%20of%2052.5%20on%20the%20COCO%20dataset%2C%20which%20is%20an%20improvement%20of%0A10%20points%20over%20the%20best%20model%20with%20similar%20approach%20%28Painter%29%2C%20and%20is%20superior%0Aby%202%20to%20the%20best%20performing%20diffusion-based%20method%20Pix2Seq-%24%5Cmathcal%7BD%7D%24.%0AFurthermore%2C%20we%20provide%20insights%20into%20and%20an%20in-depth%20analysis%20of%20our%0Acontributions%20through%20exhaustive%20experiments.%20Our%20source%20code%20and%20model%20weights%0Awill%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520and%2520Generalist%2520Approach%2520for%2520Panoptic%2520Segmentation%26entry.906535625%3DNedyalko%2520Prisadnikov%2520and%2520Wouter%2520Van%2520Gansbeke%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Generalist%2520vision%2520models%2520aim%2520for%2520one%2520and%2520the%2520same%2520architecture%2520for%2520a%2520variety%250Aof%2520vision%2520tasks.%2520While%2520such%2520shared%2520architecture%2520may%2520seem%2520attractive%252C%2520generalist%250Amodels%2520tend%2520to%2520be%2520outperformed%2520by%2520their%2520bespoken%2520counterparts%252C%2520especially%2520in%250Athe%2520case%2520of%2520panoptic%2520segmentation.%2520We%2520address%2520this%2520problem%2520by%2520introducing%2520two%250Akey%2520contributions%252C%2520without%2520compromising%2520the%2520desirable%2520properties%2520of%2520generalist%250Amodels.%2520These%2520contributions%2520are%253A%2520%2528i%2529%2520a%2520positional-embedding%2520%2528PE%2529%2520based%2520loss%2520for%250Aimproved%2520centroid%2520regressions%253B%2520%2528ii%2529%2520Edge%2520Distance%2520Sampling%2520%2528EDS%2529%2520for%2520the%2520better%250Aseparation%2520of%2520instance%2520boundaries.%2520The%2520PE-based%2520loss%2520facilitates%2520a%2520better%250Aper-pixel%2520regression%2520of%2520the%2520associated%2520instance%2527s%2520centroid%252C%2520whereas%2520EDS%250Acontributes%2520by%2520carefully%2520handling%2520the%2520void%2520regions%2520%2528caused%2520by%2520missing%2520labels%2529%250Aand%2520smaller%2520instances.%2520These%2520two%2520simple%2520yet%2520effective%2520modifications%250Asignificantly%2520improve%2520established%2520baselines%252C%2520while%2520achieving%2520state-of-the-art%250Aresults%2520among%2520all%2520generalist%2520solutions.%2520More%2520specifically%252C%2520our%2520method%2520achieves%250Aa%2520panoptic%2520quality%2528PQ%2529%2520of%252052.5%2520on%2520the%2520COCO%2520dataset%252C%2520which%2520is%2520an%2520improvement%2520of%250A10%2520points%2520over%2520the%2520best%2520model%2520with%2520similar%2520approach%2520%2528Painter%2529%252C%2520and%2520is%2520superior%250Aby%25202%2520to%2520the%2520best%2520performing%2520diffusion-based%2520method%2520Pix2Seq-%2524%255Cmathcal%257BD%257D%2524.%250AFurthermore%252C%2520we%2520provide%2520insights%2520into%2520and%2520an%2520in-depth%2520analysis%2520of%2520our%250Acontributions%2520through%2520exhaustive%2520experiments.%2520Our%2520source%2520code%2520and%2520model%2520weights%250Awill%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20and%20Generalist%20Approach%20for%20Panoptic%20Segmentation&entry.906535625=Nedyalko%20Prisadnikov%20and%20Wouter%20Van%20Gansbeke%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Generalist%20vision%20models%20aim%20for%20one%20and%20the%20same%20architecture%20for%20a%20variety%0Aof%20vision%20tasks.%20While%20such%20shared%20architecture%20may%20seem%20attractive%2C%20generalist%0Amodels%20tend%20to%20be%20outperformed%20by%20their%20bespoken%20counterparts%2C%20especially%20in%0Athe%20case%20of%20panoptic%20segmentation.%20We%20address%20this%20problem%20by%20introducing%20two%0Akey%20contributions%2C%20without%20compromising%20the%20desirable%20properties%20of%20generalist%0Amodels.%20These%20contributions%20are%3A%20%28i%29%20a%20positional-embedding%20%28PE%29%20based%20loss%20for%0Aimproved%20centroid%20regressions%3B%20%28ii%29%20Edge%20Distance%20Sampling%20%28EDS%29%20for%20the%20better%0Aseparation%20of%20instance%20boundaries.%20The%20PE-based%20loss%20facilitates%20a%20better%0Aper-pixel%20regression%20of%20the%20associated%20instance%27s%20centroid%2C%20whereas%20EDS%0Acontributes%20by%20carefully%20handling%20the%20void%20regions%20%28caused%20by%20missing%20labels%29%0Aand%20smaller%20instances.%20These%20two%20simple%20yet%20effective%20modifications%0Asignificantly%20improve%20established%20baselines%2C%20while%20achieving%20state-of-the-art%0Aresults%20among%20all%20generalist%20solutions.%20More%20specifically%2C%20our%20method%20achieves%0Aa%20panoptic%20quality%28PQ%29%20of%2052.5%20on%20the%20COCO%20dataset%2C%20which%20is%20an%20improvement%20of%0A10%20points%20over%20the%20best%20model%20with%20similar%20approach%20%28Painter%29%2C%20and%20is%20superior%0Aby%202%20to%20the%20best%20performing%20diffusion-based%20method%20Pix2Seq-%24%5Cmathcal%7BD%7D%24.%0AFurthermore%2C%20we%20provide%20insights%20into%20and%20an%20in-depth%20analysis%20of%20our%0Acontributions%20through%20exhaustive%20experiments.%20Our%20source%20code%20and%20model%20weights%0Awill%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16504v1&entry.124074799=Read"},
{"title": "Improving 3D deep learning segmentation with biophysically motivated\n  cell synthesis", "author": "Roman Bruch and Mario Vitacolonna and Elina N\u00fcrnberg and Simeon Sauer and R\u00fcdiger Rudolf and Markus Reischl", "abstract": "  Biomedical research increasingly relies on 3D cell culture models and\nAI-based analysis can potentially facilitate a detailed and accurate feature\nextraction on a single-cell level. However, this requires for a precise\nsegmentation of 3D cell datasets, which in turn demands high-quality ground\ntruth for training. Manual annotation, the gold standard for ground truth data,\nis too time-consuming and thus not feasible for the generation of large 3D\ntraining datasets. To address this, we present a novel framework for generating\n3D training data, which integrates biophysical modeling for realistic cell\nshape and alignment. Our approach allows the in silico generation of coherent\nmembrane and nuclei signals, that enable the training of segmentation models\nutilizing both channels for improved performance. Furthermore, we present a new\nGAN training scheme that generates not only image data but also matching\nlabels. Quantitative evaluation shows superior performance of biophysical\nmotivated synthetic training data, even outperforming manual annotation and\npretrained models. This underscores the potential of incorporating biophysical\nmodeling for enhancing synthetic training data quality.\n", "link": "http://arxiv.org/abs/2408.16471v1", "date": "2024-08-29", "relevancy": 2.1909, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5482}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5482}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%203D%20deep%20learning%20segmentation%20with%20biophysically%20motivated%0A%20%20cell%20synthesis&body=Title%3A%20Improving%203D%20deep%20learning%20segmentation%20with%20biophysically%20motivated%0A%20%20cell%20synthesis%0AAuthor%3A%20Roman%20Bruch%20and%20Mario%20Vitacolonna%20and%20Elina%20N%C3%BCrnberg%20and%20Simeon%20Sauer%20and%20R%C3%BCdiger%20Rudolf%20and%20Markus%20Reischl%0AAbstract%3A%20%20%20Biomedical%20research%20increasingly%20relies%20on%203D%20cell%20culture%20models%20and%0AAI-based%20analysis%20can%20potentially%20facilitate%20a%20detailed%20and%20accurate%20feature%0Aextraction%20on%20a%20single-cell%20level.%20However%2C%20this%20requires%20for%20a%20precise%0Asegmentation%20of%203D%20cell%20datasets%2C%20which%20in%20turn%20demands%20high-quality%20ground%0Atruth%20for%20training.%20Manual%20annotation%2C%20the%20gold%20standard%20for%20ground%20truth%20data%2C%0Ais%20too%20time-consuming%20and%20thus%20not%20feasible%20for%20the%20generation%20of%20large%203D%0Atraining%20datasets.%20To%20address%20this%2C%20we%20present%20a%20novel%20framework%20for%20generating%0A3D%20training%20data%2C%20which%20integrates%20biophysical%20modeling%20for%20realistic%20cell%0Ashape%20and%20alignment.%20Our%20approach%20allows%20the%20in%20silico%20generation%20of%20coherent%0Amembrane%20and%20nuclei%20signals%2C%20that%20enable%20the%20training%20of%20segmentation%20models%0Autilizing%20both%20channels%20for%20improved%20performance.%20Furthermore%2C%20we%20present%20a%20new%0AGAN%20training%20scheme%20that%20generates%20not%20only%20image%20data%20but%20also%20matching%0Alabels.%20Quantitative%20evaluation%20shows%20superior%20performance%20of%20biophysical%0Amotivated%20synthetic%20training%20data%2C%20even%20outperforming%20manual%20annotation%20and%0Apretrained%20models.%20This%20underscores%20the%20potential%20of%20incorporating%20biophysical%0Amodeling%20for%20enhancing%20synthetic%20training%20data%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%25203D%2520deep%2520learning%2520segmentation%2520with%2520biophysically%2520motivated%250A%2520%2520cell%2520synthesis%26entry.906535625%3DRoman%2520Bruch%2520and%2520Mario%2520Vitacolonna%2520and%2520Elina%2520N%25C3%25BCrnberg%2520and%2520Simeon%2520Sauer%2520and%2520R%25C3%25BCdiger%2520Rudolf%2520and%2520Markus%2520Reischl%26entry.1292438233%3D%2520%2520Biomedical%2520research%2520increasingly%2520relies%2520on%25203D%2520cell%2520culture%2520models%2520and%250AAI-based%2520analysis%2520can%2520potentially%2520facilitate%2520a%2520detailed%2520and%2520accurate%2520feature%250Aextraction%2520on%2520a%2520single-cell%2520level.%2520However%252C%2520this%2520requires%2520for%2520a%2520precise%250Asegmentation%2520of%25203D%2520cell%2520datasets%252C%2520which%2520in%2520turn%2520demands%2520high-quality%2520ground%250Atruth%2520for%2520training.%2520Manual%2520annotation%252C%2520the%2520gold%2520standard%2520for%2520ground%2520truth%2520data%252C%250Ais%2520too%2520time-consuming%2520and%2520thus%2520not%2520feasible%2520for%2520the%2520generation%2520of%2520large%25203D%250Atraining%2520datasets.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520novel%2520framework%2520for%2520generating%250A3D%2520training%2520data%252C%2520which%2520integrates%2520biophysical%2520modeling%2520for%2520realistic%2520cell%250Ashape%2520and%2520alignment.%2520Our%2520approach%2520allows%2520the%2520in%2520silico%2520generation%2520of%2520coherent%250Amembrane%2520and%2520nuclei%2520signals%252C%2520that%2520enable%2520the%2520training%2520of%2520segmentation%2520models%250Autilizing%2520both%2520channels%2520for%2520improved%2520performance.%2520Furthermore%252C%2520we%2520present%2520a%2520new%250AGAN%2520training%2520scheme%2520that%2520generates%2520not%2520only%2520image%2520data%2520but%2520also%2520matching%250Alabels.%2520Quantitative%2520evaluation%2520shows%2520superior%2520performance%2520of%2520biophysical%250Amotivated%2520synthetic%2520training%2520data%252C%2520even%2520outperforming%2520manual%2520annotation%2520and%250Apretrained%2520models.%2520This%2520underscores%2520the%2520potential%2520of%2520incorporating%2520biophysical%250Amodeling%2520for%2520enhancing%2520synthetic%2520training%2520data%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%203D%20deep%20learning%20segmentation%20with%20biophysically%20motivated%0A%20%20cell%20synthesis&entry.906535625=Roman%20Bruch%20and%20Mario%20Vitacolonna%20and%20Elina%20N%C3%BCrnberg%20and%20Simeon%20Sauer%20and%20R%C3%BCdiger%20Rudolf%20and%20Markus%20Reischl&entry.1292438233=%20%20Biomedical%20research%20increasingly%20relies%20on%203D%20cell%20culture%20models%20and%0AAI-based%20analysis%20can%20potentially%20facilitate%20a%20detailed%20and%20accurate%20feature%0Aextraction%20on%20a%20single-cell%20level.%20However%2C%20this%20requires%20for%20a%20precise%0Asegmentation%20of%203D%20cell%20datasets%2C%20which%20in%20turn%20demands%20high-quality%20ground%0Atruth%20for%20training.%20Manual%20annotation%2C%20the%20gold%20standard%20for%20ground%20truth%20data%2C%0Ais%20too%20time-consuming%20and%20thus%20not%20feasible%20for%20the%20generation%20of%20large%203D%0Atraining%20datasets.%20To%20address%20this%2C%20we%20present%20a%20novel%20framework%20for%20generating%0A3D%20training%20data%2C%20which%20integrates%20biophysical%20modeling%20for%20realistic%20cell%0Ashape%20and%20alignment.%20Our%20approach%20allows%20the%20in%20silico%20generation%20of%20coherent%0Amembrane%20and%20nuclei%20signals%2C%20that%20enable%20the%20training%20of%20segmentation%20models%0Autilizing%20both%20channels%20for%20improved%20performance.%20Furthermore%2C%20we%20present%20a%20new%0AGAN%20training%20scheme%20that%20generates%20not%20only%20image%20data%20but%20also%20matching%0Alabels.%20Quantitative%20evaluation%20shows%20superior%20performance%20of%20biophysical%0Amotivated%20synthetic%20training%20data%2C%20even%20outperforming%20manual%20annotation%20and%0Apretrained%20models.%20This%20underscores%20the%20potential%20of%20incorporating%20biophysical%0Amodeling%20for%20enhancing%20synthetic%20training%20data%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16471v1&entry.124074799=Read"},
{"title": "GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative\n  Models", "author": "Moreno D'Inc\u00e0 and Elia Peruzzo and Massimiliano Mancini and Xingqian Xu and Humphrey Shi and Nicu Sebe", "abstract": "  Recent progress in Text-to-Image (T2I) generative models has enabled\nhigh-quality image generation. As performance and accessibility increase, these\nmodels are gaining significant attraction and popularity: ensuring their\nfairness and safety is a priority to prevent the dissemination and perpetuation\nof biases. However, existing studies in bias detection focus on closed sets of\npredefined biases (e.g., gender, ethnicity). In this paper, we propose a\ngeneral framework to identify, quantify, and explain biases in an open set\nsetting, i.e. without requiring a predefined set. This pipeline leverages a\nLarge Language Model (LLM) to propose biases starting from a set of captions.\nNext, these captions are used by the target generative model for generating a\nset of images. Finally, Vision Question Answering (VQA) is leveraged for bias\nevaluation. We show two variations of this framework: OpenBias and GradBias.\nOpenBias detects and quantifies biases, while GradBias determines the\ncontribution of individual prompt words on biases. OpenBias effectively detects\nboth well-known and novel biases related to people, objects, and animals and\nhighly aligns with existing closed-set bias detection methods and human\njudgment. GradBias shows that neutral words can significantly influence biases\nand it outperforms several baselines, including state-of-the-art foundation\nmodels. Code available here: https://github.com/Moreno98/GradBias.\n", "link": "http://arxiv.org/abs/2408.16700v1", "date": "2024-08-29", "relevancy": 2.1866, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5599}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5511}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GradBias%3A%20Unveiling%20Word%20Influence%20on%20Bias%20in%20Text-to-Image%20Generative%0A%20%20Models&body=Title%3A%20GradBias%3A%20Unveiling%20Word%20Influence%20on%20Bias%20in%20Text-to-Image%20Generative%0A%20%20Models%0AAuthor%3A%20Moreno%20D%27Inc%C3%A0%20and%20Elia%20Peruzzo%20and%20Massimiliano%20Mancini%20and%20Xingqian%20Xu%20and%20Humphrey%20Shi%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Recent%20progress%20in%20Text-to-Image%20%28T2I%29%20generative%20models%20has%20enabled%0Ahigh-quality%20image%20generation.%20As%20performance%20and%20accessibility%20increase%2C%20these%0Amodels%20are%20gaining%20significant%20attraction%20and%20popularity%3A%20ensuring%20their%0Afairness%20and%20safety%20is%20a%20priority%20to%20prevent%20the%20dissemination%20and%20perpetuation%0Aof%20biases.%20However%2C%20existing%20studies%20in%20bias%20detection%20focus%20on%20closed%20sets%20of%0Apredefined%20biases%20%28e.g.%2C%20gender%2C%20ethnicity%29.%20In%20this%20paper%2C%20we%20propose%20a%0Ageneral%20framework%20to%20identify%2C%20quantify%2C%20and%20explain%20biases%20in%20an%20open%20set%0Asetting%2C%20i.e.%20without%20requiring%20a%20predefined%20set.%20This%20pipeline%20leverages%20a%0ALarge%20Language%20Model%20%28LLM%29%20to%20propose%20biases%20starting%20from%20a%20set%20of%20captions.%0ANext%2C%20these%20captions%20are%20used%20by%20the%20target%20generative%20model%20for%20generating%20a%0Aset%20of%20images.%20Finally%2C%20Vision%20Question%20Answering%20%28VQA%29%20is%20leveraged%20for%20bias%0Aevaluation.%20We%20show%20two%20variations%20of%20this%20framework%3A%20OpenBias%20and%20GradBias.%0AOpenBias%20detects%20and%20quantifies%20biases%2C%20while%20GradBias%20determines%20the%0Acontribution%20of%20individual%20prompt%20words%20on%20biases.%20OpenBias%20effectively%20detects%0Aboth%20well-known%20and%20novel%20biases%20related%20to%20people%2C%20objects%2C%20and%20animals%20and%0Ahighly%20aligns%20with%20existing%20closed-set%20bias%20detection%20methods%20and%20human%0Ajudgment.%20GradBias%20shows%20that%20neutral%20words%20can%20significantly%20influence%20biases%0Aand%20it%20outperforms%20several%20baselines%2C%20including%20state-of-the-art%20foundation%0Amodels.%20Code%20available%20here%3A%20https%3A//github.com/Moreno98/GradBias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradBias%253A%2520Unveiling%2520Word%2520Influence%2520on%2520Bias%2520in%2520Text-to-Image%2520Generative%250A%2520%2520Models%26entry.906535625%3DMoreno%2520D%2527Inc%25C3%25A0%2520and%2520Elia%2520Peruzzo%2520and%2520Massimiliano%2520Mancini%2520and%2520Xingqian%2520Xu%2520and%2520Humphrey%2520Shi%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520Text-to-Image%2520%2528T2I%2529%2520generative%2520models%2520has%2520enabled%250Ahigh-quality%2520image%2520generation.%2520As%2520performance%2520and%2520accessibility%2520increase%252C%2520these%250Amodels%2520are%2520gaining%2520significant%2520attraction%2520and%2520popularity%253A%2520ensuring%2520their%250Afairness%2520and%2520safety%2520is%2520a%2520priority%2520to%2520prevent%2520the%2520dissemination%2520and%2520perpetuation%250Aof%2520biases.%2520However%252C%2520existing%2520studies%2520in%2520bias%2520detection%2520focus%2520on%2520closed%2520sets%2520of%250Apredefined%2520biases%2520%2528e.g.%252C%2520gender%252C%2520ethnicity%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Ageneral%2520framework%2520to%2520identify%252C%2520quantify%252C%2520and%2520explain%2520biases%2520in%2520an%2520open%2520set%250Asetting%252C%2520i.e.%2520without%2520requiring%2520a%2520predefined%2520set.%2520This%2520pipeline%2520leverages%2520a%250ALarge%2520Language%2520Model%2520%2528LLM%2529%2520to%2520propose%2520biases%2520starting%2520from%2520a%2520set%2520of%2520captions.%250ANext%252C%2520these%2520captions%2520are%2520used%2520by%2520the%2520target%2520generative%2520model%2520for%2520generating%2520a%250Aset%2520of%2520images.%2520Finally%252C%2520Vision%2520Question%2520Answering%2520%2528VQA%2529%2520is%2520leveraged%2520for%2520bias%250Aevaluation.%2520We%2520show%2520two%2520variations%2520of%2520this%2520framework%253A%2520OpenBias%2520and%2520GradBias.%250AOpenBias%2520detects%2520and%2520quantifies%2520biases%252C%2520while%2520GradBias%2520determines%2520the%250Acontribution%2520of%2520individual%2520prompt%2520words%2520on%2520biases.%2520OpenBias%2520effectively%2520detects%250Aboth%2520well-known%2520and%2520novel%2520biases%2520related%2520to%2520people%252C%2520objects%252C%2520and%2520animals%2520and%250Ahighly%2520aligns%2520with%2520existing%2520closed-set%2520bias%2520detection%2520methods%2520and%2520human%250Ajudgment.%2520GradBias%2520shows%2520that%2520neutral%2520words%2520can%2520significantly%2520influence%2520biases%250Aand%2520it%2520outperforms%2520several%2520baselines%252C%2520including%2520state-of-the-art%2520foundation%250Amodels.%2520Code%2520available%2520here%253A%2520https%253A//github.com/Moreno98/GradBias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GradBias%3A%20Unveiling%20Word%20Influence%20on%20Bias%20in%20Text-to-Image%20Generative%0A%20%20Models&entry.906535625=Moreno%20D%27Inc%C3%A0%20and%20Elia%20Peruzzo%20and%20Massimiliano%20Mancini%20and%20Xingqian%20Xu%20and%20Humphrey%20Shi%20and%20Nicu%20Sebe&entry.1292438233=%20%20Recent%20progress%20in%20Text-to-Image%20%28T2I%29%20generative%20models%20has%20enabled%0Ahigh-quality%20image%20generation.%20As%20performance%20and%20accessibility%20increase%2C%20these%0Amodels%20are%20gaining%20significant%20attraction%20and%20popularity%3A%20ensuring%20their%0Afairness%20and%20safety%20is%20a%20priority%20to%20prevent%20the%20dissemination%20and%20perpetuation%0Aof%20biases.%20However%2C%20existing%20studies%20in%20bias%20detection%20focus%20on%20closed%20sets%20of%0Apredefined%20biases%20%28e.g.%2C%20gender%2C%20ethnicity%29.%20In%20this%20paper%2C%20we%20propose%20a%0Ageneral%20framework%20to%20identify%2C%20quantify%2C%20and%20explain%20biases%20in%20an%20open%20set%0Asetting%2C%20i.e.%20without%20requiring%20a%20predefined%20set.%20This%20pipeline%20leverages%20a%0ALarge%20Language%20Model%20%28LLM%29%20to%20propose%20biases%20starting%20from%20a%20set%20of%20captions.%0ANext%2C%20these%20captions%20are%20used%20by%20the%20target%20generative%20model%20for%20generating%20a%0Aset%20of%20images.%20Finally%2C%20Vision%20Question%20Answering%20%28VQA%29%20is%20leveraged%20for%20bias%0Aevaluation.%20We%20show%20two%20variations%20of%20this%20framework%3A%20OpenBias%20and%20GradBias.%0AOpenBias%20detects%20and%20quantifies%20biases%2C%20while%20GradBias%20determines%20the%0Acontribution%20of%20individual%20prompt%20words%20on%20biases.%20OpenBias%20effectively%20detects%0Aboth%20well-known%20and%20novel%20biases%20related%20to%20people%2C%20objects%2C%20and%20animals%20and%0Ahighly%20aligns%20with%20existing%20closed-set%20bias%20detection%20methods%20and%20human%0Ajudgment.%20GradBias%20shows%20that%20neutral%20words%20can%20significantly%20influence%20biases%0Aand%20it%20outperforms%20several%20baselines%2C%20including%20state-of-the-art%20foundation%0Amodels.%20Code%20available%20here%3A%20https%3A//github.com/Moreno98/GradBias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16700v1&entry.124074799=Read"},
{"title": "Auricular Vagus Nerve Stimulation for Enhancing Remote Pilot Training\n  and Operations", "author": "William J. Tyler", "abstract": "  The rapid growth of the drone industry, particularly in the use of small\nunmanned aerial systems (sUAS) and unmanned aerial vehicles (UAVs), requires\nthe development of advanced training protocols for remote pilots. Remote pilots\nmust develop a combination of technical and cognitive skills to manage the\ncomplexities of modern drone operations. This paper explores the integration of\nneurotechnology, specifically auricular vagus nerve stimulation (aVNS), as a\nmethod to enhance remote pilot training and performance. The scientific\nliterature shows aVNS can safely improve cognitive functions such as attention,\nlearning, and memory. It has also been shown useful to manage stress responses.\nFor safe and efficient sUAS/UAV operation, it is essential for pilots to\nmaintain high levels of vigilance and decision-making under pressure. By\nmodulating sympathetic stress and cortical arousal, aVNS can prime cognitive\nfaculties before training, help maintain focus during training and improve\nstress recovery post-training. Furthermore, aVNS has demonstrated the potential\nto enhance multitasking and cognitive control. This may help remote pilots\nduring complex sUAS operations by potentially reducing the risk of impulsive\ndecision-making or cognitive errors. This paper advocates for the inclusion of\naVNS in remote pilot training programs by proposing that it can provide\nsignificant benefits in improving cognitive readiness, skill and knowledge\nacquisition, as well as operational safety and efficiency. Future research\nshould focus on optimizing aVNS protocols for drone pilots while assessing\nlong-term benefits to industrial safety and workforce readiness in real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2408.16755v1", "date": "2024-08-29", "relevancy": 2.1795, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4578}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4259}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auricular%20Vagus%20Nerve%20Stimulation%20for%20Enhancing%20Remote%20Pilot%20Training%0A%20%20and%20Operations&body=Title%3A%20Auricular%20Vagus%20Nerve%20Stimulation%20for%20Enhancing%20Remote%20Pilot%20Training%0A%20%20and%20Operations%0AAuthor%3A%20William%20J.%20Tyler%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20the%20drone%20industry%2C%20particularly%20in%20the%20use%20of%20small%0Aunmanned%20aerial%20systems%20%28sUAS%29%20and%20unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20requires%0Athe%20development%20of%20advanced%20training%20protocols%20for%20remote%20pilots.%20Remote%20pilots%0Amust%20develop%20a%20combination%20of%20technical%20and%20cognitive%20skills%20to%20manage%20the%0Acomplexities%20of%20modern%20drone%20operations.%20This%20paper%20explores%20the%20integration%20of%0Aneurotechnology%2C%20specifically%20auricular%20vagus%20nerve%20stimulation%20%28aVNS%29%2C%20as%20a%0Amethod%20to%20enhance%20remote%20pilot%20training%20and%20performance.%20The%20scientific%0Aliterature%20shows%20aVNS%20can%20safely%20improve%20cognitive%20functions%20such%20as%20attention%2C%0Alearning%2C%20and%20memory.%20It%20has%20also%20been%20shown%20useful%20to%20manage%20stress%20responses.%0AFor%20safe%20and%20efficient%20sUAS/UAV%20operation%2C%20it%20is%20essential%20for%20pilots%20to%0Amaintain%20high%20levels%20of%20vigilance%20and%20decision-making%20under%20pressure.%20By%0Amodulating%20sympathetic%20stress%20and%20cortical%20arousal%2C%20aVNS%20can%20prime%20cognitive%0Afaculties%20before%20training%2C%20help%20maintain%20focus%20during%20training%20and%20improve%0Astress%20recovery%20post-training.%20Furthermore%2C%20aVNS%20has%20demonstrated%20the%20potential%0Ato%20enhance%20multitasking%20and%20cognitive%20control.%20This%20may%20help%20remote%20pilots%0Aduring%20complex%20sUAS%20operations%20by%20potentially%20reducing%20the%20risk%20of%20impulsive%0Adecision-making%20or%20cognitive%20errors.%20This%20paper%20advocates%20for%20the%20inclusion%20of%0AaVNS%20in%20remote%20pilot%20training%20programs%20by%20proposing%20that%20it%20can%20provide%0Asignificant%20benefits%20in%20improving%20cognitive%20readiness%2C%20skill%20and%20knowledge%0Aacquisition%2C%20as%20well%20as%20operational%20safety%20and%20efficiency.%20Future%20research%0Ashould%20focus%20on%20optimizing%20aVNS%20protocols%20for%20drone%20pilots%20while%20assessing%0Along-term%20benefits%20to%20industrial%20safety%20and%20workforce%20readiness%20in%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuricular%2520Vagus%2520Nerve%2520Stimulation%2520for%2520Enhancing%2520Remote%2520Pilot%2520Training%250A%2520%2520and%2520Operations%26entry.906535625%3DWilliam%2520J.%2520Tyler%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520the%2520drone%2520industry%252C%2520particularly%2520in%2520the%2520use%2520of%2520small%250Aunmanned%2520aerial%2520systems%2520%2528sUAS%2529%2520and%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%252C%2520requires%250Athe%2520development%2520of%2520advanced%2520training%2520protocols%2520for%2520remote%2520pilots.%2520Remote%2520pilots%250Amust%2520develop%2520a%2520combination%2520of%2520technical%2520and%2520cognitive%2520skills%2520to%2520manage%2520the%250Acomplexities%2520of%2520modern%2520drone%2520operations.%2520This%2520paper%2520explores%2520the%2520integration%2520of%250Aneurotechnology%252C%2520specifically%2520auricular%2520vagus%2520nerve%2520stimulation%2520%2528aVNS%2529%252C%2520as%2520a%250Amethod%2520to%2520enhance%2520remote%2520pilot%2520training%2520and%2520performance.%2520The%2520scientific%250Aliterature%2520shows%2520aVNS%2520can%2520safely%2520improve%2520cognitive%2520functions%2520such%2520as%2520attention%252C%250Alearning%252C%2520and%2520memory.%2520It%2520has%2520also%2520been%2520shown%2520useful%2520to%2520manage%2520stress%2520responses.%250AFor%2520safe%2520and%2520efficient%2520sUAS/UAV%2520operation%252C%2520it%2520is%2520essential%2520for%2520pilots%2520to%250Amaintain%2520high%2520levels%2520of%2520vigilance%2520and%2520decision-making%2520under%2520pressure.%2520By%250Amodulating%2520sympathetic%2520stress%2520and%2520cortical%2520arousal%252C%2520aVNS%2520can%2520prime%2520cognitive%250Afaculties%2520before%2520training%252C%2520help%2520maintain%2520focus%2520during%2520training%2520and%2520improve%250Astress%2520recovery%2520post-training.%2520Furthermore%252C%2520aVNS%2520has%2520demonstrated%2520the%2520potential%250Ato%2520enhance%2520multitasking%2520and%2520cognitive%2520control.%2520This%2520may%2520help%2520remote%2520pilots%250Aduring%2520complex%2520sUAS%2520operations%2520by%2520potentially%2520reducing%2520the%2520risk%2520of%2520impulsive%250Adecision-making%2520or%2520cognitive%2520errors.%2520This%2520paper%2520advocates%2520for%2520the%2520inclusion%2520of%250AaVNS%2520in%2520remote%2520pilot%2520training%2520programs%2520by%2520proposing%2520that%2520it%2520can%2520provide%250Asignificant%2520benefits%2520in%2520improving%2520cognitive%2520readiness%252C%2520skill%2520and%2520knowledge%250Aacquisition%252C%2520as%2520well%2520as%2520operational%2520safety%2520and%2520efficiency.%2520Future%2520research%250Ashould%2520focus%2520on%2520optimizing%2520aVNS%2520protocols%2520for%2520drone%2520pilots%2520while%2520assessing%250Along-term%2520benefits%2520to%2520industrial%2520safety%2520and%2520workforce%2520readiness%2520in%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auricular%20Vagus%20Nerve%20Stimulation%20for%20Enhancing%20Remote%20Pilot%20Training%0A%20%20and%20Operations&entry.906535625=William%20J.%20Tyler&entry.1292438233=%20%20The%20rapid%20growth%20of%20the%20drone%20industry%2C%20particularly%20in%20the%20use%20of%20small%0Aunmanned%20aerial%20systems%20%28sUAS%29%20and%20unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20requires%0Athe%20development%20of%20advanced%20training%20protocols%20for%20remote%20pilots.%20Remote%20pilots%0Amust%20develop%20a%20combination%20of%20technical%20and%20cognitive%20skills%20to%20manage%20the%0Acomplexities%20of%20modern%20drone%20operations.%20This%20paper%20explores%20the%20integration%20of%0Aneurotechnology%2C%20specifically%20auricular%20vagus%20nerve%20stimulation%20%28aVNS%29%2C%20as%20a%0Amethod%20to%20enhance%20remote%20pilot%20training%20and%20performance.%20The%20scientific%0Aliterature%20shows%20aVNS%20can%20safely%20improve%20cognitive%20functions%20such%20as%20attention%2C%0Alearning%2C%20and%20memory.%20It%20has%20also%20been%20shown%20useful%20to%20manage%20stress%20responses.%0AFor%20safe%20and%20efficient%20sUAS/UAV%20operation%2C%20it%20is%20essential%20for%20pilots%20to%0Amaintain%20high%20levels%20of%20vigilance%20and%20decision-making%20under%20pressure.%20By%0Amodulating%20sympathetic%20stress%20and%20cortical%20arousal%2C%20aVNS%20can%20prime%20cognitive%0Afaculties%20before%20training%2C%20help%20maintain%20focus%20during%20training%20and%20improve%0Astress%20recovery%20post-training.%20Furthermore%2C%20aVNS%20has%20demonstrated%20the%20potential%0Ato%20enhance%20multitasking%20and%20cognitive%20control.%20This%20may%20help%20remote%20pilots%0Aduring%20complex%20sUAS%20operations%20by%20potentially%20reducing%20the%20risk%20of%20impulsive%0Adecision-making%20or%20cognitive%20errors.%20This%20paper%20advocates%20for%20the%20inclusion%20of%0AaVNS%20in%20remote%20pilot%20training%20programs%20by%20proposing%20that%20it%20can%20provide%0Asignificant%20benefits%20in%20improving%20cognitive%20readiness%2C%20skill%20and%20knowledge%0Aacquisition%2C%20as%20well%20as%20operational%20safety%20and%20efficiency.%20Future%20research%0Ashould%20focus%20on%20optimizing%20aVNS%20protocols%20for%20drone%20pilots%20while%20assessing%0Along-term%20benefits%20to%20industrial%20safety%20and%20workforce%20readiness%20in%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16755v1&entry.124074799=Read"},
{"title": "Towards Infusing Auxiliary Knowledge for Distracted Driver Detection", "author": "Ishwar B Balappanawar and Ashmit Chamoli and Ruwan Wickramarachchi and Aditya Mishra and Ponnurangam Kumaraguru and Amit P. Sheth", "abstract": "  Distracted driving is a leading cause of road accidents globally.\nIdentification of distracted driving involves reliably detecting and\nclassifying various forms of driver distraction (e.g., texting, eating, or\nusing in-car devices) from in-vehicle camera feeds to enhance road safety. This\ntask is challenging due to the need for robust models that can generalize to a\ndiverse set of driver behaviors without requiring extensive annotated datasets.\nIn this paper, we propose KiD3, a novel method for distracted driver detection\n(DDD) by infusing auxiliary knowledge about semantic relations between entities\nin a scene and the structural configuration of the driver's pose. Specifically,\nwe construct a unified framework that integrates the scene graphs, and driver\npose information with the visual cues in video frames to create a holistic\nrepresentation of the driver's actions.Our results indicate that KiD3 achieves\na 13.64% accuracy improvement over the vision-only baseline by incorporating\nsuch auxiliary knowledge with visual information.\n", "link": "http://arxiv.org/abs/2408.16621v1", "date": "2024-08-29", "relevancy": 2.1632, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5485}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.544}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Infusing%20Auxiliary%20Knowledge%20for%20Distracted%20Driver%20Detection&body=Title%3A%20Towards%20Infusing%20Auxiliary%20Knowledge%20for%20Distracted%20Driver%20Detection%0AAuthor%3A%20Ishwar%20B%20Balappanawar%20and%20Ashmit%20Chamoli%20and%20Ruwan%20Wickramarachchi%20and%20Aditya%20Mishra%20and%20Ponnurangam%20Kumaraguru%20and%20Amit%20P.%20Sheth%0AAbstract%3A%20%20%20Distracted%20driving%20is%20a%20leading%20cause%20of%20road%20accidents%20globally.%0AIdentification%20of%20distracted%20driving%20involves%20reliably%20detecting%20and%0Aclassifying%20various%20forms%20of%20driver%20distraction%20%28e.g.%2C%20texting%2C%20eating%2C%20or%0Ausing%20in-car%20devices%29%20from%20in-vehicle%20camera%20feeds%20to%20enhance%20road%20safety.%20This%0Atask%20is%20challenging%20due%20to%20the%20need%20for%20robust%20models%20that%20can%20generalize%20to%20a%0Adiverse%20set%20of%20driver%20behaviors%20without%20requiring%20extensive%20annotated%20datasets.%0AIn%20this%20paper%2C%20we%20propose%20KiD3%2C%20a%20novel%20method%20for%20distracted%20driver%20detection%0A%28DDD%29%20by%20infusing%20auxiliary%20knowledge%20about%20semantic%20relations%20between%20entities%0Ain%20a%20scene%20and%20the%20structural%20configuration%20of%20the%20driver%27s%20pose.%20Specifically%2C%0Awe%20construct%20a%20unified%20framework%20that%20integrates%20the%20scene%20graphs%2C%20and%20driver%0Apose%20information%20with%20the%20visual%20cues%20in%20video%20frames%20to%20create%20a%20holistic%0Arepresentation%20of%20the%20driver%27s%20actions.Our%20results%20indicate%20that%20KiD3%20achieves%0Aa%2013.64%25%20accuracy%20improvement%20over%20the%20vision-only%20baseline%20by%20incorporating%0Asuch%20auxiliary%20knowledge%20with%20visual%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Infusing%2520Auxiliary%2520Knowledge%2520for%2520Distracted%2520Driver%2520Detection%26entry.906535625%3DIshwar%2520B%2520Balappanawar%2520and%2520Ashmit%2520Chamoli%2520and%2520Ruwan%2520Wickramarachchi%2520and%2520Aditya%2520Mishra%2520and%2520Ponnurangam%2520Kumaraguru%2520and%2520Amit%2520P.%2520Sheth%26entry.1292438233%3D%2520%2520Distracted%2520driving%2520is%2520a%2520leading%2520cause%2520of%2520road%2520accidents%2520globally.%250AIdentification%2520of%2520distracted%2520driving%2520involves%2520reliably%2520detecting%2520and%250Aclassifying%2520various%2520forms%2520of%2520driver%2520distraction%2520%2528e.g.%252C%2520texting%252C%2520eating%252C%2520or%250Ausing%2520in-car%2520devices%2529%2520from%2520in-vehicle%2520camera%2520feeds%2520to%2520enhance%2520road%2520safety.%2520This%250Atask%2520is%2520challenging%2520due%2520to%2520the%2520need%2520for%2520robust%2520models%2520that%2520can%2520generalize%2520to%2520a%250Adiverse%2520set%2520of%2520driver%2520behaviors%2520without%2520requiring%2520extensive%2520annotated%2520datasets.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520KiD3%252C%2520a%2520novel%2520method%2520for%2520distracted%2520driver%2520detection%250A%2528DDD%2529%2520by%2520infusing%2520auxiliary%2520knowledge%2520about%2520semantic%2520relations%2520between%2520entities%250Ain%2520a%2520scene%2520and%2520the%2520structural%2520configuration%2520of%2520the%2520driver%2527s%2520pose.%2520Specifically%252C%250Awe%2520construct%2520a%2520unified%2520framework%2520that%2520integrates%2520the%2520scene%2520graphs%252C%2520and%2520driver%250Apose%2520information%2520with%2520the%2520visual%2520cues%2520in%2520video%2520frames%2520to%2520create%2520a%2520holistic%250Arepresentation%2520of%2520the%2520driver%2527s%2520actions.Our%2520results%2520indicate%2520that%2520KiD3%2520achieves%250Aa%252013.64%2525%2520accuracy%2520improvement%2520over%2520the%2520vision-only%2520baseline%2520by%2520incorporating%250Asuch%2520auxiliary%2520knowledge%2520with%2520visual%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Infusing%20Auxiliary%20Knowledge%20for%20Distracted%20Driver%20Detection&entry.906535625=Ishwar%20B%20Balappanawar%20and%20Ashmit%20Chamoli%20and%20Ruwan%20Wickramarachchi%20and%20Aditya%20Mishra%20and%20Ponnurangam%20Kumaraguru%20and%20Amit%20P.%20Sheth&entry.1292438233=%20%20Distracted%20driving%20is%20a%20leading%20cause%20of%20road%20accidents%20globally.%0AIdentification%20of%20distracted%20driving%20involves%20reliably%20detecting%20and%0Aclassifying%20various%20forms%20of%20driver%20distraction%20%28e.g.%2C%20texting%2C%20eating%2C%20or%0Ausing%20in-car%20devices%29%20from%20in-vehicle%20camera%20feeds%20to%20enhance%20road%20safety.%20This%0Atask%20is%20challenging%20due%20to%20the%20need%20for%20robust%20models%20that%20can%20generalize%20to%20a%0Adiverse%20set%20of%20driver%20behaviors%20without%20requiring%20extensive%20annotated%20datasets.%0AIn%20this%20paper%2C%20we%20propose%20KiD3%2C%20a%20novel%20method%20for%20distracted%20driver%20detection%0A%28DDD%29%20by%20infusing%20auxiliary%20knowledge%20about%20semantic%20relations%20between%20entities%0Ain%20a%20scene%20and%20the%20structural%20configuration%20of%20the%20driver%27s%20pose.%20Specifically%2C%0Awe%20construct%20a%20unified%20framework%20that%20integrates%20the%20scene%20graphs%2C%20and%20driver%0Apose%20information%20with%20the%20visual%20cues%20in%20video%20frames%20to%20create%20a%20holistic%0Arepresentation%20of%20the%20driver%27s%20actions.Our%20results%20indicate%20that%20KiD3%20achieves%0Aa%2013.64%25%20accuracy%20improvement%20over%20the%20vision-only%20baseline%20by%20incorporating%0Asuch%20auxiliary%20knowledge%20with%20visual%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16621v1&entry.124074799=Read"},
{"title": "CSGO: Content-Style Composition in Text-to-Image Generation", "author": "Peng Xing and Haofan Wang and Yanpeng Sun and Qixun Wang and Xu Bai and Hao Ai and Renyuan Huang and Zechao Li", "abstract": "  The diffusion model has shown exceptional capabilities in controlled image\ngeneration, which has further fueled interest in image style transfer. Existing\nworks mainly focus on training free-based methods (e.g., image inversion) due\nto the scarcity of specific data. In this study, we present a data construction\npipeline for content-style-stylized image triplets that generates and\nautomatically cleanses stylized data triplets. Based on this pipeline, we\nconstruct a dataset IMAGStyle, the first large-scale style transfer dataset\ncontaining 210k image triplets, available for the community to explore and\nresearch. Equipped with IMAGStyle, we propose CSGO, a style transfer model\nbased on end-to-end training, which explicitly decouples content and style\nfeatures employing independent feature injection. The unified CSGO implements\nimage-driven style transfer, text-driven stylized synthesis, and text\nediting-driven stylized synthesis. Extensive experiments demonstrate the\neffectiveness of our approach in enhancing style control capabilities in image\ngeneration. Additional visualization and access to the source code can be\nlocated on the project page: \\url{https://csgo-gen.github.io/}.\n", "link": "http://arxiv.org/abs/2408.16766v1", "date": "2024-08-29", "relevancy": 2.1612, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5532}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5342}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSGO%3A%20Content-Style%20Composition%20in%20Text-to-Image%20Generation&body=Title%3A%20CSGO%3A%20Content-Style%20Composition%20in%20Text-to-Image%20Generation%0AAuthor%3A%20Peng%20Xing%20and%20Haofan%20Wang%20and%20Yanpeng%20Sun%20and%20Qixun%20Wang%20and%20Xu%20Bai%20and%20Hao%20Ai%20and%20Renyuan%20Huang%20and%20Zechao%20Li%0AAbstract%3A%20%20%20The%20diffusion%20model%20has%20shown%20exceptional%20capabilities%20in%20controlled%20image%0Ageneration%2C%20which%20has%20further%20fueled%20interest%20in%20image%20style%20transfer.%20Existing%0Aworks%20mainly%20focus%20on%20training%20free-based%20methods%20%28e.g.%2C%20image%20inversion%29%20due%0Ato%20the%20scarcity%20of%20specific%20data.%20In%20this%20study%2C%20we%20present%20a%20data%20construction%0Apipeline%20for%20content-style-stylized%20image%20triplets%20that%20generates%20and%0Aautomatically%20cleanses%20stylized%20data%20triplets.%20Based%20on%20this%20pipeline%2C%20we%0Aconstruct%20a%20dataset%20IMAGStyle%2C%20the%20first%20large-scale%20style%20transfer%20dataset%0Acontaining%20210k%20image%20triplets%2C%20available%20for%20the%20community%20to%20explore%20and%0Aresearch.%20Equipped%20with%20IMAGStyle%2C%20we%20propose%20CSGO%2C%20a%20style%20transfer%20model%0Abased%20on%20end-to-end%20training%2C%20which%20explicitly%20decouples%20content%20and%20style%0Afeatures%20employing%20independent%20feature%20injection.%20The%20unified%20CSGO%20implements%0Aimage-driven%20style%20transfer%2C%20text-driven%20stylized%20synthesis%2C%20and%20text%0Aediting-driven%20stylized%20synthesis.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20enhancing%20style%20control%20capabilities%20in%20image%0Ageneration.%20Additional%20visualization%20and%20access%20to%20the%20source%20code%20can%20be%0Alocated%20on%20the%20project%20page%3A%20%5Curl%7Bhttps%3A//csgo-gen.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSGO%253A%2520Content-Style%2520Composition%2520in%2520Text-to-Image%2520Generation%26entry.906535625%3DPeng%2520Xing%2520and%2520Haofan%2520Wang%2520and%2520Yanpeng%2520Sun%2520and%2520Qixun%2520Wang%2520and%2520Xu%2520Bai%2520and%2520Hao%2520Ai%2520and%2520Renyuan%2520Huang%2520and%2520Zechao%2520Li%26entry.1292438233%3D%2520%2520The%2520diffusion%2520model%2520has%2520shown%2520exceptional%2520capabilities%2520in%2520controlled%2520image%250Ageneration%252C%2520which%2520has%2520further%2520fueled%2520interest%2520in%2520image%2520style%2520transfer.%2520Existing%250Aworks%2520mainly%2520focus%2520on%2520training%2520free-based%2520methods%2520%2528e.g.%252C%2520image%2520inversion%2529%2520due%250Ato%2520the%2520scarcity%2520of%2520specific%2520data.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520data%2520construction%250Apipeline%2520for%2520content-style-stylized%2520image%2520triplets%2520that%2520generates%2520and%250Aautomatically%2520cleanses%2520stylized%2520data%2520triplets.%2520Based%2520on%2520this%2520pipeline%252C%2520we%250Aconstruct%2520a%2520dataset%2520IMAGStyle%252C%2520the%2520first%2520large-scale%2520style%2520transfer%2520dataset%250Acontaining%2520210k%2520image%2520triplets%252C%2520available%2520for%2520the%2520community%2520to%2520explore%2520and%250Aresearch.%2520Equipped%2520with%2520IMAGStyle%252C%2520we%2520propose%2520CSGO%252C%2520a%2520style%2520transfer%2520model%250Abased%2520on%2520end-to-end%2520training%252C%2520which%2520explicitly%2520decouples%2520content%2520and%2520style%250Afeatures%2520employing%2520independent%2520feature%2520injection.%2520The%2520unified%2520CSGO%2520implements%250Aimage-driven%2520style%2520transfer%252C%2520text-driven%2520stylized%2520synthesis%252C%2520and%2520text%250Aediting-driven%2520stylized%2520synthesis.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520enhancing%2520style%2520control%2520capabilities%2520in%2520image%250Ageneration.%2520Additional%2520visualization%2520and%2520access%2520to%2520the%2520source%2520code%2520can%2520be%250Alocated%2520on%2520the%2520project%2520page%253A%2520%255Curl%257Bhttps%253A//csgo-gen.github.io/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSGO%3A%20Content-Style%20Composition%20in%20Text-to-Image%20Generation&entry.906535625=Peng%20Xing%20and%20Haofan%20Wang%20and%20Yanpeng%20Sun%20and%20Qixun%20Wang%20and%20Xu%20Bai%20and%20Hao%20Ai%20and%20Renyuan%20Huang%20and%20Zechao%20Li&entry.1292438233=%20%20The%20diffusion%20model%20has%20shown%20exceptional%20capabilities%20in%20controlled%20image%0Ageneration%2C%20which%20has%20further%20fueled%20interest%20in%20image%20style%20transfer.%20Existing%0Aworks%20mainly%20focus%20on%20training%20free-based%20methods%20%28e.g.%2C%20image%20inversion%29%20due%0Ato%20the%20scarcity%20of%20specific%20data.%20In%20this%20study%2C%20we%20present%20a%20data%20construction%0Apipeline%20for%20content-style-stylized%20image%20triplets%20that%20generates%20and%0Aautomatically%20cleanses%20stylized%20data%20triplets.%20Based%20on%20this%20pipeline%2C%20we%0Aconstruct%20a%20dataset%20IMAGStyle%2C%20the%20first%20large-scale%20style%20transfer%20dataset%0Acontaining%20210k%20image%20triplets%2C%20available%20for%20the%20community%20to%20explore%20and%0Aresearch.%20Equipped%20with%20IMAGStyle%2C%20we%20propose%20CSGO%2C%20a%20style%20transfer%20model%0Abased%20on%20end-to-end%20training%2C%20which%20explicitly%20decouples%20content%20and%20style%0Afeatures%20employing%20independent%20feature%20injection.%20The%20unified%20CSGO%20implements%0Aimage-driven%20style%20transfer%2C%20text-driven%20stylized%20synthesis%2C%20and%20text%0Aediting-driven%20stylized%20synthesis.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20enhancing%20style%20control%20capabilities%20in%20image%0Ageneration.%20Additional%20visualization%20and%20access%20to%20the%20source%20code%20can%20be%0Alocated%20on%20the%20project%20page%3A%20%5Curl%7Bhttps%3A//csgo-gen.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16766v1&entry.124074799=Read"},
{"title": "An Adaptive Latent Factorization of Tensors Model for Embedding Dynamic\n  Communication Network", "author": "Xin Liao and Qicong Hu and Peng Tang", "abstract": "  The Dynamic Communication Network (DCN) describes the interactions over time\namong various communication nodes, and it is widely used in Big-data\napplications as a data source. As the number of communication nodes increases\nand temporal slots accumulate, each node interacts in with only a few nodes in\na given temporal slot, the DCN can be represented by an High-Dimensional Sparse\n(HDS) tensor. In order to extract rich behavioral patterns from an HDS tensor\nin DCN, this paper proposes an Adaptive Temporal-dependent Tensor low-rank\nrepresentation (ATT) model. It adopts a three-fold approach: a) designing a\ntemporal-dependent method to reconstruct temporal feature matrix, thereby\nprecisely represent the data by capturing the temporal patterns; b) achieving\nhyper-parameters adaptation of the model via the Differential Evolutionary\nAlgorithms (DEA) to avoid tedious hyper-parameters tuning; c) employing\nnonnegative learning schemes for the model parameters to effectively handle an\nthe nonnegativity inherent in HDS data. The experimental results on four\nreal-world DCNs demonstrate that the proposed ATT model significantly\noutperforms several state-of-the-art models in both prediction errors and\nconvergence rounds.\n", "link": "http://arxiv.org/abs/2408.16573v1", "date": "2024-08-29", "relevancy": 2.1524, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.563}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5542}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Latent%20Factorization%20of%20Tensors%20Model%20for%20Embedding%20Dynamic%0A%20%20Communication%20Network&body=Title%3A%20An%20Adaptive%20Latent%20Factorization%20of%20Tensors%20Model%20for%20Embedding%20Dynamic%0A%20%20Communication%20Network%0AAuthor%3A%20Xin%20Liao%20and%20Qicong%20Hu%20and%20Peng%20Tang%0AAbstract%3A%20%20%20The%20Dynamic%20Communication%20Network%20%28DCN%29%20describes%20the%20interactions%20over%20time%0Aamong%20various%20communication%20nodes%2C%20and%20it%20is%20widely%20used%20in%20Big-data%0Aapplications%20as%20a%20data%20source.%20As%20the%20number%20of%20communication%20nodes%20increases%0Aand%20temporal%20slots%20accumulate%2C%20each%20node%20interacts%20in%20with%20only%20a%20few%20nodes%20in%0Aa%20given%20temporal%20slot%2C%20the%20DCN%20can%20be%20represented%20by%20an%20High-Dimensional%20Sparse%0A%28HDS%29%20tensor.%20In%20order%20to%20extract%20rich%20behavioral%20patterns%20from%20an%20HDS%20tensor%0Ain%20DCN%2C%20this%20paper%20proposes%20an%20Adaptive%20Temporal-dependent%20Tensor%20low-rank%0Arepresentation%20%28ATT%29%20model.%20It%20adopts%20a%20three-fold%20approach%3A%20a%29%20designing%20a%0Atemporal-dependent%20method%20to%20reconstruct%20temporal%20feature%20matrix%2C%20thereby%0Aprecisely%20represent%20the%20data%20by%20capturing%20the%20temporal%20patterns%3B%20b%29%20achieving%0Ahyper-parameters%20adaptation%20of%20the%20model%20via%20the%20Differential%20Evolutionary%0AAlgorithms%20%28DEA%29%20to%20avoid%20tedious%20hyper-parameters%20tuning%3B%20c%29%20employing%0Anonnegative%20learning%20schemes%20for%20the%20model%20parameters%20to%20effectively%20handle%20an%0Athe%20nonnegativity%20inherent%20in%20HDS%20data.%20The%20experimental%20results%20on%20four%0Areal-world%20DCNs%20demonstrate%20that%20the%20proposed%20ATT%20model%20significantly%0Aoutperforms%20several%20state-of-the-art%20models%20in%20both%20prediction%20errors%20and%0Aconvergence%20rounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Latent%2520Factorization%2520of%2520Tensors%2520Model%2520for%2520Embedding%2520Dynamic%250A%2520%2520Communication%2520Network%26entry.906535625%3DXin%2520Liao%2520and%2520Qicong%2520Hu%2520and%2520Peng%2520Tang%26entry.1292438233%3D%2520%2520The%2520Dynamic%2520Communication%2520Network%2520%2528DCN%2529%2520describes%2520the%2520interactions%2520over%2520time%250Aamong%2520various%2520communication%2520nodes%252C%2520and%2520it%2520is%2520widely%2520used%2520in%2520Big-data%250Aapplications%2520as%2520a%2520data%2520source.%2520As%2520the%2520number%2520of%2520communication%2520nodes%2520increases%250Aand%2520temporal%2520slots%2520accumulate%252C%2520each%2520node%2520interacts%2520in%2520with%2520only%2520a%2520few%2520nodes%2520in%250Aa%2520given%2520temporal%2520slot%252C%2520the%2520DCN%2520can%2520be%2520represented%2520by%2520an%2520High-Dimensional%2520Sparse%250A%2528HDS%2529%2520tensor.%2520In%2520order%2520to%2520extract%2520rich%2520behavioral%2520patterns%2520from%2520an%2520HDS%2520tensor%250Ain%2520DCN%252C%2520this%2520paper%2520proposes%2520an%2520Adaptive%2520Temporal-dependent%2520Tensor%2520low-rank%250Arepresentation%2520%2528ATT%2529%2520model.%2520It%2520adopts%2520a%2520three-fold%2520approach%253A%2520a%2529%2520designing%2520a%250Atemporal-dependent%2520method%2520to%2520reconstruct%2520temporal%2520feature%2520matrix%252C%2520thereby%250Aprecisely%2520represent%2520the%2520data%2520by%2520capturing%2520the%2520temporal%2520patterns%253B%2520b%2529%2520achieving%250Ahyper-parameters%2520adaptation%2520of%2520the%2520model%2520via%2520the%2520Differential%2520Evolutionary%250AAlgorithms%2520%2528DEA%2529%2520to%2520avoid%2520tedious%2520hyper-parameters%2520tuning%253B%2520c%2529%2520employing%250Anonnegative%2520learning%2520schemes%2520for%2520the%2520model%2520parameters%2520to%2520effectively%2520handle%2520an%250Athe%2520nonnegativity%2520inherent%2520in%2520HDS%2520data.%2520The%2520experimental%2520results%2520on%2520four%250Areal-world%2520DCNs%2520demonstrate%2520that%2520the%2520proposed%2520ATT%2520model%2520significantly%250Aoutperforms%2520several%2520state-of-the-art%2520models%2520in%2520both%2520prediction%2520errors%2520and%250Aconvergence%2520rounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Latent%20Factorization%20of%20Tensors%20Model%20for%20Embedding%20Dynamic%0A%20%20Communication%20Network&entry.906535625=Xin%20Liao%20and%20Qicong%20Hu%20and%20Peng%20Tang&entry.1292438233=%20%20The%20Dynamic%20Communication%20Network%20%28DCN%29%20describes%20the%20interactions%20over%20time%0Aamong%20various%20communication%20nodes%2C%20and%20it%20is%20widely%20used%20in%20Big-data%0Aapplications%20as%20a%20data%20source.%20As%20the%20number%20of%20communication%20nodes%20increases%0Aand%20temporal%20slots%20accumulate%2C%20each%20node%20interacts%20in%20with%20only%20a%20few%20nodes%20in%0Aa%20given%20temporal%20slot%2C%20the%20DCN%20can%20be%20represented%20by%20an%20High-Dimensional%20Sparse%0A%28HDS%29%20tensor.%20In%20order%20to%20extract%20rich%20behavioral%20patterns%20from%20an%20HDS%20tensor%0Ain%20DCN%2C%20this%20paper%20proposes%20an%20Adaptive%20Temporal-dependent%20Tensor%20low-rank%0Arepresentation%20%28ATT%29%20model.%20It%20adopts%20a%20three-fold%20approach%3A%20a%29%20designing%20a%0Atemporal-dependent%20method%20to%20reconstruct%20temporal%20feature%20matrix%2C%20thereby%0Aprecisely%20represent%20the%20data%20by%20capturing%20the%20temporal%20patterns%3B%20b%29%20achieving%0Ahyper-parameters%20adaptation%20of%20the%20model%20via%20the%20Differential%20Evolutionary%0AAlgorithms%20%28DEA%29%20to%20avoid%20tedious%20hyper-parameters%20tuning%3B%20c%29%20employing%0Anonnegative%20learning%20schemes%20for%20the%20model%20parameters%20to%20effectively%20handle%20an%0Athe%20nonnegativity%20inherent%20in%20HDS%20data.%20The%20experimental%20results%20on%20four%0Areal-world%20DCNs%20demonstrate%20that%20the%20proposed%20ATT%20model%20significantly%0Aoutperforms%20several%20state-of-the-art%20models%20in%20both%20prediction%20errors%20and%0Aconvergence%20rounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16573v1&entry.124074799=Read"},
{"title": "OpticalRS-4M: Scaling Efficient Masked Autoencoder Learning on Large\n  Remote Sensing Dataset", "author": "Fengxiang Wang and Hongzhen Wang and Di Wang and Zonghao Guo and Zhenyu Zhong and Long Lan and Jing Zhang and Zhiyuan Liu and Maosong Sun", "abstract": "  Masked Image Modeling (MIM) has become an essential method for building\nfoundational visual models in remote sensing (RS). However, the limitations in\nsize and diversity of existing RS datasets restrict the ability of MIM methods\nto learn generalizable representations. Additionally, conventional MIM\ntechniques, which require reconstructing all tokens, introduce unnecessary\ncomputational overhead. To address these issues, we present a new pre-training\npipeline for RS models, featuring the creation of a large-scale RS dataset and\nan efficient MIM approach. We curated a high-quality dataset named OpticalRS-4M\nby collecting publicly available RS datasets and processing them through\nexclusion, slicing, and deduplication. OpticalRS-4M comprises 4 million optical\nimages covering various RS tasks, such as object detection and pixel\nsegmentation. To enhance efficiency, we propose SelectiveMAE, a pre-training\nmethod that dynamically encodes and reconstructs semantically rich patch\ntokens, thereby reducing the inefficiencies of traditional MIM models caused by\nredundant background pixels in RS images. Extensive experiments demonstrate\nthat OpticalRS-4M significantly improves classification, detection, and\nsegmentation performance, while SelectiveMAE increases training efficiency over\n2 times. This highlights the effectiveness and scalability of our pipeline in\ndeveloping RS foundational models.\n", "link": "http://arxiv.org/abs/2406.11933v2", "date": "2024-08-29", "relevancy": 2.1498, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5387}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5381}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpticalRS-4M%3A%20Scaling%20Efficient%20Masked%20Autoencoder%20Learning%20on%20Large%0A%20%20Remote%20Sensing%20Dataset&body=Title%3A%20OpticalRS-4M%3A%20Scaling%20Efficient%20Masked%20Autoencoder%20Learning%20on%20Large%0A%20%20Remote%20Sensing%20Dataset%0AAuthor%3A%20Fengxiang%20Wang%20and%20Hongzhen%20Wang%20and%20Di%20Wang%20and%20Zonghao%20Guo%20and%20Zhenyu%20Zhong%20and%20Long%20Lan%20and%20Jing%20Zhang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20become%20an%20essential%20method%20for%20building%0Afoundational%20visual%20models%20in%20remote%20sensing%20%28RS%29.%20However%2C%20the%20limitations%20in%0Asize%20and%20diversity%20of%20existing%20RS%20datasets%20restrict%20the%20ability%20of%20MIM%20methods%0Ato%20learn%20generalizable%20representations.%20Additionally%2C%20conventional%20MIM%0Atechniques%2C%20which%20require%20reconstructing%20all%20tokens%2C%20introduce%20unnecessary%0Acomputational%20overhead.%20To%20address%20these%20issues%2C%20we%20present%20a%20new%20pre-training%0Apipeline%20for%20RS%20models%2C%20featuring%20the%20creation%20of%20a%20large-scale%20RS%20dataset%20and%0Aan%20efficient%20MIM%20approach.%20We%20curated%20a%20high-quality%20dataset%20named%20OpticalRS-4M%0Aby%20collecting%20publicly%20available%20RS%20datasets%20and%20processing%20them%20through%0Aexclusion%2C%20slicing%2C%20and%20deduplication.%20OpticalRS-4M%20comprises%204%20million%20optical%0Aimages%20covering%20various%20RS%20tasks%2C%20such%20as%20object%20detection%20and%20pixel%0Asegmentation.%20To%20enhance%20efficiency%2C%20we%20propose%20SelectiveMAE%2C%20a%20pre-training%0Amethod%20that%20dynamically%20encodes%20and%20reconstructs%20semantically%20rich%20patch%0Atokens%2C%20thereby%20reducing%20the%20inefficiencies%20of%20traditional%20MIM%20models%20caused%20by%0Aredundant%20background%20pixels%20in%20RS%20images.%20Extensive%20experiments%20demonstrate%0Athat%20OpticalRS-4M%20significantly%20improves%20classification%2C%20detection%2C%20and%0Asegmentation%20performance%2C%20while%20SelectiveMAE%20increases%20training%20efficiency%20over%0A2%20times.%20This%20highlights%20the%20effectiveness%20and%20scalability%20of%20our%20pipeline%20in%0Adeveloping%20RS%20foundational%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11933v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpticalRS-4M%253A%2520Scaling%2520Efficient%2520Masked%2520Autoencoder%2520Learning%2520on%2520Large%250A%2520%2520Remote%2520Sensing%2520Dataset%26entry.906535625%3DFengxiang%2520Wang%2520and%2520Hongzhen%2520Wang%2520and%2520Di%2520Wang%2520and%2520Zonghao%2520Guo%2520and%2520Zhenyu%2520Zhong%2520and%2520Long%2520Lan%2520and%2520Jing%2520Zhang%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%2520has%2520become%2520an%2520essential%2520method%2520for%2520building%250Afoundational%2520visual%2520models%2520in%2520remote%2520sensing%2520%2528RS%2529.%2520However%252C%2520the%2520limitations%2520in%250Asize%2520and%2520diversity%2520of%2520existing%2520RS%2520datasets%2520restrict%2520the%2520ability%2520of%2520MIM%2520methods%250Ato%2520learn%2520generalizable%2520representations.%2520Additionally%252C%2520conventional%2520MIM%250Atechniques%252C%2520which%2520require%2520reconstructing%2520all%2520tokens%252C%2520introduce%2520unnecessary%250Acomputational%2520overhead.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520a%2520new%2520pre-training%250Apipeline%2520for%2520RS%2520models%252C%2520featuring%2520the%2520creation%2520of%2520a%2520large-scale%2520RS%2520dataset%2520and%250Aan%2520efficient%2520MIM%2520approach.%2520We%2520curated%2520a%2520high-quality%2520dataset%2520named%2520OpticalRS-4M%250Aby%2520collecting%2520publicly%2520available%2520RS%2520datasets%2520and%2520processing%2520them%2520through%250Aexclusion%252C%2520slicing%252C%2520and%2520deduplication.%2520OpticalRS-4M%2520comprises%25204%2520million%2520optical%250Aimages%2520covering%2520various%2520RS%2520tasks%252C%2520such%2520as%2520object%2520detection%2520and%2520pixel%250Asegmentation.%2520To%2520enhance%2520efficiency%252C%2520we%2520propose%2520SelectiveMAE%252C%2520a%2520pre-training%250Amethod%2520that%2520dynamically%2520encodes%2520and%2520reconstructs%2520semantically%2520rich%2520patch%250Atokens%252C%2520thereby%2520reducing%2520the%2520inefficiencies%2520of%2520traditional%2520MIM%2520models%2520caused%2520by%250Aredundant%2520background%2520pixels%2520in%2520RS%2520images.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520OpticalRS-4M%2520significantly%2520improves%2520classification%252C%2520detection%252C%2520and%250Asegmentation%2520performance%252C%2520while%2520SelectiveMAE%2520increases%2520training%2520efficiency%2520over%250A2%2520times.%2520This%2520highlights%2520the%2520effectiveness%2520and%2520scalability%2520of%2520our%2520pipeline%2520in%250Adeveloping%2520RS%2520foundational%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11933v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpticalRS-4M%3A%20Scaling%20Efficient%20Masked%20Autoencoder%20Learning%20on%20Large%0A%20%20Remote%20Sensing%20Dataset&entry.906535625=Fengxiang%20Wang%20and%20Hongzhen%20Wang%20and%20Di%20Wang%20and%20Zonghao%20Guo%20and%20Zhenyu%20Zhong%20and%20Long%20Lan%20and%20Jing%20Zhang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20become%20an%20essential%20method%20for%20building%0Afoundational%20visual%20models%20in%20remote%20sensing%20%28RS%29.%20However%2C%20the%20limitations%20in%0Asize%20and%20diversity%20of%20existing%20RS%20datasets%20restrict%20the%20ability%20of%20MIM%20methods%0Ato%20learn%20generalizable%20representations.%20Additionally%2C%20conventional%20MIM%0Atechniques%2C%20which%20require%20reconstructing%20all%20tokens%2C%20introduce%20unnecessary%0Acomputational%20overhead.%20To%20address%20these%20issues%2C%20we%20present%20a%20new%20pre-training%0Apipeline%20for%20RS%20models%2C%20featuring%20the%20creation%20of%20a%20large-scale%20RS%20dataset%20and%0Aan%20efficient%20MIM%20approach.%20We%20curated%20a%20high-quality%20dataset%20named%20OpticalRS-4M%0Aby%20collecting%20publicly%20available%20RS%20datasets%20and%20processing%20them%20through%0Aexclusion%2C%20slicing%2C%20and%20deduplication.%20OpticalRS-4M%20comprises%204%20million%20optical%0Aimages%20covering%20various%20RS%20tasks%2C%20such%20as%20object%20detection%20and%20pixel%0Asegmentation.%20To%20enhance%20efficiency%2C%20we%20propose%20SelectiveMAE%2C%20a%20pre-training%0Amethod%20that%20dynamically%20encodes%20and%20reconstructs%20semantically%20rich%20patch%0Atokens%2C%20thereby%20reducing%20the%20inefficiencies%20of%20traditional%20MIM%20models%20caused%20by%0Aredundant%20background%20pixels%20in%20RS%20images.%20Extensive%20experiments%20demonstrate%0Athat%20OpticalRS-4M%20significantly%20improves%20classification%2C%20detection%2C%20and%0Asegmentation%20performance%2C%20while%20SelectiveMAE%20increases%20training%20efficiency%20over%0A2%20times.%20This%20highlights%20the%20effectiveness%20and%20scalability%20of%20our%20pipeline%20in%0Adeveloping%20RS%20foundational%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11933v2&entry.124074799=Read"},
{"title": "Super-Resolution works for coastal simulations", "author": "Zhi-Song Liu and Markus Buttner and Vadym Aizinger and Andreas Rupp", "abstract": "  Learning fine-scale details of a coastal ocean simulation from a coarse\nrepresentation is a challenging task. For real-world applications,\nhigh-resolution simulations are necessary to advance understanding of many\ncoastal processes, specifically, to predict flooding resulting from tsunamis\nand storm surges. We propose a Deep Network for Coastal Super-Resolution\n(DNCSR) for spatiotemporal enhancement to efficiently learn the high-resolution\nnumerical solution. Given images of coastal simulations produced on\nlow-resolution computational meshes using low polynomial order discontinuous\nGalerkin discretizations and a coarse temporal resolution, the proposed DNCSR\nlearns to produce high-resolution free surface elevation and velocity\nvisualizations in both time and space. To efficiently model the dynamic changes\nover time and space, we propose grid-aware spatiotemporal attention to project\nthe temporal features to the spatial domain for non-local feature matching. The\ncoordinate information is also utilized via positional encoding. For the final\nreconstruction, we use the spatiotemporal bilinear operation to interpolate the\nmissing frames and then expand the feature maps to the frequency domain for\nresidual mapping. Besides data-driven losses, the proposed physics-informed\nloss guarantees gradient consistency and momentum changes. Their combination\ncontributes to the overall 24% improvements in RMSE. To train the proposed\nmodel, we propose a large-scale coastal simulation dataset and use it for model\noptimization and evaluation. Our method shows superior super-resolution quality\nand fast computation compared to the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2408.16553v1", "date": "2024-08-29", "relevancy": 2.1391, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5434}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5322}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-Resolution%20works%20for%20coastal%20simulations&body=Title%3A%20Super-Resolution%20works%20for%20coastal%20simulations%0AAuthor%3A%20Zhi-Song%20Liu%20and%20Markus%20Buttner%20and%20Vadym%20Aizinger%20and%20Andreas%20Rupp%0AAbstract%3A%20%20%20Learning%20fine-scale%20details%20of%20a%20coastal%20ocean%20simulation%20from%20a%20coarse%0Arepresentation%20is%20a%20challenging%20task.%20For%20real-world%20applications%2C%0Ahigh-resolution%20simulations%20are%20necessary%20to%20advance%20understanding%20of%20many%0Acoastal%20processes%2C%20specifically%2C%20to%20predict%20flooding%20resulting%20from%20tsunamis%0Aand%20storm%20surges.%20We%20propose%20a%20Deep%20Network%20for%20Coastal%20Super-Resolution%0A%28DNCSR%29%20for%20spatiotemporal%20enhancement%20to%20efficiently%20learn%20the%20high-resolution%0Anumerical%20solution.%20Given%20images%20of%20coastal%20simulations%20produced%20on%0Alow-resolution%20computational%20meshes%20using%20low%20polynomial%20order%20discontinuous%0AGalerkin%20discretizations%20and%20a%20coarse%20temporal%20resolution%2C%20the%20proposed%20DNCSR%0Alearns%20to%20produce%20high-resolution%20free%20surface%20elevation%20and%20velocity%0Avisualizations%20in%20both%20time%20and%20space.%20To%20efficiently%20model%20the%20dynamic%20changes%0Aover%20time%20and%20space%2C%20we%20propose%20grid-aware%20spatiotemporal%20attention%20to%20project%0Athe%20temporal%20features%20to%20the%20spatial%20domain%20for%20non-local%20feature%20matching.%20The%0Acoordinate%20information%20is%20also%20utilized%20via%20positional%20encoding.%20For%20the%20final%0Areconstruction%2C%20we%20use%20the%20spatiotemporal%20bilinear%20operation%20to%20interpolate%20the%0Amissing%20frames%20and%20then%20expand%20the%20feature%20maps%20to%20the%20frequency%20domain%20for%0Aresidual%20mapping.%20Besides%20data-driven%20losses%2C%20the%20proposed%20physics-informed%0Aloss%20guarantees%20gradient%20consistency%20and%20momentum%20changes.%20Their%20combination%0Acontributes%20to%20the%20overall%2024%25%20improvements%20in%20RMSE.%20To%20train%20the%20proposed%0Amodel%2C%20we%20propose%20a%20large-scale%20coastal%20simulation%20dataset%20and%20use%20it%20for%20model%0Aoptimization%20and%20evaluation.%20Our%20method%20shows%20superior%20super-resolution%20quality%0Aand%20fast%20computation%20compared%20to%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-Resolution%2520works%2520for%2520coastal%2520simulations%26entry.906535625%3DZhi-Song%2520Liu%2520and%2520Markus%2520Buttner%2520and%2520Vadym%2520Aizinger%2520and%2520Andreas%2520Rupp%26entry.1292438233%3D%2520%2520Learning%2520fine-scale%2520details%2520of%2520a%2520coastal%2520ocean%2520simulation%2520from%2520a%2520coarse%250Arepresentation%2520is%2520a%2520challenging%2520task.%2520For%2520real-world%2520applications%252C%250Ahigh-resolution%2520simulations%2520are%2520necessary%2520to%2520advance%2520understanding%2520of%2520many%250Acoastal%2520processes%252C%2520specifically%252C%2520to%2520predict%2520flooding%2520resulting%2520from%2520tsunamis%250Aand%2520storm%2520surges.%2520We%2520propose%2520a%2520Deep%2520Network%2520for%2520Coastal%2520Super-Resolution%250A%2528DNCSR%2529%2520for%2520spatiotemporal%2520enhancement%2520to%2520efficiently%2520learn%2520the%2520high-resolution%250Anumerical%2520solution.%2520Given%2520images%2520of%2520coastal%2520simulations%2520produced%2520on%250Alow-resolution%2520computational%2520meshes%2520using%2520low%2520polynomial%2520order%2520discontinuous%250AGalerkin%2520discretizations%2520and%2520a%2520coarse%2520temporal%2520resolution%252C%2520the%2520proposed%2520DNCSR%250Alearns%2520to%2520produce%2520high-resolution%2520free%2520surface%2520elevation%2520and%2520velocity%250Avisualizations%2520in%2520both%2520time%2520and%2520space.%2520To%2520efficiently%2520model%2520the%2520dynamic%2520changes%250Aover%2520time%2520and%2520space%252C%2520we%2520propose%2520grid-aware%2520spatiotemporal%2520attention%2520to%2520project%250Athe%2520temporal%2520features%2520to%2520the%2520spatial%2520domain%2520for%2520non-local%2520feature%2520matching.%2520The%250Acoordinate%2520information%2520is%2520also%2520utilized%2520via%2520positional%2520encoding.%2520For%2520the%2520final%250Areconstruction%252C%2520we%2520use%2520the%2520spatiotemporal%2520bilinear%2520operation%2520to%2520interpolate%2520the%250Amissing%2520frames%2520and%2520then%2520expand%2520the%2520feature%2520maps%2520to%2520the%2520frequency%2520domain%2520for%250Aresidual%2520mapping.%2520Besides%2520data-driven%2520losses%252C%2520the%2520proposed%2520physics-informed%250Aloss%2520guarantees%2520gradient%2520consistency%2520and%2520momentum%2520changes.%2520Their%2520combination%250Acontributes%2520to%2520the%2520overall%252024%2525%2520improvements%2520in%2520RMSE.%2520To%2520train%2520the%2520proposed%250Amodel%252C%2520we%2520propose%2520a%2520large-scale%2520coastal%2520simulation%2520dataset%2520and%2520use%2520it%2520for%2520model%250Aoptimization%2520and%2520evaluation.%2520Our%2520method%2520shows%2520superior%2520super-resolution%2520quality%250Aand%2520fast%2520computation%2520compared%2520to%2520the%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-Resolution%20works%20for%20coastal%20simulations&entry.906535625=Zhi-Song%20Liu%20and%20Markus%20Buttner%20and%20Vadym%20Aizinger%20and%20Andreas%20Rupp&entry.1292438233=%20%20Learning%20fine-scale%20details%20of%20a%20coastal%20ocean%20simulation%20from%20a%20coarse%0Arepresentation%20is%20a%20challenging%20task.%20For%20real-world%20applications%2C%0Ahigh-resolution%20simulations%20are%20necessary%20to%20advance%20understanding%20of%20many%0Acoastal%20processes%2C%20specifically%2C%20to%20predict%20flooding%20resulting%20from%20tsunamis%0Aand%20storm%20surges.%20We%20propose%20a%20Deep%20Network%20for%20Coastal%20Super-Resolution%0A%28DNCSR%29%20for%20spatiotemporal%20enhancement%20to%20efficiently%20learn%20the%20high-resolution%0Anumerical%20solution.%20Given%20images%20of%20coastal%20simulations%20produced%20on%0Alow-resolution%20computational%20meshes%20using%20low%20polynomial%20order%20discontinuous%0AGalerkin%20discretizations%20and%20a%20coarse%20temporal%20resolution%2C%20the%20proposed%20DNCSR%0Alearns%20to%20produce%20high-resolution%20free%20surface%20elevation%20and%20velocity%0Avisualizations%20in%20both%20time%20and%20space.%20To%20efficiently%20model%20the%20dynamic%20changes%0Aover%20time%20and%20space%2C%20we%20propose%20grid-aware%20spatiotemporal%20attention%20to%20project%0Athe%20temporal%20features%20to%20the%20spatial%20domain%20for%20non-local%20feature%20matching.%20The%0Acoordinate%20information%20is%20also%20utilized%20via%20positional%20encoding.%20For%20the%20final%0Areconstruction%2C%20we%20use%20the%20spatiotemporal%20bilinear%20operation%20to%20interpolate%20the%0Amissing%20frames%20and%20then%20expand%20the%20feature%20maps%20to%20the%20frequency%20domain%20for%0Aresidual%20mapping.%20Besides%20data-driven%20losses%2C%20the%20proposed%20physics-informed%0Aloss%20guarantees%20gradient%20consistency%20and%20momentum%20changes.%20Their%20combination%0Acontributes%20to%20the%20overall%2024%25%20improvements%20in%20RMSE.%20To%20train%20the%20proposed%0Amodel%2C%20we%20propose%20a%20large-scale%20coastal%20simulation%20dataset%20and%20use%20it%20for%20model%0Aoptimization%20and%20evaluation.%20Our%20method%20shows%20superior%20super-resolution%20quality%0Aand%20fast%20computation%20compared%20to%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16553v1&entry.124074799=Read"},
{"title": "Learning to Prompt Your Domain for Vision-Language Models", "author": "Guoyizhe Wei and Feng Wang and Anshul Shah and Rama Chellappa", "abstract": "  Prompt learning has recently become a very efficient transfer learning\nparadigm for Contrastive Language Image Pretraining (CLIP) models. Compared\nwith fine-tuning the entire encoder, prompt learning can obtain highly\ncompetitive results by optimizing only a small number of parameters, which\npresents considerably exciting benefits for federated learning applications\nthat prioritizes communication efficiency. However, in this work, we identify\nthat directly transferring prompt learning approaches into federated learning\ndoes not yield favorable results since the model often suffers from\nconsiderable domain gaps across different clients. To address this issue, we\npropose ADAPT, a novel domain-aware prompt learning approach that facilitates\nboth intra- and inter-domain prompts across federated participants. The basic\nidea of ADAPT is that the prompted CLIP should detect the input image's domain\ncorrespondence and before making the prediction of its category. Extensive\nexperiments of ADAPT demonstrate its significant efficiency and effectiveness\nin federated learning. For example, by learning and sharing only 0.08M\nparameters, our ADAPT attains a 68.4% average accuracy over six domains in the\nDomainNet dataset, which improves the original CLIP by a large margin of 14.8%.\n", "link": "http://arxiv.org/abs/2310.03103v5", "date": "2024-08-29", "relevancy": 2.1376, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5709}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5134}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Prompt%20Your%20Domain%20for%20Vision-Language%20Models&body=Title%3A%20Learning%20to%20Prompt%20Your%20Domain%20for%20Vision-Language%20Models%0AAuthor%3A%20Guoyizhe%20Wei%20and%20Feng%20Wang%20and%20Anshul%20Shah%20and%20Rama%20Chellappa%0AAbstract%3A%20%20%20Prompt%20learning%20has%20recently%20become%20a%20very%20efficient%20transfer%20learning%0Aparadigm%20for%20Contrastive%20Language%20Image%20Pretraining%20%28CLIP%29%20models.%20Compared%0Awith%20fine-tuning%20the%20entire%20encoder%2C%20prompt%20learning%20can%20obtain%20highly%0Acompetitive%20results%20by%20optimizing%20only%20a%20small%20number%20of%20parameters%2C%20which%0Apresents%20considerably%20exciting%20benefits%20for%20federated%20learning%20applications%0Athat%20prioritizes%20communication%20efficiency.%20However%2C%20in%20this%20work%2C%20we%20identify%0Athat%20directly%20transferring%20prompt%20learning%20approaches%20into%20federated%20learning%0Adoes%20not%20yield%20favorable%20results%20since%20the%20model%20often%20suffers%20from%0Aconsiderable%20domain%20gaps%20across%20different%20clients.%20To%20address%20this%20issue%2C%20we%0Apropose%20ADAPT%2C%20a%20novel%20domain-aware%20prompt%20learning%20approach%20that%20facilitates%0Aboth%20intra-%20and%20inter-domain%20prompts%20across%20federated%20participants.%20The%20basic%0Aidea%20of%20ADAPT%20is%20that%20the%20prompted%20CLIP%20should%20detect%20the%20input%20image%27s%20domain%0Acorrespondence%20and%20before%20making%20the%20prediction%20of%20its%20category.%20Extensive%0Aexperiments%20of%20ADAPT%20demonstrate%20its%20significant%20efficiency%20and%20effectiveness%0Ain%20federated%20learning.%20For%20example%2C%20by%20learning%20and%20sharing%20only%200.08M%0Aparameters%2C%20our%20ADAPT%20attains%20a%2068.4%25%20average%20accuracy%20over%20six%20domains%20in%20the%0ADomainNet%20dataset%2C%20which%20improves%20the%20original%20CLIP%20by%20a%20large%20margin%20of%2014.8%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03103v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Prompt%2520Your%2520Domain%2520for%2520Vision-Language%2520Models%26entry.906535625%3DGuoyizhe%2520Wei%2520and%2520Feng%2520Wang%2520and%2520Anshul%2520Shah%2520and%2520Rama%2520Chellappa%26entry.1292438233%3D%2520%2520Prompt%2520learning%2520has%2520recently%2520become%2520a%2520very%2520efficient%2520transfer%2520learning%250Aparadigm%2520for%2520Contrastive%2520Language%2520Image%2520Pretraining%2520%2528CLIP%2529%2520models.%2520Compared%250Awith%2520fine-tuning%2520the%2520entire%2520encoder%252C%2520prompt%2520learning%2520can%2520obtain%2520highly%250Acompetitive%2520results%2520by%2520optimizing%2520only%2520a%2520small%2520number%2520of%2520parameters%252C%2520which%250Apresents%2520considerably%2520exciting%2520benefits%2520for%2520federated%2520learning%2520applications%250Athat%2520prioritizes%2520communication%2520efficiency.%2520However%252C%2520in%2520this%2520work%252C%2520we%2520identify%250Athat%2520directly%2520transferring%2520prompt%2520learning%2520approaches%2520into%2520federated%2520learning%250Adoes%2520not%2520yield%2520favorable%2520results%2520since%2520the%2520model%2520often%2520suffers%2520from%250Aconsiderable%2520domain%2520gaps%2520across%2520different%2520clients.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520ADAPT%252C%2520a%2520novel%2520domain-aware%2520prompt%2520learning%2520approach%2520that%2520facilitates%250Aboth%2520intra-%2520and%2520inter-domain%2520prompts%2520across%2520federated%2520participants.%2520The%2520basic%250Aidea%2520of%2520ADAPT%2520is%2520that%2520the%2520prompted%2520CLIP%2520should%2520detect%2520the%2520input%2520image%2527s%2520domain%250Acorrespondence%2520and%2520before%2520making%2520the%2520prediction%2520of%2520its%2520category.%2520Extensive%250Aexperiments%2520of%2520ADAPT%2520demonstrate%2520its%2520significant%2520efficiency%2520and%2520effectiveness%250Ain%2520federated%2520learning.%2520For%2520example%252C%2520by%2520learning%2520and%2520sharing%2520only%25200.08M%250Aparameters%252C%2520our%2520ADAPT%2520attains%2520a%252068.4%2525%2520average%2520accuracy%2520over%2520six%2520domains%2520in%2520the%250ADomainNet%2520dataset%252C%2520which%2520improves%2520the%2520original%2520CLIP%2520by%2520a%2520large%2520margin%2520of%252014.8%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03103v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Prompt%20Your%20Domain%20for%20Vision-Language%20Models&entry.906535625=Guoyizhe%20Wei%20and%20Feng%20Wang%20and%20Anshul%20Shah%20and%20Rama%20Chellappa&entry.1292438233=%20%20Prompt%20learning%20has%20recently%20become%20a%20very%20efficient%20transfer%20learning%0Aparadigm%20for%20Contrastive%20Language%20Image%20Pretraining%20%28CLIP%29%20models.%20Compared%0Awith%20fine-tuning%20the%20entire%20encoder%2C%20prompt%20learning%20can%20obtain%20highly%0Acompetitive%20results%20by%20optimizing%20only%20a%20small%20number%20of%20parameters%2C%20which%0Apresents%20considerably%20exciting%20benefits%20for%20federated%20learning%20applications%0Athat%20prioritizes%20communication%20efficiency.%20However%2C%20in%20this%20work%2C%20we%20identify%0Athat%20directly%20transferring%20prompt%20learning%20approaches%20into%20federated%20learning%0Adoes%20not%20yield%20favorable%20results%20since%20the%20model%20often%20suffers%20from%0Aconsiderable%20domain%20gaps%20across%20different%20clients.%20To%20address%20this%20issue%2C%20we%0Apropose%20ADAPT%2C%20a%20novel%20domain-aware%20prompt%20learning%20approach%20that%20facilitates%0Aboth%20intra-%20and%20inter-domain%20prompts%20across%20federated%20participants.%20The%20basic%0Aidea%20of%20ADAPT%20is%20that%20the%20prompted%20CLIP%20should%20detect%20the%20input%20image%27s%20domain%0Acorrespondence%20and%20before%20making%20the%20prediction%20of%20its%20category.%20Extensive%0Aexperiments%20of%20ADAPT%20demonstrate%20its%20significant%20efficiency%20and%20effectiveness%0Ain%20federated%20learning.%20For%20example%2C%20by%20learning%20and%20sharing%20only%200.08M%0Aparameters%2C%20our%20ADAPT%20attains%20a%2068.4%25%20average%20accuracy%20over%20six%20domains%20in%20the%0ADomainNet%20dataset%2C%20which%20improves%20the%20original%20CLIP%20by%20a%20large%20margin%20of%2014.8%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03103v5&entry.124074799=Read"},
{"title": "Stochastic Adaptive Estimation in Polynomial Curvature Shape State Space\n  for Continuum Robots", "author": "Guoqing Zhang and Long Wang", "abstract": "  In continuum robotics, real-time robust shape estimation is crucial for\nplanning and control tasks that involve physical manipulation in complex\nenvironments. In this paper, we present a novel stochastic observer-based shape\nestimation framework designed specifically for continuum robots. The shape\nstate space is uniquely represented by the modal coefficients of a polynomial,\nenabled by leveraging polynomial curvature kinematics to describe the curvature\ndistribution along the arclength. Our framework processes noisy measurements\nfrom limited discrete position, orientation, or pose sensors to estimate the\nshape state robustly. We derive a novel noise-weighted observability matrix,\nproviding a detailed assessment of observability variations under diverse\nsensor configurations. To overcome the limitations of a single model, our\nobserver employs the Interacting Multiple Model (IMM) method, coupled with\nExtended Kalman Filters (EKFs), to mix polynomial curvature models of different\norders. The IMM approach, rooted in Markov processes, effectively manages\nmultiple model scenarios by dynamically adapting to different polynomial orders\nbased on real-time model probabilities. This adaptability is key to ensuring\nrobust shape estimation of the robot's behaviors under various conditions. Our\ncomprehensive analysis, supported by both simulation studies and experimental\nvalidations, confirms the robustness and accuracy of our proposed methods.\n", "link": "http://arxiv.org/abs/2210.08427v2", "date": "2024-08-29", "relevancy": 2.1362, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6246}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5246}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Adaptive%20Estimation%20in%20Polynomial%20Curvature%20Shape%20State%20Space%0A%20%20for%20Continuum%20Robots&body=Title%3A%20Stochastic%20Adaptive%20Estimation%20in%20Polynomial%20Curvature%20Shape%20State%20Space%0A%20%20for%20Continuum%20Robots%0AAuthor%3A%20Guoqing%20Zhang%20and%20Long%20Wang%0AAbstract%3A%20%20%20In%20continuum%20robotics%2C%20real-time%20robust%20shape%20estimation%20is%20crucial%20for%0Aplanning%20and%20control%20tasks%20that%20involve%20physical%20manipulation%20in%20complex%0Aenvironments.%20In%20this%20paper%2C%20we%20present%20a%20novel%20stochastic%20observer-based%20shape%0Aestimation%20framework%20designed%20specifically%20for%20continuum%20robots.%20The%20shape%0Astate%20space%20is%20uniquely%20represented%20by%20the%20modal%20coefficients%20of%20a%20polynomial%2C%0Aenabled%20by%20leveraging%20polynomial%20curvature%20kinematics%20to%20describe%20the%20curvature%0Adistribution%20along%20the%20arclength.%20Our%20framework%20processes%20noisy%20measurements%0Afrom%20limited%20discrete%20position%2C%20orientation%2C%20or%20pose%20sensors%20to%20estimate%20the%0Ashape%20state%20robustly.%20We%20derive%20a%20novel%20noise-weighted%20observability%20matrix%2C%0Aproviding%20a%20detailed%20assessment%20of%20observability%20variations%20under%20diverse%0Asensor%20configurations.%20To%20overcome%20the%20limitations%20of%20a%20single%20model%2C%20our%0Aobserver%20employs%20the%20Interacting%20Multiple%20Model%20%28IMM%29%20method%2C%20coupled%20with%0AExtended%20Kalman%20Filters%20%28EKFs%29%2C%20to%20mix%20polynomial%20curvature%20models%20of%20different%0Aorders.%20The%20IMM%20approach%2C%20rooted%20in%20Markov%20processes%2C%20effectively%20manages%0Amultiple%20model%20scenarios%20by%20dynamically%20adapting%20to%20different%20polynomial%20orders%0Abased%20on%20real-time%20model%20probabilities.%20This%20adaptability%20is%20key%20to%20ensuring%0Arobust%20shape%20estimation%20of%20the%20robot%27s%20behaviors%20under%20various%20conditions.%20Our%0Acomprehensive%20analysis%2C%20supported%20by%20both%20simulation%20studies%20and%20experimental%0Avalidations%2C%20confirms%20the%20robustness%20and%20accuracy%20of%20our%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.08427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Adaptive%2520Estimation%2520in%2520Polynomial%2520Curvature%2520Shape%2520State%2520Space%250A%2520%2520for%2520Continuum%2520Robots%26entry.906535625%3DGuoqing%2520Zhang%2520and%2520Long%2520Wang%26entry.1292438233%3D%2520%2520In%2520continuum%2520robotics%252C%2520real-time%2520robust%2520shape%2520estimation%2520is%2520crucial%2520for%250Aplanning%2520and%2520control%2520tasks%2520that%2520involve%2520physical%2520manipulation%2520in%2520complex%250Aenvironments.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520stochastic%2520observer-based%2520shape%250Aestimation%2520framework%2520designed%2520specifically%2520for%2520continuum%2520robots.%2520The%2520shape%250Astate%2520space%2520is%2520uniquely%2520represented%2520by%2520the%2520modal%2520coefficients%2520of%2520a%2520polynomial%252C%250Aenabled%2520by%2520leveraging%2520polynomial%2520curvature%2520kinematics%2520to%2520describe%2520the%2520curvature%250Adistribution%2520along%2520the%2520arclength.%2520Our%2520framework%2520processes%2520noisy%2520measurements%250Afrom%2520limited%2520discrete%2520position%252C%2520orientation%252C%2520or%2520pose%2520sensors%2520to%2520estimate%2520the%250Ashape%2520state%2520robustly.%2520We%2520derive%2520a%2520novel%2520noise-weighted%2520observability%2520matrix%252C%250Aproviding%2520a%2520detailed%2520assessment%2520of%2520observability%2520variations%2520under%2520diverse%250Asensor%2520configurations.%2520To%2520overcome%2520the%2520limitations%2520of%2520a%2520single%2520model%252C%2520our%250Aobserver%2520employs%2520the%2520Interacting%2520Multiple%2520Model%2520%2528IMM%2529%2520method%252C%2520coupled%2520with%250AExtended%2520Kalman%2520Filters%2520%2528EKFs%2529%252C%2520to%2520mix%2520polynomial%2520curvature%2520models%2520of%2520different%250Aorders.%2520The%2520IMM%2520approach%252C%2520rooted%2520in%2520Markov%2520processes%252C%2520effectively%2520manages%250Amultiple%2520model%2520scenarios%2520by%2520dynamically%2520adapting%2520to%2520different%2520polynomial%2520orders%250Abased%2520on%2520real-time%2520model%2520probabilities.%2520This%2520adaptability%2520is%2520key%2520to%2520ensuring%250Arobust%2520shape%2520estimation%2520of%2520the%2520robot%2527s%2520behaviors%2520under%2520various%2520conditions.%2520Our%250Acomprehensive%2520analysis%252C%2520supported%2520by%2520both%2520simulation%2520studies%2520and%2520experimental%250Avalidations%252C%2520confirms%2520the%2520robustness%2520and%2520accuracy%2520of%2520our%2520proposed%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.08427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Adaptive%20Estimation%20in%20Polynomial%20Curvature%20Shape%20State%20Space%0A%20%20for%20Continuum%20Robots&entry.906535625=Guoqing%20Zhang%20and%20Long%20Wang&entry.1292438233=%20%20In%20continuum%20robotics%2C%20real-time%20robust%20shape%20estimation%20is%20crucial%20for%0Aplanning%20and%20control%20tasks%20that%20involve%20physical%20manipulation%20in%20complex%0Aenvironments.%20In%20this%20paper%2C%20we%20present%20a%20novel%20stochastic%20observer-based%20shape%0Aestimation%20framework%20designed%20specifically%20for%20continuum%20robots.%20The%20shape%0Astate%20space%20is%20uniquely%20represented%20by%20the%20modal%20coefficients%20of%20a%20polynomial%2C%0Aenabled%20by%20leveraging%20polynomial%20curvature%20kinematics%20to%20describe%20the%20curvature%0Adistribution%20along%20the%20arclength.%20Our%20framework%20processes%20noisy%20measurements%0Afrom%20limited%20discrete%20position%2C%20orientation%2C%20or%20pose%20sensors%20to%20estimate%20the%0Ashape%20state%20robustly.%20We%20derive%20a%20novel%20noise-weighted%20observability%20matrix%2C%0Aproviding%20a%20detailed%20assessment%20of%20observability%20variations%20under%20diverse%0Asensor%20configurations.%20To%20overcome%20the%20limitations%20of%20a%20single%20model%2C%20our%0Aobserver%20employs%20the%20Interacting%20Multiple%20Model%20%28IMM%29%20method%2C%20coupled%20with%0AExtended%20Kalman%20Filters%20%28EKFs%29%2C%20to%20mix%20polynomial%20curvature%20models%20of%20different%0Aorders.%20The%20IMM%20approach%2C%20rooted%20in%20Markov%20processes%2C%20effectively%20manages%0Amultiple%20model%20scenarios%20by%20dynamically%20adapting%20to%20different%20polynomial%20orders%0Abased%20on%20real-time%20model%20probabilities.%20This%20adaptability%20is%20key%20to%20ensuring%0Arobust%20shape%20estimation%20of%20the%20robot%27s%20behaviors%20under%20various%20conditions.%20Our%0Acomprehensive%20analysis%2C%20supported%20by%20both%20simulation%20studies%20and%20experimental%0Avalidations%2C%20confirms%20the%20robustness%20and%20accuracy%20of%20our%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.08427v2&entry.124074799=Read"},
{"title": "VideoMambaPro: A Leap Forward for Mamba in Video Understanding", "author": "Hui Lu and Albert Ali Salah and Ronald Poppe", "abstract": "  Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to computer vision tasks, including those in\nvideo analysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. VideoMambaPro shows state-of-the-art video action recognition\nperformance compared to transformer models, and surpasses VideoMamba by clear\nmargins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2,\nrespectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400,\nonly 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The\ncombination of high performance and efficiency makes VideoMambaPro an\ninteresting alternative for transformer models.\n", "link": "http://arxiv.org/abs/2406.19006v2", "date": "2024-08-29", "relevancy": 2.1221, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5484}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoMambaPro%3A%20A%20Leap%20Forward%20for%20Mamba%20in%20Video%20Understanding&body=Title%3A%20VideoMambaPro%3A%20A%20Leap%20Forward%20for%20Mamba%20in%20Video%20Understanding%0AAuthor%3A%20Hui%20Lu%20and%20Albert%20Ali%20Salah%20and%20Ronald%20Poppe%0AAbstract%3A%20%20%20Video%20understanding%20requires%20the%20extraction%20of%20rich%20spatio-temporal%0Arepresentations%2C%20which%20transformer%20models%20achieve%20through%20self-attention.%0AUnfortunately%2C%20self-attention%20poses%20a%20computational%20burden.%20In%20NLP%2C%20Mamba%20has%0Asurfaced%20as%20an%20efficient%20alternative%20for%20transformers.%20However%2C%20Mamba%27s%0Asuccesses%20do%20not%20trivially%20extend%20to%20computer%20vision%20tasks%2C%20including%20those%20in%0Avideo%20analysis.%20In%20this%20paper%2C%20we%20theoretically%20analyze%20the%20differences%20between%0Aself-attention%20and%20Mamba.%20We%20identify%20two%20limitations%20in%20Mamba%27s%20token%0Aprocessing%3A%20historical%20decay%20and%20element%20contradiction.%20We%20propose%0AVideoMambaPro%20%28VMP%29%20that%20solves%20the%20identified%20limitations%20by%20adding%20masked%0Abackward%20computation%20and%20elemental%20residual%20connections%20to%20a%20VideoMamba%0Abackbone.%20VideoMambaPro%20shows%20state-of-the-art%20video%20action%20recognition%0Aperformance%20compared%20to%20transformer%20models%2C%20and%20surpasses%20VideoMamba%20by%20clear%0Amargins%3A%207.9%25%20and%208.1%25%20top-1%20on%20Kinetics-400%20and%20Something-Something%20V2%2C%0Arespectively.%20Our%20VideoMambaPro-M%20model%20achieves%2091.9%25%20top-1%20on%20Kinetics-400%2C%0Aonly%200.2%25%20below%20InternVideo2-6B%20but%20with%20only%201.2%25%20of%20its%20parameters.%20The%0Acombination%20of%20high%20performance%20and%20efficiency%20makes%20VideoMambaPro%20an%0Ainteresting%20alternative%20for%20transformer%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoMambaPro%253A%2520A%2520Leap%2520Forward%2520for%2520Mamba%2520in%2520Video%2520Understanding%26entry.906535625%3DHui%2520Lu%2520and%2520Albert%2520Ali%2520Salah%2520and%2520Ronald%2520Poppe%26entry.1292438233%3D%2520%2520Video%2520understanding%2520requires%2520the%2520extraction%2520of%2520rich%2520spatio-temporal%250Arepresentations%252C%2520which%2520transformer%2520models%2520achieve%2520through%2520self-attention.%250AUnfortunately%252C%2520self-attention%2520poses%2520a%2520computational%2520burden.%2520In%2520NLP%252C%2520Mamba%2520has%250Asurfaced%2520as%2520an%2520efficient%2520alternative%2520for%2520transformers.%2520However%252C%2520Mamba%2527s%250Asuccesses%2520do%2520not%2520trivially%2520extend%2520to%2520computer%2520vision%2520tasks%252C%2520including%2520those%2520in%250Avideo%2520analysis.%2520In%2520this%2520paper%252C%2520we%2520theoretically%2520analyze%2520the%2520differences%2520between%250Aself-attention%2520and%2520Mamba.%2520We%2520identify%2520two%2520limitations%2520in%2520Mamba%2527s%2520token%250Aprocessing%253A%2520historical%2520decay%2520and%2520element%2520contradiction.%2520We%2520propose%250AVideoMambaPro%2520%2528VMP%2529%2520that%2520solves%2520the%2520identified%2520limitations%2520by%2520adding%2520masked%250Abackward%2520computation%2520and%2520elemental%2520residual%2520connections%2520to%2520a%2520VideoMamba%250Abackbone.%2520VideoMambaPro%2520shows%2520state-of-the-art%2520video%2520action%2520recognition%250Aperformance%2520compared%2520to%2520transformer%2520models%252C%2520and%2520surpasses%2520VideoMamba%2520by%2520clear%250Amargins%253A%25207.9%2525%2520and%25208.1%2525%2520top-1%2520on%2520Kinetics-400%2520and%2520Something-Something%2520V2%252C%250Arespectively.%2520Our%2520VideoMambaPro-M%2520model%2520achieves%252091.9%2525%2520top-1%2520on%2520Kinetics-400%252C%250Aonly%25200.2%2525%2520below%2520InternVideo2-6B%2520but%2520with%2520only%25201.2%2525%2520of%2520its%2520parameters.%2520The%250Acombination%2520of%2520high%2520performance%2520and%2520efficiency%2520makes%2520VideoMambaPro%2520an%250Ainteresting%2520alternative%2520for%2520transformer%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoMambaPro%3A%20A%20Leap%20Forward%20for%20Mamba%20in%20Video%20Understanding&entry.906535625=Hui%20Lu%20and%20Albert%20Ali%20Salah%20and%20Ronald%20Poppe&entry.1292438233=%20%20Video%20understanding%20requires%20the%20extraction%20of%20rich%20spatio-temporal%0Arepresentations%2C%20which%20transformer%20models%20achieve%20through%20self-attention.%0AUnfortunately%2C%20self-attention%20poses%20a%20computational%20burden.%20In%20NLP%2C%20Mamba%20has%0Asurfaced%20as%20an%20efficient%20alternative%20for%20transformers.%20However%2C%20Mamba%27s%0Asuccesses%20do%20not%20trivially%20extend%20to%20computer%20vision%20tasks%2C%20including%20those%20in%0Avideo%20analysis.%20In%20this%20paper%2C%20we%20theoretically%20analyze%20the%20differences%20between%0Aself-attention%20and%20Mamba.%20We%20identify%20two%20limitations%20in%20Mamba%27s%20token%0Aprocessing%3A%20historical%20decay%20and%20element%20contradiction.%20We%20propose%0AVideoMambaPro%20%28VMP%29%20that%20solves%20the%20identified%20limitations%20by%20adding%20masked%0Abackward%20computation%20and%20elemental%20residual%20connections%20to%20a%20VideoMamba%0Abackbone.%20VideoMambaPro%20shows%20state-of-the-art%20video%20action%20recognition%0Aperformance%20compared%20to%20transformer%20models%2C%20and%20surpasses%20VideoMamba%20by%20clear%0Amargins%3A%207.9%25%20and%208.1%25%20top-1%20on%20Kinetics-400%20and%20Something-Something%20V2%2C%0Arespectively.%20Our%20VideoMambaPro-M%20model%20achieves%2091.9%25%20top-1%20on%20Kinetics-400%2C%0Aonly%200.2%25%20below%20InternVideo2-6B%20but%20with%20only%201.2%25%20of%20its%20parameters.%20The%0Acombination%20of%20high%20performance%20and%20efficiency%20makes%20VideoMambaPro%20an%0Ainteresting%20alternative%20for%20transformer%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19006v2&entry.124074799=Read"},
{"title": "COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion\n  Estimation", "author": "Jiefeng Li and Ye Yuan and Davis Rempe and Haotian Zhang and Pavlo Molchanov and Cewu Lu and Jan Kautz and Umar Iqbal", "abstract": "  Estimating global human motion from moving cameras is challenging due to the\nentanglement of human and camera motions. To mitigate the ambiguity, existing\nmethods leverage learned human motion priors, which however often result in\noversmoothed motions with misaligned 2D projections. To tackle this problem, we\npropose COIN, a control-inpainting motion diffusion prior that enables\nfine-grained control to disentangle human and camera motions. Although\npre-trained motion diffusion models encode rich motion priors, we find it\nnon-trivial to leverage such knowledge to guide global motion estimation from\nRGB videos. COIN introduces a novel control-inpainting score distillation\nsampling method to ensure well-aligned, consistent, and high-quality motion\nfrom the diffusion prior within a joint optimization framework. Furthermore, we\nintroduce a new human-scene relation loss to alleviate the scale ambiguity by\nenforcing consistency among the humans, camera, and scene. Experiments on three\nchallenging benchmarks demonstrate the effectiveness of COIN, which outperforms\nthe state-of-the-art methods in terms of global human motion estimation and\ncamera motion estimation. As an illustrative example, COIN outperforms the\nstate-of-the-art method by 33% in world joint position error (W-MPJPE) on the\nRICH dataset.\n", "link": "http://arxiv.org/abs/2408.16426v1", "date": "2024-08-29", "relevancy": 2.1162, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5321}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.532}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COIN%3A%20Control-Inpainting%20Diffusion%20Prior%20for%20Human%20and%20Camera%20Motion%0A%20%20Estimation&body=Title%3A%20COIN%3A%20Control-Inpainting%20Diffusion%20Prior%20for%20Human%20and%20Camera%20Motion%0A%20%20Estimation%0AAuthor%3A%20Jiefeng%20Li%20and%20Ye%20Yuan%20and%20Davis%20Rempe%20and%20Haotian%20Zhang%20and%20Pavlo%20Molchanov%20and%20Cewu%20Lu%20and%20Jan%20Kautz%20and%20Umar%20Iqbal%0AAbstract%3A%20%20%20Estimating%20global%20human%20motion%20from%20moving%20cameras%20is%20challenging%20due%20to%20the%0Aentanglement%20of%20human%20and%20camera%20motions.%20To%20mitigate%20the%20ambiguity%2C%20existing%0Amethods%20leverage%20learned%20human%20motion%20priors%2C%20which%20however%20often%20result%20in%0Aoversmoothed%20motions%20with%20misaligned%202D%20projections.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20COIN%2C%20a%20control-inpainting%20motion%20diffusion%20prior%20that%20enables%0Afine-grained%20control%20to%20disentangle%20human%20and%20camera%20motions.%20Although%0Apre-trained%20motion%20diffusion%20models%20encode%20rich%20motion%20priors%2C%20we%20find%20it%0Anon-trivial%20to%20leverage%20such%20knowledge%20to%20guide%20global%20motion%20estimation%20from%0ARGB%20videos.%20COIN%20introduces%20a%20novel%20control-inpainting%20score%20distillation%0Asampling%20method%20to%20ensure%20well-aligned%2C%20consistent%2C%20and%20high-quality%20motion%0Afrom%20the%20diffusion%20prior%20within%20a%20joint%20optimization%20framework.%20Furthermore%2C%20we%0Aintroduce%20a%20new%20human-scene%20relation%20loss%20to%20alleviate%20the%20scale%20ambiguity%20by%0Aenforcing%20consistency%20among%20the%20humans%2C%20camera%2C%20and%20scene.%20Experiments%20on%20three%0Achallenging%20benchmarks%20demonstrate%20the%20effectiveness%20of%20COIN%2C%20which%20outperforms%0Athe%20state-of-the-art%20methods%20in%20terms%20of%20global%20human%20motion%20estimation%20and%0Acamera%20motion%20estimation.%20As%20an%20illustrative%20example%2C%20COIN%20outperforms%20the%0Astate-of-the-art%20method%20by%2033%25%20in%20world%20joint%20position%20error%20%28W-MPJPE%29%20on%20the%0ARICH%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOIN%253A%2520Control-Inpainting%2520Diffusion%2520Prior%2520for%2520Human%2520and%2520Camera%2520Motion%250A%2520%2520Estimation%26entry.906535625%3DJiefeng%2520Li%2520and%2520Ye%2520Yuan%2520and%2520Davis%2520Rempe%2520and%2520Haotian%2520Zhang%2520and%2520Pavlo%2520Molchanov%2520and%2520Cewu%2520Lu%2520and%2520Jan%2520Kautz%2520and%2520Umar%2520Iqbal%26entry.1292438233%3D%2520%2520Estimating%2520global%2520human%2520motion%2520from%2520moving%2520cameras%2520is%2520challenging%2520due%2520to%2520the%250Aentanglement%2520of%2520human%2520and%2520camera%2520motions.%2520To%2520mitigate%2520the%2520ambiguity%252C%2520existing%250Amethods%2520leverage%2520learned%2520human%2520motion%2520priors%252C%2520which%2520however%2520often%2520result%2520in%250Aoversmoothed%2520motions%2520with%2520misaligned%25202D%2520projections.%2520To%2520tackle%2520this%2520problem%252C%2520we%250Apropose%2520COIN%252C%2520a%2520control-inpainting%2520motion%2520diffusion%2520prior%2520that%2520enables%250Afine-grained%2520control%2520to%2520disentangle%2520human%2520and%2520camera%2520motions.%2520Although%250Apre-trained%2520motion%2520diffusion%2520models%2520encode%2520rich%2520motion%2520priors%252C%2520we%2520find%2520it%250Anon-trivial%2520to%2520leverage%2520such%2520knowledge%2520to%2520guide%2520global%2520motion%2520estimation%2520from%250ARGB%2520videos.%2520COIN%2520introduces%2520a%2520novel%2520control-inpainting%2520score%2520distillation%250Asampling%2520method%2520to%2520ensure%2520well-aligned%252C%2520consistent%252C%2520and%2520high-quality%2520motion%250Afrom%2520the%2520diffusion%2520prior%2520within%2520a%2520joint%2520optimization%2520framework.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520new%2520human-scene%2520relation%2520loss%2520to%2520alleviate%2520the%2520scale%2520ambiguity%2520by%250Aenforcing%2520consistency%2520among%2520the%2520humans%252C%2520camera%252C%2520and%2520scene.%2520Experiments%2520on%2520three%250Achallenging%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520COIN%252C%2520which%2520outperforms%250Athe%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520global%2520human%2520motion%2520estimation%2520and%250Acamera%2520motion%2520estimation.%2520As%2520an%2520illustrative%2520example%252C%2520COIN%2520outperforms%2520the%250Astate-of-the-art%2520method%2520by%252033%2525%2520in%2520world%2520joint%2520position%2520error%2520%2528W-MPJPE%2529%2520on%2520the%250ARICH%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COIN%3A%20Control-Inpainting%20Diffusion%20Prior%20for%20Human%20and%20Camera%20Motion%0A%20%20Estimation&entry.906535625=Jiefeng%20Li%20and%20Ye%20Yuan%20and%20Davis%20Rempe%20and%20Haotian%20Zhang%20and%20Pavlo%20Molchanov%20and%20Cewu%20Lu%20and%20Jan%20Kautz%20and%20Umar%20Iqbal&entry.1292438233=%20%20Estimating%20global%20human%20motion%20from%20moving%20cameras%20is%20challenging%20due%20to%20the%0Aentanglement%20of%20human%20and%20camera%20motions.%20To%20mitigate%20the%20ambiguity%2C%20existing%0Amethods%20leverage%20learned%20human%20motion%20priors%2C%20which%20however%20often%20result%20in%0Aoversmoothed%20motions%20with%20misaligned%202D%20projections.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20COIN%2C%20a%20control-inpainting%20motion%20diffusion%20prior%20that%20enables%0Afine-grained%20control%20to%20disentangle%20human%20and%20camera%20motions.%20Although%0Apre-trained%20motion%20diffusion%20models%20encode%20rich%20motion%20priors%2C%20we%20find%20it%0Anon-trivial%20to%20leverage%20such%20knowledge%20to%20guide%20global%20motion%20estimation%20from%0ARGB%20videos.%20COIN%20introduces%20a%20novel%20control-inpainting%20score%20distillation%0Asampling%20method%20to%20ensure%20well-aligned%2C%20consistent%2C%20and%20high-quality%20motion%0Afrom%20the%20diffusion%20prior%20within%20a%20joint%20optimization%20framework.%20Furthermore%2C%20we%0Aintroduce%20a%20new%20human-scene%20relation%20loss%20to%20alleviate%20the%20scale%20ambiguity%20by%0Aenforcing%20consistency%20among%20the%20humans%2C%20camera%2C%20and%20scene.%20Experiments%20on%20three%0Achallenging%20benchmarks%20demonstrate%20the%20effectiveness%20of%20COIN%2C%20which%20outperforms%0Athe%20state-of-the-art%20methods%20in%20terms%20of%20global%20human%20motion%20estimation%20and%0Acamera%20motion%20estimation.%20As%20an%20illustrative%20example%2C%20COIN%20outperforms%20the%0Astate-of-the-art%20method%20by%2033%25%20in%20world%20joint%20position%20error%20%28W-MPJPE%29%20on%20the%0ARICH%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16426v1&entry.124074799=Read"},
{"title": "SODAWideNet++: Combining Attention and Convolutions for Salient Object\n  Detection", "author": "Rohit Venkata Sai Dulam and Chandra Kambhamettu", "abstract": "  Salient Object Detection (SOD) has traditionally relied on feature refinement\nmodules that utilize the features of an ImageNet pre-trained backbone. However,\nthis approach limits the possibility of pre-training the entire network because\nof the distinct nature of SOD and image classification. Additionally, the\narchitecture of these backbones originally built for Image classification is\nsub-optimal for a dense prediction task like SOD. To address these issues, we\npropose a novel encoder-decoder-style neural network called SODAWideNet++ that\nis designed explicitly for SOD. Inspired by the vision transformers ability to\nattain a global receptive field from the initial stages, we introduce the\nAttention Guided Long Range Feature Extraction (AGLRFE) module, which combines\nlarge dilated convolutions and self-attention. Specifically, we use attention\nfeatures to guide long-range information extracted by multiple dilated\nconvolutions, thus taking advantage of the inductive biases of a convolution\noperation and the input dependency brought by self-attention. In contrast to\nthe current paradigm of ImageNet pre-training, we modify 118K annotated images\nfrom the COCO semantic segmentation dataset by binarizing the annotations to\npre-train the proposed model end-to-end. Further, we supervise the background\npredictions along with the foreground to push our model to generate accurate\nsaliency predictions. SODAWideNet++ performs competitively on five different\ndatasets while only containing 35% of the trainable parameters compared to the\nstate-of-the-art models. The code and pre-computed saliency maps are provided\nat https://github.com/VimsLab/SODAWideNetPlusPlus.\n", "link": "http://arxiv.org/abs/2408.16645v1", "date": "2024-08-29", "relevancy": 2.115, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5335}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5313}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SODAWideNet%2B%2B%3A%20Combining%20Attention%20and%20Convolutions%20for%20Salient%20Object%0A%20%20Detection&body=Title%3A%20SODAWideNet%2B%2B%3A%20Combining%20Attention%20and%20Convolutions%20for%20Salient%20Object%0A%20%20Detection%0AAuthor%3A%20Rohit%20Venkata%20Sai%20Dulam%20and%20Chandra%20Kambhamettu%0AAbstract%3A%20%20%20Salient%20Object%20Detection%20%28SOD%29%20has%20traditionally%20relied%20on%20feature%20refinement%0Amodules%20that%20utilize%20the%20features%20of%20an%20ImageNet%20pre-trained%20backbone.%20However%2C%0Athis%20approach%20limits%20the%20possibility%20of%20pre-training%20the%20entire%20network%20because%0Aof%20the%20distinct%20nature%20of%20SOD%20and%20image%20classification.%20Additionally%2C%20the%0Aarchitecture%20of%20these%20backbones%20originally%20built%20for%20Image%20classification%20is%0Asub-optimal%20for%20a%20dense%20prediction%20task%20like%20SOD.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20novel%20encoder-decoder-style%20neural%20network%20called%20SODAWideNet%2B%2B%20that%0Ais%20designed%20explicitly%20for%20SOD.%20Inspired%20by%20the%20vision%20transformers%20ability%20to%0Aattain%20a%20global%20receptive%20field%20from%20the%20initial%20stages%2C%20we%20introduce%20the%0AAttention%20Guided%20Long%20Range%20Feature%20Extraction%20%28AGLRFE%29%20module%2C%20which%20combines%0Alarge%20dilated%20convolutions%20and%20self-attention.%20Specifically%2C%20we%20use%20attention%0Afeatures%20to%20guide%20long-range%20information%20extracted%20by%20multiple%20dilated%0Aconvolutions%2C%20thus%20taking%20advantage%20of%20the%20inductive%20biases%20of%20a%20convolution%0Aoperation%20and%20the%20input%20dependency%20brought%20by%20self-attention.%20In%20contrast%20to%0Athe%20current%20paradigm%20of%20ImageNet%20pre-training%2C%20we%20modify%20118K%20annotated%20images%0Afrom%20the%20COCO%20semantic%20segmentation%20dataset%20by%20binarizing%20the%20annotations%20to%0Apre-train%20the%20proposed%20model%20end-to-end.%20Further%2C%20we%20supervise%20the%20background%0Apredictions%20along%20with%20the%20foreground%20to%20push%20our%20model%20to%20generate%20accurate%0Asaliency%20predictions.%20SODAWideNet%2B%2B%20performs%20competitively%20on%20five%20different%0Adatasets%20while%20only%20containing%2035%25%20of%20the%20trainable%20parameters%20compared%20to%20the%0Astate-of-the-art%20models.%20The%20code%20and%20pre-computed%20saliency%20maps%20are%20provided%0Aat%20https%3A//github.com/VimsLab/SODAWideNetPlusPlus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSODAWideNet%252B%252B%253A%2520Combining%2520Attention%2520and%2520Convolutions%2520for%2520Salient%2520Object%250A%2520%2520Detection%26entry.906535625%3DRohit%2520Venkata%2520Sai%2520Dulam%2520and%2520Chandra%2520Kambhamettu%26entry.1292438233%3D%2520%2520Salient%2520Object%2520Detection%2520%2528SOD%2529%2520has%2520traditionally%2520relied%2520on%2520feature%2520refinement%250Amodules%2520that%2520utilize%2520the%2520features%2520of%2520an%2520ImageNet%2520pre-trained%2520backbone.%2520However%252C%250Athis%2520approach%2520limits%2520the%2520possibility%2520of%2520pre-training%2520the%2520entire%2520network%2520because%250Aof%2520the%2520distinct%2520nature%2520of%2520SOD%2520and%2520image%2520classification.%2520Additionally%252C%2520the%250Aarchitecture%2520of%2520these%2520backbones%2520originally%2520built%2520for%2520Image%2520classification%2520is%250Asub-optimal%2520for%2520a%2520dense%2520prediction%2520task%2520like%2520SOD.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520novel%2520encoder-decoder-style%2520neural%2520network%2520called%2520SODAWideNet%252B%252B%2520that%250Ais%2520designed%2520explicitly%2520for%2520SOD.%2520Inspired%2520by%2520the%2520vision%2520transformers%2520ability%2520to%250Aattain%2520a%2520global%2520receptive%2520field%2520from%2520the%2520initial%2520stages%252C%2520we%2520introduce%2520the%250AAttention%2520Guided%2520Long%2520Range%2520Feature%2520Extraction%2520%2528AGLRFE%2529%2520module%252C%2520which%2520combines%250Alarge%2520dilated%2520convolutions%2520and%2520self-attention.%2520Specifically%252C%2520we%2520use%2520attention%250Afeatures%2520to%2520guide%2520long-range%2520information%2520extracted%2520by%2520multiple%2520dilated%250Aconvolutions%252C%2520thus%2520taking%2520advantage%2520of%2520the%2520inductive%2520biases%2520of%2520a%2520convolution%250Aoperation%2520and%2520the%2520input%2520dependency%2520brought%2520by%2520self-attention.%2520In%2520contrast%2520to%250Athe%2520current%2520paradigm%2520of%2520ImageNet%2520pre-training%252C%2520we%2520modify%2520118K%2520annotated%2520images%250Afrom%2520the%2520COCO%2520semantic%2520segmentation%2520dataset%2520by%2520binarizing%2520the%2520annotations%2520to%250Apre-train%2520the%2520proposed%2520model%2520end-to-end.%2520Further%252C%2520we%2520supervise%2520the%2520background%250Apredictions%2520along%2520with%2520the%2520foreground%2520to%2520push%2520our%2520model%2520to%2520generate%2520accurate%250Asaliency%2520predictions.%2520SODAWideNet%252B%252B%2520performs%2520competitively%2520on%2520five%2520different%250Adatasets%2520while%2520only%2520containing%252035%2525%2520of%2520the%2520trainable%2520parameters%2520compared%2520to%2520the%250Astate-of-the-art%2520models.%2520The%2520code%2520and%2520pre-computed%2520saliency%2520maps%2520are%2520provided%250Aat%2520https%253A//github.com/VimsLab/SODAWideNetPlusPlus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SODAWideNet%2B%2B%3A%20Combining%20Attention%20and%20Convolutions%20for%20Salient%20Object%0A%20%20Detection&entry.906535625=Rohit%20Venkata%20Sai%20Dulam%20and%20Chandra%20Kambhamettu&entry.1292438233=%20%20Salient%20Object%20Detection%20%28SOD%29%20has%20traditionally%20relied%20on%20feature%20refinement%0Amodules%20that%20utilize%20the%20features%20of%20an%20ImageNet%20pre-trained%20backbone.%20However%2C%0Athis%20approach%20limits%20the%20possibility%20of%20pre-training%20the%20entire%20network%20because%0Aof%20the%20distinct%20nature%20of%20SOD%20and%20image%20classification.%20Additionally%2C%20the%0Aarchitecture%20of%20these%20backbones%20originally%20built%20for%20Image%20classification%20is%0Asub-optimal%20for%20a%20dense%20prediction%20task%20like%20SOD.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20novel%20encoder-decoder-style%20neural%20network%20called%20SODAWideNet%2B%2B%20that%0Ais%20designed%20explicitly%20for%20SOD.%20Inspired%20by%20the%20vision%20transformers%20ability%20to%0Aattain%20a%20global%20receptive%20field%20from%20the%20initial%20stages%2C%20we%20introduce%20the%0AAttention%20Guided%20Long%20Range%20Feature%20Extraction%20%28AGLRFE%29%20module%2C%20which%20combines%0Alarge%20dilated%20convolutions%20and%20self-attention.%20Specifically%2C%20we%20use%20attention%0Afeatures%20to%20guide%20long-range%20information%20extracted%20by%20multiple%20dilated%0Aconvolutions%2C%20thus%20taking%20advantage%20of%20the%20inductive%20biases%20of%20a%20convolution%0Aoperation%20and%20the%20input%20dependency%20brought%20by%20self-attention.%20In%20contrast%20to%0Athe%20current%20paradigm%20of%20ImageNet%20pre-training%2C%20we%20modify%20118K%20annotated%20images%0Afrom%20the%20COCO%20semantic%20segmentation%20dataset%20by%20binarizing%20the%20annotations%20to%0Apre-train%20the%20proposed%20model%20end-to-end.%20Further%2C%20we%20supervise%20the%20background%0Apredictions%20along%20with%20the%20foreground%20to%20push%20our%20model%20to%20generate%20accurate%0Asaliency%20predictions.%20SODAWideNet%2B%2B%20performs%20competitively%20on%20five%20different%0Adatasets%20while%20only%20containing%2035%25%20of%20the%20trainable%20parameters%20compared%20to%20the%0Astate-of-the-art%20models.%20The%20code%20and%20pre-computed%20saliency%20maps%20are%20provided%0Aat%20https%3A//github.com/VimsLab/SODAWideNetPlusPlus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16645v1&entry.124074799=Read"},
{"title": "The $\u03bc\\mathcal{G}$ Language for Programming Graph Neural Networks", "author": "Matteo Belenchia and Flavio Corradini and Michela Quadrini and Michele Loreti", "abstract": "  Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.\n", "link": "http://arxiv.org/abs/2407.09441v2", "date": "2024-08-29", "relevancy": 2.1049, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4373}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4285}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks&body=Title%3A%20The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks%0AAuthor%3A%20Matteo%20Belenchia%20and%20Flavio%20Corradini%20and%20Michela%20Quadrini%20and%20Michele%20Loreti%0AAbstract%3A%20%20%20Graph%20neural%20networks%20form%20a%20class%20of%20deep%20learning%20architectures%0Aspecifically%20designed%20to%20work%20with%20graph-structured%20data.%20As%20such%2C%20they%20share%0Athe%20inherent%20limitations%20and%20problems%20of%20deep%20learning%2C%20especially%20regarding%0Athe%20issues%20of%20explainability%20and%20trustworthiness.%20We%20propose%20%24%5Cmu%5Cmathcal%7BG%7D%24%2C%0Aan%20original%20domain-specific%20language%20for%20the%20specification%20of%20graph%20neural%0Anetworks%20that%20aims%20to%20overcome%20these%20issues.%20The%20language%27s%20syntax%20is%0Aintroduced%2C%20and%20its%20meaning%20is%20rigorously%20defined%20by%20a%20denotational%20semantics.%0AAn%20equivalent%20characterization%20in%20the%20form%20of%20an%20operational%20semantics%20is%20also%0Aprovided%20and%2C%20together%20with%20a%20type%20system%2C%20is%20used%20to%20prove%20the%20type%20soundness%0Aof%20%24%5Cmu%5Cmathcal%7BG%7D%24.%20We%20show%20how%20%24%5Cmu%5Cmathcal%7BG%7D%24%20programs%20can%20be%20represented%0Ain%20a%20more%20user-friendly%20graphical%20visualization%2C%20and%20provide%20examples%20of%20its%0Agenerality%20by%20showing%20how%20it%20can%20be%20used%20to%20define%20some%20of%20the%20most%20popular%0Agraph%20neural%20network%20models%2C%20or%20to%20develop%20any%20custom%20graph%20processing%0Aapplication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09441v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520%2524%25CE%25BC%255Cmathcal%257BG%257D%2524%2520Language%2520for%2520Programming%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMatteo%2520Belenchia%2520and%2520Flavio%2520Corradini%2520and%2520Michela%2520Quadrini%2520and%2520Michele%2520Loreti%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520form%2520a%2520class%2520of%2520deep%2520learning%2520architectures%250Aspecifically%2520designed%2520to%2520work%2520with%2520graph-structured%2520data.%2520As%2520such%252C%2520they%2520share%250Athe%2520inherent%2520limitations%2520and%2520problems%2520of%2520deep%2520learning%252C%2520especially%2520regarding%250Athe%2520issues%2520of%2520explainability%2520and%2520trustworthiness.%2520We%2520propose%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524%252C%250Aan%2520original%2520domain-specific%2520language%2520for%2520the%2520specification%2520of%2520graph%2520neural%250Anetworks%2520that%2520aims%2520to%2520overcome%2520these%2520issues.%2520The%2520language%2527s%2520syntax%2520is%250Aintroduced%252C%2520and%2520its%2520meaning%2520is%2520rigorously%2520defined%2520by%2520a%2520denotational%2520semantics.%250AAn%2520equivalent%2520characterization%2520in%2520the%2520form%2520of%2520an%2520operational%2520semantics%2520is%2520also%250Aprovided%2520and%252C%2520together%2520with%2520a%2520type%2520system%252C%2520is%2520used%2520to%2520prove%2520the%2520type%2520soundness%250Aof%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524.%2520We%2520show%2520how%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524%2520programs%2520can%2520be%2520represented%250Ain%2520a%2520more%2520user-friendly%2520graphical%2520visualization%252C%2520and%2520provide%2520examples%2520of%2520its%250Agenerality%2520by%2520showing%2520how%2520it%2520can%2520be%2520used%2520to%2520define%2520some%2520of%2520the%2520most%2520popular%250Agraph%2520neural%2520network%2520models%252C%2520or%2520to%2520develop%2520any%2520custom%2520graph%2520processing%250Aapplication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09441v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks&entry.906535625=Matteo%20Belenchia%20and%20Flavio%20Corradini%20and%20Michela%20Quadrini%20and%20Michele%20Loreti&entry.1292438233=%20%20Graph%20neural%20networks%20form%20a%20class%20of%20deep%20learning%20architectures%0Aspecifically%20designed%20to%20work%20with%20graph-structured%20data.%20As%20such%2C%20they%20share%0Athe%20inherent%20limitations%20and%20problems%20of%20deep%20learning%2C%20especially%20regarding%0Athe%20issues%20of%20explainability%20and%20trustworthiness.%20We%20propose%20%24%5Cmu%5Cmathcal%7BG%7D%24%2C%0Aan%20original%20domain-specific%20language%20for%20the%20specification%20of%20graph%20neural%0Anetworks%20that%20aims%20to%20overcome%20these%20issues.%20The%20language%27s%20syntax%20is%0Aintroduced%2C%20and%20its%20meaning%20is%20rigorously%20defined%20by%20a%20denotational%20semantics.%0AAn%20equivalent%20characterization%20in%20the%20form%20of%20an%20operational%20semantics%20is%20also%0Aprovided%20and%2C%20together%20with%20a%20type%20system%2C%20is%20used%20to%20prove%20the%20type%20soundness%0Aof%20%24%5Cmu%5Cmathcal%7BG%7D%24.%20We%20show%20how%20%24%5Cmu%5Cmathcal%7BG%7D%24%20programs%20can%20be%20represented%0Ain%20a%20more%20user-friendly%20graphical%20visualization%2C%20and%20provide%20examples%20of%20its%0Agenerality%20by%20showing%20how%20it%20can%20be%20used%20to%20define%20some%20of%20the%20most%20popular%0Agraph%20neural%20network%20models%2C%20or%20to%20develop%20any%20custom%20graph%20processing%0Aapplication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09441v2&entry.124074799=Read"},
{"title": "GANs Conditioning Methods: A Survey", "author": "Anis Bourou and Auguste Genovesio and Val\u00e9rie Mezger", "abstract": "  In recent years, Generative Adversarial Networks (GANs) have seen significant\nadvancements, leading to their widespread adoption across various fields. The\noriginal GAN architecture enables the generation of images without any specific\ncontrol over the content, making it an unconditional generation process.\nHowever, many practical applications require precise control over the generated\noutput, which has led to the development of conditional GANs (cGANs) that\nincorporate explicit conditioning to guide the generation process. cGANs extend\nthe original framework by incorporating additional information (conditions),\nenabling the generation of samples that adhere to that specific criteria.\nVarious conditioning methods have been proposed, each differing in how they\nintegrate the conditioning information into both the generator and the\ndiscriminator networks. In this work, we review the conditioning methods\nproposed for GANs, exploring the characteristics of each method and\nhighlighting their unique mechanisms and theoretical foundations. Furthermore,\nwe conduct a comparative analysis of these methods, evaluating their\nperformance on various image datasets. Through these analyses, we aim to\nprovide insights into the strengths and limitations of various conditioning\ntechniques, guiding future research and application in generative modeling.\n", "link": "http://arxiv.org/abs/2408.15640v2", "date": "2024-08-29", "relevancy": 2.0801, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5279}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5154}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GANs%20Conditioning%20Methods%3A%20A%20Survey&body=Title%3A%20GANs%20Conditioning%20Methods%3A%20A%20Survey%0AAuthor%3A%20Anis%20Bourou%20and%20Auguste%20Genovesio%20and%20Val%C3%A9rie%20Mezger%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20seen%20significant%0Aadvancements%2C%20leading%20to%20their%20widespread%20adoption%20across%20various%20fields.%20The%0Aoriginal%20GAN%20architecture%20enables%20the%20generation%20of%20images%20without%20any%20specific%0Acontrol%20over%20the%20content%2C%20making%20it%20an%20unconditional%20generation%20process.%0AHowever%2C%20many%20practical%20applications%20require%20precise%20control%20over%20the%20generated%0Aoutput%2C%20which%20has%20led%20to%20the%20development%20of%20conditional%20GANs%20%28cGANs%29%20that%0Aincorporate%20explicit%20conditioning%20to%20guide%20the%20generation%20process.%20cGANs%20extend%0Athe%20original%20framework%20by%20incorporating%20additional%20information%20%28conditions%29%2C%0Aenabling%20the%20generation%20of%20samples%20that%20adhere%20to%20that%20specific%20criteria.%0AVarious%20conditioning%20methods%20have%20been%20proposed%2C%20each%20differing%20in%20how%20they%0Aintegrate%20the%20conditioning%20information%20into%20both%20the%20generator%20and%20the%0Adiscriminator%20networks.%20In%20this%20work%2C%20we%20review%20the%20conditioning%20methods%0Aproposed%20for%20GANs%2C%20exploring%20the%20characteristics%20of%20each%20method%20and%0Ahighlighting%20their%20unique%20mechanisms%20and%20theoretical%20foundations.%20Furthermore%2C%0Awe%20conduct%20a%20comparative%20analysis%20of%20these%20methods%2C%20evaluating%20their%0Aperformance%20on%20various%20image%20datasets.%20Through%20these%20analyses%2C%20we%20aim%20to%0Aprovide%20insights%20into%20the%20strengths%20and%20limitations%20of%20various%20conditioning%0Atechniques%2C%20guiding%20future%20research%20and%20application%20in%20generative%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGANs%2520Conditioning%2520Methods%253A%2520A%2520Survey%26entry.906535625%3DAnis%2520Bourou%2520and%2520Auguste%2520Genovesio%2520and%2520Val%25C3%25A9rie%2520Mezger%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520have%2520seen%2520significant%250Aadvancements%252C%2520leading%2520to%2520their%2520widespread%2520adoption%2520across%2520various%2520fields.%2520The%250Aoriginal%2520GAN%2520architecture%2520enables%2520the%2520generation%2520of%2520images%2520without%2520any%2520specific%250Acontrol%2520over%2520the%2520content%252C%2520making%2520it%2520an%2520unconditional%2520generation%2520process.%250AHowever%252C%2520many%2520practical%2520applications%2520require%2520precise%2520control%2520over%2520the%2520generated%250Aoutput%252C%2520which%2520has%2520led%2520to%2520the%2520development%2520of%2520conditional%2520GANs%2520%2528cGANs%2529%2520that%250Aincorporate%2520explicit%2520conditioning%2520to%2520guide%2520the%2520generation%2520process.%2520cGANs%2520extend%250Athe%2520original%2520framework%2520by%2520incorporating%2520additional%2520information%2520%2528conditions%2529%252C%250Aenabling%2520the%2520generation%2520of%2520samples%2520that%2520adhere%2520to%2520that%2520specific%2520criteria.%250AVarious%2520conditioning%2520methods%2520have%2520been%2520proposed%252C%2520each%2520differing%2520in%2520how%2520they%250Aintegrate%2520the%2520conditioning%2520information%2520into%2520both%2520the%2520generator%2520and%2520the%250Adiscriminator%2520networks.%2520In%2520this%2520work%252C%2520we%2520review%2520the%2520conditioning%2520methods%250Aproposed%2520for%2520GANs%252C%2520exploring%2520the%2520characteristics%2520of%2520each%2520method%2520and%250Ahighlighting%2520their%2520unique%2520mechanisms%2520and%2520theoretical%2520foundations.%2520Furthermore%252C%250Awe%2520conduct%2520a%2520comparative%2520analysis%2520of%2520these%2520methods%252C%2520evaluating%2520their%250Aperformance%2520on%2520various%2520image%2520datasets.%2520Through%2520these%2520analyses%252C%2520we%2520aim%2520to%250Aprovide%2520insights%2520into%2520the%2520strengths%2520and%2520limitations%2520of%2520various%2520conditioning%250Atechniques%252C%2520guiding%2520future%2520research%2520and%2520application%2520in%2520generative%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GANs%20Conditioning%20Methods%3A%20A%20Survey&entry.906535625=Anis%20Bourou%20and%20Auguste%20Genovesio%20and%20Val%C3%A9rie%20Mezger&entry.1292438233=%20%20In%20recent%20years%2C%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20seen%20significant%0Aadvancements%2C%20leading%20to%20their%20widespread%20adoption%20across%20various%20fields.%20The%0Aoriginal%20GAN%20architecture%20enables%20the%20generation%20of%20images%20without%20any%20specific%0Acontrol%20over%20the%20content%2C%20making%20it%20an%20unconditional%20generation%20process.%0AHowever%2C%20many%20practical%20applications%20require%20precise%20control%20over%20the%20generated%0Aoutput%2C%20which%20has%20led%20to%20the%20development%20of%20conditional%20GANs%20%28cGANs%29%20that%0Aincorporate%20explicit%20conditioning%20to%20guide%20the%20generation%20process.%20cGANs%20extend%0Athe%20original%20framework%20by%20incorporating%20additional%20information%20%28conditions%29%2C%0Aenabling%20the%20generation%20of%20samples%20that%20adhere%20to%20that%20specific%20criteria.%0AVarious%20conditioning%20methods%20have%20been%20proposed%2C%20each%20differing%20in%20how%20they%0Aintegrate%20the%20conditioning%20information%20into%20both%20the%20generator%20and%20the%0Adiscriminator%20networks.%20In%20this%20work%2C%20we%20review%20the%20conditioning%20methods%0Aproposed%20for%20GANs%2C%20exploring%20the%20characteristics%20of%20each%20method%20and%0Ahighlighting%20their%20unique%20mechanisms%20and%20theoretical%20foundations.%20Furthermore%2C%0Awe%20conduct%20a%20comparative%20analysis%20of%20these%20methods%2C%20evaluating%20their%0Aperformance%20on%20various%20image%20datasets.%20Through%20these%20analyses%2C%20we%20aim%20to%0Aprovide%20insights%20into%20the%20strengths%20and%20limitations%20of%20various%20conditioning%0Atechniques%2C%20guiding%20future%20research%20and%20application%20in%20generative%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15640v2&entry.124074799=Read"},
{"title": "Batched Stochastic Bandit for Nondegenerate Functions", "author": "Yu Liu and Yunlu Shu and Tianyu Wang", "abstract": "  This paper studies batched bandit learning problems for nondegenerate\nfunctions. We introduce an algorithm that solves the batched bandit problem for\nnondegenerate functions near-optimally. More specifically, we introduce an\nalgorithm, called Geometric Narrowing (GN), whose regret bound is of order\n$\\widetilde{{\\mathcal{O}}} ( A_{+}^d \\sqrt{T} )$. In addition, GN only needs\n$\\mathcal{O} (\\log \\log T)$ batches to achieve this regret. We also provide\nlower bound analysis for this problem. More specifically, we prove that over\nsome (compact) doubling metric space of doubling dimension $d$: 1. For any\npolicy $\\pi$, there exists a problem instance on which $\\pi$ admits a regret of\norder ${\\Omega} ( A_-^d \\sqrt{T})$; 2. No policy can achieve a regret of order\n$ A_-^d \\sqrt{T} $ over all problem instances, using less than $ \\Omega ( \\log\n\\log T ) $ rounds of communications. Our lower bound analysis shows that the GN\nalgorithm achieves near optimal regret with minimal number of batches.\n", "link": "http://arxiv.org/abs/2405.05733v2", "date": "2024-08-29", "relevancy": 2.0715, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4209}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4163}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Batched%20Stochastic%20Bandit%20for%20Nondegenerate%20Functions&body=Title%3A%20Batched%20Stochastic%20Bandit%20for%20Nondegenerate%20Functions%0AAuthor%3A%20Yu%20Liu%20and%20Yunlu%20Shu%20and%20Tianyu%20Wang%0AAbstract%3A%20%20%20This%20paper%20studies%20batched%20bandit%20learning%20problems%20for%20nondegenerate%0Afunctions.%20We%20introduce%20an%20algorithm%20that%20solves%20the%20batched%20bandit%20problem%20for%0Anondegenerate%20functions%20near-optimally.%20More%20specifically%2C%20we%20introduce%20an%0Aalgorithm%2C%20called%20Geometric%20Narrowing%20%28GN%29%2C%20whose%20regret%20bound%20is%20of%20order%0A%24%5Cwidetilde%7B%7B%5Cmathcal%7BO%7D%7D%7D%20%28%20A_%7B%2B%7D%5Ed%20%5Csqrt%7BT%7D%20%29%24.%20In%20addition%2C%20GN%20only%20needs%0A%24%5Cmathcal%7BO%7D%20%28%5Clog%20%5Clog%20T%29%24%20batches%20to%20achieve%20this%20regret.%20We%20also%20provide%0Alower%20bound%20analysis%20for%20this%20problem.%20More%20specifically%2C%20we%20prove%20that%20over%0Asome%20%28compact%29%20doubling%20metric%20space%20of%20doubling%20dimension%20%24d%24%3A%201.%20For%20any%0Apolicy%20%24%5Cpi%24%2C%20there%20exists%20a%20problem%20instance%20on%20which%20%24%5Cpi%24%20admits%20a%20regret%20of%0Aorder%20%24%7B%5COmega%7D%20%28%20A_-%5Ed%20%5Csqrt%7BT%7D%29%24%3B%202.%20No%20policy%20can%20achieve%20a%20regret%20of%20order%0A%24%20A_-%5Ed%20%5Csqrt%7BT%7D%20%24%20over%20all%20problem%20instances%2C%20using%20less%20than%20%24%20%5COmega%20%28%20%5Clog%0A%5Clog%20T%20%29%20%24%20rounds%20of%20communications.%20Our%20lower%20bound%20analysis%20shows%20that%20the%20GN%0Aalgorithm%20achieves%20near%20optimal%20regret%20with%20minimal%20number%20of%20batches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBatched%2520Stochastic%2520Bandit%2520for%2520Nondegenerate%2520Functions%26entry.906535625%3DYu%2520Liu%2520and%2520Yunlu%2520Shu%2520and%2520Tianyu%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520batched%2520bandit%2520learning%2520problems%2520for%2520nondegenerate%250Afunctions.%2520We%2520introduce%2520an%2520algorithm%2520that%2520solves%2520the%2520batched%2520bandit%2520problem%2520for%250Anondegenerate%2520functions%2520near-optimally.%2520More%2520specifically%252C%2520we%2520introduce%2520an%250Aalgorithm%252C%2520called%2520Geometric%2520Narrowing%2520%2528GN%2529%252C%2520whose%2520regret%2520bound%2520is%2520of%2520order%250A%2524%255Cwidetilde%257B%257B%255Cmathcal%257BO%257D%257D%257D%2520%2528%2520A_%257B%252B%257D%255Ed%2520%255Csqrt%257BT%257D%2520%2529%2524.%2520In%2520addition%252C%2520GN%2520only%2520needs%250A%2524%255Cmathcal%257BO%257D%2520%2528%255Clog%2520%255Clog%2520T%2529%2524%2520batches%2520to%2520achieve%2520this%2520regret.%2520We%2520also%2520provide%250Alower%2520bound%2520analysis%2520for%2520this%2520problem.%2520More%2520specifically%252C%2520we%2520prove%2520that%2520over%250Asome%2520%2528compact%2529%2520doubling%2520metric%2520space%2520of%2520doubling%2520dimension%2520%2524d%2524%253A%25201.%2520For%2520any%250Apolicy%2520%2524%255Cpi%2524%252C%2520there%2520exists%2520a%2520problem%2520instance%2520on%2520which%2520%2524%255Cpi%2524%2520admits%2520a%2520regret%2520of%250Aorder%2520%2524%257B%255COmega%257D%2520%2528%2520A_-%255Ed%2520%255Csqrt%257BT%257D%2529%2524%253B%25202.%2520No%2520policy%2520can%2520achieve%2520a%2520regret%2520of%2520order%250A%2524%2520A_-%255Ed%2520%255Csqrt%257BT%257D%2520%2524%2520over%2520all%2520problem%2520instances%252C%2520using%2520less%2520than%2520%2524%2520%255COmega%2520%2528%2520%255Clog%250A%255Clog%2520T%2520%2529%2520%2524%2520rounds%2520of%2520communications.%2520Our%2520lower%2520bound%2520analysis%2520shows%2520that%2520the%2520GN%250Aalgorithm%2520achieves%2520near%2520optimal%2520regret%2520with%2520minimal%2520number%2520of%2520batches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Batched%20Stochastic%20Bandit%20for%20Nondegenerate%20Functions&entry.906535625=Yu%20Liu%20and%20Yunlu%20Shu%20and%20Tianyu%20Wang&entry.1292438233=%20%20This%20paper%20studies%20batched%20bandit%20learning%20problems%20for%20nondegenerate%0Afunctions.%20We%20introduce%20an%20algorithm%20that%20solves%20the%20batched%20bandit%20problem%20for%0Anondegenerate%20functions%20near-optimally.%20More%20specifically%2C%20we%20introduce%20an%0Aalgorithm%2C%20called%20Geometric%20Narrowing%20%28GN%29%2C%20whose%20regret%20bound%20is%20of%20order%0A%24%5Cwidetilde%7B%7B%5Cmathcal%7BO%7D%7D%7D%20%28%20A_%7B%2B%7D%5Ed%20%5Csqrt%7BT%7D%20%29%24.%20In%20addition%2C%20GN%20only%20needs%0A%24%5Cmathcal%7BO%7D%20%28%5Clog%20%5Clog%20T%29%24%20batches%20to%20achieve%20this%20regret.%20We%20also%20provide%0Alower%20bound%20analysis%20for%20this%20problem.%20More%20specifically%2C%20we%20prove%20that%20over%0Asome%20%28compact%29%20doubling%20metric%20space%20of%20doubling%20dimension%20%24d%24%3A%201.%20For%20any%0Apolicy%20%24%5Cpi%24%2C%20there%20exists%20a%20problem%20instance%20on%20which%20%24%5Cpi%24%20admits%20a%20regret%20of%0Aorder%20%24%7B%5COmega%7D%20%28%20A_-%5Ed%20%5Csqrt%7BT%7D%29%24%3B%202.%20No%20policy%20can%20achieve%20a%20regret%20of%20order%0A%24%20A_-%5Ed%20%5Csqrt%7BT%7D%20%24%20over%20all%20problem%20instances%2C%20using%20less%20than%20%24%20%5COmega%20%28%20%5Clog%0A%5Clog%20T%20%29%20%24%20rounds%20of%20communications.%20Our%20lower%20bound%20analysis%20shows%20that%20the%20GN%0Aalgorithm%20achieves%20near%20optimal%20regret%20with%20minimal%20number%20of%20batches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05733v2&entry.124074799=Read"},
{"title": "Verification of Geometric Robustness of Neural Networks via Piecewise\n  Linear Approximation and Lipschitz Optimisation", "author": "Ben Batten and Yang Zheng and Alessandro De Palma and Panagiotis Kouvaros and Alessio Lomuscio", "abstract": "  We address the problem of verifying neural networks against geometric\ntransformations of the input image, including rotation, scaling, shearing, and\ntranslation. The proposed method computes provably sound piecewise linear\nconstraints for the pixel values by using sampling and linear approximations in\ncombination with branch-and-bound Lipschitz optimisation. The method obtains\nprovably tighter over-approximations of the perturbation region than the\npresent state-of-the-art. We report results from experiments on a comprehensive\nset of verification benchmarks on MNIST and CIFAR10. We show that our proposed\nimplementation resolves up to 32% more verification cases than present\napproaches.\n", "link": "http://arxiv.org/abs/2408.13140v2", "date": "2024-08-29", "relevancy": 2.0614, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5342}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5083}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verification%20of%20Geometric%20Robustness%20of%20Neural%20Networks%20via%20Piecewise%0A%20%20Linear%20Approximation%20and%20Lipschitz%20Optimisation&body=Title%3A%20Verification%20of%20Geometric%20Robustness%20of%20Neural%20Networks%20via%20Piecewise%0A%20%20Linear%20Approximation%20and%20Lipschitz%20Optimisation%0AAuthor%3A%20Ben%20Batten%20and%20Yang%20Zheng%20and%20Alessandro%20De%20Palma%20and%20Panagiotis%20Kouvaros%20and%20Alessio%20Lomuscio%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20verifying%20neural%20networks%20against%20geometric%0Atransformations%20of%20the%20input%20image%2C%20including%20rotation%2C%20scaling%2C%20shearing%2C%20and%0Atranslation.%20The%20proposed%20method%20computes%20provably%20sound%20piecewise%20linear%0Aconstraints%20for%20the%20pixel%20values%20by%20using%20sampling%20and%20linear%20approximations%20in%0Acombination%20with%20branch-and-bound%20Lipschitz%20optimisation.%20The%20method%20obtains%0Aprovably%20tighter%20over-approximations%20of%20the%20perturbation%20region%20than%20the%0Apresent%20state-of-the-art.%20We%20report%20results%20from%20experiments%20on%20a%20comprehensive%0Aset%20of%20verification%20benchmarks%20on%20MNIST%20and%20CIFAR10.%20We%20show%20that%20our%20proposed%0Aimplementation%20resolves%20up%20to%2032%25%20more%20verification%20cases%20than%20present%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerification%2520of%2520Geometric%2520Robustness%2520of%2520Neural%2520Networks%2520via%2520Piecewise%250A%2520%2520Linear%2520Approximation%2520and%2520Lipschitz%2520Optimisation%26entry.906535625%3DBen%2520Batten%2520and%2520Yang%2520Zheng%2520and%2520Alessandro%2520De%2520Palma%2520and%2520Panagiotis%2520Kouvaros%2520and%2520Alessio%2520Lomuscio%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520verifying%2520neural%2520networks%2520against%2520geometric%250Atransformations%2520of%2520the%2520input%2520image%252C%2520including%2520rotation%252C%2520scaling%252C%2520shearing%252C%2520and%250Atranslation.%2520The%2520proposed%2520method%2520computes%2520provably%2520sound%2520piecewise%2520linear%250Aconstraints%2520for%2520the%2520pixel%2520values%2520by%2520using%2520sampling%2520and%2520linear%2520approximations%2520in%250Acombination%2520with%2520branch-and-bound%2520Lipschitz%2520optimisation.%2520The%2520method%2520obtains%250Aprovably%2520tighter%2520over-approximations%2520of%2520the%2520perturbation%2520region%2520than%2520the%250Apresent%2520state-of-the-art.%2520We%2520report%2520results%2520from%2520experiments%2520on%2520a%2520comprehensive%250Aset%2520of%2520verification%2520benchmarks%2520on%2520MNIST%2520and%2520CIFAR10.%2520We%2520show%2520that%2520our%2520proposed%250Aimplementation%2520resolves%2520up%2520to%252032%2525%2520more%2520verification%2520cases%2520than%2520present%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verification%20of%20Geometric%20Robustness%20of%20Neural%20Networks%20via%20Piecewise%0A%20%20Linear%20Approximation%20and%20Lipschitz%20Optimisation&entry.906535625=Ben%20Batten%20and%20Yang%20Zheng%20and%20Alessandro%20De%20Palma%20and%20Panagiotis%20Kouvaros%20and%20Alessio%20Lomuscio&entry.1292438233=%20%20We%20address%20the%20problem%20of%20verifying%20neural%20networks%20against%20geometric%0Atransformations%20of%20the%20input%20image%2C%20including%20rotation%2C%20scaling%2C%20shearing%2C%20and%0Atranslation.%20The%20proposed%20method%20computes%20provably%20sound%20piecewise%20linear%0Aconstraints%20for%20the%20pixel%20values%20by%20using%20sampling%20and%20linear%20approximations%20in%0Acombination%20with%20branch-and-bound%20Lipschitz%20optimisation.%20The%20method%20obtains%0Aprovably%20tighter%20over-approximations%20of%20the%20perturbation%20region%20than%20the%0Apresent%20state-of-the-art.%20We%20report%20results%20from%20experiments%20on%20a%20comprehensive%0Aset%20of%20verification%20benchmarks%20on%20MNIST%20and%20CIFAR10.%20We%20show%20that%20our%20proposed%0Aimplementation%20resolves%20up%20to%2032%25%20more%20verification%20cases%20than%20present%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13140v2&entry.124074799=Read"},
{"title": "Dissecting Out-of-Distribution Detection and Open-Set Recognition: A\n  Critical Analysis of Methods and Benchmarks", "author": "Hongjun Wang and Sagar Vaze and Kai Han", "abstract": "  Detecting test-time distribution shift has emerged as a key capability for\nsafely deployed machine learning models, with the question being tackled under\nvarious guises in recent years. In this paper, we aim to provide a consolidated\nview of the two largest sub-fields within the community: out-of-distribution\n(OOD) detection and open-set recognition (OSR). In particular, we aim to\nprovide rigorous empirical analysis of different methods across settings and\nprovide actionable takeaways for practitioners and researchers. Concretely, we\nmake the following contributions: (i) We perform rigorous cross-evaluation\nbetween state-of-the-art methods in the OOD detection and OSR settings and\nidentify a strong correlation between the performances of methods for them;\n(ii) We propose a new, large-scale benchmark setting which we suggest better\ndisentangles the problem tackled by OOD detection and OSR, re-evaluating\nstate-of-the-art OOD detection and OSR methods in this setting; (iii) We\nsurprisingly find that the best performing method on standard benchmarks\n(Outlier Exposure) struggles when tested at scale, while scoring rules which\nare sensitive to the deep feature magnitude consistently show promise; and (iv)\nWe conduct empirical analysis to explain these phenomena and highlight\ndirections for future research. Code:\n\\url{https://github.com/Visual-AI/Dissect-OOD-OSR}\n", "link": "http://arxiv.org/abs/2408.16757v1", "date": "2024-08-29", "relevancy": 2.0598, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5232}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5162}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dissecting%20Out-of-Distribution%20Detection%20and%20Open-Set%20Recognition%3A%20A%0A%20%20Critical%20Analysis%20of%20Methods%20and%20Benchmarks&body=Title%3A%20Dissecting%20Out-of-Distribution%20Detection%20and%20Open-Set%20Recognition%3A%20A%0A%20%20Critical%20Analysis%20of%20Methods%20and%20Benchmarks%0AAuthor%3A%20Hongjun%20Wang%20and%20Sagar%20Vaze%20and%20Kai%20Han%0AAbstract%3A%20%20%20Detecting%20test-time%20distribution%20shift%20has%20emerged%20as%20a%20key%20capability%20for%0Asafely%20deployed%20machine%20learning%20models%2C%20with%20the%20question%20being%20tackled%20under%0Avarious%20guises%20in%20recent%20years.%20In%20this%20paper%2C%20we%20aim%20to%20provide%20a%20consolidated%0Aview%20of%20the%20two%20largest%20sub-fields%20within%20the%20community%3A%20out-of-distribution%0A%28OOD%29%20detection%20and%20open-set%20recognition%20%28OSR%29.%20In%20particular%2C%20we%20aim%20to%0Aprovide%20rigorous%20empirical%20analysis%20of%20different%20methods%20across%20settings%20and%0Aprovide%20actionable%20takeaways%20for%20practitioners%20and%20researchers.%20Concretely%2C%20we%0Amake%20the%20following%20contributions%3A%20%28i%29%20We%20perform%20rigorous%20cross-evaluation%0Abetween%20state-of-the-art%20methods%20in%20the%20OOD%20detection%20and%20OSR%20settings%20and%0Aidentify%20a%20strong%20correlation%20between%20the%20performances%20of%20methods%20for%20them%3B%0A%28ii%29%20We%20propose%20a%20new%2C%20large-scale%20benchmark%20setting%20which%20we%20suggest%20better%0Adisentangles%20the%20problem%20tackled%20by%20OOD%20detection%20and%20OSR%2C%20re-evaluating%0Astate-of-the-art%20OOD%20detection%20and%20OSR%20methods%20in%20this%20setting%3B%20%28iii%29%20We%0Asurprisingly%20find%20that%20the%20best%20performing%20method%20on%20standard%20benchmarks%0A%28Outlier%20Exposure%29%20struggles%20when%20tested%20at%20scale%2C%20while%20scoring%20rules%20which%0Aare%20sensitive%20to%20the%20deep%20feature%20magnitude%20consistently%20show%20promise%3B%20and%20%28iv%29%0AWe%20conduct%20empirical%20analysis%20to%20explain%20these%20phenomena%20and%20highlight%0Adirections%20for%20future%20research.%20Code%3A%0A%5Curl%7Bhttps%3A//github.com/Visual-AI/Dissect-OOD-OSR%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDissecting%2520Out-of-Distribution%2520Detection%2520and%2520Open-Set%2520Recognition%253A%2520A%250A%2520%2520Critical%2520Analysis%2520of%2520Methods%2520and%2520Benchmarks%26entry.906535625%3DHongjun%2520Wang%2520and%2520Sagar%2520Vaze%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520Detecting%2520test-time%2520distribution%2520shift%2520has%2520emerged%2520as%2520a%2520key%2520capability%2520for%250Asafely%2520deployed%2520machine%2520learning%2520models%252C%2520with%2520the%2520question%2520being%2520tackled%2520under%250Avarious%2520guises%2520in%2520recent%2520years.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520provide%2520a%2520consolidated%250Aview%2520of%2520the%2520two%2520largest%2520sub-fields%2520within%2520the%2520community%253A%2520out-of-distribution%250A%2528OOD%2529%2520detection%2520and%2520open-set%2520recognition%2520%2528OSR%2529.%2520In%2520particular%252C%2520we%2520aim%2520to%250Aprovide%2520rigorous%2520empirical%2520analysis%2520of%2520different%2520methods%2520across%2520settings%2520and%250Aprovide%2520actionable%2520takeaways%2520for%2520practitioners%2520and%2520researchers.%2520Concretely%252C%2520we%250Amake%2520the%2520following%2520contributions%253A%2520%2528i%2529%2520We%2520perform%2520rigorous%2520cross-evaluation%250Abetween%2520state-of-the-art%2520methods%2520in%2520the%2520OOD%2520detection%2520and%2520OSR%2520settings%2520and%250Aidentify%2520a%2520strong%2520correlation%2520between%2520the%2520performances%2520of%2520methods%2520for%2520them%253B%250A%2528ii%2529%2520We%2520propose%2520a%2520new%252C%2520large-scale%2520benchmark%2520setting%2520which%2520we%2520suggest%2520better%250Adisentangles%2520the%2520problem%2520tackled%2520by%2520OOD%2520detection%2520and%2520OSR%252C%2520re-evaluating%250Astate-of-the-art%2520OOD%2520detection%2520and%2520OSR%2520methods%2520in%2520this%2520setting%253B%2520%2528iii%2529%2520We%250Asurprisingly%2520find%2520that%2520the%2520best%2520performing%2520method%2520on%2520standard%2520benchmarks%250A%2528Outlier%2520Exposure%2529%2520struggles%2520when%2520tested%2520at%2520scale%252C%2520while%2520scoring%2520rules%2520which%250Aare%2520sensitive%2520to%2520the%2520deep%2520feature%2520magnitude%2520consistently%2520show%2520promise%253B%2520and%2520%2528iv%2529%250AWe%2520conduct%2520empirical%2520analysis%2520to%2520explain%2520these%2520phenomena%2520and%2520highlight%250Adirections%2520for%2520future%2520research.%2520Code%253A%250A%255Curl%257Bhttps%253A//github.com/Visual-AI/Dissect-OOD-OSR%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20Out-of-Distribution%20Detection%20and%20Open-Set%20Recognition%3A%20A%0A%20%20Critical%20Analysis%20of%20Methods%20and%20Benchmarks&entry.906535625=Hongjun%20Wang%20and%20Sagar%20Vaze%20and%20Kai%20Han&entry.1292438233=%20%20Detecting%20test-time%20distribution%20shift%20has%20emerged%20as%20a%20key%20capability%20for%0Asafely%20deployed%20machine%20learning%20models%2C%20with%20the%20question%20being%20tackled%20under%0Avarious%20guises%20in%20recent%20years.%20In%20this%20paper%2C%20we%20aim%20to%20provide%20a%20consolidated%0Aview%20of%20the%20two%20largest%20sub-fields%20within%20the%20community%3A%20out-of-distribution%0A%28OOD%29%20detection%20and%20open-set%20recognition%20%28OSR%29.%20In%20particular%2C%20we%20aim%20to%0Aprovide%20rigorous%20empirical%20analysis%20of%20different%20methods%20across%20settings%20and%0Aprovide%20actionable%20takeaways%20for%20practitioners%20and%20researchers.%20Concretely%2C%20we%0Amake%20the%20following%20contributions%3A%20%28i%29%20We%20perform%20rigorous%20cross-evaluation%0Abetween%20state-of-the-art%20methods%20in%20the%20OOD%20detection%20and%20OSR%20settings%20and%0Aidentify%20a%20strong%20correlation%20between%20the%20performances%20of%20methods%20for%20them%3B%0A%28ii%29%20We%20propose%20a%20new%2C%20large-scale%20benchmark%20setting%20which%20we%20suggest%20better%0Adisentangles%20the%20problem%20tackled%20by%20OOD%20detection%20and%20OSR%2C%20re-evaluating%0Astate-of-the-art%20OOD%20detection%20and%20OSR%20methods%20in%20this%20setting%3B%20%28iii%29%20We%0Asurprisingly%20find%20that%20the%20best%20performing%20method%20on%20standard%20benchmarks%0A%28Outlier%20Exposure%29%20struggles%20when%20tested%20at%20scale%2C%20while%20scoring%20rules%20which%0Aare%20sensitive%20to%20the%20deep%20feature%20magnitude%20consistently%20show%20promise%3B%20and%20%28iv%29%0AWe%20conduct%20empirical%20analysis%20to%20explain%20these%20phenomena%20and%20highlight%0Adirections%20for%20future%20research.%20Code%3A%0A%5Curl%7Bhttps%3A//github.com/Visual-AI/Dissect-OOD-OSR%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16757v1&entry.124074799=Read"},
{"title": "sEMG-Driven Physics-Informed Gated Recurrent Networks for Modeling Upper\n  Limb Multi-Joint Movement Dynamics", "author": "Rajnish Kumar and Anand Gupta and Suriya Prakash Muthukrishnan and Lalan Kumar and Sitikantha Roy", "abstract": "  Exoskeletons and rehabilitation systems offer great potential for enhancing\nhuman strength and recovery through advanced human-machine interfaces (HMIs)\nthat adapt to movement dynamics. However, the real-time application of\nphysics-informed neural networks (PINNs) is limited by their reliance on fixed\ninput lengths and surrogate models. This study introduces a novel\nphysics-informed Gated Recurrent Network (PiGRN) designed to predict\nmulti-joint torques using surface electromyography (sEMG) data. The PiGRN model\nemploys a Gated Recurrent Unit (GRU) to convert time-series sEMG inputs into\nmulti-joint kinematics and external loads, which are then integrated into an\nequation of motion to ensure consistency with physical laws. Experimental\nvalidation with sEMG data from five participants performing elbow\nflexion-extension tasks showed that the PiGRN model accurately predicted joint\ntorques for 10 unfamiliar movements, with RMSE values between 4.02\\% and\n11.40\\% and correlation coefficients ranging from 0.87 to 0.98. These findings\nhighlight the PiGRN's potential for real-time exoskeleton and rehabilitation\napplications. Future research will explore more diverse datasets, improve\nmusculoskeletal models, and investigate unsupervised learning methods.\n", "link": "http://arxiv.org/abs/2408.16599v1", "date": "2024-08-29", "relevancy": 2.0568, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5746}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5266}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20sEMG-Driven%20Physics-Informed%20Gated%20Recurrent%20Networks%20for%20Modeling%20Upper%0A%20%20Limb%20Multi-Joint%20Movement%20Dynamics&body=Title%3A%20sEMG-Driven%20Physics-Informed%20Gated%20Recurrent%20Networks%20for%20Modeling%20Upper%0A%20%20Limb%20Multi-Joint%20Movement%20Dynamics%0AAuthor%3A%20Rajnish%20Kumar%20and%20Anand%20Gupta%20and%20Suriya%20Prakash%20Muthukrishnan%20and%20Lalan%20Kumar%20and%20Sitikantha%20Roy%0AAbstract%3A%20%20%20Exoskeletons%20and%20rehabilitation%20systems%20offer%20great%20potential%20for%20enhancing%0Ahuman%20strength%20and%20recovery%20through%20advanced%20human-machine%20interfaces%20%28HMIs%29%0Athat%20adapt%20to%20movement%20dynamics.%20However%2C%20the%20real-time%20application%20of%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20is%20limited%20by%20their%20reliance%20on%20fixed%0Ainput%20lengths%20and%20surrogate%20models.%20This%20study%20introduces%20a%20novel%0Aphysics-informed%20Gated%20Recurrent%20Network%20%28PiGRN%29%20designed%20to%20predict%0Amulti-joint%20torques%20using%20surface%20electromyography%20%28sEMG%29%20data.%20The%20PiGRN%20model%0Aemploys%20a%20Gated%20Recurrent%20Unit%20%28GRU%29%20to%20convert%20time-series%20sEMG%20inputs%20into%0Amulti-joint%20kinematics%20and%20external%20loads%2C%20which%20are%20then%20integrated%20into%20an%0Aequation%20of%20motion%20to%20ensure%20consistency%20with%20physical%20laws.%20Experimental%0Avalidation%20with%20sEMG%20data%20from%20five%20participants%20performing%20elbow%0Aflexion-extension%20tasks%20showed%20that%20the%20PiGRN%20model%20accurately%20predicted%20joint%0Atorques%20for%2010%20unfamiliar%20movements%2C%20with%20RMSE%20values%20between%204.02%5C%25%20and%0A11.40%5C%25%20and%20correlation%20coefficients%20ranging%20from%200.87%20to%200.98.%20These%20findings%0Ahighlight%20the%20PiGRN%27s%20potential%20for%20real-time%20exoskeleton%20and%20rehabilitation%0Aapplications.%20Future%20research%20will%20explore%20more%20diverse%20datasets%2C%20improve%0Amusculoskeletal%20models%2C%20and%20investigate%20unsupervised%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DsEMG-Driven%2520Physics-Informed%2520Gated%2520Recurrent%2520Networks%2520for%2520Modeling%2520Upper%250A%2520%2520Limb%2520Multi-Joint%2520Movement%2520Dynamics%26entry.906535625%3DRajnish%2520Kumar%2520and%2520Anand%2520Gupta%2520and%2520Suriya%2520Prakash%2520Muthukrishnan%2520and%2520Lalan%2520Kumar%2520and%2520Sitikantha%2520Roy%26entry.1292438233%3D%2520%2520Exoskeletons%2520and%2520rehabilitation%2520systems%2520offer%2520great%2520potential%2520for%2520enhancing%250Ahuman%2520strength%2520and%2520recovery%2520through%2520advanced%2520human-machine%2520interfaces%2520%2528HMIs%2529%250Athat%2520adapt%2520to%2520movement%2520dynamics.%2520However%252C%2520the%2520real-time%2520application%2520of%250Aphysics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520is%2520limited%2520by%2520their%2520reliance%2520on%2520fixed%250Ainput%2520lengths%2520and%2520surrogate%2520models.%2520This%2520study%2520introduces%2520a%2520novel%250Aphysics-informed%2520Gated%2520Recurrent%2520Network%2520%2528PiGRN%2529%2520designed%2520to%2520predict%250Amulti-joint%2520torques%2520using%2520surface%2520electromyography%2520%2528sEMG%2529%2520data.%2520The%2520PiGRN%2520model%250Aemploys%2520a%2520Gated%2520Recurrent%2520Unit%2520%2528GRU%2529%2520to%2520convert%2520time-series%2520sEMG%2520inputs%2520into%250Amulti-joint%2520kinematics%2520and%2520external%2520loads%252C%2520which%2520are%2520then%2520integrated%2520into%2520an%250Aequation%2520of%2520motion%2520to%2520ensure%2520consistency%2520with%2520physical%2520laws.%2520Experimental%250Avalidation%2520with%2520sEMG%2520data%2520from%2520five%2520participants%2520performing%2520elbow%250Aflexion-extension%2520tasks%2520showed%2520that%2520the%2520PiGRN%2520model%2520accurately%2520predicted%2520joint%250Atorques%2520for%252010%2520unfamiliar%2520movements%252C%2520with%2520RMSE%2520values%2520between%25204.02%255C%2525%2520and%250A11.40%255C%2525%2520and%2520correlation%2520coefficients%2520ranging%2520from%25200.87%2520to%25200.98.%2520These%2520findings%250Ahighlight%2520the%2520PiGRN%2527s%2520potential%2520for%2520real-time%2520exoskeleton%2520and%2520rehabilitation%250Aapplications.%2520Future%2520research%2520will%2520explore%2520more%2520diverse%2520datasets%252C%2520improve%250Amusculoskeletal%2520models%252C%2520and%2520investigate%2520unsupervised%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=sEMG-Driven%20Physics-Informed%20Gated%20Recurrent%20Networks%20for%20Modeling%20Upper%0A%20%20Limb%20Multi-Joint%20Movement%20Dynamics&entry.906535625=Rajnish%20Kumar%20and%20Anand%20Gupta%20and%20Suriya%20Prakash%20Muthukrishnan%20and%20Lalan%20Kumar%20and%20Sitikantha%20Roy&entry.1292438233=%20%20Exoskeletons%20and%20rehabilitation%20systems%20offer%20great%20potential%20for%20enhancing%0Ahuman%20strength%20and%20recovery%20through%20advanced%20human-machine%20interfaces%20%28HMIs%29%0Athat%20adapt%20to%20movement%20dynamics.%20However%2C%20the%20real-time%20application%20of%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20is%20limited%20by%20their%20reliance%20on%20fixed%0Ainput%20lengths%20and%20surrogate%20models.%20This%20study%20introduces%20a%20novel%0Aphysics-informed%20Gated%20Recurrent%20Network%20%28PiGRN%29%20designed%20to%20predict%0Amulti-joint%20torques%20using%20surface%20electromyography%20%28sEMG%29%20data.%20The%20PiGRN%20model%0Aemploys%20a%20Gated%20Recurrent%20Unit%20%28GRU%29%20to%20convert%20time-series%20sEMG%20inputs%20into%0Amulti-joint%20kinematics%20and%20external%20loads%2C%20which%20are%20then%20integrated%20into%20an%0Aequation%20of%20motion%20to%20ensure%20consistency%20with%20physical%20laws.%20Experimental%0Avalidation%20with%20sEMG%20data%20from%20five%20participants%20performing%20elbow%0Aflexion-extension%20tasks%20showed%20that%20the%20PiGRN%20model%20accurately%20predicted%20joint%0Atorques%20for%2010%20unfamiliar%20movements%2C%20with%20RMSE%20values%20between%204.02%5C%25%20and%0A11.40%5C%25%20and%20correlation%20coefficients%20ranging%20from%200.87%20to%200.98.%20These%20findings%0Ahighlight%20the%20PiGRN%27s%20potential%20for%20real-time%20exoskeleton%20and%20rehabilitation%0Aapplications.%20Future%20research%20will%20explore%20more%20diverse%20datasets%2C%20improve%0Amusculoskeletal%20models%2C%20and%20investigate%20unsupervised%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16599v1&entry.124074799=Read"},
{"title": "Data Quality Monitoring through Transfer Learning on Anomaly Detection\n  for the Hadron Calorimeters", "author": "Mulugeta Weldezgina Asres and Christian Walter Omlin and Long Wang and Pavel Parygin and David Yu and Jay Dittmann and The CMS-HCAL Collaboration", "abstract": "  The proliferation of sensors brings an immense volume of spatio-temporal (ST)\ndata in many domains for various purposes, including monitoring, diagnostics,\nand prognostics applications. Data curation is a time-consuming process for a\nlarge volume of data, making it challenging and expensive to deploy data\nanalytics platforms in new environments. Transfer learning (TL) mechanisms\npromise to mitigate data sparsity and model complexity by utilizing pre-trained\nmodels for a new task. Despite the triumph of TL in fields like computer vision\nand natural language processing, efforts on complex ST models for anomaly\ndetection (AD) applications are limited. In this study, we present the\npotential of TL within the context of AD for the Hadron Calorimeter of the\nCompact Muon Solenoid experiment at CERN. We have transferred the ST AD models\ntrained on data collected from one part of a calorimeter to another. We have\ninvestigated different configurations of TL on semi-supervised autoencoders of\nthe ST AD models -- transferring convolutional, graph, and recurrent neural\nnetworks of both the encoder and decoder networks. The experiment results\ndemonstrate that TL effectively enhances the model learning accuracy on a\ntarget subdetector. The TL achieves promising data reconstruction and AD\nperformance while substantially reducing the trainable parameters of the AD\nmodels. It also improves robustness against anomaly contamination in the\ntraining data sets of the semi-supervised AD models.\n", "link": "http://arxiv.org/abs/2408.16612v1", "date": "2024-08-29", "relevancy": 2.0503, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5155}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5123}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Quality%20Monitoring%20through%20Transfer%20Learning%20on%20Anomaly%20Detection%0A%20%20for%20the%20Hadron%20Calorimeters&body=Title%3A%20Data%20Quality%20Monitoring%20through%20Transfer%20Learning%20on%20Anomaly%20Detection%0A%20%20for%20the%20Hadron%20Calorimeters%0AAuthor%3A%20Mulugeta%20Weldezgina%20Asres%20and%20Christian%20Walter%20Omlin%20and%20Long%20Wang%20and%20Pavel%20Parygin%20and%20David%20Yu%20and%20Jay%20Dittmann%20and%20The%20CMS-HCAL%20Collaboration%0AAbstract%3A%20%20%20The%20proliferation%20of%20sensors%20brings%20an%20immense%20volume%20of%20spatio-temporal%20%28ST%29%0Adata%20in%20many%20domains%20for%20various%20purposes%2C%20including%20monitoring%2C%20diagnostics%2C%0Aand%20prognostics%20applications.%20Data%20curation%20is%20a%20time-consuming%20process%20for%20a%0Alarge%20volume%20of%20data%2C%20making%20it%20challenging%20and%20expensive%20to%20deploy%20data%0Aanalytics%20platforms%20in%20new%20environments.%20Transfer%20learning%20%28TL%29%20mechanisms%0Apromise%20to%20mitigate%20data%20sparsity%20and%20model%20complexity%20by%20utilizing%20pre-trained%0Amodels%20for%20a%20new%20task.%20Despite%20the%20triumph%20of%20TL%20in%20fields%20like%20computer%20vision%0Aand%20natural%20language%20processing%2C%20efforts%20on%20complex%20ST%20models%20for%20anomaly%0Adetection%20%28AD%29%20applications%20are%20limited.%20In%20this%20study%2C%20we%20present%20the%0Apotential%20of%20TL%20within%20the%20context%20of%20AD%20for%20the%20Hadron%20Calorimeter%20of%20the%0ACompact%20Muon%20Solenoid%20experiment%20at%20CERN.%20We%20have%20transferred%20the%20ST%20AD%20models%0Atrained%20on%20data%20collected%20from%20one%20part%20of%20a%20calorimeter%20to%20another.%20We%20have%0Ainvestigated%20different%20configurations%20of%20TL%20on%20semi-supervised%20autoencoders%20of%0Athe%20ST%20AD%20models%20--%20transferring%20convolutional%2C%20graph%2C%20and%20recurrent%20neural%0Anetworks%20of%20both%20the%20encoder%20and%20decoder%20networks.%20The%20experiment%20results%0Ademonstrate%20that%20TL%20effectively%20enhances%20the%20model%20learning%20accuracy%20on%20a%0Atarget%20subdetector.%20The%20TL%20achieves%20promising%20data%20reconstruction%20and%20AD%0Aperformance%20while%20substantially%20reducing%20the%20trainable%20parameters%20of%20the%20AD%0Amodels.%20It%20also%20improves%20robustness%20against%20anomaly%20contamination%20in%20the%0Atraining%20data%20sets%20of%20the%20semi-supervised%20AD%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Quality%2520Monitoring%2520through%2520Transfer%2520Learning%2520on%2520Anomaly%2520Detection%250A%2520%2520for%2520the%2520Hadron%2520Calorimeters%26entry.906535625%3DMulugeta%2520Weldezgina%2520Asres%2520and%2520Christian%2520Walter%2520Omlin%2520and%2520Long%2520Wang%2520and%2520Pavel%2520Parygin%2520and%2520David%2520Yu%2520and%2520Jay%2520Dittmann%2520and%2520The%2520CMS-HCAL%2520Collaboration%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520sensors%2520brings%2520an%2520immense%2520volume%2520of%2520spatio-temporal%2520%2528ST%2529%250Adata%2520in%2520many%2520domains%2520for%2520various%2520purposes%252C%2520including%2520monitoring%252C%2520diagnostics%252C%250Aand%2520prognostics%2520applications.%2520Data%2520curation%2520is%2520a%2520time-consuming%2520process%2520for%2520a%250Alarge%2520volume%2520of%2520data%252C%2520making%2520it%2520challenging%2520and%2520expensive%2520to%2520deploy%2520data%250Aanalytics%2520platforms%2520in%2520new%2520environments.%2520Transfer%2520learning%2520%2528TL%2529%2520mechanisms%250Apromise%2520to%2520mitigate%2520data%2520sparsity%2520and%2520model%2520complexity%2520by%2520utilizing%2520pre-trained%250Amodels%2520for%2520a%2520new%2520task.%2520Despite%2520the%2520triumph%2520of%2520TL%2520in%2520fields%2520like%2520computer%2520vision%250Aand%2520natural%2520language%2520processing%252C%2520efforts%2520on%2520complex%2520ST%2520models%2520for%2520anomaly%250Adetection%2520%2528AD%2529%2520applications%2520are%2520limited.%2520In%2520this%2520study%252C%2520we%2520present%2520the%250Apotential%2520of%2520TL%2520within%2520the%2520context%2520of%2520AD%2520for%2520the%2520Hadron%2520Calorimeter%2520of%2520the%250ACompact%2520Muon%2520Solenoid%2520experiment%2520at%2520CERN.%2520We%2520have%2520transferred%2520the%2520ST%2520AD%2520models%250Atrained%2520on%2520data%2520collected%2520from%2520one%2520part%2520of%2520a%2520calorimeter%2520to%2520another.%2520We%2520have%250Ainvestigated%2520different%2520configurations%2520of%2520TL%2520on%2520semi-supervised%2520autoencoders%2520of%250Athe%2520ST%2520AD%2520models%2520--%2520transferring%2520convolutional%252C%2520graph%252C%2520and%2520recurrent%2520neural%250Anetworks%2520of%2520both%2520the%2520encoder%2520and%2520decoder%2520networks.%2520The%2520experiment%2520results%250Ademonstrate%2520that%2520TL%2520effectively%2520enhances%2520the%2520model%2520learning%2520accuracy%2520on%2520a%250Atarget%2520subdetector.%2520The%2520TL%2520achieves%2520promising%2520data%2520reconstruction%2520and%2520AD%250Aperformance%2520while%2520substantially%2520reducing%2520the%2520trainable%2520parameters%2520of%2520the%2520AD%250Amodels.%2520It%2520also%2520improves%2520robustness%2520against%2520anomaly%2520contamination%2520in%2520the%250Atraining%2520data%2520sets%2520of%2520the%2520semi-supervised%2520AD%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Quality%20Monitoring%20through%20Transfer%20Learning%20on%20Anomaly%20Detection%0A%20%20for%20the%20Hadron%20Calorimeters&entry.906535625=Mulugeta%20Weldezgina%20Asres%20and%20Christian%20Walter%20Omlin%20and%20Long%20Wang%20and%20Pavel%20Parygin%20and%20David%20Yu%20and%20Jay%20Dittmann%20and%20The%20CMS-HCAL%20Collaboration&entry.1292438233=%20%20The%20proliferation%20of%20sensors%20brings%20an%20immense%20volume%20of%20spatio-temporal%20%28ST%29%0Adata%20in%20many%20domains%20for%20various%20purposes%2C%20including%20monitoring%2C%20diagnostics%2C%0Aand%20prognostics%20applications.%20Data%20curation%20is%20a%20time-consuming%20process%20for%20a%0Alarge%20volume%20of%20data%2C%20making%20it%20challenging%20and%20expensive%20to%20deploy%20data%0Aanalytics%20platforms%20in%20new%20environments.%20Transfer%20learning%20%28TL%29%20mechanisms%0Apromise%20to%20mitigate%20data%20sparsity%20and%20model%20complexity%20by%20utilizing%20pre-trained%0Amodels%20for%20a%20new%20task.%20Despite%20the%20triumph%20of%20TL%20in%20fields%20like%20computer%20vision%0Aand%20natural%20language%20processing%2C%20efforts%20on%20complex%20ST%20models%20for%20anomaly%0Adetection%20%28AD%29%20applications%20are%20limited.%20In%20this%20study%2C%20we%20present%20the%0Apotential%20of%20TL%20within%20the%20context%20of%20AD%20for%20the%20Hadron%20Calorimeter%20of%20the%0ACompact%20Muon%20Solenoid%20experiment%20at%20CERN.%20We%20have%20transferred%20the%20ST%20AD%20models%0Atrained%20on%20data%20collected%20from%20one%20part%20of%20a%20calorimeter%20to%20another.%20We%20have%0Ainvestigated%20different%20configurations%20of%20TL%20on%20semi-supervised%20autoencoders%20of%0Athe%20ST%20AD%20models%20--%20transferring%20convolutional%2C%20graph%2C%20and%20recurrent%20neural%0Anetworks%20of%20both%20the%20encoder%20and%20decoder%20networks.%20The%20experiment%20results%0Ademonstrate%20that%20TL%20effectively%20enhances%20the%20model%20learning%20accuracy%20on%20a%0Atarget%20subdetector.%20The%20TL%20achieves%20promising%20data%20reconstruction%20and%20AD%0Aperformance%20while%20substantially%20reducing%20the%20trainable%20parameters%20of%20the%20AD%0Amodels.%20It%20also%20improves%20robustness%20against%20anomaly%20contamination%20in%20the%0Atraining%20data%20sets%20of%20the%20semi-supervised%20AD%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16612v1&entry.124074799=Read"},
{"title": "Weakly Supervised Object Detection for Automatic Tooth-marked Tongue\n  Recognition", "author": "Yongcun Zhang and Jiajun Xu and Yina He and Shaozi Li and Zhiming Luo and Huangwei Lei", "abstract": "  Tongue diagnosis in Traditional Chinese Medicine (TCM) is a crucial\ndiagnostic method that can reflect an individual's health status. Traditional\nmethods for identifying tooth-marked tongues are subjective and inconsistent\nbecause they rely on practitioner experience. We propose a novel fully\nautomated Weakly Supervised method using Vision transformer and Multiple\ninstance learning WSVM for tongue extraction and tooth-marked tongue\nrecognition. Our approach first accurately detects and extracts the tongue\nregion from clinical images, removing any irrelevant background information.\nThen, we implement an end-to-end weakly supervised object detection method. We\nutilize Vision Transformer (ViT) to process tongue images in patches and employ\nmultiple instance loss to identify tooth-marked regions with only image-level\nannotations. WSVM achieves high accuracy in tooth-marked tongue classification,\nand visualization experiments demonstrate its effectiveness in pinpointing\nthese regions. This automated approach enhances the objectivity and accuracy of\ntooth-marked tongue diagnosis. It provides significant clinical value by\nassisting TCM practitioners in making precise diagnoses and treatment\nrecommendations. Code is available at https://github.com/yc-zh/WSVM.\n", "link": "http://arxiv.org/abs/2408.16451v1", "date": "2024-08-29", "relevancy": 2.0453, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Object%20Detection%20for%20Automatic%20Tooth-marked%20Tongue%0A%20%20Recognition&body=Title%3A%20Weakly%20Supervised%20Object%20Detection%20for%20Automatic%20Tooth-marked%20Tongue%0A%20%20Recognition%0AAuthor%3A%20Yongcun%20Zhang%20and%20Jiajun%20Xu%20and%20Yina%20He%20and%20Shaozi%20Li%20and%20Zhiming%20Luo%20and%20Huangwei%20Lei%0AAbstract%3A%20%20%20Tongue%20diagnosis%20in%20Traditional%20Chinese%20Medicine%20%28TCM%29%20is%20a%20crucial%0Adiagnostic%20method%20that%20can%20reflect%20an%20individual%27s%20health%20status.%20Traditional%0Amethods%20for%20identifying%20tooth-marked%20tongues%20are%20subjective%20and%20inconsistent%0Abecause%20they%20rely%20on%20practitioner%20experience.%20We%20propose%20a%20novel%20fully%0Aautomated%20Weakly%20Supervised%20method%20using%20Vision%20transformer%20and%20Multiple%0Ainstance%20learning%20WSVM%20for%20tongue%20extraction%20and%20tooth-marked%20tongue%0Arecognition.%20Our%20approach%20first%20accurately%20detects%20and%20extracts%20the%20tongue%0Aregion%20from%20clinical%20images%2C%20removing%20any%20irrelevant%20background%20information.%0AThen%2C%20we%20implement%20an%20end-to-end%20weakly%20supervised%20object%20detection%20method.%20We%0Autilize%20Vision%20Transformer%20%28ViT%29%20to%20process%20tongue%20images%20in%20patches%20and%20employ%0Amultiple%20instance%20loss%20to%20identify%20tooth-marked%20regions%20with%20only%20image-level%0Aannotations.%20WSVM%20achieves%20high%20accuracy%20in%20tooth-marked%20tongue%20classification%2C%0Aand%20visualization%20experiments%20demonstrate%20its%20effectiveness%20in%20pinpointing%0Athese%20regions.%20This%20automated%20approach%20enhances%20the%20objectivity%20and%20accuracy%20of%0Atooth-marked%20tongue%20diagnosis.%20It%20provides%20significant%20clinical%20value%20by%0Aassisting%20TCM%20practitioners%20in%20making%20precise%20diagnoses%20and%20treatment%0Arecommendations.%20Code%20is%20available%20at%20https%3A//github.com/yc-zh/WSVM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Object%2520Detection%2520for%2520Automatic%2520Tooth-marked%2520Tongue%250A%2520%2520Recognition%26entry.906535625%3DYongcun%2520Zhang%2520and%2520Jiajun%2520Xu%2520and%2520Yina%2520He%2520and%2520Shaozi%2520Li%2520and%2520Zhiming%2520Luo%2520and%2520Huangwei%2520Lei%26entry.1292438233%3D%2520%2520Tongue%2520diagnosis%2520in%2520Traditional%2520Chinese%2520Medicine%2520%2528TCM%2529%2520is%2520a%2520crucial%250Adiagnostic%2520method%2520that%2520can%2520reflect%2520an%2520individual%2527s%2520health%2520status.%2520Traditional%250Amethods%2520for%2520identifying%2520tooth-marked%2520tongues%2520are%2520subjective%2520and%2520inconsistent%250Abecause%2520they%2520rely%2520on%2520practitioner%2520experience.%2520We%2520propose%2520a%2520novel%2520fully%250Aautomated%2520Weakly%2520Supervised%2520method%2520using%2520Vision%2520transformer%2520and%2520Multiple%250Ainstance%2520learning%2520WSVM%2520for%2520tongue%2520extraction%2520and%2520tooth-marked%2520tongue%250Arecognition.%2520Our%2520approach%2520first%2520accurately%2520detects%2520and%2520extracts%2520the%2520tongue%250Aregion%2520from%2520clinical%2520images%252C%2520removing%2520any%2520irrelevant%2520background%2520information.%250AThen%252C%2520we%2520implement%2520an%2520end-to-end%2520weakly%2520supervised%2520object%2520detection%2520method.%2520We%250Autilize%2520Vision%2520Transformer%2520%2528ViT%2529%2520to%2520process%2520tongue%2520images%2520in%2520patches%2520and%2520employ%250Amultiple%2520instance%2520loss%2520to%2520identify%2520tooth-marked%2520regions%2520with%2520only%2520image-level%250Aannotations.%2520WSVM%2520achieves%2520high%2520accuracy%2520in%2520tooth-marked%2520tongue%2520classification%252C%250Aand%2520visualization%2520experiments%2520demonstrate%2520its%2520effectiveness%2520in%2520pinpointing%250Athese%2520regions.%2520This%2520automated%2520approach%2520enhances%2520the%2520objectivity%2520and%2520accuracy%2520of%250Atooth-marked%2520tongue%2520diagnosis.%2520It%2520provides%2520significant%2520clinical%2520value%2520by%250Aassisting%2520TCM%2520practitioners%2520in%2520making%2520precise%2520diagnoses%2520and%2520treatment%250Arecommendations.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/yc-zh/WSVM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Object%20Detection%20for%20Automatic%20Tooth-marked%20Tongue%0A%20%20Recognition&entry.906535625=Yongcun%20Zhang%20and%20Jiajun%20Xu%20and%20Yina%20He%20and%20Shaozi%20Li%20and%20Zhiming%20Luo%20and%20Huangwei%20Lei&entry.1292438233=%20%20Tongue%20diagnosis%20in%20Traditional%20Chinese%20Medicine%20%28TCM%29%20is%20a%20crucial%0Adiagnostic%20method%20that%20can%20reflect%20an%20individual%27s%20health%20status.%20Traditional%0Amethods%20for%20identifying%20tooth-marked%20tongues%20are%20subjective%20and%20inconsistent%0Abecause%20they%20rely%20on%20practitioner%20experience.%20We%20propose%20a%20novel%20fully%0Aautomated%20Weakly%20Supervised%20method%20using%20Vision%20transformer%20and%20Multiple%0Ainstance%20learning%20WSVM%20for%20tongue%20extraction%20and%20tooth-marked%20tongue%0Arecognition.%20Our%20approach%20first%20accurately%20detects%20and%20extracts%20the%20tongue%0Aregion%20from%20clinical%20images%2C%20removing%20any%20irrelevant%20background%20information.%0AThen%2C%20we%20implement%20an%20end-to-end%20weakly%20supervised%20object%20detection%20method.%20We%0Autilize%20Vision%20Transformer%20%28ViT%29%20to%20process%20tongue%20images%20in%20patches%20and%20employ%0Amultiple%20instance%20loss%20to%20identify%20tooth-marked%20regions%20with%20only%20image-level%0Aannotations.%20WSVM%20achieves%20high%20accuracy%20in%20tooth-marked%20tongue%20classification%2C%0Aand%20visualization%20experiments%20demonstrate%20its%20effectiveness%20in%20pinpointing%0Athese%20regions.%20This%20automated%20approach%20enhances%20the%20objectivity%20and%20accuracy%20of%0Atooth-marked%20tongue%20diagnosis.%20It%20provides%20significant%20clinical%20value%20by%0Aassisting%20TCM%20practitioners%20in%20making%20precise%20diagnoses%20and%20treatment%0Arecommendations.%20Code%20is%20available%20at%20https%3A//github.com/yc-zh/WSVM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16451v1&entry.124074799=Read"},
{"title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio\n  Language Modeling", "author": "Shengpeng Ji and Ziyue Jiang and Xize Cheng and Yifu Chen and Minghui Fang and Jialong Zuo and Qian Yang and Ruiqi Li and Ziang Zhang and Xiaoda Yang and Rongjie Huang and Yidi Jiang and Qian Chen and Siqi Zheng and Wen Wang and Zhou Zhao", "abstract": "  Language models have been effectively applied to modeling natural signals,\nsuch as images, video, speech, and audio. A crucial component of these models\nis the codec tokenizer, which compresses high-dimensional natural signals into\nlower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,\nwhich offers several advantages over previous SOTA acoustic codec models in the\naudio domain: 1)extreme compression. By compressing the layers of quantizers\nand the temporal dimension of the discrete codec, one-second audio of 24kHz\nsampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved\nsubjective quality. Despite the reduced number of tokens, WavTokenizer achieves\nstate-of-the-art reconstruction quality with outstanding UTMOS scores and\ninherently contains richer semantic information. Specifically, we achieve these\nresults by designing a broader VQ space, extended contextual windows, and\nimproved attention networks, as well as introducing a powerful multi-scale\ndiscriminator and an inverse Fourier transform structure. We conducted\nextensive reconstruction experiments in the domains of speech, audio, and\nmusic. WavTokenizer exhibited strong performance across various objective and\nsubjective metrics compared to state-of-the-art models. We also tested semantic\ninformation, VQ utilization, and adaptability to generative models.\nComprehensive ablation studies confirm the necessity of each module in\nWavTokenizer. The related code, demos, and pre-trained models are available at\nhttps://github.com/jishengpeng/WavTokenizer.\n", "link": "http://arxiv.org/abs/2408.16532v1", "date": "2024-08-29", "relevancy": 2.0203, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5313}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4886}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WavTokenizer%3A%20an%20Efficient%20Acoustic%20Discrete%20Codec%20Tokenizer%20for%20Audio%0A%20%20Language%20Modeling&body=Title%3A%20WavTokenizer%3A%20an%20Efficient%20Acoustic%20Discrete%20Codec%20Tokenizer%20for%20Audio%0A%20%20Language%20Modeling%0AAuthor%3A%20Shengpeng%20Ji%20and%20Ziyue%20Jiang%20and%20Xize%20Cheng%20and%20Yifu%20Chen%20and%20Minghui%20Fang%20and%20Jialong%20Zuo%20and%20Qian%20Yang%20and%20Ruiqi%20Li%20and%20Ziang%20Zhang%20and%20Xiaoda%20Yang%20and%20Rongjie%20Huang%20and%20Yidi%20Jiang%20and%20Qian%20Chen%20and%20Siqi%20Zheng%20and%20Wen%20Wang%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Language%20models%20have%20been%20effectively%20applied%20to%20modeling%20natural%20signals%2C%0Asuch%20as%20images%2C%20video%2C%20speech%2C%20and%20audio.%20A%20crucial%20component%20of%20these%20models%0Ais%20the%20codec%20tokenizer%2C%20which%20compresses%20high-dimensional%20natural%20signals%20into%0Alower-dimensional%20discrete%20tokens.%20In%20this%20paper%2C%20we%20introduce%20WavTokenizer%2C%0Awhich%20offers%20several%20advantages%20over%20previous%20SOTA%20acoustic%20codec%20models%20in%20the%0Aaudio%20domain%3A%201%29extreme%20compression.%20By%20compressing%20the%20layers%20of%20quantizers%0Aand%20the%20temporal%20dimension%20of%20the%20discrete%20codec%2C%20one-second%20audio%20of%2024kHz%0Asampling%20rate%20requires%20only%20a%20single%20quantizer%20with%2040%20or%2075%20tokens.%202%29improved%0Asubjective%20quality.%20Despite%20the%20reduced%20number%20of%20tokens%2C%20WavTokenizer%20achieves%0Astate-of-the-art%20reconstruction%20quality%20with%20outstanding%20UTMOS%20scores%20and%0Ainherently%20contains%20richer%20semantic%20information.%20Specifically%2C%20we%20achieve%20these%0Aresults%20by%20designing%20a%20broader%20VQ%20space%2C%20extended%20contextual%20windows%2C%20and%0Aimproved%20attention%20networks%2C%20as%20well%20as%20introducing%20a%20powerful%20multi-scale%0Adiscriminator%20and%20an%20inverse%20Fourier%20transform%20structure.%20We%20conducted%0Aextensive%20reconstruction%20experiments%20in%20the%20domains%20of%20speech%2C%20audio%2C%20and%0Amusic.%20WavTokenizer%20exhibited%20strong%20performance%20across%20various%20objective%20and%0Asubjective%20metrics%20compared%20to%20state-of-the-art%20models.%20We%20also%20tested%20semantic%0Ainformation%2C%20VQ%20utilization%2C%20and%20adaptability%20to%20generative%20models.%0AComprehensive%20ablation%20studies%20confirm%20the%20necessity%20of%20each%20module%20in%0AWavTokenizer.%20The%20related%20code%2C%20demos%2C%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/jishengpeng/WavTokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavTokenizer%253A%2520an%2520Efficient%2520Acoustic%2520Discrete%2520Codec%2520Tokenizer%2520for%2520Audio%250A%2520%2520Language%2520Modeling%26entry.906535625%3DShengpeng%2520Ji%2520and%2520Ziyue%2520Jiang%2520and%2520Xize%2520Cheng%2520and%2520Yifu%2520Chen%2520and%2520Minghui%2520Fang%2520and%2520Jialong%2520Zuo%2520and%2520Qian%2520Yang%2520and%2520Ruiqi%2520Li%2520and%2520Ziang%2520Zhang%2520and%2520Xiaoda%2520Yang%2520and%2520Rongjie%2520Huang%2520and%2520Yidi%2520Jiang%2520and%2520Qian%2520Chen%2520and%2520Siqi%2520Zheng%2520and%2520Wen%2520Wang%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Language%2520models%2520have%2520been%2520effectively%2520applied%2520to%2520modeling%2520natural%2520signals%252C%250Asuch%2520as%2520images%252C%2520video%252C%2520speech%252C%2520and%2520audio.%2520A%2520crucial%2520component%2520of%2520these%2520models%250Ais%2520the%2520codec%2520tokenizer%252C%2520which%2520compresses%2520high-dimensional%2520natural%2520signals%2520into%250Alower-dimensional%2520discrete%2520tokens.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520WavTokenizer%252C%250Awhich%2520offers%2520several%2520advantages%2520over%2520previous%2520SOTA%2520acoustic%2520codec%2520models%2520in%2520the%250Aaudio%2520domain%253A%25201%2529extreme%2520compression.%2520By%2520compressing%2520the%2520layers%2520of%2520quantizers%250Aand%2520the%2520temporal%2520dimension%2520of%2520the%2520discrete%2520codec%252C%2520one-second%2520audio%2520of%252024kHz%250Asampling%2520rate%2520requires%2520only%2520a%2520single%2520quantizer%2520with%252040%2520or%252075%2520tokens.%25202%2529improved%250Asubjective%2520quality.%2520Despite%2520the%2520reduced%2520number%2520of%2520tokens%252C%2520WavTokenizer%2520achieves%250Astate-of-the-art%2520reconstruction%2520quality%2520with%2520outstanding%2520UTMOS%2520scores%2520and%250Ainherently%2520contains%2520richer%2520semantic%2520information.%2520Specifically%252C%2520we%2520achieve%2520these%250Aresults%2520by%2520designing%2520a%2520broader%2520VQ%2520space%252C%2520extended%2520contextual%2520windows%252C%2520and%250Aimproved%2520attention%2520networks%252C%2520as%2520well%2520as%2520introducing%2520a%2520powerful%2520multi-scale%250Adiscriminator%2520and%2520an%2520inverse%2520Fourier%2520transform%2520structure.%2520We%2520conducted%250Aextensive%2520reconstruction%2520experiments%2520in%2520the%2520domains%2520of%2520speech%252C%2520audio%252C%2520and%250Amusic.%2520WavTokenizer%2520exhibited%2520strong%2520performance%2520across%2520various%2520objective%2520and%250Asubjective%2520metrics%2520compared%2520to%2520state-of-the-art%2520models.%2520We%2520also%2520tested%2520semantic%250Ainformation%252C%2520VQ%2520utilization%252C%2520and%2520adaptability%2520to%2520generative%2520models.%250AComprehensive%2520ablation%2520studies%2520confirm%2520the%2520necessity%2520of%2520each%2520module%2520in%250AWavTokenizer.%2520The%2520related%2520code%252C%2520demos%252C%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/jishengpeng/WavTokenizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WavTokenizer%3A%20an%20Efficient%20Acoustic%20Discrete%20Codec%20Tokenizer%20for%20Audio%0A%20%20Language%20Modeling&entry.906535625=Shengpeng%20Ji%20and%20Ziyue%20Jiang%20and%20Xize%20Cheng%20and%20Yifu%20Chen%20and%20Minghui%20Fang%20and%20Jialong%20Zuo%20and%20Qian%20Yang%20and%20Ruiqi%20Li%20and%20Ziang%20Zhang%20and%20Xiaoda%20Yang%20and%20Rongjie%20Huang%20and%20Yidi%20Jiang%20and%20Qian%20Chen%20and%20Siqi%20Zheng%20and%20Wen%20Wang%20and%20Zhou%20Zhao&entry.1292438233=%20%20Language%20models%20have%20been%20effectively%20applied%20to%20modeling%20natural%20signals%2C%0Asuch%20as%20images%2C%20video%2C%20speech%2C%20and%20audio.%20A%20crucial%20component%20of%20these%20models%0Ais%20the%20codec%20tokenizer%2C%20which%20compresses%20high-dimensional%20natural%20signals%20into%0Alower-dimensional%20discrete%20tokens.%20In%20this%20paper%2C%20we%20introduce%20WavTokenizer%2C%0Awhich%20offers%20several%20advantages%20over%20previous%20SOTA%20acoustic%20codec%20models%20in%20the%0Aaudio%20domain%3A%201%29extreme%20compression.%20By%20compressing%20the%20layers%20of%20quantizers%0Aand%20the%20temporal%20dimension%20of%20the%20discrete%20codec%2C%20one-second%20audio%20of%2024kHz%0Asampling%20rate%20requires%20only%20a%20single%20quantizer%20with%2040%20or%2075%20tokens.%202%29improved%0Asubjective%20quality.%20Despite%20the%20reduced%20number%20of%20tokens%2C%20WavTokenizer%20achieves%0Astate-of-the-art%20reconstruction%20quality%20with%20outstanding%20UTMOS%20scores%20and%0Ainherently%20contains%20richer%20semantic%20information.%20Specifically%2C%20we%20achieve%20these%0Aresults%20by%20designing%20a%20broader%20VQ%20space%2C%20extended%20contextual%20windows%2C%20and%0Aimproved%20attention%20networks%2C%20as%20well%20as%20introducing%20a%20powerful%20multi-scale%0Adiscriminator%20and%20an%20inverse%20Fourier%20transform%20structure.%20We%20conducted%0Aextensive%20reconstruction%20experiments%20in%20the%20domains%20of%20speech%2C%20audio%2C%20and%0Amusic.%20WavTokenizer%20exhibited%20strong%20performance%20across%20various%20objective%20and%0Asubjective%20metrics%20compared%20to%20state-of-the-art%20models.%20We%20also%20tested%20semantic%0Ainformation%2C%20VQ%20utilization%2C%20and%20adaptability%20to%20generative%20models.%0AComprehensive%20ablation%20studies%20confirm%20the%20necessity%20of%20each%20module%20in%0AWavTokenizer.%20The%20related%20code%2C%20demos%2C%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/jishengpeng/WavTokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16532v1&entry.124074799=Read"},
{"title": "Integrating Features for Recognizing Human Activities through Optimized\n  Parameters in Graph Convolutional Networks and Transformer Architectures", "author": "Mohammad Belal and Taimur Hassan and Abdelfatah Hassan and Nael Alsheikh and Noureldin Elhendawi and Irfan Hussain", "abstract": "  Human activity recognition is a major field of study that employs computer\nvision, machine vision, and deep learning techniques to categorize human\nactions. The field of deep learning has made significant progress, with\narchitectures that are extremely effective at capturing human dynamics. This\nstudy emphasizes the influence of feature fusion on the accuracy of activity\nrecognition. This technique addresses the limitation of conventional models,\nwhich face difficulties in identifying activities because of their limited\ncapacity to understand spatial and temporal features. The technique employs\nsensory data obtained from four publicly available datasets: HuGaDB, PKU-MMD,\nLARa, and TUG. The accuracy and F1-score of two deep learning models,\nspecifically a Transformer model and a Parameter-Optimized Graph Convolutional\nNetwork (PO-GCN), were evaluated using these datasets. The feature fusion\ntechnique integrated the final layer features from both models and inputted\nthem into a classifier. Empirical evidence demonstrates that PO-GCN outperforms\nstandard models in activity recognition. HuGaDB demonstrated a 2.3% improvement\nin accuracy and a 2.2% increase in F1-score. TUG showed a 5% increase in\naccuracy and a 0.5% rise in F1-score. On the other hand, LARa and PKU-MMD\nachieved lower accuracies of 64% and 69% respectively. This indicates that the\nintegration of features enhanced the performance of both the Transformer model\nand PO-GCN.\n", "link": "http://arxiv.org/abs/2408.16442v1", "date": "2024-08-29", "relevancy": 2.0199, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5127}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4997}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Features%20for%20Recognizing%20Human%20Activities%20through%20Optimized%0A%20%20Parameters%20in%20Graph%20Convolutional%20Networks%20and%20Transformer%20Architectures&body=Title%3A%20Integrating%20Features%20for%20Recognizing%20Human%20Activities%20through%20Optimized%0A%20%20Parameters%20in%20Graph%20Convolutional%20Networks%20and%20Transformer%20Architectures%0AAuthor%3A%20Mohammad%20Belal%20and%20Taimur%20Hassan%20and%20Abdelfatah%20Hassan%20and%20Nael%20Alsheikh%20and%20Noureldin%20Elhendawi%20and%20Irfan%20Hussain%0AAbstract%3A%20%20%20Human%20activity%20recognition%20is%20a%20major%20field%20of%20study%20that%20employs%20computer%0Avision%2C%20machine%20vision%2C%20and%20deep%20learning%20techniques%20to%20categorize%20human%0Aactions.%20The%20field%20of%20deep%20learning%20has%20made%20significant%20progress%2C%20with%0Aarchitectures%20that%20are%20extremely%20effective%20at%20capturing%20human%20dynamics.%20This%0Astudy%20emphasizes%20the%20influence%20of%20feature%20fusion%20on%20the%20accuracy%20of%20activity%0Arecognition.%20This%20technique%20addresses%20the%20limitation%20of%20conventional%20models%2C%0Awhich%20face%20difficulties%20in%20identifying%20activities%20because%20of%20their%20limited%0Acapacity%20to%20understand%20spatial%20and%20temporal%20features.%20The%20technique%20employs%0Asensory%20data%20obtained%20from%20four%20publicly%20available%20datasets%3A%20HuGaDB%2C%20PKU-MMD%2C%0ALARa%2C%20and%20TUG.%20The%20accuracy%20and%20F1-score%20of%20two%20deep%20learning%20models%2C%0Aspecifically%20a%20Transformer%20model%20and%20a%20Parameter-Optimized%20Graph%20Convolutional%0ANetwork%20%28PO-GCN%29%2C%20were%20evaluated%20using%20these%20datasets.%20The%20feature%20fusion%0Atechnique%20integrated%20the%20final%20layer%20features%20from%20both%20models%20and%20inputted%0Athem%20into%20a%20classifier.%20Empirical%20evidence%20demonstrates%20that%20PO-GCN%20outperforms%0Astandard%20models%20in%20activity%20recognition.%20HuGaDB%20demonstrated%20a%202.3%25%20improvement%0Ain%20accuracy%20and%20a%202.2%25%20increase%20in%20F1-score.%20TUG%20showed%20a%205%25%20increase%20in%0Aaccuracy%20and%20a%200.5%25%20rise%20in%20F1-score.%20On%20the%20other%20hand%2C%20LARa%20and%20PKU-MMD%0Aachieved%20lower%20accuracies%20of%2064%25%20and%2069%25%20respectively.%20This%20indicates%20that%20the%0Aintegration%20of%20features%20enhanced%20the%20performance%20of%20both%20the%20Transformer%20model%0Aand%20PO-GCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Features%2520for%2520Recognizing%2520Human%2520Activities%2520through%2520Optimized%250A%2520%2520Parameters%2520in%2520Graph%2520Convolutional%2520Networks%2520and%2520Transformer%2520Architectures%26entry.906535625%3DMohammad%2520Belal%2520and%2520Taimur%2520Hassan%2520and%2520Abdelfatah%2520Hassan%2520and%2520Nael%2520Alsheikh%2520and%2520Noureldin%2520Elhendawi%2520and%2520Irfan%2520Hussain%26entry.1292438233%3D%2520%2520Human%2520activity%2520recognition%2520is%2520a%2520major%2520field%2520of%2520study%2520that%2520employs%2520computer%250Avision%252C%2520machine%2520vision%252C%2520and%2520deep%2520learning%2520techniques%2520to%2520categorize%2520human%250Aactions.%2520The%2520field%2520of%2520deep%2520learning%2520has%2520made%2520significant%2520progress%252C%2520with%250Aarchitectures%2520that%2520are%2520extremely%2520effective%2520at%2520capturing%2520human%2520dynamics.%2520This%250Astudy%2520emphasizes%2520the%2520influence%2520of%2520feature%2520fusion%2520on%2520the%2520accuracy%2520of%2520activity%250Arecognition.%2520This%2520technique%2520addresses%2520the%2520limitation%2520of%2520conventional%2520models%252C%250Awhich%2520face%2520difficulties%2520in%2520identifying%2520activities%2520because%2520of%2520their%2520limited%250Acapacity%2520to%2520understand%2520spatial%2520and%2520temporal%2520features.%2520The%2520technique%2520employs%250Asensory%2520data%2520obtained%2520from%2520four%2520publicly%2520available%2520datasets%253A%2520HuGaDB%252C%2520PKU-MMD%252C%250ALARa%252C%2520and%2520TUG.%2520The%2520accuracy%2520and%2520F1-score%2520of%2520two%2520deep%2520learning%2520models%252C%250Aspecifically%2520a%2520Transformer%2520model%2520and%2520a%2520Parameter-Optimized%2520Graph%2520Convolutional%250ANetwork%2520%2528PO-GCN%2529%252C%2520were%2520evaluated%2520using%2520these%2520datasets.%2520The%2520feature%2520fusion%250Atechnique%2520integrated%2520the%2520final%2520layer%2520features%2520from%2520both%2520models%2520and%2520inputted%250Athem%2520into%2520a%2520classifier.%2520Empirical%2520evidence%2520demonstrates%2520that%2520PO-GCN%2520outperforms%250Astandard%2520models%2520in%2520activity%2520recognition.%2520HuGaDB%2520demonstrated%2520a%25202.3%2525%2520improvement%250Ain%2520accuracy%2520and%2520a%25202.2%2525%2520increase%2520in%2520F1-score.%2520TUG%2520showed%2520a%25205%2525%2520increase%2520in%250Aaccuracy%2520and%2520a%25200.5%2525%2520rise%2520in%2520F1-score.%2520On%2520the%2520other%2520hand%252C%2520LARa%2520and%2520PKU-MMD%250Aachieved%2520lower%2520accuracies%2520of%252064%2525%2520and%252069%2525%2520respectively.%2520This%2520indicates%2520that%2520the%250Aintegration%2520of%2520features%2520enhanced%2520the%2520performance%2520of%2520both%2520the%2520Transformer%2520model%250Aand%2520PO-GCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Features%20for%20Recognizing%20Human%20Activities%20through%20Optimized%0A%20%20Parameters%20in%20Graph%20Convolutional%20Networks%20and%20Transformer%20Architectures&entry.906535625=Mohammad%20Belal%20and%20Taimur%20Hassan%20and%20Abdelfatah%20Hassan%20and%20Nael%20Alsheikh%20and%20Noureldin%20Elhendawi%20and%20Irfan%20Hussain&entry.1292438233=%20%20Human%20activity%20recognition%20is%20a%20major%20field%20of%20study%20that%20employs%20computer%0Avision%2C%20machine%20vision%2C%20and%20deep%20learning%20techniques%20to%20categorize%20human%0Aactions.%20The%20field%20of%20deep%20learning%20has%20made%20significant%20progress%2C%20with%0Aarchitectures%20that%20are%20extremely%20effective%20at%20capturing%20human%20dynamics.%20This%0Astudy%20emphasizes%20the%20influence%20of%20feature%20fusion%20on%20the%20accuracy%20of%20activity%0Arecognition.%20This%20technique%20addresses%20the%20limitation%20of%20conventional%20models%2C%0Awhich%20face%20difficulties%20in%20identifying%20activities%20because%20of%20their%20limited%0Acapacity%20to%20understand%20spatial%20and%20temporal%20features.%20The%20technique%20employs%0Asensory%20data%20obtained%20from%20four%20publicly%20available%20datasets%3A%20HuGaDB%2C%20PKU-MMD%2C%0ALARa%2C%20and%20TUG.%20The%20accuracy%20and%20F1-score%20of%20two%20deep%20learning%20models%2C%0Aspecifically%20a%20Transformer%20model%20and%20a%20Parameter-Optimized%20Graph%20Convolutional%0ANetwork%20%28PO-GCN%29%2C%20were%20evaluated%20using%20these%20datasets.%20The%20feature%20fusion%0Atechnique%20integrated%20the%20final%20layer%20features%20from%20both%20models%20and%20inputted%0Athem%20into%20a%20classifier.%20Empirical%20evidence%20demonstrates%20that%20PO-GCN%20outperforms%0Astandard%20models%20in%20activity%20recognition.%20HuGaDB%20demonstrated%20a%202.3%25%20improvement%0Ain%20accuracy%20and%20a%202.2%25%20increase%20in%20F1-score.%20TUG%20showed%20a%205%25%20increase%20in%0Aaccuracy%20and%20a%200.5%25%20rise%20in%20F1-score.%20On%20the%20other%20hand%2C%20LARa%20and%20PKU-MMD%0Aachieved%20lower%20accuracies%20of%2064%25%20and%2069%25%20respectively.%20This%20indicates%20that%20the%0Aintegration%20of%20features%20enhanced%20the%20performance%20of%20both%20the%20Transformer%20model%0Aand%20PO-GCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16442v1&entry.124074799=Read"},
{"title": "Hyperdimensional Vector Tsetlin Machines with Applications to Sequence\n  Learning and Generation", "author": "Christian D. Blakely", "abstract": "  We construct a two-layered model for learning and generating sequential data\nthat is both computationally fast and competitive with vanilla Tsetlin\nmachines, adding numerous advantages. Through the use of hyperdimensional\nvector computing (HVC) algebras and Tsetlin machine clause structures, we\ndemonstrate that the combination of both inherits the generality of data\nencoding and decoding of HVC with the fast interpretable nature of Tsetlin\nmachines to yield a powerful machine learning model. We apply the approach in\ntwo areas, namely in forecasting, generating new sequences, and classification.\nFor the latter, we derive results for the entire UCR Time Series Archive and\ncompare with the standard benchmarks to see how well the method competes in\ntime series classification.\n", "link": "http://arxiv.org/abs/2408.16620v1", "date": "2024-08-29", "relevancy": 2.0074, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5089}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5039}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperdimensional%20Vector%20Tsetlin%20Machines%20with%20Applications%20to%20Sequence%0A%20%20Learning%20and%20Generation&body=Title%3A%20Hyperdimensional%20Vector%20Tsetlin%20Machines%20with%20Applications%20to%20Sequence%0A%20%20Learning%20and%20Generation%0AAuthor%3A%20Christian%20D.%20Blakely%0AAbstract%3A%20%20%20We%20construct%20a%20two-layered%20model%20for%20learning%20and%20generating%20sequential%20data%0Athat%20is%20both%20computationally%20fast%20and%20competitive%20with%20vanilla%20Tsetlin%0Amachines%2C%20adding%20numerous%20advantages.%20Through%20the%20use%20of%20hyperdimensional%0Avector%20computing%20%28HVC%29%20algebras%20and%20Tsetlin%20machine%20clause%20structures%2C%20we%0Ademonstrate%20that%20the%20combination%20of%20both%20inherits%20the%20generality%20of%20data%0Aencoding%20and%20decoding%20of%20HVC%20with%20the%20fast%20interpretable%20nature%20of%20Tsetlin%0Amachines%20to%20yield%20a%20powerful%20machine%20learning%20model.%20We%20apply%20the%20approach%20in%0Atwo%20areas%2C%20namely%20in%20forecasting%2C%20generating%20new%20sequences%2C%20and%20classification.%0AFor%20the%20latter%2C%20we%20derive%20results%20for%20the%20entire%20UCR%20Time%20Series%20Archive%20and%0Acompare%20with%20the%20standard%20benchmarks%20to%20see%20how%20well%20the%20method%20competes%20in%0Atime%20series%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperdimensional%2520Vector%2520Tsetlin%2520Machines%2520with%2520Applications%2520to%2520Sequence%250A%2520%2520Learning%2520and%2520Generation%26entry.906535625%3DChristian%2520D.%2520Blakely%26entry.1292438233%3D%2520%2520We%2520construct%2520a%2520two-layered%2520model%2520for%2520learning%2520and%2520generating%2520sequential%2520data%250Athat%2520is%2520both%2520computationally%2520fast%2520and%2520competitive%2520with%2520vanilla%2520Tsetlin%250Amachines%252C%2520adding%2520numerous%2520advantages.%2520Through%2520the%2520use%2520of%2520hyperdimensional%250Avector%2520computing%2520%2528HVC%2529%2520algebras%2520and%2520Tsetlin%2520machine%2520clause%2520structures%252C%2520we%250Ademonstrate%2520that%2520the%2520combination%2520of%2520both%2520inherits%2520the%2520generality%2520of%2520data%250Aencoding%2520and%2520decoding%2520of%2520HVC%2520with%2520the%2520fast%2520interpretable%2520nature%2520of%2520Tsetlin%250Amachines%2520to%2520yield%2520a%2520powerful%2520machine%2520learning%2520model.%2520We%2520apply%2520the%2520approach%2520in%250Atwo%2520areas%252C%2520namely%2520in%2520forecasting%252C%2520generating%2520new%2520sequences%252C%2520and%2520classification.%250AFor%2520the%2520latter%252C%2520we%2520derive%2520results%2520for%2520the%2520entire%2520UCR%2520Time%2520Series%2520Archive%2520and%250Acompare%2520with%2520the%2520standard%2520benchmarks%2520to%2520see%2520how%2520well%2520the%2520method%2520competes%2520in%250Atime%2520series%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperdimensional%20Vector%20Tsetlin%20Machines%20with%20Applications%20to%20Sequence%0A%20%20Learning%20and%20Generation&entry.906535625=Christian%20D.%20Blakely&entry.1292438233=%20%20We%20construct%20a%20two-layered%20model%20for%20learning%20and%20generating%20sequential%20data%0Athat%20is%20both%20computationally%20fast%20and%20competitive%20with%20vanilla%20Tsetlin%0Amachines%2C%20adding%20numerous%20advantages.%20Through%20the%20use%20of%20hyperdimensional%0Avector%20computing%20%28HVC%29%20algebras%20and%20Tsetlin%20machine%20clause%20structures%2C%20we%0Ademonstrate%20that%20the%20combination%20of%20both%20inherits%20the%20generality%20of%20data%0Aencoding%20and%20decoding%20of%20HVC%20with%20the%20fast%20interpretable%20nature%20of%20Tsetlin%0Amachines%20to%20yield%20a%20powerful%20machine%20learning%20model.%20We%20apply%20the%20approach%20in%0Atwo%20areas%2C%20namely%20in%20forecasting%2C%20generating%20new%20sequences%2C%20and%20classification.%0AFor%20the%20latter%2C%20we%20derive%20results%20for%20the%20entire%20UCR%20Time%20Series%20Archive%20and%0Acompare%20with%20the%20standard%20benchmarks%20to%20see%20how%20well%20the%20method%20competes%20in%0Atime%20series%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16620v1&entry.124074799=Read"},
{"title": "GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models", "author": "Jonathan Roberts and Kai Han and Samuel Albanie", "abstract": "  Large multimodal models (LMMs) have exhibited proficiencies across many\nvisual tasks. Although numerous well-known benchmarks exist to evaluate model\nperformance, they increasingly have insufficient headroom. As such, there is a\npressing need for a new generation of benchmarks challenging enough for the\nnext generation of LMMs. One area that LMMs show potential is graph analysis,\nspecifically, the tasks an analyst might typically perform when interpreting\nfigures such as estimating the mean, intercepts or correlations of functions\nand data series. In this work, we introduce GRAB, a graph analysis benchmark,\nfit for current and future frontier LMMs. Our benchmark is entirely synthetic,\nensuring high-quality, noise-free questions. GRAB is comprised of 2170\nquestions, covering four tasks and 23 graph properties. We evaluate 20 LMMs on\nGRAB, finding it to be a challenging benchmark, with the highest performing\nmodel attaining a score of just 21.7%. Finally, we conduct various ablations to\ninvestigate where the models succeed and struggle. We release GRAB to encourage\nprogress in this important, growing domain.\n", "link": "http://arxiv.org/abs/2408.11817v2", "date": "2024-08-29", "relevancy": 2.0016, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5071}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4959}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAB%3A%20A%20Challenging%20GRaph%20Analysis%20Benchmark%20for%20Large%20Multimodal%20Models&body=Title%3A%20GRAB%3A%20A%20Challenging%20GRaph%20Analysis%20Benchmark%20for%20Large%20Multimodal%20Models%0AAuthor%3A%20Jonathan%20Roberts%20and%20Kai%20Han%20and%20Samuel%20Albanie%0AAbstract%3A%20%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20exhibited%20proficiencies%20across%20many%0Avisual%20tasks.%20Although%20numerous%20well-known%20benchmarks%20exist%20to%20evaluate%20model%0Aperformance%2C%20they%20increasingly%20have%20insufficient%20headroom.%20As%20such%2C%20there%20is%20a%0Apressing%20need%20for%20a%20new%20generation%20of%20benchmarks%20challenging%20enough%20for%20the%0Anext%20generation%20of%20LMMs.%20One%20area%20that%20LMMs%20show%20potential%20is%20graph%20analysis%2C%0Aspecifically%2C%20the%20tasks%20an%20analyst%20might%20typically%20perform%20when%20interpreting%0Afigures%20such%20as%20estimating%20the%20mean%2C%20intercepts%20or%20correlations%20of%20functions%0Aand%20data%20series.%20In%20this%20work%2C%20we%20introduce%20GRAB%2C%20a%20graph%20analysis%20benchmark%2C%0Afit%20for%20current%20and%20future%20frontier%20LMMs.%20Our%20benchmark%20is%20entirely%20synthetic%2C%0Aensuring%20high-quality%2C%20noise-free%20questions.%20GRAB%20is%20comprised%20of%202170%0Aquestions%2C%20covering%20four%20tasks%20and%2023%20graph%20properties.%20We%20evaluate%2020%20LMMs%20on%0AGRAB%2C%20finding%20it%20to%20be%20a%20challenging%20benchmark%2C%20with%20the%20highest%20performing%0Amodel%20attaining%20a%20score%20of%20just%2021.7%25.%20Finally%2C%20we%20conduct%20various%20ablations%20to%0Ainvestigate%20where%20the%20models%20succeed%20and%20struggle.%20We%20release%20GRAB%20to%20encourage%0Aprogress%20in%20this%20important%2C%20growing%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11817v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAB%253A%2520A%2520Challenging%2520GRaph%2520Analysis%2520Benchmark%2520for%2520Large%2520Multimodal%2520Models%26entry.906535625%3DJonathan%2520Roberts%2520and%2520Kai%2520Han%2520and%2520Samuel%2520Albanie%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520exhibited%2520proficiencies%2520across%2520many%250Avisual%2520tasks.%2520Although%2520numerous%2520well-known%2520benchmarks%2520exist%2520to%2520evaluate%2520model%250Aperformance%252C%2520they%2520increasingly%2520have%2520insufficient%2520headroom.%2520As%2520such%252C%2520there%2520is%2520a%250Apressing%2520need%2520for%2520a%2520new%2520generation%2520of%2520benchmarks%2520challenging%2520enough%2520for%2520the%250Anext%2520generation%2520of%2520LMMs.%2520One%2520area%2520that%2520LMMs%2520show%2520potential%2520is%2520graph%2520analysis%252C%250Aspecifically%252C%2520the%2520tasks%2520an%2520analyst%2520might%2520typically%2520perform%2520when%2520interpreting%250Afigures%2520such%2520as%2520estimating%2520the%2520mean%252C%2520intercepts%2520or%2520correlations%2520of%2520functions%250Aand%2520data%2520series.%2520In%2520this%2520work%252C%2520we%2520introduce%2520GRAB%252C%2520a%2520graph%2520analysis%2520benchmark%252C%250Afit%2520for%2520current%2520and%2520future%2520frontier%2520LMMs.%2520Our%2520benchmark%2520is%2520entirely%2520synthetic%252C%250Aensuring%2520high-quality%252C%2520noise-free%2520questions.%2520GRAB%2520is%2520comprised%2520of%25202170%250Aquestions%252C%2520covering%2520four%2520tasks%2520and%252023%2520graph%2520properties.%2520We%2520evaluate%252020%2520LMMs%2520on%250AGRAB%252C%2520finding%2520it%2520to%2520be%2520a%2520challenging%2520benchmark%252C%2520with%2520the%2520highest%2520performing%250Amodel%2520attaining%2520a%2520score%2520of%2520just%252021.7%2525.%2520Finally%252C%2520we%2520conduct%2520various%2520ablations%2520to%250Ainvestigate%2520where%2520the%2520models%2520succeed%2520and%2520struggle.%2520We%2520release%2520GRAB%2520to%2520encourage%250Aprogress%2520in%2520this%2520important%252C%2520growing%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11817v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAB%3A%20A%20Challenging%20GRaph%20Analysis%20Benchmark%20for%20Large%20Multimodal%20Models&entry.906535625=Jonathan%20Roberts%20and%20Kai%20Han%20and%20Samuel%20Albanie&entry.1292438233=%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20exhibited%20proficiencies%20across%20many%0Avisual%20tasks.%20Although%20numerous%20well-known%20benchmarks%20exist%20to%20evaluate%20model%0Aperformance%2C%20they%20increasingly%20have%20insufficient%20headroom.%20As%20such%2C%20there%20is%20a%0Apressing%20need%20for%20a%20new%20generation%20of%20benchmarks%20challenging%20enough%20for%20the%0Anext%20generation%20of%20LMMs.%20One%20area%20that%20LMMs%20show%20potential%20is%20graph%20analysis%2C%0Aspecifically%2C%20the%20tasks%20an%20analyst%20might%20typically%20perform%20when%20interpreting%0Afigures%20such%20as%20estimating%20the%20mean%2C%20intercepts%20or%20correlations%20of%20functions%0Aand%20data%20series.%20In%20this%20work%2C%20we%20introduce%20GRAB%2C%20a%20graph%20analysis%20benchmark%2C%0Afit%20for%20current%20and%20future%20frontier%20LMMs.%20Our%20benchmark%20is%20entirely%20synthetic%2C%0Aensuring%20high-quality%2C%20noise-free%20questions.%20GRAB%20is%20comprised%20of%202170%0Aquestions%2C%20covering%20four%20tasks%20and%2023%20graph%20properties.%20We%20evaluate%2020%20LMMs%20on%0AGRAB%2C%20finding%20it%20to%20be%20a%20challenging%20benchmark%2C%20with%20the%20highest%20performing%0Amodel%20attaining%20a%20score%20of%20just%2021.7%25.%20Finally%2C%20we%20conduct%20various%20ablations%20to%0Ainvestigate%20where%20the%20models%20succeed%20and%20struggle.%20We%20release%20GRAB%20to%20encourage%0Aprogress%20in%20this%20important%2C%20growing%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11817v2&entry.124074799=Read"},
{"title": "Adaptive Variational Continual Learning via Task-Heuristic Modelling", "author": "Fan Yang", "abstract": "  Variational continual learning (VCL) is a turn-key learning algorithm that\nhas state-of-the-art performance among the best continual learning models. In\nour work, we explore an extension of the generalized variational continual\nlearning (GVCL) model, named AutoVCL, which combines task heuristics for\ninformed learning and model optimization. We demonstrate that our model\noutperforms the standard GVCL with fixed hyperparameters, benefiting from the\nautomatic adjustment of the hyperparameter based on the difficulty and\nsimilarity of the incoming task compared to the previous tasks.\n", "link": "http://arxiv.org/abs/2408.16517v1", "date": "2024-08-29", "relevancy": 1.9965, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5103}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4953}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Variational%20Continual%20Learning%20via%20Task-Heuristic%20Modelling&body=Title%3A%20Adaptive%20Variational%20Continual%20Learning%20via%20Task-Heuristic%20Modelling%0AAuthor%3A%20Fan%20Yang%0AAbstract%3A%20%20%20Variational%20continual%20learning%20%28VCL%29%20is%20a%20turn-key%20learning%20algorithm%20that%0Ahas%20state-of-the-art%20performance%20among%20the%20best%20continual%20learning%20models.%20In%0Aour%20work%2C%20we%20explore%20an%20extension%20of%20the%20generalized%20variational%20continual%0Alearning%20%28GVCL%29%20model%2C%20named%20AutoVCL%2C%20which%20combines%20task%20heuristics%20for%0Ainformed%20learning%20and%20model%20optimization.%20We%20demonstrate%20that%20our%20model%0Aoutperforms%20the%20standard%20GVCL%20with%20fixed%20hyperparameters%2C%20benefiting%20from%20the%0Aautomatic%20adjustment%20of%20the%20hyperparameter%20based%20on%20the%20difficulty%20and%0Asimilarity%20of%20the%20incoming%20task%20compared%20to%20the%20previous%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Variational%2520Continual%2520Learning%2520via%2520Task-Heuristic%2520Modelling%26entry.906535625%3DFan%2520Yang%26entry.1292438233%3D%2520%2520Variational%2520continual%2520learning%2520%2528VCL%2529%2520is%2520a%2520turn-key%2520learning%2520algorithm%2520that%250Ahas%2520state-of-the-art%2520performance%2520among%2520the%2520best%2520continual%2520learning%2520models.%2520In%250Aour%2520work%252C%2520we%2520explore%2520an%2520extension%2520of%2520the%2520generalized%2520variational%2520continual%250Alearning%2520%2528GVCL%2529%2520model%252C%2520named%2520AutoVCL%252C%2520which%2520combines%2520task%2520heuristics%2520for%250Ainformed%2520learning%2520and%2520model%2520optimization.%2520We%2520demonstrate%2520that%2520our%2520model%250Aoutperforms%2520the%2520standard%2520GVCL%2520with%2520fixed%2520hyperparameters%252C%2520benefiting%2520from%2520the%250Aautomatic%2520adjustment%2520of%2520the%2520hyperparameter%2520based%2520on%2520the%2520difficulty%2520and%250Asimilarity%2520of%2520the%2520incoming%2520task%2520compared%2520to%2520the%2520previous%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Variational%20Continual%20Learning%20via%20Task-Heuristic%20Modelling&entry.906535625=Fan%20Yang&entry.1292438233=%20%20Variational%20continual%20learning%20%28VCL%29%20is%20a%20turn-key%20learning%20algorithm%20that%0Ahas%20state-of-the-art%20performance%20among%20the%20best%20continual%20learning%20models.%20In%0Aour%20work%2C%20we%20explore%20an%20extension%20of%20the%20generalized%20variational%20continual%0Alearning%20%28GVCL%29%20model%2C%20named%20AutoVCL%2C%20which%20combines%20task%20heuristics%20for%0Ainformed%20learning%20and%20model%20optimization.%20We%20demonstrate%20that%20our%20model%0Aoutperforms%20the%20standard%20GVCL%20with%20fixed%20hyperparameters%2C%20benefiting%20from%20the%0Aautomatic%20adjustment%20of%20the%20hyperparameter%20based%20on%20the%20difficulty%20and%0Asimilarity%20of%20the%20incoming%20task%20compared%20to%20the%20previous%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16517v1&entry.124074799=Read"},
{"title": "Sparse Signal Reconstruction for Overdispersed Low-photon Count\n  Biomedical Imaging Using $\\ell_p$ Total Variation", "author": "Yu Lu and Roummel F. Marcia", "abstract": "  The negative binomial model, which generalizes the Poisson distribution\nmodel, can be found in applications involving low-photon signal recovery,\nincluding medical imaging. Recent studies have explored several regularization\nterms for the negative binomial model, such as the $\\ell_p$ quasi-norm with $0\n< p < 1$, $\\ell_1$ norm, and the total variation (TV) quasi-seminorm for\npromoting sparsity in signal recovery. These penalty terms have been shown to\nimprove image reconstruction outcomes. In this paper, we investigate the\n$\\ell_p$ quasi-seminorm, both isotropic and anisotropic $\\ell_p$ TV\nquasi-seminorms, within the framework of the negative binomial statistical\nmodel. This problem can be formulated as an optimization problem, which we\nsolve using a gradient-based approach. We present comparisons between the\nnegative binomial and Poisson statistical models using the $\\ell_p$ TV\nquasi-seminorm as well as common penalty terms. Our experimental results\nhighlight the efficacy of the proposed method.\n", "link": "http://arxiv.org/abs/2408.16622v1", "date": "2024-08-29", "relevancy": 1.9881, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4995}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4971}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Signal%20Reconstruction%20for%20Overdispersed%20Low-photon%20Count%0A%20%20Biomedical%20Imaging%20Using%20%24%5Cell_p%24%20Total%20Variation&body=Title%3A%20Sparse%20Signal%20Reconstruction%20for%20Overdispersed%20Low-photon%20Count%0A%20%20Biomedical%20Imaging%20Using%20%24%5Cell_p%24%20Total%20Variation%0AAuthor%3A%20Yu%20Lu%20and%20Roummel%20F.%20Marcia%0AAbstract%3A%20%20%20The%20negative%20binomial%20model%2C%20which%20generalizes%20the%20Poisson%20distribution%0Amodel%2C%20can%20be%20found%20in%20applications%20involving%20low-photon%20signal%20recovery%2C%0Aincluding%20medical%20imaging.%20Recent%20studies%20have%20explored%20several%20regularization%0Aterms%20for%20the%20negative%20binomial%20model%2C%20such%20as%20the%20%24%5Cell_p%24%20quasi-norm%20with%20%240%0A%3C%20p%20%3C%201%24%2C%20%24%5Cell_1%24%20norm%2C%20and%20the%20total%20variation%20%28TV%29%20quasi-seminorm%20for%0Apromoting%20sparsity%20in%20signal%20recovery.%20These%20penalty%20terms%20have%20been%20shown%20to%0Aimprove%20image%20reconstruction%20outcomes.%20In%20this%20paper%2C%20we%20investigate%20the%0A%24%5Cell_p%24%20quasi-seminorm%2C%20both%20isotropic%20and%20anisotropic%20%24%5Cell_p%24%20TV%0Aquasi-seminorms%2C%20within%20the%20framework%20of%20the%20negative%20binomial%20statistical%0Amodel.%20This%20problem%20can%20be%20formulated%20as%20an%20optimization%20problem%2C%20which%20we%0Asolve%20using%20a%20gradient-based%20approach.%20We%20present%20comparisons%20between%20the%0Anegative%20binomial%20and%20Poisson%20statistical%20models%20using%20the%20%24%5Cell_p%24%20TV%0Aquasi-seminorm%20as%20well%20as%20common%20penalty%20terms.%20Our%20experimental%20results%0Ahighlight%20the%20efficacy%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Signal%2520Reconstruction%2520for%2520Overdispersed%2520Low-photon%2520Count%250A%2520%2520Biomedical%2520Imaging%2520Using%2520%2524%255Cell_p%2524%2520Total%2520Variation%26entry.906535625%3DYu%2520Lu%2520and%2520Roummel%2520F.%2520Marcia%26entry.1292438233%3D%2520%2520The%2520negative%2520binomial%2520model%252C%2520which%2520generalizes%2520the%2520Poisson%2520distribution%250Amodel%252C%2520can%2520be%2520found%2520in%2520applications%2520involving%2520low-photon%2520signal%2520recovery%252C%250Aincluding%2520medical%2520imaging.%2520Recent%2520studies%2520have%2520explored%2520several%2520regularization%250Aterms%2520for%2520the%2520negative%2520binomial%2520model%252C%2520such%2520as%2520the%2520%2524%255Cell_p%2524%2520quasi-norm%2520with%2520%25240%250A%253C%2520p%2520%253C%25201%2524%252C%2520%2524%255Cell_1%2524%2520norm%252C%2520and%2520the%2520total%2520variation%2520%2528TV%2529%2520quasi-seminorm%2520for%250Apromoting%2520sparsity%2520in%2520signal%2520recovery.%2520These%2520penalty%2520terms%2520have%2520been%2520shown%2520to%250Aimprove%2520image%2520reconstruction%2520outcomes.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250A%2524%255Cell_p%2524%2520quasi-seminorm%252C%2520both%2520isotropic%2520and%2520anisotropic%2520%2524%255Cell_p%2524%2520TV%250Aquasi-seminorms%252C%2520within%2520the%2520framework%2520of%2520the%2520negative%2520binomial%2520statistical%250Amodel.%2520This%2520problem%2520can%2520be%2520formulated%2520as%2520an%2520optimization%2520problem%252C%2520which%2520we%250Asolve%2520using%2520a%2520gradient-based%2520approach.%2520We%2520present%2520comparisons%2520between%2520the%250Anegative%2520binomial%2520and%2520Poisson%2520statistical%2520models%2520using%2520the%2520%2524%255Cell_p%2524%2520TV%250Aquasi-seminorm%2520as%2520well%2520as%2520common%2520penalty%2520terms.%2520Our%2520experimental%2520results%250Ahighlight%2520the%2520efficacy%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Signal%20Reconstruction%20for%20Overdispersed%20Low-photon%20Count%0A%20%20Biomedical%20Imaging%20Using%20%24%5Cell_p%24%20Total%20Variation&entry.906535625=Yu%20Lu%20and%20Roummel%20F.%20Marcia&entry.1292438233=%20%20The%20negative%20binomial%20model%2C%20which%20generalizes%20the%20Poisson%20distribution%0Amodel%2C%20can%20be%20found%20in%20applications%20involving%20low-photon%20signal%20recovery%2C%0Aincluding%20medical%20imaging.%20Recent%20studies%20have%20explored%20several%20regularization%0Aterms%20for%20the%20negative%20binomial%20model%2C%20such%20as%20the%20%24%5Cell_p%24%20quasi-norm%20with%20%240%0A%3C%20p%20%3C%201%24%2C%20%24%5Cell_1%24%20norm%2C%20and%20the%20total%20variation%20%28TV%29%20quasi-seminorm%20for%0Apromoting%20sparsity%20in%20signal%20recovery.%20These%20penalty%20terms%20have%20been%20shown%20to%0Aimprove%20image%20reconstruction%20outcomes.%20In%20this%20paper%2C%20we%20investigate%20the%0A%24%5Cell_p%24%20quasi-seminorm%2C%20both%20isotropic%20and%20anisotropic%20%24%5Cell_p%24%20TV%0Aquasi-seminorms%2C%20within%20the%20framework%20of%20the%20negative%20binomial%20statistical%0Amodel.%20This%20problem%20can%20be%20formulated%20as%20an%20optimization%20problem%2C%20which%20we%0Asolve%20using%20a%20gradient-based%20approach.%20We%20present%20comparisons%20between%20the%0Anegative%20binomial%20and%20Poisson%20statistical%20models%20using%20the%20%24%5Cell_p%24%20TV%0Aquasi-seminorm%20as%20well%20as%20common%20penalty%20terms.%20Our%20experimental%20results%0Ahighlight%20the%20efficacy%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16622v1&entry.124074799=Read"},
{"title": "LLMs generate structurally realistic social networks but overestimate\n  political homophily", "author": "Serina Chang and Alicja Chaszczewicz and Emma Wang and Maya Josifovska and Emma Pierson and Jure Leskovec", "abstract": "  Generating social networks is essential for many applications, such as\nepidemic modeling and social simulations. Prior approaches either involve deep\nlearning models, which require many observed networks for training, or stylized\nmodels, which are limited in their realism and flexibility. In contrast, LLMs\noffer the potential for zero-shot and flexible network generation. However, two\nkey questions are: (1) are LLM's generated networks realistic, and (2) what are\nrisks of bias, given the importance of demographics in forming social ties? To\nanswer these questions, we develop three prompting methods for network\ngeneration and compare the generated networks to real social networks. We find\nthat more realistic networks are generated with \"local\" methods, where the LLM\nconstructs relations for one persona at a time, compared to \"global\" methods\nthat construct the entire network at once. We also find that the generated\nnetworks match real networks on many characteristics, including density,\nclustering, community structure, and degree. However, we find that LLMs\nemphasize political homophily over all other types of homophily and\noverestimate political homophily relative to real-world measures.\n", "link": "http://arxiv.org/abs/2408.16629v1", "date": "2024-08-29", "relevancy": 1.984, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.399}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3983}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20generate%20structurally%20realistic%20social%20networks%20but%20overestimate%0A%20%20political%20homophily&body=Title%3A%20LLMs%20generate%20structurally%20realistic%20social%20networks%20but%20overestimate%0A%20%20political%20homophily%0AAuthor%3A%20Serina%20Chang%20and%20Alicja%20Chaszczewicz%20and%20Emma%20Wang%20and%20Maya%20Josifovska%20and%20Emma%20Pierson%20and%20Jure%20Leskovec%0AAbstract%3A%20%20%20Generating%20social%20networks%20is%20essential%20for%20many%20applications%2C%20such%20as%0Aepidemic%20modeling%20and%20social%20simulations.%20Prior%20approaches%20either%20involve%20deep%0Alearning%20models%2C%20which%20require%20many%20observed%20networks%20for%20training%2C%20or%20stylized%0Amodels%2C%20which%20are%20limited%20in%20their%20realism%20and%20flexibility.%20In%20contrast%2C%20LLMs%0Aoffer%20the%20potential%20for%20zero-shot%20and%20flexible%20network%20generation.%20However%2C%20two%0Akey%20questions%20are%3A%20%281%29%20are%20LLM%27s%20generated%20networks%20realistic%2C%20and%20%282%29%20what%20are%0Arisks%20of%20bias%2C%20given%20the%20importance%20of%20demographics%20in%20forming%20social%20ties%3F%20To%0Aanswer%20these%20questions%2C%20we%20develop%20three%20prompting%20methods%20for%20network%0Ageneration%20and%20compare%20the%20generated%20networks%20to%20real%20social%20networks.%20We%20find%0Athat%20more%20realistic%20networks%20are%20generated%20with%20%22local%22%20methods%2C%20where%20the%20LLM%0Aconstructs%20relations%20for%20one%20persona%20at%20a%20time%2C%20compared%20to%20%22global%22%20methods%0Athat%20construct%20the%20entire%20network%20at%20once.%20We%20also%20find%20that%20the%20generated%0Anetworks%20match%20real%20networks%20on%20many%20characteristics%2C%20including%20density%2C%0Aclustering%2C%20community%20structure%2C%20and%20degree.%20However%2C%20we%20find%20that%20LLMs%0Aemphasize%20political%20homophily%20over%20all%20other%20types%20of%20homophily%20and%0Aoverestimate%20political%20homophily%20relative%20to%20real-world%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520generate%2520structurally%2520realistic%2520social%2520networks%2520but%2520overestimate%250A%2520%2520political%2520homophily%26entry.906535625%3DSerina%2520Chang%2520and%2520Alicja%2520Chaszczewicz%2520and%2520Emma%2520Wang%2520and%2520Maya%2520Josifovska%2520and%2520Emma%2520Pierson%2520and%2520Jure%2520Leskovec%26entry.1292438233%3D%2520%2520Generating%2520social%2520networks%2520is%2520essential%2520for%2520many%2520applications%252C%2520such%2520as%250Aepidemic%2520modeling%2520and%2520social%2520simulations.%2520Prior%2520approaches%2520either%2520involve%2520deep%250Alearning%2520models%252C%2520which%2520require%2520many%2520observed%2520networks%2520for%2520training%252C%2520or%2520stylized%250Amodels%252C%2520which%2520are%2520limited%2520in%2520their%2520realism%2520and%2520flexibility.%2520In%2520contrast%252C%2520LLMs%250Aoffer%2520the%2520potential%2520for%2520zero-shot%2520and%2520flexible%2520network%2520generation.%2520However%252C%2520two%250Akey%2520questions%2520are%253A%2520%25281%2529%2520are%2520LLM%2527s%2520generated%2520networks%2520realistic%252C%2520and%2520%25282%2529%2520what%2520are%250Arisks%2520of%2520bias%252C%2520given%2520the%2520importance%2520of%2520demographics%2520in%2520forming%2520social%2520ties%253F%2520To%250Aanswer%2520these%2520questions%252C%2520we%2520develop%2520three%2520prompting%2520methods%2520for%2520network%250Ageneration%2520and%2520compare%2520the%2520generated%2520networks%2520to%2520real%2520social%2520networks.%2520We%2520find%250Athat%2520more%2520realistic%2520networks%2520are%2520generated%2520with%2520%2522local%2522%2520methods%252C%2520where%2520the%2520LLM%250Aconstructs%2520relations%2520for%2520one%2520persona%2520at%2520a%2520time%252C%2520compared%2520to%2520%2522global%2522%2520methods%250Athat%2520construct%2520the%2520entire%2520network%2520at%2520once.%2520We%2520also%2520find%2520that%2520the%2520generated%250Anetworks%2520match%2520real%2520networks%2520on%2520many%2520characteristics%252C%2520including%2520density%252C%250Aclustering%252C%2520community%2520structure%252C%2520and%2520degree.%2520However%252C%2520we%2520find%2520that%2520LLMs%250Aemphasize%2520political%2520homophily%2520over%2520all%2520other%2520types%2520of%2520homophily%2520and%250Aoverestimate%2520political%2520homophily%2520relative%2520to%2520real-world%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20generate%20structurally%20realistic%20social%20networks%20but%20overestimate%0A%20%20political%20homophily&entry.906535625=Serina%20Chang%20and%20Alicja%20Chaszczewicz%20and%20Emma%20Wang%20and%20Maya%20Josifovska%20and%20Emma%20Pierson%20and%20Jure%20Leskovec&entry.1292438233=%20%20Generating%20social%20networks%20is%20essential%20for%20many%20applications%2C%20such%20as%0Aepidemic%20modeling%20and%20social%20simulations.%20Prior%20approaches%20either%20involve%20deep%0Alearning%20models%2C%20which%20require%20many%20observed%20networks%20for%20training%2C%20or%20stylized%0Amodels%2C%20which%20are%20limited%20in%20their%20realism%20and%20flexibility.%20In%20contrast%2C%20LLMs%0Aoffer%20the%20potential%20for%20zero-shot%20and%20flexible%20network%20generation.%20However%2C%20two%0Akey%20questions%20are%3A%20%281%29%20are%20LLM%27s%20generated%20networks%20realistic%2C%20and%20%282%29%20what%20are%0Arisks%20of%20bias%2C%20given%20the%20importance%20of%20demographics%20in%20forming%20social%20ties%3F%20To%0Aanswer%20these%20questions%2C%20we%20develop%20three%20prompting%20methods%20for%20network%0Ageneration%20and%20compare%20the%20generated%20networks%20to%20real%20social%20networks.%20We%20find%0Athat%20more%20realistic%20networks%20are%20generated%20with%20%22local%22%20methods%2C%20where%20the%20LLM%0Aconstructs%20relations%20for%20one%20persona%20at%20a%20time%2C%20compared%20to%20%22global%22%20methods%0Athat%20construct%20the%20entire%20network%20at%20once.%20We%20also%20find%20that%20the%20generated%0Anetworks%20match%20real%20networks%20on%20many%20characteristics%2C%20including%20density%2C%0Aclustering%2C%20community%20structure%2C%20and%20degree.%20However%2C%20we%20find%20that%20LLMs%0Aemphasize%20political%20homophily%20over%20all%20other%20types%20of%20homophily%20and%0Aoverestimate%20political%20homophily%20relative%20to%20real-world%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16629v1&entry.124074799=Read"},
{"title": "CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven\n  Parallel Robots", "author": "Zeqing Zhang and Linhan Yang and Cong Sun and Weiwei Shang and Jia Pan", "abstract": "  Cable-driven parallel robots (CDPRs) have gained significant attention due to\ntheir promising advantages. When deploying CDPRs in practice, the kinematic\nmodeling is a key question. Unlike serial robots, CDPRs have a simple inverse\nkinematics problem but a complex forward kinematics (FK) issue. So, the\ndevelopment of accurate and efficient FK solvers has been a prominent research\nfocus in CDPR applications. By observing the topology within CDPRs, in this\npaper, we propose a graph-based representation to model CDPRs and introduce\nCafkNet, a fast and general FK solving method, leveraging Graph Neural Network\n(GNN) to learn the topological structure and yield the real FK solutions with\nsuperior generality, high accuracy, and low time cost. CafkNet is extensively\ntested on 3D and 2D CDPRs in different configurations, both in simulators and\nreal scenarios. The results demonstrate its ability to learn CDPRs' internal\ntopology and accurately solve the FK problem. Then, the zero-shot\ngeneralization from one configuration to another is validated. Also, the\nsim2real gap can be bridged by CafkNet using both simulation and real-world\ndata. To the best of our knowledge, it is the first study that employs the GNN\nto solve the FK problem for CDPRs.\n", "link": "http://arxiv.org/abs/2402.18420v3", "date": "2024-08-29", "relevancy": 1.975, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4993}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.491}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CafkNet%3A%20GNN-Empowered%20Forward%20Kinematic%20Modeling%20for%20Cable-Driven%0A%20%20Parallel%20Robots&body=Title%3A%20CafkNet%3A%20GNN-Empowered%20Forward%20Kinematic%20Modeling%20for%20Cable-Driven%0A%20%20Parallel%20Robots%0AAuthor%3A%20Zeqing%20Zhang%20and%20Linhan%20Yang%20and%20Cong%20Sun%20and%20Weiwei%20Shang%20and%20Jia%20Pan%0AAbstract%3A%20%20%20Cable-driven%20parallel%20robots%20%28CDPRs%29%20have%20gained%20significant%20attention%20due%20to%0Atheir%20promising%20advantages.%20When%20deploying%20CDPRs%20in%20practice%2C%20the%20kinematic%0Amodeling%20is%20a%20key%20question.%20Unlike%20serial%20robots%2C%20CDPRs%20have%20a%20simple%20inverse%0Akinematics%20problem%20but%20a%20complex%20forward%20kinematics%20%28FK%29%20issue.%20So%2C%20the%0Adevelopment%20of%20accurate%20and%20efficient%20FK%20solvers%20has%20been%20a%20prominent%20research%0Afocus%20in%20CDPR%20applications.%20By%20observing%20the%20topology%20within%20CDPRs%2C%20in%20this%0Apaper%2C%20we%20propose%20a%20graph-based%20representation%20to%20model%20CDPRs%20and%20introduce%0ACafkNet%2C%20a%20fast%20and%20general%20FK%20solving%20method%2C%20leveraging%20Graph%20Neural%20Network%0A%28GNN%29%20to%20learn%20the%20topological%20structure%20and%20yield%20the%20real%20FK%20solutions%20with%0Asuperior%20generality%2C%20high%20accuracy%2C%20and%20low%20time%20cost.%20CafkNet%20is%20extensively%0Atested%20on%203D%20and%202D%20CDPRs%20in%20different%20configurations%2C%20both%20in%20simulators%20and%0Areal%20scenarios.%20The%20results%20demonstrate%20its%20ability%20to%20learn%20CDPRs%27%20internal%0Atopology%20and%20accurately%20solve%20the%20FK%20problem.%20Then%2C%20the%20zero-shot%0Ageneralization%20from%20one%20configuration%20to%20another%20is%20validated.%20Also%2C%20the%0Asim2real%20gap%20can%20be%20bridged%20by%20CafkNet%20using%20both%20simulation%20and%20real-world%0Adata.%20To%20the%20best%20of%20our%20knowledge%2C%20it%20is%20the%20first%20study%20that%20employs%20the%20GNN%0Ato%20solve%20the%20FK%20problem%20for%20CDPRs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18420v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCafkNet%253A%2520GNN-Empowered%2520Forward%2520Kinematic%2520Modeling%2520for%2520Cable-Driven%250A%2520%2520Parallel%2520Robots%26entry.906535625%3DZeqing%2520Zhang%2520and%2520Linhan%2520Yang%2520and%2520Cong%2520Sun%2520and%2520Weiwei%2520Shang%2520and%2520Jia%2520Pan%26entry.1292438233%3D%2520%2520Cable-driven%2520parallel%2520robots%2520%2528CDPRs%2529%2520have%2520gained%2520significant%2520attention%2520due%2520to%250Atheir%2520promising%2520advantages.%2520When%2520deploying%2520CDPRs%2520in%2520practice%252C%2520the%2520kinematic%250Amodeling%2520is%2520a%2520key%2520question.%2520Unlike%2520serial%2520robots%252C%2520CDPRs%2520have%2520a%2520simple%2520inverse%250Akinematics%2520problem%2520but%2520a%2520complex%2520forward%2520kinematics%2520%2528FK%2529%2520issue.%2520So%252C%2520the%250Adevelopment%2520of%2520accurate%2520and%2520efficient%2520FK%2520solvers%2520has%2520been%2520a%2520prominent%2520research%250Afocus%2520in%2520CDPR%2520applications.%2520By%2520observing%2520the%2520topology%2520within%2520CDPRs%252C%2520in%2520this%250Apaper%252C%2520we%2520propose%2520a%2520graph-based%2520representation%2520to%2520model%2520CDPRs%2520and%2520introduce%250ACafkNet%252C%2520a%2520fast%2520and%2520general%2520FK%2520solving%2520method%252C%2520leveraging%2520Graph%2520Neural%2520Network%250A%2528GNN%2529%2520to%2520learn%2520the%2520topological%2520structure%2520and%2520yield%2520the%2520real%2520FK%2520solutions%2520with%250Asuperior%2520generality%252C%2520high%2520accuracy%252C%2520and%2520low%2520time%2520cost.%2520CafkNet%2520is%2520extensively%250Atested%2520on%25203D%2520and%25202D%2520CDPRs%2520in%2520different%2520configurations%252C%2520both%2520in%2520simulators%2520and%250Areal%2520scenarios.%2520The%2520results%2520demonstrate%2520its%2520ability%2520to%2520learn%2520CDPRs%2527%2520internal%250Atopology%2520and%2520accurately%2520solve%2520the%2520FK%2520problem.%2520Then%252C%2520the%2520zero-shot%250Ageneralization%2520from%2520one%2520configuration%2520to%2520another%2520is%2520validated.%2520Also%252C%2520the%250Asim2real%2520gap%2520can%2520be%2520bridged%2520by%2520CafkNet%2520using%2520both%2520simulation%2520and%2520real-world%250Adata.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520it%2520is%2520the%2520first%2520study%2520that%2520employs%2520the%2520GNN%250Ato%2520solve%2520the%2520FK%2520problem%2520for%2520CDPRs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18420v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CafkNet%3A%20GNN-Empowered%20Forward%20Kinematic%20Modeling%20for%20Cable-Driven%0A%20%20Parallel%20Robots&entry.906535625=Zeqing%20Zhang%20and%20Linhan%20Yang%20and%20Cong%20Sun%20and%20Weiwei%20Shang%20and%20Jia%20Pan&entry.1292438233=%20%20Cable-driven%20parallel%20robots%20%28CDPRs%29%20have%20gained%20significant%20attention%20due%20to%0Atheir%20promising%20advantages.%20When%20deploying%20CDPRs%20in%20practice%2C%20the%20kinematic%0Amodeling%20is%20a%20key%20question.%20Unlike%20serial%20robots%2C%20CDPRs%20have%20a%20simple%20inverse%0Akinematics%20problem%20but%20a%20complex%20forward%20kinematics%20%28FK%29%20issue.%20So%2C%20the%0Adevelopment%20of%20accurate%20and%20efficient%20FK%20solvers%20has%20been%20a%20prominent%20research%0Afocus%20in%20CDPR%20applications.%20By%20observing%20the%20topology%20within%20CDPRs%2C%20in%20this%0Apaper%2C%20we%20propose%20a%20graph-based%20representation%20to%20model%20CDPRs%20and%20introduce%0ACafkNet%2C%20a%20fast%20and%20general%20FK%20solving%20method%2C%20leveraging%20Graph%20Neural%20Network%0A%28GNN%29%20to%20learn%20the%20topological%20structure%20and%20yield%20the%20real%20FK%20solutions%20with%0Asuperior%20generality%2C%20high%20accuracy%2C%20and%20low%20time%20cost.%20CafkNet%20is%20extensively%0Atested%20on%203D%20and%202D%20CDPRs%20in%20different%20configurations%2C%20both%20in%20simulators%20and%0Areal%20scenarios.%20The%20results%20demonstrate%20its%20ability%20to%20learn%20CDPRs%27%20internal%0Atopology%20and%20accurately%20solve%20the%20FK%20problem.%20Then%2C%20the%20zero-shot%0Ageneralization%20from%20one%20configuration%20to%20another%20is%20validated.%20Also%2C%20the%0Asim2real%20gap%20can%20be%20bridged%20by%20CafkNet%20using%20both%20simulation%20and%20real-world%0Adata.%20To%20the%20best%20of%20our%20knowledge%2C%20it%20is%20the%20first%20study%20that%20employs%20the%20GNN%0Ato%20solve%20the%20FK%20problem%20for%20CDPRs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18420v3&entry.124074799=Read"},
{"title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM", "author": "Hao Kang and Qingru Zhang and Souvik Kundu and Geonhwa Jeong and Zaoxing Liu and Tushar Krishna and Tuo Zhao", "abstract": "  Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.\n", "link": "http://arxiv.org/abs/2403.05527v3", "date": "2024-08-29", "relevancy": 1.9744, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5003}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4945}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEAR%3A%20An%20Efficient%20KV%20Cache%20Compression%20Recipe%20for%20Near-Lossless%0A%20%20Generative%20Inference%20of%20LLM&body=Title%3A%20GEAR%3A%20An%20Efficient%20KV%20Cache%20Compression%20Recipe%20for%20Near-Lossless%0A%20%20Generative%20Inference%20of%20LLM%0AAuthor%3A%20Hao%20Kang%20and%20Qingru%20Zhang%20and%20Souvik%20Kundu%20and%20Geonhwa%20Jeong%20and%20Zaoxing%20Liu%20and%20Tushar%20Krishna%20and%20Tuo%20Zhao%0AAbstract%3A%20%20%20Key-value%20%28KV%29%20caching%20has%20become%20the%20de-facto%20to%20accelerate%20generation%20speed%0Afor%20large%20language%20models%20%28LLMs%29%20inference.%20However%2C%20the%20growing%20cache%20demand%0Awith%20increasing%20sequence%20length%20has%20transformed%20LLM%20inference%20to%20be%20a%20memory%0Abound%20problem%2C%20significantly%20constraining%20the%20system%20throughput.%20Existing%0Amethods%20rely%20on%20dropping%20unimportant%20tokens%20or%20quantizing%20all%20entries%0Auniformly.%20Such%20methods%2C%20however%2C%20often%20incur%20high%20approximation%20errors%20to%0Arepresent%20the%20compressed%20matrices.%20The%20autoregressive%20decoding%20process%20further%0Acompounds%20the%20error%20of%20each%20step%2C%20resulting%20in%20critical%20deviation%20in%20model%0Ageneration%20and%20deterioration%20of%20performance.%20To%20tackle%20this%20challenge%2C%20we%0Apropose%20GEAR%2C%20an%20efficient%20KV%20cache%20compression%20framework%20that%20achieves%0Anear-lossless%20high-ratio%20compression.%20GEAR%20first%20applies%20quantization%20to%0Amajority%20of%20entries%20of%20similar%20magnitudes%20to%20ultra-low%20precision.%20It%20then%0Aemploys%20a%20low%20rank%20matrix%20to%20approximate%20the%20quantization%20error%2C%20and%20a%20sparse%0Amatrix%20to%20remedy%20individual%20errors%20from%20outlier%20entries.%20By%20adeptly%20integrating%0Athree%20techniques%2C%20GEAR%20is%20able%20to%20fully%20exploit%20their%20synergistic%20potentials.%0AOur%20experiments%20demonstrate%20that%20compared%20to%20alternatives%2C%20GEAR%20achieves%0Anear-lossless%204-bit%20KV%20cache%20compression%20with%20up%20to%202.38x%20throughput%0Aimprovement%2C%20while%20reducing%20peak-memory%20size%20up%20to%202.29x.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/HaoKang-Timmy/GEAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05527v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEAR%253A%2520An%2520Efficient%2520KV%2520Cache%2520Compression%2520Recipe%2520for%2520Near-Lossless%250A%2520%2520Generative%2520Inference%2520of%2520LLM%26entry.906535625%3DHao%2520Kang%2520and%2520Qingru%2520Zhang%2520and%2520Souvik%2520Kundu%2520and%2520Geonhwa%2520Jeong%2520and%2520Zaoxing%2520Liu%2520and%2520Tushar%2520Krishna%2520and%2520Tuo%2520Zhao%26entry.1292438233%3D%2520%2520Key-value%2520%2528KV%2529%2520caching%2520has%2520become%2520the%2520de-facto%2520to%2520accelerate%2520generation%2520speed%250Afor%2520large%2520language%2520models%2520%2528LLMs%2529%2520inference.%2520However%252C%2520the%2520growing%2520cache%2520demand%250Awith%2520increasing%2520sequence%2520length%2520has%2520transformed%2520LLM%2520inference%2520to%2520be%2520a%2520memory%250Abound%2520problem%252C%2520significantly%2520constraining%2520the%2520system%2520throughput.%2520Existing%250Amethods%2520rely%2520on%2520dropping%2520unimportant%2520tokens%2520or%2520quantizing%2520all%2520entries%250Auniformly.%2520Such%2520methods%252C%2520however%252C%2520often%2520incur%2520high%2520approximation%2520errors%2520to%250Arepresent%2520the%2520compressed%2520matrices.%2520The%2520autoregressive%2520decoding%2520process%2520further%250Acompounds%2520the%2520error%2520of%2520each%2520step%252C%2520resulting%2520in%2520critical%2520deviation%2520in%2520model%250Ageneration%2520and%2520deterioration%2520of%2520performance.%2520To%2520tackle%2520this%2520challenge%252C%2520we%250Apropose%2520GEAR%252C%2520an%2520efficient%2520KV%2520cache%2520compression%2520framework%2520that%2520achieves%250Anear-lossless%2520high-ratio%2520compression.%2520GEAR%2520first%2520applies%2520quantization%2520to%250Amajority%2520of%2520entries%2520of%2520similar%2520magnitudes%2520to%2520ultra-low%2520precision.%2520It%2520then%250Aemploys%2520a%2520low%2520rank%2520matrix%2520to%2520approximate%2520the%2520quantization%2520error%252C%2520and%2520a%2520sparse%250Amatrix%2520to%2520remedy%2520individual%2520errors%2520from%2520outlier%2520entries.%2520By%2520adeptly%2520integrating%250Athree%2520techniques%252C%2520GEAR%2520is%2520able%2520to%2520fully%2520exploit%2520their%2520synergistic%2520potentials.%250AOur%2520experiments%2520demonstrate%2520that%2520compared%2520to%2520alternatives%252C%2520GEAR%2520achieves%250Anear-lossless%25204-bit%2520KV%2520cache%2520compression%2520with%2520up%2520to%25202.38x%2520throughput%250Aimprovement%252C%2520while%2520reducing%2520peak-memory%2520size%2520up%2520to%25202.29x.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/HaoKang-Timmy/GEAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05527v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEAR%3A%20An%20Efficient%20KV%20Cache%20Compression%20Recipe%20for%20Near-Lossless%0A%20%20Generative%20Inference%20of%20LLM&entry.906535625=Hao%20Kang%20and%20Qingru%20Zhang%20and%20Souvik%20Kundu%20and%20Geonhwa%20Jeong%20and%20Zaoxing%20Liu%20and%20Tushar%20Krishna%20and%20Tuo%20Zhao&entry.1292438233=%20%20Key-value%20%28KV%29%20caching%20has%20become%20the%20de-facto%20to%20accelerate%20generation%20speed%0Afor%20large%20language%20models%20%28LLMs%29%20inference.%20However%2C%20the%20growing%20cache%20demand%0Awith%20increasing%20sequence%20length%20has%20transformed%20LLM%20inference%20to%20be%20a%20memory%0Abound%20problem%2C%20significantly%20constraining%20the%20system%20throughput.%20Existing%0Amethods%20rely%20on%20dropping%20unimportant%20tokens%20or%20quantizing%20all%20entries%0Auniformly.%20Such%20methods%2C%20however%2C%20often%20incur%20high%20approximation%20errors%20to%0Arepresent%20the%20compressed%20matrices.%20The%20autoregressive%20decoding%20process%20further%0Acompounds%20the%20error%20of%20each%20step%2C%20resulting%20in%20critical%20deviation%20in%20model%0Ageneration%20and%20deterioration%20of%20performance.%20To%20tackle%20this%20challenge%2C%20we%0Apropose%20GEAR%2C%20an%20efficient%20KV%20cache%20compression%20framework%20that%20achieves%0Anear-lossless%20high-ratio%20compression.%20GEAR%20first%20applies%20quantization%20to%0Amajority%20of%20entries%20of%20similar%20magnitudes%20to%20ultra-low%20precision.%20It%20then%0Aemploys%20a%20low%20rank%20matrix%20to%20approximate%20the%20quantization%20error%2C%20and%20a%20sparse%0Amatrix%20to%20remedy%20individual%20errors%20from%20outlier%20entries.%20By%20adeptly%20integrating%0Athree%20techniques%2C%20GEAR%20is%20able%20to%20fully%20exploit%20their%20synergistic%20potentials.%0AOur%20experiments%20demonstrate%20that%20compared%20to%20alternatives%2C%20GEAR%20achieves%0Anear-lossless%204-bit%20KV%20cache%20compression%20with%20up%20to%202.38x%20throughput%0Aimprovement%2C%20while%20reducing%20peak-memory%20size%20up%20to%202.29x.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/HaoKang-Timmy/GEAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05527v3&entry.124074799=Read"},
{"title": "Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less\n  Overfitting and Better Diversity", "author": "Ziniu Li and Congliang Chen and Tian Xu and Zeyu Qin and Jiancong Xiao and Ruoyu Sun and Zhi-Quan Luo", "abstract": "  Large language models rely on Supervised Fine-Tuning (SFT) to specialize in\ndownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it\noften leads to overfitting and limited output diversity due to its aggressive\nupdates to the data distribution. This paper aim to address these issues by\nintroducing the maximum entropy principle, which favors models with flatter\ndistributions that still effectively capture the data. Specifically, we develop\na new distribution matching method called GEM, which solves reverse\nKullback-Leibler divergence minimization with an entropy regularizer.\n  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.\nFirst, when applied to the UltraFeedback dataset to develop general\ninstruction-following abilities, GEM exhibits reduced overfitting, evidenced by\nlower perplexity and better performance on the IFEval benchmark. Furthermore,\nGEM enhances output diversity, leading to performance gains of up to 7 points\non math reasoning and code generation tasks using best-of-n sampling, even\nwithout domain-specific data. Second, when fine-tuning with domain-specific\ndatasets for math reasoning and code generation, GEM also shows less\noverfitting and improvements of up to 10 points compared with CE.\n", "link": "http://arxiv.org/abs/2408.16673v1", "date": "2024-08-29", "relevancy": 1.9708, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5183}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4828}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropic%20Distribution%20Matching%20in%20Supervised%20Fine-tuning%20of%20LLMs%3A%20Less%0A%20%20Overfitting%20and%20Better%20Diversity&body=Title%3A%20Entropic%20Distribution%20Matching%20in%20Supervised%20Fine-tuning%20of%20LLMs%3A%20Less%0A%20%20Overfitting%20and%20Better%20Diversity%0AAuthor%3A%20Ziniu%20Li%20and%20Congliang%20Chen%20and%20Tian%20Xu%20and%20Zeyu%20Qin%20and%20Jiancong%20Xiao%20and%20Ruoyu%20Sun%20and%20Zhi-Quan%20Luo%0AAbstract%3A%20%20%20Large%20language%20models%20rely%20on%20Supervised%20Fine-Tuning%20%28SFT%29%20to%20specialize%20in%0Adownstream%20tasks.%20Cross%20Entropy%20%28CE%29%20loss%20is%20the%20de%20facto%20choice%20in%20SFT%2C%20but%20it%0Aoften%20leads%20to%20overfitting%20and%20limited%20output%20diversity%20due%20to%20its%20aggressive%0Aupdates%20to%20the%20data%20distribution.%20This%20paper%20aim%20to%20address%20these%20issues%20by%0Aintroducing%20the%20maximum%20entropy%20principle%2C%20which%20favors%20models%20with%20flatter%0Adistributions%20that%20still%20effectively%20capture%20the%20data.%20Specifically%2C%20we%20develop%0Aa%20new%20distribution%20matching%20method%20called%20GEM%2C%20which%20solves%20reverse%0AKullback-Leibler%20divergence%20minimization%20with%20an%20entropy%20regularizer.%0A%20%20For%20the%20SFT%20of%20Llama-3-8B%20models%2C%20GEM%20outperforms%20CE%20in%20several%20aspects.%0AFirst%2C%20when%20applied%20to%20the%20UltraFeedback%20dataset%20to%20develop%20general%0Ainstruction-following%20abilities%2C%20GEM%20exhibits%20reduced%20overfitting%2C%20evidenced%20by%0Alower%20perplexity%20and%20better%20performance%20on%20the%20IFEval%20benchmark.%20Furthermore%2C%0AGEM%20enhances%20output%20diversity%2C%20leading%20to%20performance%20gains%20of%20up%20to%207%20points%0Aon%20math%20reasoning%20and%20code%20generation%20tasks%20using%20best-of-n%20sampling%2C%20even%0Awithout%20domain-specific%20data.%20Second%2C%20when%20fine-tuning%20with%20domain-specific%0Adatasets%20for%20math%20reasoning%20and%20code%20generation%2C%20GEM%20also%20shows%20less%0Aoverfitting%20and%20improvements%20of%20up%20to%2010%20points%20compared%20with%20CE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropic%2520Distribution%2520Matching%2520in%2520Supervised%2520Fine-tuning%2520of%2520LLMs%253A%2520Less%250A%2520%2520Overfitting%2520and%2520Better%2520Diversity%26entry.906535625%3DZiniu%2520Li%2520and%2520Congliang%2520Chen%2520and%2520Tian%2520Xu%2520and%2520Zeyu%2520Qin%2520and%2520Jiancong%2520Xiao%2520and%2520Ruoyu%2520Sun%2520and%2520Zhi-Quan%2520Luo%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520rely%2520on%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520to%2520specialize%2520in%250Adownstream%2520tasks.%2520Cross%2520Entropy%2520%2528CE%2529%2520loss%2520is%2520the%2520de%2520facto%2520choice%2520in%2520SFT%252C%2520but%2520it%250Aoften%2520leads%2520to%2520overfitting%2520and%2520limited%2520output%2520diversity%2520due%2520to%2520its%2520aggressive%250Aupdates%2520to%2520the%2520data%2520distribution.%2520This%2520paper%2520aim%2520to%2520address%2520these%2520issues%2520by%250Aintroducing%2520the%2520maximum%2520entropy%2520principle%252C%2520which%2520favors%2520models%2520with%2520flatter%250Adistributions%2520that%2520still%2520effectively%2520capture%2520the%2520data.%2520Specifically%252C%2520we%2520develop%250Aa%2520new%2520distribution%2520matching%2520method%2520called%2520GEM%252C%2520which%2520solves%2520reverse%250AKullback-Leibler%2520divergence%2520minimization%2520with%2520an%2520entropy%2520regularizer.%250A%2520%2520For%2520the%2520SFT%2520of%2520Llama-3-8B%2520models%252C%2520GEM%2520outperforms%2520CE%2520in%2520several%2520aspects.%250AFirst%252C%2520when%2520applied%2520to%2520the%2520UltraFeedback%2520dataset%2520to%2520develop%2520general%250Ainstruction-following%2520abilities%252C%2520GEM%2520exhibits%2520reduced%2520overfitting%252C%2520evidenced%2520by%250Alower%2520perplexity%2520and%2520better%2520performance%2520on%2520the%2520IFEval%2520benchmark.%2520Furthermore%252C%250AGEM%2520enhances%2520output%2520diversity%252C%2520leading%2520to%2520performance%2520gains%2520of%2520up%2520to%25207%2520points%250Aon%2520math%2520reasoning%2520and%2520code%2520generation%2520tasks%2520using%2520best-of-n%2520sampling%252C%2520even%250Awithout%2520domain-specific%2520data.%2520Second%252C%2520when%2520fine-tuning%2520with%2520domain-specific%250Adatasets%2520for%2520math%2520reasoning%2520and%2520code%2520generation%252C%2520GEM%2520also%2520shows%2520less%250Aoverfitting%2520and%2520improvements%2520of%2520up%2520to%252010%2520points%2520compared%2520with%2520CE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropic%20Distribution%20Matching%20in%20Supervised%20Fine-tuning%20of%20LLMs%3A%20Less%0A%20%20Overfitting%20and%20Better%20Diversity&entry.906535625=Ziniu%20Li%20and%20Congliang%20Chen%20and%20Tian%20Xu%20and%20Zeyu%20Qin%20and%20Jiancong%20Xiao%20and%20Ruoyu%20Sun%20and%20Zhi-Quan%20Luo&entry.1292438233=%20%20Large%20language%20models%20rely%20on%20Supervised%20Fine-Tuning%20%28SFT%29%20to%20specialize%20in%0Adownstream%20tasks.%20Cross%20Entropy%20%28CE%29%20loss%20is%20the%20de%20facto%20choice%20in%20SFT%2C%20but%20it%0Aoften%20leads%20to%20overfitting%20and%20limited%20output%20diversity%20due%20to%20its%20aggressive%0Aupdates%20to%20the%20data%20distribution.%20This%20paper%20aim%20to%20address%20these%20issues%20by%0Aintroducing%20the%20maximum%20entropy%20principle%2C%20which%20favors%20models%20with%20flatter%0Adistributions%20that%20still%20effectively%20capture%20the%20data.%20Specifically%2C%20we%20develop%0Aa%20new%20distribution%20matching%20method%20called%20GEM%2C%20which%20solves%20reverse%0AKullback-Leibler%20divergence%20minimization%20with%20an%20entropy%20regularizer.%0A%20%20For%20the%20SFT%20of%20Llama-3-8B%20models%2C%20GEM%20outperforms%20CE%20in%20several%20aspects.%0AFirst%2C%20when%20applied%20to%20the%20UltraFeedback%20dataset%20to%20develop%20general%0Ainstruction-following%20abilities%2C%20GEM%20exhibits%20reduced%20overfitting%2C%20evidenced%20by%0Alower%20perplexity%20and%20better%20performance%20on%20the%20IFEval%20benchmark.%20Furthermore%2C%0AGEM%20enhances%20output%20diversity%2C%20leading%20to%20performance%20gains%20of%20up%20to%207%20points%0Aon%20math%20reasoning%20and%20code%20generation%20tasks%20using%20best-of-n%20sampling%2C%20even%0Awithout%20domain-specific%20data.%20Second%2C%20when%20fine-tuning%20with%20domain-specific%0Adatasets%20for%20math%20reasoning%20and%20code%20generation%2C%20GEM%20also%20shows%20less%0Aoverfitting%20and%20improvements%20of%20up%20to%2010%20points%20compared%20with%20CE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16673v1&entry.124074799=Read"},
{"title": "Adaptive Reinforcement Learning Planning: Harnessing Large Language\n  Models for Complex Information Extraction", "author": "Zepeng Ding and Ruiyang Ke and Wenhao Huang and Guochao Jiang and Yanda Li and Deqing Yang and Jiaqing Liang", "abstract": "  Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs.\n", "link": "http://arxiv.org/abs/2406.11455v2", "date": "2024-08-29", "relevancy": 1.9626, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5428}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Reinforcement%20Learning%20Planning%3A%20Harnessing%20Large%20Language%0A%20%20Models%20for%20Complex%20Information%20Extraction&body=Title%3A%20Adaptive%20Reinforcement%20Learning%20Planning%3A%20Harnessing%20Large%20Language%0A%20%20Models%20for%20Complex%20Information%20Extraction%0AAuthor%3A%20Zepeng%20Ding%20and%20Ruiyang%20Ke%20and%20Wenhao%20Huang%20and%20Guochao%20Jiang%20and%20Yanda%20Li%20and%20Deqing%20Yang%20and%20Jiaqing%20Liang%0AAbstract%3A%20%20%20Existing%20research%20on%20large%20language%20models%20%28LLMs%29%20shows%20that%20they%20can%20solve%0Ainformation%20extraction%20tasks%20through%20multi-step%20planning.%20However%2C%20their%0Aextraction%20behavior%20on%20complex%20sentences%20and%20tasks%20is%20unstable%2C%20emerging%20issues%0Asuch%20as%20false%20positives%20and%20missing%20elements.%20We%20observe%20that%20decomposing%0Acomplex%20extraction%20tasks%20and%20extracting%20them%20step%20by%20step%20can%20effectively%0Aimprove%20LLMs%27%20performance%2C%20and%20the%20extraction%20orders%20of%20entities%20significantly%0Aaffect%20the%20final%20results%20of%20LLMs.%20This%20paper%20proposes%20a%20two-stage%20multi-step%0Amethod%20for%20LLM-based%20information%20extraction%20and%20adopts%20the%20RL%20framework%20to%0Aexecute%20the%20multi-step%20planning.%20We%20regard%20sequential%20extraction%20as%20a%20Markov%0Adecision%20process%2C%20build%20an%20LLM-based%20extraction%20environment%2C%20design%20a%20decision%0Amodule%20to%20adaptively%20provide%20the%20optimal%20order%20for%20sequential%20entity%20extraction%0Aon%20different%20sentences%2C%20and%20utilize%20the%20DDQN%20algorithm%20to%20train%20the%20decision%0Amodel.%20We%20also%20design%20the%20rewards%20and%20evaluation%20metrics%20suitable%20for%20the%0Aextraction%20results%20of%20LLMs.%20We%20conduct%20extensive%20experiments%20on%20multiple%20public%0Adatasets%20to%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20improving%20the%0Ainformation%20extraction%20capabilities%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11455v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Reinforcement%2520Learning%2520Planning%253A%2520Harnessing%2520Large%2520Language%250A%2520%2520Models%2520for%2520Complex%2520Information%2520Extraction%26entry.906535625%3DZepeng%2520Ding%2520and%2520Ruiyang%2520Ke%2520and%2520Wenhao%2520Huang%2520and%2520Guochao%2520Jiang%2520and%2520Yanda%2520Li%2520and%2520Deqing%2520Yang%2520and%2520Jiaqing%2520Liang%26entry.1292438233%3D%2520%2520Existing%2520research%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520shows%2520that%2520they%2520can%2520solve%250Ainformation%2520extraction%2520tasks%2520through%2520multi-step%2520planning.%2520However%252C%2520their%250Aextraction%2520behavior%2520on%2520complex%2520sentences%2520and%2520tasks%2520is%2520unstable%252C%2520emerging%2520issues%250Asuch%2520as%2520false%2520positives%2520and%2520missing%2520elements.%2520We%2520observe%2520that%2520decomposing%250Acomplex%2520extraction%2520tasks%2520and%2520extracting%2520them%2520step%2520by%2520step%2520can%2520effectively%250Aimprove%2520LLMs%2527%2520performance%252C%2520and%2520the%2520extraction%2520orders%2520of%2520entities%2520significantly%250Aaffect%2520the%2520final%2520results%2520of%2520LLMs.%2520This%2520paper%2520proposes%2520a%2520two-stage%2520multi-step%250Amethod%2520for%2520LLM-based%2520information%2520extraction%2520and%2520adopts%2520the%2520RL%2520framework%2520to%250Aexecute%2520the%2520multi-step%2520planning.%2520We%2520regard%2520sequential%2520extraction%2520as%2520a%2520Markov%250Adecision%2520process%252C%2520build%2520an%2520LLM-based%2520extraction%2520environment%252C%2520design%2520a%2520decision%250Amodule%2520to%2520adaptively%2520provide%2520the%2520optimal%2520order%2520for%2520sequential%2520entity%2520extraction%250Aon%2520different%2520sentences%252C%2520and%2520utilize%2520the%2520DDQN%2520algorithm%2520to%2520train%2520the%2520decision%250Amodel.%2520We%2520also%2520design%2520the%2520rewards%2520and%2520evaluation%2520metrics%2520suitable%2520for%2520the%250Aextraction%2520results%2520of%2520LLMs.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520multiple%2520public%250Adatasets%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520improving%2520the%250Ainformation%2520extraction%2520capabilities%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11455v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Reinforcement%20Learning%20Planning%3A%20Harnessing%20Large%20Language%0A%20%20Models%20for%20Complex%20Information%20Extraction&entry.906535625=Zepeng%20Ding%20and%20Ruiyang%20Ke%20and%20Wenhao%20Huang%20and%20Guochao%20Jiang%20and%20Yanda%20Li%20and%20Deqing%20Yang%20and%20Jiaqing%20Liang&entry.1292438233=%20%20Existing%20research%20on%20large%20language%20models%20%28LLMs%29%20shows%20that%20they%20can%20solve%0Ainformation%20extraction%20tasks%20through%20multi-step%20planning.%20However%2C%20their%0Aextraction%20behavior%20on%20complex%20sentences%20and%20tasks%20is%20unstable%2C%20emerging%20issues%0Asuch%20as%20false%20positives%20and%20missing%20elements.%20We%20observe%20that%20decomposing%0Acomplex%20extraction%20tasks%20and%20extracting%20them%20step%20by%20step%20can%20effectively%0Aimprove%20LLMs%27%20performance%2C%20and%20the%20extraction%20orders%20of%20entities%20significantly%0Aaffect%20the%20final%20results%20of%20LLMs.%20This%20paper%20proposes%20a%20two-stage%20multi-step%0Amethod%20for%20LLM-based%20information%20extraction%20and%20adopts%20the%20RL%20framework%20to%0Aexecute%20the%20multi-step%20planning.%20We%20regard%20sequential%20extraction%20as%20a%20Markov%0Adecision%20process%2C%20build%20an%20LLM-based%20extraction%20environment%2C%20design%20a%20decision%0Amodule%20to%20adaptively%20provide%20the%20optimal%20order%20for%20sequential%20entity%20extraction%0Aon%20different%20sentences%2C%20and%20utilize%20the%20DDQN%20algorithm%20to%20train%20the%20decision%0Amodel.%20We%20also%20design%20the%20rewards%20and%20evaluation%20metrics%20suitable%20for%20the%0Aextraction%20results%20of%20LLMs.%20We%20conduct%20extensive%20experiments%20on%20multiple%20public%0Adatasets%20to%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20improving%20the%0Ainformation%20extraction%20capabilities%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11455v2&entry.124074799=Read"},
{"title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling", "author": "Hritik Bansal and Arian Hosseini and Rishabh Agarwal and Vinh Q. Tran and Mehran Kazemi", "abstract": "  Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners.\n", "link": "http://arxiv.org/abs/2408.16737v1", "date": "2024-08-29", "relevancy": 1.9554, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5016}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4851}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smaller%2C%20Weaker%2C%20Yet%20Better%3A%20Training%20LLM%20Reasoners%20via%20Compute-Optimal%0A%20%20Sampling&body=Title%3A%20Smaller%2C%20Weaker%2C%20Yet%20Better%3A%20Training%20LLM%20Reasoners%20via%20Compute-Optimal%0A%20%20Sampling%0AAuthor%3A%20Hritik%20Bansal%20and%20Arian%20Hosseini%20and%20Rishabh%20Agarwal%20and%20Vinh%20Q.%20Tran%20and%20Mehran%20Kazemi%0AAbstract%3A%20%20%20Training%20on%20high-quality%20synthetic%20data%20from%20strong%20language%20models%20%28LMs%29%20is%0Aa%20common%20strategy%20to%20improve%20the%20reasoning%20performance%20of%20LMs.%20In%20this%20work%2C%20we%0Arevisit%20whether%20this%20strategy%20is%20compute-optimal%20under%20a%20fixed%20inference%20budget%0A%28e.g.%2C%20FLOPs%29.%20To%20do%20so%2C%20we%20investigate%20the%20trade-offs%20between%20generating%0Asynthetic%20data%20using%20a%20stronger%20but%20more%20expensive%20%28SE%29%20model%20versus%20a%20weaker%0Abut%20cheaper%20%28WC%29%20model.%20We%20evaluate%20the%20generated%20data%20across%20three%20key%0Ametrics%3A%20coverage%2C%20diversity%2C%20and%20false%20positive%20rate%2C%20and%20show%20that%20the%20data%0Afrom%20WC%20models%20may%20have%20higher%20coverage%20and%20diversity%2C%20but%20also%20exhibit%20higher%0Afalse%20positive%20rates.%20We%20then%20finetune%20LMs%20on%20data%20from%20SE%20and%20WC%20models%20in%0Adifferent%20settings%3A%20knowledge%20distillation%2C%20self-improvement%2C%20and%20a%20novel%0Aweak-to-strong%20improvement%20setup%20where%20a%20weaker%20LM%20teaches%20reasoning%20to%20a%0Astronger%20LM.%20Our%20findings%20reveal%20that%20models%20finetuned%20on%20WC-generated%20data%0Aconsistently%20outperform%20those%20trained%20on%20SE-generated%20data%20across%20multiple%0Abenchmarks%20and%20multiple%20choices%20of%20WC%20and%20SE%20models.%20These%20results%20challenge%0Athe%20prevailing%20practice%20of%20relying%20on%20SE%20models%20for%20synthetic%20data%20generation%2C%0Asuggesting%20that%20WC%20may%20be%20the%20compute-optimal%20approach%20for%20training%20advanced%20LM%0Areasoners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmaller%252C%2520Weaker%252C%2520Yet%2520Better%253A%2520Training%2520LLM%2520Reasoners%2520via%2520Compute-Optimal%250A%2520%2520Sampling%26entry.906535625%3DHritik%2520Bansal%2520and%2520Arian%2520Hosseini%2520and%2520Rishabh%2520Agarwal%2520and%2520Vinh%2520Q.%2520Tran%2520and%2520Mehran%2520Kazemi%26entry.1292438233%3D%2520%2520Training%2520on%2520high-quality%2520synthetic%2520data%2520from%2520strong%2520language%2520models%2520%2528LMs%2529%2520is%250Aa%2520common%2520strategy%2520to%2520improve%2520the%2520reasoning%2520performance%2520of%2520LMs.%2520In%2520this%2520work%252C%2520we%250Arevisit%2520whether%2520this%2520strategy%2520is%2520compute-optimal%2520under%2520a%2520fixed%2520inference%2520budget%250A%2528e.g.%252C%2520FLOPs%2529.%2520To%2520do%2520so%252C%2520we%2520investigate%2520the%2520trade-offs%2520between%2520generating%250Asynthetic%2520data%2520using%2520a%2520stronger%2520but%2520more%2520expensive%2520%2528SE%2529%2520model%2520versus%2520a%2520weaker%250Abut%2520cheaper%2520%2528WC%2529%2520model.%2520We%2520evaluate%2520the%2520generated%2520data%2520across%2520three%2520key%250Ametrics%253A%2520coverage%252C%2520diversity%252C%2520and%2520false%2520positive%2520rate%252C%2520and%2520show%2520that%2520the%2520data%250Afrom%2520WC%2520models%2520may%2520have%2520higher%2520coverage%2520and%2520diversity%252C%2520but%2520also%2520exhibit%2520higher%250Afalse%2520positive%2520rates.%2520We%2520then%2520finetune%2520LMs%2520on%2520data%2520from%2520SE%2520and%2520WC%2520models%2520in%250Adifferent%2520settings%253A%2520knowledge%2520distillation%252C%2520self-improvement%252C%2520and%2520a%2520novel%250Aweak-to-strong%2520improvement%2520setup%2520where%2520a%2520weaker%2520LM%2520teaches%2520reasoning%2520to%2520a%250Astronger%2520LM.%2520Our%2520findings%2520reveal%2520that%2520models%2520finetuned%2520on%2520WC-generated%2520data%250Aconsistently%2520outperform%2520those%2520trained%2520on%2520SE-generated%2520data%2520across%2520multiple%250Abenchmarks%2520and%2520multiple%2520choices%2520of%2520WC%2520and%2520SE%2520models.%2520These%2520results%2520challenge%250Athe%2520prevailing%2520practice%2520of%2520relying%2520on%2520SE%2520models%2520for%2520synthetic%2520data%2520generation%252C%250Asuggesting%2520that%2520WC%2520may%2520be%2520the%2520compute-optimal%2520approach%2520for%2520training%2520advanced%2520LM%250Areasoners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smaller%2C%20Weaker%2C%20Yet%20Better%3A%20Training%20LLM%20Reasoners%20via%20Compute-Optimal%0A%20%20Sampling&entry.906535625=Hritik%20Bansal%20and%20Arian%20Hosseini%20and%20Rishabh%20Agarwal%20and%20Vinh%20Q.%20Tran%20and%20Mehran%20Kazemi&entry.1292438233=%20%20Training%20on%20high-quality%20synthetic%20data%20from%20strong%20language%20models%20%28LMs%29%20is%0Aa%20common%20strategy%20to%20improve%20the%20reasoning%20performance%20of%20LMs.%20In%20this%20work%2C%20we%0Arevisit%20whether%20this%20strategy%20is%20compute-optimal%20under%20a%20fixed%20inference%20budget%0A%28e.g.%2C%20FLOPs%29.%20To%20do%20so%2C%20we%20investigate%20the%20trade-offs%20between%20generating%0Asynthetic%20data%20using%20a%20stronger%20but%20more%20expensive%20%28SE%29%20model%20versus%20a%20weaker%0Abut%20cheaper%20%28WC%29%20model.%20We%20evaluate%20the%20generated%20data%20across%20three%20key%0Ametrics%3A%20coverage%2C%20diversity%2C%20and%20false%20positive%20rate%2C%20and%20show%20that%20the%20data%0Afrom%20WC%20models%20may%20have%20higher%20coverage%20and%20diversity%2C%20but%20also%20exhibit%20higher%0Afalse%20positive%20rates.%20We%20then%20finetune%20LMs%20on%20data%20from%20SE%20and%20WC%20models%20in%0Adifferent%20settings%3A%20knowledge%20distillation%2C%20self-improvement%2C%20and%20a%20novel%0Aweak-to-strong%20improvement%20setup%20where%20a%20weaker%20LM%20teaches%20reasoning%20to%20a%0Astronger%20LM.%20Our%20findings%20reveal%20that%20models%20finetuned%20on%20WC-generated%20data%0Aconsistently%20outperform%20those%20trained%20on%20SE-generated%20data%20across%20multiple%0Abenchmarks%20and%20multiple%20choices%20of%20WC%20and%20SE%20models.%20These%20results%20challenge%0Athe%20prevailing%20practice%20of%20relying%20on%20SE%20models%20for%20synthetic%20data%20generation%2C%0Asuggesting%20that%20WC%20may%20be%20the%20compute-optimal%20approach%20for%20training%20advanced%20LM%0Areasoners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16737v1&entry.124074799=Read"},
{"title": "Iterative Graph Alignment", "author": "Fangyuan Yu and Hardeep Singh Arora and Matt Johnson", "abstract": "  By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment.\n", "link": "http://arxiv.org/abs/2408.16667v1", "date": "2024-08-29", "relevancy": 1.9364, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4716}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Graph%20Alignment&body=Title%3A%20Iterative%20Graph%20Alignment%0AAuthor%3A%20Fangyuan%20Yu%20and%20Hardeep%20Singh%20Arora%20and%20Matt%20Johnson%0AAbstract%3A%20%20%20By%20compressing%20diverse%20narratives%2C%20LLMs%20go%20beyond%20memorization%2C%20achieving%0Aintelligence%20by%20capturing%20generalizable%20causal%20relationships.%20However%2C%20they%0Asuffer%20from%20local%20%27representation%20gaps%27%20due%20to%20insufficient%20training%20data%0Adiversity%2C%20limiting%20their%20real-world%20utility%2C%20especially%20in%20tasks%20requiring%0Astrict%20alignment%20to%20rules.%20Traditional%20alignment%20methods%20relying%20on%20heavy%20human%0Aannotations%20are%20inefficient%20and%20unscalable.%20Recent%20self-alignment%20techniques%0Aalso%20fall%20short%2C%20as%20they%20often%20depend%20on%20self-selection%20based%20prompting%20and%0Amemorization-based%20learning.%20To%20address%20these%20issues%2C%20we%20introduce%20Iterative%0AGraph%20Alignment%20%28IGA%29%2C%20an%20annotation-free%20rule-based%20alignment%20algorithm.%20A%0Ateacher%20model%20%28VLM%29%20employs%20Iterative%20Graph%20Prompting%20%28IGP%29%20to%20create%20logical%0Agraphs%20and%20reference%20answers.%20The%20student%20model%20%28LLM%29%20identifies%20local%0Aknowledge%20gaps%20by%20attempting%20to%20align%20its%20responses%20with%20these%20references%2C%0Acollaborating%20with%20helper%20models%20to%20generate%20diverse%20answers.%20These%20aligned%0Aresponses%20are%20then%20used%20for%20iterative%20supervised%20fine-tuning%20%28SFT%29.%20Our%0Aevaluations%20across%20five%20rule-based%20scenarios%20demonstrate%20IGP%27s%20effectiveness%2C%0Awith%20a%2073.12%5C%25%20alignment%20improvement%20in%20Claude%20Sonnet%203.5%2C%20and%0ALlama3-8B-Instruct%20achieving%20an%2086.20%5C%25%20improvement%2C%20outperforming%20Claude%0ASonnet%203.5%20in%20rule-based%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Graph%2520Alignment%26entry.906535625%3DFangyuan%2520Yu%2520and%2520Hardeep%2520Singh%2520Arora%2520and%2520Matt%2520Johnson%26entry.1292438233%3D%2520%2520By%2520compressing%2520diverse%2520narratives%252C%2520LLMs%2520go%2520beyond%2520memorization%252C%2520achieving%250Aintelligence%2520by%2520capturing%2520generalizable%2520causal%2520relationships.%2520However%252C%2520they%250Asuffer%2520from%2520local%2520%2527representation%2520gaps%2527%2520due%2520to%2520insufficient%2520training%2520data%250Adiversity%252C%2520limiting%2520their%2520real-world%2520utility%252C%2520especially%2520in%2520tasks%2520requiring%250Astrict%2520alignment%2520to%2520rules.%2520Traditional%2520alignment%2520methods%2520relying%2520on%2520heavy%2520human%250Aannotations%2520are%2520inefficient%2520and%2520unscalable.%2520Recent%2520self-alignment%2520techniques%250Aalso%2520fall%2520short%252C%2520as%2520they%2520often%2520depend%2520on%2520self-selection%2520based%2520prompting%2520and%250Amemorization-based%2520learning.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Iterative%250AGraph%2520Alignment%2520%2528IGA%2529%252C%2520an%2520annotation-free%2520rule-based%2520alignment%2520algorithm.%2520A%250Ateacher%2520model%2520%2528VLM%2529%2520employs%2520Iterative%2520Graph%2520Prompting%2520%2528IGP%2529%2520to%2520create%2520logical%250Agraphs%2520and%2520reference%2520answers.%2520The%2520student%2520model%2520%2528LLM%2529%2520identifies%2520local%250Aknowledge%2520gaps%2520by%2520attempting%2520to%2520align%2520its%2520responses%2520with%2520these%2520references%252C%250Acollaborating%2520with%2520helper%2520models%2520to%2520generate%2520diverse%2520answers.%2520These%2520aligned%250Aresponses%2520are%2520then%2520used%2520for%2520iterative%2520supervised%2520fine-tuning%2520%2528SFT%2529.%2520Our%250Aevaluations%2520across%2520five%2520rule-based%2520scenarios%2520demonstrate%2520IGP%2527s%2520effectiveness%252C%250Awith%2520a%252073.12%255C%2525%2520alignment%2520improvement%2520in%2520Claude%2520Sonnet%25203.5%252C%2520and%250ALlama3-8B-Instruct%2520achieving%2520an%252086.20%255C%2525%2520improvement%252C%2520outperforming%2520Claude%250ASonnet%25203.5%2520in%2520rule-based%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Graph%20Alignment&entry.906535625=Fangyuan%20Yu%20and%20Hardeep%20Singh%20Arora%20and%20Matt%20Johnson&entry.1292438233=%20%20By%20compressing%20diverse%20narratives%2C%20LLMs%20go%20beyond%20memorization%2C%20achieving%0Aintelligence%20by%20capturing%20generalizable%20causal%20relationships.%20However%2C%20they%0Asuffer%20from%20local%20%27representation%20gaps%27%20due%20to%20insufficient%20training%20data%0Adiversity%2C%20limiting%20their%20real-world%20utility%2C%20especially%20in%20tasks%20requiring%0Astrict%20alignment%20to%20rules.%20Traditional%20alignment%20methods%20relying%20on%20heavy%20human%0Aannotations%20are%20inefficient%20and%20unscalable.%20Recent%20self-alignment%20techniques%0Aalso%20fall%20short%2C%20as%20they%20often%20depend%20on%20self-selection%20based%20prompting%20and%0Amemorization-based%20learning.%20To%20address%20these%20issues%2C%20we%20introduce%20Iterative%0AGraph%20Alignment%20%28IGA%29%2C%20an%20annotation-free%20rule-based%20alignment%20algorithm.%20A%0Ateacher%20model%20%28VLM%29%20employs%20Iterative%20Graph%20Prompting%20%28IGP%29%20to%20create%20logical%0Agraphs%20and%20reference%20answers.%20The%20student%20model%20%28LLM%29%20identifies%20local%0Aknowledge%20gaps%20by%20attempting%20to%20align%20its%20responses%20with%20these%20references%2C%0Acollaborating%20with%20helper%20models%20to%20generate%20diverse%20answers.%20These%20aligned%0Aresponses%20are%20then%20used%20for%20iterative%20supervised%20fine-tuning%20%28SFT%29.%20Our%0Aevaluations%20across%20five%20rule-based%20scenarios%20demonstrate%20IGP%27s%20effectiveness%2C%0Awith%20a%2073.12%5C%25%20alignment%20improvement%20in%20Claude%20Sonnet%203.5%2C%20and%0ALlama3-8B-Instruct%20achieving%20an%2086.20%5C%25%20improvement%2C%20outperforming%20Claude%0ASonnet%203.5%20in%20rule-based%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16667v1&entry.124074799=Read"},
{"title": "Can LLMs perform structured graph reasoning?", "author": "Palaash Agrawal and Shavak Vasania and Cheston Tan", "abstract": "  Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT).\n", "link": "http://arxiv.org/abs/2402.01805v4", "date": "2024-08-29", "relevancy": 1.9335, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5007}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4925}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20perform%20structured%20graph%20reasoning%3F&body=Title%3A%20Can%20LLMs%20perform%20structured%20graph%20reasoning%3F%0AAuthor%3A%20Palaash%20Agrawal%20and%20Shavak%20Vasania%20and%20Cheston%20Tan%0AAbstract%3A%20%20%20Pretrained%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20various%20reasoning%0Acapabilities%20through%20language-based%20prompts%20alone%2C%20particularly%20in%20unstructured%0Atask%20settings%20%28tasks%20purely%20based%20on%20language%20semantics%29.%20However%2C%20LLMs%20often%0Astruggle%20with%20structured%20tasks%2C%20because%20of%20the%20inherent%20incompatibility%20of%0Ainput%20representation.%20Reducing%20structured%20tasks%20to%20uni-dimensional%20language%0Asemantics%20often%20renders%20the%20problem%20trivial.%20Keeping%20the%20trade-off%20between%20LLM%0Acompatibility%20and%20structure%20complexity%20in%20mind%2C%20we%20design%20various%20graph%0Areasoning%20tasks%20as%20a%20proxy%20to%20semi-structured%20tasks%20in%20this%20paper%2C%20in%20order%20to%0Atest%20the%20ability%20to%20navigate%20through%20representations%20beyond%20plain%20text%20in%0Avarious%20LLMs.%20Particularly%2C%20we%20design%2010%20distinct%20problems%20of%20graph%20traversal%2C%0Aeach%20representing%20increasing%20levels%20of%20complexity%2C%20and%20benchmark%205%20different%0Ainstruct-finetuned%20LLMs%20%28GPT-4%2C%20GPT-3.5%2C%20Claude-2%2C%20Llama-2%20and%20Palm-2%29%20on%20the%0Aaforementioned%20tasks.%20Further%2C%20we%20analyse%20the%20performance%20of%20models%20across%0Avarious%20settings%20such%20as%20varying%20sizes%20of%20graphs%20as%20well%20as%20different%20forms%20of%0Ak-shot%20prompting.%20We%20highlight%20various%20limitations%2C%20biases%20and%20properties%20of%0ALLMs%20through%20this%20benchmarking%20process%2C%20such%20as%20an%20inverse%20relation%20to%20the%0Aaverage%20degrees%20of%20freedom%20of%20traversal%20per%20node%20in%20graphs%2C%20the%20overall%0Anegative%20impact%20of%20k-shot%20prompting%20on%20graph%20reasoning%20tasks%2C%20and%20a%20positive%0Aresponse%20bias%20which%20prevents%20LLMs%20from%20identifying%20the%20absence%20of%20a%20valid%0Asolution.%20Finally%2C%20we%20introduce%20a%20new%20prompting%20technique%20specially%20designed%0Afor%20graph%20traversal%20tasks%20%28PathCompare%29%2C%20which%20demonstrates%20a%20notable%20increase%0Ain%20the%20performance%20of%20LLMs%20in%20comparison%20to%20standard%20prompting%20techniques%20such%0Aas%20Chain-of-Thought%20%28CoT%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01805v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520perform%2520structured%2520graph%2520reasoning%253F%26entry.906535625%3DPalaash%2520Agrawal%2520and%2520Shavak%2520Vasania%2520and%2520Cheston%2520Tan%26entry.1292438233%3D%2520%2520Pretrained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520various%2520reasoning%250Acapabilities%2520through%2520language-based%2520prompts%2520alone%252C%2520particularly%2520in%2520unstructured%250Atask%2520settings%2520%2528tasks%2520purely%2520based%2520on%2520language%2520semantics%2529.%2520However%252C%2520LLMs%2520often%250Astruggle%2520with%2520structured%2520tasks%252C%2520because%2520of%2520the%2520inherent%2520incompatibility%2520of%250Ainput%2520representation.%2520Reducing%2520structured%2520tasks%2520to%2520uni-dimensional%2520language%250Asemantics%2520often%2520renders%2520the%2520problem%2520trivial.%2520Keeping%2520the%2520trade-off%2520between%2520LLM%250Acompatibility%2520and%2520structure%2520complexity%2520in%2520mind%252C%2520we%2520design%2520various%2520graph%250Areasoning%2520tasks%2520as%2520a%2520proxy%2520to%2520semi-structured%2520tasks%2520in%2520this%2520paper%252C%2520in%2520order%2520to%250Atest%2520the%2520ability%2520to%2520navigate%2520through%2520representations%2520beyond%2520plain%2520text%2520in%250Avarious%2520LLMs.%2520Particularly%252C%2520we%2520design%252010%2520distinct%2520problems%2520of%2520graph%2520traversal%252C%250Aeach%2520representing%2520increasing%2520levels%2520of%2520complexity%252C%2520and%2520benchmark%25205%2520different%250Ainstruct-finetuned%2520LLMs%2520%2528GPT-4%252C%2520GPT-3.5%252C%2520Claude-2%252C%2520Llama-2%2520and%2520Palm-2%2529%2520on%2520the%250Aaforementioned%2520tasks.%2520Further%252C%2520we%2520analyse%2520the%2520performance%2520of%2520models%2520across%250Avarious%2520settings%2520such%2520as%2520varying%2520sizes%2520of%2520graphs%2520as%2520well%2520as%2520different%2520forms%2520of%250Ak-shot%2520prompting.%2520We%2520highlight%2520various%2520limitations%252C%2520biases%2520and%2520properties%2520of%250ALLMs%2520through%2520this%2520benchmarking%2520process%252C%2520such%2520as%2520an%2520inverse%2520relation%2520to%2520the%250Aaverage%2520degrees%2520of%2520freedom%2520of%2520traversal%2520per%2520node%2520in%2520graphs%252C%2520the%2520overall%250Anegative%2520impact%2520of%2520k-shot%2520prompting%2520on%2520graph%2520reasoning%2520tasks%252C%2520and%2520a%2520positive%250Aresponse%2520bias%2520which%2520prevents%2520LLMs%2520from%2520identifying%2520the%2520absence%2520of%2520a%2520valid%250Asolution.%2520Finally%252C%2520we%2520introduce%2520a%2520new%2520prompting%2520technique%2520specially%2520designed%250Afor%2520graph%2520traversal%2520tasks%2520%2528PathCompare%2529%252C%2520which%2520demonstrates%2520a%2520notable%2520increase%250Ain%2520the%2520performance%2520of%2520LLMs%2520in%2520comparison%2520to%2520standard%2520prompting%2520techniques%2520such%250Aas%2520Chain-of-Thought%2520%2528CoT%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01805v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20perform%20structured%20graph%20reasoning%3F&entry.906535625=Palaash%20Agrawal%20and%20Shavak%20Vasania%20and%20Cheston%20Tan&entry.1292438233=%20%20Pretrained%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20various%20reasoning%0Acapabilities%20through%20language-based%20prompts%20alone%2C%20particularly%20in%20unstructured%0Atask%20settings%20%28tasks%20purely%20based%20on%20language%20semantics%29.%20However%2C%20LLMs%20often%0Astruggle%20with%20structured%20tasks%2C%20because%20of%20the%20inherent%20incompatibility%20of%0Ainput%20representation.%20Reducing%20structured%20tasks%20to%20uni-dimensional%20language%0Asemantics%20often%20renders%20the%20problem%20trivial.%20Keeping%20the%20trade-off%20between%20LLM%0Acompatibility%20and%20structure%20complexity%20in%20mind%2C%20we%20design%20various%20graph%0Areasoning%20tasks%20as%20a%20proxy%20to%20semi-structured%20tasks%20in%20this%20paper%2C%20in%20order%20to%0Atest%20the%20ability%20to%20navigate%20through%20representations%20beyond%20plain%20text%20in%0Avarious%20LLMs.%20Particularly%2C%20we%20design%2010%20distinct%20problems%20of%20graph%20traversal%2C%0Aeach%20representing%20increasing%20levels%20of%20complexity%2C%20and%20benchmark%205%20different%0Ainstruct-finetuned%20LLMs%20%28GPT-4%2C%20GPT-3.5%2C%20Claude-2%2C%20Llama-2%20and%20Palm-2%29%20on%20the%0Aaforementioned%20tasks.%20Further%2C%20we%20analyse%20the%20performance%20of%20models%20across%0Avarious%20settings%20such%20as%20varying%20sizes%20of%20graphs%20as%20well%20as%20different%20forms%20of%0Ak-shot%20prompting.%20We%20highlight%20various%20limitations%2C%20biases%20and%20properties%20of%0ALLMs%20through%20this%20benchmarking%20process%2C%20such%20as%20an%20inverse%20relation%20to%20the%0Aaverage%20degrees%20of%20freedom%20of%20traversal%20per%20node%20in%20graphs%2C%20the%20overall%0Anegative%20impact%20of%20k-shot%20prompting%20on%20graph%20reasoning%20tasks%2C%20and%20a%20positive%0Aresponse%20bias%20which%20prevents%20LLMs%20from%20identifying%20the%20absence%20of%20a%20valid%0Asolution.%20Finally%2C%20we%20introduce%20a%20new%20prompting%20technique%20specially%20designed%0Afor%20graph%20traversal%20tasks%20%28PathCompare%29%2C%20which%20demonstrates%20a%20notable%20increase%0Ain%20the%20performance%20of%20LLMs%20in%20comparison%20to%20standard%20prompting%20techniques%20such%0Aas%20Chain-of-Thought%20%28CoT%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01805v4&entry.124074799=Read"},
{"title": "Misam: Using ML in Dataflow Selection of Sparse-Sparse Matrix\n  Multiplication", "author": "Sanjali Yadav and Bahar Asgari", "abstract": "  Sparse matrix-matrix multiplication (SpGEMM) is a critical operation in\nnumerous fields, including scientific computing, graph analytics, and deep\nlearning. These applications exploit the sparsity of matrices to reduce storage\nand computational demands. However, the irregular structure of sparse matrices\nposes significant challenges for performance optimization. Traditional hardware\naccelerators are tailored for specific sparsity patterns with fixed dataflow\nschemes - inner, outer, and row-wise but often perform suboptimally when the\nactual sparsity deviates from these predetermined patterns. As the use of\nSpGEMM expands across various domains, each with distinct sparsity\ncharacteristics, the demand for hardware accelerators that can efficiently\nhandle a range of sparsity patterns is increasing. This paper presents a\nmachine learning based approach for adaptively selecting the most appropriate\ndataflow scheme for SpGEMM tasks with diverse sparsity patterns. By employing\ndecision trees and deep reinforcement learning, we explore the potential of\nthese techniques to surpass heuristic-based methods in identifying optimal\ndataflow schemes. We evaluate our models by comparing their performance with\nthat of a heuristic, highlighting the strengths and weaknesses of each\napproach. Our findings suggest that using machine learning for dynamic dataflow\nselection in hardware accelerators can provide upto 28 times gains.\n", "link": "http://arxiv.org/abs/2406.10166v2", "date": "2024-08-29", "relevancy": 1.9329, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4892}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4814}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Misam%3A%20Using%20ML%20in%20Dataflow%20Selection%20of%20Sparse-Sparse%20Matrix%0A%20%20Multiplication&body=Title%3A%20Misam%3A%20Using%20ML%20in%20Dataflow%20Selection%20of%20Sparse-Sparse%20Matrix%0A%20%20Multiplication%0AAuthor%3A%20Sanjali%20Yadav%20and%20Bahar%20Asgari%0AAbstract%3A%20%20%20Sparse%20matrix-matrix%20multiplication%20%28SpGEMM%29%20is%20a%20critical%20operation%20in%0Anumerous%20fields%2C%20including%20scientific%20computing%2C%20graph%20analytics%2C%20and%20deep%0Alearning.%20These%20applications%20exploit%20the%20sparsity%20of%20matrices%20to%20reduce%20storage%0Aand%20computational%20demands.%20However%2C%20the%20irregular%20structure%20of%20sparse%20matrices%0Aposes%20significant%20challenges%20for%20performance%20optimization.%20Traditional%20hardware%0Aaccelerators%20are%20tailored%20for%20specific%20sparsity%20patterns%20with%20fixed%20dataflow%0Aschemes%20-%20inner%2C%20outer%2C%20and%20row-wise%20but%20often%20perform%20suboptimally%20when%20the%0Aactual%20sparsity%20deviates%20from%20these%20predetermined%20patterns.%20As%20the%20use%20of%0ASpGEMM%20expands%20across%20various%20domains%2C%20each%20with%20distinct%20sparsity%0Acharacteristics%2C%20the%20demand%20for%20hardware%20accelerators%20that%20can%20efficiently%0Ahandle%20a%20range%20of%20sparsity%20patterns%20is%20increasing.%20This%20paper%20presents%20a%0Amachine%20learning%20based%20approach%20for%20adaptively%20selecting%20the%20most%20appropriate%0Adataflow%20scheme%20for%20SpGEMM%20tasks%20with%20diverse%20sparsity%20patterns.%20By%20employing%0Adecision%20trees%20and%20deep%20reinforcement%20learning%2C%20we%20explore%20the%20potential%20of%0Athese%20techniques%20to%20surpass%20heuristic-based%20methods%20in%20identifying%20optimal%0Adataflow%20schemes.%20We%20evaluate%20our%20models%20by%20comparing%20their%20performance%20with%0Athat%20of%20a%20heuristic%2C%20highlighting%20the%20strengths%20and%20weaknesses%20of%20each%0Aapproach.%20Our%20findings%20suggest%20that%20using%20machine%20learning%20for%20dynamic%20dataflow%0Aselection%20in%20hardware%20accelerators%20can%20provide%20upto%2028%20times%20gains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10166v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMisam%253A%2520Using%2520ML%2520in%2520Dataflow%2520Selection%2520of%2520Sparse-Sparse%2520Matrix%250A%2520%2520Multiplication%26entry.906535625%3DSanjali%2520Yadav%2520and%2520Bahar%2520Asgari%26entry.1292438233%3D%2520%2520Sparse%2520matrix-matrix%2520multiplication%2520%2528SpGEMM%2529%2520is%2520a%2520critical%2520operation%2520in%250Anumerous%2520fields%252C%2520including%2520scientific%2520computing%252C%2520graph%2520analytics%252C%2520and%2520deep%250Alearning.%2520These%2520applications%2520exploit%2520the%2520sparsity%2520of%2520matrices%2520to%2520reduce%2520storage%250Aand%2520computational%2520demands.%2520However%252C%2520the%2520irregular%2520structure%2520of%2520sparse%2520matrices%250Aposes%2520significant%2520challenges%2520for%2520performance%2520optimization.%2520Traditional%2520hardware%250Aaccelerators%2520are%2520tailored%2520for%2520specific%2520sparsity%2520patterns%2520with%2520fixed%2520dataflow%250Aschemes%2520-%2520inner%252C%2520outer%252C%2520and%2520row-wise%2520but%2520often%2520perform%2520suboptimally%2520when%2520the%250Aactual%2520sparsity%2520deviates%2520from%2520these%2520predetermined%2520patterns.%2520As%2520the%2520use%2520of%250ASpGEMM%2520expands%2520across%2520various%2520domains%252C%2520each%2520with%2520distinct%2520sparsity%250Acharacteristics%252C%2520the%2520demand%2520for%2520hardware%2520accelerators%2520that%2520can%2520efficiently%250Ahandle%2520a%2520range%2520of%2520sparsity%2520patterns%2520is%2520increasing.%2520This%2520paper%2520presents%2520a%250Amachine%2520learning%2520based%2520approach%2520for%2520adaptively%2520selecting%2520the%2520most%2520appropriate%250Adataflow%2520scheme%2520for%2520SpGEMM%2520tasks%2520with%2520diverse%2520sparsity%2520patterns.%2520By%2520employing%250Adecision%2520trees%2520and%2520deep%2520reinforcement%2520learning%252C%2520we%2520explore%2520the%2520potential%2520of%250Athese%2520techniques%2520to%2520surpass%2520heuristic-based%2520methods%2520in%2520identifying%2520optimal%250Adataflow%2520schemes.%2520We%2520evaluate%2520our%2520models%2520by%2520comparing%2520their%2520performance%2520with%250Athat%2520of%2520a%2520heuristic%252C%2520highlighting%2520the%2520strengths%2520and%2520weaknesses%2520of%2520each%250Aapproach.%2520Our%2520findings%2520suggest%2520that%2520using%2520machine%2520learning%2520for%2520dynamic%2520dataflow%250Aselection%2520in%2520hardware%2520accelerators%2520can%2520provide%2520upto%252028%2520times%2520gains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10166v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Misam%3A%20Using%20ML%20in%20Dataflow%20Selection%20of%20Sparse-Sparse%20Matrix%0A%20%20Multiplication&entry.906535625=Sanjali%20Yadav%20and%20Bahar%20Asgari&entry.1292438233=%20%20Sparse%20matrix-matrix%20multiplication%20%28SpGEMM%29%20is%20a%20critical%20operation%20in%0Anumerous%20fields%2C%20including%20scientific%20computing%2C%20graph%20analytics%2C%20and%20deep%0Alearning.%20These%20applications%20exploit%20the%20sparsity%20of%20matrices%20to%20reduce%20storage%0Aand%20computational%20demands.%20However%2C%20the%20irregular%20structure%20of%20sparse%20matrices%0Aposes%20significant%20challenges%20for%20performance%20optimization.%20Traditional%20hardware%0Aaccelerators%20are%20tailored%20for%20specific%20sparsity%20patterns%20with%20fixed%20dataflow%0Aschemes%20-%20inner%2C%20outer%2C%20and%20row-wise%20but%20often%20perform%20suboptimally%20when%20the%0Aactual%20sparsity%20deviates%20from%20these%20predetermined%20patterns.%20As%20the%20use%20of%0ASpGEMM%20expands%20across%20various%20domains%2C%20each%20with%20distinct%20sparsity%0Acharacteristics%2C%20the%20demand%20for%20hardware%20accelerators%20that%20can%20efficiently%0Ahandle%20a%20range%20of%20sparsity%20patterns%20is%20increasing.%20This%20paper%20presents%20a%0Amachine%20learning%20based%20approach%20for%20adaptively%20selecting%20the%20most%20appropriate%0Adataflow%20scheme%20for%20SpGEMM%20tasks%20with%20diverse%20sparsity%20patterns.%20By%20employing%0Adecision%20trees%20and%20deep%20reinforcement%20learning%2C%20we%20explore%20the%20potential%20of%0Athese%20techniques%20to%20surpass%20heuristic-based%20methods%20in%20identifying%20optimal%0Adataflow%20schemes.%20We%20evaluate%20our%20models%20by%20comparing%20their%20performance%20with%0Athat%20of%20a%20heuristic%2C%20highlighting%20the%20strengths%20and%20weaknesses%20of%20each%0Aapproach.%20Our%20findings%20suggest%20that%20using%20machine%20learning%20for%20dynamic%20dataflow%0Aselection%20in%20hardware%20accelerators%20can%20provide%20upto%2028%20times%20gains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10166v2&entry.124074799=Read"},
{"title": "Wasserstein Gradient Boosting: A Framework for Distribution-Valued\n  Supervised Learning", "author": "Takuo Matsubara", "abstract": "  Gradient boosting is a sequential ensemble method that fits a new weaker\nlearner to pseudo residuals at each iteration. We propose Wasserstein gradient\nboosting, a novel extension of gradient boosting that fits a new weak learner\nto alternative pseudo residuals that are Wasserstein gradients of loss\nfunctionals of probability distributions assigned at each input. It solves\ndistribution-valued supervised learning, where the output values of the\ntraining dataset are probability distributions for each input. In\nclassification and regression, a model typically returns, for each input, a\npoint estimate of a parameter of a noise distribution specified for a response\nvariable, such as the class probability parameter of a categorical distribution\nspecified for a response label. A main application of Wasserstein gradient\nboosting in this paper is tree-based evidential learning, which returns a\ndistributional estimate of the response parameter for each input. We\nempirically demonstrate the superior performance of the probabilistic\nprediction by Wasserstein gradient boosting in comparison with existing\nuncertainty quantification methods.\n", "link": "http://arxiv.org/abs/2405.09536v2", "date": "2024-08-29", "relevancy": 1.9278, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5079}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.482}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wasserstein%20Gradient%20Boosting%3A%20A%20Framework%20for%20Distribution-Valued%0A%20%20Supervised%20Learning&body=Title%3A%20Wasserstein%20Gradient%20Boosting%3A%20A%20Framework%20for%20Distribution-Valued%0A%20%20Supervised%20Learning%0AAuthor%3A%20Takuo%20Matsubara%0AAbstract%3A%20%20%20Gradient%20boosting%20is%20a%20sequential%20ensemble%20method%20that%20fits%20a%20new%20weaker%0Alearner%20to%20pseudo%20residuals%20at%20each%20iteration.%20We%20propose%20Wasserstein%20gradient%0Aboosting%2C%20a%20novel%20extension%20of%20gradient%20boosting%20that%20fits%20a%20new%20weak%20learner%0Ato%20alternative%20pseudo%20residuals%20that%20are%20Wasserstein%20gradients%20of%20loss%0Afunctionals%20of%20probability%20distributions%20assigned%20at%20each%20input.%20It%20solves%0Adistribution-valued%20supervised%20learning%2C%20where%20the%20output%20values%20of%20the%0Atraining%20dataset%20are%20probability%20distributions%20for%20each%20input.%20In%0Aclassification%20and%20regression%2C%20a%20model%20typically%20returns%2C%20for%20each%20input%2C%20a%0Apoint%20estimate%20of%20a%20parameter%20of%20a%20noise%20distribution%20specified%20for%20a%20response%0Avariable%2C%20such%20as%20the%20class%20probability%20parameter%20of%20a%20categorical%20distribution%0Aspecified%20for%20a%20response%20label.%20A%20main%20application%20of%20Wasserstein%20gradient%0Aboosting%20in%20this%20paper%20is%20tree-based%20evidential%20learning%2C%20which%20returns%20a%0Adistributional%20estimate%20of%20the%20response%20parameter%20for%20each%20input.%20We%0Aempirically%20demonstrate%20the%20superior%20performance%20of%20the%20probabilistic%0Aprediction%20by%20Wasserstein%20gradient%20boosting%20in%20comparison%20with%20existing%0Auncertainty%20quantification%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09536v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWasserstein%2520Gradient%2520Boosting%253A%2520A%2520Framework%2520for%2520Distribution-Valued%250A%2520%2520Supervised%2520Learning%26entry.906535625%3DTakuo%2520Matsubara%26entry.1292438233%3D%2520%2520Gradient%2520boosting%2520is%2520a%2520sequential%2520ensemble%2520method%2520that%2520fits%2520a%2520new%2520weaker%250Alearner%2520to%2520pseudo%2520residuals%2520at%2520each%2520iteration.%2520We%2520propose%2520Wasserstein%2520gradient%250Aboosting%252C%2520a%2520novel%2520extension%2520of%2520gradient%2520boosting%2520that%2520fits%2520a%2520new%2520weak%2520learner%250Ato%2520alternative%2520pseudo%2520residuals%2520that%2520are%2520Wasserstein%2520gradients%2520of%2520loss%250Afunctionals%2520of%2520probability%2520distributions%2520assigned%2520at%2520each%2520input.%2520It%2520solves%250Adistribution-valued%2520supervised%2520learning%252C%2520where%2520the%2520output%2520values%2520of%2520the%250Atraining%2520dataset%2520are%2520probability%2520distributions%2520for%2520each%2520input.%2520In%250Aclassification%2520and%2520regression%252C%2520a%2520model%2520typically%2520returns%252C%2520for%2520each%2520input%252C%2520a%250Apoint%2520estimate%2520of%2520a%2520parameter%2520of%2520a%2520noise%2520distribution%2520specified%2520for%2520a%2520response%250Avariable%252C%2520such%2520as%2520the%2520class%2520probability%2520parameter%2520of%2520a%2520categorical%2520distribution%250Aspecified%2520for%2520a%2520response%2520label.%2520A%2520main%2520application%2520of%2520Wasserstein%2520gradient%250Aboosting%2520in%2520this%2520paper%2520is%2520tree-based%2520evidential%2520learning%252C%2520which%2520returns%2520a%250Adistributional%2520estimate%2520of%2520the%2520response%2520parameter%2520for%2520each%2520input.%2520We%250Aempirically%2520demonstrate%2520the%2520superior%2520performance%2520of%2520the%2520probabilistic%250Aprediction%2520by%2520Wasserstein%2520gradient%2520boosting%2520in%2520comparison%2520with%2520existing%250Auncertainty%2520quantification%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09536v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wasserstein%20Gradient%20Boosting%3A%20A%20Framework%20for%20Distribution-Valued%0A%20%20Supervised%20Learning&entry.906535625=Takuo%20Matsubara&entry.1292438233=%20%20Gradient%20boosting%20is%20a%20sequential%20ensemble%20method%20that%20fits%20a%20new%20weaker%0Alearner%20to%20pseudo%20residuals%20at%20each%20iteration.%20We%20propose%20Wasserstein%20gradient%0Aboosting%2C%20a%20novel%20extension%20of%20gradient%20boosting%20that%20fits%20a%20new%20weak%20learner%0Ato%20alternative%20pseudo%20residuals%20that%20are%20Wasserstein%20gradients%20of%20loss%0Afunctionals%20of%20probability%20distributions%20assigned%20at%20each%20input.%20It%20solves%0Adistribution-valued%20supervised%20learning%2C%20where%20the%20output%20values%20of%20the%0Atraining%20dataset%20are%20probability%20distributions%20for%20each%20input.%20In%0Aclassification%20and%20regression%2C%20a%20model%20typically%20returns%2C%20for%20each%20input%2C%20a%0Apoint%20estimate%20of%20a%20parameter%20of%20a%20noise%20distribution%20specified%20for%20a%20response%0Avariable%2C%20such%20as%20the%20class%20probability%20parameter%20of%20a%20categorical%20distribution%0Aspecified%20for%20a%20response%20label.%20A%20main%20application%20of%20Wasserstein%20gradient%0Aboosting%20in%20this%20paper%20is%20tree-based%20evidential%20learning%2C%20which%20returns%20a%0Adistributional%20estimate%20of%20the%20response%20parameter%20for%20each%20input.%20We%0Aempirically%20demonstrate%20the%20superior%20performance%20of%20the%20probabilistic%0Aprediction%20by%20Wasserstein%20gradient%20boosting%20in%20comparison%20with%20existing%0Auncertainty%20quantification%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09536v2&entry.124074799=Read"},
{"title": "Not (yet) the whole story: Evaluating Visual Storytelling Requires More\n  than Measuring Coherence, Grounding, and Repetition", "author": "Aditya K Surikuchi and Raquel Fern\u00e1ndez and Sandro Pezzelle", "abstract": "  Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition.\n", "link": "http://arxiv.org/abs/2407.04559v2", "date": "2024-08-29", "relevancy": 1.9274, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.472}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20%28yet%29%20the%20whole%20story%3A%20Evaluating%20Visual%20Storytelling%20Requires%20More%0A%20%20than%20Measuring%20Coherence%2C%20Grounding%2C%20and%20Repetition&body=Title%3A%20Not%20%28yet%29%20the%20whole%20story%3A%20Evaluating%20Visual%20Storytelling%20Requires%20More%0A%20%20than%20Measuring%20Coherence%2C%20Grounding%2C%20and%20Repetition%0AAuthor%3A%20Aditya%20K%20Surikuchi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle%0AAbstract%3A%20%20%20Visual%20storytelling%20consists%20in%20generating%20a%20natural%20language%20story%20given%20a%0Atemporally%20ordered%20sequence%20of%20images.%20This%20task%20is%20not%20only%20challenging%20for%0Amodels%2C%20but%20also%20very%20difficult%20to%20evaluate%20with%20automatic%20metrics%20since%20there%0Ais%20no%20consensus%20about%20what%20makes%20a%20story%20%27good%27.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20method%20that%20measures%20story%20quality%20in%20terms%20of%20human%20likeness%20regarding%0Athree%20key%20aspects%20highlighted%20in%20previous%20work%3A%20visual%20grounding%2C%20coherence%2C%0Aand%20repetitiveness.%20We%20then%20use%20this%20method%20to%20evaluate%20the%20stories%20generated%0Aby%20several%20models%2C%20showing%20that%20the%20foundation%20model%20LLaVA%20obtains%20the%20best%0Aresult%2C%20but%20only%20slightly%20so%20compared%20to%20TAPM%2C%20a%2050-times%20smaller%20visual%0Astorytelling%20model.%20Upgrading%20the%20visual%20and%20language%20components%20of%20TAPM%0Aresults%20in%20a%20model%20that%20yields%20competitive%20performance%20with%20a%20relatively%20low%0Anumber%20of%20parameters.%20Finally%2C%20we%20carry%20out%20a%20human%20evaluation%20study%2C%20whose%0Aresults%20suggest%20that%20a%20%27good%27%20story%20may%20require%20more%20than%20a%20human-like%20level%20of%0Avisual%20grounding%2C%20coherence%2C%20and%20repetition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520%2528yet%2529%2520the%2520whole%2520story%253A%2520Evaluating%2520Visual%2520Storytelling%2520Requires%2520More%250A%2520%2520than%2520Measuring%2520Coherence%252C%2520Grounding%252C%2520and%2520Repetition%26entry.906535625%3DAditya%2520K%2520Surikuchi%2520and%2520Raquel%2520Fern%25C3%25A1ndez%2520and%2520Sandro%2520Pezzelle%26entry.1292438233%3D%2520%2520Visual%2520storytelling%2520consists%2520in%2520generating%2520a%2520natural%2520language%2520story%2520given%2520a%250Atemporally%2520ordered%2520sequence%2520of%2520images.%2520This%2520task%2520is%2520not%2520only%2520challenging%2520for%250Amodels%252C%2520but%2520also%2520very%2520difficult%2520to%2520evaluate%2520with%2520automatic%2520metrics%2520since%2520there%250Ais%2520no%2520consensus%2520about%2520what%2520makes%2520a%2520story%2520%2527good%2527.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Anovel%2520method%2520that%2520measures%2520story%2520quality%2520in%2520terms%2520of%2520human%2520likeness%2520regarding%250Athree%2520key%2520aspects%2520highlighted%2520in%2520previous%2520work%253A%2520visual%2520grounding%252C%2520coherence%252C%250Aand%2520repetitiveness.%2520We%2520then%2520use%2520this%2520method%2520to%2520evaluate%2520the%2520stories%2520generated%250Aby%2520several%2520models%252C%2520showing%2520that%2520the%2520foundation%2520model%2520LLaVA%2520obtains%2520the%2520best%250Aresult%252C%2520but%2520only%2520slightly%2520so%2520compared%2520to%2520TAPM%252C%2520a%252050-times%2520smaller%2520visual%250Astorytelling%2520model.%2520Upgrading%2520the%2520visual%2520and%2520language%2520components%2520of%2520TAPM%250Aresults%2520in%2520a%2520model%2520that%2520yields%2520competitive%2520performance%2520with%2520a%2520relatively%2520low%250Anumber%2520of%2520parameters.%2520Finally%252C%2520we%2520carry%2520out%2520a%2520human%2520evaluation%2520study%252C%2520whose%250Aresults%2520suggest%2520that%2520a%2520%2527good%2527%2520story%2520may%2520require%2520more%2520than%2520a%2520human-like%2520level%2520of%250Avisual%2520grounding%252C%2520coherence%252C%2520and%2520repetition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20%28yet%29%20the%20whole%20story%3A%20Evaluating%20Visual%20Storytelling%20Requires%20More%0A%20%20than%20Measuring%20Coherence%2C%20Grounding%2C%20and%20Repetition&entry.906535625=Aditya%20K%20Surikuchi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle&entry.1292438233=%20%20Visual%20storytelling%20consists%20in%20generating%20a%20natural%20language%20story%20given%20a%0Atemporally%20ordered%20sequence%20of%20images.%20This%20task%20is%20not%20only%20challenging%20for%0Amodels%2C%20but%20also%20very%20difficult%20to%20evaluate%20with%20automatic%20metrics%20since%20there%0Ais%20no%20consensus%20about%20what%20makes%20a%20story%20%27good%27.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20method%20that%20measures%20story%20quality%20in%20terms%20of%20human%20likeness%20regarding%0Athree%20key%20aspects%20highlighted%20in%20previous%20work%3A%20visual%20grounding%2C%20coherence%2C%0Aand%20repetitiveness.%20We%20then%20use%20this%20method%20to%20evaluate%20the%20stories%20generated%0Aby%20several%20models%2C%20showing%20that%20the%20foundation%20model%20LLaVA%20obtains%20the%20best%0Aresult%2C%20but%20only%20slightly%20so%20compared%20to%20TAPM%2C%20a%2050-times%20smaller%20visual%0Astorytelling%20model.%20Upgrading%20the%20visual%20and%20language%20components%20of%20TAPM%0Aresults%20in%20a%20model%20that%20yields%20competitive%20performance%20with%20a%20relatively%20low%0Anumber%20of%20parameters.%20Finally%2C%20we%20carry%20out%20a%20human%20evaluation%20study%2C%20whose%0Aresults%20suggest%20that%20a%20%27good%27%20story%20may%20require%20more%20than%20a%20human-like%20level%20of%0Avisual%20grounding%2C%20coherence%2C%20and%20repetition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04559v2&entry.124074799=Read"},
{"title": "Evaluation Framework for Feedback Generation Methods in Skeletal\n  Movement Assessment", "author": "Tal Hakim", "abstract": "  The application of machine-learning solutions to movement assessment from\nskeleton videos has attracted significant research attention in recent years.\nThis advancement has made rehabilitation at home more accessible, utilizing\nmovement assessment algorithms that can operate on affordable equipment for\nhuman pose detection and analysis from 2D or 3D videos. While the primary\nobjective of automatic assessment tasks is to score movements, the automatic\ngeneration of feedback highlighting key movement issues has the potential to\nsignificantly enhance and accelerate the rehabilitation process. While numerous\nresearch works exist in the field of automatic movement assessment, only a\nhandful address feedback generation. In this study, we propose terminology and\ncriteria for the classification, evaluation, and comparison of feedback\ngeneration solutions. We discuss the challenges associated with each feedback\ngeneration approach and use our proposed criteria to classify existing\nsolutions. To our knowledge, this is the first work that formulates feedback\ngeneration in skeletal movement assessment.\n", "link": "http://arxiv.org/abs/2404.09359v4", "date": "2024-08-29", "relevancy": 1.923, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4963}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4876}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20Framework%20for%20Feedback%20Generation%20Methods%20in%20Skeletal%0A%20%20Movement%20Assessment&body=Title%3A%20Evaluation%20Framework%20for%20Feedback%20Generation%20Methods%20in%20Skeletal%0A%20%20Movement%20Assessment%0AAuthor%3A%20Tal%20Hakim%0AAbstract%3A%20%20%20The%20application%20of%20machine-learning%20solutions%20to%20movement%20assessment%20from%0Askeleton%20videos%20has%20attracted%20significant%20research%20attention%20in%20recent%20years.%0AThis%20advancement%20has%20made%20rehabilitation%20at%20home%20more%20accessible%2C%20utilizing%0Amovement%20assessment%20algorithms%20that%20can%20operate%20on%20affordable%20equipment%20for%0Ahuman%20pose%20detection%20and%20analysis%20from%202D%20or%203D%20videos.%20While%20the%20primary%0Aobjective%20of%20automatic%20assessment%20tasks%20is%20to%20score%20movements%2C%20the%20automatic%0Ageneration%20of%20feedback%20highlighting%20key%20movement%20issues%20has%20the%20potential%20to%0Asignificantly%20enhance%20and%20accelerate%20the%20rehabilitation%20process.%20While%20numerous%0Aresearch%20works%20exist%20in%20the%20field%20of%20automatic%20movement%20assessment%2C%20only%20a%0Ahandful%20address%20feedback%20generation.%20In%20this%20study%2C%20we%20propose%20terminology%20and%0Acriteria%20for%20the%20classification%2C%20evaluation%2C%20and%20comparison%20of%20feedback%0Ageneration%20solutions.%20We%20discuss%20the%20challenges%20associated%20with%20each%20feedback%0Ageneration%20approach%20and%20use%20our%20proposed%20criteria%20to%20classify%20existing%0Asolutions.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20that%20formulates%20feedback%0Ageneration%20in%20skeletal%20movement%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09359v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520Framework%2520for%2520Feedback%2520Generation%2520Methods%2520in%2520Skeletal%250A%2520%2520Movement%2520Assessment%26entry.906535625%3DTal%2520Hakim%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520machine-learning%2520solutions%2520to%2520movement%2520assessment%2520from%250Askeleton%2520videos%2520has%2520attracted%2520significant%2520research%2520attention%2520in%2520recent%2520years.%250AThis%2520advancement%2520has%2520made%2520rehabilitation%2520at%2520home%2520more%2520accessible%252C%2520utilizing%250Amovement%2520assessment%2520algorithms%2520that%2520can%2520operate%2520on%2520affordable%2520equipment%2520for%250Ahuman%2520pose%2520detection%2520and%2520analysis%2520from%25202D%2520or%25203D%2520videos.%2520While%2520the%2520primary%250Aobjective%2520of%2520automatic%2520assessment%2520tasks%2520is%2520to%2520score%2520movements%252C%2520the%2520automatic%250Ageneration%2520of%2520feedback%2520highlighting%2520key%2520movement%2520issues%2520has%2520the%2520potential%2520to%250Asignificantly%2520enhance%2520and%2520accelerate%2520the%2520rehabilitation%2520process.%2520While%2520numerous%250Aresearch%2520works%2520exist%2520in%2520the%2520field%2520of%2520automatic%2520movement%2520assessment%252C%2520only%2520a%250Ahandful%2520address%2520feedback%2520generation.%2520In%2520this%2520study%252C%2520we%2520propose%2520terminology%2520and%250Acriteria%2520for%2520the%2520classification%252C%2520evaluation%252C%2520and%2520comparison%2520of%2520feedback%250Ageneration%2520solutions.%2520We%2520discuss%2520the%2520challenges%2520associated%2520with%2520each%2520feedback%250Ageneration%2520approach%2520and%2520use%2520our%2520proposed%2520criteria%2520to%2520classify%2520existing%250Asolutions.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520that%2520formulates%2520feedback%250Ageneration%2520in%2520skeletal%2520movement%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09359v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20Framework%20for%20Feedback%20Generation%20Methods%20in%20Skeletal%0A%20%20Movement%20Assessment&entry.906535625=Tal%20Hakim&entry.1292438233=%20%20The%20application%20of%20machine-learning%20solutions%20to%20movement%20assessment%20from%0Askeleton%20videos%20has%20attracted%20significant%20research%20attention%20in%20recent%20years.%0AThis%20advancement%20has%20made%20rehabilitation%20at%20home%20more%20accessible%2C%20utilizing%0Amovement%20assessment%20algorithms%20that%20can%20operate%20on%20affordable%20equipment%20for%0Ahuman%20pose%20detection%20and%20analysis%20from%202D%20or%203D%20videos.%20While%20the%20primary%0Aobjective%20of%20automatic%20assessment%20tasks%20is%20to%20score%20movements%2C%20the%20automatic%0Ageneration%20of%20feedback%20highlighting%20key%20movement%20issues%20has%20the%20potential%20to%0Asignificantly%20enhance%20and%20accelerate%20the%20rehabilitation%20process.%20While%20numerous%0Aresearch%20works%20exist%20in%20the%20field%20of%20automatic%20movement%20assessment%2C%20only%20a%0Ahandful%20address%20feedback%20generation.%20In%20this%20study%2C%20we%20propose%20terminology%20and%0Acriteria%20for%20the%20classification%2C%20evaluation%2C%20and%20comparison%20of%20feedback%0Ageneration%20solutions.%20We%20discuss%20the%20challenges%20associated%20with%20each%20feedback%0Ageneration%20approach%20and%20use%20our%20proposed%20criteria%20to%20classify%20existing%0Asolutions.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20that%20formulates%20feedback%0Ageneration%20in%20skeletal%20movement%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09359v4&entry.124074799=Read"},
{"title": "Asynchronous Spatial-Temporal Allocation for Trajectory Planning of\n  Heterogeneous Multi-Agent Systems", "author": "Yuda Chen and Haoze Dong and Zhongkui Li", "abstract": "  To plan the trajectories of a large-scale heterogeneous swarm, sequentially\nor synchronously distributed methods usually become intractable due to the lack\nof global clock synchronization. To this end, we provide a novel asynchronous\nspatial-temporal allocation method. Specifically, between a pair of agents, the\nallocation is proposed to determine their corresponding derivable time-stamped\nspace and can be updated in an asynchronous way, by inserting a waiting\nduration between two consecutive replanning steps. Via theoretical analysis,\nthe inter-agent collision is proved to be avoided and the allocation ensures\ntimely updates. Comprehensive simulations and comparisons with five baselines\nvalidate the effectiveness of the proposed method and illustrate its\nimprovement in completion time and moving distance. Finally, hardware\nexperiments are carried out, where $8$ heterogeneous unmanned ground vehicles\nwith onboard computation navigate in cluttered scenarios with high agility.\n", "link": "http://arxiv.org/abs/2309.07431v3", "date": "2024-08-29", "relevancy": 1.9227, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4966}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4902}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Spatial-Temporal%20Allocation%20for%20Trajectory%20Planning%20of%0A%20%20Heterogeneous%20Multi-Agent%20Systems&body=Title%3A%20Asynchronous%20Spatial-Temporal%20Allocation%20for%20Trajectory%20Planning%20of%0A%20%20Heterogeneous%20Multi-Agent%20Systems%0AAuthor%3A%20Yuda%20Chen%20and%20Haoze%20Dong%20and%20Zhongkui%20Li%0AAbstract%3A%20%20%20To%20plan%20the%20trajectories%20of%20a%20large-scale%20heterogeneous%20swarm%2C%20sequentially%0Aor%20synchronously%20distributed%20methods%20usually%20become%20intractable%20due%20to%20the%20lack%0Aof%20global%20clock%20synchronization.%20To%20this%20end%2C%20we%20provide%20a%20novel%20asynchronous%0Aspatial-temporal%20allocation%20method.%20Specifically%2C%20between%20a%20pair%20of%20agents%2C%20the%0Aallocation%20is%20proposed%20to%20determine%20their%20corresponding%20derivable%20time-stamped%0Aspace%20and%20can%20be%20updated%20in%20an%20asynchronous%20way%2C%20by%20inserting%20a%20waiting%0Aduration%20between%20two%20consecutive%20replanning%20steps.%20Via%20theoretical%20analysis%2C%0Athe%20inter-agent%20collision%20is%20proved%20to%20be%20avoided%20and%20the%20allocation%20ensures%0Atimely%20updates.%20Comprehensive%20simulations%20and%20comparisons%20with%20five%20baselines%0Avalidate%20the%20effectiveness%20of%20the%20proposed%20method%20and%20illustrate%20its%0Aimprovement%20in%20completion%20time%20and%20moving%20distance.%20Finally%2C%20hardware%0Aexperiments%20are%20carried%20out%2C%20where%20%248%24%20heterogeneous%20unmanned%20ground%20vehicles%0Awith%20onboard%20computation%20navigate%20in%20cluttered%20scenarios%20with%20high%20agility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.07431v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Spatial-Temporal%2520Allocation%2520for%2520Trajectory%2520Planning%2520of%250A%2520%2520Heterogeneous%2520Multi-Agent%2520Systems%26entry.906535625%3DYuda%2520Chen%2520and%2520Haoze%2520Dong%2520and%2520Zhongkui%2520Li%26entry.1292438233%3D%2520%2520To%2520plan%2520the%2520trajectories%2520of%2520a%2520large-scale%2520heterogeneous%2520swarm%252C%2520sequentially%250Aor%2520synchronously%2520distributed%2520methods%2520usually%2520become%2520intractable%2520due%2520to%2520the%2520lack%250Aof%2520global%2520clock%2520synchronization.%2520To%2520this%2520end%252C%2520we%2520provide%2520a%2520novel%2520asynchronous%250Aspatial-temporal%2520allocation%2520method.%2520Specifically%252C%2520between%2520a%2520pair%2520of%2520agents%252C%2520the%250Aallocation%2520is%2520proposed%2520to%2520determine%2520their%2520corresponding%2520derivable%2520time-stamped%250Aspace%2520and%2520can%2520be%2520updated%2520in%2520an%2520asynchronous%2520way%252C%2520by%2520inserting%2520a%2520waiting%250Aduration%2520between%2520two%2520consecutive%2520replanning%2520steps.%2520Via%2520theoretical%2520analysis%252C%250Athe%2520inter-agent%2520collision%2520is%2520proved%2520to%2520be%2520avoided%2520and%2520the%2520allocation%2520ensures%250Atimely%2520updates.%2520Comprehensive%2520simulations%2520and%2520comparisons%2520with%2520five%2520baselines%250Avalidate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520and%2520illustrate%2520its%250Aimprovement%2520in%2520completion%2520time%2520and%2520moving%2520distance.%2520Finally%252C%2520hardware%250Aexperiments%2520are%2520carried%2520out%252C%2520where%2520%25248%2524%2520heterogeneous%2520unmanned%2520ground%2520vehicles%250Awith%2520onboard%2520computation%2520navigate%2520in%2520cluttered%2520scenarios%2520with%2520high%2520agility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.07431v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Spatial-Temporal%20Allocation%20for%20Trajectory%20Planning%20of%0A%20%20Heterogeneous%20Multi-Agent%20Systems&entry.906535625=Yuda%20Chen%20and%20Haoze%20Dong%20and%20Zhongkui%20Li&entry.1292438233=%20%20To%20plan%20the%20trajectories%20of%20a%20large-scale%20heterogeneous%20swarm%2C%20sequentially%0Aor%20synchronously%20distributed%20methods%20usually%20become%20intractable%20due%20to%20the%20lack%0Aof%20global%20clock%20synchronization.%20To%20this%20end%2C%20we%20provide%20a%20novel%20asynchronous%0Aspatial-temporal%20allocation%20method.%20Specifically%2C%20between%20a%20pair%20of%20agents%2C%20the%0Aallocation%20is%20proposed%20to%20determine%20their%20corresponding%20derivable%20time-stamped%0Aspace%20and%20can%20be%20updated%20in%20an%20asynchronous%20way%2C%20by%20inserting%20a%20waiting%0Aduration%20between%20two%20consecutive%20replanning%20steps.%20Via%20theoretical%20analysis%2C%0Athe%20inter-agent%20collision%20is%20proved%20to%20be%20avoided%20and%20the%20allocation%20ensures%0Atimely%20updates.%20Comprehensive%20simulations%20and%20comparisons%20with%20five%20baselines%0Avalidate%20the%20effectiveness%20of%20the%20proposed%20method%20and%20illustrate%20its%0Aimprovement%20in%20completion%20time%20and%20moving%20distance.%20Finally%2C%20hardware%0Aexperiments%20are%20carried%20out%2C%20where%20%248%24%20heterogeneous%20unmanned%20ground%20vehicles%0Awith%20onboard%20computation%20navigate%20in%20cluttered%20scenarios%20with%20high%20agility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07431v3&entry.124074799=Read"},
{"title": "An Exploratory Deep Learning Approach for Predicting Subsequent Suicidal\n  Acts in Chinese Psychological Support Hotlines", "author": "Changwei Song and Qing Zhao and Jianqiang Li and Yining Chen and Yongsheng Tong and Guanghui Fu", "abstract": "  Psychological support hotlines are an effective suicide prevention measure\nthat typically relies on professionals using suicide risk assessment scales to\npredict individual risk scores. However, the accuracy of scale-based predictive\nmethods for suicide risk assessment can vary widely depending on the expertise\nof the operator. This limitation underscores the need for more reliable\nmethods, prompting this research's innovative exploration of the use of\nartificial intelligence to improve the accuracy and efficiency of suicide risk\nprediction within the context of psychological support hotlines. The study\nincluded data from 1,549 subjects from 2015-2017 in China who contacted a\npsychological support hotline. Each participant was followed for 12 months to\nidentify instances of suicidal behavior. We proposed a novel multi-task\nlearning method that uses the large-scale pre-trained model Whisper for feature\nextraction and fits psychological scales while predicting the risk of suicide.\nThe proposed method yields a 2.4\\% points improvement in F1-score compared to\nthe traditional manual approach based on the psychological scales. Our model\ndemonstrated superior performance compared to the other eight popular models.\nTo our knowledge, this study is the first to apply deep learning to long-term\nspeech data to predict suicide risk in China, indicating grate potential for\nclinical applications. The source code is publicly available at:\n\\url{https://github.com/songchangwei/Suicide-Risk-Prediction}.\n", "link": "http://arxiv.org/abs/2408.16463v1", "date": "2024-08-29", "relevancy": 1.7815, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.452}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4426}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Exploratory%20Deep%20Learning%20Approach%20for%20Predicting%20Subsequent%20Suicidal%0A%20%20Acts%20in%20Chinese%20Psychological%20Support%20Hotlines&body=Title%3A%20An%20Exploratory%20Deep%20Learning%20Approach%20for%20Predicting%20Subsequent%20Suicidal%0A%20%20Acts%20in%20Chinese%20Psychological%20Support%20Hotlines%0AAuthor%3A%20Changwei%20Song%20and%20Qing%20Zhao%20and%20Jianqiang%20Li%20and%20Yining%20Chen%20and%20Yongsheng%20Tong%20and%20Guanghui%20Fu%0AAbstract%3A%20%20%20Psychological%20support%20hotlines%20are%20an%20effective%20suicide%20prevention%20measure%0Athat%20typically%20relies%20on%20professionals%20using%20suicide%20risk%20assessment%20scales%20to%0Apredict%20individual%20risk%20scores.%20However%2C%20the%20accuracy%20of%20scale-based%20predictive%0Amethods%20for%20suicide%20risk%20assessment%20can%20vary%20widely%20depending%20on%20the%20expertise%0Aof%20the%20operator.%20This%20limitation%20underscores%20the%20need%20for%20more%20reliable%0Amethods%2C%20prompting%20this%20research%27s%20innovative%20exploration%20of%20the%20use%20of%0Aartificial%20intelligence%20to%20improve%20the%20accuracy%20and%20efficiency%20of%20suicide%20risk%0Aprediction%20within%20the%20context%20of%20psychological%20support%20hotlines.%20The%20study%0Aincluded%20data%20from%201%2C549%20subjects%20from%202015-2017%20in%20China%20who%20contacted%20a%0Apsychological%20support%20hotline.%20Each%20participant%20was%20followed%20for%2012%20months%20to%0Aidentify%20instances%20of%20suicidal%20behavior.%20We%20proposed%20a%20novel%20multi-task%0Alearning%20method%20that%20uses%20the%20large-scale%20pre-trained%20model%20Whisper%20for%20feature%0Aextraction%20and%20fits%20psychological%20scales%20while%20predicting%20the%20risk%20of%20suicide.%0AThe%20proposed%20method%20yields%20a%202.4%5C%25%20points%20improvement%20in%20F1-score%20compared%20to%0Athe%20traditional%20manual%20approach%20based%20on%20the%20psychological%20scales.%20Our%20model%0Ademonstrated%20superior%20performance%20compared%20to%20the%20other%20eight%20popular%20models.%0ATo%20our%20knowledge%2C%20this%20study%20is%20the%20first%20to%20apply%20deep%20learning%20to%20long-term%0Aspeech%20data%20to%20predict%20suicide%20risk%20in%20China%2C%20indicating%20grate%20potential%20for%0Aclinical%20applications.%20The%20source%20code%20is%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/songchangwei/Suicide-Risk-Prediction%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Exploratory%2520Deep%2520Learning%2520Approach%2520for%2520Predicting%2520Subsequent%2520Suicidal%250A%2520%2520Acts%2520in%2520Chinese%2520Psychological%2520Support%2520Hotlines%26entry.906535625%3DChangwei%2520Song%2520and%2520Qing%2520Zhao%2520and%2520Jianqiang%2520Li%2520and%2520Yining%2520Chen%2520and%2520Yongsheng%2520Tong%2520and%2520Guanghui%2520Fu%26entry.1292438233%3D%2520%2520Psychological%2520support%2520hotlines%2520are%2520an%2520effective%2520suicide%2520prevention%2520measure%250Athat%2520typically%2520relies%2520on%2520professionals%2520using%2520suicide%2520risk%2520assessment%2520scales%2520to%250Apredict%2520individual%2520risk%2520scores.%2520However%252C%2520the%2520accuracy%2520of%2520scale-based%2520predictive%250Amethods%2520for%2520suicide%2520risk%2520assessment%2520can%2520vary%2520widely%2520depending%2520on%2520the%2520expertise%250Aof%2520the%2520operator.%2520This%2520limitation%2520underscores%2520the%2520need%2520for%2520more%2520reliable%250Amethods%252C%2520prompting%2520this%2520research%2527s%2520innovative%2520exploration%2520of%2520the%2520use%2520of%250Aartificial%2520intelligence%2520to%2520improve%2520the%2520accuracy%2520and%2520efficiency%2520of%2520suicide%2520risk%250Aprediction%2520within%2520the%2520context%2520of%2520psychological%2520support%2520hotlines.%2520The%2520study%250Aincluded%2520data%2520from%25201%252C549%2520subjects%2520from%25202015-2017%2520in%2520China%2520who%2520contacted%2520a%250Apsychological%2520support%2520hotline.%2520Each%2520participant%2520was%2520followed%2520for%252012%2520months%2520to%250Aidentify%2520instances%2520of%2520suicidal%2520behavior.%2520We%2520proposed%2520a%2520novel%2520multi-task%250Alearning%2520method%2520that%2520uses%2520the%2520large-scale%2520pre-trained%2520model%2520Whisper%2520for%2520feature%250Aextraction%2520and%2520fits%2520psychological%2520scales%2520while%2520predicting%2520the%2520risk%2520of%2520suicide.%250AThe%2520proposed%2520method%2520yields%2520a%25202.4%255C%2525%2520points%2520improvement%2520in%2520F1-score%2520compared%2520to%250Athe%2520traditional%2520manual%2520approach%2520based%2520on%2520the%2520psychological%2520scales.%2520Our%2520model%250Ademonstrated%2520superior%2520performance%2520compared%2520to%2520the%2520other%2520eight%2520popular%2520models.%250ATo%2520our%2520knowledge%252C%2520this%2520study%2520is%2520the%2520first%2520to%2520apply%2520deep%2520learning%2520to%2520long-term%250Aspeech%2520data%2520to%2520predict%2520suicide%2520risk%2520in%2520China%252C%2520indicating%2520grate%2520potential%2520for%250Aclinical%2520applications.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/songchangwei/Suicide-Risk-Prediction%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Exploratory%20Deep%20Learning%20Approach%20for%20Predicting%20Subsequent%20Suicidal%0A%20%20Acts%20in%20Chinese%20Psychological%20Support%20Hotlines&entry.906535625=Changwei%20Song%20and%20Qing%20Zhao%20and%20Jianqiang%20Li%20and%20Yining%20Chen%20and%20Yongsheng%20Tong%20and%20Guanghui%20Fu&entry.1292438233=%20%20Psychological%20support%20hotlines%20are%20an%20effective%20suicide%20prevention%20measure%0Athat%20typically%20relies%20on%20professionals%20using%20suicide%20risk%20assessment%20scales%20to%0Apredict%20individual%20risk%20scores.%20However%2C%20the%20accuracy%20of%20scale-based%20predictive%0Amethods%20for%20suicide%20risk%20assessment%20can%20vary%20widely%20depending%20on%20the%20expertise%0Aof%20the%20operator.%20This%20limitation%20underscores%20the%20need%20for%20more%20reliable%0Amethods%2C%20prompting%20this%20research%27s%20innovative%20exploration%20of%20the%20use%20of%0Aartificial%20intelligence%20to%20improve%20the%20accuracy%20and%20efficiency%20of%20suicide%20risk%0Aprediction%20within%20the%20context%20of%20psychological%20support%20hotlines.%20The%20study%0Aincluded%20data%20from%201%2C549%20subjects%20from%202015-2017%20in%20China%20who%20contacted%20a%0Apsychological%20support%20hotline.%20Each%20participant%20was%20followed%20for%2012%20months%20to%0Aidentify%20instances%20of%20suicidal%20behavior.%20We%20proposed%20a%20novel%20multi-task%0Alearning%20method%20that%20uses%20the%20large-scale%20pre-trained%20model%20Whisper%20for%20feature%0Aextraction%20and%20fits%20psychological%20scales%20while%20predicting%20the%20risk%20of%20suicide.%0AThe%20proposed%20method%20yields%20a%202.4%5C%25%20points%20improvement%20in%20F1-score%20compared%20to%0Athe%20traditional%20manual%20approach%20based%20on%20the%20psychological%20scales.%20Our%20model%0Ademonstrated%20superior%20performance%20compared%20to%20the%20other%20eight%20popular%20models.%0ATo%20our%20knowledge%2C%20this%20study%20is%20the%20first%20to%20apply%20deep%20learning%20to%20long-term%0Aspeech%20data%20to%20predict%20suicide%20risk%20in%20China%2C%20indicating%20grate%20potential%20for%0Aclinical%20applications.%20The%20source%20code%20is%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/songchangwei/Suicide-Risk-Prediction%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16463v1&entry.124074799=Read"},
{"title": "Statistical and Geometrical properties of regularized Kernel\n  Kullback-Leibler divergence", "author": "Cl\u00e9mentine Chazal and Anna Korba and Francis Bach", "abstract": "  In this paper, we study the statistical and geometrical properties of the\nKullback-Leibler divergence with kernel covariance operators (KKL) introduced\nby Bach [2022]. Unlike the classical Kullback-Leibler (KL) divergence that\ninvolves density ratios, the KKL compares probability distributions through\ncovariance operators (embeddings) in a reproducible kernel Hilbert space\n(RKHS), and compute the Kullback-Leibler quantum divergence. This novel\ndivergence hence shares parallel but different aspects with both the standard\nKullback-Leibler between probability distributions and kernel embeddings\nmetrics such as the maximum mean discrepancy. A limitation faced with the\noriginal KKL divergence is its inability to be defined for distributions with\ndisjoint supports. To solve this problem, we propose in this paper a\nregularised variant that guarantees that the divergence is well defined for all\ndistributions. We derive bounds that quantify the deviation of the regularised\nKKL to the original one, as well as finite-sample bounds. In addition, we\nprovide a closed-form expression for the regularised KKL, specifically\napplicable when the distributions consist of finite sets of points, which makes\nit implementable. Furthermore, we derive a Wasserstein gradient descent scheme\nof the KKL divergence in the case of discrete distributions, and study\nempirically its properties to transport a set of points to a target\ndistribution.\n", "link": "http://arxiv.org/abs/2408.16543v1", "date": "2024-08-29", "relevancy": 1.2224, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.424}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4057}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20and%20Geometrical%20properties%20of%20regularized%20Kernel%0A%20%20Kullback-Leibler%20divergence&body=Title%3A%20Statistical%20and%20Geometrical%20properties%20of%20regularized%20Kernel%0A%20%20Kullback-Leibler%20divergence%0AAuthor%3A%20Cl%C3%A9mentine%20Chazal%20and%20Anna%20Korba%20and%20Francis%20Bach%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20statistical%20and%20geometrical%20properties%20of%20the%0AKullback-Leibler%20divergence%20with%20kernel%20covariance%20operators%20%28KKL%29%20introduced%0Aby%20Bach%20%5B2022%5D.%20Unlike%20the%20classical%20Kullback-Leibler%20%28KL%29%20divergence%20that%0Ainvolves%20density%20ratios%2C%20the%20KKL%20compares%20probability%20distributions%20through%0Acovariance%20operators%20%28embeddings%29%20in%20a%20reproducible%20kernel%20Hilbert%20space%0A%28RKHS%29%2C%20and%20compute%20the%20Kullback-Leibler%20quantum%20divergence.%20This%20novel%0Adivergence%20hence%20shares%20parallel%20but%20different%20aspects%20with%20both%20the%20standard%0AKullback-Leibler%20between%20probability%20distributions%20and%20kernel%20embeddings%0Ametrics%20such%20as%20the%20maximum%20mean%20discrepancy.%20A%20limitation%20faced%20with%20the%0Aoriginal%20KKL%20divergence%20is%20its%20inability%20to%20be%20defined%20for%20distributions%20with%0Adisjoint%20supports.%20To%20solve%20this%20problem%2C%20we%20propose%20in%20this%20paper%20a%0Aregularised%20variant%20that%20guarantees%20that%20the%20divergence%20is%20well%20defined%20for%20all%0Adistributions.%20We%20derive%20bounds%20that%20quantify%20the%20deviation%20of%20the%20regularised%0AKKL%20to%20the%20original%20one%2C%20as%20well%20as%20finite-sample%20bounds.%20In%20addition%2C%20we%0Aprovide%20a%20closed-form%20expression%20for%20the%20regularised%20KKL%2C%20specifically%0Aapplicable%20when%20the%20distributions%20consist%20of%20finite%20sets%20of%20points%2C%20which%20makes%0Ait%20implementable.%20Furthermore%2C%20we%20derive%20a%20Wasserstein%20gradient%20descent%20scheme%0Aof%20the%20KKL%20divergence%20in%20the%20case%20of%20discrete%20distributions%2C%20and%20study%0Aempirically%20its%20properties%20to%20transport%20a%20set%20of%20points%20to%20a%20target%0Adistribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520and%2520Geometrical%2520properties%2520of%2520regularized%2520Kernel%250A%2520%2520Kullback-Leibler%2520divergence%26entry.906535625%3DCl%25C3%25A9mentine%2520Chazal%2520and%2520Anna%2520Korba%2520and%2520Francis%2520Bach%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520statistical%2520and%2520geometrical%2520properties%2520of%2520the%250AKullback-Leibler%2520divergence%2520with%2520kernel%2520covariance%2520operators%2520%2528KKL%2529%2520introduced%250Aby%2520Bach%2520%255B2022%255D.%2520Unlike%2520the%2520classical%2520Kullback-Leibler%2520%2528KL%2529%2520divergence%2520that%250Ainvolves%2520density%2520ratios%252C%2520the%2520KKL%2520compares%2520probability%2520distributions%2520through%250Acovariance%2520operators%2520%2528embeddings%2529%2520in%2520a%2520reproducible%2520kernel%2520Hilbert%2520space%250A%2528RKHS%2529%252C%2520and%2520compute%2520the%2520Kullback-Leibler%2520quantum%2520divergence.%2520This%2520novel%250Adivergence%2520hence%2520shares%2520parallel%2520but%2520different%2520aspects%2520with%2520both%2520the%2520standard%250AKullback-Leibler%2520between%2520probability%2520distributions%2520and%2520kernel%2520embeddings%250Ametrics%2520such%2520as%2520the%2520maximum%2520mean%2520discrepancy.%2520A%2520limitation%2520faced%2520with%2520the%250Aoriginal%2520KKL%2520divergence%2520is%2520its%2520inability%2520to%2520be%2520defined%2520for%2520distributions%2520with%250Adisjoint%2520supports.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520in%2520this%2520paper%2520a%250Aregularised%2520variant%2520that%2520guarantees%2520that%2520the%2520divergence%2520is%2520well%2520defined%2520for%2520all%250Adistributions.%2520We%2520derive%2520bounds%2520that%2520quantify%2520the%2520deviation%2520of%2520the%2520regularised%250AKKL%2520to%2520the%2520original%2520one%252C%2520as%2520well%2520as%2520finite-sample%2520bounds.%2520In%2520addition%252C%2520we%250Aprovide%2520a%2520closed-form%2520expression%2520for%2520the%2520regularised%2520KKL%252C%2520specifically%250Aapplicable%2520when%2520the%2520distributions%2520consist%2520of%2520finite%2520sets%2520of%2520points%252C%2520which%2520makes%250Ait%2520implementable.%2520Furthermore%252C%2520we%2520derive%2520a%2520Wasserstein%2520gradient%2520descent%2520scheme%250Aof%2520the%2520KKL%2520divergence%2520in%2520the%2520case%2520of%2520discrete%2520distributions%252C%2520and%2520study%250Aempirically%2520its%2520properties%2520to%2520transport%2520a%2520set%2520of%2520points%2520to%2520a%2520target%250Adistribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20and%20Geometrical%20properties%20of%20regularized%20Kernel%0A%20%20Kullback-Leibler%20divergence&entry.906535625=Cl%C3%A9mentine%20Chazal%20and%20Anna%20Korba%20and%20Francis%20Bach&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20statistical%20and%20geometrical%20properties%20of%20the%0AKullback-Leibler%20divergence%20with%20kernel%20covariance%20operators%20%28KKL%29%20introduced%0Aby%20Bach%20%5B2022%5D.%20Unlike%20the%20classical%20Kullback-Leibler%20%28KL%29%20divergence%20that%0Ainvolves%20density%20ratios%2C%20the%20KKL%20compares%20probability%20distributions%20through%0Acovariance%20operators%20%28embeddings%29%20in%20a%20reproducible%20kernel%20Hilbert%20space%0A%28RKHS%29%2C%20and%20compute%20the%20Kullback-Leibler%20quantum%20divergence.%20This%20novel%0Adivergence%20hence%20shares%20parallel%20but%20different%20aspects%20with%20both%20the%20standard%0AKullback-Leibler%20between%20probability%20distributions%20and%20kernel%20embeddings%0Ametrics%20such%20as%20the%20maximum%20mean%20discrepancy.%20A%20limitation%20faced%20with%20the%0Aoriginal%20KKL%20divergence%20is%20its%20inability%20to%20be%20defined%20for%20distributions%20with%0Adisjoint%20supports.%20To%20solve%20this%20problem%2C%20we%20propose%20in%20this%20paper%20a%0Aregularised%20variant%20that%20guarantees%20that%20the%20divergence%20is%20well%20defined%20for%20all%0Adistributions.%20We%20derive%20bounds%20that%20quantify%20the%20deviation%20of%20the%20regularised%0AKKL%20to%20the%20original%20one%2C%20as%20well%20as%20finite-sample%20bounds.%20In%20addition%2C%20we%0Aprovide%20a%20closed-form%20expression%20for%20the%20regularised%20KKL%2C%20specifically%0Aapplicable%20when%20the%20distributions%20consist%20of%20finite%20sets%20of%20points%2C%20which%20makes%0Ait%20implementable.%20Furthermore%2C%20we%20derive%20a%20Wasserstein%20gradient%20descent%20scheme%0Aof%20the%20KKL%20divergence%20in%20the%20case%20of%20discrete%20distributions%2C%20and%20study%0Aempirically%20its%20properties%20to%20transport%20a%20set%20of%20points%20to%20a%20target%0Adistribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16543v1&entry.124074799=Read"},
{"title": "CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions", "author": "Laurin Wagner and Bernhard Thallinger and Mario Zusag", "abstract": "  We demonstrate that carefully adjusting the tokenizer of the Whisper speech\nrecognition model significantly improves the precision of word-level timestamps\nwhen applying dynamic time warping to the decoder's cross-attention scores. We\nfine-tune the model to produce more verbatim speech transcriptions and employ\nseveral techniques to increase robustness against multiple speakers and\nbackground noise. These adjustments achieve state-of-the-art performance on\nbenchmarks for verbatim speech transcription, word segmentation, and the timed\ndetection of filler events, and can further mitigate transcription\nhallucinations. The code is available open\nhttps://github.com/nyrahealth/CrisperWhisper.\n", "link": "http://arxiv.org/abs/2408.16589v1", "date": "2024-08-29", "relevancy": 1.7595, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4504}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4327}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrisperWhisper%3A%20Accurate%20Timestamps%20on%20Verbatim%20Speech%20Transcriptions&body=Title%3A%20CrisperWhisper%3A%20Accurate%20Timestamps%20on%20Verbatim%20Speech%20Transcriptions%0AAuthor%3A%20Laurin%20Wagner%20and%20Bernhard%20Thallinger%20and%20Mario%20Zusag%0AAbstract%3A%20%20%20We%20demonstrate%20that%20carefully%20adjusting%20the%20tokenizer%20of%20the%20Whisper%20speech%0Arecognition%20model%20significantly%20improves%20the%20precision%20of%20word-level%20timestamps%0Awhen%20applying%20dynamic%20time%20warping%20to%20the%20decoder%27s%20cross-attention%20scores.%20We%0Afine-tune%20the%20model%20to%20produce%20more%20verbatim%20speech%20transcriptions%20and%20employ%0Aseveral%20techniques%20to%20increase%20robustness%20against%20multiple%20speakers%20and%0Abackground%20noise.%20These%20adjustments%20achieve%20state-of-the-art%20performance%20on%0Abenchmarks%20for%20verbatim%20speech%20transcription%2C%20word%20segmentation%2C%20and%20the%20timed%0Adetection%20of%20filler%20events%2C%20and%20can%20further%20mitigate%20transcription%0Ahallucinations.%20The%20code%20is%20available%20open%0Ahttps%3A//github.com/nyrahealth/CrisperWhisper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrisperWhisper%253A%2520Accurate%2520Timestamps%2520on%2520Verbatim%2520Speech%2520Transcriptions%26entry.906535625%3DLaurin%2520Wagner%2520and%2520Bernhard%2520Thallinger%2520and%2520Mario%2520Zusag%26entry.1292438233%3D%2520%2520We%2520demonstrate%2520that%2520carefully%2520adjusting%2520the%2520tokenizer%2520of%2520the%2520Whisper%2520speech%250Arecognition%2520model%2520significantly%2520improves%2520the%2520precision%2520of%2520word-level%2520timestamps%250Awhen%2520applying%2520dynamic%2520time%2520warping%2520to%2520the%2520decoder%2527s%2520cross-attention%2520scores.%2520We%250Afine-tune%2520the%2520model%2520to%2520produce%2520more%2520verbatim%2520speech%2520transcriptions%2520and%2520employ%250Aseveral%2520techniques%2520to%2520increase%2520robustness%2520against%2520multiple%2520speakers%2520and%250Abackground%2520noise.%2520These%2520adjustments%2520achieve%2520state-of-the-art%2520performance%2520on%250Abenchmarks%2520for%2520verbatim%2520speech%2520transcription%252C%2520word%2520segmentation%252C%2520and%2520the%2520timed%250Adetection%2520of%2520filler%2520events%252C%2520and%2520can%2520further%2520mitigate%2520transcription%250Ahallucinations.%2520The%2520code%2520is%2520available%2520open%250Ahttps%253A//github.com/nyrahealth/CrisperWhisper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrisperWhisper%3A%20Accurate%20Timestamps%20on%20Verbatim%20Speech%20Transcriptions&entry.906535625=Laurin%20Wagner%20and%20Bernhard%20Thallinger%20and%20Mario%20Zusag&entry.1292438233=%20%20We%20demonstrate%20that%20carefully%20adjusting%20the%20tokenizer%20of%20the%20Whisper%20speech%0Arecognition%20model%20significantly%20improves%20the%20precision%20of%20word-level%20timestamps%0Awhen%20applying%20dynamic%20time%20warping%20to%20the%20decoder%27s%20cross-attention%20scores.%20We%0Afine-tune%20the%20model%20to%20produce%20more%20verbatim%20speech%20transcriptions%20and%20employ%0Aseveral%20techniques%20to%20increase%20robustness%20against%20multiple%20speakers%20and%0Abackground%20noise.%20These%20adjustments%20achieve%20state-of-the-art%20performance%20on%0Abenchmarks%20for%20verbatim%20speech%20transcription%2C%20word%20segmentation%2C%20and%20the%20timed%0Adetection%20of%20filler%20events%2C%20and%20can%20further%20mitigate%20transcription%0Ahallucinations.%20The%20code%20is%20available%20open%0Ahttps%3A//github.com/nyrahealth/CrisperWhisper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16589v1&entry.124074799=Read"},
{"title": "DeepSPoC: A Deep Learning-Based PDE Solver Governed by Sequential\n  Propagation of Chaos", "author": "Kai Du and Yongle Xie and Tao Zhou and Yuancheng Zhou", "abstract": "  Sequential propagation of chaos (SPoC) is a recently developed tool to solve\nmean-field stochastic differential equations and their related nonlinear\nFokker-Planck equations. Based on the theory of SPoC, we present a new method\n(deepSPoC) that combines the interacting particle system of SPoC and deep\nlearning. Under the framework of deepSPoC, two classes of frequently used deep\nmodels include fully connected neural networks and normalizing flows are\nconsidered. For high-dimensional problems, spatial adaptive method are designed\nto further improve the accuracy and efficiency of deepSPoC. We analysis the\nconvergence of the framework of deepSPoC under some simplified conditions and\nalso provide a posterior error estimation for the algorithm. Finally, we test\nour methods on a wide range of different types of mean-field equations.\n", "link": "http://arxiv.org/abs/2408.16403v1", "date": "2024-08-29", "relevancy": 1.2951, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4409}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4355}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepSPoC%3A%20A%20Deep%20Learning-Based%20PDE%20Solver%20Governed%20by%20Sequential%0A%20%20Propagation%20of%20Chaos&body=Title%3A%20DeepSPoC%3A%20A%20Deep%20Learning-Based%20PDE%20Solver%20Governed%20by%20Sequential%0A%20%20Propagation%20of%20Chaos%0AAuthor%3A%20Kai%20Du%20and%20Yongle%20Xie%20and%20Tao%20Zhou%20and%20Yuancheng%20Zhou%0AAbstract%3A%20%20%20Sequential%20propagation%20of%20chaos%20%28SPoC%29%20is%20a%20recently%20developed%20tool%20to%20solve%0Amean-field%20stochastic%20differential%20equations%20and%20their%20related%20nonlinear%0AFokker-Planck%20equations.%20Based%20on%20the%20theory%20of%20SPoC%2C%20we%20present%20a%20new%20method%0A%28deepSPoC%29%20that%20combines%20the%20interacting%20particle%20system%20of%20SPoC%20and%20deep%0Alearning.%20Under%20the%20framework%20of%20deepSPoC%2C%20two%20classes%20of%20frequently%20used%20deep%0Amodels%20include%20fully%20connected%20neural%20networks%20and%20normalizing%20flows%20are%0Aconsidered.%20For%20high-dimensional%20problems%2C%20spatial%20adaptive%20method%20are%20designed%0Ato%20further%20improve%20the%20accuracy%20and%20efficiency%20of%20deepSPoC.%20We%20analysis%20the%0Aconvergence%20of%20the%20framework%20of%20deepSPoC%20under%20some%20simplified%20conditions%20and%0Aalso%20provide%20a%20posterior%20error%20estimation%20for%20the%20algorithm.%20Finally%2C%20we%20test%0Aour%20methods%20on%20a%20wide%20range%20of%20different%20types%20of%20mean-field%20equations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepSPoC%253A%2520A%2520Deep%2520Learning-Based%2520PDE%2520Solver%2520Governed%2520by%2520Sequential%250A%2520%2520Propagation%2520of%2520Chaos%26entry.906535625%3DKai%2520Du%2520and%2520Yongle%2520Xie%2520and%2520Tao%2520Zhou%2520and%2520Yuancheng%2520Zhou%26entry.1292438233%3D%2520%2520Sequential%2520propagation%2520of%2520chaos%2520%2528SPoC%2529%2520is%2520a%2520recently%2520developed%2520tool%2520to%2520solve%250Amean-field%2520stochastic%2520differential%2520equations%2520and%2520their%2520related%2520nonlinear%250AFokker-Planck%2520equations.%2520Based%2520on%2520the%2520theory%2520of%2520SPoC%252C%2520we%2520present%2520a%2520new%2520method%250A%2528deepSPoC%2529%2520that%2520combines%2520the%2520interacting%2520particle%2520system%2520of%2520SPoC%2520and%2520deep%250Alearning.%2520Under%2520the%2520framework%2520of%2520deepSPoC%252C%2520two%2520classes%2520of%2520frequently%2520used%2520deep%250Amodels%2520include%2520fully%2520connected%2520neural%2520networks%2520and%2520normalizing%2520flows%2520are%250Aconsidered.%2520For%2520high-dimensional%2520problems%252C%2520spatial%2520adaptive%2520method%2520are%2520designed%250Ato%2520further%2520improve%2520the%2520accuracy%2520and%2520efficiency%2520of%2520deepSPoC.%2520We%2520analysis%2520the%250Aconvergence%2520of%2520the%2520framework%2520of%2520deepSPoC%2520under%2520some%2520simplified%2520conditions%2520and%250Aalso%2520provide%2520a%2520posterior%2520error%2520estimation%2520for%2520the%2520algorithm.%2520Finally%252C%2520we%2520test%250Aour%2520methods%2520on%2520a%2520wide%2520range%2520of%2520different%2520types%2520of%2520mean-field%2520equations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSPoC%3A%20A%20Deep%20Learning-Based%20PDE%20Solver%20Governed%20by%20Sequential%0A%20%20Propagation%20of%20Chaos&entry.906535625=Kai%20Du%20and%20Yongle%20Xie%20and%20Tao%20Zhou%20and%20Yuancheng%20Zhou&entry.1292438233=%20%20Sequential%20propagation%20of%20chaos%20%28SPoC%29%20is%20a%20recently%20developed%20tool%20to%20solve%0Amean-field%20stochastic%20differential%20equations%20and%20their%20related%20nonlinear%0AFokker-Planck%20equations.%20Based%20on%20the%20theory%20of%20SPoC%2C%20we%20present%20a%20new%20method%0A%28deepSPoC%29%20that%20combines%20the%20interacting%20particle%20system%20of%20SPoC%20and%20deep%0Alearning.%20Under%20the%20framework%20of%20deepSPoC%2C%20two%20classes%20of%20frequently%20used%20deep%0Amodels%20include%20fully%20connected%20neural%20networks%20and%20normalizing%20flows%20are%0Aconsidered.%20For%20high-dimensional%20problems%2C%20spatial%20adaptive%20method%20are%20designed%0Ato%20further%20improve%20the%20accuracy%20and%20efficiency%20of%20deepSPoC.%20We%20analysis%20the%0Aconvergence%20of%20the%20framework%20of%20deepSPoC%20under%20some%20simplified%20conditions%20and%0Aalso%20provide%20a%20posterior%20error%20estimation%20for%20the%20algorithm.%20Finally%2C%20we%20test%0Aour%20methods%20on%20a%20wide%20range%20of%20different%20types%20of%20mean-field%20equations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16403v1&entry.124074799=Read"},
{"title": "Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease\n  Classification: A Systematic Review", "author": "Lisanne van Gelderen and Cristian Tejedor-Garc\u00eda", "abstract": "  Parkinson's disease (PD), the second most prevalent neurodegenerative\ndisorder worldwide, frequently presents with early-stage speech impairments.\nRecent advancements in Artificial Intelligence (AI), particularly deep learning\n(DL), have significantly enhanced PD diagnosis through the analysis of speech\ndata. Nevertheless, the progress of research is restricted by the limited\navailability of publicly accessible speech-based PD datasets, primarily due to\nprivacy concerns. The goal of this systematic review is to explore the current\nlandscape of speech-based DL approaches for PD classification, based on 33\nscientific works published between 2020 and March 2024. We discuss their\navailable resources, capabilities, potential limitations, and issues related to\nbias, explainability, and privacy. Furthermore, this review provides an\noverview of publicly accessible speech-based datasets and open-source material\nfor PD. The DL approaches are categorized into end-to-end (E2E) learning,\ntransfer learning (TL) and deep acoustic features extraction (DAFE) approaches.\nAmong E2E approaches, Convolutional Neural Networks (CNNs) are prevalent,\nthough Transformers are increasingly popular. E2E approaches face challenges\nsuch as limited data and computational resources, especially with Transformers.\nTL addresses these issues by providing more robust PD diagnosis and better\ngeneralizability across languages. DAFE aims to improve the explainability and\ninterpretability of results by examining the specific effects of deep features\non both other DL approaches and more traditional machine learning (ML) methods.\nHowever, it often underperforms compared to E2E and TL approaches.\n", "link": "http://arxiv.org/abs/2407.17844v2", "date": "2024-08-29", "relevancy": 1.3758, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4752}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Innovative%20Speech-Based%20Deep%20Learning%20Approaches%20for%20Parkinson%27s%20Disease%0A%20%20Classification%3A%20A%20Systematic%20Review&body=Title%3A%20Innovative%20Speech-Based%20Deep%20Learning%20Approaches%20for%20Parkinson%27s%20Disease%0A%20%20Classification%3A%20A%20Systematic%20Review%0AAuthor%3A%20Lisanne%20van%20Gelderen%20and%20Cristian%20Tejedor-Garc%C3%ADa%0AAbstract%3A%20%20%20Parkinson%27s%20disease%20%28PD%29%2C%20the%20second%20most%20prevalent%20neurodegenerative%0Adisorder%20worldwide%2C%20frequently%20presents%20with%20early-stage%20speech%20impairments.%0ARecent%20advancements%20in%20Artificial%20Intelligence%20%28AI%29%2C%20particularly%20deep%20learning%0A%28DL%29%2C%20have%20significantly%20enhanced%20PD%20diagnosis%20through%20the%20analysis%20of%20speech%0Adata.%20Nevertheless%2C%20the%20progress%20of%20research%20is%20restricted%20by%20the%20limited%0Aavailability%20of%20publicly%20accessible%20speech-based%20PD%20datasets%2C%20primarily%20due%20to%0Aprivacy%20concerns.%20The%20goal%20of%20this%20systematic%20review%20is%20to%20explore%20the%20current%0Alandscape%20of%20speech-based%20DL%20approaches%20for%20PD%20classification%2C%20based%20on%2033%0Ascientific%20works%20published%20between%202020%20and%20March%202024.%20We%20discuss%20their%0Aavailable%20resources%2C%20capabilities%2C%20potential%20limitations%2C%20and%20issues%20related%20to%0Abias%2C%20explainability%2C%20and%20privacy.%20Furthermore%2C%20this%20review%20provides%20an%0Aoverview%20of%20publicly%20accessible%20speech-based%20datasets%20and%20open-source%20material%0Afor%20PD.%20The%20DL%20approaches%20are%20categorized%20into%20end-to-end%20%28E2E%29%20learning%2C%0Atransfer%20learning%20%28TL%29%20and%20deep%20acoustic%20features%20extraction%20%28DAFE%29%20approaches.%0AAmong%20E2E%20approaches%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20prevalent%2C%0Athough%20Transformers%20are%20increasingly%20popular.%20E2E%20approaches%20face%20challenges%0Asuch%20as%20limited%20data%20and%20computational%20resources%2C%20especially%20with%20Transformers.%0ATL%20addresses%20these%20issues%20by%20providing%20more%20robust%20PD%20diagnosis%20and%20better%0Ageneralizability%20across%20languages.%20DAFE%20aims%20to%20improve%20the%20explainability%20and%0Ainterpretability%20of%20results%20by%20examining%20the%20specific%20effects%20of%20deep%20features%0Aon%20both%20other%20DL%20approaches%20and%20more%20traditional%20machine%20learning%20%28ML%29%20methods.%0AHowever%2C%20it%20often%20underperforms%20compared%20to%20E2E%20and%20TL%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInnovative%2520Speech-Based%2520Deep%2520Learning%2520Approaches%2520for%2520Parkinson%2527s%2520Disease%250A%2520%2520Classification%253A%2520A%2520Systematic%2520Review%26entry.906535625%3DLisanne%2520van%2520Gelderen%2520and%2520Cristian%2520Tejedor-Garc%25C3%25ADa%26entry.1292438233%3D%2520%2520Parkinson%2527s%2520disease%2520%2528PD%2529%252C%2520the%2520second%2520most%2520prevalent%2520neurodegenerative%250Adisorder%2520worldwide%252C%2520frequently%2520presents%2520with%2520early-stage%2520speech%2520impairments.%250ARecent%2520advancements%2520in%2520Artificial%2520Intelligence%2520%2528AI%2529%252C%2520particularly%2520deep%2520learning%250A%2528DL%2529%252C%2520have%2520significantly%2520enhanced%2520PD%2520diagnosis%2520through%2520the%2520analysis%2520of%2520speech%250Adata.%2520Nevertheless%252C%2520the%2520progress%2520of%2520research%2520is%2520restricted%2520by%2520the%2520limited%250Aavailability%2520of%2520publicly%2520accessible%2520speech-based%2520PD%2520datasets%252C%2520primarily%2520due%2520to%250Aprivacy%2520concerns.%2520The%2520goal%2520of%2520this%2520systematic%2520review%2520is%2520to%2520explore%2520the%2520current%250Alandscape%2520of%2520speech-based%2520DL%2520approaches%2520for%2520PD%2520classification%252C%2520based%2520on%252033%250Ascientific%2520works%2520published%2520between%25202020%2520and%2520March%25202024.%2520We%2520discuss%2520their%250Aavailable%2520resources%252C%2520capabilities%252C%2520potential%2520limitations%252C%2520and%2520issues%2520related%2520to%250Abias%252C%2520explainability%252C%2520and%2520privacy.%2520Furthermore%252C%2520this%2520review%2520provides%2520an%250Aoverview%2520of%2520publicly%2520accessible%2520speech-based%2520datasets%2520and%2520open-source%2520material%250Afor%2520PD.%2520The%2520DL%2520approaches%2520are%2520categorized%2520into%2520end-to-end%2520%2528E2E%2529%2520learning%252C%250Atransfer%2520learning%2520%2528TL%2529%2520and%2520deep%2520acoustic%2520features%2520extraction%2520%2528DAFE%2529%2520approaches.%250AAmong%2520E2E%2520approaches%252C%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520are%2520prevalent%252C%250Athough%2520Transformers%2520are%2520increasingly%2520popular.%2520E2E%2520approaches%2520face%2520challenges%250Asuch%2520as%2520limited%2520data%2520and%2520computational%2520resources%252C%2520especially%2520with%2520Transformers.%250ATL%2520addresses%2520these%2520issues%2520by%2520providing%2520more%2520robust%2520PD%2520diagnosis%2520and%2520better%250Ageneralizability%2520across%2520languages.%2520DAFE%2520aims%2520to%2520improve%2520the%2520explainability%2520and%250Ainterpretability%2520of%2520results%2520by%2520examining%2520the%2520specific%2520effects%2520of%2520deep%2520features%250Aon%2520both%2520other%2520DL%2520approaches%2520and%2520more%2520traditional%2520machine%2520learning%2520%2528ML%2529%2520methods.%250AHowever%252C%2520it%2520often%2520underperforms%2520compared%2520to%2520E2E%2520and%2520TL%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Innovative%20Speech-Based%20Deep%20Learning%20Approaches%20for%20Parkinson%27s%20Disease%0A%20%20Classification%3A%20A%20Systematic%20Review&entry.906535625=Lisanne%20van%20Gelderen%20and%20Cristian%20Tejedor-Garc%C3%ADa&entry.1292438233=%20%20Parkinson%27s%20disease%20%28PD%29%2C%20the%20second%20most%20prevalent%20neurodegenerative%0Adisorder%20worldwide%2C%20frequently%20presents%20with%20early-stage%20speech%20impairments.%0ARecent%20advancements%20in%20Artificial%20Intelligence%20%28AI%29%2C%20particularly%20deep%20learning%0A%28DL%29%2C%20have%20significantly%20enhanced%20PD%20diagnosis%20through%20the%20analysis%20of%20speech%0Adata.%20Nevertheless%2C%20the%20progress%20of%20research%20is%20restricted%20by%20the%20limited%0Aavailability%20of%20publicly%20accessible%20speech-based%20PD%20datasets%2C%20primarily%20due%20to%0Aprivacy%20concerns.%20The%20goal%20of%20this%20systematic%20review%20is%20to%20explore%20the%20current%0Alandscape%20of%20speech-based%20DL%20approaches%20for%20PD%20classification%2C%20based%20on%2033%0Ascientific%20works%20published%20between%202020%20and%20March%202024.%20We%20discuss%20their%0Aavailable%20resources%2C%20capabilities%2C%20potential%20limitations%2C%20and%20issues%20related%20to%0Abias%2C%20explainability%2C%20and%20privacy.%20Furthermore%2C%20this%20review%20provides%20an%0Aoverview%20of%20publicly%20accessible%20speech-based%20datasets%20and%20open-source%20material%0Afor%20PD.%20The%20DL%20approaches%20are%20categorized%20into%20end-to-end%20%28E2E%29%20learning%2C%0Atransfer%20learning%20%28TL%29%20and%20deep%20acoustic%20features%20extraction%20%28DAFE%29%20approaches.%0AAmong%20E2E%20approaches%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20prevalent%2C%0Athough%20Transformers%20are%20increasingly%20popular.%20E2E%20approaches%20face%20challenges%0Asuch%20as%20limited%20data%20and%20computational%20resources%2C%20especially%20with%20Transformers.%0ATL%20addresses%20these%20issues%20by%20providing%20more%20robust%20PD%20diagnosis%20and%20better%0Ageneralizability%20across%20languages.%20DAFE%20aims%20to%20improve%20the%20explainability%20and%0Ainterpretability%20of%20results%20by%20examining%20the%20specific%20effects%20of%20deep%20features%0Aon%20both%20other%20DL%20approaches%20and%20more%20traditional%20machine%20learning%20%28ML%29%20methods.%0AHowever%2C%20it%20often%20underperforms%20compared%20to%20E2E%20and%20TL%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17844v2&entry.124074799=Read"},
{"title": "Spiking Diffusion Models", "author": "Jiahang Cao and Hanzhong Guo and Ziqing Wang and Deming Zhou and Hao Cheng and Qiang Zhang and Renjing Xu", "abstract": "  Recent years have witnessed Spiking Neural Networks (SNNs) gaining attention\nfor their ultra-low energy consumption and high biological plausibility\ncompared with traditional Artificial Neural Networks (ANNs). Despite their\ndistinguished properties, the application of SNNs in the computationally\nintensive field of image generation is still under exploration. In this paper,\nwe propose the Spiking Diffusion Models (SDMs), an innovative family of\nSNN-based generative models that excel in producing high-quality samples with\nsignificantly reduced energy consumption. In particular, we propose a\nTemporal-wise Spiking Mechanism (TSM) that allows SNNs to capture more temporal\nfeatures from a bio-plasticity perspective. In addition, we propose a\nthreshold-guided strategy that can further improve the performances by up to\n16.7% without any additional training. We also make the first attempt to use\nthe ANN-SNN approach for SNN-based generation tasks. Extensive experimental\nresults reveal that our approach not only exhibits comparable performance to\nits ANN counterpart with few spiking time steps, but also outperforms previous\nSNN-based generative models by a large margin. Moreover, we also demonstrate\nthe high-quality generation ability of SDM on large-scale datasets, e.g., LSUN\nbedroom. This development marks a pivotal advancement in the capabilities of\nSNN-based generation, paving the way for future research avenues to realize\nlow-energy and low-latency generative applications. Our code is available at\nhttps://github.com/AndyCao1125/SDM.\n", "link": "http://arxiv.org/abs/2408.16467v1", "date": "2024-08-29", "relevancy": 1.8045, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6588}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.598}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spiking%20Diffusion%20Models&body=Title%3A%20Spiking%20Diffusion%20Models%0AAuthor%3A%20Jiahang%20Cao%20and%20Hanzhong%20Guo%20and%20Ziqing%20Wang%20and%20Deming%20Zhou%20and%20Hao%20Cheng%20and%20Qiang%20Zhang%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20Spiking%20Neural%20Networks%20%28SNNs%29%20gaining%20attention%0Afor%20their%20ultra-low%20energy%20consumption%20and%20high%20biological%20plausibility%0Acompared%20with%20traditional%20Artificial%20Neural%20Networks%20%28ANNs%29.%20Despite%20their%0Adistinguished%20properties%2C%20the%20application%20of%20SNNs%20in%20the%20computationally%0Aintensive%20field%20of%20image%20generation%20is%20still%20under%20exploration.%20In%20this%20paper%2C%0Awe%20propose%20the%20Spiking%20Diffusion%20Models%20%28SDMs%29%2C%20an%20innovative%20family%20of%0ASNN-based%20generative%20models%20that%20excel%20in%20producing%20high-quality%20samples%20with%0Asignificantly%20reduced%20energy%20consumption.%20In%20particular%2C%20we%20propose%20a%0ATemporal-wise%20Spiking%20Mechanism%20%28TSM%29%20that%20allows%20SNNs%20to%20capture%20more%20temporal%0Afeatures%20from%20a%20bio-plasticity%20perspective.%20In%20addition%2C%20we%20propose%20a%0Athreshold-guided%20strategy%20that%20can%20further%20improve%20the%20performances%20by%20up%20to%0A16.7%25%20without%20any%20additional%20training.%20We%20also%20make%20the%20first%20attempt%20to%20use%0Athe%20ANN-SNN%20approach%20for%20SNN-based%20generation%20tasks.%20Extensive%20experimental%0Aresults%20reveal%20that%20our%20approach%20not%20only%20exhibits%20comparable%20performance%20to%0Aits%20ANN%20counterpart%20with%20few%20spiking%20time%20steps%2C%20but%20also%20outperforms%20previous%0ASNN-based%20generative%20models%20by%20a%20large%20margin.%20Moreover%2C%20we%20also%20demonstrate%0Athe%20high-quality%20generation%20ability%20of%20SDM%20on%20large-scale%20datasets%2C%20e.g.%2C%20LSUN%0Abedroom.%20This%20development%20marks%20a%20pivotal%20advancement%20in%20the%20capabilities%20of%0ASNN-based%20generation%2C%20paving%20the%20way%20for%20future%20research%20avenues%20to%20realize%0Alow-energy%20and%20low-latency%20generative%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AndyCao1125/SDM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpiking%2520Diffusion%2520Models%26entry.906535625%3DJiahang%2520Cao%2520and%2520Hanzhong%2520Guo%2520and%2520Ziqing%2520Wang%2520and%2520Deming%2520Zhou%2520and%2520Hao%2520Cheng%2520and%2520Qiang%2520Zhang%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520witnessed%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520gaining%2520attention%250Afor%2520their%2520ultra-low%2520energy%2520consumption%2520and%2520high%2520biological%2520plausibility%250Acompared%2520with%2520traditional%2520Artificial%2520Neural%2520Networks%2520%2528ANNs%2529.%2520Despite%2520their%250Adistinguished%2520properties%252C%2520the%2520application%2520of%2520SNNs%2520in%2520the%2520computationally%250Aintensive%2520field%2520of%2520image%2520generation%2520is%2520still%2520under%2520exploration.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520the%2520Spiking%2520Diffusion%2520Models%2520%2528SDMs%2529%252C%2520an%2520innovative%2520family%2520of%250ASNN-based%2520generative%2520models%2520that%2520excel%2520in%2520producing%2520high-quality%2520samples%2520with%250Asignificantly%2520reduced%2520energy%2520consumption.%2520In%2520particular%252C%2520we%2520propose%2520a%250ATemporal-wise%2520Spiking%2520Mechanism%2520%2528TSM%2529%2520that%2520allows%2520SNNs%2520to%2520capture%2520more%2520temporal%250Afeatures%2520from%2520a%2520bio-plasticity%2520perspective.%2520In%2520addition%252C%2520we%2520propose%2520a%250Athreshold-guided%2520strategy%2520that%2520can%2520further%2520improve%2520the%2520performances%2520by%2520up%2520to%250A16.7%2525%2520without%2520any%2520additional%2520training.%2520We%2520also%2520make%2520the%2520first%2520attempt%2520to%2520use%250Athe%2520ANN-SNN%2520approach%2520for%2520SNN-based%2520generation%2520tasks.%2520Extensive%2520experimental%250Aresults%2520reveal%2520that%2520our%2520approach%2520not%2520only%2520exhibits%2520comparable%2520performance%2520to%250Aits%2520ANN%2520counterpart%2520with%2520few%2520spiking%2520time%2520steps%252C%2520but%2520also%2520outperforms%2520previous%250ASNN-based%2520generative%2520models%2520by%2520a%2520large%2520margin.%2520Moreover%252C%2520we%2520also%2520demonstrate%250Athe%2520high-quality%2520generation%2520ability%2520of%2520SDM%2520on%2520large-scale%2520datasets%252C%2520e.g.%252C%2520LSUN%250Abedroom.%2520This%2520development%2520marks%2520a%2520pivotal%2520advancement%2520in%2520the%2520capabilities%2520of%250ASNN-based%2520generation%252C%2520paving%2520the%2520way%2520for%2520future%2520research%2520avenues%2520to%2520realize%250Alow-energy%2520and%2520low-latency%2520generative%2520applications.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AndyCao1125/SDM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiking%20Diffusion%20Models&entry.906535625=Jiahang%20Cao%20and%20Hanzhong%20Guo%20and%20Ziqing%20Wang%20and%20Deming%20Zhou%20and%20Hao%20Cheng%20and%20Qiang%20Zhang%20and%20Renjing%20Xu&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20Spiking%20Neural%20Networks%20%28SNNs%29%20gaining%20attention%0Afor%20their%20ultra-low%20energy%20consumption%20and%20high%20biological%20plausibility%0Acompared%20with%20traditional%20Artificial%20Neural%20Networks%20%28ANNs%29.%20Despite%20their%0Adistinguished%20properties%2C%20the%20application%20of%20SNNs%20in%20the%20computationally%0Aintensive%20field%20of%20image%20generation%20is%20still%20under%20exploration.%20In%20this%20paper%2C%0Awe%20propose%20the%20Spiking%20Diffusion%20Models%20%28SDMs%29%2C%20an%20innovative%20family%20of%0ASNN-based%20generative%20models%20that%20excel%20in%20producing%20high-quality%20samples%20with%0Asignificantly%20reduced%20energy%20consumption.%20In%20particular%2C%20we%20propose%20a%0ATemporal-wise%20Spiking%20Mechanism%20%28TSM%29%20that%20allows%20SNNs%20to%20capture%20more%20temporal%0Afeatures%20from%20a%20bio-plasticity%20perspective.%20In%20addition%2C%20we%20propose%20a%0Athreshold-guided%20strategy%20that%20can%20further%20improve%20the%20performances%20by%20up%20to%0A16.7%25%20without%20any%20additional%20training.%20We%20also%20make%20the%20first%20attempt%20to%20use%0Athe%20ANN-SNN%20approach%20for%20SNN-based%20generation%20tasks.%20Extensive%20experimental%0Aresults%20reveal%20that%20our%20approach%20not%20only%20exhibits%20comparable%20performance%20to%0Aits%20ANN%20counterpart%20with%20few%20spiking%20time%20steps%2C%20but%20also%20outperforms%20previous%0ASNN-based%20generative%20models%20by%20a%20large%20margin.%20Moreover%2C%20we%20also%20demonstrate%0Athe%20high-quality%20generation%20ability%20of%20SDM%20on%20large-scale%20datasets%2C%20e.g.%2C%20LSUN%0Abedroom.%20This%20development%20marks%20a%20pivotal%20advancement%20in%20the%20capabilities%20of%0ASNN-based%20generation%2C%20paving%20the%20way%20for%20future%20research%20avenues%20to%20realize%0Alow-energy%20and%20low-latency%20generative%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AndyCao1125/SDM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16467v1&entry.124074799=Read"},
{"title": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever", "author": "Rohan Jha and Bo Wang and Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Han Xiao", "abstract": "  Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis paper, we introduce several improvements to the ColBERT model architecture\nand training pipeline, leveraging techniques successful in the more established\nsingle-vector embedding model paradigm, particularly those suited for\nheterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates\nstrong performance across a range of English and multilingual retrieval tasks,\nwhile also cutting storage requirements by up to 50% compared to previous\nmodels.\n", "link": "http://arxiv.org/abs/2408.16672v1", "date": "2024-08-29", "relevancy": 1.8692, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.473}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4662}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jina-ColBERT-v2%3A%20A%20General-Purpose%20Multilingual%20Late%20Interaction%0A%20%20Retriever&body=Title%3A%20Jina-ColBERT-v2%3A%20A%20General-Purpose%20Multilingual%20Late%20Interaction%0A%20%20Retriever%0AAuthor%3A%20Rohan%20Jha%20and%20Bo%20Wang%20and%20Michael%20G%C3%BCnther%20and%20Saba%20Sturua%20and%20Mohammad%20Kalim%20Akram%20and%20Han%20Xiao%0AAbstract%3A%20%20%20Multi-vector%20dense%20models%2C%20such%20as%20ColBERT%2C%20have%20proven%20highly%20effective%20in%0Ainformation%20retrieval.%20ColBERT%27s%20late%20interaction%20scoring%20approximates%20the%0Ajoint%20query-document%20attention%20seen%20in%20cross-encoders%20while%20maintaining%0Ainference%20efficiency%20closer%20to%20traditional%20dense%20retrieval%20models%2C%20thanks%20to%0Aits%20bi-encoder%20architecture%20and%20recent%20optimizations%20in%20indexing%20and%20search.%20In%0Athis%20paper%2C%20we%20introduce%20several%20improvements%20to%20the%20ColBERT%20model%20architecture%0Aand%20training%20pipeline%2C%20leveraging%20techniques%20successful%20in%20the%20more%20established%0Asingle-vector%20embedding%20model%20paradigm%2C%20particularly%20those%20suited%20for%0Aheterogeneous%20multilingual%20data.%20Our%20new%20model%2C%20Jina-ColBERT-v2%2C%20demonstrates%0Astrong%20performance%20across%20a%20range%20of%20English%20and%20multilingual%20retrieval%20tasks%2C%0Awhile%20also%20cutting%20storage%20requirements%20by%20up%20to%2050%25%20compared%20to%20previous%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJina-ColBERT-v2%253A%2520A%2520General-Purpose%2520Multilingual%2520Late%2520Interaction%250A%2520%2520Retriever%26entry.906535625%3DRohan%2520Jha%2520and%2520Bo%2520Wang%2520and%2520Michael%2520G%25C3%25BCnther%2520and%2520Saba%2520Sturua%2520and%2520Mohammad%2520Kalim%2520Akram%2520and%2520Han%2520Xiao%26entry.1292438233%3D%2520%2520Multi-vector%2520dense%2520models%252C%2520such%2520as%2520ColBERT%252C%2520have%2520proven%2520highly%2520effective%2520in%250Ainformation%2520retrieval.%2520ColBERT%2527s%2520late%2520interaction%2520scoring%2520approximates%2520the%250Ajoint%2520query-document%2520attention%2520seen%2520in%2520cross-encoders%2520while%2520maintaining%250Ainference%2520efficiency%2520closer%2520to%2520traditional%2520dense%2520retrieval%2520models%252C%2520thanks%2520to%250Aits%2520bi-encoder%2520architecture%2520and%2520recent%2520optimizations%2520in%2520indexing%2520and%2520search.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520several%2520improvements%2520to%2520the%2520ColBERT%2520model%2520architecture%250Aand%2520training%2520pipeline%252C%2520leveraging%2520techniques%2520successful%2520in%2520the%2520more%2520established%250Asingle-vector%2520embedding%2520model%2520paradigm%252C%2520particularly%2520those%2520suited%2520for%250Aheterogeneous%2520multilingual%2520data.%2520Our%2520new%2520model%252C%2520Jina-ColBERT-v2%252C%2520demonstrates%250Astrong%2520performance%2520across%2520a%2520range%2520of%2520English%2520and%2520multilingual%2520retrieval%2520tasks%252C%250Awhile%2520also%2520cutting%2520storage%2520requirements%2520by%2520up%2520to%252050%2525%2520compared%2520to%2520previous%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jina-ColBERT-v2%3A%20A%20General-Purpose%20Multilingual%20Late%20Interaction%0A%20%20Retriever&entry.906535625=Rohan%20Jha%20and%20Bo%20Wang%20and%20Michael%20G%C3%BCnther%20and%20Saba%20Sturua%20and%20Mohammad%20Kalim%20Akram%20and%20Han%20Xiao&entry.1292438233=%20%20Multi-vector%20dense%20models%2C%20such%20as%20ColBERT%2C%20have%20proven%20highly%20effective%20in%0Ainformation%20retrieval.%20ColBERT%27s%20late%20interaction%20scoring%20approximates%20the%0Ajoint%20query-document%20attention%20seen%20in%20cross-encoders%20while%20maintaining%0Ainference%20efficiency%20closer%20to%20traditional%20dense%20retrieval%20models%2C%20thanks%20to%0Aits%20bi-encoder%20architecture%20and%20recent%20optimizations%20in%20indexing%20and%20search.%20In%0Athis%20paper%2C%20we%20introduce%20several%20improvements%20to%20the%20ColBERT%20model%20architecture%0Aand%20training%20pipeline%2C%20leveraging%20techniques%20successful%20in%20the%20more%20established%0Asingle-vector%20embedding%20model%20paradigm%2C%20particularly%20those%20suited%20for%0Aheterogeneous%20multilingual%20data.%20Our%20new%20model%2C%20Jina-ColBERT-v2%2C%20demonstrates%0Astrong%20performance%20across%20a%20range%20of%20English%20and%20multilingual%20retrieval%20tasks%2C%0Awhile%20also%20cutting%20storage%20requirements%20by%20up%20to%2050%25%20compared%20to%20previous%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16672v1&entry.124074799=Read"},
{"title": "Transformers Meet ACT-R: Repeat-Aware and Sequential Listening Session\n  Recommendation", "author": "Viet-Anh Tran and Guillaume Salha-Galvan and Bruno Sguerra and Romain Hennequin", "abstract": "  Music streaming services often leverage sequential recommender systems to\npredict the best music to showcase to users based on past sequences of\nlistening sessions. Nonetheless, most sequential recommendation methods ignore\nor insufficiently account for repetitive behaviors. This is a crucial\nlimitation for music recommendation, as repeatedly listening to the same song\nover time is a common phenomenon that can even change the way users perceive\nthis song. In this paper, we introduce PISA (Psychology-Informed Session\nembedding using ACT-R), a session-level sequential recommender system that\novercomes this limitation. PISA employs a Transformer architecture learning\nembedding representations of listening sessions and users using attention\nmechanisms inspired by Anderson's ACT-R (Adaptive Control of Thought-Rational),\na cognitive architecture modeling human information access and memory dynamics.\nThis approach enables us to capture dynamic and repetitive patterns from user\nbehaviors, allowing us to effectively predict the songs they will listen to in\nsubsequent sessions, whether they are repeated or new ones. We demonstrate the\nempirical relevance of PISA using both publicly available listening data from\nLast.fm and proprietary data from Deezer, a global music streaming service,\nconfirming the critical importance of repetition modeling for sequential\nlistening session recommendation. Along with this paper, we publicly release\nour proprietary dataset to foster future research in this field, as well as the\nsource code of PISA to facilitate its future use.\n", "link": "http://arxiv.org/abs/2408.16578v1", "date": "2024-08-29", "relevancy": 0.8666, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4476}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4286}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20Meet%20ACT-R%3A%20Repeat-Aware%20and%20Sequential%20Listening%20Session%0A%20%20Recommendation&body=Title%3A%20Transformers%20Meet%20ACT-R%3A%20Repeat-Aware%20and%20Sequential%20Listening%20Session%0A%20%20Recommendation%0AAuthor%3A%20Viet-Anh%20Tran%20and%20Guillaume%20Salha-Galvan%20and%20Bruno%20Sguerra%20and%20Romain%20Hennequin%0AAbstract%3A%20%20%20Music%20streaming%20services%20often%20leverage%20sequential%20recommender%20systems%20to%0Apredict%20the%20best%20music%20to%20showcase%20to%20users%20based%20on%20past%20sequences%20of%0Alistening%20sessions.%20Nonetheless%2C%20most%20sequential%20recommendation%20methods%20ignore%0Aor%20insufficiently%20account%20for%20repetitive%20behaviors.%20This%20is%20a%20crucial%0Alimitation%20for%20music%20recommendation%2C%20as%20repeatedly%20listening%20to%20the%20same%20song%0Aover%20time%20is%20a%20common%20phenomenon%20that%20can%20even%20change%20the%20way%20users%20perceive%0Athis%20song.%20In%20this%20paper%2C%20we%20introduce%20PISA%20%28Psychology-Informed%20Session%0Aembedding%20using%20ACT-R%29%2C%20a%20session-level%20sequential%20recommender%20system%20that%0Aovercomes%20this%20limitation.%20PISA%20employs%20a%20Transformer%20architecture%20learning%0Aembedding%20representations%20of%20listening%20sessions%20and%20users%20using%20attention%0Amechanisms%20inspired%20by%20Anderson%27s%20ACT-R%20%28Adaptive%20Control%20of%20Thought-Rational%29%2C%0Aa%20cognitive%20architecture%20modeling%20human%20information%20access%20and%20memory%20dynamics.%0AThis%20approach%20enables%20us%20to%20capture%20dynamic%20and%20repetitive%20patterns%20from%20user%0Abehaviors%2C%20allowing%20us%20to%20effectively%20predict%20the%20songs%20they%20will%20listen%20to%20in%0Asubsequent%20sessions%2C%20whether%20they%20are%20repeated%20or%20new%20ones.%20We%20demonstrate%20the%0Aempirical%20relevance%20of%20PISA%20using%20both%20publicly%20available%20listening%20data%20from%0ALast.fm%20and%20proprietary%20data%20from%20Deezer%2C%20a%20global%20music%20streaming%20service%2C%0Aconfirming%20the%20critical%20importance%20of%20repetition%20modeling%20for%20sequential%0Alistening%20session%20recommendation.%20Along%20with%20this%20paper%2C%20we%20publicly%20release%0Aour%20proprietary%20dataset%20to%20foster%20future%20research%20in%20this%20field%2C%20as%20well%20as%20the%0Asource%20code%20of%20PISA%20to%20facilitate%20its%20future%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520Meet%2520ACT-R%253A%2520Repeat-Aware%2520and%2520Sequential%2520Listening%2520Session%250A%2520%2520Recommendation%26entry.906535625%3DViet-Anh%2520Tran%2520and%2520Guillaume%2520Salha-Galvan%2520and%2520Bruno%2520Sguerra%2520and%2520Romain%2520Hennequin%26entry.1292438233%3D%2520%2520Music%2520streaming%2520services%2520often%2520leverage%2520sequential%2520recommender%2520systems%2520to%250Apredict%2520the%2520best%2520music%2520to%2520showcase%2520to%2520users%2520based%2520on%2520past%2520sequences%2520of%250Alistening%2520sessions.%2520Nonetheless%252C%2520most%2520sequential%2520recommendation%2520methods%2520ignore%250Aor%2520insufficiently%2520account%2520for%2520repetitive%2520behaviors.%2520This%2520is%2520a%2520crucial%250Alimitation%2520for%2520music%2520recommendation%252C%2520as%2520repeatedly%2520listening%2520to%2520the%2520same%2520song%250Aover%2520time%2520is%2520a%2520common%2520phenomenon%2520that%2520can%2520even%2520change%2520the%2520way%2520users%2520perceive%250Athis%2520song.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PISA%2520%2528Psychology-Informed%2520Session%250Aembedding%2520using%2520ACT-R%2529%252C%2520a%2520session-level%2520sequential%2520recommender%2520system%2520that%250Aovercomes%2520this%2520limitation.%2520PISA%2520employs%2520a%2520Transformer%2520architecture%2520learning%250Aembedding%2520representations%2520of%2520listening%2520sessions%2520and%2520users%2520using%2520attention%250Amechanisms%2520inspired%2520by%2520Anderson%2527s%2520ACT-R%2520%2528Adaptive%2520Control%2520of%2520Thought-Rational%2529%252C%250Aa%2520cognitive%2520architecture%2520modeling%2520human%2520information%2520access%2520and%2520memory%2520dynamics.%250AThis%2520approach%2520enables%2520us%2520to%2520capture%2520dynamic%2520and%2520repetitive%2520patterns%2520from%2520user%250Abehaviors%252C%2520allowing%2520us%2520to%2520effectively%2520predict%2520the%2520songs%2520they%2520will%2520listen%2520to%2520in%250Asubsequent%2520sessions%252C%2520whether%2520they%2520are%2520repeated%2520or%2520new%2520ones.%2520We%2520demonstrate%2520the%250Aempirical%2520relevance%2520of%2520PISA%2520using%2520both%2520publicly%2520available%2520listening%2520data%2520from%250ALast.fm%2520and%2520proprietary%2520data%2520from%2520Deezer%252C%2520a%2520global%2520music%2520streaming%2520service%252C%250Aconfirming%2520the%2520critical%2520importance%2520of%2520repetition%2520modeling%2520for%2520sequential%250Alistening%2520session%2520recommendation.%2520Along%2520with%2520this%2520paper%252C%2520we%2520publicly%2520release%250Aour%2520proprietary%2520dataset%2520to%2520foster%2520future%2520research%2520in%2520this%2520field%252C%2520as%2520well%2520as%2520the%250Asource%2520code%2520of%2520PISA%2520to%2520facilitate%2520its%2520future%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20Meet%20ACT-R%3A%20Repeat-Aware%20and%20Sequential%20Listening%20Session%0A%20%20Recommendation&entry.906535625=Viet-Anh%20Tran%20and%20Guillaume%20Salha-Galvan%20and%20Bruno%20Sguerra%20and%20Romain%20Hennequin&entry.1292438233=%20%20Music%20streaming%20services%20often%20leverage%20sequential%20recommender%20systems%20to%0Apredict%20the%20best%20music%20to%20showcase%20to%20users%20based%20on%20past%20sequences%20of%0Alistening%20sessions.%20Nonetheless%2C%20most%20sequential%20recommendation%20methods%20ignore%0Aor%20insufficiently%20account%20for%20repetitive%20behaviors.%20This%20is%20a%20crucial%0Alimitation%20for%20music%20recommendation%2C%20as%20repeatedly%20listening%20to%20the%20same%20song%0Aover%20time%20is%20a%20common%20phenomenon%20that%20can%20even%20change%20the%20way%20users%20perceive%0Athis%20song.%20In%20this%20paper%2C%20we%20introduce%20PISA%20%28Psychology-Informed%20Session%0Aembedding%20using%20ACT-R%29%2C%20a%20session-level%20sequential%20recommender%20system%20that%0Aovercomes%20this%20limitation.%20PISA%20employs%20a%20Transformer%20architecture%20learning%0Aembedding%20representations%20of%20listening%20sessions%20and%20users%20using%20attention%0Amechanisms%20inspired%20by%20Anderson%27s%20ACT-R%20%28Adaptive%20Control%20of%20Thought-Rational%29%2C%0Aa%20cognitive%20architecture%20modeling%20human%20information%20access%20and%20memory%20dynamics.%0AThis%20approach%20enables%20us%20to%20capture%20dynamic%20and%20repetitive%20patterns%20from%20user%0Abehaviors%2C%20allowing%20us%20to%20effectively%20predict%20the%20songs%20they%20will%20listen%20to%20in%0Asubsequent%20sessions%2C%20whether%20they%20are%20repeated%20or%20new%20ones.%20We%20demonstrate%20the%0Aempirical%20relevance%20of%20PISA%20using%20both%20publicly%20available%20listening%20data%20from%0ALast.fm%20and%20proprietary%20data%20from%20Deezer%2C%20a%20global%20music%20streaming%20service%2C%0Aconfirming%20the%20critical%20importance%20of%20repetition%20modeling%20for%20sequential%0Alistening%20session%20recommendation.%20Along%20with%20this%20paper%2C%20we%20publicly%20release%0Aour%20proprietary%20dataset%20to%20foster%20future%20research%20in%20this%20field%2C%20as%20well%20as%20the%0Asource%20code%20of%20PISA%20to%20facilitate%20its%20future%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16578v1&entry.124074799=Read"},
{"title": "XCSP3-core: A Format for Representing Constraint\n  Satisfaction/Optimization Problems", "author": "Fr\u00e9d\u00e9ric Boussemart and Christophe Lecoutre and Gilles Audemard and C\u00e9dric Piette", "abstract": "  In this document, we introduce XCSP3-core, a subset of XCSP3 that allows us\nto represent constraint satisfaction/optimization problems. The interest of\nXCSP3-core is multiple: (i) focusing on the most popular frameworks (CSP and\nCOP) and constraints, (ii) facilitating the parsing process by means of\ndedicated XCSP3-core parsers written in Java and C++ (using callback\nfunctions), (iii) and defining a core format for comparisons (competitions) of\nconstraint solvers.\n", "link": "http://arxiv.org/abs/2009.00514v4", "date": "2024-08-29", "relevancy": 1.3908, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3998}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3389}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XCSP3-core%3A%20A%20Format%20for%20Representing%20Constraint%0A%20%20Satisfaction/Optimization%20Problems&body=Title%3A%20XCSP3-core%3A%20A%20Format%20for%20Representing%20Constraint%0A%20%20Satisfaction/Optimization%20Problems%0AAuthor%3A%20Fr%C3%A9d%C3%A9ric%20Boussemart%20and%20Christophe%20Lecoutre%20and%20Gilles%20Audemard%20and%20C%C3%A9dric%20Piette%0AAbstract%3A%20%20%20In%20this%20document%2C%20we%20introduce%20XCSP3-core%2C%20a%20subset%20of%20XCSP3%20that%20allows%20us%0Ato%20represent%20constraint%20satisfaction/optimization%20problems.%20The%20interest%20of%0AXCSP3-core%20is%20multiple%3A%20%28i%29%20focusing%20on%20the%20most%20popular%20frameworks%20%28CSP%20and%0ACOP%29%20and%20constraints%2C%20%28ii%29%20facilitating%20the%20parsing%20process%20by%20means%20of%0Adedicated%20XCSP3-core%20parsers%20written%20in%20Java%20and%20C%2B%2B%20%28using%20callback%0Afunctions%29%2C%20%28iii%29%20and%20defining%20a%20core%20format%20for%20comparisons%20%28competitions%29%20of%0Aconstraint%20solvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2009.00514v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXCSP3-core%253A%2520A%2520Format%2520for%2520Representing%2520Constraint%250A%2520%2520Satisfaction/Optimization%2520Problems%26entry.906535625%3DFr%25C3%25A9d%25C3%25A9ric%2520Boussemart%2520and%2520Christophe%2520Lecoutre%2520and%2520Gilles%2520Audemard%2520and%2520C%25C3%25A9dric%2520Piette%26entry.1292438233%3D%2520%2520In%2520this%2520document%252C%2520we%2520introduce%2520XCSP3-core%252C%2520a%2520subset%2520of%2520XCSP3%2520that%2520allows%2520us%250Ato%2520represent%2520constraint%2520satisfaction/optimization%2520problems.%2520The%2520interest%2520of%250AXCSP3-core%2520is%2520multiple%253A%2520%2528i%2529%2520focusing%2520on%2520the%2520most%2520popular%2520frameworks%2520%2528CSP%2520and%250ACOP%2529%2520and%2520constraints%252C%2520%2528ii%2529%2520facilitating%2520the%2520parsing%2520process%2520by%2520means%2520of%250Adedicated%2520XCSP3-core%2520parsers%2520written%2520in%2520Java%2520and%2520C%252B%252B%2520%2528using%2520callback%250Afunctions%2529%252C%2520%2528iii%2529%2520and%2520defining%2520a%2520core%2520format%2520for%2520comparisons%2520%2528competitions%2529%2520of%250Aconstraint%2520solvers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2009.00514v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XCSP3-core%3A%20A%20Format%20for%20Representing%20Constraint%0A%20%20Satisfaction/Optimization%20Problems&entry.906535625=Fr%C3%A9d%C3%A9ric%20Boussemart%20and%20Christophe%20Lecoutre%20and%20Gilles%20Audemard%20and%20C%C3%A9dric%20Piette&entry.1292438233=%20%20In%20this%20document%2C%20we%20introduce%20XCSP3-core%2C%20a%20subset%20of%20XCSP3%20that%20allows%20us%0Ato%20represent%20constraint%20satisfaction/optimization%20problems.%20The%20interest%20of%0AXCSP3-core%20is%20multiple%3A%20%28i%29%20focusing%20on%20the%20most%20popular%20frameworks%20%28CSP%20and%0ACOP%29%20and%20constraints%2C%20%28ii%29%20facilitating%20the%20parsing%20process%20by%20means%20of%0Adedicated%20XCSP3-core%20parsers%20written%20in%20Java%20and%20C%2B%2B%20%28using%20callback%0Afunctions%29%2C%20%28iii%29%20and%20defining%20a%20core%20format%20for%20comparisons%20%28competitions%29%20of%0Aconstraint%20solvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2009.00514v4&entry.124074799=Read"},
{"title": "PromptSmooth: Certifying Robustness of Medical Vision-Language Models\n  via Prompt Learning", "author": "Noor Hussein and Fahad Shamshad and Muzammal Naseer and Karthik Nandakumar", "abstract": "  Medical vision-language models (Med-VLMs) trained on large datasets of\nmedical image-text pairs and later fine-tuned for specific tasks have emerged\nas a mainstream paradigm in medical image analysis. However, recent studies\nhave highlighted the susceptibility of these Med-VLMs to adversarial attacks,\nraising concerns about their safety and robustness. Randomized smoothing is a\nwell-known technique for turning any classifier into a model that is\ncertifiably robust to adversarial perturbations. However, this approach\nrequires retraining the Med-VLM-based classifier so that it classifies well\nunder Gaussian noise, which is often infeasible in practice. In this paper, we\npropose a novel framework called PromptSmooth to achieve efficient certified\nrobustness of Med-VLMs by leveraging the concept of prompt learning. Given any\npre-trained Med-VLM, PromptSmooth adapts it to handle Gaussian noise by\nlearning textual prompts in a zero-shot or few-shot manner, achieving a\ndelicate balance between accuracy and robustness, while minimizing the\ncomputational overhead. Moreover, PromptSmooth requires only a single model to\nhandle multiple noise levels, which substantially reduces the computational\ncost compared to traditional methods that rely on training a separate model for\neach noise level. Comprehensive experiments based on three Med-VLMs and across\nsix downstream datasets of various imaging modalities demonstrate the efficacy\nof PromptSmooth. Our code and models are available at\nhttps://github.com/nhussein/promptsmooth.\n", "link": "http://arxiv.org/abs/2408.16769v1", "date": "2024-08-29", "relevancy": 1.037, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5365}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5154}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PromptSmooth%3A%20Certifying%20Robustness%20of%20Medical%20Vision-Language%20Models%0A%20%20via%20Prompt%20Learning&body=Title%3A%20PromptSmooth%3A%20Certifying%20Robustness%20of%20Medical%20Vision-Language%20Models%0A%20%20via%20Prompt%20Learning%0AAuthor%3A%20Noor%20Hussein%20and%20Fahad%20Shamshad%20and%20Muzammal%20Naseer%20and%20Karthik%20Nandakumar%0AAbstract%3A%20%20%20Medical%20vision-language%20models%20%28Med-VLMs%29%20trained%20on%20large%20datasets%20of%0Amedical%20image-text%20pairs%20and%20later%20fine-tuned%20for%20specific%20tasks%20have%20emerged%0Aas%20a%20mainstream%20paradigm%20in%20medical%20image%20analysis.%20However%2C%20recent%20studies%0Ahave%20highlighted%20the%20susceptibility%20of%20these%20Med-VLMs%20to%20adversarial%20attacks%2C%0Araising%20concerns%20about%20their%20safety%20and%20robustness.%20Randomized%20smoothing%20is%20a%0Awell-known%20technique%20for%20turning%20any%20classifier%20into%20a%20model%20that%20is%0Acertifiably%20robust%20to%20adversarial%20perturbations.%20However%2C%20this%20approach%0Arequires%20retraining%20the%20Med-VLM-based%20classifier%20so%20that%20it%20classifies%20well%0Aunder%20Gaussian%20noise%2C%20which%20is%20often%20infeasible%20in%20practice.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20framework%20called%20PromptSmooth%20to%20achieve%20efficient%20certified%0Arobustness%20of%20Med-VLMs%20by%20leveraging%20the%20concept%20of%20prompt%20learning.%20Given%20any%0Apre-trained%20Med-VLM%2C%20PromptSmooth%20adapts%20it%20to%20handle%20Gaussian%20noise%20by%0Alearning%20textual%20prompts%20in%20a%20zero-shot%20or%20few-shot%20manner%2C%20achieving%20a%0Adelicate%20balance%20between%20accuracy%20and%20robustness%2C%20while%20minimizing%20the%0Acomputational%20overhead.%20Moreover%2C%20PromptSmooth%20requires%20only%20a%20single%20model%20to%0Ahandle%20multiple%20noise%20levels%2C%20which%20substantially%20reduces%20the%20computational%0Acost%20compared%20to%20traditional%20methods%20that%20rely%20on%20training%20a%20separate%20model%20for%0Aeach%20noise%20level.%20Comprehensive%20experiments%20based%20on%20three%20Med-VLMs%20and%20across%0Asix%20downstream%20datasets%20of%20various%20imaging%20modalities%20demonstrate%20the%20efficacy%0Aof%20PromptSmooth.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/nhussein/promptsmooth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromptSmooth%253A%2520Certifying%2520Robustness%2520of%2520Medical%2520Vision-Language%2520Models%250A%2520%2520via%2520Prompt%2520Learning%26entry.906535625%3DNoor%2520Hussein%2520and%2520Fahad%2520Shamshad%2520and%2520Muzammal%2520Naseer%2520and%2520Karthik%2520Nandakumar%26entry.1292438233%3D%2520%2520Medical%2520vision-language%2520models%2520%2528Med-VLMs%2529%2520trained%2520on%2520large%2520datasets%2520of%250Amedical%2520image-text%2520pairs%2520and%2520later%2520fine-tuned%2520for%2520specific%2520tasks%2520have%2520emerged%250Aas%2520a%2520mainstream%2520paradigm%2520in%2520medical%2520image%2520analysis.%2520However%252C%2520recent%2520studies%250Ahave%2520highlighted%2520the%2520susceptibility%2520of%2520these%2520Med-VLMs%2520to%2520adversarial%2520attacks%252C%250Araising%2520concerns%2520about%2520their%2520safety%2520and%2520robustness.%2520Randomized%2520smoothing%2520is%2520a%250Awell-known%2520technique%2520for%2520turning%2520any%2520classifier%2520into%2520a%2520model%2520that%2520is%250Acertifiably%2520robust%2520to%2520adversarial%2520perturbations.%2520However%252C%2520this%2520approach%250Arequires%2520retraining%2520the%2520Med-VLM-based%2520classifier%2520so%2520that%2520it%2520classifies%2520well%250Aunder%2520Gaussian%2520noise%252C%2520which%2520is%2520often%2520infeasible%2520in%2520practice.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520called%2520PromptSmooth%2520to%2520achieve%2520efficient%2520certified%250Arobustness%2520of%2520Med-VLMs%2520by%2520leveraging%2520the%2520concept%2520of%2520prompt%2520learning.%2520Given%2520any%250Apre-trained%2520Med-VLM%252C%2520PromptSmooth%2520adapts%2520it%2520to%2520handle%2520Gaussian%2520noise%2520by%250Alearning%2520textual%2520prompts%2520in%2520a%2520zero-shot%2520or%2520few-shot%2520manner%252C%2520achieving%2520a%250Adelicate%2520balance%2520between%2520accuracy%2520and%2520robustness%252C%2520while%2520minimizing%2520the%250Acomputational%2520overhead.%2520Moreover%252C%2520PromptSmooth%2520requires%2520only%2520a%2520single%2520model%2520to%250Ahandle%2520multiple%2520noise%2520levels%252C%2520which%2520substantially%2520reduces%2520the%2520computational%250Acost%2520compared%2520to%2520traditional%2520methods%2520that%2520rely%2520on%2520training%2520a%2520separate%2520model%2520for%250Aeach%2520noise%2520level.%2520Comprehensive%2520experiments%2520based%2520on%2520three%2520Med-VLMs%2520and%2520across%250Asix%2520downstream%2520datasets%2520of%2520various%2520imaging%2520modalities%2520demonstrate%2520the%2520efficacy%250Aof%2520PromptSmooth.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/nhussein/promptsmooth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PromptSmooth%3A%20Certifying%20Robustness%20of%20Medical%20Vision-Language%20Models%0A%20%20via%20Prompt%20Learning&entry.906535625=Noor%20Hussein%20and%20Fahad%20Shamshad%20and%20Muzammal%20Naseer%20and%20Karthik%20Nandakumar&entry.1292438233=%20%20Medical%20vision-language%20models%20%28Med-VLMs%29%20trained%20on%20large%20datasets%20of%0Amedical%20image-text%20pairs%20and%20later%20fine-tuned%20for%20specific%20tasks%20have%20emerged%0Aas%20a%20mainstream%20paradigm%20in%20medical%20image%20analysis.%20However%2C%20recent%20studies%0Ahave%20highlighted%20the%20susceptibility%20of%20these%20Med-VLMs%20to%20adversarial%20attacks%2C%0Araising%20concerns%20about%20their%20safety%20and%20robustness.%20Randomized%20smoothing%20is%20a%0Awell-known%20technique%20for%20turning%20any%20classifier%20into%20a%20model%20that%20is%0Acertifiably%20robust%20to%20adversarial%20perturbations.%20However%2C%20this%20approach%0Arequires%20retraining%20the%20Med-VLM-based%20classifier%20so%20that%20it%20classifies%20well%0Aunder%20Gaussian%20noise%2C%20which%20is%20often%20infeasible%20in%20practice.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20framework%20called%20PromptSmooth%20to%20achieve%20efficient%20certified%0Arobustness%20of%20Med-VLMs%20by%20leveraging%20the%20concept%20of%20prompt%20learning.%20Given%20any%0Apre-trained%20Med-VLM%2C%20PromptSmooth%20adapts%20it%20to%20handle%20Gaussian%20noise%20by%0Alearning%20textual%20prompts%20in%20a%20zero-shot%20or%20few-shot%20manner%2C%20achieving%20a%0Adelicate%20balance%20between%20accuracy%20and%20robustness%2C%20while%20minimizing%20the%0Acomputational%20overhead.%20Moreover%2C%20PromptSmooth%20requires%20only%20a%20single%20model%20to%0Ahandle%20multiple%20noise%20levels%2C%20which%20substantially%20reduces%20the%20computational%0Acost%20compared%20to%20traditional%20methods%20that%20rely%20on%20training%20a%20separate%20model%20for%0Aeach%20noise%20level.%20Comprehensive%20experiments%20based%20on%20three%20Med-VLMs%20and%20across%0Asix%20downstream%20datasets%20of%20various%20imaging%20modalities%20demonstrate%20the%20efficacy%0Aof%20PromptSmooth.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/nhussein/promptsmooth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16769v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


