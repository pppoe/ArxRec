<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241027.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DiffGS: Functional Gaussian Splatting Diffusion", "author": "Junsheng Zhou and Weiqi Zhang and Yu-Shen Liu", "abstract": "  3D Gaussian Splatting (3DGS) has shown convincing performance in rendering\nspeed and fidelity, yet the generation of Gaussian Splatting remains a\nchallenge due to its discreteness and unstructured nature. In this work, we\npropose DiffGS, a general Gaussian generator based on latent diffusion models.\nDiffGS is a powerful and efficient 3D generative model which is capable of\ngenerating Gaussian primitives at arbitrary numbers for high-fidelity rendering\nwith rasterization. The key insight is to represent Gaussian Splatting in a\ndisentangled manner via three novel functions to model Gaussian probabilities,\ncolors and transforms. Through the novel disentanglement of 3DGS, we represent\nthe discrete and unstructured 3DGS with continuous Gaussian Splatting\nfunctions, where we then train a latent diffusion model with the target of\ngenerating these Gaussian Splatting functions both unconditionally and\nconditionally. Meanwhile, we introduce a discretization algorithm to extract\nGaussians at arbitrary numbers from the generated functions via octree-guided\nsampling and optimization. We explore DiffGS for various tasks, including\nunconditional generation, conditional generation from text, image, and partial\n3DGS, as well as Point-to-Gaussian generation. We believe that DiffGS provides\na new direction for flexibly modeling and generating Gaussian Splatting.\n", "link": "http://arxiv.org/abs/2410.19657v1", "date": "2024-10-25", "relevancy": 3.2345, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6573}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6464}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffGS%3A%20Functional%20Gaussian%20Splatting%20Diffusion&body=Title%3A%20DiffGS%3A%20Functional%20Gaussian%20Splatting%20Diffusion%0AAuthor%3A%20Junsheng%20Zhou%20and%20Weiqi%20Zhang%20and%20Yu-Shen%20Liu%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20convincing%20performance%20in%20rendering%0Aspeed%20and%20fidelity%2C%20yet%20the%20generation%20of%20Gaussian%20Splatting%20remains%20a%0Achallenge%20due%20to%20its%20discreteness%20and%20unstructured%20nature.%20In%20this%20work%2C%20we%0Apropose%20DiffGS%2C%20a%20general%20Gaussian%20generator%20based%20on%20latent%20diffusion%20models.%0ADiffGS%20is%20a%20powerful%20and%20efficient%203D%20generative%20model%20which%20is%20capable%20of%0Agenerating%20Gaussian%20primitives%20at%20arbitrary%20numbers%20for%20high-fidelity%20rendering%0Awith%20rasterization.%20The%20key%20insight%20is%20to%20represent%20Gaussian%20Splatting%20in%20a%0Adisentangled%20manner%20via%20three%20novel%20functions%20to%20model%20Gaussian%20probabilities%2C%0Acolors%20and%20transforms.%20Through%20the%20novel%20disentanglement%20of%203DGS%2C%20we%20represent%0Athe%20discrete%20and%20unstructured%203DGS%20with%20continuous%20Gaussian%20Splatting%0Afunctions%2C%20where%20we%20then%20train%20a%20latent%20diffusion%20model%20with%20the%20target%20of%0Agenerating%20these%20Gaussian%20Splatting%20functions%20both%20unconditionally%20and%0Aconditionally.%20Meanwhile%2C%20we%20introduce%20a%20discretization%20algorithm%20to%20extract%0AGaussians%20at%20arbitrary%20numbers%20from%20the%20generated%20functions%20via%20octree-guided%0Asampling%20and%20optimization.%20We%20explore%20DiffGS%20for%20various%20tasks%2C%20including%0Aunconditional%20generation%2C%20conditional%20generation%20from%20text%2C%20image%2C%20and%20partial%0A3DGS%2C%20as%20well%20as%20Point-to-Gaussian%20generation.%20We%20believe%20that%20DiffGS%20provides%0Aa%20new%20direction%20for%20flexibly%20modeling%20and%20generating%20Gaussian%20Splatting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffGS%253A%2520Functional%2520Gaussian%2520Splatting%2520Diffusion%26entry.906535625%3DJunsheng%2520Zhou%2520and%2520Weiqi%2520Zhang%2520and%2520Yu-Shen%2520Liu%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520shown%2520convincing%2520performance%2520in%2520rendering%250Aspeed%2520and%2520fidelity%252C%2520yet%2520the%2520generation%2520of%2520Gaussian%2520Splatting%2520remains%2520a%250Achallenge%2520due%2520to%2520its%2520discreteness%2520and%2520unstructured%2520nature.%2520In%2520this%2520work%252C%2520we%250Apropose%2520DiffGS%252C%2520a%2520general%2520Gaussian%2520generator%2520based%2520on%2520latent%2520diffusion%2520models.%250ADiffGS%2520is%2520a%2520powerful%2520and%2520efficient%25203D%2520generative%2520model%2520which%2520is%2520capable%2520of%250Agenerating%2520Gaussian%2520primitives%2520at%2520arbitrary%2520numbers%2520for%2520high-fidelity%2520rendering%250Awith%2520rasterization.%2520The%2520key%2520insight%2520is%2520to%2520represent%2520Gaussian%2520Splatting%2520in%2520a%250Adisentangled%2520manner%2520via%2520three%2520novel%2520functions%2520to%2520model%2520Gaussian%2520probabilities%252C%250Acolors%2520and%2520transforms.%2520Through%2520the%2520novel%2520disentanglement%2520of%25203DGS%252C%2520we%2520represent%250Athe%2520discrete%2520and%2520unstructured%25203DGS%2520with%2520continuous%2520Gaussian%2520Splatting%250Afunctions%252C%2520where%2520we%2520then%2520train%2520a%2520latent%2520diffusion%2520model%2520with%2520the%2520target%2520of%250Agenerating%2520these%2520Gaussian%2520Splatting%2520functions%2520both%2520unconditionally%2520and%250Aconditionally.%2520Meanwhile%252C%2520we%2520introduce%2520a%2520discretization%2520algorithm%2520to%2520extract%250AGaussians%2520at%2520arbitrary%2520numbers%2520from%2520the%2520generated%2520functions%2520via%2520octree-guided%250Asampling%2520and%2520optimization.%2520We%2520explore%2520DiffGS%2520for%2520various%2520tasks%252C%2520including%250Aunconditional%2520generation%252C%2520conditional%2520generation%2520from%2520text%252C%2520image%252C%2520and%2520partial%250A3DGS%252C%2520as%2520well%2520as%2520Point-to-Gaussian%2520generation.%2520We%2520believe%2520that%2520DiffGS%2520provides%250Aa%2520new%2520direction%2520for%2520flexibly%2520modeling%2520and%2520generating%2520Gaussian%2520Splatting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffGS%3A%20Functional%20Gaussian%20Splatting%20Diffusion&entry.906535625=Junsheng%20Zhou%20and%20Weiqi%20Zhang%20and%20Yu-Shen%20Liu&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20convincing%20performance%20in%20rendering%0Aspeed%20and%20fidelity%2C%20yet%20the%20generation%20of%20Gaussian%20Splatting%20remains%20a%0Achallenge%20due%20to%20its%20discreteness%20and%20unstructured%20nature.%20In%20this%20work%2C%20we%0Apropose%20DiffGS%2C%20a%20general%20Gaussian%20generator%20based%20on%20latent%20diffusion%20models.%0ADiffGS%20is%20a%20powerful%20and%20efficient%203D%20generative%20model%20which%20is%20capable%20of%0Agenerating%20Gaussian%20primitives%20at%20arbitrary%20numbers%20for%20high-fidelity%20rendering%0Awith%20rasterization.%20The%20key%20insight%20is%20to%20represent%20Gaussian%20Splatting%20in%20a%0Adisentangled%20manner%20via%20three%20novel%20functions%20to%20model%20Gaussian%20probabilities%2C%0Acolors%20and%20transforms.%20Through%20the%20novel%20disentanglement%20of%203DGS%2C%20we%20represent%0Athe%20discrete%20and%20unstructured%203DGS%20with%20continuous%20Gaussian%20Splatting%0Afunctions%2C%20where%20we%20then%20train%20a%20latent%20diffusion%20model%20with%20the%20target%20of%0Agenerating%20these%20Gaussian%20Splatting%20functions%20both%20unconditionally%20and%0Aconditionally.%20Meanwhile%2C%20we%20introduce%20a%20discretization%20algorithm%20to%20extract%0AGaussians%20at%20arbitrary%20numbers%20from%20the%20generated%20functions%20via%20octree-guided%0Asampling%20and%20optimization.%20We%20explore%20DiffGS%20for%20various%20tasks%2C%20including%0Aunconditional%20generation%2C%20conditional%20generation%20from%20text%2C%20image%2C%20and%20partial%0A3DGS%2C%20as%20well%20as%20Point-to-Gaussian%20generation.%20We%20believe%20that%20DiffGS%20provides%0Aa%20new%20direction%20for%20flexibly%20modeling%20and%20generating%20Gaussian%20Splatting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19657v1&entry.124074799=Read"},
{"title": "MonoDGP: Monocular 3D Object Detection with Decoupled-Query and\n  Geometry-Error Priors", "author": "Fanqi Pu and Yifan Wang and Jiru Deng and Wenming Yang", "abstract": "  Perspective projection has been extensively utilized in monocular 3D object\ndetection methods. It introduces geometric priors from 2D bounding boxes and 3D\nobject dimensions to reduce the uncertainty of depth estimation. However, due\nto depth errors originating from the object's visual surface, the height of the\nbounding box often fails to represent the actual projected central height,\nwhich undermines the effectiveness of geometric depth. Direct prediction for\nthe projected height unavoidably results in a loss of 2D priors, while\nmulti-depth prediction with complex branches does not fully leverage geometric\ndepth. This paper presents a Transformer-based monocular 3D object detection\nmethod called MonoDGP, which adopts perspective-invariant geometry errors to\nmodify the projection formula. We also try to systematically discuss and\nexplain the mechanisms and efficacy behind geometry errors, which serve as a\nsimple but effective alternative to multi-depth prediction. Additionally,\nMonoDGP decouples the depth-guided decoder and constructs a 2D decoder only\ndependent on visual features, providing 2D priors and initializing object\nqueries without the disturbance of 3D detection. To further optimize and\nfine-tune input tokens of the transformer decoder, we also introduce a Region\nSegment Head (RSH) that generates enhanced features and segment embeddings. Our\nmonocular method demonstrates state-of-the-art performance on the KITTI\nbenchmark without extra data. Code is available at\nhttps://github.com/PuFanqi23/MonoDGP.\n", "link": "http://arxiv.org/abs/2410.19590v1", "date": "2024-10-25", "relevancy": 2.9613, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6021}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.588}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoDGP%3A%20Monocular%203D%20Object%20Detection%20with%20Decoupled-Query%20and%0A%20%20Geometry-Error%20Priors&body=Title%3A%20MonoDGP%3A%20Monocular%203D%20Object%20Detection%20with%20Decoupled-Query%20and%0A%20%20Geometry-Error%20Priors%0AAuthor%3A%20Fanqi%20Pu%20and%20Yifan%20Wang%20and%20Jiru%20Deng%20and%20Wenming%20Yang%0AAbstract%3A%20%20%20Perspective%20projection%20has%20been%20extensively%20utilized%20in%20monocular%203D%20object%0Adetection%20methods.%20It%20introduces%20geometric%20priors%20from%202D%20bounding%20boxes%20and%203D%0Aobject%20dimensions%20to%20reduce%20the%20uncertainty%20of%20depth%20estimation.%20However%2C%20due%0Ato%20depth%20errors%20originating%20from%20the%20object%27s%20visual%20surface%2C%20the%20height%20of%20the%0Abounding%20box%20often%20fails%20to%20represent%20the%20actual%20projected%20central%20height%2C%0Awhich%20undermines%20the%20effectiveness%20of%20geometric%20depth.%20Direct%20prediction%20for%0Athe%20projected%20height%20unavoidably%20results%20in%20a%20loss%20of%202D%20priors%2C%20while%0Amulti-depth%20prediction%20with%20complex%20branches%20does%20not%20fully%20leverage%20geometric%0Adepth.%20This%20paper%20presents%20a%20Transformer-based%20monocular%203D%20object%20detection%0Amethod%20called%20MonoDGP%2C%20which%20adopts%20perspective-invariant%20geometry%20errors%20to%0Amodify%20the%20projection%20formula.%20We%20also%20try%20to%20systematically%20discuss%20and%0Aexplain%20the%20mechanisms%20and%20efficacy%20behind%20geometry%20errors%2C%20which%20serve%20as%20a%0Asimple%20but%20effective%20alternative%20to%20multi-depth%20prediction.%20Additionally%2C%0AMonoDGP%20decouples%20the%20depth-guided%20decoder%20and%20constructs%20a%202D%20decoder%20only%0Adependent%20on%20visual%20features%2C%20providing%202D%20priors%20and%20initializing%20object%0Aqueries%20without%20the%20disturbance%20of%203D%20detection.%20To%20further%20optimize%20and%0Afine-tune%20input%20tokens%20of%20the%20transformer%20decoder%2C%20we%20also%20introduce%20a%20Region%0ASegment%20Head%20%28RSH%29%20that%20generates%20enhanced%20features%20and%20segment%20embeddings.%20Our%0Amonocular%20method%20demonstrates%20state-of-the-art%20performance%20on%20the%20KITTI%0Abenchmark%20without%20extra%20data.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PuFanqi23/MonoDGP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoDGP%253A%2520Monocular%25203D%2520Object%2520Detection%2520with%2520Decoupled-Query%2520and%250A%2520%2520Geometry-Error%2520Priors%26entry.906535625%3DFanqi%2520Pu%2520and%2520Yifan%2520Wang%2520and%2520Jiru%2520Deng%2520and%2520Wenming%2520Yang%26entry.1292438233%3D%2520%2520Perspective%2520projection%2520has%2520been%2520extensively%2520utilized%2520in%2520monocular%25203D%2520object%250Adetection%2520methods.%2520It%2520introduces%2520geometric%2520priors%2520from%25202D%2520bounding%2520boxes%2520and%25203D%250Aobject%2520dimensions%2520to%2520reduce%2520the%2520uncertainty%2520of%2520depth%2520estimation.%2520However%252C%2520due%250Ato%2520depth%2520errors%2520originating%2520from%2520the%2520object%2527s%2520visual%2520surface%252C%2520the%2520height%2520of%2520the%250Abounding%2520box%2520often%2520fails%2520to%2520represent%2520the%2520actual%2520projected%2520central%2520height%252C%250Awhich%2520undermines%2520the%2520effectiveness%2520of%2520geometric%2520depth.%2520Direct%2520prediction%2520for%250Athe%2520projected%2520height%2520unavoidably%2520results%2520in%2520a%2520loss%2520of%25202D%2520priors%252C%2520while%250Amulti-depth%2520prediction%2520with%2520complex%2520branches%2520does%2520not%2520fully%2520leverage%2520geometric%250Adepth.%2520This%2520paper%2520presents%2520a%2520Transformer-based%2520monocular%25203D%2520object%2520detection%250Amethod%2520called%2520MonoDGP%252C%2520which%2520adopts%2520perspective-invariant%2520geometry%2520errors%2520to%250Amodify%2520the%2520projection%2520formula.%2520We%2520also%2520try%2520to%2520systematically%2520discuss%2520and%250Aexplain%2520the%2520mechanisms%2520and%2520efficacy%2520behind%2520geometry%2520errors%252C%2520which%2520serve%2520as%2520a%250Asimple%2520but%2520effective%2520alternative%2520to%2520multi-depth%2520prediction.%2520Additionally%252C%250AMonoDGP%2520decouples%2520the%2520depth-guided%2520decoder%2520and%2520constructs%2520a%25202D%2520decoder%2520only%250Adependent%2520on%2520visual%2520features%252C%2520providing%25202D%2520priors%2520and%2520initializing%2520object%250Aqueries%2520without%2520the%2520disturbance%2520of%25203D%2520detection.%2520To%2520further%2520optimize%2520and%250Afine-tune%2520input%2520tokens%2520of%2520the%2520transformer%2520decoder%252C%2520we%2520also%2520introduce%2520a%2520Region%250ASegment%2520Head%2520%2528RSH%2529%2520that%2520generates%2520enhanced%2520features%2520and%2520segment%2520embeddings.%2520Our%250Amonocular%2520method%2520demonstrates%2520state-of-the-art%2520performance%2520on%2520the%2520KITTI%250Abenchmark%2520without%2520extra%2520data.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/PuFanqi23/MonoDGP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoDGP%3A%20Monocular%203D%20Object%20Detection%20with%20Decoupled-Query%20and%0A%20%20Geometry-Error%20Priors&entry.906535625=Fanqi%20Pu%20and%20Yifan%20Wang%20and%20Jiru%20Deng%20and%20Wenming%20Yang&entry.1292438233=%20%20Perspective%20projection%20has%20been%20extensively%20utilized%20in%20monocular%203D%20object%0Adetection%20methods.%20It%20introduces%20geometric%20priors%20from%202D%20bounding%20boxes%20and%203D%0Aobject%20dimensions%20to%20reduce%20the%20uncertainty%20of%20depth%20estimation.%20However%2C%20due%0Ato%20depth%20errors%20originating%20from%20the%20object%27s%20visual%20surface%2C%20the%20height%20of%20the%0Abounding%20box%20often%20fails%20to%20represent%20the%20actual%20projected%20central%20height%2C%0Awhich%20undermines%20the%20effectiveness%20of%20geometric%20depth.%20Direct%20prediction%20for%0Athe%20projected%20height%20unavoidably%20results%20in%20a%20loss%20of%202D%20priors%2C%20while%0Amulti-depth%20prediction%20with%20complex%20branches%20does%20not%20fully%20leverage%20geometric%0Adepth.%20This%20paper%20presents%20a%20Transformer-based%20monocular%203D%20object%20detection%0Amethod%20called%20MonoDGP%2C%20which%20adopts%20perspective-invariant%20geometry%20errors%20to%0Amodify%20the%20projection%20formula.%20We%20also%20try%20to%20systematically%20discuss%20and%0Aexplain%20the%20mechanisms%20and%20efficacy%20behind%20geometry%20errors%2C%20which%20serve%20as%20a%0Asimple%20but%20effective%20alternative%20to%20multi-depth%20prediction.%20Additionally%2C%0AMonoDGP%20decouples%20the%20depth-guided%20decoder%20and%20constructs%20a%202D%20decoder%20only%0Adependent%20on%20visual%20features%2C%20providing%202D%20priors%20and%20initializing%20object%0Aqueries%20without%20the%20disturbance%20of%203D%20detection.%20To%20further%20optimize%20and%0Afine-tune%20input%20tokens%20of%20the%20transformer%20decoder%2C%20we%20also%20introduce%20a%20Region%0ASegment%20Head%20%28RSH%29%20that%20generates%20enhanced%20features%20and%20segment%20embeddings.%20Our%0Amonocular%20method%20demonstrates%20state-of-the-art%20performance%20on%20the%20KITTI%0Abenchmark%20without%20extra%20data.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PuFanqi23/MonoDGP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19590v1&entry.124074799=Read"},
{"title": "Rethinking Visual Dependency in Long-Context Reasoning for Large\n  Vision-Language Models", "author": "Yucheng Zhou and Zhi Rao and Jun Wan and Jianbing Shen", "abstract": "  Large Vision-Language Models (LVLMs) excel in cross-model tasks but\nexperience performance declines in long-context reasoning due to overreliance\non textual information and reduced visual dependency. In this study, we\nempirically analyze LVLMs in long-context reasoning, revealing that increased\ncontext length leads to a higher dependence on language at the expense of\nvisual dependency. To address this issue, we propose a novel training-free\ncontext pruning method that selectively removes less critical textual\ninformation. Our approach enhances visual dependency and reduces textual noise,\nthereby improving LVLM performance in long-context reasoning. We validate our\nmethod by constructing a long-context dataset, demonstrating its effectiveness\nacross various LVLMs. Moreover, further analysis confirms the robustness of\ndifferent token pruning strategies and preliminary explores scaling laws\nbetween pruning rates and context length.\n", "link": "http://arxiv.org/abs/2410.19732v1", "date": "2024-10-25", "relevancy": 2.9189, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6257}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Visual%20Dependency%20in%20Long-Context%20Reasoning%20for%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20Rethinking%20Visual%20Dependency%20in%20Long-Context%20Reasoning%20for%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Yucheng%20Zhou%20and%20Zhi%20Rao%20and%20Jun%20Wan%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20excel%20in%20cross-model%20tasks%20but%0Aexperience%20performance%20declines%20in%20long-context%20reasoning%20due%20to%20overreliance%0Aon%20textual%20information%20and%20reduced%20visual%20dependency.%20In%20this%20study%2C%20we%0Aempirically%20analyze%20LVLMs%20in%20long-context%20reasoning%2C%20revealing%20that%20increased%0Acontext%20length%20leads%20to%20a%20higher%20dependence%20on%20language%20at%20the%20expense%20of%0Avisual%20dependency.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20training-free%0Acontext%20pruning%20method%20that%20selectively%20removes%20less%20critical%20textual%0Ainformation.%20Our%20approach%20enhances%20visual%20dependency%20and%20reduces%20textual%20noise%2C%0Athereby%20improving%20LVLM%20performance%20in%20long-context%20reasoning.%20We%20validate%20our%0Amethod%20by%20constructing%20a%20long-context%20dataset%2C%20demonstrating%20its%20effectiveness%0Aacross%20various%20LVLMs.%20Moreover%2C%20further%20analysis%20confirms%20the%20robustness%20of%0Adifferent%20token%20pruning%20strategies%20and%20preliminary%20explores%20scaling%20laws%0Abetween%20pruning%20rates%20and%20context%20length.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Visual%2520Dependency%2520in%2520Long-Context%2520Reasoning%2520for%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DYucheng%2520Zhou%2520and%2520Zhi%2520Rao%2520and%2520Jun%2520Wan%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520excel%2520in%2520cross-model%2520tasks%2520but%250Aexperience%2520performance%2520declines%2520in%2520long-context%2520reasoning%2520due%2520to%2520overreliance%250Aon%2520textual%2520information%2520and%2520reduced%2520visual%2520dependency.%2520In%2520this%2520study%252C%2520we%250Aempirically%2520analyze%2520LVLMs%2520in%2520long-context%2520reasoning%252C%2520revealing%2520that%2520increased%250Acontext%2520length%2520leads%2520to%2520a%2520higher%2520dependence%2520on%2520language%2520at%2520the%2520expense%2520of%250Avisual%2520dependency.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520training-free%250Acontext%2520pruning%2520method%2520that%2520selectively%2520removes%2520less%2520critical%2520textual%250Ainformation.%2520Our%2520approach%2520enhances%2520visual%2520dependency%2520and%2520reduces%2520textual%2520noise%252C%250Athereby%2520improving%2520LVLM%2520performance%2520in%2520long-context%2520reasoning.%2520We%2520validate%2520our%250Amethod%2520by%2520constructing%2520a%2520long-context%2520dataset%252C%2520demonstrating%2520its%2520effectiveness%250Aacross%2520various%2520LVLMs.%2520Moreover%252C%2520further%2520analysis%2520confirms%2520the%2520robustness%2520of%250Adifferent%2520token%2520pruning%2520strategies%2520and%2520preliminary%2520explores%2520scaling%2520laws%250Abetween%2520pruning%2520rates%2520and%2520context%2520length.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Visual%20Dependency%20in%20Long-Context%20Reasoning%20for%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Yucheng%20Zhou%20and%20Zhi%20Rao%20and%20Jun%20Wan%20and%20Jianbing%20Shen&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20excel%20in%20cross-model%20tasks%20but%0Aexperience%20performance%20declines%20in%20long-context%20reasoning%20due%20to%20overreliance%0Aon%20textual%20information%20and%20reduced%20visual%20dependency.%20In%20this%20study%2C%20we%0Aempirically%20analyze%20LVLMs%20in%20long-context%20reasoning%2C%20revealing%20that%20increased%0Acontext%20length%20leads%20to%20a%20higher%20dependence%20on%20language%20at%20the%20expense%20of%0Avisual%20dependency.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20training-free%0Acontext%20pruning%20method%20that%20selectively%20removes%20less%20critical%20textual%0Ainformation.%20Our%20approach%20enhances%20visual%20dependency%20and%20reduces%20textual%20noise%2C%0Athereby%20improving%20LVLM%20performance%20in%20long-context%20reasoning.%20We%20validate%20our%0Amethod%20by%20constructing%20a%20long-context%20dataset%2C%20demonstrating%20its%20effectiveness%0Aacross%20various%20LVLMs.%20Moreover%2C%20further%20analysis%20confirms%20the%20robustness%20of%0Adifferent%20token%20pruning%20strategies%20and%20preliminary%20explores%20scaling%20laws%0Abetween%20pruning%20rates%20and%20context%20length.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19732v1&entry.124074799=Read"},
{"title": "OneRef: Unified One-tower Expression Grounding and Segmentation with\n  Mask Referring Modeling", "author": "Linhui Xiao and Xiaoshan Yang and Fang Peng and Yaowei Wang and Changsheng Xu", "abstract": "  Constrained by the separate encoding of vision and language, existing\ngrounding and referring segmentation works heavily rely on bulky\nTransformer-based fusion en-/decoders and a variety of early-stage interaction\ntechnologies. Simultaneously, the current mask visual language modeling (MVLM)\nfails to capture the nuanced referential relationship between image-text in\nreferring tasks. In this paper, we propose OneRef, a minimalist referring\nframework built on the modality-shared one-tower transformer that unifies the\nvisual and linguistic feature spaces. To modeling the referential relationship,\nwe introduce a novel MVLM paradigm called Mask Referring Modeling (MRefM),\nwhich encompasses both referring-aware mask image modeling and referring-aware\nmask language modeling. Both modules not only reconstruct modality-related\ncontent but also cross-modal referring content. Within MRefM, we propose a\nreferring-aware dynamic image masking strategy that is aware of the referred\nregion rather than relying on fixed ratios or generic random masking schemes.\nBy leveraging the unified visual language feature space and incorporating\nMRefM's ability to model the referential relations, our approach enables direct\nregression of the referring results without resorting to various complex\ntechniques. Our method consistently surpasses existing approaches and achieves\nSoTA performance on both grounding and segmentation tasks, providing valuable\ninsights for future research. Our code and models are available at\nhttps://github.com/linhuixiao/OneRef.\n", "link": "http://arxiv.org/abs/2410.08021v2", "date": "2024-10-25", "relevancy": 2.8652, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneRef%3A%20Unified%20One-tower%20Expression%20Grounding%20and%20Segmentation%20with%0A%20%20Mask%20Referring%20Modeling&body=Title%3A%20OneRef%3A%20Unified%20One-tower%20Expression%20Grounding%20and%20Segmentation%20with%0A%20%20Mask%20Referring%20Modeling%0AAuthor%3A%20Linhui%20Xiao%20and%20Xiaoshan%20Yang%20and%20Fang%20Peng%20and%20Yaowei%20Wang%20and%20Changsheng%20Xu%0AAbstract%3A%20%20%20Constrained%20by%20the%20separate%20encoding%20of%20vision%20and%20language%2C%20existing%0Agrounding%20and%20referring%20segmentation%20works%20heavily%20rely%20on%20bulky%0ATransformer-based%20fusion%20en-/decoders%20and%20a%20variety%20of%20early-stage%20interaction%0Atechnologies.%20Simultaneously%2C%20the%20current%20mask%20visual%20language%20modeling%20%28MVLM%29%0Afails%20to%20capture%20the%20nuanced%20referential%20relationship%20between%20image-text%20in%0Areferring%20tasks.%20In%20this%20paper%2C%20we%20propose%20OneRef%2C%20a%20minimalist%20referring%0Aframework%20built%20on%20the%20modality-shared%20one-tower%20transformer%20that%20unifies%20the%0Avisual%20and%20linguistic%20feature%20spaces.%20To%20modeling%20the%20referential%20relationship%2C%0Awe%20introduce%20a%20novel%20MVLM%20paradigm%20called%20Mask%20Referring%20Modeling%20%28MRefM%29%2C%0Awhich%20encompasses%20both%20referring-aware%20mask%20image%20modeling%20and%20referring-aware%0Amask%20language%20modeling.%20Both%20modules%20not%20only%20reconstruct%20modality-related%0Acontent%20but%20also%20cross-modal%20referring%20content.%20Within%20MRefM%2C%20we%20propose%20a%0Areferring-aware%20dynamic%20image%20masking%20strategy%20that%20is%20aware%20of%20the%20referred%0Aregion%20rather%20than%20relying%20on%20fixed%20ratios%20or%20generic%20random%20masking%20schemes.%0ABy%20leveraging%20the%20unified%20visual%20language%20feature%20space%20and%20incorporating%0AMRefM%27s%20ability%20to%20model%20the%20referential%20relations%2C%20our%20approach%20enables%20direct%0Aregression%20of%20the%20referring%20results%20without%20resorting%20to%20various%20complex%0Atechniques.%20Our%20method%20consistently%20surpasses%20existing%20approaches%20and%20achieves%0ASoTA%20performance%20on%20both%20grounding%20and%20segmentation%20tasks%2C%20providing%20valuable%0Ainsights%20for%20future%20research.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/linhuixiao/OneRef.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08021v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneRef%253A%2520Unified%2520One-tower%2520Expression%2520Grounding%2520and%2520Segmentation%2520with%250A%2520%2520Mask%2520Referring%2520Modeling%26entry.906535625%3DLinhui%2520Xiao%2520and%2520Xiaoshan%2520Yang%2520and%2520Fang%2520Peng%2520and%2520Yaowei%2520Wang%2520and%2520Changsheng%2520Xu%26entry.1292438233%3D%2520%2520Constrained%2520by%2520the%2520separate%2520encoding%2520of%2520vision%2520and%2520language%252C%2520existing%250Agrounding%2520and%2520referring%2520segmentation%2520works%2520heavily%2520rely%2520on%2520bulky%250ATransformer-based%2520fusion%2520en-/decoders%2520and%2520a%2520variety%2520of%2520early-stage%2520interaction%250Atechnologies.%2520Simultaneously%252C%2520the%2520current%2520mask%2520visual%2520language%2520modeling%2520%2528MVLM%2529%250Afails%2520to%2520capture%2520the%2520nuanced%2520referential%2520relationship%2520between%2520image-text%2520in%250Areferring%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520OneRef%252C%2520a%2520minimalist%2520referring%250Aframework%2520built%2520on%2520the%2520modality-shared%2520one-tower%2520transformer%2520that%2520unifies%2520the%250Avisual%2520and%2520linguistic%2520feature%2520spaces.%2520To%2520modeling%2520the%2520referential%2520relationship%252C%250Awe%2520introduce%2520a%2520novel%2520MVLM%2520paradigm%2520called%2520Mask%2520Referring%2520Modeling%2520%2528MRefM%2529%252C%250Awhich%2520encompasses%2520both%2520referring-aware%2520mask%2520image%2520modeling%2520and%2520referring-aware%250Amask%2520language%2520modeling.%2520Both%2520modules%2520not%2520only%2520reconstruct%2520modality-related%250Acontent%2520but%2520also%2520cross-modal%2520referring%2520content.%2520Within%2520MRefM%252C%2520we%2520propose%2520a%250Areferring-aware%2520dynamic%2520image%2520masking%2520strategy%2520that%2520is%2520aware%2520of%2520the%2520referred%250Aregion%2520rather%2520than%2520relying%2520on%2520fixed%2520ratios%2520or%2520generic%2520random%2520masking%2520schemes.%250ABy%2520leveraging%2520the%2520unified%2520visual%2520language%2520feature%2520space%2520and%2520incorporating%250AMRefM%2527s%2520ability%2520to%2520model%2520the%2520referential%2520relations%252C%2520our%2520approach%2520enables%2520direct%250Aregression%2520of%2520the%2520referring%2520results%2520without%2520resorting%2520to%2520various%2520complex%250Atechniques.%2520Our%2520method%2520consistently%2520surpasses%2520existing%2520approaches%2520and%2520achieves%250ASoTA%2520performance%2520on%2520both%2520grounding%2520and%2520segmentation%2520tasks%252C%2520providing%2520valuable%250Ainsights%2520for%2520future%2520research.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/linhuixiao/OneRef.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08021v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneRef%3A%20Unified%20One-tower%20Expression%20Grounding%20and%20Segmentation%20with%0A%20%20Mask%20Referring%20Modeling&entry.906535625=Linhui%20Xiao%20and%20Xiaoshan%20Yang%20and%20Fang%20Peng%20and%20Yaowei%20Wang%20and%20Changsheng%20Xu&entry.1292438233=%20%20Constrained%20by%20the%20separate%20encoding%20of%20vision%20and%20language%2C%20existing%0Agrounding%20and%20referring%20segmentation%20works%20heavily%20rely%20on%20bulky%0ATransformer-based%20fusion%20en-/decoders%20and%20a%20variety%20of%20early-stage%20interaction%0Atechnologies.%20Simultaneously%2C%20the%20current%20mask%20visual%20language%20modeling%20%28MVLM%29%0Afails%20to%20capture%20the%20nuanced%20referential%20relationship%20between%20image-text%20in%0Areferring%20tasks.%20In%20this%20paper%2C%20we%20propose%20OneRef%2C%20a%20minimalist%20referring%0Aframework%20built%20on%20the%20modality-shared%20one-tower%20transformer%20that%20unifies%20the%0Avisual%20and%20linguistic%20feature%20spaces.%20To%20modeling%20the%20referential%20relationship%2C%0Awe%20introduce%20a%20novel%20MVLM%20paradigm%20called%20Mask%20Referring%20Modeling%20%28MRefM%29%2C%0Awhich%20encompasses%20both%20referring-aware%20mask%20image%20modeling%20and%20referring-aware%0Amask%20language%20modeling.%20Both%20modules%20not%20only%20reconstruct%20modality-related%0Acontent%20but%20also%20cross-modal%20referring%20content.%20Within%20MRefM%2C%20we%20propose%20a%0Areferring-aware%20dynamic%20image%20masking%20strategy%20that%20is%20aware%20of%20the%20referred%0Aregion%20rather%20than%20relying%20on%20fixed%20ratios%20or%20generic%20random%20masking%20schemes.%0ABy%20leveraging%20the%20unified%20visual%20language%20feature%20space%20and%20incorporating%0AMRefM%27s%20ability%20to%20model%20the%20referential%20relations%2C%20our%20approach%20enables%20direct%0Aregression%20of%20the%20referring%20results%20without%20resorting%20to%20various%20complex%0Atechniques.%20Our%20method%20consistently%20surpasses%20existing%20approaches%20and%20achieves%0ASoTA%20performance%20on%20both%20grounding%20and%20segmentation%20tasks%2C%20providing%20valuable%0Ainsights%20for%20future%20research.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/linhuixiao/OneRef.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08021v2&entry.124074799=Read"},
{"title": "Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad?", "author": "Antonia W\u00fcst and Tim Tobiasch and Lukas Helff and Devendra S. Dhami and Constantin A. Rothkopf and Kristian Kersting", "abstract": "  Recently, newly developed Vision-Language Models (VLMs), such as OpenAI's\nGPT-4o, have emerged, seemingly demonstrating advanced reasoning capabilities\nacross text and image modalities. Yet, the depth of these advances in\nlanguage-guided perception and abstract reasoning remains underexplored, and it\nis unclear whether these models can truly live up to their ambitious promises.\nTo assess the progress and identify shortcomings, we enter the wonderland of\nBongard problems, a set of classical visual reasoning puzzles that require\nhuman-like abilities of pattern recognition and abstract reasoning. While VLMs\noccasionally succeed in identifying discriminative concepts and solving some of\nthe problems, they frequently falter, failing to understand and reason about\nvisual concepts. Surprisingly, even elementary concepts that may seem trivial\nto humans, such as simple spirals, pose significant challenges. Moreover, even\nwhen asked to explicitly focus on and analyze these concepts, they continue to\nfalter, suggesting not only a lack of understanding of these elementary visual\nconcepts but also an inability to generalize to unseen concepts. These\nobservations underscore the current limitations of VLMs, emphasize that a\nsignificant gap remains between human-like visual reasoning and machine\ncognition, and highlight the ongoing need for innovation in this area.\n", "link": "http://arxiv.org/abs/2410.19546v1", "date": "2024-10-25", "relevancy": 2.8344, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5948}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5948}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bongard%20in%20Wonderland%3A%20Visual%20Puzzles%20that%20Still%20Make%20AI%20Go%20Mad%3F&body=Title%3A%20Bongard%20in%20Wonderland%3A%20Visual%20Puzzles%20that%20Still%20Make%20AI%20Go%20Mad%3F%0AAuthor%3A%20Antonia%20W%C3%BCst%20and%20Tim%20Tobiasch%20and%20Lukas%20Helff%20and%20Devendra%20S.%20Dhami%20and%20Constantin%20A.%20Rothkopf%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Recently%2C%20newly%20developed%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20OpenAI%27s%0AGPT-4o%2C%20have%20emerged%2C%20seemingly%20demonstrating%20advanced%20reasoning%20capabilities%0Aacross%20text%20and%20image%20modalities.%20Yet%2C%20the%20depth%20of%20these%20advances%20in%0Alanguage-guided%20perception%20and%20abstract%20reasoning%20remains%20underexplored%2C%20and%20it%0Ais%20unclear%20whether%20these%20models%20can%20truly%20live%20up%20to%20their%20ambitious%20promises.%0ATo%20assess%20the%20progress%20and%20identify%20shortcomings%2C%20we%20enter%20the%20wonderland%20of%0ABongard%20problems%2C%20a%20set%20of%20classical%20visual%20reasoning%20puzzles%20that%20require%0Ahuman-like%20abilities%20of%20pattern%20recognition%20and%20abstract%20reasoning.%20While%20VLMs%0Aoccasionally%20succeed%20in%20identifying%20discriminative%20concepts%20and%20solving%20some%20of%0Athe%20problems%2C%20they%20frequently%20falter%2C%20failing%20to%20understand%20and%20reason%20about%0Avisual%20concepts.%20Surprisingly%2C%20even%20elementary%20concepts%20that%20may%20seem%20trivial%0Ato%20humans%2C%20such%20as%20simple%20spirals%2C%20pose%20significant%20challenges.%20Moreover%2C%20even%0Awhen%20asked%20to%20explicitly%20focus%20on%20and%20analyze%20these%20concepts%2C%20they%20continue%20to%0Afalter%2C%20suggesting%20not%20only%20a%20lack%20of%20understanding%20of%20these%20elementary%20visual%0Aconcepts%20but%20also%20an%20inability%20to%20generalize%20to%20unseen%20concepts.%20These%0Aobservations%20underscore%20the%20current%20limitations%20of%20VLMs%2C%20emphasize%20that%20a%0Asignificant%20gap%20remains%20between%20human-like%20visual%20reasoning%20and%20machine%0Acognition%2C%20and%20highlight%20the%20ongoing%20need%20for%20innovation%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBongard%2520in%2520Wonderland%253A%2520Visual%2520Puzzles%2520that%2520Still%2520Make%2520AI%2520Go%2520Mad%253F%26entry.906535625%3DAntonia%2520W%25C3%25BCst%2520and%2520Tim%2520Tobiasch%2520and%2520Lukas%2520Helff%2520and%2520Devendra%2520S.%2520Dhami%2520and%2520Constantin%2520A.%2520Rothkopf%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Recently%252C%2520newly%2520developed%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520such%2520as%2520OpenAI%2527s%250AGPT-4o%252C%2520have%2520emerged%252C%2520seemingly%2520demonstrating%2520advanced%2520reasoning%2520capabilities%250Aacross%2520text%2520and%2520image%2520modalities.%2520Yet%252C%2520the%2520depth%2520of%2520these%2520advances%2520in%250Alanguage-guided%2520perception%2520and%2520abstract%2520reasoning%2520remains%2520underexplored%252C%2520and%2520it%250Ais%2520unclear%2520whether%2520these%2520models%2520can%2520truly%2520live%2520up%2520to%2520their%2520ambitious%2520promises.%250ATo%2520assess%2520the%2520progress%2520and%2520identify%2520shortcomings%252C%2520we%2520enter%2520the%2520wonderland%2520of%250ABongard%2520problems%252C%2520a%2520set%2520of%2520classical%2520visual%2520reasoning%2520puzzles%2520that%2520require%250Ahuman-like%2520abilities%2520of%2520pattern%2520recognition%2520and%2520abstract%2520reasoning.%2520While%2520VLMs%250Aoccasionally%2520succeed%2520in%2520identifying%2520discriminative%2520concepts%2520and%2520solving%2520some%2520of%250Athe%2520problems%252C%2520they%2520frequently%2520falter%252C%2520failing%2520to%2520understand%2520and%2520reason%2520about%250Avisual%2520concepts.%2520Surprisingly%252C%2520even%2520elementary%2520concepts%2520that%2520may%2520seem%2520trivial%250Ato%2520humans%252C%2520such%2520as%2520simple%2520spirals%252C%2520pose%2520significant%2520challenges.%2520Moreover%252C%2520even%250Awhen%2520asked%2520to%2520explicitly%2520focus%2520on%2520and%2520analyze%2520these%2520concepts%252C%2520they%2520continue%2520to%250Afalter%252C%2520suggesting%2520not%2520only%2520a%2520lack%2520of%2520understanding%2520of%2520these%2520elementary%2520visual%250Aconcepts%2520but%2520also%2520an%2520inability%2520to%2520generalize%2520to%2520unseen%2520concepts.%2520These%250Aobservations%2520underscore%2520the%2520current%2520limitations%2520of%2520VLMs%252C%2520emphasize%2520that%2520a%250Asignificant%2520gap%2520remains%2520between%2520human-like%2520visual%2520reasoning%2520and%2520machine%250Acognition%252C%2520and%2520highlight%2520the%2520ongoing%2520need%2520for%2520innovation%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bongard%20in%20Wonderland%3A%20Visual%20Puzzles%20that%20Still%20Make%20AI%20Go%20Mad%3F&entry.906535625=Antonia%20W%C3%BCst%20and%20Tim%20Tobiasch%20and%20Lukas%20Helff%20and%20Devendra%20S.%20Dhami%20and%20Constantin%20A.%20Rothkopf%20and%20Kristian%20Kersting&entry.1292438233=%20%20Recently%2C%20newly%20developed%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20OpenAI%27s%0AGPT-4o%2C%20have%20emerged%2C%20seemingly%20demonstrating%20advanced%20reasoning%20capabilities%0Aacross%20text%20and%20image%20modalities.%20Yet%2C%20the%20depth%20of%20these%20advances%20in%0Alanguage-guided%20perception%20and%20abstract%20reasoning%20remains%20underexplored%2C%20and%20it%0Ais%20unclear%20whether%20these%20models%20can%20truly%20live%20up%20to%20their%20ambitious%20promises.%0ATo%20assess%20the%20progress%20and%20identify%20shortcomings%2C%20we%20enter%20the%20wonderland%20of%0ABongard%20problems%2C%20a%20set%20of%20classical%20visual%20reasoning%20puzzles%20that%20require%0Ahuman-like%20abilities%20of%20pattern%20recognition%20and%20abstract%20reasoning.%20While%20VLMs%0Aoccasionally%20succeed%20in%20identifying%20discriminative%20concepts%20and%20solving%20some%20of%0Athe%20problems%2C%20they%20frequently%20falter%2C%20failing%20to%20understand%20and%20reason%20about%0Avisual%20concepts.%20Surprisingly%2C%20even%20elementary%20concepts%20that%20may%20seem%20trivial%0Ato%20humans%2C%20such%20as%20simple%20spirals%2C%20pose%20significant%20challenges.%20Moreover%2C%20even%0Awhen%20asked%20to%20explicitly%20focus%20on%20and%20analyze%20these%20concepts%2C%20they%20continue%20to%0Afalter%2C%20suggesting%20not%20only%20a%20lack%20of%20understanding%20of%20these%20elementary%20visual%0Aconcepts%20but%20also%20an%20inability%20to%20generalize%20to%20unseen%20concepts.%20These%0Aobservations%20underscore%20the%20current%20limitations%20of%20VLMs%2C%20emphasize%20that%20a%0Asignificant%20gap%20remains%20between%20human-like%20visual%20reasoning%20and%20machine%0Acognition%2C%20and%20highlight%20the%20ongoing%20need%20for%20innovation%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19546v1&entry.124074799=Read"},
{"title": "Inferring Neural Signed Distance Functions by Overfitting on Single\n  Noisy Point Clouds through Finetuning Data-Driven based Priors", "author": "Chao Chen and Yu-Shen Liu and Zhizhong Han", "abstract": "  It is important to estimate an accurate signed distance function (SDF) from a\npoint cloud in many computer vision applications. The latest methods learn\nneural SDFs using either a data-driven based or an overfitting-based strategy.\nHowever, these two kinds of methods are with either poor generalization or slow\nconvergence, which limits their capability under challenging scenarios like\nhighly noisy point clouds. To resolve this issue, we propose a method to\npromote pros of both data-driven based and overfitting-based methods for better\ngeneralization, faster inference, and higher accuracy in learning neural SDFs.\nWe introduce a novel statistical reasoning algorithm in local regions which is\nable to finetune data-driven based priors without signed distance supervision,\nclean point cloud, or point normals. This helps our method start with a good\ninitialization, and converge to a minimum in a much faster way. Our numerical\nand visual comparisons with the state-of-the-art methods show our superiority\nover these methods in surface reconstruction and point cloud denoising on\nwidely used shape and scene benchmarks. The code is available at\nhttps://github.com/chenchao15/LocalN2NM.\n", "link": "http://arxiv.org/abs/2410.19680v1", "date": "2024-10-25", "relevancy": 2.8234, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5871}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inferring%20Neural%20Signed%20Distance%20Functions%20by%20Overfitting%20on%20Single%0A%20%20Noisy%20Point%20Clouds%20through%20Finetuning%20Data-Driven%20based%20Priors&body=Title%3A%20Inferring%20Neural%20Signed%20Distance%20Functions%20by%20Overfitting%20on%20Single%0A%20%20Noisy%20Point%20Clouds%20through%20Finetuning%20Data-Driven%20based%20Priors%0AAuthor%3A%20Chao%20Chen%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han%0AAbstract%3A%20%20%20It%20is%20important%20to%20estimate%20an%20accurate%20signed%20distance%20function%20%28SDF%29%20from%20a%0Apoint%20cloud%20in%20many%20computer%20vision%20applications.%20The%20latest%20methods%20learn%0Aneural%20SDFs%20using%20either%20a%20data-driven%20based%20or%20an%20overfitting-based%20strategy.%0AHowever%2C%20these%20two%20kinds%20of%20methods%20are%20with%20either%20poor%20generalization%20or%20slow%0Aconvergence%2C%20which%20limits%20their%20capability%20under%20challenging%20scenarios%20like%0Ahighly%20noisy%20point%20clouds.%20To%20resolve%20this%20issue%2C%20we%20propose%20a%20method%20to%0Apromote%20pros%20of%20both%20data-driven%20based%20and%20overfitting-based%20methods%20for%20better%0Ageneralization%2C%20faster%20inference%2C%20and%20higher%20accuracy%20in%20learning%20neural%20SDFs.%0AWe%20introduce%20a%20novel%20statistical%20reasoning%20algorithm%20in%20local%20regions%20which%20is%0Aable%20to%20finetune%20data-driven%20based%20priors%20without%20signed%20distance%20supervision%2C%0Aclean%20point%20cloud%2C%20or%20point%20normals.%20This%20helps%20our%20method%20start%20with%20a%20good%0Ainitialization%2C%20and%20converge%20to%20a%20minimum%20in%20a%20much%20faster%20way.%20Our%20numerical%0Aand%20visual%20comparisons%20with%20the%20state-of-the-art%20methods%20show%20our%20superiority%0Aover%20these%20methods%20in%20surface%20reconstruction%20and%20point%20cloud%20denoising%20on%0Awidely%20used%20shape%20and%20scene%20benchmarks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/chenchao15/LocalN2NM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInferring%2520Neural%2520Signed%2520Distance%2520Functions%2520by%2520Overfitting%2520on%2520Single%250A%2520%2520Noisy%2520Point%2520Clouds%2520through%2520Finetuning%2520Data-Driven%2520based%2520Priors%26entry.906535625%3DChao%2520Chen%2520and%2520Yu-Shen%2520Liu%2520and%2520Zhizhong%2520Han%26entry.1292438233%3D%2520%2520It%2520is%2520important%2520to%2520estimate%2520an%2520accurate%2520signed%2520distance%2520function%2520%2528SDF%2529%2520from%2520a%250Apoint%2520cloud%2520in%2520many%2520computer%2520vision%2520applications.%2520The%2520latest%2520methods%2520learn%250Aneural%2520SDFs%2520using%2520either%2520a%2520data-driven%2520based%2520or%2520an%2520overfitting-based%2520strategy.%250AHowever%252C%2520these%2520two%2520kinds%2520of%2520methods%2520are%2520with%2520either%2520poor%2520generalization%2520or%2520slow%250Aconvergence%252C%2520which%2520limits%2520their%2520capability%2520under%2520challenging%2520scenarios%2520like%250Ahighly%2520noisy%2520point%2520clouds.%2520To%2520resolve%2520this%2520issue%252C%2520we%2520propose%2520a%2520method%2520to%250Apromote%2520pros%2520of%2520both%2520data-driven%2520based%2520and%2520overfitting-based%2520methods%2520for%2520better%250Ageneralization%252C%2520faster%2520inference%252C%2520and%2520higher%2520accuracy%2520in%2520learning%2520neural%2520SDFs.%250AWe%2520introduce%2520a%2520novel%2520statistical%2520reasoning%2520algorithm%2520in%2520local%2520regions%2520which%2520is%250Aable%2520to%2520finetune%2520data-driven%2520based%2520priors%2520without%2520signed%2520distance%2520supervision%252C%250Aclean%2520point%2520cloud%252C%2520or%2520point%2520normals.%2520This%2520helps%2520our%2520method%2520start%2520with%2520a%2520good%250Ainitialization%252C%2520and%2520converge%2520to%2520a%2520minimum%2520in%2520a%2520much%2520faster%2520way.%2520Our%2520numerical%250Aand%2520visual%2520comparisons%2520with%2520the%2520state-of-the-art%2520methods%2520show%2520our%2520superiority%250Aover%2520these%2520methods%2520in%2520surface%2520reconstruction%2520and%2520point%2520cloud%2520denoising%2520on%250Awidely%2520used%2520shape%2520and%2520scene%2520benchmarks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/chenchao15/LocalN2NM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inferring%20Neural%20Signed%20Distance%20Functions%20by%20Overfitting%20on%20Single%0A%20%20Noisy%20Point%20Clouds%20through%20Finetuning%20Data-Driven%20based%20Priors&entry.906535625=Chao%20Chen%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han&entry.1292438233=%20%20It%20is%20important%20to%20estimate%20an%20accurate%20signed%20distance%20function%20%28SDF%29%20from%20a%0Apoint%20cloud%20in%20many%20computer%20vision%20applications.%20The%20latest%20methods%20learn%0Aneural%20SDFs%20using%20either%20a%20data-driven%20based%20or%20an%20overfitting-based%20strategy.%0AHowever%2C%20these%20two%20kinds%20of%20methods%20are%20with%20either%20poor%20generalization%20or%20slow%0Aconvergence%2C%20which%20limits%20their%20capability%20under%20challenging%20scenarios%20like%0Ahighly%20noisy%20point%20clouds.%20To%20resolve%20this%20issue%2C%20we%20propose%20a%20method%20to%0Apromote%20pros%20of%20both%20data-driven%20based%20and%20overfitting-based%20methods%20for%20better%0Ageneralization%2C%20faster%20inference%2C%20and%20higher%20accuracy%20in%20learning%20neural%20SDFs.%0AWe%20introduce%20a%20novel%20statistical%20reasoning%20algorithm%20in%20local%20regions%20which%20is%0Aable%20to%20finetune%20data-driven%20based%20priors%20without%20signed%20distance%20supervision%2C%0Aclean%20point%20cloud%2C%20or%20point%20normals.%20This%20helps%20our%20method%20start%20with%20a%20good%0Ainitialization%2C%20and%20converge%20to%20a%20minimum%20in%20a%20much%20faster%20way.%20Our%20numerical%0Aand%20visual%20comparisons%20with%20the%20state-of-the-art%20methods%20show%20our%20superiority%0Aover%20these%20methods%20in%20surface%20reconstruction%20and%20point%20cloud%20denoising%20on%0Awidely%20used%20shape%20and%20scene%20benchmarks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/chenchao15/LocalN2NM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19680v1&entry.124074799=Read"},
{"title": "On Model-Free Re-ranking for Visual Place Recognition with Deep Learned\n  Local Features", "author": "Tom\u00e1\u0161 Pivo\u0148ka and Libor P\u0159eu\u010dil", "abstract": "  Re-ranking is the second stage of a visual place recognition task, in which\nthe system chooses the best-matching images from a pre-selected subset of\ncandidates. Model-free approaches compute the image pair similarity based on a\nspatial comparison of corresponding local visual features, eliminating the need\nfor computationally expensive estimation of a model describing transformation\nbetween images. The article focuses on model-free re-ranking based on standard\nlocal visual features and their applicability in long-term autonomy systems. It\nintroduces three new model-free re-ranking methods that were designed primarily\nfor deep-learned local visual features. These features evince high robustness\nto various appearance changes, which stands as a crucial property for use with\nlong-term autonomy systems. All the introduced methods were employed in a new\nvisual place recognition system together with the D2-net feature detector\n(Dusmanu, 2019) and experimentally tested with diverse, challenging public\ndatasets. The obtained results are on par with current state-of-the-art\nmethods, affirming that model-free approaches are a viable and worthwhile path\nfor long-term visual place recognition.\n", "link": "http://arxiv.org/abs/2410.18573v2", "date": "2024-10-25", "relevancy": 2.8193, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Model-Free%20Re-ranking%20for%20Visual%20Place%20Recognition%20with%20Deep%20Learned%0A%20%20Local%20Features&body=Title%3A%20On%20Model-Free%20Re-ranking%20for%20Visual%20Place%20Recognition%20with%20Deep%20Learned%0A%20%20Local%20Features%0AAuthor%3A%20Tom%C3%A1%C5%A1%20Pivo%C5%88ka%20and%20Libor%20P%C5%99eu%C4%8Dil%0AAbstract%3A%20%20%20Re-ranking%20is%20the%20second%20stage%20of%20a%20visual%20place%20recognition%20task%2C%20in%20which%0Athe%20system%20chooses%20the%20best-matching%20images%20from%20a%20pre-selected%20subset%20of%0Acandidates.%20Model-free%20approaches%20compute%20the%20image%20pair%20similarity%20based%20on%20a%0Aspatial%20comparison%20of%20corresponding%20local%20visual%20features%2C%20eliminating%20the%20need%0Afor%20computationally%20expensive%20estimation%20of%20a%20model%20describing%20transformation%0Abetween%20images.%20The%20article%20focuses%20on%20model-free%20re-ranking%20based%20on%20standard%0Alocal%20visual%20features%20and%20their%20applicability%20in%20long-term%20autonomy%20systems.%20It%0Aintroduces%20three%20new%20model-free%20re-ranking%20methods%20that%20were%20designed%20primarily%0Afor%20deep-learned%20local%20visual%20features.%20These%20features%20evince%20high%20robustness%0Ato%20various%20appearance%20changes%2C%20which%20stands%20as%20a%20crucial%20property%20for%20use%20with%0Along-term%20autonomy%20systems.%20All%20the%20introduced%20methods%20were%20employed%20in%20a%20new%0Avisual%20place%20recognition%20system%20together%20with%20the%20D2-net%20feature%20detector%0A%28Dusmanu%2C%202019%29%20and%20experimentally%20tested%20with%20diverse%2C%20challenging%20public%0Adatasets.%20The%20obtained%20results%20are%20on%20par%20with%20current%20state-of-the-art%0Amethods%2C%20affirming%20that%20model-free%20approaches%20are%20a%20viable%20and%20worthwhile%20path%0Afor%20long-term%20visual%20place%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18573v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Model-Free%2520Re-ranking%2520for%2520Visual%2520Place%2520Recognition%2520with%2520Deep%2520Learned%250A%2520%2520Local%2520Features%26entry.906535625%3DTom%25C3%25A1%25C5%25A1%2520Pivo%25C5%2588ka%2520and%2520Libor%2520P%25C5%2599eu%25C4%258Dil%26entry.1292438233%3D%2520%2520Re-ranking%2520is%2520the%2520second%2520stage%2520of%2520a%2520visual%2520place%2520recognition%2520task%252C%2520in%2520which%250Athe%2520system%2520chooses%2520the%2520best-matching%2520images%2520from%2520a%2520pre-selected%2520subset%2520of%250Acandidates.%2520Model-free%2520approaches%2520compute%2520the%2520image%2520pair%2520similarity%2520based%2520on%2520a%250Aspatial%2520comparison%2520of%2520corresponding%2520local%2520visual%2520features%252C%2520eliminating%2520the%2520need%250Afor%2520computationally%2520expensive%2520estimation%2520of%2520a%2520model%2520describing%2520transformation%250Abetween%2520images.%2520The%2520article%2520focuses%2520on%2520model-free%2520re-ranking%2520based%2520on%2520standard%250Alocal%2520visual%2520features%2520and%2520their%2520applicability%2520in%2520long-term%2520autonomy%2520systems.%2520It%250Aintroduces%2520three%2520new%2520model-free%2520re-ranking%2520methods%2520that%2520were%2520designed%2520primarily%250Afor%2520deep-learned%2520local%2520visual%2520features.%2520These%2520features%2520evince%2520high%2520robustness%250Ato%2520various%2520appearance%2520changes%252C%2520which%2520stands%2520as%2520a%2520crucial%2520property%2520for%2520use%2520with%250Along-term%2520autonomy%2520systems.%2520All%2520the%2520introduced%2520methods%2520were%2520employed%2520in%2520a%2520new%250Avisual%2520place%2520recognition%2520system%2520together%2520with%2520the%2520D2-net%2520feature%2520detector%250A%2528Dusmanu%252C%25202019%2529%2520and%2520experimentally%2520tested%2520with%2520diverse%252C%2520challenging%2520public%250Adatasets.%2520The%2520obtained%2520results%2520are%2520on%2520par%2520with%2520current%2520state-of-the-art%250Amethods%252C%2520affirming%2520that%2520model-free%2520approaches%2520are%2520a%2520viable%2520and%2520worthwhile%2520path%250Afor%2520long-term%2520visual%2520place%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18573v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Model-Free%20Re-ranking%20for%20Visual%20Place%20Recognition%20with%20Deep%20Learned%0A%20%20Local%20Features&entry.906535625=Tom%C3%A1%C5%A1%20Pivo%C5%88ka%20and%20Libor%20P%C5%99eu%C4%8Dil&entry.1292438233=%20%20Re-ranking%20is%20the%20second%20stage%20of%20a%20visual%20place%20recognition%20task%2C%20in%20which%0Athe%20system%20chooses%20the%20best-matching%20images%20from%20a%20pre-selected%20subset%20of%0Acandidates.%20Model-free%20approaches%20compute%20the%20image%20pair%20similarity%20based%20on%20a%0Aspatial%20comparison%20of%20corresponding%20local%20visual%20features%2C%20eliminating%20the%20need%0Afor%20computationally%20expensive%20estimation%20of%20a%20model%20describing%20transformation%0Abetween%20images.%20The%20article%20focuses%20on%20model-free%20re-ranking%20based%20on%20standard%0Alocal%20visual%20features%20and%20their%20applicability%20in%20long-term%20autonomy%20systems.%20It%0Aintroduces%20three%20new%20model-free%20re-ranking%20methods%20that%20were%20designed%20primarily%0Afor%20deep-learned%20local%20visual%20features.%20These%20features%20evince%20high%20robustness%0Ato%20various%20appearance%20changes%2C%20which%20stands%20as%20a%20crucial%20property%20for%20use%20with%0Along-term%20autonomy%20systems.%20All%20the%20introduced%20methods%20were%20employed%20in%20a%20new%0Avisual%20place%20recognition%20system%20together%20with%20the%20D2-net%20feature%20detector%0A%28Dusmanu%2C%202019%29%20and%20experimentally%20tested%20with%20diverse%2C%20challenging%20public%0Adatasets.%20The%20obtained%20results%20are%20on%20par%20with%20current%20state-of-the-art%0Amethods%2C%20affirming%20that%20model-free%20approaches%20are%20a%20viable%20and%20worthwhile%20path%0Afor%20long-term%20visual%20place%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18573v2&entry.124074799=Read"},
{"title": "Connecting Joint-Embedding Predictive Architecture with Contrastive\n  Self-supervised Learning", "author": "Shentong Mo and Shengbang Tong", "abstract": "  In recent advancements in unsupervised visual representation learning, the\nJoint-Embedding Predictive Architecture (JEPA) has emerged as a significant\nmethod for extracting visual features from unlabeled imagery through an\ninnovative masking strategy. Despite its success, two primary limitations have\nbeen identified: the inefficacy of Exponential Moving Average (EMA) from I-JEPA\nin preventing entire collapse and the inadequacy of I-JEPA prediction in\naccurately learning the mean of patch representations. Addressing these\nchallenges, this study introduces a novel framework, namely C-JEPA\n(Contrastive-JEPA), which integrates the Image-based Joint-Embedding Predictive\nArchitecture with the Variance-Invariance-Covariance Regularization (VICReg)\nstrategy. This integration is designed to effectively learn the\nvariance/covariance for preventing entire collapse and ensuring invariance in\nthe mean of augmented views, thereby overcoming the identified limitations.\nThrough empirical and theoretical evaluations, our work demonstrates that\nC-JEPA significantly enhances the stability and quality of visual\nrepresentation learning. When pre-trained on the ImageNet-1K dataset, C-JEPA\nexhibits rapid and improved convergence in both linear probing and fine-tuning\nperformance metrics.\n", "link": "http://arxiv.org/abs/2410.19560v1", "date": "2024-10-25", "relevancy": 2.8146, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5876}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5688}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connecting%20Joint-Embedding%20Predictive%20Architecture%20with%20Contrastive%0A%20%20Self-supervised%20Learning&body=Title%3A%20Connecting%20Joint-Embedding%20Predictive%20Architecture%20with%20Contrastive%0A%20%20Self-supervised%20Learning%0AAuthor%3A%20Shentong%20Mo%20and%20Shengbang%20Tong%0AAbstract%3A%20%20%20In%20recent%20advancements%20in%20unsupervised%20visual%20representation%20learning%2C%20the%0AJoint-Embedding%20Predictive%20Architecture%20%28JEPA%29%20has%20emerged%20as%20a%20significant%0Amethod%20for%20extracting%20visual%20features%20from%20unlabeled%20imagery%20through%20an%0Ainnovative%20masking%20strategy.%20Despite%20its%20success%2C%20two%20primary%20limitations%20have%0Abeen%20identified%3A%20the%20inefficacy%20of%20Exponential%20Moving%20Average%20%28EMA%29%20from%20I-JEPA%0Ain%20preventing%20entire%20collapse%20and%20the%20inadequacy%20of%20I-JEPA%20prediction%20in%0Aaccurately%20learning%20the%20mean%20of%20patch%20representations.%20Addressing%20these%0Achallenges%2C%20this%20study%20introduces%20a%20novel%20framework%2C%20namely%20C-JEPA%0A%28Contrastive-JEPA%29%2C%20which%20integrates%20the%20Image-based%20Joint-Embedding%20Predictive%0AArchitecture%20with%20the%20Variance-Invariance-Covariance%20Regularization%20%28VICReg%29%0Astrategy.%20This%20integration%20is%20designed%20to%20effectively%20learn%20the%0Avariance/covariance%20for%20preventing%20entire%20collapse%20and%20ensuring%20invariance%20in%0Athe%20mean%20of%20augmented%20views%2C%20thereby%20overcoming%20the%20identified%20limitations.%0AThrough%20empirical%20and%20theoretical%20evaluations%2C%20our%20work%20demonstrates%20that%0AC-JEPA%20significantly%20enhances%20the%20stability%20and%20quality%20of%20visual%0Arepresentation%20learning.%20When%20pre-trained%20on%20the%20ImageNet-1K%20dataset%2C%20C-JEPA%0Aexhibits%20rapid%20and%20improved%20convergence%20in%20both%20linear%20probing%20and%20fine-tuning%0Aperformance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnecting%2520Joint-Embedding%2520Predictive%2520Architecture%2520with%2520Contrastive%250A%2520%2520Self-supervised%2520Learning%26entry.906535625%3DShentong%2520Mo%2520and%2520Shengbang%2520Tong%26entry.1292438233%3D%2520%2520In%2520recent%2520advancements%2520in%2520unsupervised%2520visual%2520representation%2520learning%252C%2520the%250AJoint-Embedding%2520Predictive%2520Architecture%2520%2528JEPA%2529%2520has%2520emerged%2520as%2520a%2520significant%250Amethod%2520for%2520extracting%2520visual%2520features%2520from%2520unlabeled%2520imagery%2520through%2520an%250Ainnovative%2520masking%2520strategy.%2520Despite%2520its%2520success%252C%2520two%2520primary%2520limitations%2520have%250Abeen%2520identified%253A%2520the%2520inefficacy%2520of%2520Exponential%2520Moving%2520Average%2520%2528EMA%2529%2520from%2520I-JEPA%250Ain%2520preventing%2520entire%2520collapse%2520and%2520the%2520inadequacy%2520of%2520I-JEPA%2520prediction%2520in%250Aaccurately%2520learning%2520the%2520mean%2520of%2520patch%2520representations.%2520Addressing%2520these%250Achallenges%252C%2520this%2520study%2520introduces%2520a%2520novel%2520framework%252C%2520namely%2520C-JEPA%250A%2528Contrastive-JEPA%2529%252C%2520which%2520integrates%2520the%2520Image-based%2520Joint-Embedding%2520Predictive%250AArchitecture%2520with%2520the%2520Variance-Invariance-Covariance%2520Regularization%2520%2528VICReg%2529%250Astrategy.%2520This%2520integration%2520is%2520designed%2520to%2520effectively%2520learn%2520the%250Avariance/covariance%2520for%2520preventing%2520entire%2520collapse%2520and%2520ensuring%2520invariance%2520in%250Athe%2520mean%2520of%2520augmented%2520views%252C%2520thereby%2520overcoming%2520the%2520identified%2520limitations.%250AThrough%2520empirical%2520and%2520theoretical%2520evaluations%252C%2520our%2520work%2520demonstrates%2520that%250AC-JEPA%2520significantly%2520enhances%2520the%2520stability%2520and%2520quality%2520of%2520visual%250Arepresentation%2520learning.%2520When%2520pre-trained%2520on%2520the%2520ImageNet-1K%2520dataset%252C%2520C-JEPA%250Aexhibits%2520rapid%2520and%2520improved%2520convergence%2520in%2520both%2520linear%2520probing%2520and%2520fine-tuning%250Aperformance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connecting%20Joint-Embedding%20Predictive%20Architecture%20with%20Contrastive%0A%20%20Self-supervised%20Learning&entry.906535625=Shentong%20Mo%20and%20Shengbang%20Tong&entry.1292438233=%20%20In%20recent%20advancements%20in%20unsupervised%20visual%20representation%20learning%2C%20the%0AJoint-Embedding%20Predictive%20Architecture%20%28JEPA%29%20has%20emerged%20as%20a%20significant%0Amethod%20for%20extracting%20visual%20features%20from%20unlabeled%20imagery%20through%20an%0Ainnovative%20masking%20strategy.%20Despite%20its%20success%2C%20two%20primary%20limitations%20have%0Abeen%20identified%3A%20the%20inefficacy%20of%20Exponential%20Moving%20Average%20%28EMA%29%20from%20I-JEPA%0Ain%20preventing%20entire%20collapse%20and%20the%20inadequacy%20of%20I-JEPA%20prediction%20in%0Aaccurately%20learning%20the%20mean%20of%20patch%20representations.%20Addressing%20these%0Achallenges%2C%20this%20study%20introduces%20a%20novel%20framework%2C%20namely%20C-JEPA%0A%28Contrastive-JEPA%29%2C%20which%20integrates%20the%20Image-based%20Joint-Embedding%20Predictive%0AArchitecture%20with%20the%20Variance-Invariance-Covariance%20Regularization%20%28VICReg%29%0Astrategy.%20This%20integration%20is%20designed%20to%20effectively%20learn%20the%0Avariance/covariance%20for%20preventing%20entire%20collapse%20and%20ensuring%20invariance%20in%0Athe%20mean%20of%20augmented%20views%2C%20thereby%20overcoming%20the%20identified%20limitations.%0AThrough%20empirical%20and%20theoretical%20evaluations%2C%20our%20work%20demonstrates%20that%0AC-JEPA%20significantly%20enhances%20the%20stability%20and%20quality%20of%20visual%0Arepresentation%20learning.%20When%20pre-trained%20on%20the%20ImageNet-1K%20dataset%2C%20C-JEPA%0Aexhibits%20rapid%20and%20improved%20convergence%20in%20both%20linear%20probing%20and%20fine-tuning%0Aperformance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19560v1&entry.124074799=Read"},
{"title": "Diverse Sign Language Translation", "author": "Xin Shen and Lei Shen and Shaozu Yuan and Heming Du and Haiyang Sun and Xin Yu", "abstract": "  Like spoken languages, a single sign language expression could correspond to\nmultiple valid textual interpretations. Hence, learning a rigid one-to-one\nmapping for sign language translation (SLT) models might be inadequate,\nparticularly in the case of limited data. In this work, we introduce a Diverse\nSign Language Translation (DivSLT) task, aiming to generate diverse yet\naccurate translations for sign language videos. Firstly, we employ large\nlanguage models (LLM) to generate multiple references for the widely-used\nCSL-Daily and PHOENIX14T SLT datasets. Here, native speakers are only invited\nto touch up inaccurate references, thus significantly improving the annotation\nefficiency. Secondly, we provide a benchmark model to spur research in this\ntask. Specifically, we investigate multi-reference training strategies to\nenable our DivSLT model to achieve diverse translations. Then, to enhance\ntranslation accuracy, we employ the max-reward-driven reinforcement learning\nobjective that maximizes the reward of the translated result. Additionally, we\nutilize multiple metrics to assess the accuracy, diversity, and semantic\nprecision of the DivSLT task. Experimental results on the enriched datasets\ndemonstrate that our DivSLT method achieves not only better translation\nperformance but also diverse translation results.\n", "link": "http://arxiv.org/abs/2410.19586v1", "date": "2024-10-25", "relevancy": 2.7287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diverse%20Sign%20Language%20Translation&body=Title%3A%20Diverse%20Sign%20Language%20Translation%0AAuthor%3A%20Xin%20Shen%20and%20Lei%20Shen%20and%20Shaozu%20Yuan%20and%20Heming%20Du%20and%20Haiyang%20Sun%20and%20Xin%20Yu%0AAbstract%3A%20%20%20Like%20spoken%20languages%2C%20a%20single%20sign%20language%20expression%20could%20correspond%20to%0Amultiple%20valid%20textual%20interpretations.%20Hence%2C%20learning%20a%20rigid%20one-to-one%0Amapping%20for%20sign%20language%20translation%20%28SLT%29%20models%20might%20be%20inadequate%2C%0Aparticularly%20in%20the%20case%20of%20limited%20data.%20In%20this%20work%2C%20we%20introduce%20a%20Diverse%0ASign%20Language%20Translation%20%28DivSLT%29%20task%2C%20aiming%20to%20generate%20diverse%20yet%0Aaccurate%20translations%20for%20sign%20language%20videos.%20Firstly%2C%20we%20employ%20large%0Alanguage%20models%20%28LLM%29%20to%20generate%20multiple%20references%20for%20the%20widely-used%0ACSL-Daily%20and%20PHOENIX14T%20SLT%20datasets.%20Here%2C%20native%20speakers%20are%20only%20invited%0Ato%20touch%20up%20inaccurate%20references%2C%20thus%20significantly%20improving%20the%20annotation%0Aefficiency.%20Secondly%2C%20we%20provide%20a%20benchmark%20model%20to%20spur%20research%20in%20this%0Atask.%20Specifically%2C%20we%20investigate%20multi-reference%20training%20strategies%20to%0Aenable%20our%20DivSLT%20model%20to%20achieve%20diverse%20translations.%20Then%2C%20to%20enhance%0Atranslation%20accuracy%2C%20we%20employ%20the%20max-reward-driven%20reinforcement%20learning%0Aobjective%20that%20maximizes%20the%20reward%20of%20the%20translated%20result.%20Additionally%2C%20we%0Autilize%20multiple%20metrics%20to%20assess%20the%20accuracy%2C%20diversity%2C%20and%20semantic%0Aprecision%20of%20the%20DivSLT%20task.%20Experimental%20results%20on%20the%20enriched%20datasets%0Ademonstrate%20that%20our%20DivSLT%20method%20achieves%20not%20only%20better%20translation%0Aperformance%20but%20also%20diverse%20translation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverse%2520Sign%2520Language%2520Translation%26entry.906535625%3DXin%2520Shen%2520and%2520Lei%2520Shen%2520and%2520Shaozu%2520Yuan%2520and%2520Heming%2520Du%2520and%2520Haiyang%2520Sun%2520and%2520Xin%2520Yu%26entry.1292438233%3D%2520%2520Like%2520spoken%2520languages%252C%2520a%2520single%2520sign%2520language%2520expression%2520could%2520correspond%2520to%250Amultiple%2520valid%2520textual%2520interpretations.%2520Hence%252C%2520learning%2520a%2520rigid%2520one-to-one%250Amapping%2520for%2520sign%2520language%2520translation%2520%2528SLT%2529%2520models%2520might%2520be%2520inadequate%252C%250Aparticularly%2520in%2520the%2520case%2520of%2520limited%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520Diverse%250ASign%2520Language%2520Translation%2520%2528DivSLT%2529%2520task%252C%2520aiming%2520to%2520generate%2520diverse%2520yet%250Aaccurate%2520translations%2520for%2520sign%2520language%2520videos.%2520Firstly%252C%2520we%2520employ%2520large%250Alanguage%2520models%2520%2528LLM%2529%2520to%2520generate%2520multiple%2520references%2520for%2520the%2520widely-used%250ACSL-Daily%2520and%2520PHOENIX14T%2520SLT%2520datasets.%2520Here%252C%2520native%2520speakers%2520are%2520only%2520invited%250Ato%2520touch%2520up%2520inaccurate%2520references%252C%2520thus%2520significantly%2520improving%2520the%2520annotation%250Aefficiency.%2520Secondly%252C%2520we%2520provide%2520a%2520benchmark%2520model%2520to%2520spur%2520research%2520in%2520this%250Atask.%2520Specifically%252C%2520we%2520investigate%2520multi-reference%2520training%2520strategies%2520to%250Aenable%2520our%2520DivSLT%2520model%2520to%2520achieve%2520diverse%2520translations.%2520Then%252C%2520to%2520enhance%250Atranslation%2520accuracy%252C%2520we%2520employ%2520the%2520max-reward-driven%2520reinforcement%2520learning%250Aobjective%2520that%2520maximizes%2520the%2520reward%2520of%2520the%2520translated%2520result.%2520Additionally%252C%2520we%250Autilize%2520multiple%2520metrics%2520to%2520assess%2520the%2520accuracy%252C%2520diversity%252C%2520and%2520semantic%250Aprecision%2520of%2520the%2520DivSLT%2520task.%2520Experimental%2520results%2520on%2520the%2520enriched%2520datasets%250Ademonstrate%2520that%2520our%2520DivSLT%2520method%2520achieves%2520not%2520only%2520better%2520translation%250Aperformance%2520but%2520also%2520diverse%2520translation%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diverse%20Sign%20Language%20Translation&entry.906535625=Xin%20Shen%20and%20Lei%20Shen%20and%20Shaozu%20Yuan%20and%20Heming%20Du%20and%20Haiyang%20Sun%20and%20Xin%20Yu&entry.1292438233=%20%20Like%20spoken%20languages%2C%20a%20single%20sign%20language%20expression%20could%20correspond%20to%0Amultiple%20valid%20textual%20interpretations.%20Hence%2C%20learning%20a%20rigid%20one-to-one%0Amapping%20for%20sign%20language%20translation%20%28SLT%29%20models%20might%20be%20inadequate%2C%0Aparticularly%20in%20the%20case%20of%20limited%20data.%20In%20this%20work%2C%20we%20introduce%20a%20Diverse%0ASign%20Language%20Translation%20%28DivSLT%29%20task%2C%20aiming%20to%20generate%20diverse%20yet%0Aaccurate%20translations%20for%20sign%20language%20videos.%20Firstly%2C%20we%20employ%20large%0Alanguage%20models%20%28LLM%29%20to%20generate%20multiple%20references%20for%20the%20widely-used%0ACSL-Daily%20and%20PHOENIX14T%20SLT%20datasets.%20Here%2C%20native%20speakers%20are%20only%20invited%0Ato%20touch%20up%20inaccurate%20references%2C%20thus%20significantly%20improving%20the%20annotation%0Aefficiency.%20Secondly%2C%20we%20provide%20a%20benchmark%20model%20to%20spur%20research%20in%20this%0Atask.%20Specifically%2C%20we%20investigate%20multi-reference%20training%20strategies%20to%0Aenable%20our%20DivSLT%20model%20to%20achieve%20diverse%20translations.%20Then%2C%20to%20enhance%0Atranslation%20accuracy%2C%20we%20employ%20the%20max-reward-driven%20reinforcement%20learning%0Aobjective%20that%20maximizes%20the%20reward%20of%20the%20translated%20result.%20Additionally%2C%20we%0Autilize%20multiple%20metrics%20to%20assess%20the%20accuracy%2C%20diversity%2C%20and%20semantic%0Aprecision%20of%20the%20DivSLT%20task.%20Experimental%20results%20on%20the%20enriched%20datasets%0Ademonstrate%20that%20our%20DivSLT%20method%20achieves%20not%20only%20better%20translation%0Aperformance%20but%20also%20diverse%20translation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19586v1&entry.124074799=Read"},
{"title": "MotionCraft: Physics-based Zero-Shot Video Generation", "author": "Luca Savant Aira and Antonio Montanaro and Emanuele Aiello and Diego Valsesia and Enrico Magli", "abstract": "  Generating videos with realistic and physically plausible motion is one of\nthe main recent challenges in computer vision. While diffusion models are\nachieving compelling results in image generation, video diffusion models are\nlimited by heavy training and huge models, resulting in videos that are still\nbiased to the training dataset. In this work we propose MotionCraft, a new\nzero-shot video generator to craft physics-based and realistic videos.\nMotionCraft is able to warp the noise latent space of an image diffusion model,\nsuch as Stable Diffusion, by applying an optical flow derived from a physics\nsimulation. We show that warping the noise latent space results in coherent\napplication of the desired motion while allowing the model to generate missing\nelements consistent with the scene evolution, which would otherwise result in\nartefacts or missing content if the flow was applied in the pixel space. We\ncompare our method with the state-of-the-art Text2Video-Zero reporting\nqualitative and quantitative improvements, demonstrating the effectiveness of\nour approach to generate videos with finely-prescribed complex motion dynamics.\nProject page: https://mezzelfo.github.io/MotionCraft/\n", "link": "http://arxiv.org/abs/2405.13557v2", "date": "2024-10-25", "relevancy": 2.7129, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7096}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6873}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionCraft%3A%20Physics-based%20Zero-Shot%20Video%20Generation&body=Title%3A%20MotionCraft%3A%20Physics-based%20Zero-Shot%20Video%20Generation%0AAuthor%3A%20Luca%20Savant%20Aira%20and%20Antonio%20Montanaro%20and%20Emanuele%20Aiello%20and%20Diego%20Valsesia%20and%20Enrico%20Magli%0AAbstract%3A%20%20%20Generating%20videos%20with%20realistic%20and%20physically%20plausible%20motion%20is%20one%20of%0Athe%20main%20recent%20challenges%20in%20computer%20vision.%20While%20diffusion%20models%20are%0Aachieving%20compelling%20results%20in%20image%20generation%2C%20video%20diffusion%20models%20are%0Alimited%20by%20heavy%20training%20and%20huge%20models%2C%20resulting%20in%20videos%20that%20are%20still%0Abiased%20to%20the%20training%20dataset.%20In%20this%20work%20we%20propose%20MotionCraft%2C%20a%20new%0Azero-shot%20video%20generator%20to%20craft%20physics-based%20and%20realistic%20videos.%0AMotionCraft%20is%20able%20to%20warp%20the%20noise%20latent%20space%20of%20an%20image%20diffusion%20model%2C%0Asuch%20as%20Stable%20Diffusion%2C%20by%20applying%20an%20optical%20flow%20derived%20from%20a%20physics%0Asimulation.%20We%20show%20that%20warping%20the%20noise%20latent%20space%20results%20in%20coherent%0Aapplication%20of%20the%20desired%20motion%20while%20allowing%20the%20model%20to%20generate%20missing%0Aelements%20consistent%20with%20the%20scene%20evolution%2C%20which%20would%20otherwise%20result%20in%0Aartefacts%20or%20missing%20content%20if%20the%20flow%20was%20applied%20in%20the%20pixel%20space.%20We%0Acompare%20our%20method%20with%20the%20state-of-the-art%20Text2Video-Zero%20reporting%0Aqualitative%20and%20quantitative%20improvements%2C%20demonstrating%20the%20effectiveness%20of%0Aour%20approach%20to%20generate%20videos%20with%20finely-prescribed%20complex%20motion%20dynamics.%0AProject%20page%3A%20https%3A//mezzelfo.github.io/MotionCraft/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionCraft%253A%2520Physics-based%2520Zero-Shot%2520Video%2520Generation%26entry.906535625%3DLuca%2520Savant%2520Aira%2520and%2520Antonio%2520Montanaro%2520and%2520Emanuele%2520Aiello%2520and%2520Diego%2520Valsesia%2520and%2520Enrico%2520Magli%26entry.1292438233%3D%2520%2520Generating%2520videos%2520with%2520realistic%2520and%2520physically%2520plausible%2520motion%2520is%2520one%2520of%250Athe%2520main%2520recent%2520challenges%2520in%2520computer%2520vision.%2520While%2520diffusion%2520models%2520are%250Aachieving%2520compelling%2520results%2520in%2520image%2520generation%252C%2520video%2520diffusion%2520models%2520are%250Alimited%2520by%2520heavy%2520training%2520and%2520huge%2520models%252C%2520resulting%2520in%2520videos%2520that%2520are%2520still%250Abiased%2520to%2520the%2520training%2520dataset.%2520In%2520this%2520work%2520we%2520propose%2520MotionCraft%252C%2520a%2520new%250Azero-shot%2520video%2520generator%2520to%2520craft%2520physics-based%2520and%2520realistic%2520videos.%250AMotionCraft%2520is%2520able%2520to%2520warp%2520the%2520noise%2520latent%2520space%2520of%2520an%2520image%2520diffusion%2520model%252C%250Asuch%2520as%2520Stable%2520Diffusion%252C%2520by%2520applying%2520an%2520optical%2520flow%2520derived%2520from%2520a%2520physics%250Asimulation.%2520We%2520show%2520that%2520warping%2520the%2520noise%2520latent%2520space%2520results%2520in%2520coherent%250Aapplication%2520of%2520the%2520desired%2520motion%2520while%2520allowing%2520the%2520model%2520to%2520generate%2520missing%250Aelements%2520consistent%2520with%2520the%2520scene%2520evolution%252C%2520which%2520would%2520otherwise%2520result%2520in%250Aartefacts%2520or%2520missing%2520content%2520if%2520the%2520flow%2520was%2520applied%2520in%2520the%2520pixel%2520space.%2520We%250Acompare%2520our%2520method%2520with%2520the%2520state-of-the-art%2520Text2Video-Zero%2520reporting%250Aqualitative%2520and%2520quantitative%2520improvements%252C%2520demonstrating%2520the%2520effectiveness%2520of%250Aour%2520approach%2520to%2520generate%2520videos%2520with%2520finely-prescribed%2520complex%2520motion%2520dynamics.%250AProject%2520page%253A%2520https%253A//mezzelfo.github.io/MotionCraft/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionCraft%3A%20Physics-based%20Zero-Shot%20Video%20Generation&entry.906535625=Luca%20Savant%20Aira%20and%20Antonio%20Montanaro%20and%20Emanuele%20Aiello%20and%20Diego%20Valsesia%20and%20Enrico%20Magli&entry.1292438233=%20%20Generating%20videos%20with%20realistic%20and%20physically%20plausible%20motion%20is%20one%20of%0Athe%20main%20recent%20challenges%20in%20computer%20vision.%20While%20diffusion%20models%20are%0Aachieving%20compelling%20results%20in%20image%20generation%2C%20video%20diffusion%20models%20are%0Alimited%20by%20heavy%20training%20and%20huge%20models%2C%20resulting%20in%20videos%20that%20are%20still%0Abiased%20to%20the%20training%20dataset.%20In%20this%20work%20we%20propose%20MotionCraft%2C%20a%20new%0Azero-shot%20video%20generator%20to%20craft%20physics-based%20and%20realistic%20videos.%0AMotionCraft%20is%20able%20to%20warp%20the%20noise%20latent%20space%20of%20an%20image%20diffusion%20model%2C%0Asuch%20as%20Stable%20Diffusion%2C%20by%20applying%20an%20optical%20flow%20derived%20from%20a%20physics%0Asimulation.%20We%20show%20that%20warping%20the%20noise%20latent%20space%20results%20in%20coherent%0Aapplication%20of%20the%20desired%20motion%20while%20allowing%20the%20model%20to%20generate%20missing%0Aelements%20consistent%20with%20the%20scene%20evolution%2C%20which%20would%20otherwise%20result%20in%0Aartefacts%20or%20missing%20content%20if%20the%20flow%20was%20applied%20in%20the%20pixel%20space.%20We%0Acompare%20our%20method%20with%20the%20state-of-the-art%20Text2Video-Zero%20reporting%0Aqualitative%20and%20quantitative%20improvements%2C%20demonstrating%20the%20effectiveness%20of%0Aour%20approach%20to%20generate%20videos%20with%20finely-prescribed%20complex%20motion%20dynamics.%0AProject%20page%3A%20https%3A//mezzelfo.github.io/MotionCraft/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13557v2&entry.124074799=Read"},
{"title": "T\u0159iVis: Versatile, Reliable, and High-Performance Tool for Computing\n  Visibility in Polygonal Environments", "author": "Jan Mikula and Miroslav Kulich and Libor P\u0159eu\u010dil", "abstract": "  Visibility is a fundamental concept in computational geometry, with numerous\napplications in surveillance, robotics, and games. This software paper presents\nT\\v{r}iVis, a C++ library developed by the authors for computing numerous\nvisibility-related queries in highly complex polygonal environments. Adapting\nthe triangular expansion algorithm, T\\v{r}iVis stands out as a versatile,\nhigh-performance, more reliable and easy-to-use alternative to current\nsolutions that is also free of heavy dependencies. Through evaluation on a\nchallenging dataset, T\\v{r}iVis has been benchmarked against existing\nvisibility libraries. The results demonstrate that T\\v{r}iVis outperforms the\ncompeting solutions by at least an order of magnitude in query times, while\nexhibiting more reliable runtime behavior. T\\v{r}iVis is freely available for\nprivate, research, and institutional use at\nhttps://github.com/janmikulacz/trivis.\n", "link": "http://arxiv.org/abs/2410.08752v2", "date": "2024-10-25", "relevancy": 2.6987, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5595}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5595}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T%C5%99iVis%3A%20Versatile%2C%20Reliable%2C%20and%20High-Performance%20Tool%20for%20Computing%0A%20%20Visibility%20in%20Polygonal%20Environments&body=Title%3A%20T%C5%99iVis%3A%20Versatile%2C%20Reliable%2C%20and%20High-Performance%20Tool%20for%20Computing%0A%20%20Visibility%20in%20Polygonal%20Environments%0AAuthor%3A%20Jan%20Mikula%20and%20Miroslav%20Kulich%20and%20Libor%20P%C5%99eu%C4%8Dil%0AAbstract%3A%20%20%20Visibility%20is%20a%20fundamental%20concept%20in%20computational%20geometry%2C%20with%20numerous%0Aapplications%20in%20surveillance%2C%20robotics%2C%20and%20games.%20This%20software%20paper%20presents%0AT%5Cv%7Br%7DiVis%2C%20a%20C%2B%2B%20library%20developed%20by%20the%20authors%20for%20computing%20numerous%0Avisibility-related%20queries%20in%20highly%20complex%20polygonal%20environments.%20Adapting%0Athe%20triangular%20expansion%20algorithm%2C%20T%5Cv%7Br%7DiVis%20stands%20out%20as%20a%20versatile%2C%0Ahigh-performance%2C%20more%20reliable%20and%20easy-to-use%20alternative%20to%20current%0Asolutions%20that%20is%20also%20free%20of%20heavy%20dependencies.%20Through%20evaluation%20on%20a%0Achallenging%20dataset%2C%20T%5Cv%7Br%7DiVis%20has%20been%20benchmarked%20against%20existing%0Avisibility%20libraries.%20The%20results%20demonstrate%20that%20T%5Cv%7Br%7DiVis%20outperforms%20the%0Acompeting%20solutions%20by%20at%20least%20an%20order%20of%20magnitude%20in%20query%20times%2C%20while%0Aexhibiting%20more%20reliable%20runtime%20behavior.%20T%5Cv%7Br%7DiVis%20is%20freely%20available%20for%0Aprivate%2C%20research%2C%20and%20institutional%20use%20at%0Ahttps%3A//github.com/janmikulacz/trivis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08752v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT%25C5%2599iVis%253A%2520Versatile%252C%2520Reliable%252C%2520and%2520High-Performance%2520Tool%2520for%2520Computing%250A%2520%2520Visibility%2520in%2520Polygonal%2520Environments%26entry.906535625%3DJan%2520Mikula%2520and%2520Miroslav%2520Kulich%2520and%2520Libor%2520P%25C5%2599eu%25C4%258Dil%26entry.1292438233%3D%2520%2520Visibility%2520is%2520a%2520fundamental%2520concept%2520in%2520computational%2520geometry%252C%2520with%2520numerous%250Aapplications%2520in%2520surveillance%252C%2520robotics%252C%2520and%2520games.%2520This%2520software%2520paper%2520presents%250AT%255Cv%257Br%257DiVis%252C%2520a%2520C%252B%252B%2520library%2520developed%2520by%2520the%2520authors%2520for%2520computing%2520numerous%250Avisibility-related%2520queries%2520in%2520highly%2520complex%2520polygonal%2520environments.%2520Adapting%250Athe%2520triangular%2520expansion%2520algorithm%252C%2520T%255Cv%257Br%257DiVis%2520stands%2520out%2520as%2520a%2520versatile%252C%250Ahigh-performance%252C%2520more%2520reliable%2520and%2520easy-to-use%2520alternative%2520to%2520current%250Asolutions%2520that%2520is%2520also%2520free%2520of%2520heavy%2520dependencies.%2520Through%2520evaluation%2520on%2520a%250Achallenging%2520dataset%252C%2520T%255Cv%257Br%257DiVis%2520has%2520been%2520benchmarked%2520against%2520existing%250Avisibility%2520libraries.%2520The%2520results%2520demonstrate%2520that%2520T%255Cv%257Br%257DiVis%2520outperforms%2520the%250Acompeting%2520solutions%2520by%2520at%2520least%2520an%2520order%2520of%2520magnitude%2520in%2520query%2520times%252C%2520while%250Aexhibiting%2520more%2520reliable%2520runtime%2520behavior.%2520T%255Cv%257Br%257DiVis%2520is%2520freely%2520available%2520for%250Aprivate%252C%2520research%252C%2520and%2520institutional%2520use%2520at%250Ahttps%253A//github.com/janmikulacz/trivis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08752v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T%C5%99iVis%3A%20Versatile%2C%20Reliable%2C%20and%20High-Performance%20Tool%20for%20Computing%0A%20%20Visibility%20in%20Polygonal%20Environments&entry.906535625=Jan%20Mikula%20and%20Miroslav%20Kulich%20and%20Libor%20P%C5%99eu%C4%8Dil&entry.1292438233=%20%20Visibility%20is%20a%20fundamental%20concept%20in%20computational%20geometry%2C%20with%20numerous%0Aapplications%20in%20surveillance%2C%20robotics%2C%20and%20games.%20This%20software%20paper%20presents%0AT%5Cv%7Br%7DiVis%2C%20a%20C%2B%2B%20library%20developed%20by%20the%20authors%20for%20computing%20numerous%0Avisibility-related%20queries%20in%20highly%20complex%20polygonal%20environments.%20Adapting%0Athe%20triangular%20expansion%20algorithm%2C%20T%5Cv%7Br%7DiVis%20stands%20out%20as%20a%20versatile%2C%0Ahigh-performance%2C%20more%20reliable%20and%20easy-to-use%20alternative%20to%20current%0Asolutions%20that%20is%20also%20free%20of%20heavy%20dependencies.%20Through%20evaluation%20on%20a%0Achallenging%20dataset%2C%20T%5Cv%7Br%7DiVis%20has%20been%20benchmarked%20against%20existing%0Avisibility%20libraries.%20The%20results%20demonstrate%20that%20T%5Cv%7Br%7DiVis%20outperforms%20the%0Acompeting%20solutions%20by%20at%20least%20an%20order%20of%20magnitude%20in%20query%20times%2C%20while%0Aexhibiting%20more%20reliable%20runtime%20behavior.%20T%5Cv%7Br%7DiVis%20is%20freely%20available%20for%0Aprivate%2C%20research%2C%20and%20institutional%20use%20at%0Ahttps%3A//github.com/janmikulacz/trivis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08752v2&entry.124074799=Read"},
{"title": "Automatically Generating UI Code from Screenshot: A\n  Divide-and-Conquer-Based Approach", "author": "Yuxuan Wan and Chaozheng Wang and Yi Dong and Wenxuan Wang and Shuqing Li and Yintong Huo and Michael R. Lyu", "abstract": "  Websites are critical in today's digital world, with over 1.11 billion\ncurrently active and approximately 252,000 new sites launched daily. Converting\nwebsite layout design into functional UI code is a time-consuming yet\nindispensable step of website development. Manual methods of converting visual\ndesigns into functional code present significant challenges, especially for\nnon-experts. To explore automatic design-to-code solutions, we first conduct a\nmotivating study on GPT-4o and identify three types of issues in generating UI\ncode: element omission, element distortion, and element misarrangement. We\nfurther reveal that a focus on smaller visual segments can help multimodal\nlarge language models (MLLMs) mitigate these failures in the generation\nprocess. In this paper, we propose DCGen, a divide-and-conquer-based approach\nto automate the translation of webpage design to UI code. DCGen starts by\ndividing screenshots into manageable segments, generating descriptions for each\nsegment, and then reassembling them into complete UI code for the entire\nscreenshot. We conduct extensive testing with a dataset comprised of real-world\nwebsites and various MLLMs and demonstrate that DCGen achieves up to a 14%\nimprovement in visual similarity over competing methods. To the best of our\nknowledge, DCGen is the first segment-aware prompt-based approach for\ngenerating UI code directly from screenshots.\n", "link": "http://arxiv.org/abs/2406.16386v2", "date": "2024-10-25", "relevancy": 2.6862, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.589}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5166}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatically%20Generating%20UI%20Code%20from%20Screenshot%3A%20A%0A%20%20Divide-and-Conquer-Based%20Approach&body=Title%3A%20Automatically%20Generating%20UI%20Code%20from%20Screenshot%3A%20A%0A%20%20Divide-and-Conquer-Based%20Approach%0AAuthor%3A%20Yuxuan%20Wan%20and%20Chaozheng%20Wang%20and%20Yi%20Dong%20and%20Wenxuan%20Wang%20and%20Shuqing%20Li%20and%20Yintong%20Huo%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Websites%20are%20critical%20in%20today%27s%20digital%20world%2C%20with%20over%201.11%20billion%0Acurrently%20active%20and%20approximately%20252%2C000%20new%20sites%20launched%20daily.%20Converting%0Awebsite%20layout%20design%20into%20functional%20UI%20code%20is%20a%20time-consuming%20yet%0Aindispensable%20step%20of%20website%20development.%20Manual%20methods%20of%20converting%20visual%0Adesigns%20into%20functional%20code%20present%20significant%20challenges%2C%20especially%20for%0Anon-experts.%20To%20explore%20automatic%20design-to-code%20solutions%2C%20we%20first%20conduct%20a%0Amotivating%20study%20on%20GPT-4o%20and%20identify%20three%20types%20of%20issues%20in%20generating%20UI%0Acode%3A%20element%20omission%2C%20element%20distortion%2C%20and%20element%20misarrangement.%20We%0Afurther%20reveal%20that%20a%20focus%20on%20smaller%20visual%20segments%20can%20help%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20mitigate%20these%20failures%20in%20the%20generation%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20DCGen%2C%20a%20divide-and-conquer-based%20approach%0Ato%20automate%20the%20translation%20of%20webpage%20design%20to%20UI%20code.%20DCGen%20starts%20by%0Adividing%20screenshots%20into%20manageable%20segments%2C%20generating%20descriptions%20for%20each%0Asegment%2C%20and%20then%20reassembling%20them%20into%20complete%20UI%20code%20for%20the%20entire%0Ascreenshot.%20We%20conduct%20extensive%20testing%20with%20a%20dataset%20comprised%20of%20real-world%0Awebsites%20and%20various%20MLLMs%20and%20demonstrate%20that%20DCGen%20achieves%20up%20to%20a%2014%25%0Aimprovement%20in%20visual%20similarity%20over%20competing%20methods.%20To%20the%20best%20of%20our%0Aknowledge%2C%20DCGen%20is%20the%20first%20segment-aware%20prompt-based%20approach%20for%0Agenerating%20UI%20code%20directly%20from%20screenshots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatically%2520Generating%2520UI%2520Code%2520from%2520Screenshot%253A%2520A%250A%2520%2520Divide-and-Conquer-Based%2520Approach%26entry.906535625%3DYuxuan%2520Wan%2520and%2520Chaozheng%2520Wang%2520and%2520Yi%2520Dong%2520and%2520Wenxuan%2520Wang%2520and%2520Shuqing%2520Li%2520and%2520Yintong%2520Huo%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Websites%2520are%2520critical%2520in%2520today%2527s%2520digital%2520world%252C%2520with%2520over%25201.11%2520billion%250Acurrently%2520active%2520and%2520approximately%2520252%252C000%2520new%2520sites%2520launched%2520daily.%2520Converting%250Awebsite%2520layout%2520design%2520into%2520functional%2520UI%2520code%2520is%2520a%2520time-consuming%2520yet%250Aindispensable%2520step%2520of%2520website%2520development.%2520Manual%2520methods%2520of%2520converting%2520visual%250Adesigns%2520into%2520functional%2520code%2520present%2520significant%2520challenges%252C%2520especially%2520for%250Anon-experts.%2520To%2520explore%2520automatic%2520design-to-code%2520solutions%252C%2520we%2520first%2520conduct%2520a%250Amotivating%2520study%2520on%2520GPT-4o%2520and%2520identify%2520three%2520types%2520of%2520issues%2520in%2520generating%2520UI%250Acode%253A%2520element%2520omission%252C%2520element%2520distortion%252C%2520and%2520element%2520misarrangement.%2520We%250Afurther%2520reveal%2520that%2520a%2520focus%2520on%2520smaller%2520visual%2520segments%2520can%2520help%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520mitigate%2520these%2520failures%2520in%2520the%2520generation%250Aprocess.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DCGen%252C%2520a%2520divide-and-conquer-based%2520approach%250Ato%2520automate%2520the%2520translation%2520of%2520webpage%2520design%2520to%2520UI%2520code.%2520DCGen%2520starts%2520by%250Adividing%2520screenshots%2520into%2520manageable%2520segments%252C%2520generating%2520descriptions%2520for%2520each%250Asegment%252C%2520and%2520then%2520reassembling%2520them%2520into%2520complete%2520UI%2520code%2520for%2520the%2520entire%250Ascreenshot.%2520We%2520conduct%2520extensive%2520testing%2520with%2520a%2520dataset%2520comprised%2520of%2520real-world%250Awebsites%2520and%2520various%2520MLLMs%2520and%2520demonstrate%2520that%2520DCGen%2520achieves%2520up%2520to%2520a%252014%2525%250Aimprovement%2520in%2520visual%2520similarity%2520over%2520competing%2520methods.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520DCGen%2520is%2520the%2520first%2520segment-aware%2520prompt-based%2520approach%2520for%250Agenerating%2520UI%2520code%2520directly%2520from%2520screenshots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatically%20Generating%20UI%20Code%20from%20Screenshot%3A%20A%0A%20%20Divide-and-Conquer-Based%20Approach&entry.906535625=Yuxuan%20Wan%20and%20Chaozheng%20Wang%20and%20Yi%20Dong%20and%20Wenxuan%20Wang%20and%20Shuqing%20Li%20and%20Yintong%20Huo%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Websites%20are%20critical%20in%20today%27s%20digital%20world%2C%20with%20over%201.11%20billion%0Acurrently%20active%20and%20approximately%20252%2C000%20new%20sites%20launched%20daily.%20Converting%0Awebsite%20layout%20design%20into%20functional%20UI%20code%20is%20a%20time-consuming%20yet%0Aindispensable%20step%20of%20website%20development.%20Manual%20methods%20of%20converting%20visual%0Adesigns%20into%20functional%20code%20present%20significant%20challenges%2C%20especially%20for%0Anon-experts.%20To%20explore%20automatic%20design-to-code%20solutions%2C%20we%20first%20conduct%20a%0Amotivating%20study%20on%20GPT-4o%20and%20identify%20three%20types%20of%20issues%20in%20generating%20UI%0Acode%3A%20element%20omission%2C%20element%20distortion%2C%20and%20element%20misarrangement.%20We%0Afurther%20reveal%20that%20a%20focus%20on%20smaller%20visual%20segments%20can%20help%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20mitigate%20these%20failures%20in%20the%20generation%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20DCGen%2C%20a%20divide-and-conquer-based%20approach%0Ato%20automate%20the%20translation%20of%20webpage%20design%20to%20UI%20code.%20DCGen%20starts%20by%0Adividing%20screenshots%20into%20manageable%20segments%2C%20generating%20descriptions%20for%20each%0Asegment%2C%20and%20then%20reassembling%20them%20into%20complete%20UI%20code%20for%20the%20entire%0Ascreenshot.%20We%20conduct%20extensive%20testing%20with%20a%20dataset%20comprised%20of%20real-world%0Awebsites%20and%20various%20MLLMs%20and%20demonstrate%20that%20DCGen%20achieves%20up%20to%20a%2014%25%0Aimprovement%20in%20visual%20similarity%20over%20competing%20methods.%20To%20the%20best%20of%20our%0Aknowledge%2C%20DCGen%20is%20the%20first%20segment-aware%20prompt-based%20approach%20for%0Agenerating%20UI%20code%20directly%20from%20screenshots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16386v2&entry.124074799=Read"},
{"title": "2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional\n  Supervision", "author": "Shilong Li and Yancheng He and Hui Huang and Xingyuan Bu and Jiaheng Liu and Hangyu Guo and Weixun Wang and Jihao Gu and Wenbo Su and Bo Zheng", "abstract": "  Recent advancements in Direct Preference Optimization (DPO) have\nsignificantly enhanced the alignment of Large Language Models (LLMs) with human\npreferences, owing to its simplicity and effectiveness. However, existing\nmethods typically optimize a scalar score or ranking reward, thereby\noverlooking the multi-dimensional nature of human preferences. In this work, we\npropose to extend the preference of DPO to two dimensions: segments and\naspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For\nthe segment dimension, we divide the response into sentences and assign scores\nto each segment. For the aspect dimension, we meticulously design several\ncriteria covering the response quality rubrics. With the 2-dimensional signals\nas feedback, we develop a 2D-DPO framework, decomposing the overall objective\ninto multi-segment and multi-aspect objectives. Extensive experiments on\npopular benchmarks demonstrate that 2D-DPO performs better than methods that\noptimize for scalar or 1-dimensional preferences.\n", "link": "http://arxiv.org/abs/2410.19720v1", "date": "2024-10-25", "relevancy": 2.6593, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202D-DPO%3A%20Scaling%20Direct%20Preference%20Optimization%20with%202-Dimensional%0A%20%20Supervision&body=Title%3A%202D-DPO%3A%20Scaling%20Direct%20Preference%20Optimization%20with%202-Dimensional%0A%20%20Supervision%0AAuthor%3A%20Shilong%20Li%20and%20Yancheng%20He%20and%20Hui%20Huang%20and%20Xingyuan%20Bu%20and%20Jiaheng%20Liu%20and%20Hangyu%20Guo%20and%20Weixun%20Wang%20and%20Jihao%20Gu%20and%20Wenbo%20Su%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Direct%20Preference%20Optimization%20%28DPO%29%20have%0Asignificantly%20enhanced%20the%20alignment%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20human%0Apreferences%2C%20owing%20to%20its%20simplicity%20and%20effectiveness.%20However%2C%20existing%0Amethods%20typically%20optimize%20a%20scalar%20score%20or%20ranking%20reward%2C%20thereby%0Aoverlooking%20the%20multi-dimensional%20nature%20of%20human%20preferences.%20In%20this%20work%2C%20we%0Apropose%20to%20extend%20the%20preference%20of%20DPO%20to%20two%20dimensions%3A%20segments%20and%0Aaspects.%20We%20first%20introduce%20a%202D%20supervision%20dataset%20called%20HelpSteer-2D.%20For%0Athe%20segment%20dimension%2C%20we%20divide%20the%20response%20into%20sentences%20and%20assign%20scores%0Ato%20each%20segment.%20For%20the%20aspect%20dimension%2C%20we%20meticulously%20design%20several%0Acriteria%20covering%20the%20response%20quality%20rubrics.%20With%20the%202-dimensional%20signals%0Aas%20feedback%2C%20we%20develop%20a%202D-DPO%20framework%2C%20decomposing%20the%20overall%20objective%0Ainto%20multi-segment%20and%20multi-aspect%20objectives.%20Extensive%20experiments%20on%0Apopular%20benchmarks%20demonstrate%20that%202D-DPO%20performs%20better%20than%20methods%20that%0Aoptimize%20for%20scalar%20or%201-dimensional%20preferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2D-DPO%253A%2520Scaling%2520Direct%2520Preference%2520Optimization%2520with%25202-Dimensional%250A%2520%2520Supervision%26entry.906535625%3DShilong%2520Li%2520and%2520Yancheng%2520He%2520and%2520Hui%2520Huang%2520and%2520Xingyuan%2520Bu%2520and%2520Jiaheng%2520Liu%2520and%2520Hangyu%2520Guo%2520and%2520Weixun%2520Wang%2520and%2520Jihao%2520Gu%2520and%2520Wenbo%2520Su%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520have%250Asignificantly%2520enhanced%2520the%2520alignment%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520human%250Apreferences%252C%2520owing%2520to%2520its%2520simplicity%2520and%2520effectiveness.%2520However%252C%2520existing%250Amethods%2520typically%2520optimize%2520a%2520scalar%2520score%2520or%2520ranking%2520reward%252C%2520thereby%250Aoverlooking%2520the%2520multi-dimensional%2520nature%2520of%2520human%2520preferences.%2520In%2520this%2520work%252C%2520we%250Apropose%2520to%2520extend%2520the%2520preference%2520of%2520DPO%2520to%2520two%2520dimensions%253A%2520segments%2520and%250Aaspects.%2520We%2520first%2520introduce%2520a%25202D%2520supervision%2520dataset%2520called%2520HelpSteer-2D.%2520For%250Athe%2520segment%2520dimension%252C%2520we%2520divide%2520the%2520response%2520into%2520sentences%2520and%2520assign%2520scores%250Ato%2520each%2520segment.%2520For%2520the%2520aspect%2520dimension%252C%2520we%2520meticulously%2520design%2520several%250Acriteria%2520covering%2520the%2520response%2520quality%2520rubrics.%2520With%2520the%25202-dimensional%2520signals%250Aas%2520feedback%252C%2520we%2520develop%2520a%25202D-DPO%2520framework%252C%2520decomposing%2520the%2520overall%2520objective%250Ainto%2520multi-segment%2520and%2520multi-aspect%2520objectives.%2520Extensive%2520experiments%2520on%250Apopular%2520benchmarks%2520demonstrate%2520that%25202D-DPO%2520performs%2520better%2520than%2520methods%2520that%250Aoptimize%2520for%2520scalar%2520or%25201-dimensional%2520preferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2D-DPO%3A%20Scaling%20Direct%20Preference%20Optimization%20with%202-Dimensional%0A%20%20Supervision&entry.906535625=Shilong%20Li%20and%20Yancheng%20He%20and%20Hui%20Huang%20and%20Xingyuan%20Bu%20and%20Jiaheng%20Liu%20and%20Hangyu%20Guo%20and%20Weixun%20Wang%20and%20Jihao%20Gu%20and%20Wenbo%20Su%20and%20Bo%20Zheng&entry.1292438233=%20%20Recent%20advancements%20in%20Direct%20Preference%20Optimization%20%28DPO%29%20have%0Asignificantly%20enhanced%20the%20alignment%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20human%0Apreferences%2C%20owing%20to%20its%20simplicity%20and%20effectiveness.%20However%2C%20existing%0Amethods%20typically%20optimize%20a%20scalar%20score%20or%20ranking%20reward%2C%20thereby%0Aoverlooking%20the%20multi-dimensional%20nature%20of%20human%20preferences.%20In%20this%20work%2C%20we%0Apropose%20to%20extend%20the%20preference%20of%20DPO%20to%20two%20dimensions%3A%20segments%20and%0Aaspects.%20We%20first%20introduce%20a%202D%20supervision%20dataset%20called%20HelpSteer-2D.%20For%0Athe%20segment%20dimension%2C%20we%20divide%20the%20response%20into%20sentences%20and%20assign%20scores%0Ato%20each%20segment.%20For%20the%20aspect%20dimension%2C%20we%20meticulously%20design%20several%0Acriteria%20covering%20the%20response%20quality%20rubrics.%20With%20the%202-dimensional%20signals%0Aas%20feedback%2C%20we%20develop%20a%202D-DPO%20framework%2C%20decomposing%20the%20overall%20objective%0Ainto%20multi-segment%20and%20multi-aspect%20objectives.%20Extensive%20experiments%20on%0Apopular%20benchmarks%20demonstrate%20that%202D-DPO%20performs%20better%20than%20methods%20that%0Aoptimize%20for%20scalar%20or%201-dimensional%20preferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19720v1&entry.124074799=Read"},
{"title": "Not (yet) the whole story: Evaluating Visual Storytelling Requires More\n  than Measuring Coherence, Grounding, and Repetition", "author": "Aditya K Surikuchi and Raquel Fern\u00e1ndez and Sandro Pezzelle", "abstract": "  Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition.\n", "link": "http://arxiv.org/abs/2407.04559v4", "date": "2024-10-25", "relevancy": 2.655, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20%28yet%29%20the%20whole%20story%3A%20Evaluating%20Visual%20Storytelling%20Requires%20More%0A%20%20than%20Measuring%20Coherence%2C%20Grounding%2C%20and%20Repetition&body=Title%3A%20Not%20%28yet%29%20the%20whole%20story%3A%20Evaluating%20Visual%20Storytelling%20Requires%20More%0A%20%20than%20Measuring%20Coherence%2C%20Grounding%2C%20and%20Repetition%0AAuthor%3A%20Aditya%20K%20Surikuchi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle%0AAbstract%3A%20%20%20Visual%20storytelling%20consists%20in%20generating%20a%20natural%20language%20story%20given%20a%0Atemporally%20ordered%20sequence%20of%20images.%20This%20task%20is%20not%20only%20challenging%20for%0Amodels%2C%20but%20also%20very%20difficult%20to%20evaluate%20with%20automatic%20metrics%20since%20there%0Ais%20no%20consensus%20about%20what%20makes%20a%20story%20%27good%27.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20method%20that%20measures%20story%20quality%20in%20terms%20of%20human%20likeness%20regarding%0Athree%20key%20aspects%20highlighted%20in%20previous%20work%3A%20visual%20grounding%2C%20coherence%2C%0Aand%20repetitiveness.%20We%20then%20use%20this%20method%20to%20evaluate%20the%20stories%20generated%0Aby%20several%20models%2C%20showing%20that%20the%20foundation%20model%20LLaVA%20obtains%20the%20best%0Aresult%2C%20but%20only%20slightly%20so%20compared%20to%20TAPM%2C%20a%2050-times%20smaller%20visual%0Astorytelling%20model.%20Upgrading%20the%20visual%20and%20language%20components%20of%20TAPM%0Aresults%20in%20a%20model%20that%20yields%20competitive%20performance%20with%20a%20relatively%20low%0Anumber%20of%20parameters.%20Finally%2C%20we%20carry%20out%20a%20human%20evaluation%20study%2C%20whose%0Aresults%20suggest%20that%20a%20%27good%27%20story%20may%20require%20more%20than%20a%20human-like%20level%20of%0Avisual%20grounding%2C%20coherence%2C%20and%20repetition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04559v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520%2528yet%2529%2520the%2520whole%2520story%253A%2520Evaluating%2520Visual%2520Storytelling%2520Requires%2520More%250A%2520%2520than%2520Measuring%2520Coherence%252C%2520Grounding%252C%2520and%2520Repetition%26entry.906535625%3DAditya%2520K%2520Surikuchi%2520and%2520Raquel%2520Fern%25C3%25A1ndez%2520and%2520Sandro%2520Pezzelle%26entry.1292438233%3D%2520%2520Visual%2520storytelling%2520consists%2520in%2520generating%2520a%2520natural%2520language%2520story%2520given%2520a%250Atemporally%2520ordered%2520sequence%2520of%2520images.%2520This%2520task%2520is%2520not%2520only%2520challenging%2520for%250Amodels%252C%2520but%2520also%2520very%2520difficult%2520to%2520evaluate%2520with%2520automatic%2520metrics%2520since%2520there%250Ais%2520no%2520consensus%2520about%2520what%2520makes%2520a%2520story%2520%2527good%2527.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Anovel%2520method%2520that%2520measures%2520story%2520quality%2520in%2520terms%2520of%2520human%2520likeness%2520regarding%250Athree%2520key%2520aspects%2520highlighted%2520in%2520previous%2520work%253A%2520visual%2520grounding%252C%2520coherence%252C%250Aand%2520repetitiveness.%2520We%2520then%2520use%2520this%2520method%2520to%2520evaluate%2520the%2520stories%2520generated%250Aby%2520several%2520models%252C%2520showing%2520that%2520the%2520foundation%2520model%2520LLaVA%2520obtains%2520the%2520best%250Aresult%252C%2520but%2520only%2520slightly%2520so%2520compared%2520to%2520TAPM%252C%2520a%252050-times%2520smaller%2520visual%250Astorytelling%2520model.%2520Upgrading%2520the%2520visual%2520and%2520language%2520components%2520of%2520TAPM%250Aresults%2520in%2520a%2520model%2520that%2520yields%2520competitive%2520performance%2520with%2520a%2520relatively%2520low%250Anumber%2520of%2520parameters.%2520Finally%252C%2520we%2520carry%2520out%2520a%2520human%2520evaluation%2520study%252C%2520whose%250Aresults%2520suggest%2520that%2520a%2520%2527good%2527%2520story%2520may%2520require%2520more%2520than%2520a%2520human-like%2520level%2520of%250Avisual%2520grounding%252C%2520coherence%252C%2520and%2520repetition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04559v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20%28yet%29%20the%20whole%20story%3A%20Evaluating%20Visual%20Storytelling%20Requires%20More%0A%20%20than%20Measuring%20Coherence%2C%20Grounding%2C%20and%20Repetition&entry.906535625=Aditya%20K%20Surikuchi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle&entry.1292438233=%20%20Visual%20storytelling%20consists%20in%20generating%20a%20natural%20language%20story%20given%20a%0Atemporally%20ordered%20sequence%20of%20images.%20This%20task%20is%20not%20only%20challenging%20for%0Amodels%2C%20but%20also%20very%20difficult%20to%20evaluate%20with%20automatic%20metrics%20since%20there%0Ais%20no%20consensus%20about%20what%20makes%20a%20story%20%27good%27.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20method%20that%20measures%20story%20quality%20in%20terms%20of%20human%20likeness%20regarding%0Athree%20key%20aspects%20highlighted%20in%20previous%20work%3A%20visual%20grounding%2C%20coherence%2C%0Aand%20repetitiveness.%20We%20then%20use%20this%20method%20to%20evaluate%20the%20stories%20generated%0Aby%20several%20models%2C%20showing%20that%20the%20foundation%20model%20LLaVA%20obtains%20the%20best%0Aresult%2C%20but%20only%20slightly%20so%20compared%20to%20TAPM%2C%20a%2050-times%20smaller%20visual%0Astorytelling%20model.%20Upgrading%20the%20visual%20and%20language%20components%20of%20TAPM%0Aresults%20in%20a%20model%20that%20yields%20competitive%20performance%20with%20a%20relatively%20low%0Anumber%20of%20parameters.%20Finally%2C%20we%20carry%20out%20a%20human%20evaluation%20study%2C%20whose%0Aresults%20suggest%20that%20a%20%27good%27%20story%20may%20require%20more%20than%20a%20human-like%20level%20of%0Avisual%20grounding%2C%20coherence%2C%20and%20repetition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04559v4&entry.124074799=Read"},
{"title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal\n  Change Detection in Remote Sensing", "author": "Hosam Elgendy and Ahmed Sharshar and Ahmed Aboeitta and Yasser Ashraf and Mohsen Guizani", "abstract": "  Detecting temporal changes in geographical landscapes is critical for\napplications like environmental monitoring and urban planning. While remote\nsensing data is abundant, existing vision-language models (VLMs) often fail to\ncapture temporal dynamics effectively. This paper addresses these limitations\nby introducing an annotated dataset of video frame pairs to track evolving\ngeographical patterns over time. Using fine-tuning techniques like Low-Rank\nAdaptation (LoRA), quantized LoRA (QLoRA), and model pruning on models such as\nVideo-LLaVA and LLaVA-NeXT-Video, we significantly enhance VLM performance in\nprocessing remote sensing temporal changes. Results show significant\nimprovements, with the best performance achieving a BERT score of 0.864 and\nROUGE-1 score of 0.576, demonstrating superior accuracy in describing land-use\ntransformations.\n", "link": "http://arxiv.org/abs/2410.19552v1", "date": "2024-10-25", "relevancy": 2.6511, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoLLaVA%3A%20Efficient%20Fine-Tuned%20Vision-Language%20Models%20for%20Temporal%0A%20%20Change%20Detection%20in%20Remote%20Sensing&body=Title%3A%20GeoLLaVA%3A%20Efficient%20Fine-Tuned%20Vision-Language%20Models%20for%20Temporal%0A%20%20Change%20Detection%20in%20Remote%20Sensing%0AAuthor%3A%20Hosam%20Elgendy%20and%20Ahmed%20Sharshar%20and%20Ahmed%20Aboeitta%20and%20Yasser%20Ashraf%20and%20Mohsen%20Guizani%0AAbstract%3A%20%20%20Detecting%20temporal%20changes%20in%20geographical%20landscapes%20is%20critical%20for%0Aapplications%20like%20environmental%20monitoring%20and%20urban%20planning.%20While%20remote%0Asensing%20data%20is%20abundant%2C%20existing%20vision-language%20models%20%28VLMs%29%20often%20fail%20to%0Acapture%20temporal%20dynamics%20effectively.%20This%20paper%20addresses%20these%20limitations%0Aby%20introducing%20an%20annotated%20dataset%20of%20video%20frame%20pairs%20to%20track%20evolving%0Ageographical%20patterns%20over%20time.%20Using%20fine-tuning%20techniques%20like%20Low-Rank%0AAdaptation%20%28LoRA%29%2C%20quantized%20LoRA%20%28QLoRA%29%2C%20and%20model%20pruning%20on%20models%20such%20as%0AVideo-LLaVA%20and%20LLaVA-NeXT-Video%2C%20we%20significantly%20enhance%20VLM%20performance%20in%0Aprocessing%20remote%20sensing%20temporal%20changes.%20Results%20show%20significant%0Aimprovements%2C%20with%20the%20best%20performance%20achieving%20a%20BERT%20score%20of%200.864%20and%0AROUGE-1%20score%20of%200.576%2C%20demonstrating%20superior%20accuracy%20in%20describing%20land-use%0Atransformations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoLLaVA%253A%2520Efficient%2520Fine-Tuned%2520Vision-Language%2520Models%2520for%2520Temporal%250A%2520%2520Change%2520Detection%2520in%2520Remote%2520Sensing%26entry.906535625%3DHosam%2520Elgendy%2520and%2520Ahmed%2520Sharshar%2520and%2520Ahmed%2520Aboeitta%2520and%2520Yasser%2520Ashraf%2520and%2520Mohsen%2520Guizani%26entry.1292438233%3D%2520%2520Detecting%2520temporal%2520changes%2520in%2520geographical%2520landscapes%2520is%2520critical%2520for%250Aapplications%2520like%2520environmental%2520monitoring%2520and%2520urban%2520planning.%2520While%2520remote%250Asensing%2520data%2520is%2520abundant%252C%2520existing%2520vision-language%2520models%2520%2528VLMs%2529%2520often%2520fail%2520to%250Acapture%2520temporal%2520dynamics%2520effectively.%2520This%2520paper%2520addresses%2520these%2520limitations%250Aby%2520introducing%2520an%2520annotated%2520dataset%2520of%2520video%2520frame%2520pairs%2520to%2520track%2520evolving%250Ageographical%2520patterns%2520over%2520time.%2520Using%2520fine-tuning%2520techniques%2520like%2520Low-Rank%250AAdaptation%2520%2528LoRA%2529%252C%2520quantized%2520LoRA%2520%2528QLoRA%2529%252C%2520and%2520model%2520pruning%2520on%2520models%2520such%2520as%250AVideo-LLaVA%2520and%2520LLaVA-NeXT-Video%252C%2520we%2520significantly%2520enhance%2520VLM%2520performance%2520in%250Aprocessing%2520remote%2520sensing%2520temporal%2520changes.%2520Results%2520show%2520significant%250Aimprovements%252C%2520with%2520the%2520best%2520performance%2520achieving%2520a%2520BERT%2520score%2520of%25200.864%2520and%250AROUGE-1%2520score%2520of%25200.576%252C%2520demonstrating%2520superior%2520accuracy%2520in%2520describing%2520land-use%250Atransformations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoLLaVA%3A%20Efficient%20Fine-Tuned%20Vision-Language%20Models%20for%20Temporal%0A%20%20Change%20Detection%20in%20Remote%20Sensing&entry.906535625=Hosam%20Elgendy%20and%20Ahmed%20Sharshar%20and%20Ahmed%20Aboeitta%20and%20Yasser%20Ashraf%20and%20Mohsen%20Guizani&entry.1292438233=%20%20Detecting%20temporal%20changes%20in%20geographical%20landscapes%20is%20critical%20for%0Aapplications%20like%20environmental%20monitoring%20and%20urban%20planning.%20While%20remote%0Asensing%20data%20is%20abundant%2C%20existing%20vision-language%20models%20%28VLMs%29%20often%20fail%20to%0Acapture%20temporal%20dynamics%20effectively.%20This%20paper%20addresses%20these%20limitations%0Aby%20introducing%20an%20annotated%20dataset%20of%20video%20frame%20pairs%20to%20track%20evolving%0Ageographical%20patterns%20over%20time.%20Using%20fine-tuning%20techniques%20like%20Low-Rank%0AAdaptation%20%28LoRA%29%2C%20quantized%20LoRA%20%28QLoRA%29%2C%20and%20model%20pruning%20on%20models%20such%20as%0AVideo-LLaVA%20and%20LLaVA-NeXT-Video%2C%20we%20significantly%20enhance%20VLM%20performance%20in%0Aprocessing%20remote%20sensing%20temporal%20changes.%20Results%20show%20significant%0Aimprovements%2C%20with%20the%20best%20performance%20achieving%20a%20BERT%20score%20of%200.864%20and%0AROUGE-1%20score%20of%200.576%2C%20demonstrating%20superior%20accuracy%20in%20describing%20land-use%0Atransformations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19552v1&entry.124074799=Read"},
{"title": "Brain-like Functional Organization within Large Language Models", "author": "H. Sun and L. Zhao and Z. Wu and X. Gao and Y. Hu and M. Zuo and W. Zhang and J. Han and T. Liu and X. Hu", "abstract": "  The human brain has long inspired the pursuit of artificial intelligence\n(AI). Recently, neuroimaging studies provide compelling evidence of alignment\nbetween the computational representation of artificial neural networks (ANNs)\nand the neural responses of the human brain to stimuli, suggesting that ANNs\nmay employ brain-like information processing strategies. While such alignment\nhas been observed across sensory modalities--visual, auditory, and\nlinguistic--much of the focus has been on the behaviors of artificial neurons\n(ANs) at the population level, leaving the functional organization of\nindividual ANs that facilitates such brain-like processes largely unexplored.\nIn this study, we bridge this gap by directly coupling sub-groups of artificial\nneurons with functional brain networks (FBNs), the foundational organizational\nstructure of the human brain. Specifically, we extract representative patterns\nfrom temporal responses of ANs in large language models (LLMs), and use them as\nfixed regressors to construct voxel-wise encoding models to predict brain\nactivity recorded by functional magnetic resonance imaging (fMRI). This\nframework links the AN sub-groups to FBNs, enabling the delineation of\nbrain-like functional organization within LLMs. Our findings reveal that LLMs\n(BERT and Llama 1-3) exhibit brain-like functional architecture, with\nsub-groups of artificial neurons mirroring the organizational patterns of\nwell-established FBNs. Notably, the brain-like functional organization of LLMs\nevolves with the increased sophistication and capability, achieving an improved\nbalance between the diversity of computational behaviors and the consistency of\nfunctional specializations. This research represents the first exploration of\nbrain-like functional organization within LLMs, offering novel insights to\ninform the development of artificial general intelligence (AGI) with human\nbrain principles.\n", "link": "http://arxiv.org/abs/2410.19542v1", "date": "2024-10-25", "relevancy": 2.6174, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-like%20Functional%20Organization%20within%20Large%20Language%20Models&body=Title%3A%20Brain-like%20Functional%20Organization%20within%20Large%20Language%20Models%0AAuthor%3A%20H.%20Sun%20and%20L.%20Zhao%20and%20Z.%20Wu%20and%20X.%20Gao%20and%20Y.%20Hu%20and%20M.%20Zuo%20and%20W.%20Zhang%20and%20J.%20Han%20and%20T.%20Liu%20and%20X.%20Hu%0AAbstract%3A%20%20%20The%20human%20brain%20has%20long%20inspired%20the%20pursuit%20of%20artificial%20intelligence%0A%28AI%29.%20Recently%2C%20neuroimaging%20studies%20provide%20compelling%20evidence%20of%20alignment%0Abetween%20the%20computational%20representation%20of%20artificial%20neural%20networks%20%28ANNs%29%0Aand%20the%20neural%20responses%20of%20the%20human%20brain%20to%20stimuli%2C%20suggesting%20that%20ANNs%0Amay%20employ%20brain-like%20information%20processing%20strategies.%20While%20such%20alignment%0Ahas%20been%20observed%20across%20sensory%20modalities--visual%2C%20auditory%2C%20and%0Alinguistic--much%20of%20the%20focus%20has%20been%20on%20the%20behaviors%20of%20artificial%20neurons%0A%28ANs%29%20at%20the%20population%20level%2C%20leaving%20the%20functional%20organization%20of%0Aindividual%20ANs%20that%20facilitates%20such%20brain-like%20processes%20largely%20unexplored.%0AIn%20this%20study%2C%20we%20bridge%20this%20gap%20by%20directly%20coupling%20sub-groups%20of%20artificial%0Aneurons%20with%20functional%20brain%20networks%20%28FBNs%29%2C%20the%20foundational%20organizational%0Astructure%20of%20the%20human%20brain.%20Specifically%2C%20we%20extract%20representative%20patterns%0Afrom%20temporal%20responses%20of%20ANs%20in%20large%20language%20models%20%28LLMs%29%2C%20and%20use%20them%20as%0Afixed%20regressors%20to%20construct%20voxel-wise%20encoding%20models%20to%20predict%20brain%0Aactivity%20recorded%20by%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29.%20This%0Aframework%20links%20the%20AN%20sub-groups%20to%20FBNs%2C%20enabling%20the%20delineation%20of%0Abrain-like%20functional%20organization%20within%20LLMs.%20Our%20findings%20reveal%20that%20LLMs%0A%28BERT%20and%20Llama%201-3%29%20exhibit%20brain-like%20functional%20architecture%2C%20with%0Asub-groups%20of%20artificial%20neurons%20mirroring%20the%20organizational%20patterns%20of%0Awell-established%20FBNs.%20Notably%2C%20the%20brain-like%20functional%20organization%20of%20LLMs%0Aevolves%20with%20the%20increased%20sophistication%20and%20capability%2C%20achieving%20an%20improved%0Abalance%20between%20the%20diversity%20of%20computational%20behaviors%20and%20the%20consistency%20of%0Afunctional%20specializations.%20This%20research%20represents%20the%20first%20exploration%20of%0Abrain-like%20functional%20organization%20within%20LLMs%2C%20offering%20novel%20insights%20to%0Ainform%20the%20development%20of%20artificial%20general%20intelligence%20%28AGI%29%20with%20human%0Abrain%20principles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-like%2520Functional%2520Organization%2520within%2520Large%2520Language%2520Models%26entry.906535625%3DH.%2520Sun%2520and%2520L.%2520Zhao%2520and%2520Z.%2520Wu%2520and%2520X.%2520Gao%2520and%2520Y.%2520Hu%2520and%2520M.%2520Zuo%2520and%2520W.%2520Zhang%2520and%2520J.%2520Han%2520and%2520T.%2520Liu%2520and%2520X.%2520Hu%26entry.1292438233%3D%2520%2520The%2520human%2520brain%2520has%2520long%2520inspired%2520the%2520pursuit%2520of%2520artificial%2520intelligence%250A%2528AI%2529.%2520Recently%252C%2520neuroimaging%2520studies%2520provide%2520compelling%2520evidence%2520of%2520alignment%250Abetween%2520the%2520computational%2520representation%2520of%2520artificial%2520neural%2520networks%2520%2528ANNs%2529%250Aand%2520the%2520neural%2520responses%2520of%2520the%2520human%2520brain%2520to%2520stimuli%252C%2520suggesting%2520that%2520ANNs%250Amay%2520employ%2520brain-like%2520information%2520processing%2520strategies.%2520While%2520such%2520alignment%250Ahas%2520been%2520observed%2520across%2520sensory%2520modalities--visual%252C%2520auditory%252C%2520and%250Alinguistic--much%2520of%2520the%2520focus%2520has%2520been%2520on%2520the%2520behaviors%2520of%2520artificial%2520neurons%250A%2528ANs%2529%2520at%2520the%2520population%2520level%252C%2520leaving%2520the%2520functional%2520organization%2520of%250Aindividual%2520ANs%2520that%2520facilitates%2520such%2520brain-like%2520processes%2520largely%2520unexplored.%250AIn%2520this%2520study%252C%2520we%2520bridge%2520this%2520gap%2520by%2520directly%2520coupling%2520sub-groups%2520of%2520artificial%250Aneurons%2520with%2520functional%2520brain%2520networks%2520%2528FBNs%2529%252C%2520the%2520foundational%2520organizational%250Astructure%2520of%2520the%2520human%2520brain.%2520Specifically%252C%2520we%2520extract%2520representative%2520patterns%250Afrom%2520temporal%2520responses%2520of%2520ANs%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520and%2520use%2520them%2520as%250Afixed%2520regressors%2520to%2520construct%2520voxel-wise%2520encoding%2520models%2520to%2520predict%2520brain%250Aactivity%2520recorded%2520by%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529.%2520This%250Aframework%2520links%2520the%2520AN%2520sub-groups%2520to%2520FBNs%252C%2520enabling%2520the%2520delineation%2520of%250Abrain-like%2520functional%2520organization%2520within%2520LLMs.%2520Our%2520findings%2520reveal%2520that%2520LLMs%250A%2528BERT%2520and%2520Llama%25201-3%2529%2520exhibit%2520brain-like%2520functional%2520architecture%252C%2520with%250Asub-groups%2520of%2520artificial%2520neurons%2520mirroring%2520the%2520organizational%2520patterns%2520of%250Awell-established%2520FBNs.%2520Notably%252C%2520the%2520brain-like%2520functional%2520organization%2520of%2520LLMs%250Aevolves%2520with%2520the%2520increased%2520sophistication%2520and%2520capability%252C%2520achieving%2520an%2520improved%250Abalance%2520between%2520the%2520diversity%2520of%2520computational%2520behaviors%2520and%2520the%2520consistency%2520of%250Afunctional%2520specializations.%2520This%2520research%2520represents%2520the%2520first%2520exploration%2520of%250Abrain-like%2520functional%2520organization%2520within%2520LLMs%252C%2520offering%2520novel%2520insights%2520to%250Ainform%2520the%2520development%2520of%2520artificial%2520general%2520intelligence%2520%2528AGI%2529%2520with%2520human%250Abrain%2520principles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-like%20Functional%20Organization%20within%20Large%20Language%20Models&entry.906535625=H.%20Sun%20and%20L.%20Zhao%20and%20Z.%20Wu%20and%20X.%20Gao%20and%20Y.%20Hu%20and%20M.%20Zuo%20and%20W.%20Zhang%20and%20J.%20Han%20and%20T.%20Liu%20and%20X.%20Hu&entry.1292438233=%20%20The%20human%20brain%20has%20long%20inspired%20the%20pursuit%20of%20artificial%20intelligence%0A%28AI%29.%20Recently%2C%20neuroimaging%20studies%20provide%20compelling%20evidence%20of%20alignment%0Abetween%20the%20computational%20representation%20of%20artificial%20neural%20networks%20%28ANNs%29%0Aand%20the%20neural%20responses%20of%20the%20human%20brain%20to%20stimuli%2C%20suggesting%20that%20ANNs%0Amay%20employ%20brain-like%20information%20processing%20strategies.%20While%20such%20alignment%0Ahas%20been%20observed%20across%20sensory%20modalities--visual%2C%20auditory%2C%20and%0Alinguistic--much%20of%20the%20focus%20has%20been%20on%20the%20behaviors%20of%20artificial%20neurons%0A%28ANs%29%20at%20the%20population%20level%2C%20leaving%20the%20functional%20organization%20of%0Aindividual%20ANs%20that%20facilitates%20such%20brain-like%20processes%20largely%20unexplored.%0AIn%20this%20study%2C%20we%20bridge%20this%20gap%20by%20directly%20coupling%20sub-groups%20of%20artificial%0Aneurons%20with%20functional%20brain%20networks%20%28FBNs%29%2C%20the%20foundational%20organizational%0Astructure%20of%20the%20human%20brain.%20Specifically%2C%20we%20extract%20representative%20patterns%0Afrom%20temporal%20responses%20of%20ANs%20in%20large%20language%20models%20%28LLMs%29%2C%20and%20use%20them%20as%0Afixed%20regressors%20to%20construct%20voxel-wise%20encoding%20models%20to%20predict%20brain%0Aactivity%20recorded%20by%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29.%20This%0Aframework%20links%20the%20AN%20sub-groups%20to%20FBNs%2C%20enabling%20the%20delineation%20of%0Abrain-like%20functional%20organization%20within%20LLMs.%20Our%20findings%20reveal%20that%20LLMs%0A%28BERT%20and%20Llama%201-3%29%20exhibit%20brain-like%20functional%20architecture%2C%20with%0Asub-groups%20of%20artificial%20neurons%20mirroring%20the%20organizational%20patterns%20of%0Awell-established%20FBNs.%20Notably%2C%20the%20brain-like%20functional%20organization%20of%20LLMs%0Aevolves%20with%20the%20increased%20sophistication%20and%20capability%2C%20achieving%20an%20improved%0Abalance%20between%20the%20diversity%20of%20computational%20behaviors%20and%20the%20consistency%20of%0Afunctional%20specializations.%20This%20research%20represents%20the%20first%20exploration%20of%0Abrain-like%20functional%20organization%20within%20LLMs%2C%20offering%20novel%20insights%20to%0Ainform%20the%20development%20of%20artificial%20general%20intelligence%20%28AGI%29%20with%20human%0Abrain%20principles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19542v1&entry.124074799=Read"},
{"title": "A constrained optimization approach to improve robustness of neural\n  networks", "author": "Shudian Zhao and Jan Kronqvist", "abstract": "  In this paper, we present a novel nonlinear programming-based approach to\nfine-tune pre-trained neural networks to improve robustness against adversarial\nattacks while maintaining high accuracy on clean data. Our method introduces\nadversary-correction constraints to ensure correct classification of\nadversarial data and minimizes changes to the model parameters. We propose an\nefficient cutting-plane-based algorithm to iteratively solve the large-scale\nnonconvex optimization problem by approximating the feasible region through\npolyhedral cuts and balancing between robustness and accuracy. Computational\nexperiments on standard datasets such as MNIST and CIFAR10 demonstrate that the\nproposed approach significantly improves robustness, even with a very small set\nof adversarial data, while maintaining minimal impact on accuracy.\n", "link": "http://arxiv.org/abs/2409.13770v2", "date": "2024-10-25", "relevancy": 2.61, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5459}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5159}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20constrained%20optimization%20approach%20to%20improve%20robustness%20of%20neural%0A%20%20networks&body=Title%3A%20A%20constrained%20optimization%20approach%20to%20improve%20robustness%20of%20neural%0A%20%20networks%0AAuthor%3A%20Shudian%20Zhao%20and%20Jan%20Kronqvist%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20nonlinear%20programming-based%20approach%20to%0Afine-tune%20pre-trained%20neural%20networks%20to%20improve%20robustness%20against%20adversarial%0Aattacks%20while%20maintaining%20high%20accuracy%20on%20clean%20data.%20Our%20method%20introduces%0Aadversary-correction%20constraints%20to%20ensure%20correct%20classification%20of%0Aadversarial%20data%20and%20minimizes%20changes%20to%20the%20model%20parameters.%20We%20propose%20an%0Aefficient%20cutting-plane-based%20algorithm%20to%20iteratively%20solve%20the%20large-scale%0Anonconvex%20optimization%20problem%20by%20approximating%20the%20feasible%20region%20through%0Apolyhedral%20cuts%20and%20balancing%20between%20robustness%20and%20accuracy.%20Computational%0Aexperiments%20on%20standard%20datasets%20such%20as%20MNIST%20and%20CIFAR10%20demonstrate%20that%20the%0Aproposed%20approach%20significantly%20improves%20robustness%2C%20even%20with%20a%20very%20small%20set%0Aof%20adversarial%20data%2C%20while%20maintaining%20minimal%20impact%20on%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13770v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520constrained%2520optimization%2520approach%2520to%2520improve%2520robustness%2520of%2520neural%250A%2520%2520networks%26entry.906535625%3DShudian%2520Zhao%2520and%2520Jan%2520Kronqvist%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520nonlinear%2520programming-based%2520approach%2520to%250Afine-tune%2520pre-trained%2520neural%2520networks%2520to%2520improve%2520robustness%2520against%2520adversarial%250Aattacks%2520while%2520maintaining%2520high%2520accuracy%2520on%2520clean%2520data.%2520Our%2520method%2520introduces%250Aadversary-correction%2520constraints%2520to%2520ensure%2520correct%2520classification%2520of%250Aadversarial%2520data%2520and%2520minimizes%2520changes%2520to%2520the%2520model%2520parameters.%2520We%2520propose%2520an%250Aefficient%2520cutting-plane-based%2520algorithm%2520to%2520iteratively%2520solve%2520the%2520large-scale%250Anonconvex%2520optimization%2520problem%2520by%2520approximating%2520the%2520feasible%2520region%2520through%250Apolyhedral%2520cuts%2520and%2520balancing%2520between%2520robustness%2520and%2520accuracy.%2520Computational%250Aexperiments%2520on%2520standard%2520datasets%2520such%2520as%2520MNIST%2520and%2520CIFAR10%2520demonstrate%2520that%2520the%250Aproposed%2520approach%2520significantly%2520improves%2520robustness%252C%2520even%2520with%2520a%2520very%2520small%2520set%250Aof%2520adversarial%2520data%252C%2520while%2520maintaining%2520minimal%2520impact%2520on%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13770v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20constrained%20optimization%20approach%20to%20improve%20robustness%20of%20neural%0A%20%20networks&entry.906535625=Shudian%20Zhao%20and%20Jan%20Kronqvist&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20nonlinear%20programming-based%20approach%20to%0Afine-tune%20pre-trained%20neural%20networks%20to%20improve%20robustness%20against%20adversarial%0Aattacks%20while%20maintaining%20high%20accuracy%20on%20clean%20data.%20Our%20method%20introduces%0Aadversary-correction%20constraints%20to%20ensure%20correct%20classification%20of%0Aadversarial%20data%20and%20minimizes%20changes%20to%20the%20model%20parameters.%20We%20propose%20an%0Aefficient%20cutting-plane-based%20algorithm%20to%20iteratively%20solve%20the%20large-scale%0Anonconvex%20optimization%20problem%20by%20approximating%20the%20feasible%20region%20through%0Apolyhedral%20cuts%20and%20balancing%20between%20robustness%20and%20accuracy.%20Computational%0Aexperiments%20on%20standard%20datasets%20such%20as%20MNIST%20and%20CIFAR10%20demonstrate%20that%20the%0Aproposed%20approach%20significantly%20improves%20robustness%2C%20even%20with%20a%20very%20small%20set%0Aof%20adversarial%20data%2C%20while%20maintaining%20minimal%20impact%20on%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13770v2&entry.124074799=Read"},
{"title": "FLAC: Fairness-Aware Representation Learning by Suppressing\n  Attribute-Class Associations", "author": "Ioannis Sarridis and Christos Koutlis and Symeon Papadopoulos and Christos Diou", "abstract": "  Bias in computer vision systems can perpetuate or even amplify discrimination\nagainst certain populations. Considering that bias is often introduced by\nbiased visual datasets, many recent research efforts focus on training fair\nmodels using such data. However, most of them heavily rely on the availability\nof protected attribute labels in the dataset, which limits their applicability,\nwhile label-unaware approaches, i.e., approaches operating without such labels,\nexhibit considerably lower performance. To overcome these limitations, this\nwork introduces FLAC, a methodology that minimizes mutual information between\nthe features extracted by the model and a protected attribute, without the use\nof attribute labels. To do that, FLAC proposes a sampling strategy that\nhighlights underrepresented samples in the dataset, and casts the problem of\nlearning fair representations as a probability matching problem that leverages\nrepresentations extracted by a bias-capturing classifier. It is theoretically\nshown that FLAC can indeed lead to fair representations, that are independent\nof the protected attributes. FLAC surpasses the current state-of-the-art on\nBiased-MNIST, CelebA, and UTKFace, by 29.1%, 18.1%, and 21.9%, respectively.\nAdditionally, FLAC exhibits 2.2% increased accuracy on ImageNet-A and up to\n4.2% increased accuracy on Corrupted-Cifar10. Finally, in most experiments,\nFLAC even outperforms the bias label-aware state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2304.14252v2", "date": "2024-10-25", "relevancy": 2.6085, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5312}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAC%3A%20Fairness-Aware%20Representation%20Learning%20by%20Suppressing%0A%20%20Attribute-Class%20Associations&body=Title%3A%20FLAC%3A%20Fairness-Aware%20Representation%20Learning%20by%20Suppressing%0A%20%20Attribute-Class%20Associations%0AAuthor%3A%20Ioannis%20Sarridis%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos%20and%20Christos%20Diou%0AAbstract%3A%20%20%20Bias%20in%20computer%20vision%20systems%20can%20perpetuate%20or%20even%20amplify%20discrimination%0Aagainst%20certain%20populations.%20Considering%20that%20bias%20is%20often%20introduced%20by%0Abiased%20visual%20datasets%2C%20many%20recent%20research%20efforts%20focus%20on%20training%20fair%0Amodels%20using%20such%20data.%20However%2C%20most%20of%20them%20heavily%20rely%20on%20the%20availability%0Aof%20protected%20attribute%20labels%20in%20the%20dataset%2C%20which%20limits%20their%20applicability%2C%0Awhile%20label-unaware%20approaches%2C%20i.e.%2C%20approaches%20operating%20without%20such%20labels%2C%0Aexhibit%20considerably%20lower%20performance.%20To%20overcome%20these%20limitations%2C%20this%0Awork%20introduces%20FLAC%2C%20a%20methodology%20that%20minimizes%20mutual%20information%20between%0Athe%20features%20extracted%20by%20the%20model%20and%20a%20protected%20attribute%2C%20without%20the%20use%0Aof%20attribute%20labels.%20To%20do%20that%2C%20FLAC%20proposes%20a%20sampling%20strategy%20that%0Ahighlights%20underrepresented%20samples%20in%20the%20dataset%2C%20and%20casts%20the%20problem%20of%0Alearning%20fair%20representations%20as%20a%20probability%20matching%20problem%20that%20leverages%0Arepresentations%20extracted%20by%20a%20bias-capturing%20classifier.%20It%20is%20theoretically%0Ashown%20that%20FLAC%20can%20indeed%20lead%20to%20fair%20representations%2C%20that%20are%20independent%0Aof%20the%20protected%20attributes.%20FLAC%20surpasses%20the%20current%20state-of-the-art%20on%0ABiased-MNIST%2C%20CelebA%2C%20and%20UTKFace%2C%20by%2029.1%25%2C%2018.1%25%2C%20and%2021.9%25%2C%20respectively.%0AAdditionally%2C%20FLAC%20exhibits%202.2%25%20increased%20accuracy%20on%20ImageNet-A%20and%20up%20to%0A4.2%25%20increased%20accuracy%20on%20Corrupted-Cifar10.%20Finally%2C%20in%20most%20experiments%2C%0AFLAC%20even%20outperforms%20the%20bias%20label-aware%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.14252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAC%253A%2520Fairness-Aware%2520Representation%2520Learning%2520by%2520Suppressing%250A%2520%2520Attribute-Class%2520Associations%26entry.906535625%3DIoannis%2520Sarridis%2520and%2520Christos%2520Koutlis%2520and%2520Symeon%2520Papadopoulos%2520and%2520Christos%2520Diou%26entry.1292438233%3D%2520%2520Bias%2520in%2520computer%2520vision%2520systems%2520can%2520perpetuate%2520or%2520even%2520amplify%2520discrimination%250Aagainst%2520certain%2520populations.%2520Considering%2520that%2520bias%2520is%2520often%2520introduced%2520by%250Abiased%2520visual%2520datasets%252C%2520many%2520recent%2520research%2520efforts%2520focus%2520on%2520training%2520fair%250Amodels%2520using%2520such%2520data.%2520However%252C%2520most%2520of%2520them%2520heavily%2520rely%2520on%2520the%2520availability%250Aof%2520protected%2520attribute%2520labels%2520in%2520the%2520dataset%252C%2520which%2520limits%2520their%2520applicability%252C%250Awhile%2520label-unaware%2520approaches%252C%2520i.e.%252C%2520approaches%2520operating%2520without%2520such%2520labels%252C%250Aexhibit%2520considerably%2520lower%2520performance.%2520To%2520overcome%2520these%2520limitations%252C%2520this%250Awork%2520introduces%2520FLAC%252C%2520a%2520methodology%2520that%2520minimizes%2520mutual%2520information%2520between%250Athe%2520features%2520extracted%2520by%2520the%2520model%2520and%2520a%2520protected%2520attribute%252C%2520without%2520the%2520use%250Aof%2520attribute%2520labels.%2520To%2520do%2520that%252C%2520FLAC%2520proposes%2520a%2520sampling%2520strategy%2520that%250Ahighlights%2520underrepresented%2520samples%2520in%2520the%2520dataset%252C%2520and%2520casts%2520the%2520problem%2520of%250Alearning%2520fair%2520representations%2520as%2520a%2520probability%2520matching%2520problem%2520that%2520leverages%250Arepresentations%2520extracted%2520by%2520a%2520bias-capturing%2520classifier.%2520It%2520is%2520theoretically%250Ashown%2520that%2520FLAC%2520can%2520indeed%2520lead%2520to%2520fair%2520representations%252C%2520that%2520are%2520independent%250Aof%2520the%2520protected%2520attributes.%2520FLAC%2520surpasses%2520the%2520current%2520state-of-the-art%2520on%250ABiased-MNIST%252C%2520CelebA%252C%2520and%2520UTKFace%252C%2520by%252029.1%2525%252C%252018.1%2525%252C%2520and%252021.9%2525%252C%2520respectively.%250AAdditionally%252C%2520FLAC%2520exhibits%25202.2%2525%2520increased%2520accuracy%2520on%2520ImageNet-A%2520and%2520up%2520to%250A4.2%2525%2520increased%2520accuracy%2520on%2520Corrupted-Cifar10.%2520Finally%252C%2520in%2520most%2520experiments%252C%250AFLAC%2520even%2520outperforms%2520the%2520bias%2520label-aware%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.14252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAC%3A%20Fairness-Aware%20Representation%20Learning%20by%20Suppressing%0A%20%20Attribute-Class%20Associations&entry.906535625=Ioannis%20Sarridis%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos%20and%20Christos%20Diou&entry.1292438233=%20%20Bias%20in%20computer%20vision%20systems%20can%20perpetuate%20or%20even%20amplify%20discrimination%0Aagainst%20certain%20populations.%20Considering%20that%20bias%20is%20often%20introduced%20by%0Abiased%20visual%20datasets%2C%20many%20recent%20research%20efforts%20focus%20on%20training%20fair%0Amodels%20using%20such%20data.%20However%2C%20most%20of%20them%20heavily%20rely%20on%20the%20availability%0Aof%20protected%20attribute%20labels%20in%20the%20dataset%2C%20which%20limits%20their%20applicability%2C%0Awhile%20label-unaware%20approaches%2C%20i.e.%2C%20approaches%20operating%20without%20such%20labels%2C%0Aexhibit%20considerably%20lower%20performance.%20To%20overcome%20these%20limitations%2C%20this%0Awork%20introduces%20FLAC%2C%20a%20methodology%20that%20minimizes%20mutual%20information%20between%0Athe%20features%20extracted%20by%20the%20model%20and%20a%20protected%20attribute%2C%20without%20the%20use%0Aof%20attribute%20labels.%20To%20do%20that%2C%20FLAC%20proposes%20a%20sampling%20strategy%20that%0Ahighlights%20underrepresented%20samples%20in%20the%20dataset%2C%20and%20casts%20the%20problem%20of%0Alearning%20fair%20representations%20as%20a%20probability%20matching%20problem%20that%20leverages%0Arepresentations%20extracted%20by%20a%20bias-capturing%20classifier.%20It%20is%20theoretically%0Ashown%20that%20FLAC%20can%20indeed%20lead%20to%20fair%20representations%2C%20that%20are%20independent%0Aof%20the%20protected%20attributes.%20FLAC%20surpasses%20the%20current%20state-of-the-art%20on%0ABiased-MNIST%2C%20CelebA%2C%20and%20UTKFace%2C%20by%2029.1%25%2C%2018.1%25%2C%20and%2021.9%25%2C%20respectively.%0AAdditionally%2C%20FLAC%20exhibits%202.2%25%20increased%20accuracy%20on%20ImageNet-A%20and%20up%20to%0A4.2%25%20increased%20accuracy%20on%20Corrupted-Cifar10.%20Finally%2C%20in%20most%20experiments%2C%0AFLAC%20even%20outperforms%20the%20bias%20label-aware%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.14252v2&entry.124074799=Read"},
{"title": "FastPCI: Motion-Structure Guided Fast Point Cloud Frame Interpolation", "author": "Tianyu Zhang and Guocheng Qian and Jin Xie and Jian Yang", "abstract": "  Point cloud frame interpolation is a challenging task that involves accurate\nscene flow estimation across frames and maintaining the geometry structure.\nPrevailing techniques often rely on pre-trained motion estimators or intensive\ntesting-time optimization, resulting in compromised interpolation accuracy or\nprolonged inference. This work presents FastPCI that introduces Pyramid\nConvolution-Transformer architecture for point cloud frame interpolation. Our\nhybrid Convolution-Transformer improves the local and long-range feature\nlearning, while the pyramid network offers multilevel features and reduces the\ncomputation. In addition, FastPCI proposes a unique Dual-Direction\nMotion-Structure block for more accurate scene flow estimation. Our design is\nmotivated by two facts: (1) accurate scene flow preserves 3D structure, and (2)\npoint cloud at the previous timestep should be reconstructable using reverse\nmotion from future timestep. Extensive experiments show that FastPCI\nsignificantly outperforms the state-of-the-art PointINet and NeuralPCI with\nnotable gains (e.g. 26.6% and 18.3% reduction in Chamfer Distance in KITTI),\nwhile being more than 10x and 600x faster, respectively. Code is available at\nhttps://github.com/genuszty/FastPCI\n", "link": "http://arxiv.org/abs/2410.19573v1", "date": "2024-10-25", "relevancy": 2.5936, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5274}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5207}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastPCI%3A%20Motion-Structure%20Guided%20Fast%20Point%20Cloud%20Frame%20Interpolation&body=Title%3A%20FastPCI%3A%20Motion-Structure%20Guided%20Fast%20Point%20Cloud%20Frame%20Interpolation%0AAuthor%3A%20Tianyu%20Zhang%20and%20Guocheng%20Qian%20and%20Jin%20Xie%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Point%20cloud%20frame%20interpolation%20is%20a%20challenging%20task%20that%20involves%20accurate%0Ascene%20flow%20estimation%20across%20frames%20and%20maintaining%20the%20geometry%20structure.%0APrevailing%20techniques%20often%20rely%20on%20pre-trained%20motion%20estimators%20or%20intensive%0Atesting-time%20optimization%2C%20resulting%20in%20compromised%20interpolation%20accuracy%20or%0Aprolonged%20inference.%20This%20work%20presents%20FastPCI%20that%20introduces%20Pyramid%0AConvolution-Transformer%20architecture%20for%20point%20cloud%20frame%20interpolation.%20Our%0Ahybrid%20Convolution-Transformer%20improves%20the%20local%20and%20long-range%20feature%0Alearning%2C%20while%20the%20pyramid%20network%20offers%20multilevel%20features%20and%20reduces%20the%0Acomputation.%20In%20addition%2C%20FastPCI%20proposes%20a%20unique%20Dual-Direction%0AMotion-Structure%20block%20for%20more%20accurate%20scene%20flow%20estimation.%20Our%20design%20is%0Amotivated%20by%20two%20facts%3A%20%281%29%20accurate%20scene%20flow%20preserves%203D%20structure%2C%20and%20%282%29%0Apoint%20cloud%20at%20the%20previous%20timestep%20should%20be%20reconstructable%20using%20reverse%0Amotion%20from%20future%20timestep.%20Extensive%20experiments%20show%20that%20FastPCI%0Asignificantly%20outperforms%20the%20state-of-the-art%20PointINet%20and%20NeuralPCI%20with%0Anotable%20gains%20%28e.g.%2026.6%25%20and%2018.3%25%20reduction%20in%20Chamfer%20Distance%20in%20KITTI%29%2C%0Awhile%20being%20more%20than%2010x%20and%20600x%20faster%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/genuszty/FastPCI%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastPCI%253A%2520Motion-Structure%2520Guided%2520Fast%2520Point%2520Cloud%2520Frame%2520Interpolation%26entry.906535625%3DTianyu%2520Zhang%2520and%2520Guocheng%2520Qian%2520and%2520Jin%2520Xie%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Point%2520cloud%2520frame%2520interpolation%2520is%2520a%2520challenging%2520task%2520that%2520involves%2520accurate%250Ascene%2520flow%2520estimation%2520across%2520frames%2520and%2520maintaining%2520the%2520geometry%2520structure.%250APrevailing%2520techniques%2520often%2520rely%2520on%2520pre-trained%2520motion%2520estimators%2520or%2520intensive%250Atesting-time%2520optimization%252C%2520resulting%2520in%2520compromised%2520interpolation%2520accuracy%2520or%250Aprolonged%2520inference.%2520This%2520work%2520presents%2520FastPCI%2520that%2520introduces%2520Pyramid%250AConvolution-Transformer%2520architecture%2520for%2520point%2520cloud%2520frame%2520interpolation.%2520Our%250Ahybrid%2520Convolution-Transformer%2520improves%2520the%2520local%2520and%2520long-range%2520feature%250Alearning%252C%2520while%2520the%2520pyramid%2520network%2520offers%2520multilevel%2520features%2520and%2520reduces%2520the%250Acomputation.%2520In%2520addition%252C%2520FastPCI%2520proposes%2520a%2520unique%2520Dual-Direction%250AMotion-Structure%2520block%2520for%2520more%2520accurate%2520scene%2520flow%2520estimation.%2520Our%2520design%2520is%250Amotivated%2520by%2520two%2520facts%253A%2520%25281%2529%2520accurate%2520scene%2520flow%2520preserves%25203D%2520structure%252C%2520and%2520%25282%2529%250Apoint%2520cloud%2520at%2520the%2520previous%2520timestep%2520should%2520be%2520reconstructable%2520using%2520reverse%250Amotion%2520from%2520future%2520timestep.%2520Extensive%2520experiments%2520show%2520that%2520FastPCI%250Asignificantly%2520outperforms%2520the%2520state-of-the-art%2520PointINet%2520and%2520NeuralPCI%2520with%250Anotable%2520gains%2520%2528e.g.%252026.6%2525%2520and%252018.3%2525%2520reduction%2520in%2520Chamfer%2520Distance%2520in%2520KITTI%2529%252C%250Awhile%2520being%2520more%2520than%252010x%2520and%2520600x%2520faster%252C%2520respectively.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/genuszty/FastPCI%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastPCI%3A%20Motion-Structure%20Guided%20Fast%20Point%20Cloud%20Frame%20Interpolation&entry.906535625=Tianyu%20Zhang%20and%20Guocheng%20Qian%20and%20Jin%20Xie%20and%20Jian%20Yang&entry.1292438233=%20%20Point%20cloud%20frame%20interpolation%20is%20a%20challenging%20task%20that%20involves%20accurate%0Ascene%20flow%20estimation%20across%20frames%20and%20maintaining%20the%20geometry%20structure.%0APrevailing%20techniques%20often%20rely%20on%20pre-trained%20motion%20estimators%20or%20intensive%0Atesting-time%20optimization%2C%20resulting%20in%20compromised%20interpolation%20accuracy%20or%0Aprolonged%20inference.%20This%20work%20presents%20FastPCI%20that%20introduces%20Pyramid%0AConvolution-Transformer%20architecture%20for%20point%20cloud%20frame%20interpolation.%20Our%0Ahybrid%20Convolution-Transformer%20improves%20the%20local%20and%20long-range%20feature%0Alearning%2C%20while%20the%20pyramid%20network%20offers%20multilevel%20features%20and%20reduces%20the%0Acomputation.%20In%20addition%2C%20FastPCI%20proposes%20a%20unique%20Dual-Direction%0AMotion-Structure%20block%20for%20more%20accurate%20scene%20flow%20estimation.%20Our%20design%20is%0Amotivated%20by%20two%20facts%3A%20%281%29%20accurate%20scene%20flow%20preserves%203D%20structure%2C%20and%20%282%29%0Apoint%20cloud%20at%20the%20previous%20timestep%20should%20be%20reconstructable%20using%20reverse%0Amotion%20from%20future%20timestep.%20Extensive%20experiments%20show%20that%20FastPCI%0Asignificantly%20outperforms%20the%20state-of-the-art%20PointINet%20and%20NeuralPCI%20with%0Anotable%20gains%20%28e.g.%2026.6%25%20and%2018.3%25%20reduction%20in%20Chamfer%20Distance%20in%20KITTI%29%2C%0Awhile%20being%20more%20than%2010x%20and%20600x%20faster%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/genuszty/FastPCI%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19573v1&entry.124074799=Read"},
{"title": "GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language\n  Models", "author": "Ali Abdollahi and Mahdi Ghaznavi and Mohammad Reza Karimi Nejad and Arash Mari Oriyad and Reza Abbasi and Ali Salesi and Melika Behjati and Mohammad Hossein Rohban and Mahdieh Soleymani Baghshah", "abstract": "  Vision-language models (VLMs) are intensively used in many downstream tasks,\nincluding those requiring assessments of individuals appearing in the images.\nWhile VLMs perform well in simple single-person scenarios, in real-world\napplications, we often face complex situations in which there are persons of\ndifferent genders doing different activities. We show that in such cases, VLMs\nare biased towards identifying the individual with the expected gender\n(according to ingrained gender stereotypes in the model or other forms of\nsample selection bias) as the performer of the activity. We refer to this bias\nin associating an activity with the gender of its actual performer in an image\nor text as the Gender-Activity Binding (GAB) bias and analyze how this bias is\ninternalized in VLMs. To assess this bias, we have introduced the GAB dataset\nwith approximately 5500 AI-generated images that represent a variety of\nactivities, addressing the scarcity of real-world images for some scenarios. To\nhave extensive quality control, the generated images are evaluated for their\ndiversity, quality, and realism. We have tested 12 renowned pre-trained VLMs on\nthis dataset in the context of text-to-image and image-to-text retrieval to\nmeasure the effect of this bias on their predictions. Additionally, we have\ncarried out supplementary experiments to quantify the bias in VLMs' text\nencoders and to evaluate VLMs' capability to recognize activities. Our\nexperiments indicate that VLMs experience an average performance decline of\nabout 13.2% when confronted with gender-activity binding bias.\n", "link": "http://arxiv.org/abs/2407.21001v3", "date": "2024-10-25", "relevancy": 2.5651, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GABInsight%3A%20Exploring%20Gender-Activity%20Binding%20Bias%20in%20Vision-Language%0A%20%20Models&body=Title%3A%20GABInsight%3A%20Exploring%20Gender-Activity%20Binding%20Bias%20in%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Ali%20Abdollahi%20and%20Mahdi%20Ghaznavi%20and%20Mohammad%20Reza%20Karimi%20Nejad%20and%20Arash%20Mari%20Oriyad%20and%20Reza%20Abbasi%20and%20Ali%20Salesi%20and%20Melika%20Behjati%20and%20Mohammad%20Hossein%20Rohban%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20are%20intensively%20used%20in%20many%20downstream%20tasks%2C%0Aincluding%20those%20requiring%20assessments%20of%20individuals%20appearing%20in%20the%20images.%0AWhile%20VLMs%20perform%20well%20in%20simple%20single-person%20scenarios%2C%20in%20real-world%0Aapplications%2C%20we%20often%20face%20complex%20situations%20in%20which%20there%20are%20persons%20of%0Adifferent%20genders%20doing%20different%20activities.%20We%20show%20that%20in%20such%20cases%2C%20VLMs%0Aare%20biased%20towards%20identifying%20the%20individual%20with%20the%20expected%20gender%0A%28according%20to%20ingrained%20gender%20stereotypes%20in%20the%20model%20or%20other%20forms%20of%0Asample%20selection%20bias%29%20as%20the%20performer%20of%20the%20activity.%20We%20refer%20to%20this%20bias%0Ain%20associating%20an%20activity%20with%20the%20gender%20of%20its%20actual%20performer%20in%20an%20image%0Aor%20text%20as%20the%20Gender-Activity%20Binding%20%28GAB%29%20bias%20and%20analyze%20how%20this%20bias%20is%0Ainternalized%20in%20VLMs.%20To%20assess%20this%20bias%2C%20we%20have%20introduced%20the%20GAB%20dataset%0Awith%20approximately%205500%20AI-generated%20images%20that%20represent%20a%20variety%20of%0Aactivities%2C%20addressing%20the%20scarcity%20of%20real-world%20images%20for%20some%20scenarios.%20To%0Ahave%20extensive%20quality%20control%2C%20the%20generated%20images%20are%20evaluated%20for%20their%0Adiversity%2C%20quality%2C%20and%20realism.%20We%20have%20tested%2012%20renowned%20pre-trained%20VLMs%20on%0Athis%20dataset%20in%20the%20context%20of%20text-to-image%20and%20image-to-text%20retrieval%20to%0Ameasure%20the%20effect%20of%20this%20bias%20on%20their%20predictions.%20Additionally%2C%20we%20have%0Acarried%20out%20supplementary%20experiments%20to%20quantify%20the%20bias%20in%20VLMs%27%20text%0Aencoders%20and%20to%20evaluate%20VLMs%27%20capability%20to%20recognize%20activities.%20Our%0Aexperiments%20indicate%20that%20VLMs%20experience%20an%20average%20performance%20decline%20of%0Aabout%2013.2%25%20when%20confronted%20with%20gender-activity%20binding%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21001v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGABInsight%253A%2520Exploring%2520Gender-Activity%2520Binding%2520Bias%2520in%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DAli%2520Abdollahi%2520and%2520Mahdi%2520Ghaznavi%2520and%2520Mohammad%2520Reza%2520Karimi%2520Nejad%2520and%2520Arash%2520Mari%2520Oriyad%2520and%2520Reza%2520Abbasi%2520and%2520Ali%2520Salesi%2520and%2520Melika%2520Behjati%2520and%2520Mohammad%2520Hossein%2520Rohban%2520and%2520Mahdieh%2520Soleymani%2520Baghshah%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520are%2520intensively%2520used%2520in%2520many%2520downstream%2520tasks%252C%250Aincluding%2520those%2520requiring%2520assessments%2520of%2520individuals%2520appearing%2520in%2520the%2520images.%250AWhile%2520VLMs%2520perform%2520well%2520in%2520simple%2520single-person%2520scenarios%252C%2520in%2520real-world%250Aapplications%252C%2520we%2520often%2520face%2520complex%2520situations%2520in%2520which%2520there%2520are%2520persons%2520of%250Adifferent%2520genders%2520doing%2520different%2520activities.%2520We%2520show%2520that%2520in%2520such%2520cases%252C%2520VLMs%250Aare%2520biased%2520towards%2520identifying%2520the%2520individual%2520with%2520the%2520expected%2520gender%250A%2528according%2520to%2520ingrained%2520gender%2520stereotypes%2520in%2520the%2520model%2520or%2520other%2520forms%2520of%250Asample%2520selection%2520bias%2529%2520as%2520the%2520performer%2520of%2520the%2520activity.%2520We%2520refer%2520to%2520this%2520bias%250Ain%2520associating%2520an%2520activity%2520with%2520the%2520gender%2520of%2520its%2520actual%2520performer%2520in%2520an%2520image%250Aor%2520text%2520as%2520the%2520Gender-Activity%2520Binding%2520%2528GAB%2529%2520bias%2520and%2520analyze%2520how%2520this%2520bias%2520is%250Ainternalized%2520in%2520VLMs.%2520To%2520assess%2520this%2520bias%252C%2520we%2520have%2520introduced%2520the%2520GAB%2520dataset%250Awith%2520approximately%25205500%2520AI-generated%2520images%2520that%2520represent%2520a%2520variety%2520of%250Aactivities%252C%2520addressing%2520the%2520scarcity%2520of%2520real-world%2520images%2520for%2520some%2520scenarios.%2520To%250Ahave%2520extensive%2520quality%2520control%252C%2520the%2520generated%2520images%2520are%2520evaluated%2520for%2520their%250Adiversity%252C%2520quality%252C%2520and%2520realism.%2520We%2520have%2520tested%252012%2520renowned%2520pre-trained%2520VLMs%2520on%250Athis%2520dataset%2520in%2520the%2520context%2520of%2520text-to-image%2520and%2520image-to-text%2520retrieval%2520to%250Ameasure%2520the%2520effect%2520of%2520this%2520bias%2520on%2520their%2520predictions.%2520Additionally%252C%2520we%2520have%250Acarried%2520out%2520supplementary%2520experiments%2520to%2520quantify%2520the%2520bias%2520in%2520VLMs%2527%2520text%250Aencoders%2520and%2520to%2520evaluate%2520VLMs%2527%2520capability%2520to%2520recognize%2520activities.%2520Our%250Aexperiments%2520indicate%2520that%2520VLMs%2520experience%2520an%2520average%2520performance%2520decline%2520of%250Aabout%252013.2%2525%2520when%2520confronted%2520with%2520gender-activity%2520binding%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21001v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GABInsight%3A%20Exploring%20Gender-Activity%20Binding%20Bias%20in%20Vision-Language%0A%20%20Models&entry.906535625=Ali%20Abdollahi%20and%20Mahdi%20Ghaznavi%20and%20Mohammad%20Reza%20Karimi%20Nejad%20and%20Arash%20Mari%20Oriyad%20and%20Reza%20Abbasi%20and%20Ali%20Salesi%20and%20Melika%20Behjati%20and%20Mohammad%20Hossein%20Rohban%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20are%20intensively%20used%20in%20many%20downstream%20tasks%2C%0Aincluding%20those%20requiring%20assessments%20of%20individuals%20appearing%20in%20the%20images.%0AWhile%20VLMs%20perform%20well%20in%20simple%20single-person%20scenarios%2C%20in%20real-world%0Aapplications%2C%20we%20often%20face%20complex%20situations%20in%20which%20there%20are%20persons%20of%0Adifferent%20genders%20doing%20different%20activities.%20We%20show%20that%20in%20such%20cases%2C%20VLMs%0Aare%20biased%20towards%20identifying%20the%20individual%20with%20the%20expected%20gender%0A%28according%20to%20ingrained%20gender%20stereotypes%20in%20the%20model%20or%20other%20forms%20of%0Asample%20selection%20bias%29%20as%20the%20performer%20of%20the%20activity.%20We%20refer%20to%20this%20bias%0Ain%20associating%20an%20activity%20with%20the%20gender%20of%20its%20actual%20performer%20in%20an%20image%0Aor%20text%20as%20the%20Gender-Activity%20Binding%20%28GAB%29%20bias%20and%20analyze%20how%20this%20bias%20is%0Ainternalized%20in%20VLMs.%20To%20assess%20this%20bias%2C%20we%20have%20introduced%20the%20GAB%20dataset%0Awith%20approximately%205500%20AI-generated%20images%20that%20represent%20a%20variety%20of%0Aactivities%2C%20addressing%20the%20scarcity%20of%20real-world%20images%20for%20some%20scenarios.%20To%0Ahave%20extensive%20quality%20control%2C%20the%20generated%20images%20are%20evaluated%20for%20their%0Adiversity%2C%20quality%2C%20and%20realism.%20We%20have%20tested%2012%20renowned%20pre-trained%20VLMs%20on%0Athis%20dataset%20in%20the%20context%20of%20text-to-image%20and%20image-to-text%20retrieval%20to%0Ameasure%20the%20effect%20of%20this%20bias%20on%20their%20predictions.%20Additionally%2C%20we%20have%0Acarried%20out%20supplementary%20experiments%20to%20quantify%20the%20bias%20in%20VLMs%27%20text%0Aencoders%20and%20to%20evaluate%20VLMs%27%20capability%20to%20recognize%20activities.%20Our%0Aexperiments%20indicate%20that%20VLMs%20experience%20an%20average%20performance%20decline%20of%0Aabout%2013.2%25%20when%20confronted%20with%20gender-activity%20binding%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21001v3&entry.124074799=Read"},
{"title": "LOCAL: Learning with Orientation Matrix to Infer Causal Structure from\n  Time Series Data", "author": "Yue Cheng and Jiajun Zhang and Weiwei Xing and Xiaoyu Guo and Xiaohui Gao", "abstract": "  Discovering the underlying Directed Acyclic Graph (DAG) from time series\nobservational data is highly challenging due to the dynamic nature and complex\nnonlinear interactions between variables. Existing methods often struggle with\ninefficiency and the handling of high-dimensional data. To address these\nresearch gap, we propose LOCAL, a highly efficient, easy-to-implement, and\nconstraint-free method for recovering dynamic causal structures. LOCAL is the\nfirst attempt to formulate a quasi-maximum likelihood-based score function for\nlearning the dynamic DAG equivalent to the ground truth. On this basis, we\npropose two adaptive modules for enhancing the algebraic characterization of\nacyclicity with new capabilities: Asymptotic Causal Mask Learning (ACML) and\nDynamic Graph Parameter Learning (DGPL). ACML generates causal masks using\nlearnable priority vectors and the Gumbel-Sigmoid function, ensuring the\ncreation of DAGs while optimizing computational efficiency. DGPL transforms\ncausal learning into decomposed matrix products, capturing the dynamic causal\nstructure of high-dimensional data and enhancing interpretability. Extensive\nexperiments on synthetic and real-world datasets demonstrate that LOCAL\nsignificantly outperforms existing methods, and highlight LOCAL's potential as\na robust and efficient method for dynamic causal discovery. Our code will be\navailable soon.\n", "link": "http://arxiv.org/abs/2410.19464v1", "date": "2024-10-25", "relevancy": 2.5412, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5268}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5029}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOCAL%3A%20Learning%20with%20Orientation%20Matrix%20to%20Infer%20Causal%20Structure%20from%0A%20%20Time%20Series%20Data&body=Title%3A%20LOCAL%3A%20Learning%20with%20Orientation%20Matrix%20to%20Infer%20Causal%20Structure%20from%0A%20%20Time%20Series%20Data%0AAuthor%3A%20Yue%20Cheng%20and%20Jiajun%20Zhang%20and%20Weiwei%20Xing%20and%20Xiaoyu%20Guo%20and%20Xiaohui%20Gao%0AAbstract%3A%20%20%20Discovering%20the%20underlying%20Directed%20Acyclic%20Graph%20%28DAG%29%20from%20time%20series%0Aobservational%20data%20is%20highly%20challenging%20due%20to%20the%20dynamic%20nature%20and%20complex%0Anonlinear%20interactions%20between%20variables.%20Existing%20methods%20often%20struggle%20with%0Ainefficiency%20and%20the%20handling%20of%20high-dimensional%20data.%20To%20address%20these%0Aresearch%20gap%2C%20we%20propose%20LOCAL%2C%20a%20highly%20efficient%2C%20easy-to-implement%2C%20and%0Aconstraint-free%20method%20for%20recovering%20dynamic%20causal%20structures.%20LOCAL%20is%20the%0Afirst%20attempt%20to%20formulate%20a%20quasi-maximum%20likelihood-based%20score%20function%20for%0Alearning%20the%20dynamic%20DAG%20equivalent%20to%20the%20ground%20truth.%20On%20this%20basis%2C%20we%0Apropose%20two%20adaptive%20modules%20for%20enhancing%20the%20algebraic%20characterization%20of%0Aacyclicity%20with%20new%20capabilities%3A%20Asymptotic%20Causal%20Mask%20Learning%20%28ACML%29%20and%0ADynamic%20Graph%20Parameter%20Learning%20%28DGPL%29.%20ACML%20generates%20causal%20masks%20using%0Alearnable%20priority%20vectors%20and%20the%20Gumbel-Sigmoid%20function%2C%20ensuring%20the%0Acreation%20of%20DAGs%20while%20optimizing%20computational%20efficiency.%20DGPL%20transforms%0Acausal%20learning%20into%20decomposed%20matrix%20products%2C%20capturing%20the%20dynamic%20causal%0Astructure%20of%20high-dimensional%20data%20and%20enhancing%20interpretability.%20Extensive%0Aexperiments%20on%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20LOCAL%0Asignificantly%20outperforms%20existing%20methods%2C%20and%20highlight%20LOCAL%27s%20potential%20as%0Aa%20robust%20and%20efficient%20method%20for%20dynamic%20causal%20discovery.%20Our%20code%20will%20be%0Aavailable%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOCAL%253A%2520Learning%2520with%2520Orientation%2520Matrix%2520to%2520Infer%2520Causal%2520Structure%2520from%250A%2520%2520Time%2520Series%2520Data%26entry.906535625%3DYue%2520Cheng%2520and%2520Jiajun%2520Zhang%2520and%2520Weiwei%2520Xing%2520and%2520Xiaoyu%2520Guo%2520and%2520Xiaohui%2520Gao%26entry.1292438233%3D%2520%2520Discovering%2520the%2520underlying%2520Directed%2520Acyclic%2520Graph%2520%2528DAG%2529%2520from%2520time%2520series%250Aobservational%2520data%2520is%2520highly%2520challenging%2520due%2520to%2520the%2520dynamic%2520nature%2520and%2520complex%250Anonlinear%2520interactions%2520between%2520variables.%2520Existing%2520methods%2520often%2520struggle%2520with%250Ainefficiency%2520and%2520the%2520handling%2520of%2520high-dimensional%2520data.%2520To%2520address%2520these%250Aresearch%2520gap%252C%2520we%2520propose%2520LOCAL%252C%2520a%2520highly%2520efficient%252C%2520easy-to-implement%252C%2520and%250Aconstraint-free%2520method%2520for%2520recovering%2520dynamic%2520causal%2520structures.%2520LOCAL%2520is%2520the%250Afirst%2520attempt%2520to%2520formulate%2520a%2520quasi-maximum%2520likelihood-based%2520score%2520function%2520for%250Alearning%2520the%2520dynamic%2520DAG%2520equivalent%2520to%2520the%2520ground%2520truth.%2520On%2520this%2520basis%252C%2520we%250Apropose%2520two%2520adaptive%2520modules%2520for%2520enhancing%2520the%2520algebraic%2520characterization%2520of%250Aacyclicity%2520with%2520new%2520capabilities%253A%2520Asymptotic%2520Causal%2520Mask%2520Learning%2520%2528ACML%2529%2520and%250ADynamic%2520Graph%2520Parameter%2520Learning%2520%2528DGPL%2529.%2520ACML%2520generates%2520causal%2520masks%2520using%250Alearnable%2520priority%2520vectors%2520and%2520the%2520Gumbel-Sigmoid%2520function%252C%2520ensuring%2520the%250Acreation%2520of%2520DAGs%2520while%2520optimizing%2520computational%2520efficiency.%2520DGPL%2520transforms%250Acausal%2520learning%2520into%2520decomposed%2520matrix%2520products%252C%2520capturing%2520the%2520dynamic%2520causal%250Astructure%2520of%2520high-dimensional%2520data%2520and%2520enhancing%2520interpretability.%2520Extensive%250Aexperiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520LOCAL%250Asignificantly%2520outperforms%2520existing%2520methods%252C%2520and%2520highlight%2520LOCAL%2527s%2520potential%2520as%250Aa%2520robust%2520and%2520efficient%2520method%2520for%2520dynamic%2520causal%2520discovery.%2520Our%2520code%2520will%2520be%250Aavailable%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOCAL%3A%20Learning%20with%20Orientation%20Matrix%20to%20Infer%20Causal%20Structure%20from%0A%20%20Time%20Series%20Data&entry.906535625=Yue%20Cheng%20and%20Jiajun%20Zhang%20and%20Weiwei%20Xing%20and%20Xiaoyu%20Guo%20and%20Xiaohui%20Gao&entry.1292438233=%20%20Discovering%20the%20underlying%20Directed%20Acyclic%20Graph%20%28DAG%29%20from%20time%20series%0Aobservational%20data%20is%20highly%20challenging%20due%20to%20the%20dynamic%20nature%20and%20complex%0Anonlinear%20interactions%20between%20variables.%20Existing%20methods%20often%20struggle%20with%0Ainefficiency%20and%20the%20handling%20of%20high-dimensional%20data.%20To%20address%20these%0Aresearch%20gap%2C%20we%20propose%20LOCAL%2C%20a%20highly%20efficient%2C%20easy-to-implement%2C%20and%0Aconstraint-free%20method%20for%20recovering%20dynamic%20causal%20structures.%20LOCAL%20is%20the%0Afirst%20attempt%20to%20formulate%20a%20quasi-maximum%20likelihood-based%20score%20function%20for%0Alearning%20the%20dynamic%20DAG%20equivalent%20to%20the%20ground%20truth.%20On%20this%20basis%2C%20we%0Apropose%20two%20adaptive%20modules%20for%20enhancing%20the%20algebraic%20characterization%20of%0Aacyclicity%20with%20new%20capabilities%3A%20Asymptotic%20Causal%20Mask%20Learning%20%28ACML%29%20and%0ADynamic%20Graph%20Parameter%20Learning%20%28DGPL%29.%20ACML%20generates%20causal%20masks%20using%0Alearnable%20priority%20vectors%20and%20the%20Gumbel-Sigmoid%20function%2C%20ensuring%20the%0Acreation%20of%20DAGs%20while%20optimizing%20computational%20efficiency.%20DGPL%20transforms%0Acausal%20learning%20into%20decomposed%20matrix%20products%2C%20capturing%20the%20dynamic%20causal%0Astructure%20of%20high-dimensional%20data%20and%20enhancing%20interpretability.%20Extensive%0Aexperiments%20on%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20LOCAL%0Asignificantly%20outperforms%20existing%20methods%2C%20and%20highlight%20LOCAL%27s%20potential%20as%0Aa%20robust%20and%20efficient%20method%20for%20dynamic%20causal%20discovery.%20Our%20code%20will%20be%0Aavailable%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19464v1&entry.124074799=Read"},
{"title": "MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language\n  Recognition Dataset", "author": "Xin Shen and Heming Du and Hongwei Sheng and Shuyun Wang and Hui Chen and Huiqiang Chen and Zhuojie Wu and Xiaobiao Du and Jiaying Ying and Ruihan Lu and Qingzheng Xu and Xin Yu", "abstract": "  Isolated Sign Language Recognition (ISLR) focuses on identifying individual\nsign language glosses. Considering the diversity of sign languages across\ngeographical regions, developing region-specific ISLR datasets is crucial for\nsupporting communication and research. Auslan, as a sign language specific to\nAustralia, still lacks a dedicated large-scale word-level dataset for the ISLR\ntask. To fill this gap, we curate \\underline{\\textbf{the first}} large-scale\nMulti-view Multi-modal Word-Level Australian Sign Language recognition dataset,\ndubbed MM-WLAuslan. Compared to other publicly available datasets, MM-WLAuslan\nexhibits three significant advantages: (1) the largest amount of data, (2) the\nmost extensive vocabulary, and (3) the most diverse of multi-modal camera\nviews. Specifically, we record 282K+ sign videos covering 3,215 commonly used\nAuslan glosses presented by 73 signers in a studio environment. Moreover, our\nfilming system includes two different types of cameras, i.e., three Kinect-V2\ncameras and a RealSense camera. We position cameras hemispherically around the\nfront half of the model and simultaneously record videos using all four\ncameras. Furthermore, we benchmark results with state-of-the-art methods for\nvarious multi-modal ISLR settings on MM-WLAuslan, including multi-view,\ncross-camera, and cross-view. Experiment results indicate that MM-WLAuslan is a\nchallenging ISLR dataset, and we hope this dataset will contribute to the\ndevelopment of Auslan and the advancement of sign languages worldwide. All\ndatasets and benchmarks are available at MM-WLAuslan.\n", "link": "http://arxiv.org/abs/2410.19488v1", "date": "2024-10-25", "relevancy": 2.5317, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-WLAuslan%3A%20Multi-View%20Multi-Modal%20Word-Level%20Australian%20Sign%20Language%0A%20%20Recognition%20Dataset&body=Title%3A%20MM-WLAuslan%3A%20Multi-View%20Multi-Modal%20Word-Level%20Australian%20Sign%20Language%0A%20%20Recognition%20Dataset%0AAuthor%3A%20Xin%20Shen%20and%20Heming%20Du%20and%20Hongwei%20Sheng%20and%20Shuyun%20Wang%20and%20Hui%20Chen%20and%20Huiqiang%20Chen%20and%20Zhuojie%20Wu%20and%20Xiaobiao%20Du%20and%20Jiaying%20Ying%20and%20Ruihan%20Lu%20and%20Qingzheng%20Xu%20and%20Xin%20Yu%0AAbstract%3A%20%20%20Isolated%20Sign%20Language%20Recognition%20%28ISLR%29%20focuses%20on%20identifying%20individual%0Asign%20language%20glosses.%20Considering%20the%20diversity%20of%20sign%20languages%20across%0Ageographical%20regions%2C%20developing%20region-specific%20ISLR%20datasets%20is%20crucial%20for%0Asupporting%20communication%20and%20research.%20Auslan%2C%20as%20a%20sign%20language%20specific%20to%0AAustralia%2C%20still%20lacks%20a%20dedicated%20large-scale%20word-level%20dataset%20for%20the%20ISLR%0Atask.%20To%20fill%20this%20gap%2C%20we%20curate%20%5Cunderline%7B%5Ctextbf%7Bthe%20first%7D%7D%20large-scale%0AMulti-view%20Multi-modal%20Word-Level%20Australian%20Sign%20Language%20recognition%20dataset%2C%0Adubbed%20MM-WLAuslan.%20Compared%20to%20other%20publicly%20available%20datasets%2C%20MM-WLAuslan%0Aexhibits%20three%20significant%20advantages%3A%20%281%29%20the%20largest%20amount%20of%20data%2C%20%282%29%20the%0Amost%20extensive%20vocabulary%2C%20and%20%283%29%20the%20most%20diverse%20of%20multi-modal%20camera%0Aviews.%20Specifically%2C%20we%20record%20282K%2B%20sign%20videos%20covering%203%2C215%20commonly%20used%0AAuslan%20glosses%20presented%20by%2073%20signers%20in%20a%20studio%20environment.%20Moreover%2C%20our%0Afilming%20system%20includes%20two%20different%20types%20of%20cameras%2C%20i.e.%2C%20three%20Kinect-V2%0Acameras%20and%20a%20RealSense%20camera.%20We%20position%20cameras%20hemispherically%20around%20the%0Afront%20half%20of%20the%20model%20and%20simultaneously%20record%20videos%20using%20all%20four%0Acameras.%20Furthermore%2C%20we%20benchmark%20results%20with%20state-of-the-art%20methods%20for%0Avarious%20multi-modal%20ISLR%20settings%20on%20MM-WLAuslan%2C%20including%20multi-view%2C%0Across-camera%2C%20and%20cross-view.%20Experiment%20results%20indicate%20that%20MM-WLAuslan%20is%20a%0Achallenging%20ISLR%20dataset%2C%20and%20we%20hope%20this%20dataset%20will%20contribute%20to%20the%0Adevelopment%20of%20Auslan%20and%20the%20advancement%20of%20sign%20languages%20worldwide.%20All%0Adatasets%20and%20benchmarks%20are%20available%20at%20MM-WLAuslan.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-WLAuslan%253A%2520Multi-View%2520Multi-Modal%2520Word-Level%2520Australian%2520Sign%2520Language%250A%2520%2520Recognition%2520Dataset%26entry.906535625%3DXin%2520Shen%2520and%2520Heming%2520Du%2520and%2520Hongwei%2520Sheng%2520and%2520Shuyun%2520Wang%2520and%2520Hui%2520Chen%2520and%2520Huiqiang%2520Chen%2520and%2520Zhuojie%2520Wu%2520and%2520Xiaobiao%2520Du%2520and%2520Jiaying%2520Ying%2520and%2520Ruihan%2520Lu%2520and%2520Qingzheng%2520Xu%2520and%2520Xin%2520Yu%26entry.1292438233%3D%2520%2520Isolated%2520Sign%2520Language%2520Recognition%2520%2528ISLR%2529%2520focuses%2520on%2520identifying%2520individual%250Asign%2520language%2520glosses.%2520Considering%2520the%2520diversity%2520of%2520sign%2520languages%2520across%250Ageographical%2520regions%252C%2520developing%2520region-specific%2520ISLR%2520datasets%2520is%2520crucial%2520for%250Asupporting%2520communication%2520and%2520research.%2520Auslan%252C%2520as%2520a%2520sign%2520language%2520specific%2520to%250AAustralia%252C%2520still%2520lacks%2520a%2520dedicated%2520large-scale%2520word-level%2520dataset%2520for%2520the%2520ISLR%250Atask.%2520To%2520fill%2520this%2520gap%252C%2520we%2520curate%2520%255Cunderline%257B%255Ctextbf%257Bthe%2520first%257D%257D%2520large-scale%250AMulti-view%2520Multi-modal%2520Word-Level%2520Australian%2520Sign%2520Language%2520recognition%2520dataset%252C%250Adubbed%2520MM-WLAuslan.%2520Compared%2520to%2520other%2520publicly%2520available%2520datasets%252C%2520MM-WLAuslan%250Aexhibits%2520three%2520significant%2520advantages%253A%2520%25281%2529%2520the%2520largest%2520amount%2520of%2520data%252C%2520%25282%2529%2520the%250Amost%2520extensive%2520vocabulary%252C%2520and%2520%25283%2529%2520the%2520most%2520diverse%2520of%2520multi-modal%2520camera%250Aviews.%2520Specifically%252C%2520we%2520record%2520282K%252B%2520sign%2520videos%2520covering%25203%252C215%2520commonly%2520used%250AAuslan%2520glosses%2520presented%2520by%252073%2520signers%2520in%2520a%2520studio%2520environment.%2520Moreover%252C%2520our%250Afilming%2520system%2520includes%2520two%2520different%2520types%2520of%2520cameras%252C%2520i.e.%252C%2520three%2520Kinect-V2%250Acameras%2520and%2520a%2520RealSense%2520camera.%2520We%2520position%2520cameras%2520hemispherically%2520around%2520the%250Afront%2520half%2520of%2520the%2520model%2520and%2520simultaneously%2520record%2520videos%2520using%2520all%2520four%250Acameras.%2520Furthermore%252C%2520we%2520benchmark%2520results%2520with%2520state-of-the-art%2520methods%2520for%250Avarious%2520multi-modal%2520ISLR%2520settings%2520on%2520MM-WLAuslan%252C%2520including%2520multi-view%252C%250Across-camera%252C%2520and%2520cross-view.%2520Experiment%2520results%2520indicate%2520that%2520MM-WLAuslan%2520is%2520a%250Achallenging%2520ISLR%2520dataset%252C%2520and%2520we%2520hope%2520this%2520dataset%2520will%2520contribute%2520to%2520the%250Adevelopment%2520of%2520Auslan%2520and%2520the%2520advancement%2520of%2520sign%2520languages%2520worldwide.%2520All%250Adatasets%2520and%2520benchmarks%2520are%2520available%2520at%2520MM-WLAuslan.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-WLAuslan%3A%20Multi-View%20Multi-Modal%20Word-Level%20Australian%20Sign%20Language%0A%20%20Recognition%20Dataset&entry.906535625=Xin%20Shen%20and%20Heming%20Du%20and%20Hongwei%20Sheng%20and%20Shuyun%20Wang%20and%20Hui%20Chen%20and%20Huiqiang%20Chen%20and%20Zhuojie%20Wu%20and%20Xiaobiao%20Du%20and%20Jiaying%20Ying%20and%20Ruihan%20Lu%20and%20Qingzheng%20Xu%20and%20Xin%20Yu&entry.1292438233=%20%20Isolated%20Sign%20Language%20Recognition%20%28ISLR%29%20focuses%20on%20identifying%20individual%0Asign%20language%20glosses.%20Considering%20the%20diversity%20of%20sign%20languages%20across%0Ageographical%20regions%2C%20developing%20region-specific%20ISLR%20datasets%20is%20crucial%20for%0Asupporting%20communication%20and%20research.%20Auslan%2C%20as%20a%20sign%20language%20specific%20to%0AAustralia%2C%20still%20lacks%20a%20dedicated%20large-scale%20word-level%20dataset%20for%20the%20ISLR%0Atask.%20To%20fill%20this%20gap%2C%20we%20curate%20%5Cunderline%7B%5Ctextbf%7Bthe%20first%7D%7D%20large-scale%0AMulti-view%20Multi-modal%20Word-Level%20Australian%20Sign%20Language%20recognition%20dataset%2C%0Adubbed%20MM-WLAuslan.%20Compared%20to%20other%20publicly%20available%20datasets%2C%20MM-WLAuslan%0Aexhibits%20three%20significant%20advantages%3A%20%281%29%20the%20largest%20amount%20of%20data%2C%20%282%29%20the%0Amost%20extensive%20vocabulary%2C%20and%20%283%29%20the%20most%20diverse%20of%20multi-modal%20camera%0Aviews.%20Specifically%2C%20we%20record%20282K%2B%20sign%20videos%20covering%203%2C215%20commonly%20used%0AAuslan%20glosses%20presented%20by%2073%20signers%20in%20a%20studio%20environment.%20Moreover%2C%20our%0Afilming%20system%20includes%20two%20different%20types%20of%20cameras%2C%20i.e.%2C%20three%20Kinect-V2%0Acameras%20and%20a%20RealSense%20camera.%20We%20position%20cameras%20hemispherically%20around%20the%0Afront%20half%20of%20the%20model%20and%20simultaneously%20record%20videos%20using%20all%20four%0Acameras.%20Furthermore%2C%20we%20benchmark%20results%20with%20state-of-the-art%20methods%20for%0Avarious%20multi-modal%20ISLR%20settings%20on%20MM-WLAuslan%2C%20including%20multi-view%2C%0Across-camera%2C%20and%20cross-view.%20Experiment%20results%20indicate%20that%20MM-WLAuslan%20is%20a%0Achallenging%20ISLR%20dataset%2C%20and%20we%20hope%20this%20dataset%20will%20contribute%20to%20the%0Adevelopment%20of%20Auslan%20and%20the%20advancement%20of%20sign%20languages%20worldwide.%20All%0Adatasets%20and%20benchmarks%20are%20available%20at%20MM-WLAuslan.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19488v1&entry.124074799=Read"},
{"title": "Model merging with SVD to tie the Knots", "author": "George Stoica and Pratik Ramesh and Boglarka Ecsedi and Leshem Choshen and Judy Hoffman", "abstract": "  Recent model merging methods demonstrate that the parameters of\nfully-finetuned models specializing in distinct tasks can be combined into one\nmodel capable of solving all tasks without retraining. Yet, this success does\nnot transfer well when merging LoRA finetuned models. We study this phenomenon\nand observe that the weights of LoRA finetuned models showcase a lower degree\nof alignment compared to their fully-finetuned counterparts. We hypothesize\nthat improving this alignment is key to obtaining better LoRA model merges, and\npropose KnOTS to address this problem. KnOTS uses the SVD to jointly transform\nthe weights of different LoRA models into an aligned space, where existing\nmerging methods can be applied. In addition, we introduce a new benchmark that\nexplicitly evaluates whether merged models are general models. Notably, KnOTS\nconsistently improves LoRA merging by up to 4.3% across several vision and\nlanguage benchmarks, including our new setting. We release our code at:\nhttps://github.com/gstoica27/KnOTS.\n", "link": "http://arxiv.org/abs/2410.19735v1", "date": "2024-10-25", "relevancy": 2.4932, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20merging%20with%20SVD%20to%20tie%20the%20Knots&body=Title%3A%20Model%20merging%20with%20SVD%20to%20tie%20the%20Knots%0AAuthor%3A%20George%20Stoica%20and%20Pratik%20Ramesh%20and%20Boglarka%20Ecsedi%20and%20Leshem%20Choshen%20and%20Judy%20Hoffman%0AAbstract%3A%20%20%20Recent%20model%20merging%20methods%20demonstrate%20that%20the%20parameters%20of%0Afully-finetuned%20models%20specializing%20in%20distinct%20tasks%20can%20be%20combined%20into%20one%0Amodel%20capable%20of%20solving%20all%20tasks%20without%20retraining.%20Yet%2C%20this%20success%20does%0Anot%20transfer%20well%20when%20merging%20LoRA%20finetuned%20models.%20We%20study%20this%20phenomenon%0Aand%20observe%20that%20the%20weights%20of%20LoRA%20finetuned%20models%20showcase%20a%20lower%20degree%0Aof%20alignment%20compared%20to%20their%20fully-finetuned%20counterparts.%20We%20hypothesize%0Athat%20improving%20this%20alignment%20is%20key%20to%20obtaining%20better%20LoRA%20model%20merges%2C%20and%0Apropose%20KnOTS%20to%20address%20this%20problem.%20KnOTS%20uses%20the%20SVD%20to%20jointly%20transform%0Athe%20weights%20of%20different%20LoRA%20models%20into%20an%20aligned%20space%2C%20where%20existing%0Amerging%20methods%20can%20be%20applied.%20In%20addition%2C%20we%20introduce%20a%20new%20benchmark%20that%0Aexplicitly%20evaluates%20whether%20merged%20models%20are%20general%20models.%20Notably%2C%20KnOTS%0Aconsistently%20improves%20LoRA%20merging%20by%20up%20to%204.3%25%20across%20several%20vision%20and%0Alanguage%20benchmarks%2C%20including%20our%20new%20setting.%20We%20release%20our%20code%20at%3A%0Ahttps%3A//github.com/gstoica27/KnOTS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520merging%2520with%2520SVD%2520to%2520tie%2520the%2520Knots%26entry.906535625%3DGeorge%2520Stoica%2520and%2520Pratik%2520Ramesh%2520and%2520Boglarka%2520Ecsedi%2520and%2520Leshem%2520Choshen%2520and%2520Judy%2520Hoffman%26entry.1292438233%3D%2520%2520Recent%2520model%2520merging%2520methods%2520demonstrate%2520that%2520the%2520parameters%2520of%250Afully-finetuned%2520models%2520specializing%2520in%2520distinct%2520tasks%2520can%2520be%2520combined%2520into%2520one%250Amodel%2520capable%2520of%2520solving%2520all%2520tasks%2520without%2520retraining.%2520Yet%252C%2520this%2520success%2520does%250Anot%2520transfer%2520well%2520when%2520merging%2520LoRA%2520finetuned%2520models.%2520We%2520study%2520this%2520phenomenon%250Aand%2520observe%2520that%2520the%2520weights%2520of%2520LoRA%2520finetuned%2520models%2520showcase%2520a%2520lower%2520degree%250Aof%2520alignment%2520compared%2520to%2520their%2520fully-finetuned%2520counterparts.%2520We%2520hypothesize%250Athat%2520improving%2520this%2520alignment%2520is%2520key%2520to%2520obtaining%2520better%2520LoRA%2520model%2520merges%252C%2520and%250Apropose%2520KnOTS%2520to%2520address%2520this%2520problem.%2520KnOTS%2520uses%2520the%2520SVD%2520to%2520jointly%2520transform%250Athe%2520weights%2520of%2520different%2520LoRA%2520models%2520into%2520an%2520aligned%2520space%252C%2520where%2520existing%250Amerging%2520methods%2520can%2520be%2520applied.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520that%250Aexplicitly%2520evaluates%2520whether%2520merged%2520models%2520are%2520general%2520models.%2520Notably%252C%2520KnOTS%250Aconsistently%2520improves%2520LoRA%2520merging%2520by%2520up%2520to%25204.3%2525%2520across%2520several%2520vision%2520and%250Alanguage%2520benchmarks%252C%2520including%2520our%2520new%2520setting.%2520We%2520release%2520our%2520code%2520at%253A%250Ahttps%253A//github.com/gstoica27/KnOTS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20merging%20with%20SVD%20to%20tie%20the%20Knots&entry.906535625=George%20Stoica%20and%20Pratik%20Ramesh%20and%20Boglarka%20Ecsedi%20and%20Leshem%20Choshen%20and%20Judy%20Hoffman&entry.1292438233=%20%20Recent%20model%20merging%20methods%20demonstrate%20that%20the%20parameters%20of%0Afully-finetuned%20models%20specializing%20in%20distinct%20tasks%20can%20be%20combined%20into%20one%0Amodel%20capable%20of%20solving%20all%20tasks%20without%20retraining.%20Yet%2C%20this%20success%20does%0Anot%20transfer%20well%20when%20merging%20LoRA%20finetuned%20models.%20We%20study%20this%20phenomenon%0Aand%20observe%20that%20the%20weights%20of%20LoRA%20finetuned%20models%20showcase%20a%20lower%20degree%0Aof%20alignment%20compared%20to%20their%20fully-finetuned%20counterparts.%20We%20hypothesize%0Athat%20improving%20this%20alignment%20is%20key%20to%20obtaining%20better%20LoRA%20model%20merges%2C%20and%0Apropose%20KnOTS%20to%20address%20this%20problem.%20KnOTS%20uses%20the%20SVD%20to%20jointly%20transform%0Athe%20weights%20of%20different%20LoRA%20models%20into%20an%20aligned%20space%2C%20where%20existing%0Amerging%20methods%20can%20be%20applied.%20In%20addition%2C%20we%20introduce%20a%20new%20benchmark%20that%0Aexplicitly%20evaluates%20whether%20merged%20models%20are%20general%20models.%20Notably%2C%20KnOTS%0Aconsistently%20improves%20LoRA%20merging%20by%20up%20to%204.3%25%20across%20several%20vision%20and%0Alanguage%20benchmarks%2C%20including%20our%20new%20setting.%20We%20release%20our%20code%20at%3A%0Ahttps%3A//github.com/gstoica27/KnOTS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19735v1&entry.124074799=Read"},
{"title": "Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical\n  Machine Reading Comprehension", "author": "Shubham Vatsal and Ayush Singh", "abstract": "  Large language models (LLMs) have shown remarkable performance on many tasks\nin different domains. However, their performance in closed-book biomedical\nmachine reading comprehension (MRC) has not been evaluated in depth. In this\nwork, we evaluate GPT on four closed-book biomedical MRC benchmarks. We\nexperiment with different conventional prompting techniques as well as\nintroduce our own novel prompting method. To solve some of the retrieval\nproblems inherent to LLMs, we propose a prompting strategy named Implicit\nRetrieval Augmented Generation (RAG) that alleviates the need for using vector\ndatabases to retrieve important chunks in traditional RAG setups. Moreover, we\nreport qualitative assessments on the natural language generation outputs from\nour approach. The results show that our new prompting technique is able to get\nthe best performance in two out of four datasets and ranks second in rest of\nthem. Experiments show that modern-day LLMs like GPT even in a zero-shot\nsetting can outperform supervised models, leading to new state-of-the-art\n(SoTA) results on two of the benchmarks.\n", "link": "http://arxiv.org/abs/2405.18682v2", "date": "2024-10-25", "relevancy": 2.4821, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20GPT%20Redefine%20Medical%20Understanding%3F%20Evaluating%20GPT%20on%20Biomedical%0A%20%20Machine%20Reading%20Comprehension&body=Title%3A%20Can%20GPT%20Redefine%20Medical%20Understanding%3F%20Evaluating%20GPT%20on%20Biomedical%0A%20%20Machine%20Reading%20Comprehension%0AAuthor%3A%20Shubham%20Vatsal%20and%20Ayush%20Singh%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20on%20many%20tasks%0Ain%20different%20domains.%20However%2C%20their%20performance%20in%20closed-book%20biomedical%0Amachine%20reading%20comprehension%20%28MRC%29%20has%20not%20been%20evaluated%20in%20depth.%20In%20this%0Awork%2C%20we%20evaluate%20GPT%20on%20four%20closed-book%20biomedical%20MRC%20benchmarks.%20We%0Aexperiment%20with%20different%20conventional%20prompting%20techniques%20as%20well%20as%0Aintroduce%20our%20own%20novel%20prompting%20method.%20To%20solve%20some%20of%20the%20retrieval%0Aproblems%20inherent%20to%20LLMs%2C%20we%20propose%20a%20prompting%20strategy%20named%20Implicit%0ARetrieval%20Augmented%20Generation%20%28RAG%29%20that%20alleviates%20the%20need%20for%20using%20vector%0Adatabases%20to%20retrieve%20important%20chunks%20in%20traditional%20RAG%20setups.%20Moreover%2C%20we%0Areport%20qualitative%20assessments%20on%20the%20natural%20language%20generation%20outputs%20from%0Aour%20approach.%20The%20results%20show%20that%20our%20new%20prompting%20technique%20is%20able%20to%20get%0Athe%20best%20performance%20in%20two%20out%20of%20four%20datasets%20and%20ranks%20second%20in%20rest%20of%0Athem.%20Experiments%20show%20that%20modern-day%20LLMs%20like%20GPT%20even%20in%20a%20zero-shot%0Asetting%20can%20outperform%20supervised%20models%2C%20leading%20to%20new%20state-of-the-art%0A%28SoTA%29%20results%20on%20two%20of%20the%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520GPT%2520Redefine%2520Medical%2520Understanding%253F%2520Evaluating%2520GPT%2520on%2520Biomedical%250A%2520%2520Machine%2520Reading%2520Comprehension%26entry.906535625%3DShubham%2520Vatsal%2520and%2520Ayush%2520Singh%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520on%2520many%2520tasks%250Ain%2520different%2520domains.%2520However%252C%2520their%2520performance%2520in%2520closed-book%2520biomedical%250Amachine%2520reading%2520comprehension%2520%2528MRC%2529%2520has%2520not%2520been%2520evaluated%2520in%2520depth.%2520In%2520this%250Awork%252C%2520we%2520evaluate%2520GPT%2520on%2520four%2520closed-book%2520biomedical%2520MRC%2520benchmarks.%2520We%250Aexperiment%2520with%2520different%2520conventional%2520prompting%2520techniques%2520as%2520well%2520as%250Aintroduce%2520our%2520own%2520novel%2520prompting%2520method.%2520To%2520solve%2520some%2520of%2520the%2520retrieval%250Aproblems%2520inherent%2520to%2520LLMs%252C%2520we%2520propose%2520a%2520prompting%2520strategy%2520named%2520Implicit%250ARetrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520that%2520alleviates%2520the%2520need%2520for%2520using%2520vector%250Adatabases%2520to%2520retrieve%2520important%2520chunks%2520in%2520traditional%2520RAG%2520setups.%2520Moreover%252C%2520we%250Areport%2520qualitative%2520assessments%2520on%2520the%2520natural%2520language%2520generation%2520outputs%2520from%250Aour%2520approach.%2520The%2520results%2520show%2520that%2520our%2520new%2520prompting%2520technique%2520is%2520able%2520to%2520get%250Athe%2520best%2520performance%2520in%2520two%2520out%2520of%2520four%2520datasets%2520and%2520ranks%2520second%2520in%2520rest%2520of%250Athem.%2520Experiments%2520show%2520that%2520modern-day%2520LLMs%2520like%2520GPT%2520even%2520in%2520a%2520zero-shot%250Asetting%2520can%2520outperform%2520supervised%2520models%252C%2520leading%2520to%2520new%2520state-of-the-art%250A%2528SoTA%2529%2520results%2520on%2520two%2520of%2520the%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20GPT%20Redefine%20Medical%20Understanding%3F%20Evaluating%20GPT%20on%20Biomedical%0A%20%20Machine%20Reading%20Comprehension&entry.906535625=Shubham%20Vatsal%20and%20Ayush%20Singh&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20on%20many%20tasks%0Ain%20different%20domains.%20However%2C%20their%20performance%20in%20closed-book%20biomedical%0Amachine%20reading%20comprehension%20%28MRC%29%20has%20not%20been%20evaluated%20in%20depth.%20In%20this%0Awork%2C%20we%20evaluate%20GPT%20on%20four%20closed-book%20biomedical%20MRC%20benchmarks.%20We%0Aexperiment%20with%20different%20conventional%20prompting%20techniques%20as%20well%20as%0Aintroduce%20our%20own%20novel%20prompting%20method.%20To%20solve%20some%20of%20the%20retrieval%0Aproblems%20inherent%20to%20LLMs%2C%20we%20propose%20a%20prompting%20strategy%20named%20Implicit%0ARetrieval%20Augmented%20Generation%20%28RAG%29%20that%20alleviates%20the%20need%20for%20using%20vector%0Adatabases%20to%20retrieve%20important%20chunks%20in%20traditional%20RAG%20setups.%20Moreover%2C%20we%0Areport%20qualitative%20assessments%20on%20the%20natural%20language%20generation%20outputs%20from%0Aour%20approach.%20The%20results%20show%20that%20our%20new%20prompting%20technique%20is%20able%20to%20get%0Athe%20best%20performance%20in%20two%20out%20of%20four%20datasets%20and%20ranks%20second%20in%20rest%20of%0Athem.%20Experiments%20show%20that%20modern-day%20LLMs%20like%20GPT%20even%20in%20a%20zero-shot%0Asetting%20can%20outperform%20supervised%20models%2C%20leading%20to%20new%20state-of-the-art%0A%28SoTA%29%20results%20on%20two%20of%20the%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18682v2&entry.124074799=Read"},
{"title": "Intelligent Understanding of Large Language Models in Traditional\n  Chinese Medicine Based on Prompt Engineering Framework", "author": "Yirui Chen and Qinyu Xiao and Jia Yi and Jing Chen and Mengyang Wang", "abstract": "  This paper explores the application of prompt engineering to enhance the\nperformance of large language models (LLMs) in the domain of Traditional\nChinese Medicine (TCM). We propose TCM-Prompt, a framework that integrates\nvarious pre-trained language models (PLMs), templates, tokenization, and\nverbalization methods, allowing researchers to easily construct and fine-tune\nmodels for specific TCM-related tasks. We conducted experiments on disease\nclassification, syndrome identification, herbal medicine recommendation, and\ngeneral NLP tasks, demonstrating the effectiveness and superiority of our\napproach compared to baseline methods. Our findings suggest that prompt\nengineering is a promising technique for improving the performance of LLMs in\nspecialized domains like TCM, with potential applications in digitalization,\nmodernization, and personalized medicine.\n", "link": "http://arxiv.org/abs/2410.19451v1", "date": "2024-10-25", "relevancy": 2.4547, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Understanding%20of%20Large%20Language%20Models%20in%20Traditional%0A%20%20Chinese%20Medicine%20Based%20on%20Prompt%20Engineering%20Framework&body=Title%3A%20Intelligent%20Understanding%20of%20Large%20Language%20Models%20in%20Traditional%0A%20%20Chinese%20Medicine%20Based%20on%20Prompt%20Engineering%20Framework%0AAuthor%3A%20Yirui%20Chen%20and%20Qinyu%20Xiao%20and%20Jia%20Yi%20and%20Jing%20Chen%20and%20Mengyang%20Wang%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20application%20of%20prompt%20engineering%20to%20enhance%20the%0Aperformance%20of%20large%20language%20models%20%28LLMs%29%20in%20the%20domain%20of%20Traditional%0AChinese%20Medicine%20%28TCM%29.%20We%20propose%20TCM-Prompt%2C%20a%20framework%20that%20integrates%0Avarious%20pre-trained%20language%20models%20%28PLMs%29%2C%20templates%2C%20tokenization%2C%20and%0Averbalization%20methods%2C%20allowing%20researchers%20to%20easily%20construct%20and%20fine-tune%0Amodels%20for%20specific%20TCM-related%20tasks.%20We%20conducted%20experiments%20on%20disease%0Aclassification%2C%20syndrome%20identification%2C%20herbal%20medicine%20recommendation%2C%20and%0Ageneral%20NLP%20tasks%2C%20demonstrating%20the%20effectiveness%20and%20superiority%20of%20our%0Aapproach%20compared%20to%20baseline%20methods.%20Our%20findings%20suggest%20that%20prompt%0Aengineering%20is%20a%20promising%20technique%20for%20improving%20the%20performance%20of%20LLMs%20in%0Aspecialized%20domains%20like%20TCM%2C%20with%20potential%20applications%20in%20digitalization%2C%0Amodernization%2C%20and%20personalized%20medicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Understanding%2520of%2520Large%2520Language%2520Models%2520in%2520Traditional%250A%2520%2520Chinese%2520Medicine%2520Based%2520on%2520Prompt%2520Engineering%2520Framework%26entry.906535625%3DYirui%2520Chen%2520and%2520Qinyu%2520Xiao%2520and%2520Jia%2520Yi%2520and%2520Jing%2520Chen%2520and%2520Mengyang%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520application%2520of%2520prompt%2520engineering%2520to%2520enhance%2520the%250Aperformance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520the%2520domain%2520of%2520Traditional%250AChinese%2520Medicine%2520%2528TCM%2529.%2520We%2520propose%2520TCM-Prompt%252C%2520a%2520framework%2520that%2520integrates%250Avarious%2520pre-trained%2520language%2520models%2520%2528PLMs%2529%252C%2520templates%252C%2520tokenization%252C%2520and%250Averbalization%2520methods%252C%2520allowing%2520researchers%2520to%2520easily%2520construct%2520and%2520fine-tune%250Amodels%2520for%2520specific%2520TCM-related%2520tasks.%2520We%2520conducted%2520experiments%2520on%2520disease%250Aclassification%252C%2520syndrome%2520identification%252C%2520herbal%2520medicine%2520recommendation%252C%2520and%250Ageneral%2520NLP%2520tasks%252C%2520demonstrating%2520the%2520effectiveness%2520and%2520superiority%2520of%2520our%250Aapproach%2520compared%2520to%2520baseline%2520methods.%2520Our%2520findings%2520suggest%2520that%2520prompt%250Aengineering%2520is%2520a%2520promising%2520technique%2520for%2520improving%2520the%2520performance%2520of%2520LLMs%2520in%250Aspecialized%2520domains%2520like%2520TCM%252C%2520with%2520potential%2520applications%2520in%2520digitalization%252C%250Amodernization%252C%2520and%2520personalized%2520medicine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Understanding%20of%20Large%20Language%20Models%20in%20Traditional%0A%20%20Chinese%20Medicine%20Based%20on%20Prompt%20Engineering%20Framework&entry.906535625=Yirui%20Chen%20and%20Qinyu%20Xiao%20and%20Jia%20Yi%20and%20Jing%20Chen%20and%20Mengyang%20Wang&entry.1292438233=%20%20This%20paper%20explores%20the%20application%20of%20prompt%20engineering%20to%20enhance%20the%0Aperformance%20of%20large%20language%20models%20%28LLMs%29%20in%20the%20domain%20of%20Traditional%0AChinese%20Medicine%20%28TCM%29.%20We%20propose%20TCM-Prompt%2C%20a%20framework%20that%20integrates%0Avarious%20pre-trained%20language%20models%20%28PLMs%29%2C%20templates%2C%20tokenization%2C%20and%0Averbalization%20methods%2C%20allowing%20researchers%20to%20easily%20construct%20and%20fine-tune%0Amodels%20for%20specific%20TCM-related%20tasks.%20We%20conducted%20experiments%20on%20disease%0Aclassification%2C%20syndrome%20identification%2C%20herbal%20medicine%20recommendation%2C%20and%0Ageneral%20NLP%20tasks%2C%20demonstrating%20the%20effectiveness%20and%20superiority%20of%20our%0Aapproach%20compared%20to%20baseline%20methods.%20Our%20findings%20suggest%20that%20prompt%0Aengineering%20is%20a%20promising%20technique%20for%20improving%20the%20performance%20of%20LLMs%20in%0Aspecialized%20domains%20like%20TCM%2C%20with%20potential%20applications%20in%20digitalization%2C%0Amodernization%2C%20and%20personalized%20medicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19451v1&entry.124074799=Read"},
{"title": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient\n  Finetuning of LLMs", "author": "Yifei Zhang and Hao Zhu and Aiwei Liu and Han Yu and Piotr Koniusz and Irwin King", "abstract": "  Fine-tuning Large Language Models (LLMs) has become a crucial technique for\nadapting pre-trained models to downstream tasks. However, the enormous size of\nLLMs poses significant challenges in terms of computational complexity and\nresource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution. However, there exists a gap between the practical performance of\nlow-rank adaptations and its theoretical optimum. In this work, we propose\neXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this\ngap by leveraging the power of ensemble learning. Inspired by gradient\nboosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations\nto refine model predictions. It achieves better performance than the standard\nLoRA, while enjoying the computational efficiency of rank-1 adaptations. We\nprovide theoretical analysis to show the convergence and optimality of our\napproach, and conduct extensive experiments on a range of natural language\nprocessing tasks. The results demonstrate that XGBLoRA consistently outperforms\nstandard LoRA and achieves performance comparable to full fine-tuning with\nsignificantly fewer trainable parameters. This work advances\nparameter-efficient fine-tuning for LLMs, and offers a promising solution for\nadapting LLMs to downstream tasks while optimizing performance and efficiency.\n", "link": "http://arxiv.org/abs/2410.19694v1", "date": "2024-10-25", "relevancy": 2.4298, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5037}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4839}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Extreme%20Gradient%20Boost%20Rank-1%20Adaption%20for%20Efficient%0A%20%20Finetuning%20of%20LLMs&body=Title%3A%20Less%20is%20More%3A%20Extreme%20Gradient%20Boost%20Rank-1%20Adaption%20for%20Efficient%0A%20%20Finetuning%20of%20LLMs%0AAuthor%3A%20Yifei%20Zhang%20and%20Hao%20Zhu%20and%20Aiwei%20Liu%20and%20Han%20Yu%20and%20Piotr%20Koniusz%20and%20Irwin%20King%0AAbstract%3A%20%20%20Fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20has%20become%20a%20crucial%20technique%20for%0Aadapting%20pre-trained%20models%20to%20downstream%20tasks.%20However%2C%20the%20enormous%20size%20of%0ALLMs%20poses%20significant%20challenges%20in%20terms%20of%20computational%20complexity%20and%0Aresource%20requirements.%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20emerged%20as%20a%20promising%0Asolution.%20However%2C%20there%20exists%20a%20gap%20between%20the%20practical%20performance%20of%0Alow-rank%20adaptations%20and%20its%20theoretical%20optimum.%20In%20this%20work%2C%20we%20propose%0AeXtreme%20Gradient%20Boosting%20LoRA%20%28XGBLoRA%29%2C%20a%20novel%20framework%20that%20bridges%20this%0Agap%20by%20leveraging%20the%20power%20of%20ensemble%20learning.%20Inspired%20by%20gradient%0Aboosting%2C%20XGBLoRA%20iteratively%20learns%20and%20merges%20a%20sequence%20of%20LoRA%20adaptations%0Ato%20refine%20model%20predictions.%20It%20achieves%20better%20performance%20than%20the%20standard%0ALoRA%2C%20while%20enjoying%20the%20computational%20efficiency%20of%20rank-1%20adaptations.%20We%0Aprovide%20theoretical%20analysis%20to%20show%20the%20convergence%20and%20optimality%20of%20our%0Aapproach%2C%20and%20conduct%20extensive%20experiments%20on%20a%20range%20of%20natural%20language%0Aprocessing%20tasks.%20The%20results%20demonstrate%20that%20XGBLoRA%20consistently%20outperforms%0Astandard%20LoRA%20and%20achieves%20performance%20comparable%20to%20full%20fine-tuning%20with%0Asignificantly%20fewer%20trainable%20parameters.%20This%20work%20advances%0Aparameter-efficient%20fine-tuning%20for%20LLMs%2C%20and%20offers%20a%20promising%20solution%20for%0Aadapting%20LLMs%20to%20downstream%20tasks%20while%20optimizing%20performance%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Extreme%2520Gradient%2520Boost%2520Rank-1%2520Adaption%2520for%2520Efficient%250A%2520%2520Finetuning%2520of%2520LLMs%26entry.906535625%3DYifei%2520Zhang%2520and%2520Hao%2520Zhu%2520and%2520Aiwei%2520Liu%2520and%2520Han%2520Yu%2520and%2520Piotr%2520Koniusz%2520and%2520Irwin%2520King%26entry.1292438233%3D%2520%2520Fine-tuning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520become%2520a%2520crucial%2520technique%2520for%250Aadapting%2520pre-trained%2520models%2520to%2520downstream%2520tasks.%2520However%252C%2520the%2520enormous%2520size%2520of%250ALLMs%2520poses%2520significant%2520challenges%2520in%2520terms%2520of%2520computational%2520complexity%2520and%250Aresource%2520requirements.%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520emerged%2520as%2520a%2520promising%250Asolution.%2520However%252C%2520there%2520exists%2520a%2520gap%2520between%2520the%2520practical%2520performance%2520of%250Alow-rank%2520adaptations%2520and%2520its%2520theoretical%2520optimum.%2520In%2520this%2520work%252C%2520we%2520propose%250AeXtreme%2520Gradient%2520Boosting%2520LoRA%2520%2528XGBLoRA%2529%252C%2520a%2520novel%2520framework%2520that%2520bridges%2520this%250Agap%2520by%2520leveraging%2520the%2520power%2520of%2520ensemble%2520learning.%2520Inspired%2520by%2520gradient%250Aboosting%252C%2520XGBLoRA%2520iteratively%2520learns%2520and%2520merges%2520a%2520sequence%2520of%2520LoRA%2520adaptations%250Ato%2520refine%2520model%2520predictions.%2520It%2520achieves%2520better%2520performance%2520than%2520the%2520standard%250ALoRA%252C%2520while%2520enjoying%2520the%2520computational%2520efficiency%2520of%2520rank-1%2520adaptations.%2520We%250Aprovide%2520theoretical%2520analysis%2520to%2520show%2520the%2520convergence%2520and%2520optimality%2520of%2520our%250Aapproach%252C%2520and%2520conduct%2520extensive%2520experiments%2520on%2520a%2520range%2520of%2520natural%2520language%250Aprocessing%2520tasks.%2520The%2520results%2520demonstrate%2520that%2520XGBLoRA%2520consistently%2520outperforms%250Astandard%2520LoRA%2520and%2520achieves%2520performance%2520comparable%2520to%2520full%2520fine-tuning%2520with%250Asignificantly%2520fewer%2520trainable%2520parameters.%2520This%2520work%2520advances%250Aparameter-efficient%2520fine-tuning%2520for%2520LLMs%252C%2520and%2520offers%2520a%2520promising%2520solution%2520for%250Aadapting%2520LLMs%2520to%2520downstream%2520tasks%2520while%2520optimizing%2520performance%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Extreme%20Gradient%20Boost%20Rank-1%20Adaption%20for%20Efficient%0A%20%20Finetuning%20of%20LLMs&entry.906535625=Yifei%20Zhang%20and%20Hao%20Zhu%20and%20Aiwei%20Liu%20and%20Han%20Yu%20and%20Piotr%20Koniusz%20and%20Irwin%20King&entry.1292438233=%20%20Fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20has%20become%20a%20crucial%20technique%20for%0Aadapting%20pre-trained%20models%20to%20downstream%20tasks.%20However%2C%20the%20enormous%20size%20of%0ALLMs%20poses%20significant%20challenges%20in%20terms%20of%20computational%20complexity%20and%0Aresource%20requirements.%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20emerged%20as%20a%20promising%0Asolution.%20However%2C%20there%20exists%20a%20gap%20between%20the%20practical%20performance%20of%0Alow-rank%20adaptations%20and%20its%20theoretical%20optimum.%20In%20this%20work%2C%20we%20propose%0AeXtreme%20Gradient%20Boosting%20LoRA%20%28XGBLoRA%29%2C%20a%20novel%20framework%20that%20bridges%20this%0Agap%20by%20leveraging%20the%20power%20of%20ensemble%20learning.%20Inspired%20by%20gradient%0Aboosting%2C%20XGBLoRA%20iteratively%20learns%20and%20merges%20a%20sequence%20of%20LoRA%20adaptations%0Ato%20refine%20model%20predictions.%20It%20achieves%20better%20performance%20than%20the%20standard%0ALoRA%2C%20while%20enjoying%20the%20computational%20efficiency%20of%20rank-1%20adaptations.%20We%0Aprovide%20theoretical%20analysis%20to%20show%20the%20convergence%20and%20optimality%20of%20our%0Aapproach%2C%20and%20conduct%20extensive%20experiments%20on%20a%20range%20of%20natural%20language%0Aprocessing%20tasks.%20The%20results%20demonstrate%20that%20XGBLoRA%20consistently%20outperforms%0Astandard%20LoRA%20and%20achieves%20performance%20comparable%20to%20full%20fine-tuning%20with%0Asignificantly%20fewer%20trainable%20parameters.%20This%20work%20advances%0Aparameter-efficient%20fine-tuning%20for%20LLMs%2C%20and%20offers%20a%20promising%20solution%20for%0Aadapting%20LLMs%20to%20downstream%20tasks%20while%20optimizing%20performance%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19694v1&entry.124074799=Read"},
{"title": "Sparse Decomposition of Graph Neural Networks", "author": "Yaochen Hu and Mai Zeng and Ge Zhang and Pavel Rumiantsev and Liheng Ma and Yingxue Zhang and Mark Coates", "abstract": "  Graph Neural Networks (GNN) exhibit superior performance in graph\nrepresentation learning, but their inference cost can be high, due to an\naggregation operation that can require a memory fetch for a very large number\nof nodes. This inference cost is the major obstacle to deploying GNN models\nwith \\emph{online prediction} to reflect the potentially dynamic node features.\nTo address this, we propose an approach to reduce the number of nodes that are\nincluded during aggregation. We achieve this through a sparse decomposition,\nlearning to approximate node representations using a weighted sum of linearly\ntransformed features of a carefully selected subset of nodes within the\nextended neighbourhood. The approach achieves linear complexity with respect to\nthe average node degree and the number of layers in the graph neural network.\nWe introduce an algorithm to compute the optimal parameters for the sparse\ndecomposition, ensuring an accurate approximation of the original GNN model,\nand present effective strategies to reduce the training time and improve the\nlearning process. We demonstrate via extensive experiments that our method\noutperforms other baselines designed for inference speedup, achieving\nsignificant accuracy gains with comparable inference times for both node\nclassification and spatio-temporal forecasting tasks.\n", "link": "http://arxiv.org/abs/2410.19723v1", "date": "2024-10-25", "relevancy": 2.4168, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4842}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4831}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Decomposition%20of%20Graph%20Neural%20Networks&body=Title%3A%20Sparse%20Decomposition%20of%20Graph%20Neural%20Networks%0AAuthor%3A%20Yaochen%20Hu%20and%20Mai%20Zeng%20and%20Ge%20Zhang%20and%20Pavel%20Rumiantsev%20and%20Liheng%20Ma%20and%20Yingxue%20Zhang%20and%20Mark%20Coates%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNN%29%20exhibit%20superior%20performance%20in%20graph%0Arepresentation%20learning%2C%20but%20their%20inference%20cost%20can%20be%20high%2C%20due%20to%20an%0Aaggregation%20operation%20that%20can%20require%20a%20memory%20fetch%20for%20a%20very%20large%20number%0Aof%20nodes.%20This%20inference%20cost%20is%20the%20major%20obstacle%20to%20deploying%20GNN%20models%0Awith%20%5Cemph%7Bonline%20prediction%7D%20to%20reflect%20the%20potentially%20dynamic%20node%20features.%0ATo%20address%20this%2C%20we%20propose%20an%20approach%20to%20reduce%20the%20number%20of%20nodes%20that%20are%0Aincluded%20during%20aggregation.%20We%20achieve%20this%20through%20a%20sparse%20decomposition%2C%0Alearning%20to%20approximate%20node%20representations%20using%20a%20weighted%20sum%20of%20linearly%0Atransformed%20features%20of%20a%20carefully%20selected%20subset%20of%20nodes%20within%20the%0Aextended%20neighbourhood.%20The%20approach%20achieves%20linear%20complexity%20with%20respect%20to%0Athe%20average%20node%20degree%20and%20the%20number%20of%20layers%20in%20the%20graph%20neural%20network.%0AWe%20introduce%20an%20algorithm%20to%20compute%20the%20optimal%20parameters%20for%20the%20sparse%0Adecomposition%2C%20ensuring%20an%20accurate%20approximation%20of%20the%20original%20GNN%20model%2C%0Aand%20present%20effective%20strategies%20to%20reduce%20the%20training%20time%20and%20improve%20the%0Alearning%20process.%20We%20demonstrate%20via%20extensive%20experiments%20that%20our%20method%0Aoutperforms%20other%20baselines%20designed%20for%20inference%20speedup%2C%20achieving%0Asignificant%20accuracy%20gains%20with%20comparable%20inference%20times%20for%20both%20node%0Aclassification%20and%20spatio-temporal%20forecasting%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Decomposition%2520of%2520Graph%2520Neural%2520Networks%26entry.906535625%3DYaochen%2520Hu%2520and%2520Mai%2520Zeng%2520and%2520Ge%2520Zhang%2520and%2520Pavel%2520Rumiantsev%2520and%2520Liheng%2520Ma%2520and%2520Yingxue%2520Zhang%2520and%2520Mark%2520Coates%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNN%2529%2520exhibit%2520superior%2520performance%2520in%2520graph%250Arepresentation%2520learning%252C%2520but%2520their%2520inference%2520cost%2520can%2520be%2520high%252C%2520due%2520to%2520an%250Aaggregation%2520operation%2520that%2520can%2520require%2520a%2520memory%2520fetch%2520for%2520a%2520very%2520large%2520number%250Aof%2520nodes.%2520This%2520inference%2520cost%2520is%2520the%2520major%2520obstacle%2520to%2520deploying%2520GNN%2520models%250Awith%2520%255Cemph%257Bonline%2520prediction%257D%2520to%2520reflect%2520the%2520potentially%2520dynamic%2520node%2520features.%250ATo%2520address%2520this%252C%2520we%2520propose%2520an%2520approach%2520to%2520reduce%2520the%2520number%2520of%2520nodes%2520that%2520are%250Aincluded%2520during%2520aggregation.%2520We%2520achieve%2520this%2520through%2520a%2520sparse%2520decomposition%252C%250Alearning%2520to%2520approximate%2520node%2520representations%2520using%2520a%2520weighted%2520sum%2520of%2520linearly%250Atransformed%2520features%2520of%2520a%2520carefully%2520selected%2520subset%2520of%2520nodes%2520within%2520the%250Aextended%2520neighbourhood.%2520The%2520approach%2520achieves%2520linear%2520complexity%2520with%2520respect%2520to%250Athe%2520average%2520node%2520degree%2520and%2520the%2520number%2520of%2520layers%2520in%2520the%2520graph%2520neural%2520network.%250AWe%2520introduce%2520an%2520algorithm%2520to%2520compute%2520the%2520optimal%2520parameters%2520for%2520the%2520sparse%250Adecomposition%252C%2520ensuring%2520an%2520accurate%2520approximation%2520of%2520the%2520original%2520GNN%2520model%252C%250Aand%2520present%2520effective%2520strategies%2520to%2520reduce%2520the%2520training%2520time%2520and%2520improve%2520the%250Alearning%2520process.%2520We%2520demonstrate%2520via%2520extensive%2520experiments%2520that%2520our%2520method%250Aoutperforms%2520other%2520baselines%2520designed%2520for%2520inference%2520speedup%252C%2520achieving%250Asignificant%2520accuracy%2520gains%2520with%2520comparable%2520inference%2520times%2520for%2520both%2520node%250Aclassification%2520and%2520spatio-temporal%2520forecasting%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Decomposition%20of%20Graph%20Neural%20Networks&entry.906535625=Yaochen%20Hu%20and%20Mai%20Zeng%20and%20Ge%20Zhang%20and%20Pavel%20Rumiantsev%20and%20Liheng%20Ma%20and%20Yingxue%20Zhang%20and%20Mark%20Coates&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNN%29%20exhibit%20superior%20performance%20in%20graph%0Arepresentation%20learning%2C%20but%20their%20inference%20cost%20can%20be%20high%2C%20due%20to%20an%0Aaggregation%20operation%20that%20can%20require%20a%20memory%20fetch%20for%20a%20very%20large%20number%0Aof%20nodes.%20This%20inference%20cost%20is%20the%20major%20obstacle%20to%20deploying%20GNN%20models%0Awith%20%5Cemph%7Bonline%20prediction%7D%20to%20reflect%20the%20potentially%20dynamic%20node%20features.%0ATo%20address%20this%2C%20we%20propose%20an%20approach%20to%20reduce%20the%20number%20of%20nodes%20that%20are%0Aincluded%20during%20aggregation.%20We%20achieve%20this%20through%20a%20sparse%20decomposition%2C%0Alearning%20to%20approximate%20node%20representations%20using%20a%20weighted%20sum%20of%20linearly%0Atransformed%20features%20of%20a%20carefully%20selected%20subset%20of%20nodes%20within%20the%0Aextended%20neighbourhood.%20The%20approach%20achieves%20linear%20complexity%20with%20respect%20to%0Athe%20average%20node%20degree%20and%20the%20number%20of%20layers%20in%20the%20graph%20neural%20network.%0AWe%20introduce%20an%20algorithm%20to%20compute%20the%20optimal%20parameters%20for%20the%20sparse%0Adecomposition%2C%20ensuring%20an%20accurate%20approximation%20of%20the%20original%20GNN%20model%2C%0Aand%20present%20effective%20strategies%20to%20reduce%20the%20training%20time%20and%20improve%20the%0Alearning%20process.%20We%20demonstrate%20via%20extensive%20experiments%20that%20our%20method%0Aoutperforms%20other%20baselines%20designed%20for%20inference%20speedup%2C%20achieving%0Asignificant%20accuracy%20gains%20with%20comparable%20inference%20times%20for%20both%20node%0Aclassification%20and%20spatio-temporal%20forecasting%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19723v1&entry.124074799=Read"},
{"title": "Improving Object Detection via Local-global Contrastive Learning", "author": "Danai Triantafyllidou and Sarah Parisot and Ales Leonardis and Steven McDonagh", "abstract": "  Visual domain gaps often impact object detection performance. Image-to-image\ntranslation can mitigate this effect, where contrastive approaches enable\nlearning of the image-to-image mapping under unsupervised regimes. However,\nexisting methods often fail to handle content-rich scenes with multiple object\ninstances, which manifests in unsatisfactory detection performance. Sensitivity\nto such instance-level content is typically only gained through object\nannotations, which can be expensive to obtain. Towards addressing this issue,\nwe present a novel image-to-image translation method that specifically targets\ncross-domain object detection. We formulate our approach as a contrastive\nlearning framework with an inductive prior that optimises the appearance of\nobject instances through spatial attention masks, implicitly delineating the\nscene into foreground regions associated with the target object instances and\nbackground non-object regions. Instead of relying on object annotations to\nexplicitly account for object instances during translation, our approach learns\nto represent objects by contrasting local-global information. This affords\ninvestigation of an under-explored challenge: obtaining performant detection,\nunder domain shifts, without relying on object annotations nor detector model\nfine-tuning. We experiment with multiple cross-domain object detection settings\nacross three challenging benchmarks and report state-of-the-art performance.\nProject page: https://local-global-detection.github.io\n", "link": "http://arxiv.org/abs/2410.05058v2", "date": "2024-10-25", "relevancy": 2.4023, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5792}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Object%20Detection%20via%20Local-global%20Contrastive%20Learning&body=Title%3A%20Improving%20Object%20Detection%20via%20Local-global%20Contrastive%20Learning%0AAuthor%3A%20Danai%20Triantafyllidou%20and%20Sarah%20Parisot%20and%20Ales%20Leonardis%20and%20Steven%20McDonagh%0AAbstract%3A%20%20%20Visual%20domain%20gaps%20often%20impact%20object%20detection%20performance.%20Image-to-image%0Atranslation%20can%20mitigate%20this%20effect%2C%20where%20contrastive%20approaches%20enable%0Alearning%20of%20the%20image-to-image%20mapping%20under%20unsupervised%20regimes.%20However%2C%0Aexisting%20methods%20often%20fail%20to%20handle%20content-rich%20scenes%20with%20multiple%20object%0Ainstances%2C%20which%20manifests%20in%20unsatisfactory%20detection%20performance.%20Sensitivity%0Ato%20such%20instance-level%20content%20is%20typically%20only%20gained%20through%20object%0Aannotations%2C%20which%20can%20be%20expensive%20to%20obtain.%20Towards%20addressing%20this%20issue%2C%0Awe%20present%20a%20novel%20image-to-image%20translation%20method%20that%20specifically%20targets%0Across-domain%20object%20detection.%20We%20formulate%20our%20approach%20as%20a%20contrastive%0Alearning%20framework%20with%20an%20inductive%20prior%20that%20optimises%20the%20appearance%20of%0Aobject%20instances%20through%20spatial%20attention%20masks%2C%20implicitly%20delineating%20the%0Ascene%20into%20foreground%20regions%20associated%20with%20the%20target%20object%20instances%20and%0Abackground%20non-object%20regions.%20Instead%20of%20relying%20on%20object%20annotations%20to%0Aexplicitly%20account%20for%20object%20instances%20during%20translation%2C%20our%20approach%20learns%0Ato%20represent%20objects%20by%20contrasting%20local-global%20information.%20This%20affords%0Ainvestigation%20of%20an%20under-explored%20challenge%3A%20obtaining%20performant%20detection%2C%0Aunder%20domain%20shifts%2C%20without%20relying%20on%20object%20annotations%20nor%20detector%20model%0Afine-tuning.%20We%20experiment%20with%20multiple%20cross-domain%20object%20detection%20settings%0Aacross%20three%20challenging%20benchmarks%20and%20report%20state-of-the-art%20performance.%0AProject%20page%3A%20https%3A//local-global-detection.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05058v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Object%2520Detection%2520via%2520Local-global%2520Contrastive%2520Learning%26entry.906535625%3DDanai%2520Triantafyllidou%2520and%2520Sarah%2520Parisot%2520and%2520Ales%2520Leonardis%2520and%2520Steven%2520McDonagh%26entry.1292438233%3D%2520%2520Visual%2520domain%2520gaps%2520often%2520impact%2520object%2520detection%2520performance.%2520Image-to-image%250Atranslation%2520can%2520mitigate%2520this%2520effect%252C%2520where%2520contrastive%2520approaches%2520enable%250Alearning%2520of%2520the%2520image-to-image%2520mapping%2520under%2520unsupervised%2520regimes.%2520However%252C%250Aexisting%2520methods%2520often%2520fail%2520to%2520handle%2520content-rich%2520scenes%2520with%2520multiple%2520object%250Ainstances%252C%2520which%2520manifests%2520in%2520unsatisfactory%2520detection%2520performance.%2520Sensitivity%250Ato%2520such%2520instance-level%2520content%2520is%2520typically%2520only%2520gained%2520through%2520object%250Aannotations%252C%2520which%2520can%2520be%2520expensive%2520to%2520obtain.%2520Towards%2520addressing%2520this%2520issue%252C%250Awe%2520present%2520a%2520novel%2520image-to-image%2520translation%2520method%2520that%2520specifically%2520targets%250Across-domain%2520object%2520detection.%2520We%2520formulate%2520our%2520approach%2520as%2520a%2520contrastive%250Alearning%2520framework%2520with%2520an%2520inductive%2520prior%2520that%2520optimises%2520the%2520appearance%2520of%250Aobject%2520instances%2520through%2520spatial%2520attention%2520masks%252C%2520implicitly%2520delineating%2520the%250Ascene%2520into%2520foreground%2520regions%2520associated%2520with%2520the%2520target%2520object%2520instances%2520and%250Abackground%2520non-object%2520regions.%2520Instead%2520of%2520relying%2520on%2520object%2520annotations%2520to%250Aexplicitly%2520account%2520for%2520object%2520instances%2520during%2520translation%252C%2520our%2520approach%2520learns%250Ato%2520represent%2520objects%2520by%2520contrasting%2520local-global%2520information.%2520This%2520affords%250Ainvestigation%2520of%2520an%2520under-explored%2520challenge%253A%2520obtaining%2520performant%2520detection%252C%250Aunder%2520domain%2520shifts%252C%2520without%2520relying%2520on%2520object%2520annotations%2520nor%2520detector%2520model%250Afine-tuning.%2520We%2520experiment%2520with%2520multiple%2520cross-domain%2520object%2520detection%2520settings%250Aacross%2520three%2520challenging%2520benchmarks%2520and%2520report%2520state-of-the-art%2520performance.%250AProject%2520page%253A%2520https%253A//local-global-detection.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05058v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Object%20Detection%20via%20Local-global%20Contrastive%20Learning&entry.906535625=Danai%20Triantafyllidou%20and%20Sarah%20Parisot%20and%20Ales%20Leonardis%20and%20Steven%20McDonagh&entry.1292438233=%20%20Visual%20domain%20gaps%20often%20impact%20object%20detection%20performance.%20Image-to-image%0Atranslation%20can%20mitigate%20this%20effect%2C%20where%20contrastive%20approaches%20enable%0Alearning%20of%20the%20image-to-image%20mapping%20under%20unsupervised%20regimes.%20However%2C%0Aexisting%20methods%20often%20fail%20to%20handle%20content-rich%20scenes%20with%20multiple%20object%0Ainstances%2C%20which%20manifests%20in%20unsatisfactory%20detection%20performance.%20Sensitivity%0Ato%20such%20instance-level%20content%20is%20typically%20only%20gained%20through%20object%0Aannotations%2C%20which%20can%20be%20expensive%20to%20obtain.%20Towards%20addressing%20this%20issue%2C%0Awe%20present%20a%20novel%20image-to-image%20translation%20method%20that%20specifically%20targets%0Across-domain%20object%20detection.%20We%20formulate%20our%20approach%20as%20a%20contrastive%0Alearning%20framework%20with%20an%20inductive%20prior%20that%20optimises%20the%20appearance%20of%0Aobject%20instances%20through%20spatial%20attention%20masks%2C%20implicitly%20delineating%20the%0Ascene%20into%20foreground%20regions%20associated%20with%20the%20target%20object%20instances%20and%0Abackground%20non-object%20regions.%20Instead%20of%20relying%20on%20object%20annotations%20to%0Aexplicitly%20account%20for%20object%20instances%20during%20translation%2C%20our%20approach%20learns%0Ato%20represent%20objects%20by%20contrasting%20local-global%20information.%20This%20affords%0Ainvestigation%20of%20an%20under-explored%20challenge%3A%20obtaining%20performant%20detection%2C%0Aunder%20domain%20shifts%2C%20without%20relying%20on%20object%20annotations%20nor%20detector%20model%0Afine-tuning.%20We%20experiment%20with%20multiple%20cross-domain%20object%20detection%20settings%0Aacross%20three%20challenging%20benchmarks%20and%20report%20state-of-the-art%20performance.%0AProject%20page%3A%20https%3A//local-global-detection.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05058v2&entry.124074799=Read"},
{"title": "Robotic Learning in your Backyard: A Neural Simulator from Open Source\n  Components", "author": "Liyou Zhou and Oleg Sinavski and Athanasios Polydoros", "abstract": "  The emergence of 3D Gaussian Splatting for fast and high-quality novel view\nsynthesize has opened up the possibility to construct photo-realistic\nsimulations from video for robotic reinforcement learning. While the approach\nhas been demonstrated in several research papers, the software tools used to\nbuild such a simulator remain unavailable or proprietary. We present SplatGym,\nan open source neural simulator for training data-driven robotic control\npolicies. The simulator creates a photorealistic virtual environment from a\nsingle video. It supports ego camera view generation, collision detection, and\nvirtual object in-painting. We demonstrate training several visual navigation\npolicies via reinforcement learning. SplatGym represents a notable first step\ntowards an open-source general-purpose neural environment for robotic learning.\nIt broadens the range of applications that can effectively utilise\nreinforcement learning by providing convenient and unrestricted tooling, and by\neliminating the need for the manual development of conventional 3D\nenvironments.\n", "link": "http://arxiv.org/abs/2410.19564v1", "date": "2024-10-25", "relevancy": 2.3998, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6362}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5854}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20Learning%20in%20your%20Backyard%3A%20A%20Neural%20Simulator%20from%20Open%20Source%0A%20%20Components&body=Title%3A%20Robotic%20Learning%20in%20your%20Backyard%3A%20A%20Neural%20Simulator%20from%20Open%20Source%0A%20%20Components%0AAuthor%3A%20Liyou%20Zhou%20and%20Oleg%20Sinavski%20and%20Athanasios%20Polydoros%0AAbstract%3A%20%20%20The%20emergence%20of%203D%20Gaussian%20Splatting%20for%20fast%20and%20high-quality%20novel%20view%0Asynthesize%20has%20opened%20up%20the%20possibility%20to%20construct%20photo-realistic%0Asimulations%20from%20video%20for%20robotic%20reinforcement%20learning.%20While%20the%20approach%0Ahas%20been%20demonstrated%20in%20several%20research%20papers%2C%20the%20software%20tools%20used%20to%0Abuild%20such%20a%20simulator%20remain%20unavailable%20or%20proprietary.%20We%20present%20SplatGym%2C%0Aan%20open%20source%20neural%20simulator%20for%20training%20data-driven%20robotic%20control%0Apolicies.%20The%20simulator%20creates%20a%20photorealistic%20virtual%20environment%20from%20a%0Asingle%20video.%20It%20supports%20ego%20camera%20view%20generation%2C%20collision%20detection%2C%20and%0Avirtual%20object%20in-painting.%20We%20demonstrate%20training%20several%20visual%20navigation%0Apolicies%20via%20reinforcement%20learning.%20SplatGym%20represents%20a%20notable%20first%20step%0Atowards%20an%20open-source%20general-purpose%20neural%20environment%20for%20robotic%20learning.%0AIt%20broadens%20the%20range%20of%20applications%20that%20can%20effectively%20utilise%0Areinforcement%20learning%20by%20providing%20convenient%20and%20unrestricted%20tooling%2C%20and%20by%0Aeliminating%20the%20need%20for%20the%20manual%20development%20of%20conventional%203D%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520Learning%2520in%2520your%2520Backyard%253A%2520A%2520Neural%2520Simulator%2520from%2520Open%2520Source%250A%2520%2520Components%26entry.906535625%3DLiyou%2520Zhou%2520and%2520Oleg%2520Sinavski%2520and%2520Athanasios%2520Polydoros%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%25203D%2520Gaussian%2520Splatting%2520for%2520fast%2520and%2520high-quality%2520novel%2520view%250Asynthesize%2520has%2520opened%2520up%2520the%2520possibility%2520to%2520construct%2520photo-realistic%250Asimulations%2520from%2520video%2520for%2520robotic%2520reinforcement%2520learning.%2520While%2520the%2520approach%250Ahas%2520been%2520demonstrated%2520in%2520several%2520research%2520papers%252C%2520the%2520software%2520tools%2520used%2520to%250Abuild%2520such%2520a%2520simulator%2520remain%2520unavailable%2520or%2520proprietary.%2520We%2520present%2520SplatGym%252C%250Aan%2520open%2520source%2520neural%2520simulator%2520for%2520training%2520data-driven%2520robotic%2520control%250Apolicies.%2520The%2520simulator%2520creates%2520a%2520photorealistic%2520virtual%2520environment%2520from%2520a%250Asingle%2520video.%2520It%2520supports%2520ego%2520camera%2520view%2520generation%252C%2520collision%2520detection%252C%2520and%250Avirtual%2520object%2520in-painting.%2520We%2520demonstrate%2520training%2520several%2520visual%2520navigation%250Apolicies%2520via%2520reinforcement%2520learning.%2520SplatGym%2520represents%2520a%2520notable%2520first%2520step%250Atowards%2520an%2520open-source%2520general-purpose%2520neural%2520environment%2520for%2520robotic%2520learning.%250AIt%2520broadens%2520the%2520range%2520of%2520applications%2520that%2520can%2520effectively%2520utilise%250Areinforcement%2520learning%2520by%2520providing%2520convenient%2520and%2520unrestricted%2520tooling%252C%2520and%2520by%250Aeliminating%2520the%2520need%2520for%2520the%2520manual%2520development%2520of%2520conventional%25203D%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20Learning%20in%20your%20Backyard%3A%20A%20Neural%20Simulator%20from%20Open%20Source%0A%20%20Components&entry.906535625=Liyou%20Zhou%20and%20Oleg%20Sinavski%20and%20Athanasios%20Polydoros&entry.1292438233=%20%20The%20emergence%20of%203D%20Gaussian%20Splatting%20for%20fast%20and%20high-quality%20novel%20view%0Asynthesize%20has%20opened%20up%20the%20possibility%20to%20construct%20photo-realistic%0Asimulations%20from%20video%20for%20robotic%20reinforcement%20learning.%20While%20the%20approach%0Ahas%20been%20demonstrated%20in%20several%20research%20papers%2C%20the%20software%20tools%20used%20to%0Abuild%20such%20a%20simulator%20remain%20unavailable%20or%20proprietary.%20We%20present%20SplatGym%2C%0Aan%20open%20source%20neural%20simulator%20for%20training%20data-driven%20robotic%20control%0Apolicies.%20The%20simulator%20creates%20a%20photorealistic%20virtual%20environment%20from%20a%0Asingle%20video.%20It%20supports%20ego%20camera%20view%20generation%2C%20collision%20detection%2C%20and%0Avirtual%20object%20in-painting.%20We%20demonstrate%20training%20several%20visual%20navigation%0Apolicies%20via%20reinforcement%20learning.%20SplatGym%20represents%20a%20notable%20first%20step%0Atowards%20an%20open-source%20general-purpose%20neural%20environment%20for%20robotic%20learning.%0AIt%20broadens%20the%20range%20of%20applications%20that%20can%20effectively%20utilise%0Areinforcement%20learning%20by%20providing%20convenient%20and%20unrestricted%20tooling%2C%20and%20by%0Aeliminating%20the%20need%20for%20the%20manual%20development%20of%20conventional%203D%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19564v1&entry.124074799=Read"},
{"title": "On Designing Effective RL Reward at Training Time for LLM Reasoning", "author": "Jiaxuan Gao and Shusheng Xu and Wenjie Ye and Weilin Liu and Chuyi He and Wei Fu and Zhiyu Mei and Guangju Wang and Yi Wu", "abstract": "  Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.\n", "link": "http://arxiv.org/abs/2410.15115v2", "date": "2024-10-25", "relevancy": 2.3901, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Designing%20Effective%20RL%20Reward%20at%20Training%20Time%20for%20LLM%20Reasoning&body=Title%3A%20On%20Designing%20Effective%20RL%20Reward%20at%20Training%20Time%20for%20LLM%20Reasoning%0AAuthor%3A%20Jiaxuan%20Gao%20and%20Shusheng%20Xu%20and%20Wenjie%20Ye%20and%20Weilin%20Liu%20and%20Chuyi%20He%20and%20Wei%20Fu%20and%20Zhiyu%20Mei%20and%20Guangju%20Wang%20and%20Yi%20Wu%0AAbstract%3A%20%20%20Reward%20models%20have%20been%20increasingly%20critical%20for%20improving%20the%20reasoning%0Acapability%20of%20LLMs.%20Existing%20research%20has%20shown%20that%20a%20well-trained%20reward%0Amodel%20can%20substantially%20improve%20model%20performances%20at%20inference%20time%20via%0Asearch.%20However%2C%20the%20potential%20of%20reward%20models%20during%20RL%20training%20time%20still%0Aremains%20largely%20under-explored.%20It%20is%20currently%20unclear%20whether%20these%20reward%0Amodels%20can%20provide%20additional%20training%20signals%20to%20enhance%20the%20reasoning%0Acapabilities%20of%20LLMs%20in%20RL%20training%20that%20uses%20sparse%20success%20rewards%2C%20which%0Averify%20the%20correctness%20of%20solutions.%20In%20this%20work%2C%20we%20evaluate%20popular%20reward%0Amodels%20for%20RL%20training%2C%20including%20the%20Outcome-supervised%20Reward%20Model%20%28ORM%29%20and%0Athe%20Process-supervised%20Reward%20Model%20%28PRM%29%2C%20and%20train%20a%20collection%20of%20LLMs%20for%0Amath%20problems%20using%20RL%20by%20combining%20these%20learned%20rewards%20with%20success%20rewards.%0ASurprisingly%2C%20even%20though%20these%20learned%20reward%20models%20have%20strong%0Ainference-time%20performances%2C%20they%20may%20NOT%20help%20or%20even%20hurt%20RL%20training%2C%0Aproducing%20worse%20performances%20than%20LLMs%20trained%20with%20the%20success%20reward%20only.%0AOur%20analysis%20reveals%20that%20an%20LLM%20can%20receive%20high%20rewards%20from%20some%20of%20these%0Areward%20models%20by%20repeating%20correct%20but%20unnecessary%20reasoning%20steps%2C%20leading%20to%0Aa%20severe%20reward%20hacking%20issue.%20Therefore%2C%20we%20introduce%20two%20novel%20reward%0Arefinement%20techniques%2C%20including%20Clipping%20and%20Delta.%20The%20key%20idea%20is%20to%20ensure%0Athe%20accumulative%20reward%20of%20any%20reasoning%20trajectory%20is%20upper-bounded%20to%20keep%20a%0Alearned%20reward%20model%20effective%20without%20being%20exploited.%20We%20evaluate%20our%0Atechniques%20with%20multiple%20reward%20models%20over%20a%20set%20of%201.5B%20and%207B%20LLMs%20on%20MATH%0Aand%20GSM8K%20benchmarks%20and%20demonstrate%20that%20with%20a%20carefully%20designed%20reward%0Afunction%2C%20RL%20training%20without%20any%20additional%20supervised%20tuning%20can%20improve%20all%0Athe%20evaluated%20LLMs%2C%20including%20the%20state-of-the-art%207B%20LLM%0AQwen2.5-Math-7B-Instruct%20on%20MATH%20and%20GSM8K%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Designing%2520Effective%2520RL%2520Reward%2520at%2520Training%2520Time%2520for%2520LLM%2520Reasoning%26entry.906535625%3DJiaxuan%2520Gao%2520and%2520Shusheng%2520Xu%2520and%2520Wenjie%2520Ye%2520and%2520Weilin%2520Liu%2520and%2520Chuyi%2520He%2520and%2520Wei%2520Fu%2520and%2520Zhiyu%2520Mei%2520and%2520Guangju%2520Wang%2520and%2520Yi%2520Wu%26entry.1292438233%3D%2520%2520Reward%2520models%2520have%2520been%2520increasingly%2520critical%2520for%2520improving%2520the%2520reasoning%250Acapability%2520of%2520LLMs.%2520Existing%2520research%2520has%2520shown%2520that%2520a%2520well-trained%2520reward%250Amodel%2520can%2520substantially%2520improve%2520model%2520performances%2520at%2520inference%2520time%2520via%250Asearch.%2520However%252C%2520the%2520potential%2520of%2520reward%2520models%2520during%2520RL%2520training%2520time%2520still%250Aremains%2520largely%2520under-explored.%2520It%2520is%2520currently%2520unclear%2520whether%2520these%2520reward%250Amodels%2520can%2520provide%2520additional%2520training%2520signals%2520to%2520enhance%2520the%2520reasoning%250Acapabilities%2520of%2520LLMs%2520in%2520RL%2520training%2520that%2520uses%2520sparse%2520success%2520rewards%252C%2520which%250Averify%2520the%2520correctness%2520of%2520solutions.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520popular%2520reward%250Amodels%2520for%2520RL%2520training%252C%2520including%2520the%2520Outcome-supervised%2520Reward%2520Model%2520%2528ORM%2529%2520and%250Athe%2520Process-supervised%2520Reward%2520Model%2520%2528PRM%2529%252C%2520and%2520train%2520a%2520collection%2520of%2520LLMs%2520for%250Amath%2520problems%2520using%2520RL%2520by%2520combining%2520these%2520learned%2520rewards%2520with%2520success%2520rewards.%250ASurprisingly%252C%2520even%2520though%2520these%2520learned%2520reward%2520models%2520have%2520strong%250Ainference-time%2520performances%252C%2520they%2520may%2520NOT%2520help%2520or%2520even%2520hurt%2520RL%2520training%252C%250Aproducing%2520worse%2520performances%2520than%2520LLMs%2520trained%2520with%2520the%2520success%2520reward%2520only.%250AOur%2520analysis%2520reveals%2520that%2520an%2520LLM%2520can%2520receive%2520high%2520rewards%2520from%2520some%2520of%2520these%250Areward%2520models%2520by%2520repeating%2520correct%2520but%2520unnecessary%2520reasoning%2520steps%252C%2520leading%2520to%250Aa%2520severe%2520reward%2520hacking%2520issue.%2520Therefore%252C%2520we%2520introduce%2520two%2520novel%2520reward%250Arefinement%2520techniques%252C%2520including%2520Clipping%2520and%2520Delta.%2520The%2520key%2520idea%2520is%2520to%2520ensure%250Athe%2520accumulative%2520reward%2520of%2520any%2520reasoning%2520trajectory%2520is%2520upper-bounded%2520to%2520keep%2520a%250Alearned%2520reward%2520model%2520effective%2520without%2520being%2520exploited.%2520We%2520evaluate%2520our%250Atechniques%2520with%2520multiple%2520reward%2520models%2520over%2520a%2520set%2520of%25201.5B%2520and%25207B%2520LLMs%2520on%2520MATH%250Aand%2520GSM8K%2520benchmarks%2520and%2520demonstrate%2520that%2520with%2520a%2520carefully%2520designed%2520reward%250Afunction%252C%2520RL%2520training%2520without%2520any%2520additional%2520supervised%2520tuning%2520can%2520improve%2520all%250Athe%2520evaluated%2520LLMs%252C%2520including%2520the%2520state-of-the-art%25207B%2520LLM%250AQwen2.5-Math-7B-Instruct%2520on%2520MATH%2520and%2520GSM8K%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Designing%20Effective%20RL%20Reward%20at%20Training%20Time%20for%20LLM%20Reasoning&entry.906535625=Jiaxuan%20Gao%20and%20Shusheng%20Xu%20and%20Wenjie%20Ye%20and%20Weilin%20Liu%20and%20Chuyi%20He%20and%20Wei%20Fu%20and%20Zhiyu%20Mei%20and%20Guangju%20Wang%20and%20Yi%20Wu&entry.1292438233=%20%20Reward%20models%20have%20been%20increasingly%20critical%20for%20improving%20the%20reasoning%0Acapability%20of%20LLMs.%20Existing%20research%20has%20shown%20that%20a%20well-trained%20reward%0Amodel%20can%20substantially%20improve%20model%20performances%20at%20inference%20time%20via%0Asearch.%20However%2C%20the%20potential%20of%20reward%20models%20during%20RL%20training%20time%20still%0Aremains%20largely%20under-explored.%20It%20is%20currently%20unclear%20whether%20these%20reward%0Amodels%20can%20provide%20additional%20training%20signals%20to%20enhance%20the%20reasoning%0Acapabilities%20of%20LLMs%20in%20RL%20training%20that%20uses%20sparse%20success%20rewards%2C%20which%0Averify%20the%20correctness%20of%20solutions.%20In%20this%20work%2C%20we%20evaluate%20popular%20reward%0Amodels%20for%20RL%20training%2C%20including%20the%20Outcome-supervised%20Reward%20Model%20%28ORM%29%20and%0Athe%20Process-supervised%20Reward%20Model%20%28PRM%29%2C%20and%20train%20a%20collection%20of%20LLMs%20for%0Amath%20problems%20using%20RL%20by%20combining%20these%20learned%20rewards%20with%20success%20rewards.%0ASurprisingly%2C%20even%20though%20these%20learned%20reward%20models%20have%20strong%0Ainference-time%20performances%2C%20they%20may%20NOT%20help%20or%20even%20hurt%20RL%20training%2C%0Aproducing%20worse%20performances%20than%20LLMs%20trained%20with%20the%20success%20reward%20only.%0AOur%20analysis%20reveals%20that%20an%20LLM%20can%20receive%20high%20rewards%20from%20some%20of%20these%0Areward%20models%20by%20repeating%20correct%20but%20unnecessary%20reasoning%20steps%2C%20leading%20to%0Aa%20severe%20reward%20hacking%20issue.%20Therefore%2C%20we%20introduce%20two%20novel%20reward%0Arefinement%20techniques%2C%20including%20Clipping%20and%20Delta.%20The%20key%20idea%20is%20to%20ensure%0Athe%20accumulative%20reward%20of%20any%20reasoning%20trajectory%20is%20upper-bounded%20to%20keep%20a%0Alearned%20reward%20model%20effective%20without%20being%20exploited.%20We%20evaluate%20our%0Atechniques%20with%20multiple%20reward%20models%20over%20a%20set%20of%201.5B%20and%207B%20LLMs%20on%20MATH%0Aand%20GSM8K%20benchmarks%20and%20demonstrate%20that%20with%20a%20carefully%20designed%20reward%0Afunction%2C%20RL%20training%20without%20any%20additional%20supervised%20tuning%20can%20improve%20all%0Athe%20evaluated%20LLMs%2C%20including%20the%20state-of-the-art%207B%20LLM%0AQwen2.5-Math-7B-Instruct%20on%20MATH%20and%20GSM8K%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15115v2&entry.124074799=Read"},
{"title": "NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video\n  Reconstruction", "author": "Zixuan Gong and Guangyin Bao and Qi Zhang and Zhongwei Wan and Duoqian Miao and Shoujin Wang and Lei Zhu and Changwei Wang and Rongtao Xu and Liang Hu and Ke Liu and Yu Zhang", "abstract": "  Reconstruction of static visual stimuli from non-invasion brain activity fMRI\nachieves great success, owning to advanced deep learning models such as CLIP\nand Stable Diffusion. However, the research on fMRI-to-video reconstruction\nremains limited since decoding the spatiotemporal perception of continuous\nvisual experiences is formidably challenging. We contend that the key to\naddressing these challenges lies in accurately decoding both high-level\nsemantics and low-level perception flows, as perceived by the brain in response\nto video stimuli. To the end, we propose NeuroClips, an innovative framework to\ndecode high-fidelity and smooth video from fMRI. NeuroClips utilizes a\nsemantics reconstructor to reconstruct video keyframes, guiding semantic\naccuracy and consistency, and employs a perception reconstructor to capture\nlow-level perceptual details, ensuring video smoothness. During inference, it\nadopts a pre-trained T2V diffusion model injected with both keyframes and\nlow-level perception flows for video reconstruction. Evaluated on a publicly\navailable fMRI-video dataset, NeuroClips achieves smooth high-fidelity video\nreconstruction of up to 6s at 8FPS, gaining significant improvements over\nstate-of-the-art models in various metrics, e.g., a 128\\% improvement in SSIM\nand an 81\\% improvement in spatiotemporal metrics. Our project is available at\nhttps://github.com/gongzix/NeuroClips}{https://github.com/gongzix/NeuroClips.\n", "link": "http://arxiv.org/abs/2410.19452v1", "date": "2024-10-25", "relevancy": 2.3674, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6328}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5745}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroClips%3A%20Towards%20High-fidelity%20and%20Smooth%20fMRI-to-Video%0A%20%20Reconstruction&body=Title%3A%20NeuroClips%3A%20Towards%20High-fidelity%20and%20Smooth%20fMRI-to-Video%0A%20%20Reconstruction%0AAuthor%3A%20Zixuan%20Gong%20and%20Guangyin%20Bao%20and%20Qi%20Zhang%20and%20Zhongwei%20Wan%20and%20Duoqian%20Miao%20and%20Shoujin%20Wang%20and%20Lei%20Zhu%20and%20Changwei%20Wang%20and%20Rongtao%20Xu%20and%20Liang%20Hu%20and%20Ke%20Liu%20and%20Yu%20Zhang%0AAbstract%3A%20%20%20Reconstruction%20of%20static%20visual%20stimuli%20from%20non-invasion%20brain%20activity%20fMRI%0Aachieves%20great%20success%2C%20owning%20to%20advanced%20deep%20learning%20models%20such%20as%20CLIP%0Aand%20Stable%20Diffusion.%20However%2C%20the%20research%20on%20fMRI-to-video%20reconstruction%0Aremains%20limited%20since%20decoding%20the%20spatiotemporal%20perception%20of%20continuous%0Avisual%20experiences%20is%20formidably%20challenging.%20We%20contend%20that%20the%20key%20to%0Aaddressing%20these%20challenges%20lies%20in%20accurately%20decoding%20both%20high-level%0Asemantics%20and%20low-level%20perception%20flows%2C%20as%20perceived%20by%20the%20brain%20in%20response%0Ato%20video%20stimuli.%20To%20the%20end%2C%20we%20propose%20NeuroClips%2C%20an%20innovative%20framework%20to%0Adecode%20high-fidelity%20and%20smooth%20video%20from%20fMRI.%20NeuroClips%20utilizes%20a%0Asemantics%20reconstructor%20to%20reconstruct%20video%20keyframes%2C%20guiding%20semantic%0Aaccuracy%20and%20consistency%2C%20and%20employs%20a%20perception%20reconstructor%20to%20capture%0Alow-level%20perceptual%20details%2C%20ensuring%20video%20smoothness.%20During%20inference%2C%20it%0Aadopts%20a%20pre-trained%20T2V%20diffusion%20model%20injected%20with%20both%20keyframes%20and%0Alow-level%20perception%20flows%20for%20video%20reconstruction.%20Evaluated%20on%20a%20publicly%0Aavailable%20fMRI-video%20dataset%2C%20NeuroClips%20achieves%20smooth%20high-fidelity%20video%0Areconstruction%20of%20up%20to%206s%20at%208FPS%2C%20gaining%20significant%20improvements%20over%0Astate-of-the-art%20models%20in%20various%20metrics%2C%20e.g.%2C%20a%20128%5C%25%20improvement%20in%20SSIM%0Aand%20an%2081%5C%25%20improvement%20in%20spatiotemporal%20metrics.%20Our%20project%20is%20available%20at%0Ahttps%3A//github.com/gongzix/NeuroClips%7D%7Bhttps%3A//github.com/gongzix/NeuroClips.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroClips%253A%2520Towards%2520High-fidelity%2520and%2520Smooth%2520fMRI-to-Video%250A%2520%2520Reconstruction%26entry.906535625%3DZixuan%2520Gong%2520and%2520Guangyin%2520Bao%2520and%2520Qi%2520Zhang%2520and%2520Zhongwei%2520Wan%2520and%2520Duoqian%2520Miao%2520and%2520Shoujin%2520Wang%2520and%2520Lei%2520Zhu%2520and%2520Changwei%2520Wang%2520and%2520Rongtao%2520Xu%2520and%2520Liang%2520Hu%2520and%2520Ke%2520Liu%2520and%2520Yu%2520Zhang%26entry.1292438233%3D%2520%2520Reconstruction%2520of%2520static%2520visual%2520stimuli%2520from%2520non-invasion%2520brain%2520activity%2520fMRI%250Aachieves%2520great%2520success%252C%2520owning%2520to%2520advanced%2520deep%2520learning%2520models%2520such%2520as%2520CLIP%250Aand%2520Stable%2520Diffusion.%2520However%252C%2520the%2520research%2520on%2520fMRI-to-video%2520reconstruction%250Aremains%2520limited%2520since%2520decoding%2520the%2520spatiotemporal%2520perception%2520of%2520continuous%250Avisual%2520experiences%2520is%2520formidably%2520challenging.%2520We%2520contend%2520that%2520the%2520key%2520to%250Aaddressing%2520these%2520challenges%2520lies%2520in%2520accurately%2520decoding%2520both%2520high-level%250Asemantics%2520and%2520low-level%2520perception%2520flows%252C%2520as%2520perceived%2520by%2520the%2520brain%2520in%2520response%250Ato%2520video%2520stimuli.%2520To%2520the%2520end%252C%2520we%2520propose%2520NeuroClips%252C%2520an%2520innovative%2520framework%2520to%250Adecode%2520high-fidelity%2520and%2520smooth%2520video%2520from%2520fMRI.%2520NeuroClips%2520utilizes%2520a%250Asemantics%2520reconstructor%2520to%2520reconstruct%2520video%2520keyframes%252C%2520guiding%2520semantic%250Aaccuracy%2520and%2520consistency%252C%2520and%2520employs%2520a%2520perception%2520reconstructor%2520to%2520capture%250Alow-level%2520perceptual%2520details%252C%2520ensuring%2520video%2520smoothness.%2520During%2520inference%252C%2520it%250Aadopts%2520a%2520pre-trained%2520T2V%2520diffusion%2520model%2520injected%2520with%2520both%2520keyframes%2520and%250Alow-level%2520perception%2520flows%2520for%2520video%2520reconstruction.%2520Evaluated%2520on%2520a%2520publicly%250Aavailable%2520fMRI-video%2520dataset%252C%2520NeuroClips%2520achieves%2520smooth%2520high-fidelity%2520video%250Areconstruction%2520of%2520up%2520to%25206s%2520at%25208FPS%252C%2520gaining%2520significant%2520improvements%2520over%250Astate-of-the-art%2520models%2520in%2520various%2520metrics%252C%2520e.g.%252C%2520a%2520128%255C%2525%2520improvement%2520in%2520SSIM%250Aand%2520an%252081%255C%2525%2520improvement%2520in%2520spatiotemporal%2520metrics.%2520Our%2520project%2520is%2520available%2520at%250Ahttps%253A//github.com/gongzix/NeuroClips%257D%257Bhttps%253A//github.com/gongzix/NeuroClips.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroClips%3A%20Towards%20High-fidelity%20and%20Smooth%20fMRI-to-Video%0A%20%20Reconstruction&entry.906535625=Zixuan%20Gong%20and%20Guangyin%20Bao%20and%20Qi%20Zhang%20and%20Zhongwei%20Wan%20and%20Duoqian%20Miao%20and%20Shoujin%20Wang%20and%20Lei%20Zhu%20and%20Changwei%20Wang%20and%20Rongtao%20Xu%20and%20Liang%20Hu%20and%20Ke%20Liu%20and%20Yu%20Zhang&entry.1292438233=%20%20Reconstruction%20of%20static%20visual%20stimuli%20from%20non-invasion%20brain%20activity%20fMRI%0Aachieves%20great%20success%2C%20owning%20to%20advanced%20deep%20learning%20models%20such%20as%20CLIP%0Aand%20Stable%20Diffusion.%20However%2C%20the%20research%20on%20fMRI-to-video%20reconstruction%0Aremains%20limited%20since%20decoding%20the%20spatiotemporal%20perception%20of%20continuous%0Avisual%20experiences%20is%20formidably%20challenging.%20We%20contend%20that%20the%20key%20to%0Aaddressing%20these%20challenges%20lies%20in%20accurately%20decoding%20both%20high-level%0Asemantics%20and%20low-level%20perception%20flows%2C%20as%20perceived%20by%20the%20brain%20in%20response%0Ato%20video%20stimuli.%20To%20the%20end%2C%20we%20propose%20NeuroClips%2C%20an%20innovative%20framework%20to%0Adecode%20high-fidelity%20and%20smooth%20video%20from%20fMRI.%20NeuroClips%20utilizes%20a%0Asemantics%20reconstructor%20to%20reconstruct%20video%20keyframes%2C%20guiding%20semantic%0Aaccuracy%20and%20consistency%2C%20and%20employs%20a%20perception%20reconstructor%20to%20capture%0Alow-level%20perceptual%20details%2C%20ensuring%20video%20smoothness.%20During%20inference%2C%20it%0Aadopts%20a%20pre-trained%20T2V%20diffusion%20model%20injected%20with%20both%20keyframes%20and%0Alow-level%20perception%20flows%20for%20video%20reconstruction.%20Evaluated%20on%20a%20publicly%0Aavailable%20fMRI-video%20dataset%2C%20NeuroClips%20achieves%20smooth%20high-fidelity%20video%0Areconstruction%20of%20up%20to%206s%20at%208FPS%2C%20gaining%20significant%20improvements%20over%0Astate-of-the-art%20models%20in%20various%20metrics%2C%20e.g.%2C%20a%20128%5C%25%20improvement%20in%20SSIM%0Aand%20an%2081%5C%25%20improvement%20in%20spatiotemporal%20metrics.%20Our%20project%20is%20available%20at%0Ahttps%3A//github.com/gongzix/NeuroClips%7D%7Bhttps%3A//github.com/gongzix/NeuroClips.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19452v1&entry.124074799=Read"},
{"title": "Measuring memorization in RLHF for code completion", "author": "Aneesh Pappu and Billy Porter and Ilia Shumailov and Jamie Hayes", "abstract": "  Reinforcement learning with human feedback (RLHF) has become the dominant\nmethod to align large models to user preferences. Unlike fine-tuning, for which\nthere are many studies regarding training data memorization, it is not clear\nhow memorization is affected by or introduced in the RLHF alignment process.\nUnderstanding this relationship is important as real user data may be collected\nand used to align large models; if user data is memorized during RLHF and later\nregurgitated, this could raise privacy concerns. In addition to RLHF, other\nmethods such as Direct Preference Optimization (DPO) and $\\Psi$PO have gained\npopularity for learning directly from human preferences, removing the need for\noptimizing intermediary reward models with reinforcement learning. In this\nwork, we analyze how training data memorization can surface and propagate\nthrough each phase of RLHF and direct preference learning. We focus our study\non code completion models, as code completion is one of the most popular use\ncases for large language models. We find that RLHF significantly decreases the\nchance that data used for reward modeling and reinforcement learning is\nmemorized in comparison to directly fine-tuning on this data, but that examples\nalready memorized during the fine-tuning stage of RLHF, will, in the majority\nof cases, remain memorized after RLHF. In contrast, we find that aligning by\nlearning directly from human preference data via a special case of $\\Psi$PO,\nIdentity Preference Optimization (IPO), increases the likelihood that training\ndata is regurgitated compared to RLHF. Our work suggests that RLHF, as opposed\nto direct preference learning, is a safer way to mitigate the risk of\nregurgitating sensitive preference data when aligning large language models. We\nfind our conclusions are robust across multiple code completion datasets,\ntasks, and model scales.\n", "link": "http://arxiv.org/abs/2406.11715v2", "date": "2024-10-25", "relevancy": 2.3569, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4729}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20memorization%20in%20RLHF%20for%20code%20completion&body=Title%3A%20Measuring%20memorization%20in%20RLHF%20for%20code%20completion%0AAuthor%3A%20Aneesh%20Pappu%20and%20Billy%20Porter%20and%20Ilia%20Shumailov%20and%20Jamie%20Hayes%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20human%20feedback%20%28RLHF%29%20has%20become%20the%20dominant%0Amethod%20to%20align%20large%20models%20to%20user%20preferences.%20Unlike%20fine-tuning%2C%20for%20which%0Athere%20are%20many%20studies%20regarding%20training%20data%20memorization%2C%20it%20is%20not%20clear%0Ahow%20memorization%20is%20affected%20by%20or%20introduced%20in%20the%20RLHF%20alignment%20process.%0AUnderstanding%20this%20relationship%20is%20important%20as%20real%20user%20data%20may%20be%20collected%0Aand%20used%20to%20align%20large%20models%3B%20if%20user%20data%20is%20memorized%20during%20RLHF%20and%20later%0Aregurgitated%2C%20this%20could%20raise%20privacy%20concerns.%20In%20addition%20to%20RLHF%2C%20other%0Amethods%20such%20as%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20%24%5CPsi%24PO%20have%20gained%0Apopularity%20for%20learning%20directly%20from%20human%20preferences%2C%20removing%20the%20need%20for%0Aoptimizing%20intermediary%20reward%20models%20with%20reinforcement%20learning.%20In%20this%0Awork%2C%20we%20analyze%20how%20training%20data%20memorization%20can%20surface%20and%20propagate%0Athrough%20each%20phase%20of%20RLHF%20and%20direct%20preference%20learning.%20We%20focus%20our%20study%0Aon%20code%20completion%20models%2C%20as%20code%20completion%20is%20one%20of%20the%20most%20popular%20use%0Acases%20for%20large%20language%20models.%20We%20find%20that%20RLHF%20significantly%20decreases%20the%0Achance%20that%20data%20used%20for%20reward%20modeling%20and%20reinforcement%20learning%20is%0Amemorized%20in%20comparison%20to%20directly%20fine-tuning%20on%20this%20data%2C%20but%20that%20examples%0Aalready%20memorized%20during%20the%20fine-tuning%20stage%20of%20RLHF%2C%20will%2C%20in%20the%20majority%0Aof%20cases%2C%20remain%20memorized%20after%20RLHF.%20In%20contrast%2C%20we%20find%20that%20aligning%20by%0Alearning%20directly%20from%20human%20preference%20data%20via%20a%20special%20case%20of%20%24%5CPsi%24PO%2C%0AIdentity%20Preference%20Optimization%20%28IPO%29%2C%20increases%20the%20likelihood%20that%20training%0Adata%20is%20regurgitated%20compared%20to%20RLHF.%20Our%20work%20suggests%20that%20RLHF%2C%20as%20opposed%0Ato%20direct%20preference%20learning%2C%20is%20a%20safer%20way%20to%20mitigate%20the%20risk%20of%0Aregurgitating%20sensitive%20preference%20data%20when%20aligning%20large%20language%20models.%20We%0Afind%20our%20conclusions%20are%20robust%20across%20multiple%20code%20completion%20datasets%2C%0Atasks%2C%20and%20model%20scales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520memorization%2520in%2520RLHF%2520for%2520code%2520completion%26entry.906535625%3DAneesh%2520Pappu%2520and%2520Billy%2520Porter%2520and%2520Ilia%2520Shumailov%2520and%2520Jamie%2520Hayes%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520human%2520feedback%2520%2528RLHF%2529%2520has%2520become%2520the%2520dominant%250Amethod%2520to%2520align%2520large%2520models%2520to%2520user%2520preferences.%2520Unlike%2520fine-tuning%252C%2520for%2520which%250Athere%2520are%2520many%2520studies%2520regarding%2520training%2520data%2520memorization%252C%2520it%2520is%2520not%2520clear%250Ahow%2520memorization%2520is%2520affected%2520by%2520or%2520introduced%2520in%2520the%2520RLHF%2520alignment%2520process.%250AUnderstanding%2520this%2520relationship%2520is%2520important%2520as%2520real%2520user%2520data%2520may%2520be%2520collected%250Aand%2520used%2520to%2520align%2520large%2520models%253B%2520if%2520user%2520data%2520is%2520memorized%2520during%2520RLHF%2520and%2520later%250Aregurgitated%252C%2520this%2520could%2520raise%2520privacy%2520concerns.%2520In%2520addition%2520to%2520RLHF%252C%2520other%250Amethods%2520such%2520as%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520and%2520%2524%255CPsi%2524PO%2520have%2520gained%250Apopularity%2520for%2520learning%2520directly%2520from%2520human%2520preferences%252C%2520removing%2520the%2520need%2520for%250Aoptimizing%2520intermediary%2520reward%2520models%2520with%2520reinforcement%2520learning.%2520In%2520this%250Awork%252C%2520we%2520analyze%2520how%2520training%2520data%2520memorization%2520can%2520surface%2520and%2520propagate%250Athrough%2520each%2520phase%2520of%2520RLHF%2520and%2520direct%2520preference%2520learning.%2520We%2520focus%2520our%2520study%250Aon%2520code%2520completion%2520models%252C%2520as%2520code%2520completion%2520is%2520one%2520of%2520the%2520most%2520popular%2520use%250Acases%2520for%2520large%2520language%2520models.%2520We%2520find%2520that%2520RLHF%2520significantly%2520decreases%2520the%250Achance%2520that%2520data%2520used%2520for%2520reward%2520modeling%2520and%2520reinforcement%2520learning%2520is%250Amemorized%2520in%2520comparison%2520to%2520directly%2520fine-tuning%2520on%2520this%2520data%252C%2520but%2520that%2520examples%250Aalready%2520memorized%2520during%2520the%2520fine-tuning%2520stage%2520of%2520RLHF%252C%2520will%252C%2520in%2520the%2520majority%250Aof%2520cases%252C%2520remain%2520memorized%2520after%2520RLHF.%2520In%2520contrast%252C%2520we%2520find%2520that%2520aligning%2520by%250Alearning%2520directly%2520from%2520human%2520preference%2520data%2520via%2520a%2520special%2520case%2520of%2520%2524%255CPsi%2524PO%252C%250AIdentity%2520Preference%2520Optimization%2520%2528IPO%2529%252C%2520increases%2520the%2520likelihood%2520that%2520training%250Adata%2520is%2520regurgitated%2520compared%2520to%2520RLHF.%2520Our%2520work%2520suggests%2520that%2520RLHF%252C%2520as%2520opposed%250Ato%2520direct%2520preference%2520learning%252C%2520is%2520a%2520safer%2520way%2520to%2520mitigate%2520the%2520risk%2520of%250Aregurgitating%2520sensitive%2520preference%2520data%2520when%2520aligning%2520large%2520language%2520models.%2520We%250Afind%2520our%2520conclusions%2520are%2520robust%2520across%2520multiple%2520code%2520completion%2520datasets%252C%250Atasks%252C%2520and%2520model%2520scales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20memorization%20in%20RLHF%20for%20code%20completion&entry.906535625=Aneesh%20Pappu%20and%20Billy%20Porter%20and%20Ilia%20Shumailov%20and%20Jamie%20Hayes&entry.1292438233=%20%20Reinforcement%20learning%20with%20human%20feedback%20%28RLHF%29%20has%20become%20the%20dominant%0Amethod%20to%20align%20large%20models%20to%20user%20preferences.%20Unlike%20fine-tuning%2C%20for%20which%0Athere%20are%20many%20studies%20regarding%20training%20data%20memorization%2C%20it%20is%20not%20clear%0Ahow%20memorization%20is%20affected%20by%20or%20introduced%20in%20the%20RLHF%20alignment%20process.%0AUnderstanding%20this%20relationship%20is%20important%20as%20real%20user%20data%20may%20be%20collected%0Aand%20used%20to%20align%20large%20models%3B%20if%20user%20data%20is%20memorized%20during%20RLHF%20and%20later%0Aregurgitated%2C%20this%20could%20raise%20privacy%20concerns.%20In%20addition%20to%20RLHF%2C%20other%0Amethods%20such%20as%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20%24%5CPsi%24PO%20have%20gained%0Apopularity%20for%20learning%20directly%20from%20human%20preferences%2C%20removing%20the%20need%20for%0Aoptimizing%20intermediary%20reward%20models%20with%20reinforcement%20learning.%20In%20this%0Awork%2C%20we%20analyze%20how%20training%20data%20memorization%20can%20surface%20and%20propagate%0Athrough%20each%20phase%20of%20RLHF%20and%20direct%20preference%20learning.%20We%20focus%20our%20study%0Aon%20code%20completion%20models%2C%20as%20code%20completion%20is%20one%20of%20the%20most%20popular%20use%0Acases%20for%20large%20language%20models.%20We%20find%20that%20RLHF%20significantly%20decreases%20the%0Achance%20that%20data%20used%20for%20reward%20modeling%20and%20reinforcement%20learning%20is%0Amemorized%20in%20comparison%20to%20directly%20fine-tuning%20on%20this%20data%2C%20but%20that%20examples%0Aalready%20memorized%20during%20the%20fine-tuning%20stage%20of%20RLHF%2C%20will%2C%20in%20the%20majority%0Aof%20cases%2C%20remain%20memorized%20after%20RLHF.%20In%20contrast%2C%20we%20find%20that%20aligning%20by%0Alearning%20directly%20from%20human%20preference%20data%20via%20a%20special%20case%20of%20%24%5CPsi%24PO%2C%0AIdentity%20Preference%20Optimization%20%28IPO%29%2C%20increases%20the%20likelihood%20that%20training%0Adata%20is%20regurgitated%20compared%20to%20RLHF.%20Our%20work%20suggests%20that%20RLHF%2C%20as%20opposed%0Ato%20direct%20preference%20learning%2C%20is%20a%20safer%20way%20to%20mitigate%20the%20risk%20of%0Aregurgitating%20sensitive%20preference%20data%20when%20aligning%20large%20language%20models.%20We%0Afind%20our%20conclusions%20are%20robust%20across%20multiple%20code%20completion%20datasets%2C%0Atasks%2C%20and%20model%20scales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11715v2&entry.124074799=Read"},
{"title": "Fusion-then-Distillation: Toward Cross-modal Positive Distillation for\n  Domain Adaptive 3D Semantic Segmentation", "author": "Yao Wu and Mingwei Xing and Yachao Zhang and Yuan Xie and Yanyun Qu", "abstract": "  In cross-modal unsupervised domain adaptation, a model trained on\nsource-domain data (e.g., synthetic) is adapted to target-domain data (e.g.,\nreal-world) without access to target annotation. Previous methods seek to\nmutually mimic cross-modal outputs in each domain, which enforces a class\nprobability distribution that is agreeable in different domains. However, they\noverlook the complementarity brought by the heterogeneous fusion in cross-modal\nlearning. In light of this, we propose a novel fusion-then-distillation (FtD++)\nmethod to explore cross-modal positive distillation of the source and target\ndomains for 3D semantic segmentation. FtD++ realizes distribution consistency\nbetween outputs not only for 2D images and 3D point clouds but also for\nsource-domain and augment-domain. Specially, our method contains three key\ningredients. First, we present a model-agnostic feature fusion module to\ngenerate the cross-modal fusion representation for establishing a latent space.\nIn this space, two modalities are enforced maximum correlation and\ncomplementarity. Second, the proposed cross-modal positive distillation\npreserves the complete information of multi-modal input and combines the\nsemantic content of the source domain with the style of the target domain,\nthereby achieving domain-modality alignment. Finally, cross-modal debiased\npseudo-labeling is devised to model the uncertainty of pseudo-labels via a\nself-training manner. Extensive experiments report state-of-the-art results on\nseveral domain adaptive scenarios under unsupervised and semi-supervised\nsettings. Code is available at https://github.com/Barcaaaa/FtD-PlusPlus.\n", "link": "http://arxiv.org/abs/2410.19446v1", "date": "2024-10-25", "relevancy": 2.3532, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5641}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion-then-Distillation%3A%20Toward%20Cross-modal%20Positive%20Distillation%20for%0A%20%20Domain%20Adaptive%203D%20Semantic%20Segmentation&body=Title%3A%20Fusion-then-Distillation%3A%20Toward%20Cross-modal%20Positive%20Distillation%20for%0A%20%20Domain%20Adaptive%203D%20Semantic%20Segmentation%0AAuthor%3A%20Yao%20Wu%20and%20Mingwei%20Xing%20and%20Yachao%20Zhang%20and%20Yuan%20Xie%20and%20Yanyun%20Qu%0AAbstract%3A%20%20%20In%20cross-modal%20unsupervised%20domain%20adaptation%2C%20a%20model%20trained%20on%0Asource-domain%20data%20%28e.g.%2C%20synthetic%29%20is%20adapted%20to%20target-domain%20data%20%28e.g.%2C%0Areal-world%29%20without%20access%20to%20target%20annotation.%20Previous%20methods%20seek%20to%0Amutually%20mimic%20cross-modal%20outputs%20in%20each%20domain%2C%20which%20enforces%20a%20class%0Aprobability%20distribution%20that%20is%20agreeable%20in%20different%20domains.%20However%2C%20they%0Aoverlook%20the%20complementarity%20brought%20by%20the%20heterogeneous%20fusion%20in%20cross-modal%0Alearning.%20In%20light%20of%20this%2C%20we%20propose%20a%20novel%20fusion-then-distillation%20%28FtD%2B%2B%29%0Amethod%20to%20explore%20cross-modal%20positive%20distillation%20of%20the%20source%20and%20target%0Adomains%20for%203D%20semantic%20segmentation.%20FtD%2B%2B%20realizes%20distribution%20consistency%0Abetween%20outputs%20not%20only%20for%202D%20images%20and%203D%20point%20clouds%20but%20also%20for%0Asource-domain%20and%20augment-domain.%20Specially%2C%20our%20method%20contains%20three%20key%0Aingredients.%20First%2C%20we%20present%20a%20model-agnostic%20feature%20fusion%20module%20to%0Agenerate%20the%20cross-modal%20fusion%20representation%20for%20establishing%20a%20latent%20space.%0AIn%20this%20space%2C%20two%20modalities%20are%20enforced%20maximum%20correlation%20and%0Acomplementarity.%20Second%2C%20the%20proposed%20cross-modal%20positive%20distillation%0Apreserves%20the%20complete%20information%20of%20multi-modal%20input%20and%20combines%20the%0Asemantic%20content%20of%20the%20source%20domain%20with%20the%20style%20of%20the%20target%20domain%2C%0Athereby%20achieving%20domain-modality%20alignment.%20Finally%2C%20cross-modal%20debiased%0Apseudo-labeling%20is%20devised%20to%20model%20the%20uncertainty%20of%20pseudo-labels%20via%20a%0Aself-training%20manner.%20Extensive%20experiments%20report%20state-of-the-art%20results%20on%0Aseveral%20domain%20adaptive%20scenarios%20under%20unsupervised%20and%20semi-supervised%0Asettings.%20Code%20is%20available%20at%20https%3A//github.com/Barcaaaa/FtD-PlusPlus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion-then-Distillation%253A%2520Toward%2520Cross-modal%2520Positive%2520Distillation%2520for%250A%2520%2520Domain%2520Adaptive%25203D%2520Semantic%2520Segmentation%26entry.906535625%3DYao%2520Wu%2520and%2520Mingwei%2520Xing%2520and%2520Yachao%2520Zhang%2520and%2520Yuan%2520Xie%2520and%2520Yanyun%2520Qu%26entry.1292438233%3D%2520%2520In%2520cross-modal%2520unsupervised%2520domain%2520adaptation%252C%2520a%2520model%2520trained%2520on%250Asource-domain%2520data%2520%2528e.g.%252C%2520synthetic%2529%2520is%2520adapted%2520to%2520target-domain%2520data%2520%2528e.g.%252C%250Areal-world%2529%2520without%2520access%2520to%2520target%2520annotation.%2520Previous%2520methods%2520seek%2520to%250Amutually%2520mimic%2520cross-modal%2520outputs%2520in%2520each%2520domain%252C%2520which%2520enforces%2520a%2520class%250Aprobability%2520distribution%2520that%2520is%2520agreeable%2520in%2520different%2520domains.%2520However%252C%2520they%250Aoverlook%2520the%2520complementarity%2520brought%2520by%2520the%2520heterogeneous%2520fusion%2520in%2520cross-modal%250Alearning.%2520In%2520light%2520of%2520this%252C%2520we%2520propose%2520a%2520novel%2520fusion-then-distillation%2520%2528FtD%252B%252B%2529%250Amethod%2520to%2520explore%2520cross-modal%2520positive%2520distillation%2520of%2520the%2520source%2520and%2520target%250Adomains%2520for%25203D%2520semantic%2520segmentation.%2520FtD%252B%252B%2520realizes%2520distribution%2520consistency%250Abetween%2520outputs%2520not%2520only%2520for%25202D%2520images%2520and%25203D%2520point%2520clouds%2520but%2520also%2520for%250Asource-domain%2520and%2520augment-domain.%2520Specially%252C%2520our%2520method%2520contains%2520three%2520key%250Aingredients.%2520First%252C%2520we%2520present%2520a%2520model-agnostic%2520feature%2520fusion%2520module%2520to%250Agenerate%2520the%2520cross-modal%2520fusion%2520representation%2520for%2520establishing%2520a%2520latent%2520space.%250AIn%2520this%2520space%252C%2520two%2520modalities%2520are%2520enforced%2520maximum%2520correlation%2520and%250Acomplementarity.%2520Second%252C%2520the%2520proposed%2520cross-modal%2520positive%2520distillation%250Apreserves%2520the%2520complete%2520information%2520of%2520multi-modal%2520input%2520and%2520combines%2520the%250Asemantic%2520content%2520of%2520the%2520source%2520domain%2520with%2520the%2520style%2520of%2520the%2520target%2520domain%252C%250Athereby%2520achieving%2520domain-modality%2520alignment.%2520Finally%252C%2520cross-modal%2520debiased%250Apseudo-labeling%2520is%2520devised%2520to%2520model%2520the%2520uncertainty%2520of%2520pseudo-labels%2520via%2520a%250Aself-training%2520manner.%2520Extensive%2520experiments%2520report%2520state-of-the-art%2520results%2520on%250Aseveral%2520domain%2520adaptive%2520scenarios%2520under%2520unsupervised%2520and%2520semi-supervised%250Asettings.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Barcaaaa/FtD-PlusPlus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion-then-Distillation%3A%20Toward%20Cross-modal%20Positive%20Distillation%20for%0A%20%20Domain%20Adaptive%203D%20Semantic%20Segmentation&entry.906535625=Yao%20Wu%20and%20Mingwei%20Xing%20and%20Yachao%20Zhang%20and%20Yuan%20Xie%20and%20Yanyun%20Qu&entry.1292438233=%20%20In%20cross-modal%20unsupervised%20domain%20adaptation%2C%20a%20model%20trained%20on%0Asource-domain%20data%20%28e.g.%2C%20synthetic%29%20is%20adapted%20to%20target-domain%20data%20%28e.g.%2C%0Areal-world%29%20without%20access%20to%20target%20annotation.%20Previous%20methods%20seek%20to%0Amutually%20mimic%20cross-modal%20outputs%20in%20each%20domain%2C%20which%20enforces%20a%20class%0Aprobability%20distribution%20that%20is%20agreeable%20in%20different%20domains.%20However%2C%20they%0Aoverlook%20the%20complementarity%20brought%20by%20the%20heterogeneous%20fusion%20in%20cross-modal%0Alearning.%20In%20light%20of%20this%2C%20we%20propose%20a%20novel%20fusion-then-distillation%20%28FtD%2B%2B%29%0Amethod%20to%20explore%20cross-modal%20positive%20distillation%20of%20the%20source%20and%20target%0Adomains%20for%203D%20semantic%20segmentation.%20FtD%2B%2B%20realizes%20distribution%20consistency%0Abetween%20outputs%20not%20only%20for%202D%20images%20and%203D%20point%20clouds%20but%20also%20for%0Asource-domain%20and%20augment-domain.%20Specially%2C%20our%20method%20contains%20three%20key%0Aingredients.%20First%2C%20we%20present%20a%20model-agnostic%20feature%20fusion%20module%20to%0Agenerate%20the%20cross-modal%20fusion%20representation%20for%20establishing%20a%20latent%20space.%0AIn%20this%20space%2C%20two%20modalities%20are%20enforced%20maximum%20correlation%20and%0Acomplementarity.%20Second%2C%20the%20proposed%20cross-modal%20positive%20distillation%0Apreserves%20the%20complete%20information%20of%20multi-modal%20input%20and%20combines%20the%0Asemantic%20content%20of%20the%20source%20domain%20with%20the%20style%20of%20the%20target%20domain%2C%0Athereby%20achieving%20domain-modality%20alignment.%20Finally%2C%20cross-modal%20debiased%0Apseudo-labeling%20is%20devised%20to%20model%20the%20uncertainty%20of%20pseudo-labels%20via%20a%0Aself-training%20manner.%20Extensive%20experiments%20report%20state-of-the-art%20results%20on%0Aseveral%20domain%20adaptive%20scenarios%20under%20unsupervised%20and%20semi-supervised%0Asettings.%20Code%20is%20available%20at%20https%3A//github.com/Barcaaaa/FtD-PlusPlus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19446v1&entry.124074799=Read"},
{"title": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning", "author": "Xiangyu Zeng and Kunchang Li and Chenting Wang and Xinhao Li and Tianxiang Jiang and Ziang Yan and Songze Li and Yansong Shi and Zhengrong Yue and Yi Wang and Yali Wang and Yu Qiao and Limin Wang", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n", "link": "http://arxiv.org/abs/2410.19702v1", "date": "2024-10-25", "relevancy": 2.3365, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5989}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5748}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeSuite%3A%20Improving%20MLLMs%20for%20Long%20Video%20Understanding%20via%20Grounded%0A%20%20Tuning&body=Title%3A%20TimeSuite%3A%20Improving%20MLLMs%20for%20Long%20Video%20Understanding%20via%20Grounded%0A%20%20Tuning%0AAuthor%3A%20Xiangyu%20Zeng%20and%20Kunchang%20Li%20and%20Chenting%20Wang%20and%20Xinhao%20Li%20and%20Tianxiang%20Jiang%20and%20Ziang%20Yan%20and%20Songze%20Li%20and%20Yansong%20Shi%20and%20Zhengrong%20Yue%20and%20Yi%20Wang%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%0Aperformance%20in%20short%20video%20understanding.%20However%2C%20understanding%20long-form%0Avideos%20still%20remains%20challenging%20for%20MLLMs.%20This%20paper%20proposes%20TimeSuite%2C%20a%0Acollection%20of%20new%20designs%20to%20adapt%20the%20existing%20short-form%20video%20MLLMs%20for%20long%0Avideo%20understanding%2C%20including%20a%20simple%20yet%20efficient%20framework%20to%20process%20long%0Avideo%20sequence%2C%20a%20high-quality%20video%20dataset%20for%20grounded%20tuning%20of%20MLLMs%2C%20and%0Aa%20carefully-designed%20instruction%20tuning%20task%20to%20explicitly%20incorporate%20the%0Agrounding%20supervision%20in%20the%20traditional%20QA%20format.%20Specifically%2C%20based%20on%0AVideoChat%2C%20we%20propose%20our%20long-video%20MLLM%2C%20coined%20as%20VideoChat-T%2C%20by%0Aimplementing%20a%20token%20shuffling%20to%20compress%20long%20video%20tokens%20and%20introducing%0ATemporal%20Adaptive%20Position%20Encoding%20%28TAPE%29%20to%20enhance%20the%20temporal%20awareness%20of%0Avisual%20representation.%20Meanwhile%2C%20we%20introduce%20the%20TimePro%2C%20a%20comprehensive%0Agrounding-centric%20instruction%20tuning%20dataset%20composed%20of%209%20tasks%20and%20349k%0Ahigh-quality%20grounded%20annotations.%20Notably%2C%20we%20design%20a%20new%20instruction%20tuning%0Atask%20type%2C%20called%20Temporal%20Grounded%20Caption%2C%20to%20peform%20detailed%20video%0Adescriptions%20with%20the%20corresponding%20time%20stamps%20prediction.%20This%20explicit%0Atemporal%20location%20prediction%20will%20guide%20MLLM%20to%20correctly%20attend%20on%20the%20visual%0Acontent%20when%20generating%20description%2C%20and%20thus%20reduce%20the%20hallucination%20risk%0Acaused%20by%20the%20LLMs.%20Experimental%20results%20demonstrate%20that%20our%20TimeSuite%0Aprovides%20a%20successful%20solution%20to%20enhance%20the%20long%20video%20understanding%0Acapability%20of%20short-form%20MLLM%2C%20achieving%20improvement%20of%205.6%25%20and%206.8%25%20on%20the%0Abenchmarks%20of%20Egoschema%20and%20VideoMME%2C%20respectively.%20In%20addition%2C%20VideoChat-T%0Aexhibits%20robust%20zero-shot%20temporal%20grounding%20capabilities%2C%20significantly%0Aoutperforming%20the%20existing%20state-of-the-art%20MLLMs.%20After%20fine-tuning%2C%20it%0Aperforms%20on%20par%20with%20the%20traditional%20supervised%20expert%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeSuite%253A%2520Improving%2520MLLMs%2520for%2520Long%2520Video%2520Understanding%2520via%2520Grounded%250A%2520%2520Tuning%26entry.906535625%3DXiangyu%2520Zeng%2520and%2520Kunchang%2520Li%2520and%2520Chenting%2520Wang%2520and%2520Xinhao%2520Li%2520and%2520Tianxiang%2520Jiang%2520and%2520Ziang%2520Yan%2520and%2520Songze%2520Li%2520and%2520Yansong%2520Shi%2520and%2520Zhengrong%2520Yue%2520and%2520Yi%2520Wang%2520and%2520Yali%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520impressive%250Aperformance%2520in%2520short%2520video%2520understanding.%2520However%252C%2520understanding%2520long-form%250Avideos%2520still%2520remains%2520challenging%2520for%2520MLLMs.%2520This%2520paper%2520proposes%2520TimeSuite%252C%2520a%250Acollection%2520of%2520new%2520designs%2520to%2520adapt%2520the%2520existing%2520short-form%2520video%2520MLLMs%2520for%2520long%250Avideo%2520understanding%252C%2520including%2520a%2520simple%2520yet%2520efficient%2520framework%2520to%2520process%2520long%250Avideo%2520sequence%252C%2520a%2520high-quality%2520video%2520dataset%2520for%2520grounded%2520tuning%2520of%2520MLLMs%252C%2520and%250Aa%2520carefully-designed%2520instruction%2520tuning%2520task%2520to%2520explicitly%2520incorporate%2520the%250Agrounding%2520supervision%2520in%2520the%2520traditional%2520QA%2520format.%2520Specifically%252C%2520based%2520on%250AVideoChat%252C%2520we%2520propose%2520our%2520long-video%2520MLLM%252C%2520coined%2520as%2520VideoChat-T%252C%2520by%250Aimplementing%2520a%2520token%2520shuffling%2520to%2520compress%2520long%2520video%2520tokens%2520and%2520introducing%250ATemporal%2520Adaptive%2520Position%2520Encoding%2520%2528TAPE%2529%2520to%2520enhance%2520the%2520temporal%2520awareness%2520of%250Avisual%2520representation.%2520Meanwhile%252C%2520we%2520introduce%2520the%2520TimePro%252C%2520a%2520comprehensive%250Agrounding-centric%2520instruction%2520tuning%2520dataset%2520composed%2520of%25209%2520tasks%2520and%2520349k%250Ahigh-quality%2520grounded%2520annotations.%2520Notably%252C%2520we%2520design%2520a%2520new%2520instruction%2520tuning%250Atask%2520type%252C%2520called%2520Temporal%2520Grounded%2520Caption%252C%2520to%2520peform%2520detailed%2520video%250Adescriptions%2520with%2520the%2520corresponding%2520time%2520stamps%2520prediction.%2520This%2520explicit%250Atemporal%2520location%2520prediction%2520will%2520guide%2520MLLM%2520to%2520correctly%2520attend%2520on%2520the%2520visual%250Acontent%2520when%2520generating%2520description%252C%2520and%2520thus%2520reduce%2520the%2520hallucination%2520risk%250Acaused%2520by%2520the%2520LLMs.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520TimeSuite%250Aprovides%2520a%2520successful%2520solution%2520to%2520enhance%2520the%2520long%2520video%2520understanding%250Acapability%2520of%2520short-form%2520MLLM%252C%2520achieving%2520improvement%2520of%25205.6%2525%2520and%25206.8%2525%2520on%2520the%250Abenchmarks%2520of%2520Egoschema%2520and%2520VideoMME%252C%2520respectively.%2520In%2520addition%252C%2520VideoChat-T%250Aexhibits%2520robust%2520zero-shot%2520temporal%2520grounding%2520capabilities%252C%2520significantly%250Aoutperforming%2520the%2520existing%2520state-of-the-art%2520MLLMs.%2520After%2520fine-tuning%252C%2520it%250Aperforms%2520on%2520par%2520with%2520the%2520traditional%2520supervised%2520expert%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeSuite%3A%20Improving%20MLLMs%20for%20Long%20Video%20Understanding%20via%20Grounded%0A%20%20Tuning&entry.906535625=Xiangyu%20Zeng%20and%20Kunchang%20Li%20and%20Chenting%20Wang%20and%20Xinhao%20Li%20and%20Tianxiang%20Jiang%20and%20Ziang%20Yan%20and%20Songze%20Li%20and%20Yansong%20Shi%20and%20Zhengrong%20Yue%20and%20Yi%20Wang%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%0Aperformance%20in%20short%20video%20understanding.%20However%2C%20understanding%20long-form%0Avideos%20still%20remains%20challenging%20for%20MLLMs.%20This%20paper%20proposes%20TimeSuite%2C%20a%0Acollection%20of%20new%20designs%20to%20adapt%20the%20existing%20short-form%20video%20MLLMs%20for%20long%0Avideo%20understanding%2C%20including%20a%20simple%20yet%20efficient%20framework%20to%20process%20long%0Avideo%20sequence%2C%20a%20high-quality%20video%20dataset%20for%20grounded%20tuning%20of%20MLLMs%2C%20and%0Aa%20carefully-designed%20instruction%20tuning%20task%20to%20explicitly%20incorporate%20the%0Agrounding%20supervision%20in%20the%20traditional%20QA%20format.%20Specifically%2C%20based%20on%0AVideoChat%2C%20we%20propose%20our%20long-video%20MLLM%2C%20coined%20as%20VideoChat-T%2C%20by%0Aimplementing%20a%20token%20shuffling%20to%20compress%20long%20video%20tokens%20and%20introducing%0ATemporal%20Adaptive%20Position%20Encoding%20%28TAPE%29%20to%20enhance%20the%20temporal%20awareness%20of%0Avisual%20representation.%20Meanwhile%2C%20we%20introduce%20the%20TimePro%2C%20a%20comprehensive%0Agrounding-centric%20instruction%20tuning%20dataset%20composed%20of%209%20tasks%20and%20349k%0Ahigh-quality%20grounded%20annotations.%20Notably%2C%20we%20design%20a%20new%20instruction%20tuning%0Atask%20type%2C%20called%20Temporal%20Grounded%20Caption%2C%20to%20peform%20detailed%20video%0Adescriptions%20with%20the%20corresponding%20time%20stamps%20prediction.%20This%20explicit%0Atemporal%20location%20prediction%20will%20guide%20MLLM%20to%20correctly%20attend%20on%20the%20visual%0Acontent%20when%20generating%20description%2C%20and%20thus%20reduce%20the%20hallucination%20risk%0Acaused%20by%20the%20LLMs.%20Experimental%20results%20demonstrate%20that%20our%20TimeSuite%0Aprovides%20a%20successful%20solution%20to%20enhance%20the%20long%20video%20understanding%0Acapability%20of%20short-form%20MLLM%2C%20achieving%20improvement%20of%205.6%25%20and%206.8%25%20on%20the%0Abenchmarks%20of%20Egoschema%20and%20VideoMME%2C%20respectively.%20In%20addition%2C%20VideoChat-T%0Aexhibits%20robust%20zero-shot%20temporal%20grounding%20capabilities%2C%20significantly%0Aoutperforming%20the%20existing%20state-of-the-art%20MLLMs.%20After%20fine-tuning%2C%20it%0Aperforms%20on%20par%20with%20the%20traditional%20supervised%20expert%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19702v1&entry.124074799=Read"},
{"title": "Improving Inverse Folding for Peptide Design with Diversity-regularized\n  Direct Preference Optimization", "author": "Ryan Park and Darren J. Hsu and C. Brian Roland and Maria Korshunova and Chen Tessler and Shie Mannor and Olivia Viessmann and Bruno Trentini", "abstract": "  Inverse folding models play an important role in structure-based design by\npredicting amino acid sequences that fold into desired reference structures.\nModels like ProteinMPNN, a message-passing encoder-decoder model, are trained\nto reliably produce new sequences from a reference structure. However, when\napplied to peptides, these models are prone to generating repetitive sequences\nthat do not fold into the reference structure. To address this, we fine-tune\nProteinMPNN to produce diverse and structurally consistent peptide sequences\nvia Direct Preference Optimization (DPO). We derive two enhancements to DPO:\nonline diversity regularization and domain-specific priors. Additionally, we\ndevelop a new understanding on improving diversity in decoder models. When\nconditioned on OpenFold generated structures, our fine-tuned models achieve\nstate-of-the-art structural similarity scores, improving base ProteinMPNN by at\nleast 8%. Compared to standard DPO, our regularized method achieves up to 20%\nhigher sequence diversity with no loss in structural similarity score.\n", "link": "http://arxiv.org/abs/2410.19471v1", "date": "2024-10-25", "relevancy": 2.3252, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4655}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Inverse%20Folding%20for%20Peptide%20Design%20with%20Diversity-regularized%0A%20%20Direct%20Preference%20Optimization&body=Title%3A%20Improving%20Inverse%20Folding%20for%20Peptide%20Design%20with%20Diversity-regularized%0A%20%20Direct%20Preference%20Optimization%0AAuthor%3A%20Ryan%20Park%20and%20Darren%20J.%20Hsu%20and%20C.%20Brian%20Roland%20and%20Maria%20Korshunova%20and%20Chen%20Tessler%20and%20Shie%20Mannor%20and%20Olivia%20Viessmann%20and%20Bruno%20Trentini%0AAbstract%3A%20%20%20Inverse%20folding%20models%20play%20an%20important%20role%20in%20structure-based%20design%20by%0Apredicting%20amino%20acid%20sequences%20that%20fold%20into%20desired%20reference%20structures.%0AModels%20like%20ProteinMPNN%2C%20a%20message-passing%20encoder-decoder%20model%2C%20are%20trained%0Ato%20reliably%20produce%20new%20sequences%20from%20a%20reference%20structure.%20However%2C%20when%0Aapplied%20to%20peptides%2C%20these%20models%20are%20prone%20to%20generating%20repetitive%20sequences%0Athat%20do%20not%20fold%20into%20the%20reference%20structure.%20To%20address%20this%2C%20we%20fine-tune%0AProteinMPNN%20to%20produce%20diverse%20and%20structurally%20consistent%20peptide%20sequences%0Avia%20Direct%20Preference%20Optimization%20%28DPO%29.%20We%20derive%20two%20enhancements%20to%20DPO%3A%0Aonline%20diversity%20regularization%20and%20domain-specific%20priors.%20Additionally%2C%20we%0Adevelop%20a%20new%20understanding%20on%20improving%20diversity%20in%20decoder%20models.%20When%0Aconditioned%20on%20OpenFold%20generated%20structures%2C%20our%20fine-tuned%20models%20achieve%0Astate-of-the-art%20structural%20similarity%20scores%2C%20improving%20base%20ProteinMPNN%20by%20at%0Aleast%208%25.%20Compared%20to%20standard%20DPO%2C%20our%20regularized%20method%20achieves%20up%20to%2020%25%0Ahigher%20sequence%20diversity%20with%20no%20loss%20in%20structural%20similarity%20score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Inverse%2520Folding%2520for%2520Peptide%2520Design%2520with%2520Diversity-regularized%250A%2520%2520Direct%2520Preference%2520Optimization%26entry.906535625%3DRyan%2520Park%2520and%2520Darren%2520J.%2520Hsu%2520and%2520C.%2520Brian%2520Roland%2520and%2520Maria%2520Korshunova%2520and%2520Chen%2520Tessler%2520and%2520Shie%2520Mannor%2520and%2520Olivia%2520Viessmann%2520and%2520Bruno%2520Trentini%26entry.1292438233%3D%2520%2520Inverse%2520folding%2520models%2520play%2520an%2520important%2520role%2520in%2520structure-based%2520design%2520by%250Apredicting%2520amino%2520acid%2520sequences%2520that%2520fold%2520into%2520desired%2520reference%2520structures.%250AModels%2520like%2520ProteinMPNN%252C%2520a%2520message-passing%2520encoder-decoder%2520model%252C%2520are%2520trained%250Ato%2520reliably%2520produce%2520new%2520sequences%2520from%2520a%2520reference%2520structure.%2520However%252C%2520when%250Aapplied%2520to%2520peptides%252C%2520these%2520models%2520are%2520prone%2520to%2520generating%2520repetitive%2520sequences%250Athat%2520do%2520not%2520fold%2520into%2520the%2520reference%2520structure.%2520To%2520address%2520this%252C%2520we%2520fine-tune%250AProteinMPNN%2520to%2520produce%2520diverse%2520and%2520structurally%2520consistent%2520peptide%2520sequences%250Avia%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529.%2520We%2520derive%2520two%2520enhancements%2520to%2520DPO%253A%250Aonline%2520diversity%2520regularization%2520and%2520domain-specific%2520priors.%2520Additionally%252C%2520we%250Adevelop%2520a%2520new%2520understanding%2520on%2520improving%2520diversity%2520in%2520decoder%2520models.%2520When%250Aconditioned%2520on%2520OpenFold%2520generated%2520structures%252C%2520our%2520fine-tuned%2520models%2520achieve%250Astate-of-the-art%2520structural%2520similarity%2520scores%252C%2520improving%2520base%2520ProteinMPNN%2520by%2520at%250Aleast%25208%2525.%2520Compared%2520to%2520standard%2520DPO%252C%2520our%2520regularized%2520method%2520achieves%2520up%2520to%252020%2525%250Ahigher%2520sequence%2520diversity%2520with%2520no%2520loss%2520in%2520structural%2520similarity%2520score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Inverse%20Folding%20for%20Peptide%20Design%20with%20Diversity-regularized%0A%20%20Direct%20Preference%20Optimization&entry.906535625=Ryan%20Park%20and%20Darren%20J.%20Hsu%20and%20C.%20Brian%20Roland%20and%20Maria%20Korshunova%20and%20Chen%20Tessler%20and%20Shie%20Mannor%20and%20Olivia%20Viessmann%20and%20Bruno%20Trentini&entry.1292438233=%20%20Inverse%20folding%20models%20play%20an%20important%20role%20in%20structure-based%20design%20by%0Apredicting%20amino%20acid%20sequences%20that%20fold%20into%20desired%20reference%20structures.%0AModels%20like%20ProteinMPNN%2C%20a%20message-passing%20encoder-decoder%20model%2C%20are%20trained%0Ato%20reliably%20produce%20new%20sequences%20from%20a%20reference%20structure.%20However%2C%20when%0Aapplied%20to%20peptides%2C%20these%20models%20are%20prone%20to%20generating%20repetitive%20sequences%0Athat%20do%20not%20fold%20into%20the%20reference%20structure.%20To%20address%20this%2C%20we%20fine-tune%0AProteinMPNN%20to%20produce%20diverse%20and%20structurally%20consistent%20peptide%20sequences%0Avia%20Direct%20Preference%20Optimization%20%28DPO%29.%20We%20derive%20two%20enhancements%20to%20DPO%3A%0Aonline%20diversity%20regularization%20and%20domain-specific%20priors.%20Additionally%2C%20we%0Adevelop%20a%20new%20understanding%20on%20improving%20diversity%20in%20decoder%20models.%20When%0Aconditioned%20on%20OpenFold%20generated%20structures%2C%20our%20fine-tuned%20models%20achieve%0Astate-of-the-art%20structural%20similarity%20scores%2C%20improving%20base%20ProteinMPNN%20by%20at%0Aleast%208%25.%20Compared%20to%20standard%20DPO%2C%20our%20regularized%20method%20achieves%20up%20to%2020%25%0Ahigher%20sequence%20diversity%20with%20no%20loss%20in%20structural%20similarity%20score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19471v1&entry.124074799=Read"},
{"title": "EDGE: Enhanced Grounded GUI Understanding with Enriched\n  Multi-Granularity Synthetic Data", "author": "Xuetian Chen and Hangcheng Li and Jiaqing Liang and Sihang Jiang and Deqing Yang", "abstract": "  Autonomous agents operating on the graphical user interfaces (GUIs) of\nvarious applications hold immense practical value. Unlike the large language\nmodel (LLM)-based methods which rely on structured texts and customized\nbackends, the approaches using large vision-language models (LVLMs) are more\nintuitive and adaptable as they can visually perceive and directly interact\nwith screens, making them indispensable in general scenarios without text\nmetadata and tailored backends. Given the lack of high-quality training data\nfor GUI-related tasks in existing work, this paper aims to enhance the GUI\nunderstanding and interacting capabilities of LVLMs through a data-driven\napproach. We propose EDGE, a general data synthesis framework that\nautomatically generates large-scale, multi-granularity training data from\nwebpages across the Web. Evaluation results on various GUI and agent benchmarks\ndemonstrate that the model trained with the dataset generated through EDGE\nexhibits superior webpage understanding capabilities, which can then be easily\ntransferred to previously unseen desktop and mobile environments. Our approach\nsignificantly reduces the dependence on manual annotations, empowering\nresearchers to harness the vast public resources available on the Web to\nadvance their work. Our source code, the dataset and the model are available at\nhttps://anonymous.4open.science/r/EDGE-1CDB.\n", "link": "http://arxiv.org/abs/2410.19461v1", "date": "2024-10-25", "relevancy": 2.3234, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5863}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5817}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EDGE%3A%20Enhanced%20Grounded%20GUI%20Understanding%20with%20Enriched%0A%20%20Multi-Granularity%20Synthetic%20Data&body=Title%3A%20EDGE%3A%20Enhanced%20Grounded%20GUI%20Understanding%20with%20Enriched%0A%20%20Multi-Granularity%20Synthetic%20Data%0AAuthor%3A%20Xuetian%20Chen%20and%20Hangcheng%20Li%20and%20Jiaqing%20Liang%20and%20Sihang%20Jiang%20and%20Deqing%20Yang%0AAbstract%3A%20%20%20Autonomous%20agents%20operating%20on%20the%20graphical%20user%20interfaces%20%28GUIs%29%20of%0Avarious%20applications%20hold%20immense%20practical%20value.%20Unlike%20the%20large%20language%0Amodel%20%28LLM%29-based%20methods%20which%20rely%20on%20structured%20texts%20and%20customized%0Abackends%2C%20the%20approaches%20using%20large%20vision-language%20models%20%28LVLMs%29%20are%20more%0Aintuitive%20and%20adaptable%20as%20they%20can%20visually%20perceive%20and%20directly%20interact%0Awith%20screens%2C%20making%20them%20indispensable%20in%20general%20scenarios%20without%20text%0Ametadata%20and%20tailored%20backends.%20Given%20the%20lack%20of%20high-quality%20training%20data%0Afor%20GUI-related%20tasks%20in%20existing%20work%2C%20this%20paper%20aims%20to%20enhance%20the%20GUI%0Aunderstanding%20and%20interacting%20capabilities%20of%20LVLMs%20through%20a%20data-driven%0Aapproach.%20We%20propose%20EDGE%2C%20a%20general%20data%20synthesis%20framework%20that%0Aautomatically%20generates%20large-scale%2C%20multi-granularity%20training%20data%20from%0Awebpages%20across%20the%20Web.%20Evaluation%20results%20on%20various%20GUI%20and%20agent%20benchmarks%0Ademonstrate%20that%20the%20model%20trained%20with%20the%20dataset%20generated%20through%20EDGE%0Aexhibits%20superior%20webpage%20understanding%20capabilities%2C%20which%20can%20then%20be%20easily%0Atransferred%20to%20previously%20unseen%20desktop%20and%20mobile%20environments.%20Our%20approach%0Asignificantly%20reduces%20the%20dependence%20on%20manual%20annotations%2C%20empowering%0Aresearchers%20to%20harness%20the%20vast%20public%20resources%20available%20on%20the%20Web%20to%0Aadvance%20their%20work.%20Our%20source%20code%2C%20the%20dataset%20and%20the%20model%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/EDGE-1CDB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEDGE%253A%2520Enhanced%2520Grounded%2520GUI%2520Understanding%2520with%2520Enriched%250A%2520%2520Multi-Granularity%2520Synthetic%2520Data%26entry.906535625%3DXuetian%2520Chen%2520and%2520Hangcheng%2520Li%2520and%2520Jiaqing%2520Liang%2520and%2520Sihang%2520Jiang%2520and%2520Deqing%2520Yang%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520operating%2520on%2520the%2520graphical%2520user%2520interfaces%2520%2528GUIs%2529%2520of%250Avarious%2520applications%2520hold%2520immense%2520practical%2520value.%2520Unlike%2520the%2520large%2520language%250Amodel%2520%2528LLM%2529-based%2520methods%2520which%2520rely%2520on%2520structured%2520texts%2520and%2520customized%250Abackends%252C%2520the%2520approaches%2520using%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520more%250Aintuitive%2520and%2520adaptable%2520as%2520they%2520can%2520visually%2520perceive%2520and%2520directly%2520interact%250Awith%2520screens%252C%2520making%2520them%2520indispensable%2520in%2520general%2520scenarios%2520without%2520text%250Ametadata%2520and%2520tailored%2520backends.%2520Given%2520the%2520lack%2520of%2520high-quality%2520training%2520data%250Afor%2520GUI-related%2520tasks%2520in%2520existing%2520work%252C%2520this%2520paper%2520aims%2520to%2520enhance%2520the%2520GUI%250Aunderstanding%2520and%2520interacting%2520capabilities%2520of%2520LVLMs%2520through%2520a%2520data-driven%250Aapproach.%2520We%2520propose%2520EDGE%252C%2520a%2520general%2520data%2520synthesis%2520framework%2520that%250Aautomatically%2520generates%2520large-scale%252C%2520multi-granularity%2520training%2520data%2520from%250Awebpages%2520across%2520the%2520Web.%2520Evaluation%2520results%2520on%2520various%2520GUI%2520and%2520agent%2520benchmarks%250Ademonstrate%2520that%2520the%2520model%2520trained%2520with%2520the%2520dataset%2520generated%2520through%2520EDGE%250Aexhibits%2520superior%2520webpage%2520understanding%2520capabilities%252C%2520which%2520can%2520then%2520be%2520easily%250Atransferred%2520to%2520previously%2520unseen%2520desktop%2520and%2520mobile%2520environments.%2520Our%2520approach%250Asignificantly%2520reduces%2520the%2520dependence%2520on%2520manual%2520annotations%252C%2520empowering%250Aresearchers%2520to%2520harness%2520the%2520vast%2520public%2520resources%2520available%2520on%2520the%2520Web%2520to%250Aadvance%2520their%2520work.%2520Our%2520source%2520code%252C%2520the%2520dataset%2520and%2520the%2520model%2520are%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/EDGE-1CDB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EDGE%3A%20Enhanced%20Grounded%20GUI%20Understanding%20with%20Enriched%0A%20%20Multi-Granularity%20Synthetic%20Data&entry.906535625=Xuetian%20Chen%20and%20Hangcheng%20Li%20and%20Jiaqing%20Liang%20and%20Sihang%20Jiang%20and%20Deqing%20Yang&entry.1292438233=%20%20Autonomous%20agents%20operating%20on%20the%20graphical%20user%20interfaces%20%28GUIs%29%20of%0Avarious%20applications%20hold%20immense%20practical%20value.%20Unlike%20the%20large%20language%0Amodel%20%28LLM%29-based%20methods%20which%20rely%20on%20structured%20texts%20and%20customized%0Abackends%2C%20the%20approaches%20using%20large%20vision-language%20models%20%28LVLMs%29%20are%20more%0Aintuitive%20and%20adaptable%20as%20they%20can%20visually%20perceive%20and%20directly%20interact%0Awith%20screens%2C%20making%20them%20indispensable%20in%20general%20scenarios%20without%20text%0Ametadata%20and%20tailored%20backends.%20Given%20the%20lack%20of%20high-quality%20training%20data%0Afor%20GUI-related%20tasks%20in%20existing%20work%2C%20this%20paper%20aims%20to%20enhance%20the%20GUI%0Aunderstanding%20and%20interacting%20capabilities%20of%20LVLMs%20through%20a%20data-driven%0Aapproach.%20We%20propose%20EDGE%2C%20a%20general%20data%20synthesis%20framework%20that%0Aautomatically%20generates%20large-scale%2C%20multi-granularity%20training%20data%20from%0Awebpages%20across%20the%20Web.%20Evaluation%20results%20on%20various%20GUI%20and%20agent%20benchmarks%0Ademonstrate%20that%20the%20model%20trained%20with%20the%20dataset%20generated%20through%20EDGE%0Aexhibits%20superior%20webpage%20understanding%20capabilities%2C%20which%20can%20then%20be%20easily%0Atransferred%20to%20previously%20unseen%20desktop%20and%20mobile%20environments.%20Our%20approach%0Asignificantly%20reduces%20the%20dependence%20on%20manual%20annotations%2C%20empowering%0Aresearchers%20to%20harness%20the%20vast%20public%20resources%20available%20on%20the%20Web%20to%0Aadvance%20their%20work.%20Our%20source%20code%2C%20the%20dataset%20and%20the%20model%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/EDGE-1CDB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19461v1&entry.124074799=Read"},
{"title": "Frozen-DETR: Enhancing DETR with Image Understanding from Frozen\n  Foundation Models", "author": "Shenghao Fu and Junkai Yan and Qize Yang and Xihan Wei and Xiaohua Xie and Wei-Shi Zheng", "abstract": "  Recent vision foundation models can extract universal representations and\nshow impressive abilities in various tasks. However, their application on\nobject detection is largely overlooked, especially without fine-tuning them. In\nthis work, we show that frozen foundation models can be a versatile feature\nenhancer, even though they are not pre-trained for object detection.\nSpecifically, we explore directly transferring the high-level image\nunderstanding of foundation models to detectors in the following two ways.\nFirst, the class token in foundation models provides an in-depth understanding\nof the complex scene, which facilitates decoding object queries in the\ndetector's decoder by providing a compact context. Additionally, the patch\ntokens in foundation models can enrich the features in the detector's encoder\nby providing semantic details. Utilizing frozen foundation models as\nplug-and-play modules rather than the commonly used backbone can significantly\nenhance the detector's performance while preventing the problems caused by the\narchitecture discrepancy between the detector's backbone and the foundation\nmodel. With such a novel paradigm, we boost the SOTA query-based detector DINO\nfrom 49.0% AP to 51.9% AP (+2.9% AP) and further to 53.8% AP (+4.8% AP) by\nintegrating one or two foundation models respectively, on the COCO validation\nset after training for 12 epochs with R50 as the detector's backbone.\n", "link": "http://arxiv.org/abs/2410.19635v1", "date": "2024-10-25", "relevancy": 2.3113, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frozen-DETR%3A%20Enhancing%20DETR%20with%20Image%20Understanding%20from%20Frozen%0A%20%20Foundation%20Models&body=Title%3A%20Frozen-DETR%3A%20Enhancing%20DETR%20with%20Image%20Understanding%20from%20Frozen%0A%20%20Foundation%20Models%0AAuthor%3A%20Shenghao%20Fu%20and%20Junkai%20Yan%20and%20Qize%20Yang%20and%20Xihan%20Wei%20and%20Xiaohua%20Xie%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20%20%20Recent%20vision%20foundation%20models%20can%20extract%20universal%20representations%20and%0Ashow%20impressive%20abilities%20in%20various%20tasks.%20However%2C%20their%20application%20on%0Aobject%20detection%20is%20largely%20overlooked%2C%20especially%20without%20fine-tuning%20them.%20In%0Athis%20work%2C%20we%20show%20that%20frozen%20foundation%20models%20can%20be%20a%20versatile%20feature%0Aenhancer%2C%20even%20though%20they%20are%20not%20pre-trained%20for%20object%20detection.%0ASpecifically%2C%20we%20explore%20directly%20transferring%20the%20high-level%20image%0Aunderstanding%20of%20foundation%20models%20to%20detectors%20in%20the%20following%20two%20ways.%0AFirst%2C%20the%20class%20token%20in%20foundation%20models%20provides%20an%20in-depth%20understanding%0Aof%20the%20complex%20scene%2C%20which%20facilitates%20decoding%20object%20queries%20in%20the%0Adetector%27s%20decoder%20by%20providing%20a%20compact%20context.%20Additionally%2C%20the%20patch%0Atokens%20in%20foundation%20models%20can%20enrich%20the%20features%20in%20the%20detector%27s%20encoder%0Aby%20providing%20semantic%20details.%20Utilizing%20frozen%20foundation%20models%20as%0Aplug-and-play%20modules%20rather%20than%20the%20commonly%20used%20backbone%20can%20significantly%0Aenhance%20the%20detector%27s%20performance%20while%20preventing%20the%20problems%20caused%20by%20the%0Aarchitecture%20discrepancy%20between%20the%20detector%27s%20backbone%20and%20the%20foundation%0Amodel.%20With%20such%20a%20novel%20paradigm%2C%20we%20boost%20the%20SOTA%20query-based%20detector%20DINO%0Afrom%2049.0%25%20AP%20to%2051.9%25%20AP%20%28%2B2.9%25%20AP%29%20and%20further%20to%2053.8%25%20AP%20%28%2B4.8%25%20AP%29%20by%0Aintegrating%20one%20or%20two%20foundation%20models%20respectively%2C%20on%20the%20COCO%20validation%0Aset%20after%20training%20for%2012%20epochs%20with%20R50%20as%20the%20detector%27s%20backbone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrozen-DETR%253A%2520Enhancing%2520DETR%2520with%2520Image%2520Understanding%2520from%2520Frozen%250A%2520%2520Foundation%2520Models%26entry.906535625%3DShenghao%2520Fu%2520and%2520Junkai%2520Yan%2520and%2520Qize%2520Yang%2520and%2520Xihan%2520Wei%2520and%2520Xiaohua%2520Xie%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3D%2520%2520Recent%2520vision%2520foundation%2520models%2520can%2520extract%2520universal%2520representations%2520and%250Ashow%2520impressive%2520abilities%2520in%2520various%2520tasks.%2520However%252C%2520their%2520application%2520on%250Aobject%2520detection%2520is%2520largely%2520overlooked%252C%2520especially%2520without%2520fine-tuning%2520them.%2520In%250Athis%2520work%252C%2520we%2520show%2520that%2520frozen%2520foundation%2520models%2520can%2520be%2520a%2520versatile%2520feature%250Aenhancer%252C%2520even%2520though%2520they%2520are%2520not%2520pre-trained%2520for%2520object%2520detection.%250ASpecifically%252C%2520we%2520explore%2520directly%2520transferring%2520the%2520high-level%2520image%250Aunderstanding%2520of%2520foundation%2520models%2520to%2520detectors%2520in%2520the%2520following%2520two%2520ways.%250AFirst%252C%2520the%2520class%2520token%2520in%2520foundation%2520models%2520provides%2520an%2520in-depth%2520understanding%250Aof%2520the%2520complex%2520scene%252C%2520which%2520facilitates%2520decoding%2520object%2520queries%2520in%2520the%250Adetector%2527s%2520decoder%2520by%2520providing%2520a%2520compact%2520context.%2520Additionally%252C%2520the%2520patch%250Atokens%2520in%2520foundation%2520models%2520can%2520enrich%2520the%2520features%2520in%2520the%2520detector%2527s%2520encoder%250Aby%2520providing%2520semantic%2520details.%2520Utilizing%2520frozen%2520foundation%2520models%2520as%250Aplug-and-play%2520modules%2520rather%2520than%2520the%2520commonly%2520used%2520backbone%2520can%2520significantly%250Aenhance%2520the%2520detector%2527s%2520performance%2520while%2520preventing%2520the%2520problems%2520caused%2520by%2520the%250Aarchitecture%2520discrepancy%2520between%2520the%2520detector%2527s%2520backbone%2520and%2520the%2520foundation%250Amodel.%2520With%2520such%2520a%2520novel%2520paradigm%252C%2520we%2520boost%2520the%2520SOTA%2520query-based%2520detector%2520DINO%250Afrom%252049.0%2525%2520AP%2520to%252051.9%2525%2520AP%2520%2528%252B2.9%2525%2520AP%2529%2520and%2520further%2520to%252053.8%2525%2520AP%2520%2528%252B4.8%2525%2520AP%2529%2520by%250Aintegrating%2520one%2520or%2520two%2520foundation%2520models%2520respectively%252C%2520on%2520the%2520COCO%2520validation%250Aset%2520after%2520training%2520for%252012%2520epochs%2520with%2520R50%2520as%2520the%2520detector%2527s%2520backbone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frozen-DETR%3A%20Enhancing%20DETR%20with%20Image%20Understanding%20from%20Frozen%0A%20%20Foundation%20Models&entry.906535625=Shenghao%20Fu%20and%20Junkai%20Yan%20and%20Qize%20Yang%20and%20Xihan%20Wei%20and%20Xiaohua%20Xie%20and%20Wei-Shi%20Zheng&entry.1292438233=%20%20Recent%20vision%20foundation%20models%20can%20extract%20universal%20representations%20and%0Ashow%20impressive%20abilities%20in%20various%20tasks.%20However%2C%20their%20application%20on%0Aobject%20detection%20is%20largely%20overlooked%2C%20especially%20without%20fine-tuning%20them.%20In%0Athis%20work%2C%20we%20show%20that%20frozen%20foundation%20models%20can%20be%20a%20versatile%20feature%0Aenhancer%2C%20even%20though%20they%20are%20not%20pre-trained%20for%20object%20detection.%0ASpecifically%2C%20we%20explore%20directly%20transferring%20the%20high-level%20image%0Aunderstanding%20of%20foundation%20models%20to%20detectors%20in%20the%20following%20two%20ways.%0AFirst%2C%20the%20class%20token%20in%20foundation%20models%20provides%20an%20in-depth%20understanding%0Aof%20the%20complex%20scene%2C%20which%20facilitates%20decoding%20object%20queries%20in%20the%0Adetector%27s%20decoder%20by%20providing%20a%20compact%20context.%20Additionally%2C%20the%20patch%0Atokens%20in%20foundation%20models%20can%20enrich%20the%20features%20in%20the%20detector%27s%20encoder%0Aby%20providing%20semantic%20details.%20Utilizing%20frozen%20foundation%20models%20as%0Aplug-and-play%20modules%20rather%20than%20the%20commonly%20used%20backbone%20can%20significantly%0Aenhance%20the%20detector%27s%20performance%20while%20preventing%20the%20problems%20caused%20by%20the%0Aarchitecture%20discrepancy%20between%20the%20detector%27s%20backbone%20and%20the%20foundation%0Amodel.%20With%20such%20a%20novel%20paradigm%2C%20we%20boost%20the%20SOTA%20query-based%20detector%20DINO%0Afrom%2049.0%25%20AP%20to%2051.9%25%20AP%20%28%2B2.9%25%20AP%29%20and%20further%20to%2053.8%25%20AP%20%28%2B4.8%25%20AP%29%20by%0Aintegrating%20one%20or%20two%20foundation%20models%20respectively%2C%20on%20the%20COCO%20validation%0Aset%20after%20training%20for%2012%20epochs%20with%20R50%20as%20the%20detector%27s%20backbone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19635v1&entry.124074799=Read"},
{"title": "Floralens: a Deep Learning Model for the Portuguese Native Flora", "author": "Ant\u00f3nio Filgueiras and Eduardo R. B. Marques and Lu\u00eds M. B. Lopes and Miguel Marques and Hugo Silva", "abstract": "  Machine-learning techniques, especially deep convolutional neural networks,\nare pivotal for image-based identification of biological species in many\nCitizen Science platforms. In this paper, we describe the construction of a\ndataset for the Portuguese native flora based on publicly available\nresearch-grade datasets, and the derivation of a high-accuracy model from it\nusing off-the-shelf deep convolutional neural networks. We anchored the dataset\nin high-quality data provided by Sociedade Portuguesa de Bot\\^anica and added\nfurther sampled data from research-grade datasets available from GBIF. We find\nthat with a careful dataset design, off-the-shelf machine-learning cloud\nservices such as Google's AutoML Vision produce accurate models, with results\ncomparable to those of Pl@ntNet, a state-of-the-art citizen science platform.\nThe best model we derived, dubbed Floralens, has been integrated into the\npublic website of Project Biolens, where we gather models for other taxa as\nwell. The dataset used to train the model is also publicly available on Zenodo.\n", "link": "http://arxiv.org/abs/2403.12072v2", "date": "2024-10-25", "relevancy": 2.3069, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Floralens%3A%20a%20Deep%20Learning%20Model%20for%20the%20Portuguese%20Native%20Flora&body=Title%3A%20Floralens%3A%20a%20Deep%20Learning%20Model%20for%20the%20Portuguese%20Native%20Flora%0AAuthor%3A%20Ant%C3%B3nio%20Filgueiras%20and%20Eduardo%20R.%20B.%20Marques%20and%20Lu%C3%ADs%20M.%20B.%20Lopes%20and%20Miguel%20Marques%20and%20Hugo%20Silva%0AAbstract%3A%20%20%20Machine-learning%20techniques%2C%20especially%20deep%20convolutional%20neural%20networks%2C%0Aare%20pivotal%20for%20image-based%20identification%20of%20biological%20species%20in%20many%0ACitizen%20Science%20platforms.%20In%20this%20paper%2C%20we%20describe%20the%20construction%20of%20a%0Adataset%20for%20the%20Portuguese%20native%20flora%20based%20on%20publicly%20available%0Aresearch-grade%20datasets%2C%20and%20the%20derivation%20of%20a%20high-accuracy%20model%20from%20it%0Ausing%20off-the-shelf%20deep%20convolutional%20neural%20networks.%20We%20anchored%20the%20dataset%0Ain%20high-quality%20data%20provided%20by%20Sociedade%20Portuguesa%20de%20Bot%5C%5Eanica%20and%20added%0Afurther%20sampled%20data%20from%20research-grade%20datasets%20available%20from%20GBIF.%20We%20find%0Athat%20with%20a%20careful%20dataset%20design%2C%20off-the-shelf%20machine-learning%20cloud%0Aservices%20such%20as%20Google%27s%20AutoML%20Vision%20produce%20accurate%20models%2C%20with%20results%0Acomparable%20to%20those%20of%20Pl%40ntNet%2C%20a%20state-of-the-art%20citizen%20science%20platform.%0AThe%20best%20model%20we%20derived%2C%20dubbed%20Floralens%2C%20has%20been%20integrated%20into%20the%0Apublic%20website%20of%20Project%20Biolens%2C%20where%20we%20gather%20models%20for%20other%20taxa%20as%0Awell.%20The%20dataset%20used%20to%20train%20the%20model%20is%20also%20publicly%20available%20on%20Zenodo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12072v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFloralens%253A%2520a%2520Deep%2520Learning%2520Model%2520for%2520the%2520Portuguese%2520Native%2520Flora%26entry.906535625%3DAnt%25C3%25B3nio%2520Filgueiras%2520and%2520Eduardo%2520R.%2520B.%2520Marques%2520and%2520Lu%25C3%25ADs%2520M.%2520B.%2520Lopes%2520and%2520Miguel%2520Marques%2520and%2520Hugo%2520Silva%26entry.1292438233%3D%2520%2520Machine-learning%2520techniques%252C%2520especially%2520deep%2520convolutional%2520neural%2520networks%252C%250Aare%2520pivotal%2520for%2520image-based%2520identification%2520of%2520biological%2520species%2520in%2520many%250ACitizen%2520Science%2520platforms.%2520In%2520this%2520paper%252C%2520we%2520describe%2520the%2520construction%2520of%2520a%250Adataset%2520for%2520the%2520Portuguese%2520native%2520flora%2520based%2520on%2520publicly%2520available%250Aresearch-grade%2520datasets%252C%2520and%2520the%2520derivation%2520of%2520a%2520high-accuracy%2520model%2520from%2520it%250Ausing%2520off-the-shelf%2520deep%2520convolutional%2520neural%2520networks.%2520We%2520anchored%2520the%2520dataset%250Ain%2520high-quality%2520data%2520provided%2520by%2520Sociedade%2520Portuguesa%2520de%2520Bot%255C%255Eanica%2520and%2520added%250Afurther%2520sampled%2520data%2520from%2520research-grade%2520datasets%2520available%2520from%2520GBIF.%2520We%2520find%250Athat%2520with%2520a%2520careful%2520dataset%2520design%252C%2520off-the-shelf%2520machine-learning%2520cloud%250Aservices%2520such%2520as%2520Google%2527s%2520AutoML%2520Vision%2520produce%2520accurate%2520models%252C%2520with%2520results%250Acomparable%2520to%2520those%2520of%2520Pl%2540ntNet%252C%2520a%2520state-of-the-art%2520citizen%2520science%2520platform.%250AThe%2520best%2520model%2520we%2520derived%252C%2520dubbed%2520Floralens%252C%2520has%2520been%2520integrated%2520into%2520the%250Apublic%2520website%2520of%2520Project%2520Biolens%252C%2520where%2520we%2520gather%2520models%2520for%2520other%2520taxa%2520as%250Awell.%2520The%2520dataset%2520used%2520to%2520train%2520the%2520model%2520is%2520also%2520publicly%2520available%2520on%2520Zenodo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12072v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Floralens%3A%20a%20Deep%20Learning%20Model%20for%20the%20Portuguese%20Native%20Flora&entry.906535625=Ant%C3%B3nio%20Filgueiras%20and%20Eduardo%20R.%20B.%20Marques%20and%20Lu%C3%ADs%20M.%20B.%20Lopes%20and%20Miguel%20Marques%20and%20Hugo%20Silva&entry.1292438233=%20%20Machine-learning%20techniques%2C%20especially%20deep%20convolutional%20neural%20networks%2C%0Aare%20pivotal%20for%20image-based%20identification%20of%20biological%20species%20in%20many%0ACitizen%20Science%20platforms.%20In%20this%20paper%2C%20we%20describe%20the%20construction%20of%20a%0Adataset%20for%20the%20Portuguese%20native%20flora%20based%20on%20publicly%20available%0Aresearch-grade%20datasets%2C%20and%20the%20derivation%20of%20a%20high-accuracy%20model%20from%20it%0Ausing%20off-the-shelf%20deep%20convolutional%20neural%20networks.%20We%20anchored%20the%20dataset%0Ain%20high-quality%20data%20provided%20by%20Sociedade%20Portuguesa%20de%20Bot%5C%5Eanica%20and%20added%0Afurther%20sampled%20data%20from%20research-grade%20datasets%20available%20from%20GBIF.%20We%20find%0Athat%20with%20a%20careful%20dataset%20design%2C%20off-the-shelf%20machine-learning%20cloud%0Aservices%20such%20as%20Google%27s%20AutoML%20Vision%20produce%20accurate%20models%2C%20with%20results%0Acomparable%20to%20those%20of%20Pl%40ntNet%2C%20a%20state-of-the-art%20citizen%20science%20platform.%0AThe%20best%20model%20we%20derived%2C%20dubbed%20Floralens%2C%20has%20been%20integrated%20into%20the%0Apublic%20website%20of%20Project%20Biolens%2C%20where%20we%20gather%20models%20for%20other%20taxa%20as%0Awell.%20The%20dataset%20used%20to%20train%20the%20model%20is%20also%20publicly%20available%20on%20Zenodo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12072v2&entry.124074799=Read"},
{"title": "UnCLe: Unsupervised Continual Learning of Depth Completion", "author": "Suchisrit Gangopadhyay and Xien Chen and Michael Chu and Patrick Rim and Hyoungseob Park and Alex Wong", "abstract": "  We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform.\n", "link": "http://arxiv.org/abs/2410.18074v2", "date": "2024-10-25", "relevancy": 2.2604, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5736}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.567}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnCLe%3A%20Unsupervised%20Continual%20Learning%20of%20Depth%20Completion&body=Title%3A%20UnCLe%3A%20Unsupervised%20Continual%20Learning%20of%20Depth%20Completion%0AAuthor%3A%20Suchisrit%20Gangopadhyay%20and%20Xien%20Chen%20and%20Michael%20Chu%20and%20Patrick%20Rim%20and%20Hyoungseob%20Park%20and%20Alex%20Wong%0AAbstract%3A%20%20%20We%20propose%20UnCLe%2C%20a%20standardized%20benchmark%20for%20Unsupervised%20Continual%0ALearning%20of%20a%20multimodal%20depth%20estimation%20task%3A%20Depth%20completion%20aims%20to%20infer%0Aa%20dense%20depth%20map%20from%20a%20pair%20of%20synchronized%20RGB%20image%20and%20sparse%20depth%20map.%0AWe%20benchmark%20depth%20completion%20models%20under%20the%20practical%20scenario%20of%0Aunsupervised%20learning%20over%20continuous%20streams%20of%20data.%20Existing%20methods%20are%0Atypically%20trained%20on%20a%20static%2C%20or%20stationary%2C%20dataset.%20However%2C%20when%20adapting%0Ato%20novel%20non-stationary%20distributions%2C%20they%20%22catastrophically%20forget%22%0Apreviously%20learned%20information.%20UnCLe%20simulates%20these%20non-stationary%0Adistributions%20by%20adapting%20depth%20completion%20models%20to%20sequences%20of%20datasets%0Acontaining%20diverse%20scenes%20captured%20from%20distinct%20domains%20using%20different%20visual%0Aand%20range%20sensors.%20We%20adopt%20representative%20methods%20from%20continual%20learning%0Aparadigms%20and%20translate%20them%20to%20enable%20unsupervised%20continual%20learning%20of%20depth%0Acompletion.%20We%20benchmark%20these%20models%20for%20indoor%20and%20outdoor%20and%20investigate%0Athe%20degree%20of%20catastrophic%20forgetting%20through%20standard%20quantitative%20metrics.%0AFurthermore%2C%20we%20introduce%20model%20inversion%20quality%20as%20an%20additional%20measure%20of%0Aforgetting.%20We%20find%20that%20unsupervised%20continual%20learning%20of%20depth%20completion%20is%0Aan%20open%20problem%2C%20and%20we%20invite%20researchers%20to%20leverage%20UnCLe%20as%20a%20development%0Aplatform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18074v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnCLe%253A%2520Unsupervised%2520Continual%2520Learning%2520of%2520Depth%2520Completion%26entry.906535625%3DSuchisrit%2520Gangopadhyay%2520and%2520Xien%2520Chen%2520and%2520Michael%2520Chu%2520and%2520Patrick%2520Rim%2520and%2520Hyoungseob%2520Park%2520and%2520Alex%2520Wong%26entry.1292438233%3D%2520%2520We%2520propose%2520UnCLe%252C%2520a%2520standardized%2520benchmark%2520for%2520Unsupervised%2520Continual%250ALearning%2520of%2520a%2520multimodal%2520depth%2520estimation%2520task%253A%2520Depth%2520completion%2520aims%2520to%2520infer%250Aa%2520dense%2520depth%2520map%2520from%2520a%2520pair%2520of%2520synchronized%2520RGB%2520image%2520and%2520sparse%2520depth%2520map.%250AWe%2520benchmark%2520depth%2520completion%2520models%2520under%2520the%2520practical%2520scenario%2520of%250Aunsupervised%2520learning%2520over%2520continuous%2520streams%2520of%2520data.%2520Existing%2520methods%2520are%250Atypically%2520trained%2520on%2520a%2520static%252C%2520or%2520stationary%252C%2520dataset.%2520However%252C%2520when%2520adapting%250Ato%2520novel%2520non-stationary%2520distributions%252C%2520they%2520%2522catastrophically%2520forget%2522%250Apreviously%2520learned%2520information.%2520UnCLe%2520simulates%2520these%2520non-stationary%250Adistributions%2520by%2520adapting%2520depth%2520completion%2520models%2520to%2520sequences%2520of%2520datasets%250Acontaining%2520diverse%2520scenes%2520captured%2520from%2520distinct%2520domains%2520using%2520different%2520visual%250Aand%2520range%2520sensors.%2520We%2520adopt%2520representative%2520methods%2520from%2520continual%2520learning%250Aparadigms%2520and%2520translate%2520them%2520to%2520enable%2520unsupervised%2520continual%2520learning%2520of%2520depth%250Acompletion.%2520We%2520benchmark%2520these%2520models%2520for%2520indoor%2520and%2520outdoor%2520and%2520investigate%250Athe%2520degree%2520of%2520catastrophic%2520forgetting%2520through%2520standard%2520quantitative%2520metrics.%250AFurthermore%252C%2520we%2520introduce%2520model%2520inversion%2520quality%2520as%2520an%2520additional%2520measure%2520of%250Aforgetting.%2520We%2520find%2520that%2520unsupervised%2520continual%2520learning%2520of%2520depth%2520completion%2520is%250Aan%2520open%2520problem%252C%2520and%2520we%2520invite%2520researchers%2520to%2520leverage%2520UnCLe%2520as%2520a%2520development%250Aplatform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18074v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnCLe%3A%20Unsupervised%20Continual%20Learning%20of%20Depth%20Completion&entry.906535625=Suchisrit%20Gangopadhyay%20and%20Xien%20Chen%20and%20Michael%20Chu%20and%20Patrick%20Rim%20and%20Hyoungseob%20Park%20and%20Alex%20Wong&entry.1292438233=%20%20We%20propose%20UnCLe%2C%20a%20standardized%20benchmark%20for%20Unsupervised%20Continual%0ALearning%20of%20a%20multimodal%20depth%20estimation%20task%3A%20Depth%20completion%20aims%20to%20infer%0Aa%20dense%20depth%20map%20from%20a%20pair%20of%20synchronized%20RGB%20image%20and%20sparse%20depth%20map.%0AWe%20benchmark%20depth%20completion%20models%20under%20the%20practical%20scenario%20of%0Aunsupervised%20learning%20over%20continuous%20streams%20of%20data.%20Existing%20methods%20are%0Atypically%20trained%20on%20a%20static%2C%20or%20stationary%2C%20dataset.%20However%2C%20when%20adapting%0Ato%20novel%20non-stationary%20distributions%2C%20they%20%22catastrophically%20forget%22%0Apreviously%20learned%20information.%20UnCLe%20simulates%20these%20non-stationary%0Adistributions%20by%20adapting%20depth%20completion%20models%20to%20sequences%20of%20datasets%0Acontaining%20diverse%20scenes%20captured%20from%20distinct%20domains%20using%20different%20visual%0Aand%20range%20sensors.%20We%20adopt%20representative%20methods%20from%20continual%20learning%0Aparadigms%20and%20translate%20them%20to%20enable%20unsupervised%20continual%20learning%20of%20depth%0Acompletion.%20We%20benchmark%20these%20models%20for%20indoor%20and%20outdoor%20and%20investigate%0Athe%20degree%20of%20catastrophic%20forgetting%20through%20standard%20quantitative%20metrics.%0AFurthermore%2C%20we%20introduce%20model%20inversion%20quality%20as%20an%20additional%20measure%20of%0Aforgetting.%20We%20find%20that%20unsupervised%20continual%20learning%20of%20depth%20completion%20is%0Aan%20open%20problem%2C%20and%20we%20invite%20researchers%20to%20leverage%20UnCLe%20as%20a%20development%0Aplatform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18074v2&entry.124074799=Read"},
{"title": "Multi-modal Motion Prediction using Temporal Ensembling with\n  Learning-based Aggregation", "author": "Kai-Yin Hong and Chieh-Chih Wang and Wen-Chieh Lin", "abstract": "  Recent years have seen a shift towards learning-based methods for trajectory\nprediction, with challenges remaining in addressing uncertainty and capturing\nmulti-modal distributions. This paper introduces Temporal Ensembling with\nLearning-based Aggregation, a meta-algorithm designed to mitigate the issue of\nmissing behaviors in trajectory prediction, which leads to inconsistent\npredictions across consecutive frames. Unlike conventional model ensembling,\ntemporal ensembling leverages predictions from nearby frames to enhance spatial\ncoverage and prediction diversity. By confirming predictions from multiple\nframes, temporal ensembling compensates for occasional errors in individual\nframe predictions. Furthermore, trajectory-level aggregation, often utilized in\nmodel ensembling, is insufficient for temporal ensembling due to a lack of\nconsideration of traffic context and its tendency to assign candidate\ntrajectories with incorrect driving behaviors to final predictions. We further\nemphasize the necessity of learning-based aggregation by utilizing mode queries\nwithin a DETR-like architecture for our temporal ensembling, leveraging the\ncharacteristics of predictions from nearby frames. Our method, validated on the\nArgoverse 2 dataset, shows notable improvements: a 4% reduction in minADE, a 5%\ndecrease in minFDE, and a 1.16% reduction in the miss rate compared to the\nstrongest baseline, QCNet, highlighting its efficacy and potential in\nautonomous driving.\n", "link": "http://arxiv.org/abs/2410.19606v1", "date": "2024-10-25", "relevancy": 2.2452, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5537}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Motion%20Prediction%20using%20Temporal%20Ensembling%20with%0A%20%20Learning-based%20Aggregation&body=Title%3A%20Multi-modal%20Motion%20Prediction%20using%20Temporal%20Ensembling%20with%0A%20%20Learning-based%20Aggregation%0AAuthor%3A%20Kai-Yin%20Hong%20and%20Chieh-Chih%20Wang%20and%20Wen-Chieh%20Lin%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20a%20shift%20towards%20learning-based%20methods%20for%20trajectory%0Aprediction%2C%20with%20challenges%20remaining%20in%20addressing%20uncertainty%20and%20capturing%0Amulti-modal%20distributions.%20This%20paper%20introduces%20Temporal%20Ensembling%20with%0ALearning-based%20Aggregation%2C%20a%20meta-algorithm%20designed%20to%20mitigate%20the%20issue%20of%0Amissing%20behaviors%20in%20trajectory%20prediction%2C%20which%20leads%20to%20inconsistent%0Apredictions%20across%20consecutive%20frames.%20Unlike%20conventional%20model%20ensembling%2C%0Atemporal%20ensembling%20leverages%20predictions%20from%20nearby%20frames%20to%20enhance%20spatial%0Acoverage%20and%20prediction%20diversity.%20By%20confirming%20predictions%20from%20multiple%0Aframes%2C%20temporal%20ensembling%20compensates%20for%20occasional%20errors%20in%20individual%0Aframe%20predictions.%20Furthermore%2C%20trajectory-level%20aggregation%2C%20often%20utilized%20in%0Amodel%20ensembling%2C%20is%20insufficient%20for%20temporal%20ensembling%20due%20to%20a%20lack%20of%0Aconsideration%20of%20traffic%20context%20and%20its%20tendency%20to%20assign%20candidate%0Atrajectories%20with%20incorrect%20driving%20behaviors%20to%20final%20predictions.%20We%20further%0Aemphasize%20the%20necessity%20of%20learning-based%20aggregation%20by%20utilizing%20mode%20queries%0Awithin%20a%20DETR-like%20architecture%20for%20our%20temporal%20ensembling%2C%20leveraging%20the%0Acharacteristics%20of%20predictions%20from%20nearby%20frames.%20Our%20method%2C%20validated%20on%20the%0AArgoverse%202%20dataset%2C%20shows%20notable%20improvements%3A%20a%204%25%20reduction%20in%20minADE%2C%20a%205%25%0Adecrease%20in%20minFDE%2C%20and%20a%201.16%25%20reduction%20in%20the%20miss%20rate%20compared%20to%20the%0Astrongest%20baseline%2C%20QCNet%2C%20highlighting%20its%20efficacy%20and%20potential%20in%0Aautonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Motion%2520Prediction%2520using%2520Temporal%2520Ensembling%2520with%250A%2520%2520Learning-based%2520Aggregation%26entry.906535625%3DKai-Yin%2520Hong%2520and%2520Chieh-Chih%2520Wang%2520and%2520Wen-Chieh%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520a%2520shift%2520towards%2520learning-based%2520methods%2520for%2520trajectory%250Aprediction%252C%2520with%2520challenges%2520remaining%2520in%2520addressing%2520uncertainty%2520and%2520capturing%250Amulti-modal%2520distributions.%2520This%2520paper%2520introduces%2520Temporal%2520Ensembling%2520with%250ALearning-based%2520Aggregation%252C%2520a%2520meta-algorithm%2520designed%2520to%2520mitigate%2520the%2520issue%2520of%250Amissing%2520behaviors%2520in%2520trajectory%2520prediction%252C%2520which%2520leads%2520to%2520inconsistent%250Apredictions%2520across%2520consecutive%2520frames.%2520Unlike%2520conventional%2520model%2520ensembling%252C%250Atemporal%2520ensembling%2520leverages%2520predictions%2520from%2520nearby%2520frames%2520to%2520enhance%2520spatial%250Acoverage%2520and%2520prediction%2520diversity.%2520By%2520confirming%2520predictions%2520from%2520multiple%250Aframes%252C%2520temporal%2520ensembling%2520compensates%2520for%2520occasional%2520errors%2520in%2520individual%250Aframe%2520predictions.%2520Furthermore%252C%2520trajectory-level%2520aggregation%252C%2520often%2520utilized%2520in%250Amodel%2520ensembling%252C%2520is%2520insufficient%2520for%2520temporal%2520ensembling%2520due%2520to%2520a%2520lack%2520of%250Aconsideration%2520of%2520traffic%2520context%2520and%2520its%2520tendency%2520to%2520assign%2520candidate%250Atrajectories%2520with%2520incorrect%2520driving%2520behaviors%2520to%2520final%2520predictions.%2520We%2520further%250Aemphasize%2520the%2520necessity%2520of%2520learning-based%2520aggregation%2520by%2520utilizing%2520mode%2520queries%250Awithin%2520a%2520DETR-like%2520architecture%2520for%2520our%2520temporal%2520ensembling%252C%2520leveraging%2520the%250Acharacteristics%2520of%2520predictions%2520from%2520nearby%2520frames.%2520Our%2520method%252C%2520validated%2520on%2520the%250AArgoverse%25202%2520dataset%252C%2520shows%2520notable%2520improvements%253A%2520a%25204%2525%2520reduction%2520in%2520minADE%252C%2520a%25205%2525%250Adecrease%2520in%2520minFDE%252C%2520and%2520a%25201.16%2525%2520reduction%2520in%2520the%2520miss%2520rate%2520compared%2520to%2520the%250Astrongest%2520baseline%252C%2520QCNet%252C%2520highlighting%2520its%2520efficacy%2520and%2520potential%2520in%250Aautonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Motion%20Prediction%20using%20Temporal%20Ensembling%20with%0A%20%20Learning-based%20Aggregation&entry.906535625=Kai-Yin%20Hong%20and%20Chieh-Chih%20Wang%20and%20Wen-Chieh%20Lin&entry.1292438233=%20%20Recent%20years%20have%20seen%20a%20shift%20towards%20learning-based%20methods%20for%20trajectory%0Aprediction%2C%20with%20challenges%20remaining%20in%20addressing%20uncertainty%20and%20capturing%0Amulti-modal%20distributions.%20This%20paper%20introduces%20Temporal%20Ensembling%20with%0ALearning-based%20Aggregation%2C%20a%20meta-algorithm%20designed%20to%20mitigate%20the%20issue%20of%0Amissing%20behaviors%20in%20trajectory%20prediction%2C%20which%20leads%20to%20inconsistent%0Apredictions%20across%20consecutive%20frames.%20Unlike%20conventional%20model%20ensembling%2C%0Atemporal%20ensembling%20leverages%20predictions%20from%20nearby%20frames%20to%20enhance%20spatial%0Acoverage%20and%20prediction%20diversity.%20By%20confirming%20predictions%20from%20multiple%0Aframes%2C%20temporal%20ensembling%20compensates%20for%20occasional%20errors%20in%20individual%0Aframe%20predictions.%20Furthermore%2C%20trajectory-level%20aggregation%2C%20often%20utilized%20in%0Amodel%20ensembling%2C%20is%20insufficient%20for%20temporal%20ensembling%20due%20to%20a%20lack%20of%0Aconsideration%20of%20traffic%20context%20and%20its%20tendency%20to%20assign%20candidate%0Atrajectories%20with%20incorrect%20driving%20behaviors%20to%20final%20predictions.%20We%20further%0Aemphasize%20the%20necessity%20of%20learning-based%20aggregation%20by%20utilizing%20mode%20queries%0Awithin%20a%20DETR-like%20architecture%20for%20our%20temporal%20ensembling%2C%20leveraging%20the%0Acharacteristics%20of%20predictions%20from%20nearby%20frames.%20Our%20method%2C%20validated%20on%20the%0AArgoverse%202%20dataset%2C%20shows%20notable%20improvements%3A%20a%204%25%20reduction%20in%20minADE%2C%20a%205%25%0Adecrease%20in%20minFDE%2C%20and%20a%201.16%25%20reduction%20in%20the%20miss%20rate%20compared%20to%20the%0Astrongest%20baseline%2C%20QCNet%2C%20highlighting%20its%20efficacy%20and%20potential%20in%0Aautonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19606v1&entry.124074799=Read"},
{"title": "Adversarial Environment Design via Regret-Guided Diffusion Models", "author": "Hojun Chung and Junseo Lee and Minsoo Kim and Dohyeong Kim and Songhwai Oh", "abstract": "  Training agents that are robust to environmental changes remains a\nsignificant challenge in deep reinforcement learning (RL). Unsupervised\nenvironment design (UED) has recently emerged to address this issue by\ngenerating a set of training environments tailored to the agent's capabilities.\nWhile prior works demonstrate that UED has the potential to learn a robust\npolicy, their performance is constrained by the capabilities of the environment\ngeneration. To this end, we propose a novel UED algorithm, adversarial\nenvironment design via regret-guided diffusion models (ADD). The proposed\nmethod guides the diffusion-based environment generator with the regret of the\nagent to produce environments that the agent finds challenging but conducive to\nfurther improvement. By exploiting the representation power of diffusion\nmodels, ADD can directly generate adversarial environments while maintaining\nthe diversity of training environments, enabling the agent to effectively learn\na robust policy. Our experimental results demonstrate that the proposed method\nsuccessfully generates an instructive curriculum of environments, outperforming\nUED baselines in zero-shot generalization across novel, out-of-distribution\nenvironments. Project page: https://github.com/rllab-snu.github.io/projects/ADD\n", "link": "http://arxiv.org/abs/2410.19715v1", "date": "2024-10-25", "relevancy": 2.2332, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5729}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5518}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Environment%20Design%20via%20Regret-Guided%20Diffusion%20Models&body=Title%3A%20Adversarial%20Environment%20Design%20via%20Regret-Guided%20Diffusion%20Models%0AAuthor%3A%20Hojun%20Chung%20and%20Junseo%20Lee%20and%20Minsoo%20Kim%20and%20Dohyeong%20Kim%20and%20Songhwai%20Oh%0AAbstract%3A%20%20%20Training%20agents%20that%20are%20robust%20to%20environmental%20changes%20remains%20a%0Asignificant%20challenge%20in%20deep%20reinforcement%20learning%20%28RL%29.%20Unsupervised%0Aenvironment%20design%20%28UED%29%20has%20recently%20emerged%20to%20address%20this%20issue%20by%0Agenerating%20a%20set%20of%20training%20environments%20tailored%20to%20the%20agent%27s%20capabilities.%0AWhile%20prior%20works%20demonstrate%20that%20UED%20has%20the%20potential%20to%20learn%20a%20robust%0Apolicy%2C%20their%20performance%20is%20constrained%20by%20the%20capabilities%20of%20the%20environment%0Ageneration.%20To%20this%20end%2C%20we%20propose%20a%20novel%20UED%20algorithm%2C%20adversarial%0Aenvironment%20design%20via%20regret-guided%20diffusion%20models%20%28ADD%29.%20The%20proposed%0Amethod%20guides%20the%20diffusion-based%20environment%20generator%20with%20the%20regret%20of%20the%0Aagent%20to%20produce%20environments%20that%20the%20agent%20finds%20challenging%20but%20conducive%20to%0Afurther%20improvement.%20By%20exploiting%20the%20representation%20power%20of%20diffusion%0Amodels%2C%20ADD%20can%20directly%20generate%20adversarial%20environments%20while%20maintaining%0Athe%20diversity%20of%20training%20environments%2C%20enabling%20the%20agent%20to%20effectively%20learn%0Aa%20robust%20policy.%20Our%20experimental%20results%20demonstrate%20that%20the%20proposed%20method%0Asuccessfully%20generates%20an%20instructive%20curriculum%20of%20environments%2C%20outperforming%0AUED%20baselines%20in%20zero-shot%20generalization%20across%20novel%2C%20out-of-distribution%0Aenvironments.%20Project%20page%3A%20https%3A//github.com/rllab-snu.github.io/projects/ADD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Environment%2520Design%2520via%2520Regret-Guided%2520Diffusion%2520Models%26entry.906535625%3DHojun%2520Chung%2520and%2520Junseo%2520Lee%2520and%2520Minsoo%2520Kim%2520and%2520Dohyeong%2520Kim%2520and%2520Songhwai%2520Oh%26entry.1292438233%3D%2520%2520Training%2520agents%2520that%2520are%2520robust%2520to%2520environmental%2520changes%2520remains%2520a%250Asignificant%2520challenge%2520in%2520deep%2520reinforcement%2520learning%2520%2528RL%2529.%2520Unsupervised%250Aenvironment%2520design%2520%2528UED%2529%2520has%2520recently%2520emerged%2520to%2520address%2520this%2520issue%2520by%250Agenerating%2520a%2520set%2520of%2520training%2520environments%2520tailored%2520to%2520the%2520agent%2527s%2520capabilities.%250AWhile%2520prior%2520works%2520demonstrate%2520that%2520UED%2520has%2520the%2520potential%2520to%2520learn%2520a%2520robust%250Apolicy%252C%2520their%2520performance%2520is%2520constrained%2520by%2520the%2520capabilities%2520of%2520the%2520environment%250Ageneration.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520UED%2520algorithm%252C%2520adversarial%250Aenvironment%2520design%2520via%2520regret-guided%2520diffusion%2520models%2520%2528ADD%2529.%2520The%2520proposed%250Amethod%2520guides%2520the%2520diffusion-based%2520environment%2520generator%2520with%2520the%2520regret%2520of%2520the%250Aagent%2520to%2520produce%2520environments%2520that%2520the%2520agent%2520finds%2520challenging%2520but%2520conducive%2520to%250Afurther%2520improvement.%2520By%2520exploiting%2520the%2520representation%2520power%2520of%2520diffusion%250Amodels%252C%2520ADD%2520can%2520directly%2520generate%2520adversarial%2520environments%2520while%2520maintaining%250Athe%2520diversity%2520of%2520training%2520environments%252C%2520enabling%2520the%2520agent%2520to%2520effectively%2520learn%250Aa%2520robust%2520policy.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%250Asuccessfully%2520generates%2520an%2520instructive%2520curriculum%2520of%2520environments%252C%2520outperforming%250AUED%2520baselines%2520in%2520zero-shot%2520generalization%2520across%2520novel%252C%2520out-of-distribution%250Aenvironments.%2520Project%2520page%253A%2520https%253A//github.com/rllab-snu.github.io/projects/ADD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Environment%20Design%20via%20Regret-Guided%20Diffusion%20Models&entry.906535625=Hojun%20Chung%20and%20Junseo%20Lee%20and%20Minsoo%20Kim%20and%20Dohyeong%20Kim%20and%20Songhwai%20Oh&entry.1292438233=%20%20Training%20agents%20that%20are%20robust%20to%20environmental%20changes%20remains%20a%0Asignificant%20challenge%20in%20deep%20reinforcement%20learning%20%28RL%29.%20Unsupervised%0Aenvironment%20design%20%28UED%29%20has%20recently%20emerged%20to%20address%20this%20issue%20by%0Agenerating%20a%20set%20of%20training%20environments%20tailored%20to%20the%20agent%27s%20capabilities.%0AWhile%20prior%20works%20demonstrate%20that%20UED%20has%20the%20potential%20to%20learn%20a%20robust%0Apolicy%2C%20their%20performance%20is%20constrained%20by%20the%20capabilities%20of%20the%20environment%0Ageneration.%20To%20this%20end%2C%20we%20propose%20a%20novel%20UED%20algorithm%2C%20adversarial%0Aenvironment%20design%20via%20regret-guided%20diffusion%20models%20%28ADD%29.%20The%20proposed%0Amethod%20guides%20the%20diffusion-based%20environment%20generator%20with%20the%20regret%20of%20the%0Aagent%20to%20produce%20environments%20that%20the%20agent%20finds%20challenging%20but%20conducive%20to%0Afurther%20improvement.%20By%20exploiting%20the%20representation%20power%20of%20diffusion%0Amodels%2C%20ADD%20can%20directly%20generate%20adversarial%20environments%20while%20maintaining%0Athe%20diversity%20of%20training%20environments%2C%20enabling%20the%20agent%20to%20effectively%20learn%0Aa%20robust%20policy.%20Our%20experimental%20results%20demonstrate%20that%20the%20proposed%20method%0Asuccessfully%20generates%20an%20instructive%20curriculum%20of%20environments%2C%20outperforming%0AUED%20baselines%20in%20zero-shot%20generalization%20across%20novel%2C%20out-of-distribution%0Aenvironments.%20Project%20page%3A%20https%3A//github.com/rllab-snu.github.io/projects/ADD%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19715v1&entry.124074799=Read"},
{"title": "A Robust and Efficient Visual-Inertial Initialization with Probabilistic\n  Normal Epipolar Constraint", "author": "Changshi Mu and Daquan Feng and Qi Zheng and Yuan Zhuang", "abstract": "  Accurate and robust initialization is essential for Visual-Inertial Odometry\n(VIO), as poor initialization can severely degrade pose accuracy. During\ninitialization, it is crucial to estimate parameters such as accelerometer\nbias, gyroscope bias, initial velocity, and gravity, etc. The IMU sensor\nrequires precise estimation of gyroscope bias because gyroscope bias affects\nrotation, velocity and position. Most existing VIO initialization methods adopt\nStructure from Motion (SfM) to solve for gyroscope bias. However, SfM is not\nstable and efficient enough in fast motion or degenerate scenes. To overcome\nthese limitations, we extended the rotation-translation-decoupling framework by\nadding new uncertainty parameters and optimization modules. First, we adopt a\ngyroscope bias optimizer that incorporates probabilistic normal epipolar\nconstraints. Second, we fuse IMU and visual measurements to solve for velocity,\ngravity, and scale efficiently. Finally, we design an additional refinement\nmodule that effectively diminishes gravity and scale errors. Extensive\ninitialization tests on the EuRoC dataset show that our method reduces the\ngyroscope bias and rotation estimation error by an average of 16% and 4%\nrespectively. It also significantly reduces the gravity error, with an average\nreduction of 29%.\n", "link": "http://arxiv.org/abs/2410.19473v1", "date": "2024-10-25", "relevancy": 2.2315, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5747}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5638}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Robust%20and%20Efficient%20Visual-Inertial%20Initialization%20with%20Probabilistic%0A%20%20Normal%20Epipolar%20Constraint&body=Title%3A%20A%20Robust%20and%20Efficient%20Visual-Inertial%20Initialization%20with%20Probabilistic%0A%20%20Normal%20Epipolar%20Constraint%0AAuthor%3A%20Changshi%20Mu%20and%20Daquan%20Feng%20and%20Qi%20Zheng%20and%20Yuan%20Zhuang%0AAbstract%3A%20%20%20Accurate%20and%20robust%20initialization%20is%20essential%20for%20Visual-Inertial%20Odometry%0A%28VIO%29%2C%20as%20poor%20initialization%20can%20severely%20degrade%20pose%20accuracy.%20During%0Ainitialization%2C%20it%20is%20crucial%20to%20estimate%20parameters%20such%20as%20accelerometer%0Abias%2C%20gyroscope%20bias%2C%20initial%20velocity%2C%20and%20gravity%2C%20etc.%20The%20IMU%20sensor%0Arequires%20precise%20estimation%20of%20gyroscope%20bias%20because%20gyroscope%20bias%20affects%0Arotation%2C%20velocity%20and%20position.%20Most%20existing%20VIO%20initialization%20methods%20adopt%0AStructure%20from%20Motion%20%28SfM%29%20to%20solve%20for%20gyroscope%20bias.%20However%2C%20SfM%20is%20not%0Astable%20and%20efficient%20enough%20in%20fast%20motion%20or%20degenerate%20scenes.%20To%20overcome%0Athese%20limitations%2C%20we%20extended%20the%20rotation-translation-decoupling%20framework%20by%0Aadding%20new%20uncertainty%20parameters%20and%20optimization%20modules.%20First%2C%20we%20adopt%20a%0Agyroscope%20bias%20optimizer%20that%20incorporates%20probabilistic%20normal%20epipolar%0Aconstraints.%20Second%2C%20we%20fuse%20IMU%20and%20visual%20measurements%20to%20solve%20for%20velocity%2C%0Agravity%2C%20and%20scale%20efficiently.%20Finally%2C%20we%20design%20an%20additional%20refinement%0Amodule%20that%20effectively%20diminishes%20gravity%20and%20scale%20errors.%20Extensive%0Ainitialization%20tests%20on%20the%20EuRoC%20dataset%20show%20that%20our%20method%20reduces%20the%0Agyroscope%20bias%20and%20rotation%20estimation%20error%20by%20an%20average%20of%2016%25%20and%204%25%0Arespectively.%20It%20also%20significantly%20reduces%20the%20gravity%20error%2C%20with%20an%20average%0Areduction%20of%2029%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Robust%2520and%2520Efficient%2520Visual-Inertial%2520Initialization%2520with%2520Probabilistic%250A%2520%2520Normal%2520Epipolar%2520Constraint%26entry.906535625%3DChangshi%2520Mu%2520and%2520Daquan%2520Feng%2520and%2520Qi%2520Zheng%2520and%2520Yuan%2520Zhuang%26entry.1292438233%3D%2520%2520Accurate%2520and%2520robust%2520initialization%2520is%2520essential%2520for%2520Visual-Inertial%2520Odometry%250A%2528VIO%2529%252C%2520as%2520poor%2520initialization%2520can%2520severely%2520degrade%2520pose%2520accuracy.%2520During%250Ainitialization%252C%2520it%2520is%2520crucial%2520to%2520estimate%2520parameters%2520such%2520as%2520accelerometer%250Abias%252C%2520gyroscope%2520bias%252C%2520initial%2520velocity%252C%2520and%2520gravity%252C%2520etc.%2520The%2520IMU%2520sensor%250Arequires%2520precise%2520estimation%2520of%2520gyroscope%2520bias%2520because%2520gyroscope%2520bias%2520affects%250Arotation%252C%2520velocity%2520and%2520position.%2520Most%2520existing%2520VIO%2520initialization%2520methods%2520adopt%250AStructure%2520from%2520Motion%2520%2528SfM%2529%2520to%2520solve%2520for%2520gyroscope%2520bias.%2520However%252C%2520SfM%2520is%2520not%250Astable%2520and%2520efficient%2520enough%2520in%2520fast%2520motion%2520or%2520degenerate%2520scenes.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520extended%2520the%2520rotation-translation-decoupling%2520framework%2520by%250Aadding%2520new%2520uncertainty%2520parameters%2520and%2520optimization%2520modules.%2520First%252C%2520we%2520adopt%2520a%250Agyroscope%2520bias%2520optimizer%2520that%2520incorporates%2520probabilistic%2520normal%2520epipolar%250Aconstraints.%2520Second%252C%2520we%2520fuse%2520IMU%2520and%2520visual%2520measurements%2520to%2520solve%2520for%2520velocity%252C%250Agravity%252C%2520and%2520scale%2520efficiently.%2520Finally%252C%2520we%2520design%2520an%2520additional%2520refinement%250Amodule%2520that%2520effectively%2520diminishes%2520gravity%2520and%2520scale%2520errors.%2520Extensive%250Ainitialization%2520tests%2520on%2520the%2520EuRoC%2520dataset%2520show%2520that%2520our%2520method%2520reduces%2520the%250Agyroscope%2520bias%2520and%2520rotation%2520estimation%2520error%2520by%2520an%2520average%2520of%252016%2525%2520and%25204%2525%250Arespectively.%2520It%2520also%2520significantly%2520reduces%2520the%2520gravity%2520error%252C%2520with%2520an%2520average%250Areduction%2520of%252029%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Robust%20and%20Efficient%20Visual-Inertial%20Initialization%20with%20Probabilistic%0A%20%20Normal%20Epipolar%20Constraint&entry.906535625=Changshi%20Mu%20and%20Daquan%20Feng%20and%20Qi%20Zheng%20and%20Yuan%20Zhuang&entry.1292438233=%20%20Accurate%20and%20robust%20initialization%20is%20essential%20for%20Visual-Inertial%20Odometry%0A%28VIO%29%2C%20as%20poor%20initialization%20can%20severely%20degrade%20pose%20accuracy.%20During%0Ainitialization%2C%20it%20is%20crucial%20to%20estimate%20parameters%20such%20as%20accelerometer%0Abias%2C%20gyroscope%20bias%2C%20initial%20velocity%2C%20and%20gravity%2C%20etc.%20The%20IMU%20sensor%0Arequires%20precise%20estimation%20of%20gyroscope%20bias%20because%20gyroscope%20bias%20affects%0Arotation%2C%20velocity%20and%20position.%20Most%20existing%20VIO%20initialization%20methods%20adopt%0AStructure%20from%20Motion%20%28SfM%29%20to%20solve%20for%20gyroscope%20bias.%20However%2C%20SfM%20is%20not%0Astable%20and%20efficient%20enough%20in%20fast%20motion%20or%20degenerate%20scenes.%20To%20overcome%0Athese%20limitations%2C%20we%20extended%20the%20rotation-translation-decoupling%20framework%20by%0Aadding%20new%20uncertainty%20parameters%20and%20optimization%20modules.%20First%2C%20we%20adopt%20a%0Agyroscope%20bias%20optimizer%20that%20incorporates%20probabilistic%20normal%20epipolar%0Aconstraints.%20Second%2C%20we%20fuse%20IMU%20and%20visual%20measurements%20to%20solve%20for%20velocity%2C%0Agravity%2C%20and%20scale%20efficiently.%20Finally%2C%20we%20design%20an%20additional%20refinement%0Amodule%20that%20effectively%20diminishes%20gravity%20and%20scale%20errors.%20Extensive%0Ainitialization%20tests%20on%20the%20EuRoC%20dataset%20show%20that%20our%20method%20reduces%20the%0Agyroscope%20bias%20and%20rotation%20estimation%20error%20by%20an%20average%20of%2016%25%20and%204%25%0Arespectively.%20It%20also%20significantly%20reduces%20the%20gravity%20error%2C%20with%20an%20average%0Areduction%20of%2029%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19473v1&entry.124074799=Read"},
{"title": "Perception, Control and Hardware for In-Hand Slip-Aware Object\n  Manipulation with Parallel Grippers", "author": "Gabriel Arslan Waltersson and Yiannis Karayiannidis", "abstract": "  Dexterous in-hand manipulation offers significant potential to enhance\nrobotic manipulator capabilities. This paper presents a comprehensive study on\ncustom sensors and parallel gripper hardware specifically designed for in-hand\nslippage control. The gripper features rapid closed-loop, low-level force\ncontrol, and is equipped with sensors capable of independently measuring\ncontact forces and sliding velocities. Our system can quickly estimate\nessential object properties during pick-up using only in-hand sensing, without\nrelying on prior object information. We introduce four distinct slippage\ncontrollers: gravity-assisted trajectory following for both rotational and\nlinear slippage, a hinge controller that maintains the object's orientation\nwhile the gripper rotates, and a slip-avoidance controller. The system is\nmounted on a robot arm and validated through extensive experiments involving a\ndiverse range of objects, demonstrating its novel capabilities.\n", "link": "http://arxiv.org/abs/2410.19660v1", "date": "2024-10-25", "relevancy": 2.2193, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5709}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5612}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%2C%20Control%20and%20Hardware%20for%20In-Hand%20Slip-Aware%20Object%0A%20%20Manipulation%20with%20Parallel%20Grippers&body=Title%3A%20Perception%2C%20Control%20and%20Hardware%20for%20In-Hand%20Slip-Aware%20Object%0A%20%20Manipulation%20with%20Parallel%20Grippers%0AAuthor%3A%20Gabriel%20Arslan%20Waltersson%20and%20Yiannis%20Karayiannidis%0AAbstract%3A%20%20%20Dexterous%20in-hand%20manipulation%20offers%20significant%20potential%20to%20enhance%0Arobotic%20manipulator%20capabilities.%20This%20paper%20presents%20a%20comprehensive%20study%20on%0Acustom%20sensors%20and%20parallel%20gripper%20hardware%20specifically%20designed%20for%20in-hand%0Aslippage%20control.%20The%20gripper%20features%20rapid%20closed-loop%2C%20low-level%20force%0Acontrol%2C%20and%20is%20equipped%20with%20sensors%20capable%20of%20independently%20measuring%0Acontact%20forces%20and%20sliding%20velocities.%20Our%20system%20can%20quickly%20estimate%0Aessential%20object%20properties%20during%20pick-up%20using%20only%20in-hand%20sensing%2C%20without%0Arelying%20on%20prior%20object%20information.%20We%20introduce%20four%20distinct%20slippage%0Acontrollers%3A%20gravity-assisted%20trajectory%20following%20for%20both%20rotational%20and%0Alinear%20slippage%2C%20a%20hinge%20controller%20that%20maintains%20the%20object%27s%20orientation%0Awhile%20the%20gripper%20rotates%2C%20and%20a%20slip-avoidance%20controller.%20The%20system%20is%0Amounted%20on%20a%20robot%20arm%20and%20validated%20through%20extensive%20experiments%20involving%20a%0Adiverse%20range%20of%20objects%2C%20demonstrating%20its%20novel%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%252C%2520Control%2520and%2520Hardware%2520for%2520In-Hand%2520Slip-Aware%2520Object%250A%2520%2520Manipulation%2520with%2520Parallel%2520Grippers%26entry.906535625%3DGabriel%2520Arslan%2520Waltersson%2520and%2520Yiannis%2520Karayiannidis%26entry.1292438233%3D%2520%2520Dexterous%2520in-hand%2520manipulation%2520offers%2520significant%2520potential%2520to%2520enhance%250Arobotic%2520manipulator%2520capabilities.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520study%2520on%250Acustom%2520sensors%2520and%2520parallel%2520gripper%2520hardware%2520specifically%2520designed%2520for%2520in-hand%250Aslippage%2520control.%2520The%2520gripper%2520features%2520rapid%2520closed-loop%252C%2520low-level%2520force%250Acontrol%252C%2520and%2520is%2520equipped%2520with%2520sensors%2520capable%2520of%2520independently%2520measuring%250Acontact%2520forces%2520and%2520sliding%2520velocities.%2520Our%2520system%2520can%2520quickly%2520estimate%250Aessential%2520object%2520properties%2520during%2520pick-up%2520using%2520only%2520in-hand%2520sensing%252C%2520without%250Arelying%2520on%2520prior%2520object%2520information.%2520We%2520introduce%2520four%2520distinct%2520slippage%250Acontrollers%253A%2520gravity-assisted%2520trajectory%2520following%2520for%2520both%2520rotational%2520and%250Alinear%2520slippage%252C%2520a%2520hinge%2520controller%2520that%2520maintains%2520the%2520object%2527s%2520orientation%250Awhile%2520the%2520gripper%2520rotates%252C%2520and%2520a%2520slip-avoidance%2520controller.%2520The%2520system%2520is%250Amounted%2520on%2520a%2520robot%2520arm%2520and%2520validated%2520through%2520extensive%2520experiments%2520involving%2520a%250Adiverse%2520range%2520of%2520objects%252C%2520demonstrating%2520its%2520novel%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%2C%20Control%20and%20Hardware%20for%20In-Hand%20Slip-Aware%20Object%0A%20%20Manipulation%20with%20Parallel%20Grippers&entry.906535625=Gabriel%20Arslan%20Waltersson%20and%20Yiannis%20Karayiannidis&entry.1292438233=%20%20Dexterous%20in-hand%20manipulation%20offers%20significant%20potential%20to%20enhance%0Arobotic%20manipulator%20capabilities.%20This%20paper%20presents%20a%20comprehensive%20study%20on%0Acustom%20sensors%20and%20parallel%20gripper%20hardware%20specifically%20designed%20for%20in-hand%0Aslippage%20control.%20The%20gripper%20features%20rapid%20closed-loop%2C%20low-level%20force%0Acontrol%2C%20and%20is%20equipped%20with%20sensors%20capable%20of%20independently%20measuring%0Acontact%20forces%20and%20sliding%20velocities.%20Our%20system%20can%20quickly%20estimate%0Aessential%20object%20properties%20during%20pick-up%20using%20only%20in-hand%20sensing%2C%20without%0Arelying%20on%20prior%20object%20information.%20We%20introduce%20four%20distinct%20slippage%0Acontrollers%3A%20gravity-assisted%20trajectory%20following%20for%20both%20rotational%20and%0Alinear%20slippage%2C%20a%20hinge%20controller%20that%20maintains%20the%20object%27s%20orientation%0Awhile%20the%20gripper%20rotates%2C%20and%20a%20slip-avoidance%20controller.%20The%20system%20is%0Amounted%20on%20a%20robot%20arm%20and%20validated%20through%20extensive%20experiments%20involving%20a%0Adiverse%20range%20of%20objects%2C%20demonstrating%20its%20novel%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19660v1&entry.124074799=Read"},
{"title": "Evaluation of strategies for efficient rate-distortion NeRF streaming", "author": "Pedro Martin and Ant\u00f3nio Rodrigues and Jo\u00e3o Ascenso and Maria Paula Queluz", "abstract": "  Neural Radiance Fields (NeRF) have revolutionized the field of 3D visual\nrepresentation by enabling highly realistic and detailed scene reconstructions\nfrom a sparse set of images. NeRF uses a volumetric functional representation\nthat maps 3D points to their corresponding colors and opacities, allowing for\nphotorealistic view synthesis from arbitrary viewpoints. Despite its\nadvancements, the efficient streaming of NeRF content remains a significant\nchallenge due to the large amount of data involved. This paper investigates the\nrate-distortion performance of two NeRF streaming strategies: pixel-based and\nneural network (NN) parameter-based streaming. While in the former, images are\ncoded and then transmitted throughout the network, in the latter, the\nrespective NeRF model parameters are coded and transmitted instead. This work\nalso highlights the trade-offs in complexity and performance, demonstrating\nthat the NN parameter-based strategy generally offers superior efficiency,\nmaking it suitable for one-to-many streaming scenarios.\n", "link": "http://arxiv.org/abs/2410.19459v1", "date": "2024-10-25", "relevancy": 2.2025, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5653}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5437}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20strategies%20for%20efficient%20rate-distortion%20NeRF%20streaming&body=Title%3A%20Evaluation%20of%20strategies%20for%20efficient%20rate-distortion%20NeRF%20streaming%0AAuthor%3A%20Pedro%20Martin%20and%20Ant%C3%B3nio%20Rodrigues%20and%20Jo%C3%A3o%20Ascenso%20and%20Maria%20Paula%20Queluz%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20revolutionized%20the%20field%20of%203D%20visual%0Arepresentation%20by%20enabling%20highly%20realistic%20and%20detailed%20scene%20reconstructions%0Afrom%20a%20sparse%20set%20of%20images.%20NeRF%20uses%20a%20volumetric%20functional%20representation%0Athat%20maps%203D%20points%20to%20their%20corresponding%20colors%20and%20opacities%2C%20allowing%20for%0Aphotorealistic%20view%20synthesis%20from%20arbitrary%20viewpoints.%20Despite%20its%0Aadvancements%2C%20the%20efficient%20streaming%20of%20NeRF%20content%20remains%20a%20significant%0Achallenge%20due%20to%20the%20large%20amount%20of%20data%20involved.%20This%20paper%20investigates%20the%0Arate-distortion%20performance%20of%20two%20NeRF%20streaming%20strategies%3A%20pixel-based%20and%0Aneural%20network%20%28NN%29%20parameter-based%20streaming.%20While%20in%20the%20former%2C%20images%20are%0Acoded%20and%20then%20transmitted%20throughout%20the%20network%2C%20in%20the%20latter%2C%20the%0Arespective%20NeRF%20model%20parameters%20are%20coded%20and%20transmitted%20instead.%20This%20work%0Aalso%20highlights%20the%20trade-offs%20in%20complexity%20and%20performance%2C%20demonstrating%0Athat%20the%20NN%20parameter-based%20strategy%20generally%20offers%20superior%20efficiency%2C%0Amaking%20it%20suitable%20for%20one-to-many%20streaming%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520strategies%2520for%2520efficient%2520rate-distortion%2520NeRF%2520streaming%26entry.906535625%3DPedro%2520Martin%2520and%2520Ant%25C3%25B3nio%2520Rodrigues%2520and%2520Jo%25C3%25A3o%2520Ascenso%2520and%2520Maria%2520Paula%2520Queluz%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520revolutionized%2520the%2520field%2520of%25203D%2520visual%250Arepresentation%2520by%2520enabling%2520highly%2520realistic%2520and%2520detailed%2520scene%2520reconstructions%250Afrom%2520a%2520sparse%2520set%2520of%2520images.%2520NeRF%2520uses%2520a%2520volumetric%2520functional%2520representation%250Athat%2520maps%25203D%2520points%2520to%2520their%2520corresponding%2520colors%2520and%2520opacities%252C%2520allowing%2520for%250Aphotorealistic%2520view%2520synthesis%2520from%2520arbitrary%2520viewpoints.%2520Despite%2520its%250Aadvancements%252C%2520the%2520efficient%2520streaming%2520of%2520NeRF%2520content%2520remains%2520a%2520significant%250Achallenge%2520due%2520to%2520the%2520large%2520amount%2520of%2520data%2520involved.%2520This%2520paper%2520investigates%2520the%250Arate-distortion%2520performance%2520of%2520two%2520NeRF%2520streaming%2520strategies%253A%2520pixel-based%2520and%250Aneural%2520network%2520%2528NN%2529%2520parameter-based%2520streaming.%2520While%2520in%2520the%2520former%252C%2520images%2520are%250Acoded%2520and%2520then%2520transmitted%2520throughout%2520the%2520network%252C%2520in%2520the%2520latter%252C%2520the%250Arespective%2520NeRF%2520model%2520parameters%2520are%2520coded%2520and%2520transmitted%2520instead.%2520This%2520work%250Aalso%2520highlights%2520the%2520trade-offs%2520in%2520complexity%2520and%2520performance%252C%2520demonstrating%250Athat%2520the%2520NN%2520parameter-based%2520strategy%2520generally%2520offers%2520superior%2520efficiency%252C%250Amaking%2520it%2520suitable%2520for%2520one-to-many%2520streaming%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20strategies%20for%20efficient%20rate-distortion%20NeRF%20streaming&entry.906535625=Pedro%20Martin%20and%20Ant%C3%B3nio%20Rodrigues%20and%20Jo%C3%A3o%20Ascenso%20and%20Maria%20Paula%20Queluz&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20revolutionized%20the%20field%20of%203D%20visual%0Arepresentation%20by%20enabling%20highly%20realistic%20and%20detailed%20scene%20reconstructions%0Afrom%20a%20sparse%20set%20of%20images.%20NeRF%20uses%20a%20volumetric%20functional%20representation%0Athat%20maps%203D%20points%20to%20their%20corresponding%20colors%20and%20opacities%2C%20allowing%20for%0Aphotorealistic%20view%20synthesis%20from%20arbitrary%20viewpoints.%20Despite%20its%0Aadvancements%2C%20the%20efficient%20streaming%20of%20NeRF%20content%20remains%20a%20significant%0Achallenge%20due%20to%20the%20large%20amount%20of%20data%20involved.%20This%20paper%20investigates%20the%0Arate-distortion%20performance%20of%20two%20NeRF%20streaming%20strategies%3A%20pixel-based%20and%0Aneural%20network%20%28NN%29%20parameter-based%20streaming.%20While%20in%20the%20former%2C%20images%20are%0Acoded%20and%20then%20transmitted%20throughout%20the%20network%2C%20in%20the%20latter%2C%20the%0Arespective%20NeRF%20model%20parameters%20are%20coded%20and%20transmitted%20instead.%20This%20work%0Aalso%20highlights%20the%20trade-offs%20in%20complexity%20and%20performance%2C%20demonstrating%0Athat%20the%20NN%20parameter-based%20strategy%20generally%20offers%20superior%20efficiency%2C%0Amaking%20it%20suitable%20for%20one-to-many%20streaming%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19459v1&entry.124074799=Read"},
{"title": "Continuously Learning, Adapting, and Improving: A Dual-Process Approach\n  to Autonomous Driving", "author": "Jianbiao Mei and Yukai Ma and Xuemeng Yang and Licheng Wen and Xinyu Cai and Xin Li and Daocheng Fu and Bo Zhang and Pinlong Cai and Min Dou and Botian Shi and Liang He and Yong Liu and Yu Qiao", "abstract": "  Autonomous driving has advanced significantly due to sensors, machine\nlearning, and artificial intelligence improvements. However, prevailing methods\nstruggle with intricate scenarios and causal relationships, hindering\nadaptability and interpretability in varied environments. To address the above\nproblems, we introduce LeapAD, a novel paradigm for autonomous driving inspired\nby the human cognitive process. Specifically, LeapAD emulates human attention\nby selecting critical objects relevant to driving decisions, simplifying\nenvironmental interpretation, and mitigating decision-making complexities.\nAdditionally, LeapAD incorporates an innovative dual-process decision-making\nmodule, which consists of an Analytic Process (System-II) for thorough analysis\nand reasoning, along with a Heuristic Process (System-I) for swift and\nempirical processing. The Analytic Process leverages its logical reasoning to\naccumulate linguistic driving experience, which is then transferred to the\nHeuristic Process by supervised fine-tuning. Through reflection mechanisms and\na growing memory bank, LeapAD continuously improves itself from past mistakes\nin a closed-loop environment. Closed-loop testing in CARLA shows that LeapAD\noutperforms all methods relying solely on camera input, requiring 1-2 orders of\nmagnitude less labeled data. Experiments also demonstrate that as the memory\nbank expands, the Heuristic Process with only 1.8B parameters can inherit the\nknowledge from a GPT-4 powered Analytic Process and achieve continuous\nperformance improvement. Project page: https://pjlab-adg.github.io/LeapAD.\n", "link": "http://arxiv.org/abs/2405.15324v2", "date": "2024-10-25", "relevancy": 2.1915, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5701}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5517}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuously%20Learning%2C%20Adapting%2C%20and%20Improving%3A%20A%20Dual-Process%20Approach%0A%20%20to%20Autonomous%20Driving&body=Title%3A%20Continuously%20Learning%2C%20Adapting%2C%20and%20Improving%3A%20A%20Dual-Process%20Approach%0A%20%20to%20Autonomous%20Driving%0AAuthor%3A%20Jianbiao%20Mei%20and%20Yukai%20Ma%20and%20Xuemeng%20Yang%20and%20Licheng%20Wen%20and%20Xinyu%20Cai%20and%20Xin%20Li%20and%20Daocheng%20Fu%20and%20Bo%20Zhang%20and%20Pinlong%20Cai%20and%20Min%20Dou%20and%20Botian%20Shi%20and%20Liang%20He%20and%20Yong%20Liu%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Autonomous%20driving%20has%20advanced%20significantly%20due%20to%20sensors%2C%20machine%0Alearning%2C%20and%20artificial%20intelligence%20improvements.%20However%2C%20prevailing%20methods%0Astruggle%20with%20intricate%20scenarios%20and%20causal%20relationships%2C%20hindering%0Aadaptability%20and%20interpretability%20in%20varied%20environments.%20To%20address%20the%20above%0Aproblems%2C%20we%20introduce%20LeapAD%2C%20a%20novel%20paradigm%20for%20autonomous%20driving%20inspired%0Aby%20the%20human%20cognitive%20process.%20Specifically%2C%20LeapAD%20emulates%20human%20attention%0Aby%20selecting%20critical%20objects%20relevant%20to%20driving%20decisions%2C%20simplifying%0Aenvironmental%20interpretation%2C%20and%20mitigating%20decision-making%20complexities.%0AAdditionally%2C%20LeapAD%20incorporates%20an%20innovative%20dual-process%20decision-making%0Amodule%2C%20which%20consists%20of%20an%20Analytic%20Process%20%28System-II%29%20for%20thorough%20analysis%0Aand%20reasoning%2C%20along%20with%20a%20Heuristic%20Process%20%28System-I%29%20for%20swift%20and%0Aempirical%20processing.%20The%20Analytic%20Process%20leverages%20its%20logical%20reasoning%20to%0Aaccumulate%20linguistic%20driving%20experience%2C%20which%20is%20then%20transferred%20to%20the%0AHeuristic%20Process%20by%20supervised%20fine-tuning.%20Through%20reflection%20mechanisms%20and%0Aa%20growing%20memory%20bank%2C%20LeapAD%20continuously%20improves%20itself%20from%20past%20mistakes%0Ain%20a%20closed-loop%20environment.%20Closed-loop%20testing%20in%20CARLA%20shows%20that%20LeapAD%0Aoutperforms%20all%20methods%20relying%20solely%20on%20camera%20input%2C%20requiring%201-2%20orders%20of%0Amagnitude%20less%20labeled%20data.%20Experiments%20also%20demonstrate%20that%20as%20the%20memory%0Abank%20expands%2C%20the%20Heuristic%20Process%20with%20only%201.8B%20parameters%20can%20inherit%20the%0Aknowledge%20from%20a%20GPT-4%20powered%20Analytic%20Process%20and%20achieve%20continuous%0Aperformance%20improvement.%20Project%20page%3A%20https%3A//pjlab-adg.github.io/LeapAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuously%2520Learning%252C%2520Adapting%252C%2520and%2520Improving%253A%2520A%2520Dual-Process%2520Approach%250A%2520%2520to%2520Autonomous%2520Driving%26entry.906535625%3DJianbiao%2520Mei%2520and%2520Yukai%2520Ma%2520and%2520Xuemeng%2520Yang%2520and%2520Licheng%2520Wen%2520and%2520Xinyu%2520Cai%2520and%2520Xin%2520Li%2520and%2520Daocheng%2520Fu%2520and%2520Bo%2520Zhang%2520and%2520Pinlong%2520Cai%2520and%2520Min%2520Dou%2520and%2520Botian%2520Shi%2520and%2520Liang%2520He%2520and%2520Yong%2520Liu%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520has%2520advanced%2520significantly%2520due%2520to%2520sensors%252C%2520machine%250Alearning%252C%2520and%2520artificial%2520intelligence%2520improvements.%2520However%252C%2520prevailing%2520methods%250Astruggle%2520with%2520intricate%2520scenarios%2520and%2520causal%2520relationships%252C%2520hindering%250Aadaptability%2520and%2520interpretability%2520in%2520varied%2520environments.%2520To%2520address%2520the%2520above%250Aproblems%252C%2520we%2520introduce%2520LeapAD%252C%2520a%2520novel%2520paradigm%2520for%2520autonomous%2520driving%2520inspired%250Aby%2520the%2520human%2520cognitive%2520process.%2520Specifically%252C%2520LeapAD%2520emulates%2520human%2520attention%250Aby%2520selecting%2520critical%2520objects%2520relevant%2520to%2520driving%2520decisions%252C%2520simplifying%250Aenvironmental%2520interpretation%252C%2520and%2520mitigating%2520decision-making%2520complexities.%250AAdditionally%252C%2520LeapAD%2520incorporates%2520an%2520innovative%2520dual-process%2520decision-making%250Amodule%252C%2520which%2520consists%2520of%2520an%2520Analytic%2520Process%2520%2528System-II%2529%2520for%2520thorough%2520analysis%250Aand%2520reasoning%252C%2520along%2520with%2520a%2520Heuristic%2520Process%2520%2528System-I%2529%2520for%2520swift%2520and%250Aempirical%2520processing.%2520The%2520Analytic%2520Process%2520leverages%2520its%2520logical%2520reasoning%2520to%250Aaccumulate%2520linguistic%2520driving%2520experience%252C%2520which%2520is%2520then%2520transferred%2520to%2520the%250AHeuristic%2520Process%2520by%2520supervised%2520fine-tuning.%2520Through%2520reflection%2520mechanisms%2520and%250Aa%2520growing%2520memory%2520bank%252C%2520LeapAD%2520continuously%2520improves%2520itself%2520from%2520past%2520mistakes%250Ain%2520a%2520closed-loop%2520environment.%2520Closed-loop%2520testing%2520in%2520CARLA%2520shows%2520that%2520LeapAD%250Aoutperforms%2520all%2520methods%2520relying%2520solely%2520on%2520camera%2520input%252C%2520requiring%25201-2%2520orders%2520of%250Amagnitude%2520less%2520labeled%2520data.%2520Experiments%2520also%2520demonstrate%2520that%2520as%2520the%2520memory%250Abank%2520expands%252C%2520the%2520Heuristic%2520Process%2520with%2520only%25201.8B%2520parameters%2520can%2520inherit%2520the%250Aknowledge%2520from%2520a%2520GPT-4%2520powered%2520Analytic%2520Process%2520and%2520achieve%2520continuous%250Aperformance%2520improvement.%2520Project%2520page%253A%2520https%253A//pjlab-adg.github.io/LeapAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuously%20Learning%2C%20Adapting%2C%20and%20Improving%3A%20A%20Dual-Process%20Approach%0A%20%20to%20Autonomous%20Driving&entry.906535625=Jianbiao%20Mei%20and%20Yukai%20Ma%20and%20Xuemeng%20Yang%20and%20Licheng%20Wen%20and%20Xinyu%20Cai%20and%20Xin%20Li%20and%20Daocheng%20Fu%20and%20Bo%20Zhang%20and%20Pinlong%20Cai%20and%20Min%20Dou%20and%20Botian%20Shi%20and%20Liang%20He%20and%20Yong%20Liu%20and%20Yu%20Qiao&entry.1292438233=%20%20Autonomous%20driving%20has%20advanced%20significantly%20due%20to%20sensors%2C%20machine%0Alearning%2C%20and%20artificial%20intelligence%20improvements.%20However%2C%20prevailing%20methods%0Astruggle%20with%20intricate%20scenarios%20and%20causal%20relationships%2C%20hindering%0Aadaptability%20and%20interpretability%20in%20varied%20environments.%20To%20address%20the%20above%0Aproblems%2C%20we%20introduce%20LeapAD%2C%20a%20novel%20paradigm%20for%20autonomous%20driving%20inspired%0Aby%20the%20human%20cognitive%20process.%20Specifically%2C%20LeapAD%20emulates%20human%20attention%0Aby%20selecting%20critical%20objects%20relevant%20to%20driving%20decisions%2C%20simplifying%0Aenvironmental%20interpretation%2C%20and%20mitigating%20decision-making%20complexities.%0AAdditionally%2C%20LeapAD%20incorporates%20an%20innovative%20dual-process%20decision-making%0Amodule%2C%20which%20consists%20of%20an%20Analytic%20Process%20%28System-II%29%20for%20thorough%20analysis%0Aand%20reasoning%2C%20along%20with%20a%20Heuristic%20Process%20%28System-I%29%20for%20swift%20and%0Aempirical%20processing.%20The%20Analytic%20Process%20leverages%20its%20logical%20reasoning%20to%0Aaccumulate%20linguistic%20driving%20experience%2C%20which%20is%20then%20transferred%20to%20the%0AHeuristic%20Process%20by%20supervised%20fine-tuning.%20Through%20reflection%20mechanisms%20and%0Aa%20growing%20memory%20bank%2C%20LeapAD%20continuously%20improves%20itself%20from%20past%20mistakes%0Ain%20a%20closed-loop%20environment.%20Closed-loop%20testing%20in%20CARLA%20shows%20that%20LeapAD%0Aoutperforms%20all%20methods%20relying%20solely%20on%20camera%20input%2C%20requiring%201-2%20orders%20of%0Amagnitude%20less%20labeled%20data.%20Experiments%20also%20demonstrate%20that%20as%20the%20memory%0Abank%20expands%2C%20the%20Heuristic%20Process%20with%20only%201.8B%20parameters%20can%20inherit%20the%0Aknowledge%20from%20a%20GPT-4%20powered%20Analytic%20Process%20and%20achieve%20continuous%0Aperformance%20improvement.%20Project%20page%3A%20https%3A//pjlab-adg.github.io/LeapAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15324v2&entry.124074799=Read"},
{"title": "Multi-view biomedical foundation models for molecule-target and property\n  prediction", "author": "Parthasarathy Suryanarayanan and Yunguang Qiu and Shreyans Sethi and Diwakar Mahajan and Hongyang Li and Yuxin Yang and Elif Eyigoz and Aldo Guzman Saenz and Daniel E. Platt and Timothy H. Rumbell and Kenney Ng and Sanjoy Dey and Myson Burch and Bum Chul Kwon and Pablo Meyer and Feixiong Cheng and Jianying Hu and Joseph A. Morrone", "abstract": "  Foundation models applied to bio-molecular space hold promise to accelerate\ndrug discovery. Molecular representation is key to building such models.\nPrevious works have typically focused on a single representation or view of the\nmolecules. Here, we develop a multi-view foundation model approach, that\nintegrates molecular views of graph, image and text. Single-view foundation\nmodels are each pre-trained on a dataset of up to 200M molecules and then\naggregated into combined representations. Our multi-view model is validated on\na diverse set of 18 tasks, encompassing ligand-protein binding, molecular\nsolubility, metabolism and toxicity. We show that the multi-view models perform\nrobustly and are able to balance the strengths and weaknesses of specific\nviews. We then apply this model to screen compounds against a large (>100\ntargets) set of G Protein-Coupled receptors (GPCRs). From this library of\ntargets, we identify 33 that are related to Alzheimer's disease. On this\nsubset, we employ our model to identify strong binders, which are validated\nthrough structure-based modeling and identification of key binding motifs.\n", "link": "http://arxiv.org/abs/2410.19704v1", "date": "2024-10-25", "relevancy": 2.1873, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20biomedical%20foundation%20models%20for%20molecule-target%20and%20property%0A%20%20prediction&body=Title%3A%20Multi-view%20biomedical%20foundation%20models%20for%20molecule-target%20and%20property%0A%20%20prediction%0AAuthor%3A%20Parthasarathy%20Suryanarayanan%20and%20Yunguang%20Qiu%20and%20Shreyans%20Sethi%20and%20Diwakar%20Mahajan%20and%20Hongyang%20Li%20and%20Yuxin%20Yang%20and%20Elif%20Eyigoz%20and%20Aldo%20Guzman%20Saenz%20and%20Daniel%20E.%20Platt%20and%20Timothy%20H.%20Rumbell%20and%20Kenney%20Ng%20and%20Sanjoy%20Dey%20and%20Myson%20Burch%20and%20Bum%20Chul%20Kwon%20and%20Pablo%20Meyer%20and%20Feixiong%20Cheng%20and%20Jianying%20Hu%20and%20Joseph%20A.%20Morrone%0AAbstract%3A%20%20%20Foundation%20models%20applied%20to%20bio-molecular%20space%20hold%20promise%20to%20accelerate%0Adrug%20discovery.%20Molecular%20representation%20is%20key%20to%20building%20such%20models.%0APrevious%20works%20have%20typically%20focused%20on%20a%20single%20representation%20or%20view%20of%20the%0Amolecules.%20Here%2C%20we%20develop%20a%20multi-view%20foundation%20model%20approach%2C%20that%0Aintegrates%20molecular%20views%20of%20graph%2C%20image%20and%20text.%20Single-view%20foundation%0Amodels%20are%20each%20pre-trained%20on%20a%20dataset%20of%20up%20to%20200M%20molecules%20and%20then%0Aaggregated%20into%20combined%20representations.%20Our%20multi-view%20model%20is%20validated%20on%0Aa%20diverse%20set%20of%2018%20tasks%2C%20encompassing%20ligand-protein%20binding%2C%20molecular%0Asolubility%2C%20metabolism%20and%20toxicity.%20We%20show%20that%20the%20multi-view%20models%20perform%0Arobustly%20and%20are%20able%20to%20balance%20the%20strengths%20and%20weaknesses%20of%20specific%0Aviews.%20We%20then%20apply%20this%20model%20to%20screen%20compounds%20against%20a%20large%20%28%3E100%0Atargets%29%20set%20of%20G%20Protein-Coupled%20receptors%20%28GPCRs%29.%20From%20this%20library%20of%0Atargets%2C%20we%20identify%2033%20that%20are%20related%20to%20Alzheimer%27s%20disease.%20On%20this%0Asubset%2C%20we%20employ%20our%20model%20to%20identify%20strong%20binders%2C%20which%20are%20validated%0Athrough%20structure-based%20modeling%20and%20identification%20of%20key%20binding%20motifs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520biomedical%2520foundation%2520models%2520for%2520molecule-target%2520and%2520property%250A%2520%2520prediction%26entry.906535625%3DParthasarathy%2520Suryanarayanan%2520and%2520Yunguang%2520Qiu%2520and%2520Shreyans%2520Sethi%2520and%2520Diwakar%2520Mahajan%2520and%2520Hongyang%2520Li%2520and%2520Yuxin%2520Yang%2520and%2520Elif%2520Eyigoz%2520and%2520Aldo%2520Guzman%2520Saenz%2520and%2520Daniel%2520E.%2520Platt%2520and%2520Timothy%2520H.%2520Rumbell%2520and%2520Kenney%2520Ng%2520and%2520Sanjoy%2520Dey%2520and%2520Myson%2520Burch%2520and%2520Bum%2520Chul%2520Kwon%2520and%2520Pablo%2520Meyer%2520and%2520Feixiong%2520Cheng%2520and%2520Jianying%2520Hu%2520and%2520Joseph%2520A.%2520Morrone%26entry.1292438233%3D%2520%2520Foundation%2520models%2520applied%2520to%2520bio-molecular%2520space%2520hold%2520promise%2520to%2520accelerate%250Adrug%2520discovery.%2520Molecular%2520representation%2520is%2520key%2520to%2520building%2520such%2520models.%250APrevious%2520works%2520have%2520typically%2520focused%2520on%2520a%2520single%2520representation%2520or%2520view%2520of%2520the%250Amolecules.%2520Here%252C%2520we%2520develop%2520a%2520multi-view%2520foundation%2520model%2520approach%252C%2520that%250Aintegrates%2520molecular%2520views%2520of%2520graph%252C%2520image%2520and%2520text.%2520Single-view%2520foundation%250Amodels%2520are%2520each%2520pre-trained%2520on%2520a%2520dataset%2520of%2520up%2520to%2520200M%2520molecules%2520and%2520then%250Aaggregated%2520into%2520combined%2520representations.%2520Our%2520multi-view%2520model%2520is%2520validated%2520on%250Aa%2520diverse%2520set%2520of%252018%2520tasks%252C%2520encompassing%2520ligand-protein%2520binding%252C%2520molecular%250Asolubility%252C%2520metabolism%2520and%2520toxicity.%2520We%2520show%2520that%2520the%2520multi-view%2520models%2520perform%250Arobustly%2520and%2520are%2520able%2520to%2520balance%2520the%2520strengths%2520and%2520weaknesses%2520of%2520specific%250Aviews.%2520We%2520then%2520apply%2520this%2520model%2520to%2520screen%2520compounds%2520against%2520a%2520large%2520%2528%253E100%250Atargets%2529%2520set%2520of%2520G%2520Protein-Coupled%2520receptors%2520%2528GPCRs%2529.%2520From%2520this%2520library%2520of%250Atargets%252C%2520we%2520identify%252033%2520that%2520are%2520related%2520to%2520Alzheimer%2527s%2520disease.%2520On%2520this%250Asubset%252C%2520we%2520employ%2520our%2520model%2520to%2520identify%2520strong%2520binders%252C%2520which%2520are%2520validated%250Athrough%2520structure-based%2520modeling%2520and%2520identification%2520of%2520key%2520binding%2520motifs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20biomedical%20foundation%20models%20for%20molecule-target%20and%20property%0A%20%20prediction&entry.906535625=Parthasarathy%20Suryanarayanan%20and%20Yunguang%20Qiu%20and%20Shreyans%20Sethi%20and%20Diwakar%20Mahajan%20and%20Hongyang%20Li%20and%20Yuxin%20Yang%20and%20Elif%20Eyigoz%20and%20Aldo%20Guzman%20Saenz%20and%20Daniel%20E.%20Platt%20and%20Timothy%20H.%20Rumbell%20and%20Kenney%20Ng%20and%20Sanjoy%20Dey%20and%20Myson%20Burch%20and%20Bum%20Chul%20Kwon%20and%20Pablo%20Meyer%20and%20Feixiong%20Cheng%20and%20Jianying%20Hu%20and%20Joseph%20A.%20Morrone&entry.1292438233=%20%20Foundation%20models%20applied%20to%20bio-molecular%20space%20hold%20promise%20to%20accelerate%0Adrug%20discovery.%20Molecular%20representation%20is%20key%20to%20building%20such%20models.%0APrevious%20works%20have%20typically%20focused%20on%20a%20single%20representation%20or%20view%20of%20the%0Amolecules.%20Here%2C%20we%20develop%20a%20multi-view%20foundation%20model%20approach%2C%20that%0Aintegrates%20molecular%20views%20of%20graph%2C%20image%20and%20text.%20Single-view%20foundation%0Amodels%20are%20each%20pre-trained%20on%20a%20dataset%20of%20up%20to%20200M%20molecules%20and%20then%0Aaggregated%20into%20combined%20representations.%20Our%20multi-view%20model%20is%20validated%20on%0Aa%20diverse%20set%20of%2018%20tasks%2C%20encompassing%20ligand-protein%20binding%2C%20molecular%0Asolubility%2C%20metabolism%20and%20toxicity.%20We%20show%20that%20the%20multi-view%20models%20perform%0Arobustly%20and%20are%20able%20to%20balance%20the%20strengths%20and%20weaknesses%20of%20specific%0Aviews.%20We%20then%20apply%20this%20model%20to%20screen%20compounds%20against%20a%20large%20%28%3E100%0Atargets%29%20set%20of%20G%20Protein-Coupled%20receptors%20%28GPCRs%29.%20From%20this%20library%20of%0Atargets%2C%20we%20identify%2033%20that%20are%20related%20to%20Alzheimer%27s%20disease.%20On%20this%0Asubset%2C%20we%20employ%20our%20model%20to%20identify%20strong%20binders%2C%20which%20are%20validated%0Athrough%20structure-based%20modeling%20and%20identification%20of%20key%20binding%20motifs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19704v1&entry.124074799=Read"},
{"title": "Arabic Music Classification and Generation using Deep Learning", "author": "Mohamed Elshaarawy and Ashrakat Saeed and Mariam Sheta and Abdelrahman Said and Asem Bakr and Omar Bahaa and Walid Gomaa", "abstract": "  This paper proposes a machine learning approach for classifying classical and\nnew Egyptian music by composer and generating new similar music. The proposed\nsystem utilizes a convolutional neural network (CNN) for classification and a\nCNN autoencoder for generation. The dataset used in this project consists of\nnew and classical Egyptian music pieces composed by different composers.\n  To classify the music by composer, each sample is normalized and transformed\ninto a mel spectrogram. The CNN model is trained on the dataset using the mel\nspectrograms as input features and the composer labels as output classes. The\nmodel achieves 81.4\\% accuracy in classifying the music by composer,\ndemonstrating the effectiveness of the proposed approach.\n  To generate new music similar to the original pieces, a CNN autoencoder is\ntrained on a similar dataset. The model is trained to encode the mel\nspectrograms of the original pieces into a lower-dimensional latent space and\nthen decode them back into the original mel spectrogram. The generated music is\nproduced by sampling from the latent space and decoding the samples back into\nmel spectrograms, which are then transformed into audio.\n  In conclusion, the proposed system provides a promising approach to\nclassifying and generating classical Egyptian music, which can be applied in\nvarious musical applications, such as music recommendation systems, music\nproduction, and music education.\n", "link": "http://arxiv.org/abs/2410.19719v1", "date": "2024-10-25", "relevancy": 2.1854, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4453}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4341}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arabic%20Music%20Classification%20and%20Generation%20using%20Deep%20Learning&body=Title%3A%20Arabic%20Music%20Classification%20and%20Generation%20using%20Deep%20Learning%0AAuthor%3A%20Mohamed%20Elshaarawy%20and%20Ashrakat%20Saeed%20and%20Mariam%20Sheta%20and%20Abdelrahman%20Said%20and%20Asem%20Bakr%20and%20Omar%20Bahaa%20and%20Walid%20Gomaa%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20machine%20learning%20approach%20for%20classifying%20classical%20and%0Anew%20Egyptian%20music%20by%20composer%20and%20generating%20new%20similar%20music.%20The%20proposed%0Asystem%20utilizes%20a%20convolutional%20neural%20network%20%28CNN%29%20for%20classification%20and%20a%0ACNN%20autoencoder%20for%20generation.%20The%20dataset%20used%20in%20this%20project%20consists%20of%0Anew%20and%20classical%20Egyptian%20music%20pieces%20composed%20by%20different%20composers.%0A%20%20To%20classify%20the%20music%20by%20composer%2C%20each%20sample%20is%20normalized%20and%20transformed%0Ainto%20a%20mel%20spectrogram.%20The%20CNN%20model%20is%20trained%20on%20the%20dataset%20using%20the%20mel%0Aspectrograms%20as%20input%20features%20and%20the%20composer%20labels%20as%20output%20classes.%20The%0Amodel%20achieves%2081.4%5C%25%20accuracy%20in%20classifying%20the%20music%20by%20composer%2C%0Ademonstrating%20the%20effectiveness%20of%20the%20proposed%20approach.%0A%20%20To%20generate%20new%20music%20similar%20to%20the%20original%20pieces%2C%20a%20CNN%20autoencoder%20is%0Atrained%20on%20a%20similar%20dataset.%20The%20model%20is%20trained%20to%20encode%20the%20mel%0Aspectrograms%20of%20the%20original%20pieces%20into%20a%20lower-dimensional%20latent%20space%20and%0Athen%20decode%20them%20back%20into%20the%20original%20mel%20spectrogram.%20The%20generated%20music%20is%0Aproduced%20by%20sampling%20from%20the%20latent%20space%20and%20decoding%20the%20samples%20back%20into%0Amel%20spectrograms%2C%20which%20are%20then%20transformed%20into%20audio.%0A%20%20In%20conclusion%2C%20the%20proposed%20system%20provides%20a%20promising%20approach%20to%0Aclassifying%20and%20generating%20classical%20Egyptian%20music%2C%20which%20can%20be%20applied%20in%0Avarious%20musical%20applications%2C%20such%20as%20music%20recommendation%20systems%2C%20music%0Aproduction%2C%20and%20music%20education.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArabic%2520Music%2520Classification%2520and%2520Generation%2520using%2520Deep%2520Learning%26entry.906535625%3DMohamed%2520Elshaarawy%2520and%2520Ashrakat%2520Saeed%2520and%2520Mariam%2520Sheta%2520and%2520Abdelrahman%2520Said%2520and%2520Asem%2520Bakr%2520and%2520Omar%2520Bahaa%2520and%2520Walid%2520Gomaa%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520machine%2520learning%2520approach%2520for%2520classifying%2520classical%2520and%250Anew%2520Egyptian%2520music%2520by%2520composer%2520and%2520generating%2520new%2520similar%2520music.%2520The%2520proposed%250Asystem%2520utilizes%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520for%2520classification%2520and%2520a%250ACNN%2520autoencoder%2520for%2520generation.%2520The%2520dataset%2520used%2520in%2520this%2520project%2520consists%2520of%250Anew%2520and%2520classical%2520Egyptian%2520music%2520pieces%2520composed%2520by%2520different%2520composers.%250A%2520%2520To%2520classify%2520the%2520music%2520by%2520composer%252C%2520each%2520sample%2520is%2520normalized%2520and%2520transformed%250Ainto%2520a%2520mel%2520spectrogram.%2520The%2520CNN%2520model%2520is%2520trained%2520on%2520the%2520dataset%2520using%2520the%2520mel%250Aspectrograms%2520as%2520input%2520features%2520and%2520the%2520composer%2520labels%2520as%2520output%2520classes.%2520The%250Amodel%2520achieves%252081.4%255C%2525%2520accuracy%2520in%2520classifying%2520the%2520music%2520by%2520composer%252C%250Ademonstrating%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach.%250A%2520%2520To%2520generate%2520new%2520music%2520similar%2520to%2520the%2520original%2520pieces%252C%2520a%2520CNN%2520autoencoder%2520is%250Atrained%2520on%2520a%2520similar%2520dataset.%2520The%2520model%2520is%2520trained%2520to%2520encode%2520the%2520mel%250Aspectrograms%2520of%2520the%2520original%2520pieces%2520into%2520a%2520lower-dimensional%2520latent%2520space%2520and%250Athen%2520decode%2520them%2520back%2520into%2520the%2520original%2520mel%2520spectrogram.%2520The%2520generated%2520music%2520is%250Aproduced%2520by%2520sampling%2520from%2520the%2520latent%2520space%2520and%2520decoding%2520the%2520samples%2520back%2520into%250Amel%2520spectrograms%252C%2520which%2520are%2520then%2520transformed%2520into%2520audio.%250A%2520%2520In%2520conclusion%252C%2520the%2520proposed%2520system%2520provides%2520a%2520promising%2520approach%2520to%250Aclassifying%2520and%2520generating%2520classical%2520Egyptian%2520music%252C%2520which%2520can%2520be%2520applied%2520in%250Avarious%2520musical%2520applications%252C%2520such%2520as%2520music%2520recommendation%2520systems%252C%2520music%250Aproduction%252C%2520and%2520music%2520education.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arabic%20Music%20Classification%20and%20Generation%20using%20Deep%20Learning&entry.906535625=Mohamed%20Elshaarawy%20and%20Ashrakat%20Saeed%20and%20Mariam%20Sheta%20and%20Abdelrahman%20Said%20and%20Asem%20Bakr%20and%20Omar%20Bahaa%20and%20Walid%20Gomaa&entry.1292438233=%20%20This%20paper%20proposes%20a%20machine%20learning%20approach%20for%20classifying%20classical%20and%0Anew%20Egyptian%20music%20by%20composer%20and%20generating%20new%20similar%20music.%20The%20proposed%0Asystem%20utilizes%20a%20convolutional%20neural%20network%20%28CNN%29%20for%20classification%20and%20a%0ACNN%20autoencoder%20for%20generation.%20The%20dataset%20used%20in%20this%20project%20consists%20of%0Anew%20and%20classical%20Egyptian%20music%20pieces%20composed%20by%20different%20composers.%0A%20%20To%20classify%20the%20music%20by%20composer%2C%20each%20sample%20is%20normalized%20and%20transformed%0Ainto%20a%20mel%20spectrogram.%20The%20CNN%20model%20is%20trained%20on%20the%20dataset%20using%20the%20mel%0Aspectrograms%20as%20input%20features%20and%20the%20composer%20labels%20as%20output%20classes.%20The%0Amodel%20achieves%2081.4%5C%25%20accuracy%20in%20classifying%20the%20music%20by%20composer%2C%0Ademonstrating%20the%20effectiveness%20of%20the%20proposed%20approach.%0A%20%20To%20generate%20new%20music%20similar%20to%20the%20original%20pieces%2C%20a%20CNN%20autoencoder%20is%0Atrained%20on%20a%20similar%20dataset.%20The%20model%20is%20trained%20to%20encode%20the%20mel%0Aspectrograms%20of%20the%20original%20pieces%20into%20a%20lower-dimensional%20latent%20space%20and%0Athen%20decode%20them%20back%20into%20the%20original%20mel%20spectrogram.%20The%20generated%20music%20is%0Aproduced%20by%20sampling%20from%20the%20latent%20space%20and%20decoding%20the%20samples%20back%20into%0Amel%20spectrograms%2C%20which%20are%20then%20transformed%20into%20audio.%0A%20%20In%20conclusion%2C%20the%20proposed%20system%20provides%20a%20promising%20approach%20to%0Aclassifying%20and%20generating%20classical%20Egyptian%20music%2C%20which%20can%20be%20applied%20in%0Avarious%20musical%20applications%2C%20such%20as%20music%20recommendation%20systems%2C%20music%0Aproduction%2C%20and%20music%20education.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19719v1&entry.124074799=Read"},
{"title": "Mask-Weighted Spatial Likelihood Coding for Speaker-Independent Joint\n  Localization and Mask Estimation", "author": "Jakob Kienegger and Alina Mannanova and Timo Gerkmann", "abstract": "  Due to their robustness and flexibility, neural-driven beamformers are a\npopular choice for speech separation in challenging environments with a varying\namount of simultaneous speakers alongside noise and reverberation.\nTime-frequency masks and relative directions of the speakers regarding a fixed\nspatial grid can be used to estimate the beamformer's parameters. To some\ndegree, speaker-independence is achieved by ensuring a greater amount of\nspatial partitions than speech sources. In this work, we analyze how to encode\nboth mask and positioning into such a grid to enable joint estimation of both\nquantities. We propose mask-weighted spatial likelihood coding and show that it\nachieves considerable performance in both tasks compared to baseline encodings\noptimized for either localization or mask estimation. In the same setup, we\ndemonstrate superiority for joint estimation of both quantities. Conclusively,\nwe propose a universal approach which can replace an upstream sound source\nlocalization system solely by adapting the training framework, making it highly\nrelevant in performance-critical scenarios.\n", "link": "http://arxiv.org/abs/2410.19595v1", "date": "2024-10-25", "relevancy": 2.1807, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5485}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask-Weighted%20Spatial%20Likelihood%20Coding%20for%20Speaker-Independent%20Joint%0A%20%20Localization%20and%20Mask%20Estimation&body=Title%3A%20Mask-Weighted%20Spatial%20Likelihood%20Coding%20for%20Speaker-Independent%20Joint%0A%20%20Localization%20and%20Mask%20Estimation%0AAuthor%3A%20Jakob%20Kienegger%20and%20Alina%20Mannanova%20and%20Timo%20Gerkmann%0AAbstract%3A%20%20%20Due%20to%20their%20robustness%20and%20flexibility%2C%20neural-driven%20beamformers%20are%20a%0Apopular%20choice%20for%20speech%20separation%20in%20challenging%20environments%20with%20a%20varying%0Aamount%20of%20simultaneous%20speakers%20alongside%20noise%20and%20reverberation.%0ATime-frequency%20masks%20and%20relative%20directions%20of%20the%20speakers%20regarding%20a%20fixed%0Aspatial%20grid%20can%20be%20used%20to%20estimate%20the%20beamformer%27s%20parameters.%20To%20some%0Adegree%2C%20speaker-independence%20is%20achieved%20by%20ensuring%20a%20greater%20amount%20of%0Aspatial%20partitions%20than%20speech%20sources.%20In%20this%20work%2C%20we%20analyze%20how%20to%20encode%0Aboth%20mask%20and%20positioning%20into%20such%20a%20grid%20to%20enable%20joint%20estimation%20of%20both%0Aquantities.%20We%20propose%20mask-weighted%20spatial%20likelihood%20coding%20and%20show%20that%20it%0Aachieves%20considerable%20performance%20in%20both%20tasks%20compared%20to%20baseline%20encodings%0Aoptimized%20for%20either%20localization%20or%20mask%20estimation.%20In%20the%20same%20setup%2C%20we%0Ademonstrate%20superiority%20for%20joint%20estimation%20of%20both%20quantities.%20Conclusively%2C%0Awe%20propose%20a%20universal%20approach%20which%20can%20replace%20an%20upstream%20sound%20source%0Alocalization%20system%20solely%20by%20adapting%20the%20training%20framework%2C%20making%20it%20highly%0Arelevant%20in%20performance-critical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask-Weighted%2520Spatial%2520Likelihood%2520Coding%2520for%2520Speaker-Independent%2520Joint%250A%2520%2520Localization%2520and%2520Mask%2520Estimation%26entry.906535625%3DJakob%2520Kienegger%2520and%2520Alina%2520Mannanova%2520and%2520Timo%2520Gerkmann%26entry.1292438233%3D%2520%2520Due%2520to%2520their%2520robustness%2520and%2520flexibility%252C%2520neural-driven%2520beamformers%2520are%2520a%250Apopular%2520choice%2520for%2520speech%2520separation%2520in%2520challenging%2520environments%2520with%2520a%2520varying%250Aamount%2520of%2520simultaneous%2520speakers%2520alongside%2520noise%2520and%2520reverberation.%250ATime-frequency%2520masks%2520and%2520relative%2520directions%2520of%2520the%2520speakers%2520regarding%2520a%2520fixed%250Aspatial%2520grid%2520can%2520be%2520used%2520to%2520estimate%2520the%2520beamformer%2527s%2520parameters.%2520To%2520some%250Adegree%252C%2520speaker-independence%2520is%2520achieved%2520by%2520ensuring%2520a%2520greater%2520amount%2520of%250Aspatial%2520partitions%2520than%2520speech%2520sources.%2520In%2520this%2520work%252C%2520we%2520analyze%2520how%2520to%2520encode%250Aboth%2520mask%2520and%2520positioning%2520into%2520such%2520a%2520grid%2520to%2520enable%2520joint%2520estimation%2520of%2520both%250Aquantities.%2520We%2520propose%2520mask-weighted%2520spatial%2520likelihood%2520coding%2520and%2520show%2520that%2520it%250Aachieves%2520considerable%2520performance%2520in%2520both%2520tasks%2520compared%2520to%2520baseline%2520encodings%250Aoptimized%2520for%2520either%2520localization%2520or%2520mask%2520estimation.%2520In%2520the%2520same%2520setup%252C%2520we%250Ademonstrate%2520superiority%2520for%2520joint%2520estimation%2520of%2520both%2520quantities.%2520Conclusively%252C%250Awe%2520propose%2520a%2520universal%2520approach%2520which%2520can%2520replace%2520an%2520upstream%2520sound%2520source%250Alocalization%2520system%2520solely%2520by%2520adapting%2520the%2520training%2520framework%252C%2520making%2520it%2520highly%250Arelevant%2520in%2520performance-critical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask-Weighted%20Spatial%20Likelihood%20Coding%20for%20Speaker-Independent%20Joint%0A%20%20Localization%20and%20Mask%20Estimation&entry.906535625=Jakob%20Kienegger%20and%20Alina%20Mannanova%20and%20Timo%20Gerkmann&entry.1292438233=%20%20Due%20to%20their%20robustness%20and%20flexibility%2C%20neural-driven%20beamformers%20are%20a%0Apopular%20choice%20for%20speech%20separation%20in%20challenging%20environments%20with%20a%20varying%0Aamount%20of%20simultaneous%20speakers%20alongside%20noise%20and%20reverberation.%0ATime-frequency%20masks%20and%20relative%20directions%20of%20the%20speakers%20regarding%20a%20fixed%0Aspatial%20grid%20can%20be%20used%20to%20estimate%20the%20beamformer%27s%20parameters.%20To%20some%0Adegree%2C%20speaker-independence%20is%20achieved%20by%20ensuring%20a%20greater%20amount%20of%0Aspatial%20partitions%20than%20speech%20sources.%20In%20this%20work%2C%20we%20analyze%20how%20to%20encode%0Aboth%20mask%20and%20positioning%20into%20such%20a%20grid%20to%20enable%20joint%20estimation%20of%20both%0Aquantities.%20We%20propose%20mask-weighted%20spatial%20likelihood%20coding%20and%20show%20that%20it%0Aachieves%20considerable%20performance%20in%20both%20tasks%20compared%20to%20baseline%20encodings%0Aoptimized%20for%20either%20localization%20or%20mask%20estimation.%20In%20the%20same%20setup%2C%20we%0Ademonstrate%20superiority%20for%20joint%20estimation%20of%20both%20quantities.%20Conclusively%2C%0Awe%20propose%20a%20universal%20approach%20which%20can%20replace%20an%20upstream%20sound%20source%0Alocalization%20system%20solely%20by%20adapting%20the%20training%20framework%2C%20making%20it%20highly%0Arelevant%20in%20performance-critical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19595v1&entry.124074799=Read"},
{"title": "Block and Detail: Scaffolding Sketch-to-Image Generation", "author": "Vishnu Sarukkai and Lu Yuan and Mia Tang and Maneesh Agrawala and Kayvon Fatahalian", "abstract": "  We introduce a novel sketch-to-image tool that aligns with the iterative\nrefinement process of artists. Our tool lets users sketch blocking strokes to\ncoarsely represent the placement and form of objects and detail strokes to\nrefine their shape and silhouettes. We develop a two-pass algorithm for\ngenerating high-fidelity images from such sketches at any point in the\niterative process. In the first pass we use a ControlNet to generate an image\nthat strictly follows all the strokes (blocking and detail) and in the second\npass we add variation by renoising regions surrounding blocking strokes. We\nalso present a dataset generation scheme that, when used to train a ControlNet\narchitecture, allows regions that do not contain strokes to be interpreted as\nnot-yet-specified regions rather than empty space. We show that this\npartial-sketch-aware ControlNet can generate coherent elements from partial\nsketches that only contain a small number of strokes. The high-fidelity images\nproduced by our approach serve as scaffolds that can help the user adjust the\nshape and proportions of objects or add additional elements to the composition.\nWe demonstrate the effectiveness of our approach with a variety of examples and\nevaluative comparisons. Quantitatively, evaluative user feedback indicates that\nnovice viewers prefer the quality of images from our algorithm over a baseline\nScribble ControlNet for 84% of the pairs and found our images had less\ndistortion in 81% of the pairs.\n", "link": "http://arxiv.org/abs/2402.18116v2", "date": "2024-10-25", "relevancy": 2.178, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5605}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Block%20and%20Detail%3A%20Scaffolding%20Sketch-to-Image%20Generation&body=Title%3A%20Block%20and%20Detail%3A%20Scaffolding%20Sketch-to-Image%20Generation%0AAuthor%3A%20Vishnu%20Sarukkai%20and%20Lu%20Yuan%20and%20Mia%20Tang%20and%20Maneesh%20Agrawala%20and%20Kayvon%20Fatahalian%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20sketch-to-image%20tool%20that%20aligns%20with%20the%20iterative%0Arefinement%20process%20of%20artists.%20Our%20tool%20lets%20users%20sketch%20blocking%20strokes%20to%0Acoarsely%20represent%20the%20placement%20and%20form%20of%20objects%20and%20detail%20strokes%20to%0Arefine%20their%20shape%20and%20silhouettes.%20We%20develop%20a%20two-pass%20algorithm%20for%0Agenerating%20high-fidelity%20images%20from%20such%20sketches%20at%20any%20point%20in%20the%0Aiterative%20process.%20In%20the%20first%20pass%20we%20use%20a%20ControlNet%20to%20generate%20an%20image%0Athat%20strictly%20follows%20all%20the%20strokes%20%28blocking%20and%20detail%29%20and%20in%20the%20second%0Apass%20we%20add%20variation%20by%20renoising%20regions%20surrounding%20blocking%20strokes.%20We%0Aalso%20present%20a%20dataset%20generation%20scheme%20that%2C%20when%20used%20to%20train%20a%20ControlNet%0Aarchitecture%2C%20allows%20regions%20that%20do%20not%20contain%20strokes%20to%20be%20interpreted%20as%0Anot-yet-specified%20regions%20rather%20than%20empty%20space.%20We%20show%20that%20this%0Apartial-sketch-aware%20ControlNet%20can%20generate%20coherent%20elements%20from%20partial%0Asketches%20that%20only%20contain%20a%20small%20number%20of%20strokes.%20The%20high-fidelity%20images%0Aproduced%20by%20our%20approach%20serve%20as%20scaffolds%20that%20can%20help%20the%20user%20adjust%20the%0Ashape%20and%20proportions%20of%20objects%20or%20add%20additional%20elements%20to%20the%20composition.%0AWe%20demonstrate%20the%20effectiveness%20of%20our%20approach%20with%20a%20variety%20of%20examples%20and%0Aevaluative%20comparisons.%20Quantitatively%2C%20evaluative%20user%20feedback%20indicates%20that%0Anovice%20viewers%20prefer%20the%20quality%20of%20images%20from%20our%20algorithm%20over%20a%20baseline%0AScribble%20ControlNet%20for%2084%25%20of%20the%20pairs%20and%20found%20our%20images%20had%20less%0Adistortion%20in%2081%25%20of%20the%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18116v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlock%2520and%2520Detail%253A%2520Scaffolding%2520Sketch-to-Image%2520Generation%26entry.906535625%3DVishnu%2520Sarukkai%2520and%2520Lu%2520Yuan%2520and%2520Mia%2520Tang%2520and%2520Maneesh%2520Agrawala%2520and%2520Kayvon%2520Fatahalian%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520sketch-to-image%2520tool%2520that%2520aligns%2520with%2520the%2520iterative%250Arefinement%2520process%2520of%2520artists.%2520Our%2520tool%2520lets%2520users%2520sketch%2520blocking%2520strokes%2520to%250Acoarsely%2520represent%2520the%2520placement%2520and%2520form%2520of%2520objects%2520and%2520detail%2520strokes%2520to%250Arefine%2520their%2520shape%2520and%2520silhouettes.%2520We%2520develop%2520a%2520two-pass%2520algorithm%2520for%250Agenerating%2520high-fidelity%2520images%2520from%2520such%2520sketches%2520at%2520any%2520point%2520in%2520the%250Aiterative%2520process.%2520In%2520the%2520first%2520pass%2520we%2520use%2520a%2520ControlNet%2520to%2520generate%2520an%2520image%250Athat%2520strictly%2520follows%2520all%2520the%2520strokes%2520%2528blocking%2520and%2520detail%2529%2520and%2520in%2520the%2520second%250Apass%2520we%2520add%2520variation%2520by%2520renoising%2520regions%2520surrounding%2520blocking%2520strokes.%2520We%250Aalso%2520present%2520a%2520dataset%2520generation%2520scheme%2520that%252C%2520when%2520used%2520to%2520train%2520a%2520ControlNet%250Aarchitecture%252C%2520allows%2520regions%2520that%2520do%2520not%2520contain%2520strokes%2520to%2520be%2520interpreted%2520as%250Anot-yet-specified%2520regions%2520rather%2520than%2520empty%2520space.%2520We%2520show%2520that%2520this%250Apartial-sketch-aware%2520ControlNet%2520can%2520generate%2520coherent%2520elements%2520from%2520partial%250Asketches%2520that%2520only%2520contain%2520a%2520small%2520number%2520of%2520strokes.%2520The%2520high-fidelity%2520images%250Aproduced%2520by%2520our%2520approach%2520serve%2520as%2520scaffolds%2520that%2520can%2520help%2520the%2520user%2520adjust%2520the%250Ashape%2520and%2520proportions%2520of%2520objects%2520or%2520add%2520additional%2520elements%2520to%2520the%2520composition.%250AWe%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520with%2520a%2520variety%2520of%2520examples%2520and%250Aevaluative%2520comparisons.%2520Quantitatively%252C%2520evaluative%2520user%2520feedback%2520indicates%2520that%250Anovice%2520viewers%2520prefer%2520the%2520quality%2520of%2520images%2520from%2520our%2520algorithm%2520over%2520a%2520baseline%250AScribble%2520ControlNet%2520for%252084%2525%2520of%2520the%2520pairs%2520and%2520found%2520our%2520images%2520had%2520less%250Adistortion%2520in%252081%2525%2520of%2520the%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18116v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Block%20and%20Detail%3A%20Scaffolding%20Sketch-to-Image%20Generation&entry.906535625=Vishnu%20Sarukkai%20and%20Lu%20Yuan%20and%20Mia%20Tang%20and%20Maneesh%20Agrawala%20and%20Kayvon%20Fatahalian&entry.1292438233=%20%20We%20introduce%20a%20novel%20sketch-to-image%20tool%20that%20aligns%20with%20the%20iterative%0Arefinement%20process%20of%20artists.%20Our%20tool%20lets%20users%20sketch%20blocking%20strokes%20to%0Acoarsely%20represent%20the%20placement%20and%20form%20of%20objects%20and%20detail%20strokes%20to%0Arefine%20their%20shape%20and%20silhouettes.%20We%20develop%20a%20two-pass%20algorithm%20for%0Agenerating%20high-fidelity%20images%20from%20such%20sketches%20at%20any%20point%20in%20the%0Aiterative%20process.%20In%20the%20first%20pass%20we%20use%20a%20ControlNet%20to%20generate%20an%20image%0Athat%20strictly%20follows%20all%20the%20strokes%20%28blocking%20and%20detail%29%20and%20in%20the%20second%0Apass%20we%20add%20variation%20by%20renoising%20regions%20surrounding%20blocking%20strokes.%20We%0Aalso%20present%20a%20dataset%20generation%20scheme%20that%2C%20when%20used%20to%20train%20a%20ControlNet%0Aarchitecture%2C%20allows%20regions%20that%20do%20not%20contain%20strokes%20to%20be%20interpreted%20as%0Anot-yet-specified%20regions%20rather%20than%20empty%20space.%20We%20show%20that%20this%0Apartial-sketch-aware%20ControlNet%20can%20generate%20coherent%20elements%20from%20partial%0Asketches%20that%20only%20contain%20a%20small%20number%20of%20strokes.%20The%20high-fidelity%20images%0Aproduced%20by%20our%20approach%20serve%20as%20scaffolds%20that%20can%20help%20the%20user%20adjust%20the%0Ashape%20and%20proportions%20of%20objects%20or%20add%20additional%20elements%20to%20the%20composition.%0AWe%20demonstrate%20the%20effectiveness%20of%20our%20approach%20with%20a%20variety%20of%20examples%20and%0Aevaluative%20comparisons.%20Quantitatively%2C%20evaluative%20user%20feedback%20indicates%20that%0Anovice%20viewers%20prefer%20the%20quality%20of%20images%20from%20our%20algorithm%20over%20a%20baseline%0AScribble%20ControlNet%20for%2084%25%20of%20the%20pairs%20and%20found%20our%20images%20had%20less%0Adistortion%20in%2081%25%20of%20the%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18116v2&entry.124074799=Read"},
{"title": "Transductive Learning for Near-Duplicate Image Detection in Scanned\n  Photo Collections", "author": "Francesc Net and Marc Folia and Pep Casals and Lluis Gomez", "abstract": "  This paper presents a comparative study of near-duplicate image detection\ntechniques in a real-world use case scenario, where a document management\ncompany is commissioned to manually annotate a collection of scanned\nphotographs. Detecting duplicate and near-duplicate photographs can reduce the\ntime spent on manual annotation by archivists. This real use case differs from\nlaboratory settings as the deployment dataset is available in advance, allowing\nthe use of transductive learning. We propose a transductive learning approach\nthat leverages state-of-the-art deep learning architectures such as\nconvolutional neural networks (CNNs) and Vision Transformers (ViTs). Our\napproach involves pre-training a deep neural network on a large dataset and\nthen fine-tuning the network on the unlabeled target collection with\nself-supervised learning. The results show that the proposed approach\noutperforms the baseline methods in the task of near-duplicate image detection\nin the UKBench and an in-house private dataset.\n", "link": "http://arxiv.org/abs/2410.19437v1", "date": "2024-10-25", "relevancy": 2.1604, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5685}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5285}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transductive%20Learning%20for%20Near-Duplicate%20Image%20Detection%20in%20Scanned%0A%20%20Photo%20Collections&body=Title%3A%20Transductive%20Learning%20for%20Near-Duplicate%20Image%20Detection%20in%20Scanned%0A%20%20Photo%20Collections%0AAuthor%3A%20Francesc%20Net%20and%20Marc%20Folia%20and%20Pep%20Casals%20and%20Lluis%20Gomez%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comparative%20study%20of%20near-duplicate%20image%20detection%0Atechniques%20in%20a%20real-world%20use%20case%20scenario%2C%20where%20a%20document%20management%0Acompany%20is%20commissioned%20to%20manually%20annotate%20a%20collection%20of%20scanned%0Aphotographs.%20Detecting%20duplicate%20and%20near-duplicate%20photographs%20can%20reduce%20the%0Atime%20spent%20on%20manual%20annotation%20by%20archivists.%20This%20real%20use%20case%20differs%20from%0Alaboratory%20settings%20as%20the%20deployment%20dataset%20is%20available%20in%20advance%2C%20allowing%0Athe%20use%20of%20transductive%20learning.%20We%20propose%20a%20transductive%20learning%20approach%0Athat%20leverages%20state-of-the-art%20deep%20learning%20architectures%20such%20as%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29.%20Our%0Aapproach%20involves%20pre-training%20a%20deep%20neural%20network%20on%20a%20large%20dataset%20and%0Athen%20fine-tuning%20the%20network%20on%20the%20unlabeled%20target%20collection%20with%0Aself-supervised%20learning.%20The%20results%20show%20that%20the%20proposed%20approach%0Aoutperforms%20the%20baseline%20methods%20in%20the%20task%20of%20near-duplicate%20image%20detection%0Ain%20the%20UKBench%20and%20an%20in-house%20private%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransductive%2520Learning%2520for%2520Near-Duplicate%2520Image%2520Detection%2520in%2520Scanned%250A%2520%2520Photo%2520Collections%26entry.906535625%3DFrancesc%2520Net%2520and%2520Marc%2520Folia%2520and%2520Pep%2520Casals%2520and%2520Lluis%2520Gomez%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comparative%2520study%2520of%2520near-duplicate%2520image%2520detection%250Atechniques%2520in%2520a%2520real-world%2520use%2520case%2520scenario%252C%2520where%2520a%2520document%2520management%250Acompany%2520is%2520commissioned%2520to%2520manually%2520annotate%2520a%2520collection%2520of%2520scanned%250Aphotographs.%2520Detecting%2520duplicate%2520and%2520near-duplicate%2520photographs%2520can%2520reduce%2520the%250Atime%2520spent%2520on%2520manual%2520annotation%2520by%2520archivists.%2520This%2520real%2520use%2520case%2520differs%2520from%250Alaboratory%2520settings%2520as%2520the%2520deployment%2520dataset%2520is%2520available%2520in%2520advance%252C%2520allowing%250Athe%2520use%2520of%2520transductive%2520learning.%2520We%2520propose%2520a%2520transductive%2520learning%2520approach%250Athat%2520leverages%2520state-of-the-art%2520deep%2520learning%2520architectures%2520such%2520as%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%2520Vision%2520Transformers%2520%2528ViTs%2529.%2520Our%250Aapproach%2520involves%2520pre-training%2520a%2520deep%2520neural%2520network%2520on%2520a%2520large%2520dataset%2520and%250Athen%2520fine-tuning%2520the%2520network%2520on%2520the%2520unlabeled%2520target%2520collection%2520with%250Aself-supervised%2520learning.%2520The%2520results%2520show%2520that%2520the%2520proposed%2520approach%250Aoutperforms%2520the%2520baseline%2520methods%2520in%2520the%2520task%2520of%2520near-duplicate%2520image%2520detection%250Ain%2520the%2520UKBench%2520and%2520an%2520in-house%2520private%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transductive%20Learning%20for%20Near-Duplicate%20Image%20Detection%20in%20Scanned%0A%20%20Photo%20Collections&entry.906535625=Francesc%20Net%20and%20Marc%20Folia%20and%20Pep%20Casals%20and%20Lluis%20Gomez&entry.1292438233=%20%20This%20paper%20presents%20a%20comparative%20study%20of%20near-duplicate%20image%20detection%0Atechniques%20in%20a%20real-world%20use%20case%20scenario%2C%20where%20a%20document%20management%0Acompany%20is%20commissioned%20to%20manually%20annotate%20a%20collection%20of%20scanned%0Aphotographs.%20Detecting%20duplicate%20and%20near-duplicate%20photographs%20can%20reduce%20the%0Atime%20spent%20on%20manual%20annotation%20by%20archivists.%20This%20real%20use%20case%20differs%20from%0Alaboratory%20settings%20as%20the%20deployment%20dataset%20is%20available%20in%20advance%2C%20allowing%0Athe%20use%20of%20transductive%20learning.%20We%20propose%20a%20transductive%20learning%20approach%0Athat%20leverages%20state-of-the-art%20deep%20learning%20architectures%20such%20as%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29.%20Our%0Aapproach%20involves%20pre-training%20a%20deep%20neural%20network%20on%20a%20large%20dataset%20and%0Athen%20fine-tuning%20the%20network%20on%20the%20unlabeled%20target%20collection%20with%0Aself-supervised%20learning.%20The%20results%20show%20that%20the%20proposed%20approach%0Aoutperforms%20the%20baseline%20methods%20in%20the%20task%20of%20near-duplicate%20image%20detection%0Ain%20the%20UKBench%20and%20an%20in-house%20private%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19437v1&entry.124074799=Read"},
{"title": "FLiP: Privacy-Preserving Federated Learning based on the Principle of\n  Least Privileg", "author": "ShiMao Xu and Xiaopeng Ke and Xing Su and Shucheng Li and Hao wu and Fengyuan Xu and Sheng Zhong", "abstract": "  Federated Learning (FL) allows users to share knowledge instead of raw data\nto train a model with high accuracy. Unfortunately, during the training, users\nlose control over the knowledge shared, which causes serious data privacy\nissues. We hold that users are only willing and need to share the essential\nknowledge to the training task to obtain the FL model with high accuracy.\nHowever, existing efforts cannot help users minimize the shared knowledge\naccording to the user intention in the FL training procedure. This work\nproposes FLiP, which aims to bring the principle of least privilege (PoLP) to\nFL training. The key design of FLiP is applying elaborate information reduction\non the training data through a local-global dataset distillation design. We\nmeasure the privacy performance through attribute inference and membership\ninference attacks. Extensive experiments show that FLiP strikes a good balance\nbetween model accuracy and privacy protection.\n", "link": "http://arxiv.org/abs/2410.19548v1", "date": "2024-10-25", "relevancy": 2.1568, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4482}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4377}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLiP%3A%20Privacy-Preserving%20Federated%20Learning%20based%20on%20the%20Principle%20of%0A%20%20Least%20Privileg&body=Title%3A%20FLiP%3A%20Privacy-Preserving%20Federated%20Learning%20based%20on%20the%20Principle%20of%0A%20%20Least%20Privileg%0AAuthor%3A%20ShiMao%20Xu%20and%20Xiaopeng%20Ke%20and%20Xing%20Su%20and%20Shucheng%20Li%20and%20Hao%20wu%20and%20Fengyuan%20Xu%20and%20Sheng%20Zhong%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20allows%20users%20to%20share%20knowledge%20instead%20of%20raw%20data%0Ato%20train%20a%20model%20with%20high%20accuracy.%20Unfortunately%2C%20during%20the%20training%2C%20users%0Alose%20control%20over%20the%20knowledge%20shared%2C%20which%20causes%20serious%20data%20privacy%0Aissues.%20We%20hold%20that%20users%20are%20only%20willing%20and%20need%20to%20share%20the%20essential%0Aknowledge%20to%20the%20training%20task%20to%20obtain%20the%20FL%20model%20with%20high%20accuracy.%0AHowever%2C%20existing%20efforts%20cannot%20help%20users%20minimize%20the%20shared%20knowledge%0Aaccording%20to%20the%20user%20intention%20in%20the%20FL%20training%20procedure.%20This%20work%0Aproposes%20FLiP%2C%20which%20aims%20to%20bring%20the%20principle%20of%20least%20privilege%20%28PoLP%29%20to%0AFL%20training.%20The%20key%20design%20of%20FLiP%20is%20applying%20elaborate%20information%20reduction%0Aon%20the%20training%20data%20through%20a%20local-global%20dataset%20distillation%20design.%20We%0Ameasure%20the%20privacy%20performance%20through%20attribute%20inference%20and%20membership%0Ainference%20attacks.%20Extensive%20experiments%20show%20that%20FLiP%20strikes%20a%20good%20balance%0Abetween%20model%20accuracy%20and%20privacy%20protection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLiP%253A%2520Privacy-Preserving%2520Federated%2520Learning%2520based%2520on%2520the%2520Principle%2520of%250A%2520%2520Least%2520Privileg%26entry.906535625%3DShiMao%2520Xu%2520and%2520Xiaopeng%2520Ke%2520and%2520Xing%2520Su%2520and%2520Shucheng%2520Li%2520and%2520Hao%2520wu%2520and%2520Fengyuan%2520Xu%2520and%2520Sheng%2520Zhong%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520allows%2520users%2520to%2520share%2520knowledge%2520instead%2520of%2520raw%2520data%250Ato%2520train%2520a%2520model%2520with%2520high%2520accuracy.%2520Unfortunately%252C%2520during%2520the%2520training%252C%2520users%250Alose%2520control%2520over%2520the%2520knowledge%2520shared%252C%2520which%2520causes%2520serious%2520data%2520privacy%250Aissues.%2520We%2520hold%2520that%2520users%2520are%2520only%2520willing%2520and%2520need%2520to%2520share%2520the%2520essential%250Aknowledge%2520to%2520the%2520training%2520task%2520to%2520obtain%2520the%2520FL%2520model%2520with%2520high%2520accuracy.%250AHowever%252C%2520existing%2520efforts%2520cannot%2520help%2520users%2520minimize%2520the%2520shared%2520knowledge%250Aaccording%2520to%2520the%2520user%2520intention%2520in%2520the%2520FL%2520training%2520procedure.%2520This%2520work%250Aproposes%2520FLiP%252C%2520which%2520aims%2520to%2520bring%2520the%2520principle%2520of%2520least%2520privilege%2520%2528PoLP%2529%2520to%250AFL%2520training.%2520The%2520key%2520design%2520of%2520FLiP%2520is%2520applying%2520elaborate%2520information%2520reduction%250Aon%2520the%2520training%2520data%2520through%2520a%2520local-global%2520dataset%2520distillation%2520design.%2520We%250Ameasure%2520the%2520privacy%2520performance%2520through%2520attribute%2520inference%2520and%2520membership%250Ainference%2520attacks.%2520Extensive%2520experiments%2520show%2520that%2520FLiP%2520strikes%2520a%2520good%2520balance%250Abetween%2520model%2520accuracy%2520and%2520privacy%2520protection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLiP%3A%20Privacy-Preserving%20Federated%20Learning%20based%20on%20the%20Principle%20of%0A%20%20Least%20Privileg&entry.906535625=ShiMao%20Xu%20and%20Xiaopeng%20Ke%20and%20Xing%20Su%20and%20Shucheng%20Li%20and%20Hao%20wu%20and%20Fengyuan%20Xu%20and%20Sheng%20Zhong&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20allows%20users%20to%20share%20knowledge%20instead%20of%20raw%20data%0Ato%20train%20a%20model%20with%20high%20accuracy.%20Unfortunately%2C%20during%20the%20training%2C%20users%0Alose%20control%20over%20the%20knowledge%20shared%2C%20which%20causes%20serious%20data%20privacy%0Aissues.%20We%20hold%20that%20users%20are%20only%20willing%20and%20need%20to%20share%20the%20essential%0Aknowledge%20to%20the%20training%20task%20to%20obtain%20the%20FL%20model%20with%20high%20accuracy.%0AHowever%2C%20existing%20efforts%20cannot%20help%20users%20minimize%20the%20shared%20knowledge%0Aaccording%20to%20the%20user%20intention%20in%20the%20FL%20training%20procedure.%20This%20work%0Aproposes%20FLiP%2C%20which%20aims%20to%20bring%20the%20principle%20of%20least%20privilege%20%28PoLP%29%20to%0AFL%20training.%20The%20key%20design%20of%20FLiP%20is%20applying%20elaborate%20information%20reduction%0Aon%20the%20training%20data%20through%20a%20local-global%20dataset%20distillation%20design.%20We%0Ameasure%20the%20privacy%20performance%20through%20attribute%20inference%20and%20membership%0Ainference%20attacks.%20Extensive%20experiments%20show%20that%20FLiP%20strikes%20a%20good%20balance%0Abetween%20model%20accuracy%20and%20privacy%20protection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19548v1&entry.124074799=Read"},
{"title": "Balancing the Scales: Enhancing Fairness in Facial Expression\n  Recognition with Latent Alignment", "author": "Syed Sameen Ahmad Rizvi and Aryan Seth and Pratik Narang", "abstract": "  Automatically recognizing emotional intent using facial expression has been a\nthoroughly investigated topic in the realm of computer vision. Facial\nExpression Recognition (FER), being a supervised learning task, relies heavily\non substantially large data exemplifying various socio-cultural demographic\nattributes. Over the past decade, several real-world in-the-wild FER datasets\nthat have been proposed were collected through crowd-sourcing or web-scraping.\nHowever, most of these practically used datasets employ a manual annotation\nmethodology for labeling emotional intent, which inherently propagates\nindividual demographic biases. Moreover, these datasets also lack an equitable\nrepresentation of various socio-cultural demographic groups, thereby inducing a\nclass imbalance. Bias analysis and its mitigation have been investigated across\nmultiple domains and problem settings, however, in the FER domain, this is a\nrelatively lesser explored area. This work leverages representation learning\nbased on latent spaces to mitigate bias in facial expression recognition\nsystems, thereby enhancing a deep learning model's fairness and overall\naccuracy.\n", "link": "http://arxiv.org/abs/2410.19444v1", "date": "2024-10-25", "relevancy": 2.1502, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5527}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5412}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20the%20Scales%3A%20Enhancing%20Fairness%20in%20Facial%20Expression%0A%20%20Recognition%20with%20Latent%20Alignment&body=Title%3A%20Balancing%20the%20Scales%3A%20Enhancing%20Fairness%20in%20Facial%20Expression%0A%20%20Recognition%20with%20Latent%20Alignment%0AAuthor%3A%20Syed%20Sameen%20Ahmad%20Rizvi%20and%20Aryan%20Seth%20and%20Pratik%20Narang%0AAbstract%3A%20%20%20Automatically%20recognizing%20emotional%20intent%20using%20facial%20expression%20has%20been%20a%0Athoroughly%20investigated%20topic%20in%20the%20realm%20of%20computer%20vision.%20Facial%0AExpression%20Recognition%20%28FER%29%2C%20being%20a%20supervised%20learning%20task%2C%20relies%20heavily%0Aon%20substantially%20large%20data%20exemplifying%20various%20socio-cultural%20demographic%0Aattributes.%20Over%20the%20past%20decade%2C%20several%20real-world%20in-the-wild%20FER%20datasets%0Athat%20have%20been%20proposed%20were%20collected%20through%20crowd-sourcing%20or%20web-scraping.%0AHowever%2C%20most%20of%20these%20practically%20used%20datasets%20employ%20a%20manual%20annotation%0Amethodology%20for%20labeling%20emotional%20intent%2C%20which%20inherently%20propagates%0Aindividual%20demographic%20biases.%20Moreover%2C%20these%20datasets%20also%20lack%20an%20equitable%0Arepresentation%20of%20various%20socio-cultural%20demographic%20groups%2C%20thereby%20inducing%20a%0Aclass%20imbalance.%20Bias%20analysis%20and%20its%20mitigation%20have%20been%20investigated%20across%0Amultiple%20domains%20and%20problem%20settings%2C%20however%2C%20in%20the%20FER%20domain%2C%20this%20is%20a%0Arelatively%20lesser%20explored%20area.%20This%20work%20leverages%20representation%20learning%0Abased%20on%20latent%20spaces%20to%20mitigate%20bias%20in%20facial%20expression%20recognition%0Asystems%2C%20thereby%20enhancing%20a%20deep%20learning%20model%27s%20fairness%20and%20overall%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520the%2520Scales%253A%2520Enhancing%2520Fairness%2520in%2520Facial%2520Expression%250A%2520%2520Recognition%2520with%2520Latent%2520Alignment%26entry.906535625%3DSyed%2520Sameen%2520Ahmad%2520Rizvi%2520and%2520Aryan%2520Seth%2520and%2520Pratik%2520Narang%26entry.1292438233%3D%2520%2520Automatically%2520recognizing%2520emotional%2520intent%2520using%2520facial%2520expression%2520has%2520been%2520a%250Athoroughly%2520investigated%2520topic%2520in%2520the%2520realm%2520of%2520computer%2520vision.%2520Facial%250AExpression%2520Recognition%2520%2528FER%2529%252C%2520being%2520a%2520supervised%2520learning%2520task%252C%2520relies%2520heavily%250Aon%2520substantially%2520large%2520data%2520exemplifying%2520various%2520socio-cultural%2520demographic%250Aattributes.%2520Over%2520the%2520past%2520decade%252C%2520several%2520real-world%2520in-the-wild%2520FER%2520datasets%250Athat%2520have%2520been%2520proposed%2520were%2520collected%2520through%2520crowd-sourcing%2520or%2520web-scraping.%250AHowever%252C%2520most%2520of%2520these%2520practically%2520used%2520datasets%2520employ%2520a%2520manual%2520annotation%250Amethodology%2520for%2520labeling%2520emotional%2520intent%252C%2520which%2520inherently%2520propagates%250Aindividual%2520demographic%2520biases.%2520Moreover%252C%2520these%2520datasets%2520also%2520lack%2520an%2520equitable%250Arepresentation%2520of%2520various%2520socio-cultural%2520demographic%2520groups%252C%2520thereby%2520inducing%2520a%250Aclass%2520imbalance.%2520Bias%2520analysis%2520and%2520its%2520mitigation%2520have%2520been%2520investigated%2520across%250Amultiple%2520domains%2520and%2520problem%2520settings%252C%2520however%252C%2520in%2520the%2520FER%2520domain%252C%2520this%2520is%2520a%250Arelatively%2520lesser%2520explored%2520area.%2520This%2520work%2520leverages%2520representation%2520learning%250Abased%2520on%2520latent%2520spaces%2520to%2520mitigate%2520bias%2520in%2520facial%2520expression%2520recognition%250Asystems%252C%2520thereby%2520enhancing%2520a%2520deep%2520learning%2520model%2527s%2520fairness%2520and%2520overall%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20the%20Scales%3A%20Enhancing%20Fairness%20in%20Facial%20Expression%0A%20%20Recognition%20with%20Latent%20Alignment&entry.906535625=Syed%20Sameen%20Ahmad%20Rizvi%20and%20Aryan%20Seth%20and%20Pratik%20Narang&entry.1292438233=%20%20Automatically%20recognizing%20emotional%20intent%20using%20facial%20expression%20has%20been%20a%0Athoroughly%20investigated%20topic%20in%20the%20realm%20of%20computer%20vision.%20Facial%0AExpression%20Recognition%20%28FER%29%2C%20being%20a%20supervised%20learning%20task%2C%20relies%20heavily%0Aon%20substantially%20large%20data%20exemplifying%20various%20socio-cultural%20demographic%0Aattributes.%20Over%20the%20past%20decade%2C%20several%20real-world%20in-the-wild%20FER%20datasets%0Athat%20have%20been%20proposed%20were%20collected%20through%20crowd-sourcing%20or%20web-scraping.%0AHowever%2C%20most%20of%20these%20practically%20used%20datasets%20employ%20a%20manual%20annotation%0Amethodology%20for%20labeling%20emotional%20intent%2C%20which%20inherently%20propagates%0Aindividual%20demographic%20biases.%20Moreover%2C%20these%20datasets%20also%20lack%20an%20equitable%0Arepresentation%20of%20various%20socio-cultural%20demographic%20groups%2C%20thereby%20inducing%20a%0Aclass%20imbalance.%20Bias%20analysis%20and%20its%20mitigation%20have%20been%20investigated%20across%0Amultiple%20domains%20and%20problem%20settings%2C%20however%2C%20in%20the%20FER%20domain%2C%20this%20is%20a%0Arelatively%20lesser%20explored%20area.%20This%20work%20leverages%20representation%20learning%0Abased%20on%20latent%20spaces%20to%20mitigate%20bias%20in%20facial%20expression%20recognition%0Asystems%2C%20thereby%20enhancing%20a%20deep%20learning%20model%27s%20fairness%20and%20overall%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19444v1&entry.124074799=Read"},
{"title": "DMT-HI: MOE-based Hyperbolic Interpretable Deep Manifold Transformation\n  for Unspervised Dimensionality Reduction", "author": "Zelin Zang and Yuhao Wang and Jinlin Wu and Hong Liu and Yue Shen and Stan. Z Li and Zhen Lei", "abstract": "  Dimensionality reduction (DR) plays a crucial role in various fields,\nincluding data engineering and visualization, by simplifying complex datasets\nwhile retaining essential information. However, the challenge of balancing DR\naccuracy and interpretability remains crucial, particularly for users dealing\nwith high-dimensional data. Traditional DR methods often face a trade-off\nbetween precision and transparency, where optimizing for performance can lead\nto reduced interpretability, and vice versa. This limitation is especially\nprominent in real-world applications such as image, tabular, and text data\nanalysis, where both accuracy and interpretability are critical. To address\nthese challenges, this work introduces the MOE-based Hyperbolic Interpretable\nDeep Manifold Transformation (DMT-HI). The proposed approach combines\nhyperbolic embeddings, which effectively capture complex hierarchical\nstructures, with Mixture of Experts (MOE) models, which dynamically allocate\ntasks based on input features. DMT-HI enhances DR accuracy by leveraging\nhyperbolic embeddings to represent the hierarchical nature of data, while also\nimproving interpretability by explicitly linking input data, embedding\noutcomes, and key features through the MOE structure. Extensive experiments\ndemonstrate that DMT-HI consistently achieves superior performance in both DR\naccuracy and model interpretability, making it a robust solution for complex\ndata analysis. The code is available at\n\\url{https://github.com/zangzelin/code_dmthi}.\n", "link": "http://arxiv.org/abs/2410.19504v1", "date": "2024-10-25", "relevancy": 2.1478, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5515}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DMT-HI%3A%20MOE-based%20Hyperbolic%20Interpretable%20Deep%20Manifold%20Transformation%0A%20%20for%20Unspervised%20Dimensionality%20Reduction&body=Title%3A%20DMT-HI%3A%20MOE-based%20Hyperbolic%20Interpretable%20Deep%20Manifold%20Transformation%0A%20%20for%20Unspervised%20Dimensionality%20Reduction%0AAuthor%3A%20Zelin%20Zang%20and%20Yuhao%20Wang%20and%20Jinlin%20Wu%20and%20Hong%20Liu%20and%20Yue%20Shen%20and%20Stan.%20Z%20Li%20and%20Zhen%20Lei%0AAbstract%3A%20%20%20Dimensionality%20reduction%20%28DR%29%20plays%20a%20crucial%20role%20in%20various%20fields%2C%0Aincluding%20data%20engineering%20and%20visualization%2C%20by%20simplifying%20complex%20datasets%0Awhile%20retaining%20essential%20information.%20However%2C%20the%20challenge%20of%20balancing%20DR%0Aaccuracy%20and%20interpretability%20remains%20crucial%2C%20particularly%20for%20users%20dealing%0Awith%20high-dimensional%20data.%20Traditional%20DR%20methods%20often%20face%20a%20trade-off%0Abetween%20precision%20and%20transparency%2C%20where%20optimizing%20for%20performance%20can%20lead%0Ato%20reduced%20interpretability%2C%20and%20vice%20versa.%20This%20limitation%20is%20especially%0Aprominent%20in%20real-world%20applications%20such%20as%20image%2C%20tabular%2C%20and%20text%20data%0Aanalysis%2C%20where%20both%20accuracy%20and%20interpretability%20are%20critical.%20To%20address%0Athese%20challenges%2C%20this%20work%20introduces%20the%20MOE-based%20Hyperbolic%20Interpretable%0ADeep%20Manifold%20Transformation%20%28DMT-HI%29.%20The%20proposed%20approach%20combines%0Ahyperbolic%20embeddings%2C%20which%20effectively%20capture%20complex%20hierarchical%0Astructures%2C%20with%20Mixture%20of%20Experts%20%28MOE%29%20models%2C%20which%20dynamically%20allocate%0Atasks%20based%20on%20input%20features.%20DMT-HI%20enhances%20DR%20accuracy%20by%20leveraging%0Ahyperbolic%20embeddings%20to%20represent%20the%20hierarchical%20nature%20of%20data%2C%20while%20also%0Aimproving%20interpretability%20by%20explicitly%20linking%20input%20data%2C%20embedding%0Aoutcomes%2C%20and%20key%20features%20through%20the%20MOE%20structure.%20Extensive%20experiments%0Ademonstrate%20that%20DMT-HI%20consistently%20achieves%20superior%20performance%20in%20both%20DR%0Aaccuracy%20and%20model%20interpretability%2C%20making%20it%20a%20robust%20solution%20for%20complex%0Adata%20analysis.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zangzelin/code_dmthi%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDMT-HI%253A%2520MOE-based%2520Hyperbolic%2520Interpretable%2520Deep%2520Manifold%2520Transformation%250A%2520%2520for%2520Unspervised%2520Dimensionality%2520Reduction%26entry.906535625%3DZelin%2520Zang%2520and%2520Yuhao%2520Wang%2520and%2520Jinlin%2520Wu%2520and%2520Hong%2520Liu%2520and%2520Yue%2520Shen%2520and%2520Stan.%2520Z%2520Li%2520and%2520Zhen%2520Lei%26entry.1292438233%3D%2520%2520Dimensionality%2520reduction%2520%2528DR%2529%2520plays%2520a%2520crucial%2520role%2520in%2520various%2520fields%252C%250Aincluding%2520data%2520engineering%2520and%2520visualization%252C%2520by%2520simplifying%2520complex%2520datasets%250Awhile%2520retaining%2520essential%2520information.%2520However%252C%2520the%2520challenge%2520of%2520balancing%2520DR%250Aaccuracy%2520and%2520interpretability%2520remains%2520crucial%252C%2520particularly%2520for%2520users%2520dealing%250Awith%2520high-dimensional%2520data.%2520Traditional%2520DR%2520methods%2520often%2520face%2520a%2520trade-off%250Abetween%2520precision%2520and%2520transparency%252C%2520where%2520optimizing%2520for%2520performance%2520can%2520lead%250Ato%2520reduced%2520interpretability%252C%2520and%2520vice%2520versa.%2520This%2520limitation%2520is%2520especially%250Aprominent%2520in%2520real-world%2520applications%2520such%2520as%2520image%252C%2520tabular%252C%2520and%2520text%2520data%250Aanalysis%252C%2520where%2520both%2520accuracy%2520and%2520interpretability%2520are%2520critical.%2520To%2520address%250Athese%2520challenges%252C%2520this%2520work%2520introduces%2520the%2520MOE-based%2520Hyperbolic%2520Interpretable%250ADeep%2520Manifold%2520Transformation%2520%2528DMT-HI%2529.%2520The%2520proposed%2520approach%2520combines%250Ahyperbolic%2520embeddings%252C%2520which%2520effectively%2520capture%2520complex%2520hierarchical%250Astructures%252C%2520with%2520Mixture%2520of%2520Experts%2520%2528MOE%2529%2520models%252C%2520which%2520dynamically%2520allocate%250Atasks%2520based%2520on%2520input%2520features.%2520DMT-HI%2520enhances%2520DR%2520accuracy%2520by%2520leveraging%250Ahyperbolic%2520embeddings%2520to%2520represent%2520the%2520hierarchical%2520nature%2520of%2520data%252C%2520while%2520also%250Aimproving%2520interpretability%2520by%2520explicitly%2520linking%2520input%2520data%252C%2520embedding%250Aoutcomes%252C%2520and%2520key%2520features%2520through%2520the%2520MOE%2520structure.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520DMT-HI%2520consistently%2520achieves%2520superior%2520performance%2520in%2520both%2520DR%250Aaccuracy%2520and%2520model%2520interpretability%252C%2520making%2520it%2520a%2520robust%2520solution%2520for%2520complex%250Adata%2520analysis.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/zangzelin/code_dmthi%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DMT-HI%3A%20MOE-based%20Hyperbolic%20Interpretable%20Deep%20Manifold%20Transformation%0A%20%20for%20Unspervised%20Dimensionality%20Reduction&entry.906535625=Zelin%20Zang%20and%20Yuhao%20Wang%20and%20Jinlin%20Wu%20and%20Hong%20Liu%20and%20Yue%20Shen%20and%20Stan.%20Z%20Li%20and%20Zhen%20Lei&entry.1292438233=%20%20Dimensionality%20reduction%20%28DR%29%20plays%20a%20crucial%20role%20in%20various%20fields%2C%0Aincluding%20data%20engineering%20and%20visualization%2C%20by%20simplifying%20complex%20datasets%0Awhile%20retaining%20essential%20information.%20However%2C%20the%20challenge%20of%20balancing%20DR%0Aaccuracy%20and%20interpretability%20remains%20crucial%2C%20particularly%20for%20users%20dealing%0Awith%20high-dimensional%20data.%20Traditional%20DR%20methods%20often%20face%20a%20trade-off%0Abetween%20precision%20and%20transparency%2C%20where%20optimizing%20for%20performance%20can%20lead%0Ato%20reduced%20interpretability%2C%20and%20vice%20versa.%20This%20limitation%20is%20especially%0Aprominent%20in%20real-world%20applications%20such%20as%20image%2C%20tabular%2C%20and%20text%20data%0Aanalysis%2C%20where%20both%20accuracy%20and%20interpretability%20are%20critical.%20To%20address%0Athese%20challenges%2C%20this%20work%20introduces%20the%20MOE-based%20Hyperbolic%20Interpretable%0ADeep%20Manifold%20Transformation%20%28DMT-HI%29.%20The%20proposed%20approach%20combines%0Ahyperbolic%20embeddings%2C%20which%20effectively%20capture%20complex%20hierarchical%0Astructures%2C%20with%20Mixture%20of%20Experts%20%28MOE%29%20models%2C%20which%20dynamically%20allocate%0Atasks%20based%20on%20input%20features.%20DMT-HI%20enhances%20DR%20accuracy%20by%20leveraging%0Ahyperbolic%20embeddings%20to%20represent%20the%20hierarchical%20nature%20of%20data%2C%20while%20also%0Aimproving%20interpretability%20by%20explicitly%20linking%20input%20data%2C%20embedding%0Aoutcomes%2C%20and%20key%20features%20through%20the%20MOE%20structure.%20Extensive%20experiments%0Ademonstrate%20that%20DMT-HI%20consistently%20achieves%20superior%20performance%20in%20both%20DR%0Aaccuracy%20and%20model%20interpretability%2C%20making%20it%20a%20robust%20solution%20for%20complex%0Adata%20analysis.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zangzelin/code_dmthi%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19504v1&entry.124074799=Read"},
{"title": "MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular\n  Metaverse Services", "author": "Hongjia Wu and Hui Zeng and Zehui Xiong and Jiawen Kang and Zhiping Cai and Tse-Tin Chan and Dusit Niyato and Zhu Han", "abstract": "  Updates of extensive Internet of Things (IoT) data are critical to the\nimmersion of vehicular metaverse services. However, providing high-quality and\nsustainable data in unstable and resource-constrained vehicular networks\nremains a significant challenge. To address this problem, we put forth a novel\nimmersion-aware model trading framework that incentivizes metaverse users (MUs)\nto contribute learning models trained by their latest local data for augmented\nreality (AR) services in the vehicular metaverse, while preserving their\nprivacy through federated learning. To comprehensively evaluate the\ncontribution of locally trained learning models provided by MUs to AR services,\nwe design a new immersion metric that captures service immersion by considering\nthe freshness and accuracy of learning models, as well as the amount and\npotential value of raw data used for training. We model the trading\ninteractions between metaverse service providers (MSPs) and MUs as an\nequilibrium problem with equilibrium constraints (EPEC) to analyze and balance\ntheir costs and gains. Moreover, considering dynamic network conditions and\nprivacy concerns, we formulate the reward decisions of MSPs as a multi-agent\nMarkov decision process. Then, a fully distributed dynamic reward method based\non deep reinforcement learning is presented, which operates without any private\ninformation about MUs and other MSPs. Experimental results demonstrate that the\nproposed framework can effectively provide higher-value models for object\ndetection and classification in AR services on real AR-related vehicle datasets\ncompared to benchmark schemes.\n", "link": "http://arxiv.org/abs/2410.19665v1", "date": "2024-10-25", "relevancy": 2.1417, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5424}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5339}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaTrading%3A%20An%20Immersion-Aware%20Model%20Trading%20Framework%20for%20Vehicular%0A%20%20Metaverse%20Services&body=Title%3A%20MetaTrading%3A%20An%20Immersion-Aware%20Model%20Trading%20Framework%20for%20Vehicular%0A%20%20Metaverse%20Services%0AAuthor%3A%20Hongjia%20Wu%20and%20Hui%20Zeng%20and%20Zehui%20Xiong%20and%20Jiawen%20Kang%20and%20Zhiping%20Cai%20and%20Tse-Tin%20Chan%20and%20Dusit%20Niyato%20and%20Zhu%20Han%0AAbstract%3A%20%20%20Updates%20of%20extensive%20Internet%20of%20Things%20%28IoT%29%20data%20are%20critical%20to%20the%0Aimmersion%20of%20vehicular%20metaverse%20services.%20However%2C%20providing%20high-quality%20and%0Asustainable%20data%20in%20unstable%20and%20resource-constrained%20vehicular%20networks%0Aremains%20a%20significant%20challenge.%20To%20address%20this%20problem%2C%20we%20put%20forth%20a%20novel%0Aimmersion-aware%20model%20trading%20framework%20that%20incentivizes%20metaverse%20users%20%28MUs%29%0Ato%20contribute%20learning%20models%20trained%20by%20their%20latest%20local%20data%20for%20augmented%0Areality%20%28AR%29%20services%20in%20the%20vehicular%20metaverse%2C%20while%20preserving%20their%0Aprivacy%20through%20federated%20learning.%20To%20comprehensively%20evaluate%20the%0Acontribution%20of%20locally%20trained%20learning%20models%20provided%20by%20MUs%20to%20AR%20services%2C%0Awe%20design%20a%20new%20immersion%20metric%20that%20captures%20service%20immersion%20by%20considering%0Athe%20freshness%20and%20accuracy%20of%20learning%20models%2C%20as%20well%20as%20the%20amount%20and%0Apotential%20value%20of%20raw%20data%20used%20for%20training.%20We%20model%20the%20trading%0Ainteractions%20between%20metaverse%20service%20providers%20%28MSPs%29%20and%20MUs%20as%20an%0Aequilibrium%20problem%20with%20equilibrium%20constraints%20%28EPEC%29%20to%20analyze%20and%20balance%0Atheir%20costs%20and%20gains.%20Moreover%2C%20considering%20dynamic%20network%20conditions%20and%0Aprivacy%20concerns%2C%20we%20formulate%20the%20reward%20decisions%20of%20MSPs%20as%20a%20multi-agent%0AMarkov%20decision%20process.%20Then%2C%20a%20fully%20distributed%20dynamic%20reward%20method%20based%0Aon%20deep%20reinforcement%20learning%20is%20presented%2C%20which%20operates%20without%20any%20private%0Ainformation%20about%20MUs%20and%20other%20MSPs.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20framework%20can%20effectively%20provide%20higher-value%20models%20for%20object%0Adetection%20and%20classification%20in%20AR%20services%20on%20real%20AR-related%20vehicle%20datasets%0Acompared%20to%20benchmark%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaTrading%253A%2520An%2520Immersion-Aware%2520Model%2520Trading%2520Framework%2520for%2520Vehicular%250A%2520%2520Metaverse%2520Services%26entry.906535625%3DHongjia%2520Wu%2520and%2520Hui%2520Zeng%2520and%2520Zehui%2520Xiong%2520and%2520Jiawen%2520Kang%2520and%2520Zhiping%2520Cai%2520and%2520Tse-Tin%2520Chan%2520and%2520Dusit%2520Niyato%2520and%2520Zhu%2520Han%26entry.1292438233%3D%2520%2520Updates%2520of%2520extensive%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520data%2520are%2520critical%2520to%2520the%250Aimmersion%2520of%2520vehicular%2520metaverse%2520services.%2520However%252C%2520providing%2520high-quality%2520and%250Asustainable%2520data%2520in%2520unstable%2520and%2520resource-constrained%2520vehicular%2520networks%250Aremains%2520a%2520significant%2520challenge.%2520To%2520address%2520this%2520problem%252C%2520we%2520put%2520forth%2520a%2520novel%250Aimmersion-aware%2520model%2520trading%2520framework%2520that%2520incentivizes%2520metaverse%2520users%2520%2528MUs%2529%250Ato%2520contribute%2520learning%2520models%2520trained%2520by%2520their%2520latest%2520local%2520data%2520for%2520augmented%250Areality%2520%2528AR%2529%2520services%2520in%2520the%2520vehicular%2520metaverse%252C%2520while%2520preserving%2520their%250Aprivacy%2520through%2520federated%2520learning.%2520To%2520comprehensively%2520evaluate%2520the%250Acontribution%2520of%2520locally%2520trained%2520learning%2520models%2520provided%2520by%2520MUs%2520to%2520AR%2520services%252C%250Awe%2520design%2520a%2520new%2520immersion%2520metric%2520that%2520captures%2520service%2520immersion%2520by%2520considering%250Athe%2520freshness%2520and%2520accuracy%2520of%2520learning%2520models%252C%2520as%2520well%2520as%2520the%2520amount%2520and%250Apotential%2520value%2520of%2520raw%2520data%2520used%2520for%2520training.%2520We%2520model%2520the%2520trading%250Ainteractions%2520between%2520metaverse%2520service%2520providers%2520%2528MSPs%2529%2520and%2520MUs%2520as%2520an%250Aequilibrium%2520problem%2520with%2520equilibrium%2520constraints%2520%2528EPEC%2529%2520to%2520analyze%2520and%2520balance%250Atheir%2520costs%2520and%2520gains.%2520Moreover%252C%2520considering%2520dynamic%2520network%2520conditions%2520and%250Aprivacy%2520concerns%252C%2520we%2520formulate%2520the%2520reward%2520decisions%2520of%2520MSPs%2520as%2520a%2520multi-agent%250AMarkov%2520decision%2520process.%2520Then%252C%2520a%2520fully%2520distributed%2520dynamic%2520reward%2520method%2520based%250Aon%2520deep%2520reinforcement%2520learning%2520is%2520presented%252C%2520which%2520operates%2520without%2520any%2520private%250Ainformation%2520about%2520MUs%2520and%2520other%2520MSPs.%2520Experimental%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520framework%2520can%2520effectively%2520provide%2520higher-value%2520models%2520for%2520object%250Adetection%2520and%2520classification%2520in%2520AR%2520services%2520on%2520real%2520AR-related%2520vehicle%2520datasets%250Acompared%2520to%2520benchmark%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaTrading%3A%20An%20Immersion-Aware%20Model%20Trading%20Framework%20for%20Vehicular%0A%20%20Metaverse%20Services&entry.906535625=Hongjia%20Wu%20and%20Hui%20Zeng%20and%20Zehui%20Xiong%20and%20Jiawen%20Kang%20and%20Zhiping%20Cai%20and%20Tse-Tin%20Chan%20and%20Dusit%20Niyato%20and%20Zhu%20Han&entry.1292438233=%20%20Updates%20of%20extensive%20Internet%20of%20Things%20%28IoT%29%20data%20are%20critical%20to%20the%0Aimmersion%20of%20vehicular%20metaverse%20services.%20However%2C%20providing%20high-quality%20and%0Asustainable%20data%20in%20unstable%20and%20resource-constrained%20vehicular%20networks%0Aremains%20a%20significant%20challenge.%20To%20address%20this%20problem%2C%20we%20put%20forth%20a%20novel%0Aimmersion-aware%20model%20trading%20framework%20that%20incentivizes%20metaverse%20users%20%28MUs%29%0Ato%20contribute%20learning%20models%20trained%20by%20their%20latest%20local%20data%20for%20augmented%0Areality%20%28AR%29%20services%20in%20the%20vehicular%20metaverse%2C%20while%20preserving%20their%0Aprivacy%20through%20federated%20learning.%20To%20comprehensively%20evaluate%20the%0Acontribution%20of%20locally%20trained%20learning%20models%20provided%20by%20MUs%20to%20AR%20services%2C%0Awe%20design%20a%20new%20immersion%20metric%20that%20captures%20service%20immersion%20by%20considering%0Athe%20freshness%20and%20accuracy%20of%20learning%20models%2C%20as%20well%20as%20the%20amount%20and%0Apotential%20value%20of%20raw%20data%20used%20for%20training.%20We%20model%20the%20trading%0Ainteractions%20between%20metaverse%20service%20providers%20%28MSPs%29%20and%20MUs%20as%20an%0Aequilibrium%20problem%20with%20equilibrium%20constraints%20%28EPEC%29%20to%20analyze%20and%20balance%0Atheir%20costs%20and%20gains.%20Moreover%2C%20considering%20dynamic%20network%20conditions%20and%0Aprivacy%20concerns%2C%20we%20formulate%20the%20reward%20decisions%20of%20MSPs%20as%20a%20multi-agent%0AMarkov%20decision%20process.%20Then%2C%20a%20fully%20distributed%20dynamic%20reward%20method%20based%0Aon%20deep%20reinforcement%20learning%20is%20presented%2C%20which%20operates%20without%20any%20private%0Ainformation%20about%20MUs%20and%20other%20MSPs.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20framework%20can%20effectively%20provide%20higher-value%20models%20for%20object%0Adetection%20and%20classification%20in%20AR%20services%20on%20real%20AR-related%20vehicle%20datasets%0Acompared%20to%20benchmark%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19665v1&entry.124074799=Read"},
{"title": "On Occlusions in Video Action Detection: Benchmark Datasets And Training\n  Recipes", "author": "Rajat Modi and Vibhav Vineet and Yogesh Singh Rawat", "abstract": "  This paper explores the impact of occlusions in video action detection. We\nfacilitate this study by introducing five new benchmark datasets namely O-UCF\nand O-JHMDB consisting of synthetically controlled static/dynamic occlusions,\nOVIS-UCF and OVIS-JHMDB consisting of occlusions with realistic motions and\nReal-OUCF for occlusions in realistic-world scenarios. We formally confirm an\nintuitive expectation: existing models suffer a lot as occlusion severity is\nincreased and exhibit different behaviours when occluders are static vs when\nthey are moving. We discover several intriguing phenomenon emerging in neural\nnets: 1) transformers can naturally outperform CNN models which might have even\nused occlusion as a form of data augmentation during training 2) incorporating\nsymbolic-components like capsules to such backbones allows them to bind to\noccluders never even seen during training and 3) Islands of agreement can\nemerge in realistic images/videos without instance-level supervision,\ndistillation or contrastive-based objectives2(eg. video-textual training). Such\nemergent properties allow us to derive simple yet effective training recipes\nwhich lead to robust occlusion models inductively satisfying the first two\nstages of the binding mechanism (grouping/segregation). Models leveraging these\nrecipes outperform existing video action-detectors under occlusion by 32.3% on\nO-UCF, 32.7% on O-JHMDB & 2.6% on Real-OUCF in terms of the vMAP metric. The\ncode for this work has been released at\nhttps://github.com/rajatmodi62/OccludedActionBenchmark.\n", "link": "http://arxiv.org/abs/2410.19553v1", "date": "2024-10-25", "relevancy": 2.1366, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5426}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Occlusions%20in%20Video%20Action%20Detection%3A%20Benchmark%20Datasets%20And%20Training%0A%20%20Recipes&body=Title%3A%20On%20Occlusions%20in%20Video%20Action%20Detection%3A%20Benchmark%20Datasets%20And%20Training%0A%20%20Recipes%0AAuthor%3A%20Rajat%20Modi%20and%20Vibhav%20Vineet%20and%20Yogesh%20Singh%20Rawat%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20impact%20of%20occlusions%20in%20video%20action%20detection.%20We%0Afacilitate%20this%20study%20by%20introducing%20five%20new%20benchmark%20datasets%20namely%20O-UCF%0Aand%20O-JHMDB%20consisting%20of%20synthetically%20controlled%20static/dynamic%20occlusions%2C%0AOVIS-UCF%20and%20OVIS-JHMDB%20consisting%20of%20occlusions%20with%20realistic%20motions%20and%0AReal-OUCF%20for%20occlusions%20in%20realistic-world%20scenarios.%20We%20formally%20confirm%20an%0Aintuitive%20expectation%3A%20existing%20models%20suffer%20a%20lot%20as%20occlusion%20severity%20is%0Aincreased%20and%20exhibit%20different%20behaviours%20when%20occluders%20are%20static%20vs%20when%0Athey%20are%20moving.%20We%20discover%20several%20intriguing%20phenomenon%20emerging%20in%20neural%0Anets%3A%201%29%20transformers%20can%20naturally%20outperform%20CNN%20models%20which%20might%20have%20even%0Aused%20occlusion%20as%20a%20form%20of%20data%20augmentation%20during%20training%202%29%20incorporating%0Asymbolic-components%20like%20capsules%20to%20such%20backbones%20allows%20them%20to%20bind%20to%0Aoccluders%20never%20even%20seen%20during%20training%20and%203%29%20Islands%20of%20agreement%20can%0Aemerge%20in%20realistic%20images/videos%20without%20instance-level%20supervision%2C%0Adistillation%20or%20contrastive-based%20objectives2%28eg.%20video-textual%20training%29.%20Such%0Aemergent%20properties%20allow%20us%20to%20derive%20simple%20yet%20effective%20training%20recipes%0Awhich%20lead%20to%20robust%20occlusion%20models%20inductively%20satisfying%20the%20first%20two%0Astages%20of%20the%20binding%20mechanism%20%28grouping/segregation%29.%20Models%20leveraging%20these%0Arecipes%20outperform%20existing%20video%20action-detectors%20under%20occlusion%20by%2032.3%25%20on%0AO-UCF%2C%2032.7%25%20on%20O-JHMDB%20%26%202.6%25%20on%20Real-OUCF%20in%20terms%20of%20the%20vMAP%20metric.%20The%0Acode%20for%20this%20work%20has%20been%20released%20at%0Ahttps%3A//github.com/rajatmodi62/OccludedActionBenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Occlusions%2520in%2520Video%2520Action%2520Detection%253A%2520Benchmark%2520Datasets%2520And%2520Training%250A%2520%2520Recipes%26entry.906535625%3DRajat%2520Modi%2520and%2520Vibhav%2520Vineet%2520and%2520Yogesh%2520Singh%2520Rawat%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520impact%2520of%2520occlusions%2520in%2520video%2520action%2520detection.%2520We%250Afacilitate%2520this%2520study%2520by%2520introducing%2520five%2520new%2520benchmark%2520datasets%2520namely%2520O-UCF%250Aand%2520O-JHMDB%2520consisting%2520of%2520synthetically%2520controlled%2520static/dynamic%2520occlusions%252C%250AOVIS-UCF%2520and%2520OVIS-JHMDB%2520consisting%2520of%2520occlusions%2520with%2520realistic%2520motions%2520and%250AReal-OUCF%2520for%2520occlusions%2520in%2520realistic-world%2520scenarios.%2520We%2520formally%2520confirm%2520an%250Aintuitive%2520expectation%253A%2520existing%2520models%2520suffer%2520a%2520lot%2520as%2520occlusion%2520severity%2520is%250Aincreased%2520and%2520exhibit%2520different%2520behaviours%2520when%2520occluders%2520are%2520static%2520vs%2520when%250Athey%2520are%2520moving.%2520We%2520discover%2520several%2520intriguing%2520phenomenon%2520emerging%2520in%2520neural%250Anets%253A%25201%2529%2520transformers%2520can%2520naturally%2520outperform%2520CNN%2520models%2520which%2520might%2520have%2520even%250Aused%2520occlusion%2520as%2520a%2520form%2520of%2520data%2520augmentation%2520during%2520training%25202%2529%2520incorporating%250Asymbolic-components%2520like%2520capsules%2520to%2520such%2520backbones%2520allows%2520them%2520to%2520bind%2520to%250Aoccluders%2520never%2520even%2520seen%2520during%2520training%2520and%25203%2529%2520Islands%2520of%2520agreement%2520can%250Aemerge%2520in%2520realistic%2520images/videos%2520without%2520instance-level%2520supervision%252C%250Adistillation%2520or%2520contrastive-based%2520objectives2%2528eg.%2520video-textual%2520training%2529.%2520Such%250Aemergent%2520properties%2520allow%2520us%2520to%2520derive%2520simple%2520yet%2520effective%2520training%2520recipes%250Awhich%2520lead%2520to%2520robust%2520occlusion%2520models%2520inductively%2520satisfying%2520the%2520first%2520two%250Astages%2520of%2520the%2520binding%2520mechanism%2520%2528grouping/segregation%2529.%2520Models%2520leveraging%2520these%250Arecipes%2520outperform%2520existing%2520video%2520action-detectors%2520under%2520occlusion%2520by%252032.3%2525%2520on%250AO-UCF%252C%252032.7%2525%2520on%2520O-JHMDB%2520%2526%25202.6%2525%2520on%2520Real-OUCF%2520in%2520terms%2520of%2520the%2520vMAP%2520metric.%2520The%250Acode%2520for%2520this%2520work%2520has%2520been%2520released%2520at%250Ahttps%253A//github.com/rajatmodi62/OccludedActionBenchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Occlusions%20in%20Video%20Action%20Detection%3A%20Benchmark%20Datasets%20And%20Training%0A%20%20Recipes&entry.906535625=Rajat%20Modi%20and%20Vibhav%20Vineet%20and%20Yogesh%20Singh%20Rawat&entry.1292438233=%20%20This%20paper%20explores%20the%20impact%20of%20occlusions%20in%20video%20action%20detection.%20We%0Afacilitate%20this%20study%20by%20introducing%20five%20new%20benchmark%20datasets%20namely%20O-UCF%0Aand%20O-JHMDB%20consisting%20of%20synthetically%20controlled%20static/dynamic%20occlusions%2C%0AOVIS-UCF%20and%20OVIS-JHMDB%20consisting%20of%20occlusions%20with%20realistic%20motions%20and%0AReal-OUCF%20for%20occlusions%20in%20realistic-world%20scenarios.%20We%20formally%20confirm%20an%0Aintuitive%20expectation%3A%20existing%20models%20suffer%20a%20lot%20as%20occlusion%20severity%20is%0Aincreased%20and%20exhibit%20different%20behaviours%20when%20occluders%20are%20static%20vs%20when%0Athey%20are%20moving.%20We%20discover%20several%20intriguing%20phenomenon%20emerging%20in%20neural%0Anets%3A%201%29%20transformers%20can%20naturally%20outperform%20CNN%20models%20which%20might%20have%20even%0Aused%20occlusion%20as%20a%20form%20of%20data%20augmentation%20during%20training%202%29%20incorporating%0Asymbolic-components%20like%20capsules%20to%20such%20backbones%20allows%20them%20to%20bind%20to%0Aoccluders%20never%20even%20seen%20during%20training%20and%203%29%20Islands%20of%20agreement%20can%0Aemerge%20in%20realistic%20images/videos%20without%20instance-level%20supervision%2C%0Adistillation%20or%20contrastive-based%20objectives2%28eg.%20video-textual%20training%29.%20Such%0Aemergent%20properties%20allow%20us%20to%20derive%20simple%20yet%20effective%20training%20recipes%0Awhich%20lead%20to%20robust%20occlusion%20models%20inductively%20satisfying%20the%20first%20two%0Astages%20of%20the%20binding%20mechanism%20%28grouping/segregation%29.%20Models%20leveraging%20these%0Arecipes%20outperform%20existing%20video%20action-detectors%20under%20occlusion%20by%2032.3%25%20on%0AO-UCF%2C%2032.7%25%20on%20O-JHMDB%20%26%202.6%25%20on%20Real-OUCF%20in%20terms%20of%20the%20vMAP%20metric.%20The%0Acode%20for%20this%20work%20has%20been%20released%20at%0Ahttps%3A//github.com/rajatmodi62/OccludedActionBenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19553v1&entry.124074799=Read"},
{"title": "Semantic Segmentation in Satellite Hyperspectral Imagery by Deep\n  Learning", "author": "Jon Alvarez Justo and Alexandru Ghita and Daniel Kovac and Joseph L. Garrett and Mariana-Iuliana Georgescu and Jesus Gonzalez-Llorente and Radu Tudor Ionescu and Tor Arne Johansen", "abstract": "  Satellites are increasingly adopting on-board AI to optimize operations and\nincrease autonomy through in-orbit inference. The use of Deep Learning (DL)\nmodels for segmentation in hyperspectral imagery offers advantages for remote\nsensing applications. In this work, we train and test 20 models for multi-class\nsegmentation in hyperspectral imagery, selected for their potential in future\nspace deployment. These models include 1D and 2D Convolutional Neural Networks\n(CNNs) and the latest vision transformers (ViTs). We propose a lightweight\n1D-CNN model, 1D-Justo-LiuNet, which outperforms state-of-the-art models in the\nhypespectral domain. 1D-Justo-LiuNet exceeds the performance of 2D-CNN UNets\nand outperforms Apple's lightweight vision transformers designed for mobile\ninference. 1D-Justo-LiuNet achieves the highest accuracy (0.93) with the\nsmallest model size (4,563 parameters) among all tested models, while\nmaintaining fast inference. Unlike 2D-CNNs and ViTs, which encode both spectral\nand spatial information, 1D-Justo-LiuNet focuses solely on the rich spectral\nfeatures in hyperspectral data, benefitting from the high-dimensional feature\nspace. Our findings are validated across various satellite datasets, with the\nHYPSO-1 mission serving as the primary case study for sea, land, and cloud\nsegmentation. We further confirm our conclusions through generalization tests\non other hyperspectral missions, such as NASA's EO-1. Based on its superior\nperformance and compact size, we conclude that 1D-Justo-LiuNet is highly\nsuitable for in-orbit deployment, providing an effective solution for\noptimizing and automating satellite operations at edge.\n", "link": "http://arxiv.org/abs/2310.16210v4", "date": "2024-10-25", "relevancy": 2.1344, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Segmentation%20in%20Satellite%20Hyperspectral%20Imagery%20by%20Deep%0A%20%20Learning&body=Title%3A%20Semantic%20Segmentation%20in%20Satellite%20Hyperspectral%20Imagery%20by%20Deep%0A%20%20Learning%0AAuthor%3A%20Jon%20Alvarez%20Justo%20and%20Alexandru%20Ghita%20and%20Daniel%20Kovac%20and%20Joseph%20L.%20Garrett%20and%20Mariana-Iuliana%20Georgescu%20and%20Jesus%20Gonzalez-Llorente%20and%20Radu%20Tudor%20Ionescu%20and%20Tor%20Arne%20Johansen%0AAbstract%3A%20%20%20Satellites%20are%20increasingly%20adopting%20on-board%20AI%20to%20optimize%20operations%20and%0Aincrease%20autonomy%20through%20in-orbit%20inference.%20The%20use%20of%20Deep%20Learning%20%28DL%29%0Amodels%20for%20segmentation%20in%20hyperspectral%20imagery%20offers%20advantages%20for%20remote%0Asensing%20applications.%20In%20this%20work%2C%20we%20train%20and%20test%2020%20models%20for%20multi-class%0Asegmentation%20in%20hyperspectral%20imagery%2C%20selected%20for%20their%20potential%20in%20future%0Aspace%20deployment.%20These%20models%20include%201D%20and%202D%20Convolutional%20Neural%20Networks%0A%28CNNs%29%20and%20the%20latest%20vision%20transformers%20%28ViTs%29.%20We%20propose%20a%20lightweight%0A1D-CNN%20model%2C%201D-Justo-LiuNet%2C%20which%20outperforms%20state-of-the-art%20models%20in%20the%0Ahypespectral%20domain.%201D-Justo-LiuNet%20exceeds%20the%20performance%20of%202D-CNN%20UNets%0Aand%20outperforms%20Apple%27s%20lightweight%20vision%20transformers%20designed%20for%20mobile%0Ainference.%201D-Justo-LiuNet%20achieves%20the%20highest%20accuracy%20%280.93%29%20with%20the%0Asmallest%20model%20size%20%284%2C563%20parameters%29%20among%20all%20tested%20models%2C%20while%0Amaintaining%20fast%20inference.%20Unlike%202D-CNNs%20and%20ViTs%2C%20which%20encode%20both%20spectral%0Aand%20spatial%20information%2C%201D-Justo-LiuNet%20focuses%20solely%20on%20the%20rich%20spectral%0Afeatures%20in%20hyperspectral%20data%2C%20benefitting%20from%20the%20high-dimensional%20feature%0Aspace.%20Our%20findings%20are%20validated%20across%20various%20satellite%20datasets%2C%20with%20the%0AHYPSO-1%20mission%20serving%20as%20the%20primary%20case%20study%20for%20sea%2C%20land%2C%20and%20cloud%0Asegmentation.%20We%20further%20confirm%20our%20conclusions%20through%20generalization%20tests%0Aon%20other%20hyperspectral%20missions%2C%20such%20as%20NASA%27s%20EO-1.%20Based%20on%20its%20superior%0Aperformance%20and%20compact%20size%2C%20we%20conclude%20that%201D-Justo-LiuNet%20is%20highly%0Asuitable%20for%20in-orbit%20deployment%2C%20providing%20an%20effective%20solution%20for%0Aoptimizing%20and%20automating%20satellite%20operations%20at%20edge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16210v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Segmentation%2520in%2520Satellite%2520Hyperspectral%2520Imagery%2520by%2520Deep%250A%2520%2520Learning%26entry.906535625%3DJon%2520Alvarez%2520Justo%2520and%2520Alexandru%2520Ghita%2520and%2520Daniel%2520Kovac%2520and%2520Joseph%2520L.%2520Garrett%2520and%2520Mariana-Iuliana%2520Georgescu%2520and%2520Jesus%2520Gonzalez-Llorente%2520and%2520Radu%2520Tudor%2520Ionescu%2520and%2520Tor%2520Arne%2520Johansen%26entry.1292438233%3D%2520%2520Satellites%2520are%2520increasingly%2520adopting%2520on-board%2520AI%2520to%2520optimize%2520operations%2520and%250Aincrease%2520autonomy%2520through%2520in-orbit%2520inference.%2520The%2520use%2520of%2520Deep%2520Learning%2520%2528DL%2529%250Amodels%2520for%2520segmentation%2520in%2520hyperspectral%2520imagery%2520offers%2520advantages%2520for%2520remote%250Asensing%2520applications.%2520In%2520this%2520work%252C%2520we%2520train%2520and%2520test%252020%2520models%2520for%2520multi-class%250Asegmentation%2520in%2520hyperspectral%2520imagery%252C%2520selected%2520for%2520their%2520potential%2520in%2520future%250Aspace%2520deployment.%2520These%2520models%2520include%25201D%2520and%25202D%2520Convolutional%2520Neural%2520Networks%250A%2528CNNs%2529%2520and%2520the%2520latest%2520vision%2520transformers%2520%2528ViTs%2529.%2520We%2520propose%2520a%2520lightweight%250A1D-CNN%2520model%252C%25201D-Justo-LiuNet%252C%2520which%2520outperforms%2520state-of-the-art%2520models%2520in%2520the%250Ahypespectral%2520domain.%25201D-Justo-LiuNet%2520exceeds%2520the%2520performance%2520of%25202D-CNN%2520UNets%250Aand%2520outperforms%2520Apple%2527s%2520lightweight%2520vision%2520transformers%2520designed%2520for%2520mobile%250Ainference.%25201D-Justo-LiuNet%2520achieves%2520the%2520highest%2520accuracy%2520%25280.93%2529%2520with%2520the%250Asmallest%2520model%2520size%2520%25284%252C563%2520parameters%2529%2520among%2520all%2520tested%2520models%252C%2520while%250Amaintaining%2520fast%2520inference.%2520Unlike%25202D-CNNs%2520and%2520ViTs%252C%2520which%2520encode%2520both%2520spectral%250Aand%2520spatial%2520information%252C%25201D-Justo-LiuNet%2520focuses%2520solely%2520on%2520the%2520rich%2520spectral%250Afeatures%2520in%2520hyperspectral%2520data%252C%2520benefitting%2520from%2520the%2520high-dimensional%2520feature%250Aspace.%2520Our%2520findings%2520are%2520validated%2520across%2520various%2520satellite%2520datasets%252C%2520with%2520the%250AHYPSO-1%2520mission%2520serving%2520as%2520the%2520primary%2520case%2520study%2520for%2520sea%252C%2520land%252C%2520and%2520cloud%250Asegmentation.%2520We%2520further%2520confirm%2520our%2520conclusions%2520through%2520generalization%2520tests%250Aon%2520other%2520hyperspectral%2520missions%252C%2520such%2520as%2520NASA%2527s%2520EO-1.%2520Based%2520on%2520its%2520superior%250Aperformance%2520and%2520compact%2520size%252C%2520we%2520conclude%2520that%25201D-Justo-LiuNet%2520is%2520highly%250Asuitable%2520for%2520in-orbit%2520deployment%252C%2520providing%2520an%2520effective%2520solution%2520for%250Aoptimizing%2520and%2520automating%2520satellite%2520operations%2520at%2520edge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.16210v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Segmentation%20in%20Satellite%20Hyperspectral%20Imagery%20by%20Deep%0A%20%20Learning&entry.906535625=Jon%20Alvarez%20Justo%20and%20Alexandru%20Ghita%20and%20Daniel%20Kovac%20and%20Joseph%20L.%20Garrett%20and%20Mariana-Iuliana%20Georgescu%20and%20Jesus%20Gonzalez-Llorente%20and%20Radu%20Tudor%20Ionescu%20and%20Tor%20Arne%20Johansen&entry.1292438233=%20%20Satellites%20are%20increasingly%20adopting%20on-board%20AI%20to%20optimize%20operations%20and%0Aincrease%20autonomy%20through%20in-orbit%20inference.%20The%20use%20of%20Deep%20Learning%20%28DL%29%0Amodels%20for%20segmentation%20in%20hyperspectral%20imagery%20offers%20advantages%20for%20remote%0Asensing%20applications.%20In%20this%20work%2C%20we%20train%20and%20test%2020%20models%20for%20multi-class%0Asegmentation%20in%20hyperspectral%20imagery%2C%20selected%20for%20their%20potential%20in%20future%0Aspace%20deployment.%20These%20models%20include%201D%20and%202D%20Convolutional%20Neural%20Networks%0A%28CNNs%29%20and%20the%20latest%20vision%20transformers%20%28ViTs%29.%20We%20propose%20a%20lightweight%0A1D-CNN%20model%2C%201D-Justo-LiuNet%2C%20which%20outperforms%20state-of-the-art%20models%20in%20the%0Ahypespectral%20domain.%201D-Justo-LiuNet%20exceeds%20the%20performance%20of%202D-CNN%20UNets%0Aand%20outperforms%20Apple%27s%20lightweight%20vision%20transformers%20designed%20for%20mobile%0Ainference.%201D-Justo-LiuNet%20achieves%20the%20highest%20accuracy%20%280.93%29%20with%20the%0Asmallest%20model%20size%20%284%2C563%20parameters%29%20among%20all%20tested%20models%2C%20while%0Amaintaining%20fast%20inference.%20Unlike%202D-CNNs%20and%20ViTs%2C%20which%20encode%20both%20spectral%0Aand%20spatial%20information%2C%201D-Justo-LiuNet%20focuses%20solely%20on%20the%20rich%20spectral%0Afeatures%20in%20hyperspectral%20data%2C%20benefitting%20from%20the%20high-dimensional%20feature%0Aspace.%20Our%20findings%20are%20validated%20across%20various%20satellite%20datasets%2C%20with%20the%0AHYPSO-1%20mission%20serving%20as%20the%20primary%20case%20study%20for%20sea%2C%20land%2C%20and%20cloud%0Asegmentation.%20We%20further%20confirm%20our%20conclusions%20through%20generalization%20tests%0Aon%20other%20hyperspectral%20missions%2C%20such%20as%20NASA%27s%20EO-1.%20Based%20on%20its%20superior%0Aperformance%20and%20compact%20size%2C%20we%20conclude%20that%201D-Justo-LiuNet%20is%20highly%0Asuitable%20for%20in-orbit%20deployment%2C%20providing%20an%20effective%20solution%20for%0Aoptimizing%20and%20automating%20satellite%20operations%20at%20edge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16210v4&entry.124074799=Read"},
{"title": "Content-Aware Radiance Fields: Aligning Model Complexity with Scene\n  Intricacy Through Learned Bitwidth Quantization", "author": "Weihang Liu and Xue Xian Zheng and Jingyi Yu and Xin Lou", "abstract": "  The recent popular radiance field models, exemplified by Neural Radiance\nFields (NeRF), Instant-NGP and 3D Gaussian Splat?ting, are designed to\nrepresent 3D content by that training models for each individual scene. This\nunique characteristic of scene representation and per-scene training\ndistinguishes radiance field models from other neural models, because complex\nscenes necessitate models with higher representational capacity and vice versa.\nIn this paper, we propose content?aware radiance fields, aligning the model\ncomplexity with the scene intricacies through Adversarial Content-Aware\nQuantization (A-CAQ). Specifically, we make the bitwidth of parameters\ndifferentiable and train?able, tailored to the unique characteristics of\nspecific scenes and requirements. The proposed framework has been assessed on\nInstant-NGP, a well-known NeRF variant and evaluated using various datasets.\nExperimental results demonstrate a notable reduction in computational\ncomplexity, while preserving the requisite reconstruction and rendering\nquality, making it beneficial for practical deployment of radiance fields\nmodels. Codes are available at\nhttps://github.com/WeihangLiu2024/Content_Aware_NeRF.\n", "link": "http://arxiv.org/abs/2410.19483v1", "date": "2024-10-25", "relevancy": 2.1341, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5338}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5338}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Content-Aware%20Radiance%20Fields%3A%20Aligning%20Model%20Complexity%20with%20Scene%0A%20%20Intricacy%20Through%20Learned%20Bitwidth%20Quantization&body=Title%3A%20Content-Aware%20Radiance%20Fields%3A%20Aligning%20Model%20Complexity%20with%20Scene%0A%20%20Intricacy%20Through%20Learned%20Bitwidth%20Quantization%0AAuthor%3A%20Weihang%20Liu%20and%20Xue%20Xian%20Zheng%20and%20Jingyi%20Yu%20and%20Xin%20Lou%0AAbstract%3A%20%20%20The%20recent%20popular%20radiance%20field%20models%2C%20exemplified%20by%20Neural%20Radiance%0AFields%20%28NeRF%29%2C%20Instant-NGP%20and%203D%20Gaussian%20Splat%3Fting%2C%20are%20designed%20to%0Arepresent%203D%20content%20by%20that%20training%20models%20for%20each%20individual%20scene.%20This%0Aunique%20characteristic%20of%20scene%20representation%20and%20per-scene%20training%0Adistinguishes%20radiance%20field%20models%20from%20other%20neural%20models%2C%20because%20complex%0Ascenes%20necessitate%20models%20with%20higher%20representational%20capacity%20and%20vice%20versa.%0AIn%20this%20paper%2C%20we%20propose%20content%3Faware%20radiance%20fields%2C%20aligning%20the%20model%0Acomplexity%20with%20the%20scene%20intricacies%20through%20Adversarial%20Content-Aware%0AQuantization%20%28A-CAQ%29.%20Specifically%2C%20we%20make%20the%20bitwidth%20of%20parameters%0Adifferentiable%20and%20train%3Fable%2C%20tailored%20to%20the%20unique%20characteristics%20of%0Aspecific%20scenes%20and%20requirements.%20The%20proposed%20framework%20has%20been%20assessed%20on%0AInstant-NGP%2C%20a%20well-known%20NeRF%20variant%20and%20evaluated%20using%20various%20datasets.%0AExperimental%20results%20demonstrate%20a%20notable%20reduction%20in%20computational%0Acomplexity%2C%20while%20preserving%20the%20requisite%20reconstruction%20and%20rendering%0Aquality%2C%20making%20it%20beneficial%20for%20practical%20deployment%20of%20radiance%20fields%0Amodels.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/WeihangLiu2024/Content_Aware_NeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContent-Aware%2520Radiance%2520Fields%253A%2520Aligning%2520Model%2520Complexity%2520with%2520Scene%250A%2520%2520Intricacy%2520Through%2520Learned%2520Bitwidth%2520Quantization%26entry.906535625%3DWeihang%2520Liu%2520and%2520Xue%2520Xian%2520Zheng%2520and%2520Jingyi%2520Yu%2520and%2520Xin%2520Lou%26entry.1292438233%3D%2520%2520The%2520recent%2520popular%2520radiance%2520field%2520models%252C%2520exemplified%2520by%2520Neural%2520Radiance%250AFields%2520%2528NeRF%2529%252C%2520Instant-NGP%2520and%25203D%2520Gaussian%2520Splat%253Fting%252C%2520are%2520designed%2520to%250Arepresent%25203D%2520content%2520by%2520that%2520training%2520models%2520for%2520each%2520individual%2520scene.%2520This%250Aunique%2520characteristic%2520of%2520scene%2520representation%2520and%2520per-scene%2520training%250Adistinguishes%2520radiance%2520field%2520models%2520from%2520other%2520neural%2520models%252C%2520because%2520complex%250Ascenes%2520necessitate%2520models%2520with%2520higher%2520representational%2520capacity%2520and%2520vice%2520versa.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520content%253Faware%2520radiance%2520fields%252C%2520aligning%2520the%2520model%250Acomplexity%2520with%2520the%2520scene%2520intricacies%2520through%2520Adversarial%2520Content-Aware%250AQuantization%2520%2528A-CAQ%2529.%2520Specifically%252C%2520we%2520make%2520the%2520bitwidth%2520of%2520parameters%250Adifferentiable%2520and%2520train%253Fable%252C%2520tailored%2520to%2520the%2520unique%2520characteristics%2520of%250Aspecific%2520scenes%2520and%2520requirements.%2520The%2520proposed%2520framework%2520has%2520been%2520assessed%2520on%250AInstant-NGP%252C%2520a%2520well-known%2520NeRF%2520variant%2520and%2520evaluated%2520using%2520various%2520datasets.%250AExperimental%2520results%2520demonstrate%2520a%2520notable%2520reduction%2520in%2520computational%250Acomplexity%252C%2520while%2520preserving%2520the%2520requisite%2520reconstruction%2520and%2520rendering%250Aquality%252C%2520making%2520it%2520beneficial%2520for%2520practical%2520deployment%2520of%2520radiance%2520fields%250Amodels.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/WeihangLiu2024/Content_Aware_NeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Content-Aware%20Radiance%20Fields%3A%20Aligning%20Model%20Complexity%20with%20Scene%0A%20%20Intricacy%20Through%20Learned%20Bitwidth%20Quantization&entry.906535625=Weihang%20Liu%20and%20Xue%20Xian%20Zheng%20and%20Jingyi%20Yu%20and%20Xin%20Lou&entry.1292438233=%20%20The%20recent%20popular%20radiance%20field%20models%2C%20exemplified%20by%20Neural%20Radiance%0AFields%20%28NeRF%29%2C%20Instant-NGP%20and%203D%20Gaussian%20Splat%3Fting%2C%20are%20designed%20to%0Arepresent%203D%20content%20by%20that%20training%20models%20for%20each%20individual%20scene.%20This%0Aunique%20characteristic%20of%20scene%20representation%20and%20per-scene%20training%0Adistinguishes%20radiance%20field%20models%20from%20other%20neural%20models%2C%20because%20complex%0Ascenes%20necessitate%20models%20with%20higher%20representational%20capacity%20and%20vice%20versa.%0AIn%20this%20paper%2C%20we%20propose%20content%3Faware%20radiance%20fields%2C%20aligning%20the%20model%0Acomplexity%20with%20the%20scene%20intricacies%20through%20Adversarial%20Content-Aware%0AQuantization%20%28A-CAQ%29.%20Specifically%2C%20we%20make%20the%20bitwidth%20of%20parameters%0Adifferentiable%20and%20train%3Fable%2C%20tailored%20to%20the%20unique%20characteristics%20of%0Aspecific%20scenes%20and%20requirements.%20The%20proposed%20framework%20has%20been%20assessed%20on%0AInstant-NGP%2C%20a%20well-known%20NeRF%20variant%20and%20evaluated%20using%20various%20datasets.%0AExperimental%20results%20demonstrate%20a%20notable%20reduction%20in%20computational%0Acomplexity%2C%20while%20preserving%20the%20requisite%20reconstruction%20and%20rendering%0Aquality%2C%20making%20it%20beneficial%20for%20practical%20deployment%20of%20radiance%20fields%0Amodels.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/WeihangLiu2024/Content_Aware_NeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19483v1&entry.124074799=Read"},
{"title": "Temporal Convolution-based Hybrid Model Approach with Representation\n  Learning for Real-Time Acoustic Anomaly Detection", "author": "Sahan Dissanayaka and Manjusri Wickramasinghe and Pasindu Marasinghe", "abstract": "  The early detection of potential failures in industrial machinery components\nis paramount for ensuring the reliability and safety of operations, thereby\npreserving Machine Condition Monitoring (MCM). This research addresses this\nimperative by introducing an innovative approach to Real-Time Acoustic Anomaly\nDetection. Our method combines semi-supervised temporal convolution with\nrepresentation learning and a hybrid model strategy with Temporal Convolutional\nNetworks (TCN) to handle various intricate anomaly patterns found in acoustic\ndata effectively. The proposed model demonstrates superior performance compared\nto established research in the field, underscoring the effectiveness of this\napproach. Not only do we present quantitative evidence of its superiority, but\nwe also employ visual representations, such as t-SNE plots, to further\nsubstantiate the model's efficacy.\n", "link": "http://arxiv.org/abs/2410.19722v1", "date": "2024-10-25", "relevancy": 2.1145, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5414}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Convolution-based%20Hybrid%20Model%20Approach%20with%20Representation%0A%20%20Learning%20for%20Real-Time%20Acoustic%20Anomaly%20Detection&body=Title%3A%20Temporal%20Convolution-based%20Hybrid%20Model%20Approach%20with%20Representation%0A%20%20Learning%20for%20Real-Time%20Acoustic%20Anomaly%20Detection%0AAuthor%3A%20Sahan%20Dissanayaka%20and%20Manjusri%20Wickramasinghe%20and%20Pasindu%20Marasinghe%0AAbstract%3A%20%20%20The%20early%20detection%20of%20potential%20failures%20in%20industrial%20machinery%20components%0Ais%20paramount%20for%20ensuring%20the%20reliability%20and%20safety%20of%20operations%2C%20thereby%0Apreserving%20Machine%20Condition%20Monitoring%20%28MCM%29.%20This%20research%20addresses%20this%0Aimperative%20by%20introducing%20an%20innovative%20approach%20to%20Real-Time%20Acoustic%20Anomaly%0ADetection.%20Our%20method%20combines%20semi-supervised%20temporal%20convolution%20with%0Arepresentation%20learning%20and%20a%20hybrid%20model%20strategy%20with%20Temporal%20Convolutional%0ANetworks%20%28TCN%29%20to%20handle%20various%20intricate%20anomaly%20patterns%20found%20in%20acoustic%0Adata%20effectively.%20The%20proposed%20model%20demonstrates%20superior%20performance%20compared%0Ato%20established%20research%20in%20the%20field%2C%20underscoring%20the%20effectiveness%20of%20this%0Aapproach.%20Not%20only%20do%20we%20present%20quantitative%20evidence%20of%20its%20superiority%2C%20but%0Awe%20also%20employ%20visual%20representations%2C%20such%20as%20t-SNE%20plots%2C%20to%20further%0Asubstantiate%20the%20model%27s%20efficacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Convolution-based%2520Hybrid%2520Model%2520Approach%2520with%2520Representation%250A%2520%2520Learning%2520for%2520Real-Time%2520Acoustic%2520Anomaly%2520Detection%26entry.906535625%3DSahan%2520Dissanayaka%2520and%2520Manjusri%2520Wickramasinghe%2520and%2520Pasindu%2520Marasinghe%26entry.1292438233%3D%2520%2520The%2520early%2520detection%2520of%2520potential%2520failures%2520in%2520industrial%2520machinery%2520components%250Ais%2520paramount%2520for%2520ensuring%2520the%2520reliability%2520and%2520safety%2520of%2520operations%252C%2520thereby%250Apreserving%2520Machine%2520Condition%2520Monitoring%2520%2528MCM%2529.%2520This%2520research%2520addresses%2520this%250Aimperative%2520by%2520introducing%2520an%2520innovative%2520approach%2520to%2520Real-Time%2520Acoustic%2520Anomaly%250ADetection.%2520Our%2520method%2520combines%2520semi-supervised%2520temporal%2520convolution%2520with%250Arepresentation%2520learning%2520and%2520a%2520hybrid%2520model%2520strategy%2520with%2520Temporal%2520Convolutional%250ANetworks%2520%2528TCN%2529%2520to%2520handle%2520various%2520intricate%2520anomaly%2520patterns%2520found%2520in%2520acoustic%250Adata%2520effectively.%2520The%2520proposed%2520model%2520demonstrates%2520superior%2520performance%2520compared%250Ato%2520established%2520research%2520in%2520the%2520field%252C%2520underscoring%2520the%2520effectiveness%2520of%2520this%250Aapproach.%2520Not%2520only%2520do%2520we%2520present%2520quantitative%2520evidence%2520of%2520its%2520superiority%252C%2520but%250Awe%2520also%2520employ%2520visual%2520representations%252C%2520such%2520as%2520t-SNE%2520plots%252C%2520to%2520further%250Asubstantiate%2520the%2520model%2527s%2520efficacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Convolution-based%20Hybrid%20Model%20Approach%20with%20Representation%0A%20%20Learning%20for%20Real-Time%20Acoustic%20Anomaly%20Detection&entry.906535625=Sahan%20Dissanayaka%20and%20Manjusri%20Wickramasinghe%20and%20Pasindu%20Marasinghe&entry.1292438233=%20%20The%20early%20detection%20of%20potential%20failures%20in%20industrial%20machinery%20components%0Ais%20paramount%20for%20ensuring%20the%20reliability%20and%20safety%20of%20operations%2C%20thereby%0Apreserving%20Machine%20Condition%20Monitoring%20%28MCM%29.%20This%20research%20addresses%20this%0Aimperative%20by%20introducing%20an%20innovative%20approach%20to%20Real-Time%20Acoustic%20Anomaly%0ADetection.%20Our%20method%20combines%20semi-supervised%20temporal%20convolution%20with%0Arepresentation%20learning%20and%20a%20hybrid%20model%20strategy%20with%20Temporal%20Convolutional%0ANetworks%20%28TCN%29%20to%20handle%20various%20intricate%20anomaly%20patterns%20found%20in%20acoustic%0Adata%20effectively.%20The%20proposed%20model%20demonstrates%20superior%20performance%20compared%0Ato%20established%20research%20in%20the%20field%2C%20underscoring%20the%20effectiveness%20of%20this%0Aapproach.%20Not%20only%20do%20we%20present%20quantitative%20evidence%20of%20its%20superiority%2C%20but%0Awe%20also%20employ%20visual%20representations%2C%20such%20as%20t-SNE%20plots%2C%20to%20further%0Asubstantiate%20the%20model%27s%20efficacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19722v1&entry.124074799=Read"},
{"title": "Selective Test-Time Adaptation for Unsupervised Anomaly Detection using\n  Neural Implicit Representations", "author": "Sameer Ambekar and Julia A. Schnabel and Cosmin I. Bercea", "abstract": "  Deep learning models in medical imaging often encounter challenges when\nadapting to new clinical settings unseen during training. Test-time adaptation\noffers a promising approach to optimize models for these unseen domains, yet\nits application in anomaly detection (AD) remains largely unexplored. AD aims\nto efficiently identify deviations from normative distributions; however, full\nadaptation, including pathological shifts, may inadvertently learn the\nanomalies it intends to detect. We introduce a novel concept of selective\ntest-time adaptation that utilizes the inherent characteristics of deep\npre-trained features to adapt selectively in a zero-shot manner to any test\nimage from an unseen domain. This approach employs a model-agnostic,\nlightweight multi-layer perceptron for neural implicit representations,\nenabling the adaptation of outputs from any reconstruction-based AD method\nwithout altering the source-trained model. Rigorous validation in brain AD\ndemonstrated that our strategy substantially enhances detection accuracy for\nmultiple conditions and different target distributions. Specifically, our\nmethod improves the detection rates by up to 78% for enlarged ventricles and\n24% for edemas.\n", "link": "http://arxiv.org/abs/2410.03306v2", "date": "2024-10-25", "relevancy": 2.1022, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5326}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5287}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Test-Time%20Adaptation%20for%20Unsupervised%20Anomaly%20Detection%20using%0A%20%20Neural%20Implicit%20Representations&body=Title%3A%20Selective%20Test-Time%20Adaptation%20for%20Unsupervised%20Anomaly%20Detection%20using%0A%20%20Neural%20Implicit%20Representations%0AAuthor%3A%20Sameer%20Ambekar%20and%20Julia%20A.%20Schnabel%20and%20Cosmin%20I.%20Bercea%0AAbstract%3A%20%20%20Deep%20learning%20models%20in%20medical%20imaging%20often%20encounter%20challenges%20when%0Aadapting%20to%20new%20clinical%20settings%20unseen%20during%20training.%20Test-time%20adaptation%0Aoffers%20a%20promising%20approach%20to%20optimize%20models%20for%20these%20unseen%20domains%2C%20yet%0Aits%20application%20in%20anomaly%20detection%20%28AD%29%20remains%20largely%20unexplored.%20AD%20aims%0Ato%20efficiently%20identify%20deviations%20from%20normative%20distributions%3B%20however%2C%20full%0Aadaptation%2C%20including%20pathological%20shifts%2C%20may%20inadvertently%20learn%20the%0Aanomalies%20it%20intends%20to%20detect.%20We%20introduce%20a%20novel%20concept%20of%20selective%0Atest-time%20adaptation%20that%20utilizes%20the%20inherent%20characteristics%20of%20deep%0Apre-trained%20features%20to%20adapt%20selectively%20in%20a%20zero-shot%20manner%20to%20any%20test%0Aimage%20from%20an%20unseen%20domain.%20This%20approach%20employs%20a%20model-agnostic%2C%0Alightweight%20multi-layer%20perceptron%20for%20neural%20implicit%20representations%2C%0Aenabling%20the%20adaptation%20of%20outputs%20from%20any%20reconstruction-based%20AD%20method%0Awithout%20altering%20the%20source-trained%20model.%20Rigorous%20validation%20in%20brain%20AD%0Ademonstrated%20that%20our%20strategy%20substantially%20enhances%20detection%20accuracy%20for%0Amultiple%20conditions%20and%20different%20target%20distributions.%20Specifically%2C%20our%0Amethod%20improves%20the%20detection%20rates%20by%20up%20to%2078%25%20for%20enlarged%20ventricles%20and%0A24%25%20for%20edemas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Test-Time%2520Adaptation%2520for%2520Unsupervised%2520Anomaly%2520Detection%2520using%250A%2520%2520Neural%2520Implicit%2520Representations%26entry.906535625%3DSameer%2520Ambekar%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Cosmin%2520I.%2520Bercea%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520in%2520medical%2520imaging%2520often%2520encounter%2520challenges%2520when%250Aadapting%2520to%2520new%2520clinical%2520settings%2520unseen%2520during%2520training.%2520Test-time%2520adaptation%250Aoffers%2520a%2520promising%2520approach%2520to%2520optimize%2520models%2520for%2520these%2520unseen%2520domains%252C%2520yet%250Aits%2520application%2520in%2520anomaly%2520detection%2520%2528AD%2529%2520remains%2520largely%2520unexplored.%2520AD%2520aims%250Ato%2520efficiently%2520identify%2520deviations%2520from%2520normative%2520distributions%253B%2520however%252C%2520full%250Aadaptation%252C%2520including%2520pathological%2520shifts%252C%2520may%2520inadvertently%2520learn%2520the%250Aanomalies%2520it%2520intends%2520to%2520detect.%2520We%2520introduce%2520a%2520novel%2520concept%2520of%2520selective%250Atest-time%2520adaptation%2520that%2520utilizes%2520the%2520inherent%2520characteristics%2520of%2520deep%250Apre-trained%2520features%2520to%2520adapt%2520selectively%2520in%2520a%2520zero-shot%2520manner%2520to%2520any%2520test%250Aimage%2520from%2520an%2520unseen%2520domain.%2520This%2520approach%2520employs%2520a%2520model-agnostic%252C%250Alightweight%2520multi-layer%2520perceptron%2520for%2520neural%2520implicit%2520representations%252C%250Aenabling%2520the%2520adaptation%2520of%2520outputs%2520from%2520any%2520reconstruction-based%2520AD%2520method%250Awithout%2520altering%2520the%2520source-trained%2520model.%2520Rigorous%2520validation%2520in%2520brain%2520AD%250Ademonstrated%2520that%2520our%2520strategy%2520substantially%2520enhances%2520detection%2520accuracy%2520for%250Amultiple%2520conditions%2520and%2520different%2520target%2520distributions.%2520Specifically%252C%2520our%250Amethod%2520improves%2520the%2520detection%2520rates%2520by%2520up%2520to%252078%2525%2520for%2520enlarged%2520ventricles%2520and%250A24%2525%2520for%2520edemas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Test-Time%20Adaptation%20for%20Unsupervised%20Anomaly%20Detection%20using%0A%20%20Neural%20Implicit%20Representations&entry.906535625=Sameer%20Ambekar%20and%20Julia%20A.%20Schnabel%20and%20Cosmin%20I.%20Bercea&entry.1292438233=%20%20Deep%20learning%20models%20in%20medical%20imaging%20often%20encounter%20challenges%20when%0Aadapting%20to%20new%20clinical%20settings%20unseen%20during%20training.%20Test-time%20adaptation%0Aoffers%20a%20promising%20approach%20to%20optimize%20models%20for%20these%20unseen%20domains%2C%20yet%0Aits%20application%20in%20anomaly%20detection%20%28AD%29%20remains%20largely%20unexplored.%20AD%20aims%0Ato%20efficiently%20identify%20deviations%20from%20normative%20distributions%3B%20however%2C%20full%0Aadaptation%2C%20including%20pathological%20shifts%2C%20may%20inadvertently%20learn%20the%0Aanomalies%20it%20intends%20to%20detect.%20We%20introduce%20a%20novel%20concept%20of%20selective%0Atest-time%20adaptation%20that%20utilizes%20the%20inherent%20characteristics%20of%20deep%0Apre-trained%20features%20to%20adapt%20selectively%20in%20a%20zero-shot%20manner%20to%20any%20test%0Aimage%20from%20an%20unseen%20domain.%20This%20approach%20employs%20a%20model-agnostic%2C%0Alightweight%20multi-layer%20perceptron%20for%20neural%20implicit%20representations%2C%0Aenabling%20the%20adaptation%20of%20outputs%20from%20any%20reconstruction-based%20AD%20method%0Awithout%20altering%20the%20source-trained%20model.%20Rigorous%20validation%20in%20brain%20AD%0Ademonstrated%20that%20our%20strategy%20substantially%20enhances%20detection%20accuracy%20for%0Amultiple%20conditions%20and%20different%20target%20distributions.%20Specifically%2C%20our%0Amethod%20improves%20the%20detection%20rates%20by%20up%20to%2078%25%20for%20enlarged%20ventricles%20and%0A24%25%20for%20edemas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03306v2&entry.124074799=Read"},
{"title": "LFME: A Simple Framework for Learning from Multiple Experts in Domain\n  Generalization", "author": "Liang Chen and Yong Zhang and Yibing Song and Zhiqiang Shen and Lingqiao Liu", "abstract": "  Domain generalization (DG) methods aim to maintain good performance in an\nunseen target domain by using training data from multiple source domains. While\nsuccess on certain occasions are observed, enhancing the baseline across most\nscenarios remains challenging. This work introduces a simple yet effective\nframework, dubbed learning from multiple experts (LFME), that aims to make the\ntarget model an expert in all source domains to improve DG. Specifically,\nbesides learning the target model used in inference, LFME will also train\nmultiple experts specialized in different domains, whose output probabilities\nprovide professional guidance by simply regularizing the logit of the target\nmodel. Delving deep into the framework, we reveal that the introduced logit\nregularization term implicitly provides effects of enabling the target model to\nharness more information, and mining hard samples from the experts during\ntraining. Extensive experiments on benchmarks from different DG tasks\ndemonstrate that LFME is consistently beneficial to the baseline and can\nachieve comparable performance to existing arts. Code is available\nat~\\url{https://github.com/liangchen527/LFME}.\n", "link": "http://arxiv.org/abs/2410.17020v2", "date": "2024-10-25", "relevancy": 2.0945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LFME%3A%20A%20Simple%20Framework%20for%20Learning%20from%20Multiple%20Experts%20in%20Domain%0A%20%20Generalization&body=Title%3A%20LFME%3A%20A%20Simple%20Framework%20for%20Learning%20from%20Multiple%20Experts%20in%20Domain%0A%20%20Generalization%0AAuthor%3A%20Liang%20Chen%20and%20Yong%20Zhang%20and%20Yibing%20Song%20and%20Zhiqiang%20Shen%20and%20Lingqiao%20Liu%0AAbstract%3A%20%20%20Domain%20generalization%20%28DG%29%20methods%20aim%20to%20maintain%20good%20performance%20in%20an%0Aunseen%20target%20domain%20by%20using%20training%20data%20from%20multiple%20source%20domains.%20While%0Asuccess%20on%20certain%20occasions%20are%20observed%2C%20enhancing%20the%20baseline%20across%20most%0Ascenarios%20remains%20challenging.%20This%20work%20introduces%20a%20simple%20yet%20effective%0Aframework%2C%20dubbed%20learning%20from%20multiple%20experts%20%28LFME%29%2C%20that%20aims%20to%20make%20the%0Atarget%20model%20an%20expert%20in%20all%20source%20domains%20to%20improve%20DG.%20Specifically%2C%0Abesides%20learning%20the%20target%20model%20used%20in%20inference%2C%20LFME%20will%20also%20train%0Amultiple%20experts%20specialized%20in%20different%20domains%2C%20whose%20output%20probabilities%0Aprovide%20professional%20guidance%20by%20simply%20regularizing%20the%20logit%20of%20the%20target%0Amodel.%20Delving%20deep%20into%20the%20framework%2C%20we%20reveal%20that%20the%20introduced%20logit%0Aregularization%20term%20implicitly%20provides%20effects%20of%20enabling%20the%20target%20model%20to%0Aharness%20more%20information%2C%20and%20mining%20hard%20samples%20from%20the%20experts%20during%0Atraining.%20Extensive%20experiments%20on%20benchmarks%20from%20different%20DG%20tasks%0Ademonstrate%20that%20LFME%20is%20consistently%20beneficial%20to%20the%20baseline%20and%20can%0Aachieve%20comparable%20performance%20to%20existing%20arts.%20Code%20is%20available%0Aat~%5Curl%7Bhttps%3A//github.com/liangchen527/LFME%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLFME%253A%2520A%2520Simple%2520Framework%2520for%2520Learning%2520from%2520Multiple%2520Experts%2520in%2520Domain%250A%2520%2520Generalization%26entry.906535625%3DLiang%2520Chen%2520and%2520Yong%2520Zhang%2520and%2520Yibing%2520Song%2520and%2520Zhiqiang%2520Shen%2520and%2520Lingqiao%2520Liu%26entry.1292438233%3D%2520%2520Domain%2520generalization%2520%2528DG%2529%2520methods%2520aim%2520to%2520maintain%2520good%2520performance%2520in%2520an%250Aunseen%2520target%2520domain%2520by%2520using%2520training%2520data%2520from%2520multiple%2520source%2520domains.%2520While%250Asuccess%2520on%2520certain%2520occasions%2520are%2520observed%252C%2520enhancing%2520the%2520baseline%2520across%2520most%250Ascenarios%2520remains%2520challenging.%2520This%2520work%2520introduces%2520a%2520simple%2520yet%2520effective%250Aframework%252C%2520dubbed%2520learning%2520from%2520multiple%2520experts%2520%2528LFME%2529%252C%2520that%2520aims%2520to%2520make%2520the%250Atarget%2520model%2520an%2520expert%2520in%2520all%2520source%2520domains%2520to%2520improve%2520DG.%2520Specifically%252C%250Abesides%2520learning%2520the%2520target%2520model%2520used%2520in%2520inference%252C%2520LFME%2520will%2520also%2520train%250Amultiple%2520experts%2520specialized%2520in%2520different%2520domains%252C%2520whose%2520output%2520probabilities%250Aprovide%2520professional%2520guidance%2520by%2520simply%2520regularizing%2520the%2520logit%2520of%2520the%2520target%250Amodel.%2520Delving%2520deep%2520into%2520the%2520framework%252C%2520we%2520reveal%2520that%2520the%2520introduced%2520logit%250Aregularization%2520term%2520implicitly%2520provides%2520effects%2520of%2520enabling%2520the%2520target%2520model%2520to%250Aharness%2520more%2520information%252C%2520and%2520mining%2520hard%2520samples%2520from%2520the%2520experts%2520during%250Atraining.%2520Extensive%2520experiments%2520on%2520benchmarks%2520from%2520different%2520DG%2520tasks%250Ademonstrate%2520that%2520LFME%2520is%2520consistently%2520beneficial%2520to%2520the%2520baseline%2520and%2520can%250Aachieve%2520comparable%2520performance%2520to%2520existing%2520arts.%2520Code%2520is%2520available%250Aat~%255Curl%257Bhttps%253A//github.com/liangchen527/LFME%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LFME%3A%20A%20Simple%20Framework%20for%20Learning%20from%20Multiple%20Experts%20in%20Domain%0A%20%20Generalization&entry.906535625=Liang%20Chen%20and%20Yong%20Zhang%20and%20Yibing%20Song%20and%20Zhiqiang%20Shen%20and%20Lingqiao%20Liu&entry.1292438233=%20%20Domain%20generalization%20%28DG%29%20methods%20aim%20to%20maintain%20good%20performance%20in%20an%0Aunseen%20target%20domain%20by%20using%20training%20data%20from%20multiple%20source%20domains.%20While%0Asuccess%20on%20certain%20occasions%20are%20observed%2C%20enhancing%20the%20baseline%20across%20most%0Ascenarios%20remains%20challenging.%20This%20work%20introduces%20a%20simple%20yet%20effective%0Aframework%2C%20dubbed%20learning%20from%20multiple%20experts%20%28LFME%29%2C%20that%20aims%20to%20make%20the%0Atarget%20model%20an%20expert%20in%20all%20source%20domains%20to%20improve%20DG.%20Specifically%2C%0Abesides%20learning%20the%20target%20model%20used%20in%20inference%2C%20LFME%20will%20also%20train%0Amultiple%20experts%20specialized%20in%20different%20domains%2C%20whose%20output%20probabilities%0Aprovide%20professional%20guidance%20by%20simply%20regularizing%20the%20logit%20of%20the%20target%0Amodel.%20Delving%20deep%20into%20the%20framework%2C%20we%20reveal%20that%20the%20introduced%20logit%0Aregularization%20term%20implicitly%20provides%20effects%20of%20enabling%20the%20target%20model%20to%0Aharness%20more%20information%2C%20and%20mining%20hard%20samples%20from%20the%20experts%20during%0Atraining.%20Extensive%20experiments%20on%20benchmarks%20from%20different%20DG%20tasks%0Ademonstrate%20that%20LFME%20is%20consistently%20beneficial%20to%20the%20baseline%20and%20can%0Aachieve%20comparable%20performance%20to%20existing%20arts.%20Code%20is%20available%0Aat~%5Curl%7Bhttps%3A//github.com/liangchen527/LFME%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17020v2&entry.124074799=Read"},
{"title": "Few Exemplar-Based General Medical Image Segmentation via Domain-Aware\n  Selective Adaptation", "author": "Chen Xu and Qiming Huang and Yuqi Hou and Jiangxing Wu and Fan Zhang and Hyung Jin Chang and Jianbo Jiao", "abstract": "  Medical image segmentation poses challenges due to domain gaps, data modality\nvariations, and dependency on domain knowledge or experts, especially for low-\nand middle-income countries (LMICs). Whereas for humans, given a few exemplars\n(with corresponding labels), we are able to segment different medical images\neven without exten-sive domain-specific clinical training. In addition, current\nSAM-based medical segmentation models use fine-grained visual prompts, such as\nthe bounding rectangle generated from manually annotated target segmentation\nmask, as the bounding box (bbox) prompt during the testing phase. However, in\nactual clinical scenarios, no such precise prior knowledge is available. Our\nexperimental results also reveal that previous models nearly fail to predict\nwhen given coarser bbox prompts. Considering these issues, in this paper, we\nintroduce a domain-aware selective adaptation approach to adapt the general\nknowledge learned from a large model trained with natural images to the\ncorresponding medical domains/modalities, with access to only a few (e.g. less\nthan 5) exemplars. Our method mitigates the aforementioned limitations,\nproviding an efficient and LMICs-friendly solution. Extensive experimental\nanalysis showcases the effectiveness of our approach, offering potential\nadvancements in healthcare diagnostics and clinical applications in LMICs.\n", "link": "http://arxiv.org/abs/2410.09254v2", "date": "2024-10-25", "relevancy": 2.0833, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5307}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5173}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few%20Exemplar-Based%20General%20Medical%20Image%20Segmentation%20via%20Domain-Aware%0A%20%20Selective%20Adaptation&body=Title%3A%20Few%20Exemplar-Based%20General%20Medical%20Image%20Segmentation%20via%20Domain-Aware%0A%20%20Selective%20Adaptation%0AAuthor%3A%20Chen%20Xu%20and%20Qiming%20Huang%20and%20Yuqi%20Hou%20and%20Jiangxing%20Wu%20and%20Fan%20Zhang%20and%20Hyung%20Jin%20Chang%20and%20Jianbo%20Jiao%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20poses%20challenges%20due%20to%20domain%20gaps%2C%20data%20modality%0Avariations%2C%20and%20dependency%20on%20domain%20knowledge%20or%20experts%2C%20especially%20for%20low-%0Aand%20middle-income%20countries%20%28LMICs%29.%20Whereas%20for%20humans%2C%20given%20a%20few%20exemplars%0A%28with%20corresponding%20labels%29%2C%20we%20are%20able%20to%20segment%20different%20medical%20images%0Aeven%20without%20exten-sive%20domain-specific%20clinical%20training.%20In%20addition%2C%20current%0ASAM-based%20medical%20segmentation%20models%20use%20fine-grained%20visual%20prompts%2C%20such%20as%0Athe%20bounding%20rectangle%20generated%20from%20manually%20annotated%20target%20segmentation%0Amask%2C%20as%20the%20bounding%20box%20%28bbox%29%20prompt%20during%20the%20testing%20phase.%20However%2C%20in%0Aactual%20clinical%20scenarios%2C%20no%20such%20precise%20prior%20knowledge%20is%20available.%20Our%0Aexperimental%20results%20also%20reveal%20that%20previous%20models%20nearly%20fail%20to%20predict%0Awhen%20given%20coarser%20bbox%20prompts.%20Considering%20these%20issues%2C%20in%20this%20paper%2C%20we%0Aintroduce%20a%20domain-aware%20selective%20adaptation%20approach%20to%20adapt%20the%20general%0Aknowledge%20learned%20from%20a%20large%20model%20trained%20with%20natural%20images%20to%20the%0Acorresponding%20medical%20domains/modalities%2C%20with%20access%20to%20only%20a%20few%20%28e.g.%20less%0Athan%205%29%20exemplars.%20Our%20method%20mitigates%20the%20aforementioned%20limitations%2C%0Aproviding%20an%20efficient%20and%20LMICs-friendly%20solution.%20Extensive%20experimental%0Aanalysis%20showcases%20the%20effectiveness%20of%20our%20approach%2C%20offering%20potential%0Aadvancements%20in%20healthcare%20diagnostics%20and%20clinical%20applications%20in%20LMICs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew%2520Exemplar-Based%2520General%2520Medical%2520Image%2520Segmentation%2520via%2520Domain-Aware%250A%2520%2520Selective%2520Adaptation%26entry.906535625%3DChen%2520Xu%2520and%2520Qiming%2520Huang%2520and%2520Yuqi%2520Hou%2520and%2520Jiangxing%2520Wu%2520and%2520Fan%2520Zhang%2520and%2520Hyung%2520Jin%2520Chang%2520and%2520Jianbo%2520Jiao%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520poses%2520challenges%2520due%2520to%2520domain%2520gaps%252C%2520data%2520modality%250Avariations%252C%2520and%2520dependency%2520on%2520domain%2520knowledge%2520or%2520experts%252C%2520especially%2520for%2520low-%250Aand%2520middle-income%2520countries%2520%2528LMICs%2529.%2520Whereas%2520for%2520humans%252C%2520given%2520a%2520few%2520exemplars%250A%2528with%2520corresponding%2520labels%2529%252C%2520we%2520are%2520able%2520to%2520segment%2520different%2520medical%2520images%250Aeven%2520without%2520exten-sive%2520domain-specific%2520clinical%2520training.%2520In%2520addition%252C%2520current%250ASAM-based%2520medical%2520segmentation%2520models%2520use%2520fine-grained%2520visual%2520prompts%252C%2520such%2520as%250Athe%2520bounding%2520rectangle%2520generated%2520from%2520manually%2520annotated%2520target%2520segmentation%250Amask%252C%2520as%2520the%2520bounding%2520box%2520%2528bbox%2529%2520prompt%2520during%2520the%2520testing%2520phase.%2520However%252C%2520in%250Aactual%2520clinical%2520scenarios%252C%2520no%2520such%2520precise%2520prior%2520knowledge%2520is%2520available.%2520Our%250Aexperimental%2520results%2520also%2520reveal%2520that%2520previous%2520models%2520nearly%2520fail%2520to%2520predict%250Awhen%2520given%2520coarser%2520bbox%2520prompts.%2520Considering%2520these%2520issues%252C%2520in%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520domain-aware%2520selective%2520adaptation%2520approach%2520to%2520adapt%2520the%2520general%250Aknowledge%2520learned%2520from%2520a%2520large%2520model%2520trained%2520with%2520natural%2520images%2520to%2520the%250Acorresponding%2520medical%2520domains/modalities%252C%2520with%2520access%2520to%2520only%2520a%2520few%2520%2528e.g.%2520less%250Athan%25205%2529%2520exemplars.%2520Our%2520method%2520mitigates%2520the%2520aforementioned%2520limitations%252C%250Aproviding%2520an%2520efficient%2520and%2520LMICs-friendly%2520solution.%2520Extensive%2520experimental%250Aanalysis%2520showcases%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520offering%2520potential%250Aadvancements%2520in%2520healthcare%2520diagnostics%2520and%2520clinical%2520applications%2520in%2520LMICs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few%20Exemplar-Based%20General%20Medical%20Image%20Segmentation%20via%20Domain-Aware%0A%20%20Selective%20Adaptation&entry.906535625=Chen%20Xu%20and%20Qiming%20Huang%20and%20Yuqi%20Hou%20and%20Jiangxing%20Wu%20and%20Fan%20Zhang%20and%20Hyung%20Jin%20Chang%20and%20Jianbo%20Jiao&entry.1292438233=%20%20Medical%20image%20segmentation%20poses%20challenges%20due%20to%20domain%20gaps%2C%20data%20modality%0Avariations%2C%20and%20dependency%20on%20domain%20knowledge%20or%20experts%2C%20especially%20for%20low-%0Aand%20middle-income%20countries%20%28LMICs%29.%20Whereas%20for%20humans%2C%20given%20a%20few%20exemplars%0A%28with%20corresponding%20labels%29%2C%20we%20are%20able%20to%20segment%20different%20medical%20images%0Aeven%20without%20exten-sive%20domain-specific%20clinical%20training.%20In%20addition%2C%20current%0ASAM-based%20medical%20segmentation%20models%20use%20fine-grained%20visual%20prompts%2C%20such%20as%0Athe%20bounding%20rectangle%20generated%20from%20manually%20annotated%20target%20segmentation%0Amask%2C%20as%20the%20bounding%20box%20%28bbox%29%20prompt%20during%20the%20testing%20phase.%20However%2C%20in%0Aactual%20clinical%20scenarios%2C%20no%20such%20precise%20prior%20knowledge%20is%20available.%20Our%0Aexperimental%20results%20also%20reveal%20that%20previous%20models%20nearly%20fail%20to%20predict%0Awhen%20given%20coarser%20bbox%20prompts.%20Considering%20these%20issues%2C%20in%20this%20paper%2C%20we%0Aintroduce%20a%20domain-aware%20selective%20adaptation%20approach%20to%20adapt%20the%20general%0Aknowledge%20learned%20from%20a%20large%20model%20trained%20with%20natural%20images%20to%20the%0Acorresponding%20medical%20domains/modalities%2C%20with%20access%20to%20only%20a%20few%20%28e.g.%20less%0Athan%205%29%20exemplars.%20Our%20method%20mitigates%20the%20aforementioned%20limitations%2C%0Aproviding%20an%20efficient%20and%20LMICs-friendly%20solution.%20Extensive%20experimental%0Aanalysis%20showcases%20the%20effectiveness%20of%20our%20approach%2C%20offering%20potential%0Aadvancements%20in%20healthcare%20diagnostics%20and%20clinical%20applications%20in%20LMICs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09254v2&entry.124074799=Read"},
{"title": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models", "author": "Christos Xypolopoulos and Guokan Shang and Xiao Fei and Giannis Nikolentzos and Hadi Abdine and Iakovos Evdaimon and Michail Chatzianastasis and Giorgos Stamou and Michalis Vazirgiannis", "abstract": "  Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph machine learning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term graph linearization, so that LLMs can handle graphs naturally.\nWe consider that graphs should be linearized meaningfully to reflect certain\nproperties of natural language text, such as local dependency and global\nalignment, in order to ease contemporary LLMs, trained on trillions of textual\ntokens, better understand graphs. To achieve this, we developed several graph\nlinearization methods based on graph centrality, degeneracy, and node\nrelabeling schemes. We then investigated their effect on LLM performance in\ngraph reasoning tasks. Experimental results on synthetic graphs demonstrate the\neffectiveness of our methods compared to random linearization baselines. Our\nwork introduces novel graph representations suitable for LLMs, contributing to\nthe potential integration of graph machine learning with the trend of\nmulti-modal processing using a unified transformer model.\n", "link": "http://arxiv.org/abs/2410.19494v1", "date": "2024-10-25", "relevancy": 2.0749, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5345}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5147}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Linearization%20Methods%20for%20Reasoning%20on%20Graphs%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20Graph%20Linearization%20Methods%20for%20Reasoning%20on%20Graphs%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Christos%20Xypolopoulos%20and%20Guokan%20Shang%20and%20Xiao%20Fei%20and%20Giannis%20Nikolentzos%20and%20Hadi%20Abdine%20and%20Iakovos%20Evdaimon%20and%20Michail%20Chatzianastasis%20and%20Giorgos%20Stamou%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Large%20language%20models%20have%20evolved%20to%20process%20multiple%20modalities%20beyond%0Atext%2C%20such%20as%20images%20and%20audio%2C%20which%20motivates%20us%20to%20explore%20how%20to%0Aeffectively%20leverage%20them%20for%20graph%20machine%20learning%20tasks.%20The%20key%20question%2C%0Atherefore%2C%20is%20how%20to%20transform%20graphs%20into%20linear%20sequences%20of%20tokens%2C%20a%0Aprocess%20we%20term%20graph%20linearization%2C%20so%20that%20LLMs%20can%20handle%20graphs%20naturally.%0AWe%20consider%20that%20graphs%20should%20be%20linearized%20meaningfully%20to%20reflect%20certain%0Aproperties%20of%20natural%20language%20text%2C%20such%20as%20local%20dependency%20and%20global%0Aalignment%2C%20in%20order%20to%20ease%20contemporary%20LLMs%2C%20trained%20on%20trillions%20of%20textual%0Atokens%2C%20better%20understand%20graphs.%20To%20achieve%20this%2C%20we%20developed%20several%20graph%0Alinearization%20methods%20based%20on%20graph%20centrality%2C%20degeneracy%2C%20and%20node%0Arelabeling%20schemes.%20We%20then%20investigated%20their%20effect%20on%20LLM%20performance%20in%0Agraph%20reasoning%20tasks.%20Experimental%20results%20on%20synthetic%20graphs%20demonstrate%20the%0Aeffectiveness%20of%20our%20methods%20compared%20to%20random%20linearization%20baselines.%20Our%0Awork%20introduces%20novel%20graph%20representations%20suitable%20for%20LLMs%2C%20contributing%20to%0Athe%20potential%20integration%20of%20graph%20machine%20learning%20with%20the%20trend%20of%0Amulti-modal%20processing%20using%20a%20unified%20transformer%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Linearization%2520Methods%2520for%2520Reasoning%2520on%2520Graphs%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DChristos%2520Xypolopoulos%2520and%2520Guokan%2520Shang%2520and%2520Xiao%2520Fei%2520and%2520Giannis%2520Nikolentzos%2520and%2520Hadi%2520Abdine%2520and%2520Iakovos%2520Evdaimon%2520and%2520Michail%2520Chatzianastasis%2520and%2520Giorgos%2520Stamou%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520evolved%2520to%2520process%2520multiple%2520modalities%2520beyond%250Atext%252C%2520such%2520as%2520images%2520and%2520audio%252C%2520which%2520motivates%2520us%2520to%2520explore%2520how%2520to%250Aeffectively%2520leverage%2520them%2520for%2520graph%2520machine%2520learning%2520tasks.%2520The%2520key%2520question%252C%250Atherefore%252C%2520is%2520how%2520to%2520transform%2520graphs%2520into%2520linear%2520sequences%2520of%2520tokens%252C%2520a%250Aprocess%2520we%2520term%2520graph%2520linearization%252C%2520so%2520that%2520LLMs%2520can%2520handle%2520graphs%2520naturally.%250AWe%2520consider%2520that%2520graphs%2520should%2520be%2520linearized%2520meaningfully%2520to%2520reflect%2520certain%250Aproperties%2520of%2520natural%2520language%2520text%252C%2520such%2520as%2520local%2520dependency%2520and%2520global%250Aalignment%252C%2520in%2520order%2520to%2520ease%2520contemporary%2520LLMs%252C%2520trained%2520on%2520trillions%2520of%2520textual%250Atokens%252C%2520better%2520understand%2520graphs.%2520To%2520achieve%2520this%252C%2520we%2520developed%2520several%2520graph%250Alinearization%2520methods%2520based%2520on%2520graph%2520centrality%252C%2520degeneracy%252C%2520and%2520node%250Arelabeling%2520schemes.%2520We%2520then%2520investigated%2520their%2520effect%2520on%2520LLM%2520performance%2520in%250Agraph%2520reasoning%2520tasks.%2520Experimental%2520results%2520on%2520synthetic%2520graphs%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520methods%2520compared%2520to%2520random%2520linearization%2520baselines.%2520Our%250Awork%2520introduces%2520novel%2520graph%2520representations%2520suitable%2520for%2520LLMs%252C%2520contributing%2520to%250Athe%2520potential%2520integration%2520of%2520graph%2520machine%2520learning%2520with%2520the%2520trend%2520of%250Amulti-modal%2520processing%2520using%2520a%2520unified%2520transformer%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Linearization%20Methods%20for%20Reasoning%20on%20Graphs%20with%20Large%20Language%0A%20%20Models&entry.906535625=Christos%20Xypolopoulos%20and%20Guokan%20Shang%20and%20Xiao%20Fei%20and%20Giannis%20Nikolentzos%20and%20Hadi%20Abdine%20and%20Iakovos%20Evdaimon%20and%20Michail%20Chatzianastasis%20and%20Giorgos%20Stamou%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Large%20language%20models%20have%20evolved%20to%20process%20multiple%20modalities%20beyond%0Atext%2C%20such%20as%20images%20and%20audio%2C%20which%20motivates%20us%20to%20explore%20how%20to%0Aeffectively%20leverage%20them%20for%20graph%20machine%20learning%20tasks.%20The%20key%20question%2C%0Atherefore%2C%20is%20how%20to%20transform%20graphs%20into%20linear%20sequences%20of%20tokens%2C%20a%0Aprocess%20we%20term%20graph%20linearization%2C%20so%20that%20LLMs%20can%20handle%20graphs%20naturally.%0AWe%20consider%20that%20graphs%20should%20be%20linearized%20meaningfully%20to%20reflect%20certain%0Aproperties%20of%20natural%20language%20text%2C%20such%20as%20local%20dependency%20and%20global%0Aalignment%2C%20in%20order%20to%20ease%20contemporary%20LLMs%2C%20trained%20on%20trillions%20of%20textual%0Atokens%2C%20better%20understand%20graphs.%20To%20achieve%20this%2C%20we%20developed%20several%20graph%0Alinearization%20methods%20based%20on%20graph%20centrality%2C%20degeneracy%2C%20and%20node%0Arelabeling%20schemes.%20We%20then%20investigated%20their%20effect%20on%20LLM%20performance%20in%0Agraph%20reasoning%20tasks.%20Experimental%20results%20on%20synthetic%20graphs%20demonstrate%20the%0Aeffectiveness%20of%20our%20methods%20compared%20to%20random%20linearization%20baselines.%20Our%0Awork%20introduces%20novel%20graph%20representations%20suitable%20for%20LLMs%2C%20contributing%20to%0Athe%20potential%20integration%20of%20graph%20machine%20learning%20with%20the%20trend%20of%0Amulti-modal%20processing%20using%20a%20unified%20transformer%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19494v1&entry.124074799=Read"},
{"title": "Bootstrapping Reinforcement Learning with Imitation for Vision-Based\n  Agile Flight", "author": "Jiaxu Xing and Angel Romero and Leonard Bauersfeld and Davide Scaramuzza", "abstract": "  Learning visuomotor policies for agile quadrotor flight presents significant\ndifficulties, primarily from inefficient policy exploration caused by\nhigh-dimensional visual inputs and the need for precise and low-latency\ncontrol. To address these challenges, we propose a novel approach that combines\nthe performance of Reinforcement Learning (RL) and the sample efficiency of\nImitation Learning (IL) in the task of vision-based autonomous drone racing.\nWhile RL provides a framework for learning high-performance controllers through\ntrial and error, it faces challenges with sample efficiency and computational\ndemands due to the high dimensionality of visual inputs. Conversely, IL\nefficiently learns from visual expert demonstrations, but it remains limited by\nthe expert's performance and state distribution. To overcome these limitations,\nour policy learning framework integrates the strengths of both approaches. Our\nframework contains three phases: training a teacher policy using RL with\nprivileged state information, distilling it into a student policy via IL, and\nadaptive fine-tuning via RL. Testing in both simulated and real-world scenarios\nshows our approach can not only learn in scenarios where RL from scratch fails\nbut also outperforms existing IL methods in both robustness and performance,\nsuccessfully navigating a quadrotor through a race course using only visual\ninformation.\n", "link": "http://arxiv.org/abs/2403.12203v2", "date": "2024-10-25", "relevancy": 2.0738, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5167}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20Reinforcement%20Learning%20with%20Imitation%20for%20Vision-Based%0A%20%20Agile%20Flight&body=Title%3A%20Bootstrapping%20Reinforcement%20Learning%20with%20Imitation%20for%20Vision-Based%0A%20%20Agile%20Flight%0AAuthor%3A%20Jiaxu%20Xing%20and%20Angel%20Romero%20and%20Leonard%20Bauersfeld%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Learning%20visuomotor%20policies%20for%20agile%20quadrotor%20flight%20presents%20significant%0Adifficulties%2C%20primarily%20from%20inefficient%20policy%20exploration%20caused%20by%0Ahigh-dimensional%20visual%20inputs%20and%20the%20need%20for%20precise%20and%20low-latency%0Acontrol.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20that%20combines%0Athe%20performance%20of%20Reinforcement%20Learning%20%28RL%29%20and%20the%20sample%20efficiency%20of%0AImitation%20Learning%20%28IL%29%20in%20the%20task%20of%20vision-based%20autonomous%20drone%20racing.%0AWhile%20RL%20provides%20a%20framework%20for%20learning%20high-performance%20controllers%20through%0Atrial%20and%20error%2C%20it%20faces%20challenges%20with%20sample%20efficiency%20and%20computational%0Ademands%20due%20to%20the%20high%20dimensionality%20of%20visual%20inputs.%20Conversely%2C%20IL%0Aefficiently%20learns%20from%20visual%20expert%20demonstrations%2C%20but%20it%20remains%20limited%20by%0Athe%20expert%27s%20performance%20and%20state%20distribution.%20To%20overcome%20these%20limitations%2C%0Aour%20policy%20learning%20framework%20integrates%20the%20strengths%20of%20both%20approaches.%20Our%0Aframework%20contains%20three%20phases%3A%20training%20a%20teacher%20policy%20using%20RL%20with%0Aprivileged%20state%20information%2C%20distilling%20it%20into%20a%20student%20policy%20via%20IL%2C%20and%0Aadaptive%20fine-tuning%20via%20RL.%20Testing%20in%20both%20simulated%20and%20real-world%20scenarios%0Ashows%20our%20approach%20can%20not%20only%20learn%20in%20scenarios%20where%20RL%20from%20scratch%20fails%0Abut%20also%20outperforms%20existing%20IL%20methods%20in%20both%20robustness%20and%20performance%2C%0Asuccessfully%20navigating%20a%20quadrotor%20through%20a%20race%20course%20using%20only%20visual%0Ainformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12203v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrapping%2520Reinforcement%2520Learning%2520with%2520Imitation%2520for%2520Vision-Based%250A%2520%2520Agile%2520Flight%26entry.906535625%3DJiaxu%2520Xing%2520and%2520Angel%2520Romero%2520and%2520Leonard%2520Bauersfeld%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Learning%2520visuomotor%2520policies%2520for%2520agile%2520quadrotor%2520flight%2520presents%2520significant%250Adifficulties%252C%2520primarily%2520from%2520inefficient%2520policy%2520exploration%2520caused%2520by%250Ahigh-dimensional%2520visual%2520inputs%2520and%2520the%2520need%2520for%2520precise%2520and%2520low-latency%250Acontrol.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520combines%250Athe%2520performance%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529%2520and%2520the%2520sample%2520efficiency%2520of%250AImitation%2520Learning%2520%2528IL%2529%2520in%2520the%2520task%2520of%2520vision-based%2520autonomous%2520drone%2520racing.%250AWhile%2520RL%2520provides%2520a%2520framework%2520for%2520learning%2520high-performance%2520controllers%2520through%250Atrial%2520and%2520error%252C%2520it%2520faces%2520challenges%2520with%2520sample%2520efficiency%2520and%2520computational%250Ademands%2520due%2520to%2520the%2520high%2520dimensionality%2520of%2520visual%2520inputs.%2520Conversely%252C%2520IL%250Aefficiently%2520learns%2520from%2520visual%2520expert%2520demonstrations%252C%2520but%2520it%2520remains%2520limited%2520by%250Athe%2520expert%2527s%2520performance%2520and%2520state%2520distribution.%2520To%2520overcome%2520these%2520limitations%252C%250Aour%2520policy%2520learning%2520framework%2520integrates%2520the%2520strengths%2520of%2520both%2520approaches.%2520Our%250Aframework%2520contains%2520three%2520phases%253A%2520training%2520a%2520teacher%2520policy%2520using%2520RL%2520with%250Aprivileged%2520state%2520information%252C%2520distilling%2520it%2520into%2520a%2520student%2520policy%2520via%2520IL%252C%2520and%250Aadaptive%2520fine-tuning%2520via%2520RL.%2520Testing%2520in%2520both%2520simulated%2520and%2520real-world%2520scenarios%250Ashows%2520our%2520approach%2520can%2520not%2520only%2520learn%2520in%2520scenarios%2520where%2520RL%2520from%2520scratch%2520fails%250Abut%2520also%2520outperforms%2520existing%2520IL%2520methods%2520in%2520both%2520robustness%2520and%2520performance%252C%250Asuccessfully%2520navigating%2520a%2520quadrotor%2520through%2520a%2520race%2520course%2520using%2520only%2520visual%250Ainformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12203v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20Reinforcement%20Learning%20with%20Imitation%20for%20Vision-Based%0A%20%20Agile%20Flight&entry.906535625=Jiaxu%20Xing%20and%20Angel%20Romero%20and%20Leonard%20Bauersfeld%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Learning%20visuomotor%20policies%20for%20agile%20quadrotor%20flight%20presents%20significant%0Adifficulties%2C%20primarily%20from%20inefficient%20policy%20exploration%20caused%20by%0Ahigh-dimensional%20visual%20inputs%20and%20the%20need%20for%20precise%20and%20low-latency%0Acontrol.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20that%20combines%0Athe%20performance%20of%20Reinforcement%20Learning%20%28RL%29%20and%20the%20sample%20efficiency%20of%0AImitation%20Learning%20%28IL%29%20in%20the%20task%20of%20vision-based%20autonomous%20drone%20racing.%0AWhile%20RL%20provides%20a%20framework%20for%20learning%20high-performance%20controllers%20through%0Atrial%20and%20error%2C%20it%20faces%20challenges%20with%20sample%20efficiency%20and%20computational%0Ademands%20due%20to%20the%20high%20dimensionality%20of%20visual%20inputs.%20Conversely%2C%20IL%0Aefficiently%20learns%20from%20visual%20expert%20demonstrations%2C%20but%20it%20remains%20limited%20by%0Athe%20expert%27s%20performance%20and%20state%20distribution.%20To%20overcome%20these%20limitations%2C%0Aour%20policy%20learning%20framework%20integrates%20the%20strengths%20of%20both%20approaches.%20Our%0Aframework%20contains%20three%20phases%3A%20training%20a%20teacher%20policy%20using%20RL%20with%0Aprivileged%20state%20information%2C%20distilling%20it%20into%20a%20student%20policy%20via%20IL%2C%20and%0Aadaptive%20fine-tuning%20via%20RL.%20Testing%20in%20both%20simulated%20and%20real-world%20scenarios%0Ashows%20our%20approach%20can%20not%20only%20learn%20in%20scenarios%20where%20RL%20from%20scratch%20fails%0Abut%20also%20outperforms%20existing%20IL%20methods%20in%20both%20robustness%20and%20performance%2C%0Asuccessfully%20navigating%20a%20quadrotor%20through%20a%20race%20course%20using%20only%20visual%0Ainformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12203v2&entry.124074799=Read"},
{"title": "Soft Finger Grasp Force and Contact State Estimation from Tactile\n  Sensors", "author": "Hun Jang and Joonbum Bae and Kevin Haninger", "abstract": "  Soft robotic fingers can improve adaptability in grasping and manipulation,\ncompensating for geometric variation in object or environmental contact, but\ntoday lack force capacity and fine dexterity. Integrated tactile sensors can\nprovide grasp and task information which can improve dexterity,but should\nideally not require object-specific training. The total force vector exerted by\na finger provides general information to the internal grasp forces (e.g. for\ngrasp stability) and, when summed over fingers, an estimate of the external\nforce acting on the grasped object (e.g. for task-level control). In this\nstudy, we investigate the efficacy of estimating finger force from integrated\nsoft sensors and use it to estimate contact states. We use a neural network for\nforce regression, collecting labelled data with a force/torque sensor and a\nrange of test objects. Subsequently, we apply this model in a plug-in task\nscenario and demonstrate its validity in estimating contact states.\n", "link": "http://arxiv.org/abs/2410.19684v1", "date": "2024-10-25", "relevancy": 2.0607, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5548}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5341}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Finger%20Grasp%20Force%20and%20Contact%20State%20Estimation%20from%20Tactile%0A%20%20Sensors&body=Title%3A%20Soft%20Finger%20Grasp%20Force%20and%20Contact%20State%20Estimation%20from%20Tactile%0A%20%20Sensors%0AAuthor%3A%20Hun%20Jang%20and%20Joonbum%20Bae%20and%20Kevin%20Haninger%0AAbstract%3A%20%20%20Soft%20robotic%20fingers%20can%20improve%20adaptability%20in%20grasping%20and%20manipulation%2C%0Acompensating%20for%20geometric%20variation%20in%20object%20or%20environmental%20contact%2C%20but%0Atoday%20lack%20force%20capacity%20and%20fine%20dexterity.%20Integrated%20tactile%20sensors%20can%0Aprovide%20grasp%20and%20task%20information%20which%20can%20improve%20dexterity%2Cbut%20should%0Aideally%20not%20require%20object-specific%20training.%20The%20total%20force%20vector%20exerted%20by%0Aa%20finger%20provides%20general%20information%20to%20the%20internal%20grasp%20forces%20%28e.g.%20for%0Agrasp%20stability%29%20and%2C%20when%20summed%20over%20fingers%2C%20an%20estimate%20of%20the%20external%0Aforce%20acting%20on%20the%20grasped%20object%20%28e.g.%20for%20task-level%20control%29.%20In%20this%0Astudy%2C%20we%20investigate%20the%20efficacy%20of%20estimating%20finger%20force%20from%20integrated%0Asoft%20sensors%20and%20use%20it%20to%20estimate%20contact%20states.%20We%20use%20a%20neural%20network%20for%0Aforce%20regression%2C%20collecting%20labelled%20data%20with%20a%20force/torque%20sensor%20and%20a%0Arange%20of%20test%20objects.%20Subsequently%2C%20we%20apply%20this%20model%20in%20a%20plug-in%20task%0Ascenario%20and%20demonstrate%20its%20validity%20in%20estimating%20contact%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Finger%2520Grasp%2520Force%2520and%2520Contact%2520State%2520Estimation%2520from%2520Tactile%250A%2520%2520Sensors%26entry.906535625%3DHun%2520Jang%2520and%2520Joonbum%2520Bae%2520and%2520Kevin%2520Haninger%26entry.1292438233%3D%2520%2520Soft%2520robotic%2520fingers%2520can%2520improve%2520adaptability%2520in%2520grasping%2520and%2520manipulation%252C%250Acompensating%2520for%2520geometric%2520variation%2520in%2520object%2520or%2520environmental%2520contact%252C%2520but%250Atoday%2520lack%2520force%2520capacity%2520and%2520fine%2520dexterity.%2520Integrated%2520tactile%2520sensors%2520can%250Aprovide%2520grasp%2520and%2520task%2520information%2520which%2520can%2520improve%2520dexterity%252Cbut%2520should%250Aideally%2520not%2520require%2520object-specific%2520training.%2520The%2520total%2520force%2520vector%2520exerted%2520by%250Aa%2520finger%2520provides%2520general%2520information%2520to%2520the%2520internal%2520grasp%2520forces%2520%2528e.g.%2520for%250Agrasp%2520stability%2529%2520and%252C%2520when%2520summed%2520over%2520fingers%252C%2520an%2520estimate%2520of%2520the%2520external%250Aforce%2520acting%2520on%2520the%2520grasped%2520object%2520%2528e.g.%2520for%2520task-level%2520control%2529.%2520In%2520this%250Astudy%252C%2520we%2520investigate%2520the%2520efficacy%2520of%2520estimating%2520finger%2520force%2520from%2520integrated%250Asoft%2520sensors%2520and%2520use%2520it%2520to%2520estimate%2520contact%2520states.%2520We%2520use%2520a%2520neural%2520network%2520for%250Aforce%2520regression%252C%2520collecting%2520labelled%2520data%2520with%2520a%2520force/torque%2520sensor%2520and%2520a%250Arange%2520of%2520test%2520objects.%2520Subsequently%252C%2520we%2520apply%2520this%2520model%2520in%2520a%2520plug-in%2520task%250Ascenario%2520and%2520demonstrate%2520its%2520validity%2520in%2520estimating%2520contact%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Finger%20Grasp%20Force%20and%20Contact%20State%20Estimation%20from%20Tactile%0A%20%20Sensors&entry.906535625=Hun%20Jang%20and%20Joonbum%20Bae%20and%20Kevin%20Haninger&entry.1292438233=%20%20Soft%20robotic%20fingers%20can%20improve%20adaptability%20in%20grasping%20and%20manipulation%2C%0Acompensating%20for%20geometric%20variation%20in%20object%20or%20environmental%20contact%2C%20but%0Atoday%20lack%20force%20capacity%20and%20fine%20dexterity.%20Integrated%20tactile%20sensors%20can%0Aprovide%20grasp%20and%20task%20information%20which%20can%20improve%20dexterity%2Cbut%20should%0Aideally%20not%20require%20object-specific%20training.%20The%20total%20force%20vector%20exerted%20by%0Aa%20finger%20provides%20general%20information%20to%20the%20internal%20grasp%20forces%20%28e.g.%20for%0Agrasp%20stability%29%20and%2C%20when%20summed%20over%20fingers%2C%20an%20estimate%20of%20the%20external%0Aforce%20acting%20on%20the%20grasped%20object%20%28e.g.%20for%20task-level%20control%29.%20In%20this%0Astudy%2C%20we%20investigate%20the%20efficacy%20of%20estimating%20finger%20force%20from%20integrated%0Asoft%20sensors%20and%20use%20it%20to%20estimate%20contact%20states.%20We%20use%20a%20neural%20network%20for%0Aforce%20regression%2C%20collecting%20labelled%20data%20with%20a%20force/torque%20sensor%20and%20a%0Arange%20of%20test%20objects.%20Subsequently%2C%20we%20apply%20this%20model%20in%20a%20plug-in%20task%0Ascenario%20and%20demonstrate%20its%20validity%20in%20estimating%20contact%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19684v1&entry.124074799=Read"},
{"title": "Are Data Augmentation Methods in Named Entity Recognition Applicable for\n  Uncertainty Estimation?", "author": "Wataru Hashimoto and Hidetaka Kamigaito and Taro Watanabe", "abstract": "  This work investigates the impact of data augmentation on confidence\ncalibration and uncertainty estimation in Named Entity Recognition (NER) tasks.\nFor the future advance of NER in safety-critical fields like healthcare and\nfinance, it is essential to achieve accurate predictions with calibrated\nconfidence when applying Deep Neural Networks (DNNs), including Pre-trained\nLanguage Models (PLMs), as a real-world application. However, DNNs are prone to\nmiscalibration, which limits their applicability. Moreover, existing methods\nfor calibration and uncertainty estimation are computational expensive. Our\ninvestigation in NER found that data augmentation improves calibration and\nuncertainty in cross-genre and cross-lingual setting, especially in-domain\nsetting. Furthermore, we showed that the calibration for NER tends to be more\neffective when the perplexity of the sentences generated by data augmentation\nis lower, and that increasing the size of the augmentation further improves\ncalibration and uncertainty.\n", "link": "http://arxiv.org/abs/2407.02062v2", "date": "2024-10-25", "relevancy": 2.0461, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6042}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5246}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Data%20Augmentation%20Methods%20in%20Named%20Entity%20Recognition%20Applicable%20for%0A%20%20Uncertainty%20Estimation%3F&body=Title%3A%20Are%20Data%20Augmentation%20Methods%20in%20Named%20Entity%20Recognition%20Applicable%20for%0A%20%20Uncertainty%20Estimation%3F%0AAuthor%3A%20Wataru%20Hashimoto%20and%20Hidetaka%20Kamigaito%20and%20Taro%20Watanabe%0AAbstract%3A%20%20%20This%20work%20investigates%20the%20impact%20of%20data%20augmentation%20on%20confidence%0Acalibration%20and%20uncertainty%20estimation%20in%20Named%20Entity%20Recognition%20%28NER%29%20tasks.%0AFor%20the%20future%20advance%20of%20NER%20in%20safety-critical%20fields%20like%20healthcare%20and%0Afinance%2C%20it%20is%20essential%20to%20achieve%20accurate%20predictions%20with%20calibrated%0Aconfidence%20when%20applying%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20including%20Pre-trained%0ALanguage%20Models%20%28PLMs%29%2C%20as%20a%20real-world%20application.%20However%2C%20DNNs%20are%20prone%20to%0Amiscalibration%2C%20which%20limits%20their%20applicability.%20Moreover%2C%20existing%20methods%0Afor%20calibration%20and%20uncertainty%20estimation%20are%20computational%20expensive.%20Our%0Ainvestigation%20in%20NER%20found%20that%20data%20augmentation%20improves%20calibration%20and%0Auncertainty%20in%20cross-genre%20and%20cross-lingual%20setting%2C%20especially%20in-domain%0Asetting.%20Furthermore%2C%20we%20showed%20that%20the%20calibration%20for%20NER%20tends%20to%20be%20more%0Aeffective%20when%20the%20perplexity%20of%20the%20sentences%20generated%20by%20data%20augmentation%0Ais%20lower%2C%20and%20that%20increasing%20the%20size%20of%20the%20augmentation%20further%20improves%0Acalibration%20and%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02062v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Data%2520Augmentation%2520Methods%2520in%2520Named%2520Entity%2520Recognition%2520Applicable%2520for%250A%2520%2520Uncertainty%2520Estimation%253F%26entry.906535625%3DWataru%2520Hashimoto%2520and%2520Hidetaka%2520Kamigaito%2520and%2520Taro%2520Watanabe%26entry.1292438233%3D%2520%2520This%2520work%2520investigates%2520the%2520impact%2520of%2520data%2520augmentation%2520on%2520confidence%250Acalibration%2520and%2520uncertainty%2520estimation%2520in%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%2520tasks.%250AFor%2520the%2520future%2520advance%2520of%2520NER%2520in%2520safety-critical%2520fields%2520like%2520healthcare%2520and%250Afinance%252C%2520it%2520is%2520essential%2520to%2520achieve%2520accurate%2520predictions%2520with%2520calibrated%250Aconfidence%2520when%2520applying%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%252C%2520including%2520Pre-trained%250ALanguage%2520Models%2520%2528PLMs%2529%252C%2520as%2520a%2520real-world%2520application.%2520However%252C%2520DNNs%2520are%2520prone%2520to%250Amiscalibration%252C%2520which%2520limits%2520their%2520applicability.%2520Moreover%252C%2520existing%2520methods%250Afor%2520calibration%2520and%2520uncertainty%2520estimation%2520are%2520computational%2520expensive.%2520Our%250Ainvestigation%2520in%2520NER%2520found%2520that%2520data%2520augmentation%2520improves%2520calibration%2520and%250Auncertainty%2520in%2520cross-genre%2520and%2520cross-lingual%2520setting%252C%2520especially%2520in-domain%250Asetting.%2520Furthermore%252C%2520we%2520showed%2520that%2520the%2520calibration%2520for%2520NER%2520tends%2520to%2520be%2520more%250Aeffective%2520when%2520the%2520perplexity%2520of%2520the%2520sentences%2520generated%2520by%2520data%2520augmentation%250Ais%2520lower%252C%2520and%2520that%2520increasing%2520the%2520size%2520of%2520the%2520augmentation%2520further%2520improves%250Acalibration%2520and%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02062v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Data%20Augmentation%20Methods%20in%20Named%20Entity%20Recognition%20Applicable%20for%0A%20%20Uncertainty%20Estimation%3F&entry.906535625=Wataru%20Hashimoto%20and%20Hidetaka%20Kamigaito%20and%20Taro%20Watanabe&entry.1292438233=%20%20This%20work%20investigates%20the%20impact%20of%20data%20augmentation%20on%20confidence%0Acalibration%20and%20uncertainty%20estimation%20in%20Named%20Entity%20Recognition%20%28NER%29%20tasks.%0AFor%20the%20future%20advance%20of%20NER%20in%20safety-critical%20fields%20like%20healthcare%20and%0Afinance%2C%20it%20is%20essential%20to%20achieve%20accurate%20predictions%20with%20calibrated%0Aconfidence%20when%20applying%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20including%20Pre-trained%0ALanguage%20Models%20%28PLMs%29%2C%20as%20a%20real-world%20application.%20However%2C%20DNNs%20are%20prone%20to%0Amiscalibration%2C%20which%20limits%20their%20applicability.%20Moreover%2C%20existing%20methods%0Afor%20calibration%20and%20uncertainty%20estimation%20are%20computational%20expensive.%20Our%0Ainvestigation%20in%20NER%20found%20that%20data%20augmentation%20improves%20calibration%20and%0Auncertainty%20in%20cross-genre%20and%20cross-lingual%20setting%2C%20especially%20in-domain%0Asetting.%20Furthermore%2C%20we%20showed%20that%20the%20calibration%20for%20NER%20tends%20to%20be%20more%0Aeffective%20when%20the%20perplexity%20of%20the%20sentences%20generated%20by%20data%20augmentation%0Ais%20lower%2C%20and%20that%20increasing%20the%20size%20of%20the%20augmentation%20further%20improves%0Acalibration%20and%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02062v2&entry.124074799=Read"},
{"title": "Learning the Regularization Strength for Deep Fine-Tuning via a\n  Data-Emphasized Variational Objective", "author": "Ethan Harvey and Mikhail Petrov and Michael C. Hughes", "abstract": "  A number of popular transfer learning methods rely on grid search to select\nregularization hyperparameters that control over-fitting. This grid search\nrequirement has several key disadvantages: the search is computationally\nexpensive, requires carving out a validation set that reduces the size of\navailable data for model training, and requires practitioners to specify\ncandidate values. In this paper, we propose an alternative to grid search:\ndirectly learning regularization hyperparameters on the full training set via\nmodel selection techniques based on the evidence lower bound (\"ELBo\") objective\nfrom variational methods. For deep neural networks with millions of parameters,\nwe specifically recommend a modified ELBo that upweights the influence of the\ndata likelihood relative to the prior while remaining a valid bound on the\nevidence for Bayesian model selection. Our proposed technique overcomes all\nthree disadvantages of grid search. We demonstrate effectiveness on image\nclassification tasks on several datasets, yielding heldout accuracy comparable\nto existing approaches with far less compute time.\n", "link": "http://arxiv.org/abs/2410.19675v1", "date": "2024-10-25", "relevancy": 2.0425, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5295}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20Regularization%20Strength%20for%20Deep%20Fine-Tuning%20via%20a%0A%20%20Data-Emphasized%20Variational%20Objective&body=Title%3A%20Learning%20the%20Regularization%20Strength%20for%20Deep%20Fine-Tuning%20via%20a%0A%20%20Data-Emphasized%20Variational%20Objective%0AAuthor%3A%20Ethan%20Harvey%20and%20Mikhail%20Petrov%20and%20Michael%20C.%20Hughes%0AAbstract%3A%20%20%20A%20number%20of%20popular%20transfer%20learning%20methods%20rely%20on%20grid%20search%20to%20select%0Aregularization%20hyperparameters%20that%20control%20over-fitting.%20This%20grid%20search%0Arequirement%20has%20several%20key%20disadvantages%3A%20the%20search%20is%20computationally%0Aexpensive%2C%20requires%20carving%20out%20a%20validation%20set%20that%20reduces%20the%20size%20of%0Aavailable%20data%20for%20model%20training%2C%20and%20requires%20practitioners%20to%20specify%0Acandidate%20values.%20In%20this%20paper%2C%20we%20propose%20an%20alternative%20to%20grid%20search%3A%0Adirectly%20learning%20regularization%20hyperparameters%20on%20the%20full%20training%20set%20via%0Amodel%20selection%20techniques%20based%20on%20the%20evidence%20lower%20bound%20%28%22ELBo%22%29%20objective%0Afrom%20variational%20methods.%20For%20deep%20neural%20networks%20with%20millions%20of%20parameters%2C%0Awe%20specifically%20recommend%20a%20modified%20ELBo%20that%20upweights%20the%20influence%20of%20the%0Adata%20likelihood%20relative%20to%20the%20prior%20while%20remaining%20a%20valid%20bound%20on%20the%0Aevidence%20for%20Bayesian%20model%20selection.%20Our%20proposed%20technique%20overcomes%20all%0Athree%20disadvantages%20of%20grid%20search.%20We%20demonstrate%20effectiveness%20on%20image%0Aclassification%20tasks%20on%20several%20datasets%2C%20yielding%20heldout%20accuracy%20comparable%0Ato%20existing%20approaches%20with%20far%20less%20compute%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520the%2520Regularization%2520Strength%2520for%2520Deep%2520Fine-Tuning%2520via%2520a%250A%2520%2520Data-Emphasized%2520Variational%2520Objective%26entry.906535625%3DEthan%2520Harvey%2520and%2520Mikhail%2520Petrov%2520and%2520Michael%2520C.%2520Hughes%26entry.1292438233%3D%2520%2520A%2520number%2520of%2520popular%2520transfer%2520learning%2520methods%2520rely%2520on%2520grid%2520search%2520to%2520select%250Aregularization%2520hyperparameters%2520that%2520control%2520over-fitting.%2520This%2520grid%2520search%250Arequirement%2520has%2520several%2520key%2520disadvantages%253A%2520the%2520search%2520is%2520computationally%250Aexpensive%252C%2520requires%2520carving%2520out%2520a%2520validation%2520set%2520that%2520reduces%2520the%2520size%2520of%250Aavailable%2520data%2520for%2520model%2520training%252C%2520and%2520requires%2520practitioners%2520to%2520specify%250Acandidate%2520values.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520alternative%2520to%2520grid%2520search%253A%250Adirectly%2520learning%2520regularization%2520hyperparameters%2520on%2520the%2520full%2520training%2520set%2520via%250Amodel%2520selection%2520techniques%2520based%2520on%2520the%2520evidence%2520lower%2520bound%2520%2528%2522ELBo%2522%2529%2520objective%250Afrom%2520variational%2520methods.%2520For%2520deep%2520neural%2520networks%2520with%2520millions%2520of%2520parameters%252C%250Awe%2520specifically%2520recommend%2520a%2520modified%2520ELBo%2520that%2520upweights%2520the%2520influence%2520of%2520the%250Adata%2520likelihood%2520relative%2520to%2520the%2520prior%2520while%2520remaining%2520a%2520valid%2520bound%2520on%2520the%250Aevidence%2520for%2520Bayesian%2520model%2520selection.%2520Our%2520proposed%2520technique%2520overcomes%2520all%250Athree%2520disadvantages%2520of%2520grid%2520search.%2520We%2520demonstrate%2520effectiveness%2520on%2520image%250Aclassification%2520tasks%2520on%2520several%2520datasets%252C%2520yielding%2520heldout%2520accuracy%2520comparable%250Ato%2520existing%2520approaches%2520with%2520far%2520less%2520compute%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20Regularization%20Strength%20for%20Deep%20Fine-Tuning%20via%20a%0A%20%20Data-Emphasized%20Variational%20Objective&entry.906535625=Ethan%20Harvey%20and%20Mikhail%20Petrov%20and%20Michael%20C.%20Hughes&entry.1292438233=%20%20A%20number%20of%20popular%20transfer%20learning%20methods%20rely%20on%20grid%20search%20to%20select%0Aregularization%20hyperparameters%20that%20control%20over-fitting.%20This%20grid%20search%0Arequirement%20has%20several%20key%20disadvantages%3A%20the%20search%20is%20computationally%0Aexpensive%2C%20requires%20carving%20out%20a%20validation%20set%20that%20reduces%20the%20size%20of%0Aavailable%20data%20for%20model%20training%2C%20and%20requires%20practitioners%20to%20specify%0Acandidate%20values.%20In%20this%20paper%2C%20we%20propose%20an%20alternative%20to%20grid%20search%3A%0Adirectly%20learning%20regularization%20hyperparameters%20on%20the%20full%20training%20set%20via%0Amodel%20selection%20techniques%20based%20on%20the%20evidence%20lower%20bound%20%28%22ELBo%22%29%20objective%0Afrom%20variational%20methods.%20For%20deep%20neural%20networks%20with%20millions%20of%20parameters%2C%0Awe%20specifically%20recommend%20a%20modified%20ELBo%20that%20upweights%20the%20influence%20of%20the%0Adata%20likelihood%20relative%20to%20the%20prior%20while%20remaining%20a%20valid%20bound%20on%20the%0Aevidence%20for%20Bayesian%20model%20selection.%20Our%20proposed%20technique%20overcomes%20all%0Athree%20disadvantages%20of%20grid%20search.%20We%20demonstrate%20effectiveness%20on%20image%0Aclassification%20tasks%20on%20several%20datasets%2C%20yielding%20heldout%20accuracy%20comparable%0Ato%20existing%20approaches%20with%20far%20less%20compute%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19675v1&entry.124074799=Read"},
{"title": "Human-like Episodic Memory for Infinite Context LLMs", "author": "Zafeirios Fountas and Martin A Benfeghoul and Adnan Oomerjee and Fenia Christopoulou and Gerasimos Lampouras and Haitham Bou-Ammar and Jun Wang", "abstract": "  Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs\nwith no fine-tuning, enabling them to handle practically infinite context\nlengths while maintaining computational efficiency. EM-LLM organises sequences\nof tokens into coherent episodic events using a combination of Bayesian\nsurprise and graph-theoretic boundary refinement in an online fashion. When\nneeded, these events are retrieved through a two-stage memory process,\ncombining similarity-based and temporally contiguous retrieval for efficient\nand human-like access to relevant information. Experiments on the LongBench and\nInfiniteBench benchmarks demonstrate EM-LLM's superior performance,\nconsistently outperforming the state-of-the-art retrieval model InfLLM across\nvarious baseline LLMs. In addition, EM-LLM outperforms its popular counterpart,\nRAG, in a wide range of tasks, while requiring similar resources. Notably,\nEM-LLM's performance even surpasses full-context models in most tasks, while\nsuccessfully performing retrieval across 10 million tokens - a scale\ncomputationally infeasible for such models. Finally, our analysis reveals\nstrong correlations between EM-LLM's event segmentation and human-perceived\nevents, suggesting a bridge between this artificial system and its biological\ncounterpart, thereby offering a novel computational framework for exploring\nhuman memory mechanisms.\n", "link": "http://arxiv.org/abs/2407.09450v2", "date": "2024-10-25", "relevancy": 2.0406, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-like%20Episodic%20Memory%20for%20Infinite%20Context%20LLMs&body=Title%3A%20Human-like%20Episodic%20Memory%20for%20Infinite%20Context%20LLMs%0AAuthor%3A%20Zafeirios%20Fountas%20and%20Martin%20A%20Benfeghoul%20and%20Adnan%20Oomerjee%20and%20Fenia%20Christopoulou%20and%20Gerasimos%20Lampouras%20and%20Haitham%20Bou-Ammar%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%2C%20but%20still%0Astruggle%20with%20processing%20extensive%20contexts%2C%20limiting%20their%20ability%20to%20maintain%0Acoherence%20and%20accuracy%20over%20long%20sequences.%20In%20contrast%2C%20the%20human%20brain%20excels%0Aat%20organising%20and%20retrieving%20episodic%20experiences%20across%20vast%20temporal%20scales%2C%0Aspanning%20a%20lifetime.%20In%20this%20work%2C%20we%20introduce%20EM-LLM%2C%20a%20novel%20approach%20that%0Aintegrates%20key%20aspects%20of%20human%20episodic%20memory%20and%20event%20cognition%20into%20LLMs%0Awith%20no%20fine-tuning%2C%20enabling%20them%20to%20handle%20practically%20infinite%20context%0Alengths%20while%20maintaining%20computational%20efficiency.%20EM-LLM%20organises%20sequences%0Aof%20tokens%20into%20coherent%20episodic%20events%20using%20a%20combination%20of%20Bayesian%0Asurprise%20and%20graph-theoretic%20boundary%20refinement%20in%20an%20online%20fashion.%20When%0Aneeded%2C%20these%20events%20are%20retrieved%20through%20a%20two-stage%20memory%20process%2C%0Acombining%20similarity-based%20and%20temporally%20contiguous%20retrieval%20for%20efficient%0Aand%20human-like%20access%20to%20relevant%20information.%20Experiments%20on%20the%20LongBench%20and%0AInfiniteBench%20benchmarks%20demonstrate%20EM-LLM%27s%20superior%20performance%2C%0Aconsistently%20outperforming%20the%20state-of-the-art%20retrieval%20model%20InfLLM%20across%0Avarious%20baseline%20LLMs.%20In%20addition%2C%20EM-LLM%20outperforms%20its%20popular%20counterpart%2C%0ARAG%2C%20in%20a%20wide%20range%20of%20tasks%2C%20while%20requiring%20similar%20resources.%20Notably%2C%0AEM-LLM%27s%20performance%20even%20surpasses%20full-context%20models%20in%20most%20tasks%2C%20while%0Asuccessfully%20performing%20retrieval%20across%2010%20million%20tokens%20-%20a%20scale%0Acomputationally%20infeasible%20for%20such%20models.%20Finally%2C%20our%20analysis%20reveals%0Astrong%20correlations%20between%20EM-LLM%27s%20event%20segmentation%20and%20human-perceived%0Aevents%2C%20suggesting%20a%20bridge%20between%20this%20artificial%20system%20and%20its%20biological%0Acounterpart%2C%20thereby%20offering%20a%20novel%20computational%20framework%20for%20exploring%0Ahuman%20memory%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09450v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-like%2520Episodic%2520Memory%2520for%2520Infinite%2520Context%2520LLMs%26entry.906535625%3DZafeirios%2520Fountas%2520and%2520Martin%2520A%2520Benfeghoul%2520and%2520Adnan%2520Oomerjee%2520and%2520Fenia%2520Christopoulou%2520and%2520Gerasimos%2520Lampouras%2520and%2520Haitham%2520Bou-Ammar%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%252C%2520but%2520still%250Astruggle%2520with%2520processing%2520extensive%2520contexts%252C%2520limiting%2520their%2520ability%2520to%2520maintain%250Acoherence%2520and%2520accuracy%2520over%2520long%2520sequences.%2520In%2520contrast%252C%2520the%2520human%2520brain%2520excels%250Aat%2520organising%2520and%2520retrieving%2520episodic%2520experiences%2520across%2520vast%2520temporal%2520scales%252C%250Aspanning%2520a%2520lifetime.%2520In%2520this%2520work%252C%2520we%2520introduce%2520EM-LLM%252C%2520a%2520novel%2520approach%2520that%250Aintegrates%2520key%2520aspects%2520of%2520human%2520episodic%2520memory%2520and%2520event%2520cognition%2520into%2520LLMs%250Awith%2520no%2520fine-tuning%252C%2520enabling%2520them%2520to%2520handle%2520practically%2520infinite%2520context%250Alengths%2520while%2520maintaining%2520computational%2520efficiency.%2520EM-LLM%2520organises%2520sequences%250Aof%2520tokens%2520into%2520coherent%2520episodic%2520events%2520using%2520a%2520combination%2520of%2520Bayesian%250Asurprise%2520and%2520graph-theoretic%2520boundary%2520refinement%2520in%2520an%2520online%2520fashion.%2520When%250Aneeded%252C%2520these%2520events%2520are%2520retrieved%2520through%2520a%2520two-stage%2520memory%2520process%252C%250Acombining%2520similarity-based%2520and%2520temporally%2520contiguous%2520retrieval%2520for%2520efficient%250Aand%2520human-like%2520access%2520to%2520relevant%2520information.%2520Experiments%2520on%2520the%2520LongBench%2520and%250AInfiniteBench%2520benchmarks%2520demonstrate%2520EM-LLM%2527s%2520superior%2520performance%252C%250Aconsistently%2520outperforming%2520the%2520state-of-the-art%2520retrieval%2520model%2520InfLLM%2520across%250Avarious%2520baseline%2520LLMs.%2520In%2520addition%252C%2520EM-LLM%2520outperforms%2520its%2520popular%2520counterpart%252C%250ARAG%252C%2520in%2520a%2520wide%2520range%2520of%2520tasks%252C%2520while%2520requiring%2520similar%2520resources.%2520Notably%252C%250AEM-LLM%2527s%2520performance%2520even%2520surpasses%2520full-context%2520models%2520in%2520most%2520tasks%252C%2520while%250Asuccessfully%2520performing%2520retrieval%2520across%252010%2520million%2520tokens%2520-%2520a%2520scale%250Acomputationally%2520infeasible%2520for%2520such%2520models.%2520Finally%252C%2520our%2520analysis%2520reveals%250Astrong%2520correlations%2520between%2520EM-LLM%2527s%2520event%2520segmentation%2520and%2520human-perceived%250Aevents%252C%2520suggesting%2520a%2520bridge%2520between%2520this%2520artificial%2520system%2520and%2520its%2520biological%250Acounterpart%252C%2520thereby%2520offering%2520a%2520novel%2520computational%2520framework%2520for%2520exploring%250Ahuman%2520memory%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09450v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-like%20Episodic%20Memory%20for%20Infinite%20Context%20LLMs&entry.906535625=Zafeirios%20Fountas%20and%20Martin%20A%20Benfeghoul%20and%20Adnan%20Oomerjee%20and%20Fenia%20Christopoulou%20and%20Gerasimos%20Lampouras%20and%20Haitham%20Bou-Ammar%20and%20Jun%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%2C%20but%20still%0Astruggle%20with%20processing%20extensive%20contexts%2C%20limiting%20their%20ability%20to%20maintain%0Acoherence%20and%20accuracy%20over%20long%20sequences.%20In%20contrast%2C%20the%20human%20brain%20excels%0Aat%20organising%20and%20retrieving%20episodic%20experiences%20across%20vast%20temporal%20scales%2C%0Aspanning%20a%20lifetime.%20In%20this%20work%2C%20we%20introduce%20EM-LLM%2C%20a%20novel%20approach%20that%0Aintegrates%20key%20aspects%20of%20human%20episodic%20memory%20and%20event%20cognition%20into%20LLMs%0Awith%20no%20fine-tuning%2C%20enabling%20them%20to%20handle%20practically%20infinite%20context%0Alengths%20while%20maintaining%20computational%20efficiency.%20EM-LLM%20organises%20sequences%0Aof%20tokens%20into%20coherent%20episodic%20events%20using%20a%20combination%20of%20Bayesian%0Asurprise%20and%20graph-theoretic%20boundary%20refinement%20in%20an%20online%20fashion.%20When%0Aneeded%2C%20these%20events%20are%20retrieved%20through%20a%20two-stage%20memory%20process%2C%0Acombining%20similarity-based%20and%20temporally%20contiguous%20retrieval%20for%20efficient%0Aand%20human-like%20access%20to%20relevant%20information.%20Experiments%20on%20the%20LongBench%20and%0AInfiniteBench%20benchmarks%20demonstrate%20EM-LLM%27s%20superior%20performance%2C%0Aconsistently%20outperforming%20the%20state-of-the-art%20retrieval%20model%20InfLLM%20across%0Avarious%20baseline%20LLMs.%20In%20addition%2C%20EM-LLM%20outperforms%20its%20popular%20counterpart%2C%0ARAG%2C%20in%20a%20wide%20range%20of%20tasks%2C%20while%20requiring%20similar%20resources.%20Notably%2C%0AEM-LLM%27s%20performance%20even%20surpasses%20full-context%20models%20in%20most%20tasks%2C%20while%0Asuccessfully%20performing%20retrieval%20across%2010%20million%20tokens%20-%20a%20scale%0Acomputationally%20infeasible%20for%20such%20models.%20Finally%2C%20our%20analysis%20reveals%0Astrong%20correlations%20between%20EM-LLM%27s%20event%20segmentation%20and%20human-perceived%0Aevents%2C%20suggesting%20a%20bridge%20between%20this%20artificial%20system%20and%20its%20biological%0Acounterpart%2C%20thereby%20offering%20a%20novel%20computational%20framework%20for%20exploring%0Ahuman%20memory%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09450v2&entry.124074799=Read"},
{"title": "Prediction of microstructural representativity from a single image", "author": "Amir Dahari and Ronan Docherty and Steve Kench and Samuel J. Cooper", "abstract": "  In this study, we present a method for predicting the representativity of the\nphase fraction observed in a single image (2D or 3D) of a material. Traditional\napproaches often require large datasets and extensive statistical analysis to\nestimate the Integral Range, a key factor in determining the variance of\nmicrostructural properties. Our method leverages the Two-Point Correlation\nfunction to directly estimate the variance from a single image (2D or 3D),\nthereby enabling phase fraction prediction with associated confidence levels.\nWe validate our approach using open-source datasets, demonstrating its efficacy\nacross diverse microstructures. This technique significantly reduces the data\nrequirements for representativity analysis, providing a practical tool for\nmaterial scientists and engineers working with limited microstructural data. To\nmake the method easily accessible, we have created a web-application,\n\\url{www.imagerep.io}, for quick, simple and informative use of the method.\n", "link": "http://arxiv.org/abs/2410.19568v1", "date": "2024-10-25", "relevancy": 2.0369, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5111}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction%20of%20microstructural%20representativity%20from%20a%20single%20image&body=Title%3A%20Prediction%20of%20microstructural%20representativity%20from%20a%20single%20image%0AAuthor%3A%20Amir%20Dahari%20and%20Ronan%20Docherty%20and%20Steve%20Kench%20and%20Samuel%20J.%20Cooper%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20present%20a%20method%20for%20predicting%20the%20representativity%20of%20the%0Aphase%20fraction%20observed%20in%20a%20single%20image%20%282D%20or%203D%29%20of%20a%20material.%20Traditional%0Aapproaches%20often%20require%20large%20datasets%20and%20extensive%20statistical%20analysis%20to%0Aestimate%20the%20Integral%20Range%2C%20a%20key%20factor%20in%20determining%20the%20variance%20of%0Amicrostructural%20properties.%20Our%20method%20leverages%20the%20Two-Point%20Correlation%0Afunction%20to%20directly%20estimate%20the%20variance%20from%20a%20single%20image%20%282D%20or%203D%29%2C%0Athereby%20enabling%20phase%20fraction%20prediction%20with%20associated%20confidence%20levels.%0AWe%20validate%20our%20approach%20using%20open-source%20datasets%2C%20demonstrating%20its%20efficacy%0Aacross%20diverse%20microstructures.%20This%20technique%20significantly%20reduces%20the%20data%0Arequirements%20for%20representativity%20analysis%2C%20providing%20a%20practical%20tool%20for%0Amaterial%20scientists%20and%20engineers%20working%20with%20limited%20microstructural%20data.%20To%0Amake%20the%20method%20easily%20accessible%2C%20we%20have%20created%20a%20web-application%2C%0A%5Curl%7Bwww.imagerep.io%7D%2C%20for%20quick%2C%20simple%20and%20informative%20use%20of%20the%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction%2520of%2520microstructural%2520representativity%2520from%2520a%2520single%2520image%26entry.906535625%3DAmir%2520Dahari%2520and%2520Ronan%2520Docherty%2520and%2520Steve%2520Kench%2520and%2520Samuel%2520J.%2520Cooper%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520method%2520for%2520predicting%2520the%2520representativity%2520of%2520the%250Aphase%2520fraction%2520observed%2520in%2520a%2520single%2520image%2520%25282D%2520or%25203D%2529%2520of%2520a%2520material.%2520Traditional%250Aapproaches%2520often%2520require%2520large%2520datasets%2520and%2520extensive%2520statistical%2520analysis%2520to%250Aestimate%2520the%2520Integral%2520Range%252C%2520a%2520key%2520factor%2520in%2520determining%2520the%2520variance%2520of%250Amicrostructural%2520properties.%2520Our%2520method%2520leverages%2520the%2520Two-Point%2520Correlation%250Afunction%2520to%2520directly%2520estimate%2520the%2520variance%2520from%2520a%2520single%2520image%2520%25282D%2520or%25203D%2529%252C%250Athereby%2520enabling%2520phase%2520fraction%2520prediction%2520with%2520associated%2520confidence%2520levels.%250AWe%2520validate%2520our%2520approach%2520using%2520open-source%2520datasets%252C%2520demonstrating%2520its%2520efficacy%250Aacross%2520diverse%2520microstructures.%2520This%2520technique%2520significantly%2520reduces%2520the%2520data%250Arequirements%2520for%2520representativity%2520analysis%252C%2520providing%2520a%2520practical%2520tool%2520for%250Amaterial%2520scientists%2520and%2520engineers%2520working%2520with%2520limited%2520microstructural%2520data.%2520To%250Amake%2520the%2520method%2520easily%2520accessible%252C%2520we%2520have%2520created%2520a%2520web-application%252C%250A%255Curl%257Bwww.imagerep.io%257D%252C%2520for%2520quick%252C%2520simple%2520and%2520informative%2520use%2520of%2520the%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20of%20microstructural%20representativity%20from%20a%20single%20image&entry.906535625=Amir%20Dahari%20and%20Ronan%20Docherty%20and%20Steve%20Kench%20and%20Samuel%20J.%20Cooper&entry.1292438233=%20%20In%20this%20study%2C%20we%20present%20a%20method%20for%20predicting%20the%20representativity%20of%20the%0Aphase%20fraction%20observed%20in%20a%20single%20image%20%282D%20or%203D%29%20of%20a%20material.%20Traditional%0Aapproaches%20often%20require%20large%20datasets%20and%20extensive%20statistical%20analysis%20to%0Aestimate%20the%20Integral%20Range%2C%20a%20key%20factor%20in%20determining%20the%20variance%20of%0Amicrostructural%20properties.%20Our%20method%20leverages%20the%20Two-Point%20Correlation%0Afunction%20to%20directly%20estimate%20the%20variance%20from%20a%20single%20image%20%282D%20or%203D%29%2C%0Athereby%20enabling%20phase%20fraction%20prediction%20with%20associated%20confidence%20levels.%0AWe%20validate%20our%20approach%20using%20open-source%20datasets%2C%20demonstrating%20its%20efficacy%0Aacross%20diverse%20microstructures.%20This%20technique%20significantly%20reduces%20the%20data%0Arequirements%20for%20representativity%20analysis%2C%20providing%20a%20practical%20tool%20for%0Amaterial%20scientists%20and%20engineers%20working%20with%20limited%20microstructural%20data.%20To%0Amake%20the%20method%20easily%20accessible%2C%20we%20have%20created%20a%20web-application%2C%0A%5Curl%7Bwww.imagerep.io%7D%2C%20for%20quick%2C%20simple%20and%20informative%20use%20of%20the%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19568v1&entry.124074799=Read"},
{"title": "Toward Generalizable Multiple Sclerosis Lesion Segmentation Models", "author": "Liviu Badea and Maria Popa", "abstract": "  Automating Multiple Sclerosis (MS) lesion segmentation would be of great\nbenefit in initial diagnosis as well as monitoring disease progression. Deep\nlearning based segmentation models perform well in many domains, but the\nstate-of-the-art in MS lesion segmentation is still suboptimal. Complementary\nto previous MS lesion segmentation challenges which focused on optimizing the\nperformance on a single evaluation dataset, this study aims to develop models\nthat generalize across diverse evaluation datasets, mirroring real-world\nclinical scenarios that involve varied scanners, settings, and patient cohorts.\nTo this end, we used all high-quality publicly-available MS lesion segmentation\ndatasets on which we systematically trained a state-of-the-art UNet++\narchitecture. The resulting models demonstrate consistent performance across\nthe remaining test datasets (are generalizable), with larger and more\nheterogeneous datasets leading to better models. To the best of our knowledge,\nthis represents the most comprehensive cross-dataset evaluation of MS lesion\nsegmentation models to date using publicly available datasets. Additionally,\nexplicitly enhancing dataset size by merging datasets improved model\nperformance. Specifically, a model trained on the combined MSSEG2016-train,\nISBI2015, and 3D-MR-MS datasets surpasses the winner of the MICCAI-2016\ncompetition. Moreover, we demonstrate that the generalizability of our models\nalso relies on our original use of quantile normalization on MRI intensities.\n", "link": "http://arxiv.org/abs/2410.19623v1", "date": "2024-10-25", "relevancy": 2.0346, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Generalizable%20Multiple%20Sclerosis%20Lesion%20Segmentation%20Models&body=Title%3A%20Toward%20Generalizable%20Multiple%20Sclerosis%20Lesion%20Segmentation%20Models%0AAuthor%3A%20Liviu%20Badea%20and%20Maria%20Popa%0AAbstract%3A%20%20%20Automating%20Multiple%20Sclerosis%20%28MS%29%20lesion%20segmentation%20would%20be%20of%20great%0Abenefit%20in%20initial%20diagnosis%20as%20well%20as%20monitoring%20disease%20progression.%20Deep%0Alearning%20based%20segmentation%20models%20perform%20well%20in%20many%20domains%2C%20but%20the%0Astate-of-the-art%20in%20MS%20lesion%20segmentation%20is%20still%20suboptimal.%20Complementary%0Ato%20previous%20MS%20lesion%20segmentation%20challenges%20which%20focused%20on%20optimizing%20the%0Aperformance%20on%20a%20single%20evaluation%20dataset%2C%20this%20study%20aims%20to%20develop%20models%0Athat%20generalize%20across%20diverse%20evaluation%20datasets%2C%20mirroring%20real-world%0Aclinical%20scenarios%20that%20involve%20varied%20scanners%2C%20settings%2C%20and%20patient%20cohorts.%0ATo%20this%20end%2C%20we%20used%20all%20high-quality%20publicly-available%20MS%20lesion%20segmentation%0Adatasets%20on%20which%20we%20systematically%20trained%20a%20state-of-the-art%20UNet%2B%2B%0Aarchitecture.%20The%20resulting%20models%20demonstrate%20consistent%20performance%20across%0Athe%20remaining%20test%20datasets%20%28are%20generalizable%29%2C%20with%20larger%20and%20more%0Aheterogeneous%20datasets%20leading%20to%20better%20models.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20represents%20the%20most%20comprehensive%20cross-dataset%20evaluation%20of%20MS%20lesion%0Asegmentation%20models%20to%20date%20using%20publicly%20available%20datasets.%20Additionally%2C%0Aexplicitly%20enhancing%20dataset%20size%20by%20merging%20datasets%20improved%20model%0Aperformance.%20Specifically%2C%20a%20model%20trained%20on%20the%20combined%20MSSEG2016-train%2C%0AISBI2015%2C%20and%203D-MR-MS%20datasets%20surpasses%20the%20winner%20of%20the%20MICCAI-2016%0Acompetition.%20Moreover%2C%20we%20demonstrate%20that%20the%20generalizability%20of%20our%20models%0Aalso%20relies%20on%20our%20original%20use%20of%20quantile%20normalization%20on%20MRI%20intensities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Generalizable%2520Multiple%2520Sclerosis%2520Lesion%2520Segmentation%2520Models%26entry.906535625%3DLiviu%2520Badea%2520and%2520Maria%2520Popa%26entry.1292438233%3D%2520%2520Automating%2520Multiple%2520Sclerosis%2520%2528MS%2529%2520lesion%2520segmentation%2520would%2520be%2520of%2520great%250Abenefit%2520in%2520initial%2520diagnosis%2520as%2520well%2520as%2520monitoring%2520disease%2520progression.%2520Deep%250Alearning%2520based%2520segmentation%2520models%2520perform%2520well%2520in%2520many%2520domains%252C%2520but%2520the%250Astate-of-the-art%2520in%2520MS%2520lesion%2520segmentation%2520is%2520still%2520suboptimal.%2520Complementary%250Ato%2520previous%2520MS%2520lesion%2520segmentation%2520challenges%2520which%2520focused%2520on%2520optimizing%2520the%250Aperformance%2520on%2520a%2520single%2520evaluation%2520dataset%252C%2520this%2520study%2520aims%2520to%2520develop%2520models%250Athat%2520generalize%2520across%2520diverse%2520evaluation%2520datasets%252C%2520mirroring%2520real-world%250Aclinical%2520scenarios%2520that%2520involve%2520varied%2520scanners%252C%2520settings%252C%2520and%2520patient%2520cohorts.%250ATo%2520this%2520end%252C%2520we%2520used%2520all%2520high-quality%2520publicly-available%2520MS%2520lesion%2520segmentation%250Adatasets%2520on%2520which%2520we%2520systematically%2520trained%2520a%2520state-of-the-art%2520UNet%252B%252B%250Aarchitecture.%2520The%2520resulting%2520models%2520demonstrate%2520consistent%2520performance%2520across%250Athe%2520remaining%2520test%2520datasets%2520%2528are%2520generalizable%2529%252C%2520with%2520larger%2520and%2520more%250Aheterogeneous%2520datasets%2520leading%2520to%2520better%2520models.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250Athis%2520represents%2520the%2520most%2520comprehensive%2520cross-dataset%2520evaluation%2520of%2520MS%2520lesion%250Asegmentation%2520models%2520to%2520date%2520using%2520publicly%2520available%2520datasets.%2520Additionally%252C%250Aexplicitly%2520enhancing%2520dataset%2520size%2520by%2520merging%2520datasets%2520improved%2520model%250Aperformance.%2520Specifically%252C%2520a%2520model%2520trained%2520on%2520the%2520combined%2520MSSEG2016-train%252C%250AISBI2015%252C%2520and%25203D-MR-MS%2520datasets%2520surpasses%2520the%2520winner%2520of%2520the%2520MICCAI-2016%250Acompetition.%2520Moreover%252C%2520we%2520demonstrate%2520that%2520the%2520generalizability%2520of%2520our%2520models%250Aalso%2520relies%2520on%2520our%2520original%2520use%2520of%2520quantile%2520normalization%2520on%2520MRI%2520intensities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Generalizable%20Multiple%20Sclerosis%20Lesion%20Segmentation%20Models&entry.906535625=Liviu%20Badea%20and%20Maria%20Popa&entry.1292438233=%20%20Automating%20Multiple%20Sclerosis%20%28MS%29%20lesion%20segmentation%20would%20be%20of%20great%0Abenefit%20in%20initial%20diagnosis%20as%20well%20as%20monitoring%20disease%20progression.%20Deep%0Alearning%20based%20segmentation%20models%20perform%20well%20in%20many%20domains%2C%20but%20the%0Astate-of-the-art%20in%20MS%20lesion%20segmentation%20is%20still%20suboptimal.%20Complementary%0Ato%20previous%20MS%20lesion%20segmentation%20challenges%20which%20focused%20on%20optimizing%20the%0Aperformance%20on%20a%20single%20evaluation%20dataset%2C%20this%20study%20aims%20to%20develop%20models%0Athat%20generalize%20across%20diverse%20evaluation%20datasets%2C%20mirroring%20real-world%0Aclinical%20scenarios%20that%20involve%20varied%20scanners%2C%20settings%2C%20and%20patient%20cohorts.%0ATo%20this%20end%2C%20we%20used%20all%20high-quality%20publicly-available%20MS%20lesion%20segmentation%0Adatasets%20on%20which%20we%20systematically%20trained%20a%20state-of-the-art%20UNet%2B%2B%0Aarchitecture.%20The%20resulting%20models%20demonstrate%20consistent%20performance%20across%0Athe%20remaining%20test%20datasets%20%28are%20generalizable%29%2C%20with%20larger%20and%20more%0Aheterogeneous%20datasets%20leading%20to%20better%20models.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20represents%20the%20most%20comprehensive%20cross-dataset%20evaluation%20of%20MS%20lesion%0Asegmentation%20models%20to%20date%20using%20publicly%20available%20datasets.%20Additionally%2C%0Aexplicitly%20enhancing%20dataset%20size%20by%20merging%20datasets%20improved%20model%0Aperformance.%20Specifically%2C%20a%20model%20trained%20on%20the%20combined%20MSSEG2016-train%2C%0AISBI2015%2C%20and%203D-MR-MS%20datasets%20surpasses%20the%20winner%20of%20the%20MICCAI-2016%0Acompetition.%20Moreover%2C%20we%20demonstrate%20that%20the%20generalizability%20of%20our%20models%0Aalso%20relies%20on%20our%20original%20use%20of%20quantile%20normalization%20on%20MRI%20intensities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19623v1&entry.124074799=Read"},
{"title": "LoLCATs: On Low-Rank Linearizing of Large Language Models", "author": "Michael Zhang and Simran Arora and Rahul Chalamala and Alan Wu and Benjamin Spector and Aaryan Singhal and Krithik Ramesh and Christopher R\u00e9", "abstract": "  Recent works show we can linearize large language models (LLMs) -- swapping\nthe quadratic attentions of popular Transformer-based LLMs with subquadratic\nanalogs, such as linear attention -- avoiding the expensive pretraining costs.\nHowever, linearizing LLMs often significantly degrades model quality, still\nrequires training over billions of tokens, and remains limited to smaller 1.3B\nto 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer\n(LoLCATs), a simple two-step method that improves LLM linearizing quality with\norders of magnitudes less memory and compute. We base these steps on two\nfindings. First, we can replace an LLM's softmax attentions with\nclosely-approximating linear attentions, simply by training the linear\nattentions to match their softmax counterparts with an output MSE loss\n(\"attention transfer\"). Then, this enables adjusting for approximation errors\nand recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs\nsignificantly improves linearizing quality, training efficiency, and\nscalability. We significantly reduce the linearizing quality gap and produce\nstate-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading\nto 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with\nonly 0.2% of past methods' model parameters and 0.4% of their training tokens.\nFinally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50x\nlarger than prior work). When compared with prior approaches under the same\ncompute budgets, LoLCATs significantly improves linearizing quality, closing\nthe gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8%\nand 78.1% on 5-shot MMLU.\n", "link": "http://arxiv.org/abs/2410.10254v2", "date": "2024-10-25", "relevancy": 2.0342, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5108}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5083}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoLCATs%3A%20On%20Low-Rank%20Linearizing%20of%20Large%20Language%20Models&body=Title%3A%20LoLCATs%3A%20On%20Low-Rank%20Linearizing%20of%20Large%20Language%20Models%0AAuthor%3A%20Michael%20Zhang%20and%20Simran%20Arora%20and%20Rahul%20Chalamala%20and%20Alan%20Wu%20and%20Benjamin%20Spector%20and%20Aaryan%20Singhal%20and%20Krithik%20Ramesh%20and%20Christopher%20R%C3%A9%0AAbstract%3A%20%20%20Recent%20works%20show%20we%20can%20linearize%20large%20language%20models%20%28LLMs%29%20--%20swapping%0Athe%20quadratic%20attentions%20of%20popular%20Transformer-based%20LLMs%20with%20subquadratic%0Aanalogs%2C%20such%20as%20linear%20attention%20--%20avoiding%20the%20expensive%20pretraining%20costs.%0AHowever%2C%20linearizing%20LLMs%20often%20significantly%20degrades%20model%20quality%2C%20still%0Arequires%20training%20over%20billions%20of%20tokens%2C%20and%20remains%20limited%20to%20smaller%201.3B%0Ato%207B%20LLMs.%20We%20thus%20propose%20Low-rank%20Linear%20Conversion%20via%20Attention%20Transfer%0A%28LoLCATs%29%2C%20a%20simple%20two-step%20method%20that%20improves%20LLM%20linearizing%20quality%20with%0Aorders%20of%20magnitudes%20less%20memory%20and%20compute.%20We%20base%20these%20steps%20on%20two%0Afindings.%20First%2C%20we%20can%20replace%20an%20LLM%27s%20softmax%20attentions%20with%0Aclosely-approximating%20linear%20attentions%2C%20simply%20by%20training%20the%20linear%0Aattentions%20to%20match%20their%20softmax%20counterparts%20with%20an%20output%20MSE%20loss%0A%28%22attention%20transfer%22%29.%20Then%2C%20this%20enables%20adjusting%20for%20approximation%20errors%0Aand%20recovering%20LLM%20quality%20simply%20with%20low-rank%20adaptation%20%28LoRA%29.%20LoLCATs%0Asignificantly%20improves%20linearizing%20quality%2C%20training%20efficiency%2C%20and%0Ascalability.%20We%20significantly%20reduce%20the%20linearizing%20quality%20gap%20and%20produce%0Astate-of-the-art%20subquadratic%20LLMs%20from%20Llama%203%208B%20and%20Mistral%207B%20v0.1%2C%20leading%0Ato%2020%2B%20points%20of%20improvement%20on%205-shot%20MMLU.%20Furthermore%2C%20LoLCATs%20does%20so%20with%0Aonly%200.2%25%20of%20past%20methods%27%20model%20parameters%20and%200.4%25%20of%20their%20training%20tokens.%0AFinally%2C%20we%20apply%20LoLCATs%20to%20create%20the%20first%20linearized%2070B%20and%20405B%20LLMs%20%2850x%0Alarger%20than%20prior%20work%29.%20When%20compared%20with%20prior%20approaches%20under%20the%20same%0Acompute%20budgets%2C%20LoLCATs%20significantly%20improves%20linearizing%20quality%2C%20closing%0Athe%20gap%20between%20linearized%20and%20original%20Llama%203.1%2070B%20and%20405B%20LLMs%20by%2077.8%25%0Aand%2078.1%25%20on%205-shot%20MMLU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoLCATs%253A%2520On%2520Low-Rank%2520Linearizing%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DMichael%2520Zhang%2520and%2520Simran%2520Arora%2520and%2520Rahul%2520Chalamala%2520and%2520Alan%2520Wu%2520and%2520Benjamin%2520Spector%2520and%2520Aaryan%2520Singhal%2520and%2520Krithik%2520Ramesh%2520and%2520Christopher%2520R%25C3%25A9%26entry.1292438233%3D%2520%2520Recent%2520works%2520show%2520we%2520can%2520linearize%2520large%2520language%2520models%2520%2528LLMs%2529%2520--%2520swapping%250Athe%2520quadratic%2520attentions%2520of%2520popular%2520Transformer-based%2520LLMs%2520with%2520subquadratic%250Aanalogs%252C%2520such%2520as%2520linear%2520attention%2520--%2520avoiding%2520the%2520expensive%2520pretraining%2520costs.%250AHowever%252C%2520linearizing%2520LLMs%2520often%2520significantly%2520degrades%2520model%2520quality%252C%2520still%250Arequires%2520training%2520over%2520billions%2520of%2520tokens%252C%2520and%2520remains%2520limited%2520to%2520smaller%25201.3B%250Ato%25207B%2520LLMs.%2520We%2520thus%2520propose%2520Low-rank%2520Linear%2520Conversion%2520via%2520Attention%2520Transfer%250A%2528LoLCATs%2529%252C%2520a%2520simple%2520two-step%2520method%2520that%2520improves%2520LLM%2520linearizing%2520quality%2520with%250Aorders%2520of%2520magnitudes%2520less%2520memory%2520and%2520compute.%2520We%2520base%2520these%2520steps%2520on%2520two%250Afindings.%2520First%252C%2520we%2520can%2520replace%2520an%2520LLM%2527s%2520softmax%2520attentions%2520with%250Aclosely-approximating%2520linear%2520attentions%252C%2520simply%2520by%2520training%2520the%2520linear%250Aattentions%2520to%2520match%2520their%2520softmax%2520counterparts%2520with%2520an%2520output%2520MSE%2520loss%250A%2528%2522attention%2520transfer%2522%2529.%2520Then%252C%2520this%2520enables%2520adjusting%2520for%2520approximation%2520errors%250Aand%2520recovering%2520LLM%2520quality%2520simply%2520with%2520low-rank%2520adaptation%2520%2528LoRA%2529.%2520LoLCATs%250Asignificantly%2520improves%2520linearizing%2520quality%252C%2520training%2520efficiency%252C%2520and%250Ascalability.%2520We%2520significantly%2520reduce%2520the%2520linearizing%2520quality%2520gap%2520and%2520produce%250Astate-of-the-art%2520subquadratic%2520LLMs%2520from%2520Llama%25203%25208B%2520and%2520Mistral%25207B%2520v0.1%252C%2520leading%250Ato%252020%252B%2520points%2520of%2520improvement%2520on%25205-shot%2520MMLU.%2520Furthermore%252C%2520LoLCATs%2520does%2520so%2520with%250Aonly%25200.2%2525%2520of%2520past%2520methods%2527%2520model%2520parameters%2520and%25200.4%2525%2520of%2520their%2520training%2520tokens.%250AFinally%252C%2520we%2520apply%2520LoLCATs%2520to%2520create%2520the%2520first%2520linearized%252070B%2520and%2520405B%2520LLMs%2520%252850x%250Alarger%2520than%2520prior%2520work%2529.%2520When%2520compared%2520with%2520prior%2520approaches%2520under%2520the%2520same%250Acompute%2520budgets%252C%2520LoLCATs%2520significantly%2520improves%2520linearizing%2520quality%252C%2520closing%250Athe%2520gap%2520between%2520linearized%2520and%2520original%2520Llama%25203.1%252070B%2520and%2520405B%2520LLMs%2520by%252077.8%2525%250Aand%252078.1%2525%2520on%25205-shot%2520MMLU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoLCATs%3A%20On%20Low-Rank%20Linearizing%20of%20Large%20Language%20Models&entry.906535625=Michael%20Zhang%20and%20Simran%20Arora%20and%20Rahul%20Chalamala%20and%20Alan%20Wu%20and%20Benjamin%20Spector%20and%20Aaryan%20Singhal%20and%20Krithik%20Ramesh%20and%20Christopher%20R%C3%A9&entry.1292438233=%20%20Recent%20works%20show%20we%20can%20linearize%20large%20language%20models%20%28LLMs%29%20--%20swapping%0Athe%20quadratic%20attentions%20of%20popular%20Transformer-based%20LLMs%20with%20subquadratic%0Aanalogs%2C%20such%20as%20linear%20attention%20--%20avoiding%20the%20expensive%20pretraining%20costs.%0AHowever%2C%20linearizing%20LLMs%20often%20significantly%20degrades%20model%20quality%2C%20still%0Arequires%20training%20over%20billions%20of%20tokens%2C%20and%20remains%20limited%20to%20smaller%201.3B%0Ato%207B%20LLMs.%20We%20thus%20propose%20Low-rank%20Linear%20Conversion%20via%20Attention%20Transfer%0A%28LoLCATs%29%2C%20a%20simple%20two-step%20method%20that%20improves%20LLM%20linearizing%20quality%20with%0Aorders%20of%20magnitudes%20less%20memory%20and%20compute.%20We%20base%20these%20steps%20on%20two%0Afindings.%20First%2C%20we%20can%20replace%20an%20LLM%27s%20softmax%20attentions%20with%0Aclosely-approximating%20linear%20attentions%2C%20simply%20by%20training%20the%20linear%0Aattentions%20to%20match%20their%20softmax%20counterparts%20with%20an%20output%20MSE%20loss%0A%28%22attention%20transfer%22%29.%20Then%2C%20this%20enables%20adjusting%20for%20approximation%20errors%0Aand%20recovering%20LLM%20quality%20simply%20with%20low-rank%20adaptation%20%28LoRA%29.%20LoLCATs%0Asignificantly%20improves%20linearizing%20quality%2C%20training%20efficiency%2C%20and%0Ascalability.%20We%20significantly%20reduce%20the%20linearizing%20quality%20gap%20and%20produce%0Astate-of-the-art%20subquadratic%20LLMs%20from%20Llama%203%208B%20and%20Mistral%207B%20v0.1%2C%20leading%0Ato%2020%2B%20points%20of%20improvement%20on%205-shot%20MMLU.%20Furthermore%2C%20LoLCATs%20does%20so%20with%0Aonly%200.2%25%20of%20past%20methods%27%20model%20parameters%20and%200.4%25%20of%20their%20training%20tokens.%0AFinally%2C%20we%20apply%20LoLCATs%20to%20create%20the%20first%20linearized%2070B%20and%20405B%20LLMs%20%2850x%0Alarger%20than%20prior%20work%29.%20When%20compared%20with%20prior%20approaches%20under%20the%20same%0Acompute%20budgets%2C%20LoLCATs%20significantly%20improves%20linearizing%20quality%2C%20closing%0Athe%20gap%20between%20linearized%20and%20original%20Llama%203.1%2070B%20and%20405B%20LLMs%20by%2077.8%25%0Aand%2078.1%25%20on%205-shot%20MMLU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10254v2&entry.124074799=Read"},
{"title": "On the Application of Deep Learning for Precise Indoor Positioning in 6G", "author": "Sai Prasanth Kotturi and Anil Kumar Yerrapragada and Sai Prasad and Radha Krishna Ganti", "abstract": "  Accurate localization in indoor environments is a challenge due to the Non\nLine of Sight (NLoS) nature of the signaling. In this paper, we explore the use\nof AI/ML techniques for positioning accuracy enhancement in Indoor Factory\n(InF) scenarios. The proposed neural network, which we term LocNet, is trained\non measurements such as Channel Impulse Response (CIR) and Reference Signal\nReceived Power (RSRP) from multiple Transmit Receive Points (TRPs). Simulation\nresults show that when using measurements from 18 TRPs, LocNet achieves a 9 cm\npositioning accuracy at the 90th percentile. Additionally, we demonstrate that\nthe same model generalizes effectively even when measurements from some TRPs\nrandomly become unavailable. Lastly, we provide insights on the robustness of\nthe trained model to the errors in ground truth labels used for training.\n", "link": "http://arxiv.org/abs/2410.19436v1", "date": "2024-10-25", "relevancy": 2.0279, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5338}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5013}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Application%20of%20Deep%20Learning%20for%20Precise%20Indoor%20Positioning%20in%206G&body=Title%3A%20On%20the%20Application%20of%20Deep%20Learning%20for%20Precise%20Indoor%20Positioning%20in%206G%0AAuthor%3A%20Sai%20Prasanth%20Kotturi%20and%20Anil%20Kumar%20Yerrapragada%20and%20Sai%20Prasad%20and%20Radha%20Krishna%20Ganti%0AAbstract%3A%20%20%20Accurate%20localization%20in%20indoor%20environments%20is%20a%20challenge%20due%20to%20the%20Non%0ALine%20of%20Sight%20%28NLoS%29%20nature%20of%20the%20signaling.%20In%20this%20paper%2C%20we%20explore%20the%20use%0Aof%20AI/ML%20techniques%20for%20positioning%20accuracy%20enhancement%20in%20Indoor%20Factory%0A%28InF%29%20scenarios.%20The%20proposed%20neural%20network%2C%20which%20we%20term%20LocNet%2C%20is%20trained%0Aon%20measurements%20such%20as%20Channel%20Impulse%20Response%20%28CIR%29%20and%20Reference%20Signal%0AReceived%20Power%20%28RSRP%29%20from%20multiple%20Transmit%20Receive%20Points%20%28TRPs%29.%20Simulation%0Aresults%20show%20that%20when%20using%20measurements%20from%2018%20TRPs%2C%20LocNet%20achieves%20a%209%20cm%0Apositioning%20accuracy%20at%20the%2090th%20percentile.%20Additionally%2C%20we%20demonstrate%20that%0Athe%20same%20model%20generalizes%20effectively%20even%20when%20measurements%20from%20some%20TRPs%0Arandomly%20become%20unavailable.%20Lastly%2C%20we%20provide%20insights%20on%20the%20robustness%20of%0Athe%20trained%20model%20to%20the%20errors%20in%20ground%20truth%20labels%20used%20for%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Application%2520of%2520Deep%2520Learning%2520for%2520Precise%2520Indoor%2520Positioning%2520in%25206G%26entry.906535625%3DSai%2520Prasanth%2520Kotturi%2520and%2520Anil%2520Kumar%2520Yerrapragada%2520and%2520Sai%2520Prasad%2520and%2520Radha%2520Krishna%2520Ganti%26entry.1292438233%3D%2520%2520Accurate%2520localization%2520in%2520indoor%2520environments%2520is%2520a%2520challenge%2520due%2520to%2520the%2520Non%250ALine%2520of%2520Sight%2520%2528NLoS%2529%2520nature%2520of%2520the%2520signaling.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520use%250Aof%2520AI/ML%2520techniques%2520for%2520positioning%2520accuracy%2520enhancement%2520in%2520Indoor%2520Factory%250A%2528InF%2529%2520scenarios.%2520The%2520proposed%2520neural%2520network%252C%2520which%2520we%2520term%2520LocNet%252C%2520is%2520trained%250Aon%2520measurements%2520such%2520as%2520Channel%2520Impulse%2520Response%2520%2528CIR%2529%2520and%2520Reference%2520Signal%250AReceived%2520Power%2520%2528RSRP%2529%2520from%2520multiple%2520Transmit%2520Receive%2520Points%2520%2528TRPs%2529.%2520Simulation%250Aresults%2520show%2520that%2520when%2520using%2520measurements%2520from%252018%2520TRPs%252C%2520LocNet%2520achieves%2520a%25209%2520cm%250Apositioning%2520accuracy%2520at%2520the%252090th%2520percentile.%2520Additionally%252C%2520we%2520demonstrate%2520that%250Athe%2520same%2520model%2520generalizes%2520effectively%2520even%2520when%2520measurements%2520from%2520some%2520TRPs%250Arandomly%2520become%2520unavailable.%2520Lastly%252C%2520we%2520provide%2520insights%2520on%2520the%2520robustness%2520of%250Athe%2520trained%2520model%2520to%2520the%2520errors%2520in%2520ground%2520truth%2520labels%2520used%2520for%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Application%20of%20Deep%20Learning%20for%20Precise%20Indoor%20Positioning%20in%206G&entry.906535625=Sai%20Prasanth%20Kotturi%20and%20Anil%20Kumar%20Yerrapragada%20and%20Sai%20Prasad%20and%20Radha%20Krishna%20Ganti&entry.1292438233=%20%20Accurate%20localization%20in%20indoor%20environments%20is%20a%20challenge%20due%20to%20the%20Non%0ALine%20of%20Sight%20%28NLoS%29%20nature%20of%20the%20signaling.%20In%20this%20paper%2C%20we%20explore%20the%20use%0Aof%20AI/ML%20techniques%20for%20positioning%20accuracy%20enhancement%20in%20Indoor%20Factory%0A%28InF%29%20scenarios.%20The%20proposed%20neural%20network%2C%20which%20we%20term%20LocNet%2C%20is%20trained%0Aon%20measurements%20such%20as%20Channel%20Impulse%20Response%20%28CIR%29%20and%20Reference%20Signal%0AReceived%20Power%20%28RSRP%29%20from%20multiple%20Transmit%20Receive%20Points%20%28TRPs%29.%20Simulation%0Aresults%20show%20that%20when%20using%20measurements%20from%2018%20TRPs%2C%20LocNet%20achieves%20a%209%20cm%0Apositioning%20accuracy%20at%20the%2090th%20percentile.%20Additionally%2C%20we%20demonstrate%20that%0Athe%20same%20model%20generalizes%20effectively%20even%20when%20measurements%20from%20some%20TRPs%0Arandomly%20become%20unavailable.%20Lastly%2C%20we%20provide%20insights%20on%20the%20robustness%20of%0Athe%20trained%20model%20to%20the%20errors%20in%20ground%20truth%20labels%20used%20for%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19436v1&entry.124074799=Read"},
{"title": "The Endoscapes Dataset for Surgical Scene Segmentation, Object\n  Detection, and Critical View of Safety Assessment: Official Splits and\n  Benchmark", "author": "Aditya Murali and Deepak Alapatt and Pietro Mascagni and Armine Vardazaryan and Alain Garcia and Nariaki Okamoto and Guido Costamagna and Didier Mutter and Jacques Marescaux and Bernard Dallemagne and Nicolas Padoy", "abstract": "  This technical report provides a detailed overview of Endoscapes, a dataset\nof laparoscopic cholecystectomy (LC) videos with highly intricate annotations\ntargeted at automated assessment of the Critical View of Safety (CVS).\nEndoscapes comprises 201 LC videos with frames annotated sparsely but regularly\nwith segmentation masks, bounding boxes, and CVS assessment by three different\nclinical experts. Altogether, there are 11090 frames annotated with CVS and\n1933 frames annotated with tool and anatomy bounding boxes from the 201 videos,\nas well as an additional 422 frames from 50 of the 201 videos annotated with\ntool and anatomy segmentation masks. In this report, we provide detailed\ndataset statistics (size, class distribution, dataset splits, etc.) and a\ncomprehensive performance benchmark for instance segmentation, object\ndetection, and CVS prediction. The dataset and model checkpoints are publically\navailable at https://github.com/CAMMA-public/Endoscapes.\n", "link": "http://arxiv.org/abs/2312.12429v3", "date": "2024-10-25", "relevancy": 2.0257, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5147}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Endoscapes%20Dataset%20for%20Surgical%20Scene%20Segmentation%2C%20Object%0A%20%20Detection%2C%20and%20Critical%20View%20of%20Safety%20Assessment%3A%20Official%20Splits%20and%0A%20%20Benchmark&body=Title%3A%20The%20Endoscapes%20Dataset%20for%20Surgical%20Scene%20Segmentation%2C%20Object%0A%20%20Detection%2C%20and%20Critical%20View%20of%20Safety%20Assessment%3A%20Official%20Splits%20and%0A%20%20Benchmark%0AAuthor%3A%20Aditya%20Murali%20and%20Deepak%20Alapatt%20and%20Pietro%20Mascagni%20and%20Armine%20Vardazaryan%20and%20Alain%20Garcia%20and%20Nariaki%20Okamoto%20and%20Guido%20Costamagna%20and%20Didier%20Mutter%20and%20Jacques%20Marescaux%20and%20Bernard%20Dallemagne%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20This%20technical%20report%20provides%20a%20detailed%20overview%20of%20Endoscapes%2C%20a%20dataset%0Aof%20laparoscopic%20cholecystectomy%20%28LC%29%20videos%20with%20highly%20intricate%20annotations%0Atargeted%20at%20automated%20assessment%20of%20the%20Critical%20View%20of%20Safety%20%28CVS%29.%0AEndoscapes%20comprises%20201%20LC%20videos%20with%20frames%20annotated%20sparsely%20but%20regularly%0Awith%20segmentation%20masks%2C%20bounding%20boxes%2C%20and%20CVS%20assessment%20by%20three%20different%0Aclinical%20experts.%20Altogether%2C%20there%20are%2011090%20frames%20annotated%20with%20CVS%20and%0A1933%20frames%20annotated%20with%20tool%20and%20anatomy%20bounding%20boxes%20from%20the%20201%20videos%2C%0Aas%20well%20as%20an%20additional%20422%20frames%20from%2050%20of%20the%20201%20videos%20annotated%20with%0Atool%20and%20anatomy%20segmentation%20masks.%20In%20this%20report%2C%20we%20provide%20detailed%0Adataset%20statistics%20%28size%2C%20class%20distribution%2C%20dataset%20splits%2C%20etc.%29%20and%20a%0Acomprehensive%20performance%20benchmark%20for%20instance%20segmentation%2C%20object%0Adetection%2C%20and%20CVS%20prediction.%20The%20dataset%20and%20model%20checkpoints%20are%20publically%0Aavailable%20at%20https%3A//github.com/CAMMA-public/Endoscapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12429v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Endoscapes%2520Dataset%2520for%2520Surgical%2520Scene%2520Segmentation%252C%2520Object%250A%2520%2520Detection%252C%2520and%2520Critical%2520View%2520of%2520Safety%2520Assessment%253A%2520Official%2520Splits%2520and%250A%2520%2520Benchmark%26entry.906535625%3DAditya%2520Murali%2520and%2520Deepak%2520Alapatt%2520and%2520Pietro%2520Mascagni%2520and%2520Armine%2520Vardazaryan%2520and%2520Alain%2520Garcia%2520and%2520Nariaki%2520Okamoto%2520and%2520Guido%2520Costamagna%2520and%2520Didier%2520Mutter%2520and%2520Jacques%2520Marescaux%2520and%2520Bernard%2520Dallemagne%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520provides%2520a%2520detailed%2520overview%2520of%2520Endoscapes%252C%2520a%2520dataset%250Aof%2520laparoscopic%2520cholecystectomy%2520%2528LC%2529%2520videos%2520with%2520highly%2520intricate%2520annotations%250Atargeted%2520at%2520automated%2520assessment%2520of%2520the%2520Critical%2520View%2520of%2520Safety%2520%2528CVS%2529.%250AEndoscapes%2520comprises%2520201%2520LC%2520videos%2520with%2520frames%2520annotated%2520sparsely%2520but%2520regularly%250Awith%2520segmentation%2520masks%252C%2520bounding%2520boxes%252C%2520and%2520CVS%2520assessment%2520by%2520three%2520different%250Aclinical%2520experts.%2520Altogether%252C%2520there%2520are%252011090%2520frames%2520annotated%2520with%2520CVS%2520and%250A1933%2520frames%2520annotated%2520with%2520tool%2520and%2520anatomy%2520bounding%2520boxes%2520from%2520the%2520201%2520videos%252C%250Aas%2520well%2520as%2520an%2520additional%2520422%2520frames%2520from%252050%2520of%2520the%2520201%2520videos%2520annotated%2520with%250Atool%2520and%2520anatomy%2520segmentation%2520masks.%2520In%2520this%2520report%252C%2520we%2520provide%2520detailed%250Adataset%2520statistics%2520%2528size%252C%2520class%2520distribution%252C%2520dataset%2520splits%252C%2520etc.%2529%2520and%2520a%250Acomprehensive%2520performance%2520benchmark%2520for%2520instance%2520segmentation%252C%2520object%250Adetection%252C%2520and%2520CVS%2520prediction.%2520The%2520dataset%2520and%2520model%2520checkpoints%2520are%2520publically%250Aavailable%2520at%2520https%253A//github.com/CAMMA-public/Endoscapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12429v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Endoscapes%20Dataset%20for%20Surgical%20Scene%20Segmentation%2C%20Object%0A%20%20Detection%2C%20and%20Critical%20View%20of%20Safety%20Assessment%3A%20Official%20Splits%20and%0A%20%20Benchmark&entry.906535625=Aditya%20Murali%20and%20Deepak%20Alapatt%20and%20Pietro%20Mascagni%20and%20Armine%20Vardazaryan%20and%20Alain%20Garcia%20and%20Nariaki%20Okamoto%20and%20Guido%20Costamagna%20and%20Didier%20Mutter%20and%20Jacques%20Marescaux%20and%20Bernard%20Dallemagne%20and%20Nicolas%20Padoy&entry.1292438233=%20%20This%20technical%20report%20provides%20a%20detailed%20overview%20of%20Endoscapes%2C%20a%20dataset%0Aof%20laparoscopic%20cholecystectomy%20%28LC%29%20videos%20with%20highly%20intricate%20annotations%0Atargeted%20at%20automated%20assessment%20of%20the%20Critical%20View%20of%20Safety%20%28CVS%29.%0AEndoscapes%20comprises%20201%20LC%20videos%20with%20frames%20annotated%20sparsely%20but%20regularly%0Awith%20segmentation%20masks%2C%20bounding%20boxes%2C%20and%20CVS%20assessment%20by%20three%20different%0Aclinical%20experts.%20Altogether%2C%20there%20are%2011090%20frames%20annotated%20with%20CVS%20and%0A1933%20frames%20annotated%20with%20tool%20and%20anatomy%20bounding%20boxes%20from%20the%20201%20videos%2C%0Aas%20well%20as%20an%20additional%20422%20frames%20from%2050%20of%20the%20201%20videos%20annotated%20with%0Atool%20and%20anatomy%20segmentation%20masks.%20In%20this%20report%2C%20we%20provide%20detailed%0Adataset%20statistics%20%28size%2C%20class%20distribution%2C%20dataset%20splits%2C%20etc.%29%20and%20a%0Acomprehensive%20performance%20benchmark%20for%20instance%20segmentation%2C%20object%0Adetection%2C%20and%20CVS%20prediction.%20The%20dataset%20and%20model%20checkpoints%20are%20publically%0Aavailable%20at%20https%3A//github.com/CAMMA-public/Endoscapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12429v3&entry.124074799=Read"},
{"title": "COR-MP: Conservation of Resources Model for Maneuver Planning", "author": "Karim Essalmi and Fernando Garrido and Fawzi Nashashibi", "abstract": "  Decision-making for automated driving remains a challenging task. For their\nintegration into real platforms, these algorithms must guarantee passenger\nsafety and comfort while ensuring interpretability and an appropriate\ncomputational time. To model and solve this decision-making problem, we have\ndeveloped a novel approach called COR-MP (Conservation of Resources model for\nManeuver Planning). This model is based on the Conservation of Resources\ntheory, a psychological concept applied to human behavior. COR-MP is based on\nvarious driving parameters, such as comfort, safety, or energy, and provides in\nreal-time a profit value that enables us to quantify the impact of a decision\non the decision-maker. Our method has been tested and validated through\nclosed-loop simulations using RTMaps middleware, and preliminary results have\nbeen obtained by testing COR-MP on a real vehicle.\n", "link": "http://arxiv.org/abs/2410.19510v1", "date": "2024-10-25", "relevancy": 2.0248, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5028}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COR-MP%3A%20Conservation%20of%20Resources%20Model%20for%20Maneuver%20Planning&body=Title%3A%20COR-MP%3A%20Conservation%20of%20Resources%20Model%20for%20Maneuver%20Planning%0AAuthor%3A%20Karim%20Essalmi%20and%20Fernando%20Garrido%20and%20Fawzi%20Nashashibi%0AAbstract%3A%20%20%20Decision-making%20for%20automated%20driving%20remains%20a%20challenging%20task.%20For%20their%0Aintegration%20into%20real%20platforms%2C%20these%20algorithms%20must%20guarantee%20passenger%0Asafety%20and%20comfort%20while%20ensuring%20interpretability%20and%20an%20appropriate%0Acomputational%20time.%20To%20model%20and%20solve%20this%20decision-making%20problem%2C%20we%20have%0Adeveloped%20a%20novel%20approach%20called%20COR-MP%20%28Conservation%20of%20Resources%20model%20for%0AManeuver%20Planning%29.%20This%20model%20is%20based%20on%20the%20Conservation%20of%20Resources%0Atheory%2C%20a%20psychological%20concept%20applied%20to%20human%20behavior.%20COR-MP%20is%20based%20on%0Avarious%20driving%20parameters%2C%20such%20as%20comfort%2C%20safety%2C%20or%20energy%2C%20and%20provides%20in%0Areal-time%20a%20profit%20value%20that%20enables%20us%20to%20quantify%20the%20impact%20of%20a%20decision%0Aon%20the%20decision-maker.%20Our%20method%20has%20been%20tested%20and%20validated%20through%0Aclosed-loop%20simulations%20using%20RTMaps%20middleware%2C%20and%20preliminary%20results%20have%0Abeen%20obtained%20by%20testing%20COR-MP%20on%20a%20real%20vehicle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOR-MP%253A%2520Conservation%2520of%2520Resources%2520Model%2520for%2520Maneuver%2520Planning%26entry.906535625%3DKarim%2520Essalmi%2520and%2520Fernando%2520Garrido%2520and%2520Fawzi%2520Nashashibi%26entry.1292438233%3D%2520%2520Decision-making%2520for%2520automated%2520driving%2520remains%2520a%2520challenging%2520task.%2520For%2520their%250Aintegration%2520into%2520real%2520platforms%252C%2520these%2520algorithms%2520must%2520guarantee%2520passenger%250Asafety%2520and%2520comfort%2520while%2520ensuring%2520interpretability%2520and%2520an%2520appropriate%250Acomputational%2520time.%2520To%2520model%2520and%2520solve%2520this%2520decision-making%2520problem%252C%2520we%2520have%250Adeveloped%2520a%2520novel%2520approach%2520called%2520COR-MP%2520%2528Conservation%2520of%2520Resources%2520model%2520for%250AManeuver%2520Planning%2529.%2520This%2520model%2520is%2520based%2520on%2520the%2520Conservation%2520of%2520Resources%250Atheory%252C%2520a%2520psychological%2520concept%2520applied%2520to%2520human%2520behavior.%2520COR-MP%2520is%2520based%2520on%250Avarious%2520driving%2520parameters%252C%2520such%2520as%2520comfort%252C%2520safety%252C%2520or%2520energy%252C%2520and%2520provides%2520in%250Areal-time%2520a%2520profit%2520value%2520that%2520enables%2520us%2520to%2520quantify%2520the%2520impact%2520of%2520a%2520decision%250Aon%2520the%2520decision-maker.%2520Our%2520method%2520has%2520been%2520tested%2520and%2520validated%2520through%250Aclosed-loop%2520simulations%2520using%2520RTMaps%2520middleware%252C%2520and%2520preliminary%2520results%2520have%250Abeen%2520obtained%2520by%2520testing%2520COR-MP%2520on%2520a%2520real%2520vehicle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COR-MP%3A%20Conservation%20of%20Resources%20Model%20for%20Maneuver%20Planning&entry.906535625=Karim%20Essalmi%20and%20Fernando%20Garrido%20and%20Fawzi%20Nashashibi&entry.1292438233=%20%20Decision-making%20for%20automated%20driving%20remains%20a%20challenging%20task.%20For%20their%0Aintegration%20into%20real%20platforms%2C%20these%20algorithms%20must%20guarantee%20passenger%0Asafety%20and%20comfort%20while%20ensuring%20interpretability%20and%20an%20appropriate%0Acomputational%20time.%20To%20model%20and%20solve%20this%20decision-making%20problem%2C%20we%20have%0Adeveloped%20a%20novel%20approach%20called%20COR-MP%20%28Conservation%20of%20Resources%20model%20for%0AManeuver%20Planning%29.%20This%20model%20is%20based%20on%20the%20Conservation%20of%20Resources%0Atheory%2C%20a%20psychological%20concept%20applied%20to%20human%20behavior.%20COR-MP%20is%20based%20on%0Avarious%20driving%20parameters%2C%20such%20as%20comfort%2C%20safety%2C%20or%20energy%2C%20and%20provides%20in%0Areal-time%20a%20profit%20value%20that%20enables%20us%20to%20quantify%20the%20impact%20of%20a%20decision%0Aon%20the%20decision-maker.%20Our%20method%20has%20been%20tested%20and%20validated%20through%0Aclosed-loop%20simulations%20using%20RTMaps%20middleware%2C%20and%20preliminary%20results%20have%0Abeen%20obtained%20by%20testing%20COR-MP%20on%20a%20real%20vehicle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19510v1&entry.124074799=Read"},
{"title": "Gradient Descent Efficiency Index", "author": "Aviral Dhingra", "abstract": "  Gradient descent is a widely used iterative algorithm for finding local\nminima in multivariate functions. However, the final iterations often either\novershoot the minima or make minimal progress, making it challenging to\ndetermine an optimal stopping point. This study introduces a new efficiency\nmetric, Ek, designed to quantify the effectiveness of each iteration. The\nproposed metric accounts for both the relative change in error and the\nstability of the loss function across iterations. This measure is particularly\nvaluable in resource-constrained environments, where costs are closely tied to\ntraining time. Experimental validation across multiple datasets and models\ndemonstrates that Ek provides valuable insights into the convergence behavior\nof gradient descent, complementing traditional performance metrics. The index\nhas the potential to guide more informed decisions in the selection and tuning\nof optimization algorithms in machine learning applications and be used to\ncompare the \"effectiveness\" of models relative to each other.\n", "link": "http://arxiv.org/abs/2410.19448v1", "date": "2024-10-25", "relevancy": 2.0104, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4237}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4015}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Descent%20Efficiency%20Index&body=Title%3A%20Gradient%20Descent%20Efficiency%20Index%0AAuthor%3A%20Aviral%20Dhingra%0AAbstract%3A%20%20%20Gradient%20descent%20is%20a%20widely%20used%20iterative%20algorithm%20for%20finding%20local%0Aminima%20in%20multivariate%20functions.%20However%2C%20the%20final%20iterations%20often%20either%0Aovershoot%20the%20minima%20or%20make%20minimal%20progress%2C%20making%20it%20challenging%20to%0Adetermine%20an%20optimal%20stopping%20point.%20This%20study%20introduces%20a%20new%20efficiency%0Ametric%2C%20Ek%2C%20designed%20to%20quantify%20the%20effectiveness%20of%20each%20iteration.%20The%0Aproposed%20metric%20accounts%20for%20both%20the%20relative%20change%20in%20error%20and%20the%0Astability%20of%20the%20loss%20function%20across%20iterations.%20This%20measure%20is%20particularly%0Avaluable%20in%20resource-constrained%20environments%2C%20where%20costs%20are%20closely%20tied%20to%0Atraining%20time.%20Experimental%20validation%20across%20multiple%20datasets%20and%20models%0Ademonstrates%20that%20Ek%20provides%20valuable%20insights%20into%20the%20convergence%20behavior%0Aof%20gradient%20descent%2C%20complementing%20traditional%20performance%20metrics.%20The%20index%0Ahas%20the%20potential%20to%20guide%20more%20informed%20decisions%20in%20the%20selection%20and%20tuning%0Aof%20optimization%20algorithms%20in%20machine%20learning%20applications%20and%20be%20used%20to%0Acompare%20the%20%22effectiveness%22%20of%20models%20relative%20to%20each%20other.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Descent%2520Efficiency%2520Index%26entry.906535625%3DAviral%2520Dhingra%26entry.1292438233%3D%2520%2520Gradient%2520descent%2520is%2520a%2520widely%2520used%2520iterative%2520algorithm%2520for%2520finding%2520local%250Aminima%2520in%2520multivariate%2520functions.%2520However%252C%2520the%2520final%2520iterations%2520often%2520either%250Aovershoot%2520the%2520minima%2520or%2520make%2520minimal%2520progress%252C%2520making%2520it%2520challenging%2520to%250Adetermine%2520an%2520optimal%2520stopping%2520point.%2520This%2520study%2520introduces%2520a%2520new%2520efficiency%250Ametric%252C%2520Ek%252C%2520designed%2520to%2520quantify%2520the%2520effectiveness%2520of%2520each%2520iteration.%2520The%250Aproposed%2520metric%2520accounts%2520for%2520both%2520the%2520relative%2520change%2520in%2520error%2520and%2520the%250Astability%2520of%2520the%2520loss%2520function%2520across%2520iterations.%2520This%2520measure%2520is%2520particularly%250Avaluable%2520in%2520resource-constrained%2520environments%252C%2520where%2520costs%2520are%2520closely%2520tied%2520to%250Atraining%2520time.%2520Experimental%2520validation%2520across%2520multiple%2520datasets%2520and%2520models%250Ademonstrates%2520that%2520Ek%2520provides%2520valuable%2520insights%2520into%2520the%2520convergence%2520behavior%250Aof%2520gradient%2520descent%252C%2520complementing%2520traditional%2520performance%2520metrics.%2520The%2520index%250Ahas%2520the%2520potential%2520to%2520guide%2520more%2520informed%2520decisions%2520in%2520the%2520selection%2520and%2520tuning%250Aof%2520optimization%2520algorithms%2520in%2520machine%2520learning%2520applications%2520and%2520be%2520used%2520to%250Acompare%2520the%2520%2522effectiveness%2522%2520of%2520models%2520relative%2520to%2520each%2520other.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Descent%20Efficiency%20Index&entry.906535625=Aviral%20Dhingra&entry.1292438233=%20%20Gradient%20descent%20is%20a%20widely%20used%20iterative%20algorithm%20for%20finding%20local%0Aminima%20in%20multivariate%20functions.%20However%2C%20the%20final%20iterations%20often%20either%0Aovershoot%20the%20minima%20or%20make%20minimal%20progress%2C%20making%20it%20challenging%20to%0Adetermine%20an%20optimal%20stopping%20point.%20This%20study%20introduces%20a%20new%20efficiency%0Ametric%2C%20Ek%2C%20designed%20to%20quantify%20the%20effectiveness%20of%20each%20iteration.%20The%0Aproposed%20metric%20accounts%20for%20both%20the%20relative%20change%20in%20error%20and%20the%0Astability%20of%20the%20loss%20function%20across%20iterations.%20This%20measure%20is%20particularly%0Avaluable%20in%20resource-constrained%20environments%2C%20where%20costs%20are%20closely%20tied%20to%0Atraining%20time.%20Experimental%20validation%20across%20multiple%20datasets%20and%20models%0Ademonstrates%20that%20Ek%20provides%20valuable%20insights%20into%20the%20convergence%20behavior%0Aof%20gradient%20descent%2C%20complementing%20traditional%20performance%20metrics.%20The%20index%0Ahas%20the%20potential%20to%20guide%20more%20informed%20decisions%20in%20the%20selection%20and%20tuning%0Aof%20optimization%20algorithms%20in%20machine%20learning%20applications%20and%20be%20used%20to%0Acompare%20the%20%22effectiveness%22%20of%20models%20relative%20to%20each%20other.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19448v1&entry.124074799=Read"},
{"title": "Counting Ability of Large Language Models and Impact of Tokenization", "author": "Xiang Zhang and Juntai Cao and Chenyu You", "abstract": "  Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC$^0$, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs.\n", "link": "http://arxiv.org/abs/2410.19730v1", "date": "2024-10-25", "relevancy": 2.009, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5186}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counting%20Ability%20of%20Large%20Language%20Models%20and%20Impact%20of%20Tokenization&body=Title%3A%20Counting%20Ability%20of%20Large%20Language%20Models%20and%20Impact%20of%20Tokenization%0AAuthor%3A%20Xiang%20Zhang%20and%20Juntai%20Cao%20and%20Chenyu%20You%0AAbstract%3A%20%20%20Transformers%2C%20the%20backbone%20of%20modern%20large%20language%20models%20%28LLMs%29%2C%20face%0Ainherent%20architectural%20limitations%20that%20impede%20their%20reasoning%20capabilities.%0AUnlike%20recurrent%20networks%2C%20Transformers%20lack%20recurrent%20connections%2C%20confining%0Athem%20to%20constant-depth%20computation.%20This%20restriction%20places%20them%20in%20the%0Acomplexity%20class%20TC%24%5E0%24%2C%20making%20them%20theoretically%20incapable%20of%20solving%20tasks%0Athat%20demand%20increasingly%20deep%20reasoning%20as%20input%20length%20grows.%20Counting%2C%20a%0Afundamental%20component%20of%20many%20reasoning%20tasks%2C%20also%20requires%20reasoning%20depth%20to%0Agrow%20linearly%20to%20be%20performed%20inductively.%20While%20previous%20studies%20have%0Aestablished%20the%20upper%20limits%20of%20counting%20ability%20in%20Transformer-based%20expert%0Amodels%20%28i.e.%2C%20models%20specifically%20trained%20for%20counting%20tasks%29%2C%20these%20findings%0Ado%20not%20directly%20extend%20to%20general-purpose%20LLMs%20due%20to%20differences%20in%20reasoning%0Amechanisms.%20Recent%20work%20has%20highlighted%20how%20Chain%20of%20Thought%20%28CoT%29%20reasoning%0Acan%20help%20alleviate%20some%20of%20the%20architectural%20limitations%20of%20Transformers%20in%0Acounting%20tasks.%20However%2C%20little%20attention%20has%20been%20paid%20to%20the%20role%20of%0Atokenization%20in%20these%20models.%20Unlike%20expert%20models%20that%20often%20use%0Acharacter-level%20tokenization%2C%20LLMs%20typically%20rely%20on%20byte-level%20%28BPE%29%0Atokenizers%2C%20which%20fundamentally%20alters%20the%20way%20reasoning%20is%20processed.%20Our%20work%0Ainvestigates%20the%20impact%20of%20tokenization%20on%20the%20counting%20abilities%20of%20LLMs%2C%0Auncovering%20substantial%20performance%20variations%20based%20on%20input%20tokenization%0Adifferences.%20We%20provide%20both%20theoretical%20and%20experimental%20analyses%2C%20offering%0Ainsights%20into%20how%20tokenization%20choices%20can%20undermine%20models%27%20theoretical%0Acomputability%2C%20thereby%20inspiring%20the%20design%20of%20new%20tokenization%20methods%20to%0Aenhance%20reasoning%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounting%2520Ability%2520of%2520Large%2520Language%2520Models%2520and%2520Impact%2520of%2520Tokenization%26entry.906535625%3DXiang%2520Zhang%2520and%2520Juntai%2520Cao%2520and%2520Chenyu%2520You%26entry.1292438233%3D%2520%2520Transformers%252C%2520the%2520backbone%2520of%2520modern%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520face%250Ainherent%2520architectural%2520limitations%2520that%2520impede%2520their%2520reasoning%2520capabilities.%250AUnlike%2520recurrent%2520networks%252C%2520Transformers%2520lack%2520recurrent%2520connections%252C%2520confining%250Athem%2520to%2520constant-depth%2520computation.%2520This%2520restriction%2520places%2520them%2520in%2520the%250Acomplexity%2520class%2520TC%2524%255E0%2524%252C%2520making%2520them%2520theoretically%2520incapable%2520of%2520solving%2520tasks%250Athat%2520demand%2520increasingly%2520deep%2520reasoning%2520as%2520input%2520length%2520grows.%2520Counting%252C%2520a%250Afundamental%2520component%2520of%2520many%2520reasoning%2520tasks%252C%2520also%2520requires%2520reasoning%2520depth%2520to%250Agrow%2520linearly%2520to%2520be%2520performed%2520inductively.%2520While%2520previous%2520studies%2520have%250Aestablished%2520the%2520upper%2520limits%2520of%2520counting%2520ability%2520in%2520Transformer-based%2520expert%250Amodels%2520%2528i.e.%252C%2520models%2520specifically%2520trained%2520for%2520counting%2520tasks%2529%252C%2520these%2520findings%250Ado%2520not%2520directly%2520extend%2520to%2520general-purpose%2520LLMs%2520due%2520to%2520differences%2520in%2520reasoning%250Amechanisms.%2520Recent%2520work%2520has%2520highlighted%2520how%2520Chain%2520of%2520Thought%2520%2528CoT%2529%2520reasoning%250Acan%2520help%2520alleviate%2520some%2520of%2520the%2520architectural%2520limitations%2520of%2520Transformers%2520in%250Acounting%2520tasks.%2520However%252C%2520little%2520attention%2520has%2520been%2520paid%2520to%2520the%2520role%2520of%250Atokenization%2520in%2520these%2520models.%2520Unlike%2520expert%2520models%2520that%2520often%2520use%250Acharacter-level%2520tokenization%252C%2520LLMs%2520typically%2520rely%2520on%2520byte-level%2520%2528BPE%2529%250Atokenizers%252C%2520which%2520fundamentally%2520alters%2520the%2520way%2520reasoning%2520is%2520processed.%2520Our%2520work%250Ainvestigates%2520the%2520impact%2520of%2520tokenization%2520on%2520the%2520counting%2520abilities%2520of%2520LLMs%252C%250Auncovering%2520substantial%2520performance%2520variations%2520based%2520on%2520input%2520tokenization%250Adifferences.%2520We%2520provide%2520both%2520theoretical%2520and%2520experimental%2520analyses%252C%2520offering%250Ainsights%2520into%2520how%2520tokenization%2520choices%2520can%2520undermine%2520models%2527%2520theoretical%250Acomputability%252C%2520thereby%2520inspiring%2520the%2520design%2520of%2520new%2520tokenization%2520methods%2520to%250Aenhance%2520reasoning%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counting%20Ability%20of%20Large%20Language%20Models%20and%20Impact%20of%20Tokenization&entry.906535625=Xiang%20Zhang%20and%20Juntai%20Cao%20and%20Chenyu%20You&entry.1292438233=%20%20Transformers%2C%20the%20backbone%20of%20modern%20large%20language%20models%20%28LLMs%29%2C%20face%0Ainherent%20architectural%20limitations%20that%20impede%20their%20reasoning%20capabilities.%0AUnlike%20recurrent%20networks%2C%20Transformers%20lack%20recurrent%20connections%2C%20confining%0Athem%20to%20constant-depth%20computation.%20This%20restriction%20places%20them%20in%20the%0Acomplexity%20class%20TC%24%5E0%24%2C%20making%20them%20theoretically%20incapable%20of%20solving%20tasks%0Athat%20demand%20increasingly%20deep%20reasoning%20as%20input%20length%20grows.%20Counting%2C%20a%0Afundamental%20component%20of%20many%20reasoning%20tasks%2C%20also%20requires%20reasoning%20depth%20to%0Agrow%20linearly%20to%20be%20performed%20inductively.%20While%20previous%20studies%20have%0Aestablished%20the%20upper%20limits%20of%20counting%20ability%20in%20Transformer-based%20expert%0Amodels%20%28i.e.%2C%20models%20specifically%20trained%20for%20counting%20tasks%29%2C%20these%20findings%0Ado%20not%20directly%20extend%20to%20general-purpose%20LLMs%20due%20to%20differences%20in%20reasoning%0Amechanisms.%20Recent%20work%20has%20highlighted%20how%20Chain%20of%20Thought%20%28CoT%29%20reasoning%0Acan%20help%20alleviate%20some%20of%20the%20architectural%20limitations%20of%20Transformers%20in%0Acounting%20tasks.%20However%2C%20little%20attention%20has%20been%20paid%20to%20the%20role%20of%0Atokenization%20in%20these%20models.%20Unlike%20expert%20models%20that%20often%20use%0Acharacter-level%20tokenization%2C%20LLMs%20typically%20rely%20on%20byte-level%20%28BPE%29%0Atokenizers%2C%20which%20fundamentally%20alters%20the%20way%20reasoning%20is%20processed.%20Our%20work%0Ainvestigates%20the%20impact%20of%20tokenization%20on%20the%20counting%20abilities%20of%20LLMs%2C%0Auncovering%20substantial%20performance%20variations%20based%20on%20input%20tokenization%0Adifferences.%20We%20provide%20both%20theoretical%20and%20experimental%20analyses%2C%20offering%0Ainsights%20into%20how%20tokenization%20choices%20can%20undermine%20models%27%20theoretical%0Acomputability%2C%20thereby%20inspiring%20the%20design%20of%20new%20tokenization%20methods%20to%0Aenhance%20reasoning%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19730v1&entry.124074799=Read"},
{"title": "Improving Low-Resource Knowledge Tracing Tasks by Supervised\n  Pre-training and Importance Mechanism Fine-tuning", "author": "Hengyuan Zhang and Zitao Liu and Shuyan Huang and Chenming Shang and Bojun Zhan and Yong Jiang", "abstract": "  Knowledge tracing (KT) aims to estimate student's knowledge mastery based on\ntheir historical interactions. Recently, the deep learning based KT (DLKT)\napproaches have achieved impressive performance in the KT task. These DLKT\nmodels heavily rely on the large number of available student interactions.\nHowever, due to various reasons such as budget constraints and privacy\nconcerns, observed interactions are very limited in many real-world scenarios,\na.k.a, low-resource KT datasets. Directly training a DLKT model on a\nlow-resource KT dataset may lead to overfitting and it is difficult to choose\nthe appropriate deep neural architecture. Therefore, in this paper, we propose\na low-resource KT framework called LoReKT to address above challenges. Inspired\nby the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn\ntransferable parameters and representations from rich-resource KT datasets\nduring the pre-training stage and subsequently facilitate effective adaptation\nto low-resource KT datasets. Specifically, we simplify existing sophisticated\nDLKT model architectures with purely a stack of transformer decoders. We design\nan encoding mechanism to incorporate student interactions from multiple KT data\nsources and develop an importance mechanism to prioritize updating parameters\nwith high importance while constraining less important ones during the\nfine-tuning stage. We evaluate LoReKT on six public KT datasets and\nexperimental results demonstrate the superiority of our approach in terms of\nAUC and Accuracy. To encourage reproducible research, we make our data and code\npublicly available at https://github.com/rattlesnakey/LoReKT.\n", "link": "http://arxiv.org/abs/2403.06725v4", "date": "2024-10-25", "relevancy": 2.0088, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5225}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4932}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Low-Resource%20Knowledge%20Tracing%20Tasks%20by%20Supervised%0A%20%20Pre-training%20and%20Importance%20Mechanism%20Fine-tuning&body=Title%3A%20Improving%20Low-Resource%20Knowledge%20Tracing%20Tasks%20by%20Supervised%0A%20%20Pre-training%20and%20Importance%20Mechanism%20Fine-tuning%0AAuthor%3A%20Hengyuan%20Zhang%20and%20Zitao%20Liu%20and%20Shuyan%20Huang%20and%20Chenming%20Shang%20and%20Bojun%20Zhan%20and%20Yong%20Jiang%0AAbstract%3A%20%20%20Knowledge%20tracing%20%28KT%29%20aims%20to%20estimate%20student%27s%20knowledge%20mastery%20based%20on%0Atheir%20historical%20interactions.%20Recently%2C%20the%20deep%20learning%20based%20KT%20%28DLKT%29%0Aapproaches%20have%20achieved%20impressive%20performance%20in%20the%20KT%20task.%20These%20DLKT%0Amodels%20heavily%20rely%20on%20the%20large%20number%20of%20available%20student%20interactions.%0AHowever%2C%20due%20to%20various%20reasons%20such%20as%20budget%20constraints%20and%20privacy%0Aconcerns%2C%20observed%20interactions%20are%20very%20limited%20in%20many%20real-world%20scenarios%2C%0Aa.k.a%2C%20low-resource%20KT%20datasets.%20Directly%20training%20a%20DLKT%20model%20on%20a%0Alow-resource%20KT%20dataset%20may%20lead%20to%20overfitting%20and%20it%20is%20difficult%20to%20choose%0Athe%20appropriate%20deep%20neural%20architecture.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%0Aa%20low-resource%20KT%20framework%20called%20LoReKT%20to%20address%20above%20challenges.%20Inspired%0Aby%20the%20prevalent%20%22pre-training%20and%20fine-tuning%22%20paradigm%2C%20we%20aim%20to%20learn%0Atransferable%20parameters%20and%20representations%20from%20rich-resource%20KT%20datasets%0Aduring%20the%20pre-training%20stage%20and%20subsequently%20facilitate%20effective%20adaptation%0Ato%20low-resource%20KT%20datasets.%20Specifically%2C%20we%20simplify%20existing%20sophisticated%0ADLKT%20model%20architectures%20with%20purely%20a%20stack%20of%20transformer%20decoders.%20We%20design%0Aan%20encoding%20mechanism%20to%20incorporate%20student%20interactions%20from%20multiple%20KT%20data%0Asources%20and%20develop%20an%20importance%20mechanism%20to%20prioritize%20updating%20parameters%0Awith%20high%20importance%20while%20constraining%20less%20important%20ones%20during%20the%0Afine-tuning%20stage.%20We%20evaluate%20LoReKT%20on%20six%20public%20KT%20datasets%20and%0Aexperimental%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20in%20terms%20of%0AAUC%20and%20Accuracy.%20To%20encourage%20reproducible%20research%2C%20we%20make%20our%20data%20and%20code%0Apublicly%20available%20at%20https%3A//github.com/rattlesnakey/LoReKT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06725v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Low-Resource%2520Knowledge%2520Tracing%2520Tasks%2520by%2520Supervised%250A%2520%2520Pre-training%2520and%2520Importance%2520Mechanism%2520Fine-tuning%26entry.906535625%3DHengyuan%2520Zhang%2520and%2520Zitao%2520Liu%2520and%2520Shuyan%2520Huang%2520and%2520Chenming%2520Shang%2520and%2520Bojun%2520Zhan%2520and%2520Yong%2520Jiang%26entry.1292438233%3D%2520%2520Knowledge%2520tracing%2520%2528KT%2529%2520aims%2520to%2520estimate%2520student%2527s%2520knowledge%2520mastery%2520based%2520on%250Atheir%2520historical%2520interactions.%2520Recently%252C%2520the%2520deep%2520learning%2520based%2520KT%2520%2528DLKT%2529%250Aapproaches%2520have%2520achieved%2520impressive%2520performance%2520in%2520the%2520KT%2520task.%2520These%2520DLKT%250Amodels%2520heavily%2520rely%2520on%2520the%2520large%2520number%2520of%2520available%2520student%2520interactions.%250AHowever%252C%2520due%2520to%2520various%2520reasons%2520such%2520as%2520budget%2520constraints%2520and%2520privacy%250Aconcerns%252C%2520observed%2520interactions%2520are%2520very%2520limited%2520in%2520many%2520real-world%2520scenarios%252C%250Aa.k.a%252C%2520low-resource%2520KT%2520datasets.%2520Directly%2520training%2520a%2520DLKT%2520model%2520on%2520a%250Alow-resource%2520KT%2520dataset%2520may%2520lead%2520to%2520overfitting%2520and%2520it%2520is%2520difficult%2520to%2520choose%250Athe%2520appropriate%2520deep%2520neural%2520architecture.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520propose%250Aa%2520low-resource%2520KT%2520framework%2520called%2520LoReKT%2520to%2520address%2520above%2520challenges.%2520Inspired%250Aby%2520the%2520prevalent%2520%2522pre-training%2520and%2520fine-tuning%2522%2520paradigm%252C%2520we%2520aim%2520to%2520learn%250Atransferable%2520parameters%2520and%2520representations%2520from%2520rich-resource%2520KT%2520datasets%250Aduring%2520the%2520pre-training%2520stage%2520and%2520subsequently%2520facilitate%2520effective%2520adaptation%250Ato%2520low-resource%2520KT%2520datasets.%2520Specifically%252C%2520we%2520simplify%2520existing%2520sophisticated%250ADLKT%2520model%2520architectures%2520with%2520purely%2520a%2520stack%2520of%2520transformer%2520decoders.%2520We%2520design%250Aan%2520encoding%2520mechanism%2520to%2520incorporate%2520student%2520interactions%2520from%2520multiple%2520KT%2520data%250Asources%2520and%2520develop%2520an%2520importance%2520mechanism%2520to%2520prioritize%2520updating%2520parameters%250Awith%2520high%2520importance%2520while%2520constraining%2520less%2520important%2520ones%2520during%2520the%250Afine-tuning%2520stage.%2520We%2520evaluate%2520LoReKT%2520on%2520six%2520public%2520KT%2520datasets%2520and%250Aexperimental%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520in%2520terms%2520of%250AAUC%2520and%2520Accuracy.%2520To%2520encourage%2520reproducible%2520research%252C%2520we%2520make%2520our%2520data%2520and%2520code%250Apublicly%2520available%2520at%2520https%253A//github.com/rattlesnakey/LoReKT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06725v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Low-Resource%20Knowledge%20Tracing%20Tasks%20by%20Supervised%0A%20%20Pre-training%20and%20Importance%20Mechanism%20Fine-tuning&entry.906535625=Hengyuan%20Zhang%20and%20Zitao%20Liu%20and%20Shuyan%20Huang%20and%20Chenming%20Shang%20and%20Bojun%20Zhan%20and%20Yong%20Jiang&entry.1292438233=%20%20Knowledge%20tracing%20%28KT%29%20aims%20to%20estimate%20student%27s%20knowledge%20mastery%20based%20on%0Atheir%20historical%20interactions.%20Recently%2C%20the%20deep%20learning%20based%20KT%20%28DLKT%29%0Aapproaches%20have%20achieved%20impressive%20performance%20in%20the%20KT%20task.%20These%20DLKT%0Amodels%20heavily%20rely%20on%20the%20large%20number%20of%20available%20student%20interactions.%0AHowever%2C%20due%20to%20various%20reasons%20such%20as%20budget%20constraints%20and%20privacy%0Aconcerns%2C%20observed%20interactions%20are%20very%20limited%20in%20many%20real-world%20scenarios%2C%0Aa.k.a%2C%20low-resource%20KT%20datasets.%20Directly%20training%20a%20DLKT%20model%20on%20a%0Alow-resource%20KT%20dataset%20may%20lead%20to%20overfitting%20and%20it%20is%20difficult%20to%20choose%0Athe%20appropriate%20deep%20neural%20architecture.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%0Aa%20low-resource%20KT%20framework%20called%20LoReKT%20to%20address%20above%20challenges.%20Inspired%0Aby%20the%20prevalent%20%22pre-training%20and%20fine-tuning%22%20paradigm%2C%20we%20aim%20to%20learn%0Atransferable%20parameters%20and%20representations%20from%20rich-resource%20KT%20datasets%0Aduring%20the%20pre-training%20stage%20and%20subsequently%20facilitate%20effective%20adaptation%0Ato%20low-resource%20KT%20datasets.%20Specifically%2C%20we%20simplify%20existing%20sophisticated%0ADLKT%20model%20architectures%20with%20purely%20a%20stack%20of%20transformer%20decoders.%20We%20design%0Aan%20encoding%20mechanism%20to%20incorporate%20student%20interactions%20from%20multiple%20KT%20data%0Asources%20and%20develop%20an%20importance%20mechanism%20to%20prioritize%20updating%20parameters%0Awith%20high%20importance%20while%20constraining%20less%20important%20ones%20during%20the%0Afine-tuning%20stage.%20We%20evaluate%20LoReKT%20on%20six%20public%20KT%20datasets%20and%0Aexperimental%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20in%20terms%20of%0AAUC%20and%20Accuracy.%20To%20encourage%20reproducible%20research%2C%20we%20make%20our%20data%20and%20code%0Apublicly%20available%20at%20https%3A//github.com/rattlesnakey/LoReKT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06725v4&entry.124074799=Read"},
{"title": "Adaptive High-Frequency Transformer for Diverse Wildlife\n  Re-Identification", "author": "Chenyue Li and Shuoyi Chen and Mang Ye", "abstract": "  Wildlife ReID involves utilizing visual technology to identify specific\nindividuals of wild animals in different scenarios, holding significant\nimportance for wildlife conservation, ecological research, and environmental\nmonitoring. Existing wildlife ReID methods are predominantly tailored to\nspecific species, exhibiting limited applicability. Although some approaches\nleverage extensively studied person ReID techniques, they struggle to address\nthe unique challenges posed by wildlife. Therefore, in this paper, we present a\nunified, multi-species general framework for wildlife ReID. Given that\nhigh-frequency information is a consistent representation of unique features in\nvarious species, significantly aiding in identifying contours and details such\nas fur textures, we propose the Adaptive High-Frequency Transformer model with\nthe goal of enhancing high-frequency information learning. To mitigate the\ninevitable high-frequency interference in the wilderness environment, we\nintroduce an object-aware high-frequency selection strategy to adaptively\ncapture more valuable high-frequency components. Notably, we unify the\nexperimental settings of multiple wildlife datasets for ReID, achieving\nsuperior performance over state-of-the-art ReID methods. In domain\ngeneralization scenarios, our approach demonstrates robust generalization to\nunknown species.\n", "link": "http://arxiv.org/abs/2410.06977v2", "date": "2024-10-25", "relevancy": 2.003, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5133}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4986}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20High-Frequency%20Transformer%20for%20Diverse%20Wildlife%0A%20%20Re-Identification&body=Title%3A%20Adaptive%20High-Frequency%20Transformer%20for%20Diverse%20Wildlife%0A%20%20Re-Identification%0AAuthor%3A%20Chenyue%20Li%20and%20Shuoyi%20Chen%20and%20Mang%20Ye%0AAbstract%3A%20%20%20Wildlife%20ReID%20involves%20utilizing%20visual%20technology%20to%20identify%20specific%0Aindividuals%20of%20wild%20animals%20in%20different%20scenarios%2C%20holding%20significant%0Aimportance%20for%20wildlife%20conservation%2C%20ecological%20research%2C%20and%20environmental%0Amonitoring.%20Existing%20wildlife%20ReID%20methods%20are%20predominantly%20tailored%20to%0Aspecific%20species%2C%20exhibiting%20limited%20applicability.%20Although%20some%20approaches%0Aleverage%20extensively%20studied%20person%20ReID%20techniques%2C%20they%20struggle%20to%20address%0Athe%20unique%20challenges%20posed%20by%20wildlife.%20Therefore%2C%20in%20this%20paper%2C%20we%20present%20a%0Aunified%2C%20multi-species%20general%20framework%20for%20wildlife%20ReID.%20Given%20that%0Ahigh-frequency%20information%20is%20a%20consistent%20representation%20of%20unique%20features%20in%0Avarious%20species%2C%20significantly%20aiding%20in%20identifying%20contours%20and%20details%20such%0Aas%20fur%20textures%2C%20we%20propose%20the%20Adaptive%20High-Frequency%20Transformer%20model%20with%0Athe%20goal%20of%20enhancing%20high-frequency%20information%20learning.%20To%20mitigate%20the%0Ainevitable%20high-frequency%20interference%20in%20the%20wilderness%20environment%2C%20we%0Aintroduce%20an%20object-aware%20high-frequency%20selection%20strategy%20to%20adaptively%0Acapture%20more%20valuable%20high-frequency%20components.%20Notably%2C%20we%20unify%20the%0Aexperimental%20settings%20of%20multiple%20wildlife%20datasets%20for%20ReID%2C%20achieving%0Asuperior%20performance%20over%20state-of-the-art%20ReID%20methods.%20In%20domain%0Ageneralization%20scenarios%2C%20our%20approach%20demonstrates%20robust%20generalization%20to%0Aunknown%20species.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520High-Frequency%2520Transformer%2520for%2520Diverse%2520Wildlife%250A%2520%2520Re-Identification%26entry.906535625%3DChenyue%2520Li%2520and%2520Shuoyi%2520Chen%2520and%2520Mang%2520Ye%26entry.1292438233%3D%2520%2520Wildlife%2520ReID%2520involves%2520utilizing%2520visual%2520technology%2520to%2520identify%2520specific%250Aindividuals%2520of%2520wild%2520animals%2520in%2520different%2520scenarios%252C%2520holding%2520significant%250Aimportance%2520for%2520wildlife%2520conservation%252C%2520ecological%2520research%252C%2520and%2520environmental%250Amonitoring.%2520Existing%2520wildlife%2520ReID%2520methods%2520are%2520predominantly%2520tailored%2520to%250Aspecific%2520species%252C%2520exhibiting%2520limited%2520applicability.%2520Although%2520some%2520approaches%250Aleverage%2520extensively%2520studied%2520person%2520ReID%2520techniques%252C%2520they%2520struggle%2520to%2520address%250Athe%2520unique%2520challenges%2520posed%2520by%2520wildlife.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520present%2520a%250Aunified%252C%2520multi-species%2520general%2520framework%2520for%2520wildlife%2520ReID.%2520Given%2520that%250Ahigh-frequency%2520information%2520is%2520a%2520consistent%2520representation%2520of%2520unique%2520features%2520in%250Avarious%2520species%252C%2520significantly%2520aiding%2520in%2520identifying%2520contours%2520and%2520details%2520such%250Aas%2520fur%2520textures%252C%2520we%2520propose%2520the%2520Adaptive%2520High-Frequency%2520Transformer%2520model%2520with%250Athe%2520goal%2520of%2520enhancing%2520high-frequency%2520information%2520learning.%2520To%2520mitigate%2520the%250Ainevitable%2520high-frequency%2520interference%2520in%2520the%2520wilderness%2520environment%252C%2520we%250Aintroduce%2520an%2520object-aware%2520high-frequency%2520selection%2520strategy%2520to%2520adaptively%250Acapture%2520more%2520valuable%2520high-frequency%2520components.%2520Notably%252C%2520we%2520unify%2520the%250Aexperimental%2520settings%2520of%2520multiple%2520wildlife%2520datasets%2520for%2520ReID%252C%2520achieving%250Asuperior%2520performance%2520over%2520state-of-the-art%2520ReID%2520methods.%2520In%2520domain%250Ageneralization%2520scenarios%252C%2520our%2520approach%2520demonstrates%2520robust%2520generalization%2520to%250Aunknown%2520species.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20High-Frequency%20Transformer%20for%20Diverse%20Wildlife%0A%20%20Re-Identification&entry.906535625=Chenyue%20Li%20and%20Shuoyi%20Chen%20and%20Mang%20Ye&entry.1292438233=%20%20Wildlife%20ReID%20involves%20utilizing%20visual%20technology%20to%20identify%20specific%0Aindividuals%20of%20wild%20animals%20in%20different%20scenarios%2C%20holding%20significant%0Aimportance%20for%20wildlife%20conservation%2C%20ecological%20research%2C%20and%20environmental%0Amonitoring.%20Existing%20wildlife%20ReID%20methods%20are%20predominantly%20tailored%20to%0Aspecific%20species%2C%20exhibiting%20limited%20applicability.%20Although%20some%20approaches%0Aleverage%20extensively%20studied%20person%20ReID%20techniques%2C%20they%20struggle%20to%20address%0Athe%20unique%20challenges%20posed%20by%20wildlife.%20Therefore%2C%20in%20this%20paper%2C%20we%20present%20a%0Aunified%2C%20multi-species%20general%20framework%20for%20wildlife%20ReID.%20Given%20that%0Ahigh-frequency%20information%20is%20a%20consistent%20representation%20of%20unique%20features%20in%0Avarious%20species%2C%20significantly%20aiding%20in%20identifying%20contours%20and%20details%20such%0Aas%20fur%20textures%2C%20we%20propose%20the%20Adaptive%20High-Frequency%20Transformer%20model%20with%0Athe%20goal%20of%20enhancing%20high-frequency%20information%20learning.%20To%20mitigate%20the%0Ainevitable%20high-frequency%20interference%20in%20the%20wilderness%20environment%2C%20we%0Aintroduce%20an%20object-aware%20high-frequency%20selection%20strategy%20to%20adaptively%0Acapture%20more%20valuable%20high-frequency%20components.%20Notably%2C%20we%20unify%20the%0Aexperimental%20settings%20of%20multiple%20wildlife%20datasets%20for%20ReID%2C%20achieving%0Asuperior%20performance%20over%20state-of-the-art%20ReID%20methods.%20In%20domain%0Ageneralization%20scenarios%2C%20our%20approach%20demonstrates%20robust%20generalization%20to%0Aunknown%20species.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06977v2&entry.124074799=Read"},
{"title": "What Variables Affect Out-of-Distribution Generalization in Pretrained\n  Models?", "author": "Md Yousuf Harun and Kyungbok Lee and Jhair Gallardo and Giri Krishnan and Christopher Kanan", "abstract": "  Embeddings produced by pre-trained deep neural networks (DNNs) are widely\nused; however, their efficacy for downstream tasks can vary widely. We study\nthe factors influencing transferability and out-of-distribution (OOD)\ngeneralization of pre-trained DNN embeddings through the lens of the tunnel\neffect hypothesis, which is closely related to intermediate neural collapse.\nThis hypothesis suggests that deeper DNN layers compress representations and\nhinder OOD generalization. Contrary to earlier work, our experiments show this\nis not a universal phenomenon. We comprehensively investigate the impact of DNN\narchitecture, training data, image resolution, and augmentations on\ntransferability. We identify that training with high-resolution datasets\ncontaining many classes greatly reduces representation compression and improves\ntransferability. Our results emphasize the danger of generalizing findings from\ntoy datasets to broader contexts.\n", "link": "http://arxiv.org/abs/2405.15018v3", "date": "2024-10-25", "relevancy": 2.0017, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5041}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5018}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Variables%20Affect%20Out-of-Distribution%20Generalization%20in%20Pretrained%0A%20%20Models%3F&body=Title%3A%20What%20Variables%20Affect%20Out-of-Distribution%20Generalization%20in%20Pretrained%0A%20%20Models%3F%0AAuthor%3A%20Md%20Yousuf%20Harun%20and%20Kyungbok%20Lee%20and%20Jhair%20Gallardo%20and%20Giri%20Krishnan%20and%20Christopher%20Kanan%0AAbstract%3A%20%20%20Embeddings%20produced%20by%20pre-trained%20deep%20neural%20networks%20%28DNNs%29%20are%20widely%0Aused%3B%20however%2C%20their%20efficacy%20for%20downstream%20tasks%20can%20vary%20widely.%20We%20study%0Athe%20factors%20influencing%20transferability%20and%20out-of-distribution%20%28OOD%29%0Ageneralization%20of%20pre-trained%20DNN%20embeddings%20through%20the%20lens%20of%20the%20tunnel%0Aeffect%20hypothesis%2C%20which%20is%20closely%20related%20to%20intermediate%20neural%20collapse.%0AThis%20hypothesis%20suggests%20that%20deeper%20DNN%20layers%20compress%20representations%20and%0Ahinder%20OOD%20generalization.%20Contrary%20to%20earlier%20work%2C%20our%20experiments%20show%20this%0Ais%20not%20a%20universal%20phenomenon.%20We%20comprehensively%20investigate%20the%20impact%20of%20DNN%0Aarchitecture%2C%20training%20data%2C%20image%20resolution%2C%20and%20augmentations%20on%0Atransferability.%20We%20identify%20that%20training%20with%20high-resolution%20datasets%0Acontaining%20many%20classes%20greatly%20reduces%20representation%20compression%20and%20improves%0Atransferability.%20Our%20results%20emphasize%20the%20danger%20of%20generalizing%20findings%20from%0Atoy%20datasets%20to%20broader%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15018v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Variables%2520Affect%2520Out-of-Distribution%2520Generalization%2520in%2520Pretrained%250A%2520%2520Models%253F%26entry.906535625%3DMd%2520Yousuf%2520Harun%2520and%2520Kyungbok%2520Lee%2520and%2520Jhair%2520Gallardo%2520and%2520Giri%2520Krishnan%2520and%2520Christopher%2520Kanan%26entry.1292438233%3D%2520%2520Embeddings%2520produced%2520by%2520pre-trained%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520widely%250Aused%253B%2520however%252C%2520their%2520efficacy%2520for%2520downstream%2520tasks%2520can%2520vary%2520widely.%2520We%2520study%250Athe%2520factors%2520influencing%2520transferability%2520and%2520out-of-distribution%2520%2528OOD%2529%250Ageneralization%2520of%2520pre-trained%2520DNN%2520embeddings%2520through%2520the%2520lens%2520of%2520the%2520tunnel%250Aeffect%2520hypothesis%252C%2520which%2520is%2520closely%2520related%2520to%2520intermediate%2520neural%2520collapse.%250AThis%2520hypothesis%2520suggests%2520that%2520deeper%2520DNN%2520layers%2520compress%2520representations%2520and%250Ahinder%2520OOD%2520generalization.%2520Contrary%2520to%2520earlier%2520work%252C%2520our%2520experiments%2520show%2520this%250Ais%2520not%2520a%2520universal%2520phenomenon.%2520We%2520comprehensively%2520investigate%2520the%2520impact%2520of%2520DNN%250Aarchitecture%252C%2520training%2520data%252C%2520image%2520resolution%252C%2520and%2520augmentations%2520on%250Atransferability.%2520We%2520identify%2520that%2520training%2520with%2520high-resolution%2520datasets%250Acontaining%2520many%2520classes%2520greatly%2520reduces%2520representation%2520compression%2520and%2520improves%250Atransferability.%2520Our%2520results%2520emphasize%2520the%2520danger%2520of%2520generalizing%2520findings%2520from%250Atoy%2520datasets%2520to%2520broader%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15018v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Variables%20Affect%20Out-of-Distribution%20Generalization%20in%20Pretrained%0A%20%20Models%3F&entry.906535625=Md%20Yousuf%20Harun%20and%20Kyungbok%20Lee%20and%20Jhair%20Gallardo%20and%20Giri%20Krishnan%20and%20Christopher%20Kanan&entry.1292438233=%20%20Embeddings%20produced%20by%20pre-trained%20deep%20neural%20networks%20%28DNNs%29%20are%20widely%0Aused%3B%20however%2C%20their%20efficacy%20for%20downstream%20tasks%20can%20vary%20widely.%20We%20study%0Athe%20factors%20influencing%20transferability%20and%20out-of-distribution%20%28OOD%29%0Ageneralization%20of%20pre-trained%20DNN%20embeddings%20through%20the%20lens%20of%20the%20tunnel%0Aeffect%20hypothesis%2C%20which%20is%20closely%20related%20to%20intermediate%20neural%20collapse.%0AThis%20hypothesis%20suggests%20that%20deeper%20DNN%20layers%20compress%20representations%20and%0Ahinder%20OOD%20generalization.%20Contrary%20to%20earlier%20work%2C%20our%20experiments%20show%20this%0Ais%20not%20a%20universal%20phenomenon.%20We%20comprehensively%20investigate%20the%20impact%20of%20DNN%0Aarchitecture%2C%20training%20data%2C%20image%20resolution%2C%20and%20augmentations%20on%0Atransferability.%20We%20identify%20that%20training%20with%20high-resolution%20datasets%0Acontaining%20many%20classes%20greatly%20reduces%20representation%20compression%20and%20improves%0Atransferability.%20Our%20results%20emphasize%20the%20danger%20of%20generalizing%20findings%20from%0Atoy%20datasets%20to%20broader%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15018v3&entry.124074799=Read"},
{"title": "x-RAGE: eXtended Reality -- Action & Gesture Events Dataset", "author": "Vivek Parmar and Dwijay Bane and Syed Shakib Sarwar and Kleber Stangherlin and Barbara De Salvo and Manan Suri", "abstract": "  With the emergence of the Metaverse and focus on wearable devices in the\nrecent years gesture based human-computer interaction has gained significance.\nTo enable gesture recognition for VR/AR headsets and glasses several datasets\nfocusing on egocentric i.e. first-person view have emerged in recent years.\nHowever, standard frame-based vision suffers from limitations in data bandwidth\nrequirements as well as ability to capture fast motions. To overcome these\nlimitation bio-inspired approaches such as event-based cameras present an\nattractive alternative. In this work, we present the first event-camera based\negocentric gesture dataset for enabling neuromorphic, low-power solutions for\nXR-centric gesture recognition. The dataset has been made available publicly at\nthe following URL: https://gitlab.com/NVM_IITD_Research/xrage.\n", "link": "http://arxiv.org/abs/2410.19486v1", "date": "2024-10-25", "relevancy": 1.9837, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5129}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4857}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20x-RAGE%3A%20eXtended%20Reality%20--%20Action%20%26%20Gesture%20Events%20Dataset&body=Title%3A%20x-RAGE%3A%20eXtended%20Reality%20--%20Action%20%26%20Gesture%20Events%20Dataset%0AAuthor%3A%20Vivek%20Parmar%20and%20Dwijay%20Bane%20and%20Syed%20Shakib%20Sarwar%20and%20Kleber%20Stangherlin%20and%20Barbara%20De%20Salvo%20and%20Manan%20Suri%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20the%20Metaverse%20and%20focus%20on%20wearable%20devices%20in%20the%0Arecent%20years%20gesture%20based%20human-computer%20interaction%20has%20gained%20significance.%0ATo%20enable%20gesture%20recognition%20for%20VR/AR%20headsets%20and%20glasses%20several%20datasets%0Afocusing%20on%20egocentric%20i.e.%20first-person%20view%20have%20emerged%20in%20recent%20years.%0AHowever%2C%20standard%20frame-based%20vision%20suffers%20from%20limitations%20in%20data%20bandwidth%0Arequirements%20as%20well%20as%20ability%20to%20capture%20fast%20motions.%20To%20overcome%20these%0Alimitation%20bio-inspired%20approaches%20such%20as%20event-based%20cameras%20present%20an%0Aattractive%20alternative.%20In%20this%20work%2C%20we%20present%20the%20first%20event-camera%20based%0Aegocentric%20gesture%20dataset%20for%20enabling%20neuromorphic%2C%20low-power%20solutions%20for%0AXR-centric%20gesture%20recognition.%20The%20dataset%20has%20been%20made%20available%20publicly%20at%0Athe%20following%20URL%3A%20https%3A//gitlab.com/NVM_IITD_Research/xrage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dx-RAGE%253A%2520eXtended%2520Reality%2520--%2520Action%2520%2526%2520Gesture%2520Events%2520Dataset%26entry.906535625%3DVivek%2520Parmar%2520and%2520Dwijay%2520Bane%2520and%2520Syed%2520Shakib%2520Sarwar%2520and%2520Kleber%2520Stangherlin%2520and%2520Barbara%2520De%2520Salvo%2520and%2520Manan%2520Suri%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520the%2520Metaverse%2520and%2520focus%2520on%2520wearable%2520devices%2520in%2520the%250Arecent%2520years%2520gesture%2520based%2520human-computer%2520interaction%2520has%2520gained%2520significance.%250ATo%2520enable%2520gesture%2520recognition%2520for%2520VR/AR%2520headsets%2520and%2520glasses%2520several%2520datasets%250Afocusing%2520on%2520egocentric%2520i.e.%2520first-person%2520view%2520have%2520emerged%2520in%2520recent%2520years.%250AHowever%252C%2520standard%2520frame-based%2520vision%2520suffers%2520from%2520limitations%2520in%2520data%2520bandwidth%250Arequirements%2520as%2520well%2520as%2520ability%2520to%2520capture%2520fast%2520motions.%2520To%2520overcome%2520these%250Alimitation%2520bio-inspired%2520approaches%2520such%2520as%2520event-based%2520cameras%2520present%2520an%250Aattractive%2520alternative.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520event-camera%2520based%250Aegocentric%2520gesture%2520dataset%2520for%2520enabling%2520neuromorphic%252C%2520low-power%2520solutions%2520for%250AXR-centric%2520gesture%2520recognition.%2520The%2520dataset%2520has%2520been%2520made%2520available%2520publicly%2520at%250Athe%2520following%2520URL%253A%2520https%253A//gitlab.com/NVM_IITD_Research/xrage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=x-RAGE%3A%20eXtended%20Reality%20--%20Action%20%26%20Gesture%20Events%20Dataset&entry.906535625=Vivek%20Parmar%20and%20Dwijay%20Bane%20and%20Syed%20Shakib%20Sarwar%20and%20Kleber%20Stangherlin%20and%20Barbara%20De%20Salvo%20and%20Manan%20Suri&entry.1292438233=%20%20With%20the%20emergence%20of%20the%20Metaverse%20and%20focus%20on%20wearable%20devices%20in%20the%0Arecent%20years%20gesture%20based%20human-computer%20interaction%20has%20gained%20significance.%0ATo%20enable%20gesture%20recognition%20for%20VR/AR%20headsets%20and%20glasses%20several%20datasets%0Afocusing%20on%20egocentric%20i.e.%20first-person%20view%20have%20emerged%20in%20recent%20years.%0AHowever%2C%20standard%20frame-based%20vision%20suffers%20from%20limitations%20in%20data%20bandwidth%0Arequirements%20as%20well%20as%20ability%20to%20capture%20fast%20motions.%20To%20overcome%20these%0Alimitation%20bio-inspired%20approaches%20such%20as%20event-based%20cameras%20present%20an%0Aattractive%20alternative.%20In%20this%20work%2C%20we%20present%20the%20first%20event-camera%20based%0Aegocentric%20gesture%20dataset%20for%20enabling%20neuromorphic%2C%20low-power%20solutions%20for%0AXR-centric%20gesture%20recognition.%20The%20dataset%20has%20been%20made%20available%20publicly%20at%0Athe%20following%20URL%3A%20https%3A//gitlab.com/NVM_IITD_Research/xrage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19486v1&entry.124074799=Read"},
{"title": "Equilibrium Adaptation-Based Control for Track Stand of Single-Track\n  Two-Wheeled Robots", "author": "Boyi Wang and Yang Deng and Feilong Jing and Yiyong Sun and Zhang Chen and Bin Liang", "abstract": "  Stationary balance control is challenging for single-track two-wheeled (STTW)\nrobots due to the lack of elegant balancing mechanisms and the conflict between\nthe limited attraction domain and external disturbances. To address the absence\nof balancing mechanisms, we draw inspiration from cyclists and leverage the\ntrack stand maneuver, which relies solely on steering and rear-wheel actuation.\nTo achieve accurate tracking in the presence of matched and mismatched\ndisturbances, we propose an equilibrium adaptation-based control (EABC) scheme\nthat can be seamlessly integrated with standard disturbance observers and\ncontrollers. This scheme enables adaptation to slow-varying disturbances by\nutilizing a disturbed equilibrium estimator, effectively handling both matched\nand mismatched disturbances in a unified manner while ensuring accurate\ntracking with zero steady-state error. We integrate the EABC scheme with\nnonlinear model predictive control (MPC) for the track stand of STTW robots and\nvalidate its effectiveness through two experimental scenarios. Our method\ndemonstrates significant improvements in tracking accuracy, reducing errors by\nseveral orders of magnitude.\n", "link": "http://arxiv.org/abs/2410.19615v1", "date": "2024-10-25", "relevancy": 1.9768, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5129}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4971}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equilibrium%20Adaptation-Based%20Control%20for%20Track%20Stand%20of%20Single-Track%0A%20%20Two-Wheeled%20Robots&body=Title%3A%20Equilibrium%20Adaptation-Based%20Control%20for%20Track%20Stand%20of%20Single-Track%0A%20%20Two-Wheeled%20Robots%0AAuthor%3A%20Boyi%20Wang%20and%20Yang%20Deng%20and%20Feilong%20Jing%20and%20Yiyong%20Sun%20and%20Zhang%20Chen%20and%20Bin%20Liang%0AAbstract%3A%20%20%20Stationary%20balance%20control%20is%20challenging%20for%20single-track%20two-wheeled%20%28STTW%29%0Arobots%20due%20to%20the%20lack%20of%20elegant%20balancing%20mechanisms%20and%20the%20conflict%20between%0Athe%20limited%20attraction%20domain%20and%20external%20disturbances.%20To%20address%20the%20absence%0Aof%20balancing%20mechanisms%2C%20we%20draw%20inspiration%20from%20cyclists%20and%20leverage%20the%0Atrack%20stand%20maneuver%2C%20which%20relies%20solely%20on%20steering%20and%20rear-wheel%20actuation.%0ATo%20achieve%20accurate%20tracking%20in%20the%20presence%20of%20matched%20and%20mismatched%0Adisturbances%2C%20we%20propose%20an%20equilibrium%20adaptation-based%20control%20%28EABC%29%20scheme%0Athat%20can%20be%20seamlessly%20integrated%20with%20standard%20disturbance%20observers%20and%0Acontrollers.%20This%20scheme%20enables%20adaptation%20to%20slow-varying%20disturbances%20by%0Autilizing%20a%20disturbed%20equilibrium%20estimator%2C%20effectively%20handling%20both%20matched%0Aand%20mismatched%20disturbances%20in%20a%20unified%20manner%20while%20ensuring%20accurate%0Atracking%20with%20zero%20steady-state%20error.%20We%20integrate%20the%20EABC%20scheme%20with%0Anonlinear%20model%20predictive%20control%20%28MPC%29%20for%20the%20track%20stand%20of%20STTW%20robots%20and%0Avalidate%20its%20effectiveness%20through%20two%20experimental%20scenarios.%20Our%20method%0Ademonstrates%20significant%20improvements%20in%20tracking%20accuracy%2C%20reducing%20errors%20by%0Aseveral%20orders%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquilibrium%2520Adaptation-Based%2520Control%2520for%2520Track%2520Stand%2520of%2520Single-Track%250A%2520%2520Two-Wheeled%2520Robots%26entry.906535625%3DBoyi%2520Wang%2520and%2520Yang%2520Deng%2520and%2520Feilong%2520Jing%2520and%2520Yiyong%2520Sun%2520and%2520Zhang%2520Chen%2520and%2520Bin%2520Liang%26entry.1292438233%3D%2520%2520Stationary%2520balance%2520control%2520is%2520challenging%2520for%2520single-track%2520two-wheeled%2520%2528STTW%2529%250Arobots%2520due%2520to%2520the%2520lack%2520of%2520elegant%2520balancing%2520mechanisms%2520and%2520the%2520conflict%2520between%250Athe%2520limited%2520attraction%2520domain%2520and%2520external%2520disturbances.%2520To%2520address%2520the%2520absence%250Aof%2520balancing%2520mechanisms%252C%2520we%2520draw%2520inspiration%2520from%2520cyclists%2520and%2520leverage%2520the%250Atrack%2520stand%2520maneuver%252C%2520which%2520relies%2520solely%2520on%2520steering%2520and%2520rear-wheel%2520actuation.%250ATo%2520achieve%2520accurate%2520tracking%2520in%2520the%2520presence%2520of%2520matched%2520and%2520mismatched%250Adisturbances%252C%2520we%2520propose%2520an%2520equilibrium%2520adaptation-based%2520control%2520%2528EABC%2529%2520scheme%250Athat%2520can%2520be%2520seamlessly%2520integrated%2520with%2520standard%2520disturbance%2520observers%2520and%250Acontrollers.%2520This%2520scheme%2520enables%2520adaptation%2520to%2520slow-varying%2520disturbances%2520by%250Autilizing%2520a%2520disturbed%2520equilibrium%2520estimator%252C%2520effectively%2520handling%2520both%2520matched%250Aand%2520mismatched%2520disturbances%2520in%2520a%2520unified%2520manner%2520while%2520ensuring%2520accurate%250Atracking%2520with%2520zero%2520steady-state%2520error.%2520We%2520integrate%2520the%2520EABC%2520scheme%2520with%250Anonlinear%2520model%2520predictive%2520control%2520%2528MPC%2529%2520for%2520the%2520track%2520stand%2520of%2520STTW%2520robots%2520and%250Avalidate%2520its%2520effectiveness%2520through%2520two%2520experimental%2520scenarios.%2520Our%2520method%250Ademonstrates%2520significant%2520improvements%2520in%2520tracking%2520accuracy%252C%2520reducing%2520errors%2520by%250Aseveral%2520orders%2520of%2520magnitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equilibrium%20Adaptation-Based%20Control%20for%20Track%20Stand%20of%20Single-Track%0A%20%20Two-Wheeled%20Robots&entry.906535625=Boyi%20Wang%20and%20Yang%20Deng%20and%20Feilong%20Jing%20and%20Yiyong%20Sun%20and%20Zhang%20Chen%20and%20Bin%20Liang&entry.1292438233=%20%20Stationary%20balance%20control%20is%20challenging%20for%20single-track%20two-wheeled%20%28STTW%29%0Arobots%20due%20to%20the%20lack%20of%20elegant%20balancing%20mechanisms%20and%20the%20conflict%20between%0Athe%20limited%20attraction%20domain%20and%20external%20disturbances.%20To%20address%20the%20absence%0Aof%20balancing%20mechanisms%2C%20we%20draw%20inspiration%20from%20cyclists%20and%20leverage%20the%0Atrack%20stand%20maneuver%2C%20which%20relies%20solely%20on%20steering%20and%20rear-wheel%20actuation.%0ATo%20achieve%20accurate%20tracking%20in%20the%20presence%20of%20matched%20and%20mismatched%0Adisturbances%2C%20we%20propose%20an%20equilibrium%20adaptation-based%20control%20%28EABC%29%20scheme%0Athat%20can%20be%20seamlessly%20integrated%20with%20standard%20disturbance%20observers%20and%0Acontrollers.%20This%20scheme%20enables%20adaptation%20to%20slow-varying%20disturbances%20by%0Autilizing%20a%20disturbed%20equilibrium%20estimator%2C%20effectively%20handling%20both%20matched%0Aand%20mismatched%20disturbances%20in%20a%20unified%20manner%20while%20ensuring%20accurate%0Atracking%20with%20zero%20steady-state%20error.%20We%20integrate%20the%20EABC%20scheme%20with%0Anonlinear%20model%20predictive%20control%20%28MPC%29%20for%20the%20track%20stand%20of%20STTW%20robots%20and%0Avalidate%20its%20effectiveness%20through%20two%20experimental%20scenarios.%20Our%20method%0Ademonstrates%20significant%20improvements%20in%20tracking%20accuracy%2C%20reducing%20errors%20by%0Aseveral%20orders%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19615v1&entry.124074799=Read"},
{"title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little\n  Overhead", "author": "Rickard Br\u00fcel-Gabrielsson and Jiacheng Zhu and Onkar Bhardwaj and Leshem Choshen and Kristjan Greenewald and Mikhail Yurochkin and Justin Solomon", "abstract": "  Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of model compression when serving LoRAs. We propose a method for joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are more\namenable to joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 500 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA.\n", "link": "http://arxiv.org/abs/2407.00066v2", "date": "2024-10-25", "relevancy": 1.9761, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4983}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.498}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compress%20then%20Serve%3A%20Serving%20Thousands%20of%20LoRA%20Adapters%20with%20Little%0A%20%20Overhead&body=Title%3A%20Compress%20then%20Serve%3A%20Serving%20Thousands%20of%20LoRA%20Adapters%20with%20Little%0A%20%20Overhead%0AAuthor%3A%20Rickard%20Br%C3%BCel-Gabrielsson%20and%20Jiacheng%20Zhu%20and%20Onkar%20Bhardwaj%20and%20Leshem%20Choshen%20and%20Kristjan%20Greenewald%20and%20Mikhail%20Yurochkin%20and%20Justin%20Solomon%0AAbstract%3A%20%20%20Fine-tuning%20large%20language%20models%20%28LLMs%29%20with%20low-rank%20adaptations%20%28LoRAs%29%0Ahas%20become%20common%20practice%2C%20often%20yielding%20numerous%20copies%20of%20the%20same%20LLM%0Adiffering%20only%20in%20their%20LoRA%20updates.%20This%20paradigm%20presents%20challenges%20for%0Asystems%20that%20serve%20real-time%20responses%20to%20queries%20that%20each%20involve%20a%20different%0ALoRA.%20Prior%20works%20optimize%20the%20design%20of%20such%20systems%20but%20still%20require%0Acontinuous%20loading%20and%20offloading%20of%20LoRAs%2C%20as%20it%20is%20infeasible%20to%20store%0Athousands%20of%20LoRAs%20in%20GPU%20memory.%20To%20mitigate%20this%20issue%2C%20we%20investigate%20the%0Aefficacy%20of%20model%20compression%20when%20serving%20LoRAs.%20We%20propose%20a%20method%20for%20joint%0Acompression%20of%20LoRAs%20into%20a%20shared%20basis%20paired%20with%20LoRA-specific%20scaling%0Amatrices.%20We%20extend%20our%20algorithm%20to%20learn%20clusters%20of%20LoRAs%20that%20are%20more%0Aamenable%20to%20joint%20compression%2C%20allowing%20it%20to%20scale%20gracefully%20to%20large%20LoRA%0Acollections.%20Our%20experiments%20with%20up%20to%20500%20LoRAs%20demonstrate%20that%20compressed%0ALoRAs%20preserve%20performance%20while%20offering%20major%20throughput%20gains%20in%20realistic%0Aserving%20scenarios%20with%20over%20a%20thousand%20LoRAs%2C%20maintaining%2080%25%20of%20the%20throughput%0Aof%20serving%20a%20single%20LoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00066v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompress%2520then%2520Serve%253A%2520Serving%2520Thousands%2520of%2520LoRA%2520Adapters%2520with%2520Little%250A%2520%2520Overhead%26entry.906535625%3DRickard%2520Br%25C3%25BCel-Gabrielsson%2520and%2520Jiacheng%2520Zhu%2520and%2520Onkar%2520Bhardwaj%2520and%2520Leshem%2520Choshen%2520and%2520Kristjan%2520Greenewald%2520and%2520Mikhail%2520Yurochkin%2520and%2520Justin%2520Solomon%26entry.1292438233%3D%2520%2520Fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520low-rank%2520adaptations%2520%2528LoRAs%2529%250Ahas%2520become%2520common%2520practice%252C%2520often%2520yielding%2520numerous%2520copies%2520of%2520the%2520same%2520LLM%250Adiffering%2520only%2520in%2520their%2520LoRA%2520updates.%2520This%2520paradigm%2520presents%2520challenges%2520for%250Asystems%2520that%2520serve%2520real-time%2520responses%2520to%2520queries%2520that%2520each%2520involve%2520a%2520different%250ALoRA.%2520Prior%2520works%2520optimize%2520the%2520design%2520of%2520such%2520systems%2520but%2520still%2520require%250Acontinuous%2520loading%2520and%2520offloading%2520of%2520LoRAs%252C%2520as%2520it%2520is%2520infeasible%2520to%2520store%250Athousands%2520of%2520LoRAs%2520in%2520GPU%2520memory.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520investigate%2520the%250Aefficacy%2520of%2520model%2520compression%2520when%2520serving%2520LoRAs.%2520We%2520propose%2520a%2520method%2520for%2520joint%250Acompression%2520of%2520LoRAs%2520into%2520a%2520shared%2520basis%2520paired%2520with%2520LoRA-specific%2520scaling%250Amatrices.%2520We%2520extend%2520our%2520algorithm%2520to%2520learn%2520clusters%2520of%2520LoRAs%2520that%2520are%2520more%250Aamenable%2520to%2520joint%2520compression%252C%2520allowing%2520it%2520to%2520scale%2520gracefully%2520to%2520large%2520LoRA%250Acollections.%2520Our%2520experiments%2520with%2520up%2520to%2520500%2520LoRAs%2520demonstrate%2520that%2520compressed%250ALoRAs%2520preserve%2520performance%2520while%2520offering%2520major%2520throughput%2520gains%2520in%2520realistic%250Aserving%2520scenarios%2520with%2520over%2520a%2520thousand%2520LoRAs%252C%2520maintaining%252080%2525%2520of%2520the%2520throughput%250Aof%2520serving%2520a%2520single%2520LoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00066v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compress%20then%20Serve%3A%20Serving%20Thousands%20of%20LoRA%20Adapters%20with%20Little%0A%20%20Overhead&entry.906535625=Rickard%20Br%C3%BCel-Gabrielsson%20and%20Jiacheng%20Zhu%20and%20Onkar%20Bhardwaj%20and%20Leshem%20Choshen%20and%20Kristjan%20Greenewald%20and%20Mikhail%20Yurochkin%20and%20Justin%20Solomon&entry.1292438233=%20%20Fine-tuning%20large%20language%20models%20%28LLMs%29%20with%20low-rank%20adaptations%20%28LoRAs%29%0Ahas%20become%20common%20practice%2C%20often%20yielding%20numerous%20copies%20of%20the%20same%20LLM%0Adiffering%20only%20in%20their%20LoRA%20updates.%20This%20paradigm%20presents%20challenges%20for%0Asystems%20that%20serve%20real-time%20responses%20to%20queries%20that%20each%20involve%20a%20different%0ALoRA.%20Prior%20works%20optimize%20the%20design%20of%20such%20systems%20but%20still%20require%0Acontinuous%20loading%20and%20offloading%20of%20LoRAs%2C%20as%20it%20is%20infeasible%20to%20store%0Athousands%20of%20LoRAs%20in%20GPU%20memory.%20To%20mitigate%20this%20issue%2C%20we%20investigate%20the%0Aefficacy%20of%20model%20compression%20when%20serving%20LoRAs.%20We%20propose%20a%20method%20for%20joint%0Acompression%20of%20LoRAs%20into%20a%20shared%20basis%20paired%20with%20LoRA-specific%20scaling%0Amatrices.%20We%20extend%20our%20algorithm%20to%20learn%20clusters%20of%20LoRAs%20that%20are%20more%0Aamenable%20to%20joint%20compression%2C%20allowing%20it%20to%20scale%20gracefully%20to%20large%20LoRA%0Acollections.%20Our%20experiments%20with%20up%20to%20500%20LoRAs%20demonstrate%20that%20compressed%0ALoRAs%20preserve%20performance%20while%20offering%20major%20throughput%20gains%20in%20realistic%0Aserving%20scenarios%20with%20over%20a%20thousand%20LoRAs%2C%20maintaining%2080%25%20of%20the%20throughput%0Aof%20serving%20a%20single%20LoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00066v2&entry.124074799=Read"},
{"title": "Microplastic Identification Using AI-Driven Image Segmentation and\n  GAN-Generated Ecological Context", "author": "Alex Dils and David Raymond and Jack Spottiswood and Samay Kodige and Dylan Karmin and Rikhil Kokal and Win Cowger and Chris Sad\u00e9e", "abstract": "  Current methods for microplastic identification in water samples are costly\nand require expert analysis. Here, we propose a deep learning segmentation\nmodel to automatically identify microplastics in microscopic images. We labeled\nimages of microplastic from the Moore Institute for Plastic Pollution Research\nand employ a Generative Adversarial Network (GAN) to supplement and generate\ndiverse training data. To verify the validity of the generated data, we\nconducted a reader study where an expert was able to discern the generated\nmicroplastic from real microplastic at a rate of 68 percent. Our segmentation\nmodel trained on the combined data achieved an F1-Score of 0.91 on a diverse\ndataset, compared to the model without generated data's 0.82. With our findings\nwe aim to enhance the ability of both experts and citizens to detect\nmicroplastic across diverse ecological contexts, thereby improving the cost and\naccessibility of microplastic analysis.\n", "link": "http://arxiv.org/abs/2410.19604v1", "date": "2024-10-25", "relevancy": 1.9571, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.495}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4908}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Microplastic%20Identification%20Using%20AI-Driven%20Image%20Segmentation%20and%0A%20%20GAN-Generated%20Ecological%20Context&body=Title%3A%20Microplastic%20Identification%20Using%20AI-Driven%20Image%20Segmentation%20and%0A%20%20GAN-Generated%20Ecological%20Context%0AAuthor%3A%20Alex%20Dils%20and%20David%20Raymond%20and%20Jack%20Spottiswood%20and%20Samay%20Kodige%20and%20Dylan%20Karmin%20and%20Rikhil%20Kokal%20and%20Win%20Cowger%20and%20Chris%20Sad%C3%A9e%0AAbstract%3A%20%20%20Current%20methods%20for%20microplastic%20identification%20in%20water%20samples%20are%20costly%0Aand%20require%20expert%20analysis.%20Here%2C%20we%20propose%20a%20deep%20learning%20segmentation%0Amodel%20to%20automatically%20identify%20microplastics%20in%20microscopic%20images.%20We%20labeled%0Aimages%20of%20microplastic%20from%20the%20Moore%20Institute%20for%20Plastic%20Pollution%20Research%0Aand%20employ%20a%20Generative%20Adversarial%20Network%20%28GAN%29%20to%20supplement%20and%20generate%0Adiverse%20training%20data.%20To%20verify%20the%20validity%20of%20the%20generated%20data%2C%20we%0Aconducted%20a%20reader%20study%20where%20an%20expert%20was%20able%20to%20discern%20the%20generated%0Amicroplastic%20from%20real%20microplastic%20at%20a%20rate%20of%2068%20percent.%20Our%20segmentation%0Amodel%20trained%20on%20the%20combined%20data%20achieved%20an%20F1-Score%20of%200.91%20on%20a%20diverse%0Adataset%2C%20compared%20to%20the%20model%20without%20generated%20data%27s%200.82.%20With%20our%20findings%0Awe%20aim%20to%20enhance%20the%20ability%20of%20both%20experts%20and%20citizens%20to%20detect%0Amicroplastic%20across%20diverse%20ecological%20contexts%2C%20thereby%20improving%20the%20cost%20and%0Aaccessibility%20of%20microplastic%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicroplastic%2520Identification%2520Using%2520AI-Driven%2520Image%2520Segmentation%2520and%250A%2520%2520GAN-Generated%2520Ecological%2520Context%26entry.906535625%3DAlex%2520Dils%2520and%2520David%2520Raymond%2520and%2520Jack%2520Spottiswood%2520and%2520Samay%2520Kodige%2520and%2520Dylan%2520Karmin%2520and%2520Rikhil%2520Kokal%2520and%2520Win%2520Cowger%2520and%2520Chris%2520Sad%25C3%25A9e%26entry.1292438233%3D%2520%2520Current%2520methods%2520for%2520microplastic%2520identification%2520in%2520water%2520samples%2520are%2520costly%250Aand%2520require%2520expert%2520analysis.%2520Here%252C%2520we%2520propose%2520a%2520deep%2520learning%2520segmentation%250Amodel%2520to%2520automatically%2520identify%2520microplastics%2520in%2520microscopic%2520images.%2520We%2520labeled%250Aimages%2520of%2520microplastic%2520from%2520the%2520Moore%2520Institute%2520for%2520Plastic%2520Pollution%2520Research%250Aand%2520employ%2520a%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520to%2520supplement%2520and%2520generate%250Adiverse%2520training%2520data.%2520To%2520verify%2520the%2520validity%2520of%2520the%2520generated%2520data%252C%2520we%250Aconducted%2520a%2520reader%2520study%2520where%2520an%2520expert%2520was%2520able%2520to%2520discern%2520the%2520generated%250Amicroplastic%2520from%2520real%2520microplastic%2520at%2520a%2520rate%2520of%252068%2520percent.%2520Our%2520segmentation%250Amodel%2520trained%2520on%2520the%2520combined%2520data%2520achieved%2520an%2520F1-Score%2520of%25200.91%2520on%2520a%2520diverse%250Adataset%252C%2520compared%2520to%2520the%2520model%2520without%2520generated%2520data%2527s%25200.82.%2520With%2520our%2520findings%250Awe%2520aim%2520to%2520enhance%2520the%2520ability%2520of%2520both%2520experts%2520and%2520citizens%2520to%2520detect%250Amicroplastic%2520across%2520diverse%2520ecological%2520contexts%252C%2520thereby%2520improving%2520the%2520cost%2520and%250Aaccessibility%2520of%2520microplastic%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Microplastic%20Identification%20Using%20AI-Driven%20Image%20Segmentation%20and%0A%20%20GAN-Generated%20Ecological%20Context&entry.906535625=Alex%20Dils%20and%20David%20Raymond%20and%20Jack%20Spottiswood%20and%20Samay%20Kodige%20and%20Dylan%20Karmin%20and%20Rikhil%20Kokal%20and%20Win%20Cowger%20and%20Chris%20Sad%C3%A9e&entry.1292438233=%20%20Current%20methods%20for%20microplastic%20identification%20in%20water%20samples%20are%20costly%0Aand%20require%20expert%20analysis.%20Here%2C%20we%20propose%20a%20deep%20learning%20segmentation%0Amodel%20to%20automatically%20identify%20microplastics%20in%20microscopic%20images.%20We%20labeled%0Aimages%20of%20microplastic%20from%20the%20Moore%20Institute%20for%20Plastic%20Pollution%20Research%0Aand%20employ%20a%20Generative%20Adversarial%20Network%20%28GAN%29%20to%20supplement%20and%20generate%0Adiverse%20training%20data.%20To%20verify%20the%20validity%20of%20the%20generated%20data%2C%20we%0Aconducted%20a%20reader%20study%20where%20an%20expert%20was%20able%20to%20discern%20the%20generated%0Amicroplastic%20from%20real%20microplastic%20at%20a%20rate%20of%2068%20percent.%20Our%20segmentation%0Amodel%20trained%20on%20the%20combined%20data%20achieved%20an%20F1-Score%20of%200.91%20on%20a%20diverse%0Adataset%2C%20compared%20to%20the%20model%20without%20generated%20data%27s%200.82.%20With%20our%20findings%0Awe%20aim%20to%20enhance%20the%20ability%20of%20both%20experts%20and%20citizens%20to%20detect%0Amicroplastic%20across%20diverse%20ecological%20contexts%2C%20thereby%20improving%20the%20cost%20and%0Aaccessibility%20of%20microplastic%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19604v1&entry.124074799=Read"},
{"title": "Gradient-free online learning of subgrid-scale dynamics with neural\n  emulators", "author": "Hugo Frezat and Ronan Fablet and Guillaume Balarac and Julien Le Sommer", "abstract": "  In this paper, we propose a generic algorithm to train machine learning-based\nsubgrid parametrizations online, i.e., with a posteriori loss functions, but\nfor non-differentiable numerical solvers. The proposed approach leverages a\nneural emulator to approximate the reduced state-space solver, which is then\nused to allow gradient propagation through temporal integration steps. We apply\nthis methodology on a single layer quasi-geostrophic system with topography,\nknown to be highly unstable in around 500 temporal iterations with offline\nstrategies. Using our algorithm, we are able to train a parametrization that\nrecovers most of the benefits of online strategies without having to compute\nthe gradient of the original solver. It is demonstrated that training the\nneural emulator and parametrization components separately with different loss\nquantities is necessary in order to minimize the propagation of approximation\nbiases. Experiments on emulator architectures with different complexities also\nindicates that emulator performance is key in order to learn an accurate\nparametrization. This work is a step towards learning parametrization with\nonline strategies for weather models.\n", "link": "http://arxiv.org/abs/2310.19385v4", "date": "2024-10-25", "relevancy": 1.9433, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4895}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4854}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-free%20online%20learning%20of%20subgrid-scale%20dynamics%20with%20neural%0A%20%20emulators&body=Title%3A%20Gradient-free%20online%20learning%20of%20subgrid-scale%20dynamics%20with%20neural%0A%20%20emulators%0AAuthor%3A%20Hugo%20Frezat%20and%20Ronan%20Fablet%20and%20Guillaume%20Balarac%20and%20Julien%20Le%20Sommer%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20generic%20algorithm%20to%20train%20machine%20learning-based%0Asubgrid%20parametrizations%20online%2C%20i.e.%2C%20with%20a%20posteriori%20loss%20functions%2C%20but%0Afor%20non-differentiable%20numerical%20solvers.%20The%20proposed%20approach%20leverages%20a%0Aneural%20emulator%20to%20approximate%20the%20reduced%20state-space%20solver%2C%20which%20is%20then%0Aused%20to%20allow%20gradient%20propagation%20through%20temporal%20integration%20steps.%20We%20apply%0Athis%20methodology%20on%20a%20single%20layer%20quasi-geostrophic%20system%20with%20topography%2C%0Aknown%20to%20be%20highly%20unstable%20in%20around%20500%20temporal%20iterations%20with%20offline%0Astrategies.%20Using%20our%20algorithm%2C%20we%20are%20able%20to%20train%20a%20parametrization%20that%0Arecovers%20most%20of%20the%20benefits%20of%20online%20strategies%20without%20having%20to%20compute%0Athe%20gradient%20of%20the%20original%20solver.%20It%20is%20demonstrated%20that%20training%20the%0Aneural%20emulator%20and%20parametrization%20components%20separately%20with%20different%20loss%0Aquantities%20is%20necessary%20in%20order%20to%20minimize%20the%20propagation%20of%20approximation%0Abiases.%20Experiments%20on%20emulator%20architectures%20with%20different%20complexities%20also%0Aindicates%20that%20emulator%20performance%20is%20key%20in%20order%20to%20learn%20an%20accurate%0Aparametrization.%20This%20work%20is%20a%20step%20towards%20learning%20parametrization%20with%0Aonline%20strategies%20for%20weather%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19385v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-free%2520online%2520learning%2520of%2520subgrid-scale%2520dynamics%2520with%2520neural%250A%2520%2520emulators%26entry.906535625%3DHugo%2520Frezat%2520and%2520Ronan%2520Fablet%2520and%2520Guillaume%2520Balarac%2520and%2520Julien%2520Le%2520Sommer%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520generic%2520algorithm%2520to%2520train%2520machine%2520learning-based%250Asubgrid%2520parametrizations%2520online%252C%2520i.e.%252C%2520with%2520a%2520posteriori%2520loss%2520functions%252C%2520but%250Afor%2520non-differentiable%2520numerical%2520solvers.%2520The%2520proposed%2520approach%2520leverages%2520a%250Aneural%2520emulator%2520to%2520approximate%2520the%2520reduced%2520state-space%2520solver%252C%2520which%2520is%2520then%250Aused%2520to%2520allow%2520gradient%2520propagation%2520through%2520temporal%2520integration%2520steps.%2520We%2520apply%250Athis%2520methodology%2520on%2520a%2520single%2520layer%2520quasi-geostrophic%2520system%2520with%2520topography%252C%250Aknown%2520to%2520be%2520highly%2520unstable%2520in%2520around%2520500%2520temporal%2520iterations%2520with%2520offline%250Astrategies.%2520Using%2520our%2520algorithm%252C%2520we%2520are%2520able%2520to%2520train%2520a%2520parametrization%2520that%250Arecovers%2520most%2520of%2520the%2520benefits%2520of%2520online%2520strategies%2520without%2520having%2520to%2520compute%250Athe%2520gradient%2520of%2520the%2520original%2520solver.%2520It%2520is%2520demonstrated%2520that%2520training%2520the%250Aneural%2520emulator%2520and%2520parametrization%2520components%2520separately%2520with%2520different%2520loss%250Aquantities%2520is%2520necessary%2520in%2520order%2520to%2520minimize%2520the%2520propagation%2520of%2520approximation%250Abiases.%2520Experiments%2520on%2520emulator%2520architectures%2520with%2520different%2520complexities%2520also%250Aindicates%2520that%2520emulator%2520performance%2520is%2520key%2520in%2520order%2520to%2520learn%2520an%2520accurate%250Aparametrization.%2520This%2520work%2520is%2520a%2520step%2520towards%2520learning%2520parametrization%2520with%250Aonline%2520strategies%2520for%2520weather%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19385v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-free%20online%20learning%20of%20subgrid-scale%20dynamics%20with%20neural%0A%20%20emulators&entry.906535625=Hugo%20Frezat%20and%20Ronan%20Fablet%20and%20Guillaume%20Balarac%20and%20Julien%20Le%20Sommer&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20generic%20algorithm%20to%20train%20machine%20learning-based%0Asubgrid%20parametrizations%20online%2C%20i.e.%2C%20with%20a%20posteriori%20loss%20functions%2C%20but%0Afor%20non-differentiable%20numerical%20solvers.%20The%20proposed%20approach%20leverages%20a%0Aneural%20emulator%20to%20approximate%20the%20reduced%20state-space%20solver%2C%20which%20is%20then%0Aused%20to%20allow%20gradient%20propagation%20through%20temporal%20integration%20steps.%20We%20apply%0Athis%20methodology%20on%20a%20single%20layer%20quasi-geostrophic%20system%20with%20topography%2C%0Aknown%20to%20be%20highly%20unstable%20in%20around%20500%20temporal%20iterations%20with%20offline%0Astrategies.%20Using%20our%20algorithm%2C%20we%20are%20able%20to%20train%20a%20parametrization%20that%0Arecovers%20most%20of%20the%20benefits%20of%20online%20strategies%20without%20having%20to%20compute%0Athe%20gradient%20of%20the%20original%20solver.%20It%20is%20demonstrated%20that%20training%20the%0Aneural%20emulator%20and%20parametrization%20components%20separately%20with%20different%20loss%0Aquantities%20is%20necessary%20in%20order%20to%20minimize%20the%20propagation%20of%20approximation%0Abiases.%20Experiments%20on%20emulator%20architectures%20with%20different%20complexities%20also%0Aindicates%20that%20emulator%20performance%20is%20key%20in%20order%20to%20learn%20an%20accurate%0Aparametrization.%20This%20work%20is%20a%20step%20towards%20learning%20parametrization%20with%0Aonline%20strategies%20for%20weather%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19385v4&entry.124074799=Read"},
{"title": "Learning Unlabeled Clients Divergence for Federated Semi-Supervised\n  Learning via Anchor Model Aggregation", "author": "Marawan Elbatel and Hualiang Wang and Jixiang Chen and Hao Wang and Xiaomeng Li", "abstract": "  Federated semi-supervised learning (FedSemi) refers to scenarios where there\nmay be clients with fully labeled data, clients with partially labeled, and\neven fully unlabeled clients while preserving data privacy. However, challenges\narise from client drift due to undefined heterogeneous class distributions and\nerroneous pseudo-labels. Existing FedSemi methods typically fail to aggregate\nmodels from unlabeled clients due to their inherent unreliability, thus\noverlooking unique information from their heterogeneous data distribution,\nleading to sub-optimal results. In this paper, we enable unlabeled client\naggregation through SemiAnAgg, a novel Semi-supervised Anchor-Based federated\nAggregation. SemiAnAgg learns unlabeled client contributions via an anchor\nmodel, effectively harnessing their informative value. Our key idea is that by\nfeeding local client data to the same global model and the same consistently\ninitialized anchor model (i.e., random model), we can measure the importance of\neach unlabeled client accordingly. Extensive experiments demonstrate that\nSemiAnAgg achieves new state-of-the-art results on four widely used FedSemi\nbenchmarks, leading to substantial performance improvements: a 9% increase in\naccuracy on CIFAR-100 and a 7.6% improvement in recall on the medical dataset\nISIC-18, compared with prior state-of-the-art. Code is available at:\nhttps://github.com/xmed-lab/SemiAnAgg.\n", "link": "http://arxiv.org/abs/2407.10327v2", "date": "2024-10-25", "relevancy": 1.9426, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5323}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4862}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Unlabeled%20Clients%20Divergence%20for%20Federated%20Semi-Supervised%0A%20%20Learning%20via%20Anchor%20Model%20Aggregation&body=Title%3A%20Learning%20Unlabeled%20Clients%20Divergence%20for%20Federated%20Semi-Supervised%0A%20%20Learning%20via%20Anchor%20Model%20Aggregation%0AAuthor%3A%20Marawan%20Elbatel%20and%20Hualiang%20Wang%20and%20Jixiang%20Chen%20and%20Hao%20Wang%20and%20Xiaomeng%20Li%0AAbstract%3A%20%20%20Federated%20semi-supervised%20learning%20%28FedSemi%29%20refers%20to%20scenarios%20where%20there%0Amay%20be%20clients%20with%20fully%20labeled%20data%2C%20clients%20with%20partially%20labeled%2C%20and%0Aeven%20fully%20unlabeled%20clients%20while%20preserving%20data%20privacy.%20However%2C%20challenges%0Aarise%20from%20client%20drift%20due%20to%20undefined%20heterogeneous%20class%20distributions%20and%0Aerroneous%20pseudo-labels.%20Existing%20FedSemi%20methods%20typically%20fail%20to%20aggregate%0Amodels%20from%20unlabeled%20clients%20due%20to%20their%20inherent%20unreliability%2C%20thus%0Aoverlooking%20unique%20information%20from%20their%20heterogeneous%20data%20distribution%2C%0Aleading%20to%20sub-optimal%20results.%20In%20this%20paper%2C%20we%20enable%20unlabeled%20client%0Aaggregation%20through%20SemiAnAgg%2C%20a%20novel%20Semi-supervised%20Anchor-Based%20federated%0AAggregation.%20SemiAnAgg%20learns%20unlabeled%20client%20contributions%20via%20an%20anchor%0Amodel%2C%20effectively%20harnessing%20their%20informative%20value.%20Our%20key%20idea%20is%20that%20by%0Afeeding%20local%20client%20data%20to%20the%20same%20global%20model%20and%20the%20same%20consistently%0Ainitialized%20anchor%20model%20%28i.e.%2C%20random%20model%29%2C%20we%20can%20measure%20the%20importance%20of%0Aeach%20unlabeled%20client%20accordingly.%20Extensive%20experiments%20demonstrate%20that%0ASemiAnAgg%20achieves%20new%20state-of-the-art%20results%20on%20four%20widely%20used%20FedSemi%0Abenchmarks%2C%20leading%20to%20substantial%20performance%20improvements%3A%20a%209%25%20increase%20in%0Aaccuracy%20on%20CIFAR-100%20and%20a%207.6%25%20improvement%20in%20recall%20on%20the%20medical%20dataset%0AISIC-18%2C%20compared%20with%20prior%20state-of-the-art.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/xmed-lab/SemiAnAgg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Unlabeled%2520Clients%2520Divergence%2520for%2520Federated%2520Semi-Supervised%250A%2520%2520Learning%2520via%2520Anchor%2520Model%2520Aggregation%26entry.906535625%3DMarawan%2520Elbatel%2520and%2520Hualiang%2520Wang%2520and%2520Jixiang%2520Chen%2520and%2520Hao%2520Wang%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3D%2520%2520Federated%2520semi-supervised%2520learning%2520%2528FedSemi%2529%2520refers%2520to%2520scenarios%2520where%2520there%250Amay%2520be%2520clients%2520with%2520fully%2520labeled%2520data%252C%2520clients%2520with%2520partially%2520labeled%252C%2520and%250Aeven%2520fully%2520unlabeled%2520clients%2520while%2520preserving%2520data%2520privacy.%2520However%252C%2520challenges%250Aarise%2520from%2520client%2520drift%2520due%2520to%2520undefined%2520heterogeneous%2520class%2520distributions%2520and%250Aerroneous%2520pseudo-labels.%2520Existing%2520FedSemi%2520methods%2520typically%2520fail%2520to%2520aggregate%250Amodels%2520from%2520unlabeled%2520clients%2520due%2520to%2520their%2520inherent%2520unreliability%252C%2520thus%250Aoverlooking%2520unique%2520information%2520from%2520their%2520heterogeneous%2520data%2520distribution%252C%250Aleading%2520to%2520sub-optimal%2520results.%2520In%2520this%2520paper%252C%2520we%2520enable%2520unlabeled%2520client%250Aaggregation%2520through%2520SemiAnAgg%252C%2520a%2520novel%2520Semi-supervised%2520Anchor-Based%2520federated%250AAggregation.%2520SemiAnAgg%2520learns%2520unlabeled%2520client%2520contributions%2520via%2520an%2520anchor%250Amodel%252C%2520effectively%2520harnessing%2520their%2520informative%2520value.%2520Our%2520key%2520idea%2520is%2520that%2520by%250Afeeding%2520local%2520client%2520data%2520to%2520the%2520same%2520global%2520model%2520and%2520the%2520same%2520consistently%250Ainitialized%2520anchor%2520model%2520%2528i.e.%252C%2520random%2520model%2529%252C%2520we%2520can%2520measure%2520the%2520importance%2520of%250Aeach%2520unlabeled%2520client%2520accordingly.%2520Extensive%2520experiments%2520demonstrate%2520that%250ASemiAnAgg%2520achieves%2520new%2520state-of-the-art%2520results%2520on%2520four%2520widely%2520used%2520FedSemi%250Abenchmarks%252C%2520leading%2520to%2520substantial%2520performance%2520improvements%253A%2520a%25209%2525%2520increase%2520in%250Aaccuracy%2520on%2520CIFAR-100%2520and%2520a%25207.6%2525%2520improvement%2520in%2520recall%2520on%2520the%2520medical%2520dataset%250AISIC-18%252C%2520compared%2520with%2520prior%2520state-of-the-art.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/xmed-lab/SemiAnAgg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Unlabeled%20Clients%20Divergence%20for%20Federated%20Semi-Supervised%0A%20%20Learning%20via%20Anchor%20Model%20Aggregation&entry.906535625=Marawan%20Elbatel%20and%20Hualiang%20Wang%20and%20Jixiang%20Chen%20and%20Hao%20Wang%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Federated%20semi-supervised%20learning%20%28FedSemi%29%20refers%20to%20scenarios%20where%20there%0Amay%20be%20clients%20with%20fully%20labeled%20data%2C%20clients%20with%20partially%20labeled%2C%20and%0Aeven%20fully%20unlabeled%20clients%20while%20preserving%20data%20privacy.%20However%2C%20challenges%0Aarise%20from%20client%20drift%20due%20to%20undefined%20heterogeneous%20class%20distributions%20and%0Aerroneous%20pseudo-labels.%20Existing%20FedSemi%20methods%20typically%20fail%20to%20aggregate%0Amodels%20from%20unlabeled%20clients%20due%20to%20their%20inherent%20unreliability%2C%20thus%0Aoverlooking%20unique%20information%20from%20their%20heterogeneous%20data%20distribution%2C%0Aleading%20to%20sub-optimal%20results.%20In%20this%20paper%2C%20we%20enable%20unlabeled%20client%0Aaggregation%20through%20SemiAnAgg%2C%20a%20novel%20Semi-supervised%20Anchor-Based%20federated%0AAggregation.%20SemiAnAgg%20learns%20unlabeled%20client%20contributions%20via%20an%20anchor%0Amodel%2C%20effectively%20harnessing%20their%20informative%20value.%20Our%20key%20idea%20is%20that%20by%0Afeeding%20local%20client%20data%20to%20the%20same%20global%20model%20and%20the%20same%20consistently%0Ainitialized%20anchor%20model%20%28i.e.%2C%20random%20model%29%2C%20we%20can%20measure%20the%20importance%20of%0Aeach%20unlabeled%20client%20accordingly.%20Extensive%20experiments%20demonstrate%20that%0ASemiAnAgg%20achieves%20new%20state-of-the-art%20results%20on%20four%20widely%20used%20FedSemi%0Abenchmarks%2C%20leading%20to%20substantial%20performance%20improvements%3A%20a%209%25%20increase%20in%0Aaccuracy%20on%20CIFAR-100%20and%20a%207.6%25%20improvement%20in%20recall%20on%20the%20medical%20dataset%0AISIC-18%2C%20compared%20with%20prior%20state-of-the-art.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/xmed-lab/SemiAnAgg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10327v2&entry.124074799=Read"},
{"title": "Accelerating AI Performance using Anderson Extrapolation on GPUs", "author": "Saleem Abdul Fattah Ahmed Al Dajani and David E. Keyes", "abstract": "  We present a novel approach for accelerating AI performance by leveraging\nAnderson extrapolation, a vector-to-vector mapping technique based on a window\nof historical iterations. By identifying the crossover point where a mixing\npenalty is incurred, the method focuses on reducing iterations to convergence,\nwith fewer more compute-intensive but generally cacheable iterations, balancing\nspeed and memory usage with accuracy and algorithmic stability, respectively.\nWe demonstrate significant improvements, in both training and inference,\nmotivated by scalability and efficiency extensions to the realm of\nhigh-performance computing (HPC).\n", "link": "http://arxiv.org/abs/2410.19460v1", "date": "2024-10-25", "relevancy": 1.9379, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5028}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4883}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20AI%20Performance%20using%20Anderson%20Extrapolation%20on%20GPUs&body=Title%3A%20Accelerating%20AI%20Performance%20using%20Anderson%20Extrapolation%20on%20GPUs%0AAuthor%3A%20Saleem%20Abdul%20Fattah%20Ahmed%20Al%20Dajani%20and%20David%20E.%20Keyes%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20accelerating%20AI%20performance%20by%20leveraging%0AAnderson%20extrapolation%2C%20a%20vector-to-vector%20mapping%20technique%20based%20on%20a%20window%0Aof%20historical%20iterations.%20By%20identifying%20the%20crossover%20point%20where%20a%20mixing%0Apenalty%20is%20incurred%2C%20the%20method%20focuses%20on%20reducing%20iterations%20to%20convergence%2C%0Awith%20fewer%20more%20compute-intensive%20but%20generally%20cacheable%20iterations%2C%20balancing%0Aspeed%20and%20memory%20usage%20with%20accuracy%20and%20algorithmic%20stability%2C%20respectively.%0AWe%20demonstrate%20significant%20improvements%2C%20in%20both%20training%20and%20inference%2C%0Amotivated%20by%20scalability%20and%20efficiency%20extensions%20to%20the%20realm%20of%0Ahigh-performance%20computing%20%28HPC%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520AI%2520Performance%2520using%2520Anderson%2520Extrapolation%2520on%2520GPUs%26entry.906535625%3DSaleem%2520Abdul%2520Fattah%2520Ahmed%2520Al%2520Dajani%2520and%2520David%2520E.%2520Keyes%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520for%2520accelerating%2520AI%2520performance%2520by%2520leveraging%250AAnderson%2520extrapolation%252C%2520a%2520vector-to-vector%2520mapping%2520technique%2520based%2520on%2520a%2520window%250Aof%2520historical%2520iterations.%2520By%2520identifying%2520the%2520crossover%2520point%2520where%2520a%2520mixing%250Apenalty%2520is%2520incurred%252C%2520the%2520method%2520focuses%2520on%2520reducing%2520iterations%2520to%2520convergence%252C%250Awith%2520fewer%2520more%2520compute-intensive%2520but%2520generally%2520cacheable%2520iterations%252C%2520balancing%250Aspeed%2520and%2520memory%2520usage%2520with%2520accuracy%2520and%2520algorithmic%2520stability%252C%2520respectively.%250AWe%2520demonstrate%2520significant%2520improvements%252C%2520in%2520both%2520training%2520and%2520inference%252C%250Amotivated%2520by%2520scalability%2520and%2520efficiency%2520extensions%2520to%2520the%2520realm%2520of%250Ahigh-performance%2520computing%2520%2528HPC%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20AI%20Performance%20using%20Anderson%20Extrapolation%20on%20GPUs&entry.906535625=Saleem%20Abdul%20Fattah%20Ahmed%20Al%20Dajani%20and%20David%20E.%20Keyes&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20accelerating%20AI%20performance%20by%20leveraging%0AAnderson%20extrapolation%2C%20a%20vector-to-vector%20mapping%20technique%20based%20on%20a%20window%0Aof%20historical%20iterations.%20By%20identifying%20the%20crossover%20point%20where%20a%20mixing%0Apenalty%20is%20incurred%2C%20the%20method%20focuses%20on%20reducing%20iterations%20to%20convergence%2C%0Awith%20fewer%20more%20compute-intensive%20but%20generally%20cacheable%20iterations%2C%20balancing%0Aspeed%20and%20memory%20usage%20with%20accuracy%20and%20algorithmic%20stability%2C%20respectively.%0AWe%20demonstrate%20significant%20improvements%2C%20in%20both%20training%20and%20inference%2C%0Amotivated%20by%20scalability%20and%20efficiency%20extensions%20to%20the%20realm%20of%0Ahigh-performance%20computing%20%28HPC%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19460v1&entry.124074799=Read"},
{"title": "NIDS Neural Networks Using Sliding Time Window Data Processing with\n  Trainable Activations and its Generalization Capability", "author": "Anton Raskovalov and Nikita Gabdullin and Ilya Androsov", "abstract": "  This paper presents neural networks for network intrusion detection systems\n(NIDS), that operate on flow data preprocessed with a time window. It requires\nonly eleven features which do not rely on deep packet inspection and can be\nfound in most NIDS datasets and easily obtained from conventional flow\ncollectors. The time window aggregates information with respect to hosts\nfacilitating the identification of flow signatures that are missed by other\naggregation methods. Several network architectures are studied and the use of\nKolmogorov-Arnold Network (KAN)-inspired trainable activation functions that\nhelp to achieve higher accuracy with simpler network structure is proposed. The\nreported training accuracy exceeds 99% for the proposed method with as little\nas twenty neural network input features. This work also studies the\ngeneralization capability of NIDS, a crucial aspect that has not been\nadequately addressed in the previous studies. The generalization experiments\nare conducted using CICIDS2017 dataset and a custom dataset collected as part\nof this study. It is shown that the performance metrics decline significantly\nwhen changing datasets, and the reduction in performance metrics can be\nattributed to the difference in signatures of the same type flows in different\ndatasets, which in turn can be attributed to the differences between the\nunderlying networks. It is shown that the generalization accuracy of some\nneural networks can be very unstable and sensitive to random initialization\nparameters, and neural networks with fewer parameters and well-tuned\nactivations are more stable and achieve higher accuracy.\n", "link": "http://arxiv.org/abs/2410.18658v2", "date": "2024-10-25", "relevancy": 1.9336, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.491}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4809}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NIDS%20Neural%20Networks%20Using%20Sliding%20Time%20Window%20Data%20Processing%20with%0A%20%20Trainable%20Activations%20and%20its%20Generalization%20Capability&body=Title%3A%20NIDS%20Neural%20Networks%20Using%20Sliding%20Time%20Window%20Data%20Processing%20with%0A%20%20Trainable%20Activations%20and%20its%20Generalization%20Capability%0AAuthor%3A%20Anton%20Raskovalov%20and%20Nikita%20Gabdullin%20and%20Ilya%20Androsov%0AAbstract%3A%20%20%20This%20paper%20presents%20neural%20networks%20for%20network%20intrusion%20detection%20systems%0A%28NIDS%29%2C%20that%20operate%20on%20flow%20data%20preprocessed%20with%20a%20time%20window.%20It%20requires%0Aonly%20eleven%20features%20which%20do%20not%20rely%20on%20deep%20packet%20inspection%20and%20can%20be%0Afound%20in%20most%20NIDS%20datasets%20and%20easily%20obtained%20from%20conventional%20flow%0Acollectors.%20The%20time%20window%20aggregates%20information%20with%20respect%20to%20hosts%0Afacilitating%20the%20identification%20of%20flow%20signatures%20that%20are%20missed%20by%20other%0Aaggregation%20methods.%20Several%20network%20architectures%20are%20studied%20and%20the%20use%20of%0AKolmogorov-Arnold%20Network%20%28KAN%29-inspired%20trainable%20activation%20functions%20that%0Ahelp%20to%20achieve%20higher%20accuracy%20with%20simpler%20network%20structure%20is%20proposed.%20The%0Areported%20training%20accuracy%20exceeds%2099%25%20for%20the%20proposed%20method%20with%20as%20little%0Aas%20twenty%20neural%20network%20input%20features.%20This%20work%20also%20studies%20the%0Ageneralization%20capability%20of%20NIDS%2C%20a%20crucial%20aspect%20that%20has%20not%20been%0Aadequately%20addressed%20in%20the%20previous%20studies.%20The%20generalization%20experiments%0Aare%20conducted%20using%20CICIDS2017%20dataset%20and%20a%20custom%20dataset%20collected%20as%20part%0Aof%20this%20study.%20It%20is%20shown%20that%20the%20performance%20metrics%20decline%20significantly%0Awhen%20changing%20datasets%2C%20and%20the%20reduction%20in%20performance%20metrics%20can%20be%0Aattributed%20to%20the%20difference%20in%20signatures%20of%20the%20same%20type%20flows%20in%20different%0Adatasets%2C%20which%20in%20turn%20can%20be%20attributed%20to%20the%20differences%20between%20the%0Aunderlying%20networks.%20It%20is%20shown%20that%20the%20generalization%20accuracy%20of%20some%0Aneural%20networks%20can%20be%20very%20unstable%20and%20sensitive%20to%20random%20initialization%0Aparameters%2C%20and%20neural%20networks%20with%20fewer%20parameters%20and%20well-tuned%0Aactivations%20are%20more%20stable%20and%20achieve%20higher%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18658v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNIDS%2520Neural%2520Networks%2520Using%2520Sliding%2520Time%2520Window%2520Data%2520Processing%2520with%250A%2520%2520Trainable%2520Activations%2520and%2520its%2520Generalization%2520Capability%26entry.906535625%3DAnton%2520Raskovalov%2520and%2520Nikita%2520Gabdullin%2520and%2520Ilya%2520Androsov%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520neural%2520networks%2520for%2520network%2520intrusion%2520detection%2520systems%250A%2528NIDS%2529%252C%2520that%2520operate%2520on%2520flow%2520data%2520preprocessed%2520with%2520a%2520time%2520window.%2520It%2520requires%250Aonly%2520eleven%2520features%2520which%2520do%2520not%2520rely%2520on%2520deep%2520packet%2520inspection%2520and%2520can%2520be%250Afound%2520in%2520most%2520NIDS%2520datasets%2520and%2520easily%2520obtained%2520from%2520conventional%2520flow%250Acollectors.%2520The%2520time%2520window%2520aggregates%2520information%2520with%2520respect%2520to%2520hosts%250Afacilitating%2520the%2520identification%2520of%2520flow%2520signatures%2520that%2520are%2520missed%2520by%2520other%250Aaggregation%2520methods.%2520Several%2520network%2520architectures%2520are%2520studied%2520and%2520the%2520use%2520of%250AKolmogorov-Arnold%2520Network%2520%2528KAN%2529-inspired%2520trainable%2520activation%2520functions%2520that%250Ahelp%2520to%2520achieve%2520higher%2520accuracy%2520with%2520simpler%2520network%2520structure%2520is%2520proposed.%2520The%250Areported%2520training%2520accuracy%2520exceeds%252099%2525%2520for%2520the%2520proposed%2520method%2520with%2520as%2520little%250Aas%2520twenty%2520neural%2520network%2520input%2520features.%2520This%2520work%2520also%2520studies%2520the%250Ageneralization%2520capability%2520of%2520NIDS%252C%2520a%2520crucial%2520aspect%2520that%2520has%2520not%2520been%250Aadequately%2520addressed%2520in%2520the%2520previous%2520studies.%2520The%2520generalization%2520experiments%250Aare%2520conducted%2520using%2520CICIDS2017%2520dataset%2520and%2520a%2520custom%2520dataset%2520collected%2520as%2520part%250Aof%2520this%2520study.%2520It%2520is%2520shown%2520that%2520the%2520performance%2520metrics%2520decline%2520significantly%250Awhen%2520changing%2520datasets%252C%2520and%2520the%2520reduction%2520in%2520performance%2520metrics%2520can%2520be%250Aattributed%2520to%2520the%2520difference%2520in%2520signatures%2520of%2520the%2520same%2520type%2520flows%2520in%2520different%250Adatasets%252C%2520which%2520in%2520turn%2520can%2520be%2520attributed%2520to%2520the%2520differences%2520between%2520the%250Aunderlying%2520networks.%2520It%2520is%2520shown%2520that%2520the%2520generalization%2520accuracy%2520of%2520some%250Aneural%2520networks%2520can%2520be%2520very%2520unstable%2520and%2520sensitive%2520to%2520random%2520initialization%250Aparameters%252C%2520and%2520neural%2520networks%2520with%2520fewer%2520parameters%2520and%2520well-tuned%250Aactivations%2520are%2520more%2520stable%2520and%2520achieve%2520higher%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18658v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NIDS%20Neural%20Networks%20Using%20Sliding%20Time%20Window%20Data%20Processing%20with%0A%20%20Trainable%20Activations%20and%20its%20Generalization%20Capability&entry.906535625=Anton%20Raskovalov%20and%20Nikita%20Gabdullin%20and%20Ilya%20Androsov&entry.1292438233=%20%20This%20paper%20presents%20neural%20networks%20for%20network%20intrusion%20detection%20systems%0A%28NIDS%29%2C%20that%20operate%20on%20flow%20data%20preprocessed%20with%20a%20time%20window.%20It%20requires%0Aonly%20eleven%20features%20which%20do%20not%20rely%20on%20deep%20packet%20inspection%20and%20can%20be%0Afound%20in%20most%20NIDS%20datasets%20and%20easily%20obtained%20from%20conventional%20flow%0Acollectors.%20The%20time%20window%20aggregates%20information%20with%20respect%20to%20hosts%0Afacilitating%20the%20identification%20of%20flow%20signatures%20that%20are%20missed%20by%20other%0Aaggregation%20methods.%20Several%20network%20architectures%20are%20studied%20and%20the%20use%20of%0AKolmogorov-Arnold%20Network%20%28KAN%29-inspired%20trainable%20activation%20functions%20that%0Ahelp%20to%20achieve%20higher%20accuracy%20with%20simpler%20network%20structure%20is%20proposed.%20The%0Areported%20training%20accuracy%20exceeds%2099%25%20for%20the%20proposed%20method%20with%20as%20little%0Aas%20twenty%20neural%20network%20input%20features.%20This%20work%20also%20studies%20the%0Ageneralization%20capability%20of%20NIDS%2C%20a%20crucial%20aspect%20that%20has%20not%20been%0Aadequately%20addressed%20in%20the%20previous%20studies.%20The%20generalization%20experiments%0Aare%20conducted%20using%20CICIDS2017%20dataset%20and%20a%20custom%20dataset%20collected%20as%20part%0Aof%20this%20study.%20It%20is%20shown%20that%20the%20performance%20metrics%20decline%20significantly%0Awhen%20changing%20datasets%2C%20and%20the%20reduction%20in%20performance%20metrics%20can%20be%0Aattributed%20to%20the%20difference%20in%20signatures%20of%20the%20same%20type%20flows%20in%20different%0Adatasets%2C%20which%20in%20turn%20can%20be%20attributed%20to%20the%20differences%20between%20the%0Aunderlying%20networks.%20It%20is%20shown%20that%20the%20generalization%20accuracy%20of%20some%0Aneural%20networks%20can%20be%20very%20unstable%20and%20sensitive%20to%20random%20initialization%0Aparameters%2C%20and%20neural%20networks%20with%20fewer%20parameters%20and%20well-tuned%0Aactivations%20are%20more%20stable%20and%20achieve%20higher%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18658v2&entry.124074799=Read"},
{"title": "Measuring memorization through probabilistic discoverable extraction", "author": "Jamie Hayes and Marika Swanberg and Harsh Chaudhari and Itay Yona and Ilia Shumailov", "abstract": "  Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns due to the potential extraction of sensitive information.\nCurrent methods to measure memorization rates of LLMs, primarily discoverable\nextraction (Carlini et al., 2022), rely on single-sequence greedy sampling,\npotentially underestimating the true extent of memorization. This paper\nintroduces a probabilistic relaxation of discoverable extraction that\nquantifies the probability of extracting a target sequence within a set of\ngenerated samples, considering various sampling schemes and multiple attempts.\nThis approach addresses the limitations of reporting memorization rates through\ndiscoverable extraction by accounting for the probabilistic nature of LLMs and\nuser interaction patterns. Our experiments demonstrate that this probabilistic\nmeasure can reveal cases of higher memorization rates compared to rates found\nthrough discoverable extraction. We further investigate the impact of different\nsampling schemes on extractability, providing a more comprehensive and\nrealistic assessment of LLM memorization and its associated risks. Our\ncontributions include a new probabilistic memorization definition, empirical\nevidence of its effectiveness, and a thorough evaluation across different\nmodels, sizes, sampling schemes, and training data repetitions.\n", "link": "http://arxiv.org/abs/2410.19482v1", "date": "2024-10-25", "relevancy": 1.932, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20memorization%20through%20probabilistic%20discoverable%20extraction&body=Title%3A%20Measuring%20memorization%20through%20probabilistic%20discoverable%20extraction%0AAuthor%3A%20Jamie%20Hayes%20and%20Marika%20Swanberg%20and%20Harsh%20Chaudhari%20and%20Itay%20Yona%20and%20Ilia%20Shumailov%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20susceptible%20to%20memorizing%20training%20data%2C%0Araising%20concerns%20due%20to%20the%20potential%20extraction%20of%20sensitive%20information.%0ACurrent%20methods%20to%20measure%20memorization%20rates%20of%20LLMs%2C%20primarily%20discoverable%0Aextraction%20%28Carlini%20et%20al.%2C%202022%29%2C%20rely%20on%20single-sequence%20greedy%20sampling%2C%0Apotentially%20underestimating%20the%20true%20extent%20of%20memorization.%20This%20paper%0Aintroduces%20a%20probabilistic%20relaxation%20of%20discoverable%20extraction%20that%0Aquantifies%20the%20probability%20of%20extracting%20a%20target%20sequence%20within%20a%20set%20of%0Agenerated%20samples%2C%20considering%20various%20sampling%20schemes%20and%20multiple%20attempts.%0AThis%20approach%20addresses%20the%20limitations%20of%20reporting%20memorization%20rates%20through%0Adiscoverable%20extraction%20by%20accounting%20for%20the%20probabilistic%20nature%20of%20LLMs%20and%0Auser%20interaction%20patterns.%20Our%20experiments%20demonstrate%20that%20this%20probabilistic%0Ameasure%20can%20reveal%20cases%20of%20higher%20memorization%20rates%20compared%20to%20rates%20found%0Athrough%20discoverable%20extraction.%20We%20further%20investigate%20the%20impact%20of%20different%0Asampling%20schemes%20on%20extractability%2C%20providing%20a%20more%20comprehensive%20and%0Arealistic%20assessment%20of%20LLM%20memorization%20and%20its%20associated%20risks.%20Our%0Acontributions%20include%20a%20new%20probabilistic%20memorization%20definition%2C%20empirical%0Aevidence%20of%20its%20effectiveness%2C%20and%20a%20thorough%20evaluation%20across%20different%0Amodels%2C%20sizes%2C%20sampling%20schemes%2C%20and%20training%20data%20repetitions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520memorization%2520through%2520probabilistic%2520discoverable%2520extraction%26entry.906535625%3DJamie%2520Hayes%2520and%2520Marika%2520Swanberg%2520and%2520Harsh%2520Chaudhari%2520and%2520Itay%2520Yona%2520and%2520Ilia%2520Shumailov%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520susceptible%2520to%2520memorizing%2520training%2520data%252C%250Araising%2520concerns%2520due%2520to%2520the%2520potential%2520extraction%2520of%2520sensitive%2520information.%250ACurrent%2520methods%2520to%2520measure%2520memorization%2520rates%2520of%2520LLMs%252C%2520primarily%2520discoverable%250Aextraction%2520%2528Carlini%2520et%2520al.%252C%25202022%2529%252C%2520rely%2520on%2520single-sequence%2520greedy%2520sampling%252C%250Apotentially%2520underestimating%2520the%2520true%2520extent%2520of%2520memorization.%2520This%2520paper%250Aintroduces%2520a%2520probabilistic%2520relaxation%2520of%2520discoverable%2520extraction%2520that%250Aquantifies%2520the%2520probability%2520of%2520extracting%2520a%2520target%2520sequence%2520within%2520a%2520set%2520of%250Agenerated%2520samples%252C%2520considering%2520various%2520sampling%2520schemes%2520and%2520multiple%2520attempts.%250AThis%2520approach%2520addresses%2520the%2520limitations%2520of%2520reporting%2520memorization%2520rates%2520through%250Adiscoverable%2520extraction%2520by%2520accounting%2520for%2520the%2520probabilistic%2520nature%2520of%2520LLMs%2520and%250Auser%2520interaction%2520patterns.%2520Our%2520experiments%2520demonstrate%2520that%2520this%2520probabilistic%250Ameasure%2520can%2520reveal%2520cases%2520of%2520higher%2520memorization%2520rates%2520compared%2520to%2520rates%2520found%250Athrough%2520discoverable%2520extraction.%2520We%2520further%2520investigate%2520the%2520impact%2520of%2520different%250Asampling%2520schemes%2520on%2520extractability%252C%2520providing%2520a%2520more%2520comprehensive%2520and%250Arealistic%2520assessment%2520of%2520LLM%2520memorization%2520and%2520its%2520associated%2520risks.%2520Our%250Acontributions%2520include%2520a%2520new%2520probabilistic%2520memorization%2520definition%252C%2520empirical%250Aevidence%2520of%2520its%2520effectiveness%252C%2520and%2520a%2520thorough%2520evaluation%2520across%2520different%250Amodels%252C%2520sizes%252C%2520sampling%2520schemes%252C%2520and%2520training%2520data%2520repetitions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20memorization%20through%20probabilistic%20discoverable%20extraction&entry.906535625=Jamie%20Hayes%20and%20Marika%20Swanberg%20and%20Harsh%20Chaudhari%20and%20Itay%20Yona%20and%20Ilia%20Shumailov&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20susceptible%20to%20memorizing%20training%20data%2C%0Araising%20concerns%20due%20to%20the%20potential%20extraction%20of%20sensitive%20information.%0ACurrent%20methods%20to%20measure%20memorization%20rates%20of%20LLMs%2C%20primarily%20discoverable%0Aextraction%20%28Carlini%20et%20al.%2C%202022%29%2C%20rely%20on%20single-sequence%20greedy%20sampling%2C%0Apotentially%20underestimating%20the%20true%20extent%20of%20memorization.%20This%20paper%0Aintroduces%20a%20probabilistic%20relaxation%20of%20discoverable%20extraction%20that%0Aquantifies%20the%20probability%20of%20extracting%20a%20target%20sequence%20within%20a%20set%20of%0Agenerated%20samples%2C%20considering%20various%20sampling%20schemes%20and%20multiple%20attempts.%0AThis%20approach%20addresses%20the%20limitations%20of%20reporting%20memorization%20rates%20through%0Adiscoverable%20extraction%20by%20accounting%20for%20the%20probabilistic%20nature%20of%20LLMs%20and%0Auser%20interaction%20patterns.%20Our%20experiments%20demonstrate%20that%20this%20probabilistic%0Ameasure%20can%20reveal%20cases%20of%20higher%20memorization%20rates%20compared%20to%20rates%20found%0Athrough%20discoverable%20extraction.%20We%20further%20investigate%20the%20impact%20of%20different%0Asampling%20schemes%20on%20extractability%2C%20providing%20a%20more%20comprehensive%20and%0Arealistic%20assessment%20of%20LLM%20memorization%20and%20its%20associated%20risks.%20Our%0Acontributions%20include%20a%20new%20probabilistic%20memorization%20definition%2C%20empirical%0Aevidence%20of%20its%20effectiveness%2C%20and%20a%20thorough%20evaluation%20across%20different%0Amodels%2C%20sizes%2C%20sampling%20schemes%2C%20and%20training%20data%20repetitions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19482v1&entry.124074799=Read"},
{"title": "Spatial Shortcuts in Graph Neural Controlled Differential Equations", "author": "Michael Detzel and Gabriel Nobis and Jackie Ma and Wojciech Samek", "abstract": "  We incorporate prior graph topology information into a Neural Controlled\nDifferential Equation (NCDE) to predict the future states of a dynamical system\ndefined on a graph. The informed NCDE infers the future dynamics at the\nvertices of simulated advection data on graph edges with a known causal graph,\nobserved only at vertices during training. We investigate different positions\nin the model architecture to inform the NCDE with graph information and\nidentify an outer position between hidden state and control as theoretically\nand empirically favorable. Our such informed NCDE requires fewer parameters to\nreach a lower Mean Absolute Error (MAE) compared to previous methods that do\nnot incorporate additional graph topology information.\n", "link": "http://arxiv.org/abs/2410.19673v1", "date": "2024-10-25", "relevancy": 1.927, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4983}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4729}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Shortcuts%20in%20Graph%20Neural%20Controlled%20Differential%20Equations&body=Title%3A%20Spatial%20Shortcuts%20in%20Graph%20Neural%20Controlled%20Differential%20Equations%0AAuthor%3A%20Michael%20Detzel%20and%20Gabriel%20Nobis%20and%20Jackie%20Ma%20and%20Wojciech%20Samek%0AAbstract%3A%20%20%20We%20incorporate%20prior%20graph%20topology%20information%20into%20a%20Neural%20Controlled%0ADifferential%20Equation%20%28NCDE%29%20to%20predict%20the%20future%20states%20of%20a%20dynamical%20system%0Adefined%20on%20a%20graph.%20The%20informed%20NCDE%20infers%20the%20future%20dynamics%20at%20the%0Avertices%20of%20simulated%20advection%20data%20on%20graph%20edges%20with%20a%20known%20causal%20graph%2C%0Aobserved%20only%20at%20vertices%20during%20training.%20We%20investigate%20different%20positions%0Ain%20the%20model%20architecture%20to%20inform%20the%20NCDE%20with%20graph%20information%20and%0Aidentify%20an%20outer%20position%20between%20hidden%20state%20and%20control%20as%20theoretically%0Aand%20empirically%20favorable.%20Our%20such%20informed%20NCDE%20requires%20fewer%20parameters%20to%0Areach%20a%20lower%20Mean%20Absolute%20Error%20%28MAE%29%20compared%20to%20previous%20methods%20that%20do%0Anot%20incorporate%20additional%20graph%20topology%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Shortcuts%2520in%2520Graph%2520Neural%2520Controlled%2520Differential%2520Equations%26entry.906535625%3DMichael%2520Detzel%2520and%2520Gabriel%2520Nobis%2520and%2520Jackie%2520Ma%2520and%2520Wojciech%2520Samek%26entry.1292438233%3D%2520%2520We%2520incorporate%2520prior%2520graph%2520topology%2520information%2520into%2520a%2520Neural%2520Controlled%250ADifferential%2520Equation%2520%2528NCDE%2529%2520to%2520predict%2520the%2520future%2520states%2520of%2520a%2520dynamical%2520system%250Adefined%2520on%2520a%2520graph.%2520The%2520informed%2520NCDE%2520infers%2520the%2520future%2520dynamics%2520at%2520the%250Avertices%2520of%2520simulated%2520advection%2520data%2520on%2520graph%2520edges%2520with%2520a%2520known%2520causal%2520graph%252C%250Aobserved%2520only%2520at%2520vertices%2520during%2520training.%2520We%2520investigate%2520different%2520positions%250Ain%2520the%2520model%2520architecture%2520to%2520inform%2520the%2520NCDE%2520with%2520graph%2520information%2520and%250Aidentify%2520an%2520outer%2520position%2520between%2520hidden%2520state%2520and%2520control%2520as%2520theoretically%250Aand%2520empirically%2520favorable.%2520Our%2520such%2520informed%2520NCDE%2520requires%2520fewer%2520parameters%2520to%250Areach%2520a%2520lower%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520compared%2520to%2520previous%2520methods%2520that%2520do%250Anot%2520incorporate%2520additional%2520graph%2520topology%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Shortcuts%20in%20Graph%20Neural%20Controlled%20Differential%20Equations&entry.906535625=Michael%20Detzel%20and%20Gabriel%20Nobis%20and%20Jackie%20Ma%20and%20Wojciech%20Samek&entry.1292438233=%20%20We%20incorporate%20prior%20graph%20topology%20information%20into%20a%20Neural%20Controlled%0ADifferential%20Equation%20%28NCDE%29%20to%20predict%20the%20future%20states%20of%20a%20dynamical%20system%0Adefined%20on%20a%20graph.%20The%20informed%20NCDE%20infers%20the%20future%20dynamics%20at%20the%0Avertices%20of%20simulated%20advection%20data%20on%20graph%20edges%20with%20a%20known%20causal%20graph%2C%0Aobserved%20only%20at%20vertices%20during%20training.%20We%20investigate%20different%20positions%0Ain%20the%20model%20architecture%20to%20inform%20the%20NCDE%20with%20graph%20information%20and%0Aidentify%20an%20outer%20position%20between%20hidden%20state%20and%20control%20as%20theoretically%0Aand%20empirically%20favorable.%20Our%20such%20informed%20NCDE%20requires%20fewer%20parameters%20to%0Areach%20a%20lower%20Mean%20Absolute%20Error%20%28MAE%29%20compared%20to%20previous%20methods%20that%20do%0Anot%20incorporate%20additional%20graph%20topology%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19673v1&entry.124074799=Read"},
{"title": "Impact of Leakage on Data Harmonization in Machine Learning Pipelines in\n  Class Imbalance Across Sites", "author": "Nicol\u00e1s Nieto and Simon B. Eickhoff and Christian Jung and Martin Reuter and Kersten Diers and Malte Kelm and Artur Lichtenberg and Federico Raimondo and Kaustubh R. Patil", "abstract": "  Machine learning (ML) models benefit from large datasets. Collecting data in\nbiomedical domains is costly and challenging, hence, combining datasets has\nbecome a common practice. However, datasets obtained under different conditions\ncould present undesired site-specific variability. Data harmonization methods\naim to remove site-specific variance while retaining biologically relevant\ninformation. This study evaluates the effectiveness of popularly used\nComBat-based methods for harmonizing data in scenarios where the class balance\nis not equal across sites. We find that these methods struggle with data\nleakage issues. To overcome this problem, we propose a novel approach\nPrettYharmonize, designed to harmonize data by pretending the target labels. We\nvalidate our approach using controlled datasets designed to benchmark the\nutility of harmonization. Finally, using real-world MRI and clinical data, we\ncompare leakage-prone methods with PrettYharmonize and show that it achieves\ncomparable performance while avoiding data leakage, particularly in\nsite-target-dependence scenarios.\n", "link": "http://arxiv.org/abs/2410.19643v1", "date": "2024-10-25", "relevancy": 1.9249, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.49}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4886}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20Leakage%20on%20Data%20Harmonization%20in%20Machine%20Learning%20Pipelines%20in%0A%20%20Class%20Imbalance%20Across%20Sites&body=Title%3A%20Impact%20of%20Leakage%20on%20Data%20Harmonization%20in%20Machine%20Learning%20Pipelines%20in%0A%20%20Class%20Imbalance%20Across%20Sites%0AAuthor%3A%20Nicol%C3%A1s%20Nieto%20and%20Simon%20B.%20Eickhoff%20and%20Christian%20Jung%20and%20Martin%20Reuter%20and%20Kersten%20Diers%20and%20Malte%20Kelm%20and%20Artur%20Lichtenberg%20and%20Federico%20Raimondo%20and%20Kaustubh%20R.%20Patil%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20models%20benefit%20from%20large%20datasets.%20Collecting%20data%20in%0Abiomedical%20domains%20is%20costly%20and%20challenging%2C%20hence%2C%20combining%20datasets%20has%0Abecome%20a%20common%20practice.%20However%2C%20datasets%20obtained%20under%20different%20conditions%0Acould%20present%20undesired%20site-specific%20variability.%20Data%20harmonization%20methods%0Aaim%20to%20remove%20site-specific%20variance%20while%20retaining%20biologically%20relevant%0Ainformation.%20This%20study%20evaluates%20the%20effectiveness%20of%20popularly%20used%0AComBat-based%20methods%20for%20harmonizing%20data%20in%20scenarios%20where%20the%20class%20balance%0Ais%20not%20equal%20across%20sites.%20We%20find%20that%20these%20methods%20struggle%20with%20data%0Aleakage%20issues.%20To%20overcome%20this%20problem%2C%20we%20propose%20a%20novel%20approach%0APrettYharmonize%2C%20designed%20to%20harmonize%20data%20by%20pretending%20the%20target%20labels.%20We%0Avalidate%20our%20approach%20using%20controlled%20datasets%20designed%20to%20benchmark%20the%0Autility%20of%20harmonization.%20Finally%2C%20using%20real-world%20MRI%20and%20clinical%20data%2C%20we%0Acompare%20leakage-prone%20methods%20with%20PrettYharmonize%20and%20show%20that%20it%20achieves%0Acomparable%20performance%20while%20avoiding%20data%20leakage%2C%20particularly%20in%0Asite-target-dependence%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%2520Leakage%2520on%2520Data%2520Harmonization%2520in%2520Machine%2520Learning%2520Pipelines%2520in%250A%2520%2520Class%2520Imbalance%2520Across%2520Sites%26entry.906535625%3DNicol%25C3%25A1s%2520Nieto%2520and%2520Simon%2520B.%2520Eickhoff%2520and%2520Christian%2520Jung%2520and%2520Martin%2520Reuter%2520and%2520Kersten%2520Diers%2520and%2520Malte%2520Kelm%2520and%2520Artur%2520Lichtenberg%2520and%2520Federico%2520Raimondo%2520and%2520Kaustubh%2520R.%2520Patil%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520models%2520benefit%2520from%2520large%2520datasets.%2520Collecting%2520data%2520in%250Abiomedical%2520domains%2520is%2520costly%2520and%2520challenging%252C%2520hence%252C%2520combining%2520datasets%2520has%250Abecome%2520a%2520common%2520practice.%2520However%252C%2520datasets%2520obtained%2520under%2520different%2520conditions%250Acould%2520present%2520undesired%2520site-specific%2520variability.%2520Data%2520harmonization%2520methods%250Aaim%2520to%2520remove%2520site-specific%2520variance%2520while%2520retaining%2520biologically%2520relevant%250Ainformation.%2520This%2520study%2520evaluates%2520the%2520effectiveness%2520of%2520popularly%2520used%250AComBat-based%2520methods%2520for%2520harmonizing%2520data%2520in%2520scenarios%2520where%2520the%2520class%2520balance%250Ais%2520not%2520equal%2520across%2520sites.%2520We%2520find%2520that%2520these%2520methods%2520struggle%2520with%2520data%250Aleakage%2520issues.%2520To%2520overcome%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520approach%250APrettYharmonize%252C%2520designed%2520to%2520harmonize%2520data%2520by%2520pretending%2520the%2520target%2520labels.%2520We%250Avalidate%2520our%2520approach%2520using%2520controlled%2520datasets%2520designed%2520to%2520benchmark%2520the%250Autility%2520of%2520harmonization.%2520Finally%252C%2520using%2520real-world%2520MRI%2520and%2520clinical%2520data%252C%2520we%250Acompare%2520leakage-prone%2520methods%2520with%2520PrettYharmonize%2520and%2520show%2520that%2520it%2520achieves%250Acomparable%2520performance%2520while%2520avoiding%2520data%2520leakage%252C%2520particularly%2520in%250Asite-target-dependence%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20Leakage%20on%20Data%20Harmonization%20in%20Machine%20Learning%20Pipelines%20in%0A%20%20Class%20Imbalance%20Across%20Sites&entry.906535625=Nicol%C3%A1s%20Nieto%20and%20Simon%20B.%20Eickhoff%20and%20Christian%20Jung%20and%20Martin%20Reuter%20and%20Kersten%20Diers%20and%20Malte%20Kelm%20and%20Artur%20Lichtenberg%20and%20Federico%20Raimondo%20and%20Kaustubh%20R.%20Patil&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20models%20benefit%20from%20large%20datasets.%20Collecting%20data%20in%0Abiomedical%20domains%20is%20costly%20and%20challenging%2C%20hence%2C%20combining%20datasets%20has%0Abecome%20a%20common%20practice.%20However%2C%20datasets%20obtained%20under%20different%20conditions%0Acould%20present%20undesired%20site-specific%20variability.%20Data%20harmonization%20methods%0Aaim%20to%20remove%20site-specific%20variance%20while%20retaining%20biologically%20relevant%0Ainformation.%20This%20study%20evaluates%20the%20effectiveness%20of%20popularly%20used%0AComBat-based%20methods%20for%20harmonizing%20data%20in%20scenarios%20where%20the%20class%20balance%0Ais%20not%20equal%20across%20sites.%20We%20find%20that%20these%20methods%20struggle%20with%20data%0Aleakage%20issues.%20To%20overcome%20this%20problem%2C%20we%20propose%20a%20novel%20approach%0APrettYharmonize%2C%20designed%20to%20harmonize%20data%20by%20pretending%20the%20target%20labels.%20We%0Avalidate%20our%20approach%20using%20controlled%20datasets%20designed%20to%20benchmark%20the%0Autility%20of%20harmonization.%20Finally%2C%20using%20real-world%20MRI%20and%20clinical%20data%2C%20we%0Acompare%20leakage-prone%20methods%20with%20PrettYharmonize%20and%20show%20that%20it%20achieves%0Acomparable%20performance%20while%20avoiding%20data%20leakage%2C%20particularly%20in%0Asite-target-dependence%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19643v1&entry.124074799=Read"},
{"title": "Data-Driven Filter Design in FBP: Transforming CT Reconstruction with\n  Trainable Fourier Series", "author": "Yipeng Sun and Linda-Sophie Schneider and Fuxin Fan and Mareike Thies and Mingxuan Gu and Siyuan Mei and Yuzhong Zhou and Siming Bayer and Andreas Maier", "abstract": "  In this study, we introduce a Fourier series-based trainable filter for\ncomputed tomography (CT) reconstruction within the filtered backprojection\n(FBP) framework. This method overcomes the limitation in noise reduction by\noptimizing Fourier series coefficients to construct the filter, maintaining\ncomputational efficiency with minimal increment for the trainable parameters\ncompared to other deep learning frameworks. Additionally, we propose Gaussian\nedge-enhanced (GEE) loss function that prioritizes the $L_1$ norm of\nhigh-frequency magnitudes, effectively countering the blurring problems\nprevalent in mean squared error (MSE) approaches. The model's foundation in the\nFBP algorithm ensures excellent interpretability, as it relies on a data-driven\nfilter with all other parameters derived through rigorous mathematical\nprocedures. Designed as a plug-and-play solution, our Fourier series-based\nfilter can be easily integrated into existing CT reconstruction models, making\nit an adaptable tool for a wide range of practical applications. Code and data\nare available at https://github.com/sypsyp97/Trainable-Fourier-Series.\n", "link": "http://arxiv.org/abs/2401.16039v2", "date": "2024-10-25", "relevancy": 1.9244, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4939}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4849}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Filter%20Design%20in%20FBP%3A%20Transforming%20CT%20Reconstruction%20with%0A%20%20Trainable%20Fourier%20Series&body=Title%3A%20Data-Driven%20Filter%20Design%20in%20FBP%3A%20Transforming%20CT%20Reconstruction%20with%0A%20%20Trainable%20Fourier%20Series%0AAuthor%3A%20Yipeng%20Sun%20and%20Linda-Sophie%20Schneider%20and%20Fuxin%20Fan%20and%20Mareike%20Thies%20and%20Mingxuan%20Gu%20and%20Siyuan%20Mei%20and%20Yuzhong%20Zhou%20and%20Siming%20Bayer%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20a%20Fourier%20series-based%20trainable%20filter%20for%0Acomputed%20tomography%20%28CT%29%20reconstruction%20within%20the%20filtered%20backprojection%0A%28FBP%29%20framework.%20This%20method%20overcomes%20the%20limitation%20in%20noise%20reduction%20by%0Aoptimizing%20Fourier%20series%20coefficients%20to%20construct%20the%20filter%2C%20maintaining%0Acomputational%20efficiency%20with%20minimal%20increment%20for%20the%20trainable%20parameters%0Acompared%20to%20other%20deep%20learning%20frameworks.%20Additionally%2C%20we%20propose%20Gaussian%0Aedge-enhanced%20%28GEE%29%20loss%20function%20that%20prioritizes%20the%20%24L_1%24%20norm%20of%0Ahigh-frequency%20magnitudes%2C%20effectively%20countering%20the%20blurring%20problems%0Aprevalent%20in%20mean%20squared%20error%20%28MSE%29%20approaches.%20The%20model%27s%20foundation%20in%20the%0AFBP%20algorithm%20ensures%20excellent%20interpretability%2C%20as%20it%20relies%20on%20a%20data-driven%0Afilter%20with%20all%20other%20parameters%20derived%20through%20rigorous%20mathematical%0Aprocedures.%20Designed%20as%20a%20plug-and-play%20solution%2C%20our%20Fourier%20series-based%0Afilter%20can%20be%20easily%20integrated%20into%20existing%20CT%20reconstruction%20models%2C%20making%0Ait%20an%20adaptable%20tool%20for%20a%20wide%20range%20of%20practical%20applications.%20Code%20and%20data%0Aare%20available%20at%20https%3A//github.com/sypsyp97/Trainable-Fourier-Series.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Filter%2520Design%2520in%2520FBP%253A%2520Transforming%2520CT%2520Reconstruction%2520with%250A%2520%2520Trainable%2520Fourier%2520Series%26entry.906535625%3DYipeng%2520Sun%2520and%2520Linda-Sophie%2520Schneider%2520and%2520Fuxin%2520Fan%2520and%2520Mareike%2520Thies%2520and%2520Mingxuan%2520Gu%2520and%2520Siyuan%2520Mei%2520and%2520Yuzhong%2520Zhou%2520and%2520Siming%2520Bayer%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520Fourier%2520series-based%2520trainable%2520filter%2520for%250Acomputed%2520tomography%2520%2528CT%2529%2520reconstruction%2520within%2520the%2520filtered%2520backprojection%250A%2528FBP%2529%2520framework.%2520This%2520method%2520overcomes%2520the%2520limitation%2520in%2520noise%2520reduction%2520by%250Aoptimizing%2520Fourier%2520series%2520coefficients%2520to%2520construct%2520the%2520filter%252C%2520maintaining%250Acomputational%2520efficiency%2520with%2520minimal%2520increment%2520for%2520the%2520trainable%2520parameters%250Acompared%2520to%2520other%2520deep%2520learning%2520frameworks.%2520Additionally%252C%2520we%2520propose%2520Gaussian%250Aedge-enhanced%2520%2528GEE%2529%2520loss%2520function%2520that%2520prioritizes%2520the%2520%2524L_1%2524%2520norm%2520of%250Ahigh-frequency%2520magnitudes%252C%2520effectively%2520countering%2520the%2520blurring%2520problems%250Aprevalent%2520in%2520mean%2520squared%2520error%2520%2528MSE%2529%2520approaches.%2520The%2520model%2527s%2520foundation%2520in%2520the%250AFBP%2520algorithm%2520ensures%2520excellent%2520interpretability%252C%2520as%2520it%2520relies%2520on%2520a%2520data-driven%250Afilter%2520with%2520all%2520other%2520parameters%2520derived%2520through%2520rigorous%2520mathematical%250Aprocedures.%2520Designed%2520as%2520a%2520plug-and-play%2520solution%252C%2520our%2520Fourier%2520series-based%250Afilter%2520can%2520be%2520easily%2520integrated%2520into%2520existing%2520CT%2520reconstruction%2520models%252C%2520making%250Ait%2520an%2520adaptable%2520tool%2520for%2520a%2520wide%2520range%2520of%2520practical%2520applications.%2520Code%2520and%2520data%250Aare%2520available%2520at%2520https%253A//github.com/sypsyp97/Trainable-Fourier-Series.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Filter%20Design%20in%20FBP%3A%20Transforming%20CT%20Reconstruction%20with%0A%20%20Trainable%20Fourier%20Series&entry.906535625=Yipeng%20Sun%20and%20Linda-Sophie%20Schneider%20and%20Fuxin%20Fan%20and%20Mareike%20Thies%20and%20Mingxuan%20Gu%20and%20Siyuan%20Mei%20and%20Yuzhong%20Zhou%20and%20Siming%20Bayer%20and%20Andreas%20Maier&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20a%20Fourier%20series-based%20trainable%20filter%20for%0Acomputed%20tomography%20%28CT%29%20reconstruction%20within%20the%20filtered%20backprojection%0A%28FBP%29%20framework.%20This%20method%20overcomes%20the%20limitation%20in%20noise%20reduction%20by%0Aoptimizing%20Fourier%20series%20coefficients%20to%20construct%20the%20filter%2C%20maintaining%0Acomputational%20efficiency%20with%20minimal%20increment%20for%20the%20trainable%20parameters%0Acompared%20to%20other%20deep%20learning%20frameworks.%20Additionally%2C%20we%20propose%20Gaussian%0Aedge-enhanced%20%28GEE%29%20loss%20function%20that%20prioritizes%20the%20%24L_1%24%20norm%20of%0Ahigh-frequency%20magnitudes%2C%20effectively%20countering%20the%20blurring%20problems%0Aprevalent%20in%20mean%20squared%20error%20%28MSE%29%20approaches.%20The%20model%27s%20foundation%20in%20the%0AFBP%20algorithm%20ensures%20excellent%20interpretability%2C%20as%20it%20relies%20on%20a%20data-driven%0Afilter%20with%20all%20other%20parameters%20derived%20through%20rigorous%20mathematical%0Aprocedures.%20Designed%20as%20a%20plug-and-play%20solution%2C%20our%20Fourier%20series-based%0Afilter%20can%20be%20easily%20integrated%20into%20existing%20CT%20reconstruction%20models%2C%20making%0Ait%20an%20adaptable%20tool%20for%20a%20wide%20range%20of%20practical%20applications.%20Code%20and%20data%0Aare%20available%20at%20https%3A//github.com/sypsyp97/Trainable-Fourier-Series.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16039v2&entry.124074799=Read"},
{"title": "Human-AI Collaborative Multi-modal Multi-rater Learning for\n  Endometriosis Diagnosis", "author": "Hu Wang and David Butler and Yuan Zhang and Jodie Avery and Steven Knox and Congbo Ma and Louise Hull and Gustavo Carneiro", "abstract": "  Endometriosis, affecting about 10% of individuals assigned female at birth,\nis challenging to diagnose and manage. Diagnosis typically involves the\nidentification of various signs of the disease using either laparoscopic\nsurgery or the analysis of T1/T2 MRI images, with the latter being quicker and\ncheaper but less accurate. A key diagnostic sign of endometriosis is the\nobliteration of the Pouch of Douglas (POD). However, even experienced\nclinicians struggle with accurately classifying POD obliteration from MRI\nimages, which complicates the training of reliable AI models. In this paper, we\nintroduce the Human-AI Collaborative Multi-modal Multi-rater Learning (HAICOMM)\nmethodology to address the challenge above. HAICOMM is the first method that\nexplores three important aspects of this problem: 1) multi-rater learning to\nextract a cleaner label from the multiple \"noisy\" labels available per training\nsample; 2) multi-modal learning to leverage the presence of T1/T2 MRI images\nfor training and testing; and 3) human-AI collaboration to build a system that\nleverages the predictions from clinicians and the AI model to provide more\naccurate classification than standalone clinicians and AI models. Presenting\nresults on the multi-rater T1/T2 MRI endometriosis dataset that we collected to\nvalidate our methodology, the proposed HAICOMM model outperforms an ensemble of\nclinicians, noisy-label learning models, and multi-rater learning methods.\n", "link": "http://arxiv.org/abs/2409.02046v3", "date": "2024-10-25", "relevancy": 1.6169, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5571}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-AI%20Collaborative%20Multi-modal%20Multi-rater%20Learning%20for%0A%20%20Endometriosis%20Diagnosis&body=Title%3A%20Human-AI%20Collaborative%20Multi-modal%20Multi-rater%20Learning%20for%0A%20%20Endometriosis%20Diagnosis%0AAuthor%3A%20Hu%20Wang%20and%20David%20Butler%20and%20Yuan%20Zhang%20and%20Jodie%20Avery%20and%20Steven%20Knox%20and%20Congbo%20Ma%20and%20Louise%20Hull%20and%20Gustavo%20Carneiro%0AAbstract%3A%20%20%20Endometriosis%2C%20affecting%20about%2010%25%20of%20individuals%20assigned%20female%20at%20birth%2C%0Ais%20challenging%20to%20diagnose%20and%20manage.%20Diagnosis%20typically%20involves%20the%0Aidentification%20of%20various%20signs%20of%20the%20disease%20using%20either%20laparoscopic%0Asurgery%20or%20the%20analysis%20of%20T1/T2%20MRI%20images%2C%20with%20the%20latter%20being%20quicker%20and%0Acheaper%20but%20less%20accurate.%20A%20key%20diagnostic%20sign%20of%20endometriosis%20is%20the%0Aobliteration%20of%20the%20Pouch%20of%20Douglas%20%28POD%29.%20However%2C%20even%20experienced%0Aclinicians%20struggle%20with%20accurately%20classifying%20POD%20obliteration%20from%20MRI%0Aimages%2C%20which%20complicates%20the%20training%20of%20reliable%20AI%20models.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20Human-AI%20Collaborative%20Multi-modal%20Multi-rater%20Learning%20%28HAICOMM%29%0Amethodology%20to%20address%20the%20challenge%20above.%20HAICOMM%20is%20the%20first%20method%20that%0Aexplores%20three%20important%20aspects%20of%20this%20problem%3A%201%29%20multi-rater%20learning%20to%0Aextract%20a%20cleaner%20label%20from%20the%20multiple%20%22noisy%22%20labels%20available%20per%20training%0Asample%3B%202%29%20multi-modal%20learning%20to%20leverage%20the%20presence%20of%20T1/T2%20MRI%20images%0Afor%20training%20and%20testing%3B%20and%203%29%20human-AI%20collaboration%20to%20build%20a%20system%20that%0Aleverages%20the%20predictions%20from%20clinicians%20and%20the%20AI%20model%20to%20provide%20more%0Aaccurate%20classification%20than%20standalone%20clinicians%20and%20AI%20models.%20Presenting%0Aresults%20on%20the%20multi-rater%20T1/T2%20MRI%20endometriosis%20dataset%20that%20we%20collected%20to%0Avalidate%20our%20methodology%2C%20the%20proposed%20HAICOMM%20model%20outperforms%20an%20ensemble%20of%0Aclinicians%2C%20noisy-label%20learning%20models%2C%20and%20multi-rater%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02046v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-AI%2520Collaborative%2520Multi-modal%2520Multi-rater%2520Learning%2520for%250A%2520%2520Endometriosis%2520Diagnosis%26entry.906535625%3DHu%2520Wang%2520and%2520David%2520Butler%2520and%2520Yuan%2520Zhang%2520and%2520Jodie%2520Avery%2520and%2520Steven%2520Knox%2520and%2520Congbo%2520Ma%2520and%2520Louise%2520Hull%2520and%2520Gustavo%2520Carneiro%26entry.1292438233%3D%2520%2520Endometriosis%252C%2520affecting%2520about%252010%2525%2520of%2520individuals%2520assigned%2520female%2520at%2520birth%252C%250Ais%2520challenging%2520to%2520diagnose%2520and%2520manage.%2520Diagnosis%2520typically%2520involves%2520the%250Aidentification%2520of%2520various%2520signs%2520of%2520the%2520disease%2520using%2520either%2520laparoscopic%250Asurgery%2520or%2520the%2520analysis%2520of%2520T1/T2%2520MRI%2520images%252C%2520with%2520the%2520latter%2520being%2520quicker%2520and%250Acheaper%2520but%2520less%2520accurate.%2520A%2520key%2520diagnostic%2520sign%2520of%2520endometriosis%2520is%2520the%250Aobliteration%2520of%2520the%2520Pouch%2520of%2520Douglas%2520%2528POD%2529.%2520However%252C%2520even%2520experienced%250Aclinicians%2520struggle%2520with%2520accurately%2520classifying%2520POD%2520obliteration%2520from%2520MRI%250Aimages%252C%2520which%2520complicates%2520the%2520training%2520of%2520reliable%2520AI%2520models.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520Human-AI%2520Collaborative%2520Multi-modal%2520Multi-rater%2520Learning%2520%2528HAICOMM%2529%250Amethodology%2520to%2520address%2520the%2520challenge%2520above.%2520HAICOMM%2520is%2520the%2520first%2520method%2520that%250Aexplores%2520three%2520important%2520aspects%2520of%2520this%2520problem%253A%25201%2529%2520multi-rater%2520learning%2520to%250Aextract%2520a%2520cleaner%2520label%2520from%2520the%2520multiple%2520%2522noisy%2522%2520labels%2520available%2520per%2520training%250Asample%253B%25202%2529%2520multi-modal%2520learning%2520to%2520leverage%2520the%2520presence%2520of%2520T1/T2%2520MRI%2520images%250Afor%2520training%2520and%2520testing%253B%2520and%25203%2529%2520human-AI%2520collaboration%2520to%2520build%2520a%2520system%2520that%250Aleverages%2520the%2520predictions%2520from%2520clinicians%2520and%2520the%2520AI%2520model%2520to%2520provide%2520more%250Aaccurate%2520classification%2520than%2520standalone%2520clinicians%2520and%2520AI%2520models.%2520Presenting%250Aresults%2520on%2520the%2520multi-rater%2520T1/T2%2520MRI%2520endometriosis%2520dataset%2520that%2520we%2520collected%2520to%250Avalidate%2520our%2520methodology%252C%2520the%2520proposed%2520HAICOMM%2520model%2520outperforms%2520an%2520ensemble%2520of%250Aclinicians%252C%2520noisy-label%2520learning%2520models%252C%2520and%2520multi-rater%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02046v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-AI%20Collaborative%20Multi-modal%20Multi-rater%20Learning%20for%0A%20%20Endometriosis%20Diagnosis&entry.906535625=Hu%20Wang%20and%20David%20Butler%20and%20Yuan%20Zhang%20and%20Jodie%20Avery%20and%20Steven%20Knox%20and%20Congbo%20Ma%20and%20Louise%20Hull%20and%20Gustavo%20Carneiro&entry.1292438233=%20%20Endometriosis%2C%20affecting%20about%2010%25%20of%20individuals%20assigned%20female%20at%20birth%2C%0Ais%20challenging%20to%20diagnose%20and%20manage.%20Diagnosis%20typically%20involves%20the%0Aidentification%20of%20various%20signs%20of%20the%20disease%20using%20either%20laparoscopic%0Asurgery%20or%20the%20analysis%20of%20T1/T2%20MRI%20images%2C%20with%20the%20latter%20being%20quicker%20and%0Acheaper%20but%20less%20accurate.%20A%20key%20diagnostic%20sign%20of%20endometriosis%20is%20the%0Aobliteration%20of%20the%20Pouch%20of%20Douglas%20%28POD%29.%20However%2C%20even%20experienced%0Aclinicians%20struggle%20with%20accurately%20classifying%20POD%20obliteration%20from%20MRI%0Aimages%2C%20which%20complicates%20the%20training%20of%20reliable%20AI%20models.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20Human-AI%20Collaborative%20Multi-modal%20Multi-rater%20Learning%20%28HAICOMM%29%0Amethodology%20to%20address%20the%20challenge%20above.%20HAICOMM%20is%20the%20first%20method%20that%0Aexplores%20three%20important%20aspects%20of%20this%20problem%3A%201%29%20multi-rater%20learning%20to%0Aextract%20a%20cleaner%20label%20from%20the%20multiple%20%22noisy%22%20labels%20available%20per%20training%0Asample%3B%202%29%20multi-modal%20learning%20to%20leverage%20the%20presence%20of%20T1/T2%20MRI%20images%0Afor%20training%20and%20testing%3B%20and%203%29%20human-AI%20collaboration%20to%20build%20a%20system%20that%0Aleverages%20the%20predictions%20from%20clinicians%20and%20the%20AI%20model%20to%20provide%20more%0Aaccurate%20classification%20than%20standalone%20clinicians%20and%20AI%20models.%20Presenting%0Aresults%20on%20the%20multi-rater%20T1/T2%20MRI%20endometriosis%20dataset%20that%20we%20collected%20to%0Avalidate%20our%20methodology%2C%20the%20proposed%20HAICOMM%20model%20outperforms%20an%20ensemble%20of%0Aclinicians%2C%20noisy-label%20learning%20models%2C%20and%20multi-rater%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02046v3&entry.124074799=Read"},
{"title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing,\n  Neural-Conditioning, Expert Swarms, and Task Planning", "author": "Nicole Cho and Nishan Srishankar and Lucas Cecchi and William Watson", "abstract": "  Financial intelligence generation from vast data sources has typically relied\non traditional methods of knowledge-graph construction or database engineering.\nRecently, fine-tuned financial domain-specific Large Language Models (LLMs),\nhave emerged. While these advancements are promising, limitations such as high\ninference costs, hallucinations, and the complexity of concurrently analyzing\nhigh-dimensional financial data, emerge. This motivates our invention FISHNET\n(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,\nExpert swarming, and Task planning), an agentic architecture that accomplishes\nhighly complex analytical tasks for more than 98,000 regulatory filings that\nvary immensely in terms of semantics, data hierarchy, or format. FISHNET shows\nremarkable performance for financial insight generation (61.8% success rate\nover 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to\nempirically prove the success of FISHNET, each agent's importance, and the\noptimized performance of assembling all agents. Our modular architecture can be\nleveraged for a myriad of use-cases, enabling scalability, flexibility, and\ndata integrity that are critical for financial tasks.\n", "link": "http://arxiv.org/abs/2410.19727v1", "date": "2024-10-25", "relevancy": 1.88, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FISHNET%3A%20Financial%20Intelligence%20from%20Sub-querying%2C%20Harmonizing%2C%0A%20%20Neural-Conditioning%2C%20Expert%20Swarms%2C%20and%20Task%20Planning&body=Title%3A%20FISHNET%3A%20Financial%20Intelligence%20from%20Sub-querying%2C%20Harmonizing%2C%0A%20%20Neural-Conditioning%2C%20Expert%20Swarms%2C%20and%20Task%20Planning%0AAuthor%3A%20Nicole%20Cho%20and%20Nishan%20Srishankar%20and%20Lucas%20Cecchi%20and%20William%20Watson%0AAbstract%3A%20%20%20Financial%20intelligence%20generation%20from%20vast%20data%20sources%20has%20typically%20relied%0Aon%20traditional%20methods%20of%20knowledge-graph%20construction%20or%20database%20engineering.%0ARecently%2C%20fine-tuned%20financial%20domain-specific%20Large%20Language%20Models%20%28LLMs%29%2C%0Ahave%20emerged.%20While%20these%20advancements%20are%20promising%2C%20limitations%20such%20as%20high%0Ainference%20costs%2C%20hallucinations%2C%20and%20the%20complexity%20of%20concurrently%20analyzing%0Ahigh-dimensional%20financial%20data%2C%20emerge.%20This%20motivates%20our%20invention%20FISHNET%0A%28Financial%20Intelligence%20from%20Sub-querying%2C%20Harmonizing%2C%20Neural-Conditioning%2C%0AExpert%20swarming%2C%20and%20Task%20planning%29%2C%20an%20agentic%20architecture%20that%20accomplishes%0Ahighly%20complex%20analytical%20tasks%20for%20more%20than%2098%2C000%20regulatory%20filings%20that%0Avary%20immensely%20in%20terms%20of%20semantics%2C%20data%20hierarchy%2C%20or%20format.%20FISHNET%20shows%0Aremarkable%20performance%20for%20financial%20insight%20generation%20%2861.8%25%20success%20rate%0Aover%205.0%25%20Routing%2C%2045.6%25%20RAG%20R-Precision%29.%20We%20conduct%20rigorous%20ablations%20to%0Aempirically%20prove%20the%20success%20of%20FISHNET%2C%20each%20agent%27s%20importance%2C%20and%20the%0Aoptimized%20performance%20of%20assembling%20all%20agents.%20Our%20modular%20architecture%20can%20be%0Aleveraged%20for%20a%20myriad%20of%20use-cases%2C%20enabling%20scalability%2C%20flexibility%2C%20and%0Adata%20integrity%20that%20are%20critical%20for%20financial%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFISHNET%253A%2520Financial%2520Intelligence%2520from%2520Sub-querying%252C%2520Harmonizing%252C%250A%2520%2520Neural-Conditioning%252C%2520Expert%2520Swarms%252C%2520and%2520Task%2520Planning%26entry.906535625%3DNicole%2520Cho%2520and%2520Nishan%2520Srishankar%2520and%2520Lucas%2520Cecchi%2520and%2520William%2520Watson%26entry.1292438233%3D%2520%2520Financial%2520intelligence%2520generation%2520from%2520vast%2520data%2520sources%2520has%2520typically%2520relied%250Aon%2520traditional%2520methods%2520of%2520knowledge-graph%2520construction%2520or%2520database%2520engineering.%250ARecently%252C%2520fine-tuned%2520financial%2520domain-specific%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Ahave%2520emerged.%2520While%2520these%2520advancements%2520are%2520promising%252C%2520limitations%2520such%2520as%2520high%250Ainference%2520costs%252C%2520hallucinations%252C%2520and%2520the%2520complexity%2520of%2520concurrently%2520analyzing%250Ahigh-dimensional%2520financial%2520data%252C%2520emerge.%2520This%2520motivates%2520our%2520invention%2520FISHNET%250A%2528Financial%2520Intelligence%2520from%2520Sub-querying%252C%2520Harmonizing%252C%2520Neural-Conditioning%252C%250AExpert%2520swarming%252C%2520and%2520Task%2520planning%2529%252C%2520an%2520agentic%2520architecture%2520that%2520accomplishes%250Ahighly%2520complex%2520analytical%2520tasks%2520for%2520more%2520than%252098%252C000%2520regulatory%2520filings%2520that%250Avary%2520immensely%2520in%2520terms%2520of%2520semantics%252C%2520data%2520hierarchy%252C%2520or%2520format.%2520FISHNET%2520shows%250Aremarkable%2520performance%2520for%2520financial%2520insight%2520generation%2520%252861.8%2525%2520success%2520rate%250Aover%25205.0%2525%2520Routing%252C%252045.6%2525%2520RAG%2520R-Precision%2529.%2520We%2520conduct%2520rigorous%2520ablations%2520to%250Aempirically%2520prove%2520the%2520success%2520of%2520FISHNET%252C%2520each%2520agent%2527s%2520importance%252C%2520and%2520the%250Aoptimized%2520performance%2520of%2520assembling%2520all%2520agents.%2520Our%2520modular%2520architecture%2520can%2520be%250Aleveraged%2520for%2520a%2520myriad%2520of%2520use-cases%252C%2520enabling%2520scalability%252C%2520flexibility%252C%2520and%250Adata%2520integrity%2520that%2520are%2520critical%2520for%2520financial%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FISHNET%3A%20Financial%20Intelligence%20from%20Sub-querying%2C%20Harmonizing%2C%0A%20%20Neural-Conditioning%2C%20Expert%20Swarms%2C%20and%20Task%20Planning&entry.906535625=Nicole%20Cho%20and%20Nishan%20Srishankar%20and%20Lucas%20Cecchi%20and%20William%20Watson&entry.1292438233=%20%20Financial%20intelligence%20generation%20from%20vast%20data%20sources%20has%20typically%20relied%0Aon%20traditional%20methods%20of%20knowledge-graph%20construction%20or%20database%20engineering.%0ARecently%2C%20fine-tuned%20financial%20domain-specific%20Large%20Language%20Models%20%28LLMs%29%2C%0Ahave%20emerged.%20While%20these%20advancements%20are%20promising%2C%20limitations%20such%20as%20high%0Ainference%20costs%2C%20hallucinations%2C%20and%20the%20complexity%20of%20concurrently%20analyzing%0Ahigh-dimensional%20financial%20data%2C%20emerge.%20This%20motivates%20our%20invention%20FISHNET%0A%28Financial%20Intelligence%20from%20Sub-querying%2C%20Harmonizing%2C%20Neural-Conditioning%2C%0AExpert%20swarming%2C%20and%20Task%20planning%29%2C%20an%20agentic%20architecture%20that%20accomplishes%0Ahighly%20complex%20analytical%20tasks%20for%20more%20than%2098%2C000%20regulatory%20filings%20that%0Avary%20immensely%20in%20terms%20of%20semantics%2C%20data%20hierarchy%2C%20or%20format.%20FISHNET%20shows%0Aremarkable%20performance%20for%20financial%20insight%20generation%20%2861.8%25%20success%20rate%0Aover%205.0%25%20Routing%2C%2045.6%25%20RAG%20R-Precision%29.%20We%20conduct%20rigorous%20ablations%20to%0Aempirically%20prove%20the%20success%20of%20FISHNET%2C%20each%20agent%27s%20importance%2C%20and%20the%0Aoptimized%20performance%20of%20assembling%20all%20agents.%20Our%20modular%20architecture%20can%20be%0Aleveraged%20for%20a%20myriad%20of%20use-cases%2C%20enabling%20scalability%2C%20flexibility%2C%20and%0Adata%20integrity%20that%20are%20critical%20for%20financial%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19727v1&entry.124074799=Read"},
{"title": "Configura\u00e7\u00e3o e opera\u00e7\u00e3o da plataforma Clearpath Husky A200 e\n  Manipulador Cobot UR5 2-Finger Gripper", "author": "Hiago Sodre and Sebastian Barcelona and Vincent Sandin and Pablo Moraes and Christopher Peters and Braian Vidal and Ang\u00e9l da Silva and Gabriela Flores and Ahilen Mazondo and Santiago Fern\u00e1ndez and Assun\u00e7\u00e3o Nathalie and de Vargas Bruna and Grando Ricardo and Kelbouscas Andr\u00e9", "abstract": "  This article presents initial configuration work and use of the robotic\nplatform and manipulator in question. The development of the ideal\nconfiguration for using this robot serves as a guide for new users and also\nvalidates its functionality for use in projects. Husky is a large payload\ncapacity and power systems robotics development platform that accommodates a\nwide variety of payloads, customized to meet research needs. Together with the\nCobot UR5 Manipulator attached to its base, it expands the application area of\nits capacity in projects. Advances in robots and mobile manipulators have\nrevolutionized industries by automating tasks that previously required human\nintervention. These innovations alone increase productivity but also reduce\noperating costs, which makes the company more competitive in an evolving global\nmarket. Therefore, this article investigates the functionalities of this robot\nto validate its execution in robotics projects.\n", "link": "http://arxiv.org/abs/2410.17453v2", "date": "2024-10-25", "relevancy": 1.7026, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4614}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4223}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Configura%C3%A7%C3%A3o%20e%20opera%C3%A7%C3%A3o%20da%20plataforma%20Clearpath%20Husky%20A200%20e%0A%20%20Manipulador%20Cobot%20UR5%202-Finger%20Gripper&body=Title%3A%20Configura%C3%A7%C3%A3o%20e%20opera%C3%A7%C3%A3o%20da%20plataforma%20Clearpath%20Husky%20A200%20e%0A%20%20Manipulador%20Cobot%20UR5%202-Finger%20Gripper%0AAuthor%3A%20Hiago%20Sodre%20and%20Sebastian%20Barcelona%20and%20Vincent%20Sandin%20and%20Pablo%20Moraes%20and%20Christopher%20Peters%20and%20Braian%20Vidal%20and%20Ang%C3%A9l%20da%20Silva%20and%20Gabriela%20Flores%20and%20Ahilen%20Mazondo%20and%20Santiago%20Fern%C3%A1ndez%20and%20Assun%C3%A7%C3%A3o%20Nathalie%20and%20de%20Vargas%20Bruna%20and%20Grando%20Ricardo%20and%20Kelbouscas%20Andr%C3%A9%0AAbstract%3A%20%20%20This%20article%20presents%20initial%20configuration%20work%20and%20use%20of%20the%20robotic%0Aplatform%20and%20manipulator%20in%20question.%20The%20development%20of%20the%20ideal%0Aconfiguration%20for%20using%20this%20robot%20serves%20as%20a%20guide%20for%20new%20users%20and%20also%0Avalidates%20its%20functionality%20for%20use%20in%20projects.%20Husky%20is%20a%20large%20payload%0Acapacity%20and%20power%20systems%20robotics%20development%20platform%20that%20accommodates%20a%0Awide%20variety%20of%20payloads%2C%20customized%20to%20meet%20research%20needs.%20Together%20with%20the%0ACobot%20UR5%20Manipulator%20attached%20to%20its%20base%2C%20it%20expands%20the%20application%20area%20of%0Aits%20capacity%20in%20projects.%20Advances%20in%20robots%20and%20mobile%20manipulators%20have%0Arevolutionized%20industries%20by%20automating%20tasks%20that%20previously%20required%20human%0Aintervention.%20These%20innovations%20alone%20increase%20productivity%20but%20also%20reduce%0Aoperating%20costs%2C%20which%20makes%20the%20company%20more%20competitive%20in%20an%20evolving%20global%0Amarket.%20Therefore%2C%20this%20article%20investigates%20the%20functionalities%20of%20this%20robot%0Ato%20validate%20its%20execution%20in%20robotics%20projects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17453v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfigura%25C3%25A7%25C3%25A3o%2520e%2520opera%25C3%25A7%25C3%25A3o%2520da%2520plataforma%2520Clearpath%2520Husky%2520A200%2520e%250A%2520%2520Manipulador%2520Cobot%2520UR5%25202-Finger%2520Gripper%26entry.906535625%3DHiago%2520Sodre%2520and%2520Sebastian%2520Barcelona%2520and%2520Vincent%2520Sandin%2520and%2520Pablo%2520Moraes%2520and%2520Christopher%2520Peters%2520and%2520Braian%2520Vidal%2520and%2520Ang%25C3%25A9l%2520da%2520Silva%2520and%2520Gabriela%2520Flores%2520and%2520Ahilen%2520Mazondo%2520and%2520Santiago%2520Fern%25C3%25A1ndez%2520and%2520Assun%25C3%25A7%25C3%25A3o%2520Nathalie%2520and%2520de%2520Vargas%2520Bruna%2520and%2520Grando%2520Ricardo%2520and%2520Kelbouscas%2520Andr%25C3%25A9%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520initial%2520configuration%2520work%2520and%2520use%2520of%2520the%2520robotic%250Aplatform%2520and%2520manipulator%2520in%2520question.%2520The%2520development%2520of%2520the%2520ideal%250Aconfiguration%2520for%2520using%2520this%2520robot%2520serves%2520as%2520a%2520guide%2520for%2520new%2520users%2520and%2520also%250Avalidates%2520its%2520functionality%2520for%2520use%2520in%2520projects.%2520Husky%2520is%2520a%2520large%2520payload%250Acapacity%2520and%2520power%2520systems%2520robotics%2520development%2520platform%2520that%2520accommodates%2520a%250Awide%2520variety%2520of%2520payloads%252C%2520customized%2520to%2520meet%2520research%2520needs.%2520Together%2520with%2520the%250ACobot%2520UR5%2520Manipulator%2520attached%2520to%2520its%2520base%252C%2520it%2520expands%2520the%2520application%2520area%2520of%250Aits%2520capacity%2520in%2520projects.%2520Advances%2520in%2520robots%2520and%2520mobile%2520manipulators%2520have%250Arevolutionized%2520industries%2520by%2520automating%2520tasks%2520that%2520previously%2520required%2520human%250Aintervention.%2520These%2520innovations%2520alone%2520increase%2520productivity%2520but%2520also%2520reduce%250Aoperating%2520costs%252C%2520which%2520makes%2520the%2520company%2520more%2520competitive%2520in%2520an%2520evolving%2520global%250Amarket.%2520Therefore%252C%2520this%2520article%2520investigates%2520the%2520functionalities%2520of%2520this%2520robot%250Ato%2520validate%2520its%2520execution%2520in%2520robotics%2520projects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17453v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Configura%C3%A7%C3%A3o%20e%20opera%C3%A7%C3%A3o%20da%20plataforma%20Clearpath%20Husky%20A200%20e%0A%20%20Manipulador%20Cobot%20UR5%202-Finger%20Gripper&entry.906535625=Hiago%20Sodre%20and%20Sebastian%20Barcelona%20and%20Vincent%20Sandin%20and%20Pablo%20Moraes%20and%20Christopher%20Peters%20and%20Braian%20Vidal%20and%20Ang%C3%A9l%20da%20Silva%20and%20Gabriela%20Flores%20and%20Ahilen%20Mazondo%20and%20Santiago%20Fern%C3%A1ndez%20and%20Assun%C3%A7%C3%A3o%20Nathalie%20and%20de%20Vargas%20Bruna%20and%20Grando%20Ricardo%20and%20Kelbouscas%20Andr%C3%A9&entry.1292438233=%20%20This%20article%20presents%20initial%20configuration%20work%20and%20use%20of%20the%20robotic%0Aplatform%20and%20manipulator%20in%20question.%20The%20development%20of%20the%20ideal%0Aconfiguration%20for%20using%20this%20robot%20serves%20as%20a%20guide%20for%20new%20users%20and%20also%0Avalidates%20its%20functionality%20for%20use%20in%20projects.%20Husky%20is%20a%20large%20payload%0Acapacity%20and%20power%20systems%20robotics%20development%20platform%20that%20accommodates%20a%0Awide%20variety%20of%20payloads%2C%20customized%20to%20meet%20research%20needs.%20Together%20with%20the%0ACobot%20UR5%20Manipulator%20attached%20to%20its%20base%2C%20it%20expands%20the%20application%20area%20of%0Aits%20capacity%20in%20projects.%20Advances%20in%20robots%20and%20mobile%20manipulators%20have%0Arevolutionized%20industries%20by%20automating%20tasks%20that%20previously%20required%20human%0Aintervention.%20These%20innovations%20alone%20increase%20productivity%20but%20also%20reduce%0Aoperating%20costs%2C%20which%20makes%20the%20company%20more%20competitive%20in%20an%20evolving%20global%0Amarket.%20Therefore%2C%20this%20article%20investigates%20the%20functionalities%20of%20this%20robot%0Ato%20validate%20its%20execution%20in%20robotics%20projects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17453v2&entry.124074799=Read"},
{"title": "Beyond the Cascade: Juggling Vanilla Siteswap Patterns", "author": "Mario Gomez Andreu and Kai Ploeger and Jan Peters", "abstract": "  Being widespread in human motor behavior, dynamic movements demonstrate\nhigher efficiency and greater capacity to address a broader range of skill\ndomains compared to their quasi-static counterparts. Among the frequently\nstudied dynamic manipulation problems, robotic juggling tasks stand out due to\ntheir inherent ability to scale their difficulty levels to arbitrary extents,\nmaking them an excellent subject for investigation. In this study, we explore\njuggling patterns with mixed throw heights, following the vanilla siteswap\njuggling notation, which jugglers widely adopted to describe toss juggling\npatterns. This requires extending our previous analysis of the simpler cascade\njuggling task by a throw-height sequence planner and further constraints on the\nend effector trajectory. These are not necessary for cascade patterns but are\nvital to achieving patterns with mixed throw heights. Using a simulated\nenvironment, we demonstrate successful juggling of most common 3-9 ball\nsiteswap patterns up to 9 ball height, transitions between these patterns, and\nrandom sequences covering all possible vanilla siteswap patterns with throws\nbetween 2 and 9 ball height. https://kai-ploeger.com/beyond-cascades\n", "link": "http://arxiv.org/abs/2410.19591v1", "date": "2024-10-25", "relevancy": 1.3397, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4617}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4557}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Cascade%3A%20Juggling%20Vanilla%20Siteswap%20Patterns&body=Title%3A%20Beyond%20the%20Cascade%3A%20Juggling%20Vanilla%20Siteswap%20Patterns%0AAuthor%3A%20Mario%20Gomez%20Andreu%20and%20Kai%20Ploeger%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Being%20widespread%20in%20human%20motor%20behavior%2C%20dynamic%20movements%20demonstrate%0Ahigher%20efficiency%20and%20greater%20capacity%20to%20address%20a%20broader%20range%20of%20skill%0Adomains%20compared%20to%20their%20quasi-static%20counterparts.%20Among%20the%20frequently%0Astudied%20dynamic%20manipulation%20problems%2C%20robotic%20juggling%20tasks%20stand%20out%20due%20to%0Atheir%20inherent%20ability%20to%20scale%20their%20difficulty%20levels%20to%20arbitrary%20extents%2C%0Amaking%20them%20an%20excellent%20subject%20for%20investigation.%20In%20this%20study%2C%20we%20explore%0Ajuggling%20patterns%20with%20mixed%20throw%20heights%2C%20following%20the%20vanilla%20siteswap%0Ajuggling%20notation%2C%20which%20jugglers%20widely%20adopted%20to%20describe%20toss%20juggling%0Apatterns.%20This%20requires%20extending%20our%20previous%20analysis%20of%20the%20simpler%20cascade%0Ajuggling%20task%20by%20a%20throw-height%20sequence%20planner%20and%20further%20constraints%20on%20the%0Aend%20effector%20trajectory.%20These%20are%20not%20necessary%20for%20cascade%20patterns%20but%20are%0Avital%20to%20achieving%20patterns%20with%20mixed%20throw%20heights.%20Using%20a%20simulated%0Aenvironment%2C%20we%20demonstrate%20successful%20juggling%20of%20most%20common%203-9%20ball%0Asiteswap%20patterns%20up%20to%209%20ball%20height%2C%20transitions%20between%20these%20patterns%2C%20and%0Arandom%20sequences%20covering%20all%20possible%20vanilla%20siteswap%20patterns%20with%20throws%0Abetween%202%20and%209%20ball%20height.%20https%3A//kai-ploeger.com/beyond-cascades%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Cascade%253A%2520Juggling%2520Vanilla%2520Siteswap%2520Patterns%26entry.906535625%3DMario%2520Gomez%2520Andreu%2520and%2520Kai%2520Ploeger%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Being%2520widespread%2520in%2520human%2520motor%2520behavior%252C%2520dynamic%2520movements%2520demonstrate%250Ahigher%2520efficiency%2520and%2520greater%2520capacity%2520to%2520address%2520a%2520broader%2520range%2520of%2520skill%250Adomains%2520compared%2520to%2520their%2520quasi-static%2520counterparts.%2520Among%2520the%2520frequently%250Astudied%2520dynamic%2520manipulation%2520problems%252C%2520robotic%2520juggling%2520tasks%2520stand%2520out%2520due%2520to%250Atheir%2520inherent%2520ability%2520to%2520scale%2520their%2520difficulty%2520levels%2520to%2520arbitrary%2520extents%252C%250Amaking%2520them%2520an%2520excellent%2520subject%2520for%2520investigation.%2520In%2520this%2520study%252C%2520we%2520explore%250Ajuggling%2520patterns%2520with%2520mixed%2520throw%2520heights%252C%2520following%2520the%2520vanilla%2520siteswap%250Ajuggling%2520notation%252C%2520which%2520jugglers%2520widely%2520adopted%2520to%2520describe%2520toss%2520juggling%250Apatterns.%2520This%2520requires%2520extending%2520our%2520previous%2520analysis%2520of%2520the%2520simpler%2520cascade%250Ajuggling%2520task%2520by%2520a%2520throw-height%2520sequence%2520planner%2520and%2520further%2520constraints%2520on%2520the%250Aend%2520effector%2520trajectory.%2520These%2520are%2520not%2520necessary%2520for%2520cascade%2520patterns%2520but%2520are%250Avital%2520to%2520achieving%2520patterns%2520with%2520mixed%2520throw%2520heights.%2520Using%2520a%2520simulated%250Aenvironment%252C%2520we%2520demonstrate%2520successful%2520juggling%2520of%2520most%2520common%25203-9%2520ball%250Asiteswap%2520patterns%2520up%2520to%25209%2520ball%2520height%252C%2520transitions%2520between%2520these%2520patterns%252C%2520and%250Arandom%2520sequences%2520covering%2520all%2520possible%2520vanilla%2520siteswap%2520patterns%2520with%2520throws%250Abetween%25202%2520and%25209%2520ball%2520height.%2520https%253A//kai-ploeger.com/beyond-cascades%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Cascade%3A%20Juggling%20Vanilla%20Siteswap%20Patterns&entry.906535625=Mario%20Gomez%20Andreu%20and%20Kai%20Ploeger%20and%20Jan%20Peters&entry.1292438233=%20%20Being%20widespread%20in%20human%20motor%20behavior%2C%20dynamic%20movements%20demonstrate%0Ahigher%20efficiency%20and%20greater%20capacity%20to%20address%20a%20broader%20range%20of%20skill%0Adomains%20compared%20to%20their%20quasi-static%20counterparts.%20Among%20the%20frequently%0Astudied%20dynamic%20manipulation%20problems%2C%20robotic%20juggling%20tasks%20stand%20out%20due%20to%0Atheir%20inherent%20ability%20to%20scale%20their%20difficulty%20levels%20to%20arbitrary%20extents%2C%0Amaking%20them%20an%20excellent%20subject%20for%20investigation.%20In%20this%20study%2C%20we%20explore%0Ajuggling%20patterns%20with%20mixed%20throw%20heights%2C%20following%20the%20vanilla%20siteswap%0Ajuggling%20notation%2C%20which%20jugglers%20widely%20adopted%20to%20describe%20toss%20juggling%0Apatterns.%20This%20requires%20extending%20our%20previous%20analysis%20of%20the%20simpler%20cascade%0Ajuggling%20task%20by%20a%20throw-height%20sequence%20planner%20and%20further%20constraints%20on%20the%0Aend%20effector%20trajectory.%20These%20are%20not%20necessary%20for%20cascade%20patterns%20but%20are%0Avital%20to%20achieving%20patterns%20with%20mixed%20throw%20heights.%20Using%20a%20simulated%0Aenvironment%2C%20we%20demonstrate%20successful%20juggling%20of%20most%20common%203-9%20ball%0Asiteswap%20patterns%20up%20to%209%20ball%20height%2C%20transitions%20between%20these%20patterns%2C%20and%0Arandom%20sequences%20covering%20all%20possible%20vanilla%20siteswap%20patterns%20with%20throws%0Abetween%202%20and%209%20ball%20height.%20https%3A//kai-ploeger.com/beyond-cascades%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19591v1&entry.124074799=Read"},
{"title": "Peter Parker or Spiderman? Disambiguating Multiple Class Labels", "author": "Nuthan Mummani and Simran Ketha and Venkatakrishnan Ramaswamy", "abstract": "  In the supervised classification setting, during inference, deep networks\ntypically make multiple predictions. For a pair of such predictions (that are\nin the top-k predictions), two distinct possibilities might occur. On the one\nhand, each of the two predictions might be primarily driven by two distinct\nsets of entities in the input. On the other hand, it is possible that there is\na single entity or set of entities that is driving the prediction for both the\nclasses in question. This latter case, in effect, corresponds to the network\nmaking two separate guesses about the identity of a single entity type.\nClearly, both the guesses cannot be true, i.e. both the labels cannot be\npresent in the input. Current techniques in interpretability research do not\nreadily disambiguate these two cases, since they typically consider input\nattributions for one class label at a time. Here, we present a framework and\nmethod to do so, leveraging modern segmentation and input attribution\ntechniques. Notably, our framework also provides a simple counterfactual\n\"proof\" of each case, which can be verified for the input on the model (i.e.\nwithout running the method again). We demonstrate that the method performs well\nfor a number of samples from the ImageNet validation set and on multiple\nmodels.\n", "link": "http://arxiv.org/abs/2410.19479v1", "date": "2024-10-25", "relevancy": 1.574, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5468}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.53}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Peter%20Parker%20or%20Spiderman%3F%20Disambiguating%20Multiple%20Class%20Labels&body=Title%3A%20Peter%20Parker%20or%20Spiderman%3F%20Disambiguating%20Multiple%20Class%20Labels%0AAuthor%3A%20Nuthan%20Mummani%20and%20Simran%20Ketha%20and%20Venkatakrishnan%20Ramaswamy%0AAbstract%3A%20%20%20In%20the%20supervised%20classification%20setting%2C%20during%20inference%2C%20deep%20networks%0Atypically%20make%20multiple%20predictions.%20For%20a%20pair%20of%20such%20predictions%20%28that%20are%0Ain%20the%20top-k%20predictions%29%2C%20two%20distinct%20possibilities%20might%20occur.%20On%20the%20one%0Ahand%2C%20each%20of%20the%20two%20predictions%20might%20be%20primarily%20driven%20by%20two%20distinct%0Asets%20of%20entities%20in%20the%20input.%20On%20the%20other%20hand%2C%20it%20is%20possible%20that%20there%20is%0Aa%20single%20entity%20or%20set%20of%20entities%20that%20is%20driving%20the%20prediction%20for%20both%20the%0Aclasses%20in%20question.%20This%20latter%20case%2C%20in%20effect%2C%20corresponds%20to%20the%20network%0Amaking%20two%20separate%20guesses%20about%20the%20identity%20of%20a%20single%20entity%20type.%0AClearly%2C%20both%20the%20guesses%20cannot%20be%20true%2C%20i.e.%20both%20the%20labels%20cannot%20be%0Apresent%20in%20the%20input.%20Current%20techniques%20in%20interpretability%20research%20do%20not%0Areadily%20disambiguate%20these%20two%20cases%2C%20since%20they%20typically%20consider%20input%0Aattributions%20for%20one%20class%20label%20at%20a%20time.%20Here%2C%20we%20present%20a%20framework%20and%0Amethod%20to%20do%20so%2C%20leveraging%20modern%20segmentation%20and%20input%20attribution%0Atechniques.%20Notably%2C%20our%20framework%20also%20provides%20a%20simple%20counterfactual%0A%22proof%22%20of%20each%20case%2C%20which%20can%20be%20verified%20for%20the%20input%20on%20the%20model%20%28i.e.%0Awithout%20running%20the%20method%20again%29.%20We%20demonstrate%20that%20the%20method%20performs%20well%0Afor%20a%20number%20of%20samples%20from%20the%20ImageNet%20validation%20set%20and%20on%20multiple%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPeter%2520Parker%2520or%2520Spiderman%253F%2520Disambiguating%2520Multiple%2520Class%2520Labels%26entry.906535625%3DNuthan%2520Mummani%2520and%2520Simran%2520Ketha%2520and%2520Venkatakrishnan%2520Ramaswamy%26entry.1292438233%3D%2520%2520In%2520the%2520supervised%2520classification%2520setting%252C%2520during%2520inference%252C%2520deep%2520networks%250Atypically%2520make%2520multiple%2520predictions.%2520For%2520a%2520pair%2520of%2520such%2520predictions%2520%2528that%2520are%250Ain%2520the%2520top-k%2520predictions%2529%252C%2520two%2520distinct%2520possibilities%2520might%2520occur.%2520On%2520the%2520one%250Ahand%252C%2520each%2520of%2520the%2520two%2520predictions%2520might%2520be%2520primarily%2520driven%2520by%2520two%2520distinct%250Asets%2520of%2520entities%2520in%2520the%2520input.%2520On%2520the%2520other%2520hand%252C%2520it%2520is%2520possible%2520that%2520there%2520is%250Aa%2520single%2520entity%2520or%2520set%2520of%2520entities%2520that%2520is%2520driving%2520the%2520prediction%2520for%2520both%2520the%250Aclasses%2520in%2520question.%2520This%2520latter%2520case%252C%2520in%2520effect%252C%2520corresponds%2520to%2520the%2520network%250Amaking%2520two%2520separate%2520guesses%2520about%2520the%2520identity%2520of%2520a%2520single%2520entity%2520type.%250AClearly%252C%2520both%2520the%2520guesses%2520cannot%2520be%2520true%252C%2520i.e.%2520both%2520the%2520labels%2520cannot%2520be%250Apresent%2520in%2520the%2520input.%2520Current%2520techniques%2520in%2520interpretability%2520research%2520do%2520not%250Areadily%2520disambiguate%2520these%2520two%2520cases%252C%2520since%2520they%2520typically%2520consider%2520input%250Aattributions%2520for%2520one%2520class%2520label%2520at%2520a%2520time.%2520Here%252C%2520we%2520present%2520a%2520framework%2520and%250Amethod%2520to%2520do%2520so%252C%2520leveraging%2520modern%2520segmentation%2520and%2520input%2520attribution%250Atechniques.%2520Notably%252C%2520our%2520framework%2520also%2520provides%2520a%2520simple%2520counterfactual%250A%2522proof%2522%2520of%2520each%2520case%252C%2520which%2520can%2520be%2520verified%2520for%2520the%2520input%2520on%2520the%2520model%2520%2528i.e.%250Awithout%2520running%2520the%2520method%2520again%2529.%2520We%2520demonstrate%2520that%2520the%2520method%2520performs%2520well%250Afor%2520a%2520number%2520of%2520samples%2520from%2520the%2520ImageNet%2520validation%2520set%2520and%2520on%2520multiple%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Peter%20Parker%20or%20Spiderman%3F%20Disambiguating%20Multiple%20Class%20Labels&entry.906535625=Nuthan%20Mummani%20and%20Simran%20Ketha%20and%20Venkatakrishnan%20Ramaswamy&entry.1292438233=%20%20In%20the%20supervised%20classification%20setting%2C%20during%20inference%2C%20deep%20networks%0Atypically%20make%20multiple%20predictions.%20For%20a%20pair%20of%20such%20predictions%20%28that%20are%0Ain%20the%20top-k%20predictions%29%2C%20two%20distinct%20possibilities%20might%20occur.%20On%20the%20one%0Ahand%2C%20each%20of%20the%20two%20predictions%20might%20be%20primarily%20driven%20by%20two%20distinct%0Asets%20of%20entities%20in%20the%20input.%20On%20the%20other%20hand%2C%20it%20is%20possible%20that%20there%20is%0Aa%20single%20entity%20or%20set%20of%20entities%20that%20is%20driving%20the%20prediction%20for%20both%20the%0Aclasses%20in%20question.%20This%20latter%20case%2C%20in%20effect%2C%20corresponds%20to%20the%20network%0Amaking%20two%20separate%20guesses%20about%20the%20identity%20of%20a%20single%20entity%20type.%0AClearly%2C%20both%20the%20guesses%20cannot%20be%20true%2C%20i.e.%20both%20the%20labels%20cannot%20be%0Apresent%20in%20the%20input.%20Current%20techniques%20in%20interpretability%20research%20do%20not%0Areadily%20disambiguate%20these%20two%20cases%2C%20since%20they%20typically%20consider%20input%0Aattributions%20for%20one%20class%20label%20at%20a%20time.%20Here%2C%20we%20present%20a%20framework%20and%0Amethod%20to%20do%20so%2C%20leveraging%20modern%20segmentation%20and%20input%20attribution%0Atechniques.%20Notably%2C%20our%20framework%20also%20provides%20a%20simple%20counterfactual%0A%22proof%22%20of%20each%20case%2C%20which%20can%20be%20verified%20for%20the%20input%20on%20the%20model%20%28i.e.%0Awithout%20running%20the%20method%20again%29.%20We%20demonstrate%20that%20the%20method%20performs%20well%0Afor%20a%20number%20of%20samples%20from%20the%20ImageNet%20validation%20set%20and%20on%20multiple%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19479v1&entry.124074799=Read"},
{"title": "SODA: a Soft Origami Dynamic utensil for Assisted feeding", "author": "Yuxin Ray Song and Shufan Wang", "abstract": "  SODA aims to revolutionize assistive feeding systems by designing a\nmulti-purpose utensil using origami-inspired artificial muscles. Traditional\nutensils, such as forks and spoons,are hard and stiff, causing discomfort and\nfear among users, especially when operated by autonomous robotic arms.\nAdditionally, these systems require frequent utensil changes to handle\ndifferent food types. Our innovative utensil design addresses these issues by\noffering a versatile, adaptive solution that can seamlessly transition between\ngripping and scooping various foods without the need for manual intervention.\nUtilizing the flexibility and strength of origami-inspired artificial muscles,\nthe utensil ensures safe and comfortable interactions, enhancing user\nexperience and efficiency. This approach not only simplifies the feeding\nprocess but also promotes greater independence for individuals with limited\nmobility, contributing to the advancement of soft robotics in healthcare\napplications.\n", "link": "http://arxiv.org/abs/2410.19558v1", "date": "2024-10-25", "relevancy": 1.813, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4649}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4621}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SODA%3A%20a%20Soft%20Origami%20Dynamic%20utensil%20for%20Assisted%20feeding&body=Title%3A%20SODA%3A%20a%20Soft%20Origami%20Dynamic%20utensil%20for%20Assisted%20feeding%0AAuthor%3A%20Yuxin%20Ray%20Song%20and%20Shufan%20Wang%0AAbstract%3A%20%20%20SODA%20aims%20to%20revolutionize%20assistive%20feeding%20systems%20by%20designing%20a%0Amulti-purpose%20utensil%20using%20origami-inspired%20artificial%20muscles.%20Traditional%0Autensils%2C%20such%20as%20forks%20and%20spoons%2Care%20hard%20and%20stiff%2C%20causing%20discomfort%20and%0Afear%20among%20users%2C%20especially%20when%20operated%20by%20autonomous%20robotic%20arms.%0AAdditionally%2C%20these%20systems%20require%20frequent%20utensil%20changes%20to%20handle%0Adifferent%20food%20types.%20Our%20innovative%20utensil%20design%20addresses%20these%20issues%20by%0Aoffering%20a%20versatile%2C%20adaptive%20solution%20that%20can%20seamlessly%20transition%20between%0Agripping%20and%20scooping%20various%20foods%20without%20the%20need%20for%20manual%20intervention.%0AUtilizing%20the%20flexibility%20and%20strength%20of%20origami-inspired%20artificial%20muscles%2C%0Athe%20utensil%20ensures%20safe%20and%20comfortable%20interactions%2C%20enhancing%20user%0Aexperience%20and%20efficiency.%20This%20approach%20not%20only%20simplifies%20the%20feeding%0Aprocess%20but%20also%20promotes%20greater%20independence%20for%20individuals%20with%20limited%0Amobility%2C%20contributing%20to%20the%20advancement%20of%20soft%20robotics%20in%20healthcare%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSODA%253A%2520a%2520Soft%2520Origami%2520Dynamic%2520utensil%2520for%2520Assisted%2520feeding%26entry.906535625%3DYuxin%2520Ray%2520Song%2520and%2520Shufan%2520Wang%26entry.1292438233%3D%2520%2520SODA%2520aims%2520to%2520revolutionize%2520assistive%2520feeding%2520systems%2520by%2520designing%2520a%250Amulti-purpose%2520utensil%2520using%2520origami-inspired%2520artificial%2520muscles.%2520Traditional%250Autensils%252C%2520such%2520as%2520forks%2520and%2520spoons%252Care%2520hard%2520and%2520stiff%252C%2520causing%2520discomfort%2520and%250Afear%2520among%2520users%252C%2520especially%2520when%2520operated%2520by%2520autonomous%2520robotic%2520arms.%250AAdditionally%252C%2520these%2520systems%2520require%2520frequent%2520utensil%2520changes%2520to%2520handle%250Adifferent%2520food%2520types.%2520Our%2520innovative%2520utensil%2520design%2520addresses%2520these%2520issues%2520by%250Aoffering%2520a%2520versatile%252C%2520adaptive%2520solution%2520that%2520can%2520seamlessly%2520transition%2520between%250Agripping%2520and%2520scooping%2520various%2520foods%2520without%2520the%2520need%2520for%2520manual%2520intervention.%250AUtilizing%2520the%2520flexibility%2520and%2520strength%2520of%2520origami-inspired%2520artificial%2520muscles%252C%250Athe%2520utensil%2520ensures%2520safe%2520and%2520comfortable%2520interactions%252C%2520enhancing%2520user%250Aexperience%2520and%2520efficiency.%2520This%2520approach%2520not%2520only%2520simplifies%2520the%2520feeding%250Aprocess%2520but%2520also%2520promotes%2520greater%2520independence%2520for%2520individuals%2520with%2520limited%250Amobility%252C%2520contributing%2520to%2520the%2520advancement%2520of%2520soft%2520robotics%2520in%2520healthcare%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SODA%3A%20a%20Soft%20Origami%20Dynamic%20utensil%20for%20Assisted%20feeding&entry.906535625=Yuxin%20Ray%20Song%20and%20Shufan%20Wang&entry.1292438233=%20%20SODA%20aims%20to%20revolutionize%20assistive%20feeding%20systems%20by%20designing%20a%0Amulti-purpose%20utensil%20using%20origami-inspired%20artificial%20muscles.%20Traditional%0Autensils%2C%20such%20as%20forks%20and%20spoons%2Care%20hard%20and%20stiff%2C%20causing%20discomfort%20and%0Afear%20among%20users%2C%20especially%20when%20operated%20by%20autonomous%20robotic%20arms.%0AAdditionally%2C%20these%20systems%20require%20frequent%20utensil%20changes%20to%20handle%0Adifferent%20food%20types.%20Our%20innovative%20utensil%20design%20addresses%20these%20issues%20by%0Aoffering%20a%20versatile%2C%20adaptive%20solution%20that%20can%20seamlessly%20transition%20between%0Agripping%20and%20scooping%20various%20foods%20without%20the%20need%20for%20manual%20intervention.%0AUtilizing%20the%20flexibility%20and%20strength%20of%20origami-inspired%20artificial%20muscles%2C%0Athe%20utensil%20ensures%20safe%20and%20comfortable%20interactions%2C%20enhancing%20user%0Aexperience%20and%20efficiency.%20This%20approach%20not%20only%20simplifies%20the%20feeding%0Aprocess%20but%20also%20promotes%20greater%20independence%20for%20individuals%20with%20limited%0Amobility%2C%20contributing%20to%20the%20advancement%20of%20soft%20robotics%20in%20healthcare%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19558v1&entry.124074799=Read"},
{"title": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation", "author": "Yuting Liu and Jinghao Zhang and Yizhou Dang and Yuliang Liang and Qiang Liu and Guibing Guo and Jianzhe Zhao and Xingwei Wang", "abstract": "  Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, \\textbf{Co}llaborative \\textbf{Lo}RA (CoRA),\nwith a collaborative query generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings and inject them into a set number of\nlearnable queries. We then convert collaborative queries into collaborative\nweights with low-rank properties and merge the collaborative weights into LLM's\nweights, enabling LLM to perceive the collaborative signals and generate\npersonalized recommendations without fine-tuning or extra collaborative tokens\nin prompts. Extensive experiments confirm that CoRA effectively integrates\ncollaborative information into LLM, enhancing recommendation performance.\n", "link": "http://arxiv.org/abs/2408.10645v3", "date": "2024-10-25", "relevancy": 1.7908, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoRA%3A%20Collaborative%20Information%20Perception%20by%20Large%20Language%20Model%27s%0A%20%20Weights%20for%20Recommendation&body=Title%3A%20CoRA%3A%20Collaborative%20Information%20Perception%20by%20Large%20Language%20Model%27s%0A%20%20Weights%20for%20Recommendation%0AAuthor%3A%20Yuting%20Liu%20and%20Jinghao%20Zhang%20and%20Yizhou%20Dang%20and%20Yuliang%20Liang%20and%20Qiang%20Liu%20and%20Guibing%20Guo%20and%20Jianzhe%20Zhao%20and%20Xingwei%20Wang%0AAbstract%3A%20%20%20Involving%20collaborative%20information%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20a%0Apromising%20technique%20for%20adapting%20LLMs%20for%20recommendation.%20Existing%20methods%0Aachieve%20this%20by%20concatenating%20collaborative%20features%20with%20text%20tokens%20into%20a%0Aunified%20sequence%20input%20and%20then%20fine-tuning%20to%20align%20these%20features%20with%20LLM%27s%0Ainput%20space.%20Although%20effective%2C%20in%20this%20work%2C%20we%20identify%20two%20limitations%20when%0Aadapting%20LLMs%20to%20recommendation%20tasks%2C%20which%20hinder%20the%20integration%20of%20general%0Aknowledge%20and%20collaborative%20information%2C%20resulting%20in%20sub-optimal%0Arecommendation%20performance.%20%281%29%20Fine-tuning%20LLM%20with%20recommendation%20data%20can%0Aundermine%20its%20inherent%20world%20knowledge%20and%20fundamental%20competencies%2C%20which%20are%0Acrucial%20for%20interpreting%20and%20inferring%20recommendation%20text.%20%282%29%20Incorporating%0Acollaborative%20features%20into%20textual%20prompts%20disrupts%20the%20semantics%20of%20the%0Aoriginal%20prompts%2C%20preventing%20LLM%20from%20generating%20appropriate%20outputs.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20paradigm%2C%20%5Ctextbf%7BCo%7Dllaborative%20%5Ctextbf%7BLo%7DRA%20%28CoRA%29%2C%0Awith%20a%20collaborative%20query%20generator.%20Rather%20than%20input%20space%20alignment%2C%20this%0Amethod%20aligns%20collaborative%20information%20with%20LLM%27s%20parameter%20space%2C%0Arepresenting%20them%20as%20incremental%20weights%20to%20update%20LLM%27s%20output.%20This%20way%2C%20LLM%0Aperceives%20collaborative%20information%20without%20altering%20its%20general%20knowledge%20and%0Atext%20inference%20capabilities.%20Specifically%2C%20we%20employ%20a%20collaborative%20filtering%0Amodel%20to%20extract%20user%20and%20item%20embeddings%20and%20inject%20them%20into%20a%20set%20number%20of%0Alearnable%20queries.%20We%20then%20convert%20collaborative%20queries%20into%20collaborative%0Aweights%20with%20low-rank%20properties%20and%20merge%20the%20collaborative%20weights%20into%20LLM%27s%0Aweights%2C%20enabling%20LLM%20to%20perceive%20the%20collaborative%20signals%20and%20generate%0Apersonalized%20recommendations%20without%20fine-tuning%20or%20extra%20collaborative%20tokens%0Ain%20prompts.%20Extensive%20experiments%20confirm%20that%20CoRA%20effectively%20integrates%0Acollaborative%20information%20into%20LLM%2C%20enhancing%20recommendation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10645v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoRA%253A%2520Collaborative%2520Information%2520Perception%2520by%2520Large%2520Language%2520Model%2527s%250A%2520%2520Weights%2520for%2520Recommendation%26entry.906535625%3DYuting%2520Liu%2520and%2520Jinghao%2520Zhang%2520and%2520Yizhou%2520Dang%2520and%2520Yuliang%2520Liang%2520and%2520Qiang%2520Liu%2520and%2520Guibing%2520Guo%2520and%2520Jianzhe%2520Zhao%2520and%2520Xingwei%2520Wang%26entry.1292438233%3D%2520%2520Involving%2520collaborative%2520information%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520a%250Apromising%2520technique%2520for%2520adapting%2520LLMs%2520for%2520recommendation.%2520Existing%2520methods%250Aachieve%2520this%2520by%2520concatenating%2520collaborative%2520features%2520with%2520text%2520tokens%2520into%2520a%250Aunified%2520sequence%2520input%2520and%2520then%2520fine-tuning%2520to%2520align%2520these%2520features%2520with%2520LLM%2527s%250Ainput%2520space.%2520Although%2520effective%252C%2520in%2520this%2520work%252C%2520we%2520identify%2520two%2520limitations%2520when%250Aadapting%2520LLMs%2520to%2520recommendation%2520tasks%252C%2520which%2520hinder%2520the%2520integration%2520of%2520general%250Aknowledge%2520and%2520collaborative%2520information%252C%2520resulting%2520in%2520sub-optimal%250Arecommendation%2520performance.%2520%25281%2529%2520Fine-tuning%2520LLM%2520with%2520recommendation%2520data%2520can%250Aundermine%2520its%2520inherent%2520world%2520knowledge%2520and%2520fundamental%2520competencies%252C%2520which%2520are%250Acrucial%2520for%2520interpreting%2520and%2520inferring%2520recommendation%2520text.%2520%25282%2529%2520Incorporating%250Acollaborative%2520features%2520into%2520textual%2520prompts%2520disrupts%2520the%2520semantics%2520of%2520the%250Aoriginal%2520prompts%252C%2520preventing%2520LLM%2520from%2520generating%2520appropriate%2520outputs.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520new%2520paradigm%252C%2520%255Ctextbf%257BCo%257Dllaborative%2520%255Ctextbf%257BLo%257DRA%2520%2528CoRA%2529%252C%250Awith%2520a%2520collaborative%2520query%2520generator.%2520Rather%2520than%2520input%2520space%2520alignment%252C%2520this%250Amethod%2520aligns%2520collaborative%2520information%2520with%2520LLM%2527s%2520parameter%2520space%252C%250Arepresenting%2520them%2520as%2520incremental%2520weights%2520to%2520update%2520LLM%2527s%2520output.%2520This%2520way%252C%2520LLM%250Aperceives%2520collaborative%2520information%2520without%2520altering%2520its%2520general%2520knowledge%2520and%250Atext%2520inference%2520capabilities.%2520Specifically%252C%2520we%2520employ%2520a%2520collaborative%2520filtering%250Amodel%2520to%2520extract%2520user%2520and%2520item%2520embeddings%2520and%2520inject%2520them%2520into%2520a%2520set%2520number%2520of%250Alearnable%2520queries.%2520We%2520then%2520convert%2520collaborative%2520queries%2520into%2520collaborative%250Aweights%2520with%2520low-rank%2520properties%2520and%2520merge%2520the%2520collaborative%2520weights%2520into%2520LLM%2527s%250Aweights%252C%2520enabling%2520LLM%2520to%2520perceive%2520the%2520collaborative%2520signals%2520and%2520generate%250Apersonalized%2520recommendations%2520without%2520fine-tuning%2520or%2520extra%2520collaborative%2520tokens%250Ain%2520prompts.%2520Extensive%2520experiments%2520confirm%2520that%2520CoRA%2520effectively%2520integrates%250Acollaborative%2520information%2520into%2520LLM%252C%2520enhancing%2520recommendation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10645v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoRA%3A%20Collaborative%20Information%20Perception%20by%20Large%20Language%20Model%27s%0A%20%20Weights%20for%20Recommendation&entry.906535625=Yuting%20Liu%20and%20Jinghao%20Zhang%20and%20Yizhou%20Dang%20and%20Yuliang%20Liang%20and%20Qiang%20Liu%20and%20Guibing%20Guo%20and%20Jianzhe%20Zhao%20and%20Xingwei%20Wang&entry.1292438233=%20%20Involving%20collaborative%20information%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20a%0Apromising%20technique%20for%20adapting%20LLMs%20for%20recommendation.%20Existing%20methods%0Aachieve%20this%20by%20concatenating%20collaborative%20features%20with%20text%20tokens%20into%20a%0Aunified%20sequence%20input%20and%20then%20fine-tuning%20to%20align%20these%20features%20with%20LLM%27s%0Ainput%20space.%20Although%20effective%2C%20in%20this%20work%2C%20we%20identify%20two%20limitations%20when%0Aadapting%20LLMs%20to%20recommendation%20tasks%2C%20which%20hinder%20the%20integration%20of%20general%0Aknowledge%20and%20collaborative%20information%2C%20resulting%20in%20sub-optimal%0Arecommendation%20performance.%20%281%29%20Fine-tuning%20LLM%20with%20recommendation%20data%20can%0Aundermine%20its%20inherent%20world%20knowledge%20and%20fundamental%20competencies%2C%20which%20are%0Acrucial%20for%20interpreting%20and%20inferring%20recommendation%20text.%20%282%29%20Incorporating%0Acollaborative%20features%20into%20textual%20prompts%20disrupts%20the%20semantics%20of%20the%0Aoriginal%20prompts%2C%20preventing%20LLM%20from%20generating%20appropriate%20outputs.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20paradigm%2C%20%5Ctextbf%7BCo%7Dllaborative%20%5Ctextbf%7BLo%7DRA%20%28CoRA%29%2C%0Awith%20a%20collaborative%20query%20generator.%20Rather%20than%20input%20space%20alignment%2C%20this%0Amethod%20aligns%20collaborative%20information%20with%20LLM%27s%20parameter%20space%2C%0Arepresenting%20them%20as%20incremental%20weights%20to%20update%20LLM%27s%20output.%20This%20way%2C%20LLM%0Aperceives%20collaborative%20information%20without%20altering%20its%20general%20knowledge%20and%0Atext%20inference%20capabilities.%20Specifically%2C%20we%20employ%20a%20collaborative%20filtering%0Amodel%20to%20extract%20user%20and%20item%20embeddings%20and%20inject%20them%20into%20a%20set%20number%20of%0Alearnable%20queries.%20We%20then%20convert%20collaborative%20queries%20into%20collaborative%0Aweights%20with%20low-rank%20properties%20and%20merge%20the%20collaborative%20weights%20into%20LLM%27s%0Aweights%2C%20enabling%20LLM%20to%20perceive%20the%20collaborative%20signals%20and%20generate%0Apersonalized%20recommendations%20without%20fine-tuning%20or%20extra%20collaborative%20tokens%0Ain%20prompts.%20Extensive%20experiments%20confirm%20that%20CoRA%20effectively%20integrates%0Acollaborative%20information%20into%20LLM%2C%20enhancing%20recommendation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10645v3&entry.124074799=Read"},
{"title": "DepthART: Monocular Depth Estimation as Autoregressive Refinement Task", "author": "Bulat Gabdullin and Nina Konovalova and Nikolay Patakin and Dmitry Senushkin and Anton Konushin", "abstract": "  Despite recent success in discriminative approaches in monocular depth\nestimation its quality remains limited by training datasets. Generative\napproaches mitigate this issue by leveraging strong priors derived from\ntraining on internet-scale datasets. Recent studies have demonstrated that\nlarge text-to-image diffusion models achieve state-of-the-art results in depth\nestimation when fine-tuned on small depth datasets. Concurrently,\nautoregressive generative approaches, such as the Visual AutoRegressive\nmodeling~(VAR), have shown promising results in conditioned image synthesis.\nFollowing the visual autoregressive modeling paradigm, we introduce the first\nautoregressive depth estimation model based on the visual autoregressive\ntransformer. Our primary contribution is DepthART -- a novel training method\nformulated as Depth Autoregressive Refinement Task. Unlike the original VAR\ntraining procedure, which employs static targets, our method utilizes a dynamic\ntarget formulation that enables model self-refinement and incorporates\nmulti-modal guidance during training. Specifically, we use model predictions as\ninputs instead of ground truth token maps during training, framing the\nobjective as residual minimization. Our experiments demonstrate that the\nproposed training approach significantly outperforms visual autoregressive\nmodeling via next-scale prediction in the depth estimation task. The Visual\nAutoregressive Transformer trained with our approach on Hypersim achieves\nsuperior results on a set of unseen benchmarks compared to other generative and\ndiscriminative baselines.\n", "link": "http://arxiv.org/abs/2409.15010v2", "date": "2024-10-25", "relevancy": 1.7566, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.617}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5802}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepthART%3A%20Monocular%20Depth%20Estimation%20as%20Autoregressive%20Refinement%20Task&body=Title%3A%20DepthART%3A%20Monocular%20Depth%20Estimation%20as%20Autoregressive%20Refinement%20Task%0AAuthor%3A%20Bulat%20Gabdullin%20and%20Nina%20Konovalova%20and%20Nikolay%20Patakin%20and%20Dmitry%20Senushkin%20and%20Anton%20Konushin%0AAbstract%3A%20%20%20Despite%20recent%20success%20in%20discriminative%20approaches%20in%20monocular%20depth%0Aestimation%20its%20quality%20remains%20limited%20by%20training%20datasets.%20Generative%0Aapproaches%20mitigate%20this%20issue%20by%20leveraging%20strong%20priors%20derived%20from%0Atraining%20on%20internet-scale%20datasets.%20Recent%20studies%20have%20demonstrated%20that%0Alarge%20text-to-image%20diffusion%20models%20achieve%20state-of-the-art%20results%20in%20depth%0Aestimation%20when%20fine-tuned%20on%20small%20depth%20datasets.%20Concurrently%2C%0Aautoregressive%20generative%20approaches%2C%20such%20as%20the%20Visual%20AutoRegressive%0Amodeling~%28VAR%29%2C%20have%20shown%20promising%20results%20in%20conditioned%20image%20synthesis.%0AFollowing%20the%20visual%20autoregressive%20modeling%20paradigm%2C%20we%20introduce%20the%20first%0Aautoregressive%20depth%20estimation%20model%20based%20on%20the%20visual%20autoregressive%0Atransformer.%20Our%20primary%20contribution%20is%20DepthART%20--%20a%20novel%20training%20method%0Aformulated%20as%20Depth%20Autoregressive%20Refinement%20Task.%20Unlike%20the%20original%20VAR%0Atraining%20procedure%2C%20which%20employs%20static%20targets%2C%20our%20method%20utilizes%20a%20dynamic%0Atarget%20formulation%20that%20enables%20model%20self-refinement%20and%20incorporates%0Amulti-modal%20guidance%20during%20training.%20Specifically%2C%20we%20use%20model%20predictions%20as%0Ainputs%20instead%20of%20ground%20truth%20token%20maps%20during%20training%2C%20framing%20the%0Aobjective%20as%20residual%20minimization.%20Our%20experiments%20demonstrate%20that%20the%0Aproposed%20training%20approach%20significantly%20outperforms%20visual%20autoregressive%0Amodeling%20via%20next-scale%20prediction%20in%20the%20depth%20estimation%20task.%20The%20Visual%0AAutoregressive%20Transformer%20trained%20with%20our%20approach%20on%20Hypersim%20achieves%0Asuperior%20results%20on%20a%20set%20of%20unseen%20benchmarks%20compared%20to%20other%20generative%20and%0Adiscriminative%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepthART%253A%2520Monocular%2520Depth%2520Estimation%2520as%2520Autoregressive%2520Refinement%2520Task%26entry.906535625%3DBulat%2520Gabdullin%2520and%2520Nina%2520Konovalova%2520and%2520Nikolay%2520Patakin%2520and%2520Dmitry%2520Senushkin%2520and%2520Anton%2520Konushin%26entry.1292438233%3D%2520%2520Despite%2520recent%2520success%2520in%2520discriminative%2520approaches%2520in%2520monocular%2520depth%250Aestimation%2520its%2520quality%2520remains%2520limited%2520by%2520training%2520datasets.%2520Generative%250Aapproaches%2520mitigate%2520this%2520issue%2520by%2520leveraging%2520strong%2520priors%2520derived%2520from%250Atraining%2520on%2520internet-scale%2520datasets.%2520Recent%2520studies%2520have%2520demonstrated%2520that%250Alarge%2520text-to-image%2520diffusion%2520models%2520achieve%2520state-of-the-art%2520results%2520in%2520depth%250Aestimation%2520when%2520fine-tuned%2520on%2520small%2520depth%2520datasets.%2520Concurrently%252C%250Aautoregressive%2520generative%2520approaches%252C%2520such%2520as%2520the%2520Visual%2520AutoRegressive%250Amodeling~%2528VAR%2529%252C%2520have%2520shown%2520promising%2520results%2520in%2520conditioned%2520image%2520synthesis.%250AFollowing%2520the%2520visual%2520autoregressive%2520modeling%2520paradigm%252C%2520we%2520introduce%2520the%2520first%250Aautoregressive%2520depth%2520estimation%2520model%2520based%2520on%2520the%2520visual%2520autoregressive%250Atransformer.%2520Our%2520primary%2520contribution%2520is%2520DepthART%2520--%2520a%2520novel%2520training%2520method%250Aformulated%2520as%2520Depth%2520Autoregressive%2520Refinement%2520Task.%2520Unlike%2520the%2520original%2520VAR%250Atraining%2520procedure%252C%2520which%2520employs%2520static%2520targets%252C%2520our%2520method%2520utilizes%2520a%2520dynamic%250Atarget%2520formulation%2520that%2520enables%2520model%2520self-refinement%2520and%2520incorporates%250Amulti-modal%2520guidance%2520during%2520training.%2520Specifically%252C%2520we%2520use%2520model%2520predictions%2520as%250Ainputs%2520instead%2520of%2520ground%2520truth%2520token%2520maps%2520during%2520training%252C%2520framing%2520the%250Aobjective%2520as%2520residual%2520minimization.%2520Our%2520experiments%2520demonstrate%2520that%2520the%250Aproposed%2520training%2520approach%2520significantly%2520outperforms%2520visual%2520autoregressive%250Amodeling%2520via%2520next-scale%2520prediction%2520in%2520the%2520depth%2520estimation%2520task.%2520The%2520Visual%250AAutoregressive%2520Transformer%2520trained%2520with%2520our%2520approach%2520on%2520Hypersim%2520achieves%250Asuperior%2520results%2520on%2520a%2520set%2520of%2520unseen%2520benchmarks%2520compared%2520to%2520other%2520generative%2520and%250Adiscriminative%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthART%3A%20Monocular%20Depth%20Estimation%20as%20Autoregressive%20Refinement%20Task&entry.906535625=Bulat%20Gabdullin%20and%20Nina%20Konovalova%20and%20Nikolay%20Patakin%20and%20Dmitry%20Senushkin%20and%20Anton%20Konushin&entry.1292438233=%20%20Despite%20recent%20success%20in%20discriminative%20approaches%20in%20monocular%20depth%0Aestimation%20its%20quality%20remains%20limited%20by%20training%20datasets.%20Generative%0Aapproaches%20mitigate%20this%20issue%20by%20leveraging%20strong%20priors%20derived%20from%0Atraining%20on%20internet-scale%20datasets.%20Recent%20studies%20have%20demonstrated%20that%0Alarge%20text-to-image%20diffusion%20models%20achieve%20state-of-the-art%20results%20in%20depth%0Aestimation%20when%20fine-tuned%20on%20small%20depth%20datasets.%20Concurrently%2C%0Aautoregressive%20generative%20approaches%2C%20such%20as%20the%20Visual%20AutoRegressive%0Amodeling~%28VAR%29%2C%20have%20shown%20promising%20results%20in%20conditioned%20image%20synthesis.%0AFollowing%20the%20visual%20autoregressive%20modeling%20paradigm%2C%20we%20introduce%20the%20first%0Aautoregressive%20depth%20estimation%20model%20based%20on%20the%20visual%20autoregressive%0Atransformer.%20Our%20primary%20contribution%20is%20DepthART%20--%20a%20novel%20training%20method%0Aformulated%20as%20Depth%20Autoregressive%20Refinement%20Task.%20Unlike%20the%20original%20VAR%0Atraining%20procedure%2C%20which%20employs%20static%20targets%2C%20our%20method%20utilizes%20a%20dynamic%0Atarget%20formulation%20that%20enables%20model%20self-refinement%20and%20incorporates%0Amulti-modal%20guidance%20during%20training.%20Specifically%2C%20we%20use%20model%20predictions%20as%0Ainputs%20instead%20of%20ground%20truth%20token%20maps%20during%20training%2C%20framing%20the%0Aobjective%20as%20residual%20minimization.%20Our%20experiments%20demonstrate%20that%20the%0Aproposed%20training%20approach%20significantly%20outperforms%20visual%20autoregressive%0Amodeling%20via%20next-scale%20prediction%20in%20the%20depth%20estimation%20task.%20The%20Visual%0AAutoregressive%20Transformer%20trained%20with%20our%20approach%20on%20Hypersim%20achieves%0Asuperior%20results%20on%20a%20set%20of%20unseen%20benchmarks%20compared%20to%20other%20generative%20and%0Adiscriminative%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15010v2&entry.124074799=Read"},
{"title": "Neuromorphic IoT Architecture for Efficient Water Management: A Smart\n  Village Case Study", "author": "Mugdim Bublin and Heimo Hirner and Antoine-Martin Lanners and Radu Grosu", "abstract": "  The exponential growth of IoT networks necessitates a paradigm shift towards\narchitectures that offer high flexibility and learning capabilities while\nmaintaining low energy consumption, minimal communication overhead, and low\nlatency. Traditional IoT systems, particularly when integrated with machine\nlearning approaches, often suffer from high communication overhead and\nsignificant energy consumption. This work addresses these challenges by\nproposing a neuromorphic architecture inspired by biological systems. To\nillustrate the practical application of our proposed architecture, we present a\ncase study focusing on water management in the Carinthian community of Neuhaus.\nPreliminary results regarding water consumption prediction and anomaly\ndetection in this community are presented. We also introduce a novel\nneuromorphic IoT architecture that integrates biological principles into the\ndesign of IoT systems. This architecture is specifically tailored for edge\ncomputing scenarios, where low power and high efficiency are crucial. Our\napproach leverages the inherent advantages of neuromorphic computing, such as\nasynchronous processing and event-driven communication, to create an IoT\nframework that is both energy-efficient and responsive. This case study\ndemonstrates how the neuromorphic IoT architecture can be deployed in a\nreal-world scenario, highlighting its benefits in terms of energy savings,\nreduced communication overhead, and improved system responsiveness.\n", "link": "http://arxiv.org/abs/2410.19562v1", "date": "2024-10-25", "relevancy": 1.2514, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4291}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4206}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuromorphic%20IoT%20Architecture%20for%20Efficient%20Water%20Management%3A%20A%20Smart%0A%20%20Village%20Case%20Study&body=Title%3A%20Neuromorphic%20IoT%20Architecture%20for%20Efficient%20Water%20Management%3A%20A%20Smart%0A%20%20Village%20Case%20Study%0AAuthor%3A%20Mugdim%20Bublin%20and%20Heimo%20Hirner%20and%20Antoine-Martin%20Lanners%20and%20Radu%20Grosu%0AAbstract%3A%20%20%20The%20exponential%20growth%20of%20IoT%20networks%20necessitates%20a%20paradigm%20shift%20towards%0Aarchitectures%20that%20offer%20high%20flexibility%20and%20learning%20capabilities%20while%0Amaintaining%20low%20energy%20consumption%2C%20minimal%20communication%20overhead%2C%20and%20low%0Alatency.%20Traditional%20IoT%20systems%2C%20particularly%20when%20integrated%20with%20machine%0Alearning%20approaches%2C%20often%20suffer%20from%20high%20communication%20overhead%20and%0Asignificant%20energy%20consumption.%20This%20work%20addresses%20these%20challenges%20by%0Aproposing%20a%20neuromorphic%20architecture%20inspired%20by%20biological%20systems.%20To%0Aillustrate%20the%20practical%20application%20of%20our%20proposed%20architecture%2C%20we%20present%20a%0Acase%20study%20focusing%20on%20water%20management%20in%20the%20Carinthian%20community%20of%20Neuhaus.%0APreliminary%20results%20regarding%20water%20consumption%20prediction%20and%20anomaly%0Adetection%20in%20this%20community%20are%20presented.%20We%20also%20introduce%20a%20novel%0Aneuromorphic%20IoT%20architecture%20that%20integrates%20biological%20principles%20into%20the%0Adesign%20of%20IoT%20systems.%20This%20architecture%20is%20specifically%20tailored%20for%20edge%0Acomputing%20scenarios%2C%20where%20low%20power%20and%20high%20efficiency%20are%20crucial.%20Our%0Aapproach%20leverages%20the%20inherent%20advantages%20of%20neuromorphic%20computing%2C%20such%20as%0Aasynchronous%20processing%20and%20event-driven%20communication%2C%20to%20create%20an%20IoT%0Aframework%20that%20is%20both%20energy-efficient%20and%20responsive.%20This%20case%20study%0Ademonstrates%20how%20the%20neuromorphic%20IoT%20architecture%20can%20be%20deployed%20in%20a%0Areal-world%20scenario%2C%20highlighting%20its%20benefits%20in%20terms%20of%20energy%20savings%2C%0Areduced%20communication%20overhead%2C%20and%20improved%20system%20responsiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuromorphic%2520IoT%2520Architecture%2520for%2520Efficient%2520Water%2520Management%253A%2520A%2520Smart%250A%2520%2520Village%2520Case%2520Study%26entry.906535625%3DMugdim%2520Bublin%2520and%2520Heimo%2520Hirner%2520and%2520Antoine-Martin%2520Lanners%2520and%2520Radu%2520Grosu%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520of%2520IoT%2520networks%2520necessitates%2520a%2520paradigm%2520shift%2520towards%250Aarchitectures%2520that%2520offer%2520high%2520flexibility%2520and%2520learning%2520capabilities%2520while%250Amaintaining%2520low%2520energy%2520consumption%252C%2520minimal%2520communication%2520overhead%252C%2520and%2520low%250Alatency.%2520Traditional%2520IoT%2520systems%252C%2520particularly%2520when%2520integrated%2520with%2520machine%250Alearning%2520approaches%252C%2520often%2520suffer%2520from%2520high%2520communication%2520overhead%2520and%250Asignificant%2520energy%2520consumption.%2520This%2520work%2520addresses%2520these%2520challenges%2520by%250Aproposing%2520a%2520neuromorphic%2520architecture%2520inspired%2520by%2520biological%2520systems.%2520To%250Aillustrate%2520the%2520practical%2520application%2520of%2520our%2520proposed%2520architecture%252C%2520we%2520present%2520a%250Acase%2520study%2520focusing%2520on%2520water%2520management%2520in%2520the%2520Carinthian%2520community%2520of%2520Neuhaus.%250APreliminary%2520results%2520regarding%2520water%2520consumption%2520prediction%2520and%2520anomaly%250Adetection%2520in%2520this%2520community%2520are%2520presented.%2520We%2520also%2520introduce%2520a%2520novel%250Aneuromorphic%2520IoT%2520architecture%2520that%2520integrates%2520biological%2520principles%2520into%2520the%250Adesign%2520of%2520IoT%2520systems.%2520This%2520architecture%2520is%2520specifically%2520tailored%2520for%2520edge%250Acomputing%2520scenarios%252C%2520where%2520low%2520power%2520and%2520high%2520efficiency%2520are%2520crucial.%2520Our%250Aapproach%2520leverages%2520the%2520inherent%2520advantages%2520of%2520neuromorphic%2520computing%252C%2520such%2520as%250Aasynchronous%2520processing%2520and%2520event-driven%2520communication%252C%2520to%2520create%2520an%2520IoT%250Aframework%2520that%2520is%2520both%2520energy-efficient%2520and%2520responsive.%2520This%2520case%2520study%250Ademonstrates%2520how%2520the%2520neuromorphic%2520IoT%2520architecture%2520can%2520be%2520deployed%2520in%2520a%250Areal-world%2520scenario%252C%2520highlighting%2520its%2520benefits%2520in%2520terms%2520of%2520energy%2520savings%252C%250Areduced%2520communication%2520overhead%252C%2520and%2520improved%2520system%2520responsiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuromorphic%20IoT%20Architecture%20for%20Efficient%20Water%20Management%3A%20A%20Smart%0A%20%20Village%20Case%20Study&entry.906535625=Mugdim%20Bublin%20and%20Heimo%20Hirner%20and%20Antoine-Martin%20Lanners%20and%20Radu%20Grosu&entry.1292438233=%20%20The%20exponential%20growth%20of%20IoT%20networks%20necessitates%20a%20paradigm%20shift%20towards%0Aarchitectures%20that%20offer%20high%20flexibility%20and%20learning%20capabilities%20while%0Amaintaining%20low%20energy%20consumption%2C%20minimal%20communication%20overhead%2C%20and%20low%0Alatency.%20Traditional%20IoT%20systems%2C%20particularly%20when%20integrated%20with%20machine%0Alearning%20approaches%2C%20often%20suffer%20from%20high%20communication%20overhead%20and%0Asignificant%20energy%20consumption.%20This%20work%20addresses%20these%20challenges%20by%0Aproposing%20a%20neuromorphic%20architecture%20inspired%20by%20biological%20systems.%20To%0Aillustrate%20the%20practical%20application%20of%20our%20proposed%20architecture%2C%20we%20present%20a%0Acase%20study%20focusing%20on%20water%20management%20in%20the%20Carinthian%20community%20of%20Neuhaus.%0APreliminary%20results%20regarding%20water%20consumption%20prediction%20and%20anomaly%0Adetection%20in%20this%20community%20are%20presented.%20We%20also%20introduce%20a%20novel%0Aneuromorphic%20IoT%20architecture%20that%20integrates%20biological%20principles%20into%20the%0Adesign%20of%20IoT%20systems.%20This%20architecture%20is%20specifically%20tailored%20for%20edge%0Acomputing%20scenarios%2C%20where%20low%20power%20and%20high%20efficiency%20are%20crucial.%20Our%0Aapproach%20leverages%20the%20inherent%20advantages%20of%20neuromorphic%20computing%2C%20such%20as%0Aasynchronous%20processing%20and%20event-driven%20communication%2C%20to%20create%20an%20IoT%0Aframework%20that%20is%20both%20energy-efficient%20and%20responsive.%20This%20case%20study%0Ademonstrates%20how%20the%20neuromorphic%20IoT%20architecture%20can%20be%20deployed%20in%20a%0Areal-world%20scenario%2C%20highlighting%20its%20benefits%20in%20terms%20of%20energy%20savings%2C%0Areduced%20communication%20overhead%2C%20and%20improved%20system%20responsiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19562v1&entry.124074799=Read"},
{"title": "CloserMusicDB: A Modern Multipurpose Dataset of High Quality Music", "author": "Aleksandra Piekarzewicz and Tomasz Sroka and Aleksander Tym and Mateusz Modrzejewski", "abstract": "  In this paper, we introduce CloserMusicDB, a collection of full length studio\nquality tracks annotated by a team of human experts. We describe the selected\nqualities of our dataset, along with three example tasks possible to perform\nusing this dataset: hook detection, contextual tagging and artist\nidentification. We conduct baseline experiments and provide initial benchmarks\nfor these tasks.\n", "link": "http://arxiv.org/abs/2410.19540v1", "date": "2024-10-25", "relevancy": 1.5125, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CloserMusicDB%3A%20A%20Modern%20Multipurpose%20Dataset%20of%20High%20Quality%20Music&body=Title%3A%20CloserMusicDB%3A%20A%20Modern%20Multipurpose%20Dataset%20of%20High%20Quality%20Music%0AAuthor%3A%20Aleksandra%20Piekarzewicz%20and%20Tomasz%20Sroka%20and%20Aleksander%20Tym%20and%20Mateusz%20Modrzejewski%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20CloserMusicDB%2C%20a%20collection%20of%20full%20length%20studio%0Aquality%20tracks%20annotated%20by%20a%20team%20of%20human%20experts.%20We%20describe%20the%20selected%0Aqualities%20of%20our%20dataset%2C%20along%20with%20three%20example%20tasks%20possible%20to%20perform%0Ausing%20this%20dataset%3A%20hook%20detection%2C%20contextual%20tagging%20and%20artist%0Aidentification.%20We%20conduct%20baseline%20experiments%20and%20provide%20initial%20benchmarks%0Afor%20these%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCloserMusicDB%253A%2520A%2520Modern%2520Multipurpose%2520Dataset%2520of%2520High%2520Quality%2520Music%26entry.906535625%3DAleksandra%2520Piekarzewicz%2520and%2520Tomasz%2520Sroka%2520and%2520Aleksander%2520Tym%2520and%2520Mateusz%2520Modrzejewski%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CloserMusicDB%252C%2520a%2520collection%2520of%2520full%2520length%2520studio%250Aquality%2520tracks%2520annotated%2520by%2520a%2520team%2520of%2520human%2520experts.%2520We%2520describe%2520the%2520selected%250Aqualities%2520of%2520our%2520dataset%252C%2520along%2520with%2520three%2520example%2520tasks%2520possible%2520to%2520perform%250Ausing%2520this%2520dataset%253A%2520hook%2520detection%252C%2520contextual%2520tagging%2520and%2520artist%250Aidentification.%2520We%2520conduct%2520baseline%2520experiments%2520and%2520provide%2520initial%2520benchmarks%250Afor%2520these%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CloserMusicDB%3A%20A%20Modern%20Multipurpose%20Dataset%20of%20High%20Quality%20Music&entry.906535625=Aleksandra%20Piekarzewicz%20and%20Tomasz%20Sroka%20and%20Aleksander%20Tym%20and%20Mateusz%20Modrzejewski&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20CloserMusicDB%2C%20a%20collection%20of%20full%20length%20studio%0Aquality%20tracks%20annotated%20by%20a%20team%20of%20human%20experts.%20We%20describe%20the%20selected%0Aqualities%20of%20our%20dataset%2C%20along%20with%20three%20example%20tasks%20possible%20to%20perform%0Ausing%20this%20dataset%3A%20hook%20detection%2C%20contextual%20tagging%20and%20artist%0Aidentification.%20We%20conduct%20baseline%20experiments%20and%20provide%20initial%20benchmarks%0Afor%20these%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19540v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


