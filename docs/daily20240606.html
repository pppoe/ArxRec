<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240605.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion", "author": "Hao Wen and Zehuan Huang and Yaohui Wang and Xinyuan Chen and Yu Qiao and Lu Sheng", "abstract": "  Existing single image-to-3D creation methods typically involve a two-stage\nprocess, first generating multi-view images, and then using these images for 3D\nreconstruction. However, training these two stages separately leads to\nsignificant data bias in the inference phase, thus affecting the quality of\nreconstructed results. We introduce a unified 3D generation framework, named\nOuroboros3D, which integrates diffusion-based multi-view image generation and\n3D reconstruction into a recursive diffusion process. In our framework, these\ntwo modules are jointly trained through a self-conditioning mechanism, allowing\nthem to adapt to each other's characteristics for robust inference. During the\nmulti-view denoising process, the multi-view diffusion model uses the 3D-aware\nmaps rendered by the reconstruction module at the previous timestep as\nadditional conditions. The recursive diffusion framework with 3D-aware feedback\nunites the entire process and improves geometric consistency.Experiments show\nthat our framework outperforms separation of these two stages and existing\nmethods that combine them at the inference phase. Project page:\nhttps://costwen.github.io/Ouroboros3D/\n", "link": "http://arxiv.org/abs/2406.03184v1", "date": "2024-06-05", "relevancy": 3.3734, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7124}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7124}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ouroboros3D%3A%20Image-to-3D%20Generation%20via%203D-aware%20Recursive%20Diffusion&body=Title%3A%20Ouroboros3D%3A%20Image-to-3D%20Generation%20via%203D-aware%20Recursive%20Diffusion%0AAuthor%3A%20Hao%20Wen%20and%20Zehuan%20Huang%20and%20Yaohui%20Wang%20and%20Xinyuan%20Chen%20and%20Yu%20Qiao%20and%20Lu%20Sheng%0AAbstract%3A%20%20%20Existing%20single%20image-to-3D%20creation%20methods%20typically%20involve%20a%20two-stage%0Aprocess%2C%20first%20generating%20multi-view%20images%2C%20and%20then%20using%20these%20images%20for%203D%0Areconstruction.%20However%2C%20training%20these%20two%20stages%20separately%20leads%20to%0Asignificant%20data%20bias%20in%20the%20inference%20phase%2C%20thus%20affecting%20the%20quality%20of%0Areconstructed%20results.%20We%20introduce%20a%20unified%203D%20generation%20framework%2C%20named%0AOuroboros3D%2C%20which%20integrates%20diffusion-based%20multi-view%20image%20generation%20and%0A3D%20reconstruction%20into%20a%20recursive%20diffusion%20process.%20In%20our%20framework%2C%20these%0Atwo%20modules%20are%20jointly%20trained%20through%20a%20self-conditioning%20mechanism%2C%20allowing%0Athem%20to%20adapt%20to%20each%20other%27s%20characteristics%20for%20robust%20inference.%20During%20the%0Amulti-view%20denoising%20process%2C%20the%20multi-view%20diffusion%20model%20uses%20the%203D-aware%0Amaps%20rendered%20by%20the%20reconstruction%20module%20at%20the%20previous%20timestep%20as%0Aadditional%20conditions.%20The%20recursive%20diffusion%20framework%20with%203D-aware%20feedback%0Aunites%20the%20entire%20process%20and%20improves%20geometric%20consistency.Experiments%20show%0Athat%20our%20framework%20outperforms%20separation%20of%20these%20two%20stages%20and%20existing%0Amethods%20that%20combine%20them%20at%20the%20inference%20phase.%20Project%20page%3A%0Ahttps%3A//costwen.github.io/Ouroboros3D/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOuroboros3D%253A%2520Image-to-3D%2520Generation%2520via%25203D-aware%2520Recursive%2520Diffusion%26entry.906535625%3DHao%2520Wen%2520and%2520Zehuan%2520Huang%2520and%2520Yaohui%2520Wang%2520and%2520Xinyuan%2520Chen%2520and%2520Yu%2520Qiao%2520and%2520Lu%2520Sheng%26entry.1292438233%3D%2520%2520Existing%2520single%2520image-to-3D%2520creation%2520methods%2520typically%2520involve%2520a%2520two-stage%250Aprocess%252C%2520first%2520generating%2520multi-view%2520images%252C%2520and%2520then%2520using%2520these%2520images%2520for%25203D%250Areconstruction.%2520However%252C%2520training%2520these%2520two%2520stages%2520separately%2520leads%2520to%250Asignificant%2520data%2520bias%2520in%2520the%2520inference%2520phase%252C%2520thus%2520affecting%2520the%2520quality%2520of%250Areconstructed%2520results.%2520We%2520introduce%2520a%2520unified%25203D%2520generation%2520framework%252C%2520named%250AOuroboros3D%252C%2520which%2520integrates%2520diffusion-based%2520multi-view%2520image%2520generation%2520and%250A3D%2520reconstruction%2520into%2520a%2520recursive%2520diffusion%2520process.%2520In%2520our%2520framework%252C%2520these%250Atwo%2520modules%2520are%2520jointly%2520trained%2520through%2520a%2520self-conditioning%2520mechanism%252C%2520allowing%250Athem%2520to%2520adapt%2520to%2520each%2520other%2527s%2520characteristics%2520for%2520robust%2520inference.%2520During%2520the%250Amulti-view%2520denoising%2520process%252C%2520the%2520multi-view%2520diffusion%2520model%2520uses%2520the%25203D-aware%250Amaps%2520rendered%2520by%2520the%2520reconstruction%2520module%2520at%2520the%2520previous%2520timestep%2520as%250Aadditional%2520conditions.%2520The%2520recursive%2520diffusion%2520framework%2520with%25203D-aware%2520feedback%250Aunites%2520the%2520entire%2520process%2520and%2520improves%2520geometric%2520consistency.Experiments%2520show%250Athat%2520our%2520framework%2520outperforms%2520separation%2520of%2520these%2520two%2520stages%2520and%2520existing%250Amethods%2520that%2520combine%2520them%2520at%2520the%2520inference%2520phase.%2520Project%2520page%253A%250Ahttps%253A//costwen.github.io/Ouroboros3D/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ouroboros3D%3A%20Image-to-3D%20Generation%20via%203D-aware%20Recursive%20Diffusion&entry.906535625=Hao%20Wen%20and%20Zehuan%20Huang%20and%20Yaohui%20Wang%20and%20Xinyuan%20Chen%20and%20Yu%20Qiao%20and%20Lu%20Sheng&entry.1292438233=%20%20Existing%20single%20image-to-3D%20creation%20methods%20typically%20involve%20a%20two-stage%0Aprocess%2C%20first%20generating%20multi-view%20images%2C%20and%20then%20using%20these%20images%20for%203D%0Areconstruction.%20However%2C%20training%20these%20two%20stages%20separately%20leads%20to%0Asignificant%20data%20bias%20in%20the%20inference%20phase%2C%20thus%20affecting%20the%20quality%20of%0Areconstructed%20results.%20We%20introduce%20a%20unified%203D%20generation%20framework%2C%20named%0AOuroboros3D%2C%20which%20integrates%20diffusion-based%20multi-view%20image%20generation%20and%0A3D%20reconstruction%20into%20a%20recursive%20diffusion%20process.%20In%20our%20framework%2C%20these%0Atwo%20modules%20are%20jointly%20trained%20through%20a%20self-conditioning%20mechanism%2C%20allowing%0Athem%20to%20adapt%20to%20each%20other%27s%20characteristics%20for%20robust%20inference.%20During%20the%0Amulti-view%20denoising%20process%2C%20the%20multi-view%20diffusion%20model%20uses%20the%203D-aware%0Amaps%20rendered%20by%20the%20reconstruction%20module%20at%20the%20previous%20timestep%20as%0Aadditional%20conditions.%20The%20recursive%20diffusion%20framework%20with%203D-aware%20feedback%0Aunites%20the%20entire%20process%20and%20improves%20geometric%20consistency.Experiments%20show%0Athat%20our%20framework%20outperforms%20separation%20of%20these%20two%20stages%20and%20existing%0Amethods%20that%20combine%20them%20at%20the%20inference%20phase.%20Project%20page%3A%0Ahttps%3A//costwen.github.io/Ouroboros3D/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03184v1&entry.124074799=Read"},
{"title": "SelfReDepth: Self-Supervised Real-Time Depth Restoration for\n  Consumer-Grade Sensors", "author": "Alexandre Duarte and Francisco Fernandes and Jo\u00e3o M. Pereira and Catarina Moreira and Jacinto C. Nascimento and Joaquim Jorge", "abstract": "  Depth maps produced by consumer-grade sensors suffer from inaccurate\nmeasurements and missing data from either system or scene-specific sources.\nData-driven denoising algorithms can mitigate such problems. However, they\nrequire vast amounts of ground truth depth data. Recent research has tackled\nthis limitation using self-supervised learning techniques, but it requires\nmultiple RGB-D sensors. Moreover, most existing approaches focus on denoising\nsingle isolated depth maps or specific subjects of interest, highlighting a\nneed for methods to effectively denoise depth maps in real-time dynamic\nenvironments. This paper extends state-of-the-art approaches for\ndepth-denoising commodity depth devices, proposing SelfReDepth, a\nself-supervised deep learning technique for depth restoration, via denoising\nand hole-filling by inpainting full-depth maps captured with RGB-D sensors. The\nalgorithm targets depth data in video streams, utilizing multiple sequential\ndepth frames coupled with color data to achieve high-quality depth videos with\ntemporal coherence. Finally, SelfReDepth is designed to be compatible with\nvarious RGB-D sensors and usable in real-time scenarios as a pre-processing\nstep before applying other depth-dependent algorithms. Our results demonstrate\nour approach's real-time performance on real-world datasets. They show that it\noutperforms state-of-the-art denoising and restoration performance at over\n30fps on Commercial Depth Cameras, with potential benefits for augmented and\nmixed-reality applications.\n", "link": "http://arxiv.org/abs/2406.03388v1", "date": "2024-06-05", "relevancy": 2.8811, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6185}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5731}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfReDepth%3A%20Self-Supervised%20Real-Time%20Depth%20Restoration%20for%0A%20%20Consumer-Grade%20Sensors&body=Title%3A%20SelfReDepth%3A%20Self-Supervised%20Real-Time%20Depth%20Restoration%20for%0A%20%20Consumer-Grade%20Sensors%0AAuthor%3A%20Alexandre%20Duarte%20and%20Francisco%20Fernandes%20and%20Jo%C3%A3o%20M.%20Pereira%20and%20Catarina%20Moreira%20and%20Jacinto%20C.%20Nascimento%20and%20Joaquim%20Jorge%0AAbstract%3A%20%20%20Depth%20maps%20produced%20by%20consumer-grade%20sensors%20suffer%20from%20inaccurate%0Ameasurements%20and%20missing%20data%20from%20either%20system%20or%20scene-specific%20sources.%0AData-driven%20denoising%20algorithms%20can%20mitigate%20such%20problems.%20However%2C%20they%0Arequire%20vast%20amounts%20of%20ground%20truth%20depth%20data.%20Recent%20research%20has%20tackled%0Athis%20limitation%20using%20self-supervised%20learning%20techniques%2C%20but%20it%20requires%0Amultiple%20RGB-D%20sensors.%20Moreover%2C%20most%20existing%20approaches%20focus%20on%20denoising%0Asingle%20isolated%20depth%20maps%20or%20specific%20subjects%20of%20interest%2C%20highlighting%20a%0Aneed%20for%20methods%20to%20effectively%20denoise%20depth%20maps%20in%20real-time%20dynamic%0Aenvironments.%20This%20paper%20extends%20state-of-the-art%20approaches%20for%0Adepth-denoising%20commodity%20depth%20devices%2C%20proposing%20SelfReDepth%2C%20a%0Aself-supervised%20deep%20learning%20technique%20for%20depth%20restoration%2C%20via%20denoising%0Aand%20hole-filling%20by%20inpainting%20full-depth%20maps%20captured%20with%20RGB-D%20sensors.%20The%0Aalgorithm%20targets%20depth%20data%20in%20video%20streams%2C%20utilizing%20multiple%20sequential%0Adepth%20frames%20coupled%20with%20color%20data%20to%20achieve%20high-quality%20depth%20videos%20with%0Atemporal%20coherence.%20Finally%2C%20SelfReDepth%20is%20designed%20to%20be%20compatible%20with%0Avarious%20RGB-D%20sensors%20and%20usable%20in%20real-time%20scenarios%20as%20a%20pre-processing%0Astep%20before%20applying%20other%20depth-dependent%20algorithms.%20Our%20results%20demonstrate%0Aour%20approach%27s%20real-time%20performance%20on%20real-world%20datasets.%20They%20show%20that%20it%0Aoutperforms%20state-of-the-art%20denoising%20and%20restoration%20performance%20at%20over%0A30fps%20on%20Commercial%20Depth%20Cameras%2C%20with%20potential%20benefits%20for%20augmented%20and%0Amixed-reality%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfReDepth%253A%2520Self-Supervised%2520Real-Time%2520Depth%2520Restoration%2520for%250A%2520%2520Consumer-Grade%2520Sensors%26entry.906535625%3DAlexandre%2520Duarte%2520and%2520Francisco%2520Fernandes%2520and%2520Jo%25C3%25A3o%2520M.%2520Pereira%2520and%2520Catarina%2520Moreira%2520and%2520Jacinto%2520C.%2520Nascimento%2520and%2520Joaquim%2520Jorge%26entry.1292438233%3D%2520%2520Depth%2520maps%2520produced%2520by%2520consumer-grade%2520sensors%2520suffer%2520from%2520inaccurate%250Ameasurements%2520and%2520missing%2520data%2520from%2520either%2520system%2520or%2520scene-specific%2520sources.%250AData-driven%2520denoising%2520algorithms%2520can%2520mitigate%2520such%2520problems.%2520However%252C%2520they%250Arequire%2520vast%2520amounts%2520of%2520ground%2520truth%2520depth%2520data.%2520Recent%2520research%2520has%2520tackled%250Athis%2520limitation%2520using%2520self-supervised%2520learning%2520techniques%252C%2520but%2520it%2520requires%250Amultiple%2520RGB-D%2520sensors.%2520Moreover%252C%2520most%2520existing%2520approaches%2520focus%2520on%2520denoising%250Asingle%2520isolated%2520depth%2520maps%2520or%2520specific%2520subjects%2520of%2520interest%252C%2520highlighting%2520a%250Aneed%2520for%2520methods%2520to%2520effectively%2520denoise%2520depth%2520maps%2520in%2520real-time%2520dynamic%250Aenvironments.%2520This%2520paper%2520extends%2520state-of-the-art%2520approaches%2520for%250Adepth-denoising%2520commodity%2520depth%2520devices%252C%2520proposing%2520SelfReDepth%252C%2520a%250Aself-supervised%2520deep%2520learning%2520technique%2520for%2520depth%2520restoration%252C%2520via%2520denoising%250Aand%2520hole-filling%2520by%2520inpainting%2520full-depth%2520maps%2520captured%2520with%2520RGB-D%2520sensors.%2520The%250Aalgorithm%2520targets%2520depth%2520data%2520in%2520video%2520streams%252C%2520utilizing%2520multiple%2520sequential%250Adepth%2520frames%2520coupled%2520with%2520color%2520data%2520to%2520achieve%2520high-quality%2520depth%2520videos%2520with%250Atemporal%2520coherence.%2520Finally%252C%2520SelfReDepth%2520is%2520designed%2520to%2520be%2520compatible%2520with%250Avarious%2520RGB-D%2520sensors%2520and%2520usable%2520in%2520real-time%2520scenarios%2520as%2520a%2520pre-processing%250Astep%2520before%2520applying%2520other%2520depth-dependent%2520algorithms.%2520Our%2520results%2520demonstrate%250Aour%2520approach%2527s%2520real-time%2520performance%2520on%2520real-world%2520datasets.%2520They%2520show%2520that%2520it%250Aoutperforms%2520state-of-the-art%2520denoising%2520and%2520restoration%2520performance%2520at%2520over%250A30fps%2520on%2520Commercial%2520Depth%2520Cameras%252C%2520with%2520potential%2520benefits%2520for%2520augmented%2520and%250Amixed-reality%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfReDepth%3A%20Self-Supervised%20Real-Time%20Depth%20Restoration%20for%0A%20%20Consumer-Grade%20Sensors&entry.906535625=Alexandre%20Duarte%20and%20Francisco%20Fernandes%20and%20Jo%C3%A3o%20M.%20Pereira%20and%20Catarina%20Moreira%20and%20Jacinto%20C.%20Nascimento%20and%20Joaquim%20Jorge&entry.1292438233=%20%20Depth%20maps%20produced%20by%20consumer-grade%20sensors%20suffer%20from%20inaccurate%0Ameasurements%20and%20missing%20data%20from%20either%20system%20or%20scene-specific%20sources.%0AData-driven%20denoising%20algorithms%20can%20mitigate%20such%20problems.%20However%2C%20they%0Arequire%20vast%20amounts%20of%20ground%20truth%20depth%20data.%20Recent%20research%20has%20tackled%0Athis%20limitation%20using%20self-supervised%20learning%20techniques%2C%20but%20it%20requires%0Amultiple%20RGB-D%20sensors.%20Moreover%2C%20most%20existing%20approaches%20focus%20on%20denoising%0Asingle%20isolated%20depth%20maps%20or%20specific%20subjects%20of%20interest%2C%20highlighting%20a%0Aneed%20for%20methods%20to%20effectively%20denoise%20depth%20maps%20in%20real-time%20dynamic%0Aenvironments.%20This%20paper%20extends%20state-of-the-art%20approaches%20for%0Adepth-denoising%20commodity%20depth%20devices%2C%20proposing%20SelfReDepth%2C%20a%0Aself-supervised%20deep%20learning%20technique%20for%20depth%20restoration%2C%20via%20denoising%0Aand%20hole-filling%20by%20inpainting%20full-depth%20maps%20captured%20with%20RGB-D%20sensors.%20The%0Aalgorithm%20targets%20depth%20data%20in%20video%20streams%2C%20utilizing%20multiple%20sequential%0Adepth%20frames%20coupled%20with%20color%20data%20to%20achieve%20high-quality%20depth%20videos%20with%0Atemporal%20coherence.%20Finally%2C%20SelfReDepth%20is%20designed%20to%20be%20compatible%20with%0Avarious%20RGB-D%20sensors%20and%20usable%20in%20real-time%20scenarios%20as%20a%20pre-processing%0Astep%20before%20applying%20other%20depth-dependent%20algorithms.%20Our%20results%20demonstrate%0Aour%20approach%27s%20real-time%20performance%20on%20real-world%20datasets.%20They%20show%20that%20it%0Aoutperforms%20state-of-the-art%20denoising%20and%20restoration%20performance%20at%20over%0A30fps%20on%20Commercial%20Depth%20Cameras%2C%20with%20potential%20benefits%20for%20augmented%20and%0Amixed-reality%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03388v1&entry.124074799=Read"},
{"title": "Gaussian Representation for Deformable Image Registration", "author": "Jihe Li and Fabian Zhang and Xia Li and Tianhao Zhang and Ye Zhang and Joachim Buhmann", "abstract": "  Deformable image registration (DIR) is a fundamental task in radiotherapy,\nwith existing methods often struggling to balance computational efficiency,\nregistration accuracy, and speed effectively. We introduce a novel DIR approach\nemploying parametric 3D Gaussian control points achieving a better tradeoff. It\nprovides an explicit and flexible representation for spatial deformation fields\nbetween 3D volumetric medical images, producing a displacement vector field\n(DVF) across all volumetric positions. The movement of individual voxels is\nderived using linear blend skinning (LBS) through localized interpolation of\ntransformations associated with neighboring Gaussians. This interpolation\nstrategy not only simplifies the determination of voxel motions but also acts\nas an effective regularization technique. Our approach incorporates a unified\noptimization process through backpropagation, enabling iterative learning of\nboth the parameters of the 3D Gaussians and their transformations.\nAdditionally, the density of Gaussians is adjusted adaptively during the\nlearning phase to accommodate varying degrees of motion complexity. We\nvalidated our approach on the 4D-CT lung DIR-Lab and cardiac ACDC datasets,\nachieving an average target registration error (TRE) of 1.06 mm within a\nmuch-improved processing time of 2.43 seconds for the DIR-Lab dataset over\nexisting methods, demonstrating significant advancements in both accuracy and\nefficiency.\n", "link": "http://arxiv.org/abs/2406.03394v1", "date": "2024-06-05", "relevancy": 2.8478, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5908}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5801}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Representation%20for%20Deformable%20Image%20Registration&body=Title%3A%20Gaussian%20Representation%20for%20Deformable%20Image%20Registration%0AAuthor%3A%20Jihe%20Li%20and%20Fabian%20Zhang%20and%20Xia%20Li%20and%20Tianhao%20Zhang%20and%20Ye%20Zhang%20and%20Joachim%20Buhmann%0AAbstract%3A%20%20%20Deformable%20image%20registration%20%28DIR%29%20is%20a%20fundamental%20task%20in%20radiotherapy%2C%0Awith%20existing%20methods%20often%20struggling%20to%20balance%20computational%20efficiency%2C%0Aregistration%20accuracy%2C%20and%20speed%20effectively.%20We%20introduce%20a%20novel%20DIR%20approach%0Aemploying%20parametric%203D%20Gaussian%20control%20points%20achieving%20a%20better%20tradeoff.%20It%0Aprovides%20an%20explicit%20and%20flexible%20representation%20for%20spatial%20deformation%20fields%0Abetween%203D%20volumetric%20medical%20images%2C%20producing%20a%20displacement%20vector%20field%0A%28DVF%29%20across%20all%20volumetric%20positions.%20The%20movement%20of%20individual%20voxels%20is%0Aderived%20using%20linear%20blend%20skinning%20%28LBS%29%20through%20localized%20interpolation%20of%0Atransformations%20associated%20with%20neighboring%20Gaussians.%20This%20interpolation%0Astrategy%20not%20only%20simplifies%20the%20determination%20of%20voxel%20motions%20but%20also%20acts%0Aas%20an%20effective%20regularization%20technique.%20Our%20approach%20incorporates%20a%20unified%0Aoptimization%20process%20through%20backpropagation%2C%20enabling%20iterative%20learning%20of%0Aboth%20the%20parameters%20of%20the%203D%20Gaussians%20and%20their%20transformations.%0AAdditionally%2C%20the%20density%20of%20Gaussians%20is%20adjusted%20adaptively%20during%20the%0Alearning%20phase%20to%20accommodate%20varying%20degrees%20of%20motion%20complexity.%20We%0Avalidated%20our%20approach%20on%20the%204D-CT%20lung%20DIR-Lab%20and%20cardiac%20ACDC%20datasets%2C%0Aachieving%20an%20average%20target%20registration%20error%20%28TRE%29%20of%201.06%20mm%20within%20a%0Amuch-improved%20processing%20time%20of%202.43%20seconds%20for%20the%20DIR-Lab%20dataset%20over%0Aexisting%20methods%2C%20demonstrating%20significant%20advancements%20in%20both%20accuracy%20and%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Representation%2520for%2520Deformable%2520Image%2520Registration%26entry.906535625%3DJihe%2520Li%2520and%2520Fabian%2520Zhang%2520and%2520Xia%2520Li%2520and%2520Tianhao%2520Zhang%2520and%2520Ye%2520Zhang%2520and%2520Joachim%2520Buhmann%26entry.1292438233%3D%2520%2520Deformable%2520image%2520registration%2520%2528DIR%2529%2520is%2520a%2520fundamental%2520task%2520in%2520radiotherapy%252C%250Awith%2520existing%2520methods%2520often%2520struggling%2520to%2520balance%2520computational%2520efficiency%252C%250Aregistration%2520accuracy%252C%2520and%2520speed%2520effectively.%2520We%2520introduce%2520a%2520novel%2520DIR%2520approach%250Aemploying%2520parametric%25203D%2520Gaussian%2520control%2520points%2520achieving%2520a%2520better%2520tradeoff.%2520It%250Aprovides%2520an%2520explicit%2520and%2520flexible%2520representation%2520for%2520spatial%2520deformation%2520fields%250Abetween%25203D%2520volumetric%2520medical%2520images%252C%2520producing%2520a%2520displacement%2520vector%2520field%250A%2528DVF%2529%2520across%2520all%2520volumetric%2520positions.%2520The%2520movement%2520of%2520individual%2520voxels%2520is%250Aderived%2520using%2520linear%2520blend%2520skinning%2520%2528LBS%2529%2520through%2520localized%2520interpolation%2520of%250Atransformations%2520associated%2520with%2520neighboring%2520Gaussians.%2520This%2520interpolation%250Astrategy%2520not%2520only%2520simplifies%2520the%2520determination%2520of%2520voxel%2520motions%2520but%2520also%2520acts%250Aas%2520an%2520effective%2520regularization%2520technique.%2520Our%2520approach%2520incorporates%2520a%2520unified%250Aoptimization%2520process%2520through%2520backpropagation%252C%2520enabling%2520iterative%2520learning%2520of%250Aboth%2520the%2520parameters%2520of%2520the%25203D%2520Gaussians%2520and%2520their%2520transformations.%250AAdditionally%252C%2520the%2520density%2520of%2520Gaussians%2520is%2520adjusted%2520adaptively%2520during%2520the%250Alearning%2520phase%2520to%2520accommodate%2520varying%2520degrees%2520of%2520motion%2520complexity.%2520We%250Avalidated%2520our%2520approach%2520on%2520the%25204D-CT%2520lung%2520DIR-Lab%2520and%2520cardiac%2520ACDC%2520datasets%252C%250Aachieving%2520an%2520average%2520target%2520registration%2520error%2520%2528TRE%2529%2520of%25201.06%2520mm%2520within%2520a%250Amuch-improved%2520processing%2520time%2520of%25202.43%2520seconds%2520for%2520the%2520DIR-Lab%2520dataset%2520over%250Aexisting%2520methods%252C%2520demonstrating%2520significant%2520advancements%2520in%2520both%2520accuracy%2520and%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Representation%20for%20Deformable%20Image%20Registration&entry.906535625=Jihe%20Li%20and%20Fabian%20Zhang%20and%20Xia%20Li%20and%20Tianhao%20Zhang%20and%20Ye%20Zhang%20and%20Joachim%20Buhmann&entry.1292438233=%20%20Deformable%20image%20registration%20%28DIR%29%20is%20a%20fundamental%20task%20in%20radiotherapy%2C%0Awith%20existing%20methods%20often%20struggling%20to%20balance%20computational%20efficiency%2C%0Aregistration%20accuracy%2C%20and%20speed%20effectively.%20We%20introduce%20a%20novel%20DIR%20approach%0Aemploying%20parametric%203D%20Gaussian%20control%20points%20achieving%20a%20better%20tradeoff.%20It%0Aprovides%20an%20explicit%20and%20flexible%20representation%20for%20spatial%20deformation%20fields%0Abetween%203D%20volumetric%20medical%20images%2C%20producing%20a%20displacement%20vector%20field%0A%28DVF%29%20across%20all%20volumetric%20positions.%20The%20movement%20of%20individual%20voxels%20is%0Aderived%20using%20linear%20blend%20skinning%20%28LBS%29%20through%20localized%20interpolation%20of%0Atransformations%20associated%20with%20neighboring%20Gaussians.%20This%20interpolation%0Astrategy%20not%20only%20simplifies%20the%20determination%20of%20voxel%20motions%20but%20also%20acts%0Aas%20an%20effective%20regularization%20technique.%20Our%20approach%20incorporates%20a%20unified%0Aoptimization%20process%20through%20backpropagation%2C%20enabling%20iterative%20learning%20of%0Aboth%20the%20parameters%20of%20the%203D%20Gaussians%20and%20their%20transformations.%0AAdditionally%2C%20the%20density%20of%20Gaussians%20is%20adjusted%20adaptively%20during%20the%0Alearning%20phase%20to%20accommodate%20varying%20degrees%20of%20motion%20complexity.%20We%0Avalidated%20our%20approach%20on%20the%204D-CT%20lung%20DIR-Lab%20and%20cardiac%20ACDC%20datasets%2C%0Aachieving%20an%20average%20target%20registration%20error%20%28TRE%29%20of%201.06%20mm%20within%20a%0Amuch-improved%20processing%20time%20of%202.43%20seconds%20for%20the%20DIR-Lab%20dataset%20over%0Aexisting%20methods%2C%20demonstrating%20significant%20advancements%20in%20both%20accuracy%20and%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03394v1&entry.124074799=Read"},
{"title": "Mixture of Gaussian-distributed Prototypes with Generative Modelling for\n  Interpretable and Trustworthy Image Recognition", "author": "Chong Wang and Yuanhong Chen and Fengbei Liu and Yuyuan Liu and Davis James McCarthy and Helen Frazer and Gustavo Carneiro", "abstract": "  Prototypical-part methods, e.g., ProtoPNet, enhance interpretability in image\nrecognition by linking predictions to training prototypes, thereby offering\nintuitive insights into their decision-making. Existing methods, which rely on\na point-based learning of prototypes, typically face two critical issues: 1)\nthe learned prototypes have limited representation power and are not suitable\nto detect Out-of-Distribution (OoD) inputs, reducing their decision\ntrustworthiness; and 2) the necessary projection of the learned prototypes back\ninto the space of training images causes a drastic degradation in the\npredictive performance. Furthermore, current prototype learning adopts an\naggressive approach that considers only the most active object parts during\ntraining, while overlooking sub-salient object regions which still hold crucial\nclassification information. In this paper, we present a new generative paradigm\nto learn prototype distributions, termed as Mixture of Gaussian-distributed\nPrototypes (MGProto). The distribution of prototypes from MGProto enables both\ninterpretable image classification and trustworthy recognition of OoD inputs.\nThe optimisation of MGProto naturally projects the learned prototype\ndistributions back into the training image space, thereby addressing the\nperformance degradation caused by prototype projection. Additionally, we\ndevelop a novel and effective prototype mining strategy that considers not only\nthe most active but also sub-salient object parts. To promote model\ncompactness, we further propose to prune MGProto by removing prototypes with\nlow importance priors. Experiments on CUB-200-2011, Stanford Cars, Stanford\nDogs, and Oxford-IIIT Pets datasets show that MGProto achieves state-of-the-art\nimage recognition and OoD detection performances, while providing encouraging\ninterpretability results.\n", "link": "http://arxiv.org/abs/2312.00092v2", "date": "2024-06-05", "relevancy": 2.8101, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5706}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5604}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Gaussian-distributed%20Prototypes%20with%20Generative%20Modelling%20for%0A%20%20Interpretable%20and%20Trustworthy%20Image%20Recognition&body=Title%3A%20Mixture%20of%20Gaussian-distributed%20Prototypes%20with%20Generative%20Modelling%20for%0A%20%20Interpretable%20and%20Trustworthy%20Image%20Recognition%0AAuthor%3A%20Chong%20Wang%20and%20Yuanhong%20Chen%20and%20Fengbei%20Liu%20and%20Yuyuan%20Liu%20and%20Davis%20James%20McCarthy%20and%20Helen%20Frazer%20and%20Gustavo%20Carneiro%0AAbstract%3A%20%20%20Prototypical-part%20methods%2C%20e.g.%2C%20ProtoPNet%2C%20enhance%20interpretability%20in%20image%0Arecognition%20by%20linking%20predictions%20to%20training%20prototypes%2C%20thereby%20offering%0Aintuitive%20insights%20into%20their%20decision-making.%20Existing%20methods%2C%20which%20rely%20on%0Aa%20point-based%20learning%20of%20prototypes%2C%20typically%20face%20two%20critical%20issues%3A%201%29%0Athe%20learned%20prototypes%20have%20limited%20representation%20power%20and%20are%20not%20suitable%0Ato%20detect%20Out-of-Distribution%20%28OoD%29%20inputs%2C%20reducing%20their%20decision%0Atrustworthiness%3B%20and%202%29%20the%20necessary%20projection%20of%20the%20learned%20prototypes%20back%0Ainto%20the%20space%20of%20training%20images%20causes%20a%20drastic%20degradation%20in%20the%0Apredictive%20performance.%20Furthermore%2C%20current%20prototype%20learning%20adopts%20an%0Aaggressive%20approach%20that%20considers%20only%20the%20most%20active%20object%20parts%20during%0Atraining%2C%20while%20overlooking%20sub-salient%20object%20regions%20which%20still%20hold%20crucial%0Aclassification%20information.%20In%20this%20paper%2C%20we%20present%20a%20new%20generative%20paradigm%0Ato%20learn%20prototype%20distributions%2C%20termed%20as%20Mixture%20of%20Gaussian-distributed%0APrototypes%20%28MGProto%29.%20The%20distribution%20of%20prototypes%20from%20MGProto%20enables%20both%0Ainterpretable%20image%20classification%20and%20trustworthy%20recognition%20of%20OoD%20inputs.%0AThe%20optimisation%20of%20MGProto%20naturally%20projects%20the%20learned%20prototype%0Adistributions%20back%20into%20the%20training%20image%20space%2C%20thereby%20addressing%20the%0Aperformance%20degradation%20caused%20by%20prototype%20projection.%20Additionally%2C%20we%0Adevelop%20a%20novel%20and%20effective%20prototype%20mining%20strategy%20that%20considers%20not%20only%0Athe%20most%20active%20but%20also%20sub-salient%20object%20parts.%20To%20promote%20model%0Acompactness%2C%20we%20further%20propose%20to%20prune%20MGProto%20by%20removing%20prototypes%20with%0Alow%20importance%20priors.%20Experiments%20on%20CUB-200-2011%2C%20Stanford%20Cars%2C%20Stanford%0ADogs%2C%20and%20Oxford-IIIT%20Pets%20datasets%20show%20that%20MGProto%20achieves%20state-of-the-art%0Aimage%20recognition%20and%20OoD%20detection%20performances%2C%20while%20providing%20encouraging%0Ainterpretability%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Gaussian-distributed%2520Prototypes%2520with%2520Generative%2520Modelling%2520for%250A%2520%2520Interpretable%2520and%2520Trustworthy%2520Image%2520Recognition%26entry.906535625%3DChong%2520Wang%2520and%2520Yuanhong%2520Chen%2520and%2520Fengbei%2520Liu%2520and%2520Yuyuan%2520Liu%2520and%2520Davis%2520James%2520McCarthy%2520and%2520Helen%2520Frazer%2520and%2520Gustavo%2520Carneiro%26entry.1292438233%3D%2520%2520Prototypical-part%2520methods%252C%2520e.g.%252C%2520ProtoPNet%252C%2520enhance%2520interpretability%2520in%2520image%250Arecognition%2520by%2520linking%2520predictions%2520to%2520training%2520prototypes%252C%2520thereby%2520offering%250Aintuitive%2520insights%2520into%2520their%2520decision-making.%2520Existing%2520methods%252C%2520which%2520rely%2520on%250Aa%2520point-based%2520learning%2520of%2520prototypes%252C%2520typically%2520face%2520two%2520critical%2520issues%253A%25201%2529%250Athe%2520learned%2520prototypes%2520have%2520limited%2520representation%2520power%2520and%2520are%2520not%2520suitable%250Ato%2520detect%2520Out-of-Distribution%2520%2528OoD%2529%2520inputs%252C%2520reducing%2520their%2520decision%250Atrustworthiness%253B%2520and%25202%2529%2520the%2520necessary%2520projection%2520of%2520the%2520learned%2520prototypes%2520back%250Ainto%2520the%2520space%2520of%2520training%2520images%2520causes%2520a%2520drastic%2520degradation%2520in%2520the%250Apredictive%2520performance.%2520Furthermore%252C%2520current%2520prototype%2520learning%2520adopts%2520an%250Aaggressive%2520approach%2520that%2520considers%2520only%2520the%2520most%2520active%2520object%2520parts%2520during%250Atraining%252C%2520while%2520overlooking%2520sub-salient%2520object%2520regions%2520which%2520still%2520hold%2520crucial%250Aclassification%2520information.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520generative%2520paradigm%250Ato%2520learn%2520prototype%2520distributions%252C%2520termed%2520as%2520Mixture%2520of%2520Gaussian-distributed%250APrototypes%2520%2528MGProto%2529.%2520The%2520distribution%2520of%2520prototypes%2520from%2520MGProto%2520enables%2520both%250Ainterpretable%2520image%2520classification%2520and%2520trustworthy%2520recognition%2520of%2520OoD%2520inputs.%250AThe%2520optimisation%2520of%2520MGProto%2520naturally%2520projects%2520the%2520learned%2520prototype%250Adistributions%2520back%2520into%2520the%2520training%2520image%2520space%252C%2520thereby%2520addressing%2520the%250Aperformance%2520degradation%2520caused%2520by%2520prototype%2520projection.%2520Additionally%252C%2520we%250Adevelop%2520a%2520novel%2520and%2520effective%2520prototype%2520mining%2520strategy%2520that%2520considers%2520not%2520only%250Athe%2520most%2520active%2520but%2520also%2520sub-salient%2520object%2520parts.%2520To%2520promote%2520model%250Acompactness%252C%2520we%2520further%2520propose%2520to%2520prune%2520MGProto%2520by%2520removing%2520prototypes%2520with%250Alow%2520importance%2520priors.%2520Experiments%2520on%2520CUB-200-2011%252C%2520Stanford%2520Cars%252C%2520Stanford%250ADogs%252C%2520and%2520Oxford-IIIT%2520Pets%2520datasets%2520show%2520that%2520MGProto%2520achieves%2520state-of-the-art%250Aimage%2520recognition%2520and%2520OoD%2520detection%2520performances%252C%2520while%2520providing%2520encouraging%250Ainterpretability%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Gaussian-distributed%20Prototypes%20with%20Generative%20Modelling%20for%0A%20%20Interpretable%20and%20Trustworthy%20Image%20Recognition&entry.906535625=Chong%20Wang%20and%20Yuanhong%20Chen%20and%20Fengbei%20Liu%20and%20Yuyuan%20Liu%20and%20Davis%20James%20McCarthy%20and%20Helen%20Frazer%20and%20Gustavo%20Carneiro&entry.1292438233=%20%20Prototypical-part%20methods%2C%20e.g.%2C%20ProtoPNet%2C%20enhance%20interpretability%20in%20image%0Arecognition%20by%20linking%20predictions%20to%20training%20prototypes%2C%20thereby%20offering%0Aintuitive%20insights%20into%20their%20decision-making.%20Existing%20methods%2C%20which%20rely%20on%0Aa%20point-based%20learning%20of%20prototypes%2C%20typically%20face%20two%20critical%20issues%3A%201%29%0Athe%20learned%20prototypes%20have%20limited%20representation%20power%20and%20are%20not%20suitable%0Ato%20detect%20Out-of-Distribution%20%28OoD%29%20inputs%2C%20reducing%20their%20decision%0Atrustworthiness%3B%20and%202%29%20the%20necessary%20projection%20of%20the%20learned%20prototypes%20back%0Ainto%20the%20space%20of%20training%20images%20causes%20a%20drastic%20degradation%20in%20the%0Apredictive%20performance.%20Furthermore%2C%20current%20prototype%20learning%20adopts%20an%0Aaggressive%20approach%20that%20considers%20only%20the%20most%20active%20object%20parts%20during%0Atraining%2C%20while%20overlooking%20sub-salient%20object%20regions%20which%20still%20hold%20crucial%0Aclassification%20information.%20In%20this%20paper%2C%20we%20present%20a%20new%20generative%20paradigm%0Ato%20learn%20prototype%20distributions%2C%20termed%20as%20Mixture%20of%20Gaussian-distributed%0APrototypes%20%28MGProto%29.%20The%20distribution%20of%20prototypes%20from%20MGProto%20enables%20both%0Ainterpretable%20image%20classification%20and%20trustworthy%20recognition%20of%20OoD%20inputs.%0AThe%20optimisation%20of%20MGProto%20naturally%20projects%20the%20learned%20prototype%0Adistributions%20back%20into%20the%20training%20image%20space%2C%20thereby%20addressing%20the%0Aperformance%20degradation%20caused%20by%20prototype%20projection.%20Additionally%2C%20we%0Adevelop%20a%20novel%20and%20effective%20prototype%20mining%20strategy%20that%20considers%20not%20only%0Athe%20most%20active%20but%20also%20sub-salient%20object%20parts.%20To%20promote%20model%0Acompactness%2C%20we%20further%20propose%20to%20prune%20MGProto%20by%20removing%20prototypes%20with%0Alow%20importance%20priors.%20Experiments%20on%20CUB-200-2011%2C%20Stanford%20Cars%2C%20Stanford%0ADogs%2C%20and%20Oxford-IIIT%20Pets%20datasets%20show%20that%20MGProto%20achieves%20state-of-the-art%0Aimage%20recognition%20and%20OoD%20detection%20performances%2C%20while%20providing%20encouraging%0Ainterpretability%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00092v2&entry.124074799=Read"},
{"title": "Once-for-All: Controllable Generative Image Compression with Dynamic\n  Granularity Adaption", "author": "Anqi Li and Yuxi Liu and Huihui Bai and Feng Li and Runmin Cong and Meng Wang and Yao Zhao", "abstract": "  Although recent generative image compression methods have demonstrated\nimpressive potential in optimizing the rate-distortion-perception trade-off,\nthey still face the critical challenge of flexible rate adaption to diverse\ncompression necessities and scenarios. To overcome this challenge, this paper\nproposes a Controllable Generative Image Compression framework, Control-GIC,\nthe first capable of fine-grained bitrate adaption across a broad spectrum\nwhile ensuring high-fidelity and generality compression. We base Control-GIC on\na VQGAN framework representing an image as a sequence of variable-length codes\n(i.e. VQ-indices), which can be losslessly compressed and exhibits a direct\npositive correlation with the bitrates. Therefore, drawing inspiration from the\nclassical coding principle, we naturally correlate the information density of\nlocal image patches with their granular representations, to achieve dynamic\nadjustment of the code quantity following different granularity decisions. This\nimplies we can flexibly determine a proper allocation of granularity for the\npatches to acquire desirable compression rates. We further develop a\nprobabilistic conditional decoder that can trace back to historic encoded\nmulti-granularity representations according to transmitted codes, and then\nreconstruct hierarchical granular features in the formalization of conditional\nprobability, enabling more informative aggregation to improve reconstruction\nrealism. Our experiments show that Control-GIC allows highly flexible and\ncontrollable bitrate adaption and even once compression on an entire dataset to\nfulfill constrained bitrate conditions. Experimental results demonstrate its\nsuperior performance over recent state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.00758v2", "date": "2024-06-05", "relevancy": 2.7873, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5657}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5579}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Once-for-All%3A%20Controllable%20Generative%20Image%20Compression%20with%20Dynamic%0A%20%20Granularity%20Adaption&body=Title%3A%20Once-for-All%3A%20Controllable%20Generative%20Image%20Compression%20with%20Dynamic%0A%20%20Granularity%20Adaption%0AAuthor%3A%20Anqi%20Li%20and%20Yuxi%20Liu%20and%20Huihui%20Bai%20and%20Feng%20Li%20and%20Runmin%20Cong%20and%20Meng%20Wang%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20Although%20recent%20generative%20image%20compression%20methods%20have%20demonstrated%0Aimpressive%20potential%20in%20optimizing%20the%20rate-distortion-perception%20trade-off%2C%0Athey%20still%20face%20the%20critical%20challenge%20of%20flexible%20rate%20adaption%20to%20diverse%0Acompression%20necessities%20and%20scenarios.%20To%20overcome%20this%20challenge%2C%20this%20paper%0Aproposes%20a%20Controllable%20Generative%20Image%20Compression%20framework%2C%20Control-GIC%2C%0Athe%20first%20capable%20of%20fine-grained%20bitrate%20adaption%20across%20a%20broad%20spectrum%0Awhile%20ensuring%20high-fidelity%20and%20generality%20compression.%20We%20base%20Control-GIC%20on%0Aa%20VQGAN%20framework%20representing%20an%20image%20as%20a%20sequence%20of%20variable-length%20codes%0A%28i.e.%20VQ-indices%29%2C%20which%20can%20be%20losslessly%20compressed%20and%20exhibits%20a%20direct%0Apositive%20correlation%20with%20the%20bitrates.%20Therefore%2C%20drawing%20inspiration%20from%20the%0Aclassical%20coding%20principle%2C%20we%20naturally%20correlate%20the%20information%20density%20of%0Alocal%20image%20patches%20with%20their%20granular%20representations%2C%20to%20achieve%20dynamic%0Aadjustment%20of%20the%20code%20quantity%20following%20different%20granularity%20decisions.%20This%0Aimplies%20we%20can%20flexibly%20determine%20a%20proper%20allocation%20of%20granularity%20for%20the%0Apatches%20to%20acquire%20desirable%20compression%20rates.%20We%20further%20develop%20a%0Aprobabilistic%20conditional%20decoder%20that%20can%20trace%20back%20to%20historic%20encoded%0Amulti-granularity%20representations%20according%20to%20transmitted%20codes%2C%20and%20then%0Areconstruct%20hierarchical%20granular%20features%20in%20the%20formalization%20of%20conditional%0Aprobability%2C%20enabling%20more%20informative%20aggregation%20to%20improve%20reconstruction%0Arealism.%20Our%20experiments%20show%20that%20Control-GIC%20allows%20highly%20flexible%20and%0Acontrollable%20bitrate%20adaption%20and%20even%20once%20compression%20on%20an%20entire%20dataset%20to%0Afulfill%20constrained%20bitrate%20conditions.%20Experimental%20results%20demonstrate%20its%0Asuperior%20performance%20over%20recent%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00758v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnce-for-All%253A%2520Controllable%2520Generative%2520Image%2520Compression%2520with%2520Dynamic%250A%2520%2520Granularity%2520Adaption%26entry.906535625%3DAnqi%2520Li%2520and%2520Yuxi%2520Liu%2520and%2520Huihui%2520Bai%2520and%2520Feng%2520Li%2520and%2520Runmin%2520Cong%2520and%2520Meng%2520Wang%2520and%2520Yao%2520Zhao%26entry.1292438233%3D%2520%2520Although%2520recent%2520generative%2520image%2520compression%2520methods%2520have%2520demonstrated%250Aimpressive%2520potential%2520in%2520optimizing%2520the%2520rate-distortion-perception%2520trade-off%252C%250Athey%2520still%2520face%2520the%2520critical%2520challenge%2520of%2520flexible%2520rate%2520adaption%2520to%2520diverse%250Acompression%2520necessities%2520and%2520scenarios.%2520To%2520overcome%2520this%2520challenge%252C%2520this%2520paper%250Aproposes%2520a%2520Controllable%2520Generative%2520Image%2520Compression%2520framework%252C%2520Control-GIC%252C%250Athe%2520first%2520capable%2520of%2520fine-grained%2520bitrate%2520adaption%2520across%2520a%2520broad%2520spectrum%250Awhile%2520ensuring%2520high-fidelity%2520and%2520generality%2520compression.%2520We%2520base%2520Control-GIC%2520on%250Aa%2520VQGAN%2520framework%2520representing%2520an%2520image%2520as%2520a%2520sequence%2520of%2520variable-length%2520codes%250A%2528i.e.%2520VQ-indices%2529%252C%2520which%2520can%2520be%2520losslessly%2520compressed%2520and%2520exhibits%2520a%2520direct%250Apositive%2520correlation%2520with%2520the%2520bitrates.%2520Therefore%252C%2520drawing%2520inspiration%2520from%2520the%250Aclassical%2520coding%2520principle%252C%2520we%2520naturally%2520correlate%2520the%2520information%2520density%2520of%250Alocal%2520image%2520patches%2520with%2520their%2520granular%2520representations%252C%2520to%2520achieve%2520dynamic%250Aadjustment%2520of%2520the%2520code%2520quantity%2520following%2520different%2520granularity%2520decisions.%2520This%250Aimplies%2520we%2520can%2520flexibly%2520determine%2520a%2520proper%2520allocation%2520of%2520granularity%2520for%2520the%250Apatches%2520to%2520acquire%2520desirable%2520compression%2520rates.%2520We%2520further%2520develop%2520a%250Aprobabilistic%2520conditional%2520decoder%2520that%2520can%2520trace%2520back%2520to%2520historic%2520encoded%250Amulti-granularity%2520representations%2520according%2520to%2520transmitted%2520codes%252C%2520and%2520then%250Areconstruct%2520hierarchical%2520granular%2520features%2520in%2520the%2520formalization%2520of%2520conditional%250Aprobability%252C%2520enabling%2520more%2520informative%2520aggregation%2520to%2520improve%2520reconstruction%250Arealism.%2520Our%2520experiments%2520show%2520that%2520Control-GIC%2520allows%2520highly%2520flexible%2520and%250Acontrollable%2520bitrate%2520adaption%2520and%2520even%2520once%2520compression%2520on%2520an%2520entire%2520dataset%2520to%250Afulfill%2520constrained%2520bitrate%2520conditions.%2520Experimental%2520results%2520demonstrate%2520its%250Asuperior%2520performance%2520over%2520recent%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00758v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Once-for-All%3A%20Controllable%20Generative%20Image%20Compression%20with%20Dynamic%0A%20%20Granularity%20Adaption&entry.906535625=Anqi%20Li%20and%20Yuxi%20Liu%20and%20Huihui%20Bai%20and%20Feng%20Li%20and%20Runmin%20Cong%20and%20Meng%20Wang%20and%20Yao%20Zhao&entry.1292438233=%20%20Although%20recent%20generative%20image%20compression%20methods%20have%20demonstrated%0Aimpressive%20potential%20in%20optimizing%20the%20rate-distortion-perception%20trade-off%2C%0Athey%20still%20face%20the%20critical%20challenge%20of%20flexible%20rate%20adaption%20to%20diverse%0Acompression%20necessities%20and%20scenarios.%20To%20overcome%20this%20challenge%2C%20this%20paper%0Aproposes%20a%20Controllable%20Generative%20Image%20Compression%20framework%2C%20Control-GIC%2C%0Athe%20first%20capable%20of%20fine-grained%20bitrate%20adaption%20across%20a%20broad%20spectrum%0Awhile%20ensuring%20high-fidelity%20and%20generality%20compression.%20We%20base%20Control-GIC%20on%0Aa%20VQGAN%20framework%20representing%20an%20image%20as%20a%20sequence%20of%20variable-length%20codes%0A%28i.e.%20VQ-indices%29%2C%20which%20can%20be%20losslessly%20compressed%20and%20exhibits%20a%20direct%0Apositive%20correlation%20with%20the%20bitrates.%20Therefore%2C%20drawing%20inspiration%20from%20the%0Aclassical%20coding%20principle%2C%20we%20naturally%20correlate%20the%20information%20density%20of%0Alocal%20image%20patches%20with%20their%20granular%20representations%2C%20to%20achieve%20dynamic%0Aadjustment%20of%20the%20code%20quantity%20following%20different%20granularity%20decisions.%20This%0Aimplies%20we%20can%20flexibly%20determine%20a%20proper%20allocation%20of%20granularity%20for%20the%0Apatches%20to%20acquire%20desirable%20compression%20rates.%20We%20further%20develop%20a%0Aprobabilistic%20conditional%20decoder%20that%20can%20trace%20back%20to%20historic%20encoded%0Amulti-granularity%20representations%20according%20to%20transmitted%20codes%2C%20and%20then%0Areconstruct%20hierarchical%20granular%20features%20in%20the%20formalization%20of%20conditional%0Aprobability%2C%20enabling%20more%20informative%20aggregation%20to%20improve%20reconstruction%0Arealism.%20Our%20experiments%20show%20that%20Control-GIC%20allows%20highly%20flexible%20and%0Acontrollable%20bitrate%20adaption%20and%20even%20once%20compression%20on%20an%20entire%20dataset%20to%0Afulfill%20constrained%20bitrate%20conditions.%20Experimental%20results%20demonstrate%20its%0Asuperior%20performance%20over%20recent%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00758v2&entry.124074799=Read"},
{"title": "A Flexible Recursive Network for Video Stereo Matching Based on Residual\n  Estimation", "author": "Youchen Zhao and Guorong Luo and Hua Zhong and Haixiong Li", "abstract": "  Due to the high similarity of disparity between consecutive frames in video\nsequences, the area where disparity changes is defined as the residual map,\nwhich can be calculated. Based on this, we propose RecSM, a network based on\nresidual estimation with a flexible recursive structure for video stereo\nmatching. The RecSM network accelerates stereo matching using a Multi-scale\nResidual Estimation Module (MREM), which employs the temporal context as a\nreference and rapidly calculates the disparity for the current frame by\ncomputing only the residual values between the current and previous frames. To\nfurther reduce the error of estimated disparities, we use the Disparity\nOptimization Module (DOM) and Temporal Attention Module (TAM) to enforce\nconstraints between each module, and together with MREM, form a flexible\nStackable Computation Structure (SCS), which allows for the design of different\nnumbers of SCS based on practical scenarios. Experimental results demonstrate\nthat with a stack count of 3, RecSM achieves a 4x speed improvement compared to\nACVNet, running at 0.054 seconds based on one NVIDIA RTX 2080TI GPU, with an\naccuracy decrease of only 0.7%. Code is available at\nhttps://github.com/Y0uchenZ/RecSM.\n", "link": "http://arxiv.org/abs/2406.03333v1", "date": "2024-06-05", "relevancy": 2.6936, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5437}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5372}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Flexible%20Recursive%20Network%20for%20Video%20Stereo%20Matching%20Based%20on%20Residual%0A%20%20Estimation&body=Title%3A%20A%20Flexible%20Recursive%20Network%20for%20Video%20Stereo%20Matching%20Based%20on%20Residual%0A%20%20Estimation%0AAuthor%3A%20Youchen%20Zhao%20and%20Guorong%20Luo%20and%20Hua%20Zhong%20and%20Haixiong%20Li%0AAbstract%3A%20%20%20Due%20to%20the%20high%20similarity%20of%20disparity%20between%20consecutive%20frames%20in%20video%0Asequences%2C%20the%20area%20where%20disparity%20changes%20is%20defined%20as%20the%20residual%20map%2C%0Awhich%20can%20be%20calculated.%20Based%20on%20this%2C%20we%20propose%20RecSM%2C%20a%20network%20based%20on%0Aresidual%20estimation%20with%20a%20flexible%20recursive%20structure%20for%20video%20stereo%0Amatching.%20The%20RecSM%20network%20accelerates%20stereo%20matching%20using%20a%20Multi-scale%0AResidual%20Estimation%20Module%20%28MREM%29%2C%20which%20employs%20the%20temporal%20context%20as%20a%0Areference%20and%20rapidly%20calculates%20the%20disparity%20for%20the%20current%20frame%20by%0Acomputing%20only%20the%20residual%20values%20between%20the%20current%20and%20previous%20frames.%20To%0Afurther%20reduce%20the%20error%20of%20estimated%20disparities%2C%20we%20use%20the%20Disparity%0AOptimization%20Module%20%28DOM%29%20and%20Temporal%20Attention%20Module%20%28TAM%29%20to%20enforce%0Aconstraints%20between%20each%20module%2C%20and%20together%20with%20MREM%2C%20form%20a%20flexible%0AStackable%20Computation%20Structure%20%28SCS%29%2C%20which%20allows%20for%20the%20design%20of%20different%0Anumbers%20of%20SCS%20based%20on%20practical%20scenarios.%20Experimental%20results%20demonstrate%0Athat%20with%20a%20stack%20count%20of%203%2C%20RecSM%20achieves%20a%204x%20speed%20improvement%20compared%20to%0AACVNet%2C%20running%20at%200.054%20seconds%20based%20on%20one%20NVIDIA%20RTX%202080TI%20GPU%2C%20with%20an%0Aaccuracy%20decrease%20of%20only%200.7%25.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Y0uchenZ/RecSM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Flexible%2520Recursive%2520Network%2520for%2520Video%2520Stereo%2520Matching%2520Based%2520on%2520Residual%250A%2520%2520Estimation%26entry.906535625%3DYouchen%2520Zhao%2520and%2520Guorong%2520Luo%2520and%2520Hua%2520Zhong%2520and%2520Haixiong%2520Li%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520high%2520similarity%2520of%2520disparity%2520between%2520consecutive%2520frames%2520in%2520video%250Asequences%252C%2520the%2520area%2520where%2520disparity%2520changes%2520is%2520defined%2520as%2520the%2520residual%2520map%252C%250Awhich%2520can%2520be%2520calculated.%2520Based%2520on%2520this%252C%2520we%2520propose%2520RecSM%252C%2520a%2520network%2520based%2520on%250Aresidual%2520estimation%2520with%2520a%2520flexible%2520recursive%2520structure%2520for%2520video%2520stereo%250Amatching.%2520The%2520RecSM%2520network%2520accelerates%2520stereo%2520matching%2520using%2520a%2520Multi-scale%250AResidual%2520Estimation%2520Module%2520%2528MREM%2529%252C%2520which%2520employs%2520the%2520temporal%2520context%2520as%2520a%250Areference%2520and%2520rapidly%2520calculates%2520the%2520disparity%2520for%2520the%2520current%2520frame%2520by%250Acomputing%2520only%2520the%2520residual%2520values%2520between%2520the%2520current%2520and%2520previous%2520frames.%2520To%250Afurther%2520reduce%2520the%2520error%2520of%2520estimated%2520disparities%252C%2520we%2520use%2520the%2520Disparity%250AOptimization%2520Module%2520%2528DOM%2529%2520and%2520Temporal%2520Attention%2520Module%2520%2528TAM%2529%2520to%2520enforce%250Aconstraints%2520between%2520each%2520module%252C%2520and%2520together%2520with%2520MREM%252C%2520form%2520a%2520flexible%250AStackable%2520Computation%2520Structure%2520%2528SCS%2529%252C%2520which%2520allows%2520for%2520the%2520design%2520of%2520different%250Anumbers%2520of%2520SCS%2520based%2520on%2520practical%2520scenarios.%2520Experimental%2520results%2520demonstrate%250Athat%2520with%2520a%2520stack%2520count%2520of%25203%252C%2520RecSM%2520achieves%2520a%25204x%2520speed%2520improvement%2520compared%2520to%250AACVNet%252C%2520running%2520at%25200.054%2520seconds%2520based%2520on%2520one%2520NVIDIA%2520RTX%25202080TI%2520GPU%252C%2520with%2520an%250Aaccuracy%2520decrease%2520of%2520only%25200.7%2525.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Y0uchenZ/RecSM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Flexible%20Recursive%20Network%20for%20Video%20Stereo%20Matching%20Based%20on%20Residual%0A%20%20Estimation&entry.906535625=Youchen%20Zhao%20and%20Guorong%20Luo%20and%20Hua%20Zhong%20and%20Haixiong%20Li&entry.1292438233=%20%20Due%20to%20the%20high%20similarity%20of%20disparity%20between%20consecutive%20frames%20in%20video%0Asequences%2C%20the%20area%20where%20disparity%20changes%20is%20defined%20as%20the%20residual%20map%2C%0Awhich%20can%20be%20calculated.%20Based%20on%20this%2C%20we%20propose%20RecSM%2C%20a%20network%20based%20on%0Aresidual%20estimation%20with%20a%20flexible%20recursive%20structure%20for%20video%20stereo%0Amatching.%20The%20RecSM%20network%20accelerates%20stereo%20matching%20using%20a%20Multi-scale%0AResidual%20Estimation%20Module%20%28MREM%29%2C%20which%20employs%20the%20temporal%20context%20as%20a%0Areference%20and%20rapidly%20calculates%20the%20disparity%20for%20the%20current%20frame%20by%0Acomputing%20only%20the%20residual%20values%20between%20the%20current%20and%20previous%20frames.%20To%0Afurther%20reduce%20the%20error%20of%20estimated%20disparities%2C%20we%20use%20the%20Disparity%0AOptimization%20Module%20%28DOM%29%20and%20Temporal%20Attention%20Module%20%28TAM%29%20to%20enforce%0Aconstraints%20between%20each%20module%2C%20and%20together%20with%20MREM%2C%20form%20a%20flexible%0AStackable%20Computation%20Structure%20%28SCS%29%2C%20which%20allows%20for%20the%20design%20of%20different%0Anumbers%20of%20SCS%20based%20on%20practical%20scenarios.%20Experimental%20results%20demonstrate%0Athat%20with%20a%20stack%20count%20of%203%2C%20RecSM%20achieves%20a%204x%20speed%20improvement%20compared%20to%0AACVNet%2C%20running%20at%200.054%20seconds%20based%20on%20one%20NVIDIA%20RTX%202080TI%20GPU%2C%20with%20an%0Aaccuracy%20decrease%20of%20only%200.7%25.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Y0uchenZ/RecSM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03333v1&entry.124074799=Read"},
{"title": "CoopHash: Cooperative Learning of Multipurpose Descriptor and\n  Contrastive Pair Generator via Variational MCMC Teaching for Supervised Image\n  Hashing", "author": "Khoa D. Doan and Jianwen Xie and Yaxuan Zhu and Yang Zhao and Ping Li", "abstract": "  Leveraging supervised information can lead to superior retrieval performance\nin the image hashing domain but the performance degrades significantly without\nenough labeled data. One effective solution to boost performance is to employ\ngenerative models, such as Generative Adversarial Networks (GANs), to generate\nsynthetic data in an image hashing model. However, GAN-based methods are\ndifficult to train, which prevents the hashing approaches from jointly training\nthe generative models and the hash functions. This limitation results in\nsub-optimal retrieval performance. To overcome this limitation, we propose a\nnovel framework, the generative cooperative hashing network, which is based on\nenergy-based cooperative learning. This framework jointly learns a powerful\ngenerative representation of the data and a robust hash function via two\ncomponents: a top-down contrastive pair generator that synthesizes contrastive\nimages and a bottom-up multipurpose descriptor that simultaneously represents\nthe images from multiple perspectives, including probability density, hash\ncode, latent code, and category. The two components are jointly learned via a\nnovel likelihood-based cooperative learning scheme. We conduct experiments on\nseveral real-world datasets and show that the proposed method outperforms the\ncompeting hashing supervised methods, achieving up to 10\\% relative improvement\nover the current state-of-the-art supervised hashing methods, and exhibits a\nsignificantly better performance in out-of-distribution retrieval.\n", "link": "http://arxiv.org/abs/2210.04288v2", "date": "2024-06-05", "relevancy": 2.6741, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5464}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5403}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoopHash%3A%20Cooperative%20Learning%20of%20Multipurpose%20Descriptor%20and%0A%20%20Contrastive%20Pair%20Generator%20via%20Variational%20MCMC%20Teaching%20for%20Supervised%20Image%0A%20%20Hashing&body=Title%3A%20CoopHash%3A%20Cooperative%20Learning%20of%20Multipurpose%20Descriptor%20and%0A%20%20Contrastive%20Pair%20Generator%20via%20Variational%20MCMC%20Teaching%20for%20Supervised%20Image%0A%20%20Hashing%0AAuthor%3A%20Khoa%20D.%20Doan%20and%20Jianwen%20Xie%20and%20Yaxuan%20Zhu%20and%20Yang%20Zhao%20and%20Ping%20Li%0AAbstract%3A%20%20%20Leveraging%20supervised%20information%20can%20lead%20to%20superior%20retrieval%20performance%0Ain%20the%20image%20hashing%20domain%20but%20the%20performance%20degrades%20significantly%20without%0Aenough%20labeled%20data.%20One%20effective%20solution%20to%20boost%20performance%20is%20to%20employ%0Agenerative%20models%2C%20such%20as%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20to%20generate%0Asynthetic%20data%20in%20an%20image%20hashing%20model.%20However%2C%20GAN-based%20methods%20are%0Adifficult%20to%20train%2C%20which%20prevents%20the%20hashing%20approaches%20from%20jointly%20training%0Athe%20generative%20models%20and%20the%20hash%20functions.%20This%20limitation%20results%20in%0Asub-optimal%20retrieval%20performance.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%0Anovel%20framework%2C%20the%20generative%20cooperative%20hashing%20network%2C%20which%20is%20based%20on%0Aenergy-based%20cooperative%20learning.%20This%20framework%20jointly%20learns%20a%20powerful%0Agenerative%20representation%20of%20the%20data%20and%20a%20robust%20hash%20function%20via%20two%0Acomponents%3A%20a%20top-down%20contrastive%20pair%20generator%20that%20synthesizes%20contrastive%0Aimages%20and%20a%20bottom-up%20multipurpose%20descriptor%20that%20simultaneously%20represents%0Athe%20images%20from%20multiple%20perspectives%2C%20including%20probability%20density%2C%20hash%0Acode%2C%20latent%20code%2C%20and%20category.%20The%20two%20components%20are%20jointly%20learned%20via%20a%0Anovel%20likelihood-based%20cooperative%20learning%20scheme.%20We%20conduct%20experiments%20on%0Aseveral%20real-world%20datasets%20and%20show%20that%20the%20proposed%20method%20outperforms%20the%0Acompeting%20hashing%20supervised%20methods%2C%20achieving%20up%20to%2010%5C%25%20relative%20improvement%0Aover%20the%20current%20state-of-the-art%20supervised%20hashing%20methods%2C%20and%20exhibits%20a%0Asignificantly%20better%20performance%20in%20out-of-distribution%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.04288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoopHash%253A%2520Cooperative%2520Learning%2520of%2520Multipurpose%2520Descriptor%2520and%250A%2520%2520Contrastive%2520Pair%2520Generator%2520via%2520Variational%2520MCMC%2520Teaching%2520for%2520Supervised%2520Image%250A%2520%2520Hashing%26entry.906535625%3DKhoa%2520D.%2520Doan%2520and%2520Jianwen%2520Xie%2520and%2520Yaxuan%2520Zhu%2520and%2520Yang%2520Zhao%2520and%2520Ping%2520Li%26entry.1292438233%3D%2520%2520Leveraging%2520supervised%2520information%2520can%2520lead%2520to%2520superior%2520retrieval%2520performance%250Ain%2520the%2520image%2520hashing%2520domain%2520but%2520the%2520performance%2520degrades%2520significantly%2520without%250Aenough%2520labeled%2520data.%2520One%2520effective%2520solution%2520to%2520boost%2520performance%2520is%2520to%2520employ%250Agenerative%2520models%252C%2520such%2520as%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%252C%2520to%2520generate%250Asynthetic%2520data%2520in%2520an%2520image%2520hashing%2520model.%2520However%252C%2520GAN-based%2520methods%2520are%250Adifficult%2520to%2520train%252C%2520which%2520prevents%2520the%2520hashing%2520approaches%2520from%2520jointly%2520training%250Athe%2520generative%2520models%2520and%2520the%2520hash%2520functions.%2520This%2520limitation%2520results%2520in%250Asub-optimal%2520retrieval%2520performance.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520the%2520generative%2520cooperative%2520hashing%2520network%252C%2520which%2520is%2520based%2520on%250Aenergy-based%2520cooperative%2520learning.%2520This%2520framework%2520jointly%2520learns%2520a%2520powerful%250Agenerative%2520representation%2520of%2520the%2520data%2520and%2520a%2520robust%2520hash%2520function%2520via%2520two%250Acomponents%253A%2520a%2520top-down%2520contrastive%2520pair%2520generator%2520that%2520synthesizes%2520contrastive%250Aimages%2520and%2520a%2520bottom-up%2520multipurpose%2520descriptor%2520that%2520simultaneously%2520represents%250Athe%2520images%2520from%2520multiple%2520perspectives%252C%2520including%2520probability%2520density%252C%2520hash%250Acode%252C%2520latent%2520code%252C%2520and%2520category.%2520The%2520two%2520components%2520are%2520jointly%2520learned%2520via%2520a%250Anovel%2520likelihood-based%2520cooperative%2520learning%2520scheme.%2520We%2520conduct%2520experiments%2520on%250Aseveral%2520real-world%2520datasets%2520and%2520show%2520that%2520the%2520proposed%2520method%2520outperforms%2520the%250Acompeting%2520hashing%2520supervised%2520methods%252C%2520achieving%2520up%2520to%252010%255C%2525%2520relative%2520improvement%250Aover%2520the%2520current%2520state-of-the-art%2520supervised%2520hashing%2520methods%252C%2520and%2520exhibits%2520a%250Asignificantly%2520better%2520performance%2520in%2520out-of-distribution%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.04288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoopHash%3A%20Cooperative%20Learning%20of%20Multipurpose%20Descriptor%20and%0A%20%20Contrastive%20Pair%20Generator%20via%20Variational%20MCMC%20Teaching%20for%20Supervised%20Image%0A%20%20Hashing&entry.906535625=Khoa%20D.%20Doan%20and%20Jianwen%20Xie%20and%20Yaxuan%20Zhu%20and%20Yang%20Zhao%20and%20Ping%20Li&entry.1292438233=%20%20Leveraging%20supervised%20information%20can%20lead%20to%20superior%20retrieval%20performance%0Ain%20the%20image%20hashing%20domain%20but%20the%20performance%20degrades%20significantly%20without%0Aenough%20labeled%20data.%20One%20effective%20solution%20to%20boost%20performance%20is%20to%20employ%0Agenerative%20models%2C%20such%20as%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20to%20generate%0Asynthetic%20data%20in%20an%20image%20hashing%20model.%20However%2C%20GAN-based%20methods%20are%0Adifficult%20to%20train%2C%20which%20prevents%20the%20hashing%20approaches%20from%20jointly%20training%0Athe%20generative%20models%20and%20the%20hash%20functions.%20This%20limitation%20results%20in%0Asub-optimal%20retrieval%20performance.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%0Anovel%20framework%2C%20the%20generative%20cooperative%20hashing%20network%2C%20which%20is%20based%20on%0Aenergy-based%20cooperative%20learning.%20This%20framework%20jointly%20learns%20a%20powerful%0Agenerative%20representation%20of%20the%20data%20and%20a%20robust%20hash%20function%20via%20two%0Acomponents%3A%20a%20top-down%20contrastive%20pair%20generator%20that%20synthesizes%20contrastive%0Aimages%20and%20a%20bottom-up%20multipurpose%20descriptor%20that%20simultaneously%20represents%0Athe%20images%20from%20multiple%20perspectives%2C%20including%20probability%20density%2C%20hash%0Acode%2C%20latent%20code%2C%20and%20category.%20The%20two%20components%20are%20jointly%20learned%20via%20a%0Anovel%20likelihood-based%20cooperative%20learning%20scheme.%20We%20conduct%20experiments%20on%0Aseveral%20real-world%20datasets%20and%20show%20that%20the%20proposed%20method%20outperforms%20the%0Acompeting%20hashing%20supervised%20methods%2C%20achieving%20up%20to%2010%5C%25%20relative%20improvement%0Aover%20the%20current%20state-of-the-art%20supervised%20hashing%20methods%2C%20and%20exhibits%20a%0Asignificantly%20better%20performance%20in%20out-of-distribution%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.04288v2&entry.124074799=Read"},
{"title": "Image Copy-Move Forgery Detection and Localization Scheme: How to Avoid\n  Missed Detection and False Alarm", "author": "Li Jiang and Zhaowei Lu and Yuebing Gao and Yifan Wang", "abstract": "  Image copy-move is an operation that replaces one part of the image with\nanother part of the same image, which can be used for illegal purposes due to\nthe potential semantic changes. Recent studies have shown that keypoint-based\nalgorithms achieved excellent and robust localization performance even when\nsmall or smooth tampered areas were involved. However, when the input image is\nlow-resolution, most existing keypoint-based algorithms are difficult to\ngenerate sufficient keypoints, resulting in more missed detections. In\naddition, existing algorithms are usually unable to distinguish between Similar\nbut Genuine Objects (SGO) images and tampered images, resulting in more false\nalarms. This is mainly due to the lack of further verification of local\nhomography matrix in forgery localization stage. To tackle these problems, this\npaper firstly proposes an excessive keypoint extraction strategy to overcome\nmissed detection. Subsequently, a group matching algorithm is used to speed up\nthe matching of excessive keypoints. Finally, a new iterative forgery\nlocalization algorithm is introduced to quickly form pixel-level localization\nresults while ensuring a lower false alarm. Extensive experimental results show\nthat our scheme has superior performance than state-of-the-art algorithms in\novercoming missed detection and false alarm. Our code is available at\nhttps://github.com/LUZW1998/CMFDL.\n", "link": "http://arxiv.org/abs/2406.03271v1", "date": "2024-06-05", "relevancy": 2.6476, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5719}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5333}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Copy-Move%20Forgery%20Detection%20and%20Localization%20Scheme%3A%20How%20to%20Avoid%0A%20%20Missed%20Detection%20and%20False%20Alarm&body=Title%3A%20Image%20Copy-Move%20Forgery%20Detection%20and%20Localization%20Scheme%3A%20How%20to%20Avoid%0A%20%20Missed%20Detection%20and%20False%20Alarm%0AAuthor%3A%20Li%20Jiang%20and%20Zhaowei%20Lu%20and%20Yuebing%20Gao%20and%20Yifan%20Wang%0AAbstract%3A%20%20%20Image%20copy-move%20is%20an%20operation%20that%20replaces%20one%20part%20of%20the%20image%20with%0Aanother%20part%20of%20the%20same%20image%2C%20which%20can%20be%20used%20for%20illegal%20purposes%20due%20to%0Athe%20potential%20semantic%20changes.%20Recent%20studies%20have%20shown%20that%20keypoint-based%0Aalgorithms%20achieved%20excellent%20and%20robust%20localization%20performance%20even%20when%0Asmall%20or%20smooth%20tampered%20areas%20were%20involved.%20However%2C%20when%20the%20input%20image%20is%0Alow-resolution%2C%20most%20existing%20keypoint-based%20algorithms%20are%20difficult%20to%0Agenerate%20sufficient%20keypoints%2C%20resulting%20in%20more%20missed%20detections.%20In%0Aaddition%2C%20existing%20algorithms%20are%20usually%20unable%20to%20distinguish%20between%20Similar%0Abut%20Genuine%20Objects%20%28SGO%29%20images%20and%20tampered%20images%2C%20resulting%20in%20more%20false%0Aalarms.%20This%20is%20mainly%20due%20to%20the%20lack%20of%20further%20verification%20of%20local%0Ahomography%20matrix%20in%20forgery%20localization%20stage.%20To%20tackle%20these%20problems%2C%20this%0Apaper%20firstly%20proposes%20an%20excessive%20keypoint%20extraction%20strategy%20to%20overcome%0Amissed%20detection.%20Subsequently%2C%20a%20group%20matching%20algorithm%20is%20used%20to%20speed%20up%0Athe%20matching%20of%20excessive%20keypoints.%20Finally%2C%20a%20new%20iterative%20forgery%0Alocalization%20algorithm%20is%20introduced%20to%20quickly%20form%20pixel-level%20localization%0Aresults%20while%20ensuring%20a%20lower%20false%20alarm.%20Extensive%20experimental%20results%20show%0Athat%20our%20scheme%20has%20superior%20performance%20than%20state-of-the-art%20algorithms%20in%0Aovercoming%20missed%20detection%20and%20false%20alarm.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LUZW1998/CMFDL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Copy-Move%2520Forgery%2520Detection%2520and%2520Localization%2520Scheme%253A%2520How%2520to%2520Avoid%250A%2520%2520Missed%2520Detection%2520and%2520False%2520Alarm%26entry.906535625%3DLi%2520Jiang%2520and%2520Zhaowei%2520Lu%2520and%2520Yuebing%2520Gao%2520and%2520Yifan%2520Wang%26entry.1292438233%3D%2520%2520Image%2520copy-move%2520is%2520an%2520operation%2520that%2520replaces%2520one%2520part%2520of%2520the%2520image%2520with%250Aanother%2520part%2520of%2520the%2520same%2520image%252C%2520which%2520can%2520be%2520used%2520for%2520illegal%2520purposes%2520due%2520to%250Athe%2520potential%2520semantic%2520changes.%2520Recent%2520studies%2520have%2520shown%2520that%2520keypoint-based%250Aalgorithms%2520achieved%2520excellent%2520and%2520robust%2520localization%2520performance%2520even%2520when%250Asmall%2520or%2520smooth%2520tampered%2520areas%2520were%2520involved.%2520However%252C%2520when%2520the%2520input%2520image%2520is%250Alow-resolution%252C%2520most%2520existing%2520keypoint-based%2520algorithms%2520are%2520difficult%2520to%250Agenerate%2520sufficient%2520keypoints%252C%2520resulting%2520in%2520more%2520missed%2520detections.%2520In%250Aaddition%252C%2520existing%2520algorithms%2520are%2520usually%2520unable%2520to%2520distinguish%2520between%2520Similar%250Abut%2520Genuine%2520Objects%2520%2528SGO%2529%2520images%2520and%2520tampered%2520images%252C%2520resulting%2520in%2520more%2520false%250Aalarms.%2520This%2520is%2520mainly%2520due%2520to%2520the%2520lack%2520of%2520further%2520verification%2520of%2520local%250Ahomography%2520matrix%2520in%2520forgery%2520localization%2520stage.%2520To%2520tackle%2520these%2520problems%252C%2520this%250Apaper%2520firstly%2520proposes%2520an%2520excessive%2520keypoint%2520extraction%2520strategy%2520to%2520overcome%250Amissed%2520detection.%2520Subsequently%252C%2520a%2520group%2520matching%2520algorithm%2520is%2520used%2520to%2520speed%2520up%250Athe%2520matching%2520of%2520excessive%2520keypoints.%2520Finally%252C%2520a%2520new%2520iterative%2520forgery%250Alocalization%2520algorithm%2520is%2520introduced%2520to%2520quickly%2520form%2520pixel-level%2520localization%250Aresults%2520while%2520ensuring%2520a%2520lower%2520false%2520alarm.%2520Extensive%2520experimental%2520results%2520show%250Athat%2520our%2520scheme%2520has%2520superior%2520performance%2520than%2520state-of-the-art%2520algorithms%2520in%250Aovercoming%2520missed%2520detection%2520and%2520false%2520alarm.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LUZW1998/CMFDL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Copy-Move%20Forgery%20Detection%20and%20Localization%20Scheme%3A%20How%20to%20Avoid%0A%20%20Missed%20Detection%20and%20False%20Alarm&entry.906535625=Li%20Jiang%20and%20Zhaowei%20Lu%20and%20Yuebing%20Gao%20and%20Yifan%20Wang&entry.1292438233=%20%20Image%20copy-move%20is%20an%20operation%20that%20replaces%20one%20part%20of%20the%20image%20with%0Aanother%20part%20of%20the%20same%20image%2C%20which%20can%20be%20used%20for%20illegal%20purposes%20due%20to%0Athe%20potential%20semantic%20changes.%20Recent%20studies%20have%20shown%20that%20keypoint-based%0Aalgorithms%20achieved%20excellent%20and%20robust%20localization%20performance%20even%20when%0Asmall%20or%20smooth%20tampered%20areas%20were%20involved.%20However%2C%20when%20the%20input%20image%20is%0Alow-resolution%2C%20most%20existing%20keypoint-based%20algorithms%20are%20difficult%20to%0Agenerate%20sufficient%20keypoints%2C%20resulting%20in%20more%20missed%20detections.%20In%0Aaddition%2C%20existing%20algorithms%20are%20usually%20unable%20to%20distinguish%20between%20Similar%0Abut%20Genuine%20Objects%20%28SGO%29%20images%20and%20tampered%20images%2C%20resulting%20in%20more%20false%0Aalarms.%20This%20is%20mainly%20due%20to%20the%20lack%20of%20further%20verification%20of%20local%0Ahomography%20matrix%20in%20forgery%20localization%20stage.%20To%20tackle%20these%20problems%2C%20this%0Apaper%20firstly%20proposes%20an%20excessive%20keypoint%20extraction%20strategy%20to%20overcome%0Amissed%20detection.%20Subsequently%2C%20a%20group%20matching%20algorithm%20is%20used%20to%20speed%20up%0Athe%20matching%20of%20excessive%20keypoints.%20Finally%2C%20a%20new%20iterative%20forgery%0Alocalization%20algorithm%20is%20introduced%20to%20quickly%20form%20pixel-level%20localization%0Aresults%20while%20ensuring%20a%20lower%20false%20alarm.%20Extensive%20experimental%20results%20show%0Athat%20our%20scheme%20has%20superior%20performance%20than%20state-of-the-art%20algorithms%20in%0Aovercoming%20missed%20detection%20and%20false%20alarm.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LUZW1998/CMFDL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03271v1&entry.124074799=Read"},
{"title": "Polarization Wavefront Lidar: Learning Large Scene Reconstruction from\n  Polarized Wavefronts", "author": "Dominik Scheuble and Chenyang Lei and Seung-Hwan Baek and Mario Bijelic and Felix Heide", "abstract": "  Lidar has become a cornerstone sensing modality for 3D vision, especially for\nlarge outdoor scenarios and autonomous driving. Conventional lidar sensors are\ncapable of providing centimeter-accurate distance information by emitting laser\npulses into a scene and measuring the time-of-flight (ToF) of the reflection.\nHowever, the polarization of the received light that depends on the surface\norientation and material properties is usually not considered. As such, the\npolarization modality has the potential to improve scene reconstruction beyond\ndistance measurements. In this work, we introduce a novel long-range\npolarization wavefront lidar sensor (PolLidar) that modulates the polarization\nof the emitted and received light. Departing from conventional lidar sensors,\nPolLidar allows access to the raw time-resolved polarimetric wavefronts. We\nleverage polarimetric wavefronts to estimate normals, distance, and material\nproperties in outdoor scenarios with a novel learned reconstruction method. To\ntrain and evaluate the method, we introduce a simulated and real-world\nlong-range dataset with paired raw lidar data, ground truth distance, and\nnormal maps. We find that the proposed method improves normal and distance\nreconstruction by 53\\% mean angular error and 41\\% mean absolute error compared\nto existing shape-from-polarization (SfP) and ToF methods. Code and data are\nopen-sourced at https://light.princeton.edu/pollidar.\n", "link": "http://arxiv.org/abs/2406.03461v1", "date": "2024-06-05", "relevancy": 2.6457, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5451}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5418}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polarization%20Wavefront%20Lidar%3A%20Learning%20Large%20Scene%20Reconstruction%20from%0A%20%20Polarized%20Wavefronts&body=Title%3A%20Polarization%20Wavefront%20Lidar%3A%20Learning%20Large%20Scene%20Reconstruction%20from%0A%20%20Polarized%20Wavefronts%0AAuthor%3A%20Dominik%20Scheuble%20and%20Chenyang%20Lei%20and%20Seung-Hwan%20Baek%20and%20Mario%20Bijelic%20and%20Felix%20Heide%0AAbstract%3A%20%20%20Lidar%20has%20become%20a%20cornerstone%20sensing%20modality%20for%203D%20vision%2C%20especially%20for%0Alarge%20outdoor%20scenarios%20and%20autonomous%20driving.%20Conventional%20lidar%20sensors%20are%0Acapable%20of%20providing%20centimeter-accurate%20distance%20information%20by%20emitting%20laser%0Apulses%20into%20a%20scene%20and%20measuring%20the%20time-of-flight%20%28ToF%29%20of%20the%20reflection.%0AHowever%2C%20the%20polarization%20of%20the%20received%20light%20that%20depends%20on%20the%20surface%0Aorientation%20and%20material%20properties%20is%20usually%20not%20considered.%20As%20such%2C%20the%0Apolarization%20modality%20has%20the%20potential%20to%20improve%20scene%20reconstruction%20beyond%0Adistance%20measurements.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20long-range%0Apolarization%20wavefront%20lidar%20sensor%20%28PolLidar%29%20that%20modulates%20the%20polarization%0Aof%20the%20emitted%20and%20received%20light.%20Departing%20from%20conventional%20lidar%20sensors%2C%0APolLidar%20allows%20access%20to%20the%20raw%20time-resolved%20polarimetric%20wavefronts.%20We%0Aleverage%20polarimetric%20wavefronts%20to%20estimate%20normals%2C%20distance%2C%20and%20material%0Aproperties%20in%20outdoor%20scenarios%20with%20a%20novel%20learned%20reconstruction%20method.%20To%0Atrain%20and%20evaluate%20the%20method%2C%20we%20introduce%20a%20simulated%20and%20real-world%0Along-range%20dataset%20with%20paired%20raw%20lidar%20data%2C%20ground%20truth%20distance%2C%20and%0Anormal%20maps.%20We%20find%20that%20the%20proposed%20method%20improves%20normal%20and%20distance%0Areconstruction%20by%2053%5C%25%20mean%20angular%20error%20and%2041%5C%25%20mean%20absolute%20error%20compared%0Ato%20existing%20shape-from-polarization%20%28SfP%29%20and%20ToF%20methods.%20Code%20and%20data%20are%0Aopen-sourced%20at%20https%3A//light.princeton.edu/pollidar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolarization%2520Wavefront%2520Lidar%253A%2520Learning%2520Large%2520Scene%2520Reconstruction%2520from%250A%2520%2520Polarized%2520Wavefronts%26entry.906535625%3DDominik%2520Scheuble%2520and%2520Chenyang%2520Lei%2520and%2520Seung-Hwan%2520Baek%2520and%2520Mario%2520Bijelic%2520and%2520Felix%2520Heide%26entry.1292438233%3D%2520%2520Lidar%2520has%2520become%2520a%2520cornerstone%2520sensing%2520modality%2520for%25203D%2520vision%252C%2520especially%2520for%250Alarge%2520outdoor%2520scenarios%2520and%2520autonomous%2520driving.%2520Conventional%2520lidar%2520sensors%2520are%250Acapable%2520of%2520providing%2520centimeter-accurate%2520distance%2520information%2520by%2520emitting%2520laser%250Apulses%2520into%2520a%2520scene%2520and%2520measuring%2520the%2520time-of-flight%2520%2528ToF%2529%2520of%2520the%2520reflection.%250AHowever%252C%2520the%2520polarization%2520of%2520the%2520received%2520light%2520that%2520depends%2520on%2520the%2520surface%250Aorientation%2520and%2520material%2520properties%2520is%2520usually%2520not%2520considered.%2520As%2520such%252C%2520the%250Apolarization%2520modality%2520has%2520the%2520potential%2520to%2520improve%2520scene%2520reconstruction%2520beyond%250Adistance%2520measurements.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520long-range%250Apolarization%2520wavefront%2520lidar%2520sensor%2520%2528PolLidar%2529%2520that%2520modulates%2520the%2520polarization%250Aof%2520the%2520emitted%2520and%2520received%2520light.%2520Departing%2520from%2520conventional%2520lidar%2520sensors%252C%250APolLidar%2520allows%2520access%2520to%2520the%2520raw%2520time-resolved%2520polarimetric%2520wavefronts.%2520We%250Aleverage%2520polarimetric%2520wavefronts%2520to%2520estimate%2520normals%252C%2520distance%252C%2520and%2520material%250Aproperties%2520in%2520outdoor%2520scenarios%2520with%2520a%2520novel%2520learned%2520reconstruction%2520method.%2520To%250Atrain%2520and%2520evaluate%2520the%2520method%252C%2520we%2520introduce%2520a%2520simulated%2520and%2520real-world%250Along-range%2520dataset%2520with%2520paired%2520raw%2520lidar%2520data%252C%2520ground%2520truth%2520distance%252C%2520and%250Anormal%2520maps.%2520We%2520find%2520that%2520the%2520proposed%2520method%2520improves%2520normal%2520and%2520distance%250Areconstruction%2520by%252053%255C%2525%2520mean%2520angular%2520error%2520and%252041%255C%2525%2520mean%2520absolute%2520error%2520compared%250Ato%2520existing%2520shape-from-polarization%2520%2528SfP%2529%2520and%2520ToF%2520methods.%2520Code%2520and%2520data%2520are%250Aopen-sourced%2520at%2520https%253A//light.princeton.edu/pollidar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polarization%20Wavefront%20Lidar%3A%20Learning%20Large%20Scene%20Reconstruction%20from%0A%20%20Polarized%20Wavefronts&entry.906535625=Dominik%20Scheuble%20and%20Chenyang%20Lei%20and%20Seung-Hwan%20Baek%20and%20Mario%20Bijelic%20and%20Felix%20Heide&entry.1292438233=%20%20Lidar%20has%20become%20a%20cornerstone%20sensing%20modality%20for%203D%20vision%2C%20especially%20for%0Alarge%20outdoor%20scenarios%20and%20autonomous%20driving.%20Conventional%20lidar%20sensors%20are%0Acapable%20of%20providing%20centimeter-accurate%20distance%20information%20by%20emitting%20laser%0Apulses%20into%20a%20scene%20and%20measuring%20the%20time-of-flight%20%28ToF%29%20of%20the%20reflection.%0AHowever%2C%20the%20polarization%20of%20the%20received%20light%20that%20depends%20on%20the%20surface%0Aorientation%20and%20material%20properties%20is%20usually%20not%20considered.%20As%20such%2C%20the%0Apolarization%20modality%20has%20the%20potential%20to%20improve%20scene%20reconstruction%20beyond%0Adistance%20measurements.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20long-range%0Apolarization%20wavefront%20lidar%20sensor%20%28PolLidar%29%20that%20modulates%20the%20polarization%0Aof%20the%20emitted%20and%20received%20light.%20Departing%20from%20conventional%20lidar%20sensors%2C%0APolLidar%20allows%20access%20to%20the%20raw%20time-resolved%20polarimetric%20wavefronts.%20We%0Aleverage%20polarimetric%20wavefronts%20to%20estimate%20normals%2C%20distance%2C%20and%20material%0Aproperties%20in%20outdoor%20scenarios%20with%20a%20novel%20learned%20reconstruction%20method.%20To%0Atrain%20and%20evaluate%20the%20method%2C%20we%20introduce%20a%20simulated%20and%20real-world%0Along-range%20dataset%20with%20paired%20raw%20lidar%20data%2C%20ground%20truth%20distance%2C%20and%0Anormal%20maps.%20We%20find%20that%20the%20proposed%20method%20improves%20normal%20and%20distance%0Areconstruction%20by%2053%5C%25%20mean%20angular%20error%20and%2041%5C%25%20mean%20absolute%20error%20compared%0Ato%20existing%20shape-from-polarization%20%28SfP%29%20and%20ToF%20methods.%20Code%20and%20data%20are%0Aopen-sourced%20at%20https%3A//light.princeton.edu/pollidar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03461v1&entry.124074799=Read"},
{"title": "Tackling GenAI Copyright Issues: Originality Estimation and\n  Genericization", "author": "Hiroaki Chiba-Okabe and Weijie J. Su", "abstract": "  The rapid progress of generative AI technology has sparked significant\ncopyright concerns, leading to numerous lawsuits filed against AI developers.\nWhile some studies explore methods to mitigate copyright risks by steering the\noutputs of generative models away from those resembling copyrighted data,\nlittle attention has been paid to the question of how much of a resemblance is\nundesirable; more original or unique data are afforded stronger protection, and\nthe threshold level of resemblance for constituting infringement\ncorrespondingly lower. Here, leveraging this principle, we propose a\ngenericization method that modifies the outputs of a generative model to make\nthem more generic and less likely to infringe copyright. To achieve this, we\nintroduce a metric for quantifying the level of originality of data in a manner\nthat is consistent with the legal framework. This metric can be practically\nestimated by drawing samples from a generative model, which is then used for\nthe genericization process. Experiments demonstrate that our genericization\nmethod successfully modifies the output of a text-to-image generative model so\nthat it produces more generic, copyright-compliant images.\n", "link": "http://arxiv.org/abs/2406.03341v1", "date": "2024-06-05", "relevancy": 2.6336, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5464}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.527}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tackling%20GenAI%20Copyright%20Issues%3A%20Originality%20Estimation%20and%0A%20%20Genericization&body=Title%3A%20Tackling%20GenAI%20Copyright%20Issues%3A%20Originality%20Estimation%20and%0A%20%20Genericization%0AAuthor%3A%20Hiroaki%20Chiba-Okabe%20and%20Weijie%20J.%20Su%0AAbstract%3A%20%20%20The%20rapid%20progress%20of%20generative%20AI%20technology%20has%20sparked%20significant%0Acopyright%20concerns%2C%20leading%20to%20numerous%20lawsuits%20filed%20against%20AI%20developers.%0AWhile%20some%20studies%20explore%20methods%20to%20mitigate%20copyright%20risks%20by%20steering%20the%0Aoutputs%20of%20generative%20models%20away%20from%20those%20resembling%20copyrighted%20data%2C%0Alittle%20attention%20has%20been%20paid%20to%20the%20question%20of%20how%20much%20of%20a%20resemblance%20is%0Aundesirable%3B%20more%20original%20or%20unique%20data%20are%20afforded%20stronger%20protection%2C%20and%0Athe%20threshold%20level%20of%20resemblance%20for%20constituting%20infringement%0Acorrespondingly%20lower.%20Here%2C%20leveraging%20this%20principle%2C%20we%20propose%20a%0Agenericization%20method%20that%20modifies%20the%20outputs%20of%20a%20generative%20model%20to%20make%0Athem%20more%20generic%20and%20less%20likely%20to%20infringe%20copyright.%20To%20achieve%20this%2C%20we%0Aintroduce%20a%20metric%20for%20quantifying%20the%20level%20of%20originality%20of%20data%20in%20a%20manner%0Athat%20is%20consistent%20with%20the%20legal%20framework.%20This%20metric%20can%20be%20practically%0Aestimated%20by%20drawing%20samples%20from%20a%20generative%20model%2C%20which%20is%20then%20used%20for%0Athe%20genericization%20process.%20Experiments%20demonstrate%20that%20our%20genericization%0Amethod%20successfully%20modifies%20the%20output%20of%20a%20text-to-image%20generative%20model%20so%0Athat%20it%20produces%20more%20generic%2C%20copyright-compliant%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTackling%2520GenAI%2520Copyright%2520Issues%253A%2520Originality%2520Estimation%2520and%250A%2520%2520Genericization%26entry.906535625%3DHiroaki%2520Chiba-Okabe%2520and%2520Weijie%2520J.%2520Su%26entry.1292438233%3D%2520%2520The%2520rapid%2520progress%2520of%2520generative%2520AI%2520technology%2520has%2520sparked%2520significant%250Acopyright%2520concerns%252C%2520leading%2520to%2520numerous%2520lawsuits%2520filed%2520against%2520AI%2520developers.%250AWhile%2520some%2520studies%2520explore%2520methods%2520to%2520mitigate%2520copyright%2520risks%2520by%2520steering%2520the%250Aoutputs%2520of%2520generative%2520models%2520away%2520from%2520those%2520resembling%2520copyrighted%2520data%252C%250Alittle%2520attention%2520has%2520been%2520paid%2520to%2520the%2520question%2520of%2520how%2520much%2520of%2520a%2520resemblance%2520is%250Aundesirable%253B%2520more%2520original%2520or%2520unique%2520data%2520are%2520afforded%2520stronger%2520protection%252C%2520and%250Athe%2520threshold%2520level%2520of%2520resemblance%2520for%2520constituting%2520infringement%250Acorrespondingly%2520lower.%2520Here%252C%2520leveraging%2520this%2520principle%252C%2520we%2520propose%2520a%250Agenericization%2520method%2520that%2520modifies%2520the%2520outputs%2520of%2520a%2520generative%2520model%2520to%2520make%250Athem%2520more%2520generic%2520and%2520less%2520likely%2520to%2520infringe%2520copyright.%2520To%2520achieve%2520this%252C%2520we%250Aintroduce%2520a%2520metric%2520for%2520quantifying%2520the%2520level%2520of%2520originality%2520of%2520data%2520in%2520a%2520manner%250Athat%2520is%2520consistent%2520with%2520the%2520legal%2520framework.%2520This%2520metric%2520can%2520be%2520practically%250Aestimated%2520by%2520drawing%2520samples%2520from%2520a%2520generative%2520model%252C%2520which%2520is%2520then%2520used%2520for%250Athe%2520genericization%2520process.%2520Experiments%2520demonstrate%2520that%2520our%2520genericization%250Amethod%2520successfully%2520modifies%2520the%2520output%2520of%2520a%2520text-to-image%2520generative%2520model%2520so%250Athat%2520it%2520produces%2520more%2520generic%252C%2520copyright-compliant%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20GenAI%20Copyright%20Issues%3A%20Originality%20Estimation%20and%0A%20%20Genericization&entry.906535625=Hiroaki%20Chiba-Okabe%20and%20Weijie%20J.%20Su&entry.1292438233=%20%20The%20rapid%20progress%20of%20generative%20AI%20technology%20has%20sparked%20significant%0Acopyright%20concerns%2C%20leading%20to%20numerous%20lawsuits%20filed%20against%20AI%20developers.%0AWhile%20some%20studies%20explore%20methods%20to%20mitigate%20copyright%20risks%20by%20steering%20the%0Aoutputs%20of%20generative%20models%20away%20from%20those%20resembling%20copyrighted%20data%2C%0Alittle%20attention%20has%20been%20paid%20to%20the%20question%20of%20how%20much%20of%20a%20resemblance%20is%0Aundesirable%3B%20more%20original%20or%20unique%20data%20are%20afforded%20stronger%20protection%2C%20and%0Athe%20threshold%20level%20of%20resemblance%20for%20constituting%20infringement%0Acorrespondingly%20lower.%20Here%2C%20leveraging%20this%20principle%2C%20we%20propose%20a%0Agenericization%20method%20that%20modifies%20the%20outputs%20of%20a%20generative%20model%20to%20make%0Athem%20more%20generic%20and%20less%20likely%20to%20infringe%20copyright.%20To%20achieve%20this%2C%20we%0Aintroduce%20a%20metric%20for%20quantifying%20the%20level%20of%20originality%20of%20data%20in%20a%20manner%0Athat%20is%20consistent%20with%20the%20legal%20framework.%20This%20metric%20can%20be%20practically%0Aestimated%20by%20drawing%20samples%20from%20a%20generative%20model%2C%20which%20is%20then%20used%20for%0Athe%20genericization%20process.%20Experiments%20demonstrate%20that%20our%20genericization%0Amethod%20successfully%20modifies%20the%20output%20of%20a%20text-to-image%20generative%20model%20so%0Athat%20it%20produces%20more%20generic%2C%20copyright-compliant%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03341v1&entry.124074799=Read"},
{"title": "UnWave-Net: Unrolled Wavelet Network for Compton Tomography Image\n  Reconstruction", "author": "Ishak Ayad and C\u00e9cilia Tarpau and Javier Cebeiro and Ma\u00ef K. Nguyen", "abstract": "  Computed tomography (CT) is a widely used medical imaging technique to scan\ninternal structures of a body, typically involving collimation and mechanical\nrotation. Compton scatter tomography (CST) presents an interesting alternative\nto conventional CT by leveraging Compton physics instead of collimation to\ngather information from multiple directions. While CST introduces new imaging\nopportunities with several advantages such as high sensitivity, compactness,\nand entirely fixed systems, image reconstruction remains an open problem due to\nthe mathematical challenges of CST modeling. In contrast, deep unrolling\nnetworks have demonstrated potential in CT image reconstruction, despite their\ncomputationally intensive nature. In this study, we investigate the efficiency\nof unrolling networks for CST image reconstruction. To address the important\ncomputational cost required for training, we propose UnWave-Net, a novel\nunrolled wavelet-based reconstruction network. This architecture includes a\nnon-local regularization term based on wavelets, which captures long-range\ndependencies within images and emphasizes the multi-scale components of the\nwavelet transform. We evaluate our approach using a CST of circular geometry\nwhich stays completely static during data acquisition, where UnWave-Net\nfacilitates image reconstruction in the absence of a specific reconstruction\nformula. Our method outperforms existing approaches and achieves\nstate-of-the-art performance in terms of SSIM and PSNR, and offers an improved\ncomputational efficiency compared to traditional unrolling networks.\n", "link": "http://arxiv.org/abs/2406.03413v1", "date": "2024-06-05", "relevancy": 2.5616, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5186}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5186}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnWave-Net%3A%20Unrolled%20Wavelet%20Network%20for%20Compton%20Tomography%20Image%0A%20%20Reconstruction&body=Title%3A%20UnWave-Net%3A%20Unrolled%20Wavelet%20Network%20for%20Compton%20Tomography%20Image%0A%20%20Reconstruction%0AAuthor%3A%20Ishak%20Ayad%20and%20C%C3%A9cilia%20Tarpau%20and%20Javier%20Cebeiro%20and%20Ma%C3%AF%20K.%20Nguyen%0AAbstract%3A%20%20%20Computed%20tomography%20%28CT%29%20is%20a%20widely%20used%20medical%20imaging%20technique%20to%20scan%0Ainternal%20structures%20of%20a%20body%2C%20typically%20involving%20collimation%20and%20mechanical%0Arotation.%20Compton%20scatter%20tomography%20%28CST%29%20presents%20an%20interesting%20alternative%0Ato%20conventional%20CT%20by%20leveraging%20Compton%20physics%20instead%20of%20collimation%20to%0Agather%20information%20from%20multiple%20directions.%20While%20CST%20introduces%20new%20imaging%0Aopportunities%20with%20several%20advantages%20such%20as%20high%20sensitivity%2C%20compactness%2C%0Aand%20entirely%20fixed%20systems%2C%20image%20reconstruction%20remains%20an%20open%20problem%20due%20to%0Athe%20mathematical%20challenges%20of%20CST%20modeling.%20In%20contrast%2C%20deep%20unrolling%0Anetworks%20have%20demonstrated%20potential%20in%20CT%20image%20reconstruction%2C%20despite%20their%0Acomputationally%20intensive%20nature.%20In%20this%20study%2C%20we%20investigate%20the%20efficiency%0Aof%20unrolling%20networks%20for%20CST%20image%20reconstruction.%20To%20address%20the%20important%0Acomputational%20cost%20required%20for%20training%2C%20we%20propose%20UnWave-Net%2C%20a%20novel%0Aunrolled%20wavelet-based%20reconstruction%20network.%20This%20architecture%20includes%20a%0Anon-local%20regularization%20term%20based%20on%20wavelets%2C%20which%20captures%20long-range%0Adependencies%20within%20images%20and%20emphasizes%20the%20multi-scale%20components%20of%20the%0Awavelet%20transform.%20We%20evaluate%20our%20approach%20using%20a%20CST%20of%20circular%20geometry%0Awhich%20stays%20completely%20static%20during%20data%20acquisition%2C%20where%20UnWave-Net%0Afacilitates%20image%20reconstruction%20in%20the%20absence%20of%20a%20specific%20reconstruction%0Aformula.%20Our%20method%20outperforms%20existing%20approaches%20and%20achieves%0Astate-of-the-art%20performance%20in%20terms%20of%20SSIM%20and%20PSNR%2C%20and%20offers%20an%20improved%0Acomputational%20efficiency%20compared%20to%20traditional%20unrolling%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnWave-Net%253A%2520Unrolled%2520Wavelet%2520Network%2520for%2520Compton%2520Tomography%2520Image%250A%2520%2520Reconstruction%26entry.906535625%3DIshak%2520Ayad%2520and%2520C%25C3%25A9cilia%2520Tarpau%2520and%2520Javier%2520Cebeiro%2520and%2520Ma%25C3%25AF%2520K.%2520Nguyen%26entry.1292438233%3D%2520%2520Computed%2520tomography%2520%2528CT%2529%2520is%2520a%2520widely%2520used%2520medical%2520imaging%2520technique%2520to%2520scan%250Ainternal%2520structures%2520of%2520a%2520body%252C%2520typically%2520involving%2520collimation%2520and%2520mechanical%250Arotation.%2520Compton%2520scatter%2520tomography%2520%2528CST%2529%2520presents%2520an%2520interesting%2520alternative%250Ato%2520conventional%2520CT%2520by%2520leveraging%2520Compton%2520physics%2520instead%2520of%2520collimation%2520to%250Agather%2520information%2520from%2520multiple%2520directions.%2520While%2520CST%2520introduces%2520new%2520imaging%250Aopportunities%2520with%2520several%2520advantages%2520such%2520as%2520high%2520sensitivity%252C%2520compactness%252C%250Aand%2520entirely%2520fixed%2520systems%252C%2520image%2520reconstruction%2520remains%2520an%2520open%2520problem%2520due%2520to%250Athe%2520mathematical%2520challenges%2520of%2520CST%2520modeling.%2520In%2520contrast%252C%2520deep%2520unrolling%250Anetworks%2520have%2520demonstrated%2520potential%2520in%2520CT%2520image%2520reconstruction%252C%2520despite%2520their%250Acomputationally%2520intensive%2520nature.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520efficiency%250Aof%2520unrolling%2520networks%2520for%2520CST%2520image%2520reconstruction.%2520To%2520address%2520the%2520important%250Acomputational%2520cost%2520required%2520for%2520training%252C%2520we%2520propose%2520UnWave-Net%252C%2520a%2520novel%250Aunrolled%2520wavelet-based%2520reconstruction%2520network.%2520This%2520architecture%2520includes%2520a%250Anon-local%2520regularization%2520term%2520based%2520on%2520wavelets%252C%2520which%2520captures%2520long-range%250Adependencies%2520within%2520images%2520and%2520emphasizes%2520the%2520multi-scale%2520components%2520of%2520the%250Awavelet%2520transform.%2520We%2520evaluate%2520our%2520approach%2520using%2520a%2520CST%2520of%2520circular%2520geometry%250Awhich%2520stays%2520completely%2520static%2520during%2520data%2520acquisition%252C%2520where%2520UnWave-Net%250Afacilitates%2520image%2520reconstruction%2520in%2520the%2520absence%2520of%2520a%2520specific%2520reconstruction%250Aformula.%2520Our%2520method%2520outperforms%2520existing%2520approaches%2520and%2520achieves%250Astate-of-the-art%2520performance%2520in%2520terms%2520of%2520SSIM%2520and%2520PSNR%252C%2520and%2520offers%2520an%2520improved%250Acomputational%2520efficiency%2520compared%2520to%2520traditional%2520unrolling%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnWave-Net%3A%20Unrolled%20Wavelet%20Network%20for%20Compton%20Tomography%20Image%0A%20%20Reconstruction&entry.906535625=Ishak%20Ayad%20and%20C%C3%A9cilia%20Tarpau%20and%20Javier%20Cebeiro%20and%20Ma%C3%AF%20K.%20Nguyen&entry.1292438233=%20%20Computed%20tomography%20%28CT%29%20is%20a%20widely%20used%20medical%20imaging%20technique%20to%20scan%0Ainternal%20structures%20of%20a%20body%2C%20typically%20involving%20collimation%20and%20mechanical%0Arotation.%20Compton%20scatter%20tomography%20%28CST%29%20presents%20an%20interesting%20alternative%0Ato%20conventional%20CT%20by%20leveraging%20Compton%20physics%20instead%20of%20collimation%20to%0Agather%20information%20from%20multiple%20directions.%20While%20CST%20introduces%20new%20imaging%0Aopportunities%20with%20several%20advantages%20such%20as%20high%20sensitivity%2C%20compactness%2C%0Aand%20entirely%20fixed%20systems%2C%20image%20reconstruction%20remains%20an%20open%20problem%20due%20to%0Athe%20mathematical%20challenges%20of%20CST%20modeling.%20In%20contrast%2C%20deep%20unrolling%0Anetworks%20have%20demonstrated%20potential%20in%20CT%20image%20reconstruction%2C%20despite%20their%0Acomputationally%20intensive%20nature.%20In%20this%20study%2C%20we%20investigate%20the%20efficiency%0Aof%20unrolling%20networks%20for%20CST%20image%20reconstruction.%20To%20address%20the%20important%0Acomputational%20cost%20required%20for%20training%2C%20we%20propose%20UnWave-Net%2C%20a%20novel%0Aunrolled%20wavelet-based%20reconstruction%20network.%20This%20architecture%20includes%20a%0Anon-local%20regularization%20term%20based%20on%20wavelets%2C%20which%20captures%20long-range%0Adependencies%20within%20images%20and%20emphasizes%20the%20multi-scale%20components%20of%20the%0Awavelet%20transform.%20We%20evaluate%20our%20approach%20using%20a%20CST%20of%20circular%20geometry%0Awhich%20stays%20completely%20static%20during%20data%20acquisition%2C%20where%20UnWave-Net%0Afacilitates%20image%20reconstruction%20in%20the%20absence%20of%20a%20specific%20reconstruction%0Aformula.%20Our%20method%20outperforms%20existing%20approaches%20and%20achieves%0Astate-of-the-art%20performance%20in%20terms%20of%20SSIM%20and%20PSNR%2C%20and%20offers%20an%20improved%0Acomputational%20efficiency%20compared%20to%20traditional%20unrolling%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03413v1&entry.124074799=Read"},
{"title": "Grokfast: Accelerated Grokking by Amplifying Slow Gradients", "author": "Jaerin Lee and Bong Gyun Kang and Kihoon Kim and Kyoung Mu Lee", "abstract": "  One puzzling artifact in machine learning dubbed grokking is where delayed\ngeneralization is achieved tenfolds of iterations after near perfect\noverfitting to the training data. Focusing on the long delay itself on behalf\nof machine learning practitioners, our goal is to accelerate generalization of\na model under grokking phenomenon. By regarding a series of gradients of a\nparameter over training iterations as a random signal over time, we can\nspectrally decompose the parameter trajectories under gradient descent into two\ncomponents: the fast-varying, overfitting-yielding component and the\nslow-varying, generalization-inducing component. This analysis allows us to\naccelerate the grokking phenomenon more than $\\times 50$ with only a few lines\nof code that amplifies the slow-varying components of gradients. The\nexperiments show that our algorithm applies to diverse tasks involving images,\nlanguages, and graphs, enabling practical availability of this peculiar\nartifact of sudden generalization. Our code is available at\nhttps://github.com/ironjr/grokfast.\n", "link": "http://arxiv.org/abs/2405.20233v2", "date": "2024-06-05", "relevancy": 2.5615, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5246}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5077}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grokfast%3A%20Accelerated%20Grokking%20by%20Amplifying%20Slow%20Gradients&body=Title%3A%20Grokfast%3A%20Accelerated%20Grokking%20by%20Amplifying%20Slow%20Gradients%0AAuthor%3A%20Jaerin%20Lee%20and%20Bong%20Gyun%20Kang%20and%20Kihoon%20Kim%20and%20Kyoung%20Mu%20Lee%0AAbstract%3A%20%20%20One%20puzzling%20artifact%20in%20machine%20learning%20dubbed%20grokking%20is%20where%20delayed%0Ageneralization%20is%20achieved%20tenfolds%20of%20iterations%20after%20near%20perfect%0Aoverfitting%20to%20the%20training%20data.%20Focusing%20on%20the%20long%20delay%20itself%20on%20behalf%0Aof%20machine%20learning%20practitioners%2C%20our%20goal%20is%20to%20accelerate%20generalization%20of%0Aa%20model%20under%20grokking%20phenomenon.%20By%20regarding%20a%20series%20of%20gradients%20of%20a%0Aparameter%20over%20training%20iterations%20as%20a%20random%20signal%20over%20time%2C%20we%20can%0Aspectrally%20decompose%20the%20parameter%20trajectories%20under%20gradient%20descent%20into%20two%0Acomponents%3A%20the%20fast-varying%2C%20overfitting-yielding%20component%20and%20the%0Aslow-varying%2C%20generalization-inducing%20component.%20This%20analysis%20allows%20us%20to%0Aaccelerate%20the%20grokking%20phenomenon%20more%20than%20%24%5Ctimes%2050%24%20with%20only%20a%20few%20lines%0Aof%20code%20that%20amplifies%20the%20slow-varying%20components%20of%20gradients.%20The%0Aexperiments%20show%20that%20our%20algorithm%20applies%20to%20diverse%20tasks%20involving%20images%2C%0Alanguages%2C%20and%20graphs%2C%20enabling%20practical%20availability%20of%20this%20peculiar%0Aartifact%20of%20sudden%20generalization.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ironjr/grokfast.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20233v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrokfast%253A%2520Accelerated%2520Grokking%2520by%2520Amplifying%2520Slow%2520Gradients%26entry.906535625%3DJaerin%2520Lee%2520and%2520Bong%2520Gyun%2520Kang%2520and%2520Kihoon%2520Kim%2520and%2520Kyoung%2520Mu%2520Lee%26entry.1292438233%3D%2520%2520One%2520puzzling%2520artifact%2520in%2520machine%2520learning%2520dubbed%2520grokking%2520is%2520where%2520delayed%250Ageneralization%2520is%2520achieved%2520tenfolds%2520of%2520iterations%2520after%2520near%2520perfect%250Aoverfitting%2520to%2520the%2520training%2520data.%2520Focusing%2520on%2520the%2520long%2520delay%2520itself%2520on%2520behalf%250Aof%2520machine%2520learning%2520practitioners%252C%2520our%2520goal%2520is%2520to%2520accelerate%2520generalization%2520of%250Aa%2520model%2520under%2520grokking%2520phenomenon.%2520By%2520regarding%2520a%2520series%2520of%2520gradients%2520of%2520a%250Aparameter%2520over%2520training%2520iterations%2520as%2520a%2520random%2520signal%2520over%2520time%252C%2520we%2520can%250Aspectrally%2520decompose%2520the%2520parameter%2520trajectories%2520under%2520gradient%2520descent%2520into%2520two%250Acomponents%253A%2520the%2520fast-varying%252C%2520overfitting-yielding%2520component%2520and%2520the%250Aslow-varying%252C%2520generalization-inducing%2520component.%2520This%2520analysis%2520allows%2520us%2520to%250Aaccelerate%2520the%2520grokking%2520phenomenon%2520more%2520than%2520%2524%255Ctimes%252050%2524%2520with%2520only%2520a%2520few%2520lines%250Aof%2520code%2520that%2520amplifies%2520the%2520slow-varying%2520components%2520of%2520gradients.%2520The%250Aexperiments%2520show%2520that%2520our%2520algorithm%2520applies%2520to%2520diverse%2520tasks%2520involving%2520images%252C%250Alanguages%252C%2520and%2520graphs%252C%2520enabling%2520practical%2520availability%2520of%2520this%2520peculiar%250Aartifact%2520of%2520sudden%2520generalization.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ironjr/grokfast.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20233v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grokfast%3A%20Accelerated%20Grokking%20by%20Amplifying%20Slow%20Gradients&entry.906535625=Jaerin%20Lee%20and%20Bong%20Gyun%20Kang%20and%20Kihoon%20Kim%20and%20Kyoung%20Mu%20Lee&entry.1292438233=%20%20One%20puzzling%20artifact%20in%20machine%20learning%20dubbed%20grokking%20is%20where%20delayed%0Ageneralization%20is%20achieved%20tenfolds%20of%20iterations%20after%20near%20perfect%0Aoverfitting%20to%20the%20training%20data.%20Focusing%20on%20the%20long%20delay%20itself%20on%20behalf%0Aof%20machine%20learning%20practitioners%2C%20our%20goal%20is%20to%20accelerate%20generalization%20of%0Aa%20model%20under%20grokking%20phenomenon.%20By%20regarding%20a%20series%20of%20gradients%20of%20a%0Aparameter%20over%20training%20iterations%20as%20a%20random%20signal%20over%20time%2C%20we%20can%0Aspectrally%20decompose%20the%20parameter%20trajectories%20under%20gradient%20descent%20into%20two%0Acomponents%3A%20the%20fast-varying%2C%20overfitting-yielding%20component%20and%20the%0Aslow-varying%2C%20generalization-inducing%20component.%20This%20analysis%20allows%20us%20to%0Aaccelerate%20the%20grokking%20phenomenon%20more%20than%20%24%5Ctimes%2050%24%20with%20only%20a%20few%20lines%0Aof%20code%20that%20amplifies%20the%20slow-varying%20components%20of%20gradients.%20The%0Aexperiments%20show%20that%20our%20algorithm%20applies%20to%20diverse%20tasks%20involving%20images%2C%0Alanguages%2C%20and%20graphs%2C%20enabling%20practical%20availability%20of%20this%20peculiar%0Aartifact%20of%20sudden%20generalization.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ironjr/grokfast.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20233v2&entry.124074799=Read"},
{"title": "JumpCoder: Go Beyond Autoregressive Coder via Online Modification", "author": "Mouxiang Chen and Hao Tian and Zhongxin Liu and Xiaoxue Ren and Jianling Sun", "abstract": "  While existing code large language models (code LLMs) exhibit impressive\ncapabilities in code generation, their autoregressive sequential generation\ninherently lacks reversibility. This limitation hinders them from timely\ncorrecting previous missing statements during coding as humans do, often\nleading to error propagation and suboptimal performance. We introduce\nJumpCoder, a novel model-agnostic framework that enables human-like online\nmodification and non-sequential generation to augment code LLMs. The key idea\nbehind JumpCoder is to insert new code into the currently generated code when\nnecessary during generation, which is achieved through an auxiliary infilling\nmodel that works in tandem with the code LLM. Since identifying the best infill\nposition beforehand is intractable, we adopt an \\textit{infill-first,\njudge-later} strategy, which experiments with filling at the $k$ most critical\npositions following the generation of each line, and uses an Abstract Syntax\nTree (AST) parser alongside the Generation Model Scoring to effectively judge\nthe validity of each potential infill. Extensive experiments using six\nstate-of-the-art code LLMs across multiple and multilingual benchmarks\nconsistently indicate significant improvements over all baselines. Our code is\npublic at https://github.com/Keytoyze/JumpCoder.\n", "link": "http://arxiv.org/abs/2401.07870v2", "date": "2024-06-05", "relevancy": 2.5384, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5228}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5155}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JumpCoder%3A%20Go%20Beyond%20Autoregressive%20Coder%20via%20Online%20Modification&body=Title%3A%20JumpCoder%3A%20Go%20Beyond%20Autoregressive%20Coder%20via%20Online%20Modification%0AAuthor%3A%20Mouxiang%20Chen%20and%20Hao%20Tian%20and%20Zhongxin%20Liu%20and%20Xiaoxue%20Ren%20and%20Jianling%20Sun%0AAbstract%3A%20%20%20While%20existing%20code%20large%20language%20models%20%28code%20LLMs%29%20exhibit%20impressive%0Acapabilities%20in%20code%20generation%2C%20their%20autoregressive%20sequential%20generation%0Ainherently%20lacks%20reversibility.%20This%20limitation%20hinders%20them%20from%20timely%0Acorrecting%20previous%20missing%20statements%20during%20coding%20as%20humans%20do%2C%20often%0Aleading%20to%20error%20propagation%20and%20suboptimal%20performance.%20We%20introduce%0AJumpCoder%2C%20a%20novel%20model-agnostic%20framework%20that%20enables%20human-like%20online%0Amodification%20and%20non-sequential%20generation%20to%20augment%20code%20LLMs.%20The%20key%20idea%0Abehind%20JumpCoder%20is%20to%20insert%20new%20code%20into%20the%20currently%20generated%20code%20when%0Anecessary%20during%20generation%2C%20which%20is%20achieved%20through%20an%20auxiliary%20infilling%0Amodel%20that%20works%20in%20tandem%20with%20the%20code%20LLM.%20Since%20identifying%20the%20best%20infill%0Aposition%20beforehand%20is%20intractable%2C%20we%20adopt%20an%20%5Ctextit%7Binfill-first%2C%0Ajudge-later%7D%20strategy%2C%20which%20experiments%20with%20filling%20at%20the%20%24k%24%20most%20critical%0Apositions%20following%20the%20generation%20of%20each%20line%2C%20and%20uses%20an%20Abstract%20Syntax%0ATree%20%28AST%29%20parser%20alongside%20the%20Generation%20Model%20Scoring%20to%20effectively%20judge%0Athe%20validity%20of%20each%20potential%20infill.%20Extensive%20experiments%20using%20six%0Astate-of-the-art%20code%20LLMs%20across%20multiple%20and%20multilingual%20benchmarks%0Aconsistently%20indicate%20significant%20improvements%20over%20all%20baselines.%20Our%20code%20is%0Apublic%20at%20https%3A//github.com/Keytoyze/JumpCoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJumpCoder%253A%2520Go%2520Beyond%2520Autoregressive%2520Coder%2520via%2520Online%2520Modification%26entry.906535625%3DMouxiang%2520Chen%2520and%2520Hao%2520Tian%2520and%2520Zhongxin%2520Liu%2520and%2520Xiaoxue%2520Ren%2520and%2520Jianling%2520Sun%26entry.1292438233%3D%2520%2520While%2520existing%2520code%2520large%2520language%2520models%2520%2528code%2520LLMs%2529%2520exhibit%2520impressive%250Acapabilities%2520in%2520code%2520generation%252C%2520their%2520autoregressive%2520sequential%2520generation%250Ainherently%2520lacks%2520reversibility.%2520This%2520limitation%2520hinders%2520them%2520from%2520timely%250Acorrecting%2520previous%2520missing%2520statements%2520during%2520coding%2520as%2520humans%2520do%252C%2520often%250Aleading%2520to%2520error%2520propagation%2520and%2520suboptimal%2520performance.%2520We%2520introduce%250AJumpCoder%252C%2520a%2520novel%2520model-agnostic%2520framework%2520that%2520enables%2520human-like%2520online%250Amodification%2520and%2520non-sequential%2520generation%2520to%2520augment%2520code%2520LLMs.%2520The%2520key%2520idea%250Abehind%2520JumpCoder%2520is%2520to%2520insert%2520new%2520code%2520into%2520the%2520currently%2520generated%2520code%2520when%250Anecessary%2520during%2520generation%252C%2520which%2520is%2520achieved%2520through%2520an%2520auxiliary%2520infilling%250Amodel%2520that%2520works%2520in%2520tandem%2520with%2520the%2520code%2520LLM.%2520Since%2520identifying%2520the%2520best%2520infill%250Aposition%2520beforehand%2520is%2520intractable%252C%2520we%2520adopt%2520an%2520%255Ctextit%257Binfill-first%252C%250Ajudge-later%257D%2520strategy%252C%2520which%2520experiments%2520with%2520filling%2520at%2520the%2520%2524k%2524%2520most%2520critical%250Apositions%2520following%2520the%2520generation%2520of%2520each%2520line%252C%2520and%2520uses%2520an%2520Abstract%2520Syntax%250ATree%2520%2528AST%2529%2520parser%2520alongside%2520the%2520Generation%2520Model%2520Scoring%2520to%2520effectively%2520judge%250Athe%2520validity%2520of%2520each%2520potential%2520infill.%2520Extensive%2520experiments%2520using%2520six%250Astate-of-the-art%2520code%2520LLMs%2520across%2520multiple%2520and%2520multilingual%2520benchmarks%250Aconsistently%2520indicate%2520significant%2520improvements%2520over%2520all%2520baselines.%2520Our%2520code%2520is%250Apublic%2520at%2520https%253A//github.com/Keytoyze/JumpCoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JumpCoder%3A%20Go%20Beyond%20Autoregressive%20Coder%20via%20Online%20Modification&entry.906535625=Mouxiang%20Chen%20and%20Hao%20Tian%20and%20Zhongxin%20Liu%20and%20Xiaoxue%20Ren%20and%20Jianling%20Sun&entry.1292438233=%20%20While%20existing%20code%20large%20language%20models%20%28code%20LLMs%29%20exhibit%20impressive%0Acapabilities%20in%20code%20generation%2C%20their%20autoregressive%20sequential%20generation%0Ainherently%20lacks%20reversibility.%20This%20limitation%20hinders%20them%20from%20timely%0Acorrecting%20previous%20missing%20statements%20during%20coding%20as%20humans%20do%2C%20often%0Aleading%20to%20error%20propagation%20and%20suboptimal%20performance.%20We%20introduce%0AJumpCoder%2C%20a%20novel%20model-agnostic%20framework%20that%20enables%20human-like%20online%0Amodification%20and%20non-sequential%20generation%20to%20augment%20code%20LLMs.%20The%20key%20idea%0Abehind%20JumpCoder%20is%20to%20insert%20new%20code%20into%20the%20currently%20generated%20code%20when%0Anecessary%20during%20generation%2C%20which%20is%20achieved%20through%20an%20auxiliary%20infilling%0Amodel%20that%20works%20in%20tandem%20with%20the%20code%20LLM.%20Since%20identifying%20the%20best%20infill%0Aposition%20beforehand%20is%20intractable%2C%20we%20adopt%20an%20%5Ctextit%7Binfill-first%2C%0Ajudge-later%7D%20strategy%2C%20which%20experiments%20with%20filling%20at%20the%20%24k%24%20most%20critical%0Apositions%20following%20the%20generation%20of%20each%20line%2C%20and%20uses%20an%20Abstract%20Syntax%0ATree%20%28AST%29%20parser%20alongside%20the%20Generation%20Model%20Scoring%20to%20effectively%20judge%0Athe%20validity%20of%20each%20potential%20infill.%20Extensive%20experiments%20using%20six%0Astate-of-the-art%20code%20LLMs%20across%20multiple%20and%20multilingual%20benchmarks%0Aconsistently%20indicate%20significant%20improvements%20over%20all%20baselines.%20Our%20code%20is%0Apublic%20at%20https%3A//github.com/Keytoyze/JumpCoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07870v2&entry.124074799=Read"},
{"title": "CoFie: Learning Compact Neural Surface Representations with Coordinate\n  Fields", "author": "Hanwen Jiang and Haitao Yang and Georgios Pavlakos and Qixing Huang", "abstract": "  This paper introduces CoFie, a novel local geometry-aware neural surface\nrepresentation. CoFie is motivated by the theoretical analysis of local SDFs\nwith quadratic approximation. We find that local shapes are highly compressive\nin an aligned coordinate frame defined by the normal and tangent directions of\nlocal shapes. Accordingly, we introduce Coordinate Field, which is a\ncomposition of coordinate frames of all local shapes. The Coordinate Field is\noptimizable and is used to transform the local shapes from the world coordinate\nframe to the aligned shape coordinate frame. It largely reduces the complexity\nof local shapes and benefits the learning of MLP-based implicit\nrepresentations. Moreover, we introduce quadratic layers into the MLP to\nenhance expressiveness concerning local shape geometry. CoFie is a\ngeneralizable surface representation. It is trained on a curated set of 3D\nshapes and works on novel shape instances during testing. When using the same\namount of parameters with prior works, CoFie reduces the shape error by 48% and\n56% on novel instances of both training and unseen shape categories. Moreover,\nCoFie demonstrates comparable performance to prior works when using only 70%\nfewer parameters.\n", "link": "http://arxiv.org/abs/2406.03417v1", "date": "2024-06-05", "relevancy": 2.5354, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5032}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoFie%3A%20Learning%20Compact%20Neural%20Surface%20Representations%20with%20Coordinate%0A%20%20Fields&body=Title%3A%20CoFie%3A%20Learning%20Compact%20Neural%20Surface%20Representations%20with%20Coordinate%0A%20%20Fields%0AAuthor%3A%20Hanwen%20Jiang%20and%20Haitao%20Yang%20and%20Georgios%20Pavlakos%20and%20Qixing%20Huang%0AAbstract%3A%20%20%20This%20paper%20introduces%20CoFie%2C%20a%20novel%20local%20geometry-aware%20neural%20surface%0Arepresentation.%20CoFie%20is%20motivated%20by%20the%20theoretical%20analysis%20of%20local%20SDFs%0Awith%20quadratic%20approximation.%20We%20find%20that%20local%20shapes%20are%20highly%20compressive%0Ain%20an%20aligned%20coordinate%20frame%20defined%20by%20the%20normal%20and%20tangent%20directions%20of%0Alocal%20shapes.%20Accordingly%2C%20we%20introduce%20Coordinate%20Field%2C%20which%20is%20a%0Acomposition%20of%20coordinate%20frames%20of%20all%20local%20shapes.%20The%20Coordinate%20Field%20is%0Aoptimizable%20and%20is%20used%20to%20transform%20the%20local%20shapes%20from%20the%20world%20coordinate%0Aframe%20to%20the%20aligned%20shape%20coordinate%20frame.%20It%20largely%20reduces%20the%20complexity%0Aof%20local%20shapes%20and%20benefits%20the%20learning%20of%20MLP-based%20implicit%0Arepresentations.%20Moreover%2C%20we%20introduce%20quadratic%20layers%20into%20the%20MLP%20to%0Aenhance%20expressiveness%20concerning%20local%20shape%20geometry.%20CoFie%20is%20a%0Ageneralizable%20surface%20representation.%20It%20is%20trained%20on%20a%20curated%20set%20of%203D%0Ashapes%20and%20works%20on%20novel%20shape%20instances%20during%20testing.%20When%20using%20the%20same%0Aamount%20of%20parameters%20with%20prior%20works%2C%20CoFie%20reduces%20the%20shape%20error%20by%2048%25%20and%0A56%25%20on%20novel%20instances%20of%20both%20training%20and%20unseen%20shape%20categories.%20Moreover%2C%0ACoFie%20demonstrates%20comparable%20performance%20to%20prior%20works%20when%20using%20only%2070%25%0Afewer%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoFie%253A%2520Learning%2520Compact%2520Neural%2520Surface%2520Representations%2520with%2520Coordinate%250A%2520%2520Fields%26entry.906535625%3DHanwen%2520Jiang%2520and%2520Haitao%2520Yang%2520and%2520Georgios%2520Pavlakos%2520and%2520Qixing%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520CoFie%252C%2520a%2520novel%2520local%2520geometry-aware%2520neural%2520surface%250Arepresentation.%2520CoFie%2520is%2520motivated%2520by%2520the%2520theoretical%2520analysis%2520of%2520local%2520SDFs%250Awith%2520quadratic%2520approximation.%2520We%2520find%2520that%2520local%2520shapes%2520are%2520highly%2520compressive%250Ain%2520an%2520aligned%2520coordinate%2520frame%2520defined%2520by%2520the%2520normal%2520and%2520tangent%2520directions%2520of%250Alocal%2520shapes.%2520Accordingly%252C%2520we%2520introduce%2520Coordinate%2520Field%252C%2520which%2520is%2520a%250Acomposition%2520of%2520coordinate%2520frames%2520of%2520all%2520local%2520shapes.%2520The%2520Coordinate%2520Field%2520is%250Aoptimizable%2520and%2520is%2520used%2520to%2520transform%2520the%2520local%2520shapes%2520from%2520the%2520world%2520coordinate%250Aframe%2520to%2520the%2520aligned%2520shape%2520coordinate%2520frame.%2520It%2520largely%2520reduces%2520the%2520complexity%250Aof%2520local%2520shapes%2520and%2520benefits%2520the%2520learning%2520of%2520MLP-based%2520implicit%250Arepresentations.%2520Moreover%252C%2520we%2520introduce%2520quadratic%2520layers%2520into%2520the%2520MLP%2520to%250Aenhance%2520expressiveness%2520concerning%2520local%2520shape%2520geometry.%2520CoFie%2520is%2520a%250Ageneralizable%2520surface%2520representation.%2520It%2520is%2520trained%2520on%2520a%2520curated%2520set%2520of%25203D%250Ashapes%2520and%2520works%2520on%2520novel%2520shape%2520instances%2520during%2520testing.%2520When%2520using%2520the%2520same%250Aamount%2520of%2520parameters%2520with%2520prior%2520works%252C%2520CoFie%2520reduces%2520the%2520shape%2520error%2520by%252048%2525%2520and%250A56%2525%2520on%2520novel%2520instances%2520of%2520both%2520training%2520and%2520unseen%2520shape%2520categories.%2520Moreover%252C%250ACoFie%2520demonstrates%2520comparable%2520performance%2520to%2520prior%2520works%2520when%2520using%2520only%252070%2525%250Afewer%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoFie%3A%20Learning%20Compact%20Neural%20Surface%20Representations%20with%20Coordinate%0A%20%20Fields&entry.906535625=Hanwen%20Jiang%20and%20Haitao%20Yang%20and%20Georgios%20Pavlakos%20and%20Qixing%20Huang&entry.1292438233=%20%20This%20paper%20introduces%20CoFie%2C%20a%20novel%20local%20geometry-aware%20neural%20surface%0Arepresentation.%20CoFie%20is%20motivated%20by%20the%20theoretical%20analysis%20of%20local%20SDFs%0Awith%20quadratic%20approximation.%20We%20find%20that%20local%20shapes%20are%20highly%20compressive%0Ain%20an%20aligned%20coordinate%20frame%20defined%20by%20the%20normal%20and%20tangent%20directions%20of%0Alocal%20shapes.%20Accordingly%2C%20we%20introduce%20Coordinate%20Field%2C%20which%20is%20a%0Acomposition%20of%20coordinate%20frames%20of%20all%20local%20shapes.%20The%20Coordinate%20Field%20is%0Aoptimizable%20and%20is%20used%20to%20transform%20the%20local%20shapes%20from%20the%20world%20coordinate%0Aframe%20to%20the%20aligned%20shape%20coordinate%20frame.%20It%20largely%20reduces%20the%20complexity%0Aof%20local%20shapes%20and%20benefits%20the%20learning%20of%20MLP-based%20implicit%0Arepresentations.%20Moreover%2C%20we%20introduce%20quadratic%20layers%20into%20the%20MLP%20to%0Aenhance%20expressiveness%20concerning%20local%20shape%20geometry.%20CoFie%20is%20a%0Ageneralizable%20surface%20representation.%20It%20is%20trained%20on%20a%20curated%20set%20of%203D%0Ashapes%20and%20works%20on%20novel%20shape%20instances%20during%20testing.%20When%20using%20the%20same%0Aamount%20of%20parameters%20with%20prior%20works%2C%20CoFie%20reduces%20the%20shape%20error%20by%2048%25%20and%0A56%25%20on%20novel%20instances%20of%20both%20training%20and%20unseen%20shape%20categories.%20Moreover%2C%0ACoFie%20demonstrates%20comparable%20performance%20to%20prior%20works%20when%20using%20only%2070%25%0Afewer%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03417v1&entry.124074799=Read"},
{"title": "Self-Augmented In-Context Learning for Unsupervised Word Translation", "author": "Yaoyiran Li and Anna Korhonen and Ivan Vuli\u0107", "abstract": "  Recent work has shown that, while large language models (LLMs) demonstrate\nstrong word translation or bilingual lexicon induction (BLI) capabilities in\nfew-shot setups, they still cannot match the performance of 'traditional'\nmapping-based approaches in the unsupervised scenario where no seed translation\npairs are available, especially for lower-resource languages. To address this\nchallenge with LLMs, we propose self-augmented in-context learning (SAIL) for\nunsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a\nset of high-confidence word translation pairs for in-context learning (ICL)\nfrom an LLM, which it then reapplies to the same LLM in the ICL fashion. Our\nmethod shows substantial gains over zero-shot prompting of LLMs on two\nestablished BLI benchmarks spanning a wide range of language pairs, also\noutperforming mapping-based baselines across the board. In addition to\nachieving state-of-the-art unsupervised BLI performance, we also conduct\ncomprehensive analyses on SAIL and discuss its limitations.\n", "link": "http://arxiv.org/abs/2402.10024v2", "date": "2024-06-05", "relevancy": 2.5174, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5328}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5091}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Augmented%20In-Context%20Learning%20for%20Unsupervised%20Word%20Translation&body=Title%3A%20Self-Augmented%20In-Context%20Learning%20for%20Unsupervised%20Word%20Translation%0AAuthor%3A%20Yaoyiran%20Li%20and%20Anna%20Korhonen%20and%20Ivan%20Vuli%C4%87%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20that%2C%20while%20large%20language%20models%20%28LLMs%29%20demonstrate%0Astrong%20word%20translation%20or%20bilingual%20lexicon%20induction%20%28BLI%29%20capabilities%20in%0Afew-shot%20setups%2C%20they%20still%20cannot%20match%20the%20performance%20of%20%27traditional%27%0Amapping-based%20approaches%20in%20the%20unsupervised%20scenario%20where%20no%20seed%20translation%0Apairs%20are%20available%2C%20especially%20for%20lower-resource%20languages.%20To%20address%20this%0Achallenge%20with%20LLMs%2C%20we%20propose%20self-augmented%20in-context%20learning%20%28SAIL%29%20for%0Aunsupervised%20BLI%3A%20starting%20from%20a%20zero-shot%20prompt%2C%20SAIL%20iteratively%20induces%20a%0Aset%20of%20high-confidence%20word%20translation%20pairs%20for%20in-context%20learning%20%28ICL%29%0Afrom%20an%20LLM%2C%20which%20it%20then%20reapplies%20to%20the%20same%20LLM%20in%20the%20ICL%20fashion.%20Our%0Amethod%20shows%20substantial%20gains%20over%20zero-shot%20prompting%20of%20LLMs%20on%20two%0Aestablished%20BLI%20benchmarks%20spanning%20a%20wide%20range%20of%20language%20pairs%2C%20also%0Aoutperforming%20mapping-based%20baselines%20across%20the%20board.%20In%20addition%20to%0Aachieving%20state-of-the-art%20unsupervised%20BLI%20performance%2C%20we%20also%20conduct%0Acomprehensive%20analyses%20on%20SAIL%20and%20discuss%20its%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Augmented%2520In-Context%2520Learning%2520for%2520Unsupervised%2520Word%2520Translation%26entry.906535625%3DYaoyiran%2520Li%2520and%2520Anna%2520Korhonen%2520and%2520Ivan%2520Vuli%25C4%2587%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520that%252C%2520while%2520large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%250Astrong%2520word%2520translation%2520or%2520bilingual%2520lexicon%2520induction%2520%2528BLI%2529%2520capabilities%2520in%250Afew-shot%2520setups%252C%2520they%2520still%2520cannot%2520match%2520the%2520performance%2520of%2520%2527traditional%2527%250Amapping-based%2520approaches%2520in%2520the%2520unsupervised%2520scenario%2520where%2520no%2520seed%2520translation%250Apairs%2520are%2520available%252C%2520especially%2520for%2520lower-resource%2520languages.%2520To%2520address%2520this%250Achallenge%2520with%2520LLMs%252C%2520we%2520propose%2520self-augmented%2520in-context%2520learning%2520%2528SAIL%2529%2520for%250Aunsupervised%2520BLI%253A%2520starting%2520from%2520a%2520zero-shot%2520prompt%252C%2520SAIL%2520iteratively%2520induces%2520a%250Aset%2520of%2520high-confidence%2520word%2520translation%2520pairs%2520for%2520in-context%2520learning%2520%2528ICL%2529%250Afrom%2520an%2520LLM%252C%2520which%2520it%2520then%2520reapplies%2520to%2520the%2520same%2520LLM%2520in%2520the%2520ICL%2520fashion.%2520Our%250Amethod%2520shows%2520substantial%2520gains%2520over%2520zero-shot%2520prompting%2520of%2520LLMs%2520on%2520two%250Aestablished%2520BLI%2520benchmarks%2520spanning%2520a%2520wide%2520range%2520of%2520language%2520pairs%252C%2520also%250Aoutperforming%2520mapping-based%2520baselines%2520across%2520the%2520board.%2520In%2520addition%2520to%250Aachieving%2520state-of-the-art%2520unsupervised%2520BLI%2520performance%252C%2520we%2520also%2520conduct%250Acomprehensive%2520analyses%2520on%2520SAIL%2520and%2520discuss%2520its%2520limitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Augmented%20In-Context%20Learning%20for%20Unsupervised%20Word%20Translation&entry.906535625=Yaoyiran%20Li%20and%20Anna%20Korhonen%20and%20Ivan%20Vuli%C4%87&entry.1292438233=%20%20Recent%20work%20has%20shown%20that%2C%20while%20large%20language%20models%20%28LLMs%29%20demonstrate%0Astrong%20word%20translation%20or%20bilingual%20lexicon%20induction%20%28BLI%29%20capabilities%20in%0Afew-shot%20setups%2C%20they%20still%20cannot%20match%20the%20performance%20of%20%27traditional%27%0Amapping-based%20approaches%20in%20the%20unsupervised%20scenario%20where%20no%20seed%20translation%0Apairs%20are%20available%2C%20especially%20for%20lower-resource%20languages.%20To%20address%20this%0Achallenge%20with%20LLMs%2C%20we%20propose%20self-augmented%20in-context%20learning%20%28SAIL%29%20for%0Aunsupervised%20BLI%3A%20starting%20from%20a%20zero-shot%20prompt%2C%20SAIL%20iteratively%20induces%20a%0Aset%20of%20high-confidence%20word%20translation%20pairs%20for%20in-context%20learning%20%28ICL%29%0Afrom%20an%20LLM%2C%20which%20it%20then%20reapplies%20to%20the%20same%20LLM%20in%20the%20ICL%20fashion.%20Our%0Amethod%20shows%20substantial%20gains%20over%20zero-shot%20prompting%20of%20LLMs%20on%20two%0Aestablished%20BLI%20benchmarks%20spanning%20a%20wide%20range%20of%20language%20pairs%2C%20also%0Aoutperforming%20mapping-based%20baselines%20across%20the%20board.%20In%20addition%20to%0Aachieving%20state-of-the-art%20unsupervised%20BLI%20performance%2C%20we%20also%20conduct%0Acomprehensive%20analyses%20on%20SAIL%20and%20discuss%20its%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10024v2&entry.124074799=Read"},
{"title": "PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting", "author": "Qiaowei Miao and Yawei Luo and Yi Yang", "abstract": "  As text-conditioned diffusion models (DMs) achieve breakthroughs in image,\nvideo, and 3D generation, the research community's focus has shifted to the\nmore challenging task of text-to-4D synthesis, which introduces a temporal\ndimension to generate dynamic 3D objects. In this context, we identify Score\nDistillation Sampling (SDS), a widely used technique for text-to-3D synthesis,\nas a significant hindrance to text-to-4D performance due to its Janus-faced and\ntexture-unrealistic problems coupled with high computational costs. In this\npaper, we propose \\textbf{P}ixel-\\textbf{L}evel \\textbf{A}lignments for\nText-to-\\textbf{4D} Gaussian Splatting (\\textbf{PLA4D}), a novel method that\nutilizes text-to-video frames as explicit pixel alignment targets to generate\nstatic 3D objects and inject motion into them. Specifically, we introduce Focal\nAlignment to calibrate camera poses for rendering and GS-Mesh Contrastive\nLearning to distill geometry priors from rendered image contrasts at the pixel\nlevel. Additionally, we develop Motion Alignment using a deformation network to\ndrive changes in Gaussians and implement Reference Refinement for smooth 4D\nobject surfaces. These techniques enable 4D Gaussian Splatting to align\ngeometry, texture, and motion with generated videos at the pixel level.\nCompared to previous methods, PLA4D produces synthesized outputs with better\ntexture details in less time and effectively mitigates the Janus-faced problem.\nPLA4D is fully implemented using open-source models, offering an accessible,\nuser-friendly, and promising direction for 4D digital content creation. Our\nproject page: https://miaoqiaowei.github.io/PLA4D/.\n", "link": "http://arxiv.org/abs/2405.19957v3", "date": "2024-06-05", "relevancy": 2.5004, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6378}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6306}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLA4D%3A%20Pixel-Level%20Alignments%20for%20Text-to-4D%20Gaussian%20Splatting&body=Title%3A%20PLA4D%3A%20Pixel-Level%20Alignments%20for%20Text-to-4D%20Gaussian%20Splatting%0AAuthor%3A%20Qiaowei%20Miao%20and%20Yawei%20Luo%20and%20Yi%20Yang%0AAbstract%3A%20%20%20As%20text-conditioned%20diffusion%20models%20%28DMs%29%20achieve%20breakthroughs%20in%20image%2C%0Avideo%2C%20and%203D%20generation%2C%20the%20research%20community%27s%20focus%20has%20shifted%20to%20the%0Amore%20challenging%20task%20of%20text-to-4D%20synthesis%2C%20which%20introduces%20a%20temporal%0Adimension%20to%20generate%20dynamic%203D%20objects.%20In%20this%20context%2C%20we%20identify%20Score%0ADistillation%20Sampling%20%28SDS%29%2C%20a%20widely%20used%20technique%20for%20text-to-3D%20synthesis%2C%0Aas%20a%20significant%20hindrance%20to%20text-to-4D%20performance%20due%20to%20its%20Janus-faced%20and%0Atexture-unrealistic%20problems%20coupled%20with%20high%20computational%20costs.%20In%20this%0Apaper%2C%20we%20propose%20%5Ctextbf%7BP%7Dixel-%5Ctextbf%7BL%7Devel%20%5Ctextbf%7BA%7Dlignments%20for%0AText-to-%5Ctextbf%7B4D%7D%20Gaussian%20Splatting%20%28%5Ctextbf%7BPLA4D%7D%29%2C%20a%20novel%20method%20that%0Autilizes%20text-to-video%20frames%20as%20explicit%20pixel%20alignment%20targets%20to%20generate%0Astatic%203D%20objects%20and%20inject%20motion%20into%20them.%20Specifically%2C%20we%20introduce%20Focal%0AAlignment%20to%20calibrate%20camera%20poses%20for%20rendering%20and%20GS-Mesh%20Contrastive%0ALearning%20to%20distill%20geometry%20priors%20from%20rendered%20image%20contrasts%20at%20the%20pixel%0Alevel.%20Additionally%2C%20we%20develop%20Motion%20Alignment%20using%20a%20deformation%20network%20to%0Adrive%20changes%20in%20Gaussians%20and%20implement%20Reference%20Refinement%20for%20smooth%204D%0Aobject%20surfaces.%20These%20techniques%20enable%204D%20Gaussian%20Splatting%20to%20align%0Ageometry%2C%20texture%2C%20and%20motion%20with%20generated%20videos%20at%20the%20pixel%20level.%0ACompared%20to%20previous%20methods%2C%20PLA4D%20produces%20synthesized%20outputs%20with%20better%0Atexture%20details%20in%20less%20time%20and%20effectively%20mitigates%20the%20Janus-faced%20problem.%0APLA4D%20is%20fully%20implemented%20using%20open-source%20models%2C%20offering%20an%20accessible%2C%0Auser-friendly%2C%20and%20promising%20direction%20for%204D%20digital%20content%20creation.%20Our%0Aproject%20page%3A%20https%3A//miaoqiaowei.github.io/PLA4D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLA4D%253A%2520Pixel-Level%2520Alignments%2520for%2520Text-to-4D%2520Gaussian%2520Splatting%26entry.906535625%3DQiaowei%2520Miao%2520and%2520Yawei%2520Luo%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520As%2520text-conditioned%2520diffusion%2520models%2520%2528DMs%2529%2520achieve%2520breakthroughs%2520in%2520image%252C%250Avideo%252C%2520and%25203D%2520generation%252C%2520the%2520research%2520community%2527s%2520focus%2520has%2520shifted%2520to%2520the%250Amore%2520challenging%2520task%2520of%2520text-to-4D%2520synthesis%252C%2520which%2520introduces%2520a%2520temporal%250Adimension%2520to%2520generate%2520dynamic%25203D%2520objects.%2520In%2520this%2520context%252C%2520we%2520identify%2520Score%250ADistillation%2520Sampling%2520%2528SDS%2529%252C%2520a%2520widely%2520used%2520technique%2520for%2520text-to-3D%2520synthesis%252C%250Aas%2520a%2520significant%2520hindrance%2520to%2520text-to-4D%2520performance%2520due%2520to%2520its%2520Janus-faced%2520and%250Atexture-unrealistic%2520problems%2520coupled%2520with%2520high%2520computational%2520costs.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520%255Ctextbf%257BP%257Dixel-%255Ctextbf%257BL%257Devel%2520%255Ctextbf%257BA%257Dlignments%2520for%250AText-to-%255Ctextbf%257B4D%257D%2520Gaussian%2520Splatting%2520%2528%255Ctextbf%257BPLA4D%257D%2529%252C%2520a%2520novel%2520method%2520that%250Autilizes%2520text-to-video%2520frames%2520as%2520explicit%2520pixel%2520alignment%2520targets%2520to%2520generate%250Astatic%25203D%2520objects%2520and%2520inject%2520motion%2520into%2520them.%2520Specifically%252C%2520we%2520introduce%2520Focal%250AAlignment%2520to%2520calibrate%2520camera%2520poses%2520for%2520rendering%2520and%2520GS-Mesh%2520Contrastive%250ALearning%2520to%2520distill%2520geometry%2520priors%2520from%2520rendered%2520image%2520contrasts%2520at%2520the%2520pixel%250Alevel.%2520Additionally%252C%2520we%2520develop%2520Motion%2520Alignment%2520using%2520a%2520deformation%2520network%2520to%250Adrive%2520changes%2520in%2520Gaussians%2520and%2520implement%2520Reference%2520Refinement%2520for%2520smooth%25204D%250Aobject%2520surfaces.%2520These%2520techniques%2520enable%25204D%2520Gaussian%2520Splatting%2520to%2520align%250Ageometry%252C%2520texture%252C%2520and%2520motion%2520with%2520generated%2520videos%2520at%2520the%2520pixel%2520level.%250ACompared%2520to%2520previous%2520methods%252C%2520PLA4D%2520produces%2520synthesized%2520outputs%2520with%2520better%250Atexture%2520details%2520in%2520less%2520time%2520and%2520effectively%2520mitigates%2520the%2520Janus-faced%2520problem.%250APLA4D%2520is%2520fully%2520implemented%2520using%2520open-source%2520models%252C%2520offering%2520an%2520accessible%252C%250Auser-friendly%252C%2520and%2520promising%2520direction%2520for%25204D%2520digital%2520content%2520creation.%2520Our%250Aproject%2520page%253A%2520https%253A//miaoqiaowei.github.io/PLA4D/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLA4D%3A%20Pixel-Level%20Alignments%20for%20Text-to-4D%20Gaussian%20Splatting&entry.906535625=Qiaowei%20Miao%20and%20Yawei%20Luo%20and%20Yi%20Yang&entry.1292438233=%20%20As%20text-conditioned%20diffusion%20models%20%28DMs%29%20achieve%20breakthroughs%20in%20image%2C%0Avideo%2C%20and%203D%20generation%2C%20the%20research%20community%27s%20focus%20has%20shifted%20to%20the%0Amore%20challenging%20task%20of%20text-to-4D%20synthesis%2C%20which%20introduces%20a%20temporal%0Adimension%20to%20generate%20dynamic%203D%20objects.%20In%20this%20context%2C%20we%20identify%20Score%0ADistillation%20Sampling%20%28SDS%29%2C%20a%20widely%20used%20technique%20for%20text-to-3D%20synthesis%2C%0Aas%20a%20significant%20hindrance%20to%20text-to-4D%20performance%20due%20to%20its%20Janus-faced%20and%0Atexture-unrealistic%20problems%20coupled%20with%20high%20computational%20costs.%20In%20this%0Apaper%2C%20we%20propose%20%5Ctextbf%7BP%7Dixel-%5Ctextbf%7BL%7Devel%20%5Ctextbf%7BA%7Dlignments%20for%0AText-to-%5Ctextbf%7B4D%7D%20Gaussian%20Splatting%20%28%5Ctextbf%7BPLA4D%7D%29%2C%20a%20novel%20method%20that%0Autilizes%20text-to-video%20frames%20as%20explicit%20pixel%20alignment%20targets%20to%20generate%0Astatic%203D%20objects%20and%20inject%20motion%20into%20them.%20Specifically%2C%20we%20introduce%20Focal%0AAlignment%20to%20calibrate%20camera%20poses%20for%20rendering%20and%20GS-Mesh%20Contrastive%0ALearning%20to%20distill%20geometry%20priors%20from%20rendered%20image%20contrasts%20at%20the%20pixel%0Alevel.%20Additionally%2C%20we%20develop%20Motion%20Alignment%20using%20a%20deformation%20network%20to%0Adrive%20changes%20in%20Gaussians%20and%20implement%20Reference%20Refinement%20for%20smooth%204D%0Aobject%20surfaces.%20These%20techniques%20enable%204D%20Gaussian%20Splatting%20to%20align%0Ageometry%2C%20texture%2C%20and%20motion%20with%20generated%20videos%20at%20the%20pixel%20level.%0ACompared%20to%20previous%20methods%2C%20PLA4D%20produces%20synthesized%20outputs%20with%20better%0Atexture%20details%20in%20less%20time%20and%20effectively%20mitigates%20the%20Janus-faced%20problem.%0APLA4D%20is%20fully%20implemented%20using%20open-source%20models%2C%20offering%20an%20accessible%2C%0Auser-friendly%2C%20and%20promising%20direction%20for%204D%20digital%20content%20creation.%20Our%0Aproject%20page%3A%20https%3A//miaoqiaowei.github.io/PLA4D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19957v3&entry.124074799=Read"},
{"title": "Effects of Exponential Gaussian Distribution on (Double Sampling)\n  Randomized Smoothing", "author": "Youwei Shu and Xi Xiao and Derui Wang and Yuxin Cao and Siji Chen and Jason Xue and Linyi Li and Bo Li", "abstract": "  Randomized Smoothing (RS) is currently a scalable certified defense method\nproviding robustness certification against adversarial examples. Although\nsignificant progress has been achieved in providing defenses against $\\ell_p$\nadversaries, the interaction between the smoothing distribution and the\nrobustness certification still remains vague. In this work, we comprehensively\nstudy the effect of two families of distributions, named Exponential Standard\nGaussian (ESG) and Exponential General Gaussian (EGG) distributions, on\nRandomized Smoothing and Double Sampling Randomized Smoothing (DSRS). We derive\nan analytic formula for ESG's certified radius, which converges to the origin\nformula of RS as the dimension $d$ increases. Additionally, we prove that EGG\ncan provide tighter constant factors than DSRS in providing $\\Omega(\\sqrt{d})$\nlower bounds of $\\ell_2$ certified radius, and thus further addresses the curse\nof dimensionality in RS. Our experiments on real-world datasets confirm our\ntheoretical analysis of the ESG distributions, that they provide almost the\nsame certification under different exponents $\\eta$ for both RS and DSRS. In\naddition, EGG brings a significant improvement to the DSRS certification, but\nthe mechanism can be different when the classifier properties are different.\nCompared to the primitive DSRS, the increase in certified accuracy provided by\nEGG is prominent, up to 6.4% on ImageNet.\n", "link": "http://arxiv.org/abs/2406.02309v2", "date": "2024-06-05", "relevancy": 2.4964, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5152}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5055}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effects%20of%20Exponential%20Gaussian%20Distribution%20on%20%28Double%20Sampling%29%0A%20%20Randomized%20Smoothing&body=Title%3A%20Effects%20of%20Exponential%20Gaussian%20Distribution%20on%20%28Double%20Sampling%29%0A%20%20Randomized%20Smoothing%0AAuthor%3A%20Youwei%20Shu%20and%20Xi%20Xiao%20and%20Derui%20Wang%20and%20Yuxin%20Cao%20and%20Siji%20Chen%20and%20Jason%20Xue%20and%20Linyi%20Li%20and%20Bo%20Li%0AAbstract%3A%20%20%20Randomized%20Smoothing%20%28RS%29%20is%20currently%20a%20scalable%20certified%20defense%20method%0Aproviding%20robustness%20certification%20against%20adversarial%20examples.%20Although%0Asignificant%20progress%20has%20been%20achieved%20in%20providing%20defenses%20against%20%24%5Cell_p%24%0Aadversaries%2C%20the%20interaction%20between%20the%20smoothing%20distribution%20and%20the%0Arobustness%20certification%20still%20remains%20vague.%20In%20this%20work%2C%20we%20comprehensively%0Astudy%20the%20effect%20of%20two%20families%20of%20distributions%2C%20named%20Exponential%20Standard%0AGaussian%20%28ESG%29%20and%20Exponential%20General%20Gaussian%20%28EGG%29%20distributions%2C%20on%0ARandomized%20Smoothing%20and%20Double%20Sampling%20Randomized%20Smoothing%20%28DSRS%29.%20We%20derive%0Aan%20analytic%20formula%20for%20ESG%27s%20certified%20radius%2C%20which%20converges%20to%20the%20origin%0Aformula%20of%20RS%20as%20the%20dimension%20%24d%24%20increases.%20Additionally%2C%20we%20prove%20that%20EGG%0Acan%20provide%20tighter%20constant%20factors%20than%20DSRS%20in%20providing%20%24%5COmega%28%5Csqrt%7Bd%7D%29%24%0Alower%20bounds%20of%20%24%5Cell_2%24%20certified%20radius%2C%20and%20thus%20further%20addresses%20the%20curse%0Aof%20dimensionality%20in%20RS.%20Our%20experiments%20on%20real-world%20datasets%20confirm%20our%0Atheoretical%20analysis%20of%20the%20ESG%20distributions%2C%20that%20they%20provide%20almost%20the%0Asame%20certification%20under%20different%20exponents%20%24%5Ceta%24%20for%20both%20RS%20and%20DSRS.%20In%0Aaddition%2C%20EGG%20brings%20a%20significant%20improvement%20to%20the%20DSRS%20certification%2C%20but%0Athe%20mechanism%20can%20be%20different%20when%20the%20classifier%20properties%20are%20different.%0ACompared%20to%20the%20primitive%20DSRS%2C%20the%20increase%20in%20certified%20accuracy%20provided%20by%0AEGG%20is%20prominent%2C%20up%20to%206.4%25%20on%20ImageNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffects%2520of%2520Exponential%2520Gaussian%2520Distribution%2520on%2520%2528Double%2520Sampling%2529%250A%2520%2520Randomized%2520Smoothing%26entry.906535625%3DYouwei%2520Shu%2520and%2520Xi%2520Xiao%2520and%2520Derui%2520Wang%2520and%2520Yuxin%2520Cao%2520and%2520Siji%2520Chen%2520and%2520Jason%2520Xue%2520and%2520Linyi%2520Li%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520Randomized%2520Smoothing%2520%2528RS%2529%2520is%2520currently%2520a%2520scalable%2520certified%2520defense%2520method%250Aproviding%2520robustness%2520certification%2520against%2520adversarial%2520examples.%2520Although%250Asignificant%2520progress%2520has%2520been%2520achieved%2520in%2520providing%2520defenses%2520against%2520%2524%255Cell_p%2524%250Aadversaries%252C%2520the%2520interaction%2520between%2520the%2520smoothing%2520distribution%2520and%2520the%250Arobustness%2520certification%2520still%2520remains%2520vague.%2520In%2520this%2520work%252C%2520we%2520comprehensively%250Astudy%2520the%2520effect%2520of%2520two%2520families%2520of%2520distributions%252C%2520named%2520Exponential%2520Standard%250AGaussian%2520%2528ESG%2529%2520and%2520Exponential%2520General%2520Gaussian%2520%2528EGG%2529%2520distributions%252C%2520on%250ARandomized%2520Smoothing%2520and%2520Double%2520Sampling%2520Randomized%2520Smoothing%2520%2528DSRS%2529.%2520We%2520derive%250Aan%2520analytic%2520formula%2520for%2520ESG%2527s%2520certified%2520radius%252C%2520which%2520converges%2520to%2520the%2520origin%250Aformula%2520of%2520RS%2520as%2520the%2520dimension%2520%2524d%2524%2520increases.%2520Additionally%252C%2520we%2520prove%2520that%2520EGG%250Acan%2520provide%2520tighter%2520constant%2520factors%2520than%2520DSRS%2520in%2520providing%2520%2524%255COmega%2528%255Csqrt%257Bd%257D%2529%2524%250Alower%2520bounds%2520of%2520%2524%255Cell_2%2524%2520certified%2520radius%252C%2520and%2520thus%2520further%2520addresses%2520the%2520curse%250Aof%2520dimensionality%2520in%2520RS.%2520Our%2520experiments%2520on%2520real-world%2520datasets%2520confirm%2520our%250Atheoretical%2520analysis%2520of%2520the%2520ESG%2520distributions%252C%2520that%2520they%2520provide%2520almost%2520the%250Asame%2520certification%2520under%2520different%2520exponents%2520%2524%255Ceta%2524%2520for%2520both%2520RS%2520and%2520DSRS.%2520In%250Aaddition%252C%2520EGG%2520brings%2520a%2520significant%2520improvement%2520to%2520the%2520DSRS%2520certification%252C%2520but%250Athe%2520mechanism%2520can%2520be%2520different%2520when%2520the%2520classifier%2520properties%2520are%2520different.%250ACompared%2520to%2520the%2520primitive%2520DSRS%252C%2520the%2520increase%2520in%2520certified%2520accuracy%2520provided%2520by%250AEGG%2520is%2520prominent%252C%2520up%2520to%25206.4%2525%2520on%2520ImageNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effects%20of%20Exponential%20Gaussian%20Distribution%20on%20%28Double%20Sampling%29%0A%20%20Randomized%20Smoothing&entry.906535625=Youwei%20Shu%20and%20Xi%20Xiao%20and%20Derui%20Wang%20and%20Yuxin%20Cao%20and%20Siji%20Chen%20and%20Jason%20Xue%20and%20Linyi%20Li%20and%20Bo%20Li&entry.1292438233=%20%20Randomized%20Smoothing%20%28RS%29%20is%20currently%20a%20scalable%20certified%20defense%20method%0Aproviding%20robustness%20certification%20against%20adversarial%20examples.%20Although%0Asignificant%20progress%20has%20been%20achieved%20in%20providing%20defenses%20against%20%24%5Cell_p%24%0Aadversaries%2C%20the%20interaction%20between%20the%20smoothing%20distribution%20and%20the%0Arobustness%20certification%20still%20remains%20vague.%20In%20this%20work%2C%20we%20comprehensively%0Astudy%20the%20effect%20of%20two%20families%20of%20distributions%2C%20named%20Exponential%20Standard%0AGaussian%20%28ESG%29%20and%20Exponential%20General%20Gaussian%20%28EGG%29%20distributions%2C%20on%0ARandomized%20Smoothing%20and%20Double%20Sampling%20Randomized%20Smoothing%20%28DSRS%29.%20We%20derive%0Aan%20analytic%20formula%20for%20ESG%27s%20certified%20radius%2C%20which%20converges%20to%20the%20origin%0Aformula%20of%20RS%20as%20the%20dimension%20%24d%24%20increases.%20Additionally%2C%20we%20prove%20that%20EGG%0Acan%20provide%20tighter%20constant%20factors%20than%20DSRS%20in%20providing%20%24%5COmega%28%5Csqrt%7Bd%7D%29%24%0Alower%20bounds%20of%20%24%5Cell_2%24%20certified%20radius%2C%20and%20thus%20further%20addresses%20the%20curse%0Aof%20dimensionality%20in%20RS.%20Our%20experiments%20on%20real-world%20datasets%20confirm%20our%0Atheoretical%20analysis%20of%20the%20ESG%20distributions%2C%20that%20they%20provide%20almost%20the%0Asame%20certification%20under%20different%20exponents%20%24%5Ceta%24%20for%20both%20RS%20and%20DSRS.%20In%0Aaddition%2C%20EGG%20brings%20a%20significant%20improvement%20to%20the%20DSRS%20certification%2C%20but%0Athe%20mechanism%20can%20be%20different%20when%20the%20classifier%20properties%20are%20different.%0ACompared%20to%20the%20primitive%20DSRS%2C%20the%20increase%20in%20certified%20accuracy%20provided%20by%0AEGG%20is%20prominent%2C%20up%20to%206.4%25%20on%20ImageNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02309v2&entry.124074799=Read"},
{"title": "Intersectional Unfairness Discovery", "author": "Gezheng Xu and Qi Chen and Charles Ling and Boyu Wang and Changjian Shui", "abstract": "  AI systems have been shown to produce unfair results for certain subgroups of\npopulation, highlighting the need to understand bias on certain sensitive\nattributes. Current research often falls short, primarily focusing on the\nsubgroups characterized by a single sensitive attribute, while neglecting the\nnature of intersectional fairness of multiple sensitive attributes. This paper\nfocuses on its one fundamental aspect by discovering diverse high-bias\nsubgroups under intersectional sensitive attributes. Specifically, we propose a\nBias-Guided Generative Network (BGGN). By treating each bias value as a reward,\nBGGN efficiently generates high-bias intersectional sensitive attributes.\nExperiments on real-world text and image datasets demonstrate a diverse and\nefficient discovery of BGGN. To further evaluate the generated unseen but\npossible unfair intersectional sensitive attributes, we formulate them as\nprompts and use modern generative AI to produce new texts and images. The\nresults of frequently generating biased data provides new insights of\ndiscovering potential unfairness in popular modern generative AI systems.\nWarning: This paper contains generative examples that are offensive in nature.\n", "link": "http://arxiv.org/abs/2405.20790v2", "date": "2024-06-05", "relevancy": 2.4899, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5143}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5049}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intersectional%20Unfairness%20Discovery&body=Title%3A%20Intersectional%20Unfairness%20Discovery%0AAuthor%3A%20Gezheng%20Xu%20and%20Qi%20Chen%20and%20Charles%20Ling%20and%20Boyu%20Wang%20and%20Changjian%20Shui%0AAbstract%3A%20%20%20AI%20systems%20have%20been%20shown%20to%20produce%20unfair%20results%20for%20certain%20subgroups%20of%0Apopulation%2C%20highlighting%20the%20need%20to%20understand%20bias%20on%20certain%20sensitive%0Aattributes.%20Current%20research%20often%20falls%20short%2C%20primarily%20focusing%20on%20the%0Asubgroups%20characterized%20by%20a%20single%20sensitive%20attribute%2C%20while%20neglecting%20the%0Anature%20of%20intersectional%20fairness%20of%20multiple%20sensitive%20attributes.%20This%20paper%0Afocuses%20on%20its%20one%20fundamental%20aspect%20by%20discovering%20diverse%20high-bias%0Asubgroups%20under%20intersectional%20sensitive%20attributes.%20Specifically%2C%20we%20propose%20a%0ABias-Guided%20Generative%20Network%20%28BGGN%29.%20By%20treating%20each%20bias%20value%20as%20a%20reward%2C%0ABGGN%20efficiently%20generates%20high-bias%20intersectional%20sensitive%20attributes.%0AExperiments%20on%20real-world%20text%20and%20image%20datasets%20demonstrate%20a%20diverse%20and%0Aefficient%20discovery%20of%20BGGN.%20To%20further%20evaluate%20the%20generated%20unseen%20but%0Apossible%20unfair%20intersectional%20sensitive%20attributes%2C%20we%20formulate%20them%20as%0Aprompts%20and%20use%20modern%20generative%20AI%20to%20produce%20new%20texts%20and%20images.%20The%0Aresults%20of%20frequently%20generating%20biased%20data%20provides%20new%20insights%20of%0Adiscovering%20potential%20unfairness%20in%20popular%20modern%20generative%20AI%20systems.%0AWarning%3A%20This%20paper%20contains%20generative%20examples%20that%20are%20offensive%20in%20nature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20790v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntersectional%2520Unfairness%2520Discovery%26entry.906535625%3DGezheng%2520Xu%2520and%2520Qi%2520Chen%2520and%2520Charles%2520Ling%2520and%2520Boyu%2520Wang%2520and%2520Changjian%2520Shui%26entry.1292438233%3D%2520%2520AI%2520systems%2520have%2520been%2520shown%2520to%2520produce%2520unfair%2520results%2520for%2520certain%2520subgroups%2520of%250Apopulation%252C%2520highlighting%2520the%2520need%2520to%2520understand%2520bias%2520on%2520certain%2520sensitive%250Aattributes.%2520Current%2520research%2520often%2520falls%2520short%252C%2520primarily%2520focusing%2520on%2520the%250Asubgroups%2520characterized%2520by%2520a%2520single%2520sensitive%2520attribute%252C%2520while%2520neglecting%2520the%250Anature%2520of%2520intersectional%2520fairness%2520of%2520multiple%2520sensitive%2520attributes.%2520This%2520paper%250Afocuses%2520on%2520its%2520one%2520fundamental%2520aspect%2520by%2520discovering%2520diverse%2520high-bias%250Asubgroups%2520under%2520intersectional%2520sensitive%2520attributes.%2520Specifically%252C%2520we%2520propose%2520a%250ABias-Guided%2520Generative%2520Network%2520%2528BGGN%2529.%2520By%2520treating%2520each%2520bias%2520value%2520as%2520a%2520reward%252C%250ABGGN%2520efficiently%2520generates%2520high-bias%2520intersectional%2520sensitive%2520attributes.%250AExperiments%2520on%2520real-world%2520text%2520and%2520image%2520datasets%2520demonstrate%2520a%2520diverse%2520and%250Aefficient%2520discovery%2520of%2520BGGN.%2520To%2520further%2520evaluate%2520the%2520generated%2520unseen%2520but%250Apossible%2520unfair%2520intersectional%2520sensitive%2520attributes%252C%2520we%2520formulate%2520them%2520as%250Aprompts%2520and%2520use%2520modern%2520generative%2520AI%2520to%2520produce%2520new%2520texts%2520and%2520images.%2520The%250Aresults%2520of%2520frequently%2520generating%2520biased%2520data%2520provides%2520new%2520insights%2520of%250Adiscovering%2520potential%2520unfairness%2520in%2520popular%2520modern%2520generative%2520AI%2520systems.%250AWarning%253A%2520This%2520paper%2520contains%2520generative%2520examples%2520that%2520are%2520offensive%2520in%2520nature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20790v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intersectional%20Unfairness%20Discovery&entry.906535625=Gezheng%20Xu%20and%20Qi%20Chen%20and%20Charles%20Ling%20and%20Boyu%20Wang%20and%20Changjian%20Shui&entry.1292438233=%20%20AI%20systems%20have%20been%20shown%20to%20produce%20unfair%20results%20for%20certain%20subgroups%20of%0Apopulation%2C%20highlighting%20the%20need%20to%20understand%20bias%20on%20certain%20sensitive%0Aattributes.%20Current%20research%20often%20falls%20short%2C%20primarily%20focusing%20on%20the%0Asubgroups%20characterized%20by%20a%20single%20sensitive%20attribute%2C%20while%20neglecting%20the%0Anature%20of%20intersectional%20fairness%20of%20multiple%20sensitive%20attributes.%20This%20paper%0Afocuses%20on%20its%20one%20fundamental%20aspect%20by%20discovering%20diverse%20high-bias%0Asubgroups%20under%20intersectional%20sensitive%20attributes.%20Specifically%2C%20we%20propose%20a%0ABias-Guided%20Generative%20Network%20%28BGGN%29.%20By%20treating%20each%20bias%20value%20as%20a%20reward%2C%0ABGGN%20efficiently%20generates%20high-bias%20intersectional%20sensitive%20attributes.%0AExperiments%20on%20real-world%20text%20and%20image%20datasets%20demonstrate%20a%20diverse%20and%0Aefficient%20discovery%20of%20BGGN.%20To%20further%20evaluate%20the%20generated%20unseen%20but%0Apossible%20unfair%20intersectional%20sensitive%20attributes%2C%20we%20formulate%20them%20as%0Aprompts%20and%20use%20modern%20generative%20AI%20to%20produce%20new%20texts%20and%20images.%20The%0Aresults%20of%20frequently%20generating%20biased%20data%20provides%20new%20insights%20of%0Adiscovering%20potential%20unfairness%20in%20popular%20modern%20generative%20AI%20systems.%0AWarning%3A%20This%20paper%20contains%20generative%20examples%20that%20are%20offensive%20in%20nature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20790v2&entry.124074799=Read"},
{"title": "Remove that Square Root: A New Efficient Scale-Invariant Version of\n  AdaGrad", "author": "Sayantan Choudhury and Nazarii Tupitsa and Nicolas Loizou and Samuel Horvath and Martin Takac and Eduard Gorbunov", "abstract": "  Adaptive methods are extremely popular in machine learning as they make\nlearning rate tuning less expensive. This paper introduces a novel optimization\nalgorithm named KATE, which presents a scale-invariant adaptation of the\nwell-known AdaGrad algorithm. We prove the scale-invariance of KATE for the\ncase of Generalized Linear Models. Moreover, for general smooth non-convex\nproblems, we establish a convergence rate of $O \\left(\\frac{\\log T}{\\sqrt{T}}\n\\right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also\ncompare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in\nnumerical experiments with different problems, including complex machine\nlearning tasks like image classification and text classification on real data.\nThe results indicate that KATE consistently outperforms AdaGrad and\nmatches/surpasses the performance of Adam in all considered scenarios.\n", "link": "http://arxiv.org/abs/2403.02648v2", "date": "2024-06-05", "relevancy": 2.4239, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5116}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4728}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Remove%20that%20Square%20Root%3A%20A%20New%20Efficient%20Scale-Invariant%20Version%20of%0A%20%20AdaGrad&body=Title%3A%20Remove%20that%20Square%20Root%3A%20A%20New%20Efficient%20Scale-Invariant%20Version%20of%0A%20%20AdaGrad%0AAuthor%3A%20Sayantan%20Choudhury%20and%20Nazarii%20Tupitsa%20and%20Nicolas%20Loizou%20and%20Samuel%20Horvath%20and%20Martin%20Takac%20and%20Eduard%20Gorbunov%0AAbstract%3A%20%20%20Adaptive%20methods%20are%20extremely%20popular%20in%20machine%20learning%20as%20they%20make%0Alearning%20rate%20tuning%20less%20expensive.%20This%20paper%20introduces%20a%20novel%20optimization%0Aalgorithm%20named%20KATE%2C%20which%20presents%20a%20scale-invariant%20adaptation%20of%20the%0Awell-known%20AdaGrad%20algorithm.%20We%20prove%20the%20scale-invariance%20of%20KATE%20for%20the%0Acase%20of%20Generalized%20Linear%20Models.%20Moreover%2C%20for%20general%20smooth%20non-convex%0Aproblems%2C%20we%20establish%20a%20convergence%20rate%20of%20%24O%20%5Cleft%28%5Cfrac%7B%5Clog%20T%7D%7B%5Csqrt%7BT%7D%7D%0A%5Cright%29%24%20for%20KATE%2C%20matching%20the%20best-known%20ones%20for%20AdaGrad%20and%20Adam.%20We%20also%0Acompare%20KATE%20to%20other%20state-of-the-art%20adaptive%20algorithms%20Adam%20and%20AdaGrad%20in%0Anumerical%20experiments%20with%20different%20problems%2C%20including%20complex%20machine%0Alearning%20tasks%20like%20image%20classification%20and%20text%20classification%20on%20real%20data.%0AThe%20results%20indicate%20that%20KATE%20consistently%20outperforms%20AdaGrad%20and%0Amatches/surpasses%20the%20performance%20of%20Adam%20in%20all%20considered%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02648v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemove%2520that%2520Square%2520Root%253A%2520A%2520New%2520Efficient%2520Scale-Invariant%2520Version%2520of%250A%2520%2520AdaGrad%26entry.906535625%3DSayantan%2520Choudhury%2520and%2520Nazarii%2520Tupitsa%2520and%2520Nicolas%2520Loizou%2520and%2520Samuel%2520Horvath%2520and%2520Martin%2520Takac%2520and%2520Eduard%2520Gorbunov%26entry.1292438233%3D%2520%2520Adaptive%2520methods%2520are%2520extremely%2520popular%2520in%2520machine%2520learning%2520as%2520they%2520make%250Alearning%2520rate%2520tuning%2520less%2520expensive.%2520This%2520paper%2520introduces%2520a%2520novel%2520optimization%250Aalgorithm%2520named%2520KATE%252C%2520which%2520presents%2520a%2520scale-invariant%2520adaptation%2520of%2520the%250Awell-known%2520AdaGrad%2520algorithm.%2520We%2520prove%2520the%2520scale-invariance%2520of%2520KATE%2520for%2520the%250Acase%2520of%2520Generalized%2520Linear%2520Models.%2520Moreover%252C%2520for%2520general%2520smooth%2520non-convex%250Aproblems%252C%2520we%2520establish%2520a%2520convergence%2520rate%2520of%2520%2524O%2520%255Cleft%2528%255Cfrac%257B%255Clog%2520T%257D%257B%255Csqrt%257BT%257D%257D%250A%255Cright%2529%2524%2520for%2520KATE%252C%2520matching%2520the%2520best-known%2520ones%2520for%2520AdaGrad%2520and%2520Adam.%2520We%2520also%250Acompare%2520KATE%2520to%2520other%2520state-of-the-art%2520adaptive%2520algorithms%2520Adam%2520and%2520AdaGrad%2520in%250Anumerical%2520experiments%2520with%2520different%2520problems%252C%2520including%2520complex%2520machine%250Alearning%2520tasks%2520like%2520image%2520classification%2520and%2520text%2520classification%2520on%2520real%2520data.%250AThe%2520results%2520indicate%2520that%2520KATE%2520consistently%2520outperforms%2520AdaGrad%2520and%250Amatches/surpasses%2520the%2520performance%2520of%2520Adam%2520in%2520all%2520considered%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02648v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Remove%20that%20Square%20Root%3A%20A%20New%20Efficient%20Scale-Invariant%20Version%20of%0A%20%20AdaGrad&entry.906535625=Sayantan%20Choudhury%20and%20Nazarii%20Tupitsa%20and%20Nicolas%20Loizou%20and%20Samuel%20Horvath%20and%20Martin%20Takac%20and%20Eduard%20Gorbunov&entry.1292438233=%20%20Adaptive%20methods%20are%20extremely%20popular%20in%20machine%20learning%20as%20they%20make%0Alearning%20rate%20tuning%20less%20expensive.%20This%20paper%20introduces%20a%20novel%20optimization%0Aalgorithm%20named%20KATE%2C%20which%20presents%20a%20scale-invariant%20adaptation%20of%20the%0Awell-known%20AdaGrad%20algorithm.%20We%20prove%20the%20scale-invariance%20of%20KATE%20for%20the%0Acase%20of%20Generalized%20Linear%20Models.%20Moreover%2C%20for%20general%20smooth%20non-convex%0Aproblems%2C%20we%20establish%20a%20convergence%20rate%20of%20%24O%20%5Cleft%28%5Cfrac%7B%5Clog%20T%7D%7B%5Csqrt%7BT%7D%7D%0A%5Cright%29%24%20for%20KATE%2C%20matching%20the%20best-known%20ones%20for%20AdaGrad%20and%20Adam.%20We%20also%0Acompare%20KATE%20to%20other%20state-of-the-art%20adaptive%20algorithms%20Adam%20and%20AdaGrad%20in%0Anumerical%20experiments%20with%20different%20problems%2C%20including%20complex%20machine%0Alearning%20tasks%20like%20image%20classification%20and%20text%20classification%20on%20real%20data.%0AThe%20results%20indicate%20that%20KATE%20consistently%20outperforms%20AdaGrad%20and%0Amatches/surpasses%20the%20performance%20of%20Adam%20in%20all%20considered%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02648v2&entry.124074799=Read"},
{"title": "Post-hoc Part-prototype Networks", "author": "Andong Tan and Fengtao Zhou and Hao Chen", "abstract": "  Post-hoc explainability methods such as Grad-CAM are popular because they do\nnot influence the performance of a trained model. However, they mainly reveal\n\"where\" a model looks at for a given input, fail to explain \"what\" the model\nlooks for (e.g., what is important to classify a bird image to a Scott\nOriole?). Existing part-prototype networks leverage part-prototypes (e.g.,\ncharacteristic Scott Oriole's wing and head) to answer both \"where\" and \"what\",\nbut often under-perform their black box counterparts in the accuracy.\nTherefore, a natural question is: can one construct a network that answers both\n\"where\" and \"what\" in a post-hoc manner to guarantee the model's performance?\nTo this end, we propose the first post-hoc part-prototype network via\ndecomposing the classification head of a trained model into a set of\ninterpretable part-prototypes. Concretely, we propose an unsupervised prototype\ndiscovery and refining strategy to obtain prototypes that can precisely\nreconstruct the classification head, yet being interpretable. Besides\nguaranteeing the performance, we show that our network offers more faithful\nexplanations qualitatively and yields even better part-prototypes\nquantitatively than prior part-prototype networks.\n", "link": "http://arxiv.org/abs/2406.03421v1", "date": "2024-06-05", "relevancy": 2.419, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4835}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-hoc%20Part-prototype%20Networks&body=Title%3A%20Post-hoc%20Part-prototype%20Networks%0AAuthor%3A%20Andong%20Tan%20and%20Fengtao%20Zhou%20and%20Hao%20Chen%0AAbstract%3A%20%20%20Post-hoc%20explainability%20methods%20such%20as%20Grad-CAM%20are%20popular%20because%20they%20do%0Anot%20influence%20the%20performance%20of%20a%20trained%20model.%20However%2C%20they%20mainly%20reveal%0A%22where%22%20a%20model%20looks%20at%20for%20a%20given%20input%2C%20fail%20to%20explain%20%22what%22%20the%20model%0Alooks%20for%20%28e.g.%2C%20what%20is%20important%20to%20classify%20a%20bird%20image%20to%20a%20Scott%0AOriole%3F%29.%20Existing%20part-prototype%20networks%20leverage%20part-prototypes%20%28e.g.%2C%0Acharacteristic%20Scott%20Oriole%27s%20wing%20and%20head%29%20to%20answer%20both%20%22where%22%20and%20%22what%22%2C%0Abut%20often%20under-perform%20their%20black%20box%20counterparts%20in%20the%20accuracy.%0ATherefore%2C%20a%20natural%20question%20is%3A%20can%20one%20construct%20a%20network%20that%20answers%20both%0A%22where%22%20and%20%22what%22%20in%20a%20post-hoc%20manner%20to%20guarantee%20the%20model%27s%20performance%3F%0ATo%20this%20end%2C%20we%20propose%20the%20first%20post-hoc%20part-prototype%20network%20via%0Adecomposing%20the%20classification%20head%20of%20a%20trained%20model%20into%20a%20set%20of%0Ainterpretable%20part-prototypes.%20Concretely%2C%20we%20propose%20an%20unsupervised%20prototype%0Adiscovery%20and%20refining%20strategy%20to%20obtain%20prototypes%20that%20can%20precisely%0Areconstruct%20the%20classification%20head%2C%20yet%20being%20interpretable.%20Besides%0Aguaranteeing%20the%20performance%2C%20we%20show%20that%20our%20network%20offers%20more%20faithful%0Aexplanations%20qualitatively%20and%20yields%20even%20better%20part-prototypes%0Aquantitatively%20than%20prior%20part-prototype%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-hoc%2520Part-prototype%2520Networks%26entry.906535625%3DAndong%2520Tan%2520and%2520Fengtao%2520Zhou%2520and%2520Hao%2520Chen%26entry.1292438233%3D%2520%2520Post-hoc%2520explainability%2520methods%2520such%2520as%2520Grad-CAM%2520are%2520popular%2520because%2520they%2520do%250Anot%2520influence%2520the%2520performance%2520of%2520a%2520trained%2520model.%2520However%252C%2520they%2520mainly%2520reveal%250A%2522where%2522%2520a%2520model%2520looks%2520at%2520for%2520a%2520given%2520input%252C%2520fail%2520to%2520explain%2520%2522what%2522%2520the%2520model%250Alooks%2520for%2520%2528e.g.%252C%2520what%2520is%2520important%2520to%2520classify%2520a%2520bird%2520image%2520to%2520a%2520Scott%250AOriole%253F%2529.%2520Existing%2520part-prototype%2520networks%2520leverage%2520part-prototypes%2520%2528e.g.%252C%250Acharacteristic%2520Scott%2520Oriole%2527s%2520wing%2520and%2520head%2529%2520to%2520answer%2520both%2520%2522where%2522%2520and%2520%2522what%2522%252C%250Abut%2520often%2520under-perform%2520their%2520black%2520box%2520counterparts%2520in%2520the%2520accuracy.%250ATherefore%252C%2520a%2520natural%2520question%2520is%253A%2520can%2520one%2520construct%2520a%2520network%2520that%2520answers%2520both%250A%2522where%2522%2520and%2520%2522what%2522%2520in%2520a%2520post-hoc%2520manner%2520to%2520guarantee%2520the%2520model%2527s%2520performance%253F%250ATo%2520this%2520end%252C%2520we%2520propose%2520the%2520first%2520post-hoc%2520part-prototype%2520network%2520via%250Adecomposing%2520the%2520classification%2520head%2520of%2520a%2520trained%2520model%2520into%2520a%2520set%2520of%250Ainterpretable%2520part-prototypes.%2520Concretely%252C%2520we%2520propose%2520an%2520unsupervised%2520prototype%250Adiscovery%2520and%2520refining%2520strategy%2520to%2520obtain%2520prototypes%2520that%2520can%2520precisely%250Areconstruct%2520the%2520classification%2520head%252C%2520yet%2520being%2520interpretable.%2520Besides%250Aguaranteeing%2520the%2520performance%252C%2520we%2520show%2520that%2520our%2520network%2520offers%2520more%2520faithful%250Aexplanations%2520qualitatively%2520and%2520yields%2520even%2520better%2520part-prototypes%250Aquantitatively%2520than%2520prior%2520part-prototype%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-hoc%20Part-prototype%20Networks&entry.906535625=Andong%20Tan%20and%20Fengtao%20Zhou%20and%20Hao%20Chen&entry.1292438233=%20%20Post-hoc%20explainability%20methods%20such%20as%20Grad-CAM%20are%20popular%20because%20they%20do%0Anot%20influence%20the%20performance%20of%20a%20trained%20model.%20However%2C%20they%20mainly%20reveal%0A%22where%22%20a%20model%20looks%20at%20for%20a%20given%20input%2C%20fail%20to%20explain%20%22what%22%20the%20model%0Alooks%20for%20%28e.g.%2C%20what%20is%20important%20to%20classify%20a%20bird%20image%20to%20a%20Scott%0AOriole%3F%29.%20Existing%20part-prototype%20networks%20leverage%20part-prototypes%20%28e.g.%2C%0Acharacteristic%20Scott%20Oriole%27s%20wing%20and%20head%29%20to%20answer%20both%20%22where%22%20and%20%22what%22%2C%0Abut%20often%20under-perform%20their%20black%20box%20counterparts%20in%20the%20accuracy.%0ATherefore%2C%20a%20natural%20question%20is%3A%20can%20one%20construct%20a%20network%20that%20answers%20both%0A%22where%22%20and%20%22what%22%20in%20a%20post-hoc%20manner%20to%20guarantee%20the%20model%27s%20performance%3F%0ATo%20this%20end%2C%20we%20propose%20the%20first%20post-hoc%20part-prototype%20network%20via%0Adecomposing%20the%20classification%20head%20of%20a%20trained%20model%20into%20a%20set%20of%0Ainterpretable%20part-prototypes.%20Concretely%2C%20we%20propose%20an%20unsupervised%20prototype%0Adiscovery%20and%20refining%20strategy%20to%20obtain%20prototypes%20that%20can%20precisely%0Areconstruct%20the%20classification%20head%2C%20yet%20being%20interpretable.%20Besides%0Aguaranteeing%20the%20performance%2C%20we%20show%20that%20our%20network%20offers%20more%20faithful%0Aexplanations%20qualitatively%20and%20yields%20even%20better%20part-prototypes%0Aquantitatively%20than%20prior%20part-prototype%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03421v1&entry.124074799=Read"},
{"title": "The Heuristic Core: Understanding Subnetwork Generalization in\n  Pretrained Language Models", "author": "Adithya Bhaskar and Dan Friedman and Danqi Chen", "abstract": "  Prior work has found that pretrained language models (LMs) fine-tuned with\ndifferent random seeds can achieve similar in-domain performance but generalize\ndifferently on tests of syntactic generalization. In this work, we show that,\neven within a single model, we can find multiple subnetworks that perform\nsimilarly in-domain, but generalize vastly differently. To better understand\nthese phenomena, we investigate if they can be understood in terms of\n\"competing subnetworks\": the model initially represents a variety of distinct\nalgorithms, corresponding to different subnetworks, and generalization occurs\nwhen it ultimately converges to one. This explanation has been used to account\nfor generalization in simple algorithmic tasks (\"grokking\"). Instead of finding\ncompeting subnetworks, we find that all subnetworks -- whether they generalize\nor not -- share a set of attention heads, which we refer to as the heuristic\ncore. Further analysis suggests that these attention heads emerge early in\ntraining and compute shallow, non-generalizing features. The model learns to\ngeneralize by incorporating additional attention heads, which depend on the\noutputs of the \"heuristic\" heads to compute higher-level features. Overall, our\nresults offer a more detailed picture of the mechanisms for syntactic\ngeneralization in pretrained LMs.\n", "link": "http://arxiv.org/abs/2403.03942v2", "date": "2024-06-05", "relevancy": 2.3796, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4823}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4788}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Heuristic%20Core%3A%20Understanding%20Subnetwork%20Generalization%20in%0A%20%20Pretrained%20Language%20Models&body=Title%3A%20The%20Heuristic%20Core%3A%20Understanding%20Subnetwork%20Generalization%20in%0A%20%20Pretrained%20Language%20Models%0AAuthor%3A%20Adithya%20Bhaskar%20and%20Dan%20Friedman%20and%20Danqi%20Chen%0AAbstract%3A%20%20%20Prior%20work%20has%20found%20that%20pretrained%20language%20models%20%28LMs%29%20fine-tuned%20with%0Adifferent%20random%20seeds%20can%20achieve%20similar%20in-domain%20performance%20but%20generalize%0Adifferently%20on%20tests%20of%20syntactic%20generalization.%20In%20this%20work%2C%20we%20show%20that%2C%0Aeven%20within%20a%20single%20model%2C%20we%20can%20find%20multiple%20subnetworks%20that%20perform%0Asimilarly%20in-domain%2C%20but%20generalize%20vastly%20differently.%20To%20better%20understand%0Athese%20phenomena%2C%20we%20investigate%20if%20they%20can%20be%20understood%20in%20terms%20of%0A%22competing%20subnetworks%22%3A%20the%20model%20initially%20represents%20a%20variety%20of%20distinct%0Aalgorithms%2C%20corresponding%20to%20different%20subnetworks%2C%20and%20generalization%20occurs%0Awhen%20it%20ultimately%20converges%20to%20one.%20This%20explanation%20has%20been%20used%20to%20account%0Afor%20generalization%20in%20simple%20algorithmic%20tasks%20%28%22grokking%22%29.%20Instead%20of%20finding%0Acompeting%20subnetworks%2C%20we%20find%20that%20all%20subnetworks%20--%20whether%20they%20generalize%0Aor%20not%20--%20share%20a%20set%20of%20attention%20heads%2C%20which%20we%20refer%20to%20as%20the%20heuristic%0Acore.%20Further%20analysis%20suggests%20that%20these%20attention%20heads%20emerge%20early%20in%0Atraining%20and%20compute%20shallow%2C%20non-generalizing%20features.%20The%20model%20learns%20to%0Ageneralize%20by%20incorporating%20additional%20attention%20heads%2C%20which%20depend%20on%20the%0Aoutputs%20of%20the%20%22heuristic%22%20heads%20to%20compute%20higher-level%20features.%20Overall%2C%20our%0Aresults%20offer%20a%20more%20detailed%20picture%20of%20the%20mechanisms%20for%20syntactic%0Ageneralization%20in%20pretrained%20LMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03942v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Heuristic%2520Core%253A%2520Understanding%2520Subnetwork%2520Generalization%2520in%250A%2520%2520Pretrained%2520Language%2520Models%26entry.906535625%3DAdithya%2520Bhaskar%2520and%2520Dan%2520Friedman%2520and%2520Danqi%2520Chen%26entry.1292438233%3D%2520%2520Prior%2520work%2520has%2520found%2520that%2520pretrained%2520language%2520models%2520%2528LMs%2529%2520fine-tuned%2520with%250Adifferent%2520random%2520seeds%2520can%2520achieve%2520similar%2520in-domain%2520performance%2520but%2520generalize%250Adifferently%2520on%2520tests%2520of%2520syntactic%2520generalization.%2520In%2520this%2520work%252C%2520we%2520show%2520that%252C%250Aeven%2520within%2520a%2520single%2520model%252C%2520we%2520can%2520find%2520multiple%2520subnetworks%2520that%2520perform%250Asimilarly%2520in-domain%252C%2520but%2520generalize%2520vastly%2520differently.%2520To%2520better%2520understand%250Athese%2520phenomena%252C%2520we%2520investigate%2520if%2520they%2520can%2520be%2520understood%2520in%2520terms%2520of%250A%2522competing%2520subnetworks%2522%253A%2520the%2520model%2520initially%2520represents%2520a%2520variety%2520of%2520distinct%250Aalgorithms%252C%2520corresponding%2520to%2520different%2520subnetworks%252C%2520and%2520generalization%2520occurs%250Awhen%2520it%2520ultimately%2520converges%2520to%2520one.%2520This%2520explanation%2520has%2520been%2520used%2520to%2520account%250Afor%2520generalization%2520in%2520simple%2520algorithmic%2520tasks%2520%2528%2522grokking%2522%2529.%2520Instead%2520of%2520finding%250Acompeting%2520subnetworks%252C%2520we%2520find%2520that%2520all%2520subnetworks%2520--%2520whether%2520they%2520generalize%250Aor%2520not%2520--%2520share%2520a%2520set%2520of%2520attention%2520heads%252C%2520which%2520we%2520refer%2520to%2520as%2520the%2520heuristic%250Acore.%2520Further%2520analysis%2520suggests%2520that%2520these%2520attention%2520heads%2520emerge%2520early%2520in%250Atraining%2520and%2520compute%2520shallow%252C%2520non-generalizing%2520features.%2520The%2520model%2520learns%2520to%250Ageneralize%2520by%2520incorporating%2520additional%2520attention%2520heads%252C%2520which%2520depend%2520on%2520the%250Aoutputs%2520of%2520the%2520%2522heuristic%2522%2520heads%2520to%2520compute%2520higher-level%2520features.%2520Overall%252C%2520our%250Aresults%2520offer%2520a%2520more%2520detailed%2520picture%2520of%2520the%2520mechanisms%2520for%2520syntactic%250Ageneralization%2520in%2520pretrained%2520LMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03942v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Heuristic%20Core%3A%20Understanding%20Subnetwork%20Generalization%20in%0A%20%20Pretrained%20Language%20Models&entry.906535625=Adithya%20Bhaskar%20and%20Dan%20Friedman%20and%20Danqi%20Chen&entry.1292438233=%20%20Prior%20work%20has%20found%20that%20pretrained%20language%20models%20%28LMs%29%20fine-tuned%20with%0Adifferent%20random%20seeds%20can%20achieve%20similar%20in-domain%20performance%20but%20generalize%0Adifferently%20on%20tests%20of%20syntactic%20generalization.%20In%20this%20work%2C%20we%20show%20that%2C%0Aeven%20within%20a%20single%20model%2C%20we%20can%20find%20multiple%20subnetworks%20that%20perform%0Asimilarly%20in-domain%2C%20but%20generalize%20vastly%20differently.%20To%20better%20understand%0Athese%20phenomena%2C%20we%20investigate%20if%20they%20can%20be%20understood%20in%20terms%20of%0A%22competing%20subnetworks%22%3A%20the%20model%20initially%20represents%20a%20variety%20of%20distinct%0Aalgorithms%2C%20corresponding%20to%20different%20subnetworks%2C%20and%20generalization%20occurs%0Awhen%20it%20ultimately%20converges%20to%20one.%20This%20explanation%20has%20been%20used%20to%20account%0Afor%20generalization%20in%20simple%20algorithmic%20tasks%20%28%22grokking%22%29.%20Instead%20of%20finding%0Acompeting%20subnetworks%2C%20we%20find%20that%20all%20subnetworks%20--%20whether%20they%20generalize%0Aor%20not%20--%20share%20a%20set%20of%20attention%20heads%2C%20which%20we%20refer%20to%20as%20the%20heuristic%0Acore.%20Further%20analysis%20suggests%20that%20these%20attention%20heads%20emerge%20early%20in%0Atraining%20and%20compute%20shallow%2C%20non-generalizing%20features.%20The%20model%20learns%20to%0Ageneralize%20by%20incorporating%20additional%20attention%20heads%2C%20which%20depend%20on%20the%0Aoutputs%20of%20the%20%22heuristic%22%20heads%20to%20compute%20higher-level%20features.%20Overall%2C%20our%0Aresults%20offer%20a%20more%20detailed%20picture%20of%20the%20mechanisms%20for%20syntactic%0Ageneralization%20in%20pretrained%20LMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03942v2&entry.124074799=Read"},
{"title": "Fiducial Tag Localization on a 3D LiDAR Prior Map", "author": "Yibo Liu and Jinjun Shan and Hunter Schofield", "abstract": "  The LiDAR fiducial tag, akin to the well-known AprilTag used in camera\napplications, serves as a convenient resource to impart artificial features to\nthe LiDAR sensor, facilitating robotics applications. Unfortunately, the\nexisting LiDAR fiducial tag localization methods do not apply to 3D LiDAR maps\nwhile resolving this problem is beneficial to LiDAR-based relocalization and\nnavigation. In this paper, we develop a novel approach to directly localize\nfiducial tags on a 3D LiDAR prior map, returning the tag poses (labeled by ID\nnumber) and vertex locations (labeled by index) w.r.t. the global coordinate\nsystem of the map. In particular, considering that fiducial tags are thin sheet\nobjects indistinguishable from the attached planes, we design a new pipeline\nthat gradually analyzes the 3D point cloud of the map from the intensity and\ngeometry perspectives, extracting potential tag-containing point clusters.\nThen, we introduce an intermediate-plane-based method to further check if each\npotential cluster has a tag and compute the vertex locations and tag pose if\nfound. We conduct both qualitative and quantitative experiments to demonstrate\nthat our approach is the first method applicable to localize tags on a 3D LiDAR\nmap while achieving better accuracy compared to previous methods. The\nopen-source implementation of this work is available at:\nhttps://github.com/York-SDCNLab/Marker-Detection-General.\n", "link": "http://arxiv.org/abs/2209.01072v3", "date": "2024-06-05", "relevancy": 2.3746, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6304}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5792}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fiducial%20Tag%20Localization%20on%20a%203D%20LiDAR%20Prior%20Map&body=Title%3A%20Fiducial%20Tag%20Localization%20on%20a%203D%20LiDAR%20Prior%20Map%0AAuthor%3A%20Yibo%20Liu%20and%20Jinjun%20Shan%20and%20Hunter%20Schofield%0AAbstract%3A%20%20%20The%20LiDAR%20fiducial%20tag%2C%20akin%20to%20the%20well-known%20AprilTag%20used%20in%20camera%0Aapplications%2C%20serves%20as%20a%20convenient%20resource%20to%20impart%20artificial%20features%20to%0Athe%20LiDAR%20sensor%2C%20facilitating%20robotics%20applications.%20Unfortunately%2C%20the%0Aexisting%20LiDAR%20fiducial%20tag%20localization%20methods%20do%20not%20apply%20to%203D%20LiDAR%20maps%0Awhile%20resolving%20this%20problem%20is%20beneficial%20to%20LiDAR-based%20relocalization%20and%0Anavigation.%20In%20this%20paper%2C%20we%20develop%20a%20novel%20approach%20to%20directly%20localize%0Afiducial%20tags%20on%20a%203D%20LiDAR%20prior%20map%2C%20returning%20the%20tag%20poses%20%28labeled%20by%20ID%0Anumber%29%20and%20vertex%20locations%20%28labeled%20by%20index%29%20w.r.t.%20the%20global%20coordinate%0Asystem%20of%20the%20map.%20In%20particular%2C%20considering%20that%20fiducial%20tags%20are%20thin%20sheet%0Aobjects%20indistinguishable%20from%20the%20attached%20planes%2C%20we%20design%20a%20new%20pipeline%0Athat%20gradually%20analyzes%20the%203D%20point%20cloud%20of%20the%20map%20from%20the%20intensity%20and%0Ageometry%20perspectives%2C%20extracting%20potential%20tag-containing%20point%20clusters.%0AThen%2C%20we%20introduce%20an%20intermediate-plane-based%20method%20to%20further%20check%20if%20each%0Apotential%20cluster%20has%20a%20tag%20and%20compute%20the%20vertex%20locations%20and%20tag%20pose%20if%0Afound.%20We%20conduct%20both%20qualitative%20and%20quantitative%20experiments%20to%20demonstrate%0Athat%20our%20approach%20is%20the%20first%20method%20applicable%20to%20localize%20tags%20on%20a%203D%20LiDAR%0Amap%20while%20achieving%20better%20accuracy%20compared%20to%20previous%20methods.%20The%0Aopen-source%20implementation%20of%20this%20work%20is%20available%20at%3A%0Ahttps%3A//github.com/York-SDCNLab/Marker-Detection-General.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.01072v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFiducial%2520Tag%2520Localization%2520on%2520a%25203D%2520LiDAR%2520Prior%2520Map%26entry.906535625%3DYibo%2520Liu%2520and%2520Jinjun%2520Shan%2520and%2520Hunter%2520Schofield%26entry.1292438233%3D%2520%2520The%2520LiDAR%2520fiducial%2520tag%252C%2520akin%2520to%2520the%2520well-known%2520AprilTag%2520used%2520in%2520camera%250Aapplications%252C%2520serves%2520as%2520a%2520convenient%2520resource%2520to%2520impart%2520artificial%2520features%2520to%250Athe%2520LiDAR%2520sensor%252C%2520facilitating%2520robotics%2520applications.%2520Unfortunately%252C%2520the%250Aexisting%2520LiDAR%2520fiducial%2520tag%2520localization%2520methods%2520do%2520not%2520apply%2520to%25203D%2520LiDAR%2520maps%250Awhile%2520resolving%2520this%2520problem%2520is%2520beneficial%2520to%2520LiDAR-based%2520relocalization%2520and%250Anavigation.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520novel%2520approach%2520to%2520directly%2520localize%250Afiducial%2520tags%2520on%2520a%25203D%2520LiDAR%2520prior%2520map%252C%2520returning%2520the%2520tag%2520poses%2520%2528labeled%2520by%2520ID%250Anumber%2529%2520and%2520vertex%2520locations%2520%2528labeled%2520by%2520index%2529%2520w.r.t.%2520the%2520global%2520coordinate%250Asystem%2520of%2520the%2520map.%2520In%2520particular%252C%2520considering%2520that%2520fiducial%2520tags%2520are%2520thin%2520sheet%250Aobjects%2520indistinguishable%2520from%2520the%2520attached%2520planes%252C%2520we%2520design%2520a%2520new%2520pipeline%250Athat%2520gradually%2520analyzes%2520the%25203D%2520point%2520cloud%2520of%2520the%2520map%2520from%2520the%2520intensity%2520and%250Ageometry%2520perspectives%252C%2520extracting%2520potential%2520tag-containing%2520point%2520clusters.%250AThen%252C%2520we%2520introduce%2520an%2520intermediate-plane-based%2520method%2520to%2520further%2520check%2520if%2520each%250Apotential%2520cluster%2520has%2520a%2520tag%2520and%2520compute%2520the%2520vertex%2520locations%2520and%2520tag%2520pose%2520if%250Afound.%2520We%2520conduct%2520both%2520qualitative%2520and%2520quantitative%2520experiments%2520to%2520demonstrate%250Athat%2520our%2520approach%2520is%2520the%2520first%2520method%2520applicable%2520to%2520localize%2520tags%2520on%2520a%25203D%2520LiDAR%250Amap%2520while%2520achieving%2520better%2520accuracy%2520compared%2520to%2520previous%2520methods.%2520The%250Aopen-source%2520implementation%2520of%2520this%2520work%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/York-SDCNLab/Marker-Detection-General.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.01072v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fiducial%20Tag%20Localization%20on%20a%203D%20LiDAR%20Prior%20Map&entry.906535625=Yibo%20Liu%20and%20Jinjun%20Shan%20and%20Hunter%20Schofield&entry.1292438233=%20%20The%20LiDAR%20fiducial%20tag%2C%20akin%20to%20the%20well-known%20AprilTag%20used%20in%20camera%0Aapplications%2C%20serves%20as%20a%20convenient%20resource%20to%20impart%20artificial%20features%20to%0Athe%20LiDAR%20sensor%2C%20facilitating%20robotics%20applications.%20Unfortunately%2C%20the%0Aexisting%20LiDAR%20fiducial%20tag%20localization%20methods%20do%20not%20apply%20to%203D%20LiDAR%20maps%0Awhile%20resolving%20this%20problem%20is%20beneficial%20to%20LiDAR-based%20relocalization%20and%0Anavigation.%20In%20this%20paper%2C%20we%20develop%20a%20novel%20approach%20to%20directly%20localize%0Afiducial%20tags%20on%20a%203D%20LiDAR%20prior%20map%2C%20returning%20the%20tag%20poses%20%28labeled%20by%20ID%0Anumber%29%20and%20vertex%20locations%20%28labeled%20by%20index%29%20w.r.t.%20the%20global%20coordinate%0Asystem%20of%20the%20map.%20In%20particular%2C%20considering%20that%20fiducial%20tags%20are%20thin%20sheet%0Aobjects%20indistinguishable%20from%20the%20attached%20planes%2C%20we%20design%20a%20new%20pipeline%0Athat%20gradually%20analyzes%20the%203D%20point%20cloud%20of%20the%20map%20from%20the%20intensity%20and%0Ageometry%20perspectives%2C%20extracting%20potential%20tag-containing%20point%20clusters.%0AThen%2C%20we%20introduce%20an%20intermediate-plane-based%20method%20to%20further%20check%20if%20each%0Apotential%20cluster%20has%20a%20tag%20and%20compute%20the%20vertex%20locations%20and%20tag%20pose%20if%0Afound.%20We%20conduct%20both%20qualitative%20and%20quantitative%20experiments%20to%20demonstrate%0Athat%20our%20approach%20is%20the%20first%20method%20applicable%20to%20localize%20tags%20on%20a%203D%20LiDAR%0Amap%20while%20achieving%20better%20accuracy%20compared%20to%20previous%20methods.%20The%0Aopen-source%20implementation%20of%20this%20work%20is%20available%20at%3A%0Ahttps%3A//github.com/York-SDCNLab/Marker-Detection-General.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.01072v3&entry.124074799=Read"},
{"title": "CattleFace-RGBT: RGB-T Cattle Facial Landmark Benchmark", "author": "Ethan Coffman and Reagan Clark and Nhat-Tan Bui and Trong Thang Pham and Beth Kegley and Jeremy G. Powell and Jiangchao Zhao and Ngan Le", "abstract": "  To address this challenge, we introduce CattleFace-RGBT, a RGB-T Cattle\nFacial Landmark dataset consisting of 2,300 RGB-T image pairs, a total of 4,600\nimages. Creating a landmark dataset is time-consuming, but AI-assisted\nannotation can help. However, applying AI to thermal images is challenging due\nto suboptimal results from direct thermal training and infeasible RGB-thermal\nalignment due to different camera views. Therefore, we opt to transfer models\ntrained on RGB to thermal images and refine them using our AI-assisted\nannotation tool following a semi-automatic annotation approach. Accurately\nlocalizing facial key points on both RGB and thermal images enables us to not\nonly discern the cattle's respiratory signs but also measure temperatures to\nassess the animal's thermal state. To the best of our knowledge, this is the\nfirst dataset for the cattle facial landmark on RGB-T images. We conduct\nbenchmarking of the CattleFace-RGBT dataset across various backbone\narchitectures, with the objective of establishing baselines for future\nresearch, analysis, and comparison. The dataset and models are at\nhttps://github.com/UARK-AICV/CattleFace-RGBT-benchmark\n", "link": "http://arxiv.org/abs/2406.03431v1", "date": "2024-06-05", "relevancy": 2.3587, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5217}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4485}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CattleFace-RGBT%3A%20RGB-T%20Cattle%20Facial%20Landmark%20Benchmark&body=Title%3A%20CattleFace-RGBT%3A%20RGB-T%20Cattle%20Facial%20Landmark%20Benchmark%0AAuthor%3A%20Ethan%20Coffman%20and%20Reagan%20Clark%20and%20Nhat-Tan%20Bui%20and%20Trong%20Thang%20Pham%20and%20Beth%20Kegley%20and%20Jeremy%20G.%20Powell%20and%20Jiangchao%20Zhao%20and%20Ngan%20Le%0AAbstract%3A%20%20%20To%20address%20this%20challenge%2C%20we%20introduce%20CattleFace-RGBT%2C%20a%20RGB-T%20Cattle%0AFacial%20Landmark%20dataset%20consisting%20of%202%2C300%20RGB-T%20image%20pairs%2C%20a%20total%20of%204%2C600%0Aimages.%20Creating%20a%20landmark%20dataset%20is%20time-consuming%2C%20but%20AI-assisted%0Aannotation%20can%20help.%20However%2C%20applying%20AI%20to%20thermal%20images%20is%20challenging%20due%0Ato%20suboptimal%20results%20from%20direct%20thermal%20training%20and%20infeasible%20RGB-thermal%0Aalignment%20due%20to%20different%20camera%20views.%20Therefore%2C%20we%20opt%20to%20transfer%20models%0Atrained%20on%20RGB%20to%20thermal%20images%20and%20refine%20them%20using%20our%20AI-assisted%0Aannotation%20tool%20following%20a%20semi-automatic%20annotation%20approach.%20Accurately%0Alocalizing%20facial%20key%20points%20on%20both%20RGB%20and%20thermal%20images%20enables%20us%20to%20not%0Aonly%20discern%20the%20cattle%27s%20respiratory%20signs%20but%20also%20measure%20temperatures%20to%0Aassess%20the%20animal%27s%20thermal%20state.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%0Afirst%20dataset%20for%20the%20cattle%20facial%20landmark%20on%20RGB-T%20images.%20We%20conduct%0Abenchmarking%20of%20the%20CattleFace-RGBT%20dataset%20across%20various%20backbone%0Aarchitectures%2C%20with%20the%20objective%20of%20establishing%20baselines%20for%20future%0Aresearch%2C%20analysis%2C%20and%20comparison.%20The%20dataset%20and%20models%20are%20at%0Ahttps%3A//github.com/UARK-AICV/CattleFace-RGBT-benchmark%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCattleFace-RGBT%253A%2520RGB-T%2520Cattle%2520Facial%2520Landmark%2520Benchmark%26entry.906535625%3DEthan%2520Coffman%2520and%2520Reagan%2520Clark%2520and%2520Nhat-Tan%2520Bui%2520and%2520Trong%2520Thang%2520Pham%2520and%2520Beth%2520Kegley%2520and%2520Jeremy%2520G.%2520Powell%2520and%2520Jiangchao%2520Zhao%2520and%2520Ngan%2520Le%26entry.1292438233%3D%2520%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520CattleFace-RGBT%252C%2520a%2520RGB-T%2520Cattle%250AFacial%2520Landmark%2520dataset%2520consisting%2520of%25202%252C300%2520RGB-T%2520image%2520pairs%252C%2520a%2520total%2520of%25204%252C600%250Aimages.%2520Creating%2520a%2520landmark%2520dataset%2520is%2520time-consuming%252C%2520but%2520AI-assisted%250Aannotation%2520can%2520help.%2520However%252C%2520applying%2520AI%2520to%2520thermal%2520images%2520is%2520challenging%2520due%250Ato%2520suboptimal%2520results%2520from%2520direct%2520thermal%2520training%2520and%2520infeasible%2520RGB-thermal%250Aalignment%2520due%2520to%2520different%2520camera%2520views.%2520Therefore%252C%2520we%2520opt%2520to%2520transfer%2520models%250Atrained%2520on%2520RGB%2520to%2520thermal%2520images%2520and%2520refine%2520them%2520using%2520our%2520AI-assisted%250Aannotation%2520tool%2520following%2520a%2520semi-automatic%2520annotation%2520approach.%2520Accurately%250Alocalizing%2520facial%2520key%2520points%2520on%2520both%2520RGB%2520and%2520thermal%2520images%2520enables%2520us%2520to%2520not%250Aonly%2520discern%2520the%2520cattle%2527s%2520respiratory%2520signs%2520but%2520also%2520measure%2520temperatures%2520to%250Aassess%2520the%2520animal%2527s%2520thermal%2520state.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520dataset%2520for%2520the%2520cattle%2520facial%2520landmark%2520on%2520RGB-T%2520images.%2520We%2520conduct%250Abenchmarking%2520of%2520the%2520CattleFace-RGBT%2520dataset%2520across%2520various%2520backbone%250Aarchitectures%252C%2520with%2520the%2520objective%2520of%2520establishing%2520baselines%2520for%2520future%250Aresearch%252C%2520analysis%252C%2520and%2520comparison.%2520The%2520dataset%2520and%2520models%2520are%2520at%250Ahttps%253A//github.com/UARK-AICV/CattleFace-RGBT-benchmark%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CattleFace-RGBT%3A%20RGB-T%20Cattle%20Facial%20Landmark%20Benchmark&entry.906535625=Ethan%20Coffman%20and%20Reagan%20Clark%20and%20Nhat-Tan%20Bui%20and%20Trong%20Thang%20Pham%20and%20Beth%20Kegley%20and%20Jeremy%20G.%20Powell%20and%20Jiangchao%20Zhao%20and%20Ngan%20Le&entry.1292438233=%20%20To%20address%20this%20challenge%2C%20we%20introduce%20CattleFace-RGBT%2C%20a%20RGB-T%20Cattle%0AFacial%20Landmark%20dataset%20consisting%20of%202%2C300%20RGB-T%20image%20pairs%2C%20a%20total%20of%204%2C600%0Aimages.%20Creating%20a%20landmark%20dataset%20is%20time-consuming%2C%20but%20AI-assisted%0Aannotation%20can%20help.%20However%2C%20applying%20AI%20to%20thermal%20images%20is%20challenging%20due%0Ato%20suboptimal%20results%20from%20direct%20thermal%20training%20and%20infeasible%20RGB-thermal%0Aalignment%20due%20to%20different%20camera%20views.%20Therefore%2C%20we%20opt%20to%20transfer%20models%0Atrained%20on%20RGB%20to%20thermal%20images%20and%20refine%20them%20using%20our%20AI-assisted%0Aannotation%20tool%20following%20a%20semi-automatic%20annotation%20approach.%20Accurately%0Alocalizing%20facial%20key%20points%20on%20both%20RGB%20and%20thermal%20images%20enables%20us%20to%20not%0Aonly%20discern%20the%20cattle%27s%20respiratory%20signs%20but%20also%20measure%20temperatures%20to%0Aassess%20the%20animal%27s%20thermal%20state.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%0Afirst%20dataset%20for%20the%20cattle%20facial%20landmark%20on%20RGB-T%20images.%20We%20conduct%0Abenchmarking%20of%20the%20CattleFace-RGBT%20dataset%20across%20various%20backbone%0Aarchitectures%2C%20with%20the%20objective%20of%20establishing%20baselines%20for%20future%0Aresearch%2C%20analysis%2C%20and%20comparison.%20The%20dataset%20and%20models%20are%20at%0Ahttps%3A//github.com/UARK-AICV/CattleFace-RGBT-benchmark%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03431v1&entry.124074799=Read"},
{"title": "SPIN: Sparsifying and Integrating Internal Neurons in Large Language\n  Models for Text Classification", "author": "Difan Jiao and Yilun Liu and Zhenwei Tang and Daniel Matter and J\u00fcrgen Pfeffer and Ashton Anderson", "abstract": "  Among the many tasks that Large Language Models (LLMs) have revolutionized is\ntext classification. Current text classification paradigms, however, rely\nsolely on the output of the final layer in the LLM, with the rich information\ncontained in internal neurons largely untapped. In this study, we present SPIN:\na model-agnostic framework that sparsifies and integrates internal neurons of\nintermediate layers of LLMs for text classification. Specifically, SPIN\nsparsifies internal neurons by linear probing-based salient neuron selection\nlayer by layer, avoiding noise from unrelated neurons and ensuring efficiency.\nThe cross-layer salient neurons are then integrated to serve as multi-layered\nfeatures for the classification head. Extensive experimental results show our\nproposed SPIN significantly improves text classification accuracy, efficiency,\nand interpretability.\n", "link": "http://arxiv.org/abs/2311.15983v2", "date": "2024-06-05", "relevancy": 2.3538, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4632}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPIN%3A%20Sparsifying%20and%20Integrating%20Internal%20Neurons%20in%20Large%20Language%0A%20%20Models%20for%20Text%20Classification&body=Title%3A%20SPIN%3A%20Sparsifying%20and%20Integrating%20Internal%20Neurons%20in%20Large%20Language%0A%20%20Models%20for%20Text%20Classification%0AAuthor%3A%20Difan%20Jiao%20and%20Yilun%20Liu%20and%20Zhenwei%20Tang%20and%20Daniel%20Matter%20and%20J%C3%BCrgen%20Pfeffer%20and%20Ashton%20Anderson%0AAbstract%3A%20%20%20Among%20the%20many%20tasks%20that%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20is%0Atext%20classification.%20Current%20text%20classification%20paradigms%2C%20however%2C%20rely%0Asolely%20on%20the%20output%20of%20the%20final%20layer%20in%20the%20LLM%2C%20with%20the%20rich%20information%0Acontained%20in%20internal%20neurons%20largely%20untapped.%20In%20this%20study%2C%20we%20present%20SPIN%3A%0Aa%20model-agnostic%20framework%20that%20sparsifies%20and%20integrates%20internal%20neurons%20of%0Aintermediate%20layers%20of%20LLMs%20for%20text%20classification.%20Specifically%2C%20SPIN%0Asparsifies%20internal%20neurons%20by%20linear%20probing-based%20salient%20neuron%20selection%0Alayer%20by%20layer%2C%20avoiding%20noise%20from%20unrelated%20neurons%20and%20ensuring%20efficiency.%0AThe%20cross-layer%20salient%20neurons%20are%20then%20integrated%20to%20serve%20as%20multi-layered%0Afeatures%20for%20the%20classification%20head.%20Extensive%20experimental%20results%20show%20our%0Aproposed%20SPIN%20significantly%20improves%20text%20classification%20accuracy%2C%20efficiency%2C%0Aand%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPIN%253A%2520Sparsifying%2520and%2520Integrating%2520Internal%2520Neurons%2520in%2520Large%2520Language%250A%2520%2520Models%2520for%2520Text%2520Classification%26entry.906535625%3DDifan%2520Jiao%2520and%2520Yilun%2520Liu%2520and%2520Zhenwei%2520Tang%2520and%2520Daniel%2520Matter%2520and%2520J%25C3%25BCrgen%2520Pfeffer%2520and%2520Ashton%2520Anderson%26entry.1292438233%3D%2520%2520Among%2520the%2520many%2520tasks%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520is%250Atext%2520classification.%2520Current%2520text%2520classification%2520paradigms%252C%2520however%252C%2520rely%250Asolely%2520on%2520the%2520output%2520of%2520the%2520final%2520layer%2520in%2520the%2520LLM%252C%2520with%2520the%2520rich%2520information%250Acontained%2520in%2520internal%2520neurons%2520largely%2520untapped.%2520In%2520this%2520study%252C%2520we%2520present%2520SPIN%253A%250Aa%2520model-agnostic%2520framework%2520that%2520sparsifies%2520and%2520integrates%2520internal%2520neurons%2520of%250Aintermediate%2520layers%2520of%2520LLMs%2520for%2520text%2520classification.%2520Specifically%252C%2520SPIN%250Asparsifies%2520internal%2520neurons%2520by%2520linear%2520probing-based%2520salient%2520neuron%2520selection%250Alayer%2520by%2520layer%252C%2520avoiding%2520noise%2520from%2520unrelated%2520neurons%2520and%2520ensuring%2520efficiency.%250AThe%2520cross-layer%2520salient%2520neurons%2520are%2520then%2520integrated%2520to%2520serve%2520as%2520multi-layered%250Afeatures%2520for%2520the%2520classification%2520head.%2520Extensive%2520experimental%2520results%2520show%2520our%250Aproposed%2520SPIN%2520significantly%2520improves%2520text%2520classification%2520accuracy%252C%2520efficiency%252C%250Aand%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPIN%3A%20Sparsifying%20and%20Integrating%20Internal%20Neurons%20in%20Large%20Language%0A%20%20Models%20for%20Text%20Classification&entry.906535625=Difan%20Jiao%20and%20Yilun%20Liu%20and%20Zhenwei%20Tang%20and%20Daniel%20Matter%20and%20J%C3%BCrgen%20Pfeffer%20and%20Ashton%20Anderson&entry.1292438233=%20%20Among%20the%20many%20tasks%20that%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20is%0Atext%20classification.%20Current%20text%20classification%20paradigms%2C%20however%2C%20rely%0Asolely%20on%20the%20output%20of%20the%20final%20layer%20in%20the%20LLM%2C%20with%20the%20rich%20information%0Acontained%20in%20internal%20neurons%20largely%20untapped.%20In%20this%20study%2C%20we%20present%20SPIN%3A%0Aa%20model-agnostic%20framework%20that%20sparsifies%20and%20integrates%20internal%20neurons%20of%0Aintermediate%20layers%20of%20LLMs%20for%20text%20classification.%20Specifically%2C%20SPIN%0Asparsifies%20internal%20neurons%20by%20linear%20probing-based%20salient%20neuron%20selection%0Alayer%20by%20layer%2C%20avoiding%20noise%20from%20unrelated%20neurons%20and%20ensuring%20efficiency.%0AThe%20cross-layer%20salient%20neurons%20are%20then%20integrated%20to%20serve%20as%20multi-layered%0Afeatures%20for%20the%20classification%20head.%20Extensive%20experimental%20results%20show%20our%0Aproposed%20SPIN%20significantly%20improves%20text%20classification%20accuracy%2C%20efficiency%2C%0Aand%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15983v2&entry.124074799=Read"},
{"title": "ChatLang-8: An LLM-Based Synthetic Data Generation Framework for\n  Grammatical Error Correction", "author": "Jeiyoon Park and Chanjun Park and Heuiseok Lim", "abstract": "  We explore and improve the capabilities of LLMs to generate data for\ngrammatical error correction (GEC). When merely producing parallel sentences,\ntheir patterns are too simplistic to be valuable as a corpus. To address this\nissue, we propose an automated framework that includes a Subject Selector,\nGrammar Selector, Prompt Manager, and Evaluator. Additionally, we introduce a\nnew dataset for GEC tasks, named \\textbf{ChatLang-8}, which encompasses eight\ntypes of subject nouns and 23 types of grammar. It consists of 1 million pairs\nfeaturing human-like grammatical errors. Our experiments reveal that ChatLang-8\nexhibits a more uniform pattern composition compared to existing GEC datasets.\nFurthermore, we observe improved model performance when using ChatLang-8\ninstead of existing GEC datasets. The experimental results suggest that our\nframework and ChatLang-8 are valuable resources for enhancing ChatGPT's data\ngeneration capabilities.\n", "link": "http://arxiv.org/abs/2406.03202v1", "date": "2024-06-05", "relevancy": 2.3217, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4894}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4726}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatLang-8%3A%20An%20LLM-Based%20Synthetic%20Data%20Generation%20Framework%20for%0A%20%20Grammatical%20Error%20Correction&body=Title%3A%20ChatLang-8%3A%20An%20LLM-Based%20Synthetic%20Data%20Generation%20Framework%20for%0A%20%20Grammatical%20Error%20Correction%0AAuthor%3A%20Jeiyoon%20Park%20and%20Chanjun%20Park%20and%20Heuiseok%20Lim%0AAbstract%3A%20%20%20We%20explore%20and%20improve%20the%20capabilities%20of%20LLMs%20to%20generate%20data%20for%0Agrammatical%20error%20correction%20%28GEC%29.%20When%20merely%20producing%20parallel%20sentences%2C%0Atheir%20patterns%20are%20too%20simplistic%20to%20be%20valuable%20as%20a%20corpus.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20automated%20framework%20that%20includes%20a%20Subject%20Selector%2C%0AGrammar%20Selector%2C%20Prompt%20Manager%2C%20and%20Evaluator.%20Additionally%2C%20we%20introduce%20a%0Anew%20dataset%20for%20GEC%20tasks%2C%20named%20%5Ctextbf%7BChatLang-8%7D%2C%20which%20encompasses%20eight%0Atypes%20of%20subject%20nouns%20and%2023%20types%20of%20grammar.%20It%20consists%20of%201%20million%20pairs%0Afeaturing%20human-like%20grammatical%20errors.%20Our%20experiments%20reveal%20that%20ChatLang-8%0Aexhibits%20a%20more%20uniform%20pattern%20composition%20compared%20to%20existing%20GEC%20datasets.%0AFurthermore%2C%20we%20observe%20improved%20model%20performance%20when%20using%20ChatLang-8%0Ainstead%20of%20existing%20GEC%20datasets.%20The%20experimental%20results%20suggest%20that%20our%0Aframework%20and%20ChatLang-8%20are%20valuable%20resources%20for%20enhancing%20ChatGPT%27s%20data%0Ageneration%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatLang-8%253A%2520An%2520LLM-Based%2520Synthetic%2520Data%2520Generation%2520Framework%2520for%250A%2520%2520Grammatical%2520Error%2520Correction%26entry.906535625%3DJeiyoon%2520Park%2520and%2520Chanjun%2520Park%2520and%2520Heuiseok%2520Lim%26entry.1292438233%3D%2520%2520We%2520explore%2520and%2520improve%2520the%2520capabilities%2520of%2520LLMs%2520to%2520generate%2520data%2520for%250Agrammatical%2520error%2520correction%2520%2528GEC%2529.%2520When%2520merely%2520producing%2520parallel%2520sentences%252C%250Atheir%2520patterns%2520are%2520too%2520simplistic%2520to%2520be%2520valuable%2520as%2520a%2520corpus.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520an%2520automated%2520framework%2520that%2520includes%2520a%2520Subject%2520Selector%252C%250AGrammar%2520Selector%252C%2520Prompt%2520Manager%252C%2520and%2520Evaluator.%2520Additionally%252C%2520we%2520introduce%2520a%250Anew%2520dataset%2520for%2520GEC%2520tasks%252C%2520named%2520%255Ctextbf%257BChatLang-8%257D%252C%2520which%2520encompasses%2520eight%250Atypes%2520of%2520subject%2520nouns%2520and%252023%2520types%2520of%2520grammar.%2520It%2520consists%2520of%25201%2520million%2520pairs%250Afeaturing%2520human-like%2520grammatical%2520errors.%2520Our%2520experiments%2520reveal%2520that%2520ChatLang-8%250Aexhibits%2520a%2520more%2520uniform%2520pattern%2520composition%2520compared%2520to%2520existing%2520GEC%2520datasets.%250AFurthermore%252C%2520we%2520observe%2520improved%2520model%2520performance%2520when%2520using%2520ChatLang-8%250Ainstead%2520of%2520existing%2520GEC%2520datasets.%2520The%2520experimental%2520results%2520suggest%2520that%2520our%250Aframework%2520and%2520ChatLang-8%2520are%2520valuable%2520resources%2520for%2520enhancing%2520ChatGPT%2527s%2520data%250Ageneration%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatLang-8%3A%20An%20LLM-Based%20Synthetic%20Data%20Generation%20Framework%20for%0A%20%20Grammatical%20Error%20Correction&entry.906535625=Jeiyoon%20Park%20and%20Chanjun%20Park%20and%20Heuiseok%20Lim&entry.1292438233=%20%20We%20explore%20and%20improve%20the%20capabilities%20of%20LLMs%20to%20generate%20data%20for%0Agrammatical%20error%20correction%20%28GEC%29.%20When%20merely%20producing%20parallel%20sentences%2C%0Atheir%20patterns%20are%20too%20simplistic%20to%20be%20valuable%20as%20a%20corpus.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20automated%20framework%20that%20includes%20a%20Subject%20Selector%2C%0AGrammar%20Selector%2C%20Prompt%20Manager%2C%20and%20Evaluator.%20Additionally%2C%20we%20introduce%20a%0Anew%20dataset%20for%20GEC%20tasks%2C%20named%20%5Ctextbf%7BChatLang-8%7D%2C%20which%20encompasses%20eight%0Atypes%20of%20subject%20nouns%20and%2023%20types%20of%20grammar.%20It%20consists%20of%201%20million%20pairs%0Afeaturing%20human-like%20grammatical%20errors.%20Our%20experiments%20reveal%20that%20ChatLang-8%0Aexhibits%20a%20more%20uniform%20pattern%20composition%20compared%20to%20existing%20GEC%20datasets.%0AFurthermore%2C%20we%20observe%20improved%20model%20performance%20when%20using%20ChatLang-8%0Ainstead%20of%20existing%20GEC%20datasets.%20The%20experimental%20results%20suggest%20that%20our%0Aframework%20and%20ChatLang-8%20are%20valuable%20resources%20for%20enhancing%20ChatGPT%27s%20data%0Ageneration%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03202v1&entry.124074799=Read"},
{"title": "L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap\n  Multiview Point Cloud Registration", "author": "Yibo Liu and Jinjun Shan and Amaldev Haridevan and Shuo Zhang and Kejian Lin", "abstract": "  Point cloud registration is a prerequisite for many applications in computer\nvision and robotics. Most existing methods focus on pairwise registration of\ntwo point clouds with high overlap. Although there have been some methods for\nlow overlap cases, they struggle in degraded scenarios. This paper introduces a\nnovel framework named L-PR, designed to register unordered low overlap\nmultiview point clouds leveraging LiDAR fiducial markers. We refer to them as\nLiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco\nmarkers, thin sheets of paper that do not affect the 3D geometry of the\nenvironment. We first propose an improved adaptive threshold marker detection\nmethod to provide robust detection results when the viewpoints among point\nclouds change dramatically. Then, we formulate the unordered multiview point\ncloud registration problem as a maximum a-posteriori (MAP) problem and develop\na framework consisting of two levels of graphs to address it. The first-level\ngraph, constructed as a weighted graph, is designed to efficiently and\noptimally infer initial values of scan poses from the unordered set. The\nsecond-level graph is constructed as a factor graph. By globally optimizing the\nvariables on the graph, including scan poses, marker poses, and marker corner\npositions, we tackle the MAP problem. We conduct qualitative and quantitative\nexperiments to demonstrate that the proposed method exhibits superiority over\ncompetitors in four aspects: registration accuracy, instance reconstruction\nquality, localization accuracy, and robustness to the degraded scene. To\nbenefit the community, we open-source our method and dataset at\nhttps://github.com/yorklyb/LiDAR-SFM.\n", "link": "http://arxiv.org/abs/2406.03298v1", "date": "2024-06-05", "relevancy": 2.3167, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6118}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5696}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L-PR%3A%20Exploiting%20LiDAR%20Fiducial%20Marker%20for%20Unordered%20Low%20Overlap%0A%20%20Multiview%20Point%20Cloud%20Registration&body=Title%3A%20L-PR%3A%20Exploiting%20LiDAR%20Fiducial%20Marker%20for%20Unordered%20Low%20Overlap%0A%20%20Multiview%20Point%20Cloud%20Registration%0AAuthor%3A%20Yibo%20Liu%20and%20Jinjun%20Shan%20and%20Amaldev%20Haridevan%20and%20Shuo%20Zhang%20and%20Kejian%20Lin%0AAbstract%3A%20%20%20Point%20cloud%20registration%20is%20a%20prerequisite%20for%20many%20applications%20in%20computer%0Avision%20and%20robotics.%20Most%20existing%20methods%20focus%20on%20pairwise%20registration%20of%0Atwo%20point%20clouds%20with%20high%20overlap.%20Although%20there%20have%20been%20some%20methods%20for%0Alow%20overlap%20cases%2C%20they%20struggle%20in%20degraded%20scenarios.%20This%20paper%20introduces%20a%0Anovel%20framework%20named%20L-PR%2C%20designed%20to%20register%20unordered%20low%20overlap%0Amultiview%20point%20clouds%20leveraging%20LiDAR%20fiducial%20markers.%20We%20refer%20to%20them%20as%0ALiDAR%20fiducial%20markers%2C%20but%20they%20are%20the%20same%20as%20the%20popular%20AprilTag%20and%20ArUco%0Amarkers%2C%20thin%20sheets%20of%20paper%20that%20do%20not%20affect%20the%203D%20geometry%20of%20the%0Aenvironment.%20We%20first%20propose%20an%20improved%20adaptive%20threshold%20marker%20detection%0Amethod%20to%20provide%20robust%20detection%20results%20when%20the%20viewpoints%20among%20point%0Aclouds%20change%20dramatically.%20Then%2C%20we%20formulate%20the%20unordered%20multiview%20point%0Acloud%20registration%20problem%20as%20a%20maximum%20a-posteriori%20%28MAP%29%20problem%20and%20develop%0Aa%20framework%20consisting%20of%20two%20levels%20of%20graphs%20to%20address%20it.%20The%20first-level%0Agraph%2C%20constructed%20as%20a%20weighted%20graph%2C%20is%20designed%20to%20efficiently%20and%0Aoptimally%20infer%20initial%20values%20of%20scan%20poses%20from%20the%20unordered%20set.%20The%0Asecond-level%20graph%20is%20constructed%20as%20a%20factor%20graph.%20By%20globally%20optimizing%20the%0Avariables%20on%20the%20graph%2C%20including%20scan%20poses%2C%20marker%20poses%2C%20and%20marker%20corner%0Apositions%2C%20we%20tackle%20the%20MAP%20problem.%20We%20conduct%20qualitative%20and%20quantitative%0Aexperiments%20to%20demonstrate%20that%20the%20proposed%20method%20exhibits%20superiority%20over%0Acompetitors%20in%20four%20aspects%3A%20registration%20accuracy%2C%20instance%20reconstruction%0Aquality%2C%20localization%20accuracy%2C%20and%20robustness%20to%20the%20degraded%20scene.%20To%0Abenefit%20the%20community%2C%20we%20open-source%20our%20method%20and%20dataset%20at%0Ahttps%3A//github.com/yorklyb/LiDAR-SFM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL-PR%253A%2520Exploiting%2520LiDAR%2520Fiducial%2520Marker%2520for%2520Unordered%2520Low%2520Overlap%250A%2520%2520Multiview%2520Point%2520Cloud%2520Registration%26entry.906535625%3DYibo%2520Liu%2520and%2520Jinjun%2520Shan%2520and%2520Amaldev%2520Haridevan%2520and%2520Shuo%2520Zhang%2520and%2520Kejian%2520Lin%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520is%2520a%2520prerequisite%2520for%2520many%2520applications%2520in%2520computer%250Avision%2520and%2520robotics.%2520Most%2520existing%2520methods%2520focus%2520on%2520pairwise%2520registration%2520of%250Atwo%2520point%2520clouds%2520with%2520high%2520overlap.%2520Although%2520there%2520have%2520been%2520some%2520methods%2520for%250Alow%2520overlap%2520cases%252C%2520they%2520struggle%2520in%2520degraded%2520scenarios.%2520This%2520paper%2520introduces%2520a%250Anovel%2520framework%2520named%2520L-PR%252C%2520designed%2520to%2520register%2520unordered%2520low%2520overlap%250Amultiview%2520point%2520clouds%2520leveraging%2520LiDAR%2520fiducial%2520markers.%2520We%2520refer%2520to%2520them%2520as%250ALiDAR%2520fiducial%2520markers%252C%2520but%2520they%2520are%2520the%2520same%2520as%2520the%2520popular%2520AprilTag%2520and%2520ArUco%250Amarkers%252C%2520thin%2520sheets%2520of%2520paper%2520that%2520do%2520not%2520affect%2520the%25203D%2520geometry%2520of%2520the%250Aenvironment.%2520We%2520first%2520propose%2520an%2520improved%2520adaptive%2520threshold%2520marker%2520detection%250Amethod%2520to%2520provide%2520robust%2520detection%2520results%2520when%2520the%2520viewpoints%2520among%2520point%250Aclouds%2520change%2520dramatically.%2520Then%252C%2520we%2520formulate%2520the%2520unordered%2520multiview%2520point%250Acloud%2520registration%2520problem%2520as%2520a%2520maximum%2520a-posteriori%2520%2528MAP%2529%2520problem%2520and%2520develop%250Aa%2520framework%2520consisting%2520of%2520two%2520levels%2520of%2520graphs%2520to%2520address%2520it.%2520The%2520first-level%250Agraph%252C%2520constructed%2520as%2520a%2520weighted%2520graph%252C%2520is%2520designed%2520to%2520efficiently%2520and%250Aoptimally%2520infer%2520initial%2520values%2520of%2520scan%2520poses%2520from%2520the%2520unordered%2520set.%2520The%250Asecond-level%2520graph%2520is%2520constructed%2520as%2520a%2520factor%2520graph.%2520By%2520globally%2520optimizing%2520the%250Avariables%2520on%2520the%2520graph%252C%2520including%2520scan%2520poses%252C%2520marker%2520poses%252C%2520and%2520marker%2520corner%250Apositions%252C%2520we%2520tackle%2520the%2520MAP%2520problem.%2520We%2520conduct%2520qualitative%2520and%2520quantitative%250Aexperiments%2520to%2520demonstrate%2520that%2520the%2520proposed%2520method%2520exhibits%2520superiority%2520over%250Acompetitors%2520in%2520four%2520aspects%253A%2520registration%2520accuracy%252C%2520instance%2520reconstruction%250Aquality%252C%2520localization%2520accuracy%252C%2520and%2520robustness%2520to%2520the%2520degraded%2520scene.%2520To%250Abenefit%2520the%2520community%252C%2520we%2520open-source%2520our%2520method%2520and%2520dataset%2520at%250Ahttps%253A//github.com/yorklyb/LiDAR-SFM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L-PR%3A%20Exploiting%20LiDAR%20Fiducial%20Marker%20for%20Unordered%20Low%20Overlap%0A%20%20Multiview%20Point%20Cloud%20Registration&entry.906535625=Yibo%20Liu%20and%20Jinjun%20Shan%20and%20Amaldev%20Haridevan%20and%20Shuo%20Zhang%20and%20Kejian%20Lin&entry.1292438233=%20%20Point%20cloud%20registration%20is%20a%20prerequisite%20for%20many%20applications%20in%20computer%0Avision%20and%20robotics.%20Most%20existing%20methods%20focus%20on%20pairwise%20registration%20of%0Atwo%20point%20clouds%20with%20high%20overlap.%20Although%20there%20have%20been%20some%20methods%20for%0Alow%20overlap%20cases%2C%20they%20struggle%20in%20degraded%20scenarios.%20This%20paper%20introduces%20a%0Anovel%20framework%20named%20L-PR%2C%20designed%20to%20register%20unordered%20low%20overlap%0Amultiview%20point%20clouds%20leveraging%20LiDAR%20fiducial%20markers.%20We%20refer%20to%20them%20as%0ALiDAR%20fiducial%20markers%2C%20but%20they%20are%20the%20same%20as%20the%20popular%20AprilTag%20and%20ArUco%0Amarkers%2C%20thin%20sheets%20of%20paper%20that%20do%20not%20affect%20the%203D%20geometry%20of%20the%0Aenvironment.%20We%20first%20propose%20an%20improved%20adaptive%20threshold%20marker%20detection%0Amethod%20to%20provide%20robust%20detection%20results%20when%20the%20viewpoints%20among%20point%0Aclouds%20change%20dramatically.%20Then%2C%20we%20formulate%20the%20unordered%20multiview%20point%0Acloud%20registration%20problem%20as%20a%20maximum%20a-posteriori%20%28MAP%29%20problem%20and%20develop%0Aa%20framework%20consisting%20of%20two%20levels%20of%20graphs%20to%20address%20it.%20The%20first-level%0Agraph%2C%20constructed%20as%20a%20weighted%20graph%2C%20is%20designed%20to%20efficiently%20and%0Aoptimally%20infer%20initial%20values%20of%20scan%20poses%20from%20the%20unordered%20set.%20The%0Asecond-level%20graph%20is%20constructed%20as%20a%20factor%20graph.%20By%20globally%20optimizing%20the%0Avariables%20on%20the%20graph%2C%20including%20scan%20poses%2C%20marker%20poses%2C%20and%20marker%20corner%0Apositions%2C%20we%20tackle%20the%20MAP%20problem.%20We%20conduct%20qualitative%20and%20quantitative%0Aexperiments%20to%20demonstrate%20that%20the%20proposed%20method%20exhibits%20superiority%20over%0Acompetitors%20in%20four%20aspects%3A%20registration%20accuracy%2C%20instance%20reconstruction%0Aquality%2C%20localization%20accuracy%2C%20and%20robustness%20to%20the%20degraded%20scene.%20To%0Abenefit%20the%20community%2C%20we%20open-source%20our%20method%20and%20dataset%20at%0Ahttps%3A//github.com/yorklyb/LiDAR-SFM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03298v1&entry.124074799=Read"},
{"title": "Future Directions in the Theory of Graph Machine Learning", "author": "Christopher Morris and Fabrizio Frasca and Nadav Dym and Haggai Maron and \u0130smail \u0130lkan Ceylan and Ron Levie and Derek Lim and Michael Bronstein and Martin Grohe and Stefanie Jegelka", "abstract": "  Machine learning on graphs, especially using graph neural networks (GNNs),\nhas seen a surge in interest due to the wide availability of graph data across\na broad spectrum of disciplines, from life to social and engineering sciences.\nDespite their practical success, our theoretical understanding of the\nproperties of GNNs remains highly incomplete. Recent theoretical advancements\nprimarily focus on elucidating the coarse-grained expressive power of GNNs,\npredominantly employing combinatorial techniques. However, these studies do not\nperfectly align with practice, particularly in understanding the generalization\nbehavior of GNNs when trained with stochastic first-order optimization\ntechniques. In this position paper, we argue that the graph machine learning\ncommunity needs to shift its attention to developing a balanced theory of graph\nmachine learning, focusing on a more thorough understanding of the interplay of\nexpressive power, generalization, and optimization.\n", "link": "http://arxiv.org/abs/2402.02287v3", "date": "2024-06-05", "relevancy": 2.3082, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4802}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4619}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Future%20Directions%20in%20the%20Theory%20of%20Graph%20Machine%20Learning&body=Title%3A%20Future%20Directions%20in%20the%20Theory%20of%20Graph%20Machine%20Learning%0AAuthor%3A%20Christopher%20Morris%20and%20Fabrizio%20Frasca%20and%20Nadav%20Dym%20and%20Haggai%20Maron%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Ron%20Levie%20and%20Derek%20Lim%20and%20Michael%20Bronstein%20and%20Martin%20Grohe%20and%20Stefanie%20Jegelka%0AAbstract%3A%20%20%20Machine%20learning%20on%20graphs%2C%20especially%20using%20graph%20neural%20networks%20%28GNNs%29%2C%0Ahas%20seen%20a%20surge%20in%20interest%20due%20to%20the%20wide%20availability%20of%20graph%20data%20across%0Aa%20broad%20spectrum%20of%20disciplines%2C%20from%20life%20to%20social%20and%20engineering%20sciences.%0ADespite%20their%20practical%20success%2C%20our%20theoretical%20understanding%20of%20the%0Aproperties%20of%20GNNs%20remains%20highly%20incomplete.%20Recent%20theoretical%20advancements%0Aprimarily%20focus%20on%20elucidating%20the%20coarse-grained%20expressive%20power%20of%20GNNs%2C%0Apredominantly%20employing%20combinatorial%20techniques.%20However%2C%20these%20studies%20do%20not%0Aperfectly%20align%20with%20practice%2C%20particularly%20in%20understanding%20the%20generalization%0Abehavior%20of%20GNNs%20when%20trained%20with%20stochastic%20first-order%20optimization%0Atechniques.%20In%20this%20position%20paper%2C%20we%20argue%20that%20the%20graph%20machine%20learning%0Acommunity%20needs%20to%20shift%20its%20attention%20to%20developing%20a%20balanced%20theory%20of%20graph%0Amachine%20learning%2C%20focusing%20on%20a%20more%20thorough%20understanding%20of%20the%20interplay%20of%0Aexpressive%20power%2C%20generalization%2C%20and%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02287v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuture%2520Directions%2520in%2520the%2520Theory%2520of%2520Graph%2520Machine%2520Learning%26entry.906535625%3DChristopher%2520Morris%2520and%2520Fabrizio%2520Frasca%2520and%2520Nadav%2520Dym%2520and%2520Haggai%2520Maron%2520and%2520%25C4%25B0smail%2520%25C4%25B0lkan%2520Ceylan%2520and%2520Ron%2520Levie%2520and%2520Derek%2520Lim%2520and%2520Michael%2520Bronstein%2520and%2520Martin%2520Grohe%2520and%2520Stefanie%2520Jegelka%26entry.1292438233%3D%2520%2520Machine%2520learning%2520on%2520graphs%252C%2520especially%2520using%2520graph%2520neural%2520networks%2520%2528GNNs%2529%252C%250Ahas%2520seen%2520a%2520surge%2520in%2520interest%2520due%2520to%2520the%2520wide%2520availability%2520of%2520graph%2520data%2520across%250Aa%2520broad%2520spectrum%2520of%2520disciplines%252C%2520from%2520life%2520to%2520social%2520and%2520engineering%2520sciences.%250ADespite%2520their%2520practical%2520success%252C%2520our%2520theoretical%2520understanding%2520of%2520the%250Aproperties%2520of%2520GNNs%2520remains%2520highly%2520incomplete.%2520Recent%2520theoretical%2520advancements%250Aprimarily%2520focus%2520on%2520elucidating%2520the%2520coarse-grained%2520expressive%2520power%2520of%2520GNNs%252C%250Apredominantly%2520employing%2520combinatorial%2520techniques.%2520However%252C%2520these%2520studies%2520do%2520not%250Aperfectly%2520align%2520with%2520practice%252C%2520particularly%2520in%2520understanding%2520the%2520generalization%250Abehavior%2520of%2520GNNs%2520when%2520trained%2520with%2520stochastic%2520first-order%2520optimization%250Atechniques.%2520In%2520this%2520position%2520paper%252C%2520we%2520argue%2520that%2520the%2520graph%2520machine%2520learning%250Acommunity%2520needs%2520to%2520shift%2520its%2520attention%2520to%2520developing%2520a%2520balanced%2520theory%2520of%2520graph%250Amachine%2520learning%252C%2520focusing%2520on%2520a%2520more%2520thorough%2520understanding%2520of%2520the%2520interplay%2520of%250Aexpressive%2520power%252C%2520generalization%252C%2520and%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02287v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Future%20Directions%20in%20the%20Theory%20of%20Graph%20Machine%20Learning&entry.906535625=Christopher%20Morris%20and%20Fabrizio%20Frasca%20and%20Nadav%20Dym%20and%20Haggai%20Maron%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Ron%20Levie%20and%20Derek%20Lim%20and%20Michael%20Bronstein%20and%20Martin%20Grohe%20and%20Stefanie%20Jegelka&entry.1292438233=%20%20Machine%20learning%20on%20graphs%2C%20especially%20using%20graph%20neural%20networks%20%28GNNs%29%2C%0Ahas%20seen%20a%20surge%20in%20interest%20due%20to%20the%20wide%20availability%20of%20graph%20data%20across%0Aa%20broad%20spectrum%20of%20disciplines%2C%20from%20life%20to%20social%20and%20engineering%20sciences.%0ADespite%20their%20practical%20success%2C%20our%20theoretical%20understanding%20of%20the%0Aproperties%20of%20GNNs%20remains%20highly%20incomplete.%20Recent%20theoretical%20advancements%0Aprimarily%20focus%20on%20elucidating%20the%20coarse-grained%20expressive%20power%20of%20GNNs%2C%0Apredominantly%20employing%20combinatorial%20techniques.%20However%2C%20these%20studies%20do%20not%0Aperfectly%20align%20with%20practice%2C%20particularly%20in%20understanding%20the%20generalization%0Abehavior%20of%20GNNs%20when%20trained%20with%20stochastic%20first-order%20optimization%0Atechniques.%20In%20this%20position%20paper%2C%20we%20argue%20that%20the%20graph%20machine%20learning%0Acommunity%20needs%20to%20shift%20its%20attention%20to%20developing%20a%20balanced%20theory%20of%20graph%0Amachine%20learning%2C%20focusing%20on%20a%20more%20thorough%20understanding%20of%20the%20interplay%20of%0Aexpressive%20power%2C%20generalization%2C%20and%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02287v3&entry.124074799=Read"},
{"title": "SuperFormer: Volumetric Transformer Architectures for MRI\n  Super-Resolution", "author": "Cristhian Forigua and Maria Escobar and Pablo Arbelaez", "abstract": "  This paper presents a novel framework for processing volumetric medical\ninformation using Visual Transformers (ViTs). First, We extend the\nstate-of-the-art Swin Transformer model to the 3D medical domain. Second, we\npropose a new approach for processing volumetric information and encoding\nposition in ViTs for 3D applications. We instantiate the proposed framework and\npresent SuperFormer, a volumetric transformer-based approach for Magnetic\nResonance Imaging (MRI) Super-Resolution. Our method leverages the 3D\ninformation of the MRI domain and uses a local self-attention mechanism with a\n3D relative positional encoding to recover anatomical details. In addition, our\napproach takes advantage of multi-domain information from volume and feature\ndomains and fuses them to reconstruct the High-Resolution MRI. We perform an\nextensive validation on the Human Connectome Project dataset and demonstrate\nthe superiority of volumetric transformers over 3D CNN-based methods. Our code\nand pretrained models are available at\nhttps://github.com/BCV-Uniandes/SuperFormer.\n", "link": "http://arxiv.org/abs/2406.03359v1", "date": "2024-06-05", "relevancy": 2.3002, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5914}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5718}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperFormer%3A%20Volumetric%20Transformer%20Architectures%20for%20MRI%0A%20%20Super-Resolution&body=Title%3A%20SuperFormer%3A%20Volumetric%20Transformer%20Architectures%20for%20MRI%0A%20%20Super-Resolution%0AAuthor%3A%20Cristhian%20Forigua%20and%20Maria%20Escobar%20and%20Pablo%20Arbelaez%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20framework%20for%20processing%20volumetric%20medical%0Ainformation%20using%20Visual%20Transformers%20%28ViTs%29.%20First%2C%20We%20extend%20the%0Astate-of-the-art%20Swin%20Transformer%20model%20to%20the%203D%20medical%20domain.%20Second%2C%20we%0Apropose%20a%20new%20approach%20for%20processing%20volumetric%20information%20and%20encoding%0Aposition%20in%20ViTs%20for%203D%20applications.%20We%20instantiate%20the%20proposed%20framework%20and%0Apresent%20SuperFormer%2C%20a%20volumetric%20transformer-based%20approach%20for%20Magnetic%0AResonance%20Imaging%20%28MRI%29%20Super-Resolution.%20Our%20method%20leverages%20the%203D%0Ainformation%20of%20the%20MRI%20domain%20and%20uses%20a%20local%20self-attention%20mechanism%20with%20a%0A3D%20relative%20positional%20encoding%20to%20recover%20anatomical%20details.%20In%20addition%2C%20our%0Aapproach%20takes%20advantage%20of%20multi-domain%20information%20from%20volume%20and%20feature%0Adomains%20and%20fuses%20them%20to%20reconstruct%20the%20High-Resolution%20MRI.%20We%20perform%20an%0Aextensive%20validation%20on%20the%20Human%20Connectome%20Project%20dataset%20and%20demonstrate%0Athe%20superiority%20of%20volumetric%20transformers%20over%203D%20CNN-based%20methods.%20Our%20code%0Aand%20pretrained%20models%20are%20available%20at%0Ahttps%3A//github.com/BCV-Uniandes/SuperFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperFormer%253A%2520Volumetric%2520Transformer%2520Architectures%2520for%2520MRI%250A%2520%2520Super-Resolution%26entry.906535625%3DCristhian%2520Forigua%2520and%2520Maria%2520Escobar%2520and%2520Pablo%2520Arbelaez%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520for%2520processing%2520volumetric%2520medical%250Ainformation%2520using%2520Visual%2520Transformers%2520%2528ViTs%2529.%2520First%252C%2520We%2520extend%2520the%250Astate-of-the-art%2520Swin%2520Transformer%2520model%2520to%2520the%25203D%2520medical%2520domain.%2520Second%252C%2520we%250Apropose%2520a%2520new%2520approach%2520for%2520processing%2520volumetric%2520information%2520and%2520encoding%250Aposition%2520in%2520ViTs%2520for%25203D%2520applications.%2520We%2520instantiate%2520the%2520proposed%2520framework%2520and%250Apresent%2520SuperFormer%252C%2520a%2520volumetric%2520transformer-based%2520approach%2520for%2520Magnetic%250AResonance%2520Imaging%2520%2528MRI%2529%2520Super-Resolution.%2520Our%2520method%2520leverages%2520the%25203D%250Ainformation%2520of%2520the%2520MRI%2520domain%2520and%2520uses%2520a%2520local%2520self-attention%2520mechanism%2520with%2520a%250A3D%2520relative%2520positional%2520encoding%2520to%2520recover%2520anatomical%2520details.%2520In%2520addition%252C%2520our%250Aapproach%2520takes%2520advantage%2520of%2520multi-domain%2520information%2520from%2520volume%2520and%2520feature%250Adomains%2520and%2520fuses%2520them%2520to%2520reconstruct%2520the%2520High-Resolution%2520MRI.%2520We%2520perform%2520an%250Aextensive%2520validation%2520on%2520the%2520Human%2520Connectome%2520Project%2520dataset%2520and%2520demonstrate%250Athe%2520superiority%2520of%2520volumetric%2520transformers%2520over%25203D%2520CNN-based%2520methods.%2520Our%2520code%250Aand%2520pretrained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/BCV-Uniandes/SuperFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperFormer%3A%20Volumetric%20Transformer%20Architectures%20for%20MRI%0A%20%20Super-Resolution&entry.906535625=Cristhian%20Forigua%20and%20Maria%20Escobar%20and%20Pablo%20Arbelaez&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20framework%20for%20processing%20volumetric%20medical%0Ainformation%20using%20Visual%20Transformers%20%28ViTs%29.%20First%2C%20We%20extend%20the%0Astate-of-the-art%20Swin%20Transformer%20model%20to%20the%203D%20medical%20domain.%20Second%2C%20we%0Apropose%20a%20new%20approach%20for%20processing%20volumetric%20information%20and%20encoding%0Aposition%20in%20ViTs%20for%203D%20applications.%20We%20instantiate%20the%20proposed%20framework%20and%0Apresent%20SuperFormer%2C%20a%20volumetric%20transformer-based%20approach%20for%20Magnetic%0AResonance%20Imaging%20%28MRI%29%20Super-Resolution.%20Our%20method%20leverages%20the%203D%0Ainformation%20of%20the%20MRI%20domain%20and%20uses%20a%20local%20self-attention%20mechanism%20with%20a%0A3D%20relative%20positional%20encoding%20to%20recover%20anatomical%20details.%20In%20addition%2C%20our%0Aapproach%20takes%20advantage%20of%20multi-domain%20information%20from%20volume%20and%20feature%0Adomains%20and%20fuses%20them%20to%20reconstruct%20the%20High-Resolution%20MRI.%20We%20perform%20an%0Aextensive%20validation%20on%20the%20Human%20Connectome%20Project%20dataset%20and%20demonstrate%0Athe%20superiority%20of%20volumetric%20transformers%20over%203D%20CNN-based%20methods.%20Our%20code%0Aand%20pretrained%20models%20are%20available%20at%0Ahttps%3A//github.com/BCV-Uniandes/SuperFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03359v1&entry.124074799=Read"},
{"title": "FILS: Self-Supervised Video Feature Prediction In Semantic Language\n  Space", "author": "Mona Ahmadian and Frank Guerin and Andrew Gilbert", "abstract": "  This paper demonstrates a self-supervised approach for learning semantic\nvideo representations. Recent vision studies show that a masking strategy for\nvision and natural language supervision has contributed to developing\ntransferable visual pretraining. Our goal is to achieve a more semantic video\nrepresentation by leveraging the text related to the video content during the\npretraining in a fully self-supervised manner. To this end, we present FILS, a\nnovel self-supervised video Feature prediction In semantic Language Space\n(FILS). The vision model can capture valuable structured information by\ncorrectly predicting masked feature semantics in language space. It is learned\nusing a patch-wise video-text contrastive strategy, in which the text\nrepresentations act as prototypes for transforming vision features into a\nlanguage space, which are then used as targets for semantically meaningful\nfeature prediction using our masked encoder-decoder structure. FILS\ndemonstrates remarkable transferability on downstream action recognition tasks,\nachieving state-of-the-art on challenging egocentric datasets, like\nEpic-Kitchens, Something-SomethingV2, Charades-Ego, and EGTEA, using ViT-Base.\nOur efficient method requires less computation and smaller batches compared to\nprevious works.\n", "link": "http://arxiv.org/abs/2406.03447v1", "date": "2024-06-05", "relevancy": 2.2992, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5738}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FILS%3A%20Self-Supervised%20Video%20Feature%20Prediction%20In%20Semantic%20Language%0A%20%20Space&body=Title%3A%20FILS%3A%20Self-Supervised%20Video%20Feature%20Prediction%20In%20Semantic%20Language%0A%20%20Space%0AAuthor%3A%20Mona%20Ahmadian%20and%20Frank%20Guerin%20and%20Andrew%20Gilbert%0AAbstract%3A%20%20%20This%20paper%20demonstrates%20a%20self-supervised%20approach%20for%20learning%20semantic%0Avideo%20representations.%20Recent%20vision%20studies%20show%20that%20a%20masking%20strategy%20for%0Avision%20and%20natural%20language%20supervision%20has%20contributed%20to%20developing%0Atransferable%20visual%20pretraining.%20Our%20goal%20is%20to%20achieve%20a%20more%20semantic%20video%0Arepresentation%20by%20leveraging%20the%20text%20related%20to%20the%20video%20content%20during%20the%0Apretraining%20in%20a%20fully%20self-supervised%20manner.%20To%20this%20end%2C%20we%20present%20FILS%2C%20a%0Anovel%20self-supervised%20video%20Feature%20prediction%20In%20semantic%20Language%20Space%0A%28FILS%29.%20The%20vision%20model%20can%20capture%20valuable%20structured%20information%20by%0Acorrectly%20predicting%20masked%20feature%20semantics%20in%20language%20space.%20It%20is%20learned%0Ausing%20a%20patch-wise%20video-text%20contrastive%20strategy%2C%20in%20which%20the%20text%0Arepresentations%20act%20as%20prototypes%20for%20transforming%20vision%20features%20into%20a%0Alanguage%20space%2C%20which%20are%20then%20used%20as%20targets%20for%20semantically%20meaningful%0Afeature%20prediction%20using%20our%20masked%20encoder-decoder%20structure.%20FILS%0Ademonstrates%20remarkable%20transferability%20on%20downstream%20action%20recognition%20tasks%2C%0Aachieving%20state-of-the-art%20on%20challenging%20egocentric%20datasets%2C%20like%0AEpic-Kitchens%2C%20Something-SomethingV2%2C%20Charades-Ego%2C%20and%20EGTEA%2C%20using%20ViT-Base.%0AOur%20efficient%20method%20requires%20less%20computation%20and%20smaller%20batches%20compared%20to%0Aprevious%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFILS%253A%2520Self-Supervised%2520Video%2520Feature%2520Prediction%2520In%2520Semantic%2520Language%250A%2520%2520Space%26entry.906535625%3DMona%2520Ahmadian%2520and%2520Frank%2520Guerin%2520and%2520Andrew%2520Gilbert%26entry.1292438233%3D%2520%2520This%2520paper%2520demonstrates%2520a%2520self-supervised%2520approach%2520for%2520learning%2520semantic%250Avideo%2520representations.%2520Recent%2520vision%2520studies%2520show%2520that%2520a%2520masking%2520strategy%2520for%250Avision%2520and%2520natural%2520language%2520supervision%2520has%2520contributed%2520to%2520developing%250Atransferable%2520visual%2520pretraining.%2520Our%2520goal%2520is%2520to%2520achieve%2520a%2520more%2520semantic%2520video%250Arepresentation%2520by%2520leveraging%2520the%2520text%2520related%2520to%2520the%2520video%2520content%2520during%2520the%250Apretraining%2520in%2520a%2520fully%2520self-supervised%2520manner.%2520To%2520this%2520end%252C%2520we%2520present%2520FILS%252C%2520a%250Anovel%2520self-supervised%2520video%2520Feature%2520prediction%2520In%2520semantic%2520Language%2520Space%250A%2528FILS%2529.%2520The%2520vision%2520model%2520can%2520capture%2520valuable%2520structured%2520information%2520by%250Acorrectly%2520predicting%2520masked%2520feature%2520semantics%2520in%2520language%2520space.%2520It%2520is%2520learned%250Ausing%2520a%2520patch-wise%2520video-text%2520contrastive%2520strategy%252C%2520in%2520which%2520the%2520text%250Arepresentations%2520act%2520as%2520prototypes%2520for%2520transforming%2520vision%2520features%2520into%2520a%250Alanguage%2520space%252C%2520which%2520are%2520then%2520used%2520as%2520targets%2520for%2520semantically%2520meaningful%250Afeature%2520prediction%2520using%2520our%2520masked%2520encoder-decoder%2520structure.%2520FILS%250Ademonstrates%2520remarkable%2520transferability%2520on%2520downstream%2520action%2520recognition%2520tasks%252C%250Aachieving%2520state-of-the-art%2520on%2520challenging%2520egocentric%2520datasets%252C%2520like%250AEpic-Kitchens%252C%2520Something-SomethingV2%252C%2520Charades-Ego%252C%2520and%2520EGTEA%252C%2520using%2520ViT-Base.%250AOur%2520efficient%2520method%2520requires%2520less%2520computation%2520and%2520smaller%2520batches%2520compared%2520to%250Aprevious%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FILS%3A%20Self-Supervised%20Video%20Feature%20Prediction%20In%20Semantic%20Language%0A%20%20Space&entry.906535625=Mona%20Ahmadian%20and%20Frank%20Guerin%20and%20Andrew%20Gilbert&entry.1292438233=%20%20This%20paper%20demonstrates%20a%20self-supervised%20approach%20for%20learning%20semantic%0Avideo%20representations.%20Recent%20vision%20studies%20show%20that%20a%20masking%20strategy%20for%0Avision%20and%20natural%20language%20supervision%20has%20contributed%20to%20developing%0Atransferable%20visual%20pretraining.%20Our%20goal%20is%20to%20achieve%20a%20more%20semantic%20video%0Arepresentation%20by%20leveraging%20the%20text%20related%20to%20the%20video%20content%20during%20the%0Apretraining%20in%20a%20fully%20self-supervised%20manner.%20To%20this%20end%2C%20we%20present%20FILS%2C%20a%0Anovel%20self-supervised%20video%20Feature%20prediction%20In%20semantic%20Language%20Space%0A%28FILS%29.%20The%20vision%20model%20can%20capture%20valuable%20structured%20information%20by%0Acorrectly%20predicting%20masked%20feature%20semantics%20in%20language%20space.%20It%20is%20learned%0Ausing%20a%20patch-wise%20video-text%20contrastive%20strategy%2C%20in%20which%20the%20text%0Arepresentations%20act%20as%20prototypes%20for%20transforming%20vision%20features%20into%20a%0Alanguage%20space%2C%20which%20are%20then%20used%20as%20targets%20for%20semantically%20meaningful%0Afeature%20prediction%20using%20our%20masked%20encoder-decoder%20structure.%20FILS%0Ademonstrates%20remarkable%20transferability%20on%20downstream%20action%20recognition%20tasks%2C%0Aachieving%20state-of-the-art%20on%20challenging%20egocentric%20datasets%2C%20like%0AEpic-Kitchens%2C%20Something-SomethingV2%2C%20Charades-Ego%2C%20and%20EGTEA%2C%20using%20ViT-Base.%0AOur%20efficient%20method%20requires%20less%20computation%20and%20smaller%20batches%20compared%20to%0Aprevious%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03447v1&entry.124074799=Read"},
{"title": "Methods for Class-Imbalanced Learning with Support Vector Machines: A\n  Review and an Empirical Evaluation", "author": "Salim rezvani and Farhad Pourpanah and Chee Peng Lim and Q. M. Jonathan Wu", "abstract": "  This paper presents a review on methods for class-imbalanced learning with\nthe Support Vector Machine (SVM) and its variants. We first explain the\nstructure of SVM and its variants and discuss their inefficiency in learning\nwith class-imbalanced data sets. We introduce a hierarchical categorization of\nSVM-based models with respect to class-imbalanced learning. Specifically, we\ncategorize SVM-based models into re-sampling, algorithmic, and fusion methods,\nand discuss the principles of the representative models in each category. In\naddition, we conduct a series of empirical evaluations to compare the\nperformances of various representative SVM-based models in each category using\nbenchmark imbalanced data sets, ranging from low to high imbalanced ratios. Our\nfindings reveal that while algorithmic methods are less time-consuming owing to\nno data pre-processing requirements, fusion methods, which combine both\nre-sampling and algorithmic approaches, generally perform the best, but with a\nhigher computational load. A discussion on research gaps and future research\ndirections is provided.\n", "link": "http://arxiv.org/abs/2406.03398v1", "date": "2024-06-05", "relevancy": 2.2368, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4954}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4256}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Methods%20for%20Class-Imbalanced%20Learning%20with%20Support%20Vector%20Machines%3A%20A%0A%20%20Review%20and%20an%20Empirical%20Evaluation&body=Title%3A%20Methods%20for%20Class-Imbalanced%20Learning%20with%20Support%20Vector%20Machines%3A%20A%0A%20%20Review%20and%20an%20Empirical%20Evaluation%0AAuthor%3A%20Salim%20rezvani%20and%20Farhad%20Pourpanah%20and%20Chee%20Peng%20Lim%20and%20Q.%20M.%20Jonathan%20Wu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20review%20on%20methods%20for%20class-imbalanced%20learning%20with%0Athe%20Support%20Vector%20Machine%20%28SVM%29%20and%20its%20variants.%20We%20first%20explain%20the%0Astructure%20of%20SVM%20and%20its%20variants%20and%20discuss%20their%20inefficiency%20in%20learning%0Awith%20class-imbalanced%20data%20sets.%20We%20introduce%20a%20hierarchical%20categorization%20of%0ASVM-based%20models%20with%20respect%20to%20class-imbalanced%20learning.%20Specifically%2C%20we%0Acategorize%20SVM-based%20models%20into%20re-sampling%2C%20algorithmic%2C%20and%20fusion%20methods%2C%0Aand%20discuss%20the%20principles%20of%20the%20representative%20models%20in%20each%20category.%20In%0Aaddition%2C%20we%20conduct%20a%20series%20of%20empirical%20evaluations%20to%20compare%20the%0Aperformances%20of%20various%20representative%20SVM-based%20models%20in%20each%20category%20using%0Abenchmark%20imbalanced%20data%20sets%2C%20ranging%20from%20low%20to%20high%20imbalanced%20ratios.%20Our%0Afindings%20reveal%20that%20while%20algorithmic%20methods%20are%20less%20time-consuming%20owing%20to%0Ano%20data%20pre-processing%20requirements%2C%20fusion%20methods%2C%20which%20combine%20both%0Are-sampling%20and%20algorithmic%20approaches%2C%20generally%20perform%20the%20best%2C%20but%20with%20a%0Ahigher%20computational%20load.%20A%20discussion%20on%20research%20gaps%20and%20future%20research%0Adirections%20is%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMethods%2520for%2520Class-Imbalanced%2520Learning%2520with%2520Support%2520Vector%2520Machines%253A%2520A%250A%2520%2520Review%2520and%2520an%2520Empirical%2520Evaluation%26entry.906535625%3DSalim%2520rezvani%2520and%2520Farhad%2520Pourpanah%2520and%2520Chee%2520Peng%2520Lim%2520and%2520Q.%2520M.%2520Jonathan%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520review%2520on%2520methods%2520for%2520class-imbalanced%2520learning%2520with%250Athe%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%2520and%2520its%2520variants.%2520We%2520first%2520explain%2520the%250Astructure%2520of%2520SVM%2520and%2520its%2520variants%2520and%2520discuss%2520their%2520inefficiency%2520in%2520learning%250Awith%2520class-imbalanced%2520data%2520sets.%2520We%2520introduce%2520a%2520hierarchical%2520categorization%2520of%250ASVM-based%2520models%2520with%2520respect%2520to%2520class-imbalanced%2520learning.%2520Specifically%252C%2520we%250Acategorize%2520SVM-based%2520models%2520into%2520re-sampling%252C%2520algorithmic%252C%2520and%2520fusion%2520methods%252C%250Aand%2520discuss%2520the%2520principles%2520of%2520the%2520representative%2520models%2520in%2520each%2520category.%2520In%250Aaddition%252C%2520we%2520conduct%2520a%2520series%2520of%2520empirical%2520evaluations%2520to%2520compare%2520the%250Aperformances%2520of%2520various%2520representative%2520SVM-based%2520models%2520in%2520each%2520category%2520using%250Abenchmark%2520imbalanced%2520data%2520sets%252C%2520ranging%2520from%2520low%2520to%2520high%2520imbalanced%2520ratios.%2520Our%250Afindings%2520reveal%2520that%2520while%2520algorithmic%2520methods%2520are%2520less%2520time-consuming%2520owing%2520to%250Ano%2520data%2520pre-processing%2520requirements%252C%2520fusion%2520methods%252C%2520which%2520combine%2520both%250Are-sampling%2520and%2520algorithmic%2520approaches%252C%2520generally%2520perform%2520the%2520best%252C%2520but%2520with%2520a%250Ahigher%2520computational%2520load.%2520A%2520discussion%2520on%2520research%2520gaps%2520and%2520future%2520research%250Adirections%2520is%2520provided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Methods%20for%20Class-Imbalanced%20Learning%20with%20Support%20Vector%20Machines%3A%20A%0A%20%20Review%20and%20an%20Empirical%20Evaluation&entry.906535625=Salim%20rezvani%20and%20Farhad%20Pourpanah%20and%20Chee%20Peng%20Lim%20and%20Q.%20M.%20Jonathan%20Wu&entry.1292438233=%20%20This%20paper%20presents%20a%20review%20on%20methods%20for%20class-imbalanced%20learning%20with%0Athe%20Support%20Vector%20Machine%20%28SVM%29%20and%20its%20variants.%20We%20first%20explain%20the%0Astructure%20of%20SVM%20and%20its%20variants%20and%20discuss%20their%20inefficiency%20in%20learning%0Awith%20class-imbalanced%20data%20sets.%20We%20introduce%20a%20hierarchical%20categorization%20of%0ASVM-based%20models%20with%20respect%20to%20class-imbalanced%20learning.%20Specifically%2C%20we%0Acategorize%20SVM-based%20models%20into%20re-sampling%2C%20algorithmic%2C%20and%20fusion%20methods%2C%0Aand%20discuss%20the%20principles%20of%20the%20representative%20models%20in%20each%20category.%20In%0Aaddition%2C%20we%20conduct%20a%20series%20of%20empirical%20evaluations%20to%20compare%20the%0Aperformances%20of%20various%20representative%20SVM-based%20models%20in%20each%20category%20using%0Abenchmark%20imbalanced%20data%20sets%2C%20ranging%20from%20low%20to%20high%20imbalanced%20ratios.%20Our%0Afindings%20reveal%20that%20while%20algorithmic%20methods%20are%20less%20time-consuming%20owing%20to%0Ano%20data%20pre-processing%20requirements%2C%20fusion%20methods%2C%20which%20combine%20both%0Are-sampling%20and%20algorithmic%20approaches%2C%20generally%20perform%20the%20best%2C%20but%20with%20a%0Ahigher%20computational%20load.%20A%20discussion%20on%20research%20gaps%20and%20future%20research%0Adirections%20is%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03398v1&entry.124074799=Read"},
{"title": "Node-wise Filtering in Graph Neural Networks: A Mixture of Experts\n  Approach", "author": "Haoyu Han and Juanhui Li and Wei Huang and Xianfeng Tang and Hanqing Lu and Chen Luo and Hui Liu and Jiliang Tang", "abstract": "  Graph Neural Networks (GNNs) have proven to be highly effective for node\nclassification tasks across diverse graph structural patterns. Traditionally,\nGNNs employ a uniform global filter, typically a low-pass filter for homophilic\ngraphs and a high-pass filter for heterophilic graphs. However, real-world\ngraphs often exhibit a complex mix of homophilic and heterophilic patterns,\nrendering a single global filter approach suboptimal. In this work, we\ntheoretically demonstrate that a global filter optimized for one pattern can\nadversely affect performance on nodes with differing patterns. To address this,\nwe introduce a novel GNN framework Node-MoE that utilizes a mixture of experts\nto adaptively select the appropriate filters for different nodes. Extensive\nexperiments demonstrate the effectiveness of Node-MoE on both homophilic and\nheterophilic graphs.\n", "link": "http://arxiv.org/abs/2406.03464v1", "date": "2024-06-05", "relevancy": 2.2236, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4993}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4184}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Node-wise%20Filtering%20in%20Graph%20Neural%20Networks%3A%20A%20Mixture%20of%20Experts%0A%20%20Approach&body=Title%3A%20Node-wise%20Filtering%20in%20Graph%20Neural%20Networks%3A%20A%20Mixture%20of%20Experts%0A%20%20Approach%0AAuthor%3A%20Haoyu%20Han%20and%20Juanhui%20Li%20and%20Wei%20Huang%20and%20Xianfeng%20Tang%20and%20Hanqing%20Lu%20and%20Chen%20Luo%20and%20Hui%20Liu%20and%20Jiliang%20Tang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20proven%20to%20be%20highly%20effective%20for%20node%0Aclassification%20tasks%20across%20diverse%20graph%20structural%20patterns.%20Traditionally%2C%0AGNNs%20employ%20a%20uniform%20global%20filter%2C%20typically%20a%20low-pass%20filter%20for%20homophilic%0Agraphs%20and%20a%20high-pass%20filter%20for%20heterophilic%20graphs.%20However%2C%20real-world%0Agraphs%20often%20exhibit%20a%20complex%20mix%20of%20homophilic%20and%20heterophilic%20patterns%2C%0Arendering%20a%20single%20global%20filter%20approach%20suboptimal.%20In%20this%20work%2C%20we%0Atheoretically%20demonstrate%20that%20a%20global%20filter%20optimized%20for%20one%20pattern%20can%0Aadversely%20affect%20performance%20on%20nodes%20with%20differing%20patterns.%20To%20address%20this%2C%0Awe%20introduce%20a%20novel%20GNN%20framework%20Node-MoE%20that%20utilizes%20a%20mixture%20of%20experts%0Ato%20adaptively%20select%20the%20appropriate%20filters%20for%20different%20nodes.%20Extensive%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20Node-MoE%20on%20both%20homophilic%20and%0Aheterophilic%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNode-wise%2520Filtering%2520in%2520Graph%2520Neural%2520Networks%253A%2520A%2520Mixture%2520of%2520Experts%250A%2520%2520Approach%26entry.906535625%3DHaoyu%2520Han%2520and%2520Juanhui%2520Li%2520and%2520Wei%2520Huang%2520and%2520Xianfeng%2520Tang%2520and%2520Hanqing%2520Lu%2520and%2520Chen%2520Luo%2520and%2520Hui%2520Liu%2520and%2520Jiliang%2520Tang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520proven%2520to%2520be%2520highly%2520effective%2520for%2520node%250Aclassification%2520tasks%2520across%2520diverse%2520graph%2520structural%2520patterns.%2520Traditionally%252C%250AGNNs%2520employ%2520a%2520uniform%2520global%2520filter%252C%2520typically%2520a%2520low-pass%2520filter%2520for%2520homophilic%250Agraphs%2520and%2520a%2520high-pass%2520filter%2520for%2520heterophilic%2520graphs.%2520However%252C%2520real-world%250Agraphs%2520often%2520exhibit%2520a%2520complex%2520mix%2520of%2520homophilic%2520and%2520heterophilic%2520patterns%252C%250Arendering%2520a%2520single%2520global%2520filter%2520approach%2520suboptimal.%2520In%2520this%2520work%252C%2520we%250Atheoretically%2520demonstrate%2520that%2520a%2520global%2520filter%2520optimized%2520for%2520one%2520pattern%2520can%250Aadversely%2520affect%2520performance%2520on%2520nodes%2520with%2520differing%2520patterns.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520a%2520novel%2520GNN%2520framework%2520Node-MoE%2520that%2520utilizes%2520a%2520mixture%2520of%2520experts%250Ato%2520adaptively%2520select%2520the%2520appropriate%2520filters%2520for%2520different%2520nodes.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520effectiveness%2520of%2520Node-MoE%2520on%2520both%2520homophilic%2520and%250Aheterophilic%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Node-wise%20Filtering%20in%20Graph%20Neural%20Networks%3A%20A%20Mixture%20of%20Experts%0A%20%20Approach&entry.906535625=Haoyu%20Han%20and%20Juanhui%20Li%20and%20Wei%20Huang%20and%20Xianfeng%20Tang%20and%20Hanqing%20Lu%20and%20Chen%20Luo%20and%20Hui%20Liu%20and%20Jiliang%20Tang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20proven%20to%20be%20highly%20effective%20for%20node%0Aclassification%20tasks%20across%20diverse%20graph%20structural%20patterns.%20Traditionally%2C%0AGNNs%20employ%20a%20uniform%20global%20filter%2C%20typically%20a%20low-pass%20filter%20for%20homophilic%0Agraphs%20and%20a%20high-pass%20filter%20for%20heterophilic%20graphs.%20However%2C%20real-world%0Agraphs%20often%20exhibit%20a%20complex%20mix%20of%20homophilic%20and%20heterophilic%20patterns%2C%0Arendering%20a%20single%20global%20filter%20approach%20suboptimal.%20In%20this%20work%2C%20we%0Atheoretically%20demonstrate%20that%20a%20global%20filter%20optimized%20for%20one%20pattern%20can%0Aadversely%20affect%20performance%20on%20nodes%20with%20differing%20patterns.%20To%20address%20this%2C%0Awe%20introduce%20a%20novel%20GNN%20framework%20Node-MoE%20that%20utilizes%20a%20mixture%20of%20experts%0Ato%20adaptively%20select%20the%20appropriate%20filters%20for%20different%20nodes.%20Extensive%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20Node-MoE%20on%20both%20homophilic%20and%0Aheterophilic%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03464v1&entry.124074799=Read"},
{"title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings\n  for Robust Large Vision-Language Models", "author": "Christian Schlarmann and Naman Deep Singh and Francesco Croce and Matthias Hein", "abstract": "  Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are\nincreasingly used for various real-world tasks. Prior work has shown that these\nmodels are highly vulnerable to adversarial attacks on the vision modality.\nThese attacks can be leveraged to spread fake information or defraud users, and\nthus pose a significant risk, which makes the robustness of large multi-modal\nfoundation models a pressing problem. The CLIP model, or one of its variants,\nis used as a frozen vision encoder in many large vision-language models\n(LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial\nfine-tuning scheme to obtain a robust CLIP vision encoder, which yields\nrobustness on all vision down-stream tasks (LVLMs, zero-shot classification)\nthat rely on CLIP. In particular, we show that stealth-attacks on users of\nLVLMs by a malicious third party providing manipulated images are no longer\npossible once one replaces the original CLIP model with our robust one. No\nretraining or fine-tuning of the down-stream LVLMs is required. The code and\nrobust models are available at https://github.com/chs20/RobustVLM\n", "link": "http://arxiv.org/abs/2402.12336v2", "date": "2024-06-05", "relevancy": 2.2088, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5852}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5293}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20CLIP%3A%20Unsupervised%20Adversarial%20Fine-Tuning%20of%20Vision%20Embeddings%0A%20%20for%20Robust%20Large%20Vision-Language%20Models&body=Title%3A%20Robust%20CLIP%3A%20Unsupervised%20Adversarial%20Fine-Tuning%20of%20Vision%20Embeddings%0A%20%20for%20Robust%20Large%20Vision-Language%20Models%0AAuthor%3A%20Christian%20Schlarmann%20and%20Naman%20Deep%20Singh%20and%20Francesco%20Croce%20and%20Matthias%20Hein%0AAbstract%3A%20%20%20Multi-modal%20foundation%20models%20like%20OpenFlamingo%2C%20LLaVA%2C%20and%20GPT-4%20are%0Aincreasingly%20used%20for%20various%20real-world%20tasks.%20Prior%20work%20has%20shown%20that%20these%0Amodels%20are%20highly%20vulnerable%20to%20adversarial%20attacks%20on%20the%20vision%20modality.%0AThese%20attacks%20can%20be%20leveraged%20to%20spread%20fake%20information%20or%20defraud%20users%2C%20and%0Athus%20pose%20a%20significant%20risk%2C%20which%20makes%20the%20robustness%20of%20large%20multi-modal%0Afoundation%20models%20a%20pressing%20problem.%20The%20CLIP%20model%2C%20or%20one%20of%20its%20variants%2C%0Ais%20used%20as%20a%20frozen%20vision%20encoder%20in%20many%20large%20vision-language%20models%0A%28LVLMs%29%2C%20e.g.%20LLaVA%20and%20OpenFlamingo.%20We%20propose%20an%20unsupervised%20adversarial%0Afine-tuning%20scheme%20to%20obtain%20a%20robust%20CLIP%20vision%20encoder%2C%20which%20yields%0Arobustness%20on%20all%20vision%20down-stream%20tasks%20%28LVLMs%2C%20zero-shot%20classification%29%0Athat%20rely%20on%20CLIP.%20In%20particular%2C%20we%20show%20that%20stealth-attacks%20on%20users%20of%0ALVLMs%20by%20a%20malicious%20third%20party%20providing%20manipulated%20images%20are%20no%20longer%0Apossible%20once%20one%20replaces%20the%20original%20CLIP%20model%20with%20our%20robust%20one.%20No%0Aretraining%20or%20fine-tuning%20of%20the%20down-stream%20LVLMs%20is%20required.%20The%20code%20and%0Arobust%20models%20are%20available%20at%20https%3A//github.com/chs20/RobustVLM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12336v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520CLIP%253A%2520Unsupervised%2520Adversarial%2520Fine-Tuning%2520of%2520Vision%2520Embeddings%250A%2520%2520for%2520Robust%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DChristian%2520Schlarmann%2520and%2520Naman%2520Deep%2520Singh%2520and%2520Francesco%2520Croce%2520and%2520Matthias%2520Hein%26entry.1292438233%3D%2520%2520Multi-modal%2520foundation%2520models%2520like%2520OpenFlamingo%252C%2520LLaVA%252C%2520and%2520GPT-4%2520are%250Aincreasingly%2520used%2520for%2520various%2520real-world%2520tasks.%2520Prior%2520work%2520has%2520shown%2520that%2520these%250Amodels%2520are%2520highly%2520vulnerable%2520to%2520adversarial%2520attacks%2520on%2520the%2520vision%2520modality.%250AThese%2520attacks%2520can%2520be%2520leveraged%2520to%2520spread%2520fake%2520information%2520or%2520defraud%2520users%252C%2520and%250Athus%2520pose%2520a%2520significant%2520risk%252C%2520which%2520makes%2520the%2520robustness%2520of%2520large%2520multi-modal%250Afoundation%2520models%2520a%2520pressing%2520problem.%2520The%2520CLIP%2520model%252C%2520or%2520one%2520of%2520its%2520variants%252C%250Ais%2520used%2520as%2520a%2520frozen%2520vision%2520encoder%2520in%2520many%2520large%2520vision-language%2520models%250A%2528LVLMs%2529%252C%2520e.g.%2520LLaVA%2520and%2520OpenFlamingo.%2520We%2520propose%2520an%2520unsupervised%2520adversarial%250Afine-tuning%2520scheme%2520to%2520obtain%2520a%2520robust%2520CLIP%2520vision%2520encoder%252C%2520which%2520yields%250Arobustness%2520on%2520all%2520vision%2520down-stream%2520tasks%2520%2528LVLMs%252C%2520zero-shot%2520classification%2529%250Athat%2520rely%2520on%2520CLIP.%2520In%2520particular%252C%2520we%2520show%2520that%2520stealth-attacks%2520on%2520users%2520of%250ALVLMs%2520by%2520a%2520malicious%2520third%2520party%2520providing%2520manipulated%2520images%2520are%2520no%2520longer%250Apossible%2520once%2520one%2520replaces%2520the%2520original%2520CLIP%2520model%2520with%2520our%2520robust%2520one.%2520No%250Aretraining%2520or%2520fine-tuning%2520of%2520the%2520down-stream%2520LVLMs%2520is%2520required.%2520The%2520code%2520and%250Arobust%2520models%2520are%2520available%2520at%2520https%253A//github.com/chs20/RobustVLM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12336v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20CLIP%3A%20Unsupervised%20Adversarial%20Fine-Tuning%20of%20Vision%20Embeddings%0A%20%20for%20Robust%20Large%20Vision-Language%20Models&entry.906535625=Christian%20Schlarmann%20and%20Naman%20Deep%20Singh%20and%20Francesco%20Croce%20and%20Matthias%20Hein&entry.1292438233=%20%20Multi-modal%20foundation%20models%20like%20OpenFlamingo%2C%20LLaVA%2C%20and%20GPT-4%20are%0Aincreasingly%20used%20for%20various%20real-world%20tasks.%20Prior%20work%20has%20shown%20that%20these%0Amodels%20are%20highly%20vulnerable%20to%20adversarial%20attacks%20on%20the%20vision%20modality.%0AThese%20attacks%20can%20be%20leveraged%20to%20spread%20fake%20information%20or%20defraud%20users%2C%20and%0Athus%20pose%20a%20significant%20risk%2C%20which%20makes%20the%20robustness%20of%20large%20multi-modal%0Afoundation%20models%20a%20pressing%20problem.%20The%20CLIP%20model%2C%20or%20one%20of%20its%20variants%2C%0Ais%20used%20as%20a%20frozen%20vision%20encoder%20in%20many%20large%20vision-language%20models%0A%28LVLMs%29%2C%20e.g.%20LLaVA%20and%20OpenFlamingo.%20We%20propose%20an%20unsupervised%20adversarial%0Afine-tuning%20scheme%20to%20obtain%20a%20robust%20CLIP%20vision%20encoder%2C%20which%20yields%0Arobustness%20on%20all%20vision%20down-stream%20tasks%20%28LVLMs%2C%20zero-shot%20classification%29%0Athat%20rely%20on%20CLIP.%20In%20particular%2C%20we%20show%20that%20stealth-attacks%20on%20users%20of%0ALVLMs%20by%20a%20malicious%20third%20party%20providing%20manipulated%20images%20are%20no%20longer%0Apossible%20once%20one%20replaces%20the%20original%20CLIP%20model%20with%20our%20robust%20one.%20No%0Aretraining%20or%20fine-tuning%20of%20the%20down-stream%20LVLMs%20is%20required.%20The%20code%20and%0Arobust%20models%20are%20available%20at%20https%3A//github.com/chs20/RobustVLM%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12336v2&entry.124074799=Read"},
{"title": "TKAN: Temporal Kolmogorov-Arnold Networks", "author": "Remi Genet and Hugo Inzirillo", "abstract": "  Recurrent Neural Networks (RNNs) have revolutionized many areas of machine\nlearning, particularly in natural language and data sequence processing. Long\nShort-Term Memory (LSTM) has demonstrated its ability to capture long-term\ndependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks\n(KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed\na new neural networks architecture inspired by KAN and the LSTM, the Temporal\nKolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both\nnetworks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers\nembedding memory management. This innovation enables us to perform multi-step\ntime series forecasting with enhanced accuracy and efficiency. By addressing\nthe limitations of traditional models in handling complex sequential patterns,\nthe TKAN architecture offers significant potential for advancements in fields\nrequiring more than one step ahead forecasting.\n", "link": "http://arxiv.org/abs/2405.07344v2", "date": "2024-06-05", "relevancy": 2.1913, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4429}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4382}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TKAN%3A%20Temporal%20Kolmogorov-Arnold%20Networks&body=Title%3A%20TKAN%3A%20Temporal%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Remi%20Genet%20and%20Hugo%20Inzirillo%0AAbstract%3A%20%20%20Recurrent%20Neural%20Networks%20%28RNNs%29%20have%20revolutionized%20many%20areas%20of%20machine%0Alearning%2C%20particularly%20in%20natural%20language%20and%20data%20sequence%20processing.%20Long%0AShort-Term%20Memory%20%28LSTM%29%20has%20demonstrated%20its%20ability%20to%20capture%20long-term%0Adependencies%20in%20sequential%20data.%20Inspired%20by%20the%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%20a%20promising%20alternatives%20to%20Multi-Layer%20Perceptrons%20%28MLPs%29%2C%20we%20proposed%0Aa%20new%20neural%20networks%20architecture%20inspired%20by%20KAN%20and%20the%20LSTM%2C%20the%20Temporal%0AKolomogorov-Arnold%20Networks%20%28TKANs%29.%20TKANs%20combined%20the%20strenght%20of%20both%0Anetworks%2C%20it%20is%20composed%20of%20Recurring%20Kolmogorov-Arnold%20Networks%20%28RKANs%29%20Layers%0Aembedding%20memory%20management.%20This%20innovation%20enables%20us%20to%20perform%20multi-step%0Atime%20series%20forecasting%20with%20enhanced%20accuracy%20and%20efficiency.%20By%20addressing%0Athe%20limitations%20of%20traditional%20models%20in%20handling%20complex%20sequential%20patterns%2C%0Athe%20TKAN%20architecture%20offers%20significant%20potential%20for%20advancements%20in%20fields%0Arequiring%20more%20than%20one%20step%20ahead%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTKAN%253A%2520Temporal%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DRemi%2520Genet%2520and%2520Hugo%2520Inzirillo%26entry.1292438233%3D%2520%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520have%2520revolutionized%2520many%2520areas%2520of%2520machine%250Alearning%252C%2520particularly%2520in%2520natural%2520language%2520and%2520data%2520sequence%2520processing.%2520Long%250AShort-Term%2520Memory%2520%2528LSTM%2529%2520has%2520demonstrated%2520its%2520ability%2520to%2520capture%2520long-term%250Adependencies%2520in%2520sequential%2520data.%2520Inspired%2520by%2520the%2520Kolmogorov-Arnold%2520Networks%250A%2528KANs%2529%2520a%2520promising%2520alternatives%2520to%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%252C%2520we%2520proposed%250Aa%2520new%2520neural%2520networks%2520architecture%2520inspired%2520by%2520KAN%2520and%2520the%2520LSTM%252C%2520the%2520Temporal%250AKolomogorov-Arnold%2520Networks%2520%2528TKANs%2529.%2520TKANs%2520combined%2520the%2520strenght%2520of%2520both%250Anetworks%252C%2520it%2520is%2520composed%2520of%2520Recurring%2520Kolmogorov-Arnold%2520Networks%2520%2528RKANs%2529%2520Layers%250Aembedding%2520memory%2520management.%2520This%2520innovation%2520enables%2520us%2520to%2520perform%2520multi-step%250Atime%2520series%2520forecasting%2520with%2520enhanced%2520accuracy%2520and%2520efficiency.%2520By%2520addressing%250Athe%2520limitations%2520of%2520traditional%2520models%2520in%2520handling%2520complex%2520sequential%2520patterns%252C%250Athe%2520TKAN%2520architecture%2520offers%2520significant%2520potential%2520for%2520advancements%2520in%2520fields%250Arequiring%2520more%2520than%2520one%2520step%2520ahead%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TKAN%3A%20Temporal%20Kolmogorov-Arnold%20Networks&entry.906535625=Remi%20Genet%20and%20Hugo%20Inzirillo&entry.1292438233=%20%20Recurrent%20Neural%20Networks%20%28RNNs%29%20have%20revolutionized%20many%20areas%20of%20machine%0Alearning%2C%20particularly%20in%20natural%20language%20and%20data%20sequence%20processing.%20Long%0AShort-Term%20Memory%20%28LSTM%29%20has%20demonstrated%20its%20ability%20to%20capture%20long-term%0Adependencies%20in%20sequential%20data.%20Inspired%20by%20the%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%20a%20promising%20alternatives%20to%20Multi-Layer%20Perceptrons%20%28MLPs%29%2C%20we%20proposed%0Aa%20new%20neural%20networks%20architecture%20inspired%20by%20KAN%20and%20the%20LSTM%2C%20the%20Temporal%0AKolomogorov-Arnold%20Networks%20%28TKANs%29.%20TKANs%20combined%20the%20strenght%20of%20both%0Anetworks%2C%20it%20is%20composed%20of%20Recurring%20Kolmogorov-Arnold%20Networks%20%28RKANs%29%20Layers%0Aembedding%20memory%20management.%20This%20innovation%20enables%20us%20to%20perform%20multi-step%0Atime%20series%20forecasting%20with%20enhanced%20accuracy%20and%20efficiency.%20By%20addressing%0Athe%20limitations%20of%20traditional%20models%20in%20handling%20complex%20sequential%20patterns%2C%0Athe%20TKAN%20architecture%20offers%20significant%20potential%20for%20advancements%20in%20fields%0Arequiring%20more%20than%20one%20step%20ahead%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07344v2&entry.124074799=Read"},
{"title": "Cycles of Thought: Measuring LLM Confidence through Stable Explanations", "author": "Evan Becker and Stefano Soatto", "abstract": "  In many high-risk machine learning applications it is essential for a model\nto indicate when it is uncertain about a prediction. While large language\nmodels (LLMs) can reach and even surpass human-level accuracy on a variety of\nbenchmarks, their overconfidence in incorrect responses is still a\nwell-documented failure mode. Traditional methods for ML uncertainty\nquantification can be difficult to directly adapt to LLMs due to the\ncomputational cost of implementation and closed-source nature of many models. A\nvariety of black-box methods have recently been proposed, but these often rely\non heuristics such as self-verbalized confidence. We instead propose a\nframework for measuring an LLM's uncertainty with respect to the distribution\nof generated explanations for an answer. While utilizing explanations is not a\nnew idea in and of itself, by interpreting each possible model+explanation pair\nas a test-time classifier we can calculate a posterior answer distribution over\nthe most likely of these classifiers. We demonstrate how a specific instance of\nthis framework using explanation entailment as our classifier likelihood\nimproves confidence score metrics (in particular AURC and AUROC) over baselines\nacross five different datasets. We believe these results indicate that our\nframework is both a well-principled and effective way of quantifying\nuncertainty in LLMs.\n", "link": "http://arxiv.org/abs/2406.03441v1", "date": "2024-06-05", "relevancy": 2.1911, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5815}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5575}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cycles%20of%20Thought%3A%20Measuring%20LLM%20Confidence%20through%20Stable%20Explanations&body=Title%3A%20Cycles%20of%20Thought%3A%20Measuring%20LLM%20Confidence%20through%20Stable%20Explanations%0AAuthor%3A%20Evan%20Becker%20and%20Stefano%20Soatto%0AAbstract%3A%20%20%20In%20many%20high-risk%20machine%20learning%20applications%20it%20is%20essential%20for%20a%20model%0Ato%20indicate%20when%20it%20is%20uncertain%20about%20a%20prediction.%20While%20large%20language%0Amodels%20%28LLMs%29%20can%20reach%20and%20even%20surpass%20human-level%20accuracy%20on%20a%20variety%20of%0Abenchmarks%2C%20their%20overconfidence%20in%20incorrect%20responses%20is%20still%20a%0Awell-documented%20failure%20mode.%20Traditional%20methods%20for%20ML%20uncertainty%0Aquantification%20can%20be%20difficult%20to%20directly%20adapt%20to%20LLMs%20due%20to%20the%0Acomputational%20cost%20of%20implementation%20and%20closed-source%20nature%20of%20many%20models.%20A%0Avariety%20of%20black-box%20methods%20have%20recently%20been%20proposed%2C%20but%20these%20often%20rely%0Aon%20heuristics%20such%20as%20self-verbalized%20confidence.%20We%20instead%20propose%20a%0Aframework%20for%20measuring%20an%20LLM%27s%20uncertainty%20with%20respect%20to%20the%20distribution%0Aof%20generated%20explanations%20for%20an%20answer.%20While%20utilizing%20explanations%20is%20not%20a%0Anew%20idea%20in%20and%20of%20itself%2C%20by%20interpreting%20each%20possible%20model%2Bexplanation%20pair%0Aas%20a%20test-time%20classifier%20we%20can%20calculate%20a%20posterior%20answer%20distribution%20over%0Athe%20most%20likely%20of%20these%20classifiers.%20We%20demonstrate%20how%20a%20specific%20instance%20of%0Athis%20framework%20using%20explanation%20entailment%20as%20our%20classifier%20likelihood%0Aimproves%20confidence%20score%20metrics%20%28in%20particular%20AURC%20and%20AUROC%29%20over%20baselines%0Aacross%20five%20different%20datasets.%20We%20believe%20these%20results%20indicate%20that%20our%0Aframework%20is%20both%20a%20well-principled%20and%20effective%20way%20of%20quantifying%0Auncertainty%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCycles%2520of%2520Thought%253A%2520Measuring%2520LLM%2520Confidence%2520through%2520Stable%2520Explanations%26entry.906535625%3DEvan%2520Becker%2520and%2520Stefano%2520Soatto%26entry.1292438233%3D%2520%2520In%2520many%2520high-risk%2520machine%2520learning%2520applications%2520it%2520is%2520essential%2520for%2520a%2520model%250Ato%2520indicate%2520when%2520it%2520is%2520uncertain%2520about%2520a%2520prediction.%2520While%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520can%2520reach%2520and%2520even%2520surpass%2520human-level%2520accuracy%2520on%2520a%2520variety%2520of%250Abenchmarks%252C%2520their%2520overconfidence%2520in%2520incorrect%2520responses%2520is%2520still%2520a%250Awell-documented%2520failure%2520mode.%2520Traditional%2520methods%2520for%2520ML%2520uncertainty%250Aquantification%2520can%2520be%2520difficult%2520to%2520directly%2520adapt%2520to%2520LLMs%2520due%2520to%2520the%250Acomputational%2520cost%2520of%2520implementation%2520and%2520closed-source%2520nature%2520of%2520many%2520models.%2520A%250Avariety%2520of%2520black-box%2520methods%2520have%2520recently%2520been%2520proposed%252C%2520but%2520these%2520often%2520rely%250Aon%2520heuristics%2520such%2520as%2520self-verbalized%2520confidence.%2520We%2520instead%2520propose%2520a%250Aframework%2520for%2520measuring%2520an%2520LLM%2527s%2520uncertainty%2520with%2520respect%2520to%2520the%2520distribution%250Aof%2520generated%2520explanations%2520for%2520an%2520answer.%2520While%2520utilizing%2520explanations%2520is%2520not%2520a%250Anew%2520idea%2520in%2520and%2520of%2520itself%252C%2520by%2520interpreting%2520each%2520possible%2520model%252Bexplanation%2520pair%250Aas%2520a%2520test-time%2520classifier%2520we%2520can%2520calculate%2520a%2520posterior%2520answer%2520distribution%2520over%250Athe%2520most%2520likely%2520of%2520these%2520classifiers.%2520We%2520demonstrate%2520how%2520a%2520specific%2520instance%2520of%250Athis%2520framework%2520using%2520explanation%2520entailment%2520as%2520our%2520classifier%2520likelihood%250Aimproves%2520confidence%2520score%2520metrics%2520%2528in%2520particular%2520AURC%2520and%2520AUROC%2529%2520over%2520baselines%250Aacross%2520five%2520different%2520datasets.%2520We%2520believe%2520these%2520results%2520indicate%2520that%2520our%250Aframework%2520is%2520both%2520a%2520well-principled%2520and%2520effective%2520way%2520of%2520quantifying%250Auncertainty%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cycles%20of%20Thought%3A%20Measuring%20LLM%20Confidence%20through%20Stable%20Explanations&entry.906535625=Evan%20Becker%20and%20Stefano%20Soatto&entry.1292438233=%20%20In%20many%20high-risk%20machine%20learning%20applications%20it%20is%20essential%20for%20a%20model%0Ato%20indicate%20when%20it%20is%20uncertain%20about%20a%20prediction.%20While%20large%20language%0Amodels%20%28LLMs%29%20can%20reach%20and%20even%20surpass%20human-level%20accuracy%20on%20a%20variety%20of%0Abenchmarks%2C%20their%20overconfidence%20in%20incorrect%20responses%20is%20still%20a%0Awell-documented%20failure%20mode.%20Traditional%20methods%20for%20ML%20uncertainty%0Aquantification%20can%20be%20difficult%20to%20directly%20adapt%20to%20LLMs%20due%20to%20the%0Acomputational%20cost%20of%20implementation%20and%20closed-source%20nature%20of%20many%20models.%20A%0Avariety%20of%20black-box%20methods%20have%20recently%20been%20proposed%2C%20but%20these%20often%20rely%0Aon%20heuristics%20such%20as%20self-verbalized%20confidence.%20We%20instead%20propose%20a%0Aframework%20for%20measuring%20an%20LLM%27s%20uncertainty%20with%20respect%20to%20the%20distribution%0Aof%20generated%20explanations%20for%20an%20answer.%20While%20utilizing%20explanations%20is%20not%20a%0Anew%20idea%20in%20and%20of%20itself%2C%20by%20interpreting%20each%20possible%20model%2Bexplanation%20pair%0Aas%20a%20test-time%20classifier%20we%20can%20calculate%20a%20posterior%20answer%20distribution%20over%0Athe%20most%20likely%20of%20these%20classifiers.%20We%20demonstrate%20how%20a%20specific%20instance%20of%0Athis%20framework%20using%20explanation%20entailment%20as%20our%20classifier%20likelihood%0Aimproves%20confidence%20score%20metrics%20%28in%20particular%20AURC%20and%20AUROC%29%20over%20baselines%0Aacross%20five%20different%20datasets.%20We%20believe%20these%20results%20indicate%20that%20our%0Aframework%20is%20both%20a%20well-principled%20and%20effective%20way%20of%20quantifying%0Auncertainty%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03441v1&entry.124074799=Read"},
{"title": "Self-Supervised Interpretable End-to-End Learning via Latent Functional\n  Modularity", "author": "Hyunki Seong and David Hyunchul Shim", "abstract": "  We introduce MoNet, a novel functionally modular network for self-supervised\nand interpretable end-to-end learning. By leveraging its functional modularity\nwith a latent-guided contrastive loss function, MoNet efficiently learns\ntask-specific decision-making processes in latent space without requiring\ntask-level supervision. Moreover, our method incorporates an online, post-hoc\nexplainability approach that enhances the interpretability of end-to-end\ninferences without compromising sensorimotor control performance. In real-world\nindoor environments, MoNet demonstrates effective visual autonomous navigation,\noutperforming baseline models by 7% to 28% in task specificity analysis. We\nfurther explore the interpretability of our network through post-hoc analysis\nof perceptual saliency maps and latent decision vectors. This provides valuable\ninsights into the incorporation of explainable artificial intelligence into\nrobotic learning, encompassing both perceptual and behavioral perspectives.\nSupplementary materials are available at\nhttps://sites.google.com/view/monet-lgc.\n", "link": "http://arxiv.org/abs/2403.18947v2", "date": "2024-06-05", "relevancy": 2.187, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5651}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.56}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Interpretable%20End-to-End%20Learning%20via%20Latent%20Functional%0A%20%20Modularity&body=Title%3A%20Self-Supervised%20Interpretable%20End-to-End%20Learning%20via%20Latent%20Functional%0A%20%20Modularity%0AAuthor%3A%20Hyunki%20Seong%20and%20David%20Hyunchul%20Shim%0AAbstract%3A%20%20%20We%20introduce%20MoNet%2C%20a%20novel%20functionally%20modular%20network%20for%20self-supervised%0Aand%20interpretable%20end-to-end%20learning.%20By%20leveraging%20its%20functional%20modularity%0Awith%20a%20latent-guided%20contrastive%20loss%20function%2C%20MoNet%20efficiently%20learns%0Atask-specific%20decision-making%20processes%20in%20latent%20space%20without%20requiring%0Atask-level%20supervision.%20Moreover%2C%20our%20method%20incorporates%20an%20online%2C%20post-hoc%0Aexplainability%20approach%20that%20enhances%20the%20interpretability%20of%20end-to-end%0Ainferences%20without%20compromising%20sensorimotor%20control%20performance.%20In%20real-world%0Aindoor%20environments%2C%20MoNet%20demonstrates%20effective%20visual%20autonomous%20navigation%2C%0Aoutperforming%20baseline%20models%20by%207%25%20to%2028%25%20in%20task%20specificity%20analysis.%20We%0Afurther%20explore%20the%20interpretability%20of%20our%20network%20through%20post-hoc%20analysis%0Aof%20perceptual%20saliency%20maps%20and%20latent%20decision%20vectors.%20This%20provides%20valuable%0Ainsights%20into%20the%20incorporation%20of%20explainable%20artificial%20intelligence%20into%0Arobotic%20learning%2C%20encompassing%20both%20perceptual%20and%20behavioral%20perspectives.%0ASupplementary%20materials%20are%20available%20at%0Ahttps%3A//sites.google.com/view/monet-lgc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18947v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Interpretable%2520End-to-End%2520Learning%2520via%2520Latent%2520Functional%250A%2520%2520Modularity%26entry.906535625%3DHyunki%2520Seong%2520and%2520David%2520Hyunchul%2520Shim%26entry.1292438233%3D%2520%2520We%2520introduce%2520MoNet%252C%2520a%2520novel%2520functionally%2520modular%2520network%2520for%2520self-supervised%250Aand%2520interpretable%2520end-to-end%2520learning.%2520By%2520leveraging%2520its%2520functional%2520modularity%250Awith%2520a%2520latent-guided%2520contrastive%2520loss%2520function%252C%2520MoNet%2520efficiently%2520learns%250Atask-specific%2520decision-making%2520processes%2520in%2520latent%2520space%2520without%2520requiring%250Atask-level%2520supervision.%2520Moreover%252C%2520our%2520method%2520incorporates%2520an%2520online%252C%2520post-hoc%250Aexplainability%2520approach%2520that%2520enhances%2520the%2520interpretability%2520of%2520end-to-end%250Ainferences%2520without%2520compromising%2520sensorimotor%2520control%2520performance.%2520In%2520real-world%250Aindoor%2520environments%252C%2520MoNet%2520demonstrates%2520effective%2520visual%2520autonomous%2520navigation%252C%250Aoutperforming%2520baseline%2520models%2520by%25207%2525%2520to%252028%2525%2520in%2520task%2520specificity%2520analysis.%2520We%250Afurther%2520explore%2520the%2520interpretability%2520of%2520our%2520network%2520through%2520post-hoc%2520analysis%250Aof%2520perceptual%2520saliency%2520maps%2520and%2520latent%2520decision%2520vectors.%2520This%2520provides%2520valuable%250Ainsights%2520into%2520the%2520incorporation%2520of%2520explainable%2520artificial%2520intelligence%2520into%250Arobotic%2520learning%252C%2520encompassing%2520both%2520perceptual%2520and%2520behavioral%2520perspectives.%250ASupplementary%2520materials%2520are%2520available%2520at%250Ahttps%253A//sites.google.com/view/monet-lgc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18947v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Interpretable%20End-to-End%20Learning%20via%20Latent%20Functional%0A%20%20Modularity&entry.906535625=Hyunki%20Seong%20and%20David%20Hyunchul%20Shim&entry.1292438233=%20%20We%20introduce%20MoNet%2C%20a%20novel%20functionally%20modular%20network%20for%20self-supervised%0Aand%20interpretable%20end-to-end%20learning.%20By%20leveraging%20its%20functional%20modularity%0Awith%20a%20latent-guided%20contrastive%20loss%20function%2C%20MoNet%20efficiently%20learns%0Atask-specific%20decision-making%20processes%20in%20latent%20space%20without%20requiring%0Atask-level%20supervision.%20Moreover%2C%20our%20method%20incorporates%20an%20online%2C%20post-hoc%0Aexplainability%20approach%20that%20enhances%20the%20interpretability%20of%20end-to-end%0Ainferences%20without%20compromising%20sensorimotor%20control%20performance.%20In%20real-world%0Aindoor%20environments%2C%20MoNet%20demonstrates%20effective%20visual%20autonomous%20navigation%2C%0Aoutperforming%20baseline%20models%20by%207%25%20to%2028%25%20in%20task%20specificity%20analysis.%20We%0Afurther%20explore%20the%20interpretability%20of%20our%20network%20through%20post-hoc%20analysis%0Aof%20perceptual%20saliency%20maps%20and%20latent%20decision%20vectors.%20This%20provides%20valuable%0Ainsights%20into%20the%20incorporation%20of%20explainable%20artificial%20intelligence%20into%0Arobotic%20learning%2C%20encompassing%20both%20perceptual%20and%20behavioral%20perspectives.%0ASupplementary%20materials%20are%20available%20at%0Ahttps%3A//sites.google.com/view/monet-lgc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18947v2&entry.124074799=Read"},
{"title": "AD3: Implicit Action is the Key for World Models to Distinguish the\n  Diverse Visual Distractors", "author": "Yucen Wang and Shenghua Wan and Le Gan and Shuai Feng and De-Chuan Zhan", "abstract": "  Model-based methods have significantly contributed to distinguishing\ntask-irrelevant distractors for visual control. However, prior research has\nprimarily focused on heterogeneous distractors like noisy background videos,\nleaving homogeneous distractors that closely resemble controllable agents\nlargely unexplored, which poses significant challenges to existing methods. To\ntackle this problem, we propose Implicit Action Generator (IAG) to learn the\nimplicit actions of visual distractors, and present a new algorithm named\nimplicit Action-informed Diverse visual Distractors Distinguisher (AD3), that\nleverages the action inferred by IAG to train separated world models. Implicit\nactions effectively capture the behavior of background distractors, aiding in\ndistinguishing the task-irrelevant components, and the agent can optimize the\npolicy within the task-relevant state space. Our method achieves superior\nperformance on various visual control tasks featuring both heterogeneous and\nhomogeneous distractors. The indispensable role of implicit actions learned by\nIAG is also empirically validated.\n", "link": "http://arxiv.org/abs/2403.09976v2", "date": "2024-06-05", "relevancy": 2.1704, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5616}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5325}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AD3%3A%20Implicit%20Action%20is%20the%20Key%20for%20World%20Models%20to%20Distinguish%20the%0A%20%20Diverse%20Visual%20Distractors&body=Title%3A%20AD3%3A%20Implicit%20Action%20is%20the%20Key%20for%20World%20Models%20to%20Distinguish%20the%0A%20%20Diverse%20Visual%20Distractors%0AAuthor%3A%20Yucen%20Wang%20and%20Shenghua%20Wan%20and%20Le%20Gan%20and%20Shuai%20Feng%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20Model-based%20methods%20have%20significantly%20contributed%20to%20distinguishing%0Atask-irrelevant%20distractors%20for%20visual%20control.%20However%2C%20prior%20research%20has%0Aprimarily%20focused%20on%20heterogeneous%20distractors%20like%20noisy%20background%20videos%2C%0Aleaving%20homogeneous%20distractors%20that%20closely%20resemble%20controllable%20agents%0Alargely%20unexplored%2C%20which%20poses%20significant%20challenges%20to%20existing%20methods.%20To%0Atackle%20this%20problem%2C%20we%20propose%20Implicit%20Action%20Generator%20%28IAG%29%20to%20learn%20the%0Aimplicit%20actions%20of%20visual%20distractors%2C%20and%20present%20a%20new%20algorithm%20named%0Aimplicit%20Action-informed%20Diverse%20visual%20Distractors%20Distinguisher%20%28AD3%29%2C%20that%0Aleverages%20the%20action%20inferred%20by%20IAG%20to%20train%20separated%20world%20models.%20Implicit%0Aactions%20effectively%20capture%20the%20behavior%20of%20background%20distractors%2C%20aiding%20in%0Adistinguishing%20the%20task-irrelevant%20components%2C%20and%20the%20agent%20can%20optimize%20the%0Apolicy%20within%20the%20task-relevant%20state%20space.%20Our%20method%20achieves%20superior%0Aperformance%20on%20various%20visual%20control%20tasks%20featuring%20both%20heterogeneous%20and%0Ahomogeneous%20distractors.%20The%20indispensable%20role%20of%20implicit%20actions%20learned%20by%0AIAG%20is%20also%20empirically%20validated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09976v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAD3%253A%2520Implicit%2520Action%2520is%2520the%2520Key%2520for%2520World%2520Models%2520to%2520Distinguish%2520the%250A%2520%2520Diverse%2520Visual%2520Distractors%26entry.906535625%3DYucen%2520Wang%2520and%2520Shenghua%2520Wan%2520and%2520Le%2520Gan%2520and%2520Shuai%2520Feng%2520and%2520De-Chuan%2520Zhan%26entry.1292438233%3D%2520%2520Model-based%2520methods%2520have%2520significantly%2520contributed%2520to%2520distinguishing%250Atask-irrelevant%2520distractors%2520for%2520visual%2520control.%2520However%252C%2520prior%2520research%2520has%250Aprimarily%2520focused%2520on%2520heterogeneous%2520distractors%2520like%2520noisy%2520background%2520videos%252C%250Aleaving%2520homogeneous%2520distractors%2520that%2520closely%2520resemble%2520controllable%2520agents%250Alargely%2520unexplored%252C%2520which%2520poses%2520significant%2520challenges%2520to%2520existing%2520methods.%2520To%250Atackle%2520this%2520problem%252C%2520we%2520propose%2520Implicit%2520Action%2520Generator%2520%2528IAG%2529%2520to%2520learn%2520the%250Aimplicit%2520actions%2520of%2520visual%2520distractors%252C%2520and%2520present%2520a%2520new%2520algorithm%2520named%250Aimplicit%2520Action-informed%2520Diverse%2520visual%2520Distractors%2520Distinguisher%2520%2528AD3%2529%252C%2520that%250Aleverages%2520the%2520action%2520inferred%2520by%2520IAG%2520to%2520train%2520separated%2520world%2520models.%2520Implicit%250Aactions%2520effectively%2520capture%2520the%2520behavior%2520of%2520background%2520distractors%252C%2520aiding%2520in%250Adistinguishing%2520the%2520task-irrelevant%2520components%252C%2520and%2520the%2520agent%2520can%2520optimize%2520the%250Apolicy%2520within%2520the%2520task-relevant%2520state%2520space.%2520Our%2520method%2520achieves%2520superior%250Aperformance%2520on%2520various%2520visual%2520control%2520tasks%2520featuring%2520both%2520heterogeneous%2520and%250Ahomogeneous%2520distractors.%2520The%2520indispensable%2520role%2520of%2520implicit%2520actions%2520learned%2520by%250AIAG%2520is%2520also%2520empirically%2520validated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09976v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AD3%3A%20Implicit%20Action%20is%20the%20Key%20for%20World%20Models%20to%20Distinguish%20the%0A%20%20Diverse%20Visual%20Distractors&entry.906535625=Yucen%20Wang%20and%20Shenghua%20Wan%20and%20Le%20Gan%20and%20Shuai%20Feng%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20Model-based%20methods%20have%20significantly%20contributed%20to%20distinguishing%0Atask-irrelevant%20distractors%20for%20visual%20control.%20However%2C%20prior%20research%20has%0Aprimarily%20focused%20on%20heterogeneous%20distractors%20like%20noisy%20background%20videos%2C%0Aleaving%20homogeneous%20distractors%20that%20closely%20resemble%20controllable%20agents%0Alargely%20unexplored%2C%20which%20poses%20significant%20challenges%20to%20existing%20methods.%20To%0Atackle%20this%20problem%2C%20we%20propose%20Implicit%20Action%20Generator%20%28IAG%29%20to%20learn%20the%0Aimplicit%20actions%20of%20visual%20distractors%2C%20and%20present%20a%20new%20algorithm%20named%0Aimplicit%20Action-informed%20Diverse%20visual%20Distractors%20Distinguisher%20%28AD3%29%2C%20that%0Aleverages%20the%20action%20inferred%20by%20IAG%20to%20train%20separated%20world%20models.%20Implicit%0Aactions%20effectively%20capture%20the%20behavior%20of%20background%20distractors%2C%20aiding%20in%0Adistinguishing%20the%20task-irrelevant%20components%2C%20and%20the%20agent%20can%20optimize%20the%0Apolicy%20within%20the%20task-relevant%20state%20space.%20Our%20method%20achieves%20superior%0Aperformance%20on%20various%20visual%20control%20tasks%20featuring%20both%20heterogeneous%20and%0Ahomogeneous%20distractors.%20The%20indispensable%20role%20of%20implicit%20actions%20learned%20by%0AIAG%20is%20also%20empirically%20validated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09976v2&entry.124074799=Read"},
{"title": "Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning", "author": "Xiaoyu Zhang and Matthew Chang and Pranav Kumar and Saurabh Gupta", "abstract": "  A common failure mode for policies trained with imitation is compounding\nexecution errors at test time. When the learned policy encounters states that\nare not present in the expert demonstrations, the policy fails, leading to\ndegenerate behavior. The Dataset Aggregation, or DAgger approach to this\nproblem simply collects more data to cover these failure states. However, in\npractice, this is often prohibitively expensive. In this work, we propose\nDiffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without\nthe cost for eye-in-hand imitation learning problems. Instead of collecting new\nsamples to cover out-of-distribution states, DMD uses recent advances in\ndiffusion models to synthesize these samples. This leads to robust performance\nfrom few demonstrations. We compare DMD against behavior cloning baseline\nacross four tasks: pushing, stacking, pouring, and shirt hanging. In pushing,\nDMD achieves 80% success rate with as few as 8 expert demonstrations, where\nnaive behavior cloning reaches only 20%. In stacking, DMD succeeds on average\n92% of the time across 5 cups, versus 40% for BC. When pouring coffee beans,\nDMD transfers to another cup successfully 80% of the time. Finally, DMD attains\n90% success rate for hanging shirt on a clothing rack.\n", "link": "http://arxiv.org/abs/2402.17768v2", "date": "2024-06-05", "relevancy": 2.167, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5467}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5446}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Meets%20DAgger%3A%20Supercharging%20Eye-in-hand%20Imitation%20Learning&body=Title%3A%20Diffusion%20Meets%20DAgger%3A%20Supercharging%20Eye-in-hand%20Imitation%20Learning%0AAuthor%3A%20Xiaoyu%20Zhang%20and%20Matthew%20Chang%20and%20Pranav%20Kumar%20and%20Saurabh%20Gupta%0AAbstract%3A%20%20%20A%20common%20failure%20mode%20for%20policies%20trained%20with%20imitation%20is%20compounding%0Aexecution%20errors%20at%20test%20time.%20When%20the%20learned%20policy%20encounters%20states%20that%0Aare%20not%20present%20in%20the%20expert%20demonstrations%2C%20the%20policy%20fails%2C%20leading%20to%0Adegenerate%20behavior.%20The%20Dataset%20Aggregation%2C%20or%20DAgger%20approach%20to%20this%0Aproblem%20simply%20collects%20more%20data%20to%20cover%20these%20failure%20states.%20However%2C%20in%0Apractice%2C%20this%20is%20often%20prohibitively%20expensive.%20In%20this%20work%2C%20we%20propose%0ADiffusion%20Meets%20DAgger%20%28DMD%29%2C%20a%20method%20to%20reap%20the%20benefits%20of%20DAgger%20without%0Athe%20cost%20for%20eye-in-hand%20imitation%20learning%20problems.%20Instead%20of%20collecting%20new%0Asamples%20to%20cover%20out-of-distribution%20states%2C%20DMD%20uses%20recent%20advances%20in%0Adiffusion%20models%20to%20synthesize%20these%20samples.%20This%20leads%20to%20robust%20performance%0Afrom%20few%20demonstrations.%20We%20compare%20DMD%20against%20behavior%20cloning%20baseline%0Aacross%20four%20tasks%3A%20pushing%2C%20stacking%2C%20pouring%2C%20and%20shirt%20hanging.%20In%20pushing%2C%0ADMD%20achieves%2080%25%20success%20rate%20with%20as%20few%20as%208%20expert%20demonstrations%2C%20where%0Anaive%20behavior%20cloning%20reaches%20only%2020%25.%20In%20stacking%2C%20DMD%20succeeds%20on%20average%0A92%25%20of%20the%20time%20across%205%20cups%2C%20versus%2040%25%20for%20BC.%20When%20pouring%20coffee%20beans%2C%0ADMD%20transfers%20to%20another%20cup%20successfully%2080%25%20of%20the%20time.%20Finally%2C%20DMD%20attains%0A90%25%20success%20rate%20for%20hanging%20shirt%20on%20a%20clothing%20rack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Meets%2520DAgger%253A%2520Supercharging%2520Eye-in-hand%2520Imitation%2520Learning%26entry.906535625%3DXiaoyu%2520Zhang%2520and%2520Matthew%2520Chang%2520and%2520Pranav%2520Kumar%2520and%2520Saurabh%2520Gupta%26entry.1292438233%3D%2520%2520A%2520common%2520failure%2520mode%2520for%2520policies%2520trained%2520with%2520imitation%2520is%2520compounding%250Aexecution%2520errors%2520at%2520test%2520time.%2520When%2520the%2520learned%2520policy%2520encounters%2520states%2520that%250Aare%2520not%2520present%2520in%2520the%2520expert%2520demonstrations%252C%2520the%2520policy%2520fails%252C%2520leading%2520to%250Adegenerate%2520behavior.%2520The%2520Dataset%2520Aggregation%252C%2520or%2520DAgger%2520approach%2520to%2520this%250Aproblem%2520simply%2520collects%2520more%2520data%2520to%2520cover%2520these%2520failure%2520states.%2520However%252C%2520in%250Apractice%252C%2520this%2520is%2520often%2520prohibitively%2520expensive.%2520In%2520this%2520work%252C%2520we%2520propose%250ADiffusion%2520Meets%2520DAgger%2520%2528DMD%2529%252C%2520a%2520method%2520to%2520reap%2520the%2520benefits%2520of%2520DAgger%2520without%250Athe%2520cost%2520for%2520eye-in-hand%2520imitation%2520learning%2520problems.%2520Instead%2520of%2520collecting%2520new%250Asamples%2520to%2520cover%2520out-of-distribution%2520states%252C%2520DMD%2520uses%2520recent%2520advances%2520in%250Adiffusion%2520models%2520to%2520synthesize%2520these%2520samples.%2520This%2520leads%2520to%2520robust%2520performance%250Afrom%2520few%2520demonstrations.%2520We%2520compare%2520DMD%2520against%2520behavior%2520cloning%2520baseline%250Aacross%2520four%2520tasks%253A%2520pushing%252C%2520stacking%252C%2520pouring%252C%2520and%2520shirt%2520hanging.%2520In%2520pushing%252C%250ADMD%2520achieves%252080%2525%2520success%2520rate%2520with%2520as%2520few%2520as%25208%2520expert%2520demonstrations%252C%2520where%250Anaive%2520behavior%2520cloning%2520reaches%2520only%252020%2525.%2520In%2520stacking%252C%2520DMD%2520succeeds%2520on%2520average%250A92%2525%2520of%2520the%2520time%2520across%25205%2520cups%252C%2520versus%252040%2525%2520for%2520BC.%2520When%2520pouring%2520coffee%2520beans%252C%250ADMD%2520transfers%2520to%2520another%2520cup%2520successfully%252080%2525%2520of%2520the%2520time.%2520Finally%252C%2520DMD%2520attains%250A90%2525%2520success%2520rate%2520for%2520hanging%2520shirt%2520on%2520a%2520clothing%2520rack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Meets%20DAgger%3A%20Supercharging%20Eye-in-hand%20Imitation%20Learning&entry.906535625=Xiaoyu%20Zhang%20and%20Matthew%20Chang%20and%20Pranav%20Kumar%20and%20Saurabh%20Gupta&entry.1292438233=%20%20A%20common%20failure%20mode%20for%20policies%20trained%20with%20imitation%20is%20compounding%0Aexecution%20errors%20at%20test%20time.%20When%20the%20learned%20policy%20encounters%20states%20that%0Aare%20not%20present%20in%20the%20expert%20demonstrations%2C%20the%20policy%20fails%2C%20leading%20to%0Adegenerate%20behavior.%20The%20Dataset%20Aggregation%2C%20or%20DAgger%20approach%20to%20this%0Aproblem%20simply%20collects%20more%20data%20to%20cover%20these%20failure%20states.%20However%2C%20in%0Apractice%2C%20this%20is%20often%20prohibitively%20expensive.%20In%20this%20work%2C%20we%20propose%0ADiffusion%20Meets%20DAgger%20%28DMD%29%2C%20a%20method%20to%20reap%20the%20benefits%20of%20DAgger%20without%0Athe%20cost%20for%20eye-in-hand%20imitation%20learning%20problems.%20Instead%20of%20collecting%20new%0Asamples%20to%20cover%20out-of-distribution%20states%2C%20DMD%20uses%20recent%20advances%20in%0Adiffusion%20models%20to%20synthesize%20these%20samples.%20This%20leads%20to%20robust%20performance%0Afrom%20few%20demonstrations.%20We%20compare%20DMD%20against%20behavior%20cloning%20baseline%0Aacross%20four%20tasks%3A%20pushing%2C%20stacking%2C%20pouring%2C%20and%20shirt%20hanging.%20In%20pushing%2C%0ADMD%20achieves%2080%25%20success%20rate%20with%20as%20few%20as%208%20expert%20demonstrations%2C%20where%0Anaive%20behavior%20cloning%20reaches%20only%2020%25.%20In%20stacking%2C%20DMD%20succeeds%20on%20average%0A92%25%20of%20the%20time%20across%205%20cups%2C%20versus%2040%25%20for%20BC.%20When%20pouring%20coffee%20beans%2C%0ADMD%20transfers%20to%20another%20cup%20successfully%2080%25%20of%20the%20time.%20Finally%2C%20DMD%20attains%0A90%25%20success%20rate%20for%20hanging%20shirt%20on%20a%20clothing%20rack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17768v2&entry.124074799=Read"},
{"title": "Ethical Implications of ChatGPT in Higher Education: A Scoping Review", "author": "Ming Li and Ariunaa Enkhtur and Fei Cheng and Beverley Anne Yamamoto", "abstract": "  This scoping review explores the ethical challenges of using ChatGPT in\nhigher education. By reviewing recent academic articles in English, Chinese,\nand Japanese, we aimed to provide a deep dive review and identify gaps in the\nliterature. Drawing on Arksey and O'Malley's (2005) scoping review framework,\nwe defined search terms and identified relevant publications from four\ndatabases in the three target languages. The research results showed that the\nmajority of the papers were discussion papers, but there was some early\nempirical work. The ethical issues highlighted in these works mainly concern\nacademic integrity, assessment issues, and data protection. Given the rapid\ndeployment of generative artificial intelligence, it is imperative for\neducators to conduct more empirical studies to develop sound ethical policies\nfor its use.\n", "link": "http://arxiv.org/abs/2311.14378v3", "date": "2024-06-05", "relevancy": 2.166, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4635}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4378}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ethical%20Implications%20of%20ChatGPT%20in%20Higher%20Education%3A%20A%20Scoping%20Review&body=Title%3A%20Ethical%20Implications%20of%20ChatGPT%20in%20Higher%20Education%3A%20A%20Scoping%20Review%0AAuthor%3A%20Ming%20Li%20and%20Ariunaa%20Enkhtur%20and%20Fei%20Cheng%20and%20Beverley%20Anne%20Yamamoto%0AAbstract%3A%20%20%20This%20scoping%20review%20explores%20the%20ethical%20challenges%20of%20using%20ChatGPT%20in%0Ahigher%20education.%20By%20reviewing%20recent%20academic%20articles%20in%20English%2C%20Chinese%2C%0Aand%20Japanese%2C%20we%20aimed%20to%20provide%20a%20deep%20dive%20review%20and%20identify%20gaps%20in%20the%0Aliterature.%20Drawing%20on%20Arksey%20and%20O%27Malley%27s%20%282005%29%20scoping%20review%20framework%2C%0Awe%20defined%20search%20terms%20and%20identified%20relevant%20publications%20from%20four%0Adatabases%20in%20the%20three%20target%20languages.%20The%20research%20results%20showed%20that%20the%0Amajority%20of%20the%20papers%20were%20discussion%20papers%2C%20but%20there%20was%20some%20early%0Aempirical%20work.%20The%20ethical%20issues%20highlighted%20in%20these%20works%20mainly%20concern%0Aacademic%20integrity%2C%20assessment%20issues%2C%20and%20data%20protection.%20Given%20the%20rapid%0Adeployment%20of%20generative%20artificial%20intelligence%2C%20it%20is%20imperative%20for%0Aeducators%20to%20conduct%20more%20empirical%20studies%20to%20develop%20sound%20ethical%20policies%0Afor%20its%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14378v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEthical%2520Implications%2520of%2520ChatGPT%2520in%2520Higher%2520Education%253A%2520A%2520Scoping%2520Review%26entry.906535625%3DMing%2520Li%2520and%2520Ariunaa%2520Enkhtur%2520and%2520Fei%2520Cheng%2520and%2520Beverley%2520Anne%2520Yamamoto%26entry.1292438233%3D%2520%2520This%2520scoping%2520review%2520explores%2520the%2520ethical%2520challenges%2520of%2520using%2520ChatGPT%2520in%250Ahigher%2520education.%2520By%2520reviewing%2520recent%2520academic%2520articles%2520in%2520English%252C%2520Chinese%252C%250Aand%2520Japanese%252C%2520we%2520aimed%2520to%2520provide%2520a%2520deep%2520dive%2520review%2520and%2520identify%2520gaps%2520in%2520the%250Aliterature.%2520Drawing%2520on%2520Arksey%2520and%2520O%2527Malley%2527s%2520%25282005%2529%2520scoping%2520review%2520framework%252C%250Awe%2520defined%2520search%2520terms%2520and%2520identified%2520relevant%2520publications%2520from%2520four%250Adatabases%2520in%2520the%2520three%2520target%2520languages.%2520The%2520research%2520results%2520showed%2520that%2520the%250Amajority%2520of%2520the%2520papers%2520were%2520discussion%2520papers%252C%2520but%2520there%2520was%2520some%2520early%250Aempirical%2520work.%2520The%2520ethical%2520issues%2520highlighted%2520in%2520these%2520works%2520mainly%2520concern%250Aacademic%2520integrity%252C%2520assessment%2520issues%252C%2520and%2520data%2520protection.%2520Given%2520the%2520rapid%250Adeployment%2520of%2520generative%2520artificial%2520intelligence%252C%2520it%2520is%2520imperative%2520for%250Aeducators%2520to%2520conduct%2520more%2520empirical%2520studies%2520to%2520develop%2520sound%2520ethical%2520policies%250Afor%2520its%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14378v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ethical%20Implications%20of%20ChatGPT%20in%20Higher%20Education%3A%20A%20Scoping%20Review&entry.906535625=Ming%20Li%20and%20Ariunaa%20Enkhtur%20and%20Fei%20Cheng%20and%20Beverley%20Anne%20Yamamoto&entry.1292438233=%20%20This%20scoping%20review%20explores%20the%20ethical%20challenges%20of%20using%20ChatGPT%20in%0Ahigher%20education.%20By%20reviewing%20recent%20academic%20articles%20in%20English%2C%20Chinese%2C%0Aand%20Japanese%2C%20we%20aimed%20to%20provide%20a%20deep%20dive%20review%20and%20identify%20gaps%20in%20the%0Aliterature.%20Drawing%20on%20Arksey%20and%20O%27Malley%27s%20%282005%29%20scoping%20review%20framework%2C%0Awe%20defined%20search%20terms%20and%20identified%20relevant%20publications%20from%20four%0Adatabases%20in%20the%20three%20target%20languages.%20The%20research%20results%20showed%20that%20the%0Amajority%20of%20the%20papers%20were%20discussion%20papers%2C%20but%20there%20was%20some%20early%0Aempirical%20work.%20The%20ethical%20issues%20highlighted%20in%20these%20works%20mainly%20concern%0Aacademic%20integrity%2C%20assessment%20issues%2C%20and%20data%20protection.%20Given%20the%20rapid%0Adeployment%20of%20generative%20artificial%20intelligence%2C%20it%20is%20imperative%20for%0Aeducators%20to%20conduct%20more%20empirical%20studies%20to%20develop%20sound%20ethical%20policies%0Afor%20its%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14378v3&entry.124074799=Read"},
{"title": "Does your data spark joy? Performance gains from domain upsampling at\n  the end of training", "author": "Cody Blakeney and Mansheej Paul and Brett W. Larsen and Sean Owen and Jonathan Frankle", "abstract": "  Pretraining datasets for large language models (LLMs) have grown to trillions\nof tokens composed of large amounts of CommonCrawl (CC) web scrape along with\nsmaller, domain-specific datasets. It is expensive to understand the impact of\nthese domain-specific datasets on model capabilities as training at large FLOP\nscales is required to reveal significant changes to difficult and emergent\nbenchmarks. Given the increasing cost of experimenting with pretraining data,\nhow does one determine the optimal balance between the diversity in general web\nscrapes and the information density of domain specific data? In this work, we\nshow how to leverage the smaller domain specific datasets by upsampling them\nrelative to CC at the end of training to drive performance improvements on\ndifficult benchmarks. This simple technique allows us to improve up to 6.90 pp\non MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data\nmix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2\n(7B)$\\unicode{x2014}$a model trained for twice as long. We experiment with\nablating the duration of domain upsampling from 5% to 30% of training and find\nthat 10% to 20% percent is optimal for navigating the tradeoff between general\nlanguage modeling capabilities and targeted benchmarks. We also use domain\nupsampling to characterize at scale the utility of individual datasets for\nimproving various benchmarks by removing them during this final phase of\ntraining. This tool opens up the ability to experiment with the impact of\ndifferent pretraining datasets at scale, but at an order of magnitude lower\ncost compared to full pretraining runs.\n", "link": "http://arxiv.org/abs/2406.03476v1", "date": "2024-06-05", "relevancy": 2.1652, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5689}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5311}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20your%20data%20spark%20joy%3F%20Performance%20gains%20from%20domain%20upsampling%20at%0A%20%20the%20end%20of%20training&body=Title%3A%20Does%20your%20data%20spark%20joy%3F%20Performance%20gains%20from%20domain%20upsampling%20at%0A%20%20the%20end%20of%20training%0AAuthor%3A%20Cody%20Blakeney%20and%20Mansheej%20Paul%20and%20Brett%20W.%20Larsen%20and%20Sean%20Owen%20and%20Jonathan%20Frankle%0AAbstract%3A%20%20%20Pretraining%20datasets%20for%20large%20language%20models%20%28LLMs%29%20have%20grown%20to%20trillions%0Aof%20tokens%20composed%20of%20large%20amounts%20of%20CommonCrawl%20%28CC%29%20web%20scrape%20along%20with%0Asmaller%2C%20domain-specific%20datasets.%20It%20is%20expensive%20to%20understand%20the%20impact%20of%0Athese%20domain-specific%20datasets%20on%20model%20capabilities%20as%20training%20at%20large%20FLOP%0Ascales%20is%20required%20to%20reveal%20significant%20changes%20to%20difficult%20and%20emergent%0Abenchmarks.%20Given%20the%20increasing%20cost%20of%20experimenting%20with%20pretraining%20data%2C%0Ahow%20does%20one%20determine%20the%20optimal%20balance%20between%20the%20diversity%20in%20general%20web%0Ascrapes%20and%20the%20information%20density%20of%20domain%20specific%20data%3F%20In%20this%20work%2C%20we%0Ashow%20how%20to%20leverage%20the%20smaller%20domain%20specific%20datasets%20by%20upsampling%20them%0Arelative%20to%20CC%20at%20the%20end%20of%20training%20to%20drive%20performance%20improvements%20on%0Adifficult%20benchmarks.%20This%20simple%20technique%20allows%20us%20to%20improve%20up%20to%206.90%20pp%0Aon%20MMLU%2C%208.26%20pp%20on%20GSM8K%2C%20and%206.17%20pp%20on%20HumanEval%20relative%20to%20the%20base%20data%0Amix%20for%20a%207B%20model%20trained%20for%201%20trillion%20%28T%29%20tokens%2C%20thus%20rivaling%20Llama-2%0A%287B%29%24%5Cunicode%7Bx2014%7D%24a%20model%20trained%20for%20twice%20as%20long.%20We%20experiment%20with%0Aablating%20the%20duration%20of%20domain%20upsampling%20from%205%25%20to%2030%25%20of%20training%20and%20find%0Athat%2010%25%20to%2020%25%20percent%20is%20optimal%20for%20navigating%20the%20tradeoff%20between%20general%0Alanguage%20modeling%20capabilities%20and%20targeted%20benchmarks.%20We%20also%20use%20domain%0Aupsampling%20to%20characterize%20at%20scale%20the%20utility%20of%20individual%20datasets%20for%0Aimproving%20various%20benchmarks%20by%20removing%20them%20during%20this%20final%20phase%20of%0Atraining.%20This%20tool%20opens%20up%20the%20ability%20to%20experiment%20with%20the%20impact%20of%0Adifferent%20pretraining%20datasets%20at%20scale%2C%20but%20at%20an%20order%20of%20magnitude%20lower%0Acost%20compared%20to%20full%20pretraining%20runs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520your%2520data%2520spark%2520joy%253F%2520Performance%2520gains%2520from%2520domain%2520upsampling%2520at%250A%2520%2520the%2520end%2520of%2520training%26entry.906535625%3DCody%2520Blakeney%2520and%2520Mansheej%2520Paul%2520and%2520Brett%2520W.%2520Larsen%2520and%2520Sean%2520Owen%2520and%2520Jonathan%2520Frankle%26entry.1292438233%3D%2520%2520Pretraining%2520datasets%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520grown%2520to%2520trillions%250Aof%2520tokens%2520composed%2520of%2520large%2520amounts%2520of%2520CommonCrawl%2520%2528CC%2529%2520web%2520scrape%2520along%2520with%250Asmaller%252C%2520domain-specific%2520datasets.%2520It%2520is%2520expensive%2520to%2520understand%2520the%2520impact%2520of%250Athese%2520domain-specific%2520datasets%2520on%2520model%2520capabilities%2520as%2520training%2520at%2520large%2520FLOP%250Ascales%2520is%2520required%2520to%2520reveal%2520significant%2520changes%2520to%2520difficult%2520and%2520emergent%250Abenchmarks.%2520Given%2520the%2520increasing%2520cost%2520of%2520experimenting%2520with%2520pretraining%2520data%252C%250Ahow%2520does%2520one%2520determine%2520the%2520optimal%2520balance%2520between%2520the%2520diversity%2520in%2520general%2520web%250Ascrapes%2520and%2520the%2520information%2520density%2520of%2520domain%2520specific%2520data%253F%2520In%2520this%2520work%252C%2520we%250Ashow%2520how%2520to%2520leverage%2520the%2520smaller%2520domain%2520specific%2520datasets%2520by%2520upsampling%2520them%250Arelative%2520to%2520CC%2520at%2520the%2520end%2520of%2520training%2520to%2520drive%2520performance%2520improvements%2520on%250Adifficult%2520benchmarks.%2520This%2520simple%2520technique%2520allows%2520us%2520to%2520improve%2520up%2520to%25206.90%2520pp%250Aon%2520MMLU%252C%25208.26%2520pp%2520on%2520GSM8K%252C%2520and%25206.17%2520pp%2520on%2520HumanEval%2520relative%2520to%2520the%2520base%2520data%250Amix%2520for%2520a%25207B%2520model%2520trained%2520for%25201%2520trillion%2520%2528T%2529%2520tokens%252C%2520thus%2520rivaling%2520Llama-2%250A%25287B%2529%2524%255Cunicode%257Bx2014%257D%2524a%2520model%2520trained%2520for%2520twice%2520as%2520long.%2520We%2520experiment%2520with%250Aablating%2520the%2520duration%2520of%2520domain%2520upsampling%2520from%25205%2525%2520to%252030%2525%2520of%2520training%2520and%2520find%250Athat%252010%2525%2520to%252020%2525%2520percent%2520is%2520optimal%2520for%2520navigating%2520the%2520tradeoff%2520between%2520general%250Alanguage%2520modeling%2520capabilities%2520and%2520targeted%2520benchmarks.%2520We%2520also%2520use%2520domain%250Aupsampling%2520to%2520characterize%2520at%2520scale%2520the%2520utility%2520of%2520individual%2520datasets%2520for%250Aimproving%2520various%2520benchmarks%2520by%2520removing%2520them%2520during%2520this%2520final%2520phase%2520of%250Atraining.%2520This%2520tool%2520opens%2520up%2520the%2520ability%2520to%2520experiment%2520with%2520the%2520impact%2520of%250Adifferent%2520pretraining%2520datasets%2520at%2520scale%252C%2520but%2520at%2520an%2520order%2520of%2520magnitude%2520lower%250Acost%2520compared%2520to%2520full%2520pretraining%2520runs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20your%20data%20spark%20joy%3F%20Performance%20gains%20from%20domain%20upsampling%20at%0A%20%20the%20end%20of%20training&entry.906535625=Cody%20Blakeney%20and%20Mansheej%20Paul%20and%20Brett%20W.%20Larsen%20and%20Sean%20Owen%20and%20Jonathan%20Frankle&entry.1292438233=%20%20Pretraining%20datasets%20for%20large%20language%20models%20%28LLMs%29%20have%20grown%20to%20trillions%0Aof%20tokens%20composed%20of%20large%20amounts%20of%20CommonCrawl%20%28CC%29%20web%20scrape%20along%20with%0Asmaller%2C%20domain-specific%20datasets.%20It%20is%20expensive%20to%20understand%20the%20impact%20of%0Athese%20domain-specific%20datasets%20on%20model%20capabilities%20as%20training%20at%20large%20FLOP%0Ascales%20is%20required%20to%20reveal%20significant%20changes%20to%20difficult%20and%20emergent%0Abenchmarks.%20Given%20the%20increasing%20cost%20of%20experimenting%20with%20pretraining%20data%2C%0Ahow%20does%20one%20determine%20the%20optimal%20balance%20between%20the%20diversity%20in%20general%20web%0Ascrapes%20and%20the%20information%20density%20of%20domain%20specific%20data%3F%20In%20this%20work%2C%20we%0Ashow%20how%20to%20leverage%20the%20smaller%20domain%20specific%20datasets%20by%20upsampling%20them%0Arelative%20to%20CC%20at%20the%20end%20of%20training%20to%20drive%20performance%20improvements%20on%0Adifficult%20benchmarks.%20This%20simple%20technique%20allows%20us%20to%20improve%20up%20to%206.90%20pp%0Aon%20MMLU%2C%208.26%20pp%20on%20GSM8K%2C%20and%206.17%20pp%20on%20HumanEval%20relative%20to%20the%20base%20data%0Amix%20for%20a%207B%20model%20trained%20for%201%20trillion%20%28T%29%20tokens%2C%20thus%20rivaling%20Llama-2%0A%287B%29%24%5Cunicode%7Bx2014%7D%24a%20model%20trained%20for%20twice%20as%20long.%20We%20experiment%20with%0Aablating%20the%20duration%20of%20domain%20upsampling%20from%205%25%20to%2030%25%20of%20training%20and%20find%0Athat%2010%25%20to%2020%25%20percent%20is%20optimal%20for%20navigating%20the%20tradeoff%20between%20general%0Alanguage%20modeling%20capabilities%20and%20targeted%20benchmarks.%20We%20also%20use%20domain%0Aupsampling%20to%20characterize%20at%20scale%20the%20utility%20of%20individual%20datasets%20for%0Aimproving%20various%20benchmarks%20by%20removing%20them%20during%20this%20final%20phase%20of%0Atraining.%20This%20tool%20opens%20up%20the%20ability%20to%20experiment%20with%20the%20impact%20of%0Adifferent%20pretraining%20datasets%20at%20scale%2C%20but%20at%20an%20order%20of%20magnitude%20lower%0Acost%20compared%20to%20full%20pretraining%20runs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03476v1&entry.124074799=Read"},
{"title": "Prompt-based Visual Alignment for Zero-shot Policy Transfer", "author": "Haihan Gao and Rui Zhang and Qi Yi and Hantao Yao and Haochen Li and Jiaming Guo and Shaohui Peng and Yunkai Gao and QiCheng Wang and Xing Hu and Yuanbo Wen and Zihao Zhang and Zidong Du and Ling Li and Qi Guo and Yunji Chen", "abstract": "  Overfitting in RL has become one of the main obstacles to applications in\nreinforcement learning(RL). Existing methods do not provide explicit semantic\nconstrain for the feature extractor, hindering the agent from learning a\nunified cross-domain representation and resulting in performance degradation on\nunseen domains. Besides, abundant data from multiple domains are needed. To\naddress these issues, in this work, we propose prompt-based visual alignment\n(PVA), a robust framework to mitigate the detrimental domain bias in the image\nfor zero-shot policy transfer. Inspired that Visual-Language Model (VLM) can\nserve as a bridge to connect both text space and image space, we leverage the\nsemantic information contained in a text sequence as an explicit constraint to\ntrain a visual aligner. Thus, the visual aligner can map images from multiple\ndomains to a unified domain and achieve good generalization performance. To\nbetter depict semantic information, prompt tuning is applied to learn a\nsequence of learnable tokens. With explicit constraints of semantic\ninformation, PVA can learn unified cross-domain representation under limited\naccess to cross-domain data and achieves great zero-shot generalization ability\nin unseen domains. We verify PVA on a vision-based autonomous driving task with\nCARLA simulator. Experiments show that the agent generalizes well on unseen\ndomains under limited access to multi-domain data.\n", "link": "http://arxiv.org/abs/2406.03250v1", "date": "2024-06-05", "relevancy": 2.1652, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5357}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-based%20Visual%20Alignment%20for%20Zero-shot%20Policy%20Transfer&body=Title%3A%20Prompt-based%20Visual%20Alignment%20for%20Zero-shot%20Policy%20Transfer%0AAuthor%3A%20Haihan%20Gao%20and%20Rui%20Zhang%20and%20Qi%20Yi%20and%20Hantao%20Yao%20and%20Haochen%20Li%20and%20Jiaming%20Guo%20and%20Shaohui%20Peng%20and%20Yunkai%20Gao%20and%20QiCheng%20Wang%20and%20Xing%20Hu%20and%20Yuanbo%20Wen%20and%20Zihao%20Zhang%20and%20Zidong%20Du%20and%20Ling%20Li%20and%20Qi%20Guo%20and%20Yunji%20Chen%0AAbstract%3A%20%20%20Overfitting%20in%20RL%20has%20become%20one%20of%20the%20main%20obstacles%20to%20applications%20in%0Areinforcement%20learning%28RL%29.%20Existing%20methods%20do%20not%20provide%20explicit%20semantic%0Aconstrain%20for%20the%20feature%20extractor%2C%20hindering%20the%20agent%20from%20learning%20a%0Aunified%20cross-domain%20representation%20and%20resulting%20in%20performance%20degradation%20on%0Aunseen%20domains.%20Besides%2C%20abundant%20data%20from%20multiple%20domains%20are%20needed.%20To%0Aaddress%20these%20issues%2C%20in%20this%20work%2C%20we%20propose%20prompt-based%20visual%20alignment%0A%28PVA%29%2C%20a%20robust%20framework%20to%20mitigate%20the%20detrimental%20domain%20bias%20in%20the%20image%0Afor%20zero-shot%20policy%20transfer.%20Inspired%20that%20Visual-Language%20Model%20%28VLM%29%20can%0Aserve%20as%20a%20bridge%20to%20connect%20both%20text%20space%20and%20image%20space%2C%20we%20leverage%20the%0Asemantic%20information%20contained%20in%20a%20text%20sequence%20as%20an%20explicit%20constraint%20to%0Atrain%20a%20visual%20aligner.%20Thus%2C%20the%20visual%20aligner%20can%20map%20images%20from%20multiple%0Adomains%20to%20a%20unified%20domain%20and%20achieve%20good%20generalization%20performance.%20To%0Abetter%20depict%20semantic%20information%2C%20prompt%20tuning%20is%20applied%20to%20learn%20a%0Asequence%20of%20learnable%20tokens.%20With%20explicit%20constraints%20of%20semantic%0Ainformation%2C%20PVA%20can%20learn%20unified%20cross-domain%20representation%20under%20limited%0Aaccess%20to%20cross-domain%20data%20and%20achieves%20great%20zero-shot%20generalization%20ability%0Ain%20unseen%20domains.%20We%20verify%20PVA%20on%20a%20vision-based%20autonomous%20driving%20task%20with%0ACARLA%20simulator.%20Experiments%20show%20that%20the%20agent%20generalizes%20well%20on%20unseen%0Adomains%20under%20limited%20access%20to%20multi-domain%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-based%2520Visual%2520Alignment%2520for%2520Zero-shot%2520Policy%2520Transfer%26entry.906535625%3DHaihan%2520Gao%2520and%2520Rui%2520Zhang%2520and%2520Qi%2520Yi%2520and%2520Hantao%2520Yao%2520and%2520Haochen%2520Li%2520and%2520Jiaming%2520Guo%2520and%2520Shaohui%2520Peng%2520and%2520Yunkai%2520Gao%2520and%2520QiCheng%2520Wang%2520and%2520Xing%2520Hu%2520and%2520Yuanbo%2520Wen%2520and%2520Zihao%2520Zhang%2520and%2520Zidong%2520Du%2520and%2520Ling%2520Li%2520and%2520Qi%2520Guo%2520and%2520Yunji%2520Chen%26entry.1292438233%3D%2520%2520Overfitting%2520in%2520RL%2520has%2520become%2520one%2520of%2520the%2520main%2520obstacles%2520to%2520applications%2520in%250Areinforcement%2520learning%2528RL%2529.%2520Existing%2520methods%2520do%2520not%2520provide%2520explicit%2520semantic%250Aconstrain%2520for%2520the%2520feature%2520extractor%252C%2520hindering%2520the%2520agent%2520from%2520learning%2520a%250Aunified%2520cross-domain%2520representation%2520and%2520resulting%2520in%2520performance%2520degradation%2520on%250Aunseen%2520domains.%2520Besides%252C%2520abundant%2520data%2520from%2520multiple%2520domains%2520are%2520needed.%2520To%250Aaddress%2520these%2520issues%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520prompt-based%2520visual%2520alignment%250A%2528PVA%2529%252C%2520a%2520robust%2520framework%2520to%2520mitigate%2520the%2520detrimental%2520domain%2520bias%2520in%2520the%2520image%250Afor%2520zero-shot%2520policy%2520transfer.%2520Inspired%2520that%2520Visual-Language%2520Model%2520%2528VLM%2529%2520can%250Aserve%2520as%2520a%2520bridge%2520to%2520connect%2520both%2520text%2520space%2520and%2520image%2520space%252C%2520we%2520leverage%2520the%250Asemantic%2520information%2520contained%2520in%2520a%2520text%2520sequence%2520as%2520an%2520explicit%2520constraint%2520to%250Atrain%2520a%2520visual%2520aligner.%2520Thus%252C%2520the%2520visual%2520aligner%2520can%2520map%2520images%2520from%2520multiple%250Adomains%2520to%2520a%2520unified%2520domain%2520and%2520achieve%2520good%2520generalization%2520performance.%2520To%250Abetter%2520depict%2520semantic%2520information%252C%2520prompt%2520tuning%2520is%2520applied%2520to%2520learn%2520a%250Asequence%2520of%2520learnable%2520tokens.%2520With%2520explicit%2520constraints%2520of%2520semantic%250Ainformation%252C%2520PVA%2520can%2520learn%2520unified%2520cross-domain%2520representation%2520under%2520limited%250Aaccess%2520to%2520cross-domain%2520data%2520and%2520achieves%2520great%2520zero-shot%2520generalization%2520ability%250Ain%2520unseen%2520domains.%2520We%2520verify%2520PVA%2520on%2520a%2520vision-based%2520autonomous%2520driving%2520task%2520with%250ACARLA%2520simulator.%2520Experiments%2520show%2520that%2520the%2520agent%2520generalizes%2520well%2520on%2520unseen%250Adomains%2520under%2520limited%2520access%2520to%2520multi-domain%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-based%20Visual%20Alignment%20for%20Zero-shot%20Policy%20Transfer&entry.906535625=Haihan%20Gao%20and%20Rui%20Zhang%20and%20Qi%20Yi%20and%20Hantao%20Yao%20and%20Haochen%20Li%20and%20Jiaming%20Guo%20and%20Shaohui%20Peng%20and%20Yunkai%20Gao%20and%20QiCheng%20Wang%20and%20Xing%20Hu%20and%20Yuanbo%20Wen%20and%20Zihao%20Zhang%20and%20Zidong%20Du%20and%20Ling%20Li%20and%20Qi%20Guo%20and%20Yunji%20Chen&entry.1292438233=%20%20Overfitting%20in%20RL%20has%20become%20one%20of%20the%20main%20obstacles%20to%20applications%20in%0Areinforcement%20learning%28RL%29.%20Existing%20methods%20do%20not%20provide%20explicit%20semantic%0Aconstrain%20for%20the%20feature%20extractor%2C%20hindering%20the%20agent%20from%20learning%20a%0Aunified%20cross-domain%20representation%20and%20resulting%20in%20performance%20degradation%20on%0Aunseen%20domains.%20Besides%2C%20abundant%20data%20from%20multiple%20domains%20are%20needed.%20To%0Aaddress%20these%20issues%2C%20in%20this%20work%2C%20we%20propose%20prompt-based%20visual%20alignment%0A%28PVA%29%2C%20a%20robust%20framework%20to%20mitigate%20the%20detrimental%20domain%20bias%20in%20the%20image%0Afor%20zero-shot%20policy%20transfer.%20Inspired%20that%20Visual-Language%20Model%20%28VLM%29%20can%0Aserve%20as%20a%20bridge%20to%20connect%20both%20text%20space%20and%20image%20space%2C%20we%20leverage%20the%0Asemantic%20information%20contained%20in%20a%20text%20sequence%20as%20an%20explicit%20constraint%20to%0Atrain%20a%20visual%20aligner.%20Thus%2C%20the%20visual%20aligner%20can%20map%20images%20from%20multiple%0Adomains%20to%20a%20unified%20domain%20and%20achieve%20good%20generalization%20performance.%20To%0Abetter%20depict%20semantic%20information%2C%20prompt%20tuning%20is%20applied%20to%20learn%20a%0Asequence%20of%20learnable%20tokens.%20With%20explicit%20constraints%20of%20semantic%0Ainformation%2C%20PVA%20can%20learn%20unified%20cross-domain%20representation%20under%20limited%0Aaccess%20to%20cross-domain%20data%20and%20achieves%20great%20zero-shot%20generalization%20ability%0Ain%20unseen%20domains.%20We%20verify%20PVA%20on%20a%20vision-based%20autonomous%20driving%20task%20with%0ACARLA%20simulator.%20Experiments%20show%20that%20the%20agent%20generalizes%20well%20on%20unseen%0Adomains%20under%20limited%20access%20to%20multi-domain%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03250v1&entry.124074799=Read"},
{"title": "ADer: A Comprehensive Benchmark for Multi-class Visual Anomaly Detection", "author": "Jiangning Zhang and Haoyang He and Zhenye Gan and Qingdong He and Yuxuan Cai and Zhucun Xue and Yabiao Wang and Chengjie Wang and Lei Xie and Yong Liu", "abstract": "  Visual anomaly detection aims to identify anomalous regions in images through\nunsupervised learning paradigms, with increasing application demand and value\nin fields such as industrial inspection and medical lesion detection. Despite\nsignificant progress in recent years, there is a lack of comprehensive\nbenchmarks to adequately evaluate the performance of various mainstream methods\nacross different datasets under the practical multi-class setting. The absence\nof standardized experimental setups can lead to potential biases in training\nepochs, resolution, and metric results, resulting in erroneous conclusions.\nThis paper addresses this issue by proposing a comprehensive visual anomaly\ndetection benchmark, \\textbf{\\textit{ADer}}, which is a modular framework that\nis highly extensible for new methods. The benchmark includes multiple datasets\nfrom industrial and medical domains, implementing fifteen state-of-the-art\nmethods and nine comprehensive metrics. Additionally, we have open-sourced the\nGPU-assisted \\href{https://pypi.org/project/ADEval}{ADEval} package to address\nthe slow evaluation problem of metrics like time-consuming mAU-PRO on\nlarge-scale data, significantly reducing evaluation time by more than\n\\textit{1000-fold}. Through extensive experimental results, we objectively\nreveal the strengths and weaknesses of different methods and provide insights\ninto the challenges and future directions of multi-class visual anomaly\ndetection. We hope that \\textbf{\\textit{ADer}} will become a valuable resource\nfor researchers and practitioners in the field, promoting the development of\nmore robust and generalizable anomaly detection systems. Full codes have been\nattached in Appendix and open-sourced at\n\\url{https://github.com/zhangzjn/ader}.\n", "link": "http://arxiv.org/abs/2406.03262v1", "date": "2024-06-05", "relevancy": 2.1641, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5447}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5402}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADer%3A%20A%20Comprehensive%20Benchmark%20for%20Multi-class%20Visual%20Anomaly%20Detection&body=Title%3A%20ADer%3A%20A%20Comprehensive%20Benchmark%20for%20Multi-class%20Visual%20Anomaly%20Detection%0AAuthor%3A%20Jiangning%20Zhang%20and%20Haoyang%20He%20and%20Zhenye%20Gan%20and%20Qingdong%20He%20and%20Yuxuan%20Cai%20and%20Zhucun%20Xue%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lei%20Xie%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Visual%20anomaly%20detection%20aims%20to%20identify%20anomalous%20regions%20in%20images%20through%0Aunsupervised%20learning%20paradigms%2C%20with%20increasing%20application%20demand%20and%20value%0Ain%20fields%20such%20as%20industrial%20inspection%20and%20medical%20lesion%20detection.%20Despite%0Asignificant%20progress%20in%20recent%20years%2C%20there%20is%20a%20lack%20of%20comprehensive%0Abenchmarks%20to%20adequately%20evaluate%20the%20performance%20of%20various%20mainstream%20methods%0Aacross%20different%20datasets%20under%20the%20practical%20multi-class%20setting.%20The%20absence%0Aof%20standardized%20experimental%20setups%20can%20lead%20to%20potential%20biases%20in%20training%0Aepochs%2C%20resolution%2C%20and%20metric%20results%2C%20resulting%20in%20erroneous%20conclusions.%0AThis%20paper%20addresses%20this%20issue%20by%20proposing%20a%20comprehensive%20visual%20anomaly%0Adetection%20benchmark%2C%20%5Ctextbf%7B%5Ctextit%7BADer%7D%7D%2C%20which%20is%20a%20modular%20framework%20that%0Ais%20highly%20extensible%20for%20new%20methods.%20The%20benchmark%20includes%20multiple%20datasets%0Afrom%20industrial%20and%20medical%20domains%2C%20implementing%20fifteen%20state-of-the-art%0Amethods%20and%20nine%20comprehensive%20metrics.%20Additionally%2C%20we%20have%20open-sourced%20the%0AGPU-assisted%20%5Chref%7Bhttps%3A//pypi.org/project/ADEval%7D%7BADEval%7D%20package%20to%20address%0Athe%20slow%20evaluation%20problem%20of%20metrics%20like%20time-consuming%20mAU-PRO%20on%0Alarge-scale%20data%2C%20significantly%20reducing%20evaluation%20time%20by%20more%20than%0A%5Ctextit%7B1000-fold%7D.%20Through%20extensive%20experimental%20results%2C%20we%20objectively%0Areveal%20the%20strengths%20and%20weaknesses%20of%20different%20methods%20and%20provide%20insights%0Ainto%20the%20challenges%20and%20future%20directions%20of%20multi-class%20visual%20anomaly%0Adetection.%20We%20hope%20that%20%5Ctextbf%7B%5Ctextit%7BADer%7D%7D%20will%20become%20a%20valuable%20resource%0Afor%20researchers%20and%20practitioners%20in%20the%20field%2C%20promoting%20the%20development%20of%0Amore%20robust%20and%20generalizable%20anomaly%20detection%20systems.%20Full%20codes%20have%20been%0Aattached%20in%20Appendix%20and%20open-sourced%20at%0A%5Curl%7Bhttps%3A//github.com/zhangzjn/ader%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADer%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Multi-class%2520Visual%2520Anomaly%2520Detection%26entry.906535625%3DJiangning%2520Zhang%2520and%2520Haoyang%2520He%2520and%2520Zhenye%2520Gan%2520and%2520Qingdong%2520He%2520and%2520Yuxuan%2520Cai%2520and%2520Zhucun%2520Xue%2520and%2520Yabiao%2520Wang%2520and%2520Chengjie%2520Wang%2520and%2520Lei%2520Xie%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520Visual%2520anomaly%2520detection%2520aims%2520to%2520identify%2520anomalous%2520regions%2520in%2520images%2520through%250Aunsupervised%2520learning%2520paradigms%252C%2520with%2520increasing%2520application%2520demand%2520and%2520value%250Ain%2520fields%2520such%2520as%2520industrial%2520inspection%2520and%2520medical%2520lesion%2520detection.%2520Despite%250Asignificant%2520progress%2520in%2520recent%2520years%252C%2520there%2520is%2520a%2520lack%2520of%2520comprehensive%250Abenchmarks%2520to%2520adequately%2520evaluate%2520the%2520performance%2520of%2520various%2520mainstream%2520methods%250Aacross%2520different%2520datasets%2520under%2520the%2520practical%2520multi-class%2520setting.%2520The%2520absence%250Aof%2520standardized%2520experimental%2520setups%2520can%2520lead%2520to%2520potential%2520biases%2520in%2520training%250Aepochs%252C%2520resolution%252C%2520and%2520metric%2520results%252C%2520resulting%2520in%2520erroneous%2520conclusions.%250AThis%2520paper%2520addresses%2520this%2520issue%2520by%2520proposing%2520a%2520comprehensive%2520visual%2520anomaly%250Adetection%2520benchmark%252C%2520%255Ctextbf%257B%255Ctextit%257BADer%257D%257D%252C%2520which%2520is%2520a%2520modular%2520framework%2520that%250Ais%2520highly%2520extensible%2520for%2520new%2520methods.%2520The%2520benchmark%2520includes%2520multiple%2520datasets%250Afrom%2520industrial%2520and%2520medical%2520domains%252C%2520implementing%2520fifteen%2520state-of-the-art%250Amethods%2520and%2520nine%2520comprehensive%2520metrics.%2520Additionally%252C%2520we%2520have%2520open-sourced%2520the%250AGPU-assisted%2520%255Chref%257Bhttps%253A//pypi.org/project/ADEval%257D%257BADEval%257D%2520package%2520to%2520address%250Athe%2520slow%2520evaluation%2520problem%2520of%2520metrics%2520like%2520time-consuming%2520mAU-PRO%2520on%250Alarge-scale%2520data%252C%2520significantly%2520reducing%2520evaluation%2520time%2520by%2520more%2520than%250A%255Ctextit%257B1000-fold%257D.%2520Through%2520extensive%2520experimental%2520results%252C%2520we%2520objectively%250Areveal%2520the%2520strengths%2520and%2520weaknesses%2520of%2520different%2520methods%2520and%2520provide%2520insights%250Ainto%2520the%2520challenges%2520and%2520future%2520directions%2520of%2520multi-class%2520visual%2520anomaly%250Adetection.%2520We%2520hope%2520that%2520%255Ctextbf%257B%255Ctextit%257BADer%257D%257D%2520will%2520become%2520a%2520valuable%2520resource%250Afor%2520researchers%2520and%2520practitioners%2520in%2520the%2520field%252C%2520promoting%2520the%2520development%2520of%250Amore%2520robust%2520and%2520generalizable%2520anomaly%2520detection%2520systems.%2520Full%2520codes%2520have%2520been%250Aattached%2520in%2520Appendix%2520and%2520open-sourced%2520at%250A%255Curl%257Bhttps%253A//github.com/zhangzjn/ader%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADer%3A%20A%20Comprehensive%20Benchmark%20for%20Multi-class%20Visual%20Anomaly%20Detection&entry.906535625=Jiangning%20Zhang%20and%20Haoyang%20He%20and%20Zhenye%20Gan%20and%20Qingdong%20He%20and%20Yuxuan%20Cai%20and%20Zhucun%20Xue%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lei%20Xie%20and%20Yong%20Liu&entry.1292438233=%20%20Visual%20anomaly%20detection%20aims%20to%20identify%20anomalous%20regions%20in%20images%20through%0Aunsupervised%20learning%20paradigms%2C%20with%20increasing%20application%20demand%20and%20value%0Ain%20fields%20such%20as%20industrial%20inspection%20and%20medical%20lesion%20detection.%20Despite%0Asignificant%20progress%20in%20recent%20years%2C%20there%20is%20a%20lack%20of%20comprehensive%0Abenchmarks%20to%20adequately%20evaluate%20the%20performance%20of%20various%20mainstream%20methods%0Aacross%20different%20datasets%20under%20the%20practical%20multi-class%20setting.%20The%20absence%0Aof%20standardized%20experimental%20setups%20can%20lead%20to%20potential%20biases%20in%20training%0Aepochs%2C%20resolution%2C%20and%20metric%20results%2C%20resulting%20in%20erroneous%20conclusions.%0AThis%20paper%20addresses%20this%20issue%20by%20proposing%20a%20comprehensive%20visual%20anomaly%0Adetection%20benchmark%2C%20%5Ctextbf%7B%5Ctextit%7BADer%7D%7D%2C%20which%20is%20a%20modular%20framework%20that%0Ais%20highly%20extensible%20for%20new%20methods.%20The%20benchmark%20includes%20multiple%20datasets%0Afrom%20industrial%20and%20medical%20domains%2C%20implementing%20fifteen%20state-of-the-art%0Amethods%20and%20nine%20comprehensive%20metrics.%20Additionally%2C%20we%20have%20open-sourced%20the%0AGPU-assisted%20%5Chref%7Bhttps%3A//pypi.org/project/ADEval%7D%7BADEval%7D%20package%20to%20address%0Athe%20slow%20evaluation%20problem%20of%20metrics%20like%20time-consuming%20mAU-PRO%20on%0Alarge-scale%20data%2C%20significantly%20reducing%20evaluation%20time%20by%20more%20than%0A%5Ctextit%7B1000-fold%7D.%20Through%20extensive%20experimental%20results%2C%20we%20objectively%0Areveal%20the%20strengths%20and%20weaknesses%20of%20different%20methods%20and%20provide%20insights%0Ainto%20the%20challenges%20and%20future%20directions%20of%20multi-class%20visual%20anomaly%0Adetection.%20We%20hope%20that%20%5Ctextbf%7B%5Ctextit%7BADer%7D%7D%20will%20become%20a%20valuable%20resource%0Afor%20researchers%20and%20practitioners%20in%20the%20field%2C%20promoting%20the%20development%20of%0Amore%20robust%20and%20generalizable%20anomaly%20detection%20systems.%20Full%20codes%20have%20been%0Aattached%20in%20Appendix%20and%20open-sourced%20at%0A%5Curl%7Bhttps%3A//github.com/zhangzjn/ader%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03262v1&entry.124074799=Read"},
{"title": "Highway Value Iteration Networks", "author": "Yuhui Wang and Weida Li and Francesco Faccio and Qingyuan Wu and J\u00fcrgen Schmidhuber", "abstract": "  Value iteration networks (VINs) enable end-to-end learning for planning tasks\nby employing a differentiable \"planning module\" that approximates the value\niteration algorithm. However, long-term planning remains a challenge because\ntraining very deep VINs is difficult. To address this problem, we embed highway\nvalue iteration -- a recent algorithm designed to facilitate long-term credit\nassignment -- into the structure of VINs. This improvement augments the\n\"planning module\" of the VIN with three additional components: 1) an \"aggregate\ngate,\" which constructs skip connections to improve information flow across\nmany layers; 2) an \"exploration module,\" crafted to increase the diversity of\ninformation and gradient flow in spatial dimensions; 3) a \"filter gate\"\ndesigned to ensure safe exploration. The resulting novel highway VIN can be\ntrained effectively with hundreds of layers using standard backpropagation. In\nlong-term planning tasks requiring hundreds of planning steps, deep highway\nVINs outperform both traditional VINs and several advanced, very deep NNs.\n", "link": "http://arxiv.org/abs/2406.03485v1", "date": "2024-06-05", "relevancy": 2.1454, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4396}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4244}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Highway%20Value%20Iteration%20Networks&body=Title%3A%20Highway%20Value%20Iteration%20Networks%0AAuthor%3A%20Yuhui%20Wang%20and%20Weida%20Li%20and%20Francesco%20Faccio%20and%20Qingyuan%20Wu%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Value%20iteration%20networks%20%28VINs%29%20enable%20end-to-end%20learning%20for%20planning%20tasks%0Aby%20employing%20a%20differentiable%20%22planning%20module%22%20that%20approximates%20the%20value%0Aiteration%20algorithm.%20However%2C%20long-term%20planning%20remains%20a%20challenge%20because%0Atraining%20very%20deep%20VINs%20is%20difficult.%20To%20address%20this%20problem%2C%20we%20embed%20highway%0Avalue%20iteration%20--%20a%20recent%20algorithm%20designed%20to%20facilitate%20long-term%20credit%0Aassignment%20--%20into%20the%20structure%20of%20VINs.%20This%20improvement%20augments%20the%0A%22planning%20module%22%20of%20the%20VIN%20with%20three%20additional%20components%3A%201%29%20an%20%22aggregate%0Agate%2C%22%20which%20constructs%20skip%20connections%20to%20improve%20information%20flow%20across%0Amany%20layers%3B%202%29%20an%20%22exploration%20module%2C%22%20crafted%20to%20increase%20the%20diversity%20of%0Ainformation%20and%20gradient%20flow%20in%20spatial%20dimensions%3B%203%29%20a%20%22filter%20gate%22%0Adesigned%20to%20ensure%20safe%20exploration.%20The%20resulting%20novel%20highway%20VIN%20can%20be%0Atrained%20effectively%20with%20hundreds%20of%20layers%20using%20standard%20backpropagation.%20In%0Along-term%20planning%20tasks%20requiring%20hundreds%20of%20planning%20steps%2C%20deep%20highway%0AVINs%20outperform%20both%20traditional%20VINs%20and%20several%20advanced%2C%20very%20deep%20NNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHighway%2520Value%2520Iteration%2520Networks%26entry.906535625%3DYuhui%2520Wang%2520and%2520Weida%2520Li%2520and%2520Francesco%2520Faccio%2520and%2520Qingyuan%2520Wu%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Value%2520iteration%2520networks%2520%2528VINs%2529%2520enable%2520end-to-end%2520learning%2520for%2520planning%2520tasks%250Aby%2520employing%2520a%2520differentiable%2520%2522planning%2520module%2522%2520that%2520approximates%2520the%2520value%250Aiteration%2520algorithm.%2520However%252C%2520long-term%2520planning%2520remains%2520a%2520challenge%2520because%250Atraining%2520very%2520deep%2520VINs%2520is%2520difficult.%2520To%2520address%2520this%2520problem%252C%2520we%2520embed%2520highway%250Avalue%2520iteration%2520--%2520a%2520recent%2520algorithm%2520designed%2520to%2520facilitate%2520long-term%2520credit%250Aassignment%2520--%2520into%2520the%2520structure%2520of%2520VINs.%2520This%2520improvement%2520augments%2520the%250A%2522planning%2520module%2522%2520of%2520the%2520VIN%2520with%2520three%2520additional%2520components%253A%25201%2529%2520an%2520%2522aggregate%250Agate%252C%2522%2520which%2520constructs%2520skip%2520connections%2520to%2520improve%2520information%2520flow%2520across%250Amany%2520layers%253B%25202%2529%2520an%2520%2522exploration%2520module%252C%2522%2520crafted%2520to%2520increase%2520the%2520diversity%2520of%250Ainformation%2520and%2520gradient%2520flow%2520in%2520spatial%2520dimensions%253B%25203%2529%2520a%2520%2522filter%2520gate%2522%250Adesigned%2520to%2520ensure%2520safe%2520exploration.%2520The%2520resulting%2520novel%2520highway%2520VIN%2520can%2520be%250Atrained%2520effectively%2520with%2520hundreds%2520of%2520layers%2520using%2520standard%2520backpropagation.%2520In%250Along-term%2520planning%2520tasks%2520requiring%2520hundreds%2520of%2520planning%2520steps%252C%2520deep%2520highway%250AVINs%2520outperform%2520both%2520traditional%2520VINs%2520and%2520several%2520advanced%252C%2520very%2520deep%2520NNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Highway%20Value%20Iteration%20Networks&entry.906535625=Yuhui%20Wang%20and%20Weida%20Li%20and%20Francesco%20Faccio%20and%20Qingyuan%20Wu%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Value%20iteration%20networks%20%28VINs%29%20enable%20end-to-end%20learning%20for%20planning%20tasks%0Aby%20employing%20a%20differentiable%20%22planning%20module%22%20that%20approximates%20the%20value%0Aiteration%20algorithm.%20However%2C%20long-term%20planning%20remains%20a%20challenge%20because%0Atraining%20very%20deep%20VINs%20is%20difficult.%20To%20address%20this%20problem%2C%20we%20embed%20highway%0Avalue%20iteration%20--%20a%20recent%20algorithm%20designed%20to%20facilitate%20long-term%20credit%0Aassignment%20--%20into%20the%20structure%20of%20VINs.%20This%20improvement%20augments%20the%0A%22planning%20module%22%20of%20the%20VIN%20with%20three%20additional%20components%3A%201%29%20an%20%22aggregate%0Agate%2C%22%20which%20constructs%20skip%20connections%20to%20improve%20information%20flow%20across%0Amany%20layers%3B%202%29%20an%20%22exploration%20module%2C%22%20crafted%20to%20increase%20the%20diversity%20of%0Ainformation%20and%20gradient%20flow%20in%20spatial%20dimensions%3B%203%29%20a%20%22filter%20gate%22%0Adesigned%20to%20ensure%20safe%20exploration.%20The%20resulting%20novel%20highway%20VIN%20can%20be%0Atrained%20effectively%20with%20hundreds%20of%20layers%20using%20standard%20backpropagation.%20In%0Along-term%20planning%20tasks%20requiring%20hundreds%20of%20planning%20steps%2C%20deep%20highway%0AVINs%20outperform%20both%20traditional%20VINs%20and%20several%20advanced%2C%20very%20deep%20NNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03485v1&entry.124074799=Read"},
{"title": "cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and\n  Glitch Generation", "author": "Tom Dooney and Lyana Curier and Daniel Tan and Melissa Lopez and Chris Van Den Broeck and Stefano Bromuri", "abstract": "  Simulating realistic time-domain observations of gravitational waves (GWs)\nand GW detector glitches can help in advancing GW data analysis. Simulated data\ncan be used in downstream tasks by augmenting datasets for signal searches,\nbalancing data sets for machine learning, and validating detection schemes. In\nthis work, we present Conditional Derivative GAN (cDVGAN), a novel conditional\nmodel in the Generative Adversarial Network framework for simulating multiple\nclasses of time-domain observations that represent gravitational waves (GWs)\nand detector glitches. cDVGAN can also generate generalized hybrid samples that\nspan the variation between classes through interpolation in the conditioned\nclass vector. cDVGAN introduces an additional player into the typical 2-player\nadversarial game of GANs, where an auxiliary discriminator analyzes the\nfirst-order derivative time-series. Our results show that this provides\nsynthetic data that better captures the features of the original data. cDVGAN\nconditions on three classes, two denoised from LIGO blip and tomte glitch\nevents from its 3rd observing run (O3), and the third representing binary black\nhole (BBH) mergers. Our proposed cDVGAN outperforms 4 different baseline GAN\nmodels in replicating the features of the three classes. Specifically, our\nexperiments show that training convolutional neural networks (CNNs) with our\ncDVGAN-generated data improves the detection of samples embedded in detector\nnoise beyond the synthetic data from other state-of-the-art GAN models. Our\nbest synthetic dataset yields as much as a 4.2% increase in\narea-under-the-curve (AUC) performance compared to synthetic datasets from\nbaseline GANs. Moreover, training the CNN with hybrid samples from our cDVGAN\noutperforms CNNs trained only on the standard classes, when identifying real\nsamples embedded in LIGO detector background (4% AUC improvement for cDVGAN).\n", "link": "http://arxiv.org/abs/2401.16356v4", "date": "2024-06-05", "relevancy": 2.1225, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5484}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5207}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20cDVGAN%3A%20One%20Flexible%20Model%20for%20Multi-class%20Gravitational%20Wave%20Signal%20and%0A%20%20Glitch%20Generation&body=Title%3A%20cDVGAN%3A%20One%20Flexible%20Model%20for%20Multi-class%20Gravitational%20Wave%20Signal%20and%0A%20%20Glitch%20Generation%0AAuthor%3A%20Tom%20Dooney%20and%20Lyana%20Curier%20and%20Daniel%20Tan%20and%20Melissa%20Lopez%20and%20Chris%20Van%20Den%20Broeck%20and%20Stefano%20Bromuri%0AAbstract%3A%20%20%20Simulating%20realistic%20time-domain%20observations%20of%20gravitational%20waves%20%28GWs%29%0Aand%20GW%20detector%20glitches%20can%20help%20in%20advancing%20GW%20data%20analysis.%20Simulated%20data%0Acan%20be%20used%20in%20downstream%20tasks%20by%20augmenting%20datasets%20for%20signal%20searches%2C%0Abalancing%20data%20sets%20for%20machine%20learning%2C%20and%20validating%20detection%20schemes.%20In%0Athis%20work%2C%20we%20present%20Conditional%20Derivative%20GAN%20%28cDVGAN%29%2C%20a%20novel%20conditional%0Amodel%20in%20the%20Generative%20Adversarial%20Network%20framework%20for%20simulating%20multiple%0Aclasses%20of%20time-domain%20observations%20that%20represent%20gravitational%20waves%20%28GWs%29%0Aand%20detector%20glitches.%20cDVGAN%20can%20also%20generate%20generalized%20hybrid%20samples%20that%0Aspan%20the%20variation%20between%20classes%20through%20interpolation%20in%20the%20conditioned%0Aclass%20vector.%20cDVGAN%20introduces%20an%20additional%20player%20into%20the%20typical%202-player%0Aadversarial%20game%20of%20GANs%2C%20where%20an%20auxiliary%20discriminator%20analyzes%20the%0Afirst-order%20derivative%20time-series.%20Our%20results%20show%20that%20this%20provides%0Asynthetic%20data%20that%20better%20captures%20the%20features%20of%20the%20original%20data.%20cDVGAN%0Aconditions%20on%20three%20classes%2C%20two%20denoised%20from%20LIGO%20blip%20and%20tomte%20glitch%0Aevents%20from%20its%203rd%20observing%20run%20%28O3%29%2C%20and%20the%20third%20representing%20binary%20black%0Ahole%20%28BBH%29%20mergers.%20Our%20proposed%20cDVGAN%20outperforms%204%20different%20baseline%20GAN%0Amodels%20in%20replicating%20the%20features%20of%20the%20three%20classes.%20Specifically%2C%20our%0Aexperiments%20show%20that%20training%20convolutional%20neural%20networks%20%28CNNs%29%20with%20our%0AcDVGAN-generated%20data%20improves%20the%20detection%20of%20samples%20embedded%20in%20detector%0Anoise%20beyond%20the%20synthetic%20data%20from%20other%20state-of-the-art%20GAN%20models.%20Our%0Abest%20synthetic%20dataset%20yields%20as%20much%20as%20a%204.2%25%20increase%20in%0Aarea-under-the-curve%20%28AUC%29%20performance%20compared%20to%20synthetic%20datasets%20from%0Abaseline%20GANs.%20Moreover%2C%20training%20the%20CNN%20with%20hybrid%20samples%20from%20our%20cDVGAN%0Aoutperforms%20CNNs%20trained%20only%20on%20the%20standard%20classes%2C%20when%20identifying%20real%0Asamples%20embedded%20in%20LIGO%20detector%20background%20%284%25%20AUC%20improvement%20for%20cDVGAN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16356v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DcDVGAN%253A%2520One%2520Flexible%2520Model%2520for%2520Multi-class%2520Gravitational%2520Wave%2520Signal%2520and%250A%2520%2520Glitch%2520Generation%26entry.906535625%3DTom%2520Dooney%2520and%2520Lyana%2520Curier%2520and%2520Daniel%2520Tan%2520and%2520Melissa%2520Lopez%2520and%2520Chris%2520Van%2520Den%2520Broeck%2520and%2520Stefano%2520Bromuri%26entry.1292438233%3D%2520%2520Simulating%2520realistic%2520time-domain%2520observations%2520of%2520gravitational%2520waves%2520%2528GWs%2529%250Aand%2520GW%2520detector%2520glitches%2520can%2520help%2520in%2520advancing%2520GW%2520data%2520analysis.%2520Simulated%2520data%250Acan%2520be%2520used%2520in%2520downstream%2520tasks%2520by%2520augmenting%2520datasets%2520for%2520signal%2520searches%252C%250Abalancing%2520data%2520sets%2520for%2520machine%2520learning%252C%2520and%2520validating%2520detection%2520schemes.%2520In%250Athis%2520work%252C%2520we%2520present%2520Conditional%2520Derivative%2520GAN%2520%2528cDVGAN%2529%252C%2520a%2520novel%2520conditional%250Amodel%2520in%2520the%2520Generative%2520Adversarial%2520Network%2520framework%2520for%2520simulating%2520multiple%250Aclasses%2520of%2520time-domain%2520observations%2520that%2520represent%2520gravitational%2520waves%2520%2528GWs%2529%250Aand%2520detector%2520glitches.%2520cDVGAN%2520can%2520also%2520generate%2520generalized%2520hybrid%2520samples%2520that%250Aspan%2520the%2520variation%2520between%2520classes%2520through%2520interpolation%2520in%2520the%2520conditioned%250Aclass%2520vector.%2520cDVGAN%2520introduces%2520an%2520additional%2520player%2520into%2520the%2520typical%25202-player%250Aadversarial%2520game%2520of%2520GANs%252C%2520where%2520an%2520auxiliary%2520discriminator%2520analyzes%2520the%250Afirst-order%2520derivative%2520time-series.%2520Our%2520results%2520show%2520that%2520this%2520provides%250Asynthetic%2520data%2520that%2520better%2520captures%2520the%2520features%2520of%2520the%2520original%2520data.%2520cDVGAN%250Aconditions%2520on%2520three%2520classes%252C%2520two%2520denoised%2520from%2520LIGO%2520blip%2520and%2520tomte%2520glitch%250Aevents%2520from%2520its%25203rd%2520observing%2520run%2520%2528O3%2529%252C%2520and%2520the%2520third%2520representing%2520binary%2520black%250Ahole%2520%2528BBH%2529%2520mergers.%2520Our%2520proposed%2520cDVGAN%2520outperforms%25204%2520different%2520baseline%2520GAN%250Amodels%2520in%2520replicating%2520the%2520features%2520of%2520the%2520three%2520classes.%2520Specifically%252C%2520our%250Aexperiments%2520show%2520that%2520training%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520with%2520our%250AcDVGAN-generated%2520data%2520improves%2520the%2520detection%2520of%2520samples%2520embedded%2520in%2520detector%250Anoise%2520beyond%2520the%2520synthetic%2520data%2520from%2520other%2520state-of-the-art%2520GAN%2520models.%2520Our%250Abest%2520synthetic%2520dataset%2520yields%2520as%2520much%2520as%2520a%25204.2%2525%2520increase%2520in%250Aarea-under-the-curve%2520%2528AUC%2529%2520performance%2520compared%2520to%2520synthetic%2520datasets%2520from%250Abaseline%2520GANs.%2520Moreover%252C%2520training%2520the%2520CNN%2520with%2520hybrid%2520samples%2520from%2520our%2520cDVGAN%250Aoutperforms%2520CNNs%2520trained%2520only%2520on%2520the%2520standard%2520classes%252C%2520when%2520identifying%2520real%250Asamples%2520embedded%2520in%2520LIGO%2520detector%2520background%2520%25284%2525%2520AUC%2520improvement%2520for%2520cDVGAN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16356v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=cDVGAN%3A%20One%20Flexible%20Model%20for%20Multi-class%20Gravitational%20Wave%20Signal%20and%0A%20%20Glitch%20Generation&entry.906535625=Tom%20Dooney%20and%20Lyana%20Curier%20and%20Daniel%20Tan%20and%20Melissa%20Lopez%20and%20Chris%20Van%20Den%20Broeck%20and%20Stefano%20Bromuri&entry.1292438233=%20%20Simulating%20realistic%20time-domain%20observations%20of%20gravitational%20waves%20%28GWs%29%0Aand%20GW%20detector%20glitches%20can%20help%20in%20advancing%20GW%20data%20analysis.%20Simulated%20data%0Acan%20be%20used%20in%20downstream%20tasks%20by%20augmenting%20datasets%20for%20signal%20searches%2C%0Abalancing%20data%20sets%20for%20machine%20learning%2C%20and%20validating%20detection%20schemes.%20In%0Athis%20work%2C%20we%20present%20Conditional%20Derivative%20GAN%20%28cDVGAN%29%2C%20a%20novel%20conditional%0Amodel%20in%20the%20Generative%20Adversarial%20Network%20framework%20for%20simulating%20multiple%0Aclasses%20of%20time-domain%20observations%20that%20represent%20gravitational%20waves%20%28GWs%29%0Aand%20detector%20glitches.%20cDVGAN%20can%20also%20generate%20generalized%20hybrid%20samples%20that%0Aspan%20the%20variation%20between%20classes%20through%20interpolation%20in%20the%20conditioned%0Aclass%20vector.%20cDVGAN%20introduces%20an%20additional%20player%20into%20the%20typical%202-player%0Aadversarial%20game%20of%20GANs%2C%20where%20an%20auxiliary%20discriminator%20analyzes%20the%0Afirst-order%20derivative%20time-series.%20Our%20results%20show%20that%20this%20provides%0Asynthetic%20data%20that%20better%20captures%20the%20features%20of%20the%20original%20data.%20cDVGAN%0Aconditions%20on%20three%20classes%2C%20two%20denoised%20from%20LIGO%20blip%20and%20tomte%20glitch%0Aevents%20from%20its%203rd%20observing%20run%20%28O3%29%2C%20and%20the%20third%20representing%20binary%20black%0Ahole%20%28BBH%29%20mergers.%20Our%20proposed%20cDVGAN%20outperforms%204%20different%20baseline%20GAN%0Amodels%20in%20replicating%20the%20features%20of%20the%20three%20classes.%20Specifically%2C%20our%0Aexperiments%20show%20that%20training%20convolutional%20neural%20networks%20%28CNNs%29%20with%20our%0AcDVGAN-generated%20data%20improves%20the%20detection%20of%20samples%20embedded%20in%20detector%0Anoise%20beyond%20the%20synthetic%20data%20from%20other%20state-of-the-art%20GAN%20models.%20Our%0Abest%20synthetic%20dataset%20yields%20as%20much%20as%20a%204.2%25%20increase%20in%0Aarea-under-the-curve%20%28AUC%29%20performance%20compared%20to%20synthetic%20datasets%20from%0Abaseline%20GANs.%20Moreover%2C%20training%20the%20CNN%20with%20hybrid%20samples%20from%20our%20cDVGAN%0Aoutperforms%20CNNs%20trained%20only%20on%20the%20standard%20classes%2C%20when%20identifying%20real%0Asamples%20embedded%20in%20LIGO%20detector%20background%20%284%25%20AUC%20improvement%20for%20cDVGAN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16356v4&entry.124074799=Read"},
{"title": "Interactive Text-to-Image Retrieval with Large Language Models: A\n  Plug-and-Play Approach", "author": "Saehyung Lee and Sangwon Yu and Junsung Park and Jihun Yi and Sungroh Yoon", "abstract": "  In this paper, we primarily address the issue of dialogue-form context query\nwithin the interactive text-to-image retrieval task. Our methodology, PlugIR,\nactively utilizes the general instruction-following capability of LLMs in two\nways. First, by reformulating the dialogue-form context, we eliminate the\nnecessity of fine-tuning a retrieval model on existing visual dialogue data,\nthereby enabling the use of any arbitrary black-box model. Second, we construct\nthe LLM questioner to generate non-redundant questions about the attributes of\nthe target image, based on the information of retrieval candidate images in the\ncurrent context. This approach mitigates the issues of noisiness and redundancy\nin the generated questions. Beyond our methodology, we propose a novel\nevaluation metric, Best log Rank Integral (BRI), for a comprehensive assessment\nof the interactive retrieval system. PlugIR demonstrates superior performance\ncompared to both zero-shot and fine-tuned baselines in various benchmarks.\nAdditionally, the two methodologies comprising PlugIR can be flexibly applied\ntogether or separately in various situations. Our codes are available at\nhttps://github.com/Saehyung-Lee/PlugIR.\n", "link": "http://arxiv.org/abs/2406.03411v1", "date": "2024-06-05", "relevancy": 2.1154, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5391}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5312}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Text-to-Image%20Retrieval%20with%20Large%20Language%20Models%3A%20A%0A%20%20Plug-and-Play%20Approach&body=Title%3A%20Interactive%20Text-to-Image%20Retrieval%20with%20Large%20Language%20Models%3A%20A%0A%20%20Plug-and-Play%20Approach%0AAuthor%3A%20Saehyung%20Lee%20and%20Sangwon%20Yu%20and%20Junsung%20Park%20and%20Jihun%20Yi%20and%20Sungroh%20Yoon%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20primarily%20address%20the%20issue%20of%20dialogue-form%20context%20query%0Awithin%20the%20interactive%20text-to-image%20retrieval%20task.%20Our%20methodology%2C%20PlugIR%2C%0Aactively%20utilizes%20the%20general%20instruction-following%20capability%20of%20LLMs%20in%20two%0Aways.%20First%2C%20by%20reformulating%20the%20dialogue-form%20context%2C%20we%20eliminate%20the%0Anecessity%20of%20fine-tuning%20a%20retrieval%20model%20on%20existing%20visual%20dialogue%20data%2C%0Athereby%20enabling%20the%20use%20of%20any%20arbitrary%20black-box%20model.%20Second%2C%20we%20construct%0Athe%20LLM%20questioner%20to%20generate%20non-redundant%20questions%20about%20the%20attributes%20of%0Athe%20target%20image%2C%20based%20on%20the%20information%20of%20retrieval%20candidate%20images%20in%20the%0Acurrent%20context.%20This%20approach%20mitigates%20the%20issues%20of%20noisiness%20and%20redundancy%0Ain%20the%20generated%20questions.%20Beyond%20our%20methodology%2C%20we%20propose%20a%20novel%0Aevaluation%20metric%2C%20Best%20log%20Rank%20Integral%20%28BRI%29%2C%20for%20a%20comprehensive%20assessment%0Aof%20the%20interactive%20retrieval%20system.%20PlugIR%20demonstrates%20superior%20performance%0Acompared%20to%20both%20zero-shot%20and%20fine-tuned%20baselines%20in%20various%20benchmarks.%0AAdditionally%2C%20the%20two%20methodologies%20comprising%20PlugIR%20can%20be%20flexibly%20applied%0Atogether%20or%20separately%20in%20various%20situations.%20Our%20codes%20are%20available%20at%0Ahttps%3A//github.com/Saehyung-Lee/PlugIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Text-to-Image%2520Retrieval%2520with%2520Large%2520Language%2520Models%253A%2520A%250A%2520%2520Plug-and-Play%2520Approach%26entry.906535625%3DSaehyung%2520Lee%2520and%2520Sangwon%2520Yu%2520and%2520Junsung%2520Park%2520and%2520Jihun%2520Yi%2520and%2520Sungroh%2520Yoon%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520primarily%2520address%2520the%2520issue%2520of%2520dialogue-form%2520context%2520query%250Awithin%2520the%2520interactive%2520text-to-image%2520retrieval%2520task.%2520Our%2520methodology%252C%2520PlugIR%252C%250Aactively%2520utilizes%2520the%2520general%2520instruction-following%2520capability%2520of%2520LLMs%2520in%2520two%250Aways.%2520First%252C%2520by%2520reformulating%2520the%2520dialogue-form%2520context%252C%2520we%2520eliminate%2520the%250Anecessity%2520of%2520fine-tuning%2520a%2520retrieval%2520model%2520on%2520existing%2520visual%2520dialogue%2520data%252C%250Athereby%2520enabling%2520the%2520use%2520of%2520any%2520arbitrary%2520black-box%2520model.%2520Second%252C%2520we%2520construct%250Athe%2520LLM%2520questioner%2520to%2520generate%2520non-redundant%2520questions%2520about%2520the%2520attributes%2520of%250Athe%2520target%2520image%252C%2520based%2520on%2520the%2520information%2520of%2520retrieval%2520candidate%2520images%2520in%2520the%250Acurrent%2520context.%2520This%2520approach%2520mitigates%2520the%2520issues%2520of%2520noisiness%2520and%2520redundancy%250Ain%2520the%2520generated%2520questions.%2520Beyond%2520our%2520methodology%252C%2520we%2520propose%2520a%2520novel%250Aevaluation%2520metric%252C%2520Best%2520log%2520Rank%2520Integral%2520%2528BRI%2529%252C%2520for%2520a%2520comprehensive%2520assessment%250Aof%2520the%2520interactive%2520retrieval%2520system.%2520PlugIR%2520demonstrates%2520superior%2520performance%250Acompared%2520to%2520both%2520zero-shot%2520and%2520fine-tuned%2520baselines%2520in%2520various%2520benchmarks.%250AAdditionally%252C%2520the%2520two%2520methodologies%2520comprising%2520PlugIR%2520can%2520be%2520flexibly%2520applied%250Atogether%2520or%2520separately%2520in%2520various%2520situations.%2520Our%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/Saehyung-Lee/PlugIR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Text-to-Image%20Retrieval%20with%20Large%20Language%20Models%3A%20A%0A%20%20Plug-and-Play%20Approach&entry.906535625=Saehyung%20Lee%20and%20Sangwon%20Yu%20and%20Junsung%20Park%20and%20Jihun%20Yi%20and%20Sungroh%20Yoon&entry.1292438233=%20%20In%20this%20paper%2C%20we%20primarily%20address%20the%20issue%20of%20dialogue-form%20context%20query%0Awithin%20the%20interactive%20text-to-image%20retrieval%20task.%20Our%20methodology%2C%20PlugIR%2C%0Aactively%20utilizes%20the%20general%20instruction-following%20capability%20of%20LLMs%20in%20two%0Aways.%20First%2C%20by%20reformulating%20the%20dialogue-form%20context%2C%20we%20eliminate%20the%0Anecessity%20of%20fine-tuning%20a%20retrieval%20model%20on%20existing%20visual%20dialogue%20data%2C%0Athereby%20enabling%20the%20use%20of%20any%20arbitrary%20black-box%20model.%20Second%2C%20we%20construct%0Athe%20LLM%20questioner%20to%20generate%20non-redundant%20questions%20about%20the%20attributes%20of%0Athe%20target%20image%2C%20based%20on%20the%20information%20of%20retrieval%20candidate%20images%20in%20the%0Acurrent%20context.%20This%20approach%20mitigates%20the%20issues%20of%20noisiness%20and%20redundancy%0Ain%20the%20generated%20questions.%20Beyond%20our%20methodology%2C%20we%20propose%20a%20novel%0Aevaluation%20metric%2C%20Best%20log%20Rank%20Integral%20%28BRI%29%2C%20for%20a%20comprehensive%20assessment%0Aof%20the%20interactive%20retrieval%20system.%20PlugIR%20demonstrates%20superior%20performance%0Acompared%20to%20both%20zero-shot%20and%20fine-tuned%20baselines%20in%20various%20benchmarks.%0AAdditionally%2C%20the%20two%20methodologies%20comprising%20PlugIR%20can%20be%20flexibly%20applied%0Atogether%20or%20separately%20in%20various%20situations.%20Our%20codes%20are%20available%20at%0Ahttps%3A//github.com/Saehyung-Lee/PlugIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03411v1&entry.124074799=Read"},
{"title": "Learning Visual Prompts for Guiding the Attention of Vision Transformers", "author": "Razieh Rezaei and Masoud Jalili Sabet and Jindong Gu and Daniel Rueckert and Philip Torr and Ashkan Khakzar", "abstract": "  Visual prompting infuses visual information into the input image to adapt\nmodels toward specific predictions and tasks. Recently, manually crafted\nmarkers such as red circles are shown to guide the model to attend to a target\nregion on the image. However, these markers only work on models trained with\ndata containing those markers. Moreover, finding these prompts requires\nguesswork or prior knowledge of the domain on which the model is trained. This\nwork circumvents manual design constraints by proposing to learn the visual\nprompts for guiding the attention of vision transformers. The learned visual\nprompt, added to any input image would redirect the attention of the\npre-trained vision transformer to its spatial location on the image.\nSpecifically, the prompt is learned in a self-supervised manner without\nrequiring annotations and without fine-tuning the vision transformer. Our\nexperiments demonstrate the effectiveness of the proposed optimization-based\nvisual prompting strategy across various pre-trained vision encoders.\n", "link": "http://arxiv.org/abs/2406.03303v1", "date": "2024-06-05", "relevancy": 2.1144, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5339}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5326}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Visual%20Prompts%20for%20Guiding%20the%20Attention%20of%20Vision%20Transformers&body=Title%3A%20Learning%20Visual%20Prompts%20for%20Guiding%20the%20Attention%20of%20Vision%20Transformers%0AAuthor%3A%20Razieh%20Rezaei%20and%20Masoud%20Jalili%20Sabet%20and%20Jindong%20Gu%20and%20Daniel%20Rueckert%20and%20Philip%20Torr%20and%20Ashkan%20Khakzar%0AAbstract%3A%20%20%20Visual%20prompting%20infuses%20visual%20information%20into%20the%20input%20image%20to%20adapt%0Amodels%20toward%20specific%20predictions%20and%20tasks.%20Recently%2C%20manually%20crafted%0Amarkers%20such%20as%20red%20circles%20are%20shown%20to%20guide%20the%20model%20to%20attend%20to%20a%20target%0Aregion%20on%20the%20image.%20However%2C%20these%20markers%20only%20work%20on%20models%20trained%20with%0Adata%20containing%20those%20markers.%20Moreover%2C%20finding%20these%20prompts%20requires%0Aguesswork%20or%20prior%20knowledge%20of%20the%20domain%20on%20which%20the%20model%20is%20trained.%20This%0Awork%20circumvents%20manual%20design%20constraints%20by%20proposing%20to%20learn%20the%20visual%0Aprompts%20for%20guiding%20the%20attention%20of%20vision%20transformers.%20The%20learned%20visual%0Aprompt%2C%20added%20to%20any%20input%20image%20would%20redirect%20the%20attention%20of%20the%0Apre-trained%20vision%20transformer%20to%20its%20spatial%20location%20on%20the%20image.%0ASpecifically%2C%20the%20prompt%20is%20learned%20in%20a%20self-supervised%20manner%20without%0Arequiring%20annotations%20and%20without%20fine-tuning%20the%20vision%20transformer.%20Our%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20optimization-based%0Avisual%20prompting%20strategy%20across%20various%20pre-trained%20vision%20encoders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03303v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Visual%2520Prompts%2520for%2520Guiding%2520the%2520Attention%2520of%2520Vision%2520Transformers%26entry.906535625%3DRazieh%2520Rezaei%2520and%2520Masoud%2520Jalili%2520Sabet%2520and%2520Jindong%2520Gu%2520and%2520Daniel%2520Rueckert%2520and%2520Philip%2520Torr%2520and%2520Ashkan%2520Khakzar%26entry.1292438233%3D%2520%2520Visual%2520prompting%2520infuses%2520visual%2520information%2520into%2520the%2520input%2520image%2520to%2520adapt%250Amodels%2520toward%2520specific%2520predictions%2520and%2520tasks.%2520Recently%252C%2520manually%2520crafted%250Amarkers%2520such%2520as%2520red%2520circles%2520are%2520shown%2520to%2520guide%2520the%2520model%2520to%2520attend%2520to%2520a%2520target%250Aregion%2520on%2520the%2520image.%2520However%252C%2520these%2520markers%2520only%2520work%2520on%2520models%2520trained%2520with%250Adata%2520containing%2520those%2520markers.%2520Moreover%252C%2520finding%2520these%2520prompts%2520requires%250Aguesswork%2520or%2520prior%2520knowledge%2520of%2520the%2520domain%2520on%2520which%2520the%2520model%2520is%2520trained.%2520This%250Awork%2520circumvents%2520manual%2520design%2520constraints%2520by%2520proposing%2520to%2520learn%2520the%2520visual%250Aprompts%2520for%2520guiding%2520the%2520attention%2520of%2520vision%2520transformers.%2520The%2520learned%2520visual%250Aprompt%252C%2520added%2520to%2520any%2520input%2520image%2520would%2520redirect%2520the%2520attention%2520of%2520the%250Apre-trained%2520vision%2520transformer%2520to%2520its%2520spatial%2520location%2520on%2520the%2520image.%250ASpecifically%252C%2520the%2520prompt%2520is%2520learned%2520in%2520a%2520self-supervised%2520manner%2520without%250Arequiring%2520annotations%2520and%2520without%2520fine-tuning%2520the%2520vision%2520transformer.%2520Our%250Aexperiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520optimization-based%250Avisual%2520prompting%2520strategy%2520across%2520various%2520pre-trained%2520vision%2520encoders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03303v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Visual%20Prompts%20for%20Guiding%20the%20Attention%20of%20Vision%20Transformers&entry.906535625=Razieh%20Rezaei%20and%20Masoud%20Jalili%20Sabet%20and%20Jindong%20Gu%20and%20Daniel%20Rueckert%20and%20Philip%20Torr%20and%20Ashkan%20Khakzar&entry.1292438233=%20%20Visual%20prompting%20infuses%20visual%20information%20into%20the%20input%20image%20to%20adapt%0Amodels%20toward%20specific%20predictions%20and%20tasks.%20Recently%2C%20manually%20crafted%0Amarkers%20such%20as%20red%20circles%20are%20shown%20to%20guide%20the%20model%20to%20attend%20to%20a%20target%0Aregion%20on%20the%20image.%20However%2C%20these%20markers%20only%20work%20on%20models%20trained%20with%0Adata%20containing%20those%20markers.%20Moreover%2C%20finding%20these%20prompts%20requires%0Aguesswork%20or%20prior%20knowledge%20of%20the%20domain%20on%20which%20the%20model%20is%20trained.%20This%0Awork%20circumvents%20manual%20design%20constraints%20by%20proposing%20to%20learn%20the%20visual%0Aprompts%20for%20guiding%20the%20attention%20of%20vision%20transformers.%20The%20learned%20visual%0Aprompt%2C%20added%20to%20any%20input%20image%20would%20redirect%20the%20attention%20of%20the%0Apre-trained%20vision%20transformer%20to%20its%20spatial%20location%20on%20the%20image.%0ASpecifically%2C%20the%20prompt%20is%20learned%20in%20a%20self-supervised%20manner%20without%0Arequiring%20annotations%20and%20without%20fine-tuning%20the%20vision%20transformer.%20Our%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20optimization-based%0Avisual%20prompting%20strategy%20across%20various%20pre-trained%20vision%20encoders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03303v1&entry.124074799=Read"},
{"title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference", "author": "Han Zhao and Min Zhang and Wei Zhao and Pengxiang Ding and Siteng Huang and Donglin Wang", "abstract": "  In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due\nto Cobra's linear sequential modeling. (2) Interestingly, the results of\nclosed-set challenging prediction benchmarks show that Cobra performs well in\novercoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.\n", "link": "http://arxiv.org/abs/2403.14520v3", "date": "2024-06-05", "relevancy": 2.1088, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference&body=Title%3A%20Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference%0AAuthor%3A%20Han%20Zhao%20and%20Min%20Zhang%20and%20Wei%20Zhao%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20application%20of%20multimodal%20large%20language%20models%20%28MLLM%29%0Ain%20various%20fields%20has%20achieved%20remarkable%20success.%20However%2C%20as%20the%20foundation%0Amodel%20for%20many%20downstream%20tasks%2C%20current%20MLLMs%20are%20composed%20of%20the%20well-known%0ATransformer%20network%2C%20which%20has%20a%20less%20efficient%20quadratic%20computation%0Acomplexity.%20To%20improve%20the%20efficiency%20of%20such%20basic%20models%2C%20we%20propose%20Cobra%2C%20a%0Alinear%20computational%20complexity%20MLLM.%20Specifically%2C%20Cobra%20integrates%20the%0Aefficient%20Mamba%20language%20model%20into%20the%20visual%20modality.%20Moreover%2C%20we%20explore%0Aand%20study%20various%20modal%20fusion%20schemes%20to%20create%20an%20effective%20multi-modal%0AMamba.%20Extensive%20experiments%20demonstrate%20that%20%281%29%20Cobra%20achieves%20extremely%0Acompetitive%20performance%20with%20current%20computationally%20efficient%20state-of-the-art%0Amethods%2C%20e.g.%2C%20LLaVA-Phi%2C%20TinyLLaVA%2C%20and%20MobileVLM%20v2%2C%20and%20has%20faster%20speed%20due%0Ato%20Cobra%27s%20linear%20sequential%20modeling.%20%282%29%20Interestingly%2C%20the%20results%20of%0Aclosed-set%20challenging%20prediction%20benchmarks%20show%20that%20Cobra%20performs%20well%20in%0Aovercoming%20visual%20illusions%20and%20spatial%20relationship%20judgments.%20%283%29%20Notably%2C%0ACobra%20even%20achieves%20comparable%20performance%20to%20LLaVA%20with%20about%2043%25%20of%20the%0Anumber%20of%20parameters.%20We%20will%20make%20all%20codes%20of%20Cobra%20open-source%20and%20hope%20that%0Athe%20proposed%20method%20can%20facilitate%20future%20research%20on%20complexity%20problems%20in%0AMLLM.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//sites.google.com/view/cobravlm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14520v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCobra%253A%2520Extending%2520Mamba%2520to%2520Multi-Modal%2520Large%2520Language%2520Model%2520for%2520Efficient%250A%2520%2520Inference%26entry.906535625%3DHan%2520Zhao%2520and%2520Min%2520Zhang%2520and%2520Wei%2520Zhao%2520and%2520Pengxiang%2520Ding%2520and%2520Siteng%2520Huang%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520application%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLM%2529%250Ain%2520various%2520fields%2520has%2520achieved%2520remarkable%2520success.%2520However%252C%2520as%2520the%2520foundation%250Amodel%2520for%2520many%2520downstream%2520tasks%252C%2520current%2520MLLMs%2520are%2520composed%2520of%2520the%2520well-known%250ATransformer%2520network%252C%2520which%2520has%2520a%2520less%2520efficient%2520quadratic%2520computation%250Acomplexity.%2520To%2520improve%2520the%2520efficiency%2520of%2520such%2520basic%2520models%252C%2520we%2520propose%2520Cobra%252C%2520a%250Alinear%2520computational%2520complexity%2520MLLM.%2520Specifically%252C%2520Cobra%2520integrates%2520the%250Aefficient%2520Mamba%2520language%2520model%2520into%2520the%2520visual%2520modality.%2520Moreover%252C%2520we%2520explore%250Aand%2520study%2520various%2520modal%2520fusion%2520schemes%2520to%2520create%2520an%2520effective%2520multi-modal%250AMamba.%2520Extensive%2520experiments%2520demonstrate%2520that%2520%25281%2529%2520Cobra%2520achieves%2520extremely%250Acompetitive%2520performance%2520with%2520current%2520computationally%2520efficient%2520state-of-the-art%250Amethods%252C%2520e.g.%252C%2520LLaVA-Phi%252C%2520TinyLLaVA%252C%2520and%2520MobileVLM%2520v2%252C%2520and%2520has%2520faster%2520speed%2520due%250Ato%2520Cobra%2527s%2520linear%2520sequential%2520modeling.%2520%25282%2529%2520Interestingly%252C%2520the%2520results%2520of%250Aclosed-set%2520challenging%2520prediction%2520benchmarks%2520show%2520that%2520Cobra%2520performs%2520well%2520in%250Aovercoming%2520visual%2520illusions%2520and%2520spatial%2520relationship%2520judgments.%2520%25283%2529%2520Notably%252C%250ACobra%2520even%2520achieves%2520comparable%2520performance%2520to%2520LLaVA%2520with%2520about%252043%2525%2520of%2520the%250Anumber%2520of%2520parameters.%2520We%2520will%2520make%2520all%2520codes%2520of%2520Cobra%2520open-source%2520and%2520hope%2520that%250Athe%2520proposed%2520method%2520can%2520facilitate%2520future%2520research%2520on%2520complexity%2520problems%2520in%250AMLLM.%2520Our%2520project%2520page%2520is%2520available%2520at%253A%2520https%253A//sites.google.com/view/cobravlm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14520v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference&entry.906535625=Han%20Zhao%20and%20Min%20Zhang%20and%20Wei%20Zhao%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Donglin%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20application%20of%20multimodal%20large%20language%20models%20%28MLLM%29%0Ain%20various%20fields%20has%20achieved%20remarkable%20success.%20However%2C%20as%20the%20foundation%0Amodel%20for%20many%20downstream%20tasks%2C%20current%20MLLMs%20are%20composed%20of%20the%20well-known%0ATransformer%20network%2C%20which%20has%20a%20less%20efficient%20quadratic%20computation%0Acomplexity.%20To%20improve%20the%20efficiency%20of%20such%20basic%20models%2C%20we%20propose%20Cobra%2C%20a%0Alinear%20computational%20complexity%20MLLM.%20Specifically%2C%20Cobra%20integrates%20the%0Aefficient%20Mamba%20language%20model%20into%20the%20visual%20modality.%20Moreover%2C%20we%20explore%0Aand%20study%20various%20modal%20fusion%20schemes%20to%20create%20an%20effective%20multi-modal%0AMamba.%20Extensive%20experiments%20demonstrate%20that%20%281%29%20Cobra%20achieves%20extremely%0Acompetitive%20performance%20with%20current%20computationally%20efficient%20state-of-the-art%0Amethods%2C%20e.g.%2C%20LLaVA-Phi%2C%20TinyLLaVA%2C%20and%20MobileVLM%20v2%2C%20and%20has%20faster%20speed%20due%0Ato%20Cobra%27s%20linear%20sequential%20modeling.%20%282%29%20Interestingly%2C%20the%20results%20of%0Aclosed-set%20challenging%20prediction%20benchmarks%20show%20that%20Cobra%20performs%20well%20in%0Aovercoming%20visual%20illusions%20and%20spatial%20relationship%20judgments.%20%283%29%20Notably%2C%0ACobra%20even%20achieves%20comparable%20performance%20to%20LLaVA%20with%20about%2043%25%20of%20the%0Anumber%20of%20parameters.%20We%20will%20make%20all%20codes%20of%20Cobra%20open-source%20and%20hope%20that%0Athe%20proposed%20method%20can%20facilitate%20future%20research%20on%20complexity%20problems%20in%0AMLLM.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//sites.google.com/view/cobravlm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14520v3&entry.124074799=Read"},
{"title": "Recurrent Distance Filtering for Graph Representation Learning", "author": "Yuhui Ding and Antonio Orvieto and Bobby He and Thomas Hofmann", "abstract": "  Graph neural networks based on iterative one-hop message passing have been\nshown to struggle in harnessing the information from distant nodes effectively.\nConversely, graph transformers allow each node to attend to all other nodes\ndirectly, but lack graph inductive bias and have to rely on ad-hoc positional\nencoding. In this paper, we propose a new architecture to reconcile these\nchallenges. Our approach stems from the recent breakthroughs in long-range\nmodeling provided by deep state-space models: for a given target node, our\nmodel aggregates other nodes by their shortest distances to the target and uses\na linear RNN to encode the sequence of hop representations. The linear RNN is\nparameterized in a particular diagonal form for stable long-range signal\npropagation and is theoretically expressive enough to encode the neighborhood\nhierarchy. With no need for positional encoding, we empirically show that the\nperformance of our model is comparable to or better than that of\nstate-of-the-art graph transformers on various benchmarks, with a significantly\nreduced computational cost. Our code is open-source at\nhttps://github.com/skeletondyh/GRED.\n", "link": "http://arxiv.org/abs/2312.01538v3", "date": "2024-06-05", "relevancy": 2.1054, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5702}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5376}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Distance%20Filtering%20for%20Graph%20Representation%20Learning&body=Title%3A%20Recurrent%20Distance%20Filtering%20for%20Graph%20Representation%20Learning%0AAuthor%3A%20Yuhui%20Ding%20and%20Antonio%20Orvieto%20and%20Bobby%20He%20and%20Thomas%20Hofmann%0AAbstract%3A%20%20%20Graph%20neural%20networks%20based%20on%20iterative%20one-hop%20message%20passing%20have%20been%0Ashown%20to%20struggle%20in%20harnessing%20the%20information%20from%20distant%20nodes%20effectively.%0AConversely%2C%20graph%20transformers%20allow%20each%20node%20to%20attend%20to%20all%20other%20nodes%0Adirectly%2C%20but%20lack%20graph%20inductive%20bias%20and%20have%20to%20rely%20on%20ad-hoc%20positional%0Aencoding.%20In%20this%20paper%2C%20we%20propose%20a%20new%20architecture%20to%20reconcile%20these%0Achallenges.%20Our%20approach%20stems%20from%20the%20recent%20breakthroughs%20in%20long-range%0Amodeling%20provided%20by%20deep%20state-space%20models%3A%20for%20a%20given%20target%20node%2C%20our%0Amodel%20aggregates%20other%20nodes%20by%20their%20shortest%20distances%20to%20the%20target%20and%20uses%0Aa%20linear%20RNN%20to%20encode%20the%20sequence%20of%20hop%20representations.%20The%20linear%20RNN%20is%0Aparameterized%20in%20a%20particular%20diagonal%20form%20for%20stable%20long-range%20signal%0Apropagation%20and%20is%20theoretically%20expressive%20enough%20to%20encode%20the%20neighborhood%0Ahierarchy.%20With%20no%20need%20for%20positional%20encoding%2C%20we%20empirically%20show%20that%20the%0Aperformance%20of%20our%20model%20is%20comparable%20to%20or%20better%20than%20that%20of%0Astate-of-the-art%20graph%20transformers%20on%20various%20benchmarks%2C%20with%20a%20significantly%0Areduced%20computational%20cost.%20Our%20code%20is%20open-source%20at%0Ahttps%3A//github.com/skeletondyh/GRED.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01538v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Distance%2520Filtering%2520for%2520Graph%2520Representation%2520Learning%26entry.906535625%3DYuhui%2520Ding%2520and%2520Antonio%2520Orvieto%2520and%2520Bobby%2520He%2520and%2520Thomas%2520Hofmann%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520based%2520on%2520iterative%2520one-hop%2520message%2520passing%2520have%2520been%250Ashown%2520to%2520struggle%2520in%2520harnessing%2520the%2520information%2520from%2520distant%2520nodes%2520effectively.%250AConversely%252C%2520graph%2520transformers%2520allow%2520each%2520node%2520to%2520attend%2520to%2520all%2520other%2520nodes%250Adirectly%252C%2520but%2520lack%2520graph%2520inductive%2520bias%2520and%2520have%2520to%2520rely%2520on%2520ad-hoc%2520positional%250Aencoding.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520architecture%2520to%2520reconcile%2520these%250Achallenges.%2520Our%2520approach%2520stems%2520from%2520the%2520recent%2520breakthroughs%2520in%2520long-range%250Amodeling%2520provided%2520by%2520deep%2520state-space%2520models%253A%2520for%2520a%2520given%2520target%2520node%252C%2520our%250Amodel%2520aggregates%2520other%2520nodes%2520by%2520their%2520shortest%2520distances%2520to%2520the%2520target%2520and%2520uses%250Aa%2520linear%2520RNN%2520to%2520encode%2520the%2520sequence%2520of%2520hop%2520representations.%2520The%2520linear%2520RNN%2520is%250Aparameterized%2520in%2520a%2520particular%2520diagonal%2520form%2520for%2520stable%2520long-range%2520signal%250Apropagation%2520and%2520is%2520theoretically%2520expressive%2520enough%2520to%2520encode%2520the%2520neighborhood%250Ahierarchy.%2520With%2520no%2520need%2520for%2520positional%2520encoding%252C%2520we%2520empirically%2520show%2520that%2520the%250Aperformance%2520of%2520our%2520model%2520is%2520comparable%2520to%2520or%2520better%2520than%2520that%2520of%250Astate-of-the-art%2520graph%2520transformers%2520on%2520various%2520benchmarks%252C%2520with%2520a%2520significantly%250Areduced%2520computational%2520cost.%2520Our%2520code%2520is%2520open-source%2520at%250Ahttps%253A//github.com/skeletondyh/GRED.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01538v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Distance%20Filtering%20for%20Graph%20Representation%20Learning&entry.906535625=Yuhui%20Ding%20and%20Antonio%20Orvieto%20and%20Bobby%20He%20and%20Thomas%20Hofmann&entry.1292438233=%20%20Graph%20neural%20networks%20based%20on%20iterative%20one-hop%20message%20passing%20have%20been%0Ashown%20to%20struggle%20in%20harnessing%20the%20information%20from%20distant%20nodes%20effectively.%0AConversely%2C%20graph%20transformers%20allow%20each%20node%20to%20attend%20to%20all%20other%20nodes%0Adirectly%2C%20but%20lack%20graph%20inductive%20bias%20and%20have%20to%20rely%20on%20ad-hoc%20positional%0Aencoding.%20In%20this%20paper%2C%20we%20propose%20a%20new%20architecture%20to%20reconcile%20these%0Achallenges.%20Our%20approach%20stems%20from%20the%20recent%20breakthroughs%20in%20long-range%0Amodeling%20provided%20by%20deep%20state-space%20models%3A%20for%20a%20given%20target%20node%2C%20our%0Amodel%20aggregates%20other%20nodes%20by%20their%20shortest%20distances%20to%20the%20target%20and%20uses%0Aa%20linear%20RNN%20to%20encode%20the%20sequence%20of%20hop%20representations.%20The%20linear%20RNN%20is%0Aparameterized%20in%20a%20particular%20diagonal%20form%20for%20stable%20long-range%20signal%0Apropagation%20and%20is%20theoretically%20expressive%20enough%20to%20encode%20the%20neighborhood%0Ahierarchy.%20With%20no%20need%20for%20positional%20encoding%2C%20we%20empirically%20show%20that%20the%0Aperformance%20of%20our%20model%20is%20comparable%20to%20or%20better%20than%20that%20of%0Astate-of-the-art%20graph%20transformers%20on%20various%20benchmarks%2C%20with%20a%20significantly%0Areduced%20computational%20cost.%20Our%20code%20is%20open-source%20at%0Ahttps%3A//github.com/skeletondyh/GRED.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01538v3&entry.124074799=Read"},
{"title": "Generative Diffusion Models for Fast Simulations of Particle Collisions\n  at CERN", "author": "Miko\u0142aj Kita and Jan Dubi\u0144ski and Przemys\u0142aw Rokita and Kamil Deja", "abstract": "  In High Energy Physics simulations play a crucial role in unraveling the\ncomplexities of particle collision experiments within CERN's Large Hadron\nCollider. Machine learning simulation methods have garnered attention as\npromising alternatives to traditional approaches. While existing methods mainly\nemploy Variational Autoencoders (VAEs) or Generative Adversarial Networks\n(GANs), recent advancements highlight the efficacy of diffusion models as\nstate-of-the-art generative machine learning methods. We present the first\nsimulation for Zero Degree Calorimeter (ZDC) at the ALICE experiment based on\ndiffusion models, achieving the highest fidelity compared to existing\nbaselines. We perform an analysis of trade-offs between generation times and\nthe simulation quality. The results indicate a significant potential of latent\ndiffusion model due to its rapid generation time.\n", "link": "http://arxiv.org/abs/2406.03233v1", "date": "2024-06-05", "relevancy": 2.1034, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5937}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5123}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Diffusion%20Models%20for%20Fast%20Simulations%20of%20Particle%20Collisions%0A%20%20at%20CERN&body=Title%3A%20Generative%20Diffusion%20Models%20for%20Fast%20Simulations%20of%20Particle%20Collisions%0A%20%20at%20CERN%0AAuthor%3A%20Miko%C5%82aj%20Kita%20and%20Jan%20Dubi%C5%84ski%20and%20Przemys%C5%82aw%20Rokita%20and%20Kamil%20Deja%0AAbstract%3A%20%20%20In%20High%20Energy%20Physics%20simulations%20play%20a%20crucial%20role%20in%20unraveling%20the%0Acomplexities%20of%20particle%20collision%20experiments%20within%20CERN%27s%20Large%20Hadron%0ACollider.%20Machine%20learning%20simulation%20methods%20have%20garnered%20attention%20as%0Apromising%20alternatives%20to%20traditional%20approaches.%20While%20existing%20methods%20mainly%0Aemploy%20Variational%20Autoencoders%20%28VAEs%29%20or%20Generative%20Adversarial%20Networks%0A%28GANs%29%2C%20recent%20advancements%20highlight%20the%20efficacy%20of%20diffusion%20models%20as%0Astate-of-the-art%20generative%20machine%20learning%20methods.%20We%20present%20the%20first%0Asimulation%20for%20Zero%20Degree%20Calorimeter%20%28ZDC%29%20at%20the%20ALICE%20experiment%20based%20on%0Adiffusion%20models%2C%20achieving%20the%20highest%20fidelity%20compared%20to%20existing%0Abaselines.%20We%20perform%20an%20analysis%20of%20trade-offs%20between%20generation%20times%20and%0Athe%20simulation%20quality.%20The%20results%20indicate%20a%20significant%20potential%20of%20latent%0Adiffusion%20model%20due%20to%20its%20rapid%20generation%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Diffusion%2520Models%2520for%2520Fast%2520Simulations%2520of%2520Particle%2520Collisions%250A%2520%2520at%2520CERN%26entry.906535625%3DMiko%25C5%2582aj%2520Kita%2520and%2520Jan%2520Dubi%25C5%2584ski%2520and%2520Przemys%25C5%2582aw%2520Rokita%2520and%2520Kamil%2520Deja%26entry.1292438233%3D%2520%2520In%2520High%2520Energy%2520Physics%2520simulations%2520play%2520a%2520crucial%2520role%2520in%2520unraveling%2520the%250Acomplexities%2520of%2520particle%2520collision%2520experiments%2520within%2520CERN%2527s%2520Large%2520Hadron%250ACollider.%2520Machine%2520learning%2520simulation%2520methods%2520have%2520garnered%2520attention%2520as%250Apromising%2520alternatives%2520to%2520traditional%2520approaches.%2520While%2520existing%2520methods%2520mainly%250Aemploy%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520or%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529%252C%2520recent%2520advancements%2520highlight%2520the%2520efficacy%2520of%2520diffusion%2520models%2520as%250Astate-of-the-art%2520generative%2520machine%2520learning%2520methods.%2520We%2520present%2520the%2520first%250Asimulation%2520for%2520Zero%2520Degree%2520Calorimeter%2520%2528ZDC%2529%2520at%2520the%2520ALICE%2520experiment%2520based%2520on%250Adiffusion%2520models%252C%2520achieving%2520the%2520highest%2520fidelity%2520compared%2520to%2520existing%250Abaselines.%2520We%2520perform%2520an%2520analysis%2520of%2520trade-offs%2520between%2520generation%2520times%2520and%250Athe%2520simulation%2520quality.%2520The%2520results%2520indicate%2520a%2520significant%2520potential%2520of%2520latent%250Adiffusion%2520model%2520due%2520to%2520its%2520rapid%2520generation%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Diffusion%20Models%20for%20Fast%20Simulations%20of%20Particle%20Collisions%0A%20%20at%20CERN&entry.906535625=Miko%C5%82aj%20Kita%20and%20Jan%20Dubi%C5%84ski%20and%20Przemys%C5%82aw%20Rokita%20and%20Kamil%20Deja&entry.1292438233=%20%20In%20High%20Energy%20Physics%20simulations%20play%20a%20crucial%20role%20in%20unraveling%20the%0Acomplexities%20of%20particle%20collision%20experiments%20within%20CERN%27s%20Large%20Hadron%0ACollider.%20Machine%20learning%20simulation%20methods%20have%20garnered%20attention%20as%0Apromising%20alternatives%20to%20traditional%20approaches.%20While%20existing%20methods%20mainly%0Aemploy%20Variational%20Autoencoders%20%28VAEs%29%20or%20Generative%20Adversarial%20Networks%0A%28GANs%29%2C%20recent%20advancements%20highlight%20the%20efficacy%20of%20diffusion%20models%20as%0Astate-of-the-art%20generative%20machine%20learning%20methods.%20We%20present%20the%20first%0Asimulation%20for%20Zero%20Degree%20Calorimeter%20%28ZDC%29%20at%20the%20ALICE%20experiment%20based%20on%0Adiffusion%20models%2C%20achieving%20the%20highest%20fidelity%20compared%20to%20existing%0Abaselines.%20We%20perform%20an%20analysis%20of%20trade-offs%20between%20generation%20times%20and%0Athe%20simulation%20quality.%20The%20results%20indicate%20a%20significant%20potential%20of%20latent%0Adiffusion%20model%20due%20to%20its%20rapid%20generation%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03233v1&entry.124074799=Read"},
{"title": "Reproducibility study of FairAC", "author": "Gijs de Jong and Macha J. Meijer and Derck W. E. Prinzhorn and Harold Ruiter", "abstract": "  This work aims to reproduce the findings of the paper \"Fair Attribute\nCompletion on Graph with Missing Attributes\" written by Guo, Chu, and Li\narXiv:2302.12977 by investigating the claims made in the paper. This paper\nsuggests that the results of the original paper are reproducible and thus, the\nclaims hold. However, the claim that FairAC is a generic framework for many\ndownstream tasks is very broad and could therefore only be partially tested.\nMoreover, we show that FairAC is generalizable to various datasets and\nsensitive attributes and show evidence that the improvement in group fairness\nof the FairAC framework does not come at the expense of individual fairness.\nLastly, the codebase of FairAC has been refactored and is now easily applicable\nfor various datasets and models.\n", "link": "http://arxiv.org/abs/2406.03314v1", "date": "2024-06-05", "relevancy": 2.0865, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4221}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4169}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reproducibility%20study%20of%20FairAC&body=Title%3A%20Reproducibility%20study%20of%20FairAC%0AAuthor%3A%20Gijs%20de%20Jong%20and%20Macha%20J.%20Meijer%20and%20Derck%20W.%20E.%20Prinzhorn%20and%20Harold%20Ruiter%0AAbstract%3A%20%20%20This%20work%20aims%20to%20reproduce%20the%20findings%20of%20the%20paper%20%22Fair%20Attribute%0ACompletion%20on%20Graph%20with%20Missing%20Attributes%22%20written%20by%20Guo%2C%20Chu%2C%20and%20Li%0AarXiv%3A2302.12977%20by%20investigating%20the%20claims%20made%20in%20the%20paper.%20This%20paper%0Asuggests%20that%20the%20results%20of%20the%20original%20paper%20are%20reproducible%20and%20thus%2C%20the%0Aclaims%20hold.%20However%2C%20the%20claim%20that%20FairAC%20is%20a%20generic%20framework%20for%20many%0Adownstream%20tasks%20is%20very%20broad%20and%20could%20therefore%20only%20be%20partially%20tested.%0AMoreover%2C%20we%20show%20that%20FairAC%20is%20generalizable%20to%20various%20datasets%20and%0Asensitive%20attributes%20and%20show%20evidence%20that%20the%20improvement%20in%20group%20fairness%0Aof%20the%20FairAC%20framework%20does%20not%20come%20at%20the%20expense%20of%20individual%20fairness.%0ALastly%2C%20the%20codebase%20of%20FairAC%20has%20been%20refactored%20and%20is%20now%20easily%20applicable%0Afor%20various%20datasets%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReproducibility%2520study%2520of%2520FairAC%26entry.906535625%3DGijs%2520de%2520Jong%2520and%2520Macha%2520J.%2520Meijer%2520and%2520Derck%2520W.%2520E.%2520Prinzhorn%2520and%2520Harold%2520Ruiter%26entry.1292438233%3D%2520%2520This%2520work%2520aims%2520to%2520reproduce%2520the%2520findings%2520of%2520the%2520paper%2520%2522Fair%2520Attribute%250ACompletion%2520on%2520Graph%2520with%2520Missing%2520Attributes%2522%2520written%2520by%2520Guo%252C%2520Chu%252C%2520and%2520Li%250AarXiv%253A2302.12977%2520by%2520investigating%2520the%2520claims%2520made%2520in%2520the%2520paper.%2520This%2520paper%250Asuggests%2520that%2520the%2520results%2520of%2520the%2520original%2520paper%2520are%2520reproducible%2520and%2520thus%252C%2520the%250Aclaims%2520hold.%2520However%252C%2520the%2520claim%2520that%2520FairAC%2520is%2520a%2520generic%2520framework%2520for%2520many%250Adownstream%2520tasks%2520is%2520very%2520broad%2520and%2520could%2520therefore%2520only%2520be%2520partially%2520tested.%250AMoreover%252C%2520we%2520show%2520that%2520FairAC%2520is%2520generalizable%2520to%2520various%2520datasets%2520and%250Asensitive%2520attributes%2520and%2520show%2520evidence%2520that%2520the%2520improvement%2520in%2520group%2520fairness%250Aof%2520the%2520FairAC%2520framework%2520does%2520not%2520come%2520at%2520the%2520expense%2520of%2520individual%2520fairness.%250ALastly%252C%2520the%2520codebase%2520of%2520FairAC%2520has%2520been%2520refactored%2520and%2520is%2520now%2520easily%2520applicable%250Afor%2520various%2520datasets%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reproducibility%20study%20of%20FairAC&entry.906535625=Gijs%20de%20Jong%20and%20Macha%20J.%20Meijer%20and%20Derck%20W.%20E.%20Prinzhorn%20and%20Harold%20Ruiter&entry.1292438233=%20%20This%20work%20aims%20to%20reproduce%20the%20findings%20of%20the%20paper%20%22Fair%20Attribute%0ACompletion%20on%20Graph%20with%20Missing%20Attributes%22%20written%20by%20Guo%2C%20Chu%2C%20and%20Li%0AarXiv%3A2302.12977%20by%20investigating%20the%20claims%20made%20in%20the%20paper.%20This%20paper%0Asuggests%20that%20the%20results%20of%20the%20original%20paper%20are%20reproducible%20and%20thus%2C%20the%0Aclaims%20hold.%20However%2C%20the%20claim%20that%20FairAC%20is%20a%20generic%20framework%20for%20many%0Adownstream%20tasks%20is%20very%20broad%20and%20could%20therefore%20only%20be%20partially%20tested.%0AMoreover%2C%20we%20show%20that%20FairAC%20is%20generalizable%20to%20various%20datasets%20and%0Asensitive%20attributes%20and%20show%20evidence%20that%20the%20improvement%20in%20group%20fairness%0Aof%20the%20FairAC%20framework%20does%20not%20come%20at%20the%20expense%20of%20individual%20fairness.%0ALastly%2C%20the%20codebase%20of%20FairAC%20has%20been%20refactored%20and%20is%20now%20easily%20applicable%0Afor%20various%20datasets%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03314v1&entry.124074799=Read"},
{"title": "Interactive Image Selection and Training for Brain Tumor Segmentation\n  Network", "author": "Matheus A. Cerqueira and Fl\u00e1via Sprenger and Bernardo C. A. Teixeira and Alexandre X. Falc\u00e3o", "abstract": "  Medical image segmentation is a relevant problem, with deep learning being an\nexponent. However, the necessity of a high volume of fully annotated images for\ntraining massive models can be a problem, especially for applications whose\nimages present a great diversity, such as brain tumors, which can occur in\ndifferent sizes and shapes. In contrast, a recent methodology, Feature Learning\nfrom Image Markers (FLIM), has involved an expert in the learning loop,\nproducing small networks that require few images to train the convolutional\nlayers. In this work, We employ an interactive method for image selection and\ntraining based on FLIM, exploring the user's knowledge. The results\ndemonstrated that with our methodology, we could choose a small set of images\nto train the encoder of a U-shaped network, obtaining performance equal to\nmanual selection and even surpassing the same U-shaped network trained with\nbackpropagation and all training images.\n", "link": "http://arxiv.org/abs/2406.03225v1", "date": "2024-06-05", "relevancy": 2.0762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.53}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5155}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Image%20Selection%20and%20Training%20for%20Brain%20Tumor%20Segmentation%0A%20%20Network&body=Title%3A%20Interactive%20Image%20Selection%20and%20Training%20for%20Brain%20Tumor%20Segmentation%0A%20%20Network%0AAuthor%3A%20Matheus%20A.%20Cerqueira%20and%20Fl%C3%A1via%20Sprenger%20and%20Bernardo%20C.%20A.%20Teixeira%20and%20Alexandre%20X.%20Falc%C3%A3o%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20a%20relevant%20problem%2C%20with%20deep%20learning%20being%20an%0Aexponent.%20However%2C%20the%20necessity%20of%20a%20high%20volume%20of%20fully%20annotated%20images%20for%0Atraining%20massive%20models%20can%20be%20a%20problem%2C%20especially%20for%20applications%20whose%0Aimages%20present%20a%20great%20diversity%2C%20such%20as%20brain%20tumors%2C%20which%20can%20occur%20in%0Adifferent%20sizes%20and%20shapes.%20In%20contrast%2C%20a%20recent%20methodology%2C%20Feature%20Learning%0Afrom%20Image%20Markers%20%28FLIM%29%2C%20has%20involved%20an%20expert%20in%20the%20learning%20loop%2C%0Aproducing%20small%20networks%20that%20require%20few%20images%20to%20train%20the%20convolutional%0Alayers.%20In%20this%20work%2C%20We%20employ%20an%20interactive%20method%20for%20image%20selection%20and%0Atraining%20based%20on%20FLIM%2C%20exploring%20the%20user%27s%20knowledge.%20The%20results%0Ademonstrated%20that%20with%20our%20methodology%2C%20we%20could%20choose%20a%20small%20set%20of%20images%0Ato%20train%20the%20encoder%20of%20a%20U-shaped%20network%2C%20obtaining%20performance%20equal%20to%0Amanual%20selection%20and%20even%20surpassing%20the%20same%20U-shaped%20network%20trained%20with%0Abackpropagation%20and%20all%20training%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Image%2520Selection%2520and%2520Training%2520for%2520Brain%2520Tumor%2520Segmentation%250A%2520%2520Network%26entry.906535625%3DMatheus%2520A.%2520Cerqueira%2520and%2520Fl%25C3%25A1via%2520Sprenger%2520and%2520Bernardo%2520C.%2520A.%2520Teixeira%2520and%2520Alexandre%2520X.%2520Falc%25C3%25A3o%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520a%2520relevant%2520problem%252C%2520with%2520deep%2520learning%2520being%2520an%250Aexponent.%2520However%252C%2520the%2520necessity%2520of%2520a%2520high%2520volume%2520of%2520fully%2520annotated%2520images%2520for%250Atraining%2520massive%2520models%2520can%2520be%2520a%2520problem%252C%2520especially%2520for%2520applications%2520whose%250Aimages%2520present%2520a%2520great%2520diversity%252C%2520such%2520as%2520brain%2520tumors%252C%2520which%2520can%2520occur%2520in%250Adifferent%2520sizes%2520and%2520shapes.%2520In%2520contrast%252C%2520a%2520recent%2520methodology%252C%2520Feature%2520Learning%250Afrom%2520Image%2520Markers%2520%2528FLIM%2529%252C%2520has%2520involved%2520an%2520expert%2520in%2520the%2520learning%2520loop%252C%250Aproducing%2520small%2520networks%2520that%2520require%2520few%2520images%2520to%2520train%2520the%2520convolutional%250Alayers.%2520In%2520this%2520work%252C%2520We%2520employ%2520an%2520interactive%2520method%2520for%2520image%2520selection%2520and%250Atraining%2520based%2520on%2520FLIM%252C%2520exploring%2520the%2520user%2527s%2520knowledge.%2520The%2520results%250Ademonstrated%2520that%2520with%2520our%2520methodology%252C%2520we%2520could%2520choose%2520a%2520small%2520set%2520of%2520images%250Ato%2520train%2520the%2520encoder%2520of%2520a%2520U-shaped%2520network%252C%2520obtaining%2520performance%2520equal%2520to%250Amanual%2520selection%2520and%2520even%2520surpassing%2520the%2520same%2520U-shaped%2520network%2520trained%2520with%250Abackpropagation%2520and%2520all%2520training%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Image%20Selection%20and%20Training%20for%20Brain%20Tumor%20Segmentation%0A%20%20Network&entry.906535625=Matheus%20A.%20Cerqueira%20and%20Fl%C3%A1via%20Sprenger%20and%20Bernardo%20C.%20A.%20Teixeira%20and%20Alexandre%20X.%20Falc%C3%A3o&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20a%20relevant%20problem%2C%20with%20deep%20learning%20being%20an%0Aexponent.%20However%2C%20the%20necessity%20of%20a%20high%20volume%20of%20fully%20annotated%20images%20for%0Atraining%20massive%20models%20can%20be%20a%20problem%2C%20especially%20for%20applications%20whose%0Aimages%20present%20a%20great%20diversity%2C%20such%20as%20brain%20tumors%2C%20which%20can%20occur%20in%0Adifferent%20sizes%20and%20shapes.%20In%20contrast%2C%20a%20recent%20methodology%2C%20Feature%20Learning%0Afrom%20Image%20Markers%20%28FLIM%29%2C%20has%20involved%20an%20expert%20in%20the%20learning%20loop%2C%0Aproducing%20small%20networks%20that%20require%20few%20images%20to%20train%20the%20convolutional%0Alayers.%20In%20this%20work%2C%20We%20employ%20an%20interactive%20method%20for%20image%20selection%20and%0Atraining%20based%20on%20FLIM%2C%20exploring%20the%20user%27s%20knowledge.%20The%20results%0Ademonstrated%20that%20with%20our%20methodology%2C%20we%20could%20choose%20a%20small%20set%20of%20images%0Ato%20train%20the%20encoder%20of%20a%20U-shaped%20network%2C%20obtaining%20performance%20equal%20to%0Amanual%20selection%20and%20even%20surpassing%20the%20same%20U-shaped%20network%20trained%20with%0Abackpropagation%20and%20all%20training%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03225v1&entry.124074799=Read"},
{"title": "Solution Concepts in Hierarchical Games under Bounded Rationality with\n  Applications to Autonomous Driving", "author": "Atrisha Sarkar and Krzysztof Czarnecki", "abstract": "  With autonomous vehicles (AV) set to integrate further into regular human\ntraffic, there is an increasing consensus on treating AV motion planning as a\nmulti-agent problem. However, the traditional game-theoretic assumption of\ncomplete rationality is too strong for human driving, and there is a need for\nunderstanding human driving as a \\emph{bounded rational} activity through a\nbehavioural game-theoretic lens. To that end, we adapt four metamodels of\nbounded rational behaviour: three based on Quantal level-k and one based on\nNash equilibrium with quantal errors. We formalize the different solution\nconcepts that can be applied in the context of hierarchical games, a framework\nused in multi-agent motion planning, for the purpose of creating game theoretic\nmodels of driving behaviour. Furthermore, based on a contributed dataset of\nhuman driving at a busy urban intersection with a total of approximately 4k\nagents and 44k decision points, we evaluate the behaviour models on the basis\nof model fit to naturalistic data, as well as their predictive capacity. Our\nresults suggest that among the behaviour models evaluated, at the level of\nmaneuvers, modeling driving behaviour as an adaptation of the Quantal level-k\nmodel with level-0 behaviour modelled as pure rule-following provides the best\nfit to naturalistic driving behaviour. At the level of trajectories, bounds\nsampling of actions and a maxmax non-strategic models is the most accurate\nwithin the set of models in comparison. We also find a significant impact of\nsituational factors on the performance of behaviour models.\n", "link": "http://arxiv.org/abs/2009.10033v5", "date": "2024-06-05", "relevancy": 2.0729, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5507}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5328}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solution%20Concepts%20in%20Hierarchical%20Games%20under%20Bounded%20Rationality%20with%0A%20%20Applications%20to%20Autonomous%20Driving&body=Title%3A%20Solution%20Concepts%20in%20Hierarchical%20Games%20under%20Bounded%20Rationality%20with%0A%20%20Applications%20to%20Autonomous%20Driving%0AAuthor%3A%20Atrisha%20Sarkar%20and%20Krzysztof%20Czarnecki%0AAbstract%3A%20%20%20With%20autonomous%20vehicles%20%28AV%29%20set%20to%20integrate%20further%20into%20regular%20human%0Atraffic%2C%20there%20is%20an%20increasing%20consensus%20on%20treating%20AV%20motion%20planning%20as%20a%0Amulti-agent%20problem.%20However%2C%20the%20traditional%20game-theoretic%20assumption%20of%0Acomplete%20rationality%20is%20too%20strong%20for%20human%20driving%2C%20and%20there%20is%20a%20need%20for%0Aunderstanding%20human%20driving%20as%20a%20%5Cemph%7Bbounded%20rational%7D%20activity%20through%20a%0Abehavioural%20game-theoretic%20lens.%20To%20that%20end%2C%20we%20adapt%20four%20metamodels%20of%0Abounded%20rational%20behaviour%3A%20three%20based%20on%20Quantal%20level-k%20and%20one%20based%20on%0ANash%20equilibrium%20with%20quantal%20errors.%20We%20formalize%20the%20different%20solution%0Aconcepts%20that%20can%20be%20applied%20in%20the%20context%20of%20hierarchical%20games%2C%20a%20framework%0Aused%20in%20multi-agent%20motion%20planning%2C%20for%20the%20purpose%20of%20creating%20game%20theoretic%0Amodels%20of%20driving%20behaviour.%20Furthermore%2C%20based%20on%20a%20contributed%20dataset%20of%0Ahuman%20driving%20at%20a%20busy%20urban%20intersection%20with%20a%20total%20of%20approximately%204k%0Aagents%20and%2044k%20decision%20points%2C%20we%20evaluate%20the%20behaviour%20models%20on%20the%20basis%0Aof%20model%20fit%20to%20naturalistic%20data%2C%20as%20well%20as%20their%20predictive%20capacity.%20Our%0Aresults%20suggest%20that%20among%20the%20behaviour%20models%20evaluated%2C%20at%20the%20level%20of%0Amaneuvers%2C%20modeling%20driving%20behaviour%20as%20an%20adaptation%20of%20the%20Quantal%20level-k%0Amodel%20with%20level-0%20behaviour%20modelled%20as%20pure%20rule-following%20provides%20the%20best%0Afit%20to%20naturalistic%20driving%20behaviour.%20At%20the%20level%20of%20trajectories%2C%20bounds%0Asampling%20of%20actions%20and%20a%20maxmax%20non-strategic%20models%20is%20the%20most%20accurate%0Awithin%20the%20set%20of%20models%20in%20comparison.%20We%20also%20find%20a%20significant%20impact%20of%0Asituational%20factors%20on%20the%20performance%20of%20behaviour%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2009.10033v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolution%2520Concepts%2520in%2520Hierarchical%2520Games%2520under%2520Bounded%2520Rationality%2520with%250A%2520%2520Applications%2520to%2520Autonomous%2520Driving%26entry.906535625%3DAtrisha%2520Sarkar%2520and%2520Krzysztof%2520Czarnecki%26entry.1292438233%3D%2520%2520With%2520autonomous%2520vehicles%2520%2528AV%2529%2520set%2520to%2520integrate%2520further%2520into%2520regular%2520human%250Atraffic%252C%2520there%2520is%2520an%2520increasing%2520consensus%2520on%2520treating%2520AV%2520motion%2520planning%2520as%2520a%250Amulti-agent%2520problem.%2520However%252C%2520the%2520traditional%2520game-theoretic%2520assumption%2520of%250Acomplete%2520rationality%2520is%2520too%2520strong%2520for%2520human%2520driving%252C%2520and%2520there%2520is%2520a%2520need%2520for%250Aunderstanding%2520human%2520driving%2520as%2520a%2520%255Cemph%257Bbounded%2520rational%257D%2520activity%2520through%2520a%250Abehavioural%2520game-theoretic%2520lens.%2520To%2520that%2520end%252C%2520we%2520adapt%2520four%2520metamodels%2520of%250Abounded%2520rational%2520behaviour%253A%2520three%2520based%2520on%2520Quantal%2520level-k%2520and%2520one%2520based%2520on%250ANash%2520equilibrium%2520with%2520quantal%2520errors.%2520We%2520formalize%2520the%2520different%2520solution%250Aconcepts%2520that%2520can%2520be%2520applied%2520in%2520the%2520context%2520of%2520hierarchical%2520games%252C%2520a%2520framework%250Aused%2520in%2520multi-agent%2520motion%2520planning%252C%2520for%2520the%2520purpose%2520of%2520creating%2520game%2520theoretic%250Amodels%2520of%2520driving%2520behaviour.%2520Furthermore%252C%2520based%2520on%2520a%2520contributed%2520dataset%2520of%250Ahuman%2520driving%2520at%2520a%2520busy%2520urban%2520intersection%2520with%2520a%2520total%2520of%2520approximately%25204k%250Aagents%2520and%252044k%2520decision%2520points%252C%2520we%2520evaluate%2520the%2520behaviour%2520models%2520on%2520the%2520basis%250Aof%2520model%2520fit%2520to%2520naturalistic%2520data%252C%2520as%2520well%2520as%2520their%2520predictive%2520capacity.%2520Our%250Aresults%2520suggest%2520that%2520among%2520the%2520behaviour%2520models%2520evaluated%252C%2520at%2520the%2520level%2520of%250Amaneuvers%252C%2520modeling%2520driving%2520behaviour%2520as%2520an%2520adaptation%2520of%2520the%2520Quantal%2520level-k%250Amodel%2520with%2520level-0%2520behaviour%2520modelled%2520as%2520pure%2520rule-following%2520provides%2520the%2520best%250Afit%2520to%2520naturalistic%2520driving%2520behaviour.%2520At%2520the%2520level%2520of%2520trajectories%252C%2520bounds%250Asampling%2520of%2520actions%2520and%2520a%2520maxmax%2520non-strategic%2520models%2520is%2520the%2520most%2520accurate%250Awithin%2520the%2520set%2520of%2520models%2520in%2520comparison.%2520We%2520also%2520find%2520a%2520significant%2520impact%2520of%250Asituational%2520factors%2520on%2520the%2520performance%2520of%2520behaviour%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2009.10033v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solution%20Concepts%20in%20Hierarchical%20Games%20under%20Bounded%20Rationality%20with%0A%20%20Applications%20to%20Autonomous%20Driving&entry.906535625=Atrisha%20Sarkar%20and%20Krzysztof%20Czarnecki&entry.1292438233=%20%20With%20autonomous%20vehicles%20%28AV%29%20set%20to%20integrate%20further%20into%20regular%20human%0Atraffic%2C%20there%20is%20an%20increasing%20consensus%20on%20treating%20AV%20motion%20planning%20as%20a%0Amulti-agent%20problem.%20However%2C%20the%20traditional%20game-theoretic%20assumption%20of%0Acomplete%20rationality%20is%20too%20strong%20for%20human%20driving%2C%20and%20there%20is%20a%20need%20for%0Aunderstanding%20human%20driving%20as%20a%20%5Cemph%7Bbounded%20rational%7D%20activity%20through%20a%0Abehavioural%20game-theoretic%20lens.%20To%20that%20end%2C%20we%20adapt%20four%20metamodels%20of%0Abounded%20rational%20behaviour%3A%20three%20based%20on%20Quantal%20level-k%20and%20one%20based%20on%0ANash%20equilibrium%20with%20quantal%20errors.%20We%20formalize%20the%20different%20solution%0Aconcepts%20that%20can%20be%20applied%20in%20the%20context%20of%20hierarchical%20games%2C%20a%20framework%0Aused%20in%20multi-agent%20motion%20planning%2C%20for%20the%20purpose%20of%20creating%20game%20theoretic%0Amodels%20of%20driving%20behaviour.%20Furthermore%2C%20based%20on%20a%20contributed%20dataset%20of%0Ahuman%20driving%20at%20a%20busy%20urban%20intersection%20with%20a%20total%20of%20approximately%204k%0Aagents%20and%2044k%20decision%20points%2C%20we%20evaluate%20the%20behaviour%20models%20on%20the%20basis%0Aof%20model%20fit%20to%20naturalistic%20data%2C%20as%20well%20as%20their%20predictive%20capacity.%20Our%0Aresults%20suggest%20that%20among%20the%20behaviour%20models%20evaluated%2C%20at%20the%20level%20of%0Amaneuvers%2C%20modeling%20driving%20behaviour%20as%20an%20adaptation%20of%20the%20Quantal%20level-k%0Amodel%20with%20level-0%20behaviour%20modelled%20as%20pure%20rule-following%20provides%20the%20best%0Afit%20to%20naturalistic%20driving%20behaviour.%20At%20the%20level%20of%20trajectories%2C%20bounds%0Asampling%20of%20actions%20and%20a%20maxmax%20non-strategic%20models%20is%20the%20most%20accurate%0Awithin%20the%20set%20of%20models%20in%20comparison.%20We%20also%20find%20a%20significant%20impact%20of%0Asituational%20factors%20on%20the%20performance%20of%20behaviour%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2009.10033v5&entry.124074799=Read"},
{"title": "Partial-Label Learning with a Reject Option", "author": "Tobias Fuchs and Florian Kalinke and Klemens B\u00f6hm", "abstract": "  In real-world applications, one often encounters ambiguously labeled data,\nwhere different annotators assign conflicting class labels. Partial-label\nlearning allows training classifiers in this weakly supervised setting, where\nstate-of-the-art methods already show good predictive performance. However,\neven the best algorithms give incorrect predictions, which can have severe\nconsequences when they impact actions or decisions. We propose a novel\nrisk-consistent partial-label learning algorithm with a reject option, that is,\nthe algorithm can reject unsure predictions. Extensive experiments on\nartificial and real-world datasets show that our method provides the best\ntrade-off between the number and accuracy of non-rejected predictions when\ncompared to our competitors, which use confidence thresholds for rejecting\nunsure predictions instead. When evaluated without the reject option, our\nnearest neighbor-based approach also achieves competitive prediction\nperformance.\n", "link": "http://arxiv.org/abs/2402.00592v3", "date": "2024-06-05", "relevancy": 2.0658, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5284}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5219}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partial-Label%20Learning%20with%20a%20Reject%20Option&body=Title%3A%20Partial-Label%20Learning%20with%20a%20Reject%20Option%0AAuthor%3A%20Tobias%20Fuchs%20and%20Florian%20Kalinke%20and%20Klemens%20B%C3%B6hm%0AAbstract%3A%20%20%20In%20real-world%20applications%2C%20one%20often%20encounters%20ambiguously%20labeled%20data%2C%0Awhere%20different%20annotators%20assign%20conflicting%20class%20labels.%20Partial-label%0Alearning%20allows%20training%20classifiers%20in%20this%20weakly%20supervised%20setting%2C%20where%0Astate-of-the-art%20methods%20already%20show%20good%20predictive%20performance.%20However%2C%0Aeven%20the%20best%20algorithms%20give%20incorrect%20predictions%2C%20which%20can%20have%20severe%0Aconsequences%20when%20they%20impact%20actions%20or%20decisions.%20We%20propose%20a%20novel%0Arisk-consistent%20partial-label%20learning%20algorithm%20with%20a%20reject%20option%2C%20that%20is%2C%0Athe%20algorithm%20can%20reject%20unsure%20predictions.%20Extensive%20experiments%20on%0Aartificial%20and%20real-world%20datasets%20show%20that%20our%20method%20provides%20the%20best%0Atrade-off%20between%20the%20number%20and%20accuracy%20of%20non-rejected%20predictions%20when%0Acompared%20to%20our%20competitors%2C%20which%20use%20confidence%20thresholds%20for%20rejecting%0Aunsure%20predictions%20instead.%20When%20evaluated%20without%20the%20reject%20option%2C%20our%0Anearest%20neighbor-based%20approach%20also%20achieves%20competitive%20prediction%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00592v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartial-Label%2520Learning%2520with%2520a%2520Reject%2520Option%26entry.906535625%3DTobias%2520Fuchs%2520and%2520Florian%2520Kalinke%2520and%2520Klemens%2520B%25C3%25B6hm%26entry.1292438233%3D%2520%2520In%2520real-world%2520applications%252C%2520one%2520often%2520encounters%2520ambiguously%2520labeled%2520data%252C%250Awhere%2520different%2520annotators%2520assign%2520conflicting%2520class%2520labels.%2520Partial-label%250Alearning%2520allows%2520training%2520classifiers%2520in%2520this%2520weakly%2520supervised%2520setting%252C%2520where%250Astate-of-the-art%2520methods%2520already%2520show%2520good%2520predictive%2520performance.%2520However%252C%250Aeven%2520the%2520best%2520algorithms%2520give%2520incorrect%2520predictions%252C%2520which%2520can%2520have%2520severe%250Aconsequences%2520when%2520they%2520impact%2520actions%2520or%2520decisions.%2520We%2520propose%2520a%2520novel%250Arisk-consistent%2520partial-label%2520learning%2520algorithm%2520with%2520a%2520reject%2520option%252C%2520that%2520is%252C%250Athe%2520algorithm%2520can%2520reject%2520unsure%2520predictions.%2520Extensive%2520experiments%2520on%250Aartificial%2520and%2520real-world%2520datasets%2520show%2520that%2520our%2520method%2520provides%2520the%2520best%250Atrade-off%2520between%2520the%2520number%2520and%2520accuracy%2520of%2520non-rejected%2520predictions%2520when%250Acompared%2520to%2520our%2520competitors%252C%2520which%2520use%2520confidence%2520thresholds%2520for%2520rejecting%250Aunsure%2520predictions%2520instead.%2520When%2520evaluated%2520without%2520the%2520reject%2520option%252C%2520our%250Anearest%2520neighbor-based%2520approach%2520also%2520achieves%2520competitive%2520prediction%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00592v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partial-Label%20Learning%20with%20a%20Reject%20Option&entry.906535625=Tobias%20Fuchs%20and%20Florian%20Kalinke%20and%20Klemens%20B%C3%B6hm&entry.1292438233=%20%20In%20real-world%20applications%2C%20one%20often%20encounters%20ambiguously%20labeled%20data%2C%0Awhere%20different%20annotators%20assign%20conflicting%20class%20labels.%20Partial-label%0Alearning%20allows%20training%20classifiers%20in%20this%20weakly%20supervised%20setting%2C%20where%0Astate-of-the-art%20methods%20already%20show%20good%20predictive%20performance.%20However%2C%0Aeven%20the%20best%20algorithms%20give%20incorrect%20predictions%2C%20which%20can%20have%20severe%0Aconsequences%20when%20they%20impact%20actions%20or%20decisions.%20We%20propose%20a%20novel%0Arisk-consistent%20partial-label%20learning%20algorithm%20with%20a%20reject%20option%2C%20that%20is%2C%0Athe%20algorithm%20can%20reject%20unsure%20predictions.%20Extensive%20experiments%20on%0Aartificial%20and%20real-world%20datasets%20show%20that%20our%20method%20provides%20the%20best%0Atrade-off%20between%20the%20number%20and%20accuracy%20of%20non-rejected%20predictions%20when%0Acompared%20to%20our%20competitors%2C%20which%20use%20confidence%20thresholds%20for%20rejecting%0Aunsure%20predictions%20instead.%20When%20evaluated%20without%20the%20reject%20option%2C%20our%0Anearest%20neighbor-based%20approach%20also%20achieves%20competitive%20prediction%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00592v3&entry.124074799=Read"},
{"title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for\n  Logical Reasoning", "author": "Qiming Bao and Alex Yuxuan Peng and Zhenyun Deng and Wanjun Zhong and Gael Gendron and Timothy Pistotti and Neset Tan and Nathan Young and Yang Chen and Yonghua Zhu and Paul Denny and Michael Witbrock and Jiamou Liu", "abstract": "  Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges when gathering reliable\ndata from the web to build comprehensive training datasets, subsequently\naffecting performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logical structure of the\nsentence, upon which operations are performed to generate logically modified\nAMR graphs. The modified AMR graphs are subsequently converted back into text\nto create augmented data. Notably, our methodology is architecture-agnostic and\nenhances both generative large language models, such as GPT-3.5 and GPT-4,\nthrough prompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor\nleaderboard\\footnote{\\url{https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347}}.\nThe source code and data are publicly\navailable\\footnote{\\href{https://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning}{AMR-LDA\nGitHub Repository}}.\n", "link": "http://arxiv.org/abs/2305.12599v5", "date": "2024-06-05", "relevancy": 2.0597, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5144}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abstract%20Meaning%20Representation-Based%20Logic-Driven%20Data%20Augmentation%20for%0A%20%20Logical%20Reasoning&body=Title%3A%20Abstract%20Meaning%20Representation-Based%20Logic-Driven%20Data%20Augmentation%20for%0A%20%20Logical%20Reasoning%0AAuthor%3A%20Qiming%20Bao%20and%20Alex%20Yuxuan%20Peng%20and%20Zhenyun%20Deng%20and%20Wanjun%20Zhong%20and%20Gael%20Gendron%20and%20Timothy%20Pistotti%20and%20Neset%20Tan%20and%20Nathan%20Young%20and%20Yang%20Chen%20and%20Yonghua%20Zhu%20and%20Paul%20Denny%20and%20Michael%20Witbrock%20and%20Jiamou%20Liu%0AAbstract%3A%20%20%20Combining%20large%20language%20models%20with%20logical%20reasoning%20enhances%20their%0Acapacity%20to%20address%20problems%20in%20a%20robust%20and%20reliable%20manner.%20Nevertheless%2C%20the%0Aintricate%20nature%20of%20logical%20reasoning%20poses%20challenges%20when%20gathering%20reliable%0Adata%20from%20the%20web%20to%20build%20comprehensive%20training%20datasets%2C%20subsequently%0Aaffecting%20performance%20on%20downstream%20tasks.%20To%20address%20this%2C%20we%20introduce%20a%0Anovel%20logic-driven%20data%20augmentation%20approach%2C%20AMR-LDA.%20AMR-LDA%20converts%20the%0Aoriginal%20text%20into%20an%20Abstract%20Meaning%20Representation%20%28AMR%29%20graph%2C%20a%20structured%0Asemantic%20representation%20that%20encapsulates%20the%20logical%20structure%20of%20the%0Asentence%2C%20upon%20which%20operations%20are%20performed%20to%20generate%20logically%20modified%0AAMR%20graphs.%20The%20modified%20AMR%20graphs%20are%20subsequently%20converted%20back%20into%20text%0Ato%20create%20augmented%20data.%20Notably%2C%20our%20methodology%20is%20architecture-agnostic%20and%0Aenhances%20both%20generative%20large%20language%20models%2C%20such%20as%20GPT-3.5%20and%20GPT-4%2C%0Athrough%20prompt%20augmentation%2C%20and%20discriminative%20large%20language%20models%20through%0Acontrastive%20learning%20with%20logic-driven%20data%20augmentation.%20Empirical%20evidence%0Aunderscores%20the%20efficacy%20of%20our%20proposed%20method%20with%20improvement%20in%20performance%0Aacross%20seven%20downstream%20tasks%2C%20such%20as%20reading%20comprehension%20requiring%20logical%0Areasoning%2C%20textual%20entailment%2C%20and%20natural%20language%20inference.%20Furthermore%2C%20our%0Amethod%20leads%20on%20the%20ReClor%0Aleaderboard%5Cfootnote%7B%5Curl%7Bhttps%3A//eval.ai/web/challenges/challenge-page/503/leaderboard/1347%7D%7D.%0AThe%20source%20code%20and%20data%20are%20publicly%0Aavailable%5Cfootnote%7B%5Chref%7Bhttps%3A//github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning%7D%7BAMR-LDA%0AGitHub%20Repository%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12599v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbstract%2520Meaning%2520Representation-Based%2520Logic-Driven%2520Data%2520Augmentation%2520for%250A%2520%2520Logical%2520Reasoning%26entry.906535625%3DQiming%2520Bao%2520and%2520Alex%2520Yuxuan%2520Peng%2520and%2520Zhenyun%2520Deng%2520and%2520Wanjun%2520Zhong%2520and%2520Gael%2520Gendron%2520and%2520Timothy%2520Pistotti%2520and%2520Neset%2520Tan%2520and%2520Nathan%2520Young%2520and%2520Yang%2520Chen%2520and%2520Yonghua%2520Zhu%2520and%2520Paul%2520Denny%2520and%2520Michael%2520Witbrock%2520and%2520Jiamou%2520Liu%26entry.1292438233%3D%2520%2520Combining%2520large%2520language%2520models%2520with%2520logical%2520reasoning%2520enhances%2520their%250Acapacity%2520to%2520address%2520problems%2520in%2520a%2520robust%2520and%2520reliable%2520manner.%2520Nevertheless%252C%2520the%250Aintricate%2520nature%2520of%2520logical%2520reasoning%2520poses%2520challenges%2520when%2520gathering%2520reliable%250Adata%2520from%2520the%2520web%2520to%2520build%2520comprehensive%2520training%2520datasets%252C%2520subsequently%250Aaffecting%2520performance%2520on%2520downstream%2520tasks.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%250Anovel%2520logic-driven%2520data%2520augmentation%2520approach%252C%2520AMR-LDA.%2520AMR-LDA%2520converts%2520the%250Aoriginal%2520text%2520into%2520an%2520Abstract%2520Meaning%2520Representation%2520%2528AMR%2529%2520graph%252C%2520a%2520structured%250Asemantic%2520representation%2520that%2520encapsulates%2520the%2520logical%2520structure%2520of%2520the%250Asentence%252C%2520upon%2520which%2520operations%2520are%2520performed%2520to%2520generate%2520logically%2520modified%250AAMR%2520graphs.%2520The%2520modified%2520AMR%2520graphs%2520are%2520subsequently%2520converted%2520back%2520into%2520text%250Ato%2520create%2520augmented%2520data.%2520Notably%252C%2520our%2520methodology%2520is%2520architecture-agnostic%2520and%250Aenhances%2520both%2520generative%2520large%2520language%2520models%252C%2520such%2520as%2520GPT-3.5%2520and%2520GPT-4%252C%250Athrough%2520prompt%2520augmentation%252C%2520and%2520discriminative%2520large%2520language%2520models%2520through%250Acontrastive%2520learning%2520with%2520logic-driven%2520data%2520augmentation.%2520Empirical%2520evidence%250Aunderscores%2520the%2520efficacy%2520of%2520our%2520proposed%2520method%2520with%2520improvement%2520in%2520performance%250Aacross%2520seven%2520downstream%2520tasks%252C%2520such%2520as%2520reading%2520comprehension%2520requiring%2520logical%250Areasoning%252C%2520textual%2520entailment%252C%2520and%2520natural%2520language%2520inference.%2520Furthermore%252C%2520our%250Amethod%2520leads%2520on%2520the%2520ReClor%250Aleaderboard%255Cfootnote%257B%255Curl%257Bhttps%253A//eval.ai/web/challenges/challenge-page/503/leaderboard/1347%257D%257D.%250AThe%2520source%2520code%2520and%2520data%2520are%2520publicly%250Aavailable%255Cfootnote%257B%255Chref%257Bhttps%253A//github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning%257D%257BAMR-LDA%250AGitHub%2520Repository%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.12599v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abstract%20Meaning%20Representation-Based%20Logic-Driven%20Data%20Augmentation%20for%0A%20%20Logical%20Reasoning&entry.906535625=Qiming%20Bao%20and%20Alex%20Yuxuan%20Peng%20and%20Zhenyun%20Deng%20and%20Wanjun%20Zhong%20and%20Gael%20Gendron%20and%20Timothy%20Pistotti%20and%20Neset%20Tan%20and%20Nathan%20Young%20and%20Yang%20Chen%20and%20Yonghua%20Zhu%20and%20Paul%20Denny%20and%20Michael%20Witbrock%20and%20Jiamou%20Liu&entry.1292438233=%20%20Combining%20large%20language%20models%20with%20logical%20reasoning%20enhances%20their%0Acapacity%20to%20address%20problems%20in%20a%20robust%20and%20reliable%20manner.%20Nevertheless%2C%20the%0Aintricate%20nature%20of%20logical%20reasoning%20poses%20challenges%20when%20gathering%20reliable%0Adata%20from%20the%20web%20to%20build%20comprehensive%20training%20datasets%2C%20subsequently%0Aaffecting%20performance%20on%20downstream%20tasks.%20To%20address%20this%2C%20we%20introduce%20a%0Anovel%20logic-driven%20data%20augmentation%20approach%2C%20AMR-LDA.%20AMR-LDA%20converts%20the%0Aoriginal%20text%20into%20an%20Abstract%20Meaning%20Representation%20%28AMR%29%20graph%2C%20a%20structured%0Asemantic%20representation%20that%20encapsulates%20the%20logical%20structure%20of%20the%0Asentence%2C%20upon%20which%20operations%20are%20performed%20to%20generate%20logically%20modified%0AAMR%20graphs.%20The%20modified%20AMR%20graphs%20are%20subsequently%20converted%20back%20into%20text%0Ato%20create%20augmented%20data.%20Notably%2C%20our%20methodology%20is%20architecture-agnostic%20and%0Aenhances%20both%20generative%20large%20language%20models%2C%20such%20as%20GPT-3.5%20and%20GPT-4%2C%0Athrough%20prompt%20augmentation%2C%20and%20discriminative%20large%20language%20models%20through%0Acontrastive%20learning%20with%20logic-driven%20data%20augmentation.%20Empirical%20evidence%0Aunderscores%20the%20efficacy%20of%20our%20proposed%20method%20with%20improvement%20in%20performance%0Aacross%20seven%20downstream%20tasks%2C%20such%20as%20reading%20comprehension%20requiring%20logical%0Areasoning%2C%20textual%20entailment%2C%20and%20natural%20language%20inference.%20Furthermore%2C%20our%0Amethod%20leads%20on%20the%20ReClor%0Aleaderboard%5Cfootnote%7B%5Curl%7Bhttps%3A//eval.ai/web/challenges/challenge-page/503/leaderboard/1347%7D%7D.%0AThe%20source%20code%20and%20data%20are%20publicly%0Aavailable%5Cfootnote%7B%5Chref%7Bhttps%3A//github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning%7D%7BAMR-LDA%0AGitHub%20Repository%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12599v5&entry.124074799=Read"},
{"title": "A General Framework for Learning from Weak Supervision", "author": "Hao Chen and Jindong Wang and Lei Feng and Xiang Li and Yidong Wang and Xing Xie and Masashi Sugiyama and Rita Singh and Bhiksha Raj", "abstract": "  Weakly supervised learning generally faces challenges in applicability to\nvarious scenarios with diverse weak supervision and in scalability due to the\ncomplexity of existing algorithms, thereby hindering the practical deployment.\nThis paper introduces a general framework for learning from weak supervision\n(GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization\n(EM) formulation, adeptly accommodating various weak supervision sources,\nincluding instance partial labels, aggregate statistics, pairwise observations,\nand unlabeled data. We further present an advanced algorithm that significantly\nsimplifies the EM computational demands using a Non-deterministic Finite\nAutomaton (NFA) along with a forward-backward algorithm, which effectively\nreduces time complexity from quadratic or factorial often required in existing\nsolutions to linear scale. The problem of learning from arbitrary weak\nsupervision is therefore converted to the NFA modeling of them. GLWS not only\nenhances the scalability of machine learning models but also demonstrates\nsuperior performance and versatility across 11 weak supervision scenarios. We\nhope our work paves the way for further advancements and practical deployment\nin this field.\n", "link": "http://arxiv.org/abs/2402.01922v3", "date": "2024-06-05", "relevancy": 2.0483, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5471}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4891}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20General%20Framework%20for%20Learning%20from%20Weak%20Supervision&body=Title%3A%20A%20General%20Framework%20for%20Learning%20from%20Weak%20Supervision%0AAuthor%3A%20Hao%20Chen%20and%20Jindong%20Wang%20and%20Lei%20Feng%20and%20Xiang%20Li%20and%20Yidong%20Wang%20and%20Xing%20Xie%20and%20Masashi%20Sugiyama%20and%20Rita%20Singh%20and%20Bhiksha%20Raj%0AAbstract%3A%20%20%20Weakly%20supervised%20learning%20generally%20faces%20challenges%20in%20applicability%20to%0Avarious%20scenarios%20with%20diverse%20weak%20supervision%20and%20in%20scalability%20due%20to%20the%0Acomplexity%20of%20existing%20algorithms%2C%20thereby%20hindering%20the%20practical%20deployment.%0AThis%20paper%20introduces%20a%20general%20framework%20for%20learning%20from%20weak%20supervision%0A%28GLWS%29%20with%20a%20novel%20algorithm.%20Central%20to%20GLWS%20is%20an%20Expectation-Maximization%0A%28EM%29%20formulation%2C%20adeptly%20accommodating%20various%20weak%20supervision%20sources%2C%0Aincluding%20instance%20partial%20labels%2C%20aggregate%20statistics%2C%20pairwise%20observations%2C%0Aand%20unlabeled%20data.%20We%20further%20present%20an%20advanced%20algorithm%20that%20significantly%0Asimplifies%20the%20EM%20computational%20demands%20using%20a%20Non-deterministic%20Finite%0AAutomaton%20%28NFA%29%20along%20with%20a%20forward-backward%20algorithm%2C%20which%20effectively%0Areduces%20time%20complexity%20from%20quadratic%20or%20factorial%20often%20required%20in%20existing%0Asolutions%20to%20linear%20scale.%20The%20problem%20of%20learning%20from%20arbitrary%20weak%0Asupervision%20is%20therefore%20converted%20to%20the%20NFA%20modeling%20of%20them.%20GLWS%20not%20only%0Aenhances%20the%20scalability%20of%20machine%20learning%20models%20but%20also%20demonstrates%0Asuperior%20performance%20and%20versatility%20across%2011%20weak%20supervision%20scenarios.%20We%0Ahope%20our%20work%20paves%20the%20way%20for%20further%20advancements%20and%20practical%20deployment%0Ain%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01922v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520General%2520Framework%2520for%2520Learning%2520from%2520Weak%2520Supervision%26entry.906535625%3DHao%2520Chen%2520and%2520Jindong%2520Wang%2520and%2520Lei%2520Feng%2520and%2520Xiang%2520Li%2520and%2520Yidong%2520Wang%2520and%2520Xing%2520Xie%2520and%2520Masashi%2520Sugiyama%2520and%2520Rita%2520Singh%2520and%2520Bhiksha%2520Raj%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520learning%2520generally%2520faces%2520challenges%2520in%2520applicability%2520to%250Avarious%2520scenarios%2520with%2520diverse%2520weak%2520supervision%2520and%2520in%2520scalability%2520due%2520to%2520the%250Acomplexity%2520of%2520existing%2520algorithms%252C%2520thereby%2520hindering%2520the%2520practical%2520deployment.%250AThis%2520paper%2520introduces%2520a%2520general%2520framework%2520for%2520learning%2520from%2520weak%2520supervision%250A%2528GLWS%2529%2520with%2520a%2520novel%2520algorithm.%2520Central%2520to%2520GLWS%2520is%2520an%2520Expectation-Maximization%250A%2528EM%2529%2520formulation%252C%2520adeptly%2520accommodating%2520various%2520weak%2520supervision%2520sources%252C%250Aincluding%2520instance%2520partial%2520labels%252C%2520aggregate%2520statistics%252C%2520pairwise%2520observations%252C%250Aand%2520unlabeled%2520data.%2520We%2520further%2520present%2520an%2520advanced%2520algorithm%2520that%2520significantly%250Asimplifies%2520the%2520EM%2520computational%2520demands%2520using%2520a%2520Non-deterministic%2520Finite%250AAutomaton%2520%2528NFA%2529%2520along%2520with%2520a%2520forward-backward%2520algorithm%252C%2520which%2520effectively%250Areduces%2520time%2520complexity%2520from%2520quadratic%2520or%2520factorial%2520often%2520required%2520in%2520existing%250Asolutions%2520to%2520linear%2520scale.%2520The%2520problem%2520of%2520learning%2520from%2520arbitrary%2520weak%250Asupervision%2520is%2520therefore%2520converted%2520to%2520the%2520NFA%2520modeling%2520of%2520them.%2520GLWS%2520not%2520only%250Aenhances%2520the%2520scalability%2520of%2520machine%2520learning%2520models%2520but%2520also%2520demonstrates%250Asuperior%2520performance%2520and%2520versatility%2520across%252011%2520weak%2520supervision%2520scenarios.%2520We%250Ahope%2520our%2520work%2520paves%2520the%2520way%2520for%2520further%2520advancements%2520and%2520practical%2520deployment%250Ain%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01922v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Framework%20for%20Learning%20from%20Weak%20Supervision&entry.906535625=Hao%20Chen%20and%20Jindong%20Wang%20and%20Lei%20Feng%20and%20Xiang%20Li%20and%20Yidong%20Wang%20and%20Xing%20Xie%20and%20Masashi%20Sugiyama%20and%20Rita%20Singh%20and%20Bhiksha%20Raj&entry.1292438233=%20%20Weakly%20supervised%20learning%20generally%20faces%20challenges%20in%20applicability%20to%0Avarious%20scenarios%20with%20diverse%20weak%20supervision%20and%20in%20scalability%20due%20to%20the%0Acomplexity%20of%20existing%20algorithms%2C%20thereby%20hindering%20the%20practical%20deployment.%0AThis%20paper%20introduces%20a%20general%20framework%20for%20learning%20from%20weak%20supervision%0A%28GLWS%29%20with%20a%20novel%20algorithm.%20Central%20to%20GLWS%20is%20an%20Expectation-Maximization%0A%28EM%29%20formulation%2C%20adeptly%20accommodating%20various%20weak%20supervision%20sources%2C%0Aincluding%20instance%20partial%20labels%2C%20aggregate%20statistics%2C%20pairwise%20observations%2C%0Aand%20unlabeled%20data.%20We%20further%20present%20an%20advanced%20algorithm%20that%20significantly%0Asimplifies%20the%20EM%20computational%20demands%20using%20a%20Non-deterministic%20Finite%0AAutomaton%20%28NFA%29%20along%20with%20a%20forward-backward%20algorithm%2C%20which%20effectively%0Areduces%20time%20complexity%20from%20quadratic%20or%20factorial%20often%20required%20in%20existing%0Asolutions%20to%20linear%20scale.%20The%20problem%20of%20learning%20from%20arbitrary%20weak%0Asupervision%20is%20therefore%20converted%20to%20the%20NFA%20modeling%20of%20them.%20GLWS%20not%20only%0Aenhances%20the%20scalability%20of%20machine%20learning%20models%20but%20also%20demonstrates%0Asuperior%20performance%20and%20versatility%20across%2011%20weak%20supervision%20scenarios.%20We%0Ahope%20our%20work%20paves%20the%20way%20for%20further%20advancements%20and%20practical%20deployment%0Ain%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01922v3&entry.124074799=Read"},
{"title": "Adaptive Distance Functions via Kelvin Transformation", "author": "Rafael I. Cabral Muchacho and Florian T. Pokorny", "abstract": "  The term safety in robotics is often understood as a synonym for avoidance.\nAlthough this perspective has led to progress in path planning and reactive\ncontrol, a generalization of this perspective is necessary to include task\nsemantics relevant to contact-rich manipulation tasks, especially during\nteleoperation and to ensure the safety of learned policies.\n  We introduce the semantics-aware distance function and a corresponding\ncomputational method based on the Kelvin Transformation. The semantics-aware\ndistance generalizes signed distance functions by allowing the zero level set\nto lie inside of the object in regions where contact is allowed, effectively\nincorporating task semantics -- such as object affordances and user intent --\nin an adaptive implicit representation of safe sets. In validation experiments\nwe show the capability of our method to adapt to time-varying semantic\ninformation, and to perform queries in sub-microsecond, enabling applications\nin reinforcement learning, trajectory optimization, and motion planning.\n", "link": "http://arxiv.org/abs/2406.03200v1", "date": "2024-06-05", "relevancy": 2.0429, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5712}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.517}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Distance%20Functions%20via%20Kelvin%20Transformation&body=Title%3A%20Adaptive%20Distance%20Functions%20via%20Kelvin%20Transformation%0AAuthor%3A%20Rafael%20I.%20Cabral%20Muchacho%20and%20Florian%20T.%20Pokorny%0AAbstract%3A%20%20%20The%20term%20safety%20in%20robotics%20is%20often%20understood%20as%20a%20synonym%20for%20avoidance.%0AAlthough%20this%20perspective%20has%20led%20to%20progress%20in%20path%20planning%20and%20reactive%0Acontrol%2C%20a%20generalization%20of%20this%20perspective%20is%20necessary%20to%20include%20task%0Asemantics%20relevant%20to%20contact-rich%20manipulation%20tasks%2C%20especially%20during%0Ateleoperation%20and%20to%20ensure%20the%20safety%20of%20learned%20policies.%0A%20%20We%20introduce%20the%20semantics-aware%20distance%20function%20and%20a%20corresponding%0Acomputational%20method%20based%20on%20the%20Kelvin%20Transformation.%20The%20semantics-aware%0Adistance%20generalizes%20signed%20distance%20functions%20by%20allowing%20the%20zero%20level%20set%0Ato%20lie%20inside%20of%20the%20object%20in%20regions%20where%20contact%20is%20allowed%2C%20effectively%0Aincorporating%20task%20semantics%20--%20such%20as%20object%20affordances%20and%20user%20intent%20--%0Ain%20an%20adaptive%20implicit%20representation%20of%20safe%20sets.%20In%20validation%20experiments%0Awe%20show%20the%20capability%20of%20our%20method%20to%20adapt%20to%20time-varying%20semantic%0Ainformation%2C%20and%20to%20perform%20queries%20in%20sub-microsecond%2C%20enabling%20applications%0Ain%20reinforcement%20learning%2C%20trajectory%20optimization%2C%20and%20motion%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Distance%2520Functions%2520via%2520Kelvin%2520Transformation%26entry.906535625%3DRafael%2520I.%2520Cabral%2520Muchacho%2520and%2520Florian%2520T.%2520Pokorny%26entry.1292438233%3D%2520%2520The%2520term%2520safety%2520in%2520robotics%2520is%2520often%2520understood%2520as%2520a%2520synonym%2520for%2520avoidance.%250AAlthough%2520this%2520perspective%2520has%2520led%2520to%2520progress%2520in%2520path%2520planning%2520and%2520reactive%250Acontrol%252C%2520a%2520generalization%2520of%2520this%2520perspective%2520is%2520necessary%2520to%2520include%2520task%250Asemantics%2520relevant%2520to%2520contact-rich%2520manipulation%2520tasks%252C%2520especially%2520during%250Ateleoperation%2520and%2520to%2520ensure%2520the%2520safety%2520of%2520learned%2520policies.%250A%2520%2520We%2520introduce%2520the%2520semantics-aware%2520distance%2520function%2520and%2520a%2520corresponding%250Acomputational%2520method%2520based%2520on%2520the%2520Kelvin%2520Transformation.%2520The%2520semantics-aware%250Adistance%2520generalizes%2520signed%2520distance%2520functions%2520by%2520allowing%2520the%2520zero%2520level%2520set%250Ato%2520lie%2520inside%2520of%2520the%2520object%2520in%2520regions%2520where%2520contact%2520is%2520allowed%252C%2520effectively%250Aincorporating%2520task%2520semantics%2520--%2520such%2520as%2520object%2520affordances%2520and%2520user%2520intent%2520--%250Ain%2520an%2520adaptive%2520implicit%2520representation%2520of%2520safe%2520sets.%2520In%2520validation%2520experiments%250Awe%2520show%2520the%2520capability%2520of%2520our%2520method%2520to%2520adapt%2520to%2520time-varying%2520semantic%250Ainformation%252C%2520and%2520to%2520perform%2520queries%2520in%2520sub-microsecond%252C%2520enabling%2520applications%250Ain%2520reinforcement%2520learning%252C%2520trajectory%2520optimization%252C%2520and%2520motion%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Distance%20Functions%20via%20Kelvin%20Transformation&entry.906535625=Rafael%20I.%20Cabral%20Muchacho%20and%20Florian%20T.%20Pokorny&entry.1292438233=%20%20The%20term%20safety%20in%20robotics%20is%20often%20understood%20as%20a%20synonym%20for%20avoidance.%0AAlthough%20this%20perspective%20has%20led%20to%20progress%20in%20path%20planning%20and%20reactive%0Acontrol%2C%20a%20generalization%20of%20this%20perspective%20is%20necessary%20to%20include%20task%0Asemantics%20relevant%20to%20contact-rich%20manipulation%20tasks%2C%20especially%20during%0Ateleoperation%20and%20to%20ensure%20the%20safety%20of%20learned%20policies.%0A%20%20We%20introduce%20the%20semantics-aware%20distance%20function%20and%20a%20corresponding%0Acomputational%20method%20based%20on%20the%20Kelvin%20Transformation.%20The%20semantics-aware%0Adistance%20generalizes%20signed%20distance%20functions%20by%20allowing%20the%20zero%20level%20set%0Ato%20lie%20inside%20of%20the%20object%20in%20regions%20where%20contact%20is%20allowed%2C%20effectively%0Aincorporating%20task%20semantics%20--%20such%20as%20object%20affordances%20and%20user%20intent%20--%0Ain%20an%20adaptive%20implicit%20representation%20of%20safe%20sets.%20In%20validation%20experiments%0Awe%20show%20the%20capability%20of%20our%20method%20to%20adapt%20to%20time-varying%20semantic%0Ainformation%2C%20and%20to%20perform%20queries%20in%20sub-microsecond%2C%20enabling%20applications%0Ain%20reinforcement%20learning%2C%20trajectory%20optimization%2C%20and%20motion%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03200v1&entry.124074799=Read"},
{"title": "Distribution-Free Conformal Joint Prediction Regions for Neural Marked\n  Temporal Point Processes", "author": "Victor Dheur and Tanguy Bosser and Rafael Izbicki and Souhaib Ben Taieb", "abstract": "  Sequences of labeled events observed at irregular intervals in continuous\ntime are ubiquitous across various fields. Temporal Point Processes (TPPs)\nprovide a mathematical framework for modeling these sequences, enabling\ninferences such as predicting the arrival time of future events and their\nassociated label, called mark. However, due to model misspecification or lack\nof training data, these probabilistic models may provide a poor approximation\nof the true, unknown underlying process, with prediction regions extracted from\nthem being unreliable estimates of the underlying uncertainty. This paper\ndevelops more reliable methods for uncertainty quantification in neural TPP\nmodels via the framework of conformal prediction. A primary objective is to\ngenerate a distribution-free joint prediction region for an event's arrival\ntime and mark, with a finite-sample marginal coverage guarantee. A key\nchallenge is to handle both a strictly positive, continuous response and a\ncategorical response, without distributional assumptions. We first consider a\nsimple but conservative approach that combines individual prediction regions\nfor the event's arrival time and mark. Then, we introduce a more effective\nmethod based on bivariate highest density regions derived from the joint\npredictive density of arrival times and marks. By leveraging the dependencies\nbetween these two variables, this method excludes unlikely combinations of the\ntwo, resulting in sharper prediction regions while still attaining the\npre-specified coverage level. We also explore the generation of individual\nunivariate prediction regions for events' arrival times and marks through\nconformal regression and classification techniques. Moreover, we evaluate the\nstronger notion of conditional coverage. Finally, through extensive\nexperimentation on both simulated and real-world datasets, we assess the\nvalidity and efficiency of these methods.\n", "link": "http://arxiv.org/abs/2401.04612v2", "date": "2024-06-05", "relevancy": 2.0384, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5542}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5241}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution-Free%20Conformal%20Joint%20Prediction%20Regions%20for%20Neural%20Marked%0A%20%20Temporal%20Point%20Processes&body=Title%3A%20Distribution-Free%20Conformal%20Joint%20Prediction%20Regions%20for%20Neural%20Marked%0A%20%20Temporal%20Point%20Processes%0AAuthor%3A%20Victor%20Dheur%20and%20Tanguy%20Bosser%20and%20Rafael%20Izbicki%20and%20Souhaib%20Ben%20Taieb%0AAbstract%3A%20%20%20Sequences%20of%20labeled%20events%20observed%20at%20irregular%20intervals%20in%20continuous%0Atime%20are%20ubiquitous%20across%20various%20fields.%20Temporal%20Point%20Processes%20%28TPPs%29%0Aprovide%20a%20mathematical%20framework%20for%20modeling%20these%20sequences%2C%20enabling%0Ainferences%20such%20as%20predicting%20the%20arrival%20time%20of%20future%20events%20and%20their%0Aassociated%20label%2C%20called%20mark.%20However%2C%20due%20to%20model%20misspecification%20or%20lack%0Aof%20training%20data%2C%20these%20probabilistic%20models%20may%20provide%20a%20poor%20approximation%0Aof%20the%20true%2C%20unknown%20underlying%20process%2C%20with%20prediction%20regions%20extracted%20from%0Athem%20being%20unreliable%20estimates%20of%20the%20underlying%20uncertainty.%20This%20paper%0Adevelops%20more%20reliable%20methods%20for%20uncertainty%20quantification%20in%20neural%20TPP%0Amodels%20via%20the%20framework%20of%20conformal%20prediction.%20A%20primary%20objective%20is%20to%0Agenerate%20a%20distribution-free%20joint%20prediction%20region%20for%20an%20event%27s%20arrival%0Atime%20and%20mark%2C%20with%20a%20finite-sample%20marginal%20coverage%20guarantee.%20A%20key%0Achallenge%20is%20to%20handle%20both%20a%20strictly%20positive%2C%20continuous%20response%20and%20a%0Acategorical%20response%2C%20without%20distributional%20assumptions.%20We%20first%20consider%20a%0Asimple%20but%20conservative%20approach%20that%20combines%20individual%20prediction%20regions%0Afor%20the%20event%27s%20arrival%20time%20and%20mark.%20Then%2C%20we%20introduce%20a%20more%20effective%0Amethod%20based%20on%20bivariate%20highest%20density%20regions%20derived%20from%20the%20joint%0Apredictive%20density%20of%20arrival%20times%20and%20marks.%20By%20leveraging%20the%20dependencies%0Abetween%20these%20two%20variables%2C%20this%20method%20excludes%20unlikely%20combinations%20of%20the%0Atwo%2C%20resulting%20in%20sharper%20prediction%20regions%20while%20still%20attaining%20the%0Apre-specified%20coverage%20level.%20We%20also%20explore%20the%20generation%20of%20individual%0Aunivariate%20prediction%20regions%20for%20events%27%20arrival%20times%20and%20marks%20through%0Aconformal%20regression%20and%20classification%20techniques.%20Moreover%2C%20we%20evaluate%20the%0Astronger%20notion%20of%20conditional%20coverage.%20Finally%2C%20through%20extensive%0Aexperimentation%20on%20both%20simulated%20and%20real-world%20datasets%2C%20we%20assess%20the%0Avalidity%20and%20efficiency%20of%20these%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution-Free%2520Conformal%2520Joint%2520Prediction%2520Regions%2520for%2520Neural%2520Marked%250A%2520%2520Temporal%2520Point%2520Processes%26entry.906535625%3DVictor%2520Dheur%2520and%2520Tanguy%2520Bosser%2520and%2520Rafael%2520Izbicki%2520and%2520Souhaib%2520Ben%2520Taieb%26entry.1292438233%3D%2520%2520Sequences%2520of%2520labeled%2520events%2520observed%2520at%2520irregular%2520intervals%2520in%2520continuous%250Atime%2520are%2520ubiquitous%2520across%2520various%2520fields.%2520Temporal%2520Point%2520Processes%2520%2528TPPs%2529%250Aprovide%2520a%2520mathematical%2520framework%2520for%2520modeling%2520these%2520sequences%252C%2520enabling%250Ainferences%2520such%2520as%2520predicting%2520the%2520arrival%2520time%2520of%2520future%2520events%2520and%2520their%250Aassociated%2520label%252C%2520called%2520mark.%2520However%252C%2520due%2520to%2520model%2520misspecification%2520or%2520lack%250Aof%2520training%2520data%252C%2520these%2520probabilistic%2520models%2520may%2520provide%2520a%2520poor%2520approximation%250Aof%2520the%2520true%252C%2520unknown%2520underlying%2520process%252C%2520with%2520prediction%2520regions%2520extracted%2520from%250Athem%2520being%2520unreliable%2520estimates%2520of%2520the%2520underlying%2520uncertainty.%2520This%2520paper%250Adevelops%2520more%2520reliable%2520methods%2520for%2520uncertainty%2520quantification%2520in%2520neural%2520TPP%250Amodels%2520via%2520the%2520framework%2520of%2520conformal%2520prediction.%2520A%2520primary%2520objective%2520is%2520to%250Agenerate%2520a%2520distribution-free%2520joint%2520prediction%2520region%2520for%2520an%2520event%2527s%2520arrival%250Atime%2520and%2520mark%252C%2520with%2520a%2520finite-sample%2520marginal%2520coverage%2520guarantee.%2520A%2520key%250Achallenge%2520is%2520to%2520handle%2520both%2520a%2520strictly%2520positive%252C%2520continuous%2520response%2520and%2520a%250Acategorical%2520response%252C%2520without%2520distributional%2520assumptions.%2520We%2520first%2520consider%2520a%250Asimple%2520but%2520conservative%2520approach%2520that%2520combines%2520individual%2520prediction%2520regions%250Afor%2520the%2520event%2527s%2520arrival%2520time%2520and%2520mark.%2520Then%252C%2520we%2520introduce%2520a%2520more%2520effective%250Amethod%2520based%2520on%2520bivariate%2520highest%2520density%2520regions%2520derived%2520from%2520the%2520joint%250Apredictive%2520density%2520of%2520arrival%2520times%2520and%2520marks.%2520By%2520leveraging%2520the%2520dependencies%250Abetween%2520these%2520two%2520variables%252C%2520this%2520method%2520excludes%2520unlikely%2520combinations%2520of%2520the%250Atwo%252C%2520resulting%2520in%2520sharper%2520prediction%2520regions%2520while%2520still%2520attaining%2520the%250Apre-specified%2520coverage%2520level.%2520We%2520also%2520explore%2520the%2520generation%2520of%2520individual%250Aunivariate%2520prediction%2520regions%2520for%2520events%2527%2520arrival%2520times%2520and%2520marks%2520through%250Aconformal%2520regression%2520and%2520classification%2520techniques.%2520Moreover%252C%2520we%2520evaluate%2520the%250Astronger%2520notion%2520of%2520conditional%2520coverage.%2520Finally%252C%2520through%2520extensive%250Aexperimentation%2520on%2520both%2520simulated%2520and%2520real-world%2520datasets%252C%2520we%2520assess%2520the%250Avalidity%2520and%2520efficiency%2520of%2520these%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution-Free%20Conformal%20Joint%20Prediction%20Regions%20for%20Neural%20Marked%0A%20%20Temporal%20Point%20Processes&entry.906535625=Victor%20Dheur%20and%20Tanguy%20Bosser%20and%20Rafael%20Izbicki%20and%20Souhaib%20Ben%20Taieb&entry.1292438233=%20%20Sequences%20of%20labeled%20events%20observed%20at%20irregular%20intervals%20in%20continuous%0Atime%20are%20ubiquitous%20across%20various%20fields.%20Temporal%20Point%20Processes%20%28TPPs%29%0Aprovide%20a%20mathematical%20framework%20for%20modeling%20these%20sequences%2C%20enabling%0Ainferences%20such%20as%20predicting%20the%20arrival%20time%20of%20future%20events%20and%20their%0Aassociated%20label%2C%20called%20mark.%20However%2C%20due%20to%20model%20misspecification%20or%20lack%0Aof%20training%20data%2C%20these%20probabilistic%20models%20may%20provide%20a%20poor%20approximation%0Aof%20the%20true%2C%20unknown%20underlying%20process%2C%20with%20prediction%20regions%20extracted%20from%0Athem%20being%20unreliable%20estimates%20of%20the%20underlying%20uncertainty.%20This%20paper%0Adevelops%20more%20reliable%20methods%20for%20uncertainty%20quantification%20in%20neural%20TPP%0Amodels%20via%20the%20framework%20of%20conformal%20prediction.%20A%20primary%20objective%20is%20to%0Agenerate%20a%20distribution-free%20joint%20prediction%20region%20for%20an%20event%27s%20arrival%0Atime%20and%20mark%2C%20with%20a%20finite-sample%20marginal%20coverage%20guarantee.%20A%20key%0Achallenge%20is%20to%20handle%20both%20a%20strictly%20positive%2C%20continuous%20response%20and%20a%0Acategorical%20response%2C%20without%20distributional%20assumptions.%20We%20first%20consider%20a%0Asimple%20but%20conservative%20approach%20that%20combines%20individual%20prediction%20regions%0Afor%20the%20event%27s%20arrival%20time%20and%20mark.%20Then%2C%20we%20introduce%20a%20more%20effective%0Amethod%20based%20on%20bivariate%20highest%20density%20regions%20derived%20from%20the%20joint%0Apredictive%20density%20of%20arrival%20times%20and%20marks.%20By%20leveraging%20the%20dependencies%0Abetween%20these%20two%20variables%2C%20this%20method%20excludes%20unlikely%20combinations%20of%20the%0Atwo%2C%20resulting%20in%20sharper%20prediction%20regions%20while%20still%20attaining%20the%0Apre-specified%20coverage%20level.%20We%20also%20explore%20the%20generation%20of%20individual%0Aunivariate%20prediction%20regions%20for%20events%27%20arrival%20times%20and%20marks%20through%0Aconformal%20regression%20and%20classification%20techniques.%20Moreover%2C%20we%20evaluate%20the%0Astronger%20notion%20of%20conditional%20coverage.%20Finally%2C%20through%20extensive%0Aexperimentation%20on%20both%20simulated%20and%20real-world%20datasets%2C%20we%20assess%20the%0Avalidity%20and%20efficiency%20of%20these%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04612v2&entry.124074799=Read"},
{"title": "Enhancing CTC-based speech recognition with diverse modeling units", "author": "Shiyi Han and Zhihong Lei and Mingbin Xu and Xingyu Na and Zhen Huang", "abstract": "  In recent years, the evolution of end-to-end (E2E) automatic speech\nrecognition (ASR) models has been remarkable, largely due to advances in deep\nlearning architectures like transformer. On top of E2E systems, researchers\nhave achieved substantial accuracy improvement by rescoring E2E model's N-best\nhypotheses with a phoneme-based model. This raises an interesting question\nabout where the improvements come from other than the system combination\neffect. We examine the underlying mechanisms driving these gains and propose an\nefficient joint training approach, where E2E models are trained jointly with\ndiverse modeling units. This methodology does not only align the strengths of\nboth phoneme and grapheme-based models but also reveals that using these\ndiverse modeling units in a synergistic way can significantly enhance model\naccuracy. Our findings offer new insights into the optimal integration of\nheterogeneous modeling units in the development of more robust and accurate ASR\nsystems.\n", "link": "http://arxiv.org/abs/2406.03274v1", "date": "2024-06-05", "relevancy": 2.0261, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5289}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20CTC-based%20speech%20recognition%20with%20diverse%20modeling%20units&body=Title%3A%20Enhancing%20CTC-based%20speech%20recognition%20with%20diverse%20modeling%20units%0AAuthor%3A%20Shiyi%20Han%20and%20Zhihong%20Lei%20and%20Mingbin%20Xu%20and%20Xingyu%20Na%20and%20Zhen%20Huang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20evolution%20of%20end-to-end%20%28E2E%29%20automatic%20speech%0Arecognition%20%28ASR%29%20models%20has%20been%20remarkable%2C%20largely%20due%20to%20advances%20in%20deep%0Alearning%20architectures%20like%20transformer.%20On%20top%20of%20E2E%20systems%2C%20researchers%0Ahave%20achieved%20substantial%20accuracy%20improvement%20by%20rescoring%20E2E%20model%27s%20N-best%0Ahypotheses%20with%20a%20phoneme-based%20model.%20This%20raises%20an%20interesting%20question%0Aabout%20where%20the%20improvements%20come%20from%20other%20than%20the%20system%20combination%0Aeffect.%20We%20examine%20the%20underlying%20mechanisms%20driving%20these%20gains%20and%20propose%20an%0Aefficient%20joint%20training%20approach%2C%20where%20E2E%20models%20are%20trained%20jointly%20with%0Adiverse%20modeling%20units.%20This%20methodology%20does%20not%20only%20align%20the%20strengths%20of%0Aboth%20phoneme%20and%20grapheme-based%20models%20but%20also%20reveals%20that%20using%20these%0Adiverse%20modeling%20units%20in%20a%20synergistic%20way%20can%20significantly%20enhance%20model%0Aaccuracy.%20Our%20findings%20offer%20new%20insights%20into%20the%20optimal%20integration%20of%0Aheterogeneous%20modeling%20units%20in%20the%20development%20of%20more%20robust%20and%20accurate%20ASR%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520CTC-based%2520speech%2520recognition%2520with%2520diverse%2520modeling%2520units%26entry.906535625%3DShiyi%2520Han%2520and%2520Zhihong%2520Lei%2520and%2520Mingbin%2520Xu%2520and%2520Xingyu%2520Na%2520and%2520Zhen%2520Huang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520evolution%2520of%2520end-to-end%2520%2528E2E%2529%2520automatic%2520speech%250Arecognition%2520%2528ASR%2529%2520models%2520has%2520been%2520remarkable%252C%2520largely%2520due%2520to%2520advances%2520in%2520deep%250Alearning%2520architectures%2520like%2520transformer.%2520On%2520top%2520of%2520E2E%2520systems%252C%2520researchers%250Ahave%2520achieved%2520substantial%2520accuracy%2520improvement%2520by%2520rescoring%2520E2E%2520model%2527s%2520N-best%250Ahypotheses%2520with%2520a%2520phoneme-based%2520model.%2520This%2520raises%2520an%2520interesting%2520question%250Aabout%2520where%2520the%2520improvements%2520come%2520from%2520other%2520than%2520the%2520system%2520combination%250Aeffect.%2520We%2520examine%2520the%2520underlying%2520mechanisms%2520driving%2520these%2520gains%2520and%2520propose%2520an%250Aefficient%2520joint%2520training%2520approach%252C%2520where%2520E2E%2520models%2520are%2520trained%2520jointly%2520with%250Adiverse%2520modeling%2520units.%2520This%2520methodology%2520does%2520not%2520only%2520align%2520the%2520strengths%2520of%250Aboth%2520phoneme%2520and%2520grapheme-based%2520models%2520but%2520also%2520reveals%2520that%2520using%2520these%250Adiverse%2520modeling%2520units%2520in%2520a%2520synergistic%2520way%2520can%2520significantly%2520enhance%2520model%250Aaccuracy.%2520Our%2520findings%2520offer%2520new%2520insights%2520into%2520the%2520optimal%2520integration%2520of%250Aheterogeneous%2520modeling%2520units%2520in%2520the%2520development%2520of%2520more%2520robust%2520and%2520accurate%2520ASR%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20CTC-based%20speech%20recognition%20with%20diverse%20modeling%20units&entry.906535625=Shiyi%20Han%20and%20Zhihong%20Lei%20and%20Mingbin%20Xu%20and%20Xingyu%20Na%20and%20Zhen%20Huang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20evolution%20of%20end-to-end%20%28E2E%29%20automatic%20speech%0Arecognition%20%28ASR%29%20models%20has%20been%20remarkable%2C%20largely%20due%20to%20advances%20in%20deep%0Alearning%20architectures%20like%20transformer.%20On%20top%20of%20E2E%20systems%2C%20researchers%0Ahave%20achieved%20substantial%20accuracy%20improvement%20by%20rescoring%20E2E%20model%27s%20N-best%0Ahypotheses%20with%20a%20phoneme-based%20model.%20This%20raises%20an%20interesting%20question%0Aabout%20where%20the%20improvements%20come%20from%20other%20than%20the%20system%20combination%0Aeffect.%20We%20examine%20the%20underlying%20mechanisms%20driving%20these%20gains%20and%20propose%20an%0Aefficient%20joint%20training%20approach%2C%20where%20E2E%20models%20are%20trained%20jointly%20with%0Adiverse%20modeling%20units.%20This%20methodology%20does%20not%20only%20align%20the%20strengths%20of%0Aboth%20phoneme%20and%20grapheme-based%20models%20but%20also%20reveals%20that%20using%20these%0Adiverse%20modeling%20units%20in%20a%20synergistic%20way%20can%20significantly%20enhance%20model%0Aaccuracy.%20Our%20findings%20offer%20new%20insights%20into%20the%20optimal%20integration%20of%0Aheterogeneous%20modeling%20units%20in%20the%20development%20of%20more%20robust%20and%20accurate%20ASR%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03274v1&entry.124074799=Read"},
{"title": "Deep Generative Models for Proton Zero Degree Calorimeter Simulations in\n  ALICE, CERN", "author": "Patryk B\u0119dkowski and Jan Dubi\u0144ski and Kamil Deja and Przemys\u0142aw Rokita", "abstract": "  Simulating detector responses is a crucial part of understanding the\ninner-workings of particle collisions in the Large Hadron Collider at CERN. The\ncurrent reliance on statistical Monte-Carlo simulations strains CERN's\ncomputational grid, underscoring the urgency for more efficient alternatives.\nAddressing these challenges, recent proposals advocate for generative machine\nlearning methods. In this study, we present an innovative deep learning\nsimulation approach tailored for the proton Zero Degree Calorimeter in the\nALICE experiment. Leveraging a Generative Adversarial Network model with\nSelective Diversity Increase loss, we directly simulate calorimeter responses.\nTo enhance its capabilities in modeling a broad range of calorimeter response\nintensities, we expand the SDI-GAN architecture with additional regularization.\nMoreover, to improve the spatial fidelity of the generated data, we introduce\nan auxiliary regressor network. Our method offers a significant speedup when\ncomparing to the traditional Monte-Carlo based approaches.\n", "link": "http://arxiv.org/abs/2406.03263v1", "date": "2024-06-05", "relevancy": 2.0135, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5187}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5108}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Generative%20Models%20for%20Proton%20Zero%20Degree%20Calorimeter%20Simulations%20in%0A%20%20ALICE%2C%20CERN&body=Title%3A%20Deep%20Generative%20Models%20for%20Proton%20Zero%20Degree%20Calorimeter%20Simulations%20in%0A%20%20ALICE%2C%20CERN%0AAuthor%3A%20Patryk%20B%C4%99dkowski%20and%20Jan%20Dubi%C5%84ski%20and%20Kamil%20Deja%20and%20Przemys%C5%82aw%20Rokita%0AAbstract%3A%20%20%20Simulating%20detector%20responses%20is%20a%20crucial%20part%20of%20understanding%20the%0Ainner-workings%20of%20particle%20collisions%20in%20the%20Large%20Hadron%20Collider%20at%20CERN.%20The%0Acurrent%20reliance%20on%20statistical%20Monte-Carlo%20simulations%20strains%20CERN%27s%0Acomputational%20grid%2C%20underscoring%20the%20urgency%20for%20more%20efficient%20alternatives.%0AAddressing%20these%20challenges%2C%20recent%20proposals%20advocate%20for%20generative%20machine%0Alearning%20methods.%20In%20this%20study%2C%20we%20present%20an%20innovative%20deep%20learning%0Asimulation%20approach%20tailored%20for%20the%20proton%20Zero%20Degree%20Calorimeter%20in%20the%0AALICE%20experiment.%20Leveraging%20a%20Generative%20Adversarial%20Network%20model%20with%0ASelective%20Diversity%20Increase%20loss%2C%20we%20directly%20simulate%20calorimeter%20responses.%0ATo%20enhance%20its%20capabilities%20in%20modeling%20a%20broad%20range%20of%20calorimeter%20response%0Aintensities%2C%20we%20expand%20the%20SDI-GAN%20architecture%20with%20additional%20regularization.%0AMoreover%2C%20to%20improve%20the%20spatial%20fidelity%20of%20the%20generated%20data%2C%20we%20introduce%0Aan%20auxiliary%20regressor%20network.%20Our%20method%20offers%20a%20significant%20speedup%20when%0Acomparing%20to%20the%20traditional%20Monte-Carlo%20based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Generative%2520Models%2520for%2520Proton%2520Zero%2520Degree%2520Calorimeter%2520Simulations%2520in%250A%2520%2520ALICE%252C%2520CERN%26entry.906535625%3DPatryk%2520B%25C4%2599dkowski%2520and%2520Jan%2520Dubi%25C5%2584ski%2520and%2520Kamil%2520Deja%2520and%2520Przemys%25C5%2582aw%2520Rokita%26entry.1292438233%3D%2520%2520Simulating%2520detector%2520responses%2520is%2520a%2520crucial%2520part%2520of%2520understanding%2520the%250Ainner-workings%2520of%2520particle%2520collisions%2520in%2520the%2520Large%2520Hadron%2520Collider%2520at%2520CERN.%2520The%250Acurrent%2520reliance%2520on%2520statistical%2520Monte-Carlo%2520simulations%2520strains%2520CERN%2527s%250Acomputational%2520grid%252C%2520underscoring%2520the%2520urgency%2520for%2520more%2520efficient%2520alternatives.%250AAddressing%2520these%2520challenges%252C%2520recent%2520proposals%2520advocate%2520for%2520generative%2520machine%250Alearning%2520methods.%2520In%2520this%2520study%252C%2520we%2520present%2520an%2520innovative%2520deep%2520learning%250Asimulation%2520approach%2520tailored%2520for%2520the%2520proton%2520Zero%2520Degree%2520Calorimeter%2520in%2520the%250AALICE%2520experiment.%2520Leveraging%2520a%2520Generative%2520Adversarial%2520Network%2520model%2520with%250ASelective%2520Diversity%2520Increase%2520loss%252C%2520we%2520directly%2520simulate%2520calorimeter%2520responses.%250ATo%2520enhance%2520its%2520capabilities%2520in%2520modeling%2520a%2520broad%2520range%2520of%2520calorimeter%2520response%250Aintensities%252C%2520we%2520expand%2520the%2520SDI-GAN%2520architecture%2520with%2520additional%2520regularization.%250AMoreover%252C%2520to%2520improve%2520the%2520spatial%2520fidelity%2520of%2520the%2520generated%2520data%252C%2520we%2520introduce%250Aan%2520auxiliary%2520regressor%2520network.%2520Our%2520method%2520offers%2520a%2520significant%2520speedup%2520when%250Acomparing%2520to%2520the%2520traditional%2520Monte-Carlo%2520based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Generative%20Models%20for%20Proton%20Zero%20Degree%20Calorimeter%20Simulations%20in%0A%20%20ALICE%2C%20CERN&entry.906535625=Patryk%20B%C4%99dkowski%20and%20Jan%20Dubi%C5%84ski%20and%20Kamil%20Deja%20and%20Przemys%C5%82aw%20Rokita&entry.1292438233=%20%20Simulating%20detector%20responses%20is%20a%20crucial%20part%20of%20understanding%20the%0Ainner-workings%20of%20particle%20collisions%20in%20the%20Large%20Hadron%20Collider%20at%20CERN.%20The%0Acurrent%20reliance%20on%20statistical%20Monte-Carlo%20simulations%20strains%20CERN%27s%0Acomputational%20grid%2C%20underscoring%20the%20urgency%20for%20more%20efficient%20alternatives.%0AAddressing%20these%20challenges%2C%20recent%20proposals%20advocate%20for%20generative%20machine%0Alearning%20methods.%20In%20this%20study%2C%20we%20present%20an%20innovative%20deep%20learning%0Asimulation%20approach%20tailored%20for%20the%20proton%20Zero%20Degree%20Calorimeter%20in%20the%0AALICE%20experiment.%20Leveraging%20a%20Generative%20Adversarial%20Network%20model%20with%0ASelective%20Diversity%20Increase%20loss%2C%20we%20directly%20simulate%20calorimeter%20responses.%0ATo%20enhance%20its%20capabilities%20in%20modeling%20a%20broad%20range%20of%20calorimeter%20response%0Aintensities%2C%20we%20expand%20the%20SDI-GAN%20architecture%20with%20additional%20regularization.%0AMoreover%2C%20to%20improve%20the%20spatial%20fidelity%20of%20the%20generated%20data%2C%20we%20introduce%0Aan%20auxiliary%20regressor%20network.%20Our%20method%20offers%20a%20significant%20speedup%20when%0Acomparing%20to%20the%20traditional%20Monte-Carlo%20based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03263v1&entry.124074799=Read"},
{"title": "Generalized Source Tracing: Detecting Novel Audio Deepfake Algorithm\n  with Real Emphasis and Fake Dispersion strategy", "author": "Yuankun Xie and Ruibo Fu and Zhengqi Wen and Zhiyong Wang and Xiaopeng Wang and Haonnan Cheng and Long Ye and Jianhua Tao", "abstract": "  With the proliferation of deepfake audio, there is an urgent need to\ninvestigate their attribution. Current source tracing methods can effectively\ndistinguish in-distribution (ID) categories. However, the rapid evolution of\ndeepfake algorithms poses a critical challenge in the accurate identification\nof out-of-distribution (OOD) novel deepfake algorithms. In this paper, we\npropose Real Emphasis and Fake Dispersion (REFD) strategy for audio deepfake\nalgorithm recognition, demonstrating its effectiveness in discriminating ID\nsamples while identifying OOD samples. For effective OOD detection, we first\nexplore current post-hoc OOD methods and propose NSD, a novel OOD approach in\nidentifying novel deepfake algorithms through the similarity consideration of\nboth feature and logits scores. REFD achieves 86.83% F1-score as a single\nsystem in Audio Deepfake Detection Challenge 2023 Track3, showcasing its\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2406.03240v1", "date": "2024-06-05", "relevancy": 2.0118, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5196}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4989}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Source%20Tracing%3A%20Detecting%20Novel%20Audio%20Deepfake%20Algorithm%0A%20%20with%20Real%20Emphasis%20and%20Fake%20Dispersion%20strategy&body=Title%3A%20Generalized%20Source%20Tracing%3A%20Detecting%20Novel%20Audio%20Deepfake%20Algorithm%0A%20%20with%20Real%20Emphasis%20and%20Fake%20Dispersion%20strategy%0AAuthor%3A%20Yuankun%20Xie%20and%20Ruibo%20Fu%20and%20Zhengqi%20Wen%20and%20Zhiyong%20Wang%20and%20Xiaopeng%20Wang%20and%20Haonnan%20Cheng%20and%20Long%20Ye%20and%20Jianhua%20Tao%0AAbstract%3A%20%20%20With%20the%20proliferation%20of%20deepfake%20audio%2C%20there%20is%20an%20urgent%20need%20to%0Ainvestigate%20their%20attribution.%20Current%20source%20tracing%20methods%20can%20effectively%0Adistinguish%20in-distribution%20%28ID%29%20categories.%20However%2C%20the%20rapid%20evolution%20of%0Adeepfake%20algorithms%20poses%20a%20critical%20challenge%20in%20the%20accurate%20identification%0Aof%20out-of-distribution%20%28OOD%29%20novel%20deepfake%20algorithms.%20In%20this%20paper%2C%20we%0Apropose%20Real%20Emphasis%20and%20Fake%20Dispersion%20%28REFD%29%20strategy%20for%20audio%20deepfake%0Aalgorithm%20recognition%2C%20demonstrating%20its%20effectiveness%20in%20discriminating%20ID%0Asamples%20while%20identifying%20OOD%20samples.%20For%20effective%20OOD%20detection%2C%20we%20first%0Aexplore%20current%20post-hoc%20OOD%20methods%20and%20propose%20NSD%2C%20a%20novel%20OOD%20approach%20in%0Aidentifying%20novel%20deepfake%20algorithms%20through%20the%20similarity%20consideration%20of%0Aboth%20feature%20and%20logits%20scores.%20REFD%20achieves%2086.83%25%20F1-score%20as%20a%20single%0Asystem%20in%20Audio%20Deepfake%20Detection%20Challenge%202023%20Track3%2C%20showcasing%20its%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Source%2520Tracing%253A%2520Detecting%2520Novel%2520Audio%2520Deepfake%2520Algorithm%250A%2520%2520with%2520Real%2520Emphasis%2520and%2520Fake%2520Dispersion%2520strategy%26entry.906535625%3DYuankun%2520Xie%2520and%2520Ruibo%2520Fu%2520and%2520Zhengqi%2520Wen%2520and%2520Zhiyong%2520Wang%2520and%2520Xiaopeng%2520Wang%2520and%2520Haonnan%2520Cheng%2520and%2520Long%2520Ye%2520and%2520Jianhua%2520Tao%26entry.1292438233%3D%2520%2520With%2520the%2520proliferation%2520of%2520deepfake%2520audio%252C%2520there%2520is%2520an%2520urgent%2520need%2520to%250Ainvestigate%2520their%2520attribution.%2520Current%2520source%2520tracing%2520methods%2520can%2520effectively%250Adistinguish%2520in-distribution%2520%2528ID%2529%2520categories.%2520However%252C%2520the%2520rapid%2520evolution%2520of%250Adeepfake%2520algorithms%2520poses%2520a%2520critical%2520challenge%2520in%2520the%2520accurate%2520identification%250Aof%2520out-of-distribution%2520%2528OOD%2529%2520novel%2520deepfake%2520algorithms.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Real%2520Emphasis%2520and%2520Fake%2520Dispersion%2520%2528REFD%2529%2520strategy%2520for%2520audio%2520deepfake%250Aalgorithm%2520recognition%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520discriminating%2520ID%250Asamples%2520while%2520identifying%2520OOD%2520samples.%2520For%2520effective%2520OOD%2520detection%252C%2520we%2520first%250Aexplore%2520current%2520post-hoc%2520OOD%2520methods%2520and%2520propose%2520NSD%252C%2520a%2520novel%2520OOD%2520approach%2520in%250Aidentifying%2520novel%2520deepfake%2520algorithms%2520through%2520the%2520similarity%2520consideration%2520of%250Aboth%2520feature%2520and%2520logits%2520scores.%2520REFD%2520achieves%252086.83%2525%2520F1-score%2520as%2520a%2520single%250Asystem%2520in%2520Audio%2520Deepfake%2520Detection%2520Challenge%25202023%2520Track3%252C%2520showcasing%2520its%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Source%20Tracing%3A%20Detecting%20Novel%20Audio%20Deepfake%20Algorithm%0A%20%20with%20Real%20Emphasis%20and%20Fake%20Dispersion%20strategy&entry.906535625=Yuankun%20Xie%20and%20Ruibo%20Fu%20and%20Zhengqi%20Wen%20and%20Zhiyong%20Wang%20and%20Xiaopeng%20Wang%20and%20Haonnan%20Cheng%20and%20Long%20Ye%20and%20Jianhua%20Tao&entry.1292438233=%20%20With%20the%20proliferation%20of%20deepfake%20audio%2C%20there%20is%20an%20urgent%20need%20to%0Ainvestigate%20their%20attribution.%20Current%20source%20tracing%20methods%20can%20effectively%0Adistinguish%20in-distribution%20%28ID%29%20categories.%20However%2C%20the%20rapid%20evolution%20of%0Adeepfake%20algorithms%20poses%20a%20critical%20challenge%20in%20the%20accurate%20identification%0Aof%20out-of-distribution%20%28OOD%29%20novel%20deepfake%20algorithms.%20In%20this%20paper%2C%20we%0Apropose%20Real%20Emphasis%20and%20Fake%20Dispersion%20%28REFD%29%20strategy%20for%20audio%20deepfake%0Aalgorithm%20recognition%2C%20demonstrating%20its%20effectiveness%20in%20discriminating%20ID%0Asamples%20while%20identifying%20OOD%20samples.%20For%20effective%20OOD%20detection%2C%20we%20first%0Aexplore%20current%20post-hoc%20OOD%20methods%20and%20propose%20NSD%2C%20a%20novel%20OOD%20approach%20in%0Aidentifying%20novel%20deepfake%20algorithms%20through%20the%20similarity%20consideration%20of%0Aboth%20feature%20and%20logits%20scores.%20REFD%20achieves%2086.83%25%20F1-score%20as%20a%20single%0Asystem%20in%20Audio%20Deepfake%20Detection%20Challenge%202023%20Track3%2C%20showcasing%20its%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03240v1&entry.124074799=Read"},
{"title": "Wings: Learning Multimodal LLMs without Text-only Forgetting", "author": "Yi-Kai Zhang and Shiyin Lu and Yang Li and Yanqing Ma and Qing-Guo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang and De-Chuan Zhan and Han-Jia Ye", "abstract": "  Multimodal large language models (MLLMs), initiated with a trained LLM, first\nalign images with text and then fine-tune on multimodal mixed inputs. However,\nthe MLLM catastrophically forgets the text-only instructions, which do not\ninclude images and can be addressed within the initial LLM. In this paper, we\npresent Wings, a novel MLLM that excels in both text-only dialogues and\nmultimodal comprehension. Analyzing MLLM attention in multimodal instructions\nreveals that text-only forgetting is related to the attention shifts from\npre-image to post-image text. From that, we construct extra modules that act as\nthe boosted learner to compensate for the attention shift. The complementary\nvisual and textual learners, like \"wings\" on either side, are connected in\nparallel within each layer's attention block. Initially, image and text inputs\nare aligned with visual learners operating alongside the main attention,\nbalancing focus on visual elements. Textual learners are later collaboratively\nintegrated with attention-based routing to blend the outputs of the visual and\ntextual learners. We design the Low-Rank Residual Attention (LoRRA) to\nguarantee high efficiency for learners. Our experimental results demonstrate\nthat Wings outperforms equally-scaled MLLMs in both text-only and visual\nquestion-answering tasks. On a newly constructed Interleaved Image-Text (IIT)\nbenchmark, Wings exhibits superior performance from text-only-rich to\nmultimodal-rich question-answering tasks.\n", "link": "http://arxiv.org/abs/2406.03496v1", "date": "2024-06-05", "relevancy": 2.0102, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5374}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5036}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wings%3A%20Learning%20Multimodal%20LLMs%20without%20Text-only%20Forgetting&body=Title%3A%20Wings%3A%20Learning%20Multimodal%20LLMs%20without%20Text-only%20Forgetting%0AAuthor%3A%20Yi-Kai%20Zhang%20and%20Shiyin%20Lu%20and%20Yang%20Li%20and%20Yanqing%20Ma%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20De-Chuan%20Zhan%20and%20Han-Jia%20Ye%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%2C%20initiated%20with%20a%20trained%20LLM%2C%20first%0Aalign%20images%20with%20text%20and%20then%20fine-tune%20on%20multimodal%20mixed%20inputs.%20However%2C%0Athe%20MLLM%20catastrophically%20forgets%20the%20text-only%20instructions%2C%20which%20do%20not%0Ainclude%20images%20and%20can%20be%20addressed%20within%20the%20initial%20LLM.%20In%20this%20paper%2C%20we%0Apresent%20Wings%2C%20a%20novel%20MLLM%20that%20excels%20in%20both%20text-only%20dialogues%20and%0Amultimodal%20comprehension.%20Analyzing%20MLLM%20attention%20in%20multimodal%20instructions%0Areveals%20that%20text-only%20forgetting%20is%20related%20to%20the%20attention%20shifts%20from%0Apre-image%20to%20post-image%20text.%20From%20that%2C%20we%20construct%20extra%20modules%20that%20act%20as%0Athe%20boosted%20learner%20to%20compensate%20for%20the%20attention%20shift.%20The%20complementary%0Avisual%20and%20textual%20learners%2C%20like%20%22wings%22%20on%20either%20side%2C%20are%20connected%20in%0Aparallel%20within%20each%20layer%27s%20attention%20block.%20Initially%2C%20image%20and%20text%20inputs%0Aare%20aligned%20with%20visual%20learners%20operating%20alongside%20the%20main%20attention%2C%0Abalancing%20focus%20on%20visual%20elements.%20Textual%20learners%20are%20later%20collaboratively%0Aintegrated%20with%20attention-based%20routing%20to%20blend%20the%20outputs%20of%20the%20visual%20and%0Atextual%20learners.%20We%20design%20the%20Low-Rank%20Residual%20Attention%20%28LoRRA%29%20to%0Aguarantee%20high%20efficiency%20for%20learners.%20Our%20experimental%20results%20demonstrate%0Athat%20Wings%20outperforms%20equally-scaled%20MLLMs%20in%20both%20text-only%20and%20visual%0Aquestion-answering%20tasks.%20On%20a%20newly%20constructed%20Interleaved%20Image-Text%20%28IIT%29%0Abenchmark%2C%20Wings%20exhibits%20superior%20performance%20from%20text-only-rich%20to%0Amultimodal-rich%20question-answering%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWings%253A%2520Learning%2520Multimodal%2520LLMs%2520without%2520Text-only%2520Forgetting%26entry.906535625%3DYi-Kai%2520Zhang%2520and%2520Shiyin%2520Lu%2520and%2520Yang%2520Li%2520and%2520Yanqing%2520Ma%2520and%2520Qing-Guo%2520Chen%2520and%2520Zhao%2520Xu%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%2520and%2520De-Chuan%2520Zhan%2520and%2520Han-Jia%2520Ye%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520initiated%2520with%2520a%2520trained%2520LLM%252C%2520first%250Aalign%2520images%2520with%2520text%2520and%2520then%2520fine-tune%2520on%2520multimodal%2520mixed%2520inputs.%2520However%252C%250Athe%2520MLLM%2520catastrophically%2520forgets%2520the%2520text-only%2520instructions%252C%2520which%2520do%2520not%250Ainclude%2520images%2520and%2520can%2520be%2520addressed%2520within%2520the%2520initial%2520LLM.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520Wings%252C%2520a%2520novel%2520MLLM%2520that%2520excels%2520in%2520both%2520text-only%2520dialogues%2520and%250Amultimodal%2520comprehension.%2520Analyzing%2520MLLM%2520attention%2520in%2520multimodal%2520instructions%250Areveals%2520that%2520text-only%2520forgetting%2520is%2520related%2520to%2520the%2520attention%2520shifts%2520from%250Apre-image%2520to%2520post-image%2520text.%2520From%2520that%252C%2520we%2520construct%2520extra%2520modules%2520that%2520act%2520as%250Athe%2520boosted%2520learner%2520to%2520compensate%2520for%2520the%2520attention%2520shift.%2520The%2520complementary%250Avisual%2520and%2520textual%2520learners%252C%2520like%2520%2522wings%2522%2520on%2520either%2520side%252C%2520are%2520connected%2520in%250Aparallel%2520within%2520each%2520layer%2527s%2520attention%2520block.%2520Initially%252C%2520image%2520and%2520text%2520inputs%250Aare%2520aligned%2520with%2520visual%2520learners%2520operating%2520alongside%2520the%2520main%2520attention%252C%250Abalancing%2520focus%2520on%2520visual%2520elements.%2520Textual%2520learners%2520are%2520later%2520collaboratively%250Aintegrated%2520with%2520attention-based%2520routing%2520to%2520blend%2520the%2520outputs%2520of%2520the%2520visual%2520and%250Atextual%2520learners.%2520We%2520design%2520the%2520Low-Rank%2520Residual%2520Attention%2520%2528LoRRA%2529%2520to%250Aguarantee%2520high%2520efficiency%2520for%2520learners.%2520Our%2520experimental%2520results%2520demonstrate%250Athat%2520Wings%2520outperforms%2520equally-scaled%2520MLLMs%2520in%2520both%2520text-only%2520and%2520visual%250Aquestion-answering%2520tasks.%2520On%2520a%2520newly%2520constructed%2520Interleaved%2520Image-Text%2520%2528IIT%2529%250Abenchmark%252C%2520Wings%2520exhibits%2520superior%2520performance%2520from%2520text-only-rich%2520to%250Amultimodal-rich%2520question-answering%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wings%3A%20Learning%20Multimodal%20LLMs%20without%20Text-only%20Forgetting&entry.906535625=Yi-Kai%20Zhang%20and%20Shiyin%20Lu%20and%20Yang%20Li%20and%20Yanqing%20Ma%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20De-Chuan%20Zhan%20and%20Han-Jia%20Ye&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%2C%20initiated%20with%20a%20trained%20LLM%2C%20first%0Aalign%20images%20with%20text%20and%20then%20fine-tune%20on%20multimodal%20mixed%20inputs.%20However%2C%0Athe%20MLLM%20catastrophically%20forgets%20the%20text-only%20instructions%2C%20which%20do%20not%0Ainclude%20images%20and%20can%20be%20addressed%20within%20the%20initial%20LLM.%20In%20this%20paper%2C%20we%0Apresent%20Wings%2C%20a%20novel%20MLLM%20that%20excels%20in%20both%20text-only%20dialogues%20and%0Amultimodal%20comprehension.%20Analyzing%20MLLM%20attention%20in%20multimodal%20instructions%0Areveals%20that%20text-only%20forgetting%20is%20related%20to%20the%20attention%20shifts%20from%0Apre-image%20to%20post-image%20text.%20From%20that%2C%20we%20construct%20extra%20modules%20that%20act%20as%0Athe%20boosted%20learner%20to%20compensate%20for%20the%20attention%20shift.%20The%20complementary%0Avisual%20and%20textual%20learners%2C%20like%20%22wings%22%20on%20either%20side%2C%20are%20connected%20in%0Aparallel%20within%20each%20layer%27s%20attention%20block.%20Initially%2C%20image%20and%20text%20inputs%0Aare%20aligned%20with%20visual%20learners%20operating%20alongside%20the%20main%20attention%2C%0Abalancing%20focus%20on%20visual%20elements.%20Textual%20learners%20are%20later%20collaboratively%0Aintegrated%20with%20attention-based%20routing%20to%20blend%20the%20outputs%20of%20the%20visual%20and%0Atextual%20learners.%20We%20design%20the%20Low-Rank%20Residual%20Attention%20%28LoRRA%29%20to%0Aguarantee%20high%20efficiency%20for%20learners.%20Our%20experimental%20results%20demonstrate%0Athat%20Wings%20outperforms%20equally-scaled%20MLLMs%20in%20both%20text-only%20and%20visual%0Aquestion-answering%20tasks.%20On%20a%20newly%20constructed%20Interleaved%20Image-Text%20%28IIT%29%0Abenchmark%2C%20Wings%20exhibits%20superior%20performance%20from%20text-only-rich%20to%0Amultimodal-rich%20question-answering%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03496v1&entry.124074799=Read"},
{"title": "Locality-Sensitive Hashing-Based Efficient Point Transformer with\n  Applications in High-Energy Physics", "author": "Siqi Miao and Zhiyuan Lu and Mia Liu and Javier Duarte and Pan Li", "abstract": "  This study introduces a novel transformer model optimized for large-scale\npoint cloud processing in scientific domains such as high-energy physics (HEP)\nand astrophysics. Addressing the limitations of graph neural networks and\nstandard transformers, our model integrates local inductive bias and achieves\nnear-linear complexity with hardware-friendly regular operations. One\ncontribution of this work is the quantitative analysis of the error-complexity\ntradeoff of various sparsification techniques for building efficient\ntransformers. Our findings highlight the superiority of using\nlocality-sensitive hashing (LSH), especially OR & AND-construction LSH, in\nkernel approximation for large-scale point cloud data with local inductive\nbias. Based on this finding, we propose LSH-based Efficient Point Transformer\n(HEPT), which combines E$^2$LSH with OR & AND constructions and is built upon\nregular computations. HEPT demonstrates remarkable performance on two critical\nyet time-consuming HEP tasks, significantly outperforming existing GNNs and\ntransformers in accuracy and computational speed, marking a significant\nadvancement in geometric deep learning and large-scale scientific data\nprocessing. Our code is available at https://github.com/Graph-COM/HEPT.\n", "link": "http://arxiv.org/abs/2402.12535v2", "date": "2024-06-05", "relevancy": 2.0072, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5708}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4889}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locality-Sensitive%20Hashing-Based%20Efficient%20Point%20Transformer%20with%0A%20%20Applications%20in%20High-Energy%20Physics&body=Title%3A%20Locality-Sensitive%20Hashing-Based%20Efficient%20Point%20Transformer%20with%0A%20%20Applications%20in%20High-Energy%20Physics%0AAuthor%3A%20Siqi%20Miao%20and%20Zhiyuan%20Lu%20and%20Mia%20Liu%20and%20Javier%20Duarte%20and%20Pan%20Li%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20novel%20transformer%20model%20optimized%20for%20large-scale%0Apoint%20cloud%20processing%20in%20scientific%20domains%20such%20as%20high-energy%20physics%20%28HEP%29%0Aand%20astrophysics.%20Addressing%20the%20limitations%20of%20graph%20neural%20networks%20and%0Astandard%20transformers%2C%20our%20model%20integrates%20local%20inductive%20bias%20and%20achieves%0Anear-linear%20complexity%20with%20hardware-friendly%20regular%20operations.%20One%0Acontribution%20of%20this%20work%20is%20the%20quantitative%20analysis%20of%20the%20error-complexity%0Atradeoff%20of%20various%20sparsification%20techniques%20for%20building%20efficient%0Atransformers.%20Our%20findings%20highlight%20the%20superiority%20of%20using%0Alocality-sensitive%20hashing%20%28LSH%29%2C%20especially%20OR%20%26%20AND-construction%20LSH%2C%20in%0Akernel%20approximation%20for%20large-scale%20point%20cloud%20data%20with%20local%20inductive%0Abias.%20Based%20on%20this%20finding%2C%20we%20propose%20LSH-based%20Efficient%20Point%20Transformer%0A%28HEPT%29%2C%20which%20combines%20E%24%5E2%24LSH%20with%20OR%20%26%20AND%20constructions%20and%20is%20built%20upon%0Aregular%20computations.%20HEPT%20demonstrates%20remarkable%20performance%20on%20two%20critical%0Ayet%20time-consuming%20HEP%20tasks%2C%20significantly%20outperforming%20existing%20GNNs%20and%0Atransformers%20in%20accuracy%20and%20computational%20speed%2C%20marking%20a%20significant%0Aadvancement%20in%20geometric%20deep%20learning%20and%20large-scale%20scientific%20data%0Aprocessing.%20Our%20code%20is%20available%20at%20https%3A//github.com/Graph-COM/HEPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12535v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocality-Sensitive%2520Hashing-Based%2520Efficient%2520Point%2520Transformer%2520with%250A%2520%2520Applications%2520in%2520High-Energy%2520Physics%26entry.906535625%3DSiqi%2520Miao%2520and%2520Zhiyuan%2520Lu%2520and%2520Mia%2520Liu%2520and%2520Javier%2520Duarte%2520and%2520Pan%2520Li%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520novel%2520transformer%2520model%2520optimized%2520for%2520large-scale%250Apoint%2520cloud%2520processing%2520in%2520scientific%2520domains%2520such%2520as%2520high-energy%2520physics%2520%2528HEP%2529%250Aand%2520astrophysics.%2520Addressing%2520the%2520limitations%2520of%2520graph%2520neural%2520networks%2520and%250Astandard%2520transformers%252C%2520our%2520model%2520integrates%2520local%2520inductive%2520bias%2520and%2520achieves%250Anear-linear%2520complexity%2520with%2520hardware-friendly%2520regular%2520operations.%2520One%250Acontribution%2520of%2520this%2520work%2520is%2520the%2520quantitative%2520analysis%2520of%2520the%2520error-complexity%250Atradeoff%2520of%2520various%2520sparsification%2520techniques%2520for%2520building%2520efficient%250Atransformers.%2520Our%2520findings%2520highlight%2520the%2520superiority%2520of%2520using%250Alocality-sensitive%2520hashing%2520%2528LSH%2529%252C%2520especially%2520OR%2520%2526%2520AND-construction%2520LSH%252C%2520in%250Akernel%2520approximation%2520for%2520large-scale%2520point%2520cloud%2520data%2520with%2520local%2520inductive%250Abias.%2520Based%2520on%2520this%2520finding%252C%2520we%2520propose%2520LSH-based%2520Efficient%2520Point%2520Transformer%250A%2528HEPT%2529%252C%2520which%2520combines%2520E%2524%255E2%2524LSH%2520with%2520OR%2520%2526%2520AND%2520constructions%2520and%2520is%2520built%2520upon%250Aregular%2520computations.%2520HEPT%2520demonstrates%2520remarkable%2520performance%2520on%2520two%2520critical%250Ayet%2520time-consuming%2520HEP%2520tasks%252C%2520significantly%2520outperforming%2520existing%2520GNNs%2520and%250Atransformers%2520in%2520accuracy%2520and%2520computational%2520speed%252C%2520marking%2520a%2520significant%250Aadvancement%2520in%2520geometric%2520deep%2520learning%2520and%2520large-scale%2520scientific%2520data%250Aprocessing.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Graph-COM/HEPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12535v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locality-Sensitive%20Hashing-Based%20Efficient%20Point%20Transformer%20with%0A%20%20Applications%20in%20High-Energy%20Physics&entry.906535625=Siqi%20Miao%20and%20Zhiyuan%20Lu%20and%20Mia%20Liu%20and%20Javier%20Duarte%20and%20Pan%20Li&entry.1292438233=%20%20This%20study%20introduces%20a%20novel%20transformer%20model%20optimized%20for%20large-scale%0Apoint%20cloud%20processing%20in%20scientific%20domains%20such%20as%20high-energy%20physics%20%28HEP%29%0Aand%20astrophysics.%20Addressing%20the%20limitations%20of%20graph%20neural%20networks%20and%0Astandard%20transformers%2C%20our%20model%20integrates%20local%20inductive%20bias%20and%20achieves%0Anear-linear%20complexity%20with%20hardware-friendly%20regular%20operations.%20One%0Acontribution%20of%20this%20work%20is%20the%20quantitative%20analysis%20of%20the%20error-complexity%0Atradeoff%20of%20various%20sparsification%20techniques%20for%20building%20efficient%0Atransformers.%20Our%20findings%20highlight%20the%20superiority%20of%20using%0Alocality-sensitive%20hashing%20%28LSH%29%2C%20especially%20OR%20%26%20AND-construction%20LSH%2C%20in%0Akernel%20approximation%20for%20large-scale%20point%20cloud%20data%20with%20local%20inductive%0Abias.%20Based%20on%20this%20finding%2C%20we%20propose%20LSH-based%20Efficient%20Point%20Transformer%0A%28HEPT%29%2C%20which%20combines%20E%24%5E2%24LSH%20with%20OR%20%26%20AND%20constructions%20and%20is%20built%20upon%0Aregular%20computations.%20HEPT%20demonstrates%20remarkable%20performance%20on%20two%20critical%0Ayet%20time-consuming%20HEP%20tasks%2C%20significantly%20outperforming%20existing%20GNNs%20and%0Atransformers%20in%20accuracy%20and%20computational%20speed%2C%20marking%20a%20significant%0Aadvancement%20in%20geometric%20deep%20learning%20and%20large-scale%20scientific%20data%0Aprocessing.%20Our%20code%20is%20available%20at%20https%3A//github.com/Graph-COM/HEPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12535v2&entry.124074799=Read"},
{"title": "CR-UTP: Certified Robustness against Universal Text Perturbations on\n  Large Language Models", "author": "Qian Lou and Xin Liang and Jiaqi Xue and Yancheng Zhang and Rui Xie and Mengxin Zheng", "abstract": "  It is imperative to ensure the stability of every prediction made by a\nlanguage model; that is, a language's prediction should remain consistent\ndespite minor input variations, like word substitutions. In this paper, we\ninvestigate the problem of certifying a language model's robustness against\nUniversal Text Perturbations (UTPs), which have been widely used in universal\nadversarial attacks and backdoor attacks. Existing certified robustness based\non random smoothing has shown considerable promise in certifying the\ninput-specific text perturbations (ISTPs), operating under the assumption that\nany random alteration of a sample's clean or adversarial words would negate the\nimpact of sample-wise perturbations. However, with UTPs, masking only the\nadversarial words can eliminate the attack. A naive method is to simply\nincrease the masking ratio and the likelihood of masking attack tokens, but it\nleads to a significant reduction in both certified accuracy and the certified\nradius due to input corruption by extensive masking. To solve this challenge,\nwe introduce a novel approach, the superior prompt search method, designed to\nidentify a superior prompt that maintains higher certified accuracy under\nextensive masking. Additionally, we theoretically motivate why ensembles are a\nparticularly suitable choice as base prompts for random smoothing. The method\nis denoted by superior prompt ensembling technique. We also empirically confirm\nthis technique, obtaining state-of-the-art results in multiple settings. These\nmethodologies, for the first time, enable high certified accuracy against both\nUTPs and ISTPs. The source code of CR-UTP is available at \\url\n{https://github.com/UCFML-Research/CR-UTP}.\n", "link": "http://arxiv.org/abs/2406.01873v2", "date": "2024-06-05", "relevancy": 1.9949, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5191}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5124}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CR-UTP%3A%20Certified%20Robustness%20against%20Universal%20Text%20Perturbations%20on%0A%20%20Large%20Language%20Models&body=Title%3A%20CR-UTP%3A%20Certified%20Robustness%20against%20Universal%20Text%20Perturbations%20on%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Qian%20Lou%20and%20Xin%20Liang%20and%20Jiaqi%20Xue%20and%20Yancheng%20Zhang%20and%20Rui%20Xie%20and%20Mengxin%20Zheng%0AAbstract%3A%20%20%20It%20is%20imperative%20to%20ensure%20the%20stability%20of%20every%20prediction%20made%20by%20a%0Alanguage%20model%3B%20that%20is%2C%20a%20language%27s%20prediction%20should%20remain%20consistent%0Adespite%20minor%20input%20variations%2C%20like%20word%20substitutions.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20problem%20of%20certifying%20a%20language%20model%27s%20robustness%20against%0AUniversal%20Text%20Perturbations%20%28UTPs%29%2C%20which%20have%20been%20widely%20used%20in%20universal%0Aadversarial%20attacks%20and%20backdoor%20attacks.%20Existing%20certified%20robustness%20based%0Aon%20random%20smoothing%20has%20shown%20considerable%20promise%20in%20certifying%20the%0Ainput-specific%20text%20perturbations%20%28ISTPs%29%2C%20operating%20under%20the%20assumption%20that%0Aany%20random%20alteration%20of%20a%20sample%27s%20clean%20or%20adversarial%20words%20would%20negate%20the%0Aimpact%20of%20sample-wise%20perturbations.%20However%2C%20with%20UTPs%2C%20masking%20only%20the%0Aadversarial%20words%20can%20eliminate%20the%20attack.%20A%20naive%20method%20is%20to%20simply%0Aincrease%20the%20masking%20ratio%20and%20the%20likelihood%20of%20masking%20attack%20tokens%2C%20but%20it%0Aleads%20to%20a%20significant%20reduction%20in%20both%20certified%20accuracy%20and%20the%20certified%0Aradius%20due%20to%20input%20corruption%20by%20extensive%20masking.%20To%20solve%20this%20challenge%2C%0Awe%20introduce%20a%20novel%20approach%2C%20the%20superior%20prompt%20search%20method%2C%20designed%20to%0Aidentify%20a%20superior%20prompt%20that%20maintains%20higher%20certified%20accuracy%20under%0Aextensive%20masking.%20Additionally%2C%20we%20theoretically%20motivate%20why%20ensembles%20are%20a%0Aparticularly%20suitable%20choice%20as%20base%20prompts%20for%20random%20smoothing.%20The%20method%0Ais%20denoted%20by%20superior%20prompt%20ensembling%20technique.%20We%20also%20empirically%20confirm%0Athis%20technique%2C%20obtaining%20state-of-the-art%20results%20in%20multiple%20settings.%20These%0Amethodologies%2C%20for%20the%20first%20time%2C%20enable%20high%20certified%20accuracy%20against%20both%0AUTPs%20and%20ISTPs.%20The%20source%20code%20of%20CR-UTP%20is%20available%20at%20%5Curl%0A%7Bhttps%3A//github.com/UCFML-Research/CR-UTP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01873v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCR-UTP%253A%2520Certified%2520Robustness%2520against%2520Universal%2520Text%2520Perturbations%2520on%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DQian%2520Lou%2520and%2520Xin%2520Liang%2520and%2520Jiaqi%2520Xue%2520and%2520Yancheng%2520Zhang%2520and%2520Rui%2520Xie%2520and%2520Mengxin%2520Zheng%26entry.1292438233%3D%2520%2520It%2520is%2520imperative%2520to%2520ensure%2520the%2520stability%2520of%2520every%2520prediction%2520made%2520by%2520a%250Alanguage%2520model%253B%2520that%2520is%252C%2520a%2520language%2527s%2520prediction%2520should%2520remain%2520consistent%250Adespite%2520minor%2520input%2520variations%252C%2520like%2520word%2520substitutions.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520the%2520problem%2520of%2520certifying%2520a%2520language%2520model%2527s%2520robustness%2520against%250AUniversal%2520Text%2520Perturbations%2520%2528UTPs%2529%252C%2520which%2520have%2520been%2520widely%2520used%2520in%2520universal%250Aadversarial%2520attacks%2520and%2520backdoor%2520attacks.%2520Existing%2520certified%2520robustness%2520based%250Aon%2520random%2520smoothing%2520has%2520shown%2520considerable%2520promise%2520in%2520certifying%2520the%250Ainput-specific%2520text%2520perturbations%2520%2528ISTPs%2529%252C%2520operating%2520under%2520the%2520assumption%2520that%250Aany%2520random%2520alteration%2520of%2520a%2520sample%2527s%2520clean%2520or%2520adversarial%2520words%2520would%2520negate%2520the%250Aimpact%2520of%2520sample-wise%2520perturbations.%2520However%252C%2520with%2520UTPs%252C%2520masking%2520only%2520the%250Aadversarial%2520words%2520can%2520eliminate%2520the%2520attack.%2520A%2520naive%2520method%2520is%2520to%2520simply%250Aincrease%2520the%2520masking%2520ratio%2520and%2520the%2520likelihood%2520of%2520masking%2520attack%2520tokens%252C%2520but%2520it%250Aleads%2520to%2520a%2520significant%2520reduction%2520in%2520both%2520certified%2520accuracy%2520and%2520the%2520certified%250Aradius%2520due%2520to%2520input%2520corruption%2520by%2520extensive%2520masking.%2520To%2520solve%2520this%2520challenge%252C%250Awe%2520introduce%2520a%2520novel%2520approach%252C%2520the%2520superior%2520prompt%2520search%2520method%252C%2520designed%2520to%250Aidentify%2520a%2520superior%2520prompt%2520that%2520maintains%2520higher%2520certified%2520accuracy%2520under%250Aextensive%2520masking.%2520Additionally%252C%2520we%2520theoretically%2520motivate%2520why%2520ensembles%2520are%2520a%250Aparticularly%2520suitable%2520choice%2520as%2520base%2520prompts%2520for%2520random%2520smoothing.%2520The%2520method%250Ais%2520denoted%2520by%2520superior%2520prompt%2520ensembling%2520technique.%2520We%2520also%2520empirically%2520confirm%250Athis%2520technique%252C%2520obtaining%2520state-of-the-art%2520results%2520in%2520multiple%2520settings.%2520These%250Amethodologies%252C%2520for%2520the%2520first%2520time%252C%2520enable%2520high%2520certified%2520accuracy%2520against%2520both%250AUTPs%2520and%2520ISTPs.%2520The%2520source%2520code%2520of%2520CR-UTP%2520is%2520available%2520at%2520%255Curl%250A%257Bhttps%253A//github.com/UCFML-Research/CR-UTP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01873v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CR-UTP%3A%20Certified%20Robustness%20against%20Universal%20Text%20Perturbations%20on%0A%20%20Large%20Language%20Models&entry.906535625=Qian%20Lou%20and%20Xin%20Liang%20and%20Jiaqi%20Xue%20and%20Yancheng%20Zhang%20and%20Rui%20Xie%20and%20Mengxin%20Zheng&entry.1292438233=%20%20It%20is%20imperative%20to%20ensure%20the%20stability%20of%20every%20prediction%20made%20by%20a%0Alanguage%20model%3B%20that%20is%2C%20a%20language%27s%20prediction%20should%20remain%20consistent%0Adespite%20minor%20input%20variations%2C%20like%20word%20substitutions.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20problem%20of%20certifying%20a%20language%20model%27s%20robustness%20against%0AUniversal%20Text%20Perturbations%20%28UTPs%29%2C%20which%20have%20been%20widely%20used%20in%20universal%0Aadversarial%20attacks%20and%20backdoor%20attacks.%20Existing%20certified%20robustness%20based%0Aon%20random%20smoothing%20has%20shown%20considerable%20promise%20in%20certifying%20the%0Ainput-specific%20text%20perturbations%20%28ISTPs%29%2C%20operating%20under%20the%20assumption%20that%0Aany%20random%20alteration%20of%20a%20sample%27s%20clean%20or%20adversarial%20words%20would%20negate%20the%0Aimpact%20of%20sample-wise%20perturbations.%20However%2C%20with%20UTPs%2C%20masking%20only%20the%0Aadversarial%20words%20can%20eliminate%20the%20attack.%20A%20naive%20method%20is%20to%20simply%0Aincrease%20the%20masking%20ratio%20and%20the%20likelihood%20of%20masking%20attack%20tokens%2C%20but%20it%0Aleads%20to%20a%20significant%20reduction%20in%20both%20certified%20accuracy%20and%20the%20certified%0Aradius%20due%20to%20input%20corruption%20by%20extensive%20masking.%20To%20solve%20this%20challenge%2C%0Awe%20introduce%20a%20novel%20approach%2C%20the%20superior%20prompt%20search%20method%2C%20designed%20to%0Aidentify%20a%20superior%20prompt%20that%20maintains%20higher%20certified%20accuracy%20under%0Aextensive%20masking.%20Additionally%2C%20we%20theoretically%20motivate%20why%20ensembles%20are%20a%0Aparticularly%20suitable%20choice%20as%20base%20prompts%20for%20random%20smoothing.%20The%20method%0Ais%20denoted%20by%20superior%20prompt%20ensembling%20technique.%20We%20also%20empirically%20confirm%0Athis%20technique%2C%20obtaining%20state-of-the-art%20results%20in%20multiple%20settings.%20These%0Amethodologies%2C%20for%20the%20first%20time%2C%20enable%20high%20certified%20accuracy%20against%20both%0AUTPs%20and%20ISTPs.%20The%20source%20code%20of%20CR-UTP%20is%20available%20at%20%5Curl%0A%7Bhttps%3A//github.com/UCFML-Research/CR-UTP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01873v2&entry.124074799=Read"},
{"title": "Learning Long Range Dependencies on Graphs via Random Walks", "author": "Dexiong Chen and Till Hendrik Schulz and Karsten Borgwardt", "abstract": "  Message-passing graph neural networks (GNNs), while excelling at capturing\nlocal relationships, often struggle with long-range dependencies on graphs.\nConversely, graph transformers (GTs) enable information exchange between all\nnodes but oversimplify the graph structure by treating them as a set of\nfixed-length vectors. This work proposes a novel architecture, NeuralWalker,\nthat overcomes the limitations of both methods by combining random walks with\nmessage passing. NeuralWalker achieves this by treating random walks as\nsequences, allowing for the application of recent advances in sequence models\nin order to capture long-range dependencies within these walks. Based on this\nconcept, we propose a framework that offers (1) more expressive graph\nrepresentations through random walk sequences, (2) the ability to utilize any\nsequence model for capturing long-range dependencies, and (3) the flexibility\nby integrating various GNN and GT architectures. Our experimental evaluations\ndemonstrate that NeuralWalker achieves significant performance improvements on\n19 graph and node benchmark datasets, notably outperforming existing methods by\nup to 13% on the PascalVoc-SP and COCO-SP datasets. Code is available at\nhttps://github.com/BorgwardtLab/NeuralWalker.\n", "link": "http://arxiv.org/abs/2406.03386v1", "date": "2024-06-05", "relevancy": 1.9931, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5311}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4966}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Long%20Range%20Dependencies%20on%20Graphs%20via%20Random%20Walks&body=Title%3A%20Learning%20Long%20Range%20Dependencies%20on%20Graphs%20via%20Random%20Walks%0AAuthor%3A%20Dexiong%20Chen%20and%20Till%20Hendrik%20Schulz%20and%20Karsten%20Borgwardt%0AAbstract%3A%20%20%20Message-passing%20graph%20neural%20networks%20%28GNNs%29%2C%20while%20excelling%20at%20capturing%0Alocal%20relationships%2C%20often%20struggle%20with%20long-range%20dependencies%20on%20graphs.%0AConversely%2C%20graph%20transformers%20%28GTs%29%20enable%20information%20exchange%20between%20all%0Anodes%20but%20oversimplify%20the%20graph%20structure%20by%20treating%20them%20as%20a%20set%20of%0Afixed-length%20vectors.%20This%20work%20proposes%20a%20novel%20architecture%2C%20NeuralWalker%2C%0Athat%20overcomes%20the%20limitations%20of%20both%20methods%20by%20combining%20random%20walks%20with%0Amessage%20passing.%20NeuralWalker%20achieves%20this%20by%20treating%20random%20walks%20as%0Asequences%2C%20allowing%20for%20the%20application%20of%20recent%20advances%20in%20sequence%20models%0Ain%20order%20to%20capture%20long-range%20dependencies%20within%20these%20walks.%20Based%20on%20this%0Aconcept%2C%20we%20propose%20a%20framework%20that%20offers%20%281%29%20more%20expressive%20graph%0Arepresentations%20through%20random%20walk%20sequences%2C%20%282%29%20the%20ability%20to%20utilize%20any%0Asequence%20model%20for%20capturing%20long-range%20dependencies%2C%20and%20%283%29%20the%20flexibility%0Aby%20integrating%20various%20GNN%20and%20GT%20architectures.%20Our%20experimental%20evaluations%0Ademonstrate%20that%20NeuralWalker%20achieves%20significant%20performance%20improvements%20on%0A19%20graph%20and%20node%20benchmark%20datasets%2C%20notably%20outperforming%20existing%20methods%20by%0Aup%20to%2013%25%20on%20the%20PascalVoc-SP%20and%20COCO-SP%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/BorgwardtLab/NeuralWalker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Long%2520Range%2520Dependencies%2520on%2520Graphs%2520via%2520Random%2520Walks%26entry.906535625%3DDexiong%2520Chen%2520and%2520Till%2520Hendrik%2520Schulz%2520and%2520Karsten%2520Borgwardt%26entry.1292438233%3D%2520%2520Message-passing%2520graph%2520neural%2520networks%2520%2528GNNs%2529%252C%2520while%2520excelling%2520at%2520capturing%250Alocal%2520relationships%252C%2520often%2520struggle%2520with%2520long-range%2520dependencies%2520on%2520graphs.%250AConversely%252C%2520graph%2520transformers%2520%2528GTs%2529%2520enable%2520information%2520exchange%2520between%2520all%250Anodes%2520but%2520oversimplify%2520the%2520graph%2520structure%2520by%2520treating%2520them%2520as%2520a%2520set%2520of%250Afixed-length%2520vectors.%2520This%2520work%2520proposes%2520a%2520novel%2520architecture%252C%2520NeuralWalker%252C%250Athat%2520overcomes%2520the%2520limitations%2520of%2520both%2520methods%2520by%2520combining%2520random%2520walks%2520with%250Amessage%2520passing.%2520NeuralWalker%2520achieves%2520this%2520by%2520treating%2520random%2520walks%2520as%250Asequences%252C%2520allowing%2520for%2520the%2520application%2520of%2520recent%2520advances%2520in%2520sequence%2520models%250Ain%2520order%2520to%2520capture%2520long-range%2520dependencies%2520within%2520these%2520walks.%2520Based%2520on%2520this%250Aconcept%252C%2520we%2520propose%2520a%2520framework%2520that%2520offers%2520%25281%2529%2520more%2520expressive%2520graph%250Arepresentations%2520through%2520random%2520walk%2520sequences%252C%2520%25282%2529%2520the%2520ability%2520to%2520utilize%2520any%250Asequence%2520model%2520for%2520capturing%2520long-range%2520dependencies%252C%2520and%2520%25283%2529%2520the%2520flexibility%250Aby%2520integrating%2520various%2520GNN%2520and%2520GT%2520architectures.%2520Our%2520experimental%2520evaluations%250Ademonstrate%2520that%2520NeuralWalker%2520achieves%2520significant%2520performance%2520improvements%2520on%250A19%2520graph%2520and%2520node%2520benchmark%2520datasets%252C%2520notably%2520outperforming%2520existing%2520methods%2520by%250Aup%2520to%252013%2525%2520on%2520the%2520PascalVoc-SP%2520and%2520COCO-SP%2520datasets.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/BorgwardtLab/NeuralWalker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Long%20Range%20Dependencies%20on%20Graphs%20via%20Random%20Walks&entry.906535625=Dexiong%20Chen%20and%20Till%20Hendrik%20Schulz%20and%20Karsten%20Borgwardt&entry.1292438233=%20%20Message-passing%20graph%20neural%20networks%20%28GNNs%29%2C%20while%20excelling%20at%20capturing%0Alocal%20relationships%2C%20often%20struggle%20with%20long-range%20dependencies%20on%20graphs.%0AConversely%2C%20graph%20transformers%20%28GTs%29%20enable%20information%20exchange%20between%20all%0Anodes%20but%20oversimplify%20the%20graph%20structure%20by%20treating%20them%20as%20a%20set%20of%0Afixed-length%20vectors.%20This%20work%20proposes%20a%20novel%20architecture%2C%20NeuralWalker%2C%0Athat%20overcomes%20the%20limitations%20of%20both%20methods%20by%20combining%20random%20walks%20with%0Amessage%20passing.%20NeuralWalker%20achieves%20this%20by%20treating%20random%20walks%20as%0Asequences%2C%20allowing%20for%20the%20application%20of%20recent%20advances%20in%20sequence%20models%0Ain%20order%20to%20capture%20long-range%20dependencies%20within%20these%20walks.%20Based%20on%20this%0Aconcept%2C%20we%20propose%20a%20framework%20that%20offers%20%281%29%20more%20expressive%20graph%0Arepresentations%20through%20random%20walk%20sequences%2C%20%282%29%20the%20ability%20to%20utilize%20any%0Asequence%20model%20for%20capturing%20long-range%20dependencies%2C%20and%20%283%29%20the%20flexibility%0Aby%20integrating%20various%20GNN%20and%20GT%20architectures.%20Our%20experimental%20evaluations%0Ademonstrate%20that%20NeuralWalker%20achieves%20significant%20performance%20improvements%20on%0A19%20graph%20and%20node%20benchmark%20datasets%2C%20notably%20outperforming%20existing%20methods%20by%0Aup%20to%2013%25%20on%20the%20PascalVoc-SP%20and%20COCO-SP%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/BorgwardtLab/NeuralWalker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03386v1&entry.124074799=Read"},
{"title": "IterMask2: Iterative Unsupervised Anomaly Segmentation via Spatial and\n  Frequency Masking for Brain Lesions in MRI", "author": "Ziyun Liang and Xiaoqing Guo and J. Alison Noble and Konstantinos Kamnitsas", "abstract": "  Unsupervised anomaly segmentation approaches to pathology segmentation train\na model on images of healthy subjects, that they define as the 'normal' data\ndistribution. At inference, they aim to segment any pathologies in new images\nas 'anomalies', as they exhibit patterns that deviate from those in 'normal'\ntraining data. Prevailing methods follow the 'corrupt-and-reconstruct'\nparadigm. They intentionally corrupt an input image, reconstruct it to follow\nthe learned 'normal' distribution, and subsequently segment anomalies based on\nreconstruction error. Corrupting an input image, however, inevitably leads to\nsuboptimal reconstruction even of normal regions, causing false positives. To\nalleviate this, we propose a novel iterative spatial mask-refining strategy\nIterMask2. We iteratively mask areas of the image, reconstruct them, and update\nthe mask based on reconstruction error. This iterative process progressively\nadds information about areas that are confidently normal as per the model. The\nincreasing content guides reconstruction of nearby masked areas, improving\nreconstruction of normal tissue under these areas, reducing false positives. We\nalso use high-frequency image content as an auxiliary input to provide\nadditional structural information for masked areas. This further improves\nreconstruction error of normal in comparison to anomalous areas, facilitating\nsegmentation of the latter. We conduct experiments on several brain lesion\ndatasets and demonstrate effectiveness of our method. Code is available at:\nhttps://github.com/ZiyunLiang/IterMask2\n", "link": "http://arxiv.org/abs/2406.02422v2", "date": "2024-06-05", "relevancy": 1.9912, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5048}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.493}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IterMask2%3A%20Iterative%20Unsupervised%20Anomaly%20Segmentation%20via%20Spatial%20and%0A%20%20Frequency%20Masking%20for%20Brain%20Lesions%20in%20MRI&body=Title%3A%20IterMask2%3A%20Iterative%20Unsupervised%20Anomaly%20Segmentation%20via%20Spatial%20and%0A%20%20Frequency%20Masking%20for%20Brain%20Lesions%20in%20MRI%0AAuthor%3A%20Ziyun%20Liang%20and%20Xiaoqing%20Guo%20and%20J.%20Alison%20Noble%20and%20Konstantinos%20Kamnitsas%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20segmentation%20approaches%20to%20pathology%20segmentation%20train%0Aa%20model%20on%20images%20of%20healthy%20subjects%2C%20that%20they%20define%20as%20the%20%27normal%27%20data%0Adistribution.%20At%20inference%2C%20they%20aim%20to%20segment%20any%20pathologies%20in%20new%20images%0Aas%20%27anomalies%27%2C%20as%20they%20exhibit%20patterns%20that%20deviate%20from%20those%20in%20%27normal%27%0Atraining%20data.%20Prevailing%20methods%20follow%20the%20%27corrupt-and-reconstruct%27%0Aparadigm.%20They%20intentionally%20corrupt%20an%20input%20image%2C%20reconstruct%20it%20to%20follow%0Athe%20learned%20%27normal%27%20distribution%2C%20and%20subsequently%20segment%20anomalies%20based%20on%0Areconstruction%20error.%20Corrupting%20an%20input%20image%2C%20however%2C%20inevitably%20leads%20to%0Asuboptimal%20reconstruction%20even%20of%20normal%20regions%2C%20causing%20false%20positives.%20To%0Aalleviate%20this%2C%20we%20propose%20a%20novel%20iterative%20spatial%20mask-refining%20strategy%0AIterMask2.%20We%20iteratively%20mask%20areas%20of%20the%20image%2C%20reconstruct%20them%2C%20and%20update%0Athe%20mask%20based%20on%20reconstruction%20error.%20This%20iterative%20process%20progressively%0Aadds%20information%20about%20areas%20that%20are%20confidently%20normal%20as%20per%20the%20model.%20The%0Aincreasing%20content%20guides%20reconstruction%20of%20nearby%20masked%20areas%2C%20improving%0Areconstruction%20of%20normal%20tissue%20under%20these%20areas%2C%20reducing%20false%20positives.%20We%0Aalso%20use%20high-frequency%20image%20content%20as%20an%20auxiliary%20input%20to%20provide%0Aadditional%20structural%20information%20for%20masked%20areas.%20This%20further%20improves%0Areconstruction%20error%20of%20normal%20in%20comparison%20to%20anomalous%20areas%2C%20facilitating%0Asegmentation%20of%20the%20latter.%20We%20conduct%20experiments%20on%20several%20brain%20lesion%0Adatasets%20and%20demonstrate%20effectiveness%20of%20our%20method.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ZiyunLiang/IterMask2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterMask2%253A%2520Iterative%2520Unsupervised%2520Anomaly%2520Segmentation%2520via%2520Spatial%2520and%250A%2520%2520Frequency%2520Masking%2520for%2520Brain%2520Lesions%2520in%2520MRI%26entry.906535625%3DZiyun%2520Liang%2520and%2520Xiaoqing%2520Guo%2520and%2520J.%2520Alison%2520Noble%2520and%2520Konstantinos%2520Kamnitsas%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520segmentation%2520approaches%2520to%2520pathology%2520segmentation%2520train%250Aa%2520model%2520on%2520images%2520of%2520healthy%2520subjects%252C%2520that%2520they%2520define%2520as%2520the%2520%2527normal%2527%2520data%250Adistribution.%2520At%2520inference%252C%2520they%2520aim%2520to%2520segment%2520any%2520pathologies%2520in%2520new%2520images%250Aas%2520%2527anomalies%2527%252C%2520as%2520they%2520exhibit%2520patterns%2520that%2520deviate%2520from%2520those%2520in%2520%2527normal%2527%250Atraining%2520data.%2520Prevailing%2520methods%2520follow%2520the%2520%2527corrupt-and-reconstruct%2527%250Aparadigm.%2520They%2520intentionally%2520corrupt%2520an%2520input%2520image%252C%2520reconstruct%2520it%2520to%2520follow%250Athe%2520learned%2520%2527normal%2527%2520distribution%252C%2520and%2520subsequently%2520segment%2520anomalies%2520based%2520on%250Areconstruction%2520error.%2520Corrupting%2520an%2520input%2520image%252C%2520however%252C%2520inevitably%2520leads%2520to%250Asuboptimal%2520reconstruction%2520even%2520of%2520normal%2520regions%252C%2520causing%2520false%2520positives.%2520To%250Aalleviate%2520this%252C%2520we%2520propose%2520a%2520novel%2520iterative%2520spatial%2520mask-refining%2520strategy%250AIterMask2.%2520We%2520iteratively%2520mask%2520areas%2520of%2520the%2520image%252C%2520reconstruct%2520them%252C%2520and%2520update%250Athe%2520mask%2520based%2520on%2520reconstruction%2520error.%2520This%2520iterative%2520process%2520progressively%250Aadds%2520information%2520about%2520areas%2520that%2520are%2520confidently%2520normal%2520as%2520per%2520the%2520model.%2520The%250Aincreasing%2520content%2520guides%2520reconstruction%2520of%2520nearby%2520masked%2520areas%252C%2520improving%250Areconstruction%2520of%2520normal%2520tissue%2520under%2520these%2520areas%252C%2520reducing%2520false%2520positives.%2520We%250Aalso%2520use%2520high-frequency%2520image%2520content%2520as%2520an%2520auxiliary%2520input%2520to%2520provide%250Aadditional%2520structural%2520information%2520for%2520masked%2520areas.%2520This%2520further%2520improves%250Areconstruction%2520error%2520of%2520normal%2520in%2520comparison%2520to%2520anomalous%2520areas%252C%2520facilitating%250Asegmentation%2520of%2520the%2520latter.%2520We%2520conduct%2520experiments%2520on%2520several%2520brain%2520lesion%250Adatasets%2520and%2520demonstrate%2520effectiveness%2520of%2520our%2520method.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ZiyunLiang/IterMask2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IterMask2%3A%20Iterative%20Unsupervised%20Anomaly%20Segmentation%20via%20Spatial%20and%0A%20%20Frequency%20Masking%20for%20Brain%20Lesions%20in%20MRI&entry.906535625=Ziyun%20Liang%20and%20Xiaoqing%20Guo%20and%20J.%20Alison%20Noble%20and%20Konstantinos%20Kamnitsas&entry.1292438233=%20%20Unsupervised%20anomaly%20segmentation%20approaches%20to%20pathology%20segmentation%20train%0Aa%20model%20on%20images%20of%20healthy%20subjects%2C%20that%20they%20define%20as%20the%20%27normal%27%20data%0Adistribution.%20At%20inference%2C%20they%20aim%20to%20segment%20any%20pathologies%20in%20new%20images%0Aas%20%27anomalies%27%2C%20as%20they%20exhibit%20patterns%20that%20deviate%20from%20those%20in%20%27normal%27%0Atraining%20data.%20Prevailing%20methods%20follow%20the%20%27corrupt-and-reconstruct%27%0Aparadigm.%20They%20intentionally%20corrupt%20an%20input%20image%2C%20reconstruct%20it%20to%20follow%0Athe%20learned%20%27normal%27%20distribution%2C%20and%20subsequently%20segment%20anomalies%20based%20on%0Areconstruction%20error.%20Corrupting%20an%20input%20image%2C%20however%2C%20inevitably%20leads%20to%0Asuboptimal%20reconstruction%20even%20of%20normal%20regions%2C%20causing%20false%20positives.%20To%0Aalleviate%20this%2C%20we%20propose%20a%20novel%20iterative%20spatial%20mask-refining%20strategy%0AIterMask2.%20We%20iteratively%20mask%20areas%20of%20the%20image%2C%20reconstruct%20them%2C%20and%20update%0Athe%20mask%20based%20on%20reconstruction%20error.%20This%20iterative%20process%20progressively%0Aadds%20information%20about%20areas%20that%20are%20confidently%20normal%20as%20per%20the%20model.%20The%0Aincreasing%20content%20guides%20reconstruction%20of%20nearby%20masked%20areas%2C%20improving%0Areconstruction%20of%20normal%20tissue%20under%20these%20areas%2C%20reducing%20false%20positives.%20We%0Aalso%20use%20high-frequency%20image%20content%20as%20an%20auxiliary%20input%20to%20provide%0Aadditional%20structural%20information%20for%20masked%20areas.%20This%20further%20improves%0Areconstruction%20error%20of%20normal%20in%20comparison%20to%20anomalous%20areas%2C%20facilitating%0Asegmentation%20of%20the%20latter.%20We%20conduct%20experiments%20on%20several%20brain%20lesion%0Adatasets%20and%20demonstrate%20effectiveness%20of%20our%20method.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ZiyunLiang/IterMask2%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02422v2&entry.124074799=Read"},
{"title": "Embarrassingly Parallel GFlowNets", "author": "Tiago da Silva and Luiz Max Carvalho and Amauri Souza and Samuel Kaski and Diego Mesquita", "abstract": "  GFlowNets are a promising alternative to MCMC sampling for discrete\ncompositional random variables. Training GFlowNets requires repeated\nevaluations of the unnormalized target distribution or reward function.\nHowever, for large-scale posterior sampling, this may be prohibitive since it\nincurs traversing the data several times. Moreover, if the data are distributed\nacross clients, employing standard GFlowNets leads to intensive client-server\ncommunication. To alleviate both these issues, we propose embarrassingly\nparallel GFlowNet (EP-GFlowNet). EP-GFlowNet is a provably correct\ndivide-and-conquer method to sample from product distributions of the form\n$R(\\cdot) \\propto R_1(\\cdot) ... R_N(\\cdot)$ -- e.g., in parallel or federated\nBayes, where each $R_n$ is a local posterior defined on a data partition.\nFirst, in parallel, we train a local GFlowNet targeting each $R_n$ and send the\nresulting models to the server. Then, the server learns a global GFlowNet by\nenforcing our newly proposed \\emph{aggregating balance} condition, requiring a\nsingle communication step. Importantly, EP-GFlowNets can also be applied to\nmulti-objective optimization and model reuse. Our experiments illustrate the\nEP-GFlowNets's effectiveness on many tasks, including parallel Bayesian\nphylogenetics, multi-objective multiset, sequence generation, and federated\nBayesian structure learning.\n", "link": "http://arxiv.org/abs/2406.03288v1", "date": "2024-06-05", "relevancy": 1.9867, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5179}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5048}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embarrassingly%20Parallel%20GFlowNets&body=Title%3A%20Embarrassingly%20Parallel%20GFlowNets%0AAuthor%3A%20Tiago%20da%20Silva%20and%20Luiz%20Max%20Carvalho%20and%20Amauri%20Souza%20and%20Samuel%20Kaski%20and%20Diego%20Mesquita%0AAbstract%3A%20%20%20GFlowNets%20are%20a%20promising%20alternative%20to%20MCMC%20sampling%20for%20discrete%0Acompositional%20random%20variables.%20Training%20GFlowNets%20requires%20repeated%0Aevaluations%20of%20the%20unnormalized%20target%20distribution%20or%20reward%20function.%0AHowever%2C%20for%20large-scale%20posterior%20sampling%2C%20this%20may%20be%20prohibitive%20since%20it%0Aincurs%20traversing%20the%20data%20several%20times.%20Moreover%2C%20if%20the%20data%20are%20distributed%0Aacross%20clients%2C%20employing%20standard%20GFlowNets%20leads%20to%20intensive%20client-server%0Acommunication.%20To%20alleviate%20both%20these%20issues%2C%20we%20propose%20embarrassingly%0Aparallel%20GFlowNet%20%28EP-GFlowNet%29.%20EP-GFlowNet%20is%20a%20provably%20correct%0Adivide-and-conquer%20method%20to%20sample%20from%20product%20distributions%20of%20the%20form%0A%24R%28%5Ccdot%29%20%5Cpropto%20R_1%28%5Ccdot%29%20...%20R_N%28%5Ccdot%29%24%20--%20e.g.%2C%20in%20parallel%20or%20federated%0ABayes%2C%20where%20each%20%24R_n%24%20is%20a%20local%20posterior%20defined%20on%20a%20data%20partition.%0AFirst%2C%20in%20parallel%2C%20we%20train%20a%20local%20GFlowNet%20targeting%20each%20%24R_n%24%20and%20send%20the%0Aresulting%20models%20to%20the%20server.%20Then%2C%20the%20server%20learns%20a%20global%20GFlowNet%20by%0Aenforcing%20our%20newly%20proposed%20%5Cemph%7Baggregating%20balance%7D%20condition%2C%20requiring%20a%0Asingle%20communication%20step.%20Importantly%2C%20EP-GFlowNets%20can%20also%20be%20applied%20to%0Amulti-objective%20optimization%20and%20model%20reuse.%20Our%20experiments%20illustrate%20the%0AEP-GFlowNets%27s%20effectiveness%20on%20many%20tasks%2C%20including%20parallel%20Bayesian%0Aphylogenetics%2C%20multi-objective%20multiset%2C%20sequence%20generation%2C%20and%20federated%0ABayesian%20structure%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbarrassingly%2520Parallel%2520GFlowNets%26entry.906535625%3DTiago%2520da%2520Silva%2520and%2520Luiz%2520Max%2520Carvalho%2520and%2520Amauri%2520Souza%2520and%2520Samuel%2520Kaski%2520and%2520Diego%2520Mesquita%26entry.1292438233%3D%2520%2520GFlowNets%2520are%2520a%2520promising%2520alternative%2520to%2520MCMC%2520sampling%2520for%2520discrete%250Acompositional%2520random%2520variables.%2520Training%2520GFlowNets%2520requires%2520repeated%250Aevaluations%2520of%2520the%2520unnormalized%2520target%2520distribution%2520or%2520reward%2520function.%250AHowever%252C%2520for%2520large-scale%2520posterior%2520sampling%252C%2520this%2520may%2520be%2520prohibitive%2520since%2520it%250Aincurs%2520traversing%2520the%2520data%2520several%2520times.%2520Moreover%252C%2520if%2520the%2520data%2520are%2520distributed%250Aacross%2520clients%252C%2520employing%2520standard%2520GFlowNets%2520leads%2520to%2520intensive%2520client-server%250Acommunication.%2520To%2520alleviate%2520both%2520these%2520issues%252C%2520we%2520propose%2520embarrassingly%250Aparallel%2520GFlowNet%2520%2528EP-GFlowNet%2529.%2520EP-GFlowNet%2520is%2520a%2520provably%2520correct%250Adivide-and-conquer%2520method%2520to%2520sample%2520from%2520product%2520distributions%2520of%2520the%2520form%250A%2524R%2528%255Ccdot%2529%2520%255Cpropto%2520R_1%2528%255Ccdot%2529%2520...%2520R_N%2528%255Ccdot%2529%2524%2520--%2520e.g.%252C%2520in%2520parallel%2520or%2520federated%250ABayes%252C%2520where%2520each%2520%2524R_n%2524%2520is%2520a%2520local%2520posterior%2520defined%2520on%2520a%2520data%2520partition.%250AFirst%252C%2520in%2520parallel%252C%2520we%2520train%2520a%2520local%2520GFlowNet%2520targeting%2520each%2520%2524R_n%2524%2520and%2520send%2520the%250Aresulting%2520models%2520to%2520the%2520server.%2520Then%252C%2520the%2520server%2520learns%2520a%2520global%2520GFlowNet%2520by%250Aenforcing%2520our%2520newly%2520proposed%2520%255Cemph%257Baggregating%2520balance%257D%2520condition%252C%2520requiring%2520a%250Asingle%2520communication%2520step.%2520Importantly%252C%2520EP-GFlowNets%2520can%2520also%2520be%2520applied%2520to%250Amulti-objective%2520optimization%2520and%2520model%2520reuse.%2520Our%2520experiments%2520illustrate%2520the%250AEP-GFlowNets%2527s%2520effectiveness%2520on%2520many%2520tasks%252C%2520including%2520parallel%2520Bayesian%250Aphylogenetics%252C%2520multi-objective%2520multiset%252C%2520sequence%2520generation%252C%2520and%2520federated%250ABayesian%2520structure%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embarrassingly%20Parallel%20GFlowNets&entry.906535625=Tiago%20da%20Silva%20and%20Luiz%20Max%20Carvalho%20and%20Amauri%20Souza%20and%20Samuel%20Kaski%20and%20Diego%20Mesquita&entry.1292438233=%20%20GFlowNets%20are%20a%20promising%20alternative%20to%20MCMC%20sampling%20for%20discrete%0Acompositional%20random%20variables.%20Training%20GFlowNets%20requires%20repeated%0Aevaluations%20of%20the%20unnormalized%20target%20distribution%20or%20reward%20function.%0AHowever%2C%20for%20large-scale%20posterior%20sampling%2C%20this%20may%20be%20prohibitive%20since%20it%0Aincurs%20traversing%20the%20data%20several%20times.%20Moreover%2C%20if%20the%20data%20are%20distributed%0Aacross%20clients%2C%20employing%20standard%20GFlowNets%20leads%20to%20intensive%20client-server%0Acommunication.%20To%20alleviate%20both%20these%20issues%2C%20we%20propose%20embarrassingly%0Aparallel%20GFlowNet%20%28EP-GFlowNet%29.%20EP-GFlowNet%20is%20a%20provably%20correct%0Adivide-and-conquer%20method%20to%20sample%20from%20product%20distributions%20of%20the%20form%0A%24R%28%5Ccdot%29%20%5Cpropto%20R_1%28%5Ccdot%29%20...%20R_N%28%5Ccdot%29%24%20--%20e.g.%2C%20in%20parallel%20or%20federated%0ABayes%2C%20where%20each%20%24R_n%24%20is%20a%20local%20posterior%20defined%20on%20a%20data%20partition.%0AFirst%2C%20in%20parallel%2C%20we%20train%20a%20local%20GFlowNet%20targeting%20each%20%24R_n%24%20and%20send%20the%0Aresulting%20models%20to%20the%20server.%20Then%2C%20the%20server%20learns%20a%20global%20GFlowNet%20by%0Aenforcing%20our%20newly%20proposed%20%5Cemph%7Baggregating%20balance%7D%20condition%2C%20requiring%20a%0Asingle%20communication%20step.%20Importantly%2C%20EP-GFlowNets%20can%20also%20be%20applied%20to%0Amulti-objective%20optimization%20and%20model%20reuse.%20Our%20experiments%20illustrate%20the%0AEP-GFlowNets%27s%20effectiveness%20on%20many%20tasks%2C%20including%20parallel%20Bayesian%0Aphylogenetics%2C%20multi-objective%20multiset%2C%20sequence%20generation%2C%20and%20federated%0ABayesian%20structure%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03288v1&entry.124074799=Read"},
{"title": "Exploring Higher Order Structures in Graph Explanantions", "author": "Akshit Sinha and Sreeram Vennam and Charu Sharma and Ponnurangam Kumaraguru", "abstract": "  Recent advancements in graph learning contributed to explaining predictions\ngenerated by Graph Neural Networks. However, existing methodologies often fall\nshort when applied to real-world datasets. We introduce HOGE, a framework to\ncapture higher-order structures using cell complexes, which excel at modeling\nhigher-order relationships. In the real world, higher-order structures are\nubiquitous like in molecules or social networks, thus our work significantly\nenhances the practical applicability of graph explanations. HOGE produces\nclearer and more accurate explanations compared to prior methods. Our method\ncan be integrated with all existing graph explainers, ensuring seamless\nintegration into current frameworks. We evaluate on GraphXAI benchmark\ndatasets, HOGE achieves improved or comparable performance with minimal\ncomputational overhead. Ablation studies show that the performance gain\nobserved can be attributed to the higher-order structures that come from\nintroducing cell complexes.\n", "link": "http://arxiv.org/abs/2406.03253v1", "date": "2024-06-05", "relevancy": 1.9831, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5087}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5053}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Higher%20Order%20Structures%20in%20Graph%20Explanantions&body=Title%3A%20Exploring%20Higher%20Order%20Structures%20in%20Graph%20Explanantions%0AAuthor%3A%20Akshit%20Sinha%20and%20Sreeram%20Vennam%20and%20Charu%20Sharma%20and%20Ponnurangam%20Kumaraguru%0AAbstract%3A%20%20%20Recent%20advancements%20in%20graph%20learning%20contributed%20to%20explaining%20predictions%0Agenerated%20by%20Graph%20Neural%20Networks.%20However%2C%20existing%20methodologies%20often%20fall%0Ashort%20when%20applied%20to%20real-world%20datasets.%20We%20introduce%20HOGE%2C%20a%20framework%20to%0Acapture%20higher-order%20structures%20using%20cell%20complexes%2C%20which%20excel%20at%20modeling%0Ahigher-order%20relationships.%20In%20the%20real%20world%2C%20higher-order%20structures%20are%0Aubiquitous%20like%20in%20molecules%20or%20social%20networks%2C%20thus%20our%20work%20significantly%0Aenhances%20the%20practical%20applicability%20of%20graph%20explanations.%20HOGE%20produces%0Aclearer%20and%20more%20accurate%20explanations%20compared%20to%20prior%20methods.%20Our%20method%0Acan%20be%20integrated%20with%20all%20existing%20graph%20explainers%2C%20ensuring%20seamless%0Aintegration%20into%20current%20frameworks.%20We%20evaluate%20on%20GraphXAI%20benchmark%0Adatasets%2C%20HOGE%20achieves%20improved%20or%20comparable%20performance%20with%20minimal%0Acomputational%20overhead.%20Ablation%20studies%20show%20that%20the%20performance%20gain%0Aobserved%20can%20be%20attributed%20to%20the%20higher-order%20structures%20that%20come%20from%0Aintroducing%20cell%20complexes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Higher%2520Order%2520Structures%2520in%2520Graph%2520Explanantions%26entry.906535625%3DAkshit%2520Sinha%2520and%2520Sreeram%2520Vennam%2520and%2520Charu%2520Sharma%2520and%2520Ponnurangam%2520Kumaraguru%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520graph%2520learning%2520contributed%2520to%2520explaining%2520predictions%250Agenerated%2520by%2520Graph%2520Neural%2520Networks.%2520However%252C%2520existing%2520methodologies%2520often%2520fall%250Ashort%2520when%2520applied%2520to%2520real-world%2520datasets.%2520We%2520introduce%2520HOGE%252C%2520a%2520framework%2520to%250Acapture%2520higher-order%2520structures%2520using%2520cell%2520complexes%252C%2520which%2520excel%2520at%2520modeling%250Ahigher-order%2520relationships.%2520In%2520the%2520real%2520world%252C%2520higher-order%2520structures%2520are%250Aubiquitous%2520like%2520in%2520molecules%2520or%2520social%2520networks%252C%2520thus%2520our%2520work%2520significantly%250Aenhances%2520the%2520practical%2520applicability%2520of%2520graph%2520explanations.%2520HOGE%2520produces%250Aclearer%2520and%2520more%2520accurate%2520explanations%2520compared%2520to%2520prior%2520methods.%2520Our%2520method%250Acan%2520be%2520integrated%2520with%2520all%2520existing%2520graph%2520explainers%252C%2520ensuring%2520seamless%250Aintegration%2520into%2520current%2520frameworks.%2520We%2520evaluate%2520on%2520GraphXAI%2520benchmark%250Adatasets%252C%2520HOGE%2520achieves%2520improved%2520or%2520comparable%2520performance%2520with%2520minimal%250Acomputational%2520overhead.%2520Ablation%2520studies%2520show%2520that%2520the%2520performance%2520gain%250Aobserved%2520can%2520be%2520attributed%2520to%2520the%2520higher-order%2520structures%2520that%2520come%2520from%250Aintroducing%2520cell%2520complexes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Higher%20Order%20Structures%20in%20Graph%20Explanantions&entry.906535625=Akshit%20Sinha%20and%20Sreeram%20Vennam%20and%20Charu%20Sharma%20and%20Ponnurangam%20Kumaraguru&entry.1292438233=%20%20Recent%20advancements%20in%20graph%20learning%20contributed%20to%20explaining%20predictions%0Agenerated%20by%20Graph%20Neural%20Networks.%20However%2C%20existing%20methodologies%20often%20fall%0Ashort%20when%20applied%20to%20real-world%20datasets.%20We%20introduce%20HOGE%2C%20a%20framework%20to%0Acapture%20higher-order%20structures%20using%20cell%20complexes%2C%20which%20excel%20at%20modeling%0Ahigher-order%20relationships.%20In%20the%20real%20world%2C%20higher-order%20structures%20are%0Aubiquitous%20like%20in%20molecules%20or%20social%20networks%2C%20thus%20our%20work%20significantly%0Aenhances%20the%20practical%20applicability%20of%20graph%20explanations.%20HOGE%20produces%0Aclearer%20and%20more%20accurate%20explanations%20compared%20to%20prior%20methods.%20Our%20method%0Acan%20be%20integrated%20with%20all%20existing%20graph%20explainers%2C%20ensuring%20seamless%0Aintegration%20into%20current%20frameworks.%20We%20evaluate%20on%20GraphXAI%20benchmark%0Adatasets%2C%20HOGE%20achieves%20improved%20or%20comparable%20performance%20with%20minimal%0Acomputational%20overhead.%20Ablation%20studies%20show%20that%20the%20performance%20gain%0Aobserved%20can%20be%20attributed%20to%20the%20higher-order%20structures%20that%20come%20from%0Aintroducing%20cell%20complexes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03253v1&entry.124074799=Read"},
{"title": "Position: Quo Vadis, Unsupervised Time Series Anomaly Detection?", "author": "M. Saquib Sarfraz and Mei-Yen Chen and Lukas Layer and Kunyu Peng and Marios Koulakis", "abstract": "  The current state of machine learning scholarship in Timeseries Anomaly\nDetection (TAD) is plagued by the persistent use of flawed evaluation metrics,\ninconsistent benchmarking practices, and a lack of proper justification for the\nchoices made in novel deep learning-based model designs. Our paper presents a\ncritical analysis of the status quo in TAD, revealing the misleading track of\ncurrent research and highlighting problematic methods, and evaluation\npractices. Our position advocates for a shift in focus from solely pursuing\nnovel model designs to improving benchmarking practices, creating non-trivial\ndatasets, and critically evaluating the utility of complex methods against\nsimpler baselines. Our findings demonstrate the need for rigorous evaluation\nprotocols, the creation of simple baselines, and the revelation that\nstate-of-the-art deep anomaly detection models effectively learn linear\nmappings. These findings suggest the need for more exploration and development\nof simple and interpretable TAD methods. The increment of model complexity in\nthe state-of-the-art deep-learning based models unfortunately offers very\nlittle improvement. We offer insights and suggestions for the field to move\nforward.\n  Code: https://github.com/ssarfraz/QuoVadisTAD\n", "link": "http://arxiv.org/abs/2405.02678v3", "date": "2024-06-05", "relevancy": 1.9824, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5019}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4951}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Quo%20Vadis%2C%20Unsupervised%20Time%20Series%20Anomaly%20Detection%3F&body=Title%3A%20Position%3A%20Quo%20Vadis%2C%20Unsupervised%20Time%20Series%20Anomaly%20Detection%3F%0AAuthor%3A%20M.%20Saquib%20Sarfraz%20and%20Mei-Yen%20Chen%20and%20Lukas%20Layer%20and%20Kunyu%20Peng%20and%20Marios%20Koulakis%0AAbstract%3A%20%20%20The%20current%20state%20of%20machine%20learning%20scholarship%20in%20Timeseries%20Anomaly%0ADetection%20%28TAD%29%20is%20plagued%20by%20the%20persistent%20use%20of%20flawed%20evaluation%20metrics%2C%0Ainconsistent%20benchmarking%20practices%2C%20and%20a%20lack%20of%20proper%20justification%20for%20the%0Achoices%20made%20in%20novel%20deep%20learning-based%20model%20designs.%20Our%20paper%20presents%20a%0Acritical%20analysis%20of%20the%20status%20quo%20in%20TAD%2C%20revealing%20the%20misleading%20track%20of%0Acurrent%20research%20and%20highlighting%20problematic%20methods%2C%20and%20evaluation%0Apractices.%20Our%20position%20advocates%20for%20a%20shift%20in%20focus%20from%20solely%20pursuing%0Anovel%20model%20designs%20to%20improving%20benchmarking%20practices%2C%20creating%20non-trivial%0Adatasets%2C%20and%20critically%20evaluating%20the%20utility%20of%20complex%20methods%20against%0Asimpler%20baselines.%20Our%20findings%20demonstrate%20the%20need%20for%20rigorous%20evaluation%0Aprotocols%2C%20the%20creation%20of%20simple%20baselines%2C%20and%20the%20revelation%20that%0Astate-of-the-art%20deep%20anomaly%20detection%20models%20effectively%20learn%20linear%0Amappings.%20These%20findings%20suggest%20the%20need%20for%20more%20exploration%20and%20development%0Aof%20simple%20and%20interpretable%20TAD%20methods.%20The%20increment%20of%20model%20complexity%20in%0Athe%20state-of-the-art%20deep-learning%20based%20models%20unfortunately%20offers%20very%0Alittle%20improvement.%20We%20offer%20insights%20and%20suggestions%20for%20the%20field%20to%20move%0Aforward.%0A%20%20Code%3A%20https%3A//github.com/ssarfraz/QuoVadisTAD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02678v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Quo%2520Vadis%252C%2520Unsupervised%2520Time%2520Series%2520Anomaly%2520Detection%253F%26entry.906535625%3DM.%2520Saquib%2520Sarfraz%2520and%2520Mei-Yen%2520Chen%2520and%2520Lukas%2520Layer%2520and%2520Kunyu%2520Peng%2520and%2520Marios%2520Koulakis%26entry.1292438233%3D%2520%2520The%2520current%2520state%2520of%2520machine%2520learning%2520scholarship%2520in%2520Timeseries%2520Anomaly%250ADetection%2520%2528TAD%2529%2520is%2520plagued%2520by%2520the%2520persistent%2520use%2520of%2520flawed%2520evaluation%2520metrics%252C%250Ainconsistent%2520benchmarking%2520practices%252C%2520and%2520a%2520lack%2520of%2520proper%2520justification%2520for%2520the%250Achoices%2520made%2520in%2520novel%2520deep%2520learning-based%2520model%2520designs.%2520Our%2520paper%2520presents%2520a%250Acritical%2520analysis%2520of%2520the%2520status%2520quo%2520in%2520TAD%252C%2520revealing%2520the%2520misleading%2520track%2520of%250Acurrent%2520research%2520and%2520highlighting%2520problematic%2520methods%252C%2520and%2520evaluation%250Apractices.%2520Our%2520position%2520advocates%2520for%2520a%2520shift%2520in%2520focus%2520from%2520solely%2520pursuing%250Anovel%2520model%2520designs%2520to%2520improving%2520benchmarking%2520practices%252C%2520creating%2520non-trivial%250Adatasets%252C%2520and%2520critically%2520evaluating%2520the%2520utility%2520of%2520complex%2520methods%2520against%250Asimpler%2520baselines.%2520Our%2520findings%2520demonstrate%2520the%2520need%2520for%2520rigorous%2520evaluation%250Aprotocols%252C%2520the%2520creation%2520of%2520simple%2520baselines%252C%2520and%2520the%2520revelation%2520that%250Astate-of-the-art%2520deep%2520anomaly%2520detection%2520models%2520effectively%2520learn%2520linear%250Amappings.%2520These%2520findings%2520suggest%2520the%2520need%2520for%2520more%2520exploration%2520and%2520development%250Aof%2520simple%2520and%2520interpretable%2520TAD%2520methods.%2520The%2520increment%2520of%2520model%2520complexity%2520in%250Athe%2520state-of-the-art%2520deep-learning%2520based%2520models%2520unfortunately%2520offers%2520very%250Alittle%2520improvement.%2520We%2520offer%2520insights%2520and%2520suggestions%2520for%2520the%2520field%2520to%2520move%250Aforward.%250A%2520%2520Code%253A%2520https%253A//github.com/ssarfraz/QuoVadisTAD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02678v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Quo%20Vadis%2C%20Unsupervised%20Time%20Series%20Anomaly%20Detection%3F&entry.906535625=M.%20Saquib%20Sarfraz%20and%20Mei-Yen%20Chen%20and%20Lukas%20Layer%20and%20Kunyu%20Peng%20and%20Marios%20Koulakis&entry.1292438233=%20%20The%20current%20state%20of%20machine%20learning%20scholarship%20in%20Timeseries%20Anomaly%0ADetection%20%28TAD%29%20is%20plagued%20by%20the%20persistent%20use%20of%20flawed%20evaluation%20metrics%2C%0Ainconsistent%20benchmarking%20practices%2C%20and%20a%20lack%20of%20proper%20justification%20for%20the%0Achoices%20made%20in%20novel%20deep%20learning-based%20model%20designs.%20Our%20paper%20presents%20a%0Acritical%20analysis%20of%20the%20status%20quo%20in%20TAD%2C%20revealing%20the%20misleading%20track%20of%0Acurrent%20research%20and%20highlighting%20problematic%20methods%2C%20and%20evaluation%0Apractices.%20Our%20position%20advocates%20for%20a%20shift%20in%20focus%20from%20solely%20pursuing%0Anovel%20model%20designs%20to%20improving%20benchmarking%20practices%2C%20creating%20non-trivial%0Adatasets%2C%20and%20critically%20evaluating%20the%20utility%20of%20complex%20methods%20against%0Asimpler%20baselines.%20Our%20findings%20demonstrate%20the%20need%20for%20rigorous%20evaluation%0Aprotocols%2C%20the%20creation%20of%20simple%20baselines%2C%20and%20the%20revelation%20that%0Astate-of-the-art%20deep%20anomaly%20detection%20models%20effectively%20learn%20linear%0Amappings.%20These%20findings%20suggest%20the%20need%20for%20more%20exploration%20and%20development%0Aof%20simple%20and%20interpretable%20TAD%20methods.%20The%20increment%20of%20model%20complexity%20in%0Athe%20state-of-the-art%20deep-learning%20based%20models%20unfortunately%20offers%20very%0Alittle%20improvement.%20We%20offer%20insights%20and%20suggestions%20for%20the%20field%20to%20move%0Aforward.%0A%20%20Code%3A%20https%3A//github.com/ssarfraz/QuoVadisTAD%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02678v3&entry.124074799=Read"},
{"title": "Posterior and variational inference for deep neural networks with\n  heavy-tailed weights", "author": "Isma\u00ebl Castillo and Paul Egels", "abstract": "  We consider deep neural networks in a Bayesian framework with a prior\ndistribution sampling the network weights at random. Following a recent idea of\nAgapiou and Castillo (2023), who show that heavy-tailed prior distributions\nachieve automatic adaptation to smoothness, we introduce a simple Bayesian deep\nlearning prior based on heavy-tailed weights and ReLU activation. We show that\nthe corresponding posterior distribution achieves near-optimal minimax\ncontraction rates, simultaneously adaptive to both intrinsic dimension and\nsmoothness of the underlying function, in a variety of contexts including\nnonparametric regression, geometric data and Besov spaces. While most works so\nfar need a form of model selection built-in within the prior distribution, a\nkey aspect of our approach is that it does not require to sample\nhyperparameters to learn the architecture of the network. We also provide\nvariational Bayes counterparts of the results, that show that mean-field\nvariational approximations still benefit from near-optimal theoretical support.\n", "link": "http://arxiv.org/abs/2406.03369v1", "date": "2024-06-05", "relevancy": 1.9711, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4941}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Posterior%20and%20variational%20inference%20for%20deep%20neural%20networks%20with%0A%20%20heavy-tailed%20weights&body=Title%3A%20Posterior%20and%20variational%20inference%20for%20deep%20neural%20networks%20with%0A%20%20heavy-tailed%20weights%0AAuthor%3A%20Isma%C3%ABl%20Castillo%20and%20Paul%20Egels%0AAbstract%3A%20%20%20We%20consider%20deep%20neural%20networks%20in%20a%20Bayesian%20framework%20with%20a%20prior%0Adistribution%20sampling%20the%20network%20weights%20at%20random.%20Following%20a%20recent%20idea%20of%0AAgapiou%20and%20Castillo%20%282023%29%2C%20who%20show%20that%20heavy-tailed%20prior%20distributions%0Aachieve%20automatic%20adaptation%20to%20smoothness%2C%20we%20introduce%20a%20simple%20Bayesian%20deep%0Alearning%20prior%20based%20on%20heavy-tailed%20weights%20and%20ReLU%20activation.%20We%20show%20that%0Athe%20corresponding%20posterior%20distribution%20achieves%20near-optimal%20minimax%0Acontraction%20rates%2C%20simultaneously%20adaptive%20to%20both%20intrinsic%20dimension%20and%0Asmoothness%20of%20the%20underlying%20function%2C%20in%20a%20variety%20of%20contexts%20including%0Anonparametric%20regression%2C%20geometric%20data%20and%20Besov%20spaces.%20While%20most%20works%20so%0Afar%20need%20a%20form%20of%20model%20selection%20built-in%20within%20the%20prior%20distribution%2C%20a%0Akey%20aspect%20of%20our%20approach%20is%20that%20it%20does%20not%20require%20to%20sample%0Ahyperparameters%20to%20learn%20the%20architecture%20of%20the%20network.%20We%20also%20provide%0Avariational%20Bayes%20counterparts%20of%20the%20results%2C%20that%20show%20that%20mean-field%0Avariational%20approximations%20still%20benefit%20from%20near-optimal%20theoretical%20support.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosterior%2520and%2520variational%2520inference%2520for%2520deep%2520neural%2520networks%2520with%250A%2520%2520heavy-tailed%2520weights%26entry.906535625%3DIsma%25C3%25ABl%2520Castillo%2520and%2520Paul%2520Egels%26entry.1292438233%3D%2520%2520We%2520consider%2520deep%2520neural%2520networks%2520in%2520a%2520Bayesian%2520framework%2520with%2520a%2520prior%250Adistribution%2520sampling%2520the%2520network%2520weights%2520at%2520random.%2520Following%2520a%2520recent%2520idea%2520of%250AAgapiou%2520and%2520Castillo%2520%25282023%2529%252C%2520who%2520show%2520that%2520heavy-tailed%2520prior%2520distributions%250Aachieve%2520automatic%2520adaptation%2520to%2520smoothness%252C%2520we%2520introduce%2520a%2520simple%2520Bayesian%2520deep%250Alearning%2520prior%2520based%2520on%2520heavy-tailed%2520weights%2520and%2520ReLU%2520activation.%2520We%2520show%2520that%250Athe%2520corresponding%2520posterior%2520distribution%2520achieves%2520near-optimal%2520minimax%250Acontraction%2520rates%252C%2520simultaneously%2520adaptive%2520to%2520both%2520intrinsic%2520dimension%2520and%250Asmoothness%2520of%2520the%2520underlying%2520function%252C%2520in%2520a%2520variety%2520of%2520contexts%2520including%250Anonparametric%2520regression%252C%2520geometric%2520data%2520and%2520Besov%2520spaces.%2520While%2520most%2520works%2520so%250Afar%2520need%2520a%2520form%2520of%2520model%2520selection%2520built-in%2520within%2520the%2520prior%2520distribution%252C%2520a%250Akey%2520aspect%2520of%2520our%2520approach%2520is%2520that%2520it%2520does%2520not%2520require%2520to%2520sample%250Ahyperparameters%2520to%2520learn%2520the%2520architecture%2520of%2520the%2520network.%2520We%2520also%2520provide%250Avariational%2520Bayes%2520counterparts%2520of%2520the%2520results%252C%2520that%2520show%2520that%2520mean-field%250Avariational%2520approximations%2520still%2520benefit%2520from%2520near-optimal%2520theoretical%2520support.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Posterior%20and%20variational%20inference%20for%20deep%20neural%20networks%20with%0A%20%20heavy-tailed%20weights&entry.906535625=Isma%C3%ABl%20Castillo%20and%20Paul%20Egels&entry.1292438233=%20%20We%20consider%20deep%20neural%20networks%20in%20a%20Bayesian%20framework%20with%20a%20prior%0Adistribution%20sampling%20the%20network%20weights%20at%20random.%20Following%20a%20recent%20idea%20of%0AAgapiou%20and%20Castillo%20%282023%29%2C%20who%20show%20that%20heavy-tailed%20prior%20distributions%0Aachieve%20automatic%20adaptation%20to%20smoothness%2C%20we%20introduce%20a%20simple%20Bayesian%20deep%0Alearning%20prior%20based%20on%20heavy-tailed%20weights%20and%20ReLU%20activation.%20We%20show%20that%0Athe%20corresponding%20posterior%20distribution%20achieves%20near-optimal%20minimax%0Acontraction%20rates%2C%20simultaneously%20adaptive%20to%20both%20intrinsic%20dimension%20and%0Asmoothness%20of%20the%20underlying%20function%2C%20in%20a%20variety%20of%20contexts%20including%0Anonparametric%20regression%2C%20geometric%20data%20and%20Besov%20spaces.%20While%20most%20works%20so%0Afar%20need%20a%20form%20of%20model%20selection%20built-in%20within%20the%20prior%20distribution%2C%20a%0Akey%20aspect%20of%20our%20approach%20is%20that%20it%20does%20not%20require%20to%20sample%0Ahyperparameters%20to%20learn%20the%20architecture%20of%20the%20network.%20We%20also%20provide%0Avariational%20Bayes%20counterparts%20of%20the%20results%2C%20that%20show%20that%20mean-field%0Avariational%20approximations%20still%20benefit%20from%20near-optimal%20theoretical%20support.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03369v1&entry.124074799=Read"},
{"title": "Audio Mamba: Bidirectional State Space Model for Audio Representation\n  Learning", "author": "Mehmet Hamza Erol and Arda Senocak and Jiu Feng and Joon Son Chung", "abstract": "  Transformers have rapidly become the preferred choice for audio\nclassification, surpassing methods based on CNNs. However, Audio Spectrogram\nTransformers (ASTs) exhibit quadratic scaling due to self-attention. The\nremoval of this quadratic self-attention cost presents an appealing direction.\nRecently, state space models (SSMs), such as Mamba, have demonstrated potential\nin language and vision tasks in this regard. In this study, we explore whether\nreliance on self-attention is necessary for audio classification tasks. By\nintroducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based\nmodel for audio classification, we aim to address this question. We evaluate\nAuM on various audio datasets - comprising six different benchmarks - where it\nachieves comparable or better performance compared to well-established AST\nmodel.\n", "link": "http://arxiv.org/abs/2406.03344v1", "date": "2024-06-05", "relevancy": 1.9669, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5161}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4776}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio%20Mamba%3A%20Bidirectional%20State%20Space%20Model%20for%20Audio%20Representation%0A%20%20Learning&body=Title%3A%20Audio%20Mamba%3A%20Bidirectional%20State%20Space%20Model%20for%20Audio%20Representation%0A%20%20Learning%0AAuthor%3A%20Mehmet%20Hamza%20Erol%20and%20Arda%20Senocak%20and%20Jiu%20Feng%20and%20Joon%20Son%20Chung%0AAbstract%3A%20%20%20Transformers%20have%20rapidly%20become%20the%20preferred%20choice%20for%20audio%0Aclassification%2C%20surpassing%20methods%20based%20on%20CNNs.%20However%2C%20Audio%20Spectrogram%0ATransformers%20%28ASTs%29%20exhibit%20quadratic%20scaling%20due%20to%20self-attention.%20The%0Aremoval%20of%20this%20quadratic%20self-attention%20cost%20presents%20an%20appealing%20direction.%0ARecently%2C%20state%20space%20models%20%28SSMs%29%2C%20such%20as%20Mamba%2C%20have%20demonstrated%20potential%0Ain%20language%20and%20vision%20tasks%20in%20this%20regard.%20In%20this%20study%2C%20we%20explore%20whether%0Areliance%20on%20self-attention%20is%20necessary%20for%20audio%20classification%20tasks.%20By%0Aintroducing%20Audio%20Mamba%20%28AuM%29%2C%20the%20first%20self-attention-free%2C%20purely%20SSM-based%0Amodel%20for%20audio%20classification%2C%20we%20aim%20to%20address%20this%20question.%20We%20evaluate%0AAuM%20on%20various%20audio%20datasets%20-%20comprising%20six%20different%20benchmarks%20-%20where%20it%0Aachieves%20comparable%20or%20better%20performance%20compared%20to%20well-established%20AST%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio%2520Mamba%253A%2520Bidirectional%2520State%2520Space%2520Model%2520for%2520Audio%2520Representation%250A%2520%2520Learning%26entry.906535625%3DMehmet%2520Hamza%2520Erol%2520and%2520Arda%2520Senocak%2520and%2520Jiu%2520Feng%2520and%2520Joon%2520Son%2520Chung%26entry.1292438233%3D%2520%2520Transformers%2520have%2520rapidly%2520become%2520the%2520preferred%2520choice%2520for%2520audio%250Aclassification%252C%2520surpassing%2520methods%2520based%2520on%2520CNNs.%2520However%252C%2520Audio%2520Spectrogram%250ATransformers%2520%2528ASTs%2529%2520exhibit%2520quadratic%2520scaling%2520due%2520to%2520self-attention.%2520The%250Aremoval%2520of%2520this%2520quadratic%2520self-attention%2520cost%2520presents%2520an%2520appealing%2520direction.%250ARecently%252C%2520state%2520space%2520models%2520%2528SSMs%2529%252C%2520such%2520as%2520Mamba%252C%2520have%2520demonstrated%2520potential%250Ain%2520language%2520and%2520vision%2520tasks%2520in%2520this%2520regard.%2520In%2520this%2520study%252C%2520we%2520explore%2520whether%250Areliance%2520on%2520self-attention%2520is%2520necessary%2520for%2520audio%2520classification%2520tasks.%2520By%250Aintroducing%2520Audio%2520Mamba%2520%2528AuM%2529%252C%2520the%2520first%2520self-attention-free%252C%2520purely%2520SSM-based%250Amodel%2520for%2520audio%2520classification%252C%2520we%2520aim%2520to%2520address%2520this%2520question.%2520We%2520evaluate%250AAuM%2520on%2520various%2520audio%2520datasets%2520-%2520comprising%2520six%2520different%2520benchmarks%2520-%2520where%2520it%250Aachieves%2520comparable%2520or%2520better%2520performance%2520compared%2520to%2520well-established%2520AST%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio%20Mamba%3A%20Bidirectional%20State%20Space%20Model%20for%20Audio%20Representation%0A%20%20Learning&entry.906535625=Mehmet%20Hamza%20Erol%20and%20Arda%20Senocak%20and%20Jiu%20Feng%20and%20Joon%20Son%20Chung&entry.1292438233=%20%20Transformers%20have%20rapidly%20become%20the%20preferred%20choice%20for%20audio%0Aclassification%2C%20surpassing%20methods%20based%20on%20CNNs.%20However%2C%20Audio%20Spectrogram%0ATransformers%20%28ASTs%29%20exhibit%20quadratic%20scaling%20due%20to%20self-attention.%20The%0Aremoval%20of%20this%20quadratic%20self-attention%20cost%20presents%20an%20appealing%20direction.%0ARecently%2C%20state%20space%20models%20%28SSMs%29%2C%20such%20as%20Mamba%2C%20have%20demonstrated%20potential%0Ain%20language%20and%20vision%20tasks%20in%20this%20regard.%20In%20this%20study%2C%20we%20explore%20whether%0Areliance%20on%20self-attention%20is%20necessary%20for%20audio%20classification%20tasks.%20By%0Aintroducing%20Audio%20Mamba%20%28AuM%29%2C%20the%20first%20self-attention-free%2C%20purely%20SSM-based%0Amodel%20for%20audio%20classification%2C%20we%20aim%20to%20address%20this%20question.%20We%20evaluate%0AAuM%20on%20various%20audio%20datasets%20-%20comprising%20six%20different%20benchmarks%20-%20where%20it%0Aachieves%20comparable%20or%20better%20performance%20compared%20to%20well-established%20AST%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03344v1&entry.124074799=Read"},
{"title": "Transfer Learning for Latent Variable Network Models", "author": "Akhil Jalan and Arya Mazumdar and Soumendu Sundar Mukherjee and Purnamrita Sarkar", "abstract": "  We study transfer learning for estimation in latent variable network models.\nIn our setting, the conditional edge probability matrices given the latent\nvariables are represented by $P$ for the source and $Q$ for the target. We wish\nto estimate $Q$ given two kinds of data: (1) edge data from a subgraph induced\nby an $o(1)$ fraction of the nodes of $Q$, and (2) edge data from all of $P$.\nIf the source $P$ has no relation to the target $Q$, the estimation error must\nbe $\\Omega(1)$. However, we show that if the latent variables are shared, then\nvanishing error is possible. We give an efficient algorithm that utilizes the\nordering of a suitably defined graph distance. Our algorithm achieves $o(1)$\nerror and does not assume a parametric form on the source or target networks.\nNext, for the specific case of Stochastic Block Models we prove a minimax lower\nbound and show that a simple algorithm achieves this rate. Finally, we\nempirically demonstrate our algorithm's use on real-world and simulated graph\ntransfer problems.\n", "link": "http://arxiv.org/abs/2406.03437v1", "date": "2024-06-05", "relevancy": 1.9658, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5106}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5029}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20for%20Latent%20Variable%20Network%20Models&body=Title%3A%20Transfer%20Learning%20for%20Latent%20Variable%20Network%20Models%0AAuthor%3A%20Akhil%20Jalan%20and%20Arya%20Mazumdar%20and%20Soumendu%20Sundar%20Mukherjee%20and%20Purnamrita%20Sarkar%0AAbstract%3A%20%20%20We%20study%20transfer%20learning%20for%20estimation%20in%20latent%20variable%20network%20models.%0AIn%20our%20setting%2C%20the%20conditional%20edge%20probability%20matrices%20given%20the%20latent%0Avariables%20are%20represented%20by%20%24P%24%20for%20the%20source%20and%20%24Q%24%20for%20the%20target.%20We%20wish%0Ato%20estimate%20%24Q%24%20given%20two%20kinds%20of%20data%3A%20%281%29%20edge%20data%20from%20a%20subgraph%20induced%0Aby%20an%20%24o%281%29%24%20fraction%20of%20the%20nodes%20of%20%24Q%24%2C%20and%20%282%29%20edge%20data%20from%20all%20of%20%24P%24.%0AIf%20the%20source%20%24P%24%20has%20no%20relation%20to%20the%20target%20%24Q%24%2C%20the%20estimation%20error%20must%0Abe%20%24%5COmega%281%29%24.%20However%2C%20we%20show%20that%20if%20the%20latent%20variables%20are%20shared%2C%20then%0Avanishing%20error%20is%20possible.%20We%20give%20an%20efficient%20algorithm%20that%20utilizes%20the%0Aordering%20of%20a%20suitably%20defined%20graph%20distance.%20Our%20algorithm%20achieves%20%24o%281%29%24%0Aerror%20and%20does%20not%20assume%20a%20parametric%20form%20on%20the%20source%20or%20target%20networks.%0ANext%2C%20for%20the%20specific%20case%20of%20Stochastic%20Block%20Models%20we%20prove%20a%20minimax%20lower%0Abound%20and%20show%20that%20a%20simple%20algorithm%20achieves%20this%20rate.%20Finally%2C%20we%0Aempirically%20demonstrate%20our%20algorithm%27s%20use%20on%20real-world%20and%20simulated%20graph%0Atransfer%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520for%2520Latent%2520Variable%2520Network%2520Models%26entry.906535625%3DAkhil%2520Jalan%2520and%2520Arya%2520Mazumdar%2520and%2520Soumendu%2520Sundar%2520Mukherjee%2520and%2520Purnamrita%2520Sarkar%26entry.1292438233%3D%2520%2520We%2520study%2520transfer%2520learning%2520for%2520estimation%2520in%2520latent%2520variable%2520network%2520models.%250AIn%2520our%2520setting%252C%2520the%2520conditional%2520edge%2520probability%2520matrices%2520given%2520the%2520latent%250Avariables%2520are%2520represented%2520by%2520%2524P%2524%2520for%2520the%2520source%2520and%2520%2524Q%2524%2520for%2520the%2520target.%2520We%2520wish%250Ato%2520estimate%2520%2524Q%2524%2520given%2520two%2520kinds%2520of%2520data%253A%2520%25281%2529%2520edge%2520data%2520from%2520a%2520subgraph%2520induced%250Aby%2520an%2520%2524o%25281%2529%2524%2520fraction%2520of%2520the%2520nodes%2520of%2520%2524Q%2524%252C%2520and%2520%25282%2529%2520edge%2520data%2520from%2520all%2520of%2520%2524P%2524.%250AIf%2520the%2520source%2520%2524P%2524%2520has%2520no%2520relation%2520to%2520the%2520target%2520%2524Q%2524%252C%2520the%2520estimation%2520error%2520must%250Abe%2520%2524%255COmega%25281%2529%2524.%2520However%252C%2520we%2520show%2520that%2520if%2520the%2520latent%2520variables%2520are%2520shared%252C%2520then%250Avanishing%2520error%2520is%2520possible.%2520We%2520give%2520an%2520efficient%2520algorithm%2520that%2520utilizes%2520the%250Aordering%2520of%2520a%2520suitably%2520defined%2520graph%2520distance.%2520Our%2520algorithm%2520achieves%2520%2524o%25281%2529%2524%250Aerror%2520and%2520does%2520not%2520assume%2520a%2520parametric%2520form%2520on%2520the%2520source%2520or%2520target%2520networks.%250ANext%252C%2520for%2520the%2520specific%2520case%2520of%2520Stochastic%2520Block%2520Models%2520we%2520prove%2520a%2520minimax%2520lower%250Abound%2520and%2520show%2520that%2520a%2520simple%2520algorithm%2520achieves%2520this%2520rate.%2520Finally%252C%2520we%250Aempirically%2520demonstrate%2520our%2520algorithm%2527s%2520use%2520on%2520real-world%2520and%2520simulated%2520graph%250Atransfer%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20for%20Latent%20Variable%20Network%20Models&entry.906535625=Akhil%20Jalan%20and%20Arya%20Mazumdar%20and%20Soumendu%20Sundar%20Mukherjee%20and%20Purnamrita%20Sarkar&entry.1292438233=%20%20We%20study%20transfer%20learning%20for%20estimation%20in%20latent%20variable%20network%20models.%0AIn%20our%20setting%2C%20the%20conditional%20edge%20probability%20matrices%20given%20the%20latent%0Avariables%20are%20represented%20by%20%24P%24%20for%20the%20source%20and%20%24Q%24%20for%20the%20target.%20We%20wish%0Ato%20estimate%20%24Q%24%20given%20two%20kinds%20of%20data%3A%20%281%29%20edge%20data%20from%20a%20subgraph%20induced%0Aby%20an%20%24o%281%29%24%20fraction%20of%20the%20nodes%20of%20%24Q%24%2C%20and%20%282%29%20edge%20data%20from%20all%20of%20%24P%24.%0AIf%20the%20source%20%24P%24%20has%20no%20relation%20to%20the%20target%20%24Q%24%2C%20the%20estimation%20error%20must%0Abe%20%24%5COmega%281%29%24.%20However%2C%20we%20show%20that%20if%20the%20latent%20variables%20are%20shared%2C%20then%0Avanishing%20error%20is%20possible.%20We%20give%20an%20efficient%20algorithm%20that%20utilizes%20the%0Aordering%20of%20a%20suitably%20defined%20graph%20distance.%20Our%20algorithm%20achieves%20%24o%281%29%24%0Aerror%20and%20does%20not%20assume%20a%20parametric%20form%20on%20the%20source%20or%20target%20networks.%0ANext%2C%20for%20the%20specific%20case%20of%20Stochastic%20Block%20Models%20we%20prove%20a%20minimax%20lower%0Abound%20and%20show%20that%20a%20simple%20algorithm%20achieves%20this%20rate.%20Finally%2C%20we%0Aempirically%20demonstrate%20our%20algorithm%27s%20use%20on%20real-world%20and%20simulated%20graph%0Atransfer%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03437v1&entry.124074799=Read"},
{"title": "Noisy Data Visualization using Functional Data Analysis", "author": "Haozhe Chen and Andres Felipe Duque Correa and Guy Wolf and Kevin R. Moon", "abstract": "  Data visualization via dimensionality reduction is an important tool in\nexploratory data analysis. However, when the data are noisy, many existing\nmethods fail to capture the underlying structure of the data. The method called\nEmpirical Intrinsic Geometry (EIG) was previously proposed for performing\ndimensionality reduction on high dimensional dynamical processes while\ntheoretically eliminating all noise. However, implementing EIG in practice\nrequires the construction of high-dimensional histograms, which suffer from the\ncurse of dimensionality. Here we propose a new data visualization method called\nFunctional Information Geometry (FIG) for dynamical processes that adapts the\nEIG framework while using approaches from functional data analysis to mitigate\nthe curse of dimensionality. We experimentally demonstrate that the resulting\nmethod outperforms a variant of EIG designed for visualization in terms of\ncapturing the true structure, hyperparameter robustness, and computational\nspeed. We then use our method to visualize EEG brain measurements of sleep\nactivity.\n", "link": "http://arxiv.org/abs/2406.03396v1", "date": "2024-06-05", "relevancy": 1.9647, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4993}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4887}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noisy%20Data%20Visualization%20using%20Functional%20Data%20Analysis&body=Title%3A%20Noisy%20Data%20Visualization%20using%20Functional%20Data%20Analysis%0AAuthor%3A%20Haozhe%20Chen%20and%20Andres%20Felipe%20Duque%20Correa%20and%20Guy%20Wolf%20and%20Kevin%20R.%20Moon%0AAbstract%3A%20%20%20Data%20visualization%20via%20dimensionality%20reduction%20is%20an%20important%20tool%20in%0Aexploratory%20data%20analysis.%20However%2C%20when%20the%20data%20are%20noisy%2C%20many%20existing%0Amethods%20fail%20to%20capture%20the%20underlying%20structure%20of%20the%20data.%20The%20method%20called%0AEmpirical%20Intrinsic%20Geometry%20%28EIG%29%20was%20previously%20proposed%20for%20performing%0Adimensionality%20reduction%20on%20high%20dimensional%20dynamical%20processes%20while%0Atheoretically%20eliminating%20all%20noise.%20However%2C%20implementing%20EIG%20in%20practice%0Arequires%20the%20construction%20of%20high-dimensional%20histograms%2C%20which%20suffer%20from%20the%0Acurse%20of%20dimensionality.%20Here%20we%20propose%20a%20new%20data%20visualization%20method%20called%0AFunctional%20Information%20Geometry%20%28FIG%29%20for%20dynamical%20processes%20that%20adapts%20the%0AEIG%20framework%20while%20using%20approaches%20from%20functional%20data%20analysis%20to%20mitigate%0Athe%20curse%20of%20dimensionality.%20We%20experimentally%20demonstrate%20that%20the%20resulting%0Amethod%20outperforms%20a%20variant%20of%20EIG%20designed%20for%20visualization%20in%20terms%20of%0Acapturing%20the%20true%20structure%2C%20hyperparameter%20robustness%2C%20and%20computational%0Aspeed.%20We%20then%20use%20our%20method%20to%20visualize%20EEG%20brain%20measurements%20of%20sleep%0Aactivity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoisy%2520Data%2520Visualization%2520using%2520Functional%2520Data%2520Analysis%26entry.906535625%3DHaozhe%2520Chen%2520and%2520Andres%2520Felipe%2520Duque%2520Correa%2520and%2520Guy%2520Wolf%2520and%2520Kevin%2520R.%2520Moon%26entry.1292438233%3D%2520%2520Data%2520visualization%2520via%2520dimensionality%2520reduction%2520is%2520an%2520important%2520tool%2520in%250Aexploratory%2520data%2520analysis.%2520However%252C%2520when%2520the%2520data%2520are%2520noisy%252C%2520many%2520existing%250Amethods%2520fail%2520to%2520capture%2520the%2520underlying%2520structure%2520of%2520the%2520data.%2520The%2520method%2520called%250AEmpirical%2520Intrinsic%2520Geometry%2520%2528EIG%2529%2520was%2520previously%2520proposed%2520for%2520performing%250Adimensionality%2520reduction%2520on%2520high%2520dimensional%2520dynamical%2520processes%2520while%250Atheoretically%2520eliminating%2520all%2520noise.%2520However%252C%2520implementing%2520EIG%2520in%2520practice%250Arequires%2520the%2520construction%2520of%2520high-dimensional%2520histograms%252C%2520which%2520suffer%2520from%2520the%250Acurse%2520of%2520dimensionality.%2520Here%2520we%2520propose%2520a%2520new%2520data%2520visualization%2520method%2520called%250AFunctional%2520Information%2520Geometry%2520%2528FIG%2529%2520for%2520dynamical%2520processes%2520that%2520adapts%2520the%250AEIG%2520framework%2520while%2520using%2520approaches%2520from%2520functional%2520data%2520analysis%2520to%2520mitigate%250Athe%2520curse%2520of%2520dimensionality.%2520We%2520experimentally%2520demonstrate%2520that%2520the%2520resulting%250Amethod%2520outperforms%2520a%2520variant%2520of%2520EIG%2520designed%2520for%2520visualization%2520in%2520terms%2520of%250Acapturing%2520the%2520true%2520structure%252C%2520hyperparameter%2520robustness%252C%2520and%2520computational%250Aspeed.%2520We%2520then%2520use%2520our%2520method%2520to%2520visualize%2520EEG%2520brain%2520measurements%2520of%2520sleep%250Aactivity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noisy%20Data%20Visualization%20using%20Functional%20Data%20Analysis&entry.906535625=Haozhe%20Chen%20and%20Andres%20Felipe%20Duque%20Correa%20and%20Guy%20Wolf%20and%20Kevin%20R.%20Moon&entry.1292438233=%20%20Data%20visualization%20via%20dimensionality%20reduction%20is%20an%20important%20tool%20in%0Aexploratory%20data%20analysis.%20However%2C%20when%20the%20data%20are%20noisy%2C%20many%20existing%0Amethods%20fail%20to%20capture%20the%20underlying%20structure%20of%20the%20data.%20The%20method%20called%0AEmpirical%20Intrinsic%20Geometry%20%28EIG%29%20was%20previously%20proposed%20for%20performing%0Adimensionality%20reduction%20on%20high%20dimensional%20dynamical%20processes%20while%0Atheoretically%20eliminating%20all%20noise.%20However%2C%20implementing%20EIG%20in%20practice%0Arequires%20the%20construction%20of%20high-dimensional%20histograms%2C%20which%20suffer%20from%20the%0Acurse%20of%20dimensionality.%20Here%20we%20propose%20a%20new%20data%20visualization%20method%20called%0AFunctional%20Information%20Geometry%20%28FIG%29%20for%20dynamical%20processes%20that%20adapts%20the%0AEIG%20framework%20while%20using%20approaches%20from%20functional%20data%20analysis%20to%20mitigate%0Athe%20curse%20of%20dimensionality.%20We%20experimentally%20demonstrate%20that%20the%20resulting%0Amethod%20outperforms%20a%20variant%20of%20EIG%20designed%20for%20visualization%20in%20terms%20of%0Acapturing%20the%20true%20structure%2C%20hyperparameter%20robustness%2C%20and%20computational%0Aspeed.%20We%20then%20use%20our%20method%20to%20visualize%20EEG%20brain%20measurements%20of%20sleep%0Aactivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03396v1&entry.124074799=Read"},
{"title": "Text-to-Image Rectified Flow as Plug-and-Play Priors", "author": "Xiaofeng Yang and Cheng Chen and Xulei Yang and Fayao Liu and Guosheng Lin", "abstract": "  Large-scale diffusion models have achieved remarkable performance in\ngenerative tasks. Beyond their initial training applications, these models have\nproven their ability to function as versatile plug-and-play priors. For\ninstance, 2D diffusion models can serve as loss functions to optimize 3D\nimplicit models. Rectified flow, a novel class of generative models, enforces a\nlinear progression from the source to the target distribution and has\ndemonstrated superior performance across various domains. Compared to\ndiffusion-based methods, rectified flow approaches surpass in terms of\ngeneration quality and efficiency, requiring fewer inference steps. In this\nwork, we present theoretical and experimental evidence demonstrating that\nrectified flow based methods offer similar functionalities to diffusion models\n- they can also serve as effective priors. Besides the generative capabilities\nof diffusion priors, motivated by the unique time-symmetry properties of\nrectified flow models, a variant of our method can additionally perform image\ninversion. Experimentally, our rectified flow-based priors outperform their\ndiffusion counterparts - the SDS and VSD losses - in text-to-3D generation. Our\nmethod also displays competitive performance in image inversion and editing.\n", "link": "http://arxiv.org/abs/2406.03293v1", "date": "2024-06-05", "relevancy": 1.9633, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7993}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6302}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-Image%20Rectified%20Flow%20as%20Plug-and-Play%20Priors&body=Title%3A%20Text-to-Image%20Rectified%20Flow%20as%20Plug-and-Play%20Priors%0AAuthor%3A%20Xiaofeng%20Yang%20and%20Cheng%20Chen%20and%20Xulei%20Yang%20and%20Fayao%20Liu%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20Large-scale%20diffusion%20models%20have%20achieved%20remarkable%20performance%20in%0Agenerative%20tasks.%20Beyond%20their%20initial%20training%20applications%2C%20these%20models%20have%0Aproven%20their%20ability%20to%20function%20as%20versatile%20plug-and-play%20priors.%20For%0Ainstance%2C%202D%20diffusion%20models%20can%20serve%20as%20loss%20functions%20to%20optimize%203D%0Aimplicit%20models.%20Rectified%20flow%2C%20a%20novel%20class%20of%20generative%20models%2C%20enforces%20a%0Alinear%20progression%20from%20the%20source%20to%20the%20target%20distribution%20and%20has%0Ademonstrated%20superior%20performance%20across%20various%20domains.%20Compared%20to%0Adiffusion-based%20methods%2C%20rectified%20flow%20approaches%20surpass%20in%20terms%20of%0Ageneration%20quality%20and%20efficiency%2C%20requiring%20fewer%20inference%20steps.%20In%20this%0Awork%2C%20we%20present%20theoretical%20and%20experimental%20evidence%20demonstrating%20that%0Arectified%20flow%20based%20methods%20offer%20similar%20functionalities%20to%20diffusion%20models%0A-%20they%20can%20also%20serve%20as%20effective%20priors.%20Besides%20the%20generative%20capabilities%0Aof%20diffusion%20priors%2C%20motivated%20by%20the%20unique%20time-symmetry%20properties%20of%0Arectified%20flow%20models%2C%20a%20variant%20of%20our%20method%20can%20additionally%20perform%20image%0Ainversion.%20Experimentally%2C%20our%20rectified%20flow-based%20priors%20outperform%20their%0Adiffusion%20counterparts%20-%20the%20SDS%20and%20VSD%20losses%20-%20in%20text-to-3D%20generation.%20Our%0Amethod%20also%20displays%20competitive%20performance%20in%20image%20inversion%20and%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-Image%2520Rectified%2520Flow%2520as%2520Plug-and-Play%2520Priors%26entry.906535625%3DXiaofeng%2520Yang%2520and%2520Cheng%2520Chen%2520and%2520Xulei%2520Yang%2520and%2520Fayao%2520Liu%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520Large-scale%2520diffusion%2520models%2520have%2520achieved%2520remarkable%2520performance%2520in%250Agenerative%2520tasks.%2520Beyond%2520their%2520initial%2520training%2520applications%252C%2520these%2520models%2520have%250Aproven%2520their%2520ability%2520to%2520function%2520as%2520versatile%2520plug-and-play%2520priors.%2520For%250Ainstance%252C%25202D%2520diffusion%2520models%2520can%2520serve%2520as%2520loss%2520functions%2520to%2520optimize%25203D%250Aimplicit%2520models.%2520Rectified%2520flow%252C%2520a%2520novel%2520class%2520of%2520generative%2520models%252C%2520enforces%2520a%250Alinear%2520progression%2520from%2520the%2520source%2520to%2520the%2520target%2520distribution%2520and%2520has%250Ademonstrated%2520superior%2520performance%2520across%2520various%2520domains.%2520Compared%2520to%250Adiffusion-based%2520methods%252C%2520rectified%2520flow%2520approaches%2520surpass%2520in%2520terms%2520of%250Ageneration%2520quality%2520and%2520efficiency%252C%2520requiring%2520fewer%2520inference%2520steps.%2520In%2520this%250Awork%252C%2520we%2520present%2520theoretical%2520and%2520experimental%2520evidence%2520demonstrating%2520that%250Arectified%2520flow%2520based%2520methods%2520offer%2520similar%2520functionalities%2520to%2520diffusion%2520models%250A-%2520they%2520can%2520also%2520serve%2520as%2520effective%2520priors.%2520Besides%2520the%2520generative%2520capabilities%250Aof%2520diffusion%2520priors%252C%2520motivated%2520by%2520the%2520unique%2520time-symmetry%2520properties%2520of%250Arectified%2520flow%2520models%252C%2520a%2520variant%2520of%2520our%2520method%2520can%2520additionally%2520perform%2520image%250Ainversion.%2520Experimentally%252C%2520our%2520rectified%2520flow-based%2520priors%2520outperform%2520their%250Adiffusion%2520counterparts%2520-%2520the%2520SDS%2520and%2520VSD%2520losses%2520-%2520in%2520text-to-3D%2520generation.%2520Our%250Amethod%2520also%2520displays%2520competitive%2520performance%2520in%2520image%2520inversion%2520and%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-Image%20Rectified%20Flow%20as%20Plug-and-Play%20Priors&entry.906535625=Xiaofeng%20Yang%20and%20Cheng%20Chen%20and%20Xulei%20Yang%20and%20Fayao%20Liu%20and%20Guosheng%20Lin&entry.1292438233=%20%20Large-scale%20diffusion%20models%20have%20achieved%20remarkable%20performance%20in%0Agenerative%20tasks.%20Beyond%20their%20initial%20training%20applications%2C%20these%20models%20have%0Aproven%20their%20ability%20to%20function%20as%20versatile%20plug-and-play%20priors.%20For%0Ainstance%2C%202D%20diffusion%20models%20can%20serve%20as%20loss%20functions%20to%20optimize%203D%0Aimplicit%20models.%20Rectified%20flow%2C%20a%20novel%20class%20of%20generative%20models%2C%20enforces%20a%0Alinear%20progression%20from%20the%20source%20to%20the%20target%20distribution%20and%20has%0Ademonstrated%20superior%20performance%20across%20various%20domains.%20Compared%20to%0Adiffusion-based%20methods%2C%20rectified%20flow%20approaches%20surpass%20in%20terms%20of%0Ageneration%20quality%20and%20efficiency%2C%20requiring%20fewer%20inference%20steps.%20In%20this%0Awork%2C%20we%20present%20theoretical%20and%20experimental%20evidence%20demonstrating%20that%0Arectified%20flow%20based%20methods%20offer%20similar%20functionalities%20to%20diffusion%20models%0A-%20they%20can%20also%20serve%20as%20effective%20priors.%20Besides%20the%20generative%20capabilities%0Aof%20diffusion%20priors%2C%20motivated%20by%20the%20unique%20time-symmetry%20properties%20of%0Arectified%20flow%20models%2C%20a%20variant%20of%20our%20method%20can%20additionally%20perform%20image%0Ainversion.%20Experimentally%2C%20our%20rectified%20flow-based%20priors%20outperform%20their%0Adiffusion%20counterparts%20-%20the%20SDS%20and%20VSD%20losses%20-%20in%20text-to-3D%20generation.%20Our%0Amethod%20also%20displays%20competitive%20performance%20in%20image%20inversion%20and%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03293v1&entry.124074799=Read"},
{"title": "PartialFormer: Modeling Part Instead of Whole for Machine Translation", "author": "Tong Zheng and Bei Li and Huiwen Bao and Jiale Wang and Weiqiao Shan and Tong Xiao and Jingbo Zhu", "abstract": "  The design choices in Transformer feed-forward neural networks have resulted\nin significant computational and parameter overhead. In this work, we emphasize\nthe importance of hidden dimensions in designing lightweight FFNs, a factor\noften overlooked in previous architectures. Guided by this principle, we\nintroduce PartialFormer, a parameter-efficient Transformer architecture\nutilizing multiple smaller FFNs to reduce parameters and computation while\nmaintaining essential hidden dimensions. These smaller FFNs are integrated into\na multi-head attention mechanism for effective collaboration. We also propose a\ntailored head scaling strategy to enhance PartialFormer's capabilities.\nFurthermore, we present a residual-like attention calculation to improve depth\nscaling within PartialFormer. Extensive experiments on 9 translation tasks and\n1 abstractive summarization task validate the effectiveness of our\nPartialFormer approach on machine translation and summarization tasks. Our code\nwould be available at: https://github.com/zhengkid/PartialFormer.\n", "link": "http://arxiv.org/abs/2310.14921v2", "date": "2024-06-05", "relevancy": 1.9629, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5366}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4859}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartialFormer%3A%20Modeling%20Part%20Instead%20of%20Whole%20for%20Machine%20Translation&body=Title%3A%20PartialFormer%3A%20Modeling%20Part%20Instead%20of%20Whole%20for%20Machine%20Translation%0AAuthor%3A%20Tong%20Zheng%20and%20Bei%20Li%20and%20Huiwen%20Bao%20and%20Jiale%20Wang%20and%20Weiqiao%20Shan%20and%20Tong%20Xiao%20and%20Jingbo%20Zhu%0AAbstract%3A%20%20%20The%20design%20choices%20in%20Transformer%20feed-forward%20neural%20networks%20have%20resulted%0Ain%20significant%20computational%20and%20parameter%20overhead.%20In%20this%20work%2C%20we%20emphasize%0Athe%20importance%20of%20hidden%20dimensions%20in%20designing%20lightweight%20FFNs%2C%20a%20factor%0Aoften%20overlooked%20in%20previous%20architectures.%20Guided%20by%20this%20principle%2C%20we%0Aintroduce%20PartialFormer%2C%20a%20parameter-efficient%20Transformer%20architecture%0Autilizing%20multiple%20smaller%20FFNs%20to%20reduce%20parameters%20and%20computation%20while%0Amaintaining%20essential%20hidden%20dimensions.%20These%20smaller%20FFNs%20are%20integrated%20into%0Aa%20multi-head%20attention%20mechanism%20for%20effective%20collaboration.%20We%20also%20propose%20a%0Atailored%20head%20scaling%20strategy%20to%20enhance%20PartialFormer%27s%20capabilities.%0AFurthermore%2C%20we%20present%20a%20residual-like%20attention%20calculation%20to%20improve%20depth%0Ascaling%20within%20PartialFormer.%20Extensive%20experiments%20on%209%20translation%20tasks%20and%0A1%20abstractive%20summarization%20task%20validate%20the%20effectiveness%20of%20our%0APartialFormer%20approach%20on%20machine%20translation%20and%20summarization%20tasks.%20Our%20code%0Awould%20be%20available%20at%3A%20https%3A//github.com/zhengkid/PartialFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14921v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartialFormer%253A%2520Modeling%2520Part%2520Instead%2520of%2520Whole%2520for%2520Machine%2520Translation%26entry.906535625%3DTong%2520Zheng%2520and%2520Bei%2520Li%2520and%2520Huiwen%2520Bao%2520and%2520Jiale%2520Wang%2520and%2520Weiqiao%2520Shan%2520and%2520Tong%2520Xiao%2520and%2520Jingbo%2520Zhu%26entry.1292438233%3D%2520%2520The%2520design%2520choices%2520in%2520Transformer%2520feed-forward%2520neural%2520networks%2520have%2520resulted%250Ain%2520significant%2520computational%2520and%2520parameter%2520overhead.%2520In%2520this%2520work%252C%2520we%2520emphasize%250Athe%2520importance%2520of%2520hidden%2520dimensions%2520in%2520designing%2520lightweight%2520FFNs%252C%2520a%2520factor%250Aoften%2520overlooked%2520in%2520previous%2520architectures.%2520Guided%2520by%2520this%2520principle%252C%2520we%250Aintroduce%2520PartialFormer%252C%2520a%2520parameter-efficient%2520Transformer%2520architecture%250Autilizing%2520multiple%2520smaller%2520FFNs%2520to%2520reduce%2520parameters%2520and%2520computation%2520while%250Amaintaining%2520essential%2520hidden%2520dimensions.%2520These%2520smaller%2520FFNs%2520are%2520integrated%2520into%250Aa%2520multi-head%2520attention%2520mechanism%2520for%2520effective%2520collaboration.%2520We%2520also%2520propose%2520a%250Atailored%2520head%2520scaling%2520strategy%2520to%2520enhance%2520PartialFormer%2527s%2520capabilities.%250AFurthermore%252C%2520we%2520present%2520a%2520residual-like%2520attention%2520calculation%2520to%2520improve%2520depth%250Ascaling%2520within%2520PartialFormer.%2520Extensive%2520experiments%2520on%25209%2520translation%2520tasks%2520and%250A1%2520abstractive%2520summarization%2520task%2520validate%2520the%2520effectiveness%2520of%2520our%250APartialFormer%2520approach%2520on%2520machine%2520translation%2520and%2520summarization%2520tasks.%2520Our%2520code%250Awould%2520be%2520available%2520at%253A%2520https%253A//github.com/zhengkid/PartialFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.14921v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartialFormer%3A%20Modeling%20Part%20Instead%20of%20Whole%20for%20Machine%20Translation&entry.906535625=Tong%20Zheng%20and%20Bei%20Li%20and%20Huiwen%20Bao%20and%20Jiale%20Wang%20and%20Weiqiao%20Shan%20and%20Tong%20Xiao%20and%20Jingbo%20Zhu&entry.1292438233=%20%20The%20design%20choices%20in%20Transformer%20feed-forward%20neural%20networks%20have%20resulted%0Ain%20significant%20computational%20and%20parameter%20overhead.%20In%20this%20work%2C%20we%20emphasize%0Athe%20importance%20of%20hidden%20dimensions%20in%20designing%20lightweight%20FFNs%2C%20a%20factor%0Aoften%20overlooked%20in%20previous%20architectures.%20Guided%20by%20this%20principle%2C%20we%0Aintroduce%20PartialFormer%2C%20a%20parameter-efficient%20Transformer%20architecture%0Autilizing%20multiple%20smaller%20FFNs%20to%20reduce%20parameters%20and%20computation%20while%0Amaintaining%20essential%20hidden%20dimensions.%20These%20smaller%20FFNs%20are%20integrated%20into%0Aa%20multi-head%20attention%20mechanism%20for%20effective%20collaboration.%20We%20also%20propose%20a%0Atailored%20head%20scaling%20strategy%20to%20enhance%20PartialFormer%27s%20capabilities.%0AFurthermore%2C%20we%20present%20a%20residual-like%20attention%20calculation%20to%20improve%20depth%0Ascaling%20within%20PartialFormer.%20Extensive%20experiments%20on%209%20translation%20tasks%20and%0A1%20abstractive%20summarization%20task%20validate%20the%20effectiveness%20of%20our%0APartialFormer%20approach%20on%20machine%20translation%20and%20summarization%20tasks.%20Our%20code%0Awould%20be%20available%20at%3A%20https%3A//github.com/zhengkid/PartialFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14921v2&entry.124074799=Read"},
{"title": "NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for\n  Large Language Models", "author": "Amit Dhurandhar and Tejaswini Pedapati and Ronny Luss and Soham Dan and Aurelie Lozano and Payel Das and Georgios Kollias", "abstract": "  Transformer-based Language Models have become ubiquitous in Natural Language\nProcessing (NLP) due to their impressive performance on various tasks. However,\nexpensive training as well as inference remains a significant impediment to\ntheir widespread applicability. While enforcing sparsity at various levels of\nthe model architecture has found promise in addressing scaling and efficiency\nissues, there remains a disconnect between how sparsity affects network\ntopology. Inspired by brain neuronal networks, we explore sparsity approaches\nthrough the lens of network topology. Specifically, we exploit mechanisms seen\nin biological networks, such as preferential attachment and redundant synapse\npruning, and show that principled, model-agnostic sparsity approaches are\nperformant and efficient across diverse NLP tasks, spanning both classification\n(such as natural language inference) and generation (summarization, machine\ntranslation), despite our sole objective not being optimizing performance.\nNeuroPrune is competitive with (or sometimes superior to) baselines on\nperformance and can be up to $10$x faster in terms of training time for a given\nlevel of sparsity, simultaneously exhibiting measurable improvements in\ninference time in many cases.\n", "link": "http://arxiv.org/abs/2404.01306v3", "date": "2024-06-05", "relevancy": 1.9616, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5071}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4884}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroPrune%3A%20A%20Neuro-inspired%20Topological%20Sparse%20Training%20Algorithm%20for%0A%20%20Large%20Language%20Models&body=Title%3A%20NeuroPrune%3A%20A%20Neuro-inspired%20Topological%20Sparse%20Training%20Algorithm%20for%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Amit%20Dhurandhar%20and%20Tejaswini%20Pedapati%20and%20Ronny%20Luss%20and%20Soham%20Dan%20and%20Aurelie%20Lozano%20and%20Payel%20Das%20and%20Georgios%20Kollias%0AAbstract%3A%20%20%20Transformer-based%20Language%20Models%20have%20become%20ubiquitous%20in%20Natural%20Language%0AProcessing%20%28NLP%29%20due%20to%20their%20impressive%20performance%20on%20various%20tasks.%20However%2C%0Aexpensive%20training%20as%20well%20as%20inference%20remains%20a%20significant%20impediment%20to%0Atheir%20widespread%20applicability.%20While%20enforcing%20sparsity%20at%20various%20levels%20of%0Athe%20model%20architecture%20has%20found%20promise%20in%20addressing%20scaling%20and%20efficiency%0Aissues%2C%20there%20remains%20a%20disconnect%20between%20how%20sparsity%20affects%20network%0Atopology.%20Inspired%20by%20brain%20neuronal%20networks%2C%20we%20explore%20sparsity%20approaches%0Athrough%20the%20lens%20of%20network%20topology.%20Specifically%2C%20we%20exploit%20mechanisms%20seen%0Ain%20biological%20networks%2C%20such%20as%20preferential%20attachment%20and%20redundant%20synapse%0Apruning%2C%20and%20show%20that%20principled%2C%20model-agnostic%20sparsity%20approaches%20are%0Aperformant%20and%20efficient%20across%20diverse%20NLP%20tasks%2C%20spanning%20both%20classification%0A%28such%20as%20natural%20language%20inference%29%20and%20generation%20%28summarization%2C%20machine%0Atranslation%29%2C%20despite%20our%20sole%20objective%20not%20being%20optimizing%20performance.%0ANeuroPrune%20is%20competitive%20with%20%28or%20sometimes%20superior%20to%29%20baselines%20on%0Aperformance%20and%20can%20be%20up%20to%20%2410%24x%20faster%20in%20terms%20of%20training%20time%20for%20a%20given%0Alevel%20of%20sparsity%2C%20simultaneously%20exhibiting%20measurable%20improvements%20in%0Ainference%20time%20in%20many%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01306v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroPrune%253A%2520A%2520Neuro-inspired%2520Topological%2520Sparse%2520Training%2520Algorithm%2520for%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DAmit%2520Dhurandhar%2520and%2520Tejaswini%2520Pedapati%2520and%2520Ronny%2520Luss%2520and%2520Soham%2520Dan%2520and%2520Aurelie%2520Lozano%2520and%2520Payel%2520Das%2520and%2520Georgios%2520Kollias%26entry.1292438233%3D%2520%2520Transformer-based%2520Language%2520Models%2520have%2520become%2520ubiquitous%2520in%2520Natural%2520Language%250AProcessing%2520%2528NLP%2529%2520due%2520to%2520their%2520impressive%2520performance%2520on%2520various%2520tasks.%2520However%252C%250Aexpensive%2520training%2520as%2520well%2520as%2520inference%2520remains%2520a%2520significant%2520impediment%2520to%250Atheir%2520widespread%2520applicability.%2520While%2520enforcing%2520sparsity%2520at%2520various%2520levels%2520of%250Athe%2520model%2520architecture%2520has%2520found%2520promise%2520in%2520addressing%2520scaling%2520and%2520efficiency%250Aissues%252C%2520there%2520remains%2520a%2520disconnect%2520between%2520how%2520sparsity%2520affects%2520network%250Atopology.%2520Inspired%2520by%2520brain%2520neuronal%2520networks%252C%2520we%2520explore%2520sparsity%2520approaches%250Athrough%2520the%2520lens%2520of%2520network%2520topology.%2520Specifically%252C%2520we%2520exploit%2520mechanisms%2520seen%250Ain%2520biological%2520networks%252C%2520such%2520as%2520preferential%2520attachment%2520and%2520redundant%2520synapse%250Apruning%252C%2520and%2520show%2520that%2520principled%252C%2520model-agnostic%2520sparsity%2520approaches%2520are%250Aperformant%2520and%2520efficient%2520across%2520diverse%2520NLP%2520tasks%252C%2520spanning%2520both%2520classification%250A%2528such%2520as%2520natural%2520language%2520inference%2529%2520and%2520generation%2520%2528summarization%252C%2520machine%250Atranslation%2529%252C%2520despite%2520our%2520sole%2520objective%2520not%2520being%2520optimizing%2520performance.%250ANeuroPrune%2520is%2520competitive%2520with%2520%2528or%2520sometimes%2520superior%2520to%2529%2520baselines%2520on%250Aperformance%2520and%2520can%2520be%2520up%2520to%2520%252410%2524x%2520faster%2520in%2520terms%2520of%2520training%2520time%2520for%2520a%2520given%250Alevel%2520of%2520sparsity%252C%2520simultaneously%2520exhibiting%2520measurable%2520improvements%2520in%250Ainference%2520time%2520in%2520many%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01306v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroPrune%3A%20A%20Neuro-inspired%20Topological%20Sparse%20Training%20Algorithm%20for%0A%20%20Large%20Language%20Models&entry.906535625=Amit%20Dhurandhar%20and%20Tejaswini%20Pedapati%20and%20Ronny%20Luss%20and%20Soham%20Dan%20and%20Aurelie%20Lozano%20and%20Payel%20Das%20and%20Georgios%20Kollias&entry.1292438233=%20%20Transformer-based%20Language%20Models%20have%20become%20ubiquitous%20in%20Natural%20Language%0AProcessing%20%28NLP%29%20due%20to%20their%20impressive%20performance%20on%20various%20tasks.%20However%2C%0Aexpensive%20training%20as%20well%20as%20inference%20remains%20a%20significant%20impediment%20to%0Atheir%20widespread%20applicability.%20While%20enforcing%20sparsity%20at%20various%20levels%20of%0Athe%20model%20architecture%20has%20found%20promise%20in%20addressing%20scaling%20and%20efficiency%0Aissues%2C%20there%20remains%20a%20disconnect%20between%20how%20sparsity%20affects%20network%0Atopology.%20Inspired%20by%20brain%20neuronal%20networks%2C%20we%20explore%20sparsity%20approaches%0Athrough%20the%20lens%20of%20network%20topology.%20Specifically%2C%20we%20exploit%20mechanisms%20seen%0Ain%20biological%20networks%2C%20such%20as%20preferential%20attachment%20and%20redundant%20synapse%0Apruning%2C%20and%20show%20that%20principled%2C%20model-agnostic%20sparsity%20approaches%20are%0Aperformant%20and%20efficient%20across%20diverse%20NLP%20tasks%2C%20spanning%20both%20classification%0A%28such%20as%20natural%20language%20inference%29%20and%20generation%20%28summarization%2C%20machine%0Atranslation%29%2C%20despite%20our%20sole%20objective%20not%20being%20optimizing%20performance.%0ANeuroPrune%20is%20competitive%20with%20%28or%20sometimes%20superior%20to%29%20baselines%20on%0Aperformance%20and%20can%20be%20up%20to%20%2410%24x%20faster%20in%20terms%20of%20training%20time%20for%20a%20given%0Alevel%20of%20sparsity%2C%20simultaneously%20exhibiting%20measurable%20improvements%20in%0Ainference%20time%20in%20many%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01306v3&entry.124074799=Read"},
{"title": "Feature learning in finite-width Bayesian deep linear networks with\n  multiple outputs and convolutional layers", "author": "Federico Bassetti and Marco Gherardi and Alessandro Ingrosso and Mauro Pastore and Pietro Rotondo", "abstract": "  Deep linear networks have been extensively studied, as they provide\nsimplified models of deep learning. However, little is known in the case of\nfinite-width architectures with multiple outputs and convolutional layers. In\nthis manuscript, we provide rigorous results for the statistics of functions\nimplemented by the aforementioned class of networks, thus moving closer to a\ncomplete characterization of feature learning in the Bayesian setting. Our\nresults include: (i) an exact and elementary non-asymptotic integral\nrepresentation for the joint prior distribution over the outputs, given in\nterms of a mixture of Gaussians; (ii) an analytical formula for the posterior\ndistribution in the case of squared error loss function (Gaussian likelihood);\n(iii) a quantitative description of the feature learning infinite-width regime,\nusing large deviation theory. From a physical perspective, deep architectures\nwith multiple outputs or convolutional layers represent different\nmanifestations of kernel shape renormalization, and our work provides a\ndictionary that translates this physics intuition and terminology into rigorous\nBayesian statistics.\n", "link": "http://arxiv.org/abs/2406.03260v1", "date": "2024-06-05", "relevancy": 1.9587, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.534}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4854}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20learning%20in%20finite-width%20Bayesian%20deep%20linear%20networks%20with%0A%20%20multiple%20outputs%20and%20convolutional%20layers&body=Title%3A%20Feature%20learning%20in%20finite-width%20Bayesian%20deep%20linear%20networks%20with%0A%20%20multiple%20outputs%20and%20convolutional%20layers%0AAuthor%3A%20Federico%20Bassetti%20and%20Marco%20Gherardi%20and%20Alessandro%20Ingrosso%20and%20Mauro%20Pastore%20and%20Pietro%20Rotondo%0AAbstract%3A%20%20%20Deep%20linear%20networks%20have%20been%20extensively%20studied%2C%20as%20they%20provide%0Asimplified%20models%20of%20deep%20learning.%20However%2C%20little%20is%20known%20in%20the%20case%20of%0Afinite-width%20architectures%20with%20multiple%20outputs%20and%20convolutional%20layers.%20In%0Athis%20manuscript%2C%20we%20provide%20rigorous%20results%20for%20the%20statistics%20of%20functions%0Aimplemented%20by%20the%20aforementioned%20class%20of%20networks%2C%20thus%20moving%20closer%20to%20a%0Acomplete%20characterization%20of%20feature%20learning%20in%20the%20Bayesian%20setting.%20Our%0Aresults%20include%3A%20%28i%29%20an%20exact%20and%20elementary%20non-asymptotic%20integral%0Arepresentation%20for%20the%20joint%20prior%20distribution%20over%20the%20outputs%2C%20given%20in%0Aterms%20of%20a%20mixture%20of%20Gaussians%3B%20%28ii%29%20an%20analytical%20formula%20for%20the%20posterior%0Adistribution%20in%20the%20case%20of%20squared%20error%20loss%20function%20%28Gaussian%20likelihood%29%3B%0A%28iii%29%20a%20quantitative%20description%20of%20the%20feature%20learning%20infinite-width%20regime%2C%0Ausing%20large%20deviation%20theory.%20From%20a%20physical%20perspective%2C%20deep%20architectures%0Awith%20multiple%20outputs%20or%20convolutional%20layers%20represent%20different%0Amanifestations%20of%20kernel%20shape%20renormalization%2C%20and%20our%20work%20provides%20a%0Adictionary%20that%20translates%20this%20physics%20intuition%20and%20terminology%20into%20rigorous%0ABayesian%20statistics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520learning%2520in%2520finite-width%2520Bayesian%2520deep%2520linear%2520networks%2520with%250A%2520%2520multiple%2520outputs%2520and%2520convolutional%2520layers%26entry.906535625%3DFederico%2520Bassetti%2520and%2520Marco%2520Gherardi%2520and%2520Alessandro%2520Ingrosso%2520and%2520Mauro%2520Pastore%2520and%2520Pietro%2520Rotondo%26entry.1292438233%3D%2520%2520Deep%2520linear%2520networks%2520have%2520been%2520extensively%2520studied%252C%2520as%2520they%2520provide%250Asimplified%2520models%2520of%2520deep%2520learning.%2520However%252C%2520little%2520is%2520known%2520in%2520the%2520case%2520of%250Afinite-width%2520architectures%2520with%2520multiple%2520outputs%2520and%2520convolutional%2520layers.%2520In%250Athis%2520manuscript%252C%2520we%2520provide%2520rigorous%2520results%2520for%2520the%2520statistics%2520of%2520functions%250Aimplemented%2520by%2520the%2520aforementioned%2520class%2520of%2520networks%252C%2520thus%2520moving%2520closer%2520to%2520a%250Acomplete%2520characterization%2520of%2520feature%2520learning%2520in%2520the%2520Bayesian%2520setting.%2520Our%250Aresults%2520include%253A%2520%2528i%2529%2520an%2520exact%2520and%2520elementary%2520non-asymptotic%2520integral%250Arepresentation%2520for%2520the%2520joint%2520prior%2520distribution%2520over%2520the%2520outputs%252C%2520given%2520in%250Aterms%2520of%2520a%2520mixture%2520of%2520Gaussians%253B%2520%2528ii%2529%2520an%2520analytical%2520formula%2520for%2520the%2520posterior%250Adistribution%2520in%2520the%2520case%2520of%2520squared%2520error%2520loss%2520function%2520%2528Gaussian%2520likelihood%2529%253B%250A%2528iii%2529%2520a%2520quantitative%2520description%2520of%2520the%2520feature%2520learning%2520infinite-width%2520regime%252C%250Ausing%2520large%2520deviation%2520theory.%2520From%2520a%2520physical%2520perspective%252C%2520deep%2520architectures%250Awith%2520multiple%2520outputs%2520or%2520convolutional%2520layers%2520represent%2520different%250Amanifestations%2520of%2520kernel%2520shape%2520renormalization%252C%2520and%2520our%2520work%2520provides%2520a%250Adictionary%2520that%2520translates%2520this%2520physics%2520intuition%2520and%2520terminology%2520into%2520rigorous%250ABayesian%2520statistics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20learning%20in%20finite-width%20Bayesian%20deep%20linear%20networks%20with%0A%20%20multiple%20outputs%20and%20convolutional%20layers&entry.906535625=Federico%20Bassetti%20and%20Marco%20Gherardi%20and%20Alessandro%20Ingrosso%20and%20Mauro%20Pastore%20and%20Pietro%20Rotondo&entry.1292438233=%20%20Deep%20linear%20networks%20have%20been%20extensively%20studied%2C%20as%20they%20provide%0Asimplified%20models%20of%20deep%20learning.%20However%2C%20little%20is%20known%20in%20the%20case%20of%0Afinite-width%20architectures%20with%20multiple%20outputs%20and%20convolutional%20layers.%20In%0Athis%20manuscript%2C%20we%20provide%20rigorous%20results%20for%20the%20statistics%20of%20functions%0Aimplemented%20by%20the%20aforementioned%20class%20of%20networks%2C%20thus%20moving%20closer%20to%20a%0Acomplete%20characterization%20of%20feature%20learning%20in%20the%20Bayesian%20setting.%20Our%0Aresults%20include%3A%20%28i%29%20an%20exact%20and%20elementary%20non-asymptotic%20integral%0Arepresentation%20for%20the%20joint%20prior%20distribution%20over%20the%20outputs%2C%20given%20in%0Aterms%20of%20a%20mixture%20of%20Gaussians%3B%20%28ii%29%20an%20analytical%20formula%20for%20the%20posterior%0Adistribution%20in%20the%20case%20of%20squared%20error%20loss%20function%20%28Gaussian%20likelihood%29%3B%0A%28iii%29%20a%20quantitative%20description%20of%20the%20feature%20learning%20infinite-width%20regime%2C%0Ausing%20large%20deviation%20theory.%20From%20a%20physical%20perspective%2C%20deep%20architectures%0Awith%20multiple%20outputs%20or%20convolutional%20layers%20represent%20different%0Amanifestations%20of%20kernel%20shape%20renormalization%2C%20and%20our%20work%20provides%20a%0Adictionary%20that%20translates%20this%20physics%20intuition%20and%20terminology%20into%20rigorous%0ABayesian%20statistics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03260v1&entry.124074799=Read"},
{"title": "Graph Neural Network Explanations are Fragile", "author": "Jiate Li and Meng Pang and Yun Dong and Jinyuan Jia and Binghui Wang", "abstract": "  Explainable Graph Neural Network (GNN) has emerged recently to foster the\ntrust of using GNNs. Existing GNN explainers are developed from various\nperspectives to enhance the explanation performance. We take the first step to\nstudy GNN explainers under adversarial attack--We found that an adversary\nslightly perturbing graph structure can ensure GNN model makes correct\npredictions, but the GNN explainer yields a drastically different explanation\non the perturbed graph. Specifically, we first formulate the attack problem\nunder a practical threat model (i.e., the adversary has limited knowledge about\nthe GNN explainer and a restricted perturbation budget). We then design two\nmethods (i.e., one is loss-based and the other is deduction-based) to realize\nthe attack. We evaluate our attacks on various GNN explainers and the results\nshow these explainers are fragile.\n", "link": "http://arxiv.org/abs/2406.03193v1", "date": "2024-06-05", "relevancy": 1.9574, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4132}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3837}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Network%20Explanations%20are%20Fragile&body=Title%3A%20Graph%20Neural%20Network%20Explanations%20are%20Fragile%0AAuthor%3A%20Jiate%20Li%20and%20Meng%20Pang%20and%20Yun%20Dong%20and%20Jinyuan%20Jia%20and%20Binghui%20Wang%0AAbstract%3A%20%20%20Explainable%20Graph%20Neural%20Network%20%28GNN%29%20has%20emerged%20recently%20to%20foster%20the%0Atrust%20of%20using%20GNNs.%20Existing%20GNN%20explainers%20are%20developed%20from%20various%0Aperspectives%20to%20enhance%20the%20explanation%20performance.%20We%20take%20the%20first%20step%20to%0Astudy%20GNN%20explainers%20under%20adversarial%20attack--We%20found%20that%20an%20adversary%0Aslightly%20perturbing%20graph%20structure%20can%20ensure%20GNN%20model%20makes%20correct%0Apredictions%2C%20but%20the%20GNN%20explainer%20yields%20a%20drastically%20different%20explanation%0Aon%20the%20perturbed%20graph.%20Specifically%2C%20we%20first%20formulate%20the%20attack%20problem%0Aunder%20a%20practical%20threat%20model%20%28i.e.%2C%20the%20adversary%20has%20limited%20knowledge%20about%0Athe%20GNN%20explainer%20and%20a%20restricted%20perturbation%20budget%29.%20We%20then%20design%20two%0Amethods%20%28i.e.%2C%20one%20is%20loss-based%20and%20the%20other%20is%20deduction-based%29%20to%20realize%0Athe%20attack.%20We%20evaluate%20our%20attacks%20on%20various%20GNN%20explainers%20and%20the%20results%0Ashow%20these%20explainers%20are%20fragile.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Network%2520Explanations%2520are%2520Fragile%26entry.906535625%3DJiate%2520Li%2520and%2520Meng%2520Pang%2520and%2520Yun%2520Dong%2520and%2520Jinyuan%2520Jia%2520and%2520Binghui%2520Wang%26entry.1292438233%3D%2520%2520Explainable%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520has%2520emerged%2520recently%2520to%2520foster%2520the%250Atrust%2520of%2520using%2520GNNs.%2520Existing%2520GNN%2520explainers%2520are%2520developed%2520from%2520various%250Aperspectives%2520to%2520enhance%2520the%2520explanation%2520performance.%2520We%2520take%2520the%2520first%2520step%2520to%250Astudy%2520GNN%2520explainers%2520under%2520adversarial%2520attack--We%2520found%2520that%2520an%2520adversary%250Aslightly%2520perturbing%2520graph%2520structure%2520can%2520ensure%2520GNN%2520model%2520makes%2520correct%250Apredictions%252C%2520but%2520the%2520GNN%2520explainer%2520yields%2520a%2520drastically%2520different%2520explanation%250Aon%2520the%2520perturbed%2520graph.%2520Specifically%252C%2520we%2520first%2520formulate%2520the%2520attack%2520problem%250Aunder%2520a%2520practical%2520threat%2520model%2520%2528i.e.%252C%2520the%2520adversary%2520has%2520limited%2520knowledge%2520about%250Athe%2520GNN%2520explainer%2520and%2520a%2520restricted%2520perturbation%2520budget%2529.%2520We%2520then%2520design%2520two%250Amethods%2520%2528i.e.%252C%2520one%2520is%2520loss-based%2520and%2520the%2520other%2520is%2520deduction-based%2529%2520to%2520realize%250Athe%2520attack.%2520We%2520evaluate%2520our%2520attacks%2520on%2520various%2520GNN%2520explainers%2520and%2520the%2520results%250Ashow%2520these%2520explainers%2520are%2520fragile.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Network%20Explanations%20are%20Fragile&entry.906535625=Jiate%20Li%20and%20Meng%20Pang%20and%20Yun%20Dong%20and%20Jinyuan%20Jia%20and%20Binghui%20Wang&entry.1292438233=%20%20Explainable%20Graph%20Neural%20Network%20%28GNN%29%20has%20emerged%20recently%20to%20foster%20the%0Atrust%20of%20using%20GNNs.%20Existing%20GNN%20explainers%20are%20developed%20from%20various%0Aperspectives%20to%20enhance%20the%20explanation%20performance.%20We%20take%20the%20first%20step%20to%0Astudy%20GNN%20explainers%20under%20adversarial%20attack--We%20found%20that%20an%20adversary%0Aslightly%20perturbing%20graph%20structure%20can%20ensure%20GNN%20model%20makes%20correct%0Apredictions%2C%20but%20the%20GNN%20explainer%20yields%20a%20drastically%20different%20explanation%0Aon%20the%20perturbed%20graph.%20Specifically%2C%20we%20first%20formulate%20the%20attack%20problem%0Aunder%20a%20practical%20threat%20model%20%28i.e.%2C%20the%20adversary%20has%20limited%20knowledge%20about%0Athe%20GNN%20explainer%20and%20a%20restricted%20perturbation%20budget%29.%20We%20then%20design%20two%0Amethods%20%28i.e.%2C%20one%20is%20loss-based%20and%20the%20other%20is%20deduction-based%29%20to%20realize%0Athe%20attack.%20We%20evaluate%20our%20attacks%20on%20various%20GNN%20explainers%20and%20the%20results%0Ashow%20these%20explainers%20are%20fragile.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03193v1&entry.124074799=Read"},
{"title": "Listenable Maps for Audio Classifiers", "author": "Francesco Paissan and Mirco Ravanelli and Cem Subakan", "abstract": "  Despite the impressive performance of deep learning models across diverse\ntasks, their complexity poses challenges for interpretation. This challenge is\nparticularly evident for audio signals, where conveying interpretations becomes\ninherently difficult. To address this issue, we introduce Listenable Maps for\nAudio Classifiers (L-MAC), a posthoc interpretation method that generates\nfaithful and listenable interpretations. L-MAC utilizes a decoder on top of a\npretrained classifier to generate binary masks that highlight relevant portions\nof the input audio. We train the decoder with a loss function that maximizes\nthe confidence of the classifier decision on the masked-in portion of the audio\nwhile minimizing the probability of model output for the masked-out portion.\nQuantitative evaluations on both in-domain and out-of-domain data demonstrate\nthat L-MAC consistently produces more faithful interpretations than several\ngradient and masking-based methodologies. Furthermore, a user study confirms\nthat, on average, users prefer the interpretations generated by the proposed\ntechnique.\n", "link": "http://arxiv.org/abs/2403.13086v2", "date": "2024-06-05", "relevancy": 1.9554, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4927}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4873}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Listenable%20Maps%20for%20Audio%20Classifiers&body=Title%3A%20Listenable%20Maps%20for%20Audio%20Classifiers%0AAuthor%3A%20Francesco%20Paissan%20and%20Mirco%20Ravanelli%20and%20Cem%20Subakan%0AAbstract%3A%20%20%20Despite%20the%20impressive%20performance%20of%20deep%20learning%20models%20across%20diverse%0Atasks%2C%20their%20complexity%20poses%20challenges%20for%20interpretation.%20This%20challenge%20is%0Aparticularly%20evident%20for%20audio%20signals%2C%20where%20conveying%20interpretations%20becomes%0Ainherently%20difficult.%20To%20address%20this%20issue%2C%20we%20introduce%20Listenable%20Maps%20for%0AAudio%20Classifiers%20%28L-MAC%29%2C%20a%20posthoc%20interpretation%20method%20that%20generates%0Afaithful%20and%20listenable%20interpretations.%20L-MAC%20utilizes%20a%20decoder%20on%20top%20of%20a%0Apretrained%20classifier%20to%20generate%20binary%20masks%20that%20highlight%20relevant%20portions%0Aof%20the%20input%20audio.%20We%20train%20the%20decoder%20with%20a%20loss%20function%20that%20maximizes%0Athe%20confidence%20of%20the%20classifier%20decision%20on%20the%20masked-in%20portion%20of%20the%20audio%0Awhile%20minimizing%20the%20probability%20of%20model%20output%20for%20the%20masked-out%20portion.%0AQuantitative%20evaluations%20on%20both%20in-domain%20and%20out-of-domain%20data%20demonstrate%0Athat%20L-MAC%20consistently%20produces%20more%20faithful%20interpretations%20than%20several%0Agradient%20and%20masking-based%20methodologies.%20Furthermore%2C%20a%20user%20study%20confirms%0Athat%2C%20on%20average%2C%20users%20prefer%20the%20interpretations%20generated%20by%20the%20proposed%0Atechnique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DListenable%2520Maps%2520for%2520Audio%2520Classifiers%26entry.906535625%3DFrancesco%2520Paissan%2520and%2520Mirco%2520Ravanelli%2520and%2520Cem%2520Subakan%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520performance%2520of%2520deep%2520learning%2520models%2520across%2520diverse%250Atasks%252C%2520their%2520complexity%2520poses%2520challenges%2520for%2520interpretation.%2520This%2520challenge%2520is%250Aparticularly%2520evident%2520for%2520audio%2520signals%252C%2520where%2520conveying%2520interpretations%2520becomes%250Ainherently%2520difficult.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Listenable%2520Maps%2520for%250AAudio%2520Classifiers%2520%2528L-MAC%2529%252C%2520a%2520posthoc%2520interpretation%2520method%2520that%2520generates%250Afaithful%2520and%2520listenable%2520interpretations.%2520L-MAC%2520utilizes%2520a%2520decoder%2520on%2520top%2520of%2520a%250Apretrained%2520classifier%2520to%2520generate%2520binary%2520masks%2520that%2520highlight%2520relevant%2520portions%250Aof%2520the%2520input%2520audio.%2520We%2520train%2520the%2520decoder%2520with%2520a%2520loss%2520function%2520that%2520maximizes%250Athe%2520confidence%2520of%2520the%2520classifier%2520decision%2520on%2520the%2520masked-in%2520portion%2520of%2520the%2520audio%250Awhile%2520minimizing%2520the%2520probability%2520of%2520model%2520output%2520for%2520the%2520masked-out%2520portion.%250AQuantitative%2520evaluations%2520on%2520both%2520in-domain%2520and%2520out-of-domain%2520data%2520demonstrate%250Athat%2520L-MAC%2520consistently%2520produces%2520more%2520faithful%2520interpretations%2520than%2520several%250Agradient%2520and%2520masking-based%2520methodologies.%2520Furthermore%252C%2520a%2520user%2520study%2520confirms%250Athat%252C%2520on%2520average%252C%2520users%2520prefer%2520the%2520interpretations%2520generated%2520by%2520the%2520proposed%250Atechnique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Listenable%20Maps%20for%20Audio%20Classifiers&entry.906535625=Francesco%20Paissan%20and%20Mirco%20Ravanelli%20and%20Cem%20Subakan&entry.1292438233=%20%20Despite%20the%20impressive%20performance%20of%20deep%20learning%20models%20across%20diverse%0Atasks%2C%20their%20complexity%20poses%20challenges%20for%20interpretation.%20This%20challenge%20is%0Aparticularly%20evident%20for%20audio%20signals%2C%20where%20conveying%20interpretations%20becomes%0Ainherently%20difficult.%20To%20address%20this%20issue%2C%20we%20introduce%20Listenable%20Maps%20for%0AAudio%20Classifiers%20%28L-MAC%29%2C%20a%20posthoc%20interpretation%20method%20that%20generates%0Afaithful%20and%20listenable%20interpretations.%20L-MAC%20utilizes%20a%20decoder%20on%20top%20of%20a%0Apretrained%20classifier%20to%20generate%20binary%20masks%20that%20highlight%20relevant%20portions%0Aof%20the%20input%20audio.%20We%20train%20the%20decoder%20with%20a%20loss%20function%20that%20maximizes%0Athe%20confidence%20of%20the%20classifier%20decision%20on%20the%20masked-in%20portion%20of%20the%20audio%0Awhile%20minimizing%20the%20probability%20of%20model%20output%20for%20the%20masked-out%20portion.%0AQuantitative%20evaluations%20on%20both%20in-domain%20and%20out-of-domain%20data%20demonstrate%0Athat%20L-MAC%20consistently%20produces%20more%20faithful%20interpretations%20than%20several%0Agradient%20and%20masking-based%20methodologies.%20Furthermore%2C%20a%20user%20study%20confirms%0Athat%2C%20on%20average%2C%20users%20prefer%20the%20interpretations%20generated%20by%20the%20proposed%0Atechnique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13086v2&entry.124074799=Read"},
{"title": "Training of Physical Neural Networks", "author": "Ali Momeni and Babak Rahmani and Benjamin Scellier and Logan G. Wright and Peter L. McMahon and Clara C. Wanjura and Yuhang Li and Anas Skalli and Natalia G. Berloff and Tatsuhiro Onodera and Ilker Oguz and Francesco Morichetti and Philipp del Hougne and Manuel Le Gallo and Abu Sebastian and Azalia Mirhoseini and Cheng Zhang and Danijela Markovi\u0107 and Daniel Brunner and Christophe Moser and Sylvain Gigan and Florian Marquardt and Aydogan Ozcan and Julie Grollier and Andrea J. Liu and Demetri Psaltis and Andrea Al\u00f9 and Romain Fleury", "abstract": "  Physical neural networks (PNNs) are a class of neural-like networks that\nleverage the properties of physical systems to perform computation. While PNNs\nare so far a niche research area with small-scale laboratory demonstrations,\nthey are arguably one of the most underappreciated important opportunities in\nmodern AI. Could we train AI models 1000x larger than current ones? Could we do\nthis and also have them perform inference locally and privately on edge\ndevices, such as smartphones or sensors? Research over the past few years has\nshown that the answer to all these questions is likely \"yes, with enough\nresearch\": PNNs could one day radically change what is possible and practical\nfor AI systems. To do this will however require rethinking both how AI models\nwork, and how they are trained - primarily by considering the problems through\nthe constraints of the underlying hardware physics. To train PNNs at large\nscale, many methods including backpropagation-based and backpropagation-free\napproaches are now being explored. These methods have various trade-offs, and\nso far no method has been shown to scale to the same scale and performance as\nthe backpropagation algorithm widely used in deep learning today. However, this\nis rapidly changing, and a diverse ecosystem of training techniques provides\nclues for how PNNs may one day be utilized to create both more efficient\nrealizations of current-scale AI models, and to enable unprecedented-scale\nmodels.\n", "link": "http://arxiv.org/abs/2406.03372v1", "date": "2024-06-05", "relevancy": 1.9547, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5159}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4945}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20of%20Physical%20Neural%20Networks&body=Title%3A%20Training%20of%20Physical%20Neural%20Networks%0AAuthor%3A%20Ali%20Momeni%20and%20Babak%20Rahmani%20and%20Benjamin%20Scellier%20and%20Logan%20G.%20Wright%20and%20Peter%20L.%20McMahon%20and%20Clara%20C.%20Wanjura%20and%20Yuhang%20Li%20and%20Anas%20Skalli%20and%20Natalia%20G.%20Berloff%20and%20Tatsuhiro%20Onodera%20and%20Ilker%20Oguz%20and%20Francesco%20Morichetti%20and%20Philipp%20del%20Hougne%20and%20Manuel%20Le%20Gallo%20and%20Abu%20Sebastian%20and%20Azalia%20Mirhoseini%20and%20Cheng%20Zhang%20and%20Danijela%20Markovi%C4%87%20and%20Daniel%20Brunner%20and%20Christophe%20Moser%20and%20Sylvain%20Gigan%20and%20Florian%20Marquardt%20and%20Aydogan%20Ozcan%20and%20Julie%20Grollier%20and%20Andrea%20J.%20Liu%20and%20Demetri%20Psaltis%20and%20Andrea%20Al%C3%B9%20and%20Romain%20Fleury%0AAbstract%3A%20%20%20Physical%20neural%20networks%20%28PNNs%29%20are%20a%20class%20of%20neural-like%20networks%20that%0Aleverage%20the%20properties%20of%20physical%20systems%20to%20perform%20computation.%20While%20PNNs%0Aare%20so%20far%20a%20niche%20research%20area%20with%20small-scale%20laboratory%20demonstrations%2C%0Athey%20are%20arguably%20one%20of%20the%20most%20underappreciated%20important%20opportunities%20in%0Amodern%20AI.%20Could%20we%20train%20AI%20models%201000x%20larger%20than%20current%20ones%3F%20Could%20we%20do%0Athis%20and%20also%20have%20them%20perform%20inference%20locally%20and%20privately%20on%20edge%0Adevices%2C%20such%20as%20smartphones%20or%20sensors%3F%20Research%20over%20the%20past%20few%20years%20has%0Ashown%20that%20the%20answer%20to%20all%20these%20questions%20is%20likely%20%22yes%2C%20with%20enough%0Aresearch%22%3A%20PNNs%20could%20one%20day%20radically%20change%20what%20is%20possible%20and%20practical%0Afor%20AI%20systems.%20To%20do%20this%20will%20however%20require%20rethinking%20both%20how%20AI%20models%0Awork%2C%20and%20how%20they%20are%20trained%20-%20primarily%20by%20considering%20the%20problems%20through%0Athe%20constraints%20of%20the%20underlying%20hardware%20physics.%20To%20train%20PNNs%20at%20large%0Ascale%2C%20many%20methods%20including%20backpropagation-based%20and%20backpropagation-free%0Aapproaches%20are%20now%20being%20explored.%20These%20methods%20have%20various%20trade-offs%2C%20and%0Aso%20far%20no%20method%20has%20been%20shown%20to%20scale%20to%20the%20same%20scale%20and%20performance%20as%0Athe%20backpropagation%20algorithm%20widely%20used%20in%20deep%20learning%20today.%20However%2C%20this%0Ais%20rapidly%20changing%2C%20and%20a%20diverse%20ecosystem%20of%20training%20techniques%20provides%0Aclues%20for%20how%20PNNs%20may%20one%20day%20be%20utilized%20to%20create%20both%20more%20efficient%0Arealizations%20of%20current-scale%20AI%20models%2C%20and%20to%20enable%20unprecedented-scale%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520of%2520Physical%2520Neural%2520Networks%26entry.906535625%3DAli%2520Momeni%2520and%2520Babak%2520Rahmani%2520and%2520Benjamin%2520Scellier%2520and%2520Logan%2520G.%2520Wright%2520and%2520Peter%2520L.%2520McMahon%2520and%2520Clara%2520C.%2520Wanjura%2520and%2520Yuhang%2520Li%2520and%2520Anas%2520Skalli%2520and%2520Natalia%2520G.%2520Berloff%2520and%2520Tatsuhiro%2520Onodera%2520and%2520Ilker%2520Oguz%2520and%2520Francesco%2520Morichetti%2520and%2520Philipp%2520del%2520Hougne%2520and%2520Manuel%2520Le%2520Gallo%2520and%2520Abu%2520Sebastian%2520and%2520Azalia%2520Mirhoseini%2520and%2520Cheng%2520Zhang%2520and%2520Danijela%2520Markovi%25C4%2587%2520and%2520Daniel%2520Brunner%2520and%2520Christophe%2520Moser%2520and%2520Sylvain%2520Gigan%2520and%2520Florian%2520Marquardt%2520and%2520Aydogan%2520Ozcan%2520and%2520Julie%2520Grollier%2520and%2520Andrea%2520J.%2520Liu%2520and%2520Demetri%2520Psaltis%2520and%2520Andrea%2520Al%25C3%25B9%2520and%2520Romain%2520Fleury%26entry.1292438233%3D%2520%2520Physical%2520neural%2520networks%2520%2528PNNs%2529%2520are%2520a%2520class%2520of%2520neural-like%2520networks%2520that%250Aleverage%2520the%2520properties%2520of%2520physical%2520systems%2520to%2520perform%2520computation.%2520While%2520PNNs%250Aare%2520so%2520far%2520a%2520niche%2520research%2520area%2520with%2520small-scale%2520laboratory%2520demonstrations%252C%250Athey%2520are%2520arguably%2520one%2520of%2520the%2520most%2520underappreciated%2520important%2520opportunities%2520in%250Amodern%2520AI.%2520Could%2520we%2520train%2520AI%2520models%25201000x%2520larger%2520than%2520current%2520ones%253F%2520Could%2520we%2520do%250Athis%2520and%2520also%2520have%2520them%2520perform%2520inference%2520locally%2520and%2520privately%2520on%2520edge%250Adevices%252C%2520such%2520as%2520smartphones%2520or%2520sensors%253F%2520Research%2520over%2520the%2520past%2520few%2520years%2520has%250Ashown%2520that%2520the%2520answer%2520to%2520all%2520these%2520questions%2520is%2520likely%2520%2522yes%252C%2520with%2520enough%250Aresearch%2522%253A%2520PNNs%2520could%2520one%2520day%2520radically%2520change%2520what%2520is%2520possible%2520and%2520practical%250Afor%2520AI%2520systems.%2520To%2520do%2520this%2520will%2520however%2520require%2520rethinking%2520both%2520how%2520AI%2520models%250Awork%252C%2520and%2520how%2520they%2520are%2520trained%2520-%2520primarily%2520by%2520considering%2520the%2520problems%2520through%250Athe%2520constraints%2520of%2520the%2520underlying%2520hardware%2520physics.%2520To%2520train%2520PNNs%2520at%2520large%250Ascale%252C%2520many%2520methods%2520including%2520backpropagation-based%2520and%2520backpropagation-free%250Aapproaches%2520are%2520now%2520being%2520explored.%2520These%2520methods%2520have%2520various%2520trade-offs%252C%2520and%250Aso%2520far%2520no%2520method%2520has%2520been%2520shown%2520to%2520scale%2520to%2520the%2520same%2520scale%2520and%2520performance%2520as%250Athe%2520backpropagation%2520algorithm%2520widely%2520used%2520in%2520deep%2520learning%2520today.%2520However%252C%2520this%250Ais%2520rapidly%2520changing%252C%2520and%2520a%2520diverse%2520ecosystem%2520of%2520training%2520techniques%2520provides%250Aclues%2520for%2520how%2520PNNs%2520may%2520one%2520day%2520be%2520utilized%2520to%2520create%2520both%2520more%2520efficient%250Arealizations%2520of%2520current-scale%2520AI%2520models%252C%2520and%2520to%2520enable%2520unprecedented-scale%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20of%20Physical%20Neural%20Networks&entry.906535625=Ali%20Momeni%20and%20Babak%20Rahmani%20and%20Benjamin%20Scellier%20and%20Logan%20G.%20Wright%20and%20Peter%20L.%20McMahon%20and%20Clara%20C.%20Wanjura%20and%20Yuhang%20Li%20and%20Anas%20Skalli%20and%20Natalia%20G.%20Berloff%20and%20Tatsuhiro%20Onodera%20and%20Ilker%20Oguz%20and%20Francesco%20Morichetti%20and%20Philipp%20del%20Hougne%20and%20Manuel%20Le%20Gallo%20and%20Abu%20Sebastian%20and%20Azalia%20Mirhoseini%20and%20Cheng%20Zhang%20and%20Danijela%20Markovi%C4%87%20and%20Daniel%20Brunner%20and%20Christophe%20Moser%20and%20Sylvain%20Gigan%20and%20Florian%20Marquardt%20and%20Aydogan%20Ozcan%20and%20Julie%20Grollier%20and%20Andrea%20J.%20Liu%20and%20Demetri%20Psaltis%20and%20Andrea%20Al%C3%B9%20and%20Romain%20Fleury&entry.1292438233=%20%20Physical%20neural%20networks%20%28PNNs%29%20are%20a%20class%20of%20neural-like%20networks%20that%0Aleverage%20the%20properties%20of%20physical%20systems%20to%20perform%20computation.%20While%20PNNs%0Aare%20so%20far%20a%20niche%20research%20area%20with%20small-scale%20laboratory%20demonstrations%2C%0Athey%20are%20arguably%20one%20of%20the%20most%20underappreciated%20important%20opportunities%20in%0Amodern%20AI.%20Could%20we%20train%20AI%20models%201000x%20larger%20than%20current%20ones%3F%20Could%20we%20do%0Athis%20and%20also%20have%20them%20perform%20inference%20locally%20and%20privately%20on%20edge%0Adevices%2C%20such%20as%20smartphones%20or%20sensors%3F%20Research%20over%20the%20past%20few%20years%20has%0Ashown%20that%20the%20answer%20to%20all%20these%20questions%20is%20likely%20%22yes%2C%20with%20enough%0Aresearch%22%3A%20PNNs%20could%20one%20day%20radically%20change%20what%20is%20possible%20and%20practical%0Afor%20AI%20systems.%20To%20do%20this%20will%20however%20require%20rethinking%20both%20how%20AI%20models%0Awork%2C%20and%20how%20they%20are%20trained%20-%20primarily%20by%20considering%20the%20problems%20through%0Athe%20constraints%20of%20the%20underlying%20hardware%20physics.%20To%20train%20PNNs%20at%20large%0Ascale%2C%20many%20methods%20including%20backpropagation-based%20and%20backpropagation-free%0Aapproaches%20are%20now%20being%20explored.%20These%20methods%20have%20various%20trade-offs%2C%20and%0Aso%20far%20no%20method%20has%20been%20shown%20to%20scale%20to%20the%20same%20scale%20and%20performance%20as%0Athe%20backpropagation%20algorithm%20widely%20used%20in%20deep%20learning%20today.%20However%2C%20this%0Ais%20rapidly%20changing%2C%20and%20a%20diverse%20ecosystem%20of%20training%20techniques%20provides%0Aclues%20for%20how%20PNNs%20may%20one%20day%20be%20utilized%20to%20create%20both%20more%20efficient%0Arealizations%20of%20current-scale%20AI%20models%2C%20and%20to%20enable%20unprecedented-scale%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03372v1&entry.124074799=Read"},
{"title": "Cooperative learning of Pl@ntNet's Artificial Intelligence algorithm:\n  how does it work and how can we improve it?", "author": "Tanguy Lefort and Antoine Affouard and Benjamin Charlier and Jean-Christophe Lombardo and Mathias Chouet and Herv\u00e9 Go\u00ebau and Joseph Salmon and Pierre Bonnet and Alexis Joly", "abstract": "  Deep learning models for plant species identification rely on large annotated\ndatasets. The PlantNet system enables global data collection by allowing users\nto upload and annotate plant observations, leading to noisy labels due to\ndiverse user skills. Achieving consensus is crucial for training, but the vast\nscale of collected data makes traditional label aggregation strategies\nchallenging. Existing methods either retain all observations, resulting in\nnoisy training data or selectively keep those with sufficient votes, discarding\nvaluable information. Additionally, as many species are rarely observed, user\nexpertise can not be evaluated as an inter-user agreement: otherwise, botanical\nexperts would have a lower weight in the AI training step than the average\nuser. Our proposed label aggregation strategy aims to cooperatively train plant\nidentification AI models. This strategy estimates user expertise as a trust\nscore per user based on their ability to identify plant species from\ncrowdsourced data. The trust score is recursively estimated from correctly\nidentified species given the current estimated labels. This interpretable score\nexploits botanical experts' knowledge and the heterogeneity of users.\nSubsequently, our strategy removes unreliable observations but retains those\nwith limited trusted annotations, unlike other approaches. We evaluate\nPlantNet's strategy on a released large subset of the PlantNet database focused\non European flora, comprising over 6M observations and 800K users. We\ndemonstrate that estimating users' skills based on the diversity of their\nexpertise enhances labeling performance. Our findings emphasize the synergy of\nhuman annotation and data filtering in improving AI performance for a refined\ndataset. We explore incorporating AI-based votes alongside human input. This\ncan further enhance human-AI interactions to detect unreliable observations.\n", "link": "http://arxiv.org/abs/2406.03356v1", "date": "2024-06-05", "relevancy": 1.9497, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5009}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.49}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20learning%20of%20Pl%40ntNet%27s%20Artificial%20Intelligence%20algorithm%3A%0A%20%20how%20does%20it%20work%20and%20how%20can%20we%20improve%20it%3F&body=Title%3A%20Cooperative%20learning%20of%20Pl%40ntNet%27s%20Artificial%20Intelligence%20algorithm%3A%0A%20%20how%20does%20it%20work%20and%20how%20can%20we%20improve%20it%3F%0AAuthor%3A%20Tanguy%20Lefort%20and%20Antoine%20Affouard%20and%20Benjamin%20Charlier%20and%20Jean-Christophe%20Lombardo%20and%20Mathias%20Chouet%20and%20Herv%C3%A9%20Go%C3%ABau%20and%20Joseph%20Salmon%20and%20Pierre%20Bonnet%20and%20Alexis%20Joly%0AAbstract%3A%20%20%20Deep%20learning%20models%20for%20plant%20species%20identification%20rely%20on%20large%20annotated%0Adatasets.%20The%20PlantNet%20system%20enables%20global%20data%20collection%20by%20allowing%20users%0Ato%20upload%20and%20annotate%20plant%20observations%2C%20leading%20to%20noisy%20labels%20due%20to%0Adiverse%20user%20skills.%20Achieving%20consensus%20is%20crucial%20for%20training%2C%20but%20the%20vast%0Ascale%20of%20collected%20data%20makes%20traditional%20label%20aggregation%20strategies%0Achallenging.%20Existing%20methods%20either%20retain%20all%20observations%2C%20resulting%20in%0Anoisy%20training%20data%20or%20selectively%20keep%20those%20with%20sufficient%20votes%2C%20discarding%0Avaluable%20information.%20Additionally%2C%20as%20many%20species%20are%20rarely%20observed%2C%20user%0Aexpertise%20can%20not%20be%20evaluated%20as%20an%20inter-user%20agreement%3A%20otherwise%2C%20botanical%0Aexperts%20would%20have%20a%20lower%20weight%20in%20the%20AI%20training%20step%20than%20the%20average%0Auser.%20Our%20proposed%20label%20aggregation%20strategy%20aims%20to%20cooperatively%20train%20plant%0Aidentification%20AI%20models.%20This%20strategy%20estimates%20user%20expertise%20as%20a%20trust%0Ascore%20per%20user%20based%20on%20their%20ability%20to%20identify%20plant%20species%20from%0Acrowdsourced%20data.%20The%20trust%20score%20is%20recursively%20estimated%20from%20correctly%0Aidentified%20species%20given%20the%20current%20estimated%20labels.%20This%20interpretable%20score%0Aexploits%20botanical%20experts%27%20knowledge%20and%20the%20heterogeneity%20of%20users.%0ASubsequently%2C%20our%20strategy%20removes%20unreliable%20observations%20but%20retains%20those%0Awith%20limited%20trusted%20annotations%2C%20unlike%20other%20approaches.%20We%20evaluate%0APlantNet%27s%20strategy%20on%20a%20released%20large%20subset%20of%20the%20PlantNet%20database%20focused%0Aon%20European%20flora%2C%20comprising%20over%206M%20observations%20and%20800K%20users.%20We%0Ademonstrate%20that%20estimating%20users%27%20skills%20based%20on%20the%20diversity%20of%20their%0Aexpertise%20enhances%20labeling%20performance.%20Our%20findings%20emphasize%20the%20synergy%20of%0Ahuman%20annotation%20and%20data%20filtering%20in%20improving%20AI%20performance%20for%20a%20refined%0Adataset.%20We%20explore%20incorporating%20AI-based%20votes%20alongside%20human%20input.%20This%0Acan%20further%20enhance%20human-AI%20interactions%20to%20detect%20unreliable%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520learning%2520of%2520Pl%2540ntNet%2527s%2520Artificial%2520Intelligence%2520algorithm%253A%250A%2520%2520how%2520does%2520it%2520work%2520and%2520how%2520can%2520we%2520improve%2520it%253F%26entry.906535625%3DTanguy%2520Lefort%2520and%2520Antoine%2520Affouard%2520and%2520Benjamin%2520Charlier%2520and%2520Jean-Christophe%2520Lombardo%2520and%2520Mathias%2520Chouet%2520and%2520Herv%25C3%25A9%2520Go%25C3%25ABau%2520and%2520Joseph%2520Salmon%2520and%2520Pierre%2520Bonnet%2520and%2520Alexis%2520Joly%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520for%2520plant%2520species%2520identification%2520rely%2520on%2520large%2520annotated%250Adatasets.%2520The%2520PlantNet%2520system%2520enables%2520global%2520data%2520collection%2520by%2520allowing%2520users%250Ato%2520upload%2520and%2520annotate%2520plant%2520observations%252C%2520leading%2520to%2520noisy%2520labels%2520due%2520to%250Adiverse%2520user%2520skills.%2520Achieving%2520consensus%2520is%2520crucial%2520for%2520training%252C%2520but%2520the%2520vast%250Ascale%2520of%2520collected%2520data%2520makes%2520traditional%2520label%2520aggregation%2520strategies%250Achallenging.%2520Existing%2520methods%2520either%2520retain%2520all%2520observations%252C%2520resulting%2520in%250Anoisy%2520training%2520data%2520or%2520selectively%2520keep%2520those%2520with%2520sufficient%2520votes%252C%2520discarding%250Avaluable%2520information.%2520Additionally%252C%2520as%2520many%2520species%2520are%2520rarely%2520observed%252C%2520user%250Aexpertise%2520can%2520not%2520be%2520evaluated%2520as%2520an%2520inter-user%2520agreement%253A%2520otherwise%252C%2520botanical%250Aexperts%2520would%2520have%2520a%2520lower%2520weight%2520in%2520the%2520AI%2520training%2520step%2520than%2520the%2520average%250Auser.%2520Our%2520proposed%2520label%2520aggregation%2520strategy%2520aims%2520to%2520cooperatively%2520train%2520plant%250Aidentification%2520AI%2520models.%2520This%2520strategy%2520estimates%2520user%2520expertise%2520as%2520a%2520trust%250Ascore%2520per%2520user%2520based%2520on%2520their%2520ability%2520to%2520identify%2520plant%2520species%2520from%250Acrowdsourced%2520data.%2520The%2520trust%2520score%2520is%2520recursively%2520estimated%2520from%2520correctly%250Aidentified%2520species%2520given%2520the%2520current%2520estimated%2520labels.%2520This%2520interpretable%2520score%250Aexploits%2520botanical%2520experts%2527%2520knowledge%2520and%2520the%2520heterogeneity%2520of%2520users.%250ASubsequently%252C%2520our%2520strategy%2520removes%2520unreliable%2520observations%2520but%2520retains%2520those%250Awith%2520limited%2520trusted%2520annotations%252C%2520unlike%2520other%2520approaches.%2520We%2520evaluate%250APlantNet%2527s%2520strategy%2520on%2520a%2520released%2520large%2520subset%2520of%2520the%2520PlantNet%2520database%2520focused%250Aon%2520European%2520flora%252C%2520comprising%2520over%25206M%2520observations%2520and%2520800K%2520users.%2520We%250Ademonstrate%2520that%2520estimating%2520users%2527%2520skills%2520based%2520on%2520the%2520diversity%2520of%2520their%250Aexpertise%2520enhances%2520labeling%2520performance.%2520Our%2520findings%2520emphasize%2520the%2520synergy%2520of%250Ahuman%2520annotation%2520and%2520data%2520filtering%2520in%2520improving%2520AI%2520performance%2520for%2520a%2520refined%250Adataset.%2520We%2520explore%2520incorporating%2520AI-based%2520votes%2520alongside%2520human%2520input.%2520This%250Acan%2520further%2520enhance%2520human-AI%2520interactions%2520to%2520detect%2520unreliable%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20learning%20of%20Pl%40ntNet%27s%20Artificial%20Intelligence%20algorithm%3A%0A%20%20how%20does%20it%20work%20and%20how%20can%20we%20improve%20it%3F&entry.906535625=Tanguy%20Lefort%20and%20Antoine%20Affouard%20and%20Benjamin%20Charlier%20and%20Jean-Christophe%20Lombardo%20and%20Mathias%20Chouet%20and%20Herv%C3%A9%20Go%C3%ABau%20and%20Joseph%20Salmon%20and%20Pierre%20Bonnet%20and%20Alexis%20Joly&entry.1292438233=%20%20Deep%20learning%20models%20for%20plant%20species%20identification%20rely%20on%20large%20annotated%0Adatasets.%20The%20PlantNet%20system%20enables%20global%20data%20collection%20by%20allowing%20users%0Ato%20upload%20and%20annotate%20plant%20observations%2C%20leading%20to%20noisy%20labels%20due%20to%0Adiverse%20user%20skills.%20Achieving%20consensus%20is%20crucial%20for%20training%2C%20but%20the%20vast%0Ascale%20of%20collected%20data%20makes%20traditional%20label%20aggregation%20strategies%0Achallenging.%20Existing%20methods%20either%20retain%20all%20observations%2C%20resulting%20in%0Anoisy%20training%20data%20or%20selectively%20keep%20those%20with%20sufficient%20votes%2C%20discarding%0Avaluable%20information.%20Additionally%2C%20as%20many%20species%20are%20rarely%20observed%2C%20user%0Aexpertise%20can%20not%20be%20evaluated%20as%20an%20inter-user%20agreement%3A%20otherwise%2C%20botanical%0Aexperts%20would%20have%20a%20lower%20weight%20in%20the%20AI%20training%20step%20than%20the%20average%0Auser.%20Our%20proposed%20label%20aggregation%20strategy%20aims%20to%20cooperatively%20train%20plant%0Aidentification%20AI%20models.%20This%20strategy%20estimates%20user%20expertise%20as%20a%20trust%0Ascore%20per%20user%20based%20on%20their%20ability%20to%20identify%20plant%20species%20from%0Acrowdsourced%20data.%20The%20trust%20score%20is%20recursively%20estimated%20from%20correctly%0Aidentified%20species%20given%20the%20current%20estimated%20labels.%20This%20interpretable%20score%0Aexploits%20botanical%20experts%27%20knowledge%20and%20the%20heterogeneity%20of%20users.%0ASubsequently%2C%20our%20strategy%20removes%20unreliable%20observations%20but%20retains%20those%0Awith%20limited%20trusted%20annotations%2C%20unlike%20other%20approaches.%20We%20evaluate%0APlantNet%27s%20strategy%20on%20a%20released%20large%20subset%20of%20the%20PlantNet%20database%20focused%0Aon%20European%20flora%2C%20comprising%20over%206M%20observations%20and%20800K%20users.%20We%0Ademonstrate%20that%20estimating%20users%27%20skills%20based%20on%20the%20diversity%20of%20their%0Aexpertise%20enhances%20labeling%20performance.%20Our%20findings%20emphasize%20the%20synergy%20of%0Ahuman%20annotation%20and%20data%20filtering%20in%20improving%20AI%20performance%20for%20a%20refined%0Adataset.%20We%20explore%20incorporating%20AI-based%20votes%20alongside%20human%20input.%20This%0Acan%20further%20enhance%20human-AI%20interactions%20to%20detect%20unreliable%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03356v1&entry.124074799=Read"},
{"title": "How do Transformers perform In-Context Autoregressive Learning?", "author": "Michael E. Sander and Raja Giryes and Taiji Suzuki and Mathieu Blondel and Gabriel Peyr\u00e9", "abstract": "  Transformers have achieved state-of-the-art performance in language modeling\ntasks. However, the reasons behind their tremendous success are still unclear.\nIn this paper, towards a better understanding, we train a Transformer model on\na simple next token prediction task, where sequences are generated as a\nfirst-order autoregressive process $s_{t+1} = W s_t$. We show how a trained\nTransformer predicts the next token by first learning $W$ in-context, then\napplying a prediction mapping. We call the resulting procedure in-context\nautoregressive learning. More precisely, focusing on commuting orthogonal\nmatrices $W$, we first show that a trained one-layer linear Transformer\nimplements one step of gradient descent for the minimization of an inner\nobjective function, when considering augmented tokens. When the tokens are not\naugmented, we characterize the global minima of a one-layer diagonal linear\nmulti-head Transformer. Importantly, we exhibit orthogonality between heads and\nshow that positional encoding captures trigonometric relations in the data. On\nthe experimental side, we consider the general case of non-commuting orthogonal\nmatrices and generalize our theoretical findings.\n", "link": "http://arxiv.org/abs/2402.05787v2", "date": "2024-06-05", "relevancy": 1.9466, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.58}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4716}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20do%20Transformers%20perform%20In-Context%20Autoregressive%20Learning%3F&body=Title%3A%20How%20do%20Transformers%20perform%20In-Context%20Autoregressive%20Learning%3F%0AAuthor%3A%20Michael%20E.%20Sander%20and%20Raja%20Giryes%20and%20Taiji%20Suzuki%20and%20Mathieu%20Blondel%20and%20Gabriel%20Peyr%C3%A9%0AAbstract%3A%20%20%20Transformers%20have%20achieved%20state-of-the-art%20performance%20in%20language%20modeling%0Atasks.%20However%2C%20the%20reasons%20behind%20their%20tremendous%20success%20are%20still%20unclear.%0AIn%20this%20paper%2C%20towards%20a%20better%20understanding%2C%20we%20train%20a%20Transformer%20model%20on%0Aa%20simple%20next%20token%20prediction%20task%2C%20where%20sequences%20are%20generated%20as%20a%0Afirst-order%20autoregressive%20process%20%24s_%7Bt%2B1%7D%20%3D%20W%20s_t%24.%20We%20show%20how%20a%20trained%0ATransformer%20predicts%20the%20next%20token%20by%20first%20learning%20%24W%24%20in-context%2C%20then%0Aapplying%20a%20prediction%20mapping.%20We%20call%20the%20resulting%20procedure%20in-context%0Aautoregressive%20learning.%20More%20precisely%2C%20focusing%20on%20commuting%20orthogonal%0Amatrices%20%24W%24%2C%20we%20first%20show%20that%20a%20trained%20one-layer%20linear%20Transformer%0Aimplements%20one%20step%20of%20gradient%20descent%20for%20the%20minimization%20of%20an%20inner%0Aobjective%20function%2C%20when%20considering%20augmented%20tokens.%20When%20the%20tokens%20are%20not%0Aaugmented%2C%20we%20characterize%20the%20global%20minima%20of%20a%20one-layer%20diagonal%20linear%0Amulti-head%20Transformer.%20Importantly%2C%20we%20exhibit%20orthogonality%20between%20heads%20and%0Ashow%20that%20positional%20encoding%20captures%20trigonometric%20relations%20in%20the%20data.%20On%0Athe%20experimental%20side%2C%20we%20consider%20the%20general%20case%20of%20non-commuting%20orthogonal%0Amatrices%20and%20generalize%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05787v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520do%2520Transformers%2520perform%2520In-Context%2520Autoregressive%2520Learning%253F%26entry.906535625%3DMichael%2520E.%2520Sander%2520and%2520Raja%2520Giryes%2520and%2520Taiji%2520Suzuki%2520and%2520Mathieu%2520Blondel%2520and%2520Gabriel%2520Peyr%25C3%25A9%26entry.1292438233%3D%2520%2520Transformers%2520have%2520achieved%2520state-of-the-art%2520performance%2520in%2520language%2520modeling%250Atasks.%2520However%252C%2520the%2520reasons%2520behind%2520their%2520tremendous%2520success%2520are%2520still%2520unclear.%250AIn%2520this%2520paper%252C%2520towards%2520a%2520better%2520understanding%252C%2520we%2520train%2520a%2520Transformer%2520model%2520on%250Aa%2520simple%2520next%2520token%2520prediction%2520task%252C%2520where%2520sequences%2520are%2520generated%2520as%2520a%250Afirst-order%2520autoregressive%2520process%2520%2524s_%257Bt%252B1%257D%2520%253D%2520W%2520s_t%2524.%2520We%2520show%2520how%2520a%2520trained%250ATransformer%2520predicts%2520the%2520next%2520token%2520by%2520first%2520learning%2520%2524W%2524%2520in-context%252C%2520then%250Aapplying%2520a%2520prediction%2520mapping.%2520We%2520call%2520the%2520resulting%2520procedure%2520in-context%250Aautoregressive%2520learning.%2520More%2520precisely%252C%2520focusing%2520on%2520commuting%2520orthogonal%250Amatrices%2520%2524W%2524%252C%2520we%2520first%2520show%2520that%2520a%2520trained%2520one-layer%2520linear%2520Transformer%250Aimplements%2520one%2520step%2520of%2520gradient%2520descent%2520for%2520the%2520minimization%2520of%2520an%2520inner%250Aobjective%2520function%252C%2520when%2520considering%2520augmented%2520tokens.%2520When%2520the%2520tokens%2520are%2520not%250Aaugmented%252C%2520we%2520characterize%2520the%2520global%2520minima%2520of%2520a%2520one-layer%2520diagonal%2520linear%250Amulti-head%2520Transformer.%2520Importantly%252C%2520we%2520exhibit%2520orthogonality%2520between%2520heads%2520and%250Ashow%2520that%2520positional%2520encoding%2520captures%2520trigonometric%2520relations%2520in%2520the%2520data.%2520On%250Athe%2520experimental%2520side%252C%2520we%2520consider%2520the%2520general%2520case%2520of%2520non-commuting%2520orthogonal%250Amatrices%2520and%2520generalize%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05787v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20do%20Transformers%20perform%20In-Context%20Autoregressive%20Learning%3F&entry.906535625=Michael%20E.%20Sander%20and%20Raja%20Giryes%20and%20Taiji%20Suzuki%20and%20Mathieu%20Blondel%20and%20Gabriel%20Peyr%C3%A9&entry.1292438233=%20%20Transformers%20have%20achieved%20state-of-the-art%20performance%20in%20language%20modeling%0Atasks.%20However%2C%20the%20reasons%20behind%20their%20tremendous%20success%20are%20still%20unclear.%0AIn%20this%20paper%2C%20towards%20a%20better%20understanding%2C%20we%20train%20a%20Transformer%20model%20on%0Aa%20simple%20next%20token%20prediction%20task%2C%20where%20sequences%20are%20generated%20as%20a%0Afirst-order%20autoregressive%20process%20%24s_%7Bt%2B1%7D%20%3D%20W%20s_t%24.%20We%20show%20how%20a%20trained%0ATransformer%20predicts%20the%20next%20token%20by%20first%20learning%20%24W%24%20in-context%2C%20then%0Aapplying%20a%20prediction%20mapping.%20We%20call%20the%20resulting%20procedure%20in-context%0Aautoregressive%20learning.%20More%20precisely%2C%20focusing%20on%20commuting%20orthogonal%0Amatrices%20%24W%24%2C%20we%20first%20show%20that%20a%20trained%20one-layer%20linear%20Transformer%0Aimplements%20one%20step%20of%20gradient%20descent%20for%20the%20minimization%20of%20an%20inner%0Aobjective%20function%2C%20when%20considering%20augmented%20tokens.%20When%20the%20tokens%20are%20not%0Aaugmented%2C%20we%20characterize%20the%20global%20minima%20of%20a%20one-layer%20diagonal%20linear%0Amulti-head%20Transformer.%20Importantly%2C%20we%20exhibit%20orthogonality%20between%20heads%20and%0Ashow%20that%20positional%20encoding%20captures%20trigonometric%20relations%20in%20the%20data.%20On%0Athe%20experimental%20side%2C%20we%20consider%20the%20general%20case%20of%20non-commuting%20orthogonal%0Amatrices%20and%20generalize%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05787v2&entry.124074799=Read"},
{"title": "Near-field Beamforming for Extremely Large-scale MIMO Based on\n  Unsupervised Deep Learning", "author": "Jiali Nie and Yuanhao Cui and Zhaohui Yang and Weijie Yuan and Xiaojun Jing", "abstract": "  Extremely Large-scale Array (ELAA) is considered a frontier technology for\nfuture communication systems, pivotal in improving wireless systems' rate and\nspectral efficiency. However, as ELAA employs a multitude of antennas operating\nat higher frequencies, users are typically situated in the near-field region\nwhere the spherical wavefront propagates. This inevitably leads to a\nsignificant increase in the overhead of beam training, requiring complex\ntwo-dimensional beam searching in both the angle domain and the distance\ndomain. To address this problem, we propose a near-field beamforming method\nbased on unsupervised deep learning. Our convolutional neural network\nefficiently extracts complex channel state information features by\nstrategically selecting padding and kernel size. We optimize the beamformers to\nmaximize achievable rates in a multi-user network without relying on predefined\ncustom codebooks. Upon deployment, the model requires solely the input of\npre-estimated channel state information to derive the optimal beamforming\nvector. Simulation results show that our proposed scheme can obtain stable\nbeamforming gain compared with the baseline scheme. Furthermore, owing to the\ninherent traits of deep learning methodologies, this approach substantially\ndiminishes the beam training costs in near-field regions.\n", "link": "http://arxiv.org/abs/2406.03249v1", "date": "2024-06-05", "relevancy": 1.9432, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5041}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4757}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-field%20Beamforming%20for%20Extremely%20Large-scale%20MIMO%20Based%20on%0A%20%20Unsupervised%20Deep%20Learning&body=Title%3A%20Near-field%20Beamforming%20for%20Extremely%20Large-scale%20MIMO%20Based%20on%0A%20%20Unsupervised%20Deep%20Learning%0AAuthor%3A%20Jiali%20Nie%20and%20Yuanhao%20Cui%20and%20Zhaohui%20Yang%20and%20Weijie%20Yuan%20and%20Xiaojun%20Jing%0AAbstract%3A%20%20%20Extremely%20Large-scale%20Array%20%28ELAA%29%20is%20considered%20a%20frontier%20technology%20for%0Afuture%20communication%20systems%2C%20pivotal%20in%20improving%20wireless%20systems%27%20rate%20and%0Aspectral%20efficiency.%20However%2C%20as%20ELAA%20employs%20a%20multitude%20of%20antennas%20operating%0Aat%20higher%20frequencies%2C%20users%20are%20typically%20situated%20in%20the%20near-field%20region%0Awhere%20the%20spherical%20wavefront%20propagates.%20This%20inevitably%20leads%20to%20a%0Asignificant%20increase%20in%20the%20overhead%20of%20beam%20training%2C%20requiring%20complex%0Atwo-dimensional%20beam%20searching%20in%20both%20the%20angle%20domain%20and%20the%20distance%0Adomain.%20To%20address%20this%20problem%2C%20we%20propose%20a%20near-field%20beamforming%20method%0Abased%20on%20unsupervised%20deep%20learning.%20Our%20convolutional%20neural%20network%0Aefficiently%20extracts%20complex%20channel%20state%20information%20features%20by%0Astrategically%20selecting%20padding%20and%20kernel%20size.%20We%20optimize%20the%20beamformers%20to%0Amaximize%20achievable%20rates%20in%20a%20multi-user%20network%20without%20relying%20on%20predefined%0Acustom%20codebooks.%20Upon%20deployment%2C%20the%20model%20requires%20solely%20the%20input%20of%0Apre-estimated%20channel%20state%20information%20to%20derive%20the%20optimal%20beamforming%0Avector.%20Simulation%20results%20show%20that%20our%20proposed%20scheme%20can%20obtain%20stable%0Abeamforming%20gain%20compared%20with%20the%20baseline%20scheme.%20Furthermore%2C%20owing%20to%20the%0Ainherent%20traits%20of%20deep%20learning%20methodologies%2C%20this%20approach%20substantially%0Adiminishes%20the%20beam%20training%20costs%20in%20near-field%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-field%2520Beamforming%2520for%2520Extremely%2520Large-scale%2520MIMO%2520Based%2520on%250A%2520%2520Unsupervised%2520Deep%2520Learning%26entry.906535625%3DJiali%2520Nie%2520and%2520Yuanhao%2520Cui%2520and%2520Zhaohui%2520Yang%2520and%2520Weijie%2520Yuan%2520and%2520Xiaojun%2520Jing%26entry.1292438233%3D%2520%2520Extremely%2520Large-scale%2520Array%2520%2528ELAA%2529%2520is%2520considered%2520a%2520frontier%2520technology%2520for%250Afuture%2520communication%2520systems%252C%2520pivotal%2520in%2520improving%2520wireless%2520systems%2527%2520rate%2520and%250Aspectral%2520efficiency.%2520However%252C%2520as%2520ELAA%2520employs%2520a%2520multitude%2520of%2520antennas%2520operating%250Aat%2520higher%2520frequencies%252C%2520users%2520are%2520typically%2520situated%2520in%2520the%2520near-field%2520region%250Awhere%2520the%2520spherical%2520wavefront%2520propagates.%2520This%2520inevitably%2520leads%2520to%2520a%250Asignificant%2520increase%2520in%2520the%2520overhead%2520of%2520beam%2520training%252C%2520requiring%2520complex%250Atwo-dimensional%2520beam%2520searching%2520in%2520both%2520the%2520angle%2520domain%2520and%2520the%2520distance%250Adomain.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520near-field%2520beamforming%2520method%250Abased%2520on%2520unsupervised%2520deep%2520learning.%2520Our%2520convolutional%2520neural%2520network%250Aefficiently%2520extracts%2520complex%2520channel%2520state%2520information%2520features%2520by%250Astrategically%2520selecting%2520padding%2520and%2520kernel%2520size.%2520We%2520optimize%2520the%2520beamformers%2520to%250Amaximize%2520achievable%2520rates%2520in%2520a%2520multi-user%2520network%2520without%2520relying%2520on%2520predefined%250Acustom%2520codebooks.%2520Upon%2520deployment%252C%2520the%2520model%2520requires%2520solely%2520the%2520input%2520of%250Apre-estimated%2520channel%2520state%2520information%2520to%2520derive%2520the%2520optimal%2520beamforming%250Avector.%2520Simulation%2520results%2520show%2520that%2520our%2520proposed%2520scheme%2520can%2520obtain%2520stable%250Abeamforming%2520gain%2520compared%2520with%2520the%2520baseline%2520scheme.%2520Furthermore%252C%2520owing%2520to%2520the%250Ainherent%2520traits%2520of%2520deep%2520learning%2520methodologies%252C%2520this%2520approach%2520substantially%250Adiminishes%2520the%2520beam%2520training%2520costs%2520in%2520near-field%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-field%20Beamforming%20for%20Extremely%20Large-scale%20MIMO%20Based%20on%0A%20%20Unsupervised%20Deep%20Learning&entry.906535625=Jiali%20Nie%20and%20Yuanhao%20Cui%20and%20Zhaohui%20Yang%20and%20Weijie%20Yuan%20and%20Xiaojun%20Jing&entry.1292438233=%20%20Extremely%20Large-scale%20Array%20%28ELAA%29%20is%20considered%20a%20frontier%20technology%20for%0Afuture%20communication%20systems%2C%20pivotal%20in%20improving%20wireless%20systems%27%20rate%20and%0Aspectral%20efficiency.%20However%2C%20as%20ELAA%20employs%20a%20multitude%20of%20antennas%20operating%0Aat%20higher%20frequencies%2C%20users%20are%20typically%20situated%20in%20the%20near-field%20region%0Awhere%20the%20spherical%20wavefront%20propagates.%20This%20inevitably%20leads%20to%20a%0Asignificant%20increase%20in%20the%20overhead%20of%20beam%20training%2C%20requiring%20complex%0Atwo-dimensional%20beam%20searching%20in%20both%20the%20angle%20domain%20and%20the%20distance%0Adomain.%20To%20address%20this%20problem%2C%20we%20propose%20a%20near-field%20beamforming%20method%0Abased%20on%20unsupervised%20deep%20learning.%20Our%20convolutional%20neural%20network%0Aefficiently%20extracts%20complex%20channel%20state%20information%20features%20by%0Astrategically%20selecting%20padding%20and%20kernel%20size.%20We%20optimize%20the%20beamformers%20to%0Amaximize%20achievable%20rates%20in%20a%20multi-user%20network%20without%20relying%20on%20predefined%0Acustom%20codebooks.%20Upon%20deployment%2C%20the%20model%20requires%20solely%20the%20input%20of%0Apre-estimated%20channel%20state%20information%20to%20derive%20the%20optimal%20beamforming%0Avector.%20Simulation%20results%20show%20that%20our%20proposed%20scheme%20can%20obtain%20stable%0Abeamforming%20gain%20compared%20with%20the%20baseline%20scheme.%20Furthermore%2C%20owing%20to%20the%0Ainherent%20traits%20of%20deep%20learning%20methodologies%2C%20this%20approach%20substantially%0Adiminishes%20the%20beam%20training%20costs%20in%20near-field%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03249v1&entry.124074799=Read"},
{"title": "Geometric Localization of Homology Cycles", "author": "Amritendu Dhar and Vijay Natarajan and Abhishek Rathod", "abstract": "  Computing an optimal cycle in a given homology class, also referred to as the\nhomology localization problem, is known to be an NP-hard problem in general.\nFurthermore, there is currently no known optimality criterion that localizes\nclasses geometrically and admits a stability property under the setting of\npersistent homology. We present a geometric optimization of the cycles that is\ncomputable in polynomial time and is stable in an approximate sense. Tailoring\nour search criterion to different settings, we obtain various optimization\nproblems like optimal homologous cycle, minimum homology basis, and minimum\npersistent homology basis. In practice, the (trivial) exact algorithm is\ncomputationally expensive despite having a worst case polynomial runtime.\nTherefore, we design approximation algorithms for the above problems and study\ntheir performance experimentally. These algorithms have reasonable runtimes for\nmoderate sized datasets and the cycles computed by these algorithms are\nconsistently of high quality as demonstrated via experiments on multiple\ndatasets.\n", "link": "http://arxiv.org/abs/2406.03183v1", "date": "2024-06-05", "relevancy": 1.9421, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4004}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3869}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Localization%20of%20Homology%20Cycles&body=Title%3A%20Geometric%20Localization%20of%20Homology%20Cycles%0AAuthor%3A%20Amritendu%20Dhar%20and%20Vijay%20Natarajan%20and%20Abhishek%20Rathod%0AAbstract%3A%20%20%20Computing%20an%20optimal%20cycle%20in%20a%20given%20homology%20class%2C%20also%20referred%20to%20as%20the%0Ahomology%20localization%20problem%2C%20is%20known%20to%20be%20an%20NP-hard%20problem%20in%20general.%0AFurthermore%2C%20there%20is%20currently%20no%20known%20optimality%20criterion%20that%20localizes%0Aclasses%20geometrically%20and%20admits%20a%20stability%20property%20under%20the%20setting%20of%0Apersistent%20homology.%20We%20present%20a%20geometric%20optimization%20of%20the%20cycles%20that%20is%0Acomputable%20in%20polynomial%20time%20and%20is%20stable%20in%20an%20approximate%20sense.%20Tailoring%0Aour%20search%20criterion%20to%20different%20settings%2C%20we%20obtain%20various%20optimization%0Aproblems%20like%20optimal%20homologous%20cycle%2C%20minimum%20homology%20basis%2C%20and%20minimum%0Apersistent%20homology%20basis.%20In%20practice%2C%20the%20%28trivial%29%20exact%20algorithm%20is%0Acomputationally%20expensive%20despite%20having%20a%20worst%20case%20polynomial%20runtime.%0ATherefore%2C%20we%20design%20approximation%20algorithms%20for%20the%20above%20problems%20and%20study%0Atheir%20performance%20experimentally.%20These%20algorithms%20have%20reasonable%20runtimes%20for%0Amoderate%20sized%20datasets%20and%20the%20cycles%20computed%20by%20these%20algorithms%20are%0Aconsistently%20of%20high%20quality%20as%20demonstrated%20via%20experiments%20on%20multiple%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Localization%2520of%2520Homology%2520Cycles%26entry.906535625%3DAmritendu%2520Dhar%2520and%2520Vijay%2520Natarajan%2520and%2520Abhishek%2520Rathod%26entry.1292438233%3D%2520%2520Computing%2520an%2520optimal%2520cycle%2520in%2520a%2520given%2520homology%2520class%252C%2520also%2520referred%2520to%2520as%2520the%250Ahomology%2520localization%2520problem%252C%2520is%2520known%2520to%2520be%2520an%2520NP-hard%2520problem%2520in%2520general.%250AFurthermore%252C%2520there%2520is%2520currently%2520no%2520known%2520optimality%2520criterion%2520that%2520localizes%250Aclasses%2520geometrically%2520and%2520admits%2520a%2520stability%2520property%2520under%2520the%2520setting%2520of%250Apersistent%2520homology.%2520We%2520present%2520a%2520geometric%2520optimization%2520of%2520the%2520cycles%2520that%2520is%250Acomputable%2520in%2520polynomial%2520time%2520and%2520is%2520stable%2520in%2520an%2520approximate%2520sense.%2520Tailoring%250Aour%2520search%2520criterion%2520to%2520different%2520settings%252C%2520we%2520obtain%2520various%2520optimization%250Aproblems%2520like%2520optimal%2520homologous%2520cycle%252C%2520minimum%2520homology%2520basis%252C%2520and%2520minimum%250Apersistent%2520homology%2520basis.%2520In%2520practice%252C%2520the%2520%2528trivial%2529%2520exact%2520algorithm%2520is%250Acomputationally%2520expensive%2520despite%2520having%2520a%2520worst%2520case%2520polynomial%2520runtime.%250ATherefore%252C%2520we%2520design%2520approximation%2520algorithms%2520for%2520the%2520above%2520problems%2520and%2520study%250Atheir%2520performance%2520experimentally.%2520These%2520algorithms%2520have%2520reasonable%2520runtimes%2520for%250Amoderate%2520sized%2520datasets%2520and%2520the%2520cycles%2520computed%2520by%2520these%2520algorithms%2520are%250Aconsistently%2520of%2520high%2520quality%2520as%2520demonstrated%2520via%2520experiments%2520on%2520multiple%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Localization%20of%20Homology%20Cycles&entry.906535625=Amritendu%20Dhar%20and%20Vijay%20Natarajan%20and%20Abhishek%20Rathod&entry.1292438233=%20%20Computing%20an%20optimal%20cycle%20in%20a%20given%20homology%20class%2C%20also%20referred%20to%20as%20the%0Ahomology%20localization%20problem%2C%20is%20known%20to%20be%20an%20NP-hard%20problem%20in%20general.%0AFurthermore%2C%20there%20is%20currently%20no%20known%20optimality%20criterion%20that%20localizes%0Aclasses%20geometrically%20and%20admits%20a%20stability%20property%20under%20the%20setting%20of%0Apersistent%20homology.%20We%20present%20a%20geometric%20optimization%20of%20the%20cycles%20that%20is%0Acomputable%20in%20polynomial%20time%20and%20is%20stable%20in%20an%20approximate%20sense.%20Tailoring%0Aour%20search%20criterion%20to%20different%20settings%2C%20we%20obtain%20various%20optimization%0Aproblems%20like%20optimal%20homologous%20cycle%2C%20minimum%20homology%20basis%2C%20and%20minimum%0Apersistent%20homology%20basis.%20In%20practice%2C%20the%20%28trivial%29%20exact%20algorithm%20is%0Acomputationally%20expensive%20despite%20having%20a%20worst%20case%20polynomial%20runtime.%0ATherefore%2C%20we%20design%20approximation%20algorithms%20for%20the%20above%20problems%20and%20study%0Atheir%20performance%20experimentally.%20These%20algorithms%20have%20reasonable%20runtimes%20for%0Amoderate%20sized%20datasets%20and%20the%20cycles%20computed%20by%20these%20algorithms%20are%0Aconsistently%20of%20high%20quality%20as%20demonstrated%20via%20experiments%20on%20multiple%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03183v1&entry.124074799=Read"},
{"title": "Feature Contamination: Neural Networks Learn Uncorrelated Features and\n  Fail to Generalize", "author": "Tianren Zhang and Chujie Zhao and Guanyu Chen and Yizhou Jiang and Feng Chen", "abstract": "  Learning representations that generalize under distribution shifts is\ncritical for building robust machine learning models. However, despite\nsignificant efforts in recent years, algorithmic advances in this direction\nhave been limited. In this work, we seek to understand the fundamental\ndifficulty of out-of-distribution generalization with deep neural networks. We\nfirst empirically show that perhaps surprisingly, even allowing a neural\nnetwork to explicitly fit the representations obtained from a teacher network\nthat can generalize out-of-distribution is insufficient for the generalization\nof the student network. Then, by a theoretical study of two-layer ReLU networks\noptimized by stochastic gradient descent (SGD) under a structured feature\nmodel, we identify a fundamental yet unexplored feature learning proclivity of\nneural networks, feature contamination: neural networks can learn uncorrelated\nfeatures together with predictive features, resulting in generalization failure\nunder distribution shifts. Notably, this mechanism essentially differs from the\nprevailing narrative in the literature that attributes the generalization\nfailure to spurious correlations. Overall, our results offer new insights into\nthe non-linear feature learning dynamics of neural networks and highlight the\nnecessity of considering inductive biases in out-of-distribution\ngeneralization.\n", "link": "http://arxiv.org/abs/2406.03345v1", "date": "2024-06-05", "relevancy": 1.9365, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4973}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4843}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Contamination%3A%20Neural%20Networks%20Learn%20Uncorrelated%20Features%20and%0A%20%20Fail%20to%20Generalize&body=Title%3A%20Feature%20Contamination%3A%20Neural%20Networks%20Learn%20Uncorrelated%20Features%20and%0A%20%20Fail%20to%20Generalize%0AAuthor%3A%20Tianren%20Zhang%20and%20Chujie%20Zhao%20and%20Guanyu%20Chen%20and%20Yizhou%20Jiang%20and%20Feng%20Chen%0AAbstract%3A%20%20%20Learning%20representations%20that%20generalize%20under%20distribution%20shifts%20is%0Acritical%20for%20building%20robust%20machine%20learning%20models.%20However%2C%20despite%0Asignificant%20efforts%20in%20recent%20years%2C%20algorithmic%20advances%20in%20this%20direction%0Ahave%20been%20limited.%20In%20this%20work%2C%20we%20seek%20to%20understand%20the%20fundamental%0Adifficulty%20of%20out-of-distribution%20generalization%20with%20deep%20neural%20networks.%20We%0Afirst%20empirically%20show%20that%20perhaps%20surprisingly%2C%20even%20allowing%20a%20neural%0Anetwork%20to%20explicitly%20fit%20the%20representations%20obtained%20from%20a%20teacher%20network%0Athat%20can%20generalize%20out-of-distribution%20is%20insufficient%20for%20the%20generalization%0Aof%20the%20student%20network.%20Then%2C%20by%20a%20theoretical%20study%20of%20two-layer%20ReLU%20networks%0Aoptimized%20by%20stochastic%20gradient%20descent%20%28SGD%29%20under%20a%20structured%20feature%0Amodel%2C%20we%20identify%20a%20fundamental%20yet%20unexplored%20feature%20learning%20proclivity%20of%0Aneural%20networks%2C%20feature%20contamination%3A%20neural%20networks%20can%20learn%20uncorrelated%0Afeatures%20together%20with%20predictive%20features%2C%20resulting%20in%20generalization%20failure%0Aunder%20distribution%20shifts.%20Notably%2C%20this%20mechanism%20essentially%20differs%20from%20the%0Aprevailing%20narrative%20in%20the%20literature%20that%20attributes%20the%20generalization%0Afailure%20to%20spurious%20correlations.%20Overall%2C%20our%20results%20offer%20new%20insights%20into%0Athe%20non-linear%20feature%20learning%20dynamics%20of%20neural%20networks%20and%20highlight%20the%0Anecessity%20of%20considering%20inductive%20biases%20in%20out-of-distribution%0Ageneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Contamination%253A%2520Neural%2520Networks%2520Learn%2520Uncorrelated%2520Features%2520and%250A%2520%2520Fail%2520to%2520Generalize%26entry.906535625%3DTianren%2520Zhang%2520and%2520Chujie%2520Zhao%2520and%2520Guanyu%2520Chen%2520and%2520Yizhou%2520Jiang%2520and%2520Feng%2520Chen%26entry.1292438233%3D%2520%2520Learning%2520representations%2520that%2520generalize%2520under%2520distribution%2520shifts%2520is%250Acritical%2520for%2520building%2520robust%2520machine%2520learning%2520models.%2520However%252C%2520despite%250Asignificant%2520efforts%2520in%2520recent%2520years%252C%2520algorithmic%2520advances%2520in%2520this%2520direction%250Ahave%2520been%2520limited.%2520In%2520this%2520work%252C%2520we%2520seek%2520to%2520understand%2520the%2520fundamental%250Adifficulty%2520of%2520out-of-distribution%2520generalization%2520with%2520deep%2520neural%2520networks.%2520We%250Afirst%2520empirically%2520show%2520that%2520perhaps%2520surprisingly%252C%2520even%2520allowing%2520a%2520neural%250Anetwork%2520to%2520explicitly%2520fit%2520the%2520representations%2520obtained%2520from%2520a%2520teacher%2520network%250Athat%2520can%2520generalize%2520out-of-distribution%2520is%2520insufficient%2520for%2520the%2520generalization%250Aof%2520the%2520student%2520network.%2520Then%252C%2520by%2520a%2520theoretical%2520study%2520of%2520two-layer%2520ReLU%2520networks%250Aoptimized%2520by%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520under%2520a%2520structured%2520feature%250Amodel%252C%2520we%2520identify%2520a%2520fundamental%2520yet%2520unexplored%2520feature%2520learning%2520proclivity%2520of%250Aneural%2520networks%252C%2520feature%2520contamination%253A%2520neural%2520networks%2520can%2520learn%2520uncorrelated%250Afeatures%2520together%2520with%2520predictive%2520features%252C%2520resulting%2520in%2520generalization%2520failure%250Aunder%2520distribution%2520shifts.%2520Notably%252C%2520this%2520mechanism%2520essentially%2520differs%2520from%2520the%250Aprevailing%2520narrative%2520in%2520the%2520literature%2520that%2520attributes%2520the%2520generalization%250Afailure%2520to%2520spurious%2520correlations.%2520Overall%252C%2520our%2520results%2520offer%2520new%2520insights%2520into%250Athe%2520non-linear%2520feature%2520learning%2520dynamics%2520of%2520neural%2520networks%2520and%2520highlight%2520the%250Anecessity%2520of%2520considering%2520inductive%2520biases%2520in%2520out-of-distribution%250Ageneralization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Contamination%3A%20Neural%20Networks%20Learn%20Uncorrelated%20Features%20and%0A%20%20Fail%20to%20Generalize&entry.906535625=Tianren%20Zhang%20and%20Chujie%20Zhao%20and%20Guanyu%20Chen%20and%20Yizhou%20Jiang%20and%20Feng%20Chen&entry.1292438233=%20%20Learning%20representations%20that%20generalize%20under%20distribution%20shifts%20is%0Acritical%20for%20building%20robust%20machine%20learning%20models.%20However%2C%20despite%0Asignificant%20efforts%20in%20recent%20years%2C%20algorithmic%20advances%20in%20this%20direction%0Ahave%20been%20limited.%20In%20this%20work%2C%20we%20seek%20to%20understand%20the%20fundamental%0Adifficulty%20of%20out-of-distribution%20generalization%20with%20deep%20neural%20networks.%20We%0Afirst%20empirically%20show%20that%20perhaps%20surprisingly%2C%20even%20allowing%20a%20neural%0Anetwork%20to%20explicitly%20fit%20the%20representations%20obtained%20from%20a%20teacher%20network%0Athat%20can%20generalize%20out-of-distribution%20is%20insufficient%20for%20the%20generalization%0Aof%20the%20student%20network.%20Then%2C%20by%20a%20theoretical%20study%20of%20two-layer%20ReLU%20networks%0Aoptimized%20by%20stochastic%20gradient%20descent%20%28SGD%29%20under%20a%20structured%20feature%0Amodel%2C%20we%20identify%20a%20fundamental%20yet%20unexplored%20feature%20learning%20proclivity%20of%0Aneural%20networks%2C%20feature%20contamination%3A%20neural%20networks%20can%20learn%20uncorrelated%0Afeatures%20together%20with%20predictive%20features%2C%20resulting%20in%20generalization%20failure%0Aunder%20distribution%20shifts.%20Notably%2C%20this%20mechanism%20essentially%20differs%20from%20the%0Aprevailing%20narrative%20in%20the%20literature%20that%20attributes%20the%20generalization%0Afailure%20to%20spurious%20correlations.%20Overall%2C%20our%20results%20offer%20new%20insights%20into%0Athe%20non-linear%20feature%20learning%20dynamics%20of%20neural%20networks%20and%20highlight%20the%0Anecessity%20of%20considering%20inductive%20biases%20in%20out-of-distribution%0Ageneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03345v1&entry.124074799=Read"},
{"title": "Linear Transformers with Learnable Kernel Functions are Better\n  In-Context Models", "author": "Yaroslav Aksenov and Nikita Balagansky and Sofia Maria Lo Cicero Vaina and Boris Shaposhnikov and Alexey Gorbatovski and Daniil Gavrilov", "abstract": "  Advancing the frontier of subquadratic architectures for Language Models\n(LMs) is crucial in the rapidly evolving field of natural language processing.\nCurrent innovations, including State Space Models, were initially celebrated\nfor surpassing Transformer performance on language modeling tasks. However,\nthese models have revealed deficiencies in essential In-Context Learning\ncapabilities - a domain where the Transformer traditionally shines. The Based\nmodel emerged as a hybrid solution, blending a Linear Transformer with a kernel\ninspired by the Taylor expansion of exponential functions, augmented by\nconvolutional networks. Mirroring the Transformer's in-context adeptness, it\nbecame a strong contender in the field. In our work, we present a singular,\nelegant alteration to the Based kernel that amplifies its In-Context Learning\nabilities evaluated with the Multi-Query Associative Recall task and overall\nlanguage modeling process, as demonstrated on the Pile dataset.\n", "link": "http://arxiv.org/abs/2402.10644v2", "date": "2024-06-05", "relevancy": 1.9324, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5108}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4886}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Transformers%20with%20Learnable%20Kernel%20Functions%20are%20Better%0A%20%20In-Context%20Models&body=Title%3A%20Linear%20Transformers%20with%20Learnable%20Kernel%20Functions%20are%20Better%0A%20%20In-Context%20Models%0AAuthor%3A%20Yaroslav%20Aksenov%20and%20Nikita%20Balagansky%20and%20Sofia%20Maria%20Lo%20Cicero%20Vaina%20and%20Boris%20Shaposhnikov%20and%20Alexey%20Gorbatovski%20and%20Daniil%20Gavrilov%0AAbstract%3A%20%20%20Advancing%20the%20frontier%20of%20subquadratic%20architectures%20for%20Language%20Models%0A%28LMs%29%20is%20crucial%20in%20the%20rapidly%20evolving%20field%20of%20natural%20language%20processing.%0ACurrent%20innovations%2C%20including%20State%20Space%20Models%2C%20were%20initially%20celebrated%0Afor%20surpassing%20Transformer%20performance%20on%20language%20modeling%20tasks.%20However%2C%0Athese%20models%20have%20revealed%20deficiencies%20in%20essential%20In-Context%20Learning%0Acapabilities%20-%20a%20domain%20where%20the%20Transformer%20traditionally%20shines.%20The%20Based%0Amodel%20emerged%20as%20a%20hybrid%20solution%2C%20blending%20a%20Linear%20Transformer%20with%20a%20kernel%0Ainspired%20by%20the%20Taylor%20expansion%20of%20exponential%20functions%2C%20augmented%20by%0Aconvolutional%20networks.%20Mirroring%20the%20Transformer%27s%20in-context%20adeptness%2C%20it%0Abecame%20a%20strong%20contender%20in%20the%20field.%20In%20our%20work%2C%20we%20present%20a%20singular%2C%0Aelegant%20alteration%20to%20the%20Based%20kernel%20that%20amplifies%20its%20In-Context%20Learning%0Aabilities%20evaluated%20with%20the%20Multi-Query%20Associative%20Recall%20task%20and%20overall%0Alanguage%20modeling%20process%2C%20as%20demonstrated%20on%20the%20Pile%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10644v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Transformers%2520with%2520Learnable%2520Kernel%2520Functions%2520are%2520Better%250A%2520%2520In-Context%2520Models%26entry.906535625%3DYaroslav%2520Aksenov%2520and%2520Nikita%2520Balagansky%2520and%2520Sofia%2520Maria%2520Lo%2520Cicero%2520Vaina%2520and%2520Boris%2520Shaposhnikov%2520and%2520Alexey%2520Gorbatovski%2520and%2520Daniil%2520Gavrilov%26entry.1292438233%3D%2520%2520Advancing%2520the%2520frontier%2520of%2520subquadratic%2520architectures%2520for%2520Language%2520Models%250A%2528LMs%2529%2520is%2520crucial%2520in%2520the%2520rapidly%2520evolving%2520field%2520of%2520natural%2520language%2520processing.%250ACurrent%2520innovations%252C%2520including%2520State%2520Space%2520Models%252C%2520were%2520initially%2520celebrated%250Afor%2520surpassing%2520Transformer%2520performance%2520on%2520language%2520modeling%2520tasks.%2520However%252C%250Athese%2520models%2520have%2520revealed%2520deficiencies%2520in%2520essential%2520In-Context%2520Learning%250Acapabilities%2520-%2520a%2520domain%2520where%2520the%2520Transformer%2520traditionally%2520shines.%2520The%2520Based%250Amodel%2520emerged%2520as%2520a%2520hybrid%2520solution%252C%2520blending%2520a%2520Linear%2520Transformer%2520with%2520a%2520kernel%250Ainspired%2520by%2520the%2520Taylor%2520expansion%2520of%2520exponential%2520functions%252C%2520augmented%2520by%250Aconvolutional%2520networks.%2520Mirroring%2520the%2520Transformer%2527s%2520in-context%2520adeptness%252C%2520it%250Abecame%2520a%2520strong%2520contender%2520in%2520the%2520field.%2520In%2520our%2520work%252C%2520we%2520present%2520a%2520singular%252C%250Aelegant%2520alteration%2520to%2520the%2520Based%2520kernel%2520that%2520amplifies%2520its%2520In-Context%2520Learning%250Aabilities%2520evaluated%2520with%2520the%2520Multi-Query%2520Associative%2520Recall%2520task%2520and%2520overall%250Alanguage%2520modeling%2520process%252C%2520as%2520demonstrated%2520on%2520the%2520Pile%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10644v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Transformers%20with%20Learnable%20Kernel%20Functions%20are%20Better%0A%20%20In-Context%20Models&entry.906535625=Yaroslav%20Aksenov%20and%20Nikita%20Balagansky%20and%20Sofia%20Maria%20Lo%20Cicero%20Vaina%20and%20Boris%20Shaposhnikov%20and%20Alexey%20Gorbatovski%20and%20Daniil%20Gavrilov&entry.1292438233=%20%20Advancing%20the%20frontier%20of%20subquadratic%20architectures%20for%20Language%20Models%0A%28LMs%29%20is%20crucial%20in%20the%20rapidly%20evolving%20field%20of%20natural%20language%20processing.%0ACurrent%20innovations%2C%20including%20State%20Space%20Models%2C%20were%20initially%20celebrated%0Afor%20surpassing%20Transformer%20performance%20on%20language%20modeling%20tasks.%20However%2C%0Athese%20models%20have%20revealed%20deficiencies%20in%20essential%20In-Context%20Learning%0Acapabilities%20-%20a%20domain%20where%20the%20Transformer%20traditionally%20shines.%20The%20Based%0Amodel%20emerged%20as%20a%20hybrid%20solution%2C%20blending%20a%20Linear%20Transformer%20with%20a%20kernel%0Ainspired%20by%20the%20Taylor%20expansion%20of%20exponential%20functions%2C%20augmented%20by%0Aconvolutional%20networks.%20Mirroring%20the%20Transformer%27s%20in-context%20adeptness%2C%20it%0Abecame%20a%20strong%20contender%20in%20the%20field.%20In%20our%20work%2C%20we%20present%20a%20singular%2C%0Aelegant%20alteration%20to%20the%20Based%20kernel%20that%20amplifies%20its%20In-Context%20Learning%0Aabilities%20evaluated%20with%20the%20Multi-Query%20Associative%20Recall%20task%20and%20overall%0Alanguage%20modeling%20process%2C%20as%20demonstrated%20on%20the%20Pile%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10644v2&entry.124074799=Read"},
{"title": "An Information Theoretic Approach to Machine Unlearning", "author": "Jack Foster and Kyle Fogarty and Stefan Schoepf and Cengiz \u00d6ztireli and Alexandra Brintrup", "abstract": "  To comply with AI and data regulations, the need to forget private or\ncopyrighted information from trained machine learning models is increasingly\nimportant. The key challenge in unlearning is forgetting the necessary data in\na timely manner, while preserving model performance. In this work, we address\nthe zero-shot unlearning scenario, whereby an unlearning algorithm must be able\nto remove data given only a trained model and the data to be forgotten. We\nexplore unlearning from an information theoretic perspective, connecting the\ninfluence of a sample to the information gain a model receives by observing it.\nFrom this, we derive a simple but principled zero-shot unlearning method based\non the geometry of the model. Our approach takes the form of minimising the\ngradient of a learned function with respect to a small neighbourhood around a\ntarget forget point. This induces a smoothing effect, causing forgetting by\nmoving the boundary of the classifier. We explore the intuition behind why this\napproach can jointly unlearn forget samples while preserving general model\nperformance through a series of low-dimensional experiments. We perform\nextensive empirical evaluation of our method over a range of contemporary\nbenchmarks, verifying that our method is competitive with state-of-the-art\nperformance under the strict constraints of zero-shot unlearning.\n", "link": "http://arxiv.org/abs/2402.01401v3", "date": "2024-06-05", "relevancy": 1.9317, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.489}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4806}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Information%20Theoretic%20Approach%20to%20Machine%20Unlearning&body=Title%3A%20An%20Information%20Theoretic%20Approach%20to%20Machine%20Unlearning%0AAuthor%3A%20Jack%20Foster%20and%20Kyle%20Fogarty%20and%20Stefan%20Schoepf%20and%20Cengiz%20%C3%96ztireli%20and%20Alexandra%20Brintrup%0AAbstract%3A%20%20%20To%20comply%20with%20AI%20and%20data%20regulations%2C%20the%20need%20to%20forget%20private%20or%0Acopyrighted%20information%20from%20trained%20machine%20learning%20models%20is%20increasingly%0Aimportant.%20The%20key%20challenge%20in%20unlearning%20is%20forgetting%20the%20necessary%20data%20in%0Aa%20timely%20manner%2C%20while%20preserving%20model%20performance.%20In%20this%20work%2C%20we%20address%0Athe%20zero-shot%20unlearning%20scenario%2C%20whereby%20an%20unlearning%20algorithm%20must%20be%20able%0Ato%20remove%20data%20given%20only%20a%20trained%20model%20and%20the%20data%20to%20be%20forgotten.%20We%0Aexplore%20unlearning%20from%20an%20information%20theoretic%20perspective%2C%20connecting%20the%0Ainfluence%20of%20a%20sample%20to%20the%20information%20gain%20a%20model%20receives%20by%20observing%20it.%0AFrom%20this%2C%20we%20derive%20a%20simple%20but%20principled%20zero-shot%20unlearning%20method%20based%0Aon%20the%20geometry%20of%20the%20model.%20Our%20approach%20takes%20the%20form%20of%20minimising%20the%0Agradient%20of%20a%20learned%20function%20with%20respect%20to%20a%20small%20neighbourhood%20around%20a%0Atarget%20forget%20point.%20This%20induces%20a%20smoothing%20effect%2C%20causing%20forgetting%20by%0Amoving%20the%20boundary%20of%20the%20classifier.%20We%20explore%20the%20intuition%20behind%20why%20this%0Aapproach%20can%20jointly%20unlearn%20forget%20samples%20while%20preserving%20general%20model%0Aperformance%20through%20a%20series%20of%20low-dimensional%20experiments.%20We%20perform%0Aextensive%20empirical%20evaluation%20of%20our%20method%20over%20a%20range%20of%20contemporary%0Abenchmarks%2C%20verifying%20that%20our%20method%20is%20competitive%20with%20state-of-the-art%0Aperformance%20under%20the%20strict%20constraints%20of%20zero-shot%20unlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01401v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Information%2520Theoretic%2520Approach%2520to%2520Machine%2520Unlearning%26entry.906535625%3DJack%2520Foster%2520and%2520Kyle%2520Fogarty%2520and%2520Stefan%2520Schoepf%2520and%2520Cengiz%2520%25C3%2596ztireli%2520and%2520Alexandra%2520Brintrup%26entry.1292438233%3D%2520%2520To%2520comply%2520with%2520AI%2520and%2520data%2520regulations%252C%2520the%2520need%2520to%2520forget%2520private%2520or%250Acopyrighted%2520information%2520from%2520trained%2520machine%2520learning%2520models%2520is%2520increasingly%250Aimportant.%2520The%2520key%2520challenge%2520in%2520unlearning%2520is%2520forgetting%2520the%2520necessary%2520data%2520in%250Aa%2520timely%2520manner%252C%2520while%2520preserving%2520model%2520performance.%2520In%2520this%2520work%252C%2520we%2520address%250Athe%2520zero-shot%2520unlearning%2520scenario%252C%2520whereby%2520an%2520unlearning%2520algorithm%2520must%2520be%2520able%250Ato%2520remove%2520data%2520given%2520only%2520a%2520trained%2520model%2520and%2520the%2520data%2520to%2520be%2520forgotten.%2520We%250Aexplore%2520unlearning%2520from%2520an%2520information%2520theoretic%2520perspective%252C%2520connecting%2520the%250Ainfluence%2520of%2520a%2520sample%2520to%2520the%2520information%2520gain%2520a%2520model%2520receives%2520by%2520observing%2520it.%250AFrom%2520this%252C%2520we%2520derive%2520a%2520simple%2520but%2520principled%2520zero-shot%2520unlearning%2520method%2520based%250Aon%2520the%2520geometry%2520of%2520the%2520model.%2520Our%2520approach%2520takes%2520the%2520form%2520of%2520minimising%2520the%250Agradient%2520of%2520a%2520learned%2520function%2520with%2520respect%2520to%2520a%2520small%2520neighbourhood%2520around%2520a%250Atarget%2520forget%2520point.%2520This%2520induces%2520a%2520smoothing%2520effect%252C%2520causing%2520forgetting%2520by%250Amoving%2520the%2520boundary%2520of%2520the%2520classifier.%2520We%2520explore%2520the%2520intuition%2520behind%2520why%2520this%250Aapproach%2520can%2520jointly%2520unlearn%2520forget%2520samples%2520while%2520preserving%2520general%2520model%250Aperformance%2520through%2520a%2520series%2520of%2520low-dimensional%2520experiments.%2520We%2520perform%250Aextensive%2520empirical%2520evaluation%2520of%2520our%2520method%2520over%2520a%2520range%2520of%2520contemporary%250Abenchmarks%252C%2520verifying%2520that%2520our%2520method%2520is%2520competitive%2520with%2520state-of-the-art%250Aperformance%2520under%2520the%2520strict%2520constraints%2520of%2520zero-shot%2520unlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01401v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Information%20Theoretic%20Approach%20to%20Machine%20Unlearning&entry.906535625=Jack%20Foster%20and%20Kyle%20Fogarty%20and%20Stefan%20Schoepf%20and%20Cengiz%20%C3%96ztireli%20and%20Alexandra%20Brintrup&entry.1292438233=%20%20To%20comply%20with%20AI%20and%20data%20regulations%2C%20the%20need%20to%20forget%20private%20or%0Acopyrighted%20information%20from%20trained%20machine%20learning%20models%20is%20increasingly%0Aimportant.%20The%20key%20challenge%20in%20unlearning%20is%20forgetting%20the%20necessary%20data%20in%0Aa%20timely%20manner%2C%20while%20preserving%20model%20performance.%20In%20this%20work%2C%20we%20address%0Athe%20zero-shot%20unlearning%20scenario%2C%20whereby%20an%20unlearning%20algorithm%20must%20be%20able%0Ato%20remove%20data%20given%20only%20a%20trained%20model%20and%20the%20data%20to%20be%20forgotten.%20We%0Aexplore%20unlearning%20from%20an%20information%20theoretic%20perspective%2C%20connecting%20the%0Ainfluence%20of%20a%20sample%20to%20the%20information%20gain%20a%20model%20receives%20by%20observing%20it.%0AFrom%20this%2C%20we%20derive%20a%20simple%20but%20principled%20zero-shot%20unlearning%20method%20based%0Aon%20the%20geometry%20of%20the%20model.%20Our%20approach%20takes%20the%20form%20of%20minimising%20the%0Agradient%20of%20a%20learned%20function%20with%20respect%20to%20a%20small%20neighbourhood%20around%20a%0Atarget%20forget%20point.%20This%20induces%20a%20smoothing%20effect%2C%20causing%20forgetting%20by%0Amoving%20the%20boundary%20of%20the%20classifier.%20We%20explore%20the%20intuition%20behind%20why%20this%0Aapproach%20can%20jointly%20unlearn%20forget%20samples%20while%20preserving%20general%20model%0Aperformance%20through%20a%20series%20of%20low-dimensional%20experiments.%20We%20perform%0Aextensive%20empirical%20evaluation%20of%20our%20method%20over%20a%20range%20of%20contemporary%0Abenchmarks%2C%20verifying%20that%20our%20method%20is%20competitive%20with%20state-of-the-art%0Aperformance%20under%20the%20strict%20constraints%20of%20zero-shot%20unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01401v3&entry.124074799=Read"},
{"title": "Defending Large Language Models Against Attacks With Residual Stream\n  Activation Analysis", "author": "Amelia Kawasaki and Andrew Davis and Houssam Abbas", "abstract": "  The widespread adoption of Large Language Models (LLMs), exemplified by\nOpenAI's ChatGPT, brings to the forefront the imperative to defend against\nadversarial threats on these models. These attacks, which manipulate an LLM's\noutput by introducing malicious inputs, undermine the model's integrity and the\ntrust users place in its outputs. In response to this challenge, our paper\npresents an innovative defensive strategy, given white box access to an LLM,\nthat harnesses residual activation analysis between transformer layers of the\nLLM. We apply an established methodology for analyzing distinctive activation\npatterns in the residual streams for a novel result of attack prompt\nclassification. We curate multiple datasets to demonstrate how this method of\nclassification has high accuracy across multiple types of attack scenarios,\nincluding our newly-created attack dataset. Furthermore, we enhance the model's\nresilience by integrating safety fine-tuning techniques for LLMs in order to\nmeasure its effect on our capability to detect attacks. The results underscore\nthe effectiveness of our approach in enhancing the detection and mitigation of\nadversarial inputs, advancing the security framework within which LLMs operate.\n", "link": "http://arxiv.org/abs/2406.03230v1", "date": "2024-06-05", "relevancy": 1.9316, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4908}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4841}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defending%20Large%20Language%20Models%20Against%20Attacks%20With%20Residual%20Stream%0A%20%20Activation%20Analysis&body=Title%3A%20Defending%20Large%20Language%20Models%20Against%20Attacks%20With%20Residual%20Stream%0A%20%20Activation%20Analysis%0AAuthor%3A%20Amelia%20Kawasaki%20and%20Andrew%20Davis%20and%20Houssam%20Abbas%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20exemplified%20by%0AOpenAI%27s%20ChatGPT%2C%20brings%20to%20the%20forefront%20the%20imperative%20to%20defend%20against%0Aadversarial%20threats%20on%20these%20models.%20These%20attacks%2C%20which%20manipulate%20an%20LLM%27s%0Aoutput%20by%20introducing%20malicious%20inputs%2C%20undermine%20the%20model%27s%20integrity%20and%20the%0Atrust%20users%20place%20in%20its%20outputs.%20In%20response%20to%20this%20challenge%2C%20our%20paper%0Apresents%20an%20innovative%20defensive%20strategy%2C%20given%20white%20box%20access%20to%20an%20LLM%2C%0Athat%20harnesses%20residual%20activation%20analysis%20between%20transformer%20layers%20of%20the%0ALLM.%20We%20apply%20an%20established%20methodology%20for%20analyzing%20distinctive%20activation%0Apatterns%20in%20the%20residual%20streams%20for%20a%20novel%20result%20of%20attack%20prompt%0Aclassification.%20We%20curate%20multiple%20datasets%20to%20demonstrate%20how%20this%20method%20of%0Aclassification%20has%20high%20accuracy%20across%20multiple%20types%20of%20attack%20scenarios%2C%0Aincluding%20our%20newly-created%20attack%20dataset.%20Furthermore%2C%20we%20enhance%20the%20model%27s%0Aresilience%20by%20integrating%20safety%20fine-tuning%20techniques%20for%20LLMs%20in%20order%20to%0Ameasure%20its%20effect%20on%20our%20capability%20to%20detect%20attacks.%20The%20results%20underscore%0Athe%20effectiveness%20of%20our%20approach%20in%20enhancing%20the%20detection%20and%20mitigation%20of%0Aadversarial%20inputs%2C%20advancing%20the%20security%20framework%20within%20which%20LLMs%20operate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefending%2520Large%2520Language%2520Models%2520Against%2520Attacks%2520With%2520Residual%2520Stream%250A%2520%2520Activation%2520Analysis%26entry.906535625%3DAmelia%2520Kawasaki%2520and%2520Andrew%2520Davis%2520and%2520Houssam%2520Abbas%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520exemplified%2520by%250AOpenAI%2527s%2520ChatGPT%252C%2520brings%2520to%2520the%2520forefront%2520the%2520imperative%2520to%2520defend%2520against%250Aadversarial%2520threats%2520on%2520these%2520models.%2520These%2520attacks%252C%2520which%2520manipulate%2520an%2520LLM%2527s%250Aoutput%2520by%2520introducing%2520malicious%2520inputs%252C%2520undermine%2520the%2520model%2527s%2520integrity%2520and%2520the%250Atrust%2520users%2520place%2520in%2520its%2520outputs.%2520In%2520response%2520to%2520this%2520challenge%252C%2520our%2520paper%250Apresents%2520an%2520innovative%2520defensive%2520strategy%252C%2520given%2520white%2520box%2520access%2520to%2520an%2520LLM%252C%250Athat%2520harnesses%2520residual%2520activation%2520analysis%2520between%2520transformer%2520layers%2520of%2520the%250ALLM.%2520We%2520apply%2520an%2520established%2520methodology%2520for%2520analyzing%2520distinctive%2520activation%250Apatterns%2520in%2520the%2520residual%2520streams%2520for%2520a%2520novel%2520result%2520of%2520attack%2520prompt%250Aclassification.%2520We%2520curate%2520multiple%2520datasets%2520to%2520demonstrate%2520how%2520this%2520method%2520of%250Aclassification%2520has%2520high%2520accuracy%2520across%2520multiple%2520types%2520of%2520attack%2520scenarios%252C%250Aincluding%2520our%2520newly-created%2520attack%2520dataset.%2520Furthermore%252C%2520we%2520enhance%2520the%2520model%2527s%250Aresilience%2520by%2520integrating%2520safety%2520fine-tuning%2520techniques%2520for%2520LLMs%2520in%2520order%2520to%250Ameasure%2520its%2520effect%2520on%2520our%2520capability%2520to%2520detect%2520attacks.%2520The%2520results%2520underscore%250Athe%2520effectiveness%2520of%2520our%2520approach%2520in%2520enhancing%2520the%2520detection%2520and%2520mitigation%2520of%250Aadversarial%2520inputs%252C%2520advancing%2520the%2520security%2520framework%2520within%2520which%2520LLMs%2520operate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defending%20Large%20Language%20Models%20Against%20Attacks%20With%20Residual%20Stream%0A%20%20Activation%20Analysis&entry.906535625=Amelia%20Kawasaki%20and%20Andrew%20Davis%20and%20Houssam%20Abbas&entry.1292438233=%20%20The%20widespread%20adoption%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20exemplified%20by%0AOpenAI%27s%20ChatGPT%2C%20brings%20to%20the%20forefront%20the%20imperative%20to%20defend%20against%0Aadversarial%20threats%20on%20these%20models.%20These%20attacks%2C%20which%20manipulate%20an%20LLM%27s%0Aoutput%20by%20introducing%20malicious%20inputs%2C%20undermine%20the%20model%27s%20integrity%20and%20the%0Atrust%20users%20place%20in%20its%20outputs.%20In%20response%20to%20this%20challenge%2C%20our%20paper%0Apresents%20an%20innovative%20defensive%20strategy%2C%20given%20white%20box%20access%20to%20an%20LLM%2C%0Athat%20harnesses%20residual%20activation%20analysis%20between%20transformer%20layers%20of%20the%0ALLM.%20We%20apply%20an%20established%20methodology%20for%20analyzing%20distinctive%20activation%0Apatterns%20in%20the%20residual%20streams%20for%20a%20novel%20result%20of%20attack%20prompt%0Aclassification.%20We%20curate%20multiple%20datasets%20to%20demonstrate%20how%20this%20method%20of%0Aclassification%20has%20high%20accuracy%20across%20multiple%20types%20of%20attack%20scenarios%2C%0Aincluding%20our%20newly-created%20attack%20dataset.%20Furthermore%2C%20we%20enhance%20the%20model%27s%0Aresilience%20by%20integrating%20safety%20fine-tuning%20techniques%20for%20LLMs%20in%20order%20to%0Ameasure%20its%20effect%20on%20our%20capability%20to%20detect%20attacks.%20The%20results%20underscore%0Athe%20effectiveness%20of%20our%20approach%20in%20enhancing%20the%20detection%20and%20mitigation%20of%0Aadversarial%20inputs%2C%20advancing%20the%20security%20framework%20within%20which%20LLMs%20operate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03230v1&entry.124074799=Read"},
{"title": "Compressed Federated Reinforcement Learning with a Generative Model", "author": "Ali Beikmohammadi and Sarit Khirirat and Sindri Magn\u00fasson", "abstract": "  Reinforcement learning has recently gained unprecedented popularity, yet it\nstill grapples with sample inefficiency. Addressing this challenge, federated\nreinforcement learning (FedRL) has emerged, wherein agents collaboratively\nlearn a single policy by aggregating local estimations. However, this\naggregation step incurs significant communication costs. In this paper, we\npropose CompFedRL, a communication-efficient FedRL approach incorporating both\n\\textit{periodic aggregation} and (direct/error-feedback) compression\nmechanisms. Specifically, we consider compressed federated $Q$-learning with a\ngenerative model setup, where a central server learns an optimal $Q$-function\nby periodically aggregating compressed $Q$-estimates from local agents. For the\nfirst time, we characterize the impact of these two mechanisms (which have\nremained elusive) by providing a finite-time analysis of our algorithm,\ndemonstrating strong convergence behaviors when utilizing either direct or\nerror-feedback compression. Our bounds indicate improved solution accuracy\nconcerning the number of agents and other federated hyperparameters while\nsimultaneously reducing communication costs. To corroborate our theory, we also\nconduct in-depth numerical experiments to verify our findings, considering\nTop-$K$ and Sparsified-$K$ sparsification operators.\n", "link": "http://arxiv.org/abs/2404.10635v2", "date": "2024-06-05", "relevancy": 1.931, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.498}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.482}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressed%20Federated%20Reinforcement%20Learning%20with%20a%20Generative%20Model&body=Title%3A%20Compressed%20Federated%20Reinforcement%20Learning%20with%20a%20Generative%20Model%0AAuthor%3A%20Ali%20Beikmohammadi%20and%20Sarit%20Khirirat%20and%20Sindri%20Magn%C3%BAsson%0AAbstract%3A%20%20%20Reinforcement%20learning%20has%20recently%20gained%20unprecedented%20popularity%2C%20yet%20it%0Astill%20grapples%20with%20sample%20inefficiency.%20Addressing%20this%20challenge%2C%20federated%0Areinforcement%20learning%20%28FedRL%29%20has%20emerged%2C%20wherein%20agents%20collaboratively%0Alearn%20a%20single%20policy%20by%20aggregating%20local%20estimations.%20However%2C%20this%0Aaggregation%20step%20incurs%20significant%20communication%20costs.%20In%20this%20paper%2C%20we%0Apropose%20CompFedRL%2C%20a%20communication-efficient%20FedRL%20approach%20incorporating%20both%0A%5Ctextit%7Bperiodic%20aggregation%7D%20and%20%28direct/error-feedback%29%20compression%0Amechanisms.%20Specifically%2C%20we%20consider%20compressed%20federated%20%24Q%24-learning%20with%20a%0Agenerative%20model%20setup%2C%20where%20a%20central%20server%20learns%20an%20optimal%20%24Q%24-function%0Aby%20periodically%20aggregating%20compressed%20%24Q%24-estimates%20from%20local%20agents.%20For%20the%0Afirst%20time%2C%20we%20characterize%20the%20impact%20of%20these%20two%20mechanisms%20%28which%20have%0Aremained%20elusive%29%20by%20providing%20a%20finite-time%20analysis%20of%20our%20algorithm%2C%0Ademonstrating%20strong%20convergence%20behaviors%20when%20utilizing%20either%20direct%20or%0Aerror-feedback%20compression.%20Our%20bounds%20indicate%20improved%20solution%20accuracy%0Aconcerning%20the%20number%20of%20agents%20and%20other%20federated%20hyperparameters%20while%0Asimultaneously%20reducing%20communication%20costs.%20To%20corroborate%20our%20theory%2C%20we%20also%0Aconduct%20in-depth%20numerical%20experiments%20to%20verify%20our%20findings%2C%20considering%0ATop-%24K%24%20and%20Sparsified-%24K%24%20sparsification%20operators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressed%2520Federated%2520Reinforcement%2520Learning%2520with%2520a%2520Generative%2520Model%26entry.906535625%3DAli%2520Beikmohammadi%2520and%2520Sarit%2520Khirirat%2520and%2520Sindri%2520Magn%25C3%25BAsson%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520has%2520recently%2520gained%2520unprecedented%2520popularity%252C%2520yet%2520it%250Astill%2520grapples%2520with%2520sample%2520inefficiency.%2520Addressing%2520this%2520challenge%252C%2520federated%250Areinforcement%2520learning%2520%2528FedRL%2529%2520has%2520emerged%252C%2520wherein%2520agents%2520collaboratively%250Alearn%2520a%2520single%2520policy%2520by%2520aggregating%2520local%2520estimations.%2520However%252C%2520this%250Aaggregation%2520step%2520incurs%2520significant%2520communication%2520costs.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520CompFedRL%252C%2520a%2520communication-efficient%2520FedRL%2520approach%2520incorporating%2520both%250A%255Ctextit%257Bperiodic%2520aggregation%257D%2520and%2520%2528direct/error-feedback%2529%2520compression%250Amechanisms.%2520Specifically%252C%2520we%2520consider%2520compressed%2520federated%2520%2524Q%2524-learning%2520with%2520a%250Agenerative%2520model%2520setup%252C%2520where%2520a%2520central%2520server%2520learns%2520an%2520optimal%2520%2524Q%2524-function%250Aby%2520periodically%2520aggregating%2520compressed%2520%2524Q%2524-estimates%2520from%2520local%2520agents.%2520For%2520the%250Afirst%2520time%252C%2520we%2520characterize%2520the%2520impact%2520of%2520these%2520two%2520mechanisms%2520%2528which%2520have%250Aremained%2520elusive%2529%2520by%2520providing%2520a%2520finite-time%2520analysis%2520of%2520our%2520algorithm%252C%250Ademonstrating%2520strong%2520convergence%2520behaviors%2520when%2520utilizing%2520either%2520direct%2520or%250Aerror-feedback%2520compression.%2520Our%2520bounds%2520indicate%2520improved%2520solution%2520accuracy%250Aconcerning%2520the%2520number%2520of%2520agents%2520and%2520other%2520federated%2520hyperparameters%2520while%250Asimultaneously%2520reducing%2520communication%2520costs.%2520To%2520corroborate%2520our%2520theory%252C%2520we%2520also%250Aconduct%2520in-depth%2520numerical%2520experiments%2520to%2520verify%2520our%2520findings%252C%2520considering%250ATop-%2524K%2524%2520and%2520Sparsified-%2524K%2524%2520sparsification%2520operators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressed%20Federated%20Reinforcement%20Learning%20with%20a%20Generative%20Model&entry.906535625=Ali%20Beikmohammadi%20and%20Sarit%20Khirirat%20and%20Sindri%20Magn%C3%BAsson&entry.1292438233=%20%20Reinforcement%20learning%20has%20recently%20gained%20unprecedented%20popularity%2C%20yet%20it%0Astill%20grapples%20with%20sample%20inefficiency.%20Addressing%20this%20challenge%2C%20federated%0Areinforcement%20learning%20%28FedRL%29%20has%20emerged%2C%20wherein%20agents%20collaboratively%0Alearn%20a%20single%20policy%20by%20aggregating%20local%20estimations.%20However%2C%20this%0Aaggregation%20step%20incurs%20significant%20communication%20costs.%20In%20this%20paper%2C%20we%0Apropose%20CompFedRL%2C%20a%20communication-efficient%20FedRL%20approach%20incorporating%20both%0A%5Ctextit%7Bperiodic%20aggregation%7D%20and%20%28direct/error-feedback%29%20compression%0Amechanisms.%20Specifically%2C%20we%20consider%20compressed%20federated%20%24Q%24-learning%20with%20a%0Agenerative%20model%20setup%2C%20where%20a%20central%20server%20learns%20an%20optimal%20%24Q%24-function%0Aby%20periodically%20aggregating%20compressed%20%24Q%24-estimates%20from%20local%20agents.%20For%20the%0Afirst%20time%2C%20we%20characterize%20the%20impact%20of%20these%20two%20mechanisms%20%28which%20have%0Aremained%20elusive%29%20by%20providing%20a%20finite-time%20analysis%20of%20our%20algorithm%2C%0Ademonstrating%20strong%20convergence%20behaviors%20when%20utilizing%20either%20direct%20or%0Aerror-feedback%20compression.%20Our%20bounds%20indicate%20improved%20solution%20accuracy%0Aconcerning%20the%20number%20of%20agents%20and%20other%20federated%20hyperparameters%20while%0Asimultaneously%20reducing%20communication%20costs.%20To%20corroborate%20our%20theory%2C%20we%20also%0Aconduct%20in-depth%20numerical%20experiments%20to%20verify%20our%20findings%2C%20considering%0ATop-%24K%24%20and%20Sparsified-%24K%24%20sparsification%20operators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10635v2&entry.124074799=Read"},
{"title": "Unified PAC-Bayesian Study of Pessimism for Offline Policy Learning with\n  Regularized Importance Sampling", "author": "Imad Aouali and Victor-Emmanuel Brunel and David Rohde and Anna Korba", "abstract": "  Off-policy learning (OPL) often involves minimizing a risk estimator based on\nimportance weighting to correct bias from the logging policy used to collect\ndata. However, this method can produce an estimator with a high variance. A\ncommon solution is to regularize the importance weights and learn the policy by\nminimizing an estimator with penalties derived from generalization bounds\nspecific to the estimator. This approach, known as pessimism, has gained recent\nattention but lacks a unified framework for analysis. To address this gap, we\nintroduce a comprehensive PAC-Bayesian framework to examine pessimism with\nregularized importance weighting. We derive a tractable PAC-Bayesian\ngeneralization bound that universally applies to common importance weight\nregularizations, enabling their comparison within a single framework. Our\nempirical results challenge common understanding, demonstrating the\neffectiveness of standard IW regularization techniques.\n", "link": "http://arxiv.org/abs/2406.03434v1", "date": "2024-06-05", "relevancy": 1.9198, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20PAC-Bayesian%20Study%20of%20Pessimism%20for%20Offline%20Policy%20Learning%20with%0A%20%20Regularized%20Importance%20Sampling&body=Title%3A%20Unified%20PAC-Bayesian%20Study%20of%20Pessimism%20for%20Offline%20Policy%20Learning%20with%0A%20%20Regularized%20Importance%20Sampling%0AAuthor%3A%20Imad%20Aouali%20and%20Victor-Emmanuel%20Brunel%20and%20David%20Rohde%20and%20Anna%20Korba%0AAbstract%3A%20%20%20Off-policy%20learning%20%28OPL%29%20often%20involves%20minimizing%20a%20risk%20estimator%20based%20on%0Aimportance%20weighting%20to%20correct%20bias%20from%20the%20logging%20policy%20used%20to%20collect%0Adata.%20However%2C%20this%20method%20can%20produce%20an%20estimator%20with%20a%20high%20variance.%20A%0Acommon%20solution%20is%20to%20regularize%20the%20importance%20weights%20and%20learn%20the%20policy%20by%0Aminimizing%20an%20estimator%20with%20penalties%20derived%20from%20generalization%20bounds%0Aspecific%20to%20the%20estimator.%20This%20approach%2C%20known%20as%20pessimism%2C%20has%20gained%20recent%0Aattention%20but%20lacks%20a%20unified%20framework%20for%20analysis.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20a%20comprehensive%20PAC-Bayesian%20framework%20to%20examine%20pessimism%20with%0Aregularized%20importance%20weighting.%20We%20derive%20a%20tractable%20PAC-Bayesian%0Ageneralization%20bound%20that%20universally%20applies%20to%20common%20importance%20weight%0Aregularizations%2C%20enabling%20their%20comparison%20within%20a%20single%20framework.%20Our%0Aempirical%20results%20challenge%20common%20understanding%2C%20demonstrating%20the%0Aeffectiveness%20of%20standard%20IW%20regularization%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520PAC-Bayesian%2520Study%2520of%2520Pessimism%2520for%2520Offline%2520Policy%2520Learning%2520with%250A%2520%2520Regularized%2520Importance%2520Sampling%26entry.906535625%3DImad%2520Aouali%2520and%2520Victor-Emmanuel%2520Brunel%2520and%2520David%2520Rohde%2520and%2520Anna%2520Korba%26entry.1292438233%3D%2520%2520Off-policy%2520learning%2520%2528OPL%2529%2520often%2520involves%2520minimizing%2520a%2520risk%2520estimator%2520based%2520on%250Aimportance%2520weighting%2520to%2520correct%2520bias%2520from%2520the%2520logging%2520policy%2520used%2520to%2520collect%250Adata.%2520However%252C%2520this%2520method%2520can%2520produce%2520an%2520estimator%2520with%2520a%2520high%2520variance.%2520A%250Acommon%2520solution%2520is%2520to%2520regularize%2520the%2520importance%2520weights%2520and%2520learn%2520the%2520policy%2520by%250Aminimizing%2520an%2520estimator%2520with%2520penalties%2520derived%2520from%2520generalization%2520bounds%250Aspecific%2520to%2520the%2520estimator.%2520This%2520approach%252C%2520known%2520as%2520pessimism%252C%2520has%2520gained%2520recent%250Aattention%2520but%2520lacks%2520a%2520unified%2520framework%2520for%2520analysis.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520a%2520comprehensive%2520PAC-Bayesian%2520framework%2520to%2520examine%2520pessimism%2520with%250Aregularized%2520importance%2520weighting.%2520We%2520derive%2520a%2520tractable%2520PAC-Bayesian%250Ageneralization%2520bound%2520that%2520universally%2520applies%2520to%2520common%2520importance%2520weight%250Aregularizations%252C%2520enabling%2520their%2520comparison%2520within%2520a%2520single%2520framework.%2520Our%250Aempirical%2520results%2520challenge%2520common%2520understanding%252C%2520demonstrating%2520the%250Aeffectiveness%2520of%2520standard%2520IW%2520regularization%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20PAC-Bayesian%20Study%20of%20Pessimism%20for%20Offline%20Policy%20Learning%20with%0A%20%20Regularized%20Importance%20Sampling&entry.906535625=Imad%20Aouali%20and%20Victor-Emmanuel%20Brunel%20and%20David%20Rohde%20and%20Anna%20Korba&entry.1292438233=%20%20Off-policy%20learning%20%28OPL%29%20often%20involves%20minimizing%20a%20risk%20estimator%20based%20on%0Aimportance%20weighting%20to%20correct%20bias%20from%20the%20logging%20policy%20used%20to%20collect%0Adata.%20However%2C%20this%20method%20can%20produce%20an%20estimator%20with%20a%20high%20variance.%20A%0Acommon%20solution%20is%20to%20regularize%20the%20importance%20weights%20and%20learn%20the%20policy%20by%0Aminimizing%20an%20estimator%20with%20penalties%20derived%20from%20generalization%20bounds%0Aspecific%20to%20the%20estimator.%20This%20approach%2C%20known%20as%20pessimism%2C%20has%20gained%20recent%0Aattention%20but%20lacks%20a%20unified%20framework%20for%20analysis.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20a%20comprehensive%20PAC-Bayesian%20framework%20to%20examine%20pessimism%20with%0Aregularized%20importance%20weighting.%20We%20derive%20a%20tractable%20PAC-Bayesian%0Ageneralization%20bound%20that%20universally%20applies%20to%20common%20importance%20weight%0Aregularizations%2C%20enabling%20their%20comparison%20within%20a%20single%20framework.%20Our%0Aempirical%20results%20challenge%20common%20understanding%2C%20demonstrating%20the%0Aeffectiveness%20of%20standard%20IW%20regularization%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03434v1&entry.124074799=Read"},
{"title": "VWise: A novel benchmark for evaluating scene classification for\n  vehicular applications", "author": "Pedro Azevedo and Emanuella Ara\u00fajo and Gabriel Pierre and Willams de Lima Costa and Jo\u00e3o Marcelo Teixeira and Valter Ferreira and Roberto Jones and Veronica Teichrieb", "abstract": "  Current datasets for vehicular applications are mostly collected in North\nAmerica or Europe. Models trained or evaluated on these datasets might suffer\nfrom geographical bias when deployed in other regions. Specifically, for scene\nclassification, a highway in a Latin American country differs drastically from\nan Autobahn, for example, both in design and maintenance levels. We propose\nVWise, a novel benchmark for road-type classification and scene classification\ntasks, in addition to tasks focused on external contexts related to vehicular\napplications in LatAm. We collected over 520 video clips covering diverse urban\nand rural environments across Latin American countries, annotated with six\nclasses of road types. We also evaluated several state-of-the-art\nclassification models in baseline experiments, obtaining over 84% accuracy.\nWith this dataset, we aim to enhance research on vehicular tasks in Latin\nAmerica.\n", "link": "http://arxiv.org/abs/2406.03273v1", "date": "2024-06-05", "relevancy": 1.9191, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.511}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4783}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VWise%3A%20A%20novel%20benchmark%20for%20evaluating%20scene%20classification%20for%0A%20%20vehicular%20applications&body=Title%3A%20VWise%3A%20A%20novel%20benchmark%20for%20evaluating%20scene%20classification%20for%0A%20%20vehicular%20applications%0AAuthor%3A%20Pedro%20Azevedo%20and%20Emanuella%20Ara%C3%BAjo%20and%20Gabriel%20Pierre%20and%20Willams%20de%20Lima%20Costa%20and%20Jo%C3%A3o%20Marcelo%20Teixeira%20and%20Valter%20Ferreira%20and%20Roberto%20Jones%20and%20Veronica%20Teichrieb%0AAbstract%3A%20%20%20Current%20datasets%20for%20vehicular%20applications%20are%20mostly%20collected%20in%20North%0AAmerica%20or%20Europe.%20Models%20trained%20or%20evaluated%20on%20these%20datasets%20might%20suffer%0Afrom%20geographical%20bias%20when%20deployed%20in%20other%20regions.%20Specifically%2C%20for%20scene%0Aclassification%2C%20a%20highway%20in%20a%20Latin%20American%20country%20differs%20drastically%20from%0Aan%20Autobahn%2C%20for%20example%2C%20both%20in%20design%20and%20maintenance%20levels.%20We%20propose%0AVWise%2C%20a%20novel%20benchmark%20for%20road-type%20classification%20and%20scene%20classification%0Atasks%2C%20in%20addition%20to%20tasks%20focused%20on%20external%20contexts%20related%20to%20vehicular%0Aapplications%20in%20LatAm.%20We%20collected%20over%20520%20video%20clips%20covering%20diverse%20urban%0Aand%20rural%20environments%20across%20Latin%20American%20countries%2C%20annotated%20with%20six%0Aclasses%20of%20road%20types.%20We%20also%20evaluated%20several%20state-of-the-art%0Aclassification%20models%20in%20baseline%20experiments%2C%20obtaining%20over%2084%25%20accuracy.%0AWith%20this%20dataset%2C%20we%20aim%20to%20enhance%20research%20on%20vehicular%20tasks%20in%20Latin%0AAmerica.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVWise%253A%2520A%2520novel%2520benchmark%2520for%2520evaluating%2520scene%2520classification%2520for%250A%2520%2520vehicular%2520applications%26entry.906535625%3DPedro%2520Azevedo%2520and%2520Emanuella%2520Ara%25C3%25BAjo%2520and%2520Gabriel%2520Pierre%2520and%2520Willams%2520de%2520Lima%2520Costa%2520and%2520Jo%25C3%25A3o%2520Marcelo%2520Teixeira%2520and%2520Valter%2520Ferreira%2520and%2520Roberto%2520Jones%2520and%2520Veronica%2520Teichrieb%26entry.1292438233%3D%2520%2520Current%2520datasets%2520for%2520vehicular%2520applications%2520are%2520mostly%2520collected%2520in%2520North%250AAmerica%2520or%2520Europe.%2520Models%2520trained%2520or%2520evaluated%2520on%2520these%2520datasets%2520might%2520suffer%250Afrom%2520geographical%2520bias%2520when%2520deployed%2520in%2520other%2520regions.%2520Specifically%252C%2520for%2520scene%250Aclassification%252C%2520a%2520highway%2520in%2520a%2520Latin%2520American%2520country%2520differs%2520drastically%2520from%250Aan%2520Autobahn%252C%2520for%2520example%252C%2520both%2520in%2520design%2520and%2520maintenance%2520levels.%2520We%2520propose%250AVWise%252C%2520a%2520novel%2520benchmark%2520for%2520road-type%2520classification%2520and%2520scene%2520classification%250Atasks%252C%2520in%2520addition%2520to%2520tasks%2520focused%2520on%2520external%2520contexts%2520related%2520to%2520vehicular%250Aapplications%2520in%2520LatAm.%2520We%2520collected%2520over%2520520%2520video%2520clips%2520covering%2520diverse%2520urban%250Aand%2520rural%2520environments%2520across%2520Latin%2520American%2520countries%252C%2520annotated%2520with%2520six%250Aclasses%2520of%2520road%2520types.%2520We%2520also%2520evaluated%2520several%2520state-of-the-art%250Aclassification%2520models%2520in%2520baseline%2520experiments%252C%2520obtaining%2520over%252084%2525%2520accuracy.%250AWith%2520this%2520dataset%252C%2520we%2520aim%2520to%2520enhance%2520research%2520on%2520vehicular%2520tasks%2520in%2520Latin%250AAmerica.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VWise%3A%20A%20novel%20benchmark%20for%20evaluating%20scene%20classification%20for%0A%20%20vehicular%20applications&entry.906535625=Pedro%20Azevedo%20and%20Emanuella%20Ara%C3%BAjo%20and%20Gabriel%20Pierre%20and%20Willams%20de%20Lima%20Costa%20and%20Jo%C3%A3o%20Marcelo%20Teixeira%20and%20Valter%20Ferreira%20and%20Roberto%20Jones%20and%20Veronica%20Teichrieb&entry.1292438233=%20%20Current%20datasets%20for%20vehicular%20applications%20are%20mostly%20collected%20in%20North%0AAmerica%20or%20Europe.%20Models%20trained%20or%20evaluated%20on%20these%20datasets%20might%20suffer%0Afrom%20geographical%20bias%20when%20deployed%20in%20other%20regions.%20Specifically%2C%20for%20scene%0Aclassification%2C%20a%20highway%20in%20a%20Latin%20American%20country%20differs%20drastically%20from%0Aan%20Autobahn%2C%20for%20example%2C%20both%20in%20design%20and%20maintenance%20levels.%20We%20propose%0AVWise%2C%20a%20novel%20benchmark%20for%20road-type%20classification%20and%20scene%20classification%0Atasks%2C%20in%20addition%20to%20tasks%20focused%20on%20external%20contexts%20related%20to%20vehicular%0Aapplications%20in%20LatAm.%20We%20collected%20over%20520%20video%20clips%20covering%20diverse%20urban%0Aand%20rural%20environments%20across%20Latin%20American%20countries%2C%20annotated%20with%20six%0Aclasses%20of%20road%20types.%20We%20also%20evaluated%20several%20state-of-the-art%0Aclassification%20models%20in%20baseline%20experiments%2C%20obtaining%20over%2084%25%20accuracy.%0AWith%20this%20dataset%2C%20we%20aim%20to%20enhance%20research%20on%20vehicular%20tasks%20in%20Latin%0AAmerica.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03273v1&entry.124074799=Read"},
{"title": "RAFT: Adapting Language Model to Domain Specific RAG", "author": "Tianjun Zhang and Shishir G. Patil and Naman Jain and Sheng Shen and Matei Zaharia and Ion Stoica and Joseph E. Gonzalez", "abstract": "  Pretraining Large Language Models (LLMs) on large corpora of textual data is\nnow a standard paradigm. When using these LLMs for many downstream\napplications, it is common to additionally bake in new knowledge (e.g.,\ntime-critical news, or private domain knowledge) into the pretrained model\neither through RAG-based-prompting, or fine-tuning. However, the optimal\nmethodology for the model to gain such new knowledge remains an open question.\nIn this paper, we present Retrieval Augmented FineTuning (RAFT), a training\nrecipe that improves the model's ability to answer questions in a \"open-book\"\nin-domain settings. In RAFT, given a question, and a set of retrieved\ndocuments, we train the model to ignore those documents that don't help in\nanswering the question, which we call, distractor documents. RAFT accomplishes\nthis by citing verbatim the right sequence from the relevant document that\nwould help answer the question. This coupled with RAFT's chain-of-thought-style\nresponse helps improve the model's ability to reason. In domain-specific RAG,\nRAFT consistently improves the model's performance across PubMed, HotpotQA, and\nGorilla datasets, presenting a post-training recipe to improve pre-trained LLMs\nto in-domain RAG. RAFT's code and demo are open-sourced at\ngithub.com/ShishirPatil/gorilla.\n", "link": "http://arxiv.org/abs/2403.10131v2", "date": "2024-06-05", "relevancy": 1.9168, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4913}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4741}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAFT%3A%20Adapting%20Language%20Model%20to%20Domain%20Specific%20RAG&body=Title%3A%20RAFT%3A%20Adapting%20Language%20Model%20to%20Domain%20Specific%20RAG%0AAuthor%3A%20Tianjun%20Zhang%20and%20Shishir%20G.%20Patil%20and%20Naman%20Jain%20and%20Sheng%20Shen%20and%20Matei%20Zaharia%20and%20Ion%20Stoica%20and%20Joseph%20E.%20Gonzalez%0AAbstract%3A%20%20%20Pretraining%20Large%20Language%20Models%20%28LLMs%29%20on%20large%20corpora%20of%20textual%20data%20is%0Anow%20a%20standard%20paradigm.%20When%20using%20these%20LLMs%20for%20many%20downstream%0Aapplications%2C%20it%20is%20common%20to%20additionally%20bake%20in%20new%20knowledge%20%28e.g.%2C%0Atime-critical%20news%2C%20or%20private%20domain%20knowledge%29%20into%20the%20pretrained%20model%0Aeither%20through%20RAG-based-prompting%2C%20or%20fine-tuning.%20However%2C%20the%20optimal%0Amethodology%20for%20the%20model%20to%20gain%20such%20new%20knowledge%20remains%20an%20open%20question.%0AIn%20this%20paper%2C%20we%20present%20Retrieval%20Augmented%20FineTuning%20%28RAFT%29%2C%20a%20training%0Arecipe%20that%20improves%20the%20model%27s%20ability%20to%20answer%20questions%20in%20a%20%22open-book%22%0Ain-domain%20settings.%20In%20RAFT%2C%20given%20a%20question%2C%20and%20a%20set%20of%20retrieved%0Adocuments%2C%20we%20train%20the%20model%20to%20ignore%20those%20documents%20that%20don%27t%20help%20in%0Aanswering%20the%20question%2C%20which%20we%20call%2C%20distractor%20documents.%20RAFT%20accomplishes%0Athis%20by%20citing%20verbatim%20the%20right%20sequence%20from%20the%20relevant%20document%20that%0Awould%20help%20answer%20the%20question.%20This%20coupled%20with%20RAFT%27s%20chain-of-thought-style%0Aresponse%20helps%20improve%20the%20model%27s%20ability%20to%20reason.%20In%20domain-specific%20RAG%2C%0ARAFT%20consistently%20improves%20the%20model%27s%20performance%20across%20PubMed%2C%20HotpotQA%2C%20and%0AGorilla%20datasets%2C%20presenting%20a%20post-training%20recipe%20to%20improve%20pre-trained%20LLMs%0Ato%20in-domain%20RAG.%20RAFT%27s%20code%20and%20demo%20are%20open-sourced%20at%0Agithub.com/ShishirPatil/gorilla.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAFT%253A%2520Adapting%2520Language%2520Model%2520to%2520Domain%2520Specific%2520RAG%26entry.906535625%3DTianjun%2520Zhang%2520and%2520Shishir%2520G.%2520Patil%2520and%2520Naman%2520Jain%2520and%2520Sheng%2520Shen%2520and%2520Matei%2520Zaharia%2520and%2520Ion%2520Stoica%2520and%2520Joseph%2520E.%2520Gonzalez%26entry.1292438233%3D%2520%2520Pretraining%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520large%2520corpora%2520of%2520textual%2520data%2520is%250Anow%2520a%2520standard%2520paradigm.%2520When%2520using%2520these%2520LLMs%2520for%2520many%2520downstream%250Aapplications%252C%2520it%2520is%2520common%2520to%2520additionally%2520bake%2520in%2520new%2520knowledge%2520%2528e.g.%252C%250Atime-critical%2520news%252C%2520or%2520private%2520domain%2520knowledge%2529%2520into%2520the%2520pretrained%2520model%250Aeither%2520through%2520RAG-based-prompting%252C%2520or%2520fine-tuning.%2520However%252C%2520the%2520optimal%250Amethodology%2520for%2520the%2520model%2520to%2520gain%2520such%2520new%2520knowledge%2520remains%2520an%2520open%2520question.%250AIn%2520this%2520paper%252C%2520we%2520present%2520Retrieval%2520Augmented%2520FineTuning%2520%2528RAFT%2529%252C%2520a%2520training%250Arecipe%2520that%2520improves%2520the%2520model%2527s%2520ability%2520to%2520answer%2520questions%2520in%2520a%2520%2522open-book%2522%250Ain-domain%2520settings.%2520In%2520RAFT%252C%2520given%2520a%2520question%252C%2520and%2520a%2520set%2520of%2520retrieved%250Adocuments%252C%2520we%2520train%2520the%2520model%2520to%2520ignore%2520those%2520documents%2520that%2520don%2527t%2520help%2520in%250Aanswering%2520the%2520question%252C%2520which%2520we%2520call%252C%2520distractor%2520documents.%2520RAFT%2520accomplishes%250Athis%2520by%2520citing%2520verbatim%2520the%2520right%2520sequence%2520from%2520the%2520relevant%2520document%2520that%250Awould%2520help%2520answer%2520the%2520question.%2520This%2520coupled%2520with%2520RAFT%2527s%2520chain-of-thought-style%250Aresponse%2520helps%2520improve%2520the%2520model%2527s%2520ability%2520to%2520reason.%2520In%2520domain-specific%2520RAG%252C%250ARAFT%2520consistently%2520improves%2520the%2520model%2527s%2520performance%2520across%2520PubMed%252C%2520HotpotQA%252C%2520and%250AGorilla%2520datasets%252C%2520presenting%2520a%2520post-training%2520recipe%2520to%2520improve%2520pre-trained%2520LLMs%250Ato%2520in-domain%2520RAG.%2520RAFT%2527s%2520code%2520and%2520demo%2520are%2520open-sourced%2520at%250Agithub.com/ShishirPatil/gorilla.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAFT%3A%20Adapting%20Language%20Model%20to%20Domain%20Specific%20RAG&entry.906535625=Tianjun%20Zhang%20and%20Shishir%20G.%20Patil%20and%20Naman%20Jain%20and%20Sheng%20Shen%20and%20Matei%20Zaharia%20and%20Ion%20Stoica%20and%20Joseph%20E.%20Gonzalez&entry.1292438233=%20%20Pretraining%20Large%20Language%20Models%20%28LLMs%29%20on%20large%20corpora%20of%20textual%20data%20is%0Anow%20a%20standard%20paradigm.%20When%20using%20these%20LLMs%20for%20many%20downstream%0Aapplications%2C%20it%20is%20common%20to%20additionally%20bake%20in%20new%20knowledge%20%28e.g.%2C%0Atime-critical%20news%2C%20or%20private%20domain%20knowledge%29%20into%20the%20pretrained%20model%0Aeither%20through%20RAG-based-prompting%2C%20or%20fine-tuning.%20However%2C%20the%20optimal%0Amethodology%20for%20the%20model%20to%20gain%20such%20new%20knowledge%20remains%20an%20open%20question.%0AIn%20this%20paper%2C%20we%20present%20Retrieval%20Augmented%20FineTuning%20%28RAFT%29%2C%20a%20training%0Arecipe%20that%20improves%20the%20model%27s%20ability%20to%20answer%20questions%20in%20a%20%22open-book%22%0Ain-domain%20settings.%20In%20RAFT%2C%20given%20a%20question%2C%20and%20a%20set%20of%20retrieved%0Adocuments%2C%20we%20train%20the%20model%20to%20ignore%20those%20documents%20that%20don%27t%20help%20in%0Aanswering%20the%20question%2C%20which%20we%20call%2C%20distractor%20documents.%20RAFT%20accomplishes%0Athis%20by%20citing%20verbatim%20the%20right%20sequence%20from%20the%20relevant%20document%20that%0Awould%20help%20answer%20the%20question.%20This%20coupled%20with%20RAFT%27s%20chain-of-thought-style%0Aresponse%20helps%20improve%20the%20model%27s%20ability%20to%20reason.%20In%20domain-specific%20RAG%2C%0ARAFT%20consistently%20improves%20the%20model%27s%20performance%20across%20PubMed%2C%20HotpotQA%2C%20and%0AGorilla%20datasets%2C%20presenting%20a%20post-training%20recipe%20to%20improve%20pre-trained%20LLMs%0Ato%20in-domain%20RAG.%20RAFT%27s%20code%20and%20demo%20are%20open-sourced%20at%0Agithub.com/ShishirPatil/gorilla.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10131v2&entry.124074799=Read"},
{"title": "Active Preference Optimization for Sample Efficient RLHF", "author": "Nirjhar Das and Souradip Chakraborty and Aldo Pacchiano and Sayak Ray Chowdhury", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning\nLarge Language Models (LLMs) with human preferences. Although aligned\ngenerative models have shown remarkable abilities in various tasks, their\nreliance on high-quality human preference data creates a costly bottleneck in\nthe practical application of RLHF. One primary reason is that current methods\nrely on uniformly picking prompt-generation pairs from a dataset of\nprompt-generations, to collect human feedback, resulting in sub-optimal\nalignment under a constrained budget, which highlights the criticality of\nadaptive strategies in efficient alignment. Recent works [Mehta et al., 2023,\nMuldrew et al., 2024] have tried to address this problem by designing various\nheuristics based on generation uncertainty. However, either the assumptions in\n[Mehta et al., 2023] are restrictive, or [Muldrew et al., 2024] do not provide\nany rigorous theoretical guarantee. To address these, we reformulate RLHF\nwithin contextual preference bandit framework, treating prompts as contexts,\nand develop an active-learning algorithm, $\\textit{Active Preference\nOptimization}$ ($\\texttt{APO}$), which enhances model alignment by querying\npreference data from the most important samples, achieving superior performance\nfor small sample budget. We analyze the theoretical performance guarantees of\n$\\texttt{APO}$ under the BTL preference model showing that the suboptimality\ngap of the policy learned via $\\texttt{APO}$ scales as $O(1/\\sqrt{T})$ for a\nbudget of $T$. We also show that collecting preference data by choosing prompts\nrandomly leads to a policy that suffers a constant sub-optimality. We perform\ndetailed experimental evaluations on practical preference datasets to validate\n$\\texttt{APO}$'s efficacy over the existing methods, establishing it as a\nsample-efficient and practical solution of alignment in a cost-effective and\nscalable manner.\n", "link": "http://arxiv.org/abs/2402.10500v2", "date": "2024-06-05", "relevancy": 1.8633, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4724}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Preference%20Optimization%20for%20Sample%20Efficient%20RLHF&body=Title%3A%20Active%20Preference%20Optimization%20for%20Sample%20Efficient%20RLHF%0AAuthor%3A%20Nirjhar%20Das%20and%20Souradip%20Chakraborty%20and%20Aldo%20Pacchiano%20and%20Sayak%20Ray%20Chowdhury%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20pivotal%20in%20aligning%0ALarge%20Language%20Models%20%28LLMs%29%20with%20human%20preferences.%20Although%20aligned%0Agenerative%20models%20have%20shown%20remarkable%20abilities%20in%20various%20tasks%2C%20their%0Areliance%20on%20high-quality%20human%20preference%20data%20creates%20a%20costly%20bottleneck%20in%0Athe%20practical%20application%20of%20RLHF.%20One%20primary%20reason%20is%20that%20current%20methods%0Arely%20on%20uniformly%20picking%20prompt-generation%20pairs%20from%20a%20dataset%20of%0Aprompt-generations%2C%20to%20collect%20human%20feedback%2C%20resulting%20in%20sub-optimal%0Aalignment%20under%20a%20constrained%20budget%2C%20which%20highlights%20the%20criticality%20of%0Aadaptive%20strategies%20in%20efficient%20alignment.%20Recent%20works%20%5BMehta%20et%20al.%2C%202023%2C%0AMuldrew%20et%20al.%2C%202024%5D%20have%20tried%20to%20address%20this%20problem%20by%20designing%20various%0Aheuristics%20based%20on%20generation%20uncertainty.%20However%2C%20either%20the%20assumptions%20in%0A%5BMehta%20et%20al.%2C%202023%5D%20are%20restrictive%2C%20or%20%5BMuldrew%20et%20al.%2C%202024%5D%20do%20not%20provide%0Aany%20rigorous%20theoretical%20guarantee.%20To%20address%20these%2C%20we%20reformulate%20RLHF%0Awithin%20contextual%20preference%20bandit%20framework%2C%20treating%20prompts%20as%20contexts%2C%0Aand%20develop%20an%20active-learning%20algorithm%2C%20%24%5Ctextit%7BActive%20Preference%0AOptimization%7D%24%20%28%24%5Ctexttt%7BAPO%7D%24%29%2C%20which%20enhances%20model%20alignment%20by%20querying%0Apreference%20data%20from%20the%20most%20important%20samples%2C%20achieving%20superior%20performance%0Afor%20small%20sample%20budget.%20We%20analyze%20the%20theoretical%20performance%20guarantees%20of%0A%24%5Ctexttt%7BAPO%7D%24%20under%20the%20BTL%20preference%20model%20showing%20that%20the%20suboptimality%0Agap%20of%20the%20policy%20learned%20via%20%24%5Ctexttt%7BAPO%7D%24%20scales%20as%20%24O%281/%5Csqrt%7BT%7D%29%24%20for%20a%0Abudget%20of%20%24T%24.%20We%20also%20show%20that%20collecting%20preference%20data%20by%20choosing%20prompts%0Arandomly%20leads%20to%20a%20policy%20that%20suffers%20a%20constant%20sub-optimality.%20We%20perform%0Adetailed%20experimental%20evaluations%20on%20practical%20preference%20datasets%20to%20validate%0A%24%5Ctexttt%7BAPO%7D%24%27s%20efficacy%20over%20the%20existing%20methods%2C%20establishing%20it%20as%20a%0Asample-efficient%20and%20practical%20solution%20of%20alignment%20in%20a%20cost-effective%20and%0Ascalable%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Preference%2520Optimization%2520for%2520Sample%2520Efficient%2520RLHF%26entry.906535625%3DNirjhar%2520Das%2520and%2520Souradip%2520Chakraborty%2520and%2520Aldo%2520Pacchiano%2520and%2520Sayak%2520Ray%2520Chowdhury%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520is%2520pivotal%2520in%2520aligning%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520human%2520preferences.%2520Although%2520aligned%250Agenerative%2520models%2520have%2520shown%2520remarkable%2520abilities%2520in%2520various%2520tasks%252C%2520their%250Areliance%2520on%2520high-quality%2520human%2520preference%2520data%2520creates%2520a%2520costly%2520bottleneck%2520in%250Athe%2520practical%2520application%2520of%2520RLHF.%2520One%2520primary%2520reason%2520is%2520that%2520current%2520methods%250Arely%2520on%2520uniformly%2520picking%2520prompt-generation%2520pairs%2520from%2520a%2520dataset%2520of%250Aprompt-generations%252C%2520to%2520collect%2520human%2520feedback%252C%2520resulting%2520in%2520sub-optimal%250Aalignment%2520under%2520a%2520constrained%2520budget%252C%2520which%2520highlights%2520the%2520criticality%2520of%250Aadaptive%2520strategies%2520in%2520efficient%2520alignment.%2520Recent%2520works%2520%255BMehta%2520et%2520al.%252C%25202023%252C%250AMuldrew%2520et%2520al.%252C%25202024%255D%2520have%2520tried%2520to%2520address%2520this%2520problem%2520by%2520designing%2520various%250Aheuristics%2520based%2520on%2520generation%2520uncertainty.%2520However%252C%2520either%2520the%2520assumptions%2520in%250A%255BMehta%2520et%2520al.%252C%25202023%255D%2520are%2520restrictive%252C%2520or%2520%255BMuldrew%2520et%2520al.%252C%25202024%255D%2520do%2520not%2520provide%250Aany%2520rigorous%2520theoretical%2520guarantee.%2520To%2520address%2520these%252C%2520we%2520reformulate%2520RLHF%250Awithin%2520contextual%2520preference%2520bandit%2520framework%252C%2520treating%2520prompts%2520as%2520contexts%252C%250Aand%2520develop%2520an%2520active-learning%2520algorithm%252C%2520%2524%255Ctextit%257BActive%2520Preference%250AOptimization%257D%2524%2520%2528%2524%255Ctexttt%257BAPO%257D%2524%2529%252C%2520which%2520enhances%2520model%2520alignment%2520by%2520querying%250Apreference%2520data%2520from%2520the%2520most%2520important%2520samples%252C%2520achieving%2520superior%2520performance%250Afor%2520small%2520sample%2520budget.%2520We%2520analyze%2520the%2520theoretical%2520performance%2520guarantees%2520of%250A%2524%255Ctexttt%257BAPO%257D%2524%2520under%2520the%2520BTL%2520preference%2520model%2520showing%2520that%2520the%2520suboptimality%250Agap%2520of%2520the%2520policy%2520learned%2520via%2520%2524%255Ctexttt%257BAPO%257D%2524%2520scales%2520as%2520%2524O%25281/%255Csqrt%257BT%257D%2529%2524%2520for%2520a%250Abudget%2520of%2520%2524T%2524.%2520We%2520also%2520show%2520that%2520collecting%2520preference%2520data%2520by%2520choosing%2520prompts%250Arandomly%2520leads%2520to%2520a%2520policy%2520that%2520suffers%2520a%2520constant%2520sub-optimality.%2520We%2520perform%250Adetailed%2520experimental%2520evaluations%2520on%2520practical%2520preference%2520datasets%2520to%2520validate%250A%2524%255Ctexttt%257BAPO%257D%2524%2527s%2520efficacy%2520over%2520the%2520existing%2520methods%252C%2520establishing%2520it%2520as%2520a%250Asample-efficient%2520and%2520practical%2520solution%2520of%2520alignment%2520in%2520a%2520cost-effective%2520and%250Ascalable%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Preference%20Optimization%20for%20Sample%20Efficient%20RLHF&entry.906535625=Nirjhar%20Das%20and%20Souradip%20Chakraborty%20and%20Aldo%20Pacchiano%20and%20Sayak%20Ray%20Chowdhury&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20pivotal%20in%20aligning%0ALarge%20Language%20Models%20%28LLMs%29%20with%20human%20preferences.%20Although%20aligned%0Agenerative%20models%20have%20shown%20remarkable%20abilities%20in%20various%20tasks%2C%20their%0Areliance%20on%20high-quality%20human%20preference%20data%20creates%20a%20costly%20bottleneck%20in%0Athe%20practical%20application%20of%20RLHF.%20One%20primary%20reason%20is%20that%20current%20methods%0Arely%20on%20uniformly%20picking%20prompt-generation%20pairs%20from%20a%20dataset%20of%0Aprompt-generations%2C%20to%20collect%20human%20feedback%2C%20resulting%20in%20sub-optimal%0Aalignment%20under%20a%20constrained%20budget%2C%20which%20highlights%20the%20criticality%20of%0Aadaptive%20strategies%20in%20efficient%20alignment.%20Recent%20works%20%5BMehta%20et%20al.%2C%202023%2C%0AMuldrew%20et%20al.%2C%202024%5D%20have%20tried%20to%20address%20this%20problem%20by%20designing%20various%0Aheuristics%20based%20on%20generation%20uncertainty.%20However%2C%20either%20the%20assumptions%20in%0A%5BMehta%20et%20al.%2C%202023%5D%20are%20restrictive%2C%20or%20%5BMuldrew%20et%20al.%2C%202024%5D%20do%20not%20provide%0Aany%20rigorous%20theoretical%20guarantee.%20To%20address%20these%2C%20we%20reformulate%20RLHF%0Awithin%20contextual%20preference%20bandit%20framework%2C%20treating%20prompts%20as%20contexts%2C%0Aand%20develop%20an%20active-learning%20algorithm%2C%20%24%5Ctextit%7BActive%20Preference%0AOptimization%7D%24%20%28%24%5Ctexttt%7BAPO%7D%24%29%2C%20which%20enhances%20model%20alignment%20by%20querying%0Apreference%20data%20from%20the%20most%20important%20samples%2C%20achieving%20superior%20performance%0Afor%20small%20sample%20budget.%20We%20analyze%20the%20theoretical%20performance%20guarantees%20of%0A%24%5Ctexttt%7BAPO%7D%24%20under%20the%20BTL%20preference%20model%20showing%20that%20the%20suboptimality%0Agap%20of%20the%20policy%20learned%20via%20%24%5Ctexttt%7BAPO%7D%24%20scales%20as%20%24O%281/%5Csqrt%7BT%7D%29%24%20for%20a%0Abudget%20of%20%24T%24.%20We%20also%20show%20that%20collecting%20preference%20data%20by%20choosing%20prompts%0Arandomly%20leads%20to%20a%20policy%20that%20suffers%20a%20constant%20sub-optimality.%20We%20perform%0Adetailed%20experimental%20evaluations%20on%20practical%20preference%20datasets%20to%20validate%0A%24%5Ctexttt%7BAPO%7D%24%27s%20efficacy%20over%20the%20existing%20methods%2C%20establishing%20it%20as%20a%0Asample-efficient%20and%20practical%20solution%20of%20alignment%20in%20a%20cost-effective%20and%0Ascalable%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10500v2&entry.124074799=Read"},
{"title": "CLMASP: Coupling Large Language Models with Answer Set Programming for\n  Robotic Task Planning", "author": "Xinrui Lin and Yangfan Wu and Huanyu Yang and Yu Zhang and Yanyong Zhang and Jianmin Ji", "abstract": "  Large Language Models (LLMs) possess extensive foundational knowledge and\nmoderate reasoning abilities, making them suitable for general task planning in\nopen-world scenarios. However, it is challenging to ground a LLM-generated plan\nto be executable for the specified robot with certain restrictions. This paper\nintroduces CLMASP, an approach that couples LLMs with Answer Set Programming\n(ASP) to overcome the limitations, where ASP is a non-monotonic logic\nprogramming formalism renowned for its capacity to represent and reason about a\nrobot's action knowledge. CLMASP initiates with a LLM generating a basic\nskeleton plan, which is subsequently tailored to the specific scenario using a\nvector database. This plan is then refined by an ASP program with a robot's\naction knowledge, which integrates implementation details into the skeleton,\ngrounding the LLM's abstract outputs in practical robot contexts. Our\nexperiments conducted on the VirtualHome platform demonstrate CLMASP's\nefficacy. Compared to the baseline executable rate of under 2% with LLM\napproaches, CLMASP significantly improves this to over 90%.\n", "link": "http://arxiv.org/abs/2406.03367v1", "date": "2024-06-05", "relevancy": 1.5827, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6076}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5477}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLMASP%3A%20Coupling%20Large%20Language%20Models%20with%20Answer%20Set%20Programming%20for%0A%20%20Robotic%20Task%20Planning&body=Title%3A%20CLMASP%3A%20Coupling%20Large%20Language%20Models%20with%20Answer%20Set%20Programming%20for%0A%20%20Robotic%20Task%20Planning%0AAuthor%3A%20Xinrui%20Lin%20and%20Yangfan%20Wu%20and%20Huanyu%20Yang%20and%20Yu%20Zhang%20and%20Yanyong%20Zhang%20and%20Jianmin%20Ji%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20possess%20extensive%20foundational%20knowledge%20and%0Amoderate%20reasoning%20abilities%2C%20making%20them%20suitable%20for%20general%20task%20planning%20in%0Aopen-world%20scenarios.%20However%2C%20it%20is%20challenging%20to%20ground%20a%20LLM-generated%20plan%0Ato%20be%20executable%20for%20the%20specified%20robot%20with%20certain%20restrictions.%20This%20paper%0Aintroduces%20CLMASP%2C%20an%20approach%20that%20couples%20LLMs%20with%20Answer%20Set%20Programming%0A%28ASP%29%20to%20overcome%20the%20limitations%2C%20where%20ASP%20is%20a%20non-monotonic%20logic%0Aprogramming%20formalism%20renowned%20for%20its%20capacity%20to%20represent%20and%20reason%20about%20a%0Arobot%27s%20action%20knowledge.%20CLMASP%20initiates%20with%20a%20LLM%20generating%20a%20basic%0Askeleton%20plan%2C%20which%20is%20subsequently%20tailored%20to%20the%20specific%20scenario%20using%20a%0Avector%20database.%20This%20plan%20is%20then%20refined%20by%20an%20ASP%20program%20with%20a%20robot%27s%0Aaction%20knowledge%2C%20which%20integrates%20implementation%20details%20into%20the%20skeleton%2C%0Agrounding%20the%20LLM%27s%20abstract%20outputs%20in%20practical%20robot%20contexts.%20Our%0Aexperiments%20conducted%20on%20the%20VirtualHome%20platform%20demonstrate%20CLMASP%27s%0Aefficacy.%20Compared%20to%20the%20baseline%20executable%20rate%20of%20under%202%25%20with%20LLM%0Aapproaches%2C%20CLMASP%20significantly%20improves%20this%20to%20over%2090%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLMASP%253A%2520Coupling%2520Large%2520Language%2520Models%2520with%2520Answer%2520Set%2520Programming%2520for%250A%2520%2520Robotic%2520Task%2520Planning%26entry.906535625%3DXinrui%2520Lin%2520and%2520Yangfan%2520Wu%2520and%2520Huanyu%2520Yang%2520and%2520Yu%2520Zhang%2520and%2520Yanyong%2520Zhang%2520and%2520Jianmin%2520Ji%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520possess%2520extensive%2520foundational%2520knowledge%2520and%250Amoderate%2520reasoning%2520abilities%252C%2520making%2520them%2520suitable%2520for%2520general%2520task%2520planning%2520in%250Aopen-world%2520scenarios.%2520However%252C%2520it%2520is%2520challenging%2520to%2520ground%2520a%2520LLM-generated%2520plan%250Ato%2520be%2520executable%2520for%2520the%2520specified%2520robot%2520with%2520certain%2520restrictions.%2520This%2520paper%250Aintroduces%2520CLMASP%252C%2520an%2520approach%2520that%2520couples%2520LLMs%2520with%2520Answer%2520Set%2520Programming%250A%2528ASP%2529%2520to%2520overcome%2520the%2520limitations%252C%2520where%2520ASP%2520is%2520a%2520non-monotonic%2520logic%250Aprogramming%2520formalism%2520renowned%2520for%2520its%2520capacity%2520to%2520represent%2520and%2520reason%2520about%2520a%250Arobot%2527s%2520action%2520knowledge.%2520CLMASP%2520initiates%2520with%2520a%2520LLM%2520generating%2520a%2520basic%250Askeleton%2520plan%252C%2520which%2520is%2520subsequently%2520tailored%2520to%2520the%2520specific%2520scenario%2520using%2520a%250Avector%2520database.%2520This%2520plan%2520is%2520then%2520refined%2520by%2520an%2520ASP%2520program%2520with%2520a%2520robot%2527s%250Aaction%2520knowledge%252C%2520which%2520integrates%2520implementation%2520details%2520into%2520the%2520skeleton%252C%250Agrounding%2520the%2520LLM%2527s%2520abstract%2520outputs%2520in%2520practical%2520robot%2520contexts.%2520Our%250Aexperiments%2520conducted%2520on%2520the%2520VirtualHome%2520platform%2520demonstrate%2520CLMASP%2527s%250Aefficacy.%2520Compared%2520to%2520the%2520baseline%2520executable%2520rate%2520of%2520under%25202%2525%2520with%2520LLM%250Aapproaches%252C%2520CLMASP%2520significantly%2520improves%2520this%2520to%2520over%252090%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLMASP%3A%20Coupling%20Large%20Language%20Models%20with%20Answer%20Set%20Programming%20for%0A%20%20Robotic%20Task%20Planning&entry.906535625=Xinrui%20Lin%20and%20Yangfan%20Wu%20and%20Huanyu%20Yang%20and%20Yu%20Zhang%20and%20Yanyong%20Zhang%20and%20Jianmin%20Ji&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20possess%20extensive%20foundational%20knowledge%20and%0Amoderate%20reasoning%20abilities%2C%20making%20them%20suitable%20for%20general%20task%20planning%20in%0Aopen-world%20scenarios.%20However%2C%20it%20is%20challenging%20to%20ground%20a%20LLM-generated%20plan%0Ato%20be%20executable%20for%20the%20specified%20robot%20with%20certain%20restrictions.%20This%20paper%0Aintroduces%20CLMASP%2C%20an%20approach%20that%20couples%20LLMs%20with%20Answer%20Set%20Programming%0A%28ASP%29%20to%20overcome%20the%20limitations%2C%20where%20ASP%20is%20a%20non-monotonic%20logic%0Aprogramming%20formalism%20renowned%20for%20its%20capacity%20to%20represent%20and%20reason%20about%20a%0Arobot%27s%20action%20knowledge.%20CLMASP%20initiates%20with%20a%20LLM%20generating%20a%20basic%0Askeleton%20plan%2C%20which%20is%20subsequently%20tailored%20to%20the%20specific%20scenario%20using%20a%0Avector%20database.%20This%20plan%20is%20then%20refined%20by%20an%20ASP%20program%20with%20a%20robot%27s%0Aaction%20knowledge%2C%20which%20integrates%20implementation%20details%20into%20the%20skeleton%2C%0Agrounding%20the%20LLM%27s%20abstract%20outputs%20in%20practical%20robot%20contexts.%20Our%0Aexperiments%20conducted%20on%20the%20VirtualHome%20platform%20demonstrate%20CLMASP%27s%0Aefficacy.%20Compared%20to%20the%20baseline%20executable%20rate%20of%20under%202%25%20with%20LLM%0Aapproaches%2C%20CLMASP%20significantly%20improves%20this%20to%20over%2090%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03367v1&entry.124074799=Read"},
{"title": "Can Implicit Bias Imply Adversarial Robustness?", "author": "Hancheng Min and Ren\u00e9 Vidal", "abstract": "  The implicit bias of gradient-based training algorithms has been considered\nmostly beneficial as it leads to trained networks that often generalize well.\nHowever, Frei et al. (2023) show that such implicit bias can harm adversarial\nrobustness. Specifically, they show that if the data consists of clusters with\nsmall inter-cluster correlation, a shallow (two-layer) ReLU network trained by\ngradient flow generalizes well, but it is not robust to adversarial attacks of\nsmall radius. Moreover, this phenomenon occurs despite the existence of a much\nmore robust classifier that can be explicitly constructed from a shallow\nnetwork. In this paper, we extend recent analyses of neuron alignment to show\nthat a shallow network with a polynomial ReLU activation (pReLU) trained by\ngradient flow not only generalizes well but is also robust to adversarial\nattacks. Our results highlight the importance of the interplay between data\nstructure and architecture design in the implicit bias and robustness of\ntrained networks.\n", "link": "http://arxiv.org/abs/2405.15942v2", "date": "2024-06-05", "relevancy": 1.9014, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5023}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4647}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Implicit%20Bias%20Imply%20Adversarial%20Robustness%3F&body=Title%3A%20Can%20Implicit%20Bias%20Imply%20Adversarial%20Robustness%3F%0AAuthor%3A%20Hancheng%20Min%20and%20Ren%C3%A9%20Vidal%0AAbstract%3A%20%20%20The%20implicit%20bias%20of%20gradient-based%20training%20algorithms%20has%20been%20considered%0Amostly%20beneficial%20as%20it%20leads%20to%20trained%20networks%20that%20often%20generalize%20well.%0AHowever%2C%20Frei%20et%20al.%20%282023%29%20show%20that%20such%20implicit%20bias%20can%20harm%20adversarial%0Arobustness.%20Specifically%2C%20they%20show%20that%20if%20the%20data%20consists%20of%20clusters%20with%0Asmall%20inter-cluster%20correlation%2C%20a%20shallow%20%28two-layer%29%20ReLU%20network%20trained%20by%0Agradient%20flow%20generalizes%20well%2C%20but%20it%20is%20not%20robust%20to%20adversarial%20attacks%20of%0Asmall%20radius.%20Moreover%2C%20this%20phenomenon%20occurs%20despite%20the%20existence%20of%20a%20much%0Amore%20robust%20classifier%20that%20can%20be%20explicitly%20constructed%20from%20a%20shallow%0Anetwork.%20In%20this%20paper%2C%20we%20extend%20recent%20analyses%20of%20neuron%20alignment%20to%20show%0Athat%20a%20shallow%20network%20with%20a%20polynomial%20ReLU%20activation%20%28pReLU%29%20trained%20by%0Agradient%20flow%20not%20only%20generalizes%20well%20but%20is%20also%20robust%20to%20adversarial%0Aattacks.%20Our%20results%20highlight%20the%20importance%20of%20the%20interplay%20between%20data%0Astructure%20and%20architecture%20design%20in%20the%20implicit%20bias%20and%20robustness%20of%0Atrained%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15942v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Implicit%2520Bias%2520Imply%2520Adversarial%2520Robustness%253F%26entry.906535625%3DHancheng%2520Min%2520and%2520Ren%25C3%25A9%2520Vidal%26entry.1292438233%3D%2520%2520The%2520implicit%2520bias%2520of%2520gradient-based%2520training%2520algorithms%2520has%2520been%2520considered%250Amostly%2520beneficial%2520as%2520it%2520leads%2520to%2520trained%2520networks%2520that%2520often%2520generalize%2520well.%250AHowever%252C%2520Frei%2520et%2520al.%2520%25282023%2529%2520show%2520that%2520such%2520implicit%2520bias%2520can%2520harm%2520adversarial%250Arobustness.%2520Specifically%252C%2520they%2520show%2520that%2520if%2520the%2520data%2520consists%2520of%2520clusters%2520with%250Asmall%2520inter-cluster%2520correlation%252C%2520a%2520shallow%2520%2528two-layer%2529%2520ReLU%2520network%2520trained%2520by%250Agradient%2520flow%2520generalizes%2520well%252C%2520but%2520it%2520is%2520not%2520robust%2520to%2520adversarial%2520attacks%2520of%250Asmall%2520radius.%2520Moreover%252C%2520this%2520phenomenon%2520occurs%2520despite%2520the%2520existence%2520of%2520a%2520much%250Amore%2520robust%2520classifier%2520that%2520can%2520be%2520explicitly%2520constructed%2520from%2520a%2520shallow%250Anetwork.%2520In%2520this%2520paper%252C%2520we%2520extend%2520recent%2520analyses%2520of%2520neuron%2520alignment%2520to%2520show%250Athat%2520a%2520shallow%2520network%2520with%2520a%2520polynomial%2520ReLU%2520activation%2520%2528pReLU%2529%2520trained%2520by%250Agradient%2520flow%2520not%2520only%2520generalizes%2520well%2520but%2520is%2520also%2520robust%2520to%2520adversarial%250Aattacks.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520the%2520interplay%2520between%2520data%250Astructure%2520and%2520architecture%2520design%2520in%2520the%2520implicit%2520bias%2520and%2520robustness%2520of%250Atrained%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15942v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Implicit%20Bias%20Imply%20Adversarial%20Robustness%3F&entry.906535625=Hancheng%20Min%20and%20Ren%C3%A9%20Vidal&entry.1292438233=%20%20The%20implicit%20bias%20of%20gradient-based%20training%20algorithms%20has%20been%20considered%0Amostly%20beneficial%20as%20it%20leads%20to%20trained%20networks%20that%20often%20generalize%20well.%0AHowever%2C%20Frei%20et%20al.%20%282023%29%20show%20that%20such%20implicit%20bias%20can%20harm%20adversarial%0Arobustness.%20Specifically%2C%20they%20show%20that%20if%20the%20data%20consists%20of%20clusters%20with%0Asmall%20inter-cluster%20correlation%2C%20a%20shallow%20%28two-layer%29%20ReLU%20network%20trained%20by%0Agradient%20flow%20generalizes%20well%2C%20but%20it%20is%20not%20robust%20to%20adversarial%20attacks%20of%0Asmall%20radius.%20Moreover%2C%20this%20phenomenon%20occurs%20despite%20the%20existence%20of%20a%20much%0Amore%20robust%20classifier%20that%20can%20be%20explicitly%20constructed%20from%20a%20shallow%0Anetwork.%20In%20this%20paper%2C%20we%20extend%20recent%20analyses%20of%20neuron%20alignment%20to%20show%0Athat%20a%20shallow%20network%20with%20a%20polynomial%20ReLU%20activation%20%28pReLU%29%20trained%20by%0Agradient%20flow%20not%20only%20generalizes%20well%20but%20is%20also%20robust%20to%20adversarial%0Aattacks.%20Our%20results%20highlight%20the%20importance%20of%20the%20interplay%20between%20data%0Astructure%20and%20architecture%20design%20in%20the%20implicit%20bias%20and%20robustness%20of%0Atrained%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15942v2&entry.124074799=Read"},
{"title": "Trust Regions for Explanations via Black-Box Probabilistic Certification", "author": "Amit Dhurandhar and Swagatam Haldar and Dennis Wei and Karthikeyan Natesan Ramamurthy", "abstract": "  Given the black box nature of machine learning models, a plethora of\nexplainability methods have been developed to decipher the factors behind\nindividual decisions. In this paper, we introduce a novel problem of black box\n(probabilistic) explanation certification. We ask the question: Given a black\nbox model with only query access, an explanation for an example and a quality\nmetric (viz. fidelity, stability), can we find the largest hypercube (i.e.,\n$\\ell_{\\infty}$ ball) centered at the example such that when the explanation is\napplied to all examples within the hypercube, (with high probability) a quality\ncriterion is met (viz. fidelity greater than some value)? Being able to\nefficiently find such a \\emph{trust region} has multiple benefits: i) insight\ninto model behavior in a \\emph{region}, with a \\emph{guarantee}; ii)\nascertained \\emph{stability} of the explanation; iii) \\emph{explanation reuse},\nwhich can save time, energy and money by not having to find explanations for\nevery example; and iv) a possible \\emph{meta-metric} to compare explanation\nmethods. Our contributions include formalizing this problem, proposing\nsolutions, providing theoretical guarantees for these solutions that are\ncomputable, and experimentally showing their efficacy on synthetic and real\ndata.\n", "link": "http://arxiv.org/abs/2402.11168v3", "date": "2024-06-05", "relevancy": 1.4629, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4961}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trust%20Regions%20for%20Explanations%20via%20Black-Box%20Probabilistic%20Certification&body=Title%3A%20Trust%20Regions%20for%20Explanations%20via%20Black-Box%20Probabilistic%20Certification%0AAuthor%3A%20Amit%20Dhurandhar%20and%20Swagatam%20Haldar%20and%20Dennis%20Wei%20and%20Karthikeyan%20Natesan%20Ramamurthy%0AAbstract%3A%20%20%20Given%20the%20black%20box%20nature%20of%20machine%20learning%20models%2C%20a%20plethora%20of%0Aexplainability%20methods%20have%20been%20developed%20to%20decipher%20the%20factors%20behind%0Aindividual%20decisions.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20problem%20of%20black%20box%0A%28probabilistic%29%20explanation%20certification.%20We%20ask%20the%20question%3A%20Given%20a%20black%0Abox%20model%20with%20only%20query%20access%2C%20an%20explanation%20for%20an%20example%20and%20a%20quality%0Ametric%20%28viz.%20fidelity%2C%20stability%29%2C%20can%20we%20find%20the%20largest%20hypercube%20%28i.e.%2C%0A%24%5Cell_%7B%5Cinfty%7D%24%20ball%29%20centered%20at%20the%20example%20such%20that%20when%20the%20explanation%20is%0Aapplied%20to%20all%20examples%20within%20the%20hypercube%2C%20%28with%20high%20probability%29%20a%20quality%0Acriterion%20is%20met%20%28viz.%20fidelity%20greater%20than%20some%20value%29%3F%20Being%20able%20to%0Aefficiently%20find%20such%20a%20%5Cemph%7Btrust%20region%7D%20has%20multiple%20benefits%3A%20i%29%20insight%0Ainto%20model%20behavior%20in%20a%20%5Cemph%7Bregion%7D%2C%20with%20a%20%5Cemph%7Bguarantee%7D%3B%20ii%29%0Aascertained%20%5Cemph%7Bstability%7D%20of%20the%20explanation%3B%20iii%29%20%5Cemph%7Bexplanation%20reuse%7D%2C%0Awhich%20can%20save%20time%2C%20energy%20and%20money%20by%20not%20having%20to%20find%20explanations%20for%0Aevery%20example%3B%20and%20iv%29%20a%20possible%20%5Cemph%7Bmeta-metric%7D%20to%20compare%20explanation%0Amethods.%20Our%20contributions%20include%20formalizing%20this%20problem%2C%20proposing%0Asolutions%2C%20providing%20theoretical%20guarantees%20for%20these%20solutions%20that%20are%0Acomputable%2C%20and%20experimentally%20showing%20their%20efficacy%20on%20synthetic%20and%20real%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11168v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrust%2520Regions%2520for%2520Explanations%2520via%2520Black-Box%2520Probabilistic%2520Certification%26entry.906535625%3DAmit%2520Dhurandhar%2520and%2520Swagatam%2520Haldar%2520and%2520Dennis%2520Wei%2520and%2520Karthikeyan%2520Natesan%2520Ramamurthy%26entry.1292438233%3D%2520%2520Given%2520the%2520black%2520box%2520nature%2520of%2520machine%2520learning%2520models%252C%2520a%2520plethora%2520of%250Aexplainability%2520methods%2520have%2520been%2520developed%2520to%2520decipher%2520the%2520factors%2520behind%250Aindividual%2520decisions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520problem%2520of%2520black%2520box%250A%2528probabilistic%2529%2520explanation%2520certification.%2520We%2520ask%2520the%2520question%253A%2520Given%2520a%2520black%250Abox%2520model%2520with%2520only%2520query%2520access%252C%2520an%2520explanation%2520for%2520an%2520example%2520and%2520a%2520quality%250Ametric%2520%2528viz.%2520fidelity%252C%2520stability%2529%252C%2520can%2520we%2520find%2520the%2520largest%2520hypercube%2520%2528i.e.%252C%250A%2524%255Cell_%257B%255Cinfty%257D%2524%2520ball%2529%2520centered%2520at%2520the%2520example%2520such%2520that%2520when%2520the%2520explanation%2520is%250Aapplied%2520to%2520all%2520examples%2520within%2520the%2520hypercube%252C%2520%2528with%2520high%2520probability%2529%2520a%2520quality%250Acriterion%2520is%2520met%2520%2528viz.%2520fidelity%2520greater%2520than%2520some%2520value%2529%253F%2520Being%2520able%2520to%250Aefficiently%2520find%2520such%2520a%2520%255Cemph%257Btrust%2520region%257D%2520has%2520multiple%2520benefits%253A%2520i%2529%2520insight%250Ainto%2520model%2520behavior%2520in%2520a%2520%255Cemph%257Bregion%257D%252C%2520with%2520a%2520%255Cemph%257Bguarantee%257D%253B%2520ii%2529%250Aascertained%2520%255Cemph%257Bstability%257D%2520of%2520the%2520explanation%253B%2520iii%2529%2520%255Cemph%257Bexplanation%2520reuse%257D%252C%250Awhich%2520can%2520save%2520time%252C%2520energy%2520and%2520money%2520by%2520not%2520having%2520to%2520find%2520explanations%2520for%250Aevery%2520example%253B%2520and%2520iv%2529%2520a%2520possible%2520%255Cemph%257Bmeta-metric%257D%2520to%2520compare%2520explanation%250Amethods.%2520Our%2520contributions%2520include%2520formalizing%2520this%2520problem%252C%2520proposing%250Asolutions%252C%2520providing%2520theoretical%2520guarantees%2520for%2520these%2520solutions%2520that%2520are%250Acomputable%252C%2520and%2520experimentally%2520showing%2520their%2520efficacy%2520on%2520synthetic%2520and%2520real%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11168v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust%20Regions%20for%20Explanations%20via%20Black-Box%20Probabilistic%20Certification&entry.906535625=Amit%20Dhurandhar%20and%20Swagatam%20Haldar%20and%20Dennis%20Wei%20and%20Karthikeyan%20Natesan%20Ramamurthy&entry.1292438233=%20%20Given%20the%20black%20box%20nature%20of%20machine%20learning%20models%2C%20a%20plethora%20of%0Aexplainability%20methods%20have%20been%20developed%20to%20decipher%20the%20factors%20behind%0Aindividual%20decisions.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20problem%20of%20black%20box%0A%28probabilistic%29%20explanation%20certification.%20We%20ask%20the%20question%3A%20Given%20a%20black%0Abox%20model%20with%20only%20query%20access%2C%20an%20explanation%20for%20an%20example%20and%20a%20quality%0Ametric%20%28viz.%20fidelity%2C%20stability%29%2C%20can%20we%20find%20the%20largest%20hypercube%20%28i.e.%2C%0A%24%5Cell_%7B%5Cinfty%7D%24%20ball%29%20centered%20at%20the%20example%20such%20that%20when%20the%20explanation%20is%0Aapplied%20to%20all%20examples%20within%20the%20hypercube%2C%20%28with%20high%20probability%29%20a%20quality%0Acriterion%20is%20met%20%28viz.%20fidelity%20greater%20than%20some%20value%29%3F%20Being%20able%20to%0Aefficiently%20find%20such%20a%20%5Cemph%7Btrust%20region%7D%20has%20multiple%20benefits%3A%20i%29%20insight%0Ainto%20model%20behavior%20in%20a%20%5Cemph%7Bregion%7D%2C%20with%20a%20%5Cemph%7Bguarantee%7D%3B%20ii%29%0Aascertained%20%5Cemph%7Bstability%7D%20of%20the%20explanation%3B%20iii%29%20%5Cemph%7Bexplanation%20reuse%7D%2C%0Awhich%20can%20save%20time%2C%20energy%20and%20money%20by%20not%20having%20to%20find%20explanations%20for%0Aevery%20example%3B%20and%20iv%29%20a%20possible%20%5Cemph%7Bmeta-metric%7D%20to%20compare%20explanation%0Amethods.%20Our%20contributions%20include%20formalizing%20this%20problem%2C%20proposing%0Asolutions%2C%20providing%20theoretical%20guarantees%20for%20these%20solutions%20that%20are%0Acomputable%2C%20and%20experimentally%20showing%20their%20efficacy%20on%20synthetic%20and%20real%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11168v3&entry.124074799=Read"},
{"title": "Comparative Benchmarking of Failure Detection Methods in Medical Image\n  Segmentation: Unveiling the Role of Confidence Aggregation", "author": "Maximilian Zenk and David Zimmerer and Fabian Isensee and Jeremias Traub and Tobias Norajitra and Paul F. J\u00e4ger and Klaus Maier-Hein", "abstract": "  Semantic segmentation is an essential component of medical image analysis\nresearch, with recent deep learning algorithms offering out-of-the-box\napplicability across diverse datasets. Despite these advancements, segmentation\nfailures remain a significant concern for real-world clinical applications,\nnecessitating reliable detection mechanisms. This paper introduces a\ncomprehensive benchmarking framework aimed at evaluating failure detection\nmethodologies within medical image segmentation. Through our analysis, we\nidentify the strengths and limitations of current failure detection metrics,\nadvocating for the risk-coverage analysis as a holistic evaluation approach.\nUtilizing a collective dataset comprising five public 3D medical image\ncollections, we assess the efficacy of various failure detection strategies\nunder realistic test-time distribution shifts. Our findings highlight the\nimportance of pixel confidence aggregation and we observe superior performance\nof the pairwise Dice score (Roy et al., 2019) between ensemble predictions,\npositioning it as a simple and robust baseline for failure detection in medical\nimage segmentation. To promote ongoing research, we make the benchmarking\nframework available to the community.\n", "link": "http://arxiv.org/abs/2406.03323v1", "date": "2024-06-05", "relevancy": 1.5414, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5514}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5076}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Benchmarking%20of%20Failure%20Detection%20Methods%20in%20Medical%20Image%0A%20%20Segmentation%3A%20Unveiling%20the%20Role%20of%20Confidence%20Aggregation&body=Title%3A%20Comparative%20Benchmarking%20of%20Failure%20Detection%20Methods%20in%20Medical%20Image%0A%20%20Segmentation%3A%20Unveiling%20the%20Role%20of%20Confidence%20Aggregation%0AAuthor%3A%20Maximilian%20Zenk%20and%20David%20Zimmerer%20and%20Fabian%20Isensee%20and%20Jeremias%20Traub%20and%20Tobias%20Norajitra%20and%20Paul%20F.%20J%C3%A4ger%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20Semantic%20segmentation%20is%20an%20essential%20component%20of%20medical%20image%20analysis%0Aresearch%2C%20with%20recent%20deep%20learning%20algorithms%20offering%20out-of-the-box%0Aapplicability%20across%20diverse%20datasets.%20Despite%20these%20advancements%2C%20segmentation%0Afailures%20remain%20a%20significant%20concern%20for%20real-world%20clinical%20applications%2C%0Anecessitating%20reliable%20detection%20mechanisms.%20This%20paper%20introduces%20a%0Acomprehensive%20benchmarking%20framework%20aimed%20at%20evaluating%20failure%20detection%0Amethodologies%20within%20medical%20image%20segmentation.%20Through%20our%20analysis%2C%20we%0Aidentify%20the%20strengths%20and%20limitations%20of%20current%20failure%20detection%20metrics%2C%0Aadvocating%20for%20the%20risk-coverage%20analysis%20as%20a%20holistic%20evaluation%20approach.%0AUtilizing%20a%20collective%20dataset%20comprising%20five%20public%203D%20medical%20image%0Acollections%2C%20we%20assess%20the%20efficacy%20of%20various%20failure%20detection%20strategies%0Aunder%20realistic%20test-time%20distribution%20shifts.%20Our%20findings%20highlight%20the%0Aimportance%20of%20pixel%20confidence%20aggregation%20and%20we%20observe%20superior%20performance%0Aof%20the%20pairwise%20Dice%20score%20%28Roy%20et%20al.%2C%202019%29%20between%20ensemble%20predictions%2C%0Apositioning%20it%20as%20a%20simple%20and%20robust%20baseline%20for%20failure%20detection%20in%20medical%0Aimage%20segmentation.%20To%20promote%20ongoing%20research%2C%20we%20make%20the%20benchmarking%0Aframework%20available%20to%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Benchmarking%2520of%2520Failure%2520Detection%2520Methods%2520in%2520Medical%2520Image%250A%2520%2520Segmentation%253A%2520Unveiling%2520the%2520Role%2520of%2520Confidence%2520Aggregation%26entry.906535625%3DMaximilian%2520Zenk%2520and%2520David%2520Zimmerer%2520and%2520Fabian%2520Isensee%2520and%2520Jeremias%2520Traub%2520and%2520Tobias%2520Norajitra%2520and%2520Paul%2520F.%2520J%25C3%25A4ger%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520is%2520an%2520essential%2520component%2520of%2520medical%2520image%2520analysis%250Aresearch%252C%2520with%2520recent%2520deep%2520learning%2520algorithms%2520offering%2520out-of-the-box%250Aapplicability%2520across%2520diverse%2520datasets.%2520Despite%2520these%2520advancements%252C%2520segmentation%250Afailures%2520remain%2520a%2520significant%2520concern%2520for%2520real-world%2520clinical%2520applications%252C%250Anecessitating%2520reliable%2520detection%2520mechanisms.%2520This%2520paper%2520introduces%2520a%250Acomprehensive%2520benchmarking%2520framework%2520aimed%2520at%2520evaluating%2520failure%2520detection%250Amethodologies%2520within%2520medical%2520image%2520segmentation.%2520Through%2520our%2520analysis%252C%2520we%250Aidentify%2520the%2520strengths%2520and%2520limitations%2520of%2520current%2520failure%2520detection%2520metrics%252C%250Aadvocating%2520for%2520the%2520risk-coverage%2520analysis%2520as%2520a%2520holistic%2520evaluation%2520approach.%250AUtilizing%2520a%2520collective%2520dataset%2520comprising%2520five%2520public%25203D%2520medical%2520image%250Acollections%252C%2520we%2520assess%2520the%2520efficacy%2520of%2520various%2520failure%2520detection%2520strategies%250Aunder%2520realistic%2520test-time%2520distribution%2520shifts.%2520Our%2520findings%2520highlight%2520the%250Aimportance%2520of%2520pixel%2520confidence%2520aggregation%2520and%2520we%2520observe%2520superior%2520performance%250Aof%2520the%2520pairwise%2520Dice%2520score%2520%2528Roy%2520et%2520al.%252C%25202019%2529%2520between%2520ensemble%2520predictions%252C%250Apositioning%2520it%2520as%2520a%2520simple%2520and%2520robust%2520baseline%2520for%2520failure%2520detection%2520in%2520medical%250Aimage%2520segmentation.%2520To%2520promote%2520ongoing%2520research%252C%2520we%2520make%2520the%2520benchmarking%250Aframework%2520available%2520to%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Benchmarking%20of%20Failure%20Detection%20Methods%20in%20Medical%20Image%0A%20%20Segmentation%3A%20Unveiling%20the%20Role%20of%20Confidence%20Aggregation&entry.906535625=Maximilian%20Zenk%20and%20David%20Zimmerer%20and%20Fabian%20Isensee%20and%20Jeremias%20Traub%20and%20Tobias%20Norajitra%20and%20Paul%20F.%20J%C3%A4ger%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20Semantic%20segmentation%20is%20an%20essential%20component%20of%20medical%20image%20analysis%0Aresearch%2C%20with%20recent%20deep%20learning%20algorithms%20offering%20out-of-the-box%0Aapplicability%20across%20diverse%20datasets.%20Despite%20these%20advancements%2C%20segmentation%0Afailures%20remain%20a%20significant%20concern%20for%20real-world%20clinical%20applications%2C%0Anecessitating%20reliable%20detection%20mechanisms.%20This%20paper%20introduces%20a%0Acomprehensive%20benchmarking%20framework%20aimed%20at%20evaluating%20failure%20detection%0Amethodologies%20within%20medical%20image%20segmentation.%20Through%20our%20analysis%2C%20we%0Aidentify%20the%20strengths%20and%20limitations%20of%20current%20failure%20detection%20metrics%2C%0Aadvocating%20for%20the%20risk-coverage%20analysis%20as%20a%20holistic%20evaluation%20approach.%0AUtilizing%20a%20collective%20dataset%20comprising%20five%20public%203D%20medical%20image%0Acollections%2C%20we%20assess%20the%20efficacy%20of%20various%20failure%20detection%20strategies%0Aunder%20realistic%20test-time%20distribution%20shifts.%20Our%20findings%20highlight%20the%0Aimportance%20of%20pixel%20confidence%20aggregation%20and%20we%20observe%20superior%20performance%0Aof%20the%20pairwise%20Dice%20score%20%28Roy%20et%20al.%2C%202019%29%20between%20ensemble%20predictions%2C%0Apositioning%20it%20as%20a%20simple%20and%20robust%20baseline%20for%20failure%20detection%20in%20medical%0Aimage%20segmentation.%20To%20promote%20ongoing%20research%2C%20we%20make%20the%20benchmarking%0Aframework%20available%20to%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03323v1&entry.124074799=Read"},
{"title": "No-Regret Algorithms for Safe Bayesian Optimization with Monotonicity\n  Constraints", "author": "Arpan Losalka and Jonathan Scarlett", "abstract": "  We consider the problem of sequentially maximizing an unknown function $f$\nover a set of actions of the form $(s,\\mathbf{x})$, where the selected actions\nmust satisfy a safety constraint with respect to an unknown safety function\n$g$. We model $f$ and $g$ as lying in a reproducing kernel Hilbert space\n(RKHS), which facilitates the use of Gaussian process methods. While existing\nworks for this setting have provided algorithms that are guaranteed to identify\na near-optimal safe action, the problem of attaining low cumulative regret has\nremained largely unexplored, with a key challenge being that expanding the safe\nregion can incur high regret. To address this challenge, we show that if $g$ is\nmonotone with respect to just the single variable $s$ (with no such constraint\non $f$), sublinear regret becomes achievable with our proposed algorithm. In\naddition, we show that a modified version of our algorithm is able to attain\nsublinear regret (for suitably defined notions of regret) for the task of\nfinding a near-optimal $s$ corresponding to every $\\mathbf{x}$, as opposed to\nonly finding the global safe optimum. Our findings are supported with empirical\nevaluations on various objective and safety functions.\n", "link": "http://arxiv.org/abs/2406.03264v1", "date": "2024-06-05", "relevancy": 1.2987, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4756}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4282}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No-Regret%20Algorithms%20for%20Safe%20Bayesian%20Optimization%20with%20Monotonicity%0A%20%20Constraints&body=Title%3A%20No-Regret%20Algorithms%20for%20Safe%20Bayesian%20Optimization%20with%20Monotonicity%0A%20%20Constraints%0AAuthor%3A%20Arpan%20Losalka%20and%20Jonathan%20Scarlett%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20sequentially%20maximizing%20an%20unknown%20function%20%24f%24%0Aover%20a%20set%20of%20actions%20of%20the%20form%20%24%28s%2C%5Cmathbf%7Bx%7D%29%24%2C%20where%20the%20selected%20actions%0Amust%20satisfy%20a%20safety%20constraint%20with%20respect%20to%20an%20unknown%20safety%20function%0A%24g%24.%20We%20model%20%24f%24%20and%20%24g%24%20as%20lying%20in%20a%20reproducing%20kernel%20Hilbert%20space%0A%28RKHS%29%2C%20which%20facilitates%20the%20use%20of%20Gaussian%20process%20methods.%20While%20existing%0Aworks%20for%20this%20setting%20have%20provided%20algorithms%20that%20are%20guaranteed%20to%20identify%0Aa%20near-optimal%20safe%20action%2C%20the%20problem%20of%20attaining%20low%20cumulative%20regret%20has%0Aremained%20largely%20unexplored%2C%20with%20a%20key%20challenge%20being%20that%20expanding%20the%20safe%0Aregion%20can%20incur%20high%20regret.%20To%20address%20this%20challenge%2C%20we%20show%20that%20if%20%24g%24%20is%0Amonotone%20with%20respect%20to%20just%20the%20single%20variable%20%24s%24%20%28with%20no%20such%20constraint%0Aon%20%24f%24%29%2C%20sublinear%20regret%20becomes%20achievable%20with%20our%20proposed%20algorithm.%20In%0Aaddition%2C%20we%20show%20that%20a%20modified%20version%20of%20our%20algorithm%20is%20able%20to%20attain%0Asublinear%20regret%20%28for%20suitably%20defined%20notions%20of%20regret%29%20for%20the%20task%20of%0Afinding%20a%20near-optimal%20%24s%24%20corresponding%20to%20every%20%24%5Cmathbf%7Bx%7D%24%2C%20as%20opposed%20to%0Aonly%20finding%20the%20global%20safe%20optimum.%20Our%20findings%20are%20supported%20with%20empirical%0Aevaluations%20on%20various%20objective%20and%20safety%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo-Regret%2520Algorithms%2520for%2520Safe%2520Bayesian%2520Optimization%2520with%2520Monotonicity%250A%2520%2520Constraints%26entry.906535625%3DArpan%2520Losalka%2520and%2520Jonathan%2520Scarlett%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520sequentially%2520maximizing%2520an%2520unknown%2520function%2520%2524f%2524%250Aover%2520a%2520set%2520of%2520actions%2520of%2520the%2520form%2520%2524%2528s%252C%255Cmathbf%257Bx%257D%2529%2524%252C%2520where%2520the%2520selected%2520actions%250Amust%2520satisfy%2520a%2520safety%2520constraint%2520with%2520respect%2520to%2520an%2520unknown%2520safety%2520function%250A%2524g%2524.%2520We%2520model%2520%2524f%2524%2520and%2520%2524g%2524%2520as%2520lying%2520in%2520a%2520reproducing%2520kernel%2520Hilbert%2520space%250A%2528RKHS%2529%252C%2520which%2520facilitates%2520the%2520use%2520of%2520Gaussian%2520process%2520methods.%2520While%2520existing%250Aworks%2520for%2520this%2520setting%2520have%2520provided%2520algorithms%2520that%2520are%2520guaranteed%2520to%2520identify%250Aa%2520near-optimal%2520safe%2520action%252C%2520the%2520problem%2520of%2520attaining%2520low%2520cumulative%2520regret%2520has%250Aremained%2520largely%2520unexplored%252C%2520with%2520a%2520key%2520challenge%2520being%2520that%2520expanding%2520the%2520safe%250Aregion%2520can%2520incur%2520high%2520regret.%2520To%2520address%2520this%2520challenge%252C%2520we%2520show%2520that%2520if%2520%2524g%2524%2520is%250Amonotone%2520with%2520respect%2520to%2520just%2520the%2520single%2520variable%2520%2524s%2524%2520%2528with%2520no%2520such%2520constraint%250Aon%2520%2524f%2524%2529%252C%2520sublinear%2520regret%2520becomes%2520achievable%2520with%2520our%2520proposed%2520algorithm.%2520In%250Aaddition%252C%2520we%2520show%2520that%2520a%2520modified%2520version%2520of%2520our%2520algorithm%2520is%2520able%2520to%2520attain%250Asublinear%2520regret%2520%2528for%2520suitably%2520defined%2520notions%2520of%2520regret%2529%2520for%2520the%2520task%2520of%250Afinding%2520a%2520near-optimal%2520%2524s%2524%2520corresponding%2520to%2520every%2520%2524%255Cmathbf%257Bx%257D%2524%252C%2520as%2520opposed%2520to%250Aonly%2520finding%2520the%2520global%2520safe%2520optimum.%2520Our%2520findings%2520are%2520supported%2520with%2520empirical%250Aevaluations%2520on%2520various%2520objective%2520and%2520safety%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No-Regret%20Algorithms%20for%20Safe%20Bayesian%20Optimization%20with%20Monotonicity%0A%20%20Constraints&entry.906535625=Arpan%20Losalka%20and%20Jonathan%20Scarlett&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20sequentially%20maximizing%20an%20unknown%20function%20%24f%24%0Aover%20a%20set%20of%20actions%20of%20the%20form%20%24%28s%2C%5Cmathbf%7Bx%7D%29%24%2C%20where%20the%20selected%20actions%0Amust%20satisfy%20a%20safety%20constraint%20with%20respect%20to%20an%20unknown%20safety%20function%0A%24g%24.%20We%20model%20%24f%24%20and%20%24g%24%20as%20lying%20in%20a%20reproducing%20kernel%20Hilbert%20space%0A%28RKHS%29%2C%20which%20facilitates%20the%20use%20of%20Gaussian%20process%20methods.%20While%20existing%0Aworks%20for%20this%20setting%20have%20provided%20algorithms%20that%20are%20guaranteed%20to%20identify%0Aa%20near-optimal%20safe%20action%2C%20the%20problem%20of%20attaining%20low%20cumulative%20regret%20has%0Aremained%20largely%20unexplored%2C%20with%20a%20key%20challenge%20being%20that%20expanding%20the%20safe%0Aregion%20can%20incur%20high%20regret.%20To%20address%20this%20challenge%2C%20we%20show%20that%20if%20%24g%24%20is%0Amonotone%20with%20respect%20to%20just%20the%20single%20variable%20%24s%24%20%28with%20no%20such%20constraint%0Aon%20%24f%24%29%2C%20sublinear%20regret%20becomes%20achievable%20with%20our%20proposed%20algorithm.%20In%0Aaddition%2C%20we%20show%20that%20a%20modified%20version%20of%20our%20algorithm%20is%20able%20to%20attain%0Asublinear%20regret%20%28for%20suitably%20defined%20notions%20of%20regret%29%20for%20the%20task%20of%0Afinding%20a%20near-optimal%20%24s%24%20corresponding%20to%20every%20%24%5Cmathbf%7Bx%7D%24%2C%20as%20opposed%20to%0Aonly%20finding%20the%20global%20safe%20optimum.%20Our%20findings%20are%20supported%20with%20empirical%0Aevaluations%20on%20various%20objective%20and%20safety%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03264v1&entry.124074799=Read"},
{"title": "Error-preserving Automatic Speech Recognition of Young English Learners'\n  Language", "author": "Janick Michot and Manuela H\u00fcrlimann and Jan Deriu and Luzia Sauer and Katsiaryna Mlynchyk and Mark Cieliebak", "abstract": "  One of the central skills that language learners need to practice is speaking\nthe language. Currently, students in school do not get enough speaking\nopportunities and lack conversational practice. Recent advances in speech\ntechnology and natural language processing allow for the creation of novel\ntools to practice their speaking skills. In this work, we tackle the first\ncomponent of such a pipeline, namely, the automated speech recognition module\n(ASR), which faces a number of challenges: first, state-of-the-art ASR models\nare often trained on adult read-aloud data by native speakers and do not\ntransfer well to young language learners' speech. Second, most ASR systems\ncontain a powerful language model, which smooths out errors made by the\nspeakers. To give corrective feedback, which is a crucial part of language\nlearning, the ASR systems in our setting need to preserve the errors made by\nthe language learners. In this work, we build an ASR system that satisfies\nthese requirements: it works on spontaneous speech by young language learners\nand preserves their errors. For this, we collected a corpus containing around\n85 hours of English audio spoken by learners in Switzerland from grades 4 to 6\non different language learning tasks, which we used to train an ASR model. Our\nexperiments show that our model benefits from direct fine-tuning on children's\nvoices and has a much higher error preservation rate than other models.\n", "link": "http://arxiv.org/abs/2406.03235v1", "date": "2024-06-05", "relevancy": 1.7229, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4323}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4298}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Error-preserving%20Automatic%20Speech%20Recognition%20of%20Young%20English%20Learners%27%0A%20%20Language&body=Title%3A%20Error-preserving%20Automatic%20Speech%20Recognition%20of%20Young%20English%20Learners%27%0A%20%20Language%0AAuthor%3A%20Janick%20Michot%20and%20Manuela%20H%C3%BCrlimann%20and%20Jan%20Deriu%20and%20Luzia%20Sauer%20and%20Katsiaryna%20Mlynchyk%20and%20Mark%20Cieliebak%0AAbstract%3A%20%20%20One%20of%20the%20central%20skills%20that%20language%20learners%20need%20to%20practice%20is%20speaking%0Athe%20language.%20Currently%2C%20students%20in%20school%20do%20not%20get%20enough%20speaking%0Aopportunities%20and%20lack%20conversational%20practice.%20Recent%20advances%20in%20speech%0Atechnology%20and%20natural%20language%20processing%20allow%20for%20the%20creation%20of%20novel%0Atools%20to%20practice%20their%20speaking%20skills.%20In%20this%20work%2C%20we%20tackle%20the%20first%0Acomponent%20of%20such%20a%20pipeline%2C%20namely%2C%20the%20automated%20speech%20recognition%20module%0A%28ASR%29%2C%20which%20faces%20a%20number%20of%20challenges%3A%20first%2C%20state-of-the-art%20ASR%20models%0Aare%20often%20trained%20on%20adult%20read-aloud%20data%20by%20native%20speakers%20and%20do%20not%0Atransfer%20well%20to%20young%20language%20learners%27%20speech.%20Second%2C%20most%20ASR%20systems%0Acontain%20a%20powerful%20language%20model%2C%20which%20smooths%20out%20errors%20made%20by%20the%0Aspeakers.%20To%20give%20corrective%20feedback%2C%20which%20is%20a%20crucial%20part%20of%20language%0Alearning%2C%20the%20ASR%20systems%20in%20our%20setting%20need%20to%20preserve%20the%20errors%20made%20by%0Athe%20language%20learners.%20In%20this%20work%2C%20we%20build%20an%20ASR%20system%20that%20satisfies%0Athese%20requirements%3A%20it%20works%20on%20spontaneous%20speech%20by%20young%20language%20learners%0Aand%20preserves%20their%20errors.%20For%20this%2C%20we%20collected%20a%20corpus%20containing%20around%0A85%20hours%20of%20English%20audio%20spoken%20by%20learners%20in%20Switzerland%20from%20grades%204%20to%206%0Aon%20different%20language%20learning%20tasks%2C%20which%20we%20used%20to%20train%20an%20ASR%20model.%20Our%0Aexperiments%20show%20that%20our%20model%20benefits%20from%20direct%20fine-tuning%20on%20children%27s%0Avoices%20and%20has%20a%20much%20higher%20error%20preservation%20rate%20than%20other%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DError-preserving%2520Automatic%2520Speech%2520Recognition%2520of%2520Young%2520English%2520Learners%2527%250A%2520%2520Language%26entry.906535625%3DJanick%2520Michot%2520and%2520Manuela%2520H%25C3%25BCrlimann%2520and%2520Jan%2520Deriu%2520and%2520Luzia%2520Sauer%2520and%2520Katsiaryna%2520Mlynchyk%2520and%2520Mark%2520Cieliebak%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520central%2520skills%2520that%2520language%2520learners%2520need%2520to%2520practice%2520is%2520speaking%250Athe%2520language.%2520Currently%252C%2520students%2520in%2520school%2520do%2520not%2520get%2520enough%2520speaking%250Aopportunities%2520and%2520lack%2520conversational%2520practice.%2520Recent%2520advances%2520in%2520speech%250Atechnology%2520and%2520natural%2520language%2520processing%2520allow%2520for%2520the%2520creation%2520of%2520novel%250Atools%2520to%2520practice%2520their%2520speaking%2520skills.%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520first%250Acomponent%2520of%2520such%2520a%2520pipeline%252C%2520namely%252C%2520the%2520automated%2520speech%2520recognition%2520module%250A%2528ASR%2529%252C%2520which%2520faces%2520a%2520number%2520of%2520challenges%253A%2520first%252C%2520state-of-the-art%2520ASR%2520models%250Aare%2520often%2520trained%2520on%2520adult%2520read-aloud%2520data%2520by%2520native%2520speakers%2520and%2520do%2520not%250Atransfer%2520well%2520to%2520young%2520language%2520learners%2527%2520speech.%2520Second%252C%2520most%2520ASR%2520systems%250Acontain%2520a%2520powerful%2520language%2520model%252C%2520which%2520smooths%2520out%2520errors%2520made%2520by%2520the%250Aspeakers.%2520To%2520give%2520corrective%2520feedback%252C%2520which%2520is%2520a%2520crucial%2520part%2520of%2520language%250Alearning%252C%2520the%2520ASR%2520systems%2520in%2520our%2520setting%2520need%2520to%2520preserve%2520the%2520errors%2520made%2520by%250Athe%2520language%2520learners.%2520In%2520this%2520work%252C%2520we%2520build%2520an%2520ASR%2520system%2520that%2520satisfies%250Athese%2520requirements%253A%2520it%2520works%2520on%2520spontaneous%2520speech%2520by%2520young%2520language%2520learners%250Aand%2520preserves%2520their%2520errors.%2520For%2520this%252C%2520we%2520collected%2520a%2520corpus%2520containing%2520around%250A85%2520hours%2520of%2520English%2520audio%2520spoken%2520by%2520learners%2520in%2520Switzerland%2520from%2520grades%25204%2520to%25206%250Aon%2520different%2520language%2520learning%2520tasks%252C%2520which%2520we%2520used%2520to%2520train%2520an%2520ASR%2520model.%2520Our%250Aexperiments%2520show%2520that%2520our%2520model%2520benefits%2520from%2520direct%2520fine-tuning%2520on%2520children%2527s%250Avoices%2520and%2520has%2520a%2520much%2520higher%2520error%2520preservation%2520rate%2520than%2520other%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Error-preserving%20Automatic%20Speech%20Recognition%20of%20Young%20English%20Learners%27%0A%20%20Language&entry.906535625=Janick%20Michot%20and%20Manuela%20H%C3%BCrlimann%20and%20Jan%20Deriu%20and%20Luzia%20Sauer%20and%20Katsiaryna%20Mlynchyk%20and%20Mark%20Cieliebak&entry.1292438233=%20%20One%20of%20the%20central%20skills%20that%20language%20learners%20need%20to%20practice%20is%20speaking%0Athe%20language.%20Currently%2C%20students%20in%20school%20do%20not%20get%20enough%20speaking%0Aopportunities%20and%20lack%20conversational%20practice.%20Recent%20advances%20in%20speech%0Atechnology%20and%20natural%20language%20processing%20allow%20for%20the%20creation%20of%20novel%0Atools%20to%20practice%20their%20speaking%20skills.%20In%20this%20work%2C%20we%20tackle%20the%20first%0Acomponent%20of%20such%20a%20pipeline%2C%20namely%2C%20the%20automated%20speech%20recognition%20module%0A%28ASR%29%2C%20which%20faces%20a%20number%20of%20challenges%3A%20first%2C%20state-of-the-art%20ASR%20models%0Aare%20often%20trained%20on%20adult%20read-aloud%20data%20by%20native%20speakers%20and%20do%20not%0Atransfer%20well%20to%20young%20language%20learners%27%20speech.%20Second%2C%20most%20ASR%20systems%0Acontain%20a%20powerful%20language%20model%2C%20which%20smooths%20out%20errors%20made%20by%20the%0Aspeakers.%20To%20give%20corrective%20feedback%2C%20which%20is%20a%20crucial%20part%20of%20language%0Alearning%2C%20the%20ASR%20systems%20in%20our%20setting%20need%20to%20preserve%20the%20errors%20made%20by%0Athe%20language%20learners.%20In%20this%20work%2C%20we%20build%20an%20ASR%20system%20that%20satisfies%0Athese%20requirements%3A%20it%20works%20on%20spontaneous%20speech%20by%20young%20language%20learners%0Aand%20preserves%20their%20errors.%20For%20this%2C%20we%20collected%20a%20corpus%20containing%20around%0A85%20hours%20of%20English%20audio%20spoken%20by%20learners%20in%20Switzerland%20from%20grades%204%20to%206%0Aon%20different%20language%20learning%20tasks%2C%20which%20we%20used%20to%20train%20an%20ASR%20model.%20Our%0Aexperiments%20show%20that%20our%20model%20benefits%20from%20direct%20fine-tuning%20on%20children%27s%0Avoices%20and%20has%20a%20much%20higher%20error%20preservation%20rate%20than%20other%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03235v1&entry.124074799=Read"},
{"title": "Object Manipulation in Marine Environments using Reinforcement Learning", "author": "Ahmed Nader and Muhayy Ud Din and Mughni Irfan and Irfan Hussain", "abstract": "  Performing intervention tasks in the maritime domain is crucial for safety\nand operational efficiency. The unpredictable and dynamic marine environment\nmakes the intervention tasks such as object manipulation extremely challenging.\nThis study proposes a robust solution for object manipulation from a dock in\nthe presence of disturbances caused by sea waves. To tackle this challenging\nproblem, we apply a deep reinforcement learning (DRL) based algorithm called\nSoft. Actor-Critic (SAC). SAC employs an actor-critic framework; the actors\nlearn a policy that minimizes an objective function while the critic evaluates\nthe learned policy and provides feedback to guide the actor-learning process.\nWe trained the agent using the PyBullet dynamic simulator and tested it in a\nrealistic simulation environment called MBZIRC maritime simulator. This\nsimulator allows the simulation of different wave conditions according to the\nWorld Meteorological Organization (WMO) sea state code. Simulation results\ndemonstrate a high success rate in retrieving the objects from the dock. The\ntrained agent achieved an 80 percent success rate when applied in the\nsimulation environment in the presence of waves characterized by sea state 2,\naccording to the WMO sea state code\n", "link": "http://arxiv.org/abs/2406.03223v1", "date": "2024-06-05", "relevancy": 1.5966, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5423}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Manipulation%20in%20Marine%20Environments%20using%20Reinforcement%20Learning&body=Title%3A%20Object%20Manipulation%20in%20Marine%20Environments%20using%20Reinforcement%20Learning%0AAuthor%3A%20Ahmed%20Nader%20and%20Muhayy%20Ud%20Din%20and%20Mughni%20Irfan%20and%20Irfan%20Hussain%0AAbstract%3A%20%20%20Performing%20intervention%20tasks%20in%20the%20maritime%20domain%20is%20crucial%20for%20safety%0Aand%20operational%20efficiency.%20The%20unpredictable%20and%20dynamic%20marine%20environment%0Amakes%20the%20intervention%20tasks%20such%20as%20object%20manipulation%20extremely%20challenging.%0AThis%20study%20proposes%20a%20robust%20solution%20for%20object%20manipulation%20from%20a%20dock%20in%0Athe%20presence%20of%20disturbances%20caused%20by%20sea%20waves.%20To%20tackle%20this%20challenging%0Aproblem%2C%20we%20apply%20a%20deep%20reinforcement%20learning%20%28DRL%29%20based%20algorithm%20called%0ASoft.%20Actor-Critic%20%28SAC%29.%20SAC%20employs%20an%20actor-critic%20framework%3B%20the%20actors%0Alearn%20a%20policy%20that%20minimizes%20an%20objective%20function%20while%20the%20critic%20evaluates%0Athe%20learned%20policy%20and%20provides%20feedback%20to%20guide%20the%20actor-learning%20process.%0AWe%20trained%20the%20agent%20using%20the%20PyBullet%20dynamic%20simulator%20and%20tested%20it%20in%20a%0Arealistic%20simulation%20environment%20called%20MBZIRC%20maritime%20simulator.%20This%0Asimulator%20allows%20the%20simulation%20of%20different%20wave%20conditions%20according%20to%20the%0AWorld%20Meteorological%20Organization%20%28WMO%29%20sea%20state%20code.%20Simulation%20results%0Ademonstrate%20a%20high%20success%20rate%20in%20retrieving%20the%20objects%20from%20the%20dock.%20The%0Atrained%20agent%20achieved%20an%2080%20percent%20success%20rate%20when%20applied%20in%20the%0Asimulation%20environment%20in%20the%20presence%20of%20waves%20characterized%20by%20sea%20state%202%2C%0Aaccording%20to%20the%20WMO%20sea%20state%20code%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Manipulation%2520in%2520Marine%2520Environments%2520using%2520Reinforcement%2520Learning%26entry.906535625%3DAhmed%2520Nader%2520and%2520Muhayy%2520Ud%2520Din%2520and%2520Mughni%2520Irfan%2520and%2520Irfan%2520Hussain%26entry.1292438233%3D%2520%2520Performing%2520intervention%2520tasks%2520in%2520the%2520maritime%2520domain%2520is%2520crucial%2520for%2520safety%250Aand%2520operational%2520efficiency.%2520The%2520unpredictable%2520and%2520dynamic%2520marine%2520environment%250Amakes%2520the%2520intervention%2520tasks%2520such%2520as%2520object%2520manipulation%2520extremely%2520challenging.%250AThis%2520study%2520proposes%2520a%2520robust%2520solution%2520for%2520object%2520manipulation%2520from%2520a%2520dock%2520in%250Athe%2520presence%2520of%2520disturbances%2520caused%2520by%2520sea%2520waves.%2520To%2520tackle%2520this%2520challenging%250Aproblem%252C%2520we%2520apply%2520a%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520based%2520algorithm%2520called%250ASoft.%2520Actor-Critic%2520%2528SAC%2529.%2520SAC%2520employs%2520an%2520actor-critic%2520framework%253B%2520the%2520actors%250Alearn%2520a%2520policy%2520that%2520minimizes%2520an%2520objective%2520function%2520while%2520the%2520critic%2520evaluates%250Athe%2520learned%2520policy%2520and%2520provides%2520feedback%2520to%2520guide%2520the%2520actor-learning%2520process.%250AWe%2520trained%2520the%2520agent%2520using%2520the%2520PyBullet%2520dynamic%2520simulator%2520and%2520tested%2520it%2520in%2520a%250Arealistic%2520simulation%2520environment%2520called%2520MBZIRC%2520maritime%2520simulator.%2520This%250Asimulator%2520allows%2520the%2520simulation%2520of%2520different%2520wave%2520conditions%2520according%2520to%2520the%250AWorld%2520Meteorological%2520Organization%2520%2528WMO%2529%2520sea%2520state%2520code.%2520Simulation%2520results%250Ademonstrate%2520a%2520high%2520success%2520rate%2520in%2520retrieving%2520the%2520objects%2520from%2520the%2520dock.%2520The%250Atrained%2520agent%2520achieved%2520an%252080%2520percent%2520success%2520rate%2520when%2520applied%2520in%2520the%250Asimulation%2520environment%2520in%2520the%2520presence%2520of%2520waves%2520characterized%2520by%2520sea%2520state%25202%252C%250Aaccording%2520to%2520the%2520WMO%2520sea%2520state%2520code%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Manipulation%20in%20Marine%20Environments%20using%20Reinforcement%20Learning&entry.906535625=Ahmed%20Nader%20and%20Muhayy%20Ud%20Din%20and%20Mughni%20Irfan%20and%20Irfan%20Hussain&entry.1292438233=%20%20Performing%20intervention%20tasks%20in%20the%20maritime%20domain%20is%20crucial%20for%20safety%0Aand%20operational%20efficiency.%20The%20unpredictable%20and%20dynamic%20marine%20environment%0Amakes%20the%20intervention%20tasks%20such%20as%20object%20manipulation%20extremely%20challenging.%0AThis%20study%20proposes%20a%20robust%20solution%20for%20object%20manipulation%20from%20a%20dock%20in%0Athe%20presence%20of%20disturbances%20caused%20by%20sea%20waves.%20To%20tackle%20this%20challenging%0Aproblem%2C%20we%20apply%20a%20deep%20reinforcement%20learning%20%28DRL%29%20based%20algorithm%20called%0ASoft.%20Actor-Critic%20%28SAC%29.%20SAC%20employs%20an%20actor-critic%20framework%3B%20the%20actors%0Alearn%20a%20policy%20that%20minimizes%20an%20objective%20function%20while%20the%20critic%20evaluates%0Athe%20learned%20policy%20and%20provides%20feedback%20to%20guide%20the%20actor-learning%20process.%0AWe%20trained%20the%20agent%20using%20the%20PyBullet%20dynamic%20simulator%20and%20tested%20it%20in%20a%0Arealistic%20simulation%20environment%20called%20MBZIRC%20maritime%20simulator.%20This%0Asimulator%20allows%20the%20simulation%20of%20different%20wave%20conditions%20according%20to%20the%0AWorld%20Meteorological%20Organization%20%28WMO%29%20sea%20state%20code.%20Simulation%20results%0Ademonstrate%20a%20high%20success%20rate%20in%20retrieving%20the%20objects%20from%20the%20dock.%20The%0Atrained%20agent%20achieved%20an%2080%20percent%20success%20rate%20when%20applied%20in%20the%0Asimulation%20environment%20in%20the%20presence%20of%20waves%20characterized%20by%20sea%20state%202%2C%0Aaccording%20to%20the%20WMO%20sea%20state%20code%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03223v1&entry.124074799=Read"},
{"title": "Large Language Models Can Infer Psychological Dispositions of Social\n  Media Users", "author": "Heinrich Peters and Sandra Matz", "abstract": "  Large Language Models (LLMs) demonstrate increasingly human-like abilities\nacross a wide variety of tasks. In this paper, we investigate whether LLMs like\nChatGPT can accurately infer the psychological dispositions of social media\nusers and whether their ability to do so varies across socio-demographic\ngroups. Specifically, we test whether GPT-3.5 and GPT-4 can derive the Big Five\npersonality traits from users' Facebook status updates in a zero-shot learning\nscenario. Our results show an average correlation of r = .29 (range = [.22,\n.33]) between LLM-inferred and self-reported trait scores - a level of accuracy\nthat is similar to that of supervised machine learning models specifically\ntrained to infer personality. Our findings also highlight heterogeneity in the\naccuracy of personality inferences across different age groups and gender\ncategories: predictions were found to be more accurate for women and younger\nindividuals on several traits, suggesting a potential bias stemming from the\nunderlying training data or differences in online self-expression. The ability\nof LLMs to infer psychological dispositions from user-generated text has the\npotential to democratize access to cheap and scalable psychometric assessments\nfor both researchers and practitioners. On the one hand, this democratization\nmight facilitate large-scale research of high ecological validity and spark\ninnovation in personalized services. On the other hand, it also raises ethical\nconcerns regarding user privacy and self-determination, highlighting the need\nfor stringent ethical frameworks and regulation.\n", "link": "http://arxiv.org/abs/2309.08631v2", "date": "2024-06-05", "relevancy": 1.3384, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4556}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4477}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Can%20Infer%20Psychological%20Dispositions%20of%20Social%0A%20%20Media%20Users&body=Title%3A%20Large%20Language%20Models%20Can%20Infer%20Psychological%20Dispositions%20of%20Social%0A%20%20Media%20Users%0AAuthor%3A%20Heinrich%20Peters%20and%20Sandra%20Matz%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20increasingly%20human-like%20abilities%0Aacross%20a%20wide%20variety%20of%20tasks.%20In%20this%20paper%2C%20we%20investigate%20whether%20LLMs%20like%0AChatGPT%20can%20accurately%20infer%20the%20psychological%20dispositions%20of%20social%20media%0Ausers%20and%20whether%20their%20ability%20to%20do%20so%20varies%20across%20socio-demographic%0Agroups.%20Specifically%2C%20we%20test%20whether%20GPT-3.5%20and%20GPT-4%20can%20derive%20the%20Big%20Five%0Apersonality%20traits%20from%20users%27%20Facebook%20status%20updates%20in%20a%20zero-shot%20learning%0Ascenario.%20Our%20results%20show%20an%20average%20correlation%20of%20r%20%3D%20.29%20%28range%20%3D%20%5B.22%2C%0A.33%5D%29%20between%20LLM-inferred%20and%20self-reported%20trait%20scores%20-%20a%20level%20of%20accuracy%0Athat%20is%20similar%20to%20that%20of%20supervised%20machine%20learning%20models%20specifically%0Atrained%20to%20infer%20personality.%20Our%20findings%20also%20highlight%20heterogeneity%20in%20the%0Aaccuracy%20of%20personality%20inferences%20across%20different%20age%20groups%20and%20gender%0Acategories%3A%20predictions%20were%20found%20to%20be%20more%20accurate%20for%20women%20and%20younger%0Aindividuals%20on%20several%20traits%2C%20suggesting%20a%20potential%20bias%20stemming%20from%20the%0Aunderlying%20training%20data%20or%20differences%20in%20online%20self-expression.%20The%20ability%0Aof%20LLMs%20to%20infer%20psychological%20dispositions%20from%20user-generated%20text%20has%20the%0Apotential%20to%20democratize%20access%20to%20cheap%20and%20scalable%20psychometric%20assessments%0Afor%20both%20researchers%20and%20practitioners.%20On%20the%20one%20hand%2C%20this%20democratization%0Amight%20facilitate%20large-scale%20research%20of%20high%20ecological%20validity%20and%20spark%0Ainnovation%20in%20personalized%20services.%20On%20the%20other%20hand%2C%20it%20also%20raises%20ethical%0Aconcerns%20regarding%20user%20privacy%20and%20self-determination%2C%20highlighting%20the%20need%0Afor%20stringent%20ethical%20frameworks%20and%20regulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Can%2520Infer%2520Psychological%2520Dispositions%2520of%2520Social%250A%2520%2520Media%2520Users%26entry.906535625%3DHeinrich%2520Peters%2520and%2520Sandra%2520Matz%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520increasingly%2520human-like%2520abilities%250Aacross%2520a%2520wide%2520variety%2520of%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520whether%2520LLMs%2520like%250AChatGPT%2520can%2520accurately%2520infer%2520the%2520psychological%2520dispositions%2520of%2520social%2520media%250Ausers%2520and%2520whether%2520their%2520ability%2520to%2520do%2520so%2520varies%2520across%2520socio-demographic%250Agroups.%2520Specifically%252C%2520we%2520test%2520whether%2520GPT-3.5%2520and%2520GPT-4%2520can%2520derive%2520the%2520Big%2520Five%250Apersonality%2520traits%2520from%2520users%2527%2520Facebook%2520status%2520updates%2520in%2520a%2520zero-shot%2520learning%250Ascenario.%2520Our%2520results%2520show%2520an%2520average%2520correlation%2520of%2520r%2520%253D%2520.29%2520%2528range%2520%253D%2520%255B.22%252C%250A.33%255D%2529%2520between%2520LLM-inferred%2520and%2520self-reported%2520trait%2520scores%2520-%2520a%2520level%2520of%2520accuracy%250Athat%2520is%2520similar%2520to%2520that%2520of%2520supervised%2520machine%2520learning%2520models%2520specifically%250Atrained%2520to%2520infer%2520personality.%2520Our%2520findings%2520also%2520highlight%2520heterogeneity%2520in%2520the%250Aaccuracy%2520of%2520personality%2520inferences%2520across%2520different%2520age%2520groups%2520and%2520gender%250Acategories%253A%2520predictions%2520were%2520found%2520to%2520be%2520more%2520accurate%2520for%2520women%2520and%2520younger%250Aindividuals%2520on%2520several%2520traits%252C%2520suggesting%2520a%2520potential%2520bias%2520stemming%2520from%2520the%250Aunderlying%2520training%2520data%2520or%2520differences%2520in%2520online%2520self-expression.%2520The%2520ability%250Aof%2520LLMs%2520to%2520infer%2520psychological%2520dispositions%2520from%2520user-generated%2520text%2520has%2520the%250Apotential%2520to%2520democratize%2520access%2520to%2520cheap%2520and%2520scalable%2520psychometric%2520assessments%250Afor%2520both%2520researchers%2520and%2520practitioners.%2520On%2520the%2520one%2520hand%252C%2520this%2520democratization%250Amight%2520facilitate%2520large-scale%2520research%2520of%2520high%2520ecological%2520validity%2520and%2520spark%250Ainnovation%2520in%2520personalized%2520services.%2520On%2520the%2520other%2520hand%252C%2520it%2520also%2520raises%2520ethical%250Aconcerns%2520regarding%2520user%2520privacy%2520and%2520self-determination%252C%2520highlighting%2520the%2520need%250Afor%2520stringent%2520ethical%2520frameworks%2520and%2520regulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Can%20Infer%20Psychological%20Dispositions%20of%20Social%0A%20%20Media%20Users&entry.906535625=Heinrich%20Peters%20and%20Sandra%20Matz&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20increasingly%20human-like%20abilities%0Aacross%20a%20wide%20variety%20of%20tasks.%20In%20this%20paper%2C%20we%20investigate%20whether%20LLMs%20like%0AChatGPT%20can%20accurately%20infer%20the%20psychological%20dispositions%20of%20social%20media%0Ausers%20and%20whether%20their%20ability%20to%20do%20so%20varies%20across%20socio-demographic%0Agroups.%20Specifically%2C%20we%20test%20whether%20GPT-3.5%20and%20GPT-4%20can%20derive%20the%20Big%20Five%0Apersonality%20traits%20from%20users%27%20Facebook%20status%20updates%20in%20a%20zero-shot%20learning%0Ascenario.%20Our%20results%20show%20an%20average%20correlation%20of%20r%20%3D%20.29%20%28range%20%3D%20%5B.22%2C%0A.33%5D%29%20between%20LLM-inferred%20and%20self-reported%20trait%20scores%20-%20a%20level%20of%20accuracy%0Athat%20is%20similar%20to%20that%20of%20supervised%20machine%20learning%20models%20specifically%0Atrained%20to%20infer%20personality.%20Our%20findings%20also%20highlight%20heterogeneity%20in%20the%0Aaccuracy%20of%20personality%20inferences%20across%20different%20age%20groups%20and%20gender%0Acategories%3A%20predictions%20were%20found%20to%20be%20more%20accurate%20for%20women%20and%20younger%0Aindividuals%20on%20several%20traits%2C%20suggesting%20a%20potential%20bias%20stemming%20from%20the%0Aunderlying%20training%20data%20or%20differences%20in%20online%20self-expression.%20The%20ability%0Aof%20LLMs%20to%20infer%20psychological%20dispositions%20from%20user-generated%20text%20has%20the%0Apotential%20to%20democratize%20access%20to%20cheap%20and%20scalable%20psychometric%20assessments%0Afor%20both%20researchers%20and%20practitioners.%20On%20the%20one%20hand%2C%20this%20democratization%0Amight%20facilitate%20large-scale%20research%20of%20high%20ecological%20validity%20and%20spark%0Ainnovation%20in%20personalized%20services.%20On%20the%20other%20hand%2C%20it%20also%20raises%20ethical%0Aconcerns%20regarding%20user%20privacy%20and%20self-determination%2C%20highlighting%20the%20need%0Afor%20stringent%20ethical%20frameworks%20and%20regulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08631v2&entry.124074799=Read"},
{"title": "MoMo: Momentum Models for Adaptive Learning Rates", "author": "Fabian Schaipp and Ruben Ohana and Michael Eickenberg and Aaron Defazio and Robert M. Gower", "abstract": "  Training a modern machine learning architecture on a new task requires\nextensive learning-rate tuning, which comes at a high computational cost. Here\nwe develop new Polyak-type adaptive learning rates that can be used on top of\nany momentum method, and require less tuning to perform well. We first develop\nMoMo, a Momentum Model based adaptive learning rate for SGD-M (stochastic\ngradient descent with momentum). MoMo uses momentum estimates of the losses and\ngradients sampled at each iteration to build a model of the loss function. Our\nmodel makes use of any known lower bound of the loss function by using\ntruncation, e.g. most losses are lower-bounded by zero. The model is then\napproximately minimized at each iteration to compute the next step. We show how\nMoMo can be used in combination with any momentum-based method, and showcase\nthis by developing MoMo-Adam, which is Adam with our new model-based adaptive\nlearning rate. We show that MoMo attains a $\\mathcal{O}(1/\\sqrt{K})$\nconvergence rate for convex problems with interpolation, needing knowledge of\nno problem-specific quantities other than the optimal value. Additionally, for\nlosses with unknown lower bounds, we develop on-the-fly estimates of a lower\nbound, that are incorporated in our model. We show that MoMo and MoMo-Adam\nimprove over SGD-M and Adam in terms of robustness to hyperparameter tuning for\ntraining image classifiers on MNIST, CIFAR, and Imagenet, for recommender\nsystems on Criteo, for a transformer model on the translation task IWSLT14, and\nfor a diffusion model.\n", "link": "http://arxiv.org/abs/2305.07583v3", "date": "2024-06-05", "relevancy": 1.5483, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5149}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoMo%3A%20Momentum%20Models%20for%20Adaptive%20Learning%20Rates&body=Title%3A%20MoMo%3A%20Momentum%20Models%20for%20Adaptive%20Learning%20Rates%0AAuthor%3A%20Fabian%20Schaipp%20and%20Ruben%20Ohana%20and%20Michael%20Eickenberg%20and%20Aaron%20Defazio%20and%20Robert%20M.%20Gower%0AAbstract%3A%20%20%20Training%20a%20modern%20machine%20learning%20architecture%20on%20a%20new%20task%20requires%0Aextensive%20learning-rate%20tuning%2C%20which%20comes%20at%20a%20high%20computational%20cost.%20Here%0Awe%20develop%20new%20Polyak-type%20adaptive%20learning%20rates%20that%20can%20be%20used%20on%20top%20of%0Aany%20momentum%20method%2C%20and%20require%20less%20tuning%20to%20perform%20well.%20We%20first%20develop%0AMoMo%2C%20a%20Momentum%20Model%20based%20adaptive%20learning%20rate%20for%20SGD-M%20%28stochastic%0Agradient%20descent%20with%20momentum%29.%20MoMo%20uses%20momentum%20estimates%20of%20the%20losses%20and%0Agradients%20sampled%20at%20each%20iteration%20to%20build%20a%20model%20of%20the%20loss%20function.%20Our%0Amodel%20makes%20use%20of%20any%20known%20lower%20bound%20of%20the%20loss%20function%20by%20using%0Atruncation%2C%20e.g.%20most%20losses%20are%20lower-bounded%20by%20zero.%20The%20model%20is%20then%0Aapproximately%20minimized%20at%20each%20iteration%20to%20compute%20the%20next%20step.%20We%20show%20how%0AMoMo%20can%20be%20used%20in%20combination%20with%20any%20momentum-based%20method%2C%20and%20showcase%0Athis%20by%20developing%20MoMo-Adam%2C%20which%20is%20Adam%20with%20our%20new%20model-based%20adaptive%0Alearning%20rate.%20We%20show%20that%20MoMo%20attains%20a%20%24%5Cmathcal%7BO%7D%281/%5Csqrt%7BK%7D%29%24%0Aconvergence%20rate%20for%20convex%20problems%20with%20interpolation%2C%20needing%20knowledge%20of%0Ano%20problem-specific%20quantities%20other%20than%20the%20optimal%20value.%20Additionally%2C%20for%0Alosses%20with%20unknown%20lower%20bounds%2C%20we%20develop%20on-the-fly%20estimates%20of%20a%20lower%0Abound%2C%20that%20are%20incorporated%20in%20our%20model.%20We%20show%20that%20MoMo%20and%20MoMo-Adam%0Aimprove%20over%20SGD-M%20and%20Adam%20in%20terms%20of%20robustness%20to%20hyperparameter%20tuning%20for%0Atraining%20image%20classifiers%20on%20MNIST%2C%20CIFAR%2C%20and%20Imagenet%2C%20for%20recommender%0Asystems%20on%20Criteo%2C%20for%20a%20transformer%20model%20on%20the%20translation%20task%20IWSLT14%2C%20and%0Afor%20a%20diffusion%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.07583v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoMo%253A%2520Momentum%2520Models%2520for%2520Adaptive%2520Learning%2520Rates%26entry.906535625%3DFabian%2520Schaipp%2520and%2520Ruben%2520Ohana%2520and%2520Michael%2520Eickenberg%2520and%2520Aaron%2520Defazio%2520and%2520Robert%2520M.%2520Gower%26entry.1292438233%3D%2520%2520Training%2520a%2520modern%2520machine%2520learning%2520architecture%2520on%2520a%2520new%2520task%2520requires%250Aextensive%2520learning-rate%2520tuning%252C%2520which%2520comes%2520at%2520a%2520high%2520computational%2520cost.%2520Here%250Awe%2520develop%2520new%2520Polyak-type%2520adaptive%2520learning%2520rates%2520that%2520can%2520be%2520used%2520on%2520top%2520of%250Aany%2520momentum%2520method%252C%2520and%2520require%2520less%2520tuning%2520to%2520perform%2520well.%2520We%2520first%2520develop%250AMoMo%252C%2520a%2520Momentum%2520Model%2520based%2520adaptive%2520learning%2520rate%2520for%2520SGD-M%2520%2528stochastic%250Agradient%2520descent%2520with%2520momentum%2529.%2520MoMo%2520uses%2520momentum%2520estimates%2520of%2520the%2520losses%2520and%250Agradients%2520sampled%2520at%2520each%2520iteration%2520to%2520build%2520a%2520model%2520of%2520the%2520loss%2520function.%2520Our%250Amodel%2520makes%2520use%2520of%2520any%2520known%2520lower%2520bound%2520of%2520the%2520loss%2520function%2520by%2520using%250Atruncation%252C%2520e.g.%2520most%2520losses%2520are%2520lower-bounded%2520by%2520zero.%2520The%2520model%2520is%2520then%250Aapproximately%2520minimized%2520at%2520each%2520iteration%2520to%2520compute%2520the%2520next%2520step.%2520We%2520show%2520how%250AMoMo%2520can%2520be%2520used%2520in%2520combination%2520with%2520any%2520momentum-based%2520method%252C%2520and%2520showcase%250Athis%2520by%2520developing%2520MoMo-Adam%252C%2520which%2520is%2520Adam%2520with%2520our%2520new%2520model-based%2520adaptive%250Alearning%2520rate.%2520We%2520show%2520that%2520MoMo%2520attains%2520a%2520%2524%255Cmathcal%257BO%257D%25281/%255Csqrt%257BK%257D%2529%2524%250Aconvergence%2520rate%2520for%2520convex%2520problems%2520with%2520interpolation%252C%2520needing%2520knowledge%2520of%250Ano%2520problem-specific%2520quantities%2520other%2520than%2520the%2520optimal%2520value.%2520Additionally%252C%2520for%250Alosses%2520with%2520unknown%2520lower%2520bounds%252C%2520we%2520develop%2520on-the-fly%2520estimates%2520of%2520a%2520lower%250Abound%252C%2520that%2520are%2520incorporated%2520in%2520our%2520model.%2520We%2520show%2520that%2520MoMo%2520and%2520MoMo-Adam%250Aimprove%2520over%2520SGD-M%2520and%2520Adam%2520in%2520terms%2520of%2520robustness%2520to%2520hyperparameter%2520tuning%2520for%250Atraining%2520image%2520classifiers%2520on%2520MNIST%252C%2520CIFAR%252C%2520and%2520Imagenet%252C%2520for%2520recommender%250Asystems%2520on%2520Criteo%252C%2520for%2520a%2520transformer%2520model%2520on%2520the%2520translation%2520task%2520IWSLT14%252C%2520and%250Afor%2520a%2520diffusion%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.07583v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoMo%3A%20Momentum%20Models%20for%20Adaptive%20Learning%20Rates&entry.906535625=Fabian%20Schaipp%20and%20Ruben%20Ohana%20and%20Michael%20Eickenberg%20and%20Aaron%20Defazio%20and%20Robert%20M.%20Gower&entry.1292438233=%20%20Training%20a%20modern%20machine%20learning%20architecture%20on%20a%20new%20task%20requires%0Aextensive%20learning-rate%20tuning%2C%20which%20comes%20at%20a%20high%20computational%20cost.%20Here%0Awe%20develop%20new%20Polyak-type%20adaptive%20learning%20rates%20that%20can%20be%20used%20on%20top%20of%0Aany%20momentum%20method%2C%20and%20require%20less%20tuning%20to%20perform%20well.%20We%20first%20develop%0AMoMo%2C%20a%20Momentum%20Model%20based%20adaptive%20learning%20rate%20for%20SGD-M%20%28stochastic%0Agradient%20descent%20with%20momentum%29.%20MoMo%20uses%20momentum%20estimates%20of%20the%20losses%20and%0Agradients%20sampled%20at%20each%20iteration%20to%20build%20a%20model%20of%20the%20loss%20function.%20Our%0Amodel%20makes%20use%20of%20any%20known%20lower%20bound%20of%20the%20loss%20function%20by%20using%0Atruncation%2C%20e.g.%20most%20losses%20are%20lower-bounded%20by%20zero.%20The%20model%20is%20then%0Aapproximately%20minimized%20at%20each%20iteration%20to%20compute%20the%20next%20step.%20We%20show%20how%0AMoMo%20can%20be%20used%20in%20combination%20with%20any%20momentum-based%20method%2C%20and%20showcase%0Athis%20by%20developing%20MoMo-Adam%2C%20which%20is%20Adam%20with%20our%20new%20model-based%20adaptive%0Alearning%20rate.%20We%20show%20that%20MoMo%20attains%20a%20%24%5Cmathcal%7BO%7D%281/%5Csqrt%7BK%7D%29%24%0Aconvergence%20rate%20for%20convex%20problems%20with%20interpolation%2C%20needing%20knowledge%20of%0Ano%20problem-specific%20quantities%20other%20than%20the%20optimal%20value.%20Additionally%2C%20for%0Alosses%20with%20unknown%20lower%20bounds%2C%20we%20develop%20on-the-fly%20estimates%20of%20a%20lower%0Abound%2C%20that%20are%20incorporated%20in%20our%20model.%20We%20show%20that%20MoMo%20and%20MoMo-Adam%0Aimprove%20over%20SGD-M%20and%20Adam%20in%20terms%20of%20robustness%20to%20hyperparameter%20tuning%20for%0Atraining%20image%20classifiers%20on%20MNIST%2C%20CIFAR%2C%20and%20Imagenet%2C%20for%20recommender%0Asystems%20on%20Criteo%2C%20for%20a%20transformer%20model%20on%20the%20translation%20task%20IWSLT14%2C%20and%0Afor%20a%20diffusion%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.07583v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


