<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DELAN: Dual-Level Alignment for Vision-and-Language Navigation by\n  Cross-Modal Contrastive Learning", "author": "Mengfei Du and Binhao Wu and Jiwen Zhang and Zhihao Fan and Zejun Li and Ruipu Luo and Xuanjing Huang and Zhongyu Wei", "abstract": "  Vision-and-Language navigation (VLN) requires an agent to navigate in unseen\nenvironment by following natural language instruction. For task completion, the\nagent needs to align and integrate various navigation modalities, including\ninstruction, observation and navigation history. Existing works primarily\nconcentrate on cross-modal attention at the fusion stage to achieve this\nobjective. Nevertheless, modality features generated by disparate uni-encoders\nreside in their own spaces, leading to a decline in the quality of cross-modal\nfusion and decision. To address this problem, we propose a Dual-levEL AligNment\n(DELAN) framework by cross-modal contrastive learning. This framework is\ndesigned to align various navigation-related modalities before fusion, thereby\nenhancing cross-modal interaction and action decision-making. Specifically, we\ndivide the pre-fusion alignment into dual levels: instruction-history level and\nlandmark-observation level according to their semantic correlations. We also\nreconstruct a dual-level instruction for adaptation to the dual-level\nalignment. As the training signals for pre-fusion alignment are extremely\nlimited, self-supervised contrastive learning strategies are employed to\nenforce the matching between different modalities. Our approach seamlessly\nintegrates with the majority of existing models, resulting in improved\nnavigation performance on various VLN benchmarks, including R2R, R4R, RxR and\nCVDN.\n", "link": "http://arxiv.org/abs/2404.01994v1", "date": "2024-04-02", "relevancy": 2.7791, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5762}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5642}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.527}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DELAN%3A%20Dual-Level%20Alignment%20for%20Vision-and-Language%20Navigation%20by%0A%20%20Cross-Modal%20Contrastive%20Learning&body=Title%3A%20DELAN%3A%20Dual-Level%20Alignment%20for%20Vision-and-Language%20Navigation%20by%0A%20%20Cross-Modal%20Contrastive%20Learning%0AAuthor%3A%20Mengfei%20Du%20and%20Binhao%20Wu%20and%20Jiwen%20Zhang%20and%20Zhihao%20Fan%20and%20Zejun%20Li%20and%20Ruipu%20Luo%20and%20Xuanjing%20Huang%20and%20Zhongyu%20Wei%0AAbstract%3A%20%20%20Vision-and-Language%20navigation%20%28VLN%29%20requires%20an%20agent%20to%20navigate%20in%20unseen%0Aenvironment%20by%20following%20natural%20language%20instruction.%20For%20task%20completion%2C%20the%0Aagent%20needs%20to%20align%20and%20integrate%20various%20navigation%20modalities%2C%20including%0Ainstruction%2C%20observation%20and%20navigation%20history.%20Existing%20works%20primarily%0Aconcentrate%20on%20cross-modal%20attention%20at%20the%20fusion%20stage%20to%20achieve%20this%0Aobjective.%20Nevertheless%2C%20modality%20features%20generated%20by%20disparate%20uni-encoders%0Areside%20in%20their%20own%20spaces%2C%20leading%20to%20a%20decline%20in%20the%20quality%20of%20cross-modal%0Afusion%20and%20decision.%20To%20address%20this%20problem%2C%20we%20propose%20a%20Dual-levEL%20AligNment%0A%28DELAN%29%20framework%20by%20cross-modal%20contrastive%20learning.%20This%20framework%20is%0Adesigned%20to%20align%20various%20navigation-related%20modalities%20before%20fusion%2C%20thereby%0Aenhancing%20cross-modal%20interaction%20and%20action%20decision-making.%20Specifically%2C%20we%0Adivide%20the%20pre-fusion%20alignment%20into%20dual%20levels%3A%20instruction-history%20level%20and%0Alandmark-observation%20level%20according%20to%20their%20semantic%20correlations.%20We%20also%0Areconstruct%20a%20dual-level%20instruction%20for%20adaptation%20to%20the%20dual-level%0Aalignment.%20As%20the%20training%20signals%20for%20pre-fusion%20alignment%20are%20extremely%0Alimited%2C%20self-supervised%20contrastive%20learning%20strategies%20are%20employed%20to%0Aenforce%20the%20matching%20between%20different%20modalities.%20Our%20approach%20seamlessly%0Aintegrates%20with%20the%20majority%20of%20existing%20models%2C%20resulting%20in%20improved%0Anavigation%20performance%20on%20various%20VLN%20benchmarks%2C%20including%20R2R%2C%20R4R%2C%20RxR%20and%0ACVDN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01994v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DELAN%3A%20Dual-Level%20Alignment%20for%20Vision-and-Language%20Navigation%20by%0A%20%20Cross-Modal%20Contrastive%20Learning&entry.906535625=Mengfei%20Du%20and%20Binhao%20Wu%20and%20Jiwen%20Zhang%20and%20Zhihao%20Fan%20and%20Zejun%20Li%20and%20Ruipu%20Luo%20and%20Xuanjing%20Huang%20and%20Zhongyu%20Wei&entry.1292438233=%20%20Vision-and-Language%20navigation%20%28VLN%29%20requires%20an%20agent%20to%20navigate%20in%20unseen%0Aenvironment%20by%20following%20natural%20language%20instruction.%20For%20task%20completion%2C%20the%0Aagent%20needs%20to%20align%20and%20integrate%20various%20navigation%20modalities%2C%20including%0Ainstruction%2C%20observation%20and%20navigation%20history.%20Existing%20works%20primarily%0Aconcentrate%20on%20cross-modal%20attention%20at%20the%20fusion%20stage%20to%20achieve%20this%0Aobjective.%20Nevertheless%2C%20modality%20features%20generated%20by%20disparate%20uni-encoders%0Areside%20in%20their%20own%20spaces%2C%20leading%20to%20a%20decline%20in%20the%20quality%20of%20cross-modal%0Afusion%20and%20decision.%20To%20address%20this%20problem%2C%20we%20propose%20a%20Dual-levEL%20AligNment%0A%28DELAN%29%20framework%20by%20cross-modal%20contrastive%20learning.%20This%20framework%20is%0Adesigned%20to%20align%20various%20navigation-related%20modalities%20before%20fusion%2C%20thereby%0Aenhancing%20cross-modal%20interaction%20and%20action%20decision-making.%20Specifically%2C%20we%0Adivide%20the%20pre-fusion%20alignment%20into%20dual%20levels%3A%20instruction-history%20level%20and%0Alandmark-observation%20level%20according%20to%20their%20semantic%20correlations.%20We%20also%0Areconstruct%20a%20dual-level%20instruction%20for%20adaptation%20to%20the%20dual-level%0Aalignment.%20As%20the%20training%20signals%20for%20pre-fusion%20alignment%20are%20extremely%0Alimited%2C%20self-supervised%20contrastive%20learning%20strategies%20are%20employed%20to%0Aenforce%20the%20matching%20between%20different%20modalities.%20Our%20approach%20seamlessly%0Aintegrates%20with%20the%20majority%20of%20existing%20models%2C%20resulting%20in%20improved%0Anavigation%20performance%20on%20various%20VLN%20benchmarks%2C%20including%20R2R%2C%20R4R%2C%20RxR%20and%0ACVDN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01994v1&entry.124074799=Read"},
{"title": "A Simple Recipe for Language-guided Domain Generalized Segmentation", "author": "Mohammad Fahes and Tuan-Hung Vu and Andrei Bursuc and Patrick P\u00e9rez and Raoul de Charette", "abstract": "  Generalization to new domains not seen during training is one of the\nlong-standing challenges in deploying neural networks in real-world\napplications. Existing generalization techniques either necessitate external\nimages for augmentation, and/or aim at learning invariant representations by\nimposing various alignment constraints. Large-scale pretraining has recently\nshown promising generalization capabilities, along with the potential of\nbinding different modalities. For instance, the advent of vision-language\nmodels like CLIP has opened the doorway for vision models to exploit the\ntextual modality. In this paper, we introduce a simple framework for\ngeneralizing semantic segmentation networks by employing language as the source\nof randomization. Our recipe comprises three key ingredients: (i) the\npreservation of the intrinsic CLIP robustness through minimal fine-tuning, (ii)\nlanguage-driven local style augmentation, and (iii) randomization by locally\nmixing the source and augmented styles during training. Extensive experiments\nreport state-of-the-art results on various generalization benchmarks. Code is\naccessible at https://github.com/astra-vision/FAMix .\n", "link": "http://arxiv.org/abs/2311.17922v2", "date": "2024-04-02", "relevancy": 2.6817, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5595}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5251}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5244}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Recipe%20for%20Language-guided%20Domain%20Generalized%20Segmentation&body=Title%3A%20A%20Simple%20Recipe%20for%20Language-guided%20Domain%20Generalized%20Segmentation%0AAuthor%3A%20Mohammad%20Fahes%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Patrick%20P%C3%A9rez%20and%20Raoul%20de%20Charette%0AAbstract%3A%20%20%20Generalization%20to%20new%20domains%20not%20seen%20during%20training%20is%20one%20of%20the%0Along-standing%20challenges%20in%20deploying%20neural%20networks%20in%20real-world%0Aapplications.%20Existing%20generalization%20techniques%20either%20necessitate%20external%0Aimages%20for%20augmentation%2C%20and/or%20aim%20at%20learning%20invariant%20representations%20by%0Aimposing%20various%20alignment%20constraints.%20Large-scale%20pretraining%20has%20recently%0Ashown%20promising%20generalization%20capabilities%2C%20along%20with%20the%20potential%20of%0Abinding%20different%20modalities.%20For%20instance%2C%20the%20advent%20of%20vision-language%0Amodels%20like%20CLIP%20has%20opened%20the%20doorway%20for%20vision%20models%20to%20exploit%20the%0Atextual%20modality.%20In%20this%20paper%2C%20we%20introduce%20a%20simple%20framework%20for%0Ageneralizing%20semantic%20segmentation%20networks%20by%20employing%20language%20as%20the%20source%0Aof%20randomization.%20Our%20recipe%20comprises%20three%20key%20ingredients%3A%20%28i%29%20the%0Apreservation%20of%20the%20intrinsic%20CLIP%20robustness%20through%20minimal%20fine-tuning%2C%20%28ii%29%0Alanguage-driven%20local%20style%20augmentation%2C%20and%20%28iii%29%20randomization%20by%20locally%0Amixing%20the%20source%20and%20augmented%20styles%20during%20training.%20Extensive%20experiments%0Areport%20state-of-the-art%20results%20on%20various%20generalization%20benchmarks.%20Code%20is%0Aaccessible%20at%20https%3A//github.com/astra-vision/FAMix%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17922v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Recipe%20for%20Language-guided%20Domain%20Generalized%20Segmentation&entry.906535625=Mohammad%20Fahes%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Patrick%20P%C3%A9rez%20and%20Raoul%20de%20Charette&entry.1292438233=%20%20Generalization%20to%20new%20domains%20not%20seen%20during%20training%20is%20one%20of%20the%0Along-standing%20challenges%20in%20deploying%20neural%20networks%20in%20real-world%0Aapplications.%20Existing%20generalization%20techniques%20either%20necessitate%20external%0Aimages%20for%20augmentation%2C%20and/or%20aim%20at%20learning%20invariant%20representations%20by%0Aimposing%20various%20alignment%20constraints.%20Large-scale%20pretraining%20has%20recently%0Ashown%20promising%20generalization%20capabilities%2C%20along%20with%20the%20potential%20of%0Abinding%20different%20modalities.%20For%20instance%2C%20the%20advent%20of%20vision-language%0Amodels%20like%20CLIP%20has%20opened%20the%20doorway%20for%20vision%20models%20to%20exploit%20the%0Atextual%20modality.%20In%20this%20paper%2C%20we%20introduce%20a%20simple%20framework%20for%0Ageneralizing%20semantic%20segmentation%20networks%20by%20employing%20language%20as%20the%20source%0Aof%20randomization.%20Our%20recipe%20comprises%20three%20key%20ingredients%3A%20%28i%29%20the%0Apreservation%20of%20the%20intrinsic%20CLIP%20robustness%20through%20minimal%20fine-tuning%2C%20%28ii%29%0Alanguage-driven%20local%20style%20augmentation%2C%20and%20%28iii%29%20randomization%20by%20locally%0Amixing%20the%20source%20and%20augmented%20styles%20during%20training.%20Extensive%20experiments%0Areport%20state-of-the-art%20results%20on%20various%20generalization%20benchmarks.%20Code%20is%0Aaccessible%20at%20https%3A//github.com/astra-vision/FAMix%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17922v2&entry.124074799=Read"},
{"title": "UINav: A Practical Approach to Train On-Device Automation Agents", "author": "Wei Li and Fu-Lin Hsu and Will Bishop and Folawiyo Campbell-Ajala and Max Lin and Oriana Riva", "abstract": "  Automation systems that can autonomously drive application user interfaces to\ncomplete user tasks are of great benefit, especially when users are\nsituationally or permanently impaired. Prior automation systems do not produce\ngeneralizable models while AI-based automation agents work reliably only in\nsimple, hand-crafted applications or incur high computation costs. We propose\nUINav, a demonstration-based approach to train automation agents that fit\nmobile devices, yet achieving high success rates with modest numbers of\ndemonstrations. To reduce the demonstration overhead, UINav uses a referee\nmodel that provides users with immediate feedback on tasks where the agent\nfails, and automatically augments human demonstrations to increase diversity in\ntraining data. Our evaluation shows that with only 10 demonstrations UINav can\nachieve 70% accuracy, and that with enough demonstrations it can surpass 90%\naccuracy.\n", "link": "http://arxiv.org/abs/2312.10170v2", "date": "2024-04-02", "relevancy": 2.6502, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5552}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5274}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5074}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UINav%3A%20A%20Practical%20Approach%20to%20Train%20On-Device%20Automation%20Agents&body=Title%3A%20UINav%3A%20A%20Practical%20Approach%20to%20Train%20On-Device%20Automation%20Agents%0AAuthor%3A%20Wei%20Li%20and%20Fu-Lin%20Hsu%20and%20Will%20Bishop%20and%20Folawiyo%20Campbell-Ajala%20and%20Max%20Lin%20and%20Oriana%20Riva%0AAbstract%3A%20%20%20Automation%20systems%20that%20can%20autonomously%20drive%20application%20user%20interfaces%20to%0Acomplete%20user%20tasks%20are%20of%20great%20benefit%2C%20especially%20when%20users%20are%0Asituationally%20or%20permanently%20impaired.%20Prior%20automation%20systems%20do%20not%20produce%0Ageneralizable%20models%20while%20AI-based%20automation%20agents%20work%20reliably%20only%20in%0Asimple%2C%20hand-crafted%20applications%20or%20incur%20high%20computation%20costs.%20We%20propose%0AUINav%2C%20a%20demonstration-based%20approach%20to%20train%20automation%20agents%20that%20fit%0Amobile%20devices%2C%20yet%20achieving%20high%20success%20rates%20with%20modest%20numbers%20of%0Ademonstrations.%20To%20reduce%20the%20demonstration%20overhead%2C%20UINav%20uses%20a%20referee%0Amodel%20that%20provides%20users%20with%20immediate%20feedback%20on%20tasks%20where%20the%20agent%0Afails%2C%20and%20automatically%20augments%20human%20demonstrations%20to%20increase%20diversity%20in%0Atraining%20data.%20Our%20evaluation%20shows%20that%20with%20only%2010%20demonstrations%20UINav%20can%0Aachieve%2070%25%20accuracy%2C%20and%20that%20with%20enough%20demonstrations%20it%20can%20surpass%2090%25%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10170v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UINav%3A%20A%20Practical%20Approach%20to%20Train%20On-Device%20Automation%20Agents&entry.906535625=Wei%20Li%20and%20Fu-Lin%20Hsu%20and%20Will%20Bishop%20and%20Folawiyo%20Campbell-Ajala%20and%20Max%20Lin%20and%20Oriana%20Riva&entry.1292438233=%20%20Automation%20systems%20that%20can%20autonomously%20drive%20application%20user%20interfaces%20to%0Acomplete%20user%20tasks%20are%20of%20great%20benefit%2C%20especially%20when%20users%20are%0Asituationally%20or%20permanently%20impaired.%20Prior%20automation%20systems%20do%20not%20produce%0Ageneralizable%20models%20while%20AI-based%20automation%20agents%20work%20reliably%20only%20in%0Asimple%2C%20hand-crafted%20applications%20or%20incur%20high%20computation%20costs.%20We%20propose%0AUINav%2C%20a%20demonstration-based%20approach%20to%20train%20automation%20agents%20that%20fit%0Amobile%20devices%2C%20yet%20achieving%20high%20success%20rates%20with%20modest%20numbers%20of%0Ademonstrations.%20To%20reduce%20the%20demonstration%20overhead%2C%20UINav%20uses%20a%20referee%0Amodel%20that%20provides%20users%20with%20immediate%20feedback%20on%20tasks%20where%20the%20agent%0Afails%2C%20and%20automatically%20augments%20human%20demonstrations%20to%20increase%20diversity%20in%0Atraining%20data.%20Our%20evaluation%20shows%20that%20with%20only%2010%20demonstrations%20UINav%20can%0Aachieve%2070%25%20accuracy%2C%20and%20that%20with%20enough%20demonstrations%20it%20can%20surpass%2090%25%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10170v2&entry.124074799=Read"},
{"title": "Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of\n  Orthogonal Diffusion Models", "author": "Zeyu Yang and Zijie Pan and Chun Gu and Li Zhang", "abstract": "  Recent advancements in 3D generation are predominantly propelled by\nimprovements in 3D-aware image diffusion models which are pretrained on\nInternet-scale image data and fine-tuned on massive 3D data, offering the\ncapability of producing highly consistent multi-view images. However, due to\nthe scarcity of synchronized multi-view video data, it is impractical to adapt\nthis paradigm to 4D generation directly. Despite that, the available video and\n3D data are adequate for training video and multi-view diffusion models that\ncan provide satisfactory dynamic and geometric priors respectively. In this\npaper, we present Diffusion$^2$, a novel framework for dynamic 3D content\ncreation that leverages the knowledge about geometric consistency and temporal\nsmoothness from these models to directly sample dense multi-view and\nmulti-frame images which can be employed to optimize continuous 4D\nrepresentation. Specifically, we design a simple yet effective denoising\nstrategy via score composition of video and multi-view diffusion models based\non the probability structure of the images to be generated. Owing to the high\nparallelism of the image generation and the efficiency of the modern 4D\nreconstruction pipeline, our framework can generate 4D content within few\nminutes. Furthermore, our method circumvents the reliance on 4D data, thereby\nhaving the potential to benefit from the scalability of the foundation video\nand multi-view diffusion models. Extensive experiments demonstrate the efficacy\nof our proposed framework and its capability to flexibly adapt to various types\nof prompts.\n", "link": "http://arxiv.org/abs/2404.02148v1", "date": "2024-04-02", "relevancy": 2.6475, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6691}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6633}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6576}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Diffusion%24%5E2%24%3A%20Dynamic%203D%20Content%20Generation%20via%20Score%20Composition%20of%0A%20%20Orthogonal%20Diffusion%20Models&body=Title%3A%20Diffusion%24%5E2%24%3A%20Dynamic%203D%20Content%20Generation%20via%20Score%20Composition%20of%0A%20%20Orthogonal%20Diffusion%20Models%0AAuthor%3A%20Zeyu%20Yang%20and%20Zijie%20Pan%20and%20Chun%20Gu%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20generation%20are%20predominantly%20propelled%20by%0Aimprovements%20in%203D-aware%20image%20diffusion%20models%20which%20are%20pretrained%20on%0AInternet-scale%20image%20data%20and%20fine-tuned%20on%20massive%203D%20data%2C%20offering%20the%0Acapability%20of%20producing%20highly%20consistent%20multi-view%20images.%20However%2C%20due%20to%0Athe%20scarcity%20of%20synchronized%20multi-view%20video%20data%2C%20it%20is%20impractical%20to%20adapt%0Athis%20paradigm%20to%204D%20generation%20directly.%20Despite%20that%2C%20the%20available%20video%20and%0A3D%20data%20are%20adequate%20for%20training%20video%20and%20multi-view%20diffusion%20models%20that%0Acan%20provide%20satisfactory%20dynamic%20and%20geometric%20priors%20respectively.%20In%20this%0Apaper%2C%20we%20present%20Diffusion%24%5E2%24%2C%20a%20novel%20framework%20for%20dynamic%203D%20content%0Acreation%20that%20leverages%20the%20knowledge%20about%20geometric%20consistency%20and%20temporal%0Asmoothness%20from%20these%20models%20to%20directly%20sample%20dense%20multi-view%20and%0Amulti-frame%20images%20which%20can%20be%20employed%20to%20optimize%20continuous%204D%0Arepresentation.%20Specifically%2C%20we%20design%20a%20simple%20yet%20effective%20denoising%0Astrategy%20via%20score%20composition%20of%20video%20and%20multi-view%20diffusion%20models%20based%0Aon%20the%20probability%20structure%20of%20the%20images%20to%20be%20generated.%20Owing%20to%20the%20high%0Aparallelism%20of%20the%20image%20generation%20and%20the%20efficiency%20of%20the%20modern%204D%0Areconstruction%20pipeline%2C%20our%20framework%20can%20generate%204D%20content%20within%20few%0Aminutes.%20Furthermore%2C%20our%20method%20circumvents%20the%20reliance%20on%204D%20data%2C%20thereby%0Ahaving%20the%20potential%20to%20benefit%20from%20the%20scalability%20of%20the%20foundation%20video%0Aand%20multi-view%20diffusion%20models.%20Extensive%20experiments%20demonstrate%20the%20efficacy%0Aof%20our%20proposed%20framework%20and%20its%20capability%20to%20flexibly%20adapt%20to%20various%20types%0Aof%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02148v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%24%5E2%24%3A%20Dynamic%203D%20Content%20Generation%20via%20Score%20Composition%20of%0A%20%20Orthogonal%20Diffusion%20Models&entry.906535625=Zeyu%20Yang%20and%20Zijie%20Pan%20and%20Chun%20Gu%20and%20Li%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%203D%20generation%20are%20predominantly%20propelled%20by%0Aimprovements%20in%203D-aware%20image%20diffusion%20models%20which%20are%20pretrained%20on%0AInternet-scale%20image%20data%20and%20fine-tuned%20on%20massive%203D%20data%2C%20offering%20the%0Acapability%20of%20producing%20highly%20consistent%20multi-view%20images.%20However%2C%20due%20to%0Athe%20scarcity%20of%20synchronized%20multi-view%20video%20data%2C%20it%20is%20impractical%20to%20adapt%0Athis%20paradigm%20to%204D%20generation%20directly.%20Despite%20that%2C%20the%20available%20video%20and%0A3D%20data%20are%20adequate%20for%20training%20video%20and%20multi-view%20diffusion%20models%20that%0Acan%20provide%20satisfactory%20dynamic%20and%20geometric%20priors%20respectively.%20In%20this%0Apaper%2C%20we%20present%20Diffusion%24%5E2%24%2C%20a%20novel%20framework%20for%20dynamic%203D%20content%0Acreation%20that%20leverages%20the%20knowledge%20about%20geometric%20consistency%20and%20temporal%0Asmoothness%20from%20these%20models%20to%20directly%20sample%20dense%20multi-view%20and%0Amulti-frame%20images%20which%20can%20be%20employed%20to%20optimize%20continuous%204D%0Arepresentation.%20Specifically%2C%20we%20design%20a%20simple%20yet%20effective%20denoising%0Astrategy%20via%20score%20composition%20of%20video%20and%20multi-view%20diffusion%20models%20based%0Aon%20the%20probability%20structure%20of%20the%20images%20to%20be%20generated.%20Owing%20to%20the%20high%0Aparallelism%20of%20the%20image%20generation%20and%20the%20efficiency%20of%20the%20modern%204D%0Areconstruction%20pipeline%2C%20our%20framework%20can%20generate%204D%20content%20within%20few%0Aminutes.%20Furthermore%2C%20our%20method%20circumvents%20the%20reliance%20on%204D%20data%2C%20thereby%0Ahaving%20the%20potential%20to%20benefit%20from%20the%20scalability%20of%20the%20foundation%20video%0Aand%20multi-view%20diffusion%20models.%20Extensive%20experiments%20demonstrate%20the%20efficacy%0Aof%20our%20proposed%20framework%20and%20its%20capability%20to%20flexibly%20adapt%20to%20various%20types%0Aof%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02148v1&entry.124074799=Read"},
{"title": "Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific\n  Boundaries for Domain Adaptation", "author": "Ba Hung Ngo and Nhat-Tuong Do-Tran and Tuan-Ngoc Nguyen and Hae-Gon Jeon and Tae Jong Choi", "abstract": "  Most domain adaptation (DA) methods are based on either a convolutional\nneural networks (CNNs) or a vision transformers (ViTs). They align the\ndistribution differences between domains as encoders without considering their\nunique characteristics. For instance, ViT excels in accuracy due to its\nsuperior ability to capture global representations, while CNN has an advantage\nin capturing local representations. This fact has led us to design a hybrid\nmethod to fully take advantage of both ViT and CNN, called Explicitly\nClass-specific Boundaries (ECB). ECB learns CNN on ViT to combine their\ndistinct strengths. In particular, we leverage ViT's properties to explicitly\nfind class-specific decision boundaries by maximizing the discrepancy between\nthe outputs of the two classifiers to detect target samples far from the source\nsupport. In contrast, the CNN encoder clusters target features based on the\npreviously defined class-specific boundaries by minimizing the discrepancy\nbetween the probabilities of the two classifiers. Finally, ViT and CNN mutually\nexchange knowledge to improve the quality of pseudo labels and reduce the\nknowledge discrepancies of these models. Compared to conventional DA methods,\nour ECB achieves superior performance, which verifies its effectiveness in this\nhybrid model. The project website can be found\nhttps://dotrannhattuong.github.io/ECB/website/.\n", "link": "http://arxiv.org/abs/2403.18360v2", "date": "2024-04-02", "relevancy": 2.6281, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5399}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5237}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5133}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20CNN%20on%20ViT%3A%20A%20Hybrid%20Model%20to%20Explicitly%20Class-specific%0A%20%20Boundaries%20for%20Domain%20Adaptation&body=Title%3A%20Learning%20CNN%20on%20ViT%3A%20A%20Hybrid%20Model%20to%20Explicitly%20Class-specific%0A%20%20Boundaries%20for%20Domain%20Adaptation%0AAuthor%3A%20Ba%20Hung%20Ngo%20and%20Nhat-Tuong%20Do-Tran%20and%20Tuan-Ngoc%20Nguyen%20and%20Hae-Gon%20Jeon%20and%20Tae%20Jong%20Choi%0AAbstract%3A%20%20%20Most%20domain%20adaptation%20%28DA%29%20methods%20are%20based%20on%20either%20a%20convolutional%0Aneural%20networks%20%28CNNs%29%20or%20a%20vision%20transformers%20%28ViTs%29.%20They%20align%20the%0Adistribution%20differences%20between%20domains%20as%20encoders%20without%20considering%20their%0Aunique%20characteristics.%20For%20instance%2C%20ViT%20excels%20in%20accuracy%20due%20to%20its%0Asuperior%20ability%20to%20capture%20global%20representations%2C%20while%20CNN%20has%20an%20advantage%0Ain%20capturing%20local%20representations.%20This%20fact%20has%20led%20us%20to%20design%20a%20hybrid%0Amethod%20to%20fully%20take%20advantage%20of%20both%20ViT%20and%20CNN%2C%20called%20Explicitly%0AClass-specific%20Boundaries%20%28ECB%29.%20ECB%20learns%20CNN%20on%20ViT%20to%20combine%20their%0Adistinct%20strengths.%20In%20particular%2C%20we%20leverage%20ViT%27s%20properties%20to%20explicitly%0Afind%20class-specific%20decision%20boundaries%20by%20maximizing%20the%20discrepancy%20between%0Athe%20outputs%20of%20the%20two%20classifiers%20to%20detect%20target%20samples%20far%20from%20the%20source%0Asupport.%20In%20contrast%2C%20the%20CNN%20encoder%20clusters%20target%20features%20based%20on%20the%0Apreviously%20defined%20class-specific%20boundaries%20by%20minimizing%20the%20discrepancy%0Abetween%20the%20probabilities%20of%20the%20two%20classifiers.%20Finally%2C%20ViT%20and%20CNN%20mutually%0Aexchange%20knowledge%20to%20improve%20the%20quality%20of%20pseudo%20labels%20and%20reduce%20the%0Aknowledge%20discrepancies%20of%20these%20models.%20Compared%20to%20conventional%20DA%20methods%2C%0Aour%20ECB%20achieves%20superior%20performance%2C%20which%20verifies%20its%20effectiveness%20in%20this%0Ahybrid%20model.%20The%20project%20website%20can%20be%20found%0Ahttps%3A//dotrannhattuong.github.io/ECB/website/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18360v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20CNN%20on%20ViT%3A%20A%20Hybrid%20Model%20to%20Explicitly%20Class-specific%0A%20%20Boundaries%20for%20Domain%20Adaptation&entry.906535625=Ba%20Hung%20Ngo%20and%20Nhat-Tuong%20Do-Tran%20and%20Tuan-Ngoc%20Nguyen%20and%20Hae-Gon%20Jeon%20and%20Tae%20Jong%20Choi&entry.1292438233=%20%20Most%20domain%20adaptation%20%28DA%29%20methods%20are%20based%20on%20either%20a%20convolutional%0Aneural%20networks%20%28CNNs%29%20or%20a%20vision%20transformers%20%28ViTs%29.%20They%20align%20the%0Adistribution%20differences%20between%20domains%20as%20encoders%20without%20considering%20their%0Aunique%20characteristics.%20For%20instance%2C%20ViT%20excels%20in%20accuracy%20due%20to%20its%0Asuperior%20ability%20to%20capture%20global%20representations%2C%20while%20CNN%20has%20an%20advantage%0Ain%20capturing%20local%20representations.%20This%20fact%20has%20led%20us%20to%20design%20a%20hybrid%0Amethod%20to%20fully%20take%20advantage%20of%20both%20ViT%20and%20CNN%2C%20called%20Explicitly%0AClass-specific%20Boundaries%20%28ECB%29.%20ECB%20learns%20CNN%20on%20ViT%20to%20combine%20their%0Adistinct%20strengths.%20In%20particular%2C%20we%20leverage%20ViT%27s%20properties%20to%20explicitly%0Afind%20class-specific%20decision%20boundaries%20by%20maximizing%20the%20discrepancy%20between%0Athe%20outputs%20of%20the%20two%20classifiers%20to%20detect%20target%20samples%20far%20from%20the%20source%0Asupport.%20In%20contrast%2C%20the%20CNN%20encoder%20clusters%20target%20features%20based%20on%20the%0Apreviously%20defined%20class-specific%20boundaries%20by%20minimizing%20the%20discrepancy%0Abetween%20the%20probabilities%20of%20the%20two%20classifiers.%20Finally%2C%20ViT%20and%20CNN%20mutually%0Aexchange%20knowledge%20to%20improve%20the%20quality%20of%20pseudo%20labels%20and%20reduce%20the%0Aknowledge%20discrepancies%20of%20these%20models.%20Compared%20to%20conventional%20DA%20methods%2C%0Aour%20ECB%20achieves%20superior%20performance%2C%20which%20verifies%20its%20effectiveness%20in%20this%0Ahybrid%20model.%20The%20project%20website%20can%20be%20found%0Ahttps%3A//dotrannhattuong.github.io/ECB/website/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18360v2&entry.124074799=Read"},
{"title": "ResNet with Integrated Convolutional Block Attention Module for Ship\n  Classification Using Transfer Learning on Optical Satellite Imagery", "author": "Ryan Donghan Kwon and Gangjoo Robin Nam and Jisoo Tak and Yeom Hyeok and Junseob Shin and Hyerin Cha and Kim Soo Bin", "abstract": "  This study proposes a novel transfer learning framework for effective ship\nclassification using high-resolution optical remote sensing satellite imagery.\nThe framework is based on the deep convolutional neural network model ResNet50\nand incorporates the Convolutional Block Attention Module (CBAM) to enhance\nperformance. CBAM enables the model to attend to salient features in the\nimages, allowing it to better discriminate between subtle differences between\nships and backgrounds. Furthermore, this study adopts a transfer learning\napproach tailored for accurately classifying diverse types of ships by\nfine-tuning a pre-trained model for the specific task. Experimental results\ndemonstrate the efficacy of the proposed framework in ship classification using\noptical remote sensing imagery, achieving a high classification accuracy of 94%\nacross 5 classes, outperforming existing methods. This research holds potential\napplications in maritime surveillance and management, illegal fishing\ndetection, and maritime traffic monitoring.\n", "link": "http://arxiv.org/abs/2404.02135v1", "date": "2024-04-02", "relevancy": 2.5671, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4951}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ResNet%20with%20Integrated%20Convolutional%20Block%20Attention%20Module%20for%20Ship%0A%20%20Classification%20Using%20Transfer%20Learning%20on%20Optical%20Satellite%20Imagery&body=Title%3A%20ResNet%20with%20Integrated%20Convolutional%20Block%20Attention%20Module%20for%20Ship%0A%20%20Classification%20Using%20Transfer%20Learning%20on%20Optical%20Satellite%20Imagery%0AAuthor%3A%20Ryan%20Donghan%20Kwon%20and%20Gangjoo%20Robin%20Nam%20and%20Jisoo%20Tak%20and%20Yeom%20Hyeok%20and%20Junseob%20Shin%20and%20Hyerin%20Cha%20and%20Kim%20Soo%20Bin%0AAbstract%3A%20%20%20This%20study%20proposes%20a%20novel%20transfer%20learning%20framework%20for%20effective%20ship%0Aclassification%20using%20high-resolution%20optical%20remote%20sensing%20satellite%20imagery.%0AThe%20framework%20is%20based%20on%20the%20deep%20convolutional%20neural%20network%20model%20ResNet50%0Aand%20incorporates%20the%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20to%20enhance%0Aperformance.%20CBAM%20enables%20the%20model%20to%20attend%20to%20salient%20features%20in%20the%0Aimages%2C%20allowing%20it%20to%20better%20discriminate%20between%20subtle%20differences%20between%0Aships%20and%20backgrounds.%20Furthermore%2C%20this%20study%20adopts%20a%20transfer%20learning%0Aapproach%20tailored%20for%20accurately%20classifying%20diverse%20types%20of%20ships%20by%0Afine-tuning%20a%20pre-trained%20model%20for%20the%20specific%20task.%20Experimental%20results%0Ademonstrate%20the%20efficacy%20of%20the%20proposed%20framework%20in%20ship%20classification%20using%0Aoptical%20remote%20sensing%20imagery%2C%20achieving%20a%20high%20classification%20accuracy%20of%2094%25%0Aacross%205%20classes%2C%20outperforming%20existing%20methods.%20This%20research%20holds%20potential%0Aapplications%20in%20maritime%20surveillance%20and%20management%2C%20illegal%20fishing%0Adetection%2C%20and%20maritime%20traffic%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02135v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResNet%20with%20Integrated%20Convolutional%20Block%20Attention%20Module%20for%20Ship%0A%20%20Classification%20Using%20Transfer%20Learning%20on%20Optical%20Satellite%20Imagery&entry.906535625=Ryan%20Donghan%20Kwon%20and%20Gangjoo%20Robin%20Nam%20and%20Jisoo%20Tak%20and%20Yeom%20Hyeok%20and%20Junseob%20Shin%20and%20Hyerin%20Cha%20and%20Kim%20Soo%20Bin&entry.1292438233=%20%20This%20study%20proposes%20a%20novel%20transfer%20learning%20framework%20for%20effective%20ship%0Aclassification%20using%20high-resolution%20optical%20remote%20sensing%20satellite%20imagery.%0AThe%20framework%20is%20based%20on%20the%20deep%20convolutional%20neural%20network%20model%20ResNet50%0Aand%20incorporates%20the%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20to%20enhance%0Aperformance.%20CBAM%20enables%20the%20model%20to%20attend%20to%20salient%20features%20in%20the%0Aimages%2C%20allowing%20it%20to%20better%20discriminate%20between%20subtle%20differences%20between%0Aships%20and%20backgrounds.%20Furthermore%2C%20this%20study%20adopts%20a%20transfer%20learning%0Aapproach%20tailored%20for%20accurately%20classifying%20diverse%20types%20of%20ships%20by%0Afine-tuning%20a%20pre-trained%20model%20for%20the%20specific%20task.%20Experimental%20results%0Ademonstrate%20the%20efficacy%20of%20the%20proposed%20framework%20in%20ship%20classification%20using%0Aoptical%20remote%20sensing%20imagery%2C%20achieving%20a%20high%20classification%20accuracy%20of%2094%25%0Aacross%205%20classes%2C%20outperforming%20existing%20methods.%20This%20research%20holds%20potential%0Aapplications%20in%20maritime%20surveillance%20and%20management%2C%20illegal%20fishing%0Adetection%2C%20and%20maritime%20traffic%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02135v1&entry.124074799=Read"},
{"title": "Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on\n  Unseen Fundus Images", "author": "Jiyuan Zhong and Hu Ke and Ming Yan", "abstract": "  Fundus image segmentation on unseen domains is challenging, especially for\nthe over-parameterized deep models trained on the small medical datasets. To\naddress this challenge, we propose a method named Adaptive Feature-fusion\nNeural Network (AFNN) for glaucoma segmentation on unseen domains, which mainly\nconsists of three modules: domain adaptor, feature-fusion network, and\nself-supervised multi-task learning. Specifically, the domain adaptor helps the\npretrained-model fast adapt from other image domains to the medical fundus\nimage domain. Feature-fusion network and self-supervised multi-task learning\nfor the encoder and decoder are introduced to improve the domain generalization\nability. In addition, we also design the weighted-dice-loss to improve model\nperformance on complex optic-cup segmentation tasks. Our proposed method\nachieves a competitive performance over existing fundus segmentation methods on\nfour public glaucoma datasets.\n", "link": "http://arxiv.org/abs/2404.02084v1", "date": "2024-04-02", "relevancy": 2.5425, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5093}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5023}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Feature%20Fusion%20Neural%20Network%20for%20Glaucoma%20Segmentation%20on%0A%20%20Unseen%20Fundus%20Images&body=Title%3A%20Adaptive%20Feature%20Fusion%20Neural%20Network%20for%20Glaucoma%20Segmentation%20on%0A%20%20Unseen%20Fundus%20Images%0AAuthor%3A%20Jiyuan%20Zhong%20and%20Hu%20Ke%20and%20Ming%20Yan%0AAbstract%3A%20%20%20Fundus%20image%20segmentation%20on%20unseen%20domains%20is%20challenging%2C%20especially%20for%0Athe%20over-parameterized%20deep%20models%20trained%20on%20the%20small%20medical%20datasets.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20method%20named%20Adaptive%20Feature-fusion%0ANeural%20Network%20%28AFNN%29%20for%20glaucoma%20segmentation%20on%20unseen%20domains%2C%20which%20mainly%0Aconsists%20of%20three%20modules%3A%20domain%20adaptor%2C%20feature-fusion%20network%2C%20and%0Aself-supervised%20multi-task%20learning.%20Specifically%2C%20the%20domain%20adaptor%20helps%20the%0Apretrained-model%20fast%20adapt%20from%20other%20image%20domains%20to%20the%20medical%20fundus%0Aimage%20domain.%20Feature-fusion%20network%20and%20self-supervised%20multi-task%20learning%0Afor%20the%20encoder%20and%20decoder%20are%20introduced%20to%20improve%20the%20domain%20generalization%0Aability.%20In%20addition%2C%20we%20also%20design%20the%20weighted-dice-loss%20to%20improve%20model%0Aperformance%20on%20complex%20optic-cup%20segmentation%20tasks.%20Our%20proposed%20method%0Aachieves%20a%20competitive%20performance%20over%20existing%20fundus%20segmentation%20methods%20on%0Afour%20public%20glaucoma%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02084v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Feature%20Fusion%20Neural%20Network%20for%20Glaucoma%20Segmentation%20on%0A%20%20Unseen%20Fundus%20Images&entry.906535625=Jiyuan%20Zhong%20and%20Hu%20Ke%20and%20Ming%20Yan&entry.1292438233=%20%20Fundus%20image%20segmentation%20on%20unseen%20domains%20is%20challenging%2C%20especially%20for%0Athe%20over-parameterized%20deep%20models%20trained%20on%20the%20small%20medical%20datasets.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20method%20named%20Adaptive%20Feature-fusion%0ANeural%20Network%20%28AFNN%29%20for%20glaucoma%20segmentation%20on%20unseen%20domains%2C%20which%20mainly%0Aconsists%20of%20three%20modules%3A%20domain%20adaptor%2C%20feature-fusion%20network%2C%20and%0Aself-supervised%20multi-task%20learning.%20Specifically%2C%20the%20domain%20adaptor%20helps%20the%0Apretrained-model%20fast%20adapt%20from%20other%20image%20domains%20to%20the%20medical%20fundus%0Aimage%20domain.%20Feature-fusion%20network%20and%20self-supervised%20multi-task%20learning%0Afor%20the%20encoder%20and%20decoder%20are%20introduced%20to%20improve%20the%20domain%20generalization%0Aability.%20In%20addition%2C%20we%20also%20design%20the%20weighted-dice-loss%20to%20improve%20model%0Aperformance%20on%20complex%20optic-cup%20segmentation%20tasks.%20Our%20proposed%20method%0Aachieves%20a%20competitive%20performance%20over%20existing%20fundus%20segmentation%20methods%20on%0Afour%20public%20glaucoma%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02084v1&entry.124074799=Read"},
{"title": "BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory\n  Speech Recognition", "author": "Alexandros Haliassos and Andreas Zinonos and Rodrigo Mira and Stavros Petridis and Maja Pantic", "abstract": "  Self-supervision has recently shown great promise for learning visual and\nauditory speech representations from unlabelled data. In this work, we propose\nBRAVEn, an extension to the recent RAVEn method, which learns speech\nrepresentations entirely from raw audio-visual data. Our modifications to RAVEn\nenable BRAVEn to achieve state-of-the-art results among self-supervised methods\nin various settings. Moreover, we observe favourable scaling behaviour by\nincreasing the amount of unlabelled data well beyond other self-supervised\nworks. In particular, we achieve 20.0% / 1.7% word error rate for VSR / ASR on\nthe LRS3 test set, with only 30 hours of labelled data and no external ASR\nmodels. Our results suggest that readily available unlabelled audio-visual data\ncan largely replace costly transcribed data.\n", "link": "http://arxiv.org/abs/2404.02098v1", "date": "2024-04-02", "relevancy": 2.4808, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5018}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4941}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4926}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BRAVEn%3A%20Improving%20Self-Supervised%20Pre-training%20for%20Visual%20and%20Auditory%0A%20%20Speech%20Recognition&body=Title%3A%20BRAVEn%3A%20Improving%20Self-Supervised%20Pre-training%20for%20Visual%20and%20Auditory%0A%20%20Speech%20Recognition%0AAuthor%3A%20Alexandros%20Haliassos%20and%20Andreas%20Zinonos%20and%20Rodrigo%20Mira%20and%20Stavros%20Petridis%20and%20Maja%20Pantic%0AAbstract%3A%20%20%20Self-supervision%20has%20recently%20shown%20great%20promise%20for%20learning%20visual%20and%0Aauditory%20speech%20representations%20from%20unlabelled%20data.%20In%20this%20work%2C%20we%20propose%0ABRAVEn%2C%20an%20extension%20to%20the%20recent%20RAVEn%20method%2C%20which%20learns%20speech%0Arepresentations%20entirely%20from%20raw%20audio-visual%20data.%20Our%20modifications%20to%20RAVEn%0Aenable%20BRAVEn%20to%20achieve%20state-of-the-art%20results%20among%20self-supervised%20methods%0Ain%20various%20settings.%20Moreover%2C%20we%20observe%20favourable%20scaling%20behaviour%20by%0Aincreasing%20the%20amount%20of%20unlabelled%20data%20well%20beyond%20other%20self-supervised%0Aworks.%20In%20particular%2C%20we%20achieve%2020.0%25%20/%201.7%25%20word%20error%20rate%20for%20VSR%20/%20ASR%20on%0Athe%20LRS3%20test%20set%2C%20with%20only%2030%20hours%20of%20labelled%20data%20and%20no%20external%20ASR%0Amodels.%20Our%20results%20suggest%20that%20readily%20available%20unlabelled%20audio-visual%20data%0Acan%20largely%20replace%20costly%20transcribed%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02098v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRAVEn%3A%20Improving%20Self-Supervised%20Pre-training%20for%20Visual%20and%20Auditory%0A%20%20Speech%20Recognition&entry.906535625=Alexandros%20Haliassos%20and%20Andreas%20Zinonos%20and%20Rodrigo%20Mira%20and%20Stavros%20Petridis%20and%20Maja%20Pantic&entry.1292438233=%20%20Self-supervision%20has%20recently%20shown%20great%20promise%20for%20learning%20visual%20and%0Aauditory%20speech%20representations%20from%20unlabelled%20data.%20In%20this%20work%2C%20we%20propose%0ABRAVEn%2C%20an%20extension%20to%20the%20recent%20RAVEn%20method%2C%20which%20learns%20speech%0Arepresentations%20entirely%20from%20raw%20audio-visual%20data.%20Our%20modifications%20to%20RAVEn%0Aenable%20BRAVEn%20to%20achieve%20state-of-the-art%20results%20among%20self-supervised%20methods%0Ain%20various%20settings.%20Moreover%2C%20we%20observe%20favourable%20scaling%20behaviour%20by%0Aincreasing%20the%20amount%20of%20unlabelled%20data%20well%20beyond%20other%20self-supervised%0Aworks.%20In%20particular%2C%20we%20achieve%2020.0%25%20/%201.7%25%20word%20error%20rate%20for%20VSR%20/%20ASR%20on%0Athe%20LRS3%20test%20set%2C%20with%20only%2030%20hours%20of%20labelled%20data%20and%20no%20external%20ASR%0Amodels.%20Our%20results%20suggest%20that%20readily%20available%20unlabelled%20audio-visual%20data%0Acan%20largely%20replace%20costly%20transcribed%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02098v1&entry.124074799=Read"},
{"title": "3D Congealing: 3D-Aware Image Alignment in the Wild", "author": "Yunzhi Zhang and Zizhang Li and Amit Raj and Andreas Engelhardt and Yuanzhen Li and Tingbo Hou and Jiajun Wu and Varun Jampani", "abstract": "  We propose 3D Congealing, a novel problem of 3D-aware alignment for 2D images\ncapturing semantically similar objects. Given a collection of unlabeled\nInternet images, our goal is to associate the shared semantic parts from the\ninputs and aggregate the knowledge from 2D images to a shared 3D canonical\nspace. We introduce a general framework that tackles the task without assuming\nshape templates, poses, or any camera parameters. At its core is a canonical 3D\nrepresentation that encapsulates geometric and semantic information. The\nframework optimizes for the canonical representation together with the pose for\neach input image, and a per-image coordinate map that warps 2D pixel\ncoordinates to the 3D canonical frame to account for the shape matching. The\noptimization procedure fuses prior knowledge from a pre-trained image\ngenerative model and semantic information from input images. The former\nprovides strong knowledge guidance for this under-constraint task, while the\nlatter provides the necessary information to mitigate the training data bias\nfrom the pre-trained model. Our framework can be used for various tasks such as\ncorrespondence matching, pose estimation, and image editing, achieving strong\nresults on real-world image datasets under challenging illumination conditions\nand on in-the-wild online image collections.\n", "link": "http://arxiv.org/abs/2404.02125v1", "date": "2024-04-02", "relevancy": 2.397, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6292}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5831}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5647}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D%20Congealing%3A%203D-Aware%20Image%20Alignment%20in%20the%20Wild&body=Title%3A%203D%20Congealing%3A%203D-Aware%20Image%20Alignment%20in%20the%20Wild%0AAuthor%3A%20Yunzhi%20Zhang%20and%20Zizhang%20Li%20and%20Amit%20Raj%20and%20Andreas%20Engelhardt%20and%20Yuanzhen%20Li%20and%20Tingbo%20Hou%20and%20Jiajun%20Wu%20and%20Varun%20Jampani%0AAbstract%3A%20%20%20We%20propose%203D%20Congealing%2C%20a%20novel%20problem%20of%203D-aware%20alignment%20for%202D%20images%0Acapturing%20semantically%20similar%20objects.%20Given%20a%20collection%20of%20unlabeled%0AInternet%20images%2C%20our%20goal%20is%20to%20associate%20the%20shared%20semantic%20parts%20from%20the%0Ainputs%20and%20aggregate%20the%20knowledge%20from%202D%20images%20to%20a%20shared%203D%20canonical%0Aspace.%20We%20introduce%20a%20general%20framework%20that%20tackles%20the%20task%20without%20assuming%0Ashape%20templates%2C%20poses%2C%20or%20any%20camera%20parameters.%20At%20its%20core%20is%20a%20canonical%203D%0Arepresentation%20that%20encapsulates%20geometric%20and%20semantic%20information.%20The%0Aframework%20optimizes%20for%20the%20canonical%20representation%20together%20with%20the%20pose%20for%0Aeach%20input%20image%2C%20and%20a%20per-image%20coordinate%20map%20that%20warps%202D%20pixel%0Acoordinates%20to%20the%203D%20canonical%20frame%20to%20account%20for%20the%20shape%20matching.%20The%0Aoptimization%20procedure%20fuses%20prior%20knowledge%20from%20a%20pre-trained%20image%0Agenerative%20model%20and%20semantic%20information%20from%20input%20images.%20The%20former%0Aprovides%20strong%20knowledge%20guidance%20for%20this%20under-constraint%20task%2C%20while%20the%0Alatter%20provides%20the%20necessary%20information%20to%20mitigate%20the%20training%20data%20bias%0Afrom%20the%20pre-trained%20model.%20Our%20framework%20can%20be%20used%20for%20various%20tasks%20such%20as%0Acorrespondence%20matching%2C%20pose%20estimation%2C%20and%20image%20editing%2C%20achieving%20strong%0Aresults%20on%20real-world%20image%20datasets%20under%20challenging%20illumination%20conditions%0Aand%20on%20in-the-wild%20online%20image%20collections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02125v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Congealing%3A%203D-Aware%20Image%20Alignment%20in%20the%20Wild&entry.906535625=Yunzhi%20Zhang%20and%20Zizhang%20Li%20and%20Amit%20Raj%20and%20Andreas%20Engelhardt%20and%20Yuanzhen%20Li%20and%20Tingbo%20Hou%20and%20Jiajun%20Wu%20and%20Varun%20Jampani&entry.1292438233=%20%20We%20propose%203D%20Congealing%2C%20a%20novel%20problem%20of%203D-aware%20alignment%20for%202D%20images%0Acapturing%20semantically%20similar%20objects.%20Given%20a%20collection%20of%20unlabeled%0AInternet%20images%2C%20our%20goal%20is%20to%20associate%20the%20shared%20semantic%20parts%20from%20the%0Ainputs%20and%20aggregate%20the%20knowledge%20from%202D%20images%20to%20a%20shared%203D%20canonical%0Aspace.%20We%20introduce%20a%20general%20framework%20that%20tackles%20the%20task%20without%20assuming%0Ashape%20templates%2C%20poses%2C%20or%20any%20camera%20parameters.%20At%20its%20core%20is%20a%20canonical%203D%0Arepresentation%20that%20encapsulates%20geometric%20and%20semantic%20information.%20The%0Aframework%20optimizes%20for%20the%20canonical%20representation%20together%20with%20the%20pose%20for%0Aeach%20input%20image%2C%20and%20a%20per-image%20coordinate%20map%20that%20warps%202D%20pixel%0Acoordinates%20to%20the%203D%20canonical%20frame%20to%20account%20for%20the%20shape%20matching.%20The%0Aoptimization%20procedure%20fuses%20prior%20knowledge%20from%20a%20pre-trained%20image%0Agenerative%20model%20and%20semantic%20information%20from%20input%20images.%20The%20former%0Aprovides%20strong%20knowledge%20guidance%20for%20this%20under-constraint%20task%2C%20while%20the%0Alatter%20provides%20the%20necessary%20information%20to%20mitigate%20the%20training%20data%20bias%0Afrom%20the%20pre-trained%20model.%20Our%20framework%20can%20be%20used%20for%20various%20tasks%20such%20as%0Acorrespondence%20matching%2C%20pose%20estimation%2C%20and%20image%20editing%2C%20achieving%20strong%0Aresults%20on%20real-world%20image%20datasets%20under%20challenging%20illumination%20conditions%0Aand%20on%20in-the-wild%20online%20image%20collections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02125v1&entry.124074799=Read"},
{"title": "SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation", "author": "Vinkle Srivastav and Keqi Chen and Nicolas Padoy", "abstract": "  We present a new self-supervised approach, SelfPose3d, for estimating 3d\nposes of multiple persons from multiple camera views. Unlike current\nstate-of-the-art fully-supervised methods, our approach does not require any 2d\nor 3d ground-truth poses and uses only the multi-view input images from a\ncalibrated camera setup and 2d pseudo poses generated from an off-the-shelf 2d\nhuman pose estimator. We propose two self-supervised learning objectives:\nself-supervised person localization in 3d space and self-supervised 3d pose\nestimation. We achieve self-supervised 3d person localization by training the\nmodel on synthetically generated 3d points, serving as 3d person root\npositions, and on the projected root-heatmaps in all the views. We then model\nthe 3d poses of all the localized persons with a bottleneck representation, map\nthem onto all views obtaining 2d joints, and render them using 2d Gaussian\nheatmaps in an end-to-end differentiable manner. Afterwards, we use the\ncorresponding 2d joints and heatmaps from the pseudo 2d poses for learning. To\nalleviate the intrinsic inaccuracy of the pseudo labels, we propose an adaptive\nsupervision attention mechanism to guide the self-supervision. Our experiments\nand analysis on three public benchmark datasets, including Panoptic, Shelf, and\nCampus, show the effectiveness of our approach, which is comparable to\nfully-supervised methods. Code is available at\n\\url{https://github.com/CAMMA-public/SelfPose3D}\n", "link": "http://arxiv.org/abs/2404.02041v1", "date": "2024-04-02", "relevancy": 2.3684, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6193}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5735}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5723}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SelfPose3d%3A%20Self-Supervised%20Multi-Person%20Multi-View%203d%20Pose%20Estimation&body=Title%3A%20SelfPose3d%3A%20Self-Supervised%20Multi-Person%20Multi-View%203d%20Pose%20Estimation%0AAuthor%3A%20Vinkle%20Srivastav%20and%20Keqi%20Chen%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20We%20present%20a%20new%20self-supervised%20approach%2C%20SelfPose3d%2C%20for%20estimating%203d%0Aposes%20of%20multiple%20persons%20from%20multiple%20camera%20views.%20Unlike%20current%0Astate-of-the-art%20fully-supervised%20methods%2C%20our%20approach%20does%20not%20require%20any%202d%0Aor%203d%20ground-truth%20poses%20and%20uses%20only%20the%20multi-view%20input%20images%20from%20a%0Acalibrated%20camera%20setup%20and%202d%20pseudo%20poses%20generated%20from%20an%20off-the-shelf%202d%0Ahuman%20pose%20estimator.%20We%20propose%20two%20self-supervised%20learning%20objectives%3A%0Aself-supervised%20person%20localization%20in%203d%20space%20and%20self-supervised%203d%20pose%0Aestimation.%20We%20achieve%20self-supervised%203d%20person%20localization%20by%20training%20the%0Amodel%20on%20synthetically%20generated%203d%20points%2C%20serving%20as%203d%20person%20root%0Apositions%2C%20and%20on%20the%20projected%20root-heatmaps%20in%20all%20the%20views.%20We%20then%20model%0Athe%203d%20poses%20of%20all%20the%20localized%20persons%20with%20a%20bottleneck%20representation%2C%20map%0Athem%20onto%20all%20views%20obtaining%202d%20joints%2C%20and%20render%20them%20using%202d%20Gaussian%0Aheatmaps%20in%20an%20end-to-end%20differentiable%20manner.%20Afterwards%2C%20we%20use%20the%0Acorresponding%202d%20joints%20and%20heatmaps%20from%20the%20pseudo%202d%20poses%20for%20learning.%20To%0Aalleviate%20the%20intrinsic%20inaccuracy%20of%20the%20pseudo%20labels%2C%20we%20propose%20an%20adaptive%0Asupervision%20attention%20mechanism%20to%20guide%20the%20self-supervision.%20Our%20experiments%0Aand%20analysis%20on%20three%20public%20benchmark%20datasets%2C%20including%20Panoptic%2C%20Shelf%2C%20and%0ACampus%2C%20show%20the%20effectiveness%20of%20our%20approach%2C%20which%20is%20comparable%20to%0Afully-supervised%20methods.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/CAMMA-public/SelfPose3D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02041v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfPose3d%3A%20Self-Supervised%20Multi-Person%20Multi-View%203d%20Pose%20Estimation&entry.906535625=Vinkle%20Srivastav%20and%20Keqi%20Chen%20and%20Nicolas%20Padoy&entry.1292438233=%20%20We%20present%20a%20new%20self-supervised%20approach%2C%20SelfPose3d%2C%20for%20estimating%203d%0Aposes%20of%20multiple%20persons%20from%20multiple%20camera%20views.%20Unlike%20current%0Astate-of-the-art%20fully-supervised%20methods%2C%20our%20approach%20does%20not%20require%20any%202d%0Aor%203d%20ground-truth%20poses%20and%20uses%20only%20the%20multi-view%20input%20images%20from%20a%0Acalibrated%20camera%20setup%20and%202d%20pseudo%20poses%20generated%20from%20an%20off-the-shelf%202d%0Ahuman%20pose%20estimator.%20We%20propose%20two%20self-supervised%20learning%20objectives%3A%0Aself-supervised%20person%20localization%20in%203d%20space%20and%20self-supervised%203d%20pose%0Aestimation.%20We%20achieve%20self-supervised%203d%20person%20localization%20by%20training%20the%0Amodel%20on%20synthetically%20generated%203d%20points%2C%20serving%20as%203d%20person%20root%0Apositions%2C%20and%20on%20the%20projected%20root-heatmaps%20in%20all%20the%20views.%20We%20then%20model%0Athe%203d%20poses%20of%20all%20the%20localized%20persons%20with%20a%20bottleneck%20representation%2C%20map%0Athem%20onto%20all%20views%20obtaining%202d%20joints%2C%20and%20render%20them%20using%202d%20Gaussian%0Aheatmaps%20in%20an%20end-to-end%20differentiable%20manner.%20Afterwards%2C%20we%20use%20the%0Acorresponding%202d%20joints%20and%20heatmaps%20from%20the%20pseudo%202d%20poses%20for%20learning.%20To%0Aalleviate%20the%20intrinsic%20inaccuracy%20of%20the%20pseudo%20labels%2C%20we%20propose%20an%20adaptive%0Asupervision%20attention%20mechanism%20to%20guide%20the%20self-supervision.%20Our%20experiments%0Aand%20analysis%20on%20three%20public%20benchmark%20datasets%2C%20including%20Panoptic%2C%20Shelf%2C%20and%0ACampus%2C%20show%20the%20effectiveness%20of%20our%20approach%2C%20which%20is%20comparable%20to%0Afully-supervised%20methods.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/CAMMA-public/SelfPose3D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02041v1&entry.124074799=Read"},
{"title": "Samplet basis pursuit: Multiresolution scattered data approximation with\n  sparsity constraints", "author": "Davide Baroli and Helmut Harbrecht and Michael Multerer", "abstract": "  We consider scattered data approximation in samplet coordinates with\n$\\ell_1$-regularization. The application of an $\\ell_1$-regularization term\nenforces sparsity of the coefficients with respect to the samplet basis.\nSamplets are wavelet-type signed measures, which are tailored to scattered\ndata. Therefore, samplets enable the use of well-established multiresolution\ntechniques on general scattered data sets. They provide similar properties as\nwavelets in terms of localization, multiresolution analysis, and data\ncompression. By using the Riesz isometry, we embed samplets into reproducing\nkernel Hilbert spaces and discuss the properties of the resulting functions. We\nargue that the class of signals that are sparse with respect to the embedded\nsamplet basis is considerably larger than the class of signals that are sparse\nwith respect to the basis of kernel translates. Vice versa, every signal that\nis a linear combination of only a few kernel translates is sparse in samplet\ncoordinates.\n  We propose the rapid solution of the problem under consideration by combining\nsoft-shrinkage with the semi-smooth Newton method. Leveraging on the sparse\nrepresentation of kernel matrices in samplet coordinates, this approach\nconverges faster than the fast iterative shrinkage thresholding algorithm and\nis feasible for large-scale data. Numerical benchmarks are presented and\ndemonstrate the superiority of the multiresolution approach over the\nsingle-scale approach. As large-scale applications, the surface reconstruction\nfrom scattered data and the reconstruction of scattered temperature data using\na dictionary of multiple kernels are considered.\n", "link": "http://arxiv.org/abs/2306.10180v4", "date": "2024-04-02", "relevancy": 2.349, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4771}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4583}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Samplet%20basis%20pursuit%3A%20Multiresolution%20scattered%20data%20approximation%20with%0A%20%20sparsity%20constraints&body=Title%3A%20Samplet%20basis%20pursuit%3A%20Multiresolution%20scattered%20data%20approximation%20with%0A%20%20sparsity%20constraints%0AAuthor%3A%20Davide%20Baroli%20and%20Helmut%20Harbrecht%20and%20Michael%20Multerer%0AAbstract%3A%20%20%20We%20consider%20scattered%20data%20approximation%20in%20samplet%20coordinates%20with%0A%24%5Cell_1%24-regularization.%20The%20application%20of%20an%20%24%5Cell_1%24-regularization%20term%0Aenforces%20sparsity%20of%20the%20coefficients%20with%20respect%20to%20the%20samplet%20basis.%0ASamplets%20are%20wavelet-type%20signed%20measures%2C%20which%20are%20tailored%20to%20scattered%0Adata.%20Therefore%2C%20samplets%20enable%20the%20use%20of%20well-established%20multiresolution%0Atechniques%20on%20general%20scattered%20data%20sets.%20They%20provide%20similar%20properties%20as%0Awavelets%20in%20terms%20of%20localization%2C%20multiresolution%20analysis%2C%20and%20data%0Acompression.%20By%20using%20the%20Riesz%20isometry%2C%20we%20embed%20samplets%20into%20reproducing%0Akernel%20Hilbert%20spaces%20and%20discuss%20the%20properties%20of%20the%20resulting%20functions.%20We%0Aargue%20that%20the%20class%20of%20signals%20that%20are%20sparse%20with%20respect%20to%20the%20embedded%0Asamplet%20basis%20is%20considerably%20larger%20than%20the%20class%20of%20signals%20that%20are%20sparse%0Awith%20respect%20to%20the%20basis%20of%20kernel%20translates.%20Vice%20versa%2C%20every%20signal%20that%0Ais%20a%20linear%20combination%20of%20only%20a%20few%20kernel%20translates%20is%20sparse%20in%20samplet%0Acoordinates.%0A%20%20We%20propose%20the%20rapid%20solution%20of%20the%20problem%20under%20consideration%20by%20combining%0Asoft-shrinkage%20with%20the%20semi-smooth%20Newton%20method.%20Leveraging%20on%20the%20sparse%0Arepresentation%20of%20kernel%20matrices%20in%20samplet%20coordinates%2C%20this%20approach%0Aconverges%20faster%20than%20the%20fast%20iterative%20shrinkage%20thresholding%20algorithm%20and%0Ais%20feasible%20for%20large-scale%20data.%20Numerical%20benchmarks%20are%20presented%20and%0Ademonstrate%20the%20superiority%20of%20the%20multiresolution%20approach%20over%20the%0Asingle-scale%20approach.%20As%20large-scale%20applications%2C%20the%20surface%20reconstruction%0Afrom%20scattered%20data%20and%20the%20reconstruction%20of%20scattered%20temperature%20data%20using%0Aa%20dictionary%20of%20multiple%20kernels%20are%20considered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10180v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Samplet%20basis%20pursuit%3A%20Multiresolution%20scattered%20data%20approximation%20with%0A%20%20sparsity%20constraints&entry.906535625=Davide%20Baroli%20and%20Helmut%20Harbrecht%20and%20Michael%20Multerer&entry.1292438233=%20%20We%20consider%20scattered%20data%20approximation%20in%20samplet%20coordinates%20with%0A%24%5Cell_1%24-regularization.%20The%20application%20of%20an%20%24%5Cell_1%24-regularization%20term%0Aenforces%20sparsity%20of%20the%20coefficients%20with%20respect%20to%20the%20samplet%20basis.%0ASamplets%20are%20wavelet-type%20signed%20measures%2C%20which%20are%20tailored%20to%20scattered%0Adata.%20Therefore%2C%20samplets%20enable%20the%20use%20of%20well-established%20multiresolution%0Atechniques%20on%20general%20scattered%20data%20sets.%20They%20provide%20similar%20properties%20as%0Awavelets%20in%20terms%20of%20localization%2C%20multiresolution%20analysis%2C%20and%20data%0Acompression.%20By%20using%20the%20Riesz%20isometry%2C%20we%20embed%20samplets%20into%20reproducing%0Akernel%20Hilbert%20spaces%20and%20discuss%20the%20properties%20of%20the%20resulting%20functions.%20We%0Aargue%20that%20the%20class%20of%20signals%20that%20are%20sparse%20with%20respect%20to%20the%20embedded%0Asamplet%20basis%20is%20considerably%20larger%20than%20the%20class%20of%20signals%20that%20are%20sparse%0Awith%20respect%20to%20the%20basis%20of%20kernel%20translates.%20Vice%20versa%2C%20every%20signal%20that%0Ais%20a%20linear%20combination%20of%20only%20a%20few%20kernel%20translates%20is%20sparse%20in%20samplet%0Acoordinates.%0A%20%20We%20propose%20the%20rapid%20solution%20of%20the%20problem%20under%20consideration%20by%20combining%0Asoft-shrinkage%20with%20the%20semi-smooth%20Newton%20method.%20Leveraging%20on%20the%20sparse%0Arepresentation%20of%20kernel%20matrices%20in%20samplet%20coordinates%2C%20this%20approach%0Aconverges%20faster%20than%20the%20fast%20iterative%20shrinkage%20thresholding%20algorithm%20and%0Ais%20feasible%20for%20large-scale%20data.%20Numerical%20benchmarks%20are%20presented%20and%0Ademonstrate%20the%20superiority%20of%20the%20multiresolution%20approach%20over%20the%0Asingle-scale%20approach.%20As%20large-scale%20applications%2C%20the%20surface%20reconstruction%0Afrom%20scattered%20data%20and%20the%20reconstruction%20of%20scattered%20temperature%20data%20using%0Aa%20dictionary%20of%20multiple%20kernels%20are%20considered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10180v4&entry.124074799=Read"},
{"title": "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from\n  a Single Image", "author": "Chong Bao and Yinda Zhang and Yuan Li and Xiyu Zhang and Bangbang Yang and Hujun Bao and Marc Pollefeys and Guofeng Zhang and Zhaopeng Cui", "abstract": "  Recently, we have witnessed the explosive growth of various volumetric\nrepresentations in modeling animatable head avatars. However, due to the\ndiversity of frameworks, there is no practical method to support high-level\napplications like 3D head avatar editing across different representations. In\nthis paper, we propose a generic avatar editing approach that can be\nuniversally applied to various 3DMM driving volumetric head avatars. To achieve\nthis goal, we design a novel expression-aware modification generative model,\nwhich enables lift 2D editing from a single image to a consistent 3D\nmodification field. To ensure the effectiveness of the generative modification\nprocess, we develop several techniques, including an expression-dependent\nmodification distillation scheme to draw knowledge from the large-scale head\navatar model and 2D facial texture editing tools, implicit latent space\nguidance to enhance model convergence, and a segmentation-based loss reweight\nstrategy for fine-grained texture inversion. Extensive experiments demonstrate\nthat our method delivers high-quality and consistent results across multiple\nexpression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/\n", "link": "http://arxiv.org/abs/2404.02152v1", "date": "2024-04-02", "relevancy": 2.3283, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6038}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5746}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5633}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GeneAvatar%3A%20Generic%20Expression-Aware%20Volumetric%20Head%20Avatar%20Editing%20from%0A%20%20a%20Single%20Image&body=Title%3A%20GeneAvatar%3A%20Generic%20Expression-Aware%20Volumetric%20Head%20Avatar%20Editing%20from%0A%20%20a%20Single%20Image%0AAuthor%3A%20Chong%20Bao%20and%20Yinda%20Zhang%20and%20Yuan%20Li%20and%20Xiyu%20Zhang%20and%20Bangbang%20Yang%20and%20Hujun%20Bao%20and%20Marc%20Pollefeys%20and%20Guofeng%20Zhang%20and%20Zhaopeng%20Cui%0AAbstract%3A%20%20%20Recently%2C%20we%20have%20witnessed%20the%20explosive%20growth%20of%20various%20volumetric%0Arepresentations%20in%20modeling%20animatable%20head%20avatars.%20However%2C%20due%20to%20the%0Adiversity%20of%20frameworks%2C%20there%20is%20no%20practical%20method%20to%20support%20high-level%0Aapplications%20like%203D%20head%20avatar%20editing%20across%20different%20representations.%20In%0Athis%20paper%2C%20we%20propose%20a%20generic%20avatar%20editing%20approach%20that%20can%20be%0Auniversally%20applied%20to%20various%203DMM%20driving%20volumetric%20head%20avatars.%20To%20achieve%0Athis%20goal%2C%20we%20design%20a%20novel%20expression-aware%20modification%20generative%20model%2C%0Awhich%20enables%20lift%202D%20editing%20from%20a%20single%20image%20to%20a%20consistent%203D%0Amodification%20field.%20To%20ensure%20the%20effectiveness%20of%20the%20generative%20modification%0Aprocess%2C%20we%20develop%20several%20techniques%2C%20including%20an%20expression-dependent%0Amodification%20distillation%20scheme%20to%20draw%20knowledge%20from%20the%20large-scale%20head%0Aavatar%20model%20and%202D%20facial%20texture%20editing%20tools%2C%20implicit%20latent%20space%0Aguidance%20to%20enhance%20model%20convergence%2C%20and%20a%20segmentation-based%20loss%20reweight%0Astrategy%20for%20fine-grained%20texture%20inversion.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20delivers%20high-quality%20and%20consistent%20results%20across%20multiple%0Aexpression%20and%20viewpoints.%20Project%20page%3A%20https%3A//zju3dv.github.io/geneavatar/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02152v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeneAvatar%3A%20Generic%20Expression-Aware%20Volumetric%20Head%20Avatar%20Editing%20from%0A%20%20a%20Single%20Image&entry.906535625=Chong%20Bao%20and%20Yinda%20Zhang%20and%20Yuan%20Li%20and%20Xiyu%20Zhang%20and%20Bangbang%20Yang%20and%20Hujun%20Bao%20and%20Marc%20Pollefeys%20and%20Guofeng%20Zhang%20and%20Zhaopeng%20Cui&entry.1292438233=%20%20Recently%2C%20we%20have%20witnessed%20the%20explosive%20growth%20of%20various%20volumetric%0Arepresentations%20in%20modeling%20animatable%20head%20avatars.%20However%2C%20due%20to%20the%0Adiversity%20of%20frameworks%2C%20there%20is%20no%20practical%20method%20to%20support%20high-level%0Aapplications%20like%203D%20head%20avatar%20editing%20across%20different%20representations.%20In%0Athis%20paper%2C%20we%20propose%20a%20generic%20avatar%20editing%20approach%20that%20can%20be%0Auniversally%20applied%20to%20various%203DMM%20driving%20volumetric%20head%20avatars.%20To%20achieve%0Athis%20goal%2C%20we%20design%20a%20novel%20expression-aware%20modification%20generative%20model%2C%0Awhich%20enables%20lift%202D%20editing%20from%20a%20single%20image%20to%20a%20consistent%203D%0Amodification%20field.%20To%20ensure%20the%20effectiveness%20of%20the%20generative%20modification%0Aprocess%2C%20we%20develop%20several%20techniques%2C%20including%20an%20expression-dependent%0Amodification%20distillation%20scheme%20to%20draw%20knowledge%20from%20the%20large-scale%20head%0Aavatar%20model%20and%202D%20facial%20texture%20editing%20tools%2C%20implicit%20latent%20space%0Aguidance%20to%20enhance%20model%20convergence%2C%20and%20a%20segmentation-based%20loss%20reweight%0Astrategy%20for%20fine-grained%20texture%20inversion.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20delivers%20high-quality%20and%20consistent%20results%20across%20multiple%0Aexpression%20and%20viewpoints.%20Project%20page%3A%20https%3A//zju3dv.github.io/geneavatar/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02152v1&entry.124074799=Read"},
{"title": "Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image\n  Restoration", "author": "Akshay Dudhane and Omkar Thawakar and Syed Waqas Zamir and Salman Khan and Fahad Shahbaz Khan and Ming-Hsuan Yang", "abstract": "  All-in-one image restoration tackles different types of degradations with a\nunified model instead of having task-specific, non-generic models for each\ndegradation. The requirement to tackle multiple degradations using the same\nmodel can lead to high-complexity designs with fixed configuration that lack\nthe adaptability to more efficient alternatives. We propose DyNet, a dynamic\nfamily of networks designed in an encoder-decoder style for all-in-one image\nrestoration tasks. Our DyNet can seamlessly switch between its bulkier and\nlightweight variants, thereby offering flexibility for efficient model\ndeployment with a single round of training. This seamless switching is enabled\nby our weights-sharing mechanism, forming the core of our architecture and\nfacilitating the reuse of initialized module weights. Further, to establish\nrobust weights initialization, we introduce a dynamic pre-training strategy\nthat trains variants of the proposed DyNet concurrently, thereby achieving a\n50% reduction in GPU hours. To tackle the unavailability of large-scale dataset\nrequired in pre-training, we curate a high-quality, high-resolution image\ndataset named Million-IRD having 2M image samples. We validate our DyNet for\nimage denoising, deraining, and dehazing in all-in-one setting, achieving\nstate-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction\nin parameters compared to baseline models. The source codes and trained models\nare available at https://github.com/akshaydudhane16/DyNet.\n", "link": "http://arxiv.org/abs/2404.02154v1", "date": "2024-04-02", "relevancy": 2.3249, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5945}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5879}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5653}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Pre-training%3A%20Towards%20Efficient%20and%20Scalable%20All-in-One%20Image%0A%20%20Restoration&body=Title%3A%20Dynamic%20Pre-training%3A%20Towards%20Efficient%20and%20Scalable%20All-in-One%20Image%0A%20%20Restoration%0AAuthor%3A%20Akshay%20Dudhane%20and%20Omkar%20Thawakar%20and%20Syed%20Waqas%20Zamir%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20All-in-one%20image%20restoration%20tackles%20different%20types%20of%20degradations%20with%20a%0Aunified%20model%20instead%20of%20having%20task-specific%2C%20non-generic%20models%20for%20each%0Adegradation.%20The%20requirement%20to%20tackle%20multiple%20degradations%20using%20the%20same%0Amodel%20can%20lead%20to%20high-complexity%20designs%20with%20fixed%20configuration%20that%20lack%0Athe%20adaptability%20to%20more%20efficient%20alternatives.%20We%20propose%20DyNet%2C%20a%20dynamic%0Afamily%20of%20networks%20designed%20in%20an%20encoder-decoder%20style%20for%20all-in-one%20image%0Arestoration%20tasks.%20Our%20DyNet%20can%20seamlessly%20switch%20between%20its%20bulkier%20and%0Alightweight%20variants%2C%20thereby%20offering%20flexibility%20for%20efficient%20model%0Adeployment%20with%20a%20single%20round%20of%20training.%20This%20seamless%20switching%20is%20enabled%0Aby%20our%20weights-sharing%20mechanism%2C%20forming%20the%20core%20of%20our%20architecture%20and%0Afacilitating%20the%20reuse%20of%20initialized%20module%20weights.%20Further%2C%20to%20establish%0Arobust%20weights%20initialization%2C%20we%20introduce%20a%20dynamic%20pre-training%20strategy%0Athat%20trains%20variants%20of%20the%20proposed%20DyNet%20concurrently%2C%20thereby%20achieving%20a%0A50%25%20reduction%20in%20GPU%20hours.%20To%20tackle%20the%20unavailability%20of%20large-scale%20dataset%0Arequired%20in%20pre-training%2C%20we%20curate%20a%20high-quality%2C%20high-resolution%20image%0Adataset%20named%20Million-IRD%20having%202M%20image%20samples.%20We%20validate%20our%20DyNet%20for%0Aimage%20denoising%2C%20deraining%2C%20and%20dehazing%20in%20all-in-one%20setting%2C%20achieving%0Astate-of-the-art%20results%20with%2031.34%25%20reduction%20in%20GFlops%20and%20a%2056.75%25%20reduction%0Ain%20parameters%20compared%20to%20baseline%20models.%20The%20source%20codes%20and%20trained%20models%0Aare%20available%20at%20https%3A//github.com/akshaydudhane16/DyNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02154v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Pre-training%3A%20Towards%20Efficient%20and%20Scalable%20All-in-One%20Image%0A%20%20Restoration&entry.906535625=Akshay%20Dudhane%20and%20Omkar%20Thawakar%20and%20Syed%20Waqas%20Zamir%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20All-in-one%20image%20restoration%20tackles%20different%20types%20of%20degradations%20with%20a%0Aunified%20model%20instead%20of%20having%20task-specific%2C%20non-generic%20models%20for%20each%0Adegradation.%20The%20requirement%20to%20tackle%20multiple%20degradations%20using%20the%20same%0Amodel%20can%20lead%20to%20high-complexity%20designs%20with%20fixed%20configuration%20that%20lack%0Athe%20adaptability%20to%20more%20efficient%20alternatives.%20We%20propose%20DyNet%2C%20a%20dynamic%0Afamily%20of%20networks%20designed%20in%20an%20encoder-decoder%20style%20for%20all-in-one%20image%0Arestoration%20tasks.%20Our%20DyNet%20can%20seamlessly%20switch%20between%20its%20bulkier%20and%0Alightweight%20variants%2C%20thereby%20offering%20flexibility%20for%20efficient%20model%0Adeployment%20with%20a%20single%20round%20of%20training.%20This%20seamless%20switching%20is%20enabled%0Aby%20our%20weights-sharing%20mechanism%2C%20forming%20the%20core%20of%20our%20architecture%20and%0Afacilitating%20the%20reuse%20of%20initialized%20module%20weights.%20Further%2C%20to%20establish%0Arobust%20weights%20initialization%2C%20we%20introduce%20a%20dynamic%20pre-training%20strategy%0Athat%20trains%20variants%20of%20the%20proposed%20DyNet%20concurrently%2C%20thereby%20achieving%20a%0A50%25%20reduction%20in%20GPU%20hours.%20To%20tackle%20the%20unavailability%20of%20large-scale%20dataset%0Arequired%20in%20pre-training%2C%20we%20curate%20a%20high-quality%2C%20high-resolution%20image%0Adataset%20named%20Million-IRD%20having%202M%20image%20samples.%20We%20validate%20our%20DyNet%20for%0Aimage%20denoising%2C%20deraining%2C%20and%20dehazing%20in%20all-in-one%20setting%2C%20achieving%0Astate-of-the-art%20results%20with%2031.34%25%20reduction%20in%20GFlops%20and%20a%2056.75%25%20reduction%0Ain%20parameters%20compared%20to%20baseline%20models.%20The%20source%20codes%20and%20trained%20models%0Aare%20available%20at%20https%3A//github.com/akshaydudhane16/DyNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02154v1&entry.124074799=Read"},
{"title": "Segment Any 3D Object with Language", "author": "Seungjun Lee and Yuyang Zhao and Gim Hee Lee", "abstract": "  In this paper, we investigate Open-Vocabulary 3D Instance Segmentation\n(OV-3DIS) with free-form language instructions. Earlier works that rely on only\nannotated base categories for training suffer from limited generalization to\nunseen novel categories. Recent works mitigate poor generalizability to novel\ncategories by generating class-agnostic masks or projecting generalized masks\nfrom 2D to 3D, but disregard semantic or geometry information, leading to\nsub-optimal performance. Instead, generating generalizable but semantic-related\nmasks directly from 3D point clouds would result in superior outcomes. In this\npaper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a\nsemantic and geometric-aware visual-language learning framework with strong\ngeneralizability by generating semantic-related masks directly from 3D point\nclouds. Specifically, we propose a multimodal fusion network to incorporate\nmultimodal semantics in both backbone and decoder. In addition, to align the 3D\nsegmentation model with various language instructions and enhance the mask\nquality, we introduce three types of multimodal associations as supervision.\nOur SOLE outperforms previous methods by a large margin on ScanNetv2,\nScanNet200, and Replica benchmarks, and the results are even close to the\nfully-supervised counterpart despite the absence of class annotations in the\ntraining. Furthermore, extensive qualitative results demonstrate the\nversatility of our SOLE to language instructions.\n", "link": "http://arxiv.org/abs/2404.02157v1", "date": "2024-04-02", "relevancy": 2.3036, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5988}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5667}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5416}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Segment%20Any%203D%20Object%20with%20Language&body=Title%3A%20Segment%20Any%203D%20Object%20with%20Language%0AAuthor%3A%20Seungjun%20Lee%20and%20Yuyang%20Zhao%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20Open-Vocabulary%203D%20Instance%20Segmentation%0A%28OV-3DIS%29%20with%20free-form%20language%20instructions.%20Earlier%20works%20that%20rely%20on%20only%0Aannotated%20base%20categories%20for%20training%20suffer%20from%20limited%20generalization%20to%0Aunseen%20novel%20categories.%20Recent%20works%20mitigate%20poor%20generalizability%20to%20novel%0Acategories%20by%20generating%20class-agnostic%20masks%20or%20projecting%20generalized%20masks%0Afrom%202D%20to%203D%2C%20but%20disregard%20semantic%20or%20geometry%20information%2C%20leading%20to%0Asub-optimal%20performance.%20Instead%2C%20generating%20generalizable%20but%20semantic-related%0Amasks%20directly%20from%203D%20point%20clouds%20would%20result%20in%20superior%20outcomes.%20In%20this%0Apaper%2C%20we%20introduce%20Segment%20any%203D%20Object%20with%20LanguagE%20%28SOLE%29%2C%20which%20is%20a%0Asemantic%20and%20geometric-aware%20visual-language%20learning%20framework%20with%20strong%0Ageneralizability%20by%20generating%20semantic-related%20masks%20directly%20from%203D%20point%0Aclouds.%20Specifically%2C%20we%20propose%20a%20multimodal%20fusion%20network%20to%20incorporate%0Amultimodal%20semantics%20in%20both%20backbone%20and%20decoder.%20In%20addition%2C%20to%20align%20the%203D%0Asegmentation%20model%20with%20various%20language%20instructions%20and%20enhance%20the%20mask%0Aquality%2C%20we%20introduce%20three%20types%20of%20multimodal%20associations%20as%20supervision.%0AOur%20SOLE%20outperforms%20previous%20methods%20by%20a%20large%20margin%20on%20ScanNetv2%2C%0AScanNet200%2C%20and%20Replica%20benchmarks%2C%20and%20the%20results%20are%20even%20close%20to%20the%0Afully-supervised%20counterpart%20despite%20the%20absence%20of%20class%20annotations%20in%20the%0Atraining.%20Furthermore%2C%20extensive%20qualitative%20results%20demonstrate%20the%0Aversatility%20of%20our%20SOLE%20to%20language%20instructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02157v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Any%203D%20Object%20with%20Language&entry.906535625=Seungjun%20Lee%20and%20Yuyang%20Zhao%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20Open-Vocabulary%203D%20Instance%20Segmentation%0A%28OV-3DIS%29%20with%20free-form%20language%20instructions.%20Earlier%20works%20that%20rely%20on%20only%0Aannotated%20base%20categories%20for%20training%20suffer%20from%20limited%20generalization%20to%0Aunseen%20novel%20categories.%20Recent%20works%20mitigate%20poor%20generalizability%20to%20novel%0Acategories%20by%20generating%20class-agnostic%20masks%20or%20projecting%20generalized%20masks%0Afrom%202D%20to%203D%2C%20but%20disregard%20semantic%20or%20geometry%20information%2C%20leading%20to%0Asub-optimal%20performance.%20Instead%2C%20generating%20generalizable%20but%20semantic-related%0Amasks%20directly%20from%203D%20point%20clouds%20would%20result%20in%20superior%20outcomes.%20In%20this%0Apaper%2C%20we%20introduce%20Segment%20any%203D%20Object%20with%20LanguagE%20%28SOLE%29%2C%20which%20is%20a%0Asemantic%20and%20geometric-aware%20visual-language%20learning%20framework%20with%20strong%0Ageneralizability%20by%20generating%20semantic-related%20masks%20directly%20from%203D%20point%0Aclouds.%20Specifically%2C%20we%20propose%20a%20multimodal%20fusion%20network%20to%20incorporate%0Amultimodal%20semantics%20in%20both%20backbone%20and%20decoder.%20In%20addition%2C%20to%20align%20the%203D%0Asegmentation%20model%20with%20various%20language%20instructions%20and%20enhance%20the%20mask%0Aquality%2C%20we%20introduce%20three%20types%20of%20multimodal%20associations%20as%20supervision.%0AOur%20SOLE%20outperforms%20previous%20methods%20by%20a%20large%20margin%20on%20ScanNetv2%2C%0AScanNet200%2C%20and%20Replica%20benchmarks%2C%20and%20the%20results%20are%20even%20close%20to%20the%0Afully-supervised%20counterpart%20despite%20the%20absence%20of%20class%20annotations%20in%20the%0Atraining.%20Furthermore%2C%20extensive%20qualitative%20results%20demonstrate%20the%0Aversatility%20of%20our%20SOLE%20to%20language%20instructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02157v1&entry.124074799=Read"},
{"title": "CameraCtrl: Enabling Camera Control for Text-to-Video Generation", "author": "Hao He and Yinghao Xu and Yuwei Guo and Gordon Wetzstein and Bo Dai and Hongsheng Li and Ceyuan Yang", "abstract": "  Controllability plays a crucial role in video generation since it allows\nusers to create desired content. However, existing models largely overlooked\nthe precise control of camera pose that serves as a cinematic language to\nexpress deeper narrative nuances. To alleviate this issue, we introduce\nCameraCtrl, enabling accurate camera pose control for text-to-video(T2V)\nmodels. After precisely parameterizing the camera trajectory, a plug-and-play\ncamera module is then trained on a T2V model, leaving others untouched.\nAdditionally, a comprehensive study on the effect of various datasets is also\nconducted, suggesting that videos with diverse camera distribution and similar\nappearances indeed enhance controllability and generalization. Experimental\nresults demonstrate the effectiveness of CameraCtrl in achieving precise and\ndomain-adaptive camera control, marking a step forward in the pursuit of\ndynamic and customized video storytelling from textual and camera pose inputs.\nOur project website is at: https://hehao13.github.io/projects-CameraCtrl/.\n", "link": "http://arxiv.org/abs/2404.02101v1", "date": "2024-04-02", "relevancy": 2.2967, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6261}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5979}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5297}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CameraCtrl%3A%20Enabling%20Camera%20Control%20for%20Text-to-Video%20Generation&body=Title%3A%20CameraCtrl%3A%20Enabling%20Camera%20Control%20for%20Text-to-Video%20Generation%0AAuthor%3A%20Hao%20He%20and%20Yinghao%20Xu%20and%20Yuwei%20Guo%20and%20Gordon%20Wetzstein%20and%20Bo%20Dai%20and%20Hongsheng%20Li%20and%20Ceyuan%20Yang%0AAbstract%3A%20%20%20Controllability%20plays%20a%20crucial%20role%20in%20video%20generation%20since%20it%20allows%0Ausers%20to%20create%20desired%20content.%20However%2C%20existing%20models%20largely%20overlooked%0Athe%20precise%20control%20of%20camera%20pose%20that%20serves%20as%20a%20cinematic%20language%20to%0Aexpress%20deeper%20narrative%20nuances.%20To%20alleviate%20this%20issue%2C%20we%20introduce%0ACameraCtrl%2C%20enabling%20accurate%20camera%20pose%20control%20for%20text-to-video%28T2V%29%0Amodels.%20After%20precisely%20parameterizing%20the%20camera%20trajectory%2C%20a%20plug-and-play%0Acamera%20module%20is%20then%20trained%20on%20a%20T2V%20model%2C%20leaving%20others%20untouched.%0AAdditionally%2C%20a%20comprehensive%20study%20on%20the%20effect%20of%20various%20datasets%20is%20also%0Aconducted%2C%20suggesting%20that%20videos%20with%20diverse%20camera%20distribution%20and%20similar%0Aappearances%20indeed%20enhance%20controllability%20and%20generalization.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20CameraCtrl%20in%20achieving%20precise%20and%0Adomain-adaptive%20camera%20control%2C%20marking%20a%20step%20forward%20in%20the%20pursuit%20of%0Adynamic%20and%20customized%20video%20storytelling%20from%20textual%20and%20camera%20pose%20inputs.%0AOur%20project%20website%20is%20at%3A%20https%3A//hehao13.github.io/projects-CameraCtrl/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02101v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CameraCtrl%3A%20Enabling%20Camera%20Control%20for%20Text-to-Video%20Generation&entry.906535625=Hao%20He%20and%20Yinghao%20Xu%20and%20Yuwei%20Guo%20and%20Gordon%20Wetzstein%20and%20Bo%20Dai%20and%20Hongsheng%20Li%20and%20Ceyuan%20Yang&entry.1292438233=%20%20Controllability%20plays%20a%20crucial%20role%20in%20video%20generation%20since%20it%20allows%0Ausers%20to%20create%20desired%20content.%20However%2C%20existing%20models%20largely%20overlooked%0Athe%20precise%20control%20of%20camera%20pose%20that%20serves%20as%20a%20cinematic%20language%20to%0Aexpress%20deeper%20narrative%20nuances.%20To%20alleviate%20this%20issue%2C%20we%20introduce%0ACameraCtrl%2C%20enabling%20accurate%20camera%20pose%20control%20for%20text-to-video%28T2V%29%0Amodels.%20After%20precisely%20parameterizing%20the%20camera%20trajectory%2C%20a%20plug-and-play%0Acamera%20module%20is%20then%20trained%20on%20a%20T2V%20model%2C%20leaving%20others%20untouched.%0AAdditionally%2C%20a%20comprehensive%20study%20on%20the%20effect%20of%20various%20datasets%20is%20also%0Aconducted%2C%20suggesting%20that%20videos%20with%20diverse%20camera%20distribution%20and%20similar%0Aappearances%20indeed%20enhance%20controllability%20and%20generalization.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20CameraCtrl%20in%20achieving%20precise%20and%0Adomain-adaptive%20camera%20control%2C%20marking%20a%20step%20forward%20in%20the%20pursuit%20of%0Adynamic%20and%20customized%20video%20storytelling%20from%20textual%20and%20camera%20pose%20inputs.%0AOur%20project%20website%20is%20at%3A%20https%3A//hehao13.github.io/projects-CameraCtrl/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02101v1&entry.124074799=Read"},
{"title": "Multi-Level Label Correction by Distilling Proximate Patterns for\n  Semi-supervised Semantic Segmentation", "author": "Hui Xiao and Yuting Hong and Li Dong and Diqun Yan and Jiayan Zhuang and Junjie Xiong and Dongtai Liang and Chengbin Peng", "abstract": "  Semi-supervised semantic segmentation relieves the reliance on large-scale\nlabeled data by leveraging unlabeled data. Recent semi-supervised semantic\nsegmentation approaches mainly resort to pseudo-labeling methods to exploit\nunlabeled data. However, unreliable pseudo-labeling can undermine the\nsemi-supervision processes. In this paper, we propose an algorithm called\nMulti-Level Label Correction (MLLC), which aims to use graph neural networks to\ncapture structural relationships in Semantic-Level Graphs (SLGs) and\nClass-Level Graphs (CLGs) to rectify erroneous pseudo-labels. Specifically,\nSLGs represent semantic affinities between pairs of pixel features, and CLGs\ndescribe classification consistencies between pairs of pixel labels. With the\nsupport of proximate pattern information from graphs, MLLC can rectify\nincorrectly predicted pseudo-labels and can facilitate discriminative feature\nrepresentations. We design an end-to-end network to train and perform this\neffective label corrections mechanism. Experiments demonstrate that MLLC can\nsignificantly improve supervised baselines and outperforms state-of-the-art\napproaches in different scenarios on Cityscapes and PASCAL VOC 2012 datasets.\nSpecifically, MLLC improves the supervised baseline by at least 5% and 2% with\nDeepLabV2 and DeepLabV3+ respectively under different partition protocols.\n", "link": "http://arxiv.org/abs/2404.02065v1", "date": "2024-04-02", "relevancy": 2.2931, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5746}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.564}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Level%20Label%20Correction%20by%20Distilling%20Proximate%20Patterns%20for%0A%20%20Semi-supervised%20Semantic%20Segmentation&body=Title%3A%20Multi-Level%20Label%20Correction%20by%20Distilling%20Proximate%20Patterns%20for%0A%20%20Semi-supervised%20Semantic%20Segmentation%0AAuthor%3A%20Hui%20Xiao%20and%20Yuting%20Hong%20and%20Li%20Dong%20and%20Diqun%20Yan%20and%20Jiayan%20Zhuang%20and%20Junjie%20Xiong%20and%20Dongtai%20Liang%20and%20Chengbin%20Peng%0AAbstract%3A%20%20%20Semi-supervised%20semantic%20segmentation%20relieves%20the%20reliance%20on%20large-scale%0Alabeled%20data%20by%20leveraging%20unlabeled%20data.%20Recent%20semi-supervised%20semantic%0Asegmentation%20approaches%20mainly%20resort%20to%20pseudo-labeling%20methods%20to%20exploit%0Aunlabeled%20data.%20However%2C%20unreliable%20pseudo-labeling%20can%20undermine%20the%0Asemi-supervision%20processes.%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20called%0AMulti-Level%20Label%20Correction%20%28MLLC%29%2C%20which%20aims%20to%20use%20graph%20neural%20networks%20to%0Acapture%20structural%20relationships%20in%20Semantic-Level%20Graphs%20%28SLGs%29%20and%0AClass-Level%20Graphs%20%28CLGs%29%20to%20rectify%20erroneous%20pseudo-labels.%20Specifically%2C%0ASLGs%20represent%20semantic%20affinities%20between%20pairs%20of%20pixel%20features%2C%20and%20CLGs%0Adescribe%20classification%20consistencies%20between%20pairs%20of%20pixel%20labels.%20With%20the%0Asupport%20of%20proximate%20pattern%20information%20from%20graphs%2C%20MLLC%20can%20rectify%0Aincorrectly%20predicted%20pseudo-labels%20and%20can%20facilitate%20discriminative%20feature%0Arepresentations.%20We%20design%20an%20end-to-end%20network%20to%20train%20and%20perform%20this%0Aeffective%20label%20corrections%20mechanism.%20Experiments%20demonstrate%20that%20MLLC%20can%0Asignificantly%20improve%20supervised%20baselines%20and%20outperforms%20state-of-the-art%0Aapproaches%20in%20different%20scenarios%20on%20Cityscapes%20and%20PASCAL%20VOC%202012%20datasets.%0ASpecifically%2C%20MLLC%20improves%20the%20supervised%20baseline%20by%20at%20least%205%25%20and%202%25%20with%0ADeepLabV2%20and%20DeepLabV3%2B%20respectively%20under%20different%20partition%20protocols.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02065v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Level%20Label%20Correction%20by%20Distilling%20Proximate%20Patterns%20for%0A%20%20Semi-supervised%20Semantic%20Segmentation&entry.906535625=Hui%20Xiao%20and%20Yuting%20Hong%20and%20Li%20Dong%20and%20Diqun%20Yan%20and%20Jiayan%20Zhuang%20and%20Junjie%20Xiong%20and%20Dongtai%20Liang%20and%20Chengbin%20Peng&entry.1292438233=%20%20Semi-supervised%20semantic%20segmentation%20relieves%20the%20reliance%20on%20large-scale%0Alabeled%20data%20by%20leveraging%20unlabeled%20data.%20Recent%20semi-supervised%20semantic%0Asegmentation%20approaches%20mainly%20resort%20to%20pseudo-labeling%20methods%20to%20exploit%0Aunlabeled%20data.%20However%2C%20unreliable%20pseudo-labeling%20can%20undermine%20the%0Asemi-supervision%20processes.%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20called%0AMulti-Level%20Label%20Correction%20%28MLLC%29%2C%20which%20aims%20to%20use%20graph%20neural%20networks%20to%0Acapture%20structural%20relationships%20in%20Semantic-Level%20Graphs%20%28SLGs%29%20and%0AClass-Level%20Graphs%20%28CLGs%29%20to%20rectify%20erroneous%20pseudo-labels.%20Specifically%2C%0ASLGs%20represent%20semantic%20affinities%20between%20pairs%20of%20pixel%20features%2C%20and%20CLGs%0Adescribe%20classification%20consistencies%20between%20pairs%20of%20pixel%20labels.%20With%20the%0Asupport%20of%20proximate%20pattern%20information%20from%20graphs%2C%20MLLC%20can%20rectify%0Aincorrectly%20predicted%20pseudo-labels%20and%20can%20facilitate%20discriminative%20feature%0Arepresentations.%20We%20design%20an%20end-to-end%20network%20to%20train%20and%20perform%20this%0Aeffective%20label%20corrections%20mechanism.%20Experiments%20demonstrate%20that%20MLLC%20can%0Asignificantly%20improve%20supervised%20baselines%20and%20outperforms%20state-of-the-art%0Aapproaches%20in%20different%20scenarios%20on%20Cityscapes%20and%20PASCAL%20VOC%202012%20datasets.%0ASpecifically%2C%20MLLC%20improves%20the%20supervised%20baseline%20by%20at%20least%205%25%20and%202%25%20with%0ADeepLabV2%20and%20DeepLabV3%2B%20respectively%20under%20different%20partition%20protocols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02065v1&entry.124074799=Read"},
{"title": "Resource-Aware Collaborative Monte Carlo Localization with Distribution\n  Compression", "author": "Nicky Zimmerman and Alessandro Giusti and J\u00e9r\u00f4me Guzzi", "abstract": "  Global localization is essential in enabling robot autonomy, and\ncollaborative localization is key for multi-robot systems. In this paper, we\naddress the task of collaborative global localization under computational and\ncommunication constraints. We propose a method which reduces the amount of\ninformation exchanged and the computational cost. We also analyze, implement\nand open-source seminal approaches, which we believe to be a valuable\ncontribution to the community. We exploit techniques for distribution\ncompression in near-linear time, with error guarantees. We evaluate our\napproach and the implemented baselines on multiple challenging scenarios,\nsimulated and real-world. Our approach can run online on an onboard computer.\nWe release an open-source C++/ROS2 implementation of our approach, as well as\nthe baselines\n", "link": "http://arxiv.org/abs/2404.02010v1", "date": "2024-04-02", "relevancy": 2.2525, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5958}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.553}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5346}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Resource-Aware%20Collaborative%20Monte%20Carlo%20Localization%20with%20Distribution%0A%20%20Compression&body=Title%3A%20Resource-Aware%20Collaborative%20Monte%20Carlo%20Localization%20with%20Distribution%0A%20%20Compression%0AAuthor%3A%20Nicky%20Zimmerman%20and%20Alessandro%20Giusti%20and%20J%C3%A9r%C3%B4me%20Guzzi%0AAbstract%3A%20%20%20Global%20localization%20is%20essential%20in%20enabling%20robot%20autonomy%2C%20and%0Acollaborative%20localization%20is%20key%20for%20multi-robot%20systems.%20In%20this%20paper%2C%20we%0Aaddress%20the%20task%20of%20collaborative%20global%20localization%20under%20computational%20and%0Acommunication%20constraints.%20We%20propose%20a%20method%20which%20reduces%20the%20amount%20of%0Ainformation%20exchanged%20and%20the%20computational%20cost.%20We%20also%20analyze%2C%20implement%0Aand%20open-source%20seminal%20approaches%2C%20which%20we%20believe%20to%20be%20a%20valuable%0Acontribution%20to%20the%20community.%20We%20exploit%20techniques%20for%20distribution%0Acompression%20in%20near-linear%20time%2C%20with%20error%20guarantees.%20We%20evaluate%20our%0Aapproach%20and%20the%20implemented%20baselines%20on%20multiple%20challenging%20scenarios%2C%0Asimulated%20and%20real-world.%20Our%20approach%20can%20run%20online%20on%20an%20onboard%20computer.%0AWe%20release%20an%20open-source%20C%2B%2B/ROS2%20implementation%20of%20our%20approach%2C%20as%20well%20as%0Athe%20baselines%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02010v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource-Aware%20Collaborative%20Monte%20Carlo%20Localization%20with%20Distribution%0A%20%20Compression&entry.906535625=Nicky%20Zimmerman%20and%20Alessandro%20Giusti%20and%20J%C3%A9r%C3%B4me%20Guzzi&entry.1292438233=%20%20Global%20localization%20is%20essential%20in%20enabling%20robot%20autonomy%2C%20and%0Acollaborative%20localization%20is%20key%20for%20multi-robot%20systems.%20In%20this%20paper%2C%20we%0Aaddress%20the%20task%20of%20collaborative%20global%20localization%20under%20computational%20and%0Acommunication%20constraints.%20We%20propose%20a%20method%20which%20reduces%20the%20amount%20of%0Ainformation%20exchanged%20and%20the%20computational%20cost.%20We%20also%20analyze%2C%20implement%0Aand%20open-source%20seminal%20approaches%2C%20which%20we%20believe%20to%20be%20a%20valuable%0Acontribution%20to%20the%20community.%20We%20exploit%20techniques%20for%20distribution%0Acompression%20in%20near-linear%20time%2C%20with%20error%20guarantees.%20We%20evaluate%20our%0Aapproach%20and%20the%20implemented%20baselines%20on%20multiple%20challenging%20scenarios%2C%0Asimulated%20and%20real-world.%20Our%20approach%20can%20run%20online%20on%20an%20onboard%20computer.%0AWe%20release%20an%20open-source%20C%2B%2B/ROS2%20implementation%20of%20our%20approach%2C%20as%20well%20as%0Athe%20baselines%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02010v1&entry.124074799=Read"},
{"title": "Open-Vocabulary Federated Learning with Multimodal Prototyping", "author": "Huimin Zeng and Zhenrui Yue and Dong Wang", "abstract": "  Existing federated learning (FL) studies usually assume the training label\nspace and test label space are identical. However, in real-world applications,\nthis assumption is too ideal to be true. A new user could come up with queries\nthat involve data from unseen classes, and such open-vocabulary queries would\ndirectly defect such FL systems. Therefore, in this work, we explicitly focus\non the under-explored open-vocabulary challenge in FL. That is, for a new user,\nthe global server shall understand her/his query that involves arbitrary\nunknown classes. To address this problem, we leverage the pre-trained\nvision-language models (VLMs). In particular, we present a novel adaptation\nframework tailored for VLMs in the context of FL, named as Federated Multimodal\nPrototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights\nbased on light-weight client residuals, and makes predictions based on a novel\nmultimodal prototyping mechanism. Fed-MP exploits the knowledge learned from\nthe seen classes, and robustifies the adapted VLM to unseen categories. Our\nempirical evaluation on various datasets validates the effectiveness of Fed-MP.\n", "link": "http://arxiv.org/abs/2404.01232v2", "date": "2024-04-02", "relevancy": 2.2195, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.58}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5455}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5335}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%20Federated%20Learning%20with%20Multimodal%20Prototyping&body=Title%3A%20Open-Vocabulary%20Federated%20Learning%20with%20Multimodal%20Prototyping%0AAuthor%3A%20Huimin%20Zeng%20and%20Zhenrui%20Yue%20and%20Dong%20Wang%0AAbstract%3A%20%20%20Existing%20federated%20learning%20%28FL%29%20studies%20usually%20assume%20the%20training%20label%0Aspace%20and%20test%20label%20space%20are%20identical.%20However%2C%20in%20real-world%20applications%2C%0Athis%20assumption%20is%20too%20ideal%20to%20be%20true.%20A%20new%20user%20could%20come%20up%20with%20queries%0Athat%20involve%20data%20from%20unseen%20classes%2C%20and%20such%20open-vocabulary%20queries%20would%0Adirectly%20defect%20such%20FL%20systems.%20Therefore%2C%20in%20this%20work%2C%20we%20explicitly%20focus%0Aon%20the%20under-explored%20open-vocabulary%20challenge%20in%20FL.%20That%20is%2C%20for%20a%20new%20user%2C%0Athe%20global%20server%20shall%20understand%20her/his%20query%20that%20involves%20arbitrary%0Aunknown%20classes.%20To%20address%20this%20problem%2C%20we%20leverage%20the%20pre-trained%0Avision-language%20models%20%28VLMs%29.%20In%20particular%2C%20we%20present%20a%20novel%20adaptation%0Aframework%20tailored%20for%20VLMs%20in%20the%20context%20of%20FL%2C%20named%20as%20Federated%20Multimodal%0APrototyping%20%28Fed-MP%29.%20Fed-MP%20adaptively%20aggregates%20the%20local%20model%20weights%0Abased%20on%20light-weight%20client%20residuals%2C%20and%20makes%20predictions%20based%20on%20a%20novel%0Amultimodal%20prototyping%20mechanism.%20Fed-MP%20exploits%20the%20knowledge%20learned%20from%0Athe%20seen%20classes%2C%20and%20robustifies%20the%20adapted%20VLM%20to%20unseen%20categories.%20Our%0Aempirical%20evaluation%20on%20various%20datasets%20validates%20the%20effectiveness%20of%20Fed-MP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01232v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%20Federated%20Learning%20with%20Multimodal%20Prototyping&entry.906535625=Huimin%20Zeng%20and%20Zhenrui%20Yue%20and%20Dong%20Wang&entry.1292438233=%20%20Existing%20federated%20learning%20%28FL%29%20studies%20usually%20assume%20the%20training%20label%0Aspace%20and%20test%20label%20space%20are%20identical.%20However%2C%20in%20real-world%20applications%2C%0Athis%20assumption%20is%20too%20ideal%20to%20be%20true.%20A%20new%20user%20could%20come%20up%20with%20queries%0Athat%20involve%20data%20from%20unseen%20classes%2C%20and%20such%20open-vocabulary%20queries%20would%0Adirectly%20defect%20such%20FL%20systems.%20Therefore%2C%20in%20this%20work%2C%20we%20explicitly%20focus%0Aon%20the%20under-explored%20open-vocabulary%20challenge%20in%20FL.%20That%20is%2C%20for%20a%20new%20user%2C%0Athe%20global%20server%20shall%20understand%20her/his%20query%20that%20involves%20arbitrary%0Aunknown%20classes.%20To%20address%20this%20problem%2C%20we%20leverage%20the%20pre-trained%0Avision-language%20models%20%28VLMs%29.%20In%20particular%2C%20we%20present%20a%20novel%20adaptation%0Aframework%20tailored%20for%20VLMs%20in%20the%20context%20of%20FL%2C%20named%20as%20Federated%20Multimodal%0APrototyping%20%28Fed-MP%29.%20Fed-MP%20adaptively%20aggregates%20the%20local%20model%20weights%0Abased%20on%20light-weight%20client%20residuals%2C%20and%20makes%20predictions%20based%20on%20a%20novel%0Amultimodal%20prototyping%20mechanism.%20Fed-MP%20exploits%20the%20knowledge%20learned%20from%0Athe%20seen%20classes%2C%20and%20robustifies%20the%20adapted%20VLM%20to%20unseen%20categories.%20Our%0Aempirical%20evaluation%20on%20various%20datasets%20validates%20the%20effectiveness%20of%20Fed-MP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01232v2&entry.124074799=Read"},
{"title": "Cooperative Students: Navigating Unsupervised Domain Adaptation in\n  Nighttime Object Detection", "author": "Jicheng Yuan and Anh Le-Tuan and Manfred Hauswirth and Danh Le-Phuoc", "abstract": "  Unsupervised Domain Adaptation (UDA) has shown significant advancements in\nobject detection under well-lit conditions; however, its performance degrades\nnotably in low-visibility scenarios, especially at night, posing challenges not\nonly for its adaptability in low signal-to-noise ratio (SNR) conditions but\nalso for the reliability and efficiency of automated vehicles. To address this\nproblem, we propose a \\textbf{Co}operative \\textbf{S}tudents (\\textbf{CoS})\nframework that innovatively employs global-local transformations (GLT) and a\nproxy-based target consistency (PTC) mechanism to capture the spatial\nconsistency in day- and night-time scenarios effectively, and thus bridge the\nsignificant domain shift across contexts. Building upon this, we further devise\nan adaptive IoU-informed thresholding (AIT) module to gradually avoid\noverlooking potential true positives and enrich the latent information in the\ntarget domain. Comprehensive experiments show that CoS essentially enhanced UDA\nperformance in low-visibility conditions and surpasses current state-of-the-art\ntechniques, achieving an increase in mAP of 3.0\\%, 1.9\\%, and 2.5\\% on BDD100K,\nSHIFT, and ACDC datasets, respectively. Code is available at\nhttps://github.com/jichengyuan/Cooperitive_Students.\n", "link": "http://arxiv.org/abs/2404.01988v1", "date": "2024-04-02", "relevancy": 2.2127, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5607}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5495}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5436}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Students%3A%20Navigating%20Unsupervised%20Domain%20Adaptation%20in%0A%20%20Nighttime%20Object%20Detection&body=Title%3A%20Cooperative%20Students%3A%20Navigating%20Unsupervised%20Domain%20Adaptation%20in%0A%20%20Nighttime%20Object%20Detection%0AAuthor%3A%20Jicheng%20Yuan%20and%20Anh%20Le-Tuan%20and%20Manfred%20Hauswirth%20and%20Danh%20Le-Phuoc%0AAbstract%3A%20%20%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20has%20shown%20significant%20advancements%20in%0Aobject%20detection%20under%20well-lit%20conditions%3B%20however%2C%20its%20performance%20degrades%0Anotably%20in%20low-visibility%20scenarios%2C%20especially%20at%20night%2C%20posing%20challenges%20not%0Aonly%20for%20its%20adaptability%20in%20low%20signal-to-noise%20ratio%20%28SNR%29%20conditions%20but%0Aalso%20for%20the%20reliability%20and%20efficiency%20of%20automated%20vehicles.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20%5Ctextbf%7BCo%7Doperative%20%5Ctextbf%7BS%7Dtudents%20%28%5Ctextbf%7BCoS%7D%29%0Aframework%20that%20innovatively%20employs%20global-local%20transformations%20%28GLT%29%20and%20a%0Aproxy-based%20target%20consistency%20%28PTC%29%20mechanism%20to%20capture%20the%20spatial%0Aconsistency%20in%20day-%20and%20night-time%20scenarios%20effectively%2C%20and%20thus%20bridge%20the%0Asignificant%20domain%20shift%20across%20contexts.%20Building%20upon%20this%2C%20we%20further%20devise%0Aan%20adaptive%20IoU-informed%20thresholding%20%28AIT%29%20module%20to%20gradually%20avoid%0Aoverlooking%20potential%20true%20positives%20and%20enrich%20the%20latent%20information%20in%20the%0Atarget%20domain.%20Comprehensive%20experiments%20show%20that%20CoS%20essentially%20enhanced%20UDA%0Aperformance%20in%20low-visibility%20conditions%20and%20surpasses%20current%20state-of-the-art%0Atechniques%2C%20achieving%20an%20increase%20in%20mAP%20of%203.0%5C%25%2C%201.9%5C%25%2C%20and%202.5%5C%25%20on%20BDD100K%2C%0ASHIFT%2C%20and%20ACDC%20datasets%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jichengyuan/Cooperitive_Students.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01988v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Students%3A%20Navigating%20Unsupervised%20Domain%20Adaptation%20in%0A%20%20Nighttime%20Object%20Detection&entry.906535625=Jicheng%20Yuan%20and%20Anh%20Le-Tuan%20and%20Manfred%20Hauswirth%20and%20Danh%20Le-Phuoc&entry.1292438233=%20%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20has%20shown%20significant%20advancements%20in%0Aobject%20detection%20under%20well-lit%20conditions%3B%20however%2C%20its%20performance%20degrades%0Anotably%20in%20low-visibility%20scenarios%2C%20especially%20at%20night%2C%20posing%20challenges%20not%0Aonly%20for%20its%20adaptability%20in%20low%20signal-to-noise%20ratio%20%28SNR%29%20conditions%20but%0Aalso%20for%20the%20reliability%20and%20efficiency%20of%20automated%20vehicles.%20To%20address%20this%0Aproblem%2C%20we%20propose%20a%20%5Ctextbf%7BCo%7Doperative%20%5Ctextbf%7BS%7Dtudents%20%28%5Ctextbf%7BCoS%7D%29%0Aframework%20that%20innovatively%20employs%20global-local%20transformations%20%28GLT%29%20and%20a%0Aproxy-based%20target%20consistency%20%28PTC%29%20mechanism%20to%20capture%20the%20spatial%0Aconsistency%20in%20day-%20and%20night-time%20scenarios%20effectively%2C%20and%20thus%20bridge%20the%0Asignificant%20domain%20shift%20across%20contexts.%20Building%20upon%20this%2C%20we%20further%20devise%0Aan%20adaptive%20IoU-informed%20thresholding%20%28AIT%29%20module%20to%20gradually%20avoid%0Aoverlooking%20potential%20true%20positives%20and%20enrich%20the%20latent%20information%20in%20the%0Atarget%20domain.%20Comprehensive%20experiments%20show%20that%20CoS%20essentially%20enhanced%20UDA%0Aperformance%20in%20low-visibility%20conditions%20and%20surpasses%20current%20state-of-the-art%0Atechniques%2C%20achieving%20an%20increase%20in%20mAP%20of%203.0%5C%25%2C%201.9%5C%25%2C%20and%202.5%5C%25%20on%20BDD100K%2C%0ASHIFT%2C%20and%20ACDC%20datasets%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jichengyuan/Cooperitive_Students.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01988v1&entry.124074799=Read"},
{"title": "Africa-Centric Self-Supervised Pre-Training for Multilingual Speech\n  Representation in a Sub-Saharan Context", "author": "Antoine Caubri\u00e8re and Elodie Gauthier", "abstract": "  We present the first self-supervised multilingual speech model trained\nexclusively on African speech. The model learned from nearly 60 000 hours of\nunlabeled speech segments in 21 languages and dialects spoken in sub-Saharan\nAfrica. On the SSA subset of the FLEURS-102 dataset, our approach based on a\nHuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR\ndownstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed\nin the FLEURS benchmark, while being more efficient by using 7x less data and\n6x less parameters. Furthermore, in the context of a LID downstream task, our\napproach outperforms FLEURS baselines accuracy by over 22\\%.\n", "link": "http://arxiv.org/abs/2404.02000v1", "date": "2024-04-02", "relevancy": 2.179, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4577}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4262}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4235}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Africa-Centric%20Self-Supervised%20Pre-Training%20for%20Multilingual%20Speech%0A%20%20Representation%20in%20a%20Sub-Saharan%20Context&body=Title%3A%20Africa-Centric%20Self-Supervised%20Pre-Training%20for%20Multilingual%20Speech%0A%20%20Representation%20in%20a%20Sub-Saharan%20Context%0AAuthor%3A%20Antoine%20Caubri%C3%A8re%20and%20Elodie%20Gauthier%0AAbstract%3A%20%20%20We%20present%20the%20first%20self-supervised%20multilingual%20speech%20model%20trained%0Aexclusively%20on%20African%20speech.%20The%20model%20learned%20from%20nearly%2060%20000%20hours%20of%0Aunlabeled%20speech%20segments%20in%2021%20languages%20and%20dialects%20spoken%20in%20sub-Saharan%0AAfrica.%20On%20the%20SSA%20subset%20of%20the%20FLEURS-102%20dataset%2C%20our%20approach%20based%20on%20a%0AHuBERT%24_%7Bbase%7D%24%20%280.09B%29%20architecture%20shows%20competitive%20results%2C%20for%20ASR%0Adownstream%20task%2C%20compared%20to%20the%20w2v-bert-51%20%280.6B%29%20pre-trained%20model%20proposed%0Ain%20the%20FLEURS%20benchmark%2C%20while%20being%20more%20efficient%20by%20using%207x%20less%20data%20and%0A6x%20less%20parameters.%20Furthermore%2C%20in%20the%20context%20of%20a%20LID%20downstream%20task%2C%20our%0Aapproach%20outperforms%20FLEURS%20baselines%20accuracy%20by%20over%2022%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02000v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Africa-Centric%20Self-Supervised%20Pre-Training%20for%20Multilingual%20Speech%0A%20%20Representation%20in%20a%20Sub-Saharan%20Context&entry.906535625=Antoine%20Caubri%C3%A8re%20and%20Elodie%20Gauthier&entry.1292438233=%20%20We%20present%20the%20first%20self-supervised%20multilingual%20speech%20model%20trained%0Aexclusively%20on%20African%20speech.%20The%20model%20learned%20from%20nearly%2060%20000%20hours%20of%0Aunlabeled%20speech%20segments%20in%2021%20languages%20and%20dialects%20spoken%20in%20sub-Saharan%0AAfrica.%20On%20the%20SSA%20subset%20of%20the%20FLEURS-102%20dataset%2C%20our%20approach%20based%20on%20a%0AHuBERT%24_%7Bbase%7D%24%20%280.09B%29%20architecture%20shows%20competitive%20results%2C%20for%20ASR%0Adownstream%20task%2C%20compared%20to%20the%20w2v-bert-51%20%280.6B%29%20pre-trained%20model%20proposed%0Ain%20the%20FLEURS%20benchmark%2C%20while%20being%20more%20efficient%20by%20using%207x%20less%20data%20and%0A6x%20less%20parameters.%20Furthermore%2C%20in%20the%20context%20of%20a%20LID%20downstream%20task%2C%20our%0Aapproach%20outperforms%20FLEURS%20baselines%20accuracy%20by%20over%2022%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02000v1&entry.124074799=Read"},
{"title": "Iterated Learning Improves Compositionality in Large Vision-Language\n  Models", "author": "Chenhao Zheng and Jieyu Zhang and Aniruddha Kembhavi and Ranjay Krishna", "abstract": "  A fundamental characteristic common to both human vision and natural language\nis their compositional nature. Yet, despite the performance gains contributed\nby large vision and language pretraining, recent investigations find that\nmost-if not all-our state-of-the-art vision-language models struggle at\ncompositionality. They are unable to distinguish between images of \" a girl in\nwhite facing a man in black\" and \"a girl in black facing a man in white\".\nMoreover, prior work suggests that compositionality doesn't arise with scale:\nlarger model sizes or training data don't help. This paper develops a new\niterated training algorithm that incentivizes compositionality. We draw on\ndecades of cognitive science research that identifies cultural transmission-the\nneed to teach a new generation-as a necessary inductive prior that incentivizes\nhumans to develop compositional languages. Specifically, we reframe\nvision-language contrastive learning as the Lewis Signaling Game between a\nvision agent and a language agent, and operationalize cultural transmission by\niteratively resetting one of the agent's weights during training. After every\niteration, this training paradigm induces representations that become \"easier\nto learn\", a property of compositional languages: e.g. our model trained on\nCC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the\nSugarCrepe benchmark.\n", "link": "http://arxiv.org/abs/2404.02145v1", "date": "2024-04-02", "relevancy": 2.1651, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.552}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5377}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5235}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Iterated%20Learning%20Improves%20Compositionality%20in%20Large%20Vision-Language%0A%20%20Models&body=Title%3A%20Iterated%20Learning%20Improves%20Compositionality%20in%20Large%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Chenhao%20Zheng%20and%20Jieyu%20Zhang%20and%20Aniruddha%20Kembhavi%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20A%20fundamental%20characteristic%20common%20to%20both%20human%20vision%20and%20natural%20language%0Ais%20their%20compositional%20nature.%20Yet%2C%20despite%20the%20performance%20gains%20contributed%0Aby%20large%20vision%20and%20language%20pretraining%2C%20recent%20investigations%20find%20that%0Amost-if%20not%20all-our%20state-of-the-art%20vision-language%20models%20struggle%20at%0Acompositionality.%20They%20are%20unable%20to%20distinguish%20between%20images%20of%20%22%20a%20girl%20in%0Awhite%20facing%20a%20man%20in%20black%22%20and%20%22a%20girl%20in%20black%20facing%20a%20man%20in%20white%22.%0AMoreover%2C%20prior%20work%20suggests%20that%20compositionality%20doesn%27t%20arise%20with%20scale%3A%0Alarger%20model%20sizes%20or%20training%20data%20don%27t%20help.%20This%20paper%20develops%20a%20new%0Aiterated%20training%20algorithm%20that%20incentivizes%20compositionality.%20We%20draw%20on%0Adecades%20of%20cognitive%20science%20research%20that%20identifies%20cultural%20transmission-the%0Aneed%20to%20teach%20a%20new%20generation-as%20a%20necessary%20inductive%20prior%20that%20incentivizes%0Ahumans%20to%20develop%20compositional%20languages.%20Specifically%2C%20we%20reframe%0Avision-language%20contrastive%20learning%20as%20the%20Lewis%20Signaling%20Game%20between%20a%0Avision%20agent%20and%20a%20language%20agent%2C%20and%20operationalize%20cultural%20transmission%20by%0Aiteratively%20resetting%20one%20of%20the%20agent%27s%20weights%20during%20training.%20After%20every%0Aiteration%2C%20this%20training%20paradigm%20induces%20representations%20that%20become%20%22easier%0Ato%20learn%22%2C%20a%20property%20of%20compositional%20languages%3A%20e.g.%20our%20model%20trained%20on%0ACC3M%20and%20CC12M%20improves%20standard%20CLIP%20by%204.7%25%2C%204.0%25%20respectfully%20in%20the%0ASugarCrepe%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02145v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterated%20Learning%20Improves%20Compositionality%20in%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Chenhao%20Zheng%20and%20Jieyu%20Zhang%20and%20Aniruddha%20Kembhavi%20and%20Ranjay%20Krishna&entry.1292438233=%20%20A%20fundamental%20characteristic%20common%20to%20both%20human%20vision%20and%20natural%20language%0Ais%20their%20compositional%20nature.%20Yet%2C%20despite%20the%20performance%20gains%20contributed%0Aby%20large%20vision%20and%20language%20pretraining%2C%20recent%20investigations%20find%20that%0Amost-if%20not%20all-our%20state-of-the-art%20vision-language%20models%20struggle%20at%0Acompositionality.%20They%20are%20unable%20to%20distinguish%20between%20images%20of%20%22%20a%20girl%20in%0Awhite%20facing%20a%20man%20in%20black%22%20and%20%22a%20girl%20in%20black%20facing%20a%20man%20in%20white%22.%0AMoreover%2C%20prior%20work%20suggests%20that%20compositionality%20doesn%27t%20arise%20with%20scale%3A%0Alarger%20model%20sizes%20or%20training%20data%20don%27t%20help.%20This%20paper%20develops%20a%20new%0Aiterated%20training%20algorithm%20that%20incentivizes%20compositionality.%20We%20draw%20on%0Adecades%20of%20cognitive%20science%20research%20that%20identifies%20cultural%20transmission-the%0Aneed%20to%20teach%20a%20new%20generation-as%20a%20necessary%20inductive%20prior%20that%20incentivizes%0Ahumans%20to%20develop%20compositional%20languages.%20Specifically%2C%20we%20reframe%0Avision-language%20contrastive%20learning%20as%20the%20Lewis%20Signaling%20Game%20between%20a%0Avision%20agent%20and%20a%20language%20agent%2C%20and%20operationalize%20cultural%20transmission%20by%0Aiteratively%20resetting%20one%20of%20the%20agent%27s%20weights%20during%20training.%20After%20every%0Aiteration%2C%20this%20training%20paradigm%20induces%20representations%20that%20become%20%22easier%0Ato%20learn%22%2C%20a%20property%20of%20compositional%20languages%3A%20e.g.%20our%20model%20trained%20on%0ACC3M%20and%20CC12M%20improves%20standard%20CLIP%20by%204.7%25%2C%204.0%25%20respectfully%20in%20the%0ASugarCrepe%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02145v1&entry.124074799=Read"},
{"title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright\n  Protection for Text-to-Image Generative Models", "author": "Xiang Li and Qianli Shen and Kenji Kawaguchi", "abstract": "  The booming use of text-to-image generative models has raised concerns about\ntheir high risk of producing copyright-infringing content. While probabilistic\ncopyright protection methods provide a probabilistic guarantee against such\ninfringement, in this paper, we introduce Virtually Assured Amplification\nAttack (VA3), a novel online attack framework that exposes the vulnerabilities\nof these protection mechanisms. The proposed framework significantly amplifies\nthe probability of generating infringing content on the sustained interactions\nwith generative models and a non-trivial lower-bound on the success probability\nof each engagement. Our theoretical and experimental results demonstrate the\neffectiveness of our approach under various scenarios. These findings highlight\nthe potential risk of implementing probabilistic copyright protection in\npractical applications of text-to-image generative models. Code is available at\nhttps://github.com/South7X/VA3.\n", "link": "http://arxiv.org/abs/2312.00057v2", "date": "2024-04-02", "relevancy": 2.1577, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5607}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5322}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.521}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VA3%3A%20Virtually%20Assured%20Amplification%20Attack%20on%20Probabilistic%20Copyright%0A%20%20Protection%20for%20Text-to-Image%20Generative%20Models&body=Title%3A%20VA3%3A%20Virtually%20Assured%20Amplification%20Attack%20on%20Probabilistic%20Copyright%0A%20%20Protection%20for%20Text-to-Image%20Generative%20Models%0AAuthor%3A%20Xiang%20Li%20and%20Qianli%20Shen%20and%20Kenji%20Kawaguchi%0AAbstract%3A%20%20%20The%20booming%20use%20of%20text-to-image%20generative%20models%20has%20raised%20concerns%20about%0Atheir%20high%20risk%20of%20producing%20copyright-infringing%20content.%20While%20probabilistic%0Acopyright%20protection%20methods%20provide%20a%20probabilistic%20guarantee%20against%20such%0Ainfringement%2C%20in%20this%20paper%2C%20we%20introduce%20Virtually%20Assured%20Amplification%0AAttack%20%28VA3%29%2C%20a%20novel%20online%20attack%20framework%20that%20exposes%20the%20vulnerabilities%0Aof%20these%20protection%20mechanisms.%20The%20proposed%20framework%20significantly%20amplifies%0Athe%20probability%20of%20generating%20infringing%20content%20on%20the%20sustained%20interactions%0Awith%20generative%20models%20and%20a%20non-trivial%20lower-bound%20on%20the%20success%20probability%0Aof%20each%20engagement.%20Our%20theoretical%20and%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20under%20various%20scenarios.%20These%20findings%20highlight%0Athe%20potential%20risk%20of%20implementing%20probabilistic%20copyright%20protection%20in%0Apractical%20applications%20of%20text-to-image%20generative%20models.%20Code%20is%20available%20at%0Ahttps%3A//github.com/South7X/VA3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00057v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VA3%3A%20Virtually%20Assured%20Amplification%20Attack%20on%20Probabilistic%20Copyright%0A%20%20Protection%20for%20Text-to-Image%20Generative%20Models&entry.906535625=Xiang%20Li%20and%20Qianli%20Shen%20and%20Kenji%20Kawaguchi&entry.1292438233=%20%20The%20booming%20use%20of%20text-to-image%20generative%20models%20has%20raised%20concerns%20about%0Atheir%20high%20risk%20of%20producing%20copyright-infringing%20content.%20While%20probabilistic%0Acopyright%20protection%20methods%20provide%20a%20probabilistic%20guarantee%20against%20such%0Ainfringement%2C%20in%20this%20paper%2C%20we%20introduce%20Virtually%20Assured%20Amplification%0AAttack%20%28VA3%29%2C%20a%20novel%20online%20attack%20framework%20that%20exposes%20the%20vulnerabilities%0Aof%20these%20protection%20mechanisms.%20The%20proposed%20framework%20significantly%20amplifies%0Athe%20probability%20of%20generating%20infringing%20content%20on%20the%20sustained%20interactions%0Awith%20generative%20models%20and%20a%20non-trivial%20lower-bound%20on%20the%20success%20probability%0Aof%20each%20engagement.%20Our%20theoretical%20and%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20under%20various%20scenarios.%20These%20findings%20highlight%0Athe%20potential%20risk%20of%20implementing%20probabilistic%20copyright%20protection%20in%0Apractical%20applications%20of%20text-to-image%20generative%20models.%20Code%20is%20available%20at%0Ahttps%3A//github.com/South7X/VA3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00057v2&entry.124074799=Read"},
{"title": "Neural Ordinary Differential Equation based Sequential Image\n  Registration for Dynamic Characterization", "author": "Yifan Wu and Mengjin Dong and Rohit Jena and Chen Qin and James C. Gee", "abstract": "  Deformable image registration (DIR) is crucial in medical image analysis,\nenabling the exploration of biological dynamics such as organ motions and\nlongitudinal changes in imaging. Leveraging Neural Ordinary Differential\nEquations (ODE) for registration, this extension work discusses how this\nframework can aid in the characterization of sequential biological processes.\nUtilizing the Neural ODE's ability to model state derivatives with neural\nnetworks, our Neural Ordinary Differential Equation Optimization-based (NODEO)\nframework considers voxels as particles within a dynamic system, defining\ndeformation fields through the integration of neural differential equations.\nThis method learns dynamics directly from data, bypassing the need for physical\npriors, making it exceptionally suitable for medical scenarios where such\npriors are unavailable or inapplicable. Consequently, the framework can discern\nunderlying dynamics and use sequence data to regularize the transformation\ntrajectory. We evaluated our framework on two clinical datasets: one for\ncardiac motion tracking and another for longitudinal brain MRI analysis.\nDemonstrating its efficacy in both 2D and 3D imaging scenarios, our framework\noffers flexibility and model agnosticism, capable of managing image sequences\nand facilitating label propagation throughout these sequences. This study\nprovides a comprehensive understanding of how the Neural ODE-based framework\nuniquely benefits the image registration challenge.\n", "link": "http://arxiv.org/abs/2404.02106v1", "date": "2024-04-02", "relevancy": 2.1253, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5456}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5244}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5198}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Ordinary%20Differential%20Equation%20based%20Sequential%20Image%0A%20%20Registration%20for%20Dynamic%20Characterization&body=Title%3A%20Neural%20Ordinary%20Differential%20Equation%20based%20Sequential%20Image%0A%20%20Registration%20for%20Dynamic%20Characterization%0AAuthor%3A%20Yifan%20Wu%20and%20Mengjin%20Dong%20and%20Rohit%20Jena%20and%20Chen%20Qin%20and%20James%20C.%20Gee%0AAbstract%3A%20%20%20Deformable%20image%20registration%20%28DIR%29%20is%20crucial%20in%20medical%20image%20analysis%2C%0Aenabling%20the%20exploration%20of%20biological%20dynamics%20such%20as%20organ%20motions%20and%0Alongitudinal%20changes%20in%20imaging.%20Leveraging%20Neural%20Ordinary%20Differential%0AEquations%20%28ODE%29%20for%20registration%2C%20this%20extension%20work%20discusses%20how%20this%0Aframework%20can%20aid%20in%20the%20characterization%20of%20sequential%20biological%20processes.%0AUtilizing%20the%20Neural%20ODE%27s%20ability%20to%20model%20state%20derivatives%20with%20neural%0Anetworks%2C%20our%20Neural%20Ordinary%20Differential%20Equation%20Optimization-based%20%28NODEO%29%0Aframework%20considers%20voxels%20as%20particles%20within%20a%20dynamic%20system%2C%20defining%0Adeformation%20fields%20through%20the%20integration%20of%20neural%20differential%20equations.%0AThis%20method%20learns%20dynamics%20directly%20from%20data%2C%20bypassing%20the%20need%20for%20physical%0Apriors%2C%20making%20it%20exceptionally%20suitable%20for%20medical%20scenarios%20where%20such%0Apriors%20are%20unavailable%20or%20inapplicable.%20Consequently%2C%20the%20framework%20can%20discern%0Aunderlying%20dynamics%20and%20use%20sequence%20data%20to%20regularize%20the%20transformation%0Atrajectory.%20We%20evaluated%20our%20framework%20on%20two%20clinical%20datasets%3A%20one%20for%0Acardiac%20motion%20tracking%20and%20another%20for%20longitudinal%20brain%20MRI%20analysis.%0ADemonstrating%20its%20efficacy%20in%20both%202D%20and%203D%20imaging%20scenarios%2C%20our%20framework%0Aoffers%20flexibility%20and%20model%20agnosticism%2C%20capable%20of%20managing%20image%20sequences%0Aand%20facilitating%20label%20propagation%20throughout%20these%20sequences.%20This%20study%0Aprovides%20a%20comprehensive%20understanding%20of%20how%20the%20Neural%20ODE-based%20framework%0Auniquely%20benefits%20the%20image%20registration%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02106v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Ordinary%20Differential%20Equation%20based%20Sequential%20Image%0A%20%20Registration%20for%20Dynamic%20Characterization&entry.906535625=Yifan%20Wu%20and%20Mengjin%20Dong%20and%20Rohit%20Jena%20and%20Chen%20Qin%20and%20James%20C.%20Gee&entry.1292438233=%20%20Deformable%20image%20registration%20%28DIR%29%20is%20crucial%20in%20medical%20image%20analysis%2C%0Aenabling%20the%20exploration%20of%20biological%20dynamics%20such%20as%20organ%20motions%20and%0Alongitudinal%20changes%20in%20imaging.%20Leveraging%20Neural%20Ordinary%20Differential%0AEquations%20%28ODE%29%20for%20registration%2C%20this%20extension%20work%20discusses%20how%20this%0Aframework%20can%20aid%20in%20the%20characterization%20of%20sequential%20biological%20processes.%0AUtilizing%20the%20Neural%20ODE%27s%20ability%20to%20model%20state%20derivatives%20with%20neural%0Anetworks%2C%20our%20Neural%20Ordinary%20Differential%20Equation%20Optimization-based%20%28NODEO%29%0Aframework%20considers%20voxels%20as%20particles%20within%20a%20dynamic%20system%2C%20defining%0Adeformation%20fields%20through%20the%20integration%20of%20neural%20differential%20equations.%0AThis%20method%20learns%20dynamics%20directly%20from%20data%2C%20bypassing%20the%20need%20for%20physical%0Apriors%2C%20making%20it%20exceptionally%20suitable%20for%20medical%20scenarios%20where%20such%0Apriors%20are%20unavailable%20or%20inapplicable.%20Consequently%2C%20the%20framework%20can%20discern%0Aunderlying%20dynamics%20and%20use%20sequence%20data%20to%20regularize%20the%20transformation%0Atrajectory.%20We%20evaluated%20our%20framework%20on%20two%20clinical%20datasets%3A%20one%20for%0Acardiac%20motion%20tracking%20and%20another%20for%20longitudinal%20brain%20MRI%20analysis.%0ADemonstrating%20its%20efficacy%20in%20both%202D%20and%203D%20imaging%20scenarios%2C%20our%20framework%0Aoffers%20flexibility%20and%20model%20agnosticism%2C%20capable%20of%20managing%20image%20sequences%0Aand%20facilitating%20label%20propagation%20throughout%20these%20sequences.%20This%20study%0Aprovides%20a%20comprehensive%20understanding%20of%20how%20the%20Neural%20ODE-based%20framework%0Auniquely%20benefits%20the%20image%20registration%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02106v1&entry.124074799=Read"},
{"title": "IISAN: Efficiently Adapting Multimodal Representation for Sequential\n  Recommendation with Decoupled PEFT", "author": "Junchen Fu and Xuri Ge and Xin Xin and Alexandros Karatzoglou and Ioannis Arapakis and Jie Wang and Joemon M Jose", "abstract": "  Multimodal foundation models are transformative in sequential recommender\nsystems, leveraging powerful representation learning capabilities. While\nParameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation\nmodels for recommendation tasks, most research prioritizes parameter\nefficiency, often overlooking critical factors like GPU memory efficiency and\ntraining speed. Addressing this gap, our paper introduces IISAN (Intra- and\nInter-modal Side Adapted Network for Multimodal Representation), a simple\nplug-and-play architecture using a Decoupled PEFT structure and exploiting both\nintra- and inter-modal adaptation.\n  IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art\nPEFT. More importantly, it significantly reduces GPU memory usage - from 47GB\nto just 3GB for multimodal sequential recommendation tasks. Additionally, it\naccelerates training time per epoch from 443s to 22s compared to FFT. This is\nalso a notable improvement over the Adapter and LoRA, which require 37-39 GB\nGPU memory and 350-380 seconds per epoch for training.\n  Furthermore, we propose a new composite efficiency metric, TPME\n(Training-time, Parameter, and GPU Memory Efficiency) to alleviate the\nprevalent misconception that \"parameter efficiency represents overall\nefficiency\". TPME provides more comprehensive insights into practical\nefficiency comparisons between different methods. Besides, we give an\naccessible efficiency analysis of all PEFT and FFT approaches, which\ndemonstrate the superiority of IISAN. We release our codes and other materials\nat https://github.com/jjGenAILab/IISAN.\n", "link": "http://arxiv.org/abs/2404.02059v1", "date": "2024-04-02", "relevancy": 2.1025, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5462}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5253}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5177}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IISAN%3A%20Efficiently%20Adapting%20Multimodal%20Representation%20for%20Sequential%0A%20%20Recommendation%20with%20Decoupled%20PEFT&body=Title%3A%20IISAN%3A%20Efficiently%20Adapting%20Multimodal%20Representation%20for%20Sequential%0A%20%20Recommendation%20with%20Decoupled%20PEFT%0AAuthor%3A%20Junchen%20Fu%20and%20Xuri%20Ge%20and%20Xin%20Xin%20and%20Alexandros%20Karatzoglou%20and%20Ioannis%20Arapakis%20and%20Jie%20Wang%20and%20Joemon%20M%20Jose%0AAbstract%3A%20%20%20Multimodal%20foundation%20models%20are%20transformative%20in%20sequential%20recommender%0Asystems%2C%20leveraging%20powerful%20representation%20learning%20capabilities.%20While%0AParameter-efficient%20Fine-tuning%20%28PEFT%29%20is%20commonly%20used%20to%20adapt%20foundation%0Amodels%20for%20recommendation%20tasks%2C%20most%20research%20prioritizes%20parameter%0Aefficiency%2C%20often%20overlooking%20critical%20factors%20like%20GPU%20memory%20efficiency%20and%0Atraining%20speed.%20Addressing%20this%20gap%2C%20our%20paper%20introduces%20IISAN%20%28Intra-%20and%0AInter-modal%20Side%20Adapted%20Network%20for%20Multimodal%20Representation%29%2C%20a%20simple%0Aplug-and-play%20architecture%20using%20a%20Decoupled%20PEFT%20structure%20and%20exploiting%20both%0Aintra-%20and%20inter-modal%20adaptation.%0A%20%20IISAN%20matches%20the%20performance%20of%20full%20fine-tuning%20%28FFT%29%20and%20state-of-the-art%0APEFT.%20More%20importantly%2C%20it%20significantly%20reduces%20GPU%20memory%20usage%20-%20from%2047GB%0Ato%20just%203GB%20for%20multimodal%20sequential%20recommendation%20tasks.%20Additionally%2C%20it%0Aaccelerates%20training%20time%20per%20epoch%20from%20443s%20to%2022s%20compared%20to%20FFT.%20This%20is%0Aalso%20a%20notable%20improvement%20over%20the%20Adapter%20and%20LoRA%2C%20which%20require%2037-39%20GB%0AGPU%20memory%20and%20350-380%20seconds%20per%20epoch%20for%20training.%0A%20%20Furthermore%2C%20we%20propose%20a%20new%20composite%20efficiency%20metric%2C%20TPME%0A%28Training-time%2C%20Parameter%2C%20and%20GPU%20Memory%20Efficiency%29%20to%20alleviate%20the%0Aprevalent%20misconception%20that%20%22parameter%20efficiency%20represents%20overall%0Aefficiency%22.%20TPME%20provides%20more%20comprehensive%20insights%20into%20practical%0Aefficiency%20comparisons%20between%20different%20methods.%20Besides%2C%20we%20give%20an%0Aaccessible%20efficiency%20analysis%20of%20all%20PEFT%20and%20FFT%20approaches%2C%20which%0Ademonstrate%20the%20superiority%20of%20IISAN.%20We%20release%20our%20codes%20and%20other%20materials%0Aat%20https%3A//github.com/jjGenAILab/IISAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02059v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IISAN%3A%20Efficiently%20Adapting%20Multimodal%20Representation%20for%20Sequential%0A%20%20Recommendation%20with%20Decoupled%20PEFT&entry.906535625=Junchen%20Fu%20and%20Xuri%20Ge%20and%20Xin%20Xin%20and%20Alexandros%20Karatzoglou%20and%20Ioannis%20Arapakis%20and%20Jie%20Wang%20and%20Joemon%20M%20Jose&entry.1292438233=%20%20Multimodal%20foundation%20models%20are%20transformative%20in%20sequential%20recommender%0Asystems%2C%20leveraging%20powerful%20representation%20learning%20capabilities.%20While%0AParameter-efficient%20Fine-tuning%20%28PEFT%29%20is%20commonly%20used%20to%20adapt%20foundation%0Amodels%20for%20recommendation%20tasks%2C%20most%20research%20prioritizes%20parameter%0Aefficiency%2C%20often%20overlooking%20critical%20factors%20like%20GPU%20memory%20efficiency%20and%0Atraining%20speed.%20Addressing%20this%20gap%2C%20our%20paper%20introduces%20IISAN%20%28Intra-%20and%0AInter-modal%20Side%20Adapted%20Network%20for%20Multimodal%20Representation%29%2C%20a%20simple%0Aplug-and-play%20architecture%20using%20a%20Decoupled%20PEFT%20structure%20and%20exploiting%20both%0Aintra-%20and%20inter-modal%20adaptation.%0A%20%20IISAN%20matches%20the%20performance%20of%20full%20fine-tuning%20%28FFT%29%20and%20state-of-the-art%0APEFT.%20More%20importantly%2C%20it%20significantly%20reduces%20GPU%20memory%20usage%20-%20from%2047GB%0Ato%20just%203GB%20for%20multimodal%20sequential%20recommendation%20tasks.%20Additionally%2C%20it%0Aaccelerates%20training%20time%20per%20epoch%20from%20443s%20to%2022s%20compared%20to%20FFT.%20This%20is%0Aalso%20a%20notable%20improvement%20over%20the%20Adapter%20and%20LoRA%2C%20which%20require%2037-39%20GB%0AGPU%20memory%20and%20350-380%20seconds%20per%20epoch%20for%20training.%0A%20%20Furthermore%2C%20we%20propose%20a%20new%20composite%20efficiency%20metric%2C%20TPME%0A%28Training-time%2C%20Parameter%2C%20and%20GPU%20Memory%20Efficiency%29%20to%20alleviate%20the%0Aprevalent%20misconception%20that%20%22parameter%20efficiency%20represents%20overall%0Aefficiency%22.%20TPME%20provides%20more%20comprehensive%20insights%20into%20practical%0Aefficiency%20comparisons%20between%20different%20methods.%20Besides%2C%20we%20give%20an%0Aaccessible%20efficiency%20analysis%20of%20all%20PEFT%20and%20FFT%20approaches%2C%20which%0Ademonstrate%20the%20superiority%20of%20IISAN.%20We%20release%20our%20codes%20and%20other%20materials%0Aat%20https%3A//github.com/jjGenAILab/IISAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02059v1&entry.124074799=Read"},
{"title": "Specularity Factorization for Low-Light Enhancement", "author": "Saurabh Saini and P J Narayanan", "abstract": "  We present a new additive image factorization technique that treats images to\nbe composed of multiple latent specular components which can be simply\nestimated recursively by modulating the sparsity during decomposition. Our\nmodel-driven {\\em RSFNet} estimates these factors by unrolling the optimization\ninto network layers requiring only a few scalars to be learned. The resultant\nfactors are interpretable by design and can be fused for different image\nenhancement tasks via a network or combined directly by the user in a\ncontrollable fashion. Based on RSFNet, we detail a zero-reference Low Light\nEnhancement (LLE) application trained without paired or unpaired supervision.\nOur system improves the state-of-the-art performance on standard benchmarks and\nachieves better generalization on multiple other datasets. We also integrate\nour factors with other task specific fusion networks for applications like\nderaining, deblurring and dehazing with negligible overhead thereby\nhighlighting the multi-domain and multi-task generalizability of our proposed\nRSFNet. The code and data is released for reproducibility on the project\nhomepage.\n", "link": "http://arxiv.org/abs/2404.01998v1", "date": "2024-04-02", "relevancy": 2.0792, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5797}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5101}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5056}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Specularity%20Factorization%20for%20Low-Light%20Enhancement&body=Title%3A%20Specularity%20Factorization%20for%20Low-Light%20Enhancement%0AAuthor%3A%20Saurabh%20Saini%20and%20P%20J%20Narayanan%0AAbstract%3A%20%20%20We%20present%20a%20new%20additive%20image%20factorization%20technique%20that%20treats%20images%20to%0Abe%20composed%20of%20multiple%20latent%20specular%20components%20which%20can%20be%20simply%0Aestimated%20recursively%20by%20modulating%20the%20sparsity%20during%20decomposition.%20Our%0Amodel-driven%20%7B%5Cem%20RSFNet%7D%20estimates%20these%20factors%20by%20unrolling%20the%20optimization%0Ainto%20network%20layers%20requiring%20only%20a%20few%20scalars%20to%20be%20learned.%20The%20resultant%0Afactors%20are%20interpretable%20by%20design%20and%20can%20be%20fused%20for%20different%20image%0Aenhancement%20tasks%20via%20a%20network%20or%20combined%20directly%20by%20the%20user%20in%20a%0Acontrollable%20fashion.%20Based%20on%20RSFNet%2C%20we%20detail%20a%20zero-reference%20Low%20Light%0AEnhancement%20%28LLE%29%20application%20trained%20without%20paired%20or%20unpaired%20supervision.%0AOur%20system%20improves%20the%20state-of-the-art%20performance%20on%20standard%20benchmarks%20and%0Aachieves%20better%20generalization%20on%20multiple%20other%20datasets.%20We%20also%20integrate%0Aour%20factors%20with%20other%20task%20specific%20fusion%20networks%20for%20applications%20like%0Aderaining%2C%20deblurring%20and%20dehazing%20with%20negligible%20overhead%20thereby%0Ahighlighting%20the%20multi-domain%20and%20multi-task%20generalizability%20of%20our%20proposed%0ARSFNet.%20The%20code%20and%20data%20is%20released%20for%20reproducibility%20on%20the%20project%0Ahomepage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01998v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Specularity%20Factorization%20for%20Low-Light%20Enhancement&entry.906535625=Saurabh%20Saini%20and%20P%20J%20Narayanan&entry.1292438233=%20%20We%20present%20a%20new%20additive%20image%20factorization%20technique%20that%20treats%20images%20to%0Abe%20composed%20of%20multiple%20latent%20specular%20components%20which%20can%20be%20simply%0Aestimated%20recursively%20by%20modulating%20the%20sparsity%20during%20decomposition.%20Our%0Amodel-driven%20%7B%5Cem%20RSFNet%7D%20estimates%20these%20factors%20by%20unrolling%20the%20optimization%0Ainto%20network%20layers%20requiring%20only%20a%20few%20scalars%20to%20be%20learned.%20The%20resultant%0Afactors%20are%20interpretable%20by%20design%20and%20can%20be%20fused%20for%20different%20image%0Aenhancement%20tasks%20via%20a%20network%20or%20combined%20directly%20by%20the%20user%20in%20a%0Acontrollable%20fashion.%20Based%20on%20RSFNet%2C%20we%20detail%20a%20zero-reference%20Low%20Light%0AEnhancement%20%28LLE%29%20application%20trained%20without%20paired%20or%20unpaired%20supervision.%0AOur%20system%20improves%20the%20state-of-the-art%20performance%20on%20standard%20benchmarks%20and%0Aachieves%20better%20generalization%20on%20multiple%20other%20datasets.%20We%20also%20integrate%0Aour%20factors%20with%20other%20task%20specific%20fusion%20networks%20for%20applications%20like%0Aderaining%2C%20deblurring%20and%20dehazing%20with%20negligible%20overhead%20thereby%0Ahighlighting%20the%20multi-domain%20and%20multi-task%20generalizability%20of%20our%20proposed%0ARSFNet.%20The%20code%20and%20data%20is%20released%20for%20reproducibility%20on%20the%20project%0Ahomepage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01998v1&entry.124074799=Read"},
{"title": "Pre-trained Vision and Language Transformers Are Few-Shot Incremental\n  Learners", "author": "Keon-Hee Park and Kyungwoo Song and Gyeong-Moon Park", "abstract": "  Few-Shot Class Incremental Learning (FSCIL) is a task that requires a model\nto learn new classes incrementally without forgetting when only a few samples\nfor each class are given. FSCIL encounters two significant challenges:\ncatastrophic forgetting and overfitting, and these challenges have driven prior\nstudies to primarily rely on shallow models, such as ResNet-18. Even though\ntheir limited capacity can mitigate both forgetting and overfitting issues, it\nleads to inadequate knowledge transfer during few-shot incremental sessions. In\nthis paper, we argue that large models such as vision and language transformers\npre-trained on large datasets can be excellent few-shot incremental learners.\nTo this end, we propose a novel FSCIL framework called PriViLege, Pre-trained\nVision and Language transformers with prompting functions and knowledge\ndistillation. Our framework effectively addresses the challenges of\ncatastrophic forgetting and overfitting in large models through new pre-trained\nknowledge tuning (PKT) and two losses: entropy-based divergence loss and\nsemantic knowledge distillation loss. Experimental results show that the\nproposed PriViLege significantly outperforms the existing state-of-the-art\nmethods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and\n+13.36% in miniImageNet. Our implementation code is available at\nhttps://github.com/KHU-AGI/PriViLege.\n", "link": "http://arxiv.org/abs/2404.02117v1", "date": "2024-04-02", "relevancy": 2.0716, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5257}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5022}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pre-trained%20Vision%20and%20Language%20Transformers%20Are%20Few-Shot%20Incremental%0A%20%20Learners&body=Title%3A%20Pre-trained%20Vision%20and%20Language%20Transformers%20Are%20Few-Shot%20Incremental%0A%20%20Learners%0AAuthor%3A%20Keon-Hee%20Park%20and%20Kyungwoo%20Song%20and%20Gyeong-Moon%20Park%0AAbstract%3A%20%20%20Few-Shot%20Class%20Incremental%20Learning%20%28FSCIL%29%20is%20a%20task%20that%20requires%20a%20model%0Ato%20learn%20new%20classes%20incrementally%20without%20forgetting%20when%20only%20a%20few%20samples%0Afor%20each%20class%20are%20given.%20FSCIL%20encounters%20two%20significant%20challenges%3A%0Acatastrophic%20forgetting%20and%20overfitting%2C%20and%20these%20challenges%20have%20driven%20prior%0Astudies%20to%20primarily%20rely%20on%20shallow%20models%2C%20such%20as%20ResNet-18.%20Even%20though%0Atheir%20limited%20capacity%20can%20mitigate%20both%20forgetting%20and%20overfitting%20issues%2C%20it%0Aleads%20to%20inadequate%20knowledge%20transfer%20during%20few-shot%20incremental%20sessions.%20In%0Athis%20paper%2C%20we%20argue%20that%20large%20models%20such%20as%20vision%20and%20language%20transformers%0Apre-trained%20on%20large%20datasets%20can%20be%20excellent%20few-shot%20incremental%20learners.%0ATo%20this%20end%2C%20we%20propose%20a%20novel%20FSCIL%20framework%20called%20PriViLege%2C%20Pre-trained%0AVision%20and%20Language%20transformers%20with%20prompting%20functions%20and%20knowledge%0Adistillation.%20Our%20framework%20effectively%20addresses%20the%20challenges%20of%0Acatastrophic%20forgetting%20and%20overfitting%20in%20large%20models%20through%20new%20pre-trained%0Aknowledge%20tuning%20%28PKT%29%20and%20two%20losses%3A%20entropy-based%20divergence%20loss%20and%0Asemantic%20knowledge%20distillation%20loss.%20Experimental%20results%20show%20that%20the%0Aproposed%20PriViLege%20significantly%20outperforms%20the%20existing%20state-of-the-art%0Amethods%20with%20a%20large%20margin%2C%20e.g.%2C%20%2B9.38%25%20in%20CUB200%2C%20%2B20.58%25%20in%20CIFAR-100%2C%20and%0A%2B13.36%25%20in%20miniImageNet.%20Our%20implementation%20code%20is%20available%20at%0Ahttps%3A//github.com/KHU-AGI/PriViLege.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02117v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-trained%20Vision%20and%20Language%20Transformers%20Are%20Few-Shot%20Incremental%0A%20%20Learners&entry.906535625=Keon-Hee%20Park%20and%20Kyungwoo%20Song%20and%20Gyeong-Moon%20Park&entry.1292438233=%20%20Few-Shot%20Class%20Incremental%20Learning%20%28FSCIL%29%20is%20a%20task%20that%20requires%20a%20model%0Ato%20learn%20new%20classes%20incrementally%20without%20forgetting%20when%20only%20a%20few%20samples%0Afor%20each%20class%20are%20given.%20FSCIL%20encounters%20two%20significant%20challenges%3A%0Acatastrophic%20forgetting%20and%20overfitting%2C%20and%20these%20challenges%20have%20driven%20prior%0Astudies%20to%20primarily%20rely%20on%20shallow%20models%2C%20such%20as%20ResNet-18.%20Even%20though%0Atheir%20limited%20capacity%20can%20mitigate%20both%20forgetting%20and%20overfitting%20issues%2C%20it%0Aleads%20to%20inadequate%20knowledge%20transfer%20during%20few-shot%20incremental%20sessions.%20In%0Athis%20paper%2C%20we%20argue%20that%20large%20models%20such%20as%20vision%20and%20language%20transformers%0Apre-trained%20on%20large%20datasets%20can%20be%20excellent%20few-shot%20incremental%20learners.%0ATo%20this%20end%2C%20we%20propose%20a%20novel%20FSCIL%20framework%20called%20PriViLege%2C%20Pre-trained%0AVision%20and%20Language%20transformers%20with%20prompting%20functions%20and%20knowledge%0Adistillation.%20Our%20framework%20effectively%20addresses%20the%20challenges%20of%0Acatastrophic%20forgetting%20and%20overfitting%20in%20large%20models%20through%20new%20pre-trained%0Aknowledge%20tuning%20%28PKT%29%20and%20two%20losses%3A%20entropy-based%20divergence%20loss%20and%0Asemantic%20knowledge%20distillation%20loss.%20Experimental%20results%20show%20that%20the%0Aproposed%20PriViLege%20significantly%20outperforms%20the%20existing%20state-of-the-art%0Amethods%20with%20a%20large%20margin%2C%20e.g.%2C%20%2B9.38%25%20in%20CUB200%2C%20%2B20.58%25%20in%20CIFAR-100%2C%20and%0A%2B13.36%25%20in%20miniImageNet.%20Our%20implementation%20code%20is%20available%20at%0Ahttps%3A//github.com/KHU-AGI/PriViLege.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02117v1&entry.124074799=Read"},
{"title": "Universal representations for financial transactional data: embracing\n  local, global, and external contexts", "author": "Alexandra Bazarova and Maria Kovaleva and Ilya Kuleshov and Evgenia Romanenkova and Alexander Stepikin and Alexandr Yugay and Dzhambulat Mollaev and Ivan Kireev and Andrey Savchenko and Alexey Zaytsev", "abstract": "  Effective processing of financial transactions is essential for banking data\nanalysis. However, in this domain, most methods focus on specialized solutions\nto stand-alone problems instead of constructing universal representations\nsuitable for many problems. We present a representation learning framework that\naddresses diverse business challenges. We also suggest novel generative models\nthat account for data specifics, and a way to integrate external information\ninto a client's representation, leveraging insights from other customers'\nactions. Finally, we offer a benchmark, describing representation quality\nglobally, concerning the entire transaction history; locally, reflecting the\nclient's current state; and dynamically, capturing representation evolution\nover time. Our generative approach demonstrates superior performance in local\ntasks, with an increase in ROC-AUC of up to 14\\% for the next MCC prediction\ntask and up to 46\\% for downstream tasks from existing contrastive baselines.\nIncorporating external information improves the scores by an additional 20\\%.\n", "link": "http://arxiv.org/abs/2404.02047v1", "date": "2024-04-02", "relevancy": 2.0106, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4915}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4889}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Universal%20representations%20for%20financial%20transactional%20data%3A%20embracing%0A%20%20local%2C%20global%2C%20and%20external%20contexts&body=Title%3A%20Universal%20representations%20for%20financial%20transactional%20data%3A%20embracing%0A%20%20local%2C%20global%2C%20and%20external%20contexts%0AAuthor%3A%20Alexandra%20Bazarova%20and%20Maria%20Kovaleva%20and%20Ilya%20Kuleshov%20and%20Evgenia%20Romanenkova%20and%20Alexander%20Stepikin%20and%20Alexandr%20Yugay%20and%20Dzhambulat%20Mollaev%20and%20Ivan%20Kireev%20and%20Andrey%20Savchenko%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20Effective%20processing%20of%20financial%20transactions%20is%20essential%20for%20banking%20data%0Aanalysis.%20However%2C%20in%20this%20domain%2C%20most%20methods%20focus%20on%20specialized%20solutions%0Ato%20stand-alone%20problems%20instead%20of%20constructing%20universal%20representations%0Asuitable%20for%20many%20problems.%20We%20present%20a%20representation%20learning%20framework%20that%0Aaddresses%20diverse%20business%20challenges.%20We%20also%20suggest%20novel%20generative%20models%0Athat%20account%20for%20data%20specifics%2C%20and%20a%20way%20to%20integrate%20external%20information%0Ainto%20a%20client%27s%20representation%2C%20leveraging%20insights%20from%20other%20customers%27%0Aactions.%20Finally%2C%20we%20offer%20a%20benchmark%2C%20describing%20representation%20quality%0Aglobally%2C%20concerning%20the%20entire%20transaction%20history%3B%20locally%2C%20reflecting%20the%0Aclient%27s%20current%20state%3B%20and%20dynamically%2C%20capturing%20representation%20evolution%0Aover%20time.%20Our%20generative%20approach%20demonstrates%20superior%20performance%20in%20local%0Atasks%2C%20with%20an%20increase%20in%20ROC-AUC%20of%20up%20to%2014%5C%25%20for%20the%20next%20MCC%20prediction%0Atask%20and%20up%20to%2046%5C%25%20for%20downstream%20tasks%20from%20existing%20contrastive%20baselines.%0AIncorporating%20external%20information%20improves%20the%20scores%20by%20an%20additional%2020%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02047v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20representations%20for%20financial%20transactional%20data%3A%20embracing%0A%20%20local%2C%20global%2C%20and%20external%20contexts&entry.906535625=Alexandra%20Bazarova%20and%20Maria%20Kovaleva%20and%20Ilya%20Kuleshov%20and%20Evgenia%20Romanenkova%20and%20Alexander%20Stepikin%20and%20Alexandr%20Yugay%20and%20Dzhambulat%20Mollaev%20and%20Ivan%20Kireev%20and%20Andrey%20Savchenko%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20Effective%20processing%20of%20financial%20transactions%20is%20essential%20for%20banking%20data%0Aanalysis.%20However%2C%20in%20this%20domain%2C%20most%20methods%20focus%20on%20specialized%20solutions%0Ato%20stand-alone%20problems%20instead%20of%20constructing%20universal%20representations%0Asuitable%20for%20many%20problems.%20We%20present%20a%20representation%20learning%20framework%20that%0Aaddresses%20diverse%20business%20challenges.%20We%20also%20suggest%20novel%20generative%20models%0Athat%20account%20for%20data%20specifics%2C%20and%20a%20way%20to%20integrate%20external%20information%0Ainto%20a%20client%27s%20representation%2C%20leveraging%20insights%20from%20other%20customers%27%0Aactions.%20Finally%2C%20we%20offer%20a%20benchmark%2C%20describing%20representation%20quality%0Aglobally%2C%20concerning%20the%20entire%20transaction%20history%3B%20locally%2C%20reflecting%20the%0Aclient%27s%20current%20state%3B%20and%20dynamically%2C%20capturing%20representation%20evolution%0Aover%20time.%20Our%20generative%20approach%20demonstrates%20superior%20performance%20in%20local%0Atasks%2C%20with%20an%20increase%20in%20ROC-AUC%20of%20up%20to%2014%5C%25%20for%20the%20next%20MCC%20prediction%0Atask%20and%20up%20to%2046%5C%25%20for%20downstream%20tasks%20from%20existing%20contrastive%20baselines.%0AIncorporating%20external%20information%20improves%20the%20scores%20by%20an%20additional%2020%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02047v1&entry.124074799=Read"},
{"title": "MAgNET: A Graph U-Net Architecture for Mesh-Based Simulations", "author": "Saurabh Deshpande and St\u00e9phane P. A. Bordas and Jakub Lengiewicz", "abstract": "  In many cutting-edge applications, high-fidelity computational models prove\nto be too slow for practical use and are therefore replaced by much faster\nsurrogate models. Recently, deep learning techniques have increasingly been\nutilized to accelerate such predictions. To enable learning on\nlarge-dimensional and complex data, specific neural network architectures have\nbeen developed, including convolutional and graph neural networks. In this\nwork, we present a novel encoder-decoder geometric deep learning framework\ncalled MAgNET, which extends the well-known convolutional neural networks to\naccommodate arbitrary graph-structured data. MAgNET consists of innovative\nMultichannel Aggregation (MAg) layers and graph pooling/unpooling layers,\nforming a graph U-Net architecture that is analogous to convolutional U-Nets.\nWe demonstrate the predictive capabilities of MAgNET in surrogate modeling for\nnon-linear finite element simulations in the mechanics of solids.\n", "link": "http://arxiv.org/abs/2211.00713v3", "date": "2024-04-02", "relevancy": 2.0061, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5114}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5016}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4975}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MAgNET%3A%20A%20Graph%20U-Net%20Architecture%20for%20Mesh-Based%20Simulations&body=Title%3A%20MAgNET%3A%20A%20Graph%20U-Net%20Architecture%20for%20Mesh-Based%20Simulations%0AAuthor%3A%20Saurabh%20Deshpande%20and%20St%C3%A9phane%20P.%20A.%20Bordas%20and%20Jakub%20Lengiewicz%0AAbstract%3A%20%20%20In%20many%20cutting-edge%20applications%2C%20high-fidelity%20computational%20models%20prove%0Ato%20be%20too%20slow%20for%20practical%20use%20and%20are%20therefore%20replaced%20by%20much%20faster%0Asurrogate%20models.%20Recently%2C%20deep%20learning%20techniques%20have%20increasingly%20been%0Autilized%20to%20accelerate%20such%20predictions.%20To%20enable%20learning%20on%0Alarge-dimensional%20and%20complex%20data%2C%20specific%20neural%20network%20architectures%20have%0Abeen%20developed%2C%20including%20convolutional%20and%20graph%20neural%20networks.%20In%20this%0Awork%2C%20we%20present%20a%20novel%20encoder-decoder%20geometric%20deep%20learning%20framework%0Acalled%20MAgNET%2C%20which%20extends%20the%20well-known%20convolutional%20neural%20networks%20to%0Aaccommodate%20arbitrary%20graph-structured%20data.%20MAgNET%20consists%20of%20innovative%0AMultichannel%20Aggregation%20%28MAg%29%20layers%20and%20graph%20pooling/unpooling%20layers%2C%0Aforming%20a%20graph%20U-Net%20architecture%20that%20is%20analogous%20to%20convolutional%20U-Nets.%0AWe%20demonstrate%20the%20predictive%20capabilities%20of%20MAgNET%20in%20surrogate%20modeling%20for%0Anon-linear%20finite%20element%20simulations%20in%20the%20mechanics%20of%20solids.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.00713v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAgNET%3A%20A%20Graph%20U-Net%20Architecture%20for%20Mesh-Based%20Simulations&entry.906535625=Saurabh%20Deshpande%20and%20St%C3%A9phane%20P.%20A.%20Bordas%20and%20Jakub%20Lengiewicz&entry.1292438233=%20%20In%20many%20cutting-edge%20applications%2C%20high-fidelity%20computational%20models%20prove%0Ato%20be%20too%20slow%20for%20practical%20use%20and%20are%20therefore%20replaced%20by%20much%20faster%0Asurrogate%20models.%20Recently%2C%20deep%20learning%20techniques%20have%20increasingly%20been%0Autilized%20to%20accelerate%20such%20predictions.%20To%20enable%20learning%20on%0Alarge-dimensional%20and%20complex%20data%2C%20specific%20neural%20network%20architectures%20have%0Abeen%20developed%2C%20including%20convolutional%20and%20graph%20neural%20networks.%20In%20this%0Awork%2C%20we%20present%20a%20novel%20encoder-decoder%20geometric%20deep%20learning%20framework%0Acalled%20MAgNET%2C%20which%20extends%20the%20well-known%20convolutional%20neural%20networks%20to%0Aaccommodate%20arbitrary%20graph-structured%20data.%20MAgNET%20consists%20of%20innovative%0AMultichannel%20Aggregation%20%28MAg%29%20layers%20and%20graph%20pooling/unpooling%20layers%2C%0Aforming%20a%20graph%20U-Net%20architecture%20that%20is%20analogous%20to%20convolutional%20U-Nets.%0AWe%20demonstrate%20the%20predictive%20capabilities%20of%20MAgNET%20in%20surrogate%20modeling%20for%0Anon-linear%20finite%20element%20simulations%20in%20the%20mechanics%20of%20solids.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.00713v3&entry.124074799=Read"},
{"title": "Long-context LLMs Struggle with Long In-context Learning", "author": "Tianle Li and Ge Zhang and Quy Duc Do and Xiang Yue and Wenhu Chen", "abstract": "  Large Language Models (LLMs) have made significant strides in handling long\nsequences exceeding 32K tokens. However, their performance evaluation has\nlargely been confined to metrics like perplexity and synthetic tasks, which may\nnot fully capture their abilities in more nuanced, real-world scenarios. This\nstudy introduces a specialized benchmark (LIConBench) focusing on long\nin-context learning within the realm of extreme-label classification. We\nmeticulously selected six datasets with a label range spanning 28 to 174\nclasses covering different input (few-shot demonstration) length from 2K to\n50K. Our benchmark requires LLMs to comprehend the entire input to recognize\nthe massive label spaces to make correct prediction. We evaluate 13\nlong-context LLMs on our benchmarks. We find that the long-context LLMs perform\nrelatively well under the token length of 20K and the performance benefits from\nutilizing the long context window. However, after the context window exceeds\n20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap\nin current LLM capabilities for processing and understanding long, context-rich\nsequences. Further analysis revealed a tendency among models to favor\npredictions for labels presented towards the end at the sequence. Their ability\nto reason over multiple pieces in the long sequence is yet to be improved. Our\nstudy reveals that long context understanding and reasoning is still a\nchallenging task for the existing LLMs. We believe LIConBench could serve as a\nmore realistic evaluation for the future long context LLMs.\n", "link": "http://arxiv.org/abs/2404.02060v1", "date": "2024-04-02", "relevancy": 1.9755, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5057}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4861}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4838}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Long-context%20LLMs%20Struggle%20with%20Long%20In-context%20Learning&body=Title%3A%20Long-context%20LLMs%20Struggle%20with%20Long%20In-context%20Learning%0AAuthor%3A%20Tianle%20Li%20and%20Ge%20Zhang%20and%20Quy%20Duc%20Do%20and%20Xiang%20Yue%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20handling%20long%0Asequences%20exceeding%2032K%20tokens.%20However%2C%20their%20performance%20evaluation%20has%0Alargely%20been%20confined%20to%20metrics%20like%20perplexity%20and%20synthetic%20tasks%2C%20which%20may%0Anot%20fully%20capture%20their%20abilities%20in%20more%20nuanced%2C%20real-world%20scenarios.%20This%0Astudy%20introduces%20a%20specialized%20benchmark%20%28LIConBench%29%20focusing%20on%20long%0Ain-context%20learning%20within%20the%20realm%20of%20extreme-label%20classification.%20We%0Ameticulously%20selected%20six%20datasets%20with%20a%20label%20range%20spanning%2028%20to%20174%0Aclasses%20covering%20different%20input%20%28few-shot%20demonstration%29%20length%20from%202K%20to%0A50K.%20Our%20benchmark%20requires%20LLMs%20to%20comprehend%20the%20entire%20input%20to%20recognize%0Athe%20massive%20label%20spaces%20to%20make%20correct%20prediction.%20We%20evaluate%2013%0Along-context%20LLMs%20on%20our%20benchmarks.%20We%20find%20that%20the%20long-context%20LLMs%20perform%0Arelatively%20well%20under%20the%20token%20length%20of%2020K%20and%20the%20performance%20benefits%20from%0Autilizing%20the%20long%20context%20window.%20However%2C%20after%20the%20context%20window%20exceeds%0A20K%2C%20most%20LLMs%20except%20GPT-4%20will%20dip%20dramatically.%20This%20suggests%20a%20notable%20gap%0Ain%20current%20LLM%20capabilities%20for%20processing%20and%20understanding%20long%2C%20context-rich%0Asequences.%20Further%20analysis%20revealed%20a%20tendency%20among%20models%20to%20favor%0Apredictions%20for%20labels%20presented%20towards%20the%20end%20at%20the%20sequence.%20Their%20ability%0Ato%20reason%20over%20multiple%20pieces%20in%20the%20long%20sequence%20is%20yet%20to%20be%20improved.%20Our%0Astudy%20reveals%20that%20long%20context%20understanding%20and%20reasoning%20is%20still%20a%0Achallenging%20task%20for%20the%20existing%20LLMs.%20We%20believe%20LIConBench%20could%20serve%20as%20a%0Amore%20realistic%20evaluation%20for%20the%20future%20long%20context%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02060v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-context%20LLMs%20Struggle%20with%20Long%20In-context%20Learning&entry.906535625=Tianle%20Li%20and%20Ge%20Zhang%20and%20Quy%20Duc%20Do%20and%20Xiang%20Yue%20and%20Wenhu%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20handling%20long%0Asequences%20exceeding%2032K%20tokens.%20However%2C%20their%20performance%20evaluation%20has%0Alargely%20been%20confined%20to%20metrics%20like%20perplexity%20and%20synthetic%20tasks%2C%20which%20may%0Anot%20fully%20capture%20their%20abilities%20in%20more%20nuanced%2C%20real-world%20scenarios.%20This%0Astudy%20introduces%20a%20specialized%20benchmark%20%28LIConBench%29%20focusing%20on%20long%0Ain-context%20learning%20within%20the%20realm%20of%20extreme-label%20classification.%20We%0Ameticulously%20selected%20six%20datasets%20with%20a%20label%20range%20spanning%2028%20to%20174%0Aclasses%20covering%20different%20input%20%28few-shot%20demonstration%29%20length%20from%202K%20to%0A50K.%20Our%20benchmark%20requires%20LLMs%20to%20comprehend%20the%20entire%20input%20to%20recognize%0Athe%20massive%20label%20spaces%20to%20make%20correct%20prediction.%20We%20evaluate%2013%0Along-context%20LLMs%20on%20our%20benchmarks.%20We%20find%20that%20the%20long-context%20LLMs%20perform%0Arelatively%20well%20under%20the%20token%20length%20of%2020K%20and%20the%20performance%20benefits%20from%0Autilizing%20the%20long%20context%20window.%20However%2C%20after%20the%20context%20window%20exceeds%0A20K%2C%20most%20LLMs%20except%20GPT-4%20will%20dip%20dramatically.%20This%20suggests%20a%20notable%20gap%0Ain%20current%20LLM%20capabilities%20for%20processing%20and%20understanding%20long%2C%20context-rich%0Asequences.%20Further%20analysis%20revealed%20a%20tendency%20among%20models%20to%20favor%0Apredictions%20for%20labels%20presented%20towards%20the%20end%20at%20the%20sequence.%20Their%20ability%0Ato%20reason%20over%20multiple%20pieces%20in%20the%20long%20sequence%20is%20yet%20to%20be%20improved.%20Our%0Astudy%20reveals%20that%20long%20context%20understanding%20and%20reasoning%20is%20still%20a%0Achallenging%20task%20for%20the%20existing%20LLMs.%20We%20believe%20LIConBench%20could%20serve%20as%20a%0Amore%20realistic%20evaluation%20for%20the%20future%20long%20context%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02060v1&entry.124074799=Read"},
{"title": "Enhancing wind field resolution in complex terrain through a\n  knowledge-driven machine learning approach", "author": "Jacob Wulff Wold and Florian Stadtmann and Adil Rasheed and Mandar Tabib and Omer San and Jan-Tore Horn", "abstract": "  Atmospheric flows are governed by a broad variety of spatio-temporal scales,\nthus making real-time numerical modeling of such turbulent flows in complex\nterrain at high resolution computationally intractable. In this study, we\ndemonstrate a neural network approach motivated by Enhanced Super-Resolution\nGenerative Adversarial Networks to upscale low-resolution wind fields to\ngenerate high-resolution wind fields in an actual wind farm in Bessaker,\nNorway. The neural network-based model is shown to successfully reconstruct\nfully resolved 3D velocity fields from a coarser scale while respecting the\nlocal terrain and that it easily outperforms trilinear interpolation. We also\ndemonstrate that by using appropriate cost function based on domain knowledge,\nwe can alleviate the use of adversarial training.\n", "link": "http://arxiv.org/abs/2309.10172v2", "date": "2024-04-02", "relevancy": 1.9675, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5455}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.483}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4793}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20wind%20field%20resolution%20in%20complex%20terrain%20through%20a%0A%20%20knowledge-driven%20machine%20learning%20approach&body=Title%3A%20Enhancing%20wind%20field%20resolution%20in%20complex%20terrain%20through%20a%0A%20%20knowledge-driven%20machine%20learning%20approach%0AAuthor%3A%20Jacob%20Wulff%20Wold%20and%20Florian%20Stadtmann%20and%20Adil%20Rasheed%20and%20Mandar%20Tabib%20and%20Omer%20San%20and%20Jan-Tore%20Horn%0AAbstract%3A%20%20%20Atmospheric%20flows%20are%20governed%20by%20a%20broad%20variety%20of%20spatio-temporal%20scales%2C%0Athus%20making%20real-time%20numerical%20modeling%20of%20such%20turbulent%20flows%20in%20complex%0Aterrain%20at%20high%20resolution%20computationally%20intractable.%20In%20this%20study%2C%20we%0Ademonstrate%20a%20neural%20network%20approach%20motivated%20by%20Enhanced%20Super-Resolution%0AGenerative%20Adversarial%20Networks%20to%20upscale%20low-resolution%20wind%20fields%20to%0Agenerate%20high-resolution%20wind%20fields%20in%20an%20actual%20wind%20farm%20in%20Bessaker%2C%0ANorway.%20The%20neural%20network-based%20model%20is%20shown%20to%20successfully%20reconstruct%0Afully%20resolved%203D%20velocity%20fields%20from%20a%20coarser%20scale%20while%20respecting%20the%0Alocal%20terrain%20and%20that%20it%20easily%20outperforms%20trilinear%20interpolation.%20We%20also%0Ademonstrate%20that%20by%20using%20appropriate%20cost%20function%20based%20on%20domain%20knowledge%2C%0Awe%20can%20alleviate%20the%20use%20of%20adversarial%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10172v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20wind%20field%20resolution%20in%20complex%20terrain%20through%20a%0A%20%20knowledge-driven%20machine%20learning%20approach&entry.906535625=Jacob%20Wulff%20Wold%20and%20Florian%20Stadtmann%20and%20Adil%20Rasheed%20and%20Mandar%20Tabib%20and%20Omer%20San%20and%20Jan-Tore%20Horn&entry.1292438233=%20%20Atmospheric%20flows%20are%20governed%20by%20a%20broad%20variety%20of%20spatio-temporal%20scales%2C%0Athus%20making%20real-time%20numerical%20modeling%20of%20such%20turbulent%20flows%20in%20complex%0Aterrain%20at%20high%20resolution%20computationally%20intractable.%20In%20this%20study%2C%20we%0Ademonstrate%20a%20neural%20network%20approach%20motivated%20by%20Enhanced%20Super-Resolution%0AGenerative%20Adversarial%20Networks%20to%20upscale%20low-resolution%20wind%20fields%20to%0Agenerate%20high-resolution%20wind%20fields%20in%20an%20actual%20wind%20farm%20in%20Bessaker%2C%0ANorway.%20The%20neural%20network-based%20model%20is%20shown%20to%20successfully%20reconstruct%0Afully%20resolved%203D%20velocity%20fields%20from%20a%20coarser%20scale%20while%20respecting%20the%0Alocal%20terrain%20and%20that%20it%20easily%20outperforms%20trilinear%20interpolation.%20We%20also%0Ademonstrate%20that%20by%20using%20appropriate%20cost%20function%20based%20on%20domain%20knowledge%2C%0Awe%20can%20alleviate%20the%20use%20of%20adversarial%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10172v2&entry.124074799=Read"},
{"title": "Lifelong Continual Learning for Anomaly Detection: New Challenges,\n  Perspectives, and Insights", "author": "Kamil Faber and Roberto Corizzo and Bartlomiej Sniezynski and Nathalie Japkowicz", "abstract": "  Anomaly detection is of paramount importance in many real-world domains,\ncharacterized by evolving behavior. Lifelong learning represents an emerging\ntrend, answering the need for machine learning models that continuously adapt\nto new challenges in dynamic environments while retaining past knowledge.\nHowever, limited efforts are dedicated to building foundations for lifelong\nanomaly detection, which provides intrinsically different challenges compared\nto the more widely explored classification setting. In this paper, we face this\nissue by exploring, motivating, and discussing lifelong anomaly detection,\ntrying to build foundations for its wider adoption. First, we explain why\nlifelong anomaly detection is relevant, defining challenges and opportunities\nto design anomaly detection methods that deal with lifelong learning\ncomplexities. Second, we characterize learning settings and a scenario\ngeneration procedure that enables researchers to experiment with lifelong\nanomaly detection using existing datasets. Third, we perform experiments with\npopular anomaly detection methods on proposed lifelong scenarios, emphasizing\nthe gap in performance that could be gained with the adoption of lifelong\nlearning. Overall, we conclude that the adoption of lifelong anomaly detection\nis important to design more robust models that provide a comprehensive view of\nthe environment, as well as simultaneous adaptation and knowledge retention.\n", "link": "http://arxiv.org/abs/2303.07557v2", "date": "2024-04-02", "relevancy": 1.9538, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4964}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4897}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.484}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lifelong%20Continual%20Learning%20for%20Anomaly%20Detection%3A%20New%20Challenges%2C%0A%20%20Perspectives%2C%20and%20Insights&body=Title%3A%20Lifelong%20Continual%20Learning%20for%20Anomaly%20Detection%3A%20New%20Challenges%2C%0A%20%20Perspectives%2C%20and%20Insights%0AAuthor%3A%20Kamil%20Faber%20and%20Roberto%20Corizzo%20and%20Bartlomiej%20Sniezynski%20and%20Nathalie%20Japkowicz%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20of%20paramount%20importance%20in%20many%20real-world%20domains%2C%0Acharacterized%20by%20evolving%20behavior.%20Lifelong%20learning%20represents%20an%20emerging%0Atrend%2C%20answering%20the%20need%20for%20machine%20learning%20models%20that%20continuously%20adapt%0Ato%20new%20challenges%20in%20dynamic%20environments%20while%20retaining%20past%20knowledge.%0AHowever%2C%20limited%20efforts%20are%20dedicated%20to%20building%20foundations%20for%20lifelong%0Aanomaly%20detection%2C%20which%20provides%20intrinsically%20different%20challenges%20compared%0Ato%20the%20more%20widely%20explored%20classification%20setting.%20In%20this%20paper%2C%20we%20face%20this%0Aissue%20by%20exploring%2C%20motivating%2C%20and%20discussing%20lifelong%20anomaly%20detection%2C%0Atrying%20to%20build%20foundations%20for%20its%20wider%20adoption.%20First%2C%20we%20explain%20why%0Alifelong%20anomaly%20detection%20is%20relevant%2C%20defining%20challenges%20and%20opportunities%0Ato%20design%20anomaly%20detection%20methods%20that%20deal%20with%20lifelong%20learning%0Acomplexities.%20Second%2C%20we%20characterize%20learning%20settings%20and%20a%20scenario%0Ageneration%20procedure%20that%20enables%20researchers%20to%20experiment%20with%20lifelong%0Aanomaly%20detection%20using%20existing%20datasets.%20Third%2C%20we%20perform%20experiments%20with%0Apopular%20anomaly%20detection%20methods%20on%20proposed%20lifelong%20scenarios%2C%20emphasizing%0Athe%20gap%20in%20performance%20that%20could%20be%20gained%20with%20the%20adoption%20of%20lifelong%0Alearning.%20Overall%2C%20we%20conclude%20that%20the%20adoption%20of%20lifelong%20anomaly%20detection%0Ais%20important%20to%20design%20more%20robust%20models%20that%20provide%20a%20comprehensive%20view%20of%0Athe%20environment%2C%20as%20well%20as%20simultaneous%20adaptation%20and%20knowledge%20retention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.07557v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lifelong%20Continual%20Learning%20for%20Anomaly%20Detection%3A%20New%20Challenges%2C%0A%20%20Perspectives%2C%20and%20Insights&entry.906535625=Kamil%20Faber%20and%20Roberto%20Corizzo%20and%20Bartlomiej%20Sniezynski%20and%20Nathalie%20Japkowicz&entry.1292438233=%20%20Anomaly%20detection%20is%20of%20paramount%20importance%20in%20many%20real-world%20domains%2C%0Acharacterized%20by%20evolving%20behavior.%20Lifelong%20learning%20represents%20an%20emerging%0Atrend%2C%20answering%20the%20need%20for%20machine%20learning%20models%20that%20continuously%20adapt%0Ato%20new%20challenges%20in%20dynamic%20environments%20while%20retaining%20past%20knowledge.%0AHowever%2C%20limited%20efforts%20are%20dedicated%20to%20building%20foundations%20for%20lifelong%0Aanomaly%20detection%2C%20which%20provides%20intrinsically%20different%20challenges%20compared%0Ato%20the%20more%20widely%20explored%20classification%20setting.%20In%20this%20paper%2C%20we%20face%20this%0Aissue%20by%20exploring%2C%20motivating%2C%20and%20discussing%20lifelong%20anomaly%20detection%2C%0Atrying%20to%20build%20foundations%20for%20its%20wider%20adoption.%20First%2C%20we%20explain%20why%0Alifelong%20anomaly%20detection%20is%20relevant%2C%20defining%20challenges%20and%20opportunities%0Ato%20design%20anomaly%20detection%20methods%20that%20deal%20with%20lifelong%20learning%0Acomplexities.%20Second%2C%20we%20characterize%20learning%20settings%20and%20a%20scenario%0Ageneration%20procedure%20that%20enables%20researchers%20to%20experiment%20with%20lifelong%0Aanomaly%20detection%20using%20existing%20datasets.%20Third%2C%20we%20perform%20experiments%20with%0Apopular%20anomaly%20detection%20methods%20on%20proposed%20lifelong%20scenarios%2C%20emphasizing%0Athe%20gap%20in%20performance%20that%20could%20be%20gained%20with%20the%20adoption%20of%20lifelong%0Alearning.%20Overall%2C%20we%20conclude%20that%20the%20adoption%20of%20lifelong%20anomaly%20detection%0Ais%20important%20to%20design%20more%20robust%20models%20that%20provide%20a%20comprehensive%20view%20of%0Athe%20environment%2C%20as%20well%20as%20simultaneous%20adaptation%20and%20knowledge%20retention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.07557v2&entry.124074799=Read"},
{"title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious\n  Demonstrations Shows their Vulnerabilities", "author": "Lingbo Mo and Boshi Wang and Muhao Chen and Huan Sun", "abstract": "  The rapid progress in open-source Large Language Models (LLMs) is\nsignificantly driving AI development forward. However, there is still a limited\nunderstanding of their trustworthiness. Deploying these models at scale without\nsufficient trustworthiness can pose significant risks, highlighting the need to\nuncover these issues promptly. In this work, we conduct an adversarial\nassessment of open-source LLMs on trustworthiness, scrutinizing them across\neight different aspects including toxicity, stereotypes, ethics, hallucination,\nfairness, sycophancy, privacy, and robustness against adversarial\ndemonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU)\nprompting strategy by incorporating carefully crafted malicious demonstrations\nfor trustworthiness attack. Our extensive experiments encompass recent and\nrepresentative series of open-source LLMs, including Vicuna, MPT, Falcon,\nMistral, and Llama 2. The empirical outcomes underscore the efficacy of our\nattack strategy across diverse aspects. More interestingly, our result analysis\nreveals that models with superior performance in general NLP tasks do not\nalways have greater trustworthiness; in fact, larger models can be more\nvulnerable to attacks. Additionally, models that have undergone instruction\ntuning, focusing on instruction following, tend to be more susceptible,\nalthough fine-tuning LLMs for safety alignment proves effective in mitigating\nadversarial trustworthiness attacks.\n", "link": "http://arxiv.org/abs/2311.09447v2", "date": "2024-04-02", "relevancy": 1.9303, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5355}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4861}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4579}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20How%20Trustworthy%20are%20Open-Source%20LLMs%3F%20An%20Assessment%20under%20Malicious%0A%20%20Demonstrations%20Shows%20their%20Vulnerabilities&body=Title%3A%20How%20Trustworthy%20are%20Open-Source%20LLMs%3F%20An%20Assessment%20under%20Malicious%0A%20%20Demonstrations%20Shows%20their%20Vulnerabilities%0AAuthor%3A%20Lingbo%20Mo%20and%20Boshi%20Wang%20and%20Muhao%20Chen%20and%20Huan%20Sun%0AAbstract%3A%20%20%20The%20rapid%20progress%20in%20open-source%20Large%20Language%20Models%20%28LLMs%29%20is%0Asignificantly%20driving%20AI%20development%20forward.%20However%2C%20there%20is%20still%20a%20limited%0Aunderstanding%20of%20their%20trustworthiness.%20Deploying%20these%20models%20at%20scale%20without%0Asufficient%20trustworthiness%20can%20pose%20significant%20risks%2C%20highlighting%20the%20need%20to%0Auncover%20these%20issues%20promptly.%20In%20this%20work%2C%20we%20conduct%20an%20adversarial%0Aassessment%20of%20open-source%20LLMs%20on%20trustworthiness%2C%20scrutinizing%20them%20across%0Aeight%20different%20aspects%20including%20toxicity%2C%20stereotypes%2C%20ethics%2C%20hallucination%2C%0Afairness%2C%20sycophancy%2C%20privacy%2C%20and%20robustness%20against%20adversarial%0Ademonstrations.%20We%20propose%20advCoU%2C%20an%20extended%20Chain%20of%20Utterances-based%20%28CoU%29%0Aprompting%20strategy%20by%20incorporating%20carefully%20crafted%20malicious%20demonstrations%0Afor%20trustworthiness%20attack.%20Our%20extensive%20experiments%20encompass%20recent%20and%0Arepresentative%20series%20of%20open-source%20LLMs%2C%20including%20Vicuna%2C%20MPT%2C%20Falcon%2C%0AMistral%2C%20and%20Llama%202.%20The%20empirical%20outcomes%20underscore%20the%20efficacy%20of%20our%0Aattack%20strategy%20across%20diverse%20aspects.%20More%20interestingly%2C%20our%20result%20analysis%0Areveals%20that%20models%20with%20superior%20performance%20in%20general%20NLP%20tasks%20do%20not%0Aalways%20have%20greater%20trustworthiness%3B%20in%20fact%2C%20larger%20models%20can%20be%20more%0Avulnerable%20to%20attacks.%20Additionally%2C%20models%20that%20have%20undergone%20instruction%0Atuning%2C%20focusing%20on%20instruction%20following%2C%20tend%20to%20be%20more%20susceptible%2C%0Aalthough%20fine-tuning%20LLMs%20for%20safety%20alignment%20proves%20effective%20in%20mitigating%0Aadversarial%20trustworthiness%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09447v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Trustworthy%20are%20Open-Source%20LLMs%3F%20An%20Assessment%20under%20Malicious%0A%20%20Demonstrations%20Shows%20their%20Vulnerabilities&entry.906535625=Lingbo%20Mo%20and%20Boshi%20Wang%20and%20Muhao%20Chen%20and%20Huan%20Sun&entry.1292438233=%20%20The%20rapid%20progress%20in%20open-source%20Large%20Language%20Models%20%28LLMs%29%20is%0Asignificantly%20driving%20AI%20development%20forward.%20However%2C%20there%20is%20still%20a%20limited%0Aunderstanding%20of%20their%20trustworthiness.%20Deploying%20these%20models%20at%20scale%20without%0Asufficient%20trustworthiness%20can%20pose%20significant%20risks%2C%20highlighting%20the%20need%20to%0Auncover%20these%20issues%20promptly.%20In%20this%20work%2C%20we%20conduct%20an%20adversarial%0Aassessment%20of%20open-source%20LLMs%20on%20trustworthiness%2C%20scrutinizing%20them%20across%0Aeight%20different%20aspects%20including%20toxicity%2C%20stereotypes%2C%20ethics%2C%20hallucination%2C%0Afairness%2C%20sycophancy%2C%20privacy%2C%20and%20robustness%20against%20adversarial%0Ademonstrations.%20We%20propose%20advCoU%2C%20an%20extended%20Chain%20of%20Utterances-based%20%28CoU%29%0Aprompting%20strategy%20by%20incorporating%20carefully%20crafted%20malicious%20demonstrations%0Afor%20trustworthiness%20attack.%20Our%20extensive%20experiments%20encompass%20recent%20and%0Arepresentative%20series%20of%20open-source%20LLMs%2C%20including%20Vicuna%2C%20MPT%2C%20Falcon%2C%0AMistral%2C%20and%20Llama%202.%20The%20empirical%20outcomes%20underscore%20the%20efficacy%20of%20our%0Aattack%20strategy%20across%20diverse%20aspects.%20More%20interestingly%2C%20our%20result%20analysis%0Areveals%20that%20models%20with%20superior%20performance%20in%20general%20NLP%20tasks%20do%20not%0Aalways%20have%20greater%20trustworthiness%3B%20in%20fact%2C%20larger%20models%20can%20be%20more%0Avulnerable%20to%20attacks.%20Additionally%2C%20models%20that%20have%20undergone%20instruction%0Atuning%2C%20focusing%20on%20instruction%20following%2C%20tend%20to%20be%20more%20susceptible%2C%0Aalthough%20fine-tuning%20LLMs%20for%20safety%20alignment%20proves%20effective%20in%20mitigating%0Aadversarial%20trustworthiness%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09447v2&entry.124074799=Read"},
{"title": "MonoBox: Tightness-free Box-supervised Polyp Segmentation using\n  Monotonicity Constraint", "author": "Qiang Hu and Zhenyu Yi and Ying Zhou and Ting Li and Fan Huang and Mei Liu and Qiang Li and Zhiwei Wang", "abstract": "  We propose MonoBox, an innovative box-supervised segmentation method\nconstrained by monotonicity to liberate its training from the user-unfriendly\nbox-tightness assumption. In contrast to conventional box-supervised\nsegmentation, where the box edges must precisely touch the target boundaries,\nMonoBox leverages imprecisely-annotated boxes to achieve robust pixel-wise\nsegmentation. The 'linchpin' is that, within the noisy zones around box edges,\nMonoBox discards the traditional misguiding multiple-instance learning loss,\nand instead optimizes a carefully-designed objective, termed monotonicity\nconstraint. Along directions transitioning from the foreground to background,\nthis new constraint steers responses to adhere to a trend of monotonically\ndecreasing values. Consequently, the originally unreliable learning within the\nnoisy zones is transformed into a correct and effective monotonicity\noptimization. Moreover, an adaptive label correction is introduced, enabling\nMonoBox to enhance the tightness of box annotations using predicted masks from\nthe previous epoch and dynamically shrink the noisy zones as training\nprogresses. We verify MonoBox in the box-supervised segmentation task of\npolyps, where satisfying box-tightness is challenging due to the vague\nboundaries between the polyp and normal tissues. Experiments on both public\nsynthetic and in-house real noisy datasets demonstrate that MonoBox exceeds\nother anti-noise state-of-the-arts by improving Dice by at least 5.5% and 3.3%,\nrespectively. Codes are at https://github.com/Huster-Hq/MonoBox.\n", "link": "http://arxiv.org/abs/2404.01188v2", "date": "2024-04-02", "relevancy": 1.9221, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5089}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4867}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.463}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MonoBox%3A%20Tightness-free%20Box-supervised%20Polyp%20Segmentation%20using%0A%20%20Monotonicity%20Constraint&body=Title%3A%20MonoBox%3A%20Tightness-free%20Box-supervised%20Polyp%20Segmentation%20using%0A%20%20Monotonicity%20Constraint%0AAuthor%3A%20Qiang%20Hu%20and%20Zhenyu%20Yi%20and%20Ying%20Zhou%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Mei%20Liu%20and%20Qiang%20Li%20and%20Zhiwei%20Wang%0AAbstract%3A%20%20%20We%20propose%20MonoBox%2C%20an%20innovative%20box-supervised%20segmentation%20method%0Aconstrained%20by%20monotonicity%20to%20liberate%20its%20training%20from%20the%20user-unfriendly%0Abox-tightness%20assumption.%20In%20contrast%20to%20conventional%20box-supervised%0Asegmentation%2C%20where%20the%20box%20edges%20must%20precisely%20touch%20the%20target%20boundaries%2C%0AMonoBox%20leverages%20imprecisely-annotated%20boxes%20to%20achieve%20robust%20pixel-wise%0Asegmentation.%20The%20%27linchpin%27%20is%20that%2C%20within%20the%20noisy%20zones%20around%20box%20edges%2C%0AMonoBox%20discards%20the%20traditional%20misguiding%20multiple-instance%20learning%20loss%2C%0Aand%20instead%20optimizes%20a%20carefully-designed%20objective%2C%20termed%20monotonicity%0Aconstraint.%20Along%20directions%20transitioning%20from%20the%20foreground%20to%20background%2C%0Athis%20new%20constraint%20steers%20responses%20to%20adhere%20to%20a%20trend%20of%20monotonically%0Adecreasing%20values.%20Consequently%2C%20the%20originally%20unreliable%20learning%20within%20the%0Anoisy%20zones%20is%20transformed%20into%20a%20correct%20and%20effective%20monotonicity%0Aoptimization.%20Moreover%2C%20an%20adaptive%20label%20correction%20is%20introduced%2C%20enabling%0AMonoBox%20to%20enhance%20the%20tightness%20of%20box%20annotations%20using%20predicted%20masks%20from%0Athe%20previous%20epoch%20and%20dynamically%20shrink%20the%20noisy%20zones%20as%20training%0Aprogresses.%20We%20verify%20MonoBox%20in%20the%20box-supervised%20segmentation%20task%20of%0Apolyps%2C%20where%20satisfying%20box-tightness%20is%20challenging%20due%20to%20the%20vague%0Aboundaries%20between%20the%20polyp%20and%20normal%20tissues.%20Experiments%20on%20both%20public%0Asynthetic%20and%20in-house%20real%20noisy%20datasets%20demonstrate%20that%20MonoBox%20exceeds%0Aother%20anti-noise%20state-of-the-arts%20by%20improving%20Dice%20by%20at%20least%205.5%25%20and%203.3%25%2C%0Arespectively.%20Codes%20are%20at%20https%3A//github.com/Huster-Hq/MonoBox.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01188v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoBox%3A%20Tightness-free%20Box-supervised%20Polyp%20Segmentation%20using%0A%20%20Monotonicity%20Constraint&entry.906535625=Qiang%20Hu%20and%20Zhenyu%20Yi%20and%20Ying%20Zhou%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Mei%20Liu%20and%20Qiang%20Li%20and%20Zhiwei%20Wang&entry.1292438233=%20%20We%20propose%20MonoBox%2C%20an%20innovative%20box-supervised%20segmentation%20method%0Aconstrained%20by%20monotonicity%20to%20liberate%20its%20training%20from%20the%20user-unfriendly%0Abox-tightness%20assumption.%20In%20contrast%20to%20conventional%20box-supervised%0Asegmentation%2C%20where%20the%20box%20edges%20must%20precisely%20touch%20the%20target%20boundaries%2C%0AMonoBox%20leverages%20imprecisely-annotated%20boxes%20to%20achieve%20robust%20pixel-wise%0Asegmentation.%20The%20%27linchpin%27%20is%20that%2C%20within%20the%20noisy%20zones%20around%20box%20edges%2C%0AMonoBox%20discards%20the%20traditional%20misguiding%20multiple-instance%20learning%20loss%2C%0Aand%20instead%20optimizes%20a%20carefully-designed%20objective%2C%20termed%20monotonicity%0Aconstraint.%20Along%20directions%20transitioning%20from%20the%20foreground%20to%20background%2C%0Athis%20new%20constraint%20steers%20responses%20to%20adhere%20to%20a%20trend%20of%20monotonically%0Adecreasing%20values.%20Consequently%2C%20the%20originally%20unreliable%20learning%20within%20the%0Anoisy%20zones%20is%20transformed%20into%20a%20correct%20and%20effective%20monotonicity%0Aoptimization.%20Moreover%2C%20an%20adaptive%20label%20correction%20is%20introduced%2C%20enabling%0AMonoBox%20to%20enhance%20the%20tightness%20of%20box%20annotations%20using%20predicted%20masks%20from%0Athe%20previous%20epoch%20and%20dynamically%20shrink%20the%20noisy%20zones%20as%20training%0Aprogresses.%20We%20verify%20MonoBox%20in%20the%20box-supervised%20segmentation%20task%20of%0Apolyps%2C%20where%20satisfying%20box-tightness%20is%20challenging%20due%20to%20the%20vague%0Aboundaries%20between%20the%20polyp%20and%20normal%20tissues.%20Experiments%20on%20both%20public%0Asynthetic%20and%20in-house%20real%20noisy%20datasets%20demonstrate%20that%20MonoBox%20exceeds%0Aother%20anti-noise%20state-of-the-arts%20by%20improving%20Dice%20by%20at%20least%205.5%25%20and%203.3%25%2C%0Arespectively.%20Codes%20are%20at%20https%3A//github.com/Huster-Hq/MonoBox.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01188v2&entry.124074799=Read"},
{"title": "Noise Masking Attacks and Defenses for Pretrained Speech Models", "author": "Matthew Jagielski and Om Thakkar and Lun Wang", "abstract": "  Speech models are often trained on sensitive data in order to improve model\nperformance, leading to potential privacy leakage. Our work considers noise\nmasking attacks, introduced by Amid et al. 2022, which attack automatic speech\nrecognition (ASR) models by requesting a transcript of an utterance which is\npartially replaced with noise. They show that when a record has been seen at\ntraining time, the model will transcribe the noisy record with its memorized\nsensitive transcript. In our work, we extend these attacks beyond ASR models,\nto attack pretrained speech encoders. Our method fine-tunes the encoder to\nproduce an ASR model, and then performs noise masking on this model, which we\nfind recovers private information from the pretraining data, despite the model\nnever having seen transcripts at pretraining time! We show how to improve the\nprecision of these attacks and investigate a number of countermeasures to our\nattacks.\n", "link": "http://arxiv.org/abs/2404.02052v1", "date": "2024-04-02", "relevancy": 1.8979, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4847}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4703}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4594}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Noise%20Masking%20Attacks%20and%20Defenses%20for%20Pretrained%20Speech%20Models&body=Title%3A%20Noise%20Masking%20Attacks%20and%20Defenses%20for%20Pretrained%20Speech%20Models%0AAuthor%3A%20Matthew%20Jagielski%20and%20Om%20Thakkar%20and%20Lun%20Wang%0AAbstract%3A%20%20%20Speech%20models%20are%20often%20trained%20on%20sensitive%20data%20in%20order%20to%20improve%20model%0Aperformance%2C%20leading%20to%20potential%20privacy%20leakage.%20Our%20work%20considers%20noise%0Amasking%20attacks%2C%20introduced%20by%20Amid%20et%20al.%202022%2C%20which%20attack%20automatic%20speech%0Arecognition%20%28ASR%29%20models%20by%20requesting%20a%20transcript%20of%20an%20utterance%20which%20is%0Apartially%20replaced%20with%20noise.%20They%20show%20that%20when%20a%20record%20has%20been%20seen%20at%0Atraining%20time%2C%20the%20model%20will%20transcribe%20the%20noisy%20record%20with%20its%20memorized%0Asensitive%20transcript.%20In%20our%20work%2C%20we%20extend%20these%20attacks%20beyond%20ASR%20models%2C%0Ato%20attack%20pretrained%20speech%20encoders.%20Our%20method%20fine-tunes%20the%20encoder%20to%0Aproduce%20an%20ASR%20model%2C%20and%20then%20performs%20noise%20masking%20on%20this%20model%2C%20which%20we%0Afind%20recovers%20private%20information%20from%20the%20pretraining%20data%2C%20despite%20the%20model%0Anever%20having%20seen%20transcripts%20at%20pretraining%20time%21%20We%20show%20how%20to%20improve%20the%0Aprecision%20of%20these%20attacks%20and%20investigate%20a%20number%20of%20countermeasures%20to%20our%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02052v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise%20Masking%20Attacks%20and%20Defenses%20for%20Pretrained%20Speech%20Models&entry.906535625=Matthew%20Jagielski%20and%20Om%20Thakkar%20and%20Lun%20Wang&entry.1292438233=%20%20Speech%20models%20are%20often%20trained%20on%20sensitive%20data%20in%20order%20to%20improve%20model%0Aperformance%2C%20leading%20to%20potential%20privacy%20leakage.%20Our%20work%20considers%20noise%0Amasking%20attacks%2C%20introduced%20by%20Amid%20et%20al.%202022%2C%20which%20attack%20automatic%20speech%0Arecognition%20%28ASR%29%20models%20by%20requesting%20a%20transcript%20of%20an%20utterance%20which%20is%0Apartially%20replaced%20with%20noise.%20They%20show%20that%20when%20a%20record%20has%20been%20seen%20at%0Atraining%20time%2C%20the%20model%20will%20transcribe%20the%20noisy%20record%20with%20its%20memorized%0Asensitive%20transcript.%20In%20our%20work%2C%20we%20extend%20these%20attacks%20beyond%20ASR%20models%2C%0Ato%20attack%20pretrained%20speech%20encoders.%20Our%20method%20fine-tunes%20the%20encoder%20to%0Aproduce%20an%20ASR%20model%2C%20and%20then%20performs%20noise%20masking%20on%20this%20model%2C%20which%20we%0Afind%20recovers%20private%20information%20from%20the%20pretraining%20data%2C%20despite%20the%20model%0Anever%20having%20seen%20transcripts%20at%20pretraining%20time%21%20We%20show%20how%20to%20improve%20the%0Aprecision%20of%20these%20attacks%20and%20investigate%20a%20number%20of%20countermeasures%20to%20our%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02052v1&entry.124074799=Read"},
{"title": "GINopic: Topic Modeling with Graph Isomorphism Network", "author": "Suman Adhya and Debarshi Kumar Sanyal", "abstract": "  Topic modeling is a widely used approach for analyzing and exploring large\ndocument collections. Recent research efforts have incorporated pre-trained\ncontextualized language models, such as BERT embeddings, into topic modeling.\nHowever, they often neglect the intrinsic informational value conveyed by\nmutual dependencies between words. In this study, we introduce GINopic, a topic\nmodeling framework based on graph isomorphism networks to capture the\ncorrelation between words. By conducting intrinsic (quantitative as well as\nqualitative) and extrinsic evaluations on diverse benchmark datasets, we\ndemonstrate the effectiveness of GINopic compared to existing topic models and\nhighlight its potential for advancing topic modeling.\n", "link": "http://arxiv.org/abs/2404.02115v1", "date": "2024-04-02", "relevancy": 1.8918, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4759}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.475}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4605}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GINopic%3A%20Topic%20Modeling%20with%20Graph%20Isomorphism%20Network&body=Title%3A%20GINopic%3A%20Topic%20Modeling%20with%20Graph%20Isomorphism%20Network%0AAuthor%3A%20Suman%20Adhya%20and%20Debarshi%20Kumar%20Sanyal%0AAbstract%3A%20%20%20Topic%20modeling%20is%20a%20widely%20used%20approach%20for%20analyzing%20and%20exploring%20large%0Adocument%20collections.%20Recent%20research%20efforts%20have%20incorporated%20pre-trained%0Acontextualized%20language%20models%2C%20such%20as%20BERT%20embeddings%2C%20into%20topic%20modeling.%0AHowever%2C%20they%20often%20neglect%20the%20intrinsic%20informational%20value%20conveyed%20by%0Amutual%20dependencies%20between%20words.%20In%20this%20study%2C%20we%20introduce%20GINopic%2C%20a%20topic%0Amodeling%20framework%20based%20on%20graph%20isomorphism%20networks%20to%20capture%20the%0Acorrelation%20between%20words.%20By%20conducting%20intrinsic%20%28quantitative%20as%20well%20as%0Aqualitative%29%20and%20extrinsic%20evaluations%20on%20diverse%20benchmark%20datasets%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20GINopic%20compared%20to%20existing%20topic%20models%20and%0Ahighlight%20its%20potential%20for%20advancing%20topic%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02115v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GINopic%3A%20Topic%20Modeling%20with%20Graph%20Isomorphism%20Network&entry.906535625=Suman%20Adhya%20and%20Debarshi%20Kumar%20Sanyal&entry.1292438233=%20%20Topic%20modeling%20is%20a%20widely%20used%20approach%20for%20analyzing%20and%20exploring%20large%0Adocument%20collections.%20Recent%20research%20efforts%20have%20incorporated%20pre-trained%0Acontextualized%20language%20models%2C%20such%20as%20BERT%20embeddings%2C%20into%20topic%20modeling.%0AHowever%2C%20they%20often%20neglect%20the%20intrinsic%20informational%20value%20conveyed%20by%0Amutual%20dependencies%20between%20words.%20In%20this%20study%2C%20we%20introduce%20GINopic%2C%20a%20topic%0Amodeling%20framework%20based%20on%20graph%20isomorphism%20networks%20to%20capture%20the%0Acorrelation%20between%20words.%20By%20conducting%20intrinsic%20%28quantitative%20as%20well%20as%0Aqualitative%29%20and%20extrinsic%20evaluations%20on%20diverse%20benchmark%20datasets%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20GINopic%20compared%20to%20existing%20topic%20models%20and%0Ahighlight%20its%20potential%20for%20advancing%20topic%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02115v1&entry.124074799=Read"},
{"title": "Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials", "author": "Ali Akram and Marija Stanojevic and Malikeh Ehghaghi and Jekaterina Novikova", "abstract": "  Due to the substantial number of clinicians, patients, and data collection\nenvironments involved in clinical trials, gathering data of superior quality\nposes a significant challenge. In clinical trials, patients are assessed based\non their speech data to detect and monitor cognitive and mental health\ndisorders. We propose using these speech recordings to verify the identities of\nenrolled patients and identify and exclude the individuals who try to enroll\nmultiple times in the same trial. Since clinical studies are often conducted\nacross different countries, creating a system that can perform speaker\nverification in diverse languages without additional development effort is\nimperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models\nby enrolling and testing with speech-impaired patients speaking English,\nGerman, Danish, Spanish, and Arabic languages. Our results demonstrate that\ntested models can effectively generalize to clinical speakers, with less than\n2.7% EER for European Languages and 8.26% EER for Arabic. This represents a\nsignificant step in developing more versatile and efficient speaker\nverification systems for cognitive and mental health clinical trials that can\nbe used across a wide range of languages and dialects, substantially reducing\nthe effort required to develop speaker verification systems for multiple\nlanguages. We also evaluate how speech tasks and number of speakers involved in\nthe trial influence the performance and show that the type of speech tasks\nimpacts the model performance.\n", "link": "http://arxiv.org/abs/2404.01981v1", "date": "2024-04-02", "relevancy": 1.8739, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4935}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4759}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4405}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Multi-Lingual%20Speaker%20Verification%20in%20Clinical%20Trials&body=Title%3A%20Zero-Shot%20Multi-Lingual%20Speaker%20Verification%20in%20Clinical%20Trials%0AAuthor%3A%20Ali%20Akram%20and%20Marija%20Stanojevic%20and%20Malikeh%20Ehghaghi%20and%20Jekaterina%20Novikova%0AAbstract%3A%20%20%20Due%20to%20the%20substantial%20number%20of%20clinicians%2C%20patients%2C%20and%20data%20collection%0Aenvironments%20involved%20in%20clinical%20trials%2C%20gathering%20data%20of%20superior%20quality%0Aposes%20a%20significant%20challenge.%20In%20clinical%20trials%2C%20patients%20are%20assessed%20based%0Aon%20their%20speech%20data%20to%20detect%20and%20monitor%20cognitive%20and%20mental%20health%0Adisorders.%20We%20propose%20using%20these%20speech%20recordings%20to%20verify%20the%20identities%20of%0Aenrolled%20patients%20and%20identify%20and%20exclude%20the%20individuals%20who%20try%20to%20enroll%0Amultiple%20times%20in%20the%20same%20trial.%20Since%20clinical%20studies%20are%20often%20conducted%0Aacross%20different%20countries%2C%20creating%20a%20system%20that%20can%20perform%20speaker%0Averification%20in%20diverse%20languages%20without%20additional%20development%20effort%20is%0Aimperative.%20We%20evaluate%20pre-trained%20TitaNet%2C%20ECAPA-TDNN%2C%20and%20SpeakerNet%20models%0Aby%20enrolling%20and%20testing%20with%20speech-impaired%20patients%20speaking%20English%2C%0AGerman%2C%20Danish%2C%20Spanish%2C%20and%20Arabic%20languages.%20Our%20results%20demonstrate%20that%0Atested%20models%20can%20effectively%20generalize%20to%20clinical%20speakers%2C%20with%20less%20than%0A2.7%25%20EER%20for%20European%20Languages%20and%208.26%25%20EER%20for%20Arabic.%20This%20represents%20a%0Asignificant%20step%20in%20developing%20more%20versatile%20and%20efficient%20speaker%0Averification%20systems%20for%20cognitive%20and%20mental%20health%20clinical%20trials%20that%20can%0Abe%20used%20across%20a%20wide%20range%20of%20languages%20and%20dialects%2C%20substantially%20reducing%0Athe%20effort%20required%20to%20develop%20speaker%20verification%20systems%20for%20multiple%0Alanguages.%20We%20also%20evaluate%20how%20speech%20tasks%20and%20number%20of%20speakers%20involved%20in%0Athe%20trial%20influence%20the%20performance%20and%20show%20that%20the%20type%20of%20speech%20tasks%0Aimpacts%20the%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01981v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Multi-Lingual%20Speaker%20Verification%20in%20Clinical%20Trials&entry.906535625=Ali%20Akram%20and%20Marija%20Stanojevic%20and%20Malikeh%20Ehghaghi%20and%20Jekaterina%20Novikova&entry.1292438233=%20%20Due%20to%20the%20substantial%20number%20of%20clinicians%2C%20patients%2C%20and%20data%20collection%0Aenvironments%20involved%20in%20clinical%20trials%2C%20gathering%20data%20of%20superior%20quality%0Aposes%20a%20significant%20challenge.%20In%20clinical%20trials%2C%20patients%20are%20assessed%20based%0Aon%20their%20speech%20data%20to%20detect%20and%20monitor%20cognitive%20and%20mental%20health%0Adisorders.%20We%20propose%20using%20these%20speech%20recordings%20to%20verify%20the%20identities%20of%0Aenrolled%20patients%20and%20identify%20and%20exclude%20the%20individuals%20who%20try%20to%20enroll%0Amultiple%20times%20in%20the%20same%20trial.%20Since%20clinical%20studies%20are%20often%20conducted%0Aacross%20different%20countries%2C%20creating%20a%20system%20that%20can%20perform%20speaker%0Averification%20in%20diverse%20languages%20without%20additional%20development%20effort%20is%0Aimperative.%20We%20evaluate%20pre-trained%20TitaNet%2C%20ECAPA-TDNN%2C%20and%20SpeakerNet%20models%0Aby%20enrolling%20and%20testing%20with%20speech-impaired%20patients%20speaking%20English%2C%0AGerman%2C%20Danish%2C%20Spanish%2C%20and%20Arabic%20languages.%20Our%20results%20demonstrate%20that%0Atested%20models%20can%20effectively%20generalize%20to%20clinical%20speakers%2C%20with%20less%20than%0A2.7%25%20EER%20for%20European%20Languages%20and%208.26%25%20EER%20for%20Arabic.%20This%20represents%20a%0Asignificant%20step%20in%20developing%20more%20versatile%20and%20efficient%20speaker%0Averification%20systems%20for%20cognitive%20and%20mental%20health%20clinical%20trials%20that%20can%0Abe%20used%20across%20a%20wide%20range%20of%20languages%20and%20dialects%2C%20substantially%20reducing%0Athe%20effort%20required%20to%20develop%20speaker%20verification%20systems%20for%20multiple%0Alanguages.%20We%20also%20evaluate%20how%20speech%20tasks%20and%20number%20of%20speakers%20involved%20in%0Athe%20trial%20influence%20the%20performance%20and%20show%20that%20the%20type%20of%20speech%20tasks%0Aimpacts%20the%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01981v1&entry.124074799=Read"},
{"title": "Don't throw away your value model! Generating more preferable text with\n  Value-Guided Monte-Carlo Tree Search decoding", "author": "Jiacheng Liu and Andrew Cohen and Ramakanth Pasunuru and Yejin Choi and Hannaneh Hajishirzi and Asli Celikyilmaz", "abstract": "  Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may\nseem unnecessary when generating natural language text based on\nstate-of-the-art reinforcement learning such as Proximal Policy Optimization\n(PPO). In this paper, we demonstrate that it is possible to get extra mileage\nout of PPO by integrating MCTS on top. The key idea is not to throw out the\nvalue network, a byproduct of PPO training for evaluating partial output\nsequences, when decoding text out of the policy network. More concretely, we\npresent a novel value-guided decoding algorithm called PPO-MCTS, which can\nintegrate the value network from PPO to work closely with the policy network\nduring inference-time generation. Compared to prior approaches based on MCTS\nfor controlled text generation, the key strength of our approach is to reduce\nthe fundamental mismatch of the scoring mechanisms of the partial outputs\nbetween training and test. Evaluation on four text generation tasks demonstrate\nthat PPO-MCTS greatly improves the preferability of generated text compared to\nthe standard practice of using only the PPO policy. Our results demonstrate the\npromise of search algorithms even on top of the aligned language models from\nPPO, and the under-explored benefit of the value network.\n", "link": "http://arxiv.org/abs/2309.15028v3", "date": "2024-04-02", "relevancy": 1.8642, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4794}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4653}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4614}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Don%27t%20throw%20away%20your%20value%20model%21%20Generating%20more%20preferable%20text%20with%0A%20%20Value-Guided%20Monte-Carlo%20Tree%20Search%20decoding&body=Title%3A%20Don%27t%20throw%20away%20your%20value%20model%21%20Generating%20more%20preferable%20text%20with%0A%20%20Value-Guided%20Monte-Carlo%20Tree%20Search%20decoding%0AAuthor%3A%20Jiacheng%20Liu%20and%20Andrew%20Cohen%20and%20Ramakanth%20Pasunuru%20and%20Yejin%20Choi%20and%20Hannaneh%20Hajishirzi%20and%20Asli%20Celikyilmaz%0AAbstract%3A%20%20%20Inference-time%20search%20algorithms%20such%20as%20Monte-Carlo%20Tree%20Search%20%28MCTS%29%20may%0Aseem%20unnecessary%20when%20generating%20natural%20language%20text%20based%20on%0Astate-of-the-art%20reinforcement%20learning%20such%20as%20Proximal%20Policy%20Optimization%0A%28PPO%29.%20In%20this%20paper%2C%20we%20demonstrate%20that%20it%20is%20possible%20to%20get%20extra%20mileage%0Aout%20of%20PPO%20by%20integrating%20MCTS%20on%20top.%20The%20key%20idea%20is%20not%20to%20throw%20out%20the%0Avalue%20network%2C%20a%20byproduct%20of%20PPO%20training%20for%20evaluating%20partial%20output%0Asequences%2C%20when%20decoding%20text%20out%20of%20the%20policy%20network.%20More%20concretely%2C%20we%0Apresent%20a%20novel%20value-guided%20decoding%20algorithm%20called%20PPO-MCTS%2C%20which%20can%0Aintegrate%20the%20value%20network%20from%20PPO%20to%20work%20closely%20with%20the%20policy%20network%0Aduring%20inference-time%20generation.%20Compared%20to%20prior%20approaches%20based%20on%20MCTS%0Afor%20controlled%20text%20generation%2C%20the%20key%20strength%20of%20our%20approach%20is%20to%20reduce%0Athe%20fundamental%20mismatch%20of%20the%20scoring%20mechanisms%20of%20the%20partial%20outputs%0Abetween%20training%20and%20test.%20Evaluation%20on%20four%20text%20generation%20tasks%20demonstrate%0Athat%20PPO-MCTS%20greatly%20improves%20the%20preferability%20of%20generated%20text%20compared%20to%0Athe%20standard%20practice%20of%20using%20only%20the%20PPO%20policy.%20Our%20results%20demonstrate%20the%0Apromise%20of%20search%20algorithms%20even%20on%20top%20of%20the%20aligned%20language%20models%20from%0APPO%2C%20and%20the%20under-explored%20benefit%20of%20the%20value%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.15028v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20throw%20away%20your%20value%20model%21%20Generating%20more%20preferable%20text%20with%0A%20%20Value-Guided%20Monte-Carlo%20Tree%20Search%20decoding&entry.906535625=Jiacheng%20Liu%20and%20Andrew%20Cohen%20and%20Ramakanth%20Pasunuru%20and%20Yejin%20Choi%20and%20Hannaneh%20Hajishirzi%20and%20Asli%20Celikyilmaz&entry.1292438233=%20%20Inference-time%20search%20algorithms%20such%20as%20Monte-Carlo%20Tree%20Search%20%28MCTS%29%20may%0Aseem%20unnecessary%20when%20generating%20natural%20language%20text%20based%20on%0Astate-of-the-art%20reinforcement%20learning%20such%20as%20Proximal%20Policy%20Optimization%0A%28PPO%29.%20In%20this%20paper%2C%20we%20demonstrate%20that%20it%20is%20possible%20to%20get%20extra%20mileage%0Aout%20of%20PPO%20by%20integrating%20MCTS%20on%20top.%20The%20key%20idea%20is%20not%20to%20throw%20out%20the%0Avalue%20network%2C%20a%20byproduct%20of%20PPO%20training%20for%20evaluating%20partial%20output%0Asequences%2C%20when%20decoding%20text%20out%20of%20the%20policy%20network.%20More%20concretely%2C%20we%0Apresent%20a%20novel%20value-guided%20decoding%20algorithm%20called%20PPO-MCTS%2C%20which%20can%0Aintegrate%20the%20value%20network%20from%20PPO%20to%20work%20closely%20with%20the%20policy%20network%0Aduring%20inference-time%20generation.%20Compared%20to%20prior%20approaches%20based%20on%20MCTS%0Afor%20controlled%20text%20generation%2C%20the%20key%20strength%20of%20our%20approach%20is%20to%20reduce%0Athe%20fundamental%20mismatch%20of%20the%20scoring%20mechanisms%20of%20the%20partial%20outputs%0Abetween%20training%20and%20test.%20Evaluation%20on%20four%20text%20generation%20tasks%20demonstrate%0Athat%20PPO-MCTS%20greatly%20improves%20the%20preferability%20of%20generated%20text%20compared%20to%0Athe%20standard%20practice%20of%20using%20only%20the%20PPO%20policy.%20Our%20results%20demonstrate%20the%0Apromise%20of%20search%20algorithms%20even%20on%20top%20of%20the%20aligned%20language%20models%20from%0APPO%2C%20and%20the%20under-explored%20benefit%20of%20the%20value%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15028v3&entry.124074799=Read"},
{"title": "Topic-based Watermarks for LLM-Generated Text", "author": "Alexander Nemecek and Yuzhou Jiang and Erman Ayday", "abstract": "  Recent advancements of large language models (LLMs) have resulted in\nindistinguishable text outputs comparable to human-generated text. Watermarking\nalgorithms are potential tools that offer a way to differentiate between LLM-\nand human-generated text by embedding detectable signatures within\nLLM-generated output. However, current watermarking schemes lack robustness\nagainst known attacks against watermarking algorithms. In addition, they are\nimpractical considering an LLM generates tens of thousands of text outputs per\nday and the watermarking algorithm needs to memorize each output it generates\nfor the detection to work. In this work, focusing on the limitations of current\nwatermarking schemes, we propose the concept of a \"topic-based watermarking\nalgorithm\" for LLMs. The proposed algorithm determines how to generate tokens\nfor the watermarked LLM output based on extracted topics of an input prompt or\nthe output of a non-watermarked LLM. Inspired from previous work, we propose\nusing a pair of lists (that are generated based on the specified extracted\ntopic(s)) that specify certain tokens to be included or excluded while\ngenerating the watermarked output of the LLM. Using the proposed watermarking\nalgorithm, we show the practicality of a watermark detection algorithm.\nFurthermore, we discuss a wide range of attacks that can emerge against\nwatermarking algorithms for LLMs and the benefit of the proposed watermarking\nscheme for the feasibility of modeling a potential attacker considering its\nbenefit vs. loss.\n", "link": "http://arxiv.org/abs/2404.02138v1", "date": "2024-04-02", "relevancy": 1.8463, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4636}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4605}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4601}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Topic-based%20Watermarks%20for%20LLM-Generated%20Text&body=Title%3A%20Topic-based%20Watermarks%20for%20LLM-Generated%20Text%0AAuthor%3A%20Alexander%20Nemecek%20and%20Yuzhou%20Jiang%20and%20Erman%20Ayday%0AAbstract%3A%20%20%20Recent%20advancements%20of%20large%20language%20models%20%28LLMs%29%20have%20resulted%20in%0Aindistinguishable%20text%20outputs%20comparable%20to%20human-generated%20text.%20Watermarking%0Aalgorithms%20are%20potential%20tools%20that%20offer%20a%20way%20to%20differentiate%20between%20LLM-%0Aand%20human-generated%20text%20by%20embedding%20detectable%20signatures%20within%0ALLM-generated%20output.%20However%2C%20current%20watermarking%20schemes%20lack%20robustness%0Aagainst%20known%20attacks%20against%20watermarking%20algorithms.%20In%20addition%2C%20they%20are%0Aimpractical%20considering%20an%20LLM%20generates%20tens%20of%20thousands%20of%20text%20outputs%20per%0Aday%20and%20the%20watermarking%20algorithm%20needs%20to%20memorize%20each%20output%20it%20generates%0Afor%20the%20detection%20to%20work.%20In%20this%20work%2C%20focusing%20on%20the%20limitations%20of%20current%0Awatermarking%20schemes%2C%20we%20propose%20the%20concept%20of%20a%20%22topic-based%20watermarking%0Aalgorithm%22%20for%20LLMs.%20The%20proposed%20algorithm%20determines%20how%20to%20generate%20tokens%0Afor%20the%20watermarked%20LLM%20output%20based%20on%20extracted%20topics%20of%20an%20input%20prompt%20or%0Athe%20output%20of%20a%20non-watermarked%20LLM.%20Inspired%20from%20previous%20work%2C%20we%20propose%0Ausing%20a%20pair%20of%20lists%20%28that%20are%20generated%20based%20on%20the%20specified%20extracted%0Atopic%28s%29%29%20that%20specify%20certain%20tokens%20to%20be%20included%20or%20excluded%20while%0Agenerating%20the%20watermarked%20output%20of%20the%20LLM.%20Using%20the%20proposed%20watermarking%0Aalgorithm%2C%20we%20show%20the%20practicality%20of%20a%20watermark%20detection%20algorithm.%0AFurthermore%2C%20we%20discuss%20a%20wide%20range%20of%20attacks%20that%20can%20emerge%20against%0Awatermarking%20algorithms%20for%20LLMs%20and%20the%20benefit%20of%20the%20proposed%20watermarking%0Ascheme%20for%20the%20feasibility%20of%20modeling%20a%20potential%20attacker%20considering%20its%0Abenefit%20vs.%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02138v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topic-based%20Watermarks%20for%20LLM-Generated%20Text&entry.906535625=Alexander%20Nemecek%20and%20Yuzhou%20Jiang%20and%20Erman%20Ayday&entry.1292438233=%20%20Recent%20advancements%20of%20large%20language%20models%20%28LLMs%29%20have%20resulted%20in%0Aindistinguishable%20text%20outputs%20comparable%20to%20human-generated%20text.%20Watermarking%0Aalgorithms%20are%20potential%20tools%20that%20offer%20a%20way%20to%20differentiate%20between%20LLM-%0Aand%20human-generated%20text%20by%20embedding%20detectable%20signatures%20within%0ALLM-generated%20output.%20However%2C%20current%20watermarking%20schemes%20lack%20robustness%0Aagainst%20known%20attacks%20against%20watermarking%20algorithms.%20In%20addition%2C%20they%20are%0Aimpractical%20considering%20an%20LLM%20generates%20tens%20of%20thousands%20of%20text%20outputs%20per%0Aday%20and%20the%20watermarking%20algorithm%20needs%20to%20memorize%20each%20output%20it%20generates%0Afor%20the%20detection%20to%20work.%20In%20this%20work%2C%20focusing%20on%20the%20limitations%20of%20current%0Awatermarking%20schemes%2C%20we%20propose%20the%20concept%20of%20a%20%22topic-based%20watermarking%0Aalgorithm%22%20for%20LLMs.%20The%20proposed%20algorithm%20determines%20how%20to%20generate%20tokens%0Afor%20the%20watermarked%20LLM%20output%20based%20on%20extracted%20topics%20of%20an%20input%20prompt%20or%0Athe%20output%20of%20a%20non-watermarked%20LLM.%20Inspired%20from%20previous%20work%2C%20we%20propose%0Ausing%20a%20pair%20of%20lists%20%28that%20are%20generated%20based%20on%20the%20specified%20extracted%0Atopic%28s%29%29%20that%20specify%20certain%20tokens%20to%20be%20included%20or%20excluded%20while%0Agenerating%20the%20watermarked%20output%20of%20the%20LLM.%20Using%20the%20proposed%20watermarking%0Aalgorithm%2C%20we%20show%20the%20practicality%20of%20a%20watermark%20detection%20algorithm.%0AFurthermore%2C%20we%20discuss%20a%20wide%20range%20of%20attacks%20that%20can%20emerge%20against%0Awatermarking%20algorithms%20for%20LLMs%20and%20the%20benefit%20of%20the%20proposed%20watermarking%0Ascheme%20for%20the%20feasibility%20of%20modeling%20a%20potential%20attacker%20considering%20its%0Abenefit%20vs.%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02138v1&entry.124074799=Read"},
{"title": "Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge\n  Transfer Approaches", "author": "Daryna Dementieva and Valeriia Khylenko and Georg Groh", "abstract": "  Despite the extensive amount of labeled datasets in the NLP text\nclassification field, the persistent imbalance in data availability across\nvarious languages remains evident. Ukrainian, in particular, stands as a\nlanguage that still can benefit from the continued refinement of cross-lingual\nmethodologies. Due to our knowledge, there is a tremendous lack of Ukrainian\ncorpora for typical text classification tasks. In this work, we leverage the\nstate-of-the-art advances in NLP, exploring cross-lingual knowledge transfer\nmethods avoiding manual data curation: large multilingual encoders and\ntranslation systems, LLMs, and language adapters. We test the approaches on\nthree text classification tasks -- toxicity classification, formality\nclassification, and natural language inference -- providing the \"recipe\" for\nthe optimal setups.\n", "link": "http://arxiv.org/abs/2404.02043v1", "date": "2024-04-02", "relevancy": 1.8326, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4546}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4532}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ukrainian%20Texts%20Classification%3A%20Exploration%20of%20Cross-lingual%20Knowledge%0A%20%20Transfer%20Approaches&body=Title%3A%20Ukrainian%20Texts%20Classification%3A%20Exploration%20of%20Cross-lingual%20Knowledge%0A%20%20Transfer%20Approaches%0AAuthor%3A%20Daryna%20Dementieva%20and%20Valeriia%20Khylenko%20and%20Georg%20Groh%0AAbstract%3A%20%20%20Despite%20the%20extensive%20amount%20of%20labeled%20datasets%20in%20the%20NLP%20text%0Aclassification%20field%2C%20the%20persistent%20imbalance%20in%20data%20availability%20across%0Avarious%20languages%20remains%20evident.%20Ukrainian%2C%20in%20particular%2C%20stands%20as%20a%0Alanguage%20that%20still%20can%20benefit%20from%20the%20continued%20refinement%20of%20cross-lingual%0Amethodologies.%20Due%20to%20our%20knowledge%2C%20there%20is%20a%20tremendous%20lack%20of%20Ukrainian%0Acorpora%20for%20typical%20text%20classification%20tasks.%20In%20this%20work%2C%20we%20leverage%20the%0Astate-of-the-art%20advances%20in%20NLP%2C%20exploring%20cross-lingual%20knowledge%20transfer%0Amethods%20avoiding%20manual%20data%20curation%3A%20large%20multilingual%20encoders%20and%0Atranslation%20systems%2C%20LLMs%2C%20and%20language%20adapters.%20We%20test%20the%20approaches%20on%0Athree%20text%20classification%20tasks%20--%20toxicity%20classification%2C%20formality%0Aclassification%2C%20and%20natural%20language%20inference%20--%20providing%20the%20%22recipe%22%20for%0Athe%20optimal%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02043v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ukrainian%20Texts%20Classification%3A%20Exploration%20of%20Cross-lingual%20Knowledge%0A%20%20Transfer%20Approaches&entry.906535625=Daryna%20Dementieva%20and%20Valeriia%20Khylenko%20and%20Georg%20Groh&entry.1292438233=%20%20Despite%20the%20extensive%20amount%20of%20labeled%20datasets%20in%20the%20NLP%20text%0Aclassification%20field%2C%20the%20persistent%20imbalance%20in%20data%20availability%20across%0Avarious%20languages%20remains%20evident.%20Ukrainian%2C%20in%20particular%2C%20stands%20as%20a%0Alanguage%20that%20still%20can%20benefit%20from%20the%20continued%20refinement%20of%20cross-lingual%0Amethodologies.%20Due%20to%20our%20knowledge%2C%20there%20is%20a%20tremendous%20lack%20of%20Ukrainian%0Acorpora%20for%20typical%20text%20classification%20tasks.%20In%20this%20work%2C%20we%20leverage%20the%0Astate-of-the-art%20advances%20in%20NLP%2C%20exploring%20cross-lingual%20knowledge%20transfer%0Amethods%20avoiding%20manual%20data%20curation%3A%20large%20multilingual%20encoders%20and%0Atranslation%20systems%2C%20LLMs%2C%20and%20language%20adapters.%20We%20test%20the%20approaches%20on%0Athree%20text%20classification%20tasks%20--%20toxicity%20classification%2C%20formality%0Aclassification%2C%20and%20natural%20language%20inference%20--%20providing%20the%20%22recipe%22%20for%0Athe%20optimal%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02043v1&entry.124074799=Read"},
{"title": "ImageNot: A contrast with ImageNet preserves model rankings", "author": "Olawale Salaudeen and Moritz Hardt", "abstract": "  We introduce ImageNot, a dataset designed to match the scale of ImageNet\nwhile differing drastically in other aspects. We show that key model\narchitectures developed for ImageNet over the years rank identically when\ntrained and evaluated on ImageNot to how they rank on ImageNet. This is true\nwhen training models from scratch or fine-tuning them. Moreover, the relative\nimprovements of each model over earlier models strongly correlate in both\ndatasets. We further give evidence that ImageNot has a similar utility as\nImageNet for transfer learning purposes. Our work demonstrates a surprising\ndegree of external validity in the relative performance of image classification\nmodels. This stands in contrast with absolute accuracy numbers that typically\ndrop sharply even under small changes to a dataset.\n", "link": "http://arxiv.org/abs/2404.02112v1", "date": "2024-04-02", "relevancy": 1.8242, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4811}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4531}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4489}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ImageNot%3A%20A%20contrast%20with%20ImageNet%20preserves%20model%20rankings&body=Title%3A%20ImageNot%3A%20A%20contrast%20with%20ImageNet%20preserves%20model%20rankings%0AAuthor%3A%20Olawale%20Salaudeen%20and%20Moritz%20Hardt%0AAbstract%3A%20%20%20We%20introduce%20ImageNot%2C%20a%20dataset%20designed%20to%20match%20the%20scale%20of%20ImageNet%0Awhile%20differing%20drastically%20in%20other%20aspects.%20We%20show%20that%20key%20model%0Aarchitectures%20developed%20for%20ImageNet%20over%20the%20years%20rank%20identically%20when%0Atrained%20and%20evaluated%20on%20ImageNot%20to%20how%20they%20rank%20on%20ImageNet.%20This%20is%20true%0Awhen%20training%20models%20from%20scratch%20or%20fine-tuning%20them.%20Moreover%2C%20the%20relative%0Aimprovements%20of%20each%20model%20over%20earlier%20models%20strongly%20correlate%20in%20both%0Adatasets.%20We%20further%20give%20evidence%20that%20ImageNot%20has%20a%20similar%20utility%20as%0AImageNet%20for%20transfer%20learning%20purposes.%20Our%20work%20demonstrates%20a%20surprising%0Adegree%20of%20external%20validity%20in%20the%20relative%20performance%20of%20image%20classification%0Amodels.%20This%20stands%20in%20contrast%20with%20absolute%20accuracy%20numbers%20that%20typically%0Adrop%20sharply%20even%20under%20small%20changes%20to%20a%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02112v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImageNot%3A%20A%20contrast%20with%20ImageNet%20preserves%20model%20rankings&entry.906535625=Olawale%20Salaudeen%20and%20Moritz%20Hardt&entry.1292438233=%20%20We%20introduce%20ImageNot%2C%20a%20dataset%20designed%20to%20match%20the%20scale%20of%20ImageNet%0Awhile%20differing%20drastically%20in%20other%20aspects.%20We%20show%20that%20key%20model%0Aarchitectures%20developed%20for%20ImageNet%20over%20the%20years%20rank%20identically%20when%0Atrained%20and%20evaluated%20on%20ImageNot%20to%20how%20they%20rank%20on%20ImageNet.%20This%20is%20true%0Awhen%20training%20models%20from%20scratch%20or%20fine-tuning%20them.%20Moreover%2C%20the%20relative%0Aimprovements%20of%20each%20model%20over%20earlier%20models%20strongly%20correlate%20in%20both%0Adatasets.%20We%20further%20give%20evidence%20that%20ImageNot%20has%20a%20similar%20utility%20as%0AImageNet%20for%20transfer%20learning%20purposes.%20Our%20work%20demonstrates%20a%20surprising%0Adegree%20of%20external%20validity%20in%20the%20relative%20performance%20of%20image%20classification%0Amodels.%20This%20stands%20in%20contrast%20with%20absolute%20accuracy%20numbers%20that%20typically%0Adrop%20sharply%20even%20under%20small%20changes%20to%20a%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02112v1&entry.124074799=Read"},
{"title": "FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data\n  Mixtures for Legal Reasoning", "author": "Joel Niklaus and Lucia Zheng and Arya D. McCarthy and Christopher Hahn and Brian M. Rosen and Peter Henderson and Daniel E. Ho and Garrett Honke and Percy Liang and Christopher Manning", "abstract": "  Instruction tuning is an important step in making language models useful for\ndirect user interaction. However, many legal tasks remain out of reach for most\nopen LLMs and there do not yet exist any large scale instruction datasets for\nthe domain. This critically limits research in this application area. In this\nwork, we curate LawInstruct, a large legal instruction dataset, covering 17\njurisdictions, 24 languages and a total of 12M examples. We present evidence\nthat domain-specific pretraining and instruction tuning improve performance on\nLegalBench, including improving Flan-T5 XL by 8 points or 16\\% over the\nbaseline. However, the effect does not generalize across all tasks, training\nregimes, model sizes, and other factors. LawInstruct is a resource for\naccelerating the development of models with stronger information processing and\ndecision making capabilities in the legal domain.\n", "link": "http://arxiv.org/abs/2404.02127v1", "date": "2024-04-02", "relevancy": 1.8034, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4663}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4558}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4397}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FLawN-T5%3A%20An%20Empirical%20Examination%20of%20Effective%20Instruction-Tuning%20Data%0A%20%20Mixtures%20for%20Legal%20Reasoning&body=Title%3A%20FLawN-T5%3A%20An%20Empirical%20Examination%20of%20Effective%20Instruction-Tuning%20Data%0A%20%20Mixtures%20for%20Legal%20Reasoning%0AAuthor%3A%20Joel%20Niklaus%20and%20Lucia%20Zheng%20and%20Arya%20D.%20McCarthy%20and%20Christopher%20Hahn%20and%20Brian%20M.%20Rosen%20and%20Peter%20Henderson%20and%20Daniel%20E.%20Ho%20and%20Garrett%20Honke%20and%20Percy%20Liang%20and%20Christopher%20Manning%0AAbstract%3A%20%20%20Instruction%20tuning%20is%20an%20important%20step%20in%20making%20language%20models%20useful%20for%0Adirect%20user%20interaction.%20However%2C%20many%20legal%20tasks%20remain%20out%20of%20reach%20for%20most%0Aopen%20LLMs%20and%20there%20do%20not%20yet%20exist%20any%20large%20scale%20instruction%20datasets%20for%0Athe%20domain.%20This%20critically%20limits%20research%20in%20this%20application%20area.%20In%20this%0Awork%2C%20we%20curate%20LawInstruct%2C%20a%20large%20legal%20instruction%20dataset%2C%20covering%2017%0Ajurisdictions%2C%2024%20languages%20and%20a%20total%20of%2012M%20examples.%20We%20present%20evidence%0Athat%20domain-specific%20pretraining%20and%20instruction%20tuning%20improve%20performance%20on%0ALegalBench%2C%20including%20improving%20Flan-T5%20XL%20by%208%20points%20or%2016%5C%25%20over%20the%0Abaseline.%20However%2C%20the%20effect%20does%20not%20generalize%20across%20all%20tasks%2C%20training%0Aregimes%2C%20model%20sizes%2C%20and%20other%20factors.%20LawInstruct%20is%20a%20resource%20for%0Aaccelerating%20the%20development%20of%20models%20with%20stronger%20information%20processing%20and%0Adecision%20making%20capabilities%20in%20the%20legal%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02127v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLawN-T5%3A%20An%20Empirical%20Examination%20of%20Effective%20Instruction-Tuning%20Data%0A%20%20Mixtures%20for%20Legal%20Reasoning&entry.906535625=Joel%20Niklaus%20and%20Lucia%20Zheng%20and%20Arya%20D.%20McCarthy%20and%20Christopher%20Hahn%20and%20Brian%20M.%20Rosen%20and%20Peter%20Henderson%20and%20Daniel%20E.%20Ho%20and%20Garrett%20Honke%20and%20Percy%20Liang%20and%20Christopher%20Manning&entry.1292438233=%20%20Instruction%20tuning%20is%20an%20important%20step%20in%20making%20language%20models%20useful%20for%0Adirect%20user%20interaction.%20However%2C%20many%20legal%20tasks%20remain%20out%20of%20reach%20for%20most%0Aopen%20LLMs%20and%20there%20do%20not%20yet%20exist%20any%20large%20scale%20instruction%20datasets%20for%0Athe%20domain.%20This%20critically%20limits%20research%20in%20this%20application%20area.%20In%20this%0Awork%2C%20we%20curate%20LawInstruct%2C%20a%20large%20legal%20instruction%20dataset%2C%20covering%2017%0Ajurisdictions%2C%2024%20languages%20and%20a%20total%20of%2012M%20examples.%20We%20present%20evidence%0Athat%20domain-specific%20pretraining%20and%20instruction%20tuning%20improve%20performance%20on%0ALegalBench%2C%20including%20improving%20Flan-T5%20XL%20by%208%20points%20or%2016%5C%25%20over%20the%0Abaseline.%20However%2C%20the%20effect%20does%20not%20generalize%20across%20all%20tasks%2C%20training%0Aregimes%2C%20model%20sizes%2C%20and%20other%20factors.%20LawInstruct%20is%20a%20resource%20for%0Aaccelerating%20the%20development%20of%20models%20with%20stronger%20information%20processing%20and%0Adecision%20making%20capabilities%20in%20the%20legal%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02127v1&entry.124074799=Read"},
{"title": "MuChin: A Chinese Colloquial Description Benchmark for Evaluating\n  Language Models in the Field of Music", "author": "Zihao Wang and Shuyu Li and Tao Zhang and Qi Wang and Pengfei Yu and Jinyang Luo and Yan Liu and Ming Xi and Kejun Zhang", "abstract": "  The rapidly evolving multimodal Large Language Models (LLMs) urgently require\nnew benchmarks to uniformly evaluate their performance on understanding and\ntextually describing music. However, due to semantic gaps between Music\nInformation Retrieval (MIR) algorithms and human understanding, discrepancies\nbetween professionals and the public, and low precision of annotations,\nexisting music description datasets cannot serve as benchmarks. To this end, we\npresent MuChin, the first open-source music description benchmark in Chinese\ncolloquial language, designed to evaluate the performance of multimodal LLMs in\nunderstanding and describing music. We established the Caichong Music\nAnnotation Platform (CaiMAP) that employs an innovative multi-person,\nmulti-stage assurance method, and recruited both amateurs and professionals to\nensure the precision of annotations and alignment with popular semantics.\nUtilizing this method, we built a dataset with multi-dimensional,\nhigh-precision music annotations, the Caichong Music Dataset (CaiMD), and\ncarefully selected 1,000 high-quality entries to serve as the test set for\nMuChin. Based on MuChin, we analyzed the discrepancies between professionals\nand amateurs in terms of music description, and empirically demonstrated the\neffectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed\nMuChin to evaluate existing music understanding models on their ability to\nprovide colloquial descriptions of music. All data related to the benchmark and\nthe code for scoring have been open-sourced.\n", "link": "http://arxiv.org/abs/2402.09871v2", "date": "2024-04-02", "relevancy": 1.8027, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4627}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.431}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MuChin%3A%20A%20Chinese%20Colloquial%20Description%20Benchmark%20for%20Evaluating%0A%20%20Language%20Models%20in%20the%20Field%20of%20Music&body=Title%3A%20MuChin%3A%20A%20Chinese%20Colloquial%20Description%20Benchmark%20for%20Evaluating%0A%20%20Language%20Models%20in%20the%20Field%20of%20Music%0AAuthor%3A%20Zihao%20Wang%20and%20Shuyu%20Li%20and%20Tao%20Zhang%20and%20Qi%20Wang%20and%20Pengfei%20Yu%20and%20Jinyang%20Luo%20and%20Yan%20Liu%20and%20Ming%20Xi%20and%20Kejun%20Zhang%0AAbstract%3A%20%20%20The%20rapidly%20evolving%20multimodal%20Large%20Language%20Models%20%28LLMs%29%20urgently%20require%0Anew%20benchmarks%20to%20uniformly%20evaluate%20their%20performance%20on%20understanding%20and%0Atextually%20describing%20music.%20However%2C%20due%20to%20semantic%20gaps%20between%20Music%0AInformation%20Retrieval%20%28MIR%29%20algorithms%20and%20human%20understanding%2C%20discrepancies%0Abetween%20professionals%20and%20the%20public%2C%20and%20low%20precision%20of%20annotations%2C%0Aexisting%20music%20description%20datasets%20cannot%20serve%20as%20benchmarks.%20To%20this%20end%2C%20we%0Apresent%20MuChin%2C%20the%20first%20open-source%20music%20description%20benchmark%20in%20Chinese%0Acolloquial%20language%2C%20designed%20to%20evaluate%20the%20performance%20of%20multimodal%20LLMs%20in%0Aunderstanding%20and%20describing%20music.%20We%20established%20the%20Caichong%20Music%0AAnnotation%20Platform%20%28CaiMAP%29%20that%20employs%20an%20innovative%20multi-person%2C%0Amulti-stage%20assurance%20method%2C%20and%20recruited%20both%20amateurs%20and%20professionals%20to%0Aensure%20the%20precision%20of%20annotations%20and%20alignment%20with%20popular%20semantics.%0AUtilizing%20this%20method%2C%20we%20built%20a%20dataset%20with%20multi-dimensional%2C%0Ahigh-precision%20music%20annotations%2C%20the%20Caichong%20Music%20Dataset%20%28CaiMD%29%2C%20and%0Acarefully%20selected%201%2C000%20high-quality%20entries%20to%20serve%20as%20the%20test%20set%20for%0AMuChin.%20Based%20on%20MuChin%2C%20we%20analyzed%20the%20discrepancies%20between%20professionals%0Aand%20amateurs%20in%20terms%20of%20music%20description%2C%20and%20empirically%20demonstrated%20the%0Aeffectiveness%20of%20annotated%20data%20for%20fine-tuning%20LLMs.%20Ultimately%2C%20we%20employed%0AMuChin%20to%20evaluate%20existing%20music%20understanding%20models%20on%20their%20ability%20to%0Aprovide%20colloquial%20descriptions%20of%20music.%20All%20data%20related%20to%20the%20benchmark%20and%0Athe%20code%20for%20scoring%20have%20been%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09871v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuChin%3A%20A%20Chinese%20Colloquial%20Description%20Benchmark%20for%20Evaluating%0A%20%20Language%20Models%20in%20the%20Field%20of%20Music&entry.906535625=Zihao%20Wang%20and%20Shuyu%20Li%20and%20Tao%20Zhang%20and%20Qi%20Wang%20and%20Pengfei%20Yu%20and%20Jinyang%20Luo%20and%20Yan%20Liu%20and%20Ming%20Xi%20and%20Kejun%20Zhang&entry.1292438233=%20%20The%20rapidly%20evolving%20multimodal%20Large%20Language%20Models%20%28LLMs%29%20urgently%20require%0Anew%20benchmarks%20to%20uniformly%20evaluate%20their%20performance%20on%20understanding%20and%0Atextually%20describing%20music.%20However%2C%20due%20to%20semantic%20gaps%20between%20Music%0AInformation%20Retrieval%20%28MIR%29%20algorithms%20and%20human%20understanding%2C%20discrepancies%0Abetween%20professionals%20and%20the%20public%2C%20and%20low%20precision%20of%20annotations%2C%0Aexisting%20music%20description%20datasets%20cannot%20serve%20as%20benchmarks.%20To%20this%20end%2C%20we%0Apresent%20MuChin%2C%20the%20first%20open-source%20music%20description%20benchmark%20in%20Chinese%0Acolloquial%20language%2C%20designed%20to%20evaluate%20the%20performance%20of%20multimodal%20LLMs%20in%0Aunderstanding%20and%20describing%20music.%20We%20established%20the%20Caichong%20Music%0AAnnotation%20Platform%20%28CaiMAP%29%20that%20employs%20an%20innovative%20multi-person%2C%0Amulti-stage%20assurance%20method%2C%20and%20recruited%20both%20amateurs%20and%20professionals%20to%0Aensure%20the%20precision%20of%20annotations%20and%20alignment%20with%20popular%20semantics.%0AUtilizing%20this%20method%2C%20we%20built%20a%20dataset%20with%20multi-dimensional%2C%0Ahigh-precision%20music%20annotations%2C%20the%20Caichong%20Music%20Dataset%20%28CaiMD%29%2C%20and%0Acarefully%20selected%201%2C000%20high-quality%20entries%20to%20serve%20as%20the%20test%20set%20for%0AMuChin.%20Based%20on%20MuChin%2C%20we%20analyzed%20the%20discrepancies%20between%20professionals%0Aand%20amateurs%20in%20terms%20of%20music%20description%2C%20and%20empirically%20demonstrated%20the%0Aeffectiveness%20of%20annotated%20data%20for%20fine-tuning%20LLMs.%20Ultimately%2C%20we%20employed%0AMuChin%20to%20evaluate%20existing%20music%20understanding%20models%20on%20their%20ability%20to%0Aprovide%20colloquial%20descriptions%20of%20music.%20All%20data%20related%20to%20the%20benchmark%20and%0Athe%20code%20for%20scoring%20have%20been%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09871v2&entry.124074799=Read"},
{"title": "Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using\n  Stable Diffusion", "author": "Junjiao Tian and Lavisha Aggarwal and Andrea Colaco and Zsolt Kira and Mar Gonzalez-Franco", "abstract": "  Producing quality segmentation masks for images is a fundamental problem in\ncomputer vision. Recent research has explored large-scale supervised training\nto enable zero-shot segmentation on virtually any image style and unsupervised\ntraining to enable segmentation without dense annotations. However,\nconstructing a model capable of segmenting anything in a zero-shot manner\nwithout any annotations is still challenging. In this paper, we propose to\nutilize the self-attention layers in stable diffusion models to achieve this\ngoal because the pre-trained stable diffusion model has learned inherent\nconcepts of objects within its attention layers. Specifically, we introduce a\nsimple yet effective iterative merging process based on measuring KL divergence\namong attention maps to merge them into valid segmentation masks. The proposed\nmethod does not require any training or language dependency to extract quality\nsegmentation for any images. On COCO-Stuff-27, our method surpasses the prior\nunsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%\nin mean IoU. The project page is at\n\\url{https://sites.google.com/view/diffseg/home}.\n", "link": "http://arxiv.org/abs/2308.12469v3", "date": "2024-04-02", "relevancy": 1.7869, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6142}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5992}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Diffuse%2C%20Attend%2C%20and%20Segment%3A%20Unsupervised%20Zero-Shot%20Segmentation%20using%0A%20%20Stable%20Diffusion&body=Title%3A%20Diffuse%2C%20Attend%2C%20and%20Segment%3A%20Unsupervised%20Zero-Shot%20Segmentation%20using%0A%20%20Stable%20Diffusion%0AAuthor%3A%20Junjiao%20Tian%20and%20Lavisha%20Aggarwal%20and%20Andrea%20Colaco%20and%20Zsolt%20Kira%20and%20Mar%20Gonzalez-Franco%0AAbstract%3A%20%20%20Producing%20quality%20segmentation%20masks%20for%20images%20is%20a%20fundamental%20problem%20in%0Acomputer%20vision.%20Recent%20research%20has%20explored%20large-scale%20supervised%20training%0Ato%20enable%20zero-shot%20segmentation%20on%20virtually%20any%20image%20style%20and%20unsupervised%0Atraining%20to%20enable%20segmentation%20without%20dense%20annotations.%20However%2C%0Aconstructing%20a%20model%20capable%20of%20segmenting%20anything%20in%20a%20zero-shot%20manner%0Awithout%20any%20annotations%20is%20still%20challenging.%20In%20this%20paper%2C%20we%20propose%20to%0Autilize%20the%20self-attention%20layers%20in%20stable%20diffusion%20models%20to%20achieve%20this%0Agoal%20because%20the%20pre-trained%20stable%20diffusion%20model%20has%20learned%20inherent%0Aconcepts%20of%20objects%20within%20its%20attention%20layers.%20Specifically%2C%20we%20introduce%20a%0Asimple%20yet%20effective%20iterative%20merging%20process%20based%20on%20measuring%20KL%20divergence%0Aamong%20attention%20maps%20to%20merge%20them%20into%20valid%20segmentation%20masks.%20The%20proposed%0Amethod%20does%20not%20require%20any%20training%20or%20language%20dependency%20to%20extract%20quality%0Asegmentation%20for%20any%20images.%20On%20COCO-Stuff-27%2C%20our%20method%20surpasses%20the%20prior%0Aunsupervised%20zero-shot%20SOTA%20method%20by%20an%20absolute%2026%25%20in%20pixel%20accuracy%20and%2017%25%0Ain%20mean%20IoU.%20The%20project%20page%20is%20at%0A%5Curl%7Bhttps%3A//sites.google.com/view/diffseg/home%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12469v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffuse%2C%20Attend%2C%20and%20Segment%3A%20Unsupervised%20Zero-Shot%20Segmentation%20using%0A%20%20Stable%20Diffusion&entry.906535625=Junjiao%20Tian%20and%20Lavisha%20Aggarwal%20and%20Andrea%20Colaco%20and%20Zsolt%20Kira%20and%20Mar%20Gonzalez-Franco&entry.1292438233=%20%20Producing%20quality%20segmentation%20masks%20for%20images%20is%20a%20fundamental%20problem%20in%0Acomputer%20vision.%20Recent%20research%20has%20explored%20large-scale%20supervised%20training%0Ato%20enable%20zero-shot%20segmentation%20on%20virtually%20any%20image%20style%20and%20unsupervised%0Atraining%20to%20enable%20segmentation%20without%20dense%20annotations.%20However%2C%0Aconstructing%20a%20model%20capable%20of%20segmenting%20anything%20in%20a%20zero-shot%20manner%0Awithout%20any%20annotations%20is%20still%20challenging.%20In%20this%20paper%2C%20we%20propose%20to%0Autilize%20the%20self-attention%20layers%20in%20stable%20diffusion%20models%20to%20achieve%20this%0Agoal%20because%20the%20pre-trained%20stable%20diffusion%20model%20has%20learned%20inherent%0Aconcepts%20of%20objects%20within%20its%20attention%20layers.%20Specifically%2C%20we%20introduce%20a%0Asimple%20yet%20effective%20iterative%20merging%20process%20based%20on%20measuring%20KL%20divergence%0Aamong%20attention%20maps%20to%20merge%20them%20into%20valid%20segmentation%20masks.%20The%20proposed%0Amethod%20does%20not%20require%20any%20training%20or%20language%20dependency%20to%20extract%20quality%0Asegmentation%20for%20any%20images.%20On%20COCO-Stuff-27%2C%20our%20method%20surpasses%20the%20prior%0Aunsupervised%20zero-shot%20SOTA%20method%20by%20an%20absolute%2026%25%20in%20pixel%20accuracy%20and%2017%25%0Ain%20mean%20IoU.%20The%20project%20page%20is%20at%0A%5Curl%7Bhttps%3A//sites.google.com/view/diffseg/home%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12469v3&entry.124074799=Read"},
{"title": "SPMamba: State-space model is all you need in speech separation", "author": "Kai Li and Guo Chen", "abstract": "  In speech separation, both CNN- and Transformer-based models have\ndemonstrated robust separation capabilities, garnering significant attention\nwithin the research community. However, CNN-based methods have limited\nmodelling capability for long-sequence audio, leading to suboptimal separation\nperformance. Conversely, Transformer-based methods are limited in practical\napplications due to their high computational complexity. Notably, within\ncomputer vision, Mamba-based methods have been celebrated for their formidable\nperformance and reduced computational requirements. In this paper, we propose a\nnetwork architecture for speech separation using a state-space model, namely\nSPMamba. We adopt the TF-GridNet model as the foundational framework and\nsubstitute its Transformer component with a bidirectional Mamba module, aiming\nto capture a broader range of contextual information. Our experimental results\nreveal an important role in the performance aspects of Mamba-based models.\nSPMamba demonstrates superior performance with a significant advantage over\nexisting separation models in a dataset built on Librispeech. Notably, SPMamba\nachieves a substantial improvement in separation quality, with a 2.42 dB\nenhancement in SI-SNRi compared to the TF-GridNet. The source code for SPMamba\nis publicly accessible at https://github.com/JusperLee/SPMamba .\n", "link": "http://arxiv.org/abs/2404.02063v1", "date": "2024-04-02", "relevancy": 1.7806, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4501}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4399}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SPMamba%3A%20State-space%20model%20is%20all%20you%20need%20in%20speech%20separation&body=Title%3A%20SPMamba%3A%20State-space%20model%20is%20all%20you%20need%20in%20speech%20separation%0AAuthor%3A%20Kai%20Li%20and%20Guo%20Chen%0AAbstract%3A%20%20%20In%20speech%20separation%2C%20both%20CNN-%20and%20Transformer-based%20models%20have%0Ademonstrated%20robust%20separation%20capabilities%2C%20garnering%20significant%20attention%0Awithin%20the%20research%20community.%20However%2C%20CNN-based%20methods%20have%20limited%0Amodelling%20capability%20for%20long-sequence%20audio%2C%20leading%20to%20suboptimal%20separation%0Aperformance.%20Conversely%2C%20Transformer-based%20methods%20are%20limited%20in%20practical%0Aapplications%20due%20to%20their%20high%20computational%20complexity.%20Notably%2C%20within%0Acomputer%20vision%2C%20Mamba-based%20methods%20have%20been%20celebrated%20for%20their%20formidable%0Aperformance%20and%20reduced%20computational%20requirements.%20In%20this%20paper%2C%20we%20propose%20a%0Anetwork%20architecture%20for%20speech%20separation%20using%20a%20state-space%20model%2C%20namely%0ASPMamba.%20We%20adopt%20the%20TF-GridNet%20model%20as%20the%20foundational%20framework%20and%0Asubstitute%20its%20Transformer%20component%20with%20a%20bidirectional%20Mamba%20module%2C%20aiming%0Ato%20capture%20a%20broader%20range%20of%20contextual%20information.%20Our%20experimental%20results%0Areveal%20an%20important%20role%20in%20the%20performance%20aspects%20of%20Mamba-based%20models.%0ASPMamba%20demonstrates%20superior%20performance%20with%20a%20significant%20advantage%20over%0Aexisting%20separation%20models%20in%20a%20dataset%20built%20on%20Librispeech.%20Notably%2C%20SPMamba%0Aachieves%20a%20substantial%20improvement%20in%20separation%20quality%2C%20with%20a%202.42%20dB%0Aenhancement%20in%20SI-SNRi%20compared%20to%20the%20TF-GridNet.%20The%20source%20code%20for%20SPMamba%0Ais%20publicly%20accessible%20at%20https%3A//github.com/JusperLee/SPMamba%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02063v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPMamba%3A%20State-space%20model%20is%20all%20you%20need%20in%20speech%20separation&entry.906535625=Kai%20Li%20and%20Guo%20Chen&entry.1292438233=%20%20In%20speech%20separation%2C%20both%20CNN-%20and%20Transformer-based%20models%20have%0Ademonstrated%20robust%20separation%20capabilities%2C%20garnering%20significant%20attention%0Awithin%20the%20research%20community.%20However%2C%20CNN-based%20methods%20have%20limited%0Amodelling%20capability%20for%20long-sequence%20audio%2C%20leading%20to%20suboptimal%20separation%0Aperformance.%20Conversely%2C%20Transformer-based%20methods%20are%20limited%20in%20practical%0Aapplications%20due%20to%20their%20high%20computational%20complexity.%20Notably%2C%20within%0Acomputer%20vision%2C%20Mamba-based%20methods%20have%20been%20celebrated%20for%20their%20formidable%0Aperformance%20and%20reduced%20computational%20requirements.%20In%20this%20paper%2C%20we%20propose%20a%0Anetwork%20architecture%20for%20speech%20separation%20using%20a%20state-space%20model%2C%20namely%0ASPMamba.%20We%20adopt%20the%20TF-GridNet%20model%20as%20the%20foundational%20framework%20and%0Asubstitute%20its%20Transformer%20component%20with%20a%20bidirectional%20Mamba%20module%2C%20aiming%0Ato%20capture%20a%20broader%20range%20of%20contextual%20information.%20Our%20experimental%20results%0Areveal%20an%20important%20role%20in%20the%20performance%20aspects%20of%20Mamba-based%20models.%0ASPMamba%20demonstrates%20superior%20performance%20with%20a%20significant%20advantage%20over%0Aexisting%20separation%20models%20in%20a%20dataset%20built%20on%20Librispeech.%20Notably%2C%20SPMamba%0Aachieves%20a%20substantial%20improvement%20in%20separation%20quality%2C%20with%20a%202.42%20dB%0Aenhancement%20in%20SI-SNRi%20compared%20to%20the%20TF-GridNet.%20The%20source%20code%20for%20SPMamba%0Ais%20publicly%20accessible%20at%20https%3A//github.com/JusperLee/SPMamba%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02063v1&entry.124074799=Read"},
{"title": "Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1:\n  Framework and Benchmarks", "author": "Jackson Burns and William Green", "abstract": "  Quantitative Structure Property Relationship studies aim to define a mapping\nbetween molecular structure and arbitrary quantities of interest. This was\nhistorically accomplished via the development of descriptors which requires\nsignificant domain expertise and struggles to generalize. Thus the field has\nmorphed into Molecular Property Prediction and been given over to learned\nrepresentations which are highly generalizable. The paper introduces fastprop,\na DeepQSPR framework which uses a cogent set of molecular level descriptors to\nmeet and exceed the performance of learned representations on diverse datasets\nin dramatically less time. fastprop is freely available on github at\ngithub.com/JacksonBurns/fastprop.\n", "link": "http://arxiv.org/abs/2404.02058v1", "date": "2024-04-02", "relevancy": 1.773, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5048}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4319}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.43}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalizable%2C%20Fast%2C%20and%20Accurate%20DeepQSPR%20with%20fastprop%20Part%201%3A%0A%20%20Framework%20and%20Benchmarks&body=Title%3A%20Generalizable%2C%20Fast%2C%20and%20Accurate%20DeepQSPR%20with%20fastprop%20Part%201%3A%0A%20%20Framework%20and%20Benchmarks%0AAuthor%3A%20Jackson%20Burns%20and%20William%20Green%0AAbstract%3A%20%20%20Quantitative%20Structure%20Property%20Relationship%20studies%20aim%20to%20define%20a%20mapping%0Abetween%20molecular%20structure%20and%20arbitrary%20quantities%20of%20interest.%20This%20was%0Ahistorically%20accomplished%20via%20the%20development%20of%20descriptors%20which%20requires%0Asignificant%20domain%20expertise%20and%20struggles%20to%20generalize.%20Thus%20the%20field%20has%0Amorphed%20into%20Molecular%20Property%20Prediction%20and%20been%20given%20over%20to%20learned%0Arepresentations%20which%20are%20highly%20generalizable.%20The%20paper%20introduces%20fastprop%2C%0Aa%20DeepQSPR%20framework%20which%20uses%20a%20cogent%20set%20of%20molecular%20level%20descriptors%20to%0Ameet%20and%20exceed%20the%20performance%20of%20learned%20representations%20on%20diverse%20datasets%0Ain%20dramatically%20less%20time.%20fastprop%20is%20freely%20available%20on%20github%20at%0Agithub.com/JacksonBurns/fastprop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02058v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%2C%20Fast%2C%20and%20Accurate%20DeepQSPR%20with%20fastprop%20Part%201%3A%0A%20%20Framework%20and%20Benchmarks&entry.906535625=Jackson%20Burns%20and%20William%20Green&entry.1292438233=%20%20Quantitative%20Structure%20Property%20Relationship%20studies%20aim%20to%20define%20a%20mapping%0Abetween%20molecular%20structure%20and%20arbitrary%20quantities%20of%20interest.%20This%20was%0Ahistorically%20accomplished%20via%20the%20development%20of%20descriptors%20which%20requires%0Asignificant%20domain%20expertise%20and%20struggles%20to%20generalize.%20Thus%20the%20field%20has%0Amorphed%20into%20Molecular%20Property%20Prediction%20and%20been%20given%20over%20to%20learned%0Arepresentations%20which%20are%20highly%20generalizable.%20The%20paper%20introduces%20fastprop%2C%0Aa%20DeepQSPR%20framework%20which%20uses%20a%20cogent%20set%20of%20molecular%20level%20descriptors%20to%0Ameet%20and%20exceed%20the%20performance%20of%20learned%20representations%20on%20diverse%20datasets%0Ain%20dramatically%20less%20time.%20fastprop%20is%20freely%20available%20on%20github%20at%0Agithub.com/JacksonBurns/fastprop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02058v1&entry.124074799=Read"},
{"title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks", "author": "Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion", "abstract": "  We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize the target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve\nnearly 100\\% attack success rate -- according to GPT-4 as a judge -- on\nGPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was\nadversarially trained against the GCG attack. We also show how to jailbreak all\nClaude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with 100\\% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). We provide the code, prompts, and\nlogs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.\n", "link": "http://arxiv.org/abs/2404.02151v1", "date": "2024-04-02", "relevancy": 1.7642, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4366}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4054}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Jailbreaking%20Leading%20Safety-Aligned%20LLMs%20with%20Simple%20Adaptive%20Attacks&body=Title%3A%20Jailbreaking%20Leading%20Safety-Aligned%20LLMs%20with%20Simple%20Adaptive%20Attacks%0AAuthor%3A%20Maksym%20Andriushchenko%20and%20Francesco%20Croce%20and%20Nicolas%20Flammarion%0AAbstract%3A%20%20%20We%20show%20that%20even%20the%20most%20recent%20safety-aligned%20LLMs%20are%20not%20robust%20to%0Asimple%20adaptive%20jailbreaking%20attacks.%20First%2C%20we%20demonstrate%20how%20to%20successfully%0Aleverage%20access%20to%20logprobs%20for%20jailbreaking%3A%20we%20initially%20design%20an%0Aadversarial%20prompt%20template%20%28sometimes%20adapted%20to%20the%20target%20LLM%29%2C%20and%20then%20we%0Aapply%20random%20search%20on%20a%20suffix%20to%20maximize%20the%20target%20logprob%20%28e.g.%2C%20of%20the%0Atoken%20%22Sure%22%29%2C%20potentially%20with%20multiple%20restarts.%20In%20this%20way%2C%20we%20achieve%0Anearly%20100%5C%25%20attack%20success%20rate%20--%20according%20to%20GPT-4%20as%20a%20judge%20--%20on%0AGPT-3.5/4%2C%20Llama-2-Chat-7B/13B/70B%2C%20Gemma-7B%2C%20and%20R2D2%20from%20HarmBench%20that%20was%0Aadversarially%20trained%20against%20the%20GCG%20attack.%20We%20also%20show%20how%20to%20jailbreak%20all%0AClaude%20models%20--%20that%20do%20not%20expose%20logprobs%20--%20via%20either%20a%20transfer%20or%0Aprefilling%20attack%20with%20100%5C%25%20success%20rate.%20In%20addition%2C%20we%20show%20how%20to%20use%0Arandom%20search%20on%20a%20restricted%20set%20of%20tokens%20for%20finding%20trojan%20strings%20in%0Apoisoned%20models%20--%20a%20task%20that%20shares%20many%20similarities%20with%20jailbreaking%20--%0Awhich%20is%20the%20algorithm%20that%20brought%20us%20the%20first%20place%20in%20the%20SaTML%2724%20Trojan%0ADetection%20Competition.%20The%20common%20theme%20behind%20these%20attacks%20is%20that%20adaptivity%0Ais%20crucial%3A%20different%20models%20are%20vulnerable%20to%20different%20prompting%20templates%0A%28e.g.%2C%20R2D2%20is%20very%20sensitive%20to%20in-context%20learning%20prompts%29%2C%20some%20models%20have%0Aunique%20vulnerabilities%20based%20on%20their%20APIs%20%28e.g.%2C%20prefilling%20for%20Claude%29%2C%20and%0Ain%20some%20settings%20it%20is%20crucial%20to%20restrict%20the%20token%20search%20space%20based%20on%0Aprior%20knowledge%20%28e.g.%2C%20for%20trojan%20detection%29.%20We%20provide%20the%20code%2C%20prompts%2C%20and%0Alogs%20of%20the%20attacks%20at%20https%3A//github.com/tml-epfl/llm-adaptive-attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02151v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jailbreaking%20Leading%20Safety-Aligned%20LLMs%20with%20Simple%20Adaptive%20Attacks&entry.906535625=Maksym%20Andriushchenko%20and%20Francesco%20Croce%20and%20Nicolas%20Flammarion&entry.1292438233=%20%20We%20show%20that%20even%20the%20most%20recent%20safety-aligned%20LLMs%20are%20not%20robust%20to%0Asimple%20adaptive%20jailbreaking%20attacks.%20First%2C%20we%20demonstrate%20how%20to%20successfully%0Aleverage%20access%20to%20logprobs%20for%20jailbreaking%3A%20we%20initially%20design%20an%0Aadversarial%20prompt%20template%20%28sometimes%20adapted%20to%20the%20target%20LLM%29%2C%20and%20then%20we%0Aapply%20random%20search%20on%20a%20suffix%20to%20maximize%20the%20target%20logprob%20%28e.g.%2C%20of%20the%0Atoken%20%22Sure%22%29%2C%20potentially%20with%20multiple%20restarts.%20In%20this%20way%2C%20we%20achieve%0Anearly%20100%5C%25%20attack%20success%20rate%20--%20according%20to%20GPT-4%20as%20a%20judge%20--%20on%0AGPT-3.5/4%2C%20Llama-2-Chat-7B/13B/70B%2C%20Gemma-7B%2C%20and%20R2D2%20from%20HarmBench%20that%20was%0Aadversarially%20trained%20against%20the%20GCG%20attack.%20We%20also%20show%20how%20to%20jailbreak%20all%0AClaude%20models%20--%20that%20do%20not%20expose%20logprobs%20--%20via%20either%20a%20transfer%20or%0Aprefilling%20attack%20with%20100%5C%25%20success%20rate.%20In%20addition%2C%20we%20show%20how%20to%20use%0Arandom%20search%20on%20a%20restricted%20set%20of%20tokens%20for%20finding%20trojan%20strings%20in%0Apoisoned%20models%20--%20a%20task%20that%20shares%20many%20similarities%20with%20jailbreaking%20--%0Awhich%20is%20the%20algorithm%20that%20brought%20us%20the%20first%20place%20in%20the%20SaTML%2724%20Trojan%0ADetection%20Competition.%20The%20common%20theme%20behind%20these%20attacks%20is%20that%20adaptivity%0Ais%20crucial%3A%20different%20models%20are%20vulnerable%20to%20different%20prompting%20templates%0A%28e.g.%2C%20R2D2%20is%20very%20sensitive%20to%20in-context%20learning%20prompts%29%2C%20some%20models%20have%0Aunique%20vulnerabilities%20based%20on%20their%20APIs%20%28e.g.%2C%20prefilling%20for%20Claude%29%2C%20and%0Ain%20some%20settings%20it%20is%20crucial%20to%20restrict%20the%20token%20search%20space%20based%20on%0Aprior%20knowledge%20%28e.g.%2C%20for%20trojan%20detection%29.%20We%20provide%20the%20code%2C%20prompts%2C%20and%0Alogs%20of%20the%20attacks%20at%20https%3A//github.com/tml-epfl/llm-adaptive-attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02151v1&entry.124074799=Read"},
{"title": "ViTamin: Designing Scalable Vision Models in the Vision-Language Era", "author": "Jienneg Chen and Qihang Yu and Xiaohui Shen and Alan Yuille and Liang-Chieh Chen", "abstract": "  Recent breakthroughs in vision-language models (VLMs) start a new page in the\nvision community. The VLMs provide stronger and more generalizable feature\nembeddings compared to those from ImageNet-pretrained models, thanks to the\ntraining on the large-scale Internet image-text pairs. However, despite the\namazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain\nthe default choice for the image encoder. Although pure transformer proves its\neffectiveness in the text encoding area, it remains questionable whether it is\nalso the case for image encoding, especially considering that various types of\nnetworks are proposed on the ImageNet benchmark, which, unfortunately, are\nrarely studied in VLMs. Due to small data/model scale, the original conclusions\nof model design on ImageNet can be limited and biased. In this paper, we aim at\nbuilding an evaluation protocol of vision models in the vision-language era\nunder the contrastive language-image pretraining (CLIP) framework. We provide a\ncomprehensive way to benchmark different vision models, covering their\nzero-shot performance and scalability in both model and training data sizes. To\nthis end, we introduce ViTamin, a new vision models tailored for VLMs.\nViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy,\nwhen using the same publicly available DataComp-1B dataset and the same\nOpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse\nbenchmarks, including classification, retrieval, open-vocabulary detection and\nsegmentation, and large multi-modal models. When further scaling up the model\nsize, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot\naccuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters\n(4.4B).\n", "link": "http://arxiv.org/abs/2404.02132v1", "date": "2024-04-02", "relevancy": 1.7054, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5915}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.546}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5333}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ViTamin%3A%20Designing%20Scalable%20Vision%20Models%20in%20the%20Vision-Language%20Era&body=Title%3A%20ViTamin%3A%20Designing%20Scalable%20Vision%20Models%20in%20the%20Vision-Language%20Era%0AAuthor%3A%20Jienneg%20Chen%20and%20Qihang%20Yu%20and%20Xiaohui%20Shen%20and%20Alan%20Yuille%20and%20Liang-Chieh%20Chen%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20vision-language%20models%20%28VLMs%29%20start%20a%20new%20page%20in%20the%0Avision%20community.%20The%20VLMs%20provide%20stronger%20and%20more%20generalizable%20feature%0Aembeddings%20compared%20to%20those%20from%20ImageNet-pretrained%20models%2C%20thanks%20to%20the%0Atraining%20on%20the%20large-scale%20Internet%20image-text%20pairs.%20However%2C%20despite%20the%0Aamazing%20achievement%20from%20the%20VLMs%2C%20vanilla%20Vision%20Transformers%20%28ViTs%29%20remain%0Athe%20default%20choice%20for%20the%20image%20encoder.%20Although%20pure%20transformer%20proves%20its%0Aeffectiveness%20in%20the%20text%20encoding%20area%2C%20it%20remains%20questionable%20whether%20it%20is%0Aalso%20the%20case%20for%20image%20encoding%2C%20especially%20considering%20that%20various%20types%20of%0Anetworks%20are%20proposed%20on%20the%20ImageNet%20benchmark%2C%20which%2C%20unfortunately%2C%20are%0Ararely%20studied%20in%20VLMs.%20Due%20to%20small%20data/model%20scale%2C%20the%20original%20conclusions%0Aof%20model%20design%20on%20ImageNet%20can%20be%20limited%20and%20biased.%20In%20this%20paper%2C%20we%20aim%20at%0Abuilding%20an%20evaluation%20protocol%20of%20vision%20models%20in%20the%20vision-language%20era%0Aunder%20the%20contrastive%20language-image%20pretraining%20%28CLIP%29%20framework.%20We%20provide%20a%0Acomprehensive%20way%20to%20benchmark%20different%20vision%20models%2C%20covering%20their%0Azero-shot%20performance%20and%20scalability%20in%20both%20model%20and%20training%20data%20sizes.%20To%0Athis%20end%2C%20we%20introduce%20ViTamin%2C%20a%20new%20vision%20models%20tailored%20for%20VLMs.%0AViTamin-L%20significantly%20outperforms%20ViT-L%20by%202.0%25%20ImageNet%20zero-shot%20accuracy%2C%0Awhen%20using%20the%20same%20publicly%20available%20DataComp-1B%20dataset%20and%20the%20same%0AOpenCLIP%20training%20scheme.%20ViTamin-L%20presents%20promising%20results%20on%2060%20diverse%0Abenchmarks%2C%20including%20classification%2C%20retrieval%2C%20open-vocabulary%20detection%20and%0Asegmentation%2C%20and%20large%20multi-modal%20models.%20When%20further%20scaling%20up%20the%20model%0Asize%2C%20our%20ViTamin-XL%20with%20only%20436M%20parameters%20attains%2082.9%25%20ImageNet%20zero-shot%0Aaccuracy%2C%20surpassing%2082.0%25%20achieved%20by%20EVA-E%20that%20has%20ten%20times%20more%20parameters%0A%284.4B%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02132v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTamin%3A%20Designing%20Scalable%20Vision%20Models%20in%20the%20Vision-Language%20Era&entry.906535625=Jienneg%20Chen%20and%20Qihang%20Yu%20and%20Xiaohui%20Shen%20and%20Alan%20Yuille%20and%20Liang-Chieh%20Chen&entry.1292438233=%20%20Recent%20breakthroughs%20in%20vision-language%20models%20%28VLMs%29%20start%20a%20new%20page%20in%20the%0Avision%20community.%20The%20VLMs%20provide%20stronger%20and%20more%20generalizable%20feature%0Aembeddings%20compared%20to%20those%20from%20ImageNet-pretrained%20models%2C%20thanks%20to%20the%0Atraining%20on%20the%20large-scale%20Internet%20image-text%20pairs.%20However%2C%20despite%20the%0Aamazing%20achievement%20from%20the%20VLMs%2C%20vanilla%20Vision%20Transformers%20%28ViTs%29%20remain%0Athe%20default%20choice%20for%20the%20image%20encoder.%20Although%20pure%20transformer%20proves%20its%0Aeffectiveness%20in%20the%20text%20encoding%20area%2C%20it%20remains%20questionable%20whether%20it%20is%0Aalso%20the%20case%20for%20image%20encoding%2C%20especially%20considering%20that%20various%20types%20of%0Anetworks%20are%20proposed%20on%20the%20ImageNet%20benchmark%2C%20which%2C%20unfortunately%2C%20are%0Ararely%20studied%20in%20VLMs.%20Due%20to%20small%20data/model%20scale%2C%20the%20original%20conclusions%0Aof%20model%20design%20on%20ImageNet%20can%20be%20limited%20and%20biased.%20In%20this%20paper%2C%20we%20aim%20at%0Abuilding%20an%20evaluation%20protocol%20of%20vision%20models%20in%20the%20vision-language%20era%0Aunder%20the%20contrastive%20language-image%20pretraining%20%28CLIP%29%20framework.%20We%20provide%20a%0Acomprehensive%20way%20to%20benchmark%20different%20vision%20models%2C%20covering%20their%0Azero-shot%20performance%20and%20scalability%20in%20both%20model%20and%20training%20data%20sizes.%20To%0Athis%20end%2C%20we%20introduce%20ViTamin%2C%20a%20new%20vision%20models%20tailored%20for%20VLMs.%0AViTamin-L%20significantly%20outperforms%20ViT-L%20by%202.0%25%20ImageNet%20zero-shot%20accuracy%2C%0Awhen%20using%20the%20same%20publicly%20available%20DataComp-1B%20dataset%20and%20the%20same%0AOpenCLIP%20training%20scheme.%20ViTamin-L%20presents%20promising%20results%20on%2060%20diverse%0Abenchmarks%2C%20including%20classification%2C%20retrieval%2C%20open-vocabulary%20detection%20and%0Asegmentation%2C%20and%20large%20multi-modal%20models.%20When%20further%20scaling%20up%20the%20model%0Asize%2C%20our%20ViTamin-XL%20with%20only%20436M%20parameters%20attains%2082.9%25%20ImageNet%20zero-shot%0Aaccuracy%2C%20surpassing%2082.0%25%20achieved%20by%20EVA-E%20that%20has%20ten%20times%20more%20parameters%0A%284.4B%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02132v1&entry.124074799=Read"},
{"title": "Energy-Optimized Planning in Non-Uniform Wind Fields with Fixed-Wing\n  Aerial Vehicles", "author": "Yufei Duan and Florian Achermann and Jaeyoung Lim and Roland Siegwart", "abstract": "  Fixed-wing small uncrewed aerial vehicles (sUAVs) possess the capability to\nremain airborne for extended durations and traverse vast distances. However,\ntheir operation is susceptible to wind conditions, particularly in regions of\ncomplex terrain where high wind speeds may push the aircraft beyond its\noperational limitations, potentially raising safety concerns. Moreover, wind\nimpacts the energy required to follow a path, especially in locations where the\nwind direction and speed are not favorable. Incorporating wind information into\nmission planning is essential to ensure both safety and energy efficiency. In\nthis paper, we propose a sampling-based planner using the kinematic Dubins\naircraft paths with respect to the ground, to plan energy-efficient paths in\nnon-uniform wind fields. We study the planner characteristics with synthetic\nand real-world wind data and compare its performance against baseline cost and\npath formulations. We demonstrate that the energy-optimized planner effectively\nutilizes updrafts to minimize energy consumption, albeit at the expense of\nincreased travel time. The ground-relative path formulation facilitates the\ngeneration of safe trajectories onboard sUAVs within reasonable computational\ntimeframes.\n", "link": "http://arxiv.org/abs/2404.02077v1", "date": "2024-04-02", "relevancy": 1.6906, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4135}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4049}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Energy-Optimized%20Planning%20in%20Non-Uniform%20Wind%20Fields%20with%20Fixed-Wing%0A%20%20Aerial%20Vehicles&body=Title%3A%20Energy-Optimized%20Planning%20in%20Non-Uniform%20Wind%20Fields%20with%20Fixed-Wing%0A%20%20Aerial%20Vehicles%0AAuthor%3A%20Yufei%20Duan%20and%20Florian%20Achermann%20and%20Jaeyoung%20Lim%20and%20Roland%20Siegwart%0AAbstract%3A%20%20%20Fixed-wing%20small%20uncrewed%20aerial%20vehicles%20%28sUAVs%29%20possess%20the%20capability%20to%0Aremain%20airborne%20for%20extended%20durations%20and%20traverse%20vast%20distances.%20However%2C%0Atheir%20operation%20is%20susceptible%20to%20wind%20conditions%2C%20particularly%20in%20regions%20of%0Acomplex%20terrain%20where%20high%20wind%20speeds%20may%20push%20the%20aircraft%20beyond%20its%0Aoperational%20limitations%2C%20potentially%20raising%20safety%20concerns.%20Moreover%2C%20wind%0Aimpacts%20the%20energy%20required%20to%20follow%20a%20path%2C%20especially%20in%20locations%20where%20the%0Awind%20direction%20and%20speed%20are%20not%20favorable.%20Incorporating%20wind%20information%20into%0Amission%20planning%20is%20essential%20to%20ensure%20both%20safety%20and%20energy%20efficiency.%20In%0Athis%20paper%2C%20we%20propose%20a%20sampling-based%20planner%20using%20the%20kinematic%20Dubins%0Aaircraft%20paths%20with%20respect%20to%20the%20ground%2C%20to%20plan%20energy-efficient%20paths%20in%0Anon-uniform%20wind%20fields.%20We%20study%20the%20planner%20characteristics%20with%20synthetic%0Aand%20real-world%20wind%20data%20and%20compare%20its%20performance%20against%20baseline%20cost%20and%0Apath%20formulations.%20We%20demonstrate%20that%20the%20energy-optimized%20planner%20effectively%0Autilizes%20updrafts%20to%20minimize%20energy%20consumption%2C%20albeit%20at%20the%20expense%20of%0Aincreased%20travel%20time.%20The%20ground-relative%20path%20formulation%20facilitates%20the%0Ageneration%20of%20safe%20trajectories%20onboard%20sUAVs%20within%20reasonable%20computational%0Atimeframes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02077v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Optimized%20Planning%20in%20Non-Uniform%20Wind%20Fields%20with%20Fixed-Wing%0A%20%20Aerial%20Vehicles&entry.906535625=Yufei%20Duan%20and%20Florian%20Achermann%20and%20Jaeyoung%20Lim%20and%20Roland%20Siegwart&entry.1292438233=%20%20Fixed-wing%20small%20uncrewed%20aerial%20vehicles%20%28sUAVs%29%20possess%20the%20capability%20to%0Aremain%20airborne%20for%20extended%20durations%20and%20traverse%20vast%20distances.%20However%2C%0Atheir%20operation%20is%20susceptible%20to%20wind%20conditions%2C%20particularly%20in%20regions%20of%0Acomplex%20terrain%20where%20high%20wind%20speeds%20may%20push%20the%20aircraft%20beyond%20its%0Aoperational%20limitations%2C%20potentially%20raising%20safety%20concerns.%20Moreover%2C%20wind%0Aimpacts%20the%20energy%20required%20to%20follow%20a%20path%2C%20especially%20in%20locations%20where%20the%0Awind%20direction%20and%20speed%20are%20not%20favorable.%20Incorporating%20wind%20information%20into%0Amission%20planning%20is%20essential%20to%20ensure%20both%20safety%20and%20energy%20efficiency.%20In%0Athis%20paper%2C%20we%20propose%20a%20sampling-based%20planner%20using%20the%20kinematic%20Dubins%0Aaircraft%20paths%20with%20respect%20to%20the%20ground%2C%20to%20plan%20energy-efficient%20paths%20in%0Anon-uniform%20wind%20fields.%20We%20study%20the%20planner%20characteristics%20with%20synthetic%0Aand%20real-world%20wind%20data%20and%20compare%20its%20performance%20against%20baseline%20cost%20and%0Apath%20formulations.%20We%20demonstrate%20that%20the%20energy-optimized%20planner%20effectively%0Autilizes%20updrafts%20to%20minimize%20energy%20consumption%2C%20albeit%20at%20the%20expense%20of%0Aincreased%20travel%20time.%20The%20ground-relative%20path%20formulation%20facilitates%20the%0Ageneration%20of%20safe%20trajectories%20onboard%20sUAVs%20within%20reasonable%20computational%0Atimeframes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02077v1&entry.124074799=Read"},
{"title": "VC dimension of Graph Neural Networks with Pfaffian activation functions", "author": "Giuseppe Alessio D'Inverno and Monica Bianchini and Franco Scarselli", "abstract": "  Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool\nto learn tasks across a wide range of graph domains in a data-driven fashion;\nbased on a message passing mechanism, GNNs have gained increasing popularity\ndue to their intuitive formulation, closely linked with the Weisfeiler-Lehman\n(WL) test for graph isomorphism, to which they have proven equivalent. From a\ntheoretical point of view, GNNs have been shown to be universal approximators,\nand their generalization capability (namely, bounds on the Vapnik Chervonekis\n(VC) dimension) has recently been investigated for GNNs with piecewise\npolynomial activation functions. The aim of our work is to extend this analysis\non the VC dimension of GNNs to other commonly used activation functions, such\nas sigmoid and hyperbolic tangent, using the framework of Pfaffian function\ntheory. Bounds are provided with respect to architecture parameters (depth,\nnumber of neurons, input size) as well as with respect to the number of colors\nresulting from the 1-WL test applied on the graph domain. The theoretical\nanalysis is supported by a preliminary experimental study.\n", "link": "http://arxiv.org/abs/2401.12362v2", "date": "2024-04-02", "relevancy": 1.6868, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4425}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4072}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4067}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VC%20dimension%20of%20Graph%20Neural%20Networks%20with%20Pfaffian%20activation%20functions&body=Title%3A%20VC%20dimension%20of%20Graph%20Neural%20Networks%20with%20Pfaffian%20activation%20functions%0AAuthor%3A%20Giuseppe%20Alessio%20D%27Inverno%20and%20Monica%20Bianchini%20and%20Franco%20Scarselli%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20in%20recent%20years%20as%20a%20powerful%20tool%0Ato%20learn%20tasks%20across%20a%20wide%20range%20of%20graph%20domains%20in%20a%20data-driven%20fashion%3B%0Abased%20on%20a%20message%20passing%20mechanism%2C%20GNNs%20have%20gained%20increasing%20popularity%0Adue%20to%20their%20intuitive%20formulation%2C%20closely%20linked%20with%20the%20Weisfeiler-Lehman%0A%28WL%29%20test%20for%20graph%20isomorphism%2C%20to%20which%20they%20have%20proven%20equivalent.%20From%20a%0Atheoretical%20point%20of%20view%2C%20GNNs%20have%20been%20shown%20to%20be%20universal%20approximators%2C%0Aand%20their%20generalization%20capability%20%28namely%2C%20bounds%20on%20the%20Vapnik%20Chervonekis%0A%28VC%29%20dimension%29%20has%20recently%20been%20investigated%20for%20GNNs%20with%20piecewise%0Apolynomial%20activation%20functions.%20The%20aim%20of%20our%20work%20is%20to%20extend%20this%20analysis%0Aon%20the%20VC%20dimension%20of%20GNNs%20to%20other%20commonly%20used%20activation%20functions%2C%20such%0Aas%20sigmoid%20and%20hyperbolic%20tangent%2C%20using%20the%20framework%20of%20Pfaffian%20function%0Atheory.%20Bounds%20are%20provided%20with%20respect%20to%20architecture%20parameters%20%28depth%2C%0Anumber%20of%20neurons%2C%20input%20size%29%20as%20well%20as%20with%20respect%20to%20the%20number%20of%20colors%0Aresulting%20from%20the%201-WL%20test%20applied%20on%20the%20graph%20domain.%20The%20theoretical%0Aanalysis%20is%20supported%20by%20a%20preliminary%20experimental%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12362v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VC%20dimension%20of%20Graph%20Neural%20Networks%20with%20Pfaffian%20activation%20functions&entry.906535625=Giuseppe%20Alessio%20D%27Inverno%20and%20Monica%20Bianchini%20and%20Franco%20Scarselli&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20in%20recent%20years%20as%20a%20powerful%20tool%0Ato%20learn%20tasks%20across%20a%20wide%20range%20of%20graph%20domains%20in%20a%20data-driven%20fashion%3B%0Abased%20on%20a%20message%20passing%20mechanism%2C%20GNNs%20have%20gained%20increasing%20popularity%0Adue%20to%20their%20intuitive%20formulation%2C%20closely%20linked%20with%20the%20Weisfeiler-Lehman%0A%28WL%29%20test%20for%20graph%20isomorphism%2C%20to%20which%20they%20have%20proven%20equivalent.%20From%20a%0Atheoretical%20point%20of%20view%2C%20GNNs%20have%20been%20shown%20to%20be%20universal%20approximators%2C%0Aand%20their%20generalization%20capability%20%28namely%2C%20bounds%20on%20the%20Vapnik%20Chervonekis%0A%28VC%29%20dimension%29%20has%20recently%20been%20investigated%20for%20GNNs%20with%20piecewise%0Apolynomial%20activation%20functions.%20The%20aim%20of%20our%20work%20is%20to%20extend%20this%20analysis%0Aon%20the%20VC%20dimension%20of%20GNNs%20to%20other%20commonly%20used%20activation%20functions%2C%20such%0Aas%20sigmoid%20and%20hyperbolic%20tangent%2C%20using%20the%20framework%20of%20Pfaffian%20function%0Atheory.%20Bounds%20are%20provided%20with%20respect%20to%20architecture%20parameters%20%28depth%2C%0Anumber%20of%20neurons%2C%20input%20size%29%20as%20well%20as%20with%20respect%20to%20the%20number%20of%20colors%0Aresulting%20from%20the%201-WL%20test%20applied%20on%20the%20graph%20domain.%20The%20theoretical%0Aanalysis%20is%20supported%20by%20a%20preliminary%20experimental%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12362v2&entry.124074799=Read"},
{"title": "Robustly estimating heterogeneity in factorial data using Rashomon\n  Partitions", "author": "Aparajithan Venkateswaran and Anirudh Sankar and Arun G. Chandrasekhar and Tyler H. McCormick", "abstract": "  Many statistical analyses, in both observational data and randomized control\ntrials, ask: how does the outcome of interest vary with combinations of\nobservable covariates? How do various drug combinations affect health outcomes,\nor how does technology adoption depend on incentives and demographics? Our goal\nis to partition this factorial space into ``pools'' of covariate combinations\nwhere the outcome differs across the pools (but not within a pool). Existing\napproaches (i) search for a single ``optimal'' partition under assumptions\nabout the association between covariates or (ii) sample from the entire set of\npossible partitions. Both these approaches ignore the reality that, especially\nwith correlation structure in covariates, many ways to partition the covariate\nspace may be statistically indistinguishable, despite very different\nimplications for policy or science. We develop an alternative perspective,\ncalled Rashomon Partition Sets (RPSs). Each item in the RPS partitions the\nspace of covariates using a tree-like geometry. RPSs incorporate all partitions\nthat have posterior values near the maximum a posteriori partition, even if\nthey offer substantively different explanations, and do so using a prior that\nmakes no assumptions about associations between covariates. This prior is the\n$\\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate\nthe posterior of any measurable function of the feature effects vector on\noutcomes, conditional on being in the RPS. We also characterize approximation\nerror relative to the entire posterior and provide bounds on the size of the\nRPS. Simulations demonstrate this framework allows for robust conclusions\nrelative to conventional regularization techniques. We apply our method to\nthree empirical settings: price effects on charitable giving, chromosomal\nstructure (telomere length), and the introduction of microfinance.\n", "link": "http://arxiv.org/abs/2404.02141v1", "date": "2024-04-02", "relevancy": 1.6756, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4676}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4155}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4029}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robustly%20estimating%20heterogeneity%20in%20factorial%20data%20using%20Rashomon%0A%20%20Partitions&body=Title%3A%20Robustly%20estimating%20heterogeneity%20in%20factorial%20data%20using%20Rashomon%0A%20%20Partitions%0AAuthor%3A%20Aparajithan%20Venkateswaran%20and%20Anirudh%20Sankar%20and%20Arun%20G.%20Chandrasekhar%20and%20Tyler%20H.%20McCormick%0AAbstract%3A%20%20%20Many%20statistical%20analyses%2C%20in%20both%20observational%20data%20and%20randomized%20control%0Atrials%2C%20ask%3A%20how%20does%20the%20outcome%20of%20interest%20vary%20with%20combinations%20of%0Aobservable%20covariates%3F%20How%20do%20various%20drug%20combinations%20affect%20health%20outcomes%2C%0Aor%20how%20does%20technology%20adoption%20depend%20on%20incentives%20and%20demographics%3F%20Our%20goal%0Ais%20to%20partition%20this%20factorial%20space%20into%20%60%60pools%27%27%20of%20covariate%20combinations%0Awhere%20the%20outcome%20differs%20across%20the%20pools%20%28but%20not%20within%20a%20pool%29.%20Existing%0Aapproaches%20%28i%29%20search%20for%20a%20single%20%60%60optimal%27%27%20partition%20under%20assumptions%0Aabout%20the%20association%20between%20covariates%20or%20%28ii%29%20sample%20from%20the%20entire%20set%20of%0Apossible%20partitions.%20Both%20these%20approaches%20ignore%20the%20reality%20that%2C%20especially%0Awith%20correlation%20structure%20in%20covariates%2C%20many%20ways%20to%20partition%20the%20covariate%0Aspace%20may%20be%20statistically%20indistinguishable%2C%20despite%20very%20different%0Aimplications%20for%20policy%20or%20science.%20We%20develop%20an%20alternative%20perspective%2C%0Acalled%20Rashomon%20Partition%20Sets%20%28RPSs%29.%20Each%20item%20in%20the%20RPS%20partitions%20the%0Aspace%20of%20covariates%20using%20a%20tree-like%20geometry.%20RPSs%20incorporate%20all%20partitions%0Athat%20have%20posterior%20values%20near%20the%20maximum%20a%20posteriori%20partition%2C%20even%20if%0Athey%20offer%20substantively%20different%20explanations%2C%20and%20do%20so%20using%20a%20prior%20that%0Amakes%20no%20assumptions%20about%20associations%20between%20covariates.%20This%20prior%20is%20the%0A%24%5Cell_0%24%20prior%2C%20which%20we%20show%20is%20minimax%20optimal.%20Given%20the%20RPS%20we%20calculate%0Athe%20posterior%20of%20any%20measurable%20function%20of%20the%20feature%20effects%20vector%20on%0Aoutcomes%2C%20conditional%20on%20being%20in%20the%20RPS.%20We%20also%20characterize%20approximation%0Aerror%20relative%20to%20the%20entire%20posterior%20and%20provide%20bounds%20on%20the%20size%20of%20the%0ARPS.%20Simulations%20demonstrate%20this%20framework%20allows%20for%20robust%20conclusions%0Arelative%20to%20conventional%20regularization%20techniques.%20We%20apply%20our%20method%20to%0Athree%20empirical%20settings%3A%20price%20effects%20on%20charitable%20giving%2C%20chromosomal%0Astructure%20%28telomere%20length%29%2C%20and%20the%20introduction%20of%20microfinance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02141v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustly%20estimating%20heterogeneity%20in%20factorial%20data%20using%20Rashomon%0A%20%20Partitions&entry.906535625=Aparajithan%20Venkateswaran%20and%20Anirudh%20Sankar%20and%20Arun%20G.%20Chandrasekhar%20and%20Tyler%20H.%20McCormick&entry.1292438233=%20%20Many%20statistical%20analyses%2C%20in%20both%20observational%20data%20and%20randomized%20control%0Atrials%2C%20ask%3A%20how%20does%20the%20outcome%20of%20interest%20vary%20with%20combinations%20of%0Aobservable%20covariates%3F%20How%20do%20various%20drug%20combinations%20affect%20health%20outcomes%2C%0Aor%20how%20does%20technology%20adoption%20depend%20on%20incentives%20and%20demographics%3F%20Our%20goal%0Ais%20to%20partition%20this%20factorial%20space%20into%20%60%60pools%27%27%20of%20covariate%20combinations%0Awhere%20the%20outcome%20differs%20across%20the%20pools%20%28but%20not%20within%20a%20pool%29.%20Existing%0Aapproaches%20%28i%29%20search%20for%20a%20single%20%60%60optimal%27%27%20partition%20under%20assumptions%0Aabout%20the%20association%20between%20covariates%20or%20%28ii%29%20sample%20from%20the%20entire%20set%20of%0Apossible%20partitions.%20Both%20these%20approaches%20ignore%20the%20reality%20that%2C%20especially%0Awith%20correlation%20structure%20in%20covariates%2C%20many%20ways%20to%20partition%20the%20covariate%0Aspace%20may%20be%20statistically%20indistinguishable%2C%20despite%20very%20different%0Aimplications%20for%20policy%20or%20science.%20We%20develop%20an%20alternative%20perspective%2C%0Acalled%20Rashomon%20Partition%20Sets%20%28RPSs%29.%20Each%20item%20in%20the%20RPS%20partitions%20the%0Aspace%20of%20covariates%20using%20a%20tree-like%20geometry.%20RPSs%20incorporate%20all%20partitions%0Athat%20have%20posterior%20values%20near%20the%20maximum%20a%20posteriori%20partition%2C%20even%20if%0Athey%20offer%20substantively%20different%20explanations%2C%20and%20do%20so%20using%20a%20prior%20that%0Amakes%20no%20assumptions%20about%20associations%20between%20covariates.%20This%20prior%20is%20the%0A%24%5Cell_0%24%20prior%2C%20which%20we%20show%20is%20minimax%20optimal.%20Given%20the%20RPS%20we%20calculate%0Athe%20posterior%20of%20any%20measurable%20function%20of%20the%20feature%20effects%20vector%20on%0Aoutcomes%2C%20conditional%20on%20being%20in%20the%20RPS.%20We%20also%20characterize%20approximation%0Aerror%20relative%20to%20the%20entire%20posterior%20and%20provide%20bounds%20on%20the%20size%20of%20the%0ARPS.%20Simulations%20demonstrate%20this%20framework%20allows%20for%20robust%20conclusions%0Arelative%20to%20conventional%20regularization%20techniques.%20We%20apply%20our%20method%20to%0Athree%20empirical%20settings%3A%20price%20effects%20on%20charitable%20giving%2C%20chromosomal%0Astructure%20%28telomere%20length%29%2C%20and%20the%20introduction%20of%20microfinance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02141v1&entry.124074799=Read"},
{"title": "A discussion about violin reduction: geometric analysis of contour lines\n  and channel of minima", "author": "Phil\u00e9mon Beghin and Anne-Emmanuelle Ceulemans and Fran\u00e7ois Glineur", "abstract": "  Some early violins have been reduced during their history to fit imposed\nmorphological standards, while more recent ones have been built directly to\nthese standards. We can observe differences between reduced and unreduced\ninstruments, particularly in their contour lines and channel of minima. In a\nrecent preliminary work, we computed and highlighted those two features for two\ninstruments using triangular 3D meshes acquired by photogrammetry, whose\nfidelity has been assessed and validated with sub-millimetre accuracy. We\npropose here an extension to a corpus of 38 violins, violas and cellos, and\nintroduce improved procedures, leading to a stronger discussion of the\ngeometric analysis. We first recall the material we are working with. We then\ndiscuss how to derive the best reference plane for the violin alignment, which\nis crucial for the computation of contour lines and channel of minima. Finally,\nwe show how to compute efficiently both characteristics and we illustrate our\nresults with a few examples.\n", "link": "http://arxiv.org/abs/2404.01995v1", "date": "2024-04-02", "relevancy": 1.6635, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4478}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4214}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3817}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20discussion%20about%20violin%20reduction%3A%20geometric%20analysis%20of%20contour%20lines%0A%20%20and%20channel%20of%20minima&body=Title%3A%20A%20discussion%20about%20violin%20reduction%3A%20geometric%20analysis%20of%20contour%20lines%0A%20%20and%20channel%20of%20minima%0AAuthor%3A%20Phil%C3%A9mon%20Beghin%20and%20Anne-Emmanuelle%20Ceulemans%20and%20Fran%C3%A7ois%20Glineur%0AAbstract%3A%20%20%20Some%20early%20violins%20have%20been%20reduced%20during%20their%20history%20to%20fit%20imposed%0Amorphological%20standards%2C%20while%20more%20recent%20ones%20have%20been%20built%20directly%20to%0Athese%20standards.%20We%20can%20observe%20differences%20between%20reduced%20and%20unreduced%0Ainstruments%2C%20particularly%20in%20their%20contour%20lines%20and%20channel%20of%20minima.%20In%20a%0Arecent%20preliminary%20work%2C%20we%20computed%20and%20highlighted%20those%20two%20features%20for%20two%0Ainstruments%20using%20triangular%203D%20meshes%20acquired%20by%20photogrammetry%2C%20whose%0Afidelity%20has%20been%20assessed%20and%20validated%20with%20sub-millimetre%20accuracy.%20We%0Apropose%20here%20an%20extension%20to%20a%20corpus%20of%2038%20violins%2C%20violas%20and%20cellos%2C%20and%0Aintroduce%20improved%20procedures%2C%20leading%20to%20a%20stronger%20discussion%20of%20the%0Ageometric%20analysis.%20We%20first%20recall%20the%20material%20we%20are%20working%20with.%20We%20then%0Adiscuss%20how%20to%20derive%20the%20best%20reference%20plane%20for%20the%20violin%20alignment%2C%20which%0Ais%20crucial%20for%20the%20computation%20of%20contour%20lines%20and%20channel%20of%20minima.%20Finally%2C%0Awe%20show%20how%20to%20compute%20efficiently%20both%20characteristics%20and%20we%20illustrate%20our%0Aresults%20with%20a%20few%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01995v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20discussion%20about%20violin%20reduction%3A%20geometric%20analysis%20of%20contour%20lines%0A%20%20and%20channel%20of%20minima&entry.906535625=Phil%C3%A9mon%20Beghin%20and%20Anne-Emmanuelle%20Ceulemans%20and%20Fran%C3%A7ois%20Glineur&entry.1292438233=%20%20Some%20early%20violins%20have%20been%20reduced%20during%20their%20history%20to%20fit%20imposed%0Amorphological%20standards%2C%20while%20more%20recent%20ones%20have%20been%20built%20directly%20to%0Athese%20standards.%20We%20can%20observe%20differences%20between%20reduced%20and%20unreduced%0Ainstruments%2C%20particularly%20in%20their%20contour%20lines%20and%20channel%20of%20minima.%20In%20a%0Arecent%20preliminary%20work%2C%20we%20computed%20and%20highlighted%20those%20two%20features%20for%20two%0Ainstruments%20using%20triangular%203D%20meshes%20acquired%20by%20photogrammetry%2C%20whose%0Afidelity%20has%20been%20assessed%20and%20validated%20with%20sub-millimetre%20accuracy.%20We%0Apropose%20here%20an%20extension%20to%20a%20corpus%20of%2038%20violins%2C%20violas%20and%20cellos%2C%20and%0Aintroduce%20improved%20procedures%2C%20leading%20to%20a%20stronger%20discussion%20of%20the%0Ageometric%20analysis.%20We%20first%20recall%20the%20material%20we%20are%20working%20with.%20We%20then%0Adiscuss%20how%20to%20derive%20the%20best%20reference%20plane%20for%20the%20violin%20alignment%2C%20which%0Ais%20crucial%20for%20the%20computation%20of%20contour%20lines%20and%20channel%20of%20minima.%20Finally%2C%0Awe%20show%20how%20to%20compute%20efficiently%20both%20characteristics%20and%20we%20illustrate%20our%0Aresults%20with%20a%20few%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01995v1&entry.124074799=Read"},
{"title": "WcDT: World-centric Diffusion Transformer for Traffic Scene Generation", "author": "Chen Yang and Aaron Xuxiang Tian and Dong Chen and Tianyu Shi and Arsalan Heydarian", "abstract": "  In this paper, we introduce a novel approach for autonomous driving\ntrajectory generation by harnessing the complementary strengths of diffusion\nprobabilistic models (a.k.a., diffusion models) and transformers. Our proposed\nframework, termed the \"World-Centric Diffusion Transformer\" (WcDT), optimizes\nthe entire trajectory generation process, from feature extraction to model\ninference. To enhance the scene diversity and stochasticity, the historical\ntrajectory data is first preprocessed and encoded into latent space using\nDenoising Diffusion Probabilistic Models (DDPM) enhanced with Diffusion with\nTransformer (DiT) blocks. Then, the latent features, historical trajectories,\nHD map features, and historical traffic signal information are fused with\nvarious transformer-based encoders. The encoded traffic scenes are then decoded\nby a trajectory decoder to generate multimodal future trajectories.\nComprehensive experimental results show that the proposed approach exhibits\nsuperior performance in generating both realistic and diverse trajectories,\nshowing its potential for integration into automatic driving simulation\nsystems.\n", "link": "http://arxiv.org/abs/2404.02082v1", "date": "2024-04-02", "relevancy": 1.6611, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.603}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5677}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5284}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WcDT%3A%20World-centric%20Diffusion%20Transformer%20for%20Traffic%20Scene%20Generation&body=Title%3A%20WcDT%3A%20World-centric%20Diffusion%20Transformer%20for%20Traffic%20Scene%20Generation%0AAuthor%3A%20Chen%20Yang%20and%20Aaron%20Xuxiang%20Tian%20and%20Dong%20Chen%20and%20Tianyu%20Shi%20and%20Arsalan%20Heydarian%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20for%20autonomous%20driving%0Atrajectory%20generation%20by%20harnessing%20the%20complementary%20strengths%20of%20diffusion%0Aprobabilistic%20models%20%28a.k.a.%2C%20diffusion%20models%29%20and%20transformers.%20Our%20proposed%0Aframework%2C%20termed%20the%20%22World-Centric%20Diffusion%20Transformer%22%20%28WcDT%29%2C%20optimizes%0Athe%20entire%20trajectory%20generation%20process%2C%20from%20feature%20extraction%20to%20model%0Ainference.%20To%20enhance%20the%20scene%20diversity%20and%20stochasticity%2C%20the%20historical%0Atrajectory%20data%20is%20first%20preprocessed%20and%20encoded%20into%20latent%20space%20using%0ADenoising%20Diffusion%20Probabilistic%20Models%20%28DDPM%29%20enhanced%20with%20Diffusion%20with%0ATransformer%20%28DiT%29%20blocks.%20Then%2C%20the%20latent%20features%2C%20historical%20trajectories%2C%0AHD%20map%20features%2C%20and%20historical%20traffic%20signal%20information%20are%20fused%20with%0Avarious%20transformer-based%20encoders.%20The%20encoded%20traffic%20scenes%20are%20then%20decoded%0Aby%20a%20trajectory%20decoder%20to%20generate%20multimodal%20future%20trajectories.%0AComprehensive%20experimental%20results%20show%20that%20the%20proposed%20approach%20exhibits%0Asuperior%20performance%20in%20generating%20both%20realistic%20and%20diverse%20trajectories%2C%0Ashowing%20its%20potential%20for%20integration%20into%20automatic%20driving%20simulation%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02082v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WcDT%3A%20World-centric%20Diffusion%20Transformer%20for%20Traffic%20Scene%20Generation&entry.906535625=Chen%20Yang%20and%20Aaron%20Xuxiang%20Tian%20and%20Dong%20Chen%20and%20Tianyu%20Shi%20and%20Arsalan%20Heydarian&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20for%20autonomous%20driving%0Atrajectory%20generation%20by%20harnessing%20the%20complementary%20strengths%20of%20diffusion%0Aprobabilistic%20models%20%28a.k.a.%2C%20diffusion%20models%29%20and%20transformers.%20Our%20proposed%0Aframework%2C%20termed%20the%20%22World-Centric%20Diffusion%20Transformer%22%20%28WcDT%29%2C%20optimizes%0Athe%20entire%20trajectory%20generation%20process%2C%20from%20feature%20extraction%20to%20model%0Ainference.%20To%20enhance%20the%20scene%20diversity%20and%20stochasticity%2C%20the%20historical%0Atrajectory%20data%20is%20first%20preprocessed%20and%20encoded%20into%20latent%20space%20using%0ADenoising%20Diffusion%20Probabilistic%20Models%20%28DDPM%29%20enhanced%20with%20Diffusion%20with%0ATransformer%20%28DiT%29%20blocks.%20Then%2C%20the%20latent%20features%2C%20historical%20trajectories%2C%0AHD%20map%20features%2C%20and%20historical%20traffic%20signal%20information%20are%20fused%20with%0Avarious%20transformer-based%20encoders.%20The%20encoded%20traffic%20scenes%20are%20then%20decoded%0Aby%20a%20trajectory%20decoder%20to%20generate%20multimodal%20future%20trajectories.%0AComprehensive%20experimental%20results%20show%20that%20the%20proposed%20approach%20exhibits%0Asuperior%20performance%20in%20generating%20both%20realistic%20and%20diverse%20trajectories%2C%0Ashowing%20its%20potential%20for%20integration%20into%20automatic%20driving%20simulation%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02082v1&entry.124074799=Read"},
{"title": "Predicting the Intention to Interact with a Service Robot:the Role of\n  Gaze Cues", "author": "Simone Arreghini and Gabriele Abbate and Alessandro Giusti and Antonio Paolillo", "abstract": "  For a service robot, it is crucial to perceive as early as possible that an\napproaching person intends to interact: in this case, it can proactively enact\nfriendly behaviors that lead to an improved user experience. We solve this\nperception task with a sequence-to-sequence classifier of a potential user\nintention to interact, which can be trained in a self-supervised way. Our main\ncontribution is a study of the benefit of features representing the person's\ngaze in this context. Extensive experiments on a novel dataset show that the\ninclusion of gaze cues significantly improves the classifier performance (AUROC\nincreases from 84.5% to 91.2%); the distance at which an accurate\nclassification can be achieved improves from 2.4 m to 3.2 m. We also quantify\nthe system's ability to adapt to new environments without external supervision.\nQualitative experiments show practical applications with a waiter robot.\n", "link": "http://arxiv.org/abs/2404.01986v1", "date": "2024-04-02", "relevancy": 1.6576, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5632}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5581}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5461}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Predicting%20the%20Intention%20to%20Interact%20with%20a%20Service%20Robot%3Athe%20Role%20of%0A%20%20Gaze%20Cues&body=Title%3A%20Predicting%20the%20Intention%20to%20Interact%20with%20a%20Service%20Robot%3Athe%20Role%20of%0A%20%20Gaze%20Cues%0AAuthor%3A%20Simone%20Arreghini%20and%20Gabriele%20Abbate%20and%20Alessandro%20Giusti%20and%20Antonio%20Paolillo%0AAbstract%3A%20%20%20For%20a%20service%20robot%2C%20it%20is%20crucial%20to%20perceive%20as%20early%20as%20possible%20that%20an%0Aapproaching%20person%20intends%20to%20interact%3A%20in%20this%20case%2C%20it%20can%20proactively%20enact%0Afriendly%20behaviors%20that%20lead%20to%20an%20improved%20user%20experience.%20We%20solve%20this%0Aperception%20task%20with%20a%20sequence-to-sequence%20classifier%20of%20a%20potential%20user%0Aintention%20to%20interact%2C%20which%20can%20be%20trained%20in%20a%20self-supervised%20way.%20Our%20main%0Acontribution%20is%20a%20study%20of%20the%20benefit%20of%20features%20representing%20the%20person%27s%0Agaze%20in%20this%20context.%20Extensive%20experiments%20on%20a%20novel%20dataset%20show%20that%20the%0Ainclusion%20of%20gaze%20cues%20significantly%20improves%20the%20classifier%20performance%20%28AUROC%0Aincreases%20from%2084.5%25%20to%2091.2%25%29%3B%20the%20distance%20at%20which%20an%20accurate%0Aclassification%20can%20be%20achieved%20improves%20from%202.4%20m%20to%203.2%20m.%20We%20also%20quantify%0Athe%20system%27s%20ability%20to%20adapt%20to%20new%20environments%20without%20external%20supervision.%0AQualitative%20experiments%20show%20practical%20applications%20with%20a%20waiter%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01986v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20the%20Intention%20to%20Interact%20with%20a%20Service%20Robot%3Athe%20Role%20of%0A%20%20Gaze%20Cues&entry.906535625=Simone%20Arreghini%20and%20Gabriele%20Abbate%20and%20Alessandro%20Giusti%20and%20Antonio%20Paolillo&entry.1292438233=%20%20For%20a%20service%20robot%2C%20it%20is%20crucial%20to%20perceive%20as%20early%20as%20possible%20that%20an%0Aapproaching%20person%20intends%20to%20interact%3A%20in%20this%20case%2C%20it%20can%20proactively%20enact%0Afriendly%20behaviors%20that%20lead%20to%20an%20improved%20user%20experience.%20We%20solve%20this%0Aperception%20task%20with%20a%20sequence-to-sequence%20classifier%20of%20a%20potential%20user%0Aintention%20to%20interact%2C%20which%20can%20be%20trained%20in%20a%20self-supervised%20way.%20Our%20main%0Acontribution%20is%20a%20study%20of%20the%20benefit%20of%20features%20representing%20the%20person%27s%0Agaze%20in%20this%20context.%20Extensive%20experiments%20on%20a%20novel%20dataset%20show%20that%20the%0Ainclusion%20of%20gaze%20cues%20significantly%20improves%20the%20classifier%20performance%20%28AUROC%0Aincreases%20from%2084.5%25%20to%2091.2%25%29%3B%20the%20distance%20at%20which%20an%20accurate%0Aclassification%20can%20be%20achieved%20improves%20from%202.4%20m%20to%203.2%20m.%20We%20also%20quantify%0Athe%20system%27s%20ability%20to%20adapt%20to%20new%20environments%20without%20external%20supervision.%0AQualitative%20experiments%20show%20practical%20applications%20with%20a%20waiter%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01986v1&entry.124074799=Read"},
{"title": "Cross-modality debiasing: using language to mitigate sub-population\n  shifts in imaging", "author": "Yijiang Pang and Bao Hoang and Jiayu Zhou", "abstract": "  Sub-population shift is a specific type of domain shift that highlights\nchanges in data distribution within specific sub-groups or populations between\ntraining and testing. Sub-population shift accounts for a significant source of\nalgorithmic bias and calls for distributional robustness. Recent studies found\ninherent distributional robustness in multi-modality foundation models, such as\nthe vision-language model CLIP, yet this robustness is vulnerable through\nparameter fine-tuning. In this paper, we propose leveraging the connection of\nrobustness among different modalities and reshaping the distributional\nrobustness of one modality with another. Specifically, in the context of the\ndistributional robustness of CLIP, we propose to leverage natural language\ninputs to debias the image feature representations, to improve worst-case\nperformance on sub-populations. Our extensive empirical studies show that image\nrepresentations debiased by natural language can achieve significant\nperformance improvement and reduction of performance instability under\nsub-population shifts.\n", "link": "http://arxiv.org/abs/2403.07888v2", "date": "2024-04-02", "relevancy": 1.6559, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5634}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5473}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5281}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-modality%20debiasing%3A%20using%20language%20to%20mitigate%20sub-population%0A%20%20shifts%20in%20imaging&body=Title%3A%20Cross-modality%20debiasing%3A%20using%20language%20to%20mitigate%20sub-population%0A%20%20shifts%20in%20imaging%0AAuthor%3A%20Yijiang%20Pang%20and%20Bao%20Hoang%20and%20Jiayu%20Zhou%0AAbstract%3A%20%20%20Sub-population%20shift%20is%20a%20specific%20type%20of%20domain%20shift%20that%20highlights%0Achanges%20in%20data%20distribution%20within%20specific%20sub-groups%20or%20populations%20between%0Atraining%20and%20testing.%20Sub-population%20shift%20accounts%20for%20a%20significant%20source%20of%0Aalgorithmic%20bias%20and%20calls%20for%20distributional%20robustness.%20Recent%20studies%20found%0Ainherent%20distributional%20robustness%20in%20multi-modality%20foundation%20models%2C%20such%20as%0Athe%20vision-language%20model%20CLIP%2C%20yet%20this%20robustness%20is%20vulnerable%20through%0Aparameter%20fine-tuning.%20In%20this%20paper%2C%20we%20propose%20leveraging%20the%20connection%20of%0Arobustness%20among%20different%20modalities%20and%20reshaping%20the%20distributional%0Arobustness%20of%20one%20modality%20with%20another.%20Specifically%2C%20in%20the%20context%20of%20the%0Adistributional%20robustness%20of%20CLIP%2C%20we%20propose%20to%20leverage%20natural%20language%0Ainputs%20to%20debias%20the%20image%20feature%20representations%2C%20to%20improve%20worst-case%0Aperformance%20on%20sub-populations.%20Our%20extensive%20empirical%20studies%20show%20that%20image%0Arepresentations%20debiased%20by%20natural%20language%20can%20achieve%20significant%0Aperformance%20improvement%20and%20reduction%20of%20performance%20instability%20under%0Asub-population%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07888v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-modality%20debiasing%3A%20using%20language%20to%20mitigate%20sub-population%0A%20%20shifts%20in%20imaging&entry.906535625=Yijiang%20Pang%20and%20Bao%20Hoang%20and%20Jiayu%20Zhou&entry.1292438233=%20%20Sub-population%20shift%20is%20a%20specific%20type%20of%20domain%20shift%20that%20highlights%0Achanges%20in%20data%20distribution%20within%20specific%20sub-groups%20or%20populations%20between%0Atraining%20and%20testing.%20Sub-population%20shift%20accounts%20for%20a%20significant%20source%20of%0Aalgorithmic%20bias%20and%20calls%20for%20distributional%20robustness.%20Recent%20studies%20found%0Ainherent%20distributional%20robustness%20in%20multi-modality%20foundation%20models%2C%20such%20as%0Athe%20vision-language%20model%20CLIP%2C%20yet%20this%20robustness%20is%20vulnerable%20through%0Aparameter%20fine-tuning.%20In%20this%20paper%2C%20we%20propose%20leveraging%20the%20connection%20of%0Arobustness%20among%20different%20modalities%20and%20reshaping%20the%20distributional%0Arobustness%20of%20one%20modality%20with%20another.%20Specifically%2C%20in%20the%20context%20of%20the%0Adistributional%20robustness%20of%20CLIP%2C%20we%20propose%20to%20leverage%20natural%20language%0Ainputs%20to%20debias%20the%20image%20feature%20representations%2C%20to%20improve%20worst-case%0Aperformance%20on%20sub-populations.%20Our%20extensive%20empirical%20studies%20show%20that%20image%0Arepresentations%20debiased%20by%20natural%20language%20can%20achieve%20significant%0Aperformance%20improvement%20and%20reduction%20of%20performance%20instability%20under%0Asub-population%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07888v2&entry.124074799=Read"},
{"title": "Deployable Reinforcement Learning with Variable Control Rate", "author": "Dong Wang and Giovanni Beltrame", "abstract": "  Deploying controllers trained with Reinforcement Learning (RL) on real robots\ncan be challenging: RL relies on agents' policies being modeled as Markov\nDecision Processes (MDPs), which assume an inherently discrete passage of time.\nThe use of MDPs results in that nearly all RL-based control systems employ a\nfixed-rate control strategy with a period (or time step) typically chosen based\non the developer's experience or specific characteristics of the application\nenvironment. Unfortunately, the system should be controlled at the highest,\nworst-case frequency to ensure stability, which can demand significant\ncomputational and energy resources and hinder the deployability of the\ncontroller on onboard hardware. Adhering to the principles of reactive\nprogramming, we surmise that applying control actions only when necessary\nenables the use of simpler hardware and helps reduce energy consumption. We\nchallenge the fixed frequency assumption by proposing a variant of RL with\nvariable control rate. In this approach, the policy decides the action the\nagent should take as well as the duration of the time step associated with that\naction. In our new setting, we expand Soft Actor-Critic (SAC) to compute the\noptimal policy with a variable control rate, introducing the Soft Elastic\nActor-Critic (SEAC) algorithm. We show the efficacy of SEAC through a\nproof-of-concept simulation driving an agent with Newtonian kinematics. Our\nexperiments show higher average returns, shorter task completion times, and\nreduced computational resources when compared to fixed rate policies.\n", "link": "http://arxiv.org/abs/2401.09286v2", "date": "2024-04-02", "relevancy": 1.6512, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5587}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5446}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5353}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deployable%20Reinforcement%20Learning%20with%20Variable%20Control%20Rate&body=Title%3A%20Deployable%20Reinforcement%20Learning%20with%20Variable%20Control%20Rate%0AAuthor%3A%20Dong%20Wang%20and%20Giovanni%20Beltrame%0AAbstract%3A%20%20%20Deploying%20controllers%20trained%20with%20Reinforcement%20Learning%20%28RL%29%20on%20real%20robots%0Acan%20be%20challenging%3A%20RL%20relies%20on%20agents%27%20policies%20being%20modeled%20as%20Markov%0ADecision%20Processes%20%28MDPs%29%2C%20which%20assume%20an%20inherently%20discrete%20passage%20of%20time.%0AThe%20use%20of%20MDPs%20results%20in%20that%20nearly%20all%20RL-based%20control%20systems%20employ%20a%0Afixed-rate%20control%20strategy%20with%20a%20period%20%28or%20time%20step%29%20typically%20chosen%20based%0Aon%20the%20developer%27s%20experience%20or%20specific%20characteristics%20of%20the%20application%0Aenvironment.%20Unfortunately%2C%20the%20system%20should%20be%20controlled%20at%20the%20highest%2C%0Aworst-case%20frequency%20to%20ensure%20stability%2C%20which%20can%20demand%20significant%0Acomputational%20and%20energy%20resources%20and%20hinder%20the%20deployability%20of%20the%0Acontroller%20on%20onboard%20hardware.%20Adhering%20to%20the%20principles%20of%20reactive%0Aprogramming%2C%20we%20surmise%20that%20applying%20control%20actions%20only%20when%20necessary%0Aenables%20the%20use%20of%20simpler%20hardware%20and%20helps%20reduce%20energy%20consumption.%20We%0Achallenge%20the%20fixed%20frequency%20assumption%20by%20proposing%20a%20variant%20of%20RL%20with%0Avariable%20control%20rate.%20In%20this%20approach%2C%20the%20policy%20decides%20the%20action%20the%0Aagent%20should%20take%20as%20well%20as%20the%20duration%20of%20the%20time%20step%20associated%20with%20that%0Aaction.%20In%20our%20new%20setting%2C%20we%20expand%20Soft%20Actor-Critic%20%28SAC%29%20to%20compute%20the%0Aoptimal%20policy%20with%20a%20variable%20control%20rate%2C%20introducing%20the%20Soft%20Elastic%0AActor-Critic%20%28SEAC%29%20algorithm.%20We%20show%20the%20efficacy%20of%20SEAC%20through%20a%0Aproof-of-concept%20simulation%20driving%20an%20agent%20with%20Newtonian%20kinematics.%20Our%0Aexperiments%20show%20higher%20average%20returns%2C%20shorter%20task%20completion%20times%2C%20and%0Areduced%20computational%20resources%20when%20compared%20to%20fixed%20rate%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09286v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deployable%20Reinforcement%20Learning%20with%20Variable%20Control%20Rate&entry.906535625=Dong%20Wang%20and%20Giovanni%20Beltrame&entry.1292438233=%20%20Deploying%20controllers%20trained%20with%20Reinforcement%20Learning%20%28RL%29%20on%20real%20robots%0Acan%20be%20challenging%3A%20RL%20relies%20on%20agents%27%20policies%20being%20modeled%20as%20Markov%0ADecision%20Processes%20%28MDPs%29%2C%20which%20assume%20an%20inherently%20discrete%20passage%20of%20time.%0AThe%20use%20of%20MDPs%20results%20in%20that%20nearly%20all%20RL-based%20control%20systems%20employ%20a%0Afixed-rate%20control%20strategy%20with%20a%20period%20%28or%20time%20step%29%20typically%20chosen%20based%0Aon%20the%20developer%27s%20experience%20or%20specific%20characteristics%20of%20the%20application%0Aenvironment.%20Unfortunately%2C%20the%20system%20should%20be%20controlled%20at%20the%20highest%2C%0Aworst-case%20frequency%20to%20ensure%20stability%2C%20which%20can%20demand%20significant%0Acomputational%20and%20energy%20resources%20and%20hinder%20the%20deployability%20of%20the%0Acontroller%20on%20onboard%20hardware.%20Adhering%20to%20the%20principles%20of%20reactive%0Aprogramming%2C%20we%20surmise%20that%20applying%20control%20actions%20only%20when%20necessary%0Aenables%20the%20use%20of%20simpler%20hardware%20and%20helps%20reduce%20energy%20consumption.%20We%0Achallenge%20the%20fixed%20frequency%20assumption%20by%20proposing%20a%20variant%20of%20RL%20with%0Avariable%20control%20rate.%20In%20this%20approach%2C%20the%20policy%20decides%20the%20action%20the%0Aagent%20should%20take%20as%20well%20as%20the%20duration%20of%20the%20time%20step%20associated%20with%20that%0Aaction.%20In%20our%20new%20setting%2C%20we%20expand%20Soft%20Actor-Critic%20%28SAC%29%20to%20compute%20the%0Aoptimal%20policy%20with%20a%20variable%20control%20rate%2C%20introducing%20the%20Soft%20Elastic%0AActor-Critic%20%28SEAC%29%20algorithm.%20We%20show%20the%20efficacy%20of%20SEAC%20through%20a%0Aproof-of-concept%20simulation%20driving%20an%20agent%20with%20Newtonian%20kinematics.%20Our%0Aexperiments%20show%20higher%20average%20returns%2C%20shorter%20task%20completion%20times%2C%20and%0Areduced%20computational%20resources%20when%20compared%20to%20fixed%20rate%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09286v2&entry.124074799=Read"},
{"title": "Semantically-Prompted Language Models Improve Visual Descriptions", "author": "Michael Ogezi and Bradley Hauer and Grzegorz Kondrak", "abstract": "  Language-vision models like CLIP have made significant strides in vision\ntasks, such as zero-shot image classification (ZSIC). However, generating\nspecific and expressive visual descriptions remains challenging; descriptions\nproduced by current methods are often ambiguous and lacking in granularity. To\ntackle these issues, we propose V-GLOSS: Visual Glosses, a novel method built\nupon two key ideas. The first is Semantic Prompting, which conditions a\nlanguage model on structured semantic knowledge. The second is a new\ncontrastive algorithm that elicits fine-grained distinctions between similar\nconcepts. With both ideas, we demonstrate that V-GLOSS improves visual\ndescriptions and achieves strong results in the zero-shot setting on general\nand fine-grained image-classification datasets, including ImageNet, STL-10,\nFGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilities\ncontribute to enhancing image-generation performance. Finally, we introduce a\nquality-tested silver dataset with descriptions generated with V-GLOSS for all\nImageNet classes.\n", "link": "http://arxiv.org/abs/2306.06077v3", "date": "2024-04-02", "relevancy": 1.6427, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5538}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5193}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantically-Prompted%20Language%20Models%20Improve%20Visual%20Descriptions&body=Title%3A%20Semantically-Prompted%20Language%20Models%20Improve%20Visual%20Descriptions%0AAuthor%3A%20Michael%20Ogezi%20and%20Bradley%20Hauer%20and%20Grzegorz%20Kondrak%0AAbstract%3A%20%20%20Language-vision%20models%20like%20CLIP%20have%20made%20significant%20strides%20in%20vision%0Atasks%2C%20such%20as%20zero-shot%20image%20classification%20%28ZSIC%29.%20However%2C%20generating%0Aspecific%20and%20expressive%20visual%20descriptions%20remains%20challenging%3B%20descriptions%0Aproduced%20by%20current%20methods%20are%20often%20ambiguous%20and%20lacking%20in%20granularity.%20To%0Atackle%20these%20issues%2C%20we%20propose%20V-GLOSS%3A%20Visual%20Glosses%2C%20a%20novel%20method%20built%0Aupon%20two%20key%20ideas.%20The%20first%20is%20Semantic%20Prompting%2C%20which%20conditions%20a%0Alanguage%20model%20on%20structured%20semantic%20knowledge.%20The%20second%20is%20a%20new%0Acontrastive%20algorithm%20that%20elicits%20fine-grained%20distinctions%20between%20similar%0Aconcepts.%20With%20both%20ideas%2C%20we%20demonstrate%20that%20V-GLOSS%20improves%20visual%0Adescriptions%20and%20achieves%20strong%20results%20in%20the%20zero-shot%20setting%20on%20general%0Aand%20fine-grained%20image-classification%20datasets%2C%20including%20ImageNet%2C%20STL-10%2C%0AFGVC%20Aircraft%2C%20and%20Flowers%20102.%20Moreover%2C%20these%20descriptive%20capabilities%0Acontribute%20to%20enhancing%20image-generation%20performance.%20Finally%2C%20we%20introduce%20a%0Aquality-tested%20silver%20dataset%20with%20descriptions%20generated%20with%20V-GLOSS%20for%20all%0AImageNet%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06077v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantically-Prompted%20Language%20Models%20Improve%20Visual%20Descriptions&entry.906535625=Michael%20Ogezi%20and%20Bradley%20Hauer%20and%20Grzegorz%20Kondrak&entry.1292438233=%20%20Language-vision%20models%20like%20CLIP%20have%20made%20significant%20strides%20in%20vision%0Atasks%2C%20such%20as%20zero-shot%20image%20classification%20%28ZSIC%29.%20However%2C%20generating%0Aspecific%20and%20expressive%20visual%20descriptions%20remains%20challenging%3B%20descriptions%0Aproduced%20by%20current%20methods%20are%20often%20ambiguous%20and%20lacking%20in%20granularity.%20To%0Atackle%20these%20issues%2C%20we%20propose%20V-GLOSS%3A%20Visual%20Glosses%2C%20a%20novel%20method%20built%0Aupon%20two%20key%20ideas.%20The%20first%20is%20Semantic%20Prompting%2C%20which%20conditions%20a%0Alanguage%20model%20on%20structured%20semantic%20knowledge.%20The%20second%20is%20a%20new%0Acontrastive%20algorithm%20that%20elicits%20fine-grained%20distinctions%20between%20similar%0Aconcepts.%20With%20both%20ideas%2C%20we%20demonstrate%20that%20V-GLOSS%20improves%20visual%0Adescriptions%20and%20achieves%20strong%20results%20in%20the%20zero-shot%20setting%20on%20general%0Aand%20fine-grained%20image-classification%20datasets%2C%20including%20ImageNet%2C%20STL-10%2C%0AFGVC%20Aircraft%2C%20and%20Flowers%20102.%20Moreover%2C%20these%20descriptive%20capabilities%0Acontribute%20to%20enhancing%20image-generation%20performance.%20Finally%2C%20we%20introduce%20a%0Aquality-tested%20silver%20dataset%20with%20descriptions%20generated%20with%20V-GLOSS%20for%20all%0AImageNet%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06077v3&entry.124074799=Read"},
{"title": "Joint Multimodal Transformer for Emotion Recognition in the Wild", "author": "Paul Waligora and Haseeb Aslam and Osama Zeeshan and Soufiane Belharbi and Alessandro Lameiras Koerich and Marco Pedersoli and Simon Bacon and Eric Granger", "abstract": "  Systems for multimodal emotion recognition (MMER) can typically outperform\nunimodal systems by leveraging the inter- and intra-modal relationships\nbetween, e.g., visual, textual, physiological, and auditory modalities. In this\npaper, an MMER method is proposed that relies on a joint multimodal transformer\nfor fusion with key-based cross-attention. This framework aims to exploit the\ndiverse and complementary nature of different modalities to improve predictive\naccuracy. Separate backbones capture intra-modal spatiotemporal dependencies\nwithin each modality over video sequences. Subsequently, a joint multimodal\ntransformer fusion architecture integrates the individual modality embeddings,\nallowing the model to capture inter-modal and intra-modal relationships\neffectively. Extensive experiments on two challenging expression recognition\ntasks: (1) dimensional emotion recognition on the Affwild2 dataset (with face\nand voice), and (2) pain estimation on the Biovid dataset (with face and\nbiosensors), indicate that the proposed method can work effectively with\ndifferent modalities. Empirical results show that MMER systems with our\nproposed fusion method allow us to outperform relevant baseline and\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.10488v2", "date": "2024-04-02", "relevancy": 1.6414, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5285}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Joint%20Multimodal%20Transformer%20for%20Emotion%20Recognition%20in%20the%20Wild&body=Title%3A%20Joint%20Multimodal%20Transformer%20for%20Emotion%20Recognition%20in%20the%20Wild%0AAuthor%3A%20Paul%20Waligora%20and%20Haseeb%20Aslam%20and%20Osama%20Zeeshan%20and%20Soufiane%20Belharbi%20and%20Alessandro%20Lameiras%20Koerich%20and%20Marco%20Pedersoli%20and%20Simon%20Bacon%20and%20Eric%20Granger%0AAbstract%3A%20%20%20Systems%20for%20multimodal%20emotion%20recognition%20%28MMER%29%20can%20typically%20outperform%0Aunimodal%20systems%20by%20leveraging%20the%20inter-%20and%20intra-modal%20relationships%0Abetween%2C%20e.g.%2C%20visual%2C%20textual%2C%20physiological%2C%20and%20auditory%20modalities.%20In%20this%0Apaper%2C%20an%20MMER%20method%20is%20proposed%20that%20relies%20on%20a%20joint%20multimodal%20transformer%0Afor%20fusion%20with%20key-based%20cross-attention.%20This%20framework%20aims%20to%20exploit%20the%0Adiverse%20and%20complementary%20nature%20of%20different%20modalities%20to%20improve%20predictive%0Aaccuracy.%20Separate%20backbones%20capture%20intra-modal%20spatiotemporal%20dependencies%0Awithin%20each%20modality%20over%20video%20sequences.%20Subsequently%2C%20a%20joint%20multimodal%0Atransformer%20fusion%20architecture%20integrates%20the%20individual%20modality%20embeddings%2C%0Aallowing%20the%20model%20to%20capture%20inter-modal%20and%20intra-modal%20relationships%0Aeffectively.%20Extensive%20experiments%20on%20two%20challenging%20expression%20recognition%0Atasks%3A%20%281%29%20dimensional%20emotion%20recognition%20on%20the%20Affwild2%20dataset%20%28with%20face%0Aand%20voice%29%2C%20and%20%282%29%20pain%20estimation%20on%20the%20Biovid%20dataset%20%28with%20face%20and%0Abiosensors%29%2C%20indicate%20that%20the%20proposed%20method%20can%20work%20effectively%20with%0Adifferent%20modalities.%20Empirical%20results%20show%20that%20MMER%20systems%20with%20our%0Aproposed%20fusion%20method%20allow%20us%20to%20outperform%20relevant%20baseline%20and%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10488v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Multimodal%20Transformer%20for%20Emotion%20Recognition%20in%20the%20Wild&entry.906535625=Paul%20Waligora%20and%20Haseeb%20Aslam%20and%20Osama%20Zeeshan%20and%20Soufiane%20Belharbi%20and%20Alessandro%20Lameiras%20Koerich%20and%20Marco%20Pedersoli%20and%20Simon%20Bacon%20and%20Eric%20Granger&entry.1292438233=%20%20Systems%20for%20multimodal%20emotion%20recognition%20%28MMER%29%20can%20typically%20outperform%0Aunimodal%20systems%20by%20leveraging%20the%20inter-%20and%20intra-modal%20relationships%0Abetween%2C%20e.g.%2C%20visual%2C%20textual%2C%20physiological%2C%20and%20auditory%20modalities.%20In%20this%0Apaper%2C%20an%20MMER%20method%20is%20proposed%20that%20relies%20on%20a%20joint%20multimodal%20transformer%0Afor%20fusion%20with%20key-based%20cross-attention.%20This%20framework%20aims%20to%20exploit%20the%0Adiverse%20and%20complementary%20nature%20of%20different%20modalities%20to%20improve%20predictive%0Aaccuracy.%20Separate%20backbones%20capture%20intra-modal%20spatiotemporal%20dependencies%0Awithin%20each%20modality%20over%20video%20sequences.%20Subsequently%2C%20a%20joint%20multimodal%0Atransformer%20fusion%20architecture%20integrates%20the%20individual%20modality%20embeddings%2C%0Aallowing%20the%20model%20to%20capture%20inter-modal%20and%20intra-modal%20relationships%0Aeffectively.%20Extensive%20experiments%20on%20two%20challenging%20expression%20recognition%0Atasks%3A%20%281%29%20dimensional%20emotion%20recognition%20on%20the%20Affwild2%20dataset%20%28with%20face%0Aand%20voice%29%2C%20and%20%282%29%20pain%20estimation%20on%20the%20Biovid%20dataset%20%28with%20face%20and%0Abiosensors%29%2C%20indicate%20that%20the%20proposed%20method%20can%20work%20effectively%20with%0Adifferent%20modalities.%20Empirical%20results%20show%20that%20MMER%20systems%20with%20our%0Aproposed%20fusion%20method%20allow%20us%20to%20outperform%20relevant%20baseline%20and%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10488v2&entry.124074799=Read"},
{"title": "EGTR: Extracting Graph from Transformer for Scene Graph Generation", "author": "Jinbae Im and JeongYeon Nam and Nokyung Park and Hyungmin Lee and Seunghyun Park", "abstract": "  Scene Graph Generation (SGG) is a challenging task of detecting objects and\npredicting relationships between objects. After DETR was developed, one-stage\nSGG models based on a one-stage object detector have been actively studied.\nHowever, complex modeling is used to predict the relationship between objects,\nand the inherent relationship between object queries learned in the multi-head\nself-attention of the object detector has been neglected. We propose a\nlightweight one-stage SGG model that extracts the relation graph from the\nvarious relationships learned in the multi-head self-attention layers of the\nDETR decoder. By fully utilizing the self-attention by-products, the relation\ngraph can be extracted effectively with a shallow relation extraction head.\nConsidering the dependency of the relation extraction task on the object\ndetection task, we propose a novel relation smoothing technique that adjusts\nthe relation label adaptively according to the quality of the detected objects.\nBy the relation smoothing, the model is trained according to the continuous\ncurriculum that focuses on object detection task at the beginning of training\nand performs multi-task learning as the object detection performance gradually\nimproves. Furthermore, we propose a connectivity prediction task that predicts\nwhether a relation exists between object pairs as an auxiliary task of the\nrelation extraction. We demonstrate the effectiveness and efficiency of our\nmethod for the Visual Genome and Open Image V6 datasets. Our code is publicly\navailable at https://github.com/naver-ai/egtr .\n", "link": "http://arxiv.org/abs/2404.02072v1", "date": "2024-04-02", "relevancy": 1.6224, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.562}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5228}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5059}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EGTR%3A%20Extracting%20Graph%20from%20Transformer%20for%20Scene%20Graph%20Generation&body=Title%3A%20EGTR%3A%20Extracting%20Graph%20from%20Transformer%20for%20Scene%20Graph%20Generation%0AAuthor%3A%20Jinbae%20Im%20and%20JeongYeon%20Nam%20and%20Nokyung%20Park%20and%20Hyungmin%20Lee%20and%20Seunghyun%20Park%0AAbstract%3A%20%20%20Scene%20Graph%20Generation%20%28SGG%29%20is%20a%20challenging%20task%20of%20detecting%20objects%20and%0Apredicting%20relationships%20between%20objects.%20After%20DETR%20was%20developed%2C%20one-stage%0ASGG%20models%20based%20on%20a%20one-stage%20object%20detector%20have%20been%20actively%20studied.%0AHowever%2C%20complex%20modeling%20is%20used%20to%20predict%20the%20relationship%20between%20objects%2C%0Aand%20the%20inherent%20relationship%20between%20object%20queries%20learned%20in%20the%20multi-head%0Aself-attention%20of%20the%20object%20detector%20has%20been%20neglected.%20We%20propose%20a%0Alightweight%20one-stage%20SGG%20model%20that%20extracts%20the%20relation%20graph%20from%20the%0Avarious%20relationships%20learned%20in%20the%20multi-head%20self-attention%20layers%20of%20the%0ADETR%20decoder.%20By%20fully%20utilizing%20the%20self-attention%20by-products%2C%20the%20relation%0Agraph%20can%20be%20extracted%20effectively%20with%20a%20shallow%20relation%20extraction%20head.%0AConsidering%20the%20dependency%20of%20the%20relation%20extraction%20task%20on%20the%20object%0Adetection%20task%2C%20we%20propose%20a%20novel%20relation%20smoothing%20technique%20that%20adjusts%0Athe%20relation%20label%20adaptively%20according%20to%20the%20quality%20of%20the%20detected%20objects.%0ABy%20the%20relation%20smoothing%2C%20the%20model%20is%20trained%20according%20to%20the%20continuous%0Acurriculum%20that%20focuses%20on%20object%20detection%20task%20at%20the%20beginning%20of%20training%0Aand%20performs%20multi-task%20learning%20as%20the%20object%20detection%20performance%20gradually%0Aimproves.%20Furthermore%2C%20we%20propose%20a%20connectivity%20prediction%20task%20that%20predicts%0Awhether%20a%20relation%20exists%20between%20object%20pairs%20as%20an%20auxiliary%20task%20of%20the%0Arelation%20extraction.%20We%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%0Amethod%20for%20the%20Visual%20Genome%20and%20Open%20Image%20V6%20datasets.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/naver-ai/egtr%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02072v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EGTR%3A%20Extracting%20Graph%20from%20Transformer%20for%20Scene%20Graph%20Generation&entry.906535625=Jinbae%20Im%20and%20JeongYeon%20Nam%20and%20Nokyung%20Park%20and%20Hyungmin%20Lee%20and%20Seunghyun%20Park&entry.1292438233=%20%20Scene%20Graph%20Generation%20%28SGG%29%20is%20a%20challenging%20task%20of%20detecting%20objects%20and%0Apredicting%20relationships%20between%20objects.%20After%20DETR%20was%20developed%2C%20one-stage%0ASGG%20models%20based%20on%20a%20one-stage%20object%20detector%20have%20been%20actively%20studied.%0AHowever%2C%20complex%20modeling%20is%20used%20to%20predict%20the%20relationship%20between%20objects%2C%0Aand%20the%20inherent%20relationship%20between%20object%20queries%20learned%20in%20the%20multi-head%0Aself-attention%20of%20the%20object%20detector%20has%20been%20neglected.%20We%20propose%20a%0Alightweight%20one-stage%20SGG%20model%20that%20extracts%20the%20relation%20graph%20from%20the%0Avarious%20relationships%20learned%20in%20the%20multi-head%20self-attention%20layers%20of%20the%0ADETR%20decoder.%20By%20fully%20utilizing%20the%20self-attention%20by-products%2C%20the%20relation%0Agraph%20can%20be%20extracted%20effectively%20with%20a%20shallow%20relation%20extraction%20head.%0AConsidering%20the%20dependency%20of%20the%20relation%20extraction%20task%20on%20the%20object%0Adetection%20task%2C%20we%20propose%20a%20novel%20relation%20smoothing%20technique%20that%20adjusts%0Athe%20relation%20label%20adaptively%20according%20to%20the%20quality%20of%20the%20detected%20objects.%0ABy%20the%20relation%20smoothing%2C%20the%20model%20is%20trained%20according%20to%20the%20continuous%0Acurriculum%20that%20focuses%20on%20object%20detection%20task%20at%20the%20beginning%20of%20training%0Aand%20performs%20multi-task%20learning%20as%20the%20object%20detection%20performance%20gradually%0Aimproves.%20Furthermore%2C%20we%20propose%20a%20connectivity%20prediction%20task%20that%20predicts%0Awhether%20a%20relation%20exists%20between%20object%20pairs%20as%20an%20auxiliary%20task%20of%20the%0Arelation%20extraction.%20We%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%0Amethod%20for%20the%20Visual%20Genome%20and%20Open%20Image%20V6%20datasets.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/naver-ai/egtr%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02072v1&entry.124074799=Read"},
{"title": "Modular Control Architecture for Safe Marine Navigation: Reinforcement\n  Learning and Predictive Safety Filters", "author": "Aksel Vaaler and Svein Jostein Husa and Daniel Menges and Thomas Nakken Larsen and Adil Rasheed", "abstract": "  Many autonomous systems face safety challenges, requiring robust closed-loop\ncontrol to handle physical limitations and safety constraints. Real-world\nsystems, like autonomous ships, encounter nonlinear dynamics and environmental\ndisturbances. Reinforcement learning is increasingly used to adapt to complex\nscenarios, but standard frameworks ensuring safety and stability are lacking.\nPredictive Safety Filters (PSF) offer a promising solution, ensuring constraint\nsatisfaction in learning-based control without explicit constraint handling.\nThis modular approach allows using arbitrary control policies, with the safety\nfilter optimizing proposed actions to meet physical and safety constraints. We\napply this approach to marine navigation, combining RL with PSF on a simulated\nCybership II model. The RL agent is trained on path following and collision\navpodance, while the PSF monitors and modifies control actions for safety.\nResults demonstrate the PSF's effectiveness in maintaining safety without\nhindering the RL agent's learning rate and performance, evaluated against a\nstandard RL agent without PSF.\n", "link": "http://arxiv.org/abs/2312.01855v2", "date": "2024-04-02", "relevancy": 1.6178, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5621}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5427}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5078}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Modular%20Control%20Architecture%20for%20Safe%20Marine%20Navigation%3A%20Reinforcement%0A%20%20Learning%20and%20Predictive%20Safety%20Filters&body=Title%3A%20Modular%20Control%20Architecture%20for%20Safe%20Marine%20Navigation%3A%20Reinforcement%0A%20%20Learning%20and%20Predictive%20Safety%20Filters%0AAuthor%3A%20Aksel%20Vaaler%20and%20Svein%20Jostein%20Husa%20and%20Daniel%20Menges%20and%20Thomas%20Nakken%20Larsen%20and%20Adil%20Rasheed%0AAbstract%3A%20%20%20Many%20autonomous%20systems%20face%20safety%20challenges%2C%20requiring%20robust%20closed-loop%0Acontrol%20to%20handle%20physical%20limitations%20and%20safety%20constraints.%20Real-world%0Asystems%2C%20like%20autonomous%20ships%2C%20encounter%20nonlinear%20dynamics%20and%20environmental%0Adisturbances.%20Reinforcement%20learning%20is%20increasingly%20used%20to%20adapt%20to%20complex%0Ascenarios%2C%20but%20standard%20frameworks%20ensuring%20safety%20and%20stability%20are%20lacking.%0APredictive%20Safety%20Filters%20%28PSF%29%20offer%20a%20promising%20solution%2C%20ensuring%20constraint%0Asatisfaction%20in%20learning-based%20control%20without%20explicit%20constraint%20handling.%0AThis%20modular%20approach%20allows%20using%20arbitrary%20control%20policies%2C%20with%20the%20safety%0Afilter%20optimizing%20proposed%20actions%20to%20meet%20physical%20and%20safety%20constraints.%20We%0Aapply%20this%20approach%20to%20marine%20navigation%2C%20combining%20RL%20with%20PSF%20on%20a%20simulated%0ACybership%20II%20model.%20The%20RL%20agent%20is%20trained%20on%20path%20following%20and%20collision%0Aavpodance%2C%20while%20the%20PSF%20monitors%20and%20modifies%20control%20actions%20for%20safety.%0AResults%20demonstrate%20the%20PSF%27s%20effectiveness%20in%20maintaining%20safety%20without%0Ahindering%20the%20RL%20agent%27s%20learning%20rate%20and%20performance%2C%20evaluated%20against%20a%0Astandard%20RL%20agent%20without%20PSF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01855v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Control%20Architecture%20for%20Safe%20Marine%20Navigation%3A%20Reinforcement%0A%20%20Learning%20and%20Predictive%20Safety%20Filters&entry.906535625=Aksel%20Vaaler%20and%20Svein%20Jostein%20Husa%20and%20Daniel%20Menges%20and%20Thomas%20Nakken%20Larsen%20and%20Adil%20Rasheed&entry.1292438233=%20%20Many%20autonomous%20systems%20face%20safety%20challenges%2C%20requiring%20robust%20closed-loop%0Acontrol%20to%20handle%20physical%20limitations%20and%20safety%20constraints.%20Real-world%0Asystems%2C%20like%20autonomous%20ships%2C%20encounter%20nonlinear%20dynamics%20and%20environmental%0Adisturbances.%20Reinforcement%20learning%20is%20increasingly%20used%20to%20adapt%20to%20complex%0Ascenarios%2C%20but%20standard%20frameworks%20ensuring%20safety%20and%20stability%20are%20lacking.%0APredictive%20Safety%20Filters%20%28PSF%29%20offer%20a%20promising%20solution%2C%20ensuring%20constraint%0Asatisfaction%20in%20learning-based%20control%20without%20explicit%20constraint%20handling.%0AThis%20modular%20approach%20allows%20using%20arbitrary%20control%20policies%2C%20with%20the%20safety%0Afilter%20optimizing%20proposed%20actions%20to%20meet%20physical%20and%20safety%20constraints.%20We%0Aapply%20this%20approach%20to%20marine%20navigation%2C%20combining%20RL%20with%20PSF%20on%20a%20simulated%0ACybership%20II%20model.%20The%20RL%20agent%20is%20trained%20on%20path%20following%20and%20collision%0Aavpodance%2C%20while%20the%20PSF%20monitors%20and%20modifies%20control%20actions%20for%20safety.%0AResults%20demonstrate%20the%20PSF%27s%20effectiveness%20in%20maintaining%20safety%20without%0Ahindering%20the%20RL%20agent%27s%20learning%20rate%20and%20performance%2C%20evaluated%20against%20a%0Astandard%20RL%20agent%20without%20PSF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01855v2&entry.124074799=Read"},
{"title": "Already Moderate Population Sizes Provably Yield Strong Robustness to\n  Noise", "author": "Denis Antipov and Benjamin Doerr and Alexandra Ivanova", "abstract": "  Experience shows that typical evolutionary algorithms can cope well with\nstochastic disturbances such as noisy function evaluations.\n  In this first mathematical runtime analysis of the $(1+\\lambda)$ and\n$(1,\\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise,\nwe show that both algorithms can tolerate constant noise probabilities without\nincreasing the asymptotic runtime on the OneMax benchmark. For this, a\npopulation size $\\lambda$ suffices that is at least logarithmic in the problem\nsize $n$. The only previous result in this direction regarded the less\nrealistic one-bit noise model, required a population size super-linear in the\nproblem size, and proved a runtime guarantee roughly cubic in the noiseless\nruntime for the OneMax benchmark. Our significantly stronger results are based\non the novel proof argument that the noiseless offspring can be seen as a\nbiased uniform crossover between the parent and the noisy offspring. We are\noptimistic that the technical lemmas resulting from this insight will find\napplications also in future mathematical runtime analyses of evolutionary\nalgorithms.\n", "link": "http://arxiv.org/abs/2404.02090v1", "date": "2024-04-02", "relevancy": 1.6148, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.415}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3965}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3953}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Already%20Moderate%20Population%20Sizes%20Provably%20Yield%20Strong%20Robustness%20to%0A%20%20Noise&body=Title%3A%20Already%20Moderate%20Population%20Sizes%20Provably%20Yield%20Strong%20Robustness%20to%0A%20%20Noise%0AAuthor%3A%20Denis%20Antipov%20and%20Benjamin%20Doerr%20and%20Alexandra%20Ivanova%0AAbstract%3A%20%20%20Experience%20shows%20that%20typical%20evolutionary%20algorithms%20can%20cope%20well%20with%0Astochastic%20disturbances%20such%20as%20noisy%20function%20evaluations.%0A%20%20In%20this%20first%20mathematical%20runtime%20analysis%20of%20the%20%24%281%2B%5Clambda%29%24%20and%0A%24%281%2C%5Clambda%29%24%20evolutionary%20algorithms%20in%20the%20presence%20of%20prior%20bit-wise%20noise%2C%0Awe%20show%20that%20both%20algorithms%20can%20tolerate%20constant%20noise%20probabilities%20without%0Aincreasing%20the%20asymptotic%20runtime%20on%20the%20OneMax%20benchmark.%20For%20this%2C%20a%0Apopulation%20size%20%24%5Clambda%24%20suffices%20that%20is%20at%20least%20logarithmic%20in%20the%20problem%0Asize%20%24n%24.%20The%20only%20previous%20result%20in%20this%20direction%20regarded%20the%20less%0Arealistic%20one-bit%20noise%20model%2C%20required%20a%20population%20size%20super-linear%20in%20the%0Aproblem%20size%2C%20and%20proved%20a%20runtime%20guarantee%20roughly%20cubic%20in%20the%20noiseless%0Aruntime%20for%20the%20OneMax%20benchmark.%20Our%20significantly%20stronger%20results%20are%20based%0Aon%20the%20novel%20proof%20argument%20that%20the%20noiseless%20offspring%20can%20be%20seen%20as%20a%0Abiased%20uniform%20crossover%20between%20the%20parent%20and%20the%20noisy%20offspring.%20We%20are%0Aoptimistic%20that%20the%20technical%20lemmas%20resulting%20from%20this%20insight%20will%20find%0Aapplications%20also%20in%20future%20mathematical%20runtime%20analyses%20of%20evolutionary%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02090v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Already%20Moderate%20Population%20Sizes%20Provably%20Yield%20Strong%20Robustness%20to%0A%20%20Noise&entry.906535625=Denis%20Antipov%20and%20Benjamin%20Doerr%20and%20Alexandra%20Ivanova&entry.1292438233=%20%20Experience%20shows%20that%20typical%20evolutionary%20algorithms%20can%20cope%20well%20with%0Astochastic%20disturbances%20such%20as%20noisy%20function%20evaluations.%0A%20%20In%20this%20first%20mathematical%20runtime%20analysis%20of%20the%20%24%281%2B%5Clambda%29%24%20and%0A%24%281%2C%5Clambda%29%24%20evolutionary%20algorithms%20in%20the%20presence%20of%20prior%20bit-wise%20noise%2C%0Awe%20show%20that%20both%20algorithms%20can%20tolerate%20constant%20noise%20probabilities%20without%0Aincreasing%20the%20asymptotic%20runtime%20on%20the%20OneMax%20benchmark.%20For%20this%2C%20a%0Apopulation%20size%20%24%5Clambda%24%20suffices%20that%20is%20at%20least%20logarithmic%20in%20the%20problem%0Asize%20%24n%24.%20The%20only%20previous%20result%20in%20this%20direction%20regarded%20the%20less%0Arealistic%20one-bit%20noise%20model%2C%20required%20a%20population%20size%20super-linear%20in%20the%0Aproblem%20size%2C%20and%20proved%20a%20runtime%20guarantee%20roughly%20cubic%20in%20the%20noiseless%0Aruntime%20for%20the%20OneMax%20benchmark.%20Our%20significantly%20stronger%20results%20are%20based%0Aon%20the%20novel%20proof%20argument%20that%20the%20noiseless%20offspring%20can%20be%20seen%20as%20a%0Abiased%20uniform%20crossover%20between%20the%20parent%20and%20the%20noisy%20offspring.%20We%20are%0Aoptimistic%20that%20the%20technical%20lemmas%20resulting%20from%20this%20insight%20will%20find%0Aapplications%20also%20in%20future%20mathematical%20runtime%20analyses%20of%20evolutionary%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02090v1&entry.124074799=Read"},
{"title": "Collaborative Safe Formation Control for Coupled Multi-Agent Systems", "author": "Brooks A. Butler and Chi Ho Leung and Philip E. Par\u00e9", "abstract": "  The safe control of multi-robot swarms is a challenging and active field of\nresearch, where common goals include maintaining group cohesion while\nsimultaneously avoiding obstacles and inter-agent collision. Building off our\npreviously developed theory for distributed collaborative safety-critical\ncontrol for networked dynamic systems, we propose a distributed algorithm for\nthe formation control of robot swarms given individual agent dynamics, induced\nformation dynamics, and local neighborhood position and velocity information\nwithin a defined sensing radius for each agent. Individual safety guarantees\nfor each agent are obtained using rounds of communication between neighbors to\nrestrict unsafe control actions among cooperating agents through safety\nconditions derived from high-order control barrier functions. We provide\nconditions under which a swarm is guaranteed to achieve collective safety with\nrespect to multiple obstacles using a modified collaborative safety algorithm.\nWe demonstrate the performance of our distributed algorithm via simulation in a\nsimplified physics-based environment.\n", "link": "http://arxiv.org/abs/2311.11156v3", "date": "2024-04-02", "relevancy": 1.6069, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5704}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.495}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4894}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Safe%20Formation%20Control%20for%20Coupled%20Multi-Agent%20Systems&body=Title%3A%20Collaborative%20Safe%20Formation%20Control%20for%20Coupled%20Multi-Agent%20Systems%0AAuthor%3A%20Brooks%20A.%20Butler%20and%20Chi%20Ho%20Leung%20and%20Philip%20E.%20Par%C3%A9%0AAbstract%3A%20%20%20The%20safe%20control%20of%20multi-robot%20swarms%20is%20a%20challenging%20and%20active%20field%20of%0Aresearch%2C%20where%20common%20goals%20include%20maintaining%20group%20cohesion%20while%0Asimultaneously%20avoiding%20obstacles%20and%20inter-agent%20collision.%20Building%20off%20our%0Apreviously%20developed%20theory%20for%20distributed%20collaborative%20safety-critical%0Acontrol%20for%20networked%20dynamic%20systems%2C%20we%20propose%20a%20distributed%20algorithm%20for%0Athe%20formation%20control%20of%20robot%20swarms%20given%20individual%20agent%20dynamics%2C%20induced%0Aformation%20dynamics%2C%20and%20local%20neighborhood%20position%20and%20velocity%20information%0Awithin%20a%20defined%20sensing%20radius%20for%20each%20agent.%20Individual%20safety%20guarantees%0Afor%20each%20agent%20are%20obtained%20using%20rounds%20of%20communication%20between%20neighbors%20to%0Arestrict%20unsafe%20control%20actions%20among%20cooperating%20agents%20through%20safety%0Aconditions%20derived%20from%20high-order%20control%20barrier%20functions.%20We%20provide%0Aconditions%20under%20which%20a%20swarm%20is%20guaranteed%20to%20achieve%20collective%20safety%20with%0Arespect%20to%20multiple%20obstacles%20using%20a%20modified%20collaborative%20safety%20algorithm.%0AWe%20demonstrate%20the%20performance%20of%20our%20distributed%20algorithm%20via%20simulation%20in%20a%0Asimplified%20physics-based%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11156v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Safe%20Formation%20Control%20for%20Coupled%20Multi-Agent%20Systems&entry.906535625=Brooks%20A.%20Butler%20and%20Chi%20Ho%20Leung%20and%20Philip%20E.%20Par%C3%A9&entry.1292438233=%20%20The%20safe%20control%20of%20multi-robot%20swarms%20is%20a%20challenging%20and%20active%20field%20of%0Aresearch%2C%20where%20common%20goals%20include%20maintaining%20group%20cohesion%20while%0Asimultaneously%20avoiding%20obstacles%20and%20inter-agent%20collision.%20Building%20off%20our%0Apreviously%20developed%20theory%20for%20distributed%20collaborative%20safety-critical%0Acontrol%20for%20networked%20dynamic%20systems%2C%20we%20propose%20a%20distributed%20algorithm%20for%0Athe%20formation%20control%20of%20robot%20swarms%20given%20individual%20agent%20dynamics%2C%20induced%0Aformation%20dynamics%2C%20and%20local%20neighborhood%20position%20and%20velocity%20information%0Awithin%20a%20defined%20sensing%20radius%20for%20each%20agent.%20Individual%20safety%20guarantees%0Afor%20each%20agent%20are%20obtained%20using%20rounds%20of%20communication%20between%20neighbors%20to%0Arestrict%20unsafe%20control%20actions%20among%20cooperating%20agents%20through%20safety%0Aconditions%20derived%20from%20high-order%20control%20barrier%20functions.%20We%20provide%0Aconditions%20under%20which%20a%20swarm%20is%20guaranteed%20to%20achieve%20collective%20safety%20with%0Arespect%20to%20multiple%20obstacles%20using%20a%20modified%20collaborative%20safety%20algorithm.%0AWe%20demonstrate%20the%20performance%20of%20our%20distributed%20algorithm%20via%20simulation%20in%20a%0Asimplified%20physics-based%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11156v3&entry.124074799=Read"},
{"title": "Large Language Models for Orchestrating Bimanual Robots", "author": "Kun Chu and Xufeng Zhao and Cornelius Weber and Mengdi Li and Wenhao Lu and Stefan Wermter", "abstract": "  Although there has been rapid progress in endowing robots with the ability to\nsolve complex manipulation tasks, generating control policies for bimanual\nrobots to solve tasks involving two hands is still challenging because of the\ndifficulties in effective temporal and spatial coordination. With emergent\nabilities in terms of step-by-step reasoning and in-context learning, Large\nLanguage Models (LLMs) have taken control of a variety of robotic tasks.\nHowever, the nature of language communication via a single sequence of discrete\nsymbols makes LLM-based coordination in continuous space a particular challenge\nfor bimanual tasks. To tackle this challenge for the first time by an LLM, we\npresent LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing\nan LLM to analyze task configurations and devise coordination control policies\nfor addressing long-horizon bimanual tasks. In the simulated environment, the\nLABOR agent is evaluated through several everyday tasks on the NICOL humanoid\nrobot. Reported success rates indicate that overall coordination efficiency is\nclose to optimal performance, while the analysis of failure causes, classified\ninto spatial and temporal coordination and skill selection, shows that these\nvary over tasks. The project website can be found at\nhttp://labor-agent.github.io\n", "link": "http://arxiv.org/abs/2404.02018v1", "date": "2024-04-02", "relevancy": 1.5998, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6142}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5641}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4886}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Orchestrating%20Bimanual%20Robots&body=Title%3A%20Large%20Language%20Models%20for%20Orchestrating%20Bimanual%20Robots%0AAuthor%3A%20Kun%20Chu%20and%20Xufeng%20Zhao%20and%20Cornelius%20Weber%20and%20Mengdi%20Li%20and%20Wenhao%20Lu%20and%20Stefan%20Wermter%0AAbstract%3A%20%20%20Although%20there%20has%20been%20rapid%20progress%20in%20endowing%20robots%20with%20the%20ability%20to%0Asolve%20complex%20manipulation%20tasks%2C%20generating%20control%20policies%20for%20bimanual%0Arobots%20to%20solve%20tasks%20involving%20two%20hands%20is%20still%20challenging%20because%20of%20the%0Adifficulties%20in%20effective%20temporal%20and%20spatial%20coordination.%20With%20emergent%0Aabilities%20in%20terms%20of%20step-by-step%20reasoning%20and%20in-context%20learning%2C%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20taken%20control%20of%20a%20variety%20of%20robotic%20tasks.%0AHowever%2C%20the%20nature%20of%20language%20communication%20via%20a%20single%20sequence%20of%20discrete%0Asymbols%20makes%20LLM-based%20coordination%20in%20continuous%20space%20a%20particular%20challenge%0Afor%20bimanual%20tasks.%20To%20tackle%20this%20challenge%20for%20the%20first%20time%20by%20an%20LLM%2C%20we%0Apresent%20LAnguage-model-based%20Bimanual%20ORchestration%20%28LABOR%29%2C%20an%20agent%20utilizing%0Aan%20LLM%20to%20analyze%20task%20configurations%20and%20devise%20coordination%20control%20policies%0Afor%20addressing%20long-horizon%20bimanual%20tasks.%20In%20the%20simulated%20environment%2C%20the%0ALABOR%20agent%20is%20evaluated%20through%20several%20everyday%20tasks%20on%20the%20NICOL%20humanoid%0Arobot.%20Reported%20success%20rates%20indicate%20that%20overall%20coordination%20efficiency%20is%0Aclose%20to%20optimal%20performance%2C%20while%20the%20analysis%20of%20failure%20causes%2C%20classified%0Ainto%20spatial%20and%20temporal%20coordination%20and%20skill%20selection%2C%20shows%20that%20these%0Avary%20over%20tasks.%20The%20project%20website%20can%20be%20found%20at%0Ahttp%3A//labor-agent.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02018v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Orchestrating%20Bimanual%20Robots&entry.906535625=Kun%20Chu%20and%20Xufeng%20Zhao%20and%20Cornelius%20Weber%20and%20Mengdi%20Li%20and%20Wenhao%20Lu%20and%20Stefan%20Wermter&entry.1292438233=%20%20Although%20there%20has%20been%20rapid%20progress%20in%20endowing%20robots%20with%20the%20ability%20to%0Asolve%20complex%20manipulation%20tasks%2C%20generating%20control%20policies%20for%20bimanual%0Arobots%20to%20solve%20tasks%20involving%20two%20hands%20is%20still%20challenging%20because%20of%20the%0Adifficulties%20in%20effective%20temporal%20and%20spatial%20coordination.%20With%20emergent%0Aabilities%20in%20terms%20of%20step-by-step%20reasoning%20and%20in-context%20learning%2C%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20taken%20control%20of%20a%20variety%20of%20robotic%20tasks.%0AHowever%2C%20the%20nature%20of%20language%20communication%20via%20a%20single%20sequence%20of%20discrete%0Asymbols%20makes%20LLM-based%20coordination%20in%20continuous%20space%20a%20particular%20challenge%0Afor%20bimanual%20tasks.%20To%20tackle%20this%20challenge%20for%20the%20first%20time%20by%20an%20LLM%2C%20we%0Apresent%20LAnguage-model-based%20Bimanual%20ORchestration%20%28LABOR%29%2C%20an%20agent%20utilizing%0Aan%20LLM%20to%20analyze%20task%20configurations%20and%20devise%20coordination%20control%20policies%0Afor%20addressing%20long-horizon%20bimanual%20tasks.%20In%20the%20simulated%20environment%2C%20the%0ALABOR%20agent%20is%20evaluated%20through%20several%20everyday%20tasks%20on%20the%20NICOL%20humanoid%0Arobot.%20Reported%20success%20rates%20indicate%20that%20overall%20coordination%20efficiency%20is%0Aclose%20to%20optimal%20performance%2C%20while%20the%20analysis%20of%20failure%20causes%2C%20classified%0Ainto%20spatial%20and%20temporal%20coordination%20and%20skill%20selection%2C%20shows%20that%20these%0Avary%20over%20tasks.%20The%20project%20website%20can%20be%20found%20at%0Ahttp%3A//labor-agent.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02018v1&entry.124074799=Read"},
{"title": "Joint-Task Regularization for Partially Labeled Multi-Task Learning", "author": "Kento Nishi and Junsik Kim and Wanhua Li and Hanspeter Pfister", "abstract": "  Multi-task learning has become increasingly popular in the machine learning\nfield, but its practicality is hindered by the need for large, labeled\ndatasets. Most multi-task learning methods depend on fully labeled datasets\nwherein each input example is accompanied by ground-truth labels for all target\ntasks. Unfortunately, curating such datasets can be prohibitively expensive and\nimpractical, especially for dense prediction tasks which require per-pixel\nlabels for each image. With this in mind, we propose Joint-Task Regularization\n(JTR), an intuitive technique which leverages cross-task relations to\nsimultaneously regularize all tasks in a single joint-task latent space to\nimprove learning when data is not fully labeled for all tasks. JTR stands out\nfrom existing approaches in that it regularizes all tasks jointly rather than\nseparately in pairs -- therefore, it achieves linear complexity relative to the\nnumber of tasks while previous methods scale quadratically. To demonstrate the\nvalidity of our approach, we extensively benchmark our method across a wide\nvariety of partially labeled scenarios based on NYU-v2, Cityscapes, and\nTaskonomy.\n", "link": "http://arxiv.org/abs/2404.01976v1", "date": "2024-04-02", "relevancy": 1.5896, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5483}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5192}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4945}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Joint-Task%20Regularization%20for%20Partially%20Labeled%20Multi-Task%20Learning&body=Title%3A%20Joint-Task%20Regularization%20for%20Partially%20Labeled%20Multi-Task%20Learning%0AAuthor%3A%20Kento%20Nishi%20and%20Junsik%20Kim%20and%20Wanhua%20Li%20and%20Hanspeter%20Pfister%0AAbstract%3A%20%20%20Multi-task%20learning%20has%20become%20increasingly%20popular%20in%20the%20machine%20learning%0Afield%2C%20but%20its%20practicality%20is%20hindered%20by%20the%20need%20for%20large%2C%20labeled%0Adatasets.%20Most%20multi-task%20learning%20methods%20depend%20on%20fully%20labeled%20datasets%0Awherein%20each%20input%20example%20is%20accompanied%20by%20ground-truth%20labels%20for%20all%20target%0Atasks.%20Unfortunately%2C%20curating%20such%20datasets%20can%20be%20prohibitively%20expensive%20and%0Aimpractical%2C%20especially%20for%20dense%20prediction%20tasks%20which%20require%20per-pixel%0Alabels%20for%20each%20image.%20With%20this%20in%20mind%2C%20we%20propose%20Joint-Task%20Regularization%0A%28JTR%29%2C%20an%20intuitive%20technique%20which%20leverages%20cross-task%20relations%20to%0Asimultaneously%20regularize%20all%20tasks%20in%20a%20single%20joint-task%20latent%20space%20to%0Aimprove%20learning%20when%20data%20is%20not%20fully%20labeled%20for%20all%20tasks.%20JTR%20stands%20out%0Afrom%20existing%20approaches%20in%20that%20it%20regularizes%20all%20tasks%20jointly%20rather%20than%0Aseparately%20in%20pairs%20--%20therefore%2C%20it%20achieves%20linear%20complexity%20relative%20to%20the%0Anumber%20of%20tasks%20while%20previous%20methods%20scale%20quadratically.%20To%20demonstrate%20the%0Avalidity%20of%20our%20approach%2C%20we%20extensively%20benchmark%20our%20method%20across%20a%20wide%0Avariety%20of%20partially%20labeled%20scenarios%20based%20on%20NYU-v2%2C%20Cityscapes%2C%20and%0ATaskonomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01976v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint-Task%20Regularization%20for%20Partially%20Labeled%20Multi-Task%20Learning&entry.906535625=Kento%20Nishi%20and%20Junsik%20Kim%20and%20Wanhua%20Li%20and%20Hanspeter%20Pfister&entry.1292438233=%20%20Multi-task%20learning%20has%20become%20increasingly%20popular%20in%20the%20machine%20learning%0Afield%2C%20but%20its%20practicality%20is%20hindered%20by%20the%20need%20for%20large%2C%20labeled%0Adatasets.%20Most%20multi-task%20learning%20methods%20depend%20on%20fully%20labeled%20datasets%0Awherein%20each%20input%20example%20is%20accompanied%20by%20ground-truth%20labels%20for%20all%20target%0Atasks.%20Unfortunately%2C%20curating%20such%20datasets%20can%20be%20prohibitively%20expensive%20and%0Aimpractical%2C%20especially%20for%20dense%20prediction%20tasks%20which%20require%20per-pixel%0Alabels%20for%20each%20image.%20With%20this%20in%20mind%2C%20we%20propose%20Joint-Task%20Regularization%0A%28JTR%29%2C%20an%20intuitive%20technique%20which%20leverages%20cross-task%20relations%20to%0Asimultaneously%20regularize%20all%20tasks%20in%20a%20single%20joint-task%20latent%20space%20to%0Aimprove%20learning%20when%20data%20is%20not%20fully%20labeled%20for%20all%20tasks.%20JTR%20stands%20out%0Afrom%20existing%20approaches%20in%20that%20it%20regularizes%20all%20tasks%20jointly%20rather%20than%0Aseparately%20in%20pairs%20--%20therefore%2C%20it%20achieves%20linear%20complexity%20relative%20to%20the%0Anumber%20of%20tasks%20while%20previous%20methods%20scale%20quadratically.%20To%20demonstrate%20the%0Avalidity%20of%20our%20approach%2C%20we%20extensively%20benchmark%20our%20method%20across%20a%20wide%0Avariety%20of%20partially%20labeled%20scenarios%20based%20on%20NYU-v2%2C%20Cityscapes%2C%20and%0ATaskonomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01976v1&entry.124074799=Read"},
{"title": "Zero-shot Safety Prediction for Autonomous Robots with Foundation World\n  Models", "author": "Zhenjiang Mao and Siqi Dai and Yuang Geng and Ivan Ruchkin", "abstract": "  A world model creates a surrogate world to train a controller and predict\nsafety violations by learning the internal dynamic model of systems. However,\nthe existing world models rely solely on statistical learning of how\nobservations change in response to actions, lacking precise quantification of\nhow accurate the surrogate dynamics are, which poses a significant challenge in\nsafety-critical systems. To address this challenge, we propose foundation world\nmodels that embed observations into meaningful and causally latent\nrepresentations. This enables the surrogate dynamics to directly predict causal\nfuture states by leveraging a training-free large language model. In two common\nbenchmarks, this novel model outperforms standard world models in the safety\nprediction task and has a performance comparable to supervised learning despite\nnot using any data. We evaluate its performance with a more specialized and\nsystem-relevant metric by comparing estimated states instead of aggregating\nobservation-wide error.\n", "link": "http://arxiv.org/abs/2404.00462v2", "date": "2024-04-02", "relevancy": 1.5881, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5484}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5403}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4829}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Safety%20Prediction%20for%20Autonomous%20Robots%20with%20Foundation%20World%0A%20%20Models&body=Title%3A%20Zero-shot%20Safety%20Prediction%20for%20Autonomous%20Robots%20with%20Foundation%20World%0A%20%20Models%0AAuthor%3A%20Zhenjiang%20Mao%20and%20Siqi%20Dai%20and%20Yuang%20Geng%20and%20Ivan%20Ruchkin%0AAbstract%3A%20%20%20A%20world%20model%20creates%20a%20surrogate%20world%20to%20train%20a%20controller%20and%20predict%0Asafety%20violations%20by%20learning%20the%20internal%20dynamic%20model%20of%20systems.%20However%2C%0Athe%20existing%20world%20models%20rely%20solely%20on%20statistical%20learning%20of%20how%0Aobservations%20change%20in%20response%20to%20actions%2C%20lacking%20precise%20quantification%20of%0Ahow%20accurate%20the%20surrogate%20dynamics%20are%2C%20which%20poses%20a%20significant%20challenge%20in%0Asafety-critical%20systems.%20To%20address%20this%20challenge%2C%20we%20propose%20foundation%20world%0Amodels%20that%20embed%20observations%20into%20meaningful%20and%20causally%20latent%0Arepresentations.%20This%20enables%20the%20surrogate%20dynamics%20to%20directly%20predict%20causal%0Afuture%20states%20by%20leveraging%20a%20training-free%20large%20language%20model.%20In%20two%20common%0Abenchmarks%2C%20this%20novel%20model%20outperforms%20standard%20world%20models%20in%20the%20safety%0Aprediction%20task%20and%20has%20a%20performance%20comparable%20to%20supervised%20learning%20despite%0Anot%20using%20any%20data.%20We%20evaluate%20its%20performance%20with%20a%20more%20specialized%20and%0Asystem-relevant%20metric%20by%20comparing%20estimated%20states%20instead%20of%20aggregating%0Aobservation-wide%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00462v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Safety%20Prediction%20for%20Autonomous%20Robots%20with%20Foundation%20World%0A%20%20Models&entry.906535625=Zhenjiang%20Mao%20and%20Siqi%20Dai%20and%20Yuang%20Geng%20and%20Ivan%20Ruchkin&entry.1292438233=%20%20A%20world%20model%20creates%20a%20surrogate%20world%20to%20train%20a%20controller%20and%20predict%0Asafety%20violations%20by%20learning%20the%20internal%20dynamic%20model%20of%20systems.%20However%2C%0Athe%20existing%20world%20models%20rely%20solely%20on%20statistical%20learning%20of%20how%0Aobservations%20change%20in%20response%20to%20actions%2C%20lacking%20precise%20quantification%20of%0Ahow%20accurate%20the%20surrogate%20dynamics%20are%2C%20which%20poses%20a%20significant%20challenge%20in%0Asafety-critical%20systems.%20To%20address%20this%20challenge%2C%20we%20propose%20foundation%20world%0Amodels%20that%20embed%20observations%20into%20meaningful%20and%20causally%20latent%0Arepresentations.%20This%20enables%20the%20surrogate%20dynamics%20to%20directly%20predict%20causal%0Afuture%20states%20by%20leveraging%20a%20training-free%20large%20language%20model.%20In%20two%20common%0Abenchmarks%2C%20this%20novel%20model%20outperforms%20standard%20world%20models%20in%20the%20safety%0Aprediction%20task%20and%20has%20a%20performance%20comparable%20to%20supervised%20learning%20despite%0Anot%20using%20any%20data.%20We%20evaluate%20its%20performance%20with%20a%20more%20specialized%20and%0Asystem-relevant%20metric%20by%20comparing%20estimated%20states%20instead%20of%20aggregating%0Aobservation-wide%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00462v2&entry.124074799=Read"},
{"title": "JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using\n  in-context learning with GPT and instruction-tuned Llama models", "author": " Arefa and Mohammed Abbas Ansari and Chandni Saxena and Tanvir Ahmad", "abstract": "  This paper presents our system development for SemEval-2024 Task 3: \"The\nCompetition of Multimodal Emotion Cause Analysis in Conversations\". Effectively\ncapturing emotions in human conversations requires integrating multiple\nmodalities such as text, audio, and video. However, the complexities of these\ndiverse modalities pose challenges for developing an efficient multimodal\nemotion cause analysis (ECA) system. Our proposed approach addresses these\nchallenges by a two-step framework. We adopt two different approaches in our\nimplementation. In Approach 1, we employ instruction-tuning with two separate\nLlama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V\nfor conversation-level video description and employ in-context learning with\nannotated conversation using GPT 3.5. Our system wins rank 4, and system\nablation experiments demonstrate that our proposed solutions achieve\nsignificant performance gains. All the experimental codes are available on\nGithub.\n", "link": "http://arxiv.org/abs/2403.04798v2", "date": "2024-04-02", "relevancy": 1.569, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5394}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5028}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20JMI%20at%20SemEval%202024%20Task%203%3A%20Two-step%20approach%20for%20multimodal%20ECAC%20using%0A%20%20in-context%20learning%20with%20GPT%20and%20instruction-tuned%20Llama%20models&body=Title%3A%20JMI%20at%20SemEval%202024%20Task%203%3A%20Two-step%20approach%20for%20multimodal%20ECAC%20using%0A%20%20in-context%20learning%20with%20GPT%20and%20instruction-tuned%20Llama%20models%0AAuthor%3A%20%20Arefa%20and%20Mohammed%20Abbas%20Ansari%20and%20Chandni%20Saxena%20and%20Tanvir%20Ahmad%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20system%20development%20for%20SemEval-2024%20Task%203%3A%20%22The%0ACompetition%20of%20Multimodal%20Emotion%20Cause%20Analysis%20in%20Conversations%22.%20Effectively%0Acapturing%20emotions%20in%20human%20conversations%20requires%20integrating%20multiple%0Amodalities%20such%20as%20text%2C%20audio%2C%20and%20video.%20However%2C%20the%20complexities%20of%20these%0Adiverse%20modalities%20pose%20challenges%20for%20developing%20an%20efficient%20multimodal%0Aemotion%20cause%20analysis%20%28ECA%29%20system.%20Our%20proposed%20approach%20addresses%20these%0Achallenges%20by%20a%20two-step%20framework.%20We%20adopt%20two%20different%20approaches%20in%20our%0Aimplementation.%20In%20Approach%201%2C%20we%20employ%20instruction-tuning%20with%20two%20separate%0ALlama%202%20models%20for%20emotion%20and%20cause%20prediction.%20In%20Approach%202%2C%20we%20use%20GPT-4V%0Afor%20conversation-level%20video%20description%20and%20employ%20in-context%20learning%20with%0Aannotated%20conversation%20using%20GPT%203.5.%20Our%20system%20wins%20rank%204%2C%20and%20system%0Aablation%20experiments%20demonstrate%20that%20our%20proposed%20solutions%20achieve%0Asignificant%20performance%20gains.%20All%20the%20experimental%20codes%20are%20available%20on%0AGithub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04798v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JMI%20at%20SemEval%202024%20Task%203%3A%20Two-step%20approach%20for%20multimodal%20ECAC%20using%0A%20%20in-context%20learning%20with%20GPT%20and%20instruction-tuned%20Llama%20models&entry.906535625=%20Arefa%20and%20Mohammed%20Abbas%20Ansari%20and%20Chandni%20Saxena%20and%20Tanvir%20Ahmad&entry.1292438233=%20%20This%20paper%20presents%20our%20system%20development%20for%20SemEval-2024%20Task%203%3A%20%22The%0ACompetition%20of%20Multimodal%20Emotion%20Cause%20Analysis%20in%20Conversations%22.%20Effectively%0Acapturing%20emotions%20in%20human%20conversations%20requires%20integrating%20multiple%0Amodalities%20such%20as%20text%2C%20audio%2C%20and%20video.%20However%2C%20the%20complexities%20of%20these%0Adiverse%20modalities%20pose%20challenges%20for%20developing%20an%20efficient%20multimodal%0Aemotion%20cause%20analysis%20%28ECA%29%20system.%20Our%20proposed%20approach%20addresses%20these%0Achallenges%20by%20a%20two-step%20framework.%20We%20adopt%20two%20different%20approaches%20in%20our%0Aimplementation.%20In%20Approach%201%2C%20we%20employ%20instruction-tuning%20with%20two%20separate%0ALlama%202%20models%20for%20emotion%20and%20cause%20prediction.%20In%20Approach%202%2C%20we%20use%20GPT-4V%0Afor%20conversation-level%20video%20description%20and%20employ%20in-context%20learning%20with%0Aannotated%20conversation%20using%20GPT%203.5.%20Our%20system%20wins%20rank%204%2C%20and%20system%0Aablation%20experiments%20demonstrate%20that%20our%20proposed%20solutions%20achieve%0Asignificant%20performance%20gains.%20All%20the%20experimental%20codes%20are%20available%20on%0AGithub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04798v2&entry.124074799=Read"},
{"title": "Emergence of Chemotactic Strategies with Multi-Agent Reinforcement\n  Learning", "author": "Samuel Tovey and Christoph Lohrmann and Christian Holm", "abstract": "  Reinforcement learning (RL) is a flexible and efficient method for\nprogramming micro-robots in complex environments. Here we investigate whether\nreinforcement learning can provide insights into biological systems when\ntrained to perform chemotaxis. Namely, whether we can learn about how\nintelligent agents process given information in order to swim towards a target.\nWe run simulations covering a range of agent shapes, sizes, and swim speeds to\ndetermine if the physical constraints on biological swimmers, namely Brownian\nmotion, lead to regions where reinforcement learners' training fails. We find\nthat the RL agents can perform chemotaxis as soon as it is physically possible\nand, in some cases, even before the active swimming overpowers the stochastic\nenvironment. We study the efficiency of the emergent policy and identify\nconvergence in agent size and swim speeds. Finally, we study the strategy\nadopted by the reinforcement learning algorithm to explain how the agents\nperform their tasks. To this end, we identify three emerging dominant\nstrategies and several rare approaches taken. These strategies, whilst\nproducing almost identical trajectories in simulation, are distinct and give\ninsight into the possible mechanisms behind which biological agents explore\ntheir environment and respond to changing conditions.\n", "link": "http://arxiv.org/abs/2404.01999v1", "date": "2024-04-02", "relevancy": 1.5676, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5451}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5204}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5143}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20Chemotactic%20Strategies%20with%20Multi-Agent%20Reinforcement%0A%20%20Learning&body=Title%3A%20Emergence%20of%20Chemotactic%20Strategies%20with%20Multi-Agent%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Samuel%20Tovey%20and%20Christoph%20Lohrmann%20and%20Christian%20Holm%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20is%20a%20flexible%20and%20efficient%20method%20for%0Aprogramming%20micro-robots%20in%20complex%20environments.%20Here%20we%20investigate%20whether%0Areinforcement%20learning%20can%20provide%20insights%20into%20biological%20systems%20when%0Atrained%20to%20perform%20chemotaxis.%20Namely%2C%20whether%20we%20can%20learn%20about%20how%0Aintelligent%20agents%20process%20given%20information%20in%20order%20to%20swim%20towards%20a%20target.%0AWe%20run%20simulations%20covering%20a%20range%20of%20agent%20shapes%2C%20sizes%2C%20and%20swim%20speeds%20to%0Adetermine%20if%20the%20physical%20constraints%20on%20biological%20swimmers%2C%20namely%20Brownian%0Amotion%2C%20lead%20to%20regions%20where%20reinforcement%20learners%27%20training%20fails.%20We%20find%0Athat%20the%20RL%20agents%20can%20perform%20chemotaxis%20as%20soon%20as%20it%20is%20physically%20possible%0Aand%2C%20in%20some%20cases%2C%20even%20before%20the%20active%20swimming%20overpowers%20the%20stochastic%0Aenvironment.%20We%20study%20the%20efficiency%20of%20the%20emergent%20policy%20and%20identify%0Aconvergence%20in%20agent%20size%20and%20swim%20speeds.%20Finally%2C%20we%20study%20the%20strategy%0Aadopted%20by%20the%20reinforcement%20learning%20algorithm%20to%20explain%20how%20the%20agents%0Aperform%20their%20tasks.%20To%20this%20end%2C%20we%20identify%20three%20emerging%20dominant%0Astrategies%20and%20several%20rare%20approaches%20taken.%20These%20strategies%2C%20whilst%0Aproducing%20almost%20identical%20trajectories%20in%20simulation%2C%20are%20distinct%20and%20give%0Ainsight%20into%20the%20possible%20mechanisms%20behind%20which%20biological%20agents%20explore%0Atheir%20environment%20and%20respond%20to%20changing%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01999v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20Chemotactic%20Strategies%20with%20Multi-Agent%20Reinforcement%0A%20%20Learning&entry.906535625=Samuel%20Tovey%20and%20Christoph%20Lohrmann%20and%20Christian%20Holm&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20is%20a%20flexible%20and%20efficient%20method%20for%0Aprogramming%20micro-robots%20in%20complex%20environments.%20Here%20we%20investigate%20whether%0Areinforcement%20learning%20can%20provide%20insights%20into%20biological%20systems%20when%0Atrained%20to%20perform%20chemotaxis.%20Namely%2C%20whether%20we%20can%20learn%20about%20how%0Aintelligent%20agents%20process%20given%20information%20in%20order%20to%20swim%20towards%20a%20target.%0AWe%20run%20simulations%20covering%20a%20range%20of%20agent%20shapes%2C%20sizes%2C%20and%20swim%20speeds%20to%0Adetermine%20if%20the%20physical%20constraints%20on%20biological%20swimmers%2C%20namely%20Brownian%0Amotion%2C%20lead%20to%20regions%20where%20reinforcement%20learners%27%20training%20fails.%20We%20find%0Athat%20the%20RL%20agents%20can%20perform%20chemotaxis%20as%20soon%20as%20it%20is%20physically%20possible%0Aand%2C%20in%20some%20cases%2C%20even%20before%20the%20active%20swimming%20overpowers%20the%20stochastic%0Aenvironment.%20We%20study%20the%20efficiency%20of%20the%20emergent%20policy%20and%20identify%0Aconvergence%20in%20agent%20size%20and%20swim%20speeds.%20Finally%2C%20we%20study%20the%20strategy%0Aadopted%20by%20the%20reinforcement%20learning%20algorithm%20to%20explain%20how%20the%20agents%0Aperform%20their%20tasks.%20To%20this%20end%2C%20we%20identify%20three%20emerging%20dominant%0Astrategies%20and%20several%20rare%20approaches%20taken.%20These%20strategies%2C%20whilst%0Aproducing%20almost%20identical%20trajectories%20in%20simulation%2C%20are%20distinct%20and%20give%0Ainsight%20into%20the%20possible%20mechanisms%20behind%20which%20biological%20agents%20explore%0Atheir%20environment%20and%20respond%20to%20changing%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01999v1&entry.124074799=Read"},
{"title": "Quadratic Programming-based Reference Spreading Control for Dual-Arm\n  Robotic Manipulation with Planned Simultaneous Impacts", "author": "Jari van Steen and Gijs van den Brandt and Nathan van de Wouw and Jens Kober and Alessandro Saccon", "abstract": "  With the aim of further enabling the exploitation of intentional impacts in\nrobotic manipulation, a control framework is presented that directly tackles\nthe challenges posed by tracking control of robotic manipulators that are\ntasked to perform nominally simultaneous impacts. This framework is an\nextension of the reference spreading control framework, in which overlapping\nante- and post-impact references that are consistent with impact dynamics are\ndefined. In this work, such a reference is constructed starting from a\nteleoperation-based approach. By using the corresponding ante- and post-impact\ncontrol modes in the scope of a quadratic programming control approach, peaking\nof the velocity error and control inputs due to impacts is avoided while\nmaintaining high tracking performance. With the inclusion of a novel interim\nmode, we aim to also avoid input peaks and steps when uncertainty in the\nenvironment causes a series of unplanned single impacts to occur rather than\nthe planned simultaneous impact. This work in particular presents for the first\ntime an experimental evaluation of reference spreading control on a robotic\nsetup, showcasing its robustness against uncertainty in the environment\ncompared to three baseline control approaches.\n", "link": "http://arxiv.org/abs/2305.08643v3", "date": "2024-04-02", "relevancy": 1.5407, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.522}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5052}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Quadratic%20Programming-based%20Reference%20Spreading%20Control%20for%20Dual-Arm%0A%20%20Robotic%20Manipulation%20with%20Planned%20Simultaneous%20Impacts&body=Title%3A%20Quadratic%20Programming-based%20Reference%20Spreading%20Control%20for%20Dual-Arm%0A%20%20Robotic%20Manipulation%20with%20Planned%20Simultaneous%20Impacts%0AAuthor%3A%20Jari%20van%20Steen%20and%20Gijs%20van%20den%20Brandt%20and%20Nathan%20van%20de%20Wouw%20and%20Jens%20Kober%20and%20Alessandro%20Saccon%0AAbstract%3A%20%20%20With%20the%20aim%20of%20further%20enabling%20the%20exploitation%20of%20intentional%20impacts%20in%0Arobotic%20manipulation%2C%20a%20control%20framework%20is%20presented%20that%20directly%20tackles%0Athe%20challenges%20posed%20by%20tracking%20control%20of%20robotic%20manipulators%20that%20are%0Atasked%20to%20perform%20nominally%20simultaneous%20impacts.%20This%20framework%20is%20an%0Aextension%20of%20the%20reference%20spreading%20control%20framework%2C%20in%20which%20overlapping%0Aante-%20and%20post-impact%20references%20that%20are%20consistent%20with%20impact%20dynamics%20are%0Adefined.%20In%20this%20work%2C%20such%20a%20reference%20is%20constructed%20starting%20from%20a%0Ateleoperation-based%20approach.%20By%20using%20the%20corresponding%20ante-%20and%20post-impact%0Acontrol%20modes%20in%20the%20scope%20of%20a%20quadratic%20programming%20control%20approach%2C%20peaking%0Aof%20the%20velocity%20error%20and%20control%20inputs%20due%20to%20impacts%20is%20avoided%20while%0Amaintaining%20high%20tracking%20performance.%20With%20the%20inclusion%20of%20a%20novel%20interim%0Amode%2C%20we%20aim%20to%20also%20avoid%20input%20peaks%20and%20steps%20when%20uncertainty%20in%20the%0Aenvironment%20causes%20a%20series%20of%20unplanned%20single%20impacts%20to%20occur%20rather%20than%0Athe%20planned%20simultaneous%20impact.%20This%20work%20in%20particular%20presents%20for%20the%20first%0Atime%20an%20experimental%20evaluation%20of%20reference%20spreading%20control%20on%20a%20robotic%0Asetup%2C%20showcasing%20its%20robustness%20against%20uncertainty%20in%20the%20environment%0Acompared%20to%20three%20baseline%20control%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.08643v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quadratic%20Programming-based%20Reference%20Spreading%20Control%20for%20Dual-Arm%0A%20%20Robotic%20Manipulation%20with%20Planned%20Simultaneous%20Impacts&entry.906535625=Jari%20van%20Steen%20and%20Gijs%20van%20den%20Brandt%20and%20Nathan%20van%20de%20Wouw%20and%20Jens%20Kober%20and%20Alessandro%20Saccon&entry.1292438233=%20%20With%20the%20aim%20of%20further%20enabling%20the%20exploitation%20of%20intentional%20impacts%20in%0Arobotic%20manipulation%2C%20a%20control%20framework%20is%20presented%20that%20directly%20tackles%0Athe%20challenges%20posed%20by%20tracking%20control%20of%20robotic%20manipulators%20that%20are%0Atasked%20to%20perform%20nominally%20simultaneous%20impacts.%20This%20framework%20is%20an%0Aextension%20of%20the%20reference%20spreading%20control%20framework%2C%20in%20which%20overlapping%0Aante-%20and%20post-impact%20references%20that%20are%20consistent%20with%20impact%20dynamics%20are%0Adefined.%20In%20this%20work%2C%20such%20a%20reference%20is%20constructed%20starting%20from%20a%0Ateleoperation-based%20approach.%20By%20using%20the%20corresponding%20ante-%20and%20post-impact%0Acontrol%20modes%20in%20the%20scope%20of%20a%20quadratic%20programming%20control%20approach%2C%20peaking%0Aof%20the%20velocity%20error%20and%20control%20inputs%20due%20to%20impacts%20is%20avoided%20while%0Amaintaining%20high%20tracking%20performance.%20With%20the%20inclusion%20of%20a%20novel%20interim%0Amode%2C%20we%20aim%20to%20also%20avoid%20input%20peaks%20and%20steps%20when%20uncertainty%20in%20the%0Aenvironment%20causes%20a%20series%20of%20unplanned%20single%20impacts%20to%20occur%20rather%20than%0Athe%20planned%20simultaneous%20impact.%20This%20work%20in%20particular%20presents%20for%20the%20first%0Atime%20an%20experimental%20evaluation%20of%20reference%20spreading%20control%20on%20a%20robotic%0Asetup%2C%20showcasing%20its%20robustness%20against%20uncertainty%20in%20the%20environment%0Acompared%20to%20three%20baseline%20control%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.08643v3&entry.124074799=Read"},
{"title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic\n  Representations", "author": "Deqing Fu and Ghazal Khalighinejad and Ollie Liu and Bhuwan Dhingra and Dani Yogatama and Robin Jia and Willie Neiswanger", "abstract": "  Current foundation models exhibit impressive capabilities when prompted\neither with text only or with both image and text inputs. But do their\ncapabilities change depending on the input modality? In this work, we propose\n$\\textbf{IsoBench}$, a benchmark dataset containing problems from four major\nareas: math, science, algorithms, and games. Each example is presented with\nmultiple $\\textbf{isomorphic representations}$ of inputs, such as visual,\ntextual, and mathematical presentations. IsoBench provides fine-grained\nfeedback to diagnose performance gaps caused by the form of the representation.\nAcross various foundation models, we observe that on the same problem, models\nhave a consistent preference towards textual representations. Most prominently,\nwhen evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points\nworse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7\npoints worse and Gemini Pro is 14.9 points worse. Finally, we present two\nprompting techniques, $\\textit{IsoCombination}$ and $\\textit{IsoScratchPad}$,\nwhich improve model performance by considering combinations of, and\ntranslations between, different input representations.\n", "link": "http://arxiv.org/abs/2404.01266v2", "date": "2024-04-02", "relevancy": 1.5315, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5154}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5115}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5033}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IsoBench%3A%20Benchmarking%20Multimodal%20Foundation%20Models%20on%20Isomorphic%0A%20%20Representations&body=Title%3A%20IsoBench%3A%20Benchmarking%20Multimodal%20Foundation%20Models%20on%20Isomorphic%0A%20%20Representations%0AAuthor%3A%20Deqing%20Fu%20and%20Ghazal%20Khalighinejad%20and%20Ollie%20Liu%20and%20Bhuwan%20Dhingra%20and%20Dani%20Yogatama%20and%20Robin%20Jia%20and%20Willie%20Neiswanger%0AAbstract%3A%20%20%20Current%20foundation%20models%20exhibit%20impressive%20capabilities%20when%20prompted%0Aeither%20with%20text%20only%20or%20with%20both%20image%20and%20text%20inputs.%20But%20do%20their%0Acapabilities%20change%20depending%20on%20the%20input%20modality%3F%20In%20this%20work%2C%20we%20propose%0A%24%5Ctextbf%7BIsoBench%7D%24%2C%20a%20benchmark%20dataset%20containing%20problems%20from%20four%20major%0Aareas%3A%20math%2C%20science%2C%20algorithms%2C%20and%20games.%20Each%20example%20is%20presented%20with%0Amultiple%20%24%5Ctextbf%7Bisomorphic%20representations%7D%24%20of%20inputs%2C%20such%20as%20visual%2C%0Atextual%2C%20and%20mathematical%20presentations.%20IsoBench%20provides%20fine-grained%0Afeedback%20to%20diagnose%20performance%20gaps%20caused%20by%20the%20form%20of%20the%20representation.%0AAcross%20various%20foundation%20models%2C%20we%20observe%20that%20on%20the%20same%20problem%2C%20models%0Ahave%20a%20consistent%20preference%20towards%20textual%20representations.%20Most%20prominently%2C%0Awhen%20evaluated%20on%20all%20IsoBench%20problems%2C%20Claude-3%20Opus%20performs%2028.7%20points%0Aworse%20when%20provided%20with%20images%20instead%20of%20text%3B%20similarly%2C%20GPT-4%20Turbo%20is%2018.7%0Apoints%20worse%20and%20Gemini%20Pro%20is%2014.9%20points%20worse.%20Finally%2C%20we%20present%20two%0Aprompting%20techniques%2C%20%24%5Ctextit%7BIsoCombination%7D%24%20and%20%24%5Ctextit%7BIsoScratchPad%7D%24%2C%0Awhich%20improve%20model%20performance%20by%20considering%20combinations%20of%2C%20and%0Atranslations%20between%2C%20different%20input%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01266v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IsoBench%3A%20Benchmarking%20Multimodal%20Foundation%20Models%20on%20Isomorphic%0A%20%20Representations&entry.906535625=Deqing%20Fu%20and%20Ghazal%20Khalighinejad%20and%20Ollie%20Liu%20and%20Bhuwan%20Dhingra%20and%20Dani%20Yogatama%20and%20Robin%20Jia%20and%20Willie%20Neiswanger&entry.1292438233=%20%20Current%20foundation%20models%20exhibit%20impressive%20capabilities%20when%20prompted%0Aeither%20with%20text%20only%20or%20with%20both%20image%20and%20text%20inputs.%20But%20do%20their%0Acapabilities%20change%20depending%20on%20the%20input%20modality%3F%20In%20this%20work%2C%20we%20propose%0A%24%5Ctextbf%7BIsoBench%7D%24%2C%20a%20benchmark%20dataset%20containing%20problems%20from%20four%20major%0Aareas%3A%20math%2C%20science%2C%20algorithms%2C%20and%20games.%20Each%20example%20is%20presented%20with%0Amultiple%20%24%5Ctextbf%7Bisomorphic%20representations%7D%24%20of%20inputs%2C%20such%20as%20visual%2C%0Atextual%2C%20and%20mathematical%20presentations.%20IsoBench%20provides%20fine-grained%0Afeedback%20to%20diagnose%20performance%20gaps%20caused%20by%20the%20form%20of%20the%20representation.%0AAcross%20various%20foundation%20models%2C%20we%20observe%20that%20on%20the%20same%20problem%2C%20models%0Ahave%20a%20consistent%20preference%20towards%20textual%20representations.%20Most%20prominently%2C%0Awhen%20evaluated%20on%20all%20IsoBench%20problems%2C%20Claude-3%20Opus%20performs%2028.7%20points%0Aworse%20when%20provided%20with%20images%20instead%20of%20text%3B%20similarly%2C%20GPT-4%20Turbo%20is%2018.7%0Apoints%20worse%20and%20Gemini%20Pro%20is%2014.9%20points%20worse.%20Finally%2C%20we%20present%20two%0Aprompting%20techniques%2C%20%24%5Ctextit%7BIsoCombination%7D%24%20and%20%24%5Ctextit%7BIsoScratchPad%7D%24%2C%0Awhich%20improve%20model%20performance%20by%20considering%20combinations%20of%2C%20and%0Atranslations%20between%2C%20different%20input%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01266v2&entry.124074799=Read"},
{"title": "Red-Teaming Segment Anything Model", "author": "Krzysztof Jankowski and Bartlomiej Sobieski and Mateusz Kwiatkowski and Jakub Szulc and Michal Janik and Hubert Baniecki and Przemyslaw Biecek", "abstract": "  Foundation models have emerged as pivotal tools, tackling many complex tasks\nthrough pre-training on vast datasets and subsequent fine-tuning for specific\napplications. The Segment Anything Model is one of the first and most\nwell-known foundation models for computer vision segmentation tasks. This work\npresents a multi-faceted red-teaming analysis that tests the Segment Anything\nModel against challenging tasks: (1) We analyze the impact of style transfer on\nsegmentation masks, demonstrating that applying adverse weather conditions and\nraindrops to dashboard images of city roads significantly distorts generated\nmasks. (2) We focus on assessing whether the model can be used for attacks on\nprivacy, such as recognizing celebrities' faces, and show that the model\npossesses some undesired knowledge in this task. (3) Finally, we check how\nrobust the model is to adversarial attacks on segmentation masks under text\nprompts. We not only show the effectiveness of popular white-box attacks and\nresistance to black-box attacks but also introduce a novel approach - Focused\nIterative Gradient Attack (FIGA) that combines white-box approaches to\nconstruct an efficient attack resulting in a smaller number of modified pixels.\nAll of our testing methods and analyses indicate a need for enhanced safety\nmeasures in foundation models for image segmentation.\n", "link": "http://arxiv.org/abs/2404.02067v1", "date": "2024-04-02", "relevancy": 1.5307, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5196}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5076}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Red-Teaming%20Segment%20Anything%20Model&body=Title%3A%20Red-Teaming%20Segment%20Anything%20Model%0AAuthor%3A%20Krzysztof%20Jankowski%20and%20Bartlomiej%20Sobieski%20and%20Mateusz%20Kwiatkowski%20and%20Jakub%20Szulc%20and%20Michal%20Janik%20and%20Hubert%20Baniecki%20and%20Przemyslaw%20Biecek%0AAbstract%3A%20%20%20Foundation%20models%20have%20emerged%20as%20pivotal%20tools%2C%20tackling%20many%20complex%20tasks%0Athrough%20pre-training%20on%20vast%20datasets%20and%20subsequent%20fine-tuning%20for%20specific%0Aapplications.%20The%20Segment%20Anything%20Model%20is%20one%20of%20the%20first%20and%20most%0Awell-known%20foundation%20models%20for%20computer%20vision%20segmentation%20tasks.%20This%20work%0Apresents%20a%20multi-faceted%20red-teaming%20analysis%20that%20tests%20the%20Segment%20Anything%0AModel%20against%20challenging%20tasks%3A%20%281%29%20We%20analyze%20the%20impact%20of%20style%20transfer%20on%0Asegmentation%20masks%2C%20demonstrating%20that%20applying%20adverse%20weather%20conditions%20and%0Araindrops%20to%20dashboard%20images%20of%20city%20roads%20significantly%20distorts%20generated%0Amasks.%20%282%29%20We%20focus%20on%20assessing%20whether%20the%20model%20can%20be%20used%20for%20attacks%20on%0Aprivacy%2C%20such%20as%20recognizing%20celebrities%27%20faces%2C%20and%20show%20that%20the%20model%0Apossesses%20some%20undesired%20knowledge%20in%20this%20task.%20%283%29%20Finally%2C%20we%20check%20how%0Arobust%20the%20model%20is%20to%20adversarial%20attacks%20on%20segmentation%20masks%20under%20text%0Aprompts.%20We%20not%20only%20show%20the%20effectiveness%20of%20popular%20white-box%20attacks%20and%0Aresistance%20to%20black-box%20attacks%20but%20also%20introduce%20a%20novel%20approach%20-%20Focused%0AIterative%20Gradient%20Attack%20%28FIGA%29%20that%20combines%20white-box%20approaches%20to%0Aconstruct%20an%20efficient%20attack%20resulting%20in%20a%20smaller%20number%20of%20modified%20pixels.%0AAll%20of%20our%20testing%20methods%20and%20analyses%20indicate%20a%20need%20for%20enhanced%20safety%0Ameasures%20in%20foundation%20models%20for%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02067v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Red-Teaming%20Segment%20Anything%20Model&entry.906535625=Krzysztof%20Jankowski%20and%20Bartlomiej%20Sobieski%20and%20Mateusz%20Kwiatkowski%20and%20Jakub%20Szulc%20and%20Michal%20Janik%20and%20Hubert%20Baniecki%20and%20Przemyslaw%20Biecek&entry.1292438233=%20%20Foundation%20models%20have%20emerged%20as%20pivotal%20tools%2C%20tackling%20many%20complex%20tasks%0Athrough%20pre-training%20on%20vast%20datasets%20and%20subsequent%20fine-tuning%20for%20specific%0Aapplications.%20The%20Segment%20Anything%20Model%20is%20one%20of%20the%20first%20and%20most%0Awell-known%20foundation%20models%20for%20computer%20vision%20segmentation%20tasks.%20This%20work%0Apresents%20a%20multi-faceted%20red-teaming%20analysis%20that%20tests%20the%20Segment%20Anything%0AModel%20against%20challenging%20tasks%3A%20%281%29%20We%20analyze%20the%20impact%20of%20style%20transfer%20on%0Asegmentation%20masks%2C%20demonstrating%20that%20applying%20adverse%20weather%20conditions%20and%0Araindrops%20to%20dashboard%20images%20of%20city%20roads%20significantly%20distorts%20generated%0Amasks.%20%282%29%20We%20focus%20on%20assessing%20whether%20the%20model%20can%20be%20used%20for%20attacks%20on%0Aprivacy%2C%20such%20as%20recognizing%20celebrities%27%20faces%2C%20and%20show%20that%20the%20model%0Apossesses%20some%20undesired%20knowledge%20in%20this%20task.%20%283%29%20Finally%2C%20we%20check%20how%0Arobust%20the%20model%20is%20to%20adversarial%20attacks%20on%20segmentation%20masks%20under%20text%0Aprompts.%20We%20not%20only%20show%20the%20effectiveness%20of%20popular%20white-box%20attacks%20and%0Aresistance%20to%20black-box%20attacks%20but%20also%20introduce%20a%20novel%20approach%20-%20Focused%0AIterative%20Gradient%20Attack%20%28FIGA%29%20that%20combines%20white-box%20approaches%20to%0Aconstruct%20an%20efficient%20attack%20resulting%20in%20a%20smaller%20number%20of%20modified%20pixels.%0AAll%20of%20our%20testing%20methods%20and%20analyses%20indicate%20a%20need%20for%20enhanced%20safety%0Ameasures%20in%20foundation%20models%20for%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02067v1&entry.124074799=Read"},
{"title": "MedMamba: Vision Mamba for Medical Image Classification", "author": "Yubiao Yue and Zhenzhang Li", "abstract": "  Medical image classification is a very fundamental and crucial task in the\nfield of computer vision. These years, CNN-based and Transformer-based models\nhave been widely used to classify various medical images. Unfortunately, The\nlimitation of CNNs in long-range modeling capabilities prevents them from\neffectively extracting features in medical images, while Transformers are\nhampered by their quadratic computational complexity. Recent research has shown\nthat the state space model (SSM) represented by Mamba can efficiently model\nlong-range interactions while maintaining linear computational complexity.\nInspired by this, we propose Vision Mamba for medical image classification\n(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSM\ncombines the local feature extraction ability of convolutional layers with the\nability of SSM to capture long-range dependency, thereby modeling medical\nimages with different modalities. To demonstrate the potential of MedMamba, we\nconducted extensive experiments using 14 publicly available medical datasets\nwith different imaging techniques and two private datasets built by ourselves.\nExtensive experimental results demonstrate that the proposed MedMamba performs\nwell in detecting lesions in various medical images. To the best of our\nknowledge, this is the first Vision Mamba tailored for medical image\nclassification. The purpose of this work is to establish a new baseline for\nmedical image classification tasks and provide valuable insights for the future\ndevelopment of more efficient and effective SSM-based artificial intelligence\nalgorithms and application systems in the medical. Source code has been\navailable at https://github.com/YubiaoYue/MedMamba.\n", "link": "http://arxiv.org/abs/2403.03849v3", "date": "2024-04-02", "relevancy": 1.5232, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5183}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MedMamba%3A%20Vision%20Mamba%20for%20Medical%20Image%20Classification&body=Title%3A%20MedMamba%3A%20Vision%20Mamba%20for%20Medical%20Image%20Classification%0AAuthor%3A%20Yubiao%20Yue%20and%20Zhenzhang%20Li%0AAbstract%3A%20%20%20Medical%20image%20classification%20is%20a%20very%20fundamental%20and%20crucial%20task%20in%20the%0Afield%20of%20computer%20vision.%20These%20years%2C%20CNN-based%20and%20Transformer-based%20models%0Ahave%20been%20widely%20used%20to%20classify%20various%20medical%20images.%20Unfortunately%2C%20The%0Alimitation%20of%20CNNs%20in%20long-range%20modeling%20capabilities%20prevents%20them%20from%0Aeffectively%20extracting%20features%20in%20medical%20images%2C%20while%20Transformers%20are%0Ahampered%20by%20their%20quadratic%20computational%20complexity.%20Recent%20research%20has%20shown%0Athat%20the%20state%20space%20model%20%28SSM%29%20represented%20by%20Mamba%20can%20efficiently%20model%0Along-range%20interactions%20while%20maintaining%20linear%20computational%20complexity.%0AInspired%20by%20this%2C%20we%20propose%20Vision%20Mamba%20for%20medical%20image%20classification%0A%28MedMamba%29.%20More%20specifically%2C%20we%20introduce%20a%20novel%20Conv-SSM%20module.%20Conv-SSM%0Acombines%20the%20local%20feature%20extraction%20ability%20of%20convolutional%20layers%20with%20the%0Aability%20of%20SSM%20to%20capture%20long-range%20dependency%2C%20thereby%20modeling%20medical%0Aimages%20with%20different%20modalities.%20To%20demonstrate%20the%20potential%20of%20MedMamba%2C%20we%0Aconducted%20extensive%20experiments%20using%2014%20publicly%20available%20medical%20datasets%0Awith%20different%20imaging%20techniques%20and%20two%20private%20datasets%20built%20by%20ourselves.%0AExtensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20MedMamba%20performs%0Awell%20in%20detecting%20lesions%20in%20various%20medical%20images.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20Vision%20Mamba%20tailored%20for%20medical%20image%0Aclassification.%20The%20purpose%20of%20this%20work%20is%20to%20establish%20a%20new%20baseline%20for%0Amedical%20image%20classification%20tasks%20and%20provide%20valuable%20insights%20for%20the%20future%0Adevelopment%20of%20more%20efficient%20and%20effective%20SSM-based%20artificial%20intelligence%0Aalgorithms%20and%20application%20systems%20in%20the%20medical.%20Source%20code%20has%20been%0Aavailable%20at%20https%3A//github.com/YubiaoYue/MedMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03849v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedMamba%3A%20Vision%20Mamba%20for%20Medical%20Image%20Classification&entry.906535625=Yubiao%20Yue%20and%20Zhenzhang%20Li&entry.1292438233=%20%20Medical%20image%20classification%20is%20a%20very%20fundamental%20and%20crucial%20task%20in%20the%0Afield%20of%20computer%20vision.%20These%20years%2C%20CNN-based%20and%20Transformer-based%20models%0Ahave%20been%20widely%20used%20to%20classify%20various%20medical%20images.%20Unfortunately%2C%20The%0Alimitation%20of%20CNNs%20in%20long-range%20modeling%20capabilities%20prevents%20them%20from%0Aeffectively%20extracting%20features%20in%20medical%20images%2C%20while%20Transformers%20are%0Ahampered%20by%20their%20quadratic%20computational%20complexity.%20Recent%20research%20has%20shown%0Athat%20the%20state%20space%20model%20%28SSM%29%20represented%20by%20Mamba%20can%20efficiently%20model%0Along-range%20interactions%20while%20maintaining%20linear%20computational%20complexity.%0AInspired%20by%20this%2C%20we%20propose%20Vision%20Mamba%20for%20medical%20image%20classification%0A%28MedMamba%29.%20More%20specifically%2C%20we%20introduce%20a%20novel%20Conv-SSM%20module.%20Conv-SSM%0Acombines%20the%20local%20feature%20extraction%20ability%20of%20convolutional%20layers%20with%20the%0Aability%20of%20SSM%20to%20capture%20long-range%20dependency%2C%20thereby%20modeling%20medical%0Aimages%20with%20different%20modalities.%20To%20demonstrate%20the%20potential%20of%20MedMamba%2C%20we%0Aconducted%20extensive%20experiments%20using%2014%20publicly%20available%20medical%20datasets%0Awith%20different%20imaging%20techniques%20and%20two%20private%20datasets%20built%20by%20ourselves.%0AExtensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20MedMamba%20performs%0Awell%20in%20detecting%20lesions%20in%20various%20medical%20images.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20Vision%20Mamba%20tailored%20for%20medical%20image%0Aclassification.%20The%20purpose%20of%20this%20work%20is%20to%20establish%20a%20new%20baseline%20for%0Amedical%20image%20classification%20tasks%20and%20provide%20valuable%20insights%20for%20the%20future%0Adevelopment%20of%20more%20efficient%20and%20effective%20SSM-based%20artificial%20intelligence%0Aalgorithms%20and%20application%20systems%20in%20the%20medical.%20Source%20code%20has%20been%0Aavailable%20at%20https%3A//github.com/YubiaoYue/MedMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03849v3&entry.124074799=Read"},
{"title": "Rephrase, Augment, Reason: Visual Grounding of Questions for\n  Vision-Language Models", "author": "Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal", "abstract": "  An increasing number of vision-language tasks can be handled with little to\nno training, i.e., in a zero and few-shot manner, by marrying large language\nmodels (LLMs) to vision encoders, resulting in large vision-language models\n(LVLMs). While this has huge upsides, such as not requiring training data or\ncustom architectures, how an input is presented to an LVLM can have a major\nimpact on zero-shot model performance. In particular, inputs phrased in an\nunderspecified way can result in incorrect answers due to factors like missing\nvisual information, complex implicit reasoning, or linguistic ambiguity.\nTherefore, adding visually-grounded information to the input as a preemptive\nclarification should improve model performance by reducing underspecification,\ne.g., by localizing objects and disambiguating references. Similarly, in the\nVQA setting, changing the way questions are framed can make them easier for\nmodels to answer. To this end, we present Rephrase, Augment and Reason\n(RepARe), a gradient-free framework that extracts salient details about the\nimage using the underlying LVLM as a captioner and reasoner, in order to\npropose modifications to the original question. We then use the LVLM's\nconfidence over a generated answer as an unsupervised scoring function to\nselect the rephrased question most likely to improve zero-shot performance.\nFocusing on three visual question answering tasks, we show that RepARe can\nresult in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%,\nand 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we\nfind that using gold answers for oracle question candidate selection achieves a\nsubstantial gain in VQA accuracy by up to 14.41%. Through extensive analysis,\nwe demonstrate that outputs from RepARe increase syntactic complexity, and\neffectively utilize vision-language interaction and the frozen LLM.\n", "link": "http://arxiv.org/abs/2310.05861v2", "date": "2024-04-02", "relevancy": 1.5146, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5181}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4828}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rephrase%2C%20Augment%2C%20Reason%3A%20Visual%20Grounding%20of%20Questions%20for%0A%20%20Vision-Language%20Models&body=Title%3A%20Rephrase%2C%20Augment%2C%20Reason%3A%20Visual%20Grounding%20of%20Questions%20for%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20An%20increasing%20number%20of%20vision-language%20tasks%20can%20be%20handled%20with%20little%20to%0Ano%20training%2C%20i.e.%2C%20in%20a%20zero%20and%20few-shot%20manner%2C%20by%20marrying%20large%20language%0Amodels%20%28LLMs%29%20to%20vision%20encoders%2C%20resulting%20in%20large%20vision-language%20models%0A%28LVLMs%29.%20While%20this%20has%20huge%20upsides%2C%20such%20as%20not%20requiring%20training%20data%20or%0Acustom%20architectures%2C%20how%20an%20input%20is%20presented%20to%20an%20LVLM%20can%20have%20a%20major%0Aimpact%20on%20zero-shot%20model%20performance.%20In%20particular%2C%20inputs%20phrased%20in%20an%0Aunderspecified%20way%20can%20result%20in%20incorrect%20answers%20due%20to%20factors%20like%20missing%0Avisual%20information%2C%20complex%20implicit%20reasoning%2C%20or%20linguistic%20ambiguity.%0ATherefore%2C%20adding%20visually-grounded%20information%20to%20the%20input%20as%20a%20preemptive%0Aclarification%20should%20improve%20model%20performance%20by%20reducing%20underspecification%2C%0Ae.g.%2C%20by%20localizing%20objects%20and%20disambiguating%20references.%20Similarly%2C%20in%20the%0AVQA%20setting%2C%20changing%20the%20way%20questions%20are%20framed%20can%20make%20them%20easier%20for%0Amodels%20to%20answer.%20To%20this%20end%2C%20we%20present%20Rephrase%2C%20Augment%20and%20Reason%0A%28RepARe%29%2C%20a%20gradient-free%20framework%20that%20extracts%20salient%20details%20about%20the%0Aimage%20using%20the%20underlying%20LVLM%20as%20a%20captioner%20and%20reasoner%2C%20in%20order%20to%0Apropose%20modifications%20to%20the%20original%20question.%20We%20then%20use%20the%20LVLM%27s%0Aconfidence%20over%20a%20generated%20answer%20as%20an%20unsupervised%20scoring%20function%20to%0Aselect%20the%20rephrased%20question%20most%20likely%20to%20improve%20zero-shot%20performance.%0AFocusing%20on%20three%20visual%20question%20answering%20tasks%2C%20we%20show%20that%20RepARe%20can%0Aresult%20in%20a%203.85%25%20%28absolute%29%20increase%20in%20zero-shot%20accuracy%20on%20VQAv2%2C%206.41%25%2C%0Aand%207.94%25%20points%20increase%20on%20A-OKVQA%2C%20and%20VizWiz%20respectively.%20Additionally%2C%20we%0Afind%20that%20using%20gold%20answers%20for%20oracle%20question%20candidate%20selection%20achieves%20a%0Asubstantial%20gain%20in%20VQA%20accuracy%20by%20up%20to%2014.41%25.%20Through%20extensive%20analysis%2C%0Awe%20demonstrate%20that%20outputs%20from%20RepARe%20increase%20syntactic%20complexity%2C%20and%0Aeffectively%20utilize%20vision-language%20interaction%20and%20the%20frozen%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05861v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rephrase%2C%20Augment%2C%20Reason%3A%20Visual%20Grounding%20of%20Questions%20for%0A%20%20Vision-Language%20Models&entry.906535625=Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal&entry.1292438233=%20%20An%20increasing%20number%20of%20vision-language%20tasks%20can%20be%20handled%20with%20little%20to%0Ano%20training%2C%20i.e.%2C%20in%20a%20zero%20and%20few-shot%20manner%2C%20by%20marrying%20large%20language%0Amodels%20%28LLMs%29%20to%20vision%20encoders%2C%20resulting%20in%20large%20vision-language%20models%0A%28LVLMs%29.%20While%20this%20has%20huge%20upsides%2C%20such%20as%20not%20requiring%20training%20data%20or%0Acustom%20architectures%2C%20how%20an%20input%20is%20presented%20to%20an%20LVLM%20can%20have%20a%20major%0Aimpact%20on%20zero-shot%20model%20performance.%20In%20particular%2C%20inputs%20phrased%20in%20an%0Aunderspecified%20way%20can%20result%20in%20incorrect%20answers%20due%20to%20factors%20like%20missing%0Avisual%20information%2C%20complex%20implicit%20reasoning%2C%20or%20linguistic%20ambiguity.%0ATherefore%2C%20adding%20visually-grounded%20information%20to%20the%20input%20as%20a%20preemptive%0Aclarification%20should%20improve%20model%20performance%20by%20reducing%20underspecification%2C%0Ae.g.%2C%20by%20localizing%20objects%20and%20disambiguating%20references.%20Similarly%2C%20in%20the%0AVQA%20setting%2C%20changing%20the%20way%20questions%20are%20framed%20can%20make%20them%20easier%20for%0Amodels%20to%20answer.%20To%20this%20end%2C%20we%20present%20Rephrase%2C%20Augment%20and%20Reason%0A%28RepARe%29%2C%20a%20gradient-free%20framework%20that%20extracts%20salient%20details%20about%20the%0Aimage%20using%20the%20underlying%20LVLM%20as%20a%20captioner%20and%20reasoner%2C%20in%20order%20to%0Apropose%20modifications%20to%20the%20original%20question.%20We%20then%20use%20the%20LVLM%27s%0Aconfidence%20over%20a%20generated%20answer%20as%20an%20unsupervised%20scoring%20function%20to%0Aselect%20the%20rephrased%20question%20most%20likely%20to%20improve%20zero-shot%20performance.%0AFocusing%20on%20three%20visual%20question%20answering%20tasks%2C%20we%20show%20that%20RepARe%20can%0Aresult%20in%20a%203.85%25%20%28absolute%29%20increase%20in%20zero-shot%20accuracy%20on%20VQAv2%2C%206.41%25%2C%0Aand%207.94%25%20points%20increase%20on%20A-OKVQA%2C%20and%20VizWiz%20respectively.%20Additionally%2C%20we%0Afind%20that%20using%20gold%20answers%20for%20oracle%20question%20candidate%20selection%20achieves%20a%0Asubstantial%20gain%20in%20VQA%20accuracy%20by%20up%20to%2014.41%25.%20Through%20extensive%20analysis%2C%0Awe%20demonstrate%20that%20outputs%20from%20RepARe%20increase%20syntactic%20complexity%2C%20and%0Aeffectively%20utilize%20vision-language%20interaction%20and%20the%20frozen%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05861v2&entry.124074799=Read"},
{"title": "pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable\n  Generalizable 3D Reconstruction", "author": "David Charatan and Sizhe Li and Andrea Tagliasacchi and Vincent Sitzmann", "abstract": "  We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D\nradiance fields parameterized by 3D Gaussian primitives from pairs of images.\nOur model features real-time and memory-efficient rendering for scalable\ntraining as well as fast 3D reconstruction at inference time. To overcome local\nminima inherent to sparse and locally supported representations, we predict a\ndense probability distribution over 3D and sample Gaussian means from that\nprobability distribution. We make this sampling operation differentiable via a\nreparameterization trick, allowing us to back-propagate gradients through the\nGaussian splatting representation. We benchmark our method on wide-baseline\nnovel view synthesis on the real-world RealEstate10k and ACID datasets, where\nwe outperform state-of-the-art light field transformers and accelerate\nrendering by 2.5 orders of magnitude while reconstructing an interpretable and\neditable 3D radiance field.\n", "link": "http://arxiv.org/abs/2312.12337v3", "date": "2024-04-02", "relevancy": 1.5133, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5382}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5278}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4816}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20pixelSplat%3A%203D%20Gaussian%20Splats%20from%20Image%20Pairs%20for%20Scalable%0A%20%20Generalizable%203D%20Reconstruction&body=Title%3A%20pixelSplat%3A%203D%20Gaussian%20Splats%20from%20Image%20Pairs%20for%20Scalable%0A%20%20Generalizable%203D%20Reconstruction%0AAuthor%3A%20David%20Charatan%20and%20Sizhe%20Li%20and%20Andrea%20Tagliasacchi%20and%20Vincent%20Sitzmann%0AAbstract%3A%20%20%20We%20introduce%20pixelSplat%2C%20a%20feed-forward%20model%20that%20learns%20to%20reconstruct%203D%0Aradiance%20fields%20parameterized%20by%203D%20Gaussian%20primitives%20from%20pairs%20of%20images.%0AOur%20model%20features%20real-time%20and%20memory-efficient%20rendering%20for%20scalable%0Atraining%20as%20well%20as%20fast%203D%20reconstruction%20at%20inference%20time.%20To%20overcome%20local%0Aminima%20inherent%20to%20sparse%20and%20locally%20supported%20representations%2C%20we%20predict%20a%0Adense%20probability%20distribution%20over%203D%20and%20sample%20Gaussian%20means%20from%20that%0Aprobability%20distribution.%20We%20make%20this%20sampling%20operation%20differentiable%20via%20a%0Areparameterization%20trick%2C%20allowing%20us%20to%20back-propagate%20gradients%20through%20the%0AGaussian%20splatting%20representation.%20We%20benchmark%20our%20method%20on%20wide-baseline%0Anovel%20view%20synthesis%20on%20the%20real-world%20RealEstate10k%20and%20ACID%20datasets%2C%20where%0Awe%20outperform%20state-of-the-art%20light%20field%20transformers%20and%20accelerate%0Arendering%20by%202.5%20orders%20of%20magnitude%20while%20reconstructing%20an%20interpretable%20and%0Aeditable%203D%20radiance%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12337v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=pixelSplat%3A%203D%20Gaussian%20Splats%20from%20Image%20Pairs%20for%20Scalable%0A%20%20Generalizable%203D%20Reconstruction&entry.906535625=David%20Charatan%20and%20Sizhe%20Li%20and%20Andrea%20Tagliasacchi%20and%20Vincent%20Sitzmann&entry.1292438233=%20%20We%20introduce%20pixelSplat%2C%20a%20feed-forward%20model%20that%20learns%20to%20reconstruct%203D%0Aradiance%20fields%20parameterized%20by%203D%20Gaussian%20primitives%20from%20pairs%20of%20images.%0AOur%20model%20features%20real-time%20and%20memory-efficient%20rendering%20for%20scalable%0Atraining%20as%20well%20as%20fast%203D%20reconstruction%20at%20inference%20time.%20To%20overcome%20local%0Aminima%20inherent%20to%20sparse%20and%20locally%20supported%20representations%2C%20we%20predict%20a%0Adense%20probability%20distribution%20over%203D%20and%20sample%20Gaussian%20means%20from%20that%0Aprobability%20distribution.%20We%20make%20this%20sampling%20operation%20differentiable%20via%20a%0Areparameterization%20trick%2C%20allowing%20us%20to%20back-propagate%20gradients%20through%20the%0AGaussian%20splatting%20representation.%20We%20benchmark%20our%20method%20on%20wide-baseline%0Anovel%20view%20synthesis%20on%20the%20real-world%20RealEstate10k%20and%20ACID%20datasets%2C%20where%0Awe%20outperform%20state-of-the-art%20light%20field%20transformers%20and%20accelerate%0Arendering%20by%202.5%20orders%20of%20magnitude%20while%20reconstructing%20an%20interpretable%20and%0Aeditable%203D%20radiance%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12337v3&entry.124074799=Read"},
{"title": "Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL", "author": "Golnaz Mesbahi and Olya Mastikhina and Parham Mohammad Panahi and Martha White and Adam White", "abstract": "  In continual or lifelong reinforcement learning access to the environment\nshould be limited. If we aspire to design algorithms that can run for\nlong-periods of time, continually adapting to new, unexpected situations then\nwe must be willing to deploy our agents without tuning their hyperparameters\nover the agent's entire lifetime. The standard practice in deep RL -- and even\ncontinual RL -- is to assume unfettered access to deployment environment for\nthe full lifetime of the agent. This paper explores the notion that progress in\nlifelong RL research has been held back by inappropriate empirical\nmethodologies. In this paper we propose a new approach for tuning and\nevaluating lifelong RL agents where only one percent of the experiment data can\nbe used for hyperparameter tuning. We then conduct an empirical study of DQN\nand Soft Actor Critic across a variety of continuing and non-stationary\ndomains. We find both methods generally perform poorly when restricted to\none-percent tuning, whereas several algorithmic mitigations designed to\nmaintain network plasticity perform surprising well. In addition, we find that\nproperties designed to measure the network's ability to learn continually\nindeed correlate with performance under one-percent tuning.\n", "link": "http://arxiv.org/abs/2404.02113v1", "date": "2024-04-02", "relevancy": 1.5022, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5198}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4798}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.474}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tuning%20for%20the%20Unknown%3A%20Revisiting%20Evaluation%20Strategies%20for%20Lifelong%20RL&body=Title%3A%20Tuning%20for%20the%20Unknown%3A%20Revisiting%20Evaluation%20Strategies%20for%20Lifelong%20RL%0AAuthor%3A%20Golnaz%20Mesbahi%20and%20Olya%20Mastikhina%20and%20Parham%20Mohammad%20Panahi%20and%20Martha%20White%20and%20Adam%20White%0AAbstract%3A%20%20%20In%20continual%20or%20lifelong%20reinforcement%20learning%20access%20to%20the%20environment%0Ashould%20be%20limited.%20If%20we%20aspire%20to%20design%20algorithms%20that%20can%20run%20for%0Along-periods%20of%20time%2C%20continually%20adapting%20to%20new%2C%20unexpected%20situations%20then%0Awe%20must%20be%20willing%20to%20deploy%20our%20agents%20without%20tuning%20their%20hyperparameters%0Aover%20the%20agent%27s%20entire%20lifetime.%20The%20standard%20practice%20in%20deep%20RL%20--%20and%20even%0Acontinual%20RL%20--%20is%20to%20assume%20unfettered%20access%20to%20deployment%20environment%20for%0Athe%20full%20lifetime%20of%20the%20agent.%20This%20paper%20explores%20the%20notion%20that%20progress%20in%0Alifelong%20RL%20research%20has%20been%20held%20back%20by%20inappropriate%20empirical%0Amethodologies.%20In%20this%20paper%20we%20propose%20a%20new%20approach%20for%20tuning%20and%0Aevaluating%20lifelong%20RL%20agents%20where%20only%20one%20percent%20of%20the%20experiment%20data%20can%0Abe%20used%20for%20hyperparameter%20tuning.%20We%20then%20conduct%20an%20empirical%20study%20of%20DQN%0Aand%20Soft%20Actor%20Critic%20across%20a%20variety%20of%20continuing%20and%20non-stationary%0Adomains.%20We%20find%20both%20methods%20generally%20perform%20poorly%20when%20restricted%20to%0Aone-percent%20tuning%2C%20whereas%20several%20algorithmic%20mitigations%20designed%20to%0Amaintain%20network%20plasticity%20perform%20surprising%20well.%20In%20addition%2C%20we%20find%20that%0Aproperties%20designed%20to%20measure%20the%20network%27s%20ability%20to%20learn%20continually%0Aindeed%20correlate%20with%20performance%20under%20one-percent%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02113v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning%20for%20the%20Unknown%3A%20Revisiting%20Evaluation%20Strategies%20for%20Lifelong%20RL&entry.906535625=Golnaz%20Mesbahi%20and%20Olya%20Mastikhina%20and%20Parham%20Mohammad%20Panahi%20and%20Martha%20White%20and%20Adam%20White&entry.1292438233=%20%20In%20continual%20or%20lifelong%20reinforcement%20learning%20access%20to%20the%20environment%0Ashould%20be%20limited.%20If%20we%20aspire%20to%20design%20algorithms%20that%20can%20run%20for%0Along-periods%20of%20time%2C%20continually%20adapting%20to%20new%2C%20unexpected%20situations%20then%0Awe%20must%20be%20willing%20to%20deploy%20our%20agents%20without%20tuning%20their%20hyperparameters%0Aover%20the%20agent%27s%20entire%20lifetime.%20The%20standard%20practice%20in%20deep%20RL%20--%20and%20even%0Acontinual%20RL%20--%20is%20to%20assume%20unfettered%20access%20to%20deployment%20environment%20for%0Athe%20full%20lifetime%20of%20the%20agent.%20This%20paper%20explores%20the%20notion%20that%20progress%20in%0Alifelong%20RL%20research%20has%20been%20held%20back%20by%20inappropriate%20empirical%0Amethodologies.%20In%20this%20paper%20we%20propose%20a%20new%20approach%20for%20tuning%20and%0Aevaluating%20lifelong%20RL%20agents%20where%20only%20one%20percent%20of%20the%20experiment%20data%20can%0Abe%20used%20for%20hyperparameter%20tuning.%20We%20then%20conduct%20an%20empirical%20study%20of%20DQN%0Aand%20Soft%20Actor%20Critic%20across%20a%20variety%20of%20continuing%20and%20non-stationary%0Adomains.%20We%20find%20both%20methods%20generally%20perform%20poorly%20when%20restricted%20to%0Aone-percent%20tuning%2C%20whereas%20several%20algorithmic%20mitigations%20designed%20to%0Amaintain%20network%20plasticity%20perform%20surprising%20well.%20In%20addition%2C%20we%20find%20that%0Aproperties%20designed%20to%20measure%20the%20network%27s%20ability%20to%20learn%20continually%0Aindeed%20correlate%20with%20performance%20under%20one-percent%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02113v1&entry.124074799=Read"},
{"title": "Causality-based Transfer of Driving Scenarios to Unseen Intersections", "author": "Christoph Glasmacher and Michael Schuldes and Sleiman El Masri and Lutz Eckstein", "abstract": "  Scenario-based testing of automated driving functions has become a promising\nmethod to reduce time and cost compared to real-world testing. In\nscenario-based testing automated functions are evaluated in a set of\npre-defined scenarios. These scenarios provide information about vehicle\nbehaviors, environmental conditions, or road characteristics using parameters.\nTo create realistic scenarios, parameters and parameter dependencies have to be\nfitted utilizing real-world data. However, due to the large variety of\nintersections and movement constellations found in reality, data may not be\navailable for certain scenarios. This paper proposes a methodology to\nsystematically analyze relations between parameters of scenarios. Bayesian\nnetworks are utilized to analyze causal dependencies in order to decrease the\namount of required data and to transfer causal patterns creating unseen\nscenarios. Thereby, infrastructural influences on movement patterns are\ninvestigated to generate realistic scenarios on unobserved intersections. For\nevaluation, scenarios and underlying parameters are extracted from the inD\ndataset. Movement patterns are estimated, transferred and checked against\nrecorded data from those initially unseen intersections.\n", "link": "http://arxiv.org/abs/2404.02046v1", "date": "2024-04-02", "relevancy": 1.4998, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5426}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4897}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.483}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Causality-based%20Transfer%20of%20Driving%20Scenarios%20to%20Unseen%20Intersections&body=Title%3A%20Causality-based%20Transfer%20of%20Driving%20Scenarios%20to%20Unseen%20Intersections%0AAuthor%3A%20Christoph%20Glasmacher%20and%20Michael%20Schuldes%20and%20Sleiman%20El%20Masri%20and%20Lutz%20Eckstein%0AAbstract%3A%20%20%20Scenario-based%20testing%20of%20automated%20driving%20functions%20has%20become%20a%20promising%0Amethod%20to%20reduce%20time%20and%20cost%20compared%20to%20real-world%20testing.%20In%0Ascenario-based%20testing%20automated%20functions%20are%20evaluated%20in%20a%20set%20of%0Apre-defined%20scenarios.%20These%20scenarios%20provide%20information%20about%20vehicle%0Abehaviors%2C%20environmental%20conditions%2C%20or%20road%20characteristics%20using%20parameters.%0ATo%20create%20realistic%20scenarios%2C%20parameters%20and%20parameter%20dependencies%20have%20to%20be%0Afitted%20utilizing%20real-world%20data.%20However%2C%20due%20to%20the%20large%20variety%20of%0Aintersections%20and%20movement%20constellations%20found%20in%20reality%2C%20data%20may%20not%20be%0Aavailable%20for%20certain%20scenarios.%20This%20paper%20proposes%20a%20methodology%20to%0Asystematically%20analyze%20relations%20between%20parameters%20of%20scenarios.%20Bayesian%0Anetworks%20are%20utilized%20to%20analyze%20causal%20dependencies%20in%20order%20to%20decrease%20the%0Aamount%20of%20required%20data%20and%20to%20transfer%20causal%20patterns%20creating%20unseen%0Ascenarios.%20Thereby%2C%20infrastructural%20influences%20on%20movement%20patterns%20are%0Ainvestigated%20to%20generate%20realistic%20scenarios%20on%20unobserved%20intersections.%20For%0Aevaluation%2C%20scenarios%20and%20underlying%20parameters%20are%20extracted%20from%20the%20inD%0Adataset.%20Movement%20patterns%20are%20estimated%2C%20transferred%20and%20checked%20against%0Arecorded%20data%20from%20those%20initially%20unseen%20intersections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02046v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causality-based%20Transfer%20of%20Driving%20Scenarios%20to%20Unseen%20Intersections&entry.906535625=Christoph%20Glasmacher%20and%20Michael%20Schuldes%20and%20Sleiman%20El%20Masri%20and%20Lutz%20Eckstein&entry.1292438233=%20%20Scenario-based%20testing%20of%20automated%20driving%20functions%20has%20become%20a%20promising%0Amethod%20to%20reduce%20time%20and%20cost%20compared%20to%20real-world%20testing.%20In%0Ascenario-based%20testing%20automated%20functions%20are%20evaluated%20in%20a%20set%20of%0Apre-defined%20scenarios.%20These%20scenarios%20provide%20information%20about%20vehicle%0Abehaviors%2C%20environmental%20conditions%2C%20or%20road%20characteristics%20using%20parameters.%0ATo%20create%20realistic%20scenarios%2C%20parameters%20and%20parameter%20dependencies%20have%20to%20be%0Afitted%20utilizing%20real-world%20data.%20However%2C%20due%20to%20the%20large%20variety%20of%0Aintersections%20and%20movement%20constellations%20found%20in%20reality%2C%20data%20may%20not%20be%0Aavailable%20for%20certain%20scenarios.%20This%20paper%20proposes%20a%20methodology%20to%0Asystematically%20analyze%20relations%20between%20parameters%20of%20scenarios.%20Bayesian%0Anetworks%20are%20utilized%20to%20analyze%20causal%20dependencies%20in%20order%20to%20decrease%20the%0Aamount%20of%20required%20data%20and%20to%20transfer%20causal%20patterns%20creating%20unseen%0Ascenarios.%20Thereby%2C%20infrastructural%20influences%20on%20movement%20patterns%20are%0Ainvestigated%20to%20generate%20realistic%20scenarios%20on%20unobserved%20intersections.%20For%0Aevaluation%2C%20scenarios%20and%20underlying%20parameters%20are%20extracted%20from%20the%20inD%0Adataset.%20Movement%20patterns%20are%20estimated%2C%20transferred%20and%20checked%20against%0Arecorded%20data%20from%20those%20initially%20unseen%20intersections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02046v1&entry.124074799=Read"},
{"title": "MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in\n  Conversations with Multimodal Language Models", "author": "Zebang Cheng and Fuqiang Niu and Yuxiang Lin and Zhi-Qi Cheng and Bowen Zhang and Xiaojiang Peng", "abstract": "  This paper presents our winning submission to Subtask 2 of SemEval 2024 Task\n3 on multimodal emotion cause analysis in conversations. We propose a novel\nMultimodal Emotion Recognition and Multimodal Emotion Cause Extraction\n(MER-MCE) framework that integrates text, audio, and visual modalities using\nspecialized emotion encoders. Our approach sets itself apart from\ntop-performing teams by leveraging modality-specific features for enhanced\nemotion understanding and causality inference. Experimental evaluation\ndemonstrates the advantages of our multimodal approach, with our submission\nachieving a competitive weighted F1 score of 0.3435, ranking third with a\nmargin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team.\nProject: https://github.com/MIPS-COLT/MER-MCE.git\n", "link": "http://arxiv.org/abs/2404.00511v2", "date": "2024-04-02", "relevancy": 1.4965, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4964}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4769}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MIPS%20at%20SemEval-2024%20Task%203%3A%20Multimodal%20Emotion-Cause%20Pair%20Extraction%20in%0A%20%20Conversations%20with%20Multimodal%20Language%20Models&body=Title%3A%20MIPS%20at%20SemEval-2024%20Task%203%3A%20Multimodal%20Emotion-Cause%20Pair%20Extraction%20in%0A%20%20Conversations%20with%20Multimodal%20Language%20Models%0AAuthor%3A%20Zebang%20Cheng%20and%20Fuqiang%20Niu%20and%20Yuxiang%20Lin%20and%20Zhi-Qi%20Cheng%20and%20Bowen%20Zhang%20and%20Xiaojiang%20Peng%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20winning%20submission%20to%20Subtask%202%20of%20SemEval%202024%20Task%0A3%20on%20multimodal%20emotion%20cause%20analysis%20in%20conversations.%20We%20propose%20a%20novel%0AMultimodal%20Emotion%20Recognition%20and%20Multimodal%20Emotion%20Cause%20Extraction%0A%28MER-MCE%29%20framework%20that%20integrates%20text%2C%20audio%2C%20and%20visual%20modalities%20using%0Aspecialized%20emotion%20encoders.%20Our%20approach%20sets%20itself%20apart%20from%0Atop-performing%20teams%20by%20leveraging%20modality-specific%20features%20for%20enhanced%0Aemotion%20understanding%20and%20causality%20inference.%20Experimental%20evaluation%0Ademonstrates%20the%20advantages%20of%20our%20multimodal%20approach%2C%20with%20our%20submission%0Aachieving%20a%20competitive%20weighted%20F1%20score%20of%200.3435%2C%20ranking%20third%20with%20a%0Amargin%20of%20only%200.0339%20behind%20the%201st%20team%20and%200.0025%20behind%20the%202nd%20team.%0AProject%3A%20https%3A//github.com/MIPS-COLT/MER-MCE.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00511v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIPS%20at%20SemEval-2024%20Task%203%3A%20Multimodal%20Emotion-Cause%20Pair%20Extraction%20in%0A%20%20Conversations%20with%20Multimodal%20Language%20Models&entry.906535625=Zebang%20Cheng%20and%20Fuqiang%20Niu%20and%20Yuxiang%20Lin%20and%20Zhi-Qi%20Cheng%20and%20Bowen%20Zhang%20and%20Xiaojiang%20Peng&entry.1292438233=%20%20This%20paper%20presents%20our%20winning%20submission%20to%20Subtask%202%20of%20SemEval%202024%20Task%0A3%20on%20multimodal%20emotion%20cause%20analysis%20in%20conversations.%20We%20propose%20a%20novel%0AMultimodal%20Emotion%20Recognition%20and%20Multimodal%20Emotion%20Cause%20Extraction%0A%28MER-MCE%29%20framework%20that%20integrates%20text%2C%20audio%2C%20and%20visual%20modalities%20using%0Aspecialized%20emotion%20encoders.%20Our%20approach%20sets%20itself%20apart%20from%0Atop-performing%20teams%20by%20leveraging%20modality-specific%20features%20for%20enhanced%0Aemotion%20understanding%20and%20causality%20inference.%20Experimental%20evaluation%0Ademonstrates%20the%20advantages%20of%20our%20multimodal%20approach%2C%20with%20our%20submission%0Aachieving%20a%20competitive%20weighted%20F1%20score%20of%200.3435%2C%20ranking%20third%20with%20a%0Amargin%20of%20only%200.0339%20behind%20the%201st%20team%20and%200.0025%20behind%20the%202nd%20team.%0AProject%3A%20https%3A//github.com/MIPS-COLT/MER-MCE.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00511v2&entry.124074799=Read"},
{"title": "Neural Embedding Compression For Efficient Multi-Task Earth Observation\n  Modelling", "author": "Carlos Gomes and Thomas Brunschwiler", "abstract": "  As repositories of large scale data in earth observation (EO) have grown, so\nhave transfer and storage costs for model training and inference, expending\nsignificant resources. We introduce Neural Embedding Compression (NEC), based\non the transfer of compressed embeddings to data consumers instead of raw data.\nWe adapt foundation models (FM) through learned neural compression to generate\nmulti-task embeddings while navigating the tradeoff between compression rate\nand embedding utility. We update only a small fraction of the FM parameters\n(10%) for a short training period (1% of the iterations of pre-training). We\nevaluate NEC on two EO tasks: scene classification and semantic segmentation.\nCompared with applying traditional compression to the raw data, NEC achieves\nsimilar accuracy with a 75% to 90% reduction in data. Even at 99.7%\ncompression, performance drops by only 5% on the scene classification task.\nOverall, NEC is a data-efficient yet performant approach for multi-task EO\nmodelling.\n", "link": "http://arxiv.org/abs/2403.17886v2", "date": "2024-04-02", "relevancy": 1.4948, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5024}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5002}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4958}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Embedding%20Compression%20For%20Efficient%20Multi-Task%20Earth%20Observation%0A%20%20Modelling&body=Title%3A%20Neural%20Embedding%20Compression%20For%20Efficient%20Multi-Task%20Earth%20Observation%0A%20%20Modelling%0AAuthor%3A%20Carlos%20Gomes%20and%20Thomas%20Brunschwiler%0AAbstract%3A%20%20%20As%20repositories%20of%20large%20scale%20data%20in%20earth%20observation%20%28EO%29%20have%20grown%2C%20so%0Ahave%20transfer%20and%20storage%20costs%20for%20model%20training%20and%20inference%2C%20expending%0Asignificant%20resources.%20We%20introduce%20Neural%20Embedding%20Compression%20%28NEC%29%2C%20based%0Aon%20the%20transfer%20of%20compressed%20embeddings%20to%20data%20consumers%20instead%20of%20raw%20data.%0AWe%20adapt%20foundation%20models%20%28FM%29%20through%20learned%20neural%20compression%20to%20generate%0Amulti-task%20embeddings%20while%20navigating%20the%20tradeoff%20between%20compression%20rate%0Aand%20embedding%20utility.%20We%20update%20only%20a%20small%20fraction%20of%20the%20FM%20parameters%0A%2810%25%29%20for%20a%20short%20training%20period%20%281%25%20of%20the%20iterations%20of%20pre-training%29.%20We%0Aevaluate%20NEC%20on%20two%20EO%20tasks%3A%20scene%20classification%20and%20semantic%20segmentation.%0ACompared%20with%20applying%20traditional%20compression%20to%20the%20raw%20data%2C%20NEC%20achieves%0Asimilar%20accuracy%20with%20a%2075%25%20to%2090%25%20reduction%20in%20data.%20Even%20at%2099.7%25%0Acompression%2C%20performance%20drops%20by%20only%205%25%20on%20the%20scene%20classification%20task.%0AOverall%2C%20NEC%20is%20a%20data-efficient%20yet%20performant%20approach%20for%20multi-task%20EO%0Amodelling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17886v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Embedding%20Compression%20For%20Efficient%20Multi-Task%20Earth%20Observation%0A%20%20Modelling&entry.906535625=Carlos%20Gomes%20and%20Thomas%20Brunschwiler&entry.1292438233=%20%20As%20repositories%20of%20large%20scale%20data%20in%20earth%20observation%20%28EO%29%20have%20grown%2C%20so%0Ahave%20transfer%20and%20storage%20costs%20for%20model%20training%20and%20inference%2C%20expending%0Asignificant%20resources.%20We%20introduce%20Neural%20Embedding%20Compression%20%28NEC%29%2C%20based%0Aon%20the%20transfer%20of%20compressed%20embeddings%20to%20data%20consumers%20instead%20of%20raw%20data.%0AWe%20adapt%20foundation%20models%20%28FM%29%20through%20learned%20neural%20compression%20to%20generate%0Amulti-task%20embeddings%20while%20navigating%20the%20tradeoff%20between%20compression%20rate%0Aand%20embedding%20utility.%20We%20update%20only%20a%20small%20fraction%20of%20the%20FM%20parameters%0A%2810%25%29%20for%20a%20short%20training%20period%20%281%25%20of%20the%20iterations%20of%20pre-training%29.%20We%0Aevaluate%20NEC%20on%20two%20EO%20tasks%3A%20scene%20classification%20and%20semantic%20segmentation.%0ACompared%20with%20applying%20traditional%20compression%20to%20the%20raw%20data%2C%20NEC%20achieves%0Asimilar%20accuracy%20with%20a%2075%25%20to%2090%25%20reduction%20in%20data.%20Even%20at%2099.7%25%0Acompression%2C%20performance%20drops%20by%20only%205%25%20on%20the%20scene%20classification%20task.%0AOverall%2C%20NEC%20is%20a%20data-efficient%20yet%20performant%20approach%20for%20multi-task%20EO%0Amodelling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17886v2&entry.124074799=Read"},
{"title": "FISTNet: FusIon of STyle-path generative Networks for Facial Style\n  Transfer", "author": "Sunder Ali Khowaja and Lewis Nkenyereye and Ghulam Mujtaba and Ik Hyun Lee and Giancarlo Fortino and Kapal Dev", "abstract": "  With the surge in emerging technologies such as Metaverse, spatial computing,\nand generative AI, the application of facial style transfer has gained a lot of\ninterest from researchers as well as startups enthusiasts alike. StyleGAN\nmethods have paved the way for transfer-learning strategies that could reduce\nthe dependency on the huge volume of data that is available for the training\nprocess. However, StyleGAN methods have the tendency of overfitting that\nresults in the introduction of artifacts in the facial images. Studies, such as\nDualStyleGAN, proposed the use of multipath networks but they require the\nnetworks to be trained for a specific style rather than generating a fusion of\nfacial styles at once. In this paper, we propose a FusIon of STyles (FIST)\nnetwork for facial images that leverages pre-trained multipath style transfer\nnetworks to eliminate the problem associated with lack of huge data volume in\nthe training phase along with the fusion of multiple styles at the output. We\nleverage pre-trained styleGAN networks with an external style pass that use\nresidual modulation block instead of a transform coding block. The method also\npreserves facial structure, identity, and details via the gated mapping unit\nintroduced in this study. The aforementioned components enable us to train the\nnetwork with very limited amount of data while generating high-quality stylized\nimages. Our training process adapts curriculum learning strategy to perform\nefficient, flexible style and model fusion in the generative space. We perform\nextensive experiments to show the superiority of FISTNet in comparison to\nexisting state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2307.09020v3", "date": "2024-04-02", "relevancy": 1.4663, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5044}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4911}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4816}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FISTNet%3A%20FusIon%20of%20STyle-path%20generative%20Networks%20for%20Facial%20Style%0A%20%20Transfer&body=Title%3A%20FISTNet%3A%20FusIon%20of%20STyle-path%20generative%20Networks%20for%20Facial%20Style%0A%20%20Transfer%0AAuthor%3A%20Sunder%20Ali%20Khowaja%20and%20Lewis%20Nkenyereye%20and%20Ghulam%20Mujtaba%20and%20Ik%20Hyun%20Lee%20and%20Giancarlo%20Fortino%20and%20Kapal%20Dev%0AAbstract%3A%20%20%20With%20the%20surge%20in%20emerging%20technologies%20such%20as%20Metaverse%2C%20spatial%20computing%2C%0Aand%20generative%20AI%2C%20the%20application%20of%20facial%20style%20transfer%20has%20gained%20a%20lot%20of%0Ainterest%20from%20researchers%20as%20well%20as%20startups%20enthusiasts%20alike.%20StyleGAN%0Amethods%20have%20paved%20the%20way%20for%20transfer-learning%20strategies%20that%20could%20reduce%0Athe%20dependency%20on%20the%20huge%20volume%20of%20data%20that%20is%20available%20for%20the%20training%0Aprocess.%20However%2C%20StyleGAN%20methods%20have%20the%20tendency%20of%20overfitting%20that%0Aresults%20in%20the%20introduction%20of%20artifacts%20in%20the%20facial%20images.%20Studies%2C%20such%20as%0ADualStyleGAN%2C%20proposed%20the%20use%20of%20multipath%20networks%20but%20they%20require%20the%0Anetworks%20to%20be%20trained%20for%20a%20specific%20style%20rather%20than%20generating%20a%20fusion%20of%0Afacial%20styles%20at%20once.%20In%20this%20paper%2C%20we%20propose%20a%20FusIon%20of%20STyles%20%28FIST%29%0Anetwork%20for%20facial%20images%20that%20leverages%20pre-trained%20multipath%20style%20transfer%0Anetworks%20to%20eliminate%20the%20problem%20associated%20with%20lack%20of%20huge%20data%20volume%20in%0Athe%20training%20phase%20along%20with%20the%20fusion%20of%20multiple%20styles%20at%20the%20output.%20We%0Aleverage%20pre-trained%20styleGAN%20networks%20with%20an%20external%20style%20pass%20that%20use%0Aresidual%20modulation%20block%20instead%20of%20a%20transform%20coding%20block.%20The%20method%20also%0Apreserves%20facial%20structure%2C%20identity%2C%20and%20details%20via%20the%20gated%20mapping%20unit%0Aintroduced%20in%20this%20study.%20The%20aforementioned%20components%20enable%20us%20to%20train%20the%0Anetwork%20with%20very%20limited%20amount%20of%20data%20while%20generating%20high-quality%20stylized%0Aimages.%20Our%20training%20process%20adapts%20curriculum%20learning%20strategy%20to%20perform%0Aefficient%2C%20flexible%20style%20and%20model%20fusion%20in%20the%20generative%20space.%20We%20perform%0Aextensive%20experiments%20to%20show%20the%20superiority%20of%20FISTNet%20in%20comparison%20to%0Aexisting%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09020v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FISTNet%3A%20FusIon%20of%20STyle-path%20generative%20Networks%20for%20Facial%20Style%0A%20%20Transfer&entry.906535625=Sunder%20Ali%20Khowaja%20and%20Lewis%20Nkenyereye%20and%20Ghulam%20Mujtaba%20and%20Ik%20Hyun%20Lee%20and%20Giancarlo%20Fortino%20and%20Kapal%20Dev&entry.1292438233=%20%20With%20the%20surge%20in%20emerging%20technologies%20such%20as%20Metaverse%2C%20spatial%20computing%2C%0Aand%20generative%20AI%2C%20the%20application%20of%20facial%20style%20transfer%20has%20gained%20a%20lot%20of%0Ainterest%20from%20researchers%20as%20well%20as%20startups%20enthusiasts%20alike.%20StyleGAN%0Amethods%20have%20paved%20the%20way%20for%20transfer-learning%20strategies%20that%20could%20reduce%0Athe%20dependency%20on%20the%20huge%20volume%20of%20data%20that%20is%20available%20for%20the%20training%0Aprocess.%20However%2C%20StyleGAN%20methods%20have%20the%20tendency%20of%20overfitting%20that%0Aresults%20in%20the%20introduction%20of%20artifacts%20in%20the%20facial%20images.%20Studies%2C%20such%20as%0ADualStyleGAN%2C%20proposed%20the%20use%20of%20multipath%20networks%20but%20they%20require%20the%0Anetworks%20to%20be%20trained%20for%20a%20specific%20style%20rather%20than%20generating%20a%20fusion%20of%0Afacial%20styles%20at%20once.%20In%20this%20paper%2C%20we%20propose%20a%20FusIon%20of%20STyles%20%28FIST%29%0Anetwork%20for%20facial%20images%20that%20leverages%20pre-trained%20multipath%20style%20transfer%0Anetworks%20to%20eliminate%20the%20problem%20associated%20with%20lack%20of%20huge%20data%20volume%20in%0Athe%20training%20phase%20along%20with%20the%20fusion%20of%20multiple%20styles%20at%20the%20output.%20We%0Aleverage%20pre-trained%20styleGAN%20networks%20with%20an%20external%20style%20pass%20that%20use%0Aresidual%20modulation%20block%20instead%20of%20a%20transform%20coding%20block.%20The%20method%20also%0Apreserves%20facial%20structure%2C%20identity%2C%20and%20details%20via%20the%20gated%20mapping%20unit%0Aintroduced%20in%20this%20study.%20The%20aforementioned%20components%20enable%20us%20to%20train%20the%0Anetwork%20with%20very%20limited%20amount%20of%20data%20while%20generating%20high-quality%20stylized%0Aimages.%20Our%20training%20process%20adapts%20curriculum%20learning%20strategy%20to%20perform%0Aefficient%2C%20flexible%20style%20and%20model%20fusion%20in%20the%20generative%20space.%20We%20perform%0Aextensive%20experiments%20to%20show%20the%20superiority%20of%20FISTNet%20in%20comparison%20to%0Aexisting%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09020v3&entry.124074799=Read"},
{"title": "Traffic State Estimation from Vehicle Trajectories with Anisotropic\n  Gaussian Processes", "author": "Fan Wu and Zhanhong Cheng and Huiyu Chen and Tony Z. Qiu and Lijun Sun", "abstract": "  Accurately monitoring road traffic state is crucial for various applications,\nincluding travel time prediction, traffic control, and traffic safety. However,\nthe lack of sensors often results in incomplete traffic state data, making it\nchallenging to obtain reliable information for decision-making. This paper\nproposes a novel method for imputing traffic state data using Gaussian\nprocesses (GP) to address this issue. We propose a kernel rotation\nre-parametrization scheme that transforms a standard isotropic GP kernel into\nan anisotropic kernel, which can better model the congestion propagation in\ntraffic flow data. The model parameters can be estimated by statistical\ninference using data from sparse probe vehicles or loop detectors. Moreover,\nthe rotated GP method provides statistical uncertainty quantification for the\nimputed traffic state, making it more reliable. We also extend our approach to\na multi-output GP, which allows for simultaneously estimating the traffic state\nfor multiple lanes. We evaluate our method using real-world traffic data from\nthe Next Generation simulation (NGSIM) and HighD programs, along with simulated\ndata representing a traffic bottleneck scenario. Considering current and future\nmixed traffic of connected vehicles (CVs) and human-driven vehicles (HVs), we\nexperiment with the traffic state estimation (TSE) scheme from 5% to 50%\navailable trajectories, mimicking different CV penetration rates in a mixed\ntraffic environment. We also test the traffic state estimation when traffic\nflow information is obtained from loop detectors. The results demonstrate the\nadaptability of our TSE method across different CV penetration rates and types\nof detectors, achieving state-of-the-art accuracy in scenarios with sparse\nobservation rates.\n", "link": "http://arxiv.org/abs/2303.02311v2", "date": "2024-04-02", "relevancy": 1.4617, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5231}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4931}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4366}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Traffic%20State%20Estimation%20from%20Vehicle%20Trajectories%20with%20Anisotropic%0A%20%20Gaussian%20Processes&body=Title%3A%20Traffic%20State%20Estimation%20from%20Vehicle%20Trajectories%20with%20Anisotropic%0A%20%20Gaussian%20Processes%0AAuthor%3A%20Fan%20Wu%20and%20Zhanhong%20Cheng%20and%20Huiyu%20Chen%20and%20Tony%20Z.%20Qiu%20and%20Lijun%20Sun%0AAbstract%3A%20%20%20Accurately%20monitoring%20road%20traffic%20state%20is%20crucial%20for%20various%20applications%2C%0Aincluding%20travel%20time%20prediction%2C%20traffic%20control%2C%20and%20traffic%20safety.%20However%2C%0Athe%20lack%20of%20sensors%20often%20results%20in%20incomplete%20traffic%20state%20data%2C%20making%20it%0Achallenging%20to%20obtain%20reliable%20information%20for%20decision-making.%20This%20paper%0Aproposes%20a%20novel%20method%20for%20imputing%20traffic%20state%20data%20using%20Gaussian%0Aprocesses%20%28GP%29%20to%20address%20this%20issue.%20We%20propose%20a%20kernel%20rotation%0Are-parametrization%20scheme%20that%20transforms%20a%20standard%20isotropic%20GP%20kernel%20into%0Aan%20anisotropic%20kernel%2C%20which%20can%20better%20model%20the%20congestion%20propagation%20in%0Atraffic%20flow%20data.%20The%20model%20parameters%20can%20be%20estimated%20by%20statistical%0Ainference%20using%20data%20from%20sparse%20probe%20vehicles%20or%20loop%20detectors.%20Moreover%2C%0Athe%20rotated%20GP%20method%20provides%20statistical%20uncertainty%20quantification%20for%20the%0Aimputed%20traffic%20state%2C%20making%20it%20more%20reliable.%20We%20also%20extend%20our%20approach%20to%0Aa%20multi-output%20GP%2C%20which%20allows%20for%20simultaneously%20estimating%20the%20traffic%20state%0Afor%20multiple%20lanes.%20We%20evaluate%20our%20method%20using%20real-world%20traffic%20data%20from%0Athe%20Next%20Generation%20simulation%20%28NGSIM%29%20and%20HighD%20programs%2C%20along%20with%20simulated%0Adata%20representing%20a%20traffic%20bottleneck%20scenario.%20Considering%20current%20and%20future%0Amixed%20traffic%20of%20connected%20vehicles%20%28CVs%29%20and%20human-driven%20vehicles%20%28HVs%29%2C%20we%0Aexperiment%20with%20the%20traffic%20state%20estimation%20%28TSE%29%20scheme%20from%205%25%20to%2050%25%0Aavailable%20trajectories%2C%20mimicking%20different%20CV%20penetration%20rates%20in%20a%20mixed%0Atraffic%20environment.%20We%20also%20test%20the%20traffic%20state%20estimation%20when%20traffic%0Aflow%20information%20is%20obtained%20from%20loop%20detectors.%20The%20results%20demonstrate%20the%0Aadaptability%20of%20our%20TSE%20method%20across%20different%20CV%20penetration%20rates%20and%20types%0Aof%20detectors%2C%20achieving%20state-of-the-art%20accuracy%20in%20scenarios%20with%20sparse%0Aobservation%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.02311v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Traffic%20State%20Estimation%20from%20Vehicle%20Trajectories%20with%20Anisotropic%0A%20%20Gaussian%20Processes&entry.906535625=Fan%20Wu%20and%20Zhanhong%20Cheng%20and%20Huiyu%20Chen%20and%20Tony%20Z.%20Qiu%20and%20Lijun%20Sun&entry.1292438233=%20%20Accurately%20monitoring%20road%20traffic%20state%20is%20crucial%20for%20various%20applications%2C%0Aincluding%20travel%20time%20prediction%2C%20traffic%20control%2C%20and%20traffic%20safety.%20However%2C%0Athe%20lack%20of%20sensors%20often%20results%20in%20incomplete%20traffic%20state%20data%2C%20making%20it%0Achallenging%20to%20obtain%20reliable%20information%20for%20decision-making.%20This%20paper%0Aproposes%20a%20novel%20method%20for%20imputing%20traffic%20state%20data%20using%20Gaussian%0Aprocesses%20%28GP%29%20to%20address%20this%20issue.%20We%20propose%20a%20kernel%20rotation%0Are-parametrization%20scheme%20that%20transforms%20a%20standard%20isotropic%20GP%20kernel%20into%0Aan%20anisotropic%20kernel%2C%20which%20can%20better%20model%20the%20congestion%20propagation%20in%0Atraffic%20flow%20data.%20The%20model%20parameters%20can%20be%20estimated%20by%20statistical%0Ainference%20using%20data%20from%20sparse%20probe%20vehicles%20or%20loop%20detectors.%20Moreover%2C%0Athe%20rotated%20GP%20method%20provides%20statistical%20uncertainty%20quantification%20for%20the%0Aimputed%20traffic%20state%2C%20making%20it%20more%20reliable.%20We%20also%20extend%20our%20approach%20to%0Aa%20multi-output%20GP%2C%20which%20allows%20for%20simultaneously%20estimating%20the%20traffic%20state%0Afor%20multiple%20lanes.%20We%20evaluate%20our%20method%20using%20real-world%20traffic%20data%20from%0Athe%20Next%20Generation%20simulation%20%28NGSIM%29%20and%20HighD%20programs%2C%20along%20with%20simulated%0Adata%20representing%20a%20traffic%20bottleneck%20scenario.%20Considering%20current%20and%20future%0Amixed%20traffic%20of%20connected%20vehicles%20%28CVs%29%20and%20human-driven%20vehicles%20%28HVs%29%2C%20we%0Aexperiment%20with%20the%20traffic%20state%20estimation%20%28TSE%29%20scheme%20from%205%25%20to%2050%25%0Aavailable%20trajectories%2C%20mimicking%20different%20CV%20penetration%20rates%20in%20a%20mixed%0Atraffic%20environment.%20We%20also%20test%20the%20traffic%20state%20estimation%20when%20traffic%0Aflow%20information%20is%20obtained%20from%20loop%20detectors.%20The%20results%20demonstrate%20the%0Aadaptability%20of%20our%20TSE%20method%20across%20different%20CV%20penetration%20rates%20and%20types%0Aof%20detectors%2C%20achieving%20state-of-the-art%20accuracy%20in%20scenarios%20with%20sparse%0Aobservation%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.02311v2&entry.124074799=Read"},
{"title": "A Survey on Large Language Model-Based Game Agents", "author": "Sihao Hu and Tiansheng Huang and Fatih Ilhan and Selim Tekin and Gaowen Liu and Ramana Kompella and Ling Liu", "abstract": "  The development of game agents holds a critical role in advancing towards\nArtificial General Intelligence (AGI). The progress of LLMs and their\nmultimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve\nand empower game agents with human-like decision-making capabilities in complex\ncomputer game environments. This paper provides a comprehensive overview of\nLLM-based game agents from a holistic viewpoint. First, we introduce the\nconceptual architecture of LLM-based game agents, centered around six essential\nfunctional components: perception, memory, thinking, role-playing, action, and\nlearning. Second, we survey existing representative LLM-based game agents\ndocumented in the literature with respect to methodologies and adaptation\nagility across six genres of games, including adventure, communication,\ncompetition, cooperation, simulation, and crafting & exploration games.\nFinally, we present an outlook of future research and development directions in\nthis burgeoning field. A curated list of relevant papers is maintained and made\naccessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.\n", "link": "http://arxiv.org/abs/2404.02039v1", "date": "2024-04-02", "relevancy": 1.4427, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5035}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4768}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4735}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Large%20Language%20Model-Based%20Game%20Agents&body=Title%3A%20A%20Survey%20on%20Large%20Language%20Model-Based%20Game%20Agents%0AAuthor%3A%20Sihao%20Hu%20and%20Tiansheng%20Huang%20and%20Fatih%20Ilhan%20and%20Selim%20Tekin%20and%20Gaowen%20Liu%20and%20Ramana%20Kompella%20and%20Ling%20Liu%0AAbstract%3A%20%20%20The%20development%20of%20game%20agents%20holds%20a%20critical%20role%20in%20advancing%20towards%0AArtificial%20General%20Intelligence%20%28AGI%29.%20The%20progress%20of%20LLMs%20and%20their%0Amultimodal%20counterparts%20%28MLLMs%29%20offers%20an%20unprecedented%20opportunity%20to%20evolve%0Aand%20empower%20game%20agents%20with%20human-like%20decision-making%20capabilities%20in%20complex%0Acomputer%20game%20environments.%20This%20paper%20provides%20a%20comprehensive%20overview%20of%0ALLM-based%20game%20agents%20from%20a%20holistic%20viewpoint.%20First%2C%20we%20introduce%20the%0Aconceptual%20architecture%20of%20LLM-based%20game%20agents%2C%20centered%20around%20six%20essential%0Afunctional%20components%3A%20perception%2C%20memory%2C%20thinking%2C%20role-playing%2C%20action%2C%20and%0Alearning.%20Second%2C%20we%20survey%20existing%20representative%20LLM-based%20game%20agents%0Adocumented%20in%20the%20literature%20with%20respect%20to%20methodologies%20and%20adaptation%0Aagility%20across%20six%20genres%20of%20games%2C%20including%20adventure%2C%20communication%2C%0Acompetition%2C%20cooperation%2C%20simulation%2C%20and%20crafting%20%26%20exploration%20games.%0AFinally%2C%20we%20present%20an%20outlook%20of%20future%20research%20and%20development%20directions%20in%0Athis%20burgeoning%20field.%20A%20curated%20list%20of%20relevant%20papers%20is%20maintained%20and%20made%0Aaccessible%20at%3A%20https%3A//github.com/git-disl/awesome-LLM-game-agent-papers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02039v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Large%20Language%20Model-Based%20Game%20Agents&entry.906535625=Sihao%20Hu%20and%20Tiansheng%20Huang%20and%20Fatih%20Ilhan%20and%20Selim%20Tekin%20and%20Gaowen%20Liu%20and%20Ramana%20Kompella%20and%20Ling%20Liu&entry.1292438233=%20%20The%20development%20of%20game%20agents%20holds%20a%20critical%20role%20in%20advancing%20towards%0AArtificial%20General%20Intelligence%20%28AGI%29.%20The%20progress%20of%20LLMs%20and%20their%0Amultimodal%20counterparts%20%28MLLMs%29%20offers%20an%20unprecedented%20opportunity%20to%20evolve%0Aand%20empower%20game%20agents%20with%20human-like%20decision-making%20capabilities%20in%20complex%0Acomputer%20game%20environments.%20This%20paper%20provides%20a%20comprehensive%20overview%20of%0ALLM-based%20game%20agents%20from%20a%20holistic%20viewpoint.%20First%2C%20we%20introduce%20the%0Aconceptual%20architecture%20of%20LLM-based%20game%20agents%2C%20centered%20around%20six%20essential%0Afunctional%20components%3A%20perception%2C%20memory%2C%20thinking%2C%20role-playing%2C%20action%2C%20and%0Alearning.%20Second%2C%20we%20survey%20existing%20representative%20LLM-based%20game%20agents%0Adocumented%20in%20the%20literature%20with%20respect%20to%20methodologies%20and%20adaptation%0Aagility%20across%20six%20genres%20of%20games%2C%20including%20adventure%2C%20communication%2C%0Acompetition%2C%20cooperation%2C%20simulation%2C%20and%20crafting%20%26%20exploration%20games.%0AFinally%2C%20we%20present%20an%20outlook%20of%20future%20research%20and%20development%20directions%20in%0Athis%20burgeoning%20field.%20A%20curated%20list%20of%20relevant%20papers%20is%20maintained%20and%20made%0Aaccessible%20at%3A%20https%3A//github.com/git-disl/awesome-LLM-game-agent-papers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02039v1&entry.124074799=Read"},
{"title": "AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug\n  Design", "author": "Xinze Li and Penglei Wang and Tianfan Fu and Wenhao Gao and Chengtao Li and Leilei Shi and Junhong Liu", "abstract": "  Structure-based drug design (SBDD), which aims to generate molecules that can\nbind tightly to the target protein, is an essential problem in drug discovery,\nand previous approaches have achieved initial success. However, most existing\nmethods still suffer from invalid local structure or unrealistic conformation\nissues, which are mainly due to the poor leaning of bond angles or torsional\nangles. To alleviate these problems, we propose AUTODIFF, a diffusion-based\nfragment-wise autoregressive generation model. Specifically, we design a novel\nmolecule assembly strategy named conformal motif that preserves the\nconformation of local structures of molecules first, then we encode the\ninteraction of the protein-ligand complex with an SE(3)-equivariant\nconvolutional network and generate molecules motif-by-motif with diffusion\nmodeling. In addition, we also improve the evaluation framework of SBDD by\nconstraining the molecular weights of the generated molecules in the same\nrange, together with some new metrics, which make the evaluation more fair and\npractical. Extensive experiments on CrossDocked2020 demonstrate that our\napproach outperforms the existing models in generating realistic molecules with\nvalid structures and conformations while maintaining high binding affinity.\n", "link": "http://arxiv.org/abs/2404.02003v1", "date": "2024-04-02", "relevancy": 1.441, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5149}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4715}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.47}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AUTODIFF%3A%20Autoregressive%20Diffusion%20Modeling%20for%20Structure-based%20Drug%0A%20%20Design&body=Title%3A%20AUTODIFF%3A%20Autoregressive%20Diffusion%20Modeling%20for%20Structure-based%20Drug%0A%20%20Design%0AAuthor%3A%20Xinze%20Li%20and%20Penglei%20Wang%20and%20Tianfan%20Fu%20and%20Wenhao%20Gao%20and%20Chengtao%20Li%20and%20Leilei%20Shi%20and%20Junhong%20Liu%0AAbstract%3A%20%20%20Structure-based%20drug%20design%20%28SBDD%29%2C%20which%20aims%20to%20generate%20molecules%20that%20can%0Abind%20tightly%20to%20the%20target%20protein%2C%20is%20an%20essential%20problem%20in%20drug%20discovery%2C%0Aand%20previous%20approaches%20have%20achieved%20initial%20success.%20However%2C%20most%20existing%0Amethods%20still%20suffer%20from%20invalid%20local%20structure%20or%20unrealistic%20conformation%0Aissues%2C%20which%20are%20mainly%20due%20to%20the%20poor%20leaning%20of%20bond%20angles%20or%20torsional%0Aangles.%20To%20alleviate%20these%20problems%2C%20we%20propose%20AUTODIFF%2C%20a%20diffusion-based%0Afragment-wise%20autoregressive%20generation%20model.%20Specifically%2C%20we%20design%20a%20novel%0Amolecule%20assembly%20strategy%20named%20conformal%20motif%20that%20preserves%20the%0Aconformation%20of%20local%20structures%20of%20molecules%20first%2C%20then%20we%20encode%20the%0Ainteraction%20of%20the%20protein-ligand%20complex%20with%20an%20SE%283%29-equivariant%0Aconvolutional%20network%20and%20generate%20molecules%20motif-by-motif%20with%20diffusion%0Amodeling.%20In%20addition%2C%20we%20also%20improve%20the%20evaluation%20framework%20of%20SBDD%20by%0Aconstraining%20the%20molecular%20weights%20of%20the%20generated%20molecules%20in%20the%20same%0Arange%2C%20together%20with%20some%20new%20metrics%2C%20which%20make%20the%20evaluation%20more%20fair%20and%0Apractical.%20Extensive%20experiments%20on%20CrossDocked2020%20demonstrate%20that%20our%0Aapproach%20outperforms%20the%20existing%20models%20in%20generating%20realistic%20molecules%20with%0Avalid%20structures%20and%20conformations%20while%20maintaining%20high%20binding%20affinity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02003v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AUTODIFF%3A%20Autoregressive%20Diffusion%20Modeling%20for%20Structure-based%20Drug%0A%20%20Design&entry.906535625=Xinze%20Li%20and%20Penglei%20Wang%20and%20Tianfan%20Fu%20and%20Wenhao%20Gao%20and%20Chengtao%20Li%20and%20Leilei%20Shi%20and%20Junhong%20Liu&entry.1292438233=%20%20Structure-based%20drug%20design%20%28SBDD%29%2C%20which%20aims%20to%20generate%20molecules%20that%20can%0Abind%20tightly%20to%20the%20target%20protein%2C%20is%20an%20essential%20problem%20in%20drug%20discovery%2C%0Aand%20previous%20approaches%20have%20achieved%20initial%20success.%20However%2C%20most%20existing%0Amethods%20still%20suffer%20from%20invalid%20local%20structure%20or%20unrealistic%20conformation%0Aissues%2C%20which%20are%20mainly%20due%20to%20the%20poor%20leaning%20of%20bond%20angles%20or%20torsional%0Aangles.%20To%20alleviate%20these%20problems%2C%20we%20propose%20AUTODIFF%2C%20a%20diffusion-based%0Afragment-wise%20autoregressive%20generation%20model.%20Specifically%2C%20we%20design%20a%20novel%0Amolecule%20assembly%20strategy%20named%20conformal%20motif%20that%20preserves%20the%0Aconformation%20of%20local%20structures%20of%20molecules%20first%2C%20then%20we%20encode%20the%0Ainteraction%20of%20the%20protein-ligand%20complex%20with%20an%20SE%283%29-equivariant%0Aconvolutional%20network%20and%20generate%20molecules%20motif-by-motif%20with%20diffusion%0Amodeling.%20In%20addition%2C%20we%20also%20improve%20the%20evaluation%20framework%20of%20SBDD%20by%0Aconstraining%20the%20molecular%20weights%20of%20the%20generated%20molecules%20in%20the%20same%0Arange%2C%20together%20with%20some%20new%20metrics%2C%20which%20make%20the%20evaluation%20more%20fair%20and%0Apractical.%20Extensive%20experiments%20on%20CrossDocked2020%20demonstrate%20that%20our%0Aapproach%20outperforms%20the%20existing%20models%20in%20generating%20realistic%20molecules%20with%0Avalid%20structures%20and%20conformations%20while%20maintaining%20high%20binding%20affinity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02003v1&entry.124074799=Read"},
{"title": "Advancing LLM Reasoning Generalists with Preference Trees", "author": "Lifan Yuan and Ganqu Cui and Hanbin Wang and Ning Ding and Xingyao Wang and Jia Deng and Boji Shan and Huimin Chen and Ruobing Xie and Yankai Lin and Zhenghao Liu and Bowen Zhou and Hao Peng and Zhiyuan Liu and Maosong Sun", "abstract": "  We introduce Eurus, a suite of large language models (LLMs) optimized for\nreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve\nstate-of-the-art results among open-source models on a diverse set of\nbenchmarks covering mathematics, code generation, and logical reasoning\nproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a\ncomprehensive benchmarking across 12 tests covering five tasks, and achieves a\n33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging\nbenchmarks, substantially outperforming existing open-source models by margins\nmore than 13.3%. The strong performance of Eurus can be primarily attributed to\nUltraInteract, our newly-curated large-scale, high-quality alignment dataset\nspecifically designed for complex reasoning tasks. UltraInteract can be used in\nboth supervised fine-tuning and preference learning. For each instruction, it\nincludes a preference tree consisting of (1) reasoning chains with diverse\nplanning strategies in a unified format, (2) multi-turn interaction\ntrajectories with the environment and the critique, and (3) pairwise data to\nfacilitate preference learning. UltraInteract allows us to conduct an in-depth\nexploration of preference learning for reasoning tasks. Our investigation\nreveals that some well-established preference learning algorithms may be less\nsuitable for reasoning tasks compared to their effectiveness in general\nconversations. Inspired by this, we derive a novel reward modeling objective\nwhich, together with UltraInteract, leads to a strong reward model.\n", "link": "http://arxiv.org/abs/2404.02078v1", "date": "2024-04-02", "relevancy": 1.4406, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4741}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Advancing%20LLM%20Reasoning%20Generalists%20with%20Preference%20Trees&body=Title%3A%20Advancing%20LLM%20Reasoning%20Generalists%20with%20Preference%20Trees%0AAuthor%3A%20Lifan%20Yuan%20and%20Ganqu%20Cui%20and%20Hanbin%20Wang%20and%20Ning%20Ding%20and%20Xingyao%20Wang%20and%20Jia%20Deng%20and%20Boji%20Shan%20and%20Huimin%20Chen%20and%20Ruobing%20Xie%20and%20Yankai%20Lin%20and%20Zhenghao%20Liu%20and%20Bowen%20Zhou%20and%20Hao%20Peng%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20We%20introduce%20Eurus%2C%20a%20suite%20of%20large%20language%20models%20%28LLMs%29%20optimized%20for%0Areasoning.%20Finetuned%20from%20Mistral-7B%20and%20CodeLlama-70B%2C%20Eurus%20models%20achieve%0Astate-of-the-art%20results%20among%20open-source%20models%20on%20a%20diverse%20set%20of%0Abenchmarks%20covering%20mathematics%2C%20code%20generation%2C%20and%20logical%20reasoning%0Aproblems.%20Notably%2C%20Eurus-70B%20beats%20GPT-3.5%20Turbo%20in%20reasoning%20through%20a%0Acomprehensive%20benchmarking%20across%2012%20tests%20covering%20five%20tasks%2C%20and%20achieves%20a%0A33.3%25%20pass%401%20accuracy%20on%20LeetCode%20and%2032.6%25%20on%20TheoremQA%2C%20two%20challenging%0Abenchmarks%2C%20substantially%20outperforming%20existing%20open-source%20models%20by%20margins%0Amore%20than%2013.3%25.%20The%20strong%20performance%20of%20Eurus%20can%20be%20primarily%20attributed%20to%0AUltraInteract%2C%20our%20newly-curated%20large-scale%2C%20high-quality%20alignment%20dataset%0Aspecifically%20designed%20for%20complex%20reasoning%20tasks.%20UltraInteract%20can%20be%20used%20in%0Aboth%20supervised%20fine-tuning%20and%20preference%20learning.%20For%20each%20instruction%2C%20it%0Aincludes%20a%20preference%20tree%20consisting%20of%20%281%29%20reasoning%20chains%20with%20diverse%0Aplanning%20strategies%20in%20a%20unified%20format%2C%20%282%29%20multi-turn%20interaction%0Atrajectories%20with%20the%20environment%20and%20the%20critique%2C%20and%20%283%29%20pairwise%20data%20to%0Afacilitate%20preference%20learning.%20UltraInteract%20allows%20us%20to%20conduct%20an%20in-depth%0Aexploration%20of%20preference%20learning%20for%20reasoning%20tasks.%20Our%20investigation%0Areveals%20that%20some%20well-established%20preference%20learning%20algorithms%20may%20be%20less%0Asuitable%20for%20reasoning%20tasks%20compared%20to%20their%20effectiveness%20in%20general%0Aconversations.%20Inspired%20by%20this%2C%20we%20derive%20a%20novel%20reward%20modeling%20objective%0Awhich%2C%20together%20with%20UltraInteract%2C%20leads%20to%20a%20strong%20reward%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02078v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20LLM%20Reasoning%20Generalists%20with%20Preference%20Trees&entry.906535625=Lifan%20Yuan%20and%20Ganqu%20Cui%20and%20Hanbin%20Wang%20and%20Ning%20Ding%20and%20Xingyao%20Wang%20and%20Jia%20Deng%20and%20Boji%20Shan%20and%20Huimin%20Chen%20and%20Ruobing%20Xie%20and%20Yankai%20Lin%20and%20Zhenghao%20Liu%20and%20Bowen%20Zhou%20and%20Hao%20Peng%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20We%20introduce%20Eurus%2C%20a%20suite%20of%20large%20language%20models%20%28LLMs%29%20optimized%20for%0Areasoning.%20Finetuned%20from%20Mistral-7B%20and%20CodeLlama-70B%2C%20Eurus%20models%20achieve%0Astate-of-the-art%20results%20among%20open-source%20models%20on%20a%20diverse%20set%20of%0Abenchmarks%20covering%20mathematics%2C%20code%20generation%2C%20and%20logical%20reasoning%0Aproblems.%20Notably%2C%20Eurus-70B%20beats%20GPT-3.5%20Turbo%20in%20reasoning%20through%20a%0Acomprehensive%20benchmarking%20across%2012%20tests%20covering%20five%20tasks%2C%20and%20achieves%20a%0A33.3%25%20pass%401%20accuracy%20on%20LeetCode%20and%2032.6%25%20on%20TheoremQA%2C%20two%20challenging%0Abenchmarks%2C%20substantially%20outperforming%20existing%20open-source%20models%20by%20margins%0Amore%20than%2013.3%25.%20The%20strong%20performance%20of%20Eurus%20can%20be%20primarily%20attributed%20to%0AUltraInteract%2C%20our%20newly-curated%20large-scale%2C%20high-quality%20alignment%20dataset%0Aspecifically%20designed%20for%20complex%20reasoning%20tasks.%20UltraInteract%20can%20be%20used%20in%0Aboth%20supervised%20fine-tuning%20and%20preference%20learning.%20For%20each%20instruction%2C%20it%0Aincludes%20a%20preference%20tree%20consisting%20of%20%281%29%20reasoning%20chains%20with%20diverse%0Aplanning%20strategies%20in%20a%20unified%20format%2C%20%282%29%20multi-turn%20interaction%0Atrajectories%20with%20the%20environment%20and%20the%20critique%2C%20and%20%283%29%20pairwise%20data%20to%0Afacilitate%20preference%20learning.%20UltraInteract%20allows%20us%20to%20conduct%20an%20in-depth%0Aexploration%20of%20preference%20learning%20for%20reasoning%20tasks.%20Our%20investigation%0Areveals%20that%20some%20well-established%20preference%20learning%20algorithms%20may%20be%20less%0Asuitable%20for%20reasoning%20tasks%20compared%20to%20their%20effectiveness%20in%20general%0Aconversations.%20Inspired%20by%20this%2C%20we%20derive%20a%20novel%20reward%20modeling%20objective%0Awhich%2C%20together%20with%20UltraInteract%2C%20leads%20to%20a%20strong%20reward%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02078v1&entry.124074799=Read"},
{"title": "HALO: An Ontology for Representing and Categorizing Hallucinations in\n  Large Language Models", "author": "Navapat Nananukul and Mayank Kejriwal", "abstract": "  Recent progress in generative AI, including large language models (LLMs) like\nChatGPT, has opened up significant opportunities in fields ranging from natural\nlanguage processing to knowledge discovery and data mining. However, there is\nalso a growing awareness that the models can be prone to problems such as\nmaking information up or `hallucinations', and faulty reasoning on seemingly\nsimple problems. Because of the popularity of models like ChatGPT, both\nacademic scholars and citizen scientists have documented hallucinations of\nseveral different types and severity. Despite this body of work, a formal model\nfor describing and representing these hallucinations (with relevant meta-data)\nat a fine-grained level, is still lacking. In this paper, we address this gap\nby presenting the Hallucination Ontology or HALO, a formal, extensible ontology\nwritten in OWL that currently offers support for six different types of\nhallucinations known to arise in LLMs, along with support for provenance and\nexperimental metadata. We also collect and publish a dataset containing\nhallucinations that we inductively gathered across multiple independent Web\nsources, and show that HALO can be successfully used to model this dataset and\nanswer competency questions.\n", "link": "http://arxiv.org/abs/2312.05209v2", "date": "2024-04-02", "relevancy": 1.4346, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4836}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4805}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4669}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HALO%3A%20An%20Ontology%20for%20Representing%20and%20Categorizing%20Hallucinations%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20HALO%3A%20An%20Ontology%20for%20Representing%20and%20Categorizing%20Hallucinations%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Navapat%20Nananukul%20and%20Mayank%20Kejriwal%0AAbstract%3A%20%20%20Recent%20progress%20in%20generative%20AI%2C%20including%20large%20language%20models%20%28LLMs%29%20like%0AChatGPT%2C%20has%20opened%20up%20significant%20opportunities%20in%20fields%20ranging%20from%20natural%0Alanguage%20processing%20to%20knowledge%20discovery%20and%20data%20mining.%20However%2C%20there%20is%0Aalso%20a%20growing%20awareness%20that%20the%20models%20can%20be%20prone%20to%20problems%20such%20as%0Amaking%20information%20up%20or%20%60hallucinations%27%2C%20and%20faulty%20reasoning%20on%20seemingly%0Asimple%20problems.%20Because%20of%20the%20popularity%20of%20models%20like%20ChatGPT%2C%20both%0Aacademic%20scholars%20and%20citizen%20scientists%20have%20documented%20hallucinations%20of%0Aseveral%20different%20types%20and%20severity.%20Despite%20this%20body%20of%20work%2C%20a%20formal%20model%0Afor%20describing%20and%20representing%20these%20hallucinations%20%28with%20relevant%20meta-data%29%0Aat%20a%20fine-grained%20level%2C%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20address%20this%20gap%0Aby%20presenting%20the%20Hallucination%20Ontology%20or%20HALO%2C%20a%20formal%2C%20extensible%20ontology%0Awritten%20in%20OWL%20that%20currently%20offers%20support%20for%20six%20different%20types%20of%0Ahallucinations%20known%20to%20arise%20in%20LLMs%2C%20along%20with%20support%20for%20provenance%20and%0Aexperimental%20metadata.%20We%20also%20collect%20and%20publish%20a%20dataset%20containing%0Ahallucinations%20that%20we%20inductively%20gathered%20across%20multiple%20independent%20Web%0Asources%2C%20and%20show%20that%20HALO%20can%20be%20successfully%20used%20to%20model%20this%20dataset%20and%0Aanswer%20competency%20questions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05209v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HALO%3A%20An%20Ontology%20for%20Representing%20and%20Categorizing%20Hallucinations%20in%0A%20%20Large%20Language%20Models&entry.906535625=Navapat%20Nananukul%20and%20Mayank%20Kejriwal&entry.1292438233=%20%20Recent%20progress%20in%20generative%20AI%2C%20including%20large%20language%20models%20%28LLMs%29%20like%0AChatGPT%2C%20has%20opened%20up%20significant%20opportunities%20in%20fields%20ranging%20from%20natural%0Alanguage%20processing%20to%20knowledge%20discovery%20and%20data%20mining.%20However%2C%20there%20is%0Aalso%20a%20growing%20awareness%20that%20the%20models%20can%20be%20prone%20to%20problems%20such%20as%0Amaking%20information%20up%20or%20%60hallucinations%27%2C%20and%20faulty%20reasoning%20on%20seemingly%0Asimple%20problems.%20Because%20of%20the%20popularity%20of%20models%20like%20ChatGPT%2C%20both%0Aacademic%20scholars%20and%20citizen%20scientists%20have%20documented%20hallucinations%20of%0Aseveral%20different%20types%20and%20severity.%20Despite%20this%20body%20of%20work%2C%20a%20formal%20model%0Afor%20describing%20and%20representing%20these%20hallucinations%20%28with%20relevant%20meta-data%29%0Aat%20a%20fine-grained%20level%2C%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20address%20this%20gap%0Aby%20presenting%20the%20Hallucination%20Ontology%20or%20HALO%2C%20a%20formal%2C%20extensible%20ontology%0Awritten%20in%20OWL%20that%20currently%20offers%20support%20for%20six%20different%20types%20of%0Ahallucinations%20known%20to%20arise%20in%20LLMs%2C%20along%20with%20support%20for%20provenance%20and%0Aexperimental%20metadata.%20We%20also%20collect%20and%20publish%20a%20dataset%20containing%0Ahallucinations%20that%20we%20inductively%20gathered%20across%20multiple%20independent%20Web%0Asources%2C%20and%20show%20that%20HALO%20can%20be%20successfully%20used%20to%20model%20this%20dataset%20and%0Aanswer%20competency%20questions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05209v2&entry.124074799=Read"},
{"title": "Digital Forgetting in Large Language Models: A Survey of Unlearning\n  Methods", "author": "Alberto Blanco-Justicia and Najeeb Jebreel and Benet Manzanares and David S\u00e1nchez and Josep Domingo-Ferrer and Guillem Collell and Kuan Eeik Tan", "abstract": "  The objective of digital forgetting is, given a model with undesirable\nknowledge or behavior, obtain a new model where the detected issues are no\nlonger present. The motivations for forgetting include privacy protection,\ncopyright protection, elimination of biases and discrimination, and prevention\nof harmful content generation. Effective digital forgetting has to be effective\n(meaning how well the new model has forgotten the undesired\nknowledge/behavior), retain the performance of the original model on the\ndesirable tasks, and be scalable (in particular forgetting has to be more\nefficient than retraining from scratch on just the tasks/data to be retained).\nThis survey focuses on forgetting in large language models (LLMs). We first\nprovide background on LLMs, including their components, the types of LLMs, and\ntheir usual training pipeline. Second, we describe the motivations, types, and\ndesired properties of digital forgetting. Third, we introduce the approaches to\ndigital forgetting in LLMs, among which unlearning methodologies stand out as\nthe state of the art. Fourth, we provide a detailed taxonomy of machine\nunlearning methods for LLMs, and we survey and compare current approaches.\nFifth, we detail datasets, models and metrics used for the evaluation of\nforgetting, retaining and runtime. Sixth, we discuss challenges in the area.\nFinally, we provide some concluding remarks.\n", "link": "http://arxiv.org/abs/2404.02062v1", "date": "2024-04-02", "relevancy": 1.4332, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4815}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4746}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4715}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Digital%20Forgetting%20in%20Large%20Language%20Models%3A%20A%20Survey%20of%20Unlearning%0A%20%20Methods&body=Title%3A%20Digital%20Forgetting%20in%20Large%20Language%20Models%3A%20A%20Survey%20of%20Unlearning%0A%20%20Methods%0AAuthor%3A%20Alberto%20Blanco-Justicia%20and%20Najeeb%20Jebreel%20and%20Benet%20Manzanares%20and%20David%20S%C3%A1nchez%20and%20Josep%20Domingo-Ferrer%20and%20Guillem%20Collell%20and%20Kuan%20Eeik%20Tan%0AAbstract%3A%20%20%20The%20objective%20of%20digital%20forgetting%20is%2C%20given%20a%20model%20with%20undesirable%0Aknowledge%20or%20behavior%2C%20obtain%20a%20new%20model%20where%20the%20detected%20issues%20are%20no%0Alonger%20present.%20The%20motivations%20for%20forgetting%20include%20privacy%20protection%2C%0Acopyright%20protection%2C%20elimination%20of%20biases%20and%20discrimination%2C%20and%20prevention%0Aof%20harmful%20content%20generation.%20Effective%20digital%20forgetting%20has%20to%20be%20effective%0A%28meaning%20how%20well%20the%20new%20model%20has%20forgotten%20the%20undesired%0Aknowledge/behavior%29%2C%20retain%20the%20performance%20of%20the%20original%20model%20on%20the%0Adesirable%20tasks%2C%20and%20be%20scalable%20%28in%20particular%20forgetting%20has%20to%20be%20more%0Aefficient%20than%20retraining%20from%20scratch%20on%20just%20the%20tasks/data%20to%20be%20retained%29.%0AThis%20survey%20focuses%20on%20forgetting%20in%20large%20language%20models%20%28LLMs%29.%20We%20first%0Aprovide%20background%20on%20LLMs%2C%20including%20their%20components%2C%20the%20types%20of%20LLMs%2C%20and%0Atheir%20usual%20training%20pipeline.%20Second%2C%20we%20describe%20the%20motivations%2C%20types%2C%20and%0Adesired%20properties%20of%20digital%20forgetting.%20Third%2C%20we%20introduce%20the%20approaches%20to%0Adigital%20forgetting%20in%20LLMs%2C%20among%20which%20unlearning%20methodologies%20stand%20out%20as%0Athe%20state%20of%20the%20art.%20Fourth%2C%20we%20provide%20a%20detailed%20taxonomy%20of%20machine%0Aunlearning%20methods%20for%20LLMs%2C%20and%20we%20survey%20and%20compare%20current%20approaches.%0AFifth%2C%20we%20detail%20datasets%2C%20models%20and%20metrics%20used%20for%20the%20evaluation%20of%0Aforgetting%2C%20retaining%20and%20runtime.%20Sixth%2C%20we%20discuss%20challenges%20in%20the%20area.%0AFinally%2C%20we%20provide%20some%20concluding%20remarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02062v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Forgetting%20in%20Large%20Language%20Models%3A%20A%20Survey%20of%20Unlearning%0A%20%20Methods&entry.906535625=Alberto%20Blanco-Justicia%20and%20Najeeb%20Jebreel%20and%20Benet%20Manzanares%20and%20David%20S%C3%A1nchez%20and%20Josep%20Domingo-Ferrer%20and%20Guillem%20Collell%20and%20Kuan%20Eeik%20Tan&entry.1292438233=%20%20The%20objective%20of%20digital%20forgetting%20is%2C%20given%20a%20model%20with%20undesirable%0Aknowledge%20or%20behavior%2C%20obtain%20a%20new%20model%20where%20the%20detected%20issues%20are%20no%0Alonger%20present.%20The%20motivations%20for%20forgetting%20include%20privacy%20protection%2C%0Acopyright%20protection%2C%20elimination%20of%20biases%20and%20discrimination%2C%20and%20prevention%0Aof%20harmful%20content%20generation.%20Effective%20digital%20forgetting%20has%20to%20be%20effective%0A%28meaning%20how%20well%20the%20new%20model%20has%20forgotten%20the%20undesired%0Aknowledge/behavior%29%2C%20retain%20the%20performance%20of%20the%20original%20model%20on%20the%0Adesirable%20tasks%2C%20and%20be%20scalable%20%28in%20particular%20forgetting%20has%20to%20be%20more%0Aefficient%20than%20retraining%20from%20scratch%20on%20just%20the%20tasks/data%20to%20be%20retained%29.%0AThis%20survey%20focuses%20on%20forgetting%20in%20large%20language%20models%20%28LLMs%29.%20We%20first%0Aprovide%20background%20on%20LLMs%2C%20including%20their%20components%2C%20the%20types%20of%20LLMs%2C%20and%0Atheir%20usual%20training%20pipeline.%20Second%2C%20we%20describe%20the%20motivations%2C%20types%2C%20and%0Adesired%20properties%20of%20digital%20forgetting.%20Third%2C%20we%20introduce%20the%20approaches%20to%0Adigital%20forgetting%20in%20LLMs%2C%20among%20which%20unlearning%20methodologies%20stand%20out%20as%0Athe%20state%20of%20the%20art.%20Fourth%2C%20we%20provide%20a%20detailed%20taxonomy%20of%20machine%0Aunlearning%20methods%20for%20LLMs%2C%20and%20we%20survey%20and%20compare%20current%20approaches.%0AFifth%2C%20we%20detail%20datasets%2C%20models%20and%20metrics%20used%20for%20the%20evaluation%20of%0Aforgetting%2C%20retaining%20and%20runtime.%20Sixth%2C%20we%20discuss%20challenges%20in%20the%20area.%0AFinally%2C%20we%20provide%20some%20concluding%20remarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02062v1&entry.124074799=Read"},
{"title": "The Unreasonable Effectiveness of Random Target Embeddings for\n  Continuous-Output Neural Machine Translation", "author": "Evgeniia Tokarchuk and Vlad Niculae", "abstract": "  Continuous-output neural machine translation (CoNMT) replaces the discrete\nnext-word prediction problem with an embedding prediction. The semantic\nstructure of the target embedding space (i.e., closeness of related words) is\nintuitively believed to be crucial. We challenge this assumption and show that\ncompletely random output embeddings can outperform laboriously pretrained ones,\nespecially on larger datasets. Further investigation shows this surprising\neffect is strongest for rare words, due to the geometry of their embeddings. We\nshed further light on this finding by designing a mixed strategy that combines\nrandom and pre-trained embeddings for different tokens.\n", "link": "http://arxiv.org/abs/2310.20620v2", "date": "2024-04-02", "relevancy": 1.4329, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4778}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4692}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Unreasonable%20Effectiveness%20of%20Random%20Target%20Embeddings%20for%0A%20%20Continuous-Output%20Neural%20Machine%20Translation&body=Title%3A%20The%20Unreasonable%20Effectiveness%20of%20Random%20Target%20Embeddings%20for%0A%20%20Continuous-Output%20Neural%20Machine%20Translation%0AAuthor%3A%20Evgeniia%20Tokarchuk%20and%20Vlad%20Niculae%0AAbstract%3A%20%20%20Continuous-output%20neural%20machine%20translation%20%28CoNMT%29%20replaces%20the%20discrete%0Anext-word%20prediction%20problem%20with%20an%20embedding%20prediction.%20The%20semantic%0Astructure%20of%20the%20target%20embedding%20space%20%28i.e.%2C%20closeness%20of%20related%20words%29%20is%0Aintuitively%20believed%20to%20be%20crucial.%20We%20challenge%20this%20assumption%20and%20show%20that%0Acompletely%20random%20output%20embeddings%20can%20outperform%20laboriously%20pretrained%20ones%2C%0Aespecially%20on%20larger%20datasets.%20Further%20investigation%20shows%20this%20surprising%0Aeffect%20is%20strongest%20for%20rare%20words%2C%20due%20to%20the%20geometry%20of%20their%20embeddings.%20We%0Ashed%20further%20light%20on%20this%20finding%20by%20designing%20a%20mixed%20strategy%20that%20combines%0Arandom%20and%20pre-trained%20embeddings%20for%20different%20tokens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20620v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Unreasonable%20Effectiveness%20of%20Random%20Target%20Embeddings%20for%0A%20%20Continuous-Output%20Neural%20Machine%20Translation&entry.906535625=Evgeniia%20Tokarchuk%20and%20Vlad%20Niculae&entry.1292438233=%20%20Continuous-output%20neural%20machine%20translation%20%28CoNMT%29%20replaces%20the%20discrete%0Anext-word%20prediction%20problem%20with%20an%20embedding%20prediction.%20The%20semantic%0Astructure%20of%20the%20target%20embedding%20space%20%28i.e.%2C%20closeness%20of%20related%20words%29%20is%0Aintuitively%20believed%20to%20be%20crucial.%20We%20challenge%20this%20assumption%20and%20show%20that%0Acompletely%20random%20output%20embeddings%20can%20outperform%20laboriously%20pretrained%20ones%2C%0Aespecially%20on%20larger%20datasets.%20Further%20investigation%20shows%20this%20surprising%0Aeffect%20is%20strongest%20for%20rare%20words%2C%20due%20to%20the%20geometry%20of%20their%20embeddings.%20We%0Ashed%20further%20light%20on%20this%20finding%20by%20designing%20a%20mixed%20strategy%20that%20combines%0Arandom%20and%20pre-trained%20embeddings%20for%20different%20tokens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20620v2&entry.124074799=Read"},
{"title": "Alpha Invariance: On Inverse Scaling Between Distance and Volume Density\n  in Neural Radiance Fields", "author": "Joshua Ahn and Haochen Wang and Raymond A. Yeh and Greg Shakhnarovich", "abstract": "  Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of\nvolumetric densities in neural radiance fields, i.e., the densities double when\nscene size is halved, and vice versa. We call this property alpha invariance.\nFor NeRFs to better maintain alpha invariance, we recommend 1) parameterizing\nboth distance and volume densities in log space, and 2) a\ndiscretization-agnostic initialization strategy to guarantee high ray\ntransmittance. We revisit a few popular radiance field models and find that\nthese systems use various heuristics to deal with issues arising from scene\nscaling. We test their behaviors and show our recipe to be more robust.\n", "link": "http://arxiv.org/abs/2404.02155v1", "date": "2024-04-02", "relevancy": 1.4292, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5236}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4769}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4573}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Alpha%20Invariance%3A%20On%20Inverse%20Scaling%20Between%20Distance%20and%20Volume%20Density%0A%20%20in%20Neural%20Radiance%20Fields&body=Title%3A%20Alpha%20Invariance%3A%20On%20Inverse%20Scaling%20Between%20Distance%20and%20Volume%20Density%0A%20%20in%20Neural%20Radiance%20Fields%0AAuthor%3A%20Joshua%20Ahn%20and%20Haochen%20Wang%20and%20Raymond%20A.%20Yeh%20and%20Greg%20Shakhnarovich%0AAbstract%3A%20%20%20Scale-ambiguity%20in%203D%20scene%20dimensions%20leads%20to%20magnitude-ambiguity%20of%0Avolumetric%20densities%20in%20neural%20radiance%20fields%2C%20i.e.%2C%20the%20densities%20double%20when%0Ascene%20size%20is%20halved%2C%20and%20vice%20versa.%20We%20call%20this%20property%20alpha%20invariance.%0AFor%20NeRFs%20to%20better%20maintain%20alpha%20invariance%2C%20we%20recommend%201%29%20parameterizing%0Aboth%20distance%20and%20volume%20densities%20in%20log%20space%2C%20and%202%29%20a%0Adiscretization-agnostic%20initialization%20strategy%20to%20guarantee%20high%20ray%0Atransmittance.%20We%20revisit%20a%20few%20popular%20radiance%20field%20models%20and%20find%20that%0Athese%20systems%20use%20various%20heuristics%20to%20deal%20with%20issues%20arising%20from%20scene%0Ascaling.%20We%20test%20their%20behaviors%20and%20show%20our%20recipe%20to%20be%20more%20robust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02155v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alpha%20Invariance%3A%20On%20Inverse%20Scaling%20Between%20Distance%20and%20Volume%20Density%0A%20%20in%20Neural%20Radiance%20Fields&entry.906535625=Joshua%20Ahn%20and%20Haochen%20Wang%20and%20Raymond%20A.%20Yeh%20and%20Greg%20Shakhnarovich&entry.1292438233=%20%20Scale-ambiguity%20in%203D%20scene%20dimensions%20leads%20to%20magnitude-ambiguity%20of%0Avolumetric%20densities%20in%20neural%20radiance%20fields%2C%20i.e.%2C%20the%20densities%20double%20when%0Ascene%20size%20is%20halved%2C%20and%20vice%20versa.%20We%20call%20this%20property%20alpha%20invariance.%0AFor%20NeRFs%20to%20better%20maintain%20alpha%20invariance%2C%20we%20recommend%201%29%20parameterizing%0Aboth%20distance%20and%20volume%20densities%20in%20log%20space%2C%20and%202%29%20a%0Adiscretization-agnostic%20initialization%20strategy%20to%20guarantee%20high%20ray%0Atransmittance.%20We%20revisit%20a%20few%20popular%20radiance%20field%20models%20and%20find%20that%0Athese%20systems%20use%20various%20heuristics%20to%20deal%20with%20issues%20arising%20from%20scene%0Ascaling.%20We%20test%20their%20behaviors%20and%20show%20our%20recipe%20to%20be%20more%20robust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02155v1&entry.124074799=Read"},
{"title": "Measuring and Controlling Instruction (In)Stability in Language Model\n  Dialogs", "author": "Kenneth Li and Tianle Liu and Naomi Bashkansky and David Bau and Fernanda Vi\u00e9gas and Hanspeter Pfister and Martin Wattenberg", "abstract": "  System-prompting is a standard tool for customizing language-model chatbots,\nenabling them to follow a specific instruction. An implicit assumption in the\nuse of system prompts is that they will be stable, so the chatbot will continue\nto generate text according to the stipulated instructions for the duration of a\nconversation. We propose a quantitative benchmark to test this assumption,\nevaluating instruction stability via self-chats between two instructed\nchatbots. Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a\nsignificant instruction drift within eight rounds of conversations. An\nempirical and theoretical analysis of this phenomenon suggests the transformer\nattention mechanism plays a role, due to attention decay over long exchanges.\nTo combat attention decay and instruction drift, we propose a lightweight\nmethod called split-softmax, which compares favorably against two strong\nbaselines.\n", "link": "http://arxiv.org/abs/2402.10962v2", "date": "2024-04-02", "relevancy": 1.4268, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4876}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4804}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4406}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Measuring%20and%20Controlling%20Instruction%20%28In%29Stability%20in%20Language%20Model%0A%20%20Dialogs&body=Title%3A%20Measuring%20and%20Controlling%20Instruction%20%28In%29Stability%20in%20Language%20Model%0A%20%20Dialogs%0AAuthor%3A%20Kenneth%20Li%20and%20Tianle%20Liu%20and%20Naomi%20Bashkansky%20and%20David%20Bau%20and%20Fernanda%20Vi%C3%A9gas%20and%20Hanspeter%20Pfister%20and%20Martin%20Wattenberg%0AAbstract%3A%20%20%20System-prompting%20is%20a%20standard%20tool%20for%20customizing%20language-model%20chatbots%2C%0Aenabling%20them%20to%20follow%20a%20specific%20instruction.%20An%20implicit%20assumption%20in%20the%0Ause%20of%20system%20prompts%20is%20that%20they%20will%20be%20stable%2C%20so%20the%20chatbot%20will%20continue%0Ato%20generate%20text%20according%20to%20the%20stipulated%20instructions%20for%20the%20duration%20of%20a%0Aconversation.%20We%20propose%20a%20quantitative%20benchmark%20to%20test%20this%20assumption%2C%0Aevaluating%20instruction%20stability%20via%20self-chats%20between%20two%20instructed%0Achatbots.%20Testing%20popular%20models%20like%20LLaMA2-chat-70B%20and%20GPT-3.5%2C%20we%20reveal%20a%0Asignificant%20instruction%20drift%20within%20eight%20rounds%20of%20conversations.%20An%0Aempirical%20and%20theoretical%20analysis%20of%20this%20phenomenon%20suggests%20the%20transformer%0Aattention%20mechanism%20plays%20a%20role%2C%20due%20to%20attention%20decay%20over%20long%20exchanges.%0ATo%20combat%20attention%20decay%20and%20instruction%20drift%2C%20we%20propose%20a%20lightweight%0Amethod%20called%20split-softmax%2C%20which%20compares%20favorably%20against%20two%20strong%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10962v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20and%20Controlling%20Instruction%20%28In%29Stability%20in%20Language%20Model%0A%20%20Dialogs&entry.906535625=Kenneth%20Li%20and%20Tianle%20Liu%20and%20Naomi%20Bashkansky%20and%20David%20Bau%20and%20Fernanda%20Vi%C3%A9gas%20and%20Hanspeter%20Pfister%20and%20Martin%20Wattenberg&entry.1292438233=%20%20System-prompting%20is%20a%20standard%20tool%20for%20customizing%20language-model%20chatbots%2C%0Aenabling%20them%20to%20follow%20a%20specific%20instruction.%20An%20implicit%20assumption%20in%20the%0Ause%20of%20system%20prompts%20is%20that%20they%20will%20be%20stable%2C%20so%20the%20chatbot%20will%20continue%0Ato%20generate%20text%20according%20to%20the%20stipulated%20instructions%20for%20the%20duration%20of%20a%0Aconversation.%20We%20propose%20a%20quantitative%20benchmark%20to%20test%20this%20assumption%2C%0Aevaluating%20instruction%20stability%20via%20self-chats%20between%20two%20instructed%0Achatbots.%20Testing%20popular%20models%20like%20LLaMA2-chat-70B%20and%20GPT-3.5%2C%20we%20reveal%20a%0Asignificant%20instruction%20drift%20within%20eight%20rounds%20of%20conversations.%20An%0Aempirical%20and%20theoretical%20analysis%20of%20this%20phenomenon%20suggests%20the%20transformer%0Aattention%20mechanism%20plays%20a%20role%2C%20due%20to%20attention%20decay%20over%20long%20exchanges.%0ATo%20combat%20attention%20decay%20and%20instruction%20drift%2C%20we%20propose%20a%20lightweight%0Amethod%20called%20split-softmax%2C%20which%20compares%20favorably%20against%20two%20strong%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10962v2&entry.124074799=Read"},
{"title": "MultiParaDetox: Extending Text Detoxification with Parallel Data to New\n  Languages", "author": "Daryna Dementieva and Nikolay Babakov and Alexander Panchenko", "abstract": "  Text detoxification is a textual style transfer (TST) task where a text is\nparaphrased from a toxic surface form, e.g. featuring rude words, to the\nneutral register. Recently, text detoxification methods found their\napplications in various task such as detoxification of Large Language Models\n(LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic\nspeech combating in social networks (Deng et al., 2023; Mun et al., 2023;\nAgarwal et al., 2023). All these applications are extremely important to ensure\nsafe communication in modern digital worlds. However, the previous approaches\nfor parallel text detoxification corpora collection -- ParaDetox (Logacheva et\nal., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in\nmonolingual setup. In this work, we aim to extend ParaDetox pipeline to\nmultiple languages presenting MultiParaDetox to automate parallel\ndetoxification corpus collection for potentially any language. Then, we\nexperiment with different text detoxification models -- from unsupervised\nbaselines to LLMs and fine-tuned models on the presented parallel corpora --\nshowing the great benefit of parallel corpus presence to obtain\nstate-of-the-art text detoxification models for any language.\n", "link": "http://arxiv.org/abs/2404.02037v1", "date": "2024-04-02", "relevancy": 1.4167, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4788}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4664}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4616}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MultiParaDetox%3A%20Extending%20Text%20Detoxification%20with%20Parallel%20Data%20to%20New%0A%20%20Languages&body=Title%3A%20MultiParaDetox%3A%20Extending%20Text%20Detoxification%20with%20Parallel%20Data%20to%20New%0A%20%20Languages%0AAuthor%3A%20Daryna%20Dementieva%20and%20Nikolay%20Babakov%20and%20Alexander%20Panchenko%0AAbstract%3A%20%20%20Text%20detoxification%20is%20a%20textual%20style%20transfer%20%28TST%29%20task%20where%20a%20text%20is%0Aparaphrased%20from%20a%20toxic%20surface%20form%2C%20e.g.%20featuring%20rude%20words%2C%20to%20the%0Aneutral%20register.%20Recently%2C%20text%20detoxification%20methods%20found%20their%0Aapplications%20in%20various%20task%20such%20as%20detoxification%20of%20Large%20Language%20Models%0A%28LLMs%29%20%28Leong%20et%20al.%2C%202023%3B%20He%20et%20al.%2C%202024%3B%20Tang%20et%20al.%2C%202023%29%20and%20toxic%0Aspeech%20combating%20in%20social%20networks%20%28Deng%20et%20al.%2C%202023%3B%20Mun%20et%20al.%2C%202023%3B%0AAgarwal%20et%20al.%2C%202023%29.%20All%20these%20applications%20are%20extremely%20important%20to%20ensure%0Asafe%20communication%20in%20modern%20digital%20worlds.%20However%2C%20the%20previous%20approaches%0Afor%20parallel%20text%20detoxification%20corpora%20collection%20--%20ParaDetox%20%28Logacheva%20et%0Aal.%2C%202022%29%20and%20APPADIA%20%28Atwell%20et%20al.%2C%202022%29%20--%20were%20explored%20only%20in%0Amonolingual%20setup.%20In%20this%20work%2C%20we%20aim%20to%20extend%20ParaDetox%20pipeline%20to%0Amultiple%20languages%20presenting%20MultiParaDetox%20to%20automate%20parallel%0Adetoxification%20corpus%20collection%20for%20potentially%20any%20language.%20Then%2C%20we%0Aexperiment%20with%20different%20text%20detoxification%20models%20--%20from%20unsupervised%0Abaselines%20to%20LLMs%20and%20fine-tuned%20models%20on%20the%20presented%20parallel%20corpora%20--%0Ashowing%20the%20great%20benefit%20of%20parallel%20corpus%20presence%20to%20obtain%0Astate-of-the-art%20text%20detoxification%20models%20for%20any%20language.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02037v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiParaDetox%3A%20Extending%20Text%20Detoxification%20with%20Parallel%20Data%20to%20New%0A%20%20Languages&entry.906535625=Daryna%20Dementieva%20and%20Nikolay%20Babakov%20and%20Alexander%20Panchenko&entry.1292438233=%20%20Text%20detoxification%20is%20a%20textual%20style%20transfer%20%28TST%29%20task%20where%20a%20text%20is%0Aparaphrased%20from%20a%20toxic%20surface%20form%2C%20e.g.%20featuring%20rude%20words%2C%20to%20the%0Aneutral%20register.%20Recently%2C%20text%20detoxification%20methods%20found%20their%0Aapplications%20in%20various%20task%20such%20as%20detoxification%20of%20Large%20Language%20Models%0A%28LLMs%29%20%28Leong%20et%20al.%2C%202023%3B%20He%20et%20al.%2C%202024%3B%20Tang%20et%20al.%2C%202023%29%20and%20toxic%0Aspeech%20combating%20in%20social%20networks%20%28Deng%20et%20al.%2C%202023%3B%20Mun%20et%20al.%2C%202023%3B%0AAgarwal%20et%20al.%2C%202023%29.%20All%20these%20applications%20are%20extremely%20important%20to%20ensure%0Asafe%20communication%20in%20modern%20digital%20worlds.%20However%2C%20the%20previous%20approaches%0Afor%20parallel%20text%20detoxification%20corpora%20collection%20--%20ParaDetox%20%28Logacheva%20et%0Aal.%2C%202022%29%20and%20APPADIA%20%28Atwell%20et%20al.%2C%202022%29%20--%20were%20explored%20only%20in%0Amonolingual%20setup.%20In%20this%20work%2C%20we%20aim%20to%20extend%20ParaDetox%20pipeline%20to%0Amultiple%20languages%20presenting%20MultiParaDetox%20to%20automate%20parallel%0Adetoxification%20corpus%20collection%20for%20potentially%20any%20language.%20Then%2C%20we%0Aexperiment%20with%20different%20text%20detoxification%20models%20--%20from%20unsupervised%0Abaselines%20to%20LLMs%20and%20fine-tuned%20models%20on%20the%20presented%20parallel%20corpora%20--%0Ashowing%20the%20great%20benefit%20of%20parallel%20corpus%20presence%20to%20obtain%0Astate-of-the-art%20text%20detoxification%20models%20for%20any%20language.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02037v1&entry.124074799=Read"},
{"title": "Immature Green Apple Detection and Sizing in Commercial Orchards using\n  YOLOv8 and Shape Fitting Techniques", "author": "Ranjan Sapkota and Dawood Ahmed and Martin Churuvija and Manoj Karkee", "abstract": "  Detecting and estimating size of apples during the early stages of growth is\ncrucial for predicting yield, pest management, and making informed decisions\nrelated to crop-load management, harvest and post-harvest logistics, and\nmarketing. Traditional fruit size measurement methods are laborious and\ntimeconsuming. This study employs the state-of-the-art YOLOv8 object detection\nand instance segmentation algorithm in conjunction with geometric shape fitting\ntechniques on 3D point cloud data to accurately determine the size of immature\ngreen apples (or fruitlet) in a commercial orchard environment. The methodology\nutilized two RGB-D sensors: Intel RealSense D435i and Microsoft Azure Kinect\nDK. Notably, the YOLOv8 instance segmentation models exhibited proficiency in\nimmature green apple detection, with the YOLOv8m-seg model achieving the\nhighest AP@0.5 and AP@0.75 scores of 0.94 and 0.91, respectively. Using the\nellipsoid fitting technique on images from the Azure Kinect, we achieved an\nRMSE of 2.35 mm, MAE of 1.66 mm, MAPE of 6.15 mm, and an R-squared value of 0.9\nin estimating the size of apple fruitlets. Challenges such as partial occlusion\ncaused some error in accurately delineating and sizing green apples using the\nYOLOv8-based segmentation technique, particularly in fruit clusters. In a\ncomparison with 102 outdoor samples, the size estimation technique performed\nbetter on the images acquired with Microsoft Azure Kinect than the same with\nIntel Realsense D435i. This superiority is evident from the metrics: the RMSE\nvalues (2.35 mm for Azure Kinect vs. 9.65 mm for Realsense D435i), MAE values\n(1.66 mm for Azure Kinect vs. 7.8 mm for Realsense D435i), and the R-squared\nvalues (0.9 for Azure Kinect vs. 0.77 for Realsense D435i).\n", "link": "http://arxiv.org/abs/2401.08629v2", "date": "2024-04-02", "relevancy": 1.4148, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4666}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4665}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Immature%20Green%20Apple%20Detection%20and%20Sizing%20in%20Commercial%20Orchards%20using%0A%20%20YOLOv8%20and%20Shape%20Fitting%20Techniques&body=Title%3A%20Immature%20Green%20Apple%20Detection%20and%20Sizing%20in%20Commercial%20Orchards%20using%0A%20%20YOLOv8%20and%20Shape%20Fitting%20Techniques%0AAuthor%3A%20Ranjan%20Sapkota%20and%20Dawood%20Ahmed%20and%20Martin%20Churuvija%20and%20Manoj%20Karkee%0AAbstract%3A%20%20%20Detecting%20and%20estimating%20size%20of%20apples%20during%20the%20early%20stages%20of%20growth%20is%0Acrucial%20for%20predicting%20yield%2C%20pest%20management%2C%20and%20making%20informed%20decisions%0Arelated%20to%20crop-load%20management%2C%20harvest%20and%20post-harvest%20logistics%2C%20and%0Amarketing.%20Traditional%20fruit%20size%20measurement%20methods%20are%20laborious%20and%0Atimeconsuming.%20This%20study%20employs%20the%20state-of-the-art%20YOLOv8%20object%20detection%0Aand%20instance%20segmentation%20algorithm%20in%20conjunction%20with%20geometric%20shape%20fitting%0Atechniques%20on%203D%20point%20cloud%20data%20to%20accurately%20determine%20the%20size%20of%20immature%0Agreen%20apples%20%28or%20fruitlet%29%20in%20a%20commercial%20orchard%20environment.%20The%20methodology%0Autilized%20two%20RGB-D%20sensors%3A%20Intel%20RealSense%20D435i%20and%20Microsoft%20Azure%20Kinect%0ADK.%20Notably%2C%20the%20YOLOv8%20instance%20segmentation%20models%20exhibited%20proficiency%20in%0Aimmature%20green%20apple%20detection%2C%20with%20the%20YOLOv8m-seg%20model%20achieving%20the%0Ahighest%20AP%400.5%20and%20AP%400.75%20scores%20of%200.94%20and%200.91%2C%20respectively.%20Using%20the%0Aellipsoid%20fitting%20technique%20on%20images%20from%20the%20Azure%20Kinect%2C%20we%20achieved%20an%0ARMSE%20of%202.35%20mm%2C%20MAE%20of%201.66%20mm%2C%20MAPE%20of%206.15%20mm%2C%20and%20an%20R-squared%20value%20of%200.9%0Ain%20estimating%20the%20size%20of%20apple%20fruitlets.%20Challenges%20such%20as%20partial%20occlusion%0Acaused%20some%20error%20in%20accurately%20delineating%20and%20sizing%20green%20apples%20using%20the%0AYOLOv8-based%20segmentation%20technique%2C%20particularly%20in%20fruit%20clusters.%20In%20a%0Acomparison%20with%20102%20outdoor%20samples%2C%20the%20size%20estimation%20technique%20performed%0Abetter%20on%20the%20images%20acquired%20with%20Microsoft%20Azure%20Kinect%20than%20the%20same%20with%0AIntel%20Realsense%20D435i.%20This%20superiority%20is%20evident%20from%20the%20metrics%3A%20the%20RMSE%0Avalues%20%282.35%20mm%20for%20Azure%20Kinect%20vs.%209.65%20mm%20for%20Realsense%20D435i%29%2C%20MAE%20values%0A%281.66%20mm%20for%20Azure%20Kinect%20vs.%207.8%20mm%20for%20Realsense%20D435i%29%2C%20and%20the%20R-squared%0Avalues%20%280.9%20for%20Azure%20Kinect%20vs.%200.77%20for%20Realsense%20D435i%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08629v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Immature%20Green%20Apple%20Detection%20and%20Sizing%20in%20Commercial%20Orchards%20using%0A%20%20YOLOv8%20and%20Shape%20Fitting%20Techniques&entry.906535625=Ranjan%20Sapkota%20and%20Dawood%20Ahmed%20and%20Martin%20Churuvija%20and%20Manoj%20Karkee&entry.1292438233=%20%20Detecting%20and%20estimating%20size%20of%20apples%20during%20the%20early%20stages%20of%20growth%20is%0Acrucial%20for%20predicting%20yield%2C%20pest%20management%2C%20and%20making%20informed%20decisions%0Arelated%20to%20crop-load%20management%2C%20harvest%20and%20post-harvest%20logistics%2C%20and%0Amarketing.%20Traditional%20fruit%20size%20measurement%20methods%20are%20laborious%20and%0Atimeconsuming.%20This%20study%20employs%20the%20state-of-the-art%20YOLOv8%20object%20detection%0Aand%20instance%20segmentation%20algorithm%20in%20conjunction%20with%20geometric%20shape%20fitting%0Atechniques%20on%203D%20point%20cloud%20data%20to%20accurately%20determine%20the%20size%20of%20immature%0Agreen%20apples%20%28or%20fruitlet%29%20in%20a%20commercial%20orchard%20environment.%20The%20methodology%0Autilized%20two%20RGB-D%20sensors%3A%20Intel%20RealSense%20D435i%20and%20Microsoft%20Azure%20Kinect%0ADK.%20Notably%2C%20the%20YOLOv8%20instance%20segmentation%20models%20exhibited%20proficiency%20in%0Aimmature%20green%20apple%20detection%2C%20with%20the%20YOLOv8m-seg%20model%20achieving%20the%0Ahighest%20AP%400.5%20and%20AP%400.75%20scores%20of%200.94%20and%200.91%2C%20respectively.%20Using%20the%0Aellipsoid%20fitting%20technique%20on%20images%20from%20the%20Azure%20Kinect%2C%20we%20achieved%20an%0ARMSE%20of%202.35%20mm%2C%20MAE%20of%201.66%20mm%2C%20MAPE%20of%206.15%20mm%2C%20and%20an%20R-squared%20value%20of%200.9%0Ain%20estimating%20the%20size%20of%20apple%20fruitlets.%20Challenges%20such%20as%20partial%20occlusion%0Acaused%20some%20error%20in%20accurately%20delineating%20and%20sizing%20green%20apples%20using%20the%0AYOLOv8-based%20segmentation%20technique%2C%20particularly%20in%20fruit%20clusters.%20In%20a%0Acomparison%20with%20102%20outdoor%20samples%2C%20the%20size%20estimation%20technique%20performed%0Abetter%20on%20the%20images%20acquired%20with%20Microsoft%20Azure%20Kinect%20than%20the%20same%20with%0AIntel%20Realsense%20D435i.%20This%20superiority%20is%20evident%20from%20the%20metrics%3A%20the%20RMSE%0Avalues%20%282.35%20mm%20for%20Azure%20Kinect%20vs.%209.65%20mm%20for%20Realsense%20D435i%29%2C%20MAE%20values%0A%281.66%20mm%20for%20Azure%20Kinect%20vs.%207.8%20mm%20for%20Realsense%20D435i%29%2C%20and%20the%20R-squared%0Avalues%20%280.9%20for%20Azure%20Kinect%20vs.%200.77%20for%20Realsense%20D435i%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08629v2&entry.124074799=Read"},
{"title": "Large Human Language Models: A Need and the Challenges", "author": "Nikita Soni and H. Andrew Schwartz and Jo\u00e3o Sedoc and Niranjan Balasubramanian", "abstract": "  As research in human-centered NLP advances, there is a growing recognition of\nthe importance of incorporating human and social factors into NLP models. At\nthe same time, our NLP systems have become heavily reliant on LLMs, most of\nwhich do not model authors. To build NLP systems that can truly understand\nhuman language, we must better integrate human contexts into LLMs. This brings\nto the fore a range of design considerations and challenges in terms of what\nhuman aspects to capture, how to represent them, and what modeling strategies\nto pursue. To address these, we advocate for three positions toward creating\nlarge human language models (LHLMs) using concepts from psychological and\nbehavioral sciences: First, LM training should include the human context.\nSecond, LHLMs should recognize that people are more than their group(s). Third,\nLHLMs should be able to account for the dynamic and temporally-dependent nature\nof the human context. We refer to relevant advances and present open challenges\nthat need to be addressed and their possible solutions in realizing these\ngoals.\n", "link": "http://arxiv.org/abs/2312.07751v2", "date": "2024-04-02", "relevancy": 1.4038, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4769}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4604}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4531}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Human%20Language%20Models%3A%20A%20Need%20and%20the%20Challenges&body=Title%3A%20Large%20Human%20Language%20Models%3A%20A%20Need%20and%20the%20Challenges%0AAuthor%3A%20Nikita%20Soni%20and%20H.%20Andrew%20Schwartz%20and%20Jo%C3%A3o%20Sedoc%20and%20Niranjan%20Balasubramanian%0AAbstract%3A%20%20%20As%20research%20in%20human-centered%20NLP%20advances%2C%20there%20is%20a%20growing%20recognition%20of%0Athe%20importance%20of%20incorporating%20human%20and%20social%20factors%20into%20NLP%20models.%20At%0Athe%20same%20time%2C%20our%20NLP%20systems%20have%20become%20heavily%20reliant%20on%20LLMs%2C%20most%20of%0Awhich%20do%20not%20model%20authors.%20To%20build%20NLP%20systems%20that%20can%20truly%20understand%0Ahuman%20language%2C%20we%20must%20better%20integrate%20human%20contexts%20into%20LLMs.%20This%20brings%0Ato%20the%20fore%20a%20range%20of%20design%20considerations%20and%20challenges%20in%20terms%20of%20what%0Ahuman%20aspects%20to%20capture%2C%20how%20to%20represent%20them%2C%20and%20what%20modeling%20strategies%0Ato%20pursue.%20To%20address%20these%2C%20we%20advocate%20for%20three%20positions%20toward%20creating%0Alarge%20human%20language%20models%20%28LHLMs%29%20using%20concepts%20from%20psychological%20and%0Abehavioral%20sciences%3A%20First%2C%20LM%20training%20should%20include%20the%20human%20context.%0ASecond%2C%20LHLMs%20should%20recognize%20that%20people%20are%20more%20than%20their%20group%28s%29.%20Third%2C%0ALHLMs%20should%20be%20able%20to%20account%20for%20the%20dynamic%20and%20temporally-dependent%20nature%0Aof%20the%20human%20context.%20We%20refer%20to%20relevant%20advances%20and%20present%20open%20challenges%0Athat%20need%20to%20be%20addressed%20and%20their%20possible%20solutions%20in%20realizing%20these%0Agoals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07751v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Human%20Language%20Models%3A%20A%20Need%20and%20the%20Challenges&entry.906535625=Nikita%20Soni%20and%20H.%20Andrew%20Schwartz%20and%20Jo%C3%A3o%20Sedoc%20and%20Niranjan%20Balasubramanian&entry.1292438233=%20%20As%20research%20in%20human-centered%20NLP%20advances%2C%20there%20is%20a%20growing%20recognition%20of%0Athe%20importance%20of%20incorporating%20human%20and%20social%20factors%20into%20NLP%20models.%20At%0Athe%20same%20time%2C%20our%20NLP%20systems%20have%20become%20heavily%20reliant%20on%20LLMs%2C%20most%20of%0Awhich%20do%20not%20model%20authors.%20To%20build%20NLP%20systems%20that%20can%20truly%20understand%0Ahuman%20language%2C%20we%20must%20better%20integrate%20human%20contexts%20into%20LLMs.%20This%20brings%0Ato%20the%20fore%20a%20range%20of%20design%20considerations%20and%20challenges%20in%20terms%20of%20what%0Ahuman%20aspects%20to%20capture%2C%20how%20to%20represent%20them%2C%20and%20what%20modeling%20strategies%0Ato%20pursue.%20To%20address%20these%2C%20we%20advocate%20for%20three%20positions%20toward%20creating%0Alarge%20human%20language%20models%20%28LHLMs%29%20using%20concepts%20from%20psychological%20and%0Abehavioral%20sciences%3A%20First%2C%20LM%20training%20should%20include%20the%20human%20context.%0ASecond%2C%20LHLMs%20should%20recognize%20that%20people%20are%20more%20than%20their%20group%28s%29.%20Third%2C%0ALHLMs%20should%20be%20able%20to%20account%20for%20the%20dynamic%20and%20temporally-dependent%20nature%0Aof%20the%20human%20context.%20We%20refer%20to%20relevant%20advances%20and%20present%20open%20challenges%0Athat%20need%20to%20be%20addressed%20and%20their%20possible%20solutions%20in%20realizing%20these%0Agoals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07751v2&entry.124074799=Read"},
{"title": "Transformers as Transducers", "author": "Lena Strobl and Dana Angluin and David Chiang and Jonathan Rawski and Ashish Sabharwal", "abstract": "  We study the sequence-to-sequence mapping capacity of transformers by\nrelating them to finite transducers, and find that they can express\nsurprisingly large classes of transductions. We do so using variants of RASP, a\nprogramming language designed to help people \"think like transformers,\" as an\nintermediate representation. We extend the existing Boolean variant B-RASP to\nsequence-to-sequence functions and show that it computes exactly the\nfirst-order rational functions (such as string rotation). Then, we introduce\ntwo new extensions. B-RASP[pos] enables calculations on positions (such as\ncopying the first half of a string) and contains all first-order regular\nfunctions. S-RASP adds prefix sum, which enables additional arithmetic\noperations (such as squaring a string) and contains all first-order polyregular\nfunctions. Finally, we show that masked average-hard attention transformers can\nsimulate S-RASP. A corollary of our results is a new proof that transformer\ndecoders are Turing-complete.\n", "link": "http://arxiv.org/abs/2404.02040v1", "date": "2024-04-02", "relevancy": 1.401, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4519}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4371}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Transformers%20as%20Transducers&body=Title%3A%20Transformers%20as%20Transducers%0AAuthor%3A%20Lena%20Strobl%20and%20Dana%20Angluin%20and%20David%20Chiang%20and%20Jonathan%20Rawski%20and%20Ashish%20Sabharwal%0AAbstract%3A%20%20%20We%20study%20the%20sequence-to-sequence%20mapping%20capacity%20of%20transformers%20by%0Arelating%20them%20to%20finite%20transducers%2C%20and%20find%20that%20they%20can%20express%0Asurprisingly%20large%20classes%20of%20transductions.%20We%20do%20so%20using%20variants%20of%20RASP%2C%20a%0Aprogramming%20language%20designed%20to%20help%20people%20%22think%20like%20transformers%2C%22%20as%20an%0Aintermediate%20representation.%20We%20extend%20the%20existing%20Boolean%20variant%20B-RASP%20to%0Asequence-to-sequence%20functions%20and%20show%20that%20it%20computes%20exactly%20the%0Afirst-order%20rational%20functions%20%28such%20as%20string%20rotation%29.%20Then%2C%20we%20introduce%0Atwo%20new%20extensions.%20B-RASP%5Bpos%5D%20enables%20calculations%20on%20positions%20%28such%20as%0Acopying%20the%20first%20half%20of%20a%20string%29%20and%20contains%20all%20first-order%20regular%0Afunctions.%20S-RASP%20adds%20prefix%20sum%2C%20which%20enables%20additional%20arithmetic%0Aoperations%20%28such%20as%20squaring%20a%20string%29%20and%20contains%20all%20first-order%20polyregular%0Afunctions.%20Finally%2C%20we%20show%20that%20masked%20average-hard%20attention%20transformers%20can%0Asimulate%20S-RASP.%20A%20corollary%20of%20our%20results%20is%20a%20new%20proof%20that%20transformer%0Adecoders%20are%20Turing-complete.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02040v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20as%20Transducers&entry.906535625=Lena%20Strobl%20and%20Dana%20Angluin%20and%20David%20Chiang%20and%20Jonathan%20Rawski%20and%20Ashish%20Sabharwal&entry.1292438233=%20%20We%20study%20the%20sequence-to-sequence%20mapping%20capacity%20of%20transformers%20by%0Arelating%20them%20to%20finite%20transducers%2C%20and%20find%20that%20they%20can%20express%0Asurprisingly%20large%20classes%20of%20transductions.%20We%20do%20so%20using%20variants%20of%20RASP%2C%20a%0Aprogramming%20language%20designed%20to%20help%20people%20%22think%20like%20transformers%2C%22%20as%20an%0Aintermediate%20representation.%20We%20extend%20the%20existing%20Boolean%20variant%20B-RASP%20to%0Asequence-to-sequence%20functions%20and%20show%20that%20it%20computes%20exactly%20the%0Afirst-order%20rational%20functions%20%28such%20as%20string%20rotation%29.%20Then%2C%20we%20introduce%0Atwo%20new%20extensions.%20B-RASP%5Bpos%5D%20enables%20calculations%20on%20positions%20%28such%20as%0Acopying%20the%20first%20half%20of%20a%20string%29%20and%20contains%20all%20first-order%20regular%0Afunctions.%20S-RASP%20adds%20prefix%20sum%2C%20which%20enables%20additional%20arithmetic%0Aoperations%20%28such%20as%20squaring%20a%20string%29%20and%20contains%20all%20first-order%20polyregular%0Afunctions.%20Finally%2C%20we%20show%20that%20masked%20average-hard%20attention%20transformers%20can%0Asimulate%20S-RASP.%20A%20corollary%20of%20our%20results%20is%20a%20new%20proof%20that%20transformer%0Adecoders%20are%20Turing-complete.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02040v1&entry.124074799=Read"},
{"title": "Fashion Style Editing with Generative Human Prior", "author": "Chaerin Kong and Seungyong Lee and Soohyeok Im and Wonsuk Yang", "abstract": "  Image editing has been a long-standing challenge in the research community\nwith its far-reaching impact on numerous applications. Recently, text-driven\nmethods started to deliver promising results in domains like human faces, but\ntheir applications to more complex domains have been relatively limited. In\nthis work, we explore the task of fashion style editing, where we aim to\nmanipulate the fashion style of human imagery using text descriptions.\nSpecifically, we leverage a generative human prior and achieve fashion style\nediting by navigating its learned latent space. We first verify that the\nexisting text-driven editing methods fall short for our problem due to their\noverly simplified guidance signal, and propose two directions to reinforce the\nguidance: textual augmentation and visual referencing. Combined with our\nempirical findings on the latent space structure, our Fashion Style Editing\nframework (FaSE) successfully projects abstract fashion concepts onto human\nimages and introduces exciting new applications to the field.\n", "link": "http://arxiv.org/abs/2404.01984v1", "date": "2024-04-02", "relevancy": 1.1901, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7175}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5347}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5329}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fashion%20Style%20Editing%20with%20Generative%20Human%20Prior&body=Title%3A%20Fashion%20Style%20Editing%20with%20Generative%20Human%20Prior%0AAuthor%3A%20Chaerin%20Kong%20and%20Seungyong%20Lee%20and%20Soohyeok%20Im%20and%20Wonsuk%20Yang%0AAbstract%3A%20%20%20Image%20editing%20has%20been%20a%20long-standing%20challenge%20in%20the%20research%20community%0Awith%20its%20far-reaching%20impact%20on%20numerous%20applications.%20Recently%2C%20text-driven%0Amethods%20started%20to%20deliver%20promising%20results%20in%20domains%20like%20human%20faces%2C%20but%0Atheir%20applications%20to%20more%20complex%20domains%20have%20been%20relatively%20limited.%20In%0Athis%20work%2C%20we%20explore%20the%20task%20of%20fashion%20style%20editing%2C%20where%20we%20aim%20to%0Amanipulate%20the%20fashion%20style%20of%20human%20imagery%20using%20text%20descriptions.%0ASpecifically%2C%20we%20leverage%20a%20generative%20human%20prior%20and%20achieve%20fashion%20style%0Aediting%20by%20navigating%20its%20learned%20latent%20space.%20We%20first%20verify%20that%20the%0Aexisting%20text-driven%20editing%20methods%20fall%20short%20for%20our%20problem%20due%20to%20their%0Aoverly%20simplified%20guidance%20signal%2C%20and%20propose%20two%20directions%20to%20reinforce%20the%0Aguidance%3A%20textual%20augmentation%20and%20visual%20referencing.%20Combined%20with%20our%0Aempirical%20findings%20on%20the%20latent%20space%20structure%2C%20our%20Fashion%20Style%20Editing%0Aframework%20%28FaSE%29%20successfully%20projects%20abstract%20fashion%20concepts%20onto%20human%0Aimages%20and%20introduces%20exciting%20new%20applications%20to%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01984v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fashion%20Style%20Editing%20with%20Generative%20Human%20Prior&entry.906535625=Chaerin%20Kong%20and%20Seungyong%20Lee%20and%20Soohyeok%20Im%20and%20Wonsuk%20Yang&entry.1292438233=%20%20Image%20editing%20has%20been%20a%20long-standing%20challenge%20in%20the%20research%20community%0Awith%20its%20far-reaching%20impact%20on%20numerous%20applications.%20Recently%2C%20text-driven%0Amethods%20started%20to%20deliver%20promising%20results%20in%20domains%20like%20human%20faces%2C%20but%0Atheir%20applications%20to%20more%20complex%20domains%20have%20been%20relatively%20limited.%20In%0Athis%20work%2C%20we%20explore%20the%20task%20of%20fashion%20style%20editing%2C%20where%20we%20aim%20to%0Amanipulate%20the%20fashion%20style%20of%20human%20imagery%20using%20text%20descriptions.%0ASpecifically%2C%20we%20leverage%20a%20generative%20human%20prior%20and%20achieve%20fashion%20style%0Aediting%20by%20navigating%20its%20learned%20latent%20space.%20We%20first%20verify%20that%20the%0Aexisting%20text-driven%20editing%20methods%20fall%20short%20for%20our%20problem%20due%20to%20their%0Aoverly%20simplified%20guidance%20signal%2C%20and%20propose%20two%20directions%20to%20reinforce%20the%0Aguidance%3A%20textual%20augmentation%20and%20visual%20referencing.%20Combined%20with%20our%0Aempirical%20findings%20on%20the%20latent%20space%20structure%2C%20our%20Fashion%20Style%20Editing%0Aframework%20%28FaSE%29%20successfully%20projects%20abstract%20fashion%20concepts%20onto%20human%0Aimages%20and%20introduces%20exciting%20new%20applications%20to%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01984v1&entry.124074799=Read"},
{"title": "GDA: Generalized Diffusion for Robust Test-time Adaptation", "author": "Yun-Yun Tsai and Fu-Chen Chen and Albert Y. C. Chen and Junfeng Yang and Che-Chun Su and Min Sun and Cheng-Hao Kuo", "abstract": "  Machine learning models struggle with generalization when encountering\nout-of-distribution (OOD) samples with unexpected distribution shifts. For\nvision tasks, recent studies have shown that test-time adaptation employing\ndiffusion models can achieve state-of-the-art accuracy improvements on OOD\nsamples by generating new samples that align with the model's domain without\nthe need to modify the model's weights. Unfortunately, those studies have\nprimarily focused on pixel-level corruptions, thereby lacking the\ngeneralization to adapt to a broader range of OOD types. We introduce\nGeneralized Diffusion Adaptation (GDA), a novel diffusion-based test-time\nadaptation method robust against diverse OOD types. Specifically, GDA\niteratively guides the diffusion by applying a marginal entropy loss derived\nfrom the model, in conjunction with style and content preservation losses\nduring the reverse sampling process. In other words, GDA considers the model's\noutput behavior with the semantic information of the samples as a whole, which\ncan reduce ambiguity in downstream tasks during the generation process.\nEvaluation across various popular model architectures and OOD benchmarks shows\nthat GDA consistently outperforms prior work on diffusion-driven adaptation.\nNotably, it achieves the highest classification accuracy improvements, ranging\nfrom 4.4\\% to 5.02\\% on ImageNet-C and 2.5\\% to 7.4\\% on Rendition, Sketch, and\nStylized benchmarks. This performance highlights GDA's generalization to a\nbroader range of OOD benchmarks.\n", "link": "http://arxiv.org/abs/2404.00095v2", "date": "2024-04-02", "relevancy": 1.153, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6125}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5595}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5574}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GDA%3A%20Generalized%20Diffusion%20for%20Robust%20Test-time%20Adaptation&body=Title%3A%20GDA%3A%20Generalized%20Diffusion%20for%20Robust%20Test-time%20Adaptation%0AAuthor%3A%20Yun-Yun%20Tsai%20and%20Fu-Chen%20Chen%20and%20Albert%20Y.%20C.%20Chen%20and%20Junfeng%20Yang%20and%20Che-Chun%20Su%20and%20Min%20Sun%20and%20Cheng-Hao%20Kuo%0AAbstract%3A%20%20%20Machine%20learning%20models%20struggle%20with%20generalization%20when%20encountering%0Aout-of-distribution%20%28OOD%29%20samples%20with%20unexpected%20distribution%20shifts.%20For%0Avision%20tasks%2C%20recent%20studies%20have%20shown%20that%20test-time%20adaptation%20employing%0Adiffusion%20models%20can%20achieve%20state-of-the-art%20accuracy%20improvements%20on%20OOD%0Asamples%20by%20generating%20new%20samples%20that%20align%20with%20the%20model%27s%20domain%20without%0Athe%20need%20to%20modify%20the%20model%27s%20weights.%20Unfortunately%2C%20those%20studies%20have%0Aprimarily%20focused%20on%20pixel-level%20corruptions%2C%20thereby%20lacking%20the%0Ageneralization%20to%20adapt%20to%20a%20broader%20range%20of%20OOD%20types.%20We%20introduce%0AGeneralized%20Diffusion%20Adaptation%20%28GDA%29%2C%20a%20novel%20diffusion-based%20test-time%0Aadaptation%20method%20robust%20against%20diverse%20OOD%20types.%20Specifically%2C%20GDA%0Aiteratively%20guides%20the%20diffusion%20by%20applying%20a%20marginal%20entropy%20loss%20derived%0Afrom%20the%20model%2C%20in%20conjunction%20with%20style%20and%20content%20preservation%20losses%0Aduring%20the%20reverse%20sampling%20process.%20In%20other%20words%2C%20GDA%20considers%20the%20model%27s%0Aoutput%20behavior%20with%20the%20semantic%20information%20of%20the%20samples%20as%20a%20whole%2C%20which%0Acan%20reduce%20ambiguity%20in%20downstream%20tasks%20during%20the%20generation%20process.%0AEvaluation%20across%20various%20popular%20model%20architectures%20and%20OOD%20benchmarks%20shows%0Athat%20GDA%20consistently%20outperforms%20prior%20work%20on%20diffusion-driven%20adaptation.%0ANotably%2C%20it%20achieves%20the%20highest%20classification%20accuracy%20improvements%2C%20ranging%0Afrom%204.4%5C%25%20to%205.02%5C%25%20on%20ImageNet-C%20and%202.5%5C%25%20to%207.4%5C%25%20on%20Rendition%2C%20Sketch%2C%20and%0AStylized%20benchmarks.%20This%20performance%20highlights%20GDA%27s%20generalization%20to%20a%0Abroader%20range%20of%20OOD%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00095v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GDA%3A%20Generalized%20Diffusion%20for%20Robust%20Test-time%20Adaptation&entry.906535625=Yun-Yun%20Tsai%20and%20Fu-Chen%20Chen%20and%20Albert%20Y.%20C.%20Chen%20and%20Junfeng%20Yang%20and%20Che-Chun%20Su%20and%20Min%20Sun%20and%20Cheng-Hao%20Kuo&entry.1292438233=%20%20Machine%20learning%20models%20struggle%20with%20generalization%20when%20encountering%0Aout-of-distribution%20%28OOD%29%20samples%20with%20unexpected%20distribution%20shifts.%20For%0Avision%20tasks%2C%20recent%20studies%20have%20shown%20that%20test-time%20adaptation%20employing%0Adiffusion%20models%20can%20achieve%20state-of-the-art%20accuracy%20improvements%20on%20OOD%0Asamples%20by%20generating%20new%20samples%20that%20align%20with%20the%20model%27s%20domain%20without%0Athe%20need%20to%20modify%20the%20model%27s%20weights.%20Unfortunately%2C%20those%20studies%20have%0Aprimarily%20focused%20on%20pixel-level%20corruptions%2C%20thereby%20lacking%20the%0Ageneralization%20to%20adapt%20to%20a%20broader%20range%20of%20OOD%20types.%20We%20introduce%0AGeneralized%20Diffusion%20Adaptation%20%28GDA%29%2C%20a%20novel%20diffusion-based%20test-time%0Aadaptation%20method%20robust%20against%20diverse%20OOD%20types.%20Specifically%2C%20GDA%0Aiteratively%20guides%20the%20diffusion%20by%20applying%20a%20marginal%20entropy%20loss%20derived%0Afrom%20the%20model%2C%20in%20conjunction%20with%20style%20and%20content%20preservation%20losses%0Aduring%20the%20reverse%20sampling%20process.%20In%20other%20words%2C%20GDA%20considers%20the%20model%27s%0Aoutput%20behavior%20with%20the%20semantic%20information%20of%20the%20samples%20as%20a%20whole%2C%20which%0Acan%20reduce%20ambiguity%20in%20downstream%20tasks%20during%20the%20generation%20process.%0AEvaluation%20across%20various%20popular%20model%20architectures%20and%20OOD%20benchmarks%20shows%0Athat%20GDA%20consistently%20outperforms%20prior%20work%20on%20diffusion-driven%20adaptation.%0ANotably%2C%20it%20achieves%20the%20highest%20classification%20accuracy%20improvements%2C%20ranging%0Afrom%204.4%5C%25%20to%205.02%5C%25%20on%20ImageNet-C%20and%202.5%5C%25%20to%207.4%5C%25%20on%20Rendition%2C%20Sketch%2C%20and%0AStylized%20benchmarks.%20This%20performance%20highlights%20GDA%27s%20generalization%20to%20a%0Abroader%20range%20of%20OOD%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00095v2&entry.124074799=Read"},
{"title": "Variance-Reduced Policy Gradient Approaches for Infinite Horizon Average\n  Reward Markov Decision Processes", "author": "Swetha Ganesh and Washim Uddin Mondal and Vaneet Aggarwal", "abstract": "  We present two Policy Gradient-based methods with general parameterization in\nthe context of infinite horizon average reward Markov Decision Processes. The\nfirst approach employs Implicit Gradient Transport for variance reduction,\nensuring an expected regret of the order $\\tilde{\\mathcal{O}}(T^{3/5})$. The\nsecond approach, rooted in Hessian-based techniques, ensures an expected regret\nof the order $\\tilde{\\mathcal{O}}(\\sqrt{T})$. These results significantly\nimprove the state of the art of the problem, which achieves a regret of\n$\\tilde{\\mathcal{O}}(T^{3/4})$.\n", "link": "http://arxiv.org/abs/2404.02108v1", "date": "2024-04-02", "relevancy": 1.2495, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4258}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4235}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3896}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Variance-Reduced%20Policy%20Gradient%20Approaches%20for%20Infinite%20Horizon%20Average%0A%20%20Reward%20Markov%20Decision%20Processes&body=Title%3A%20Variance-Reduced%20Policy%20Gradient%20Approaches%20for%20Infinite%20Horizon%20Average%0A%20%20Reward%20Markov%20Decision%20Processes%0AAuthor%3A%20Swetha%20Ganesh%20and%20Washim%20Uddin%20Mondal%20and%20Vaneet%20Aggarwal%0AAbstract%3A%20%20%20We%20present%20two%20Policy%20Gradient-based%20methods%20with%20general%20parameterization%20in%0Athe%20context%20of%20infinite%20horizon%20average%20reward%20Markov%20Decision%20Processes.%20The%0Afirst%20approach%20employs%20Implicit%20Gradient%20Transport%20for%20variance%20reduction%2C%0Aensuring%20an%20expected%20regret%20of%20the%20order%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B3/5%7D%29%24.%20The%0Asecond%20approach%2C%20rooted%20in%20Hessian-based%20techniques%2C%20ensures%20an%20expected%20regret%0Aof%20the%20order%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Csqrt%7BT%7D%29%24.%20These%20results%20significantly%0Aimprove%20the%20state%20of%20the%20art%20of%20the%20problem%2C%20which%20achieves%20a%20regret%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B3/4%7D%29%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02108v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variance-Reduced%20Policy%20Gradient%20Approaches%20for%20Infinite%20Horizon%20Average%0A%20%20Reward%20Markov%20Decision%20Processes&entry.906535625=Swetha%20Ganesh%20and%20Washim%20Uddin%20Mondal%20and%20Vaneet%20Aggarwal&entry.1292438233=%20%20We%20present%20two%20Policy%20Gradient-based%20methods%20with%20general%20parameterization%20in%0Athe%20context%20of%20infinite%20horizon%20average%20reward%20Markov%20Decision%20Processes.%20The%0Afirst%20approach%20employs%20Implicit%20Gradient%20Transport%20for%20variance%20reduction%2C%0Aensuring%20an%20expected%20regret%20of%20the%20order%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B3/5%7D%29%24.%20The%0Asecond%20approach%2C%20rooted%20in%20Hessian-based%20techniques%2C%20ensures%20an%20expected%20regret%0Aof%20the%20order%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Csqrt%7BT%7D%29%24.%20These%20results%20significantly%0Aimprove%20the%20state%20of%20the%20art%20of%20the%20problem%2C%20which%20achieves%20a%20regret%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B3/4%7D%29%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02108v1&entry.124074799=Read"},
{"title": "XLB: A differentiable massively parallel lattice Boltzmann library in\n  Python", "author": "Mohammadmehdi Ataei and Hesam Salehipour", "abstract": "  The lattice Boltzmann method (LBM) has emerged as a prominent technique for\nsolving fluid dynamics problems due to its algorithmic potential for\ncomputational scalability. We introduce XLB library, a Python-based\ndifferentiable LBM library based on the JAX platform. The architecture of XLB\nis predicated upon ensuring accessibility, extensibility, and computational\nperformance, enabling scaling effectively across CPU, TPU, multi-GPU, and\ndistributed multi-GPU or TPU systems. The library can be readily augmented with\nnovel boundary conditions, collision models, or multi-physics simulation\ncapabilities. XLB's differentiability and data structure is compatible with the\nextensive JAX-based machine learning ecosystem, enabling it to address\nphysics-based machine learning, optimization, and inverse problems. XLB has\nbeen successfully scaled to handle simulations with billions of cells,\nachieving giga-scale lattice updates per second. XLB is released under the\npermissive Apache-2.0 license and is available on GitHub at\nhttps://github.com/Autodesk/XLB.\n", "link": "http://arxiv.org/abs/2311.16080v3", "date": "2024-04-02", "relevancy": 1.2662, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.44}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4201}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4092}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20XLB%3A%20A%20differentiable%20massively%20parallel%20lattice%20Boltzmann%20library%20in%0A%20%20Python&body=Title%3A%20XLB%3A%20A%20differentiable%20massively%20parallel%20lattice%20Boltzmann%20library%20in%0A%20%20Python%0AAuthor%3A%20Mohammadmehdi%20Ataei%20and%20Hesam%20Salehipour%0AAbstract%3A%20%20%20The%20lattice%20Boltzmann%20method%20%28LBM%29%20has%20emerged%20as%20a%20prominent%20technique%20for%0Asolving%20fluid%20dynamics%20problems%20due%20to%20its%20algorithmic%20potential%20for%0Acomputational%20scalability.%20We%20introduce%20XLB%20library%2C%20a%20Python-based%0Adifferentiable%20LBM%20library%20based%20on%20the%20JAX%20platform.%20The%20architecture%20of%20XLB%0Ais%20predicated%20upon%20ensuring%20accessibility%2C%20extensibility%2C%20and%20computational%0Aperformance%2C%20enabling%20scaling%20effectively%20across%20CPU%2C%20TPU%2C%20multi-GPU%2C%20and%0Adistributed%20multi-GPU%20or%20TPU%20systems.%20The%20library%20can%20be%20readily%20augmented%20with%0Anovel%20boundary%20conditions%2C%20collision%20models%2C%20or%20multi-physics%20simulation%0Acapabilities.%20XLB%27s%20differentiability%20and%20data%20structure%20is%20compatible%20with%20the%0Aextensive%20JAX-based%20machine%20learning%20ecosystem%2C%20enabling%20it%20to%20address%0Aphysics-based%20machine%20learning%2C%20optimization%2C%20and%20inverse%20problems.%20XLB%20has%0Abeen%20successfully%20scaled%20to%20handle%20simulations%20with%20billions%20of%20cells%2C%0Aachieving%20giga-scale%20lattice%20updates%20per%20second.%20XLB%20is%20released%20under%20the%0Apermissive%20Apache-2.0%20license%20and%20is%20available%20on%20GitHub%20at%0Ahttps%3A//github.com/Autodesk/XLB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16080v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XLB%3A%20A%20differentiable%20massively%20parallel%20lattice%20Boltzmann%20library%20in%0A%20%20Python&entry.906535625=Mohammadmehdi%20Ataei%20and%20Hesam%20Salehipour&entry.1292438233=%20%20The%20lattice%20Boltzmann%20method%20%28LBM%29%20has%20emerged%20as%20a%20prominent%20technique%20for%0Asolving%20fluid%20dynamics%20problems%20due%20to%20its%20algorithmic%20potential%20for%0Acomputational%20scalability.%20We%20introduce%20XLB%20library%2C%20a%20Python-based%0Adifferentiable%20LBM%20library%20based%20on%20the%20JAX%20platform.%20The%20architecture%20of%20XLB%0Ais%20predicated%20upon%20ensuring%20accessibility%2C%20extensibility%2C%20and%20computational%0Aperformance%2C%20enabling%20scaling%20effectively%20across%20CPU%2C%20TPU%2C%20multi-GPU%2C%20and%0Adistributed%20multi-GPU%20or%20TPU%20systems.%20The%20library%20can%20be%20readily%20augmented%20with%0Anovel%20boundary%20conditions%2C%20collision%20models%2C%20or%20multi-physics%20simulation%0Acapabilities.%20XLB%27s%20differentiability%20and%20data%20structure%20is%20compatible%20with%20the%0Aextensive%20JAX-based%20machine%20learning%20ecosystem%2C%20enabling%20it%20to%20address%0Aphysics-based%20machine%20learning%2C%20optimization%2C%20and%20inverse%20problems.%20XLB%20has%0Abeen%20successfully%20scaled%20to%20handle%20simulations%20with%20billions%20of%20cells%2C%0Aachieving%20giga-scale%20lattice%20updates%20per%20second.%20XLB%20is%20released%20under%20the%0Apermissive%20Apache-2.0%20license%20and%20is%20available%20on%20GitHub%20at%0Ahttps%3A//github.com/Autodesk/XLB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16080v3&entry.124074799=Read"},
{"title": "Large Language Models for Mathematicians", "author": "Simon Frieder and Julius Berner and Philipp Petersen and Thomas Lukasiewicz", "abstract": "  Large language models (LLMs) such as ChatGPT have received immense interest\nfor their general-purpose language understanding and, in particular, their\nability to generate high-quality text or computer code. For many professions,\nLLMs represent an invaluable tool that can speed up and improve the quality of\nwork. In this note, we discuss to what extent they can aid professional\nmathematicians. We first provide a mathematical description of the transformer\nmodel used in all modern language models. Based on recent studies, we then\noutline best practices and potential issues and report on the mathematical\nabilities of language models. Finally, we shed light on the potential of LLMs\nto change how mathematicians work.\n", "link": "http://arxiv.org/abs/2312.04556v2", "date": "2024-04-02", "relevancy": 1.3592, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4872}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4557}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4124}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Mathematicians&body=Title%3A%20Large%20Language%20Models%20for%20Mathematicians%0AAuthor%3A%20Simon%20Frieder%20and%20Julius%20Berner%20and%20Philipp%20Petersen%20and%20Thomas%20Lukasiewicz%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20such%20as%20ChatGPT%20have%20received%20immense%20interest%0Afor%20their%20general-purpose%20language%20understanding%20and%2C%20in%20particular%2C%20their%0Aability%20to%20generate%20high-quality%20text%20or%20computer%20code.%20For%20many%20professions%2C%0ALLMs%20represent%20an%20invaluable%20tool%20that%20can%20speed%20up%20and%20improve%20the%20quality%20of%0Awork.%20In%20this%20note%2C%20we%20discuss%20to%20what%20extent%20they%20can%20aid%20professional%0Amathematicians.%20We%20first%20provide%20a%20mathematical%20description%20of%20the%20transformer%0Amodel%20used%20in%20all%20modern%20language%20models.%20Based%20on%20recent%20studies%2C%20we%20then%0Aoutline%20best%20practices%20and%20potential%20issues%20and%20report%20on%20the%20mathematical%0Aabilities%20of%20language%20models.%20Finally%2C%20we%20shed%20light%20on%20the%20potential%20of%20LLMs%0Ato%20change%20how%20mathematicians%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04556v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Mathematicians&entry.906535625=Simon%20Frieder%20and%20Julius%20Berner%20and%20Philipp%20Petersen%20and%20Thomas%20Lukasiewicz&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20such%20as%20ChatGPT%20have%20received%20immense%20interest%0Afor%20their%20general-purpose%20language%20understanding%20and%2C%20in%20particular%2C%20their%0Aability%20to%20generate%20high-quality%20text%20or%20computer%20code.%20For%20many%20professions%2C%0ALLMs%20represent%20an%20invaluable%20tool%20that%20can%20speed%20up%20and%20improve%20the%20quality%20of%0Awork.%20In%20this%20note%2C%20we%20discuss%20to%20what%20extent%20they%20can%20aid%20professional%0Amathematicians.%20We%20first%20provide%20a%20mathematical%20description%20of%20the%20transformer%0Amodel%20used%20in%20all%20modern%20language%20models.%20Based%20on%20recent%20studies%2C%20we%20then%0Aoutline%20best%20practices%20and%20potential%20issues%20and%20report%20on%20the%20mathematical%0Aabilities%20of%20language%20models.%20Finally%2C%20we%20shed%20light%20on%20the%20potential%20of%20LLMs%0Ato%20change%20how%20mathematicians%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04556v2&entry.124074799=Read"},
{"title": "Ink and Individuality: Crafting a Personalised Narrative in the Age of\n  LLMs", "author": "Azmine Toushik Wasi and Raima Islam and Mst Rafia Islam", "abstract": "  Individuality and personalization comprise the distinctive characteristics\nthat make each writer unique and influence their words in order to effectively\nengage readers while conveying authenticity. However, our growing reliance on\nLLM-based writing assistants risks compromising our creativity and\nindividuality over time. We often overlook the negative impacts of this trend\non our creativity and uniqueness, despite the possible consequences. This study\ninvestigates these concerns by performing a brief survey to explore different\nperspectives and concepts, as well as trying to understand people's viewpoints,\nin conjunction with past studies in the area. Addressing these issues is\nessential for improving human-computer interaction systems and enhancing\nwriting assistants for personalization and individuality.\n", "link": "http://arxiv.org/abs/2404.00026v2", "date": "2024-04-02", "relevancy": 1.2006, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4048}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.3956}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3932}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ink%20and%20Individuality%3A%20Crafting%20a%20Personalised%20Narrative%20in%20the%20Age%20of%0A%20%20LLMs&body=Title%3A%20Ink%20and%20Individuality%3A%20Crafting%20a%20Personalised%20Narrative%20in%20the%20Age%20of%0A%20%20LLMs%0AAuthor%3A%20Azmine%20Toushik%20Wasi%20and%20Raima%20Islam%20and%20Mst%20Rafia%20Islam%0AAbstract%3A%20%20%20Individuality%20and%20personalization%20comprise%20the%20distinctive%20characteristics%0Athat%20make%20each%20writer%20unique%20and%20influence%20their%20words%20in%20order%20to%20effectively%0Aengage%20readers%20while%20conveying%20authenticity.%20However%2C%20our%20growing%20reliance%20on%0ALLM-based%20writing%20assistants%20risks%20compromising%20our%20creativity%20and%0Aindividuality%20over%20time.%20We%20often%20overlook%20the%20negative%20impacts%20of%20this%20trend%0Aon%20our%20creativity%20and%20uniqueness%2C%20despite%20the%20possible%20consequences.%20This%20study%0Ainvestigates%20these%20concerns%20by%20performing%20a%20brief%20survey%20to%20explore%20different%0Aperspectives%20and%20concepts%2C%20as%20well%20as%20trying%20to%20understand%20people%27s%20viewpoints%2C%0Ain%20conjunction%20with%20past%20studies%20in%20the%20area.%20Addressing%20these%20issues%20is%0Aessential%20for%20improving%20human-computer%20interaction%20systems%20and%20enhancing%0Awriting%20assistants%20for%20personalization%20and%20individuality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00026v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ink%20and%20Individuality%3A%20Crafting%20a%20Personalised%20Narrative%20in%20the%20Age%20of%0A%20%20LLMs&entry.906535625=Azmine%20Toushik%20Wasi%20and%20Raima%20Islam%20and%20Mst%20Rafia%20Islam&entry.1292438233=%20%20Individuality%20and%20personalization%20comprise%20the%20distinctive%20characteristics%0Athat%20make%20each%20writer%20unique%20and%20influence%20their%20words%20in%20order%20to%20effectively%0Aengage%20readers%20while%20conveying%20authenticity.%20However%2C%20our%20growing%20reliance%20on%0ALLM-based%20writing%20assistants%20risks%20compromising%20our%20creativity%20and%0Aindividuality%20over%20time.%20We%20often%20overlook%20the%20negative%20impacts%20of%20this%20trend%0Aon%20our%20creativity%20and%20uniqueness%2C%20despite%20the%20possible%20consequences.%20This%20study%0Ainvestigates%20these%20concerns%20by%20performing%20a%20brief%20survey%20to%20explore%20different%0Aperspectives%20and%20concepts%2C%20as%20well%20as%20trying%20to%20understand%20people%27s%20viewpoints%2C%0Ain%20conjunction%20with%20past%20studies%20in%20the%20area.%20Addressing%20these%20issues%20is%0Aessential%20for%20improving%20human-computer%20interaction%20systems%20and%20enhancing%0Awriting%20assistants%20for%20personalization%20and%20individuality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00026v2&entry.124074799=Read"},
{"title": "PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language\n  Models", "author": "Sihao Hu and Tiansheng Huang and Ling Liu", "abstract": "  We introduce PokeLLMon, the first LLM-embodied agent that achieves\nhuman-parity performance in tactical battle games, as demonstrated in Pokemon\nbattles. The design of PokeLLMon incorporates three key strategies: (i)\nIn-context reinforcement learning that instantly consumes text-based feedback\nderived from battles to iteratively refine the policy; (ii) Knowledge-augmented\ngeneration that retrieves external knowledge to counteract hallucination and\nenables the agent to act timely and properly; (iii) Consistent action\ngeneration to mitigate the panic switching phenomenon when the agent faces a\npowerful opponent and wants to elude the battle. We show that online battles\nagainst human demonstrates PokeLLMon's human-like battle strategies and\njust-in-time decision making, achieving 49% of win rate in the Ladder\ncompetitions and 56% of win rate in the invited battles. Our implementation and\nplayable battle logs are available at: https://github.com/git-disl/PokeLLMon.\n", "link": "http://arxiv.org/abs/2402.01118v3", "date": "2024-04-02", "relevancy": 1.3928, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4732}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4618}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4443}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PokeLLMon%3A%20A%20Human-Parity%20Agent%20for%20Pokemon%20Battles%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20PokeLLMon%3A%20A%20Human-Parity%20Agent%20for%20Pokemon%20Battles%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Sihao%20Hu%20and%20Tiansheng%20Huang%20and%20Ling%20Liu%0AAbstract%3A%20%20%20We%20introduce%20PokeLLMon%2C%20the%20first%20LLM-embodied%20agent%20that%20achieves%0Ahuman-parity%20performance%20in%20tactical%20battle%20games%2C%20as%20demonstrated%20in%20Pokemon%0Abattles.%20The%20design%20of%20PokeLLMon%20incorporates%20three%20key%20strategies%3A%20%28i%29%0AIn-context%20reinforcement%20learning%20that%20instantly%20consumes%20text-based%20feedback%0Aderived%20from%20battles%20to%20iteratively%20refine%20the%20policy%3B%20%28ii%29%20Knowledge-augmented%0Ageneration%20that%20retrieves%20external%20knowledge%20to%20counteract%20hallucination%20and%0Aenables%20the%20agent%20to%20act%20timely%20and%20properly%3B%20%28iii%29%20Consistent%20action%0Ageneration%20to%20mitigate%20the%20panic%20switching%20phenomenon%20when%20the%20agent%20faces%20a%0Apowerful%20opponent%20and%20wants%20to%20elude%20the%20battle.%20We%20show%20that%20online%20battles%0Aagainst%20human%20demonstrates%20PokeLLMon%27s%20human-like%20battle%20strategies%20and%0Ajust-in-time%20decision%20making%2C%20achieving%2049%25%20of%20win%20rate%20in%20the%20Ladder%0Acompetitions%20and%2056%25%20of%20win%20rate%20in%20the%20invited%20battles.%20Our%20implementation%20and%0Aplayable%20battle%20logs%20are%20available%20at%3A%20https%3A//github.com/git-disl/PokeLLMon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01118v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PokeLLMon%3A%20A%20Human-Parity%20Agent%20for%20Pokemon%20Battles%20with%20Large%20Language%0A%20%20Models&entry.906535625=Sihao%20Hu%20and%20Tiansheng%20Huang%20and%20Ling%20Liu&entry.1292438233=%20%20We%20introduce%20PokeLLMon%2C%20the%20first%20LLM-embodied%20agent%20that%20achieves%0Ahuman-parity%20performance%20in%20tactical%20battle%20games%2C%20as%20demonstrated%20in%20Pokemon%0Abattles.%20The%20design%20of%20PokeLLMon%20incorporates%20three%20key%20strategies%3A%20%28i%29%0AIn-context%20reinforcement%20learning%20that%20instantly%20consumes%20text-based%20feedback%0Aderived%20from%20battles%20to%20iteratively%20refine%20the%20policy%3B%20%28ii%29%20Knowledge-augmented%0Ageneration%20that%20retrieves%20external%20knowledge%20to%20counteract%20hallucination%20and%0Aenables%20the%20agent%20to%20act%20timely%20and%20properly%3B%20%28iii%29%20Consistent%20action%0Ageneration%20to%20mitigate%20the%20panic%20switching%20phenomenon%20when%20the%20agent%20faces%20a%0Apowerful%20opponent%20and%20wants%20to%20elude%20the%20battle.%20We%20show%20that%20online%20battles%0Aagainst%20human%20demonstrates%20PokeLLMon%27s%20human-like%20battle%20strategies%20and%0Ajust-in-time%20decision%20making%2C%20achieving%2049%25%20of%20win%20rate%20in%20the%20Ladder%0Acompetitions%20and%2056%25%20of%20win%20rate%20in%20the%20invited%20battles.%20Our%20implementation%20and%0Aplayable%20battle%20logs%20are%20available%20at%3A%20https%3A//github.com/git-disl/PokeLLMon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01118v3&entry.124074799=Read"},
{"title": "Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights,\n  and Duties", "author": "Taylor Sorensen and Liwei Jiang and Jena Hwang and Sydney Levine and Valentina Pyatkin and Peter West and Nouha Dziri and Ximing Lu and Kavel Rao and Chandra Bhagavatula and Maarten Sap and John Tasioulas and Yejin Choi", "abstract": "  Human values are crucial to human decision-making. Value pluralism is the\nview that multiple correct values may be held in tension with one another\n(e.g., when considering lying to a friend to protect their feelings, how does\none balance honesty with friendship?). As statistical learners, AI systems fit\nto averages by default, washing out these potentially irreducible value\nconflicts. To improve AI systems to better reflect value pluralism, the\nfirst-order challenge is to explore the extent to which AI systems can model\npluralistic human values, rights, and duties as well as their interaction.\n  We introduce ValuePrism, a large-scale dataset of 218k values, rights, and\nduties connected to 31k human-written situations. ValuePrism's contextualized\nvalues are generated by GPT-4 and deemed high-quality by human annotators 91%\nof the time. We conduct a large-scale study with annotators across diverse\nsocial and demographic backgrounds to try to understand whose values are\nrepresented.\n  With ValuePrism, we build Kaleido, an open, light-weight, and structured\nlanguage-based multi-task model that generates, explains, and assesses the\nrelevance and valence (i.e., support or oppose) of human values, rights, and\nduties within a specific context. Humans prefer the sets of values output by\nour system over the teacher GPT-4, finding them more accurate and with broader\ncoverage. In addition, we demonstrate that Kaleido can help explain variability\nin human decision-making by outputting contrasting values. Finally, we show\nthat Kaleido's representations transfer to other philosophical frameworks and\ndatasets, confirming the benefit of an explicit, modular, and interpretable\napproach to value pluralism. We hope that our work will serve as a step to\nmaking more explicit the implicit values behind human decision-making and to\nsteering AI systems to make decisions that are more in accordance with them.\n", "link": "http://arxiv.org/abs/2309.00779v2", "date": "2024-04-02", "relevancy": 1.3534, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4449}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4445}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Value%20Kaleidoscope%3A%20Engaging%20AI%20with%20Pluralistic%20Human%20Values%2C%20Rights%2C%0A%20%20and%20Duties&body=Title%3A%20Value%20Kaleidoscope%3A%20Engaging%20AI%20with%20Pluralistic%20Human%20Values%2C%20Rights%2C%0A%20%20and%20Duties%0AAuthor%3A%20Taylor%20Sorensen%20and%20Liwei%20Jiang%20and%20Jena%20Hwang%20and%20Sydney%20Levine%20and%20Valentina%20Pyatkin%20and%20Peter%20West%20and%20Nouha%20Dziri%20and%20Ximing%20Lu%20and%20Kavel%20Rao%20and%20Chandra%20Bhagavatula%20and%20Maarten%20Sap%20and%20John%20Tasioulas%20and%20Yejin%20Choi%0AAbstract%3A%20%20%20Human%20values%20are%20crucial%20to%20human%20decision-making.%20Value%20pluralism%20is%20the%0Aview%20that%20multiple%20correct%20values%20may%20be%20held%20in%20tension%20with%20one%20another%0A%28e.g.%2C%20when%20considering%20lying%20to%20a%20friend%20to%20protect%20their%20feelings%2C%20how%20does%0Aone%20balance%20honesty%20with%20friendship%3F%29.%20As%20statistical%20learners%2C%20AI%20systems%20fit%0Ato%20averages%20by%20default%2C%20washing%20out%20these%20potentially%20irreducible%20value%0Aconflicts.%20To%20improve%20AI%20systems%20to%20better%20reflect%20value%20pluralism%2C%20the%0Afirst-order%20challenge%20is%20to%20explore%20the%20extent%20to%20which%20AI%20systems%20can%20model%0Apluralistic%20human%20values%2C%20rights%2C%20and%20duties%20as%20well%20as%20their%20interaction.%0A%20%20We%20introduce%20ValuePrism%2C%20a%20large-scale%20dataset%20of%20218k%20values%2C%20rights%2C%20and%0Aduties%20connected%20to%2031k%20human-written%20situations.%20ValuePrism%27s%20contextualized%0Avalues%20are%20generated%20by%20GPT-4%20and%20deemed%20high-quality%20by%20human%20annotators%2091%25%0Aof%20the%20time.%20We%20conduct%20a%20large-scale%20study%20with%20annotators%20across%20diverse%0Asocial%20and%20demographic%20backgrounds%20to%20try%20to%20understand%20whose%20values%20are%0Arepresented.%0A%20%20With%20ValuePrism%2C%20we%20build%20Kaleido%2C%20an%20open%2C%20light-weight%2C%20and%20structured%0Alanguage-based%20multi-task%20model%20that%20generates%2C%20explains%2C%20and%20assesses%20the%0Arelevance%20and%20valence%20%28i.e.%2C%20support%20or%20oppose%29%20of%20human%20values%2C%20rights%2C%20and%0Aduties%20within%20a%20specific%20context.%20Humans%20prefer%20the%20sets%20of%20values%20output%20by%0Aour%20system%20over%20the%20teacher%20GPT-4%2C%20finding%20them%20more%20accurate%20and%20with%20broader%0Acoverage.%20In%20addition%2C%20we%20demonstrate%20that%20Kaleido%20can%20help%20explain%20variability%0Ain%20human%20decision-making%20by%20outputting%20contrasting%20values.%20Finally%2C%20we%20show%0Athat%20Kaleido%27s%20representations%20transfer%20to%20other%20philosophical%20frameworks%20and%0Adatasets%2C%20confirming%20the%20benefit%20of%20an%20explicit%2C%20modular%2C%20and%20interpretable%0Aapproach%20to%20value%20pluralism.%20We%20hope%20that%20our%20work%20will%20serve%20as%20a%20step%20to%0Amaking%20more%20explicit%20the%20implicit%20values%20behind%20human%20decision-making%20and%20to%0Asteering%20AI%20systems%20to%20make%20decisions%20that%20are%20more%20in%20accordance%20with%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00779v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Value%20Kaleidoscope%3A%20Engaging%20AI%20with%20Pluralistic%20Human%20Values%2C%20Rights%2C%0A%20%20and%20Duties&entry.906535625=Taylor%20Sorensen%20and%20Liwei%20Jiang%20and%20Jena%20Hwang%20and%20Sydney%20Levine%20and%20Valentina%20Pyatkin%20and%20Peter%20West%20and%20Nouha%20Dziri%20and%20Ximing%20Lu%20and%20Kavel%20Rao%20and%20Chandra%20Bhagavatula%20and%20Maarten%20Sap%20and%20John%20Tasioulas%20and%20Yejin%20Choi&entry.1292438233=%20%20Human%20values%20are%20crucial%20to%20human%20decision-making.%20Value%20pluralism%20is%20the%0Aview%20that%20multiple%20correct%20values%20may%20be%20held%20in%20tension%20with%20one%20another%0A%28e.g.%2C%20when%20considering%20lying%20to%20a%20friend%20to%20protect%20their%20feelings%2C%20how%20does%0Aone%20balance%20honesty%20with%20friendship%3F%29.%20As%20statistical%20learners%2C%20AI%20systems%20fit%0Ato%20averages%20by%20default%2C%20washing%20out%20these%20potentially%20irreducible%20value%0Aconflicts.%20To%20improve%20AI%20systems%20to%20better%20reflect%20value%20pluralism%2C%20the%0Afirst-order%20challenge%20is%20to%20explore%20the%20extent%20to%20which%20AI%20systems%20can%20model%0Apluralistic%20human%20values%2C%20rights%2C%20and%20duties%20as%20well%20as%20their%20interaction.%0A%20%20We%20introduce%20ValuePrism%2C%20a%20large-scale%20dataset%20of%20218k%20values%2C%20rights%2C%20and%0Aduties%20connected%20to%2031k%20human-written%20situations.%20ValuePrism%27s%20contextualized%0Avalues%20are%20generated%20by%20GPT-4%20and%20deemed%20high-quality%20by%20human%20annotators%2091%25%0Aof%20the%20time.%20We%20conduct%20a%20large-scale%20study%20with%20annotators%20across%20diverse%0Asocial%20and%20demographic%20backgrounds%20to%20try%20to%20understand%20whose%20values%20are%0Arepresented.%0A%20%20With%20ValuePrism%2C%20we%20build%20Kaleido%2C%20an%20open%2C%20light-weight%2C%20and%20structured%0Alanguage-based%20multi-task%20model%20that%20generates%2C%20explains%2C%20and%20assesses%20the%0Arelevance%20and%20valence%20%28i.e.%2C%20support%20or%20oppose%29%20of%20human%20values%2C%20rights%2C%20and%0Aduties%20within%20a%20specific%20context.%20Humans%20prefer%20the%20sets%20of%20values%20output%20by%0Aour%20system%20over%20the%20teacher%20GPT-4%2C%20finding%20them%20more%20accurate%20and%20with%20broader%0Acoverage.%20In%20addition%2C%20we%20demonstrate%20that%20Kaleido%20can%20help%20explain%20variability%0Ain%20human%20decision-making%20by%20outputting%20contrasting%20values.%20Finally%2C%20we%20show%0Athat%20Kaleido%27s%20representations%20transfer%20to%20other%20philosophical%20frameworks%20and%0Adatasets%2C%20confirming%20the%20benefit%20of%20an%20explicit%2C%20modular%2C%20and%20interpretable%0Aapproach%20to%20value%20pluralism.%20We%20hope%20that%20our%20work%20will%20serve%20as%20a%20step%20to%0Amaking%20more%20explicit%20the%20implicit%20values%20behind%20human%20decision-making%20and%20to%0Asteering%20AI%20systems%20to%20make%20decisions%20that%20are%20more%20in%20accordance%20with%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00779v2&entry.124074799=Read"},
{"title": "Deciphering the Interplay between Local Differential Privacy, Average\n  Bayesian Privacy, and Maximum Bayesian Privacy", "author": "Xiaojin Zhang and Yulin Fei and Wei Chen", "abstract": "  The swift evolution of machine learning has led to emergence of various\ndefinitions of privacy due to the threats it poses to privacy, including the\nconcept of local differential privacy (LDP). Although widely embraced and\nutilized across numerous domains, this conventional approach to measure privacy\nstill exhibits certain limitations, spanning from failure to prevent\ninferential disclosure to lack of consideration for the adversary's background\nknowledge. In this comprehensive study, we introduce Bayesian privacy and delve\ninto the intricate relationship between LDP and its Bayesian counterparts,\nunveiling novel insights into utility-privacy trade-offs. We introduce a\nframework that encapsulates both attack and defense strategies, highlighting\ntheir interplay and effectiveness. The relationship between LDP and Maximum\nBayesian Privacy (MBP) is first revealed, demonstrating that under uniform\nprior distribution, a mechanism satisfying $\\xi$-LDP will satisfy $\\xi$-MBP and\nconversely $\\xi$-MBP also confers 2$\\xi$-LDP. Our next theoretical contribution\nare anchored in the rigorous definitions and relationships between Average\nBayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by\nequations $\\epsilon_{p,a} \\leq \\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} +\n\\epsilon)\\cdot(e^{\\epsilon_{p,m} + \\epsilon} - 1)}$. These relationships\nfortify our understanding of the privacy guarantees provided by various\nmechanisms. Our work not only lays the groundwork for future empirical\nexploration but also promises to facilitate the design of privacy-preserving\nalgorithms, thereby fostering the development of trustworthy machine learning\nsolutions.\n", "link": "http://arxiv.org/abs/2403.16591v3", "date": "2024-04-02", "relevancy": 1.3764, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4676}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4566}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4561}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deciphering%20the%20Interplay%20between%20Local%20Differential%20Privacy%2C%20Average%0A%20%20Bayesian%20Privacy%2C%20and%20Maximum%20Bayesian%20Privacy&body=Title%3A%20Deciphering%20the%20Interplay%20between%20Local%20Differential%20Privacy%2C%20Average%0A%20%20Bayesian%20Privacy%2C%20and%20Maximum%20Bayesian%20Privacy%0AAuthor%3A%20Xiaojin%20Zhang%20and%20Yulin%20Fei%20and%20Wei%20Chen%0AAbstract%3A%20%20%20The%20swift%20evolution%20of%20machine%20learning%20has%20led%20to%20emergence%20of%20various%0Adefinitions%20of%20privacy%20due%20to%20the%20threats%20it%20poses%20to%20privacy%2C%20including%20the%0Aconcept%20of%20local%20differential%20privacy%20%28LDP%29.%20Although%20widely%20embraced%20and%0Autilized%20across%20numerous%20domains%2C%20this%20conventional%20approach%20to%20measure%20privacy%0Astill%20exhibits%20certain%20limitations%2C%20spanning%20from%20failure%20to%20prevent%0Ainferential%20disclosure%20to%20lack%20of%20consideration%20for%20the%20adversary%27s%20background%0Aknowledge.%20In%20this%20comprehensive%20study%2C%20we%20introduce%20Bayesian%20privacy%20and%20delve%0Ainto%20the%20intricate%20relationship%20between%20LDP%20and%20its%20Bayesian%20counterparts%2C%0Aunveiling%20novel%20insights%20into%20utility-privacy%20trade-offs.%20We%20introduce%20a%0Aframework%20that%20encapsulates%20both%20attack%20and%20defense%20strategies%2C%20highlighting%0Atheir%20interplay%20and%20effectiveness.%20The%20relationship%20between%20LDP%20and%20Maximum%0ABayesian%20Privacy%20%28MBP%29%20is%20first%20revealed%2C%20demonstrating%20that%20under%20uniform%0Aprior%20distribution%2C%20a%20mechanism%20satisfying%20%24%5Cxi%24-LDP%20will%20satisfy%20%24%5Cxi%24-MBP%20and%0Aconversely%20%24%5Cxi%24-MBP%20also%20confers%202%24%5Cxi%24-LDP.%20Our%20next%20theoretical%20contribution%0Aare%20anchored%20in%20the%20rigorous%20definitions%20and%20relationships%20between%20Average%0ABayesian%20Privacy%20%28ABP%29%20and%20Maximum%20Bayesian%20Privacy%20%28MBP%29%2C%20encapsulated%20by%0Aequations%20%24%5Cepsilon_%7Bp%2Ca%7D%20%5Cleq%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Csqrt%7B%28%5Cepsilon_%7Bp%2Cm%7D%20%2B%0A%5Cepsilon%29%5Ccdot%28e%5E%7B%5Cepsilon_%7Bp%2Cm%7D%20%2B%20%5Cepsilon%7D%20-%201%29%7D%24.%20These%20relationships%0Afortify%20our%20understanding%20of%20the%20privacy%20guarantees%20provided%20by%20various%0Amechanisms.%20Our%20work%20not%20only%20lays%20the%20groundwork%20for%20future%20empirical%0Aexploration%20but%20also%20promises%20to%20facilitate%20the%20design%20of%20privacy-preserving%0Aalgorithms%2C%20thereby%20fostering%20the%20development%20of%20trustworthy%20machine%20learning%0Asolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16591v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deciphering%20the%20Interplay%20between%20Local%20Differential%20Privacy%2C%20Average%0A%20%20Bayesian%20Privacy%2C%20and%20Maximum%20Bayesian%20Privacy&entry.906535625=Xiaojin%20Zhang%20and%20Yulin%20Fei%20and%20Wei%20Chen&entry.1292438233=%20%20The%20swift%20evolution%20of%20machine%20learning%20has%20led%20to%20emergence%20of%20various%0Adefinitions%20of%20privacy%20due%20to%20the%20threats%20it%20poses%20to%20privacy%2C%20including%20the%0Aconcept%20of%20local%20differential%20privacy%20%28LDP%29.%20Although%20widely%20embraced%20and%0Autilized%20across%20numerous%20domains%2C%20this%20conventional%20approach%20to%20measure%20privacy%0Astill%20exhibits%20certain%20limitations%2C%20spanning%20from%20failure%20to%20prevent%0Ainferential%20disclosure%20to%20lack%20of%20consideration%20for%20the%20adversary%27s%20background%0Aknowledge.%20In%20this%20comprehensive%20study%2C%20we%20introduce%20Bayesian%20privacy%20and%20delve%0Ainto%20the%20intricate%20relationship%20between%20LDP%20and%20its%20Bayesian%20counterparts%2C%0Aunveiling%20novel%20insights%20into%20utility-privacy%20trade-offs.%20We%20introduce%20a%0Aframework%20that%20encapsulates%20both%20attack%20and%20defense%20strategies%2C%20highlighting%0Atheir%20interplay%20and%20effectiveness.%20The%20relationship%20between%20LDP%20and%20Maximum%0ABayesian%20Privacy%20%28MBP%29%20is%20first%20revealed%2C%20demonstrating%20that%20under%20uniform%0Aprior%20distribution%2C%20a%20mechanism%20satisfying%20%24%5Cxi%24-LDP%20will%20satisfy%20%24%5Cxi%24-MBP%20and%0Aconversely%20%24%5Cxi%24-MBP%20also%20confers%202%24%5Cxi%24-LDP.%20Our%20next%20theoretical%20contribution%0Aare%20anchored%20in%20the%20rigorous%20definitions%20and%20relationships%20between%20Average%0ABayesian%20Privacy%20%28ABP%29%20and%20Maximum%20Bayesian%20Privacy%20%28MBP%29%2C%20encapsulated%20by%0Aequations%20%24%5Cepsilon_%7Bp%2Ca%7D%20%5Cleq%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Csqrt%7B%28%5Cepsilon_%7Bp%2Cm%7D%20%2B%0A%5Cepsilon%29%5Ccdot%28e%5E%7B%5Cepsilon_%7Bp%2Cm%7D%20%2B%20%5Cepsilon%7D%20-%201%29%7D%24.%20These%20relationships%0Afortify%20our%20understanding%20of%20the%20privacy%20guarantees%20provided%20by%20various%0Amechanisms.%20Our%20work%20not%20only%20lays%20the%20groundwork%20for%20future%20empirical%0Aexploration%20but%20also%20promises%20to%20facilitate%20the%20design%20of%20privacy-preserving%0Aalgorithms%2C%20thereby%20fostering%20the%20development%20of%20trustworthy%20machine%20learning%0Asolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16591v3&entry.124074799=Read"},
{"title": "LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership\n  and Reasoning", "author": "Azmine Toushik Wasi and Mst Rafia Islam and Raima Islam", "abstract": "  Sense of ownership in writing confines our investment of thoughts, time, and\ncontribution, leading to attachment to the output. However, using writing\nassistants introduces a mental dilemma, as some content isn't directly our\ncreation. For instance, we tend to credit Large Language Models (LLMs) more in\ncreative tasks, even though all tasks are equal for them. Additionally, while\nwe may not claim complete ownership of LLM-generated content, we freely claim\nauthorship. We conduct a short survey to examine these issues and understand\nunderlying cognitive processes in order to gain a better knowledge of\nhuman-computer interaction in writing and improve writing aid systems.\n", "link": "http://arxiv.org/abs/2404.00027v2", "date": "2024-04-02", "relevancy": 1.1954, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.414}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4099}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3877}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLMs%20as%20Writing%20Assistants%3A%20Exploring%20Perspectives%20on%20Sense%20of%20Ownership%0A%20%20and%20Reasoning&body=Title%3A%20LLMs%20as%20Writing%20Assistants%3A%20Exploring%20Perspectives%20on%20Sense%20of%20Ownership%0A%20%20and%20Reasoning%0AAuthor%3A%20Azmine%20Toushik%20Wasi%20and%20Mst%20Rafia%20Islam%20and%20Raima%20Islam%0AAbstract%3A%20%20%20Sense%20of%20ownership%20in%20writing%20confines%20our%20investment%20of%20thoughts%2C%20time%2C%20and%0Acontribution%2C%20leading%20to%20attachment%20to%20the%20output.%20However%2C%20using%20writing%0Aassistants%20introduces%20a%20mental%20dilemma%2C%20as%20some%20content%20isn%27t%20directly%20our%0Acreation.%20For%20instance%2C%20we%20tend%20to%20credit%20Large%20Language%20Models%20%28LLMs%29%20more%20in%0Acreative%20tasks%2C%20even%20though%20all%20tasks%20are%20equal%20for%20them.%20Additionally%2C%20while%0Awe%20may%20not%20claim%20complete%20ownership%20of%20LLM-generated%20content%2C%20we%20freely%20claim%0Aauthorship.%20We%20conduct%20a%20short%20survey%20to%20examine%20these%20issues%20and%20understand%0Aunderlying%20cognitive%20processes%20in%20order%20to%20gain%20a%20better%20knowledge%20of%0Ahuman-computer%20interaction%20in%20writing%20and%20improve%20writing%20aid%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00027v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20as%20Writing%20Assistants%3A%20Exploring%20Perspectives%20on%20Sense%20of%20Ownership%0A%20%20and%20Reasoning&entry.906535625=Azmine%20Toushik%20Wasi%20and%20Mst%20Rafia%20Islam%20and%20Raima%20Islam&entry.1292438233=%20%20Sense%20of%20ownership%20in%20writing%20confines%20our%20investment%20of%20thoughts%2C%20time%2C%20and%0Acontribution%2C%20leading%20to%20attachment%20to%20the%20output.%20However%2C%20using%20writing%0Aassistants%20introduces%20a%20mental%20dilemma%2C%20as%20some%20content%20isn%27t%20directly%20our%0Acreation.%20For%20instance%2C%20we%20tend%20to%20credit%20Large%20Language%20Models%20%28LLMs%29%20more%20in%0Acreative%20tasks%2C%20even%20though%20all%20tasks%20are%20equal%20for%20them.%20Additionally%2C%20while%0Awe%20may%20not%20claim%20complete%20ownership%20of%20LLM-generated%20content%2C%20we%20freely%20claim%0Aauthorship.%20We%20conduct%20a%20short%20survey%20to%20examine%20these%20issues%20and%20understand%0Aunderlying%20cognitive%20processes%20in%20order%20to%20gain%20a%20better%20knowledge%20of%0Ahuman-computer%20interaction%20in%20writing%20and%20improve%20writing%20aid%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00027v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


